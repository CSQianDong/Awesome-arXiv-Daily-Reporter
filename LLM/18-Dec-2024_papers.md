# Are Your LLMs Capable of Stable Reasoning? 

**Title (ZH)**: 你的大型语言模型具备稳定推理的能力吗？ 

**Authors**: Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.13147)  

**Abstract**: The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model's peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs' "realistic" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）的迅速发展已经在复杂推理任务中展现出了显著的进步。然而，基准性能与实际应用之间仍存在着显著的差距。我们发现这一差距主要源于当前的评估协议和指标，这些评估方法未能充分捕捉LLM的全部能力，尤其是在关键的复杂推理任务中，准确性和一致性至关重要。本研究做出了两项关键贡献。首先，我们引入了一种新颖的评估指标G-Pass@k，它可以在多次采样尝试中提供连续的模型性能评估，量化模型的最佳性能潜力及其稳定性。其次，我们提出了LiveMathBench，这是一种动态基准，包含了一系列具有挑战性的当代数学问题，旨在尽量减少评估过程中的数据泄露风险。通过在state-of-the-art LLMs上使用G-Pass@k和LiveMathBench进行广泛实验，我们提供了关于这两者最大能力和操作一致性的全面见解。研究结果揭示了LLMs在“实际”推理能力方面存在很大程度的改进空间，突显了需要更加稳健的评估方法的需求。该基准及其详细结果可在以下链接获取：this https URL。 

---
# A Survey of Calibration Process for Black-Box LLMs 

**Title (ZH)**: 黑盒大语言模型校准过程综述 

**Authors**: Liangru Xie, Hui Liu, Jingying Zeng, Xianfeng Tang, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Qi He  

**Link**: [PDF](https://arxiv.org/pdf/2412.12767)  

**Abstract**: Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment. This is our GitHub link: this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）在语义理解和生成方面表现出色，但准确评估其输出可靠性仍然是一个重大挑战。尽管已有许多研究探索了校准技术，这些研究主要集中在具有可访问参数的白盒LLMs上。相比之下，虽然黑盒LLMs在性能上更胜一筹，但由于其仅限于API的交互限制，对校准技术的要求更高。尽管最近对黑盒LLMs的校准取得了突破，但这些方法的系统性综述仍然缺乏。为填补这一空白，我们首次提供了黑盒LLMs校准技术的全面综述。我们首先将LLMs的校准过程定义为包含两个相互关联的关键步骤：置信度估计和校准。其次，我们对适用于黑盒环境的适用方法进行了系统的回顾，并提供有关执行这些关键步骤的独特挑战和联系的见解。此外，我们探讨了黑盒LLMs中校准过程的典型应用，并概述了具有前景的未来研究方向，为提高可靠性和人机对齐提供了新的视角。以下是我们的GitHub链接：![GitHub链接](this https URL) 

---
# How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games 

**Title (ZH)**: 不同AI聊天机器人表现出怎样的行为？大型语言模型在行为经济学游戏中的基准测试 

**Authors**: Yutong Xie, Yiyao Liu, Zhuang Ma, Lin Shi, Xiyuan Wang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei  

**Link**: [PDF](https://arxiv.org/pdf/2412.12362)  

**Abstract**: The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns. As a supplement to a recent study on the behavioral Turing test, this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios. The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles. 

**Abstract (ZH)**: 将大型语言模型（LLMs）应用于各种不同的领域需要对其决策机制和行为模式有全面的理解。作为对近期关于行为图灵测试研究的补充，本文对五种领先的大语言模型（LLM）基聊天机器人家族在一系列行为经济学游戏中表现出的行为进行了综合分析。通过对比这些AI聊天机器人的表现，我们旨在揭示和记录在不同情境下它们的共同和独特的行为模式。这些发现为理解每个LLM的战略偏好提供了宝贵的洞见，并凸显了它们在关键决策角色中部署的潜在影响。 

---
# AI PERSONA: Towards Life-long Personalization of LLMs 

**Title (ZH)**: AI 人格：走向大规模语言模型的终身个性化 

**Authors**: Tiannan Wang, Meiling Tao, Ruoyu Fang, Huilin Wang, Shuai Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.13103)  

**Abstract**: In this work, we introduce the task of life-long personalization of large language models. While recent mainstream efforts in the LLM community mainly focus on scaling data and compute for improved capabilities of LLMs, we argue that it is also very important to enable LLM systems, or language agents, to continuously adapt to the diverse and ever-changing profiles of every distinct user and provide up-to-date personalized assistance. We provide a clear task formulation and introduce a simple, general, effective, and scalable framework for life-long personalization of LLM systems and language agents. To facilitate future research on LLM personalization, we also introduce methods to synthesize realistic benchmarks and robust evaluation metrics. We will release all codes and data for building and benchmarking life-long personalized LLM systems. 

**Abstract (ZH)**: 在本文中，我们探讨了大型语言模型终身个性化这一任务。尽管语言模型（LLM）社区近期的主要努力主要集中在通过扩大数据和计算资源来提高语言模型的能力，我们认为同样重要的是使语言模型系统或语言代理能够持续适应每个独特用户的多样且不断变化的特征，并提供及时个性化的辅助。我们提供了一个明确的任务定义，并引入了一个简单、通用、有效且可扩展的框架，用于终身个性化语言模型系统和语言代理。为了促进未来在语言模型个性化方面的研究，我们还介绍了合成现实基准和稳健评估指标的方法。我们将提供构建和基准测试终身个性化语言模型系统所需的所有代码和数据。 

---
# LMUnit: Fine-grained Evaluation with Natural Language Unit Tests 

**Title (ZH)**: LMUnit：基于自然语言单元测试的细粒度评估 

**Authors**: Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, Shikib Mehri  

**Link**: [PDF](https://arxiv.org/pdf/2412.13091)  

**Abstract**: As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals. We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales. Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows. LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench. These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development. 

**Abstract (ZH)**: 随着语言模型在关键工作流程中发挥越来越重要的作用，评估其行为仍然是一项基本挑战——人工评估成本高且容易出错，而自动化指标只能提供粗略且难以解释的信号。我们提出了自然语言单元测试这一范式，该范式将响应质量分解为明确且可测试的标准，并结合了一个统一的评分模型LMUnit，该模型结合了多目标训练、直接评价和自然语言理由。通过控制实验的人类研究，我们展示了这一范式显著提高了注释者间的一致性，并使语言模型（LLM）开发流程更为有效。LMUnit在评估基准（FLASK、BigGenBench）上达到了最先进的性能，并在奖励基准（RewardBench）上取得了竞争力的结果。这些结果验证了我们提出的范式和评分模型，表明了为语言模型评估和开发确立一条有前景的道路的可能性。 

---
# RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement 

**Title (ZH)**: RAG-Star: 基于检索增强验证与精炼的 deliberative reasoning 提升方法 

**Authors**: Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, Tao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12881)  

**Abstract**: Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods. 

**Abstract (ZH)**: 现有的大型语言模型（LLMs）显示出卓越的问题解决能力，但在处理复杂的推理任务时可能会遇到困难。尽管链式思考和树状搜索方法取得了成功，但它们主要依赖于LLMs的内部知识来进行中间推理步骤的搜索，局限在处理涉及较少推理步骤的简单任务上。在本文中，我们提出了一种新的RAG方法——\textbf{RAG-Star}，该方法将检索到的信息用于指导依赖于LLMs内部知识的树状讨论性推理过程。通过利用蒙特卡洛树搜索（MCTS），RAG-Star迭代地规划基于LLMs本身的中间子查询和答案来进行推理。为了整合内部和外部知识，我们提出了一种检索增强验证方法，利用查询和答案感知的奖励建模为LLMs的内在推理提供反馈。我们的实验涉及Llama-3.1-8B-Instruct和GPT-4o表明，RAG-Star在之前的RAG和推理方法上表现出显著的优越性。 

---
# ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models 

**Title (ZH)**: ClarityEthic: 可解释的道德判断利用大规模语言模型的对比伦理洞察 

**Authors**: Yuxi Sun, Wei Gao, Jing Ma, Hongzhan Lin, Ziyang Luo, Wenxuan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12848)  

**Abstract**: With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., "don't cheat,"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的兴起及其广泛应用，确保其安全性变得至关重要，以防止对人类造成伤害并促进道德行为。然而，通过大规模数据训练直接评估价值倾向（即支持或反对）是不可靠且难以解释的。我们假设让LLMs模仿人类依靠社会规范来做出道德决策，有助于它们理解和预测道德判断。然而，捕捉人类价值仍然充满挑战，因为在特定背景下，多种相关的规范可能会发生冲突。考虑那些被大多数人接受并促进社会福祉的规范（例如，“不欺骗”）更容易被接受和广泛采纳。因此，在做出道德决策之前，了解适用于特定场景的适当规范对于LLMs来说至关重要。为实现这一目标，我们提出了一种名为**ClarityEthic**的新型道德判断方法，该方法利用LLMs的推理能力和对比学习，从不同角度揭示与人类行为相关的社会规范，并选择最可靠的规范以提高判断准确性。大量的实验证明，我们的方法在道德判断任务中优于现有最先进的方法。此外，人类评估确认生成的社会规范为判断提供了合理的解释，这表明模仿人类的道德策略建模人类的道德判断有望提高LLMs的伦理行为。 

---
# DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models 

**Title (ZH)**: DSGram：大型语言模型时代基于动态加权子指标的语法错误修正 

**Authors**: Jinxiang Xie, Yilin Li, Xunjian Yin, Xiaojun Wan  

**Link**: [PDF](https://arxiv.org/pdf/2412.12832)  

**Abstract**: Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations. 

**Abstract (ZH)**: 评估语法错误修正（GEC）模型的表现越来越具有挑战性，因为基于大规模语言模型（LLM）的GEC系统常常会产生与提供的黄金标准参考偏离较大的修正结果。这种偏离性削弱了传统基于参考的标准评估指标的可靠性。在本研究中，我们提出了一种新的评估框架——DSGram，该框架综合了语义连贯性、编辑级别和流畅性，并采用了动态权重机制。我们的框架结合使用了层次分析过程（AHP）和大规模语言模型来确定各种评估标准的相对重要性。此外，我们还开发了一个包含人类标注和大型语言模型模拟句子的数据集，用于验证我们的算法并微调更经济有效的模型。实验结果表明，我们提出的这种方法增强了GEC模型评估的有效性。 

---
# Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning 

**Title (ZH)**: 通过常识推理检测讽刺中的情感不一致 

**Authors**: Ziqi Qiu, Jianxing Yu, Yufeng Zhang, Hanjiang Lai, Yanghui Rao, Qinliang Su, Jian Yin  

**Link**: [PDF](https://arxiv.org/pdf/2412.12808)  

**Abstract**: This paper focuses on sarcasm detection, which aims to identify whether given statements convey criticism, mockery, or other negative sentiment opposite to the literal meaning. To detect sarcasm, humans often require a comprehensive understanding of the semantics in the statement and even resort to external commonsense to infer the fine-grained incongruity. However, existing methods lack commonsense inferential ability when they face complex real-world scenarios, leading to unsatisfactory performance. To address this problem, we propose a novel framework for sarcasm detection, which conducts incongruity reasoning based on commonsense augmentation, called EICR. Concretely, we first employ retrieval-augmented large language models to supplement the missing but indispensable commonsense background knowledge. To capture complex contextual associations, we construct a dependency graph and obtain the optimized topology via graph refinement. We further introduce an adaptive reasoning skeleton that integrates prior rules to extract sentiment-inconsistent subgraphs explicitly. To eliminate the possible spurious relations between words and labels, we employ adversarial contrastive learning to enhance the robustness of the detector. Experiments conducted on five datasets demonstrate the effectiveness of EICR. 

**Abstract (ZH)**: 本文专注于语境讽刺检测研究，其目标是识别给定陈述是否传达了与字面意思相反的批评、嘲笑或其他负面情感。为了检测讽刺，人类通常需要全面理解语句的语义，并且甚至需要利用常识推理来推断细微的不一致。然而，现有的方法在面对复杂的真实世界场景时缺乏常识推理能力，导致性能不佳。为了解决这一问题，我们提出了一种新颖的讽刺检测框架，该框架基于常识增强进行不一致推理，称为EICR。具体而言，我们首先采用检索增强的大语言模型来补充缺失但不可或缺的常识背景知识。为了捕捉复杂的上下文关联，我们构建了一个依赖图，并通过图优化获得最佳拓扑结构。进一步引入一种适应性推理框架，该框架整合先验规则以明确提取情感不一致的子图。为了消除单词与标签之间可能的虚假关系，我们采用了对抗对比学习以增强检测器的鲁棒性。在五个数据集上的实验结果表明了EICR的有效性。 

---
# Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree 

**Title (ZH)**: Falcon：通过增强半自回归草稿生成和定制解码树实现的大语言模型更快并行推理 

**Authors**: Xiangxiang Gao, Weisheng Xie, Yiwei Xiang, Feng Ji  

**Link**: [PDF](https://arxiv.org/pdf/2412.12639)  

**Abstract**: Striking an optimal balance between minimal drafting latency and high speculation accuracy to enhance the inference speed of Large Language Models remains a significant challenge in speculative decoding. In this paper, we introduce Falcon, an innovative semi-autoregressive speculative decoding framework fashioned to augment both the drafter's parallelism and output quality. Falcon incorporates the Coupled Sequential Glancing Distillation technique, which fortifies inter-token dependencies within the same block, leading to increased speculation accuracy. We offer a comprehensive theoretical analysis to illuminate the underlying mechanisms. Additionally, we introduce a Custom-Designed Decoding Tree, which permits the drafter to generate multiple tokens in a single forward pass and accommodates multiple forward passes as needed, thereby boosting the number of drafted tokens and significantly improving the overall acceptance rate. Comprehensive evaluations on benchmark datasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior acceleration capabilities. The framework achieves a lossless speedup ratio ranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model series. These results outstrip existing speculative decoding methods for LLMs, including Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact drafter architecture equivalent to merely two Transformer layers. 

**Abstract (ZH)**: 在推断速度快的同时保持最小的编码延迟和高猜测准确性之间达到最优平衡，依然是推测解码中的一大挑战。本文提出了一种名为Falcon的创新半自回归推测解码框架，旨在增强编稿者的并行性和输出质量。Falcon采用了一种称为耦合序列凝视蒸馏的技术，增强同一块内各个词汇之间的依赖关系，从而提高了猜测的准确性。我们进行了全面的理论分析，以阐明其背后的机制。此外，我们还引入了一种自定义解码树，该树允许编稿者在单次前向传递中生成多个词汇，并在需要时支持多次前向传递，从而增加了被编写的词汇数量，并显著提高了整体接受率。在MT-Bench、HumanEval和GSM8K等基准数据集上的综合评估表明，Falcon具有卓越的加速能力。在测试的Vicuna和LLaMA2-Chat模型系列上，框架实现了无损加速比从2.91倍到3.51倍的效果。这些结果超越了现有的针对大规模语言模型（LLM）的推测解码方法，如Eagle、Medusa、Lookahead、SPS和PLD，同时保持一个紧凑的编稿者架构，相当于仅两个Transformer层。 

---
# What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context 

**Title (ZH)**: LLMs倾向于偏好哪种外部知识？Characterizing and Exploring Chain of Evidence in Imperfect Context的学术译文如下：

LLMs偏好哪种外部知识？在不完美上下文中的推理链特征与探索 

**Authors**: Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.12632)  

**Abstract**: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and explore LLMs' preferences from their effectiveness, faithfulness and robustness, as well as CoE's usability in a naive Retrieval-Augmented Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

将外部知识纳入大型语言模型（LLMs）已成为缓解LLMs过时知识和幻觉问题的一个有前途的方法。然而，外部知识往往是不完美的。除了有用的知识外，外部知识在上下文中的无关或错误信息也很丰富，这会影响LLMs响应的可靠性。本文关注在处理多跳问答时LLMs对不完美上下文中的外部知识的偏好。借鉴刑事诉讼法中的证据链（Chain of Evidence, CoE），我们发现，LLMs偏好的外部知识应保持与问题的相关性以及知识片段之间的相互支持。据此，我们提出了一种自动化的CoE鉴别方法，并从有效性、忠实度和鲁棒性，以及在朴素检索增强生成（RAG）情景中的实用性角度探讨了LLMs的偏好。通过对五种LLMs的评估发现，证据链增强了LLMs，通过更准确的生成、更强的回答忠实度、更好的知识冲突鲁棒性以及在流行RAG情景中的更好表现。 

---
# LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning 

**Title (ZH)**: LLMCL-GEC: 基于LLM驱动的课程学习的语法错误纠正方法进步 

**Authors**: Tao Fang, Derek F. Wong, Lusheng Zhang, Keyan Jin, Qiang Zhang, Tianjiao Li, Jinlong Hou, Lidia S. Chao  

**Link**: [PDF](https://arxiv.org/pdf/2412.12541)  

**Abstract**: While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC). Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums. Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies. 

**Abstract (ZH)**: 尽管大规模语言模型（LLMs）在特定的自然语言处理（NLP）任务中展示了卓越的能力，但在某些领域，如语法错误纠正（GEC）方面，它们可能仍然不如专门化模型熟练。受到课程学习概念的启发，我们致力于通过有效的课程学习（CL）策略将LLMs培养成精通GEC的专家。在本文中，我们介绍了一种名为基于LLM的课程学习的新方法，该方法利用LLMs固有的稳健语义理解和区分能力来衡量GEC训练数据的复杂性。与传统的课程学习技术不同，我们的方法更接近于人类专家设计的课程。利用所提出的基于LLM的CL方法，我们依次选择从简单到复杂的不同难度级别的课程，并使用预训练的T5和LLaMA系列模型进行迭代训练和精细化改进。通过在英语GEC不同基准评估中的严格测试和分析，包括CoNLL14测试集、BEA19测试集和BEA19开发集，我们的方法在基础模型和传统课程学习方法上展示了显著的性能提升。 

---
# RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment 

**Title (ZH)**: RareAgents：自主多学科团队在罕见病诊断与治疗中的应用 

**Authors**: Xuanzhong Chen, Ye Jin, Xiaohao Mao, Lun Wang, Shuyang Zhang, Ting Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.12475)  

**Abstract**: Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the huge number of diseases. The complexity of symptoms and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable improvements across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical exams. However, current agent frameworks lack adaptation for real-world clinical scenarios, especially those involving the intricate demands of rare diseases. To address these challenges, we present RareAgents, the first multi-disciplinary team of LLM-based agents tailored to the complex clinical context of rare diseases. RareAgents integrates advanced planning capabilities, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents surpasses state-of-the-art domain-specific models, GPT-4o, and existing agent frameworks in both differential diagnosis and medication recommendation for rare diseases. Furthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from MIMIC-IV, to support further advancements in this field. 

**Abstract (ZH)**: 稀有疾病尽管单个疾病的发病率较低，但由于疾病种类繁多，全球约有3亿人受到这些疾病的集体影响。症状复杂性和缺乏相关经验的专业医生使得诊断和治疗稀有疾病比常见疾病更加具有挑战性。最近，由大规模语言模型（LLMs）驱动的代理已经在多个领域展示出了显著的进步。在医疗领域，一些代理方法在医学考试中的问答任务中已经超越了直接提示。然而，当前的代理框架缺乏针对真实临床场景的适应性，特别是那些涉及稀有疾病复杂需求的场景。为了解决这些挑战，我们提出了RareAgents，这是第一个针对稀有疾病复杂临床环境的多学科团队，基于LLM的代理。RareAgents集成了先进的规划能力、记忆机制和医疗工具的应用，以LLama-3.1-8B/70B作为基础模型。实验结果表明，RareAgents在稀有疾病的鉴别诊断和药物推荐方面优于现有的领域特定模型GPT-4o和现有的代理框架。此外，我们贡献了一个新的数据集MIMIC-IV-Ext-Rare，该数据集源自MIMIC-IV，以支持该领域进一步的发展。 

---
# LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework 

**Title (ZH)**: LITA：一种高效的基于大语言模型的迭代主题增强框架 

**Authors**: Chia-Hsuan Chang, Jui-Tse Tsai, Yi-Hang Tsai, San-Yih Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12459)  

**Abstract**: Topic modeling is widely used for uncovering thematic structures within text corpora, yet traditional models often struggle with specificity and coherence in domain-focused applications. Guided approaches, such as SeededLDA and CorEx, incorporate user-provided seed words to improve relevance but remain labor-intensive and static. Large language models (LLMs) offer potential for dynamic topic refinement and discovery, yet their application often incurs high API costs. To address these challenges, we propose the LLM-assisted Iterative Topic Augmentation framework (LITA), an LLM-assisted approach that integrates user-provided seeds with embedding-based clustering and iterative refinement. LITA identifies a small number of ambiguous documents and employs an LLM to reassign them to existing or new topics, minimizing API costs while enhancing topic quality. Experiments on two datasets across topic quality and clustering performance metrics demonstrate that LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an efficient and adaptable framework for advancing topic modeling and text clustering. 

**Abstract (ZH)**: 主题建模广泛应用于揭示文本语料库中的主题结构，但传统模型在领域导向应用中往往面临具体性和连贯性不足的问题。通过种子词引导的方法，如SeededLDA和CorEx，能够提升相关性，但这些方法仍然劳动强度高且缺乏灵活性。大型语言模型（LLMs）有可能实现动态主题优化和发现，然而其应用往往会带来高昂的API使用成本。为了解决这些问题，我们提出了一种大型语言模型辅助的迭代主题增强框架（LITA），这是一种结合了用户提供的种子词、基于嵌入的聚类和迭代优化的LLM辅助方法。LITA识别出少量模糊的主题文档，并利用LLM重新分配这些文档至现有或新的话题，从而在降低API使用成本的同时提高主题质量。实验结果表明，LITA在主题质量和聚类性能指标上均优于包括LDA、SeededLDA、CorEx、BERTopic和PromptTopic在内的五个基线模型。我们的工作提供了一种有效且适应性强的主题建模和文本聚类框架。 

---
# Bridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments 

**Title (ZH)**: 填补空白：通过新的基准测试、微调和文化调整，提升低资源非洲语言的大型语言模型性能 

**Authors**: Tuka Alhanai, Adam Kasumovic, Mohammad Ghassemi, Aven Zitzelberger, Jessica Lundin, Guillaume Chabot-Couture  

**Link**: [PDF](https://arxiv.org/pdf/2412.12417)  

**Abstract**: Large Language Models (LLMs) have shown remarkable performance across various tasks, yet significant disparities remain for non-English languages, and especially native African languages. This paper addresses these disparities by creating approximately 1 million human-translated words of new benchmark data in 8 low-resource African languages, covering a population of over 160 million speakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho (Southern Sotho), Setswana, and Tsonga. Our benchmarks are translations of Winogrande and three sections of MMLU: college medicine, clinical knowledge, and virology. Using the translated benchmarks, we report previously unknown performance gaps between state-of-the-art (SOTA) LLMs in English and African languages. Finally, using results from over 400 fine-tuned models, we explore several methods to reduce the LLM performance gap, including high-quality dataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and cultural appropriateness adjustments. Key findings include average mono-lingual improvements of 5.6% with fine-tuning (with 5.4% average mono-lingual improvements when using high-quality data over low-quality data), 2.9% average gains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost on culturally appropriate questions. The publicly available benchmarks, translations, and code from this study support further research and development aimed at creating more inclusive and effective language technologies. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种任务中表现出色，但在非英语语言，尤其是本土非洲语言方面仍然存在显著差距。本文通过在8种低资源非洲语言（阿姆哈拉语、班巴拉语、伊博语、塞佩迪语（北部索托语）、悚纳语、塞萨索托语（南部索托语）、;setswana语和特桑加语）上创建约100万词的人工翻译基准数据，来解决这些差距问题。这些语言共有超过1.6亿说这些语言的人口，基准数据包括Winogrande和MMLU中的三个部分：医学院校、临床知识和病毒学。利用翻译后的基准数据，我们报告了最新最佳表现（SOTA）LLMs在英语和非洲语言之间的未知性能差距。最后，通过超过400个微调模型的结果，我们探讨了减少LLM性能差距的几种方法，包括高质量数据集微调（使用LLM作为注释器）、跨语言迁移和文化适宜性调整。主要发现包括单语微调平均提高了5.6%的成绩（使用高质量数据相比低质量数据时的平均单语微调提高了5.4%），平均2.9%的跨语言迁移收益，以及在文化适宜性问题上3.0%的原生性能提升。本研究公开提供的基准数据、翻译内容和代码支持进一步的研究和发展，旨在创造更为包容且有效的语言技术。 

---
# LogBabylon: A Unified Framework for Cross-Log File Integration and Analysis 

**Title (ZH)**: LogBabylon：一个统一的日志文件集成与分析框架 

**Authors**: Rabimba Karanjai, Yang Lu, Dana Alsagheer, Keshav Kasichainula, Lei Xu, Weidong Shi, Shou-Hsuan Stephen Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12364)  

**Abstract**: Logs are critical resources that record events, activities, or messages produced by software applications, operating systems, servers, and network devices. However, consolidating the heterogeneous logs and cross-referencing them is challenging and complicated. Manually analyzing the log data is time-consuming and prone to errors. LogBabylon is a centralized log data consolidating solution that leverages Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the log data in a human-readable way and adds insight analysis of the system performance and anomaly alerts. It provides a paramount view of the system landscape, enabling proactive management and rapid incident response. LogBabylon consolidates diverse log sources and enhances the extracted information's accuracy and relevancy. This facilitates a deeper understanding of log data, supporting more effective decision-making and operational efficiency. Furthermore, LogBabylon streamlines the log analysis process, significantly reducing the time and effort required to interpret complex datasets. Its capabilities extend to generating context-aware insights, offering an invaluable tool for continuous monitoring, performance optimization, and security assurance in dynamic computing environments. 

**Abstract (ZH)**: 日志是记录由软件应用、操作系统、服务器和网络设备产生的事件、活动或消息的关键资源。然而，整合异构日志并进行跨参考是具有挑战性和复杂性的任务。手动分析日志数据既耗时又容易出错。LogBabylon 是一种利用大型语言模型（LLMs）结合检索增强生成（RAG）技术的集中化日志数据整合解决方案。LogBabylon 以人类可读的方式解释日志数据，并添加有关系统性能和异常警报的洞察分析。它提供了系统架构的整体视图，有助于前瞻性管理并实现快速故障响应。LogBabylon 整合了多种日志源，并增强了提取信息的准确性和相关性，这有助于更深入地理解日志数据，支持更为有效的决策和运营效率。此外，LogBabylon 简化了日志分析过程，显著减少了解读复杂数据集所需的时间和努力。其功能扩展到生成上下文感知的洞察，提供了一个宝贵的工具，用于动态计算环境中的持续监控、性能优化和安全保证。 

---
# Are Large Language Models Useful for Time Series Data Analysis? 

**Title (ZH)**: 大型语言模型在时间序列数据分析中是否有用？ 

**Authors**: Francis Tang, Ying Ding  

**Link**: [PDF](https://arxiv.org/pdf/2412.12219)  

**Abstract**: Time series data plays a critical role across diverse domains such as healthcare, energy, and finance, where tasks like classification, anomaly detection, and forecasting are essential for informed decision-making. Recently, large language models (LLMs) have gained prominence for their ability to handle complex data and extract meaningful insights. This study investigates whether LLMs are effective for time series data analysis by comparing their performance with non-LLM-based approaches across three tasks: classification, anomaly detection, and forecasting.
Through a series of experiments using GPT4TS and autoregressive models, we evaluate their performance on benchmark datasets and assess their accuracy, precision, and ability to generalize. Our findings indicate that while LLM-based methods excel in specific tasks like anomaly detection, their benefits are less pronounced in others, such as forecasting, where simpler models sometimes perform comparably or better. This research highlights the role of LLMs in time series analysis and lays the groundwork for future studies to systematically explore their applications and limitations in handling temporal data. 

**Abstract (ZH)**: 时间序列数据在医疗、能源和金融等多个领域发挥着关键作用，其中分类、异常检测和预测等任务对于支持明智的决策至关重要。近年来，大型语言模型（LLMs）因其处理复杂数据和提取有意义洞察的能力而备受瞩目。本研究旨在探讨LLMs在时间序列数据分析中的有效性，通过将基于LLMs的方法与其非LLMs基线方法在分类、异常检测和预测三个任务上的表现进行比较来验证这一点。

通过使用GPT4TS和自回归模型进行一系列实验，我们评估了这些方法在基准数据集上的性能，并对其准确度、精确度以及泛化能力进行了评估。研究结果表明，虽然基于LLMs的方法在特定任务如异常检测方面表现出色，但在其他任务如预测方面，其优势并不总是明显，有时简单的模型在某些情况下甚至表现得相当好或更好。本研究强调了LLMs在时间序列分析中的作用，并为未来系统探索其在处理时序数据方面应用和局限性奠定了基础。 

---
# Embracing Large Language Models in Traffic Flow Forecasting 

**Title (ZH)**: 拥抱大型语言模型在交通流预测中的应用 

**Authors**: Yusheng Zhao, Xiao Luo, Haomin Wen, Zhiping Xiao, Wei Ju, Ming Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12201)  

**Abstract**: Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF. 

**Abstract (ZH)**: 交通流量预测旨在基于历史交通条件和道路网络来预测未来的交通流量，这是智能交通系统中的一项重要问题，已有大量方法被提出。现有努力主要集中在捕捉和利用空间-时间依赖性以预测未来的交通流量。尽管这些方法非常有前景，但它们在适应测试时交通条件的环境变化方面存在不足。为应对这一挑战，我们提出将大型语言模型（LLMs）引入交通流量预测，并设计了一种新的方法，名为大型语言模型增强的交通流量预测器（LEAF）。LEAF采用两个分支，分别使用图形和超图形结构来捕捉不同的空间-时间关系。在测试阶段，这两个分支首先独立预训练，然后分别产生不同的预测。基于这些预测，使用大型语言模型选择最有可能的结果。然后，应用排名损失作为学习目标，以增强两个分支的预测能力。在多种数据集上的广泛实验验证了所提出的LEAF的有效性。 

---
# Activation Sparsity Opportunities for Compressing General Large Language Models 

**Title (ZH)**: 激活稀疏性 opportunities for 压缩通用大型语言模型 

**Authors**: Nobel Dhar, Bobin Deng, Md Romyull Islam, Kazi Fahim Ahmad Nasif, Liang Zhao, Kun Suo  

**Link**: [PDF](https://arxiv.org/pdf/2412.12178)  

**Abstract**: Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices' independent capabilities, alleviate the server's burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity method is orthogonal and combinable with existing techniques to maximize compression rate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN) components, which typically comprise a large proportion of parameters (around 3/2), ensure that our FFN optimizations would have a better chance of achieving effective compression. Moreover, our findings are beneficial to general LLMs and are not restricted to ReLU-based models. This work systematically investigates the tradeoff between enforcing activation sparsity and perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that we can obtain around 50% of main memory and computing reductions for critical FFN components with negligible accuracy degradation. This extra 50% sparsity does not naturally exist in the current LLMs, which require tuning LLMs' activation outputs by injecting zero-enforcing thresholds. To obtain the benefits of activation sparsity, we provide a guideline for the system architect for LLM prediction and prefetching. The success prediction allows the system to prefetch the necessary weights while omitting the inactive ones and their successors, therefore lowering cache and memory pollution and reducing LLM execution time on resource-constrained edge devices. 

**Abstract (ZH)**: 将边缘设备上的本地AI模型，如大型语言模型（LLMs），部署到边缘设备可以显著增强设备的独立功能，减轻服务器的负担并降低响应时间。由于这种巨大的潜力，许多大型科技公司已经发布了几种轻量级的小型语言模型（SLMs）来填补这一空白。然而，我们仍然有很大的动力将更强大的AI模型（LLMs）部署到边缘设备并提高它们的智能化水平。与传统的AI模型压缩方法不同，我们研究了激活稀疏性。激活稀疏性方法是独立且可与现有技术相结合的，可以在保持高准确率的同时最大化压缩率。LLMs中的前向传播网络（FFN）组件通常占据了很大比例的参数（约3/2），这确保了我们的FFN优化有更大的机会实现有效的压缩。此外，我们的研究结果对一般的LLMs具有普遍性，并不限于基于ReLU的模型。本文系统地研究了在最新一代LLMs中强制执行激活稀疏性与困惑度（准确率）之间的权衡。我们实证分析表明，我们可以在几乎不降低准确率的情况下，实现关键FFN组件约50%的主内存和计算量的减少。这种额外的50%稀疏性目前并不自然地存在于现有的LLMs中，因此需要通过注入零强制阈值来调整LLMs的激活输出。为了利用激活稀疏性的益处，我们为LLM预测和预取提供了系统架构的指南。成功预测允许系统预取必要的权重，同时忽略不活跃的权重及其后续项，从而减少缓存和内存污染，降低资源受限的边缘设备上LLM的执行时间。 

---
# Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning 

**Title (ZH)**: 探索心智理论：程序引导的对抗性数据生成用于心智理论推理 

**Authors**: Melanie Sclar, Jane Yu, Maryam Fazel-Zarandi, Yulia Tsvetkov, Yonatan Bisk, Yejin Choi, Asli Celikyilmaz  

**Link**: [PDF](https://arxiv.org/pdf/2412.12175)  

**Abstract**: Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce ExploreToM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks. 

**Abstract (ZH)**: 大语言模型（LLMs）具有心智理论能力吗？大量论文和基准测试已经提出，用于评估当前模型是否能够发展这种关键的社会智能能力。然而，这些评估方法均依赖于有限数据集和简单的模式，可能导致评价中的潜在盲点和对模型能力的高估。我们引入了ExploreToM，这是第一个允许生成多样且具有挑战性的理论认知数据的框架，用于稳健的训练和评估。我们的方法通过在自定义领域特定语言中使用A*搜索来生成复杂的故事情节和新颖、多样但又可信的情景，以最大限度地测试大语言模型的能力极限。评估结果显示，现有最先进的LLM模型（如Llama-3.1-70B和GPT-4o）在ExploreToM生成的数据上的准确率低至0%和9%，突显了更稳健理论认知评估的需求。由于我们生成的数据覆盖了先前工作的概念超集，基于我们数据的微调在经典的ToMi基准测试（Le et al., 2019）上提高了27个百分点的准确率。此外，ExploreToM还能够揭示模型展示心智理论所需的关键技能和缺失因素，例如不可靠的状态跟踪或数据失衡，这可能是模型在基准测试中表现不佳的原因之一。 

---
# A NotSo Simple Way to Beat Simple Bench 

**Title (ZH)**: 《超越简单基准的一种不那么简单的途径》 

**Authors**: Soham Sane, Angus McLean  

**Link**: [PDF](https://arxiv.org/pdf/2412.12173)  

**Abstract**: This paper presents a novel framework for enhancing reasoning capabilities in large language models (LLMs) by leveraging iterative reasoning and feedback-driven methodologies. Building on the limitations identified in the SimpleBench benchmark, a dataset designed to evaluate logical coherence and real-world reasoning, we propose a multi-step prompting strategy coupled with global consistency checks to improve model accuracy and robustness. Through comparative analysis of state-of-the-art models, including Claude 3 Opus, Claude 3.5, GPT- 4o, and o1-preview, we demonstrate that iterative reasoning significantly enhances model performance, with improvements observed in both standard accuracy metrics (AVG@5) and a newly introduced metric, Extreme Averaging (EAG@5). Our results reveal model-specific strengths: Claude excels in maintaining logical consistency, while GPT-4o exhibits exploratory creativity but struggles with ambiguous prompts. By analyzing case studies and identifying gaps in spatial and temporal reasoning, we highlight areas for further refinement. The findings underscore the potential of structured reasoning frameworks to address inherent model limitations, irrespective of pretraining methodologies. This study lays the groundwork for integrating dynamic feedback mechanisms, adaptive restart strategies, and diverse evaluation metrics to advance LLM reasoning capabilities across complex and multi-domain problem spaces. 

**Abstract (ZH)**: 本文提出了一种新颖的框架，通过利用迭代推理和反馈驱动的方法来增强大型语言模型（LLMs）的推理能力。基于在 SimpleBench 基准中发现的局限性，这是一种用于评估逻辑连贯性和现实世界推理的基准数据集，本文提出了一种多步提示策略并结合全局一致性检查，以提高模型的准确性和鲁棒性。通过比较领先模型（包括 Claude 3 Opus、Claude 3.5、GPT-4o 和 o1-preview），我们证明迭代推理能够显著提升模型性能，不仅在标准准确度指标（AVG@5）中有所改善，还在新引入的度量指标（Extreme Averaging, EAG@5）中也观察到了改进。我们的结果揭示了模型的特定优势：Claude 在保持逻辑一致性方面表现突出，而 GPT-4o 则表现出探索性的创造性，但在模糊提示方面存在挑战。通过对案例研究的分析并识别空间和时间推理的不足之处，我们进一步指出了需要进一步完善的领域。研究结果强调了结构化推理框架在克服模型固有问题方面的潜力，无论预训练方法如何。这项研究奠定了集成动态反馈机制、自适应重启策略和多样化评估指标的基础，以促进LLMs在复杂和多领域问题空间中的推理能力。 

---
# PickLLM: Context-Aware RL-Assisted Large Language Model Routing 

**Title (ZH)**: PickLLM：基于上下文的强化学习辅助大型语言模型路由 

**Authors**: Dimitrios Sikeridis, Dennis Ramdass, Pranay Pareek  

**Link**: [PDF](https://arxiv.org/pdf/2412.12170)  

**Abstract**: Recently, the number of off-the-shelf Large Language Models (LLMs) has exploded with many open-source options. This creates a diverse landscape regarding both serving options (e.g., inference on local hardware vs remote LLM APIs) and model heterogeneous expertise. However, it is hard for the user to efficiently optimize considering operational cost (pricing structures, expensive LLMs-as-a-service for large querying volumes), efficiency, or even per-case specific measures such as response accuracy, bias, or toxicity. Also, existing LLM routing solutions focus mainly on cost reduction, with response accuracy optimizations relying on non-generalizable supervised training, and ensemble approaches necessitating output computation for every considered LLM candidate. In this work, we tackle the challenge of selecting the optimal LLM from a model pool for specific queries with customizable objectives. We propose PickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to route on-the-fly queries to available models. We introduce a weighted reward function that considers per-query cost, inference latency, and model response accuracy by a customizable scoring function. Regarding the learning algorithms, we explore two alternatives: PickLLM router acting as a learning automaton that utilizes gradient ascent to select a specific LLM, or utilizing stateless Q-learning to explore the set of LLMs and perform selection with a $\epsilon$-greedy approach. The algorithm converges to a single LLM for the remaining session queries. To evaluate, we utilize a pool of four LLMs and benchmark prompt-response datasets with different contexts. A separate scoring function is assessing response accuracy during the experiment. We demonstrate the speed of convergence for different learning rates and improvement in hard metrics such as cost per querying session and overall response latency. 

**Abstract (ZH)**: 近年来，现成的大语言模型（LLMs）的数量激增，开源选择也越来越多。这在服务方式（例如，本地硬件推理 vs 远程LLM API推理）和模型异质性专业知识方面创造了多样化的景象。然而，用户在考虑运营成本（定价结构，大规模查询量的昂贵的LLM即服务）、效率，甚至是针对单一案例的具体措施，如响应准确性、偏见或有害内容方面进行优化时，仍然面临挑战。现有的一些LLM路由解决方案主要侧重于成本削减，响应准确性优化依赖于非通用化的监督训练，而集成方法则需要对每个考虑中的LLM候选模型进行输出计算。在这项工作中，我们致力于解决从模型池中为特定查询选择最佳LLM的问题，并实现自定义目标。我们提出了PickLLM，这是一种轻量级框架，利用强化学习（RL）在运行时将查询路由到可用模型。我们引入了一个加权奖励函数，该函数通过可定制的评分函数考虑每个查询的成本、推理延迟和模型响应准确性。在学习算法方面，我们探索了两种替代方案：PickLLM路由器作为一个学习automaton，使用梯度上升来选择特定的LLM，或者利用无状态的Q学习来探索LLM集合，并使用$\epsilon$-贪心策略进行选择。该算法在后续查询中收敛于单一的LLM。为了进行评估，我们使用了四种LLM的模型池，并对具有不同上下文的提示-响应数据集进行了基准测试。在实验期间，我们使用了一个独立的评分函数来评估响应准确性。我们展示了不同学习率下收敛速度的不同，并在成本和总体响应延迟等硬指标方面证明了性能改进。 

---
# Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off 

**Title (ZH)**: 具有可解释性的语言模型的监管很可能会导致性能权衡 

**Authors**: Eoin M. Kenny, Julie A. Shah  

**Link**: [PDF](https://arxiv.org/pdf/2412.12169)  

**Abstract**: Regulation is increasingly cited as the most important and pressing concern in machine learning. However, it is currently unknown how to implement this, and perhaps more importantly, how it would effect model performance alongside human collaboration if actually realized. In this paper, we attempt to answer these questions by building a regulatable large-language model (LLM), and then quantifying how the additional constraints involved affect (1) model performance, alongside (2) human collaboration. Our empirical results reveal that it is possible to force an LLM to use human-defined features in a transparent way, but a "regulation performance trade-off" previously not considered reveals itself in the form of a 7.34% classification performance drop. Surprisingly however, we show that despite this, such systems actually improve human task performance speed and appropriate confidence in a realistic deployment setting compared to no AI assistance, thus paving a way for fair, regulatable AI, which benefits users. 

**Abstract (ZH)**: 监管越来越被视作机器学习中最重要和紧迫的关注点。然而，目前尚不清楚如何实施这一监管，而且更为重要的是，如果实际实施，这种监管将如何影响模型性能及其与人类合作的关系。本文试图通过构建一个可调节的大规模语言模型 (LLM)，并量化额外约束条件的影响，来回答这些问题。具体来说，我们发现有可能以透明的方式强制 LLM 使用人类定义的特征。然而，一个先前未被考虑的“监管性能权衡”显现出来，表现为分类性能下降了 7.34%。然而，令人惊讶的是，我们在实际部署环境中发现，此类系统实际上能够提高人类任务执行速度和适宜的信心，与没有人工智能辅助的情况相比。这为公平且可调节的人工智能铺平了道路，从而造福用户。 

---
# Performance of a large language model-Artificial Intelligence based chatbot for counseling patients with sexually transmitted infections and genital diseases 

**Title (ZH)**: 基于人工智能的聊天机器人在咨询性传播感染和生殖器疾病患者方面的表现研究 

**Authors**: Nikhil Mehta, Sithira Ambepitiya, Thanveer Ahamad, Dinuka Wijesundara, Yudara Kularathne  

**Link**: [PDF](https://arxiv.org/pdf/2412.12166)  

**Abstract**: Introduction: Global burden of sexually transmitted infections (STIs) is rising out of proportion to specialists. Current chatbots like ChatGPT are not tailored for handling STI-related concerns out of the box. We developed Otiz, an Artificial Intelligence-based (AI-based) chatbot platform designed specifically for STI detection and counseling, and assessed its performance. Methods: Otiz employs a multi-agent system architecture based on GPT4-0613, leveraging large language model (LLM) and Deterministic Finite Automaton principles to provide contextually relevant, medically accurate, and empathetic responses. Its components include modules for general STI information, emotional recognition, Acute Stress Disorder detection, and psychotherapy. A question suggestion agent operates in parallel. Four STIs (anogenital warts, herpes, syphilis, urethritis/cervicitis) and 2 non-STIs (candidiasis, penile cancer) were evaluated using prompts mimicking patient language. Each prompt was independently graded by two venereologists conversing with Otiz as patient actors on 6 criteria using Numerical Rating Scale ranging from 0 (poor) to 5 (excellent). Results: Twenty-three venereologists did 60 evaluations of 30 prompts. Across STIs, Otiz scored highly on diagnostic accuracy (4.1-4.7), overall accuracy (4.3-4.6), correctness of information (5.0), comprehensibility (4.2-4.4), and empathy (4.5-4.8). However, relevance scores were lower (2.9-3.6), suggesting some redundancy. Diagnostic scores for non-STIs were lower (p=0.038). Inter-observer agreement was strong, with differences greater than 1 point occurring in only 12.7% of paired evaluations. Conclusions: AI conversational agents like Otiz can provide accurate, correct, discrete, non-judgmental, readily accessible and easily understandable STI-related information in an empathetic manner, and can alleviate the burden on healthcare systems. 

**Abstract (ZH)**: 介绍：性传播感染（STI）的全球负担正在不成比例地增加，而现有的聊天机器人（如ChatGPT）并未针对处理STI相关问题进行定制。我们开发了Otiz，一种基于人工智能的（AI）聊天机器人平台，专为STI检测和咨询而设计，并对其性能进行了评估。

方法：Otiz采用了基于GPT4-0613的多智能体系统架构，结合大型语言模型（LLM）和确定性有限自动机（DFA）原理，提供上下文相关、医学准确且富有同理心的响应。其模块包括一般性STI信息、情绪识别、急性应激障碍检测和心理治疗等。一个问题建议代理在同一时间运行。利用模拟患者语言的提示，对生殖器疣、梅毒、淋病/宫颈炎、念珠菌感染和阴茎癌这4种STI和2种非STI（念珠菌感染、阴茎癌）进行了评估。每次提示由两位性病专家分别以患者身份与Otiz进行交互，根据6个标准（使用0-5的数值评分等级，其中0表示“差”，5表示“优秀”）进行了独立评分。

结果：共有23位性病专家对30个提示进行了60次评估。在评估的STI中，Otiz在诊断准确性（4.1-4.7）、综合准确性（4.3-4.6）、信息准确性（5.0）、易理解性（4.2-4.4）以及同理心（4.5-4.8）等方面得分较高。然而，相关性评分较低（2.9-3.6），表明存在一定冗余。非STI的诊断评分较低（p=0.038）。观察者间的一致性很强，仅12.7%的配对评估差异大于1分。

结论：像Otiz这样的AI对话代理可以以同理心的方式提供准确、正确、简洁、非评判性的、易于获取且易于理解的STI相关信息，并有助于减轻医疗保健系统的负担。 

---
# Towards LLM-based optimization compilers. Can LLMs learn how to apply a single peephole optimization? Reasoning is all LLMs need! 

**Title (ZH)**: 基于LLM的优化编译器研究：LLM能够学会应用单个穿孔优化吗？推理是它们所需的一切！ 

**Authors**: Xiangxin Fang, Lev Mukhanov  

**Link**: [PDF](https://arxiv.org/pdf/2412.12163)  

**Abstract**: Large Language Models (LLMs) have demonstrated great potential in various language processing tasks, and recent studies have explored their application in compiler optimizations. However, all these studies focus on the conventional open-source LLMs, such as Llama2, which lack enhanced reasoning mechanisms. In this study, we investigate the errors produced by the fine-tuned 7B-parameter Llama2 model as it attempts to learn and apply a simple peephole optimization for the AArch64 assembly code. We provide an analysis of the errors produced by the LLM and compare it with state-of-the-art OpenAI models which implement advanced reasoning logic, including GPT-4o and GPT-o1 (preview). We demonstrate that OpenAI GPT-o1, despite not being fine-tuned, outperforms the fine-tuned Llama2 and GPT-4o. Our findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-o1. We hope our work will inspire further research on using LLMs with enhanced reasoning mechanisms and chain-of-thought for code generation and optimization. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种语言处理任务中展现出了巨大的潜力， recent的研究已经探索了它们在编译器优化中的应用。然而，这些研究主要集中在传统的开源LLM，如Llama2，而这些LLM缺乏增强的推理机制。在本研究中，我们分析了微调后的7亿参数Llama2模型在尝试学习并应用一种简单的孔窥优化（peephole optimization）于AArch64汇编代码时产生的错误。我们对LLM产生的错误进行了分析，并将其与实现高级推理逻辑的最新OpenAI模型进行了比较，包括GPT-4o和GPT-o1（演示版）。结果显示，尽管GPT-o1没有进行微调，它仍然优于微调后的Llama2和GPT-4o。我们的研究发现这种优势主要是由于GPT-o1中实现的链式思维推理。我们希望本研究能够激发更多关于使用增强推理机制和链式思维进行代码生成和优化的研究。 

---
# What Makes In-context Learning Effective for Mathematical Reasoning: A Theoretical Analysis 

**Title (ZH)**: 数学推理中上下文学习有效性的理论分析 

**Authors**: Jiayu Liu, Zhenya Huang, Chaokun Wang, Xunpeng Huang, Chengxiang Zhai, Enhong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.12157)  

**Abstract**: Owing to the capability of in-context learning, large language models (LLMs) have shown impressive performance across diverse mathematical reasoning benchmarks. However, we find that few-shot demonstrations can sometimes bring negative performance and their effectiveness on LLMs' reasoning abilities remains unreliable. To this end, in this paper, we aim to theoretically analyze the impact of in-context demonstrations on LLMs' reasoning performance. We prove that the reasoning efficacy (measured by empirical prediction loss) can be bounded by a LLM-oriented semantic similarity and an inference stability of demonstrations, which is general for both one-shot and few-shot scenarios. Based on this finding, we propose a straightforward, generalizable, and low-complexity demonstration selection method named LMS3. It can adaptively facilitate to select the most pertinent samples for different LLMs and includes a novel demonstration rejection mechanism to automatically filter out samples that are unsuitable for few-shot learning. Through experiments on three representative benchmarks, two LLM backbones, and multiple few-shot settings, we verify that our LMS3 has superiority and achieves consistent improvements on all datasets, which existing methods have been unable to accomplish. 

**Abstract (ZH)**: 得益于上下文中的学习能力，大规模语言模型（LLMs）在各种数学推理基准测试中展现了令人 impressive 的性能。然而，我们发现，少量示例（少样本）演示有时会带来负向性能影响，其在LLMs推理能力上的有效性仍然不够可靠。鉴于此，在本文中，我们旨在从理论上分析上下文示例对LLMs推理性能的 影响。我们证明，推理效能（通过经验预测损失衡量）可以被一个LLM导向的语义相似性和示例的推理稳定性所限制，这一结论适用于单样本和少量样本的场景。基于上述发现，我们提出了一种简单、通用且计算复杂度低的示例选择方法，名为LMS3。该方法能够自适应地选择最适合不同LLMs的不同示例，并引入了一种新颖的示例拒绝机制，以自动生成筛选出不适合少样本学习的示例。通过在三个代表性基准测试、两种LLM主干模型和多个少样本设置上的实验验证，我们证明了LMS3具有优越性，并在所有数据集上实现了持续改进，而现有的方法未能实现这一点。 

---
# PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection 

**Title (ZH)**: PyOD 2：一种基于大语言模型辅助模型选择的异常检测Python库 

**Authors**: Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, Yichi Zhang, Yushun Dong, Yue Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.12154)  

**Abstract**: Outlier detection (OD), also known as anomaly detection, is a critical machine learning (ML) task with applications in fraud detection, network intrusion detection, clickstream analysis, recommendation systems, and social network moderation. Among open-source libraries for outlier detection, the Python Outlier Detection (PyOD) library is the most widely adopted, with over 8,500 GitHub stars, 25 million downloads, and diverse industry usage. However, PyOD currently faces three limitations: (1) insufficient coverage of modern deep learning algorithms, (2) fragmented implementations across PyTorch and TensorFlow, and (3) no automated model selection, making it hard for non-experts.
To address these issues, we present PyOD Version 2 (PyOD 2), which integrates 12 state-of-the-art deep learning models into a unified PyTorch framework and introduces a large language model (LLM)-based pipeline for automated OD model selection. These improvements simplify OD workflows, provide access to 45 algorithms, and deliver robust performance on various datasets. In this paper, we demonstrate how PyOD 2 streamlines the deployment and automation of OD models and sets a new standard in both research and industry. PyOD 2 is accessible at [this https URL](this https URL). This study aligns with the Web Mining and Content Analysis track, addressing topics such as the robustness of Web mining methods and the quality of algorithmically-generated Web data. 

**Abstract (ZH)**: 异常检测（OD），也称为离群点检测，是机器学习（ML）中一个关键的任务，广泛应用于欺诈检测、网络安全入侵检测、点击流分析、推荐系统和社交网络管理等领域。在开源异常检测库中，Python 异常检测库（PyOD）是最广泛采用的，拥有超过8500颗GitHub之星、2.5亿次下载量，并且在各行各业都有广泛的应用。然而，PyOD目前面临三个局限性：（1）缺乏对现代深度学习算法的充分覆盖，（2）在PyTorch和TensorFlow中的实现碎片化，以及（3）缺乏自动模型选择功能，这使得非专家难以使用。

为了解决这些问题，我们提出了PyOD Version 2（PyOD 2），它将12种先进的深度学习模型集成到了统一的PyTorch框架中，并引入了基于大型语言模型（LLM）的流程以实现自动OD模型选择。这些改进简化了OD工作流程，提供了45种算法的访问途径，并在多种数据集上实现了可靠的性能。在本文中，我们展示了PyOD 2如何简化和自动化OD模型的部署，并树立了研究和工业领域的标准。PyOD 2的代码库可访问于[该链接](该链接)。本文的研究内容与Web挖掘和内容分析类别相契合，涵盖了Web挖掘方法的稳健性和算法生成的Web数据质量等相关主题。 

---
# GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction 

**Title (ZH)**: GraphTool-指令：通过分解子任务指令革新手动图推理的大型语言模型 

**Authors**: Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, Ke Qin  

**Link**: [PDF](https://arxiv.org/pdf/2412.12152)  

**Abstract**: Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling (e.g., function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks (i.e., graph extraction, tool name identification and tool parameter extraction), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types (e.g., graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）已被证明具有理解图的基本属性和解决各种图推理任务的能力。现有方法通过专门设计的任务指令对LLMs进行微调，以理解和执行图推理任务。然而，这些基于文本指令的方法通常表现出较差的性能。受工具学习的启发，研究人员提出了基于工具指令的方法，通过特定的工具调用（例如，函数、API和模型）来解决各种图问题，从而在图推理任务中取得了显著的改进。然而，当前的基于工具指令的方法主要集中在工具信息上，忽略了图结构信息，导致在小型LLM（少于13B参数）上性能显著较差。为了解决这一问题，我们提出了一种创新的任务调优方法GraphTool-Instruction，该方法将图推理任务分解为三个不同的子任务（即图提取、工具名称识别和工具参数提取），并为每个子任务设计专门的指令。GraphTool-Instruction可以作为插拔即用的提示，适用于不同的LLM，无需进行微调。此外，基于GraphTool-Instruction，我们构建了包含二十个图推理任务的GTools数据集，并基于Llama3-8B构建了一个基于LLM的图推理系统GraphForge。我们在包含不同图类型的二十个图推理任务中进行了广泛实验，发现GraphTool-Instruction在图提取、工具名称识别和工具参数提取任务中的性能均优于基于文本指令和基于工具指令的方法。通过在GTools上进行微调，GraphForge相比于增强后的GPT-3.5-turbo在性能上进一步提升了超过30%，并且在性能上与成本较高的GPT-4相当。我们的代码和数据可在以下链接获取：[请提供具体的网址]。 

---
# Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars 

**Title (ZH)**: 《纳美族人或伪君子：通过隐喻 avatar 突破语言模型》

注：在这里，“纳美族人”是《阿凡达》电影中的一种虚构生物，而“伪君子”则是中文成语，用来指外表良好但内心虚伪的人。这个标题利用了“Na'vi”（纳美族）和“Knave”（伪君子）两个词的双关意义。翻译时保持了原文的创意和双关意味，同时确保了学术规范和可读性。 

**Authors**: Yu Yan, Sheng Sun, Junqi Tong, Min Liu, Qi Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.12145)  

**Abstract**: Metaphor serves as an implicit approach to convey information, while enabling the generalized comprehension of complex subjects. However, metaphor can potentially be exploited to bypass the safety alignment mechanisms of Large Language Models (LLMs), leading to the theft of harmful knowledge. In our study, we introduce a novel attack framework that exploits the imaginative capacity of LLMs to achieve jailbreaking, the J\underline{\textbf{A}}ilbreak \underline{\textbf{V}}ia \underline{\textbf{A}}dversarial Me\underline{\textbf{TA}} -pho\underline{\textbf{R}} (\textit{AVATAR}). Specifically, to elicit the harmful response, AVATAR extracts harmful entities from a given harmful target and maps them to innocuous adversarial entities based on LLM's imagination. Then, according to these metaphors, the harmful target is nested within human-like interaction for jailbreaking adaptively. Experimental results demonstrate that AVATAR can effectively and transferablly jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. Our study exposes a security risk in LLMs from their endogenous imaginative capabilities. Furthermore, the analytical study reveals the vulnerability of LLM to adversarial metaphors and the necessity of developing defense methods against jailbreaking caused by the adversarial metaphor. \textcolor{orange}{ \textbf{Warning: This paper contains potentially harmful content from LLMs.}} 

**Abstract (ZH)**: 比喻作为一种隐性的信息传达方式，能够促进对复杂主题的广泛理解。然而，比喻也可能被利用来绕过大型语言模型（LLMs）的安全对齐机制，从而盗取有害知识。在我们的研究中，我们介绍了一种新的攻击框架，利用LLMs的想象力能力实现“监狱逃脱”，该框架命名为Jailbreak Via Adversarial MeTa-phoR（简称为AVATAR）。具体而言，为了引发有害响应，AVATAR从给定的有害目标中提取有害实体，并基于LLMs的想象力将它们映射到无害的对抗实体。然后，根据这些比喻，有害目标被嵌入到类人的互动中，以适应性地实现“监狱逃脱”。实验结果表明，AVATAR能够有效且可转移地“监狱逃脱”LLMs，并且在多个先进LLMs上实现了最先进的攻击成功率。我们的研究揭示了LLMs源自其内生想象力能力的安全风险。此外，分析研究揭示了LLMs对抗喻性比喻的脆弱性，以及开发针对由对抗性比喻引起的“监狱逃脱”的防御方法的必要性。请谨慎对待本文中可能包含的有害内容。 

---
# Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models 

**Title (ZH)**: 使用大型语言模型自动生成个性情境判断测验项目 

**Authors**: Chang-Jin Li, Jiyuan Zhang, Yun Tang, Jian Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.12144)  

**Abstract**: Personality assessment, particularly through situational judgment tests (SJTs), is a vital tool for psychological research, talent selection, and educational evaluation. This study explores the potential of GPT-4, a state-of-the-art large language model (LLM), to automate the generation of personality situational judgment tests (PSJTs) in Chinese. Traditional SJT development is labor-intensive and prone to biases, while GPT-4 offers a scalable, efficient alternative. Two studies were conducted: Study 1 evaluated the impact of prompt design and temperature settings on content validity, finding that optimized prompts with a temperature of 1.0 produced creative and accurate items. Study 2 assessed the psychometric properties of GPT-4-generated PSJTs, revealing that they demonstrated satisfactory reliability and validity, surpassing the performance of manually developed tests in measuring the Big Five personality traits. This research highlights GPT-4's effectiveness in developing high-quality PSJTs, providing a scalable and innovative method for psychometric test development. These findings expand the possibilities of automatic item generation and the application of LLMs in psychology, and offer practical implications for streamlining test development processes in resource-limited settings. 

**Abstract (ZH)**: 个人特质评估，特别是通过情境判断测试（SJT）进行评估，是心理研究、人才选拔和教育评估中不可或缺的工具。本研究探讨了最新的大型语言模型（LLM）GPT-4在中国语言环境中自动生成个人情境判断测试（PSJT）的潜力。传统的SJT开发过程耗时且容易产生偏见，而GPT-4则提供了一种可扩展且高效的替代方案。本研究共进行了两项研究：研究1评估了提示设计和温度设置对内容效度的影响，发现优化后的提示温度设置为1.0时，生成了具有创新性和准确性的题目。研究2评估了GPT-4生成的PSJT的心理计量特性，结果表明这些测试在测量五大人格特质方面表现出满意的可靠性和有效性，甚至超过了手工开发测试的性能。本研究突显了GPT-4在开发高质量PSJT方面的有效性，提供了一种可扩展且创新的心理测量测试开发方法。这些发现扩展了自动生成测题的可能性，并揭示了LLM在心理学领域的应用前景，为资源受限环境下测试开发过程的简化提供了实际意义。 

---
# SnakModel: Lessons Learned from Training an Open Danish Large Language Model 

**Title (ZH)**: SnakModel：训练开源丹麦大型语言模型的经验教训 

**Authors**: Mike Zhang, Max Müller-Eberstein, Elisa Bassignana, Rob van der Goot  

**Link**: [PDF](https://arxiv.org/pdf/2412.12956)  

**Abstract**: We present SnakModel, a Danish large language model (LLM) based on Llama2-7B, which we continuously pre-train on 13.6B Danish words, and further tune on 3.7M Danish instructions. As best practices for creating LLMs for smaller language communities have yet to be established, we examine the effects of early modeling and training decisions on downstream performance throughout the entire training pipeline, including (1) the creation of a strictly curated corpus of Danish text from diverse sources; (2) the language modeling and instruction-tuning training process itself, including the analysis of intermediate training dynamics, and ablations across different hyperparameters; (3) an evaluation on eight language and culturally-specific tasks. Across these experiments SnakModel achieves the highest overall performance, outperforming multiple contemporary Llama2-7B-based models. By making SnakModel, the majority of our pre-training corpus, and the associated code available under open licenses, we hope to foster further research and development in Danish Natural Language Processing, and establish training guidelines for languages with similar resource constraints. 

**Abstract (ZH)**: 我们提出了SnakModel，这是一个基于Llama2-7B的丹麦大型语言模型（LLM），我们对其进行了连续预训练，使用了136亿个丹麦语词汇，并进一步使用了370万条丹麦指令进行调优。由于为较小语言社区创建LLM的最佳做法尚未建立，我们考察了从建模到训练的各个环节中早起决策对下游性能的影响，包括（1）从各种来源严格筛选并创建丹麦文本语料库；（2）语言模型和指令调优的训练过程本身，包括对中间训练动态的分析以及不同超参数的削减实验；（3）对八项语言和文化特定任务的评估。在这些实验中，SnakModel取得了最优的整体性能，超越了多个基于Llama2-7B的现代模型。通过将SnakModel、大部分预训练语料库及相关的代码以开源许可发布，我们希望促进丹麦自然语言处理的研究与开发，并为资源有限的类似语言的训练制定指导方针。 

---
# Truthful Text Sanitization Guided by Inference Attacks 

**Title (ZH)**: 由推理攻击引导的truthful文本 sanitization 

**Authors**: Ildikó Pilán, Benet Manzanares-Salor, David Sánchez, Pierre Lison  

**Link**: [PDF](https://arxiv.org/pdf/2412.12928)  

**Abstract**: The purpose of text sanitization is to rewrite those text spans in a document that may directly or indirectly identify an individual, to ensure they no longer disclose personal information. Text sanitization must strike a balance between preventing the leakage of personal information (privacy protection) while also retaining as much of the document's original content as possible (utility preservation). We present an automated text sanitization strategy based on generalizations, which are more abstract (but still informative) terms that subsume the semantic content of the original text spans. The approach relies on instruction-tuned large language models (LLMs) and is divided into two stages. The LLM is first applied to obtain truth-preserving replacement candidates and rank them according to their abstraction level. Those candidates are then evaluated for their ability to protect privacy by conducting inference attacks with the LLM. Finally, the system selects the most informative replacement shown to be resistant to those attacks. As a consequence of this two-stage process, the chosen replacements effectively balance utility and privacy. We also present novel metrics to automatically evaluate these two aspects without the need to manually annotate data. Empirical results on the Text Anonymization Benchmark show that the proposed approach leads to enhanced utility, with only a marginal increase in the risk of re-identifying protected individuals compared to fully suppressing the original information. Furthermore, the selected replacements are shown to be more truth-preserving and abstractive than previous methods. 

**Abstract (ZH)**: 文本净化的目的是重写文档中可能直接或间接识别个体的文本片段，以确保不再泄露个人隐私信息。文本净化需要在防止个人信息泄露（隐私保护）与保留文档尽可能多的原始内容（实用保留）之间找到平衡。本文提出了一种基于泛化的自动文本净化策略，其中泛化是指更具抽象性但仍具有信息性的术语，可以概括原始文本片段的语义内容。该方法依赖于指令调优的大语言模型（LLMs），并分为两个阶段。首先应用LLM获取事实保留的替代候选，并根据其抽象层次进行排名。然后利用LLM进行推理攻击来评估这些候选替代保护隐私的能力。最后，系统选择能够抵抗这些攻击且最具有信息性替代。这一两阶段过程的结果是选择的替代能够有效平衡实用性和隐私性。此外，本文还提出了新的评估指标，可以自动评估这两个方面，而无需手动标注数据。在文本匿名化基准测试集上的实证结果显示，所提出的方法在与完全抑制原始信息相比，仅增加了微小的重新识别受保护个体的风险的同时，实现了更高的实用性。此外，所选替代在事实保持性和抽象性方面优于先前的方法。 

---
# Question: How do Large Language Models perform on the Question Answering tasks? Answer: 

**Title (ZH)**: 问题：大规模语言模型在问答任务中的表现如何？
回答： 

**Authors**: Kevin Fischer, Darren Fürst, Sebastian Steindl, Jakob Lindner, Ulrich Schäfer  

**Link**: [PDF](https://arxiv.org/pdf/2412.12893)  

**Abstract**: Large Language Models (LLMs) have been showing promising results for various NLP-tasks without the explicit need to be trained for these tasks by using few-shot or zero-shot prompting techniques. A common NLP-task is question-answering (QA). In this study, we propose a comprehensive performance comparison between smaller fine-tuned models and out-of-the-box instruction-following LLMs on the Stanford Question Answering Dataset 2.0 (SQuAD2), specifically when using a single-inference prompting technique. Since the dataset contains unanswerable questions, previous work used a double inference method. We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources. Furthermore, we investigate their generalization capabilities by comparing their performance on similar but different QA datasets, without fine-tuning neither model, emulating real-world uses where the context and questions asked may differ from the original training distribution, for example swapping Wikipedia for news articles.
Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets. 

**Abstract (ZH)**: 大型语言模型（LLMs）在不需要明确针对特定自然语言处理（NLP）任务进行训练的情况下，通过少样本或零样本提示技术，已经显示出在各种NLP任务中的有前途的结果。常见的NLP任务之一是问答（QA）。在本研究中，我们提出了一种在斯坦福问答数据集2.0（SQuAD2）上对较小的微调模型和现成的指令跟随LLMs进行全面性能比较的方法，特别是在使用单一推理提示技术的情况下。由于数据集中包含无法回答的问题，以往的工作采用了双推理方法。我们提出了一种提示风格，旨在在不需要双推理的情况下激发相同的能力，从而节省计算时间和资源。此外，我们通过在不进行微调的情况下比较它们在类似但不同的QA数据集上的性能，研究了它们的泛化能力，模拟了实际使用场景，其中上下文和提出的问题可能与原始训练分布不同，例如用维基百科文章替换新闻文章。

我们的结果显示，在微调任务上，较小的微调模型优于当前最先进的（SOTA）LLMs，但最近的SOTA模型能够在分布外测试中缩小这一差距，并在所测试的5个QA数据集中有3个数据集上甚至超过了微调模型。 

---
# Benchmarking and Understanding Compositional Relational Reasoning of LLMs 

**Title (ZH)**: 《基准测试与理解LLMs的组合关系推理能力》 

**Authors**: Ruikang Ni, Da Xiao, Qingye Meng, Xiangyu Li, Shihui Zheng, Hongliang Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12841)  

**Abstract**: Compositional relational reasoning (CRR) is a hallmark of human intelligence, but we lack a clear understanding of whether and how existing transformer large language models (LLMs) can solve CRR tasks. To enable systematic exploration of the CRR capability of LLMs, we first propose a new synthetic benchmark called Generalized Associative Recall (GAR) by integrating and generalizing the essence of several tasks in mechanistic interpretability (MI) study in a unified framework. Evaluation shows that GAR is challenging enough for existing LLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy enough for systematic MI study. Then, to understand how LLMs solve GAR tasks, we use attribution patching to discover the core circuits reused by Vicuna-33B across different tasks and a set of vital attention heads. Intervention experiments show that the correct functioning of these heads significantly impacts task performance. Especially, we identify two classes of heads whose activations represent the abstract notion of true and false in GAR tasks respectively. They play a fundamental role in CRR across various models and tasks. The dataset and code are available at this https URL. 

**Abstract (ZH)**: 组合关系推理（CRR）是人类智能的一个 hallmark，但我们尚未明确了解现有的大规模语言模型（LLMs）是否能够解决 CRR 任务，以及它们是如何解决这些任务的。为了系统地探索 LLM 的 CRR 能力，我们首先提出了一种新的合成基准，名为通用关联回忆（GAR），该基准通过对机械可解释性（MI）研究中多个任务的精髓进行整合和泛化，在统一框架下构建。评估结果显示，GAR 对现有的 LLM 具有足够的挑战性，揭示了它们在 CRR 方面的基本不足。同时，这个基准足够简单，适用于系统的 MI 研究。

为了理解 LLM 如何解决 GAR 任务，我们使用归因补丁技术发现 Vicuna-33B 在不同任务中重用的核心电路以及一系列关键的关注头。干预实验表明，这些头的正确功能显著影响任务性能。特别地，我们识别出两类头，它们在 GAR 任务中分别代表真值和假值的概念，它们在各种模型和任务中的 CRR 中发挥着基础性作用。相关数据集和代码可在以下网址获取：[提供网址的文本]。 

---
# Enhancing Naturalness in LLM-Generated Utterances through Disfluency Insertion 

**Title (ZH)**: 通过插入间歇性错误以增强LLM生成语句的自然度 

**Authors**: Syed Zohaib Hassan, Pierre Lison, Pål Halvorsen  

**Link**: [PDF](https://arxiv.org/pdf/2412.12710)  

**Abstract**: Disfluencies are a natural feature of spontaneous human speech but are typically absent from the outputs of Large Language Models (LLMs). This absence can diminish the perceived naturalness of synthesized speech, which is an important criteria when building conversational agents that aim to mimick human behaviours. We show how the insertion of disfluencies can alleviate this shortcoming. The proposed approach involves (1) fine-tuning an LLM with Low-Rank Adaptation (LoRA) to incorporate various types of disfluencies into LLM-generated utterances and (2) synthesizing those utterances using a text-to-speech model that supports the generation of speech phenomena such as disfluencies. We evaluated the quality of the generated speech across two metrics: intelligibility and perceived spontaneity. We demonstrate through a user study that the insertion of disfluencies significantly increase the perceived spontaneity of the generated speech. This increase came, however, along with a slight reduction in intelligibility. 

**Abstract (ZH)**: 不流畅现象是自发人类口语的自然特征，但通常在大型语言模型（LLMs）的输出中是不存在的。这种缺失会降低合成语音的自然感，这是在构建模仿人类行为的对话代理时一个重要的评判标准。本文展示了如何通过插入不流畅现象来改善这一缺憾。所提出的方法包括（1）使用低秩适应（LoRA）对LLM进行微调，以使其生成的语句中包含多种类型的不流畅现象；以及（2）使用支持生成诸如不流畅现象等语音现象的文本到语音模型来合成这些语句。我们在两个指标上评估了生成语音的质量：可懂度和感知自然度。通过用户研究，我们证明了插入不流畅现象显著增加了生成语音的感知自然度。然而，这种增加是以略微降低可懂度为代价的。 

---
# Trigger$^3$: Refining Query Correction via Adaptive Model Selector 

**Title (ZH)**: Trigger³：基于自适应模型选择的查询纠正 refinement 

**Authors**: Kepu Zhang, Zhongxiang Sun, Xiao Zhang, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.12701)  

**Abstract**: In search scenarios, user experience can be hindered by erroneous queries due to typos, voice errors, or knowledge gaps. Therefore, query correction is crucial for search engines. Current correction models, usually small models trained on specific data, often struggle with queries beyond their training scope or those requiring contextual understanding. While the advent of Large Language Models (LLMs) offers a potential solution, they are still limited by their pre-training data and inference cost, particularly for complex queries, making them not always effective for query correction. To tackle these, we propose Trigger$^3$, a large-small model collaboration framework that integrates the traditional correction model and LLM for query correction, capable of adaptively choosing the appropriate correction method based on the query and the correction results from the traditional correction model and LLM. Trigger$^3$ first employs a correction trigger to filter out correct queries. Incorrect queries are then corrected by the traditional correction model. If this fails, an LLM trigger is activated to call the LLM for correction. Finally, for queries that no model can correct, a fallback trigger decides to return the original query. Extensive experiments demonstrate Trigger$^3$ outperforms correction baselines while maintaining efficiency. 

**Abstract (ZH)**: 在搜索场景中，由于打字错误、语音错误或知识差距等原因，错误的查询可能阻碍用户的体验。因此，查询纠错对于搜索引擎至关重要。现有的纠错模型，通常是在特定数据上训练的小模型，往往难以处理超出其训练范围的查询，或者需要上下文理解的查询。虽然大型语言模型（LLMs）的出现提供了潜在的解决方案，但它们仍然受限于预训练数据和推理成本，特别是在处理复杂查询时，这使得它们并非总是有效的纠错工具。为了解决这些问题，我们提出了一种名为Trigger$^3$的大型与小型模型合作框架，该框架将传统的纠错模型与LLM集成用于查询纠错，可以根据查询和传统纠错模型及LLM的纠错结果，动态选择合适的纠错方法。Trigger$^3$首先使用纠错触发器过滤出正确的查询。对于错误的查询，先由传统的纠错模型进行修正。如果失败，则激活LLM触发器，调用LLM进行修正。最后，对于没有任何模型能够纠错的查询，回退触发器决定返回原始查询。广泛的实验表明，Trigger$^3$在保持效率的同时，其性能优于现有的纠错基线。 

---
# iPrOp: Interactive Prompt Optimization for Large Language Models with a Human in the Loop 

**Title (ZH)**: iPrOp：具有人为干预的大语言模型交互式提示优化 

**Authors**: Jiahui Li, Roman Klinger  

**Link**: [PDF](https://arxiv.org/pdf/2412.12644)  

**Abstract**: Prompt engineering has made significant contributions to the era of large language models, yet its effectiveness depends on the skills of a prompt author. Automatic prompt optimization can support the prompt development process, but requires annotated data. This paper introduces $\textit{iPrOp}$, a novel Interactive Prompt Optimization system, to bridge manual prompt engineering and automatic prompt optimization. With human intervention in the optimization loop, $\textit{iPrOp}$ offers users the flexibility to assess evolving prompts. We present users with prompt variations, selected instances, large language model predictions accompanied by corresponding explanations, and performance metrics derived from a subset of the training data. This approach empowers users to choose and further refine the provided prompts based on their individual preferences and needs. This system not only assists non-technical domain experts in generating optimal prompts tailored to their specific tasks or domains, but also enables to study the intrinsic parameters that influence the performance of prompt optimization. Our evaluation shows that our system has the capability to generate improved prompts, leading to enhanced task performance. 

**Abstract (ZH)**: 提示工程在大语言模型时代发挥了显著的贡献，但其有效性取决于提示撰写者的技能。自动提示优化可以支持提示开发过程，但需要标注数据。本文引入了$\textit{iPrOp}$，一种新颖的交互式提示优化系统，旨在将手工提示工程与自动提示优化相结合。通过在优化循环中的人工介入，$\textit{iPrOp}$为用户提供了一种评估不断演变提示的灵活性。我们向用户提供不同版本的提示、选定的实例、大型语言模型的预测及其相应的解释，以及从部分训练数据中得到的性能指标。通过这种方式，用户可以根据个人偏好和需求选择并进一步细化提供的提示。该系统不仅有助于非技术领域的专家生成针对特定任务或领域的最优提示，还可以研究影响提示优化性能的基本参数。我们的评估表明，该系统能够生成改进的提示，从而提高任务性能。 

---
# LLM-based Discriminative Reasoning for Knowledge Graph Question Answering 

**Title (ZH)**: 基于LLM的区分性推理在知识图谱问答中的应用 

**Authors**: Mufan Xu, Kehai Chen, Xuefeng Bai, Muyun Yang, Tiejun Zhao, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12643)  

**Abstract**: Large language models (LLMs) based on generative pre-trained Transformer have achieved remarkable performance on knowledge graph question-answering (KGQA) tasks. However, LLMs often produce ungrounded subgraph planning or reasoning results in KGQA due to the hallucinatory behavior brought by the generative paradigm, which may hinder the advancement of the LLM-based KGQA model. To deal with the issue, we propose a novel LLM-based Discriminative Reasoning (LDR) method to explicitly model the subgraph retrieval and answer inference process. By adopting discriminative strategies, the proposed LDR method not only enhances the capability of LLMs to retrieve question-related subgraphs but also alleviates the issue of ungrounded reasoning brought by the generative paradigm of LLMs. Experimental results show that the proposed approach outperforms multiple strong comparison methods, along with achieving state-of-the-art performance on two widely used WebQSP and CWQ benchmarks. 

**Abstract (ZH)**: 基于生成预训练变换器的大型语言模型（LLMs）在知识图谱问答（KGQA）任务中取得了显著的性能。然而，由于生成范式带来的幻觉行为，LLMs 经常产生与知识图谱不相关的子图规划或推理结果，这可能阻碍基于LLM的KGQA模型的发展。为了解决这一问题，我们提出了一种新颖的基于LLM的辨别推理（LDR）方法，该方法明确建模子图检索和答案推理过程。通过采用辨别策略，所提出的LDR方法不仅增强了LLM检索与问题相关子图的能力，还缓解了LLM生成范式带来的无关推理问题。实验结果表明，所提出的方法在两个广泛使用的WebQSP和CWQ基准上优于多个强对照方法，并实现了最先进的性能。 

---
# Jailbreaking? One Step Is Enough! 

**Title (ZH)**: 越狱？一步足矣！ 

**Authors**: Weixiong Zheng, Peijian Zeng, Yiwei Li, Hongyan Wu, Nankai Lin, Junhao Chen, Aimin Yang, Yongmei Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.12621)  

**Abstract**: Large language models (LLMs) excel in various tasks but remain vulnerable to jailbreak attacks, where adversaries manipulate prompts to generate harmful outputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs. However, current jailbreak methods and the target model's defenses are engaged in an independent and adversarial process, resulting in the need for frequent attack iterations and redesigning attacks for different models. To address these gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that disguises the attack intention as the "defense". intention against harmful content. Specifically, REDA starts from the target response, guiding the model to embed harmful content within its defensive measures, thereby relegating harmful content to a secondary role and making the model believe it is performing a defensive task. The attacking model considers that it is guiding the target model to deal with harmful content, while the target model thinks it is performing a defensive task, creating an illusion of cooperation between the two. Additionally, to enhance the model's confidence and guidance in "defensive" intentions, we adopt in-context learning (ICL) with a small number of attack examples and construct a corresponding dataset of attack examples. Extensive evaluations demonstrate that the REDA method enables cross-model attacks without the need to redesign attack strategies for different models, enables successful jailbreak in one iteration, and outperforms existing methods on both open-source and closed-source models. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种任务中表现出色，但在对抗破解攻击方面仍显脆弱，即攻击者通过操纵提示生成有害输出。检查对抗破解提示有助于揭示LLMs的不足之处。然而，当前的对抗破解方法和目标模型的防御措施是在独立且对抗的过程中进行的，这导致了需要频繁的攻击迭代和针对不同模型重新设计攻击策略。为了弥补这些差距，我们提出了一种逆向嵌入防御攻击（REDA）机制，它将攻击意图伪装成“防御”意图。具体而言，REDA从目标响应开始，引导模型在其防御措施中嵌入有害内容，从而将有害内容置于次要地位，并使模型认为它正在进行防御任务。攻击模型认为它正在指导目标模型处理有害内容，而目标模型则认为它正在执行防御任务，从而在两者之间创造出一种合作的假象。此外，为了增强模型在“防御”意图上的置信度和引导作用，我们采用了少量攻击样本的上下文学习（ICL）方法，并相应地构建了一个攻击样本数据集。广泛的评估表明，REDA方法能够在不针对不同模型重新设计攻击策略的情况下实现跨模型攻击，在一次迭代中成功实现对抗破解，并且在开源和闭源模型上优于现有方法。 

---
# SynthCypher: A Fully Synthetic Data Generation Framework for Text-to-Cypher Querying in Knowledge Graphs 

**Title (ZH)**: SynthCypher: 知识图中基于文本到密文查询的完全合成数据生成框架 

**Authors**: Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan  

**Link**: [PDF](https://arxiv.org/pdf/2412.12612)  

**Abstract**: Cypher, the query language for Neo4j graph databases, plays a critical role in enabling graph-based analytics and data exploration. While substantial research has been dedicated to natural language to SQL query generation (Text2SQL), the analogous problem for graph databases referred to as Text2Cypher remains underexplored. In this work, we introduce SynthCypher, a fully synthetic and automated data generation pipeline designed to address this gap. SynthCypher employs a novel LLMSupervised Generation-Verification framework, ensuring syntactically and semantically correct Cypher queries across diverse domains and query complexities. Using this pipeline, we create SynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher instances. Fine-tuning open-source large language models (LLMs), including LLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant performance improvements of up to 40% on the Text2Cypher test set and 30% on the SPIDER benchmark adapted for graph databases. This work demonstrates that high-quality synthetic data can effectively advance the state-of-the-art in Text2Cypher tasks. 

**Abstract (ZH)**: Cypher 是 Neo4j 图数据库的查询语言，在图分析和数据探索方面发挥着关键作用。虽然大量研究集中在自然语言到 SQL 查询生成 (Text2SQL) 领域，而对于图数据库中的类似问题，即 Text2Cypher，研究仍然相对不足。在本文中，我们介绍了一种名为 SynthCypher 的完整合成自动化数据生成管道，旨在解决这一问题。SynthCypher 采用了新颖的 LLMSupervised Generation-Verification 框架，确保在不同领域和查询复杂度中生成语法规正确且语义正确的 Cypher 查询。利用该管道，我们创建了包含 29,800 个 Text2Cypher 实例的大规模基准数据集 SynthCypher Dataset。对开源大规模语言模型 (LLMs)，包括 LLaMa-3.1-8B、Mistral-7B 和 QWEN-7B，进行微调后，在 Text2Cypher 测试集上取得了高达 40% 的性能提升，在适应图数据库的 SPIDER 基准上取得了 30% 的性能提升。这项工作证明，高质量的合成数据可以有效地推动 Text2Cypher 任务的技术前沿。 

---
# MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program Fine-tuning 

**Title (ZH)**: MultiLingPoT：通过多语言程序微调增强数学推理 

**Authors**: Nianqi Li, Zujie Liang, Siyu Yuan, Jiaqing Liang, Feng Wei, Yanghua Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2412.12609)  

**Abstract**: Program-of-Thought (PoT), which aims to use programming language instead of natural language as an intermediate step in reasoning, is an important way for LLMs to solve mathematical problems. Since different programming languages excel in different areas, it is natural to use the most suitable language for solving specific problems. However, current PoT research only focuses on single language PoT, ignoring the differences between different programming languages. Therefore, this paper proposes an multilingual program reasoning method, MultiLingPoT. This method allows the model to answer questions using multiple programming languages by fine-tuning on multilingual data. Additionally, prior and posterior hybrid methods are used to help the model select the most suitable language for each problem. Our experimental results show that the training of MultiLingPoT improves each program's mathematical reasoning by about 2.5\%. Moreover, with proper mixing, the performance of MultiLingPoT can be further improved, achieving a 6\% increase compared to the single-language PoT with the data this http URL of this paper can be found at this https URL. 

**Abstract (ZH)**: 程序思维（PoT）旨在使用编程语言而非自然语言作为推理的中间步骤，是大语言模型解决数学问题的重要方式。由于不同的编程语言在不同领域表现出色，因此使用最适合的语言来解决特定问题是自然的选择。然而，目前的PoT研究仅专注于单语言PoT，忽视了不同编程语言之间的差异。因此，本文提出了一种多语言程序推理方法——MultiLingPoT。该方法通过在多语言数据上进行微调，使模型能够使用多种编程语言来回答问题。此外，本研究采用了先验和后验混合方法，帮助模型为每个问题选择最合适的语言。实验结果表明，MultiLingPoT的训练可使每种程序的数学推理能力提高约2.5%。通过适当的混合使用，MultiLingPoT的性能还可以进一步提高，相较于使用单一语言PoT的数据集，其性能提高了约6%。本文的实验结果可访问该链接：[本文的链接] 

---
# LLMs are Also Effective Embedding Models: An In-depth Overview 

**Title (ZH)**: 大型语言模型也是有效的嵌入模型：深入综述 

**Authors**: Chongyang Tao, Tao Shen, Shen Gao, Junshuo Zhang, Zhen Li, Zhengwei Tao, Shuai Ma  

**Link**: [PDF](https://arxiv.org/pdf/2412.12591)  

**Abstract**: Large language models (LLMs) have revolutionized natural language processing by achieving state-of-the-art performance across various tasks. Recently, their effectiveness as embedding models has gained attention, marking a paradigm shift from traditional encoder-only models like ELMo and BERT to decoder-only, large-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an in-depth overview of this transition, beginning with foundational techniques before the LLM era, followed by LLM-based embedding models through two main strategies to derive embeddings from LLMs. 1) Direct prompting: We mainly discuss the prompt designs and the underlying rationale for deriving competitive embeddings. 2) Data-centric tuning: We cover extensive aspects that affect tuning an embedding model, including model architecture, training objectives, data constructions, etc. Upon the above, we also cover advanced methods, such as handling longer texts, and multilingual and cross-modal data. Furthermore, we discuss factors affecting choices of embedding models, such as performance/efficiency comparisons, dense vs sparse embeddings, pooling strategies, and scaling law. Lastly, the survey highlights the limitations and challenges in adapting LLMs for embeddings, including cross-task embedding quality, trade-offs between efficiency and accuracy, low-resource, long-context, data bias, robustness, etc. This survey serves as a valuable resource for researchers and practitioners by synthesizing current advancements, highlighting key challenges, and offering a comprehensive framework for future work aimed at enhancing the effectiveness and efficiency of LLMs as embedding models. 

**Abstract (ZH)**: 大规模语言模型（LLMs）通过在各种任务中实现最先进的性能，已经革新了自然语言处理。最近，它们作为嵌入模型的有效性引起了广泛关注，标志着从传统的仅编码器模型（如ELMo和BERT）到依赖于解码器的大规模LLMs（如GPT、LLaMA和Mistral）的范式转变。本文综述了这一转变，从LLM时代之前的奠基技术开始，随后介绍了基于LLM的嵌入模型，通过两种主要策略从LLM中推导嵌入：1）直接提示：我们主要讨论提示设计及其推导竞争嵌入的内在逻辑。2）数据为中心的调优：我们涵盖了影响嵌入模型调优的广泛方面，包括模型架构、训练目标、数据构建等。在此基础上，我们还涵盖了高级方法，例如处理长文本、多语言和跨模态数据。此外，我们讨论了影响嵌入模型选择的因素，包括性能/效率比较、密集嵌入与稀疏嵌入、聚合策略以及扩展定律。最后，本文综述了将LLM适应为嵌入模型时的局限性和挑战，包括跨任务嵌入质量、效率与准确性的权衡、低资源、长上下文、数据偏差、鲁棒性等方面。本文综述为研究人员和从业者提供了一个宝贵资源，综合了当前的最新进展，突出了关键挑战，并提供了一个全面的框架，旨在未来的工作中增强LLM作为嵌入模型的有效性和效率。 

---
# Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models 

**Title (ZH)**: 使用大型语言模型评估零-shot 多语言方面基于情感分析 

**Authors**: Chengyan Wu, Bolei Ma, Zheyu Zhang, Ningyuan Deng, Yanqing He, Yun Xue  

**Link**: [PDF](https://arxiv.org/pdf/2412.12564)  

**Abstract**: Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts. While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation. We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models. Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models. Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English. These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages. 

**Abstract (ZH)**: 基于aspect的 sentiment分析（ABSA），作为一种序列标注任务，在多语言背景下引起了越来越多的关注。尽管先前的研究主要集中在对ABSA进行微调或训练特定的模型上，但我们在此研究中在零样本条件下评估大型语言模型（LLMs），探讨它们在最少的任务特定适应下解决这一挑战的潜在能力。我们在九种不同模型上对一系列LLMs进行了全面的经验性评估，调查了包括纯零样本、思考链（CoT）、自我改进、自我辩论和自我一致性在内的各种提示策略。结果显示，虽然LLMs在处理多语言ABSA方面具有潜力，但它们通常无法与特定任务的微调模型相媲美。值得注意的是，在资源丰富语言（如英语）中，简单的零样本提示往往优于更复杂的方法。这些发现强调了进一步完善基于LLM的方法以有效解决跨多种语言的ABSA任务的重要性。 

---
# Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge 

**Title (ZH)**: 你能否信任大语言模型的判断？大语言模型作为法官的可靠性探究 

**Authors**: Kayla Schroeder, Zach Wood-Doughty  

**Link**: [PDF](https://arxiv.org/pdf/2412.12509)  

**Abstract**: Large Language Models (LLMs) have become increasingly powerful and ubiquitous, but their stochastic nature poses challenges to the reliability of their outputs. While deterministic settings can improve consistency, they do not guarantee reliability, as a single sample from the model's probability distribution can still be misleading. Building upon the concept of LLM-as-a-judge, we introduce a novel framework for rigorously evaluating the reliability of LLM judgments, leveraging McDonald's omega. We evaluate the reliability of LLMs when judging the outputs of other LLMs on standard single-turn and multi-turn benchmarks, simultaneously investigating the impact of temperature on reliability. By analyzing these results, we demonstrate the limitations of fixed randomness and the importance of considering multiple samples, which we show has significant implications for downstream applications. Our findings highlight the need for a nuanced understanding of LLM reliability and the potential risks associated with over-reliance on single-shot evaluations. This work provides a crucial step towards building more trustworthy and reliable LLM-based systems and applications. 

**Abstract (ZH)**: 大型语言模型（LLMs）越来越强大且普及，但它们的随机性给输出的可靠性带来了挑战。虽然确定性设置可以提高一致性，但无法确保可靠性，因为模型概率分布中的单一样本仍然可能误导性。基于LLM作为裁判的概念，我们提出了一种新的框架来严格评估LLM判决的可靠性，并利用麦当劳的ω系数进行评估。我们对LLM在评估其他LLM输出的标准单轮和多轮基准上的可靠性进行了评估，同时探讨了温度对可靠性的潜在影响。通过对这些结果的分析，我们展示了固定随机性限制和考虑多个样本的重要性，这些发现对于下游应用程序具有重要影响。我们的研究结果突显了对LLM可靠性需要精微理解的必要性以及过度依赖单次评估可能带来的潜在风险。这项工作为构建更值得信赖的LLM基础系统和应用程序提供了关键步骤。 

---
# LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for Low-Resource Language Tasks 

**Title (ZH)**: LinguaLIFT：一种有效的两阶段指令调优框架，用于低资源语言任务 

**Authors**: Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12499)  

**Abstract**: Large language models (LLMs) have demonstrated impressive multilingual understanding and reasoning capabilities, driven by extensive pre-training multilingual corpora and fine-tuning instruction data. However, a performance gap persists between high-resource and low-resource language tasks due to language imbalance in the pre-training corpus, even using more low-resource data during fine-tuning. To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language tasks. An additional language alignment layer is first integrated into the LLM to adapt a pre-trained multilingual encoder, thereby enhancing multilingual alignment through code-switched fine-tuning. The second stage fine-tunes LLM with English-only instruction data while freezing the language alignment layer, allowing LLM to transfer task-specific capabilities from English to low-resource language tasks. Additionally, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages, enabling comprehensive evaluation of multilingual reasoning. Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and other widely used benchmarks. 

**Abstract (ZH)**: 大规模语言模型（LLMs）已经在多语言理解和推理方面展示了令人印象深刻的性能，这得益于大规模预训练多语言语料库和细调指令数据。然而，即使在使用更多低资源数据进行细调的情况下，高资源语言任务和低资源语言任务之间仍存在性能差距，这主要是由于预训练语料库中语言资源的不平衡。为了解决这一问题，我们提出了一种两阶段指令调优框架LinguaLIFT，来促进低资源语言任务的发展。首先，将在LLM中集成一个额外的语言对齐层，以适应预训练的多语言编码器，从而通过代码转换的细调增强多语言对齐。第二阶段使用仅英语的指令数据对LLM进行细调，同时冻结语言对齐层，使LLM能够将特定任务能力从英语转移至低资源语言任务。此外，我们还引入了多语言数学世界问题（MMWP）基准测试，涵盖了21种低资源、17种中资源和10种高资源语言，从而可以全面评估多语言推理能力。实验结果表明，LinguaLIFT在MMWP以及其他广泛使用的基准测试中均优于几个有竞争力的基线。 

---
# NLSR: Neuron-Level Safety Realignment of Large Language Models Against Harmful Fine-Tuning 

**Title (ZH)**: NLSR：针对有害微调的大语言模型神经元级安全性重对齐 

**Authors**: Xin Yi, Shunfan Zheng, Linlin Wang, Gerard de Melo, Xiaoling Wang, Liang He  

**Link**: [PDF](https://arxiv.org/pdf/2412.12497)  

**Abstract**: The emergence of finetuning-as-a-service has revealed a new vulnerability in large language models (LLMs). A mere handful of malicious data uploaded by users can subtly manipulate the finetuning process, resulting in an alignment-broken model. Existing methods to counteract fine-tuning attacks typically require substantial computational resources. Even with parameter-efficient techniques like LoRA, gradient updates remain essential. To address these challenges, we propose \textbf{N}euron-\textbf{L}evel \textbf{S}afety \textbf{R}ealignment (\textbf{NLSR}), a training-free framework that restores the safety of LLMs based on the similarity difference of safety-critical neurons before and after fine-tuning. The core of our framework is first to construct a safety reference model from an initially aligned model to amplify safety-related features in neurons. We then utilize this reference model to identify safety-critical neurons, which we prepare as patches. Finally, we selectively restore only those neurons that exhibit significant similarity differences by transplanting these prepared patches, thereby minimally altering the fine-tuned model. Extensive experiments demonstrate significant safety enhancements in fine-tuned models across multiple downstream tasks, while greatly maintaining task-level accuracy. Our findings suggest regions of some safety-critical neurons show noticeable differences after fine-tuning, which can be effectively corrected by transplanting neurons from the reference model without requiring additional training. The code will be available at \url{this https URL} 

**Abstract (ZH)**: 以下是对该论文内容或标题的学术规范翻译：

深度调优即服务平台的出现揭示了大规模语言模型（LLMs）中的一种新漏洞。用户上传的一小部分恶意数据可以微妙地影响调优过程，导致模型的对齐失效。现有的对抗调优攻击方法通常需要大量的计算资源。即使是参数高效的技术如LoRA，梯度更新仍然是必不可少的。为了解决这些挑战，我们提出了一种训练免费的框架，名为**N**euron-\textbf{L}evel \textbf{S}afety \textbf{R}ealignment (\textbf{NLSR})，该框架基于调优前后关键神经元的相似性差异来恢复LLMs的安全性。我们框架的核心是首先从初始对齐的模型构建一个安全参考模型，以增强与安全性相关的特征。随后，利用该参考模型识别关键安全性神经元，将其作为补丁预先准备。最后，仅通过移植这些准备好的补丁恢复那些表现出显著相似性差异的神经元，从而最小限度地改变调优后的模型。广泛的实验证明，该框架在多个下游任务中显著提高了调优模型的安全性，同时在任务级别准确度上保持了较大的一致性。我们的研究发现，一些关键安全性神经元在调优后表现出明显的差异，这些差异可以通过移植参考模型中的神经元补丁得以有效纠正，而无需额外的训练。代码将在 [此处提供链接] 供公众查阅。 

---
# Core Context Aware Attention for Long Context Language Modeling 

**Title (ZH)**: 长上下文语言建模中的核心上下文感知注意力机制 

**Authors**: Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan  

**Link**: [PDF](https://arxiv.org/pdf/2412.12465)  

**Abstract**: Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute the attention score. However, when the context length L becomes very large (e.g., 32K), more redundant context information will be included w.r.t. any tokens, making the self-attention suffer from two main limitations: 1) The computational and memory complexity scales quadratically w.r.t. L; 2) The presence of redundant context information may hamper the model to capture dependencies among crucial tokens, which may degrade the representation performance. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-range context modeling, which consists of two components: 1) Globality-pooling attention that divides input tokens into groups and then dynamically merges tokens within each group into one core token based on their significance; 2) Locality-preserved attention that incorporates neighboring tokens into the attention calculation. The two complementary attentions will then be fused to the final attention, maintaining comprehensive modeling ability as the full self-attention. In this way, the core context information w.r.t. a given token will be automatically focused and strengthened, while the context information in redundant groups will be diminished during the learning process. As a result, the computational and memory complexity will be significantly reduced. More importantly, the CCA-Attention can improve the long-context modeling ability by diminishing the redundant context information. Extensive experimental results demonstrate that our CCA-Attention significantly outperforms state-of-the-art models in terms of computational efficiency and long-context modeling ability. 

**Abstract (ZH)**: 基于Transformer的大语言模型（LLMs）在各类自然语言处理任务中表现出显著的成功，这主要归功于其自我注意力机制，该机制要求每个标记考虑所有先前的标记作为其上下文来计算注意力分数。然而，当上下文长度L变得非常大（例如，32K）时，对于任何标记而言，将包括更多的冗余上下文信息，这使得自我注意力面临两大主要限制：1）计算和内存复杂度与L呈二次关系增加；2）冗余上下文信息的出现可能妨碍模型捕捉关键标记之间的联系，从而降低表示性能。为了解决这些问题，本文提出了一种可插拔的核心上下文感知（CCA）注意力机制，用于高效建模长距离上下文。该机制由两个组件组成：1）全局聚集注意力，将输入标记分为组，然后基于它们的重要性动态合并每个组内的标记为一个核心标记；2）局部保留注意力，将相邻标记纳入注意力计算。这两种互补的注意力机制随后将融合到最终的注意力机制中，保持与完整自我注意力相同的广泛建模能力。通过这种方式，给定标记的核心上下文信息将在学习过程中自动被关注并加强，而冗余组中的上下文信息则会被减弱。结果，计算和内存复杂度显著降低。更重要的是，CCA注意力机制能够通过减少冗余上下文信息来提高长距离上下文建模能力。广泛实验结果表明，与最先进的模型相比，我们的CCA注意力机制在计算效率和长距离上下文建模能力方面表现出显著的优势。 

---
# Assessing the Limitations of Large Language Models in Clinical Fact Decomposition 

**Title (ZH)**: 评估大型语言模型在临床事实分解方面的局限性 

**Authors**: Monica Munnangi, Akshay Swaminathan, Jason Alan Fries, Jenelle Jindal, Sanjana Narayanan, Ivan Lopez, Lucia Tu, Philip Chung, Jesutofunmi A. Omiye, Mehr Kashyap, Nigam Shah  

**Link**: [PDF](https://arxiv.org/pdf/2412.12422)  

**Abstract**: Verifying factual claims is critical for using large language models (LLMs) in healthcare. Recent work has proposed fact decomposition, which uses LLMs to rewrite source text into concise sentences conveying a single piece of information, as an approach for fine-grained fact verification. Clinical documentation poses unique challenges for fact decomposition due to dense terminology and diverse note types. To explore these challenges, we present FactEHR, a dataset consisting of full document fact decompositions for 2,168 clinical notes spanning four types from three hospital systems. Our evaluation, including review by clinicians, highlights significant variability in the quality of fact decomposition for four commonly used LLMs, with some LLMs generating 2.6x more facts per sentence than others. The results underscore the need for better LLM capabilities to support factual verification in clinical text. To facilitate future research in this direction, we plan to release our code at \url{this https URL}. 

**Abstract (ZH)**: 在医疗保健中使用大规模语言模型（LLMs）时，验证事实性声明至关重要。近期有研究表明，通过使用LLMs将源文本重写为简洁的句子，传达单一信息的方法——即事实分解，可以作为一种细粒度的事实验证手段。临床文档由于术语密集且笔记类型多样，为事实分解带来了独特的挑战。为了探索这些挑战，我们提出了一个名为FactEHR的数据集，该数据集包含来自三个医疗机构的2,168份临床笔记的完整文档分解，涵盖了四种类型的笔记。我们的评估包括临床医生的评审，突显了四种常用LLMs在事实分解质量上存在显著差异，有些LLMs生成的信息量是其他LLMs的2.6倍。这些结果强调了支持临床文本事实验证所需更好LLMs的能力。为了促进该领域未来的研究，我们计划在 \url{this https URL} 发布我们的代码。 

---
# Interpretable LLM-based Table Question Answering 

**Title (ZH)**: 可解释的基于大规模语言模型的表格问答方法 

**Authors**: Giang, Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue  

**Link**: [PDF](https://arxiv.org/pdf/2412.12386)  

**Abstract**: Interpretability for Table Question Answering (Table QA) is critical, particularly in high-stakes industries like finance or healthcare. Although recent approaches using Large Language Models (LLMs) have significantly improved Table QA performance, their explanations for how the answers are generated are ambiguous. To fill this gap, we introduce Plan-of-SQLs ( or POS), an interpretable, effective, and efficient approach to Table QA that answers an input query solely with SQL executions. Through qualitative and quantitative evaluations with human and LLM judges, we show that POS is most preferred among explanation methods, helps human users understand model decision boundaries, and facilitates model success and error identification. Furthermore, when evaluated in standard benchmarks (TabFact, WikiTQ, and FetaQA), POS achieves competitive or superior accuracy compared to existing methods, while maintaining greater efficiency by requiring significantly fewer LLM calls and database queries. 

**Abstract (ZH)**: 表格问答（Table QA）的可解释性对于高风险行业（如金融或医疗保健）尤为重要。尽管近期使用大规模语言模型（LLMs）的方法在提高Table QA性能方面取得了显著进展，但它们对于如何生成答案的解释仍然模糊不清。为了解决这一问题，我们提出了一种名为Plan-of-SQLs（或POS）的方法，该方法通过SQL执行来独立回答输入查询，从而提高Table QA的可解释性、有效性和效率。通过使用人类和LLM评判者的定性和定量评估，我们展示了POS方法在解释方法中得到了最高认可，有助于人类用户理解模型的决策边界，并促进了模型成功和错误识别。此外，在标准基准（TabFact、WikiTQ和FetaQA）上评估时，POS在准确性和效率方面均与现有方法相当或更优，同时仅需显著减少LLM调用和数据库查询次数。 

---
# Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning 

**Title (ZH)**: 基于分布意识鲁棒学习的大型语言模型相关性建模增强 

**Authors**: Hong Liu, Saisai Gong, Yixin Ji, Kaixin Wu, Jia Xu, Jinjie Gu  

**Link**: [PDF](https://arxiv.org/pdf/2412.12504)  

**Abstract**: With the rapid advancement of pre-trained large language models (LLMs), recent endeavors have leveraged the capabilities of LLMs in relevance modeling, resulting in enhanced performance. This is usually done through the process of fine-tuning LLMs on specifically annotated datasets to determine the relevance between queries and items. However, there are two limitations when LLMs are naively employed for relevance modeling through fine-tuning and inference. First, it is not inherently efficient for performing nuanced tasks beyond simple yes or no answers, such as assessing search relevance. It may therefore tend to be overconfident and struggle to distinguish fine-grained degrees of relevance (e.g., strong relevance, weak relevance, irrelevance) used in search engines. Second, it exhibits significant performance degradation when confronted with data distribution shift in real-world scenarios. In this paper, we propose a novel Distribution-Aware Robust Learning framework (DaRL) for relevance modeling in Alipay Search. Specifically, we design an effective loss function to enhance the discriminability of LLM-based relevance modeling across various fine-grained degrees of query-item relevance. To improve the generalizability of LLM-based relevance modeling, we first propose the Distribution-Aware Sample Augmentation (DASA) module. This module utilizes out-of-distribution (OOD) detection techniques to actively select appropriate samples that are not well covered by the original training set for model fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to simultaneously improve in-distribution (ID) and OOD performance, bridging the performance gap between them. DaRL has been deployed online to serve the Alipay's insurance product search... 

**Abstract (ZH)**: 随着预训练大型语言模型（LLMs）的迅速发展，近期的研究利用了LLMs在相关性建模方面的能力，从而提升了性能。这通常通过在特定标注数据集上微调LLMs来确定查询和项之间的相关性来实现。然而，当LLMs通过微调和推理用于相关性建模时，存在两个局限性。首先，LLMs本身并不擅长执行需要细致判断的任务，比如搜索相关性评估，可能会因此显得过于自信，难以区分搜索中使用的细粒度相关性程度（如强相关、弱相关、无关）。其次，在现实世界场景中遇到数据分布变化时，其性能会显著下降。本文中，我们提出了一个名为“分布感知稳健学习”（DaRL）框架，用于支付宝搜索中的相关性建模。具体而言，我们设计了一种有效损失函数，以增强基于LLMs的相关性建模在不同细粒度查询-项相关性之间的区分能力。为了提高基于LLMs的相关性建模的一般化能力，我们首先提出了分布感知样本增强（DASA）模块。该模块利用异常样本检测技术主动选择那些原训练集中未充分覆盖的适当样本进行模型微调。此外，我们采用多阶段微调策略，同时提升分布内（ID）和异常分布（OOD）的性能，从而弥合两者之间的性能差距。DaRL已在支付宝保险产品搜索中上线应用。 

---
# LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph Reasoning for Cold-start Sequential Recommendation 

**Title (ZH)**: 大型语言模型是知识图谱推理器：具备直觉意识的知识图谱推理方法在冷启动序列推荐中的应用 

**Authors**: Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama  

**Link**: [PDF](https://arxiv.org/pdf/2412.12464)  

**Abstract**: Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items. However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios. On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems. Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues. To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings. By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies. Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios. 

**Abstract (ZH)**: 知识图谱（KGs）以图形结构表示实体之间的关系，并已广泛研究作为考虑项目准确内容信息的推荐系统的有前途的工具。然而，传统的基于KG的推荐方法面临根本性挑战：时间信息考虑不足以及在冷启动场景中的表现不佳。另一方面，大型语言模型（LLMs）可以被视为从网络数据中学到大量知识的数据库，并且由于其作为推荐系统应用的潜力，最近引起了广泛关注。尽管将LLMs视为推荐系统的做法能够利用其高推荐素养，但由于输入标记的限制，使得难以考虑整个推荐领域数据集，从而导致可扩展性问题。为了解决这些挑战，我们提出了一种LLM感知的知识图谱推理模型（LIKR）。我们的主要想法是将LLMs视为推理器，输出对KG的直觉探索策略。为了整合LLMs和KG的知识，我们通过强化学习训练了一个推荐代理，使用一个奖励函数，该函数结合了不同的推荐策略，包括LLMs的直觉和KG嵌入。通过利用提示工程引入时间感知，并从有限的交互中生成用户偏好的文本表示，LIKR可以在冷启动场景中改进推荐性能。此外，通过使用KG表示推荐领域的数据集并限制LLMs的输出为KG探索策略，LIKR可以避免可扩展性问题。在实际数据集上的实验表明，我们的模型在冷启动序列推荐场景中优于最先进的推荐方法。 

---
