{'arxiv_id': 'arXiv:2501.18536', 'title': 'Illusions of Relevance: Using Content Injection Attacks to Deceive Retrievers, Rerankers, and LLM Judges', 'authors': 'Manveer Singh Tamber, Jimmy Lin', 'link': 'https://arxiv.org/abs/2501.18536', 'abstract': 'Consider a scenario in which a user searches for information, only to encounter texts flooded with misleading or non-relevant content. This scenario exemplifies a simple yet potent vulnerability in neural Information Retrieval (IR) pipelines: content injection attacks. We find that embedding models for retrieval, rerankers, and large language model (LLM) relevance judges are vulnerable to these attacks, in which adversaries insert misleading text into passages to manipulate model judgements. We identify two primary threats: (1) inserting unrelated or harmful content within passages that still appear deceptively "relevant", and (2) inserting entire queries or key query terms into passages to boost their perceived relevance. While the second tactic has been explored in prior research, we present, to our knowledge, the first empirical analysis of the first threat, demonstrating how state-of-the-art models can be easily misled. Our study systematically examines the factors that influence an attack\'s success, such as the placement of injected content and the balance between relevant and non-relevant material. Additionally, we explore various defense strategies, including adversarial passage classifiers, retriever fine-tuning to discount manipulated content, and prompting LLM judges to adopt a more cautious approach. However, we find that these countermeasures often involve trade-offs, sacrificing effectiveness for attack robustness and sometimes penalizing legitimate documents in the process. Our findings highlight the need for stronger defenses against these evolving adversarial strategies to maintain the trustworthiness of IR systems. We release our code and scripts to facilitate further research.', 'abstract_zh': '用户在搜索信息时，往往会遇到充斥着误导性或不相关内容的文本。这种场景揭示了神经信息检索（IR）管道中的一个简单而强大的漏洞：内容注入攻击。我们发现，用于检索、重排和大型语言模型（LLM）相关性判断的嵌入模型都容易受到这些攻击的影响，攻击者会向文段中插入误导性文本以操控模型的判断结果。我们识别出两类主要的威胁：（1）在看似相关但实际上是不相关的文段中插入无关或有害内容；（2）将整个查询或关键查询术语插入文段以提高其感知相关性。虽然第二种战术在以往的研究中已有探索，但据我们所知，本研究首次进行了第一种威胁的实证分析，展示了最先进的模型如何容易被误导。我们系统地研究了影响攻击成功率的各种因素，如插入内容的位置以及相关和不相关材料之间的平衡。此外，我们还探讨了多种防御策略，包括对抗文段分类器、对检索器进行微调以忽略操纵内容，以及提示LLM判断者采取更加谨慎的态度。然而，我们发现这些反制措施常常存在权衡，可能会牺牲有效性以增强攻击抵御能力，并且有时会对合法文档造成惩罚。我们的研究结果强调了需要更强的防御措施来应对不断演变的对抗策略，以维护信息检索系统的可信度。我们发布了代码和脚本，以促进进一步的研究。', 'title_zh': '虚幻的相关性：使用内容注入攻击欺骗检索器、 reranking器和LLM裁判机'}
{'arxiv_id': 'arXiv:2501.18265', 'title': 'Collecting Cost-Effective, High-Quality Truthfulness Assessments with LLM Summarized Evidence', 'authors': 'Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro', 'link': 'https://arxiv.org/abs/2501.18265', 'abstract': 'With the degradation of guardrails against mis- and disinformation online, it is more critical than ever to be able to effectively combat it. In this paper, we explore the efficiency and effectiveness of using crowd-sourced truthfulness assessments based on condensed, large language model (LLM) generated summaries of online sources. We compare the use of generated summaries to the use of original web pages in an A/B testing setting, where we employ a large and diverse pool of crowd-workers to perform the truthfulness assessment. We evaluate the quality of assessments, the efficiency with which assessments are performed, and the behavior and engagement of participants. Our results demonstrate that the Summary modality, which relies on summarized evidence, offers no significant change in assessment accuracy over the Standard modality, while significantly increasing the speed with which assessments are performed. Workers using summarized evidence produce a significantly higher number of assessments in the same time frame, reducing the cost needed to acquire truthfulness assessments. Additionally, the Summary modality maximizes both the inter-annotator agreements as well as the reliance on and perceived usefulness of evidence, demonstrating the utility of summarized evidence without sacrificing the quality of assessments.', 'abstract_zh': '随着防护栏对虚假和误导信息的保护能力下降，有效地对抗这些信息比以往任何时候都更为重要。在本文中，我们探讨了利用基于大型语言模型（LLM）生成的在线信息浓缩摘要的众包真实度评估的有效性和效率。我们将生成的摘要与原始网页进行对比，在A/B测试环境中，采用大量多样化的众包工人进行真实度评估。我们评估了评估的质量、执行效率，以及参与者的言行和参与度。结果显示，依赖浓缩证据的摘要模态并未显著提高评估准确性，但显著提高了评估的执行速度。使用浓缩证据的工人在相同时间内能够产生更多评估，从而降低了获取真实度评估的成本。此外，摘要模态不仅最大化了注释者之间的共识，还提高了对证据的依赖性和感知有用性，证明了浓缩证据的效用，而不会牺牲评估的质量。', 'title_zh': '使用LLM总结的证据收集高效、高质量的可信度评估'}
{'arxiv_id': 'arXiv:2501.18099', 'title': 'Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge', 'authors': 'Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang', 'link': 'https://arxiv.org/abs/2501.18099', 'abstract': 'LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-bystep reasoning process that underlies the final evaluation of a response. However, due to the lack of human annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.', 'abstract_zh': '大规模语言模型作为法官（LLM-as-a-Judge）模型生成包含推理过程的链式思考（CoT）序列，旨在捕捉最终回答评估背后的逐步推理过程。然而，由于缺乏经过人工标注的CoT进行评估，有效的推理痕迹所必需的成分和结构仍然研究不足。因此，以往的方法往往（1）限制推理痕迹到手设计的成分，如标准列表、参考答案或验证问题，（2）将规划与评估推理交织在一起。在本文中，我们提出了EvalPlanner，一种用于Thinking-LLM-as-a-Judge的偏好优化算法，该算法首先生成不加约束的评估计划，然后执行，最后得出最终判决。在自我训练循环中，EvalPlanner逐步优化合成构建的评估计划与执行，从而获得更好的最终裁决结果。尽管仅在较少的真实偏好配对和合成生成的偏好配对进行训练，我们的方法仍然在RewardBench上取得了新的最佳性能（得分为93.9）。此外，在其他基准测试，如RM-Bench、JudgeBench和FollowBenchEval的实验进一步突显了规划和推理在构建稳健的LLM-as-a-Judge推理模型中的作用。', 'title_zh': '学习规划与推理以 Thinking-LLM-as-a-Judge 进行评估'}
{'arxiv_id': 'arXiv:2501.18455', 'title': 'Conversation Games and a Strategic View of the Turing Test', 'authors': 'Kaveh Aryan', 'link': 'https://arxiv.org/abs/2501.18455', 'abstract': 'Although many game-theoretic models replicate real interactions that often rely on natural language, explicit study of games where language is central to strategic interaction remains limited. This paper introduces the \\emph{conversation game}, a multi-stage, extensive-form game based on linguistic strategic interaction. We focus on a subset of the games, called verdict games. In a verdict game, two players alternate to contribute to a conversation, which is evaluated at each stage by a non-strategic judge who may render a conclusive binary verdict, or a decision to continue the dialogue. The game ends once a limit is reached or a verdict is given. We show many familiar processes, such as interrogation or a court process fall under this category. We also, show that the Turing test is an instance of verdict game, and discuss the significance of a strategic view of the Turing test in the age of advanced AI deception. We show the practical relevance of the proposed concepts by simulation experiments, and show that a strategic agent outperforms a naive agent by a high margin.', 'abstract_zh': '尽管许多博弈论模型能够模拟常常依赖自然语言的真实互动，对于语言在战略互动中起核心作用的游戏的明确研究仍然相对有限。本文介绍了\\emph{对话博弈}，这是一种基于语言战略互动的多阶段、扩展型博弈。我们重点关注这类博弈中的一个子集，称为裁决博弈。在裁决博弈中，两名玩家交替贡献于对话，每一步由一位非战略性的法官进行评估，这位法官可能会作出最终的二元裁决或决定继续对话。博弈在达到上限或给出裁决后结束。我们表明许多熟悉的流程，如审问或法庭程序都属于此类。我们还证明图灵测试是此类博弈的一个实例，并讨论在先进人工智能欺诈时代重新审视图灵测试的战略视角的重要性。我们通过模拟实验展示了所提出概念的实用相关性，结果表明战略智能体的性能远远优于非明智智能体。', 'title_zh': '对话游戏与图灵测试的战略视角'}
{'arxiv_id': 'arXiv:2501.18081', 'title': 'Normative Evaluation of Large Language Models with Everyday Moral Dilemmas', 'authors': 'Pratik S. Sachdeva, Tom van Nuenen', 'link': 'https://arxiv.org/abs/2501.18081', 'abstract': 'The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am I the Asshole" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven LLMs to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs\' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations.', 'abstract_zh': '大规模语言模型（LLMs）的快速普及已经推动了对它们编码道德规范和决策过程的广泛研究。这些研究中的许多都依赖于用调查式问题提示LLMs，以评估模型与特定人口群体、道德信念或政治意识形态的对齐程度。尽管这些方法具有信息价值，但它们对相对表面的构建物的依从性往往会简化日常生活中的道德困境的复杂性和细微之处。我们主张，沿更详细的人类互动轴审计LLMs对于更好地评估它们可能对人类信念和行为的影响至关重要。为了实现这一目标，我们评估了来自“Am I the Asshole”（AITA）社区（Reddit用户在此寻求其他社区成员对日常冲突的道德判断）的复杂且常见的道德困境中的LLMs。我们提示了七种LLMs为超过10,000个AITA道德困境分配责任并提供解释。然后我们将LLMs的判断和解释与 redditors 的判断以及彼此之间的判断进行了比较，旨在揭示它们在道德推理中的模式。我们的结果显示，大规模语言模型在处理AITA子网页上的道德判断时表现出不同的模式，与人类评价相差甚远。LLMs在自我一致性方面表现出中等到高的水平，但在模型之间的一致性较低。进一步分析模型解释揭示了模型在运用各种道德原则时存在的不同模式。这些发现突显了在人工系统中实现一致道德推理的复杂性，并强调了仔细评估不同模型在伦理判断方面的方法的必要性。随着LLMs在需要伦理决策角色，如治疗师和伴侣等方面的应用，需要仔细评估以减轻潜在的偏见和限制。', 'title_zh': '使用日常道德困境对大型语言模型进行规范性评估'}
