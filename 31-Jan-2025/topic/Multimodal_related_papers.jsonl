{'arxiv_id': 'arXiv:2501.18533', 'title': 'Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models', 'authors': 'Yi Ding, Lijun Li, Bing Cao, Jing Shao', 'link': 'https://arxiv.org/abs/2501.18533', 'abstract': 'Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on textual or multimodal content, fall short in addressing challenging cases or disrupt the balance between helpfulness and harmlessness. Our evaluation highlights a safety reasoning gap: these methods lack safety visual reasoning ability, leading to such bottlenecks. To address this limitation and enhance both visual perception and reasoning in safety-critical contexts, we propose a novel dataset that integrates multi-image inputs with safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve model performance. Specifically, we introduce the Multi-Image Safety (MIS) dataset, an instruction-following dataset tailored for multi-image safety scenarios, consisting of training and test splits. Our experiments demonstrate that fine-tuning InternVL2.5-8B with MIS significantly outperforms both powerful open-source models and API-based models in challenging multi-image tasks requiring safety-related visual reasoning. This approach not only delivers exceptional safety performance but also preserves general capabilities without any trade-offs. Specifically, fine-tuning with MIS increases average accuracy by 0.83% across five general benchmarks and reduces the Attack Success Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are released under: \\href{this https URL}{\\texttt{this https URL}}', 'abstract_zh': '大型视觉-语言模型（VLMs）在众多任务中取得了显著的性能。然而，在安全性关键领域中的部署面临重大挑战。现有的安全微调方法集中在文本或多模态内容上，无法有效应对复杂情况或在有用性和无害性之间保持平衡。我们的评估揭示了一个安全推理的缺口：这些方法缺乏表观安全推理的能力，导致了这一瓶颈。为了解决这一局限性并增强安全性关键场景下的视觉感知和推理能力，我们提出了一种新数据集，该数据集将多张图片输入与安全推理链（CoT）标签集成，以提高模型性能。具体而言，我们引入了多张图片安全（MIS）数据集，这是一个专门针对多张图片安全场景的指令遵循数据集，包含训练集和测试集。实验结果表明，使用MIS微调InternVL2.5-8B模型在复杂的多张图片任务中要求在与安全性相关的视觉推理方面，显著优于强大的开源模型和基于API的模型。这种方法不仅提供了卓越的安全性能，还保留了一般功能而没有任何交易。具体而言，使用MIS进行微调使五个通用基准的平均准确率提高了0.83%，并大幅降低了多个安全性基准上的攻击成功率（ASR）。数据和模型已在此处发布：\\href{this https URL}{this https URL}', 'title_zh': '重新思考视觉语言模型安全性微调中的瓶颈问题'}
{'arxiv_id': 'arXiv:2501.18362', 'title': 'MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding', 'authors': 'Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou', 'link': 'https://arxiv.org/abs/2501.18362', 'abstract': 'We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 16 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.', 'abstract_zh': '我们介绍了MedXpertQA，这是一个具有高度挑战性和综合性的基准测试，用于评估专家级医学知识和高级推理能力。MedXpertQA 包含了4,460道问题，覆盖了17个专科和11个身体系统。它包括两个子集：Text（用于文本评估）和MM（用于多模态评估）。值得注意的是，MM 子集引入了包含多样化图像和丰富临床信息（包括患者记录和检查结果）的专家级考试问题，这使其区别于传统医学多模态基准测试中简单的问题-答案对，这些问题是通过图像标题生成的。MedXpertQA 通过严格的筛选和增强方法解决了现有基准（如MedQA）中存在的不足难度问题，并融入了专科考试问题以提高临床相关性和全面性。我们进行了数据合成以减轻数据泄露风险，并进行了多轮专家评审以确保准确性和可靠性。我们在MedXpertQA 上评估了16个领先的模型。此外，医学与现实世界的决策紧密相关，为评估超越数学和代码的推理能力提供了丰富且具代表性的环境。为此，我们开发了面向推理的子集，以方便评估类似o1的模型。', 'title_zh': 'MedXpertQA：专家级医学推理与理解的基准测试'}
{'arxiv_id': 'arXiv:2501.18096', 'title': 'LLMs can see and hear without any training', 'authors': 'Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar', 'link': 'https://arxiv.org/abs/2501.18096', 'abstract': 'We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities into your favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.', 'abstract_zh': '我们介绍了MILS：一种多模态迭代LLM求解器，这是一种出人意料地简单且无需训练的方法，将多模态能力注入您最喜欢的LLM。利用其天生的多步骤推理能力，MILS 提示LLM生成候选输出，每个输出都会被打分并反馈给下一个迭代过程，最终生成任务的解决方案。这使得许多通常需要在特定任务数据上训练专门模型的应用成为可能。具体而言，我们在这个领域取得了新的最佳成果，特别是在新兴的零样本图像、视频和音频描述方面。MILS 无缝应用于媒体生成，通过发现提示重写来改进文本到图像生成，并且甚至可以编辑提示进行风格转换！最后，作为无梯度优化方法，MILS 可以将多模态嵌入反转为文本，从而支持跨模态算术等应用。', 'title_zh': '大语言模型无需训练即可具备视觉和听觉能力'}
{'arxiv_id': 'arXiv:2501.18592', 'title': 'Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models', 'authors': 'Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink', 'link': 'https://arxiv.org/abs/2501.18592', 'abstract': 'In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization; (4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature at this https URL.', 'abstract_zh': '在实际应用场景中，实现领域适应和泛化面临重大挑战，因为模型必须适应或泛化到未知的目标分布。将这些能力扩展到未见过的多模态分布，即多模态领域的适应和泛化，更是具有挑战性，因为不同模态具有独特的特征。多年来，已在行动识别、语义分割等领域取得了显著进展。此外，最近大规模预训练多模态基础模型的出现，如CLIP，启发了许多利用这些模型提升领域适应和泛化性能或将其适应到下游任务的工作。本文综述提供了一个关于从传统方法到基础模型的最新进展的全面回顾，涵盖了以下方面：（1）多模态领域适应；（2）多模态测试时适应；（3）多模态领域泛化；（4）借助多模态基础模型的领域适应和泛化；（5）多模态基础模型的适应。对每一主题，我们正式定义问题，并详细回顾现有方法。此外，我们分析相关数据集和应用，突出存在的挑战，并提出未来研究方向。我们保持一个活跃的仓库，其中包含最新的文献，地址为：https://this-url。', 'title_zh': '多模态适应与泛化的进展：从传统方法到基础模型'}
{'arxiv_id': 'arXiv:2501.18271', 'title': 'Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks', 'authors': 'Hao-Zhe Tan, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2501.18271', 'abstract': 'Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: \\emph{model labeling}, which assigns labels to each VLM to describe their specialty and utility; \\emph{model selection}, which matches the requirements of the target task with model labels; and \\emph{model reuse}, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.', 'abstract_zh': '预训练的视觉-语言模型（VLMs）在各种视觉任务中越来越受欢迎，并且已经释放出了多种开源的VLM变体。然而，选择最适合特定下游任务的预训练VLM仍然是一个挑战，因为没有一种VLM能够在所有下游任务中都表现出色，而且由于时间限制和数据限制，评估所有可用的VLM是不可能的。为解决这一问题，本文提出了一种新的范式，称为模型标签学习（Model Label Learning, MLL），以选择和重复使用VLM来处理下游任务。此提案包含三个关键模块：**模型标签化**，将标签分配给每个VLM以描述其特色和实用性；**模型选择**，将目标任务的要求与模型标签相匹配；以及**模型重复使用**，以集成的方式应用所选的VLM来解决目标任务。该提案具有高度的计算效率和扩展性，因为模型标签化过程是与目标任务无关的，并且随着候选模型数量的增加，能力可以得到增强。此外，我们还引入了一种新的基准测试用于评估VLM选择方法，包括49种VLM和17个目标任务数据集。实验结果清楚地展示了所提方法在选择和重复使用VLM方面的有效性。', 'title_zh': '预训练视觉-语言模型的选择与重用用于下游任务'}
{'arxiv_id': 'arXiv:2501.18269', 'title': 'MAMS: Model-Agnostic Module Selection Framework for Video Captioning', 'authors': 'Sangho Lee, Il Yong Chun, Hogun Park', 'link': 'https://arxiv.org/abs/2501.18269', 'abstract': 'Multi-modal transformers are rapidly gaining attention in video captioning tasks. Existing multi-modal video captioning methods typically extract a fixed number of frames, which raises critical challenges. When a limited number of frames are extracted, important frames with essential information for caption generation may be missed. Conversely, extracting an excessive number of frames includes consecutive frames, potentially causing redundancy in visual tokens extracted from consecutive video frames. To extract an appropriate number of frames for each video, this paper proposes the first model-agnostic module selection framework in video captioning that has two main functions: (1) selecting a caption generation module with an appropriate size based on visual tokens extracted from video frames, and (2) constructing subsets of visual tokens for the selected caption generation module. Furthermore, we propose a new adaptive attention masking scheme that enhances attention on important visual tokens. Our experiments on three different benchmark datasets demonstrate that the proposed framework significantly improves the performance of three recent video captioning models.', 'abstract_zh': '多模态变压器在视频字幕任务中正迅速获得关注。现有的多模态视频字幕方法通常提取固定数量的帧，这引发了关键性的挑战。当提取帧的数量有限时，可能会遗漏包含用于字幕生成关键信息的重要帧。相反，提取过多的帧会导致连续帧的冗余问题，从而从连续的视频帧中提取出视觉标记的冗余。为了解决每个视频提取适当数量帧的问题，本文提出了一种新的模型无关的模块选择框架，该框架具有两个主要功能：(1) 根据从视频帧中提取的视觉标记选择一个合适的字幕生成模块；(2) 为所选的字幕生成模块构建视觉标记的子集。此外，我们提出了一种新的自适应注意力掩码方案，以增强对重要视觉标记的关注。我们在三个不同的基准数据集上的实验表明，所提出的框架显著提升了三种近期视频字幕模型的性能。', 'title_zh': 'MAMS：视频字幕生成的模型无关模块选择框架'}
{'arxiv_id': 'arXiv:2501.18237', 'title': 'Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers', 'authors': 'Malte Tölle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt', 'link': 'https://arxiv.org/abs/2501.18237', 'abstract': "A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status. These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic interventions such as medication administration, and images. While physicians are able to process and integrate diverse modalities intuitively, neural networks need specific modeling for each modality complicating the training procedure. We demonstrate that this complexity can be significantly reduced by visualizing all information as images along with unstructured text and subsequently training a conventional vision-text transformer. Our approach, Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not only simplifies data preprocessing and modeling but also outperforms current state-of-the-art methods in predicting in-hospital mortality and phenotyping, as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities include patient's clinical measurements, medications, X-ray images, and electrocardiography scans. We hope our work inspires advancements in multi-modal medical AI by reducing the training complexity to (visual) prompt engineering, thus lowering entry barriers and enabling no-code solutions for training. The source code will be made publicly available.", 'abstract_zh': '患者在每次住院期间会接受多次检查，每次检查都会提供不同的健康状况方面。这些评估包括不同采样率的时序数据、离散的单点测量、治疗干预措施（如药物给药），以及影像学检查等。虽然医生能够直观地处理和综合这些不同的模态信息，但神经网络需要针对每个模态进行特定建模，这使得训练过程变得复杂。我们通过将所有信息可视化为图像，并结合非结构化文本，然后训练一个常规的视觉-文本转换器，来证明这一复杂性可以显著降低。我们的方法，不规则采样多模态测量的视觉变换器（ViTiMM），不仅简化了数据预处理和建模过程，还在MIMIC-IV数据集上的6175名患者中验证了其在预测院内死亡率和疾病表型方面的性能，超过当前最先进的方法。这些模态包括患者的临床测量、药物、X射线影像和心电图扫描。我们希望我们的工作能够推动多模态医学AI的发展，将训练复杂性降低到（视觉）提示工程，从而降低入门门槛并使无代码解决方案成为可能。源代码将公开提供。', 'title_zh': '任意格式数据的图像化融合：基于视觉变换器的不规则时间间隔多模态患者数据融合'}
{'arxiv_id': 'arXiv:2501.18124', 'title': 'REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning', 'authors': 'Liangjing Shao, Benshuang Chen, Shuting Zhao, Xinrong Chen', 'link': 'https://arxiv.org/abs/2501.18124', 'abstract': 'Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here: \\href{this https URL}{this http URL}', 'abstract_zh': '内窥镜的实时自我运动跟踪是实现内窥镜高效导航和自动化控制的一个重要任务。本文提出了一种新的框架，用于进行内窥镜的实时自我运动跟踪。首先，提出了一种多模态视觉特征学习网络，用于进行相对姿态预测，在其中从光学流中提取运动特征，从场景特征中提取场景特征，并从两个相邻观测中提取联合特征，用于预测。由于连接图像通道维度中包含更多的相关性信息，基于注意机制设计了一种新的特征提取器，以整合连续两帧的连接信息的多维度信息。为了从融合特征中提取更完整的特征表示，提出了一种新的姿态解码器，用于预测框架末端连接特征图的姿态变换。最后，基于相对姿态计算内窥镜的绝对姿态。实验在各种内窥镜场景的三个数据集中进行，结果表明，所提出的方法优于现有最先进的方法。此外，所提出方法的推理速度超过每秒30帧，满足实时要求。项目页面在此：\\href{this https URL}{this http URL}', 'title_zh': 'REMOTE：通过多模态视觉特征学习实现各种内窥镜的实时自我运动跟踪'}
{'arxiv_id': 'arXiv:2501.18006', 'title': 'Topological Signatures of Adversaries in Multimodal Alignments', 'authors': 'Minh Vu, Geigh Zollicoffer, Huy Mai, Ben Nebgen, Boian Alexandrov, Manish Bhattarai', 'link': 'https://arxiv.org/abs/2501.18006', 'abstract': 'Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to back-propagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection.', 'abstract_zh': '多模态机器学习系统，特别是如CLIP/BLIP模型那样的文本和图像数据对齐系统，逐渐变得越来越普遍，但仍然容易受到对抗性攻击的影响。尽管在单模态背景下已经进行了大量研究以增强对抗性鲁棒性，但针对多模态系统的防御策略仍然未被充分探索。本研究探讨了图像和文本嵌入之间的拓扑特征，并展示了对抗性攻击如何破坏它们的对齐，从而产生独特的拓扑特征。我们具体利用持续同调方法，并基于整体持久性和多尺度核方法引入了两种新的拓扑对比损失，以分析由对抗性扰动引入的拓扑特征。我们观察到，在图像-文本对齐的一系列广泛的攻击中，所提出的拓扑损失呈现单调变化的趋势，随着数据中对抗样本数量的增加，这种趋势更加明显。通过设计一种算法将这些特征反向传播到输入样本，我们能够将这些特征整合到最大均值离异度检验中，由此创建了一种新的基于拓扑特征的对抗检测方法。', 'title_zh': '多模态对齐中对手的拓扑特征'}
{'arxiv_id': 'arXiv:2501.17880', 'title': 'Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure', 'authors': 'Seyd Teymoor Seydi', 'link': 'https://arxiv.org/abs/2501.17880', 'abstract': 'This study presents a comprehensive analysis of four significant California wildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts through multiple dimensions, including land cover change, jurisdictional management, structural damage, and demographic vulnerability. Using the Chebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the extent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares. Our analysis revealed that shrubland ecosystems were consistently the most affected, comprising 57.4-75.8% of burned areas across all events. The jurisdictional assessment demonstrated varying management complexities, from singular authority (98.7% in the Palisades Fire) to distributed management across multiple agencies. A structural impact analysis revealed significant disparities between urban interface fires (Eaton: 9,869 structures; Palisades: 8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17 structures). The demographic analysis showed consistent gender distributions, with 50.9% of the population identified as female and 49.1% as male. Working-age populations made up the majority of the affected populations, ranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods. The study identified strong correlations between urban interface proximity, structural damage, and population exposure. The Palisades and Eaton fires affected over 20,000 people each, compared to fewer than 500 in rural events. These findings offer valuable insights for the development of targeted wildfire management strategies, particularly in wildland urban interface zones, and emphasize the need for age- and gender-conscious approaches in emergency response planning.', 'abstract_zh': '本研究对加利福尼亚州四起重大森林火灾——Palisades、Eaton、Kenneth和Hurst进行了全面分析，探讨了这些火灾在多个维度上的影响，包括土地覆盖变化、行政管理复杂性、建筑物损毁以及人口易敏感性。研究利用Chebyshev-Kolmogorov-Arnold网络模型对Sentinel-2遥感图像进行分析，绘制了烧毁区域范围从315.36到10,960.98公顷的分布图。分析结果表明，灌木丛生态系统是最受冲击的，占所有事件中燃烧面积的57.4%至75.8%。行政管理评估显示管理复杂度存在差异，从单一行政主体（Palisades火灾中占98.7%）到多部门联合管理。建筑物损毁分析揭示了城乡边界火灾（Eaton：9,869栋建筑；Palisades：8,436栋建筑）与农村火灾（Kenneth：24栋建筑；Hurst：17栋建筑）之间显著差异。人口易敏感性分析显示，女性和男性的分布基本一致，其中50.9%的人口为女性，49.1%为男性。受影响的人口中，工作年龄人口比例最高，范围从53.7%到54.1%，灾后时期存在显著的时间变化。研究结果表明，城市边界距离、建筑物损毁和人口暴露之间存在密切关联。与农村火灾相比，Palisades和Eaton火灾分别影响了超过20,000人，而农村火灾的影响人数不到500人。这些发现为针对城乡过渡地带的野火管理策略开发提供了宝贵见解，并强调了在应急响应规划中采取年龄和性别适配方法的重要性。', 'title_zh': '2025年1月洛杉矶县野火评估：多模态影响、响应和人口暴露分析'}
