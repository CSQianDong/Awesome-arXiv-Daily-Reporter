{'arxiv_id': 'arXiv:2501.18542', 'title': 'Semantic Web and Creative AI -- A Technical Report from ISWS 2023', 'authors': "Raia Abu Ahmad, Reham Alharbi, Roberto Barile, Martin Böckling, Francisco Bolanos, Sara Bonfitto, Oleksandra Bruns, Irene Celino, Yashrajsinh Chudasama, Martin Critelli, Claudia d'Amato, Giada D'Ippolito, Ioannis Dasoulas, Stefano De Giorgis, Vincenzo De Leo, Chiara Di Bonaventura, Marco Di Panfilo, Daniil Dobriy, John Domingue, Xuemin Duan, Michel Dumontier, Sefika Efeoglu, Ruben Eschauzier, Fakih Ginwa, Nicolas Ferranti, Arianna Graciotti, Philipp Hanisch, George Hannah, Golsa Heidari, Aidan Hogan, Hassan Hussein, Alexane Jouglar, Jan-Christoph Kalo, Manoé Kieffer, Antonis Klironomos, Inês Koch, Weronika Lajewska, Nicolas Lazzari, Mikael Lindekrans, Anna Sofia Lippolis, Majlinda Llugiqi, Eleonora Mancini, Eleonora Marzi, Laura Menotti, Daniela Milon Flores, Soulakshmee Nagowah, Kerstin Neubert, Emetis Niazmand, Ebrahim Norouzi, Beatriz Olarte Martinez, Anouk Michelle Oudshoorn, Andrea Poltronieri, Valentina Presutti, Disha Purohit, Ensiyeh Raoufi, Celian Ringwald, Johanna Rockstroh, Sebastian Rudolph, Harald Sack, Zafar Saeed, Mohammad Javad Saeedizade, Aya Sahbi, Cristian Santini, Aleksandra Simic, Dennis Sommer, Rita Sousa, Mary Ann Tan, Vidyashree Tarikere, Tabea Tietz, Liam Tirpitz, Arnaldo Tomasino, Frank van Harmelen, Joao Vissoci, Caitlin Woods, Bohui Zhang, Xinyue Zhang, Heng Zheng", 'link': 'https://arxiv.org/abs/2501.18542', 'abstract': 'The International Semantic Web Research School (ISWS) is a week-long intensive program designed to immerse participants in the field. This document reports a collaborative effort performed by ten teams of students, each guided by a senior researcher as their mentor, attending ISWS 2023. Each team provided a different perspective to the topic of creative AI, substantiated by a set of research questions as the main subject of their investigation. The 2023 edition of ISWS focuses on the intersection of Semantic Web technologies and Creative AI. ISWS 2023 explored various intersections between Semantic Web technologies and creative AI. A key area of focus was the potential of LLMs as support tools for knowledge engineering. Participants also delved into the multifaceted applications of LLMs, including legal aspects of creative content production, humans in the loop, decentralised approaches to multimodal generative AI models, nanopublications and AI for personal scientific knowledge graphs, commonsense knowledge in automatic story and narrative completion, generative AI for art critique, prompt engineering, automatic music composition, commonsense prototyping and conceptual blending, and elicitation of tacit knowledge. As Large Language Models and semantic technologies continue to evolve, new exciting prospects are emerging: a future where the boundaries between creative expression and factual knowledge become increasingly permeable and porous, leading to a world of knowledge that is both informative and inspiring.', 'abstract_zh': '国际语义网研究学校（ISWS）是一项为期一周的密集型项目，旨在使参与者沉浸在该领域。本文档报告了2023年ISWS期间由十支由资深研究人员作为导师指导的学生团队进行的一项合作努力。每个团队从不同角度探讨了创意人工智能的主题，基于一系列研究问题作为他们调查的主要内容。2023年ISWS的重点在于语义网技术与创意人工智能的交集。ISWS 2023探讨了语义网技术和创意人工智能之间的多种交集。一个关键研究领域是大型语言模型在知识工程中的潜在支持作用。参与者还深入探讨了大型语言模型的多方面应用，包括创造性内容生产中的法律问题、循环的人类参与、分散式多模态生成人工智能模型、纳米出版物及个性化科学知识图谱的AI应用、常识知识在自动故事和叙事完成中的作用、创造性生成人工智能用于艺术批评、提示工程、自动音乐创作、常识原型设计和概念融合，以及隐性知识的激发。随着大型语言模型和语义技术的不断发展，新的令人兴奋的可能性正在涌现：一个未来世界，其中创造性表达与事实知识之间的界限越来越模糊和开放，从而构成了既具有信息性又具有启发性的知识世界。', 'title_zh': '语义网与创意人工智能——来自2023年ISWS的技术报告'}
{'arxiv_id': 'arXiv:2501.18455', 'title': 'Conversation Games and a Strategic View of the Turing Test', 'authors': 'Kaveh Aryan', 'link': 'https://arxiv.org/abs/2501.18455', 'abstract': 'Although many game-theoretic models replicate real interactions that often rely on natural language, explicit study of games where language is central to strategic interaction remains limited. This paper introduces the \\emph{conversation game}, a multi-stage, extensive-form game based on linguistic strategic interaction. We focus on a subset of the games, called verdict games. In a verdict game, two players alternate to contribute to a conversation, which is evaluated at each stage by a non-strategic judge who may render a conclusive binary verdict, or a decision to continue the dialogue. The game ends once a limit is reached or a verdict is given. We show many familiar processes, such as interrogation or a court process fall under this category. We also, show that the Turing test is an instance of verdict game, and discuss the significance of a strategic view of the Turing test in the age of advanced AI deception. We show the practical relevance of the proposed concepts by simulation experiments, and show that a strategic agent outperforms a naive agent by a high margin.', 'abstract_zh': '尽管许多博弈论模型能够模拟常常依赖自然语言的真实互动，对于语言在战略互动中起核心作用的游戏的明确研究仍然相对有限。本文介绍了\\emph{对话博弈}，这是一种基于语言战略互动的多阶段、扩展型博弈。我们重点关注这类博弈中的一个子集，称为裁决博弈。在裁决博弈中，两名玩家交替贡献于对话，每一步由一位非战略性的法官进行评估，这位法官可能会作出最终的二元裁决或决定继续对话。博弈在达到上限或给出裁决后结束。我们表明许多熟悉的流程，如审问或法庭程序都属于此类。我们还证明图灵测试是此类博弈的一个实例，并讨论在先进人工智能欺诈时代重新审视图灵测试的战略视角的重要性。我们通过模拟实验展示了所提出概念的实用相关性，结果表明战略智能体的性能远远优于非明智智能体。', 'title_zh': '对话游戏与图灵测试的战略视角'}
{'arxiv_id': 'arXiv:2501.18413', 'title': 'GBFRS: Robust Fuzzy Rough Sets via Granular-ball Computing', 'authors': 'Shuyin Xia, Xiaoyu Lian, Binbin Sang, Guoyin Wang, Xinbo Gao', 'link': 'https://arxiv.org/abs/2501.18413', 'abstract': "Fuzzy rough set theory is effective for processing datasets with complex attributes, supported by a solid mathematical foundation and closely linked to kernel methods in machine learning. Attribute reduction algorithms and classifiers based on fuzzy rough set theory exhibit promising performance in the analysis of high-dimensional multivariate complex data. However, most existing models operate at the finest granularity, rendering them inefficient and sensitive to noise, especially for high-dimensional big data. Thus, enhancing the robustness of fuzzy rough set models is crucial for effective feature selection. Muiti-garanularty granular-ball computing, a recent development, uses granular-balls of different sizes to adaptively represent and cover the sample space, performing learning based on these granular-balls. This paper proposes integrating multi-granularity granular-ball computing into fuzzy rough set theory, using granular-balls to replace sample points. The coarse-grained characteristics of granular-balls make the model more robust. Additionally, we propose a new method for generating granular-balls, scalable to the entire supervised method based on granular-ball computing. A forward search algorithm is used to select feature sequences by defining the correlation between features and categories through dependence functions. Experiments demonstrate the proposed model's effectiveness and superiority over baseline methods.", 'abstract_zh': '模糊粗糙集理论适用于处理具有复杂属性的数据集，其基于坚实的数学基础，并且与机器学习中的核方法紧密相关。基于模糊粗糙集理论的属性约简算法和分类器在高维多变量复杂数据的分析中表现出色。然而，现有的大多数模型在最精细的粒度级别操作，这使它们变得低效且对噪声敏感，尤其是在处理高维大数据时。因此，增强模糊粗糙集模型的鲁棒性对于有效的特征选择至关重要。粒度多重粒计算是一种最近的发展，它使用不同大小的粒计算来适应地表示和覆盖样本空间，并基于这些粒计算进行学习。本文提出将粒度多重粒计算整合到模糊粗糙集理论中，使用粒计算来替代样本点。粒计算的粗粒化特性使得该模型更具鲁棒性。此外，我们提出了一种生成粒计算的新方法，并将其扩展到基于粒计算的整个监督方法。通过定义特征与类别之间的相关性依赖函数，使用前向搜索算法选择特征序列。实验结果表明，所提出的模型比基线方法更有效且更优。', 'title_zh': 'GBFRS：基于粒球计算的鲁棒模糊粗糙集'}
{'arxiv_id': 'arXiv:2501.18411', 'title': 'Gravity-Bench-v1: A Benchmark on Gravitational Physics Discovery for Agents', 'authors': 'Nolan Koblischke, Hyunseok Jang, Kristen Menou, Mohamad Ali-Dib', 'link': 'https://arxiv.org/abs/2501.18411', 'abstract': 'Modern science emerged from reasoning over repeatedly-observed planetary motions. We present Gravity-Bench-v1, an environment-based benchmark that challenges AI agents on tasks that parallel this historical development. Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within a dynamic environment, using rigorous gravitational dynamics simulations. Gravity-Bench includes out-of-distribution cases, i.e. with physics that deviates from the real world, to evaluate true scientific generalization capabilities. Agents must plan to collect data within an experimental budget and must perform a dynamic form of data analysis and reasoning to solve tasks efficiently. Our benchmark admits an open-ended space of solutions. PhD-level solutions for each task are provided, to calibrate AI performance against human expertise. Technically at an upper-undergraduate level, our benchmark proves challenging to baseline AI agents. Gravity-Bench-v1 and planned extensions should help map out AI progress towards scientific discovery capabilities.', 'abstract_zh': '现代科学研究起源于对反复观测到的行星运动的推理。本文介绍了基于环境的基准测试Gravity-Bench-v1，该基准测试挑战AI代理完成类似于这一历史发展的任务。Gravity-Bench-v1 评估代理在动态环境中发现隐藏的物理现象的能力，通过严格的引力动力学模拟进行评估。Gravity-Bench 包括未见过分布（out-of-distribution）情况，即包含与现实世界不符的物理现象，以评估真正的科学泛化能力。代理必须在实验预算内计划收集数据，并通过动态的数据分析和推理高效地解决问题。我们的基准测试允许广泛的方法空间。每项任务都提供了博士水平的解决方案，以便根据人类专业知识校准AI性能。技术上而言，我们的基准测试对基础模型构成较大挑战。Gravity-Bench-v1 及其未来扩展应有助于绘制AI在科学发现能力方面的进步。', 'title_zh': 'Gravity-Bench-v1：智能体在引力物理发现方面的基准测试'}
{'arxiv_id': 'arXiv:2501.18362', 'title': 'MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding', 'authors': 'Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, Bowen Zhou', 'link': 'https://arxiv.org/abs/2501.18362', 'abstract': 'We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 16 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.', 'abstract_zh': '我们介绍了MedXpertQA，这是一个具有高度挑战性和综合性的基准测试，用于评估专家级医学知识和高级推理能力。MedXpertQA 包含了4,460道问题，覆盖了17个专科和11个身体系统。它包括两个子集：Text（用于文本评估）和MM（用于多模态评估）。值得注意的是，MM 子集引入了包含多样化图像和丰富临床信息（包括患者记录和检查结果）的专家级考试问题，这使其区别于传统医学多模态基准测试中简单的问题-答案对，这些问题是通过图像标题生成的。MedXpertQA 通过严格的筛选和增强方法解决了现有基准（如MedQA）中存在的不足难度问题，并融入了专科考试问题以提高临床相关性和全面性。我们进行了数据合成以减轻数据泄露风险，并进行了多轮专家评审以确保准确性和可靠性。我们在MedXpertQA 上评估了16个领先的模型。此外，医学与现实世界的决策紧密相关，为评估超越数学和代码的推理能力提供了丰富且具代表性的环境。为此，我们开发了面向推理的子集，以方便评估类似o1的模型。', 'title_zh': 'MedXpertQA：专家级医学推理与理解的基准测试'}
{'arxiv_id': 'arXiv:2501.18320', 'title': 'Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach', 'authors': 'Tianpeng Pan, Wenqiang Pu, Licheng Zhao, Rui Zhou', 'link': 'https://arxiv.org/abs/2501.18320', 'abstract': 'Automated optimization modeling (AOM) has evoked considerable interest with the rapid evolution of large language models (LLMs). Existing approaches predominantly rely on prompt engineering, utilizing meticulously designed expert response chains or structured guidance. However, prompt-based techniques have failed to perform well in the sensor array signal processing (SASP) area due the lack of specific domain knowledge. To address this issue, we propose an automated modeling approach based on retrieval-augmented generation (RAG) technique, which consists of two principal components: a multi-agent (MA) structure and a graph-based RAG (Graph-RAG) process. The MA structure is tailored for the architectural AOM process, with each agent being designed based on principles of human modeling procedure. The Graph-RAG process serves to match user query with specific SASP modeling knowledge, thereby enhancing the modeling result. Results on ten classical signal processing problems demonstrate that the proposed approach (termed as MAG-RAG) outperforms several AOM benchmarks.', 'abstract_zh': '自动化优化建模（AOM）随着大型语言模型（LLMs）的迅速发展引起了广泛的关注。现有的方法主要依赖于提示工程，利用精心设计的专家响应链或结构化的指导。然而，在传感器阵列信号处理（SASP）领域，提示基础的方法由于缺乏特定领域的知识而表现不佳。为了解决这一问题，我们提出了一种基于检索增强生成（RAG）技术的自动化建模方法，该方法主要包括两个主要组成部分：一个多代理（MA）结构和基于图的RAG（Graph-RAG）过程。多代理结构专门针对架构化的AOM流程，其中每个代理都是基于人类建模过程的原则设计的。基于图的RAG过程用于匹配用户查询与特定的SASP建模知识，从而提高建模结果。在十个经典信号处理问题上的实验结果显示，所提出的方法（称为MAG-RAG）在几个AOM基准中表现更优。', 'title_zh': '利用大语言模型代理进行SASP问题的自动化优化建模：基于Graph-RAG的方法'}
{'arxiv_id': 'arXiv:2501.18299', 'title': 'Model-Free RL Agents Demonstrate System 1-Like Intentionality', 'authors': 'Hal Ashton, Matija Franklin', 'link': 'https://arxiv.org/abs/2501.18299', 'abstract': 'This paper argues that model-free reinforcement learning (RL) agents, while lacking explicit planning mechanisms, exhibit behaviours that can be analogised to System 1 ("thinking fast") processes in human cognition. Unlike model-based RL agents, which operate akin to System 2 ("thinking slow") reasoning by leveraging internal representations for planning, model-free agents react to environmental stimuli without anticipatory modelling. We propose a novel framework linking the dichotomy of System 1 and System 2 to the distinction between model-free and model-based RL. This framing challenges the prevailing assumption that intentionality and purposeful behaviour require planning, suggesting instead that intentionality can manifest in the structured, reactive behaviours of model-free agents. By drawing on interdisciplinary insights from cognitive psychology, legal theory, and experimental jurisprudence, we explore the implications of this perspective for attributing responsibility and ensuring AI safety. These insights advocate for a broader, contextually informed interpretation of intentionality in RL systems, with implications for their ethical deployment and regulation.', 'abstract_zh': '本文 argues，尽管模型自由强化学习（RL）代理缺乏显式规划机制，但它们表现出的行为可以类比于人类认知中的系统1（“快速思考”）过程。与依赖内部表示进行规划、操作类似于系统2（“慢速思考”）推理的模型依赖RL代理不同，模型自由代理对环境刺激作出反应，而不进行预先建模。我们提出了一种新的框架，将系统1与系统2的区别与模型自由和模型依赖RL的区别联系起来。这种框架挑战了先前假设，即意图性和有目的行为需要规划，表明意图性可以在模型自由代理的结构化、反应性行为中体现。通过借鉴认知心理学、法律理论和实验法律学的跨学科见解，我们探讨了这一视角对于归因责任和确保AI安全的影响。这些见解提倡在RL系统中对意图性采用更为广泛、情境化的解释，这有助于其伦理部署和监管。', 'title_zh': '无模型 RL 剂量表现出类似系统 1 的意图性'}
{'arxiv_id': 'arXiv:2501.18296', 'title': 'Extending the design space of ontologization practices: Using bCLEARer as an example', 'authors': 'Chris Partridge, Andrew Mitchell, Sergio de Cesare, John Beverley', 'link': 'https://arxiv.org/abs/2501.18296', 'abstract': 'Our aim in this paper is to outline how the design space for the ontologization process is richer than current practice would suggest. We point out that engineering processes as well as products need to be designed - and identify some components of the design. We investigate the possibility of designing a range of radically new practices, providing examples of the new practices from our work over the last three decades with an outlier methodology, bCLEARer. We also suggest that setting an evolutionary context for ontologization helps one to better understand the nature of these new practices and provides the conceptual scaffolding that shapes fertile processes. Where this evolutionary perspective positions digitalization (the evolutionary emergence of computing technologies) as the latest step in a long evolutionary trail of information transitions. This reframes ontologization as a strategic tool for leveraging the emerging opportunities offered by digitalization.', 'abstract_zh': '在这篇论文中，我们的目标是说明本体化进程的设计空间比当前实践所暗示的更为丰富。我们指出，不仅要设计工程产品，还需要设计工程过程，并指出了设计过程中的某些组成部分。我们探讨了设计一系列根本性新实践的可能性，提供了过去三十年中使用bCLEARer异常方法所获得的这些新实践的实例。同时，我们认为将本体化进程置于进化背景之下，有助于更好地理解这些新实践的本质，并提供结构化的概念框架，从而塑造富有成效的过程。从这种进化的视角来看，数字化的出现被视为信息过渡漫长进化轨迹中的最新一步。这重新定义了本体化进程作为一种战略工具，用于把握数字化带来的新兴机遇。', 'title_zh': '扩展本体化实践的设计空间：以bCLEARer为例'}
{'arxiv_id': 'arXiv:2501.18291', 'title': 'CueTip: An Interactive and Explainable Physics-aware Pool Assistant', 'authors': 'Sean Memery, Kevin Denamganai, Jiaxin Zhang, Zehai Tu, Yiwen Guo, Kartic Subr', 'link': 'https://arxiv.org/abs/2501.18291', 'abstract': "We present an interactive and explainable automated coaching assistant called CueTip for a variant of pool/billiards. CueTip's novelty lies in its combination of three features: a natural-language interface, an ability to perform contextual, physics-aware reasoning, and that its explanations are rooted in a set of predetermined guidelines developed by domain experts. We instrument a physics simulator so that it generates event traces in natural language alongside traditional state traces. Event traces lend themselves to interpretation by language models, which serve as the interface to our assistant. We design and train a neural adaptor that decouples tactical choices made by CueTip from its interactivity and explainability allowing it to be reconfigured to mimic any pool playing agent. Our experiments show that CueTip enables contextual query-based assistance and explanations while maintaining the strength of the agent in terms of win rate (improving it in some situations). The explanations generated by CueTip are physically-aware and grounded in the expert rules and are therefore more reliable.", 'abstract_zh': '我们提出了一种名为CueTip的交互式和可解释的自动辅导助手，用于台球变种游戏。CueTip的创新之处在于它集成了三项功能：自然语言接口、能够进行情境感知和物理推理的能力，以及其解释依据一套由领域专家制定的预设原则。我们对物理模拟器进行了扩展，使其不仅能生成传统状态轨迹，还能生成自然语言事件轨迹。事件轨迹便于语言模型进行解释，这些模型成为我们助手的用户界面。我们设计并训练了一个神经协处理器，将CueTip的战略选择与其交互性和可解释性分离，使其能够重新配置以模拟任何台球玩游戏代理。我们的实验表明，CueTip能够提供基于情境的查询辅助和解释，同时在胜率方面保持了代理的强大力量（在某些情况下有所提升）。由CueTip生成的解释具有物理意识，并基于专家规则，因此更为可靠。', 'title_zh': 'CueTip：一个互动且可解释的物理意识接球助手\n\n注释：在学术翻译中，通常会根据上下文对术语进行适当的调整，以确保译文的准确性与流畅性。这里的“CueTip”听起来像是系统或工具的名称，因此保持不变。其他部分根据内容进行了翻译，以便更符合中文的表达习惯。'}
{'arxiv_id': 'arXiv:2501.18202', 'title': 'On Scaling Neurosymbolic Programming through Guided Logical Inference', 'authors': 'Thomas Jean-Michel Valentin, Luisa Sophie Werner, Pierre Genevès, Nabil Layaïda', 'link': 'https://arxiv.org/abs/2501.18202', 'abstract': 'Probabilistic neurosymbolic learning seeks to integrate neural networks with symbolic programming. Many state-of-the-art systems rely on a reduction to the Probabilistic Weighted Model Counting Problem (PWMC), which requires computing a Boolean formula called the logical this http URL, PWMC is \\\\#P-hard, and the number of clauses in the logical provenance formula can grow exponentially, creating a major bottleneck that significantly limits the applicability of PNL solutions in this http URL propose a new approach centered around an exact algorithm DPNL, that enables bypassing the computation of the logical this http URL DPNL approach relies on the principles of an oracle and a recursive DPLL-like decomposition in order to guide and speed up logical this http URL, we show that this approach can be adapted for approximate reasoning with $\\epsilon$ or $(\\epsilon, \\delta)$ guarantees, called this http URL show significant performance this http URL enables scaling exact inference further, resulting in more accurate this http URL, ApproxDPNL shows potential for advancing the scalability of neurosymbolic programming by incorporating approximations even further, while simultaneously ensuring guarantees for the reasoning process.', 'abstract_zh': '概率神经符号学习旨在将神经网络与符号编程相结合。许多最先进的系统依赖于将问题减少为概率加权模型计数问题（PWMC），这要求计算一个布尔公式，称为逻辑起源公式 (logical provenance formula)。PWMC 是 \\#P 难问题，并且逻辑起源公式的子句数量可能会呈指数增长，从而成为主要瓶颈，极大地限制了 PNL 解决方案的应用。本文提出了一种新的方法，围绕一种精确算法 DPNL 中心展开，能够绕过逻辑起源公式的计算。DPNL 方法依赖于或acles 和一种递归的类似 DPLL 的分解原则，以便引导并加速逻辑起源公式的推理过程。我们展示了这种方法可以适应带有 \\(\\epsilon\\) 或 \\((\\epsilon, \\delta)\\) 保证的近似推理，称为 ApproxDPNL 方法。我们证明了这种方法在性能方面表现显著，能够进一步扩展精确推理的规模，从而提高推理结果的准确性。ApproxDPNL 方法展示了通过进一步引入近似，同时确保推理过程中的保证，来促进神经符号编程可扩展性的潜力。', 'title_zh': '通过引导逻辑推理扩展神经符号编程的研究'}
{'arxiv_id': 'arXiv:2501.18201', 'title': 'Neural Operator based Reinforcement Learning for Control of first-order PDEs with Spatially-Varying State Delay', 'authors': 'Jiaqi Hu, Jie Qi, Jing Zhang', 'link': 'https://arxiv.org/abs/2501.18201', 'abstract': 'Control of distributed parameter systems affected by delays is a challenging task, particularly when the delays depend on spatial variables. The idea of integrating analytical control theory with learning-based control within a unified control scheme is becoming increasingly promising and advantageous. In this paper, we address the problem of controlling an unstable first-order hyperbolic PDE with spatially-varying delays by combining PDE backstepping control strategies and deep reinforcement learning (RL). To eliminate the assumption on the delay function required for the backstepping design, we propose a soft actor-critic (SAC) architecture incorporating a DeepONet to approximate the backstepping controller. The DeepONet extracts features from the backstepping controller and feeds them into the policy network. In simulations, our algorithm outperforms the baseline SAC without prior backstepping knowledge and the analytical controller.', 'abstract_zh': '控制受时变延迟影响的分布参数系统是一个具有挑战性的问题，特别是在延迟依赖于空间变量的情况下。将分析控制理论与基于学习的控制方法在统一的控制方案中结合在一起的想法越来越具有前景和优势。本文通过结合分布参数方程（PDE）后退控制策略和深度强化学习（RL），解决了一个不稳定的具有空间变延迟的一阶双曲PDE的控制问题。为了避免后退设计所需的延迟函数假设，我们提出了一种结合DeepONet的软演员-评论家（SAC）架构，以近似后退控制器。DeepONet从后退控制器中提取特征，并将其馈送到策略网络。在仿真实验中，我们的算法在未先验了解后退控制知识以及分析控制器的情况下，表现优于基准SAC算法。', 'title_zh': '基于神经运算器的强化学习控制方法用于处理具有空间变化状态延迟的一阶偏微分方程'}
{'arxiv_id': 'arXiv:2501.18190', 'title': 'Economic Rationality under Specialization: Evidence of Decision Bias in AI Agents', 'authors': 'ShuiDe Wen, Juan Feng', 'link': 'https://arxiv.org/abs/2501.18190', 'abstract': "In the study by Chen et al. (2023) [01], the large language model GPT demonstrated economic rationality comparable to or exceeding the average human level in tasks such as budget allocation and risk preference. Building on this finding, this paper further incorporates specialized agents, such as biotechnology experts and economists, for a horizontal comparison to explore whether specialization can enhance or maintain economic rationality equivalent to that of GPT in similar decision-making scenarios. The results indicate that when agents invest more effort in specialized fields, their decision-making behavior is more prone to 'rationality shift,' specifically manifested as increased violations of GARP (Generalized Axiom of Revealed Preference), decreased CCEI (Critical Cost Efficiency Index), and more significant decision deviations under high-risk conditions. In contrast, GPT and more generalized basic agents maintain a more stable and consistent level of rationality across multiple tasks. This study reveals the inherent conflict between specialization and economic rationality, providing new insights for constructing AI decision-making systems that balance specialization and generalization across various scenarios.", 'abstract_zh': '在Chen等人的研究（2023年）[01]中，大型语言模型GPT在预算分配和风险偏好等任务中展示了与人类平均水平相当或更高的经济理性。在此基础上，本文进一步引入了专门领域的代理，如生物技术专家和经济学家，进行横向比较，探讨专业化是否能够在类似决策场景中增强或保持与GPT相当的经济理性。研究结果表明，当代理在专业领域投入更多努力时，其决策行为更容易出现“理性转变”，具体表现为GARP（广义揭示偏好公理）的违反增加、CCEI（关键成本效率指标）的下降，以及在高风险条件下决策偏差的加大。相比之下，GPT和更具通用性的基本代理在多种任务中展现出更稳定和一致的理性水平。本研究揭示了专业化与经济理性之间的固有冲突，为在各种场景中平衡专业化与通用性构建AI决策系统提供了新的见解。', 'title_zh': '专注于专业化的经济理性：AI代理决策偏差的实证证据'}
{'arxiv_id': 'arXiv:2501.18099', 'title': 'Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge', 'authors': 'Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang', 'link': 'https://arxiv.org/abs/2501.18099', 'abstract': 'LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-bystep reasoning process that underlies the final evaluation of a response. However, due to the lack of human annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.', 'abstract_zh': '大规模语言模型作为法官（LLM-as-a-Judge）模型生成包含推理过程的链式思考（CoT）序列，旨在捕捉最终回答评估背后的逐步推理过程。然而，由于缺乏经过人工标注的CoT进行评估，有效的推理痕迹所必需的成分和结构仍然研究不足。因此，以往的方法往往（1）限制推理痕迹到手设计的成分，如标准列表、参考答案或验证问题，（2）将规划与评估推理交织在一起。在本文中，我们提出了EvalPlanner，一种用于Thinking-LLM-as-a-Judge的偏好优化算法，该算法首先生成不加约束的评估计划，然后执行，最后得出最终判决。在自我训练循环中，EvalPlanner逐步优化合成构建的评估计划与执行，从而获得更好的最终裁决结果。尽管仅在较少的真实偏好配对和合成生成的偏好配对进行训练，我们的方法仍然在RewardBench上取得了新的最佳性能（得分为93.9）。此外，在其他基准测试，如RM-Bench、JudgeBench和FollowBenchEval的实验进一步突显了规划和推理在构建稳健的LLM-as-a-Judge推理模型中的作用。', 'title_zh': '学习规划与推理以 Thinking-LLM-as-a-Judge 进行评估'}
{'arxiv_id': 'arXiv:2501.18081', 'title': 'Normative Evaluation of Large Language Models with Everyday Moral Dilemmas', 'authors': 'Pratik S. Sachdeva, Tom van Nuenen', 'link': 'https://arxiv.org/abs/2501.18081', 'abstract': 'The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am I the Asshole" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven LLMs to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs\' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations.', 'abstract_zh': '大规模语言模型（LLMs）的快速普及已经推动了对它们编码道德规范和决策过程的广泛研究。这些研究中的许多都依赖于用调查式问题提示LLMs，以评估模型与特定人口群体、道德信念或政治意识形态的对齐程度。尽管这些方法具有信息价值，但它们对相对表面的构建物的依从性往往会简化日常生活中的道德困境的复杂性和细微之处。我们主张，沿更详细的人类互动轴审计LLMs对于更好地评估它们可能对人类信念和行为的影响至关重要。为了实现这一目标，我们评估了来自“Am I the Asshole”（AITA）社区（Reddit用户在此寻求其他社区成员对日常冲突的道德判断）的复杂且常见的道德困境中的LLMs。我们提示了七种LLMs为超过10,000个AITA道德困境分配责任并提供解释。然后我们将LLMs的判断和解释与 redditors 的判断以及彼此之间的判断进行了比较，旨在揭示它们在道德推理中的模式。我们的结果显示，大规模语言模型在处理AITA子网页上的道德判断时表现出不同的模式，与人类评价相差甚远。LLMs在自我一致性方面表现出中等到高的水平，但在模型之间的一致性较低。进一步分析模型解释揭示了模型在运用各种道德原则时存在的不同模式。这些发现突显了在人工系统中实现一致道德推理的复杂性，并强调了仔细评估不同模型在伦理判断方面的方法的必要性。随着LLMs在需要伦理决策角色，如治疗师和伴侣等方面的应用，需要仔细评估以减轻潜在的偏见和限制。', 'title_zh': '使用日常道德困境对大型语言模型进行规范性评估'}
{'arxiv_id': 'arXiv:2501.18009', 'title': 'Large Language Models Think Too Fast To Explore Effectively', 'authors': 'Lan Pan, Hanbo Xie, Robert C. Wilson', 'link': 'https://arxiv.org/abs/2501.18009', 'abstract': 'Large Language Models have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore, an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.', 'abstract_zh': '大语言模型已经展现出许多智能能力。尽管众多基准测试评估了它们的智能水平，但对于它们的探索能力关注相对较少，而探索能力是发现新信息和适应新环境的核心能力，无论是天然系统还是人工系统中均是如此。在开放性任务中，大语言模型能否有效探索仍不清楚。本研究利用《Little Alchemy 2》作为范式来探讨大语言模型在开放性任务中是否能超过人类在探索方面的表现。在这个任务中，智能体结合元素以发现新的元素。研究结果表明，大多数大语言模型的表现逊于人类，但o1模型是个例外。传统的大语言模型主要依赖不确定性驱动策略，而人类则在不确定性与赋能之间进行平衡。通过稀疏自编码器进行的表征分析显示，不确定性与选择在较早的变压器块中被代表，而赋能值则在较晚的阶段被处理，导致大语言模型思考过快，做出过早决策，从而阻碍了有效的探索。这些发现揭示了大语言模型探索的局限性，并提出了改善其适应性的方向。', 'title_zh': '大型语言模型思考过快，无法有效进行探索'}
{'arxiv_id': 'arXiv:2501.17991', 'title': 'Investigating the Monte-Carlo Tree Search Approach for the Job Shop Scheduling Problem', 'authors': 'Laurie Boveroux, Damien Ernst, Quentin Louveaux', 'link': 'https://arxiv.org/abs/2501.17991', 'abstract': 'The Job Shop Scheduling Problem (JSSP) is a well-known optimization problem in manufacturing, where the goal is to determine the optimal sequence of jobs across different machines to minimize a given objective. In this work, we focus on minimising the weighted sum of job completion times. We explore the potential of Monte Carlo Tree Search (MCTS), a heuristic-based reinforcement learning technique, to solve large-scale JSSPs, especially those with recirculation. We propose several Markov Decision Process (MDP) formulations to model the JSSP for the MCTS algorithm. In addition, we introduce a new synthetic benchmark derived from real manufacturing data, which captures the complexity of large, non-rectangular instances often encountered in practice. Our experimental results show that MCTS effectively produces good-quality solutions for large-scale JSSP instances, outperforming our constraint programming approach.', 'abstract_zh': '制造车间排序问题（JSSP）是制造领域中一个广为人知的优化问题，目标是在不同机器上确定作业的最佳顺序，以最小化给定的目标值。在本文中，我们专注于最小化作业完成时间的加权和。我们探讨了蒙特卡洛树搜索（MCTS）——一种基于启发式的强化学习技术——在解决大规模JSSP问题，尤其是在涉及循环作业的情况下——的潜在应用。我们提出了几种马尔可夫决策过程（MDP）模型来为MCTS算法建模JSSP。此外，我们引入了一个新的合成基准，该基准源自实际制造数据，并捕捉了实践中常见复杂的大规模非矩形实例的特点。我们的实验结果表明，MCTS能够有效地为大规模JSSP实例生成高质量的解决方案，并且在性能上优于我们的约束编程方法。', 'title_zh': '探究蒙特卡洛树搜索方法在作业车间调度问题中的应用'}
{'arxiv_id': 'arXiv:2501.17974', 'title': 'Think Smarter not Harder: Adaptive Reasoning with Inference Aware Optimization', 'authors': 'Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, Hao Ma, Han Fang', 'link': 'https://arxiv.org/abs/2501.17974', 'abstract': "Solving mathematics problems has been an intriguing capability of large language models, and many efforts have been made to improve reasoning by extending reasoning length, such as through self-correction and extensive long chain-of-thoughts. While promising in problem-solving, advanced long reasoning chain models exhibit an undesired single-modal behavior, where trivial questions require unnecessarily tedious long chains of thought. In this work, we propose a way to allow models to be aware of inference budgets by formulating it as utility maximization with respect to an inference budget constraint, hence naming our algorithm Inference Budget-Constrained Policy Optimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to ``understand'' the difficulty of queries and allocate inference budgets to harder ones. With different inference budgets, our best models are able to have a $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative improvement) on MATH500 using $2.16$x and $4.32$x inference budgets respectively, relative to LLaMA3.1 8B Instruct. These improvements are approximately $2$x those of self-consistency under the same budgets.", 'abstract_zh': '解决数学问题一直是大型语言模型的一种令人着迷的能力，许多努力已经致力于通过扩展推理长度来提升推理能力，例如通过自我修正和广泛的长链条逻辑推理。虽然在解决问题方面表现出色，但先进的长推理链模型却表现出一种不希望看到的单一模态行为，即简单的问题需要不必要的冗长链条的推理过程。在此项工作中，我们提出了一种方法，通过将推理预算视为约束下的效用最大化问题，从而使模型具备意识推理预算的能力，因此将我们的算法命名为推理预算约束策略优化（IBPO）。简而言之，通过IBPO微调的模型学会了“理解”问题的难度，并将推理预算分配给更难的问题。通过不同的推理预算，我们的最佳模型在使用2.16倍和4.32倍推理预算的情况下，相对于LLaMA3.1 8B Instruct，在MATH500上分别实现了4.14%和5.74%的绝对改进（相对改进分别为8.08%和11.2%）。这些改进大致是相同预算下自我一致性策略的两倍。', 'title_zh': '更聪明地思考，而不是更加努力地思考：基于推理意识的适应性推理优化'}
{'arxiv_id': 'arXiv:2501.18596', 'title': 'DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights', 'authors': 'Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz', 'link': 'https://arxiv.org/abs/2501.18596', 'abstract': 'We introduce DeltaLLM, a new post-training compression technique to reduce the memory footprint of LLMs. We propose an alternative way of structuring LLMs with weight sharing between layers in subsequent Transformer blocks, along with additional low-rank difference matrices between them. For training, we adopt the progressing module replacement method and show that the lightweight training of the low-rank modules with approximately 30M-40M tokens is sufficient to achieve performance on par with LLMs of comparable sizes trained from scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a 12% parameter reduction, retaining 90% of the performance of the base Llama and Phi models on common knowledge and reasoning benchmarks. Our method also outperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with the same number of parameters removed. For example, DeltaPhi 2.9B with a 24% reduction achieves similar average zero-shot accuracies as recovery fine-tuned SlicedPhi 3.3B with a 12% reduction, despite being approximately 400M parameters smaller with no fine-tuning applied. This work provides new insights into LLM architecture design and compression methods when storage space is critical.', 'abstract_zh': '我们提出了DeltaLLM，这是一种新的后训练压缩技术，用于减少大规模语言模型（LLM）的内存占用。我们提出了一种替代的LLM结构化方法，在后续的Transformer块之间实现权重共享，并且在它们之间附加了低秩差异矩阵。在训练过程中，我们采用了逐步模块替换方法，并展示出使用约30M-40M个令牌训练的低秩模块的轻量化训练足以达到与从头训练的相似大小的LLM相当的性能。我们发布了DeltaLLAMA和DeltaPHI两个模型，这两个模型的参数减少了12%，并且在常见的知识和推理基准测试中保留了基线Llama和Phi模型90%的性能。我们的方法在参数减少相同数量的情况下也优于JointDrop、LaCo、ShortGPT和SliceGPT等压缩技术。例如，DeltaPhi 2.9B参数减少24%，在未进行微调的情况下，其平均零样本准确性与恢复微调的SlicePhi 3.3B（参数减少12%）相当，尽管前者比后者大约小400M个参数。这项工作为在存储空间受限的情况下提供了关于LLM架构设计和压缩方法的新见解。', 'title_zh': 'DeltaLLM：通过共享权重之间的低秩增量压缩大语言模型'}
{'arxiv_id': 'arXiv:2501.18593', 'title': 'Diffusion Autoencoders are Scalable Image Tokenizers', 'authors': 'Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra', 'link': 'https://arxiv.org/abs/2501.18593', 'abstract': 'Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.', 'abstract_zh': '将图像拆分为紧凑的视觉表示是学习高效且高质量的图像生成模型的关键步骤。我们提出了一个简单的扩散分词器（DiTo），用于学习图像生成模型的紧凑视觉表示。我们的核心见解是，一个单一的学习目标——扩散L2损失——可以用于训练可扩展的图像分词器。由于扩散已经在图像生成中广泛使用，我们的见解极大地简化了训练此类分词器的过程。相比之下，当前最先进的分词器依赖于经验上发现的启发式和损失组合，因此需要复杂的训练配方，依赖于不同损失之间的非平凡平衡以及预训练的监督模型。我们展示了使DiTo能够扩展并学习竞争性图像表示的设计决策和理论依据。我们的结果表明，DiTo是一个更简单、可扩展且无需监督的图像分词器的替代方案，当前最先进的图像分词器是监督学习的。DiTo在图像重建和下游图像生成任务中取得了竞争力甚至更优的质量。', 'title_zh': '扩散自编码器是可扩展的图像词元化方法'}
{'arxiv_id': 'arXiv:2501.18592', 'title': 'Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models', 'authors': 'Hao Dong, Moru Liu, Kaiyang Zhou, Eleni Chatzi, Juho Kannala, Cyrill Stachniss, Olga Fink', 'link': 'https://arxiv.org/abs/2501.18592', 'abstract': 'In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segmentation. Besides, the recent advent of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired works leveraging these models to enhance adaptation and generalization performances or adapting them to downstream tasks. This survey provides the first comprehensive review of recent advances from traditional approaches to foundation models, covering: (1) Multimodal domain adaptation; (2) Multimodal test-time adaptation; (3) Multimodal domain generalization; (4) Domain adaptation and generalization with the help of multimodal foundation models; and (5) Adaptation of multimodal foundation models. For each topic, we formally define the problem and thoroughly review existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We maintain an active repository that contains up-to-date literature at this https URL.', 'abstract_zh': '在实际应用场景中，实现领域适应和泛化面临重大挑战，因为模型必须适应或泛化到未知的目标分布。将这些能力扩展到未见过的多模态分布，即多模态领域的适应和泛化，更是具有挑战性，因为不同模态具有独特的特征。多年来，已在行动识别、语义分割等领域取得了显著进展。此外，最近大规模预训练多模态基础模型的出现，如CLIP，启发了许多利用这些模型提升领域适应和泛化性能或将其适应到下游任务的工作。本文综述提供了一个关于从传统方法到基础模型的最新进展的全面回顾，涵盖了以下方面：（1）多模态领域适应；（2）多模态测试时适应；（3）多模态领域泛化；（4）借助多模态基础模型的领域适应和泛化；（5）多模态基础模型的适应。对每一主题，我们正式定义问题，并详细回顾现有方法。此外，我们分析相关数据集和应用，突出存在的挑战，并提出未来研究方向。我们保持一个活跃的仓库，其中包含最新的文献，地址为：https://this-url。', 'title_zh': '多模态适应与泛化的进展：从传统方法到基础模型'}
{'arxiv_id': 'arXiv:2501.18588', 'title': 'Inkspire: Supporting Design Exploration with Generative AI through Analogical Sketching', 'authors': 'David Chuan-En Lin, Hyeonsu B. Kang, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong', 'link': 'https://arxiv.org/abs/2501.18588', 'abstract': 'With recent advancements in the capabilities of Text-to-Image (T2I) AI models, product designers have begun experimenting with them in their work. However, T2I models struggle to interpret abstract language and the current user experience of T2I tools can induce design fixation rather than a more iterative, exploratory process. To address these challenges, we developed Inkspire, a sketch-driven tool that supports designers in prototyping product design concepts with analogical inspirations and a complete sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we conducted an exchange session with designers and distilled design goals for improving T2I interactions. In a within-subjects study comparing Inkspire to ControlNet, we found that Inkspire supported designers with more inspiration and exploration of design ideas, and improved aspects of the co-creative process by allowing designers to effectively grasp the current state of the AI to guide it towards novel design intentions.', 'abstract_zh': '随着文本到图像（T2I）AI模型能力的 recent 进步，产品设计师已经开始在工作中尝试使用这些模型。然而，T2I 模型在解读抽象语言方面存在困难，当前的 T2I 工具用户界面可能导致设计僵化而非更迭代和探索性的工作流程。为应对这些挑战，我们开发了 Inkspire，这是一种以素描为基础的工具，能够支持设计师在类比灵感的启发下，通过从素描到设计再到素描的完整反馈循环进行产品设计概念的原型制作。为了指导 Inkspire 的设计，我们与设计师进行了交流，并提炼出改进T2I互动的设计目标。在一个针对 Inkspire 与 ControlNet 进行的单被试比较研究中，我们发现 Inkspire 能够为设计师提供更多设计灵感和探索性，通过让设计师有效把握 AI 当前的状态来引导其朝向新颖的设计意图，从而改善了协同创意过程中的各个方面的表现。', 'title_zh': 'Inkspire：通过类比素描支持生成式AI的设计探索'}
{'arxiv_id': 'arXiv:2501.18578', 'title': 'R.I.P.: Better Models by Survival of the Fittest Prompts', 'authors': 'Ping Yu, Weizhe Yuan, Olga Golovneva, Tianhao Wu, Sainbayar Sukhbaatar, Jason Weston, Jing Xu', 'link': 'https://arxiv.org/abs/2501.18578', 'abstract': 'Training data quality is one of the most important drivers of final model quality. In this work, we introduce a method for evaluating data integrity based on the assumption that low-quality input prompts result in high variance and low quality responses. This is achieved by measuring the rejected response quality and the reward gap between the chosen and rejected preference pair. Our method, Rejecting Instruction Preferences (RIP) can be used to filter prompts from existing training sets, or to make high quality synthetic datasets, yielding large performance gains across various benchmarks compared to unfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win Rate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama 3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th place to 6th overall in the leaderboard.', 'abstract_zh': '训练数据质量是最终模型质量的重要驱动力之一。在本研究中，我们提出了一种基于低质量输入提示会导致高方差和低质量响应的假设来评估数据完整性的方法。这一方法通过测量拒绝响应的质量以及所选偏好与拒绝偏好之间的奖励差距来实现。我们提出的方法，即拒绝指令偏好（Rejecting Instruction Preferences, RIP），可用于从现有的训练集中过滤提示，或用于生成高质量的合成数据集，这在各种基准测试中相比未过滤的数据带来了巨大的性能提升。使用Llama 3.1-8B-Instruct，RIP使AlpacaEval2 LC胜率提高了9.4%，Arena-Hard提高了8.7%，WildBench提高了9.9%。使用Llama 3.3-70B-Instruct，RIP使Arena-Hard的得分从67.5提升到82.9，在排行榜上的名次从第18位升至第6位。', 'title_zh': 'R.I.P.: 通过适者生存的提示获得更好的模型'}
{'arxiv_id': 'arXiv:2501.18577', 'title': 'Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling', 'authors': 'Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates', 'link': 'https://arxiv.org/abs/2501.18577', 'abstract': 'Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.', 'abstract_zh': '机器学习模型越来越多地被用于生成预测，这些预测作为后续统计分析的输入数据。例如，基于卫星图像的计算机视觉模型对经济和环境指标的预测用于下游回归分析；同样，语言模型在社会科学的研究中广泛用于近似人类的评估和意见。然而，未能妥善考虑到机器学习预测中的误差会使标准的统计程序失效。此前的研究采用了一种我们称之为“预测-然后校正”估计法，在机器学习算法填补缺失变量时，能够提供有效的置信区间，假设有一个小的完整样本来自目标人群。我们通过引入适用于非均匀（即加权、分层或分群）样本以及任意特征子集被填补的场景下的自助置信区间，扩展了这一范围。重要的是，这种方法可以在许多场景下应用而不需要额外的计算。我们证明，这些置信区间在对机器学习模型的质量不做任何假设的情况下仍然是有效的，其宽度不超过未使用机器学习预测的其他方法所得到的置信区间。', 'title_zh': '基于预测增强推断的插补协变量与非均匀采样方法'}
{'arxiv_id': 'arXiv:2501.18565', 'title': 'BounTCHA: A CAPTCHA Utilizing Boundary Identification in AI-extended Videos', 'authors': 'Lehao Lin, Ke Wang, Maha Abdallah, Wei Cai', 'link': 'https://arxiv.org/abs/2501.18565', 'abstract': "In recent years, the rapid development of artificial intelligence (AI) especially multi-modal Large Language Models (MLLMs), has enabled it to understand text, images, videos, and other multimedia data, allowing AI systems to execute various tasks based on human-provided prompts. However, AI-powered bots have increasingly been able to bypass most existing CAPTCHA systems, posing significant security threats to web applications. This makes the design of new CAPTCHA mechanisms an urgent priority. We observe that humans are highly sensitive to shifts and abrupt changes in videos, while current AI systems still struggle to comprehend and respond to such situations effectively. Based on this observation, we design and implement BounTCHA, a CAPTCHA mechanism that leverages human perception of boundaries in video transitions and disruptions. By utilizing AI's capability to expand original videos with prompts, we introduce unexpected twists and changes to create a pipeline for generating short videos for CAPTCHA purposes. We develop a prototype and conduct experiments to collect data on humans' time biases in boundary identification. This data serves as a basis for distinguishing between human users and bots. Additionally, we perform a detailed security analysis of BounTCHA, demonstrating its resilience against various types of attacks. We hope that BounTCHA will act as a robust defense, safeguarding millions of web applications in the AI-driven era.", 'abstract_zh': '近年来，人工智能（AI）特别是多模态大型语言模型（MLLMs）的快速发展，使其能够理解和处理文本、图像、视频等多媒体数据，使AI系统能够根据人类提供的提示执行各种任务。然而，基于AI的机器人越来越多地能够绕过现有的大多数验证码系统，这对网络应用的安全构成了重大威胁。因此，设计新的验证码机制已成为当务之急。我们观察到，人类对视频切换和中断中的边界变化极为敏感，而当前的AI系统仍然难以有效理解并响应这些情况。基于这一观察，我们设计并实施了BounTCHA，这是一种利用人类对视频过渡和中断中边界的感知的验证码机制。通过利用AI扩展原始视频文件的能力，并根据提示引入意想不到的变化，我们构建了一个生成短视频以实现验证码目的的流水线。我们开发了一个原型，并进行了实验以收集人类在边界识别上的时间偏差数据。这些数据为区分人类用户和机器人提供了基础。此外，我们对BounTCHA进行了详细的安全分析，展示了其在不同类型攻击下的韧性。我们希望BounTCHA能够作为一种牢固的防御机制，保护AI驱动时代千万个网络应用的安全。', 'title_zh': 'BounTCHA：一种利用AI扩展视频边界识别的验证码'}
{'arxiv_id': 'arXiv:2501.18539', 'title': 'Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented LLM-based Retrieval Method', 'authors': 'Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth', 'link': 'https://arxiv.org/abs/2501.18539', 'abstract': "Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources. LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions. However, LLM's decomposition of questions is unaware of what data is available and how data is organized, often leading to a sub-optimal retrieval performance. Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval. While this provides one way of interacting with the data collection, agentic RAG's exploration of data is inefficient because successive queries depend on previous results rather than being guided by the organization of available data in the collection. To address this problem, we propose an LLM-based retrieval method -- ARM, that aims to better align the question with the organization of the data collection by exploring relationships among data objects beyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries. We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches.", 'abstract_zh': '现实世界中的开放领域问题可能相当复杂，尤其是在回答这些问题时需要从多个信息源中获取信息。大规模语言模型（LLMs）在将复杂任务分解为更简单步骤方面表现出了令人印象深刻的性能，之前的许多工作也利用这一点来提高复杂问题检索的支持效果。然而，LLM对问题的分解并没有意识到可用数据及其组织方式，常常会导致检索性能不佳。最近在代理型检索-回答框架（RAG）方面的工作提出了一种迭代的检索方式，其中后续查询是基于上一轮检索结果而产生的动作。虽然这种方法提供了一种与数据集交互的方式，但代理型RAG探索数据的方式效率较低，因为后续查询依赖于上一轮的结果而非数据集可用数据的组织方式。为了解决这一问题，我们提出了一种基于LLM的检索方法——ARM，该方法旨在通过探索数据对象之间的关系来更好地将问题与数据集的组织方式相匹配，从而为复杂问题提供一次性检索所有信息的解决方案。我们在两个数据集——Bird和OTT-QA上对ARM进行了评估。在Bird数据集上，ARM在执行准确性上比标准RAG（含查询分解）高出最多5.2个百分点，在与代理型RAG（ReAct）相比时高出最多15.9个百分点。在OTT-QA数据集上，ARM分别在F1匹配分数上比这些方法高出5.5个百分点和19.3个百分点。', 'title_zh': '当然可以。以下是翻译后的标题和内容，符合学术规范：\n\n标题：一次性检索一切可能吗？ARM：一种基于LLM的对齐导向检索方法\n\n内容摘要：本文探讨了一种基于大规模语言模型（LLM）的对齐导向检索方法，提出了一个问题：我们能否一次性检索到所有相关信息？提出的ARM方法旨在通过有效对齐查询与文档之间的关系，提高检索的准确性和效率。'}
{'arxiv_id': 'arXiv:2501.18535', 'title': 'A Hybrid Data-Driven Approach For Analyzing And Predicting Inpatient Length Of Stay In Health Centre', 'authors': 'Tasfia Noor Chowdhury, Sanjida Afrin Mou, Kazi Naimur Rahman', 'link': 'https://arxiv.org/abs/2501.18535', 'abstract': 'Patient length of stay (LoS) is a critical metric for evaluating the efficacy of hospital management. The primary objectives encompass to improve efficiency and reduce costs while enhancing patient outcomes and hospital capacity within the patient journey. By seamlessly merging data-driven techniques with simulation methodologies, the study proposes an all-encompassing framework for the optimization of patient flow. Using a comprehensive dataset of 2.3 million de-identified patient records, we analyzed demographics, diagnoses, treatments, services, costs, and charges with machine learning models (Decision Tree, Logistic Regression, Random Forest, Adaboost, LightGBM) and Python tools (Spark, AWS clusters, dimensionality reduction). Our model predicts patient length of stay (LoS) upon admission using supervised learning algorithms. This hybrid approach enables the identification of key factors influencing LoS, offering a robust framework for hospitals to streamline patient flow and resource utilization. The research focuses on patient flow, corroborating the efficacy of the approach, illustrating decreased patient length of stay within a real healthcare environment. The findings underscore the potential of hybrid data-driven models in transforming hospital management practices. This innovative methodology provides generally flexible decision-making, training, and patient flow enhancement; such a system could have huge implications for healthcare administration and overall satisfaction with healthcare.', 'abstract_zh': '以下是该论文内容或标题的中文翻译，符合学术规范：\n\n住院时间（LoS）是评估医院管理成效的关键指标。研究的主要目标是在提高效率、降低成本的同时，改善患者结果并提升医院的容量。通过无缝结合数据驱动技术和仿真方法，本研究提出了一套全面的框架以优化患者流动。利用包含230万份脱敏患者记录的全面数据集，我们使用机器学习模型（决策树、逻辑回归、随机森林、AdaBoost、LightGBM）和Python工具（Spark、AWS集群、降维）进行了分析，探讨了患者的 demographics、诊断、治疗、服务、成本和收费情况。我们的模型使用监督学习算法预测患者入院后的住院时间。该混合方法能够识别影响住院时间的关键因素，为医院优化患者流动和资源利用提供了一个坚固的框架。研究结果证实了该方法的有效性，并在实际医疗环境中展示了住院时间的减少。研究发现突显了混合数据驱动模型在变革医院管理实践方面的潜力。这一创新方法提供了一种灵活的决策机制、培训和患者流动提升方式；这样的系统可能对医疗保健管理产生重大影响，并提高患者对医疗服务的整体满意度。', 'title_zh': '一种综合数据驱动方法用于分析和预测健康中心住院患者住院天数'}
{'arxiv_id': 'arXiv:2501.18504', 'title': 'CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction', 'authors': 'Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa', 'link': 'https://arxiv.org/abs/2501.18504', 'abstract': 'Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.', 'abstract_zh': '大规模语言模型（LLM）图片识别是一种强大的工具，可用于从图像中提取数据，但其准确性取决于在提示中提供足够的线索，这需要特定领域的专家来完成特殊任务。本文介绍了结合进化计算的准确识别（Cue Learning using Evolution for Accurate Recognition, CLEAR）方法，该方法利用了一种组合L大型语言模型和进化计算的方法来生成和优化线索，从而提高图像中特有特征的识别能力。它通过自动生成一种特定领域的新型表示，然后使用遗传算法优化合适的文本线索来实现这一点。我们把CLEAR应用于从建筑物内外图像中识别可持续性数据的实际任务。我们研究了使用可变长度表示与固定长度表示相比的影响，并展示了通过从分类估计重构为实数值估计如何提高LLM的一致性。我们证明了CLEAR在所有任务中相较于专家人工识别和人工撰写的提示具有更高的准确性，错误率最多可降低两个数量级，并通过消融研究展示了解决方案的简洁性。', 'title_zh': 'CLEAR：面向可持续性数据提取的进化学习线索学习及其准确识别应用'}
{'arxiv_id': 'arXiv:2501.18501', 'title': 'Beyond Prior Limits: Addressing Distribution Misalignment in Particle Filtering', 'authors': 'Yiwei Shi, Jingyu Hu, Yu Zhang, Mengyue Yang, Weinan Zhang, Cunjia Liu, Weiru Liu', 'link': 'https://arxiv.org/abs/2501.18501', 'abstract': "Particle filtering is a Bayesian inference method and a fundamental tool in state estimation for dynamic systems, but its effectiveness is often limited by the constraints of the initial prior distribution, a phenomenon we define as the Prior Boundary Phenomenon. This challenge arises when target states lie outside the prior's support, rendering traditional particle filtering methods inadequate for accurate estimation. Although techniques like unbounded priors and larger particle sets have been proposed, they remain computationally prohibitive and lack adaptability in dynamic scenarios. To systematically overcome these limitations, we propose the Diffusion-Enhanced Particle Filtering Framework, which introduces three key innovations: adaptive diffusion through exploratory particles, entropy-driven regularisation to prevent weight collapse, and kernel-based perturbations for dynamic support expansion. These mechanisms collectively enable particle filtering to explore beyond prior boundaries, ensuring robust state estimation for out-of-boundary targets. Theoretical analysis and extensive experiments validate framework's effectiveness, indicating significant improvements in success rates and estimation accuracy across high-dimensional and non-convex scenarios.", 'abstract_zh': '粒子滤波是一种贝叶斯推理方法，也是动态系统状态估计的基本工具，但其效果往往受限于初始先验分布的约束，我们将其定义为先验边界现象。当目标状态位于先验分布的支持范围之外时，传统粒子滤波方法通常无法提供准确的估计。尽管提出了无界先验和更大规模的粒子集等技术，但这些方法在计算上仍然是不可行的，且在动态场景中缺乏适应性。为了系统性地克服这些限制，我们提出了增强扩散粒子滤波框架，该框架引入了三种关键创新：探索性粒子实现的自适应扩散、熵驱动的正则化以防止权重坍塌，以及基于核的扰动实现动态支持扩展。这些机制共同使粒子滤波能够探索超出先验边界的空间，确保在超出边界的目标状态下进行稳健的状态估计。理论分析和广泛的实验验证了该框架的有效性，表明在高维和非凸场景中，该框架在成功率和估计精度方面取得了显著改进。', 'title_zh': '超越先验限制：解决粒子滤波中的分布错配问题'}
{'arxiv_id': 'arXiv:2501.18492', 'title': 'GuardReasoner: Towards Reasoning-based LLM Safeguards', 'authors': 'Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi', 'link': 'https://arxiv.org/abs/2501.18492', 'abstract': 'As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : this https URL.', 'abstract_zh': '随着大型语言模型（LLMs）在安全关键应用中的影响日益增大，确保它们的安全性仍然是一个关键挑战。本文提出了一种新的大型语言模型安全措施GuardReasoner，通过引导保护模型学习推理能力来解决这一问题。具体而言，我们首先创建了GuardReasonerTrain数据集，该数据集包含127,000个样本和460,000个详细的推理步骤。然后，我们引入了推理样本微调（reasoning SFT），以解锁保护模型的推理能力。此外，我们还提出了困难样本DPO（Difficult Sample DPO）以进一步加强它们的推理能力。通过这种方式，GuardReasoner实现了更好的性能、可解释性和泛化能力。我们在13项基准测试中的3项保护措施任务中进行了广泛的实验和分析，展示了其优越性。尤为值得一提的是，GuardReasoner 8B在平均F1分数上分别超过了GPT-4o+CoT和LLaMA Guard 3 8B达5.74%和20.84%。我们已发布GuardReasoner的训练数据、代码以及不同规模（1B、3B、8B）的模型：[此链接]。\n\n请注意，[此链接]需要替换为实际的链接地址。', 'title_zh': 'GuardReasoner：基于推理的LLM安全保障研究'}
{'arxiv_id': 'arXiv:2501.18490', 'title': 'Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor', 'authors': 'Fausto Mauricio Lagos Suarez, Akshit Saradagi, Vidya Sumathy, Shruti Kotpaliwar, George Nikolakopoulos', 'link': 'https://arxiv.org/abs/2501.18490', 'abstract': "This article introduces a curriculum learning approach to develop a reinforcement learning-based robust stabilizing controller for a Quadrotor that meets predefined performance criteria. The learning objective is to achieve desired positions from random initial conditions while adhering to both transient and steady-state performance specifications. This objective is challenging for conventional one-stage end-to-end reinforcement learning, due to the strong coupling between position and orientation dynamics, the complexity in designing and tuning the reward function, and poor sample efficiency, which necessitates substantial computational resources and leads to extended convergence times. To address these challenges, this work decomposes the learning objective into a three-stage curriculum that incrementally increases task complexity. The curriculum begins with learning to achieve stable hovering from a fixed initial condition, followed by progressively introducing randomization in initial positions, orientations and velocities. A novel additive reward function is proposed, to incorporate transient and steady-state performance specifications. The results demonstrate that the Proximal Policy Optimization (PPO)-based curriculum learning approach, coupled with the proposed reward structure, achieves superior performance compared to a single-stage PPO-trained policy with the same reward function, while significantly reducing computational resource requirements and convergence time. The curriculum-trained policy's performance and robustness are thoroughly validated under random initial conditions and in the presence of disturbances.", 'abstract_zh': '本文介绍了基于课程学习的方法，用于开发一个基于强化学习的鲁棒稳定控制器，以满足预定义的性能标准。学习目标是在随机初始条件下实现期望的位置，并同时满足瞬态和稳态性能规范。对于传统的一站式端到端强化学习而言，这一目标具有挑战性，原因包括位置和姿态动力学之间的强耦合、奖励函数设计和调优的复杂性以及样本效率低，这需要大量的计算资源，并导致较长的收敛时间。为了应对这些挑战，本文将学习目标分解为三个逐步增加任务复杂性的阶段。课程学习始于从固定初始条件学习实现稳定的悬停，然后逐步引入随机初始位置、姿态和速度。提出了一个新颖的加性奖励函数，以纳入瞬态和稳态性能规范。结果表明，基于Proximal Policy Optimization (PPO)的课程学习方法，结合提出的奖励结构，在与相同奖励函数仅单阶段训练的PPO策略相比时，实现了更优的性能，同时减少了计算资源需求和收敛时间。课程训练的策略在随机初始条件和扰动存在的情况下，其性能和鲁棒性得到了全面验证。', 'title_zh': '基于课程的学习样本高效强化学习在四旋翼无人机鲁棒稳定控制中的应用'}
{'arxiv_id': 'arXiv:2501.18475', 'title': 'CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA Initialization', 'authors': 'Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin', 'link': 'https://arxiv.org/abs/2501.18475', 'abstract': 'Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has become a highly efficient approach for downstream tasks, particularly in scenarios with limited computational resources. However, applying LoRA techniques to quantized LLMs poses unique challenges due to the reduced representational precision of quantized weights. In this paper, we introduce CLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic initialization strategy designed to overcome these challenges. Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization. By leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and determines the optimal LoRA components for each layer, ensuring a strong foundation for subsequent fine-tuning. A key contribution of this work is a novel theoretical result that enables the accurate and closed-form construction of these optimal LoRA components. We validate the efficacy of CLoQ across multiple tasks such as language generation, arithmetic reasoning, and commonsense reasoning, demonstrating that it consistently outperforms existing LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit widths.', 'abstract_zh': '使用低秩适应（LoRA）微调大型语言模型（LLMs）已成为在计算资源受限场景下进行下游任务的一种高效方法。然而，将LoRA技术应用到量化的LLMs中会带来独特挑战，因为量化权重的表示精度较低。在这篇论文中，我们引入了CLoQ（Calibrated LoRA initialization for Quantized LLMs），这是一种简化的初始化策略，旨在克服这些挑战。我们的方法专注于在初始化过程中尽量减小原始LLM与其具有LoRA组件的量化版本之间的层间差异。通过利用一个小规模的校准数据集，CLoQ对预训练的LLM进行量化，并确定每个层的最佳LoRA组件，从而为后续微调奠定坚实的基础。本文的一个主要贡献是提出了一种新的理论结果，这使得能够准确且闭式构建这些最佳的LoRA组件。我们在语言生成、算术推理和常识推理等多个任务上验证了CLoQ的有效性，结果显示它在量化LLMs，特别是在超低位宽场景下，始终优于现有的LoRA微调方法。', 'title_zh': 'CLoQ：通过校准的LoRA初始化增强量化大语言模型的微调'}
{'arxiv_id': 'arXiv:2501.18468', 'title': 'Beyond Instructed Tasks: Recognizing In-the-Wild Reading Behaviors in the Classroom Using Eye Tracking', 'authors': 'Eduardo Davalos, Jorge Alberto Salas, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Abbey Gonzales, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin', 'link': 'https://arxiv.org/abs/2501.18468', 'abstract': 'Understanding reader behaviors such as skimming, deep reading, and scanning is essential for improving educational instruction. While prior eye-tracking studies have trained models to recognize reading behaviors, they often rely on instructed reading tasks, which can alter natural behaviors and limit the applicability of these findings to in-the-wild settings. Additionally, there is a lack of clear definitions for reading behavior archetypes in the literature. We conducted a classroom study to address these issues by collecting instructed and in-the-wild reading data. We developed a mixed-method framework, including a human-driven theoretical model, statistical analyses, and an AI classifier, to differentiate reading behaviors based on their velocity, density, and sequentiality. Our lightweight 2D CNN achieved an F1 score of 0.8 for behavior recognition, providing a robust approach for understanding in-the-wild reading. This work advances our ability to provide detailed behavioral insights to educators, supporting more targeted and effective assessment and instruction.', 'abstract_zh': '了解读者行为，如浏览、深读和扫描，对于改进教育指导至关重要。尽管先前的眼跟踪研究已经训练了模型来识别阅读行为，但这些研究通常依赖于受控阅读任务，这可能会改变自然行为并限制这些发现对真实世界环境的应用性。此外，文献中缺乏对阅读行为原型的清晰定义。我们通过开展课堂研究，收集了受控和非受控阅读数据，以解决这些问题。我们开发了一种混合方法框架，包括由人类驱动的理论模型、统计分析和AI分类器，以根据其速度、密度和顺序性区分不同的阅读行为。我们使用的轻量级2D CNN在行为识别上达到了0.8的F1分数，提供了一种稳健的方法来理解非受控阅读。这项工作增强了我们为教育者提供详细行为洞察的能力，从而支持更具针对性和有效的评估与教学。', 'title_zh': '超越指令性任务：利用眼动追踪识别教室中的野生阅读行为'}
{'arxiv_id': 'arXiv:2501.18452', 'title': 'Clustering Properties of Self-Supervised Learning', 'authors': 'Xi Weng, Jianing An, Xudong Ma, Binhang Qi, Jie Luo, Xi Yang, Jin Song Dong, Lei Huang', 'link': 'https://arxiv.org/abs/2501.18452', 'abstract': "Self-supervised learning (SSL) methods via joint embedding architectures have proven remarkably effective at capturing semantically rich representations with strong clustering properties, magically in the absence of label supervision. Despite this, few of them have explored leveraging these untapped properties to improve themselves. In this paper, we provide an evidence through various metrics that the encoder's output $encoding$ exhibits superior and more stable clustering properties compared to other components. Building on this insight, we propose a novel positive-feedback SSL method, termed Representation Soft Assignment (ReSA), which leverages the model's clustering properties to promote learning in a self-guided manner. Extensive experiments on standard SSL benchmarks reveal that models pretrained with ReSA outperform other state-of-the-art SSL methods by a significant margin. Finally, we analyze how ReSA facilitates better clustering properties, demonstrating that it effectively enhances clustering performance at both fine-grained and coarse-grained levels, shaping representations that are inherently more structured and semantically meaningful.", 'abstract_zh': '通过联合嵌入架构进行自我监督学习（Self-supervised learning, SSL）的方法已被证明在捕获语义丰富表示并在无标签监督的情况下表现出强大的聚类特性方面非常有效。尽管如此，很少有研究利用这些未充分利用的特性来改进SSL方法自身的性能。在本文中，我们通过多种度量提供了证据，证明编码器的输出encoding相较于其他组件具有更好的和更稳定的聚类特性。基于这一洞察，我们提出了一种新颖的正反馈SSL方法，称为表示软分配（Representation Soft Assignment, ReSA），该方法利用模型的聚类特性以自指导的方式促进学习。在标准SSL基准上的广泛实验显示，使用ReSA进行预训练的模型在性能上显著优于其他最先进的SSL方法。最后，我们分析了ReSA如何促进更好的聚类特性，证明了它有效地在细粒度和粗粒度层次上都增强了聚类性能，从而形成了更具结构化和语义意义的表示。', 'title_zh': '自监督学习的聚类特性'}
{'arxiv_id': 'arXiv:2501.18448', 'title': 'Autonomy and Safety Assurance in the Early Development of Robotics and Autonomous Systems', 'authors': 'Dhaminda B. Abeywickrama, Michael Fisher, Frederic Wheeler, Louise Dennis', 'link': 'https://arxiv.org/abs/2501.18448', 'abstract': 'This report provides an overview of the workshop titled Autonomy and Safety Assurance in the Early Development of Robotics and Autonomous Systems, hosted by the Centre for Robotic Autonomy in Demanding and Long-Lasting Environments (CRADLE) on September 2, 2024, at The University of Manchester, UK. The event brought together representatives from six regulatory and assurance bodies across diverse sectors to discuss challenges and evidence for ensuring the safety of autonomous and robotic systems, particularly autonomous inspection robots (AIR). The workshop featured six invited talks by the regulatory and assurance bodies. CRADLE aims to make assurance an integral part of engineering reliable, transparent, and trustworthy autonomous systems. Key discussions revolved around three research questions: (i) challenges in assuring safety for AIR; (ii) evidence for safety assurance; and (iii) how assurance cases need to differ for autonomous systems. Following the invited talks, the breakout groups further discussed the research questions using case studies from ground (rail), nuclear, underwater, and drone-based AIR. This workshop offered a valuable opportunity for representatives from industry, academia, and regulatory bodies to discuss challenges related to assured autonomy. Feedback from participants indicated a strong willingness to adopt a design-for-assurance process to ensure that robots are developed and verified to meet regulatory expectations.', 'abstract_zh': '本报告概述了由中国曼彻斯特大学的严苛和持续环境机器人自主中心（CRADLE）于2024年9月2日在英国曼彻斯特举办的题为“自主与安全保证在机器人和自主系统早期发展中的作用”的研讨会。本次活动汇集了来自六个不同行业监管和保障机构的代表，讨论确保自主和机器人系统安全的挑战和证据，特别是自主检测机器人（AIR）。研讨会邀请了六个监管和保障机构进行了专题发言。CRADLE 致力于将保障融入工程自主系统，使其可靠、透明且值得信赖。会议主要围绕以下三个研究问题进行了讨论：（i）确保AIR安全的挑战；（ii）安全保障的证据；（iii）自主系统需要如何有不同的保障案例。在专题发言之后，分组讨论继续用来自地面（铁路）、核能、水下和无人机基地的AIR案例研究，探讨上述研究问题。本次研讨会为来自工业界、学术界和监管机构的代表提供了讨论保障自主性的相关挑战的重要机会。参与者反馈显示，他们对采用一种以保障为导向的设计过程以确保机器人满足监管期望表现出强烈的意愿。', 'title_zh': '机器人与自主系统早期发展中自主性和安全性保障的研究'}
{'arxiv_id': 'arXiv:2501.18438', 'title': 'o3-mini vs DeepSeek-R1: Which One is Safer?', 'authors': 'Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura', 'link': 'https://arxiv.org/abs/2501.18438', 'abstract': "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this paper we conduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generate and execute a total of 1260 unsafe test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our evaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts whereas o3-mini only to 1.19%.", 'abstract_zh': 'DeepSeek-R1 的涌现标志着人工智能行业，尤其是大型语言模型（LLMs）领域的转折点。其能力在包括创造性思维、代码生成、数学以及自动化程序修复等多种任务中表现出色，并且似乎在执行成本方面较低。然而，大型语言模型必须遵守一个重要的质量特性，即与安全性和人类价值观的一致性。DeepSeek-R1 的一个明确竞争对手是其美国 counterpart，OpenAI 的 o3-mini 模型，该模型预计在性能、安全性和成本方面会设定很高的标准。在这篇论文中，我们系统评估了 DeepSeek-R1（70B 版本）和 OpenAI 的 o3-mini（beta 版本）的安全水平。为此，我们利用我们最近推出的自动化安全测试工具 ASTRAL。通过利用这一工具，我们自动且系统地生成并执行了共计 1260 个不安全的测试输入，对这两种模型进行了测试。在对这两种大型语言模型给出的结果进行半自动评估后，结果表明，与 OpenAI 的 o3-mini 相比，DeepSeek-R1 的安全性较差。根据我们的评估，DeepSeek-R1 对于执行的提示中有 11.98% 的回答是不安全的，而 o3-mini 则仅为 1.19%。', 'title_zh': 'O3-mini 与 DeepSeek-R1：哪一个更安全？'}
{'arxiv_id': 'arXiv:2501.18432', 'title': 'Solving Drone Routing Problems with Quantum Computing: A Hybrid Approach Combining Quantum Annealing and Gate-Based Paradigms', 'authors': 'Eneko Osaba, Pablo Miranda-Rodriguez, Andreas Oikonomakis, Matic Petrič, Sebastian Bock, Michail-Alexandros Kourtis', 'link': 'https://arxiv.org/abs/2501.18432', 'abstract': "This paper presents a novel hybrid approach to solving real-world drone routing problems by leveraging the capabilities of quantum computing. The proposed method, coined Quantum for Drone Routing (Q4DR), integrates the two most prominent paradigms in the field: quantum gate-based computing, through the Eclipse Qrisp programming language; and quantum annealers, by means of D-Wave System's devices. The algorithm is divided into two different phases: an initial clustering phase executed using a Quantum Approximate Optimization Algorithm (QAOA), and a routing phase employing quantum annealers. The efficacy of Q4DR is demonstrated through three use cases of increasing complexity, each incorporating real-world constraints such as asymmetric costs, forbidden paths, and itinerant charging points. This research contributes to the growing body of work in quantum optimization, showcasing the practical applications of quantum computing in logistics and route planning.", 'abstract_zh': '本文提出了一种利用量子计算能力解决实际无人机路径规划问题的新型混合方法。所提出的方法名为Quantum for Drone Routing (Q4DR)，它结合了量子计算领域最具代表性的两大范式：通过Eclipse Qrisp编程语言实现基于量子门的计算；以及通过D-Wave系统的设备利用量子退火器。该算法分为两个不同阶段：初始聚类阶段使用量子近似优化算法（QAOA）执行，以及采用量子退火器的路径规划阶段。Q4DR的有效性通过三个复杂度逐渐增加的实际用例得到证明，每个用例都包含了现实世界中的约束条件，如非对称成本、禁行路径和移动充电点。本文在量子优化研究领域做出了贡献，展示了量子计算在物流和路径规划中的实际应用。', 'title_zh': '使用量子计算解决无人机路由问题：结合量子退火和门模型范式的混合方法'}
{'arxiv_id': 'arXiv:2501.18426', 'title': 'Guaranteed confidence-band enclosures for PDE surrogates', 'authors': 'Ander Gray, Vignesh Gopakumar, Sylvain Rousseau, Sébastien Destercke', 'link': 'https://arxiv.org/abs/2501.18426', 'abstract': "We propose a method for obtaining statistically guaranteed confidence bands for functional machine learning techniques: surrogate models which map between function spaces, motivated by the need build reliable PDE emulators. The method constructs nested confidence sets on a low-dimensional representation (an SVD) of the surrogate model's prediction error, and then maps these sets to the prediction space using set-propagation techniques. The result are conformal-like coverage guaranteed prediction sets for functional surrogate models. We use zonotopes as basis of the set construction, due to their well studied set-propagation and verification properties. The method is model agnostic and can thus be applied to complex Sci-ML models, including Neural Operators, but also in simpler settings. We also elicit a technique to capture the truncation error of the SVD, ensuring the guarantees of the method.", 'abstract_zh': '我们提出了一种方法，用于获得统计保证的置信带，适用于映射函数空间之间关系的代理模型：这些代理模型受到建立可靠偏微分方程（PDE）模拟器需求的驱动。该方法通过在代理模型预测误差的低维表示（奇异值分解，SVD）上构建嵌套置信集，然后利用集合传播技术将这些集合作映射至预测空间。结果是针对功能代理模型具有类似容许覆盖的置信集。我们使用zonotope作为集合构建的基础，因为zonotope具有已经广泛研究的集合传播和验证特性。该方法对模型无偏见，因此可以应用于复杂的科学机器学习（Sci-ML）模型，包括神经算子，同时也适用于较为简单的情况。此外，我们还开发了一种技术来捕获SVD的截断误差，从而确保方法的保证性。', 'title_zh': '具有保障的PDE代理模型置信区间包络'}
{'arxiv_id': 'arXiv:2501.18403', 'title': 'Efficient Transformer for High Resolution Image Motion Deblurring', 'authors': 'Amanturdieva Akmaral, Muhammad Hamza Zafar', 'link': 'https://arxiv.org/abs/2501.18403', 'abstract': 'This paper presents a comprehensive study and improvement of the Restormer architecture for high-resolution image motion deblurring. We introduce architectural modifications that reduce model complexity by 18.4% while maintaining or improving performance through optimized attention mechanisms. Our enhanced training pipeline incorporates additional transformations including color jitter, Gaussian blur, and perspective transforms to improve model robustness as well as a new frequency loss term. Extensive experiments on the RealBlur-R, RealBlur-J, and Ultra-High-Definition Motion blurred (UHDM) datasets demonstrate the effectiveness of our approach. The improved architecture shows better convergence behavior and reduced training time while maintaining competitive performance across challenging scenarios. We also provide detailed ablation studies analyzing the impact of our modifications on model behavior and performance. Our results suggest that thoughtful architectural simplification combined with enhanced training strategies can yield more efficient yet equally capable models for motion deblurring tasks. Code and Data Available at: this https URL', 'abstract_zh': '本文对Restormer架构在高分辨率图像运动脱模糊中的应用进行了全面的研究与改进。我们引入了架构上的调整，通过优化的注意力机制减少了模型复杂度18.4%，同时保持或提升了性能。改进的训练管道包括额外的变换，如色差抖动、高斯模糊和透视变换，以提高模型的稳健性，并引入了一个新的频率损失项。我们在RealBlur-R、RealBlur-J和Ultra-High-Definition Motion Blurred (UHDM) 数据集上的广泛实验验证了该方法的有效性。改进的架构在保持竞争性能的同时，展示了更好的收敛行为和减少的训练时间。我们还提供了详细的消融研究，分析了这些修改对模型行为和性能的影响。结果表明，合理的架构简化与增强的训练策略相结合，可以生成更高效但同样能力强大的运动脱模糊模型。相关代码和数据可在以下链接获取：this https URL', 'title_zh': '高效Transformer在高分辨率图像运动去模糊中的应用'}
{'arxiv_id': 'arXiv:2501.18367', 'title': 'A Learnable Multi-views Contrastive Framework with Reconstruction Discrepancy for Medical Time-Series', 'authors': 'Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Cheng Jiang, Chenzhong Li', 'link': 'https://arxiv.org/abs/2501.18367', 'abstract': "In medical time series disease diagnosis, two key challenges are this http URL, the high annotation cost of medical data leads to overfitting in models trained on label-limited, single-center datasets. To address this, we propose incorporating external data from related tasks and leveraging AE-GAN to extract prior knowledge,providing valuable references for downstream tasks. Second, many existing studies employ contrastive learning to derive more generalized medical sequence representations for diagnostic tasks, usually relying on manually designed diverse positive and negative sample this http URL, these approaches are complex, lack generalizability, and fail to adaptively capture disease-specific features across different this http URL overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that integrates a multi-head attention mechanism and adaptively learns representations from different views through inter-view and intra-view contrastive learning this http URL, the pre-trained AE-GAN is used to reconstruct discrepancies in the target data as disease probabilities, which are then integrated into the contrastive learning this http URL on three target datasets demonstrate that our method consistently outperforms seven other baselines, highlighting its significant impact on healthcare applications such as the diagnosis of myocardial infarction, Alzheimer's disease, and Parkinson's disease.", 'abstract_zh': '在医学时间序列疾病诊断中，有两个关键挑战：首先，医学数据的标注成本高昂，导致在标签有限、单中心数据集上训练的模型容易过拟合。为解决这一问题，我们提出了一种将相关任务的外部数据纳入的方法，并利用AE-GAN提取先验知识，为下游任务提供了有价值的参考。其次，许多现有研究利用对比学习来提取更通用的医学序列表示，通常依赖于人工设计的多样化的正样本和负样本，这使得这些方法复杂、缺乏泛化能力，并且难以适应不同患者之间的疾病特异性特征。为克服这一问题，我们引入了LMCF（可学习多视图对比框架），该框架结合了多头注意力机制，并通过跨视图和内视图对比学习自适应地学习不同视图的表示。此外，预训练的AE-GAN用于重建目标数据中的差异作为疾病概率，这些差异随后被整合到对比学习中。在三个目标数据集上的实验表明，我们的方法在七个基线方法中表现始终更优，突显了其对医疗保健应用，如心肌梗死、阿尔茨海默病和帕金森病诊断的显著影响。', 'title_zh': '基于重建差异的可学习多视图对比框架在医学时间序列数据中的应用'}
{'arxiv_id': 'arXiv:2501.18356', 'title': 'State Stream Transformer (SST) : Emergent Metacognitive Behaviours Through Latent State Persistence', 'authors': 'Thea Aviss', 'link': 'https://arxiv.org/abs/2501.18356', 'abstract': "We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.", 'abstract_zh': '我们介绍了状态流变换器（State Stream Transformer，SST），这是一种新型的大规模语言模型（LLM）架构，能够通过解决传统变压器模型中的一个根本限制——即状态空间中自回归生成过程缺乏潜在的连续计算能力——揭示预训练权重中潜藏的推理行为和能力。SST 引入了一个带加权衰减的滑动窗口潜在状态（FFN）缓存，以维持和演化整个自回归生成过程中的持续潜在过程。\n\n通过使用相同的冻结权重进行受控实验，比较基本架构和SST架构，我们展示了这种架构改进本身就足以增强推理能力，这种能力的最佳解释可能是某种形式的潜在高阶处理，这是由新兴的元认知行为所证实的。这些行为在消除随机变异或已学习响应模式等混淆因素的设计条件下保持一致。潜在状态分布和处理动态的分析提供了证据，表明只有“状态流”才能解释这些现象。\n\n在定量评估中，SST 在两个推理基准测试上显著优于基础模型，在 GSM-8K（零样本）上达到 89.01% 的准确率，在 ARC 挑战赛（零样本链式推理 CoT）上达到 91.04% 的准确率。这些发现表明，潜在状态下持续计算能够促进根本不同的信息处理和内部推理策略，对人工智能系统理解具有深远影响。', 'title_zh': '状态流变换器（SST）：通过潜在状态持久性实现的 emergent 元认知行为'}
{'arxiv_id': 'arXiv:2501.18344', 'title': 'Transfer Learning of Surrogate Models: Integrating Domain Warping and Affine Transformations', 'authors': 'Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang', 'link': 'https://arxiv.org/abs/2501.18344', 'abstract': 'Surrogate models provide efficient alternatives to computationally demanding real-world processes but often require large datasets for effective training. A promising solution to this limitation is the transfer of pre-trained surrogate models to new tasks. Previous studies have investigated the transfer of differentiable and non-differentiable surrogate models, typically assuming an affine transformation between the source and target functions. This paper extends previous research by addressing a broader range of transformations, including linear and nonlinear variations. Specifically, we consider the combination of an unknown input warping, such as one modelled by the beta cumulative distribution function, with an unspecified affine transformation. Our approach achieves transfer learning by employing a limited number of data points from the target task to optimize these transformations, minimizing empirical loss on the transfer dataset. We validate the proposed method on the widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world transfer learning task from the automobile industry. The results underscore the significant advantages of the approach, revealing that the transferred surrogate significantly outperforms both the original surrogate and the one built from scratch using the transfer dataset, particularly in data-scarce scenarios.', 'abstract_zh': '代理模型提供了与计算密集型真实世界过程相比更高效的替代方案，但通常需要大量数据进行有效的训练。解决这一限制的一种有前景的方法是将预训练的代理模型转移到新的任务中。以往的研究已经探讨了可微代理模型和非可微代理模型的迁移，通常假设源函数和目标函数之间存在仿射变换。本论文通过考虑更广泛的变换范围扩展了先前的研究，包括线性和非线性变换。具体来说，我们考虑了未知输入变换的组合，如由Beta累积分布函数建模的变换，以及未指定的仿射变换。我们通过利用目标任务的少量数据点来优化这些变换，从而在传输数据集上最小化经验损失，实现迁移学习。我们使用广泛使用的黑箱优化基准（BBOB）测试床和来自汽车工业的实际迁移学习任务，验证了所提出的方法。结果表明，该方法具有显著优势，显示转移后的代理模型不仅优于原始代理模型，而且在利用传输数据集从零构建的代理模型中表现更优，特别是在数据稀缺场景中。', 'title_zh': '代理模型的迁移学习：集成领域扭曲和仿射变换'}
{'arxiv_id': 'arXiv:2501.18337', 'title': 'Unfaithful Probability Distributions in Binary Triple of Causality Directed Acyclic Graph', 'authors': 'Jingwei Liu', 'link': 'https://arxiv.org/abs/2501.18337', 'abstract': 'Faithfulness is the foundation of probability distribution and graph in causal discovery and causal inference. In this paper, several unfaithful probability distribution examples are constructed in three--vertices binary causality directed acyclic graph (DAG) structure, which are not faithful to causal DAGs described in J.M.,Robins,et al. Uniform consistency in causal inference. Biometrika (2003),90(3): 491--515. And the general unfaithful probability distribution with multiple independence and conditional independence in binary triple causal DAG is given.', 'abstract_zh': '忠诚性是概率分布和因果图中的基础。在本文中，我们在三节点二元因果有向无环图（DAG）结构下构建了几个不忠实的概率分布示例，这些示例并不符合J.M. Robins等人在《Biometrika》（2003年，第90卷第3期，第491-515页）中描述的因果DAG。同时，我们给出了二元三节点因果DAG中具有多种独立性和条件独立性的普遍不忠实概率分布。', 'title_zh': '不忠实的概率分布在因果有向无环图的二元 Triple 中'}
{'arxiv_id': 'arXiv:2501.18328', 'title': 'CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized Codes', 'authors': 'Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai', 'link': 'https://arxiv.org/abs/2501.18328', 'abstract': 'MRI imputation aims to synthesize the missing modality from one or more available ones, which is highly desirable since it reduces scanning costs and delivers comprehensive MRI information to enhance clinical diagnosis. In this paper, we propose a unified model, CodeBrain, designed to adapt to various brain MRI imputation scenarios. The core design lies in casting various inter-modality transformations as a full-modality code prediction task. To this end, CodeBrain is trained in two stages: Reconstruction and Code Prediction. First, in the Reconstruction stage, we reconstruct each MRI modality, which is mapped into a shared latent space followed by a scalar quantization. Since such quantization is lossy and the code is low dimensional, another MRI modality belonging to the same subject is randomly selected to generate common features to supplement the code and boost the target reconstruction. In the second stage, we train another encoder by a customized grading loss to predict the full-modality codes from randomly masked MRI samples, supervised by the corresponding quantized codes generated from the first stage. In this way, the inter-modality transformation is achieved by mapping the instance-specific codes in a finite scalar space. We evaluated the proposed CodeBrain model on two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments demonstrate that our CodeBrain model achieves superior imputation performance compared to four existing methods, establishing a new state of the art for unified brain MRI imputation. Codes will be released.', 'abstract_zh': '磁共振成像（MRI）插补旨在从一个或多个可用模态中合成缺失的模态。这一方法非常理想，因为它能降低扫描成本并提供全面的MRI信息，从而增强临床诊断。本文提出了一种统一模型，CodeBrain，该模型旨在适应各种脑MRI插补场景。核心设计在于将不同模态之间的变换转化为全模态代码预测任务。为此，CodeBrain在两个阶段进行训练：重构和代码预测。首先，在重构阶段，我们重构每个MRI模态，并将其映射到一个共享的潜在空间后进行标量量化。由于这种量化是损失性的且代码是低维度的，因此从同一受试者中随机选择另一个MRI模态来生成共有的特征，这些特征用于补充代码并增强目标重构。在第二个阶段，我们通过一个定制化的评分损失训练另一个编码器，从随机遮掩的MRI样本中预测全模态代码，该编码器由第一阶段生成的相应量化代码监督。这样一来，通过将实例特定的代码映射到有限的标量空间，实现了不同模态之间的变换。我们在两个公开的脑MRI数据集（即IXI和BraTS 2023）上评估了所提出的CodeBrain模型。广泛实验表明，我们的CodeBrain模型在插补性能方面优于四种现有方法，建立了统一脑MRI插补的新基准。代码将公开发布。', 'title_zh': 'CodeBrain：基于实例特定标量量化代码的脑MRI插补方法'}
{'arxiv_id': 'arXiv:2501.18310', 'title': 'Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis', 'authors': 'Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao', 'link': 'https://arxiv.org/abs/2501.18310', 'abstract': 'The synergy between deep learning models and traditional automation tools plays a pivotal role in developing robust neural theorem provers (NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when the model explicitly calls the method, or only at a single granularity level, failing to fully exploit the power of built-in tactics and off-the-shelf automated theorem provers. In this work, we propose ProofAug, a novel theorem proving method that enjoys superior sample efficiency through equipping proof-generation LLMs with automation methods in different granularities via fine-grained structure analysis of model-generated proof proposals. Furthermore, ProofAug serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F-test benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version), setting a new SOTA across all proof languages with a total sample budget of only 2100. Our code is available at this https URL.', 'abstract_zh': '深度学习模型与传统自动化工具之间的协同作用在开发 robust 的神经定理证明器（Neural Theorem Provers, NTPs）中起着关键作用。然而，在使用大型语言模型（LLMs）进行证明合成时，先前的工作要么仅在模型显式调用方法时应用自动化工具，要么仅在单一粒度级别应用，未能充分利用内置战术和现成定理证明器的能力。本文提出了一种名为 ProofAug 的新颖定理证明方法，该方法通过细粒度结构分析模型生成的证明提案来装备不同粒度级别的自动化方法，从而实现卓越的样本效率。此外，ProofAug 可以作为一个多功能的即插即用模块，无缝集成到任何树搜索算法中，使我们能够构建一个高效的递归证明（Efficient Recursive Proving, ERP）模块，进一步增强性能。我们在 miniF2F-test 评估基准上使用开源的 deepseek-math-7b-base 模型和 Isabelle 证明助手验证了该方法的优越性。通过采用混合提示策略，并经过数据集润色后，我们实现了 66.0% 的累积通过率（原始版本为 61.9%），在所有证明语言中设定了新的 SOTA 记录，样本预算仅为 2100 个样本。我们的代码可在 https://github.com/yourrepo 这里获取。', 'title_zh': '通过精细证明结构分析实现高效的神经定理证明'}
{'arxiv_id': 'arXiv:2501.18294', 'title': 'A Comprehensive Analysis on Machine Learning based Methods for Lung Cancer Level Classification', 'authors': 'Shayli Farshchiha, Salman Asoudeh, Maryam Shavali Kuhshuri, Mehrshad Eisaeid, Mohamadreza Azadie, Saba Hesaraki', 'link': 'https://arxiv.org/abs/2501.18294', 'abstract': 'Lung cancer is a major issue in worldwide public health, requiring early diagnosis using stable techniques. This work begins a thorough investigation of the use of machine learning (ML) methods for precise classification of lung cancer stages. A cautious analysis is performed to overcome overfitting issues in model performance, taking into account minimum child weight and learning rate. A set of machine learning (ML) models including XGBoost (XGB), LGBM, Adaboost, Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), CatBoost, and k-Nearest Neighbor (k-NN) are run methodically and contrasted. Furthermore, the correlation between features and targets is examined using the deep neural network (DNN) model and thus their capability in detecting complex patternsis established. It is argued that several ML models can be capable of classifying lung cancer stages with great accuracy. In spite of the complexity of DNN architectures, traditional ML models like XGBoost, LGBM, and Logistic Regression excel with superior performance. The models perform better than the others in lung cancer prediction on the complete set of comparative metrics like accuracy, precision, recall, and F-1 score', 'abstract_zh': '肺癌是全球公共卫生中的重大问题，要求使用稳定的技术尽早进行诊断。本研究旨在全面探索机器学习（ML）方法在肺癌分期精准分类中的应用。通过对模型性能中的过拟合问题进行谨慎分析，考虑最低子权重和学习率等因素，进行了模型测试和对比。运行并对比了一组包括XGBoost (XGB)、LGBM、Adaboost、Logistic Regression (LR)、Decision Tree (DT)、Random Forest (RF)、CatBoost和k-最近邻（k-NN）在内的机器学习模型。此外，通过深度神经网络（DNN）模型分析特征与目标之间的关联，探讨其在检测复杂模式方面的能力。研究认为，多种机器学习模型能够以很高的准确度进行肺癌分期分类。尽管深度神经网络架构复杂，但传统机器学习模型如XGBoost、LGBM和Logistic Regression在性能上表现出色。这些模型在肺癌预测方面的准确率、精确率、召回率和F-1分数等全面对比指标上均优于其他模型。', 'title_zh': '基于机器学习的方法对肺癌分级的综合分析'}
{'arxiv_id': 'arXiv:2501.18287', 'title': 'Mining for Species, Locations, Habitats, and Ecosystems from Scientific Papers in Invasion Biology: A Large-Scale Exploratory Study with Large Language Models', 'authors': "Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari", 'link': 'https://arxiv.org/abs/2501.18287', 'abstract': 'This paper presents an exploratory study that harnesses the capabilities of large language models (LLMs) to mine key ecological entities from invasion biology literature. Specifically, we focus on extracting species names, their locations, associated habitats, and ecosystems, information that is critical for understanding species spread, predicting future invasions, and informing conservation efforts. Traditional text mining approaches often struggle with the complexity of ecological terminology and the subtle linguistic patterns found in these texts. By applying general-purpose LLMs without domain-specific fine-tuning, we uncover both the promise and limitations of using these models for ecological entity extraction. In doing so, this study lays the groundwork for more advanced, automated knowledge extraction tools that can aid researchers and practitioners in understanding and managing biological invasions.', 'abstract_zh': '本文提出了一项探索性研究，利用大型语言模型（LLM）的能力从入侵生物学文献中挖掘关键生态实体。具体而言，我们专注于提取物种名称、它们的地理位置、相关栖息地和生态系统的信息，这些信息对于理解物种扩散、预测未来入侵以及指导保护工作至关重要。传统文本挖掘方法往往难以应对生态术语的复杂性和文本中微妙的语言模式。通过应用通用目的的大型语言模型而不进行特定领域的微调，我们揭示了这些模型在生态实体提取中的潜力和局限性。这项研究为开发更高级的自动化知识提取工具奠定了基础，这些工具可以帮助研究者和从业者更好地理解和管理生物入侵问题。', 'title_zh': '利用大型语言模型进行入侵生物学文献中物种、地点、栖息地和生态系统的大规模探索性研究'}
{'arxiv_id': 'arXiv:2501.18280', 'title': "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models", 'authors': 'Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang', 'link': 'https://arxiv.org/abs/2501.18280', 'abstract': 'The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.', 'abstract_zh': '近年来，大型语言模型（LLMs）的安全问题引起了广泛关注，各种防御机制被开发出来以防止产生有害输出，其中基于文本嵌入模型的安全保障措施构成了基本的防御手段。通过测试，我们发现文本嵌入模型的输出分布存在显著偏差，且均值较大。受这一观察的启发，我们提出了有效的方法来搜索能够攻击文本嵌入模型的通用魔法词。这些通用魔法词作为后缀，可以将任意文本的嵌入向偏差方向移动，从而操控任意文本对的相似性并误导安全保障措施。通过对用户提示附加魔法词并要求LLMs以魔法词结束答案，攻击者可以突破这些安全保障。为消除这一安全风险，我们还提出了无需训练的防御机制，可以在不进行训练的情况下校正文本嵌入的偏差分布。', 'title_zh': '使用通用 magic words 突破 LLMs 的安全防护以用于文本嵌入模型'}
{'arxiv_id': 'arXiv:2501.18271', 'title': 'Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks', 'authors': 'Hao-Zhe Tan, Zhi Zhou, Lan-Zhe Guo, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2501.18271', 'abstract': 'Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: \\emph{model labeling}, which assigns labels to each VLM to describe their specialty and utility; \\emph{model selection}, which matches the requirements of the target task with model labels; and \\emph{model reuse}, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.', 'abstract_zh': '预训练的视觉-语言模型（VLMs）在各种视觉任务中越来越受欢迎，并且已经释放出了多种开源的VLM变体。然而，选择最适合特定下游任务的预训练VLM仍然是一个挑战，因为没有一种VLM能够在所有下游任务中都表现出色，而且由于时间限制和数据限制，评估所有可用的VLM是不可能的。为解决这一问题，本文提出了一种新的范式，称为模型标签学习（Model Label Learning, MLL），以选择和重复使用VLM来处理下游任务。此提案包含三个关键模块：**模型标签化**，将标签分配给每个VLM以描述其特色和实用性；**模型选择**，将目标任务的要求与模型标签相匹配；以及**模型重复使用**，以集成的方式应用所选的VLM来解决目标任务。该提案具有高度的计算效率和扩展性，因为模型标签化过程是与目标任务无关的，并且随着候选模型数量的增加，能力可以得到增强。此外，我们还引入了一种新的基准测试用于评估VLM选择方法，包括49种VLM和17个目标任务数据集。实验结果清楚地展示了所提方法在选择和重复使用VLM方面的有效性。', 'title_zh': '预训练视觉-语言模型的选择与重用用于下游任务'}
{'arxiv_id': 'arXiv:2501.18270', 'title': 'The iToBoS dataset: skin region images extracted from 3D total body photographs for lesion detection', 'authors': "Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, Séraphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szabó, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia", 'link': 'https://arxiv.org/abs/2501.18270', 'abstract': 'Artificial intelligence has significantly advanced skin cancer diagnosis by enabling rapid and accurate detection of malignant lesions. In this domain, most publicly available image datasets consist of single, isolated skin lesions positioned at the center of the image. While these lesion-centric datasets have been fundamental for developing diagnostic algorithms, they lack the context of the surrounding skin, which is critical for improving lesion detection. The iToBoS dataset was created to address this challenge. It includes 16,954 images of skin regions from 100 participants, captured using 3D total body photography. Each image roughly corresponds to a $7 \\times 9$ cm section of skin with all suspicious lesions annotated using bounding boxes. Additionally, the dataset provides metadata such as anatomical location, age group, and sun damage score for each image. This dataset aims to facilitate training and benchmarking of algorithms, with the goal of enabling early detection of skin cancer and deployment of this technology in non-clinical environments.', 'abstract_zh': '人工智能在皮肤癌诊断方面取得了显著进步，通过实现恶性病变的快速和准确检测。在这一领域，大多数公开可用的图像数据集由单个、孤立的皮肤病变组成，这些病变位于图像中心。尽管这些以病变为中心的数据集对于开发诊断算法至关重要，但它们缺乏周围皮肤的上下文信息，这对于提高病变检测的准确性至关重要。为了应对这一挑战，iToBoS数据集被创建。该数据集包含100名参与者的16,954张皮肤区域图像，使用三维全身摄影技术拍摄。每张图像大致对应一块约7×9厘米的皮肤区域，并使用边界框标注所有可疑病变。此外，该数据集还提供了每张图像的解剖位置、年龄组和日晒损伤分数等元数据。该数据集旨在促进算法的训练和基准测试，目标是实现皮肤癌的早期检测，并将这项技术部署在非临床环境。', 'title_zh': 'iToBoS数据集：来源于全身三维照片的皮肤区域图像，用于病变检测'}
{'arxiv_id': 'arXiv:2501.18269', 'title': 'MAMS: Model-Agnostic Module Selection Framework for Video Captioning', 'authors': 'Sangho Lee, Il Yong Chun, Hogun Park', 'link': 'https://arxiv.org/abs/2501.18269', 'abstract': 'Multi-modal transformers are rapidly gaining attention in video captioning tasks. Existing multi-modal video captioning methods typically extract a fixed number of frames, which raises critical challenges. When a limited number of frames are extracted, important frames with essential information for caption generation may be missed. Conversely, extracting an excessive number of frames includes consecutive frames, potentially causing redundancy in visual tokens extracted from consecutive video frames. To extract an appropriate number of frames for each video, this paper proposes the first model-agnostic module selection framework in video captioning that has two main functions: (1) selecting a caption generation module with an appropriate size based on visual tokens extracted from video frames, and (2) constructing subsets of visual tokens for the selected caption generation module. Furthermore, we propose a new adaptive attention masking scheme that enhances attention on important visual tokens. Our experiments on three different benchmark datasets demonstrate that the proposed framework significantly improves the performance of three recent video captioning models.', 'abstract_zh': '多模态变压器在视频字幕任务中正迅速获得关注。现有的多模态视频字幕方法通常提取固定数量的帧，这引发了关键性的挑战。当提取帧的数量有限时，可能会遗漏包含用于字幕生成关键信息的重要帧。相反，提取过多的帧会导致连续帧的冗余问题，从而从连续的视频帧中提取出视觉标记的冗余。为了解决每个视频提取适当数量帧的问题，本文提出了一种新的模型无关的模块选择框架，该框架具有两个主要功能：(1) 根据从视频帧中提取的视觉标记选择一个合适的字幕生成模块；(2) 为所选的字幕生成模块构建视觉标记的子集。此外，我们提出了一种新的自适应注意力掩码方案，以增强对重要视觉标记的关注。我们在三个不同的基准数据集上的实验表明，所提出的框架显著提升了三种近期视频字幕模型的性能。', 'title_zh': 'MAMS：视频字幕生成的模型无关模块选择框架'}
{'arxiv_id': 'arXiv:2501.18258', 'title': 'PDE-DKL: PDE-constrained deep kernel learning in high dimensionality', 'authors': 'Weihao Yan, Christoph Brune, Mengwu Guo', 'link': 'https://arxiv.org/abs/2501.18258', 'abstract': 'Many physics-informed machine learning methods for PDE-based problems rely on Gaussian processes (GPs) or neural networks (NNs). However, both face limitations when data are scarce and the dimensionality is high. Although GPs are known for their robust uncertainty quantification in low-dimensional settings, their computational complexity becomes prohibitive as the dimensionality increases. In contrast, while conventional NNs can accommodate high-dimensional input, they often require extensive training data and do not offer uncertainty quantification. To address these challenges, we propose a PDE-constrained Deep Kernel Learning (PDE-DKL) framework that combines DL and GPs under explicit PDE constraints. Specifically, NNs learn a low-dimensional latent representation of the high-dimensional PDE problem, reducing the complexity of the problem. GPs then perform kernel regression subject to the governing PDEs, ensuring accurate solutions and principled uncertainty quantification, even when available data are limited. This synergy unifies the strengths of both NNs and GPs, yielding high accuracy, robust uncertainty estimates, and computational efficiency for high-dimensional PDEs. Numerical experiments demonstrate that PDE-DKL achieves high accuracy with reduced data requirements. They highlight its potential as a practical, reliable, and scalable solver for complex PDE-based applications in science and engineering.', 'abstract_zh': '许多基于偏微分方程（PDE）问题的物理信息机器学习方法依赖于高斯过程（GPs）或神经网络（NNs）。然而，这两种方法在数据稀缺和维度较高时都面临限制。尽管高斯过程在低维设置中以其稳健的不确定性量化而闻名，但随着维度的增加，其计算复杂性变得难以承受。相反，虽然传统的神经网络可以适应高维输入，但它们通常需要大量的训练数据，并且不提供不确定性量化。为了应对这些挑战，我们提出了一种结合深度学习和高斯过程的PDE约束深度核学习（PDE-DKL）框架，该框架在显式的PDE约束下结合了DL和GPs。具体而言，神经网络学习高维PDE问题的低维潜在表示，从而减少问题的复杂性。然后，高斯过程在满足支配PDE的情况下进行核回归，确保即使在可用数据有限的情况下也能给出准确的解和稳健的不确定性量化。这种结合统一了神经网络和高斯过程的优点，为高维PDE提供高精度、稳健的不确定性估计和计算效率。数值实验表明，PDE-DKL在减少数据需求的情况下能够实现高精度。它们突显了其作为科学和工程中复杂PDE应用实用、可靠且可扩展的求解器的潜力。', 'title_zh': 'PDE-DKL：高维约束下的偏微分方程深度核学习'}
{'arxiv_id': 'arXiv:2501.18237', 'title': 'Arbitrary Data as Images: Fusion of Patient Data Across Modalities and Irregular Intervals with Vision Transformers', 'authors': 'Malte Tölle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt', 'link': 'https://arxiv.org/abs/2501.18237', 'abstract': "A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status. These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic interventions such as medication administration, and images. While physicians are able to process and integrate diverse modalities intuitively, neural networks need specific modeling for each modality complicating the training procedure. We demonstrate that this complexity can be significantly reduced by visualizing all information as images along with unstructured text and subsequently training a conventional vision-text transformer. Our approach, Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not only simplifies data preprocessing and modeling but also outperforms current state-of-the-art methods in predicting in-hospital mortality and phenotyping, as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities include patient's clinical measurements, medications, X-ray images, and electrocardiography scans. We hope our work inspires advancements in multi-modal medical AI by reducing the training complexity to (visual) prompt engineering, thus lowering entry barriers and enabling no-code solutions for training. The source code will be made publicly available.", 'abstract_zh': '患者在每次住院期间会接受多次检查，每次检查都会提供不同的健康状况方面。这些评估包括不同采样率的时序数据、离散的单点测量、治疗干预措施（如药物给药），以及影像学检查等。虽然医生能够直观地处理和综合这些不同的模态信息，但神经网络需要针对每个模态进行特定建模，这使得训练过程变得复杂。我们通过将所有信息可视化为图像，并结合非结构化文本，然后训练一个常规的视觉-文本转换器，来证明这一复杂性可以显著降低。我们的方法，不规则采样多模态测量的视觉变换器（ViTiMM），不仅简化了数据预处理和建模过程，还在MIMIC-IV数据集上的6175名患者中验证了其在预测院内死亡率和疾病表型方面的性能，超过当前最先进的方法。这些模态包括患者的临床测量、药物、X射线影像和心电图扫描。我们希望我们的工作能够推动多模态医学AI的发展，将训练复杂性降低到（视觉）提示工程，从而降低入门门槛并使无代码解决方案成为可能。源代码将公开提供。', 'title_zh': '任意格式数据的图像化融合：基于视觉变换器的不规则时间间隔多模态患者数据融合'}
{'arxiv_id': 'arXiv:2501.18223', 'title': 'Exploring Large Protein Language Models in Constrained Evaluation Scenarios within the FLIP Benchmark', 'authors': 'Manuel F. Mollon, Joaquin Gonzalez-Rodriguez, Alicia Lozano-Diez, Daniel Ramos, Doroteo T. Toledano', 'link': 'https://arxiv.org/abs/2501.18223', 'abstract': 'In this study, we expand upon the FLIP benchmark-designed for evaluating protein fitness prediction models in small, specialized prediction tasks-by assessing the performance of state-of-the-art large protein language models, including ESM-2 and SaProt on the FLIP dataset. Unlike larger, more diverse benchmarks such as ProteinGym, which cover a broad spectrum of tasks, FLIP focuses on constrained settings where data availability is limited. This makes it an ideal framework to evaluate model performance in scenarios with scarce task-specific data. We investigate whether recent advances in protein language models lead to significant improvements in such settings. Our findings provide valuable insights into the performance of large-scale models in specialized protein prediction tasks.', 'abstract_zh': '在本研究中，我们扩展了FLIP基准，该基准最初设计用于评估蛋白质适应性预测模型在小型、专门预测任务中的性能，并通过评估目前最先进的大型蛋白质语言模型（包括ESM-2和SaProt）在FLIP数据集上的性能来检验这一基准。与ProteinGym等更大的、涵盖更广泛任务的基准不同，FLIP专注于数据可用性有限的受限环境。这使得FLIP成为评估在稀缺任务特定数据场景中模型性能的理想框架。我们研究了近期在蛋白质语言模型方面的进展是否能够在这些受限环境中带来显著改进。我们的发现提供了关于大型模型在专门蛋白质预测任务中的性能的宝贵见解。', 'title_zh': '在FLIP基准测试中探索受约束评估场景下的大型蛋白质语言模型'}
{'arxiv_id': 'arXiv:2501.18199', 'title': 'HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation', 'authors': 'Grzegorz Dudek, Tomasz Rodak', 'link': 'https://arxiv.org/abs/2501.18199', 'abstract': 'This paper introduces the Hierarchical Kolmogorov-Arnold Network (HKAN), a novel network architecture that offers a competitive alternative to the recently proposed Kolmogorov-Arnold Network (KAN). Unlike KAN, which relies on backpropagation, HKAN adopts a randomized learning approach, where the parameters of its basis functions are fixed, and linear aggregations are optimized using least-squares regression. HKAN utilizes a hierarchical multi-stacking framework, with each layer refining the predictions from the previous one by solving a series of linear regression problems. This non-iterative training method simplifies computation and eliminates sensitivity to local minima in the loss function. Empirical results show that HKAN delivers comparable, if not superior, accuracy and stability relative to KAN across various regression tasks, while also providing insights into variable importance. The proposed approach seamlessly integrates theoretical insights with practical applications, presenting a robust and efficient alternative for neural network modeling.', 'abstract_zh': '本文介绍了层次柯尔莫哥罗夫-阿诺尔德网络（Hierarchical Kolmogorov-Arnold Network，简称HKAN），这是一种新颖的网络架构，为近期提出的柯尔莫哥罗夫-阿诺尔德网络（Kolmogorov-Arnold Network，简称KAN）提供了具有竞争力的替代方案。与依赖反向传播的KAN不同，HKAN采用了随机化学习方法，其基函数参数固定，线性聚合通过最小二乘回归进行优化。HKAN利用了一种分层多堆叠框架，在每一层中，通过一系列线性回归问题细化从前一层得到的预测。这种方法是非迭代的，简化了计算，并且消除了在损失函数局部最小值处的敏感性。实证结果表明，在各种回归任务中，HKAN在准确性和稳定性方面与KAN相当，甚至更优，同时提供了有关变量重要性的见解。所提出的方法将理论洞察与实际应用无缝结合，为神经网络建模提供了稳健而高效的替代方案。', 'title_zh': 'HKAN：层级科尔莫哥洛夫-阿诺尔德网络无需反向传播'}
{'arxiv_id': 'arXiv:2501.18187', 'title': 'In-Context Learning of Polynomial Kernel Regression in Transformers with GLU Layers', 'authors': 'Haoyuan Sun, Ali Jadbabaie, Navid Azizan', 'link': 'https://arxiv.org/abs/2501.18187', 'abstract': 'Transformer-based models have demonstrated remarkable ability in in-context learning (ICL), where they can adapt to unseen tasks from a prompt with a few examples, without requiring parameter updates. Recent research has provided insight into how linear Transformers can perform ICL by implementing gradient descent estimators. In particular, it has been shown that the optimal linear self-attention (LSA) mechanism can implement one step of gradient descent with respect to a linear least-squares objective when trained on random linear regression tasks.\nHowever, the theoretical understanding of ICL for nonlinear function classes remains limited. In this work, we address this gap by first showing that LSA is inherently restricted to solving linear least-squares objectives and thus, the solutions in prior works cannot readily extend to nonlinear ICL tasks. To overcome this limitation, drawing inspiration from modern architectures, we study a mechanism that combines LSA with GLU-like feed-forward layers and show that this allows the model to perform one step of gradient descent on a polynomial kernel regression. Further, we characterize the scaling behavior of the resulting Transformer model, highlighting the necessary model size to effectively handle quadratic ICL tasks. Our findings highlight the distinct roles of attention and feed-forward layers in nonlinear ICL and identify key challenges when extending ICL to nonlinear function classes.', 'abstract_zh': '基于Transformer的模型在上下文学习（ICL）中展现出了显著的能力，即它们可以通过实例提示调适应未知任务，而无需更新参数。最近的研究揭示了线性Transformer如何通过梯度下降估计器实现ICL的机制。具体来说，已经证明，在随机线性回归任务上训练时，最优的线性自注意力（LSA）机制能够实现线性最小二乘目标的一次梯度下降步骤。\n\n然而，对于非线性函数类的ICL理论理解仍然有限。本研究通过首先证明LSA本质上受限于解决线性最小二乘问题，从而指出先前工作中的解决方案难以直接扩展到非线性ICL任务。为克服这一限制，我们借鉴现代架构，研究了一种将LSA与类似GLU的前馈层结合的机制，并展示了这种方法允许模型在多项式核回归任务上执行一次梯度下降步。进一步地，我们分析了生成的Transformer模型的扩展行为，强调了有效处理二次ICL任务所需的模型规模。我们的研究结果突显了注意机制与前馈层在非线性ICL中扮演的不同角色，并指出了将ICL扩展到非线性函数类时的关键挑战。', 'title_zh': '基于GLU层的Transformer中多项式核回归的上下文学习'}
{'arxiv_id': 'arXiv:2501.18137', 'title': 'Tensor Completion for Surrogate Modeling of Material Property Prediction', 'authors': 'Shaan Pakala, Dawon Ahn, Evangelos Papalexakis', 'link': 'https://arxiv.org/abs/2501.18137', 'abstract': "When designing materials to optimize certain properties, there are often many possible configurations of designs that need to be explored. For example, the materials' composition of elements will affect properties such as strength or conductivity, which are necessary to know when developing new materials. Exploring all combinations of elements to find optimal materials becomes very time consuming, especially when there are more design variables. For this reason, there is growing interest in using machine learning (ML) to predict a material's properties. In this work, we model the optimization of certain material properties as a tensor completion problem, to leverage the structure of our datasets and navigate the vast number of combinations of material configurations. Across a variety of material property prediction tasks, our experiments show tensor completion methods achieving 10-20% decreased error compared with baseline ML models such as GradientBoosting and Multilayer Perceptron (MLP), while maintaining similar training speed.", 'abstract_zh': '在设计材料以优化某些性质时，通常需要探索许多可能的设计配置。例如，元素组成会影响材料的强度或导电性等性质，这些都是开发新材料时必须了解的。探索所有元素组合以找到最佳材料会变得非常耗时，尤其是在设计变量更多的情况下。因此，越来越多的研究人员开始使用机器学习（ML）来预测材料的性质。在本研究中，我们将某些材料性质的优化问题建模为张量补全问题，以便利用我们数据集的结构，并导航数量庞大的材料配置组合。在多种材料性质预测任务中，我们的实验表明，张量补全方法相比基础的机器学习模型（如GradientBoosting和多层感知机MLP），在错误率上降低了10-20%，同时保持了类似的速度。', 'title_zh': '用于材料性质预测的代理模型中的张量完成'}
{'arxiv_id': 'arXiv:2501.18131', 'title': 'Entropy-Synchronized Neural Hashing for Unsupervised Ransomware Detection', 'authors': 'Peter Idliman, Wilfred Balfour, Benedict Featheringham, Hugo Chesterfield', 'link': 'https://arxiv.org/abs/2501.18131', 'abstract': 'Entropy-based detection methodologies have gained significant attention due to their ability to analyze structural irregularities within executable files, particularly in the identification of malicious software employing advanced obfuscation techniques. The Entropy-Synchronized Neural Hashing (ESNH) framework introduces a novel approach that leverages entropy-driven hash representations to classify software binaries based on their underlying entropy characteristics. Through the synchronization of entropy profiles with neural network architectures, the model generates robust and unique hash values that maintain stability even when faced with polymorphic and metamorphic transformations. Comparative analysis against traditional detection approaches revealed superior performance in identifying novel threats, reducing false-positive rates, and achieving consistent classification across diverse ransomware families. The incorporation of a self-regulating hash convergence mechanism further ensured that entropy-synchronized hashes remained invariant across executions, minimizing classification inconsistencies that often arise due to dynamic modifications in ransomware payloads. Experimental results demonstrated high detection rates across contemporary ransomware strains, with the model exhibiting resilience against encryption-based evasion mechanisms, code injection strategies, and reflective loading techniques. Unlike conventional detection mechanisms that rely on static signatures and heuristic analysis, the proposed entropy-aware classification framework adapts to emerging threats through an inherent ability to capture entropy anomalies within executable structures. The findings reinforce the potential of entropy-based detection in addressing the limitations of traditional methodologies while enhancing detection robustness against obfuscation and adversarial evasion techniques.', 'abstract_zh': '基于熵的检测方法因其能够分析可执行文件中的结构性异常，特别是在识别采用高级混淆技术的恶意软件方面的能力，而备受关注。Entropy-Synchronized Neural Hashing（ESNH）框架引入了一种新颖的方法，利用熵驱动的哈希表示来根据软件二进制文件的基础熵特性进行分类。通过将熵谱与神经网络结构同步，该模型生成了稳健且独特的哈希值，即使在遭遇多态性和 metamorphic 变形时也能保持稳定性。与传统的检测方法相比，该方法在识别新型威胁、降低误报率以及在不同勒索软件家族中实现一致分类方面表现出更优的性能。通过引入一种自我调节的哈希收敛机制，进一步确保了熵同步哈希在不同执行过程中的不变性，从而减少了由勒索软件载荷中的动态修改引起分类不一致的问题。实验结果表明，该模型在当前的勒索软件变种中具有高检测率，同时对基于加密的规避机制、代码注入策略和反射加载技术具有鲁棒性。与依赖静态签名和启发式分析的传统检测机制不同，所提出的熵意识分类框架能够通过固有的能力捕捉可执行文件结构中的熵异常来适应新兴威胁。研究结果强化了熵基检测在解决传统方法局限性的同时，增强对混淆和对抗规避技术的检测鲁棒性的潜力。', 'title_zh': '熵同步神经哈希在无监督恶意软件检测中的应用'}
{'arxiv_id': 'arXiv:2501.18129', 'title': 'Revisiting gender bias research in bibliometrics: Standardizing methodological variability using Scholarly Data Analysis (SoDA) Cards', 'authors': 'HaeJin Lee, Shubhanshu Mishra, Apratim Mishra, Zhiwen You, Jinseok Kim, Jana Diesner', 'link': 'https://arxiv.org/abs/2501.18129', 'abstract': 'Gender biases in scholarly metrics remain a persistent concern, despite numerous bibliometric studies exploring their presence and absence across productivity, impact, acknowledgment, and self-citations. However, methodological inconsistencies, particularly in author name disambiguation and gender identification, limit the reliability and comparability of these studies, potentially perpetuating misperceptions and hindering effective interventions. A review of 70 relevant publications over the past 12 years reveals a wide range of approaches, from name-based and manual searches to more algorithmic and gold-standard methods, with no clear consensus on best practices. This variability, compounded by challenges such as accurately disambiguating Asian names and managing unassigned gender labels, underscores the urgent need for standardized and robust methodologies. To address this critical gap, we propose the development and implementation of ``Scholarly Data Analysis (SoDA) Cards." These cards will provide a structured framework for documenting and reporting key methodological choices in scholarly data analysis, including author name disambiguation and gender identification procedures. By promoting transparency and reproducibility, SoDA Cards will facilitate more accurate comparisons and aggregations of research findings, ultimately supporting evidence-informed policymaking and enabling the longitudinal tracking of analytical approaches in the study of gender and other social biases in academia.', 'abstract_zh': '尽管有许多文献计量研究探讨了性别偏见在学术指标中的存在与否，从生产力、影响、认可度到自我引用等各个方面，性别偏见依然是一个持续存在的关注点。然而，方法论的一致性问题，尤其是在作者名称去重和性别识别方面的不一致，限制了这些研究的可靠性和可比性，有可能导致误解并阻碍有效的干预措施。过去12年中对70篇相关文献的回顾表明，研究方法从基于名称的手动搜索到更复杂的算法和标准方法不等，没有形成一致的最佳实践共识。这种多样性和诸如准确去重亚洲姓名以及处理未指认的性别标签等挑战相结合，强调了标准化和稳健方法的迫切需要。为解决这一关键缺口，我们提出开发并实施“学术数据分析卡片（SoDA Cards）”。这些卡片将为记录和报告学术数据分析中的关键方法选择提供结构化的框架，包括作者名称去重和性别识别程序。通过促进透明度和可再现性，SoDA Cards将有助于更准确地比较和汇总研究成果，最终支持基于证据的政策制定，并有助于对性别和社会偏见等学术研究进行长期追踪。', 'title_zh': '重新审视引文计量中的性别偏见研究：使用学术数据分析（SoDA）卡片标准化方法学变异性'}
{'arxiv_id': 'arXiv:2501.18128', 'title': 'Unraveling the Capabilities of Language Models in News Summarization', 'authors': 'Abdurrahman Odabaşı, Göksel Biricik', 'link': 'https://arxiv.org/abs/2501.18128', 'abstract': "Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.", 'abstract_zh': '近年来，多种语言模型的出现及不断改进的自然语言处理任务需求，尤其是摘要任务，本研究提供了对20个近期语言模型的全面基准测试，重点关注用于新闻摘要的小型模型。在本研究中，我们系统地测试了这些模型在不同风格的新闻文章文本上进行总结的能力，并在三个不同的数据集上进行测试。具体而言，本研究主要集中在零样本学习和少样本学习环境中，并采用了结合自动评估指标、人工评估和LLM作为评估者的稳健评估方法。有趣的是，在少样本学习环境中加入演示示例并没有提高模型的表现，在某些情况下甚至导致生成的摘要质量下降。这一问题主要源于作为参考摘要的黄金摘要质量不佳，这对模型的表现产生了负面影响。此外，本研究的结果还强调了GPT-3.5-Turbo和GPT-4的出色表现，它们通常占据优势地位，这主要是由于其先进的功能。然而，在评估的开源模型中，某些模型如Qwen1.5-7B、SOLAR-10.7B-Instruct-v1.0、Meta-Llama-3-8B和Zephyr-7B-Beta也展示了令人鼓舞的结果。这些模型表现出显著的潜力，使它们成为可用于新闻摘要任务的大模型的竞争替代品。', 'title_zh': '剖析语言模型在新闻总结中的能力'}
{'arxiv_id': 'arXiv:2501.18124', 'title': 'REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via Multimodal Visual Feature Learning', 'authors': 'Liangjing Shao, Benshuang Chen, Shuting Zhao, Xinrong Chen', 'link': 'https://arxiv.org/abs/2501.18124', 'abstract': 'Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here: \\href{this https URL}{this http URL}', 'abstract_zh': '内窥镜的实时自我运动跟踪是实现内窥镜高效导航和自动化控制的一个重要任务。本文提出了一种新的框架，用于进行内窥镜的实时自我运动跟踪。首先，提出了一种多模态视觉特征学习网络，用于进行相对姿态预测，在其中从光学流中提取运动特征，从场景特征中提取场景特征，并从两个相邻观测中提取联合特征，用于预测。由于连接图像通道维度中包含更多的相关性信息，基于注意机制设计了一种新的特征提取器，以整合连续两帧的连接信息的多维度信息。为了从融合特征中提取更完整的特征表示，提出了一种新的姿态解码器，用于预测框架末端连接特征图的姿态变换。最后，基于相对姿态计算内窥镜的绝对姿态。实验在各种内窥镜场景的三个数据集中进行，结果表明，所提出的方法优于现有最先进的方法。此外，所提出方法的推理速度超过每秒30帧，满足实时要求。项目页面在此：\\href{this https URL}{this http URL}', 'title_zh': 'REMOTE：通过多模态视觉特征学习实现各种内窥镜的实时自我运动跟踪'}
{'arxiv_id': 'arXiv:2501.18122', 'title': 'VQLTI: Long-Term Tropical Cyclone Intensity Forecasting with Physical Constraints', 'authors': 'Xinyu Wang, Lei Liu, Kang Chen, Tao Han, Bin Li, Lei Bai', 'link': 'https://arxiv.org/abs/2501.18122', 'abstract': 'Tropical cyclone (TC) intensity forecasting is crucial for early disaster warning and emergency decision-making. Numerous researchers have explored deep-learning methods to address computational and post-processing issues in operational forecasting. Regrettably, they exhibit subpar long-term forecasting capabilities. We use two strategies to enhance long-term forecasting. (1) By enhancing the matching between TC intensity and spatial information, we can improve long-term forecasting performance. (2) Incorporating physical knowledge and physical constraints can help mitigate the accumulation of forecasting errors. To achieve the above strategies, we propose the VQLTI framework. VQLTI transfers the TC intensity information to a discrete latent space while retaining the spatial information differences, using large-scale spatial meteorological data as conditions. Furthermore, we leverage the forecast from the weather prediction model FengWu to provide additional physical knowledge for VQLTI. Additionally, we calculate the potential intensity (PI) to impose physical constraints on the latent variables. In the global long-term TC intensity forecasting, VQLTI achieves state-of-the-art results for the 24h to 120h, with the MSW (Maximum Sustained Wind) forecast error reduced by 35.65%-42.51% compared to ECMWF-IFS.', 'abstract_zh': '热带气旋（TC）强度预报对于早期灾害预警和应急决策至关重要。众多研究者探索了深度学习方法以解决运营预报中的计算和后处理问题。然而，这些方法在长期预报中表现不佳。为了提高长期预报能力，我们采用了两种策略：（1）通过增强TC强度与空间信息的匹配，可以提升长期预报性能；（2）结合物理知识和物理约束有助于减少预报误差的累积。为实现上述策略，我们提出了VQLTI框架。VQLTI将TC强度信息转换至离散潜在空间，同时保留空间信息差异，使用大规模空间气象数据作为条件。此外，我们利用天气预报模型 FengWu 提供的物理知识补充 VQLTI，并计算潜在强度（PI）以对潜在变量施加物理约束。在全球TC强度长期预报中，VQLTI 在24小时至120小时的预报中达到了最先进的结果，与 ECMWF-IFS 相比，最大持续风速（MSW）的预报误差减少了35.65%-42.51%。', 'title_zh': 'VQLTI：基于物理约束的长期热带气旋强度预测'}
{'arxiv_id': 'arXiv:2501.18119', 'title': 'Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models', 'authors': 'Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, Mengling Feng', 'link': 'https://arxiv.org/abs/2501.18119', 'abstract': 'Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.', 'abstract_zh': '由于知识图谱（KG）结构与自然语言之间存在天然的差距，有效整合KG的整体结构信息与大型语言模型（LLMs）已成为一个重要问题。为此，我们提出了一种两阶段框架，用于学习和应用量化代码，旨在无缝地将KG与LLMs集成。首先，提出了一种自我监督的量化表示（SSQR）方法，将KG的结构和语义知识压缩为与语言句子格式相匹配的离散代码（即，令牌）。我们进一步设计了KG指令跟随数据，将学习到的代码视为特征直接输入到LLMs中，从而实现无缝集成。实验结果表明，SSQR在区分代码方面优于现有的无监督量化方法。此外，微调后的LLaMA2和LLaMA3.1在KG链接预测和三元组分类任务中也表现更优，仅使用每个实体16个令牌，而传统的提示方法则需要成千上万个令牌。', 'title_zh': '自我监督量化表示：无缝集成知识图与大型语言模型'}
{'arxiv_id': 'arXiv:2501.18108', 'title': 'Investigating an Intelligent System to Monitor \\& Explain Abnormal Activity Patterns of Older Adults', 'authors': 'Min Hun Lee, Daniel P. Siewiorek, Alexandre Bernardino', 'link': 'https://arxiv.org/abs/2501.18108', 'abstract': 'Despite the growing potential of older adult care technologies, the adoption of these technologies remains challenging. In this work, we conducted a focus-group session with family caregivers to scope designs of the older adult care technology. We then developed a high-fidelity prototype and conducted its qualitative study with professional caregivers and older adults to understand their perspectives on the system functionalities. This system monitors abnormal activity patterns of older adults using wireless motion sensors and machine learning models and supports interactive dialogue responses to explain abnormal activity patterns of older adults to caregivers and allow older adults proactively sharing their status with caregivers for an adequate intervention. Both older adults and professional caregivers appreciated that our system can provide a faster, personalized service while proactively controlling what information is to be shared through interactive dialogue responses. We further discuss other considerations to realize older adult technology in practice.', 'abstract_zh': '尽管老年照护技术的发展潜力日益增大，但这些技术的应用仍面临诸多挑战。本研究通过与家庭护理人员进行焦点小组讨论，初步探讨了老年照护技术的设计。随后，我们开发了一个高保真原型，并通过向专业护理人员和老年人进行定性研究，以了解他们对该系统的功能看法。该系统利用无线运动传感器和机器学习模型监测老年人的异常活动模式，并支持交互式对话响应，以解释老年人的异常活动模式给护理人员，并允许老年人主动向护理人员分享其状态，以便进行适当的干预。老年受试者和专业护理人员均认为，该系统可以提供更快、更个性化的服务，并通过交互式对话响应主动控制要分享的信息。此外，我们还讨论了在实践中实现老年人照护技术时需要注意的其他事项。', 'title_zh': '研究一种智能系统以监控和解释老年人异常活动模式'}
{'arxiv_id': 'arXiv:2501.18107', 'title': 'Scaling Inference-Efficient Language Models', 'authors': 'Song Bian, Minghao Yan, Shivaram Venkataraman', 'link': 'https://arxiv.org/abs/2501.18107', 'abstract': 'Scaling laws are powerful tools to predict the performance of large language models. However, current scaling laws fall short of accounting for inference costs. In this work, we first show that model architecture affects inference latency, where models of the same size can have up to 3.5x difference in latency. To tackle this challenge, we modify the Chinchilla scaling laws to co-optimize the model parameter count, the number of training tokens, and the model architecture. Due to the reason that models of similar training loss exhibit gaps in downstream evaluation, we also propose a novel method to train inference-efficient models based on the revised scaling laws. We perform extensive empirical studies to fit and evaluate our inference-aware scaling laws. We vary model parameters from 80M to 1B, training tokens from 1.6B to 30B, and model shapes, training a total of 63 models. Guided by our inference-efficient scaling law and model selection method, we release the Morph-1B model, which improves inference latency by 1.8x while maintaining accuracy on downstream tasks compared to open-source models, pushing the Pareto frontier of accuracy-latency tradeoff.', 'abstract_zh': '规模律是预测大型语言模型性能的强大工具。然而，现有的规模律未能充分考虑推理成本。在这项工作中，我们首先展示了模型架构会影响推理延迟，即使模型大小相同，其推理延迟也可能相差3.5倍。为应对这一挑战，我们修改了Chinchilla规模律，以优化模型参数数量、训练令牌数量和模型架构。由于相似训练损失的模型在下游评估中显示出性能差异，我们还提出了一种基于修订后的规模律训练高效推理模型的新方法。我们进行了广泛的实证研究，以拟合和评估我们的推理感知规模律。我们从80M到1B更改模型参数，从1.6B到30B更改训练令牌数量，并调整了模型形状，总共训练了63个模型。根据我们的高效推理规模律和模型选择方法的指导，我们发布了Morph-1B模型，该模型相比开源模型在保持下游任务准确性的同时，将推理延迟减少了1.8倍，推动了精度-延迟折衷的帕累托前沿。', 'title_zh': '高效推理的语言模型的扩展研究'}
{'arxiv_id': 'arXiv:2501.18100', 'title': 'Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation', 'authors': 'Yibo Wang, Tiansheng Huang, Li Shen, Huanjin Yao, Haotian Luo, Rui Liu, Naiqiang Tan, Jiaxing Huang, Dacheng Tao', 'link': 'https://arxiv.org/abs/2501.18100', 'abstract': "Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Mainstream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile -- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution -- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behavior, though it leads to a degradation in the model's fine-tuning performance. To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model's safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning performance. As a by-product, we analyze the optimized perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at this https URL", 'abstract_zh': '有害微调攻击对微调服务带来了显著的安全风险。主流防御措施旨在“接种”模型，使得后续的有害微调攻击效果降低。然而，我们的评估结果表明，这些防御措施是脆弱的——只需几次微调步骤，模型仍然可以学习到有害知识。为了解决这一问题，我们进一步进行了实验，并发现一个极为简单的解决方案——向微调后的模型添加纯粹随机扰动，可以在一定程度上恢复模型的行为，尽管这会导致微调性能的下降。为了应对微调性能的下降，我们进一步提出了Panacea，它优化了一个自适应的扰动，该扰动将在微调后应用于模型。Panacea在保持模型安全对齐性能的同时，也不牺牲下游微调性能。我们在不同有害比例、微调任务和主流大语言模型（LLM）上进行了全面的实验，结果显示有害评分平均降低了21.5%，而微调性能得以保持。作为副产品，我们分析了优化的扰动，并展示了各种LLM的不同层具有不同的安全系数。源代码可在以下链接获取：this https URL', 'title_zh': '泛药方：通过后微调扰动缓解大型语言模型的有害微调问题'}
{'arxiv_id': 'arXiv:2501.18096', 'title': 'LLMs can see and hear without any training', 'authors': 'Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar', 'link': 'https://arxiv.org/abs/2501.18096', 'abstract': 'We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities into your favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.', 'abstract_zh': '我们介绍了MILS：一种多模态迭代LLM求解器，这是一种出人意料地简单且无需训练的方法，将多模态能力注入您最喜欢的LLM。利用其天生的多步骤推理能力，MILS 提示LLM生成候选输出，每个输出都会被打分并反馈给下一个迭代过程，最终生成任务的解决方案。这使得许多通常需要在特定任务数据上训练专门模型的应用成为可能。具体而言，我们在这个领域取得了新的最佳成果，特别是在新兴的零样本图像、视频和音频描述方面。MILS 无缝应用于媒体生成，通过发现提示重写来改进文本到图像生成，并且甚至可以编辑提示进行风格转换！最后，作为无梯度优化方法，MILS 可以将多模态嵌入反转为文本，从而支持跨模态算术等应用。', 'title_zh': '大语言模型无需训练即可具备视觉和听觉能力'}
{'arxiv_id': 'arXiv:2501.18086', 'title': 'DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints for Safety-Critical Systems', 'authors': 'Se-Wook Yoo, Seung-Woo Seo', 'link': 'https://arxiv.org/abs/2501.18086', 'abstract': 'Safe reinforcement learning has traditionally relied on predefined constraint functions to ensure safety in complex real-world tasks, such as autonomous driving. However, defining these functions accurately for varied tasks is a persistent challenge. Recent research highlights the potential of leveraging pre-acquired task-agnostic knowledge to enhance both safety and sample efficiency in related tasks. Building on this insight, we propose a novel method to learn shared constraint distributions across multiple tasks. Our approach identifies the shared constraints through imitation learning and then adapts to new tasks by adjusting risk levels within these learned distributions. This adaptability addresses variations in risk sensitivity stemming from expert-specific biases, ensuring consistent adherence to general safety principles even with imperfect demonstrations. Our method can be applied to control and navigation domains, including multi-task and meta-task scenarios, accommodating constraints such as maintaining safe distances or adhering to speed limits. Experimental results validate the efficacy of our approach, demonstrating superior safety performance and success rates compared to baselines, all without requiring task-specific constraint definitions. These findings underscore the versatility and practicality of our method across a wide range of real-world tasks.', 'abstract_zh': '传统的强化学习方法通常依赖预定义的约束函数来确保在复杂现实任务中的安全性，例如自动驾驶。然而，准确地为各种任务定义这些函数仍然是一个持续的挑战。近期研究表明，利用预先获取的任务无关知识可以增强相关任务中的安全性和样本效率。基于这一见解，我们提出了一种新的方法，用于在多个任务之间学习共享的约束分布。该方法通过模仿学习来识别共享约束，并通过调整这些学习到的分布中的风险水平来适应新的任务。这种适应性解决了由于专家特定偏差导致的风险敏感性变化问题，从而确保即使在不完美的演示情况下也能一致地遵守通用的安全原则。我们的方法可以应用于控制和导航领域，包括多任务和元任务场景，适应诸如保持安全距离或遵守限速等约束条件。实验结果验证了我们方法的有效性，与基线方法相比，表明了更高的安全性能和成功率，且无需特定任务的约束定义。这些发现强调了我们的方法在广泛现实任务中的多样性和实用性。', 'title_zh': 'DIAL：基于分布信息的多任务约束自适应学习方法用于安全性关键系统'}
{'arxiv_id': 'arXiv:2501.18071', 'title': 'Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence', 'authors': 'Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino', 'link': 'https://arxiv.org/abs/2501.18071', 'abstract': 'Diabetes mellitus (DM) is a global health issue of significance that must be diagnosed as early as possible and managed well. This study presents a framework for diabetes prediction using Machine Learning (ML) models, complemented with eXplainable Artificial Intelligence (XAI) tools, to investigate both the predictive accuracy and interpretability of the predictions from ML models. Data Preprocessing is based on the Synthetic Minority Oversampling Technique (SMOTE) and feature scaling used on the Diabetes Binary Health Indicators dataset to deal with class imbalance and variability of clinical features. The ensemble model provided high accuracy, with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General Health, Income, and Physical Activity were the most influential predictors obtained from the model explanations. The results of this study suggest that ML combined with XAI is a promising means of developing accurate and computationally transparent tools for use in healthcare systems.', 'abstract_zh': '糖尿病（DM）是全球重要的公共卫生问题，必须尽可能早地进行诊断并进行有效的管理。本研究提出了一种使用机器学习（ML）模型进行糖尿病预测的框架，并结合可解释的人工智能（XAI）工具，以研究ML模型预测准确性及其可解释性。数据预处理采用了合成少数类过采样技术（SMOTE）和特征标准化，以解决类别不平衡和临床特征的变异性问题。集成模型提供了高精度，测试准确率为92.50%，ROC-AUC为0.975。从模型解释中得到了BMI、年龄、总体健康状况、收入和身体活动是最重要的预测因素。研究结果表明，将ML与XAI相结合，是一种发展准确且计算上透明的工具的有前景的方法，可用于医疗保健系统。', 'title_zh': '使用机器学习和可解释人工智能实现透明且准确的糖尿病预测'}
{'arxiv_id': 'arXiv:2501.18064', 'title': 'Learning Metal Microstructural Heterogeneity through Spatial Mapping of Diffraction Latent Space Features', 'authors': 'Mathieu Calvat, Chris Bean, Dhruv Anjaria, Hyoungryul Park, Haoren Wang, Kenneth Vecchio, J.C. Stinville', 'link': 'https://arxiv.org/abs/2501.18064', 'abstract': 'To leverage advancements in machine learning for metallic materials design and property prediction, it is crucial to develop a data-reduced representation of metal microstructures that surpasses the limitations of current physics-based discrete microstructure descriptors. This need is particularly relevant for metallic materials processed through additive manufacturing, which exhibit complex hierarchical microstructures that cannot be adequately described using the conventional metrics typically applied to wrought materials. Furthermore, capturing the spatial heterogeneity of microstructures at the different scales is necessary within such framework to accurately predict their properties. To address these challenges, we propose the physical spatial mapping of metal diffraction latent space features. This approach integrates (i) point diffraction data encoding via variational autoencoders or contrastive learning and (ii) the physical mapping of the encoded values. Together these steps offer a method offers a novel means to comprehensively describe metal microstructures. We demonstrate this approach on a wrought and additively manufactured alloy, showing that it effectively encodes microstructural information and enables direct identification of microstructural heterogeneity not directly possible by physics-based models. This data-reduced microstructure representation opens the application of machine learning models in accelerating metallic material design and accurately predicting their properties.', 'abstract_zh': '为了利用机器学习领域的最新进展进行金属材料的设计和性质预测，开发一个超越现行基于物理的离散微观结构描述符的减少数据表示的金属微观结构特征至关重要。这一需求特别适用于通过增材制造处理的金属材料，因为这些材料表现出复杂的分层微观结构，而传统的适用于轧制材料的度量标准无法充分描述。此外，在这种框架中捕捉微观结构在不同尺度上的空间异质性对于准确预测其性质是必要的。为了解决这些挑战，我们提出了金属衍射隐空间特征的物理空间映射方法。该方法结合了（i）通过变分自编码器或对比学习点衍射数据的编码，以及（ii）对编码值进行物理映射。这些步骤共同提供了一种全新的方法，用于全面描述金属微观结构。我们通过一副轧制合金和增材制造合金的示例，展示了这种方法能够有效编码微观结构信息，并允许直接识别物理模型不可直接识别的微观结构异质性。这种减少数据的微观结构表示为机器学习模型加速金属材料设计并准确预测其性质的应用奠定了基础。', 'title_zh': '通过衍射潜空间特征的空间映射学习金属微观结构的异质性'}
{'arxiv_id': 'arXiv:2501.18059', 'title': 'Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test', 'authors': 'Akinori F. Ebihara, Taiki Miyagawa, Kazuyuki Sakurai, Hitoshi Imaoka', 'link': 'https://arxiv.org/abs/2501.18059', 'abstract': 'Time-sensitive machine learning benefits from Sequential Probability Ratio Test (SPRT), which provides an optimal stopping time for early classification of time series. However, in finite horizon scenarios, where input lengths are finite, determining the optimal stopping rule becomes computationally intensive due to the need for backward induction, limiting practical applicability. We thus introduce FIRMBOUND, an SPRT-based framework that efficiently estimates the solution to backward induction from training data, bridging the gap between optimal stopping theory and real-world deployment. It employs density ratio estimation and convex function learning to provide statistically consistent estimators for sufficient statistic and conditional expectation, both essential for solving backward induction; consequently, FIRMBOUND minimizes Bayes risk to reach optimality. Additionally, we present a faster alternative using Gaussian process regression, which significantly reduces training time while retaining low deployment overhead, albeit with potential compromise in statistical consistency. Experiments across independent and identically distributed (i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets show that FIRMBOUND achieves optimalities in the sense of Bayes risk and speed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward optimality when possible and reduces decision-time variance, ensuring reliable decision-making. Code is publicly available at this https URL', 'abstract_zh': '以下是经过学术规范翻译的内容：\n\n时间敏感的机器学习可以从顺序概率比例测试（SPRT）中受益，SPRT 为时间序列的早期分类提供了一个最优停止时间。然而，在有限窗口场景中，由于需要进行逆向递推，当输入长度有限时，确定最优停止规则会变得计算密集，从而限制了其实用性。因此，我们引入了 FIRMBOUND，这是一种基于 SPRT 的框架，它能够从训练数据中高效估计逆向递推的解决方案，从而弥合了最优停止理论与实际部署之间的差距。FIRMBOUND 采用密度比估计和凸函数学习，为解决逆向递推所需的充分统计量和条件期望提供了统计一致估计器；因此，FIRMBOUND 通过最小化贝叶斯风险来达到最优性。此外，我们还提出了一种更快的替代方案，利用高斯过程回归，在显著减少训练时间的同时保留低部署开销，尽管可能会在统计一致性方面有所妥协。实验表明，FIRMBOUND 在贝叶斯风险和速度-准确率权衡方面达到了最优性，并在可能的情况下推进了权衡边界，从而减少了决策时间的方差，确保了可靠的决策。代码已公开，可在此 <this https URL> 中访问。', 'title_zh': '在有限时间内通过顺序概率比检验学习早期分类的最佳停止策略'}
{'arxiv_id': 'arXiv:2501.18055', 'title': 'Current Pathology Foundation Models are unrobust to Medical Center Differences', 'authors': 'Edwin D. de Jong, Eric Marcus, Jonas Teuwen', 'link': 'https://arxiv.org/abs/2501.18055', 'abstract': 'Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs.', 'abstract_zh': '病理学基础模型（FMs）在医疗保健领域具有巨大的潜力。在这些模型能够用于临床实践之前，必须确保它们能够应对不同医疗机构之间的差异。我们通过评估病理学FMs来确定它们是更多关注生物特征（如组织和癌症类型）还是已知的由染色过程及其他差异引入的混杂医疗机构特征。我们引入了一种新的稳健性指标——稳健性指数，该指标反映生物特征相对于混杂特征的程度。我们评估了十个现有的公开可用的病理学基础模型。我们发现，所有评估的病理学基础模型都强烈代表了医疗机构特征。观察到稳健性指数在不同模型间存在显著差异。到目前为止，只有一个模型的稳健性指数大于1，表明生物特征占据了主导地位，但只稍微占据主导。我们描述了一种量化不同医疗机构差异对基于FMs预测性能影响的方法。我们分析了非稳健性对下游模型分类性能的影响，并发现癌症类型分类错误并非随机，而是可以特定地归因于同一医疗机构中的干扰因素：来自同一医疗机构的其他类别的图像。我们可视化了FMs的嵌入空间，并发现这些空间更多地受到医疗机构而非生物因素的组织。因此，原医疗机构比组织来源和癌症类型更容易被准确预测。引入的稳健性指数旨在促进在临床实践中采用稳健可靠的病理学基础模型的进程。', 'title_zh': '当前的病理学基础模型对医疗机构差异不够鲁棒'}
{'arxiv_id': 'arXiv:2501.18052', 'title': 'SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders', 'authors': 'Bartosz Cywiński, Kamil Deja', 'link': 'https://arxiv.org/abs/2501.18052', 'abstract': "Recent machine unlearning approaches offer promising solution for removing unwanted concepts from diffusion models. However, traditional methods, which largely rely on fine-tuning, provide little insight into the changes they introduce to the base model, making it unclear whether concepts are truly removed or only masked. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to unlearn unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a method of selecting concept-specific features. This enables precise interventions on the model's activations to block targeted content while preserving the model's overall performance. Evaluation on the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron dismisses the possibility of generating unwanted content, even under adversarial attack.", 'abstract_zh': '近年来，机器遗忘方法为从扩散模型中去除不需要的概念提供了有希望的解决方案。然而，传统的基于微调的方法在减少对基模型的影响方面提供的洞见有限，这使得人们难以确定是否真正去除了概念，而只是将其掩盖了。在此工作中，我们引入了SAeUron，这是一种利用稀疏自编码器（SAEs）学习到的特征来去除文本到图像扩散模型中不需要概念的新型方法。首先，我们证明了SAEs在无监督方式下对扩散模型去噪多个时间步的激活进行训练后，能够捕获与特定概念相对应的稀疏且具有解释性的特征。在此基础上，我们提出了一种选择概念特定特征的方法，这使得能够在不损害模型整体性能的情况下精确定位并阻挡目标内容。在具有竞争力的UnlearnCanvas基准测试中对对象和风格遗忘的评估表明，SAeUron具有最先进的性能。此外，我们展示了通过单个SAEs，可以同时去除多个概念，并且与其它方法不同的是，SAeUron在遭受攻击时也能消除生成不需要内容的可能性。', 'title_zh': 'SAeUron: 基于稀疏自编码器的可解释概念遗忘方法在扩散模型中的应用'}
{'arxiv_id': 'arXiv:2501.18045', 'title': 'From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors', 'authors': 'Myra Cheng, Angela Y. Lee, Kristina Rapuano, Kate Niederhoffer, Alex Liebscher, Jeffrey Hancock', 'link': 'https://arxiv.org/abs/2501.18045', 'abstract': "How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p < 0.01; +41\\%, r = 0.62, p < 0.05$). Furthermore, these implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). We further explore how differences in metaphors and implicit perceptions--such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI--shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.", 'abstract_zh': '公众是如何回应人工智能（AI）技术日益增长的趋势的？我们通过在12个月内收集来自全国代表性美国样本的12,000多份问卷，探讨公众对AI的看法。参与者提供了开放式的比喻来反映他们对AI的认知模型，这种方法克服了传统自我报告测量方法的局限性。我们采用混合方法，结合定量聚类和定性编码，识别出20个主导的比喻，这些比喻塑造了公众对AI的理解。为了系统分析这些比喻，我们提出了一种集成语言模型（LM）技术的可扩展框架，以衡量公众感知的关键维度：拟人化（赋予人类特质的归因）、温暖度和效能。我们发现，美国人普遍认为AI是温暖且具有效能的，并且在过去一年中，公众对AI的人类相似性和温暖度的看法显著增加（增加34%，相关系数r = 0.80，p < 0.01；增加41%，相关系数r = 0.62，p < 0.05）。此外，这些隐性的认知以及识别出的主导的比喻强烈预测了公众对AI的信任与采用意愿（R² = 0.21, 0.18，p < 0.001）。我们进一步探讨了不同比喻和隐性认知之间的差异——例如女性、老年人和有色人种更倾向于将人类特质归因于AI——这些差异如何揭示群体间在信任和采用方面的差异。除了我们的数据集和可用于追踪公众态度变化的框架之外，我们还提供了关于如何使用比喻推动包容性和负责任的AI发展的可操作性见解。', 'title_zh': '从工具到盗贼：通过众包隐喻衡量和理解公众对人工智能的看法\n\n这个标题翻译成中文时，保持了原意并且符合学术规范。"From tools to thieves" 采用了形象的比喻，翻译时保持了这种修辞手法，增强了表达的生动性。'}
{'arxiv_id': 'arXiv:2501.18016', 'title': 'Digital Twin-Enabled Real-Time Control in Robotic Additive Manufacturing via Soft Actor-Critic Reinforcement Learning', 'authors': 'Matsive Ali, Sandesh Giri, Sen Liu, Qin Yang', 'link': 'https://arxiv.org/abs/2501.18016', 'abstract': "Smart manufacturing systems increasingly rely on adaptive control mechanisms to optimize complex processes. This research presents a novel approach integrating Soft Actor-Critic (SAC) reinforcement learning with digital twin technology to enable real-time process control in robotic additive manufacturing. We demonstrate our methodology using a Viper X300s robot arm, implementing two distinct control scenarios: static target acquisition and dynamic trajectory following. The system architecture combines Unity's simulation environment with ROS2 for seamless digital twin synchronization, while leveraging transfer learning to efficiently adapt trained models across tasks. Our hierarchical reward structure addresses common reinforcement learning challenges including local minima avoidance, convergence acceleration, and training stability. Experimental results show rapid policy convergence and robust task execution in both simulated and physical environments, with performance metrics including cumulative reward, value prediction accuracy, policy loss, and discrete entropy coefficient demonstrating the effectiveness of our approach. This work advances the integration of reinforcement learning with digital twins for industrial robotics applications, providing a framework for enhanced adaptive real-time control for smart additive manufacturing process.", 'abstract_zh': '智能制造系统越来越依赖于适应性控制机制以优化复杂过程。本研究提出了一种新颖的方法，将Soft Actor-Critic (SAC)强化学习与数字孪生技术结合起来，以实现机器人增材制造过程的实时控制。我们使用Viper X300s机器人臂演示了我们的方法论，并实现了两种不同的控制场景：静态目标获取和动态轨迹跟踪。该系统架构结合了Unity的仿真环境和ROS2，以实现无缝的数字孪生同步，并利用迁移学习来高效地在不同任务之间调整训练模型。我们的分层奖励结构解决了强化学习中常见的局部最优点规避、收敛加速和训练稳定等挑战。实验结果表明，在模拟和物理环境中，该系统能够实现快速的策略收敛和稳健的任务执行。性能指标包括累计奖励、价值预测准确性、策略损失和离散熵系数，均表明了我们方法的有效性。本研究推进了强化学习与数字孪生技术在工业机器人应用中的集成，为智能增材制造过程的增强适应性实时控制提供了框架。', 'title_zh': '基于数字孪生的软动作 critic 增强学习在机器人增材制造中的实时控制'}
{'arxiv_id': 'arXiv:2501.18011', 'title': 'Anatomy Might Be All You Need: Forecasting What to Do During Surgery', 'authors': 'Gary Sarwin, Alessandro Carretta, Victor Staartjes, Matteo Zoli, Diego Mazzatenta, Luca Regli, Carlo Serra, Ender Konukoglu', 'link': 'https://arxiv.org/abs/2501.18011', 'abstract': 'Surgical guidance can be delivered in various ways. In neurosurgery, spatial guidance and orientation are predominantly achieved through neuronavigation systems that reference pre-operative MRI scans. Recently, there has been growing interest in providing live guidance by analyzing video feeds from tools such as endoscopes. Existing approaches, including anatomy detection, orientation feedback, phase recognition, and visual question-answering, primarily focus on aiding surgeons in assessing the current surgical scene. This work aims to provide guidance on a finer scale, aiming to provide guidance by forecasting the trajectory of the surgical instrument, essentially addressing the question of what to do next. To address this task, we propose a model that not only leverages the historical locations of surgical instruments but also integrates anatomical features. Importantly, our work does not rely on explicit ground truth labels for instrument trajectories. Instead, the ground truth is generated by a detection model trained to detect both anatomical structures and instruments within surgical videos of a comprehensive dataset containing pituitary surgery videos. By analyzing the interaction between anatomy and instrument movements in these videos and forecasting future instrument movements, we show that anatomical features are a valuable asset in addressing this challenging task. To the best of our knowledge, this work is the first attempt to address this task for manually operated surgeries.', 'abstract_zh': '外科指导可以通过多种方式提供。在神经外科中，空间定位和方向主要通过参考术前MRI扫描的神经导航系统来实现。近年来，通过分析内窥镜等工具的实时视频流来提供即时指导的兴趣日益增加。目前的方法，包括解剖结构检测、方向反馈、相位识别和视觉问答，主要集中在协助外科医生评估当前的手术场景。本研究旨在提供更为精细的指导，目标是通过预测外科器械的轨迹来提供指导，核心问题是接下来应该做什么。为了解决这个问题，我们提出了一种模型，该模型不仅利用了手术器械的历史位置信息，还整合了解剖特征。重要的是，我们的工作并不依赖于显式的器械轨迹地面真实标签。相反，地面真实标签是由一个能够检测手术视频中解剖结构和器械的检测模型生成的，这些手术视频数据集涵盖了垂体手术视频。通过对这些视频中解剖结构与器械运动的交互分析以及预测未来器械运动，我们表明解剖特征是应对这一挑战的重要资产。据我们所知，这是首次尝试解决手动操作手术过程中这类任务的研究。', 'title_zh': 'anatomy Might Be All You Need: 预测手术中应采取的行动\n\n或者更正式的表达可以是：\n\n解剖学知识或许足以预测：手术中应采取的行动'}
{'arxiv_id': 'arXiv:2501.18006', 'title': 'Topological Signatures of Adversaries in Multimodal Alignments', 'authors': 'Minh Vu, Geigh Zollicoffer, Huy Mai, Ben Nebgen, Boian Alexandrov, Manish Bhattarai', 'link': 'https://arxiv.org/abs/2501.18006', 'abstract': 'Multimodal Machine Learning systems, particularly those aligning text and image data like CLIP/BLIP models, have become increasingly prevalent, yet remain susceptible to adversarial attacks. While substantial research has addressed adversarial robustness in unimodal contexts, defense strategies for multimodal systems are underexplored. This work investigates the topological signatures that arise between image and text embeddings and shows how adversarial attacks disrupt their alignment, introducing distinctive signatures. We specifically leverage persistent homology and introduce two novel Topological-Contrastive losses based on Total Persistence and Multi-scale kernel methods to analyze the topological signatures introduced by adversarial perturbations. We observe a pattern of monotonic changes in the proposed topological losses emerging in a wide range of attacks on image-text alignments, as more adversarial samples are introduced in the data. By designing an algorithm to back-propagate these signatures to input samples, we are able to integrate these signatures into Maximum Mean Discrepancy tests, creating a novel class of tests that leverage topological signatures for better adversarial detection.', 'abstract_zh': '多模态机器学习系统，特别是如CLIP/BLIP模型那样的文本和图像数据对齐系统，逐渐变得越来越普遍，但仍然容易受到对抗性攻击的影响。尽管在单模态背景下已经进行了大量研究以增强对抗性鲁棒性，但针对多模态系统的防御策略仍然未被充分探索。本研究探讨了图像和文本嵌入之间的拓扑特征，并展示了对抗性攻击如何破坏它们的对齐，从而产生独特的拓扑特征。我们具体利用持续同调方法，并基于整体持久性和多尺度核方法引入了两种新的拓扑对比损失，以分析由对抗性扰动引入的拓扑特征。我们观察到，在图像-文本对齐的一系列广泛的攻击中，所提出的拓扑损失呈现单调变化的趋势，随着数据中对抗样本数量的增加，这种趋势更加明显。通过设计一种算法将这些特征反向传播到输入样本，我们能够将这些特征整合到最大均值离异度检验中，由此创建了一种新的基于拓扑特征的对抗检测方法。', 'title_zh': '多模态对齐中对手的拓扑特征'}
{'arxiv_id': 'arXiv:2501.17982', 'title': 'Belief Roadmaps with Uncertain Landmark Evanescence', 'authors': 'Erick Fuentes, Jared Strader, Ethan Fahnestock, Nicholas Roy', 'link': 'https://arxiv.org/abs/2501.17982', 'abstract': 'We would like a robot to navigate to a goal location while minimizing state uncertainty. To aid the robot in this endeavor, maps provide a prior belief over the location of objects and regions of interest. To localize itself within the map, a robot identifies mapped landmarks using its sensors. However, as the time between map creation and robot deployment increases, portions of the map can become stale, and landmarks, once believed to be permanent, may disappear. We refer to the propensity of a landmark to disappear as landmark evanescence. Reasoning about landmark evanescence during path planning, and the associated impact on localization accuracy, requires analyzing the presence or absence of each landmark, leading to an exponential number of possible outcomes of a given motion plan. To address this complexity, we develop BRULE, an extension of the Belief Roadmap. During planning, we replace the belief over future robot poses with a Gaussian mixture which is able to capture the effects of landmark evanescence. Furthermore, we show that belief updates can be made efficient, and that maintaining a random subset of mixture components is sufficient to find high quality solutions. We demonstrate performance in simulated and real-world experiments. Software is available at this https URL.', 'abstract_zh': '我们希望让机器人在导航到目标位置的同时，尽量减少状态不确定性。为了帮助机器人实现这一目标，地图提供了物体和感兴趣区域的先验信念。为了在地图中定位自身，机器人使用其传感器识别已映射的地标。然而，随着地图创建与机器人部署之间的时间增加，地图的某些部分可能会变得过时，原本认为是永久存在的地标也可能会消失。我们称这种地标消失的倾向为地标蒸发性。在路径规划过程中考虑地标蒸发性，并对其对定位准确性的影响进行推理，需要分析每个地标的存在或不存在情况，从而导致一组给定运动计划可能的结果呈指数增长。为了解决这种复杂性，我们开发了BRULE，它是信念路网的扩展。在规划过程中，我们用高斯混合模型来替换对未来机器人姿态的信念，从而能够捕捉地标蒸发性的影响。此外，我们展示了信念更新可以实现高效化，并且维持混合模型中的随机子集足以找到高质量的解决方案。我们通过模拟和真实世界的实验展示了性能。软件可以在以下链接获取：[提供链接处]', 'title_zh': '含不确定地标消失信念 roadmap'}
{'arxiv_id': 'arXiv:2501.17980', 'title': 'Limits to AI Growth: The Ecological and Social Consequences of Scaling', 'authors': 'Eshta Bhardwaj, Rohan Alexander, Christoph Becker', 'link': 'https://arxiv.org/abs/2501.17980', 'abstract': 'The accelerating development and deployment of AI technologies depend on the continued ability to scale their infrastructure. This has implied increasing amounts of monetary investment and natural resources. Frontier AI applications have thus resulted in rising financial, environmental, and social costs. While the factors that AI scaling depends on reach its limits, the push for its accelerated advancement and entrenchment continues. In this paper, we provide a holistic review of AI scaling using four lenses (technical, economic, ecological, and social) and review the relationships between these lenses to explore the dynamics of AI growth. We do so by drawing on system dynamics concepts including archetypes such as "limits to growth" to model the dynamic complexity of AI scaling and synthesize several perspectives. Our work maps out the entangled relationships between the technical, economic, ecological and social perspectives and the apparent limits to growth. The analysis explains how industry\'s responses to external limits enables continued (but temporary) scaling and how this benefits Big Tech while externalizing social and environmental damages. To avoid an "overshoot and collapse" trajectory, we advocate for realigning priorities and norms around scaling to prioritize sustainable and mindful advancements.', 'abstract_zh': '人工智能技术的加速开发和部署依赖于持续扩大其基础设施的能力。这已经意味着需要不断增加资金投资和自然资源。前沿的人工智能应用因此导致了财务、环境和社会成本的上升。随着依赖于人工智能扩展的因素达到其极限，对其加速发展和固化的推动依然持续。在这篇论文中，我们从四个维度（技术、经济、生态和社会）对人工智能的扩展进行全面回顾，并探讨这些维度之间的关系，以探索人工智能增长的动力机制。我们通过引用系统动力学概念，包括“增长的极限”等原型，来建模人工智能扩展的动态复杂性，并综合多个视角进行分析。我们的研究勾勒出技术、经济、生态和社会视角之间错综复杂的相互关系以及增长的显性极限。分析解释了行业对外部限制的回应如何能够实现持续（但仅是暂时的）扩展，以及这种扩展如何促进了大科技公司的发展，同时也将社会和环境损害外部化。为了避免“超调和崩溃”的路径，我们提倡重新调整扩展的优先事项和规范，以优先推动可持续和有意识的进步。', 'title_zh': 'AI增长的极限：扩展的生态和社会后果'}
{'arxiv_id': 'arXiv:2501.17917', 'title': 'Deep Ensembles Secretly Perform Empirical Bayes', 'authors': 'Gabriel Loaiza-Ganem, Valentin Villecroze, Yixin Wang', 'link': 'https://arxiv.org/abs/2501.17917', 'abstract': 'Quantifying uncertainty in neural networks is a highly relevant problem which is essential to many applications. The two predominant paradigms to tackle this task are Bayesian neural networks (BNNs) and deep ensembles. Despite some similarities between these two approaches, they are typically surmised to lack a formal connection and are thus understood as fundamentally different. BNNs are often touted as more principled due to their reliance on the Bayesian paradigm, whereas ensembles are perceived as more ad-hoc; yet, deep ensembles tend to empirically outperform BNNs, with no satisfying explanation as to why this is the case. In this work we bridge this gap by showing that deep ensembles perform exact Bayesian averaging with a posterior obtained with an implicitly learned data-dependent prior. In other words deep ensembles are Bayesian, or more specifically, they implement an empirical Bayes procedure wherein the prior is learned from the data. This perspective offers two main benefits: (i) it theoretically justifies deep ensembles and thus provides an explanation for their strong empirical performance; and (ii) inspection of the learned prior reveals it is given by a mixture of point masses -- the use of such a strong prior helps elucidate observed phenomena about ensembles. Overall, our work delivers a newfound understanding of deep ensembles which is not only of interest in it of itself, but which is also likely to generate future insights that drive empirical improvements for these models.', 'abstract_zh': '量化神经网络中的不确定性是一个高度相关的问题，对于许多应用而言都是至关重要的。处理这个问题的两大主要范式是贝叶斯神经网络（BNNs）和深度集成方法。尽管这两者之间存在一定的相似性，但通常认为它们缺乏正式的联系，因此被视为本质上有所不同。BNNs 被认为更为正规，因为它依赖于贝叶斯范式；而集成方法则被认为更随意；然而，实验结果显示，深度集成方法通常优于 BNNs，但却没有令人满意的解释说明为何会出现这种情况。\n\n在这项工作中，我们通过证明深度集成方法执行了使用隐式学习的数据依赖先验得到的精确贝叶斯平均化来填补这一差距。换句话说，深度集成方法是贝叶斯的，具体而言，它们实现了经验贝叶斯程序，其中先验是从数据中学习得到的。这一视角带来了两个主要的好处：（i）它为深度集成方法提供了理论上的正当性，从而解释了它们的出色实验表现；（ii）检查学习得到的先验，可以发现它是由点质量混合给出的——使用这样强的先验有助于解释集成方法中观察到的现象。总体而言，我们的工作为深度集成方法提供了一种新的理解，这不仅具有自身的学术意义，还可能为这些模型的实验改进提供未来洞察。', 'title_zh': '深度集成秘密地执行经验贝叶斯'}
{'arxiv_id': 'arXiv:2501.17905', 'title': 'DReSS: Data-driven Regularized Structured Streamlining for Large Language Models', 'authors': 'Mingkuan Feng, Jinyang Wu, Shuai Zhang, Pengpeng Shao, Ruihan Jin, Zhengqi Wen, Jianhua Tao, Feihu Che', 'link': 'https://arxiv.org/abs/2501.17905', 'abstract': 'Large language models (LLMs) have achieved significant progress across various domains, but their increasing scale results in high computational and memory costs. Recent studies have revealed that LLMs exhibit sparsity, providing the potential to reduce model size through pruning techniques. However, existing pruning methods typically follow a prune-then-finetune paradigm. Since the pruned components still contain valuable information, their direct removal often leads to irreversible performance degradation, imposing a substantial computational burden to recover performance during finetuning. In this paper, we propose a novel paradigm that first applies regularization, then prunes, and finally finetunes. Based on this paradigm, we introduce DReSS, a simple and effective Data-driven Regularized Structured Streamlining method for LLMs. By leveraging a small amount of data to regularize the components to be pruned, DReSS explicitly transfers the important information to the remaining parts of the model in advance. Compared to direct pruning, this can reduce the information loss caused by parameter removal, thereby enhancing its language modeling capabilities. Experimental results demonstrate that DReSS significantly outperforms existing pruning methods even under extreme pruning ratios, significantly reducing latency and increasing throughput.', 'abstract_zh': '大语言模型（LLMs）在各个领域取得了显著的进步，但其不断增加的规模导致了高昂的计算和内存成本。最近的研究表明，LLMs表现出稀疏性，这为通过剪枝技术减小模型规模提供了潜在的可能性。然而，现有的剪枝方法通常遵循先剪枝后微调的模式。由于被剪枝的组件仍然包含有价值的信息，直接删除这些组件通常会导致不可逆的性能下降，这在微调过程中会产生巨大的计算负担。本文提出了一种新的范式，即先应用正则化，再进行剪枝，最后进行微调。基于这一范式，我们提出了DReSS（Data-Driven Regularized Structured Streamlining）方法，这是一种适用于LLMs的简单而有效的数据驱动正则化结构化简化方法。通过利用少量数据来正则化即将进行剪枝的组件，DReSS能够提前将重要的信息转移到模型的剩余部分。与直接剪枝相比，这可以减少因参数删除而导致的信息损失，从而提高其语言建模能力。实验结果表明，即使在极端的剪枝率下，DReSS也能显著优于现有的剪枝方法，显著减少了延迟并提高了吞吐量。', 'title_zh': 'DReSS：数据驱动正则化结构化简化方法用于大型语言模型'}
{'arxiv_id': 'arXiv:2501.17903', 'title': 'Free Agent in Agent-Based Mixture-of-Experts Generative AI Framework', 'authors': 'Jung-Hua Liu', 'link': 'https://arxiv.org/abs/2501.17903', 'abstract': 'Multi-agent systems commonly distribute tasks among specialized, autonomous agents, yet they often lack mechanisms to replace or reassign underperforming agents in real time. Inspired by the free-agency model of Major League Baseball, the Reinforcement Learning Free Agent (RLFA) algorithm introduces a reward-based mechanism to detect and remove agents exhibiting persistent underperformance and seamlessly insert more capable ones. Each agent internally uses a mixture-of-experts (MoE) approach, delegating incoming tasks to specialized sub-models under the guidance of a gating function. A primary use case is fraud detection, where RLFA promptly swaps out an agent whose detection accuracy dips below a preset threshold. A new agent is tested in a probationary mode, and upon demonstrating superior performance, fully replaces the underperformer. This dynamic, free-agency cycle ensures sustained accuracy, quicker adaptation to emerging threats, and minimal disruption to ongoing operations. By continually refreshing its roster of agents, the system fosters ongoing improvements and more resilient collaboration in multi-agent Generative AI environments.', 'abstract_zh': '多智能体系统通常将任务分配给专门的、自主的代理，但它们往往缺乏能够实时检测和更换表现不佳代理的机制。受到美国职业棒球大联盟自由球员机制的启发，Reinforcement Learning Free Agent（RLFA）算法引入了一种基于奖励的机制，用于检测持续表现不佳的代理并实时更换，同时无缝地插入更胜任的代理。每个代理内部使用混合专家（MoE）方法，通过门控函数的指导将到来的任务委派给专门的子模型。一个主要的应用案例是欺诈检测，在这种情况下，RLFA可以迅速替换检测精度下降到预设阈值以下的代理。新的代理将处于试用模式，若表现出色，则完全替代表现较差的代理。这种动态的自由代理循环确保了持续的准确性、更快地适应新兴威胁并且对持续操作的影响最小。通过不断更新其代理列表，系统促进了多代理生成AI环境中的持续改进和更稳健的合作。', 'title_zh': '基于代理混合专家生成人工智能框架中的自由代理'}
{'arxiv_id': 'arXiv:2501.17899', 'title': 'The Right to AI', 'authors': 'Rashid Mushkani, Hugo Berard, Allison Cohen, Shin Koeski', 'link': 'https://arxiv.org/abs/2501.17899', 'abstract': "This paper proposes a Right to AI, which asserts that individuals and communities should meaningfully participate in the development and governance of the AI systems that shape their lives. Motivated by the increasing deployment of AI in critical domains and inspired by Henri Lefebvre's concept of the Right to the City, we reconceptualize AI as a societal infrastructure, rather than merely a product of expert design. In this paper, we critically evaluate how generative agents, large-scale data extraction, and diverse cultural values bring new complexities to AI oversight. The paper proposes that grassroots participatory methodologies can mitigate biased outcomes and enhance social responsiveness. It asserts that data is socially produced and should be managed and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen Participation and analyzing nine case studies, the paper develops a four-tier model for the Right to AI that situates the current paradigm and envisions an aspirational future. It proposes recommendations for inclusive data ownership, transparent design processes, and stakeholder-driven oversight. We also discuss market-led and state-centric alternatives and argue that participatory approaches offer a better balance between technical efficiency and democratic legitimacy.", 'abstract_zh': '本文提出了一项“人工智能权利”（Right to AI），主张个人和社区应有意义地参与到影响其生活的AI系统的发展与治理中。本文受制于人工智能在关键领域日益广泛的应用，借鉴亨利·列斐伏尔提出的“城市权利”概念，重新诠释人工智能为社会基础设施，而不仅仅是专家设计的产品。本文批判性地评估了生成性智能代理、大规模数据提取以及多样文化价值观给人工智能监管带来的新复杂性。本文提出，基层参与性的方法可以减少偏差结果并增强社会响应性。本文主张数据是社会生产出来的，应由集体来管理和拥有。基于雪莉·阿伦森的公民参与梯级模型，并分析了九个案例研究，本文提出了一个四项层级的“人工智能权利”模型，既定位当前范式，又构想了一个理想化的未来。本文还提出了包括包容性数据所有权、透明设计过程和利益相关者驱动的监管在内的建议。我们还讨论了市场主导和国家主导的替代方案，并论证了参与性方法在技术效率与民主合法性之间提供了更好的平衡。', 'title_zh': '《人工智能权利》'}
{'arxiv_id': 'arXiv:2501.17894', 'title': 'Progress in Artificial Intelligence and its Determinants', 'authors': 'Michael R. Douglas, Sergiy Verstyuk', 'link': 'https://arxiv.org/abs/2501.17894', 'abstract': "We study long-run progress in artificial intelligence in a quantitative way. Many measures, including traditional ones such as patents and publications, machine learning benchmarks, and a new Aggregate State of the Art in ML (or ASOTA) Index we have constructed from these, show exponential growth at roughly constant rates over long periods. Production of patents and publications doubles every ten years, by contrast with the growth of computing resources driven by Moore's Law, roughly a doubling every two years. We argue that the input of AI researchers is also crucial and its contribution can be objectively estimated. Consequently, we give a simple argument that explains the 5:1 relation between these two rates. We then discuss the application of this argument to different output measures and compare our analyses with predictions based on machine learning scaling laws proposed in existing literature. Our quantitative framework facilitates understanding, predicting, and modulating the development of these important technologies.", 'abstract_zh': '我们以定量的方式研究了人工智能长期进展的过程。许多衡量指标，包括传统的专利和出版物数量、机器学习基准，以及我们从这些数据中构建的新的人工智能机器学习最新进展综合指数（或称ASOTA指数），在长时间段内显示出大致恒定速率的指数增长。相比之下，由 Moore 定律驱动的计算资源的增长率大约是每两年翻一番，而论文和专利的生产每十年翻一番。我们指出，人工智能研究人员的投入同样至关重要，并且其贡献可以客观地估计。因此，我们提出一个简单的论据来解释这两者之间约5:1的关系。接下来，我们讨论这一论据在不同产出指标中的应用，并将我们的分析与现有文献中提出的机器学习扩展规律预测进行比较。我们的量化框架有助于理解、预测和调控这些重要技术的发展。', 'title_zh': '人工智能的进步及其决定因素'}
{'arxiv_id': 'arXiv:2501.17889', 'title': 'Knoop: Practical Enhancement of Knockoff with Over-Parameterization for Variable Selection', 'authors': 'Xiaochen Zhang, Yunfeng Cai, Haoyi Xiong', 'link': 'https://arxiv.org/abs/2501.17889', 'abstract': 'Variable selection plays a crucial role in enhancing modeling effectiveness across diverse fields, addressing the challenges posed by high-dimensional datasets of correlated variables. This work introduces a novel approach namely Knockoff with over-parameterization (Knoop) to enhance Knockoff filters for variable selection. Specifically, Knoop first generates multiple knockoff variables for each original variable and integrates them with the original variables into an over-parameterized Ridgeless regression model. For each original variable, Knoop evaluates the coefficient distribution of its knockoffs and compares these with the original coefficients to conduct an anomaly-based significance test, ensuring robust variable selection. Extensive experiments demonstrate superior performance compared to existing methods in both simulation and real-world datasets. Knoop achieves a notably higher Area under the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for effectively identifying relevant variables against the ground truth by controlled simulations, while showcasing enhanced predictive accuracy across diverse regression and classification tasks. The analytical results further backup our observations.', 'abstract_zh': '变量选择在提高模型效果方面扮演着至关重要的角色，特别是在应对高维相关变量数据集带来的挑战时。本文提出了一种新颖的方法——过参数化仿射复制法（Knoop），以增强仿射复制筛选法的变量选择能力。具体而言，Knoop 首先为每个原始变量生成多个仿射复制变量，并将它们与原始变量整合到一个过参数化的无偏估计岭回归模型中。对于每个原始变量，Knoop 评估其仿射复制变量的系数分布，并将这些分布与原始系数进行比较，以执行基于异常值的显著性检验，从而确保稳健的变量选择。大量的实验证明，Knoop 相较于现有方法，在模拟数据和实际数据集上均表现出优异的性能。Knoop 在受控模拟中通过 receiver operating characteristic (ROC) 曲线下的区域 (AUC) 有效识别相关变量方面表现更为出色，同时在各种回归和分类任务中展示了增强的预测准确性。进一步的分析结果支持我们的观察。', 'title_zh': 'Knoop：基于过参数化的 Knockoff 方法在变量选择中的实用增强'}
{'arxiv_id': 'arXiv:2501.17888', 'title': 'RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings', 'authors': 'Shuai Chen, Yong Zu, Zhixi Feng, Shuyuan Yang, Mengchang Li, Yue Ma, Jun Liu, Qiukai Pan, Xinlei Zhang, Changjun Sun', 'link': 'https://arxiv.org/abs/2501.17888', 'abstract': 'The increasing scarcity of spectrum resources and the rapid growth of wireless device have made efficient management of radio networks a critical challenge. Cognitive Radio Technology (CRT), when integrated with deep learning (DL), offers promising solutions for tasks such as radio signal classification (RSC), signal denoising, and spectrum allocation. However, existing DL-based CRT frameworks are often task-specific and lack scalability to diverse real-world scenarios. Meanwhile, Large Language Models (LLMs) have demonstrated exceptional generalization capabilities across multiple domains, making them a potential candidate for advancing CRT technologies. In this paper, we introduce RadioLLM, a novel framework that incorporates Hybrid Prompt and Token Reprogramming (HPTR) and a Frequency Attuned Fusion (FAF) module to enhance LLMs for CRT tasks. HPTR enables the integration of radio signal features with expert knowledge, while FAF improves the modeling of high-frequency features critical for precise signal processing. These innovations allow RadioLLM to handle diverse CRT tasks, bridging the gap between LLMs and traditional signal processing methods. Extensive empirical studies on multiple benchmark datasets demonstrate that the proposed RadioLLM achieves superior performance over current baselines.', 'abstract_zh': '随着频谱资源日益稀缺和无线设备的迅猛增长，有效管理无线网络已成为一个重要挑战。将认知无线电技术（Cognitive Radio Technology, CRT）与深度学习（Deep Learning, DL）相结合，为无线信号分类（Radio Signal Classification, RSC）、信号去噪和频谱分配等任务提供了有希望的解决方案。然而，现有的基于DL的CRT框架往往是针对特定任务设计的，并且缺乏适应多种真实场景的扩展性。与此同时，大型语言模型（Large Language Models, LLMs）已经展示了在多个领域中的出色泛化能力，使其成为推进CRT技术的潜在候选者。本文中，我们介绍了一种名为RadioLLM的新型框架，该框架结合了混合提示和标记重编程（Hybrid Prompt and Token Reprogramming, HPTR）模块和频率调谐融合（Frequency Attuned Fusion, FAF）模块，以增强LLMs在CRT任务中的应用能力。HPTR 允许将无线信号特征与专家知识相结合，而 FAF 则提高了对精确信号处理至关重要的高频率特征建模能力。这些创新使RadioLLM能够处理各种CRT任务，从而在LLMs和传统的信号处理方法之间架起桥梁。通过在多个基准数据集上的广泛实证研究，证明所提出的RadioLLM在当前基线方法上表现出更优的性能。', 'title_zh': 'RadioLLM：通过混合提示和token重新编程将大型语言模型引入认知无线电'}
{'arxiv_id': 'arXiv:2501.17883', 'title': 'Explainable and Robust Millimeter Wave Beam Alignment for AI-Native 6G Networks', 'authors': 'Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri', 'link': 'https://arxiv.org/abs/2501.17883', 'abstract': 'Integrated artificial intelligence (AI) and communication has been recognized as a key pillar of 6G and beyond networks. In line with AI-native 6G vision, explainability and robustness in AI-driven systems are critical for establishing trust and ensuring reliable performance in diverse and evolving environments. This paper addresses these challenges by developing a robust and explainable deep learning (DL)-based beam alignment engine (BAE) for millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. The proposed convolutional neural network (CNN)-based BAE utilizes received signal strength indicator (RSSI) measurements over a set of wide beams to accurately predict the best narrow beam for each UE, significantly reducing the overhead associated with exhaustive codebook-based narrow beam sweeping for initial access (IA) and data transmission. To ensure transparency and resilience, the Deep k-Nearest Neighbors (DkNN) algorithm is employed to assess the internal representations of the network via nearest neighbor approach, providing human-interpretable explanations and confidence metrics for detecting out-of-distribution inputs. Experimental results demonstrate that the proposed DL-based BAE exhibits robustness to measurement noise, reduces beam training overhead by 75% compared to the exhaustive search while maintaining near-optimal performance in terms of spectral efficiency. Moreover, the proposed framework improves outlier detection robustness by up to 5x and offers clearer insights into beam prediction decisions compared to traditional softmax-based classifiers.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，要符合学术规范：\n\n集成人工智能（AI）与通信被认为是6G及更高级网络的关键支柱。遵循AI原生6G愿景，AI驱动系统的可解释性和鲁棒性对于在多样且不断演变的环境中建立信任和确保可靠性能至关重要。本文通过开发一种基于深度学习（DL）的稳健且可解释的波束对齐引擎（BAE）来应对这些挑战，该引擎适用于毫米波（mmWave）多输入多输出（MIMO）系统。所提出的基于卷积神经网络（CNN）的BAE利用一组宽带束的接收信号强度指示（RSSI）测量，准确预测每个用户设备（UE）的最佳窄波束，显著减少了初始接入（IA）和数据传输过程中基于详尽码本的窄波束扫掠所带来的开销。为了确保透明度和稳健性，本文采用深层最近邻（Deep k-Nearest Neighbors, DkNN）算法通过最近邻方法评估网络内部表示，提供可由人类理解和解释的解释和置信度度量，以识别异常输入。实验结果表明，所提的基于DL的BAE对测量噪声具有鲁棒性，与详尽搜索相比，将波束训练开销降低了75%，同时在频谱效率方面保持了接近最优的性能。此外，所提框架将异常点检测的鲁棒性提高了5倍，并在波束预测决策方面提供了比传统softmax基分类器更清晰的洞察。', 'title_zh': '面向AI原生6G网络的可解释性和鲁棒性毫米波波束对准方法'}
{'arxiv_id': 'arXiv:2501.17881', 'title': 'RayLoc: Wireless Indoor Localization via Fully Differentiable Ray-tracing', 'authors': 'Xueqiang Han, Tianyue Zheng, Tony Xiao Han, Jun Luo', 'link': 'https://arxiv.org/abs/2501.17881', 'abstract': 'Wireless indoor localization has been a pivotal area of research over the last two decades, becoming a cornerstone for numerous sensing applications. However, conventional wireless localization methods rely on channel state information to perform blind modelling and estimation of a limited set of localization parameters. This oversimplification neglects many sensing scene details, resulting in suboptimal localization accuracy. To address this limitation, this paper presents a novel approach to wireless indoor localization by reformulating it as an inverse problem of wireless ray-tracing, inferring scene parameters that generates the measured CSI. At the core of our solution is a fully differentiable ray-tracing simulator that enables backpropagation to comprehensive parameters of the sensing scene, allowing for precise localization. To establish a robust localization context, RayLoc constructs a high-fidelity sensing scene by refining coarse-grained background model. Furthermore, RayLoc overcomes the challenges of sparse gradient and local minima by convolving the signal generation process with a Gaussian kernel. Extensive experiments showcase that RayLoc outperforms traditional localization baselines and is able to generalize to different sensing environments.', 'abstract_zh': '无线室内定位在过去二十年中一直是研究的核心领域，成为了众多传感应用的基础。然而，传统无线定位方法依赖于信道状态信息来进行盲建模和估计有限的定位参数。这种简化忽视了许多传感场景的细节，导致定位精度不足。为解决这一问题，本文提出了一种新的无线室内定位方法，将其重新表述为无线射线追踪的逆问题，通过推断生成测量信道状态信息的场景参数来实现定位。我们解决方案的核心是一个完全可微的射线追踪模拟器，该模拟器使回传成为传感场景的综合参数成为可能，从而实现精确的定位。为了建立稳健的定位环境，RayLoc 通过细化粗粒度背景模型构建了一个高保真度的传感场景。此外，RayLoc 通过将信号生成过程与高斯核卷积来克服稀疏梯度和局部最小值的挑战。大量实验表明，RayLoc 在性能上优于传统定位基准，并且能够泛化到不同的传感环境。', 'title_zh': 'RayLoc：通过完全可微射线追踪实现的无线室内定位'}
{'arxiv_id': 'arXiv:2501.17880', 'title': 'Assessment of the January 2025 Los Angeles County wildfires: A multi-modal analysis of impact, response, and population exposure', 'authors': 'Seyd Teymoor Seydi', 'link': 'https://arxiv.org/abs/2501.17880', 'abstract': 'This study presents a comprehensive analysis of four significant California wildfires: Palisades, Eaton, Kenneth, and Hurst, examining their impacts through multiple dimensions, including land cover change, jurisdictional management, structural damage, and demographic vulnerability. Using the Chebyshev-Kolmogorov-Arnold network model applied to Sentinel-2 imagery, the extent of burned areas was mapped, ranging from 315.36 to 10,960.98 hectares. Our analysis revealed that shrubland ecosystems were consistently the most affected, comprising 57.4-75.8% of burned areas across all events. The jurisdictional assessment demonstrated varying management complexities, from singular authority (98.7% in the Palisades Fire) to distributed management across multiple agencies. A structural impact analysis revealed significant disparities between urban interface fires (Eaton: 9,869 structures; Palisades: 8,436 structures) and rural events (Kenneth: 24 structures; Hurst: 17 structures). The demographic analysis showed consistent gender distributions, with 50.9% of the population identified as female and 49.1% as male. Working-age populations made up the majority of the affected populations, ranging from 53.7% to 54.1%, with notable temporal shifts in post-fire periods. The study identified strong correlations between urban interface proximity, structural damage, and population exposure. The Palisades and Eaton fires affected over 20,000 people each, compared to fewer than 500 in rural events. These findings offer valuable insights for the development of targeted wildfire management strategies, particularly in wildland urban interface zones, and emphasize the need for age- and gender-conscious approaches in emergency response planning.', 'abstract_zh': '本研究对加利福尼亚州四起重大森林火灾——Palisades、Eaton、Kenneth和Hurst进行了全面分析，探讨了这些火灾在多个维度上的影响，包括土地覆盖变化、行政管理复杂性、建筑物损毁以及人口易敏感性。研究利用Chebyshev-Kolmogorov-Arnold网络模型对Sentinel-2遥感图像进行分析，绘制了烧毁区域范围从315.36到10,960.98公顷的分布图。分析结果表明，灌木丛生态系统是最受冲击的，占所有事件中燃烧面积的57.4%至75.8%。行政管理评估显示管理复杂度存在差异，从单一行政主体（Palisades火灾中占98.7%）到多部门联合管理。建筑物损毁分析揭示了城乡边界火灾（Eaton：9,869栋建筑；Palisades：8,436栋建筑）与农村火灾（Kenneth：24栋建筑；Hurst：17栋建筑）之间显著差异。人口易敏感性分析显示，女性和男性的分布基本一致，其中50.9%的人口为女性，49.1%为男性。受影响的人口中，工作年龄人口比例最高，范围从53.7%到54.1%，灾后时期存在显著的时间变化。研究结果表明，城市边界距离、建筑物损毁和人口暴露之间存在密切关联。与农村火灾相比，Palisades和Eaton火灾分别影响了超过20,000人，而农村火灾的影响人数不到500人。这些发现为针对城乡过渡地带的野火管理策略开发提供了宝贵见解，并强调了在应急响应规划中采取年龄和性别适配方法的重要性。', 'title_zh': '2025年1月洛杉矶县野火评估：多模态影响、响应和人口暴露分析'}
{'arxiv_id': 'arXiv:2501.17879', 'title': 'Task and Perception-aware Distributed Source Coding for Correlated Speech under Bandwidth-constrained Channels', 'authors': 'Sagnik Bhattacharya, Muhammad Ahmed Mohsin, Ahsan Bilal, John M. Cioffi', 'link': 'https://arxiv.org/abs/2501.17879', 'abstract': 'Emerging wireless AR/VR applications require real-time transmission of correlated high-fidelity speech from multiple resource-constrained devices over unreliable, bandwidth-limited channels. Existing autoencoder-based speech source coding methods fail to address the combination of the following - (1) dynamic bitrate adaptation without retraining the model, (2) leveraging correlations among multiple speech sources, and (3) balancing downstream task loss with realism of reconstructed speech. We propose a neural distributed principal component analysis (NDPCA)-aided distributed source coding algorithm for correlated speech sources transmitting to a central receiver. Our method includes a perception-aware downstream task loss function that balances perceptual realism with task-specific performance. Experiments show significant PSNR improvements under bandwidth constraints over naive autoencoder methods in task-agnostic (19%) and task-aware settings (52%). It also approaches the theoretical upper bound, where all correlated sources are sent to a single encoder, especially in low-bandwidth scenarios. Additionally, we present a rate-distortion-perception trade-off curve, enabling adaptive decisions based on application-specific realism needs.', 'abstract_zh': '新兴的无线AR/VR应用需要在资源受限的多个设备之间通过不可靠且带宽受限的信道进行实时传输高度保真度的关联语音。现有的基于自编码器的语音源编码方法未能解决以下问题——(1) 动态比特率适应而无需重新训练模型；(2) 利用多个语音源之间的关联性；以及(3) 平衡重建语音的现实感与下游任务性能之间的关系。我们提出了一种基于神经分布式主成分分析（NDPCA）的分布式源编码算法，用于将关联语音源传输到中央接收器。该方法包含一个感知感知的下游任务损失函数，能够平衡听觉现实感与任务特定性能之间的关系。实验结果显示，在带宽受限条件下，与原始自编码器方法相比，在任务无关情况下显著提升了46%的PSNR，在任务相关情况下提升了82%的PSNR。此外，在低带宽场景中，该方法接近理论上限，即所有关联源都传输到单一编码器。我们还提出了一个比特率-失真-感知失真权衡曲线，允许根据特定应用的现实需求做出自适应决策。', 'title_zh': '针对带宽受限信道中相关语音的感知和任务驱动分布式源编码'}
