{'arxiv_id': 'arXiv:2502.19416', 'title': 'Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing', 'authors': 'Akshat Gupta, Christine Fang, Atahan Ozdemir, Maochuan Lu, Ahmed Alaa, Thomas Hartvigsen, Gopala Anumanchipalli', 'link': 'https://arxiv.org/abs/2502.19416', 'abstract': 'This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model . We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.', 'abstract_zh': '本研究探讨了对大型语言模型（LLMs）进行局部更新的影响，特别是在知识编辑任务中的影响，该任务旨在引入或修改特定事实而不影响模型的其他功能。我们首先展示了，在不同的后训练干预措施中，如连续预训练、全面微调和基于LORA的微调，更新矩阵的弗罗贝尼乌斯范数始终增加。这种范数的增加对局部知识编辑尤为不利，在这种编辑中，只有一部分矩阵被更新。我们揭示了各种编辑技术（包括微调、超网络方法和查找并编辑方法）中的一致现象：更新矩阵的范数在连续更新中始终增加。这种增长破坏了模型的平衡，尤其是在孤立更新特定矩阵而其余模型保持不变的情况下，可能导致潜在的不稳定性和下游性能的退化。通过对中间激活向量的深入研究，我们发现内部激活的范数降低，并且伴随着这些激活所占据子空间的变化，表明这些激活向量在表示空间中现在占据了与未编辑模型完全不同的区域。通过本文，我们提出现有的连续和局部顺序知识编辑所面临的技术挑战及其对保持模型稳定性和实用性的含义。', 'title_zh': '局部序贯知识编辑中的规范化生长与稳定性挑战'}
{'arxiv_id': 'arXiv:2502.19412', 'title': 'The Mighty ToRR: A Benchmark for Table Reasoning and Robustness', 'authors': 'Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer', 'link': 'https://arxiv.org/abs/2502.19412', 'abstract': 'Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt. To address this gap, we create ToRR, a benchmark for Table Reasoning and Robustness, that measures model performance and robustness on table-related tasks. The benchmark includes 10 datasets that cover different types of table reasoning capabilities across varied domains. ToRR goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly, across a variety of common table representation formats. We present a leaderboard as well as comprehensive analyses of the results of leading models over ToRR. Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks. Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples. Overall, our findings show that table understanding and reasoning tasks remain a significant challenge.', 'abstract_zh': '尽管表格数据在现实世界中具有重要意义，但模型在该数据上的表现仍然被忽视，这给确定依赖哪个模型以及采用哪种提示配置带来了不确定性。为了解决这一问题，我们创建了ToRR（Table Reasoning and Robustness Benchmark），这是一个用于测量模型在表格相关任务上的性能和鲁棒性的基准。基准包括10个不同的数据集，涵盖了不同类型的表格推理能力，涉及多个领域。与仅限于模型性能排名不同，ToRR旨在评估模型在多种常见表格表示格式下的一致性鲁棒性。我们还提供了一个排行榜，并对顶级模型在ToRR上的结果进行了详尽分析。结果显示，即使是强大的模型也无法在表格数据任务中表现出鲁棒性。虽然没有具体的表格格式能够稳定地提高性能，但我们证明了在多种格式上进行测试对于可靠地评估模型能力至关重要。此外，我们展示了在多种提示上进行测试带来的可靠性提升与增加测试样本数量效果相当。总之，我们的研究结果表明，表格理解和推理任务仍然是一个重大的挑战。', 'title_zh': '《强大的ToRR：一个表格推理与鲁棒性基准》'}
{'arxiv_id': 'arXiv:2502.19411', 'title': 'Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs', 'authors': 'Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley', 'link': 'https://arxiv.org/abs/2502.19411', 'abstract': "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.", 'abstract_zh': '在大型语言模型（LLMs）中，代码和推理相辅相成：代码提供了抽象化、模块化和逻辑驱动的结构，支持推理；而推理则将高层目标转化为更小的、可执行的步骤，驱动更高级的代码智能。在本研究中，我们探讨了代码作为一种结构化媒介如何增强推理：它提供可验证的执行路径，强制进行逻辑分解，并使运行时验证成为可能。我们还研究了推理改进如何从基本的补全转变为高级能力，使模型能够通过规划和调试来应对复杂的软件工程任务。最后，我们识别了关键挑战，并提出了加强这种协同作用的未来研究方向，从而提高LLM在这两个方面的性能。', 'title_zh': '编程以思考，思考以编程：关于代码增强推理与推理驱动的代码智能在大语言模型中的综述'}
{'arxiv_id': 'arXiv:2502.19363', 'title': 'DataMan: Data Manager for Pre-training Large Language Models', 'authors': 'Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao', 'link': 'https://arxiv.org/abs/2502.19363', 'abstract': "The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by ``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.", 'abstract_zh': '大数据规模定律推动了大规模语言模型（LLMs）的性能涌现，这使得预训练数据的选择变得越来越重要。然而，现有的方法主要依赖有限的经验规则和人类直觉，缺乏全面和明确的指导原则。为了解决这一问题，我们受到“逆向思维”的启发——促使LLMs自我识别哪些标准对其性能有利。由于其预训练能力与困惑度（PPL）相关，我们从文本困惑度异常的原因中推导出14项质量标准，并引入15个常见的应用领域以支持领域混合适用。在本文中，我们训练了一个数据管理器（DataMan），使其能够从点评分中学习质量评级和领域识别，并用于对一个包含447亿标记的预训练语料库进行注释，附带14项质量评级和领域类型。我们的实验验证了这种方法的有效性，使用DataMan筛选出30亿标记来训练一个参数量为1.3亿的语言模型，显示出在上下文中学习（ICL）、困惑度和指令遵循能力方面显著优于最先进的基线模型。基于整体评分l=5的最佳模型超越了使用50%更多数据并通过均匀采样训练的模型。我们继续使用DataMan标注的高质量、领域特定的数据进行预训练，以增强特定领域的ICL性能，从而验证了DataMan的领域混合适用能力。我们的研究结果强调了质量排名的重要性、质量标准之间的互补性及其与困惑度的低相关性，并分析了PPL和ICL性能之间的不一致。我们还深入分析了预训练数据集，考察了其构成、质量评级的分布以及原始文档来源。', 'title_zh': 'DataMan：预训练大型语言模型的数据管理员'}
{'arxiv_id': 'arXiv:2502.19361', 'title': 'Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?', 'authors': 'Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.19361', 'abstract': 'Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.', 'abstract_zh': '近年来，o1-like模型受到了广泛关注，这些模型能够生成较长的链式思维（Chain-of-Thought, CoT）推理步骤，以提高现有大型语言模型（LLMs）的推理能力。本文旨在理解这些长CoT的质量，并评估现有LLMs在处理这些长CoT时的批判能力。为此，我们引入了DeltaBench，其中包括来自不同o1-like模型（例如QwQ、DeepSeek-R1）的生成长CoT，用于不同推理任务（如数学、代码、一般推理），以衡量检测长CoT推理错误的能力。基于DeltaBench，我们首先对生成的长CoT进行细致分析，以发现不同o1-like模型的有效性和效率。然后，我们对现有的过程奖励模型（PRMs）和批判模型进行了广泛的评估，以检测每一步骤的错误，这旨在研究现有PRMs和批判模型的边界和限制。最后，我们希望通过DeltaBench指导模型开发人员更好地理解其模型的长CoT推理能力。', 'title_zh': '大型语言模型能否检测长链式推理中的错误？'}
{'arxiv_id': 'arXiv:2502.19347', 'title': 'Controlled Diversity: Length-optimized Natural Language Generation', 'authors': 'Diana Marie Schenke, Timo Baumann', 'link': 'https://arxiv.org/abs/2502.19347', 'abstract': "LLMs are not generally able to adjust the length of their outputs based on strict length requirements, a capability that would improve their usefulness in applications that require adherence to diverse user and system requirements. We present an approach to train LLMs to acquire this capability by augmenting existing data and applying existing fine-tuning techniques, which we compare based on the trained models' adherence to the length requirement and overall response quality relative to the baseline model. Our results demonstrate that these techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements. Our results indicate that our method may change the response quality when using training data that was not generated by the baseline model. This allows simultaneous alignment to another training objective in certain scenarios, but is undesirable otherwise. Training on a dataset containing the model's own responses eliminates this issue.", 'abstract_zh': '大语言模型通常无法根据严格的长度要求调整其输出长度，这种能力对于需要遵守多样用户和系统要求的应用场景来说可以提高它们的实用性。我们提出了一种训练大语言模型获得此能力的方法，通过扩展现有数据和应用现有的微调技术，并基于训练模型对长度要求的遵守程度及其整体响应质量与基线模型的比较，对这些技术进行了评估。我们的结果表明，这些技术可以成功应用于训练大语言模型以遵守长度要求，训练后的模型生成的文本更好地符合了长度要求。我们的结果表明，当使用基线模型未生成的数据进行训练时，我们的方法可能会改变响应质量。这在某些场景下允许同时满足另一个训练目标，但否则是不可取的。使用包含模型自身响应的数据集进行训练可消除这一问题。', 'title_zh': '可控多样性：长度优化的自然语言生成'}
{'arxiv_id': 'arXiv:2502.19339', 'title': 'Evaluating LLMs and Pre-trained Models for Text Summarization Across Diverse Datasets', 'authors': 'Tohida Rehman, Soumabha Ghosh, Kuntal Das, Souvik Bhattacharjee, Debarshi Kumar Sanyal, Samiran Chattopadhyay', 'link': 'https://arxiv.org/abs/2502.19339', 'abstract': "Text summarization plays a crucial role in natural language processing by condensing large volumes of text into concise and coherent summaries. As digital content continues to grow rapidly and the demand for effective information retrieval increases, text summarization has become a focal point of research in recent years. This study offers a thorough evaluation of four leading pre-trained and open-source large language models: BART, FLAN-T5, LLaMA-3-8B, and Gemma-7B, across five diverse datasets CNN/DM, Gigaword, News Summary, XSum, and BBC News. The evaluation employs widely recognized automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and METEOR, to assess the models' capabilities in generating coherent and informative summaries. The results reveal the comparative strengths and limitations of these models in processing various text types.", 'abstract_zh': '文本摘要在自然语言处理中发挥着至关重要的作用，通过将大量文本浓缩为简洁连贯的摘要。随着数字内容的迅速增长以及有效信息检索需求的增加，文本摘要近年来成为了研究的焦点。本研究对四种领先的预训练开源大型语言模型——BART、FLAN-T5、LLaMA-3-8B 和 Gemma-7B——在五个不同的数据集上进行了全面评估：CNN/DM、Gigaword、News Summary、XSum 和 BBC News。评估采用广泛认可的自动评价指标，包括 ROUGE-1、ROUGE-2、ROUGE-L、BERTScore 和 METEOR，以评估模型生成连贯且信息丰富的摘要的能力。结果显示，这些模型在处理不同文本类型时各有优势和局限性。', 'title_zh': '评估跨多个多样化的数据集的大型语言模型和预训练模型在文本摘要中的表现'}
{'arxiv_id': 'arXiv:2502.19328', 'title': 'Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems', 'authors': 'Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, Juanzi Li', 'link': 'https://arxiv.org/abs/2502.19328', 'abstract': 'Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (this https URL).', 'abstract_zh': '奖励模型（RMs）对于大型语言模型（LLMs）的训练和推理时间上的扩展至关重要。然而，现有的奖励模型主要关注人类偏好，忽视了具有强烈潜在价值的可验证正确性信号。本文中，我们提出了代理奖励建模，这是一种结合了来自不同方面可验证正确性信号的奖励系统，以提供可靠的奖励。我们通过结合人的偏好奖励与两个可验证信号——事实性与指令遵循性来实践了一个奖励代理模型，名为RewardAgent，以提供更可靠的奖励。我们在现有的奖励模型基准测试和实际下游任务的最佳n搜索中进行了全面的实验。RewardAgent 显著优于传统的奖励模型，证明了其有效性。我们进一步利用RewardAgent 构建训练偏好对，并使用DPO目标对LLM进行训练，与传统的奖励模型相比，该方法在多种自然语言处理（NLP）基准测试中表现出优越的性能。我们的代码已公开发布，以促进进一步的研究（请点击此处 https://...）。', 'title_zh': '代理奖励建模：将人类偏好与可验证正确性信号整合以构建可靠的奖励系统'}
{'arxiv_id': 'arXiv:2502.19320', 'title': "Shh, don't say that! Domain Certification in LLMs", 'authors': 'Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Kayser, Tom Rainforth, Thomas Lukasiewicz, Bernard Ghanem, Philip H.S. Torr, Adel Bibi', 'link': 'https://arxiv.org/abs/2502.19320', 'abstract': 'Large language models (LLMs) are often deployed to perform constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible, potentially generating outputs outside the intended domain. To formalize, assess, and mitigate this risk, we introduce domain certification; a guarantee that accurately characterizes the out-of-domain behavior of language models. We then propose a simple yet effective approach, which we call VALID that provides adversarial bounds as a certificate. Finally, we evaluate our method across a diverse set of datasets, demonstrating that it yields meaningful certificates, which bound the probability of out-of-domain samples tightly with minimum penalty to refusal behavior.', 'abstract_zh': '大型语言模型（LLMs）通常被部署用于执行受限任务，并且这些任务具有窄领域的特点。例如，可以在LLMs的基础上构建客户服务机器人，利用其广泛的语言理解和能力来提升性能。然而，这些LLMs具有对抗性脆弱性，可能会生成超出预设领域范围的输出。为了形式化、评估并缓解这一风险，我们提出了领域认证的概念；这是一种能够准确描述语言模型超出领域行为的保证。随后，我们提出了一种简单而有效的方法，称为VALID，该方法提供对抗性边界作为一种证书。最后，我们在多种数据集中评估了该方法，结果表明，该方法能够生成有意义的证书，这些证书能够紧密地限制超出领域样本的概率，同时对拒绝行为的负面影响最小。', 'title_zh': '《嘘，不要说那个！在大型语言模型中进行领域认证》'}
{'arxiv_id': 'arXiv:2502.19279', 'title': 'CritiQ: Mining Data Quality Criteria from Human Preferences', 'authors': 'Honglin Guo, Kai Lv, Qipeng Guo, Tianyi Liang, Zhiheng Xi, Demin Song, Qiuyinzhe Zhang, Yu Sun, Kai Chen, Xipeng Qiu, Tao Gui', 'link': 'https://arxiv.org/abs/2502.19279', 'abstract': 'Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only $\\sim$30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.', 'abstract_zh': '语言模型的高度依赖高质量的数据以实现最佳性能。现有的方法依赖于手工设计的启发式规则、现有模型的困惑度、训练分类器或精细的提示工程，这些方法需要大量的专家经验和人工标注努力，同时引入了偏见。我们提出了CritiQ，一种新颖的数据选择方法，它仅需要约30个手工标注的样本对即可自动挖掘数据质量的标准，并进行高效的数据选择。CritiQ的主要组件CritiQ Flow包含一个管理员代理来演化质量标准，并使用工人代理进行两两判断。我们构建了一个知识库，从以往的工作中提取质量标准，以增强CritiQ Flow的效果。与基于困惑度和分类器的方法相比，语言标准更加可解释，并具有可重用的价值。在提取出标准后，我们训练了CritiQ Scorer来给出质量评分并进行高效的数据选择。我们在代码、数学和逻辑领域展示了我们方法的有效性，且在人类标注的数据集上取得了高精度。为了验证所选数据的质量，我们持续训练Llama 3.1模型，并观察到与均匀采样相比，下游任务性能有所提高。消融实验验证了知识库和反思过程的好处。我们分析了标准的演变过程以及多数投票的有效性。', 'title_zh': 'CritiQ: 从人类偏好中挖掘数据质量标准'}
{'arxiv_id': 'arXiv:2502.19276', 'title': 'Disentangled VAD Representations via a Variational Framework for Political Stance Detection', 'authors': 'Beiyu Xu, Zhiwei Liu, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.19276', 'abstract': 'The stance detection task aims to categorise the stance regarding specified targets. Current methods face challenges in effectively integrating sentiment information for stance detection. Moreover, the role of highly granular sentiment labelling in stance detection has been largely overlooked. This study presents a novel stance detection framework utilizing a variational autoencoder (VAE) to disentangle latent emotional features-value, arousal, and dominance (VAD)-from political discourse on social media. This approach addresses limitations in current methods, particularly in in-target and cross-target stance detection scenarios. This research uses an advanced emotional annotation tool to annotate seven-class sentiment labels for P-STANCE. Evaluations on benchmark datasets, including P-STANCE and SemEval-2016, reveal that PoliStance-VAE achieves state-of-the-art performance, surpassing models like BERT, BERTweet, and GPT-4o. PoliStance-VAE offers a robust and interpretable solution for stance detection, demonstrating the effectiveness of integrating nuanced emotional representations. This framework paves the way for advancements in natural language processing tasks, particularly those requiring detailed emotional understanding.', 'abstract_zh': '态度检测任务旨在对指定目标的态度进行分类。当前的方法在有效整合情感信息以进行态度检测方面面临着挑战。此外，高度细化的情感标签在态度检测中的作用在很大程度上被忽视了。本文提出了一种新的态度检测框架，利用变分自编码器（VAE）从社交媒体上的政治话语中分离出潜在的情感特征——价值、唤醒度和支配感（VAD）。该方法解决了当前方法中的局限性，特别是在针对目标内和跨目标的态度检测场景中。本研究使用了先进的情感标注工具对P-STANCE进行七类情感标签的标注。在P-STANCE和SemEval-2016等基准数据集上的评估表明，PoliStance-VAE达到业界领先的表现，超越了如BERT、BERTweet和GPT-4o等模型。PoliStance-VAE提供了一种稳健且可解释的解决方案，展示了在整合细腻的情感表示方面的有效性。该框架为自然语言处理任务中的进步铺平了道路，特别是在需要详细情感理解的情况下。', 'title_zh': '基于变分框架的去纠缠语音表示及其在政治立场检测中的应用'}
{'arxiv_id': 'arXiv:2502.19261', 'title': 'Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization', 'authors': 'Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, Jun Suzuki', 'link': 'https://arxiv.org/abs/2502.19261', 'abstract': "The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. Extensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more. As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs. All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE.", 'abstract_zh': '与同等容量的密集模型相比，《专家混合（Mixture of Experts, MoE）》架构能够显著降低训练和推断的成本。循环利用是一种方法，通过利用预训练的密集模型来初始化和训练MoE模型。虽然循环利用可以带来初始性能的提升，但其训练速度通常会慢于从头开始训练，长期来看会导致性能不佳。为此，我们提出了一种名为Drop-Upcycling的方法，有效地解决了这个问题。Drop-Upcycling结合了两种看似矛盾的方法：利用预训练密集模型的知识，同时通过统计重新初始化部分权重。这种方法有目的地促进了专家的专业化，显著提升了MoE模型在知识获取方面的效率。大规模的实验结果表明，Drop-Upcycling在长期性能上显著优于之前的所有MoE构建方法，特别是在使用数百亿甚至更多的令牌进行训练时。因此，我们的MoE模型在保持了与13亿参数模型相当的性能的同时，大约只需要四分之一的训练FLOPs。所有实验资源，包括源代码、训练数据、模型检查点和日志，均已公开，以促进MoE的重现性和未来研究。', 'title_zh': '滴落-升级：部分重新初始化训练稀疏专家混合模型'}
{'arxiv_id': 'arXiv:2502.19249', 'title': 'Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases', 'authors': 'Michael Y. Hu, Jackson Petty, Chuan Shi, William Merrill, Tal Linzen', 'link': 'https://arxiv.org/abs/2502.19249', 'abstract': "Pretraining language models on formal languages can improve their acquisition of natural language, but it is unclear which features of the formal language impart an inductive bias that leads to effective transfer. Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when the formal language both captures dependency structures in natural language and remains within the computational limitations of the model architecture. Focusing on transformers, we find that formal languages with both these properties enable language models to achieve lower loss on natural language and better linguistic generalization compared to other languages. In fact, pre-pretraining, or training on formal-then-natural language, reduces loss more efficiently than the same amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. We also give mechanistic evidence of cross-task transfer from formal to natural language: attention heads acquired during formal language pretraining remain crucial for the model's performance on syntactic evaluations.", 'abstract_zh': '在形式语言上预训练语言模型可以改善其对自然语言的获取能力，但尚不清楚哪些形式语言的特征能够促进有效的迁移学习。借鉴语言学和复杂性理论的见解，我们假设当形式语言能够捕捉自然语言中的依存结构，并且不超出模型架构的计算限制时，有效的迁移学习便会发生。专注于变压器模型，我们发现具备这两种特性的形式语言能使语言模型在自然语言上的损失更低，同时在语言通用性方面表现更好，相较于其他语言而言是如此。事实上，形式语言预训练后再进行自然语言训练，相比同等数量的自然语言训练，能够更有效地降低损失。对于一个参数量为1亿的语言模型，在大约16亿个自然语言标记上进行训练，形式语言预训练加自然语言训练能够在更小的标记预算（节省33%）下达到相同的损失值和更好的语言通用性。我们还提供了跨任务迁移学习的机制性证据：在形式语言预训练期间获得的注意力头对于模型在句法评估中的表现仍然至关重要。', 'title_zh': '电路与乔姆斯基之间：在正式语言上进行预训练赋予了语言偏见'}
{'arxiv_id': 'arXiv:2502.19230', 'title': 'Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time', 'authors': 'Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, Yulan He', 'link': 'https://arxiv.org/abs/2502.19230', 'abstract': 'Large Language Models (LLMs) often struggle with complex reasoning scenarios. While preference optimization methods enhance reasoning performance through training, they often lack transparency in why one reasoning outcome is preferred over another. Verbal reflection techniques improve explainability but are limited in LLMs\' critique and refinement capacity. To address these challenges, we introduce a contrastive reflection synthesis pipeline that enhances the accuracy and depth of LLM-generated reflections. We further propose a dual-model reasoning framework within a verbal reinforcement learning paradigm, decoupling inference-time self-reflection into specialized, trained models for reasoning critique and refinement. Extensive experiments show that our framework outperforms traditional preference optimization methods across all evaluation metrics. Our findings also show that "two heads are better than one", demonstrating that a collaborative Reasoner-Critic model achieves superior reasoning performance and transparency, compared to single-model approaches.', 'abstract_zh': '大型语言模型（LLMs）在处理复杂推理场景时常常存在困难。虽然偏好优化方法能够通过训练提升推理性能，但通常缺乏一种清晰的解释，说明为何某个推理结果被优先考虑。口头反思技术虽然可以提高解释性，但它们在LLMs的批评和改进方面能力有限。为了解决这些问题，我们提出了一种对比性反思合成流水线，该流水线能够提升LLM生成的反思的准确性和深度。此外，我们还在口头强化学习范式下提出了一种双模型推理框架，将推理时间的自我反思分拆为专门培训的模型，用于推理批评和改进。大量的实验表明，我们的框架在所有评估指标上都优于传统的偏好优化方法。我们的研究结果还显示，“双模型优于单模型”，表明合作式的推理-批评模型在推理性能和透明度方面显著优于单模型方法。', 'title_zh': '两个头胜过一个头：推理时的双模型语言反思'}
{'arxiv_id': 'arXiv:2502.19211', 'title': 'Negation-Induced Forgetting in LLMs', 'authors': 'Francesca Capuano, Ellen Boschert, Barbara Kaup', 'link': 'https://arxiv.org/abs/2502.19211', 'abstract': 'The study explores whether Large Language Models (LLMs) exhibit negation-induced forgetting (NIF), a cognitive phenomenon observed in humans where negating incorrect attributes of an object or event leads to diminished recall of this object or event compared to affirming correct attributes (Mayo et al., 2014; Zang et al., 2023). We adapted Zang et al. (2023) experimental framework to test this effect in ChatGPT-3.5, GPT-4o mini and Llama3-70b-instruct. Our results show that ChatGPT-3.5 exhibits NIF, with negated information being less likely to be recalled than affirmed information. GPT-4o-mini showed a marginally significant NIF effect, while LLaMA-3-70B did not exhibit NIF. The findings provide initial evidence of negation-induced forgetting in some LLMs, suggesting that similar cognitive biases may emerge in these models. This work is a preliminary step in understanding how memory-related phenomena manifest in LLMs.', 'abstract_zh': '本研究探讨了大型语言模型（LLMs）是否表现出否定诱导遗忘（NIF）效应，这是一种在人类中观察到的认知现象，即否定对象或事件的错误属性会导致对该对象或事件的回忆减弱，相比之下，对正确的属性予以肯定则不会出现这种情况（Mayo et al., 2014；Zang et al., 2023）。我们根据Zang等人（2023）的实验框架，测试了这一效应在ChatGPT-3.5、GPT-4o-mini和LLama3-70B-instruct中的表现。研究结果表明，ChatGPT-3.5表现出NIF效应，被否定的信息比被肯定的信息更不易被回忆起来。GPT-4o-mini显示出边缘显著的NIF效应，而LLaMA-3-70B则未表现出NIF效应。这些发现为部分LLMs中存在否定诱导遗忘提供了初步证据，表明这些模型中可能存在类似的认知偏差。这项工作是理解如何在LLMs中表现记忆相关现象的一个初步步骤。', 'title_zh': '大型语言模型中的否定诱导遗忘现象'}
{'arxiv_id': 'arXiv:2502.19209', 'title': "Bi'an: A Bilingual Benchmark and Model for Hallucination Detection in Retrieval-Augmented Generation", 'authors': 'Zhouyu Jiang, Mengshu Sun, Zhiqiang Zhang, Lei Liang', 'link': 'https://arxiv.org/abs/2502.19209', 'abstract': "Retrieval-Augmented Generation (RAG) effectively reduces hallucinations in Large Language Models (LLMs) but can still produce inconsistent or unsupported content. Although LLM-as-a-Judge is widely used for RAG hallucination detection due to its implementation simplicity, it faces two main challenges: the absence of comprehensive evaluation benchmarks and the lack of domain-optimized judge models. To bridge these gaps, we introduce \\textbf{Bi'an}, a novel framework featuring a bilingual benchmark dataset and lightweight judge models. The dataset supports rigorous evaluation across multiple RAG scenarios, while the judge models are fine-tuned from compact open-source LLMs. Extensive experimental evaluations on Bi'anBench show our 14B model outperforms baseline models with over five times larger parameter scales and rivals state-of-the-art closed-source LLMs. We will release our data and models soon at this https URL.", 'abstract_zh': "检索增强生成（RAG）方法有效地减少了大规模语言模型（LLMs）的幻觉现象，但在某些情况下仍会产生不一致或缺乏支持的内容。尽管语言模型作为评判者（LLM-as-a-Judge）由于其实现简单而广泛用于RAG幻觉检测，但这种方法面临两大主要挑战：缺乏全面的评估基准和缺乏针对特定领域的评判模型。为了弥补这些不足，我们引入了**Bi'an**，这是一个新颖的框架，包含双语基准数据集和轻量级评判模型。该数据集支持对多种RAG场景进行严格的评估，而评判模型则基于紧凑的开源LLMs微调。在Bi'anBench上的广泛实验评估显示，我们的14B模型在参数规模超过五倍大的基线模型中表现更优，并且与最先进的闭源LLMs不相上下。我们很快将公布我们的数据和模型，您可以在此链接访问：[非常抱歉，此处应有一个具体的链接地址，但在当前的文本环境中无法展示链接]。", 'title_zh': "Bi'an：一种用于检索增强生成中幻觉检测的双语基准和模型"}
{'arxiv_id': 'arXiv:2502.19208', 'title': "MultiConAD: A Unified Multilingual Conversational Dataset for Early Alzheimer's Detection", 'authors': 'Arezo Shakeri, Mina Farmanbar, Krisztian Balog', 'link': 'https://arxiv.org/abs/2502.19208', 'abstract': "Dementia is a progressive cognitive syndrome with Alzheimer's disease (AD) as the leading cause. Conversation-based AD detection offers a cost-effective alternative to clinical methods, as language dysfunction is an early biomarker of AD. However, most prior research has framed AD detection as a binary classification problem, limiting the ability to identify Mild Cognitive Impairment (MCI)-a crucial stage for early intervention. Also, studies primarily rely on single-language datasets, mainly in English, restricting cross-language generalizability. To address this gap, we make three key contributions. First, we introduce a novel, multilingual dataset for AD detection by unifying 16 publicly available dementia-related conversational datasets. This corpus spans English, Spanish, Chinese, and Greek and incorporates both audio and text data derived from a variety of cognitive assessment tasks. Second, we perform finer-grained classification, including MCI, and evaluate various classifiers using sparse and dense text representations. Third, we conduct experiments in monolingual and multilingual settings, finding that some languages benefit from multilingual training while others perform better independently. This study highlights the challenges in multilingual AD detection and enables future research on both language-specific approaches and techniques aimed at improving model generalization and robustness.", 'abstract_zh': '痴呆是一种进行性认知障碍综合征，阿尔茨海默病（AD）是其中最常见的类型。基于对话的AD检测提供了一种临床方法的经济有效替代方案，因为语言功能障碍是AD的早期生物标志物。然而，大多数早期研究将AD检测视为二元分类问题，这限制了对轻度认知障碍（MCI）的识别能力——这是早期干预的关键阶段。此外，研究主要依赖单语数据集，主要为英语数据集，这限制了跨语言的一般性。为填补这一空白，我们做出了三个关键贡献。首先，我们通过统一16个公开的痴呆相关对话数据集，创建了一个新的多语言AD检测数据集。该语料库涵盖了英语、西班牙语、汉语和希腊语，并包含了来自各种认知评估任务的音频和文本数据。其次，我们进行了更精细的分类，包括MCI，并使用稀疏和稠密文本表示法评估了多种分类器。最后，我们在单语和多语环境进行了实验，发现某些语言从多语训练中受益，而另一些语言则独立训练效果更好。本研究突出了多语言AD检测的挑战，并促进了对语言特定方法和技术的研究，这些方法和技术旨在提高模型的普适性和鲁棒性。', 'title_zh': 'MultiConAD：一种统一的多语言对话数据集，用于早期阿尔茨海默病检测'}
{'arxiv_id': 'arXiv:2502.19207', 'title': 'FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge', 'authors': 'Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung', 'link': 'https://arxiv.org/abs/2502.19207', 'abstract': 'Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.', 'abstract_zh': '各种研究试图从语言模型中移除敏感或私人知识，以防止其未经授权的曝光。然而，先前的研究忽视了知识的复杂性和相互关联性，即相关的知识必须仔细检查。具体而言，它们未能评估消除方法是否忠实于删除应当移除的相互关联知识，同时也未能保留那些看似相关但实际上存在于完全不同上下文中的知识。为解决这一问题，我们首先提出一个新的概念——表面性遗忘，它指的是消除方法未能正确删除应当移除的相互关联知识，或者无意中删除了不相关知识的现象。基于这一定义，我们引入了一个新的基准 FaithUn，以分析和评估现实世界知识问答（QA）设置中的消除方法的忠实性。此外，我们提出了一种新的消除方法 KLUE，它仅更新与知识相关的神经元，以实现忠实的消除。KLUE 使用可解释性方法识别知识神经元，并仅使用选定的未遗忘样本更新这些神经元。实验结果表明，广泛使用的消除方法无法确保忠实的消除，而我们的方法在现实世界知识问答消除中显示出显著的效果。', 'title_zh': 'FaithUn：通过探究知识的互联性以实现语言模型的忠实遗忘'}
{'arxiv_id': 'arXiv:2502.19202', 'title': 'LiGT: Layout-infused Generative Transformer for Visual Question Answering on Vietnamese Receipts', 'authors': 'Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen', 'link': 'https://arxiv.org/abs/2502.19202', 'abstract': '\\textbf{Purpose:} Document Visual Question Answering (document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English.\n\\textbf{Methods:} In this paper, we present ReceiptVQA (\\textbf{Receipt} \\textbf{V}isual \\textbf{Q}uestion \\textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \\textbf{9,000+} receipt images and \\textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\\textbf{L}ayout-\\textbf{i}nfused \\textbf{G}enerative \\textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules.\n\\textbf{Results:} Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models.\n\\textbf{Conclusion:} We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language.', 'abstract_zh': '**目的：** 文档视觉问答（Document Visual Question Answering, Document VQA）挑战多模态系统同时处理文本、布局和视觉模态以提供适当答案。近年来，由于文档数量的增加和对数字化的高需求，Document VQA变得越来越受欢迎。然而，大多数Document VQA数据集都是在诸如英语等高资源语言中开发的。\n**方法：** 在本文中，我们提出了ReceiptVQA（收据视觉问答），这是首个针对收据的大型Document VQA数据集，收据是一种具有高商业潜力的文档类型。该数据集包含**9,000多**张收据图像和**60,000多**个手动标注的问题-答案对。除了我们的研究之外，我们还引入了LiGT（布局-指导生成转换器），这是一种布局感知的编码器-解码器架构，旨在利用语言模型的嵌入层来操作布局嵌入，以最小化额外神经模块的使用。\n**结果：** 收据上的实验结果表明，我们的架构表现出了令人鼓舞的效果，与优秀的基础模型相比达到了竞争力的结果。通过对实验结果的进一步分析，我们发现采用仅编码器架构的模型具有相当大的劣势，相比之下，能够生成答案的架构具有明显的优势。我们还观察到，在处理我们的数据集时，必须结合多种模态信息，尽管语义理解的作用仍然至关重要。\n**结论：** 我们希望我们的工作能够鼓励并促进越南文档视觉问答的发展，为越南语语言的多样化多模态研究社区做出贡献。', 'title_zh': 'LiGT：融合布局信息的生成变压器模型在越南收据上的视觉问答'}
{'arxiv_id': 'arXiv:2502.19187', 'title': 'BIG-Bench Extra Hard', 'authors': 'Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat', 'link': 'https://arxiv.org/abs/2502.19187', 'abstract': 'Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在日常应用中的部署越来越多，这要求它们具备稳健的推理能力和多样化的推理技能。然而，目前针对LLM推理的基准测试主要侧重于数学和编程能力，这在评估更广泛推理能力方面存在不足。其中的一个例外是BIG-Bench数据集，由于其多样且具有挑战性的任务，它为评估LLM的通用推理能力提供了关键基准。BIG-Bench的数据集允许在一个统一框架中全面评估各类技能的通用推理能力。然而，近期LLM技术的进步已经使BIG-Bench和其较难版本BIG-Bench Hard（BBH）趋于饱和，最先进的模型在BBH中许多任务上达到了接近完美的分数，这降低了其实用性。为解决这一局限，我们引入了BIG-Bench Extra Hard（BBEH），这是一个新的基准测试，旨在扩展LLM推理评估的边界。BBEH将BBH中的每个任务替换为一个新的任务，该新任务探测类似的推理能力但表现出显著更高的难度。我们在BBEH上评估了多种模型，观察到最佳通用模型的平均准确率为9.8%，最佳推理专门化模型的平均准确率为44.8%，这表明在LLM中实现稳健的通用推理还有很大的改进空间，并突显了持续存在的挑战。我们已将BBEH公开发布：this https URL。', 'title_zh': '"Big-Bench Extra Hard" 可以翻译成中文为：“Big-Bench 额外困难版”。在学术规范中，为了确保翻译的准确性和专业性，可以根据具体上下文进行适当的调整。如果你需要更详细的翻译或具体的语境，请提供更多的信息。'}
{'arxiv_id': 'arXiv:2502.19175', 'title': 'MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis', 'authors': 'Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence', 'link': 'https://arxiv.org/abs/2502.19175', 'abstract': 'Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.', 'abstract_zh': '差异诊断（DDx）是临床决策中的一项基本但又复杂的工作，医生根据症状、病史和医学知识，逐步精炼可能疾病的排名列表。虽然近年来大型语言模型的发展在支持DDx方面显示出潜力，但现有方法仍面临一些关键限制，包括单数据集评估、组件孤立优化、关于完整患者档案的不现实假设以及单次诊断尝试等。我们提出了一个模块化的可解释性差异诊断代理（MEDDxAgent）框架，旨在实现交互式DDx，其中诊断推理通过逐步学习而非假设完整的患者档案是可获取的。MEDDxAgent集成了三个模块化的组件：(1) 执行器（DDxDriver），(2) 病史采集模拟器，以及(3) 用于知识检索和诊断策略的两个专门代理。为了确保稳健的评估，我们引入了一个全面的DDx基准，涵盖呼吸系统、皮肤和罕见疾病。我们分析了一次性诊断方法的重要性，并展示了在初始时不具备完整的患者档案时，逐步细化的必要性。广泛的评估表明，MEDDxAgent在交互式DDx中实现了超过10%的准确率提升，同时提供了对其诊断推理过程的重要解释性。', 'title_zh': 'MEDDxAgent：一种统一的模块化代理框架，用于可解释的自动鉴别诊断'}
{'arxiv_id': 'arXiv:2502.19163', 'title': 'TestNUC: Enhancing Test-Time Computing Approaches through Neighboring Unlabeled Data Consistency', 'authors': 'Henry Peng Zou, Zhengyao Gu, Yue Zhou, Yankai Chen, Weizhi Zhang, Liancheng Fang, Yibo Wang, Yangning Li, Kay Liu, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.19163', 'abstract': "Test-time computing approaches, which leverage additional computational resources during inference, have been proven effective in enhancing large language model performance. This work introduces a novel, linearly scaling approach, TestNUC, that improves test-time predictions by leveraging the local consistency of neighboring unlabeled data-it classifies an input instance by considering not only the model's prediction on that instance but also on neighboring unlabeled instances. We evaluate TestNUC across eight diverse datasets, spanning intent classification, topic mining, domain discovery, and emotion detection, demonstrating its consistent superiority over baseline methods such as standard prompting and self-consistency. Furthermore, TestNUC can be seamlessly integrated with existing test-time computing approaches, substantially boosting their performance. Our analysis reveals that TestNUC scales effectively with increasing amounts of unlabeled data and performs robustly across different embedding models, making it practical for real-world applications. Our code is available at this https URL.", 'abstract_zh': '利用推理时额外计算资源的测试时计算方法已被证明能够有效提升大规模语言模型的性能。本文介绍了一种新型的线性可扩展方法TestNUC，该方法通过利用相邻未标注数据的局部一致性来提高测试时的预测性能——它不仅考虑模型对该输入实例的预测，还考虑其对相邻未标注实例的预测。我们通过八个不同的数据集对TestNUC进行了评估，涵盖了意图分类、主题挖掘、领域发现和情感检测等多个领域，结果表明TestNUC在这些任务中始终优于标准提示和自我一致性等基线方法。此外，TestNUC可以无缝集成到已有的测试时计算方法中，大大提升了它们的性能。我们的分析表明，TestNUC随着未标注数据量的增加能有效扩展，并且在不同的嵌入模型中表现稳健，使其在实际应用中具有实用性。相关代码已发布于此 <此链接>。', 'title_zh': 'TestNUC：通过未标记邻近数据一致性增强测试时计算方法'}
{'arxiv_id': 'arXiv:2502.19160', 'title': 'Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models', 'authors': 'Rebekka Görge, Michael Mock, Héctor Allende-Cid', 'link': 'https://arxiv.org/abs/2502.19160', 'abstract': 'Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a sentence. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these indicators are present in these sentences and explain the strength of a stereotype. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated behaviors and characteristics. Using more few-shot examples within the prompts, significantly improves performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.', 'abstract_zh': '社会类别和刻板印象嵌入在语言中，并可能引入大型语言模型（LLMs）的数据偏差。尽管采取了防护措施，这些偏差仍然常常在模型行为中持续存在，可能会导致输出的表征性伤害。虽然社会语言学研究为刻板印象的形成提供了有价值的见解，但NLP中的刻板印象检测方法很少利用这些基础，并且往往缺乏客观性、精确性和可解释性。为此，本文提出了一种新方法，用于检测和量化句子中的刻板印象语言指标。我们基于社会类别和刻板印象交流（SCSC）框架，提取出了指示语言中社会类别表述和刻板印象形成的语言指标，并据此构建了一种分类方案。为了自动化这一方法，我们使用基于上下文学习的方法对不同的LLM进行指示，使它们能够将该方法应用于句子，LLM会检查语言特性并为精细评估提供依据。通过对不同语言指标重要性的实证评估，我们学习到一个评分函数，用于衡量语言中的刻板印象指标。我们的注释结果显示，这些指标在刻板印象句子中普遍存在，并解释了刻板印象的程度。从模型性能来看，我们的结果表明，模型在检测和分类用于表示类别标签的语言指标方面通常表现良好，但在正确评估相关行为和特征时有时会遇到困难。通过在提示中使用更多的少量示例，显著提高了性能。随着模型规模的增大，模型性能提升，Llama-3.3-70B-Instruct和GPT-4取得了可比的成绩，超过了Mixtral-8x7B-Instruct、GPT-4-mini和Llama-3.1-8B-Instruct。', 'title_zh': '使用大型语言模型检测刻板印象评估的语言指标'}
{'arxiv_id': 'arXiv:2502.19158', 'title': 'When Personalization Meets Reality: A Multi-Faceted Analysis of Personalized Preference Learning', 'authors': 'Yijiang River Dong, Tiancheng Hu, Yinhong Liu, Ahmet Üstün, Nigel Collier', 'link': 'https://arxiv.org/abs/2502.19158', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) is widely used to align Large Language Models (LLMs) with human preferences, it typically assumes homogeneous preferences across users, overlooking diverse human values and minority viewpoints. Although personalized preference learning addresses this by tailoring separate preferences for individual users, the field lacks standardized methods to assess its effectiveness. We present a multi-faceted evaluation framework that measures not only performance but also fairness, unintended effects, and adaptability across varying levels of preference divergence. Through extensive experiments comparing eight personalization methods across three preference datasets, we demonstrate that performance differences between methods could reach 36% when users strongly disagree, and personalization can introduce up to 20% safety misalignment. These findings highlight the critical need for holistic evaluation approaches to advance the development of more effective and inclusive preference learning systems.', 'abstract_zh': '尽管从人类反馈中进行强化学习（Reinforcement Learning from Human Feedback，RLHF）广泛用于使大型语言模型（Large Language Models，LLMs）与人类偏好保持一致，但通常假设所有用户的偏好是同质的，忽视了人类价值观的多样性以及少数群体的观点。尽管个性化偏好学习通过为每个用户定制不同的偏好来解决这一问题，但该领域缺乏标准化的方法来评估其效果。我们提出了一种多维度的评估框架，不仅衡量性能，还衡量公平性、意外影响和在偏好差异不同层次上的适应性。通过在三个偏好数据集上比较八种个性化方法的广泛实验，我们发现当用户意见严重不同时，方法之间的性能差异可达到36%，个性化可能导致高达20%的安全对齐偏差。这些发现强调了采用全面评估方法以促进更有效和包容性偏好学习系统发展的必要性。', 'title_zh': '当个性化遭遇现实：个性化偏好学习的多维度分析'}
{'arxiv_id': 'arXiv:2502.19148', 'title': 'Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs', 'authors': 'Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang', 'link': 'https://arxiv.org/abs/2502.19148', 'abstract': "How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.", 'abstract_zh': '如何根据静态通用数据集使大型语言模型（LLMs）与用户偏好对齐已经被频繁研究。然而，用户的偏好通常是个性化、变化的，并且在文化、价值观或时间等方面存在多样性。这导致了一个问题：实际用户的偏好往往与模型开发者在实际使用LLMs时训练的偏好不一致。由于我们无法为每一个需求收集足够多的数据并对模型进行重新训练，因此在测试时基于主LLM研究高效的实时偏好自适应方法变得尤为重要。为此，我们引入了Amulet，这是一种新颖的、无需训练的框架，它将每个词元的解码过程形式化为受简单用户提供的提示引导的单独在线学习问题，从而实现针对用户个性化偏好的实时优化。为了减少每次词元优化过程带来的计算成本，我们还为优化过程中的每一迭代步骤提供了闭式解，从而将计算时间成本降低到可以忽略的水平。详细的实验结果表明，Amulet能够在不同LLM、数据集和用户偏好的组合环境中实现显著的性能提升，同时保持可接受的计算效率。', 'title_zh': '灵符：测试时重新对齐以适应个性化偏好的语言模型调整方法'}
{'arxiv_id': 'arXiv:2502.19127', 'title': 'Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement', 'authors': 'Siyuan Zhang, Yichi Zhang, Yinpeng Dong, Hang Su', 'link': 'https://arxiv.org/abs/2502.19127', 'abstract': "Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. While post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its existing memory--the knowledge acquired from pre-training data. We introduce self-memory alignment (SMA), which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments show that SMA significantly improves LLMs' overall performance, with consistent enhancement across various benchmarks concerning factuality, as well as helpfulness and comprehensive skills.", 'abstract_zh': '大型语言模型（LLMs）往往难以使其回答与客观事实保持一致，从而导致事实性幻觉问题，这类问题难以检测且可能误导缺乏相关知识的用户。虽然已经采用了一些后训练技术来解决这一问题，但现有方法通常存在泛化能力差和不同能力之间的权衡。在本文中，我们提出通过直接增强LLM精确利用其现有记忆——预训练数据中获得的知识的基本能力来解决这一问题。我们引入了自我记忆对齐（SMA）的方法，通过对生成的响应进行微调以回答精确且简单的事实性问题，并通过偏好优化来实现。此外，我们构建了FactualBench，这是一个全面且精确的事实性问答数据集，包含跨越21个领域的181,000个中文数据，旨在促进评估和训练。广泛实验证明，SMA 显著提升了LLM的整体性能，在各种涉及事实性、帮助性和综合技能的基准测试中均表现出一致性改进。', 'title_zh': '自我记忆对齐：通过普遍改进减轻事实幻觉'}
{'arxiv_id': 'arXiv:2502.19115', 'title': 'Improving customer service with automatic topic detection in user emails', 'authors': 'Bojana Bašaragin, Darija Medvecki, Gorana Gojić, Milena Oparnica, Dragiša Mišković', 'link': 'https://arxiv.org/abs/2502.19115', 'abstract': "This study introduces a novel Natural Language Processing pipeline that enhances customer service efficiency at Telekom Srbija, a leading Serbian telecommunications company, through automated email topic detection and labelling. Central to the pipeline is BERTopic, a modular architecture that allows unsupervised topic modelling. After a series of preprocessing and post-processing steps, we assign one of 12 topics and several additional labels to incoming emails, allowing customer service to filter and access them through a custom-made application. The model's performance was evaluated by assessing the speed and correctness of the automatically assigned topics across a test dataset of 100 customer emails. The pipeline shows broad applicability across languages, particularly for those that are low-resourced and morphologically rich. The system now operates in the company's production environment, streamlining customer service operations through automated email classification.", 'abstract_zh': '本研究介绍了一种新的自然语言处理管道，通过自动化的邮件主题检测和标签化，提高了塞尔维亚领先的电信公司Telekom Srbija的客户服务效率。该管道的核心是BERTopic，这是一种模块化的架构，允许无监督的主题建模。经过一系列预处理和后处理步骤后，我们为每封新邮件分配一个主题（从12个主题中选择一个）和若干附加标签，使得客户服务能够通过定制的应用程序筛选和访问这些邮件。模型的性能通过评估测试数据集（包含100封客户邮件）中自动分配主题的速度和准确性来进行评价。该管道在多种语言中都表现出广泛的适用性，特别适用于低资源语言和形态丰富语言。该系统现已在公司的生产环境中运行，通过自动化邮件分类来简化客户服务操作。', 'title_zh': '通过自动检测用户电子邮件主题改进客户服务'}
{'arxiv_id': 'arXiv:2502.19110', 'title': 'Conformal Linguistic Calibration: Trading-off between Factuality and Specificity', 'authors': 'Zhengping Jiang, Anqi Liu, Benjamin Van Durme', 'link': 'https://arxiv.org/abs/2502.19110', 'abstract': 'Language model outputs are not always reliable; this prompts research into methods for adapting model responses based on uncertainty. Common approaches include: \\emph{abstention}, where models refrain from generating responses when uncertain; and \\emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unifying view of both approaches, Conformal Linguistic Calibration (CLC), reinterpreting linguistic calibration as answer set prediction. We begin by presenting a unified framework that connects abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation that allows for controlling the level of imprecision in model responses. Experimental results show that our method produces calibrated outputs with conformal guarantees on factual accuracy. Furthermore, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.', 'abstract_zh': '语言模型的输出并非总是可靠；这促使研究人员探索基于不确定性调整模型响应的方法。常见的方法包括：\\emph{放弃}（abstention），即模型在不确定时不生成响应；以及\\emph{语言校准}（linguistic calibration），即模型使用不确定性量化器对其陈述进行自我约束。然而，放弃可能会限制有价值信息的获取，而语言校准化响应往往难以在下游任务中发挥作用。我们提出了一种将这两种方法统一的观点，即统一语言校准（Conformal Linguistic Calibration, CLC），并重新解释语言校准为答案集预测。我们首先通过语言语用学的角度，提出一个统一起点框架，将放弃和语言校准联系起来。然后，我们描述了一种实现方法，允许控制模型响应的不确定程度。实验结果表明，我们的方法能够生成符合事实准确性的校准输出，并且具有确定性的保证。此外，我们的方法使模型能够进行不确定性感知的适应性声明重构，提供了一种关于事实性和具体性的可控平衡。', 'title_zh': '符合学术规范的翻译为：\n\n )* 形而上学语言校准：在事实性和具体性之间的权衡*\n\n这里，“形而上学”用于翻译“Conformal”，这是为了保持专业术语的一致性和准确性。在学术翻译中，保持术语的专业性是非常重要的。如果有特定领域中更常见或被认可的翻译术语，可以根据实际情况进行调整。'}
{'arxiv_id': 'arXiv:2502.19104', 'title': 'Evaluating Gender Bias in German Machine Translation', 'authors': 'Michelle Kappl', 'link': 'https://arxiv.org/abs/2502.19104', 'abstract': 'We present WinoMTDE, a new gender bias evaluation test set designed to assess occupational stereotyping and underrepresentation in German machine translation (MT) systems. Building on the automatic evaluation method introduced by arXiv:1906.00591v1 [cs.CL], we extend the approach to German, a language with grammatical gender. The WinoMTDE dataset comprises 288 German sentences that are balanced in regard to gender, as well as stereotype, which was annotated using German labor statistics. We conduct a large-scale evaluation of five widely used MT systems and a large language model. Our results reveal persistent bias in most models, with the LLM outperforming traditional systems. The dataset and evaluation code are publicly available under this https URL.', 'abstract_zh': '我们提出了WinoMTDE，这是一个新的性别偏见评估测试集，旨在评估德语机器翻译（MT）系统中职业刻板印象和代表性不足的情况。该测试集基于arXiv:1906.00591v1 [cs.CL] 中介绍的自动评估方法，适用于具有语法性别的德语。WinoMTDE数据集包含288个平衡了性别和刻板印象的德语句子，这些句子的性别和刻板印象标注依据了德国劳动力统计数据。我们对五种广泛使用的MT系统和大型语言模型进行了大规模评估。结果显示，大多数模型依然存在偏见，大型语言模型的表现优于传统系统。该数据集和评估代码已公开，可通过以下链接访问：[提供链接]。', 'title_zh': '评估德语机器翻译中的性别偏见'}
{'arxiv_id': 'arXiv:2502.19103', 'title': 'LongEval: A Comprehensive Analysis of Long-Text Generation Through a Plan-based Paradigm', 'authors': 'Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman Shanghaoran Quan Xiaoyong Wei, Riza Batista-Navarro, Chenghua Lin', 'link': 'https://arxiv.org/abs/2502.19103', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success in various natural language processing tasks, yet their ability to generate long-form content remains poorly understood and evaluated. Our analysis reveals that current LLMs struggle with length requirements and information density in long-text generation, with performance deteriorating as text length increases. To quantitively locate such a performance degradation and provide further insights on model development, we present LongEval, a benchmark that evaluates long-text generation through both direct and plan-based generation paradigms, inspired by cognitive and linguistic writing models. The comprehensive experiments in this work reveal interesting findings such as that while model size correlates with generation ability, the small-scale model (e.g., LongWriter), well-trained on long texts, has comparable performance. All code and datasets are released in this https URL.', 'abstract_zh': '大规模语言模型（LLMs）已在各种自然语言处理任务中取得了显著的成功，但在生成长文本内容方面的能力仍然不甚理解且评价不足。我们的分析表明，当前的LLMs在长文本生成时面对长度要求和信息密度方面的挑战，随着文本长度的增加，性能逐渐下降。为了定量识别这种性能退化并为模型开发提供进一步的见解，我们提出了一种新的基准——LongEval，该基准通过直接生成和计划生成两种方式评估长文本生成能力，灵感来源于认知和语言写作模型。本文中的综合实验揭示了有趣的研究结果，例如，虽然模型大小与生成能力相关，但经过长文本充分训练的小规模模型（如LongWriter）在性能上具有可比性。所有代码和数据集均可在此处访问：https://...', 'title_zh': '长文生成综述：基于计划范式的全面分析'}
{'arxiv_id': 'arXiv:2502.19078', 'title': 'Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs', 'authors': 'Yiheng Yang, Yujie Wang, Chi Ma, Lei Yu, Emmanuele Chersoni, Chu-Ren Huang', 'link': 'https://arxiv.org/abs/2502.19078', 'abstract': "Dense large language models(LLMs) face critical efficiency bottlenecks as they rigidly activate all parameters regardless of input complexity. While existing sparsity methods(static pruning or dynamic activation) address this partially, they either lack adaptivity to contextual or model structural demands or incur prohibitive computational overhead. Inspired by human brain's dual-process mechanisms - predictive coding (N400) for backbone sparsity and structural reanalysis (P600) for complex context - we propose CLADA, a \\textit{\\textbf{C}ognitive-\\textbf{L}oad-\\textbf{A}ware \\textbf{D}ynamic \\textbf{A}ctivation} framework that synergizes statistical sparsity with semantic adaptability. Our key insight is that LLM activations exhibit two complementary patterns: 1) \\textit{Global statistical sparsity} driven by sequence-level prefix information, and 2) \\textit{Local semantic adaptability} modulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs a hierarchical thresholding strategy: a baseline from offline error-controlled optimization ensures 40\\%+ sparsity, dynamically adjusted by real-time cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks demonstrate that CLADA achieves \\textbf{~20\\% average speedup with <2\\% accuracy drop}, outperforming Griffin (5\\%+ degradation) and TT (negligible speedup). Crucially, we establish the first formal connection between neurolinguistic event-related potential (ERP) components and LLM efficiency mechanisms through multi-level regression analysis ($R^2=0.17$ for sparsity-adaptation synergy). Requiring no retraining or architectural changes, CLADA offers a deployable solution for resource-aware LLM inference while advancing biologically-inspired AI design. Our code is available at \\href{this https URL}{CLADA}.", 'abstract_zh': '密集的大语言模型（LLMs）面临效率瓶颈，因为它们在处理不同输入复杂度时僵化地激活所有参数。虽然现有的稀疏化方法（如静态剪枝或动态激活）在一定程度上解决了这一问题，但它们要么缺乏对上下文或模型结构需求的适应性，要么会引入不可接受的计算开销。受人脑双重处理机制的启发——预测编码（N400）用于实现基础稀疏性，结构重分析（P600）用于应对复杂上下文——我们提出了一种认知负载感知动态激活框架CLADA，该框架将统计稀疏性与语义适应性相结合。我们的核心洞察是，LLM的激活表现出两种互补的模式：1）由序列级别的前缀信息驱动的全局统计稀疏性，以及2）由认知负荷度量（如意外性和熵）调节的局部语义适应性。CLADA采用分层次的阈值策略：基于离线错误受控优化的基线保证了超过40%的稀疏性，并通过实时认知信号动态调整。在六种主流LLM和九个基准上的评估表明，CLADA实现了约20%的平均加速，同时减少了不到2%的准确率下降，超越了Griffin（5%以上的准确率下降）和TT（几乎没有加速效果）。 crucially，我们通过多级回归分析首次正式建立了神经语言学事件相关电位（ERP）组分与LLM效率机制之间的联系（稀疏性和适应性协同性的$R^2=0.17$）。无需重新训练或架构更改，CLADA提供了一种针对资源感知LLM推理的可部署解决方案，并推进了基于生物启发的AI设计。我们的代码可从这里获取：\\href{this https URL}{CLADA}。', 'title_zh': '稀疏的大脑也同样具有适应性：面向认知负载的动态激活机制在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.19074', 'title': 'Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics', 'authors': 'Aloka Fernando, Surangika Ranathunga, Nisansa de Silva', 'link': 'https://arxiv.org/abs/2502.19074', 'abstract': 'Parallel Data Curation (PDC) techniques aim to filter out noisy parallel sentences from the web-mined corpora. Prior research has demonstrated that ranking sentence pairs using similarity scores on sentence embeddings derived from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT systems with the top-ranked samples, produces superior NMT performance than when trained using the full dataset. However, previous research has shown that the choice of multiPLM significantly impacts the ranking quality. This paper investigates the reasons behind this disparity across multiPLMs. Using the web-mined corpora CCMatrix and CCAligned for En$\\rightarrow$Si, En$\\rightarrow$Ta and Si$\\rightarrow$Ta, we show that different multiPLMs (LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which allows noisy sentences to creep into the top-ranked samples. We show that by employing a series of heuristics, this noise can be removed to a certain extent. This results in improving the results of NMT systems trained with web-mined corpora and reduces the disparity across multiPLMs.', 'abstract_zh': '平行数据校阅（PDC）技术旨在从网络挖掘的语料库中过滤掉噪声平行句子。先前的研究表明，使用预训练多语言语言模型（multiPLMs）提取的句子嵌入相似度得分对句子对进行排名，并用排名靠前的样本训练神经机器翻译（NMT）系统，相较于使用完整数据集训练时，可以获得更出色的NMT性能。然而，先前的研究显示，选择不同的multiPLM显著影响了排名质量。本文探讨了multiPLM之间这种差异的原因。使用CCMatrix和CCAligned语料库（用于En$\\rightarrow$Si、En$\\rightarrow$Ta和Si$\\rightarrow$Ta），我们发现不同的multiPLM（如LASER3、XLM-R和LaBSE）对某些类型的句子有偏见，使得噪声句子有可能进入排名靠前的样本中。通过采用一系列启发式方法，可以在此程度上去除噪声，从而提高使用网络挖掘语料库训练的NMT系统的性能，并减少multiPLM之间的差异。', 'title_zh': '使用去偏见启发式方法提高低资源语言网络挖掘平行语料库的质量'}
{'arxiv_id': 'arXiv:2502.19064', 'title': 'Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique', 'authors': 'Piotr Sawicki, Marek Grześ, Dan Brown, Fabrício Góes', 'link': 'https://arxiv.org/abs/2502.19064', 'abstract': 'The Consensual Assessment Technique (CAT) evaluates creativity through holistic expert judgments. We investigate the use of two advanced Large Language Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a methodology inspired by the CAT. Using a dataset of 90 poems, we found that these LLMs can surpass the results achieved by non-expert human judges at matching a ground truth based on publication venue, particularly when assessing smaller subsets of poems. Claude-3-Opus exhibited slightly superior performance than GPT-4o. We show that LLMs are viable tools for accurately assessing poetry, paving the way for their broader application into other creative domains.', 'abstract_zh': '共识评价技术（Consensual Assessment Technique, CAT）通过整体专家判断来评估创造力。我们研究了使用两种先进的大型语言模型（Large Language Models, LLMs）——Claude-3-Opus 和 GPT-4o——通过借鉴CAT的方法学来评价诗歌的能力。我们使用了90首诗歌的数据集，发现这些LLMs在基于出版平台的标准上，能够超越非专家人类评委的表现，尤其是在评估诗歌的子集时表现更为突出。此外，Claude-3-Opus 的表现略优于GPT-4o。我们展示了LLMs作为一种能够准确评价诗歌的可行工具，并为它们在其他创意领域的广泛应用铺平了道路。', 'title_zh': '大型语言模型能否在诗词评价中超越非专家？基于共识评估技术的比较研究'}
{'arxiv_id': 'arXiv:2502.19058', 'title': 'MathClean: A Benchmark for Synthetic Mathematical Data Cleaning', 'authors': 'Hao Liang, Meiyi Qiang, Yuying Li, Zefeng He, Yongzhen Guo, Zhengzhou Zhu, Wentao Zhang, Bin Cui', 'link': 'https://arxiv.org/abs/2502.19058', 'abstract': 'With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at this https URL.', 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展，训练数据的质量变得至关重要。在各种类型的数据中，数学数据在使LLMs获得强大的推理能力方面发挥着关键作用。虽然高质量的开源数据很重要，但往往不足以进行预训练，因此需要添加合成数学问题。然而，合成数学问题和答案可能会引入不准确性，这不仅会降低训练数据的质量，还会影响网络数据的质量。因此，有效清理合成数学数据的方法至关重要。在本文中，我们提出了MathClean基准，旨在评估数学数据清理模型的有效性。MathClean基准包括2000个正确的问题和2000个错误的问题，以及另外2000个正确和错误的答案，这些答案来源于基于GSM8K和MATH的数据增强。此外，我们还为每个问题或答案标注了错误类型，这有助于评估模型是否能够正确识别错误类别，从而为进一步改进提供依据。最后，我们使用最新的模型进行全面评估。我们的结果显示，即使是强大的模型如GPT-o1和DeepSeek-R1在这个基准上的表现也较差，突显了MathClean的实用价值。我们的代码和数据可在此访问：<https://your-link-here.com>', 'title_zh': 'MathClean: 合成数学数据清理基准'}
{'arxiv_id': 'arXiv:2502.19008', 'title': 'Binary Neural Networks for Large Language Model: A Survey', 'authors': 'Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang', 'link': 'https://arxiv.org/abs/2502.19008', 'abstract': 'Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理（NLP）领域有着广泛的应用，例如GPT-4和Llama。然而，随着模型参数量的指数级增长，LLMs带来了显著的资源开销。低比特量化作为一种关键技术，通过降低模型参数、激活值和梯度的位宽，减少了内存使用和计算需求。对于LLMs的量化，传统的方法主要采用后训练量化（PTQ）和量化感知训练（QAT）。PTQ不需要对原始模型进行重新训练，而QAT则在训练过程中优化精度以获得最佳的量化参数。BitNet团队提出了一种截然不同的方法，即从模型训练的初始阶段就开始进行量化，利用低精度二值权重进行训练。这种方法导致了大量针对大型语言模型的二值量化技术的出现。本文对这些二值量化技术进行了全面的回顾。具体而言，我们将介绍二值量化技术在深度神经网络中的应用，并进一步探讨其在LLMs中的应用，回顾它们的不同贡献、实现方式及其应用情况。', 'title_zh': '大规模语言模型中二值神经网络的研究综述'}
{'arxiv_id': 'arXiv:2502.18993', 'title': 'MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering', 'authors': 'Teng Lin', 'link': 'https://arxiv.org/abs/2502.18993', 'abstract': 'Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs\' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.', 'abstract_zh': '多实体问答（MEQA）对大规模语言模型（LLM）和检索增强生成（RAG）系统构成了重大挑战，这些系统经常难以整合分散在多篇文档中的信息。虽然现有的方法在单文档理解方面表现出色，但在跨文档聚合信息方面往往存在问题，尤其是在处理稠密实体问题（如“ACM院士在各个学科中的分布情况是什么？”）时尤为明显。这些问题需要整合来自异构来源的实体中心见解（如维基百科页面）。为解决这一问题，我们提出了MEBench，一个新颖的多文档、多实体基准测试，旨在系统评估LLM在检索、整合和推理分散信息方面的能力。该基准测试包含4,780个问题，这些问题被系统地归类为三个主要类别，并进一步细分为八种不同的类型，确保涵盖了广泛的真实世界多实体推理场景。我们的实验结果显示最新的LLM（如GPT-4、Llama-3）和RAG流水线存在关键局限性：即使最先进的模型在MBench上的准确率也只有59%。该基准测试强调了在MEQA任务中信息提取的完整性和事实精确度的重要性，并使用实体关联F1（EA-F1）指标进行粒度级别的实体级正确性和归属有效性的评估。MEBench不仅突显了当前LLM框架系统的缺陷，也为构建稳健、实体感知的问答架构提供了基础。', 'title_zh': 'MEBench：评估跨文档多实体问答的大语言模型基准测试'}
{'arxiv_id': 'arXiv:2502.18990', 'title': 'GenTool: Enhancing Tool Generalization in Language Models through Zero-to-One and Weak-to-Strong Simulation', 'authors': 'Jie He, Jennifer Neville, Mengting Wan, Longqi Yang, Hui Liu, Xiaofeng Xu, Xia Song, Jeff Z. Pan, Pei Zhou', 'link': 'https://arxiv.org/abs/2502.18990', 'abstract': 'Large Language Models (LLMs) can enhance their capabilities as AI assistants by integrating external tools, allowing them to access a wider range of information. While recent LLMs are typically fine-tuned with tool usage examples during supervised fine-tuning (SFT), questions remain about their ability to develop robust tool-usage skills and can effectively generalize to unseen queries and tools. In this work, we present GenTool, a novel training framework that prepares LLMs for diverse generalization challenges in tool utilization. Our approach addresses two fundamental dimensions critical for real-world applications: Zero-to-One Generalization, enabling the model to address queries initially lacking a suitable tool by adopting and utilizing one when it becomes available, and Weak-to-Strong Generalization, allowing models to leverage enhanced versions of existing tools to solve queries. To achieve this, we develop synthetic training data simulating these two dimensions of tool usage and introduce a two-stage fine-tuning approach: optimizing tool ranking, then refining tool selection. Through extensive experiments across four generalization scenarios, we demonstrate that our method significantly enhances the tool-usage capabilities of LLMs ranging from 1B to 8B parameters, achieving performance that surpasses GPT-4o. Furthermore, our analysis also provides valuable insights into the challenges LLMs encounter in tool generalization.', 'abstract_zh': '大型语言模型（LLMs）可以通过集成外部工具来增强其作为AI助手的能力，从而访问更广泛的信息。虽然近期的LLMs通常在监督微调（SFT）过程中通过工具使用示例进行微调，但关于它们能否发展出稳健的工具使用技能以及能否有效泛化到未见过的查询和工具的问题仍然存在。在这项工作中，我们提出了GenTool，这是一种新的训练框架，旨在准备LLMs以应对工具使用中的多样泛化挑战。我们的方法针对两个关键维度，这些维度对于实际应用至关重要：从无到有的泛化（zero-to-one generalization），使模型能够解决最初缺乏合适工具的查询，并在工具出现时采用和利用工具；以及从弱到强的泛化（weak-to-strong generalization），使模型能够利用现有工具的增强版本来解决问题。为实现这一目标，我们开发了一种模拟这两种工具使用维度的合成训练数据，并引入了一种两阶段微调方法：优化工具排名，然后完善工具选择。通过在四个泛化场景中的广泛实验，我们证明了我们的方法显著增强了从1B到8B参数各种规模LLMs的工具使用能力，其性能超越了GPT-4o。此外，我们的分析还提供了有关LLMs在工具泛化过程中面临的挑战的宝贵见解。', 'title_zh': 'GenTool：通过零-shot到少-shot和弱监督到强监督仿真增强语言模型中的工具泛化能力'}
{'arxiv_id': 'arXiv:2502.18980', 'title': 'PEToolLLM: Towards Personalized Tool Learning in Large Language Models', 'authors': 'Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18980', 'abstract': "Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.", 'abstract_zh': '工具学习已成为一个有前途的方向，通过将外部工具扩展到大型语言模型（LLMs）的功能中。现有的工具学习研究主要集中在通用工具使用能力上，这种能力解决的是用户指令中的显式需求。然而，这些研究忽略了个性化工具使用能力的重要性，导致无法处理用户的隐性偏好。为了解决这一限制，我们首先定义了个性化工具学习任务，该任务将用户的交互历史整合到个性化工具使用中。为填补缺乏基准数据的空白，我们构建了PEToolBench，该基准包含了三种不同个性化设置下反映多样化用户偏好的交互历史，并涵盖了广泛的工具使用场景。此外，我们提出了一种框架PEToolLLaMA，用于将LLMs适应个性化工具学习任务，该框架通过监督微调和直接偏好优化进行训练。PEToolBench上进行的广泛实验证明了PEToolLLaMA在现有LLMs中的优越性。', 'title_zh': 'PEToolLLM：面向大型语言模型的个性化工具学习方法'}
{'arxiv_id': 'arXiv:2502.18978', 'title': 'Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning', 'authors': 'Hongyi Cal, ie Li, Wenzhen Dong', 'link': 'https://arxiv.org/abs/2502.18978', 'abstract': "The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.", 'abstract_zh': '大型语言模型指令微调的有效性从根本上受到训练数据集质量与效率的限制。本研究引入了一种新颖的过滤框架——低置信度金标准（LCG），该框架利用基于质心的聚类和置信度引导的选择来识别有价值的指令对。通过使用轻量级分类器对代表性样本进行训练，并采用半监督的方法，LCG能够在保持数据多样性的同时，精心挑选高质量的子集。实验评估表明，使用LCG过滤后的6000样本子集进行微调的模型在MT-bench等基准测试上取得了优于现有方法的性能，并且在全面的评估指标上也取得了显著的连续改进。该框架在保持模型性能的同时表现出的效用，为高效的指令微调提供了一个有前景的方向。', 'title_zh': '低置信度金块：精炼低置信度样本以实现高效的指令调优'}
{'arxiv_id': 'arXiv:2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles', 'authors': 'Kuang Wang, Xianfei Li, Shenghao Yang, Li Zhou, Feng Jiang, Haizhou Li', 'link': 'https://arxiv.org/abs/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'abstract_zh': '用户模拟器对于复制与对话系统的人机交互至关重要，对于协同训练和自动评估极具支持作用，尤其是在大型语言模型（LLMs）方面。然而，现有的模拟器往往仅依赖于文本陈述，未能捕捉到隐含的用户特征，如个性、表达风格和目标。相比之下，基于人物设定的方法缺乏通用性，因为它们依赖于名人或原型预先定义的个人资料。为了解决这些问题，我们提出了一种名为用户模拟器带有隐含特征（User Simulator with Implicit Profiles, USP）的框架，该框架通过推理人类机对话中的隐含用户特征，并使用这些特征生成更加个性化和真实的对话。我们首先开发了一个采用大语言模型驱动的提取器，并结合了详尽的个人资料架构。然后，我们通过条件监督微调和具有周期一致性的强化学习改进模拟，从话语和对话层面优化其性能。最后，我们采用了多样化的个人资料采样器以捕捉现实世界用户个人资料的分布。实验结果表明，USP在真实性和多样性方面优于强劲的基线系统，同时实现了在一致性方面的可比性能。此外，基于USP进行的动态多轮评估与主流基准高度一致，展示了其在实际应用中的有效性。', 'title_zh': '了解自我，成为更好的自己：通过隐性特征建模类人类用户模拟器'}
{'arxiv_id': 'arXiv:2502.18940', 'title': 'MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors', 'authors': 'Jakub Macina, Nico Daheim, Ido Hakimi, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan', 'link': 'https://arxiv.org/abs/2502.18940', 'abstract': 'Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.', 'abstract_zh': '评估基于AI的辅导模型的教学能力对于该领域的引导性进步至关重要。然而，我们缺乏一种可靠、易于使用且操作简单的评价方法来反映模型的教学能力。为填补这一空白，我们提出了MathTutorBench，这是一个开源基准，用于全面的辅导模型评价。MathTutorBench 包含了一组广泛覆盖由学习科学研究定义的对话式教学中教师能力的数据集和指标。\n\n为了评分开放性教师回应的质量，我们训练了一个奖励模型，并展示了该模型能够以高准确度区分专家教师和新手教师的回应。我们使用MathTutorBench 对多种闭式和开放式权重模型进行了评估，并发现解决问题的能力（即科目专业知识）并不能立即转化为良好的教学效果。相反，教学历效和科目专业知识似乎形成了一个权衡，这种权衡由模型的辅导专业化程度决定。此外，在更长的对话中，教学似乎变得更加具有挑战性，简单的提问策略开始失效。我们公开发布了这个基准、代码和排行榜，以促进未来模型的快速基准测试。', 'title_zh': 'MathTutorBench: 一个用于评估大规模语言模型辅导工具开放性教学能力的基准测试'}
{'arxiv_id': 'arXiv:2502.18935', 'title': 'JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models', 'authors': 'Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, Xi Zhang', 'link': 'https://arxiv.org/abs/2502.18935', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities across various applications, highlighting the urgent need for comprehensive safety evaluations. In particular, the enhanced Chinese language proficiency of LLMs, combined with the unique characteristics and complexity of Chinese expressions, has driven the emergence of Chinese-specific benchmarks for safety assessment. However, these benchmarks generally fall short in effectively exposing LLM safety vulnerabilities. To address the gap, we introduce JailBench, the first comprehensive Chinese benchmark for evaluating deep-seated vulnerabilities in LLMs, featuring a refined hierarchical safety taxonomy tailored to the Chinese context. To improve generation efficiency, we employ a novel Automatic Jailbreak Prompt Engineer (AJPE) framework for JailBench construction, which incorporates jailbreak techniques to enhance assessing effectiveness and leverages LLMs to automatically scale up the dataset through context-learning. The proposed JailBench is extensively evaluated over 13 mainstream LLMs and achieves the highest attack success rate against ChatGPT compared to existing Chinese benchmarks, underscoring its efficacy in identifying latent vulnerabilities in LLMs, as well as illustrating the substantial room for improvement in the security and trustworthiness of LLMs within the Chinese context. Our benchmark is publicly available at this https URL.', 'abstract_zh': '大语言模型（LLMs）已经在各种应用中展现了惊人的能力，突显了全面安全评估的紧迫需求。特别是，LLMs 对中文语境的增强理解和独特性以及中文表达的复杂性，推动了中文特定安全基准的出现。然而，现有的这些基准通常在有效揭示LLMs的安全漏洞方面存在不足。为填补这一空白，我们引入了JailBench，这是首个专门用于评估LLMs深层次漏洞的全面中文基准，具备针对中文语境精炼的安全分类体系。为了提高生成效率，我们提出了一个新颖的自动脱笼提示工程师（AJPE）框架来构建JailBench，该框架结合了脱笼技术以增强评估效果，并利用LLMs通过语境学习自动扩展数据集。提出的JailBench在13种主流LLMs上进行了广泛评估，相对于现有的中文基准，其对抗成功率达到对ChatGPT的最佳效果，这表明其在识别LLMs中的潜在漏洞方面具有显著效果，同时强调了在中文语境中提高LLMs 安全性和可信度的巨大改进空间。我们的基准已公开发布，网址为[this https URL](this https URL)。', 'title_zh': '《狱 Bench：面向大型语言模型的全面中文安全评估基准》\n\n这个标题翻译旨在保持原意的同时，确保用词符合学术规范和中文表达习惯。其中，“JailBench”被翻译为“狱 Bench”，既保留了原名的独特性，又易于理解；“全面中文安全评估基准”简洁地表达了“Comprehensive Chinese Security Assessment Benchmark”的含义；“大型语言模型”则对应“Large Language Models”。'}
{'arxiv_id': 'arXiv:2502.18934', 'title': 'Kanana: Compute-efficient Bilingual Language Models', 'authors': 'Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee, Minchul Lee, Miok Lee, Shinbok Lee, Gaeun Seo', 'link': 'https://arxiv.org/abs/2502.18934', 'abstract': 'We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.', 'abstract_zh': '我们介绍了Kanana系列双语语言模型，这些模型在韩语方面表现出色，在英语方面亦具有竞争力。Kanana的计算成本显著低于相似规模的最新模型。报告详细阐述了在预训练过程中采用的技术，以实现计算高效且仍具竞争力的模型，包括高质量数据过滤、分阶段预训练、深度扩展和剪枝与蒸馏。此外，报告还概述了在Kanana模型后训练中采用的方法，包括监督微调和偏好优化，旨在增强其与用户的无缝交互能力。最后，报告阐述了适应特定场景的语言模型方法，如嵌入、检索增强生成和函数调用。Kanana模型系列包括参数量从21亿到325亿的多个版本，在2.1亿参数版本（基础版、指令版、嵌入版）已公开发布，以促进韩语语言模型的研究。', 'title_zh': 'Kanana：计算高效型双语语言模型'}
{'arxiv_id': 'arXiv:2502.18915', 'title': 'END: Early Noise Dropping for Efficient and Effective Context Denoising', 'authors': 'Hongye Jin, Pei Chen, Jingfeng Yang, Zhengyang Wang, Meng Jiang, Yifan Gao, Binxuan Huang, Xinyang Zhang, Zheng Li, Tianyi Liu, Huasheng Li, Bing Yin', 'link': 'https://arxiv.org/abs/2502.18915', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \\textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \\textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \\textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.", 'abstract_zh': '大型语言模型（LLMs）在各种自然语言处理任务中表现出色。然而，它们常常受到输入序列中无关或噪声信息的干扰，从而降低了输出质量。这一问题影响了长句和短句的情境，包括检索增强生成、表格问答和上下文学习等场景。我们发现，LLMs可以在生成标记之前，通过早期层隐式地识别输入序列中是否包含有用信息。基于这一认识，我们提出了早期噪声丢弃（Early Noise Dropping，\\textsc{END}）这一新方法，不需 fine-tune LLMs，即可减轻这一问题。\\textsc{END} 将输入序列分割成片段，并在LLMs的早期层使用线性探针来区分信息性片段和噪声片段。通过在过程早期丢弃噪声片段，\\textsc{END} 保留了关键信息，减少了干扰，并降低了计算成本。广泛的实验表明，\\textsc{END} 在多个评估数据集上显著提高了不同LLMs的性能和效率。此外，通过使用探针检查LLMs对输入的隐式理解，这项工作进一步深化了对LLMs如何内部进行情境推理的理解。', 'title_zh': 'END：早期噪声丢弃以实现高效有效的上下文去噪'}
{'arxiv_id': 'arXiv:2502.18913', 'title': 'CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition', 'authors': 'Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin', 'link': 'https://arxiv.org/abs/2502.18913', 'abstract': 'Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios. This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers. Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech. We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.', 'abstract_zh': '代码转换（CS）是指在同一对话中交替使用两种或多种语言的现象，这对自动语音识别（ASR）系统提出了重大挑战。现有汉语-英语代码转换数据集往往在规模、自发性以及缺乏完整的对话录音和转录方面存在不足，阻碍了针对真实场景对话的鲁棒ASR模型的发展。本文介绍了一个名为CS-Dialogue的新型大规模汉语-英语代码转换语音数据集，该数据集包含来自200名发言者、长达104小时的自发对话录音及其完整的转录。与之前的数据集不同，CS-Dialogue提供了完整的对话录音和详尽的转录，能够捕捉连续语音中的自然代码转换模式。我们描述了数据采集和标注过程，展示了数据集的详细统计信息，并使用最先进的模型建立了基准ASR性能。使用Transformer、Conformer和Branchformer进行的实验显示了代码转换ASR的挑战，并表明现有的预训练模型（如Whisper）仍有改进的空间。CS-Dialogue数据集将免费提供用于所有学术研究目的。', 'title_zh': 'CS-对话：一种包含104小时自发 Mandarin-English 双语转换对话的数据集，用于语音识别\n\n解释：\n1. "CS-Dialogue" 保持不变，因为这是专有名词。\n2. "104-Hour Dataset" 翻译为 "包含104小时"，符合学术规范。\n3. "Spontaneous" 翻译为 "自发的"，保持了原文的学术意义。\n4. "Mandarin-English Code-Switching Dialogues" 翻译为 "Mandarin-English 双语转换对话"，"Code-switching" 在语言学领域中通常指双语转换，保持了术语的准确性。\n5. "Speech Recognition" 翻译为 "语音识别"，符合学术规范。\n\n这样的翻译既准确又符合学术表达的习惯。'}
{'arxiv_id': 'arXiv:2502.18890', 'title': 'From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens', 'authors': 'Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng', 'link': 'https://arxiv.org/abs/2502.18890', 'abstract': "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at this https URL.", 'abstract_zh': '生成超长序列（超100K标记）的能力对于大型语言模型（LLMs）来说越来越重要，但仍然是一个高度耗时的任务。传统投机解码方法虽然存在，但仅仅扩展它们的生成极限并不能提高速度，并且可能会带来负面影响。通过深入分析，我们发现有三大挑战阻碍了高效生成：频繁的模型加载、动态键值（KV）管理以及重复生成。为了应对这些问题，我们提出了一种名为TOKENSWIFT的新框架，旨在大幅加速超长序列的生成过程，同时保持目标模型本身的质量。实验结果表明，TOKENSWIFT在不同规模（1.5B、7B、8B、14B）和架构（MHA、GQA）的各种模型中，可以获得超过3倍的速度提升。这种加速使得超长序列生成的时间节省了数个小时，这使TOKENSWIFT成为前所未有的长序列生成中的可扩展且有效的解决方案。相关代码可以在以下链接中找到：[这里](this\xa0https\xa0URL)。', 'title_zh': '从小时到分钟：无损加速超长序列生成，最多至100K tokens'}
{'arxiv_id': 'arXiv:2502.18886', 'title': 'On Pruning State-Space LLMs', 'authors': 'Tamer Ghattas, Michael Hassid, Roy Schwartz', 'link': 'https://arxiv.org/abs/2502.18886', 'abstract': 'Recent work proposed state-space models (SSMs) as an efficient alternative to transformer-based LLMs. Can these models be pruned to further reduce their computation costs? We adapt several pruning methods to the SSM structure, and apply them to four SSM-based LLMs across multiple tasks. We find that such models are quite robust to some pruning methods (e.g. WANDA), while using other methods lead to fast performance degradation.', 'abstract_zh': '最近的研究工作提出了状态空间模型（SSMs）作为变压器基础的大语言模型（LLMs）的高效替代方案。这些模型是否可以通过剪枝进一步减少计算成本？我们借鉴了几种剪枝方法并将其应用于多种任务的四个基于SSM的大语言模型。我们发现，某些剪枝方法（如WANDA）可以使这些模型具有很高的鲁棒性，而使用其他方法则会导致性能快速下降。', 'title_zh': '狀態空間大型語言模型的精简方法研究'}
{'arxiv_id': 'arXiv:2502.18878', 'title': 'Learning to Generate Structured Output with Schema Reinforcement Learning', 'authors': 'Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Zhiyuan Liu, Fangming Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.18878', 'abstract': "This study investigates the structured generation capabilities of large language models (LLMs), focusing on producing valid JSON outputs against a given schema. Despite the widespread use of JSON in integrating language models with programs, there is a lack of comprehensive analysis and benchmarking of these capabilities. We explore various aspects of JSON generation, such as structure understanding, escaping, and natural language description, to determine how to assess and enable LLMs to generate valid responses. Building upon this, we propose SchemaBench features around 40K different JSON schemas to obtain and assess models' abilities in generating valid JSON. We find that the latest LLMs are still struggling to generate a valid JSON string. Moreover, we demonstrate that incorporating reinforcement learning with a Fine-grained Schema Validator can further enhance models' understanding of JSON schema, leading to improved performance. Our models demonstrate significant improvement in both generating JSON outputs and downstream tasks.", 'abstract_zh': '本研究探讨了大型语言模型（LLM）的结构生成能力，重点关注根据给定的模式生成有效的JSON输出。尽管JSON在将语言模型与程序集成方面得到了广泛应用，但对其生成能力的综合分析和基准测试仍显不足。我们研究了JSON生成的各个方面，如结构理解、转义和自然语言描述，以确定如何评估和增强LLM生成有效响应的能力。在此基础上，我们提出了基于约4万个不同JSON模式的SchemaBench特征，以获取和评估模型生成有效JSON的能力。研究发现，最新的LLM仍然难以生成有效的JSON字符串。此外，我们展示了将强化学习与精细粒度的模式验证器结合使用可以进一步增强模型对JSON模式的理解，从而提高性能。我们的模型在生成JSON输出和下游任务中均表现出显著的改进。', 'title_zh': '通过模式强化学习生成结构化输出'}
{'arxiv_id': 'arXiv:2502.18874', 'title': 'Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework', 'authors': 'Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18874', 'abstract': 'Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.', 'abstract_zh': '大规模语言模型（LLMs）在各种场景中被广泛用于自动化评估。先前的研究试图微调开源LLMs，以重现强有力的专业模型（如GPT-4）的评估解释和判断。然而，这些方法主要局限于在预定义的一般标准下的文本分析，这限制了其对未见过指令的适应性，并表现出对定量和结构约束的评估中的不稳定性。为解决这些限制，我们提出了一种新颖的评估框架ARJudge，能够自适应地制定评估标准并综合基于文本和代码的分析来评估LLM的回答。ARJudge 包含两个组件：一个微调的Analyzer生成多维度的评估分析，以及一个无需微调的Refiner组合并优化所有分析以做出最终判断。我们构建了一个集成评估标准生成任务及基于文本和代码的分析生成的综合分析语料库来训练Analyzer。我们的结果显示，ARJudge 在有效性与稳健性方面优于现有的微调评估器。此外，它表明多维度评估和代码驱动的分析对于提升评估能力的重要性。', 'title_zh': '学习多维度评价对齐：一个统一且 robust 的框架'}
{'arxiv_id': 'arXiv:2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'authors': 'Md Mehrab Tanjim, Ryan A. Rossi, Mike Rimer, Xiang Chen, Sungchul Kim, Vaishnavi Muppala, Tong Yu, Zhengmian Hu, Ritwik Sinha, Wei Zhang, Iftikhar Ahamath Burhanuddin, Franck Dernoncourt', 'link': 'https://arxiv.org/abs/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'abstract_zh': '对话助手往往需要一个问答重写算法，以利用过去的部分交互来为用户提供更具有意义（准确）的答案。然而，具体的重写方法可能取决于应用场景以及对话助手支持的具体任务等其他约束。在本文中，我们系统地研究了两种不同的方法，分别为重写和融合，应用于两种基本不同的生成任务，包括文本到文本生成任务和一种多模态生成任务，该任务以文本为输入并生成可视化或数据表以回答用户的问题。研究结果表明，具体的重写或融合方法高度依赖于底层的应用场景和生成任务。特别是，我们发现对于对话型问题解答助手，重写查询的方法效果最佳；而对于基于用户与助手对话生成可视化和数据表的数据分析助手，融合方法效果更佳。值得注意的是，我们在数据分析助手应用场景中探索了两个数据集，分别用于短对话和长对话，并发现查询融合在所有情况下都表现更优，而在基于文本的对话型问题解答中，查询重写方法效果最佳。', 'title_zh': '探索不同对话任务的重写方法'}
{'arxiv_id': 'arXiv:2502.18848', 'title': 'A Causal Lens for Evaluating Faithfulness Metrics', 'authors': 'Kerem Zaman, Shashank Srivastava', 'link': 'https://arxiv.org/abs/2502.18848', 'abstract': "Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present Causal Diagnosticity, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs.", 'abstract_zh': '大型语言模型（LLMs）为模型可解释性提供了自然语言解释，作为特征归因方法的替代方案。然而，尽管这些解释可能具有一定的可信度，它们可能未能忠实地反映模型的内部推理过程，这对于理解模型的真实决策过程至关重要。虽然已经提出了多种可信度度量，但缺乏一个统一的评价框架。为了解决这一缺口，我们提出了因果诊断性（Causal Diagnosticity）框架，用于评价自然语言解释的可信度度量。该框架采用了因果诊断性的概念，并通过模型编辑方法生成忠实与不忠实的解释对。我们的基准包括四个任务：事实核查、类比推理、物体计数和多跳推理。我们评估了多种可信度度量，包括事后解释和基于思维链的方法。我们发现，所有测试的可信度度量经常无法超越随机基线。我们的工作强调了在LLMs中需要改进度量和更可靠解释方法的重要性。', 'title_zh': '一种评估忠实度指标的因果视角'}
{'arxiv_id': 'arXiv:2502.18845', 'title': 'Sliding Window Attention Training for Efficient Large Language Models', 'authors': 'Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2502.18845', 'abstract': 'Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at this https URL.', 'abstract_zh': '基于Transformer的大语言模型（LLMs）在各种任务中的近期进步显示出卓越的能力。然而，它们在处理长文档时由于序列长度导致的平方级计算复杂性仍是一个显著瓶颈。因此，已提出许多方法，如稀疏注意力和状态空间模型，以提高LLMs在长序列上的效率。虽然这些方法在提高效率方面有效，但它们往往牺牲了性能或引入了结构复杂性。因此，需要一种简单而高效的模型，同时保留基础的Transformer架构。为此，我们提出了SWAT，通过滑动窗口注意力训练实现高效处理长上下文。本文首先将Transformer的低效问题归因于softmax操作的高方差导致的注意陷阱现象。然后，我们用sigmoid函数取代softmax，并利用平衡的ALiBi和旋转位置嵌入来实现高效的信息压缩和保留。实验结果表明，SWAT在八个基准测试上优于最先进的线性递归架构，取得了最佳性能。代码可在以下链接获取：this https URL。', 'title_zh': '滑动窗口注意力训练用于高效的大型语言模型'}
{'arxiv_id': 'arXiv:2502.18841', 'title': 'Sentiment Analysis of Movie Reviews Using BERT', 'authors': 'Gibson Nkhata, Usman Anjum, Justin Zhan', 'link': 'https://arxiv.org/abs/2502.18841', 'abstract': 'Sentiment Analysis (SA) or opinion mining is analysis of emotions and opinions from any kind of text. SA helps in tracking peoples viewpoints and it is an important factor when it comes to social media monitoring product and brand recognition customer satisfaction customer loyalty advertising and promotions success and product acceptance. That is why SA is one of the active research areas in Natural Language Processing (NLP). SA is applied on data sourced from various media platforms to mine sentiment knowledge from them. Various approaches have been deployed in the literature to solve the problem. Most techniques devise complex and sophisticated frameworks in order to attain optimal accuracy. This work aims to finetune Bidirectional Encoder Representations from Transformers (BERT) with Bidirectional Long Short-Term Memory (BiLSTM) for movie reviews sentiment analysis and still provide better accuracy than the State-of-The-Art (SOTA) methods. The paper also shows how sentiment analysis can be applied if someone wants to recommend a certain movie for example by computing overall polarity of its sentiments predicted by the model. That is our proposed method serves as an upper-bound baseline in prediction of a predominant reaction to a movie. To compute overall polarity a heuristic algorithm is applied to BERTBiLSTM output vector. Our model can be extended to three-class four-class or any fine-grained classification and apply overall polarity computation again. This is intended to be exploited in future work.', 'abstract_zh': '情感分析（SA）或意见挖掘是从任何类型的文本中分析情绪和观点的过程。SA有助于跟踪人们的观点，并且在社交媒体监控产品和品牌认知、客户满意度、客户忠诚度、广告和促销的成功以及产品接受度方面是一个重要的因素。因此，SA是自然语言处理（NLP）领域的一个活跃研究方向。SA通过对各种媒体平台的数据进行应用，以便从中挖掘情感知识。文献中已经部署了多种方法来解决这个问题，大多数技术都设计出了复杂且高级的框架，以达到最优的准确度。本研究旨在微调双向转换器编码器表示（BERT）与双向长短期记忆网络（BiLSTM）用于电影评论情感分析，并能提供比当前最佳方法（SOTA）更好的准确度。此外，本文还展示了如何通过情感分析推荐某些电影，例如通过计算模型预测的情感总体极性来进行推荐。也就是说，我们的方法可作为预测电影主要反应的上界基线。为了计算总体极性，我们应用了一种启发式算法对BERT-BiLSTM输出向量进行操作。我们的模型可以扩展为三类、四类或其他精细粒度分类，并再次应用总体极性计算。这计划在未来的工作中加以利用。', 'title_zh': '使用BERT进行电影评论情感分析'}
{'arxiv_id': 'arXiv:2502.18823', 'title': 'Evidence-Driven Marker Extraction for Social Media Suicide Risk Detection', 'authors': 'Carter Adams, Caleb Carter, Jackson Simmons', 'link': 'https://arxiv.org/abs/2502.18823', 'abstract': 'Early detection of suicide risk from social media text is crucial for timely intervention. While Large Language Models (LLMs) offer promising capabilities in this domain, challenges remain in terms of interpretability and computational efficiency. This paper introduces Evidence-Driven LLM (ED-LLM), a novel approach for clinical marker extraction and suicide risk classification. ED-LLM employs a multi-task learning framework, jointly training a Mistral-7B based model to identify clinical marker spans and classify suicide risk levels. This evidence-driven strategy enhances interpretability by explicitly highlighting textual evidence supporting risk assessments. Evaluated on the CLPsych datasets, ED-LLM demonstrates competitive performance in risk classification and superior capability in clinical marker span identification compared to baselines including fine-tuned LLMs, traditional machine learning, and prompt-based methods. The results highlight the effectiveness of multi-task learning for interpretable and efficient LLM-based suicide risk assessment, paving the way for clinically relevant applications.', 'abstract_zh': '从社交媒体文本中早期检测自杀风险对于及时干预至关重要。尽管大型语言模型（LLMs）在这个领域展现出了令人鼓舞的能力，但在可解释性和计算效率方面仍存在挑战。本文介绍了证据驱动的大规模语言模型（ED-LLM），这是一种新的临床标志提取和自杀风险分类方法。ED-LLM 采用多任务学习框架，通过联合培训一个基于 Mistral-7B 的模型，识别临床标志片段并分类自杀风险水平。这种证据驱动的方法通过明确突出支持风险评估的文本证据，增强了可解释性。ED-LLM 在 CLPsych 数据集上的评价结果显示，相较于微调的语言模型、传统机器学习方法和基于提示的方法，它在风险分类和临床标志片段识别方面表现出更优的性能。研究结果强调了多任务学习在提高语言模型可解释性和效率方面的有效性，为临床相关应用铺平了道路。', 'title_zh': '基于证据驱动的标记提取方法及其在社交媒体自杀风险检测中的应用'}
{'arxiv_id': 'arXiv:2502.18817', 'title': 'Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models', 'authors': 'Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu', 'link': 'https://arxiv.org/abs/2502.18817', 'abstract': 'Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at this https URL.', 'abstract_zh': '提取增强生成（RAG）已被证明在减轻大规模语言模型（LLMs）的幻觉方面具有有效性。然而，现有的自动化评估指标无法公正地评估RAG模型在训练和评估阶段生成的输出。基于LLM的判断模型具有生成高质量判断的潜力，但它们对评估提示的高度敏感性会导致在判断RAG模型输出时的一致性问题。本文介绍了Judge-Consistency（ConsJudge）方法，旨在增强LLMs以生成更加准确的RAG模型评价。具体而言，ConsJudge促使LLMs基于多种判断维度的不同组合生成不同的判断，并利用判断一致性来评估这些判断，从中选择接受和拒绝的判断用于DPO训练。我们的实验表明，ConsJudge能够有效地为各种RAG模型和数据集中的RAG模型优化提供更加准确的判断。进一步的分析显示，ConsJudge生成的判断与优秀的LLM高度一致。所有代码可在以下链接处获取：https://xxxxx（请注意，这里的URL应替换为实际的代码存放地址）。', 'title_zh': '作为评判者：通过大型语言模型的评判一致性改进检索增强生成的评估'}
{'arxiv_id': 'arXiv:2502.18802', 'title': 'Language Models Grow Less Humanlike beyond Phase Transition', 'authors': 'Tatsuya Aoyama, Ethan Wilcox', 'link': 'https://arxiv.org/abs/2502.18802', 'abstract': "LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.", 'abstract_zh': '语言模型（LMs）在预训练过程中与人类阅读行为（即心理测量预测能力；PPP）的契合度通常是先提升到一个转折点，之后要么保持平稳，要么恶化。已有一些理论认为词汇频率、注意力的近期偏差以及上下文大小等因素会影响PPP，但目前尚无解释此类转折点存在的原因及其与LMs整体预训练动态相互作用的理论框架。我们假设这一转折点是由预训练过程中快速出现的专业化注意力头所标识的相变阶段。通过一系列相关性和因果实验，我们证明了这种相变是导致PPP转折点的原因。随后，我们发现相变不仅不会产生促进PPP下降的注意模式，反而改变了模型的后续学习动态，使得进一步的训练继续损害PPP。', 'title_zh': '语言模型在超越相变点后变得更加非拟人类化'}
{'arxiv_id': 'arXiv:2502.18798', 'title': 'ANPMI: Assessing the True Comprehension Capabilities of LLMs for Multiple Choice Questions', 'authors': 'Gyeongje Cho, Yeonkyoung So, Jaejin Lee', 'link': 'https://arxiv.org/abs/2502.18798', 'abstract': "Multiple-choice benchmarks, consisting of various prompts and choices, are among the most widely used methods to assess a language model's natural language understanding capability. Given a specific prompt, we typically compute $P(Choice|Prompt)$ to evaluate how likely a language model is to generate the correct choice compared to incorrect ones. However, we observe that performance measured using this approach reflects not only the model's comprehension of the prompt but also its inherent biases for certain choices regardless of the prompt. This issue makes it challenging to accurately measure a model's natural language understanding, as models may select the answer without fully understanding the prompt. To address this limitation, we propose a novel metric called ANPMI, which normalizes Pointwise Mutual Information (PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the model's natural language understanding by ensuring that it is challenging to answer a question without properly understanding the prompt.", 'abstract_zh': '以下内容是将给定的论文内容或标题翻译成中文，并符合学术规范：\n\n多项选择基准通常由各种提示和选项组成，是评估语言模型自然语言理解能力的最广泛使用的评估方法之一。给定一个特定的提示，我们通常计算 $P(Choice|Prompt)$，以评估语言模型生成正确选项与错误选项的可能性。然而，我们发现使用这种方法测量的表现不仅反映了模型对提示的理解能力，还反映了模型对某些选项的固有偏见，而这些问题并不依赖于提示本身。这使得准确测量模型的自然语言理解能力变得困难，因为模型可能会在未完全理解提示的情况下选择答案。为解决这一局限性，我们提出了一种新的度量方法，即ANPMI，它通过计算 $-\\log P(Choice)$ 来归一化点wise互信息（PMI）。ANPMI 提供了更准确的模型自然语言理解评估，确保不正确理解提示而回答问题是困难的。', 'title_zh': 'ANPMI: 评估大型语言模型在多项选择题中真正的理解能力'}
{'arxiv_id': 'arXiv:2502.18795', 'title': 'Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs', 'authors': 'Xiulin Yang, Tatsuya Aoyama, Yuekun Yao, Ethan Wilcox', 'link': 'https://arxiv.org/abs/2502.18795', 'abstract': "Do LLMs offer insights into human language learning? A common argument against this idea is that because their architecture and training paradigm are so vastly different from humans, LLMs can learn arbitrary inputs as easily as natural languages. In this paper, we test this claim by training LMs to model impossible and typologically unattested languages. Unlike previous work, which has focused exclusively on English, we conduct experiments on 12 natural languages from 4 language families. Our results show that while GPT-2 small can primarily distinguish attested languages from their impossible counterparts, it does not achieve perfect separation between all the attested languages and all the impossible ones. We further test whether GPT-2 small distinguishes typologically attested from unattested languages with different NP orders by manipulating word order based on Greenberg's Universal 20. We find that the model's perplexity scores do not distinguish attested vs. unattested word orders, as long as the unattested variants maintain constituency structure. These findings suggest that language models exhibit some human-like inductive biases, though these biases are weaker than those found in human learners.", 'abstract_zh': '大型语言模型是否能提供关于人类语言学习的洞察？一个常见的反驳观点是，由于它们的架构和训练范式与人类有极大的不同，大型语言模型可以像处理自然语言一样轻松地学习任意输入。在本文中，我们通过训练语言模型来模拟不可实现和类型无先例的语言，来检验这一观点。与先前仅专注于英语的研究不同，我们的实验涉及了来自4个语系的12种自然语言。结果表明，尽管GPT-2小型模型主要可以区分真实的语言与其不可实现的对应物，但它并不能完全将所有真实语言与所有不可实现的语言区分开来。进一步地，我们通过基于格林伯格的通用20（Universal 20）调整词序，测试GPT-2小型模型是能否区分类型学上真实和无先例的语言。结果显示，在不可实现的变体保持阶层结构的情况下，模型的困惑度得分无法区分真实和无先例的词序。这些发现表明，语言模型显示出一些类似人类的归纳偏见，但这些偏见弱于人类学习者的偏见。', 'title_zh': '《无所不包？跨语言视角下语言模型中超常规语言学习的可能性研究》\n\n这个翻译在保留原文含义的同时，采用了更符合中文表达习惯和学术规范的说法，特别是“（Im）possible Language Learning in LMs”部分，将其翻译为“超常规语言学习”，以准确反映LM（语言模型）中不常见或非常规语言学习的可能性研究。'}
{'arxiv_id': 'arXiv:2502.18791', 'title': 'Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs', 'authors': 'Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter', 'link': 'https://arxiv.org/abs/2502.18791', 'abstract': 'The surge of LLM studies makes synthesizing their findings challenging. Meta-analysis can uncover important trends across studies, but its use is limited by the time-consuming nature of manual data extraction. Our study presents a semi-automated approach for meta-analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs using an automatically extracted dataset, reducing the effort of paper surveying and data extraction by more than 93\\% compared to manual approaches. We validate our dataset by showing that it reproduces key findings from a recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new insights that go beyond it, showing for example that in-context examples benefit multimodal tasks but offer limited gains in mathematical tasks compared to CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through our scientific artifacts and empirical analysis, we provide novel insights into LLMs while facilitating ongoing meta-analyses of their behavior.', 'abstract_zh': '大语言模型（LLM）研究的激增使得对其研究成果的综合越来越具有挑战性。元分析可以揭示研究中的重要趋势，但其应用受到手动数据提取耗时的限制。本研究提出了一种半自动化的元分析方法，利用LLM加速数据提取过程。该方法自动识别相关arXiv论文，提取实验结果及相关属性，并将其组织成结构化数据集。我们使用自动提取的数据集对前沿LLM进行了全面的元分析，与手动方法相比，这种做法将文献回顾和数据提取的努力减少了超过93%。我们通过验证数据集，展示了它能够重现近期手动元分析中关于思维链（CoT）的关键发现，并揭示了新的见解，例如在某些情况下，上下文内的示例对多模态任务有益，但在数学任务中与CoT相比提供的增益相对较有限。我们自动可更新的数据集能够随着新数据的可用性而持续监测目标模型的研究成果。通过我们的科学制品和实证分析，我们提供了对LLM的新见解，并促进了对其行为的持续元分析。', 'title_zh': '从树木中见森林：前沿大语言模型的大型连续更新元分析'}
{'arxiv_id': 'arXiv:2502.18782', 'title': 'Active Few-Shot Learning for Text Classification', 'authors': 'Saeed Ahmadnia, Arash Yousefi Jordehi, Mahsa Hosseini Khasheh Heyran, Seyed Abolghasem Mirroshandel, Owen Rambow, Cornelia Caragea', 'link': 'https://arxiv.org/abs/2502.18782', 'abstract': 'The rise of Large Language Models (LLMs) has boosted the use of Few-Shot Learning (FSL) methods in natural language processing, achieving acceptable performance even when working with limited training data. The goal of FSL is to effectively utilize a small number of annotated samples in the learning process. However, the performance of FSL suffers when unsuitable support samples are chosen. This problem arises due to the heavy reliance on a limited number of support samples, which hampers consistent performance improvement even when more support samples are added. To address this challenge, we propose an active learning-based instance selection mechanism that identifies effective support instances from the unlabeled pool and can work with different LLMs. Our experiments on five tasks show that our method frequently improves the performance of FSL. We make our implementation available on GitHub.', 'abstract_zh': '大型语言模型（LLMs）的发展推动了少样本学习（FSL）方法在自然语言处理中的应用，即使在使用有限的训练数据时也能达到令人满意的性能。FSL 的目标是在学习过程中有效利用少量标注样本。然而，当选择不适合的支持样本时，FSL 的性能会受到影响。这个问题源于高度依赖有限的支持样本数量，即使增加了更多支持样本，也无法保证持续的性能提升。为解决这一挑战，我们提出了一种基于主动学习的实例选择机制，可以从未标注样本池中识别出有效支持实例，并且可以与不同类型的LLMs 配合使用。我们在五个任务上的实验表明，我们的方法经常能够提升FSL 的性能。我们已在GitHub 上开源了我们的实现代码。', 'title_zh': '文本分类中的活性少量样本学习'}
{'arxiv_id': 'arXiv:2502.18772', 'title': 'Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance', 'authors': 'Xueqing Peng, Triantafillos Papadopoulos, Efstathia Soufleri, Polydoros Giannouris, Ruoyu Xiang, Yan Wang, Lingfei Qian, Jimin Huang, Qianqian Xie, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.18772', 'abstract': "Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance.", 'abstract_zh': '尽管希腊在全球经济中占据重要地位，但由于希腊语言的复杂性和领域特定数据的稀缺性，大型语言模型（LLMs）在希腊金融语境中的应用仍然相对较少被探索。之前的多语言金融自然语言处理（NLP）努力暴露了显著的性能差异，但直到现在，还没有专门为希腊金融市场开发的基准测试或特定于希腊的金融LLMs。为了填补这一空白，我们介绍了Plutus-ben，该基准测试是首个希腊金融评价基准，并引入了Plutus-8B，这是首个基于希腊特定领域的大型语言模型。通过使用希腊特定领域的数据对Plutus-8B进行微调，Plutus-ben涵盖了五个希腊核心金融NLP任务：数值和文本命名实体识别、问答、缩写总结和话题分类，从而促进系统和可重复的LLM评估。为此，我们提出了三个高质量的新颖希腊金融数据集，并由专家母语为希腊的语言学家详细注释，同时扩展了两个现有资源。我们在Plutus-ben上对22个LLM进行的全面评估表明，由于语言复杂性、领域特定术语和金融推理差距，希腊金融NLP仍具有挑战性。这些发现强调了跨语言迁移的局限性、希腊训练模型中所需金融专业知识的重要性，以及适应希腊文本的金融LLMs所面临的挑战。我们公开发布了Plutus-ben、Plutus-8B及其所有相关数据集，以促进可重复研究，推进希腊金融NLP的发展，并促进金融领域的更广泛的多语言包容性。', 'title_zh': '普图斯：低资源希腊金融大型语言模型基准测试'}
{'arxiv_id': 'arXiv:2502.18746', 'title': 'Automatic Prompt Optimization via Heuristic Search: A Survey', 'authors': 'Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, Sricharan Kumar', 'link': 'https://arxiv.org/abs/2502.18746', 'abstract': 'Recent advances in Large Language Models have led to remarkable achievements across a variety of Natural Language Processing tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges pointing toward future opportunities for more robust and versatile LLM applications.', 'abstract_zh': '近年来，大型语言模型在各种自然语言处理任务中取得了显著成就，这使得提示工程在引导模型输出方面变得日益重要。虽然手动方法可能效果很好，但它们通常依赖于直觉，且不随时间自动优化提示。相比之下，基于启发式搜索算法的自动提示优化可以在最少的人工监督下系统地探索和改进提示。本文综述提出了这些方法的全面分类，按照优化发生的位置、优化的对象、驱动优化的标准、生成新提示的操作符以及应用的迭代搜索算法进行分类。我们还强调了一些专门的数据集和工具，它们支持并加速了自动提示精炼的过程。最后，我们讨论了一些关键的开放挑战，指出了未来在更稳健和多功能的大型语言模型应用方面的机遇。', 'title_zh': '基于启发式搜索的自动提示优化综述'}
{'arxiv_id': 'arXiv:2502.18729', 'title': 'Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science', 'authors': 'Xiaohua Wu, Xiaohui Tao, Wenjie Wu, Yuefeng Li, Lin Li', 'link': 'https://arxiv.org/abs/2502.18729', 'abstract': "Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.", 'abstract_zh': '在计算社会科学研究中的社会调查设计可以通过复杂的领域理论来精细规划，以有效地反映受访者的深层次思考而不隐瞒其真实情感。候选问卷选项高度依赖于受访者的先前回答，这导致了社会调查分析的复杂性、所需时间和专业知识。大型语言模型（LLMs）通过提示学习，如链式思考（CoT），增强了进行复杂推理的能力，但仍局限于自左向右的决策过程或解释中的有限路径。这意味着它们在需要探索和不确定性搜索的问题上可能会有所不足。为应对这一挑战，提出了一种新颖的大型语言模型提示方法，称为思维随机森林（RFoT），以生成不确定性推理并适用于计算社会科学研究领域。RFoT 允许大型语言模型通过生成多样化的思维空间并随机选择子思维来构建思维森林，从而进行有目的的决策。它能够扩展整体表现的探索和预测，并从广泛的响应研究空间中获益。该方法应用于两个数据集的优化，涵盖了一系列社会调查分析问题。我们的实验表明，RFoT 显著提升了语言模型在两项需要复杂推理的新社会调查分析问题上的能力。', 'title_zh': '《思维随机森林：面向计算社会科学的不确定性推理》'}
{'arxiv_id': 'arXiv:2502.18699', 'title': 'MPO: An Efficient Post-Processing Framework for Mixing Diverse Preference Alignment', 'authors': 'Tianze Wang, Dongnan Gui, Yifan Hu, Shuhang Lin, Linjun Zhang', 'link': 'https://arxiv.org/abs/2502.18699', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has shown promise in aligning large language models (LLMs). Yet its reliance on a singular reward model often overlooks the diversity of human preferences. Recent approaches address this limitation by leveraging multi-dimensional feedback to fine-tune corresponding reward models and train LLMs using reinforcement learning. However, the process is costly and unstable, especially given the competing and heterogeneous nature of human preferences. In this paper, we propose Mixing Preference Optimization (MPO), a post-processing framework for aggregating single-objective policies as an alternative to both multi-objective RLHF (MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it log-linearly combines existing policies into a unified one with the weight of each policy computed via a batch stochastic mirror descent. Empirical results demonstrate that MPO achieves balanced performance across diverse preferences, outperforming or matching existing models with significantly reduced computational costs.', 'abstract_zh': '强化学习基于人类反馈（RLHF）在对齐大型语言模型（LLMs）方面显示出潜力。然而，其依赖单一奖励模型往往会忽略人类偏好的多样性。近期的方法通过利用多维度反馈来调优相应的奖励模型，并使用强化学习训练LLMs，以克服这一局限性。然而，这一过程成本高昂且不稳定，尤其是考虑到人类偏好的竞争性和异质性。本文提出了一种混合偏好优化（MPO）后处理框架，作为一种替代多目标RLHF（MORLHF）和MaxMin-RLHF的方法，用于聚合单一目标策略。MPO避免从头开始对齐。相反，它通过批量随机镜像下降计算每个策略的权重，将现有策略线性组合成一个统一的策略。实验证明，MPO能够在多样化的偏好下实现均衡的性能，相较于现有模型实现了显著减少的计算成本，或达到了相当的性能。', 'title_zh': 'MPO：一种高效的混合多样偏好对齐后处理框架'}
{'arxiv_id': 'arXiv:2502.18679', 'title': 'Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data', 'authors': 'Siqi Guo, Ilgee Hong, Vicente Balmaseda, Tuo Zhao, Tianbao Yang', 'link': 'https://arxiv.org/abs/2502.18679', 'abstract': "Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted by SFT$\\rightarrow$PO has become the standard for improving pretrained large language models (LLMs), with PO demonstrating significant performance gains. However, PO methods rely on either human-labeled preference data or a strong reward model to generate preference data. Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT$\\rightarrow$PO? We address this question by introducing Discriminative Fine-Tuning (DFT), a novel approach that eliminates the need for preference data. Unlike SFT, which employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that that increases the probability of positive answers while suppressing potentially negative ones, shifting from token prediction to data prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\\rightarrow$PO. The code can be found at this https URL.", 'abstract_zh': '监督微调（SFT）之后进行偏好优化（PO），表示为SFT$\\rightarrow$PO，已成为提高预训练大型语言模型（LLMs）性能的标准方法，而PO方法能够显著提高性能。然而，PO方法依赖于人类标注的偏好数据或强大的奖励模型来生成偏好数据。我们能否在不使用偏好数据或奖励模型的情况下微调LLMs，并达到与SFT$\\rightarrow$PO相当甚至更好的性能？我们通过引入辨别性微调（DFT）来回答这个问题，这是一种新颖的方法，消除对偏好数据的依赖。与SFT采用生成方法并忽略负数据不同，DFT采用辨别性范式，通过增加正答案的概率并抑制潜在的负答案，从token预测转向数据预测。我们的贡献包括：（i）一种辨别性的概率框架，通过显式建模给定输入时所有可能输出中的回答的辨别性似然来进行微调；（ii）高效算法来优化这种辨别性似然；以及（iii）广泛实验表明DFT的有效性，其性能优于SFT，并且与SFT$\\rightarrow$PO相当甚至更优。相关代码可通过以下链接访问：this https URL。', 'title_zh': '无需奖励模型和偏好数据的生成型大规模语言模型的鉴别性微调'}
{'arxiv_id': 'arXiv:2502.18653', 'title': 'Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT', 'authors': 'Hediyeh Baban, Sai A Pidapar, Aashutosh Nema, Sichen Lu', 'link': 'https://arxiv.org/abs/2502.18653', 'abstract': 'We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.', 'abstract_zh': '我们介绍了一种新颖的多agent协作框架，旨在提高文本分类模型的准确性和鲁棒性。该框架以BERT为主要分类器，动态将低置信度预测提升至包括词汇、上下文、逻辑、共识和解释性agent在内的专门多agent系统。这种协作方法使全面分析和共识驱动的决策成为可能，显著提升了多种文本分类任务的分类性能。在基准数据集上的实证评估表明，与基于标准BERT的分类器相比，我们的框架实现了5.5%的准确率提高，这突显了其在自然语言处理领域推进多agent系统方面的有效性和学术新颖性。', 'title_zh': '利用BERT增强多代理协作框架以改进文本分类'}
{'arxiv_id': 'arXiv:2502.18650', 'title': 'Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources', 'authors': 'Joachim De Baer, A. Seza Doğruöz, Thomas Demeester, Chris Develder', 'link': 'https://arxiv.org/abs/2502.18650', 'abstract': 'Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains with challenges to obtain authentic human data. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for the use case of generating HR job interviews, and assess whether one method generates higher-quality dialogues that are more challenging to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialog. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We demonstrate that despite a sixfold increase in token cost, interviews generated with the dual-prompt method achieve a win rate up to ten times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or judging quality.', 'abstract_zh': '优化语言模型在会话代理中的应用需要大量示例对话。越来越多地，这些对话通过使用强大的大型语言模型（LLMs）合成生成，尤其是在难以获取真实人类数据的领域中尤其如此。人力资源（HR）领域就是其中之一。在此背景下，我们对比了两种基于LLM的对话生成方法在生成HR求职面试场景下的应用效果，并评估这些方法生成的对话是否更为高质量且更难以与真实的人类对话区分。第一种方法使用一个提示生成完整的面试对话。第二种方法使用两个代理进行互动。为了评估每种方法下的对话质量，我们让一个评判LLM判断是否使用了AI进行面试生成，方法是通过两份面试对话的配对比较来进行评估。结果显示，尽管双重提示方法的令牌成本增加了六倍，但它生成的面试对话的质量胜率比单一提示方法高十倍左右。这种差异无论使用GPT-4o还是Llama 3.3 70B进行对话生成或评判质量时，都能保持一致。', 'title_zh': '基于大型语言模型（LLMs）的单提示与双提示对话生成在人力资源领域招聘面试中的应用比较'}
{'arxiv_id': 'arXiv:2502.18644', 'title': 'Steered Generation via Gradient Descent on Sparse Features', 'authors': 'Sumanta Bhattacharyya, Pedram Rooshenas', 'link': 'https://arxiv.org/abs/2502.18644', 'abstract': "Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.", 'abstract_zh': '大型语言模型（LLMs）在其潜在表示中编码了多样化的语言特征，这些特征可以被利用以引导模型输出朝向特定的目标特征。在本文中，我们通过训练稀疏自编码器来修改LLMs的内部结构，学习查询嵌入的稀疏表示，从而实现对模型注意力分布的精确控制。我们证明通过操作这种稀疏表示，可以有效地将输出朝向不同的风格性和认知性目标进行变换。具体而言，在教育场景中，我们展示了通过修改特定层中编码查询表示的方式，可以系统地调整LLM生成反馈的认知复杂度。为此，我们使用基于梯度的优化在潜在空间中引导学习到的稀疏嵌入接近目标的认知复杂度级别的表示。', 'title_zh': '基于稀疏特征梯度下降的导向生成方法'}
{'arxiv_id': 'arXiv:2502.18642', 'title': 'Contextual effects of sentiment deployment in human and machine translation', 'authors': 'Lindy Comstock, Priyanshu Sharma, Mikhail Belov', 'link': 'https://arxiv.org/abs/2502.18642', 'abstract': 'This paper illustrates how the overall sentiment of a text may be shifted in translation and the implications for automated sentiment analyses, particularly those that utilize machine translation and assess findings via semantic similarity metrics. While human and machine translation will produce more lemmas that fit the expected frequency of sentiment in the target language, only machine translation will also reduce the overall semantic field of the text, particularly in regard to words with epistemic content.', 'abstract_zh': '本文阐述了在翻译过程中文本整体情绪可能会发生的变化，并探讨了这对自动化情绪分析的影响，特别是在使用机器翻译并通过语义相似性指标评估结果的情况下。尽管人工翻译和机器翻译都会产生更多符合目标语言情绪预期频率的词形变体，但只有机器翻译会减少文本的整体语义范畴，特别是在处理具有知识性内容的词汇时更为显著。', 'title_zh': '人类和机器翻译中情感部署的上下文影响研究'}
{'arxiv_id': 'arXiv:2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'authors': 'Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He', 'link': 'https://arxiv.org/abs/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'abstract_zh': '大规模语言模型（LLMs）通过链式思考（Chain-of-Thought, CoT）提示等机制，在解决复杂推理任务方面表现出色，强调详细、逐步的推理过程。然而，人类通常会采用一种更高效的策略：草拟简洁的中间思考，仅捕捉到关键信息。在这项工作中，我们提出了链式草稿（Chain of Draft, CoD）这一新颖的范式，该范式受到人类认知过程的启发，使LLMs在解决任务时生成简洁但富有信息量的中间推理输出。通过减少冗余信息并聚焦于关键洞见，CoD在准确度上与CoT相当甚至更优，仅使用CoT所需词汇量的7.6%，显著降低了各种推理任务的成本和延迟。', 'title_zh': '论文标题可以翻译为：思想快速生成的书写链条：写得少思考快'}
{'arxiv_id': 'arXiv:2502.18590', 'title': 'Neurobiber: Fast and Interpretable Stylistic Feature Extraction', 'authors': 'Kenan Alkiek, Anna Wegmann, Jian Zhu, David Jurgens', 'link': 'https://arxiv.org/abs/2502.18590', 'abstract': "Linguistic style is pivotal for understanding how texts convey meaning and fulfill communicative purposes, yet extracting detailed stylistic features at scale remains challenging. We present Neurobiber, a transformer-based system for fast, interpretable style profiling built on Biber's Multidimensional Analysis (MDA). Neurobiber predicts 96 Biber-style features from our open-source BiberPlus library (a Python toolkit that computes stylistic features and provides integrated analytics, e.g., PCA and factor analysis). Despite being up to 56 times faster than existing open source systems, Neurobiber replicates classic MDA insights on the CORE corpus and achieves competitive performance on the PAN 2020 authorship verification task without extensive retraining. Its efficient and interpretable representations readily integrate into downstream NLP pipelines, facilitating large-scale stylometric research, forensic analysis, and real-time text monitoring. All components are made publicly available.", 'abstract_zh': '语言风格对于理解文本如何传达意义并实现沟通目的至关重要，但大规模提取详细的风格特征仍然颇具挑战。我们介绍了Neurobiber，这是一个基于Biber的多维分析（MDA）构建的基于变换器的快速可解释风格概貌系统。Neurobiber从我们开源的BiberPlus库（该库是一个用于计算风格特征并提供集成分析的Python工具包，例如主成分分析和因子分析）中预测了96个Biber风格特征。尽管Neurobiber比现有开源系统快最多56倍，但它仍然在CORE语料库上重现了经典的MDA洞见，在PAN 2020作者身份验证任务上不需大量重新训练便达到了竞争性的性能。高效的可解释表示可以直接集成到下游自然语言处理管道中，促进了大量风格分析研究、法证分析以及实时文本监控。所有组件均已公开提供。', 'title_zh': '神经贝贝: 快速可解释的风格特征提取'}
{'arxiv_id': 'arXiv:2502.18583', 'title': 'What are Foundation Models Cooking in the Post-Soviet World?', 'authors': 'Anton Lavrouk, Tarek Naous, Alan Ritter, Wei Xu', 'link': 'https://arxiv.org/abs/2502.18583', 'abstract': "The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at this https URL.", 'abstract_zh': '后苏联国家的文化具有复杂性，深受动荡历史的影响，这种历史 continues to 影响当前事件。在本研究中，我们通过构建一个涵盖1147道俄语菜肴和823道乌克兰语菜肴的多模态数据集 BORSch，来考察基础模型对后苏联地区文化饮食知识的理解。这个数据集以后苏联地区为中心。我们证明，领先模型在文本-only 和多模态问答（QA）中都难以正确识别来自后苏联国家的菜肴起源，反而过度预测与问题语言相关的国家。通过分析预训练数据，我们表明，这些结果可以由误导性菜肴来源共现现象以及如俄语-乌克兰语代码混用这类语言现象来解释。最终，为超越问答评估，我们测试了模型生成菜肴准确视觉描述的能力。任务与问答之间的弱相关性提示我们，仅问答可能不足以评估文化理解。为了促进进一步的研究，我们将在以下链接上公开发布 BORSch 数据集：[此处提供链接]。', 'title_zh': '后苏联世界中，基础模型在烹饪些什么？'}
{'arxiv_id': 'arXiv:2502.18581', 'title': 'Scalable Best-of-N Selection for Large Language Models via Self-Certainty', 'authors': 'Zhewei Kang, Xuandong Zhao, Dawn Song', 'link': 'https://arxiv.org/abs/2502.18581', 'abstract': 'Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at this https URL', 'abstract_zh': '最佳-of-N 选择是一种通过增加测试时计算量来提高大型语言模型（LLMs）推理性能的关键技术。当前最先进的方法通常采用计算密集型的奖励模型进行响应评估和选择。无奖励替代方法，如自一致性（Self-Consistency）和通用自一致性（Universal Self-Consistency），在处理开放生成任务时能力有限，且难以有效扩展。为了解决这些限制，我们提出了一种新颖且高效的自信心（Self-Certainty）度量方法，该方法利用LLM输出的内在概率分布来估计响应质量，无需外部奖励模型。我们假设，在多个样本中汇总的更高分布性自信心与改进的响应准确性相关，因为它反映了对生成输出的更大信心。通过在各种推理任务上的广泛实验，我们证明了自信心（1）随着样本量 $N$ 的增加而有效扩展，类似于奖励模型但没有计算负担；（2）补充链式思考（Chain-of-Thought），在超越贪婪解码的情况下提高了推理性能；并且（3）在传统自一致性方法无法适用的开放生成任务中具有泛化能力。我们的研究结果确立了自信心作为一种实际且高效的方法来提高LLM的推理能力。相关代码可在以下网址获取：这个 [URL]', 'title_zh': '通过自我确信性实现大型语言模型的可扩展最佳选项选择'}
{'arxiv_id': 'arXiv:2502.18573', 'title': 'FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models', 'authors': 'Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale', 'link': 'https://arxiv.org/abs/2502.18573', 'abstract': 'Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.', 'abstract_zh': '近年来，大型语言模型（LLMs）在生成任务上展现出了巨大的能力，但在保证生成内容的准确性方面存在困难。这使得这些模型在预期需要事实准确回答的现实场景中不够可靠。在本文中，我们提出了一种新的事实性评估器FactReasoner，该评估器利用概率推理来评估长文本生成响应的事实性。具体而言，FactReasoner将响应分解为原子单元，从外部知识源检索与这些单元相关的情境，并利用逻辑关系（蕴含、矛盾）的概率编码构造原子单元和情境之间的联合概率分布。FactReasoner随后计算响应中原子单元由检索到的情境支持的后验概率。我们在标记和未标记的基准数据集上的实验结果清楚地表明，与最先进的基于提示的方法相比，FactReasoner在事实精确度和召回率方面均有显著提升。', 'title_zh': 'FactReasoner：一种大型语言模型长文事实性评估的概率方法'}
{'arxiv_id': 'arXiv:2502.18482', 'title': 'MixLLM: Dynamic Routing in Mixed Large Language Models', 'authors': 'Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.18482', 'abstract': "Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).", 'abstract_zh': '近年来，大型语言模型（LLMs）展现出潜在的人工通用智能能力，但其使用成本高且响应时间较长。鉴于LLM们各有优势和不足，LLM路由旨在识别流式查询中最合适的模型，以最大化响应质量并最小化成本和延迟。然而，面临的挑战包括：（1）质量、成本和延迟之间的动态权衡；（2）在部署系统中实现持续学习；以及（3）随着时间的推移导航不断变化的LLM候选集（例如新LLM的添加或旧LLM的移除）。为了应对这些挑战，我们开发了MixLLM——一种基于动态上下文臂赛的查询-LLM路由系统。具体而言，我们首先利用查询标签增强查询嵌入以提高路由任务的效果。接着，我们设计了轻量级预测模型来估计查询在不同LLM上的响应质量和成本。然后，我们设计了一个元决策者来选择最佳权衡响应质量、成本和延迟的查询-LLM配对。最后，该系统通过持续训练受益，使其能够适应不断变化的查询和用户反馈。我们的广泛实验表明，在时间约束条件下，MixLLM 在响应质量、成本和延迟方面达到最优权衡（在GPT-4质量的97.25%下，成本仅为24.18%）。', 'title_zh': 'MixLLM：混合大型语言模型中的动态路由'}
{'arxiv_id': 'arXiv:2502.19413', 'title': 'Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs', 'authors': 'Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sören Auer, Jenia Jitsev, Matthias Bethge', 'link': 'https://arxiv.org/abs/2502.19413', 'abstract': 'Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.', 'abstract_zh': '学术规范的中文翻译如下：\n\n付费墙、许可协议和版权规则常常限制科学知识的广泛传播和重用。我们认为，从学术文本中提取科学知识在法律和技术上都是可行的。当前的方法，比如文本嵌入，无法可靠地保留事实内容，而简单的改写可能在法律上也不够严谨。我们敦促学术界采纳一个新概念：使用大型语言模型（LLMs）将学术文档转换为知识单元（Knowledge Units）。这些单元使用结构化数据来捕获实体、属性和关系，而不包含风格化内容。我们提供了证据，表明知识单元：（1）基于对德国版权法和美国合理使用理论的法律分析，形成了一个合法合理的知识分享框架；（2）在四个研究领域中，基于原始受版权保护文本中的事实设计的多项选择题（MCQ）测试，保留了约95%的原始事实知识。从版权中解放科学知识有望为科学研究和教育带来变革性的益处，使语言模型能够重用受版权保护文本中的重要事实。为此，我们分享了用于将研究文档转换为知识单元的开源工具。总体而言，我们的工作提出现实可行的方法，以在尊重版权的同时使科学知识的获取更加民主化。', 'title_zh': '亚历山大项目：通过大型语言模型释放科学知识免受版权束缚之路'}
{'arxiv_id': 'arXiv:2502.19409', 'title': 'ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models', 'authors': 'Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott', 'link': 'https://arxiv.org/abs/2502.19409', 'abstract': 'Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.', 'abstract_zh': '多模态大型语言模型（MLLMs）在处理图像序列时依然面临挑战。尽管最近的模型在预训练过程中引入了多幅图像数据，但在识别序列结构方面仍存在问题，往往将图像独立处理。本文提出了一种名为ImageChain的框架，通过将视觉序列建模为多轮对话，增强MLLMs的序列推理能力。在ImageChain中，图像与相应的文本描述交织在一起，形成一个受控的对话，明确捕获时间依赖性和叙事进展。我们的方法旨在优化下一个场景描述任务，在该任务中，模型基于前一个视觉和文本提示生成具有上下文感知的场景描述。我们展示了我们的方法在下一个场景描述任务上的性能提升——在SimRate（衡量语义相似度的指标）上平均提高了15.3%。此外，ImageChain在从漫画到机器人学等多个跨域应用中实现了稳健的零样本表现。大量实验验证了在多模态多轮对话设计中进行指令调优是弥合静态图像理解和时序推理之间差距的关键。', 'title_zh': 'ImageChain：推动多模态大型语言模型中序列图像到文本推理的发展'}
{'arxiv_id': 'arXiv:2502.19407', 'title': 'Learning Code-Edit Embedding to Model Student Debugging Behavior', 'authors': 'Hasnain Heickal, Andrew Lan', 'link': 'https://arxiv.org/abs/2502.19407', 'abstract': "Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.", 'abstract_zh': '在计算机科学教育中，为编程作业提供有效的反馈具有挑战性：学生通过迭代提交代码、执行代码并利用编译器或自动评分系统提供的有限反馈来调试问题。在这一过程中分析学生的调试行为可揭示其知识结构，并有助于更好地开发个性化支持工具。本文中，我们提出了一种基于编码器-解码器的模型，该模型学习连续学生代码提交之间的有意义代码编辑向量表示，以捕捉其调试行为。我们的模型利用学生代码提交是否通过每个测试用例的信息，微调大型语言模型（LLMs），以学习代码编辑表示。该模型能够提供个性化下一步代码建议，既保持学生的编程风格，又提高测试用例的正确性。此外，我们的模型还通过聚类技术分析学生的代码编辑模式，以揭示常见的学生错误和调试行为。在真实的学生代码提交数据集上的实验结果显示，我们的模型在代码重构和个人化代码建议方面表现出色，并揭示了学生调试行为中的有趣模式。', 'title_zh': '学习代码-编辑嵌入以建模学生调试行为'}
{'arxiv_id': 'arXiv:2502.19400', 'title': 'TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding', 'authors': 'Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, Wenhu Chen', 'link': 'https://arxiv.org/abs/2502.19400', 'abstract': 'Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.', 'abstract_zh': '理解特定领域的定理往往需要不止文本推理，有效的结构化视觉解释对于深入理解至关重要。虽然大型语言模型（LLMs）在文本推理方面表现出色，但它们生成连贯且教学意义显著的视觉解释的能力仍然是一个开放的挑战。在本研究中，我们引入了TheoremExplainAgent，这是一个使用Manim动画生成长格式定理解释视频（超过5分钟）的自主方法。为了系统地评估多模态定理解释，我们提出了TheoremExplainBench，这是一个涵盖240个定理（涵盖多个STEM学科）的基准，并配备有5个自动评估指标。我们的结果表明，自主规划对于生成详细的长格式视频至关重要，o3-mini代理的成功率为93.8%，综合评分为0.77。然而，我们的定量和定性研究发现，大多数生成的视频存在视觉元素布局方面的较小问题。此外，多模态解释揭示了文本解释未能揭示的更深层次的推理缺陷，突显了多模态解释的重要性。', 'title_zh': '《TheoremExplainAgent：多模态定理解释的LLM理解研究》\n\n这个标题翻译成中文后，保持了学术规范，并且尽量保留了原文的核心含义。如果有更多具体的内容需要翻译或进一步的帮助，请告知。'}
{'arxiv_id': 'arXiv:2502.19387', 'title': 'Residual Speech Embeddings for Tone Classification: Removing Linguistic Content to Enhance Paralinguistic Analysis', 'authors': 'Hamdan Al Ahbabi, Gautier Marti, Saeed AlMarri, Ibrahim Elfadel', 'link': 'https://arxiv.org/abs/2502.19387', 'abstract': 'Self-supervised learning models for speech processing, such as wav2vec2, HuBERT, WavLM, and Whisper, generate embeddings that capture both linguistic and paralinguistic information, making it challenging to analyze tone independently of spoken content. In this work, we introduce a method for disentangling paralinguistic features from linguistic content by regressing speech embeddings onto their corresponding text embeddings and using the residuals as a representation of vocal tone. We evaluate this approach across multiple self-supervised speech embeddings, demonstrating that residual embeddings significantly improve tone classification performance compared to raw speech embeddings. Our results show that this method enhances linear separability, enabling improved classification even with simple models such as logistic regression. Visualization of the residual embeddings further confirms the successful removal of linguistic information while preserving tone-related features. These findings highlight the potential of residual embeddings for applications in sentiment analysis, speaker characterization, and paralinguistic speech processing.', 'abstract_zh': '为了符合学术规范，以下是将给定内容翻译成中文的结果：\n\n自监督学习模型，如 wav2vec2、HuBERT、WavLM 和 Whisper，在语音处理中生成同时捕捉语言性和副语言性信息的嵌入表示，这使得独立地分析语调变得具有挑战性。本文介绍了一种通过将语音嵌入回归到对应的文本嵌入并使用残差作为语调表示的方法来分离副语言性特征的方法。我们通过多种自监督语音嵌入对此方法进行了评估，结果表明，残差嵌入在语音语调分类性能上相比原始语音嵌入有显著提升。我们的结果表明，这种方法增强了线性可分性，即使使用简单的模型（如逻辑回归）也能获得更好的分类性能。对残差嵌入的可视化进一步证实了成功地移除了语言信息并保留了与语调相关的特征。这些发现突显了残差嵌入在情感分析、说话者特征描述以及副语言性语音处理等应用中的潜在价值。', 'title_zh': '残留语音嵌入用于声学语义分类：去除语言内容以增强副语言分析'}
{'arxiv_id': 'arXiv:2502.19312', 'title': 'FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users', 'authors': 'Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn', 'link': 'https://arxiv.org/abs/2502.19312', 'abstract': 'Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.', 'abstract_zh': '有效的个性化大语言模型（LLM）对于广泛的应用场景至关重要，如虚拟助手和内容推荐。受LLM强大上下文学习能力的启发，我们提出了一种名为少量示例偏好优化（FSPO）的方法，将其奖励建模重新定义为一个元学习问题。在这一框架下，LLM能够通过少量用户标记的偏好信息迅速适应用户，构建个性化的奖励函数。此外，由于实际的偏好数据稀缺且难以大规模收集，我们提出了精心的设计选择来构建合成偏好数据集，使用公开的LLM生成了超过100万条合成个性化偏好数据。特别是，为了成功地将合成数据转移到真实用户，我们发现数据应表现出高度多样化和一致、自洽的结构至关重要。我们在三个领域（电影评论、基于教育背景的教学适应以及通用问题回答）的1500个合成用户的个性化开放生成任务上评估了FSPO，并进行了一项受控的人类研究。总体而言，FSPO在生成针对合成用户的个性化响应方面，平均取得了87%的Alpaca Eval胜率，在开放问题回答中使用真实人类用户的胜率为72%。', 'title_zh': 'FSPO：在大型语言模型中利用少量样本优化合成偏好数据以实现对真实用户的有效个性化'}
{'arxiv_id': 'arXiv:2502.19149', 'title': 'Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with PseudoEval', 'authors': 'Jiarong Wu, Songqiang Chen, Jialun Cao, Hau Ching Lo, Shing-Chi Cheung', 'link': 'https://arxiv.org/abs/2502.19149', 'abstract': "Existing code generation benchmarks for Large Language Models (LLMs) such as HumanEval and MBPP are designed to study LLMs' end-to-end performance, where the benchmarks feed a problem description in natural language as input and examine the generated code in specific programming languages. However, the evaluation scores revealed in this way provide a little hint as to the bottleneck of the code generation -- whether LLMs are struggling with their problem-solving capability or language-coding capability. To answer this question, we construct PseudoEval, a multilingual code generation benchmark that provides a solution written in pseudocode as input. By doing so, the bottleneck of code generation in various programming languages could be isolated and identified. Our study yields several interesting findings. For example, we identify that the bottleneck of LLMs in Python programming is problem-solving, while Rust is struggling relatively more in language-coding. Also, our study indicates that problem-solving capability may transfer across programming languages, while language-coding needs more language-specific effort, especially for undertrained programming languages. Finally, we release the pipeline of constructing PseudoEval to facilitate the extension to existing benchmarks. PseudoEval is available at: this https URL.", 'abstract_zh': '现有的针对大规模语言模型（LLMs）的代码生成基准测试，如HumanEval和MBPP，旨在研究LLMs的整体性能，这些基准测试以自然语言的形式提供问题描述，并检查特定编程语言中生成的代码。然而，通过这种方式给出的评估分数，对于代码生成瓶颈的揭示甚少，即LLMs是因其问题解决能力还是语言编程能力而受到影响。为了回答这个问题，我们构建了PseudoEval，这是一种多语言代码生成基准测试，其提供伪代码形式的问题解决方案作为输入。通过这种方式，可以将不同编程语言下的代码生成瓶颈隔离和识别。我们的研究发现了几个有趣的结果。例如，我们发现Python编程中的LLMs瓶颈在于问题解决能力，而Rust在语言编程方面相对更为困难。另外，我们的研究还表明，问题解决能力可能在不同编程语言之间具有转移性，而语言编程则需要更多的特定语言努力，尤其是在训练不足的编程语言方面。最后，我们发布了构建PseudoEval的流程，以方便扩展现有基准测试。PseudoEval可在以下网址获取：[这里](this https URL)。', 'title_zh': '将语言编码与问题解决分离：通过PseudoEval评估LLMs'}
{'arxiv_id': 'arXiv:2502.19067', 'title': 'IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across Indic Languages', 'authors': 'Ujjwal Singh, Aditi Sharma, Nikhil Gupta, Deepakshi, Vivek Kumar Jha', 'link': 'https://arxiv.org/abs/2502.19067', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation from natural language prompts, revolutionizing software development workflows. As we advance towards agent-based development paradigms, these models form the cornerstone of next-generation software development lifecycles. However, current benchmarks for evaluating multilingual code generation capabilities are predominantly English-centric, limiting their applicability across the global developer community. To address this limitation, we present IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\\% of the world's population. Our benchmark bridges these languages with 12 programming languages, creating a robust evaluation framework. This work is particularly significant given India's representation of one-eighth of the global population and the crucial role Indic languages play in Indian society. IndicEval-XL represents a significant step toward expanding the linguistic diversity in code generation systems and evaluation frameworks. By developing resources that support multiple languages, we aim to make AI-powered development tools more inclusive and accessible to developers of various linguistic backgrounds. To facilitate further research and development in this direction, we make our dataset and evaluation benchmark publicly available at this https URL", 'abstract_zh': '大型语言模型（LLMs）已经在从自然语言提示生成代码方面展示了杰出的能力，革新了软件开发流程。随着我们朝着基于代理的开发范式前进，这些模型成为了下一代软件开发生命周期的基石。然而，当前用于评估多语言代码生成能力的基准测试主要集中在英语上，限制了它们在全球开发社区中的应用。为了解决这一局限，我们提出了IndicEval-XL，这是一个集成了6种主要印地语系语言的全面基准测试，这些语言共被全世界约14%的人口使用。我们的基准测试将这些语言与12种编程语言相结合，创建了一个强大的评估框架。鉴于印度人口占世界人口的八分之一，以及印地语系语言在印度社会中的重要作用，这一工作尤为重要。IndicEval-XL 代表了扩大代码生成系统和评估框架中语言多样性的一步。通过开发支持多种语言的资源，我们旨在使基于人工智能的开发工具对各种语言背景的开发人员更加包容和可访问。为了促进在此方向上的进一步研究和开发，我们已将我们的数据集和评估基准公开发布在以下网址：[该网址]', 'title_zh': 'IndicEval-XL：跨印度语言代码生成中的语言多样性桥梁'}
{'arxiv_id': 'arXiv:2502.19024', 'title': 'Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments', 'authors': 'Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu', 'link': 'https://arxiv.org/abs/2502.19024', 'abstract': 'Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.', 'abstract_zh': '视觉-语言导航（VLN）赋予代理将时间序列的视觉观察与相应指令关联起来的能力，以做出序列决策。然而，泛化仍然是一个持续的挑战，尤其是在处理视觉多变的场景或从模拟环境过渡到真实世界部署时。本文中，我们通过提出地面视角导航（Ground-level Viewpoint Navigation, GVNav）方法来解决人类中心的指令与低视场四足机器人的匹配问题，旨在缓解这一问题。本文代表了首次尝试在现实机器人部署中强调不同高度视觉观察下的泛化差距。\n\n我们的方法利用加权的历史观察来提供增强的空间时间上下文以遵循指令，通过在不同视角间分配适当的权重来有效管理单元内的特征冲突，从而使低视场机器人能够克服视觉遮挡和知觉不匹配等挑战。此外，我们将HM3D和Gibson数据集的连接图作为额外资源转移到GVNav中，增强空间先验并提供更全面的真实世界场景表示，从而提高路径点预测器在真实世界环境中的性能和泛化能力。广泛的实验结果显示，我们的地面视角导航（GVNav）方法在四足机器人的模拟环境和真实世界部署中显著提高了性能。\n\n这个翻译基本符合学术规范，同时保持了原文的意思和结构。希望这对您有帮助！', 'title_zh': '持续环境中基于地面视角的视觉-语言导航'}
{'arxiv_id': 'arXiv:2502.18969', 'title': '(Mis)Fitting: A Survey of Scaling Laws', 'authors': 'Margaret Li, Sneha Kudugunta, Luke Zettlemoyer', 'link': 'https://arxiv.org/abs/2502.18969', 'abstract': 'Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All components of this process vary, from the specific equation being fit, to the training setup, to the optimization method. Each of these factors may affect the fitted law, and therefore, the conclusions of a given study. We discuss discrepancies in the conclusions that several prior works reach, on questions such as the optimal token to parameter ratio. We augment this discussion with our own analysis of the critical impact that changes in specific details may effect in a scaling study, and the resulting altered conclusions. Additionally, we survey over 50 papers that study scaling trends: while 45 of these papers quantify these trends using a power law, most under-report crucial details needed to reproduce their findings. To mitigate this, we we propose a checklist for authors to consider while contributing to scaling law research.', 'abstract_zh': '现代基础模型在指导关键训练决策时高度依赖标度定律。研究人员通常通过描述损失或任务性能与规模之间的关系，从较小的训练运行中外推出最优架构和超参数设置。这一过程的各个组成部分各不相同，包括所拟合的具体方程、训练设置以及优化方法。这些因素可能都会影响所拟合的定律，进而影响研究结论。我们讨论了不同先前研究在类似问题上得出的结论差异，例如最优标记（token）与参数比。我们还通过自己的分析补充讨论了特定细节变化对标度研究结果的关键影响及其导致的不同结论。此外，我们调研了超过50篇研究标度趋势的论文：其中45篇论文使用幂律量化了这些趋势，但大多未能详细报告重现其结果所需的关键细节。为了缓解这一问题，我们提出了一份作者在参与标度定律研究时应考虑的清单。', 'title_zh': '不匹配性调查：关于缩放定律的综述\n\n该翻译符合学术规范，准确地传达了原文的意思。其中，“(Mis)Fitting” 被翻译为“不匹配性”，“Survey of Scaling Laws” 被翻译为“关于缩放定律的综述”。这样既能准确传达原意，又符合学术论文的表达方式。'}
{'arxiv_id': 'arXiv:2502.18943', 'title': 'Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models', 'authors': 'Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, Chun Chen', 'link': 'https://arxiv.org/abs/2502.18943', 'abstract': "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\\ie, \\textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences.\nTo alleviate these problems, we propose \\textbf{PETAL}: a label-only membership inference attack based on \\textbf{PE}r-\\textbf{T}oken sem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.", 'abstract_zh': '成员归属推断攻击（Membership Inference Attacks, MIAs）旨在预测某个数据样本是否属于模型的训练集。虽然先前的研究已经广泛探讨了在大型语言模型（LLMs）中的MIAs，但这些攻击通常需要访问完整输出logits（即logits基攻击），而在实践中这些数据通常不可用。在本文中，我们研究了预训练LLMs在仅标签（label-only）设置下的易受攻击性，即攻击者只能访问生成的标记（文本）。我们首先揭示，尽管现有仅标签的MIAs在推断个性化LLMs所使用的微调数据集方面非常有效，但在攻击预训练LLMs时效果微乎其微。我们发现其失败的根本原因有两个方面，包括更好的泛化能力和过于粗糙的扰动。具体而言，由于广泛预训练的语料库以及每个样本仅暴露几次，LLMs在成员与非成员之间体现出的鲁棒性差异极小。这使得标记级别的扰动过于粗糙，难以捕捉这些差异。\n\n为解决这些问题，我们提出了一种基于标记级别的成员归属推断攻击方法：一种基于PEr-TEKnical semAntiC simiLarity（PETAL）的标签唯一性推断攻击。PETAL 指标利用标记级别的语义相似性来近似输出概率，进而计算困惑度。基于一个普遍假设，即成员能够被更好地记忆并且具有较小的困惑度，该方法最终通过对照组来揭示成员身份。我们在WikiMIA基准和更具挑战性的MIMIR基准上进行了广泛的实验。实证结果显示，我们的PETAL在五种常见的开源LLMs上各评价指标上比现有仅标签的攻击方法更有优势，甚至在某些方面与其它先进的logits基攻击方法持平。', 'title_zh': '面向预训练大型语言模型的仅标签成员推理攻击'}
{'arxiv_id': 'arXiv:2502.18889', 'title': 'Clip-TTS: Contrastive Text-content and Mel-spectrogram, A High-Huality Text-to-Speech Method based on Contextual Semantic Understanding', 'authors': 'Tianyun Liu', 'link': 'https://arxiv.org/abs/2502.18889', 'abstract': 'Traditional text-to-speech (TTS) methods primarily focus on establishing a mapping between phonemes and mel-spectrograms. However, during the phoneme encoding stage, there is often a lack of real mel-spectrogram auxiliary information, which results in the encoding process lacking true semantic understanding. At the same time, traditional TTS systems often struggle to balance the inference speed of the model with the quality of the synthesized speech. Methods that generate high-quality synthesized speech tend to have slower inference speeds, while faster inference methods often sacrifice speech quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip architecture. This method uses the Clip framework to establish a connection between text content and real mel-spectrograms during the text encoding stage, enabling the text encoder to directly learn the true semantics of the global context, thereby ensuring the quality of the synthesized speech. In terms of model architecture, I adopt the basic structure of Transformer, which allows Clip-TTS to achieve fast inference speeds. Experimental results show that on the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves state-of-the-art MOS scores, and it also performs excellently on multi-emotion this http URL samples are available at: this https URL.', 'abstract_zh': '传统的文本到语音（TTS）方法主要集中在建立音素和梅尔频谱图之间的映射。然而，在音素编码阶段，常常缺乏真实的梅尔频谱图辅助信息，导致编码过程缺乏真正的语义理解。同时，传统的TTS系统往往难以平衡模型的推理速度与合成语音的质量。生成高质量合成语音的方法通常会牺牲推理速度，而快速的推理方法则常常以牺牲语音质量为代价。在本文中，我提出了一种基于Clip架构的TTS方法——Clip-TTS。该方法利用Clip框架在文本编码阶段建立文本内容与真实梅尔频谱图之间的联系，使文本编码器可以直接学习全局语境的真实语义，从而确保合成语音的质量。在模型架构方面，我采用了Transformer的基本结构，使Clip-TTS能够实现快速的推理速度。实验结果显示，Clip-TTS在LJSpeech和Baker数据集上生成的语音达到了目前最先进的MOS评分，并且在多情绪识别方面表现出色。相关语音样本可在：[此处链接]和[此处链接]获取。', 'title_zh': 'Clip-TTS：基于上下文语义理解的对比文本内容与梅尔谱图方法的高质文本到语音技术'}
{'arxiv_id': 'arXiv:2502.18873', 'title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'authors': 'Sen Yang, Yafu Li, Wai Lam, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.18873', 'abstract': "Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.", 'abstract_zh': '大型语言模型（LLMs）在处理复杂的推理任务时往往表现不佳，这主要是因为它们难以应对庞大且多义的推理空间。我们提出了一种名为混合搜索代理（MoSA）的新范式，该范式利用多种LLM的集体专业知识以增强基于搜索的推理。MoSA通过结合独立探索与LLM之间的迭代改进，整合了多种推理路径，从而缓解了单一模型方法的局限性。MoSA以蒙特卡洛树搜索（MCTS）为基础，使得多个代理能够提出并聚合推理步骤，从而提高准确性。我们在四个不同的推理基准测试中的全面评估表明，MoSA在复杂数学推理和常识推理任务中相比单代理和多代理基线方法，表现出一致的性能提升。', 'title_zh': '复杂问题解决中的多大规模语言模型协作搜索'}
{'arxiv_id': 'arXiv:2502.18864', 'title': 'Towards an AI co-scientist', 'authors': 'Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago R D Costa, José R Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, Vivek Natarajan', 'link': 'https://arxiv.org/abs/2502.18864', 'abstract': "Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.", 'abstract_zh': '科学发现依赖于科学家生成新颖的假设并通过严格的实验验证。为了增强这一过程，我们引入了一个基于Gemini 2.0构建的AI合作者，这是一种多代理系统。AI合作者旨在帮助揭示新的原创知识，并根据既有的证据制定可证伪的新颖研究假设和提案，同时与科学家提供的研究目标和指导保持一致。该系统的架构采用了“生成、辩论、进化”的假设生成方法，受到科学研究方法的启发，并通过扩展测试时计算量来加速这一过程。关键贡献包括：（1）一种具有异步任务执行框架的多代理架构，支持灵活的计算扩展；（2）一种自提高假设生成的锦标赛进化过程。自动评估结果显示，测试时计算量持续带来益处，提高了假设的质量。虽然具有普适性，但我们主要在以下三个生物医学领域进行了开发和验证：药物再利用、新型靶点发现以及解释细菌进化和抗微生物耐药机制。在药物再利用领域，系统提出了一些具有临床验证前景的候选药物，包括在非临床相关浓度下能够抑制急性髓系白血病的候选药物。在新型靶点发现领域，AI合作者提出了针对肝纤维化的新的表观遗传学靶点，并通过人类肝类器官中的抗纤维化活性和肝脏细胞再生得到了验证。最终，AI合作者通过并行的计算发现，重新揭示了一个新型基因转移机制在细菌进化中的作用。这些结果分别在独立的、同步发布的工作报告中得到了阐述，展示了增强生物医学和科学发现的潜力，并标志着AI赋能科学家时代的到来。', 'title_zh': '朝向一位AI合作者'}
{'arxiv_id': 'arXiv:2502.18810', 'title': 'Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal', 'authors': 'Weipeng Jiang, Juan Zhai, Shiqing Ma, Ziyan Lei, Xiaofei Xie, Yige Wang, Chao Shen', 'link': 'https://arxiv.org/abs/2502.18810', 'abstract': 'In recent years, Large Language Models (LLMs) have faced increasing demands to selectively remove sensitive information, protect privacy, and comply with copyright regulations through unlearning, by Machine Unlearning. While evaluating unlearning effectiveness is crucial, existing benchmarks are limited in scale and comprehensiveness, typically containing only a few hundred test cases. We identify two critical challenges in generating holistic audit datasets: ensuring audit adequacy and handling knowledge redundancy between forget and retain dataset. To address these challenges, we propose HANKER, an automated framework for holistic audit dataset generation leveraging knowledge graphs to achieve fine-grained coverage and eliminate redundant knowledge. Applying HANKER to the popular MUSE benchmark, we successfully generated over 69,000 and 111,000 audit cases for the News and Books datasets respectively, identifying thousands of knowledge memorization instances that the previous benchmark failed to detect. Our empirical analysis uncovers how knowledge redundancy significantly skews unlearning effectiveness metrics, with redundant instances artificially inflating the observed memorization measurements ROUGE from 19.7% to 26.1% and Entailment Scores from 32.4% to 35.2%, highlighting the necessity of systematic deduplication for accurate assessment.', 'abstract_zh': '近年来，大型语言模型（LLMs）面临越来越大的需求，即通过机器遗忘（Machine Unlearning）选择性地删除敏感信息、保护隐私并遵守版权法规。评估遗忘效果至关重要，但现有的基准测试在规模和全面性方面都有限，通常仅包含数百个测试案例。我们识别出了生成全面审计数据集的两个关键挑战：确保审计充分性和处理遗忘集与保留集之间的知识冗余。\n\n为了解决这些挑战，我们提出了一种名为HANKER的自动化框架，利用知识图谱生成全面审计数据集，从而实现细粒度的覆盖并消除冗余知识。我们将HANKER应用于流行的MUSE基准测试，在新闻和书籍数据集上分别生成了超过69,000和111,000个审计案例，识别出了先前基准测试未能检测到的数千个知识记忆实例。我们的实证分析揭示了知识冗余如何显著歪曲遗忘效果指标，冗余实例导致ROUGE测量值从19.7%增加到26.1%，验证性分数从32.4%增加到35.2%，这强调了系统去重对于准确评估的重要性。', 'title_zh': '通过知识图谱遍历和冗余去除生成全面的审计数据集以实现LLM遗忘'}
{'arxiv_id': 'arXiv:2502.18779', 'title': 'Towards Optimal Multi-draft Speculative Decoding', 'authors': 'Zhengmian Hu, Tong Zheng, Vignesh Viswanathan, Ziyi Chen, Ryan A. Rossi, Yihan Wu, Dinesh Manocha, Heng Huang', 'link': 'https://arxiv.org/abs/2502.18779', 'abstract': 'Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound.', 'abstract_zh': '大型语言模型（LLMs）已成为自然语言处理任务中不可或缺的部分。然而，自回归采样已成为效率瓶颈。多草稿推测解码（MDSD）是一种近期的方法，其中，在生成每个token时，一个小规模的草稿模型生成多个草稿，目标LLM并行验证这些草稿，以确保最终输出符合目标模型分布。MDSD的主要设计选择包括草稿采样方法和验证算法。对于固定的草稿采样方法，最优接受率是优化运输问题的一个解，但由于此问题的复杂性，很难求得最优接受率，并衡量现有的验证算法与理论上限之间的差距。本文讨论了优化运输问题的对偶问题，提供了一种有效计算最优接受率的方法。首次测量了MDSD效率的理论上限，量化了现有验证算法与该上限之间的差距。我们还基于它们的最优接受率比较了不同的草稿采样方法。研究结果表明，草稿采样方法对最优接受率有显著影响，不放回采样优于放回采样。此外，现有的验证算法在无放回采样和有放回采样情况下均未达到理论上限。我们的发现表明，精心设计的草稿采样方法可能提高最优接受率，并有助于开发与理论上限接近的验证算法。', 'title_zh': '朝向最优多稿推测解码'}
{'arxiv_id': 'arXiv:2502.18778', 'title': 'M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance', 'authors': 'Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-WeiChang, Jingdong Chen, Ming Yang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.18778', 'abstract': "We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni's language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain.", 'abstract_zh': '我们介绍了一种名为M2-omni的前沿开源全模态大语言模型，其性能媲美GPT-4o。M2-omni采用了一种统一的多模态序列建模框架，赋予大语言模型（LLMs）全面跨模态的理解和生成能力。具体而言，M2-omni能够处理任意混合的音频、视频、图像和文本模态输入，生成交织有音频、图像或文本输出的多模态序列，从而提供高级且互动的实时体验。训练这种全模态大语言模型面临着各模态间数据量差异显著以及收敛率不同的挑战。为应对这些挑战，我们在预训练阶段提出了步长平衡策略，以处理特定模态数据量的差异。此外，在指令调优阶段引入了动态自适应平衡策略，以同步各模态的训练进度，确保最优的收敛性。值得一提的是，在整个训练过程中，我们优先保持对纯文本任务的强大性能，以确保M2-omni的语言理解能力的稳健性。据我们所知，M2-omni目前是非常有竞争力的开源模型，具备全面支持模态和任务的特点，以及卓越的性能。我们期望M2-omni将进一步推动全模态大语言模型的发展，从而促进该领域的未来研究。', 'title_zh': 'M2-omni：面向全面模态支持的竞争力性能提升的全栈多模态大语言模型'}
{'arxiv_id': 'arXiv:2502.18770', 'title': 'Reward Shaping to Mitigate Reward Hacking in RLHF', 'authors': 'Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2502.18770', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to reward hacking, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. While reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests three key design principles: (1) RL reward is ideally bounded, (2) RL benefits from rapid initial growth followed by gradual convergence, and (3) RL reward is best formulated as a function of centered reward. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model itself as the signal for reinforcement learning. We evaluated PAR on two base models, Gemma2-2B and Llama3-8B, using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. Code is available at this https URL.", 'abstract_zh': '人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）对于使大型语言模型（Large Language Models, LLMs）与人类价值观保持一致至关重要。然而，RLHF 可能会受到奖励破解的影响，即代理利用奖励函数中的漏洞而非学习预期的行为，从而破坏对齐。尽管奖励塑形有助于稳定 RLHF 并部分缓解奖励破解，但对塑形技术及其原理系统的研究仍然缺失。为弥补这一不足，我们进行了对流行奖励塑形方法的全面研究。我们的分析提出了三条关键设计原则：（1）RL 奖励应理想地受到限制，（2）RL 初始快速增长后逐渐收敛，（3）RL 奖励应以中心化奖励的形式进行表述。受这些洞见的启发，我们提出了偏好作为奖励（Preference As Reward，PAR）的新方法，该方法利用奖励模型本身嵌入的潜在偏好作为强化学习的信号。我们在两个基模型 Gemma2-2B 和 Llama3-8B 上使用 Ultrafeedback-Binarized 和 HH-RLHF 两个数据集对该方法进行了评估。实验结果表明，PAR 的性能优于其他奖励塑形方法。在 AlpacaEval 2.0 基准测试中，PAR 的胜率至少比其他方法高 5 个百分点。此外，PAR 出色地体现了数据效率，在最优性能时仅需一个参考奖励，并且即使在两个完整的训练周期后仍能保持对奖励破解的鲁棒性。代码可在以下网址获取：https://github.com/example-repo。', 'title_zh': '奖励塑形以减轻RLHF中的奖励劫持问题'}
{'arxiv_id': 'arXiv:2502.18744', 'title': 'Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models', 'authors': 'Jeesu Jung, Chanjun Park, Sangkeun Jung', 'link': 'https://arxiv.org/abs/2502.18744', 'abstract': 'Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.', 'abstract_zh': '近年来，大型语言模型（LLM）对齐领域的最新进展旨在通过利用预训练模型生成偏好数据来减轻人工标注的成本。然而，现有的方法往往比较能力差异很大的模型的响应，这导致了表面性的区别，未能提供有意义的指导，明确什么是更好的响应。为了解决这一局限性，我们提出了亲和感知偏好映射（Kinship-Aware pReference MApping，KARMA）这一新颖框架，该框架系统地将具有相似能力的模型响应进行配对。通过将偏好比较限制在具有类似复杂度和质量的输出上，KARMA 提高了偏好数据的信息量，并增强了对齐信号的粒度。实证评估表明，我们的亲和感知方法导致了更为一致和可解释的对齐结果，最终促进了更为原则性和可靠的途径，以使LLM的行为与人类偏好对齐。', 'title_zh': '像父亲，像儿子：基于亲缘关系的偏好映射(KARMA)在大型语言模型中的自动对齐'}
{'arxiv_id': 'arXiv:2502.18734', 'title': 'Beyond RNNs: Benchmarking Attention-Based Image Captioning Models', 'authors': 'Hemanth Teja Yanambakkam, Rahul Chinthala', 'link': 'https://arxiv.org/abs/2502.18734', 'abstract': 'Image captioning is a challenging task at the intersection of computer vision and natural language processing, requiring models to generate meaningful textual descriptions of images. Traditional approaches rely on recurrent neural networks (RNNs), but recent advancements in attention mechanisms have demonstrated significant improvements. This study benchmarks the performance of attention-based image captioning models against RNN-based approaches using the MS-COCO dataset. We evaluate the effectiveness of Bahdanau attention in enhancing the alignment between image features and generated captions. The models are assessed using natural language processing metrics such as BLEU, METEOR, GLEU, and WER. Our results show that attention-based models outperform RNNs in generating more accurate and semantically rich captions, with better alignment to human evaluation. This work provides insights into the impact of attention mechanisms in image captioning and highlights areas for future improvements.', 'abstract_zh': '图像描述是一项结合计算机视觉和自然语言处理的挑战性任务，需要模型生成有意义的图像文本描述。传统方法依赖递归神经网络（RNN），但最近在注意力机制方面的进展表明，它们在提升了性能方面具有显著优势。本研究使用MS-COCO数据集，对比基于注意力机制的图像描述模型与基于RNN的方法，以评估其表现。我们评估了巴达诺注意力（Bahdanau attention）对图像特征与生成描述之间对齐效果的增强作用。模型通过BLEU、METEOR、GLEU和WER等自然语言处理指标进行评估。实验结果表明，基于注意力机制的模型在生成更准确且语义丰富的描述方面优于RNN，且更好地与人类评价对齐。本研究提供了关于注意力机制在图像描述中影响的见解，并指出了未来改进的方向。', 'title_zh': '超越RNN：基于注意力的图像描述模型基准测试'}
{'arxiv_id': 'arXiv:2502.18725', 'title': 'Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation', 'authors': 'Xin Liu, Ziyue Zhang, Jingxin Nie', 'link': 'https://arxiv.org/abs/2502.18725', 'abstract': 'Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.', 'abstract_zh': '传统的利用自然场景刺激的心理学实验在手动生成标注和生态效度方面面临挑战。为了解决这一问题，我们提出了一种新颖的范式，利用多模态大型语言模型（LLMs）作为代理，通过视觉问答（VQA）策略从自然场景图像中提取丰富的语义信息，分析人类的视觉语义表征。从LLM中提取的表示成功预测了fMRI测量的已建立的神经活动模式（例如，人脸、建筑），这验证了其可行性和揭示了跨皮层区域的层级语义组织。由LLM提取的表示构建的大脑语义网络识别出反映功能性和上下文关联的有意义的集群。这一创新方法为使用自然场景刺激研究大脑语义组织提供了一种有力的解决方案，克服了传统标注方法的局限性，并为进一步开展生态有效的认知探索铺平了道路。', 'title_zh': '与大脑对话：利用大型语言模型作为代理来建模大脑语义表示'}
{'arxiv_id': 'arXiv:2502.18702', 'title': 'A Cooperative Multi-Agent Framework for Zero-Shot Named Entity Recognition', 'authors': 'Zihan Wang, Ziqi Zhao, Yougang Lyu, Zhumin Chen, Maarten de Rijke, Zhaochun Ren', 'link': 'https://arxiv.org/abs/2502.18702', 'abstract': 'Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. It advances model self-learning abilities by incorporating self-annotated demonstrations. However, two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads LLMs during inference.\nIn this paper, we introduce the cooperative multi-agent system (CMAS), a novel framework for zero-shot NER that uses the collective intelligence of multiple agents to address the challenges outlined above. CMAS has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To enable controllable utilization of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence. Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.', 'abstract_zh': '零样本命名实体识别（NER）旨在开发能够从未标注文本语料库中识别实体的系统。由于涉及最少的人工干预，这一任务面临着巨大的挑战。最近的工作通过构建专有提示模板，将大规模语言模型（LLMs）应用于零样本NER，提升了模型的自我学习能力，并融入了自标注的示范实例。然而，仍存在两个重要挑战：（i）实体周围上下文之间的关联被忽略，导致实体类型预测错误或实体遗漏；（ii）通过浅层相似性策略检索的示范实例的盲目使用，在推理过程中的误导作用严重。\n\n在本文中，我们引入了协作多智能体系统（CMAS），这是一种用于零样本NER的新型框架，利用多个智能体的集体智能来解决上述挑战。CMAS 包含四个主要智能体：（i）自标注器、（ii）类型相关特征（TRF）提取器、（iii）示范鉴别器和（iv）总体预测器。为了明确捕捉实体周围上下文之间的关联，CMAS 将NER重新分解为两个子任务：识别命名实体和在目标句子中识别实体类型相关特征。为了使示范的利用可控，建立了示范鉴别器，其中包括自反思机制，自动评估目标句子的帮助评分。实验结果表明，CMAS 在六个基准测试中显著提高了零样本NER性能，包括特定领域和通用领域场景。此外，CMAS 在少量样本设置和使用各种LLM底层模型的情况下也显示了其有效性。', 'title_zh': '零样本命名实体识别的协作多智能体框架'}
{'arxiv_id': 'arXiv:2502.18685', 'title': 'Speaking the Right Language: The Impact of Expertise Alignment in User-AI Interactions', 'authors': 'Shramay Palta, Nirupama Chandrasekaran, Rachel Rudinger, Scott Counts', 'link': 'https://arxiv.org/abs/2502.18685', 'abstract': "Using a sample of 25,000 Bing Copilot conversations, we study how the agent responds to users of varying levels of domain expertise and the resulting impact on user experience along multiple dimensions. Our findings show that across a variety of topical domains, the agent largely responds at proficient or expert levels of expertise (77% of conversations) which correlates with positive user experience regardless of the user's level of expertise. Misalignment, such that the agent responds at a level of expertise below that of the user, has a negative impact on overall user experience, with the impact more profound for more complex tasks. We also show that users engage more, as measured by the number of words in the conversation, when the agent responds at a level of expertise commensurate with that of the user. Our findings underscore the importance of alignment between user and AI when designing human-centered AI systems, to ensure satisfactory and productive interactions.", 'abstract_zh': '通过对25,000场Bing Copilot对话样本的研究，我们探讨了代理在面对不同领域专业知识水平的用户时的响应方式及其对用户体验的多维度影响。研究发现，在各种主题领域中，代理的响应水平大多达到熟练或专家级别（占77%的对话），这与用户的体验满意度正相关，无论用户的专业水平如何。当代理的响应水平低于用户时，即出现不对齐的情况，这会损害整体的用户体验，尤其是在面对复杂任务时影响更为显著。我们还发现，当代理的响应水平与用户相当时，用户的参与度会更高，衡量的标准是以对话中的单词数量为准。本研究强调，在设计以用户为中心的人工智能系统时，确保用户与AI之间的对齐非常重要，以确保满意的和富有成效的互动。', 'title_zh': '合适的语言表达：领域一致性的用户-AI交互影响研究'}
{'arxiv_id': 'arXiv:2502.18673', 'title': 'Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations', 'authors': 'Ian Steenstra, Farnaz Nouraei, Timothy W. Bickmore', 'link': 'https://arxiv.org/abs/2502.18673', 'abstract': 'Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.', 'abstract_zh': '学习治疗性咨询涉及与模拟患者进行重要的角色扮演体验，目前的手动培训方法仅能提供间歇性的详细反馈。我们希望通过提供持续且详细的反馈来加速和优化咨询师的培训过程，使他们在与模拟患者互动时能够得到及时指导。我们的第一个应用领域是培训咨询师的动机性访谈技巧。动机性访谈是一种协作式咨询风格，在此过程中，患者在咨询师的引导下讨论其行为改变，而同理心咨询则是其关键组成部分。我们开发并评估了一个基于大语言模型的培训系统，该系统包括模拟患者和针对学习动机性访谈技巧的咨询师的逐轮反馈可视化。我们在专业和学生咨询师中进行了评估研究，表明该系统的高可用性和满意度。我们提出了开发自动化系统的设计建议，用于培训用户的咨询技能，并讨论了其在其他类型的社交技能训练中的广泛适用性。', 'title_zh': '支架式共情培养：基于模拟患者和句级表现可视化培训咨询师'}
{'arxiv_id': 'arXiv:2502.18635', 'title': 'Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems', 'authors': 'Matthew Barker, Andrew Bell, Evan Thomas, James Carr, Thomas Andrews, Umang Bhatt', 'link': 'https://arxiv.org/abs/2502.18635', 'abstract': 'While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.', 'abstract_zh': '尽管检索增强生成（RAG）已成为提高大型语言模型（LLM）系统性能的一种流行技术，但它引入了大量的选择、参数和超参数，这些都需要进行选择或调整。这包括LLM、嵌入和排名模型本身，以及控制RAG各个组件的超参数。然而，集体优化整个RAG或LLM系统的配置在多目标设置下仍然未被充分探索——主要是由于解空间难以解决、目标评估的噪音较大以及评估成本高昂。在本文中，我们介绍了首个针对整个LLM和RAG系统，在成本、延迟、安全性和对齐方面进行多目标参数优化的方法。我们发现，贝叶斯优化方法在两个新的RAG基准任务上显著优于基准方法，获得了更为优秀的帕累托前沿。我们的工作以重要的考虑为结束，指出设计多目标RAG系统的实践者需要注意的一些微妙之处，例如最优配置可能在不同任务和目标之间无法泛化。', 'title_zh': '更快、更低成本、更优性能：用于大语言模型和检索增强系统的目标优化超参数优化'}
{'arxiv_id': 'arXiv:2502.18632', 'title': 'Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems', 'authors': 'Zhangqi Duan, Nigel Fernandez, Sri Kanakadandi, Bita Akram, Andrew Lan', 'link': 'https://arxiv.org/abs/2502.18632', 'abstract': 'Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms. However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive quantitative and qualitative evaluations validating the effectiveness of KCGen-KT. On a real-world dataset of student code submissions to open-ended programming problems, KCGen-KT outperforms existing KT methods. We investigate the learning curves of generated KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. We also conduct a human evaluation to show that the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by human domain experts.', 'abstract_zh': '知识组件（KCs）映射到问题有助于模拟学生的学习过程，跟踪他们在细微技能上的掌握程度，从而在网上学习平台中促进个性化学习和反馈。然而，传统上由人类领域专家进行的KCs与问题的匹配和标记工作非常 labor-intensive。我们提出了一种完全自动化的基于大语言模型（LLM）的管道，用于生成和标记开放性编程问题的KCs。我们还开发了一种基于LLM的知识追踪（KT）框架，利用这些由LLM生成的KCs，我们称之为KCGen-KT。我们进行了广泛的数量性和定性的评估，验证了KCGen-KT的有效性。在实际的学生代码提交数据集中，KCGen-KT优于现有的知识追踪方法。我们研究了生成的KCs的学习曲线，并表明在性能因子分析（PFA）模型下，由大语言模型生成的KCs与人工编写的KCs具有相当的拟合程度。我们还进行了一项人类评估，表明我们的管道中的KCs标记准确性与人力领域专家相当。', 'title_zh': '自动知识组件生成与编码问题的知识追踪'}
{'arxiv_id': 'arXiv:2502.18545', 'title': 'PII-Bench: Evaluating Query-Aware Privacy Protection Systems', 'authors': 'Hao Shen, Zhouhong Gu, Haokai Hong, Weili Han', 'link': 'https://arxiv.org/abs/2502.18545', 'abstract': 'The widespread adoption of Large Language Models (LLMs) has raised significant privacy concerns regarding the exposure of personally identifiable information (PII) in user prompts. To address this challenge, we propose a query-unrelated PII masking strategy and introduce PII-Bench, the first comprehensive evaluation framework for assessing privacy protection systems. PII-Bench comprises 2,842 test samples across 55 fine-grained PII categories, featuring diverse scenarios from single-subject descriptions to complex multi-party interactions. Each sample is carefully crafted with a user query, context description, and standard answer indicating query-relevant PII. Our empirical evaluation reveals that while current models perform adequately in basic PII detection, they show significant limitations in determining PII query relevance. Even state-of-the-art LLMs struggle with this task, particularly in handling complex multi-subject scenarios, indicating substantial room for improvement in achieving intelligent PII masking.', 'abstract_zh': '大型语言模型（LLMs）的广泛应用引发了对用户提示中个人可识别信息（PII）暴露的重大隐私担忧。为应对这一挑战，我们提出了一种与查询无关的PII屏蔽策略，并引入了PII-Bench，这是首个全面评估隐私保护系统的框架。PII-Bench 包含了涵盖55个细分类别的2,842个测试样本，这些样本展示了从单个主体描述到复杂多方交互的各种场景。每个样本都精心设计了用户查询、上下文描述和标准答案，标准答案中包含与查询相关的PII。我们的实证研究表明，尽管当前模型在基本PII检测方面表现尚可，但在确定PII查询相关性方面仍存在显著局限性。即使是最先进的LLMs在这项任务上也遇到困难，尤其是在处理复杂多主体场景时，这表明在实现智能PII屏蔽方面还有很大的改进空间。', 'title_zh': 'PII-Bench：评估查询感知的隐私保护系统'}
{'arxiv_id': 'arXiv:2502.18536', 'title': 'FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA', 'authors': 'S M Sarwar', 'link': 'https://arxiv.org/abs/2502.18536', 'abstract': 'Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.', 'abstract_zh': '视觉问答要求模型通过整合视觉理解和文本理解来生成准确的答案。然而，视觉问答（VQA）模型仍然难以应对幻觉问题，即生成看似合理但实际上错误的回答，尤其是在知识驱动和分布外场景中。我们引入了FilterRAG，这是一个检索增强框架，将BLIP-VQA与检索增强生成相结合，利用外部知识源如维基百科和DBpedia来验证答案。FilterRAG在OK-VQA数据集上达到了36.5%的准确率，展示了其在提高幻觉抑制和增强鲁棒性方面的有效性，无论是在领域内还是分布外设置中。这些发现突显了FilterRAG在提高视觉问答系统实际部署能力方面的潜力。', 'title_zh': 'FilterRAG：零样本指导检索增强生成，以减轻VQA中的幻觉问题'}
{'arxiv_id': 'arXiv:2502.18531', 'title': 'Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline', 'authors': 'Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, Lei Wang', 'link': 'https://arxiv.org/abs/2502.18531', 'abstract': "Background: Recruitment for cohorts involving complex liver diseases, such as hepatocellular carcinoma and liver cirrhosis, often requires interpreting semantically complex criteria. Traditional manual screening methods are time-consuming and prone to errors. While AI-powered pre-screening offers potential solutions, challenges remain regarding accuracy, efficiency, and data privacy. Methods: We developed a novel patient pre-screening pipeline that leverages clinical expertise to guide the precise, safe, and efficient application of large language models. The pipeline breaks down complex criteria into a series of composite questions and then employs two strategies to perform semantic question-answering through electronic health records - (1) Pathway A, Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset Stances within an Agent Collaboration strategy, particularly in managing complex clinical reasoning scenarios. The pipeline is evaluated on three key metrics-precision, time consumption, and counterfactual inference - at both the question and criterion levels. Results: Our pipeline achieved high precision (0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled in complex reasoning, while Pathway A was effective in precise data extraction with faster processing times. Both pathways achieved comparable precision. The pipeline showed promising results in hepatocellular carcinoma (0.878) and cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient pipeline shows high precision in hepatopathy trials, providing promising solutions for streamlining clinical trial workflows. Its efficiency and adaptability make it suitable for improving patient recruitment. And its capability to function in resource-constrained environments further enhances its utility in clinical settings.", 'abstract_zh': '背景：对于涉及复杂肝脏疾病的队列研究，如肝细胞癌和肝硬化，招募工作往往需要解释复杂的筛查标准。传统的手工筛选方法耗时且容易出错。虽然人工智能（AI）辅助的预筛选提供了潜在的解决方案，但准确性、效率和数据隐私问题仍然是挑战。方法：我们开发了一种新颖的患者预筛选管道，该管道利用临床专业知识来引导大型语言模型的精确、安全和高效应用。该管道将复杂的筛查标准分解为一系列复合问题，然后采用两种策略通过电子健康记录进行语义问题回答：（1）途径A，拟人专家思维链策略；（2）途径B，代理协作中的预设立场策略，特别是在处理复杂临床推理场景时。该管道在问题和标准层面分别从精确度、时间消耗和反事实推理三个关键指标进行了评估。结果：我们的管道在标准层面实现了高精确度（0.921）和高效率（每任务耗时0.44秒）。途径B在复杂推理方面表现出色，而途径A在精确数据提取方面更有效且处理时间更快。两种途径在精确度方面表现相当。该管道在肝细胞癌（0.878）和肝硬化试验（0.843）中显示出令人鼓舞的结果。结论：这种数据安全和时间高效的管道在肝脏疾病试验中表现出高精度，为简化临床试验工作流程提供了有前景的解决方案。其高效性和适应性使其适合提高患者招募。并且，其能够在资源有限的环境中运行的能力进一步增强了其在临床环境中的实用性。', 'title_zh': '增强肝病临床试验效率：一种安全的大语言模型驱动的预筛选管道'}
{'arxiv_id': 'arXiv:2502.18513', 'title': 'Analyzing User Perceptions of Large Language Models (LLMs) on Reddit: Sentiment and Topic Modeling of ChatGPT and DeepSeek Discussions', 'authors': 'Krishnaveni Katta', 'link': 'https://arxiv.org/abs/2502.18513', 'abstract': "While there is an increased discourse on large language models (LLMs) like ChatGPT and DeepSeek, there is no comprehensive understanding of how users of online platforms, like Reddit, perceive these models. This is an important omission because public opinion can influence AI development, trust, and future policy. This study aims at analyzing Reddit discussions about ChatGPT and DeepSeek using sentiment and topic modeling to advance the understanding of user attitudes. Some of the significant topics such as trust in AI, user expectations, potential uses of the tools, reservations about AI biases, and ethical implications of their use are explored in this study. By examining these concerns, the study provides a sense of how public sentiment might shape the direction of AI development going forward. The report also mentions whether users have faith in the technology and what they see as its future. A word frequency approach is used to identify broad topics and sentiment trends. Also, topic modeling through the Latent Dirichlet Allocation (LDA) method identifies top topics in users' language, for example, potential benefits of LLMs, their technological applications, and their overall social ramifications. The study aims to inform developers and policymakers by making it easier to see how users comprehend and experience these game-changing technologies.", 'abstract_zh': '关于大型语言模型（LLMs）如ChatGPT和DeepSeek的研究讨论日益增多，但对在线平台用户（如Reddit用户）对这些模型的看法仍缺乏全面的理解。这是一项重要的缺失，因为公众意见能够影响AI的发展、信任以及未来政策的制定。本研究旨在通过情感分析和主题建模来分析Reddit上关于ChatGPT和DeepSeek的讨论，以增进对用户态度的理解。本研究探讨了几个重要主题，包括对AI的信任、用户期待、工具的潜在用途、对AI偏差的担忧以及其使用中的伦理问题。通过对这些关注点的分析，研究提供了公众观点如何可能影响未来AI发展方向的见解。报告还提到了用户对技术的信心以及他们对其未来有何期待。通过词频分析来识别广泛的主题和情感趋势。此外，使用潜在狄利克雷分配（LDA）方法的主题建模识别出用户语言中的顶级主题，例如LLMs的潜在好处、其技术应用以及其对社会的总体影响。本研究旨在通过使开发者和政策制定者更容易了解用户对这些变革性技术的理解和体验来提供信息。', 'title_zh': '分析 Reddit 用户对大型语言模型（LLMs）的看法：ChatGPT 和 DeepSeek 讨论的情感分析与主题建模'}
{'arxiv_id': 'arXiv:2502.18505', 'title': 'Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models', 'authors': 'Ranjan Sapkota, Shaina Raza, Manoj Karkee', 'link': 'https://arxiv.org/abs/2502.18505', 'abstract': 'Despite increasing discussions on open-source Artificial Intelligence (AI), existing research lacks a discussion on the transparency and accessibility of state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source Initiative (OSI) has recently released its first formal definition of open-source software. This definition, when combined with standard dictionary definitions and the sparse published literature, provide an initial framework to support broader accessibility to AI models such as LLMs, but more work is essential to capture the unique dynamics of openness in AI. In addition, concerns about open-washing, where models claim openness but lack full transparency, has been raised, which limits the reproducibility, bias mitigation, and domain adaptation of these models. In this context, our study critically analyzes SoTA LLMs from the last five years, including ChatGPT, DeepSeek, LLaMA, and others, to assess their adherence to transparency standards and the implications of partial openness. Specifically, we examine transparency and accessibility from two perspectives: open-source vs. open-weight models. Our findings reveal that while some models are labeled as open-source, this does not necessarily mean they are fully open-sourced. Even in the best cases, open-source models often do not report model training data, and code as well as key metrics, such as weight accessibility, and carbon emissions. To the best of our knowledge, this is the first study that systematically examines the transparency and accessibility of over 100 different SoTA LLMs through the dual lens of open-source and open-weight models. The findings open avenues for further research and call for responsible and sustainable AI practices to ensure greater transparency, accountability, and ethical deployment of these models.(DeepSeek transparency, ChatGPT accessibility, open source, DeepSeek open source)', 'abstract_zh': '尽管关于开源人工智能（AI）的讨论越来越多，现有研究尚未充分探讨最先进的（SoTA）大型语言模型（LLMs）的透明度和可访问性。开放源代码倡议（OSI）最近发布了其首个正式的开源软件定义。结合标准词典定义和零星的已发表文献，这些定义为更广泛的访问AI模型（如LLMs）提供了初步框架，但仍然需要大量工作以捕捉AI中独特开放动态的本质。此外，关于“开放漂洗”现象的担忧不断增多，即模型声称开放但缺乏全面透明度，这限制了这些模型的可再现性、偏见缓解和领域适应性。在此背景下，我们从过去五年中对ChatGPT、DeepSeek、LLaMA等SoTA LLMs进行批判性分析，评估其对透明度标准的遵守情况及部分开放的影响。具体而言，我们从两个视角——开源 vs. 开源权重——来考察透明度和可访问性。我们的研究发现，在某些模型被标示为开源的情况下，这并不意味着这些模型完全实现了开源。即使在最佳情况下，开源模型也常常不报告模型训练数据、代码以及诸如权重访问性和碳排放等关键指标。据我们所知，这是首次系统性地从开源和开放权重双重角度对超过100个SoTA LLMs的透明度和可访问性进行全面研究。研究结果开启了进一步研究的途径，并呼吁负责任和可持续的AI实践，以确保这些模型在更大程度上具有透明度、问责制和道德部署。\n\n（DeepSeek的透明度，ChatGPT的可访问性，开源，DeepSeek的开源）', 'title_zh': '全面分析ChatGPT、DeepSeek及其他领先大型语言模型的透明度与可访问性'}
{'arxiv_id': 'arXiv:2502.18504', 'title': 'TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice', 'authors': 'Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi', 'link': 'https://arxiv.org/abs/2502.18504', 'abstract': 'Jailbreaking large-language models (LLMs) involves testing their robustness against adversarial prompts and evaluating their ability to withstand prompt attacks that could elicit unauthorized or malicious responses. In this paper, we present TurboFuzzLLM, a mutation-based fuzzing technique for efficiently finding a collection of effective jailbreaking templates that, when combined with harmful questions, can lead a target LLM to produce harmful responses through black-box access via user prompts. We describe the limitations of directly applying existing template-based attacking techniques in practice, and present functional and efficiency-focused upgrades we added to mutation-based fuzzing to generate effective jailbreaking templates automatically. TurboFuzzLLM achieves $\\geq$ 95\\% attack success rates (ASR) on public datasets for leading LLMs (including GPT-4o \\& GPT-4 Turbo), shows impressive generalizability to unseen harmful questions, and helps in improving model defenses to prompt attacks.', 'abstract_zh': '本文探讨了大型语言模型（LLMs）的越狱过程，即测试模型在面对对抗性提示时的鲁棒性，并评估其抵御可能引发未经授权或恶意响应的提示攻击的能力。在本文中，我们提出了TurboFuzzLLM，这是一种基于变异的模糊测试技术，用于高效地发现一组有效的越狱模板。当这些模板与有害问题结合使用时，可以通过用户提示的黑盒访问方式引导目标LLM生成有害响应。我们描述了直接应用现有模板攻击技术在实践中的局限性，并提出了对基于变异的模糊测试进行的功能和效率优化改进，以自动生成有效的越狱模板。实验结果显示，TurboFuzzLLM在领先LLM（包括GPT-4o及GPT-4 Turbo）的公共数据集上实现了至少95%的攻击成功率（ASR），在未见过的有害问题上表现出强大的泛化能力，并有助于提高模型对提示攻击的防护能力。', 'title_zh': 'TurboFuzzLLM：基于变异的 fuzzing 技术加速实现大型语言模型的实际脱疆県攻击'}
{'arxiv_id': 'arXiv:2502.18499', 'title': 'Mechanistic Understanding of Language Models in Syntactic Code Completion', 'authors': 'Samuel Miller, Daking Rai, Ziyu Yao', 'link': 'https://arxiv.org/abs/2502.18499', 'abstract': "Recently, language models (LMs) have shown impressive proficiency in code generation tasks, especially when fine-tuned on code-specific datasets, commonly known as Code LMs. However, our understanding of the internal decision-making processes of Code LMs, such as how they use their (syntactic or semantic) knowledge, remains limited, which could lead to unintended harm as they are increasingly used in real life. This motivates us to conduct one of the first Mechanistic Interpretability works to understand how Code LMs perform a syntactic completion task, specifically the closing parenthesis task, on the CodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model requires middle-later layers until it can confidently predict the correct label for the closing parenthesis task. Additionally, we identify that while both multi-head attention (MHA) and feed-forward (FF) sub-layers play essential roles, MHA is particularly crucial. Furthermore, we also discover attention heads that keep track of the number of already closed parentheses precisely but may or may not promote a correct number of closing parentheses that are still missing, leading to a positive or negative impact on the model's performance.", 'abstract_zh': '近年来，语言模型（LMs）在代码生成任务中表现出令人印象深刻的熟练程度，尤其是在针对代码特定数据集微调后，这类模型通常被称为代码语言模型（Code LMs）。然而，我们对Code LMs内部决策过程的理解仍然有限，例如它们如何利用其（句法或语义）知识，这可能导致随着它们在现实生活中的使用增加，出现意想不到的危害。这促使我们开展一项关于Code LMs如何在CodeLlama-7b模型（Roziere et al. 2023）上执行句法完成任务（具体为闭括号任务）的机制可解释性工作。我们的研究发现表明，模型直到中后期层才能有信心预测闭括号任务的正确标签。此外，我们还发现虽然多头注意力（MHA）和前馈（FF）子层都起着重要作用，但MHA尤为重要。进一步地，我们发现一些注意力头能够精确跟踪已闭合的括号数量，但这些头可能或可能不会促进仍然缺失的正确数量的闭括号，这可能对模型的性能产生积极或消极的影响。', 'title_zh': '语言模型在语法代码补全中的机制理解'}
{'arxiv_id': 'arXiv:2502.18487', 'title': 'AuPair: Golden Example Pairs for Code Repair', 'authors': 'Aditi Mavalankar, Hassan Mansoor, Zita Marinho, Masha Samsikova, Tom Schaul', 'link': 'https://arxiv.org/abs/2502.18487', 'abstract': 'Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) without fine-tuning. An important task that can benefit from additional inference-time compute is self-repair; given an initial flawed response, or guess, the LLM corrects its own mistake and produces an improved response, or fix. We leverage the in-context learning ability of LLMs to perform self-repair in the coding domain. The key contribution of our paper is an approach that synthesises and selects an ordered set of golden example pairs, or AuPairs, of these initial guesses and subsequent fixes for the corresponding problems. Each such AuPair is provided as a single in-context example at inference time to generate a repaired solution. For an inference-time compute budget of $N$ LLM calls per problem, $N$ AuPairs are used to generate $N$ repaired solutions, out of which the highest-scoring solution is selected as the final answer. The underlying intuition is that if the LLM is given a different example of fixing an incorrect guess each time, it can subsequently generate a diverse set of repaired solutions. Our algorithm selects these AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on 5 LLMs across 7 competitive programming datasets for the code repair task. Our algorithm yields a significant boost in performance compared to best-of-$N$ and self-repair, and also exhibits strong generalisation across datasets and models. Moreover, our approach shows significantly stronger scaling with inference-time compute budget compared to baselines.', 'abstract_zh': '在推理时间计算方面扩展已有策略已被证明是提高大型语言模型（LLMs）性能的有效方法，而且无需进行微调。一种可以从额外的推理时间计算中受益的重要任务是自我修复；给定一个初始的有缺陷的响应或猜测，LLM能够纠正自己的错误并生成一个改进的响应或修复。我们利用LLMs的在上下文学习能力，在编程领域中实现自我修复。本文的主要贡献是一种合成并选择一个有序的“黄金示例对”（AuPairs）集的方法，这些对反映了初始猜测和随后的修复。每个这样的AuPair在推理时作为单一的在上下文示例提供，以生成一个修复的解决方案。对于每个问题的推理计算预算为$N$次LLM调用，使用$N$个AuPairs生成$N$个修复的解决方案，从中选择得分最高的解决方案作为最终答案。基本的原理是，如果每次给LLM提供一个不同的修复错误猜测的示例，它可以随后生成一系列多样的修复解决方案。我们的算法通过最大化互补性和实用性来选择这些AuPairs。我们展示了算法在7个编程竞赛数据集上的结果，涉及5个不同的LLMs进行代码修复任务。相较于“最佳$N$次”和自我修复，我们的算法显著提升了性能，同时也展示了在不同数据集和模型上的强大泛化能力。此外，我们的方法在推理计算预算方面相比基线模型显示出了更强的扩展性。', 'title_zh': 'AuPair：代码修复的黄金例对'}
{'arxiv_id': 'arXiv:2502.18480', 'title': 'QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration', 'authors': 'Shaola Ren, Li Ke, Longtao Huang, Dehong Gao, Hui Xue', 'link': 'https://arxiv.org/abs/2502.18480', 'abstract': 'Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items.', 'abstract_zh': '自动提取有效的查询在信息检索中具有挑战性，尤其是在有毒内容探索中，因为此类内容往往会伪装。随着生成型大规模语言模型（LLM）近年来取得的成就，我们能够利用LLM的能力直接提取类似内容探索的有效查询。本研究提出了一种基于大规模语言模型的查询提取方法，名为QExplorer，用于有毒内容探索。QExplorer方法包含两个阶段的训练过程：指令监督微调（SFT）和使用直接偏好优化（DPO）进行偏好对齐，以及通过搜索系统反馈构建数据集。为了验证QExplorer的有效性，我们在我们的实际系统上进行了离线和在线实验。离线实验证明，我们自动查询提取的性能优于几种LLM和人类。在线部署表明有毒物品的检测显著增加。', 'title_zh': 'QExplorer：基于大型语言模型的有害内容查询提取'}
{'arxiv_id': 'arXiv:2502.18471', 'title': 'FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data', 'authors': 'Ankur Sinha, Chaitanya Agarwal, Pekka Malo', 'link': 'https://arxiv.org/abs/2502.18471', 'abstract': 'Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.', 'abstract_zh': '大型语言模型（LLMs）在生成类人类的响应方面表现出色，但在处理需要实时信息访问的交互任务时常常力不从心。这种限制在金融领域尤为突出，因为模型必须访问最新的信息，如最近的新闻或价格变动，以支持决策。为了解决这一问题，我们引入了一种名为“金融代理”的知识接地方法，该方法使LLMs能够使用实时文本和表格数据来处理金融查询。我们的贡献主要体现在三个方面：首先，我们开发了一个包含超过50,000个金融查询及其所需上下文的数据集。其次，我们在包含1400万条路透社和德迅社的金融新闻文章以及1200万条美国证券交易委员会（SEC）披露文件的数据集上训练了一个自定义的70亿参数语言模型——FinBloom 7B。第三，我们使用上述的金融上下文数据集对FinBloom 7B进行微调，使其能够作为一个金融代理使用。该代理可以生成相关的金融上下文，从而高效地检索实时数据来回答用户的查询。通过减少延迟并消除用户手动提供准确数据的需要，我们的方法显著增强了LLMs处理动态金融任务的能力。我们提出的这种方法可以简化实时金融决策、算法交易以及相关任务，尤其适用于高频率数据流的环境。', 'title_zh': 'FinBloom: 基于实时金融数据的知识接地大规模语言模型'}
