# Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models 

**Title (ZH)**: 作为评判者：通过大型语言模型的评判一致性改进检索增强生成的评估 

**Authors**: Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.18817)  

**Abstract**: Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at this https URL. 

**Abstract (ZH)**: 提取增强生成（RAG）已被证明在减轻大规模语言模型（LLMs）的幻觉方面具有有效性。然而，现有的自动化评估指标无法公正地评估RAG模型在训练和评估阶段生成的输出。基于LLM的判断模型具有生成高质量判断的潜力，但它们对评估提示的高度敏感性会导致在判断RAG模型输出时的一致性问题。本文介绍了Judge-Consistency（ConsJudge）方法，旨在增强LLMs以生成更加准确的RAG模型评价。具体而言，ConsJudge促使LLMs基于多种判断维度的不同组合生成不同的判断，并利用判断一致性来评估这些判断，从中选择接受和拒绝的判断用于DPO训练。我们的实验表明，ConsJudge能够有效地为各种RAG模型和数据集中的RAG模型优化提供更加准确的判断。进一步的分析显示，ConsJudge生成的判断与优秀的LLM高度一致。所有代码可在以下链接处获取：https://xxxxx（请注意，这里的URL应替换为实际的代码存放地址）。 

---
# FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA 

**Title (ZH)**: FilterRAG：减轻VQA中幻觉现象的零样本引导检索增强生成方法 

**Authors**: S M Sarwar  

**Link**: [PDF](https://arxiv.org/pdf/2502.18536)  

**Abstract**: Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment. 

**Abstract (ZH)**: 视觉问答要求模型通过结合视觉和文本理解来生成准确的答案。然而，视觉问答（VQA）模型在应对幻觉问题时仍然存在挑战，特别是在知识驱动和分布外（Out-of-Distribution, OOD）场景中生成富有说服力但错误的答案。我们引入了FilterRAG，这是一种检索增强框架，将BLIP-VQA与检索增强生成相结合，利用外部知识源（如维基百科和DBpedia）来验证答案。在OK-VQA数据集上，FilterRAG实现了36.5%的准确率，证明了其在减少幻觉和提高系统鲁棒性方面的有效性能，无论是对于领域内还是分布外的场景。这些发现突显了FilterRAG在提升实际部署中视觉问答系统性能方面的潜力。 

---
# OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models 

**Title (ZH)**: OntologyRAG：利用本体知识图谱和大规模语言模型增强的检索增强生成（RAG）方法在生物医学代码映射中的表现与效率提升 

**Authors**: Hui Feng, Yuntzu Yin, Emiliano Reynares, Jay Nanavati  

**Link**: [PDF](https://arxiv.org/pdf/2502.18992)  

**Abstract**: Biomedical ontologies, which comprehensively define concepts and relations for biomedical entities, are crucial for structuring and formalizing domain-specific information representations. Biomedical code mapping identifies similarity or equivalence between concepts from different ontologies. Obtaining high-quality mapping usually relies on automatic generation of unrefined mapping with ontology domain fine-tuned language models (LMs), followed by manual selections or corrections by coding experts who have extensive domain expertise and familiarity with ontology schemas. The LMs usually provide unrefined code mapping suggestions as a list of candidates without reasoning or supporting evidence, hence coding experts still need to verify each suggested candidate against ontology sources to pick the best matches. This is also a recurring task as ontology sources are updated regularly to incorporate new research findings. Consequently, the need of regular LM retraining and manual refinement make code mapping time-consuming and labour intensive. In this work, we created OntologyRAG, an ontology-enhanced retrieval-augmented generation (RAG) method that leverages the inductive biases from ontological knowledge graphs for in-context-learning (ICL) in large language models (LLMs). Our solution grounds LLMs to knowledge graphs with unrefined mappings between ontologies and processes questions by generating an interpretable set of results that include prediction rational with mapping proximity assessment. Our solution doesn't require re-training LMs, as all ontology updates could be reflected by updating the knowledge graphs with a standard process. Evaluation results on a self-curated gold dataset show promises of using our method to enable coding experts to achieve better and faster code mapping. The code is available at this https URL. 

**Abstract (ZH)**: 生物医学本体是以全面定义生物医学实体的概念及其关系而著称，对于结构化和形式化专业领域信息表示至关重要。生物医学代码映射识别不同本体概念之间的相似性或等价性。获得高质量的映射通常依赖于使用细调过的本体领域语言模型（LMs）自动生成未经精炼的映射，然后由具有丰富领域专业知识和本体 schema 熟悉度的编码专家进行手工选择或修正。这些语言模型通常只是提供未经精炼的代码映射建议列表，而不进行推理或提供支持证据，因此编码专家仍需验证每个建议的候选人以挑选最佳匹配。由于本体源定期更新以纳入新的研究发现，这一任务是重复执行的。因此，定期重新训练 LM 和手动精炼使得代码映射耗时且劳动密集。在本项工作中，我们创建了 OntologyRAG，这是一种利用本体知识图谱的归纳偏倚增强了检索增强生成（RAG）方法，在大型语言模型（LLMs）的上下文学习（ICL）中发挥作用。我们的解决方案将 LLMs 接地于知识图谱，并通过生成包括预测理由和映射相似度评估的可解释结果集来处理问题。我们的解决方案无需重新训练 LM，因为所有本体更新都可以通过标准化过程更新知识图谱来反映。在自制作的黄金数据集上的评估结果表明，使用我们的方法可以帮助编码专家更快、更好地进行代码映射。代码可在以下 URL 获取：[提供的URL]。 

---
# Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions 

**Title (ZH)**: 空间RAG：扩充生成模型在实际空间推理问题中的空间检索方法 

**Authors**: Dazhou Yu, Riyang Bao, Gengchen Mai, Liang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.18470)  

**Abstract**: Spatial reasoning remains a challenge for Large Language Models (LLMs), which struggle with spatial data retrieval and reasoning. We propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to spatial tasks by integrating sparse spatial retrieval (spatial databases) and dense semantic retrieval (LLM-based similarity). A multi-objective ranking strategy balances spatial constraints and semantic relevance, while an LLM-guided generator ensures coherent responses. Experiments on a real-world tourism dataset show that Spatial-RAG significantly improves spatial question answering, bridging the gap between LLMs and spatial intelligence. 

**Abstract (ZH)**: 空间推理仍然是大型语言模型（LLMs）的一个挑战，它们在处理空间数据检索和推理方面存在困难。我们提出了空间检索增强生成（Spatial-RAG）框架，该框架通过结合稀疏空间检索（空间数据库）和密集语义检索（基于LLM的相似性），将RAG扩展到空间任务中。多目标排名策略平衡了空间约束和语义相关性，而LLM引导的生成器确保了连贯的响应。在现实世界的旅游数据集上的实验表明，Spatial-RAG 显著提高了空间问题的回答能力，弥补了LLMs在空间智能方面的差距。 

---
# Efficient Federated Search for Retrieval-Augmented Generation 

**Title (ZH)**: 高效的联邦搜索以增强生成检索 

**Authors**: Rachid Guerraoui, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos  

**Link**: [PDF](https://arxiv.org/pdf/2502.19280)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities across various domains but remain susceptible to hallucinations and inconsistencies, limiting their reliability. Retrieval-augmented generation (RAG) mitigates these issues by grounding model responses in external knowledge sources. Existing RAG workflows often leverage a single vector database, which is impractical in the common setting where information is distributed across multiple repositories. We introduce RAGRoute, a novel mechanism for federated RAG search. RAGRoute dynamically selects relevant data sources at query time using a lightweight neural network classifier. By not querying every data source, this approach significantly reduces query overhead, improves retrieval efficiency, and minimizes the retrieval of irrelevant information. We evaluate RAGRoute using the MIRAGE and MMLU benchmarks and demonstrate its effectiveness in retrieving relevant documents while reducing the number of queries. RAGRoute reduces the total number of queries up to 77.5% and communication volume up to 76.2%. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各个领域展现了卓越的能力，但仍然容易产生幻觉和不一致性，这限制了其可靠性。检索增强生成（RAG）通过将模型响应与外部知识源相结合，缓解了这些问题。现有的RAG工作流程通常依赖单一向量数据库，但在信息分散存储于多个存储库的情况下，这种方式往往是不切实际的。我们提出了RAGRoute，这是一种新颖的联邦RAG搜索机制。RAGRoute使用轻量级的神经网络分类器在查询时动态选择相关数据源。通过不查询每个数据源，这种方法显著减少了查询开销，提高了检索效率，并最小化了无关信息的检索。我们使用MIRAGE和MMLU基准评估了RAGRoute，并展示了其在获取相关文档的同时减少查询次数的效果。RAGRoute将总查询次数减少多达77.5%，通信量减少多达76.2%。 

---
