{'arxiv_id': 'arXiv:2502.19411', 'title': 'Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs', 'authors': 'Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley', 'link': 'https://arxiv.org/abs/2502.19411', 'abstract': "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.", 'abstract_zh': '在大规模语言模型（LLMs）中，代码和推理相互强化：代码提供了抽象、模块化和逻辑驱动的结构，支持推理；而推理则将高层次的目标转化为更小的、可执行的步骤，驱动更为高级的代码智能。在这项研究中，我们探讨了代码作为增强推理的结构化媒介的作用：它提供了可验证的执行路径，强制执行逻辑分解，并允许运行时验证。我们还研究了推理改善如何将代码智能从基本完成提升到高级能力，从而使模型能够通过规划和调试来应对复杂的软件工程任务。最后，我们指出了关键挑战，并提出了未来的研究方向，以加强这种协同作用，最终提高LLMs在这两个方面的性能。', 'title_zh': '从代码中思考，从思考中编码：关于代码增强推理与推理驱动的代码智能在大语言模型中的综述'}
{'arxiv_id': 'arXiv:2502.19363', 'title': 'DataMan: Data Manager for Pre-training Large Language Models', 'authors': 'Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao', 'link': 'https://arxiv.org/abs/2502.19363', 'abstract': "The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by ``reverse thinking'' -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l=5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources.", 'abstract_zh': '大型语言模型（LLMs）性能的提升，源于数据规模定律的影响，使得预训练数据的选择显得日益重要。然而，现有的方法依赖于有限的经验法则和人类直觉，缺乏全面且清晰的指导方针。为了解决这一问题，我们受到“逆向思维”的启发——促使LLMs自我识别哪些标准对其性能有益。由于其预训练能力与困惑度（PPL）密切相关，我们从文本困惑度异常的原因中提炼了14个质量标准，并引入了15个常见的应用领域以支持跨领域混合。在本文中，我们训练了一个数据管理器（DataMan），使其从点wise评分中学习质量评分和领域识别能力，并使用其对一个包含447B个标记的预训练语料库进行标注，赋予其14个质量评分和领域类型。我们的实验验证了这种方法的有效性，通过使用DataMan选择30B个标记以培训一个参数量为1.3B的语言模型，展示了在上下文学习（ICL）、困惑度和指令跟随能力等方面相较于现有基准的重大改进。基于整体评分l=5的最优模型，超越了使用均匀采样训练的数据量多50%的模型。我们继续使用DataMan标注的高评分、领域特定的数据进行预训练，以增强特定领域的ICL性能，从而验证了DataMan的跨领域能力。我们的研究结果强调了质量评分的重要性、质量标准之间的互补性质以及它们与困惑度的低相关性，并分析了PPL与ICL性能之间的不匹配。我们还详细分析了我们的预训练数据集，探讨了其构成、质量评分的分布以及原始文档来源。', 'title_zh': 'DataMan：大规模语言模型预先训练的数据管理器'}
{'arxiv_id': 'arXiv:2502.19361', 'title': 'Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?', 'authors': 'Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.19361', 'abstract': 'Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.', 'abstract_zh': '近年来，o1-like模型受到了广泛关注，这些模型能够生成较长的链式思维（Chain-of-Thought, CoT）推理步骤，以提高现有大型语言模型（LLMs）的推理能力。本文旨在理解这些长CoT的质量，并评估现有LLMs在处理这些长CoT时的批判能力。为此，我们引入了DeltaBench，其中包括来自不同o1-like模型（例如QwQ、DeepSeek-R1）的生成长CoT，用于不同推理任务（如数学、代码、一般推理），以衡量检测长CoT推理错误的能力。基于DeltaBench，我们首先对生成的长CoT进行细致分析，以发现不同o1-like模型的有效性和效率。然后，我们对现有的过程奖励模型（PRMs）和批判模型进行了广泛的评估，以检测每一步骤的错误，这旨在研究现有PRMs和批判模型的边界和限制。最后，我们希望通过DeltaBench指导模型开发人员更好地理解其模型的长CoT推理能力。', 'title_zh': '大型语言模型能否检测长链式推理中的错误？'}
{'arxiv_id': 'arXiv:2502.19230', 'title': 'Two Heads Are Better Than One: Dual-Model Verbal Reflection at Inference-Time', 'authors': 'Jiazheng Li, Yuxiang Zhou, Junru Lu, Gladys Tyen, Lin Gui, Cesare Aloisi, Yulan He', 'link': 'https://arxiv.org/abs/2502.19230', 'abstract': 'Large Language Models (LLMs) often struggle with complex reasoning scenarios. While preference optimization methods enhance reasoning performance through training, they often lack transparency in why one reasoning outcome is preferred over another. Verbal reflection techniques improve explainability but are limited in LLMs\' critique and refinement capacity. To address these challenges, we introduce a contrastive reflection synthesis pipeline that enhances the accuracy and depth of LLM-generated reflections. We further propose a dual-model reasoning framework within a verbal reinforcement learning paradigm, decoupling inference-time self-reflection into specialized, trained models for reasoning critique and refinement. Extensive experiments show that our framework outperforms traditional preference optimization methods across all evaluation metrics. Our findings also show that "two heads are better than one", demonstrating that a collaborative Reasoner-Critic model achieves superior reasoning performance and transparency, compared to single-model approaches.', 'abstract_zh': '大型语言模型（LLMs）在处理复杂推理场景时常常存在困难。虽然偏好优化方法能够通过训练提升推理性能，但通常缺乏一种清晰的解释，说明为何某个推理结果被优先考虑。口头反思技术虽然可以提高解释性，但它们在LLMs的批评和改进方面能力有限。为了解决这些问题，我们提出了一种对比性反思合成流水线，该流水线能够提升LLM生成的反思的准确性和深度。此外，我们还在口头强化学习范式下提出了一种双模型推理框架，将推理时间的自我反思分拆为专门培训的模型，用于推理批评和改进。大量的实验表明，我们的框架在所有评估指标上都优于传统的偏好优化方法。我们的研究结果还显示，“双模型优于单模型”，表明合作式的推理-批评模型在推理性能和透明度方面显著优于单模型方法。', 'title_zh': '两个头胜过一个头：推理时的双模型语言反思'}
{'arxiv_id': 'arXiv:2502.19160', 'title': 'Detecting Linguistic Indicators for Stereotype Assessment with Large Language Models', 'authors': 'Rebekka Görge, Michael Mock, Héctor Allende-Cid', 'link': 'https://arxiv.org/abs/2502.19160', 'abstract': 'Social categories and stereotypes are embedded in language and can introduce data bias into Large Language Models (LLMs). Despite safeguards, these biases often persist in model behavior, potentially leading to representational harm in outputs. While sociolinguistic research provides valuable insights into the formation of stereotypes, NLP approaches for stereotype detection rarely draw on this foundation and often lack objectivity, precision, and interpretability. To fill this gap, in this work we propose a new approach that detects and quantifies the linguistic indicators of stereotypes in a sentence. We derive linguistic indicators from the Social Category and Stereotype Communication (SCSC) framework which indicate strong social category formulation and stereotyping in language, and use them to build a categorization scheme. To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment. Based on an empirical evaluation of the importance of different linguistic indicators, we learn a scoring function that measures the linguistic indicators of a stereotype. Our annotations of stereotyped sentences show that these indicators are present in these sentences and explain the strength of a stereotype. In terms of model performance, our results show that the models generally perform well in detecting and classifying linguistic indicators of category labels used to denote a category, but sometimes struggle to correctly evaluate the associated behaviors and characteristics. Using more few-shot examples within the prompts, significantly improves performance. Model performance increases with size, as Llama-3.3-70B-Instruct and GPT-4 achieve comparable results that surpass those of Mixtral-8x7B-Instruct, GPT-4-mini and Llama-3.1-8B-Instruct.', 'abstract_zh': '社会类别和刻板印象嵌入在语言中，并可能在大型语言模型（LLMs）的数据中引入偏差。尽管采取了防护措施，这些偏差常常在模型的行为中持续存在，潜在地导致输出中的代表性伤害。尽管社会语言学研究为刻板印象的形成提供了宝贵的知识，但用于刻板印象检测的NLP方法很少以此为基础，并且往往缺乏客观性、精确性和可解释性。为弥补这一缺口，本文提出了一种新方法，用于检测和量化句子中的刻板印象语言指标。我们从社会类别和刻板印象交流（SCSC）框架中提取语言指标，这些指标显示了语言中强烈的社会类别构建和刻板印象，利用这些指标构建分类方案。为了自动化这一方法，使用基于上下文学习的方法对不同的LLM进行指令，使其将该方法应用于一个句子，在此过程中，LLM检查语言属性并提供细粒度评估的基础。通过对不同语言指标重要性的实证评估，我们学习了一个评分函数，用于衡量一种刻板印象的语言指标。我们的对带有刻板印象句子的标注表明，这些指标在这类句子中存在，并解释了刻板印象的强度。从模型性能来看，我们的结果显示，模型在检测和分类用以表示某一类别的类别标签的语言指标方面表现普遍良好，但在正确评估相关行为和特征方面存在问题。在提示中使用更少的示范示例显著提高了性能。模型性能随规模增加而提高，Llama-3.3-70B-Instruct和GPT-4的性能达到了可比的结果，超过了Mixtral-8x7B-Instruct、GPT-4-mini和Llama-3.1-8B-Instruct的表现。', 'title_zh': '使用大规模语言模型检测 stereotypes 的语言指标'}
{'arxiv_id': 'arXiv:2502.19148', 'title': 'Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs', 'authors': 'Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang', 'link': 'https://arxiv.org/abs/2502.19148', 'abstract': "How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.", 'abstract_zh': '如何根据静态通用数据集使大型语言模型（LLMs）与用户偏好对齐已经被频繁研究。然而，用户的偏好通常是个性化、变化的，并且在文化、价值观或时间等方面存在多样性。这导致了一个问题：实际用户的偏好往往与模型开发者在实际使用LLMs时训练的偏好不一致。由于我们无法为每一个需求收集足够多的数据并对模型进行重新训练，因此在测试时基于主LLM研究高效的实时偏好自适应方法变得尤为重要。为此，我们引入了Amulet，这是一种新颖的、无需训练的框架，它将每个词元的解码过程形式化为受简单用户提供的提示引导的单独在线学习问题，从而实现针对用户个性化偏好的实时优化。为了减少每次词元优化过程带来的计算成本，我们还为优化过程中的每一迭代步骤提供了闭式解，从而将计算时间成本降低到可以忽略的水平。详细的实验结果表明，Amulet能够在不同LLM、数据集和用户偏好的组合环境中实现显著的性能提升，同时保持可接受的计算效率。', 'title_zh': '灵符：测试时重新对齐以适应个性化偏好的语言模型调整方法'}
{'arxiv_id': 'arXiv:2502.19127', 'title': 'Self-Memory Alignment: Mitigating Factual Hallucinations with Generalized Improvement', 'authors': 'Siyuan Zhang, Yichi Zhang, Yinpeng Dong, Hang Su', 'link': 'https://arxiv.org/abs/2502.19127', 'abstract': "Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. While post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its existing memory--the knowledge acquired from pre-training data. We introduce self-memory alignment (SMA), which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments show that SMA significantly improves LLMs' overall performance, with consistent enhancement across various benchmarks concerning factuality, as well as helpfulness and comprehensive skills.", 'abstract_zh': '大型语言模型（LLMs）往往难以使其回答与客观事实保持一致，从而导致事实性幻觉问题，这类问题难以检测且可能误导缺乏相关知识的用户。虽然已经采用了一些后训练技术来解决这一问题，但现有方法通常存在泛化能力差和不同能力之间的权衡。在本文中，我们提出通过直接增强LLM精确利用其现有记忆——预训练数据中获得的知识的基本能力来解决这一问题。我们引入了自我记忆对齐（SMA）的方法，通过对生成的响应进行微调以回答精确且简单的事实性问题，并通过偏好优化来实现。此外，我们构建了FactualBench，这是一个全面且精确的事实性问答数据集，包含跨越21个领域的181,000个中文数据，旨在促进评估和训练。广泛实验证明，SMA 显著提升了LLM的整体性能，在各种涉及事实性、帮助性和综合技能的基准测试中均表现出一致性改进。', 'title_zh': '自我记忆对齐：通过普遍改进减轻事实幻觉'}
{'arxiv_id': 'arXiv:2502.19064', 'title': 'Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A Comparative Study Using the Consensual Assessment Technique', 'authors': 'Piotr Sawicki, Marek Grześ, Dan Brown, Fabrício Góes', 'link': 'https://arxiv.org/abs/2502.19064', 'abstract': 'The Consensual Assessment Technique (CAT) evaluates creativity through holistic expert judgments. We investigate the use of two advanced Large Language Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a methodology inspired by the CAT. Using a dataset of 90 poems, we found that these LLMs can surpass the results achieved by non-expert human judges at matching a ground truth based on publication venue, particularly when assessing smaller subsets of poems. Claude-3-Opus exhibited slightly superior performance than GPT-4o. We show that LLMs are viable tools for accurately assessing poetry, paving the way for their broader application into other creative domains.', 'abstract_zh': '共识评价技术（Consensual Assessment Technique, CAT）通过整体专家判断来评估创造力。我们研究了使用两种先进的大型语言模型（Large Language Models, LLMs）——Claude-3-Opus 和 GPT-4o——通过借鉴CAT的方法学来评价诗歌的能力。我们使用了90首诗歌的数据集，发现这些LLMs在基于出版平台的标准上，能够超越非专家人类评委的表现，尤其是在评估诗歌的子集时表现更为突出。此外，Claude-3-Opus 的表现略优于GPT-4o。我们展示了LLMs作为一种能够准确评价诗歌的可行工具，并为它们在其他创意领域的广泛应用铺平了道路。', 'title_zh': '大型语言模型能否在诗词评价中超越非专家？基于共识评估技术的比较研究'}
{'arxiv_id': 'arXiv:2502.19008', 'title': 'Binary Neural Networks for Large Language Model: A Survey', 'authors': 'Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang', 'link': 'https://arxiv.org/abs/2502.19008', 'abstract': 'Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.', 'abstract_zh': '大语言模型（LLMs）在自然语言处理（NLP）领域有着广泛的应用，如GPT-4和Llama。然而，随着模型参数量的指数级增长，LLMs带来了显著的资源开销。低比特量化作为一种关键技术，通过降低模型参数、激活值和梯度的位宽来减少内存使用和计算需求。以往针对LLMs的量化方法主要采用后训练量化（PTQ）和量化感知训练（QAT）。PTQ不需要对原始模型进行任何重新训练，而QAT则在训练过程中优化精度以获得最佳的量化参数。BitNet团队提出了一个完全不同的方法，在模型训练开始时进行量化，并在训练过程中使用低精度的二进制权重。这一方法促成了许多针对大型语言模型的二进制量化技术的出现。本文对该领域的各种二进制量化技术进行了全面综述。具体而言，我们将介绍二进制量化技术在深度神经网络中的应用，并进一步探讨其在LLMs中的应用，回顾它们的各种贡献、实现方法和应用场景。', 'title_zh': '大型语言模型中的二值神经网络：一个综述'}
{'arxiv_id': 'arXiv:2502.18993', 'title': 'MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering', 'authors': 'Teng Lin', 'link': 'https://arxiv.org/abs/2502.18993', 'abstract': 'Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs\' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.', 'abstract_zh': '多实体问答（MEQA）对大规模语言模型（LLM）和检索增强生成（RAG）系统构成了重大挑战，这些系统经常难以整合分散在多篇文档中的信息。虽然现有的方法在单文档理解方面表现出色，但在跨文档聚合信息方面往往存在问题，尤其是在处理稠密实体问题（如“ACM院士在各个学科中的分布情况是什么？”）时尤为明显。这些问题需要整合来自异构来源的实体中心见解（如维基百科页面）。为解决这一问题，我们提出了MEBench，一个新颖的多文档、多实体基准测试，旨在系统评估LLM在检索、整合和推理分散信息方面的能力。该基准测试包含4,780个问题，这些问题被系统地归类为三个主要类别，并进一步细分为八种不同的类型，确保涵盖了广泛的真实世界多实体推理场景。我们的实验结果显示最新的LLM（如GPT-4、Llama-3）和RAG流水线存在关键局限性：即使最先进的模型在MBench上的准确率也只有59%。该基准测试强调了在MEQA任务中信息提取的完整性和事实精确度的重要性，并使用实体关联F1（EA-F1）指标进行粒度级别的实体级正确性和归属有效性的评估。MEBench不仅突显了当前LLM框架系统的缺陷，也为构建稳健、实体感知的问答架构提供了基础。', 'title_zh': 'MEBench：评估跨文档多实体问答的大语言模型基准测试'}
{'arxiv_id': 'arXiv:2502.18980', 'title': 'PEToolLLM: Towards Personalized Tool Learning in Large Language Models', 'authors': 'Qiancheng Xu, Yongqi Li, Heming Xia, Fan Liu, Min Yang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18980', 'abstract': "Tool learning has emerged as a promising direction by extending Large Language Models' (LLMs) capabilities with external tools. Existing tool learning studies primarily focus on the general-purpose tool-use capability, which addresses explicit user requirements in instructions. However, they overlook the importance of personalized tool-use capability, leading to an inability to handle implicit user preferences. To address the limitation, we first formulate the task of personalized tool learning, which integrates user's interaction history towards personalized tool usage. To fill the gap of missing benchmarks, we construct PEToolBench, featuring diverse user preferences reflected in interaction history under three distinct personalized settings, and encompassing a wide range of tool-use scenarios. Moreover, we propose a framework PEToolLLaMA to adapt LLMs to the personalized tool learning task, which is trained through supervised fine-tuning and direct preference optimization. Extensive experiments on PEToolBench demonstrate the superiority of PEToolLLaMA over existing LLMs.", 'abstract_zh': '工具学习作为一种有前景的方向，通过扩展大型语言模型（LLMs）的功能，使其能够使用外部工具。现有的工具学习研究主要集中在通用工具使用能力上，该能力针对指令中的明确用户需求。然而，它们未能重视个性化工具使用能力的重要性，导致无法处理用户的隐含偏好。为了弥补这一局限，我们首先制定了个性化工具学习任务，该任务结合了用户交互历史以实现个性化工具使用。为填补缺乏基准数据的空白，我们构建了PEToolBench，该基准涵盖了三个不同个性化设置下的多样用户偏好，并包括多种工具使用场景。此外，我们提出了一种框架PEToolLLaMA，以适应个性化工具学习任务，并通过监督微调和直接偏好优化进行训练。PEToolBench上的广泛实验表明，PEToolLLaMA在性能上优于现有语言模型。', 'title_zh': 'PEToolLLM：面向大型语言模型中的个性化工具学习'}
{'arxiv_id': 'arXiv:2502.18915', 'title': 'END: Early Noise Dropping for Efficient and Effective Context Denoising', 'authors': 'Hongye Jin, Pei Chen, Jingfeng Yang, Zhengyang Wang, Meng Jiang, Yifan Gao, Binxuan Huang, Xinyang Zhang, Zheng Li, Tianyi Liu, Huasheng Li, Bing Yin', 'link': 'https://arxiv.org/abs/2502.18915', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, they are often distracted by irrelevant or noisy context in input sequences that degrades output quality. This problem affects both long- and short-context scenarios, such as retrieval-augmented generation, table question-answering, and in-context learning. We reveal that LLMs can implicitly identify whether input sequences contain useful information at early layers, prior to token generation. Leveraging this insight, we introduce Early Noise Dropping (\\textsc{END}), a novel approach to mitigate this issue without requiring fine-tuning the LLMs. \\textsc{END} segments input sequences into chunks and employs a linear prober on the early layers of LLMs to differentiate between informative and noisy chunks. By discarding noisy chunks early in the process, \\textsc{END} preserves critical information, reduces distraction, and lowers computational overhead. Extensive experiments demonstrate that \\textsc{END} significantly improves both performance and efficiency across different LLMs on multiple evaluation datasets. Furthermore, by investigating LLMs' implicit understanding to the input with the prober, this work also deepens understanding of how LLMs do reasoning with contexts internally.", 'abstract_zh': '大型语言模型（LLMs）在各种自然语言处理任务中展现了卓越的性能。然而，它们常常会被输入序列中的无关或噪声信息所干扰，从而降低输出质量。这个问题影响了长和短上下文场景，包括检索增强生成、表格问答和上下文学习。我们发现，LLMs可以在生成标记之前的早期层面上隐式地识别输入序列中是否包含有用信息。利用这一洞察，我们提出了一种名为早期噪声丢弃（\\textsc{END}）的新方法，该方法无需对LLMs进行微调即可缓解这一问题。\\textsc{END}将输入序列分段，并在LLMs的早期层面使用线性检测器来区分信息性和噪声性片段。通过在过程早期丢弃噪声性片段，\\textsc{END}保留了关键信息，减少了干扰，并降低了计算开销。广泛实验表明，\\textsc{END}在多个评估数据集上显著提高了不同LLMs的性能和效率。此外，通过使用探针探索LLMs对输入的隐式理解，这项工作还加深了对LLMs如何在内部进行上下文推理的理解。', 'title_zh': 'END：早期噪声剔除以实现高效有效的上下文去噪'}
{'arxiv_id': 'arXiv:2502.18874', 'title': 'Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework', 'authors': 'Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.18874', 'abstract': 'Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.', 'abstract_zh': '大规模语言模型（LLMs）在各种场景中被越来越多地用于自动化评估。先前的研究试图对开放源代码的LLMs进行微调，以复制强私有模型（如GPT-4）的评估解释和判断。然而，这些方法主要限于在预定的通用标准下的文本分析，导致在处理未见过的指令时的适应性降低，并在评估遵守定量和结构约束方面表现出不稳定。为解决这些局限性，我们提出了一种新的评估框架ARJudge，该框架能够适配地制定评估标准，并综合文本驱动和编码驱动的分析来评估LLM的响应。ARJudge由两个组件组成：一个微调的Analyzer生成多维度的评估分析，以及一个无需微调的Refiner综合并精炼所有分析以做出最终判断。我们构建了一个综合分析语料库，该语料库整合了评估标准生成任务以及文本驱动和编码驱动分析的生成任务，用于训练Analyzer。我们的结果显示，ARJudge在效果和稳健性方面优于现有的微调评估器。此外，它还强调了多维度评估和编码驱动分析在提高评估能力方面的重要性。', 'title_zh': '学习多方面评价对齐：一个统一且 robust 的框架'}
{'arxiv_id': 'arXiv:2502.18817', 'title': 'Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models', 'authors': 'Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu', 'link': 'https://arxiv.org/abs/2502.18817', 'abstract': 'Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at this https URL.', 'abstract_zh': '提取增强生成（RAG）已被证明在减轻大规模语言模型（LLMs）的幻觉方面具有有效性。然而，现有的自动化评估指标无法公正地评估RAG模型在训练和评估阶段生成的输出。基于LLM的判断模型具有生成高质量判断的潜力，但它们对评估提示的高度敏感性会导致在判断RAG模型输出时的一致性问题。本文介绍了Judge-Consistency（ConsJudge）方法，旨在增强LLMs以生成更加准确的RAG模型评价。具体而言，ConsJudge促使LLMs基于多种判断维度的不同组合生成不同的判断，并利用判断一致性来评估这些判断，从中选择接受和拒绝的判断用于DPO训练。我们的实验表明，ConsJudge能够有效地为各种RAG模型和数据集中的RAG模型优化提供更加准确的判断。进一步的分析显示，ConsJudge生成的判断与优秀的LLM高度一致。所有代码可在以下链接处获取：https://xxxxx（请注意，这里的URL应替换为实际的代码存放地址）。', 'title_zh': '作为评判者：通过大型语言模型的评判一致性改进检索增强生成的评估'}
{'arxiv_id': 'arXiv:2502.18798', 'title': 'ANPMI: Assessing the True Comprehension Capabilities of LLMs for Multiple Choice Questions', 'authors': 'Gyeongje Cho, Yeonkyoung So, Jaejin Lee', 'link': 'https://arxiv.org/abs/2502.18798', 'abstract': "Multiple-choice benchmarks, consisting of various prompts and choices, are among the most widely used methods to assess a language model's natural language understanding capability. Given a specific prompt, we typically compute $P(Choice|Prompt)$ to evaluate how likely a language model is to generate the correct choice compared to incorrect ones. However, we observe that performance measured using this approach reflects not only the model's comprehension of the prompt but also its inherent biases for certain choices regardless of the prompt. This issue makes it challenging to accurately measure a model's natural language understanding, as models may select the answer without fully understanding the prompt. To address this limitation, we propose a novel metric called ANPMI, which normalizes Pointwise Mutual Information (PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the model's natural language understanding by ensuring that it is challenging to answer a question without properly understanding the prompt.", 'abstract_zh': '多选基准测试由各种提示和选项组成，是评估语言模型自然语言理解能力的最常用方法之一。给定一个特定的提示，我们通常计算 $P(\\text{选择}| \\text{提示})$，以评估语言模型生成正确答案而非错误答案的可能性。然而，我们观察到，使用这种方法衡量的性能不仅反映了模型对提示的理解能力，还反映了模型对某些选项固有的偏好，这与提示无关。这一问题使得准确衡量模型的自然语言理解能力变得困难，因为模型可能会在未完全理解提示的情况下选择答案。为解决这一局限性，我们提出了一种新的度量标准，称为ANPMI（Adaptive Normalized Pointwise Mutual Information），该度量标准通过 $-\\log P(\\text{选择})$ 对点互信息（PMI）进行归一化。ANPMI 通过确保在不正确理解提示的情况下很难回答问题，提供了更准确的语言模型自然语言理解评估。', 'title_zh': 'ANPMI：评估大型语言模型在多项选择题中的真实理解能力'}
{'arxiv_id': 'arXiv:2502.18791', 'title': 'Seeing the Forest for the Trees: A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs', 'authors': 'Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter', 'link': 'https://arxiv.org/abs/2502.18791', 'abstract': 'The surge of LLM studies makes synthesizing their findings challenging. Meta-analysis can uncover important trends across studies, but its use is limited by the time-consuming nature of manual data extraction. Our study presents a semi-automated approach for meta-analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs using an automatically extracted dataset, reducing the effort of paper surveying and data extraction by more than 93\\% compared to manual approaches. We validate our dataset by showing that it reproduces key findings from a recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new insights that go beyond it, showing for example that in-context examples benefit multimodal tasks but offer limited gains in mathematical tasks compared to CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through our scientific artifacts and empirical analysis, we provide novel insights into LLMs while facilitating ongoing meta-analyses of their behavior.', 'abstract_zh': '大型语言模型（LLM）研究的激增使得综合其研究成果变得颇具挑战性。元分析能够揭示研究中的重要趋势，但其应用受限于手动数据提取的耗时性质。本研究提出了一种半自动化的方法，利用LLM加速数据提取过程，自动识别相关的arXiv论文，提取实验结果及相关属性，并将它们组织成结构化的数据集。我们利用自动提取的数据集对前沿LLM进行了全面的元分析，与手动方法相比，减少了93%以上的文章查阅和数据提取工作量。我们通过验证数据集，展示了它能够重现最近手动元分析中关于链式思维（CoT）的关键发现，并且揭示了新的见解，例如内部示例在多模态任务中受益，但在数学任务中相比于CoT提供的增益有限。我们自动生成并更新的数据集能够持续追踪目标模型，随着新数据的可用性不断增加新的评估研究。通过我们的科学制品和实证分析，我们不仅提供了关于LLM的新颖见解，还促进了对其行为的持续元分析。', 'title_zh': '从树木中见森林：前沿大语言模型的大型持续更新元分析'}
{'arxiv_id': 'arXiv:2502.18772', 'title': 'Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance', 'authors': 'Xueqing Peng, Triantafillos Papadopoulos, Efstathia Soufleri, Polydoros Giannouris, Ruoyu Xiang, Yan Wang, Lingfei Qian, Jimin Huang, Qianqian Xie, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.18772', 'abstract': "Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance.", 'abstract_zh': '尽管希腊在全球经济中占据重要地位，但由于希腊语言的复杂性和领域特定数据的稀缺性，大型语言模型（LLMs）在希腊金融语境中的应用仍然相对较少被探索。之前的多语言金融自然语言处理（NLP）努力暴露了显著的性能差异，但直到现在，还没有专门为希腊金融市场开发的基准测试或特定于希腊的金融LLMs。为了填补这一空白，我们介绍了Plutus-ben，该基准测试是首个希腊金融评价基准，并引入了Plutus-8B，这是首个基于希腊特定领域的大型语言模型。通过使用希腊特定领域的数据对Plutus-8B进行微调，Plutus-ben涵盖了五个希腊核心金融NLP任务：数值和文本命名实体识别、问答、缩写总结和话题分类，从而促进系统和可重复的LLM评估。为此，我们提出了三个高质量的新颖希腊金融数据集，并由专家母语为希腊的语言学家详细注释，同时扩展了两个现有资源。我们在Plutus-ben上对22个LLM进行的全面评估表明，由于语言复杂性、领域特定术语和金融推理差距，希腊金融NLP仍具有挑战性。这些发现强调了跨语言迁移的局限性、希腊训练模型中所需金融专业知识的重要性，以及适应希腊文本的金融LLMs所面临的挑战。我们公开发布了Plutus-ben、Plutus-8B及其所有相关数据集，以促进可重复研究，推进希腊金融NLP的发展，并促进金融领域的更广泛的多语言包容性。', 'title_zh': '普图斯：低资源希腊金融大型语言模型基准测试'}
{'arxiv_id': 'arXiv:2502.18729', 'title': 'Random Forest-of-Thoughts: Uncertainty-aware Reasoning for Computational Social Science', 'authors': 'Xiaohua Wu, Xiaohui Tao, Wenjie Wu, Yuefeng Li, Lin Li', 'link': 'https://arxiv.org/abs/2502.18729', 'abstract': "Social surveys in computational social science are well-designed by elaborate domain theories that can effectively reflect the interviewee's deep thoughts without concealing their true feelings. The candidate questionnaire options highly depend on the interviewee's previous answer, which results in the complexity of social survey analysis, the time, and the expertise required. The ability of large language models (LLMs) to perform complex reasoning is well-enhanced by prompting learning such as Chain-of-thought (CoT) but still confined to left-to-right decision-making processes or limited paths during inference. This means they can fall short in problems that require exploration and uncertainty searching. In response, a novel large language model prompting method, called Random Forest of Thoughts (RFoT), is proposed for generating uncertainty reasoning to fit the area of computational social science. The RFoT allows LLMs to perform deliberate decision-making by generating diverse thought space and randomly selecting the sub-thoughts to build the forest of thoughts. It can extend the exploration and prediction of overall performance, benefiting from the extensive research space of response. The method is applied to optimize computational social science analysis on two datasets covering a spectrum of social survey analysis problems. Our experiments show that RFoT significantly enhances language models' abilities on two novel social survey analysis problems requiring non-trivial reasoning.", 'abstract_zh': '在计算社会科学研究中的社会调查设计可以通过复杂的领域理论来精细规划，以有效地反映受访者的深层次思考而不隐瞒其真实情感。候选问卷选项高度依赖于受访者的先前回答，这导致了社会调查分析的复杂性、所需时间和专业知识。大型语言模型（LLMs）通过提示学习，如链式思考（CoT），增强了进行复杂推理的能力，但仍局限于自左向右的决策过程或解释中的有限路径。这意味着它们在需要探索和不确定性搜索的问题上可能会有所不足。为应对这一挑战，提出了一种新颖的大型语言模型提示方法，称为思维随机森林（RFoT），以生成不确定性推理并适用于计算社会科学研究领域。RFoT 允许大型语言模型通过生成多样化的思维空间并随机选择子思维来构建思维森林，从而进行有目的的决策。它能够扩展整体表现的探索和预测，并从广泛的响应研究空间中获益。该方法应用于两个数据集的优化，涵盖了一系列社会调查分析问题。我们的实验表明，RFoT 显著提升了语言模型在两项需要复杂推理的新社会调查分析问题上的能力。', 'title_zh': '《思维随机森林：面向计算社会科学的不确定性推理》'}
{'arxiv_id': 'arXiv:2502.18679', 'title': 'Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data', 'authors': 'Siqi Guo, Ilgee Hong, Vicente Balmaseda, Tuo Zhao, Tianbao Yang', 'link': 'https://arxiv.org/abs/2502.18679', 'abstract': "Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted by SFT$\\rightarrow$PO has become the standard for improving pretrained large language models (LLMs), with PO demonstrating significant performance gains. However, PO methods rely on either human-labeled preference data or a strong reward model to generate preference data. Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT$\\rightarrow$PO? We address this question by introducing Discriminative Fine-Tuning (DFT), a novel approach that eliminates the need for preference data. Unlike SFT, which employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that that increases the probability of positive answers while suppressing potentially negative ones, shifting from token prediction to data prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\\rightarrow$PO. The code can be found at this https URL.", 'abstract_zh': '监督微调（SFT）之后进行偏好优化（PO），表示为SFT$\\rightarrow$PO，已成为提高预训练大型语言模型（LLMs）性能的标准方法，而PO方法能够显著提高性能。然而，PO方法依赖于人类标注的偏好数据或强大的奖励模型来生成偏好数据。我们能否在不使用偏好数据或奖励模型的情况下微调LLMs，并达到与SFT$\\rightarrow$PO相当甚至更好的性能？我们通过引入辨别性微调（DFT）来回答这个问题，这是一种新颖的方法，消除对偏好数据的依赖。与SFT采用生成方法并忽略负数据不同，DFT采用辨别性范式，通过增加正答案的概率并抑制潜在的负答案，从token预测转向数据预测。我们的贡献包括：（i）一种辨别性的概率框架，通过显式建模给定输入时所有可能输出中的回答的辨别性似然来进行微调；（ii）高效算法来优化这种辨别性似然；以及（iii）广泛实验表明DFT的有效性，其性能优于SFT，并且与SFT$\\rightarrow$PO相当甚至更优。相关代码可通过以下链接访问：this https URL。', 'title_zh': '无需奖励模型和偏好数据的生成型大规模语言模型的鉴别性微调'}
{'arxiv_id': 'arXiv:2502.18650', 'title': 'Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources', 'authors': 'Joachim De Baer, A. Seza Doğruöz, Thomas Demeester, Chris Develder', 'link': 'https://arxiv.org/abs/2502.18650', 'abstract': 'Optimizing language models for use in conversational agents requires large quantities of example dialogues. Increasingly, these dialogues are synthetically generated by using powerful large language models (LLMs), especially in domains with challenges to obtain authentic human data. One such domain is human resources (HR). In this context, we compare two LLM-based dialogue generation methods for the use case of generating HR job interviews, and assess whether one method generates higher-quality dialogues that are more challenging to distinguish from genuine human discourse. The first method uses a single prompt to generate the complete interview dialog. The second method uses two agents that converse with each other. To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons. We demonstrate that despite a sixfold increase in token cost, interviews generated with the dual-prompt method achieve a win rate up to ten times higher than those generated with the single-prompt method. This difference remains consistent regardless of whether GPT-4o or Llama 3.3 70B is used for either interview generation or judging quality.', 'abstract_zh': '优化语言模型在会话代理中的应用需要大量示例对话。越来越多地，这些对话通过使用强大的大型语言模型（LLMs）合成生成，尤其是在难以获取真实人类数据的领域中尤其如此。人力资源（HR）领域就是其中之一。在此背景下，我们对比了两种基于LLM的对话生成方法在生成HR求职面试场景下的应用效果，并评估这些方法生成的对话是否更为高质量且更难以与真实的人类对话区分。第一种方法使用一个提示生成完整的面试对话。第二种方法使用两个代理进行互动。为了评估每种方法下的对话质量，我们让一个评判LLM判断是否使用了AI进行面试生成，方法是通过两份面试对话的配对比较来进行评估。结果显示，尽管双重提示方法的令牌成本增加了六倍，但它生成的面试对话的质量胜率比单一提示方法高十倍左右。这种差异无论使用GPT-4o还是Llama 3.3 70B进行对话生成或评判质量时，都能保持一致。', 'title_zh': '基于大型语言模型（LLMs）的单提示与双提示对话生成在人力资源领域招聘面试中的应用比较'}
{'arxiv_id': 'arXiv:2502.18644', 'title': 'Steered Generation via Gradient Descent on Sparse Features', 'authors': 'Sumanta Bhattacharyya, Pedram Rooshenas', 'link': 'https://arxiv.org/abs/2502.18644', 'abstract': "Large language models (LLMs) encode a diverse range of linguistic features within their latent representations, which can be harnessed to steer their output toward specific target characteristics. In this paper, we modify the internal structure of LLMs by training sparse autoencoders to learn a sparse representation of the query embedding, allowing precise control over the model's attention distribution. We demonstrate that manipulating this sparse representation effectively transforms the output toward different stylistic and cognitive targets. Specifically, in an educational setting, we show that the cognitive complexity of LLM-generated feedback can be systematically adjusted by modifying the encoded query representation at a specific layer. To achieve this, we guide the learned sparse embedding toward the representation of samples from the desired cognitive complexity level, using gradient-based optimization in the latent space.", 'abstract_zh': '大型语言模型（LLMs）在其潜在表示中编码了多样化的语言特征，这些特征可以被利用以引导模型输出朝向特定的目标特征。在本文中，我们通过训练稀疏自编码器来修改LLMs的内部结构，学习查询嵌入的稀疏表示，从而实现对模型注意力分布的精确控制。我们证明通过操作这种稀疏表示，可以有效地将输出朝向不同的风格性和认知性目标进行变换。具体而言，在教育场景中，我们展示了通过修改特定层中编码查询表示的方式，可以系统地调整LLM生成反馈的认知复杂度。为此，我们使用基于梯度的优化在潜在空间中引导学习到的稀疏嵌入接近目标的认知复杂度级别的表示。', 'title_zh': '基于稀疏特征梯度下降的导向生成方法'}
{'arxiv_id': 'arXiv:2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'authors': 'Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He', 'link': 'https://arxiv.org/abs/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'abstract_zh': '大规模语言模型（LLMs）通过链式思考（Chain-of-Thought, CoT）提示等机制，在解决复杂推理任务方面表现出色，强调详细、逐步的推理过程。然而，人类通常会采用一种更高效的策略：草拟简洁的中间思考，仅捕捉到关键信息。在这项工作中，我们提出了链式草稿（Chain of Draft, CoD）这一新颖的范式，该范式受到人类认知过程的启发，使LLMs在解决任务时生成简洁但富有信息量的中间推理输出。通过减少冗余信息并聚焦于关键洞见，CoD在准确度上与CoT相当甚至更优，仅使用CoT所需词汇量的7.6%，显著降低了各种推理任务的成本和延迟。', 'title_zh': '论文标题可以翻译为：思想快速生成的书写链条：写得少思考快'}
{'arxiv_id': 'arXiv:2502.18581', 'title': 'Scalable Best-of-N Selection for Large Language Models via Self-Certainty', 'authors': 'Zhewei Kang, Xuandong Zhao, Dawn Song', 'link': 'https://arxiv.org/abs/2502.18581', 'abstract': 'Best-of-N selection is a key technique for improving the reasoning performance of Large Language Models (LLMs) through increased test-time computation. Current state-of-the-art methods often employ computationally intensive reward models for response evaluation and selection. Reward-free alternatives, like self-consistency and universal self-consistency, are limited in their ability to handle open-ended generation tasks or scale effectively. To address these limitations, we propose self-certainty, a novel and efficient metric that leverages the inherent probability distribution of LLM outputs to estimate response quality without requiring external reward models. We hypothesize that higher distributional self-certainty, aggregated across multiple samples, correlates with improved response accuracy, as it reflects greater confidence in the generated output. Through extensive experiments on various reasoning tasks, we demonstrate that self-certainty (1) scales effectively with increasing sample size $N$, akin to reward models but without the computational overhead; (2) complements chain-of-thought, improving reasoning performance beyond greedy decoding; and (3) generalizes to open-ended tasks where traditional self-consistency methods fall short. Our findings establish self-certainty as a practical and efficient way for improving LLM reasoning capabilities. The code is available at this https URL', 'abstract_zh': '最佳-of-N选择是一种关键的技术，通过增加测试时的计算量，显著提升大型语言模型（LLMs）的推理性能。当前最先进的方法通常采用计算密集型的奖励模型来评估和选择响应。自无奖励选择方法，如自一致性方法和通用自一致性方法，在处理开放生成任务或有效扩展方面能力有限。为了克服这些限制，我们提出了一种新颖且高效的度量标准——自我确定性，该标准利用LLM输出的固有概率分布来估计响应质量，无需外部奖励模型。我们假设多个样本的分布自我确定性越高，生成的输出越准确。这反映了对生成输出更大的信心。通过在多种推理任务上的广泛实验，我们证明了自我确定性（1）能够有效地随样本数量N的增加而扩展，类似于奖励模型，但不需要额外的计算开销；（2）能够补充思维链方法，超越贪婪解码，提升推理性能；（3）适用于传统自一致性方法难以胜任的开放生成任务。我们的研究结果确立了自我确定性作为一种实用且有效的手段，可以提升LLM的推理能力。相关代码可在以下链接中获取：this https URL', 'title_zh': '通过自我 certainty 方式实现大规模语言模型的可扩展最佳选项选择'}
{'arxiv_id': 'arXiv:2502.18573', 'title': 'FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models', 'authors': 'Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale', 'link': 'https://arxiv.org/abs/2502.18573', 'abstract': 'Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.', 'abstract_zh': '近年来，大型语言模型（LLMs）在生成任务上展现了巨大的能力，但在保证生成内容的准确性方面却存在困难。这使得这些模型在期望获得事实准确回答的真实场景中不够可靠。本文提出了一种新的事实性评估器——FactReasoner，它依赖于概率推理来评估长文本生成响应的事实性。具体而言，FactReasoner 将响应分解为原子单位，从外部知识源检索与这些原子单位相关的内容，并通过逻辑关系（蕴含、矛盾）的概率编码构造原子和上下文的联合概率分布。FactReasoner 然后计算检索到的上下文是否支持响应中原子单位的概率。我们在有标签和无标签基准数据集上的实验清楚地表明，与基于提示的方法相比，FactReasoner 在事实精密性和召回率方面有了显著改进。', 'title_zh': 'FactReasoner：一种用于大型语言模型长篇事实性评估的概率方法'}
{'arxiv_id': 'arXiv:2502.18482', 'title': 'MixLLM: Dynamic Routing in Mixed Large Language Models', 'authors': 'Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.18482', 'abstract': "Large Language Models (LLMs) exhibit potential artificial generic intelligence recently, however, their usage is costly with high response latency. Given mixed LLMs with their own strengths and weaknesses, LLM routing aims to identify the most suitable model for each query in the stream to maximize response quality and minimize cost and latency. However, the challenges involve: (1) dynamic trade-offs among quality, cost, and latency; (2) enabling continual learning in deployed systems; and (3) navigating a varying (e.g., new LLM addition or old LLM removal) set of LLM candidates over time. To bridge these gaps, we develop MixLLM, a dynamic contextual-bandit-based routing system for query-LLM assignment. Specifically, we first leverage query tags to enhance query embeddings for the routing task. Next, we design lightweight prediction models to estimate the response qualities and costs of queries over LLMs. We then devise a meta-decision maker to choose the query-LLM assignments to best tradeoff response quality, cost, and latency. Finally, the system benefits from continual training, allowing it to adapt to evolving queries and user feedback over time. Our extensive experiments show that MixLLM achieves the best trade-offs in response quality, cost, and latency (97.25% of GPT-4's quality at 24.18% of the cost under the time constraint).", 'abstract_zh': '近年来，大型语言模型（LLMs）展现出了潜在的人工通用智能能力，但其使用成本较高且响应延迟较长。鉴于混合LLMs各自具有优势和劣势，LLM路由旨在识别流查询中最合适的模型，以最大化响应质量并最小化成本和延迟。然而，面临的挑战包括：（1）在质量、成本和延迟之间动态权衡；（2）在部署系统中实现持续学习；以及（3）随着时间变化导航LLM候选集合的变化（例如，新LLM的添加或旧LLM的移除）。为了弥补这些差距，我们开发了MixLLM，这是一种基于动态上下文臂拍策略的查询-LLM路由系统。具体来说，我们首先利用查询标签增强查询嵌入以优化路由任务。接下来，我们设计了轻量级预测模型来估计查询在不同LLM上的响应质量和成本。然后，我们设计了一个元决策者来选择最佳权衡响应质量、成本和延迟的查询-LLM组合。最后，该系统得益于持续训练，使其能够适应不断变化的查询和用户反馈。我们的广泛实验表明，MixLLM在响应质量和延迟方面实现了最佳权衡（在时间约束条件下，成本仅为GPT-4的24.18%，而响应质量达到97.25%）。', 'title_zh': 'MixLLM：混合大型语言模型中的动态路由'}
{'arxiv_id': 'arXiv:2502.19413', 'title': 'Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs', 'authors': 'Christoph Schuhmann, Gollam Rabby, Ameya Prabhu, Tawsif Ahmed, Andreas Hochlehnert, Huu Nguyen, Nick Akinci Heidrich, Ludwig Schmidt, Robert Kaczmarczyk, Sören Auer, Jenia Jitsev, Matthias Bethge', 'link': 'https://arxiv.org/abs/2502.19413', 'abstract': 'Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.', 'abstract_zh': '学术规范的中文翻译如下：\n\n付费墙、许可协议和版权规则常常限制科学知识的广泛传播和重用。我们认为，从学术文本中提取科学知识在法律和技术上都是可行的。当前的方法，比如文本嵌入，无法可靠地保留事实内容，而简单的改写可能在法律上也不够严谨。我们敦促学术界采纳一个新概念：使用大型语言模型（LLMs）将学术文档转换为知识单元（Knowledge Units）。这些单元使用结构化数据来捕获实体、属性和关系，而不包含风格化内容。我们提供了证据，表明知识单元：（1）基于对德国版权法和美国合理使用理论的法律分析，形成了一个合法合理的知识分享框架；（2）在四个研究领域中，基于原始受版权保护文本中的事实设计的多项选择题（MCQ）测试，保留了约95%的原始事实知识。从版权中解放科学知识有望为科学研究和教育带来变革性的益处，使语言模型能够重用受版权保护文本中的重要事实。为此，我们分享了用于将研究文档转换为知识单元的开源工具。总体而言，我们的工作提出现实可行的方法，以在尊重版权的同时使科学知识的获取更加民主化。', 'title_zh': '亚历山大项目：通过大型语言模型释放科学知识免受版权束缚之路'}
{'arxiv_id': 'arXiv:2502.19312', 'title': 'FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users', 'authors': 'Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn', 'link': 'https://arxiv.org/abs/2502.19312', 'abstract': 'Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.', 'abstract_zh': '有效的个性化大语言模型（LLM）对于广泛的应用场景至关重要，如虚拟助手和内容推荐。受LLM强大上下文学习能力的启发，我们提出了一种名为少量示例偏好优化（FSPO）的方法，将其奖励建模重新定义为一个元学习问题。在这一框架下，LLM能够通过少量用户标记的偏好信息迅速适应用户，构建个性化的奖励函数。此外，由于实际的偏好数据稀缺且难以大规模收集，我们提出了精心的设计选择来构建合成偏好数据集，使用公开的LLM生成了超过100万条合成个性化偏好数据。特别是，为了成功地将合成数据转移到真实用户，我们发现数据应表现出高度多样化和一致、自洽的结构至关重要。我们在三个领域（电影评论、基于教育背景的教学适应以及通用问题回答）的1500个合成用户的个性化开放生成任务上评估了FSPO，并进行了一项受控的人类研究。总体而言，FSPO在生成针对合成用户的个性化响应方面，平均取得了87%的Alpaca Eval胜率，在开放问题回答中使用真实人类用户的胜率为72%。', 'title_zh': 'FSPO：在大型语言模型中利用少量样本优化合成偏好数据以实现对真实用户的有效个性化'}
{'arxiv_id': 'arXiv:2502.18943', 'title': 'Towards Label-Only Membership Inference Attack against Pre-trained Large Language Models', 'authors': 'Yu He, Boheng Li, Liu Liu, Zhongjie Ba, Wei Dong, Yiming Li, Zhan Qin, Kui Ren, Chun Chen', 'link': 'https://arxiv.org/abs/2502.18943', 'abstract': "Membership Inference Attacks (MIAs) aim to predict whether a data sample belongs to the model's training set or not. Although prior research has extensively explored MIAs in Large Language Models (LLMs), they typically require accessing to complete output logits (\\ie, \\textit{logits-based attacks}), which are usually not available in practice. In this paper, we study the vulnerability of pre-trained LLMs to MIAs in the \\textit{label-only setting}, where the adversary can only access generated tokens (text). We first reveal that existing label-only MIAs have minor effects in attacking pre-trained LLMs, although they are highly effective in inferring fine-tuning datasets used for personalized LLMs. We find that their failure stems from two main reasons, including better generalization and overly coarse perturbation. Specifically, due to the extensive pre-training corpora and exposing each sample only a few times, LLMs exhibit minimal robustness differences between members and non-members. This makes token-level perturbations too coarse to capture such differences.\nTo alleviate these problems, we propose \\textbf{PETAL}: a label-only membership inference attack based on \\textbf{PE}r-\\textbf{T}oken sem\\textbf{A}ntic simi\\textbf{L}arity. Specifically, PETAL leverages token-level semantic similarity to approximate output probabilities and subsequently calculate the perplexity. It finally exposes membership based on the common assumption that members are `better' memorized and have smaller perplexity. We conduct extensive experiments on the WikiMIA benchmark and the more challenging MIMIR benchmark. Empirically, our PETAL performs better than the extensions of existing label-only attacks against personalized LLMs and even on par with other advanced logit-based attacks across all metrics on five prevalent open-source LLMs.", 'abstract_zh': '成员归属推断攻击（Membership Inference Attacks, MIAs）旨在预测某个数据样本是否属于模型的训练集。虽然先前的研究已经广泛探讨了在大型语言模型（LLMs）中的MIAs，但这些攻击通常需要访问完整输出logits（即logits基攻击），而在实践中这些数据通常不可用。在本文中，我们研究了预训练LLMs在仅标签（label-only）设置下的易受攻击性，即攻击者只能访问生成的标记（文本）。我们首先揭示，尽管现有仅标签的MIAs在推断个性化LLMs所使用的微调数据集方面非常有效，但在攻击预训练LLMs时效果微乎其微。我们发现其失败的根本原因有两个方面，包括更好的泛化能力和过于粗糙的扰动。具体而言，由于广泛预训练的语料库以及每个样本仅暴露几次，LLMs在成员与非成员之间体现出的鲁棒性差异极小。这使得标记级别的扰动过于粗糙，难以捕捉这些差异。\n\n为解决这些问题，我们提出了一种基于标记级别的成员归属推断攻击方法：一种基于PEr-TEKnical semAntiC simiLarity（PETAL）的标签唯一性推断攻击。PETAL 指标利用标记级别的语义相似性来近似输出概率，进而计算困惑度。基于一个普遍假设，即成员能够被更好地记忆并且具有较小的困惑度，该方法最终通过对照组来揭示成员身份。我们在WikiMIA基准和更具挑战性的MIMIR基准上进行了广泛的实验。实证结果显示，我们的PETAL在五种常见的开源LLMs上各评价指标上比现有仅标签的攻击方法更有优势，甚至在某些方面与其它先进的logits基攻击方法持平。', 'title_zh': '面向预训练大型语言模型的仅标签成员推理攻击'}
{'arxiv_id': 'arXiv:2502.18873', 'title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'authors': 'Sen Yang, Yafu Li, Wai Lam, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.18873', 'abstract': "Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.", 'abstract_zh': '大型语言模型（LLMs）在复杂推理任务中往往表现不佳，因为它们在处理广泛推理空间和自然语言固有的歧义性方面存在局限性。我们提出了搜索代理混合体（Mixture-of-Search-Agents, MoSA）范式，这是一种利用多个LLM集体专业知识来增强基于搜索的推理的新方法。MoSA 通过结合独立探索和LLMs之间的迭代完善，整合了多样的推理路径，从而减轻了单模型方法的局限性。MoSA 以蒙特卡洛树搜索（MCTS）为基础，使多个代理能够提出并聚合推理步骤，从而提高准确性。我们的全面评估表明，MoSA 在四个推理基准上的表现优于单代理和其它多代理基线，特别是在复杂的数学和常识推理任务中表现出显著改进。', 'title_zh': '多大型语言模型协作搜索在复杂问题解决中的应用'}
{'arxiv_id': 'arXiv:2502.18778', 'title': 'M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with Competitive Performance', 'authors': 'Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-WeiChang, Jingdong Chen, Ming Yang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.18778', 'abstract': "We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omni's language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain.", 'abstract_zh': '我们介绍了一种名为M2-omni的前沿开源全模态大语言模型，其性能媲美GPT-4o。M2-omni采用了一种统一的多模态序列建模框架，赋予大语言模型（LLMs）全面跨模态的理解和生成能力。具体而言，M2-omni能够处理任意混合的音频、视频、图像和文本模态输入，生成交织有音频、图像或文本输出的多模态序列，从而提供高级且互动的实时体验。训练这种全模态大语言模型面临着各模态间数据量差异显著以及收敛率不同的挑战。为应对这些挑战，我们在预训练阶段提出了步长平衡策略，以处理特定模态数据量的差异。此外，在指令调优阶段引入了动态自适应平衡策略，以同步各模态的训练进度，确保最优的收敛性。值得一提的是，在整个训练过程中，我们优先保持对纯文本任务的强大性能，以确保M2-omni的语言理解能力的稳健性。据我们所知，M2-omni目前是非常有竞争力的开源模型，具备全面支持模态和任务的特点，以及卓越的性能。我们期望M2-omni将进一步推动全模态大语言模型的发展，从而促进该领域的未来研究。', 'title_zh': 'M2-omni：面向全面模态支持的竞争力性能提升的全栈多模态大语言模型'}
{'arxiv_id': 'arXiv:2502.18744', 'title': 'Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models', 'authors': 'Jeesu Jung, Chanjun Park, Sangkeun Jung', 'link': 'https://arxiv.org/abs/2502.18744', 'abstract': 'Recent advancements in Large Language Model (LLM) alignment have sought to mitigate the cost of human annotations by leveraging pretrained models to generate preference data. However, existing methods often compare responses from models with substantially different capabilities, yielding superficial distinctions that fail to provide meaningful guidance on what constitutes a superior response. To address this limitation, we propose Kinship-Aware pReference MApping (KARMA), a novel framework that systematically pairs responses from models with comparable competencies. By constraining preference comparisons to outputs of similar complexity and quality, KARMA enhances the informativeness of preference data and improves the granularity of alignment signals. Empirical evaluations demonstrate that our kinship-aware approach leads to more consistent and interpretable alignment outcomes, ultimately facilitating a more principled and reliable pathway for aligning LLM behavior with human preferences.', 'abstract_zh': '近年来，大型语言模型（LLM）对齐领域的最新进展旨在通过利用预训练模型生成偏好数据来减轻人工标注的成本。然而，现有的方法往往比较能力差异很大的模型的响应，这导致了表面性的区别，未能提供有意义的指导，明确什么是更好的响应。为了解决这一局限性，我们提出了亲和感知偏好映射（Kinship-Aware pReference MApping，KARMA）这一新颖框架，该框架系统地将具有相似能力的模型响应进行配对。通过将偏好比较限制在具有类似复杂度和质量的输出上，KARMA 提高了偏好数据的信息量，并增强了对齐信号的粒度。实证评估表明，我们的亲和感知方法导致了更为一致和可解释的对齐结果，最终促进了更为原则性和可靠的途径，以使LLM的行为与人类偏好对齐。', 'title_zh': '像父亲，像儿子：基于亲缘关系的偏好映射(KARMA)在大型语言模型中的自动对齐'}
{'arxiv_id': 'arXiv:2502.18725', 'title': 'Talking to the brain: Using Large Language Models as Proxies to Model Brain Semantic Representation', 'authors': 'Xin Liu, Ziyue Zhang, Jingxin Nie', 'link': 'https://arxiv.org/abs/2502.18725', 'abstract': 'Traditional psychological experiments utilizing naturalistic stimuli face challenges in manual annotation and ecological validity. To address this, we introduce a novel paradigm leveraging multimodal large language models (LLMs) as proxies to extract rich semantic information from naturalistic images through a Visual Question Answering (VQA) strategy for analyzing human visual semantic representation. LLM-derived representations successfully predict established neural activity patterns measured by fMRI (e.g., faces, buildings), validating its feasibility and revealing hierarchical semantic organization across cortical regions. A brain semantic network constructed from LLM-derived representations identifies meaningful clusters reflecting functional and contextual associations. This innovative methodology offers a powerful solution for investigating brain semantic organization with naturalistic stimuli, overcoming limitations of traditional annotation methods and paving the way for more ecologically valid explorations of human cognition.', 'abstract_zh': '传统的利用自然场景刺激的心理学实验在手动标注和生态效度方面面临挑战。为了解决这些问题，我们引入了一种新的范式，利用多模态大规模语言模型（LLMs）作为代理，通过视觉问答（VQA）策略从自然图像中提取丰富的语义信息，以分析人类视觉语义表征。由LLM衍生的表征能够成功预测通过fMRI测量的已确立的神经活动模式（例如，面孔、建筑物），这验证了其可行性，并揭示了跨皮层区域的层次语义组织。从LLM衍生表征构建的大脑语义网络识别出反映功能和上下文关联的有意义的聚类。这种创新的方法论为利用自然场景刺激研究大脑语义组织提供了有力的解决方案，克服了传统标注方法的局限性，并为更生态有效的探索人类认知开辟了道路。', 'title_zh': '与大脑对话：使用大规模语言模型作为代理模型构建大脑语义表示'}
{'arxiv_id': 'arXiv:2502.18531', 'title': 'Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline', 'authors': 'Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, Lei Wang', 'link': 'https://arxiv.org/abs/2502.18531', 'abstract': "Background: Recruitment for cohorts involving complex liver diseases, such as hepatocellular carcinoma and liver cirrhosis, often requires interpreting semantically complex criteria. Traditional manual screening methods are time-consuming and prone to errors. While AI-powered pre-screening offers potential solutions, challenges remain regarding accuracy, efficiency, and data privacy. Methods: We developed a novel patient pre-screening pipeline that leverages clinical expertise to guide the precise, safe, and efficient application of large language models. The pipeline breaks down complex criteria into a series of composite questions and then employs two strategies to perform semantic question-answering through electronic health records - (1) Pathway A, Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset Stances within an Agent Collaboration strategy, particularly in managing complex clinical reasoning scenarios. The pipeline is evaluated on three key metrics-precision, time consumption, and counterfactual inference - at both the question and criterion levels. Results: Our pipeline achieved high precision (0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled in complex reasoning, while Pathway A was effective in precise data extraction with faster processing times. Both pathways achieved comparable precision. The pipeline showed promising results in hepatocellular carcinoma (0.878) and cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient pipeline shows high precision in hepatopathy trials, providing promising solutions for streamlining clinical trial workflows. Its efficiency and adaptability make it suitable for improving patient recruitment. And its capability to function in resource-constrained environments further enhances its utility in clinical settings.", 'abstract_zh': '背景：涉及复杂肝脏疾病的队列研究，如肝细胞癌和肝硬化，常常需要解读复杂的筛选标准。传统的手工筛查方法耗时且容易出错。虽然基于人工智能的预筛查提供了潜在解决方案，但在准确性和效率以及数据隐私方面仍然存在挑战。方法：我们开发了一种新的患者预筛查管道，该管道利用临床知识指导大型语言模型的精确、安全和高效应用。该管道将复杂的筛选标准分解为一系列复合问题，然后通过电子健康记录执行语义问题回答，具体策略包括：（1）途径A：拟人化专家的思维链策略；（2）途径B：代理合作中的预设立场策略，特别适用于处理复杂的临床推理情景。该管道从问题和标准层面分别以精度、耗时和反事实推理为评价指标进行了评估。结果：我们的管道在标准层面达到了高精度（0.921）和高效率（每任务0.44秒）。途径B在复杂推理方面表现出色，而途径A在精确数据提取方面更有效，且处理时间更短。两种途径在精度上表现相当。该管道在肝细胞癌（0.878）和肝硬化临床试验（0.843）中显示出有前景的结果。结论：这种数据安全且高效的工作流管道在肝病临床试验中显示出了高精度，为简化临床试验流程提供了有前景的解决方案。其高效性和适应性使其适用于提高患者招募。此外，其在资源受限环境中运行的能力进一步增强了其在临床环境中的实用性。', 'title_zh': '增强肝脏疾病临床试验效率：一种安全的大规模语言模型驱动的预筛查流程'}
{'arxiv_id': 'arXiv:2502.18480', 'title': 'QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration', 'authors': 'Shaola Ren, Li Ke, Longtao Huang, Dehong Gao, Hui Xue', 'link': 'https://arxiv.org/abs/2502.18480', 'abstract': 'Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items.', 'abstract_zh': '在信息检索中，自动提取有效的查询具有挑战性，特别是在有毒内容探索方面，因为这类内容往往会被伪装。得益于近期生成型大语言模型（LLM）的进展，我们能够利用LLM的能力直接提取用于类似内容探索的有效查询。本研究提出了一种基于大语言模型的查询提取方法——QExplorer，用于有毒内容探索。QExplorer方法包含一个两阶段的训练过程：指令监督微调（SFT）和基于直接偏好优化（DPO）的偏好对齐，同时也包括使用搜索引擎反馈构建数据集的过程。为了验证QExplorer的有效性，在我们实际系统上进行了离线和在线实验。离线实验结果表明，我们的自动查询提取性能优于几种LLM和人类的表现。在线部署结果显示，有毒物品的检测能力显著提高。', 'title_zh': 'QExplorer：基于大型语言模型的有毒内容查询提取'}
{'arxiv_id': 'arXiv:2502.18471', 'title': 'FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data', 'authors': 'Ankur Sinha, Chaitanya Agarwal, Pekka Malo', 'link': 'https://arxiv.org/abs/2502.18471', 'abstract': 'Large language models (LLMs) excel at generating human-like responses but often struggle with interactive tasks that require access to real-time information. This limitation poses challenges in finance, where models must access up-to-date information, such as recent news or price movements, to support decision-making. To address this, we introduce Financial Agent, a knowledge-grounding approach for LLMs to handle financial queries using real-time text and tabular data. Our contributions are threefold: First, we develop a Financial Context Dataset of over 50,000 financial queries paired with the required context. Second, we train FinBloom 7B, a custom 7 billion parameter LLM, on 14 million financial news articles from Reuters and Deutsche Presse-Agentur, alongside 12 million Securities and Exchange Commission (SEC) filings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to serve as a Financial Agent. This agent generates relevant financial context, enabling efficient real-time data retrieval to answer user queries. By reducing latency and eliminating the need for users to manually provide accurate data, our approach significantly enhances the capability of LLMs to handle dynamic financial tasks. Our proposed approach makes real-time financial decisions, algorithmic trading and other related tasks streamlined, and is valuable in contexts with high-velocity data flows.', 'abstract_zh': '大型语言模型（LLMs）在生成类人回复方面表现出色，但在处理需要实时信息访问的交互任务时往往显得力不从心。这种局限性在金融领域尤为突出，因为在金融领域，模型必须访问最新的信息（如最近的新闻或价格变动）以支持决策。为了解决这一问题，我们引入了“金融代理”这一知识落地方法，该方法允许LLMs使用实时文本和表格数据来处理金融查询。我们的贡献主要包括三个方面：首先，我们开发了一个包含超过50,000个金融查询及其所需上下文的数据集。其次，我们在来自路透社和德新社的1400万篇金融新闻文章以及1200万篇美国证券交易委员会（SEC）文件上训练了一个70亿参数的定制模型——FinBloom 7B。最后，我们使用财务上下文数据集对FinBloom 7B进行微调，使之成为金融代理。该代理能够生成相关的财务上下文信息，从而高效地实现实时数据检索，以回答用户查询。通过减少延迟并消除用户手动提供准确数据的需求，我们的方法显著增强了LLMs处理动态金融任务的能力。我们提出的方法使得实时金融决策、算法交易及其他相关任务得以简化，尤其适用于数据流快速的应用场景。', 'title_zh': 'FinBloom：基于实时金融数据的大规模语言模型知识 grounding'}
{'arxiv_id': 'arXiv:2502.18992', 'title': 'OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models', 'authors': 'Hui Feng, Yuntzu Yin, Emiliano Reynares, Jay Nanavati', 'link': 'https://arxiv.org/abs/2502.18992', 'abstract': "Biomedical ontologies, which comprehensively define concepts and relations for biomedical entities, are crucial for structuring and formalizing domain-specific information representations. Biomedical code mapping identifies similarity or equivalence between concepts from different ontologies. Obtaining high-quality mapping usually relies on automatic generation of unrefined mapping with ontology domain fine-tuned language models (LMs), followed by manual selections or corrections by coding experts who have extensive domain expertise and familiarity with ontology schemas. The LMs usually provide unrefined code mapping suggestions as a list of candidates without reasoning or supporting evidence, hence coding experts still need to verify each suggested candidate against ontology sources to pick the best matches. This is also a recurring task as ontology sources are updated regularly to incorporate new research findings. Consequently, the need of regular LM retraining and manual refinement make code mapping time-consuming and labour intensive. In this work, we created OntologyRAG, an ontology-enhanced retrieval-augmented generation (RAG) method that leverages the inductive biases from ontological knowledge graphs for in-context-learning (ICL) in large language models (LLMs). Our solution grounds LLMs to knowledge graphs with unrefined mappings between ontologies and processes questions by generating an interpretable set of results that include prediction rational with mapping proximity assessment. Our solution doesn't require re-training LMs, as all ontology updates could be reflected by updating the knowledge graphs with a standard process. Evaluation results on a self-curated gold dataset show promises of using our method to enable coding experts to achieve better and faster code mapping. The code is available at this https URL.", 'abstract_zh': '生物医学本体是以全面定义生物医学实体的概念及其关系而著称，对于结构化和形式化专业领域信息表示至关重要。生物医学代码映射识别不同本体概念之间的相似性或等价性。获得高质量的映射通常依赖于使用细调过的本体领域语言模型（LMs）自动生成未经精炼的映射，然后由具有丰富领域专业知识和本体 schema 熟悉度的编码专家进行手工选择或修正。这些语言模型通常只是提供未经精炼的代码映射建议列表，而不进行推理或提供支持证据，因此编码专家仍需验证每个建议的候选人以挑选最佳匹配。由于本体源定期更新以纳入新的研究发现，这一任务是重复执行的。因此，定期重新训练 LM 和手动精炼使得代码映射耗时且劳动密集。在本项工作中，我们创建了 OntologyRAG，这是一种利用本体知识图谱的归纳偏倚增强了检索增强生成（RAG）方法，在大型语言模型（LLMs）的上下文学习（ICL）中发挥作用。我们的解决方案将 LLMs 接地于知识图谱，并通过生成包括预测理由和映射相似度评估的可解释结果集来处理问题。我们的解决方案无需重新训练 LM，因为所有本体更新都可以通过标准化过程更新知识图谱来反映。在自制作的黄金数据集上的评估结果表明，使用我们的方法可以帮助编码专家更快、更好地进行代码映射。代码可在以下 URL 获取：[提供的URL]。', 'title_zh': 'OntologyRAG：利用本体知识图谱和大规模语言模型增强的检索增强生成（RAG）方法在生物医学代码映射中的表现与效率提升'}
{'arxiv_id': 'arXiv:2502.18757', 'title': 'Training Large Recommendation Models via Graph-Language Token Alignment', 'authors': 'Mingdai Yang, Zhiwei Liu, Liangwei Yang, Xiaolong Liu, Chen Wang, Hao Peng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.18757', 'abstract': 'Recommender systems (RS) have become essential tools for helping users efficiently navigate the overwhelming amount of information on e-commerce and social platforms. However, traditional RS relying on Collaborative Filtering (CF) struggles to integrate the rich semantic information from textual data. Meanwhile, large language models (LLMs) have shown promising results in natural language processing, but directly using LLMs for recommendation introduces challenges, such as ambiguity in generating item predictions and inefficiencies in scalability. In this paper, we propose a novel framework to train Large Recommendation models via Graph-Language Token Alignment. By aligning item and user nodes from the interaction graph with pretrained LLM tokens, GLTA effectively leverages the reasoning abilities of LLMs. Furthermore, we introduce Graph-Language Logits Matching (GLLM) to optimize token alignment for end-to-end item prediction, eliminating ambiguity in the free-form text as recommendation results. Extensive experiments on three benchmark datasets demonstrate the effectiveness of GLTA, with ablation studies validating each component.', 'abstract_zh': '推荐系统（RS）已成为帮助用户有效导航电子商务和社交平台上的海量信息的重要工具。然而，传统依赖于协同过滤（CF）的RS难以整合文本数据中的丰富语义信息。与此同时，大型语言模型（LLMs）在自然语言处理方面表现出色，但直接使用LLMs进行推荐也面临挑战，如项目预测中的歧义性和可扩展性的效率问题。在本文中，我们提出了一种新的框架，通过图-语言标记对齐来训练大型推荐模型。通过将交互图中的项目和用户节点与预训练的LLM标记对齐，GLTA有效地利用了LLMs的推理能力。此外，我们引入了图-语言对数匹配（GLLM）来优化标记对齐，以实现端到端的项目预测，并消除推荐结果中自由形式文本带来的歧义。在三个基准数据集上的广泛实验表明，GLTA的有效性得到了验证，消融研究也验证了每个组件的效果。', 'title_zh': '通过图-语言令牌对齐训练大型推荐模型'}
{'arxiv_id': 'arXiv:2502.18754', 'title': 'AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms', 'authors': 'Yuwei Yan, Yu Shang, Qingbin Zeng, Yu Li, Keyu Zhao, Zhiheng Zheng, Xuefei Ning, Tianji Wu, Shengen Yan, Yu Wang, Fengli Xu, Yong Li', 'link': 'https://arxiv.org/abs/2502.18754', 'abstract': 'The AgentSociety Challenge is the first competition in the Web Conference that aims to explore the potential of Large Language Model (LLM) agents in modeling user behavior and enhancing recommender systems on web platforms. The Challenge consists of two tracks: the User Modeling Track and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp, Amazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM agents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions in total over the course of 37 official competition days. The participants have achieved 21.9% and 20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1% and 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the detailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM agent designs. To support further research and development, we have open-sourced the benchmark environment at this https URL.', 'abstract_zh': '《AgentSociety挑战赛》是Web Conference上首次旨在探索大型语言模型（LLM）代理在模拟用户行为和提升网页平台推荐系统方面潜力的竞赛。该挑战包含两个赛道：用户建模赛道和推荐赛道。参赛者被要求利用从Yelp、Amazon和Goodreads获取的综合数据集以及互动环境模拟器来开发创新性的LLM代理。该挑战吸引了来自全球的295支队伍，在37个正式竞赛日中收到了超过1,400份提交。开发阶段，赛道1和赛道2的参与队伍分别取得了21.9%和20.3%的性能提升，在最终阶段，这两个赛道的参赛队伍分别实现了9.1%和15.9%的性能提升，代表了显著的成就。本文详细讨论了挑战的设计、分析了结果，并突出了最成功的LLM代理设计。为了支持进一步的研究和开发，我们已在以下网址开源了基准环境：[请插入网址]。', 'title_zh': 'AgentSociety 挑战：设计面向网络平台用户建模和推荐的大型语言模型代理'}
{'arxiv_id': 'arXiv:2502.18479', 'title': 'Disrupt Your Research Using Generative AI Powered ScienceSage', 'authors': 'Yong Zhang, Eric Herrison Gyamfi, Kelly Anderson, Sasha Roberts, Matt Barker', 'link': 'https://arxiv.org/abs/2502.18479', 'abstract': "Large Language Models (LLM) are disrupting science and research in different subjects and industries. Here we report a minimum-viable-product (MVP) web application called $\\textbf{ScienceSage}$. It leverages generative artificial intelligence (GenAI) to help researchers disrupt the speed, magnitude and scope of product innovation. $\\textbf{ScienceSage}$ enables researchers to build, store, update and query a knowledge base (KB). A KB codifies user's knowledge/information of a given domain in both vector index and knowledge graph (KG) index for efficient information retrieval and query. The knowledge/information can be extracted from user's textual documents, images, videos, audios and/or the research reports generated based on a research question and the latest relevant information on internet. The same set of KBs interconnect three functions on $\\textbf{ScienceSage}$: 'Generate Research Report', 'Chat With Your Documents' and 'Chat With Anything'. We share our learning to encourage discussion and improvement of GenAI's role in scientific research.", 'abstract_zh': '大型语言模型（LLM）正在以不同学科和行业的方式颠覆科学研究。在此，我们报告一个最小可行产品（MVP）的网络应用程序——**ScienceSage**。该应用程序利用生成型人工智能（GenAI）来帮助研究人员提高产品创新的速度、规模和范围。**ScienceSage** 使研究人员能够构建、存储、更新和查询知识库（KB）。知识库通过向量索引和知识图谱（KG）索引来编码用户的领域知识/信息，以便高效地检索和查询。知识/信息可以从用户的文本文件、图像、视频、音频或基于研究问题生成的研究报告中提取，也可以从互联网上最新的相关信息中提取。同一套知识库连接**ScienceSage** 上的三个功能：“生成研究报告”、“与您的文档对话”和“与任何事物对话”。我们分享了我们的学习经验，以促进讨论和改进GenAI在科学研究中的作用。', 'title_zh': '使用生成式AI驱动的ScienceSage颠覆您的研究'}
{'arxiv_id': 'arXiv:2502.18470', 'title': 'Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions', 'authors': 'Dazhou Yu, Riyang Bao, Gengchen Mai, Liang Zhao', 'link': 'https://arxiv.org/abs/2502.18470', 'abstract': 'Spatial reasoning remains a challenge for Large Language Models (LLMs), which struggle with spatial data retrieval and reasoning. We propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to spatial tasks by integrating sparse spatial retrieval (spatial databases) and dense semantic retrieval (LLM-based similarity). A multi-objective ranking strategy balances spatial constraints and semantic relevance, while an LLM-guided generator ensures coherent responses. Experiments on a real-world tourism dataset show that Spatial-RAG significantly improves spatial question answering, bridging the gap between LLMs and spatial intelligence.', 'abstract_zh': '空间推理仍然是大型语言模型（LLMs）的一个挑战，它们在处理空间数据检索和推理方面存在困难。我们提出了空间检索增强生成（Spatial-RAG）框架，该框架通过结合稀疏空间检索（空间数据库）和密集语义检索（基于LLM的相似性），将RAG扩展到空间任务中。多目标排名策略平衡了空间约束和语义相关性，而LLM引导的生成器确保了连贯的响应。在现实世界的旅游数据集上的实验表明，Spatial-RAG 显著提高了空间问题的回答能力，弥补了LLMs在空间智能方面的差距。', 'title_zh': '空间RAG：扩充生成模型在实际空间推理问题中的空间检索方法'}
{'arxiv_id': 'arXiv:2502.18469', 'title': 'Using LLM-Based Approaches to Enhance and Automate Topic Labeling', 'authors': 'Trishia Khandelwal', 'link': 'https://arxiv.org/abs/2502.18469', 'abstract': 'Topic modeling has become a crucial method for analyzing text data, particularly for extracting meaningful insights from large collections of documents. However, the output of these models typically consists of lists of keywords that require manual interpretation for precise labeling. This study explores the use of Large Language Models (LLMs) to automate and enhance topic labeling by generating more meaningful and contextually appropriate labels. After applying BERTopic for topic modeling, we explore different approaches to select keywords and document summaries within each topic, which are then fed into an LLM to generate labels. Each approach prioritizes different aspects, such as dominant themes or diversity, to assess their impact on label quality. Additionally, recognizing the lack of quantitative methods for evaluating topic labels, we propose a novel metric that measures how semantically representative a label is of all documents within a topic.', 'abstract_zh': '主题建模已成为分析文本数据的一种关键方法，特别是从大量文档集合中提取有意义的洞察。然而，这些模型的输出通常是一系列关键词，需要人工解释以进行精确的标签化。本研究探索了大型语言模型（LLMs）在自动化和增强主题标签化方面的应用，通过生成更加有意义且上下文相关性强的标签来提高其效果。在应用BERTopic进行主题建模之后，我们探索了不同的方法来选择每个主题内的关键词和文档摘要，然后将其输入LLM以生成标签。每种方法都侧重于不同的方面，如主导主题或多样性，以评估其对标签质量的影响。此外，鉴于缺乏可用于评估主题标签的定量方法，我们提出了一个新的评价指标，用于衡量标签在代表主题内所有文档的语义方面的程度。', 'title_zh': '使用基于大语言模型的方法增强并自动化主题标注'}
{'arxiv_id': 'arXiv:2502.19295', 'title': 'Complex LLM Planning via Automated Heuristics Discovery', 'authors': 'Hongyi Ling, Shubham Parashar, Sambhav Khurana, Blake Olson, Anwesha Basu, Gaurangi Sinha, Zhengzhong Tu, James Caverlee, Shuiwang Ji', 'link': 'https://arxiv.org/abs/2502.19295', 'abstract': 'We consider enhancing large language models (LLMs) for complex planning tasks. While existing methods allow LLMs to explore intermediate steps to make plans, they either depend on unreliable self-verification or external verifiers to evaluate these steps, which demand significant data and computations. Here, we propose automated heuristics discovery (AutoHD), a novel approach that enables LLMs to explicitly generate heuristic functions to guide inference-time search, allowing accurate evaluation of intermediate states. These heuristic functions are further refined through a heuristic evolution process, improving their robustness and effectiveness. Our proposed method requires no additional model training or fine-tuning, and the explicit definition of heuristic functions generated by the LLMs provides interpretability and insights into the reasoning process. Extensive experiments across diverse benchmarks demonstrate significant gains over multiple baselines, including nearly twice the accuracy on some datasets, establishing our approach as a reliable and interpretable solution for complex planning tasks.', 'abstract_zh': '我们考虑增强大型语言模型（LLMs）以应对复杂的规划任务。虽然现有的方法允许LLMs探索中间步骤以制定计划，但这些方法要么依赖于不可靠的自我验证，要么依赖外部验证者来评估这些步骤，这些过程需要大量的数据和计算资源。在此，我们提出了自动启发式发现（AutoHD）这一新颖方法，该方法使LLMs能够明确生成启发式函数以引导推理时的搜索，并允许对中间状态进行准确的评估。通过启发式进化过程进一步优化这些启发式函数，提高了它们的稳健性和有效性。我们提出的方法不需要额外的模型训练或微调，由LLMs生成的明确定义的启发式函数提供了可解释性和对推理过程的洞察。在多种基准测试中的广泛实验表明，与多个基线相比，我们的方法在某些数据集上几乎提高了两倍的准确性，确立了我们的方法作为复杂规划任务的一种可靠且可解释的解决方案。', 'title_zh': '通过自动启发式发现进行复杂的LLM规划'}
{'arxiv_id': 'arXiv:2502.19135', 'title': 'A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management', 'authors': 'Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri', 'link': 'https://arxiv.org/abs/2502.19135', 'abstract': 'This paper presents a novel framework, called PLANTOR (PLanning with Natural language for Task-Oriented Robots), that integrates Large Language Models (LLMs) with Prolog-based knowledge management and planning for multi-robot tasks. The system employs a two-phase generation of a robot-oriented knowledge base, ensuring reusability and compositional reasoning, as well as a three-step planning procedure that handles temporal dependencies, resource constraints, and parallel task execution via mixed-integer linear programming. The final plan is converted into a Behaviour Tree for direct use in ROS2. We tested the framework in multi-robot assembly tasks within a block world and an arch-building scenario. Results demonstrate that LLMs can produce accurate knowledge bases with modest human feedback, while Prolog guarantees formal correctness and explainability. This approach underscores the potential of LLM integration for advanced robotics tasks requiring flexible, scalable, and human-understandable planning.', 'abstract_zh': '本文提出了一种新的框架，称为PLANTOR（基于自然语言的面向任务机器人规划），该框架将大型语言模型（LLMs）与基于Prolog的知识管理和规划相结合，用于多机器人任务。该系统采用了两阶段机器人定向知识库生成方式，确保了可重用性和组合推理能力，以及包含三个步骤的规划流程，通过混合整数线性规划处理时间依赖性、资源约束和并行任务执行问题。最终的计划被转换成行为树，可以直接用于ROS2。我们在块世界和拱结构建造场景中的多机器人装配任务中测试了该框架。结果表明，LLMs能够在适度的人工反馈下生成准确的知识库，而Prolog则保证了形式正确性和可解释性。这一方法突显了LLMs在实现复杂、可扩展且易于人类理解的规划方面潜在的优势，适用于高级机器人任务。', 'title_zh': '通过LLM辅助知识库管理的多agent系统时间规划框架'}
{'arxiv_id': 'arXiv:2502.18928', 'title': 'Talking like Piping and Instrumentation Diagrams (P&IDs)', 'authors': 'Achmad Anggawirya Alimin, Dominik P. Goldstein, Lukas Schulze Balhorn, Artur M. Schweidtmann', 'link': 'https://arxiv.org/abs/2502.18928', 'abstract': "We propose a methodology that allows communication with Piping and Instrumentation Diagrams (P&IDs) using natural language. In particular, we represent P&IDs through the DEXPI data model as labeled property graphs and integrate them with Large Language Models (LLMs). The approach consists of three main parts: 1) P&IDs are cast into a graph representation from the DEXPI format using our pyDEXPI Python package. 2) A tool for generating P&ID knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to LLMs using graph-based retrieval augmented generation (graph-RAG). This approach allows users to communicate with P&IDs using natural language. It extends LLM's ability to retrieve contextual data from P&IDs and mitigate hallucinations. Leveraging the LLM's large corpus, the model is also able to interpret process information in PIDs, which could help engineers in their daily tasks. In the future, this work will also open up opportunities in the context of other generative Artificial Intelligence (genAI) solutions on P&IDs, and AI-assisted HAZOP studies.", 'abstract_zh': '我们提出了一种方法论，允许通过工艺和管道图（P&IDs）使用自然语言进行通信。特别地，我们通过DEXPI数据模型将P&IDs表示为带标签的属性图，并将其与大规模语言模型（LLMs）集成。该方法主要包括三个主要部分：1）使用我们的pyDEXPI Python包将P&IDs转换为从DEXPI格式到图表示。2）一种从pyDEXPI生成P&ID知识图的工具。3）通过基于图的检索增强生成（graph-RAG）将P&ID知识图与LLMs集成。该方法允许用户使用自然语言与P&IDs进行通信。它扩展了LLM从P&IDs检索上下文数据的能力，并减轻了幻觉现象。利用LLM庞大的语料库，该模型还能够解释PID中的过程信息，这可能有助于工程师完成日常任务。未来，这项工作还将为PID上下文中的其他生成型人工智能（genAI）解决方案以及AI辅助的HAZOP研究开辟新的机会。', 'title_zh': '模仿管道和仪表图（P&IDs）的表达方式'}
{'arxiv_id': 'arXiv:2502.18712', 'title': 'TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation', 'authors': 'Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim', 'link': 'https://arxiv.org/abs/2502.18712', 'abstract': "This work leverages Large Language Models (LLMs) to simulate human mobility, addressing challenges like high costs and privacy concerns in traditional models. Our hierarchical framework integrates persona generation, activity selection, and destination prediction, using real-world demographic and psychological data to create realistic movement patterns. Both physical models and language models are employed to explore and demonstrate different methodologies for human mobility simulation. By structuring data with summarization and weighted density metrics, the system ensures scalable memory management while retaining actionable insights. Preliminary results indicate that LLM-driven simulations align with observed real-world patterns, offering scalable, interpretable insights for social problems such as urban planning, traffic management, and public health. The framework's ability to dynamically generate personas and activities enables it to provide adaptable and realistic daily routines. This study demonstrates the transformative potential of LLMs in advancing mobility modeling for societal and urban applications. The source code and interactive demo for our framework are available at this https URL.", 'abstract_zh': '本研究利用大规模语言模型（LLMs）模拟人类移动，解决了传统模型中成本高和隐私保护等挑战。我们的分层框架整合了个性生成、活动选择和目的地预测，并利用真实世界的人口统计和心理数据创建真实的人类移动模式。在使用物理模型和语言模型探索和演示不同的移动模式模拟方法的同时，系统通过使用摘要技术和加权密度度量来结构化数据，确保了可扩展的记忆管理并保留了可操作的信息。初步结果表明，由LLM驱动的模拟与现实世界观察到的模式一致，为城市规划、交通管理以及公共健康等社会问题提供了可扩展且可解释的见解。该框架能够动态生成个性和活动，使其能够提供适应性强且真实的日常活动模式。本研究展示了LLMs在推进社会和城市应用中的移动建模方面的变革潜力。我们的框架的源代码和互动演示可在以下链接获取：[这个网址]。', 'title_zh': 'TrajLLM：一种模块化的大语言模型增强型基于代理的现实人类轨迹仿真框架'}
{'arxiv_id': 'arXiv:2502.18690', 'title': 'Hybrid Voting-Based Task Assignment in Role-Playing Games', 'authors': 'Daniel Weiner, Raj Korpan', 'link': 'https://arxiv.org/abs/2502.18690', 'abstract': "In role-playing games (RPGs), the level of immersion is critical-especially when an in-game agent conveys tasks, hints, or ideas to the player. For an agent to accurately interpret the player's emotional state and contextual nuances, a foundational level of understanding is required, which can be achieved using a Large Language Model (LLM). Maintaining the LLM's focus across multiple context changes, however, necessitates a more robust approach, such as integrating the LLM with a dedicated task allocation model to guide its performance throughout gameplay. In response to this need, we introduce Voting-Based Task Assignment (VBTA), a framework inspired by human reasoning in task allocation and completion. VBTA assigns capability profiles to agents and task descriptions to tasks, then generates a suitability matrix that quantifies the alignment between an agent's abilities and a task's requirements. Leveraging six distinct voting methods, a pre-trained LLM, and integrating conflict-based search (CBS) for path planning, VBTA efficiently identifies and assigns the most suitable agent to each task. While existing approaches focus on generating individual aspects of gameplay, such as single quests, or combat encounters, our method shows promise when generating both unique combat encounters and narratives because of its generalizable nature.", 'abstract_zh': '在角色扮演游戏（RPG）中，沉浸感的水平至关重要，尤其是在游戏中的人物代理向玩家传达任务、提示或想法时。为了使代理能够准确地解释玩家的情绪状态和情境细微差别，需要具备一定的理解和认知基础，这可以通过使用大规模语言模型（LLM）来实现。然而，维持代理在多个情境变化下的专注度需要更为稳固的方法，例如将LLM与专门的任务分配模型相结合，以在整个游戏过程中引导其表现。为此，我们提出了基于投票的任务分配框架（Voting-Based Task Assignment, VBTA），该框架借鉴了人类在任务分配和完成中的推理机制。VBTA将能力配置文件分配给代理，将任务描述分配给任务，然后生成一个适合度矩阵，量化代理能力与任务需求之间的匹配程度。通过利用六种不同的投票方法、预训练的LLM以及冲突基于搜索（CBS）进行路径规划，VBTA能够高效地识别并分配最适合的代理给每个任务。虽然现有的方法主要集中在生成游戏的个别方面，例如单个任务或战斗遭遇，但我们的方法由于其通用性，显示出在生成独特战斗遭遇和叙述方面的潜力。', 'title_zh': '基于角色扮演游戏中的混合投票任务分配算法'}
{'arxiv_id': 'arXiv:2502.18652', 'title': 'Independent Mobility GPT (IDM-GPT): A Self-Supervised Multi-Agent Large Language Model Framework for Customized Traffic Mobility Analysis Using Machine Learning Models', 'authors': 'Fengze Yang, Xiaoyue Cathy Liu, Lingjiu Lu, Bingzhang Wang, Chenxi', 'link': 'https://arxiv.org/abs/2502.18652', 'abstract': 'With the urbanization process, an increasing number of sensors are being deployed in transportation systems, leading to an explosion of big data. To harness the power of this vast transportation data, various machine learning (ML) and artificial intelligence (AI) methods have been introduced to address numerous transportation challenges. However, these methods often require significant investment in data collection, processing, storage, and the employment of professionals with expertise in transportation and ML. Additionally, privacy issues are a major concern when processing data for real-world traffic control and management. To address these challenges, the research team proposes an innovative Multi-agent framework named Independent Mobility GPT (IDM-GPT) based on large language models (LLMs) for customized traffic analysis, management suggestions, and privacy preservation. IDM-GPT efficiently connects users, transportation databases, and ML models economically. IDM-GPT trains, customizes, and applies various LLM-based AI agents for multiple functions, including user query comprehension, prompts optimization, data analysis, model selection, and performance evaluation and enhancement. With IDM-GPT, users without any background in transportation or ML can efficiently and intuitively obtain data analysis and customized suggestions in near real-time based on their questions. Experimental results demonstrate that IDM-GPT delivers satisfactory performance across multiple traffic-related tasks, providing comprehensive and actionable insights that support effective traffic management and urban mobility improvement.', 'abstract_zh': '随着城市化进程的推进，越来越多的传感器被部署在交通系统中，产生了大量数据。为了利用这些海量的交通数据，各种机器学习（ML）和人工智能（AI）方法被引入以应对各种交通挑战。然而，这些方法往往需要大量投资于数据采集、处理、存储以及需要具备交通和ML专业知识的专业人员。此外，在处理用于实时交通控制和管理的数据时，隐私问题也是一项主要关注点。为了解决这些挑战，研究团队提出了一种基于大规模语言模型（LLMs）的创新多代理框架，名为独立移动GPT（IDM-GPT），用于定制化的交通分析、管理建议以及隐私保护。IDM-GPT有效地将用户、交通数据库和ML模型经济地连接起来。IDM-GPT训练、定制并应用于多种基于LLM的AI代理进行多种功能，包括用户查询理解、提示优化、数据分析、模型选择、以及性能评估和提升。借助IDM-GPT，即使没有交通或ML背景的用户，也可以基于问题获得高效且直观的数据分析和个性化建议，实现近乎实时的响应。实验结果表明，IDM-GPT在多个与交通相关的任务中提供了令人满意的表现，提供了全面且可操作的见解，支持有效的交通管理和城市流动性改进。', 'title_zh': '独立移动GPT（IDM-GPT）：一种基于自我监督的多代理大型语言模型框架，用于使用机器学习模型进行定制化交通移动分析'}
{'arxiv_id': 'arXiv:2502.18532', 'title': 'CuDIP: Enhancing Theorem Proving in LLMs via Curriculum Learning-based Direct Preference Optimization', 'authors': 'Shuming Shi, Ruobing Zuo, Gaolei He, Jianlin Wang, Chenyang Xu, Zhengfeng Yang', 'link': 'https://arxiv.org/abs/2502.18532', 'abstract': 'Automated theorem proving (ATP) is one of the most challenging mathematical reasoning tasks for Large Language Models (LLMs). Most existing LLM-based ATP methods rely on supervised fine-tuning, which results in a limited alignment between the theorem proving process and human preferences. Direct Preference Optimization (DPO), which aligns LLMs with human preferences, has shown positive effects for certain tasks. However, the lack of high-quality preference data for theorem proving presents a significant challenge. In this paper, we innovatively apply DPO to formal automated theorem proving and introduces a Curriculum Learning-based DPO Iterative Theorem Proving (CuDIP) method. Specifically, we propose a method for constructing preference data which utilizes LLMs and existing theorem proving data to enhance the diversity of the preference data while reducing the reliance on human preference annotations. We then integrate this preference data construction method with curriculum learning to iteratively fine-tune the theorem proving model through DPO. Experimental results on the MiniF2F and ProofNet datasets demonstrate the effectiveness of the proposed method.', 'abstract_zh': '自动定理证明（ATP）是大型语言模型（LLMs）面临的最具挑战性的数学推理任务之一。现有的大多数基于LLM的ATP方法依赖于监督微调，这导致定理证明过程与人类偏好之间的对齐有限。直接受偏好优化（Direct Preference Optimization, DPO），该方法通过调整LLM与人类偏好之间的对齐来改善某些任务的效果。然而，高质量的偏好数据的缺乏为定理证明带来了重大挑战。在本文中，我们创新地将DPO应用到形式化的自动定理证明中，并提出了一种基于课程学习的DPO迭代定理证明（Curriculum-based Direct Preference Optimization Iterative Theorem Proving, CuDIP）方法。具体而言，我们提出了一种利用LLM和现有定理证明数据构建偏好数据的方法，以增强偏好数据的多样性，同时减少对人类偏好注释的依赖。然后，我们将这种偏好数据构建方法与课程学习结合，通过DPO迭代微调定理证明模型。在MiniF2F和ProofNet数据集上的实验结果表明了所提方法的有效性。', 'title_zh': 'CuDIP：通过基于课程学习的直接偏好优化提升大语言模型的定理证明能力'}
{'arxiv_id': 'arXiv:2502.18863', 'title': 'Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM', 'authors': 'Junxiao Ma, Jingjing Wang, Jiamin Luo, Peiying Yu, Guodong Zhou', 'link': 'https://arxiv.org/abs/2502.18863', 'abstract': 'Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm \\textbf{M}ulti-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.', 'abstract_zh': '以下是论文内容或标题的中文翻译，符合学术规范：\n\n先前对视频异常检测（VAD）的研究主要关注检测视频中的每一帧是否异常，很大程度上忽视了视频的结构化语义信息（即异常事件发生的时间、地点及具体内容）。基于这一点，本文提出了一种新的对话范式任务——多场景视频异常事件提取与定位（M-VAE），旨在提取异常事件四元组（即主体、事件类型、对象、场景），并定位这些事件。进一步地，本文认为这一新任务面临两个关键挑战，即全局与局部空间建模以及全局与局部空间平衡。为解决这些问题，本文提出了一种全局与局部空间敏感的大语言模型（LLM）——Sherlock，即类似福尔摩斯追踪犯罪事件，专门用于M-VAE任务。具体而言，该模型设计了一个增强全局与局部空间的混音模块（GSM）和一个空间不平衡调节器（SIR），分别解决上述两个挑战。对于我们在M-VAE指令数据集上进行的广泛实验表明，Sherlock在与几种先进的视频LLM相比时显示出显著的优势。这证实了全局与局部空间信息对于M-VAE任务的重要性以及Sherlock在捕捉这些信息方面的有效性。', 'title_zh': 'Sherlock：通过全局-局部空间敏感的大语言模型实现多场景视频异常事件提取与定位'}
{'arxiv_id': 'arXiv:2502.18862', 'title': 'Investigating Generalization of One-shot LLM Steering Vectors', 'authors': 'Jacob Dunefsky, Arman Cohan', 'link': 'https://arxiv.org/abs/2502.18862', 'abstract': "Steering vectors have emerged as a promising approach for interpreting and controlling LLMs, but current methods typically require large contrastive datasets that are often impractical to construct and may capture spurious correlations. We propose directly optimizing steering vectors through gradient descent on a single training example, and systematically investigate how these vectors generalize. We consider several steering optimization techniques, including multiple novel ones, and find that the resulting vectors effectively mediate safety-relevant behaviors in multiple models. Indeed, in experiments on an alignment-faking model, we are able to optimize one-shot steering vectors that induce harmful behavior on benign examples and whose negations suppress harmful behavior on malign examples. And in experiments on refusal suppression, we demonstrate that one-shot optimized steering vectors can transfer across inputs, yielding a Harmbench attack success rate of 96.9%. Furthermore, to quantitatively assess steering effectiveness in instruction-tuned models, we develop a novel evaluation framework using sequence probabilities from the corresponding base model. With this framework, we analyze how steering vectors modulate an instruction-tuned LLM's ability to recover from outputting false information, and find that this ability derives from the base model. Overall, our findings suggest that optimizing steering vectors on a single example can mediate misaligned behavior in LLMs, and provide a path toward better understanding the relationship between LLM behavior and activation space structure.", 'abstract_zh': '引导向量已成为解释和控制大型语言模型（LLMs）的一种有前途的方法，但当前的方法通常需要大量的对比数据集，这些数据集往往难以构建，并且可能会捕获虚假的相关性。我们提出了一种直接通过单个训练示例的梯度下降来优化引导向量的方法，并系统地研究了这些向量的一般化能力。我们考虑了几种引导优化技术，包括几种新颖的方法，并发现这些生成的向量在多种模型中有效地调节了与安全性相关的行为。实际上，在针对对齐欺骗模型的实验中，我们能够优化出能够诱导有害行为的单次射击引导向量，并通过其否定有效地抑制有害行为。在拒绝抑制实验中，我们展示了单次射击优化的引导向量可以在不同输入之间转移，使其对Harmbench的攻击成功率达到了96.9%。此外，为了量化指令调整模型中引导的有效性，我们开发了一种新颖的评估框架，使用与之对应的基模型的序列概率。通过这个框架，我们分析了引导向量如何调节一个指令调整的LLM从输出错误信息中恢复的能力，并发现这种能力来自于基模型。总体而言，我们的发现表明，通过对单个示例进行引导向量的优化可以调节LLMs中的未对齐行为，并为更好地理解LLM行为与其激活空间结构之间的关系提供了路径。', 'title_zh': '探究一-shot LLM导向向量的泛化能力'}
{'arxiv_id': 'arXiv:2502.18695', 'title': 'Policy-as-Prompt: Rethinking Content Moderation in the Age of Large Language Models', 'authors': 'Konstantina Palla, José Luis Redondo García, Claudia Hauff, Francesco Fabbri, Henrik Lindström, Daniel R. Taber, Andreas Damianou, Mounia Lalmas', 'link': 'https://arxiv.org/abs/2502.18695', 'abstract': 'Content moderation plays a critical role in shaping safe and inclusive online environments, balancing platform standards, user expectations, and regulatory frameworks. Traditionally, this process involves operationalising policies into guidelines, which are then used by downstream human moderators for enforcement, or to further annotate datasets for training machine learning moderation models. However, recent advancements in large language models (LLMs) are transforming this landscape. These models can now interpret policies directly as textual inputs, eliminating the need for extensive data curation. This approach offers unprecedented flexibility, as moderation can be dynamically adjusted through natural language interactions. This paradigm shift raises important questions about how policies are operationalised and the implications for content moderation practices. In this paper, we formalise the emerging policy-as-prompt framework and identify five key challenges across four domains: Technical Implementation (1. translating policy to prompts, 2. sensitivity to prompt structure and formatting), Sociotechnical (3. the risk of technological determinism in policy formation), Organisational (4. evolving roles between policy and machine learning teams), and Governance (5. model governance and accountability). Through analysing these challenges across technical, sociotechnical, organisational, and governance dimensions, we discuss potential mitigation approaches. This research provides actionable insights for practitioners and lays the groundwork for future exploration of scalable and adaptive content moderation systems in digital ecosystems.', 'abstract_zh': '内容审查在塑造安全和平等的在线环境方面发挥着关键作用，平衡平台标准、用户期望和监管框架。传统上，这一过程涉及将政策操作化为指导原则，然后下游的人类审核员使用这些指导原则执行任务或进一步标注数据集以训练机器学习内容审查模型。然而，近期大型语言模型（LLMs）的发展正在改变这一格局。这些模型现在可以直接将政策解释为文本输入，从而消除大量数据整理的需要。这种方法提供了前所未有的灵活性，因为可以通过自然语言互动动态调整内容审查。这一范式转变提出了关于政策操作化及其对内容审查实践影响的重要问题。本文正式提出了新兴的政策即提示（Policy-as-Prompt）框架，并在其四个领域中识别了五个关键挑战：技术实施（1. 将政策转化为提示，2. 提示结构和格式的敏感性），社会技术（3. 政策形成的科技决定论风险），组织（4. 政策与机器学习团队间角色的演变），治理（5. 模型治理与问责制）。通过在技术、社会技术、组织和治理维度上分析这些挑战，我们讨论了潜在的缓解方法。本文为实践者提供了可操作的见解，并为未来关于可扩展和适应性强的内容审查系统的探索奠定了基础。', 'title_zh': '政策即提示：在大规模语言模型时代重新思考内容审核'}
{'arxiv_id': 'arXiv:2502.18641', 'title': 'WhatELSE: Shaping Narrative Spaces at Configurable Level of Abstraction for AI-bridged Interactive Storytelling', 'authors': 'Zhuoran Lu, Qian Zhou, Yi Wang', 'link': 'https://arxiv.org/abs/2502.18641', 'abstract': 'Generative AI significantly enhances player agency in interactive narratives (IN) by enabling just-in-time content generation that adapts to player actions. While delegating generation to AI makes IN more interactive, it becomes challenging for authors to control the space of possible narratives - within which the final story experienced by the player emerges from their interaction with AI. In this paper, we present WhatELSE, an AI-bridged IN authoring system that creates narrative possibility spaces from example stories. WhatELSE provides three views (narrative pivot, outline, and variants) to help authors understand the narrative space and corresponding tools leveraging linguistic abstraction to control the boundaries of the narrative space. Taking innovative LLM-based narrative planning approaches, WhatELSE further unfolds the narrative space into executable game events. Through a user study (N=12) and technical evaluations, we found that WhatELSE enables authors to perceive and edit the narrative space and generates engaging interactive narratives at play-time.', 'abstract_zh': '生成式人工智能显著增强了交互叙事（IN）中的玩家主动权，通过实现即时内容生成，这种生成能够根据玩家的行为进行调整。虽然将生成任务委托给人工智能使互动叙事更具互动性，但这也使得作者难以控制可能故事的空间——最终由玩家与人工智能互动产生的故事就诞生于这个空间中。在本文中，我们介绍了一种名为WhatELSE的AI桥梁交互叙事创作系统，该系统能够从示例故事中创建叙事可能性空间。WhatELSE提供了三种视图（叙事枢轴、大纲和变体）以帮助作者理解叙事空间，并通过语言抽象技术提供相应的工具，以控制叙事空间的边界。通过利用创新的基于大规模语言模型（LLM）的叙事规划方法，WhatELSE进一步将叙事空间扩展为可执行的游戏事件。通过一项包含12名用户的研究和技术评估，我们发现WhatELSE使作者能够感知和编辑叙事空间，并在游戏过程中生成具有互动性的故事。', 'title_zh': 'WhatELSE：通过可配置抽象层次构建人工智能桥接互动叙事的空间叙事结构'}
{'arxiv_id': 'arXiv:2502.18517', 'title': 'RewardDS: Privacy-Preserving Fine-Tuning for Large Language Models via Reward Driven Data Synthesis', 'authors': 'Jianwei Wang, Junyao Yang, Haoran Li, Huiping Zhuang, Cen Chen, Ziqian Zeng', 'link': 'https://arxiv.org/abs/2502.18517', 'abstract': 'The success of large language models (LLMs) has attracted many individuals to fine-tune them for domain-specific tasks by uploading their data. However, in sensitive areas like healthcare and finance, privacy concerns often arise. One promising solution is to sample synthetic data with Differential Privacy (DP) guarantees to replace private data. However, these synthetic data contain significant flawed data, which are considered as noise. Existing solutions typically rely on naive filtering by comparing ROUGE-L scores or embedding similarities, which are ineffective in addressing the noise. To address this issue, we propose RewardDS, a novel privacy-preserving framework that fine-tunes a reward proxy model and uses reward signals to guide the synthetic data generation. Our RewardDS introduces two key modules, Reward Guided Filtering and Self-Optimizing Refinement, to both filter and refine the synthetic data, effectively mitigating the noise. Extensive experiments across medical, financial, and code generation domains demonstrate the effectiveness of our method.', 'abstract_zh': '大规模语言模型（LLMs）的成功吸引了许多个人通过上传其数据来针对特定领域进行微调。然而，在医疗和金融等敏感领域，隐私问题往往随之而来。一种有前景的解决方案是使用差分隐私（DP）保证的合成数据来替代私人数据。然而，这些合成数据中包含大量被视为噪声的错误数据。现有的解决方案通常依赖于比较ROUGE-L分数或嵌入相似性进行简单的过滤，这在消除噪声方面效果不佳。为了解决这个问题，我们提出了RewardDS，这是一种新颖的隐私保护框架，通过微调一个奖励代理模型并使用奖励信号来指导合成数据的生成。我们的RewardDS引入了两个关键模块——奖励引导过滤和自我优化精炼，既能过滤又能精炼合成数据，有效地减少了噪声。在医疗、金融和代码生成等多个领域的广泛实验中，验证了我们方法的有效性。', 'title_zh': 'RewardDS：通过奖励驱动的数据合成实现大型语言模型的隐私保护微调'}
{'arxiv_id': 'arXiv:2502.18489', 'title': 'LLM4EFFI: Leveraging Large Language Models to Enhance Code Efficiency and Correctness', 'authors': 'Tong Ye, Weigang Huang, Xuhong Zhang, Tengfei Ma, Peiyu Liu, Jianwei Yin, Wenhai Wang', 'link': 'https://arxiv.org/abs/2502.18489', 'abstract': 'Large Language Models (LLMs), particularly Code LLMs, have demonstrated impressive performance in code generation. Current research primarily focuses on the correctness of generated code, while efficiency remains less explored. Recent works have focused on modifying the initial version of the code to improve its efficiency. However, such refinements are limited by the algorithmic design and overall logic of the initial code, resulting in only incremental improvements. In contrast, when human developers write high-quality code, they typically begin by designing several potential solutions at the logical level, evaluating various algorithms and their complexities, and then proceeding to implement and optimize the solution. In this study, we introduce \\tool: \\uline{L}arge \\uline{L}anguage \\uline{M}odel for Code \\uline{Effi}ciency, a novel framework that enables LLMs to generate code that balances both efficiency and correctness. Specifically, \\tool divides the efficiency optimization process into two domains: algorithmic exploration in the logic domain and implementation optimization in the code domain. The correctness of the code is then guaranteed through a synthetic test case refinement process. This approach, which prioritizes efficiency before ensuring correctness, offers a new paradigm for efficient code generation. Experiments demonstrate that \\tool consistently improves both efficiency and correctness, achieving new state-of-the-art performance in code efficiency benchmarks across various LLM backbones.', 'abstract_zh': '大型语言模型（LLMs），尤其是代码LLMs，在代码生成方面展现了令人印象深刻的表现。当前的研究主要集中在生成代码的正确性上，而效率则较少被探讨。最近的研究重点在于修改初始代码版本以提高其效率。然而，这些改进受到初始代码的算法设计和总体逻辑的限制，导致只能实现渐进式的改进。相比之下，当人类开发者编写高质量代码时，他们通常会从逻辑层面开始设计几种潜在解决方案，评估不同的算法及其复杂度，然后再进行实现和优化。在这项研究中，我们引入了一个名为 \\tool 的新框架：大型语言模型用于代码效率，该框架使LLMs能够生成既高效又正确的代码。具体而言，\\tool 将效率优化过程分为两个领域：逻辑域中的算法探索和代码域中的实现优化。代码的正确性则通过合成测试用例的改进过程来保证。该方法在优先考虑效率后再确保正确性的策略，为高效的代码生成提供了一种新的范式。实验结果显示，\\tool 一致地提高了效率和正确性，在不同LLM骨干模型的代码效率基准测试中取得了新的最先进性能。', 'title_zh': 'LLM4EFFI：利用大型语言模型提升代码效率和正确性'}
