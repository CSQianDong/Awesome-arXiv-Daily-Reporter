# Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models 

**Title (ZH)**: 作为评判者：通过大型语言模型的评判一致性改进检索增强生成的评估 

**Authors**: Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.18817)  

**Abstract**: Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at this https URL. 

**Abstract (ZH)**: 提取增强生成（RAG）已被证明在减轻大规模语言模型（LLMs）的幻觉方面具有有效性。然而，现有的自动化评估指标无法公正地评估RAG模型在训练和评估阶段生成的输出。基于LLM的判断模型具有生成高质量判断的潜力，但它们对评估提示的高度敏感性会导致在判断RAG模型输出时的一致性问题。本文介绍了Judge-Consistency（ConsJudge）方法，旨在增强LLMs以生成更加准确的RAG模型评价。具体而言，ConsJudge促使LLMs基于多种判断维度的不同组合生成不同的判断，并利用判断一致性来评估这些判断，从中选择接受和拒绝的判断用于DPO训练。我们的实验表明，ConsJudge能够有效地为各种RAG模型和数据集中的RAG模型优化提供更加准确的判断。进一步的分析显示，ConsJudge生成的判断与优秀的LLM高度一致。所有代码可在以下链接处获取：https://xxxxx（请注意，这里的URL应替换为实际的代码存放地址）。 

---
# FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models 

**Title (ZH)**: FactReasoner：一种用于大型语言模型长篇事实性评估的概率方法 

**Authors**: Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale  

**Link**: [PDF](https://arxiv.org/pdf/2502.18573)  

**Abstract**: Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在生成任务上展现了巨大的能力，但在保证生成内容的准确性方面却存在困难。这使得这些模型在期望获得事实准确回答的真实场景中不够可靠。本文提出了一种新的事实性评估器——FactReasoner，它依赖于概率推理来评估长文本生成响应的事实性。具体而言，FactReasoner 将响应分解为原子单位，从外部知识源检索与这些原子单位相关的内容，并通过逻辑关系（蕴含、矛盾）的概率编码构造原子和上下文的联合概率分布。FactReasoner 然后计算检索到的上下文是否支持响应中原子单位的概率。我们在有标签和无标签基准数据集上的实验清楚地表明，与基于提示的方法相比，FactReasoner 在事实精密性和召回率方面有了显著改进。 

---
