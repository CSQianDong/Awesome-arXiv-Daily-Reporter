{'arxiv_id': 'arXiv:2506.09944', 'title': 'Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking', 'authors': 'Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, Xi Ye', 'link': 'https://arxiv.org/abs/2506.09944', 'abstract': 'Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.'}
{'arxiv_id': 'arXiv:2506.09507', 'title': 'TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding', 'authors': 'Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo', 'link': 'https://arxiv.org/abs/2506.09507', 'abstract': 'Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongruity in their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance. To address this impediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE}) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this \\ourRoPE, we introduce \\textbf{\\model}, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4K sequence length, \\model exhibits training and inference speeds that are \\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4\\% on language modeling benchmarks. \\model furthermore scales more effectively: \\model-1.3B gains \\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.'}
