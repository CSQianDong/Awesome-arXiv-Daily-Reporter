{'arxiv_id': 'arXiv:2412.14675', 'title': 'LLMs as mediators: Can they diagnose conflicts accurately?', 'authors': 'Özgecan Koçak, Phanish Puranam, Afşar Yegin', 'link': 'https://arxiv.org/abs/2412.14675', 'abstract': "Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Koçak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.", 'abstract_zh': '以往的研究表明，观察者要在两方争执中进行干预，必须能够可靠地区分他们的分歧来源，是基于对事实的不同信念（因果关系）还是基于价值观的不同（道德）。本论文旨在测试OpenAI的大规模语言模型GPT-3.5和GPT-4是否能够完成这一任务，以及哪种类型的分歧对这些语言模型来说尤为具有诊断难度。我们复制了Koçak等人（2003）的研究，使用情景设计（vignette design），并用OpenAI的GPT-3.5和GPT-4进行实验。研究发现，这两种语言模型在因果关系和道德规范之间的语义理解与人类相似，并且能够可靠地区分这两者。当要求诊断对话中的分歧来源时，与人类相比，这两种语言模型更倾向于高估因果性分歧的程度，并低估道德性分歧的程度，尤其是在涉及具体语言和具体问题时，GPT-4的表现尤为明显。GPT-3.5在使用近端或远端量表时的表现不如GPT-4或人类。本研究提供了首次测试利用语言模型诊断因果性和评价性分歧以调解冲突的可能性。', 'title_zh': 'LLM作为调解者：它们能否准确诊断冲突？'}
{'arxiv_id': 'arXiv:2412.14588', 'title': 'Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning', 'authors': 'Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu', 'link': 'https://arxiv.org/abs/2412.14588', 'abstract': "In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.", 'abstract_zh': '在法律实践中，法官应用刑法的三元法理论，依次评估犯罪行为的构成要件、违法性和罪过性，以确定某个人的行为是否构成犯罪。尽管当前的法律大规模语言模型（LLMs）在判决预测方面显示出潜在的准确性，但由于缺乏适当的基准数据集，它们在判断无辜结果方面缺乏三元推理能力，因此无法预测无辜判决。由此，每个输入都会自动被指控，限制了它们在法律情境中的实际应用价值。为弥补这一差距，我们引入了LJPIV，这是第一个包含无辜判决的法律判决预测基准数据集。遵循三元法理论，我们通过基于LLM的扩展和手动验证，扩展了三个广泛使用的法律数据集。我们的实验结果显示：（1）当前的法律LLMs在LJPIV上的表现仍有显著提升空间，即使是表现最佳的模型，在LJPIV上的F1分数也低于0.3；（2）我们的策略在提高领域内和跨领域判决预测准确性方面表现出明显优势，特别是对于导致无辜判决的案件。', 'title_zh': '超越罪恶感：基于三分推理的法律判决预测'}
