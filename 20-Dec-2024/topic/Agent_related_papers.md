# Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines 

**Title (ZH)**: Bel Esprit：多Agent框架构建AI模型流水线 

**Authors**: Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf  

**Link**: [PDF](https://arxiv.org/pdf/2412.14684)  

**Abstract**: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at this https URL. 

**Abstract (ZH)**: 随着人工智能（AI）需求的增长，以应对复杂的现实世界任务，单一模型往往不足以满足需求，需要将多个模型整合到管道中。本文介绍了Bel Esprit，这是一种基于用户自定义需求构建AI模型管道的对话型代理。Bel Esprit采用多代理框架，其中子代理合作以澄清需求、构建、验证并使用适当模型填充管道。我们通过使用人类精选和合成数据来展示该框架在从模糊用户查询中生成管道方面的有效性。详细的错误分析突显了在构建管道过程中仍面临的挑战。Bel Esprit可在此免费试用：[此链接]。 

---
# The Digital Ecosystem of Beliefs: does evolution favour AI over humans? 

**Title (ZH)**: 信念数字生态系统的演化：人工智能相较于人类更有优势吗？ 

**Authors**: David M. Bossens, Shanshan Feng, Yew-Soon Ong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14500)  

**Abstract**: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on this http URL understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles. 

**Abstract (ZH)**: 随着人工智能系统被整合到社交网络中，存在一种AI安全问题，即AI生成的内容可能主导网络，例如在流行度或影响方面。为了理解这些问题，本文提出了Belief数字生态系（Digico），这是首个基于通用达尔文主义方法的进化框架，用于在模拟社交网络中进行受控实验，以研究多群体互动。该框架模拟了一组随时间根据进化更新改变其交流策略的代理，通过消息进行互动，并通过基于传染模型的动力学相互影响信念，并通过认知拉马克主义遗传维持信念。初步实验表明：a) 当AI的交流速度、进化速度以及对推荐算法的影响更大时，它们可以获得80%到95%的曝光率，具体比例取决于影响效益的规模；b) 针对宣传设计的AI通常可以让一半的人类接受极端信念，当代理只相信有限数量的渠道时，这一比例可达到85%；c) 对违反代理信念的内容施加惩罚可以将宣传效果降低8%。此外，我们进一步讨论了该研究在控制（例如法律法规）方面的意义以及Digico作为研究进化原理工具的意义。 

---
# Python Agent in Ludii 

**Title (ZH)**: Python代理在Ludii中的应用 

**Authors**: Izaias S. de Lima Neto, Marco A. A. de Aguiar Vieira, Anderson R. Tavares  

**Link**: [PDF](https://arxiv.org/pdf/2412.14372)  

**Abstract**: Ludii is a Java general game system with a considerable number of board games, with an API for developing new agents and a game description language to create new games. To improve versatility and ease development, we provide Python interfaces for agent programming. This allows the use of Python modules to implement general game playing agents.
As a means of enabling Python for creating Ludii agents, the interfaces are implemented using different Java libraries: jpy and Py4J. The main goal of this work is to determine which version is faster. To do so, we conducted a performance analysis of two different GGP algorithms, Minimax adapted to GGP and MCTS. The analysis was performed across several combinatorial games with varying depth, branching factor, and ply time. For reproducibility, we provide tutorials and repositories.
Our analysis includes predictive models using regression, which suggest that jpy is faster than Py4J, however slower than a native Java Ludii agent, as expected. 

**Abstract (ZH)**: Ludii 是一个用 Java 编写的通用游戏系统，包含大量的桌面游戏，并提供了开发新智能体的 API 和游戏描述语言来创建新游戏。为了提高灵活性和简化开发，我们提供了 Python 接口用于智能体编程。这使得可以使用 Python 模块来实现通用游戏智能体。

为了让用户能够使用 Python 创建 Ludii 智能体，我们使用了不同的 Java 库（jpy 和 Py4J）来实现这些接口。本工作的主要目标是确定哪种版本运行得更快。为此，我们对两种不同的广义游戏智能体（GGP）算法——适应了 GGP 的 Minimax 算法和蒙特卡洛树搜索（MCTS）——进行了性能分析。这种分析跨越了多个具有不同深度、分支因子和走棋时间的组合游戏。

为了确保结果的可重复性，我们提供了教程和代码库。我们的分析还包括使用回归模型进行的预测模型，结果显示 jpy 的运行速度比 Py4J 更快，但比原生的 Java Ludii 智能体慢，这符合预期。 

---
# Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents 

**Title (ZH)**: 将罗尔斯伦理学运用于规范学习代理的公平性之中 

**Authors**: Jessica Woodgate, Paul Marshall, Nirav Ajmeri  

**Link**: [PDF](https://arxiv.org/pdf/2412.15163)  

**Abstract**: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics. 

**Abstract (ZH)**: 社会规范是社会中普遍存在的行为标准。然而，当个体在做决策时不考虑对其他人的影响时，可能会产生一些规范，从而导致某些个体的压迫。本文介绍了一种名为RAWL-E的方法，用于创建具有伦理规范学习能力的智能体。RAWL-E智能体在其决策过程中采用最大化最小效用（即最大化最低效用）的原则，这是一种源自罗尔斯理论的公平原则，以此促进伦理规范的形成，平衡社会福祉与个体目标。我们通过对模拟的采集场景进行了评估。结果表明，RAWL-E智能体社会中产生的规范能够提升社会福利、公平性和鲁棒性，并且能够确保比未实施罗尔斯伦理学的智能体社会中更高的最低体验。 

---
# Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning 

**Title (ZH)**: 基于代理-时间信用分配的理想政策 Preservation 在稀疏多代理强化学习中的优化 

**Authors**: Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht  

**Link**: [PDF](https://arxiv.org/pdf/2412.14779)  

**Abstract**: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods. 

**Abstract (ZH)**: 在多智能体环境中，智能体往往难以学习到最优策略，特别是在由于稀疏或延迟的全局奖励而难以评估中间时间步的行动时尤为明显。特别是在长期任务中，评估中间时间步的行动尤为具有挑战性。为此，我们提出了一种名为Temporal-Agent Reward Redistribution (TAR$^2$)的新方法，旨在通过在时间和智能体之间重新分配稀疏奖励来解决智能体时间性信用分配问题。TAR$^2$将稀疏的全局奖励分解为时间步特定的奖励，并计算这些奖励的智能体特定贡献。我们从理论上证明了TAR$^2$等同于基于潜能的奖励塑形方法，确保最优策略不会发生变化。实验证明TAR$^2$能够稳定并加速学习过程。此外，我们还展示了当将TAR$^2$与单智能体强化学习算法结合使用时，其性能与传统的多智能体强化学习方法相当甚至更优。 

---
# DriveGPT: Scaling Autoregressive Behavior Models for Driving 

**Title (ZH)**: DriveGPT: 扩展自回归行为模型在驾驶场景中的应用 

**Authors**: Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14415)  

**Abstract**: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling. 

**Abstract (ZH)**: 我们提出DriveGPT，一种可扩展的自主驾驶行为模型。我们将驾驶建模为一个序列决策任务，并采用变换器模型以自回归的方式预测未来代理状态。通过大幅提升模型参数和训练数据的数量级，我们能够探讨数据集规模、模型参数和计算资源在不同规模上的扩展性能。我们通过定量指标和定性示例，在规划任务的不同尺度上评估DriveGPT，包括在复杂真实场景中的闭环驾驶表现。在另一个预测任务中，DriveGPT超越了最先进的基线方法，并通过在大规模数据集上进行预训练显示出改进的性能，进一步验证了数据扩展带来的益处。 

---
# Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference 

**Title (ZH)**: 大规模实时强化学习的梯次异步推理实现 방법 

**Authors**: Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish  

**Link**: [PDF](https://arxiv.org/pdf/2412.14355)  

**Abstract**: Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pokémon and Tetris. 

**Abstract (ZH)**: 实时环境在智能体执行动作推理和学习的过程中不断变化，因此需要较高的交互频率以有效减小遗憾。然而，最近的机器学习进展涉及更大的神经网络和更长的推理时间，这在实时系统中提出了关于反应时间至关重要的情况下其适用性的疑问。我们对实时强化学习（RL）环境中的遗憾下界进行了分析，表明在典型的顺序交互和学习范式中，通常无法减小长期遗憾，但在具有足够异步计算能力时，上述情况可以改变。我们提出了新颖的算法来协调异步推理过程，以确保动作在一致的时间间隔内执行，并证明高动作推理时间的模型仅受推理窗口内环境有效随机性的限制，而非动作频率的限制。我们的分析表明，所需推理过程的数量随推理时间的增加呈线性增长，同时，在从Game Boy游戏如 Pokémon 和 Tetris 的实时模拟中学习时，可以使用比现有方法大几个数量级的模型。

请注意，我在此翻译中尽量保持了原文的学术规范和术语一致性。如需进一步优化或有特定术语的处理要求，请告知我。 

---
# Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs 

**Title (ZH)**: 多语眼医：评估和消除低收入和中等收入国家（LMICs）中LLM眼科问答偏差的多语言基准 

**Authors**: David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama  

**Link**: [PDF](https://arxiv.org/pdf/2412.14304)  

**Abstract**: Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe. 

**Abstract (ZH)**: 当前的眼科临床工作流程受到过度转诊、长时间等待以及复杂且异质性医疗记录的困扰。大型语言模型（LLMs）为自动化各种程序（如分诊、初步测试如视力评估以及报告总结）提供了有希望的解决方案。然而，在自然语言问答任务中，LLMs在不同语言上的表现差异显著，这可能导致在低收入和中等收入国家（LMICs）中医疗保健不平等现象的加剧。本研究介绍了首个经过人工精标的问题多语言眼科问答基准，这使得可以在不同语言之间进行直接的语言对比。我们在7种不同语言上对6种流行的LLMs进行了评估，揭示了不同语言之间的显著偏差，这强调了在LMICs中临床应用LLMs的风险。现有的去偏见方法如翻译链式思考或检索增强生成（RAG）单靠自己并不能弥补这一性能差距，往往无法在所有语言中提升性能，并且缺乏对医学领域的特异性。为了解决这一问题，我们提出了CLARA（跨语言反思代理系统），一种利用检索增强生成和自我验证的新颖的推理时间去偏见方法。我们的方法不仅提高了所有语言的性能，还显著缩小了多语言偏差差距，促进了全球范围内LLMs的公平应用。 

---
# Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs 

**Title (ZH)**: 异构多代理强化学习在无线局域网中分布式信道访问中的应用 

**Authors**: Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14218)  

**Abstract**: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents. 

**Abstract (ZH)**: 本文探讨了多智能体强化学习（MARL）在无线局域网中解决分布式信道访问问题的应用。特别是，我们考虑了更具有挑战性且更具实用性的案例，即智能体异构地采用基于值或基于策略的强化学习算法进行模型训练。我们提出了一种名为QPMIX的异构MARL训练框架，该框架采用了集中式训练与分布式执行的模式，以支持异构智能体之间的协作。此外，我们理论证明了在使用线性价值函数近似时，所提出的方法的收敛性。该方法最大化了网络吞吐量并确保了站间的公平性，从而增强了整体网络性能。仿真结果表明，在饱和流量场景下，所提出的QPMIX算法在吞吐量、平均延迟、延迟抖动和碰撞率方面均优于传统的载波侦听多址访问/冲突避免（CSMA/CA）。此外，在非饱和和延迟敏感的流量场景中，QPMIX算法显示出较强的鲁棒性，并促进了异构智能体之间的合作。 

---
# Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle 

**Title (ZH)**: 生成式AI工具包——一种在整个生命周期内提高基于LLM的应用质量的框架 

**Authors**: Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14215)  

**Abstract**: As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve 

**Abstract (ZH)**: 随着基于大语言模型（LLM）的应用程序用户数量达到数百万，确保它们的可扩展性和持续的质量改进对于成功至关重要。然而，目前开发、维护和运行（DevOps）这些应用程序的工作流程大多是手工的、速度慢且依赖于试错。本文介绍了一种生成式人工智能工具包，该工具包在整个生命周期中自动化了基于大语言模型的应用程序的关键工作流程。该工具包有助于配置、测试、连续监控和优化生成式人工智能应用程序，如代理，从而显著提高质量并缩短发布周期。我们通过代表性用例展示了该工具包的有效性，分享了最佳实践，并概述了未来的增强措施。由于我们相信该生成式人工智能工具包对其他团队也会很有帮助，我们将其开源，并希望其他团队能够将其用于进一步的发展、适应和改进。 

---
# Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution 

**Title (ZH)**: 树形代码：稳健复杂任务规划与执行的混合方法 

**Authors**: Ziyi Ni, Yifan Li, Daxiang Dong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14212)  

**Abstract**: The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes. 

**Abstract (ZH)**: 大型语言模型（LLMs）的卓越能力显著加速了基于代理的快速崛起和广泛采用。近期的研究表明，通过生成Python代码将基于LLM代理的动作整合到统一的动作空间（CodeAct）是一种有前景的方法，用于开发实际应用中的LLM代理。然而，这种逐步代码生成的方法往往缺乏一致性和稳健性，导致代理应用的稳定性下降，特别是在复杂推理和跨领域任务中更为明显。本文提出了一种名为Tree-of-Code（ToC）的新型方法，通过端到端的机制来应对复杂问题规划和执行中的挑战。ToC通过整合来自Tree-of-Thought和CodeAct的关键思想，综合了它们的优势，增强了解决方案的探索。在我们的框架中，每个最终代码执行结果被视为决策树中的一个节点，采用广度优先搜索策略来探索潜在解决方案。最终结果通过节点输出的投票机制来确定。 

---
# Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis 

**Title (ZH)**: 通过集合基可达到性分析在层次强化学习中的目标空间抽象 

**Authors**: Mehdi Zadem, Sergio Mover, Sao Mai Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2309.07675)  

**Abstract**: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We evaluate our approach on complex navigation tasks, showing the learned representation is interpretable, transferrable and results in data efficient learning. 

**Abstract (ZH)**: 开放式学习极大地受益于符号方法在目标表示中的应用，因为这种方法提供了结构化知识以实现高效和可迁移学习的方式。然而，现有的依赖于符号推理的层次强化学习（HRL）方法常常受到限制，因为它们需要手动进行目标表示。自主发现符号目标表示的挑战在于，必须保留关键信息，如环境动力学。在本文中，我们提出了一种基于 emergent 代理的发育机制，通过抽象环境状态集（即，将具有相似角色的状态进行分组），实现目标发现。我们引入了一种封建 HRL 算法，该算法同时学习目标表示和层次策略。该算法利用符号可达性分析近似神经网络中的状态集之间的转换关系，并进一步完善目标表示。我们在复杂导航任务上评估了该方法，结果显示学习到的表示是可解释的、可迁移的，并且能够实现高效学习。 

---
# PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children 

**Title (ZH)**: PsyDraw：一种多agent多模态系统，用于留守儿童心理健康筛查 

**Authors**: Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14769)  

**Abstract**: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment. 

**Abstract (ZH)**: 留守儿童（Left-behind Children, LBCs）在中国人数超过6600万，由于父母外出务工，他们面临严重的心理健康挑战。早期筛查和识别处于高风险的留守儿童至关重要，但由于心理健康专业人员严重短缺，尤其是在农村地区，此项工作显得尤为困难。虽然宅-树-人（House-Tree-Person, HTP）测试显示出较高的儿童参与率，但需要专家解读的特性限制了其在资源稀缺地区的应用。为应对这一挑战，我们提出了PsyDraw，这是一种基于多模态大型语言模型的多智能体系统，旨在辅助心理健康专业人员分析HTP绘画。该系统采用专门的智能体进行特征提取和心理解读，并分为两个阶段：综合特征分析和专业报告生成。对290名小学生进行的HTP绘画评价结果显示，71.03%的分析达到了与专业评价的高水平一致性，26.21%达到中等一致性，仅有2.41%达到低一致性。系统识别出了31.03%需要专业关注的案例，显示出其作为初步筛查工具的有效性。目前已经在试点学校部署，该方法显示出在资源受限地区支持心理健康专业人员的潜力，同时在心理评估方面保持高水平的专业标准。 

---
# Agent-SafetyBench: Evaluating the Safety of LLM Agents 

**Title (ZH)**: Agent-SafetyBench: 评估大规模语言模型代理的安全性 

**Authors**: Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14470)  

**Abstract**: As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \url{this https URL} to facilitate further research and innovation in agent safety evaluation and improvement. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）越来越多地被用作代理，它们与互动环境的整合以及工具使用带来的新安全挑战超出了模型本身所固有的挑战。然而，缺乏全面的安全评估基准是有效评估和进一步改进的重大障碍。本文介绍了一个全面的基准Agent-SafetyBench，用于评估LLM代理的安全性。Agent-SafetyBench 包含349种互动环境和2,000个测试案例，评估了8类安全风险，并涵盖了10种常见失败模式，这些模式在不安全的互动中频繁出现。我们的评估显示了16种流行LLM代理中的一个令人担忧的结果：没有一个代理的得分超过60%。这突显了LLM代理中存在显著的安全挑战，并强调了改进的迫切需求。通过定量分析，我们确定了关键的失败模式，并总结了当前LLM代理中的两种基本安全检测：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅仅依赖防御提示是不足以解决这些安全问题的，强调了需要更先进和可靠的策略。我们已在 \url{this https URL} 发布了Agent-SafetyBench，以促进代理安全性评估和改进的研究和创新。 

---
# GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering 

**Title (ZH)**: GraphEQA：使用3D语义场景图进行实时嵌入式问答 

**Authors**: Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer  

**Link**: [PDF](https://arxiv.org/pdf/2412.14480)  

**Abstract**: In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. Through experiments in simulation on the HM-EQA dataset and in the real world in home and office environments, we demonstrate that our method outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. 

**Abstract (ZH)**: 在具身问答（EQA）中，代理必须探索和构建对未见过环境的语义理解，以自信地回答具身处境的问题。这一问题在机器人学中仍极具挑战性，主要由于获取有用语义表示的困难、在线更新这些表示的挑战以及利用先验世界知识进行高效探索和规划的难度。为了应对这些限制，我们提出了一种名为GraphEQA的新方法，该方法利用实时的3D度量语义场景图（3DSGs）和与任务相关的图像作为多模态记忆，将视觉语言模型（VLMs）锚定到未见过环境中的EQA任务上。我们采用了层次规划方法，利用3DSGs的层次结构进行结构化规划和语义导向探索。通过在HM-EQA数据集和家庭及办公环境中的实际世界实验，我们证明了我们的方法在完成EQA任务的成功率和规划步骤方面均优于关键基准方法。 

---
