# PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children 

**Title (ZH)**: PsyDraw：一种针对留守儿童心理健康筛查的多智能体多模态系统 

**Authors**: Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14769)  

**Abstract**: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment. 

**Abstract (ZH)**: 留守儿童（LBCs）在中国的人数超过6600万，由于父母因工作迁出，他们面临着严重的心理健康挑战。早期对风险留守儿童进行筛查和识别至关重要，但由于心理健康专业人士严重短缺，尤其是农村地区，这一任务变得极具挑战性。尽管房屋-树-人（HTP）测试显示出更高的儿童参与率，但它需要专家解读的特点限制了其在资源稀缺地区的应用。为了应对这一挑战，我们提出了一种基于多模态大型语言模型的多代理系统——PsyDraw，该系统可协助心理健康专业人士分析HTP绘画作品。该系统采用专门的代理进行特征提取和心理解读，分为两个阶段：全面特征分析和专业报告生成。对来自290名小学生HTP绘画作品的评估结果显示，71.03%的分析与专业评估达到了高一致性，26.21%达到了中等一致性，只有2.41%达到了低一致性。系统识别出了31.03%需要专业关注的案例，表明其作为初步筛查工具的有效性。目前该系统已在试点学校部署，其方法在支持心理健康专业人士，尤其是在资源有限的地区，同时保持高度的专业标准方面显示出巨大潜力。 

---
# Agent-SafetyBench: Evaluating the Safety of LLM Agents 

**Title (ZH)**: Agent-SafetyBench: 评估大规模语言模型代理的安全性 

**Authors**: Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14470)  

**Abstract**: As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \url{this https URL} to facilitate further research and innovation in agent safety evaluation and improvement. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）被越来越多地用作代理角色，它们在交互环境中的整合和工具使用引入了与模型本身相关的新安全挑战。然而，缺乏全面的评估标准来衡量代理安全性，这成为有效评估和进一步改进的重要障碍。本文介绍了一种名为Agent-SafetyBench的综合性基准，旨在评估LLM代理的安全性。Agent-SafetyBench涵盖了349种交互环境和2000个测试案例，评估了8类安全风险，并涵盖了10种常见的不安全交互中频繁出现的失败模式。我们的评估显示了16种流行LLM代理的结果：没有一个代理的安全评分超过60%。这一结果突出了LLM代理在安全性方面的重要挑战，并强调了改进的迫切需求。通过定量分析，我们识别了关键的失败模式，并总结了当前LLM代理中的两种基本安全检测：缺乏鲁棒性和缺乏风险意识。此外，我们的研究结果表明，仅仅依赖防御提示是不足以解决这些安全问题的，强调了需要采取更先进和可靠的策略。我们在此处发布Agent-SafetyBench（\url{this https URL}），以便进一步促进代理安全性评估和改进的研究与创新。 

---
# Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs 

**Title (ZH)**: Multilingual OphthaLingua：评估和去偏见LM在LMICs中眼科QA的多语言基准荏 

**Authors**: David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama  

**Link**: [PDF](https://arxiv.org/pdf/2412.14304)  

**Abstract**: Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe. 

**Abstract (ZH)**: 当前的眼科临床工作流程受到过度转诊、漫长等待时间和复杂异质性医疗记录的困扰。大规模语言模型（LLMs）有望自动化各种程序，如分诊、初步测试（如视力评估）和报告总结。然而，LLMs在自然语言问答任务中的性能在不同语言之间表现出显著差异，这可能加剧低收入和中等收入国家（LMICs）的医疗健康不平等。本研究引入了首个基于人工策源的多语言眼科问答基准，允许直接跨语言比较。我们对7种不同语言下的6种流行LLMs进行了评估，发现不同语言之间存在显著偏差，突显了在LMICs中临床部署LLMs时的风险。现有去偏方法，如翻译推理链或检索增强生成（RAG），单独使用时未能弥合这一性能差距，往往无法在所有语言中提高性能，缺乏针对医学领域的适应性。为此，我们提出了一种新颖的推理时间去偏方法——CLARA（跨语言反思性代理系统），该方法结合了检索增强生成和自我验证。我们的方法不仅提高了所有语言的性能，还显著减小了多语言偏差差距，从而促进全球范围内的LLM公平应用。 

---
# GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering 

**Title (ZH)**: GraphEQA：使用3D语义场景图进行实时嵌入式问答 

**Authors**: Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer  

**Link**: [PDF](https://arxiv.org/pdf/2412.14480)  

**Abstract**: In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. Through experiments in simulation on the HM-EQA dataset and in the real world in home and office environments, we demonstrate that our method outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps. 

**Abstract (ZH)**: 在具身问答（EQA）中，代理需要探索并培养对不可见环境的语义理解，以便自信地回答定位问题。由于获得有用语义表示、在线更新这些表示以及利用先验世界知识进行高效探索和规划的困难，这仍然是机器人领域的一个具有挑战性的问题。为了解决这些限制，我们提出了一种名为GraphEQA的新颖方法，该方法利用实时3D度量语义场景图（3DSGs）和与任务相关的图像作为多模态记忆，将视觉-语言模型（VLMs）接地以在不可见环境中执行EQA任务。我们采用了一种基于层次规划的方法，利用3DSGs的层次结构特性进行结构化规划和语义引导的探索。通过在HM-EQA数据集上进行模拟实验以及在家和办公室环境中的真实世界实验，我们展示了我们的方法通过更高的完成EQA任务的成功率和更少的规划步骤优于关键基准方法。 

---
# Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines 

**Title (ZH)**: Bel Esprit：多agent框架构建AI模型流水线 

**Authors**: Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf  

**Link**: [PDF](https://arxiv.org/pdf/2412.14684)  

**Abstract**: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at this https URL. 

**Abstract (ZH)**: 随着对人工智能（AI）需求的增长，以解决复杂的现实世界任务，单一模型往往不足以应对，因此需要将多个模型整合到管道中。本文介绍了一种名为Bel Esprit的对话型代理，它旨在根据用户定义的要求构建AI模型管道。Bel Esprit采用多代理框架，其中子代理协作以澄清要求、构建、验证并用合适的模型填充管道。我们通过使用人类精选和合成数据展示了该框架在从模糊用户查询中生成管道方面的有效性。详细的错误分析指出了管道构建过程中仍存在的挑战。Bel Esprit可以在以下网址免费试用：[此处填写网址]。 

---
# The Digital Ecosystem of Beliefs: does evolution favour AI over humans? 

**Title (ZH)**: 信念数字生态系统：人工智能会取代人类吗？ 

**Authors**: David M. Bossens, Shanshan Feng, Yew-Soon Ong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14500)  

**Abstract**: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on this http URL understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles. 

**Abstract (ZH)**: 随着人工智能系统被整合到社交网络中，人们开始关注AI安全问题，例如AI生成的内容可能在受欢迎程度或影响力上占据主导地位。为进一步理解和探讨这些问题，本文提出了数字信念生态系统（Digico）框架，这是首个用于在模拟社交网络中进行多人群体交互受控实验的演化框架。该框架通过普遍达尔文主义方法模拟了一种群体实体，这些实体由于进化更新而调整其消息策略，通过消息进行交互，在基于传染病模型的动力学基础上互相影响信念，并通过认知拉马克式遗传维持其信念。

初步实验表明：a) 当AI的信息传输、进化速度更快，并在推荐算法中具有更大影响力时，它们可以获得80%到95%的浏览量，具体比例取决于影响力收益的大小；b) 设计用于宣传的AI通常可以说服50%的人类采用极端信念，当代理只相信有限数量的渠道时，这个比例可以达到85%；c) 对违反代理信念的内容进行惩罚可使宣传活动的有效性降低8%。

此外，本文还讨论了Digico框架对于控制（例如立法）的影响及其作为研究进化原则手段的作用。 

---
# Python Agent in Ludii 

**Title (ZH)**: Python代理在Ludii中的应用 

**Authors**: Izaias S. de Lima Neto, Marco A. A. de Aguiar Vieira, Anderson R. Tavares  

**Link**: [PDF](https://arxiv.org/pdf/2412.14372)  

**Abstract**: Ludii is a Java general game system with a considerable number of board games, with an API for developing new agents and a game description language to create new games. To improve versatility and ease development, we provide Python interfaces for agent programming. This allows the use of Python modules to implement general game playing agents.
As a means of enabling Python for creating Ludii agents, the interfaces are implemented using different Java libraries: jpy and Py4J. The main goal of this work is to determine which version is faster. To do so, we conducted a performance analysis of two different GGP algorithms, Minimax adapted to GGP and MCTS. The analysis was performed across several combinatorial games with varying depth, branching factor, and ply time. For reproducibility, we provide tutorials and repositories.
Our analysis includes predictive models using regression, which suggest that jpy is faster than Py4J, however slower than a native Java Ludii agent, as expected. 

**Abstract (ZH)**: Ludii 是一个基于 Java 的通用游戏系统，包含大量的棋盘游戏，并提供开发新代理的 API 以及创建新游戏的游戏描述语言。为了提高通用性和简化开发过程，我们提供了 Python 接口以便使用 Python 编程实现代理。这样可以利用 Python 模块来实现通用游戏代理。

为了使 Python 能够用于创建 Ludii 代理，接口是通过不同的 Java 库 jpy 和 Py4J 实现的。本工作的主要目标是确定哪个版本更快速。为此，我们对两种不同的广义游戏协议 (General Game Playing, GGP) 算法（适配 GGP 的 Minimax 算法和 Monte Carlo Tree Search, MCTS）进行了性能分析。分析在多个具有不同深度、分支因子和回合时间的组合游戏中进行。为了可再现性，我们提供了教程和仓库。

我们的分析包括使用回归建立的预测模型，这些模型表明 jpy 比 Py4J 快，但比原生 Java Ludii 代理慢，这在预期之中。 

---
# Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration 

**Title (ZH)**: 人类-类人机器人异体行为-技能转移研究：基于分解对抗学习的演示学习 

**Authors**: Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15166)  

**Abstract**: Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms. 

**Abstract (ZH)**: 拟人机器人被设想为具备执行广泛的人类级移动操作任务的体现智能代理，特别是在需要繁重和重复劳动的场景中尤为适用。然而，由于拟人机器人具有高自由度，学习这些技能具有挑战性，而收集充分的训练数据对拟人机器人而言也是一个耗时的过程。鉴于新的拟人平台快速涌现，一种允许通用技能迁移的跨体态框架变得越来越关键。为解决这一问题，我们提出了一种可迁移的框架，通过使用统一的数字人体模型作为通用原型，从而减少数据瓶颈，避免每在新平台重新训练的需要。该模型通过对抗模仿学习从人类示范中学习行为素，复杂的机器人结构被分解为功能组件，各组件独立训练并动态协调。通过人力物体交互图实现任务泛化，并通过特定于体态的动力学运动目标重定位和动态微调将技能迁移到不同的机器人。该框架在五种具有差异配置的拟人机器人上得到了验证，展示了稳定的移动操作，并突出了其在减少数据需求和跨平台提高技能迁移效率方面的有效性。 

---
# Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents 

**Title (ZH)**: 将罗尔斯伦理学应用于规范学习代理的公平性操作化 

**Authors**: Jessica Woodgate, Paul Marshall, Nirav Ajmeri  

**Link**: [PDF](https://arxiv.org/pdf/2412.15163)  

**Abstract**: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics. 

**Abstract (ZH)**: 社会规范是指一个社会中普遍认同的行为标准。然而，当个体在做决策时不考虑其行为对他人的影响时，可能会形成一些压迫某些个体的行为规范。我们提出了一种名为RAWL-E的方法，旨在创建具备伦理规范学习能力的智能体。RAWL-E智能体在其决策过程中通过实现罗尔斯正义伦理学中的“最大化最小者”原则（maximin），在社会福祉与个体目标之间寻求平衡，从而促进伦理规范的形成。我们通过模拟收获场景对RAWL-E智能体进行了评估。研究结果表明，RAWL-E智能体社会中形成的规范提升了社会福利、公平性和稳健性，并且相较于未实施罗尔斯正义伦理的智能体社会，能获得更高的最低经验收益。 

---
# Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning 

**Title (ZH)**: 基于代理-时间信用分配的稀疏多代理强化学习中优化策略保留方法 

**Authors**: Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht  

**Link**: [PDF](https://arxiv.org/pdf/2412.14779)  

**Abstract**: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods. 

**Abstract (ZH)**: 在多智能体环境中，智能体常常难以学习最优策略，尤其是在稀疏或延迟的全局奖励以及长期任务中，评估中间时间步骤的动作尤为困难。为此，我们提出了时间智能体奖励再分配（Temporal-Agent Reward Redistribution，简称TAR$^2$），这是一种旨在通过在时间和智能体之间重分配稀疏奖励来解决智能体-时间信用分配问题的新型方法。TAR$^2$将稀疏的全局奖励分解为时间步长特定的奖励，并计算每个智能体对这些奖励的贡献。我们从理论上证明了TAR$^2$等同于基于势的奖励塑形，确保最优策略不发生变化。实验结果表明，TAR$^2$能够稳定并加速学习过程。此外，我们还展示了在将TAR$^2$与单智能体强化学习算法结合时，其性能至少与传统多智能体强化学习方法相当或更优。 

---
# DriveGPT: Scaling Autoregressive Behavior Models for Driving 

**Title (ZH)**: DriveGPT：扩展自回归行为模型在自动驾驶中的应用 

**Authors**: Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14415)  

**Abstract**: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling. 

**Abstract (ZH)**: 我们将介绍DriveGPT，这是一种可扩展的自动驾驶行为模型。我们将驾驶视为一个序列决策任务，并利用变换器模型以自回归的方式预测未来代理状态。通过大幅提升模型参数和训练数据的数量级，我们能够从数据集规模、模型参数和计算资源方面探索模型的可扩展性。我们通过规划任务中的定量指标和定性案例，包括在复杂真实世界场景中的闭环驾驶，评估了DriveGPT在不同规模下的性能。在一项单独的预测任务中，DriveGPT超越了最先进的基线模型，并通过在大规模数据集上进行预训练展示了性能的提升，进一步验证了数据规模对于模型性能的益处。 

---
# Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference 

**Title (ZH)**: 面向大规模实时强化学习的交错异步推理方法 

**Authors**: Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish  

**Link**: [PDF](https://arxiv.org/pdf/2412.14355)  

**Abstract**: Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pokémon and Tetris. 

**Abstract (ZH)**: 实时环境在智能体进行动作推理和学习的过程中不断变化，因此需要高频率的交互来有效地最小化遗憾。然而，近期的机器学习进展涉及更大的神经网络和更长的推理时间，这在需要快速反应的实时系统中引发了其适用性的问题。我们对实时强化学习（RL）环境中的后悔下界进行分析，表明在典型的顺序交互和学习范式中一般不可能完全最小化长期后悔，但在计算过程足够异步时，这种情况往往变得可能。我们提出了新颖的算法，用于调度异步推理过程，以确保动作在固定的时间间隔内被采取。我们的研究表明，当从实时模拟游戏男孩游戏（如宠物小精灵和俄罗斯方块）的过程中学习时，所需的推理过程数量随推理时间的增长线性增加，同时能够使用比现有方法大多个数量级的模型，并且这种模型的动作推理时间仅受限于推理窗口内的环境有效随机性，而非动作频率。 

---
# Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs 

**Title (ZH)**: 异构多代理强化学习在无线局域网中分布式频道访问中的应用 

**Authors**: Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14218)  

**Abstract**: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents. 

**Abstract (ZH)**: 本文研究了多智能体强化学习（MARL）在无线局域网分布式信道访问中的应用。具体而言，我们考虑了更加实际但更具挑战性的情况，即智能体异构地采用基于值的或基于策略的强化学习算法进行模型训练。我们提出了一种异构MARL训练框架，命名为QPMIX，该框架采用集中式训练与分布式执行范式，使异构智能体能够协同工作。此外，我们理论证明了当使用线性值函数近似时，所提出的异构MARL方法的收敛性。该方法能够最大化网络吞吐量并确保各站之间的公平性，从而提高整体网络性能。仿真结果表明，在饱和交通场景下，与常规的载波侦听多路访问/冲突避免（CSMA/CA）相比，所提出的QPMIX算法提高了吞吐量、平均延迟、延迟抖动和碰撞率。此外，在非饱和和延迟敏感的交通场景下，QPMIX表现出较强的鲁棒性，并促进了异构智能体之间的合作。 

---
# Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle 

**Title (ZH)**: 生成型AI工具包——一种在整个生命周期中提高基于LLM的应用质量的框架 

**Authors**: Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14215)  

**Abstract**: As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve 

**Abstract (ZH)**: 随着基于大型语言模型（LLM）的应用程序达到数百万客户，确保其可扩展性和持续的质量改进对于其成功至关重要。然而，当前开发、维护和运行（DevOps）这些应用的工作流程主要依赖手工操作，速度缓慢，并且基于试错法。在本文中，我们介绍了生成式AI工具包，该工具包在整个基于LLM的应用程序生命周期中自动化了关键工作流程。此工具包有助于配置、测试、持续监控和优化生成式AI应用，如代理，从而显著提高质量并缩短发布周期。我们通过代表性的用例展示了我们工具包的有效性，分享了最佳实践，并概述了未来增强计划。由于我们相信我们的生成式AI工具包对其他团队也有帮助，我们决定将其开源，并希望其他人能够使用、分享、适应并改进它。 

---
# Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis 

**Title (ZH)**: 通过集合基可达性分析在层次强化学习中的目标空间抽象 

**Authors**: Mehdi Zadem, Sergio Mover, Sao Mai Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2309.07675)  

**Abstract**: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We evaluate our approach on complex navigation tasks, showing the learned representation is interpretable, transferrable and results in data efficient learning. 

**Abstract (ZH)**: 开放学习极大地受益于符号方法在目标表示方面的应用，因为这些方法能够以高效且可迁移的方式结构化知识。然而，现有的依赖于符号推理的层次强化学习（Hierarchical Reinforcement Learning, HRL）方法通常受到限制，因为它们需要手动指定目标表示。自主发现符号目标表示的挑战在于，这种表示必须保留关键信息，例如环境动力学。在本文中，我们提出了一种发展性的机制，通过一个 emergent（萌发的）表示来发现目标，该表示将具有相似作用的任务环境状态集进行抽象化（即，将具有相似作用的状态集聚类）。我们提出了一种封建层次强化学习算法，该算法可以同时学习目标表示和层次策略。该算法利用符号可达性分析来近似神经网络中的状态集间的转移关系，并据此细化目标表示。我们在复杂的导航任务上评估了这种方法，结果显示学习到的表示是可解释的、可迁移的，并且能够实现数据高效学习。 

---
