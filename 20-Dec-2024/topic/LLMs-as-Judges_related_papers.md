# LLMs as mediators: Can they diagnose conflicts accurately? 

**Title (ZH)**: LLMs作为调解者：它们能否准确诊断冲突？ 

**Authors**: Özgecan Koçak, Phanish Puranam, Afşar Yegin  

**Link**: [PDF](https://arxiv.org/pdf/2412.14675)  

**Abstract**: Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Koçak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes. 

**Abstract (ZH)**: 先前的研究表明，观察者要在双方争端中充当调解人，必须能够可靠地区分他们分歧的来源是基于对事实的理解差异（因果性）还是基于价值观的差异（道德性）。本研究旨在测试OpenAI的大语言模型GPT 3.5和GPT 4是否能够完成这一任务，并且探究是哪种类型的分歧对大语言模型来说更加具有挑战性。我们复制了Koçak等人（2003）的研究，该研究采用了情景描述的设计，使用了OpenAI的GPT 3.5和GPT 4进行实验。我们发现，这两种大语言模型在区分因果性和道德性方面的语义理解与人类相似，并且能够可靠地区分这两者。

当被要求诊断对话中的分歧来源时，与人类相比，这两种大语言模型在道德不一致的情况下都有倾向于高估因果性分歧并低估道德性分歧的倾向。特别是使用与具体问题相关的具体语言构建的邻近尺度时，这种倾向对于GPT 4尤为显著。GPT 3.5在使用邻近尺度和远端尺度时的表现都不及GPT 4或人类。该研究提供了大语言模型通过诊断因果性和评价性代码的分歧根源来调解冲突的潜在能力的初步测试。 

---
# Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning 

**Title (ZH)**: 超越罪责：基于三元推理的法律判决预测 

**Authors**: Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14588)  

**Abstract**: In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict. 

**Abstract (ZH)**: 在法律实践中，法官根据刑事法律的三元理论，依次评估犯罪行为的构成要素、非法性和故意性，以确定某人的行为是否构成犯罪。尽管现有的法律大型语言模型（LLMs）在判决预测方面展现出令人鼓舞的准确性，但由于缺乏适当的基准数据集，它们缺乏三元推理能力，因此无法预测无罪判决。因此，每一个输入都被自动分配一个罪名，限制了它们在法律环境中的实际应用价值。为了解决这一问题，我们提出了LJPIV，即首个包含无罪判决的法律判决预测基准数据集。根据三元理论，我们通过基于LLM的数据扩增和人工验证，扩展了三个广泛使用的法律数据集。我们的实验使用最新的法律LLMs和新的策略，这些策略将三元推理集成到零样本提示和微调中，结果显示：（1）当前的法律LLMs有着显著的改进空间，即使是最佳模型在LJPIV上的F1分数也低于0.3；（2）我们的策略显著提高了领域内和跨领域的判决预测准确性，特别是在预测无罪判决的案件中效果尤为明显。 

---
