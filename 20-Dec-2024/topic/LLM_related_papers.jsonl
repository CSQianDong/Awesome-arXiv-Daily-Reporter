{'arxiv_id': 'arXiv:2412.15093', 'title': 'Nano-ESG: Extracting Corporate Sustainability Information from News Articles', 'authors': 'Fabian Billert, Stefan Conrad', 'link': 'https://arxiv.org/abs/2412.15093', 'abstract': 'Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.\nAn independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at this https URL.', 'abstract_zh': '确定企业的可持续性影响是一个高度复杂的主题，近年来引起了越来越多的关注。如今，投资者主要依赖于评级机构提供的可持续性评级，以分析公司在多大程度上负责任地行动。然而，这些评级最近因难以理解且几乎不可能重现而受到批评。\n\n在企业的可持续性实践方面，新闻文章数据丰富的领域提供了一种独立的方法。本文探讨了一种不同的方法，以识别可持续性领域中公司面临的关键机会和挑战。我们呈现了一个包含超过840,000篇新闻文章的新数据集，这些文章是为2023年1月至2024年9月期间的主要德国公司收集的。通过结合自然语言处理技术，我们首先识别出相关文章，然后对其进行总结，并使用大型语言模型（LLM）提取其可持续性相关的观点和方面。此外，我们对获得的数据进行了评估，并确定LLM生成的答案是准确的。我们在此处发布两个数据集：[请插入网址]。', 'title_zh': 'Nano-ESG：从新闻文章中提取企业可持续性信息'}
{'arxiv_id': 'arXiv:2412.14454', 'title': 'Are Longer Prompts Always Better? Prompt Selection in Large Language Models for Recommendation Systems', 'authors': 'Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka', 'link': 'https://arxiv.org/abs/2412.14454', 'abstract': 'In large language models (LLM)-based recommendation systems (LLM-RSs), accurately predicting user preferences by leveraging the general knowledge of LLMs is possible without requiring extensive training data. By converting recommendation tasks into natural language inputs called prompts, LLM-RSs can efficiently solve issues that have been difficult to address due to data scarcity but are crucial in applications such as cold-start and cross-domain problems. However, when applying this in practice, selecting the prompt that matches tasks and data is essential. Although numerous prompts have been proposed in LLM-RSs and representing the target user in prompts significantly impacts recommendation accuracy, there are still no clear guidelines for selecting specific prompts.\nIn this paper, we categorize and analyze prompts from previous research to establish practical prompt selection guidelines. Through 450 experiments with 90 prompts and five real-world datasets, we examined the relationship between prompts and dataset characteristics in recommendation accuracy. We found that no single prompt consistently outperforms others; thus, selecting prompts on the basis of dataset characteristics is crucial. Here, we propose a prompt selection method that achieves higher accuracy with minimal validation data. Because increasing the number of prompts to explore raises costs, we also introduce a cost-efficient strategy using high-performance and cost-efficient LLMs, significantly reducing exploration costs while maintaining high prediction accuracy. Our work offers valuable insights into the prompt selection, advancing accurate and efficient LLM-RSs.', 'abstract_zh': '在基于大规模语言模型（LLM）的推荐系统（LLM-RSs）中，通过利用LLM的普遍知识，无需大量训练数据即可准确预测用户偏好。通过将推荐任务转化为自然语言输入（称为提示），LLM-RSs可以高效地解决由于数据稀缺而难以处理但在冷启动和跨域问题等应用中至关重要的问题。然而，在实际应用中，选择与任务和数据匹配的提示至关重要。尽管在LLM-RSs中已经提出了许多提示，且提示中的目标用户的表示对推荐准确性产生了显著影响，但仍缺乏明确的提示选择指南。\n\n本文对先前研究中的提示进行分类和分析，以确立实用的提示选择指南。通过使用90个提示和五个真实世界数据集进行450次实验，我们考察了提示与数据集特性之间的关系在推荐准确性方面的影响。我们发现没有一种提示能够始终优于其他提示，因此，基于数据集特性选择提示至关重要。在此基础上，我们提出了一种提示选择方法，仅需少量验证数据即可实现更高的准确性。由于增加提示数量以进行探索会增加成本，我们还介绍了一种成本有效的策略，使用高性能且成本效益高的LLM，显著降低了探索成本同时保持了高预测准确性。我们的工作为提示选择提供了宝贵的见解，推动了准确高效的LLM-RSs的发展。', 'title_zh': '较长的提示是否总是更好？大型语言模型在推荐系统中选择提示的研究'}
{'arxiv_id': 'arXiv:2412.14405', 'title': 'ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers', 'authors': 'Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang', 'link': 'https://arxiv.org/abs/2412.14405', 'abstract': "Large language models (LLMs) have demonstrated remarkable effectiveness in text reranking through works like RankGPT, leveraging their human-like reasoning about relevance. However, supervised fine-tuning for ranking often diminishes these models' general-purpose capabilities, including the crucial reasoning abilities that make them valuable for ranking. We introduce a novel approach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised Fine-Tuning followed by Direct Preference Optimization) pipeline to preserve these capabilities while improving ranking performance. Our experiments on TREC 2019 and 2020 Deep Learning datasets show that our approach outperforms the state-of-the-art RankZephyr while maintaining strong performance on the Massive Multitask Language Understanding (MMLU) benchmark, demonstrating effective preservation of general-purpose capabilities through thoughtful fine-tuning strategies. Our code and data will be publicly released upon the acceptance of the paper.", 'abstract_zh': '大规模语言模型（LLMs）已在通过RankGPT等作品中的文本重排序任务中展示了令人瞩目的有效性，这主要得益于它们在相关性上的类似人类的推理能力。然而，监督微调排序通常会削弱这些模型的通用能力，包括使它们在排序任务中具有价值的关键推理能力。我们提出了一种新的方法，将链式思维提示与SFT-DPO（监督微调后直接偏好优化）管道相结合，以保留这些能力并提高排序性能。我们在TREC 2019和2020年深度学习数据集上的实验表明，我们的方法在保持强劲性能的同时超越了最先进的RankZephyr方法，证明了通过谨慎的微调策略有效保持了通用能力。我们的代码和数据将在论文被接受后公开发布。', 'title_zh': 'ChainRank-DPO：链式排名直接偏好优化方法用于LLM排名器'}
{'arxiv_id': 'arXiv:2412.14835', 'title': 'Progressive Multimodal Reasoning via Active Retrieval', 'authors': 'Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2412.14835', 'abstract': 'Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.', 'abstract_zh': '多步多模态推理任务对多模态大语言模型（MLLMs）提出了显著的挑战，如何在这种情境下有效提升其性能仍然是一个未解决的问题。本文提出了一种名为AR-MCTS的通用框架，旨在通过主动检索（AR）和蒙特卡洛树搜索（MCTS）逐步提高MLLMs的推理能力。我们的方法首先开发了一个统一的检索模块，从混合模态检索语料库中检索解决复杂推理问题的关键支持见解。为了解决自动多模态推理验证中的差距，我们采用了结合主动检索机制的MCTS算法，这使得可以自动生成逐步标注。该策略动态检索每个推理步骤的关键见解，超越了传统的束搜索采样方法，以提高推理空间的多样性和可靠性。此外，我们引入了一个过程奖励模型，以逐步支持多模态推理任务的自动验证。在三个复杂的多模态推理基准上的实验结果证实了AR-MCTS框架在提升各种多模态模型性能方面的有效性。进一步的分析表明，AR-MCTS能够优化采样多样性和准确性，从而实现可靠的多模态推理。', 'title_zh': '基于主动检索的 progressive 多模态推理'}
{'arxiv_id': 'arXiv:2412.14486', 'title': 'Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling Techniques for Qualitative Data Analysis of Online Communities', 'authors': 'Amandeep Kaur, James R. Wallace', 'link': 'https://arxiv.org/abs/2412.14486', 'abstract': "Social media constitutes a rich and influential source of information for qualitative researchers. Although computational techniques like topic modelling assist with managing the volume and diversity of social media content, qualitative researcher's lack of programming expertise creates a significant barrier to their adoption. In this paper we explore how BERTopic, an advanced Large Language Model (LLM)-based topic modelling technique, can support qualitative data analysis of social media. We conducted interviews and hands-on evaluations in which qualitative researchers compared topics from three modelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12 participants for its ability to provide detailed, coherent clusters for deeper understanding and actionable insights. Participants also prioritised topic relevance, logical organisation, and the capacity to reveal unexpected relationships within the data. Our findings underscore the potential of LLM-based techniques for supporting qualitative analysis.", 'abstract_zh': '社会媒体构成了丰富且有影响力的定性研究信息来源。尽管像主题建模这样的计算技术有助于处理社会媒体内容的庞大和多样性，但定性研究者的编程技能缺乏却构成了其采用的重要障碍。本文探讨了如何通过基于大型语言模型（LLM）的主题建模技术BERTopic来支持社会媒体的定性数据分析。我们进行了访谈和实际操作评估，让定性研究人员将三种建模技术——LDA、NMF 和 BERTopic——产生的主题进行了比较。共有12名参与者中有8人青睐BERTopic，因为它能够提供详细、连贯的主题簇以加深理解并提取实质性见解。参与者还强调了主题的相关性、逻辑组织以及揭示数据中意想不到的关系的能力。我们的研究结果表明，基于大型语言模型的技术有可能支持定性分析。', 'title_zh': '超越LDA：无监督主题建模技术在在线社区定性数据分析中的比较'}
{'arxiv_id': 'arXiv:2412.14146', 'title': 'Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models', 'authors': 'Atin Sakkeer Hussain', 'link': 'https://arxiv.org/abs/2412.14146', 'abstract': 'This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics.', 'abstract_zh': '本文介绍了高级推理与转换引擎（ARTEMIS-DA，Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics），这是一种新型框架，旨在增强大型语言模型（LLMs），以解决复杂的多步骤数据解析任务。ARTEMIS-DA 融合了三个核心组件：规划器（Planner），它将复杂的用户查询分解成结构化、顺序的指令，涵盖数据预处理、转换、预测建模和可视化；编码器（Coder），它动态生成并执行 Python 代码以实现这些指令；以及绘图器（Grapher），它解释生成的可视化结果以推导出可操作的洞察。通过协调这些组件之间的协作，ARTEMIS-DA 有效地管理了涉及高级推理、多步骤转换和跨多种数据模态综合的复杂分析工作流。该框架在 WikiTableQuestions 和 TabFact 等基准测试中达到了最先进的（SOTA）性能，展示了其在精确性和适应性方面处理复杂分析任务的能力。通过结合大型语言模型的推理能力、自动化代码生成和执行以及可视化分析，ARTEMIS-DA 提供了一种稳健且可扩展的多步骤洞察综合解决方案，能够应对数据解析的广泛挑战。', 'title_zh': '基于大型语言模型的数据分析中多步洞察综合的高级推理与转换引擎'}
{'arxiv_id': 'arXiv:2412.13432', 'title': 'Large Language Model Enhanced Recommender Systems: Taxonomy, Trend, Application and Future', 'authors': 'Qidong Liu, Xiangyu Zhao, Yuhao Wang, Yejing Wang, Zijian Zhang, Yuqi Sun, Xiang Li, Maolin Wang, Pengyue Jia, Chong Chen, Wei Huang, Feng Tian', 'link': 'https://arxiv.org/abs/2412.13432', 'abstract': 'Large Language Model (LLM) has transformative potential in various domains, including recommender systems (RS). There have been a handful of research that focuses on empowering the RS by LLM. However, previous efforts mainly focus on LLM as RS, which may face the challenge of intolerant inference costs by LLM. Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender Systems (LLMERS), has garnered significant interest due to its potential to address latency and memory constraints in real-world applications. This paper presents a comprehensive survey of the latest research efforts aimed at leveraging LLM to enhance RS capabilities. We identify a critical shift in the field with the move towards incorporating LLM into the online system, notably by avoiding their use during inference. Our survey categorizes the existing LLMERS approaches into three primary types based on the component of the RS model being augmented: Knowledge Enhancement, Interaction Enhancement, and Model Enhancement. We provide an in-depth analysis of each category, discussing the methodologies, challenges, and contributions of recent studies. Furthermore, we highlight several promising research directions that could further advance the field of LLMERS.', 'abstract_zh': '大型语言模型（LLM）在各个领域具有变革性的潜力，包括推荐系统（RS）。已有少数研究专注于通过LLM增强RS。然而，以往的努力主要集中在将LLM作为RS，这可能会面临LLM无法承受的推理成本挑战。最近，将LLM集成到RS中的做法，被称为LLM增强推荐系统（LLMERS），由于其在解决实际应用中的延迟和内存约束方面的潜力，引起了广泛关注。本文对旨在利用LLM增强RS能力的最新研究进行了全面综述。我们发现领域内存在一个关键的转变，即转向将LLM纳入在线系统，特别是在推理过程中避免使用LLM。我们的综述将现有的LLMERS方法按RS模型增强的组件划分为三种主要类型：知识增强、交互增强和模型增强。我们对每种类型进行了深入分析，讨论了最近研究的方法、挑战和贡献。此外，我们还强调了一些有前景的研究方向，这些方向有助于进一步推动LLMERS领域的发展。', 'title_zh': '大型语言模型增强的推荐系统：分类、趋势、应用与未来'}
{'arxiv_id': 'arXiv:2412.15177', 'title': 'Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying', 'authors': 'Federico Castagna, Isabel Sassoon, Simon Parsons', 'link': 'https://arxiv.org/abs/2412.15177', 'abstract': "Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.", 'abstract_zh': '研究表明，尽管近期在人工智能研究领域取得突破性进展，最先进的大型语言模型（LLMs）在进行逻辑和数学推理时仍然面临挑战。这些结果表明，LLMs 在处理之前未见过的推理问题或与其训练数据样本相差较大的问题时，往往表现不佳，更像是高阶的数据模式识别器。为应对这一重要关切，本文借鉴了论证理论文献中的关键问题概念，特别关注托马斯·考林（Toulmin）的论证模型。我们表明，使用这些关键问题可以提升LLMs 的推理能力。通过对模型推理过程的深入探查，LLMs 可以评估是否存在逻辑错误，并在给出最终答复之前进行修正。这一理念源自任何有效论证方法的基本准则：结论有效，若其被公认的前提所充分支持。或者，借鉴亚里士多德原则在现实世界的近似表达，即在信息不完整和预设逻辑的情境下，结论的有效性需要通过其他证明方式来否定。此方法成功引导模型的输出通过一个推理管道，从而实现比基线及其链条推理（CoT）实现更好的性能。为此，本文提供了在MT-Bench推理和数学任务上，针对多种LLMs 进行的广泛评估。', 'title_zh': '批判性思考问题：通过论辩性查询引导大规模语言模型的推理'}
{'arxiv_id': 'arXiv:2412.14814', 'title': 'Answer Set Networks: Casting Answer Set Programming into Deep Learning', 'authors': 'Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting', 'link': 'https://arxiv.org/abs/2412.14814', 'abstract': 'Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU\'s batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the "constitutional navigation" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.', 'abstract_zh': '尽管回答集编程（ASP）能够约束神经符号（NeSy）系统，但其应用受到计算稳定模型成本高昂以及先进求解器以CPU为中心的特性的阻碍。为解决这一问题，我们提出了一种NeSy求解器——回答集网络（ASN），它是基于图神经网络（GNN）的ASP基深度概率逻辑编程（DPPL）的可扩展方法。具体来说，我们展示了如何将ASP转换为ASN，并展示了通过利用GPU的批量处理和并行化能力，ASN如何高效地求解编码问题。我们的实验评估表明，ASN在多个任务上优于现有的CPU密集型NeSy系统。同时，我们基于ASN的优点，做出了以下两个贡献：首先，我们首次展示了使用DPPL微调大型语言模型（LLM），并使用ASN以逻辑引导训练；其次，我们展示了无人机的“宪法导航”，即将公共航空法规编码到ASN中，用于导航无人驾驶航空器在不确定环境中的路径规划。', 'title_zh': '答案集网络：将回答集编程纳入深度学习'}
{'arxiv_id': 'arXiv:2412.14492', 'title': 'FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis', 'authors': 'Abdullah Khan, Rahul Nahar, Hao Chen, Gonzalo E. Constante Flores, Can Li', 'link': 'https://arxiv.org/abs/2412.14492', 'abstract': "Machine learning algorithms are increasingly being applied to fault detection and diagnosis (FDD) in chemical processes. However, existing data-driven FDD platforms often lack interpretability for process operators and struggle to identify root causes of previously unseen faults. This paper presents FaultExplainer, an interactive tool designed to improve fault detection, diagnosis, and explanation in the Tennessee Eastman Process (TEP). FaultExplainer integrates real-time sensor data visualization, Principal Component Analysis (PCA)-based fault detection, and identification of top contributing variables within an interactive user interface powered by large language models (LLMs). We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not to mimic the challenge of previously unseen faults. Experimental results using GPT-4o and o1-preview models demonstrate the system's strengths in generating plausible and actionable explanations, while also highlighting its limitations, including reliance on PCA-selected features and occasional hallucinations.", 'abstract_zh': '机器学习算法在化工过程故障检测与诊断（FDD）中的应用越来越广泛。然而，现有的基于数据驱动的FDD平台往往缺乏对工艺操作人员的解释性，并且难以识别之前未见过故障的根本原因。本文介绍了一种名为FaultExplainer的交互式工具，旨在提高Tennessee Eastman Process（TEP）的故障检测、诊断和解释能力。FaultExplainer结合了实时传感器数据可视化、基于主成分分析（PCA）的故障检测以及大型语言模型（LLMs）支持的交互式用户界面，以识别贡献最大的变量。我们在两种场景下评估了LLMs的推理能力：一种场景是有历史根本原因提供，另一种场景是没有提供以模拟之前未见过故障的挑战。使用GPT-4o和o1-preview模型的实验结果表明，该系统在生成合理和可操作的解释方面具有优势，但同时也展示了其局限性，包括对PCA选择特征的依赖以及偶尔的幻觉现象。', 'title_zh': '故障解释器：利用大语言模型进行可解释的故障检测与诊断'}
{'arxiv_id': 'arXiv:2412.15204', 'title': 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks', 'authors': 'Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li', 'link': 'https://arxiv.org/abs/2412.15204', 'abstract': 'This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at this https URL.', 'abstract_zh': '本文介绍了一种名为LongBench v2的新基准，该基准旨在评估大规模语言模型（LLM）处理长上下文问题的能力，这些问题是需要跨越多个真实世界任务进行深刻理解和推理的问题。LongBench v2包含503个具有挑战性的多项选择题，上下文范围从8000词到200万词，涵盖了六个主要任务类别：单文档问答、多文档问答、长上下文学习、长对话历史理解、代码仓库理解和长结构化数据理解。为了确保广度和实用性，我们从近100名背景多样的受过高等教育的个人中收集数据。我们采用了自动化和手动审查的双重过程来维护高质量和难度，结果表明，在15分钟的时间限制下，人工专家只能达到53.7%的准确率。评估结果显示，最佳模型直接回答问题的准确率为50.1%。相比之下，包括更长推理的o1-preview模型的准确率为57.7%，超过了人工基线4个百分点。这些结果突显了增强推理能力和扩大推理时计算量在LongBench v2的长上下文挑战中的重要性。该项目可在此链接中访问：[项目链接]。', 'title_zh': '长场景v2：朝着对真实长语境多任务有更深的理解和推理发展'}
{'arxiv_id': 'arXiv:2412.15188', 'title': 'LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation', 'authors': 'Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu', 'link': 'https://arxiv.org/abs/2412.15188', 'abstract': "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.", 'abstract_zh': '我们提出了一种名为LlamaFusion的框架，该框架能够赋予仅含文本训练的大型语言模型（LLM）跨模态生成能力，使它们能够理解和生成任意序列的文本和图像。LlamaFusion利用Llama-3现有的权重进行自回归文本处理，同时引入额外的并行变换器模块来处理带有扩散的图像。在训练过程中，来自每种模态的数据会被定向到其专门的模块：模态特异的前馈层、键-值投影和标准化层分别独立处理每种模态，而共享的自注意力层则允许文本和图像特征之间的交互。通过冻结文本特异的模块并仅训练图像特异的模块，LlamaFusion保留了文本仅LMM的语言能力，同时发展出强大的视觉理解与生成能力。与从头开始预训练跨模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs就能将图像理解能力提升20%，图像生成能力提升3.6%，同时保留Llama-3的语言能力。此外，我们还展示了该框架能够适应现有的具有跨模态生成能力的视觉-语言模型。总体而言，该框架不仅利用了仅含文本LMM的现有计算投资，还能够促进语言和视觉能力的并行开发，为高效的跨模态模型开发指明了令人鼓舞的方向。', 'title_zh': 'LlamaFusion：适应预训练语言模型的多模态生成'}
{'arxiv_id': 'arXiv:2412.14995', 'title': 'HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs', 'authors': 'Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh', 'link': 'https://arxiv.org/abs/2412.14995', 'abstract': "Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo's reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.", 'abstract_zh': '自动启发式设计（AHD）是一个活跃的研究领域，因为它在解决复杂搜索和NP难组合优化问题方面具有实用性。近年来，大型语言模型（LLMs）的进步通过将LLMs与进化计算结合起来，自动生成启发式方法（即LLM基于的进化程序搜索LLM-EPS），从而开辟了新的可能性。尽管之前的研究在多种任务中取得了出色的性能，但仍然存在对启发式搜索空间的属性理解不足以及探索与利用之间的平衡问题，尤其是在大型启发式搜索空间中，这是一个关键因素。在本研究中，我们通过提出两种多样性度量指标，并对先前的LLM-EPS方法进行分析，包括FunSearch、EoH和ReEvo，来解决这一差距。实验证明，在黑盒AHD问题上，EoH的表现多样性和FunSearch和ReEvo相比更高，但其目标分数不稳定。相反，ReEvo的反射机制能产出好的目标分数，但在优化多样性方面效果不佳。基于这一发现，我们提出了一个自适应的LLM-EPS框架HSEvo，该框架利用和谐搜索算法保持多样性与收敛之间的平衡。通过实验，我们发现HSEvo不仅能实现高多样性和好的目标分数，还能保持成本效益。这些结果强调了在设计LLM-EPS框架时平衡探索和利用及理解启发式搜索空间的重要性。', 'title_zh': 'HSEvo：通过多样性驱动的和谐搜索和遗传算法结合大语言模型提升自动启发式设计'}
{'arxiv_id': 'arXiv:2412.14965', 'title': 'Movie2Story: A framework for understanding videos and telling stories in the form of novel text', 'authors': 'Kangning Li, Zheyang Jia, Anyu Ying', 'link': 'https://arxiv.org/abs/2412.14965', 'abstract': 'Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.', 'abstract_zh': '多模态视频到文本模型已经在生成视频内容的简短描述方面取得了显著进展。然而，在生成结合视频和音频的丰富长文本描述方面依然存在不足。本文介绍了一种名为M2S的框架，旨在通过结合音频、视频和字符识别来生成新的长度文本。M2S包括用于生成视频的长文本描述和理解、基于音频的情绪分析、语速分析以及字符对齐，以及基于视觉的字符识别对齐等功能模块。通过使用大型语言模型GLM4o综合多模态信息，M2S在多模态文本生成领域脱颖而出。我们通过比较实验和人工评估展示了M2S的有效性和准确性。此外，该模型框架具有良好的可扩展性，具有重要的未来研究潜力。', 'title_zh': 'Movie2Story：一种理解视频和以新颖文本形式讲述故事的框架'}
{'arxiv_id': 'arXiv:2412.14922', 'title': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response', 'authors': 'Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang', 'link': 'https://arxiv.org/abs/2412.14922', 'abstract': "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.", 'abstract_zh': '监督微调（SFT）在使大规模语言模型（LLMs）适应特定领域或任务方面发挥着至关重要的作用。然而，正如实证实验所表明的，收集的数据不可避免地含有噪声，这对模型在下游任务中的性能构成了重大挑战。因此，迫切需要一个稳健的SFT框架以增强模型在下游任务中的能力。为应对这一挑战，我们提出了一种稳健的SFT框架（RobustFT），该框架在下游任务数据上进行噪声检测和重新标注。在噪声识别方面，我们的方法采用了一个多专家协作系统，并结合了推理增强模型，以实现卓越的噪声检测性能。在去除噪声的阶段，我们采用了一种上下文增强策略，该策略整合了最相关且最具信心的知识，并经过仔细评估生成可靠的注释。此外，我们还引入了一种基于响应熵的有效数据选择机制，以确保仅保留高质量的样本用于微调。在多个LLM在五个多数据集上的广泛实验表明，RobustFT 在噪声场景中表现出色。', 'title_zh': 'RobustFT：在嘈杂响应情况下大型语言模型的稳健监督微调'}
{'arxiv_id': 'arXiv:2412.14905', 'title': 'Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation', 'authors': 'Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie', 'link': 'https://arxiv.org/abs/2412.14905', 'abstract': 'Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks.', 'abstract_zh': '大规模语言模型（LLMs）在整合检索增强生成（RAG）之后，仍然容易生成幻觉信息。平行上下文扩展（PCE）是一条研究路线，旨在有效整合无序的平行上下文，但在适应RAG场景时仍然会遭受幻觉问题。本文中，我们提出了一种名为DePaC（Dehallucinating Parallel Context Extension）的方案，通过上下文感知的负样本训练和信息校准聚合来缓解幻觉问题。DePaC 设计用于减轻两种类型的上下文幻觉：事实虚构（即LLMs提出缺乏上下文支持的断言）和事实遗漏（即LLMs未能提供能够由上下文支持的断言）。具体而言，（1）对于事实虚构，我们应用上下文感知的负样本训练，通过微调LLMs进行负监督，从而明确引导LLMs在上下文与问题无关时拒绝作答；（2）对于事实遗漏，我们提出信息校准聚合，优先考虑具有更高信息增量的上下文窗口。在九个RAG任务上的实验结果表明，DePaC 显著缓解了这两种类型的幻觉，并且在这些任务中始终实现了更好的表现。', 'title_zh': '去 hallucination 的并行上下文扩展以增强检索生成'}
{'arxiv_id': 'arXiv:2412.14843', 'title': 'Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas', 'authors': 'Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini', 'link': 'https://arxiv.org/abs/2412.14843', 'abstract': "The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.", 'abstract_zh': '对大型语言模型（LLMs）中的政治偏见进行分析，主要将这些系统视为具有固定观点的单一实体。虽然存在多种测量偏见的方法，但基于人设（persona）的提示对LLMs政治倾向的影响尚未被探究。本研究利用PersonaHub，一个合成人设描述集合，结合政治定向测试（PCT）来绘制基于人设提示的LLMs的政治分布图。然后，我们探索是否存在通过明确的意识形态提示将这些初始定向分布显著调整至完全对立的政治倾向：右威权主义和左自由主义的情况。实验结果揭示，合成人设主要集中在左自由主义象限，当用明确的意识形态描述词提示时，各个模型显示出不同程度的响应。尽管所有模型均表现出向右威权主义位置的重大转变，但向左自由主义位置的转变则相对有限，这表明模型对意识形态操控的不对称响应可能反映了模型训练中固有的偏见。', 'title_zh': '使用合成人物映射和影响大型语言模型的政治意识形态'}
{'arxiv_id': 'arXiv:2412.14841', 'title': 'Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis', 'authors': 'Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis, Agostino Cortesi, Enea Zaffanella', 'link': 'https://arxiv.org/abs/2412.14841', 'abstract': 'Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.\nFirst, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.\nOur results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.', 'abstract_zh': '大型语言模型（LLMs）是人工智能领域最具前景的发展之一，软件工程社区也迅速认识到它们在软件开发生命周期中的潜在作用。开发人员经常要求LLMs生成代码片段，从而提高生产力，但也可能引入所有权、隐私、准确性和安全性方面的问题。先前的研究强调，主流商业LLMs生成的代码往往不安全，包含漏洞、错误和代码异味。在这篇论文中，我们提出了一种框架，利用测试和静态分析来评估由通用开源LLMs生成的代码的质量，并引导这些代码的自我改进。\n\n首先，我们要求LLMs生成C代码以解决多个编程任务。然后，我们使用正式测试来评估生成代码的（不）正确性，并使用静态分析工具检测潜在的安全漏洞。接下来，我们评估模型评估生成代码的能力，即要求它们检测错误和漏洞。最后，我们测试模型修正生成代码的能力，提供在静态分析和不正确性评估阶段产生的报告作为反馈。\n\n我们的结果表明，模型常常生成不正确的代码，并且生成的代码中可能存在安全问题。此外，它们在检测这两种问题方面表现得很差。不过，当我们提供关于测试失败或潜在漏洞的信息时，观察到模型具有显著的纠错能力，这为提高基于LLM的代码生成工具的安全性提供了一个有希望的方向。', 'title_zh': '利用测试和静态分析反馈提高大语言模型的代码生成能力'}
{'arxiv_id': 'arXiv:2412.14771', 'title': 'ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine', 'authors': 'Rabee Qasem, Mohannad Hendi, Banan Tantour', 'link': 'https://arxiv.org/abs/2412.14771', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments.', 'abstract_zh': '大规模语言模型（LLMs）在多个领域展示了显著的潜力，但在法律领域的应用，尤其是在资源稀缺的环境中，仍然受到限制。本研究旨在克服将LLMs适应于巴勒斯坦法律领域的挑战，在这种背景下，政治不稳定、法律框架碎片化以及有限的人工智能资源阻碍了有效的机器学习应用。我们基于量化的Llama-3.2-1B-Instruct模型构建了一种精细调整的模型，并使用源自巴勒斯坦法律文本的合成数据集对其进行训练。通过使用规模较小的模型和战略性生成的问题-答案对，我们实现了成本效益高、本地可持续的解决方案，能够提供准确且与上下文相关的法律指导。实验结果表明，在不同类型的查询（从是/否问题和叙述解释到复杂的法律区分）上表现出色，同时指出了改进的领域，例如处理基于计算的查询和结构化列表格式化。本研究提供了一种途径，用于部署针对资源约束环境需求定制的人工智能驱动法律辅助工具。', 'title_zh': 'ALKAFI-LLAMA3：为巴勒斯坦精准法律理解细调的大语言模型'}
{'arxiv_id': 'arXiv:2412.14689', 'title': 'How to Synthesize Text Data without Model Collapse?', 'authors': 'Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou', 'link': 'https://arxiv.org/abs/2412.14689', 'abstract': 'Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.', 'abstract_zh': '合成数据中的模型塌陷表明，迭代训练于自动生成的数据会导致模型性能逐渐下降。随着人工智能模型的数量激增，合成数据将从根本上重塑网络数据生态系统。未来的GPT-$\\{n\\}$模型不可避免地会在合成数据与人类生成的数据混合训练。在本文中，我们关注两个问题：合成数据对语言模型训练的影响是什么？如何合成数据而不导致模型塌陷？我们首先对不同比例的合成数据进行了预训练，发现合成数据的比例与模型性能之间存在负相关关系。为进一步分析合成数据，我们进行了统计分析，揭示了分布偏移现象和n-克gram特征的过度集中。受上述发现的启发，我们提出在人类生成的数据上进行标记编辑，以获得半合成数据。作为概念证明，我们从理论上证明，标记级别的编辑可以防止模型塌陷，因为在测试误差上有有限的上限。我们进行了从头预训练、持续预训练和监督微调等广泛的实验。实验结果验证了我们理论证明的观点，即标记级别的编辑可以提高数据质量并增强模型性能。', 'title_zh': '如何在不发生模型坍塌的情况下合成文本数据？'}
{'arxiv_id': 'arXiv:2412.14626', 'title': 'Learning to Generate Research Idea with Dynamic Control', 'authors': 'Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du', 'link': 'https://arxiv.org/abs/2412.14626', 'abstract': 'The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.', 'abstract_zh': '大型语言模型（LLMs）的快速 advancements 已经展示了它们在加速科学发现方面的潜力，特别是在自动化研究构想过程方面。基于LLM的系统在生成假说和研究构想方面显示出潜力。然而，当前的方法主要依赖于提示驱动的预训练模型，这限制了它们对生成内容的有效优化能力。此外，它们还缺乏处理新颖性、可行性和有效性之间复杂依赖关系和内在限制的能力，这由于这些维度之间固有的权衡（如创新与可行性冲突）而变得极具挑战性。为解决这些局限性，我们首次提出对LLM进行微调，使其更好地提出构想，并引入了一种新颖的框架，该框架结合了监督微调（SFT）和可控强化学习（RL）的两阶段方法。在SFT阶段，模型从研究论文及其后续构想的配对中学习基础模式。在RL阶段，基于精细反馈的多维度奖励建模评估和优化生成的构想，以关键指标进行衡量。维度控制器允许动态调整生成，而句子级解码器确保在推理时具备上下文感知的强调。我们的框架提供了一种平衡的研究构想方法，通过动态权衡新颖性、可行性和有效性之间的权衡，实现了高质量的结果。', 'title_zh': '具有动态控制的科研理念生成学习'}
{'arxiv_id': 'arXiv:2412.14617', 'title': 'How good is GPT at writing political speeches for the White House?', 'authors': 'Jacques Savoy', 'link': 'https://arxiv.org/abs/2412.14617', 'abstract': 'Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma "we" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author\'s style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages.', 'abstract_zh': '使用大规模语言模型（LLM），计算机能够根据用户请求生成书面文本。由于这项普及性技术可应用于多种情境，本研究通过将一个名为GPT的LLM生成的演讲与近期美国总统的演讲进行比较，分析了GPT的书面风格。为此，本研究对比了里根至拜登时期的一些国情咨文（State of the Union Address），并将这些文本与GPT-3.5和GPT-4版本生成的文本进行了对比。与美国总统的演讲相比，GPT更倾向于过度使用“我们”这一词汇，且其生成的演讲平均句子更长但文本内容较短。此外，GPT采用较为乐观的语气，更频繁地使用政治术语（如，总统、国会）、象征性词汇（如，自由）和抽象词汇（如，自由）。即使要求GPT模仿某位作者的写作风格，生成的演讲仍然与目标作者的演讲风格不同。最后，两个GPT版本展现出不同的特点，但总体来看，都不大类似真实的总统演讲。', 'title_zh': 'GPT在撰写白宫政治演讲方面的表现如何？'}
{'arxiv_id': 'arXiv:2412.14510', 'title': 'PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization', 'authors': 'Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao', 'link': 'https://arxiv.org/abs/2412.14510', 'abstract': 'The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at this https URL.', 'abstract_zh': '检索增强生成（RAG）的出现缓解了大语言模型（LLMs）生成内容时过时和幻觉的问题，但仍然揭示出许多局限性。当通用大语言模型作为RAG生成器时，它往往存在响应信息量不足、响应稳健性和引文质量差的问题。过去的解决方法要么通过添加生成响应之外的步骤，要么通过监督微调（SFT）来优化生成器，但这些方法仍未彻底满足RAG的要求。因此，在保持其端到端大语言模型形式的同时，从多个偏好角度优化RAG生成器仍是一个挑战。为解决这一问题，我们提出了一种多视角偏好对齐方法（PA-RAG），用于全面优化RAG系统的生成器。具体来说，我们通过从不同场景的提示文档质量中采样多种质量的响应来构建高质量的指令微调数据和多视角偏好数据。随后，我们使用SFT和直接偏好优化（DPO）来优化生成器。我们针对三个大语言模型的四个问答数据集进行了广泛实验，结果表明PA-RAG可以显著提高RAG生成器的性能。我们的代码和数据集可在以下链接获取：[提供的URL]。', 'title_zh': 'PA-RAG：多角度偏好优化下的RAG对齐'}
{'arxiv_id': 'arXiv:2412.14304', 'title': 'Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs', 'authors': 'David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama', 'link': 'https://arxiv.org/abs/2412.14304', 'abstract': 'Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe.', 'abstract_zh': '当前的眼科临床工作流程受到过度转诊、长时间等待以及复杂且异质性医疗记录的困扰。大型语言模型（LLMs）为自动化各种程序（如分诊、初步测试如视力评估以及报告总结）提供了有希望的解决方案。然而，在自然语言问答任务中，LLMs在不同语言上的表现差异显著，这可能导致在低收入和中等收入国家（LMICs）中医疗保健不平等现象的加剧。本研究介绍了首个经过人工精标的问题多语言眼科问答基准，这使得可以在不同语言之间进行直接的语言对比。我们在7种不同语言上对6种流行的LLMs进行了评估，揭示了不同语言之间的显著偏差，这强调了在LMICs中临床应用LLMs的风险。现有的去偏见方法如翻译链式思考或检索增强生成（RAG）单靠自己并不能弥补这一性能差距，往往无法在所有语言中提升性能，并且缺乏对医学领域的特异性。为了解决这一问题，我们提出了CLARA（跨语言反思代理系统），一种利用检索增强生成和自我验证的新颖的推理时间去偏见方法。我们的方法不仅提高了所有语言的性能，还显著缩小了多语言偏差差距，促进了全球范围内LLMs的公平应用。', 'title_zh': '多语眼医：评估和消除低收入和中等收入国家（LMICs）中LLM眼科问答偏差的多语言基准'}
{'arxiv_id': 'arXiv:2412.14276', 'title': 'Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data', 'authors': 'haina Raza, Drai Paulen-Patterson, Chen Ding', 'link': 'https://arxiv.org/abs/2412.14276', 'abstract': 'Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection', 'abstract_zh': '虚假新闻对现代社会公众意见和社会稳定构成了重大威胁。本研究对比评估了BERT-like编码器模型和自回归解码器大型语言模型（LLMs）在虚假新闻检测任务中的表现。我们介绍了通过GPT-4辅助（一种AI标注方法）和经过人类专家验证的数据集，以确保数据的可靠性。这两种模型都在该数据集上进行了微调。此外，我们还开发了一种指令微调的LLM方法，并在推理过程中使用多数投票生成标签。我们的分析表明，BERT-like模型在分类任务中普遍优于LLMs，而LLMs在对抗文本扰动时表现更加稳健。与弱标签（远处监督）数据相比，结果表明，带有人类监督的AI标签能够获得更好的分类效果。本研究强调了结合基于AI的注释与人工监督的有效性，并展示了不同类型的机器学习模型在虚假新闻检测中的性能。', 'title_zh': '假新闻检测：BERT类模型与生成人工智能标注数据的大语言模型的比较评估'}
{'arxiv_id': 'arXiv:2412.14215', 'title': 'Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle', 'authors': 'Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa', 'link': 'https://arxiv.org/abs/2412.14215', 'abstract': 'As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve', 'abstract_zh': '随着基于大语言模型（LLM）的应用程序用户数量达到数百万，确保它们的可扩展性和持续的质量改进对于成功至关重要。然而，目前开发、维护和运行（DevOps）这些应用程序的工作流程大多是手工的、速度慢且依赖于试错。本文介绍了一种生成式人工智能工具包，该工具包在整个生命周期中自动化了基于大语言模型的应用程序的关键工作流程。该工具包有助于配置、测试、连续监控和优化生成式人工智能应用程序，如代理，从而显著提高质量并缩短发布周期。我们通过代表性用例展示了该工具包的有效性，分享了最佳实践，并概述了未来的增强措施。由于我们相信该生成式人工智能工具包对其他团队也会很有帮助，我们将其开源，并希望其他团队能够将其用于进一步的发展、适应和改进。', 'title_zh': '生成式AI工具包——一种在整个生命周期内提高基于LLM的应用质量的框架'}
{'arxiv_id': 'arXiv:2412.14203', 'title': 'BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement', 'authors': 'Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang', 'link': 'https://arxiv.org/abs/2412.14203', 'abstract': 'The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at this https URL', 'abstract_zh': '尽管大型语言模型（LLMs）在其他领域展现出了显著的进步，它们在计算机辅助设计（CAD）中的应用仍是一个未被充分探索的领域。本文介绍了BlenderLLM，这是一种利用自我改进方法训练专门针对CAD任务的LLM的新型框架。为支持这一目标，我们开发了一个定制的训练数据集BlendNet，并引入了一个全面的评估套件CADBench。我们的结果显示，现有模型在生成准确的CAD脚本方面存在显著限制。然而，通过最小限度的指令微调和迭代自我改进，BlenderLLM在CAD脚本生成的功能性和准确性上显著超越了现有模型。本研究为LLMs在CAD中的应用奠定了坚实的基础，同时展示了具有自我改进能力的模型在推进CAD自动化方面的变革潜力。我们鼓励进一步探索和采用这些方法以推动该领域的创新。该数据集、模型、基准测试和源代码均在此处https://公开获取。\n\n注：为了确保准确性与学术规范，此翻译尽可能贴近原文含义，并做了适当的调整以符合中文表达习惯。', 'title_zh': 'BlenderLLM：用于计算机辅助设计的自改进大型语言模型训练'}
{'arxiv_id': 'arXiv:2412.15151', 'title': 'Language Models as Continuous Self-Evolving Data Engineers', 'authors': 'Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang', 'link': 'https://arxiv.org/abs/2412.15151', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.', 'abstract_zh': '大语言模型（LLMs）在各种任务上展示了令人瞩目的能力，但其进一步演变受限于高质量训练数据的缺乏。此外，传统的训练方法过于依赖专家标注的数据，这限制了LLMs的性能上限。为解决这一问题，我们提出了一种新的范式，使LLMs能够通过自主生成、清理、审核和注释数据（包括偏好信息）来训练自己，这一范式被称为LANCE。我们的方法展示了LLMs可以作为持续自我进化的数据工程师，显著减少了后续训练数据构造过程的时间和成本。通过在不同版本的Qwen2上进行迭代微调，我们验证了LANCE在各种任务中的有效性，表明它可以持续提高模型性能并保持高质量数据生成。在八个基准维度上，LANCE分别提高了Qwen2-7B和Qwen2-7B-Instruct的平均分数3.36和2.70。这种自主数据构建的训练范式不仅减少了对人类专家或外部模型的依赖，还确保数据与人类价值观和偏好相一致，为开发可以超越人类能力的未来超级智能系统铺平了道路。', 'title_zh': '语言模型作为连续自演化的数据工程师'}
{'arxiv_id': 'arXiv:2412.15115', 'title': 'Qwen2.5 Technical Report', 'authors': 'Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu', 'link': 'https://arxiv.org/abs/2412.15115', 'abstract': 'In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.', 'abstract_zh': '在本报告中，我们介绍了Qwen2.5，这是一个全面的大语言模型（LLM）系列，旨在满足多样化的应用需求。与之前的版本相比，Qwen2.5在预训练和后训练阶段都得到了显著的改进。在预训练方面，我们将高质量的预训练数据集从之前的7万亿个令牌扩展到18万亿个令牌。这为常识、专家知识和推理能力提供了坚实的基础。在后训练方面，我们实施了复杂的监督微调，涉及超过100万个样本，并采用多阶段强化学习技术。后训练技术提高了人工偏好，特别是在长文本生成、结构数据分析和指令执行方面表现尤为突出。为了更好地应对多种多样的应用场景，我们提供了不同规模的Qwen2.5 LLM系列模型。开源版本包括基础模型和指令调优模型，并提供了量化版本。此外，对于托管解决方案，我们的私有模型目前包括两种专家混合模型（MoE）变体：Qwen2.5-Turbo和Qwen2.5-Plus，均可从阿里云模型仓库获取。Qwen2.5在广泛的语言理解、推理、数学、编程、人工偏好对齐等基准测试中表现出顶级性能。具体来说，开源重量级旗舰版Qwen2.5-72B-Instruct在多项基准测试中表现优异，超过了多个开源和私有模型，并且其性能与约为其五倍大的最先进的开源重量级模型Llama-3-405B-Instruct相当。Qwen2.5-Turbo和Qwen2.5-Plus在成本效益方面表现出色，分别在与GPT-4o-mini和GPT-4o的竞争中表现出色。此外，Qwen2.5模型作为基础，对于训练专门模型，例如Qwen2.5-Math、Qwen2.5-Coder、QwQ和多模态模型起到了关键作用。', 'title_zh': 'Qwen2.5 技术报告'}
{'arxiv_id': 'arXiv:2412.15101', 'title': 'Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability', 'authors': 'Xiangsen Chen, Xuming Hu, Nan Tang', 'link': 'https://arxiv.org/abs/2412.15101', 'abstract': 'Retrieve-augmented generation (RAG) frameworks have emerged as a promising solution to multi-hop question answering(QA) tasks since it enables large language models (LLMs) to incorporate external knowledge and mitigate their inherent knowledge deficiencies. Despite this progress, existing RAG frameworks, which usually follows the retrieve-then-read paradigm, often struggle with multi-hop QA with temporal information since it has difficulty retrieving and synthesizing accurate time-related information. To address the challenge, this paper proposes a novel framework called review-then-refine, which aims to enhance LLM performance in multi-hop QA scenarios with temporal information. Our approach begins with a review phase, where decomposed sub-queries are dynamically rewritten with temporal information, allowing for subsequent adaptive retrieval and reasoning process. In addition, we implement adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing the potential for hallucinations. In the subsequent refine phase, the LLM synthesizes the retrieved information from each sub-query along with its internal knowledge to formulate a coherent answer. Extensive experimental results across multiple datasets demonstrate the effectiveness of our proposed framework, highlighting its potential to significantly improve multi-hop QA capabilities in LLMs.', 'abstract_zh': '检索增强生成（RAG）框架自出现以来，已成为多跳问答（QA）任务的有前途的解决方案，因为这种框架能够使大型语言模型（LLMs）结合外部知识并缓解其固有的知识不足。尽管取得了这一进展，现有的RAG框架通常遵循“先检索后阅读”的范式，在处理包含时间信息的多跳问答时往往面临挑战，因为它们在提取和综合准确的时间相关信息方面存在困难。为应对这一挑战，本文提出了一种名为“先审查后完善”的新型框架，旨在增强LLMs在包含时间信息的多跳问答场景中的性能。我们的方法首先进入一个审查阶段，在这个阶段中，分解后的子查询动态地添加了时间信息，从而为后续的自适应检索和推理过程提供支持。此外，我们还实现了一种自适应检索机制，以减少不必要的检索次数，从而降低出现幻觉的风险。在后续的完善阶段，LLMs结合从每个子查询检索到的信息及其内部知识，形成一个连贯的答案。在多个数据集上进行的广泛实验结果表明，我们提出的框架的有效性，突显了其在显著提升LLMs的多跳问答能力方面的潜力。', 'title_zh': '基于动态框架的具有时间适应性的多跳问答复习与完善方法'}
{'arxiv_id': 'arXiv:2412.15060', 'title': 'ConfliBERT: A Language Model for Political Conflict', 'authors': 'Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan', 'link': 'https://arxiv.org/abs/2412.15060', 'abstract': "Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD).", 'abstract_zh': '冲突研究学者利用基于规则的方法从新闻报道和文本中提取有关政治暴力的信息。最近的自然语言处理发展超越了僵化的基于规则的方法。我们回顾了我们最近开发的ConfliBERT语言模型（Hu et al. 2022），用于处理与政治和暴力相关的文本。该模型可以用于从关于政治冲突的文本中提取行为者和行为分类。经过微调后，结果显示ConfliBERT在准确率、精确率和召回率方面优于其他大型语言模型（LLM），如Google的Gemma 2（9B）、Meta的Llama 3.1（7B）和Alibaba的Qwen 2.5（14B）在其相关领域内。此外，它比这些更通用的大型语言模型快几百倍。这些结果通过使用BBC、re3d和全球恐怖主义数据库（GTD）中的文本进行说明。', 'title_zh': 'ConfliBERT：一个用于政治冲突的语言模型'}
{'arxiv_id': 'arXiv:2412.14964', 'title': 'Knowledge Injection via Prompt Distillation', 'authors': 'Kalle Kujanpää, Harri Valpola, Alexander Ilin', 'link': 'https://arxiv.org/abs/2412.14964', 'abstract': "In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. In this paper, we propose a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which we call prompt distillation. First, we generate question-answer pairs about the new knowledge. Then, we fine-tune a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights.", 'abstract_zh': '在许多实际应用中，大语言模型（LLMs）需要整合其预训练数据中不存在的新知识。目前，主要的方法是微调（fine-tuning）和检索增强生成（retrieval-augmented generation，RAG）。尽管RAG已成为知识注入的行业标准，但微调尚未取得与其相当的成功。在本文中，我们提出了一种新的微调技术，用于学习新知识，并证明该技术可以达到RAG的性能。我们提出的方法基于自蒸馏的方法，我们称之为提示蒸馏（prompt distillation）。首先，我们生成关于新知识的问题-答案对。然后，我们使用问题-答案对对一个学生模型进行微调，使其模仿一个额外接收新知识提示的教师模型的输出分布。学生模型与教师模型相同，只是配备了LoRA适配器。这种训练程序有助于将教师模型中的新知识提示蒸馏到学生模型的权重中。', 'title_zh': '知识注入 via 提示提炼'}
{'arxiv_id': 'arXiv:2412.14872', 'title': 'Why language models collapse when trained on recursively generated text', 'authors': 'Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei', 'link': 'https://arxiv.org/abs/2412.14872', 'abstract': 'Language models (LMs) have been widely used to generate text on the Internet. The generated text is often collected into the training corpus of the next generations of LMs. Previous work has experimentally found that LMs collapse when trained on recursively generated text. This paper contributes to existing knowledge from two aspects. We present a theoretical proof of LM collapse. Our proof reveals the cause of LM collapse and proves that all auto-regressive LMs will definitely collapse. We present a new finding: the performance of LMs gradually declines when trained on recursively generated text until they perform no better than a randomly initialized LM. The trained LMs produce large amounts of repetitive text and perform poorly across a wide range of natural language tasks. The above proof and new findings deepen our understanding of LM collapse and offer valuable insights that may inspire new training techniques to mitigate this threat.', 'abstract_zh': '语言模型（LMs）广泛应用于网络文本生成。生成的文本通常会被收集到下一代LMs的训练语料库中。此前的工作已经通过实验发现，当LMs在递归生成的文本上进行训练时会发生崩溃现象。本文从两个方面增进了现有知识。我们提供了一个关于LM崩溃现象的理论证明。我们的证明揭示了LM崩溃的原因，并证明所有自回归LMs都将不可避免地发生崩溃。我们还发现了一个新的发现：当LMs在递归生成的文本上进行训练时，其性能会逐渐下降，直到它们的表现不如随机初始化的LMs。经过训练的LMs会产生大量重复的文本，并在各种自然语言任务中表现不佳。上述证明和新发现加深了我们对LM崩溃现象的理解，并提供了宝贵的见解，可能激发新的训练技术来缓解这一威胁。', 'title_zh': '当使用递归生成的文本进行训练时，语言模型为何会出现崩溃现象'}
{'arxiv_id': 'arXiv:2412.14867', 'title': 'Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering', 'authors': 'Imed Keraghel, Mohamed Nadif', 'link': 'https://arxiv.org/abs/2412.14867', 'abstract': 'Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities.', 'abstract_zh': '近年来，机器学习领域的进展，尤其是大型语言模型（LLMs）如BERT和GPT，提供了丰富的情境嵌入，从而提高了文本表示的效果。然而，当前的文档聚类方法往往忽视了命名实体（NEs）之间的更深层次关系以及LLM嵌入的潜力。本文提出了一种新颖的方法，该方法将命名实体识别（NER）和LLM嵌入嵌入到基于图的框架中，用于文档聚类。该方法构建了一个图，图中的节点表示文档，边的权重基于命名实体相似性，并通过图卷积网络（GCN）进行优化。这确保了更有效的基于语义的相关文档分组。实验结果表明，与基于共现的常规方法相比，我们的方法在富含有名实体的文档聚类方面表现更优。', 'title_zh': '图卷积网络：文档聚类中的命名实体识别与大型语言模型嵌入'}
{'arxiv_id': 'arXiv:2412.14780', 'title': 'Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning', 'authors': 'Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng', 'link': 'https://arxiv.org/abs/2412.14780', 'abstract': 'When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles - specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format) - differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).', 'abstract_zh': '在使用代理任务数据集来增强大型语言模型（LLMs）的能力时，当前的方法通常将每个样本中的所有词元等同对待。然而，我们指出，承担不同角色的词元——具体来说，是推理词元与模板词元（例如，控制输出格式的词元）——在重要性及学习复杂度上存在显著差异，因此需要将它们解耦并区别对待。为解决这一问题，我们提出了一种新的Shuffle-Aware Discriminator（SHAD）以实现自适应词元区分。SHAD通过利用输入-输出组合在样本之间打乱后的可预测性差异进行分类：模板词元由于其在样本中的重复性，其可预测性保持不变，而推理词元则不然。借助SHAD，我们提出了推理强调微调（RFT）方法，在微调过程中自适应地强调推理词元，从而在常见的监督微调（SFT）方法之上取得了显著的性能提升。', 'title_zh': '拆分推理标记和模板标记以进行语言模型微调'}
{'arxiv_id': 'arXiv:2412.14751', 'title': 'Query pipeline optimization for cancer patient question answering systems', 'authors': 'Maolin He, Rena Gao, Mike Conway, Brian E. Chapman', 'link': 'https://arxiv.org/abs/2412.14751', 'abstract': 'Retrieval-augmented generation (RAG) mitigates hallucination in Large Language Models (LLMs) by using query pipelines to retrieve relevant external information and grounding responses in retrieved knowledge. However, query pipeline optimization for cancer patient question-answering (CPQA) systems requires separately optimizing multiple components with domain-specific considerations. We propose a novel three-aspect optimization approach for the RAG query pipeline in CPQA systems, utilizing public biomedical databases like PubMed and PubMed Central. Our optimization includes: (1) document retrieval, utilizing a comparative analysis of NCBI resources and introducing Hybrid Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval, identifying optimal pairings of dense retrievers and rerankers; and (3) semantic representation, introducing Semantic Enhanced Overlap Segmentation (SEOS) for improved contextual understanding. On a custom-developed dataset tailored for cancer-related inquiries, our optimized RAG approach improved the answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and about 3% over a naive RAG setup. This study highlights the importance of domain-specific query optimization in realizing the full potential of RAG and provides a robust framework for building more accurate and reliable CPQA systems, advancing the development of RAG-based biomedical systems.', 'abstract_zh': '检索增强生成（RAG）通过使用查询管道检索相关外部信息并使响应基于检索到的知识，从而减轻了大型语言模型（LLMs）中的幻觉问题。然而，针对癌症患者问答（CPQA）系统的查询管道优化需要分别优化多个具有特定领域考虑的组件。我们提出了一种针对CPQA系统中RAG查询管道的新型三方面优化方法，利用如PubMed和PubMed Central等公共生物医学数据库。我们的优化包括：(1) 文档检索，通过比较NCBI资源并引入混合语义实时文档检索（HSRDR）进行比较分析；(2) 段落检索，识别密集检索器和排序器的最佳配对；以及(3) 语义表示，引入语义增强重叠分段（SEOS）以提高上下文理解能力。在为癌症相关询问定制开发的数据集上，我们优化的RAG方法在Claude-3-haiku上的答案准确性相比思维链提示提高了5.24%，相比简单的RAG设置提高了约3%。本研究强调了在实现RAG最大潜力时领域特定查询优化的重要性，并提供了一个构建更准确和可靠CPQA系统的稳健框架，促进了基于RAG的生物医学系统的发展。', 'title_zh': '癌症患者问答系统中的查询管道优化'}
{'arxiv_id': 'arXiv:2412.14737', 'title': 'On Verbalized Confidence Scores for LLMs', 'authors': 'Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada', 'link': 'https://arxiv.org/abs/2412.14737', 'abstract': "The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at this https URL .", 'abstract_zh': '大型语言模型（LLMs）的兴起及其在日常生活中紧密集成，使得关注其可信度变得至关重要。对LLMs进行不确定性量化可以增加人类对它们响应的信任度，同时使LLM智能体能够基于彼此的不确定性做出更加知情的决策。为了估计响应中的不确定性，通常使用内部令牌 logits、任务特定的代理模型或多次生成响应进行采样。本文的研究重点是让LLM本身在其输出令牌中表达其不确定性并附带信心分数，这是一种具有低开销的提示和模型通用的不确定性量化方法。我们使用广泛的基准测试，评估表达的信心分数在不同数据集、模型和提示方法下的可靠性。结果表明，这些分数的可靠性很大程度上取决于模型是如何被提问的，但通过某些提示方法，也有可能提取出校准良好的信心分数。我们认为，表达的信心分数可能会成为未来简单但有效且多用途的不确定性量化方法之一。我们的代码可在以下链接获取：this https URL 。', 'title_zh': '关于语言表达的置信分数对大语言模型的影响研究'}
{'arxiv_id': 'arXiv:2412.14675', 'title': 'LLMs as mediators: Can they diagnose conflicts accurately?', 'authors': 'Özgecan Koçak, Phanish Puranam, Afşar Yegin', 'link': 'https://arxiv.org/abs/2412.14675', 'abstract': "Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Koçak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.", 'abstract_zh': '以往的研究表明，观察者要在两方争执中进行干预，必须能够可靠地区分他们的分歧来源，是基于对事实的不同信念（因果关系）还是基于价值观的不同（道德）。本论文旨在测试OpenAI的大规模语言模型GPT-3.5和GPT-4是否能够完成这一任务，以及哪种类型的分歧对这些语言模型来说尤为具有诊断难度。我们复制了Koçak等人（2003）的研究，使用情景设计（vignette design），并用OpenAI的GPT-3.5和GPT-4进行实验。研究发现，这两种语言模型在因果关系和道德规范之间的语义理解与人类相似，并且能够可靠地区分这两者。当要求诊断对话中的分歧来源时，与人类相比，这两种语言模型更倾向于高估因果性分歧的程度，并低估道德性分歧的程度，尤其是在涉及具体语言和具体问题时，GPT-4的表现尤为明显。GPT-3.5在使用近端或远端量表时的表现不如GPT-4或人类。本研究提供了首次测试利用语言模型诊断因果性和评价性分歧以调解冲突的可能性。', 'title_zh': 'LLM作为调解者：它们能否准确诊断冲突？'}
{'arxiv_id': 'arXiv:2412.14656', 'title': 'Length Controlled Generation for Black-box LLMs', 'authors': 'Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin', 'link': 'https://arxiv.org/abs/2412.14656', 'abstract': 'Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.', 'abstract_zh': '大语言模型（LLMs）在指令跟随方面展现出了令人印象深刻的能力，但在生成文本长度控制方面仍然存在困难，这是许多现实应用中的一个基本要求。现有的长度控制方法涉及对LLMs的参数进行微调，这既不高效也不适合实际应用。在本文中，我们提出了一种新的迭代采样框架，结合了Metropolis-Hastings算法和重要性采样加速策略，用于文本长度控制。该框架有效地且可靠地调控LLMs生成长度受限的文本，而无需修改底层参数，从而保留了LLMs的原始能力。实验结果表明，我们的框架在Llama3.1上实现了几乎100%的长度控制成功率，适用于诸如长度受限的抽取式总结和长度受限的指令跟随等任务，且额外的计算开销极小。这还突显了我们的方法在更广泛的领域实现精确长度控制的巨大潜力，而不牺牲LLMs的多功能性。', 'title_zh': '黑箱大语言模型中基于长度的生成控制'}
{'arxiv_id': 'arXiv:2412.14588', 'title': 'Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning', 'authors': 'Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu', 'link': 'https://arxiv.org/abs/2412.14588', 'abstract': "In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.", 'abstract_zh': '在法律实践中，法官应用刑法的三元法理论，依次评估犯罪行为的构成要件、违法性和罪过性，以确定某个人的行为是否构成犯罪。尽管当前的法律大规模语言模型（LLMs）在判决预测方面显示出潜在的准确性，但由于缺乏适当的基准数据集，它们在判断无辜结果方面缺乏三元推理能力，因此无法预测无辜判决。由此，每个输入都会自动被指控，限制了它们在法律情境中的实际应用价值。为弥补这一差距，我们引入了LJPIV，这是第一个包含无辜判决的法律判决预测基准数据集。遵循三元法理论，我们通过基于LLM的扩展和手动验证，扩展了三个广泛使用的法律数据集。我们的实验结果显示：（1）当前的法律LLMs在LJPIV上的表现仍有显著提升空间，即使是表现最佳的模型，在LJPIV上的F1分数也低于0.3；（2）我们的策略在提高领域内和跨领域判决预测准确性方面表现出明显优势，特别是对于导致无辜判决的案件。', 'title_zh': '超越罪恶感：基于三分推理的法律判决预测'}
{'arxiv_id': 'arXiv:2412.14584', 'title': 'Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues', 'authors': 'Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin', 'link': 'https://arxiv.org/abs/2412.14584', 'abstract': 'Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.', 'abstract_zh': '近年来，主动对话领域的最新进展引起了广泛关注，尤其是在更复杂的任务上（如情感支持和说服）。与传统的任务导向对话不同，主动对话需要更为高级的策略规划和灵活性，从而要求开发丰富的场景和全面的策略库。然而，现有的方法往往依赖大型语言模型（LLMs）进行用户模拟和在线学习，这导致了与现实场景不符的偏见，从而影响了系统的性能。此外，这些方法依赖于手动定义、上下文无关的粗粒度策略，这不仅增加了专家的成本，还引发了其完备性的担忧。在我们的研究中，我们强调了从原始的实时对话记录中自动发现策略的潜力。为此，我们提出了一种新的对话策略规划框架，即LDPP。该框架完全自动化了从对话记录中挖掘策略到学习策略规划的全过程。具体而言，我们使用变分自编码器的变体来发现表示为潜在向量的细粒度策略。在自动为数据打上这些潜在策略标签后，我们在潜在空间中提出了一个离线分层强化学习（RL）算法，从而开发出有效的策略规划能力。实验证明，LDPP在两个主动对话场景中优于现有方法，甚至仅使用一个拥有1.8亿参数的LLM就超过了ChatGPT。', 'title_zh': '无模拟分层隐状态策略规划以支持主动对话'}
{'arxiv_id': 'arXiv:2412.14556', 'title': 'CitaLaw: Enhancing LLM with Citations in Legal Domain', 'authors': 'Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu', 'link': 'https://arxiv.org/abs/2412.14556', 'abstract': "In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.", 'abstract_zh': '在本文中，我们提出了CitaLaw，这是第一个用于评估大语言模型（LLM）生成合法合规响应并适当引用支持文本能力的基准测试。CitaLaw包含了一系列面向普通公众和专业人士的多样化法律问题，并结合了一个全面的法律文章和先例案例参考库。这一框架使基于大语言模型的系统能够从参考库中检索支持引文，并将这些引文与响应中相应的句子对齐。此外，我们引入了借鉴三段论的评估方法来评估检索到的参考文献与大语言模型生成的响应之间的法律一致性，以及这些响应与用户问题的一致性。通过对2个开放领域和7个法律专门领域的大语言模型进行广泛实验，我们证明了集成法律参考文献显著提高了响应质量。此外，我们提出的基于三段论的评估方法与人类判断高度一致。', 'title_zh': 'CitaLaw：在法律领域增强大规模语言模型的引用功能'}
{'arxiv_id': 'arXiv:2412.14470', 'title': 'Agent-SafetyBench: Evaluating the Safety of LLM Agents', 'authors': 'Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang', 'link': 'https://arxiv.org/abs/2412.14470', 'abstract': 'As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \\url{this https URL} to facilitate further research and innovation in agent safety evaluation and improvement.', 'abstract_zh': '随着大型语言模型（LLMs）越来越多地被用作代理，它们与互动环境的整合以及工具使用带来的新安全挑战超出了模型本身所固有的挑战。然而，缺乏全面的安全评估基准是有效评估和进一步改进的重大障碍。本文介绍了一个全面的基准Agent-SafetyBench，用于评估LLM代理的安全性。Agent-SafetyBench 包含349种互动环境和2,000个测试案例，评估了8类安全风险，并涵盖了10种常见失败模式，这些模式在不安全的互动中频繁出现。我们的评估显示了16种流行LLM代理中的一个令人担忧的结果：没有一个代理的得分超过60%。这突显了LLM代理中存在显著的安全挑战，并强调了改进的迫切需求。通过定量分析，我们确定了关键的失败模式，并总结了当前LLM代理中的两种基本安全检测：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅仅依赖防御提示是不足以解决这些安全问题的，强调了需要更先进和可靠的策略。我们已在 \\url{this https URL} 发布了Agent-SafetyBench，以促进代理安全性评估和改进的研究和创新。', 'title_zh': 'Agent-SafetyBench: 评估大规模语言模型代理的安全性'}
{'arxiv_id': 'arXiv:2412.14426', 'title': 'All-in-One Tuning and Structural Pruning for Domain-Specific LLMs', 'authors': 'Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao', 'link': 'https://arxiv.org/abs/2412.14426', 'abstract': 'Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.', 'abstract_zh': '针对大型语言模型（LLMs）在特定领域的应用，现有的剪枝技术通常遵循两阶段过程：首先对预训练的一般用途LLMs进行剪枝，然后在特定领域对剪枝后的LLMs进行微调。然而，来自预训练权重的剪枝决策在微调过程中保持不变，即使权重已被更新。因此，这样的剪枝决策与微调后的权重组合可能并非最优，导致显著的性能下降。为解决这些问题，我们提出了一种新的方法：全合一微调和结构剪枝（ATP）。ATP提供了一种统一的一站式结构剪枝与微调方法，在微调过程中通过可训练的剪枝决策生成器动态识别当前最优子结构。此外，鉴于特定领域应用的数据有限，低秩适应（LoRA）常用作微调LLMs的技术。在ATP中，我们引入了LoRA意识前向传播和稀疏性正则化，以确保学习到的剪枝决策对应的子结构可以在ATP处理完成后直接移除。ATP在法律和医疗领域的任务上优于最先进的两阶段剪枝方法。具体而言，当对LLaMA2-7B和LLaMA3-8B模型分别修剪40%的参数时，ATP分别恢复了密集模型88%和91%的性能。', 'title_zh': '面向特定领域的大型语言模型的一体化调优与结构剪枝'}
{'arxiv_id': 'arXiv:2412.14373', 'title': 'ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling', 'authors': 'William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao', 'link': 'https://arxiv.org/abs/2412.14373', 'abstract': 'Large Language Models (LLMs) have shown remarkable adaptability across domains beyond text, specifically electrocardiograms (ECGs). More specifically, there is a growing body of work exploring the task of generating text from a multi-channeled ECG and corresponding textual prompt. Current approaches typically involve pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective and using the features output by the pretrained encoder to finetune a LLM for natural language generation (NLG). However, these methods are limited by 1) inefficiency from two-stage training and 2) interpretability challenges with encoder-generated features. To address these limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. This approach compresses and encodes ECG signals into tokens, enabling end-to-end LLM training by combining ECG and text tokens directly, while being much more interpretable since the ECG tokens can be directly mapped back to the original signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only half the time and ~48% of the data required by two-stage approaches.', 'abstract_zh': '大型语言模型（LLMs）在文本领域之外的多个领域中表现出了显著的适用性，特别是在心电图（ECGs）领域。具体来说，当前的研究越来越多地关注从多通道ECGs和相应的文本提示中生成文本的任务。现有的方法通常包括通过自监督学习（SSL）目标对特定于ECGs的编码器进行预训练，并使用预训练编码器输出的特征对大型语言模型进行微调以进行自然语言生成（NLG）。然而，这些方法受限于1）两阶段训练的低效率，以及2）编码器生成的特征可解释性较差等问题。为了解决这些限制，我们提出了ECG-Byte，这是一种适应性的字节对编码（BPE）分词器流水线，用于ECGs的自回归语言建模。这种方法将ECGs信号压缩并编码为令牌，通过直接结合ECG和文本令牌进行端到端的LLM训练，从而显著提高可解释性，因为ECG令牌可以直接映射回原始信号。使用ECG-Byte，我们仅在所需数据量的约48%和所需时间的一半内实现了竞争力的表现。', 'title_zh': 'ECG-Byte: 一种用于心电图语言建模的端到端生成型标记器'}
{'arxiv_id': 'arXiv:2412.14352', 'title': 'A Survey on LLM Inference-Time Self-Improvement', 'authors': 'Xiangjue Dong, Maria Teleki, James Caverlee', 'link': 'https://arxiv.org/abs/2412.14352', 'abstract': 'Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.', 'abstract_zh': '近年来，通过增加推理时的计算量来提高推理的技术得到了广泛关注。在本文综述中，我们从三个不同的角度调查了大规模语言模型（LLM）推理时自我改进的现状：独立自我改进，侧重于通过解码或采样方法进行增强；情境感知自我改进，利用额外的情境或数据存储；模型辅助自我改进，通过模型协作实现改进。我们对近期相关研究进行了全面回顾，提供了一个深入的分类体系，并讨论了面临的问题和限制，为未来的研究提供了见解。', 'title_zh': '关于LLM推理时自我改进的综述'}
{'arxiv_id': 'arXiv:2412.15113', 'title': 'Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture', 'authors': 'Thomas F Burns, Tomoki Fukai, Christopher J Earls', 'link': 'https://arxiv.org/abs/2412.15113', 'abstract': 'Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.', 'abstract_zh': '大型语言模型（LLMs）展示出利用输入序列上下文中的信息以适当响应训练过程中未见过的数据的强大能力。这种能力被称为上下文内学习（ICL）。人类和非人类动物也表现出了类似的能力，但它们的神经架构与LLMs差异巨大。尽管如此，LLMs中的一个关键组件——注意力机制，与现代联想记忆模型相似，这些模型广泛用于并与计算神经科学社区共同影响着对生物记忆系统的建模。利用这种联系，我们引入了一个能够执行ICL的联想记忆模型。我们利用这一启发设计了一种新的残差流架构，允许信息直接在注意力头之间流动。我们在两层Transformer的训练过程中测试了这一架构，并展示了这种修改后ICL能力的体现速度快于没有这种修改的情况。随后，我们将在含有800万个参数的小型语言模型中应用这种架构，重点在注意力头值上，结果显示，在这一较大且更符合自然的规模上，ICL性能也得到了改进。', 'title_zh': '关联记忆启发了一种新型注意力残差流架构在上下文学习中的性能提升'}
{'arxiv_id': 'arXiv:2412.14574', 'title': 'Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models', 'authors': 'Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2412.14574', 'abstract': 'Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \\url{this https URL}.', 'abstract_zh': '大型语言模型（LLMs）在列表级段落排序方面展示了令人兴奋的性能。由于输入长度的限制，现有方法通常采用滑动窗口策略。尽管该策略有效，但效率较低，因为它涉及重复的串行处理，通常会多次重新评估相关段落，从而导致冗余的API成本，这些成本与推理令牌的数量成正比。长语境LLM的发展使所有段落能够在单次推理中进行全面排序，从而避免了重复的API成本。在本文中，我们从效率和有效性两个方面对长语境LLM在排序任务中的应用进行了全面研究。令人惊讶的是，我们的实验表明，在监督微调设置中，使用长语境LLM进行全面排序可以显著提高性能并实现巨大的效率提升。此外，我们发现基于现有方法微调全面排序模型的两个局限性：（1）滑动窗口策略无法生成完整的排序列表作为训练标签；（2）语言模型损失不能强调标签中排名靠前的段落ID。为了解决这些问题，我们提出了一种新的完整列表级标签构建方法和全新的基于重要性的学习目标，以用于全面排序。实验表明，我们的方法在基线方法上表现出更优的效果。我们的代码可在\\url{此链接}获取。', 'title_zh': '滑动窗口并非终点：探索使用长上下文大语言模型的全面排名'}
{'arxiv_id': 'arXiv:2412.14516', 'title': 'Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment', 'authors': 'Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar', 'link': 'https://arxiv.org/abs/2412.14516', 'abstract': 'We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.', 'abstract_zh': '我们研究了将大规模语言模型（LLMs）与人类偏好数据对齐的问题。对比偏好优化通过优化与策略相关的隐含奖励，在将LLMs与可用的偏好数据对齐方面表现出了令人鼓舞的结果。然而，对比目标主要关注两个响应关联的隐含奖励的相对值，而忽视了它们的实际值，导致了与人类偏好之间的非最优对齐。为解决这一局限，我们提出了校准直接偏好优化（Cal-DPO），这是一种简单而有效的算法。我们证明，通过校准隐含奖励，确保学习到的隐含奖励在尺度上与真实奖励相当，可以显著提高与给定偏好数据的对齐程度。我们展示了Cal-DPO相对于现有方法的理论优势。我们的实验结果表明，Cal-DPO在多种标准基准上的表现显著提高了现成方法的效果。', 'title_zh': 'Cal-DPO：校准直接偏好优化的语言模型对齐方法'}
