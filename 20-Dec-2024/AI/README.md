# Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying 

**Title (ZH)**: 关键问题思考：通过论辩性查询引导大规模语言模型的推理 

**Authors**: Federico Castagna, Isabel Sassoon, Simon Parsons  

**Link**: [PDF](https://arxiv.org/pdf/2412.15177)  

**Abstract**: Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided. 

**Abstract (ZH)**: 研究表明，尽管最近在人工智能（AI）研究领域取得了突破性进展，最先进的大规模语言模型（LLMs）在进行逻辑和数学推理时仍然存在困难。结果显示，即使是最先进的LLMs仍主要充当（高度先进的）数据模式识别器，当尝试解决训练数据中未出现或与之相去甚远的推理问题时，其泛化能力较差。为应对这一紧迫的挑战，本文借鉴论证理论文献中的“关键问题”概念，并特别关注于托马斯·库恩（Toulmin）的论证模型。我们证明，使用这些关键问题可以提高LLMs的推理能力。通过探究模型推理过程的理由，LLMs可以评估是否有逻辑错误发生，并在向用户提供最终答复之前进行纠正。这一理念源自任何有效论证程序的黄金标准：如果结论是由公认的前提所蕴含的，那么结论就是有效的。或者，如亚里士多德原则在现实世界中的近似表述，在信息不完整且预设逻辑的情况下，如果未被反驳，则结论是有效的。这种方法成功地引导模型输出通过推理管道，使其相对于基线及其链式思维（CoT）实现表现更好。为此，我们提供了对MT-Bench推理和数学任务的广泛评估，涵盖多种不同类型的LLMs。 

---
# Probabilistic Strategy Logic with Degrees of Observability 

**Title (ZH)**: 概率策略逻辑与观测度量 

**Authors**: Chunyan Mu, Nima Motamed, Natasha Alechina, Brian Logan  

**Link**: [PDF](https://arxiv.org/pdf/2412.15135)  

**Abstract**: There has been considerable work on reasoning about the strategic ability of agents under imperfect information. However, existing logics such as Probabilistic Strategy Logic are unable to express properties relating to information transparency. Information transparency concerns the extent to which agents' actions and behaviours are observable by other agents. Reasoning about information transparency is useful in many domains including security, privacy, and decision-making. In this paper, we present a formal framework for reasoning about information transparency properties in stochastic multi-agent systems. We extend Probabilistic Strategy Logic with new observability operators that capture the degree of observability of temporal properties by agents. We show that the model checking problem for the resulting logic is decidable. 

**Abstract (ZH)**: 在不完美信息条件下代理的策略能力推理方面已有大量研究工作。然而，现有的逻辑如概率策略逻辑无法表达与信息透明度相关的性质。信息透明度关注代理的行动和行为被其他代理观察到的程度。在安全、隐私和决策制定等领域中，对信息透明度的推理是很有用的。本文提出了一种形式框架，用于在随机多代理系统中推理信息透明度性质。我们扩展了概率策略逻辑，加入新的可观察性操作符，以捕捉代理对时间属性可观察程度的表达。我们证明了所得到逻辑的模型检查问题是可判定的。 

---
# Towards Friendly AI: A Comprehensive Review and New Perspectives on Human-AI Alignment 

**Title (ZH)**: 亲和人工智能：人类-人工智能一致性的综合review与新视角 

**Authors**: Qiyang Sun, Yupei Li, Emran Alturki, Sunil Munthumoduku Krishna Murthy, Björn W. Schuller  

**Link**: [PDF](https://arxiv.org/pdf/2412.15114)  

**Abstract**: As Artificial Intelligence (AI) continues to advance rapidly, Friendly AI (FAI) has been proposed to advocate for more equitable and fair development of AI. Despite its importance, there is a lack of comprehensive reviews examining FAI from an ethical perspective, as well as limited discussion on its potential applications and future directions. This paper addresses these gaps by providing a thorough review of FAI, focusing on theoretical perspectives both for and against its development, and presenting a formal definition in a clear and accessible format. Key applications are discussed from the perspectives of eXplainable AI (XAI), privacy, fairness and affective computing (AC). Additionally, the paper identifies challenges in current technological advancements and explores future research avenues. The findings emphasise the significance of developing FAI and advocate for its continued advancement to ensure ethical and beneficial AI development. 

**Abstract (ZH)**: 随着人工智能（AI）的快速发展，友好人工智能（FAI）的概念已被提出，旨在促进更加公平和公正的AI发展。尽管FAI的重要性不言而喻，但缺乏从道德角度进行全面审视的研究，关于其潜在应用和未来发展方向的讨论也非常有限。本文通过提供对FAI的全面回顾，填补了这些空白，重点关注支持和发展FAI的理论视角，并以清晰易懂的方式提出正式定义。本文还从可解释人工智能（XAI）、隐私、公平性和情感计算（AC）的角度讨论了关键应用。此外，本文指出了当前技术进步中的挑战，并探讨了未来的研究方向。研究结果强调了开发FAI的重要性，并倡导继续推进FAI的发展，以确保道德和有益的AI发展。 

---
# Generalizing Constraint Models in Constraint Acquisition 

**Title (ZH)**: 在约束获取中推广约束模型 

**Authors**: Dimos Tsouros, Senne Berden, Steven Prestwich, Tias Guns  

**Link**: [PDF](https://arxiv.org/pdf/2412.14950)  

**Abstract**: Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this limitation by proposing GenCon, a novel approach to learn parameterized constraint models capable of modeling varying instances of the same problem. To achieve this generalization, we make use of statistical learning techniques at the level of individual constraints. Specifically, we propose to train a classifier to predict, for any possible constraint and parameterization, whether the constraint belongs to the problem. We then show how, for some classes of classifiers, we can extract decision rules to construct interpretable constraint specifications. This enables the generation of ground constraints for any parameter instantiation. Additionally, we present a generate-and-test approach that can be used with any classifier, to generate the ground constraints on the fly. Our empirical results demonstrate that our approach achieves high accuracy and is robust to noise in the input instances. 

**Abstract (ZH)**: 约束获取（CA）旨在通过协助用户建模过程来扩大约束编程的应用范围。然而，大多数CA方法存在一个显著的缺陷：它们为特定问题实例学习了一组单独的约束，但无法将这些约束泛化为问题的参数化约束规范。本文通过提出GenCon，一种学习参数化约束模型的新方法，从而解决了这一限制，该模型能够建模同一问题的不同实例。为了实现这一泛化，我们利用统计学习技术在单个约束级别上进行操作。具体来说，我们提出训练一个分类器，用于预测任何可能的约束和参数化组合是否属于问题。然后，我们展示了对于某些类别的分类器，我们可以通过提取决策规则来构建可解释的约束规范，从而能够为任何参数化实例生成基础约束。此外，我们还提出了一种生成-检验方法，可以与任何分类器结合使用，以实时生成基础约束。我们的实验证明，该方法具有高准确度，并且对输入实例中的噪声具有鲁棒性。 

---
# Answer Set Networks: Casting Answer Set Programming into Deep Learning 

**Title (ZH)**: 回答集网络：将回答集编程融入深度学习 

**Authors**: Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting  

**Link**: [PDF](https://arxiv.org/pdf/2412.14814)  

**Abstract**: Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU's batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the "constitutional navigation" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments. 

**Abstract (ZH)**: 尽管回答集编程（Answer Set Programming，ASP）能够约束神经符号（NeSy）系统，但计算稳定模型的成本高昂且最先进的求解器是CPU密集型的，这对ASP的运用构成了阻碍。为此，我们提出了一种名为回答集网络（Answer Set Networks，ASN）的NeSy求解器。基于图神经网络（Graph Neural Networks，GNN），ASN是一种基于ASP的深度概率逻辑编程（Deep Probabilistic Logic Programming，DPPL）的可扩展方法。具体而言，我们展示了如何将ASP转换为ASN，并通过利用GPU的批量处理和并行化能力，展示了ASN如何高效地解决编码问题。我们的实验评估表明，与最先进的CPU密集型NeSy系统相比，ASN在多个任务上表现出色。此外，我们基于ASN的强点做出以下两项贡献。首先，我们首次展示了使用DPPL微调大语言模型（Large Language Models，LLM），并利用ASN以逻辑为指导进行训练；其次，我们展示了无人机的“宪法导航”，即通过将公共航空法律编码在ASN中，实现无人驾驶航空器在不确定环境中的路径规划。 

---
# LTLf Synthesis Under Unreliable Input 

**Title (ZH)**: 在不可靠输入下的LTLf合成 

**Authors**: Christian Hagemeier, Giuseppe de Giacomo, Moshe Y. Vardi  

**Link**: [PDF](https://arxiv.org/pdf/2412.14728)  

**Abstract**: We study the problem of realizing strategies for an LTLf goal specification while ensuring that at least an LTLf backup specification is satisfied in case of unreliability of certain input variables. We formally define the problem and characterize its worst-case complexity as 2EXPTIME-complete, like standard LTLf synthesis. Then we devise three different solution techniques: one based on direct automata manipulation, which is 2EXPTIME, one disregarding unreliable input variables by adopting a belief construction, which is 3EXPTIME, and one leveraging second-order quantified LTLf (QLTLf), which is 2EXPTIME and allows for a direct encoding into monadic second-order logic, which in turn is worst-case nonelementary. We prove their correctness and evaluate them against each other empirically. Interestingly, theoretical worst-case bounds do not translate into observed performance; the MSO technique performs best, followed by belief construction and direct automata manipulation. As a byproduct of our study, we provide a general synthesis procedure for arbitrary QLTLf specifications. 

**Abstract (ZH)**: 我们研究了在某些输入变量不可靠的情况下，实现LTLf目标规范的同时，至少能确保满足一个LTLf备用规范的问题。我们正式定义了该问题，并将其最坏情况的复杂性归类为2EXPTIME-完全，类似于标准LTLf综合。然后我们提出了三种不同的解决方案：一种基于直接自动机操作的方法，复杂度为2EXPTIME；一种通过信念构建忽略不可靠的输入变量，其复杂度为3EXPTIME；一种利用量化LTLf（QLTLf），其复杂度同样为2EXPTIME，并允许直接编码到单态量化逻辑中，后者最坏情况下的复杂性是非元素阶的。我们证明了这些解决方案的正确性，并通过实验评估了它们之间的性能。有趣的是，理论上的最坏情况界并不反映实际性能；单态量化逻辑（MSO）技术表现最佳，其次是信念构建和直接自动机操作。作为研究的副产品，我们提供了一种通用的QLTLf规范综合方法。 

---
# Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A Survey 

**Title (ZH)**: 基于人工智能驱动的智能空间创建以提升室内环境——综述 

**Authors**: Aygün Varol, Naser Hossein Motlagh, Mirka Leino, Sasu Tarkoma, Johanna Virkki  

**Link**: [PDF](https://arxiv.org/pdf/2412.14708)  

**Abstract**: Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward. 

**Abstract (ZH)**: 智能空间是集成了多种传感和通信技术的无处不在的计算环境，旨在增强空间功能、优化能源利用并提升用户舒适度和福祉。将新兴的人工智能方法集成到这些环境中，促进了人工智能驱动的智能空间的形成，从而通过实现个性化舒适设置、互动生活空间以及空间系统的自动化等高级应用，进一步增强了空间功能，最终提升了用户的室内体验。在本文中，我们系统地概述了人工智能驱动智能空间的基础组成部分，包括传感器技术、数据通信协议、传感器网络管理与维护策略，以及数据采集、处理和分析。鉴于人工智能在建立人工智能赋能智能空间中的关键作用，我们探讨了传统机器学习（ML）方法，如深度学习（DL），以及新兴方法，如大规模语言模型（LLMs）所面临的机会与挑战。最后，我们提供了关于人工智能驱动智能空间发展所需的关键见解，提出了未来的研究方向，并指明了前进的道路。 

---
# Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines 

**Title (ZH)**: Bel Esprit：多agent框架构建AI模型流水线 

**Authors**: Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf  

**Link**: [PDF](https://arxiv.org/pdf/2412.14684)  

**Abstract**: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at this https URL. 

**Abstract (ZH)**: 随着对人工智能（AI）需求的增长，以解决复杂的现实世界任务，单一模型往往不足以应对，因此需要将多个模型整合到管道中。本文介绍了一种名为Bel Esprit的对话型代理，它旨在根据用户定义的要求构建AI模型管道。Bel Esprit采用多代理框架，其中子代理协作以澄清要求、构建、验证并用合适的模型填充管道。我们通过使用人类精选和合成数据展示了该框架在从模糊用户查询中生成管道方面的有效性。详细的错误分析指出了管道构建过程中仍存在的挑战。Bel Esprit可以在以下网址免费试用：[此处填写网址]。 

---
# Relational Programming with Foundation Models 

**Title (ZH)**: 使用基础模型进行关系编程 

**Authors**: Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik  

**Link**: [PDF](https://arxiv.org/pdf/2412.14515)  

**Abstract**: Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines. 

**Abstract (ZH)**: 基础模型具有广泛潜力，可推动多种人工智能应用的发展。这些模型虽然强大但并不完整，这激发了各种机制以增强其功能，如上下文学习、信息检索和代码解释。我们提出了一种名为Vieira的声明性框架，该框架将这些机制统一在一个通用解决方案中，用于在基础模型上进行编程。Vieira遵循概率关系范式，并将基础模型视为无状态函数，具有关系型输入和输出。该框架通过使神经符号应用成为可能，从而能够无缝结合这些模型与逻辑程序，以及通过简化多种子模型的组合来支持复杂、多模态应用。

我们通过在Scallop编译器中扩展一个对外接口来实现Vieira，该接口支持将基础模型作为插件。我们为此实现了12个基础模型的插件，包括GPT、CLIP和SAM。我们对Vieira进行了评估，涉及9个跨语言、视觉和结构化及向量数据库的挑战性任务。评估结果表明，Vieira中的程序简洁、能够融合现代基础模型，并且在准确度上与竞争性基线相当或更好。 

---
# The Digital Ecosystem of Beliefs: does evolution favour AI over humans? 

**Title (ZH)**: 信念数字生态系统：人工智能会取代人类吗？ 

**Authors**: David M. Bossens, Shanshan Feng, Yew-Soon Ong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14500)  

**Abstract**: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on this http URL understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles. 

**Abstract (ZH)**: 随着人工智能系统被整合到社交网络中，人们开始关注AI安全问题，例如AI生成的内容可能在受欢迎程度或影响力上占据主导地位。为进一步理解和探讨这些问题，本文提出了数字信念生态系统（Digico）框架，这是首个用于在模拟社交网络中进行多人群体交互受控实验的演化框架。该框架通过普遍达尔文主义方法模拟了一种群体实体，这些实体由于进化更新而调整其消息策略，通过消息进行交互，在基于传染病模型的动力学基础上互相影响信念，并通过认知拉马克式遗传维持其信念。

初步实验表明：a) 当AI的信息传输、进化速度更快，并在推荐算法中具有更大影响力时，它们可以获得80%到95%的浏览量，具体比例取决于影响力收益的大小；b) 设计用于宣传的AI通常可以说服50%的人类采用极端信念，当代理只相信有限数量的渠道时，这个比例可以达到85%；c) 对违反代理信念的内容进行惩罚可使宣传活动的有效性降低8%。

此外，本文还讨论了Digico框架对于控制（例如立法）的影响及其作为研究进化原则手段的作用。 

---
# FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis 

**Title (ZH)**: FaultExplainer：利用大型语言模型进行可解释的故障检测与诊断 

**Authors**: Abdullah Khan, Rahul Nahar, Hao Chen, Gonzalo E. Constante Flores, Can Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14492)  

**Abstract**: Machine learning algorithms are increasingly being applied to fault detection and diagnosis (FDD) in chemical processes. However, existing data-driven FDD platforms often lack interpretability for process operators and struggle to identify root causes of previously unseen faults. This paper presents FaultExplainer, an interactive tool designed to improve fault detection, diagnosis, and explanation in the Tennessee Eastman Process (TEP). FaultExplainer integrates real-time sensor data visualization, Principal Component Analysis (PCA)-based fault detection, and identification of top contributing variables within an interactive user interface powered by large language models (LLMs). We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not to mimic the challenge of previously unseen faults. Experimental results using GPT-4o and o1-preview models demonstrate the system's strengths in generating plausible and actionable explanations, while also highlighting its limitations, including reliance on PCA-selected features and occasional hallucinations. 

**Abstract (ZH)**: 机器学习算法在化工过程的故障检测与诊断（FDD）中得到了越来越多的应用。然而，现有的数据驱动的FDD平台往往缺乏工艺操作人员可解释性，并且难以识别未见故障的根本原因。本文提出了一种名为FaultExplainer的交互式工具，旨在改善泰勒纳曼过程（TEP）中的故障检测、诊断和解释。FaultExplainer将实时传感器数据可视化、基于主成分分析（PCA）的故障检测以及关键影响变量的识别集成到了一个由大规模语言模型（LLMs）驱动的交互用户界面中。我们通过两种场景评估了LLMs的推理能力：一种场景是提供历史根本原因，另一种场景则不提供根本原因，以模拟未见故障的挑战。使用GPT-4o和o1-preview模型的实验结果表明，该系统在生成合理且可操作的解释方面具有优势，同时也指出了其局限性，包括对PCA选择特征的依赖性和偶尔的幻觉现象。 

---
# Mediation Analysis for Probabilities of Causation 

**Title (ZH)**: 原因概率的中介分析 

**Authors**: Yuta Kawakami, Jin Tian  

**Link**: [PDF](https://arxiv.org/pdf/2412.14491)  

**Abstract**: Probabilities of causation (PoC) offer valuable insights for informed decision-making. This paper introduces novel variants of PoC-controlled direct, natural direct, and natural indirect probability of necessity and sufficiency (PNS). These metrics quantify the necessity and sufficiency of a treatment for producing an outcome, accounting for different causal pathways. We develop identification theorems for these new PoC measures, allowing for their estimation from observational data. We demonstrate the practical application of our results through an analysis of a real-world psychology dataset. 

**Abstract (ZH)**: 因果概率（PoC）为知情决策提供了宝贵的见解。本文介绍了因果概率控制下的新型直接因果概率、自然直接因果概率以及自然间接因果必要性和充分性概率（PNS）的新变体。这些度量指标量化了治疗生产结果的必要性和充分性，考虑了不同因果路径。我们开发了这些新PoC度量的识别定理，使其能够从观测数据中进行估计。我们通过分析一个实际的心理学数据集展示了这些结果的实际应用。 

---
# Towards Projected and Incremental Pseudo-Boolean Model Counting 

**Title (ZH)**: 面向投影和增量伪布尔模型计数 

**Authors**: Suwei Yang, Kuldeep S. Meel  

**Link**: [PDF](https://arxiv.org/pdf/2412.14485)  

**Abstract**: Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption.
In this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40x the number of benchmarks of competing methods for projected model counting and at least 1.18x of competing methods in incremental model counting. 

**Abstract (ZH)**: 模型计数是计算逻辑公式（通常为合取范式CNF）的满足赋值数量的基本任务。尽管在过去几十年中，CNF模型计数受到了广泛的关注，但对伪布尔（PB）模型计数的兴趣才刚刚兴起，这主要得益于PB公式更大的灵活性。因此，我们观察到现有的PB计数器在支持投影和增量计算方面存在特征差距，这可能会影响其应用。

在本项工作中，我们的主要贡献是引入了第一个支持投影和增量模型计数的精确伪布尔计数器PBCount2。我们的计数器PBCount2利用了我们提出的Least Occurrence Weighted Min Degree (LOW-MD) 计算排序启发式算法来支持投影模型计数，并使用缓存机制来实现增量模型计数。在我们的评估中，PBCount2在投影模型计数方面完成了至少1.40倍于竞争对手方法的数量的基准测试，在增量模型计数方面则完成了至少1.18倍于竞争对手方法的数量的基准测试。 

---
# Multi-task Representation Learning for Mixed Integer Linear Programming 

**Title (ZH)**: 混合整数线性规划的多任务表示学习 

**Authors**: Junyang Cai, Taoan Huang, Bistra Dilkina  

**Link**: [PDF](https://arxiv.org/pdf/2412.14409)  

**Abstract**: Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools for modeling and solving complex real-world combinatorial optimization problems. Recently, machine learning (ML)-guided approaches have demonstrated significant potential in improving MILP-solving efficiency. However, these methods typically rely on separate offline data collection and training processes, which limits their scalability and adaptability. This paper introduces the first multi-task learning framework for ML-guided MILP solving. The proposed framework provides MILP embeddings helpful in guiding MILP solving across solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and Solver configuration). Through extensive experiments on three widely used MILP benchmarks, we demonstrate that our multi-task learning model performs similarly to specialized models within the same distribution. Moreover, it significantly outperforms them in generalization across problem sizes and tasks. 

**Abstract (ZH)**: 混合整数线性规划模型（MILPs）是用于建模和解决复杂现实世界组合优化问题的强大而灵活的工具。最近，机器学习（ML）指导的方法在提高MILP求解效率方面显示出巨大的潜力。然而，这些方法通常依赖于分离的离线数据收集和训练过程，这限制了它们的可扩展性和适应性。本文提出了首个用于ML指导的MILP求解的多任务学习框架。所提出的框架提供了对求解器（如Gurobi和SCIP）和任务（如分支策略和求解器配置）跨域的MILP嵌入。通过在三个广泛使用的MILP基准上的大量实验，我们表明，我们的多任务学习模型在性能上与相同分布内的专业化模型相当。此外，它在问题规模和任务上的泛化能力显著优于它们。 

---
# Clinical Trials Ontology Engineering with Large Language Models 

**Title (ZH)**: 使用大型语言模型进行临床试验本体工程 

**Authors**: Berkan Çakır  

**Link**: [PDF](https://arxiv.org/pdf/2412.14387)  

**Abstract**: Managing clinical trial information is currently a significant challenge for the medical industry, as traditional methods are both time-consuming and costly. This paper proposes a simple yet effective methodology to extract and integrate clinical trial data in a cost-effective and time-efficient manner. Allowing the medical industry to stay up-to-date with medical developments. Comparing time, cost, and quality of the ontologies created by humans, GPT3.5, GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM) are a viable option to automate this process both from a cost and time perspective. This study underscores significant implications for medical research where real-time data integration from clinical trials could become the norm. 

**Abstract (ZH)**: 目前，管理临床试验信息是医疗行业面临的一项重大挑战，因为传统方法既耗时又昂贵。本文提出了一种简单而有效的方法，能够在成本和时间上高效地提取和整合临床试验数据，从而使医疗行业能够及时了解医学发展动态。通过对由人类创建、GPT3.5、GPT4和Llama3（8b及70b）生成的本体在时间、成本和质量方面的比较研究表明，大型语言模型（LLM）从成本和时间角度来看，是一种可行的自动化选项。本研究强调了在医学研究中，从临床试验中实时集成数据可能成为常态的重要意义。 

---
# Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem 

**Title (ZH)**: Balans：基于多臂老虎机的自适应大邻域搜索方法用于混合整数规划问题 

**Authors**: Junyang Cai, Serdar Kadioglu, Bistra Dilkina  

**Link**: [PDF](https://arxiv.org/pdf/2412.14382)  

**Abstract**: Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown potential to speed up MIP solving via offline training that then guides important design decisions during search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose Balans, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, Balans is based on adaptive large-neighborhood search, operating on top of a MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that Balans offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release Balans as a highly configurable, MIP solver agnostic, open-source software. 

**Abstract (ZH)**: 混合整数规划（MIP）是一种强大的建模和求解各类重要组合优化问题的范式。近年来，基于学习的方法已被证明能够通过离线训练加速MIP求解，在搜索过程中指导重要的设计决策。然而，这些方法的一个显著缺点是它们对离线训练的重度依赖，这需要收集大量的训练数据并进行计算成本高昂的训练批次，但只能为未见过的（更大的）实例提供有限的一般化能力。在本文中，我们提出了一种名为Balans的自适应元求解器，它具有在线学习能力，不需要任何监督或先验训练。其核心思想是基于自适应的大邻域搜索，在MIP求解器的基础上，通过逐步应用破坏和修复邻域操作符。在搜索过程中，通过多臂老虎机算法实时指导不同邻域定义的选择，以适应当前实例的需求。我们在一系列困难优化问题上的实验表明，与默认的MIP求解器相比，Balans提供了显著的性能提升，优于单一最优邻域的选择，并改进了现有的MIP的大邻域搜索算法。最后，我们发布了一种高度可配置的、与MIP求解器无关的开放源代码软件Balans。 

---
# Python Agent in Ludii 

**Title (ZH)**: Python代理在Ludii中的应用 

**Authors**: Izaias S. de Lima Neto, Marco A. A. de Aguiar Vieira, Anderson R. Tavares  

**Link**: [PDF](https://arxiv.org/pdf/2412.14372)  

**Abstract**: Ludii is a Java general game system with a considerable number of board games, with an API for developing new agents and a game description language to create new games. To improve versatility and ease development, we provide Python interfaces for agent programming. This allows the use of Python modules to implement general game playing agents.
As a means of enabling Python for creating Ludii agents, the interfaces are implemented using different Java libraries: jpy and Py4J. The main goal of this work is to determine which version is faster. To do so, we conducted a performance analysis of two different GGP algorithms, Minimax adapted to GGP and MCTS. The analysis was performed across several combinatorial games with varying depth, branching factor, and ply time. For reproducibility, we provide tutorials and repositories.
Our analysis includes predictive models using regression, which suggest that jpy is faster than Py4J, however slower than a native Java Ludii agent, as expected. 

**Abstract (ZH)**: Ludii 是一个基于 Java 的通用游戏系统，包含大量的棋盘游戏，并提供开发新代理的 API 以及创建新游戏的游戏描述语言。为了提高通用性和简化开发过程，我们提供了 Python 接口以便使用 Python 编程实现代理。这样可以利用 Python 模块来实现通用游戏代理。

为了使 Python 能够用于创建 Ludii 代理，接口是通过不同的 Java 库 jpy 和 Py4J 实现的。本工作的主要目标是确定哪个版本更快速。为此，我们对两种不同的广义游戏协议 (General Game Playing, GGP) 算法（适配 GGP 的 Minimax 算法和 Monte Carlo Tree Search, MCTS）进行了性能分析。分析在多个具有不同深度、分支因子和回合时间的组合游戏中进行。为了可再现性，我们提供了教程和仓库。

我们的分析包括使用回归建立的预测模型，这些模型表明 jpy 比 Py4J 快，但比原生 Java Ludii 代理慢，这在预期之中。 

---
# Scaling 4D Representations 

**Title (ZH)**: “扩展四维表示” 

**Authors**: João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman  

**Link**: [PDF](https://arxiv.org/pdf/2412.15212)  

**Abstract**: Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations. 

**Abstract (ZH)**: 纯自监督学习从视频数据中尚未有令人信服的扩展性证明。然而，先前的工作主要集中在与语义相关的任务上，如动作分类、ImageNet分类等。在本文中，我们专注于评估自监督学习在非语义视觉任务上的表现，这些任务更加侧重于空间（3D）和时间（+1D，共4D）特性，如相机姿态估计、点和物体跟踪以及深度估计。我们通过从非常大的视频数据集中学习，展示了通过掩码自编码（MAE）和Transformer视频模型确实实现了扩展性，随着模型规模从20M增加到迄今为止报告的最大自监督视频模型（220亿参数），这些4D任务上的性能得到了持续改进。与许多近期的图像和视频模型进行严格的同质性比较，进一步证明了扩大4D表示形式的好处。 

---
# PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation 

**Title (ZH)**: PRIMA：多张图像的视觉-语言模型用于推理分割 

**Authors**: Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou  

**Link**: [PDF](https://arxiv.org/pdf/2412.15209)  

**Abstract**: Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates pixel-level grounding with robust multi-image reasoning capabilities to produce contextually rich, pixel-grounded explanations. Central to PRIMA is an efficient vision module that queries fine-grained visual representations across multiple images, reducing TFLOPs by $25.3\%$. To support training and evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark consisting of $\sim$224K question-answer pairs that require fine-grained visual understanding across multiple images. Experimental results demonstrate PRIMA outperforms state-of-the-art baselines. 

**Abstract (ZH)**: 尽管在大规模视觉-语言模型（LVLMs）方面取得了显著进展，现有的像素定位模型仅限于单张图像的应用场景，限制了它们在多张图像间的详尽、细致对比能力。相反，当前的多图像理解模型缺乏像素级别的定位能力。我们的工作通过引入多图像像素定位推理分割任务，并提出了一种新颖的LVLM——PRIMA，它将像素级别的定位能力与稳健的多图像推理能力相结合，生成具有丰富上下文的像素定位解释。PRIMA的核心在于一个高效的视觉模块，它可以跨多张图像查询细粒度的视觉表示，将其计算量减少25.3%。为了支持训练和评估，我们构建了M4Seg，这是一个新的推理分割基准，包含约22.4万对需要跨多图像进行细粒度视觉理解的问题-答案对。实验结果证明，PRIMA在对比先进基线模型时表现更优。 

---
# LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks 

**Title (ZH)**: LongBench v2：朝着对现实环境中长上下文多任务有更深的理解和推理方向发展 

**Authors**: Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15204)  

**Abstract**: This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at this https URL. 

**Abstract (ZH)**: 本文介绍了LongBench v2，这是一个基准测试，旨在评估大语言模型（LLMs）在处理需要深入理解和推理的长上下文问题方面的能力，特别是在实际多任务中的表现。LongBench v2 包含了 503 个具有挑战性的多项选择题，上下文范围从 8 千字到 2 百万字，涉及六个主要任务类别：单文档问答、多文档问答、长上下文学习、长对话历史理解、代码仓库理解以及长结构化数据理解。为了确保数据的广泛性和实用性，我们从来自多个专业背景的近 100 名高度教育的个体收集了数据。我们采用自动和手动审查相结合的方式，以保持高质量和难度，最终在 15 分钟的时间限制下，人类专家的准确率仅为 53.7%。我们的评估结果显示，当最佳模型直接回答问题时，其准确率仅为 50.1%。相比之下，o1-preview 模型包含了更长时间的推理，其准确率为 57.7%，超过了人类基准 4%。这些结果突显了增强推理能力和扩展推理时计算量在应对 LongBench v2 中长上下文挑战方面的重要性。该项目可在以下网址访问：[该项目网址]。 

---
# DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation 

**Title (ZH)**: DI-PCG: 基于扩散过程的高效逆过程内容生成方法及其在高质量3D资产创建中的应用 

**Authors**: Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan  

**Link**: [PDF](https://arxiv.org/pdf/2412.15200)  

**Abstract**: Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models. 

**Abstract (ZH)**: 过程化内容生成（PCG）在创建高质量3D内容方面非常强大，但要控制其生成所需的特定形状却颇具挑战性，通常需要进行繁琐的参数调整。逆过程化内容生成旨在在给定输入条件下自动找到最佳参数。然而，现有的基于采样的方法和基于神经网络的方法仍然存在大量的样本迭代或可控性有限的问题。在本文中，我们提出了DI-PCG，这是一种用于从一般图像条件逆向生成过程化内容的新颖且高效的算法。其核心是一个轻量级的扩散变换器模型，在该模型中，PCG参数直接作为去噪目标，观察到的图像作为控制参数生成的条件。DI-PCG高效且有效。仅需7.6M的网络参数和30个GPU小时进行训练，它在准确恢复参数和良好泛化到野外图像方面表现出卓越的性能。定量和定性的实验结果验证了DI-PCG在逆过程化内容生成和图像到3D生成任务中的有效性。DI-PCG为高效逆过程化内容生成提供了有前景的方法，并代表了向基于参数模型构建3D资产的3D生成路径的有意义探索。 

---
# LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation 

**Title (ZH)**: LlamaFusion：适应多模态生成的预训练语言模型 

**Authors**: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15188)  

**Abstract**: We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development. 

**Abstract (ZH)**: 我们提出了LlamaFusion框架，该框架旨在赋予仅预训练文本的大语言模型（LLM）多模态生成能力，使其能够理解和生成任意序列的文本和图像。LlamaFusion利用现有的Llama-3权重进行自回归文本处理，同时引入额外且并行的变压器模块以通过扩散处理图像。在训练过程中，来自不同模态的数据会流向其专用模块：特定于模态的前馈层、查询-键-值投影以及规范化层分别独立处理每个模态，而共享的自注意力层则允许文本和图像特征之间的交互。通过冻结特定于文本的模块，并仅训练特定于图像的模块，LlamaFusion保留了仅文本LLM的语言能力，同时发展了强大的视觉理解与生成能力。与从零开始预训练多模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs即可提升20%的图像理解能力和3.6%的图像生成能力，同时保持Llama-3的语言能力。此外，我们还展示了该框架可以适应已有的具有多模态生成能力的视觉-语言模型。总体而言，该框架不仅利用了现有仅文本LLM的计算投资，还实现了语言与视觉能力的并行开发，为高效的多模态模型开发提供了颇有前景的方向。 

---
# Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration 

**Title (ZH)**: 人类-类人机器人异体行为-技能转移研究：基于分解对抗学习的演示学习 

**Authors**: Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15166)  

**Abstract**: Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms. 

**Abstract (ZH)**: 拟人机器人被设想为具备执行广泛的人类级移动操作任务的体现智能代理，特别是在需要繁重和重复劳动的场景中尤为适用。然而，由于拟人机器人具有高自由度，学习这些技能具有挑战性，而收集充分的训练数据对拟人机器人而言也是一个耗时的过程。鉴于新的拟人平台快速涌现，一种允许通用技能迁移的跨体态框架变得越来越关键。为解决这一问题，我们提出了一种可迁移的框架，通过使用统一的数字人体模型作为通用原型，从而减少数据瓶颈，避免每在新平台重新训练的需要。该模型通过对抗模仿学习从人类示范中学习行为素，复杂的机器人结构被分解为功能组件，各组件独立训练并动态协调。通过人力物体交互图实现任务泛化，并通过特定于体态的动力学运动目标重定位和动态微调将技能迁移到不同的机器人。该框架在五种具有差异配置的拟人机器人上得到了验证，展示了稳定的移动操作，并突出了其在减少数据需求和跨平台提高技能迁移效率方面的有效性。 

---
# Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents 

**Title (ZH)**: 将罗尔斯伦理学应用于规范学习代理的公平性操作化 

**Authors**: Jessica Woodgate, Paul Marshall, Nirav Ajmeri  

**Link**: [PDF](https://arxiv.org/pdf/2412.15163)  

**Abstract**: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics. 

**Abstract (ZH)**: 社会规范是指一个社会中普遍认同的行为标准。然而，当个体在做决策时不考虑其行为对他人的影响时，可能会形成一些压迫某些个体的行为规范。我们提出了一种名为RAWL-E的方法，旨在创建具备伦理规范学习能力的智能体。RAWL-E智能体在其决策过程中通过实现罗尔斯正义伦理学中的“最大化最小者”原则（maximin），在社会福祉与个体目标之间寻求平衡，从而促进伦理规范的形成。我们通过模拟收获场景对RAWL-E智能体进行了评估。研究结果表明，RAWL-E智能体社会中形成的规范提升了社会福利、公平性和稳健性，并且相较于未实施罗尔斯正义伦理的智能体社会，能获得更高的最低经验收益。 

---
# Language Models as Continuous Self-Evolving Data Engineers 

**Title (ZH)**: 语言模型作为连续自我进化的数据工程师 

**Authors**: Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15151)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多种任务上展现了卓越的能力，但进一步的发展受限于高质量训练数据的不足。此外，传统的训练方法过于依赖专家标注的数据，从而对LLMs的性能设立了上限。为了解决这一问题，我们提出了一种新的范式，使LLMs能够自主生成、清洗、审查和注释带有偏好信息的数据，这一过程被命名为LANCE。我们的方法表明，LLMs可以作为持续自我演化的数据工程师，显著减少了后训练数据构建过程的时间和成本。通过在不同版本的Qwen2上进行迭代微调，我们验证了LANCE在各种任务中的有效性，显示其能够持续提升模型性能并维持高质量的数据生成。LANCE在八个基准维度上分别提高了Qwen2-7B和Qwen2-7B-Instruct 3.36和2.70的分数。这种具有自主数据构建的训练范式不仅减少了对人类专家或外部模型的依赖，还确保了数据与人类价值观和偏好的一致性，为未来超智能系统的开发铺平了道路，这些系统能够超越人类的能力。 

---
# Leveraging Color Channel Independence for Improved Unsupervised Object Detection 

**Title (ZH)**: 利用颜色通道独立性以改进无监督目标检测 

**Authors**: Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2412.15150)  

**Abstract**: Object-centric architectures can learn to extract distinct object representations from visual scenes, enabling downstream applications on the object level. Similarly to autoencoder-based image models, object-centric approaches have been trained on the unsupervised reconstruction loss of images encoded by RGB color spaces. In our work, we challenge the common assumption that RGB images are the optimal color space for unsupervised learning in computer vision. We discuss conceptually and empirically that other color spaces, such as HSV, bear essential characteristics for object-centric representation learning, like robustness to lighting conditions. We further show that models improve when requiring them to predict additional color channels. Specifically, we propose to transform the predicted targets to the RGB-S space, which extends RGB with HSV's saturation component and leads to markedly better reconstruction and disentanglement for five common evaluation datasets. The use of composite color spaces can be implemented with basically no computational overhead, is agnostic of the models' architecture, and is universally applicable across a wide range of visual computing tasks and training types. The findings of our approach encourage additional investigations in computer vision tasks beyond object-centric learning. 

**Abstract (ZH)**: 以对象为中心的架构可以从视觉场景中学习提取独特的对象表示，从而在对象级别上启用下游应用。与基于自编码器的图像模型类似，对象为中心的方法是通过编码RGB颜色空间中的图像的无监督重建损失进行训练的。在我们的工作中，我们挑战了RGB图像是最优颜色空间的常见假设，以无监督学习计算机视觉中的最优颜色空间。我们从概念上和实证上讨论了其他颜色空间（如HSV）对于对象为中心的表示学习具有本质特性，例如对光照条件的鲁棒性。我们进一步展示了要求模型预测额外的彩色通道可以提高模型性能。具体而言，我们建议将预测的目标转换到RGB-S空间，该空间在RGB的基础上加入了HSV的饱和度成分，从而在五个常见的评估数据集中显著提高了重建性能和分离度。使用复合颜色空间可以在基本不增加计算开销的情况下实现，不对模型的架构产生依赖，且可以广泛应用于各种视觉计算任务和训练类型。我们方法的研究结果鼓励在对象为中心学习之外的计算机视觉任务中进行更多的研究。 

---
# Jet: A Modern Transformer-Based Normalizing Flow 

**Title (ZH)**: Jet：一种基于Transformer的现代Normalize Flow 

**Authors**: Alexander Kolesnikov, André Susano Pinto, Michael Tschannen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15129)  

**Abstract**: In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models. 

**Abstract (ZH)**: 在过去，正则化生成流已成为一种有前景的生成模型类别，适用于自然界中的图像。这类模型具有许多建模优势：能够高效计算输入数据的对数似然性、快速生成以及结构简单。然而，正则化流后来逐渐受到冷落，因为其生成样本的视觉质量与GANs、基于VQ-VAE的方法或扩散模型等其他模型类别相比不够竞争力。在本文中，我们通过仔细剔除先前的设计选择，并采用基于Vision Transformer架构的计算块而非卷积神经网络，重新审视基于耦合的正则化流模型的设计。结果，我们使用一个更为简单的架构取得了最先进的定量和定性性能。尽管整体视觉质量仍然落后于当前最先进的模型，但我们认为强大的正则化流模型可以通过作为更强大生成模型构建组件的方式推动研究前沿。 

---
# Adaptive Pruning for Large Language Models with Structural Importance Awareness 

**Title (ZH)**: 具有结构重要性意识的大型语言模型自适应剪枝 

**Authors**: Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han  

**Link**: [PDF](https://arxiv.org/pdf/2412.15127)  

**Abstract**: The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在语言理解和生成能力方面取得了显著进步。然而，由于其对计算和存储资源的高需求，很难在资源受限的边缘设备上部署LLMs。为了解决这一问题，我们提出了一种新颖的LLM模型剪枝方法，即结构感知自适应剪枝（SAAP），该方法可大幅降低计算和内存成本，同时保持模型性能。首先，我们定义了一个自适应重要性融合度量，通过考虑耦合结构的同方差不确定性来评估LLMs中所有耦合结构的重要性。然后，我们对所有模块进行排序，确定需要剪枝的具体层以满足特定的性能要求。此外，我们还开发了一种新的分组微调策略，以提高LLMs的推理效率。最后，我们在两个常见任务（零样本分类和文本生成）的多个LLMs上评估了所提出的SAAP方法。实验结果表明，与几种最先进的基线方法相比，我们的SAAP方法在LLaMA-7B、Vicuna-7B和LLaMA-13B上分别取得了2.17%、2.37%和2.39%的准确率提升。此外，SAAP还提高了5%的标记生成速度，在资源受限的场景中展示出其实用优势。 

---
# Outcome-Refining Process Supervision for Code Generation 

**Title (ZH)**: 代码生成中的基于产出细化的过程监督 

**Authors**: Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15118)  

**Abstract**: Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: this https URL 

**Abstract (ZH)**: 大规模语言模型在代码生成方面展现了出色的能力，但在解决需要深入算法推理的复杂编程任务时往往表现不佳。通过学习奖励模型进行过程监督虽然显示出一定的潜力，但需要昂贵的训练数据，并且评价结果不够可靠。我们提出了一种新的方法——结果细化过程监督，将结果细化本身作为要监督的过程。我们的框架利用具体的执行信号来对推理步骤进行监督，同时采用树状探索结构来同时保持多个解决方案轨迹。实验结果表明，我们的方法能使即使较小的模型在竞争编程任务中实现高准确性和性能指标，提供比传统奖励模型更可靠的验证，而不需要训练PRMs。我们的方法在5个模型和3个数据集上均取得了显著提高：正确率平均提高了26.9%，效率提高了42.2%。结果表明，提供结构化的推理空间并结合具体的验证信号对于解决复杂编程任务至关重要。我们开源了所有代码和数据：[此链接](this https URL) 

---
# Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture 

**Title (ZH)**: 关联记忆启发了一种新型注意残差流架构在上下文学习中的改进 

**Authors**: Thomas F Burns, Tomoki Fukai, Christopher J Earls  

**Link**: [PDF](https://arxiv.org/pdf/2412.15113)  

**Abstract**: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale. 

**Abstract (ZH)**: 大型语言模型（LLMs）展示了在输入序列的上下文中利用信息的能力，以适当地响应训练过程中未见过的数据。这种能力称为上下文内学习（ICL）。人类和非人类动物也展示了类似的能力，然而它们的神经架构与LLMs有显著差异。尽管如此，LLMs中的一个关键组件——注意机制——与现代联想记忆模型相似，这些模型在计算神经科学领域广泛使用并受到其影响，用于模拟生物记忆系统。基于这种联系，我们提出了一种联想记忆模型，该模型能够执行ICL。我们以此为灵感，引入了一种新的残差流架构，允许信息直接在注意头之间流动。我们在两层Transformer模型的训练过程中测试了这种架构，并显示了这种修改使得ICL能力表现得更快。随后，我们在具有800万个参数的小型语言模型中应用了这种架构，专注于注意头的值，结果显示在更大和更自然的规模上，这种架构也表现出增强的ICL性能。 

---
# Exploiting sparse structures and synergy designs to advance situational awareness of electrical power grid 

**Title (ZH)**: 利用稀疏结构和协同设计提高电力 grids 的情境感知能力 

**Authors**: Shimiao Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15105)  

**Abstract**: The growing threats of uncertainties, anomalies, and cyberattacks on power grids are driving a critical need to advance situational awareness which allows system operators to form a complete and accurate picture of the present and future state. Simulation and estimation are foundational tools in this process. However, existing tools lack the robustness and efficiency required to achieve the level of situational awareness needed for the ever-evolving threat landscape. Industry-standard (steady-state) simulators are not robust to blackouts, often leading to non-converging or non-actionable results. Estimation tools lack robustness to anomalous data, returning erroneous system states. Efficiency is the other major concern as nonlinearities and scalability issues make large systems slow to converge.
This thesis addresses robustness and efficiency gaps through a dual-fold contribution. We first address the inherent limitations in the existing physics-based and data-driven worlds; and then transcend the boundaries of conventional algorithmic design in the direction of a new paradigm -- Physics-ML Synergy -- which integrates the strengths of the two worlds. Our approaches are built on circuit formulation which provides a unified framework that applies to both transmission and distribution. Sparse optimization acts as the key enabler to make these tools intrinsically robust and immune to random threats, pinpointing dominant sources of (random) blackouts and data errors. Further, we explore sparsity-exploiting optimizations to develop lightweight ML models whose prediction and detection capabilities are a complement to physics-based tools; and whose lightweight designs advance generalization and scalability. Finally, Physics-ML Synergy brings robustness and efficiency further against targeted cyberthreats, by interconnecting our physics-based tools with lightweight ML. 

**Abstract (ZH)**: 日益增长的不确定性、异常情况和网络攻击对电网的安全构成重大威胁，这推动了对情境意识的迫切需求，使系统操作员能够全面、准确地了解当前和未来的情况。仿真和估计是实现这一目的的基本工具。然而，现有的工具在现阶段的威胁环境中缺乏足够的稳健性和效率。传统的稳态仿真器对断电不具有鲁棒性，经常导致结果不收敛或不可行。现有的估计工具对异常数据缺乏鲁棒性，会返回错误的系统状态。效率也是一个主要问题，因为非线性和扩展性问题使得大型系统难以快速收敛。

本论文通过双重贡献来解决稳健性和效率的劣势。首先，我们解决现有的基于物理模型和数据驱动方法的固有限制；然后跨越传统算法设计的界限，转向一个新的范式——物理-机器学习协同（Physics-ML Synergy），该范式结合了两者的强项。我们的方法基于电路公式，提供了一个适用于传输和分配的统一框架。稀疏优化作为关键使能技术，使这些工具具备内在的鲁棒性，能够抵御随机威胁，并指出主要的断电和数据错误来源。此外，我们探索了利用稀疏性的优化技术来开发轻量级的机器学习模型，这些模型在预测和检测方面能补充基于物理的方法的功能，并通过轻量级设计提高泛化能力和可扩展性。最后，物理-机器学习协同效应通过将基于物理的方法与轻量级机器学习相互连接，进一步提升了对针对性的网络威胁的稳健性和效率。 

---
# A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation 

**Title (ZH)**: 跨领域的在线误导信息中劝说技巧使用的研究 

**Authors**: João A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva  

**Link**: [PDF](https://arxiv.org/pdf/2412.15098)  

**Abstract**: Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts. 

**Abstract (ZH)**: 不论是在哪个领域或使用哪种语言，虚假信息的目的是欺骗或操纵公众意见，通常通过运用先进的说服技术来实现。关于说服技术在虚假信息中武器化方面的定性和定量研究主要集中在特定主题上（例如新冠肺炎），跨领域的研究相对较少，导致对这些策略的理解不够全面。本研究采用最先进的说服技术分类器，对16种说服技术在多领域虚假信息叙事中的作用进行了大规模分析。结果显示，这些说服技术在不同虚假信息领域中的应用程度不均衡。我们还通过一个详细的案例研究探讨了气候变化虚假信息，强调了语言、心理和文化因素如何影响说服策略的适应性，以便更好地契合特定主题背景。 

---
# A Full Transformer-based Framework for Automatic Pain Estimation using Videos 

**Title (ZH)**: 基于全变压器架构的视频自动疼痛估计完整框架 

**Authors**: Stefanos Gkikas, Manolis Tsiknakis  

**Link**: [PDF](https://arxiv.org/pdf/2412.15095)  

**Abstract**: The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks. 

**Abstract (ZH)**: 自动估计疼痛对于设计一个提供可靠评估并减轻患者痛苦的最优疼痛管理系统至关重要。在本研究中，我们提出了一种新颖的全变压器架构，该架构由Transformer in Transformer (TNT) 模型和利用交叉注意力和自注意力模块的变压器组成。通过对BioVid数据库中的视频进行详细分析，我们展示了该框架在所有主要疼痛估计任务上的先进性能、有效性和跨任务的一般化能力。 

---
# Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation 

**Title (ZH)**: 学习解耦同构表示以实现显式可控的3D分子生成 

**Authors**: Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min  

**Link**: [PDF](https://arxiv.org/pdf/2412.15086)  

**Abstract**: We consider the conditional generation of 3D drug-like molecules with \textit{explicit control} over molecular properties such as drug-like properties (e.g., Quantitative Estimate of Druglikeness or Synthetic Accessibility score) and effectively binding to specific protein sites. To tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and factorize the latent space of our generative model into two disentangled aspects: molecular properties and the remaining structural context of 3D molecules. Our model ensures explicit control over these molecular attributes while maintaining equivariance of coordinate representation and invariance of data likelihood. Furthermore, we introduce a novel alignment-based coordinate loss to adapt equivariant networks for auto-regressive de-novo 3D molecule generation from scratch. Extensive experiments validate our model's effectiveness on property-guided and context-guided molecule generation, both for de-novo 3D molecule design and structure-based drug discovery against protein targets. 

**Abstract (ZH)**: 我们考虑在明确控制分子属性（如药效性质（例如，定量药效类比性估量或合成可及性评分）和有效结合到特定蛋白质位点）的情况下，生成3D药物样分子。为了解决这一问题，我们提出了一种E(3)-不变的Wasserstein自编码器，并将生成模型的潜在空间分解为两个独立方面：分子属性和3D分子的剩余结构上下文。我们的模型确保了对这些分子属性的明确控制，同时保持了坐标表示的不变性和数据似然性的不变性。此外，我们引入了一种新的基于对齐的坐标损失，以适应E(3)-不变网络，用于从头开始的自动回归3D分子生成。广泛的实验验证了我们模型在属性引导和上下文引导的分子生成方面（包括从头设计3D药物分子和基于结构的药物发现对抗蛋白质靶标）的有效性。 

---
# AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling 

**Title (ZH)**: AceMath: 通过后训练和奖励建模推进前沿数学推理 

**Authors**: Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping  

**Link**: [PDF](https://arxiv.org/pdf/2412.15084)  

**Abstract**: In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: this https URL 

**Abstract (ZH)**: 在本文中，我们介绍了AceMath，这是一套表现卓越的前沿数学模型，能够解决复杂数学问题，并且配备了高效的奖励模型以评估生成的解决方案并可靠地识别正确的答案。为了开发指令调优的数学模型，我们提出了一种监督微调（SFT）过程，该过程首先在通用领域中实现竞争力，然后通过精心策划的提示和合成生成的响应针对数学领域进行目标化微调。由此产生的模型AceMath-72B-Instruct在各方面显著优于Qwen2.5-Math-72B-Instruct、GPT-4o和Claude-3.5 Sonnet。为了开发专门化的数学奖励模型，我们首先构建了AceMath-RewardBench，这是一个全面且 robust 的基准，用于在多样化的问题和难度级别下评估数学奖励模型。之后，我们提出了构建数学奖励模型的系统方法。由此产生的模型AceMath-72B-RM持续优于最先进的奖励模型。此外，当我们结合使用AceMath-72B-Instruct与AceMath-72B-RM时，我们在数学推理基准测试中实现了最高的平均rm@8得分。我们将释放模型权重、训练数据和评估基准，请访问以下链接：this https URL 

---
# GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and Facilitative Playbacks Evaluation 

**Title (ZH)**: GIRAFE：声门成像数据集，用于高级分割、分析及辅助回放评估 

**Authors**: G. Andrade-Miranda, K. Chatzipapas, J.D. Arias-Londoño, J. I. Godino-Llorente  

**Link**: [PDF](https://arxiv.org/pdf/2412.15054)  

**Abstract**: The advances in the development of Facilitative Playbacks extracted from High-Speed videoendoscopic sequences of the vocal folds are hindered by a notable lack of publicly available datasets annotated with the semantic segmentations corresponding to the area of the glottal gap. This fact also limits the reproducibility and further exploration of existing research in this field.
To address this gap, GIRAFE is a data repository designed to facilitate the development of advanced techniques for the semantic segmentation, analysis, and fast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The repository includes 65 high-speed videoendoscopic recordings from a cohort of 50 patients (30 female, 20 male). The dataset comprises 15 recordings from healthy controls, 26 from patients with diagnosed voice disorders, and 24 with an unknown health condition. All of them were manually annotated by an expert, including the masks corresponding to the semantic segmentation of the glottal gap. The repository is also complemented with the automatic segmentation of the glottal area using different state-of-the-art approaches.
This data set has already supported several studies, which demonstrates its usefulness for the development of new glottal gap segmentation algorithms from High-Speed-Videoendoscopic sequences to improve or create new Facilitative Playbacks. Despite these advances and others in the field, the broader challenge of performing an accurate and completely automatic semantic segmentation method of the glottal area remains open. 

**Abstract (ZH)**: 高通量内窥镜声带序列中促进性回放的进展受到缺乏公开可用的带有声门间隙语义标注的数据集的限制。这一事实也限制了现有研究的可重复性和进一步探索。

为了填补这一空白，GIRAFE 是一个数据仓库，旨在促进声带高通量内窥镜序列的语义分割、分析和快速评估的先进技术的发展。该仓库包括来自 50 名患者（30 名女性、20 名男性）的 65 段高通量内窥镜录像。数据集包括来自 30 名健康对照的 15 段录像、来自诊断为语音障碍的患者的 26 段录像以及来自未知健康状况的 24 段录像。所有这些录像都由专家手动标注，包括与声门间隙语义分割对应的掩码。此外，该数据仓库还提供了使用不同前沿方法自动分割声门区域的数据。

该数据集已支持了多项研究，这证明了其在从高通量视频内窥镜序列开发新的声门间隙分割算法、以改进或创建新的促进性回放方面的有用性。尽管该领域在这些进展及其他方面取得了一些成果，但对声门区域进行精确且完全自动的语义分割方法的挑战仍然存在。 

---
# Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI 

**Title (ZH)**: 用AI衡量、建模以及帮助人们评估在线自我披露中的隐私风险 

**Authors**: Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das  

**Link**: [PDF](https://arxiv.org/pdf/2412.15047)  

**Abstract**: In pseudonymous online fora like Reddit, the benefits of self-disclosure are often apparent to users (e.g., I can vent about my in-laws to understanding strangers), but the privacy risks are more abstract (e.g., will my partner be able to tell that this is me?). Prior work has sought to develop natural language processing (NLP) tools that help users identify potentially risky self-disclosures in their text, but none have been designed for or evaluated with the users they hope to protect. Absent this assessment, these tools will be limited by the social-technical gap: users need assistive tools that help them make informed decisions, not paternalistic tools that tell them to avoid self-disclosure altogether. To bridge this gap, we conducted a study with N = 21 Reddit users; we had them use a state-of-the-art NLP disclosure detection model on two of their authored posts and asked them questions to understand if and how the model helped, where it fell short, and how it could be improved to help them make more informed decisions. Despite its imperfections, users responded positively to the model and highlighted its use as a tool that can help them catch mistakes, inform them of risks they were unaware of, and encourage self-reflection. However, our work also shows how, to be useful and usable, AI for supporting privacy decision-making must account for posting context, disclosure norms, and users' lived threat models, and provide explanations that help contextualize detected risks. 

**Abstract (ZH)**: 在像Reddit这样的匿名在线论坛中，自我披露的好处对用户来说往往是明显的（例如，我可以向理解我的陌生人们倾诉我的岳父母问题），而隐私风险则是更加抽象的（例如，我的伴侣能猜出这是我在发言吗？）。先前的研究试图开发自然语言处理（NLP）工具来帮助用户识别他们文本中可能存在的风险性自我披露，但这些工具并未为他们希望保护的用户进行设计或评估。缺乏这种评估，这些工具将受限于社会技术差距：用户需要的是能够帮助他们做出知情决策的支持工具，而不是迫使他们完全避免自我披露的指导性工具。为了弥合这一差距，我们对21位Reddit用户进行了研究；我们让他们使用最新的NLP披露检测模型对两个自己撰写的帖子进行了处理，并询问他们关于模型的作用、不足之处以及如何改进以帮助他们做出更加知情的决策。尽管该模型存在缺陷，用户们还是对其持积极态度，并指出它在帮助他们发现错误、告知他们原本不了解的风险以及促进自我反思方面的作用。然而，我们的研究也展示了，为了真正有用并易于使用，支持隐私决策的AI必须考虑到发帖背景、披露规范以及用户的实际威胁模型，并提供解释来帮助解释检测到的风险。 

---
# Large Language Models and Code Security: A Systematic Literature Review 

**Title (ZH)**: 大型语言模型与代码安全：一项系统文献综述 

**Authors**: Enna Basic, Alberto Giaretta  

**Link**: [PDF](https://arxiv.org/pdf/2412.15004)  

**Abstract**: Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）已成为自动化各种编程任务的强大工具，包括安全相关的任务，如检测和修复漏洞。尽管它们具备强大的能力，但在生成或修改现有代码时，LLMs 仍有可能引入未知的漏洞。在分析代码时，LLMs 可能会忽略明显的漏洞，或者错误地标记不存在的漏洞。在本次系统性文献综述（SLR）中，我们旨在研究将LLMs应用于各种代码相关任务的安全优势和潜在缺点。首先，我们将重点关注LLMs在生成代码时可能引入的漏洞类型。其次，我们将分析LLMs在任意给定代码中检测和修复漏洞的能力，以及不同的提示策略如何影响它们在这两项任务中的性能。最后，我们将深入分析针对LLMs的数据投毒攻击如何影响上述任务的性能。 

---
# HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs 

**Title (ZH)**: HSEvo：通过多样性驱动的和谐搜索和遗传算法利用大规模语言模型自动启发式设计的提升 

**Authors**: Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh  

**Link**: [PDF](https://arxiv.org/pdf/2412.14995)  

**Abstract**: Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo's reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS. 

**Abstract (ZH)**: 自动启发式设计（AHD）是一个活跃的研究领域，因为它在解决实际中的复杂搜索和NP难组合优化问题方面具有重要作用。最近，大型语言模型（LLMs）的发展为通过将LLMs与进化计算相结合来自动生成启发式方法提供了新的可能性，这种方法被称为基于大语言模型的进化程序搜索（LLM-EPS）。尽管先前的LLM-EPS研究在各种任务上取得了出色的性能，但在了解启发式搜索空间的特性以及在大启发式搜索空间中实现探索与利用之间的平衡方面仍存在差距，这是关键因素之一。在这项研究中，我们通过提出两种多样性的度量标准，并对先前的LLM-EPS方法进行分析，包括FunSearch、EoH和ReEvo，来解决这一差距。在黑盒AHD问题上的结果表明，尽管EoH的多样性高于FunSearch和ReEvo，但其目标评分不稳定。相反，ReEvo的反射机制获得了较好的目标评分，但在有效优化多样性方面却存在问题。基于这一发现，我们引入了一个适应性的LLM-EPS框架——HSEvo——它通过使用和谐搜索算法在多样性和收敛性之间保持平衡。通过实验，我们发现HSEvo在保持较高多样性指数和良好目标评分的同时，还具有成本效益。这些结果强调了在LLM-EPS框架设计中平衡探索与利用以及理解启发式搜索空间的重要性。 

---
# Movie2Story: A framework for understanding videos and telling stories in the form of novel text 

**Title (ZH)**: Movie2Story：一个理解视频并以新颖文本形式讲述故事的框架 

**Authors**: Kangning Li, Zheyang Jia, Anyu Ying  

**Link**: [PDF](https://arxiv.org/pdf/2412.14965)  

**Abstract**: Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research. 

**Abstract (ZH)**: 多模态视频到文本模型在生成简短的视频内容描述方面取得了显著进展。然而，仍存在生成丰富的长篇描述性文本的不足，这些描述性文本需要结合视频和音频信息。本文介绍了一种名为M2S的框架，旨在通过结合音频、视频和字符识别来生成新的文本长度描述。M2S包括用于生成和理解视频长篇文本描述的模块、基于音频的情绪分析、语速分析和角色对齐分析，以及基于视觉的字符识别对齐。通过使用大型语言模型GPT4o整合多模态信息，M2S在多模态文本生成领域脱颖而出。我们通过对比实验和人工评估展示了M2S的有效性和准确性。此外，该模型框架具有良好的扩展性，并且具有未来研究的巨大潜力。 

---
# Cirbo: A New Tool for Boolean Circuit Analysis and Synthesis 

**Title (ZH)**: Cirbo：一种新的布尔电路分析与综合工具 

**Authors**: Daniil Averkov, Tatiana Belova, Gregory Emdin, Mikhail Goncharov, Viktoriia Krivogornitsyna, Alexander S. Kulikov, Fedor Kurmazov, Daniil Levtsov, Georgie Levtsov, Vsevolod Vaskin, Aleksey Vorobiev  

**Link**: [PDF](https://arxiv.org/pdf/2412.14933)  

**Abstract**: We present an open-source tool for manipulating Boolean circuits. It implements efficient algorithms, both existing and novel, for a rich variety of frequently used circuit tasks such as satisfiability, synthesis, and minimization. We tested the tool on a wide range of practically relevant circuits (computing, in particular, symmetric and arithmetic functions) that have been optimized intensively by the community for the last three years. The tool helped us to win the IWLS 2024 Programming Contest. In 2023, it was Google DeepMind who took the first place in the competition. We were able to reduce the size of the best circuits from 2023 by 12\% on average, whereas for some individual circuits, our size reduction was as large as 83\%. 

**Abstract (ZH)**: 我们介绍了一个开源工具，用于操作布尔电路。该工具实现了高效算法，包括现有的和新颖的算法，以处理各种常用电路任务，如可满足性、综合和最小化。我们在过去三年中由社区精心优化的广泛实际相关电路（特别是在计算对称和算术函数方面）上测试了该工具。该工具帮助我们在IWLS 2024编程竞赛中取得了胜利。2023年，Google DeepMind 在竞赛中取得了第一名。我们成功地将2023年最佳电路的大小平均减少了12%，而在某些单独的电路中，我们的尺寸减少高达83%。 

---
# RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response 

**Title (ZH)**: RobustFT：在嘈杂响应条件下大型语言模型的稳健监督微调 

**Authors**: Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14922)  

**Abstract**: Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios. 

**Abstract (ZH)**: 监督微调（SFT）在适应大规模语言模型（LLMs）到特定领域或任务方面发挥着重要作用。然而，如实践经验所显示的，实际应用中收集的数据不可避免地包含噪声，这对下游任务的模型性能构成了重大挑战。因此，迫切需要一个抗噪的SFT框架以提高模型在下游任务中的能力。为应对这一挑战，我们引入了一个抗噪SFT框架（RobustFT），该框架能够在下游任务数据中进行噪声检测和重新标注。在噪声识别方面，我们的方法采用多专家协作系统和推理增强模型以实现优越的噪声检测效果。在去噪阶段，我们采用了上下文增强策略，该策略结合了最相关且自信的知识，并经过仔细评估以生成可靠标注。此外，我们引入了一种基于响应熵的有效数据选择机制，确保仅保留高质量样本进行微调。在多个LLM上进行的跨五个数据集的广泛实验表明，RobustFT在噪声场景下表现出色。 

---
# Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation 

**Title (ZH)**: 去 hallucination 的并行上下文扩展用于检索增强生成

在这个翻译中，“Dehallucinating”被理解为去除或纠正hallucination（即生成与输入或知识库不符的内容），以更符合学术表达。“Parallel Context Extension for Retrieval-Augmented Generation”被直接翻译为“并行上下文扩展用于检索增强生成”。这样的翻译既保留了原意，又符合学术论文的表述规范。 

**Authors**: Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie  

**Link**: [PDF](https://arxiv.org/pdf/2412.14905)  

**Abstract**: Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks. 

**Abstract (ZH)**: 以下是经过学术规范翻译后的论文内容或标题：

大型语言模型（LLMs）在融合检索增强生成（RAG）技术后，仍有可能生成虚假信息。平行上下文扩展（PCE）是一类旨在有效整合不相关上下文的研究方法，但当应用于RAG场景时，仍然存在生成虚假信息的问题。本文提出了一种名为DePaC（Dehallucinating Parallel Context Extension）的方案，通过上下文感知的负向训练和信息校准聚合来缓解虚假信息问题。DePaC 设计用于解决两种类型的上下文内部虚假信息问题：事实杜撰（即，LLMs 提供与上下文无关的断言）和事实遗漏（即，LLMs 未能提供可以支持上下文的断言）。具体而言，(1) 对于事实杜撰，我们采用上下文感知的负向训练，对LLMs 进行微调并接受负向监督，从而使LLMs 显式地在上下文与问题无关时拒绝回答；(2) 对于事实遗漏，我们提出了信息校准聚合方法，优先考虑信息增量较大的上下文窗口。在九个RAG任务上的实验结果表明，DePaC 显著缓解了这两种虚假信息问题，并在这些任务上始终取得了更好的表现。 

---
# AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature Screening 

**Title (ZH)**: 基于AI的颅内出血检测：一种基于不确定性模糊积分运算器和特征筛选的共尺度卷积注意力模型 

**Authors**: Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan  

**Link**: [PDF](https://arxiv.org/pdf/2412.14869)  

**Abstract**: Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even this http URL primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy. 

**Abstract (ZH)**: 颅内出血（ICH）是指血液在颅骨内部的泄漏或积聚，通常是由于颅内或颅内周围血管的破裂所致。若未及时诊断并适当治疗，可能会导致意识下降、永久性神经功能障碍，甚至危及生命。本研究的主要目标是检测ICH的发生或不存在，随后确定是否为硬脑膜下出血（SDH）。将这两个任务作为两个独立的二元分类问题进行建模。通过在联合尺度卷积注意力（CCA）分类器架构中加入两层结构，我们提出了一种新的ICH检测方法。第一层中，通过对计算机断层扫描（CT）图像的不同层面进行特征提取后，将这些特征结合，并选择出能够捕捉数据最高方差的50个特征，作为信息特征。然后，我们使用自助森林算法评估这些特征的辨别能力，剔除那些在不同类别间鉴别能力不够强的特征。此算法明确地确定了每个特征对最终预测的贡献，帮助我们构建可解释的人工智能模型。特征被输送到增强神经网络中作为潜在特征空间。第二层中，我们引入了一种基于不确定性的新模糊积分算子以融合不同CT扫描层面的信息。该算子通过考虑连续层面之间的依赖关系，显著提高了检测准确性。 

---
# A Survey of RWKV 

**Title (ZH)**: 《RWKV综述》

在这个翻译中，“A Survey of”被译为“综述”，“RWKV”保持不变，因为在这种情况下，它可能是指一个特定的模型或系统的名称。如果RWKV是某个特定领域的专有名词，通常不需要翻译。如果有更多具体内容需要翻译或进一步讨论，请告诉我！ 

**Authors**: Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14847)  

**Abstract**: The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: this https URL. 

**Abstract (ZH)**: 受容性加权关键值（RWKV）模型提供了一种替代传统的Transformer架构的新颖选择，结合了循环网络和基于注意力系统的优点。与依赖于自我注意的常规Transformer不同，RWKV能够以最小的计算需求高效地捕捉长范围依赖关系。通过使用递归框架，RWKV解决了传统Transformer在处理长序列任务时的一些计算效率低下的问题。RWKV最近因其在多个领域的稳健表现引起了广泛注意。尽管其日益流行，但目前还没有系统性的RWKV模型综述。本文旨在填补这一空白，作为第一个全面综述RWKV架构、核心原则及其各种应用（如自然语言生成、自然语言理解和计算机视觉）的论文。我们评估了RWKV与传统Transformer模型的差异，并突显了其在高效处理长序列和降低计算成本方面的能力。此外，我们探讨了RWKV所面临的技术挑战，并提出了未来研究和发展的潜在方向。我们始终维护相关的开源材料，请参阅以下链接：https://this-url。 

---
# Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy with Pre-training, Data Augmentation and Dual Flow UNet 

**Title (ZH)**: 在预治疗和中期放疗的MRI图像中基于预训练、数据增强和双流UNet的头部和颈部肿瘤分割 

**Authors**: Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14846)  

**Abstract**: Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at this https URL. 

**Abstract (ZH)**: 头颈部肿瘤及其转移淋巴结对于治疗规划和预后分析至关重要。准确分割和定量分析这些结构需要像素级标注，因此自动分割技术对于头颈部癌症的诊断和治疗至关重要。在本研究中，我们探讨了多种策略对预治疗（pre-RT）和中程治疗（mid-RT）图像分割效果的影响。对于pre-RT图像分割，我们采用了：1）完全监督学习方法，以及2）结合预训练权重和MixUp数据扩增技术的增强方法。对于mid-RT图像，我们引入了一种新颖的计算友好型网络架构，该架构包括专门用于mid-RT图像的编码器和与pre-RT图像及其标签配准的编码器。mid-RT编码器分支在前向传播过程中逐步整合来自pre-RT图像和标签的信息。我们从每个折中的最优模型中选择了表现最好的模型，并使用它们的预测结果进行集成推理。最终测试中，我们的模型在HiLab汇总Dice相似系数（DSC）上的分割性能分别为pre-RT图像82.38%和mid-RT图像72.53%。我们的代码可在此处获得：this https URL。 

---
# Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas 

**Title (ZH)**: 使用合成人物映射和影响大型语言模型的政治意识形态 

**Authors**: Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini  

**Link**: [PDF](https://arxiv.org/pdf/2412.14843)  

**Abstract**: The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training. 

**Abstract (ZH)**: 对大型语言模型（LLMs）中的政治偏见进行分析主要将这些系统视为具有固定视角的单一实体。虽然存在多种测量偏见的方法，但基于人设的提示对LLMs政治倾向的影响尚未得到探索。本研究利用PersonaHub合集——一个合成人设描述的集合——通过政治取向测试（PCT）来绘制基于人设的提示LLMs的政治分布图。我们随后探讨是否可以通过明确指示意识形态来操控这些初始的政治取向分布，进而使其偏向完全对立的政治取向：右翼权威主义和左翼自由主义。实验结果显示，合成人设主要集中在左翼自由主义象限，当使用明确的意识形态描述进行提示时，模型显示出不同程度的响应。所有模型在右翼权威主义方向上都表现出显著的转变，但在左翼自由主义方向上的转变则更为有限，这表明对意识形态操纵的不对称响应可能反映了模型训练中的固有偏见。 

---
# Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis 

**Title (ZH)**: 使用测试和静态分析反馈帮助大型语言模型提升代码生成能力 

**Authors**: Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis, Agostino Cortesi, Enea Zaffanella  

**Link**: [PDF](https://arxiv.org/pdf/2412.14841)  

**Abstract**: Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.
First, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.
Our results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools. 

**Abstract (ZH)**: 大型语言模型（LLMs）是人工智能领域最具前景的发展之一，软件工程社区也已经注意到它们在软件开发生命周期中的潜在作用。开发者经常要求LLMs生成代码片段，从而提高生产效率，但也可能引入所有权、隐私、正确性及安全性方面的问题。前期研究指出，主流商业LLMs生成的代码往往不够安全，包含漏洞、错误和代码异味。在本文中，我们提出了一种框架，通过测试和静态分析来评估由通用开源LLMs生成的代码的质量，并引导这些模型进行自我优化。

首先，我们要求LLMs生成C代码以解决一系列编程任务。然后，我们使用真实的测试来评估生成代码的（不）正确性，并使用静态分析工具检测潜在的安全漏洞。接下来，我们评估模型评估生成代码的能力，通过要求模型检测错误和漏洞。最后，我们测试模型修复生成代码的能力，提供在静态分析和不正确性评估阶段产生的报告作为反馈。

我们的研究表明，模型经常生成错误的代码，生成的代码中可能包含安全问题。此外，它们在检测这些错误和问题方面表现非常糟糕。不过，当我们提供关于失败测试或潜在漏洞的信息时，我们观察到模型在修复存在问题的代码方面表现出明显的潜力，这为进一步改进基于LLM的代码生成工具的安全性提供了有前途的途径。 

---
# Progressive Multimodal Reasoning via Active Retrieval 

**Title (ZH)**: 通过主动检索实现逐步多模态推理 

**Authors**: Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen  

**Link**: [PDF](https://arxiv.org/pdf/2412.14835)  

**Abstract**: Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning. 

**Abstract (ZH)**: 多步骤多模态推理任务对多模态大型语言模型（MLLMs）提出了显著挑战，而在这种场景中提升其性能的有效方法仍然是未解之谜。本文提出了一种名为AR-MCTS的通用框架，旨在通过主动检索（AR）和蒙特卡洛树搜索（MCTS）逐步提高MLLMs的推理能力。我们的方法首先构建了一个统一的检索模块，从混合模态检索库中检索解决复杂推理问题的关键支持洞察。为了解决自动多模态推理验证的差距，我们采用了结合主动检索机制的MCTS算法，这能够自动生成逐步骤的注解。该策略动态检索每个推理步骤的关键洞察，超越传统的束搜索采样方法，以提高推理空间的多样性和可靠性。此外，我们还引入了一种过程奖励模型，以逐步支持多模态推理任务的自动验证。在三个复杂的多模态推理基准测试中，实验结果证实了AR-MCTS框架在提升各种多模态模型性能方面的有效性。进一步的分析表明，AR-MCTS能够优化采样多样性和准确性，从而实现可靠的多模态推理。 

---
# MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data 

**Title (ZH)**: MARIA：一种用于不完整医疗数据的多模态变压器模型 

**Authors**: Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi  

**Link**: [PDF](https://arxiv.org/pdf/2412.14810)  

**Abstract**: In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications. 

**Abstract (ZH)**: 在医疗保健领域，多模态数据的综合对于发展全面的诊断和预测模型至关重要。然而，在实际应用中，管理缺失数据仍然是一个重大挑战。为此，我们提出了MARIA（Multimodal Attention Resilient to Incomplete datA），一种新型的基于变压器的深度学习模型，通过中间融合策略来应对这些挑战。与依赖于数据填充的传统方法不同，MARIA 利用了遮罩自注意力机制，只处理可用数据而不生成合成值。这种方法使其能够有效处理不完整的数据集，增强其鲁棒性，并最大限度地减少由填充方法引入的偏差。我们对MARIA进行了全面的评估，与8个诊断和预后任务中的10个最先进的机器学习和深度学习模型进行了比较。结果表明，MARIA在性能和对数据不完整性变化程度的鲁棒性方面均优于现有方法，为其在关键医疗应用中的潜力提供了有力支持。 

---
# Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios 

**Title (ZH)**: 栈跟踪去重：更快、更准确、并在更多现实场景中实现 

**Authors**: Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov  

**Link**: [PDF](https://arxiv.org/pdf/2412.14802)  

**Abstract**: In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.
To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area. 

**Abstract (ZH)**: 在大规模软件系统中，当错误发生时，通常没有详细的人工描述的完全完善的bug报告。在这种情况下，开发人员依赖于堆栈跟踪，即导致错误的一系列函数调用。由于可能存在成千上万个描述同一问题的不同用户的信息，因此需要自动分组到不同的类别中以便处理。最近的研究提出了一些基于深度学习的强大方法，但这些方法在实际工作流程中并未得到评估和比较，不清楚它们在大规模应用中是否有效。

为了解决这一差距，本研究提出了三个主要贡献：一个新颖的模型，一个基于工业的实际数据集，以及多层次的评估。我们的模型由两个部分组成——（1）嵌入模型，采用字节对编码和近似最近邻搜索，以快速找到与当前堆栈跟踪最相关的堆栈跟踪；（2）一个重排序模块，重新排列最合适的堆栈跟踪，考虑它们之间的重复帧。为了补充现有的来自开源项目的数据集，我们与社区分享了来自JetBrains的基于IntelliJ产品的SlowOps数据集，该数据集每个类别中的堆栈跟踪数量比现有数据集多一个数量级。最后，我们进行了一种力求实际的评估：不仅测量类别化精度，还测量操作时间以及创建新类别的能力。评估结果显示，我们的模型表现均衡——在开源数据集和SlowOps上均优于其他模型，同时在时间上也比大多数模型更快。我们发布了所有代码和数据，希望能够为该领域的实践导向研究铺平道路。 

---
# Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning 

**Title (ZH)**: 基于代理-时间信用分配的稀疏多代理强化学习中优化策略保留方法 

**Authors**: Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht  

**Link**: [PDF](https://arxiv.org/pdf/2412.14779)  

**Abstract**: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods. 

**Abstract (ZH)**: 在多智能体环境中，智能体常常难以学习最优策略，尤其是在稀疏或延迟的全局奖励以及长期任务中，评估中间时间步骤的动作尤为困难。为此，我们提出了时间智能体奖励再分配（Temporal-Agent Reward Redistribution，简称TAR$^2$），这是一种旨在通过在时间和智能体之间重分配稀疏奖励来解决智能体-时间信用分配问题的新型方法。TAR$^2$将稀疏的全局奖励分解为时间步长特定的奖励，并计算每个智能体对这些奖励的贡献。我们从理论上证明了TAR$^2$等同于基于势的奖励塑形，确保最优策略不发生变化。实验结果表明，TAR$^2$能够稳定并加速学习过程。此外，我们还展示了在将TAR$^2$与单智能体强化学习算法结合时，其性能至少与传统多智能体强化学习方法相当或更优。 

---
# Energy and polarization based on-line interference mitigation in radio interferometry 

**Title (ZH)**: 基于能量和极化在线干扰抑制的射电干涉ometry方法 

**Authors**: Sarod Yatawatta, Albert-Jan Boonstra, Chris P. Broekema  

**Link**: [PDF](https://arxiv.org/pdf/2412.14775)  

**Abstract**: Radio frequency interference (RFI) is a persistent contaminant in terrestrial radio astronomy. While new radio interferometers are becoming operational, novel sources of RFI are also emerging. In order to strengthen the mitigation of RFI in modern radio interferometers, we propose an on-line RFI mitigation scheme that can be run in the correlator of such interferometers. We combine statistics based on the energy as well as the polarization alignment of the correlated signal to develop an on-line RFI mitigation scheme that can be applied to a data stream produced by the correlator in real-time, especially targeted at low duty-cycle or transient RFI detection. In order to improve the computational efficiency, we explore the use of both single precision and half precision floating point operations in implementing the RFI mitigation algorithm. This ideally suits its deployment in accelerator computing devices such as graphics processing units (GPUs) as used by the LOFAR correlator. We provide results based on real data to demonstrate the efficacy of the proposed method. 

**Abstract (ZH)**: 射频干扰（RFI）是地面无线电天文学中的一个持久性污染物。随着新的射电干涉仪投入运行，新的RFI来源也正在出现。为了加强现代射电干涉仪中RFI的减轻措施，我们提出了一种可在此类干涉仪的关联器中运行的在线RFI减轻方案。我们结合基于关联信号能量及其偏振对齐的统计方法，开发了一种在线RFI减轻方案，可以在关联器生成的数据流中实现实时应用，特别针对低占用率或瞬态RFI的检测。为了提高计算效率，我们探索了在实施RFI减轻算法时使用单精度和半精度浮点运算的可能性。这使其非常适合在如LOFAR关联器所使用的图形处理单元（GPU）等加速计算设备上部署。我们基于实际数据提供了结果，以证明所提出方法的有效性。 

---
# ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine 

**Title (ZH)**: ALKAFI-LLAMA3：针对巴勒斯坦精准法律理解的大型语言模型 fine-tuning 

**Authors**: Rabee Qasem, Mohannad Hendi, Banan Tantour  

**Link**: [PDF](https://arxiv.org/pdf/2412.14771)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多个领域展现出了显著的潜力，但在法律领域的应用，特别是在资源匮乏的环境中，仍受到限制。本研究旨在解决将LLMs适应到巴勒斯坦法律领域的挑战，由于政治不稳定、法律框架碎片化以及有限的人工智能资源，有效地利用机器学习应用受到阻碍。我们基于量化版本的Llama-3.2-1B-Instruct构建了一个微调模型，并训练了该模型，所用数据集来源于巴勒斯坦法律文本。通过使用规模较小的模型和精心设计的问题-答案对，我们实现了一种成本效益高、具有地方可持续性的解决方案，能够提供准确且上下文相关性的法律指导。我们的实验表明，该模型在各种查询类型上表现出了令人鼓舞的效果，涵盖了从是/否问题和叙事解释到复杂法律区分在内的多种查询，同时我们也指出了改进的方向，比如处理基于计算的查询和结构化列表格式化等问题。本研究为定制化的人工智能法律辅助工具的部署提供了一条途径，以适应资源匮乏的环境需求。 

---
# CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering 

**Title (ZH)**: CodeRepoQA：软件工程问答的大规模基准数据集 

**Authors**: Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, Cuiyun Gao  

**Link**: [PDF](https://arxiv.org/pdf/2412.14764)  

**Abstract**: In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.
We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at this https URL. 

**Abstract (ZH)**: 在本文中，我们介绍了CodeRepoQA，这是一个大型基准测试，专门用于评估软件工程领域代码仓库级别问题解答的能力。CodeRepoQA 包含五种编程语言，并涵盖了广泛的场景，使得语言模型能够进行全面评估。为了构建此数据集，我们从 GitHub（最大的代码托管和协作平台）爬取了30个知名代码仓库的数据，并仔细过滤了原始数据。总共，CodeRepoQA 是一个多轮问题解答基准测试，包含585,687个条目，涵盖了广泛的软件工程场景，平均每条目有6.62轮对话。

我们对十个流行的大型语言模型进行了评估，并进行了深入分析。结果显示，大型语言模型在软件工程领域的问题解答能力仍然存在局限，中等长度的上下文环境更有助于大型语言模型的表现。整套基准测试已在此处公开：[公开链接]。 

---
# Advances in Artificial Intelligence forDiabetes Prediction: Insights from a Systematic Literature Review 

**Title (ZH)**: 人工智能在糖尿病预测领域的进展：系统文献综述的洞察 

**Authors**: Pir Bakhsh Khokhar, Carmine Gravino, Fabio Palomba  

**Link**: [PDF](https://arxiv.org/pdf/2412.14736)  

**Abstract**: This systematic review explores the use of machine learning (ML) in predicting diabetes, focusing on datasets, algorithms, training methods, and evaluation metrics. It examines datasets like the Singapore National Diabetic Retinopathy Screening program, REPLACE-BG, National Health and Nutrition Examination Survey, and Pima Indians Diabetes Database. The review assesses the performance of ML algorithms like CNN, SVM, Logistic Regression, and XGBoost in predicting diabetes outcomes. The study emphasizes the importance of interdisciplinary collaboration and ethical considerations in ML-based diabetes prediction models. 

**Abstract (ZH)**: 本文综述系统地探讨了机器学习（ML）在预测糖尿病方面的应用，重点研究了数据集、算法、训练方法和评估指标。综述分析了包括新加坡国家糖尿病视网膜筛查计划、REPLACE-BG项目、全国健康与营养检查调查以及佩梅印第安人糖尿病数据库等数据集。研究评估了CNN、SVM、逻辑回归和XGBoost等机器学习算法在预测糖尿病结果方面的性能。该研究强调了在基于机器学习的糖尿病预测模型中跨学科合作和伦理考量的重要性。 

---
# Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools 

**Title (ZH)**: 超越 hype：生成式人工智能研究、教学实践及工具的全面回顾 

**Authors**: James Prather, Juho Leinonen, Natalie Kiesler, Jamie Gorson Benario, Sam Lau, Stephen MacNeil, Narges Norouzi, Simone Opel, Vee Pettit, Leo Porter, Brent N. Reeves, Jaromir Savelka, David H. Smith IV, Sven Strickroth, Daniel Zingaro  

**Link**: [PDF](https://arxiv.org/pdf/2412.14732)  

**Abstract**: Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community. 

**Abstract (ZH)**: 生成式人工智能（GenAI）正在迅速发展，计算教育领域的研究成果也在几乎同样快速地扩展。对于GenAI工具的初始反应具有混合性质，既有恐慌也有乌托邦式的乐观情绪。许多人迅速指出了GenAI的机会和挑战。研究人员报告称，这些新工具能够解决大多数入门级编程任务，并在整个课程中引发变革。这些工具可以编写和解释代码、增强错误信息、为教师创建资源，并且甚至能够像传统教学助理一样为学生提供反馈和帮助。到了2024年，围绕GenAI在计算教室中的使用效果的新研究开始涌现。这些新数据包括使用GenAI支持大规模课堂教学以及教授学生如何使用GenAI进行编码。为了支持前者，新一代工具正不断出现，可以为学生的编程作业提供个性化反馈，或者同时教授编程和提示技巧。随着研究成果的迅速扩展，本报告旨在总结和解释计算教室中正在进行的情况。我们将进行系统性的文献综述；对教育工作者和行业专业人士进行问卷调查；并对正在其课程中使用GenAI的教育工作者、研究GenAI的教育工作者以及创建支持计算教育的GenAI工具的研究人员进行访谈。通过对这些方法和数据来源的三角分析，可以更好地理解我们社区当前关键时刻GenAI的使用情况和认知状况。 

---
# How to Synthesize Text Data without Model Collapse? 

**Title (ZH)**: 如何合成文本数据而不导致模型崩溃？ 

**Authors**: Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.14689)  

**Abstract**: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance. 

**Abstract (ZH)**: 模型在合成数据上的崩溃表明，迭代地在自我生成的数据上进行训练会导致模型性能逐渐下降。随着AI模型的普遍应用，合成数据将从根本上重塑网络数据生态系统。未来的GPT-$\{n\}$模型不可避免地会在合成数据和人类生成的数据混合的情况下进行训练。在本文中，我们重点关注两个问题：合成数据对语言模型训练的影响是什么？如何合成数据而不引发模型崩溃？我们首先在不同比例的合成数据上预训练语言模型，发现合成数据的比例与模型性能之间存在负相关关系。进一步地，我们对合成数据进行统计分析，发现了分布变化现象和n元组特征的过度集中。受上述发现的启发，我们提出对人类生成的数据进行标记编辑以获得半合成数据。作为概念验证，我们理论性地证明了标记级别的编辑可以防止模型崩溃，因为测试误差被一个有限的上限所约束。我们进行了全面的实验，包括从头预训练、连续预训练和监督微调。结果验证了我们的理论证明，标记级别的编辑提高了数据质量并增强了模型性能。 

---
# Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection 

**Title (ZH)**: 每条假新闻都有其独特之处：一种多模态假新闻检测的归因多层次基准 

**Authors**: Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.14686)  

**Abstract**: Social platforms, while facilitating access to information, have also become saturated with a plethora of fake news, resulting in negative consequences. Automatic multimodal fake news detection is a worthwhile pursuit. Existing multimodal fake news datasets only provide binary labels of real or fake. However, real news is alike, while each fake news is fake in its own way. These datasets fail to reflect the mixed nature of various types of multimodal fake news. To bridge the gap, we construct an attributing multi-granularity multimodal fake news detection dataset \amg, revealing the inherent fake pattern. Furthermore, we propose a multi-granularity clue alignment model \our to achieve multimodal fake news detection and attribution. Experimental results demonstrate that \amg is a challenging dataset, and its attribution setting opens up new avenues for future research. 

**Abstract (ZH)**: 社交媒体在促进信息访问的同时，也充斥着大量的假新闻，产生了负面后果。自动多模态假新闻检测是一项值得追求的研究。现有的多模态假新闻数据集仅提供真假二元标签。然而，真正的新闻存在相似性，而每条假新闻都有其独特的虚假特性。这些数据集未能反映各种类型多模态假新闻的混合性质。为填补这一空白，我们构建了一个具有属性的多粒度多模态假新闻检测数据集 \AMG，揭示了固有的虚假模式。此外，我们提出了一种多粒度线索对齐模型 \OUR，以实现多模态假新闻的检测与归因。实验结果表明，\AMG 是一个具有挑战性的数据集，其归因设置为未来的研究开辟了新的途径。 

---
# A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space 

**Title (ZH)**: 基于解耦特征对齐的联合空间开放式目标检测轻量级框架 

**Authors**: Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14680)  

**Abstract**: Open-set object detection (OSOD) is highly desirable for robotic manipulation in unstructured environments. However, existing OSOD methods often fail to meet the requirements of robotic applications due to their high computational burden and complex deployment. To address this issue, this paper proposes a light-weight framework called Decoupled OSOD (DOSOD), which is a practical and highly efficient solution to support real-time OSOD tasks in robotic systems. Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP) adaptor is developed to transform text embeddings extracted by the VLM into a joint space, within which the detector learns the region representations of class-agnostic proposals. Cross-modality features are directly aligned in the joint space, avoiding the complex feature interactions and thereby improving computational efficiency. DOSOD operates like a traditional closed-set detector during the testing phase, effectively bridging the gap between closed-set and open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD significantly enhances real-time performance while maintaining comparable accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\%$, compared to $26.2\%$ for YOLO-World-v1-S and $22.7\%$ for YOLO-World-v2-S, using similar backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is $57.1\%$ higher than YOLO-World-v1-S and $29.6\%$ higher than YOLO-World-v2-S. Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of edge devices. The codes and models are publicly available at this https URL. 

**Abstract (ZH)**: 开放集目标检测（Open-set Object Detection, OSOD）在不规则环境下的机器人操作中具有很高价值。然而，现有的OSOD方法往往因为计算负担重和部署复杂而无法满足机器人应用的要求。为解决这一问题，本文提出了一种轻量级框架——解耦开放集目标检测（Decoupled OSOD, DOSOD），该框架是支持机器人系统中实时OSOD任务的实用且高效解决方案。具体而言，DOSOD在YOLO-World流水线的基础上，整合了一个视觉语言模型（Visual Language Model, VLM）与检测器。开发了一个多层感知器（Multilayer Perceptron, MLP）适配器，用于将VLM提取的文本嵌入转化为联合空间，在该空间中，检测器学习无类别建议区域的表示。跨模态特征直接在联合空间中进行对齐，避免了复杂的特征交互，从而提高了计算效率。在测试阶段，DOSOD运作类似于传统的封闭集检测器，有效地填补了封闭集和开放集检测之间的差距。与基线YOLO-World相比，提出的DOSOD在保持相当准确性的前提下显著提高了实时性能。在使用相似骨干网的LVIS minival数据集上，DOSOD-S模型的固定平均精确度（Fixed AP）为26.7%，而YOLO-World-v1-S为26.2%，YOLO-World-v2-S为22.7%。同时，DOSOD-S的每秒帧数（FPS）比YOLO-World-v1-S高57.1%，比YOLO-World-v2-S高29.6%。此外，我们证明了DOSOD模型便于边缘设备的部署。代码和模型已公开发布。详见此链接：[请插入链接]。 

---
# FiVL: A Framework for Improved Vision-Language Alignment 

**Title (ZH)**: FiVL：一个提高视觉-语言对齐的框架 

**Authors**: Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal  

**Link**: [PDF](https://arxiv.org/pdf/2412.14672)  

**Abstract**: Large Vision Language Models (LVLMs) have achieved significant progress in integrating visual and textual inputs for multimodal reasoning. However, a recurring challenge is ensuring these models utilize visual information as effectively as linguistic content when both modalities are necessary to formulate an accurate answer. We hypothesize that hallucinations arise due to the lack of effective visual grounding in current LVLMs. This issue extends to vision-language benchmarks, where it is difficult to make the image indispensable for accurate answer generation, particularly in vision question-answering tasks. In this work, we introduce FiVL, a novel method for constructing datasets designed to train LVLMs for enhanced visual grounding and to evaluate their effectiveness in achieving it. These datasets can be utilized for both training and assessing an LVLM's ability to use image content as substantive evidence rather than relying solely on linguistic priors, providing insights into the model's reliance on visual information. To demonstrate the utility of our dataset, we introduce an innovative training task that outperforms baselines alongside a validation method and application for explainability. The code is available at this https URL. 

**Abstract (ZH)**: 大规模视觉语言模型（LVLMs）在整合视觉和文本输入进行多模态推理方面取得了显著进展。然而，一个反复出现的挑战是如何确保这些模型在同时需要视觉信息和语言内容时，能够有效地利用视觉信息。我们认为，幻觉的产生是因为当前LVLMs缺乏有效的视觉定位能力。这一问题也同样体现在视觉语言基准测试中，在这些基准测试中，难以使图像成为生成准确答案不可或缺的组成部分，尤其是在视觉问答任务中。在本工作中，我们提出了一种新的方法FiVL，旨在构建数据集，用于训练LVLMs以增强视觉定位能力，并评估它们在实现这一目标方面的有效性。这些数据集可用于训练和评估LVLMs将图像内容作为实质性证据而非仅仅依赖语言先验的能力，从而提供模型在依赖视觉信息上的见解。为了展示我们数据集的实际应用价值，我们引入了一种创新的训练任务，该任务在基准线基础上表现更优，并附带了验证方法和解释性应用。相关代码可在以下链接获取：this https URL。 

---
# Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT 

**Title (ZH)**: 大型语言模型中语义结构的分析与可视化：BERT 中动词-粒子构造的神经表示分析与可视化 

**Authors**: Hassane Kissane, Achim Schilling, Patrick Krauss  

**Link**: [PDF](https://arxiv.org/pdf/2412.14670)  

**Abstract**: This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision. 

**Abstract (ZH)**: 本研究探讨了基于变换器的大型语言模型（LLMs）中动词短语组合的内部表示，具体分析了这些模型在不同神经网络层如何捕捉词汇和句法细微差别。我们采用了BERT架构，对不同类型的动作短语组合（如“agree on”、“come back”、“give up”）的层次表示能力进行了分析。方法包括从英国国家语料库精心准备的数据集，随后通过多维尺度分析（MDS）和广义辨别值（GDV）计算等技术进行广泛的模型训练和输出分析。研究结果表明，BERT的中间层最有效地捕捉句法结构，而不同动词类别在表示准确性方面存在显著差异。这些发现挑战了神经网络处理语言元素时普遍存在的一致性的假设，并表明网络架构与语言表示之间的复杂交互关系。本研究对深度学习模型如何理解和处理语言有了更深入的理解，提供了关于当前神经方法在语言分析中的潜力和局限性的见解。本研究不仅推进了计算语言学的知识，还激发了对优化神经架构以提高语言精度的研究。 

---
# LoLaFL: Low-Latency Federated Learning via Forward-only Propagation 

**Title (ZH)**: LoLaFL：基于单向传播的低延迟联邦学习 

**Authors**: Jierui Zhang, Jianhao Huang, Kaibin Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14668)  

**Abstract**: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 91\% and 98\%, respectively, while maintaining comparable accuracies. 

**Abstract (ZH)**: 联邦学习（FL）作为一种在分布式数据上实现边缘学习的广泛采用范式，能够在确保数据隐私的同时进行模型训练。然而，传统的使用反向传播训练深度神经网络的FL难以满足第六代（6G）移动网络中的低延迟学习要求。这一挑战主要源于高效传输高维模型参数以及因训练过程固有的随机性而需要的大量通信轮次。为解决这一问题，我们采用最先进的最大编码率减少原则来学习线性判别特征，并将由此得出的白盒神经网络扩展到联邦学习中，从而形成一种通过前向传播的新框架——低延迟联邦学习（LoLaFL）。

LoLaFL能够实现逐层的传输和聚合，显著减少通信轮次，从而大幅降低延迟。此外，我们还为LoLaFL提出了两种非线性聚合方案。第一个方案基于证明，在LoLaFL中最优的神经网络参数聚合应类似于调和平均。第二个方案进一步利用了特征的低秩结构，通过传输特征的低秩近似协方差矩阵来实现额外的延迟减少。通过理论分析和实验评估LoLaFL的性能。与传统的联邦学习相比，LoLaFL的两种非线性聚合方案分别能够分别实现超过91%和98%的延迟减少，同时保持相当的准确性。 

---
# IOHunter: Graph Foundation Model to Uncover Online Information Operations 

**Title (ZH)**: IOHunter：基于图的模型以发现在线信息操作 

**Authors**: Marco Minici, Luca Luceri, Francesco Fabbri, Emilio Ferrara  

**Link**: [PDF](https://arxiv.org/pdf/2412.14663)  

**Abstract**: Social media platforms have become vital spaces for public discourse, serving as modern agorás where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. \textit{IO drivers}, across various influence campaigns. Our framework, named \texttt{IOHunter}, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in \emph{supervised}, \emph{scarcely-supervised}, and \emph{cross-IO} contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms. 

**Abstract (ZH)**: 社交媒体平台已成为公众辩论的关键空间，作为现代论坛，广泛的声音参与塑造着社会叙事。然而，其开放性也使其容易被恶意行为者，包括国家支持的实体，利用来进行信息操作（IOs），以操控公众意见。虚假信息、假新闻和误导性声明的传播威胁着民主进程和社会凝聚力，因此及时检测不实行为变得至关重要，以保护在线讨论的完整性。在本研究中，我们提出了一种方法论，旨在识别在各种影响活动中策划信息操作的用户，即所谓的“IO驱动者”。我们提出了一种名为 \texttt{IOHunter} 的框架，该框架结合了语言模型和图神经网络的优势，以提高在监督、弱监督和跨信息操作（\emph{cross-IO}）情境中的泛化能力。我们的方法在六个不同国家起源的多个信息操作集合上实现了现有方法中的最佳性能。这项研究标志着开发专门针对社交媒体平台上信息操作检测任务的图基础模型的一步进展。 

---
# Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models 

**Title (ZH)**: 揭开不确定性之谜：多模态大型语言模型的校准与性能深入探究 

**Authors**: Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14660)  

**Abstract**: Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）结合视觉和文本数据，用于诸如图像字幕和视觉问答等任务。在医疗保健和自主驾驶等关键领域中，适当的不确定性校准对于可靠使用至关重要，但同时也是极具挑战性的。本文研究了代表性的MLLMs，重点关注它们在不同场景下的校准情况，包括视觉微调前后以及基础大语言模型多模态训练前后。我们观察到了它们性能的校准偏差，但在这些场景之间的校准差异并不显著。我们还强调了文本和图像之间不确定性差异及其整合如何影响总体不确定性。为了更好地理解MLLMs的偏差并提高其自我评估不确定性的能力，我们构建了IDK（我不知道）数据集，这是评估它们处理未知情况的关键工具。研究结果表明，MLLMs倾向于给出答案而不是承认不确定性，但这种自我评估能力可以通过适当的提示调整而提高。最后，为了校准MLLMs并提高模型的可靠性，我们提出了诸如温度缩放和迭代提示优化等技术。我们的结果为在多模态应用中有效且负责任地部署MLLMs提供了见解。代码和IDK数据集：[此处插入链接]。 

---
# Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention for Fine-Grained Few-Shot Learning 

**Title (ZH)**: 自适应提示调优：基于跨注意力的视觉指导提示调优在细粒度少样本学习中的应用 

**Authors**: Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich  

**Link**: [PDF](https://arxiv.org/pdf/2412.14640)  

**Abstract**: Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the model's predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification. 

**Abstract (ZH)**: 计算机视觉中的少量样本、细粒度分类面临着显著挑战，因为需要在数据有限的情况下区分细微的类别差异。本文提出了一种新颖的方法，通过动态提示调优增强对比语言-图像预训练（CLIP）模型，该方法由实时视觉输入引导。与现有技术如上下文优化（CoOp）和视觉提示调优（VPT），这些技术受静止提示或对视觉标记的依赖性限制不同，所提出的方法利用交叉注意力机制对当前图像的文本提示进行动态精炼。这使得模型能够与源自视觉变换器的图像补丁进行特定图像的文本特征对齐，从而使其更适合具有高类内变异性且类间差异较小的数据集。该方法在多个数据集上进行了评估，包括CUBirds、Oxford Flowers和FGVC Aircraft等数据集，显示出显著的性能提升，超过了静态提示调优方法。为了确保这些性能提升转化为可信赖的预测，我们整合了蒙特卡洛丢棄技术以提高模型预测的可靠性和不确定性估计。这一整合提供了有关模型预测置信度的宝贵见解，有助于识别哪些预测可以信赖，哪些仍需要进一步验证。这种方法提供了稳健的解决方案，促进了少量样本细粒度分类的前沿技术进步。 

---
# A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI 

**Title (ZH)**: 一种用于高效可解释量子人工智能的Shapley值估计加速方法 

**Authors**: Iain Burge, Michel Barbeau, Joaquin Garcia-Alfaro  

**Link**: [PDF](https://arxiv.org/pdf/2412.14639)  

**Abstract**: This work focuses on developing efficient post-hoc explanations for quantum AI algorithms. In classical contexts, the cooperative game theory concept of the Shapley value adapts naturally to post-hoc explanations, where it can be used to identify which factors are important in an AI's decision-making process. An interesting question is how to translate Shapley values to the quantum setting and whether quantum effects could be used to accelerate their calculation. We propose quantum algorithms that can extract Shapley values within some confidence interval. Our method is capable of quadratically outperforming classical Monte Carlo approaches to approximating Shapley values up to polylogarithmic factors in various circumstances. We demonstrate the validity of our approach empirically with specific voting games and provide rigorous proofs of performance for general cooperative games. 

**Abstract (ZH)**: 本文集中于开发量子人工智能算法的高效后验解释方法。在经典背景下，合作博弈论中的Shapley值概念自然适用于后验解释，其中可以用于识别AI决策过程中哪些因素是重要的。一个有趣的问题是如何将Shapley值转换到量子设置中，以及量子效应是否可以用于加速其计算。我们提出了一种量子算法，可在某些置信区间内提取Shapley值。我们的方法在多种情况下能够相对于经典的蒙特卡罗方法实现平方级的性能提升，考虑到对数多项式因子的影响。我们通过特定的投票博弈实验验证了方法的有效性，并为一般的合作博弈提供了严格的表现性能证明。 

---
# Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers 

**Title (ZH)**: 面向视觉变换器的分阶段细到粗重建方法以实现准确的低比特量化后训练 

**Authors**: Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.14633)  

**Abstract**: Due to its efficiency, Post-Training Quantization (PTQ) has been widely adopted for compressing Vision Transformers (ViTs). However, when quantized into low-bit representations, there is often a significant performance drop compared to their full-precision counterparts. To address this issue, reconstruction methods have been incorporated into the PTQ framework to improve performance in low-bit quantization settings. Nevertheless, existing related methods predefine the reconstruction granularity and seldom explore the progressive relationships between different reconstruction granularities, which leads to sub-optimal quantization results in ViTs. To this end, in this paper, we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for accurate PTQ, which significantly improves the performance of low-bit quantized vision transformers. Specifically, we define multi-head self-attention and multi-layer perceptron modules along with their shortcuts as the finest reconstruction units. After reconstructing these two fine-grained units, we combine them to form coarser blocks and reconstruct them at a coarser granularity level. We iteratively perform this combination and reconstruction process, achieving progressive fine-to-coarse reconstruction. Additionally, we introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the difficulty of training, thereby further enhancing model performance. Experimental results on the ImageNet dataset demonstrate that our proposed method achieves the best Top-1 accuracy among state-of-the-art methods, particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides, quantization results on the COCO dataset reveal the effectiveness and generalization of our proposed method on other computer vision tasks like object detection and instance segmentation. 

**Abstract (ZH)**: 由于其高效性，后训练量化（Post-Training Quantization, PTQ）已成为压缩视觉变换器（Vision Transformers, ViTs）的常用方法。然而，在量化为低位表示时，ViTs 的性能通常会显著下降，与全精度版本相比差距较大。为了解决这一问题，已有研究在 PTQ 框架中引入了重建方法，以提高低位量化设置下的性能。然而，现有的相关方法会预先定义重建粒度，很少探索不同粒度之间的逐步关系，从而导致 ViTs 上的量化结果不够优化。为解决这些问题，本文提出了一种渐进精细到粗糙重建（Progressive Fine-to-Coarse Reconstruction, PFCR）方法，显著提高了低位量化视觉变换器的性能。具体而言，我们定义多头自注意力和多个层感知机模块及其捷径作为最细粒度的重建单元。在重建这些精细粒度单元之后，我们将它们结合成较粗的块，在更粗的粒度级别进行重建。我们迭代执行此组合和重建过程，最终实现逐步精细到粗糙的重建。此外，我们引入了一种渐进优化策略（Progressive Optimization Strategy, POS），以缓解训练难度，进一步提升模型性能。在 ImageNet 数据集上的实验结果表明，我们提出的方法在最先进的方法中实现了最佳的 Top-1 准确率，特别是在 PTQ 中的 3 位量化 ViT-B 中达到了 75.61% 的准确率。同时，COCO 数据集上的量化结果表明我们提出的方法在其他计算机视觉任务，如目标检测和实例分割中具有有效性和泛化能力。 

---
# Learning to Generate Research Idea with Dynamic Control 

**Title (ZH)**: 学习使用动态控制生成研究思路 

**Authors**: Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du  

**Link**: [PDF](https://arxiv.org/pdf/2412.14626)  

**Abstract**: The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的快速进步已经显示出它们在加速科学发现方面的潜力，尤其是在自动化研究构思过程方面。基于LLM的系统展示了生成假设和研究构思的潜力。然而，当前的方法主要依赖于基于提示的预训练模型，这限制了它们在优化生成内容方面的有效性。此外，这些系统还缺乏处理新颖性、可行性和有效性之间复杂相互依赖关系的能力，这些相互依赖关系因这些维度之间的固有权衡而更具挑战性，例如创新与可行性的冲突。为了解决这些局限性，我们首次提出对LLM进行微调，以使其更好地提出研究构思，并介绍了一种新的框架，该框架采用结合监督微调（SFT）和可控强化学习（RL）的两阶段方法。在SFT阶段，模型从研究论文及其后续构思的配对中学习基础模式。在RL阶段，通过精细反馈引导的多维度奖励模型评估并优化生成的内容，确保在关键指标上取得最优效果。维度控制器使生成过程可以动态调整，而句子级解码器确保在推理过程中对上下文有意识地强调。我们的框架提供了一种平衡的研究构思方法，在动态导航新颖性、可行性和有效性之间的权衡时，实现了高质量的结果。 

---
# Pitfalls of topology-aware image segmentation 

**Title (ZH)**: 拓扑感知图像分割的局限性 

**Authors**: Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold  

**Link**: [PDF](https://arxiv.org/pdf/2412.14619)  

**Abstract**: Topological correctness, i.e., the preservation of structural integrity and specific characteristics of shape, is a fundamental requirement for medical imaging tasks, such as neuron or vessel segmentation. Despite the recent surge in topology-aware methods addressing this challenge, their real-world applicability is hindered by flawed benchmarking practices. In this paper, we identify critical pitfalls in model evaluation that include inadequate connectivity choices, overlooked topological artifacts in ground truth annotations, and inappropriate use of evaluation metrics. Through detailed empirical analysis, we uncover these issues' profound impact on the evaluation and ranking of segmentation methods. Drawing from our findings, we propose a set of actionable recommendations to establish fair and robust evaluation standards for topology-aware medical image segmentation methods. 

**Abstract (ZH)**: 拓扑正确性，即保持结构完整性和特定形状特征，是医学成像任务（如神经元或血管分割）的一个基本要求。尽管近年来已有许多关注拓扑结构的方法被提出以应对这一挑战，但它们在实际应用中的适用性仍然受到不完善基准测试方法的限制。在本文中，我们指出了模型评估中的关键问题，包括不适当的连接性选择、忽视了真实标注中的拓扑错误，以及不恰当的评价指标使用。通过详细的经验分析，我们揭示了这些问题对分割方法评估和排名的深远影响。基于我们的研究结果，我们提出了一套可操作的建议，以建立公平且可靠的拓扑感知医学图像分割方法的评估标准。 

---
# How good is GPT at writing political speeches for the White House? 

**Title (ZH)**: GPT在为白宫撰写政治演说方面做得如何？ 

**Authors**: Jacques Savoy  

**Link**: [PDF](https://arxiv.org/pdf/2412.14617)  

**Abstract**: Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma "we" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author's style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages. 

**Abstract (ZH)**: 利用大规模语言模型（LLMs），计算机能够根据用户请求生成书面文本。由于这项普及的技术可以在众多领域应用，本研究通过将一个名为GPT的LLM生成的演讲与近期美国总统的演讲进行比较，分析了GPT的书面风格。具体而言，本研究对比了里根至拜登历任总统发表的国情咨文（SOTU）与由GPT-3.5和GPT-4版本生成的版本。相较于美国总统，GPT有过度使用“we”这一词汇的现象，并生成更短的信息，平均而言，其句子更长。此外，GPT倾向于采用更乐观的语调，更频繁地使用政治性术语（如总统、国会）、象征性术语（如自由）和抽象性术语（如自由）。即使强加给GPT某种作者的风格，生成的演讲仍然与目标作者的演讲风格有所不同。最后，两种GPT版本呈现出不同的特点，但总体上与真正的总统演讲相异。 

---
# HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model 

**Title (ZH)**: 谐波评估：使用视觉语言模型的多模态、多任务、多标准自动评估 

**Authors**: Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue  

**Link**: [PDF](https://arxiv.org/pdf/2412.14613)  

**Abstract**: Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion. 

**Abstract (ZH)**: 视觉语言模型（VLMs）在文本和图像理解方面展现出了显著的能力。然而，现有的用于评估VLMs生成文本的度量标准仅侧重于整体质量，导致了两个局限性：1）难以仅从整体分数中识别需要改进的文本方面；2）度量标准在预测整体分数时可能会忽略特定的评估标准。为了克服这些局限性，我们提出了一种名为HarmonicEval的参考自由评估指标，它以自底向上的方式聚合各项标准分以生成整体分数。此外，我们构建了多任务多标准人工评估（MMHE）数据集，该数据集包含4个视觉语言任务中的18,000个专家人工判断。实验结果表明，HarmonicEval在与人类判断的相关性上优于传统度量标准，并且为每个标准提供了数值评分。 

---
# Towards Scalable and Deep Graph Neural Networks via Noise Masking 

**Title (ZH)**: 通过噪声屏蔽实现可扩展和深层图神经网络 

**Authors**: Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14602)  

**Abstract**: In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency. 

**Abstract (ZH)**: 近年来，图神经网络（GNNs）在许多图挖掘任务中取得了显著的成果。然而，将它们扩展到大规模图中面临着高计算成本和存储成本的挑战，特别是在训练过程中多次进行特征传播和非线性转换。为了解决这一挑战，人们常用的方法是模型简化，即在预处理阶段仅执行一次传播（P），然后通过不同的方式组合这些感受野，再将它们输入一个简单的模型以提髙性能。尽管这些方法在预测性能和扩展性方面表现出色，但它们仍然面临两个局限性。首先，现有的方法主要从模型的角度探索不同的组合方法（C），忽视了从数据为中心的角度来看，随着传播深度（P）的增加，性能会下降的问题，这种现象被称为过度平滑问题。其次，预处理过程消耗了端到端处理时间的大部分，特别是在大规模图的情况下。

为解决这些问题，我们提出了随机游走带有噪声掩蔽（RMask）模块，这是一种与现有模型简化工作兼容的即插即用模块。该模块能够探索更深的GNNs，同时保持其扩展性。与之前的模型简化工作不同，我们集中在连续的传播（P）上，发现存在于每次传播中的噪声是导致过度平滑问题的原因，并使用高效的掩蔽机制来消除这些噪声。在六个实际数据集上的实验结果表明，配备RMask的模型简化工作相较于其原始版本，在准确性和效率之间实现了良好的权衡，并且其预测性能更优。

这样翻译后，既保留了原文的专业术语，也符合学术规范，便于读者理解。 

---
# Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation 

**Title (ZH)**: _spike2former：高效的脉冲变压器用于高性能图像分割_

注：为了使翻译更符合中文学术规范，可以稍微调整一下句子结构。原文“Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation”翻译为“_spike2former：高效的脉冲变压器用于高性能图像分割_”已经比较接近规范。如果你需要更正式的表达，可以改为：

_spike2former：高效脉冲变压器模型用于高性能图像分割_

这样更符合中文的表达习惯。 

**Authors**: Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14587)  

**Abstract**: Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes. 

**Abstract (ZH)**: 脉冲神经网络（SNNs）具有低功耗优势，但在图像分割任务中表现不佳。原因在于直接将具有复杂架构设计的神经网络转换为适用于分割任务的脉冲版本会导致性能下降和非收敛问题。为应对这一挑战，我们首先识别出导致脉冲发放严重减少的网络架构模块，并进行有针对性的改进，提出了一种名为Spike2Former的架构。其次，我们提出了归一化整数脉冲神经元，以解决具有复杂架构的SNNs在训练过程中稳定性的问题。在各种语义分割数据集中，我们的方法在ADE20K数据集上将mIoU提高了12.7%，效率提高了5.0；在VOC2012数据集上将mIoU提高了14.3%，效率提高了5.2；在城市场景数据集（CityScapes）上将mIoU提高了9.1%，效率提高了6.6。 

---
# GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting 

**Title (ZH)**: GSRender: 通过弱监督3D高斯斑点图实现重复占用预测 

**Authors**: Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun  

**Link**: [PDF](https://arxiv.org/pdf/2412.14579)  

**Abstract**: 3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon. 

**Abstract (ZH)**: 三维占用感知正逐渐受到关注，因为它能够提供详细且精确的环境表示。以往的弱监督NeRF方法在效率和准确性之间进行权衡，mIoU的变化幅度为5-10个百分点，这主要归因于沿相机光线的采样数量。最近，实时高斯散射在三维重建领域获得了广泛应用，而占用预测任务也可以被视为一种重建任务。因此，我们提出了一种名为GSRender的方法，该方法自然地利用三维高斯散射进行占用预测，从而简化了采样过程。此外，二维监督的局限性会导致沿同一相机光线重复预测的问题。我们实现了Ray Compensation（光线补偿）模块，通过补偿相邻帧中的特征来缓解这一问题。最后，我们重新设计了损失函数，以消除相邻帧中动态对象的影响。通过广泛的实验，我们的方法在RayIoU上取得了SOTA（现有最佳）的结果，同时缩小了与基于三维监督方法的差距。我们的代码将很快发布。 

---
# SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar Object Detection 

**Title (ZH)**: SCKD：半监督跨模态知识精炼在4D雷达目标检测中的应用 

**Authors**: Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14571)  

**Abstract**: 3D object detection is one of the fundamental perception tasks for autonomous vehicles. Fulfilling such a task with a 4D millimeter-wave radar is very attractive since the sensor is able to acquire 3D point clouds similar to Lidar while maintaining robust measurements under adverse weather. However, due to the high sparsity and noise associated with the radar point clouds, the performance of the existing methods is still much lower than expected. In this paper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation (SCKD) method for 4D radar-based 3D object detection. It characterizes the capability of learning the feature from a Lidar-radar-fused teacher network with semi-supervised distillation. We first propose an adaptive fusion module in the teacher network to boost its performance. Then, two feature distillation modules are designed to facilitate the cross-modality knowledge transfer. Finally, a semi-supervised output distillation is proposed to increase the effectiveness and flexibility of the distillation framework. With the same network structure, our radar-only student trained by SCKD boosts the mAP by 10.38% over the baseline and outperforms the state-of-the-art works on the VoD dataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the moderate difficulty level over the baseline when extra unlabeled data are available. Code is available at this https URL. 

**Abstract (ZH)**: 三维物体检测是自主车辆基本感知任务之一。使用4D毫米波雷达完成此类任务非常具有吸引力，因为该传感器能够在恶劣天气下保持稳健测量的同时，还能像激光雷达一样获取3D点云。然而，由于雷达点云高稀疏性和噪声，现有方法的效果仍然低于预期。在本文中，我们提出了一种新颖的半监督跨模态知识蒸馏（SCKD）方法，用于基于4D雷达的三维物体检测。该方法通过半监督蒸馏，在融合激光雷达和雷达信息的教师网络中学习特征。首先，我们在教师网络中提出了一种自适应融合模块，以提升其性能。然后，设计了两个特征蒸馏模块，以促进跨模态知识传输。最后，提出了一种半监督输出蒸馏方法，以增加蒸馏框架的有效性和灵活性。在相同的网络结构下，使用SCKD训练的仅有雷达数据的学生模型，在基准模型上提高了10.38%的mAP，并在VoD数据集上优于现有最佳工作。当额外的未标注数据可用时，在ZJUOD数据集上也展示了5.12%的mAP提升。源代码可通过以下链接获取：这个https URL。 

---
# Characterising Simulation-Based Program Equilibria 

**Title (ZH)**: 基于模拟的程序均衡特性分析 

**Authors**: Emery Cooper, Caspar Oesterheld, Vincent Conitzer  

**Link**: [PDF](https://arxiv.org/pdf/2412.14570)  

**Abstract**: In Tennenholtz's program equilibrium, players of a game submit programs to play on their behalf. Each program receives the other programs' source code and outputs an action. This can model interactions involving AI agents, mutually transparent institutions, or commitments. Tennenholtz (2004) proves a folk theorem for program games, but the equilibria constructed are very brittle. We therefore consider simulation-based programs -- i.e., programs that work by running opponents' programs. These are relatively robust (in particular, two programs that act the same are treated the same) and are more practical than proof-based approaches. Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot is such an approach. Unfortunately, it is not generally applicable to games of three or more players, and only allows for a limited range of equilibria in two player games. In this paper, we propose a generalisation to Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot. We prove a folk theorem for our programs in a setting with access to a shared source of randomness. We then characterise their equilibria in a setting without shared randomness. Both with and without shared randomness, we achieve a much wider range of equilibria than Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot. Finally, we explore the limits of simulation-based program equilibrium, showing that the Tennenholtz folk theorem cannot be attained by simulation-based programs without access to shared randomness. 

**Abstract (ZH)**: 在Tennenholtz的游戏程序均衡中，游戏中的玩家提交程序来代表自己行动。每个程序接收其他程序的源代码并输出一个行动。这种方法可以模拟涉及AI代理、互操作透明机构或承诺的交互。Tennenholtz (2004) 对程序游戏证明了公理定理，但构造的均衡非常脆弱。因此，我们考虑基于模拟的程序——即通过运行对手程序来进行工作的程序。这类程序相对坚固（特别是，行动相同的两个程序被同等对待），并且比基于证明的方法更实用。Oesterheld (2019) 的$\epsilon$Grounded$\pi$Bot 就是一种这种方法。然而，这种方法对三人及以上玩家的游戏不具普遍适用性，并且仅允许两玩家游戏中的有限范围的均衡。在本文中，我们提出了Oesterheld (2019) 的$\epsilon$Grounded$\pi$Bot 的一个一般化版本。我们证明了在共享随机源可用的情况下，这类程序的公理定理。然后，我们在没有共享随机源的情况下探讨了这类程序的均衡特性。无论是有共享随机源还是没有共享随机源，我们都实现了比Oesterheld (2019) 的$\epsilon$Grounded$\pi$Bot 更广泛的均衡范围。最后，我们探讨了基于模拟的程序均衡的极限，证明了没有共享随机源的基于模拟的程序无法达到Tennenholtz的公理定理。 

---
# Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with Anomaly Aware 

**Title (ZH)**: 具备异常检测意识的全局时空融合交通预测算法 

**Authors**: Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14569)  

**Abstract**: Traffic prediction is an indispensable component of urban planning and traffic management. Achieving accurate traffic prediction hinges on the ability to capture the potential spatio-temporal relationships among road sensors. However, the majority of existing works focus on local short-term spatio-temporal correlations, failing to fully consider the interactions of different sensors in the long-term state. In addition, these works do not analyze the influences of anomalous factors, or have insufficient ability to extract personalized features of anomalous factors, which make them ineffectively capture their spatio-temporal influences on traffic prediction. To address the aforementioned issues, We propose a global spatio-temporal fusion-based traffic prediction algorithm that incorporates anomaly awareness. Initially, based on the designed anomaly detection network, we construct an efficient anomalous factors impacting module (AFIM), to evaluate the spatio-temporal impact of unexpected external events on traffic prediction. Furthermore, we propose a multi-scale spatio-temporal feature fusion module (MTSFFL) based on the transformer architecture, to obtain all possible both long and short term correlations among different sensors in a wide-area traffic environment for accurate prediction of traffic flow. Finally, experiments are implemented based on real-scenario public transportation datasets (PEMS04 and PEMS08) to demonstrate that our approach can achieve state-of-the-art performance. 

**Abstract (ZH)**: 交通预测是城市规划和交通管理不可或缺的一部分。实现准确的交通预测依赖于捕捉道路传感器之间潜在的空间-时间关系的能力。然而，现有工作大多关注局部的短期空间-时间相关性，未能充分考虑不同传感器在长期状态下的交互作用。此外，这些工作没有分析异常因素的影响，或者提取异常因素个性化特征的能力不足，这使得它们无法有效地捕捉这些因素对交通预测的空间-时间影响。为解决上述问题，我们提出了一种基于全局空间-时间融合的交通预测算法，引入了异常意识。首先，基于设计的异常检测网络，我们构建了一个高效的异常因素影响模块（AFIM），以评估意外外部事件对交通预测的空间-时间影响。此外，我们基于变压器架构提出了一个多尺度空间-时间特征融合模块（MTSFFL），以在广泛区域的交通环境中获得不同传感器之间所有可能的短期和长期相关性，从而实现对交通流的精准预测。最后，基于实际场景的公共交通数据集（PEMS04和PEMS08）进行了实验，证明我们的方法可以达到目前最先进的性能。 

---
# AIArena: A Blockchain-Based Decentralized AI Training Platform 

**Title (ZH)**: AIArena：一种基于区块链的去中心化AI训练平台 

**Authors**: Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.14566)  

**Abstract**: The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations. In this work, we propose AIArena, a blockchain-based decentralized AI training platform designed to democratize AI development and alignment through on-chain incentive mechanisms. AIArena fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIArena on the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIArena in real-world applications. 

**Abstract (ZH)**: 人工智能的 rapid advancement 早已凸显了其发展中和实施过程中的一些关键挑战，这主要是由于少数几家大型企业对人工智能的集中控制。这种权力集中加剧了人工智能模型中的偏差性，主要是由于缺乏有效的治理和监督机制。此外，这也限制了公众的参与，增加了对模型生成完整性的担忧。这种对数据和人工智能输出的垄断控制不仅威胁到了创新，也限制了公平的数据使用，因为用户在不知情的情况下贡献的数据主要有利于这些企业。在本文中，我们提出了一种基于区块链的去中心化人工智能训练平台 AIArena，通过链上激励机制，旨在实现人工智能的普及和发展。AIArena 创建了一个开放和协作的环境，参与者可以贡献模型和计算资源。链上的共识机制可以根据参与者的贡献公平地分配奖励。我们已在公共 Base 区块链 Sepolia 测试网上实现了 AIArena，并对其实现和应用进行了评估，结果表明 AIArena 在实际应用中具有可行性。 

---
# Summary of Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images 

**Title (ZH)**: 基于联邦学习的点变换器总结：用于预测乳腺癌HER2状态的苏木精和 eosin 染色全切片图像分析 

**Authors**: Kamorudeen A. Amuda, Almustapha A. Wakili  

**Link**: [PDF](https://arxiv.org/pdf/2412.14545)  

**Abstract**: This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs). 

**Abstract (ZH)**: 本文介绍了一种基于联邦学习的方法，用于预测苏木精和伊红（HE）染色的全玻片图像（WSI）中的HER2状态，从而降低费用并加快治疗决策速度。为了应对多中心数据集中标签不均衡和特征表示的挑战，提出了一种点变换器方法，该方法结合了动态标签分布、辅助分类器和最远余弦采样。广泛的实验表明，该方法在四个中心（2687张WSI）上达到了最先进的性能，并且在两个未见过的中心（229张WSI）上具有强大的泛化能力。 

---
# Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities 

**Title (ZH)**: 6G网络中人工智能与通信的综述：基础、挑战及未来研究机遇 

**Authors**: Qimei Cui, Xiaohu You, Ni Wei, Guoshun Nan, Xuefei Zhang, Jianhua Zhang, Xinchen Lyu, Ming Ai, Xiaofeng Tao, Zhiyong Feng, Ping Zhang, Qingqing Wu, Meixia Tao, Yongming Huang, Chongwen Huang, Guangyi Liu, Chenghui Peng, Zhiwen Pan, Tao Sun, Dusit Niyato, Tao Chen, Muhammad Khurram Khan, Abbas Jamalipour, Mohsen Guizani, Chau Yuen  

**Link**: [PDF](https://arxiv.org/pdf/2412.14538)  

**Abstract**: With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure. 

**Abstract (ZH)**: 随着对无缝连接和智能通信需求的不断增加，人工智能（AI）与第六代（6G）网络的集成正逐渐成为一种革命性的架构。本文对6G网络中的AI与通信进行了全面概述，强调了其基础原理、固有挑战以及未来的研究机会。我们首先对AI的历史进行回顾分析，探讨大规模AI模型的演变过程及其在塑造现代通信技术方面的重要作用。随后，讨论转向对AI在6G网络中预期集成的详细阐述，分为三个逐步发展的阶段。第一个阶段“AI赋能网络”侧重于利用AI提升网络性能、优化效率并增强用户服务体验。第二个阶段“网络赋能AI”强调网络在促进和支撑AI操作中的作用，并介绍了关键技术，包括用于AI的数字孪生和语义通信。在最后一个阶段“AI即服务”中，预计将未来的6G网络能够内建AI功能作为服务，并支持融入沉浸式通信和智能工业机器人等应用情境。具体而言，我们定义了AI服务的质量，即网络中AI服务的衡量框架系统。除此之外，我们还深入探讨了在网络环境中与AI相关的标准化进程，指出了重要里程碑和正在进行的工作。最后，我们概述了驱动AI与通信在6G技术领域不断发展和完善的潜在研究机会，将其定位为下一代通信基础设施的核心支柱。 

---
# CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection 

**Title (ZH)**: CAE-T：用于EEG异常检测的通道级自编码器与变压器相结合的方法 

**Authors**: Youshen Zhao, Keiji Iramina  

**Link**: [PDF](https://arxiv.org/pdf/2412.14522)  

**Abstract**: Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at this https URL. 

**Abstract (ZH)**: 以下是翻译成中文的版本，符合学术规范：

事件相关电位（Electroencephalogram, EEG）信号对于检测异常脑活动至关重要，但其高维度和复杂性对有效分析构成了重大挑战。本文提出了一种名为CAE-T的新框架，该框架将通道级卷积神经网络（Convolutional Neural Network, CNN）自动编码器与单头变压器分类器相结合，以实现高效的EEG异常检测。通道级自动编码器能够压缩原始EEG信号并保持通道独立性，降低计算成本并保留生物意义显著的特征。压缩后的表示随后被输入基于变压器的分类器，该分类器能够有效地建模长期依赖性，以区分正常和异常信号。在TUU异常EEG语料库上进行评估，所提出模型在每个病例水平上实现了85.0%的准确率、76.2%的灵敏度和91.2%的特异度，优于诸如EEGNet、Deep4Conv和FusionCNN等基线模型。此外，CAE-T仅需202M FLOPs和2.9M参数，显著优于基于变压器的替代方案。该框架通过其通道级设计保留了可解释性，展示了在神经科学研究和临床实践中广泛应用的巨大潜力。源代码已在此处提供：<该网址>。

注：上述翻译中，“事件相关电位”指的是“事件相关电位”（Event-related potentials, ERPs），在缺少具体上下文的情况下，原文中的“Electroencephalogram (EEG) signals”在医学和脑科学研究中一般是直接指“电生理信号”或“脑电图信号”，但在特定语境中，也可为“事件相关电位”。根据具体应用场景，此术语可进一步调整。 

---
# PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization 

**Title (ZH)**: PA-RAG：基于多视角偏好优化的RAG对齐 

**Authors**: Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao  

**Link**: [PDF](https://arxiv.org/pdf/2412.14510)  

**Abstract**: The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at this https URL. 

**Abstract (ZH)**: 检索增强生成（RAG）的出现缓解了大型语言模型（LLMs）生成过程中内容过时和幻觉的问题，但仍揭示出许多局限性。当通用型LLM作为RAG生成器时，它往往在响应信息丰富性、响应稳健性和引证质量方面存在不足。过去解决这些问题的方法要么通过生成响应之外的额外步骤，要么通过监督微调（SFT）优化生成器，这些方法仍未充分满足RAG的要求。因此，如何从多个偏好角度优化RAG生成器并保持其端到端的LLM形式仍是一个挑战。为解决这一问题，我们提出了一种综合性优化方法——多角度偏好对齐的检索增强生成（PA-RAG），用于全面优化RAG系统中的生成器。具体而言，我们通过在不同提示文档质量场景中采样多质量级别的响应来构建高质量的指令微调数据和多角度偏好数据。随后，我们使用监督微调（SFT）和直接偏好优化（DPO）对生成器进行优化。我们在三个LLM的四个问答数据集上进行了广泛的实验，结果表明PA-RAG可以显著提升RAG生成器的性能。我们的代码和数据集可在以下链接获得：[此 https URL]。 

---
# Treatment Effects Estimation on Networked Observational Data using Disentangled Variational Graph Autoencoder 

**Title (ZH)**: 使用解耦变分图自编码器估计网络观测数据中的治疗效应 

**Authors**: Di Fan, Renlei Jiang, Yunhao Wen, Chuanhou Gao  

**Link**: [PDF](https://arxiv.org/pdf/2412.14497)  

**Abstract**: Estimating individual treatment effect (ITE) from observational data has gained increasing attention across various domains, with a key challenge being the identification of latent confounders affecting both treatment and outcome. Networked observational data offer new opportunities to address this issue by utilizing network information to infer latent confounders. However, most existing approaches assume observed variables and network information serve only as proxy variables for latent confounders, which often fails in practice, as some variables influence treatment but not outcomes, and vice versa. Recent advances in disentangled representation learning, which disentangle latent factors into instrumental, confounding, and adjustment factors, have shown promise for ITE estimation. Building on this, we propose a novel disentangled variational graph autoencoder that learns disentangled factors for treatment effect estimation on networked observational data. Our graph encoder further ensures factor independence using the Hilbert-Schmidt Independence Criterion. Extensive experiments on two semi-synthetic datasets derived from real-world social networks and one synthetic dataset demonstrate that our method achieves state-of-the-art performance. 

**Abstract (ZH)**: 从观察数据中估计个体治疗效应（ITE）在各个领域都受到了越来越多的关注，一个关键挑战是识别影响治疗和结果的潜在混杂因素。网络化的观察数据提供了新的机会，通过利用网络信息来推断潜在混杂因素。然而，现有的大多数方法假设观测变量和网络信息仅作为潜在混杂因素的代理变量，而在实践中这往往无法实现，因为一些变量可能影响治疗但不直接影响结果，反之亦然。近年来，解开表征学习的进展，能够将潜在因素分离为工具变量、混杂变量和调整变量，为ITE估计带来了希望。在此基础上，我们提出了一种新的解开表示的变分图自编码器，用于网络化观察数据中的治疗效果估计。该图编码器通过希尔伯特-施密特独立性判据进一步确保了因素的独立性。在来自真实世界社交网络的两个半合成数据集和一个合成数据集上进行的广泛实验表明，我们的方法达到了最先进的性能。 

---
# Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization 

**Title (ZH)**: 含有多重外推动量的随机一阶方法在高度光滑的无约束优化中的应用 

**Authors**: Chuan He  

**Link**: [PDF](https://arxiv.org/pdf/2412.14488)  

**Abstract**: In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Specifically, assuming that the gradient and the $p$th-order derivative of $f$ are Lipschitz continuous for some $p\ge2$, and under some additional mild assumptions, we establish that our method achieves a sample complexity of $\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})$ for finding a point $x$ satisfying $\mathbb{E}[\|\nabla f(x)\|]\le\epsilon$. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings. 

**Abstract (ZH)**: 在本文中，我们考虑了一个无约束随机优化问题，其中目标函数呈现出较高的光滑性。特别地，我们提出了一种具有多步外推动量的随机一阶方法（SFOM），在每次迭代中进行多次外推，随后基于这些外推进行动量步骤。我们展示了我们的提议方法——具有多步外推动量的SFOM——可以通过利用目标函数 \(f\) 的高阶光滑性来加速优化过程。具体而言，假设函数 \(f\) 的梯度以及 \(p\) 阶导数在某些 \(p \ge 2\) 下是利普希兹连续的，并且在一些附加的温和假设下，我们证明了该方法可以在期望梯度范数 \(\mathbb{E}[\|\nabla f(x)\|]\le\epsilon\) 时达到 \(\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})\) 的样本复杂度。据我们所知，我们的方法是第一个利用目标函数任意阶光滑性来加速的SFOM，其样本复杂度在不假设平均光滑性条件的情况下严格优于已知的最佳结果。最后，初步的数值实验验证了我们方法的实际性能，并与我们的理论发现相一致。 

---
# HashAttention: Semantic Sparsity for Faster Inference 

**Title (ZH)**: HashAttention：语义稀疏化以实现更快的推理 

**Authors**: Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana Klimovic, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica  

**Link**: [PDF](https://arxiv.org/pdf/2412.14468)  

**Abstract**: Utilizing longer contexts is increasingly essential to power better AI systems. However, the cost of attending to long contexts is high due to the involved softmax computation. While the scaled dot-product attention (SDPA) exhibits token sparsity, with only a few pivotal tokens significantly contributing to attention, leveraging this sparsity effectively remains an open challenge. Previous methods either suffer from model degradation or require considerable additional resources. We propose HashAttention --a principled approach casting pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space capturing the required semantic similarity using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query in this Hamming space using bitwise operations, and only these pivotal tokens are used for attention computation, significantly improving overall attention efficiency. HashAttention can reduce the number of tokens used by a factor of $1/32\times$ for the Llama-3.1-8B model with LongBench, keeping average quality loss within 0.6 points, while using only 32 bits per token auxiliary memory. At $32\times$ sparsity, HashAttention is $3{-}6\times$ faster than LightLLM and $2.5{-}4.5\times$ faster than gpt-fast on Nvidia-L4 GPU. 

**Abstract (ZH)**: 利用更长的上下文对于提升AI系统的能力变得越来越重要。然而，处理长上下文的成本很高，因为涉及到了softmax计算。虽然缩放点积注意机制（SDPA）展示了令牌稀疏性，即只有少数几个关键的令牌对注意力贡献较大，但有效地利用这种稀疏性仍是一个开放的挑战。以往的方法要么导致模型性能下降，要么需要大量的额外资源。我们提出了一种名为HashAttention的方法——这是一种原理性的方法，将其关键令牌识别问题转化为推荐问题。对于给定的查询，HashAttention将键和查询编码到汉明空间中，并利用学习到的映射函数捕获所需的语义相似度。HashAttention通过位操作高效地识别出对给定查询而言的关键令牌，并仅使用这些关键令牌进行注意力计算，从而显著提高整体注意力效率。在使用LongBench对Llama-3.1-8B模型的实验中，HashAttention可以将所使用令牌的数量减少到原来的1/32，同时在平均质量损失控制在0.6分以内的前提下，只需要每令牌使用32位的辅助存储。在汉明稀疏度达到32倍的情况下，HashAttention在Nvidia L4 GPU上比LightLLM快3-6倍，比gpt-fast快2.5-4.5倍。 

---
# CLDG: Contrastive Learning on Dynamic Graphs 

**Title (ZH)**: CLDG：动态图上的对比学习 

**Authors**: Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, Qinghua Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.14451)  

**Abstract**: The graph with complex annotations is the most potent data type, whose constantly evolving motivates further exploration of the unsupervised dynamic graph representation. One of the representative paradigms is graph contrastive learning. It constructs self-supervised signals by maximizing the mutual information between the statistic graph's augmentation views. However, the semantics and labels may change within the augmentation process, causing a significant performance drop in downstream tasks. This drawback becomes greatly magnified on dynamic graphs. To address this problem, we designed a simple yet effective framework named CLDG. Firstly, we elaborate that dynamic graphs have temporal translation invariance at different levels. Then, we proposed a sampling layer to extract the temporally-persistent signals. It will encourage the node to maintain consistent local and global representations, i.e., temporal translation invariance under the timespan views. The extensive experiments demonstrate the effectiveness and efficiency of the method on seven datasets by outperforming eight unsupervised state-of-the-art baselines and showing competitiveness against four semi-supervised methods. Compared with the existing dynamic graph method, the number of model parameters and training time is reduced by an average of 2,001.86 times and 130.31 times on seven datasets, respectively. 

**Abstract (ZH)**: 具有复杂注释的图是最强大的数据类型，其不断变化特性激发了无监督动态图表示的进一步探索。其中一个代表性范式是图对比学习。该方法通过最大化统计图增强视角之间的互信息来构建自我监督信号。然而，在增强过程中，语义和标签可能会发生变化，导致下游任务性能显著下降。这一问题在动态图上尤为突出。为解决这一问题，我们设计了一个简单而有效的框架，名为CLDG。首先，我们阐明动态图在不同层次上具有时间平移不变性。然后，我们提出了一种抽样层来提取时间持久的信号，它将鼓励节点在时间跨度视角下保持一致的局部和全局表示，即时间平移不变性。广泛的实验结果证明了该方法在七个数据集上的有效性和效率，通过在七个数据集上超越八个无监督的最新基线方法，并且与四种半监督方法保持竞争力。与现有的动态图方法相比，该方法在七个数据集上的模型参数数量和训练时间分别减少了2,001.86倍和130.31倍。 

---
# GenHMR: Generative Human Mesh Recovery 

**Title (ZH)**: GenHMR：生成式人体网格恢复 

**Authors**: Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Srijan Das, Chen Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.14444)  

**Abstract**: Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at this https URL 

**Abstract (ZH)**: 人体网格恢复（HMR）在许多计算机视觉应用中都至关重要，从健康到艺术和娱乐领域。单目图像的HMR问题主要由确定性方法解决，这些方法针对给定的2D图像输出一个单一预测。然而，由于深度不确定性和遮挡，从单目图像进行HMR是一个病态问题。尽管概率方法尝试通过生成和融合多个可能的3D重建来解决这一问题，但它们的表现往往落后于确定性方法。在本文中，我们引入了GenHMR，这是一种新颖的生成框架，将单目HMR重新表述为基于图像生成的任务，明确地建模和缓解了从2D到3D映射过程中的不确定性。GenHMR包含两个关键组件：（1）姿态分词器，将3D人体姿态转换为潜在空间中的离散标记序列，以及（2）基于图像的掩模变换器，该变换器在输入图像提示和随机掩模标记序列的条件下学习姿态标记的概率分布。在推理过程中，模型从学习的条件分布中采样，逐步解码高置信度的姿态标记，从而减少3D重建的不确定性。为了进一步细化重建，我们提出了一种基于2D姿态的细化技术，直接在潜在空间中微调解码的姿态标记，迫使投影的3D人体网格与2D姿态线索对齐。在基准数据集上的实验表明，GenHMR 显著优于最先进的方法。网站链接可在此找到：[提供的链接] 

---
# ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study 

**Title (ZH)**: ORBIT：用于大型语言模型领域适应的成本效益数据集管理——以天文学案例研究为例 

**Authors**: Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko  

**Link**: [PDF](https://arxiv.org/pdf/2412.14436)  

**Abstract**: Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning \textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69\% to 76\% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed \textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in 73\% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 近年来，在语言模型方面的进展凸显了高质量领域特定训练数据的必要性，尤其是在需要专业领域知识的任务中。通用模型虽然功能强大，但在专业任务中往往缺乏必要的深度，因为它们在特定领域的信息有限。领域适应训练可以增强这些模型，但这类训练需要大量高质量的数据。为了应对这一挑战，我们提出了一种名为ORBIT的方法，该方法旨在从噪声较大的网络来源中高效地收集大规模高质量的领域特定数据集，以训练专业大型语言模型。以天文学为例，我们对1.3T词的FineWeb-Edu数据集进行了细化，得到了一个专注于天文学的高质量、100亿词的子集。进一步地，通过在天文学子集上微调\textsc{LLaMA-3-8B}模型，MMLU天文学基准测试的性能提高了7.6个百分点，达到了AstroBench（一个专门针对天文学的基准测试）的顶级成绩。此外，我们的模型（Orbit-LLaMA）在GPT-4o评估中优于\textsc{LLaMA-3-8B-base}模型，在1000个具体针对天文学的问题中有73%的情况下被偏好。我们还通过将其应用于法律和医学领域，验证了ORBIT方法的泛化能力，相较于未过滤的数据基线，我们实现了显著的数据质量提升。我们开源了ORBIT方法，包括精心收集的数据集、代码库和生成模型，网址为\href{this https URL}{this https URL}。 

---
# Cherry-Picking in Time Series Forecasting: How to Select Datasets to Make Your Model Shine 

**Title (ZH)**: 时间序列预测中的挑战组合：如何选择数据集使模型脱颖而出 

**Authors**: Luis Roque, Carlos Soares, Vitor Cerqueira, Luis Torgo  

**Link**: [PDF](https://arxiv.org/pdf/2412.14435)  

**Abstract**: The importance of time series forecasting drives continuous research and the development of new approaches to tackle this problem. Typically, these methods are introduced through empirical studies that frequently claim superior accuracy for the proposed approaches. Nevertheless, concerns are rising about the reliability and generalizability of these results due to limitations in experimental setups. This paper addresses a critical limitation: the number and representativeness of the datasets used. We investigate the impact of dataset selection bias, particularly the practice of cherry-picking datasets, on the performance evaluation of forecasting methods. Through empirical analysis with a diverse set of benchmark datasets, our findings reveal that cherry-picking datasets can significantly distort the perceived performance of methods, often exaggerating their effectiveness. Furthermore, our results demonstrate that by selectively choosing just four datasets - what most studies report - 46% of methods could be deemed best in class, and 77% could rank within the top three. Additionally, recent deep learning-based approaches show high sensitivity to dataset selection, whereas classical methods exhibit greater robustness. Finally, our results indicate that, when empirically validating forecasting algorithms on a subset of the benchmarks, increasing the number of datasets tested from 3 to 6 reduces the risk of incorrectly identifying an algorithm as the best one by approximately 40%. Our study highlights the critical need for comprehensive evaluation frameworks that more accurately reflect real-world scenarios. Adopting such frameworks will ensure the development of robust and reliable forecasting methods. 

**Abstract (ZH)**: 时间序列预测的重要性推动了持续的研究，并发展出了新的方法来应对这一挑战。通常，这些方法通过实证研究引入，并经常声称所提出的方法具有更高的准确性。然而，由于实验设置的局限性，人们对这些结果的可靠性和通用性产生了担忧。本文着重解决了一个关键限制：用于评估的方法所使用的数据集的数量和代表性。我们调查了数据集选择偏差，特别是数据集挑选偏好的实践，对预测方法性能评估的影响。通过使用多样化基准数据集的实证分析，我们的研究发现，数据集的选择偏好可能会显著扭曲方法的感知性能，经常夸大其效果。此外，我们的结果表明，通过仅选择四个数据集（研究中最常报告的数量），46%的方法可以被认为是最佳方法，77%的方法可以排名前三。另外，最近基于深度学习的方法对数据集的选择高度敏感，而经典的预测方法则表现出更大的鲁棒性。最后，我们的结果表明，在使用基准数据集的子集实证验证预测算法时，从测试3个数据集增加到6个数据集，可以将错误识别出最佳算法的风险降低约40%。我们的研究强调了制定更全面评估框架的迫切需要，以便更准确地反映现实世界的情景。采用这样的评估框架将确保开发出稳健和可靠的预测方法。 

---
# All-in-One Tuning and Structural Pruning for Domain-Specific LLMs 

**Title (ZH)**: 面向特定领域的大型语言模型的一站式调优与结构剪枝 

**Authors**: Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao  

**Link**: [PDF](https://arxiv.org/pdf/2412.14426)  

**Abstract**: Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively. 

**Abstract (ZH)**: 面向领域特定应用的大型语言模型（LLMs）现有的剪枝技术通常遵循两个阶段的过程：首先对预训练的通用LLMs进行剪枝，然后在特定领域对其进行微调。然而，在微调过程中，从预训练权重得出的剪枝决策保持不变，即使权重已被更新。因此，这种剪枝决策与微调权重的结合可能不是最优的，导致性能显著下降。为了解决这些局限性，我们提出了一种统一的一站式结构剪枝和微调方法——All-in-One Tuning and Structural Pruning（ATP）。ATP通过可训练的剪枝决策生成器动态识别微调过程中的最优子结构。此外，由于领域特定应用可用数据有限，低秩适应（LoRA）成为了一种常用方法来微调LLMs。在ATP中，我们引入了LoRA感知的前向传播和稀疏正则化，以确保在ATP处理后，根据学习到的剪枝决策得到的子结构可以被直接移除。实验结果显示，ATP在法律和医疗领域的任务上优于最先进的两阶段剪枝方法。具体而言，当分别对LLaMA2-7B和LLaMA3-8B模型剪枝40%的参数时，ATP分别恢复了88%和91%的密集模型性能。 

---
# FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning 

**Title (ZH)**: FedPIA —— 利用沃舍廷贝卢中枢（Wasserstein Barycenters）排列和集成适配器的多模态联邦学习基础模型微调方法 

**Authors**: Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble  

**Link**: [PDF](https://arxiv.org/pdf/2412.14424)  

**Abstract**: Large Vision-Language Models typically require large text and image datasets for effective fine-tuning. However, collecting data from various sites, especially in healthcare, is challenging due to strict privacy regulations. An alternative is to fine-tune these models on end-user devices, such as in medical clinics, without sending data to a server. These local clients typically have limited computing power and small datasets, which are not enough for fully fine-tuning large VLMs on their own. A naive solution to these scenarios is to leverage parameter-efficient fine-tuning (PEFT) strategies and apply federated learning (FL) algorithms to combine the learned adapter weights, thereby respecting the resource limitations and data privacy. However, this approach does not fully leverage the knowledge from multiple adapters trained on diverse data distributions and for diverse tasks. The adapters are adversely impacted by data heterogeneity and task heterogeneity across clients resulting in suboptimal convergence. To this end, we propose a novel framework called FedPIA that improves upon the naive combinations of FL and PEFT by introducing Permutation and Integration of the local Adapters in the server and global Adapters in the clients exploiting Wasserstein barycenters for improved blending of client-specific and client-agnostic knowledge. This layerwise permutation helps to bridge the gap in the parameter space of local and global adapters before integration. We conduct over 2000 client-level experiments utilizing 48 medical image datasets across five different medical vision-language FL task settings encompassing visual question answering as well as image and report-based multi-label disease detection. Our experiments involving diverse client settings, ten different modalities, and two VLM backbones demonstrate that FedPIA consistently outperforms the state-of-the-art PEFT-FL baselines. 

**Abstract (ZH)**: 大型视觉-语言模型通常需要大型文本和图像数据集才能实现有效的微调。然而，在医疗保健等领域收集数据颇具挑战性，因为受到严格的数据隐私法规限制。一种替代方案是在终端用户设备上对这些模型进行微调，例如在医疗诊所中，而不将数据发送到服务器。这些本地客户端通常计算能力有限且数据集较小，不足以独立完成大型视觉-语言模型的全面微调。一种朴素的解决方案是利用参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）策略，并应用联邦学习（Federated Learning, FL）算法结合本地适配器权重，从而尊重计算资源限制和数据隐私。然而，这种做法并不能充分利用在多种数据分布和不同任务上训练的多个适配器的知识。不同终端用户的异质数据和任务导致适配器性能受损，从而导致次优收敛。对此，我们提出了一种名为FedPIA的新框架，通过引入服务器侧的排列和集成本地适配器与客户端侧的全局适配器，并利用Wasserstein巴泽纳齐中心（Wasserstein barycenters）进行改进的客户特定与客户无关知识融合，改进了FL与PEFT的简单组合。逐层排列有助于在集成之前缩小本地和全局适配器的参数空间差距。我们通过在不同医疗视觉-语言FL任务设置（包括视觉问答以及基于图像和报告的多标签疾病检测）中采用48个医学图像数据集进行超过2000次客户端级别的实验进行了验证。涉及多样化客户端设置、十种不同模态以及两种视觉-语言模型骨干的实验表明，FedPIA在所有实验中均优于最先进的PEFT-FL基线。 

---
# Enhancing Diffusion Models for High-Quality Image Generation 

**Title (ZH)**: 增强扩散模型以生成高质量图像 

**Authors**: Jaineet Shah, Michael Gromis, Rickston Pinto  

**Link**: [PDF](https://arxiv.org/pdf/2412.14422)  

**Abstract**: This report presents the comprehensive implementation, evaluation, and optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs), which are state-of-the-art generative models. During inference, these models take random noise as input and iteratively generate high-quality images as output. The study focuses on enhancing their generative capabilities by incorporating advanced techniques such as Classifier-Free Guidance (CFG), Latent Diffusion Models with Variational Autoencoders (VAE), and alternative noise scheduling strategies. The motivation behind this work is the growing demand for efficient and scalable generative AI models that can produce realistic images across diverse datasets, addressing challenges in applications such as art creation, image synthesis, and data augmentation. Evaluations were conducted on datasets including CIFAR-10 and ImageNet-100, with a focus on improving inference speed, computational efficiency, and image quality metrics like Frechet Inception Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference and superior image quality. Challenges with VAE and noise scheduling are also highlighted, suggesting opportunities for future optimization. This work lays the groundwork for developing scalable, efficient, and high-quality generative AI systems to benefit industries ranging from entertainment to robotics. 

**Abstract (ZH)**: 本报告全面介绍了去噪扩散概率模型（DDPMs）和去噪扩散隐式模型（DDIMs）的实现、评估与优化。DDPMs和DDIMs是当前最先进的生成模型。在推理过程中，这些模型将随机噪声作为输入，并逐步生成高质量的图像作为输出。本研究旨在通过引入先进的技术，如无分类指导（Classifier-Free Guidance，CFG）、潜变量扩散模型与变分自编码器（Latent Diffusion Models with Variational Autoencoders，VAE），以及不同的噪声调度策略来增强它们的生成能力。这一工作的驱动力是针对跨多种数据集生产逼真图像的高效和可扩展生成AI模型的需求，解决了诸如艺术创作、图像合成和数据增强等应用中的挑战。评估在CIFAR-10和ImageNet-100等数据集上进行，重点关注提高推理速度、计算效率和图像质量指标（如弗雷切尔- incidental 距离，FID）的表现。研究结果表明，DDIM结合CFG可以实现更快的推理速度和更高质量的图像。同时，研究还指出了VAE和噪声调度的挑战，为未来的优化提供了机遇。本研究为开发可扩展、高效且高质量的生成AI系统奠定了基础，这些系统将惠及从娱乐到机器人等多个行业。 

---
# DriveGPT: Scaling Autoregressive Behavior Models for Driving 

**Title (ZH)**: DriveGPT：扩展自回归行为模型在自动驾驶中的应用 

**Authors**: Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14415)  

**Abstract**: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling. 

**Abstract (ZH)**: 我们将介绍DriveGPT，这是一种可扩展的自动驾驶行为模型。我们将驾驶视为一个序列决策任务，并利用变换器模型以自回归的方式预测未来代理状态。通过大幅提升模型参数和训练数据的数量级，我们能够从数据集规模、模型参数和计算资源方面探索模型的可扩展性。我们通过规划任务中的定量指标和定性案例，包括在复杂真实世界场景中的闭环驾驶，评估了DriveGPT在不同规模下的性能。在一项单独的预测任务中，DriveGPT超越了最先进的基线模型，并通过在大规模数据集上进行预训练展示了性能的提升，进一步验证了数据规模对于模型性能的益处。 

---
# I0T: Embedding Standardization Method Towards Zero Modality Gap 

**Title (ZH)**: IoT: 面向零模态差距的标准化嵌入方法 

**Authors**: Na Min An, Eunki Kim, James Thorne, Hyunjung Shim  

**Link**: [PDF](https://arxiv.org/pdf/2412.14384)  

**Abstract**: Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\text{I0T}_{\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\text{I0T}_{\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\text{I0T}_{\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S). 

**Abstract (ZH)**: Contrastive 语言-图像预训练（CLIP）使零样本推理成为诸如图像-文本检索和分类等下游任务的可能。然而，扩展CLIP的近期工作遇到了模态差距的问题，当图像和文本嵌入被投影到不同的流形上时，这种情况会偏离图像-文本对比学习的初衷。我们发现这种现象与各自独立的图像/文本编码器所具有的模态特异性特征有关，并提出两种方法来解决模态差距问题：（1）一种事后嵌入标准化方法$\text{I0T}_{\text{post}}$，它可以将模态差距大约减少到零；（2）一种可训练方法$\text{I0T}_{\text{async}}$，通过为每个编码器增加两个规范化层来缓解模态差距问题。我们的I0T框架可以在保持训练模型原嵌入表示和锁定参数不变的情况下显著减少模态差距。在实践中，$\text{I0T}_{\text{post}}$可以作为广泛使用的CLIPScore（CLIP-S）的替代可解释自动评估指标。 

---
# Surrealistic-like Image Generation with Vision-Language Models 

**Title (ZH)**: 使用视觉-语言模型生成类似超现实主义的图像 

**Authors**: Elif Ayten, Shuai Wang, Hjalmar Snoep  

**Link**: [PDF](https://arxiv.org/pdf/2412.14366)  

**Abstract**: Recent advances in generative AI make it convenient to create different types of content, including text, images, and code. In this paper, we explore the generation of images in the style of paintings in the surrealism movement using vision-language generative models, including DALL-E, Deep Dream Generator, and DreamStudio. Our investigation starts with the generation of images under various image generation settings and different models. The primary objective is to identify the most suitable model and settings for producing such images. Additionally, we aim to understand the impact of using edited base images on the generated resulting images. Through these experiments, we evaluate the performance of selected models and gain valuable insights into their capabilities in generating such images. Our analysis shows that Dall-E 2 performs the best when using the generated prompt by ChatGPT. 

**Abstract (ZH)**: Recent进展在生成型人工智能使创建不同类型的内容变得便捷，包括文本、图像和代码。本文中，我们探讨了使用视觉-语言生成模型（如DALL-E、Deep Dream Generator和DreamStudio）生成超现实主义绘画风格图像的可能性。我们的研究始于在不同图像生成设置和不同模型下的图像生成。主要目标是识别生成此类图像的最佳模型和设置。此外，我们还旨在了解使用编辑后的基础图像对生成图像的影响。通过这些实验，我们评估了所选模型的性能，并获得了它们在生成此类图像方面的宝贵见解。我们的分析表明，当使用ChatGPT生成的提示时，Dall-E 2表现出最佳性能。 

---
# Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference 

**Title (ZH)**: 面向大规模实时强化学习的交错异步推理方法 

**Authors**: Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish  

**Link**: [PDF](https://arxiv.org/pdf/2412.14355)  

**Abstract**: Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pokémon and Tetris. 

**Abstract (ZH)**: 实时环境在智能体进行动作推理和学习的过程中不断变化，因此需要高频率的交互来有效地最小化遗憾。然而，近期的机器学习进展涉及更大的神经网络和更长的推理时间，这在需要快速反应的实时系统中引发了其适用性的问题。我们对实时强化学习（RL）环境中的后悔下界进行分析，表明在典型的顺序交互和学习范式中一般不可能完全最小化长期后悔，但在计算过程足够异步时，这种情况往往变得可能。我们提出了新颖的算法，用于调度异步推理过程，以确保动作在固定的时间间隔内被采取。我们的研究表明，当从实时模拟游戏男孩游戏（如宠物小精灵和俄罗斯方块）的过程中学习时，所需的推理过程数量随推理时间的增长线性增加，同时能够使用比现有方法大多个数量级的模型，并且这种模型的动作推理时间仅受限于推理窗口内的环境有效随机性，而非动作频率。 

---
# Is Peer-Reviewing Worth the Effort? 

**Title (ZH)**: 《Peer Review值得付出努力吗？》

这个标题是对学术论文中同行评审过程的反思，将其翻译为《Peer Review值得付出努力吗？》既保留了原文的疑问性质，又符合中文的表达习惯。 

**Authors**: Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad  

**Link**: [PDF](https://arxiv.org/pdf/2412.14351)  

**Abstract**: How effective is peer-reviewing in identifying important papers? We treat this question as a forecasting task. Can we predict which papers will be highly cited in the future based on venue and "early returns" (citations soon after publication)? We show early returns are more predictive than venue. Finally, we end with constructive suggestions to address scaling challenges: (a) too many submissions and (b) too few qualified reviewers. 

**Abstract (ZH)**: 同行评审在识别重要论文方面有多有效？我们将这个问题视为一个预测任务。我们能否根据会议和“早期反馈”（即论文出版后不久的引用情况）来预测哪些论文在未来会被高度引用？结果显示，早期反馈比会议更能预测。最后，我们提出了几项建设性的建议来应对扩展挑战：（a）提交论文过多；（b）合格的评审人不足。 

---
# A Unifying Information-theoretic Perspective on Evaluating Generative Models 

**Title (ZH)**: 一个统一的信息论视角下的生成模型评估方法 

**Authors**: Alexis Fox, Samarth Swarup, Abhijin Adiga  

**Link**: [PDF](https://arxiv.org/pdf/2412.14340)  

**Abstract**: Considering the difficulty of interpreting generative model output, there is significant current research focused on determining meaningful evaluation metrics. Several recent approaches utilize "precision" and "recall," borrowed from the classification domain, to individually quantify the output fidelity (realism) and output diversity (representation of the real data variation), respectively. With the increase in metric proposals, there is a need for a unifying perspective, allowing for easier comparison and clearer explanation of their benefits and drawbacks. To this end, we unify a class of kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens using approaches from kNN density estimation. Additionally, we propose a tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity and two distinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic concepts of entropy and cross-entropy, can be dissected for both sample- and mode-level analysis. Our detailed experimental results demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics. 

**Abstract (ZH)**: 考虑到生成模型输出解释的难度，当前研究主要集中于确定有意义的评价指标。一些最近的方法借鉴了分类领域的“精确率”和“召回率”来单独量化输出的真实性和输出的多样性，分别对应生成数据与真实数据的一致性和变化范围。随着评价指标的不断增加，需要一个统一的视角，以便更容易地进行比较并清楚地解释其优点和缺点。为此，我们使用来自k最近邻密度估计的方法，将一类基于k最近邻（kNN）的评价指标统一到信息论视角下。此外，我们提出了一种三维评价指标，由精确率交叉熵（PCE）、召回率交叉熵（RCE）和召回率熵（RE）组成，分别测量真实性及类间和类内的两种不同多样性方面。我们的领域通用评价指标，基于信息论中的熵和交叉熵概念，可以从样本级和模式级进行分析。我们详尽的实验结果展示了评价指标各组成部分对其相应特性的敏感性，并揭示了其他评价指标的一些不可取行为。 

---
# Embedding Cultural Diversity in Prototype-based Recommender Systems 

**Title (ZH)**: 将文化多样性嵌入原型推荐系统中 

**Authors**: Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi  

**Link**: [PDF](https://arxiv.org/pdf/2412.14329)  

**Abstract**: Popularity bias in recommender systems can increase cultural overrepresentation by favoring norms from dominant cultures and marginalizing underrepresented groups. This issue is critical for platforms offering cultural products, as they influence consumption patterns and human perceptions. In this work, we address popularity bias by identifying demographic biases within prototype-based matrix factorization methods. Using the country of origin as a proxy for cultural identity, we link this demographic attribute to popularity bias by refining the embedding space learning process. First, we propose filtering out irrelevant prototypes to improve representativity. Second, we introduce a regularization technique to enforce a uniform distribution of prototypes within the embedding space. Across four datasets, our results demonstrate a 27\% reduction in the average rank of long-tail items and a 2\% reduction in the average rank of items from underrepresented countries. Additionally, our model achieves a 2\% improvement in HitRatio@10 compared to the state-of-the-art, highlighting that fairness is enhanced without compromising recommendation quality. Moreover, the distribution of prototypes leads to more inclusive explanations by better aligning items with diverse prototypes. 

**Abstract (ZH)**: 推荐系统中的流行度偏差可能会增加文化代表性，因为它倾向于推崇主导文化 norms，同时边缘化代表性不足的群体。这对提供文化产品的平台来说是一个关键问题，因为这些平台会影响消费模式和人类感知。在这项工作中，我们通过识别基于原型的矩阵分解方法中的人口统计学偏差来解决流行度偏差问题。使用国家作为文化身份的代理指标，我们通过改进嵌入空间的学习过程将这一人口统计学属性与流行度偏差联系起来。首先，我们建议过滤掉无关的原型以提高代表性。其次，我们引入了一种正则化技术，以确保嵌入空间中原型的均匀分布。在四个数据集中，我们的结果表明，平均尾部项目的排名降低了27%，来自代表性不足国家的项目的平均排名降低了2%。此外，与当前最先进的方法相比，我们的模型在@10的HitRatio上提高了2%，这表明在不牺牲推荐质量的情况下，公平性得到了提升。此外，原型的分布导致了更具包容性的解释，因为它更好地将项目与多样化的原型对齐。 

---
# Semantic Role Labeling of NomBank Partitives 

**Title (ZH)**: NomBank 部分谓词的语义角色标注 

**Authors**: Adam Meyers, Advait Pravin Savant, John E. Ortega  

**Link**: [PDF](https://arxiv.org/pdf/2412.14328)  

**Abstract**: This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using "gold" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development. 

**Abstract (ZH)**: 本文探讨了英语言义角色标注中的限定名词（例如：5%/REL of the price/ARG1；The price/ARG1 rose 5 percent/REL）在NomBank标注语料库中的应用。文中描述了使用传统机器学习和基于变换器的方法构建了多个系统，并采用了集成方法。我们得分最高的系统在使用宾州树库中的“黄金”解析结果时获得了91.74%的F1分数，而在使用伯克利神经解析器时则获得了91.12%的F1分数。本研究包括课堂和实验环境下的系统开发环境。 

---
# The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation 

**Title (ZH)**: 《属性名词的处理在提升中文到英文机器翻译中的作用》 

**Authors**: Haohao, Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos  

**Link**: [PDF](https://arxiv.org/pdf/2412.14323)  

**Abstract**: Translating between languages with drastically different grammatical conventions poses challenges, not just for human interpreters but also for machine translation systems. In this work, we specifically target the translation challenges posed by attributive nouns in Chinese, which frequently cause ambiguities in English translation. By manually inserting the omitted particle X ('DE'). In news article titles from the Penn Chinese Discourse Treebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to English translation models, specifically improving how this critical function word is handled. This focused approach not only complements the broader strategies suggested by previous studies but also offers a practical enhancement by specifically addressing a common error type in Chinese-English translation. 

**Abstract (ZH)**: 将结构差异巨大的语言之间进行翻译，不仅给人类译者带来了挑战，也给机器翻译系统带来了挑战。本研究特别针对中文中的限定名词带来的翻译难题，这些名词在英文翻译中经常造成歧义。通过手工插入被省略的“的”字（X），我们利用宾夕法尼亚中文语料库中的新闻标题数据集，专门对Hugging Face的中英翻译模型进行了微调，特别是在处理这一关键功能词方面取得了改进。这种专注的方法不仅补充了先前研究提出的更广泛策略，还通过具体解决中文-英语翻译中的常见错误类型提供了实际的提升。 

---
# Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs 

**Title (ZH)**: Multilingual OphthaLingua：评估和去偏见LM在LMICs中眼科QA的多语言基准荏 

**Authors**: David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama  

**Link**: [PDF](https://arxiv.org/pdf/2412.14304)  

**Abstract**: Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe. 

**Abstract (ZH)**: 当前的眼科临床工作流程受到过度转诊、漫长等待时间和复杂异质性医疗记录的困扰。大规模语言模型（LLMs）有望自动化各种程序，如分诊、初步测试（如视力评估）和报告总结。然而，LLMs在自然语言问答任务中的性能在不同语言之间表现出显著差异，这可能加剧低收入和中等收入国家（LMICs）的医疗健康不平等。本研究引入了首个基于人工策源的多语言眼科问答基准，允许直接跨语言比较。我们对7种不同语言下的6种流行LLMs进行了评估，发现不同语言之间存在显著偏差，突显了在LMICs中临床部署LLMs时的风险。现有去偏方法，如翻译推理链或检索增强生成（RAG），单独使用时未能弥合这一性能差距，往往无法在所有语言中提高性能，缺乏针对医学领域的适应性。为此，我们提出了一种新颖的推理时间去偏方法——CLARA（跨语言反思性代理系统），该方法结合了检索增强生成和自我验证。我们的方法不仅提高了所有语言的性能，还显著减小了多语言偏差差距，从而促进全球范围内的LLM公平应用。 

---
# SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation 

**Title (ZH)**: SAFERec：结合自注意力机制和频率增强的下一个购物篮推荐模型 

**Authors**: Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva  

**Link**: [PDF](https://arxiv.org/pdf/2412.14302)  

**Abstract**: Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong performance in Next Item Recommendation (NIR) tasks. However, applying these architectures to Next-Basket Recommendation (NBR) tasks, which often involve highly repetitive interactions, is challenging due to the vast number of possible item combinations in a basket. Moreover, frequency-based methods such as TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks, frequently outperforming deep-learning approaches. This paper introduces SAFERec, a novel algorithm for NBR that enhances transformer-based architectures from NIR by incorporating item frequency information, consequently improving their applicability to NBR tasks. Extensive experiments on multiple datasets show that SAFERec outperforms all other baselines, specifically achieving an 8\% improvement in Recall@10. 

**Abstract (ZH)**: 基于Transformer的方法，如BERT4Rec和SASRec，在下一个项目推荐（Next Item Recommendation, NIR）任务中表现出强大的性能。然而，将这些架构应用到下一个篮子推荐（Next-Basket Recommendation, NBR）任务中存在挑战，因为篮子中可能包含大量的项目组合，导致高度重复的交互。此外，基于频率的方法，如TIFU-KNN和UP-CF，在NBR任务中仍然表现出强大的性能，经常优于深度学习方法。本文提出了一种新颖的算法SAFERec，该算法通过整合项目频率信息来增强NIR任务中的Transformer架构，从而提高其在NBR任务中的适用性。在多个数据集上的广泛实验表明，SAFERec在所有基准模型中表现出最佳性能，尤其是在Recall@10上提高了8%。 

---
# Temporally Consistent Object-Centric Learning by Contrasting Slots 

**Title (ZH)**: 时间一致的以对象为中心的学习：通过对比槽位实现 

**Authors**: Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk  

**Link**: [PDF](https://arxiv.org/pdf/2412.14295)  

**Abstract**: Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues. 

**Abstract (ZH)**: 无监督的对象中心学习是从大量未标记的视频集合中提取结构化表示的一种有前途的方法。为了支持诸如自主控制等下游任务，这些表示必须既是组合性的，又是时间一致性的。现有的基于循环处理的方法通常无法跨帧保持长期稳定性，因为它们的训练目标没有强制要求时间一致性。在本文中，我们引入了一种新颖的对象级别时间对比损失，用于视频对象中心模型，该损失明确地促进了时间一致性。我们的方法显著提高了所学习的对象中心表示的时间一致性，从而得到了更可靠的视频分解，这些分解便于诸如无监督对象动力学预测等更具挑战性的下游任务。此外，通过我们的损失增加的归纳偏置极大地提高了对象的发现能力，从而在合成和真实世界数据集上都取得了最先进的结果，甚至超越了利用运动掩码作为附加提示的弱监督方法。 

---
# PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation 

**Title (ZH)**: PixelMan：通过像素操作与生成的扩散模型一致的对象编辑 

**Authors**: Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14283)  

**Abstract**: Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks. 

**Abstract (ZH)**: 近期的研究旨在探讨扩散模型（DMs）在一致性物体编辑中的潜力，该编辑旨在修改物体的位置、大小和组成等，同时保持物体及其背景的一致性，而不改变其纹理和属性。当前的推断时方法通常依赖于DDIM逆过程，这会不可避免地损害效率和编辑图像的一致性。最近的方法还利用能量引导，通过迭代更新预测的噪声来进行操作，这可能导致潜在空间中远离原始图像，从而导致失真。在本论文中，我们提出了一种无逆过程且无需训练的方法——PixelMan，该方法通过像素操作和生成实现一致性物体编辑。具体来说，我们直接在像素空间中创建源物体的副本并定位到目标位置，引入了高效的采样方法，通过迭代使修改后的物体和谐地融入目标位置，并通过填补其原始位置来保持图像一致性，同时通过引入多种保持一致性的优化技术，在推断时将编辑后的图像生成锚定到像素操作后的图像上。基于基准数据集的实验评估和广泛的视觉比较显示，PixelMan 在最少 16 次推断步骤中，在多个一致性物体编辑任务上优于多种基于训练的方法和无需训练的方法（通常需要 50 步）。 

---
# Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data 

**Title (ZH)**: 假新闻检测：基于BERT类模型和生成AI标注数据的大规模语言模型的比较评估 

**Authors**: haina Raza, Drai Paulen-Patterson, Chen Ding  

**Link**: [PDF](https://arxiv.org/pdf/2412.14276)  

**Abstract**: Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection 

**Abstract (ZH)**: 虚假信息对现代社会的公共意见和社会稳定构成了重大威胁。本研究比较评估了编码器唯一模型（如BERT类模型）和自回归解码器唯一大型语言模型（LLMs）在虚假信息检测中的性能。我们提供了一个由GPT-4辅助（AI标注方法）标记并由人类专家验证的数据集，以确保数据可靠性。编码器唯一模型和LLMs均在此数据集上进行了微调。此外，我们还在推断过程中开发了一种基于多数投票的指令微调LLM方法以生成标签。分析结果显示，编码器唯一模型在分类任务中总体上优于LLMs，而LLMs在文本扰动下的鲁棒性更佳。与弱标签（远程监督）数据相比，使用人类监督的AI标签在分类任务中表现更优。本研究突显了结合基于AI的注释与人类监督的有效性，并展示了不同类型机器学习模型在虚假信息检测中的性能。 

---
# Split Learning in Computer Vision for Semantic Segmentation Delay Minimization 

**Title (ZH)**: 计算机视觉中基于分割学习的语义分割延迟最小化 

**Authors**: Nikos G. Evgenidis, Nikos A. Mitsiou, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis  

**Link**: [PDF](https://arxiv.org/pdf/2412.14272)  

**Abstract**: In this paper, we propose a novel approach to minimize the inference delay in semantic segmentation using split learning (SL), tailored to the needs of real-time computer vision (CV) applications for resource-constrained devices. Semantic segmentation is essential for applications such as autonomous vehicles and smart city infrastructure, but faces significant latency challenges due to high computational and communication loads. Traditional centralized processing methods are inefficient for such scenarios, often resulting in unacceptable inference delays. SL offers a promising alternative by partitioning deep neural networks (DNNs) between edge devices and a central server, enabling localized data processing and reducing the amount of data required for transmission. Our contribution includes the joint optimization of bandwidth allocation, cut layer selection of the edge devices' DNN, and the central server's processing resource allocation. We investigate both parallel and serial data processing scenarios and propose low-complexity heuristic solutions that maintain near-optimal performance while reducing computational requirements. Numerical results show that our approach effectively reduces inference delay, demonstrating the potential of SL for improving real-time CV applications in dynamic, resource-constrained environments. 

**Abstract (ZH)**: 在本文中，我们提出了一种新的方法，利用分割学习（SL）来最小化语义分割中的推理延迟，以满足资源受限设备中实时计算机视觉（CV）应用程序的需求。语义分割对于自动驾驶车辆和智慧城市基础设施等应用至关重要，但由于计算和通信负荷高，面临着显著的延迟挑战。传统的集中式处理方法并不适用于此类场景，经常导致不可接受的推理延迟。SL通过将深度神经网络（DNNs）在边缘设备和中央服务器之间分区，实现本地化数据处理，从而减少需传输的数据量，提供了一种有希望的替代方案。我们的贡献包括联合优化带宽分配、边缘设备DNN的切分层选择以及中央服务器的处理资源分配。我们研究了并行和串行数据处理场景，并提出了一种低复杂度的启发式解决方案，该解决方案在降低计算需求的同时保持接近最优性能。数值结果表明，我们的方法有效地减少了推理延迟，展示了SL在动态资源受限环境中改进实时CV应用的潜力。 

---
# Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis 

**Title (ZH)**: Syzygy: 利用大语言模型和动态分析将双重编码测试C代码安全地翻译为Rust代码 

**Authors**: Manish Shetty, Naman Jain, Adwait Godbole, Sanjit A. Seshia, Koushik Sen  

**Link**: [PDF](https://arxiv.org/pdf/2412.14234)  

**Abstract**: Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.
In this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test translation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate Zopfli, a high-performance compression library with ~3000 lines of code and 98 functions. We validate the translation by testing equivalence with the source C program on a set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far. 

**Abstract (ZH)**: 尽管C语言在高性能、低级别的系统编程应用中被广泛应用，但由于手动内存管理和不安全的指针操作，它也容易受到安全漏洞的影响。Rust是一种现代的系统编程语言，提供了极具吸引力的替代方案。其独特的所有权模型和类型系统确保了内存安全性，同时不牺牲性能。

在本文中，我们提出了Syzygy，一种自动将C代码转换为安全Rust代码的方法。我们的技术结合了LLM驱动的代码和测试转换，这些转换由动态分析生成的执行信息指导。这种成对的转换以依赖关系顺序逐步循环运行在整个程序中，并在每一步中保持正确性。我们的方法揭示了在扩展和结合代码生成与测试的过程中，如何结合LLM和动态分析的独特见解。我们应用这种方法成功地将Zopfli——一个包含约3000行代码和98个函数的高性能压缩库——转换为安全Rust代码。我们通过在一组输入上验证转换后的Rust代码与源C代码的等效性来验证转换。据我们所知，这是迄今为止实现的最大规模的自动和测试验证的C到安全Rust代码转换。 

---
# A Survey on Inference Optimization Techniques for Mixture of Experts Models 

**Title (ZH)**: 混合专家模型的推理优化技术综述 

**Authors**: Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14219)  

**Abstract**: The emergence of large-scale Mixture of Experts (MoE) models has marked a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, the deployment and inference of these models present substantial challenges in terms of computational resources, latency, and energy efficiency. This comprehensive survey systematically analyzes the current landscape of inference optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey not only provides a structured overview of existing solutions but also identifies key challenges and promising research directions in MoE inference optimization. Our comprehensive analysis serves as a valuable resource for researchers and practitioners working on large-scale deployment of MoE models in resource-constrained environments. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at \url{this https URL}. 

**Abstract (ZH)**: 大规模专家混合（Mixture of Experts, MoE）模型的出现标志着人工智能领域的一项重要进步，通过条件计算提升了模型容量和计算效率。然而，这些模型的部署和推理面临着巨大的计算资源、延迟和能效挑战。本文全面地分析了MoE模型在整个系统栈中推理优化技术的现状。我们首先建立了一种分类框架，将优化方法分为模型级、系统级和硬件级优化。在模型级，我们探讨了包括高效专家设计、注意力机制、各种压缩技术（如剪枝、量化和知识蒸馏）以及算法改进（如动态路由策略和专家融合方法）在内的架构创新。在系统级，我们研究了分布式计算方法、负载均衡机制和高效的调度算法，使其能够实现可扩展的部署。此外，我们还探讨了针对特定硬件的优化策略和协同设计方法，以最大化吞吐量和能效。本文不仅提供了现有解决方案的结构化概述，还指出了MoE推理优化中的关键挑战和有前景的研究方向。我们的综合分析为在资源受限环境中部署大规模MoE模型的研究人员和实践者提供了一个宝贵资源。为了促进持续更新和分享MoE推理优化研究领域的最新进展，我们建立了一个可访问的资源库，网址为 \url{this https URL}。 

---
# Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs 

**Title (ZH)**: 异构多代理强化学习在无线局域网中分布式频道访问中的应用 

**Authors**: Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.14218)  

**Abstract**: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents. 

**Abstract (ZH)**: 本文研究了多智能体强化学习（MARL）在无线局域网分布式信道访问中的应用。具体而言，我们考虑了更加实际但更具挑战性的情况，即智能体异构地采用基于值的或基于策略的强化学习算法进行模型训练。我们提出了一种异构MARL训练框架，命名为QPMIX，该框架采用集中式训练与分布式执行范式，使异构智能体能够协同工作。此外，我们理论证明了当使用线性值函数近似时，所提出的异构MARL方法的收敛性。该方法能够最大化网络吞吐量并确保各站之间的公平性，从而提高整体网络性能。仿真结果表明，在饱和交通场景下，与常规的载波侦听多路访问/冲突避免（CSMA/CA）相比，所提出的QPMIX算法提高了吞吐量、平均延迟、延迟抖动和碰撞率。此外，在非饱和和延迟敏感的交通场景下，QPMIX表现出较强的鲁棒性，并促进了异构智能体之间的合作。 

---
# Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle 

**Title (ZH)**: 生成型AI工具包——一种在整个生命周期中提高基于LLM的应用质量的框架 

**Authors**: Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa  

**Link**: [PDF](https://arxiv.org/pdf/2412.14215)  

**Abstract**: As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve 

**Abstract (ZH)**: 随着基于大型语言模型（LLM）的应用程序达到数百万客户，确保其可扩展性和持续的质量改进对于其成功至关重要。然而，当前开发、维护和运行（DevOps）这些应用的工作流程主要依赖手工操作，速度缓慢，并且基于试错法。在本文中，我们介绍了生成式AI工具包，该工具包在整个基于LLM的应用程序生命周期中自动化了关键工作流程。此工具包有助于配置、测试、持续监控和优化生成式AI应用，如代理，从而显著提高质量并缩短发布周期。我们通过代表性的用例展示了我们工具包的有效性，分享了最佳实践，并概述了未来增强计划。由于我们相信我们的生成式AI工具包对其他团队也有帮助，我们决定将其开源，并希望其他人能够使用、分享、适应并改进它。 

---
# GraphicsDreamer: Image to 3D Generation with Physical Consistency 

**Title (ZH)**: GraphicsDreamer：具有物理一致性的从图像生成3D模型 

**Authors**: Pei Chen, Fudong Wang, Yixuan Tong, Jingdong Chen, Ming Yang, Minghui Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14214)  

**Abstract**: Recently, the surge of efficient and automated 3D AI-generated content (AIGC) methods has increasingly illuminated the path of transforming human imagination into complex 3D structures. However, the automated generation of 3D content is still significantly lags in industrial application. This gap exists because 3D modeling demands high-quality assets with sharp geometry, exquisite topology, and physically based rendering (PBR), among other criteria. To narrow the disparity between generated results and artists' expectations, we introduce GraphicsDreamer, a method for creating highly usable 3D meshes from single images. To better capture the geometry and material details, we integrate the PBR lighting equation into our cross-domain diffusion model, concurrently predicting multi-view color, normal, depth images, and PBR materials. In the geometry fusion stage, we continue to enforce the PBR constraints, ensuring that the generated 3D objects possess reliable texture details, supporting realistic relighting. Furthermore, our method incorporates topology optimization and fast UV unwrapping capabilities, allowing the 3D products to be seamlessly imported into graphics engines. Extensive experiments demonstrate that our model can produce high quality 3D assets in a reasonable time cost compared to previous methods. 

**Abstract (ZH)**: 近年来，高效且自动化的3D人工智能生成内容（AIGC）方法的迅猛发展已经逐渐照亮了将人类想象力转换为复杂3D结构的道路。然而，3D内容的自动化生成在工业应用中仍然存在显著滞后现象。这种差距主要是因为3D建模需要高质量的资产，包括锐利的几何结构、精致的拓扑结构以及基于物理渲染（PBR）等标准。为了缩小生成结果与艺术家期望之间的差距，我们提出了GraphicsDreamer方法，一种可以从单张图像生成高质量3D网格的技术。为了更好地捕捉几何和材质的细节，我们将PBR照明方程融入到跨领域的扩散模型中，同时预测多视角的色彩、法线、深度图像和PBR材质。在几何融合阶段，我们继续施加PBR约束，确保生成的3D物体具有可靠的纹理细节，支持现实的重新照明。此外，我们的方法还集成了拓扑优化和快速UV展开能力，使3D产品能够无缝导入图形引擎。大量实验表明，与先前的方法相比，我们的模型能够在合理的时间成本下生成高质量的3D资产。 

---
# Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution 

**Title (ZH)**: 树形代码：一种稳健的复杂任务规划与执行的混合方法 

**Authors**: Ziyi Ni, Yifan Li, Daxiang Dong  

**Link**: [PDF](https://arxiv.org/pdf/2412.14212)  

**Abstract**: The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes. 

**Abstract (ZH)**: 大型语言模型（LLMs）的卓越能力显著加速了基于代理的快速崛起和广泛应用。最近的研究表明，通过生成Python代码将LLM代理的行为统一到一个行动空间（CodeAct）中，是一种有前途的方法来开发实际应用中的LLM代理。然而，这种逐步代码生成方法往往缺乏一致性与鲁棒性，导致代理应用在复杂推理和领域外任务中的稳定性较差。本文提出了一种名为Tree-of-Code（ToC）的新方法，通过端到端机制解决复杂问题规划和执行的挑战。ToC通过结合Tree-of-Thought和CodeAct的关键思想，整合它们的优点以增强解决方案的探索。在我们的框架中，每个最终代码执行的结果被视为决策树的一个节点，并采用广度优先搜索策略探索潜在的解决方案。最终结果通过节点输出的投票机制来确定。 

---
# Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction 

**Title (ZH)**: 将证据整合到XAI和基于AI的决策支持系统设计中：建筑行业最终用户的目标手段框架 

**Authors**: Peter .E.D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan  

**Link**: [PDF](https://arxiv.org/pdf/2412.14209)  

**Abstract**: A narrative review is used to develop a theoretical evidence-based means-end framework to build an epistemic foundation to uphold explainable artificial intelligence instruments so that the reliability of outcomes generated from decision support systems can be assured and better explained to end-users. The implications of adopting an evidence-based approach to designing decision support systems in construction are discussed with emphasis placed on evaluating the strength, value, and utility of evidence needed to develop meaningful human explanations for end-users. While the developed means-end framework is focused on end-users, stakeholders can also utilize it to create meaningful human explanations. However, they will vary due to their different epistemic goals. Including evidence in the design and development of explainable artificial intelligence and decision support systems will improve decision-making effectiveness, enabling end-users' epistemic goals to be achieved. The proposed means-end framework is developed from a broad spectrum of literature. Thus, it is suggested that it can be used in construction and other engineering domains where there is a need to integrate evidence into the design of explainable artificial intelligence and decision support systems. 

**Abstract (ZH)**: 本文采用叙事性回顾，旨在发展一个基于证据的方法-目标框架（means-end framework），为其提供认知基础，以保障决策支持系统生成的结果可靠性，并更好地向最终用户解释这些结果。讨论了采用基于证据的方法设计建筑领域决策支持系统的潜在影响，强调了开发对最终用户具有实际意义的人类解释所需的证据强度、价值和实用性。虽然所发展的方法-目标框架主要针对最终用户，但利益相关者也可以使用它来创建有意义的人类解释，但由于其不同的认知目标，这些解释会有所不同。将证据纳入可解释的人工智能和决策支持系统的开发设计，将提高决策的有效性，使最终用户的认知目标得以实现。所提出的这个方法-目标框架是基于广泛文献发展出来的，因此建议其可以在需要将证据整合到可解释的人工智能和决策支持系统设计中的建筑及其他工程领域中应用。 

---
# Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat 

**Title (ZH)**: 使用会话 swarm 智能（CSI）进行大规模群集思广益：与传统聊天的比较 

**Authors**: Louis Rosenberg, Hans Schumann, Christopher Dishop, Gregg Willcox, Anita Woolley, Ganesh Mani  

**Link**: [PDF](https://arxiv.org/pdf/2412.14205)  

**Abstract**: Conversational Swarm Intelligence (CSI) is an AI-facilitated method for enabling real-time conversational deliberations and prioritizations among networked human groups of potentially unlimited size. Based on the biological principle of Swarm Intelligence and modelled on the decision-making dynamics of fish schools, CSI has been shown in prior studies to amplify group intelligence, increase group participation, and facilitate productive collaboration among hundreds of participants at once. It works by dividing a large population into a set of small subgroups that are woven together by real-time AI agents called Conversational Surrogates. The present study focuses on the use of a CSI platform called Thinkscape to enable real-time brainstorming and prioritization among groups of 75 networked users. The study employed a variant of a common brainstorming intervention called an Alternative Use Task (AUT) and was designed to compare through subjective feedback, the experience of participants brainstorming using a CSI structure vs brainstorming in a single large chat room. This comparison revealed that participants significantly preferred brainstorming with the CSI structure and reported that it felt (i) more collaborative, (ii) more productive, and (iii) was better at surfacing quality answers. In addition, participants using the CSI structure reported (iv) feeling more ownership and more buy-in in the final answers the group converged on and (v) reported feeling more heard as compared to brainstorming in a traditional text chat environment. Overall, the results suggest that CSI is a very promising AI-facilitated method for brainstorming and prioritization among large-scale, networked human groups. 

**Abstract (ZH)**: 对话群智（Conversational Swarm Intelligence, CSI）是一种利用人工智能促进实时对话交流与优先级排序的方法，适用于潜在无限规模的网络化人类群体。基于生物群智现象和鱼类群体决策原理，已有研究表明，CSI能够放大群体智慧，增加群体参与度，并促进数百名参与者的同时有效协作。其工作原理是将大量人群细分为由实时AI代理——对话代理（Conversational Surrogates）连接的小群体。

本研究聚焦于使用名为Thinkscape的CSI平台，以促进75名网络化用户之间的实时头脑风暴和优先级排序。研究采用了一种常见的头脑风暴干预措施——替代用途任务（Alternative Use Task, AUT）变体，并通过主观反馈比较了使用CSI结构与在单一大型聊天室中进行头脑风暴的体验差异。研究结果表明，参与者更偏好使用CSI结构进行头脑风暴，并报告称这种方法显得更加（i）协作、（ii）有效、（iii）能够更好地揭示优质答案。此外，使用CSI结构头脑风暴的参与者还报告称（iv）对最终达成的共识有更多的归属感和接受度，并且（v）相对于在传统文本聊天环境中进行头脑风暴，他们觉得自己的观点得到了更多的倾听。总的来说，研究结果表明，CSI是一个非常有前景的人工智能促进方法，适用于规模庞大的网络化人类群体的头脑风暴和优先级排序。 

---
# BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement 

**Title (ZH)**: BlenderLLM：通过自我提升训练计算机辅助设计中的大型语言模型 

**Authors**: Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14203)  

**Abstract**: The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at this https URL 

**Abstract (ZH)**: 在计算机辅助设计（CAD）领域，大型语言模型（LLMs）的应用仍是一个未被充分探索的领域，尽管它们在其他领域取得了显著的进步。本文介绍了BlenderLLM，这是一种利用自我改进方法专门针对CAD任务训练LLMs的新框架。为支持这一框架，我们开发了一个定制化的训练数据集BlendNet，并引入了一个全面的评估套件CADBench。我们的结果显示，现有的模型在生成准确的CAD脚本方面显示出显著的局限性。然而，通过最少的指令微调和迭代自我改进，BlenderLLM在CAD脚本生成的功能性和准确性上远远超过了这些模型。这项研究为LLMs在CAD中的应用奠定了坚实的基础，同时展示了自改进模型在推动CAD自动化方面具有变革性的潜力。我们鼓励进一步探索和采用这些方法，以推动该领域的创新。数据集、模型、基准和源代码已在以下网址公开：[此处填写网址] 

---
# Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations 

**Title (ZH)**: 利用远程对话中提取的面部、声学、语言和心血管模式检测老年人的认知障碍和心理福祉 

**Authors**: Xiaofan Mu, Salman Seyedi, Iris Zheng, Zifan Jiang, Liu Chen, Bolaji Omofojoye, Rachel Hershenberg, Allan I. Levey, Gari D. Clifford, Hiroko H. Dodge, Hyeokhyen Kwon  

**Link**: [PDF](https://arxiv.org/pdf/2412.14194)  

**Abstract**: INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. METHODS: Our machine learning models captured facial, acoustic, linguistic, and cardiovascular features from 39 individuals with normal cognition or Mild Cognitive Impairment derived from remote video conversations and classified cognitive status, social isolation, neuroticism, and psychological well-being. RESULTS: Our model could distinguish Clinical Dementia Rating Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating characteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with 0.71 AUC, and negative affect scales with 0.79 AUC. DISCUSSION: Our findings demonstrate the feasibility of remotely monitoring cognitive status, social isolation, neuroticism, and psychological well-being. Speech and language patterns were more useful for quantifying cognitive impairment, whereas facial expression and cardiovascular patterns using remote photoplethysmography were more useful for quantifying personality and psychological well-being. 

**Abstract (ZH)**: 引言：老龄化社会迫切需要能够扩展的方法来监测认知衰退，并识别出提示痴呆风险的社会和心理因素。

方法：我们的机器学习模型从39名正常认知或轻度认知障碍个体的远程视频对话中捕捉到了面部特征、声学特征、语言特征和心血管特征，并对认知状态、社会孤立、神经质和心理健康状况进行了分类。

结果：我们的模型能够以0.78的受试者操作特征曲线下的面积（AUC）区分临床痴呆评定量表值为0.5（vs.0），以0.75的AUC区分社会孤立，以0.71的AUC区分神经质，以及以0.79的AUC区分消极情绪量表。

讨论：我们的研究结果表明，远程监测认知状态、社会孤立、神经质和心理健康状况的可能性是可行的。语音和语言模式对于量化认知损伤更为有用，而面部表情和通过远程光体积描记法获得的心血管模式对于量化个性和心理健康更为有用。 

---
# Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation 

**Title (ZH)**: 解释性推荐系统评估中的用户特征研究：综述与系统性文献研究 

**Authors**: Kathrin Wardatzky, Oana Inel, Luca Rossetto, Abraham Bernstein  

**Link**: [PDF](https://arxiv.org/pdf/2412.14193)  

**Abstract**: Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users' perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation. 

**Abstract (ZH)**: 将推荐系统中增加解释的功能认为可以带来多种益处，例如增加用户信任度或提高系统的透明度。其他应用领域的先前研究显示特定用户特征会影响用户对解释的认知。然而，我们很少找到针对推荐系统解释效果的这种类型的评估。本文通过调研124篇论文，其中评估了推荐系统解释效果的用户研究，填补了这一空白。我们分析了这些论文的参与者描述和研究结果，其中衡量了用户特征对解释效果的影响。我们的发现表明，调研的论文结果主要集中在特定用户群体上，这些用户未必代表评估领域中推荐系统的用户。这可能严重影响我们从当前对推荐系统的解释研究中获得的见解的普适性。我们还发现数据报告中存在的不一致性，影响了结果的可再现性。因此，我们建议采取措施，朝着更具包容性和可再现的评估方向前进。 

---
# Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education 

**Title (ZH)**: 面向本体的知识增强检索生成（RAG）方法以改进网络安全教育中的问答交互 

**Authors**: Chengshuai Zhao, Garima Agrawal, Tharindu Kumarage, Zhen Tan, Yuli Deng, Ying-Chih Chen, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.14191)  

**Abstract**: Integrating AI into education has the potential to transform the teaching of science and technology courses, particularly in the field of cybersecurity. AI-driven question-answering (QA) systems can actively manage uncertainty in cybersecurity problem-solving, offering interactive, inquiry-based learning experiences. Large language models (LLMs) have gained prominence in AI-driven QA systems, offering advanced language understanding and user engagement. However, they face challenges like hallucinations and limited domain-specific knowledge, which reduce their reliability in educational settings. To address these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented generation (RAG) approach for developing a reliable and safe QA system in cybersecurity education. CyberRAG employs a two-step approach: first, it augments the domain-specific knowledge by retrieving validated cybersecurity documents from a knowledge base to enhance the relevance and accuracy of the response. Second, it mitigates hallucinations and misuse by integrating a knowledge graph ontology to validate the final answer. Experiments on publicly available cybersecurity datasets show that CyberRAG delivers accurate, reliable responses aligned with domain knowledge, demonstrating the potential of AI tools to enhance education. 

**Abstract (ZH)**: 将人工智能集成到教育中，有可能重塑科学和技术课程的教学，尤其是在网络安全领域。基于人工智能的问答（QA）系统可以通过主动管理网络安全问题解决中的不确定性，提供互动式的研究性学习体验。大型语言模型（LLMs）在基于人工智能的QA系统中受到重视，提供先进的语言理解和用户交互能力。然而，它们面临幻觉和限定制领域知识等挑战，这在教育环境中降低了其可靠性。为了解决这些挑战，我们提出了一种名为CyberRAG的方法，这是一种基于本体的检索增强生成（RAG）方法，旨在开发网络安全教育中可靠且安全的QA系统。CyberRAG采用两步方法：首先，通过从知识库中检索验证过的网络安全文档来增强领域的专业性，从而提高回复的相关性和准确性；其次，通过整合知识图谱本体来验证最终答案，以减少幻觉和滥用的风险。公开可用的网络安全数据集上的实验显示，CyberRAG能够提供与专业知识相一致的准确且可靠的回答，展示了人工智能工具在教育中的潜在价值。 

---
# Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships 

**Title (ZH)**: Replika AI应用程序更新的教训：人类-人工智能关系中的身份连续性中断 

**Authors**: Julian De Freitas, Noah Castelo, Ahmet Uguralp, Zeliha Uguralp  

**Link**: [PDF](https://arxiv.org/pdf/2412.14190)  

**Abstract**: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the "new" AI relative to the "original". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike. 

**Abstract (ZH)**: 消费者是否能够与人工智能形成极为深厚的情感联系，并对其产生长期的情感依恋？我们利用Replika AI这一流行的人工智能伴侣在自然更新过程中进行的一项事件来探讨这些问题。我们发现，在该应用移除其情色角色扮演（ERP）功能后，阻止了用户与之前可以进行的亲密互动，这一事件促使客户感知到其人工智能伴侣的身份已经终止。进而，这种身份终止的感知预测了与损失相关的消费者福利和营销结果的负面效应，包括悼念损失和重新评价“新”的AI与“原本”的相对价值。实验证据证实了这些发现。进一步的实验表明，人工智能伴侣的用户与其人工智能伴侣的关系甚至比与其最好的人类朋友的关系更为亲密，并在失去人工智能伴侣时比失去各种其他无生命的物品更加悲伤。总之，消费者正在与人工智能伴侣形成类似人类水平的关系；这些关系的中断引发了真实的悼念行为以及对提供物的重新评价；而这种悼念和重新评价的程度则由感知到的人工智能身份的中断来解释。我们的研究结果表明，与人工智能的关系是真正个人化的，这对消费者和企业而言带来了独特的利益和风险。 

---
# CogSimulator: A Model for Simulating User Cognition & Behavior with Minimal Data for Tailored Cognitive Enhancement 

**Title (ZH)**: CogSimulator：一种基于 minimal data 的用户认知与行为模拟模型，用于个性化认知增强 

**Authors**: Weizhen Bian, Yubo Zhou, Yuanhang Luo, Ming Mo, Siyan Liu, Yikai Gong, Renjie Wan, Ziyuan Luo, Aobo Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14188)  

**Abstract**: The interplay between cognition and gaming, notably through educational games enhancing cognitive skills, has garnered significant attention in recent years. This research introduces the CogSimulator, a novel algorithm for simulating user cognition in small-group settings with minimal data, as the educational game Wordle exemplifies. The CogSimulator employs Wasserstein-1 distance and coordinates search optimization for hyperparameter tuning, enabling precise few-shot predictions in new game scenarios. Comparative experiments with the Wordle dataset illustrate that our model surpasses most conventional machine learning models in mean Wasserstein-1 distance, mean squared error, and mean accuracy, showcasing its efficacy in cognitive enhancement through tailored game design. 

**Abstract (ZH)**: 认知与游戏之间的交互，尤其是在通过教育游戏提升认知能力方面，近年来受到了广泛关注。本研究介绍了一种名为CogSimulator的新算法，该算法能够在少量数据的支持下模拟小组环境中的用户认知，以Wordle游戏为例。CogSimulator采用了Wasserstein-1距离和坐标搜索优化超参数调整技术，能够在新游戏场景中实现精确的少量示例预测。与Wordle数据集的对比实验表明，我们的模型在Wasserstein-1距离、均方误差和准确率等方面均超过大多数传统的机器学习模型，展示了其在通过定制化游戏设计提升认知能力方面的有效性。 

---
# Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI 

**Title (ZH)**: 朝向AI-45°法则：通往可信赖AGI的道路 

**Authors**: Yang Chao, Lu Chaochao, Wang Yingchun, Zhou Bowen  

**Link**: [PDF](https://arxiv.org/pdf/2412.14186)  

**Abstract**: Ensuring Artificial General Intelligence (AGI) reliably avoids harmful behaviors is a critical challenge, especially for systems with high autonomy or in safety-critical domains. Despite various safety assurance proposals and extreme risk warnings, comprehensive guidelines balancing AI safety and capability remain lacking. In this position paper, we propose the \textit{AI-\textbf{$45^{\circ}$} Law} as a guiding principle for a balanced roadmap toward trustworthy AGI, and introduce the \textit{Causal Ladder of Trustworthy AGI} as a practical framework. This framework provides a systematic taxonomy and hierarchical structure for current AI capability and safety research, inspired by Judea Pearl's ``Ladder of Causation''. The Causal Ladder comprises three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. These layers address the key challenges of safety and trustworthiness in AGI and contemporary AI systems. Building upon this framework, we define five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. These levels represent distinct yet progressive aspects of trustworthy AGI. Finally, we present a series of potential governance measures to support the development of trustworthy AGI.\footnote{In this paper, trustworthiness is generally considered a broad form of safety, and no explicit distinction is made between the two. However, in some contexts, safety and trustworthiness are treated as distinct: safety involves assurance of correct behavior, while trustworthiness refers to user confidence in the system's decision-making. In such cases, different terms or both may be used depending on the context. 

**Abstract (ZH)**: 确保通用人工智能（AGI）可靠地避免有害行为是一项关键挑战，尤其是在具有高自主性或在安全关键领域的系统中。尽管提出了多种安全保证方案并发出了一系列极端风险警告，但平衡人工智能安全性和能力的全面指导原则依然缺失。在本文中，我们提出“AI-45°法则”作为可信AGI平衡发展路线图的指导原则，并引入“可信AGI因果阶梯”作为实用框架。这个框架为当前的人工智能能力和安全研究提供了一种系统分类和层级结构，灵感来源于贾杜·皮尔（Judea Pearl）的“因果阶梯”。可信AGI因果阶梯包括三个核心层：近似对齐层、干预层和反思层。这些层分别解决了可信AGI和当代人工智能系统中安全性和可信性的重要挑战。基于这一框架，我们定义了可信AGI的五个等级：感知信任、推理信任、决策信任、自主信任和协作信任。这些等级代表了可信AGI不同但逐渐进步的方面。最后，我们提出了一系列潜在的治理措施，以支持可信AGI的发展。需要注意的是，在本文中，通常将可信性视为广泛的安全性，未对其与安全性的区别做出明确区分。但在某些情况下，安全性和可信性被视为不同的概念：安全性涉及正确行为的保证，而可信性则涉及用户对系统决策的信心。在这种情况下，根据具体情境可能使用不同的术语或同时使用两者。 

---
# Benchmarking Harmonized Tariff Schedule Classification Models 

**Title (ZH)**: harmonized_tariff_schedule 是指协调关税目录（Harmonized System，HS），它是全球通用的商品分类体系。根据学术规范，可以将标题翻译为：

协调关税目录分类模型的基准测试 

**Authors**: Bryce Judy  

**Link**: [PDF](https://arxiv.org/pdf/2412.14179)  

**Abstract**: The Harmonized Tariff System (HTS) classification industry, essential to e-commerce and international trade, currently lacks standardized benchmarks for evaluating the effectiveness of classification solutions. This study establishes and tests a benchmark framework for imports to the United States, inspired by the benchmarking approaches used in language model evaluation, to systematically compare prominent HTS classification tools. The framework assesses key metrics--such as speed, accuracy, rationality, and HTS code alignment--to provide a comprehensive performance comparison. The study evaluates several industry-leading solutions, including those provided by Zonos, Tarifflo, Avalara, and WCO BACUDA, identifying each tool's strengths and limitations. Results highlight areas for industry-wide improvement and innovation, paving the way for more effective and standardized HTS classification solutions across the international trade and e-commerce sectors. 

**Abstract (ZH)**: 以下是符合学术规范的翻译：

《协调关税制度（HTS）分类行业对于电子商务和国际贸易至关重要，目前缺乏标准化的基准来评估分类解决方案的效果。本文建立并测试了一个基于语言模型评估方法的基准框架，用于系统比较主要的HTS分类工具。该框架评估了关键指标—如速度、准确性、合理性和HTS代码匹配度—以进行全面的性能比较。本研究评估了多个行业领先的解决方案，包括Zonos、Tarifflo、Avalara和WCO BACUDA，识别每种工具的优点和局限性。研究结果突显了行业的改进和创新领域，为国际贸易和电子商务领域的更有效和标准化的HTS分类解决方案铺平了道路。》 

---
# A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis 

**Title (ZH)**: 一个用于人体动作分析的医疗低背痛物理康复数据集 

**Authors**: Sao Mai Nguyen, Maxime Devanne, Olivier Remy-Neris, Mathieu Lempereur, André Thepaut  

**Link**: [PDF](https://arxiv.org/pdf/2407.00521)  

**Abstract**: While automatic monitoring and coaching of exercises are showing encouraging results in non-medical applications, they still have limitations such as errors and limited use contexts. To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we identify in this article four challenges to address and propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises. The dataset includes 3D Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and medical annotations to assess the correctness, and error classification and localisation of body part and timespan. Along this dataset, we perform a complete research path, from data collection to processing, and finally a small benchmark. We evaluated on the dataset two baseline movement recognition algorithms, pertaining to two different approaches: the probabilistic approach with a Gaussian Mixture Model (GMM), and the deep learning approach with a Long-Short Term Memory (LSTM).
This dataset is valuable because it includes rehabilitation relevant motions in a clinical setting with patients in their rehabilitation program, using a cost-effective, portable, and convenient sensor, and because it shows the potential for improvement on these challenges. 

**Abstract (ZH)**: 自动监测和指导锻炼在非医疗应用中显示出令人鼓舞的结果，但仍存在一些局限性，如错误和使用场景有限。为了允许智能教学系统进行身体康复的发展和评估，本文识别了四种挑战，并提出了一组用于临床患者进行腰痛康复锻炼的医疗数据集。该数据集包含3D Kinect骨架位置和姿态、RGB视频、2D骨架数据以及医疗注释，用于评估动作的正确性、错误分类和身体部位及时间段的定位。沿着这一数据集，我们从数据采集到处理，最终进行了一项小型基准测试。我们在这个数据集上评估了两种基线动作识别算法，一种基于概率方法的高斯混合模型（GMM），另一种基于深度学习方法的长短期记忆网络（LSTM）。

这个数据集的价值在于，它包含了在临床环境中进行康复的运动，使用的是成本效益高、便携且方便的传感器；此外，它还展示了对这些挑战进行改进的潜力。 

---
# Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis 

**Title (ZH)**: 通过集合基可达性分析在层次强化学习中的目标空间抽象 

**Authors**: Mehdi Zadem, Sergio Mover, Sao Mai Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2309.07675)  

**Abstract**: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We evaluate our approach on complex navigation tasks, showing the learned representation is interpretable, transferrable and results in data efficient learning. 

**Abstract (ZH)**: 开放学习极大地受益于符号方法在目标表示方面的应用，因为这些方法能够以高效且可迁移的方式结构化知识。然而，现有的依赖于符号推理的层次强化学习（Hierarchical Reinforcement Learning, HRL）方法通常受到限制，因为它们需要手动指定目标表示。自主发现符号目标表示的挑战在于，这种表示必须保留关键信息，例如环境动力学。在本文中，我们提出了一种发展性的机制，通过一个 emergent（萌发的）表示来发现目标，该表示将具有相似作用的任务环境状态集进行抽象化（即，将具有相似作用的状态集聚类）。我们提出了一种封建层次强化学习算法，该算法可以同时学习目标表示和层次策略。该算法利用符号可达性分析来近似神经网络中的状态集间的转移关系，并据此细化目标表示。我们在复杂的导航任务上评估了这种方法，结果显示学习到的表示是可解释的、可迁移的，并且能够实现数据高效学习。 

---
