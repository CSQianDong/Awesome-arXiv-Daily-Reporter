{'arxiv_id': 'arXiv:2412.15204', 'title': 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks', 'authors': 'Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li', 'link': 'https://arxiv.org/abs/2412.15204', 'abstract': 'This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at this https URL.', 'abstract_zh': '本文介绍了一种名为LongBench v2的新基准，该基准旨在评估大规模语言模型（LLM）处理长上下文问题的能力，这些问题是需要跨越多个真实世界任务进行深刻理解和推理的问题。LongBench v2包含503个具有挑战性的多项选择题，上下文范围从8000词到200万词，涵盖了六个主要任务类别：单文档问答、多文档问答、长上下文学习、长对话历史理解、代码仓库理解和长结构化数据理解。为了确保广度和实用性，我们从近100名背景多样的受过高等教育的个人中收集数据。我们采用了自动化和手动审查的双重过程来维护高质量和难度，结果表明，在15分钟的时间限制下，人工专家只能达到53.7%的准确率。评估结果显示，最佳模型直接回答问题的准确率为50.1%。相比之下，包括更长推理的o1-preview模型的准确率为57.7%，超过了人工基线4个百分点。这些结果突显了增强推理能力和扩大推理时计算量在LongBench v2的长上下文挑战中的重要性。该项目可在此链接中访问：[项目链接]。', 'title_zh': '长场景v2：朝着对真实长语境多任务有更深的理解和推理发展'}
{'arxiv_id': 'arXiv:2412.15194', 'title': 'MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark', 'authors': 'Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei', 'link': 'https://arxiv.org/abs/2412.15194', 'abstract': "Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at this https URL and the dataset refers to this https URL.", 'abstract_zh': '以下是改进后的、符合学术规范的中文翻译：\n\n像大规模多任务语言理解（MMLU）这样的多项选择题（MCQ）数据集被广泛用于评估大型语言模型（LLMs）的常识、理解和问题解决能力。然而，这些基准的开源特性以及LLMs广泛的数据来源不可避免地导致了基准数据集的污染，从而影响了评估结果的可靠性。为了解决这一问题，我们提出了一种无污染且更具挑战性的MCQ基准数据集——MMLU-CF。该基准通过规避无意的数据泄漏和恶意的数据泄漏，重新评估LLMs对世界知识的理解。为了规避无意的数据泄漏，我们从更广泛的领域中获取数据，并设计了三条去污染规则。为了防止恶意数据泄漏，我们将基准分为具有相似难度和学科分布的验证集和测试集。测试集保持非开源，以确保结果的可靠性，验证集则公开发布以促进透明度并便于独立验证。我们的评估结果表明，强大的GPT-4o在测试集上的5-shot得分为73.4%，0-shot得分为71.9%，这表明了我们方法在建立更为严格和无污染评估标准方面的有效性。GitHub仓库地址为[这个链接](https://github.com/username/project)，数据集可从[这个链接](https://dataset-link)访问。\n\n在文中，请将“这个链接”替换为实际的GitHub和数据集链接。', 'title_zh': 'MMLU-CF：一项无污染的多任务语言理解基准测试'}
{'arxiv_id': 'arXiv:2412.15189', 'title': 'Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings', 'authors': 'Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini', 'link': 'https://arxiv.org/abs/2412.15189', 'abstract': 'Natural Language Processing and Generation systems have recently shown the potential to complement and streamline the costly and time-consuming job of professional fact-checkers. In this work, we lift several constraints of current state-of-the-art pipelines for automated fact-checking based on the Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under more realistic scenarios, RAG-based methods for the generation of verdicts - i.e., short texts discussing the veracity of a claim - evaluating them on stylistically complex claims and heterogeneous, yet reliable, knowledge bases. Our findings show a complex landscape, where, for example, LLM-based retrievers outperform other retrieval techniques, though they still struggle with heterogeneous knowledge bases; larger models excel in verdict faithfulness, while smaller models provide better context adherence, with human evaluations favouring zero-shot and one-shot approaches for informativeness, and fine-tuned models for emotional alignment.', 'abstract_zh': '自然语言处理和生成系统近年来展示了补充和简化专业事实核查人员昂贵且耗时工作的潜力。在本文中，我们根据检索增强生成（RAG）范式，松解了当前自动事实核查管道中的若干限制。我们的目标是在更加现实的场景中对标，对基于RAG的方法进行验证，评估它们在生成裁决（即，讨论声明真实性的简短文本）时的表现，特别是在复杂风格的声明和异构但可靠的知识库上的表现。我们的研究发现了一个复杂的情况，例如，基于大规模语言模型的检索器在性能上优于其他检索技术，尽管它们在处理异构知识库方面仍存在问题；较大的模型在裁决的真实度方面表现优异，而较小的模型在上下文一致性方面表现更好，人类评价更倾向于零样本和一样本方法的信息性，而微调模型则更符合情感一致性。', 'title_zh': '面对事实！在实际场景中评估基于RAG的事实核查流水线'}
{'arxiv_id': 'arXiv:2412.15188', 'title': 'LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation', 'authors': 'Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu', 'link': 'https://arxiv.org/abs/2412.15188', 'abstract': "We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.", 'abstract_zh': '我们提出了一种名为LlamaFusion的框架，该框架能够赋予仅含文本训练的大型语言模型（LLM）跨模态生成能力，使它们能够理解和生成任意序列的文本和图像。LlamaFusion利用Llama-3现有的权重进行自回归文本处理，同时引入额外的并行变换器模块来处理带有扩散的图像。在训练过程中，来自每种模态的数据会被定向到其专门的模块：模态特异的前馈层、键-值投影和标准化层分别独立处理每种模态，而共享的自注意力层则允许文本和图像特征之间的交互。通过冻结文本特异的模块并仅训练图像特异的模块，LlamaFusion保留了文本仅LMM的语言能力，同时发展出强大的视觉理解与生成能力。与从头开始预训练跨模态生成模型的方法相比，我们的实验表明，LlamaFusion仅使用50%的FLOPs就能将图像理解能力提升20%，图像生成能力提升3.6%，同时保留Llama-3的语言能力。此外，我们还展示了该框架能够适应现有的具有跨模态生成能力的视觉-语言模型。总体而言，该框架不仅利用了仅含文本LMM的现有计算投资，还能够促进语言和视觉能力的并行开发，为高效的跨模态模型开发指明了令人鼓舞的方向。', 'title_zh': 'LlamaFusion：适应预训练语言模型的多模态生成'}
{'arxiv_id': 'arXiv:2412.15151', 'title': 'Language Models as Continuous Self-Evolving Data Engineers', 'authors': 'Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang', 'link': 'https://arxiv.org/abs/2412.15151', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.', 'abstract_zh': '大语言模型（LLMs）在各种任务上展示了令人瞩目的能力，但其进一步演变受限于高质量训练数据的缺乏。此外，传统的训练方法过于依赖专家标注的数据，这限制了LLMs的性能上限。为解决这一问题，我们提出了一种新的范式，使LLMs能够通过自主生成、清理、审核和注释数据（包括偏好信息）来训练自己，这一范式被称为LANCE。我们的方法展示了LLMs可以作为持续自我进化的数据工程师，显著减少了后续训练数据构造过程的时间和成本。通过在不同版本的Qwen2上进行迭代微调，我们验证了LANCE在各种任务中的有效性，表明它可以持续提高模型性能并保持高质量数据生成。在八个基准维度上，LANCE分别提高了Qwen2-7B和Qwen2-7B-Instruct的平均分数3.36和2.70。这种自主数据构建的训练范式不仅减少了对人类专家或外部模型的依赖，还确保数据与人类价值观和偏好相一致，为开发可以超越人类能力的未来超级智能系统铺平了道路。', 'title_zh': '语言模型作为连续自演化的数据工程师'}
{'arxiv_id': 'arXiv:2412.15127', 'title': 'Adaptive Pruning for Large Language Models with Structural Importance Awareness', 'authors': 'Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han', 'link': 'https://arxiv.org/abs/2412.15127', 'abstract': 'The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios.', 'abstract_zh': '近年来，大规模语言模型（LLMs）在语言理解和生成能力方面取得了显著进步。然而，由于LLMs对计算和存储资源的需求较高，将其部署在资源受限的边缘设备上仍然面临挑战。为解决这一问题，我们提出了一种新的LLM模型剪枝方法，称为结构感知自适应剪枝（SAAP），该方法能够大幅减少计算和内存成本，同时保持模型性能。我们首先定义了一个自适应重要性融合度量，以考虑所有耦合结构的同方差不确定性来评估它们的重要性。然后，我们根据所有模块的重要性排名来确定应剪枝的具体层以满足特定的性能要求。此外，我们开发了一种新的分组微调策略以提高LLMs的推理效率。最后，我们在两个常见任务，即零样本分类和文本生成上，对多个LLM进行了拟合算法的评估。实验结果表明，我们的SAAP方法优于几种最先进的基线方法，在LLaMA-7B、Vicuna-7B和LLaMA-13B上的准确性分别提升了2.17%、2.37%和2.39%。此外，SAAP方法还将标记生成速度提高了5%，展示了其在资源受限场景下的实用优势。', 'title_zh': '具有结构重要性意识的大型语言模型自适应剪枝'}
{'arxiv_id': 'arXiv:2412.15118', 'title': 'Outcome-Refining Process Supervision for Code Generation', 'authors': 'Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang', 'link': 'https://arxiv.org/abs/2412.15118', 'abstract': 'Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: this https URL', 'abstract_zh': '大规模语言模型在代码生成方面表现出非凡的能力，但在处理需要深层算法推理的复杂编程任务时常常遇到困难。通过学习奖励模型进行过程监督虽然显示出一定的前景，但需要昂贵的训练数据，并且在评价上不够可靠。我们提出了一种新的范式——结果细化的过程监督，将其视作需要监督的过程本身。我们的框架利用具体的执行信号来指导推理步骤的监督，同时使用树结构探索来同时维护多条解决方案轨迹。实验表明，我们的方法使更小的模型在竞争编程任务中实现了高准确性和性能指标，而不需训练性能奖励模型 (PRM)，就创造了更可靠的验证。我们的方法在5个模型和3个数据集上取得了显著改进：平均正确性提高了26.9%，效率提高了42.2%。结果表明，为解决复杂编程任务提供结构化的推理空间和具体的验证信号至关重要。我们已将所有代码和数据开源：this https URL', 'title_zh': '代码生成中的成果细化过程监督'}
{'arxiv_id': 'arXiv:2412.15115', 'title': 'Qwen2.5 Technical Report', 'authors': 'Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu', 'link': 'https://arxiv.org/abs/2412.15115', 'abstract': 'In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.', 'abstract_zh': '在本报告中，我们介绍了Qwen2.5，这是一个全面的大语言模型（LLM）系列，旨在满足多样化的应用需求。与之前的版本相比，Qwen2.5在预训练和后训练阶段都得到了显著的改进。在预训练方面，我们将高质量的预训练数据集从之前的7万亿个令牌扩展到18万亿个令牌。这为常识、专家知识和推理能力提供了坚实的基础。在后训练方面，我们实施了复杂的监督微调，涉及超过100万个样本，并采用多阶段强化学习技术。后训练技术提高了人工偏好，特别是在长文本生成、结构数据分析和指令执行方面表现尤为突出。为了更好地应对多种多样的应用场景，我们提供了不同规模的Qwen2.5 LLM系列模型。开源版本包括基础模型和指令调优模型，并提供了量化版本。此外，对于托管解决方案，我们的私有模型目前包括两种专家混合模型（MoE）变体：Qwen2.5-Turbo和Qwen2.5-Plus，均可从阿里云模型仓库获取。Qwen2.5在广泛的语言理解、推理、数学、编程、人工偏好对齐等基准测试中表现出顶级性能。具体来说，开源重量级旗舰版Qwen2.5-72B-Instruct在多项基准测试中表现优异，超过了多个开源和私有模型，并且其性能与约为其五倍大的最先进的开源重量级模型Llama-3-405B-Instruct相当。Qwen2.5-Turbo和Qwen2.5-Plus在成本效益方面表现出色，分别在与GPT-4o-mini和GPT-4o的竞争中表现出色。此外，Qwen2.5模型作为基础，对于训练专门模型，例如Qwen2.5-Math、Qwen2.5-Coder、QwQ和多模态模型起到了关键作用。', 'title_zh': 'Qwen2.5 技术报告'}
{'arxiv_id': 'arXiv:2412.15101', 'title': 'Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability', 'authors': 'Xiangsen Chen, Xuming Hu, Nan Tang', 'link': 'https://arxiv.org/abs/2412.15101', 'abstract': 'Retrieve-augmented generation (RAG) frameworks have emerged as a promising solution to multi-hop question answering(QA) tasks since it enables large language models (LLMs) to incorporate external knowledge and mitigate their inherent knowledge deficiencies. Despite this progress, existing RAG frameworks, which usually follows the retrieve-then-read paradigm, often struggle with multi-hop QA with temporal information since it has difficulty retrieving and synthesizing accurate time-related information. To address the challenge, this paper proposes a novel framework called review-then-refine, which aims to enhance LLM performance in multi-hop QA scenarios with temporal information. Our approach begins with a review phase, where decomposed sub-queries are dynamically rewritten with temporal information, allowing for subsequent adaptive retrieval and reasoning process. In addition, we implement adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing the potential for hallucinations. In the subsequent refine phase, the LLM synthesizes the retrieved information from each sub-query along with its internal knowledge to formulate a coherent answer. Extensive experimental results across multiple datasets demonstrate the effectiveness of our proposed framework, highlighting its potential to significantly improve multi-hop QA capabilities in LLMs.', 'abstract_zh': '检索增强生成（RAG）框架自出现以来，已成为多跳问答（QA）任务的有前途的解决方案，因为这种框架能够使大型语言模型（LLMs）结合外部知识并缓解其固有的知识不足。尽管取得了这一进展，现有的RAG框架通常遵循“先检索后阅读”的范式，在处理包含时间信息的多跳问答时往往面临挑战，因为它们在提取和综合准确的时间相关信息方面存在困难。为应对这一挑战，本文提出了一种名为“先审查后完善”的新型框架，旨在增强LLMs在包含时间信息的多跳问答场景中的性能。我们的方法首先进入一个审查阶段，在这个阶段中，分解后的子查询动态地添加了时间信息，从而为后续的自适应检索和推理过程提供支持。此外，我们还实现了一种自适应检索机制，以减少不必要的检索次数，从而降低出现幻觉的风险。在后续的完善阶段，LLMs结合从每个子查询检索到的信息及其内部知识，形成一个连贯的答案。在多个数据集上进行的广泛实验结果表明，我们提出的框架的有效性，突显了其在显著提升LLMs的多跳问答能力方面的潜力。', 'title_zh': '基于动态框架的具有时间适应性的多跳问答复习与完善方法'}
{'arxiv_id': 'arXiv:2412.15084', 'title': 'AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling', 'authors': 'Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping', 'link': 'https://arxiv.org/abs/2412.15084', 'abstract': 'In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: this https URL', 'abstract_zh': '在这篇论文中，我们介绍了AceMath，这是一个卓越于解决复杂数学问题的前沿数学模型套件，同时也配备了高效的奖励模型，能够评估生成的解决方案并可靠地识别正确的答案。为了开发指令调整的数学模型，我们提出了一种监督微调（SFT）过程，该过程首先在一般领域中实现竞争性的性能，随后通过使用精心策划的提示和合成生成的响应对数学领域进行有针对性的微调。最终生成的模型AceMath-72B-Instruct在性能上显著优于Qwen2.5-Math-72B-Instruct、GPT-4o和Claude-3.5 Sonnet。为了开发专门的数学奖励模型，我们首先构建了AceMath-RewardBench，这是一个全面且稳健的基准测试，用于评估数学奖励模型在各种问题和难度级别上的性能。之后，我们提出了一种系统的方法来构建我们的数学奖励模型。最终生成的模型AceMath-72B-RM在性能上持续优于现有的领先奖励模型。此外，当我们结合使用AceMath-72B-Instruct与AceMath-72B-RM时，在数学推理基准测试中的平均rm@8得分达到最高。我们将在以下链接发布模型权重、训练数据和评估基准：[this https URL]', 'title_zh': 'AceMath：通过后训练和奖励建模推进前沿数学推理'}
{'arxiv_id': 'arXiv:2412.15060', 'title': 'ConfliBERT: A Language Model for Political Conflict', 'authors': 'Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan', 'link': 'https://arxiv.org/abs/2412.15060', 'abstract': "Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD).", 'abstract_zh': '冲突研究学者利用基于规则的方法从新闻报道和文本中提取有关政治暴力的信息。最近的自然语言处理发展超越了僵化的基于规则的方法。我们回顾了我们最近开发的ConfliBERT语言模型（Hu et al. 2022），用于处理与政治和暴力相关的文本。该模型可以用于从关于政治冲突的文本中提取行为者和行为分类。经过微调后，结果显示ConfliBERT在准确率、精确率和召回率方面优于其他大型语言模型（LLM），如Google的Gemma 2（9B）、Meta的Llama 3.1（7B）和Alibaba的Qwen 2.5（14B）在其相关领域内。此外，它比这些更通用的大型语言模型快几百倍。这些结果通过使用BBC、re3d和全球恐怖主义数据库（GTD）中的文本进行说明。', 'title_zh': 'ConfliBERT：一个用于政治冲突的语言模型'}
{'arxiv_id': 'arXiv:2412.15035', 'title': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps', 'authors': 'Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting', 'link': 'https://arxiv.org/abs/2412.15035', 'abstract': 'Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.', 'abstract_zh': '跨多种语言构建安全的大语言模型（LLMs）对于确保安全访问和语言多样性至关重要。为此，我们介绍了M-ALERT，这是一种多语言基准，评估了五种语言（英语、法语、德语、意大利语和西班牙语）中LLMs的安全性。M-ALERT 包含每种语言15000个高质量的提示，总共75000个，遵循详细的ALERT分类。我们在10个最先进的LLMs上进行的广泛实验突显了语言特定安全性分析的重要性，揭示了模型在不同语言和类别中的显著不一致性。例如，Llama3.2在意大利语的犯罪税务类别中显示出较高的不安全性，但在其他语言中却保持安全。类似的不同也在所有模型中观察到。相反，某些类别，如毒品大麻和犯罪宣传，一贯会引发模型和语言中的危险响应。这些发现强调了在LLMs中实施稳健的多语言安全性做法以确保广泛用户社区的安全和负责任使用的必要性。', 'title_zh': 'LLMs在跨语言翻译中迷失：M-ALERT揭示了跨语言安全漏洞'}
{'arxiv_id': 'arXiv:2412.14986', 'title': 'Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small Language Models Write Young Students Texts', 'authors': 'Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu', 'link': 'https://arxiv.org/abs/2412.14986', 'abstract': "Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Models' (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference.", 'abstract_zh': '大型语言模型（LLMs）被用于针对不同的写作任务生成文本，如报告、论文和讲故事。然而，这些语言模型并未拥有关于文本写作过程的元表示，也没有与年幼的人类学徒相似的内在沟通学习需求。本文通过对多语言小型语言模型（SLMs）写作的细粒度语言和文本分析，介绍了SLMs模仿人类写作过程某些步骤的能力，如规划和评估。我们主要关注了为小学生设计的短篇故事写作任务和为本科生设计的论文写作任务。结果显示，SLMs在协助年幼儿童处理涉及校园暴力等敏感主题时遇到了困难，有时还会使用目标受众难以理解的复杂词汇。特别是在时间连接词、主题进展和指代方面，SLMs的输出与人类生成的文本在文本连贯性和一致性方面存在较大差异。', 'title_zh': '链式元写作：小型语言模型如何为年轻学生撰写文本的语言与文本分析'}
{'arxiv_id': 'arXiv:2412.14964', 'title': 'Knowledge Injection via Prompt Distillation', 'authors': 'Kalle Kujanpää, Harri Valpola, Alexander Ilin', 'link': 'https://arxiv.org/abs/2412.14964', 'abstract': "In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. In this paper, we propose a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which we call prompt distillation. First, we generate question-answer pairs about the new knowledge. Then, we fine-tune a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights.", 'abstract_zh': '在许多实际应用中，大语言模型（LLMs）需要整合其预训练数据中不存在的新知识。目前，主要的方法是微调（fine-tuning）和检索增强生成（retrieval-augmented generation，RAG）。尽管RAG已成为知识注入的行业标准，但微调尚未取得与其相当的成功。在本文中，我们提出了一种新的微调技术，用于学习新知识，并证明该技术可以达到RAG的性能。我们提出的方法基于自蒸馏的方法，我们称之为提示蒸馏（prompt distillation）。首先，我们生成关于新知识的问题-答案对。然后，我们使用问题-答案对对一个学生模型进行微调，使其模仿一个额外接收新知识提示的教师模型的输出分布。学生模型与教师模型相同，只是配备了LoRA适配器。这种训练程序有助于将教师模型中的新知识提示蒸馏到学生模型的权重中。', 'title_zh': '知识注入 via 提示提炼'}
{'arxiv_id': 'arXiv:2412.14959', 'title': "Understanding the Dark Side of LLMs' Intrinsic Self-Correction", 'authors': 'Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang', 'link': 'https://arxiv.org/abs/2412.14959', 'abstract': "Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases. By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at this https URL.", 'abstract_zh': '本文提出了通过反馈提示实现固有自我修正，以提高大型语言模型（LLM）的响应能力。然而，近期的研究表明，无需 oracle 标签作为反馈提示时，LLM 的固有自我修正会失效。本文旨在解释 LLM 的固有自我修正在不同任务中的表现，特别是针对其失败案例。通过使用包括一个简单任务和三个复杂任务，以及像 ChatGPT 家族（o1、4o、3.5-turbo）和 Llama 家族（2-7B、3-8B 和 3.1-8B）等先进水平的 LLM，我们设计了三种解释方法以揭示 LLM 固有自我修正的暗面。我们发现固有自我修正可以（1）导致 LLM 在中间答案和最终答案上犹豫不决，并在简单事实性问题上产生提示偏向；（2）在复杂任务中引入类似人类的认知偏差。基于我们的发现，我们还提供了两种简单而有效的缓解策略：重复提问和基于少量样本的监督微调。我们已将我们的工作开源，访问地址为 <https://github.com/your-repository>。', 'title_zh': '理解大型语言模型内在自我修正的暗面'}
{'arxiv_id': 'arXiv:2412.14922', 'title': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response', 'authors': 'Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang', 'link': 'https://arxiv.org/abs/2412.14922', 'abstract': "Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.", 'abstract_zh': '监督微调（SFT）在使大规模语言模型（LLMs）适应特定领域或任务方面发挥着至关重要的作用。然而，正如实证实验所表明的，收集的数据不可避免地含有噪声，这对模型在下游任务中的性能构成了重大挑战。因此，迫切需要一个稳健的SFT框架以增强模型在下游任务中的能力。为应对这一挑战，我们提出了一种稳健的SFT框架（RobustFT），该框架在下游任务数据上进行噪声检测和重新标注。在噪声识别方面，我们的方法采用了一个多专家协作系统，并结合了推理增强模型，以实现卓越的噪声检测性能。在去除噪声的阶段，我们采用了一种上下文增强策略，该策略整合了最相关且最具信心的知识，并经过仔细评估生成可靠的注释。此外，我们还引入了一种基于响应熵的有效数据选择机制，以确保仅保留高质量的样本用于微调。在多个LLM在五个多数据集上的广泛实验表明，RobustFT 在噪声场景中表现出色。', 'title_zh': 'RobustFT：在嘈杂响应情况下大型语言模型的稳健监督微调'}
{'arxiv_id': 'arXiv:2412.14905', 'title': 'Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation', 'authors': 'Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie', 'link': 'https://arxiv.org/abs/2412.14905', 'abstract': 'Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks.', 'abstract_zh': '大规模语言模型（LLMs）在整合检索增强生成（RAG）之后，仍然容易生成幻觉信息。平行上下文扩展（PCE）是一条研究路线，旨在有效整合无序的平行上下文，但在适应RAG场景时仍然会遭受幻觉问题。本文中，我们提出了一种名为DePaC（Dehallucinating Parallel Context Extension）的方案，通过上下文感知的负样本训练和信息校准聚合来缓解幻觉问题。DePaC 设计用于减轻两种类型的上下文幻觉：事实虚构（即LLMs提出缺乏上下文支持的断言）和事实遗漏（即LLMs未能提供能够由上下文支持的断言）。具体而言，（1）对于事实虚构，我们应用上下文感知的负样本训练，通过微调LLMs进行负监督，从而明确引导LLMs在上下文与问题无关时拒绝作答；（2）对于事实遗漏，我们提出信息校准聚合，优先考虑具有更高信息增量的上下文窗口。在九个RAG任务上的实验结果表明，DePaC 显著缓解了这两种类型的幻觉，并且在这些任务中始终实现了更好的表现。', 'title_zh': '去 hallucination 的并行上下文扩展以增强检索生成'}
{'arxiv_id': 'arXiv:2412.14872', 'title': 'Why language models collapse when trained on recursively generated text', 'authors': 'Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei', 'link': 'https://arxiv.org/abs/2412.14872', 'abstract': 'Language models (LMs) have been widely used to generate text on the Internet. The generated text is often collected into the training corpus of the next generations of LMs. Previous work has experimentally found that LMs collapse when trained on recursively generated text. This paper contributes to existing knowledge from two aspects. We present a theoretical proof of LM collapse. Our proof reveals the cause of LM collapse and proves that all auto-regressive LMs will definitely collapse. We present a new finding: the performance of LMs gradually declines when trained on recursively generated text until they perform no better than a randomly initialized LM. The trained LMs produce large amounts of repetitive text and perform poorly across a wide range of natural language tasks. The above proof and new findings deepen our understanding of LM collapse and offer valuable insights that may inspire new training techniques to mitigate this threat.', 'abstract_zh': '语言模型（LMs）广泛应用于网络文本生成。生成的文本通常会被收集到下一代LMs的训练语料库中。此前的工作已经通过实验发现，当LMs在递归生成的文本上进行训练时会发生崩溃现象。本文从两个方面增进了现有知识。我们提供了一个关于LM崩溃现象的理论证明。我们的证明揭示了LM崩溃的原因，并证明所有自回归LMs都将不可避免地发生崩溃。我们还发现了一个新的发现：当LMs在递归生成的文本上进行训练时，其性能会逐渐下降，直到它们的表现不如随机初始化的LMs。经过训练的LMs会产生大量重复的文本，并在各种自然语言任务中表现不佳。上述证明和新发现加深了我们对LM崩溃现象的理解，并提供了宝贵的见解，可能激发新的训练技术来缓解这一威胁。', 'title_zh': '当使用递归生成的文本进行训练时，语言模型为何会出现崩溃现象'}
{'arxiv_id': 'arXiv:2412.14867', 'title': 'Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering', 'authors': 'Imed Keraghel, Mohamed Nadif', 'link': 'https://arxiv.org/abs/2412.14867', 'abstract': 'Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities.', 'abstract_zh': '近年来，机器学习领域的进展，尤其是大型语言模型（LLMs）如BERT和GPT，提供了丰富的情境嵌入，从而提高了文本表示的效果。然而，当前的文档聚类方法往往忽视了命名实体（NEs）之间的更深层次关系以及LLM嵌入的潜力。本文提出了一种新颖的方法，该方法将命名实体识别（NER）和LLM嵌入嵌入到基于图的框架中，用于文档聚类。该方法构建了一个图，图中的节点表示文档，边的权重基于命名实体相似性，并通过图卷积网络（GCN）进行优化。这确保了更有效的基于语义的相关文档分组。实验结果表明，与基于共现的常规方法相比，我们的方法在富含有名实体的文档聚类方面表现更优。', 'title_zh': '图卷积网络：文档聚类中的命名实体识别与大型语言模型嵌入'}
{'arxiv_id': 'arXiv:2412.14860', 'title': 'Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling', 'authors': 'Junyi Li, Hwee Tou Ng', 'link': 'https://arxiv.org/abs/2412.14860', 'abstract': 'Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reflect on the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Models to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.', 'abstract_zh': '尽管大型语言模型（LLMs）具有出色的能力，但它们容易产生幻觉，并生成事实性错误的信息。这一挑战促使研究者们致力于有归属感的文本生成，即促使LLMs生成带有支持证据的内容。在本文中，我们提出了一种新颖的框架，称为Think&Cite，并将有归属感的文本生成问题形式化为一个结合搜索的多步推理问题。具体而言，我们提出了自我指导蒙特卡洛树搜索（SG-MCTS），其中利用LLMs的自我反思能力对MCTS的中间状态进行思考，以指导树的扩展过程。为了提供可靠且全面的反馈，我们引入了进度奖励模型，从生成进度和归属进度两个方面衡量从根节点到当前状态的搜索进度。我们在三个数据集上进行了广泛的实验，结果表明，我们的方法显著优于基线方法。', 'title_zh': '思与引证：通过自我指导树搜索和进步奖励建模提升带属性文本生成'}
{'arxiv_id': 'arXiv:2412.14849', 'title': 'DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis', 'authors': 'Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu', 'link': 'https://arxiv.org/abs/2412.14849', 'abstract': 'Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \\textit{key-point-driven} and \\textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \\textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.', 'abstract_zh': '最近开发的大语言模型（LLMs）为低资源场景下的数据稀缺问题提供了 promising 的新途径。在少样本方面的情感分析（ABSA）中，先前的努力探索了数据增强技术，通过修改现有样本来生成新的样本。然而，这些方法未能生成足够多样的数据，影响了其有效性。此外，一些研究通过使用特定指令和少量选定示例作为提示，应用上下文学习进行ABSA。尽管有前景，但LLMs往往会产生偏离任务要求的标签。为了克服这些局限，我们提出了一种针对少样本ABSA的双流数据合成框架DS$^2$-ABSA。该框架利用LLMs从两个互补的角度合成数据：关键点驱动和实例驱动，从而在低资源设置中有效生成多样且高质量的ABSA样本。此外，还集成了一个标签精细化模块以提高合成标签的质量。广泛实验证明，DS$^2$-ABSA显著优于之前的少样本ABSA解决方案和其他面向LLM的数据生成方法。', 'title_zh': 'DS$^2$-ABSA：基于标注精炼的双流数据合成 Few-Shot 面向方面情感分析'}
{'arxiv_id': 'arXiv:2412.14847', 'title': 'A Survey of RWKV', 'authors': 'Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu', 'link': 'https://arxiv.org/abs/2412.14847', 'abstract': 'The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: this https URL.', 'abstract_zh': '《接收性加权键值（RWKV）模型》为Transformer架构提供了一种新颖的替代方案，结合了循环网络和基于注意力系统的优点。与依赖于自我注意力的传统Transformer不同，RWKV 能够在低计算成本下有效地捕捉长距离依赖关系。通过利用循环框架，RWKV 解决了传统Transformer在处理长序列任务时的一些计算效率问题。RWKV 最近因其在多个领域中的稳健性能引起了广泛关注。尽管其 popularity 日益增长，但尚未有系统性的对RWKV模型的回顾。本文旨在填补这一空白，成为首篇全面回顾RWKV架构、其核心原理及其在自然语言生成、自然语言理解和计算机视觉等不同应用领域的综述。我们评估了RWKV与传统Transformer模型之间的差异，突出了其高效处理长序列以及降低计算成本的能力。此外，我们探讨了RWKV遇到的挑战，并提出了未来研究和发展的潜在方向。我们持续维护与RWKV相关的开源材料：[此处提供链接]。', 'title_zh': 'RWKV综述'}
{'arxiv_id': 'arXiv:2412.14843', 'title': 'Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas', 'authors': 'Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini', 'link': 'https://arxiv.org/abs/2412.14843', 'abstract': "The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.", 'abstract_zh': '对大型语言模型（LLMs）中的政治偏见进行分析，主要将这些系统视为具有固定观点的单一实体。虽然存在多种测量偏见的方法，但基于人设（persona）的提示对LLMs政治倾向的影响尚未被探究。本研究利用PersonaHub，一个合成人设描述集合，结合政治定向测试（PCT）来绘制基于人设提示的LLMs的政治分布图。然后，我们探索是否存在通过明确的意识形态提示将这些初始定向分布显著调整至完全对立的政治倾向：右威权主义和左自由主义的情况。实验结果揭示，合成人设主要集中在左自由主义象限，当用明确的意识形态描述词提示时，各个模型显示出不同程度的响应。尽管所有模型均表现出向右威权主义位置的重大转变，但向左自由主义位置的转变则相对有限，这表明模型对意识形态操控的不对称响应可能反映了模型训练中固有的偏见。', 'title_zh': '使用合成人物映射和影响大型语言模型的政治意识形态'}
{'arxiv_id': 'arXiv:2412.14838', 'title': 'DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs', 'authors': 'Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding', 'link': 'https://arxiv.org/abs/2412.14838', 'abstract': "Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.", 'abstract_zh': '在LLM中高效管理键值缓存对于处理长上下文任务（如RAG和总结）至关重要。现有的键值缓存压缩方法强制采用固定模式，忽视了任务特异性，并减少了关键信息的保留。然而，我们观察到在不同任务中各层之间存在不同的激活模式，这凸显出需要针对每个任务的独特需求制定适应性策略的必要性。基于这一洞察，我们提出了一种名为DynamicKV的方法，该方法通过根据不同任务的需求动态优化token的保留数量来调整每层保留的token数量。DynamicKV建立了全局和每层的键值缓存预算，并在推理过程中暂时保留当前层的最大预算，同时周期性地更新所有先前层的键值缓存大小。我们的方法在LongBench上的性能达到了完整键值缓存性能的约85%，同时仅保留了1.7%的键值缓存大小。值得注意的是，在极端压缩（90%）的情况下，DynamicKV在Needle-in-a-Haystack测试中使用Mistral-7B-Instruct-v0.2时，优于现有最佳方法（SOTA）11%。该代码将会公开发布。', 'title_zh': 'DynamicKV：面向任务的长上下文LLM自适应键值缓存压缩算法'}
{'arxiv_id': 'arXiv:2412.14835', 'title': 'Progressive Multimodal Reasoning via Active Retrieval', 'authors': 'Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2412.14835', 'abstract': 'Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.', 'abstract_zh': '多步多模态推理任务对多模态大型语言模型（MLLMs）提出了重大挑战，如何在这些场景中有效提高其性能依然是未解决的问题。本文提出了一种名为AR-MCTS的通用框架，旨在通过主动检索（AR）和蒙特卡洛树搜索（MCTS）逐步提高MLLMs的推理能力。我们的方法首先开发了一个统一的检索模块，该模块可以从混合模态检索库中检索解决复杂推理问题的关键支持性见解。为了解决自动化多模态推理验证的差距，我们采用结合了主动检索机制的MCTS算法，从而能够自动生成逐步注解。此策略动态检索每个推理步骤的关键见解，超越传统的束搜索采样方式，以提高推理空间的多样性和可靠性。此外，我们还引入了一种逐步对齐的过程奖励模型，以支持多模态推理任务的自动验证。通过在三个复杂多模态推理基准上的实验结果，验证了AR-MCTS框架在提高各种多模态模型性能方面的有效性。进一步的分析表明，AR-MCTS可以优化采样多样性与准确性，从而实现可靠的多模态推理。', 'title_zh': '基于主动检索的渐进多模态推理'}
{'arxiv_id': 'arXiv:2412.14829', 'title': 'Mention Attention for Pronoun Translation', 'authors': 'Gongbo Tang, Christian Hardmeier', 'link': 'https://arxiv.org/abs/2412.14829', 'abstract': 'Most pronouns are referring expressions, computers need to resolve what do the pronouns refer to, and there are divergences on pronoun usage across languages. Thus, dealing with these divergences and translating pronouns is a challenge in machine translation. Mentions are referring candidates of pronouns and have closer relations with pronouns compared to general tokens. We assume that extracting additional mention features can help pronoun translation. Therefore, we introduce an additional mention attention module in the decoder to pay extra attention to source mentions but not non-mention tokens. Our mention attention module not only extracts features from source mentions, but also considers target-side context which benefits pronoun translation. In addition, we also introduce two mention classifiers to train models to recognize mentions, whose outputs guide the mention attention. We conduct experiments on the WMT17 English-German translation task, and evaluate our models on general translation and pronoun translation, using BLEU, APT, and contrastive evaluation metrics. Our proposed model outperforms the baseline Transformer model in terms of APT and BLEU scores, this confirms our hypothesis that we can improve pronoun translation by paying additional attention to source mentions, and shows that our introduced additional modules do not have negative effect on the general translation quality.', 'abstract_zh': '大多数代词都是指代表达式，计算机需要解决代词指的是什么，而不同语言在代词使用上存在差异。因此，处理这些差异并进行代词翻译是一项挑战。提及通常是代词的候选指代项，与一般词项相比，提及与代词关系更密切。我们假设提取额外的提及特征有助于代词翻译。因此，我们引入了一个额外的提及注意力模块，在解码器中特别关注源端提及而非非提及词项。我们的提及注意力模块不仅从来源提及中提取特征，还考虑目标端上下文，这有利于代词翻译。此外，我们还引入了两个提及分类器，用于训练模型识别提及，其输出指导提及注意力。我们在WMT17英德翻译任务上进行了实验，并使用BLEU、APT和对比评价指标对我们的模型进行评估。我们的模型在APT和BLEU评分上优于基线Transformer模型，这证实了我们可以通过特别关注源端提及来提高代词翻译的假设是正确的，并表明我们引入的附加模块不会对总体翻译质量产生负面影响。', 'title_zh': '提名词注意力机制在代词翻译中的应用'}
{'arxiv_id': 'arXiv:2412.14809', 'title': 'ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis', 'authors': 'Zeao Tu, Xiangdi Meng, Yu He, Zihan Yao, Tianyu Qi, Jun Liu, Ming Li', 'link': 'https://arxiv.org/abs/2412.14809', 'abstract': 'Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.', 'abstract_zh': '大型语言模型（LLMs）在各个领域表现出显著的效果，使用GPT进行合成数据生成的数据增强方法成为主流。然而，增强数据的质量和实用性仍存在疑问，当前的方法缺乏明确的数据特性评估指标。为解决这些问题，我们提出了一种名为ResoFilter的新方法，该方法整合了模型、数据和任务，以精炼数据集。ResoFilter利用微调过程获取数据-参数特征以进行数据选择，并通过模型权重表示数据特性来提高解释性。我们的实验表明，在数学任务中，ResoFilter仅使用一半的数据即可获得与全量微调相当的结果，并且在不同模型和领域中表现出强大的泛化能力。该方法为构建合成数据集和评估高质量数据提供了宝贵的见解，是提高数据增强技术和LLMs训练数据质量的有前景的解决方案。为了可重复性，在接受后我们将发布我们的代码和数据。', 'title_zh': 'ResoFilter：通过数据-参数共振分析的大规模语言模型细粒度合成数据筛选方法'}
{'arxiv_id': 'arXiv:2412.14780', 'title': 'Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning', 'authors': 'Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng', 'link': 'https://arxiv.org/abs/2412.14780', 'abstract': 'When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles - specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format) - differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).', 'abstract_zh': '在使用代理任务数据集来增强大型语言模型（LLMs）的能力时，当前的方法通常将每个样本中的所有词元等同对待。然而，我们指出，承担不同角色的词元——具体来说，是推理词元与模板词元（例如，控制输出格式的词元）——在重要性及学习复杂度上存在显著差异，因此需要将它们解耦并区别对待。为解决这一问题，我们提出了一种新的Shuffle-Aware Discriminator（SHAD）以实现自适应词元区分。SHAD通过利用输入-输出组合在样本之间打乱后的可预测性差异进行分类：模板词元由于其在样本中的重复性，其可预测性保持不变，而推理词元则不然。借助SHAD，我们提出了推理强调微调（RFT）方法，在微调过程中自适应地强调推理词元，从而在常见的监督微调（SFT）方法之上取得了显著的性能提升。', 'title_zh': '拆分推理标记和模板标记以进行语言模型微调'}
{'arxiv_id': 'arXiv:2412.14771', 'title': 'ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine', 'authors': 'Rabee Qasem, Mohannad Hendi, Banan Tantour', 'link': 'https://arxiv.org/abs/2412.14771', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments.', 'abstract_zh': '大规模语言模型（LLMs）在多个领域展示了显著的潜力，但在法律领域的应用，尤其是在资源稀缺的环境中，仍然受到限制。本研究旨在克服将LLMs适应于巴勒斯坦法律领域的挑战，在这种背景下，政治不稳定、法律框架碎片化以及有限的人工智能资源阻碍了有效的机器学习应用。我们基于量化的Llama-3.2-1B-Instruct模型构建了一种精细调整的模型，并使用源自巴勒斯坦法律文本的合成数据集对其进行训练。通过使用规模较小的模型和战略性生成的问题-答案对，我们实现了成本效益高、本地可持续的解决方案，能够提供准确且与上下文相关的法律指导。实验结果表明，在不同类型的查询（从是/否问题和叙述解释到复杂的法律区分）上表现出色，同时指出了改进的领域，例如处理基于计算的查询和结构化列表格式化。本研究提供了一种途径，用于部署针对资源约束环境需求定制的人工智能驱动法律辅助工具。', 'title_zh': 'ALKAFI-LLAMA3：为巴勒斯坦精准法律理解细调的大语言模型'}
{'arxiv_id': 'arXiv:2412.14769', 'title': 'PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children', 'authors': 'Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang', 'link': 'https://arxiv.org/abs/2412.14769', 'abstract': 'Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \\method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.', 'abstract_zh': '留守儿童（Left-behind Children, LBCs）在中国人数超过6600万，由于父母外出务工，他们面临严重的心理健康挑战。早期筛查和识别处于高风险的留守儿童至关重要，但由于心理健康专业人员严重短缺，尤其是在农村地区，此项工作显得尤为困难。虽然宅-树-人（House-Tree-Person, HTP）测试显示出较高的儿童参与率，但需要专家解读的特性限制了其在资源稀缺地区的应用。为应对这一挑战，我们提出了PsyDraw，这是一种基于多模态大型语言模型的多智能体系统，旨在辅助心理健康专业人员分析HTP绘画。该系统采用专门的智能体进行特征提取和心理解读，并分为两个阶段：综合特征分析和专业报告生成。对290名小学生进行的HTP绘画评价结果显示，71.03%的分析达到了与专业评价的高水平一致性，26.21%达到中等一致性，仅有2.41%达到低一致性。系统识别出了31.03%需要专业关注的案例，显示出其作为初步筛查工具的有效性。目前已经在试点学校部署，该方法显示出在资源受限地区支持心理健康专业人员的潜力，同时在心理评估方面保持高水平的专业标准。', 'title_zh': 'PsyDraw：一种多agent多模态系统，用于留守儿童心理健康筛查'}
{'arxiv_id': 'arXiv:2412.14751', 'title': 'Query pipeline optimization for cancer patient question answering systems', 'authors': 'Maolin He, Rena Gao, Mike Conway, Brian E. Chapman', 'link': 'https://arxiv.org/abs/2412.14751', 'abstract': 'Retrieval-augmented generation (RAG) mitigates hallucination in Large Language Models (LLMs) by using query pipelines to retrieve relevant external information and grounding responses in retrieved knowledge. However, query pipeline optimization for cancer patient question-answering (CPQA) systems requires separately optimizing multiple components with domain-specific considerations. We propose a novel three-aspect optimization approach for the RAG query pipeline in CPQA systems, utilizing public biomedical databases like PubMed and PubMed Central. Our optimization includes: (1) document retrieval, utilizing a comparative analysis of NCBI resources and introducing Hybrid Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval, identifying optimal pairings of dense retrievers and rerankers; and (3) semantic representation, introducing Semantic Enhanced Overlap Segmentation (SEOS) for improved contextual understanding. On a custom-developed dataset tailored for cancer-related inquiries, our optimized RAG approach improved the answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and about 3% over a naive RAG setup. This study highlights the importance of domain-specific query optimization in realizing the full potential of RAG and provides a robust framework for building more accurate and reliable CPQA systems, advancing the development of RAG-based biomedical systems.', 'abstract_zh': '检索增强生成（RAG）通过使用查询管道检索相关外部信息并使响应基于检索到的知识，从而减轻了大型语言模型（LLMs）中的幻觉问题。然而，针对癌症患者问答（CPQA）系统的查询管道优化需要分别优化多个具有特定领域考虑的组件。我们提出了一种针对CPQA系统中RAG查询管道的新型三方面优化方法，利用如PubMed和PubMed Central等公共生物医学数据库。我们的优化包括：(1) 文档检索，通过比较NCBI资源并引入混合语义实时文档检索（HSRDR）进行比较分析；(2) 段落检索，识别密集检索器和排序器的最佳配对；以及(3) 语义表示，引入语义增强重叠分段（SEOS）以提高上下文理解能力。在为癌症相关询问定制开发的数据集上，我们优化的RAG方法在Claude-3-haiku上的答案准确性相比思维链提示提高了5.24%，相比简单的RAG设置提高了约3%。本研究强调了在实现RAG最大潜力时领域特定查询优化的重要性，并提供了一个构建更准确和可靠CPQA系统的稳健框架，促进了基于RAG的生物医学系统的发展。', 'title_zh': '癌症患者问答系统中的查询管道优化'}
{'arxiv_id': 'arXiv:2412.14737', 'title': 'On Verbalized Confidence Scores for LLMs', 'authors': 'Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada', 'link': 'https://arxiv.org/abs/2412.14737', 'abstract': "The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at this https URL .", 'abstract_zh': '大型语言模型（LLMs）的兴起及其在日常生活中紧密集成，使得关注其可信度变得至关重要。对LLMs进行不确定性量化可以增加人类对它们响应的信任度，同时使LLM智能体能够基于彼此的不确定性做出更加知情的决策。为了估计响应中的不确定性，通常使用内部令牌 logits、任务特定的代理模型或多次生成响应进行采样。本文的研究重点是让LLM本身在其输出令牌中表达其不确定性并附带信心分数，这是一种具有低开销的提示和模型通用的不确定性量化方法。我们使用广泛的基准测试，评估表达的信心分数在不同数据集、模型和提示方法下的可靠性。结果表明，这些分数的可靠性很大程度上取决于模型是如何被提问的，但通过某些提示方法，也有可能提取出校准良好的信心分数。我们认为，表达的信心分数可能会成为未来简单但有效且多用途的不确定性量化方法之一。我们的代码可在以下链接获取：this https URL 。', 'title_zh': '关于语言表达的置信分数对大语言模型的影响研究'}
{'arxiv_id': 'arXiv:2412.14689', 'title': 'How to Synthesize Text Data without Model Collapse?', 'authors': 'Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou', 'link': 'https://arxiv.org/abs/2412.14689', 'abstract': 'Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.', 'abstract_zh': '模型在合成数据上的崩溃表明，迭代训练自生成数据会导致性能逐渐下降。随着AI模型的普及，合成数据将从根本上重塑网络数据生态系统。未来的GPT-$\\{n\\}$模型不可避免地需要在合成数据和人类生成数据的混合数据上进行训练。本文主要关注两个问题：合成数据对语言模型训练的影响是什么，如何合成数据以避免模型崩溃？首先，我们在不同比例的合成数据下预训练语言模型，揭示了合成数据比例与模型性能之间的负相关关系。进一步通过对合成数据的统计分析，我们发现了分布转移现象以及n-gram特征的过度集中问题。受到上述发现的启发，我们提出对人类生成数据进行标记编辑以获得半合成数据。作为一种概念证明，我们理论上证明了标记编辑可以防止模型崩溃，因为测试错误受到了有限上界的约束。我们在从头开始预训练、持续预训练和监督微调三种场景下进行了广泛的实验。实验结果验证了我们的理论证明：标记编辑可以提升数据质量并增强模型性能。', 'title_zh': '如何合成文本数据而不导致模型崩塌？'}
{'arxiv_id': 'arXiv:2412.14686', 'title': 'Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection', 'authors': 'Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao', 'link': 'https://arxiv.org/abs/2412.14686', 'abstract': 'Social platforms, while facilitating access to information, have also become saturated with a plethora of fake news, resulting in negative consequences. Automatic multimodal fake news detection is a worthwhile pursuit. Existing multimodal fake news datasets only provide binary labels of real or fake. However, real news is alike, while each fake news is fake in its own way. These datasets fail to reflect the mixed nature of various types of multimodal fake news. To bridge the gap, we construct an attributing multi-granularity multimodal fake news detection dataset \\amg, revealing the inherent fake pattern. Furthermore, we propose a multi-granularity clue alignment model \\our to achieve multimodal fake news detection and attribution. Experimental results demonstrate that \\amg is a challenging dataset, and its attribution setting opens up new avenues for future research.', 'abstract_zh': '社交媒体平台在促进信息 accesibility方面发挥了重要作用，但也充斥着大量假新闻，导致了一系列负面影响。自动多模态假新闻检测是一个值得探索的研究方向。现有的多模态假新闻数据集仅提供真实或假的二元标签。然而，真实新闻有相似之处，而每篇假新闻都有其独特的假信息模式。这些数据集未能反映不同类型多模态假新闻的混合性质。为了解决这一问题，我们构建了一个带有属性的多粒度多模态假新闻检测数据集 \\amg，揭示了内在的假新闻模式。此外，我们提出了一种多粒度线索对齐模型 \\our，以实现多模态假新闻检测和归因。实验结果表明，\\amg 是一个具有挑战性的数据集，并且其归因设置为未来的研究开辟了新的途径。', 'title_zh': '每条假新闻都有其独特的虚假方式：一种面向多模态假新闻检测的归因多粒度基准'}
{'arxiv_id': 'arXiv:2412.14675', 'title': 'LLMs as mediators: Can they diagnose conflicts accurately?', 'authors': 'Özgecan Koçak, Phanish Puranam, Afşar Yegin', 'link': 'https://arxiv.org/abs/2412.14675', 'abstract': "Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Koçak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.", 'abstract_zh': '以往的研究表明，观察者要在两方争执中进行干预，必须能够可靠地区分他们的分歧来源，是基于对事实的不同信念（因果关系）还是基于价值观的不同（道德）。本论文旨在测试OpenAI的大规模语言模型GPT-3.5和GPT-4是否能够完成这一任务，以及哪种类型的分歧对这些语言模型来说尤为具有诊断难度。我们复制了Koçak等人（2003）的研究，使用情景设计（vignette design），并用OpenAI的GPT-3.5和GPT-4进行实验。研究发现，这两种语言模型在因果关系和道德规范之间的语义理解与人类相似，并且能够可靠地区分这两者。当要求诊断对话中的分歧来源时，与人类相比，这两种语言模型更倾向于高估因果性分歧的程度，并低估道德性分歧的程度，尤其是在涉及具体语言和具体问题时，GPT-4的表现尤为明显。GPT-3.5在使用近端或远端量表时的表现不如GPT-4或人类。本研究提供了首次测试利用语言模型诊断因果性和评价性分歧以调解冲突的可能性。', 'title_zh': 'LLM作为调解者：它们能否准确诊断冲突？'}
{'arxiv_id': 'arXiv:2412.14670', 'title': 'Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT', 'authors': 'Hassane Kissane, Achim Schilling, Patrick Krauss', 'link': 'https://arxiv.org/abs/2412.14670', 'abstract': "This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision.", 'abstract_zh': '本研究探讨了基于变换器的大型语言模型（LLMs）内部对动词-短语组合的表示，特别是 investigation 这些模型在不同神经网络层中如何捕捉词汇和句法细微差别。我们采用 BERT 架构，分析其各层对诸如“agree on”、“come back”和“give up”等不同动词-短语结构的表示有效性。研究方法包括从英国国家语料库（British National Corpus）中详细准备数据集，随后进行广泛的模型训练，并通过多维标度（MDS）和广义区分值（GDV）计算等技术进行输出分析。结果表明，BERT 的中间层最有效地捕捉句法结构，不同动词类别的表示准确性存在显著差异。这些发现挑战了对神经网络处理语言元素时的均匀性假设，表明网络架构与语言表示之间存在复杂的交互作用。本研究有助于更好地理解深度学习模型如何理解和处理语言，并提供了当前神经方法在语言分析中的潜力和限制的见解。本研究不仅促进了计算语言学领域知识的积累，还促使进一步研究以优化神经架构以提高语言精度。', 'title_zh': '大型语言模型中的语言结构分析与可视化：BERT中动词-副词构造的神经表示'}
{'arxiv_id': 'arXiv:2412.14656', 'title': 'Length Controlled Generation for Black-box LLMs', 'authors': 'Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin', 'link': 'https://arxiv.org/abs/2412.14656', 'abstract': 'Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.', 'abstract_zh': '大语言模型（LLMs）在指令跟随方面展现出了令人印象深刻的能力，但在生成文本长度控制方面仍然存在困难，这是许多现实应用中的一个基本要求。现有的长度控制方法涉及对LLMs的参数进行微调，这既不高效也不适合实际应用。在本文中，我们提出了一种新的迭代采样框架，结合了Metropolis-Hastings算法和重要性采样加速策略，用于文本长度控制。该框架有效地且可靠地调控LLMs生成长度受限的文本，而无需修改底层参数，从而保留了LLMs的原始能力。实验结果表明，我们的框架在Llama3.1上实现了几乎100%的长度控制成功率，适用于诸如长度受限的抽取式总结和长度受限的指令跟随等任务，且额外的计算开销极小。这还突显了我们的方法在更广泛的领域实现精确长度控制的巨大潜力，而不牺牲LLMs的多功能性。', 'title_zh': '黑箱大语言模型中基于长度的生成控制'}
{'arxiv_id': 'arXiv:2412.14642', 'title': 'TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation', 'authors': 'Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li', 'link': 'https://arxiv.org/abs/2412.14642', 'abstract': 'In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are available through this https URL.', 'abstract_zh': '在本文中，我们提出了基于文本的开放分子生成基准（TOMG-Bench），这是首个用于评估大规模语言模型（LLM）在开放域分子生成能力的基准。TOMG-Bench 包含一个数据集，共涵盖三大任务：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom）。每个任务还进一步细分为三个子任务，每个子任务包括5000个测试样本。鉴于开放分子生成固有的复杂性，我们还开发了一个自动评估系统，该系统有助于衡量生成分子的质量和准确性。通过对25个不同模型的全面基准测试，揭示了文本引导分子发现领域的现有局限性和改进潜力。此外，在OpenMolIns（专门用于解决TOMG-Bench提出的挑战的指令调优数据集）的帮助下，Llama3.1-8B甚至在TOMG-Bench上的表现超过了所有开源通用LLM，比GPT-3.5-turbo的性能高出46.5%。我们的代码和数据集可通过以下链接获取：[链接]。', 'title_zh': 'TOMG-Bench：评估基于文本的开放分子生成的LLM性能'}
{'arxiv_id': 'arXiv:2412.14626', 'title': 'Learning to Generate Research Idea with Dynamic Control', 'authors': 'Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du', 'link': 'https://arxiv.org/abs/2412.14626', 'abstract': 'The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.', 'abstract_zh': '大型语言模型（LLMs）的快速 advancements 已经展示了它们在加速科学发现方面的潜力，特别是在自动化研究构想过程方面。基于LLM的系统在生成假说和研究构想方面显示出潜力。然而，当前的方法主要依赖于提示驱动的预训练模型，这限制了它们对生成内容的有效优化能力。此外，它们还缺乏处理新颖性、可行性和有效性之间复杂依赖关系和内在限制的能力，这由于这些维度之间固有的权衡（如创新与可行性冲突）而变得极具挑战性。为解决这些局限性，我们首次提出对LLM进行微调，使其更好地提出构想，并引入了一种新颖的框架，该框架结合了监督微调（SFT）和可控强化学习（RL）的两阶段方法。在SFT阶段，模型从研究论文及其后续构想的配对中学习基础模式。在RL阶段，基于精细反馈的多维度奖励建模评估和优化生成的构想，以关键指标进行衡量。维度控制器允许动态调整生成，而句子级解码器确保在推理时具备上下文感知的强调。我们的框架提供了一种平衡的研究构想方法，通过动态权衡新颖性、可行性和有效性之间的权衡，实现了高质量的结果。', 'title_zh': '具有动态控制的科研理念生成学习'}
{'arxiv_id': 'arXiv:2412.14617', 'title': 'How good is GPT at writing political speeches for the White House?', 'authors': 'Jacques Savoy', 'link': 'https://arxiv.org/abs/2412.14617', 'abstract': 'Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma "we" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author\'s style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages.', 'abstract_zh': '使用大规模语言模型（LLM），计算机能够根据用户请求生成书面文本。由于这项普及性技术可应用于多种情境，本研究通过将一个名为GPT的LLM生成的演讲与近期美国总统的演讲进行比较，分析了GPT的书面风格。为此，本研究对比了里根至拜登时期的一些国情咨文（State of the Union Address），并将这些文本与GPT-3.5和GPT-4版本生成的文本进行了对比。与美国总统的演讲相比，GPT更倾向于过度使用“我们”这一词汇，且其生成的演讲平均句子更长但文本内容较短。此外，GPT采用较为乐观的语气，更频繁地使用政治术语（如，总统、国会）、象征性词汇（如，自由）和抽象词汇（如，自由）。即使要求GPT模仿某位作者的写作风格，生成的演讲仍然与目标作者的演讲风格不同。最后，两个GPT版本展现出不同的特点，但总体来看，都不大类似真实的总统演讲。', 'title_zh': 'GPT在撰写白宫政治演讲方面的表现如何？'}
{'arxiv_id': 'arXiv:2412.14613', 'title': 'HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model', 'authors': 'Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue', 'link': 'https://arxiv.org/abs/2412.14613', 'abstract': 'Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.', 'abstract_zh': '视觉语言模型（VLMs）在文本和图像理解方面展现出了显著的能力。然而，目前用于评估VLMs生成的文本的度量标准仅专注于整体质量，这导致了两个局限性：1）难以从整体评分中识别哪些方面需要改进；2）度量标准可能在预测整体评分时忽略特定的评价标准。为了解决这些局限性，我们提出了HarmonicEval，这是一种无需参考的评价度量标准，它通过自底向上的方式聚合各个标准的评分以生成整体评分。此外，我们构建了多任务多标准人类评估（MMHE）数据集，其中包括四个视觉语言任务中的18,000个专家的人类判断。我们的实验结果表明，与传统的度量标准相比，HarmonicEval与人类判断具有更高的相关性，并且能够为每个标准提供数值评分。', 'title_zh': '谐波评价：基于视觉语言模型的多模态、多任务、多准则自动评价方法'}
{'arxiv_id': 'arXiv:2412.14612', 'title': 'KARRIEREWEGE: A Large Scale Career Path Prediction Dataset', 'authors': 'Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank', 'link': 'https://arxiv.org/abs/2412.14612', 'abstract': 'Accurate career path prediction can support many stakeholders, like job seekers, recruiters, HR, and project managers. However, publicly available data and tools for career path prediction are scarce. In this work, we introduce KARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k career paths, significantly surpassing the size of previously available datasets. We link the dataset to the ESCO taxonomy to offer a valuable resource for predicting career trajectories. To tackle the problem of free-text inputs typically found in resumes, we enhance it by synthesizing job titles and descriptions resulting in KARRIEREWEGE+. This allows for accurate predictions from unstructured data, closely aligning with real-world application challenges. We benchmark existing state-of-the-art (SOTA) models on our dataset and a prior benchmark and observe improved performance and robustness, particularly for free-text use cases, due to the synthesized data.', 'abstract_zh': '准确的职业路径预测可以支持许多利益相关者，如求职者、招聘人员、人力资源部门和项目管理人员。然而，公开可用的职业路径预测数据和工具很少。在此研究中，我们介绍了KARRIEREWEGE，这是一个全面的、公开可用的数据集，包含超过50万条职业路径，显著超过之前可用数据集的规模。我们将数据集与ESCO分类法关联，提供了一个预测职业轨迹的重要资源。为了解决简历中常见的自由文本输入问题，我们通过合成职位名称和描述对数据集进行了增强，形成了KARRIEREWEGE+。这使得可以从非结构化数据中进行准确预测，并且与实际应用中的挑战密切相关。我们在我们的数据集和先前的基准数据集上对现有最先进的（SOTA）模型进行了基准测试，并观察到在自由文本应用场景中，由于合成数据的使用，性能和鲁棒性均有改善。', 'title_zh': '职业路径：一个大规模职业路径预测数据集'}
{'arxiv_id': 'arXiv:2412.14588', 'title': 'Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning', 'authors': 'Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu', 'link': 'https://arxiv.org/abs/2412.14588', 'abstract': "In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.", 'abstract_zh': '在法律实践中，法官应用刑法的三元法理论，依次评估犯罪行为的构成要件、违法性和罪过性，以确定某个人的行为是否构成犯罪。尽管当前的法律大规模语言模型（LLMs）在判决预测方面显示出潜在的准确性，但由于缺乏适当的基准数据集，它们在判断无辜结果方面缺乏三元推理能力，因此无法预测无辜判决。由此，每个输入都会自动被指控，限制了它们在法律情境中的实际应用价值。为弥补这一差距，我们引入了LJPIV，这是第一个包含无辜判决的法律判决预测基准数据集。遵循三元法理论，我们通过基于LLM的扩展和手动验证，扩展了三个广泛使用的法律数据集。我们的实验结果显示：（1）当前的法律LLMs在LJPIV上的表现仍有显著提升空间，即使是表现最佳的模型，在LJPIV上的F1分数也低于0.3；（2）我们的策略在提高领域内和跨领域判决预测准确性方面表现出明显优势，特别是对于导致无辜判决的案件。', 'title_zh': '超越罪恶感：基于三分推理的法律判决预测'}
{'arxiv_id': 'arXiv:2412.14584', 'title': 'Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues', 'authors': 'Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin', 'link': 'https://arxiv.org/abs/2412.14584', 'abstract': 'Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.', 'abstract_zh': '近年来，主动对话领域的最新进展引起了广泛关注，尤其是在更复杂的任务上（如情感支持和说服）。与传统的任务导向对话不同，主动对话需要更为高级的策略规划和灵活性，从而要求开发丰富的场景和全面的策略库。然而，现有的方法往往依赖大型语言模型（LLMs）进行用户模拟和在线学习，这导致了与现实场景不符的偏见，从而影响了系统的性能。此外，这些方法依赖于手动定义、上下文无关的粗粒度策略，这不仅增加了专家的成本，还引发了其完备性的担忧。在我们的研究中，我们强调了从原始的实时对话记录中自动发现策略的潜力。为此，我们提出了一种新的对话策略规划框架，即LDPP。该框架完全自动化了从对话记录中挖掘策略到学习策略规划的全过程。具体而言，我们使用变分自编码器的变体来发现表示为潜在向量的细粒度策略。在自动为数据打上这些潜在策略标签后，我们在潜在空间中提出了一个离线分层强化学习（RL）算法，从而开发出有效的策略规划能力。实验证明，LDPP在两个主动对话场景中优于现有方法，甚至仅使用一个拥有1.8亿参数的LLM就超过了ChatGPT。', 'title_zh': '无模拟分层隐状态策略规划以支持主动对话'}
{'arxiv_id': 'arXiv:2412.14581', 'title': 'CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation', 'authors': 'Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Graliński, Zhewei Yao, Yuxiong He', 'link': 'https://arxiv.org/abs/2412.14581', 'abstract': 'With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks.', 'abstract_zh': '随着检索增强生成（RAG）的应用，大规模语言模型（LLMs）有望将其生成基于检索到的上下文。然而，这受到了LLMs的位置偏见的阻碍，导致其不能平等地关注所有上下文。之前的研究通过合成带有扰动位置的正确片段，创建一个位置多样化的训练集，来解决这一问题。我们扩展了这一思路，提出了一种带有增强和蒸馏的一致性正则化方法。首先，我们通过为每个训练实例添加位置扰动来增强训练实例，从而鼓励一致的预测，而与顺序无关。我们还对该对进行了知识蒸馏，尽管在某些RAG场景中，检索器提供的顺序对于生成质量至关重要，强调知识蒸馏可能会适得其反。因此，我们提出了CORD（Consistency and Rank Distillation），它在保持一致性的基础上平衡排名蒸馏。CORD 从插值空间中自适应地采样噪声控制的扰动，确保一致性的同时尊重排名先验。实验结果表明，这种平衡使CORD在多种RAG基准测试中都表现出色。', 'title_zh': 'CORD: 平衡一致性和排名提炼以实现稳健的检索增强生成'}
{'arxiv_id': 'arXiv:2412.14556', 'title': 'CitaLaw: Enhancing LLM with Citations in Legal Domain', 'authors': 'Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu', 'link': 'https://arxiv.org/abs/2412.14556', 'abstract': "In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.", 'abstract_zh': '在本文中，我们提出了CitaLaw，这是第一个用于评估大语言模型（LLM）生成合法合规响应并适当引用支持文本能力的基准测试。CitaLaw包含了一系列面向普通公众和专业人士的多样化法律问题，并结合了一个全面的法律文章和先例案例参考库。这一框架使基于大语言模型的系统能够从参考库中检索支持引文，并将这些引文与响应中相应的句子对齐。此外，我们引入了借鉴三段论的评估方法来评估检索到的参考文献与大语言模型生成的响应之间的法律一致性，以及这些响应与用户问题的一致性。通过对2个开放领域和7个法律专门领域的大语言模型进行广泛实验，我们证明了集成法律参考文献显著提高了响应质量。此外，我们提出的基于三段论的评估方法与人类判断高度一致。', 'title_zh': 'CitaLaw：在法律领域增强大规模语言模型的引用功能'}
{'arxiv_id': 'arXiv:2412.14533', 'title': 'ClusterTalk: Corpus Exploration Framework using Multi-Dimensional Exploratory Search', 'authors': 'Ashish Chouhan, Saifeldin Mandour, Michael Gertz', 'link': 'https://arxiv.org/abs/2412.14533', 'abstract': 'Exploratory search of large text corpora is essential in domains like biomedical research, where large amounts of research literature are continuously generated. This paper presents ClusterTalk (The demo video and source code are available at: this https URL), a framework for corpus exploration using multi-dimensional exploratory search. Our system integrates document clustering with faceted search, allowing users to interactively refine their exploration and ask corpus and document-level queries. Compared to traditional one-dimensional search approaches like keyword search or clustering, this system improves the discoverability of information by encouraging a deeper interaction with the corpus. We demonstrate the functionality of the ClusterTalk framework based on four million PubMed abstracts for the four-year time frame.', 'abstract_zh': '在生物医学研究等领域，生成大量的研究文献使得大规模文本语料库的探索性搜索变得至关重要。本文提出了一种基于多维探索性搜索的语料库探索框架——ClusterTalk（演示视频和源代码可在以下链接查看：[这个链接]）。ClusterTalk框架结合了文档聚类与多维度的检索技术，使用户能够交互地细化其探索过程，并提出语料库和文档级别的查询。与传统的以关键词搜索或聚类为代表的单一维度搜索方法相比，该系统通过促进用户与语料库的更深入交互，提高了信息的可发现性。我们以四年内共计四百万篇PubMed摘要为例，展示了ClusterTalk框架的功能。', 'title_zh': 'ClusterTalk：基于多维探索性搜索的语料库探索框架'}
{'arxiv_id': 'arXiv:2412.14528', 'title': 'Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models', 'authors': 'Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li', 'link': 'https://arxiv.org/abs/2412.14528', 'abstract': 'Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.', 'abstract_zh': '知识蒸馏（KD）已成为压缩大型语言模型（LLMs）的一种主流技术。现有的KD方法受限于教师模型和学生模型需要共享相同的分词器（即词表），这限制了它们在处理不同架构家族的LLMs时的灵活性。在本文中，我们引入了多级最优传输（MultiLevelOT），这是一种新颖的方法，它推进了最优传输在通用跨分词器知识蒸馏中的应用。我们的方法通过使用多样化的代价矩阵，在令牌和序列两个级别上对教师模型和学生模型的logit分布进行对齐，从而消除了对维度或逐令牌对应关系的需求。在令牌级别，MultiLevelOT通过联合优化序列中的所有令牌来增强鲁棒性，从而整合全局和局部信息。在序列级别，我们通过Sinkhorn距离高效地捕捉logit的复杂分布结构，Sinkhorn距离可以近似Wasserstein距离作为差异度量。在诸如抽取式问答、生成式问答和摘要生成等任务上的广泛实验表明，在各种设置下，MultiLevelOT优于最先进的跨分词器KD方法。我们的方法在不同模型家族、架构和参数规模的学生模型和教师模型中表现出良好的鲁棒性。', 'title_zh': '多层级最优运输在语言模型跨分词器知识精炼中的通用方法研究'}
{'arxiv_id': 'arXiv:2412.14510', 'title': 'PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization', 'authors': 'Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao', 'link': 'https://arxiv.org/abs/2412.14510', 'abstract': 'The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at this https URL.', 'abstract_zh': '检索增强生成（RAG）的出现缓解了大语言模型（LLMs）生成内容时过时和幻觉的问题，但仍然揭示出许多局限性。当通用大语言模型作为RAG生成器时，它往往存在响应信息量不足、响应稳健性和引文质量差的问题。过去的解决方法要么通过添加生成响应之外的步骤，要么通过监督微调（SFT）来优化生成器，但这些方法仍未彻底满足RAG的要求。因此，在保持其端到端大语言模型形式的同时，从多个偏好角度优化RAG生成器仍是一个挑战。为解决这一问题，我们提出了一种多视角偏好对齐方法（PA-RAG），用于全面优化RAG系统的生成器。具体来说，我们通过从不同场景的提示文档质量中采样多种质量的响应来构建高质量的指令微调数据和多视角偏好数据。随后，我们使用SFT和直接偏好优化（DPO）来优化生成器。我们针对三个大语言模型的四个问答数据集进行了广泛实验，结果表明PA-RAG可以显著提高RAG生成器的性能。我们的代码和数据集可在以下链接获取：[提供的URL]。', 'title_zh': 'PA-RAG：多角度偏好优化下的RAG对齐'}
{'arxiv_id': 'arXiv:2412.14501', 'title': 'Do Large Language Models Defend Inferentialist Semantics?: On the Logical Expressivism and Anti-Representationalism of LLMs', 'authors': 'Yuzuki Arai, Sho Tsugawa', 'link': 'https://arxiv.org/abs/2412.14501', 'abstract': "The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans. Traditionally, LLMs have been explained through distributional semantics as their foundational semantics. However, recent research is exploring alternative foundational semantics beyond distributional semantics. This paper proposes Robert Brandom's inferentialist semantics as an suitable foundational semantics for LLMs, specifically focusing on the issue of linguistic representationalism within this post-anthropocentric trend. Here, we show that the anti-representationalism and logical expressivism of inferential semantics, as well as quasi-compositionality, are useful in interpreting the characteristics and behaviors of LLMs. Further, we propose a \\emph{consensus theory of truths} for LLMs. This paper argues that the characteristics of LLMs challenge mainstream assumptions in philosophy of language, such as semantic externalism and compositionality. We believe the argument in this paper leads to a re-evaluation of anti\\hyphen{}representationalist views of language, potentially leading to new developments in the philosophy of language.", 'abstract_zh': '语言哲学在过去一直是以人类为中心的发展，随着大型语言模型（LLMs）如ChatGPT（OpenAI）、Claude（Anthropic）的出现，它现在被迫向后人类中心主义方向前进。这些LLMs被认为具有与人类相媲美的语言能力。传统上，LLMs是通过分布语义学来进行解释的基础语义。然而，近期的研究正在探索超越分布语义学的其他基础语义学。本文提议罗伯特·布朗姆的推理语义学作为一种适合LLMs的基础语义学，特别关注后人类中心主义趋势中的语言表征问题。在这里，我们展示推理语义学的反表征论和逻辑表达主义以及准组合性在解释LLMs的特征和行为中的有用性。进一步地，我们为LLMs提出了一种“共识真理理论”。本文认为，LLMs的特性挑战了语言哲学中的主流假设，如语义外部主义和组合性。我们相信本文的论点将导致对语言反表征论的重新评估，这可能促进语言哲学的新发展。', 'title_zh': '大型语言模型是否捍卫推论主义语义学？关于大型语言模型的逻辑表达主义和反再现论探索'}
{'arxiv_id': 'arXiv:2412.14471', 'title': 'Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs', 'authors': 'Koshiro Saito, Sakae Mizuki, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, Naoaki Okazaki', 'link': 'https://arxiv.org/abs/2412.14471', 'abstract': 'Why do we build local large language models (LLMs)? What should a local LLM learn from the target language? Which abilities can be transferred from other languages? Do language-specific scaling laws exist? To explore these research questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19 evaluation benchmarks for Japanese and English, taking Japanese as a local language. Adopting an observational approach, we analyzed correlations of benchmark scores, and conducted principal component analysis (PCA) on the scores to derive \\textit{ability factors} of local LLMs. We found that training on English text can improve the scores of academic subjects in Japanese (JMMLU). In addition, it is unnecessary to specifically train on Japanese text to enhance abilities for solving Japanese code generation, arithmetic reasoning, commonsense, and reading comprehension tasks. In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \\textit{Japanese abilities} for LLMs. Furthermore, we confirmed that the Japanese abilities scale with the computational budget for Japanese text.', 'abstract_zh': '我们为什么要构建本地大语言模型（LLMs）？本地LLMs应从目标语言中学习哪些内容？哪些能力可以从其他语言中转移？存在语言特定的放大法则吗？为探索上述研究问题，我们在19个日语和英语评估基准上评估了35个日语、英语和多语言LLMs，以日语作为本地语言。采用观察性方法，我们分析了评估基准得分间的相关性，并通过主成分分析（PCA）对得分进行分析，提取出本地LLMs的能力因素。研究发现，使用英语文本训练可以提高日语学术科目（JMMLU）的得分。此外，不需要专门使用日语文本训练来增强解决日语代码生成、算术推理、常识和阅读理解任务的能力。相反，使用日语文本训练可以提高关于日语知识的问题回答任务以及日语-英语翻译任务的得分，这表明解决这些任务的能力可以被视为LLMs的“日语能力”。此外，我们还确认了日语能力与日语文本的计算预算成比例。', 'title_zh': '我们构建本地大型语言模型的原因：来自35个日语和多语言LLM的观察性分析'}
{'arxiv_id': 'arXiv:2412.14470', 'title': 'Agent-SafetyBench: Evaluating the Safety of LLM Agents', 'authors': 'Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang', 'link': 'https://arxiv.org/abs/2412.14470', 'abstract': 'As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \\url{this https URL} to facilitate further research and innovation in agent safety evaluation and improvement.', 'abstract_zh': '随着大型语言模型（LLMs）越来越多地被用作代理，它们与互动环境的整合以及工具使用带来的新安全挑战超出了模型本身所固有的挑战。然而，缺乏全面的安全评估基准是有效评估和进一步改进的重大障碍。本文介绍了一个全面的基准Agent-SafetyBench，用于评估LLM代理的安全性。Agent-SafetyBench 包含349种互动环境和2,000个测试案例，评估了8类安全风险，并涵盖了10种常见失败模式，这些模式在不安全的互动中频繁出现。我们的评估显示了16种流行LLM代理中的一个令人担忧的结果：没有一个代理的得分超过60%。这突显了LLM代理中存在显著的安全挑战，并强调了改进的迫切需求。通过定量分析，我们确定了关键的失败模式，并总结了当前LLM代理中的两种基本安全检测：缺乏鲁棒性和缺乏风险意识。此外，我们的发现表明，仅仅依赖防御提示是不足以解决这些安全问题的，强调了需要更先进和可靠的策略。我们已在 \\url{this https URL} 发布了Agent-SafetyBench，以促进代理安全性评估和改进的研究和创新。', 'title_zh': 'Agent-SafetyBench: 评估大规模语言模型代理的安全性'}
{'arxiv_id': 'arXiv:2412.14461', 'title': 'From Human Annotation to LLMs: SILICON Annotation Workflow for Management Research', 'authors': 'Xiang Cheng, Raveesh Mayya, João Sedoc', 'link': 'https://arxiv.org/abs/2412.14461', 'abstract': 'Unstructured text data annotation and analysis are fundamental to management research, often relying on human annotators through crowdsourcing platforms. While Large Language Models (LLMs) promise to provide a cost-effective and efficient alternative to human annotation, there lacks a systematic workflow that evaluate when LLMs are suitable or how to proceed with LLM-based text annotation in a reproducible manner. This paper addresses this methodological gap by introducing the ``SILICON" (\\textbf{S}ystematic \\textbf{I}nference with \\textbf{L}LMs for \\textbf{I}nformation \\textbf{C}lassificati\\textbf{o}n and \\textbf{N}otation) workflow. The workflow integrates established principles of human annotation with systematic prompt optimization and model selection, addressing challenges such as developing robust annotation guidelines, establishing high-quality human baselines, optimizing prompts, and ensuring reproducibility across LLMs. We validate the SILICON workflow through seven case studies covering common management research tasks, including business proposal evaluation, dialog intent and breakdown analysis, review attribute detection. Our findings highlight the importance of validating annotation guideline agreement, the superiority of expert-developed human baselines over crowdsourced ones, the iterative nature of prompt optimization, and the necessity of testing multiple LLMs. Notably, we propose a regression-based methodology to empirically compare LLM outputs across prompts and models. Our workflow advances management research by establishing reproducible processes for LLM-based annotation that maintain scientific rigor. We provide practical guidance for researchers to effectively navigate the evolving landscape of generative AI tools effectively while maintaining transparency and reproducibility.', 'abstract_zh': '结构化文本数据的标注与分析是管理研究的基础，通常依赖于通过众包平台的人工标注。虽然大型语言模型（LLMs）承诺提供一种成本效益高且高效的替代人类标注的方法，但缺乏一种系统的工作流来评估LLMs的适用性或如何以可重复的方式进行基于LLM的文本标注。本文通过引入“SILICON”（系统化使用大型语言模型进行信息分类和标注）工作流来填补这一方法学上的空白。该工作流将已确立的人工标注原则与系统化的提示优化和模型选择相结合，解决了诸如开发稳健的标注指南、确立高质量的人类基准、优化提示以及确保在不同大型语言模型上的一致性重现性等问题。\n\n我们通过涵盖常见管理研究任务的七个案例研究验证了SILICON工作流，包括商业提案评估、对话意图和拆解分析、审达示例检测。我们的研究发现强调了验证标注指南一致性的必要性，优选专家生成的人类基准而不仅仅是众包基准，提示优化的迭代性质，以及测试多种大型语言模型的必要性。值得注意的是，我们提出了一种基于回归的方法，以实证比较不同提示和模型之间的大型语言模型输出。我们的工作流为基于大型语言模型的标注过程提供了一套可重复的过程，以维持科学研究的严谨性，并为研究人员提供实际指导，以便他们能够有效地应对生成AI工具不断变化的环境，同时保持透明性和可再现性。', 'title_zh': '从人工标注到LLMs：SILICON标注工作流在管理研究中的应用'}
{'arxiv_id': 'arXiv:2412.14436', 'title': 'ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study', 'authors': 'Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko', 'link': 'https://arxiv.org/abs/2412.14436', 'abstract': "Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning \\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69\\% to 76\\% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed \\textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in 73\\% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at \\href{this https URL}{this https URL}.", 'abstract_zh': '近年来语言模型的发展突显了高质量领域特定训练数据的重要性，尤其是在需要专门知识的任务中。通用模型虽然功能强大，但在专家级任务中往往缺乏所需的深度，因为它们的领域特定信息有限。领域适应训练可以增强这些模型，但需要大量的高质量数据。为了解决这一问题，我们提出了ORBIT，一种经济高效的方法，用于从嘈杂的网络来源中收集大量高质量的领域特定数据集，以用于训练专业型大型语言模型。我们以天文学为主要案例研究，将1.3万亿词的FineWeb-Edu数据集提炼成一个专注于天文学的高质量100亿词子集。将\\textsc{LLaMA-3-8B}微调在10亿词的天文学子集上，将MMLU天文学基准的性能从69%提高到76%，并在专门的天文学基准AstroBench中取得了最佳结果。此外，我们的模型Orbit-LLaMA的表现优于\\textsc{LLaMA-3-8B-base}，GPT-4o评估表明，其在1000个天文学特定问题中有73%的情况被更偏好。此外，我们通过将其应用于法律和医学领域验证了ORBIT的通用性，相比于未经筛选的基线，数据质量得到了显著提升。我们开源了ORBIT方法，包括整理的数据集、代码库以及生成的模型，请参见\\href{this https URL}{此处链接}。', 'title_zh': 'ORBIT：面向天文学案例研究的大语言模型领域适配成本有效数据集管理方法'}
{'arxiv_id': 'arXiv:2412.14426', 'title': 'All-in-One Tuning and Structural Pruning for Domain-Specific LLMs', 'authors': 'Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao', 'link': 'https://arxiv.org/abs/2412.14426', 'abstract': 'Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.', 'abstract_zh': '针对大型语言模型（LLMs）在特定领域的应用，现有的剪枝技术通常遵循两阶段过程：首先对预训练的一般用途LLMs进行剪枝，然后在特定领域对剪枝后的LLMs进行微调。然而，来自预训练权重的剪枝决策在微调过程中保持不变，即使权重已被更新。因此，这样的剪枝决策与微调后的权重组合可能并非最优，导致显著的性能下降。为解决这些问题，我们提出了一种新的方法：全合一微调和结构剪枝（ATP）。ATP提供了一种统一的一站式结构剪枝与微调方法，在微调过程中通过可训练的剪枝决策生成器动态识别当前最优子结构。此外，鉴于特定领域应用的数据有限，低秩适应（LoRA）常用作微调LLMs的技术。在ATP中，我们引入了LoRA意识前向传播和稀疏性正则化，以确保学习到的剪枝决策对应的子结构可以在ATP处理完成后直接移除。ATP在法律和医疗领域的任务上优于最先进的两阶段剪枝方法。具体而言，当对LLaMA2-7B和LLaMA3-8B模型分别修剪40%的参数时，ATP分别恢复了密集模型88%和91%的性能。', 'title_zh': '面向特定领域的大型语言模型的一体化调优与结构剪枝'}
{'arxiv_id': 'arXiv:2412.14373', 'title': 'ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling', 'authors': 'William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao', 'link': 'https://arxiv.org/abs/2412.14373', 'abstract': 'Large Language Models (LLMs) have shown remarkable adaptability across domains beyond text, specifically electrocardiograms (ECGs). More specifically, there is a growing body of work exploring the task of generating text from a multi-channeled ECG and corresponding textual prompt. Current approaches typically involve pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective and using the features output by the pretrained encoder to finetune a LLM for natural language generation (NLG). However, these methods are limited by 1) inefficiency from two-stage training and 2) interpretability challenges with encoder-generated features. To address these limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. This approach compresses and encodes ECG signals into tokens, enabling end-to-end LLM training by combining ECG and text tokens directly, while being much more interpretable since the ECG tokens can be directly mapped back to the original signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only half the time and ~48% of the data required by two-stage approaches.', 'abstract_zh': '大型语言模型（LLMs）在文本领域之外的多个领域中表现出了显著的适用性，特别是在心电图（ECGs）领域。具体来说，当前的研究越来越多地关注从多通道ECGs和相应的文本提示中生成文本的任务。现有的方法通常包括通过自监督学习（SSL）目标对特定于ECGs的编码器进行预训练，并使用预训练编码器输出的特征对大型语言模型进行微调以进行自然语言生成（NLG）。然而，这些方法受限于1）两阶段训练的低效率，以及2）编码器生成的特征可解释性较差等问题。为了解决这些限制，我们提出了ECG-Byte，这是一种适应性的字节对编码（BPE）分词器流水线，用于ECGs的自回归语言建模。这种方法将ECGs信号压缩并编码为令牌，通过直接结合ECG和文本令牌进行端到端的LLM训练，从而显著提高可解释性，因为ECG令牌可以直接映射回原始信号。使用ECG-Byte，我们仅在所需数据量的约48%和所需时间的一半内实现了竞争力的表现。', 'title_zh': 'ECG-Byte: 一种用于心电图语言建模的端到端生成型标记器'}
{'arxiv_id': 'arXiv:2412.14368', 'title': "Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation", 'authors': 'Yuxuan Jiang, Francis Ferraro', 'link': 'https://arxiv.org/abs/2412.14368', 'abstract': "Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.", 'abstract_zh': '近年来，大规模语言模型（LLMs）在角色理解任务中展现了令人印象深刻的表现，如分析虚构人物的角色、性格和关系等。然而，LLMs 所使用的大量预训练数据集引发了担忧，它们可能会依靠记忆流行的小说作品，而不是真正理解和推理这些作品的内容。本文认为，在角色理解任务中，捕捉核心意义的“精髓记忆”应当是主要机制，而非逐字匹配的“精确记忆”。我们提出了一种简单而有效的方法，以减少角色理解评估中的机械化记忆现象，同时保留对理解和推理至关重要的隐含线索。我们的方法将对流行小说作品的机械记忆驱动的准确性从96%降低至72%，在各种角色理解任务中的准确性下降幅度可达18%。这些发现强调了现有基准数据中的数据污染问题，这些基准数据往往度量的是记忆而非真正的角色理解。', 'title_zh': '记忆超越推理？揭示并减轻大型语言模型在字符理解评估中照搬记忆的问题'}
{'arxiv_id': 'arXiv:2412.14354', 'title': 'State Space Models are Strong Text Rerankers', 'authors': 'Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar', 'link': 'https://arxiv.org/abs/2412.14354', 'abstract': "Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking -- a task requiring fine-grained query-document interaction and long-context understanding -- remains underexplored.\nThis study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.", 'abstract_zh': 'Transformer架构在自然语言处理（NLP）和信息检索（IR）中占据主导地位，但其推理效率低下和难以将理解扩展到更长的上下文中的问题引发了对替代模型架构的兴趣。在这些替代架构中，状态空间模型（SSMs）如Mamba展现出潜在的优势，尤其是其在推理中的$O(1)$时间复杂性。尽管它们具有潜在的优势，但它们在文本重排序任务——一项需要精细查询-文档交互和长上下文理解的任务——中的有效性仍待进一步探索。\n\n本研究将基于状态空间模型的架构（具体为Mamba-1和Mamba-2）与基于Transformer的模型在不同规模、架构和预训练目标方面进行了基准测试，重点在于文本重排序任务中的性能和效率。我们发现：（1）Mamba架构在文本排序性能方面与相似规模的基于Transformer的模型具有竞争力；（2）它们在训练和推理方面的效率相较于具有闪存注意力机制的Transformer较低；（3）Mamba-2在性能和效率方面优于Mamba-1。这些结果突显了状态空间模型作为Transformer替代品的潜在优势，并指出了未来信息检索应用中需要改进的领域。', 'title_zh': '状态空间模型是强大的文本重排序器'}
{'arxiv_id': 'arXiv:2412.14352', 'title': 'A Survey on LLM Inference-Time Self-Improvement', 'authors': 'Xiangjue Dong, Maria Teleki, James Caverlee', 'link': 'https://arxiv.org/abs/2412.14352', 'abstract': 'Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.', 'abstract_zh': '近年来，通过增加推理时的计算量来提高推理的技术得到了广泛关注。在本文综述中，我们从三个不同的角度调查了大规模语言模型（LLM）推理时自我改进的现状：独立自我改进，侧重于通过解码或采样方法进行增强；情境感知自我改进，利用额外的情境或数据存储；模型辅助自我改进，通过模型协作实现改进。我们对近期相关研究进行了全面回顾，提供了一个深入的分类体系，并讨论了面临的问题和限制，为未来的研究提供了见解。', 'title_zh': '关于LLM推理时自我改进的综述'}
{'arxiv_id': 'arXiv:2412.14351', 'title': 'Is Peer-Reviewing Worth the Effort?', 'authors': 'Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad', 'link': 'https://arxiv.org/abs/2412.14351', 'abstract': 'How effective is peer-reviewing in identifying important papers? We treat this question as a forecasting task. Can we predict which papers will be highly cited in the future based on venue and "early returns" (citations soon after publication)? We show early returns are more predictive than venue. Finally, we end with constructive suggestions to address scaling challenges: (a) too many submissions and (b) too few qualified reviewers.', 'abstract_zh': '同行评审在识别重要论文方面有多有效？我们将这个问题视为一项预测任务：我们能否基于会议场所和“早期引用”（即出版后不久的引用情况）预测哪些论文会在未来获得高度引用？我们证明“早期引用”比会议场所更具预测性。最后，我们提出了建设性的建议以应对规模挑战：（a）论文数量过多和（b）合格评审人不足。', 'title_zh': '《Peer-Reviewing值得付出努力吗？》\n\n这个标题翻译成中文后保留了原文的问句形式，符合学术论文标题的一般规范。如果需要进一步的具体内容翻译或其他帮助，请告知。'}
{'arxiv_id': 'arXiv:2412.14328', 'title': 'Semantic Role Labeling of NomBank Partitives', 'authors': 'Adam Meyers, Advait Pravin Savant, John E. Ortega', 'link': 'https://arxiv.org/abs/2412.14328', 'abstract': 'This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using "gold" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.', 'abstract_zh': '本文探讨了英语部分名词（如“5%/REL 价格/ARG1；价格/ARG1 上涨了 5%/REL”）语义角色标注在 NomBank 注释语料库中的应用。文中描述了多种系统，包括传统机器学习和基于变换器的方法，以及集成学习。我们的最高分系统在使用宾州树库的“金标准”解析时达到 91.74% 的 F1 值，在使用伯克利神经解析器时达到 91.12% 的 F1 值。这项研究涵盖了课堂和实验环境下的系统开发。', 'title_zh': 'NomBank 部分结构的语义角色标注'}
{'arxiv_id': 'arXiv:2412.14323', 'title': 'The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation', 'authors': 'Haohao, Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos', 'link': 'https://arxiv.org/abs/2412.14323', 'abstract': "Translating between languages with drastically different grammatical conventions poses challenges, not just for human interpreters but also for machine translation systems. In this work, we specifically target the translation challenges posed by attributive nouns in Chinese, which frequently cause ambiguities in English translation. By manually inserting the omitted particle X ('DE'). In news article titles from the Penn Chinese Discourse Treebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to English translation models, specifically improving how this critical function word is handled. This focused approach not only complements the broader strategies suggested by previous studies but also offers a practical enhancement by specifically addressing a common error type in Chinese-English translation.", 'abstract_zh': '将具有截然不同语法规范的语言相互翻译，既对人类译员也对机器翻译系统构成了挑战。本研究特别针对中文中主有形容词所造成的翻译难题，这些词经常导致英译中的歧义性。通过手动插入缺失的_particles_（如“的”），我们利用宾夕法尼亚大学中文语料库中的新闻文章标题，建立了专门的数据集来微调Hugging Face的中文到英文翻译模型，特别是在处理这一关键功能词方面取得了改进。这种集中性方法不仅补充了以往研究中提出的广泛策略，还通过具体解决中英翻译中常见错误类型，提供了一种实用的提升方案。', 'title_zh': '《论属性名词的处理在提升中文到英文机器翻译中的作用》'}
{'arxiv_id': 'arXiv:2412.14304', 'title': 'Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs', 'authors': 'David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama', 'link': 'https://arxiv.org/abs/2412.14304', 'abstract': 'Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe.', 'abstract_zh': '当前的眼科临床工作流程受到过度转诊、长时间等待以及复杂且异质性医疗记录的困扰。大型语言模型（LLMs）为自动化各种程序（如分诊、初步测试如视力评估以及报告总结）提供了有希望的解决方案。然而，在自然语言问答任务中，LLMs在不同语言上的表现差异显著，这可能导致在低收入和中等收入国家（LMICs）中医疗保健不平等现象的加剧。本研究介绍了首个经过人工精标的问题多语言眼科问答基准，这使得可以在不同语言之间进行直接的语言对比。我们在7种不同语言上对6种流行的LLMs进行了评估，揭示了不同语言之间的显著偏差，这强调了在LMICs中临床应用LLMs的风险。现有的去偏见方法如翻译链式思考或检索增强生成（RAG）单靠自己并不能弥补这一性能差距，往往无法在所有语言中提升性能，并且缺乏对医学领域的特异性。为了解决这一问题，我们提出了CLARA（跨语言反思代理系统），一种利用检索增强生成和自我验证的新颖的推理时间去偏见方法。我们的方法不仅提高了所有语言的性能，还显著缩小了多语言偏差差距，促进了全球范围内LLMs的公平应用。', 'title_zh': '多语眼医：评估和消除低收入和中等收入国家（LMICs）中LLM眼科问答偏差的多语言基准'}
{'arxiv_id': 'arXiv:2412.14276', 'title': 'Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data', 'authors': 'haina Raza, Drai Paulen-Patterson, Chen Ding', 'link': 'https://arxiv.org/abs/2412.14276', 'abstract': 'Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection', 'abstract_zh': '虚假新闻对现代社会公众意见和社会稳定构成了重大威胁。本研究对比评估了BERT-like编码器模型和自回归解码器大型语言模型（LLMs）在虚假新闻检测任务中的表现。我们介绍了通过GPT-4辅助（一种AI标注方法）和经过人类专家验证的数据集，以确保数据的可靠性。这两种模型都在该数据集上进行了微调。此外，我们还开发了一种指令微调的LLM方法，并在推理过程中使用多数投票生成标签。我们的分析表明，BERT-like模型在分类任务中普遍优于LLMs，而LLMs在对抗文本扰动时表现更加稳健。与弱标签（远处监督）数据相比，结果表明，带有人类监督的AI标签能够获得更好的分类效果。本研究强调了结合基于AI的注释与人工监督的有效性，并展示了不同类型的机器学习模型在虚假新闻检测中的性能。', 'title_zh': '假新闻检测：BERT类模型与生成人工智能标注数据的大语言模型的比较评估'}
{'arxiv_id': 'arXiv:2412.15210', 'title': 'Tokenisation is NP-Complete', 'authors': 'Philip Whittington, Gregor Bachmann, Tiago Pimentel', 'link': 'https://arxiv.org/abs/2412.15210', 'abstract': 'In this work, we prove the NP-completeness of two variants of tokenisation, defined as the problem of compressing a dataset to at most $\\delta$ symbols by either finding a vocabulary directly (direct tokenisation), or selecting a sequence of merge operations (bottom-up tokenisation).', 'abstract_zh': '在本文中，我们证明了两种变种的标记化问题的NP完全性。这两种标记化问题可以归结为将数据集压缩为最多$\\delta$个符号的问题：一种是直接找到词汇表（直接标记化），另一种是选择一系列合并操作的序列（自底向上的标记化）。', 'title_zh': '分词问题是NP完全问题'}
{'arxiv_id': 'arXiv:2412.15177', 'title': 'Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying', 'authors': 'Federico Castagna, Isabel Sassoon, Simon Parsons', 'link': 'https://arxiv.org/abs/2412.15177', 'abstract': "Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.", 'abstract_zh': '研究表明，尽管近期在人工智能研究领域取得突破性进展，最先进的大型语言模型（LLMs）在进行逻辑和数学推理时仍然面临挑战。这些结果表明，LLMs 在处理之前未见过的推理问题或与其训练数据样本相差较大的问题时，往往表现不佳，更像是高阶的数据模式识别器。为应对这一重要关切，本文借鉴了论证理论文献中的关键问题概念，特别关注托马斯·考林（Toulmin）的论证模型。我们表明，使用这些关键问题可以提升LLMs 的推理能力。通过对模型推理过程的深入探查，LLMs 可以评估是否存在逻辑错误，并在给出最终答复之前进行修正。这一理念源自任何有效论证方法的基本准则：结论有效，若其被公认的前提所充分支持。或者，借鉴亚里士多德原则在现实世界的近似表达，即在信息不完整和预设逻辑的情境下，结论的有效性需要通过其他证明方式来否定。此方法成功引导模型的输出通过一个推理管道，从而实现比基线及其链条推理（CoT）实现更好的性能。为此，本文提供了在MT-Bench推理和数学任务上，针对多种LLMs 进行的广泛评估。', 'title_zh': '批判性思考问题：通过论辩性查询引导大规模语言模型的推理'}
{'arxiv_id': 'arXiv:2412.15156', 'title': 'Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM', 'authors': 'Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo', 'link': 'https://arxiv.org/abs/2412.15156', 'abstract': 'Text-to-video models have made remarkable advancements through optimization on high-quality text-video pairs, where the textual prompts play a pivotal role in determining quality of output videos. However, achieving the desired output often entails multiple revisions and iterative inference to refine user-provided prompts. Current automatic methods for refining prompts encounter challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware when applied to text-to-video diffusion models. To address these problem, we introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video, which excels in crafting Video-Centric, Labor-Free and Preference-Aligned prompts tailored to specific video diffusion model. Our approach involves a meticulously crafted two-stage optimization and alignment system. Initially, we conduct a reward-guided prompt evolution pipeline to automatically create optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the LLM. Then multi-dimensional rewards are employed to generate pairwise data for the SFT model, followed by the direct preference optimization (DPO) algorithm to further facilitate preference alignment. Through extensive experimentation and comparative analyses, we validate the effectiveness of Prompt-A-Video across diverse generation models, highlighting its potential to push the boundaries of video generation.', 'abstract_zh': '文本到视频模型已经在高质量文本视频对的优化下取得了显著的进步，其中文本提示在决定输出视频的质量方面起到了关键作用。然而，实现预期的输出往往需要多次修订和迭代推理以完善用户提供的提示。当前用于文本到视频扩散模型的自动提示精炼方法遇到了模态不一致、成本差异和模型无意识等挑战。为了解决这些问题，我们提出了一个基于大语言模型的提示适应框架，称为Prompt-A-Video，该框架擅长为特定视频扩散模型量身定制视频中心化、无需劳动且偏好一致的提示。我们的方法包括一个精心设计的两阶段优化和对齐系统。首先，我们通过奖励导向的提示演化管道自动创建最优提示池，并利用这些提示进行大语言模型的监督微调（SFT）。然后，应用多维度奖励生成SFT模型的数据对，并使用直接偏好优化（DPO）算法进一步促进偏好对齐。通过广泛的实验和比较分析，我们验证了Prompt-A-Video在不同生成模型中的有效性，突显了其推动视频生成边界的可能性。', 'title_zh': 'Prompt-A-Video: 通过偏好对齐的大型语言模型优化你的视频扩散模型'}
{'arxiv_id': 'arXiv:2412.15113', 'title': 'Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture', 'authors': 'Thomas F Burns, Tomoki Fukai, Christopher J Earls', 'link': 'https://arxiv.org/abs/2412.15113', 'abstract': 'Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.', 'abstract_zh': '大型语言模型（LLMs）展示出利用输入序列上下文中的信息以适当响应训练过程中未见过的数据的强大能力。这种能力被称为上下文内学习（ICL）。人类和非人类动物也表现出了类似的能力，但它们的神经架构与LLMs差异巨大。尽管如此，LLMs中的一个关键组件——注意力机制，与现代联想记忆模型相似，这些模型广泛用于并与计算神经科学社区共同影响着对生物记忆系统的建模。利用这种联系，我们引入了一个能够执行ICL的联想记忆模型。我们利用这一启发设计了一种新的残差流架构，允许信息直接在注意力头之间流动。我们在两层Transformer的训练过程中测试了这一架构，并展示了这种修改后ICL能力的体现速度快于没有这种修改的情况。随后，我们将在含有800万个参数的小型语言模型中应用这种架构，重点在注意力头值上，结果显示，在这一较大且更符合自然的规模上，ICL性能也得到了改进。', 'title_zh': '关联记忆启发了一种新型注意力残差流架构在上下文学习中的性能提升'}
{'arxiv_id': 'arXiv:2412.15098', 'title': 'A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation', 'authors': 'João A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva', 'link': 'https://arxiv.org/abs/2412.15098', 'abstract': 'Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.', 'abstract_zh': '不论领域或语言，虚假信息旨在欺骗或操控公众意见，通常通过运用先进的说服技术。对于说服技术在虚假信息中的武器化研究，定性与定量研究多集中在特定主题（如新冠疫情）上，跨领域的研究相对有限，导致对这些策略的全面理解不足。本研究采用最先进的说服技术分类器，进行了一项大规模、跨领域的分析，考察了16种说服技术在虚假信息叙事中的作用。研究表明，不同领域内的虚假信息中，说服技术的应用存在显著差异。我们还详细探讨了气候变化虚假信息的案例，揭示了语言、心理与文化因素如何塑造说服策略，使其适应独特的内容背景。', 'title_zh': '跨领域研究在线虚假信息中说服技术的应用'}
{'arxiv_id': 'arXiv:2412.15077', 'title': 'Till the Layers Collapse: Compressing a Deep Neural Network through the Lenses of Batch Normalization Layers', 'authors': 'Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione', 'link': 'https://arxiv.org/abs/2412.15077', 'abstract': "Today, deep neural networks are widely used since they can handle a variety of complex tasks. Their generality makes them very powerful tools in modern technology. However, deep neural networks are often overparameterized. The usage of these large models consumes a lot of computation resources. In this paper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers \\textbf{C}ollapse (TLC), which compresses deep neural networks through the lenses of batch normalization layers. By reducing the depth of these networks, our method decreases deep neural networks' computational requirements and overall latency. We validate our method on popular models such as Swin-T, MobileNet-V2, and RoBERTa, across both image classification and natural language processing (NLP) tasks.", 'abstract_zh': '如今，由于深神经网络能够处理各种复杂的任务，它们在现代技术中被广泛使用。它们的高度通用性使它们成为非常强大的工具。然而，深神经网络往往会被过度参数化。使用这些大型模型消耗了大量的计算资源。本文介绍了一种名为**Till the Layers Collapse (TLC)** 的方法，该方法通过批量归一化层的角度对深神经网络进行压缩。通过减少这些网络的深度，我们的方法降低了深神经网络的计算需求和总体延迟。我们在包括Swin-T、MobileNet-V2和RoBERTa在内的流行模型上验证了该方法，覆盖了图像分类和自然语言处理（NLP）任务。', 'title_zh': '直到层坍缩：通过批次 normalization 层的视角压缩深度神经网络'}
{'arxiv_id': 'arXiv:2412.15004', 'title': 'Large Language Models and Code Security: A Systematic Literature Review', 'authors': 'Enna Basic, Alberto Giaretta', 'link': 'https://arxiv.org/abs/2412.15004', 'abstract': 'Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.', 'abstract_zh': '大规模语言模型（LLMs）已成为自动化各种编程任务的强大工具，包括安全相关的任务，如检测和修复漏洞。尽管它们具有令人鼓舞的能力，但当要求它们生成或修改现有的代码时，LLMs有可能引入程序员未知的漏洞。在分析代码时，它们可能会遗漏明显的漏洞或将不存在的漏洞标记为问题。在本次系统性文献综述（SLR）中，我们旨在探讨LLMs在各种代码相关任务中的安全优势和潜在缺点。具体来说，首先我们关注LLMs在生成代码时可能引入的漏洞类型。其次，我们分析LLMs在任何给定代码中检测和修复漏洞的能力，以及选择的提示策略如何影响它们在这两项任务中的表现。最后，我们将深入探讨数据投毒攻击如何影响LLMs在上述任务中的性能。', 'title_zh': '大型语言模型与代码安全：系统文献综述'}
{'arxiv_id': 'arXiv:2412.14965', 'title': 'Movie2Story: A framework for understanding videos and telling stories in the form of novel text', 'authors': 'Kangning Li, Zheyang Jia, Anyu Ying', 'link': 'https://arxiv.org/abs/2412.14965', 'abstract': 'Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.', 'abstract_zh': '多模态视频到文本模型已经在生成视频内容的简短描述方面取得了显著进展。然而，在生成结合视频和音频的丰富长文本描述方面依然存在不足。本文介绍了一种名为M2S的框架，旨在通过结合音频、视频和字符识别来生成新的长度文本。M2S包括用于生成视频的长文本描述和理解、基于音频的情绪分析、语速分析以及字符对齐，以及基于视觉的字符识别对齐等功能模块。通过使用大型语言模型GLM4o综合多模态信息，M2S在多模态文本生成领域脱颖而出。我们通过比较实验和人工评估展示了M2S的有效性和准确性。此外，该模型框架具有良好的可扩展性，具有重要的未来研究潜力。', 'title_zh': 'Movie2Story：一种理解视频和以新颖文本形式讲述故事的框架'}
{'arxiv_id': 'arXiv:2412.14660', 'title': 'Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models', 'authors': 'Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong', 'link': 'https://arxiv.org/abs/2412.14660', 'abstract': "Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: \\href{this https URL}{this https URL}.", 'abstract_zh': '多模态大型语言模型（MLLMs）结合视觉和文本数据，用于图像字幕和视觉问题回答等任务。在医疗保健和自动驾驶等关键领域中，正确的不确定性校准对于可靠使用至关重要，但具有挑战性。本文研究了代表性的MLLMs，重点关注它们在各种场景下的校准情况，包括视觉微调前后以及基模型多模态训练前后。我们观察到了它们性能的误校准，同时在这些场景之间的校准并无显著差异。我们还强调了文本和图像之间的不确定性差异，以及它们的整合如何影响整体不确定性。为了更好地理解MLLMs的误校准以及它们评估自身不确定性的能力，我们构建了IDK（我不确定）数据集，这对于评估它们处理未知情况的能力至关重要。我们的研究发现，MLLMs倾向于给出答案而非承认不确定性，但通过适当的提示调整可以提高自我评估的准确性。最后，为了校准MLLMs并增强模型的可靠性，我们提出了诸如温度缩放和迭代提示优化等技术。我们的结果为在多模态应用中有效和负责任地部署MLLMs提供了见解。代码和IDK数据集：[此链接](this https URL)。', 'title_zh': '揭开不确定性面纱：多模态大型语言模型校准与性能深入探究'}
{'arxiv_id': 'arXiv:2412.14596', 'title': 'LDP: Generalizing to Multilingual Visual Information Extraction by Language Decoupled Pretraining', 'authors': 'Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou', 'link': 'https://arxiv.org/abs/2412.14596', 'abstract': 'Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual/English benchmarks.', 'abstract_zh': '视觉信息提取（VIE）在半结构化文档的理解中扮演着重要角色，多种预训练模型已被开发以提升性能。然而，大多数这些工作都是单语的（通常为英语）。由于英语与其它语言之间的预训练语料在数量和质量上的极度不平衡，很少有工作能够拓展到非英语场景。本文系统地开展了实验，以证明视觉和布局模态在不同语言的图像中具有不变性。如果去除文档图像中的语言偏见，基于视觉-布局的模型可以实现显著的跨语言泛化能力。据此，我们提出了一种简单而有效的多语言预训练范式LDP（Language Decoupled Pre-training），以更好地利用单语预训练数据。我们提出的模型LDM（Language Decoupled Model）首先在语言独立的数据上进行预训练，通过扩散模型解耦语言知识，然后在下游语言上进行微调。大量实验表明，LDM在所有最先进的多语言预训练模型中表现更佳，并且在下游单一语言/英语基准测试中也保持了竞争力。', 'title_zh': 'LDP：通过语言解耦预训练实现跨语言视觉信息提取的泛化'}
{'arxiv_id': 'arXiv:2412.14574', 'title': 'Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models', 'authors': 'Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2412.14574', 'abstract': 'Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \\url{this https URL}.', 'abstract_zh': '大型语言模型（LLMs）在列表级段落排序方面展示了令人兴奋的性能。由于输入长度的限制，现有方法通常采用滑动窗口策略。尽管该策略有效，但效率较低，因为它涉及重复的串行处理，通常会多次重新评估相关段落，从而导致冗余的API成本，这些成本与推理令牌的数量成正比。长语境LLM的发展使所有段落能够在单次推理中进行全面排序，从而避免了重复的API成本。在本文中，我们从效率和有效性两个方面对长语境LLM在排序任务中的应用进行了全面研究。令人惊讶的是，我们的实验表明，在监督微调设置中，使用长语境LLM进行全面排序可以显著提高性能并实现巨大的效率提升。此外，我们发现基于现有方法微调全面排序模型的两个局限性：（1）滑动窗口策略无法生成完整的排序列表作为训练标签；（2）语言模型损失不能强调标签中排名靠前的段落ID。为了解决这些问题，我们提出了一种新的完整列表级标签构建方法和全新的基于重要性的学习目标，以用于全面排序。实验表明，我们的方法在基线方法上表现出更优的效果。我们的代码可在\\url{此链接}获取。', 'title_zh': '滑动窗口并非终点：探索使用长上下文大语言模型的全面排名'}
{'arxiv_id': 'arXiv:2412.14516', 'title': 'Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment', 'authors': 'Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar', 'link': 'https://arxiv.org/abs/2412.14516', 'abstract': 'We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.', 'abstract_zh': '我们研究了将大规模语言模型（LLMs）与人类偏好数据对齐的问题。对比偏好优化通过优化与策略相关的隐含奖励，在将LLMs与可用的偏好数据对齐方面表现出了令人鼓舞的结果。然而，对比目标主要关注两个响应关联的隐含奖励的相对值，而忽视了它们的实际值，导致了与人类偏好之间的非最优对齐。为解决这一局限，我们提出了校准直接偏好优化（Cal-DPO），这是一种简单而有效的算法。我们证明，通过校准隐含奖励，确保学习到的隐含奖励在尺度上与真实奖励相当，可以显著提高与给定偏好数据的对齐程度。我们展示了Cal-DPO相对于现有方法的理论优势。我们的实验结果表明，Cal-DPO在多种标准基准上的表现显著提高了现成方法的效果。', 'title_zh': 'Cal-DPO：校准直接偏好优化的语言模型对齐方法'}
{'arxiv_id': 'arXiv:2412.14480', 'title': 'GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering', 'authors': 'Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer', 'link': 'https://arxiv.org/abs/2412.14480', 'abstract': 'In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. Through experiments in simulation on the HM-EQA dataset and in the real world in home and office environments, we demonstrate that our method outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps.', 'abstract_zh': '在具身问答（EQA）中，代理必须探索和构建对未见过环境的语义理解，以自信地回答具身处境的问题。这一问题在机器人学中仍极具挑战性，主要由于获取有用语义表示的困难、在线更新这些表示的挑战以及利用先验世界知识进行高效探索和规划的难度。为了应对这些限制，我们提出了一种名为GraphEQA的新方法，该方法利用实时的3D度量语义场景图（3DSGs）和与任务相关的图像作为多模态记忆，将视觉语言模型（VLMs）锚定到未见过环境中的EQA任务上。我们采用了层次规划方法，利用3DSGs的层次结构进行结构化规划和语义导向探索。通过在HM-EQA数据集和家庭及办公环境中的实际世界实验，我们证明了我们的方法在完成EQA任务的成功率和规划步骤方面均优于关键基准方法。', 'title_zh': 'GraphEQA：使用3D语义场景图进行实时嵌入式问答'}
{'arxiv_id': 'arXiv:2412.14475', 'title': 'MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval', 'authors': 'Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, Yongping Xiong', 'link': 'https://arxiv.org/abs/2412.14475', 'abstract': 'Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70$\\times$ more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field.', 'abstract_zh': '尽管多模态检索的需求快速增长，但在这一领域的进步仍然受到训练数据匮乏的严重限制。本文我们介绍了一种新的数据合成方法MegaPairs，该方法利用视觉语言模型（VLMs）和开放领域图像，并结合通过该方法生成的巨大合成数据集。我们的实证分析表明，MegaPairs生成高质量的数据，使得多模态检索器的性能显著优于在现有数据集70倍数据上训练的基线模型。此外，由于MegaPairs仅依赖于通用图像数据库和开源的VLMs，因此可以轻松扩展，从而实现检索性能的持续改进。目前，我们已生成超过2600万训练实例，并使用这些数据训练了多个不同规模的模型。这些新模型在4个流行的组合图像检索（CIR）基准测试中达到最新的零样本性能，并在MMEB提供的36个数据集中获得了最高的总体性能。此外，这些模型在后续微调后还展示了显著的性能提升。我们生成的Dataset、训练好的模型以及数据合成流水线将公开发布，以便促进该领域未来的发展。', 'title_zh': 'MegaPairs：大规模数据合成用于通用多模态检索'}
{'arxiv_id': 'arXiv:2412.14454', 'title': 'Are Longer Prompts Always Better? Prompt Selection in Large Language Models for Recommendation Systems', 'authors': 'Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka', 'link': 'https://arxiv.org/abs/2412.14454', 'abstract': 'In large language models (LLM)-based recommendation systems (LLM-RSs), accurately predicting user preferences by leveraging the general knowledge of LLMs is possible without requiring extensive training data. By converting recommendation tasks into natural language inputs called prompts, LLM-RSs can efficiently solve issues that have been difficult to address due to data scarcity but are crucial in applications such as cold-start and cross-domain problems. However, when applying this in practice, selecting the prompt that matches tasks and data is essential. Although numerous prompts have been proposed in LLM-RSs and representing the target user in prompts significantly impacts recommendation accuracy, there are still no clear guidelines for selecting specific prompts.\nIn this paper, we categorize and analyze prompts from previous research to establish practical prompt selection guidelines. Through 450 experiments with 90 prompts and five real-world datasets, we examined the relationship between prompts and dataset characteristics in recommendation accuracy. We found that no single prompt consistently outperforms others; thus, selecting prompts on the basis of dataset characteristics is crucial. Here, we propose a prompt selection method that achieves higher accuracy with minimal validation data. Because increasing the number of prompts to explore raises costs, we also introduce a cost-efficient strategy using high-performance and cost-efficient LLMs, significantly reducing exploration costs while maintaining high prediction accuracy. Our work offers valuable insights into the prompt selection, advancing accurate and efficient LLM-RSs.', 'abstract_zh': '在基于大型语言模型（LLM）的推荐系统（LLM-RS）中，通过利用LLM的一般知识，可以在不依赖大量训练数据的情况下准确预测用户偏好。通过将推荐任务转化为称为提示的自然语言输入，LLM-RS可以高效解决由于数据稀缺而导致难以解决的问题，但在冷启动和跨域应用等场景中仍然至关重要。然而，在实际应用中，选择与任务和数据相匹配的提示至关重要。尽管在LLM-RS中已经提出了许多提示，且提示中对目标用户的表示显著影响推荐准确性，但仍没有明确的提示选择指南。\n\n在本文中，我们对前人研究中的提示进行了分类和分析，以建立实际的提示选择指南。通过使用5个真实数据集和90个提示的450次实验，我们探讨了提示与推荐准确度之间的关系及其与数据集特征的联系。研究发现，没有一个提示能在所有情况下表现最佳；因此，根据数据集特征选择提示至关重要。在此基础上，我们提出了一种使用最少验证数据即可实现更高准确性的提示选择方法。由于增加提示数量以探索会增加成本，我们还引入了一种成本效率高的策略，使用高性能且成本效益高的LLM，显著降低了探索成本，同时保持了高预测准确性。我们的工作为提升LLM-RS的准确性和效率提供了有价值的见解，推动了精准高效的LLM-RS的发展。', 'title_zh': '更长的提示是否always更好？大型语言模型在推荐系统中选择提示的研究'}
{'arxiv_id': 'arXiv:2412.14414', 'title': 'In-Group Love, Out-Group Hate: A Framework to Measure Affective Polarization via Contentious Online Discussions', 'authors': 'Buddhika Nettasinghe, Ashwin Rao, Bohan Jiang, Allon Percus, Kristina Lerman', 'link': 'https://arxiv.org/abs/2412.14414', 'abstract': 'Affective polarization, the emotional divide between ideological groups marked by in-group love and out-group hate, has intensified in the United States, driving contentious issues like masking and lockdowns during the COVID-19 pandemic. Despite its societal impact, existing models of opinion change fail to account for emotional dynamics nor offer methods to quantify affective polarization robustly and in real-time. In this paper, we introduce a discrete choice model that captures decision-making within affectively polarized social networks and propose a statistical inference method estimate key parameters -- in-group love and out-group hate -- from social media data. Through empirical validation from online discussions about the COVID-19 pandemic, we demonstrate that our approach accurately captures real-world polarization dynamics and explains the rapid emergence of a partisan gap in attitudes towards masking and lockdowns. This framework allows for tracking affective polarization across contentious issues has broad implications for fostering constructive online dialogues in digital spaces.', 'abstract_zh': '情感极化是指在价值观念对立的群体之间形成的情感分离，表现出对自己群体的爱和对敌对群体的憎。在美国，情感极化现象在新冠疫情（COVID-19）期间进一步加剧，导致了诸如戴口罩和封锁等争议性问题的分歧。尽管它对社会产生了重大影响，现有的意见变化模型却未能考虑到情感动态，也没有提供有效的方法来实时和稳健地量化情感极化。在这篇论文中，我们引入了一个离散选择模型来捕捉情感极化社会网络中的决策过程，并提出了一种统计推断方法，通过社交媒体数据估计关键参数——对内群体的爱和对外群体的憎。通过新冠疫情在线讨论的实证验证，我们证明了我们的方法能够准确捕捉现实世界中的极化动态，并解释了对戴口罩和封锁态度中的快速党派差距的形成。该框架可以跨多种争议性问题追踪情感极化，在数字空间促进建设性对话方面具有广泛的影响。', 'title_zh': '我们来劲，他们受厌恶：一种通过争议性在线讨论衡量情感极化的方法论'}
{'arxiv_id': 'arXiv:2412.14363', 'title': 'ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals', 'authors': 'Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang', 'link': 'https://arxiv.org/abs/2412.14363', 'abstract': 'Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）的后训练量化（PTQ）在降低推理时的计算成本方面具有潜力。将所有权重、激活和关键价值（KV）缓存张量量化为4位而不显著损害泛化能力具有挑战性，原因是在激活中存在极端异常值导致量化误差较高。为解决这一问题，我们提出了ResQ，这是一种能够大幅提升现有最佳方法的PTQ方法。通过主成分分析（PCA），ResQ识别出一个低秩子空间（通常是隐藏维度的1/8），在这个子空间中激活的方差最高，并保持该子空间内的系数以高精度表示，例如8位，而将其余部分量化为4位。在每个子空间中，应用不变随机旋转进一步抑制异常值。我们证明，这是一种证明是最佳的混合精度量化方案，能够最小化误差。在Llama家族模型上，我们展示了ResQ在多种基准测试中超越了近期的均匀精度和混合精度PTQ方法，比最近的第二优方法SpinQuant在Wikitext数据集上的困惑度降低多达33%，并且相对于16位基线实现2.4倍的速度提升。源代码可通过以下链接获取：this https URL。', 'title_zh': 'ResQ：低秩残差的混合精度量化大型语言模型'}
{'arxiv_id': 'arXiv:2412.14186', 'title': 'Towards AI-$45^{\\circ}$ Law: A Roadmap to Trustworthy AGI', 'authors': 'Yang Chao, Lu Chaochao, Wang Yingchun, Zhou Bowen', 'link': 'https://arxiv.org/abs/2412.14186', 'abstract': "Ensuring Artificial General Intelligence (AGI) reliably avoids harmful behaviors is a critical challenge, especially for systems with high autonomy or in safety-critical domains. Despite various safety assurance proposals and extreme risk warnings, comprehensive guidelines balancing AI safety and capability remain lacking. In this position paper, we propose the \\textit{AI-\\textbf{$45^{\\circ}$} Law} as a guiding principle for a balanced roadmap toward trustworthy AGI, and introduce the \\textit{Causal Ladder of Trustworthy AGI} as a practical framework. This framework provides a systematic taxonomy and hierarchical structure for current AI capability and safety research, inspired by Judea Pearl's ``Ladder of Causation''. The Causal Ladder comprises three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. These layers address the key challenges of safety and trustworthiness in AGI and contemporary AI systems. Building upon this framework, we define five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. These levels represent distinct yet progressive aspects of trustworthy AGI. Finally, we present a series of potential governance measures to support the development of trustworthy AGI.\\footnote{In this paper, trustworthiness is generally considered a broad form of safety, and no explicit distinction is made between the two. However, in some contexts, safety and trustworthiness are treated as distinct: safety involves assurance of correct behavior, while trustworthiness refers to user confidence in the system's decision-making. In such cases, different terms or both may be used depending on the context.", 'abstract_zh': '确保人工通用智能（AGI）可靠地避免有害行为是一项关键挑战，特别是对于高度自主的系统或在安全性关键领域中的系统。尽管有各种各样的安全保证提议和极端风险警告，全面平衡人工智能的安全性和能力的指导方针仍然缺乏。在本文中，我们提出“AI-45°法则”作为通往可信赖AGI的平衡路线图的指导原则，并引入“可信赖AGI因果梯子”作为实用框架。该框架为当前人工智能能力与安全研究提供了一种系统的分类法和层次结构，灵感来源于贾德亚·佩雷尔的“因果梯子”。因果梯子包含三个核心层：近似对齐层、可干预层和反思层。这些层解决了AGI及其当前人工智能系统中安全性和可信度的关键挑战。基于这一框架，我们定义了五个级别的可信AGI：感知可信度、推理可信度、决策可信度、自主性可信度和协作可信度。这些级别代表了可信AGI的独立且逐步进化的不同方面。最后，我们提出了支持可信AGI发展的若干潜在治理措施。[在本文中，通常认为可信度是一种广泛的名义上的安全形式，两者之间没有明确区分。但在某些情况下，安全性和可信度被视为不同的概念：安全性涉及确保正确的行为，而可信度则指用户对系统决策的信心。在这种情况下，根据具体上下文可能使用不同的术语或两者兼用。]', 'title_zh': '向AI-45°定律迈进：迈向可信赖的超人工智能之路'}
{'arxiv_id': 'arXiv:2412.11449', 'title': 'Whisper-GPT: A Hybrid Representation Audio Large Language Model', 'authors': 'Prateek Verma', 'link': 'https://arxiv.org/abs/2412.11449', 'abstract': 'We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.', 'abstract_zh': '我们提出WHISPER-GPT：一种同时处理连续音频表示和离散音素的生成型大型语言模型（LLM），使其能够利用单一架构将连续音频表示和离散音素作为组成部分进行操作。近年来，利用神经压缩算法（如ENCODEC）生成的离散音频音素的生成音频、语音和音乐模型取得了巨大进展。然而，这种方法的主要缺点是处理上下文长度。如果需要考虑各种频率下的所有音频内容以预测下一个音素，则会导致高保真度生成架构的上下文长度膨胀。通过结合如频谱图等连续音频表示和离散声学音素，我们保留了两者的优点：在单个音素中包含特定时间点所需的全部音频信息，同时允许LLM 预测未来音素以利用离散空间提供的采样和其他好处。我们展示了与基于音素的语音和音乐LLM 相比，我们的架构如何改善了下一音素预测的困惑度和负对数似然分数。', 'title_zh': 'Whisper-GPT：一种混合表示音频大语言模型'}
