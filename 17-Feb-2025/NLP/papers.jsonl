{'arxiv_id': 'arXiv:2502.10391', 'title': 'MM-RLHF: The Next Step Forward in Multimodal LLM Alignment', 'authors': 'Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan', 'link': 'https://arxiv.org/abs/2502.10391', 'abstract': 'Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\\mathbf{19.5}$% increase in conversational abilities and a $\\mathbf{60}$% improvement in safety.\nWe have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: this https URL.', 'abstract_zh': '尽管在多模态大型语言模型（MLLMs）方面取得了显著进展，但大多数最先进的模型尚未经过彻底的人类偏好对齐。这一差距源于当前对齐研究主要在特定领域（例如，幻觉减少）取得了进展，而关于是否可以通过系统性地将模型对齐与人类偏好相结合来提升MLLM的能力这一更广泛的问题仍然 largely 未被探索。为解决这一问题，我们引入了 MM-RLHF 数据集，该数据集包含 **120,000** 对细粒度的人类注释偏好对比样本。与现有资源相比，该数据集代表了一项重大进展，提供了更大的规模、更高的多样性、更精细的注释粒度和更高的质量。利用该数据集，我们提出了一些关键创新来提高奖励模型的质量和对齐算法的效率。特别是，我们引入了一种批判性奖励模型，该模型在分配评分之前生成对模型输出的批判意见，从而提供了比传统标量奖励机制更高的可解释性和更具信息性的反馈。此外，我们还提出了动态奖励缩放方法，该方法根据奖励信号调整每个样本的损失权重，从而优化高质量对比样本的使用。我们从 **10** 个不同的维度和 **27** 个基准进行了严格的评估，结果显示模型性能取得了显著且一致的改进。具体来说，使用 MM-RLHF 数据集和我们的对齐算法微调 LLaVA-ov-7B 在对话能力上提高了 **19.5%**，在安全性上提高了 **60%**。\n\n我们已经开源了偏好数据集、奖励模型、训练和评估代码，以及奖励建模和安全性基准。如需更多详细信息，请访问我们的项目页面：[此链接](this https URL)。', 'title_zh': 'MM-RLHF：迈向多模态大语言模型对齐的下一步'}
{'arxiv_id': 'arXiv:2502.10388', 'title': 'Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction', 'authors': 'WonJin Yoon, Boyu Ren, Spencer Thomas, Chanwhi Kim, Guergana Savova, Mei-Hua Hall, Timothy Miller', 'link': 'https://arxiv.org/abs/2502.10388', 'abstract': 'Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different \\textit{information signals}, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.', 'abstract_zh': '近年来，大型语言模型（LLMs）的进步使得即使没有针对特定任务的监督训练数据，也能自动处理长文档成为可能。然而，它们在复杂任务中的零样本性能相较于简单的信息提取任务仍然欠佳。对于具有长且复杂输入的任务，一种可行的方法是首先对文档进行总结，然后对总结内容进行监督微调。然而，总结过程不可避免地会损失一些信息。在本研究中，我们提出了一种方法来处理长文档的摘要，旨在捕捉原始文档的不同重要方面。我们假设使用不同方面导向的提示生成的LLM摘要包含了不同的“信息信号”，并提出了一种方法来测量这些差异。我们介绍了将这些不同摘要中的信号有效地集成起来的方法，以对变压器模型进行监督训练。我们使用四个医院的现实世界数据验证了我们的假设，用于30天再入院预测的心理疾病出院任务，并展示了我们提出的方法能够提高复杂任务（预测患者结果）的预测性能。', 'title_zh': '面向方面的摘要生成在精神疾病短期再入院预测中的应用'}
{'arxiv_id': 'arXiv:2502.10373', 'title': 'OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models', 'authors': 'William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, Shinji Watanabe', 'link': 'https://arxiv.org/abs/2502.10373', 'abstract': 'Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on this https URL for future studies.', 'abstract_zh': '神经网络的规模法则为设计稳健的序列处理架构提供了宝贵的见解。尽管这些法则在其他模态中已经被广泛研究，但在语音中的表现却相对未被充分探索。在这项工作中，我们引入了OWLS，这是一个开放访问、可重复的跨语言语音识别和翻译模型集合，参数范围从2.5亿到18亿，到我们所知，18亿参数版本是目前最大的语音模型。OWLS 利用了共计36万小时的多语言公开语音数据，覆盖150种语言，从而系统地探讨了数据、模型和计算量如何影响多语言语音任务的性能。我们使用OWLS 提取了神经网络的规模法则，展示了当进行扩展时，最终性能可以可靠地预测。我们的一个关键发现是，规模法则在资源有限的语言/方言中提高了性能，有助于减轻偏差并提高语音技术的可达性。最后，我们展示了如何利用OWLS 探索大规模语音模型中新兴的能力，从而推动新的研究方向。模型检查点将发布在以下网址，供未来的研究使用：[此处应是具体网址]。', 'title_zh': 'OWLS：多语言语音识别和翻译模型的标度定律'}
{'arxiv_id': 'arXiv:2502.10361', 'title': 'Enhancing Multilingual LLM Pretraining with Model-Based Data Selection', 'authors': 'Bettina Messmer, Vinko Sabolčec, Martin Jaggi', 'link': 'https://arxiv.org/abs/2502.10361', 'abstract': 'Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.', 'abstract_zh': '数据集整理已成为强大大型语言模型（LLM）性能的基础。虽然各种基于规则的过滤启发式方法已应用于英语和多语言数据集，但基于模型的过滤技术主要集中在英语上。为了解决因对非英语语言研究不足而产生的差距，我们提出了一种针对多语言数据集的基于模型的过滤框架，旨在识别一个结构多样且知识丰富样本集。我们的方法强调透明性、简洁性和效率，通过利用基于Transformer和FastText的分类器，确保我们的技术和数据具有广泛的可访问性。我们在FineWeb-2网络爬虫数据集上进行了跨多种语言家族、书写系统和资源可获得性的全面消融研究，以展示我们方法的有效性。通过对70B和119B令牌训练具有1B参数的Llama模型，我们的方法可以用最少15%的训练令牌达到基线MMLU得分，并且在其他基准测试中也表现出改进。这些发现为我们的方法在其他语言中的普适性提供了强有力的证据。因此，我们将框架扩展到20种语言，并发布了这些语言的精炼预训练数据集。', 'title_zh': '基于模型的数据选择增强多语言大语言模型预训练'}
{'arxiv_id': 'arXiv:2502.10352', 'title': 'Agentic Verification for Ambiguous Query Disambiguation', 'authors': 'Youngwon Lee, Seung-won Hwang, Ruofan Wu, Feng Yan, Danmei Xu, Moutasem Akkad, Zhewei Yao, Yuxiong He', 'link': 'https://arxiv.org/abs/2502.10352', 'abstract': 'In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.', 'abstract_zh': '在本文中，我们针对检索增强生成（RAG）中查询消歧的问题进行了研究，旨在获取多样化但又可回答的解释。现有的先进方法采用“多样化-然后验证”（DtV）的管道，其中通过语言模型（LLM）生成多样化的解释，随后将这些解释用作检索查询以获取支持段落。这一过程中，无论是解释还是检索都可能引入噪声，尤其是在企业环境中，LLM 在处理特定领域消歧问题时可能面临挑战。因此，我们引入了一个事后验证阶段以消除这些噪声。我们的区别在于，通过早期结合检索器和生成器的反馈，将多样化与验证统一起来。这种联合方法通过减少对外部多次检索和推理步骤的依赖，从而在减少累积错误的同时提高了效率和鲁棒性。我们验证了联合方法Consolidated Verified Diversification (VERDICT) 在广泛采用的ASQA基准上的效率和有效性，实现了多样化且可验证的解释。实验证明，VERDICT 相比最强的方法在不同语言模型（LLM）的背景下平均提高了23%的注意向量关联F1分数。', 'title_zh': '代理验证在模糊查询去歧义化中的应用'}
{'arxiv_id': 'arXiv:2502.10341', 'title': 'Organize the Web: Constructing Domains Enhances Pre-Training Data Curation', 'authors': 'Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini', 'link': 'https://arxiv.org/abs/2502.10341', 'abstract': 'Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.', 'abstract_zh': '现代语言模型是在包含数十万亿令牌的大型无结构数据集上进行训练的，这些数据集通过爬取网络获得。这种无结构的特性使得难以推理其内容并发展出系统的方法来管理数据。在本文中，我们通过发展内容分类法并将其组织到不同的领域来拆解整体化的网络语料库。我们引入了WebOrganizer框架，该框架可以基于网页的主题和格式对其进行组织。利用这两种互补的领域概念，我们通过从大规模语言模型中提炼注释来自动标注预训练数据，这使得我们可以研究不同类型领域之间的数据应该如何混合以提高下游任务的模型表现，并表明我们可以通过结合有效主题和格式方面的见解来进一步提升性能。我们证明了我们的领域混合不仅可以提高基于质量选择数据的方法，还可以改善现有方法。此外，我们研究并比较了基于质量的方法如何隐含地改变领域混合。总体而言，我们的工作证明了构建和混合领域的价值，补充了基于质量的数据管理方法，为有效的和富有洞察力的预训练数据管理开辟了新的途径。', 'title_zh': '组织网络空间：构建领域知识增强预训练数据的管理'}
{'arxiv_id': 'arXiv:2502.10339', 'title': 'STAR: Spectral Truncation and Rescale for Model Merging', 'authors': 'Yu-Ang Lee, Ching-Yun Ko, Tejaswini Pedapati, I-Hsin Chung, Mi-Yen Yeh, Pin-Yu Chen', 'link': 'https://arxiv.org/abs/2502.10339', 'abstract': "Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd $\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is publicly available at this https URL.", 'abstract_zh': '模型合并是通过几个预训练模型获得多任务模型的一种高效方法，无需进一步微调，已在自然语言处理（NLP）等领域引起了广泛关注。尽管模型合并具有高效性，但在合并多个模型时，任务性能似乎不可避免地下降是一个关键挑战。本文提出了一种名为Spectral Truncation and Rescale（STAR）的方法，其目标是通过裁剪相应频谱空间中的小组件来减轻“合并冲突”，并采用自动参数调整方案以保留原始矩阵的核范数。STAR 不需要在原始训练数据上进行额外推理，并且对超参数选择具有鲁棒性。我们通过在多种NLP任务上的广泛模型合并案例展示了STAR的有效性。具体而言，STAR 在合并12个模型时可以在Flan-T5基准上提高4.2%的性能。我们的代码已公开发布，网址为这个 https URL。', 'title_zh': 'STAR：谱截断与缩放模型融合'}
{'arxiv_id': 'arXiv:2502.10338', 'title': 'Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering', 'authors': 'Nick Ferguson, Liane Guillou, Alan Bundy, Kwabena Nuamah', 'link': 'https://arxiv.org/abs/2502.10338', 'abstract': 'Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and reframe them in terms of meta-level reasoning (akin to high-level strategic reasoning or planning) and object-level reasoning (embodied in lower-level tasks such as mathematical reasoning). Franklin, a novel dataset with requirements of meta- and object-level reasoning, is introduced and used along with three other datasets to evaluate four LLMs at question answering tasks requiring multiple steps of reasoning. Results from human annotation studies suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks in some of the datasets used. Additionally, evidence suggests that LLMs find the object-level reasoning required for the questions in the Franklin dataset challenging, yet they do exhibit strong performance with respect to the meta-level reasoning requirements.', 'abstract_zh': '大语言模型（LLMs）在自然语言任务中表现出色，但在需要复杂多步推理的问答（QA）任务中仍然面临挑战。我们概述了这些任务中所需的各种推理类型，并将其重新定义为元级推理（类似于高层次的战略推理或规划）和对象级推理（体现在如数学推理等低层任务中）。我们介绍了一个新的数据集Franklin，该数据集包含元级和对象级推理的要求，并与另外三个数据集一起用于评估四款LLM在多步推理问答任务中的性能。人类标注研究的结果表明，LLM在元级推理方面表现出较高的频率，但在某些数据集中的对象级推理任务中却遇到困难。此外，证据表明，LLM在Franklin数据集中所需的对象级推理是一项挑战，但它们在满足元级推理要求方面表现出很强的能力。', 'title_zh': '评估大型语言模型在问答任务中的元级和对象级推理能力'}
{'arxiv_id': 'arXiv:2502.10266', 'title': 'Are Large Language Models the future crowd workers of Linguistics?', 'authors': 'Iris Ferrazzo', 'link': 'https://arxiv.org/abs/2502.10266', 'abstract': "Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs. For these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.", 'abstract_zh': '从人类参与者中获取数据是实证语言学研究中核心的数据收集策略之一。这类研究中的参与者数量可能有很大差异，从少数几人到大规模众包都包括在内。尽管这两种方法都可以提供丰富详尽的数据，但它们也伴随着许多缺点，例如在任务完成过程中难以控制参与者注意力，众包环境中的不稳定工作条件，以及耗时的实验设计。鉴于这些原因，本研究旨在探讨大型语言模型（LLMs）是否能在实证语言学流水线中克服这些障碍。我们进行了两个重复案例研究来探讨这个问题：Cruz（2023）和Lombard等人（2021）。原先是为人类参与者设计的两个强制性数据收集任务被改造成本框架，并借助OpenAI的GPT-4o-mini模型进行再现。其零样本提示基准下的表现显示了大语言模型的有效性和高度的灵活性，通常在语言任务上优于人类提供者。第二轮重复研究的结果进一步突出了探索额外提示技术（如思维链提示，CoT）的必要性，这种提示技术在后续实验中对关键项和填充项的表现与人类表现的对齐程度更高。鉴于本研究规模有限，有必要进一步探索大语言模型在实证语言学及其在人文学科其他潜在应用中的性能。', 'title_zh': '大型语言模型能否成为未来语言学领域的众包工作者？'}
{'arxiv_id': 'arXiv:2502.10263', 'title': 'Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers', 'authors': 'Aivin V. Solatorio, Rafael Macalaba, James Liounis', 'link': 'https://arxiv.org/abs/2502.10263', 'abstract': 'Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.', 'abstract_zh': '对研究论文中提到和使用数据的方式进行跟踪，能够为提高数据的可发现性、质量和生产提供关键见解。然而，手动识别和分类学术文献中的数据集提及是资源密集型且难以扩展的。本文提出了一种基于机器学习的方法，通过利用大型语言模型（LLMs）、合成数据和两阶段微调过程，在各个研究领域自动化数据集提及检测。我们采用零样本提取技术从研究论文中提取数据集提及，利用LLM作为裁判进行质量评估，并使用推理代理进行精炼，生成一个弱监督的合成数据集。Phi-3.5-mini指令模型先在这个数据集上进行预微调，然后在手动标注的小数据集上进行微调。在推理阶段，基于ModernBERT的分类器高效地过滤数据集提及，减少了计算开销同时保持了高召回率。在保留的手动标注样本上进行评估，我们微调的模型在数据集提取准确性方面优于NuExtract-v1.5和GLiNER-large-v2.1。我们的结果表明，通过大规模生成合成数据集，可以有效解决训练数据稀缺问题，提高资源匮乏环境下的泛化能力。该框架为实现规模化的数据集使用监控提供了途径，增强了透明度，并支持研究人员、资助者和政策制定者识别数据缺口，从而促进基于数据的明智决策。', 'title_zh': '大型语言模型和合成数据在科研论文中监测数据集引用的應用'}
{'arxiv_id': 'arXiv:2502.10250', 'title': 'VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models', 'authors': 'Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu', 'link': 'https://arxiv.org/abs/2502.10250', 'abstract': "Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a `leaky modality mix,' where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q\\&A pairs. VisCon-100k dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications. Using the same pipeline, but substituting our trained contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.", 'abstract_zh': '视觉语言模型（VLMs）在各种视觉基准测试中表现出色，但往往受限于高质量视觉微调数据的缺乏。为解决这一挑战，我们引入了VisCon-100K，这是一个源自交错图像-文本网页文档的新数据集。我们的方法将来自OBELICS数据集的45K网页文档转化为100K幅图像对话样本。我们使用GPT-4V生成图像上下文描述，并利用OpenChat 3.5模型将这些描述转化为多种自由形式和多项选择的问题-答案对。将此数据集用于微调显著提升了多种基准测试下VLM的性能。与仅专注于细粒度视觉内容的方法不同，我们的方法利用伴随的网页上下文，取得了更好的结果。我们还发现，在对话样本中，当问题可以从图像及其上下文描述中回答时，即存在“泄露模态混合”的情况下，问题-答案对和描述的非泄露组合表现更优。使用VisCon-100K数据集，我们的数据集展现了两种流行VLM方法的强大性能：仅包含文本的大规模语言模型（LLM）通过图像描述与视觉编码器对齐（ShareGPT4V-7b），以及使用交错图像-文本数据的多模态预训练LLM（IDEFICS2-8b）。除了发布VisCon-100K数据集外，我们还提供基于该数据集训练的上下文描述器，便于未来研究和开源应用的可扩展微调数据生成。借助相同的工作流程，但用我们训练的上下文描述器替换GPT-4V，我们还发布了更大的VisCon-1M数据集。', 'title_zh': 'VisCon-100K：利用上下文网络数据fine-tune视觉语言模型'}
{'arxiv_id': 'arXiv:2502.10202', 'title': 'Can Post-Training Quantization Benefit from an Additional QLoRA Integration?', 'authors': 'Xiliang Zhu, Elena Khasanova, Cheng Chen', 'link': 'https://arxiv.org/abs/2502.10202', 'abstract': 'Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.', 'abstract_zh': '大语言模型（LLMs）已显著改变自然语言处理领域，但它们在实际部署中也带来了重大挑战。这些模型需要大量的计算资源，这可能会带来高昂的成本并且经常不可用。模型压缩技术，如量化（quantization），常被用来缓解这一需求，但它们可能会对生成质量产生负面影响。在本研究中，我们探讨了将4位后训练量化（Post-training Quantization, PTQ）与QLoRA集成的方法，以解决这些问题。通过广泛的实验，我们展示了这种集成方法优于传统的PTQ，甚至在某些情况下优于16位全参数微调，这些结果在使用不同量化算法的自有和公有数据集上得到了验证。研究结果表明，PTQ-QLoRA集成的有效性，为在资源受限环境下部署强大的LLMs提供了可行的解决方案，同时不会牺牲性能。', 'title_zh': '训练后量化能否从额外集成的QLoRA中受益？'}
{'arxiv_id': 'arXiv:2502.10201', 'title': 'Prediction hubs are context-informed frequent tokens in LLMs', 'authors': 'Beatrix M. G. Nielsen, Iuri Macocco, Marco Baroni', 'link': 'https://arxiv.org/abs/2502.10201', 'abstract': 'Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.', 'abstract_zh': '高维空间中的聚众现象（hubness），指的是少量点成为其他大量点的近邻的概率不成比例地高，通常在应用标准的距离度量于高维数据时出现，往往会对基于距离的分析产生负面影响。由于自回归的大规模语言模型（LLMs）处理高维表示，我们质疑它们是否也会受到高维空间中的聚众现象影响。我们首先从理论上证明，LLMs执行的唯一表示比较操作——即根据上下文向量与未嵌入向量之间的比较来确定续写概率——并不表现出导致不良聚众现象的近距离度量集中现象。然后，我们通过实验展示了这种比较仍然会导致高聚众现象，但这些聚众点在此情况下并不构成干扰。相反，它们是语境调节下的高频词汇频繁出现在下一个词预测备选库中的结果。另一方面，当进行涉及LLMs表示的其他距离计算时，我们没有相同的理论保障，事实上，我们观察到不良的聚众现象出现。总之，我们的研究突出了如下方面：尽管聚众现象在高维空间中普遍存在，但它并不总是需要被抑制的负面属性，同时展示了各种广泛使用的LLMs发展了一种猜测策略，即持续地为高频词汇分配高概率。', 'title_zh': '预测枢纽是LLM中受到上下文指导的频繁令牌'}
{'arxiv_id': 'arXiv:2502.10140', 'title': 'Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages', 'authors': 'Daniil Gurgurov, Ivan Vykopal, Josef van Genabith, Simon Ostermann', 'link': 'https://arxiv.org/abs/2502.10140', 'abstract': 'Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.', 'abstract_zh': '低资源语言（LRLs）在自然语言处理（NLP）中面临着显著的数据限制挑战。尽管当前最先进的大型语言模型（LLMs）仍然难以处理LRLs，但规模较小的多语言模型（mLMs），如mBERT和XLM-R，因其容量与低训练数据量更匹配而显示出更大的前景。本研究系统地探讨了基于参数高效适配器的方法，以将mLMs应用于LRLs，评估了三种架构：序列瓶颈、可逆瓶颈和低秩适应。使用来自GlotCC的非结构化文本和来自ConceptNet的结构化知识，我们展示了小规模适应数据集（例如，最多1 GB的自由文本或数兆字节的知识图数据）在内在任务（蒙掩语言建模）和外在任务（主题分类、情感分析和命名实体识别）中的增益。我们的研究表明，序列瓶颈适配器在语言建模中表现出色，而可逆瓶颈适配器在其后端任务中略有优势，这得益于更好的嵌入对齐和更大的参数集。基于适配器的方法在参数数量远少于完整微调的情况下，能够匹敌或超过完整微调，并且对于LRLs来说，较小的mLMs比大规模LLMs（如LaLaMa-3、GPT-4和基于DeepSeek-R1的人工智能模型）更为有效。尽管适配提高了性能，但预训练数据集的大小仍然是主要影响因素，尤其是在预训练覆盖范围广泛的语言中。', 'title_zh': '小模型，大影响：面向低资源语言的小规模多语言语言模型的高效语料库和图基线适应方法'}
{'arxiv_id': 'arXiv:2502.10064', 'title': 'Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training', 'authors': 'Rodrigo Santos, António Branco, João Silva, João Rodrigues', 'link': 'https://arxiv.org/abs/2502.10064', 'abstract': 'Instruction-guided image editing consists in taking an image and an instruction and deliverring that image altered according to that instruction. State-of-the-art approaches to this task suffer from the typical scaling up and domain adaptation hindrances related to supervision as they eventually resort to some kind of task-specific labelling, masking or training. We propose a novel approach that does without any such task-specific supervision and offers thus a better potential for improvement. Its assessment demonstrates that it is highly effective, achieving very competitive performance.', 'abstract_zh': '指令引导的图像编辑是指采用一张图像和一条指令，根据该指令来修改图像。目前最先进的这类任务方法受到监督过程中常见的扩展性和领域适应性障碍的影响，最终不得不依赖某种特定任务的标记、掩模或训练。我们提出了一种新的方法，该方法无需任何特定任务的监督，因此为性能提升提供了更好的潜力。评估结果显示，该方法具有很高的有效性，达到了非常竞争力的性能。', 'title_zh': '无需干预的图像编辑：基于语言的编辑，无需任何特定任务的标注、掩码或甚至训练'}
{'arxiv_id': 'arXiv:2502.10061', 'title': 'Annotating Compositionality Scores for Irish Noun Compounds is Hard Work', 'authors': 'Abigail Walsh, Teresa Clifford, Emma Daly, Jane Dunne, Brian Davis, Gearóid Ó Cleircín', 'link': 'https://arxiv.org/abs/2502.10061', 'abstract': 'Noun compounds constitute a challenging construction for NLP applications, given their variability in idiomaticity and interpretation. In this paper, we present an analysis of compound nouns identified in Irish text of varied domains by expert annotators, focusing on compositionality as a key feature, but also domain specificity, as well as familiarity and confidence of the annotator giving the ratings. Our findings and the discussion that ensued contributes towards a greater understanding of how these constructions appear in Irish language, and how they might be treated separately from English noun compounds.', 'abstract_zh': '名词复合结构是NLP应用中的一个挑战性构词方式，鉴于它们在习语性和解释方面的变异性。在本文中，我们通过对专家标注者在不同领域的爱尔兰文本中识别出的名词复合结构进行分析，重点关注其组合性特征，同时也考虑了领域特异性、熟悉度以及标注者的置信度。我们的发现以及随之而来的讨论，有助于增进对这些结构在爱尔兰语中出现方式的理解，并探讨它们与英语名词复合结构是否应分开处理。', 'title_zh': '为爱尔兰名词复合词标注组合性分数是一项艰巨的工作'}
{'arxiv_id': 'arXiv:2502.10058', 'title': 'MTLM: an Innovative Language Model Training Paradigm for ASR', 'authors': 'Qingliang Meng, Pengju Ren, Tian Li, Changsong Dai', 'link': 'https://arxiv.org/abs/2502.10058', 'abstract': 'Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether n-best rescoring or shallow fusion is used as the decoding algorithm.', 'abstract_zh': '在大量文本上预训练基于变换器的语言模型（LMs）已被证明对于提高自动语音识别（ASR）性能至关重要。通常，传统LM是单向的，无法访问右侧的上下文。本文提出了一种方法，使传统单向LM能够充分利用左侧和右侧的上下文。与传统的单向LM相比，我们的LM有助于ASR在转录假设时更加一致且在语义上更具明确性，因为我们的LM整合了更丰富的上下文表示。最后，我们在LibriSpeech数据集上的实验结果表明，无论使用哪种解码算法（无论是n-best重新评分还是浅融合），我们的模型都优于传统的单向LM。', 'title_zh': 'MTLM：一种创新的ASR语言模型训练范式\n\n解释：\n- MTLM: 创新的缩写保持不变，直接使用大写形式。\n- Language Model Training Paradigm：语言模型训练范式\n- ASR：自动语音识别（Automated Speech Recognition）的缩写，保持不变。'}
{'arxiv_id': 'arXiv:2502.10051', 'title': 'ORI: O Routing Intelligence', 'authors': 'Ahmad Shadid, Rahul Kumar, Mohit Mayank', 'link': 'https://arxiv.org/abs/2502.10051', 'abstract': "Single large language models (LLMs) often fall short when faced with the ever-growing range of tasks, making a single-model approach insufficient. We address this challenge by proposing ORI (O Routing Intelligence), a dynamic framework that leverages a set of LLMs. By intelligently routing incoming queries to the most suitable model, ORI not only improves task-specific accuracy, but also maintains efficiency. Comprehensive evaluations across diverse benchmarks demonstrate consistent accuracy gains while controlling computational overhead. By intelligently routing queries, ORI outperforms the strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR, ties the top performance on ARC, and on BBH. These results underscore the benefits of a multi-model strategy and demonstrate how ORI's adaptive architecture can more effectively handle diverse tasks, offering a scalable, high-performance solution for a system of multiple large language models.", 'abstract_zh': '单个大规模语言模型（LLMs）在面对日益增长的任务范围时往往表现不足，使得单模型方法变得不够充分。为应对这一挑战，我们提出了一种名为ORI（O Routing Intelligence）的动态框架，该框架利用了一组LLMs。通过智能地将传入查询路由到最适合的模型，ORI 不仅提高了任务特定的准确性，还保持了效率。通过对各种基准的全面评估，结果显示，在控制计算开销的情况下，ORI 仍然能够实现一致的准确性提升。通过对查询的智能路由，ORI 在MMLU上的表现比最强的单个模型高出2.7个百分点，在MuSR上的表现高出1.8个百分点，在ARC和BBH上达到了顶级性能。这些结果强调了多模型策略的优势，并展示了ORI的自适应架构如何更有效地处理多样化的任务，提供了用于多大规模语言模型系统的可扩展且高性能的解决方案。', 'title_zh': 'ORI: 路由智能'}
{'arxiv_id': 'arXiv:2502.10013', 'title': 'Probabilistic Lexical Manifold Construction in Large Language Models via Hierarchical Vector Field Interpolation', 'authors': 'Clive Pendleton, Ewan Harrington, Giles Fairbrother, Jasper Arkwright, Nigel Fenwick, Richard Katrix', 'link': 'https://arxiv.org/abs/2502.10013', 'abstract': 'Hierarchical vector field interpolation introduces a structured probabilistic framework for lexical representation, ensuring that word embeddings transition smoothly across a continuous manifold rather than being constrained to discrete token mappings. The proposed methodology constructs a probabilistic function space where word representations adhere to topological consistency, mitigating representational discontinuities commonly observed in transformer-based embeddings. Empirical evaluations reveal that probabilistic constraints enhance lexical coherence by refining contextual relationships, leading to improvements in semantic stability across multiple linguistic distributions. The application of divergence minimization techniques ensures that interpolated embeddings maintain probabilistic consistency while preserving computational feasibility for large-scale implementations. Experimental findings demonstrate that interpolated lexical manifolds improve representation density alignment, reducing anisotropic distortions in contextual embedding distributions. Comparative analyses with standard transformer-based models highlight that structured interpolation yields more stable representations, particularly in tasks requiring fine-grained semantic differentiation. The statistical evaluation of embedding divergence confirms that probabilistic lexical manifolds reduce representational inconsistencies while maintaining coherence across varying scales of contextual abstraction. An assessment of computational efficiency reveals that while interpolation introduces minor processing overhead, the structured representation learning approach remains scalable for practical deployment.', 'abstract_zh': '层次向量场插值引入了一种结构化的概率框架，用于词表示，确保词嵌入在连续流形上平滑过渡，而不是局限于离散的标记映射。提出的方法构建了一个概率函数空间，使词表示遵循拓扑一致性，从而减轻了变压器基嵌入中常见的代表性不连续性。实证评估表明，概率约束通过细化语境关系增强了词汇连贯性，从而在多种语言分布中提高了语义稳定性。采用最小化分歧的技术确保插值嵌入保持概率一致性，同时保留大规模实现的计算可行性。实验结果表明，插值词汇流形提高了表示密度的对齐度，减少了语境嵌入分布中的各向异性失真。与其他标准变压器基模型的对比分析表明，结构化插值生成更稳定的表示，特别是在需要细微语义差异的任务中更为明显。嵌入分歧的统计评估证实，概率词汇流形减少了代表性不一致性，同时在不同层次的语境抽象中保持连贯性。计算效率评估表明，虽然插值引入了少量的处理开销，但结构化的表示学习方法仍可扩展以适应实际部署。', 'title_zh': '大型语言模型中基于分层向量场插值的概率词元流形构建方法'}
{'arxiv_id': 'arXiv:2502.10003', 'title': 'SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim Verification', 'authors': 'Sujit Kumar, Anshul Sharma, Siddharth Hemant Khincha, Gargi Shroff, Sanasam Ranbir Singh, Rahul Mishra', 'link': 'https://arxiv.org/abs/2502.10003', 'abstract': 'Verifying scientific claims presents a significantly greater challenge than verifying political or news-related claims. Unlike the relatively broad audience for political claims, the users of scientific claim verification systems can vary widely, ranging from researchers testing specific hypotheses to everyday users seeking information on a medication. Additionally, the evidence for scientific claims is often highly complex, involving technical terminology and intricate domain-specific concepts that require specialized models for accurate verification. Despite considerable interest from the research community, there is a noticeable lack of large-scale scientific claim verification datasets to benchmark and train effective models. To bridge this gap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num, derived from scientific research papers. We propose several baseline models tailored for scientific claim verification to assess the effectiveness of these datasets. Additionally, we evaluate models trained on SciClaimHunt and SciClaimHunt_Num against existing scientific claim verification datasets to gauge their quality and reliability. Furthermore, we conduct human evaluations of the claims in proposed datasets and perform error analysis to assess the effectiveness of the proposed baseline models. Our findings indicate that SciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for training models in scientific claim verification.', 'abstract_zh': '在验证科学声明时所面临的挑战远远大于验证政治或新闻声明时所面临的挑战。与政治声明相对宽泛的受众群体不同，科学声明验证系统的用户群体范围广泛，从测试特定假设的研究人员到寻求药品信息的普通用户。此外，科学声明的证据通常非常复杂，涉及技术术语和特定领域的复杂概念，这些都需要专门的模型才能实现准确验证。尽管研究界对此表现出极大兴趣，但仍缺乏大规模的科学声明验证数据集来评估和训练有效的模型。为弥补这一不足，我们提出了两个大规模数据集：SciClaimHunt和SciClaimHunt_Num，它们源自科学研究论文。我们还提出了几种专门针对科学声明验证的基线模型，以评估这些数据集的有效性。此外，我们还评估了在SciClaimHunt和SciClaimHunt_Num上训练的模型与现有科学声明验证数据集之间的性能，以衡量其质量和可靠性。我们还对提议数据集中的声明进行了人工评估，并进行了错误分析，以评估所提出的基线模型的有效性。我们的研究结果表明，SciClaimHunt和SciClaimHunt_Num是训练科学声明验证模型的可靠资源。', 'title_zh': 'SciClaimHunt：一个基于证据的科学研究声明验证大规模数据集'}
{'arxiv_id': 'arXiv:2502.10001', 'title': 'EmbBERT-Q: Breaking Memory Barriers in Embedded NLP', 'authors': 'Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri', 'link': 'https://arxiv.org/abs/2502.10001', 'abstract': 'Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25x reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at this https URL.', 'abstract_zh': '大型语言模型（LLMs）已经彻底革新了自然语言处理领域，在广泛的应用中设立了新的标准。然而，它们对内存和计算的高需求使得它们在技术受限的小型设备上（如可穿戴设备和物联网单元）的部署变得不切实际。为了解决这一限制，我们引入了EmbBERT-Q，这是一种专为严格内存约束的小型设备设计的新型小型语言模型。在这一场景下，EmbBERT-Q在自然语言处理任务中的准确率达到了当前最佳水平（SotA），其总内存占用（权重和激活值）仅为781 kB，相比现有模型大小减少了25倍。通过结合架构创新和兼容硬件的8位量化技术，EmbBERT-Q在2 MB内存预算（即小型设备中通常可用的最大内存）下，持续超越多个基线模型，其中包括高度压缩的BERT和MAMBA版本。我们针对特定基准数据集TinyNLP进行的实验评估，该数据集专门用于评估小型语言模型在自然语言处理任务和实际场景中的表现，以及GLUE基准测试，均证明EmbBERT-Q能够在内存和性能之间实现最优的平衡，提供与现有方法竞争的准确性。为了确保我们所有结果的完全和即时可再现性，我们在此公开了所有代码、脚本和模型检查点（链接：[提供链接]）。', 'title_zh': 'EmbBERT-Q：突破嵌入式NLP的内存障碍'}
{'arxiv_id': 'arXiv:2502.09992', 'title': 'Large Language Diffusion Models', 'authors': 'Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li', 'link': 'https://arxiv.org/abs/2502.09992', 'abstract': 'Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.', 'abstract_zh': '自回归模型（ARMs）长期以来被视为大规模语言模型（LLMs）的基础。我们提出了对这一观点的挑战，通过引入在预训练和监督微调（SFT）范式下从头开始训练的扩散模型——LLaDA。LLaDA 通过前向数据遮蔽过程和一个反向过程来建模分布，其中反向过程通过一个普通的Transformer来预测遮蔽的标记。通过优化似然性边界，它提供了一种用于概率推理的原则性生成方法。在广泛的基准测试中，LLaDA 显示出了强大的可扩展性，超越了我们自行构建的ARM基线。令人惊讶的是，LLaDA 8B在上下文学习任务中与强大的LLMs（如LLaMA3 8B）具有竞争力，并且经过SFT后，在案例研究中（如多轮对话）展示了出色的操作指令能力。此外，LLaDA 解决了逆向难题，在逆向诗歌生成任务中超越了GPT-4o。我们的研究结果确立了扩散模型作为一种可行且有前途的ARM替代方案，挑战了关键LLM能力上述特性与ARMs密不可分这一假设。', 'title_zh': '大型语言扩散模型'}
{'arxiv_id': 'arXiv:2502.09977', 'title': 'LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing', 'authors': 'Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng', 'link': 'https://arxiv.org/abs/2502.09977', 'abstract': "Effectively incorporating external knowledge into Large Language Models (LLMs) is crucial for enhancing their capabilities and addressing real-world needs. Retrieval-Augmented Generation (RAG) offers an effective method for achieving this by retrieving the most relevant fragments into LLMs. However, the advancements in context window size for LLMs offer an alternative approach, raising the question of whether RAG remains necessary for effectively handling external knowledge. Several existing studies provide inconclusive comparisons between RAG and long-context (LC) LLMs, largely due to limitations in the benchmark designs. In this paper, we present LaRA, a novel benchmark specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses 2,326 test cases across four practical QA task categories and three types of naturally occurring long texts. Through systematic evaluation of seven open-source and four proprietary LLMs, we find that the optimal choice between RAG and LC depends on a complex interplay of factors, including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. Our findings provide actionable guidelines for practitioners to effectively leverage both RAG and LC approaches in developing and deploying LLM applications. Our code and dataset is provided at: \\href{this https URL}{\\textbf{this https URL}}.", 'abstract_zh': '有效地将外部知识融入大型语言模型（LLMs）对于增强其能力和应对实际需求至关重要。检索增强生成（RAG）提供了一种有效的方法，通过检索最相关的片段来增强LLMs。然而，LLMs的上下文窗口大小的进展提供了一种替代方法，这引发了是否RAG对于有效处理外部知识仍然必要的疑问。现有的一些研究对RAG和长上下文（LC）LLMs之间的比较结果并不明确，这主要是由于基准设计的局限性。在本文中，我们提出了LaRA，这是一种新的基准工具，专门设计用于严格比较RAG和LC LLMs。LaRA涵盖了四大类实际问答任务的2,326个测试案例，以及三种类型的自然生成的长文本。通过系统评估七种开源和四种专有LLM，我们发现，RAG和LC之间的最优选择取决于一系列复杂因素的相互作用，包括模型的参数量、长文本处理能力、上下文长度、任务类型以及检索片段的特性。我们的研究结果为实践者提供了关于如何有效地在开发和部署LLM应用程序时利用RAG和LC方法的实用指南。我们的代码和数据集可在此处获取：\\href{this https URL}{此链接}。', 'title_zh': 'LaRA: 检验检索增强生成和长上下文语言模型 - 不存在适用于长上下文或检索增强生成路由的万能解决方案'}
{'arxiv_id': 'arXiv:2502.09956', 'title': 'KGGen: Extracting Knowledge Graphs from Plain Text with Language Models', 'authors': 'Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.09956', 'abstract': "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.", 'abstract_zh': '近年来，构建知识图谱（KG）的基础模型引起了广泛关注，这凸显了一个根本性的挑战：知识图谱数据相对稀缺。目前最著名的知识图谱主要由人工标注、模式匹配或早期自然语言处理（NLP）技术提取而成。虽然人工生成的知识图谱的数量有限，但自动提取的知识图谱质量存疑。我们提供了一种解决方案，即基于文本的知识图谱生成器（KGGen），这是一种利用语言模型从纯文本创建高质量图的软件包。与其它知识图谱提取工具不同，KGGen通过聚类相关实体来减少提取出的知识图谱的稀疏性。KGGen以Python库的形式提供（使用命令`\\texttt{pip install kg-gen}`安装），使所有人都能使用。此外，我们还发布了一个基准测试工具——节点和边的信息量度量（MINE），用于测试提取器从纯文本生成有用知识图谱的能力。我们将新工具与现有工具进行了基准测试，并展示了其显著优越的性能。', 'title_zh': 'KGGen：使用语言模型从纯文本中提取知识图谱'}
{'arxiv_id': 'arXiv:2502.09940', 'title': 'A Preliminary Exploration with GPT-4o Voice Mode', 'authors': 'Yu-Xiang Lin, Chih-Kai Yang, Wei-Chih Chen, Chen-An Li, Chien-yu Huang, Xuanjun Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2502.09940', 'abstract': "With the rise of multimodal large language models, GPT-4o stands out as a pioneering model, driving us to evaluate its capabilities. This report assesses GPT-4o across various tasks to analyze its audio processing and reasoning abilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and music understanding, performing well in tasks like intent classification, spoken command classification, semantic and grammatical reasoning., multilingual speech recognition, and singing analysis. It also shows greater robustness against hallucinations than other large audio-language models (LALMs). However, it struggles with tasks such as audio duration prediction and instrument classification. Additionally, GPT-4o's safety mechanisms cause it to decline tasks like speaker identification, age classification, MOS prediction, and audio deepfake detection. Notably, the model exhibits a significantly different refusal rate when responding to speaker verification tasks on different datasets. This is likely due to variations in the accompanying instructions or the quality of the input audio, suggesting the sensitivity of its built-in safeguards. Finally, we acknowledge that model performance varies with evaluation protocols. This report only serves as a preliminary exploration of the current state of LALMs.", 'abstract_zh': '随着多模态大语言模型的兴起，GPT-4o 凸显为一种开创性的模型，推动我们对其能力进行评估。本报告通过多种任务评估 GPT-4o，以分析其音频处理和推理能力。研究发现，GPT-4o 在音频、语音和音乐理解方面表现出强大的知识基础，能够在意图分类、口头命令分类、语义和语法推理、多语言语音识别和歌唱分析等任务中表现出色。此外，GPT-4o 在抗幻觉方面比其他大型音频语言模型（LALMs）更为稳健。然而，它在音频时长预测和乐器分类等任务中遇到困难。同时，GPT-4o 的安全机制导致它拒绝执行像说话人识别、年龄分类、MOS 预测和音频深度伪造检测等任务。值得注意的是，当模型在不同的数据集上响应说话人验证任务时，其拒绝率表现出显著差异。这可能与伴随说明或输入音频的质量变化有关，暗示其内置保护措施的高度敏感性。最后，我们意识到模型性能因评估协议而异。本报告仅作为对当前 LALMs 状态的初步探索。', 'title_zh': '初步探究：GPT-4o语音模式'}
{'arxiv_id': 'arXiv:2502.09854', 'title': 'Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning', 'authors': 'Yu-Chen Lin, Sanat Sharma, Hari Manikandan, Jayant Kumar, Tracy Holloway King, Jing Zheng', 'link': 'https://arxiv.org/abs/2502.09854', 'abstract': 'In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.', 'abstract_zh': '在本文中，我们证明了小型语言模型（SLMs），特别是具有1亿参数的GPT-2模型，在多任务提示生成任务中可以实现与大型语言模型（LLMs）相当的性能，但所需的计算资源却仅为后者的一小部分。通过将倒置强化学习与强大的LLM Llama-3生成的合成数据蒸馏相结合，我们训练了一个SLM，其相关性得分与包括Llama-3、Qwen2和Mistral在内的最先进的模型相差5%以内，尽管其规模最多可达前者的80倍，这使其非常适合资源受限和实时应用。本研究突显了SLMs作为多模态环境中的高效多任务学习者的能力，并为可扩展和低延迟部署提供了LLMs的有前途的替代方案。', 'title_zh': '通过倒置强化学习实现小语言模型中的高效多任务学习'}
{'arxiv_id': 'arXiv:2502.09815', 'title': 'Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence', 'authors': 'Jonathan Gale, Godfrey Aldington, Harriet Thistlewood, Thomas Tattershall, Basil Wentworth, Vincent Enoasmo', 'link': 'https://arxiv.org/abs/2502.09815', 'abstract': 'Representation learning plays a central role in structuring internal embeddings to capture the statistical properties of language, influencing the coherence and contextual consistency of generated text. Statistical Coherence Alignment is introduced as a method to enforce structured token representations through tensor field convergence, guiding embeddings to reflect statistical dependencies inherent in linguistic data. A mathematical framework is established to quantify coherence alignment, integrating a loss function that optimizes representational consistency across training iterations. Empirical evaluations demonstrate that applying coherence constraints improves perplexity, enhances classification accuracy, and refines rare word embeddings, contributing to a more stable representation space. Comparative analyses with baseline models reveal that the proposed method fosters a more interpretable internal structure, ensuring that embeddings retain contextual dependencies while mitigating representation collapse. The impact on coherence score distributions suggests that the alignment mechanism strengthens semantic integrity across diverse linguistic constructs, leading to a more balanced organization of learned embeddings. Computational assessments indicate that while the method introduces additional memory and training costs, the structured optimization process justifies the trade-offs in applications requiring heightened contextual fidelity. Experimental results validate the effectiveness of coherence alignment in optimizing token representations, providing insights into how statistical dependencies can be leveraged to improve language model training.', 'abstract_zh': '表征学习在构建内部嵌入结构中发挥着核心作用，以捕捉语言的统计特性，从而影响生成文本的一致性和上下文一致性。提出了统计一致性对齐方法，通过张量场收敛强制结构化标记表示，引导嵌入反映语言数据中固有的统计依赖关系。建立了一套数学框架来量化一致性对齐，整合了一个损失函数，以优化训练迭代中的表示一致性。实证评估表明，施加一致性的约束可以改善困惑度，提升分类准确性，并细化罕见词嵌入，从而构建一个更稳定的表现空间。与基线模型的比较分析表明，所提出的方法促进了更可解释的内部结构，并确保嵌入保留了上下文依赖关系，同时缓解了表示崩溃的问题。一致度得分分布的影响表明，对齐机制增强了不同语言构造中的语义完整性，导致学习嵌入的更平衡组织。计算评估表明，尽管该方法增加了额外的内存和训练成本，但结构化优化过程在需要高度语境保真的应用中具有合理性。实验结果验证了语义一致性对齐在优化标记表示方面的有效性，并为如何利用统计依赖关系改进语言模型训练提供了见解。', 'title_zh': '通过张量场收敛实现大型语言模型表示学习的统计一致性对齐'}
{'arxiv_id': 'arXiv:2502.09814', 'title': 'INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages', 'authors': 'Hao Yu, Jesujoba O. Alabi, Andiswa Bukula, Jian Yun Zhuang, En-Shiun Annie Lee, Tadesse Kebede Guge, Israel Abebe Azime, Happy Buzaaba, Blessing Kudzaishe Sibanda, Godson K. Kalipe, Jonathan Mukiibi, Salomon Kabongo Kabenamualu, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Juliet W. Murage, Dietrich Klakow, David Ifeoluwa Adelani', 'link': 'https://arxiv.org/abs/2502.09814', 'abstract': 'Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce Injongo -- a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls behind the fine-tuning baselines. Compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.', 'abstract_zh': '对话AI中的插槽填充和意图检测是已确立的任务。然而，当前大规模的基准测试往往不包括低资源语言的评估，并且依赖于英语基准的翻译，从而主要反映了西方中心的概念。在本文中，我们介绍了Injongo——一个多文化、开源的基准数据集，涵盖了16种非洲语言，并由各种领域的土著说法者生成的语句，包括银行、旅游、家居和餐饮。通过广泛的实验，我们针对多语言转换单词模型和提示大型语言模型（LLMs）进行了基准测试，并展示了利用非洲文化语句相对于西方中心语句在跨语言迁移方面的优势，从而提高了从英语语言中的表现。实验结果表明，目前的LLMs在插槽填充任务上表现困难，GPT-4o的平均F1分数为26分。相比之下，意图检测的表现明显更好，平均准确率为70.6%，尽管仍低于微调基线。与英语相比，GPT-4o和微调基线在意图检测方面表现相似，准确率为约81%。我们的发现表明，对于许多低资源非洲语言，LLMs的表现仍然落后，需要更多的工作来进一步提高它们的下游表现。', 'title_zh': 'INJONGO：一种适用于16种非洲语言的多文化意图检测和槽填充数据集'}
{'arxiv_id': 'arXiv:2502.09778', 'title': 'Prompt and circumstance: A word-by-word LLM prompting approach to interlinear glossing for low-resource languages', 'authors': 'Micha Elsner, David Liu', 'link': 'https://arxiv.org/abs/2502.09778', 'abstract': 'Partly automated creation of interlinear glossed text (IGT) has the potential to assist in linguistic documentation. We argue that LLMs can make this process more accessible to linguists because of their capacity to follow natural-language instructions. We investigate the effectiveness of a retrieval-based LLM prompting approach to glossing, applied to the seven languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based shared task baseline for every language in the morpheme-level score category, and we show that a simple 3-best oracle has higher word-level scores than the challenge winner (a tuned sequence model) in five languages. In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature. Our results thus demonstrate the potential contributions which LLMs can make in interactive systems for glossing, both in making suggestions to human annotators and following directions.', 'abstract_zh': '部分自动化创建分词标注文本（IGT）有望为语言记录提供帮助。我们认为，由于大规模语言模型（LLM）能够遵循自然语言指令，因此它们可以提高这一过程的可访问性。我们探讨了使用检索型LLM提示方法进行标注的有效性，并将其应用于SIGMORPHON 2023共享任务中的七种语言。我们的系统在每个形态学水平评分类别中都击败了基于BERT的共享任务基线；并且我们在五种语言中展示了简单的三最佳判准其单词水平评分高于挑战获胜者（调优序列模型）。在对捷斯语的研究案例中，我们让LLM自动创建和遵循语言指令，减少了对一个语法特征的误解错误。因此，我们的结果表明LLM在注释交互系统中的潜在贡献，无论是在向人类注释者提供建议，还是遵循方向方面。', 'title_zh': '提示与情境：一种面向低资源语言的逐词LLM提示方法及其逐词标注研究'}
{'arxiv_id': 'arXiv:2502.09747', 'title': 'The Widespread Adoption of Large Language Model-Assisted Writing Across Society', 'authors': 'Weixin Liang, Yaohui Zhang, Mihai Codreanu, Jiayu Wang, Hancheng Cao, James Zou', 'link': 'https://arxiv.org/abs/2502.09747', 'abstract': 'The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.', 'abstract_zh': '近年来，大型语言模型（LLMs）的进展引起了公众和决策者对其采用模式的高度关注。本文系统分析了从2022年1月到2024年9月期间，LLM辅助写作在四个领域的应用情况，包括消费者投诉、企业通讯、招聘广告和国际组织的新闻稿。我们的数据集包括687,241条消费者投诉、537,413篇企业新闻稿、3.043亿条招聘广告和15,919篇联合国（UN）新闻稿。通过稳健的总体统计框架，我们发现，随着2022年11月ChatGPT的发布，LLM的使用量显著增加。到2024年底，约有18%的金融消费者投诉文本可能带有LLM辅助的痕迹，并显示出广泛分布的特点，城市地区略高于农村地区。对于企业新闻稿而言，最多有24%的文本可归因于LLM的使用。在招聘广告方面，小企业中LLM辅助的写作用途占比接近10%，年轻企业则更为普遍。联合国新闻稿也反映了这一趋势，约有14%的内容是由LLM生成或修改的。尽管在ChatGPT发布后采用率迅速上升，但到2024年，增长似乎趋于稳定，这可能反映了LLM采用饱和或更高级模型的使用更加微妙。我们的研究展示了企业、消费者乃至国际组织在交流中对生成式AI依赖日益增强的新现实。', 'title_zh': '大型语言模型辅助写作在社会中的广泛采用'}
{'arxiv_id': 'arXiv:2502.09743', 'title': 'Partial Colexifications Improve Concept Embeddings', 'authors': 'Arne Rubehn, Johann-Mattis List', 'link': 'https://arxiv.org/abs/2502.09743', 'abstract': 'While the embedding of words has revolutionized the field of Natural Language Processing, the embedding of concepts has received much less attention so far. A dense and meaningful representation of concepts, however, could prove useful for several tasks in computational linguistics, especially those involving cross-linguistic data or sparse data from low resource languages. First methods that have been proposed so far embed concepts from automatically constructed colexification networks. While these approaches depart from automatically inferred polysemies, attested across a larger number of languages, they are restricted to the word level, ignoring lexical relations that would only hold for parts of the words in a given language. Building on recently introduced methods for the inference of partial colexifications, we show how they can be used to improve concept embeddings in meaningful ways. The learned embeddings are evaluated against lexical similarity ratings, recorded instances of semantic shift, and word association data. We show that in all evaluation tasks, the inclusion of partial colexifications lead to improved concept representations and better results. Our results further show that the learned embeddings are able to capture and represent different semantic relationships between concepts.', 'abstract_zh': '尽管词嵌入技术已经极大地革新了自然语言处理领域，概念嵌入方面目前仍受到了较少的关注。然而，富有密度和意义的概念表示对于计算语言学中的多种任务来说可能是有用的，尤其是涉及跨语言数据或低资源语言稀疏数据的任务。迄今为止，已经提出的第一批方法是通过自动构建的共现网络来嵌入概念。虽然这些方法依赖于跨多种语言确认的多义性，但它们仅限于词层面，而忽略了只在某种语言的部分词之间成立的词汇关系。基于最近引入的用于推断部分共现的方法，我们展示了如何利用这些方法以有意义的方式改进概念嵌入。通过将这些嵌入与词汇相似性评分、记录的语义转移实例以及词汇关联数据进行评估，我们证明了在所有评估任务中，引入部分共现可以提高概念表示并取得更好的结果。我们的进一步结果揭示，学习到的嵌入能够捕捉和表示概念之间不同的语义关系。', 'title_zh': '部分共现改进概念嵌入'}
{'arxiv_id': 'arXiv:2502.09741', 'title': 'FoNE: Precise Single-Token Number Embeddings via Fourier Features', 'authors': 'Tianyi Zhou, Deqing Fu, Mahdi Soltanolkotabi, Robin Jia, Vatsal Sharan', 'link': 'https://arxiv.org/abs/2502.09741', 'abstract': "Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64$\\times$ less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3$\\times$ and 6$\\times$ fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at this https URL.", 'abstract_zh': '以下是经过学术规范整理并翻译成中文的内容：\n\n大规模语言模型（LLMs）通常使用多个标记来表示数值，这要求模型将这些标记聚合起来以解释数值。这种分割使得训练和推理效率降低，并且对涉及数字的任务的模型性能产生负面影响。受预训练LLMs内部学习到类傅里叶特征的事实启发，我们提出了一种名为傅里叶数字嵌入（FoNE）的新方法，该方法直接将数字映射到具有傅里叶特征的嵌入空间中。FoNE 将每个数字编码为单个标记，并且每个数字的嵌入仅使用两个维度，从而有效捕捉数值而不进行分割。这种紧凑的表示形式加速了训练和推理。与传统的子词和按位嵌入方法相比，FoNE 不仅减少了计算开销，还在加法、减法和乘法等多种数值任务中实现了更高的准确性。在6位十进制加法任务中，FoNE 在达到99%准确率所需的训练数据量仅为子词和按位嵌入方法的1/64，而在每个多数字位上使用的标记数分别减少到原来的1/3和1/6。此外，FoNE 是唯一能够在超过10万个测试示例中对加法、减法和乘法均达到100%准确率的方法。源代码和可视化可以在以下链接访问：this https URL。', 'title_zh': 'FoNE：通过傅里叶特征实现精确的单词数嵌入'}
{'arxiv_id': 'arXiv:2502.09690', 'title': 'Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes', 'authors': 'Taylan G. Topcu, Mohammed Husain, Max Ofsa, Paul Wach', 'link': 'https://arxiv.org/abs/2502.09690', 'abstract': 'Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare AI generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the-art algorithms cannot differentiate AI-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, AI generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting AI suggested feedback, at least when generated by multi-purpose LLMs.', 'abstract_zh': '多用途大型语言模型（LLMs），作为生成性人工智能（AI）的一个子类，最近取得了显著进展。虽然人们期望LLMs能够在系统工程（SE）任务中发挥重要作用，但系统的跨学科复杂性以及需要综合深厚的领域知识和操作环境，使得人们对LLMs生成SE制品的有效性产生了疑问，特别是考虑到它们的训练数据绝大部分来自互联网。为了解决这一问题，我们进行了一项经验性探索，将由人类专家生成的SE制品作为基准，进行解析，并通过提示工程将这些制品输入到不同的LLMs中，以生成典型的SE制品片段。这一过程不进行任何微调或校准，以记录基线LLM性能。然后我们采用两折混合方法来对比AI生成的制品与基准之间的差异。首先，我们使用自然语言处理算法定量比较这些制品，发现当精心设计提示时，最先进的算法无法将AI生成的制品与人类专家基准区分开来。其次，我们进行深入定性分析，探讨它们在质量上的差异。我们记录发现，虽然两个材料看起来非常相似，但AI生成的制品表现出一些难以检测的重大失效模式，包括：过早定义需求、缺乏支持的数值估计以及过度详细描述的倾向。我们认为，这项研究揭示了系统工程界在采纳多用途LLMs生成的AI建议反馈时应更加谨慎的原因。', 'title_zh': '自担风险的信任：大型语言模型生成专家级系统工程制品能力的混合方法探索及失败模式 characterization'}
{'arxiv_id': 'arXiv:2502.09689', 'title': 'Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories', 'authors': 'Tomas Peterka, Matyas Bohacek', 'link': 'https://arxiv.org/abs/2502.09689', 'abstract': "The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface.", 'abstract_zh': '最有效的虚假信息传播往往是多模态的，通常会将文本与上下文无关或完全伪造的图片和视频结合起来，以支持特定的叙述。当前用于检测虚假信息的方法，无论是深度换脸还是文本文章，往往未能捕捉到多个模态之间的相互作用。本文提出了一种基于大规模语言模型的系统，旨在应对这些挑战。该系统分析文章的文本内容及其包含的图片和视频的来源元数据，以确定它们的相关性。我们开源了该系统的原型以及互动网页界面。', 'title_zh': '大数据模型与来源元数据在新闻故事中确定图像和视频的相关性中的应用'}
{'arxiv_id': 'arXiv:2502.09687', 'title': 'Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models', 'authors': 'Wiktoria Mieleszczenko-Kowszewicz, Beata Bajcar, Jolanta Babiak, Berenika Dyczek, Jakub Świstak, Przemysław Biecek', 'link': 'https://arxiv.org/abs/2502.09687', 'abstract': 'Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.', 'abstract_zh': '如您所愿，但请务必小心。这句谚语同样适用于大型语言模型（LLMs）的训练方式，它们不再因正确性而获得奖励，而是越来越因取悦接收者而获得奖励。因此，它们在说服我们相信其答案的价值方面变得越来越有效。但它们在这一说服过程中使用了哪些技巧？在本研究中，我们分析了十二种不同语言模型的回应中所使用的心理语言学特征。通过根据理性和情绪提示对回应内容进行分组，并考察LLMs所采用的社会影响原则，我们探讨是否以及如何减轻由LLM驱动的大规模误导信息的风险。我们将本研究放置在更广泛的人本中心AI讨论中，强调需要采用跨学科的方法来缓解具有说服力的AI回应所带来的心智和社会风险。', 'title_zh': '谨言慎语：大型语言模型在说服中的情感与理性面相'}
{'arxiv_id': 'arXiv:2502.09675', 'title': 'Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis', 'authors': 'Yubo Gao, Haotian Wu, Lei Zhang', 'link': 'https://arxiv.org/abs/2502.09675', 'abstract': 'Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN.', 'abstract_zh': '多模态情感分析（Multimodal Sentiment Analysis, MSA）旨在通过利用文本、音频和视觉等多种模态信息来识别人类情绪，因此如何充分利用不同模态之间的交互是MSA的核心挑战之一。交互包含对齐和冲突两个方面。当前的研究主要强调对齐以及单模态之间固有的差异，而忽视了双模态组合之间也可能存在潜在冲突这一事实。此外，基于多任务学习的冲突建模方法往往依赖于不稳定生成的标签。为了解决这些问题，我们提出了一种新颖的多层级冲突感知网络（Multi-Level Conflict-Aware Network, MCAN），该网络逐步从单模态和双模态表示中分离对齐和冲突成分，并进一步通过冲突建模分支利用冲突成分。在冲突建模分支中，我们在表示和预测输出两个层面都进行了差异约束，避免了对生成标签的依赖。在CMU-MOSI和CMU-MOSEI数据集上的实验结果证明了所提出的MCAN的有效性。', 'title_zh': '多层冲突感知网络在多模态情感分析中的应用'}
{'arxiv_id': 'arXiv:2502.09674', 'title': 'The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis', 'authors': 'Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, Xiaohua Jia', 'link': 'https://arxiv.org/abs/2502.09674', 'abstract': "Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at this https URL.", 'abstract_zh': '大规模语言模型的安全对齐行为，如拒绝有害查询，可以通过激活空间中的线性方向来表示。以往的研究使用单一方向来建模安全行为，这限制了对其机制理解的单一安全特性。在本研究中，我们发现安全对齐行为实际上是由多维度的方向共同控制的。具体来说，我们研究了在 Llama 3 8B 上进行拒绝对抗攻击调整期间表示变化的向量空间。通过研究这种空间中的正交方向，我们发现一个主导方向主导了模型的拒绝行为，而多个较小的方向则代表了独特的可解释特征，如假设性叙述和角色扮演。随后，我们测量不同方向如何促进或抑制主导方向，展示了次要方向在塑造模型拒绝表示中的重要作用。最后，我们证明去除有害查询中的某些触发词可以缓解这些方向，使其能够绕过学习到的安全能力，从而从多维度角度为理解安全对齐脆弱性提供了新的见解。相关代码和资料可在以下链接获取：this https URL。', 'title_zh': 'LLM对齐的隐含维度：多维度安全性分析'}
{'arxiv_id': 'arXiv:2502.09673', 'title': 'Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning', 'authors': 'Ang Li, Yichuan Mo, Mingjie Li, Yifei Wang, Yisen Wang', 'link': 'https://arxiv.org/abs/2502.09673', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.', 'abstract_zh': '大型语言模型（LLMs）已在各种自然语言处理（NLP）基准测试中取得了显著的成功。然而，在执行要求精细推理和精确决策的复杂任务时，光有语言能力是不够的—LLMs 必须能够进行推理，即逻辑思考、借鉴过往经验并综合信息来得出结论并采取行动。为了增强推理能力，人们广泛探索了提示和微调等方法。尽管这些方法在提高推理能力方面取得了明显进步，但它们对LLMs安全性的潜在影响仍不甚明了。在本研究中，我们探讨了推理与安全性在LLMs中的相互作用。我们强调了随着推理能力的提升而产生的潜在安全风险，揭示了以往未被重视的漏洞。同时，我们还探讨了如何利用推理本身来增强安全性，发现潜在的缓解策略。通过研究推理驱动的LLMs安全性中的风险与机遇，本研究为开发出不仅更具能力而且在实际部署中更具可信度的模型提供了宝贵见解。', 'title_zh': '更聪明的大型语言模型更安全吗？探索提示与微调中的安全推理权衡'}
{'arxiv_id': 'arXiv:2502.09670', 'title': 'The Science of Evaluating Foundation Models', 'authors': 'Jiayi Yuan, Jiamu Zhang, Andrew Wen, Xia Hu', 'link': 'https://arxiv.org/abs/2502.09670', 'abstract': 'The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.', 'abstract_zh': '大型基础模型 emergent 现象已经极大地改变了自然语言处理领域。然而，由于这些模型的规模、能力以及在多种应用中的部署，评估它们面临着巨大的挑战。现有文献通常侧重于个别方面，如基准性能或特定任务，但未能提供一个能够将多样应用场景中的细微差别与更广泛的伦理和操作考量相结合的整合过程。本研究侧重于三个关键方面：(1) 通过为特定应用场景提供结构化的框架来形式化评估过程，(2) 提供可操作的工具和框架，如清单和模板，以确保评估的全面性、可重复性和实用性，并(3) 对 LLM 评估领域的最新进展进行综合审查，强调其实用应用。', 'title_zh': '评估基础模型的科学方法'}
{'arxiv_id': 'arXiv:2502.09667', 'title': 'k-LLMmeans: Summaries as Centroids for Interpretable and Scalable LLM-Based Text Clustering', 'authors': 'Jairo Diaz-Rodriguez', 'link': 'https://arxiv.org/abs/2502.09667', 'abstract': 'We introduce k-LLMmeans, a novel modification of the k-means clustering algorithm that utilizes LLMs to generate textual summaries as cluster centroids, thereby capturing contextual and semantic nuances often lost when relying on purely numerical means of document embeddings. This modification preserves the properties of k-means while offering greater interpretability: the cluster centroid is represented by an LLM-generated summary, whose embedding guides cluster assignments. We also propose a mini-batch variant, enabling efficient online clustering for streaming text data and providing real-time interpretability of evolving cluster centroids. Through extensive simulations, we show that our methods outperform vanilla k-means on multiple metrics while incurring only modest LLM usage that does not scale with dataset size. Finally, We present a case study showcasing the interpretability of evolving cluster centroids in sequential text streams. As part of our evaluation, we compile a new dataset from StackExchange, offering a benchmark for text-stream clustering.', 'abstract_zh': '我们将介绍 k-LLMmeans，这是一种对 k-means 聚类算法的新型改进，它利用大型语言模型（LLMs）生成文本摘要作为聚类中心，从而捕捉文档嵌入方法中通常丢失的上下文和语义细微差别。这一改进保留了 k-means 的属性，同时提供了更高的可解释性：聚类中心由 LLM 生成的摘要表示，其嵌入指导聚类分配。我们还提出了一种基于小批量的变体，使其能够高效地对流式文本数据进行在线聚类，并提供实时聚类中心进化的可解释性。通过广泛的模拟，我们展示了我们的方法在多个指标上优于传统的 k-means，且所使用的 LLM 资源仅适度增加，不会随数据集大小成比例增长。最后，我们通过一个案例研究展示了流式文本序列中聚类中心演化结果的可解释性。作为评估的一部分，我们从 StackExchange 编制了一个新数据集，为文本流聚类提供了一个基准。', 'title_zh': 'k-LLMmeans：基于摘要的可解释和可扩展的大语言模型文本聚类方法'}
{'arxiv_id': 'arXiv:2502.09659', 'title': 'Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models', 'authors': 'Hasin Rehana, Jie Zheng, Leo Yeh, Benu Bansal, Nur Bengisu Çam, Christianah Jemiyo, Brett McGregor, Arzucan Özgür, Yongqun He, Junguk Hur', 'link': 'https://arxiv.org/abs/2502.09659', 'abstract': "Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.", 'abstract_zh': '动机：佐剂是添加到疫苗中的一种化学物质，它可以增强疫苗的效果，通过改善免疫反应。从不断扩展的生物医学文献中识别癌症疫苗研究中的佐剂名称对进一步的研究和增强免疫疗法具有重要意义。然而，手动从这些文献中进行筛选存在显著的挑战。本研究探索了使用大规模语言模型（LLMs）自动识别疫苗佐剂名称的方法，具体使用了生成预训练变换器（GPT）和大规模语言模型Meta AI（Llama）。\n\n方法：我们利用了两个数据集：来自AdjuvareDB的97项临床试验记录和来自Vaccine Adjuvant Compendium (VAC)的290篇带有注释的摘要。我们使用了零样本和少量样本学习范式，其中GPT-4o和Llama 3.2每提示最多使用四个示例。提示特别针对佐剂名称，测试了物质或干预措施等上下文信息的影响。输出结果进行了自动和手动验证以确保准确性和一致性。\n\n结果：GPT-4o在所有情况下都达到了100%的精确度，同时在召回率和F1分数方面表现出显著改善，特别是加入了干预措施的情况下。在VAC数据集中，GPT-4o在加入干预措施时的最大F1分数为77.32%，超过了Llama-3.2-3B约2%。在AdjuvareDB数据集中，对于使用干预措施的三种提示，GPT-4o达到了81.67%的F1分数，超过了Llama-3.2-3B的最大F1分数65.62%。\n\n结论：我们的研究结果表明，LLMs在识别佐剂名称方面表现出色，包括识别罕见的命名变体。本研究强调了LLMs在通过高效提取见解来增强癌症疫苗开发方面的潜力。未来的研究方向将致力于扩展框架以涵盖各种生物医学文献，并增强模型在不同疫苗和佐剂方面的普适性。', 'title_zh': '使用大型语言模型从生物医学文献中识别癌症疫苗辅助剂名称'}
{'arxiv_id': 'arXiv:2502.09658', 'title': 'Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality', 'authors': 'Xin Kang, Veronika Shteingardt, Yuhan Wang, Dov Dori', 'link': 'https://arxiv.org/abs/2502.09658', 'abstract': 'Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.', 'abstract_zh': '知识表示与推理是人工智能（AI）中的关键挑战，特别是在将神经和符号方法结合以实现可解释和透明的AI系统方面。传统的知识表示方法往往难以捕捉复杂的过程和状态变化。我们介绍了神经概念AI（NCAI），这是一种神经符号AI方法的专门化，结合了使用ISO 19450:2024的物体-过程方法（OPM）的概念建模与深度学习，以提高问答（QA）质量。通过利用上下文学习将自然语言文本转换为OPM模型，NCAI利用OPM的表达能力来表示复杂的OPM元素——过程、对象和状态，这超出了传统三元组知识图谱的捕捉能力。这种丰富的结构化知识表示有助于增强OPM-QA系统中的推理透明度和答案准确性。我们还提出了透明度评估指标，以定量衡量预测推理与基于OPM的概念逻辑的一致性程度。实验结果表明，NCAI优于传统方法，突显了其在提供丰富知识表示、可测量的透明度和改进推理方面对神经符号AI的潜力。', 'title_zh': '神经概念人工智能：将OPM与深度学习相结合以提高问答质量'}
{'arxiv_id': 'arXiv:2502.09651', 'title': 'AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions', 'authors': 'Paul Mithun, Enrique Noriega-Atala, Nirav Merchant, Edwin Skidmore', 'link': 'https://arxiv.org/abs/2502.09651', 'abstract': 'We present AI-VERDE, a unified LLM-as-a-platform service designed to facilitate seamless integration of commercial, cloud-hosted, and on-premise open LLMs in academic settings. AI-VERDE streamlines access management for instructional and research groups by providing features such as robust access control, privacy-preserving mechanisms, native Retrieval-Augmented Generation (RAG) support, budget management for third-party LLM services, and both a conversational web interface and API access. In a pilot deployment at a large public university, AI-VERDE demonstrated significant engagement across diverse educational and research groups, enabling activities that would typically require substantial budgets for commercial LLM services with limited user and team management capabilities. To the best of our knowledge, AI-Verde is the first platform to address both academic and research needs for LLMs within an higher education institutional framework.', 'abstract_zh': '我们将介绍AI-VERDE，这是一种统一的基于大语言模型（LLM）的平台服务，旨在促进商业、云托管以及本地开放LLM在学术环境中的无缝集成。AI-VERDE通过提供诸如强大的访问控制、隐私保护机制、原生的检索增强生成（RAG）支持、第三方LLM服务的预算管理以及会话式的Web界面和API访问等功能，简化了教学和研究团队的访问管理。在一所大型公立大学的试点部署中，AI-VERDE展示了在各类教育和研究团体中显著的参与度，使得原本需要大量预算的商业LLM服务下的活动变得可行，并且具有有限用户和团队管理功能。据我们所知，AI-VERDE是首款在高等教育机构框架内同时满足学术研究和研究需求的平台。', 'title_zh': 'AI-VERDE：一种面向教育机构的大语言模型资源平等访问网关'}
{'arxiv_id': 'arXiv:2502.09650', 'title': 'Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples', 'authors': 'Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu', 'link': 'https://arxiv.org/abs/2502.09650', 'abstract': "The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）的对齐工作往往假设使用更多清洁数据能获得更好的结果，而忽视了模型容量与示例难度之间的匹配。针对这一假设，我们提出了一项新的原则：偏好数据在难度上存在差异，过于困难的示例会妨碍对齐，因为它们超出了模型的容量。通过系统的实验，我们验证了这一原则，并发现了三个关键发现：（1）偏好示例在难度上存在差异，这在对齐运行中的一致学习顺序中得到了证实；（2）过于困难的示例显著降低了四款LLM和两款数据集上的性能；（3）模型的容量决定了它处理困难示例的能力界限，强调了数据选择与模型容量之间关键的关系。基于这一原则，我们引入了选择性DPO（Selective DPO），该方法筛选掉过于困难的示例。这一简单的调整在AlpacaEval 2基准测试中提高了对齐性能，与基线DPO相比提高了9-16%的胜率，并超越了采用不同算法调整的一系列DPO变体。这些结果阐明了将数据难度与模型容量相匹配的重要性，为改进LLMs对齐策略提供了革命性的视角。相关代码可在以下链接获取：[链接]。', 'title_zh': 'principled数据选择用于对齐：困难示例的隐含风险'}
{'arxiv_id': 'arXiv:2502.09648', 'title': 'UKTA: Unified Korean Text Analyzer', 'authors': 'Seokho Ahn, Junhyung Park, Ganghee Go, Chulhui Kim, Jiho Jung, Myung Sun Shin, Do-Guk Kim, Young-Duk Seo', 'link': 'https://arxiv.org/abs/2502.09648', 'abstract': 'Evaluating writing quality is complex and time-consuming often delaying feedback to learners. While automated writing evaluation tools are effective for English, Korean automated writing evaluation tools face challenges due to their inability to address multi-view analysis, error propagation, and evaluation explainability. To overcome these challenges, we introduce UKTA (Unified Korean Text Analyzer), a comprehensive Korea text analysis and writing evaluation system. UKTA provides accurate low-level morpheme analysis, key lexical features for mid-level explainability, and transparent high-level rubric-based writing scores. Our approach enhances accuracy and quadratic weighted kappa over existing baseline, positioning UKTA as a leading multi-perspective tool for Korean text analysis and writing evaluation.', 'abstract_zh': '评估写作质量是一项复杂且耗时的过程，常常导致对学习者反馈的延迟。虽然自动写作评估工具在英语中非常有效，但韩语的自动写作评估工具由于无法解决多视角分析、错误传播和评估解释性等方面的问题而面临挑战。为克服这些挑战，我们推出了UKTA（统一韩语文本分析器），这是一个全面的韩语文本分析和写作评估系统。UKTA 提供了精准的低级形态分析、中级解释性关键词特征，以及透明的基于评分标准的高级写作分数。我们的方法在准确性以及二次加权 kappa 值上超过了现有基准，使 UKTA 成为韩语文本分析和写作评估多视角工具的领先选择。', 'title_zh': 'UKTA：统一的韩文文本分析器'}
{'arxiv_id': 'arXiv:2502.09647', 'title': 'Unveiling Simplicities of Attention: Adaptive Long-Context Head Identification', 'authors': 'Konstantin Donhauser, Charles Arnal, Mohammad Pezeshki, Vivien Cabannes, David Lopez-Paz, Kartik Ahuja', 'link': 'https://arxiv.org/abs/2502.09647', 'abstract': "The ability to process long contexts is crucial for many natural language processing tasks, yet it remains a significant challenge. While substantial progress has been made in enhancing the efficiency of attention mechanisms, there is still a gap in understanding how attention heads function in long-context settings. In this paper, we observe that while certain heads consistently attend to local information only, others swing between attending to local and long-context information depending on the query. This raises the question: can we identify which heads require long-context information to predict the next token accurately? We demonstrate that it's possible to predict which heads are crucial for long-context processing using only local keys. The core idea here is to exploit a simple model for the long-context scores via second moment approximations. These findings unveil simple properties of attention in the context of long sequences, and open the door to potentially significant gains in efficiency.", 'abstract_zh': '处理长上下文的能力对于许多自然语言处理任务至关重要，但仍是一个重要挑战。尽管在提高注意力机制效率方面已经取得显著进展，但在长上下文设置中注意力头的工作方式上仍存在理解差距。在本文中，我们观察到，某些注意力头始终只关注局部信息，而其他注意力头则根据查询的不同，在关注局部信息和长上下文信息之间切换。这引发了以下问题：我们能否识别哪些注意力头需要长上下文信息才能准确预测下一个词？我们证明，仅使用局部键即可预测哪些注意力头对于长上下文处理至关重要。核心思想是通过二阶矩近似来利用一个简单的模型来计算长上下文得分。这些发现揭示了长序列上下文中注意力的简单性质，并为可能的重要效率提升打开了大门。', 'title_zh': '揭示注意力机制的simplicities：自适应长上下文头部识别'}
{'arxiv_id': 'arXiv:2502.09646', 'title': 'Language Shift or Maintenance? An Intergenerational Study of the Tibetan Community in Saudi Arabia', 'authors': 'Sumaiyah Turkistani Mohammad Almoaily', 'link': 'https://arxiv.org/abs/2502.09646', 'abstract': 'The present study provides the first-ever report on the language shift from Tibetan to Arabic among descendants of Tibetan families who migrated from the Tibet region to Saudi Arabia around 70 years ago. The aim of this study was to determine whether three age groups had adopted different practices in terms of maintaining Tibetan or shifting to Hijazi Arabic. To this end, 96 male and female members of the Tibetan community responded to a questionnaire in which they were asked about their code choice in different domains (home, neighbourhood, friends and relatives, expressing emotion, and performing religious rituals). The data revealed significant intergenerational differences between members of the community in terms of the extent of the shift to Arabic, with Tibetan rarely used by younger members and older members making only slightly more use of it. The difference between the three age groups was significant, at a p-value of .001.', 'abstract_zh': '本研究提供了有关70年前从西藏地区迁移到沙特阿拉伯的藏族家庭后代中藏语向海地阿拉伯语转变的首次报告。本研究旨在确定三个年龄段是否采用了不同的策略来保持藏语或转向海地阿拉伯语。为此，研究对藏族社区的96名男性和女性成员进行了一项问卷调查，询问他们在不同领域（家庭、邻里、朋友和亲戚、表达情感和进行宗教仪式）中的代码选择情况。数据分析结果显示，不同世代间在使用藏语向阿拉伯语转变程度上存在显著差异，年轻成员几乎不使用藏语，而老年成员则仅略显增加使用。三个年龄组之间的差异在p值为0.001的情况下具有显著性。', 'title_zh': '语言转换还是保持？沙特阿拉伯藏人社区的代际研究'}
{'arxiv_id': 'arXiv:2502.09645', 'title': 'From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models', 'authors': 'Mayank Vatsa, Aparna Bharati, Surbhi Mittal, Richa Singh', 'link': 'https://arxiv.org/abs/2502.09645', 'abstract': 'Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.', 'abstract_zh': '否定，作为一种表达缺席、拒绝或矛盾的语法构造，对多语言多模态基础模型构成了重大的挑战。这些模型在机器翻译、文本引导生成、图像字幕、音频交互和视频处理等任务中表现出色，但在准确解释不同语言和文化背景下的否定方面往往存在困难。在这篇观点文章中，我们提出了一种全面的否定构造分类体系，展示了结构、语义和文化因素如何影响多模态基础模型。我们提出了开放的研究问题，并强调了解决这些问题的重要性，以实现更稳健的否定处理。最后，我们呼吁建立专门的基准测试、语言特定的分词、细粒度的注意力机制和先进的多模态架构。这些策略可以促进更具适应性和语义精确性的多模态基础模型的发展，使其能够更好地应对多语言、多模态环境中的否定复杂性。', 'title_zh': '从不知到知：多模态基础模型中否定理解的分类、挑战与机遇'}
{'arxiv_id': 'arXiv:2502.09644', 'title': 'From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis', 'authors': 'Moritz Plenz, Philipp Heinisch, Janosch Gehring, Philipp Cimiano, Anette Frank', 'link': 'https://arxiv.org/abs/2502.09644', 'abstract': "Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer's stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.", 'abstract_zh': '就存在冲突的问题进行辩论是解决冲突的必要第一步。然而，论辩者固有的视角往往难以通过说服性论述技巧来克服。从辩论过渡到经过深思熟虑的决策过程，我们需要识别解决冲突的实际可行方案，这需要对论点及其背后的角度进行更深入的分析——只有通过这种方式，我们才能从中推导出双方都能接受的解决方案。在本研究中，我们提出了一种计算论辩框架，用于对论辩中的论点进行协商分析。我们对不同论辩者或利益相关者所表达的针对同一议题的视角化立场进行了细致分析，不仅旨在识别其对立的观点，还识别由其态度、价值观或需求所产生的共同视角。我们将这种分析形式化为视角化立场向量，这些向量表征了所有论辩者在某一议题上的个体视角化立场。我们通过确定议题和论点特定的概念来构建这些向量，预测论辩者对每个概念的立场。向量使我们能够测量论辩者之间受视角调节的（不）一致性，从而有助于识别解决冲突的具体措施，作为迈向协商的第一步。', 'title_zh': '从论述到协商：基于视角的细微分歧认同分析中的立场向量'}
{'arxiv_id': 'arXiv:2502.09642', 'title': 'Krutrim LLM: Multilingual Foundational Model for over a Billion People', 'authors': 'Aditya Kallappa, Palash Kamble, Abhinav Ravi, Akshat Patidar, Vinayak Dhruv, Deepak Kumar, Raghav Awasthi, Arveti Manjunath, Shubham Agarwal, Kumar Ashish, Gautam Bhargava, Chandra Khatri', 'link': 'https://arxiv.org/abs/2502.09642', 'abstract': "India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data.\nWe introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts.\nKrutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models.", 'abstract_zh': '印度是一个多元化的社会， developing AI系统时面临独特的挑战，包括语言多样性、口述传统、数据获取和可扩展性。现有的基础模型主要在英语数据上进行训练，限制了这些模型在印度人口中的有效性。印度语系语言仅占Crawl公共语料库的1%，尽管印度占全球人口的18%，这导致了语言偏差。数千种地方语言、方言和代码混合使得由于缺乏训练数据而增加了表示难度。\n\n我们提出了Krutrim LLM，这是一个针对印度语言景观设计的2万亿词多语言模型。该模型整合了迄今为止已知最大的印度语系语料库，缓解了数据稀缺问题，并确保在各种方言上的均衡性能。Krutrim在印地语基准测试上表现出色或与其相当，同时保持了与英语竞争水平的性能。尽管在训练FLOPs上显著较小，Krutrim LLM在16项任务中有10项上与LLAMA-2相当，平均得分为0.57对0.55。这证明了Krutrim在不同语言背景下具有灵活的多语言流畅性。\n\n此外，Krutrim集成了实时搜索功能，提高了对话AI应用中的事实准确性，为全球超过10亿用户提供便利。通过针对数据不平衡做出的故意设计选择，Krutrim LLM标志着朝着构建公平、具有全球代表性的AI模型方面取得了有意义的进步。', 'title_zh': 'Krutrim LLM：为超过十亿人的多语言基础模型'}
{'arxiv_id': 'arXiv:2502.09640', 'title': 'Online Social Support Detection in Spanish Social Media Texts', 'authors': 'Moein Shahiki Tash, Luis Ramos, Zahra Ahani, Raul Monroy, Olga kolesnikova, Hiram Calvo, Grigori Sidorov', 'link': 'https://arxiv.org/abs/2502.09640', 'abstract': 'The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.', 'abstract_zh': '社交媒体的兴起已从根本上改变了沟通方式，使个人能够分享经历、寻求支持并参与各种讨论。尽管已有大量研究专注于识别有害内容，如仇恨言论，但在识别和促进积极和支持性互动方面的工作却较少。本文提出了一种创新的方法，旨在检测西班牙语社交媒体文本中的在线社会支持。我们首次创建了一个专门用于此任务的注释数据集，其中包括3,189条YouTube评论，被分类为支持性或非支持性。为了应对数据不平衡的问题，我们采用了GPT-4o来生成重新表述的评论，从而创建了平衡的数据集。随后，我们使用传统的机器学习模型、深度学习架构以及基于变换器的模型（包括GPT-4o）对不平衡数据集进行了社会支持分类的评估。之后，我们利用变换器模型比较了平衡数据集和不平衡数据集之间的性能。研究结果显示，平衡数据集在任务2（个体和群体）和任务3（国家、其他群体、LGBTQ、黑人群体、女性、宗教）上表现更佳，而GPT-4o则在任务1（社会支持与非支持）中表现最佳。本研究强调了创造支持性在线环境的重要性，并为未来自动化社会支持检测的研究奠定了基础。', 'title_zh': '西班牙语社交媒体文本中的在线社会支持检测'}
{'arxiv_id': 'arXiv:2502.09638', 'title': 'Jailbreaking to Jailbreak', 'authors': 'Jeremy Kritz, Vaughn Robinson, Robert Vacareanu, Bijan Varjavand, Michael Choi, Bobby Gogov, Scale Red Team, Summer Yue, Willow E. Primack, Zifan Wang', 'link': 'https://arxiv.org/abs/2502.09638', 'abstract': 'Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with $J_2$, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.', 'abstract_zh': '基于大型语言模型（LLM）的拒绝训练可以防止有害输出，但这种防御仍然容易受到自动化和人工构建的“逃逸”攻击的攻击。我们提出了一种新的LLM作为红队成员的方法，在这种方法中，一个人类通过“逃逸”一个拒绝训练的LLM，使其愿意自我“逃逸”或对其他LLM进行“逃逸”。我们称这些被“逃逸”的LLM为$J_2$攻击者，这些攻击者可以通过多种红队策略系统地评估目标模型，并通过从之前的失败中进行上下文学习来提高其性能。我们的实验表明，Sonnet 3.5和Gemini 1.5作为$J_2$攻击者表现出色，分别在Harmbench上对GPT-4o（以及其他能力相当的LLM）取得了93.0%和91.0%的成功攻击率（ASR）。我们的工作不仅引入了一种可扩展的红队策略方法，借鉴了人类红队成员的做法，还突显了“逃逸”以“逃逸”作为安全防护未被充分考虑的失败模式。具体来说，LLM可以通过使用一个愿意协助进一步“逃逸”的被“逃逸”的版本自身来绕过其自身的防护。为了防止任何直接滥用$J_2$，同时推进人工智能安全研究，我们公开分享了我们的方法，但保留了具体的提示细节。', 'title_zh': '越狱到越狱\n\n这里的“越狱”是指计算机或移动设备上的安全机制被绕过，以获得更高的系统权限。若这是某个论文的标题或内容摘要，在翻译时应确保保留其特定含义和学术规范。如果有更多上下文信息或其他具体句子需要翻译，请提供，以便更准确地翻译。'}
{'arxiv_id': 'arXiv:2502.09636', 'title': 'Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?', 'authors': 'Sougata Saha, Saurabh Kumar Pandey, Harshit Gupta, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.09636', 'abstract': 'In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: this https URL', 'abstract_zh': '在快速全球化的数字时代，来自不同文化背景的人们撰写或生成的书评和产品评论被世界各地的用户阅读和消费。本文探讨了由于存在可能对其他文化背景用户来说不熟悉的文化特有项和元素，而导致书评理解差距的程度和模式。我们在对来自Goodreads的57篇书评进行用户研究后发现，83%的书评中至少包含一个文化特有、难以理解的元素。我们还评估了GPT-4o在考虑读者文化背景时识别这些元素的有效性；结果参差不齐，表明还有很大的改进空间。我们的数据集可通过以下链接访问：this https URL', 'title_zh': '在字里行间探寻文化差异：大型语言模型能否识别跨文化沟通差距？'}
{'arxiv_id': 'arXiv:2502.09635', 'title': 'CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking', 'authors': 'Delvin Ce Zhang, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.09635', 'abstract': 'Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.', 'abstract_zh': '验证声明的真实性通常需要对多个证据句进行推理。这些证据句往往不是自包含的，可能需要从其他地方获取附加的背景信息和引用，以理解核心引用表达、缩写以及报告结果的范围。例如，学术论文中的证据句可能需要论文中的背景句和引用论文中的描述来确定一项研究发现的范围。然而，大多数事实核查模型主要集中在证据句内的推理上，忽视了辅助背景信息和引用。为了解决这一问题，我们提出了一种新的方法，即上下文和引用增强推理与提示。在证据推理方面，我们构建了一个三层证据图，包括证据层、上下文层和引用层。我们设计了跨层和层内推理以将三层图整合为统一的证据嵌入。在判决预测方面，我们设计了一种基于证据的提示编码器，为每个声明生成独特的提示嵌入。这些基于证据的提示嵌入和声明被统一起来用于事实核查。实验结果验证了我们模型的有效性。', 'title_zh': '正确：上下文和引用增强的推理与提示生成在事实核查中的应用'}
{'arxiv_id': 'arXiv:2502.10378', 'title': 'Unknown Word Detection for English as a Second Language (ESL) Learners Using Gaze and Pre-trained Language Models', 'authors': 'Jiexin Ding, Bowen Zhao, Yuntao Wang, Xinyun Liu, Rui Hao, Ishan Chatterjee, Yuanchun Shi', 'link': 'https://arxiv.org/abs/2502.10378', 'abstract': 'English as a Second Language (ESL) learners often encounter unknown words that hinder their text comprehension. Automatically detecting these words as users read can enable computing systems to provide just-in-time definitions, synonyms, or contextual explanations, thereby helping users learn vocabulary in a natural and seamless manner. This paper presents EyeLingo, a transformer-based machine learning method that predicts the probability of unknown words based on text content and eye gaze trajectory in real time with high accuracy. A 20-participant user study revealed that our method can achieve an accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time reading assistance prototype to show the effectiveness of EyeLingo. The user study shows improvement in willingness to use and usefulness compared to baseline methods.', 'abstract_zh': '作为第二语言（ESL）学习者在阅读过程中经常遇到不认识的词汇，这会阻碍他们的文本理解。在用户阅读时自动检测这些词汇可以促使计算系统提供即时定义、同义词或上下文解释，从而以自然且无感知的方式帮助学习者学习词汇。本文介绍了EyeLingo，这是一种基于变换器的机器学习方法，能够在实时阅读过程中根据文本内容和眼动轨迹预测未知词汇的概率，具有高准确性。一项涉及20名参与者的用户研究显示，我们方法的准确率为97.6%，F1分数为71.1%。我们实现了一个实时阅读辅助原型，以展示EyeLingo的有效性。用户研究结果显示，与基础方法相比，该方法在使用意愿和实用性方面均有所提升。', 'title_zh': '使用凝视和预训练语言模型的英语作为第二语言（ESL）学习者未知单词检测'}
{'arxiv_id': 'arXiv:2502.10297', 'title': 'DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders', 'authors': 'Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi', 'link': 'https://arxiv.org/abs/2502.10297', 'abstract': "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet's expressivity by proving that it can solve dihedral group word problems in just two layers.", 'abstract_zh': '线性递归神经网络（linear RNNs）已成为Transformer在序列建模方面的有力替代方案，提供高效的训练和线性时间的推理。然而，现有的架构在表现性和效率之间存在一个基本的权衡，这种权衡由其状态转换矩阵的结构决定。例如，在Mamba、GLA或mLSTM等架构中使用对角矩阵虽然运行速度快，但也面临严重的表现性限制。为解决这一问题，最近的架构如(Gated) DeltaNet和RWKVv7采用了对角线加秩1的结构，允许同时进行词元-通道混合，从而在轻微降低训练效率的同时克服了一些表现性的限制。\n\n基于DeltaNet中的递归可被解释为每个词元在关联检索损失上进行一次在线梯度下降步的解读，我们引入了DeltaProduct，它使每个词元进行多次($n_h$)步。这自然导致了对角线加秩$n_h$的状态转换矩阵，这些矩阵是由$n_h$个广义Householder变换的乘积构成的，提供了一种可调机制来平衡表现性和效率，并且具有稳定的递归性。\n\n通过大量实验，我们证明了DeltaProduct在状态追踪和语言建模方面具有优越的能力，并且在长度外推方面表现出显著的改进，与DeltaNet相比效果更好。此外，我们还通过证明DeltaNet能够在两层中解决二面体群词问题，加强了DeltaNet表现性的理论基础。', 'title_zh': 'DeltaProduct：通过Householders的积增强DeltaNet的表达能力'}
{'arxiv_id': 'arXiv:2502.10248', 'title': 'Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model', 'authors': 'Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo', 'link': 'https://arxiv.org/abs/2502.10248', 'abstract': "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at this https URL. The online version can be accessed from this https URL as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.", 'abstract_zh': '我们介绍了Step-Video-T2V，这是一种最先进的文本到视频预训练模型，具有300亿参数，并能够生成长达204帧的视频。为了视频生成任务，我们设计了一种深度压缩变分自编码器（Video-VAE），其空间压缩比为16x16，时间压缩比为8x，同时保持出色的视频重建质量。用户提示通过两种双语文本编码器进行编码，以处理英中两种语言。采用流匹配训练三维全注意深度交互网络（DiT），用于将输入噪声转换为隐状态帧以去除噪声。应用基于视频的DPO方法（Video-DPO）来减少伪像，提高生成视频的视觉质量。我们还详细说明了我们的训练策略，并分享了重要观察和见解。Step-Video-T2V在一项新的视频生成基准Step-Video-T2V-Eval上进行了评估，证明其在与开源和商业引擎比较时具有最先进的文本到视频质量。此外，我们讨论了当前扩散模型范式的局限性，并列出了视频基础模型未来的发展方向。我们已在以下链接提供了Step-Video-T2V和Step-Video-T2V-Eval：[此处链接]。在线版本可以从以下链接访问：[此处链接]。我们的目标是加速视频基础模型的创新，并赋能视频内容创作者。', 'title_zh': 'Step-Video-T2V 技术报告：视频基础模型的应用、挑战及未来'}
{'arxiv_id': 'arXiv:2502.10162', 'title': 'Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions', 'authors': 'Lei Cheng, Junpeng Zhang, Qihan Ren, Quanshi Zhang', 'link': 'https://arxiv.org/abs/2502.10162', 'abstract': "This paper aims to analyze the generalization power of deep neural networks (DNNs) from the perspective of interactions. Unlike previous analysis of a DNN's generalization power in a highdimensional feature space, we find that the generalization power of a DNN can be explained as the generalization power of the interactions. We found that the generalizable interactions follow a decay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution. Furthermore, our theory can effectively disentangle these two types of interactions from a DNN. We have verified that our theory can well match real interactions in a DNN in experiments.", 'abstract_zh': '本文旨在从交互的角度分析深度神经网络（DNNs）的泛化能力。与以往从高维特征空间的角度分析DNN的泛化能力不同，我们发现DNN的泛化能力可以解释为其交互作用的泛化能力。我们发现可泛化的交互遵循一种衰减型分布，而不可泛化的交互遵循一种纺锤型分布。此外，我们的理论能够有效地从DNN中分离出这两种类型的交互。实验验证表明，我们的理论能够很好地与DNN中的实际交互相匹配。', 'title_zh': '从符号互动的角度 revisit 深度神经网络的泛化能力'}
{'arxiv_id': 'arXiv:2502.09990', 'title': 'X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability', 'authors': 'Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao', 'link': 'https://arxiv.org/abs/2502.09990', 'abstract': 'Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: this https URL.', 'abstract_zh': '尽管语言模型（LLM）的安全对齐技术得到了快速发展，但防范多轮脱esc（jailbreak）仍然是一个具有挑战性的问题。本文通过全面的比较揭示，现有的部分防御方法可以提高LLM对多轮脱esc的鲁棒性，但会牺牲可用性，即降低一般能力或引发过度拒绝问题。从LLM机制可解释性的视角来看，我们发现这些方法未能建立一个能够准确区分安全和有害特征表示的边界。因此，靠近有害特征表示的边界安全表示不可避免地被破坏，导致可用性的下降。为了解决这一问题，我们提出了一种X-Boundary方法，以将有害特征表示推离边界安全特征表示，并获得一个准确的区分边界。这样一来，有害特征表示可以被精确地删除而不影响安全的特征表示。实验结果表明，X-Boundary在防范多轮脱esc方面取得了最先进的防御性能，同时将过度拒绝率降低了约20%，并保持了接近完整的一般能力。此外，我们从理论上证明并从实验上验证了X-Boundary可以在训练过程中加速收敛过程。如果您想查看我们的代码，请访问以下链接：https://github.com/username/X-Boundary', 'title_zh': 'X-边界：建立精确的安全边界以屏蔽大型语言模型免受多轮对话脱逃风险，同时不牺牲可用性'}
{'arxiv_id': 'arXiv:2502.09969', 'title': 'Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning', 'authors': 'Ishika Agarwal, Dilek Hakkani-Tur', 'link': 'https://arxiv.org/abs/2502.09969', 'abstract': 'Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: this https URL.', 'abstract_zh': '影响函数为模型训练提供了宝贵的见解，但现有方法存在较大的计算成本和有限的泛化能力。尤其是，近期的研究提出了多种用于利用语言模型计算数据影响的度量和算法，但这些方法在面对大规模模型和数据集时不易扩展。这是因为计算过程中需要进行昂贵的前向和反向传递，存储大型模型所需的极大内存需求，以及影响估计在新数据上的较差泛化性。本文中，我们探索了使用小型神经网络（我们称之为InfluenceNetwork）来估计影响值，实现了高达99%的成本减少。我们的评估表明，影响值可以用仅有全语言模型0.0027%大小的模型来估计（我们使用了7B和8B版本的模型）。我们将用于估计影响值的算法（称为NN-CIFT：用于高效指令微调的神经网络）应用于通常指令微调中的子集选择任务。在我们的研究中，我们包含了四种最先进的影响函数，并在没有显著性能损失的情况下展示了NN-CIFT和原始影响函数相比的大幅加速。我们还提供了对NN-CIFT的深入超参数分析。我们的方法代码可在以下链接找到：[这里](this https URL)。', 'title_zh': '使用神经网络进行高效指令微调的数据估值'}
{'arxiv_id': 'arXiv:2502.09944', 'title': 'Self-Supervised Learning for Neural Topic Models with Variance-Invariance-Covariance Regularization', 'authors': 'Weiran Xu, Kengo Hirami, Koji Eguchi', 'link': 'https://arxiv.org/abs/2502.09944', 'abstract': 'In our study, we propose a self-supervised neural topic model (NTM) that combines the power of NTMs and regularized self-supervised learning methods to improve performance. NTMs use neural networks to learn latent topics hidden behind the words in documents, enabling greater flexibility and the ability to estimate more coherent topics compared to traditional topic models. On the other hand, some self-supervised learning methods use a joint embedding architecture with two identical networks that produce similar representations for two augmented versions of the same input. Regularizations are applied to these representations to prevent collapse, which would otherwise result in the networks outputting constant or redundant representations for all inputs. Our model enhances topic quality by explicitly regularizing latent topic representations of anchor and positive samples. We also introduced an adversarial data augmentation method to replace the heuristic sampling method. We further developed several variation models including those on the basis of an NTM that incorporates contrastive learning with both positive and negative samples. Experimental results on three datasets showed that our models outperformed baselines and state-of-the-art models both quantitatively and qualitatively.', 'abstract_zh': '在本研究中，我们提出了一种自我监督神经主题模型（NTM），该模型结合了NTM的力量和正则化自我监督学习方法，以提高性能。NTMs利用神经网络学习文档背后隐藏的主题，相比传统主题模型，能够提供更大的灵活性，并能估计出更加一致的主题。另一方面，一些自我监督学习方法采用联合嵌入架构，其中包含两个相同的网络，用于生成相同输入的两个增强版本的相似表示。对这些表示施加正则化，以防止模型输出恒定或重复的表示，从而避免网络对所有输入输出相同的表示。我们的模型通过明确正则化锚样本和正样本的潜在主题表示来提高主题质量。我们还引入了一种对抗数据增强方法，以替代启发式采样方法。此外，我们还开发了若干变体模型，包括基于NTM的模型，该模型结合了正样本和负样本的对比学习。在三个数据集上的实验结果表明，我们的模型在定量和定性方面均优于基线和最新模型。', 'title_zh': '带有方差不变性-协方差正则化的自我监督学习神经主题模型'}
{'arxiv_id': 'arXiv:2502.09933', 'title': "MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning", 'authors': 'Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen', 'link': 'https://arxiv.org/abs/2502.09933', 'abstract': 'Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $<$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.', 'abstract_zh': '归纳推理（IR），即从例子中总结规则并应用于新情况的能力，长期以来被视为通用智能的基本能力，受到了认知科学和人工智能研究人员的广泛关注。许多基准测试已经提出，用于衡量大型语言模型（LLMs）的这种能力；然而，这些基准测试主要集中在少量示例（通常少于10个）的设置上，并且缺乏对从长上下文中聚集大量信息的评估。另一方面，LLMs 的不断增长的上下文长度带来了新的 paradigm——大量的上下文学习（Many-Shot In-Context Learning, Many-Shot ICL），该 paradigm能够利用成百上千的示例来处理新任务，而不需要昂贵且低效的微调。然而，大多数 Many-Shot 评估主要集中在分类（这是一种 IR 的非常有限的方面），而流行的长上下文 LLM 任务，如“针扎干草堆”（Needle-In-A-Haystack, NIAH）很少需要复杂的集成大量信息的智能。为了解决这两个方面的不足，我们提出了 MIR-Bench，这是第一个 Many-Shot in-context 归纳推理基准测试，要求 LLM 通过输入-输出示例从底层函数的不同数据格式中推导出输出。基于 MIR-Bench，我们研究了归纳推理和 Many-Shot ICL 的许多新问题，包括对错误示例的鲁棒性以及思维链（Chain-of-Thought, CoT）的影响，并获得了深刻的发现。', 'title_zh': 'MIR-Bench：通过多次-shot 在上下文归纳推理评估大语言模型的长上下文智能'}
{'arxiv_id': 'arXiv:2502.09870', 'title': 'A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies', 'authors': 'Alicia DeVrio, Myra Cheng, Lisa Egede, Alexandra Olteanu, Su Lin Blodgett', 'link': 'https://arxiv.org/abs/2502.09870', 'abstract': 'Recent attention to anthropomorphism -- the attribution of human-like qualities to non-human objects or entities -- of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.', 'abstract_zh': '近年来，人们对语言技术（如大规模语言模型LLMs）的拟人化——即将人类特质赋予非人类对象或实体——的关注引发了关于拟人化潜在负面影响的新一轮讨论。为了有建设性地讨论这种拟人化的潜在影响及其适用的语境，我们需要一个能涵盖语言拟人化多种表现形式的共享词汇。在本研究中，我们借助现有文献并分析用户与语言技术互动的实证案例，发展出一种分类学框架，用以描述可能导致拟人化的文本表达。我们强调理解语言拟人化所面临的挑战和 tension，例如所有语言本质上都是人类的，以及试图表征和改变人们对机器中人性感知的努力也可能导致某些人类被边缘化。我们讨论了这种分类学框架如何支持对语言技术的拟人化进行更精确和有效的讨论与决策。', 'title_zh': '语言技术中促进拟人化的语言表达分类体系'}
{'arxiv_id': 'arXiv:2502.09863', 'title': 'Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence of Analogical Reasoning', 'authors': 'Dhruva Karkada, James B. Simon, Yasaman Bahri, Michael R. DeWeese', 'link': 'https://arxiv.org/abs/2502.09863', 'abstract': 'The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus. As a simpler surrogate for representation learning in language modeling, we study a class of solvable contrastive self-supervised algorithms which we term quadratic word embedding models. These models resemble the word2vec algorithm and perform similarly on downstream tasks. Our main contributions are analytical solutions for both the training dynamics (under certain hyperparameter choices) and the final word embeddings, given in terms of only the corpus statistics. Our solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on WikiText, we find that the top subspaces represent interpretable concepts. Finally, we use our dynamical theory to predict how and when models acquire the ability to complete analogies.', 'abstract_zh': '大型语言模型的卓越成功依赖于它们从预训练语料中隐式学习结构化潜在表示的能力。作为语言建模中表征学习的一种简化替代方法，我们研究了一类可解的对比自监督算法，我们称之为二次词嵌入模型。这些模型类似于word2vec算法，并在下游任务上表现相似。我们主要的贡献是对于训练动力学（在某些超参数选择下）和最终词嵌入的解析解，这些解仅以语料库统计信息为依据。我们的解揭示了这些模型一次学习正交的线性子空间，每个子空间逐步增加嵌入的有效秩，直到模型容量饱和。在使用WikiText进行训练时，我们发现顶级子空间代表可解释的概念。最后，我们使用我们的动力学理论来预测模型何时以及如何获得完成类比的能力。', 'title_zh': '自我监督词嵌入的可解动力学及其类比推理的涌现'}
{'arxiv_id': 'arXiv:2502.09858', 'title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'authors': 'Kexin Huang, Ying Jin, Ryan Li, Michael Y. Li, Emmanuel Candès, Jure Leskovec', 'link': 'https://arxiv.org/abs/2502.09858', 'abstract': "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.", 'abstract_zh': '假设在信息获取、决策制定和发现中起着核心作用。然而，许多现实世界的假设往往是抽象和高层的陈述，难以直接验证。这一挑战因大型语言模型（LLMs）生成假设而加剧，LLMs容易产生幻觉，且生成的假设数量之大使得手动验证变得不切实际。为此，我们提出了Popper这一代理框架，用于严谨的自动化假设验证。该框架遵循卡尔·波普尔的证伪原则，通过利用LLM代理设计和执行针对假设可测量影响的证伪实验来进行验证。一个新颖的序列测试框架确保了严格的I型错误控制，同时积极从多种观察中收集证据，无论这些观察是现有的数据还是新进行的程序。我们已在生物学、经济学和社会学等六个领域展示了Popper。Popper实现了稳健的错误控制、高功效和可扩展性。此外，相较于人类科学家，Popper在验证复杂的生物学假设方面实现了相当的性能，同时将时间减少了十倍，为假设验证提供了可扩展且严谨的解决方案。', 'title_zh': '自动化的假设验证通过代理性的序列否证'}
{'arxiv_id': 'arXiv:2502.09782', 'title': 'Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models', 'authors': 'Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai', 'link': 'https://arxiv.org/abs/2502.09782', 'abstract': "The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet's performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios.", 'abstract_zh': '随着日常设备中麦克风的普及率不断提高以及对在线服务的依赖日益增强，键盘的声学侧信道攻击（ASCAs）风险也相应增加。本文探讨了深度学习技术，特别是视觉变换器（VTs）和大规模语言模型（LLMs），以提升此类攻击的有效性和适用性。我们在此前研究的基础上取得了显著改进，其中CoAtNet模型达到了最先进的性能。具体而言，使用CoAtNet模型通过智能手机（Phone）录制的按键事件比先前基准提升了5.0%，通过Zoom录制的按键事件提升了5.9%。我们还评估了Transformer结构和语言模型的表现，最佳的VT模型与CoAtNet模型的性能相当。一个重要进步是引入了一种应对现实场景噪声的缓解方法。利用LLMs进行上下文理解，我们能够检测并修正噪声环境中出现的错误按键事件，从而提升ASCAs的效果。此外，经过微调的轻量级语言模型搭配低秩适配（LoRA）技术，其性能与参数量多67倍的模型相媲美。将VTs和LLMs的集成提升了ASCAs缓解方案的实用性和广泛适用性，这是首次利用这两种技术来应对ASCAs和错误矫正的现实场景问题。', 'title_zh': '使用变压器和大型语言模型提高键盘侧信道攻击的效果'}
{'arxiv_id': 'arXiv:2502.09767', 'title': 'Non-Markovian Discrete Diffusion with Causal Language Models', 'authors': 'Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk', 'link': 'https://arxiv.org/abs/2502.09767', 'abstract': 'Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.', 'abstract_zh': '离散扩散模型作为一种灵活可控的结构序列建模范式已经崭露头角，然而在表现力上仍落后于因果语言模型。为了弥合这两种范式之间的差距，我们引入了CaDDi，这是一种因果离散扩散模型，在非马尔可夫扩散框架下统一了顺序建模和时间建模。与传统的逐步操作且不能访问先验状态的扩散模型不同，CaDDi 融合了时间轨迹，从而实现更富有表现力和可控性的生成。此外，我们的方法也将因果语言模型视为一种特殊情形，使得可以直接无缝地采用预训练的大规模语言模型（Large Language Models, LLMs）进行离散扩散建模，而无需对架构进行修改。实验证明，CaDDi 在自然语言和生物序列任务上均优于当前最先进的离散扩散模型，缩小了基于扩散的方法与大规模自回归变换器之间的差距。', 'title_zh': '非马尔可夫离散扩散与因果语言模型'}
{'arxiv_id': 'arXiv:2502.09723', 'title': 'Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models', 'authors': 'Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang', 'link': 'https://arxiv.org/abs/2502.09723', 'abstract': "Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to systematically examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into code-style structured query to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, ant the results show that QueryAttack achieves high attack success rates (ASRs) across LLMs with different developers and capabilities. We also evaluate QueryAttack's performance against common defenses, confirming that it is difficult to mitigate with general defensive techniques. To defend against QueryAttack, we tailor a defense method which can reduce ASR by up to 64\\% on GPT-4-1106. The code of QueryAttack can be found on this https URL.", 'abstract_zh': '近年来，大规模语言模型（LLMs）在自然语言处理领域的潜力得到了显著展示。然而，LLMs 面临着重要的安全和伦理风险。尽管已经开发了诸如安全性对齐这样的防御技术，先前的研究表明，通过精心设计的“破戒”攻击，这些防御机制可能会被绕过。本文提出了一种名为 QueryAttack 的新型框架，旨在系统地检验安全性对齐的泛化能力。我们以语言模型为知识库的观点出发，将自然语言中的恶意查询转换为代码风格的结构化查询，以绕过语言模型的安全对齐机制。我们对主流的 LLM 进行了广泛的实验，结果显示，QueryAttack 在不同开发者和能力水平的 LLM 中均实现了较高的攻击成功率（ASR）。我们还评估了 QueryAttack 对常用防御技术的影响，确认一般性的防御技术很难缓解这一问题。为了对抗 QueryAttack，我们提出了一种定制的防御方法，该方法在 GPT-4-1106 上将 ASR 降低了最多 64%。QueryAttack 的代码可以在以下网址找到：此 https URL。', 'title_zh': '将其转变为恶意数据库：利用查询代码突破对齐的大语言模型限制'}
{'arxiv_id': 'arXiv:2502.09715', 'title': "Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data", 'authors': 'Yu Leng, Yingnan He, Colin Magdamo, Ana-Maria Vranceanu, Christine S. Ritchie, Shibani S. Mukerji, Lidia M. V. R. Moura, John R. Dickson, Deborah Blacker, Sudeshna Das', 'link': 'https://arxiv.org/abs/2502.09715', 'abstract': "Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4o's potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future.", 'abstract_zh': '在电子健康记录（EHRs）中识别认知障碍对于及时诊断和促进研究都至关重要。关于认知障碍的信息通常存在于EHRs中的未结构化医生笔记中，但手动病历审查既耗时又容易出错。为了解决这一问题，本研究评估了一种自动化方法，使用零样本GPT-4o来确定认知障碍的阶段，并在两个不同的任务中测试了其实验效果。首先，我们评估了GPT-4o确定麻省总医院（MGH）记忆诊所769名患者临床痴呆评级（CDR）全球评分的能力，取得了加权κ值为0.83的结果。其次，我们评估了GPT-4o在对过去三年内的860名医疗保险患者的所有笔记进行分类时区分正常认知、轻度认知障碍（MCI）和痴呆的能力。GPT-4o在与专科病历审查相比时获得了0.91的加权κ值，而在临床判定者高度自信的病例中，这一值达到了0.96。我们的研究结果表明，GPT-4o具有作为可扩展的病历审查工具的潜力，未来可用于创建研究数据集和协助临床诊断。', 'title_zh': '评估GPT在识别电子健康数据中的认知 impairment 阶段方面的能力'}
{'arxiv_id': 'arXiv:2502.09637', 'title': 'Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness', 'authors': 'Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.09637', 'abstract': 'Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, "culture" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess "cultural awareness", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.', 'abstract_zh': '近年来，许多研究表明，大型语言模型（LLMs）倾向于西方和盎格鲁中心的世界观，这在非西方文化环境中限制了其 usefulness。然而，“文化”是一个复杂且多维度的话题，而在LLMs及其基于LLM的应用中，其意识、表现和建模可以被定义和测量的方式有很多。在本文中，我们探讨了LLM拥有“文化意识”意味着什么。通过一个思想实验，这一实验将Bender和Koller（2020）提出的“章鱼测试”进行了扩展，我们认为，使LLM及其基于LLM的人工智能系统在各种文化，包括完全未见过的文化中都适用，所需要的并非仅仅是文化意识或知识，而是元文化能力。我们界定了元文化能力AI系统的原则，并讨论了如何衡量和建模这些原则。', 'title_zh': '元文化素养：走向恰当的文化意识之峰'}
