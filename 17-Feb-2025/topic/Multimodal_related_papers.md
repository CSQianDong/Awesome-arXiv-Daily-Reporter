# Cooperative Multi-Agent Planning with Adaptive Skill Synthesis 

**Title (ZH)**: 具有自适应技能合成的协作多智能体规划 

**Authors**: Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen  

**Link**: [PDF](https://arxiv.org/pdf/2502.10148)  

**Abstract**: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios. 

**Abstract (ZH)**: 尽管在训练分布式人工智能（AI）方面取得了显著进展，但使用多智能体强化学习（MARL）构建合作多智能体系统仍面临样本效率、可解释性和迁移性方面的挑战。不同于传统的基于学习的方法需要与环境进行大量的互动，大型语言模型（LLMs）展示了在零-shot 规划和复杂推理方面的卓越能力。然而，现有的基于LLM的方法主要依赖于基于文本的观测，并且在处理部分可观测环境下非马尔可夫性多智能体交互时存在困难。我们提出了一种名为COMPASS的新型多智能体架构，该架构将视觉语言模型（VLMs）与动态技能库和结构化通信结合，以实现去中心化的闭环决策。技能库通过演示初始化，并通过规划者指导的任务逐步进化，从而启用适应性策略。在部分可观测情况下，COMPASS通过多跳通信传播实体信息。在改进的星际争霸多智能体挑战（SMACv2）上的评估结果表明，COMPASS在对称场景中可实现比现有最先进的MARL算法高达30%的胜率。 

---
# MuDoC: An Interactive Multimodal Document-grounded Conversational AI System 

**Title (ZH)**: MuDoC：一种交互式多模态文档导向对话AI系统 

**Authors**: Karan Taneja, Ashok K. Goel  

**Link**: [PDF](https://arxiv.org/pdf/2502.09843)  

**Abstract**: Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations. 

**Abstract (ZH)**: 多模态AI是朝着通过利用人类与AI通信中多种模态构建有效工具迈出的重要一步。构建一个能够与长文档交互的多模态文档导向型AI系统仍然是一项挑战。我们旨在填补直接利用文档中的目标视觉内容与文本内容进行响应生成这一研究空白。我们基于GPT-4o提出了一个名为“MuDoC”的互动对话AI代理，能够生成交织文本与图表的文档导向型响应。MuDoC的智能教科书界面增强了系统的可信度，并允许用户即时导航到文档中的源文本和图表进行验证。我们还基于MuDoC的响应进行了定性的观察，以突出其优势和局限性。 

---
# Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries 

**Title (ZH)**: 情绪对齐与时间边界同步的视频 soundtrack 生成 

**Authors**: Serkan Sulun, Paula Viana, Matthew E. P. Davies  

**Link**: [PDF](https://arxiv.org/pdf/2502.10154)  

**Abstract**: We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners. 

**Abstract (ZH)**: 我们介绍了一种名为EMSYNC的基于视频的符号化音乐生成模型，该模型能够将音乐与视频的情感内容和时间边界相匹配。EMSYNC采用两阶段框架：首先，预训练的视频情绪分类器提取情感特征；然后，条件音乐生成器在情感和时间线索的指导下生成MIDI序列。我们引入了一种新的时间条件机制，称为边界偏移，该机制使模型能够预测场景切换并使其与音乐和声部分时段对齐。与现有模型不同，我们的方法保留了基于事件的编码，从而确保精细的时间控制和表达性强的音乐细节。此外，我们还提出了一种映射方案，用于对接产生离散情绪类别的视频情绪分类器和以连续值唤起-愉悦度输入操作的条件MIDI生成器。在主观听觉测试中，EMSYNC在所有主观评估指标上均优于现有最先进的模型，包括音乐理论意识的参与者和一般听众。 

---
# Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models 

**Title (ZH)**: Manual2Skill：使用视觉-语言模型学习阅读说明书并获取家具组装的机器人技能 

**Authors**: Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao  

**Link**: [PDF](https://arxiv.org/pdf/2502.10090)  

**Abstract**: Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities. 

**Abstract (ZH)**: 人类具有通过理解和执行抽象说明来完成复杂操作任务的非凡能力。然而，对于机器人来说，这一能力仍然是一个巨大的挑战，因为它们无法理解抽象指令并将其转化为可执行的操作。在这篇论文中，我们提出了一种名为Manual2Skill的新框架，使机器人能够遵循高层次的手工说明来执行复杂的组装任务。我们的方法利用视觉-语言模型（VLM）从指示图像中提取结构化的信息，然后利用这些信息构建分层次的组装图。这些图表示部件、子装配件及其之间的关系。为了促进任务执行，姿势估计模型预测每个组装步骤中组件的相对6D姿势。同时，运动规划模块生成适用于实际机器人实现的操作序列。通过成功组装多个真实的宜家家具件，我们展示了Manual2Skill的有效性。这一应用突显了其在效率和精度方面管理长期操作任务的能力，大大增强了从说明书中学习的机器人系统的实用性。这项工作标志着在使机器人系统能够以类似人类能力的方式理解和执行复杂操作任务方面向前迈进了一步。 

---
# Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence 

**Title (ZH)**: 《granite vision：一种轻量级、开源的多模态企业智能模型》 

**Authors**: Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek-koifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, Rogerio Feris  

**Link**: [PDF](https://arxiv.org/pdf/2502.09927)  

**Abstract**: We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See this https URL for model weights. 

**Abstract (ZH)**: 我们引入了Granite Vision，这是一种轻量级的大型语言模型，具备视觉能力，特别设计用于企业应用场景，尤其擅长视觉文档理解。我们的模型是在涵盖文档相关任务的综合指令遵循数据集上进行训练的，包括从表格、图表、图表、草图和信息图中提取内容，以及一般的图像任务。Granite Vision的架构以视觉模态对齐为中心，基于一个解码器仅存的20亿参数的大规模语言模型。此外，我们还在测试时引入了一种专门的安全分类方法，该方法利用稀疏的注意力向量来识别潜在有害输入。尽管其架构轻量，但Granite Vision在标准的视觉文档理解基准测试中取得了良好的结果，并在LiveXiv基准测试中表现出色，后者通过使用不断更新的最新发布Arxiv论文集来避免测试集污染。我们以Apache-2许可协议发布该模型，允许进行研究和商业应用，同时提供完整的训练数据和其他相关细节的透明度。模型权重可在以下链接获取：[这里](https://example.com/model_weights)。 

---
# TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types 

**Title (ZH)**: TaskGalaxy：利用十万余种视觉任务类型扩展多模态指令微调规模 

**Authors**: Jiankang Chen, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Feng Cheng, Huihui Xiao, Bin Wen, Fan Yang, Tingting Gao, Di Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.09925)  

**Abstract**: Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at this https URL. 

**Abstract (ZH)**: 多模态视觉语言模型在开放世界应用中正逐渐受到重视，这得益于模型架构、训练技术以及高质量数据的进步。然而，这些模型的表现往往受到特定任务数据不足的限制，导致泛化能力差和倾向性输出。现有的通过增加微调数据集的任务多样性努力受到手动任务标注劳动密集型过程的阻碍，通常只能生成几种类型的任务。为解决这一问题，我们提出了一种名为TaskGalaxy的大规模多模态指令微调数据集，其中包括19,227种层次化任务类型和413,648个样本。TaskGalaxy利用GPT-4o来增加任务多样性，从少量手动定义的任务开始扩展，使用CLIP和GPT-4o筛选出与开源图像最匹配的任务，并生成相关的问题-答案对。多种模型被采用以确保样本质量。这一自动化过程不仅增加了任务多样性，还提高了数据质量，减少了人工干预。将TaskGalaxy集成到LLaVA-v1.5和InternVL-Chat-v1.0模型中，在16个基准测试上表现出显著的性能提升，这突显了任务多样性的关键重要性。TaskGalaxy已在此网址公开发布：[this https URL]。 

---
# AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset 

**Title (ZH)**: AttenGluco：基于多模态Transformer的AI-READI数据集血糖预测 

**Authors**: Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Eric Kim, Gautham Krishna Gudur, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, Hassan Ghasemzadeh  

**Link**: [PDF](https://arxiv.org/pdf/2502.09919)  

**Abstract**: Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively. 

**Abstract (ZH)**: 糖尿病是一种以持续高血糖水平（血糖水平，BGLs）为特征的慢性代谢疾病，可能导致严重并发症，如心血管疾病、神经病变和视网膜病变。预测血糖水平能够帮助患者维持血糖在安全范围之内，并允许护理人员通过生活方式调整采取主动措施。连续葡萄糖监测（CGM）系统提供了实时跟踪功能，是一个监测血糖水平的重要工具。然而，由于体力活动、饮食和其他因素的影响，准确预测血糖水平仍然具有挑战性。最近的深度学习模型在改善血糖预测方面显示出潜力。然而，从多模态、不规则采样的数据中进行长时间预测仍是一个具有挑战性的研究问题。在本文中，我们提出了一种基于Transformer的多模态框架AttenGluco，用于长期血糖预测。AttenGluco利用交叉注意力有效地整合CGM数据和活动数据，解决了不同采样率数据融合的挑战。此外，它采用多尺度注意力来捕捉时间数据中的长期依赖关系，从而提高预测准确性。为了评估AttenGluco的性能，我们在最近发布的AIREADI数据集上进行了预测实验，分析了其在不同受试群体中的预测准确性，包括健康个体、糖尿病前期患者以及2型糖尿病患者。此外，我们还研究了随着新受试群体的引入，其性能改进和遗忘行为。评估结果表明，AttenGluco在所有误差指标（如均方根误差RMSE、平均绝对误差MAE和相关性）上均优于多模态LSTM模型。与基线模型相比，AttenGluco在RMSE和MAE上的表现分别提高了约10%和15%。 

---
# HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation 

**Title (ZH)**: HealthGPT：一种通过异构知识适应实现统一理解和生成的医疗多模态大模型 

**Authors**: Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi  

**Link**: [PDF](https://arxiv.org/pdf/2502.09838)  

**Abstract**: We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at this https URL. 

**Abstract (ZH)**: 我们介绍了HealthGPT，这是一个强大的医学多模态（视觉-语言）模型（Med-LVLM），能够在一个统一的自回归框架中集成医学视觉理解与生成能力。我们提出的强化学习哲学是逐步适应异构的理解和生成知识到预训练的大型语言模型（LLMs）中。这一过程通过一种新颖的异构低秩适应技术（H-LoRA）实现，该技术与定制的分层视觉感知方法和三阶段学习策略相结合。为了有效学习HealthGPT，我们设计了一个全面的医学领域特定的视觉-语言理解和生成数据集，称为VL-Health。实验结果表明，HealthGPT在医学多模态统一任务中表现出色且具有良好的可扩展性。我们的项目可以在以下链接访问：[该项目的网址]。 

---
# Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis 

**Title (ZH)**: 多层冲突感知网络在多模态情感分析中的应用 

**Authors**: Yubo Gao, Haotian Wu, Lei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.09675)  

**Abstract**: Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN. 

**Abstract (ZH)**: 多模态情感分析（Multimodal Sentiment Analysis, MSA）旨在通过利用文本、音频和视觉等多种模态信息来识别人类情绪，因此如何充分利用不同模态之间的交互是MSA的核心挑战之一。交互包含对齐和冲突两个方面。当前的研究主要强调对齐以及单模态之间固有的差异，而忽视了双模态组合之间也可能存在潜在冲突这一事实。此外，基于多任务学习的冲突建模方法往往依赖于不稳定生成的标签。为了解决这些问题，我们提出了一种新颖的多层级冲突感知网络（Multi-Level Conflict-Aware Network, MCAN），该网络逐步从单模态和双模态表示中分离对齐和冲突成分，并进一步通过冲突建模分支利用冲突成分。在冲突建模分支中，我们在表示和预测输出两个层面都进行了差异约束，避免了对生成标签的依赖。在CMU-MOSI和CMU-MOSEI数据集上的实验结果证明了所提出的MCAN的有效性。 

---
# From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models 

**Title (ZH)**: 从不知到知：多模态基础模型中否定理解的分类、挑战与机遇 

**Authors**: Mayank Vatsa, Aparna Bharati, Surbhi Mittal, Richa Singh  

**Link**: [PDF](https://arxiv.org/pdf/2502.09645)  

**Abstract**: Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments. 

**Abstract (ZH)**: 否定，作为一种表达缺席、拒绝或矛盾的语法构造，对多语言多模态基础模型构成了重大的挑战。这些模型在机器翻译、文本引导生成、图像字幕、音频交互和视频处理等任务中表现出色，但在准确解释不同语言和文化背景下的否定方面往往存在困难。在这篇观点文章中，我们提出了一种全面的否定构造分类体系，展示了结构、语义和文化因素如何影响多模态基础模型。我们提出了开放的研究问题，并强调了解决这些问题的重要性，以实现更稳健的否定处理。最后，我们呼吁建立专门的基准测试、语言特定的分词、细粒度的注意力机制和先进的多模态架构。这些策略可以促进更具适应性和语义精确性的多模态基础模型的发展，使其能够更好地应对多语言、多模态环境中的否定复杂性。 

---
# MM-RLHF: The Next Step Forward in Multimodal LLM Alignment 

**Title (ZH)**: MM-RLHF：迈向多模态大语言模型对齐的下一步 

**Authors**: Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan  

**Link**: [PDF](https://arxiv.org/pdf/2502.10391)  

**Abstract**: Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\mathbf{10}$ distinct dimensions and $\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\mathbf{19.5}$% increase in conversational abilities and a $\mathbf{60}$% improvement in safety.
We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: this https URL. 

**Abstract (ZH)**: 尽管在多模态大型语言模型（MLLMs）方面取得了显著进展，但大多数最先进的模型尚未经过彻底的人类偏好对齐。这一差距源于当前对齐研究主要在特定领域（例如，幻觉减少）取得了进展，而关于是否可以通过系统性地将模型对齐与人类偏好相结合来提升MLLM的能力这一更广泛的问题仍然 largely 未被探索。为解决这一问题，我们引入了 MM-RLHF 数据集，该数据集包含 **120,000** 对细粒度的人类注释偏好对比样本。与现有资源相比，该数据集代表了一项重大进展，提供了更大的规模、更高的多样性、更精细的注释粒度和更高的质量。利用该数据集，我们提出了一些关键创新来提高奖励模型的质量和对齐算法的效率。特别是，我们引入了一种批判性奖励模型，该模型在分配评分之前生成对模型输出的批判意见，从而提供了比传统标量奖励机制更高的可解释性和更具信息性的反馈。此外，我们还提出了动态奖励缩放方法，该方法根据奖励信号调整每个样本的损失权重，从而优化高质量对比样本的使用。我们从 **10** 个不同的维度和 **27** 个基准进行了严格的评估，结果显示模型性能取得了显著且一致的改进。具体来说，使用 MM-RLHF 数据集和我们的对齐算法微调 LLaVA-ov-7B 在对话能力上提高了 **19.5%**，在安全性上提高了 **60%**。

我们已经开源了偏好数据集、奖励模型、训练和评估代码，以及奖励建模和安全性基准。如需更多详细信息，请访问我们的项目页面：[此链接](this https URL)。 

---
# VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models 

**Title (ZH)**: VisCon-100K：利用上下文网络数据fine-tune视觉语言模型 

**Authors**: Gokul Karthik Kumar, Iheb Chaabane, Kebin Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.10250)  

**Abstract**: Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a `leaky modality mix,' where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q\&A pairs. VisCon-100k dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications. Using the same pipeline, but substituting our trained contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset. 

**Abstract (ZH)**: 视觉语言模型（VLMs）在各种视觉基准测试中表现出色，但往往受限于高质量视觉微调数据的缺乏。为解决这一挑战，我们引入了VisCon-100K，这是一个源自交错图像-文本网页文档的新数据集。我们的方法将来自OBELICS数据集的45K网页文档转化为100K幅图像对话样本。我们使用GPT-4V生成图像上下文描述，并利用OpenChat 3.5模型将这些描述转化为多种自由形式和多项选择的问题-答案对。将此数据集用于微调显著提升了多种基准测试下VLM的性能。与仅专注于细粒度视觉内容的方法不同，我们的方法利用伴随的网页上下文，取得了更好的结果。我们还发现，在对话样本中，当问题可以从图像及其上下文描述中回答时，即存在“泄露模态混合”的情况下，问题-答案对和描述的非泄露组合表现更优。使用VisCon-100K数据集，我们的数据集展现了两种流行VLM方法的强大性能：仅包含文本的大规模语言模型（LLM）通过图像描述与视觉编码器对齐（ShareGPT4V-7b），以及使用交错图像-文本数据的多模态预训练LLM（IDEFICS2-8b）。除了发布VisCon-100K数据集外，我们还提供基于该数据集训练的上下文描述器，便于未来研究和开源应用的可扩展微调数据生成。借助相同的工作流程，但用我们训练的上下文描述器替换GPT-4V，我们还发布了更大的VisCon-1M数据集。 

---
# Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories 

**Title (ZH)**: 大数据模型与来源元数据在新闻故事中确定图像和视频的相关性中的应用 

**Authors**: Tomas Peterka, Matyas Bohacek  

**Link**: [PDF](https://arxiv.org/pdf/2502.09689)  

**Abstract**: The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface. 

**Abstract (ZH)**: 最有效的虚假信息传播往往是多模态的，通常会将文本与上下文无关或完全伪造的图片和视频结合起来，以支持特定的叙述。当前用于检测虚假信息的方法，无论是深度换脸还是文本文章，往往未能捕捉到多个模态之间的相互作用。本文提出了一种基于大规模语言模型的系统，旨在应对这些挑战。该系统分析文章的文本内容及其包含的图片和视频的来源元数据，以确定它们的相关性。我们开源了该系统的原型以及互动网页界面。 

---
