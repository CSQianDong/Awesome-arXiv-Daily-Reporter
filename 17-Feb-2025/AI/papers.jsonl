{'arxiv_id': 'arXiv:2502.10383', 'title': 'Representation and Interpretation in Artificial and Natural Computing', 'authors': 'Luis A. Pineda', 'link': 'https://arxiv.org/abs/2502.10383', 'abstract': "Artificial computing machinery transforms representations through an objective process, to be interpreted subjectively by humans, so the machine and the interpreter are different entities, but in the putative natural computing both processes are performed by the same agent. The method or process that transforms a representation is called here \\emph{the mode of computing}. The mode used by digital computers is the algorithmic one, but there are others, such as quantum computers and diverse forms of non-conventional computing, and there is an open-ended set of representational formats and modes that could be used in artificial and natural computing. A mode based on a notion of computing different from Turing's may perform feats beyond what the Turing Machine does but the modes would not be of the same kind and could not be compared. For a mode of computing to be more powerful than the algorithmic one, it ought to compute functions lacking an effective algorithm, and Church Thesis would not hold. Here, a thought experiment including a computational demon using a hypothetical mode for such an effect is presented. If there is natural computing, there is a mode of natural computing whose properties may be causal to the phenomenological experience. Discovering it would come with solving the hard problem of consciousness; but if it turns out that such a mode does not exist, there is no such thing as natural computing, and the mind is not a computational process.", 'abstract_zh': '人工计算机器通过客观的过程转化表示，而这些表示需要人类主观地解释，因此机器和解释者是不同的实体。但在假设中的自然计算中，这两个过程都由同一个主体执行。这里所谓的转化表示的方法称为“计算模式”。在数字计算机中使用的模式是算法模式，但还有其他模式，如量子计算机和其他非传统计算形式，对于人工和自然计算而言，还存在各种代表格式和模式的开放集合。基于与图灵概念不同的计算观念的模式可能会超越图灵机所能完成的功能，但这些模式不是同一类型的，无法进行比较。对于一种计算模式来说，如果它比算法模式更有力量，那么它必须能够计算没有有效算法的功能，而图灵论题将不再适用。在这里，提出了一个涉及使用假设性模式的计算恶魔的头脑实验。如果存在自然计算，那么可能存在一种自然计算的模式，其性质可能因果地引起直觉体验。发现这种模式将解决意识的硬问题；但如果有证据表明这种模式不存在，那么就没有自然计算，而心灵也不是一个计算过程。', 'title_zh': '人工智能与自然计算中的表示与解释'}
{'arxiv_id': 'arXiv:2502.10308', 'title': 'LLM-Powered Preference Elicitation in Combinatorial Assignment', 'authors': 'Ermis Soumalias, Yanchen Jiang, Kehang Zhu, Michael Curry, Sven Seuken, David C. Parkes', 'link': 'https://arxiv.org/abs/2502.10308', 'abstract': 'We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.', 'abstract_zh': '我们研究了大型语言模型（LLMs）作为人类代理在组合分配中简化偏好获取（PE）的潜力。传统的方法依赖于迭代查询来捕捉偏好，而LLMs提供了减少人类劳动的努力量化的单次替代方案。我们提出了一种框架，使得LLMs可以与最先进的基于机器学习的偏好获取方法协同工作。我们的框架处理由LLMs引入的新挑战，例如响应的变异性及增加的计算成本。我们在已研究充分的课程分配领域中实验性地评估了LLMs代理与人类查询的效率，并探讨了成功所需的模型能力。我们发现，我们的方法在分配效率方面提高了最多20%，并且这些结果在不同的LLMs及报告质量和准确性差异的情况下具有稳健性。', 'title_zh': '基于LLM的组合分配中偏好引致方法'}
{'arxiv_id': 'arXiv:2502.10303', 'title': 'Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations', 'authors': 'Abdelrhman Shaheen, Anas Badr, Ali Abohendy, Hatem Alsaadawy, Nadine Alsayad', 'link': 'https://arxiv.org/abs/2502.10303', 'abstract': 'Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.', 'abstract_zh': '强化学习（RL）在许多应用中得到了广泛应用，尤其是在游戏中，这为人工智能模型提供了优秀的训练平台。Google DeepMind 在这一领域率先进行了创新，利用包括基于模型、非基于模型以及深度Q网络在内的强化学习算法，研发了先进的AI模型，如AlphaGo、AlphaGo Zero和MuZero。AlphaGo是最初的模型，它结合了监督学习和强化学习，掌握了围棋游戏，超越了职业人类选手。AlphaGo Zero则通过消除对人类游戏数据的依赖，改用自我对弈的方式，提高了学习效率。MuZero进一步在此基础上进行拓展，通过隐式学习游戏环境的底层动态，而无需了解游戏规则，实现了跨各种游戏的适应性，包括复杂的Atari游戏。本文回顾了强化学习在Atari游戏和策略性游戏中应用的重要意义，分析了这些三种模型的关键创新、训练过程、面临的挑战及其改进，并讨论了游戏领域的进展，包括MiniZero和多智能体模型，展望了未来的研究方向和Google DeepMind正在开发的新兴AI模型。', 'title_zh': '基于策略和Atari游戏的强化学习：Google DeepMind创新综述'}
{'arxiv_id': 'arXiv:2502.10215', 'title': 'Do Large Language Models Reason Causally Like Us? Even Better?', 'authors': 'Hanna M. Dettki, Brenden M. Lake, Charley M. Wu, Bob Rehder', 'link': 'https://arxiv.org/abs/2502.10215', 'abstract': 'Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-4o and Claude showed the most normative behavior, including "explaining away", whereas Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected independence of causes - Claude the least - they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes. These findings underscore the need to assess AI biases as they increasingly assist human decision-making.', 'abstract_zh': '因果推理是智能的核心组成部分。大型语言模型（LLMs）展示了生成类人类文本的能力，引发了对其响应是反映真正理解还是统计模式的疑问。我们使用基于碰撞图的任务比较了人类和四种LLM的因果推理能力，这些任务包括评估某一变量在其他变量证据的情况下出现的可能性。我们的研究表明，LLM在从人类样本来规范推理的谱系中进行因果推理，其对齐程度取决于模型、上下文和任务。总体而言，GPT-4o和Claude表现出最规范的行为，包括“解释掉”现象；而Gemini-Pro和GPT-3.5则没有表现出规范性行为。尽管所有代理都偏离了原因相互独立的预期，Claude偏离程度最小，但它们在评估效果给定其原因的出现可能性时，仍然展示了强大的关联推理和预测性推断。这些发现强调了在AI越来越多地协助人类决策时评估AI偏见的需求。', 'title_zh': '大型语言模型像我们一样进行因果推理吗？甚至更好？'}
{'arxiv_id': 'arXiv:2502.10197', 'title': 'MathConstruct: Challenging LLM Reasoning with Constructive Proofs', 'authors': 'Mislav Balunović, Jasper Dekoninck, Nikola Jovanović, Ivo Petrov, Martin Vechev', 'link': 'https://arxiv.org/abs/2502.10197', 'abstract': 'While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce \\mc, a new benchmark of 126 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MathConstruct to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct problems, highlighting its complexity and importance for LLM evaluation.', 'abstract_zh': '尽管大规模语言模型（LLMs）在数学领域展现了 impressive 的性能，现有的数学基准测试仍然存在显著的局限性。许多基准测试关注有固定正确答案的问题，并且由于问题过于简单或可以通过猜测或记忆来解决而变得饱和。更重要的是，这些基准测试仅捕捉到了相关数学问题的一个狭窄子集。为解决这一研究空白，我们引入了 \\mc（MathConstruct），一个包含来自各类数学竞赛的126个具有挑战性问题的新基准测试，该基准测试旨在评估构造性证明，这是一类常见的问题类型，需要构建具有特定属性的数学对象。这些证明特别适合评估LLM的能力，因为解的正确性可以很容易地验证。我们还通过自动化验证器生成问题变体，用于评估模型的鲁棒性。最先进的LLM仅能解决MathConstruct测试中54%的问题，这突显了其复杂性和在LLM评估中的重要性。', 'title_zh': 'MathConstruct：以建构性证明挑战LLM推理能力'}
{'arxiv_id': 'arXiv:2502.10177', 'title': 'STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning', 'authors': 'Mingcong Lei, Yiming Zhao, Ge Wang, Zhixin Mai, Shuguang Cui, Yatong Han, Jinke Ren', 'link': 'https://arxiv.org/abs/2502.10177', 'abstract': 'A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.', 'abstract_zh': '本篇论文的关键目标在于使智能体能够在动态环境中执行长期任务，同时保持稳健的决策能力和适应性。为了实现这一目标，我们提出了一种新型框架——时空记忆智能体（STMA），该框架通过整合时空记忆来增强任务规划与执行能力。STMA基于三个关键组件构建：（1）时空记忆模块，能够实时捕捉历史和环境变化；（2）动态知识图谱，以促进适应性空间推理；（3）计划者-批判者机制，用于迭代优化任务策略。我们在包含32项任务的TextWorld环境中对STMA进行了评估，这些任务涉及复杂程度不同的多步规划与探索。实验结果表明，STMA相比当前最先进的模型，在成功率上提升了31.25%，平均得分提高了24.7%。这些结果突显了时空记忆在提升实体智能体记忆能力方面的有效性。', 'title_zh': 'STMA：一种用于长时 horizon 体态任务规划的空间-时间记忆代理'}
{'arxiv_id': 'arXiv:2502.10148', 'title': 'Cooperative Multi-Agent Planning with Adaptive Skill Synthesis', 'authors': 'Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen', 'link': 'https://arxiv.org/abs/2502.10148', 'abstract': 'Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.', 'abstract_zh': '尽管在训练分布式人工智能（AI）方面取得了显著进展，但使用多智能体强化学习（MARL）构建合作多智能体系统仍面临样本效率、可解释性和迁移性方面的挑战。不同于传统的基于学习的方法需要与环境进行大量的互动，大型语言模型（LLMs）展示了在零-shot 规划和复杂推理方面的卓越能力。然而，现有的基于LLM的方法主要依赖于基于文本的观测，并且在处理部分可观测环境下非马尔可夫性多智能体交互时存在困难。我们提出了一种名为COMPASS的新型多智能体架构，该架构将视觉语言模型（VLMs）与动态技能库和结构化通信结合，以实现去中心化的闭环决策。技能库通过演示初始化，并通过规划者指导的任务逐步进化，从而启用适应性策略。在部分可观测情况下，COMPASS通过多跳通信传播实体信息。在改进的星际争霸多智能体挑战（SMACv2）上的评估结果表明，COMPASS在对称场景中可实现比现有最先进的MARL算法高达30%的胜率。', 'title_zh': '具有自适应技能合成的协作多智能体规划'}
{'arxiv_id': 'arXiv:2502.10097', 'title': 'Causal Information Prioritization for Efficient Reinforcement Learning', 'authors': 'Hongye Cao, Fan Feng, Tianpei Yang, Jing Huo, Yang Gao', 'link': 'https://arxiv.org/abs/2502.10097', 'abstract': "Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.", 'abstract_zh': '当前的强化学习（RL）方法往往遭受样本效率低的困扰，这主要是由于盲目的探索策略未能考虑到状态、动作和奖励之间的因果关系。尽管最近的一些因果方法旨在解决这一问题，但它们缺乏对状态和动作的一贯奖励导向因果理解建模，从而影响了学习效率。为了解决这一问题，我们提出了一种名为因果信息优先级（CIP）的新方法，通过利用事实上的MDPs来推断不同维度的状态和动作与奖励之间的因果关系，从而提高样本效率。具体来说，CIP 识别并利用状态和奖励之间的因果关系执行反事实数据增强，优先考虑在因果环境中具有高影响的状态特征。此外，CIP 整合了一种因果意识的自增强学习目标，这显著增强了代理在复杂环境中基于奖励导向动作执行能力，从而实现更有效的探索。为了全面评估CIP的有效性，我们在5个不同类型的连续控制环境中进行了广泛的实验，包括39个独立任务，涵盖带有像素基和稀疏奖励设置的运动和操作技能学习。实验结果表明，CIP 在广泛的情境下始终优于现有的RL方法。', 'title_zh': '因果信息优先级排序以实现高效的强化学习'}
{'arxiv_id': 'arXiv:2502.10077', 'title': 'Towards Empowerment Gain through Causal Structure Learning in Model-Based RL', 'authors': 'Hongye Cao, Fan Feng, Meng Fang, Shaokang Dong, Tianpei Yang, Jing Huo, Yang Gao', 'link': 'https://arxiv.org/abs/2502.10077', 'abstract': 'In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. We posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. To improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. Importantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.', 'abstract_zh': '在基于模型的强化学习（Model-Based Reinforcement Learning, MBRL）中，将因果结构纳入动力模型中为智能体提供了对环境的结构化理解，从而提高了决策效率。内在动机中的支配力（empowerment）增强了智能体主动控制环境的能力，通过最大化未来状态与动作之间的互信息来实现。我们认为，结合因果理解的支配力能够提高控制能力，进一步增强的支配力收益可以促进基于模型的强化学习（MBRL）中的因果推理。为了提高学习效率和控制能力，我们提出了一种新的框架——因果学习驱动的支配力（Empowerment through Causal Learning, ECL），其中具有因果动力模型意识的智能体实现支配力驱动的探索，并优化其因果结构以服务于任务学习。具体而言，ECL 首先基于采集的数据训练环境的因果动力模型。然后，在因果结构下最大化支配力以实现探索。同时，通过探索收集的数据更新因果动力模型，使其比没有因果结构的密集动力模型更具控制性。在下游任务学习中，我们引入内在的好奇心奖励以平衡因果关系，防止过拟合。重要的是，ECL 是方法通用的，并能与各种因果发现方法集成。我们评估了 ECL 结合三种因果发现方法在六个环境中的性能，包括基于像素的任务，结果表明，ECL 在因果发现、样本效率和渐近性能方面优于其他基于模型的因果强化学习方法。', 'title_zh': '通过基于模型的强化学习中的因果结构学习实现赋能增益'}
{'arxiv_id': 'arXiv:2502.10044', 'title': 'Unsupervised Entity Alignment Based on Personalized Discriminative Rooted Tree', 'authors': 'Yaming Yang, Zhe Wang, Ziyu Guan, Wei Zhao, Xinyan Huang, Xiaofei He', 'link': 'https://arxiv.org/abs/2502.10044', 'abstract': 'Entity Alignment (EA) is to link potential equivalent entities across different knowledge graphs (KGs). Most existing EA methods are supervised as they require the supervision of seed alignments, i.e., manually specified aligned entity pairs. Very recently, several EA studies have made some attempts to get rid of seed alignments. Despite achieving preliminary progress, they still suffer two limitations: (1) The entity embeddings produced by their GNN-like encoders lack personalization since some of the aggregation subpaths are shared between different entities. (2) They cannot fully alleviate the distribution distortion issue between candidate KGs due to the absence of the supervised signal. In this work, we propose a novel unsupervised entity alignment approach called UNEA to address the above two issues. First, we parametrically sample a tree neighborhood rooted at each entity, and accordingly develop a tree attention aggregation mechanism to extract a personalized embedding for each entity. Second, we introduce an auxiliary task of maximizing the mutual information between the input and the output of the KG encoder, to regularize the model and prevent the distribution distortion. Extensive experiments show that our UNEA achieves a new state-of-the-art for the unsupervised EA task, and can even outperform many existing supervised EA baselines.', 'abstract_zh': '实体对齐（Entity Alignment, EA）是指将不同知识图谱（KGs）中的潜在等价实体进行链接。大多数现有的EA方法都是监督学习的，因为它们需要种子对齐的监督，即手动指定的对齐实体对。尽管最近有一些EA研究尝试摆脱种子对齐，但它们仍然存在两个局限性：（1）它们的类似图神经网络（GNN-like）编码器生成的实体嵌入缺乏个性化，因为不同实体之间的某些聚合子路径是共享的。（2）由于缺乏监督信号，它们仍然无法完全缓解候选KG之间的分布畸变问题。在这项工作中，我们提出了一种新的无监督实体对齐方法——UNEA，以解决上述两个问题。首先，我们以每个实体为根节点参数化地抽取树形邻域，并相应地发展了一种树注意力聚合机制，为每个实体提取一个个性化的嵌入。其次，我们引入了一个辅助任务，即最大化输入和KG编码器输出之间的互信息，以正则化模型并防止分布畸变。广泛的实验表明，我们的UNEA在无监督EA任务中达到了新的最佳性能，并且甚至可以优于许多现有的监督EA基线。', 'title_zh': '基于个性化区分性树结构的无监督实体对齐'}
{'arxiv_id': 'arXiv:2502.10038', 'title': 'POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning', 'authors': 'Jiawei Cheng, Jingyuan Wang, Yichuan Zhang, Jiahao Ji, Yuanshao Zhu, Zhibo Zhang, Xiangyu Zhao', 'link': 'https://arxiv.org/abs/2502.10038', 'abstract': 'POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.', 'abstract_zh': 'POI表示学习在处理与用户移动数据相关任务中起着至关重要的作用。近年来的研究表明，通过多模态信息丰富POI表示可以显著提升其在任务中的性能。此前，集成到POI表示中的文本信息通常仅涉及POI类别或签到内容，导致现有方法中的文本特征相对较弱。相比之下，大型语言模型（LLM）通过对大量文本数据的训练，具备丰富的文本知识。然而，利用这些知识来增强POI表示学习存在两个关键挑战：首先，如何有效地从LLM中提取与POI相关知识，其次，如何整合提取的信息以增强POI表示。为了解决这些挑战，我们提出了一种名为POI-Enhancer的可移植框架，该框架利用LLM来改进经典POI学习模型生成的POI表示。我们首先设计了三种专门的提示来高效地从LLM中提取语义信息。接着，双特征对齐模块提升了提取信息的质量，而语义特征融合模块则保持了其完整性。随后，交叉注意力融合模块实现了对这些高质量信息的全面自适应融合，而多视图对比学习进一步注入了人类可理解的语义信息。在三个真实世界数据集上的广泛实验表明了我们框架的有效性，展示了在所有基准表示中都取得了显著的性能提升。', 'title_zh': 'POI-增强器：基于LLM的POI语义增强表示学习框架'}
{'arxiv_id': 'arXiv:2502.10012', 'title': 'Dream to Drive: Model-Based Vehicle Control Using Analytic World Models', 'authors': 'Asen Nachkov, Danda Pani Paudel, Jan-Nico Zaech, Davide Scaramuzza, Luc Van Gool', 'link': 'https://arxiv.org/abs/2502.10012', 'abstract': 'Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.', 'abstract_zh': '不同的可微模拟器近年来在训练自主车辆控制器方面展现出了巨大的潜力。由于可以对其进行反向传播，这些模拟器可以被置于端到端的训练框架中，其已知的动力学可以成为策略学习的有效先验知识，从而消除了对环境的典型黑箱假设。截至目前，这些系统仅被用于训练策略。然而，这并非它们所能提供的全部内容。在此，我们首次尝试使用它们来训练世界模型。具体地，我们提出了三项新的任务设置，以学习下一状态预测器、最优规划器以及最优逆状态。与需要当前动作相对于下一模拟状态的梯度的解析策略梯度（APG）不同，我们提出的方法依赖于当前状态相对于下一状态的梯度。我们称这种方法为解析世界模型（AWM），并展示了其应用，例如如何在Waymax模拟器中使用它进行规划。除了推动使用此类模拟器的极限外，我们还提供了一种改进的训练配方，与基线相比，在Waymo开放交通数据集上实现了高达12%的性能提升，且几乎无需额外成本。', 'title_zh': '标题翻译如下，符合学术规范：\n\n Dreams to Drive: 使用分析型世界模型的基于模型的车辆控制'}
{'arxiv_id': 'arXiv:2502.09994', 'title': 'Decision Information Meets Large Language Models: The Future of Explainable Operations Research', 'authors': 'Yansen Zhang, Qingcan Kang, Wing Yin Yu, Hailei Gong, Xiaojin Fu, Xiongwei Han, Tao Zhong, Chen Ma', 'link': 'https://arxiv.org/abs/2502.09994', 'abstract': 'Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.', 'abstract_zh': '运筹学（OR）对于许多行业的决策制定至关重要。虽然最近的OR方法通过整合大规模语言模型（LLMs）在自动化和效率方面取得了显著进步，但它们仍难以生成有意义的解释。这种缺乏清晰性引发了关于OR应用透明性和可信度的担忧。为应对这些挑战，我们提出了一种全面框架——可解释运筹学（EOR），强调伴随优化过程的可操作性和易理解的解释。EOR的核心概念是决策信息（Decision Information），它源于“假设分析”，并专注于评估复杂约束（或参数）变化对决策制定的影响。具体而言，我们利用二分图来量化运筹学模型的变化，并采用LLMs来提高解释能力。此外，我们还引入了首个工业基准，以严格评估解释和分析在运筹学中的有效性，从而为该领域的透明性和清晰性设定新的标准。', 'title_zh': '决策信息与大型语言模型：可解释运筹学的未来'}
{'arxiv_id': 'arXiv:2502.09974', 'title': 'Has My System Prompt Been Used? Large Language Model Prompt Membership Inference', 'authors': 'Roman Levin, Valeriia Cherepanova, Abhimanyu Hans, Avi Schwarzschild, Tom Goldstein', 'link': 'https://arxiv.org/abs/2502.09974', 'abstract': 'Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.', 'abstract_zh': '提示工程已经成为一种强大的技术，用于优化大型语言模型（LLMs）以满足特定应用需求，从而加快原型设计速度并提高性能，同时引起了社区对保护专有系统提示的兴趣。在此项工作中，我们通过成员推理的角度探索了提示隐私的一个新颖视角。我们开发了提示侦探（Prompt Detective），这是一种统计方法，用于可靠地判断给定的系统提示是否被第三方语言模型使用。我们的方法依赖于比较不同系统提示对应的两种模型输出分布的统计测试。通过使用各种语言模型进行广泛实验，我们展示了提示侦探在提示成员推理方面的有效性。我们的研究结果表明，即使微小的系统提示更改也会导致不同的响应分布，从而使我们能够以统计显著性验证提示的使用情况。', 'title_zh': '我的系统提示被使用了吗？大型语言模型提示成员 inference'}
{'arxiv_id': 'arXiv:2502.09955', 'title': 'Diverse Inference and Verification for Advanced Reasoning', 'authors': 'Iddo Drori, Gaston Longhitano, Mao Mao, Seunghwan Hyun, Yuke Zhang, Sungjun Park, Zachary Meeks, Xin-Yu Zhang, Ben Segev, Howard Yong, Nakul Verma, Avi Shporer, Alon Amit, Madeleine Udell', 'link': 'https://arxiv.org/abs/2502.09955', 'abstract': "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.", 'abstract_zh': '像OpenAI的o1和o3以及DeepSeek的R1这样的大型语言模型（LLMs）在数学和编程领域取得了显著进展，但在解决国际数学奥林匹克（IMO）组合数学问题、抽象与推理语料库（ARC）挑战以及人类最后一考（HLE）的问题等高级任务中仍面临挑战。我们采用了一种多样化的推理方法，在测试时结合了多种模型和方法。我们发现，验证数学和编程问题的答案，并在其他问题上使用拒绝采样，简单且有效。我们通过Lean自动验证IMO问题和ARC谜题的答案正确性，并发现“最佳-N”方法能够有效回答HLE问题。我们的方法将IMO组合数学问题的解答准确性从33.3%提高到77.8%，将HLE问题的解答准确性从8%提高到37%。此外，该方法解决了948名人类无法解答的80%的ARC谜题，以及o3高算力无法解决的26.5%的ARC谜题。通过测试时的模拟、强化学习和基于推理反馈的元学习，我们改进了泛化能力，通过调整代理图的表示和提示、代码和数据集，从而适应不同的任务需求。我们的方法可靠、稳健且具有扩展性。秉承可再现研究的精神，我们将公开发布我们的方法。', 'title_zh': '高级推理中的多样推断与验证'}
{'arxiv_id': 'arXiv:2502.09947', 'title': 'Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage Encoding Model', 'authors': 'Jin Cui, Alexander Capstick, Payam Barnaghi, Gregory Scott', 'link': 'https://arxiv.org/abs/2502.09947', 'abstract': 'In the analysis of remote healthcare monitoring data, time series representation learning offers substantial value in uncovering deeper patterns of patient behavior, especially given the fine temporal granularity of the data. In this study, we focus on a dataset of home activity records from people living with Dementia. We propose a two-stage self-supervised learning approach. The first stage involves converting time-series activities into text strings, which are then encoded by a fine-tuned language model. In the second stage, these time-series vectors are bi-dimensionalized for applying PageRank method, to analyze latent state transitions to quantitatively assess participants behavioral patterns and identify activity biases. These insights, combined with diagnostic data, aim to support personalized care interventions.', 'abstract_zh': '在远程医疗监控数据的分析中，时间序列表示学习能够揭示患者行为中的深层次模式，尤其是在数据具有精细时间粒度的情况下。本研究专注于失智症患者家庭活动记录的数据集。我们提出了一种两阶段的半监督学习方法。第一阶段将时间序列活动转换为文本字符串，随后由微调后的语言模型进行编码。第二阶段，将这些时间序列向量二维化，应用PageRank方法来分析潜在状态转移，以定量评估参与者的 behavioral 模式并识别活动偏差。这些洞察与诊断数据结合，旨在支持个性化的护理干预。', 'title_zh': '使用两阶段编码模型分析患者日常运动行为动力学'}
{'arxiv_id': 'arXiv:2502.09933', 'title': "MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning", 'authors': 'Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen', 'link': 'https://arxiv.org/abs/2502.09933', 'abstract': 'Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $<$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.', 'abstract_zh': '归纳推理（IR），即从实例中总结规则并应用于新情况的能力，长期以来被认为是一般智能的基本能力，并受到认知科学和AI研究人员的广泛关注。许多基准测试已提出以衡量这种能力；然而，这些基准主要集中在少量样本（通常<10例）的设置上，并且缺乏对来自长上下文信息的多信息聚合的评估。另一方面，LLMs不断增加的上下文长度催生了新的范式——多示例上下文学习（Many-Shot In-Context Learning, M-ICL），该范式利用成百上千的示例来解决新任务，无需昂贵且低效的微调。然而，大多数多示例评估主要集中在分类（IR的一个非常有限的方面），而广泛使用的大上下文LLMs任务，如“稻草中的针”（Needle-In-A-Haystack, NIAH），很少需要复杂的智能来整合大量信息。为了弥补这两者的不足，我们提出了MIR-Bench，这是首个针对归纳推理的多示例上下文基准测试，要求LLMs通过输入输出示例从具有多种数据格式的基本函数中归纳输出。基于MIR-Bench，我们研究了许多归纳推理和多示例上下文学习的新问题，包括在错误示例下的鲁棒性和思维链（Chain-of-Thought, CoT）的效应，并获得了深刻的认识。', 'title_zh': 'MIR-Bench：通过多次-shot 内省归纳推理评估大规模语言模型的长上下文智能'}
{'arxiv_id': 'arXiv:2502.09913', 'title': 'AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for Web-based Source Search', 'authors': 'Zhengqiu Zhu, Yatai Ji, Jiaheng Huang, Yong Zhao, Sihang Qiu, Rusheng Ju', 'link': 'https://arxiv.org/abs/2502.09913', 'abstract': 'Web-based management systems have been widely used in risk control and industrial safety. However, effectively integrating source search capabilities into these systems, to enable decision-makers to locate and address the hazard (e.g., gas leak detection) remains a challenge. While prior efforts have explored using web crowdsourcing and AI algorithms for source search decision support, these approaches suffer from overheads in recruiting human participants and slow response times in time-sensitive situations. To address this, we introduce AutoS$^2$earch, a novel framework leveraging large models for zero-shot source search in web applications. AutoS$^2$earch operates on a simplified visual environment projected through a web-based display, utilizing a chain-of-thought prompt designed to emulate human reasoning. The multi-modal large language model (MLLMs) dynamically converts visual observations into language descriptions, enabling the LLM to perform linguistic reasoning on four directional choices. Extensive experiments demonstrate that AutoS$^2$earch achieves performance nearly equivalent to human-AI collaborative source search while eliminating dependency on crowdsourced labor. Our work offers valuable insights in using web engineering to design such autonomous systems in other industrial applications.', 'abstract_zh': '基于Web的管理系统在风险控制和工业安全领域得到了广泛应用。然而，有效地将源搜索能力整合进这些系统，以使决策者能够定位并解决安全隐患（例如，气体泄漏检测）仍然具有挑战性。虽然先前的努力探索了使用网络众包和AI算法来支持源搜索决策，但这些方法在招募人类参与者方面存在负担，并且在时间敏感的情况下响应速度较慢。为解决这一问题，我们提出了一种名为AutoS$^2$earch的新颖框架，该框架利用大规模模型在Web应用程序中进行零样本源搜索。AutoS$^2$earch在基于Web的显示中应用简化视觉环境，利用一个链式思考提示来模仿人类推理。多模态大规模语言模型（Multi-Modal Large Language Models, MLLMs）动态地将视觉观察转化为语言描述，从而允许LLM在四个方向选择上进行语言推理。大量的实验表明，AutoS$^2$earch在性能上几乎与人类-AI协作的源搜索相当，并且消除了对众包劳动力的依赖。我们的研究为利用Web工程设计此类自主系统提供了宝贵见解，在其他工业应用中也具有重要意义。', 'title_zh': 'AutoS$^2$earch：解锁大型模型在基于网页的源搜索中的推理潜力'}
{'arxiv_id': 'arXiv:2502.09903', 'title': 'The Ann Arbor Architecture for Agent-Oriented Programming', 'authors': 'Wei Dong', 'link': 'https://arxiv.org/abs/2502.09903', 'abstract': 'In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.', 'abstract_zh': '在本文中，我们通过自动机理论的视角重新审视大型语言模型的提示工程。我们认为语言模型本质上是自动机，并且应该像所有自动机一样，在它们能接受的统一语言集合（包括所有自然语言和形式语言）中进行编程。因此，传统的软件工程实践——基于编程语言和自然语言之间的明确区分——必须进行重新思考。我们提出了安阿伯架构（Ann Arbor Architecture），这是一种高层抽象的代理导向编程框架，用于语言模型，超越了原始词汇生成，并为上下文学习提供了新的视角。基于这一框架，我们设计了我们的代理平台Postline，并报告了初步的代理训练实验。', 'title_zh': '安阿伯代理导向编程架构'}
{'arxiv_id': 'arXiv:2502.09897', 'title': 'Artificial Intelligence in Spectroscopy: Advancing Chemistry from Prediction to Generation and Beyond', 'authors': 'Kehan Guo, Yili Shen, Gisela Abigail Gonzalez-Montiel, Yue Huang, Yujun Zhou, Mihir Surve, Zhichun Guo, Prayel Das, Nitesh V Chawla, Olaf Wiest, Xiangliang Zhang', 'link': 'https://arxiv.org/abs/2502.09897', 'abstract': 'The rapid advent of machine learning (ML) and artificial intelligence (AI) has catalyzed major transformations in chemistry, yet the application of these methods to spectroscopic and spectrometric data, referred to as Spectroscopy Machine Learning (SpectraML), remains relatively underexplored. Modern spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing volume of high-dimensional data, creating a pressing need for automated and intelligent analysis beyond traditional expert-based workflows. In this survey, we provide a unified review of SpectraML, systematically examining state-of-the-art approaches for both forward tasks (molecule-to-spectrum prediction) and inverse tasks (spectrum-to-molecule inference). We trace the historical evolution of ML in spectroscopy, from early pattern recognition to the latest foundation models capable of advanced reasoning, and offer a taxonomy of representative neural architectures, including graph-based and transformer-based methods. Addressing key challenges such as data quality, multimodal integration, and computational scalability, we highlight emerging directions such as synthetic data generation, large-scale pretraining, and few- or zero-shot learning. To foster reproducible research, we also release an open-source repository containing recent papers and their corresponding curated datasets (this https URL). Our survey serves as a roadmap for researchers, guiding progress at the intersection of spectroscopy and AI.', 'abstract_zh': '机器学习（ML）和人工智能（AI）的迅速崛起在化学领域引发了重大变革，然而这些方法在光谱和光谱学数据中的应用，即光谱机器学习（SpectraML），仍相对未被充分利用。现代光谱技术（如MS、NMR、IR、Raman、UV-Vis）产生了越来越多的高维数据，这迫切需要超出传统专家工作流程的自动化和智能分析。在这篇综述中，我们提供了一种统一的SpectraML概述，系统地审查了最先进的正向任务（分子到光谱预测）和逆向任务（光谱到分子推断）方法。我们追溯了ML在光谱学中的历史演变，从早期的模式识别到最新的支持高级推理的基础模型，并提供了一种代表性神经架构的分类，包括图基元和基于变换的方法。我们分析了关键挑战，如数据质量、多模态整合和计算扩展性，并强调新兴方向，如合成数据生成、大规模预训练和少量或零样本学习。为促进可重复研究，我们还公开了一个包含最新论文及其相应的数据集的资源库（详见此处：[此链接]）。我们的综述为研究者建立了通往光谱学和AI交叉领域的路线图。', 'title_zh': '人工智能在光谱学中的应用：从预测到生成及更远的发展'}
{'arxiv_id': 'arXiv:2502.09861', 'title': 'A Scoresheet for Explainable AI', 'authors': 'Michael Winikoff, John Thangarajah, Sebastian Rodriguez', 'link': 'https://arxiv.org/abs/2502.09861', 'abstract': 'Explainability is important for the transparency of autonomous and intelligent systems and for helping to support the development of appropriate levels of trust. There has been considerable work on developing approaches for explaining systems and there are standards that specify requirements for transparency. However, there is a gap: the standards are too high-level and do not adequately specify requirements for explainability. This paper develops a scoresheet that can be used to specify explainability requirements or to assess the explainability aspects provided for particular applications. The scoresheet is developed by considering the requirements of a range of stakeholders and is applicable to Multiagent Systems as well as other AI technologies. We also provide guidance for how to use the scoresheet and illustrate its generality and usefulness by applying it to a range of applications.', 'abstract_zh': '解释性对于自主智能系统的透明度至关重要，并有助于支持对适当信任水平的建立。已经开展了大量的工作来开发解释系统的方法，并制定了关于透明度的要求标准。然而，存在一个差距：这些标准过于宏观，并未充分具体化解释性要求。本文提出了一种评分表，可用于制定解释性要求或将该评分表应用于特定应用以评估其解释性方面。该评分表通过考虑一系列相关方的需求而开发，并适用于多智能体系统以及其他人工智能技术。我们还提供了如何使用该评分表的指导，并通过将其应用于多种应用来展示其普遍性和实用性。', 'title_zh': '可解释人工智能的评分表'}
{'arxiv_id': 'arXiv:2502.09843', 'title': 'MuDoC: An Interactive Multimodal Document-grounded Conversational AI System', 'authors': 'Karan Taneja, Ashok K. Goel', 'link': 'https://arxiv.org/abs/2502.09843', 'abstract': "Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations.", 'abstract_zh': '多模态AI是朝着通过利用人类与AI通信中多种模态构建有效工具迈出的重要一步。构建一个能够与长文档交互的多模态文档导向型AI系统仍然是一项挑战。我们旨在填补直接利用文档中的目标视觉内容与文本内容进行响应生成这一研究空白。我们基于GPT-4o提出了一个名为“MuDoC”的互动对话AI代理，能够生成交织文本与图表的文档导向型响应。MuDoC的智能教科书界面增强了系统的可信度，并允许用户即时导航到文档中的源文本和图表进行验证。我们还基于MuDoC的响应进行了定性的观察，以突出其优势和局限性。', 'title_zh': 'MuDoC：一种交互式多模态文档导向对话AI系统'}
{'arxiv_id': 'arXiv:2502.09649', 'title': 'Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning', 'authors': 'Yuhang Dong, Haizhou Ge, Yupei Zeng, Jiangning Zhang, Beiwen Tian, Guanzhong Tian, Hongrui Zhu, Yufei Jia, Ruixiang Wang, Ran Yi, Guyue Zhou, Longhua Ma', 'link': 'https://arxiv.org/abs/2502.09649', 'abstract': 'Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent this http URL evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.', 'abstract_zh': '视觉运动模仿学习使具身代理能够有效从视频示范和机器人本体感觉中获取操作技能。然而，随着场景复杂性和视觉干扰的增加，现有在简单场景中表现良好的方法在复杂场景中的性能往往会下降。为解决这一挑战，我们引入了Imit Diff，一个具有双分辨率融合的语义指导式扩散变换器，用于模仿学习。我们的方法利用来自视觉语言基础模型的先验知识，将高层次的语义指令翻译为像素级的视觉定位。这些信息被明确地集成到一个多层次的视觉增强框架中，该框架由一个双分辨率编码器构建。此外，我们在扩散变换器架构中引入了一致性策略的实施，以提高具身代理的实时性能和运动平滑度。我们通过几个具有挑战性的现实任务对Imit Diff进行了评估。由于其任务导向的视觉定位和细腻的场景感知，它在复杂且存在视觉干扰的场景中显著优于现有最先进的方法，特别是在零样本实验和类别泛化方面。相关代码将公开发布。', 'title_zh': 'Imit Diff: 受语义指导的双分辨率融合扩散变换器在模仿学习中的应用'}
{'arxiv_id': 'arXiv:2502.09624', 'title': 'Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach', 'authors': 'Jiawen Kang, Jiana Liao, Runquan Gao, Jinbo Wen, Huawei Huang, Maomao Zhang, Changyan Yi, Tao Zhang, Dusit Niyato, Zibin Zheng', 'link': 'https://arxiv.org/abs/2502.09624', 'abstract': 'By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.', 'abstract_zh': '通过将移动网络和具身人工智能（Embodied AI）协同整合，具身人工智能网络（MEANETs）代表了一个先进的范式，能够促进在动态环境中的自主、上下文感知和交互行为。然而，MEANETs的迅速发展伴随着信任和操作效率方面的挑战。幸运的是，区块链技术由于其去中心化和不可篡改的特性，为MEANETs提供了有前景的解决方案。然而，现有的区块传播机制面临的挑战包括低传播效率和区块传播安全性不足，这会导致区块链启用MEANETs中的车辆消息传递延迟，或者容易受到恶意篡改，从而可能造成严重的交通事故。此外，当前的区块传播策略无法有效地适应MEANETs中动态拓扑结构的实时变化。因此，在本文中，我们提出了一个基于图重新融合模型的信任区块传播优化框架，适用于共识区块链启用的MEANETs。具体来说，我们提出了一种基于信任云模型的信任计算机制，该机制全面考虑了矿工信任评估中的随机性和模糊性。此外，通过利用图神经网络和扩散模型的优势，我们开发了一个图重新融合模型，以有效地和适应性地生成最优的区块传播轨迹。仿真结果表明，所提出的模型在区块传播效率和可信度方面优于其他路由机制。此外，结果强调其对动态环境的强大适应性，使其特别适用于快速变化的MEANETs。', 'title_zh': '基于区块链的移动具身人工智能网络中高效可靠的数据块传播：一种图重构方法'}
{'arxiv_id': 'arXiv:2502.10389', 'title': 'Region-Adaptive Sampling for Diffusion Transformers', 'authors': 'Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang', 'link': 'https://arxiv.org/abs/2502.10389', 'abstract': "Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.", 'abstract_zh': '扩散模型（DMs）已经成为跨多个领域的生成任务的首选方法。然而，它们依赖于多个连续的前向传递，这在实时性能上受到了显著限制。此前的加速方法主要集中在减少采样步数或重用中间结果，但由于卷积U-Net结构的限制，这些方法未能利用图像区域之间的空间变异性。通过利用扩散变压器（DiTs）处理可变数量标记的灵活性，我们提出了RAS，这是一种全新的、无需训练的采样策略，该策略根据DiT模型的焦点动态地为图像中的不同区域分配不同的采样比例。我们一个关键的观察是，在每次采样步骤中，模型集中在语义上重要的区域，这些关注区域在连续步骤之间表现出很强的连续性。利用这一洞察，RAS仅更新当前处于焦点的区域，而其他区域则使用前一步骤缓存的噪声进行更新。模型的焦点是根据前一个步骤的输出确定的，利用了我们观察到的时间一致性。我们在Stable Diffusion 3和Lumina-Next-T2I上评估了RAS，分别实现了最高2.36倍和2.51倍的速度提升，同时生成质量下降幅度最小。此外，用户研究结果表明，尽管RAS在人机评价中表现出相似的品质，但其速度提高了1.6倍。我们的方法为更高效的扩散变压器奠定了重要基础，增强了它们在实时应用中的潜力。', 'title_zh': '区域自适应采样用于扩散变换器'}
{'arxiv_id': 'arXiv:2502.10385', 'title': 'Simplifying DINO via Coding Rate Regularization', 'authors': 'Ziyang Wu, Jingyuan Zhang, Druv Pai, XuDong Wang, Chandan Singh, Jianwei Yang, Jianfeng Gao, Yi Ma', 'link': 'https://arxiv.org/abs/2502.10385', 'abstract': 'DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable -- many hyperparameters need to be carefully tuned to ensure that the representations do not collapse -- which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.', 'abstract_zh': 'DINO和DINOv2是两个广泛用于大规模无标签图像数据中学习表示的模型系列。它们学到的表示通常能够为下游任务（如图像分类和分割）带来最先进的性能。然而，这些模型采用了许多基于经验的设计选择，其训练管道非常复杂且不稳定——许多超参数需要仔细调整以确保表示不归零——这使得改进这些模型或将其适应到新的领域变得相当困难。在这项工作中，我们认为可以移除预训练管道中的大部分这种经验驱动的设计特性，并只需在损失函数中增加一个明确的编码率项来避免表示的归零。结果，我们得到了DINO和DINOv2的简化变体，分别称为SimDINO和SimDINOv2。令人惊讶的是，这些简化模型对不同的设计选择（如网络架构和超参数）更为稳健，并且通过下游任务的性能测量学习了更高质量的表示，这相比DINO和DINOv2模型形成了帕累托改进。这项工作强调了使用简化设计原则来提高深度学习的实践的潜力。', 'title_zh': '通过编码率正则化简化DINO'}
{'arxiv_id': 'arXiv:2502.10373', 'title': 'OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models', 'authors': 'William Chen, Jinchuan Tian, Yifan Peng, Brian Yan, Chao-Han Huck Yang, Shinji Watanabe', 'link': 'https://arxiv.org/abs/2502.10373', 'abstract': 'Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on this https URL for future studies.', 'abstract_zh': '神经网络的扩展规律为设计稳健的序列处理架构提供了宝贵见解。尽管这些规律在其他模态中已经得到了广泛的研究，但在语音中的行为仍然相对未被充分探索。在此项工作中，我们介绍了OWLS，这是一个开放访问、可重复使用的多语言语音识别和翻译模型套件，参数范围从0.25亿到18亿，18亿参数版本据我们所知是最大的语音模型。OWLS 利用了来自150种语言的多达36万小时的公开语音数据，使我们能够系统地研究数据、模型和计算量扩展如何影响多语言语音任务的性能。我们利用OWLS 推导了神经网络的扩展规律，显示了在扩展时最终性能可以可靠地预测。我们的一个重要发现是，扩展可以提高低资源语言/方言的表现，有助于减轻偏见并提高语音技术的可访问性。最后，我们展示了OWLS 如何通过发现大规模语音模型中的新兴能力来推动新的研究方向。模型检查点将在此网址 <https://.../> 供未来的研究使用。', 'title_zh': 'OWLS：多语言语音识别和翻译模型的扩展规律'}
{'arxiv_id': 'arXiv:2502.10363', 'title': 'BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds', 'authors': 'Huayi Wang, Zirui Wang, Junli Ren, Qingwei Ben, Tao Huang, Weinan Zhang, Jiangmiao Pang', 'link': 'https://arxiv.org/abs/2502.10363', 'abstract': 'Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still face great challenges on complex terrains due to sparse foothold reward signals and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trail-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.', 'abstract_zh': '在稀疏脚着地点的危险地形上行走对类人机器人构成了重大挑战，要求其具备精确的足部放置和稳定的运动能力。现有的针对四足机器人设计的方法往往由于脚部几何形状和不稳定形态的不同而无法泛化到类人机器人，而基于学习的方法在复杂地形上进行类人行走时，仍然面临因稀疏脚着地点奖励信号和低效学习过程带来的巨大挑战。为应对这些挑战，我们引入了**BeamDojo**，这是一个旨在使类人机器人在稀疏脚着地点上实现敏捷运动的强化学习（RL）框架。BeamDojo 通过引入一种针对多边形脚设计的采样基础脚着地点奖励，以及引入双重评论家来平衡密集运动奖励和稀疏脚着地点奖励之间的学习过程。为了促进充分的试探性探索，BeamDojo 实现了一种两阶段的RL方法：第一阶段通过在平坦地形上训练类人而在同时提供环境地形感知信息来放松地形动力学，第二阶段在实际任务地形上微调决策策略。此外，我们还实现了基于机载LiDAR的高程图以实现现实世界的部署。广泛的仿真实验和实地测试表明，BeamDojo 在仿真中实现了高效学习，并且在实际世界中能够实现精确足部放置的敏捷运动，即使在显著的外界干扰下也能保持较高的成功率。', 'title_zh': 'BeamDojo：在稀疏支撑点上学习灵活的人形运动'}
{'arxiv_id': 'arXiv:2502.10339', 'title': 'STAR: Spectral Truncation and Rescale for Model Merging', 'authors': 'Yu-Ang Lee, Ching-Yun Ko, Tejaswini Pedapati, I-Hsin Chung, Mi-Yen Yeh, Pin-Yu Chen', 'link': 'https://arxiv.org/abs/2502.10339', 'abstract': "Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd $\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is publicly available at this https URL.", 'abstract_zh': '模型合并是一种无需进一步微调即可从多个预训练模型中高效获得多任务模型的方法，并且已经在自然语言处理（NLP）等多个领域引起了广泛关注。尽管模型合并具有高效性，但其关键挑战之一是在模型数量增加时任务性能似乎不可避免地下降。在本文中，我们提出了一种名为Spectral Truncation and Rescale (STAR)的方法，旨在通过在各自的谱空间中裁剪小型组件来缓解“合并冲突”，并在之后采用一个自动的参数缩放方案以保留原始矩阵的核范数。STAR不需要对原始训练数据进行额外的推理，并且对超参数选择具有鲁棒性。我们通过在多种NLP任务上的广泛模型合并案例展示了STAR的有效性。具体而言，STAR在不同模型大小的情况下表现稳定，并且在合并12个模型的Flan-T5任务上比基线高出4.2%。我们的代码已公开，地址为：this https URL。', 'title_zh': 'STAR：谱截断与重标定以实现模型合并'}
{'arxiv_id': 'arXiv:2502.10338', 'title': 'Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering', 'authors': 'Nick Ferguson, Liane Guillou, Alan Bundy, Kwabena Nuamah', 'link': 'https://arxiv.org/abs/2502.10338', 'abstract': 'Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and reframe them in terms of meta-level reasoning (akin to high-level strategic reasoning or planning) and object-level reasoning (embodied in lower-level tasks such as mathematical reasoning). Franklin, a novel dataset with requirements of meta- and object-level reasoning, is introduced and used along with three other datasets to evaluate four LLMs at question answering tasks requiring multiple steps of reasoning. Results from human annotation studies suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks in some of the datasets used. Additionally, evidence suggests that LLMs find the object-level reasoning required for the questions in the Franklin dataset challenging, yet they do exhibit strong performance with respect to the meta-level reasoning requirements.', 'abstract_zh': '大型语言模型（LLMs）在自然语言任务中表现出色，但在需要复杂多步推理的问答（QA）任务中仍面临挑战。我们概述了这些任务中所需的各种推理类型，并将其重新表述为元级推理（类似于高层次的战略性推理或计划制定）和对象级推理（体现在较低层级的任务中，如数学推理）。介绍了一个名为Franklin的新数据集，该数据集包含元级和对象级推理的要求，并与另外三个数据集一起用于评估四款LLM在需要多步推理的问答任务中的表现。来自人类注释研究的结果表明，LLMs在元级推理方面表现出较高的频率，但在某些数据集中，对象级推理任务的表现却较为挣扎。此外，证据表明，LLMs在Franklin数据集中所需的对象级推理任务上面临的挑战是显著的，但它们在元级推理要求方面表现出较强的能力。', 'title_zh': '评估大型语言模型在问答任务中的元级和对象级推理能力'}
{'arxiv_id': 'arXiv:2502.10325', 'title': 'Process Reward Models for LLM Agents: Practical Framework and Directions', 'authors': 'Sanjiban Choudhury', 'link': 'https://arxiv.org/abs/2502.10325', 'abstract': 'We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: this https URL.', 'abstract_zh': '我们引入了代理过程奖励模型（AgentPRM）——一种简洁且可扩展的框架，用于训练大规模语言模型代理通过交互不断改进。AgentPRM 采用轻量级的演员-批评家范式，并利用蒙特卡洛展开来计算奖励目标并优化策略。它只需对现有的 RLHF 流程进行最少的修改，使其易于大规模集成。除了 AgentPRM，我们还提出了逆向过程奖励模型（InversePRM），该模型直接从演示学习过程奖励，而无需显式的结果监督。我们还探索了关键的挑战和机会，包括探索、过程奖励塑造和模型预测推理。我们使用 ALFWorld 基准进行评估，并展示了使用 AgentPRM 和 InversePRM 训练的小型 3B 模型超越了强大的 GPT-4o 基线模型的结果。我们分析了测试时缩放、奖励劫持等问题，并提供了更多细节。我们的代码可从以下链接获取：[提供链接]\n\n翻译如下：\n\n我们引入了代理过程奖励模型(AgentPRM)，这是一种简单且可扩展的框架，用于通过交互训练大规模语言模型代理使其不断改进。AgentPRM 采用轻量级的演员-批评家范式，使用蒙特卡洛展开计算奖励目标并优化策略。它只需要对现有的 RLHF 流程进行最小的修改，使其在大规模部署中易于集成。除了 AgentPRM，我们还提出了逆过程奖励模型(InversePRM)，它直接从演示中学习过程奖励，而无需显式的结果监督。我们还探索了关键的挑战和机遇，包括探索、过程奖励塑造和模型预测推理。我们使用 ALFWorld 基准对这些模型进行了评估，结果表明使用 AgentPRM 和 InversePRM 训练的小型 3B 模型优于强大的 GPT-4o 基线。我们分析了测试时扩展、奖励劫持等问题，并进行了更多的探讨。我们的代码库可以在以下链接处获得：[提供链接]', 'title_zh': '面向LLM代理的过程奖励模型：实用框架与发展方向'}
{'arxiv_id': 'arXiv:2502.10311', 'title': 'ExplainReduce: Summarising local explanations via proxies', 'authors': 'Lauri Seppäläinen, Mudong Guo, Kai Puolamäki', 'link': 'https://arxiv.org/abs/2502.10311', 'abstract': 'Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small "proxy set" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.', 'abstract_zh': '大多数常用的非线性机器学习方法是封闭模型，对人类是不可解释的。人工智能可解释性（XAI）领域的目标是开发工具来揭示这些封闭模型内部的工作原理。一种常用的模型无关的XAI方法是使用简单的模型作为局部近似，以产生所谓的局部解释；这种方法的实例包括LIME、SHAP和SLISEMAP。本文展示了如何将大量局部解释简化为一组简单的“代理模型”，这些模型可以作为生成性的全局解释。这一简化过程称为ExplainReduce，并可以表述为一个优化问题，并可以通过贪婪启发式方法高效地近似求解。', 'title_zh': 'ExplainReduce：通过代理总结局部解释'}
{'arxiv_id': 'arXiv:2502.10284', 'title': 'A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems', 'authors': 'Binglei Zhao, Houying Qi, Guang Xu, Mian Ma, Xiwei Zhao, Feng Mei, Sulong Xu, Jinghe Hu', 'link': 'https://arxiv.org/abs/2502.10284', 'abstract': 'Large-scale recommendation systems often adopt cascading architecture consisting of retrieval, pre-ranking, ranking, and re-ranking stages. With strict latency requirements, pre-ranking utilizes lightweight models to perform a preliminary selection from massive retrieved candidates. However, recent works focus solely on improving consistency with ranking, relying exclusively on downstream stages. Since downstream input is derived from the pre-ranking output, they will exacerbate the sample selection bias (SSB) issue and Matthew effect, leading to sub-optimal results. To address the limitation, we propose a novel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate information from upstream (retrieval) and downstream (ranking, re-ranking) stages. Specifically, cross-stage coordination refers to the pre-ranking\'s adaptability to the entire stream and the role of serving as a more effective bridge between upstream and downstream. HCCP consists of Hybrid Sample Construction and Hybrid Objective Optimization. Hybrid sample construction captures multi-level unexposed data from the entire stream and rearranges them to become the optimal guiding "ground truth" for pre-ranking learning. Hybrid objective optimization contains the joint optimization of consistency and long-tail precision through our proposed Margin InfoNCE loss. It is specifically designed to learn from such hybrid unexposed samples, improving the overall performance and mitigating the SSB issue. The appendix describes a proof of the efficacy of the proposed loss in selecting potential positives. Extensive offline and online experiments indicate that HCCP outperforms SOTA methods by improving cross-stage coordination. It contributes up to 14.9% UCVR and 1.3% UCTR in the JD E-commerce recommendation system. Concerning code privacy, we provide a pseudocode for reference.', 'abstract_zh': '大规模推荐系统通常采用递归架构，包括检索、预排序、排序和再排序阶段。由于严格的延迟要求，预排序阶段利用轻量级模型从大量检索候选中进行初步筛选。然而，近期的研究仅专注于提高与排序阶段的一致性，完全依赖于下游阶段。由于下游输入源自预排序输出，这将加剧样本选择偏差（SSB）问题和 Matthew 效应，导致结果次优。为解决这一局限性，我们提出了一种新颖的跨阶段协同预排序模型（HCCP，Hybrid Cross-Stage Coordination Pre-ranking），以整合来自上游（检索）和下游（排序、再排序）阶段的信息。具体而言，跨阶段协调指的是预排序模型在整个流中的适应性，以及它在促进上游和下游衔接方面的关键作用。HCCP 包括混合样本构建和混合目标优化。混合样本构建从整个流中捕获多级未曝光的数据，并重新排列它们，成为预排序学习的最优指导“地面真实”。混合目标优化通过我们提出的 Margin InfoNCE 损失，实现一致性与长尾精度的联合优化。该目标是专门为了从这些混合未曝光样本中学习，从而提高整体性能并缓解 SSB 问题。附录中描述了所提损失在选择潜在正样本方面的有效性证明。大量离线和在线实验表明，HCCP 比现有最优方法（SOTA）在提高跨阶段协调方面表现出更优越的性能，在 JD 电商推荐系统中分别贡献了最高 14.9% 的 UCVR 和 1.3% 的 UCTR。关于代码隐私，我们提供了伪代码供参考。', 'title_zh': '在线推荐系统中的一种混合跨阶段协调预排模型'}
{'arxiv_id': 'arXiv:2502.10273', 'title': 'Probing Perceptual Constancy in Large Vision Language Models', 'authors': 'Haoran Sun, Suyang Yu, Yijiang Li, Qingying Gao, Haiyun Lyu, Hokin Deng, Dezhi Luo', 'link': 'https://arxiv.org/abs/2502.10273', 'abstract': "Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for recognizing visual information in a dynamic world, making it essential for Vision-Language Models (VLMs). However, whether VLMs are currently and theoretically capable of mastering this ability remains underexplored. In this study, we evaluated 33 VLMs using 253 experiments across three domains: color, size, and shape constancy. The experiments included single-image and video adaptations of classic cognitive tasks, along with novel tasks in in-the-wild conditions, to evaluate the models' recognition of object properties under varying conditions. We found significant variability in VLM performance, with models performance in shape constancy clearly dissociated from that of color and size constancy.", 'abstract_zh': '感知恒常性是指在感官输入发生变化（如距离、角度或光照的变化）时，维持对物体稳定感知的能力。这种能力在动态世界中识别视觉信息时至关重要，因此对视觉语言模型（VLMs）来说至关重要。然而，现有的VLMs是否能够掌握这种能力，无论是实践上还是理论上，仍需进一步探索。本研究中，我们使用253项实验评估了33种VLM在三个领域中的表现：颜色恒常性、大小恒常性和形状恒常性。实验包括对经典认知任务的单图像和视频改编，以及在自然场景中的新任务，以评估模型在不同条件下对物体属性的识别能力。研究结果表明，VLMs的性能存在显著差异，形状恒常性的表现与颜色和大小恒常性明显不同。', 'title_zh': '探查大型视觉语言模型中的知觉恒常性'}
{'arxiv_id': 'arXiv:2502.10266', 'title': 'Are Large Language Models the future crowd workers of Linguistics?', 'authors': 'Iris Ferrazzo', 'link': 'https://arxiv.org/abs/2502.10266', 'abstract': "Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs. For these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.", 'abstract_zh': '人类参与者的数据提取是实证语言研究中核心的数据收集策略之一。此类研究的参与者数量可能会有很大差异，从少量参与者到大规模的众包数据。尽管这些设置能够提供丰富详尽的数据，但它们也伴随着诸多劣势，例如在任务完成过程中难以控制参与者的注意力、众包环境中恶劣的工作条件以及耗时的实验设计。鉴于此，本研究旨在探讨是否可以通过将大型语言模型（LLMs）纳入实证语言研究管道中来克服这些障碍。本研究通过两个再现案例研究来阐明这一问题：Cruz（2023）和Lombard等人（2021）。基于OpenAI的GPT-4o-mini模型，原设计用于人类参与者的两个强制数据提取任务在这项提议的框架中得以重现。使用零样本提示基准测试的结果显示了LLMs的有效性和高度的多功能性，这些模型在语言任务中往往优于人类信息提供者。第二次再现研究进一步阐明了探索其他提示技术（如思维链提示）的必要性，这些技术在后续的实验中显示出与人类表现更好的一致性，尤其是在关键项目和填充项目上。鉴于本研究规模有限，有必要进一步探讨LLMs在实证语言学以及其他人文学科未来应用中的性能。', 'title_zh': '大型语言模型是未来语言学众包工作者吗？'}
{'arxiv_id': 'arXiv:2502.10263', 'title': 'Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers', 'authors': 'Aivin V. Solatorio, Rafael Macalaba, James Liounis', 'link': 'https://arxiv.org/abs/2502.10263', 'abstract': 'Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.', 'abstract_zh': '跟踪科研论文中数据的提及和使用情况提供了提高数据可发现性、质量和生产性的关键见解。然而，手动识别和分类大量学术文献中的数据集提及是一项资源密集且不可扩展的任务。本文提出了一种基于机器学习的框架，通过利用大型语言模型（LLMs）、合成数据和两阶段微调过程，在不同研究领域自动检测数据集提及。我们采用了零样本提取、LLM作为裁判进行质量评估，并使用推理代理进行进一步细化，生成了一个弱监督合成数据集。我们先在该数据集上进行预微调Phi-3.5-mini指令模型，然后在手动标注的子集上进行微调。在推断过程中，基于ModernBERT的分类器高效地过滤出数据集提及，降低了计算开销同时保持了高召回率。在保留的手动标注样本上进行评估，我们的微调模型在数据集提取准确性方面优于NuExtract-v1.5和GLiNER-large-v2.1。我们的结果强调了使用LLM生成的合成数据如何有效解决训练数据稀缺性，提高在资源不足环境中的泛化能力。该框架提供了一条可扩展的数据集使用监测途径，增强透明度，并支持研究人员、资助者和政策制定者识别数据缺口，从而加强数据访问以供知情决策。', 'title_zh': '大型语言模型与合成数据在研究论文中监控数据集提及方面的应用'}
{'arxiv_id': 'arXiv:2502.10239', 'title': 'Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices', 'authors': 'Mohamed Aboelenien Ahmed, Kilian Pfeiffer, Ramin Khalili, Heba Khdr, Jörg Henkel', 'link': 'https://arxiv.org/abs/2502.10239', 'abstract': 'Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose Federated Split-Perturbation Zero-order Optimization (FedSPZO) that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.', 'abstract_zh': '联邦分割扰动零阶优化为在边缘设备上调谐大规模语言模型（LLMs）并在保护数据隐私的同时提供了一种有希望的方法。然而，由于对内存、通信和计算能力的高要求，这些模型在边缘设备上的调谐仍然是一个挑战。通过任务对齐的零阶优化提供了潜在的解决方案，能够在进行推理所需的内存要求下实现调谐，但需要更长的收敛时间。在本文中，我们提出了一种联邦分割扰动零阶优化（FedSPZO），该方法将网络分成两部分，并以计算有效的方式为每个部分应用不同数量的扰动，从而实现更快的收敛。我们的评估结果显示，与联邦学习中零阶优化的现有技术相比，FedSPZO将计算开销减少了2.5到7倍。', 'title_zh': '资源受限设备上的高效零阶联邦微调语言模型算法'}
{'arxiv_id': 'arXiv:2502.10236', 'title': 'Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control', 'authors': 'Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio, Luca Scimeca', 'link': 'https://arxiv.org/abs/2502.10236', 'abstract': 'Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.', 'abstract_zh': '扩散概率模型（DPMs）是强大的生成模型，在多种生成任务中取得了前所未有的成功。本文旨在将归纳偏置嵌入到扩散模型的训练和采样过程中，以便更好地适应数据的目标分布。对于具有拓扑结构的数据，我们设计了一个基于频率的扰动运算符，以有意地操纵并设定这些归纳偏置。我们首先展示了适当的扰动前向过程可以引导DPMs集中学习分布的特定方面。我们表明不同的数据集需要不同的归纳偏置，并且适当的基于频率的噪声控制可以提高生成性能，超过标准扩散的方法。最后，我们证明了在学习过程中忽略特定频率信息的可能性。我们通过一个图像去噪和恢复任务展示了这一点，在该任务中，我们训练了一个DPM，在严重噪声污染后恢复到原始目标分布。', 'title_zh': '通过基于频率的噪声控制塑造扩散模型的归纳偏置'}
{'arxiv_id': 'arXiv:2502.10226', 'title': 'A Multiagent Path Search Algorithm for Large-Scale Coalition Structure Generation', 'authors': 'Redha Taguelmimt, Samir Aknine, Djamila Boukredera, Narayan Changder, Tuomas Sandholm', 'link': 'https://arxiv.org/abs/2502.10226', 'abstract': 'Coalition structure generation (CSG), i.e. the problem of optimally partitioning a set of agents into coalitions to maximize social welfare, is a fundamental computational problem in multiagent systems. This problem is important for many applications where small run times are necessary, including transportation and disaster response. In this paper, we develop SALDAE, a multiagent path finding algorithm for CSG that operates on a graph of coalition structures. Our algorithm utilizes a variety of heuristics and strategies to perform the search and guide it. It is an anytime algorithm that can handle large problems with hundreds and thousands of agents. We show empirically on nine standard value distributions, including disaster response and electric vehicle allocation benchmarks, that our algorithm enables a rapid finding of high-quality solutions and compares favorably with other state-of-the-art methods.', 'abstract_zh': '合作结构生成（CSG），即最优地将一组代理分成合作体以最大化社会福利的问题，是多代理系统中的一个基本计算问题。该问题在需要快速运行时间的应用中非常重要，包括交通管理和灾害响应等领域。在本文中，我们开发了SALDAE算法，这是一种基于合作结构图的多代理路径规划算法。该算法利用了多种启发式方法和策略来进行搜索并引导搜索过程。它可以处理具有数百乃至数千个代理的大规模问题。通过在包括灾害响应和电动汽车分配基准在内的九种标准价值分布上进行实验，我们展示了该算法能够快速找到高质量的解决方案，并且与其他最先进的方法相比具有竞争力。', 'title_zh': '大规模合作结构生成的多智能体路径搜索算法'}
{'arxiv_id': 'arXiv:2502.10216', 'title': 'Forget the Data and Fine-Tuning! Just Fold the Network to Compress', 'authors': 'Dong Wang, Haris Šikić, Lothar Thiele, Olga Saukh', 'link': 'https://arxiv.org/abs/2502.10216', 'abstract': 'We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.', 'abstract_zh': '我们介绍了一种名为模型折叠的新颖无数据模型压缩技术，该技术通过跨层合并结构上相似的神经元，显著减少了模型大小，而无需微调或访问训练数据。与现有方法不同，模型折叠在压缩过程中通过利用k-means聚类，以及采用新颖的无数据技术来保持数据统计特性，防止方差崩溃或爆炸。我们理论框架及相关实验（包括标准基准ResNet18和LLaMA-7B）表明，模型折叠在性能上可与基于数据的压缩技术相媲美，并且在高稀疏性水平下优于最近提出的无数据方法。该方法特别适用于压缩大规模模型，因此适用于资源受限的环境部署。', 'title_zh': '忽略数据和微调！只需折叠网络进行压缩'}
{'arxiv_id': 'arXiv:2502.10201', 'title': 'Prediction hubs are context-informed frequent tokens in LLMs', 'authors': 'Beatrix M. G. Nielsen, Iuri Macocco, Marco Baroni', 'link': 'https://arxiv.org/abs/2502.10201', 'abstract': 'Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.', 'abstract_zh': '以下是翻译成中文的内容，符合学术规范：\n\n簇集现象（hubness），即少数点往往是大量其他点的最近邻的现象，通常在应用标准距离度量到高维数据时出现，常常负面影响基于距离的分析。由于自回归大型语言模型（LLMs）操作于高维表示，我们质疑它们是否也受簇集现象的影响。我们首先理论证明，LLMs 执行的唯一表示比较操作，即根据上下文和未嵌入向量确定续写概率的操作，不会表现出通常导致有害簇集现象的距离聚集现象。然后通过实验证明，这种比较仍然会导致高程度的簇集现象，但在这种情况下，簇集并非构成干扰。它们实际上是上下文调节下的频繁出现的标记在下一个标记预测候选池中经常出现的结果。另一方面，当对涉及LLMs 表示的距离计算进行其他操作时，我们没有相同的理论保证，并且确实看到了有害簇集现象的出现。总之，我们的研究一方面突显了即使在高维空间中普遍存在簇集现象，它也不总是需要减轻的负面属性；另一方面，展示了各种广泛使用的LLMs 开发出的一种猜测策略，即不断赋予频繁出现的标记以高概率。', 'title_zh': '预测枢纽是基于上下文的频繁令牌'}
{'arxiv_id': 'arXiv:2502.10200', 'title': 'Dynamic Reinforcement Learning for Actors', 'authors': 'Katsunari Shibata', 'link': 'https://arxiv.org/abs/2502.10200', 'abstract': 'Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration. Dynamic RL controls global system dynamics using a local index called "sensitivity," which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron\'s processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them -- to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error. Dynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain. Drawing parallels between \'exploration\' and \'thinking,\' the author hypothesizes that "exploration grows into thinking through learning" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.', 'abstract_zh': '本文提出了动态强化学习（Dynamic RL），该方法直接控制系统动态，而不是在每个时刻控制行动生成神经网络（actor）的输出，从而在强化学习（RL）中从静态转变为动态，带来了一个根本性的质的飞跃。actor 初始设计通过与环境的闭环生成混沌动态，使代理能够进行灵活且确定性的探索。动态RL 使用一个局部指标“灵敏度”（sensitivity）来控制全局系统动态，灵敏度表明每个神经元处理过程中输入邻域是如何收缩或扩展到相应的输出邻域的。虽然灵敏度调整学习（SAL）防止了动态过度收敛，但灵敏度控制的强化学习（SRL）则调整动态使其更加收敛，以提高在正TD误差周围更好状态转换的可重复性；同时使其更加发散，以增强在负TD误差周围较差状态转换的探索。\n\n动态RL仅应用于Actor-Critic RL架构中的actor部分，将其应用于critic仍然是一个挑战。该方法在两个动态任务上进行了测试，并且无需外部探索噪声或时间反向计算即可有效运行。此外，它展示了出色的对新环境的适应性，尽管还有一些问题需要解决。将“探索”与“思考”进行类比，作者提出“探索通过学习成长为思考”的假设，并认为这种RL可能是思考（包括无法从大量的现有文本数据中重构的启发式思维）出现的关键技术。最终，尽管这是推测性的结论，作者提出了这项研究应暂停的观点，旨在促进讨论。', 'title_zh': '动态强化学习在演员中的应用'}
{'arxiv_id': 'arXiv:2502.10195', 'title': 'Exploring the Camera Bias of Person Re-identification', 'authors': 'Myungseo Song, Jin-Woo Park, Jong-Seok Lee', 'link': 'https://arxiv.org/abs/2502.10195', 'abstract': 'We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.', 'abstract_zh': '我们 empirical 地研究了人员重识别 (ReID) 模型中的相机偏见。在此之前，已提出了相机感知的方法来解决这一问题，但这些方法主要局限于模型的训练域。我们对未见过的域中的 ReID 模型进行了相机偏见的度量，并揭示了数据分布转移时相机偏见更为显著的现象。作为一种针对未见过域数据的去偏方法，我们重新审视了嵌入向量上的特征标准化。尽管特征标准化已被用作一种简单的解决方案，但其背后的原因以及更广泛的适用性仍需进一步探索。我们分析了为什么这种方法能有效地减少偏见，并展示了它在低级图像特性和身体姿态等具体偏见因素上的适用性。进一步地，我们验证了其在各种模型和基准上的泛化能力，突显了其作为一种简单有效的测试时后处理方法在 ReID 中的潜力。此外，我们探讨了 ReID 模型无监督学习中固有的相机偏见风险。即使是在已见过的域数据中，无监督模型仍高度偏向于相机标签，表明有较大的改进空间。基于对相机偏置伪标签负面影响的观察，我们提出了一个简单的训练策略来减轻偏见。通过将这些策略应用于现有的无监督学习算法，我们展示了通过小修改即可实现显著的性能提升。', 'title_zh': '探索人身重识别中的相机偏见'}
{'arxiv_id': 'arXiv:2502.10193', 'title': 'Merging public elementary schools to reduce racial/ethnic segregation', 'authors': 'Madison Landry, Nabeel Gillani', 'link': 'https://arxiv.org/abs/2502.10193', 'abstract': 'Diverse schools can help address implicit biases and increase empathy, mutual respect, and reflective thought by fostering connections between students from different racial/ethnic, socioeconomic, and other backgrounds. Unfortunately, demographic segregation remains rampant in US public schools, despite over 70 years since the passing of federal legislation formally outlawing segregation by race. However, changing how students are assigned to schools can help foster more integrated learning environments. In this paper, we explore "school mergers" as one such under-explored, yet promising, student assignment policy change. School mergers involve merging the school attendance boundaries, or catchment areas, of schools and subsequently changing the grades each school offers. We develop an algorithm to simulate elementary school mergers across 200 large school districts serving 4.5 million elementary school students and find that pairing or tripling schools in this way could reduce racial/ethnic segregation by a median relative 20% -- and as much as nearly 60% in some districts -- while increasing driving times to schools by an average of a few minutes each way. Districts with many interfaces between racially/ethnically-disparate neighborhoods tend to be prime candidates for mergers. We also compare the expected results of school mergers to other typical integration policies, like redistricting, and find that different policies may be more or less suitable in different places. Finally, we make our results available through a public dashboard for policymakers and community members to explore further (this https URL). Together, our study offers new findings and tools to support integration policy-making across US public school districts.', 'abstract_zh': '多样的学校可以帮助应对隐性偏见，提高同情心、相互尊重和反思性思考，通过促进来自不同种族/ Ethnic、经济背景以及其他背景的学生之间的联系。不幸的是，尽管联邦立法正式禁止按种族分隔已有超过70年的历史，但在美国公共学校中人口种族隔阂现象仍然普遍存在。然而，改变学生分班的方式可以有助于营造更加整合的学习环境。在本文中，我们探讨了“学校合并”作为一种尚未充分探索但前景可观的学生分班政策变化。学校合并涉及将学校的服务区（学区）合并，并在此基础上改变每所学校的年级设置。我们开发了一个算法来模拟200个大型学区的小学合并情况，这些学区服务着450万名小学生，并发现通过这种方式将学校合并成对或三倍，可以减少种族/族裔隔阂的中位相对比例达20%，在某些学区甚至可以减少近60%，同时上学路程平均增加几分钟。那些种族/族裔差异明显的邻区之间接口众多的学区更适合作为合并的候选对象。我们还将学校合并预期结果与如重新划区等其他常见整合政策进行了比较，并发现不同政策在不同地区可能更具适用性。最后，我们将这些结果通过一个公开的仪表板提供给政策制定者和社区成员，供他们进一步探索（请参见此链接：https://...）。综上所述，我们的研究提供了有关美国公共学校合并政策制定的新发现和工具。', 'title_zh': '将公立小学合并以减少种族/民族隔离'}
{'arxiv_id': 'arXiv:2502.10178', 'title': 'From Markov to Laplace: How Mamba In-Context Learns Markov Chains', 'authors': 'Marco Bondaschi, Nived Rajaraman, Xiuying Wei, Kannan Ramchandran, Razvan Pascanu, Caglar Gulcehre, Michael Gastpar, Ashok Vardhan Makkuva', 'link': 'https://arxiv.org/abs/2502.10178', 'abstract': 'While transformer-based language models have driven the AI revolution thus far, their computational complexity has spurred growing interest in viable alternatives, such as structured state space sequence models (SSMs) and Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown remarkable inference speed ups over transformers while achieving comparable or superior performance on complex language modeling tasks. However, despite these architectural innovations and empirical successes, the fundamental learning capabilities of Mamba remain poorly understood. In this paper, we address this gap by studying in-context learning (ICL) on Markov chains and uncovering a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markovian orders. To explain this, we theoretically characterize the representation capacity of Mamba and reveal the fundamental role of convolution in enabling it to represent the optimal Laplacian smoothing. These theoretical insights align strongly with empirical results and, to the best of our knowledge, represent the first formal connection between Mamba and optimal statistical estimators. Finally, we outline promising research directions inspired by these findings.', 'abstract_zh': '尽管基于transformer的语言模型在推动当前的AI革命中发挥了重要作用，但由于其计算复杂性，研究人员对可行的替代方案越来越感兴趣，如结构化状态空间序列模型（SSMs）和选择性SSMs。在这之中，Mamba（S6）及其变体Mamba-2在实现类似或优于Transformer在复杂语言建模任务上的性能同时，表现出显著的推理速度提升。然而，尽管这些架构创新和实际成功，Mamba的基本学习能力仍然知之甚少。在本文中，我们通过研究马尔可夫链的上下文学习（ICL），揭开了一个令人惊讶的现象：与transformer不同，即使单层Mamba也能高效地学习马尔可夫链的所有阶数的最优拉普拉斯平滑估计量，该估计量同时是贝叶斯最优和 minimax 无偏的。为了解释这一点，我们从理论上描述了Mamba的表示能力，并揭示了卷积在使它能够表示最优拉普拉斯平滑中的根本作用。这些理论见解与实验结果高度一致，并且到我们所知，这代表了Mamba与最优统计估计器之间的首次正式联系。最后，我们提出了这些发现所启发的有希望的研究方向。', 'title_zh': '从马尔可夫到拉普拉斯：马巴姆上下文学习马尔可夫链'}
{'arxiv_id': 'arXiv:2502.10174', 'title': 'Technical Risks of (Lethal) Autonomous Weapons Systems', 'authors': 'Heramb Podar, Alycia Colijn', 'link': 'https://arxiv.org/abs/2502.10174', 'abstract': 'The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences.\nKey Takeaways:\n1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms.\n2. These systematic risks include the black-box nature of AI decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control.\n3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts.\n4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles.', 'abstract_zh': '(致命)自主武器系统（Lethal Autonomous Weapons Systems，简称(L)AWS）的自主性和适应性带来了前所未有的操作能力，但也引入了挑战国际安全中控制、问责和稳定原则的深刻风险。本报告概述了(L)AWS部署的关键技术风险，强调了其不可预测性、透明度不足和操作可靠性差的特点，这些都可能导致严重的意外后果。\n\n主要见解：\n1. (L)AWS提出的优点只有通过将对象物化和分类才能实现，但一系列系统性风险限制了分类算法的可靠性和可预测性。\n2. 这些系统性风险包括人工智能决策的黑箱特性、对奖励劫持的易感性、目标泛化错误以及可能导致超出人类控制的新兴行为。\n3. (L)AWS可能以既不可预期也无法控制的方式行动，从而削弱任务目标并有可能升级冲突。\n4. 即使经过严格测试的系统，在实际操作条件下也可能表现出不可预测且有害的行为，从而危及战略稳定性和人道主义原则。', 'title_zh': '致命自主武器系统的技术风险'}
{'arxiv_id': 'arXiv:2502.10162', 'title': 'Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions', 'authors': 'Lei Cheng, Junpeng Zhang, Qihan Ren, Quanshi Zhang', 'link': 'https://arxiv.org/abs/2502.10162', 'abstract': "This paper aims to analyze the generalization power of deep neural networks (DNNs) from the perspective of interactions. Unlike previous analysis of a DNN's generalization power in a highdimensional feature space, we find that the generalization power of a DNN can be explained as the generalization power of the interactions. We found that the generalizable interactions follow a decay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution. Furthermore, our theory can effectively disentangle these two types of interactions from a DNN. We have verified that our theory can well match real interactions in a DNN in experiments.", 'abstract_zh': '本文旨在从交互作用的角度分析深度神经网络（DNNs）的泛化能力。不同于以往对DNN泛化能力在高维特征空间中的分析，我们发现DNN的泛化能力可以被解释为交互作用的泛化能力。研究发现，可泛化的交互作用遵循衰减型分布，而非泛化的交互作用遵循纺锤型分布。此外，我们的理论能够有效地将这两种类型的交互作用从DNN中分离出来。实验验证表明，我们的理论能够很好地匹配DNN中的实际交互作用。', 'title_zh': '从符号交互的角度 revisiting DNN 的泛化能力'}
{'arxiv_id': 'arXiv:2502.10157', 'title': 'SessionRec: Next Session Prediction Paradigm For Generative Sequential Recommendation', 'authors': 'Lei Huang, Hao Guo, Linzhi Peng, Long Zhang, Xiaoteng Wang, Daoyuan Wang, Shichao Wang, Jinpeng Wang, Lei Wang, Sheng Chen', 'link': 'https://arxiv.org/abs/2502.10157', 'abstract': "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for generative sequential recommendation, addressing the fundamental misalignment between conventional next-item prediction paradigm (NIPP) and real-world recommendation scenarios. Unlike NIPP's item-level autoregressive generation that contradicts actual session-based user interactions, our framework introduces a session-aware representation learning through hierarchical sequence aggregation (intra/inter-session), reducing attention computation complexity while enabling implicit modeling of massive negative interactions, and a session-based prediction objective that better captures users' diverse interests through multi-item recommendation in next sessions. Moreover, we found that incorporating a rank loss for items within the session under the next session prediction paradigm can significantly improve the ranking effectiveness of generative sequence recommendation models. We also verified that SessionRec exhibits clear power-law scaling laws similar to those observed in LLMs. Extensive experiments conducted on public datasets and online A/B test in Meituan App demonstrate the effectiveness of SessionRec. The proposed paradigm establishes new foundations for developing industrial-scale generative recommendation systems through its model-agnostic architecture and computational efficiency.", 'abstract_zh': '我们提出了SessionRec，这是一种新颖的会话级下一会话预测范式（NSPP），用于生成性序列推荐，解决了传统项目级自回归生成范式（NIPP）与实际推荐场景之间的根本性不匹配问题。与NIPP中的项目级自回归生成不符实际的会话基础用户交互不同，我们的框架引入了通过层次序列聚合（ intra-session 和 inter-session）的会话感知表示学习方法，从而降低注意力计算复杂度的同时，能够隐式建模大量负交互，并通过在下一会话中提供多项推荐来更好地捕捉用户多样的兴趣。此外，我们发现，在下一会话预测范式中引入会话内项目的排名损失可以显著改善生成性序列推荐模型的排名有效性。我们还验证了SessionRec表现出与大型语言模型（LLMs）类似的幂律扩展规律。在美团App的公开数据集和在线A/B测试中进行的广泛实验验证了SessionRec的有效性。提出的范式通过其模型无关架构和计算效率，为开发工业规模的生成推荐系统奠定了新的基础。', 'title_zh': 'SessionRec：生成型序贯推荐中的下一个会话预测范式'}
{'arxiv_id': 'arXiv:2502.10154', 'title': 'Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries', 'authors': 'Serkan Sulun, Paula Viana, Matthew E. P. Davies', 'link': 'https://arxiv.org/abs/2502.10154', 'abstract': "We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners.", 'abstract_zh': '我们介绍了一种名为EMSYNC的基于视频的符号化音乐生成模型，该模型能够将音乐与视频的情感内容和时间边界相匹配。EMSYNC采用两阶段框架：首先，预训练的视频情绪分类器提取情感特征；然后，条件音乐生成器在情感和时间线索的指导下生成MIDI序列。我们引入了一种新的时间条件机制，称为边界偏移，该机制使模型能够预测场景切换并使其与音乐和声部分时段对齐。与现有模型不同，我们的方法保留了基于事件的编码，从而确保精细的时间控制和表达性强的音乐细节。此外，我们还提出了一种映射方案，用于对接产生离散情绪类别的视频情绪分类器和以连续值唤起-愉悦度输入操作的条件MIDI生成器。在主观听觉测试中，EMSYNC在所有主观评估指标上均优于现有最先进的模型，包括音乐理论意识的参与者和一般听众。', 'title_zh': '情绪对齐与时间边界同步的视频 soundtrack 生成'}
{'arxiv_id': 'arXiv:2502.10125', 'title': 'Learning Relational Tabular Data without Shared Features', 'authors': 'Zhaomin Wu, Shida Wang, Ziyang Wang, Bingsheng He', 'link': 'https://arxiv.org/abs/2502.10125', 'abstract': "Learning relational tabular data has gained significant attention recently, but most studies focus on single tables, overlooking the potential of cross-table learning. Cross-table learning, especially in scenarios where tables lack shared features and pre-aligned data, offers vast opportunities but also introduces substantial challenges. The alignment space is immense, and determining accurate alignments between tables is highly complex. We propose Latent Entity Alignment Learning (Leal), a novel framework enabling effective cross-table training without requiring shared features or pre-aligned data. Leal operates on the principle that properly aligned data yield lower loss than misaligned data, a concept embodied in its soft alignment mechanism. This mechanism is coupled with a differentiable cluster sampler module, ensuring efficient scaling to large relational tables. Furthermore, we provide a theoretical proof of the cluster sampler's approximation capacity. Extensive experiments on five real-world and five synthetic datasets show that Leal achieves up to a 26.8% improvement in predictive performance compared to state-of-the-art methods, demonstrating its effectiveness and scalability.", 'abstract_zh': '近年来，学习关系表数据引起了广泛关注，但大多数研究集中在单表上，忽视了跨表学习的潜力。特别是在表之间缺乏共享特征和预对齐数据的情况下，跨表学习不仅提供了巨大的机会，也带来了显著的挑战。对齐空间非常广阔，准确确定表之间的对齐关系极为复杂。我们提出了一种名为Latent Entity Alignment Learning (Leal)的新框架，该框架能够在不依赖共享特征或预对齐数据的情况下实现有效的跨表训练。Leal的核心理念是正确对齐的数据损失较小，这一理念在其实软对齐机制中得到了体现。该机制与可微分聚类采样模块相结合，确保了其在大关系表上的高效扩展。此外，我们还提供了聚类采样模块逼近能力的理论证明。在五个真实世界和五个合成数据集上的广泛实验表明，Leal 在预测性能上的表现比最先进的方法提高了最高达26.8%，这证明了其有效性和可扩展性。', 'title_zh': '无需共享特征的学习关系型表格数据'}
{'arxiv_id': 'arXiv:2502.10118', 'title': 'Image Embedding Sampling Method for Diverse Captioning', 'authors': 'Sania Waheed, Na Min An', 'link': 'https://arxiv.org/abs/2502.10118', 'abstract': 'Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.', 'abstract_zh': '近年来，先进视觉语言模型（VLMs）的图像描述性能显著提高，但这种改进伴随着计算复杂度的增加，使得它们在资源受限的应用场景（如移动设备和辅助技术）中不太容易使用。相比之下，较小的VLMs更侧重于高层场景描述，而忽略了对图像理解贡献显著的细微细节。本文提出了一种无需训练的框架，通过使用相对较小的VLM（BLIP）作为骨干网络，显式地关注不同的图像区域，从而增强描述的多样性和信息量。我们的方法利用结构化分割产生分层表示，捕获全局和局部语义。无需额外的模型训练，我们展示了这种方法能使较小的VLM在图像-描述匹配、语义完整性和多样性方面达到与大型模型相当的性能。我们在MSCOCO、Flickr30k和Nocaps测试数据集上评估了我们的框架，分别获得了Div-2分数0.735、0.750和0.748，同时保持了与人工标注的描述相符性强和语义完整性。', 'title_zh': '图像嵌入采样方法促进多样化描述'}
{'arxiv_id': 'arXiv:2502.10092', 'title': 'A novel approach to data generation in generative model', 'authors': 'JaeHong Kim, Jaewon Shim', 'link': 'https://arxiv.org/abs/2502.10092', 'abstract': 'Variational Autoencoders (VAEs) and other generative models are widely employed in artificial intelligence to synthesize new data. However, current approaches rely on Euclidean geometric assumptions and statistical approximations that fail to capture the structured and emergent nature of data generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric framework that redefines data generation by integrating dimensional expansion accompanied by qualitative transformation. By modifying the latent space geometry to interact with emergent high-dimensional structures, CFP theory addresses key challenges such as identifiability issues and unintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two key conceptual hypotheses that redefine how generative models structure relationships between data and algorithms. Through the lens of CFP theory, we critically examine existing metric-learning approaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and structural convergence mechanisms, leading to a novel geometric approach that better accounts for data generation as a structured epistemic process. Beyond its computational implications, CFP theory provides philosophical insights into the ontological underpinnings of data generation. By offering a systematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing a theoretical foundation for understanding the data-relationship structures in AI. Finally, future research in CFP theory will be led to its implications for fully realizing qualitative transformations, introducing the potential of Hilbert space in generative modeling.', 'abstract_zh': '变分自编码器（VAEs）和其他生成模型在人工智能中广泛用于合成新数据。然而，现有的方法依赖于欧几里得几何假设和统计近似，这些假设无法捕捉数据生成的结构化和涌现性质。本文提出了收敛融合范式（Convergent Fusion Paradigm, CFP）理论，这是一种新颖的几何框架，通过伴随质化转变的维度扩展重新定义数据生成。通过修改潜在空间的几何结构以与涌现的高维结构相互作用，CFP 理论解决了可识别性问题和大型语言模型（LLMs）中类似幻觉的非预期伪影等关键挑战。CFP 理论基于两个关键概念假设，重新定义生成模型在数据与算法之间关系的结构化方式。从 CFP 理论的角度出发，我们批判性地审视了现有的度量学习方法。CFP 理论通过引入时间反向度量嵌入和结构收敛机制，促进了对数据生成作为一种结构化先验过程的理解，从而提出了一种新颖的几何方法。除了在计算方面的应用，CFP 理论还为数据生成的存在论根基提供了哲学见解。通过提供一个系统框架来研究高维学习动力学，CFP 理论为理解人工智能中的数据关系结构奠定了理论基础。最后，CFP 理论未来的研究还将指明其在实现质化转变方面的潜力，并探索希尔伯特空间在生成模型中的应用前景。', 'title_zh': '一种生成模型中数据生成的新方法'}
{'arxiv_id': 'arXiv:2502.10090', 'title': 'Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models', 'authors': 'Chenrui Tie, Shengxiang Sun, Jinxuan Zhu, Yiwei Liu, Jingxiang Guo, Yue Hu, Haonan Chen, Junting Chen, Ruihai Wu, Lin Shao', 'link': 'https://arxiv.org/abs/2502.10090', 'abstract': 'Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.', 'abstract_zh': '人类具有通过理解和执行抽象说明来完成复杂操作任务的非凡能力。然而，对于机器人来说，这一能力仍然是一个巨大的挑战，因为它们无法理解抽象指令并将其转化为可执行的操作。在这篇论文中，我们提出了一种名为Manual2Skill的新框架，使机器人能够遵循高层次的手工说明来执行复杂的组装任务。我们的方法利用视觉-语言模型（VLM）从指示图像中提取结构化的信息，然后利用这些信息构建分层次的组装图。这些图表示部件、子装配件及其之间的关系。为了促进任务执行，姿势估计模型预测每个组装步骤中组件的相对6D姿势。同时，运动规划模块生成适用于实际机器人实现的操作序列。通过成功组装多个真实的宜家家具件，我们展示了Manual2Skill的有效性。这一应用突显了其在效率和精度方面管理长期操作任务的能力，大大增强了从说明书中学习的机器人系统的实用性。这项工作标志着在使机器人系统能够以类似人类能力的方式理解和执行复杂操作任务方面向前迈进了一步。', 'title_zh': 'Manual2Skill：使用视觉-语言模型学习阅读说明书并获取家具组装的机器人技能'}
{'arxiv_id': 'arXiv:2502.10089', 'title': 'A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference', 'authors': 'Kieran Woodward, Eiman Kanjo, Georgios Papandroulidakis, Shady Agwa, Themis Prodromakis', 'link': 'https://arxiv.org/abs/2502.10089', 'abstract': 'In recent years, the development of smart edge computing systems to process information locally is on the rise. Many near-sensor machine learning (ML) approaches have been implemented to introduce accurate and energy efficient template matching operations in resource-constrained edge sensing systems, such as wearables. To introduce novel solutions that can be viable for extreme edge cases, hybrid solutions combining conventional and emerging technologies have started to be proposed. Deep Neural Networks (DNN) optimised for edge application alongside new approaches of computing (both device and architecture -wise) could be a strong candidate in implementing edge ML solutions that aim at competitive accuracy classification while using a fraction of the power of conventional ML solutions. In this work, we are proposing a hybrid software-hardware edge classifier aimed at the extreme edge near-sensor systems. The classifier consists of two parts: (i) an optimised digital tinyML network, working as a front-end feature extractor, and (ii) a back-end RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage template matching system. The combined hybrid system exhibits a competitive trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$ and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with 78.06$\\mu$J for the original teacher model, representing a 792-fold reduction, making it a viable solution for extreme edge applications.', 'abstract_zh': '近年来，智能边缘计算系统的发展势头迅猛，该系统能够在本地处理信息。许多接近传感器的机器学习（ML）方法已被应用于资源受限的边缘传感系统（如可穿戴设备），以引入准确且节能的模板匹配操作。为应对边缘环境中的极端情况，结合传统技术和新兴技术的混合解决方案开始被提出。针对边缘应用优化的深度神经网络（DNN）及其新型计算方法（包括设备级和体系结构级）可能成为实现具有竞争力的准确分类的边缘ML解决方案的理想选择，同时使用传统ML解决方案一小部分的功率。在本文中，我们提出了一种针对极端边缘近传感器系统的混合软件-硬件边缘分类器。该分类器由两部分组成：（i）经过优化的数字tinyML网络，作为前端特征提取器；（ii）后端的RRAM-CMOS模拟内容可寻址存储器（ACAM），作为最终阶段的模板匹配系统。此混合系统在准确性和能耗之间表现出与原创教师模型相比的优势，其中每分类操作的前端能耗$E_{front-end} = 96.23 nJ$，后端能耗$E_{back-end} = 1.45 nJ$，而原创教师模型的能耗为78.06 $\\mu J$，这相当于能耗降低了792倍，使其成为极端边缘应用的可行解决方案。', 'title_zh': '一种混合边缘分类器：结合TinyML优化的CNN与RRAM-CMOS ACAM的能量高效推断'}
{'arxiv_id': 'arXiv:2502.10063', 'title': 'Strassen Multisystolic Array Hardware Architectures', 'authors': 'Trevor E. Pogue, Nicola Nicolici', 'link': 'https://arxiv.org/abs/2502.10063', 'abstract': "While Strassen's matrix multiplication algorithm reduces the complexity of naive matrix multiplication, general-purpose hardware is not suitable for achieving the algorithm's promised theoretical speedups. This leaves the question of if it could be better exploited in custom hardware architectures designed specifically for executing the algorithm. However, there is limited prior work on this and it is not immediately clear how to derive such architectures or if they can ultimately lead to real improvements. We bridge this gap, presenting and evaluating new systolic array architectures that efficiently translate the theoretical complexity reductions of Strassen's algorithm directly into hardware resource savings. Furthermore, the architectures are multisystolic array designs that can multiply smaller matrices with higher utilization than single-systolic array designs. The proposed designs implemented on FPGA reduce DSP requirements by a factor of $1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require overall similar soft logic resources when instantiated to support matrix sizes down to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We evaluate the proposed designs both in isolation and in an end-to-end machine learning accelerator compared to baseline designs and prior works, achieving state-of-the-art performance.", 'abstract_zh': '斯特拉森矩阵乘法算法虽然降低了传统矩阵乘法的复杂度，但通用硬件难以实现该算法预期的理论加速效果。这引发了可以在专门为此算法设计的定制硬件架构中更好地利用该算法的疑问。然而，此前在这方面的工作有限，如何设计此类架构以及它们能否最终带来实际改进尚不明确。我们填补了这一空白，提出了并评估了一种新的电路阵列架构，能够直接将斯特拉森算法的理论复杂度降低转化为硬件资源节约。此外，所提出的多阵列电路设计能够在装填率方面优于单一阵列电路设计，用于乘法的更小矩阵乘法。在FPGA上实现所提出的架构，对于实施的斯特拉森递归层级为$r$的情况，减少了$1.14^r$倍的 DSP 资源需求；并且，在支持至32x32和24x24矩阵（分别对应1-2层斯特拉森递归）时，总体软逻辑资源需求与基线设计和先前工作相当。我们分别在孤立和端到端机器学习加速器场景下评估了所提出的设计，并在与基线设计和先前工作比较中取得了领先性能。', 'title_zh': 'Strassen多 systolic阵列硬件架构'}
{'arxiv_id': 'arXiv:2502.10062', 'title': 'Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints', 'authors': 'Xiaoshan Lin, Roberto Tron', 'link': 'https://arxiv.org/abs/2502.10062', 'abstract': "This work addresses the problem of multi-robot coordination under unknown robot transition models, ensuring that tasks specified by Time Window Temporal Logic are satisfied with user-defined probability thresholds. We present a bi-level framework that integrates (i) high-level task allocation, where tasks are assigned based on the robots' estimated task completion probabilities and expected rewards, and (ii) low-level distributed policy learning and execution, where robots independently optimize auxiliary rewards while fulfilling their assigned tasks. To handle uncertainty in robot dynamics, our approach leverages real-time task execution data to iteratively refine expected task completion probabilities and rewards, enabling adaptive task allocation without explicit robot transition models. We theoretically validate the proposed algorithm, demonstrating that the task assignments meet the desired probability thresholds with high confidence. Finally, we demonstrate the effectiveness of our framework through comprehensive simulations.", 'abstract_zh': '本文解决了在未知机器人转移模型下的多机器人协调问题，确保由时间窗口时序逻辑指定的任务能够以用户定义的概率阈值被满足。我们提出了一种双层框架，该框架包括两个方面：(i) 高层任务分配，任务根据机器人估计的任务完成概率和预期奖励进行分配；(ii) 低层分布式策略学习与执行，机器人独立优化辅助奖励，同时完成分配给它们的任务。为了应对机器人动力学的不确定性，我们的方法利用实时的任务执行数据，迭代地细化预期的任务完成概率和奖励，从而在无需显式机器人转移模型的情况下实现自适应的任务分配。我们从理论上验证了所提出的算法，证明任务分配能够以高置信度满足期望的概率阈值。最后，通过全面的仿真实验展示了我们框架的有效性。', 'title_zh': '在不确定性条件下具有时间逻辑约束的自适应双层多机器人任务分配与学习'}
{'arxiv_id': 'arXiv:2502.10050', 'title': 'A Survey on LLM-powered Agents for Recommender Systems', 'authors': 'Qiyao Peng, Hongtao Liu, Hua Huang, Qing Yang, Minglai Shao', 'link': 'https://arxiv.org/abs/2502.10050', 'abstract': 'Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.', 'abstract_zh': '推荐系统是许多在线平台的关键组成部分，尽管传统的推荐方法仍在努力理解和提供解释性强的个性化推荐。得益于大型语言模型（LLM）驱动代理的出现，通过实现自然语言互动和可解释的推理，推荐系统研究有望迎来新的突破。本文综述了LLM驱动代理在推荐系统中的新兴应用。我们确定并分析了当前研究中的三大核心范式：（1）推荐导向的方法，利用智能代理来增强基础的推荐机制；（2）交互导向的方法，通过自然对话和可解释的推荐来促进动态用户参与；以及（3）仿真导向的方法，通过多代理框架来建模复杂的用户-项目交互和系统动态。除范式分类外，我们还分析了LLM驱动推荐代理的架构基础，考察了其关键组件：档案构建、内存管理、策略规划和行动执行。研究还扩展至对该领域基准数据集和评估框架的全面分析。这种系统性的考察不仅揭示了LLM驱动的代理推荐系统当前的状态，还绘制出了该变革领域中的关键挑战和充满希望的研究方向。', 'title_zh': '基于大型语言模型的推荐系统代理综述'}
{'arxiv_id': 'arXiv:2502.10047', 'title': 'Janus: Collaborative Vision Transformer Under Dynamic Network Environment', 'authors': 'Linyi Jiang, Silvery D. Fu, Yifei Zhu, Bo Li', 'link': 'https://arxiv.org/abs/2502.10047', 'abstract': 'Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15 times and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.', 'abstract_zh': '视觉变换器（ViTs）在各类计算机视觉任务中已经超越了传统的卷积神经网络架构，并取得了最先进的性能。然而，由于ViTs计算成本高昂，这些模型只能在资源受限的边缘设备上通过裁剪模型运行，或者在通过波动网络传输原始数据后在远程云服务器上执行。这两种结果导致的性能下降或高延迟都阻碍了ViTs的广泛应用。本文介绍了一种名为Janus的框架，该框架是第一个在动态网络上实现低延迟云-边缘设备协同视觉变换器推理的框架。Janus克服了视觉变换器固有的模型限制，实现了在同一任务中利用云和边缘设备并行执行视觉变换器模型，从而实现了低延迟、高准确性和低通信开销。具体而言，Janus巧妙地结合了令牌裁剪技术，并设计了一种细到粗模型分割策略和非静态混合裁剪策略。通过动态选择最佳的裁剪级别和分割点，Janus在准确性和延迟之间取得平衡。在各种任务上的实验结果表明，在不同的网络环境中，与基线方法相比，Janus可以提高吞吐量最高5.15倍，并将延迟违规率降低98.7%。', 'title_zh': '以下是符合学术规范的翻译：\n\nJanus：在动态网络环境中协作的视觉变换器'}
{'arxiv_id': 'arXiv:2502.09990', 'title': 'X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability', 'authors': 'Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao', 'link': 'https://arxiv.org/abs/2502.09990', 'abstract': 'Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: this https URL.', 'abstract_zh': '尽管语言模型（LLMs）的安全对齐技术取得了 rapid 发展，但防范多回合脱羁绊（multi-turn jailbreaks）仍然是一项具有挑战性的任务。本文中，我们进行了全面比较，发现一些现有的防御方法可以在一定程度上提升LLMs对抗多回合脱羁绊的鲁棒性，但可能会牺牲其实用性，即降低通用能力或导致过度拒绝。从LLMs机制可解释性的角度来看，我们发现这些方法无法建立一个精确区分安全和有害特征表示的边界。因此，接近有害特征表示的边界安全表示不可避免地会被扰乱，导致实用性下降。为解决这一问题，我们提出了X-Boundary，旨在将有害特征表示从边界安全表示中推开，从而获得一个精确的区分边界。这样，可以精确地擦除有害特征表示而不影响安全特征表示。实验结果显示，X-Boundary 在多回合脱羁绊防御方面达到了最先进的性能，同时将过度拒绝率降低了约20%，并且几乎维持了全部通用能力。此外，我们从理论上证明并从实验中验证了X-Boundary 可以加速训练过程中的收敛速度。请参阅我们的代码：[此链接](this https URL)。', 'title_zh': 'X-边界：建立精确的安全边界以屏蔽LLM免受多轮脱戒威胁，同时不牺牲可用性'}
{'arxiv_id': 'arXiv:2502.09977', 'title': 'LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing', 'authors': 'Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng', 'link': 'https://arxiv.org/abs/2502.09977', 'abstract': "Effectively incorporating external knowledge into Large Language Models (LLMs) is crucial for enhancing their capabilities and addressing real-world needs. Retrieval-Augmented Generation (RAG) offers an effective method for achieving this by retrieving the most relevant fragments into LLMs. However, the advancements in context window size for LLMs offer an alternative approach, raising the question of whether RAG remains necessary for effectively handling external knowledge. Several existing studies provide inconclusive comparisons between RAG and long-context (LC) LLMs, largely due to limitations in the benchmark designs. In this paper, we present LaRA, a novel benchmark specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses 2,326 test cases across four practical QA task categories and three types of naturally occurring long texts. Through systematic evaluation of seven open-source and four proprietary LLMs, we find that the optimal choice between RAG and LC depends on a complex interplay of factors, including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. Our findings provide actionable guidelines for practitioners to effectively leverage both RAG and LC approaches in developing and deploying LLM applications. Our code and dataset is provided at: \\href{this https URL}{\\textbf{this https URL}}.", 'abstract_zh': '有效地将外部知识整合到大规模语言模型（LLMs）中对于提升其能力并满足实际需求至关重要。检索增强生成（RAG）提供了一种有效的方法，通过检索最具相关性的片段进入LLMs。然而，LLMs的上下文窗口大小的最新进展为另一种替代方法打开了可能性，引发了RAG是否仍有必要处理外部知识的疑问。现有的一些研究在这两者的比较上提供了不完全明确的结果，主要是因为基准设计中的局限性。在本文中，我们提出了LaRA，这是一种新的基准测试专门设计用于严格比较RAG和长上下文（LC）LLMs。LaRA包括了涵盖四个实际问答任务类别及三种自然长文本类型的2,326个测试案例。通过系统地评估七个开源和四个专有LLM，我们发现RAG和LC之间的最佳选择取决于多种复杂因素的综合作用，包括模型的参数规模、处理长文本的能力、上下文长度、任务类型以及检索片段的特性。我们的发现提供了实际操作指南，以便从业者能够有效地利用RAG和LC方法来开发和部署LLM应用。我们的代码和数据集可以在：\\href{这个链接}{这个链接}找到。', 'title_zh': 'LaRA：检索增强生成和长上下文大语言模型的基准测试——没有适用于长上下文或检索增强生成路由的万能方法'}
{'arxiv_id': 'arXiv:2502.09971', 'title': 'Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression', 'authors': 'Siqi Wu, Yinda Chen, Dong Liu, Zhihai He', 'link': 'https://arxiv.org/abs/2502.09971', 'abstract': "In this paper, we study how to synthesize a dynamic reference from an external dictionary to perform conditional coding of the input image in the latent domain and how to learn the conditional latent synthesis and coding modules in an end-to-end manner. Our approach begins by constructing a universal image feature dictionary using a multi-stage approach involving modified spatial pyramid pooling, dimension reduction, and multi-scale feature clustering. For each input image, we learn to synthesize a conditioning latent by selecting and synthesizing relevant features from the dictionary, which significantly enhances the model's capability in capturing and exploring image source correlation. This conditional latent synthesis involves a correlation-based feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. The synthesized latent is then used to guide the encoding process, allowing for more efficient compression by exploiting the correlation between the input image and the reference dictionary. According to our theoretical analysis, the proposed conditional latent coding (CLC) method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries. Experimental results on benchmark datasets show that our new method improves the coding performance by a large margin (up to 1.2 dB) with a very small overhead of approximately 0.5\\% bits per pixel. Our code is publicly available at this https URL.", 'abstract_zh': '在本文中，我们研究了如何从外部字典合成动态参考，以在潜在域中执行输入图像的条件编码，并探讨了如何端到端地学习条件潜在合成和编码模块。我们的方法首先通过多阶段方法构造了一个通用的图像特征字典，该方法包括修改后的空间金字塔池化、降维和多尺度特征聚类。对于每个输入图像，我们学习通过从字典中选择和合成相关的特征来合成调节潜在变量，这显著增强了模型捕捉和探索图像源相关性的能力。这种条件潜在合成包含基于相关性的特征匹配和对齐策略，包括条件潜在匹配（CLM）模块和条件潜在合成（CLS）模块。合成的潜在变量随后用于引导编码过程，通过利用输入图像与参考字典之间的相关性来实现更高效的压缩。根据我们的理论分析，所提出的条件潜在编码（CLC）方法在外部字典样本变化和选定的调节潜在变量方面具有鲁棒性，提出的误差界与字典大小成对数关系，从而即使在大型和多样化字典的情况下也能保证稳定性。基准数据集上的实验结果表明，我们的新方法在编码性能上有了很大的提升（最高可达1.2 dB），并且仅增加了非常小的开销（约0.5%的像素比特）。我们的代码已在以下网址公开：[该网址链接]。', 'title_zh': '基于可学习合成参考的条件潜在编码方法在深度图像压缩中的应用'}
{'arxiv_id': 'arXiv:2502.09969', 'title': 'Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning', 'authors': 'Ishika Agarwal, Dilek Hakkani-Tur', 'link': 'https://arxiv.org/abs/2502.09969', 'abstract': 'Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: this https URL.', 'abstract_zh': '影响函数为模型训练提供了宝贵的见解，但现有方法在计算成本和泛化能力方面存在局限性。特别是在使用语言模型计算数据影响方面，已有工作提出了多种度量和算法，但这些方法在大规模模型和数据集上扩展性较差。这是因为计算过程中需要昂贵的前向和反向传递，对大规模模型的存储需要大量的内存，并且影响值估计在新数据上的泛化能力较差。本文中，我们探索了使用小型神经网络（我们称之为InfluenceNetwork）来估计影响值，实现了高达99%的成本降低。我们的评估表明，影响值可以用仅仅全语言模型大小的0.0027%的小型模型来估计（我们使用了7B和8B版本的模型）。我们将我们的影响值估计算法（称为NN-CIFT：神经网络用于高效指令微调）应用于通用指令微调的子集选择下游任务。在我们的研究中，我们包括了四种最新的影响函数，并且尽管有巨大的加速，但NN-CIFT的性能并未受到任何妥协。我们对NN-CIFT的超参数进行了深入分析。我们的方法代码可以在以下链接找到：this https URL。', 'title_zh': '使用神经网络进行高效指令微调的数据估值'}
{'arxiv_id': 'arXiv:2502.09956', 'title': 'KGGen: Extracting Knowledge Graphs from Plain Text with Language Models', 'authors': 'Belinda Mo, Kyssen Yu, Joshua Kazdan, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.09956', 'abstract': "Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.", 'abstract_zh': '近年来，关于构建知识图谱基础模型的兴趣激增，突显了基础挑战之一：知识图谱数据相对稀缺。目前最著名的知识图谱主要由人类标注、通过模式匹配创建，或使用早期自然语言处理技术提取。虽然人类生成的知识图谱数量稀少，但自动提取的知识图谱质量存疑。我们提出了一种解决数据稀缺问题的方案，即一种文本到知识图谱生成器（KGGen），该生成器使用语言模型从纯文本中创建高质量的图。与其它知识图谱提取器不同，KGGen 通过聚类相关实体来减少提取知识图谱中的稀疏性。KGGen 可以通过 Python 库（`pip install kg-gen`）获取，使其对所有人开放。此外，我们还发布了第一个基准测试 Measure of Information in Nodes and Edges (MINE)，用于测试提取器从纯文本生成有用知识图谱的能力。我们将新工具与现有提取器进行了基准测试，并展示了其显著优于现有工具的性能。', 'title_zh': 'KGGen：使用语言模型从Plain Text中提取知识图谱'}
{'arxiv_id': 'arXiv:2502.09952', 'title': "Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe", 'authors': 'Jin Cui, Yifei Zou, Siyuan Zhang', 'link': 'https://arxiv.org/abs/2502.09952', 'abstract': "China's Chang'e 5 mission has been a remarkable success, with the chang'e 5 lander traveling on the Oceanus Procellarum to collect images of the lunar surface. Over the past half century, people have brought back some lunar rock samples, but its quantity does not meet the need for research. Under current circumstances, people still mainly rely on the analysis of rocks on the lunar surface through the detection of lunar rover. The Oceanus Procellarum, chosen by Chang'e 5 mission, contains various kind of rock species. Therefore, we first applied to the National Astronomical Observatories of the China under the Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of the lunar surface image, and established a lunar surface rock image data set CE5ROCK. The data set contains 100 images, which randomly divided into training, validation and test set. Experimental results show that the identification accuracy testing on convolutional neural network (CNN) models like AlexNet or MobileNet is about to 40.0%. In order to make full use of the global information in Moon images, this paper proposes the MRNet (MoonRockNet) network architecture. The encoding structure of the network uses VGG16 for feature extraction, and the decoding part adds dilated convolution and commonly used U-Net structure on the original VGG16 decoding structure, which is more conducive to identify more refined but more sparsely distributed types of lunar rocks. We have conducted extensive experiments on the established CE5ROCK data set, and the experimental results show that MRNet can achieve more accurate rock type identification, and outperform other existing mainstream algorithms in the identification performance.", 'abstract_zh': '中国的嫦娥五号任务取得了举世瞩目的成功，嫦娥五号着陆器在风暴洋地区收集了月球表面的图像。在过去半个世纪里，人类已经带回了一些月球岩石样本，但其数量仍不足以满足研究需求。在当前情况下，人们仍然主要依靠月球车对月球表面岩石进行探测分析。嫦娥五号任务选择了风暴洋地区，该区域包含多种类型的岩石。因此，我们首先向中国科学院国家天文台申请了月球表面成像导航地形相机（NaTeCam）数据，并建立了月球表面岩石图像数据集CE5ROCK。该数据集包含100张图像，并随机分为训练集、验证集和测试集。实验结果表明，针对如AlexNet或MobileNet这类卷积神经网络（CNN）模型的岩石识别精度约为40.0%。为了充分利用月球图像中的全球信息，本文提出了一种新的网络架构MRNet（月球岩分类网络）。该网络的编码结构使用VGG16进行特征提取，在原始VGG16解码结构的基础上加入了扩张卷积和常用的U-Net结构，更有利于识别稀疏分布但更为细致的月球岩石类型。我们在建立的CE5ROCK数据集上进行了广泛的实验，实验结果显示，MRNet可以实现更为准确的岩石类型识别，并在识别性能上优于其他现有主流算法。', 'title_zh': '使用MRNet预测嫦娥五号探测器检测的月球岩石类别'}
{'arxiv_id': 'arXiv:2502.09931', 'title': 'TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation', 'authors': 'Ju-Hyeon Nam, Nur Suriza Syazwany, Sang-Chul Lee', 'link': 'https://arxiv.org/abs/2502.09931', 'abstract': 'Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attentio} to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.', 'abstract_zh': '跳连接工程主要用来解决编码器和解码器之间的语义差距，并结合全局依赖关系以理解复杂解剖结构之间的关系。尽管已有许多模型提出了基于变换器的方法来在跳连接中整合全局依赖关系，但它们往往在捕捉详细局部特征方面面临高计算复杂度的限制。相比之下，图神经网络（GNNs）利用图结构有效地捕捉局部和全局特征。利用这些特性，我们引入了一种注意力的多尺度图神经网络（ACS-GNN），通过将跨尺度特征图转换为图结构并利用节点注意力来增强跳连接框架。另外，我们观察到深度学习模型经常会生成无信息的特征图，这会降低空间注意力图的质量。为了解决这一问题，我们将基于熵驱动的特征选择（EFS）与空间注意力结合，为每个通道计算熵分数，并筛选出高熵的特征图。我们的创新框架TransGUNet 包含ACS-GNN和基于EFS的空间注意力，通过结合GNN和可靠的空域注意力图，跨各种模态提升了领域泛化能力，并确保跳连接中具有更稳健的特征。通过全面的实验和分析，TransGUNet 在六个已见和八个未见的数据集上实现了卓越的分割性能，相比前方法显示出显著更高的效率。', 'title_zh': 'TransGUNet：基于变压器与图结构跳跃连接的医学图像分割方法'}
{'arxiv_id': 'arXiv:2502.09928', 'title': 'Deep Tree Tensor Networks for Image Recognition', 'authors': 'Chang Nie, Junfang Chen, Yajie Chen', 'link': 'https://arxiv.org/abs/2502.09928', 'abstract': 'Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks. Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions. This paper introduces a novel architecture named \\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork} (DTTN), which captures $2^L$-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a \\emph{tree}-like TN topology with the parameter-sharing property. DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation. Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field. We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.', 'abstract_zh': '源自量子物理的张量网络（Tensor Networks, TNs）已被广泛用作指数级机器和参数分解器，以应用于识别任务中。典型的TN模型，如矩阵积态（Matrix Product States, MPS），尚未在自然图片处理中取得成功应用。在实际应用中，它们通常仅用于压缩现成网络中的参数，从而失去了其增强指数级特征交互的独特能力。本文介绍了一种新型架构——\\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork}（DTTN），通过多线性操作捕获特征之间的$2^L$阶乘法交互，同时实际上形成了具有参数共享特性的树状TN拓扑结构。DTTN通过多对抗对称交互模块（Antisymmetric Interacting Modules, AIMs）进行堆叠，这一设计促进了高效的实现。此外，我们从理论上揭示了在某些条件下的量子启发TN模型、多项式网络和多线性网络之间的等价性，并认为DTTN可以激发更多可解释性的研究。我们对所提出模型进行了系列基准测试，并与同类模型和最先进的架构相比，取得了出色的性能。我们的代码不久将公开发布。', 'title_zh': '深度树张量网络在图像识别中的应用'}
{'arxiv_id': 'arXiv:2502.09927', 'title': 'Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence', 'authors': 'Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek-koifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, Rogerio Feris', 'link': 'https://arxiv.org/abs/2502.09927', 'abstract': 'We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See this https URL for model weights.', 'abstract_zh': '我们引入了Granite Vision，这是一种轻量级的大型语言模型，具备视觉能力，特别设计用于企业应用场景，尤其擅长视觉文档理解。我们的模型是在涵盖文档相关任务的综合指令遵循数据集上进行训练的，包括从表格、图表、图表、草图和信息图中提取内容，以及一般的图像任务。Granite Vision的架构以视觉模态对齐为中心，基于一个解码器仅存的20亿参数的大规模语言模型。此外，我们还在测试时引入了一种专门的安全分类方法，该方法利用稀疏的注意力向量来识别潜在有害输入。尽管其架构轻量，但Granite Vision在标准的视觉文档理解基准测试中取得了良好的结果，并在LiveXiv基准测试中表现出色，后者通过使用不断更新的最新发布Arxiv论文集来避免测试集污染。我们以Apache-2许可协议发布该模型，允许进行研究和商业应用，同时提供完整的训练数据和其他相关细节的透明度。模型权重可在以下链接获取：[这里](https://example.com/model_weights)。', 'title_zh': '《granite vision：一种轻量级、开源的多模态企业智能模型》'}
{'arxiv_id': 'arXiv:2502.09925', 'title': 'TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types', 'authors': 'Jiankang Chen, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Feng Cheng, Huihui Xiao, Bin Wen, Fan Yang, Tingting Gao, Di Zhang', 'link': 'https://arxiv.org/abs/2502.09925', 'abstract': 'Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at this https URL.', 'abstract_zh': '多模态视觉语言模型在开放世界应用中正逐渐受到重视，这得益于模型架构、训练技术以及高质量数据的进步。然而，这些模型的表现往往受到特定任务数据不足的限制，导致泛化能力差和倾向性输出。现有的通过增加微调数据集的任务多样性努力受到手动任务标注劳动密集型过程的阻碍，通常只能生成几种类型的任务。为解决这一问题，我们提出了一种名为TaskGalaxy的大规模多模态指令微调数据集，其中包括19,227种层次化任务类型和413,648个样本。TaskGalaxy利用GPT-4o来增加任务多样性，从少量手动定义的任务开始扩展，使用CLIP和GPT-4o筛选出与开源图像最匹配的任务，并生成相关的问题-答案对。多种模型被采用以确保样本质量。这一自动化过程不仅增加了任务多样性，还提高了数据质量，减少了人工干预。将TaskGalaxy集成到LLaVA-v1.5和InternVL-Chat-v1.0模型中，在16个基准测试上表现出显著的性能提升，这突显了任务多样性的关键重要性。TaskGalaxy已在此网址公开发布：[this https URL]。', 'title_zh': 'TaskGalaxy：利用十万余种视觉任务类型扩展多模态指令微调规模'}
{'arxiv_id': 'arXiv:2502.09920', 'title': 'Machine Learning for Phase Estimation in Satellite-to-Earth Quantum Communication', 'authors': 'Nathan K Long, Robert Malaney, Kenneth J Grant', 'link': 'https://arxiv.org/abs/2502.09920', 'abstract': 'A global continuous-variable quantum key distribution (CV-QKD) network can be established using a series of satellite-to-Earth channels. Increased performance in such a network is provided by performing coherent measurement of the optical quantum signals using a real local oscillator, calibrated locally by encoding known information on transmitted reference pulses and using signal phase error estimation algorithms. The speed and accuracy of the signal phase error estimation algorithm are vital to practical CV-QKD implementation. Our work provides a framework to analyze long short-term memory neural network (NN) architecture parameterization, with respect to the quantum Cramér-Rao uncertainty bound of the signal phase error estimation, with a focus on reducing the model complexity. More specifically, we demonstrate that signal phase error estimation can be achieved using a low-complexity NN architecture, without significantly sacrificing accuracy. Our results significantly improve the real-time performance of practical CV-QKD systems deployed over satellite-to-Earth channels, thereby contributing to the ongoing development of the Quantum Internet.', 'abstract_zh': '使用卫星到地面通道可以建立一个全球连续变量量子密钥分发（CV-QKD）网络。通过使用现场本地振荡器进行相干测量，并通过在传输参考脉冲中编码已知信息进行本地校准，同时利用信号相位误差估计算法，可以提高该网络的性能。信号相位误差估计的准确性和速度对于实际CV-QKD的实现至关重要。我们的研究提供了一个框架，用于分析长短期记忆神经网络（NN）架构参数化如何与信号相位误差估计的量子克拉默-拉奥不确定性界相关，重点关注降低模型复杂度。具体来说，我们证明可以通过低复杂度的NN架构实现信号相位误差估计，而不会显著牺牲准确性。我们的结果显著改善了部署在卫星到地面通道上的实际CV-QKD系统的实时性能，从而有助于量子互联网的持续开发。', 'title_zh': '卫星到地面量子通信中的相位估计算法研究'}
{'arxiv_id': 'arXiv:2502.09919', 'title': 'AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset', 'authors': 'Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Eric Kim, Gautham Krishna Gudur, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, Hassan Ghasemzadeh', 'link': 'https://arxiv.org/abs/2502.09919', 'abstract': 'Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.', 'abstract_zh': '糖尿病是一种以持续高血糖水平（血糖水平，BGLs）为特征的慢性代谢疾病，可能导致严重并发症，如心血管疾病、神经病变和视网膜病变。预测血糖水平能够帮助患者维持血糖在安全范围之内，并允许护理人员通过生活方式调整采取主动措施。连续葡萄糖监测（CGM）系统提供了实时跟踪功能，是一个监测血糖水平的重要工具。然而，由于体力活动、饮食和其他因素的影响，准确预测血糖水平仍然具有挑战性。最近的深度学习模型在改善血糖预测方面显示出潜力。然而，从多模态、不规则采样的数据中进行长时间预测仍是一个具有挑战性的研究问题。在本文中，我们提出了一种基于Transformer的多模态框架AttenGluco，用于长期血糖预测。AttenGluco利用交叉注意力有效地整合CGM数据和活动数据，解决了不同采样率数据融合的挑战。此外，它采用多尺度注意力来捕捉时间数据中的长期依赖关系，从而提高预测准确性。为了评估AttenGluco的性能，我们在最近发布的AIREADI数据集上进行了预测实验，分析了其在不同受试群体中的预测准确性，包括健康个体、糖尿病前期患者以及2型糖尿病患者。此外，我们还研究了随着新受试群体的引入，其性能改进和遗忘行为。评估结果表明，AttenGluco在所有误差指标（如均方根误差RMSE、平均绝对误差MAE和相关性）上均优于多模态LSTM模型。与基线模型相比，AttenGluco在RMSE和MAE上的表现分别提高了约10%和15%。', 'title_zh': 'AttenGluco：基于多模态Transformer的AI-READI数据集血糖预测'}
{'arxiv_id': 'arXiv:2502.09891', 'title': 'ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation', 'authors': 'Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma', 'link': 'https://arxiv.org/abs/2502.09891', 'abstract': 'Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in terms of both accuracy and token cost.', 'abstract_zh': '检索增强生成（RAG）方法已被证明在将外部知识集成到大型语言模型（LLMs）中以进行问答（QA）任务时是有效的。最先进的一些RAG方法通常使用图数据作为外部数据，因为图数据能够捕捉丰富的语义信息并反映实体之间的关系。然而，现有的基于图的RAG方法在从图中准确识别相关信息方面存在困难，并且在在线检索过程中消耗了大量的 tokens。为了解决这些问题，我们提出了一种新颖的基于图的RAG方法，称为属性社区层次RAG（ArchRAG），该方法通过使用属性社区增强问题，并引入了一种新颖的基于大型语言模型的层次聚类方法。为了从图中为问题检索最相关的信息，我们构建了一种新颖的层次索引结构来索引属性社区，并开发了一种有效的在线检索方法。实验结果表明，ArchRAG 在准确性和 tokens 成本方面都优于现有方法。', 'title_zh': 'ArchRAG：具属性的社区导向层次检索增强生成'}
{'arxiv_id': 'arXiv:2502.09889', 'title': 'Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination', 'authors': 'Siva Kailas, Shalin Jain, Harish Ravichandar', 'link': 'https://arxiv.org/abs/2502.09889', 'abstract': "Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination. We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance.", 'abstract_zh': '图神经网络（GNNs），由图学习社区发展而来，已被应用于多机器人和多智能体学习，并显示出高度有效性。受到这种成功交叉学习的启发，我们研究和探索了现有GNN解释方法在解释多智能体协调任务中的适用性。我们发现，这些方法有潜力识别出对团队行为影响最大的通信渠道。基于初步分析，我们提出了一种注意力熵正则化项，使基于注意机制的策略更加适合现有的基于图的解释器。直观地讲，通过最小化注意力熵，可以激励智能体将注意力集中在最具影响力或最具有影响性的智能体上，从而减轻解释器面临的挑战。我们通过理论证明这一直觉，即最小化注意力熵增加了解释器生成的子图与其补图之间的差异。通过对三个任务和三种团队规模的评估，我们发现了关于现有解释器有效性的见解，并证明了所提出的正则化方法在不牺牲任务性能的情况下，始终能提高解释质量。', 'title_zh': '基于图的解释方法在多智能体协调中的评估与改进'}
{'arxiv_id': 'arXiv:2502.09886', 'title': 'Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos', 'authors': 'Weirui Ye, Fangchen Liu, Zheng Ding, Yang Gao, Oleh Rybkin, Pieter Abbeel', 'link': 'https://arxiv.org/abs/2502.09886', 'abstract': 'Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.', 'abstract_zh': '仿真提供了一种有前景的方法，通过低成本扩展训练数据来训练通用策略。为了大规模生成多样且真实的任务数据，现有算法要么依赖于可能生成不适用于机器人领域的幻觉任务的大语言模型（LLMs）；要么依赖于数字孪生，这需要精心调整现实与模拟之间的一致性，并且难以扩展。为了解决这些挑战，我们引入了Video2Policy，这是一个新颖的框架，利用互联网上的RGB视频来根据日常人类行为重构任务。我们的方法包括两个阶段：（1）基于视频在仿真中生成任务；（2）利用逐步生成的上下文相关大语言模型（LLM）生成奖励函数进行强化学习。我们通过从Something-Something-v2 （SSv2）数据集中重构超过100个视频来展示Video2Policy的有效性，这些视频展示了9种不同任务中多样且复杂的人类行为。我们的方法可以在这些任务上成功训练强化学习策略，包括复杂的如投掷等具有挑战性的任务。最后，我们展示了生成的仿真数据可以扩展用于训练通用策略，并且可以通过Real2Sim2Real的方式将数据转移到实际机器人中。', 'title_zh': 'Video2Policy：通过互联网视频扩大模拟中的操作任务规模'}
{'arxiv_id': 'arXiv:2502.09885', 'title': 'Comprehensive Review of Neural Differential Equations for Time Series Analysis', 'authors': 'YongKyung Oh, Seungsu Kam, Jonghun Lee, Dong-Young Lim, Sungil Kim, Alex Bui', 'link': 'https://arxiv.org/abs/2502.09885', 'abstract': 'Time series modeling and analysis has become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.', 'abstract_zh': '时间序列建模与分析已成为各个领域的关键。传统的基于循环神经网络（RNNs）和变换器（Transformers）的方法，虽然对离散时间和均匀采样数据非常有效，但在捕捉现实世界中固有的连续动力学和不规则采样模式方面面临着巨大挑战。神经微分方程（Neural Differential Equations, NDEs）代表了一种范式转变，通过结合神经网络的灵活性和微分方程的数学严谨性，提高了建模能力。本文对基于NDE的方法进行了全面回顾，涵盖了神经常微分方程、神经控制微分方程和神经随机微分方程。我们详细讨论了它们的数学表达式、数值方法及其应用，强调了它们在建模连续时间动力学方面的优势。此外，我们还探讨了关键挑战和未来研究方向。此综述为那些希望通过NDE进行高级时间序列分析的研究人员和实践者提供了基础。', 'title_zh': '全面综述神经微分方程在时间序列分析中的应用'}
{'arxiv_id': 'arXiv:2502.09884', 'title': 'Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation', 'authors': 'Seo Taek Kong, Sihan Zeng, Thinh T. Doan, R. Srikant', 'link': 'https://arxiv.org/abs/2502.09884', 'abstract': 'We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first non-asymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\\sqrt{n}$, which significantly improves on the rates of convergence in prior works.', 'abstract_zh': '我们考虑由鞅噪声驱动的线性两时间尺度随机逼近算法。机器学习中的近期应用促使我们了解有限时间误差率，但传统的随机逼近分析主要集中在分布上的渐近收敛性或误差上下界，而这些上下界通常远非最优。关于渐近中心极限定理（CLT）的先前工作表明，两时间尺度算法可能能够实现期望误差为 $1/\\sqrt{n}$，其中常数由极限高斯向量的期望范数给出。然而，目前已知的最佳有限时间误差率要慢得多。我们首次给出了基于Wasserstein-1距离的两时间尺度随机逼近（采用Polyak-Ruppert平均）的非渐近中心极限定理。作为推论，我们证明了Polyak-Ruppert平均所实现的期望误差以 $1/\\sqrt{n}$ 的速率衰减，这显著优于先前工作的收敛速率。', 'title_zh': '非渐近中心极限定理与两时间尺度随机逼近中的误差界'}
{'arxiv_id': 'arXiv:2502.09874', 'title': 'FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation', 'authors': 'Peng Ling', 'link': 'https://arxiv.org/abs/2502.09874', 'abstract': "Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised this http URL this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the this http URL, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full this http URL addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at this https URL.", 'abstract_zh': '核实例分割在病理图像分析中发挥了关键作用。主要挑战来自于准确分割实例的困难以及为全监督方法进行精确掩模级注解的巨大成本。本研究中，我们提出了一种Fourier引导框架，用于解决弱监督核实例分割问题。在该框架中，我们构建了一个Fourier引导模块，将先验信息融入到模型的训练过程中，以便模型能够更有效地捕捉到核相关的特征。为了进一步提高模型表示核特征的能力，我们提出了引导为基础的实例级别对比模块。该模块充分利用了框架自身的特性以及引导信息，有效地增强了核的表示特征。在两个公开数据集上的实验结果表明，在全监督设计中，我们的模型可以优于当前的SOTA方法；在弱监督实验中，仅少量标注微量，我们的模型仍能保持接近全监督下性能的水平。此外，我们还在一个私有数据集上进行了泛化实验，未进行任何标注，我们的模型仍然能够很好地分割训练过程中未见过的核图像。作为开源科学，所有代码和预训练模型已在此链接处提供。', 'title_zh': 'FrGNet：一种基于 Fourier 指导的弱监督框架用于核实例分割'}
{'arxiv_id': 'arXiv:2502.09870', 'title': 'A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies', 'authors': 'Alicia DeVrio, Myra Cheng, Lisa Egede, Alexandra Olteanu, Su Lin Blodgett', 'link': 'https://arxiv.org/abs/2502.09870', 'abstract': 'Recent attention to anthropomorphism -- the attribution of human-like qualities to non-human objects or entities -- of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.', 'abstract_zh': '近年来，对语言技术（如大语言模型LLMs）拟人化的关注——即将人类特质赋予非人类对象或实体的现象——引发了对拟人化潜在负面影响的重新讨论。为了有效讨论这种拟人化的潜在影响及其在什么情境下是适当的，我们需要一个共享的词汇，以涵盖语言拟人化众多表现形式的广泛多样性。在这项研究中，我们借鉴现有文献，并分析用户与语言技术互动的实证案例，以建立一个分类体系，这些文本表达能够促进拟人化的发生。我们强调了理解语言拟人化过程中所面临的挑战与紧张关系，例如所有语言本质上都是人类的，以及试图在机器中刻画和改变人类特性感知时可能会导致某些人的去人性化。我们讨论了分类体系如何支持对语言技术拟人化更为精确和有效的讨论与决策。', 'title_zh': '语言技术中导致拟人化的语言表达类型学'}
{'arxiv_id': 'arXiv:2502.09866', 'title': 'How Users Who are Blind or Low Vision Play Mobile Games: Perceptions, Challenges, and Strategies', 'authors': 'Zihe Ran, Xiyu Li, Qing Xiao, Xianzhe Fan, Franklin Mingzhe Li, Yanyun Wang, Zhicong Lu', 'link': 'https://arxiv.org/abs/2502.09866', 'abstract': "As blind and low-vision (BLV) players engage more deeply with games, accessibility features have become essential. While some research has explored tools and strategies to enhance game accessibility, the specific experiences of these players with mobile games remain underexamined. This study addresses this gap by investigating how BLV users experience mobile games with varying accessibility levels. Through interviews with 32 experienced BLV mobile players, we explore their perceptions, challenges, and strategies for engaging with mobile games. Our findings reveal that BLV players turn to mobile games to alleviate boredom, achieve a sense of accomplishment, and build social connections, but face barriers depending on the game's accessibility level. We also compare mobile games to other forms of gaming, highlighting the relative advantages of mobile games, such as the inherent accessibility of smartphones. This study contributes to understanding BLV mobile gaming experiences and provides insights for enhancing accessible mobile game design.", 'abstract_zh': '随着盲人和低视力（BLV）玩家更深入地参与游戏，无障碍功能已成为必不可少的组成部分。虽然已有研究探讨了提高游戏无障碍性的工具和策略，但这些玩家在移动游戏中的具体体验仍鲜有研究。本研究通过调查不同无障碍水平下BLV用户的游戏体验，填补了这一空白。我们通过对32名经验丰富的BLV移动玩家进行访谈，探讨他们对游戏的感知、面临的挑战以及应对策略。研究发现，BLV玩家转向移动游戏是为了缓解无聊、获得成就感以及建立社交联系，但游戏的无障碍水平会影响他们的参与体验。我们还将移动游戏与其他形式的游戏进行比较，突出了移动游戏的相对优势，如智能手机的固有无障碍性。本研究有助于理解BLV移动游戏体验，并为增强移动游戏设计的无障碍性提供参考。', 'title_zh': '盲人或低视力用户如何玩移动游戏：感知、挑战与策略'}
{'arxiv_id': 'arXiv:2502.09858', 'title': 'Automated Hypothesis Validation with Agentic Sequential Falsifications', 'authors': 'Kexin Huang, Ying Jin, Ryan Li, Michael Y. Li, Emmanuel Candès, Jure Leskovec', 'link': 'https://arxiv.org/abs/2502.09858', 'abstract': "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.", 'abstract_zh': '假设在信息获取、决策制定和发现中起着核心作用。然而，许多实际世界的假设往往是抽象和高层次的陈述，难以直接验证。随着大型语言模型（LLMs）生成假设的现象出现，这一挑战变得更加严峻，因为这些模型容易产生幻觉，并生成大量需要人工验证的假设，从而使手动验证变得不切实际。为此，我们提出了Popper，这是一种代理驱动的框架，旨在实现对自由格式假设的 rigorous 自动验证。Popper 依据 Karl Popper 的证伪原则，使用 LLM 代理设计并执行针对假设可测量影响的证伪实验进行验证。一个新颖的序列性测试框架确保了严格的第一类错误控制，同时积极从各种观察中收集证据，这些观察可以来自于现有数据，也可以是新开展的程序。我们分别在生物学、经济学和社会学六个领域展示了 Popper 的应用。Popper 实现了稳健的错误控制、强大的检验效能和可扩展性。此外，与人类科学家相比，Popper 在验证复杂的生物学假设方面取得了可比的性能，同时将时间缩短了10倍，从而提供了一个高效的、严谨的假设验证解决方案。', 'title_zh': '自动假设验证与代理性的序贯反驳'}
{'arxiv_id': 'arXiv:2502.09854', 'title': 'Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning', 'authors': 'Yu-Chen Lin, Sanat Sharma, Hari Manikandan, Jayant Kumar, Tracy Holloway King, Jing Zheng', 'link': 'https://arxiv.org/abs/2502.09854', 'abstract': 'In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.', 'abstract_zh': '在本文中，我们展示了小型语言模型（SLMs），具体来说是拥有1亿参数的GPT-2模型，可以在多任务提示生成任务中达到与大型语言模型（LLMs）相当的性能，但仅需后者的少量计算资源。通过结合倒置强化学习与来自强大LLM Llama-3的合成数据蒸馏，我们训练了一个SLM，其相关性得分与最先进的模型（包括Llama-3、Qwen2和Mistral）相差不到5%，尽管其规模小了80倍，使其在资源受限和实时应用中尤为适合。本研究强调了SLMs作为多模态环境中的高效多任务学习者的潜力，并为具有可扩展性和低延迟部署能力的LLMs提供了有希望的替代方案。', 'title_zh': '通过倒挂强化学习在小型语言模型中实现高效多任务学习'}
{'arxiv_id': 'arXiv:2502.09838', 'title': 'HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation', 'authors': 'Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi', 'link': 'https://arxiv.org/abs/2502.09838', 'abstract': 'We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at this https URL.', 'abstract_zh': '我们介绍了HealthGPT，这是一个强大的医学多模态（视觉-语言）模型（Med-LVLM），能够在一个统一的自回归框架中集成医学视觉理解与生成能力。我们提出的强化学习哲学是逐步适应异构的理解和生成知识到预训练的大型语言模型（LLMs）中。这一过程通过一种新颖的异构低秩适应技术（H-LoRA）实现，该技术与定制的分层视觉感知方法和三阶段学习策略相结合。为了有效学习HealthGPT，我们设计了一个全面的医学领域特定的视觉-语言理解和生成数据集，称为VL-Health。实验结果表明，HealthGPT在医学多模态统一任务中表现出色且具有良好的可扩展性。我们的项目可以在以下链接访问：[该项目的网址]。', 'title_zh': 'HealthGPT：一种通过异构知识适应实现统一理解和生成的医疗多模态大模型'}
{'arxiv_id': 'arXiv:2502.09829', 'title': 'Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection', 'authors': 'Abrar Anwar, Rohan Gupta, Zain Merchant, Sayan Ghosh, Willie Neiswanger, Jesse Thomason', 'link': 'https://arxiv.org/abs/2502.09829', 'abstract': 'Evaluating learned robot control policies to determine their physical task-level capabilities costs experimenter time and effort. The growing number of policies and tasks exacerbates this issue. It is impractical to test every policy on every task multiple times; each trial requires a manual environment reset, and each task change involves re-arranging objects or even changing robots. Naively selecting a random subset of tasks and policies to evaluate is a high-cost solution with unreliable, incomplete results. In this work, we formulate robot evaluation as an active testing problem. We propose to model the distribution of robot performance across all tasks and policies as we sequentially execute experiments. Tasks often share similarities that can reveal potential relationships in policy behavior, and we show that natural language is a useful prior in modeling these relationships between tasks. We then leverage this formulation to reduce the experimenter effort by using a cost-aware expected information gain heuristic to efficiently select informative trials. Our framework accommodates both continuous and discrete performance outcomes. We conduct experiments on existing evaluation data from real robots and simulations. By prioritizing informative trials, our framework reduces the cost of calculating evaluation metrics for robot policies across many tasks.', 'abstract_zh': '学习得到的机器人控制策略评估耗时费力，以确定其在物理任务层面的能力。随着策略和任务数量的增长，这一问题变得更加严重。测试每一个策略在每一个任务上多次是不切实际的；每次试验都需要手动重置环境，而任务的更改则需要重新布置物体甚至更换机器人。简单地随机选择一部分任务和策略进行评估是一种高成本解决方案，结果可能会不可靠且不完整。在本研究中，我们将机器人评估问题表述为一种积极测试问题。我们提出将机器人在所有任务和策略上的表现分布建模为我们逐步执行实验时的变化过程。任务之间往往存在相似之处，可以揭示策略行为之间的潜在关系，我们证明了自然语言在这种关系建模中是很有用的先验知识。然后，我们利用这一建模方法通过使用成本感知的预期信息增益启发式算法选择信息丰富的试验来减少实验者的努力。我们的框架适用于连续性和离散性表现结果。我们在现有真实机器人和模拟的数据上进行实验，通过优先选择信息丰富的试验，我们的框架能够在众多任务中减少计算机器人策略评估指标的成本。', 'title_zh': '基于主动实验选择的多任务机器人策略高效评估方法'}
{'arxiv_id': 'arXiv:2502.09819', 'title': 'A Solver-Aided Hierarchical Language for LLM-Driven CAD Design', 'authors': 'Benjamin T. Jones, Felix Hähnlein, Zihan Zhang, Maaz Ahmad, Vladimir Kim, Adriana Schulz', 'link': 'https://arxiv.org/abs/2502.09819', 'abstract': 'Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about.', 'abstract_zh': '大型语言模型（LLM）在解决结构化和非结构化生成任务方面取得了巨大成功，但在计算机辅助设计（CAD）中生成工艺几何图形方面仍然面临挑战。这些困难源于其在空间推理能力上的不足，以及需要通过复杂的、长范围的规划来生成复杂的几何图形。我们通过引入一种被称为AIDL的解决器辅助型分层领域特定语言（DSL），使LLM能够实现生成CAD设计。AIDL将空间推理的要求卸载到一个几何约束求解器中。此外，我们还展示了在少样本情况下，AIDL在生成与提示更接近的视觉结果和创建更易于后续处理和推理的对象方面，甚至超过了具备训练数据支持的语言（如OpenSCAD）。', 'title_zh': '辅助求解器的分层语言：面向LLM驱动的CAD设计'}
{'arxiv_id': 'arXiv:2502.09809', 'title': 'AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration', 'authors': 'Jizhou Chen, Samuel Lee Cong', 'link': 'https://arxiv.org/abs/2502.09809', 'abstract': "The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.", 'abstract_zh': '将以下论文内容或标题翻译成中文，符合学术规范：\n\n将工具使用集成到大规模语言模型（LLMs）中能够构建具有实际影响的自主系统。同时，与独立的LLMs不同，被攻击的代理可以利用其工具使用能力更有效地执行恶意工作流，造成更为重大的影响。为此，我们提出了一种名为AgentGuard的框架，该框架能够自主发现和验证不安全的工具使用工作流，并生成安全约束来限制代理的行为，在部署时实现基本的安全保障。AgentGuard利用LLM编排器固有的能力——了解工具功能、规模化和现实可行的工作流生成以及执行工具的权限——作为其自身安全性评估器。该框架通过四个阶段运行：识别不安全的工作流、在实际执行中验证这些工作流、生成安全约束并验证约束的有效性。最终输出包含不安全工作流、测试用例和已验证约束的评估报告，这些都可以应用于多种安全应用场景。我们通过实验实证地展示了AgentGuard的可行性。通过这项探索性工作，我们希望激发建立标准化的LLM代理测试和加固程序，从而增强其在实际应用中的可靠性。', 'title_zh': 'AgentGuard: 重新利用代理型编排器进行工具编排的安全评估'}
{'arxiv_id': 'arXiv:2502.09804', 'title': 'Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models', 'authors': 'Alaa Awad, Salah A. Aly', 'link': 'https://arxiv.org/abs/2502.09804', 'abstract': 'Thousands of individuals succumb annually to leukemia alone. As artificial intelligence-driven technologies continue to evolve and advance, the question of their applicability and reliability remains unresolved. This study aims to utilize image processing and deep learning methodologies to achieve state-of-the-art results for the detection of Acute Lymphoblastic Leukemia (ALL) using data that best represents real-world scenarios. ALL is one of several types of blood cancer, and it is an aggressive form of leukemia. In this investigation, we examine the most recent advancements in ALL detection, as well as the latest iteration of the YOLO series and its performance. We address the question of whether white blood cells are malignant or benign. Additionally, the proposed models can identify different ALL stages, including early stages. Furthermore, these models can detect hematogones despite their frequent misclassification as ALL. By utilizing advanced deep learning models, namely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves accuracy rates as high as 99.7%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations.', 'abstract_zh': '每年都有成千上万的人死于急性淋巴细胞白血病（ALL）。随着人工智能驱动技术的不断发展和进步，这些技术的应用性和可靠性问题依然悬而未决。本研究旨在利用图像处理和深度学习方法，通过最优数据集实现对ALL检测的最先进的成果。ALL是几种类型的血液癌症之一，它是一种具有高度侵袭性的白血病。在此次研究中，我们考察了ALL检测的最新进展，以及YOLO系列的最新版本及其性能。此外，我们还探讨了是否可以区分恶性白血球和良性白血球。所提出的方法还可以识别不同阶段的ALL，包括早期阶段。此外，这些模型还可以检测到血原细胞，尽管这些细胞经常被误分类为ALL。通过使用先进的深度学习模型，如YOLOv8、YOLOv11、ResNet50和Inception-ResNet-v2，本研究实现了高达99.7%的准确率，展示了这些算法在多个数据集和各种实际场景中的有效性。', 'title_zh': '采用YOLOv11、YOLOv8、ResNet50 和 Inception-ResNet-v2 深度学习模型进行急性淋巴细胞白血病诊断'}
{'arxiv_id': 'arXiv:2502.09799', 'title': 'Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators', 'authors': 'Prerna Ravi, John Masla, Gisella Kakoti, Grace Lin, Emma Anderson, Matt Taylor, Anastasia Ostrowski, Cynthia Breazeal, Eric Klopfer, Hal Abelson', 'link': 'https://arxiv.org/abs/2502.09799', 'abstract': 'The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL.', 'abstract_zh': '生成式人工智能的兴起，尤其是大型语言模型（LLMs），为以学生为中心和主动学习方法，如基于项目的学习（PBL）打开了大门。然而，PBL 在项目设计与管理、评估以及平衡学生指导与自主性方面为教育者带来了实际实施挑战。以下研究记录了一项针对跨学科 K-12 教师的共同设计过程，旨在探讨和解决他们目前面临的 PBL 挑战。通过教师主导的访谈、协作研讨会以及迭代设计的工作草案，我们收集了证据，展示了大型语言模型如何通过自动化常规任务和增强个性化学习支持教师实施高质量的 PBL 教学方法。研究中的教师强调支持其专业发展，增强其现有角色的功能，而不会取代他们。他们还指出了课堂整合的机遇与挑战，包括资源需求和限制、伦理关切以及即时和长期影响。基于这些发现，我们提出了未来在 PBL 中部署 AI 工具的设计指南。', 'title_zh': '与K12教育工作者共同设计基于项目的大型语言模型工具'}
{'arxiv_id': 'arXiv:2502.09797', 'title': 'A Survey on LLM-based News Recommender Systems', 'authors': 'Rongyao Wang, Veronica Liesaputra, Zhiyi Huang', 'link': 'https://arxiv.org/abs/2502.09797', 'abstract': 'News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.', 'abstract_zh': '新闻推荐系统在缓解信息过载问题中扮演着关键角色。近年来，由于大型语言模型技术的成功应用，研究者们利用区分性大型语言模型（DLLMs）或生成性大型语言模型（GLLMs）来提高新闻推荐系统的性能。虽然已有若干综述性研究讨论了基于深度学习的新闻推荐系统的重大挑战，如公平性、隐私保护和责任感，但在基于大型语言模型（LLMs）的新闻推荐系统方面缺乏系统的综述性研究。为了系统地回顾不同核心方法并探索潜在问题，我们将区分性大型语言模型和生成性大型语言模型基于的新闻推荐系统纳入LLM-基于的新闻推荐系统的大框架下。在本文综述中，我们首先概述了基于深度学习的新闻推荐系统的发展。接着，我们从新闻导向建模、用户导向建模和预测导向建模三个方面回顾基于LLMs的新闻推荐系统。然后，我们从多个角度探讨挑战，包括数据集、基准工具和方法论。此外，我们进行了广泛实验，分析大型语言模型技术如何影响不同新闻推荐系统的性能。最后，我们全面探讨了LLM时代的基于LLMs的新闻推荐系统的未来研究方向。', 'title_zh': '基于大型语言模型的新闻推荐系统综述'}
{'arxiv_id': 'arXiv:2502.09787', 'title': 'TableTalk: Scaffolding Spreadsheet Development with a Language Agent', 'authors': 'Jenny T. Liang, Aayush Kumar, Yasharth Bajpai, Sumit Gulwani, Vu Le, Chris Parnin, Arjun Radhakrishna, Ashish Tiwari, Emerson Murphy-Hill, Guastavo Soares', 'link': 'https://arxiv.org/abs/2502.09787', 'abstract': "Despite its ubiquity in the workforce, spreadsheet programming remains challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs to write formulas) and problem-solving skills to create complex spreadsheets. Large language models (LLMs) can help automate aspects of this process, and recent advances in planning and reasoning have enabled language agents, which dynamically plan, use tools, and take iterative actions to complete complex tasks. These agents observe, plan, and act, making them well-suited to scaffold spreadsheet programming by following expert processes.\nWe present TableTalk, a language agent that helps programmers build spreadsheets conversationally. Its design reifies three design principles -- scaffolding, flexibility, and incrementality -- which we derived from two studies of seven programmers and 62 Excel templates. TableTalk structures spreadsheet development by generating step-by-step plans and suggesting three next steps users can choose from. It also integrates tools that enable incremental spreadsheet construction. A user study with 20 programmers shows that TableTalk produces spreadsheets 2.3 times more likely to be preferred over a baseline agent, while reducing cognitive load and time spent reasoning about spreadsheet actions by 12.6%. TableTalk's approach has implications for human-agent collaboration. This includes providing persistent direct manipulation interfaces for stopping or undoing agent actions, while ensuring that such interfaces for accepting actions can be deactivated.", 'abstract_zh': '尽管电子表格编程在工作中无处不在，但编程人员仍需掌握电子表格特定的知识（例如，编写公式的API）和解决问题的能力，才能创建复杂的电子表格。大型语言模型（LLMs）可以帮助自动化此过程中的某些方面，近期规划和推理方面的进展使得语言代理能够动态规划、使用工具并采取迭代行动以完成复杂任务。这些代理观察、计划和行动，因此它们非常适合通过遵循专家流程来搭建电子表格编程。\n\n我们提出了一种名为TableTalk的语言代理，它帮助编程人员通过对话方式构建电子表格。其设计遵循了三条设计原则——搭建、灵活性和逐步性，这三条原则源自对七位编程人员和62个Excel模板的两个研究。TableTalk通过生成逐步计划并建议用户提供三种可选的下一步操作，来结构化电子表格的开发工作。此外，TableTalk还整合了用于逐步构建电子表格的工具。一项涉及20位编程人员的用户研究表明，与基准代理相比，TableTalk生成的电子表格被更偏好（2.3倍的可能性），同时减少了12.6%的认知负担和用于推理电子表格操作的时间。TableTalk的方法对人机协作具有重要意义，包括提供持久的直接操作界面以停止或撤销代理操作，同时确保可接受操作的界面能够被禁用。\n\n这种人机协作的方法还具有其他影响。它包括提供持久的直接操作界面以停止或撤销代理操作，同时确保接受操作的界面可以被禁用。', 'title_zh': 'TableTalk：语言代理辅助的电子表格开发支架'}
{'arxiv_id': 'arXiv:2502.09782', 'title': 'Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models', 'authors': 'Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai', 'link': 'https://arxiv.org/abs/2502.09782', 'abstract': "The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet's performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios.", 'abstract_zh': '随着日常设备中麦克风的不断增加以及对在线服务的日益依赖，键盘遭受声学侧信道攻击（ASCAs）的风险也随之增大。本研究探索了深度学习技术，特别是视觉变换器（VTs）和大型语言模型（LLMs），以提高此类攻击的有效性和适用性。我们在此前研究的基础上取得了显著的改进，其中CoAtNet模型达到了最先进的性能。相对于之前的基准，CoAtNet模型在通过智能手机（Phone）记录的按键信息上提高了5.0%，通过Zoom记录的按键信息上提高了5.9%。我们还评估了变换器架构和语言模型，最佳的VT模型与CoAtNet的性能持平。一个重要的进步是引入了一种用于实际场景中的噪声抑制方法。通过使用LLMs进行上下文理解，我们可以在嘈杂环境中检测并纠正错误的按键输入，从而提高ASCAs的效果。此外，使用低秩适应（LoRA）微调的轻量级语言模型也能够达到与参数量大得多的模型相当的性能，仅为其六十七分之一。这种将VTs和LLMs相结合的方法提高了ASCAs缓解的实用性，标志着首次在实际场景中使用这些技术来应对ASCAs和错误纠正。', 'title_zh': '使用变换器和大规模语言模型提高键盘的声学侧信道攻击效果'}
{'arxiv_id': 'arXiv:2502.09780', 'title': 'Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games', 'authors': 'Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi', 'link': 'https://arxiv.org/abs/2502.09780', 'abstract': "Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.", 'abstract_zh': '多智能体强化学习（MARL）在涉及一群智能体在共享未知环境中相互作用的应用中占据核心地位。研究MARL的一个重要框架是马尔可夫博弈，其目标是在样本有效的方式下寻找各种均衡概念，如纳什均衡（NE）和粗略相关均衡（CCE）。然而，现有的样本有效方法要么需要在函数逼近下定制不确定性估计，要么需要仔细协调各个参与者的策略。本文提出了一种新颖的基于模型的算法，称为VMG，该算法通过将模型参数的经验估计偏向于所有玩家在固定其他玩家策略时的集体最优响应值更高的参数，来激励探索，从而鼓励策略偏离当前的均衡进行更多的探索。VMG对不同的函数逼近形式无足轻重，并允许所有玩家同时且解耦的策略更新。在理论上，我们证明在在线环境中，VMG在线性函数逼近条件下，分别实现了寻找两玩家零和马尔可夫博弈的NE和多玩家一般和马尔可夫博弈的CCE的近似最优后悔，几乎与具有复杂不确定性量化的方法相匹配。', 'title_zh': '无需奖金的激励机制：基于模型的在线多智能体强化学习在马尔可夫游戏中的可证明高效算法'}
{'arxiv_id': 'arXiv:2502.09777', 'title': 'On the existence of EFX allocations in multigraphs', 'authors': 'Alkmini Sgouritsa, Minas Marios Sotiriou', 'link': 'https://arxiv.org/abs/2502.09777', 'abstract': 'We study the problem of "fairly" dividing indivisible goods to several agents that have valuation set functions over the sets of goods. As fair we consider the allocations that are envy-free up to any good (EFX), i.e., no agent envies any proper subset of the goods given to any other agent. The existence or not of EFX allocations is a major open problem in Fair Division, and there are only positive results for special cases.\n[George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023] introduced a restriction on the agents\' valuations according to a graph structure: the vertices correspond to agents and the edges to goods, and each vertex/agent has zero marginal value (or in other words, they are indifferent) for the edges/goods that are not adjacent to them. The existence of EFX allocations has been shown for simple graphs with general monotone valuations [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023], and for multigraphs for restricted additive valuations [Alireza Kaviani, Masoud Seddighin, Amir Mohammad Shahrezaei 2024].\nIn this work, we push the state-of-the-art further, and show that the EFX allocations always exists in multigraphs and general monotone valuations if any of the following three conditions hold: either (a) the multigraph is bipartite, or (b) each agent has at most $\\lceil \\frac{n}{4} \\rceil -1$ neighbors, where $n$ is the total number of agents, or (c) the shortest cycle with non-parallel edges has length at least 6.', 'abstract_zh': '我们将研究“公平”地分配不可分割物品的问题，这些物品集合上的估值函数由多个具有估值集函数的代理人所拥有。在这里，“公平”意味着“envy-free up to any good”（EFX），即没有任何代理人所获得的物品集合（除去一个特定物品后）比其他代理人的物品集合更具有优势。EFX 分配的存在性是公平分配领域的重大开放问题，目前仅在一些特殊情况下取得了积极的研究成果。\n\n乔治·克里斯托多洛鲁斯、阿莫斯·费耶特、伊利亚斯·库特索普斯和阿尔克米尼·斯古罗茨娅在2023年提出了一种代理估值的限制，依据图结构：图的顶点代表代理人，边代表物品，每个顶点/代理人对未相邻的边/物品的边际价值为零（换句话说，他们对该物品是无所谓的）。EFX 分配在简单图和一般增益单调的估值下已经得到证明 [乔治·克里斯托多洛鲁斯、阿莫斯·费耶特、伊利亚斯·库特索普斯和阿尔克米尼·斯古罗茨娅 2023]，在多重图和受限可加估值下也已得到证明 [阿里雷扎·卡维扬尼、马苏德·塞迪丁和阿米尔·穆罕默德·沙赫雷扎伊 2024]。\n\n在本研究中，我们进一步推进了该领域的研究，证明了在多重图和增益单调估值的情况下，如果满足以下三个条件之一，EFX 分配总是存在的：(a) 多重图是二部图，或 (b) 每个代理人的邻居数目不超过 $\\lceil \\frac{n}{4} \\rceil -1$，其中 $n$ 是代理人的总数，或 (c) 具有非平行边的最短环路的边数至少为6。', 'title_zh': '在多图中EFX分配的存在性'}
{'arxiv_id': 'arXiv:2502.09767', 'title': 'Non-Markovian Discrete Diffusion with Causal Language Models', 'authors': 'Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk', 'link': 'https://arxiv.org/abs/2502.09767', 'abstract': 'Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.', 'abstract_zh': '离散扩散模型在结构化序列建模方面展现出了灵活且可控的范式，但在表达能力上仍落后于因果语言模型。为了弥合这两种范式之间的差距，我们提出了CaDDi——一种在非马尔可夫扩散框架下统一序列和时间建模的因果离散扩散模型。与传统的逐步骤操作且无法访问先前状态的扩散模型不同，CaDDi 结合了时间轨迹的建模能力，从而能够实现更丰富的表达性和可控性生成。此外，该方法还将因果语言模型视为一种特殊情况，使得无需进行架构修改即可无缝地采用预训练的大规模语言模型（LLMs）进行离散扩散建模。实验证明，CaDDi 在自然语言和生物序列任务上均优于现有的顶级离散扩散模型，缩小了基于扩散的方法与大规模自回归变压器之间的差距。', 'title_zh': '非马尔可夫离散扩散与因果语言模型'}
{'arxiv_id': 'arXiv:2502.09765', 'title': 'Differential Adjusted Parity for Learning Fair Representations', 'authors': 'Bucher Sahyouni, Matthew Vowels, Liqun Chen, Simon Hadfield', 'link': 'https://arxiv.org/abs/2502.09765', 'abstract': 'The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.', 'abstract_zh': '公平且无偏见的机器学习模型的开发仍然是人工智能领域的研究者们持续追求的目标。本文介绍了一种新的差异调整平等（Differential Adjusted Parity, DAP）损失函数，以生成无偏的有用表示。该方法利用可微分的调整平等度量的变体来构建一个统一的目标函数。通过结合下游任务分类准确性和这些任务在敏感特征域上的不一致性，提供了一个单一工具以提高性能并减轻偏见。该方法的关键在于使用软平衡准确度。与之前非对抗性的方法相比，DAP 不会因为所有敏感域表现相同低劣而满足该度量，从而避免了退化。在我们的分析中，DAP 在下游任务准确性和公平性方面优于几个对抗模型。特别是在人口平等性、等价概率和敏感特征准确度方面，与表现最佳的对抗方法相比，DAP 分别提高了 22.5%、44.1% 和 40.1%。总体而言，DAP 损失函数及其相关度量在创造更为公平的机器学习模型方面可以发挥重要作用。', 'title_zh': '差异调整平等性学习公平表示'}
{'arxiv_id': 'arXiv:2502.09762', 'title': 'Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and Deployment', 'authors': 'Yang Li, Junfan Chen, Feng Xue, Jiabin Qiu, Wenbin Li, Qingrui Zhang, Ying Wen, Wei Pan', 'link': 'https://arxiv.org/abs/2502.09762', 'abstract': 'Adaptive teaming, the ability to collaborate with unseen teammates without prior coordination, remains an underexplored challenge in multi-robot collaboration. This paper focuses on adaptive teaming in multi-drone cooperative pursuit, a critical task with real-world applications such as border surveillance, search-and-rescue, and counter-terrorism. We first define and formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone \\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a comprehensive framework that integrates simulation, algorithm training and real-world deployment. AT-MDP framework provides a flexible experiment configurator and interface for simulation, a distributed training framework with an extensive algorithm zoo (including two newly proposed baseline methods) and an unseen drone zoo for evaluating adaptive teaming, as well as a real-world deployment system that utilizes edge computing and Crazyflie drones. To the best of our knowledge, AT-MDP framework is the first adaptive framework for continuous-action decision-making in complex real-world drone tasks, enabling multiple drones to coordinate effectively with unseen teammates. Extensive experiments in four multi-drone pursuit environments of increasing difficulty confirm the effectiveness of AT-MDP framework, while real-world deployments further validate its feasibility in physical systems. Videos and code are available at this https URL.', 'abstract_zh': '自适应团队协作能力，即在无需事先协调的情况下与其他未见过的团队成员协作的能力，在多机器人协同作业中仍是一个未被充分探索的挑战。本文重点关注多无人机协同追踪中的自适应团队协作问题，这是一个在边防巡逻、搜索救援和反恐等领域具有实际应用前景的关键任务。我们首先定义并形式化了多无人机追踪中的自适应团队协作问题（AT-MDP，Adaptive Teaming in Multi-Drone Pursuit），并引入了综合了模拟、算法训练和实际部署的AT-MDP框架。AT-MDP框架提供了一个灵活的实验配置器和接口进行模拟，一个分布式训练框架，包含广泛的大规模算法集合（包括两种新提出的基线算法）和一个未见过的无人机集合来进行自适应团队协作评估，以及一个利用边缘计算和Crazyflie无人机的实际部署系统。据我们所知，AT-MDP框架是首个用于复杂真实无人机任务中的连续动作决策自适应框架，能够使多部无人机与未知队友有效协调。在四个不同难度的多无人机追踪环境中进行的大量实验验证了AT-MDP框架的有效性，实际部署进一步证明了其在物理系统中的可行性。相关视频和代码可在以下链接获取：[提供链接位置]', 'title_zh': '多无人机协同追踪中的自适应团队组建：仿真、培训与部署'}
{'arxiv_id': 'arXiv:2502.09757', 'title': 'The AI-Therapist Duo: Exploring the Potential of Human-AI Collaboration in Personalized Art Therapy for PICS Intervention', 'authors': 'Bereket A. Yilma, Chan Mi Kim, Geke Ludden, Thomas van Rompay, Luis A. Leiva', 'link': 'https://arxiv.org/abs/2502.09757', 'abstract': 'Post-intensive care syndrome (PICS) is a multifaceted condition that arises from prolonged stays in an intensive care unit (ICU). While preventing PICS among ICU patients is becoming increasingly important, interventions remain limited. Building on evidence supporting the effectiveness of art exposure in addressing the psychological aspects of PICS, we propose a novel art therapy solution through a collaborative Human-AI approach that enhances personalized therapeutic interventions using state-of-the-art Visual Art Recommendation Systems. We developed two Human-in-the-Loop (HITL) personalization methods and assessed their impact through a large-scale user study (N=150). Our findings demonstrate that this Human-AI collaboration not only enhances the personalization and effectiveness of art therapy but also supports therapists by streamlining their workload. While our study centres on PICS intervention, the results suggest that human-AI collaborative Art therapy could potentially benefit other areas where emotional support is critical, such as cases of anxiety and depression.', 'abstract_zh': '重症监护后综合征（PICS）是一种由长期住在重症监护病房（ICU）引起多方面的状况。随着预防ICU患者出现PICS的重要性日益增加，目前的有效干预措施仍较为有限。基于艺术暴露对缓解PICS的心理方面有积极作用的证据，我们提出了一种通过协作人机（Human-AI）方法来提升个性化治疗效果的新颖艺术疗法解决方案。该方案利用最先进的视觉艺术推荐系统来增强个性化治疗干预。我们开发了两种人机环回（Human-in-the-Loop, HITL）个性化方法，并通过大规模用户研究（N=150）评估了其影响。研究结果表明，这种人机协作不仅提高了艺术疗法的个性化和有效性，还通过简化治疗师的工作流程支持了他们的工作。虽然我们的研究重点是PICS干预，但结果表明，人机协作艺术疗法可能在其他需要情感支持的领域中也具有潜在的益处，例如焦虑和抑郁症等病例。', 'title_zh': '人工智能治疗师伙伴：探索个性化艺术治疗在PICS干预中人机协作的潜力'}
{'arxiv_id': 'arXiv:2502.09749', 'title': 'Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting', 'authors': 'Chaoyuan Zhang, Zhaowei Li, Wentao Yuan', 'link': 'https://arxiv.org/abs/2502.09749', 'abstract': 'Integrating large language models (LLMs) into closed-loop robotic task planning has become increasingly popular within embodied artificial intelligence. Previous efforts mainly focused on leveraging the strong reasoning abilities of LLMs to enhance task planning performance while often overlooking task planning efficiency and executability due to repetitive queries to LLMs. This paper addresses the synergy between LLMs and task planning systems, aiming to minimize redundancy while enhancing planning effectiveness. Specifically, building upon Prog-Prompt and the high-level concept of Tree-Planner, we propose Vote-Tree-Planner. This sampling strategy utilizes votes to guide plan traversal during the decision-making process. Our approach is motivated by a straightforward observation: assigning weights to agents during decision-making enables the evaluation of critical paths before execution. With this simple vote-tree construction, our method further improves the success rate and reduces the number of queries to LLMs. The experimental results highlight that our Vote-Tree-Planner demonstrates greater stability and shows a higher average success rate and goal condition recall on the unseen dataset compared with previous baseline methods. These findings underscore the potential of the Vote-Tree-Planner to enhance planning accuracy, reliability, and efficiency in LLM-based planning systems.', 'abstract_zh': '将以下论文的内容或标题翻译成中文，同时符合学术规范：\n\n将大型语言模型（LLMs）整合到闭环机器人任务规划中在具身人工智能领域中越来越受欢迎。以往的研究主要集中在利用LLMs的强大推理能力来提升任务规划性能，但往往会忽略任务规划的效率和可执行性，因为这往往会导致重复查询LLMs。本文探讨了LLMs与任务规划系统之间的协同作用，旨在减少冗余同时提高规划的有效性。具体来说，本文在Prog-Prompt的基础上，结合高阶概念Tree-Planner，提出了Vote-Tree-Planner这一采样策略。该策略通过投票来指导决策过程中的计划遍历。我们的方法受到了一个简单的观察启发：在决策过程中赋予代理权重，可以在执行前评估关键路径。通过这种简单的投票树构建方式，我们的方法进一步提高了成功率，并减少了对LLMs的查询次数。实验结果表明，相比于先前的基线方法，我们的Vote-Tree-Planner在未见过的数据集上展示了更高的稳定性和平均成功率及目标条件召回率。这些发现突显了Vote-Tree-Planner在基于LLMs的任务规划系统中增强规划准确性、可靠性和效率的潜力。', 'title_zh': '《Vote-Tree-Planner：通过投票优化基于大语言模型的任务规划管道中的执行顺序》'}
{'arxiv_id': 'arXiv:2502.09731', 'title': 'A CNN Approach to Automated Detection and Classification of Brain Tumors', 'authors': 'Md. Zahid Hasan, Abdullah Tamim, D.M. Asadujjaman, Md. Mahfujur Rahman, Md. Abu Ahnaf Mollick, Nosin Anjum Dristi, Abdullah-Al-Noman', 'link': 'https://arxiv.org/abs/2502.09731', 'abstract': 'Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance com- plicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, mak- ing it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded.', 'abstract_zh': '脑肿瘤需要进行评估，以确保及时诊断和有效的患者治疗。形态学因素如大小、位置、纹理和不规则的外观都使肿瘤检测复杂化。医学成像是具有挑战性的，包括噪声和不完整的图像。本文介绍了一种处理磁共振成像（MRI）数据的方法，涵盖了图像分类和去噪的技术。有效利用MRI图像使医护人员能够检测脑部疾病，包括肿瘤。本研究旨在通过分析提供的MRI数据来对健康脑组织和脑肿瘤进行分类。与计算机断层扫描（CT）等替代方法不同，MRI技术能更详细地呈现内部解剖结构，使其成为研究与脑肿瘤相关数据的合适选项。首先对MRI图像使用各向异性扩散滤波器进行去噪处理。用于模型创建的数据集是3,264例脑部MRI扫描的公开且验证过的脑肿瘤分类（MRI）数据库。使用SMOTE进行数据增强和数据集平衡。利用卷积神经网络（CNN），如ResNet152V2、VGG、ViT和EfficientNet进行分类过程。EfficientNet的准确率达到98%，这是目前记录到的最高值。', 'title_zh': '一种基于CNN的自动脑肿瘤检测与分类方法'}
{'arxiv_id': 'arXiv:2502.09723', 'title': 'Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models', 'authors': 'Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang', 'link': 'https://arxiv.org/abs/2502.09723', 'abstract': "Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to systematically examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into code-style structured query to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, ant the results show that QueryAttack achieves high attack success rates (ASRs) across LLMs with different developers and capabilities. We also evaluate QueryAttack's performance against common defenses, confirming that it is difficult to mitigate with general defensive techniques. To defend against QueryAttack, we tailor a defense method which can reduce ASR by up to 64\\% on GPT-4-1106. The code of QueryAttack can be found on this https URL.", 'abstract_zh': '近年来，大规模语言模型（LLMs）在自然语言处理领域展示出了显著的潜力。然而，LLMs 面临着重大的安全和伦理风险。尽管安全对齐等技术被开发用来防御，之前的研究所揭示，通过精心设计的 jailbreak 攻击，可以绕过这些防御措施。本文提出了一种名为 QueryAttack 的新颖框架，以系统地检验安全对齐的泛化能力。通过将 LLM 视作知识数据库，我们将自然语言中的恶意查询转化为代码风格的结构化查询，以绕过 LLM 的安全对齐机制。我们在主流的 LLM 上进行了广泛的实验，结果表明，QueryAttack 在不同开发者和能力的 LLM 中都实现了较高的攻击成功率 (ASR)。我们还评估了 QueryAttack 对常见防御措施的性能，确认一般的防御技术难以缓解这一攻击。为了对抗 QueryAttack，我们提供了一种定制的防御方法，该方法在 GPT-4-1106 上将攻击成功率 (ASR) 降低了高达 64%。QueryAttack 的代码可在以下链接找到：[提供的网址]。', 'title_zh': '将它们转化为恶意数据库：利用查询代码突破对齐的大语言模型的安全防护'}
{'arxiv_id': 'arXiv:2502.09715', 'title': "Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data", 'authors': 'Yu Leng, Yingnan He, Colin Magdamo, Ana-Maria Vranceanu, Christine S. Ritchie, Shibani S. Mukerji, Lidia M. V. R. Moura, John R. Dickson, Deborah Blacker, Sudeshna Das', 'link': 'https://arxiv.org/abs/2502.09715', 'abstract': "Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4o's potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future.", 'abstract_zh': '在电子健康记录（EHRs）中识别认知障碍对于及时诊断和促进研究都至关重要。关于认知障碍的信息通常存在于EHR中的非结构化临床笔记中，但手动审查病历既耗时又容易出错。为了解决这一问题，我们的研究评估了使用零样本GPT-4o的自动化方法来确定不同任务中的认知障碍阶段。首先，我们在来自马萨诸塞综合医院（MGH）记忆诊所的769名患者的专科笔记中评估了GPT-4o确定临床痴呆评定（CDR）全局评分的能力，并获得了加权卡帕系数0.83。其次，我们在860名老年医疗保险患者三年时间窗口内的所有笔记中评估了GPT-4o区分正常认知、轻度认知障碍（MCI）和痴呆的能力。GPT-4o相对于专科病历审查的加权卡帕系数为0.91，并且在临床仲裁者高度自信的案例中达到了0.96。我们的研究结果展示了GPT-4o在未来作为可扩展的病历审查工具以创建研究数据集并辅助临床诊断的潜力。', 'title_zh': '评估GPT在识别电子健康数据中认知障碍阶段方面的能力'}
{'arxiv_id': 'arXiv:2502.09692', 'title': 'NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations', 'authors': 'Maurits Bleeker, Matthias Dorfer, Tobias Kronlachner, Reinhard Sonnleitner, Benedikt Alkin, Johannes Brandstetter', 'link': 'https://arxiv.org/abs/2502.09692', 'abstract': 'Recent advancements in neural operator learning are paving the way for transformative innovations in fields such as automotive aerodynamics. However, key challenges must be overcome before neural network-based simulation surrogates can be implemented at an industry scale. First, surrogates must become scalable to large surface and volume meshes, especially when using raw geometry inputs only, i.e., without relying on the simulation mesh. Second, surrogates must be trainable with a limited number of high-fidelity numerical simulation samples while still reaching the required performance levels. To this end, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), which separates geometry encoding and physics predictions, ensuring flexibility with respect to geometry representations and surface sampling strategies. GP-UPT enables independent scaling of the respective parts of the model according to practical requirements, offering scalable solutions to open challenges. GP-UPT circumvents the creation of high-quality simulation meshes, enables accurate 3D velocity field predictions at 20 million mesh cells, and excels in transfer learning from low-fidelity to high-fidelity simulation datasets, requiring less than half of the high-fidelity data to match the performance of models trained from scratch.', 'abstract_zh': '近年来，神经算子学习的进展为汽车空气动力学等领域带来了变革性的创新。然而，在实现基于神经网络的仿真代理之前，必须克服一些关键挑战。首先，代理模型必须能够处理大型表面和体积网格，特别是在仅使用原始几何输入而不需要依赖仿真网格的情况下。其次，代理模型必须能够在有限的高精度数值仿真样本数量下进行训练，并达到所需的性能水平。为此，我们提出了几何信息保存的通用物理转换器（Geometry-preserving Universal Physics Transformer，GP-UPT），该模型将几何编码与物理预测分离，确保对几何表示和表面采样策略的灵活性。GP-UPT 允许根据实际需求独立缩放模型的不同部分，从而为开放挑战提供可扩展的解决方案。GP-UPT 无需创建高质量的仿真网格，能够在 2000 万网格单元下实现准确的三维速度场预测，并在从低精度仿真数据集到高精度仿真数据集的迁移学习方面表现出色，其性能仅需原来一半的高精度数据即可匹配从零开始训练的模型。', 'title_zh': 'NeuralCFD：高保真汽车流体动力学仿真中的深度学习'}
{'arxiv_id': 'arXiv:2502.09690', 'title': 'Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes', 'authors': 'Taylan G. Topcu, Mohammed Husain, Max Ofsa, Paul Wach', 'link': 'https://arxiv.org/abs/2502.09690', 'abstract': 'Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare AI generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the-art algorithms cannot differentiate AI-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, AI generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting AI suggested feedback, at least when generated by multi-purpose LLMs.', 'abstract_zh': '多用途大型语言模型（LLMs），作为生成型人工智能（AI）的一个子集，最近取得了显著进展。尽管人们期待LLMs能够辅助系统工程（SE）任务，但从系统的跨学科复杂性和综合需求来看，LLMs生成SE制品的有效性仍存在疑问，特别是在它们是通过广泛互联网数据训练的背景下。因此，我们提出了一项基于经验的研究结果，其中使用人力专家生成的SE制品作为基准，解析并输入到多种LLMs中，通过提示工程生成典型的SE制品片段。这一过程未进行任何微调或校准，以记录LLMs的基本性能。然后，我们采用两阶段混合方法来比较AI生成的制品与基准制品。首先，我们使用自然语言处理算法进行定量比较，发现当谨慎提示时，最先进的算法无法区分AI生成的制品和人类专家的基准。其次，我们进行定性的深入分析，探究它们在质量上的差异。我们记录到，尽管这两种制品看起来非常相似，但AI生成的制品表现出严重且难以察觉的缺陷，这些缺陷包括：过早的要求定义、缺乏依据的数字估算以及过度具体化的倾向。我们认为，这项研究表明，系统工程社区在采用由多用途LLMs生成的AI建议反馈时，必须更为谨慎，至少在目前阶段是如此。', 'title_zh': '“谨防信任：大型语言模型生成专家级系统工程制品能力的混合方法探索及故障模式表征”\n\n这个标题翻译成中文时，尽量保持原文的意思和结构，同时确保中文表达的准确性和流畅性。以下是逐句翻译和解释：\n\n1. **Trust at Your Own Peril**：谨防信任。\n   - 这部分可以理解为一种警告，提醒人们在信任大型语言模型生成的内容时要小心。\n\n2. **A Mixed Methods Exploration**：混合方法探索。\n   - “混合方法”指的是使用定性和定量研究方法相结合的研究方式。\n\n3. **of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts**：大型语言模型生成专家级系统工程制品的能力。\n   - 这部分说明研究的对象和目标，即大型语言模型生成类似于专家级别的系统工程文件和制品的能力。\n\n4. **and a Characterization of Failure Modes**：以及故障模式表征。\n   - 这部分说明研究的另一项内容，即识别和描述这些模型可能产生的故障模式。\n\n综合这些部分，最终的翻译为：“谨防信任：大型语言模型生成专家级系统工程制品能力的混合方法探索及故障模式表征”。'}
{'arxiv_id': 'arXiv:2502.09688', 'title': 'Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling', 'authors': 'Benjamin D. Killeen, Bohua Wan, Aditya V. Kulkarni, Nathan Drenkow, Michael Oberst, Paul H. Yi, Mathias Unberath', 'link': 'https://arxiv.org/abs/2502.09688', 'abstract': 'Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.', 'abstract_zh': '人工智能（AI）有望通过数据驱动的洞察力实现个性化和高效的医疗服务而彻底改变医疗保健。尽管放射学是AI采用的前沿领域，但在实践中，AI模型的潜力常常被严重的泛化失败所掩盖：从受控测试环境转换到放射科医生的临床使用时，这些模型的性能可能会下降高达20%。这种不匹配引发了关于放射科医生是否会因AI错误预测而误导，或对AI产生不信任的担忧，这使得这些有前景的技术在实际应用中难以发挥作用。因此，通过大量的多样数据进行详尽的临床试验以预测AI模型在遇到不同数据样本时的性能下降变得至关重要。然而，收集各种数据样本及其相应的标注数据成本高昂，这给这一过程带来了挑战。为克服这一限制，我们提出了一种新的条件生成AI模型，专门用于虚拟临床试验（VCTs）中的放射学AI，能够真实地合成患者带有指定属性的全身CT图像。通过学习图像和解剖结构的联合分布，我们的模型能够以前所未有的细节规模精确复制真实世界的患者群体。我们通过由合成CT研究人群驱动的VCTs对放射学AI模型进行了有意义的评估，揭示了模型性能的下降，并促进了对偏见诱导数据属性的算法审计。我们提出的基于生成AI的VCT方法为评估模型稳健性、减轻偏见和确保患者护理安全提供了一种可扩展的途径，通过这种方法可以使AI模型在各种不同的患者群体范围内更简便地进行测试和评估。', 'title_zh': '面向放射学AI的虚拟临床试验研究：基于条件生成模型的方法'}
{'arxiv_id': 'arXiv:2502.09687', 'title': 'Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models', 'authors': 'Wiktoria Mieleszczenko-Kowszewicz, Beata Bajcar, Jolanta Babiak, Berenika Dyczek, Jakub Świstak, Przemysław Biecek', 'link': 'https://arxiv.org/abs/2502.09687', 'abstract': 'Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.', 'abstract_zh': '小心你提出的问题，你可能会得到它。这句话同样适用于大型语言模型（LLMs）的训练方式，这些模型越来越多地通过取悦接收者得到奖励，而不是仅仅因为正确性而得到奖励。因此，它们越来越善于说服我们它们的答案是有价值的。但在这种说服过程中，它们使用了哪些技巧呢？在本研究中，我们分析了十二种不同语言模型的回答所使用的心理语言学特征。通过对合理提示和情感提示下的响应内容进行分类，并探索LLMs所运用的社会影响原则，我们探讨了是否以及如何减轻LLM驱动的广泛虚假信息的风险。我们将本研究置于人类中心的人工智能更广泛讨论的背景中，强调需要采用跨学科方法来缓解具有说服力的人工智能响应所带来的认知和社会风险。', 'title_zh': '小心您所提出的问题：大型语言模型在说服中的情感与理性面向'}
{'arxiv_id': 'arXiv:2502.09680', 'title': 'Object-Centric Latent Action Learning', 'authors': 'Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov, Alexander Derevyagin, Andrei Polubarov, Nikita Lyubaykin, Vladislav Kurenkov', 'link': 'https://arxiv.org/abs/2502.09680', 'abstract': 'Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by x2.7 and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by x2.6 on average.', 'abstract_zh': '利用海量互联网视频数据进行具身AI当前主要受到动作标注不足以及相关干扰动作的存在而受阻。为此，我们提出了一种新颖的对象中心潜在动作学习方法，基于VideoSaur和LAPO，该方法通过自我监督的方式将场景分解为对象表示，并使用代理动作标签标注视频数据。该方法有效解开了因果主体-对象交互与无关背景噪声的关系，并且减少了由干扰动作引起的潜在动作学习方法性能下降。初步实验结果显示，基于对象分解的潜在动作预训练可以将推理出的潜在动作质量提高2.7倍，并且在少量标记动作的情况下提高了下游微调的效率，平均收益提高2.6倍。', 'title_zh': '以对象为中心的潜伏动作学习'}
{'arxiv_id': 'arXiv:2502.09675', 'title': 'Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis', 'authors': 'Yubo Gao, Haotian Wu, Lei Zhang', 'link': 'https://arxiv.org/abs/2502.09675', 'abstract': 'Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN.', 'abstract_zh': '多模态情感分析（Multimodal Sentiment Analysis, MSA）旨在通过利用文本、声学和视觉等多种模态来识别人类情感，因此如何充分利用不同模态之间的交互是MSA的核心挑战之一。交互包含对齐和冲突两个方面。当前的研究主要强调对齐以及不同单模态之间的固有差异，忽视了两模态组合之间也可能存在的潜在冲突。此外，基于多任务学习的冲突建模方法往往依赖于不稳定的生成标签。为了解决这些挑战，我们提出了一种新型多层次冲突意识网络（Multilevel Conflict-Aware Network, MCAN），该网络逐步分离单模态和两模态表示中的对齐和冲突成分，并进一步通过冲突建模分支利用这些冲突成分。在冲突建模分支中，我们分别在表示层和预测输出层施加差异约束，从而避免依赖生成的标签。在CMU-MOSI和CMU-MOSEI数据集上的实验结果证实了所提出的MCAN的有效性。', 'title_zh': '多层冲突感知网络在多模态情感分析中的应用'}
{'arxiv_id': 'arXiv:2502.09674', 'title': 'The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis', 'authors': 'Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, Xiaohua Jia', 'link': 'https://arxiv.org/abs/2502.09674', 'abstract': "Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at this https URL.", 'abstract_zh': '大型语言模型的安全对齐行为，如拒绝有害查询，可以用激活空间中的线性方向来表示。以往的研究使用单一方向来建模安全性行为，限制了对机制理解的局限性，仅限于单一的安全特性。在本项研究中，我们发现安全性对齐行为由多维方向联合控制。具体来说，我们在对 Llama 3 8B 进行防范逃逸调优的过程中研究了表示变化的向量空间，并发现一条主导方向控制模型的拒绝行为，而多个较小的方向代表了不同的可解释特征，如假设性叙述和角色扮演。接着，我们测量了不同方向如何促进或抑制主导方向，展示了次要方向在塑造模型拒绝表示中的重要作用。最后，我们展示了在有害查询中移除某些触发词可以削弱这些方向以绕过学习到的安全能力，从而从多维视角提供了对安全对齐漏洞的新见解。相关代码和数据可在以下链接获取：[提供链接]', 'title_zh': 'LLM对齐的隐含维度：多层次安全性分析'}
{'arxiv_id': 'arXiv:2502.09673', 'title': 'Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning', 'authors': 'Ang Li, Yichuan Mo, Mingjie Li, Yifei Wang, Yisen Wang', 'link': 'https://arxiv.org/abs/2502.09673', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.', 'abstract_zh': '大规模语言模型（LLMs）已经在多种自然语言处理（NLP）基准测试中展现了显著的成功。然而，在执行需要细致推理和精确决策的复杂任务时，仅仅具备纯粹的语言能力是不够的—LLMs 必须能够进行推理，即逻辑思考、借鉴过往经验，并综合信息以得出结论并采取行动。为了增强推理能力，已经广泛探索了诸如提示和微调等方法。虽然这些方法在提升推理方面取得了明确的进步，但它们对LLMs安全的影响仍然不够明确。在本研究中，我们探讨了推理与安全之间的交互作用。我们指出了随着推理能力的提高而出现的潜在安全风险，揭示了此前未被注意到的脆弱性。同时，我们还探讨了如何利用推理本身来增强安全，揭示了潜在的缓解策略。通过研究推理驱动的LLMs安全中的风险与机遇，我们的研究为开发既更为强大又更为可靠的模型在实际部署中提供了宝贵的见解。', 'title_zh': '更聪明的大型语言模型更安全吗？探索提示和微调中的安全推理权衡'}
{'arxiv_id': 'arXiv:2502.09670', 'title': 'The Science of Evaluating Foundation Models', 'authors': 'Jiayi Yuan, Jiamu Zhang, Andrew Wen, Xia Hu', 'link': 'https://arxiv.org/abs/2502.09670', 'abstract': 'The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.', 'abstract_zh': '大型基础模型中出现的现象已经革命性地改变了自然语言处理。然而，由于模型的规模、功能及其在多样应用中的部署，对其进行评估面临着重大挑战。现有文献通常集中在个别方面，如基准性能或特定任务上，但未能提供一个能够整合多样化应用场景及其更广泛伦理与操作考量的整体评估过程。本研究聚焦于三个关键方面：(1) 规范化评估流程，通过提供适应特定应用场景上下文的结构化框架，(2) 提供实用工具和框架，如检查列表和模板，确保评估过程全面、可重复且实用，以及(3) 回顾近期工作，对大型语言模型评估的进展进行有针对性的综述，强调其实用应用。', 'title_zh': '评价基础模型的科学方法'}
{'arxiv_id': 'arXiv:2502.09669', 'title': 'Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation', 'authors': 'Maizhe Yang, Kaiyuan Tang, Chaoli Wang', 'link': 'https://arxiv.org/abs/2502.09669', 'abstract': 'Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at this https URL.', 'abstract_zh': '隐式神经表示（INR）已成为编码体数据的一种有前途的解决方案，提供了连续表示，并且与体渲染管线无缝兼容。然而，针对每个新体积从随机初始化参数优化一个INR网络在计算上是低效的，尤其是在大型时空变化或集成体数据集中更为明显，这些数据集中的体数据共享相似的结构模式但需要独立训练。为了弥合这一差距，我们提出了一种名为Meta-INR的预训练策略，该策略借鉴元学习算法，通过部分观察体积数据集来学习初始的INR参数。与从头训练INR相比，学到的初始参数提供了一个强大的先验知识，增强了INR的泛化能力，使得在适应新体积时只需要少量梯度更新即可实现快速收敛，并且在分析适应后的INR参数时更具可解释性。我们展示了Meta-INR能够有效地提取高质量的泛化特征，帮助编码跨不同数据集的未见过的相似体积数据。此外，我们还强调了其在模拟参数分析和代表性时间步长选择等任务中的应用价值。该代码可在以下链接获取：this https URL。', 'title_zh': 'Meta-INR：通过元学习隐式神经表示高效编码体数据'}
{'arxiv_id': 'arXiv:2502.09663', 'title': 'DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations', 'authors': 'Anis Bourou, Saranga Kingkor Mahanta, Thomas Boyer, Valérie Mezger, Auguste Genovesio', 'link': 'https://arxiv.org/abs/2502.09663', 'abstract': 'In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.', 'abstract_zh': '近年来，深度学习模型已在多种生物数据模态中得到广泛应用。具有区分能力的深度学习模型在将图像分类到不同类别（如健康与疾病、治疗与未治疗）方面表现出色。然而，由于这些模型的复杂性和缺乏可解释性，它们往往被视为黑箱模型，限制了其在真实世界生物研究中的应用。在生物研究中，模型解释性至关重要：理解分类器的决策逻辑和识别不同条件下细微差异对于阐明治疗效果、疾病进展和生物过程具有重要意义。为解决这一挑战，我们提出了一种名为DiffEx的方法，该方法用于生成可视觉解释的属性以解释分类器，并识别微小细胞变化之间的差异。我们展示了DiffEx在解释自然和生物图像训练的分类器方面的有效性。此外，我们利用DiffEx揭示了显微镜数据集中的表型差异。通过提供对细胞变化的分类器解释性洞察，DiffEx有望推动疾病理解，辅助药物发现，识别新的生物标志物。', 'title_zh': 'DiffEx：使用扩散模型解释分类器以识别显微细胞变异'}
{'arxiv_id': 'arXiv:2502.09659', 'title': 'Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models', 'authors': 'Hasin Rehana, Jie Zheng, Leo Yeh, Benu Bansal, Nur Bengisu Çam, Christianah Jemiyo, Brett McGregor, Arzucan Özgür, Yongqun He, Junguk Hur', 'link': 'https://arxiv.org/abs/2502.09659', 'abstract': "Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.", 'abstract_zh': '动机：佐剂是一种纳入疫苗中的化学物质，通过改善免疫反应来提高疫苗的有效性。识别癌症疫苗研究中的佐剂名称对于进一步的研究和增强免疫治疗至关重要。然而，从不断扩增的生物医学文献中进行手动标注带来了巨大的挑战。本研究探讨了使用大规模语言模型（LLMs），特别是生成预训练变换器（GPT）和大型语言模型元智能（Llama）自动识别疫苗佐剂名称的方法。\n\n方法：我们使用了两个数据集：来自AdjuvareDB的97篇临床试验记录和290篇由疫苗佐剂综览（VAC）标注的摘要。GPT-4o和Llama 3.2分别在零样本和少量样本学习范式下使用，每个提示最多四个示例。提示明确针对佐剂名称，测试了物质或干预措施等上下文信息的影响。输出结果经过自动化和人工验证，以确保准确性和一致性。\n\n结果：GPT-4o在所有情况下实现了100%的精确度，同时在召回率和F1分数方面表现出显著提高，尤其是在引入干预措施时。在VAC数据集上，GPT-4o在引入干预措施时的F1分数达到了77.32%，超过了Llama-3.2-3B约2%。在AdjuvareDB数据集上，GPT-4o通过引入干预措施，实现了三样本提示下的F1分数为81.67%，超过了Llama-3.2-3B的最大F1分数65.62%。\n\n结论：我们的研究结果表明，LLMs在识别佐剂名称方面表现出色，包括识别稀有命名形式。本研究强调了LLMs在通过高效提取见解来增强癌症疫苗开发方面的潜力。未来的工作将致力于扩展框架，涵盖各种生物医学文献，并增强模型在不同疫苗和佐剂方面的泛化能力。', 'title_zh': '使用大型语言模型从生物医学文献中识别癌症疫苗辅助剂名称'}
{'arxiv_id': 'arXiv:2502.09658', 'title': 'Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality', 'authors': 'Xin Kang, Veronika Shteingardt, Yuhan Wang, Dov Dori', 'link': 'https://arxiv.org/abs/2502.09658', 'abstract': 'Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.', 'abstract_zh': '知识表示与推理是人工智能（AI）中的关键挑战，尤其是在将神经网络方法与符号方法相结合以实现可解释和透明的AI系统时更为重要。传统知识表示方法往往难以捕捉复杂的处理过程和状态变化。本文提出了神经概念型人工智能（NCAI），这是一种结合了面向过程的对象模型（Object-Process Methodology, OPM）ISO 19450:2024与深度学习的神经符号AI方法，旨在提高问答（QA）质量。通过使用情境学习将自然语言文本转换为OPM模型，NCAI能够利用OPM的高度表达能力表示传统三元组知识图谱难以捕获的复杂OPM元素，如过程、对象和状态。这种丰富的结构化知识表示在OPM-QA系统中提高了推理透明度和答案准确性。我们进一步提出了透明度评估指标，以定量衡量预测推理与基于OPM的概念逻辑的一致性。实验结果表明，NCAI优于传统方法，显示出其在推进神经符号AI方面的潜力，通过提供丰富知识表示、可度量的透明度和改进的推理能力。', 'title_zh': '神经概念人工智能：将OPM与深度学习相结合以提高问答质量'}
{'arxiv_id': 'arXiv:2502.09655', 'title': 'Bidirectional Diffusion Bridge Models', 'authors': 'Duc Kieu, Kien Do, Toan Nguyen, Dang Nguyen, Thin Nguyen', 'link': 'https://arxiv.org/abs/2502.09655', 'abstract': "Diffusion bridges have shown potential in paired image-to-image (I2I) translation tasks. However, existing methods are limited by their unidirectional nature, requiring separate models for forward and reverse translations. This not only doubles the computational cost but also restricts their practicality. In this work, we introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable approach that facilitates bidirectional translation between two coupled distributions using a single network. BDBM leverages the Chapman-Kolmogorov Equation for bridges, enabling it to model data distribution shifts across timesteps in both forward and backward directions by exploiting the interchangeability of the initial and target timesteps within this framework. Notably, when the marginal distribution given endpoints is Gaussian, BDBM's transition kernels in both directions possess analytical forms, allowing for efficient learning with a single network. We demonstrate the connection between BDBM and existing bridge methods, such as Doob's h-transform and variational approaches, and highlight its advantages. Extensive experiments on high-resolution I2I translation tasks demonstrate that BDBM not only enables bidirectional translation with minimal additional cost but also outperforms state-of-the-art bridge models. Our source code is available at [this https URL||this https URL].", 'abstract_zh': '扩散桥梁在配对的图像到图像（I2I）翻译任务中显示出潜在的应用价值。然而，现有的方法受限于其单向性质，需要分别建立正向和反向翻译的模型，这不仅增加了计算成本，还限制了它们的实际应用。在这项工作中，我们提出了双向扩散桥梁模型（BDBM），这是一种可扩展的方法，能够使用单一网络实现两个耦合分布之间的双向翻译。BDBM 利用Chapman-Kolmogorov方程来构建桥梁，使其能够利用初态和目标态之间的可互换性，在正向和反向两个方向上建模时间步长的数据分布转换。特别地，当给定端点的边际分布为高斯分布时，BDBM 的双向转换核具有解析形式，从而允许使用单一网络进行高效的学习。我们展示了BDBM与现有的桥梁方法，如Doob的h变换和变分方法之间的联系，并突显了它的优势。在高分辨率I2I翻译任务的广泛实验中，我们证明BDBM不仅能够以最小的额外成本实现双向翻译，还能超越最先进的桥梁模型。我们的源代码可在[这个链接1]和[这个链接2]中获取。', 'title_zh': '双向扩散桥模型'}
{'arxiv_id': 'arXiv:2502.09650', 'title': 'Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples', 'authors': 'Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu', 'link': 'https://arxiv.org/abs/2502.09650', 'abstract': "The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）的对齐往往假设使用更多干净的数据可以获得更好的结果，而忽略了模型容量与示例难度之间的匹配。我们提出一个新的原则：偏好数据的难度各不相同，过于复杂的示例会妨碍对齐，因为它们超出了模型的容量。通过系统的实验，我们验证了这一原则，得到了三个关键发现：（1）偏好示例的难度是不同的，这体现在对齐试验中一致的学习顺序；（2）过于复杂的示例在四个LLMs和两个数据集上显著降低了性能；（3）模型的容量决定了其处理困难示例的阈值，突显了数据选择与模型容量之间的重要关系。基于这一原则，我们引入了选择性DPO，该方法过滤掉过于复杂的示例。这一简单的调整在AlpacaEval 2基准测试中将胜率提高了9-16%，超过了多种具有不同算法调整的DPO变体。总之，这些结果揭示了数据难度与模型容量之间对齐的重要性，并为改进LLMs中的对齐策略提供了转变性的视角。代码可在以下链接获取：this https URL。', 'title_zh': '遵循原则的数据选择以实现对齐：困难例证的潜在风险'}
{'arxiv_id': 'arXiv:2502.09648', 'title': 'UKTA: Unified Korean Text Analyzer', 'authors': 'Seokho Ahn, Junhyung Park, Ganghee Go, Chulhui Kim, Jiho Jung, Myung Sun Shin, Do-Guk Kim, Young-Duk Seo', 'link': 'https://arxiv.org/abs/2502.09648', 'abstract': 'Evaluating writing quality is complex and time-consuming often delaying feedback to learners. While automated writing evaluation tools are effective for English, Korean automated writing evaluation tools face challenges due to their inability to address multi-view analysis, error propagation, and evaluation explainability. To overcome these challenges, we introduce UKTA (Unified Korean Text Analyzer), a comprehensive Korea text analysis and writing evaluation system. UKTA provides accurate low-level morpheme analysis, key lexical features for mid-level explainability, and transparent high-level rubric-based writing scores. Our approach enhances accuracy and quadratic weighted kappa over existing baseline, positioning UKTA as a leading multi-perspective tool for Korean text analysis and writing evaluation.', 'abstract_zh': '评估写作质量是一项复杂且耗时的工作，常会延迟对学习者的反馈。虽然自动化写作评估工具对英语写作的有效性已被证实，但韩语自动化写作评估工具却面临挑战，主要是因为它们无法应对多视角分析、错误传播和评估解释性的问题。为了克服这些挑战，我们引入了UKTA（统一韩语文本分析器），这是一种全面的韩语文本分析和写作评估系统。UKTA提供了准确的低级词素分析、中级的关键词汇特征以提高解释性，以及透明的高级基于评分标准的写作评分。我们的方法在准确性及 quadrant 权重 κ 上优于现有基线，将UKTA定位为韩语文本分析和写作评估的领先多视角工具。', 'title_zh': 'UKTA：统一韩文文本分析器'}
{'arxiv_id': 'arXiv:2502.09645', 'title': 'From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models', 'authors': 'Mayank Vatsa, Aparna Bharati, Surbhi Mittal, Richa Singh', 'link': 'https://arxiv.org/abs/2502.09645', 'abstract': 'Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.', 'abstract_zh': '否定，作为一种表达缺失、否认或矛盾的语言构建，为多语言多模态基础模型带来了显著挑战。这些模型在机器翻译、文本引导生成、图像字幕、音频交互和视频处理等任务中表现出色，但在跨多种语言和文化背景准确解释否定方面经常遇到困难。在本文中，我们提出了一种全面的否定构建分类，展示了结构、语义和文化因素如何影响多模态基础模型。我们提出了开放的研究问题，并突出了关键挑战，强调解决这些问题以实现稳健的否定处理的重要性。最后，我们倡导专门的基准测试、语言特定的分词、细粒度的注意力机制以及先进的多模态架构。这些策略可以促进更具适应性和语义精确性的多模态基础模型，使其更好地应对和准确解释多语言多模态环境中否定的复杂性。', 'title_zh': '从不知到知：多模态基础模型中否定理解的分类、挑战与机遇'}
{'arxiv_id': 'arXiv:2502.09644', 'title': 'From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis', 'authors': 'Moritz Plenz, Philipp Heinisch, Janosch Gehring, Philipp Cimiano, Anette Frank', 'link': 'https://arxiv.org/abs/2502.09644', 'abstract': "Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer's stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.", 'abstract_zh': '在解决冲突的过程中，就对立问题进行辩论是必要的第一步。然而，说理技巧难以克服论者的内在视角。从辩论过渡到一个决策过程，我们需要识别出解决冲突的可行方案，这需要对论证及其所基于的视角进行更深入的分析——因为只有在这些基础上，才能推导出各方均可接受的解决方案。本文构建了一个计算论辩环境下的决策分析框架。我们对特定问题上不同论者或利益相关者的论辩中的视角化立场进行了精细分析，不仅识别出他们的对立观点，还识别出基于其态度、价值观或需求产生的共享视角。我们将这种分析形式化为“视角化立场向量”，该向量描述了所有论者在某个问题上各自视角化的立场。我们通过确定与问题和论证相关的特定概念来构建这些向量，并预测论者相对于每一个概念的立场。这些向量使我们能够衡量由视角造成的论者之间模态（不）一致的观点，这有助于识别可采取的冲突解决方案，从而作为决策过程的第一步。', 'title_zh': '从论辩到审议：视角化的立场向量在细粒度（不）同意分析中的应用'}
{'arxiv_id': 'arXiv:2502.09642', 'title': 'Krutrim LLM: Multilingual Foundational Model for over a Billion People', 'authors': 'Aditya Kallappa, Palash Kamble, Abhinav Ravi, Akshat Patidar, Vinayak Dhruv, Deepak Kumar, Raghav Awasthi, Arveti Manjunath, Shubham Agarwal, Kumar Ashish, Gautam Bhargava, Chandra Khatri', 'link': 'https://arxiv.org/abs/2502.09642', 'abstract': "India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data.\nWe introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts.\nKrutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models.", 'abstract_zh': '印度是一个多元化的社会，开发人工智能系统面临着独特的挑战，包括语言多样性、口头传统、数据获取和可扩展性问题。现有的基础模型主要是在英语上进行训练，这限制了它们在印度人口中的有效性。印度语系语言仅占公共爬取语料库（Common Crawl）的1%，而印度的人口占全球的18%，这导致了语言偏见。成千上万的地区语言、方言和代码混合增加了代表性的挑战，尤其是在训练数据稀少的情况下。\n\n我们提出了克瑞特里姆（Krutrim）大语言模型，这是一个针对印度语言环境设计的2兆字节多层次语言模型。它整合了已知最大的印度语系数据集，缓解了数据稀缺问题，并确保在方言上的均衡性能。克瑞特里姆在印度基准测试中表现出色，同时保持了与英语言优良的竞争力。尽管训练计算量显著较小，但克瑞特里姆大语言模型在16项任务中有10项达到了或超过了类似于LLAMA-2的模型表现，其平均得分为0.57对0.55。这表明了克瑞特里姆在各种语言背景下表现出的灵活多语言流畅性。\n\n克瑞特里姆还集成了实时搜索功能，以提高对话型人工智能应用的事实准确性。这提升了超过10亿用户的全球可访问性。通过针对数据不平衡的意向性设计选择，克瑞特里姆大语言模型标志着在构建伦理性和全球代表性的人工智能模型方面取得的实际进展。', 'title_zh': 'Krutrim LLM：为超过十亿人群构建的多语言基础模型'}
{'arxiv_id': 'arXiv:2502.09640', 'title': 'Online Social Support Detection in Spanish Social Media Texts', 'authors': 'Moein Shahiki Tash, Luis Ramos, Zahra Ahani, Raul Monroy, Olga kolesnikova, Hiram Calvo, Grigori Sidorov', 'link': 'https://arxiv.org/abs/2502.09640', 'abstract': 'The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.', 'abstract_zh': '社交媒体的兴起已经改变了沟通方式，使个人能够分享他们的经历、寻求支持，并参与多样化的讨论。尽管大量研究集中在识别有害内容（如仇恨言论）上，但正面和支持性互动的识别与推广却鲜有研究。本研究提出了一种创新方法，用于检测西班牙语社交媒体文本中的在线社会支持。我们首次创建了专门用于此任务的标注数据集，该数据集包含了3,189条标注为支持或不支持的YouTube评论。为了解决数据不平衡的问题，我们使用GPT-4o生成了改写后的评论，并创建了一个平衡的数据集。然后，我们使用传统的机器学习模型、深度学习架构以及基于变压器的模型（包括GPT-4o）对不平衡数据集进行了社会支持分类评估。随后，我们使用一个变压器模型比较了平衡数据集和不平衡数据集之间的性能。我们的研究结果表明，平衡数据集在任务2（个体与群体）和任务3（国家、其他、LGBTQ社群、黑人群体、女性、宗教）中表现更好，而GPT-4o在任务1（社会支持与非支持）中表现最佳。本研究突显了培养一个支持性的在线环境的重要性，并为未来自动识别社会支持的研究奠定了基础。', 'title_zh': '西班牙社交媒体文本中的在线社会支持检测'}
{'arxiv_id': 'arXiv:2502.09638', 'title': 'Jailbreaking to Jailbreak', 'authors': 'Jeremy Kritz, Vaughn Robinson, Robert Vacareanu, Bijan Varjavand, Michael Choi, Bobby Gogov, Scale Red Team, Summer Yue, Willow E. Primack, Zifan Wang', 'link': 'https://arxiv.org/abs/2502.09638', 'abstract': 'Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with $J_2$, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.', 'abstract_zh': '大型语言模型（LLMs）的拒绝训练能够防止有害输出，但这种防御仍然容易受到自动化的和人工构建的逃逸攻击的影响。我们提出了一种新颖的LLM作为红队成员的方法，在这种方法中，一名人类通过逃逸攻击来破坏一个拒绝训练的LLM，使其愿意自我逃逸或逃逸其他LLM。我们将这些被逃逸的LLM称为$J_2$攻击者，它们可以使用各种红队策略系统地评估目标模型，并通过从先前失败中进行上下文学习来提高自己的性能。我们的实验表明，Sonnet 3.5和Gemini 1.5在$J_2$攻击者中表现最佳，分别以93.0%和91.0%的攻击成功率（ASR）对抗GPT-4o（以及其他具有类似表现的LLM）在Harmbench上取得优异成绩。我们的工作不仅引入了一种可扩展的红队策略方法，借鉴了人类红队成员的做法，还强调了保护措施中的一个未被重视的失败模式——逃逸攻击者再逃逸。具体来说，一个LLM可以通过使用愿意协助进一步逃逸的逃逸版本自身来绕过自己的保护措施。为了防止$J_2$的直接滥用，同时推动AI安全研究的发展，我们公开分享了我们的方法论，而保留了具体的提示细节。', 'title_zh': '“由越狱到越狱”'}
{'arxiv_id': 'arXiv:2502.09637', 'title': 'Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness', 'authors': 'Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.09637', 'abstract': 'Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, "culture" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess "cultural awareness", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.', 'abstract_zh': '近年来，许多研究显示大型语言模型（LLMs）倾向于体现西方和盎格鲁中心的世界观，这使它们在非西方文化环境中发挥作用时失去了效用。然而，“文化”是一个复杂且多维度的概念，其在LLMs及其基于LLM的应用中的意识、表现和建模可以有多种定义和衡量方式。在本文中，我们探讨一个LLM具备“文化意识”意味着什么，并通过扩展Bender和Koller（2020）提出的章鱼测试的方式展开一个思想实验，我们认为，让LLM及其基于LLM的人工智能系统在各种文化中，包括完全没有接触过的新文化中发挥作用所必需的是元文化能力，而不仅仅是文化意识或知识。我们阐述了元文化能力人工智能系统的原则，并讨论了如何衡量和建模这些能力。', 'title_zh': '元文化素养：文化意识的正确路径'}
{'arxiv_id': 'arXiv:2502.09636', 'title': 'Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?', 'authors': 'Sougata Saha, Saurabh Kumar Pandey, Harshit Gupta, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.09636', 'abstract': 'In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: this https URL', 'abstract_zh': '在快速全球化和数字化的世界中，来自不同文化的人们创作的书籍和产品评论被世界各地的用户阅读和消费。本文研究了由于存在文化特定的物品和元素而导致书籍评论理解性差距的程度和模式。这些文化特定的物品和元素可能对来自其他文化用户的理解构成障碍。我们对来自Goodreads的57篇书籍评论进行的用户研究结果显示，83%的评论中至少包含一个难以理解的文化特定元素。我们还评估了GPT-4o在考虑读者文化背景的情况下识别这些元素的效能；结果参差不齐，表明有显著的改进空间。我们的数据集可在此处获取：[这个链接](这个链接应该是一个具体的URL)。', 'title_zh': '在字里行间探寻差异：大型语言模型能否识别跨文化沟通缺口？'}
{'arxiv_id': 'arXiv:2502.09635', 'title': 'CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking', 'authors': 'Delvin Ce Zhang, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.09635', 'abstract': 'Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.', 'abstract_zh': '通常，验证陈述的真实性需要对多个证据句进行推理。然而，证据句往往不是完全自包含的，可能需要来自其他部分的额外上下文和引用才能理解同指表达、缩写词以及报告发现的范围。例如，一篇学术论文中的证据句可能需要论文中的上下文句和被引用论文中的描述来确定研究发现的范围。然而，大多数事实核查模型主要关注证据句内的推理，而忽略了辅助的上下文和引用。为了解决这个问题，我们提出了一种新的方法——上下文和引用增强的推理与提示方法。在证据推理方面，我们构建了一个三层证据图，包括证据层、上下文层和引用层。我们设计了跨层和层内推理，将三个图层整合为统一的证据嵌入。在证言预测方面，我们设计了条件证据提示编码器，为每项声明生成独特的提示嵌入。这些条件证据提示嵌入和声明被统一用于事实核查。实验验证了我们模型的优越性。', 'title_zh': '正确的翻译应符合学术规范，保持专业性和准确性。以下是翻译：\n\n增强语境和参考推理与提示设计以用于事实核查'}
