# Evaluating Conversational Recommender Systems with Large Language Models: A User-Centric Evaluation Framework 

**Title (ZH)**: 用大型语言模型评估对话推荐系统：一种以用户为中心的评估框架 

**Authors**: Nuo Chen, Quanyu Dai, Xiaoyu Dong, Xiao-Ming Wu, Zhenhua Dong  

**Link**: [PDF](https://arxiv.org/pdf/2501.09493)  

**Abstract**: Conversational recommender systems (CRS) involve both recommendation and dialogue tasks, which makes their evaluation a unique challenge. Although past research has analyzed various factors that may affect user satisfaction with CRS interactions from the perspective of user studies, few evaluation metrics for CRS have been proposed. Recent studies have shown that LLMs can align with human preferences, and several LLM-based text quality evaluation measures have been introduced. However, the application of LLMs in CRS evaluation remains relatively limited. To address this research gap and advance the development of user-centric conversational recommender systems, this study proposes an automated LLM-based CRS evaluation framework, building upon existing research in human-computer interaction and psychology. The framework evaluates CRS from four dimensions: dialogue behavior, language expression, recommendation items, and response content. We use this framework to evaluate four different conversational recommender systems. 

**Abstract (ZH)**: 会话推荐系统（CRS）既涉及推荐任务，也涉及对话任务，这使得对其的评估成为一项独特的挑战。尽管过往的研究从用户研究的角度分析了可能影响用户对CRS交互满意度的各种因素，但针对CRS的评价指标却很少提出。最近的研究表明，大型语言模型（LLMs）能够与人类偏好对齐，并且已经引入了多种基于LLM的文本质量评估指标。然而，LLM在CRS评估中的应用仍然相对有限。为了解决这一研究缺口并促进以用户为中心的会话推荐系统的开发，本研究提出了一种基于LLM的自动化CRS评估框架，该框架借鉴了人机交互和心理学领域的现有研究成果。该框架从四个维度评估CRS：对话行为、语言表达、推荐项目和响应内容。我们使用该框架评估了四种不同的会话推荐系统。 

---
# A Multi-tiered Solution for Personalized Baggage Item Recommendations using FastText and Association Rule Mining 

**Title (ZH)**: 一种基于FastText和关联规则挖掘的多层次个性化行李物品推荐解决方案 

**Authors**: Mudavath Ravi, Atul Negi  

**Link**: [PDF](https://arxiv.org/pdf/2501.09359)  

**Abstract**: This paper introduces an intelligent baggage item recommendation system to optimize packing for air travelers by providing tailored suggestions based on specific travel needs and destinations. Using FastText word embeddings and Association Rule Mining (ARM), the system ensures efficient luggage space utilization, compliance with weight limits, and an enhanced travel experience. The methodology comprises four phases: (1) data collection and preprocessing with pre-trained FastText embeddings for text representation and similarity scoring (2) a content-based recommendation system enriched by user search history (3) application of ARM to user interactions to uncover meaningful item associations and (4) integration of FastText and ARM for accurate, personalized recommendations. Performance is evaluated using metrics such as coverage, support, confidence, lift, leverage, and conviction. Results demonstrate the system's effectiveness in providing relevant suggestions, improving customer satisfaction, and simplifying the packing process. These insights advance personalized recommendations, targeted marketing, and product optimization in air travel and beyond. 

**Abstract (ZH)**: 本文介绍了一种智能行李推荐系统，旨在通过提供基于特定旅行需求和目的地的定制建议来优化航空旅行者的行李打包。该系统利用FastText词嵌入和关联规则挖掘（ARM）确保有效利用行李空间、符合重量限制，并提升旅行体验。该方法论包括四个阶段：（1）数据收集和预处理，使用预训练的FastText嵌入进行文本表示和相似度评分；（2）通过融合用户搜索历史的内容基于推荐系统；（3）利用ARM分析用户互动以揭示有意义的物品关联；（4）结合FastText和ARM以提供准确的个性化建议。性能通过覆盖范围、支持度、置信度、提升度、杠杆率和信念度等指标进行评估。结果表明，该系统能够提供相关的建议，提高客户满意度，并简化打包过程。这些见解促进了航空旅行及其他领域的个性化推荐、定向营销和产品优化。 

---
# Style4Rec: Enhancing Transformer-based E-commerce Recommendation Systems with Style and Shopping Cart Information 

**Title (ZH)**: Style4Rec：通过风格和购物车信息增强基于Transformer的电商推荐系统 

**Authors**: Berke Ugurlu, Ming-Yi Hong, Che Lin  

**Link**: [PDF](https://arxiv.org/pdf/2501.09354)  

**Abstract**: Understanding users' product preferences is essential to the efficacy of a recommendation system. Precision marketing leverages users' historical data to discern these preferences and recommends products that align with them. However, recent browsing and purchase records might better reflect current purchasing inclinations. Transformer-based recommendation systems have made strides in sequential recommendation tasks, but they often fall short in utilizing product image style information and shopping cart data effectively. In light of this, we propose Style4Rec, a transformer-based e-commerce recommendation system that harnesses style and shopping cart information to enhance existing transformer-based sequential product recommendation systems. Style4Rec represents a significant step forward in personalized e-commerce recommendations, outperforming benchmarks across various evaluation metrics. Style4Rec resulted in notable improvements: HR@5 increased from 0.681 to 0.735, NDCG@5 increased from 0.594 to 0.674, and MRR@5 increased from 0.559 to 0.654. We tested our model using an e-commerce dataset from our partnering company and found that it exceeded established transformer-based sequential recommendation benchmarks across various evaluation metrics. Thus, Style4Rec presents a significant step forward in personalized e-commerce recommendation systems. 

**Abstract (ZH)**: 理解用户的产品偏好对于推荐系统的有效性至关重要。精准营销通过利用用户的 histórico 数据来识别这些偏好，并推荐与其相匹配的产品。然而，近期的浏览和购买记录可能更好地反映出当前的购买倾向。基于Transformer的推荐系统在序列推荐任务中取得了显著进展，但在有效利用产品图像风格信息和购物车数据方面往往表现不佳。鉴于此，我们提出了一种基于Transformer的电子商务推荐系统Style4Rec，该系统利用了风格和购物车信息以增强现有的基于Transformer的序列产品推荐系统。Style4Rec 在个性化电子商务推荐方面迈出了重要一步，其在多个评估指标中超过了基准。Style4Rec 实现了显著的改进：在HR@5上的提升从0.681到0.735，NDCG@5提升从0.594到0.674，MRR@5提升从0.559到0.654。我们使用合作伙伴公司提供的电子商务数据集测试了我们的模型，并发现在多个评估指标中它超过了现有的基于Transformer的序列推荐基准。因此，Style4Rec 在个性化电子商务推荐系统方面迈出了重要一步。 

---
# Guiding Retrieval using LLM-based Listwise Rankers 

**Title (ZH)**: 使用基于大规模语言模型的列表级排名器指导检索 

**Authors**: Mandeep Rathee, Sean MacAvaney, Avishek Anand  

**Link**: [PDF](https://arxiv.org/pdf/2501.09186)  

**Abstract**: Large Language Models (LLMs) have shown strong promise as rerankers, especially in ``listwise'' settings where an LLM is prompted to rerank several search results at once. However, this ``cascading'' retrieve-and-rerank approach is limited by the bounded recall problem: relevant documents not retrieved initially are permanently excluded from the final ranking. Adaptive retrieval techniques address this problem, but do not work with listwise rerankers because they assume a document's score is computed independently from other documents. In this paper, we propose an adaptation of an existing adaptive retrieval method that supports the listwise setting and helps guide the retrieval process itself (thereby overcoming the bounded recall problem for LLM rerankers). Specifically, our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point. Through extensive experiments across diverse LLM rerankers, first stage retrievers, and feedback sources, we demonstrate that our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all while keeping the total number of LLM inferences constant and overheads due to the adaptive process minimal. The work opens the door to leveraging LLM-based search in settings where the initial pool of results is limited, e.g., by legacy systems, or by the cost of deploying a semantic first-stage. 

**Abstract (ZH)**: 大型语言模型（LLMs）在重新排列表现方面显示出强烈的潜力，特别是在“列表式”设置中，即LLM被提示一次性重新排列多个搜索结果。然而，这种“级联”检索和重新排列的方法受到 bounded recall 问题的限制：未能在初始检索中获取的相关文档将永久地被排除在最终排名之外。自适应检索技术可以解决这一问题，但它们不能适用于列表式重新排列器，因为它们假设文档的评分是独立于其他文档进行计算的。在本文中，我们提出了一种对现有自适应检索方法的改编，该方法支持列表式设置，并有助于引导检索过程本身（从而克服了LLM重新排列器的 bounded recall 问题）。具体而言，我们提出的算法将初始排序结果和迄今为止最相关文档提供的反馈文档结果进行合并。通过在多种LLM重新排列器、第一阶段检索器和反馈源上进行广泛的实验，我们证明了我们的方法可以提高nDCG@10多达13.23%，召回率提升28.02%，同时保持LLM推理次数不变，并将自适应过程的开销保持在最低水平。这项工作为在初始结果池有限的环境中利用基于LLM的搜索（例如，由于遗留系统或部署语义第一阶段的成本限制）打开了大门。 

---
# Enhancing Lexicon-Based Text Embeddings with Large Language Models 

**Title (ZH)**: 基于词典的文本嵌入与大规模语言模型的增强 

**Authors**: Yibin Lei, Tao Shen, Yu Cao, Andrew Yates  

**Link**: [PDF](https://arxiv.org/pdf/2501.09749)  

**Abstract**: Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR). 

**Abstract (ZH)**: 近期的大规模语言模型（LLMs）在通用文本嵌入任务中展现了卓越的表现。尽管密集嵌入在相关研究中占主导地位，我们引入了首个基于词典的嵌入（LENS），该模型通过利用LLMs在这些任务中也取得了竞争性的表现。针对传统因果LLMs固有的标记化冗余问题和单向注意力限制，LENS通过对标记嵌入进行聚类来合并词汇空间，并探索双向注意力和各种池化策略。具体来说，LENS通过将每个维度分配给特定的标记簇来简化词典匹配问题，其中语义相似的标记被分组在一起，并通过双向注意力来充分发挥LLMs的潜力。广泛的实验表明，LENS在大规模文本嵌入基准（MTEB）上优于密集嵌入，提供了与密集嵌入大小相匹配的紧凑特征表示。值得注意的是，将LENS与密集嵌入结合使用，在MTEB的检索子集上达到了最先进的性能（即BEIR）。 

---
# Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning 

**Title (ZH)**: 基于渐进自我蒸馏的音频-视觉嵌入学习的度量学习 

**Authors**: Donghuo Zeng, Kazushi Ikeda  

**Link**: [PDF](https://arxiv.org/pdf/2501.09608)  

**Abstract**: Metric learning projects samples into an embedded space, where similarities and dissimilarities are quantified based on their learned representations. However, existing methods often rely on label-guided representation learning, where representations of different modalities, such as audio and visual data, are aligned based on annotated labels. This approach tends to underutilize latent complex features and potential relationships inherent in the distributions of audio and visual data that are not directly tied to the labels, resulting in suboptimal performance in audio-visual embedding learning. To address this issue, we propose a novel architecture that integrates cross-modal triplet loss with progressive self-distillation. Our method enhances representation learning by leveraging inherent distributions and dynamically refining soft audio-visual alignments -- probabilistic alignments between audio and visual data that capture the inherent relationships beyond explicit labels. Specifically, the model distills audio-visual distribution-based knowledge from annotated labels in a subset of each batch. This self-distilled knowledge is used t 

**Abstract (ZH)**: 度量学习将样本投影到嵌入空间中，在此空间中，相似性与差异性基于它们的学习表示进行量化。然而，现有的方法往往依赖于标签导向的表示学习，即根据不同模态（如音频和视觉数据）在标注标签基础上对表示进行对齐。这种方法往往会过度利用与标签直接相关但潜在更复杂特征和潜在关系未能充分利用，从而导致在音频-视觉嵌入学习中的表现不佳。为解决这一问题，我们提出了一种新的架构，该架构结合了跨模态三重损失与渐进式自我蒸馏。该方法通过利用内在分布并动态精炼软音频-视觉对齐来增强表示学习——这些对齐捕捉了超越显式标签的内在关系。具体而言，模型从每个批次的子集中的标注标签中提取基于音频-视觉分布的知识，并利用这种自我蒸馏的知识来进一步优化表示学习。 

---
# Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval 

**Title (ZH)**: 评估大型语言模型理解电子健康记录表格能力：患者数据提取与检索的全面研究 

**Authors**: Jesus Lovon, Martin Mouysset, Jo Oleiwan, Jose G. Moreno, Christine Damase-Michel, Lynda Tamine  

**Link**: [PDF](https://arxiv.org/pdf/2501.09384)  

**Abstract**: Electronic Health Record (EHR) tables pose unique challenges among which is the presence of hidden contextual dependencies between medical features with a high level of data dimensionality and sparsity. This study presents the first investigation into the abilities of LLMs to comprehend EHRs for patient data extraction and retrieval. We conduct extensive experiments using the MIMICSQL dataset to explore the impact of the prompt structure, instruction, context, and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task performance. Through quantitative and qualitative analyses, our findings show that optimal feature selection and serialization methods can enhance task performance by up to 26.79% compared to naive approaches. Similarly, in-context learning setups with relevant example selection improve data extraction performance by 5.95%. Based on our study findings, we propose guidelines that we believe would help the design of LLM-based models to support health search. 

**Abstract (ZH)**: 电子健康记录（EHR）表具有独特的挑战，其中包括医疗特征之间的隐藏上下文依赖关系，这些问题在高维度和稀疏数据的情况下尤为突出。本研究首次探讨了大规模语言模型（LLMs）在理解EHR以提取和检索患者数据方面的能力。我们使用MIMICSQL数据集进行了广泛实验，以探讨两种骨干模型Llama2和Meditron在任务性能基础上的提示结构、指令、上下文和演示等对模型性能的影响。通过对定量和定性的分析，我们的研究结果显示，最佳特征选择和序列化方法可将任务性能提高至至多26.79%，与朴素方法相比，表现提升显著。同样，与相关示例的选择相结合的在上下文学习设置能够提高数据提取性能达5.95%。根据我们的研究结果，我们提出了我们认为有助于设计基于LLM的模型以支持医疗搜索的指导原则。 

---
# To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation 

**Title (ZH)**: 是检索还是不检索？动态检索增强生成中的不确定性检测 

**Authors**: Kaustubh D. Dhole  

**Link**: [PDF](https://arxiv.org/pdf/2501.09292)  

**Abstract**: Retrieval-Augmented Generation equips large language models with the capability to retrieve external knowledge, thereby mitigating hallucinations by incorporating information beyond the model's intrinsic abilities. However, most prior works have focused on invoking retrieval deterministically, which makes it unsuitable for tasks such as long-form question answering. Instead, dynamically performing retrieval by invoking it only when the underlying LLM lacks the required knowledge can be more efficient. In this context, we delve deeper into the question, "To Retrieve or Not to Retrieve?" by exploring multiple uncertainty detection methods. We evaluate these methods for the task of long-form question answering, employing dynamic retrieval, and present our comparisons. Our findings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy. 

**Abstract (ZH)**: 检索增强生成使大型语言模型具备检索外部知识的能力，从而通过整合超出模型固有能力的信息来减轻幻觉现象。然而，以往大多数研究集中在以确定性方式触发检索，这使其不适合长篇文章问答等任务。相反，仅在底层LLM缺乏所需知识时动态触发检索可能会更有效。在这一背景下，我们通过探索多种不确定性检测方法，进一步探讨了“检索还是不检索”的问题。我们评估了这些方法在长篇文章问答任务中的表现，并结合动态检索进行评估，展示了我们的比较结果。研究结果表明，不确定性检测指标，如度矩阵嘉定系数和离心率，能够在仅轻微降低问答准确性的情况下，将检索调用次数减少近一半。 

---
# Fuzzy Integration of Data Lake Tables 

**Title (ZH)**: 数据湖表的模糊集成 

**Authors**: Aamod Khatiwada, Roee Shraga, Renée J. Miller  

**Link**: [PDF](https://arxiv.org/pdf/2501.09211)  

**Abstract**: Data integration is an important step in any data science pipeline where the objective is to unify the information available in different datasets for comprehensive analysis. Full Disjunction, which is an associative extension of the outer join operator, has been shown to be an effective operator for integrating datasets. It fully preserves and combines the available information. Existing Full Disjunction algorithms only consider the equi-join scenario where only tuples having the same value on joining columns are integrated. This, however, does not realistically represent an open data scenario, where datasets come from diverse sources with inconsistent values (e.g., synonyms, abbreviations, etc.) and with limited metadata. So, joining just on equal values severely limits the ability of Full Disjunction to fully combine datasets. Thus, in this work, we propose an extension of Full Disjunction to also account for "fuzzy" matches among tuples. We present a novel data-driven approach to enable the joining of approximate or fuzzy matches within Full Disjunction. Experimentally, we show that fuzzy Full Disjunction does not add significant time overhead over a state-of-the-art Full Disjunction implementation and also that it enhances the integration effectiveness. 

**Abstract (ZH)**: 数据集成是任何数据科学管道中的一个重要步骤，目标是将不同数据集中的信息统一起来进行全面分析。全断言（Full Disjunction），作为一种外连接操作的 associativity 延伸，已被证明是一个有效的数据集成操作符，能够完整地保留和组合可用信息。现有的全断言算法仅考虑了等值连接（equi-join）场景，其中只有在连接列上具有相同值的元组被集成。然而，在实际的开放数据场景中，数据集来自不同的来源，具有不一致的值（例如同义词、缩写等），并且拥有限定的元数据。因此，仅仅基于相等值进行连接严重限制了全断言将数据集完全结合在一起的能力。因此，在本文中，我们提出了一种扩展全断言的方法，以考虑元组之间的“模糊”匹配。我们提出了一种新颖的数据驱动方法，以在全断言中启用近似或模糊匹配的连接。实验结果表明，模糊全断言相对于最先进的全断言实现并未增加显著的时间开销，并且提高了数据集成的效果。 

---
# Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG 

**Title (ZH)**: 代理检索增强生成：代理RAG综述

这个翻译既保留了原文的专业术语，又符合学术写作的规范。其中，“Agentic Retrieval-Augmented Generation”翻译为“代理检索增强生成”，“Agentic RAG”翻译为“代理RAG”，同时将“Survey”翻译为“综述”，这样既保留了原文的意思，又符合中文的表达习惯。 

**Authors**: Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei  

**Link**: [PDF](https://arxiv.org/pdf/2501.09136)  

**Abstract**: Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management.
Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications.
This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG 

**Abstract (ZH)**: 大型语言模型（LLMs）通过实现类人文本生成和自然语言理解，革新了人工智能（AI）。然而，它们依赖于静态训练数据的局限性限制了其对动态、实时查询的响应能力，导致输出过时或不准确。检索增强生成（RAG）作为解决方案出现，通过集成实时数据检索来增强LLMs，从而提供上下文相关且最新的反应。尽管拥有这些优势，传统RAG系统仍受制于静态工作流程，缺乏进行多步推理和复杂任务管理所需的适应性。

自适应检索增强生成（Agentic RAG）超越了这些限制，将自主AI代理嵌入到RAG流水线中。这些代理通过利用自治设计模式中的反思、规划、工具使用和多代理协作，动态管理检索策略，迭代地优化上下文理解，并根据复杂任务需求适应工作流程。这种集成使得Agentic RAG系统能够在各种应用场景中提供无与伦比的灵活性、扩展性和上下文意识。

本综述全面探讨了Agentic RAG，从其基本原理和RAG范式的演变开始。它详细介绍了Agentic RAG架构的分类学，强调了其在医疗保健、金融和教育等行业中的关键应用，并考察了实际实施策略。此外，本文还探讨了扩展这些系统、确保伦理决策以及优化适用于实际应用的性能所面临的挑战，提供了实施Agentic RAG框架和工具的详细见解。 

---
# Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval 

**Title (ZH)**: 对比学习模型在医学图像-报告检索中鲁棒性基准研究 

**Authors**: Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang  

**Link**: [PDF](https://arxiv.org/pdf/2501.09134)  

**Abstract**: Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which associates medical images with their corresponding clinical reports. This study benchmarks the robustness of four state-of-the-art contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We introduce an occlusion retrieval task to evaluate model performance under varying levels of image corruption. Our findings reveal that all evaluated models are highly sensitive to out-of-distribution data, as evidenced by the proportional decrease in performance with increasing occlusion levels. While MedCLIP exhibits slightly more robustness, its overall performance remains significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a general-purpose dataset, struggles with medical image-report retrieval, highlighting the importance of domain-specific training data. The evaluation of this work suggests that more effort needs to be spent on improving the robustness of these models. By addressing these limitations, we can develop more reliable cross-domain retrieval models for medical applications. 

**Abstract (ZH)**: 医学图像和报告为深入了解患者健康提供了宝贵的信息。这些数据的异质性和复杂性阻碍了有效的分析。为解决这一问题，我们研究了跨域检索的对比学习模型，这些模型能够将医学图像与其相应的临床报告关联起来。本研究评估了四种最先进的对比学习模型：CLIP、CXR-RePaiR、MedCLIP 和 CXR-CLIP 的鲁棒性。我们引入了一个遮挡检索任务，以评估模型在不同图像污染水平下的性能。研究发现，所有评估的模型对离群数据均表现出高度敏感性，这体现在随着遮挡比例的增加，模型性能的相应下降。虽然 MedCLIP 表现出略微更好的鲁棒性，但其整体性能仍然显著落后于 CXR-CLIP 和 CXR-RePaiR。CLIP 模型因其训练数据集的一般性，在医学图像-报告检索方面表现出困难，这突显了特定领域训练数据的重要性。这项工作的评估结果表明，需要更多努力来提高这些模型的鲁棒性。通过解决这些局限性，我们可以开发出更可靠的跨域检索模型以应用于医学领域。 

---
