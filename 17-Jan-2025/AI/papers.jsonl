{'arxiv_id': 'arXiv:2501.09744', 'title': 'KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports', 'authors': 'Hajung Kim, Chanhwi Kim, Jiwoong Sohn, Tim Beck, Marek Rei, Sunkyu Kim, T Ian Simpson, Joram M Posma, Antoine Lain, Mujeen Sung, Jaewoo Kang', 'link': 'https://arxiv.org/abs/2501.09744', 'abstract': 'The objective of BioCreative8 Track 3 is to extract phenotypic key medical findings embedded within EHR texts and subsequently normalize these findings to their Human Phenotype Ontology (HPO) terms. However, the presence of diverse surface forms in phenotypic findings makes it challenging to accurately normalize them to the correct HPO terms. To address this challenge, we explored various models for named entity recognition and implemented data augmentation techniques such as synonym marginalization to enhance the normalization step. Our pipeline resulted in an exact extraction and normalization F1 score 2.6\\% higher than the mean score of all submissions received in response to the challenge. Furthermore, in terms of the normalization F1 score, our approach surpassed the average performance by 1.9\\%. These findings contribute to the advancement of automated medical data extraction and normalization techniques, showcasing potential pathways for future research and application in the biomedical domain.', 'abstract_zh': 'BioCreative VIII 第三赛道的目标是从电子健康记录（EHR）文本中提取关键的病理表型发现，并将这些发现规范到人类表型 ontology（HPO）术语中。然而，病理表型发现中的多样表面形式使得准确地将其规范到正确的 HPO 术语变得具有挑战性。为了应对这一挑战，我们探索了各种命名实体识别模型，并实现了同义词边际化等数据增强技术，以提高规范化步骤的效果。我们的管道在精确提取和规范化 F1 得分上比所有挑战响应提交的平均得分提高了 2.6%。此外，就规范化 F1 得分而言，我们的方法超越了平均水平 1.9%。这些发现促进了自动医疗数据提取和规范化技术的发展，展示了未来研究和在生物医药领域应用的潜在路径。', 'title_zh': 'KU AIGEN ICL EDI@BC8 任务3：先天畸形物理检查报告中表型命名实体识别与规范化的发展'}
{'arxiv_id': 'arXiv:2501.09707', 'title': 'The Goofus & Gallant Story Corpus for Practical Value Alignment', 'authors': 'Md Sultan Al Nahian, Tasmia Tasrin, Spencer Frazier, Mark Riedl, Brent Harrison', 'link': 'https://arxiv.org/abs/2501.09707', 'abstract': 'Values or principles are key elements of human society that influence people to behave and function according to an accepted standard set of social rules to maintain social order. As AI systems are becoming ubiquitous in human society, it is a major concern that they could violate these norms or values and potentially cause harm. Thus, to prevent intentional or unintentional harm, AI systems are expected to take actions that align with these principles. Training systems to exhibit this type of behavior is difficult and often requires a specialized dataset. This work presents a multi-modal dataset illustrating normative and non-normative behavior in real-life situations described through natural language and artistic images. This training set contains curated sets of images that are designed to teach young children about social principles. We argue that this is an ideal dataset to use for training socially normative agents given this fact.', 'abstract_zh': '价值观或原则是人类社会的关键要素，它们影响人们按照被广泛接受的社会规则行事，从而维持社会秩序。随着人工智能系统在人类社会中的普遍应用，人们越来越担心这些系统可能会违反这些规范或价值观，并可能导致潜在的危害。因此，为了防止有意或无意的危害，期望这些系统采取符合这些原则的行为。训练系统表现出这种行为是具有挑战性的，通常需要专门的数据集。本工作提出一个多模态数据集，该数据集通过自然语言和艺术图像描述了现实生活中的规范和非规范行为。该训练集包含一系列经过精心挑选的图像，旨在教育年幼的孩子了解社会原则。我们认为，鉴于这一点，这是一个理想的用于训练社会规范代理的数据集。', 'title_zh': '《用于实践价值对齐的Goofus & Gallant故事情节语料库》'}
{'arxiv_id': 'arXiv:2501.09686', 'title': 'Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models', 'authors': 'Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li', 'link': 'https://arxiv.org/abs/2501.09686', 'abstract': 'Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of "thought" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs\' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs\' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to "think" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI\'s o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.', 'abstract_zh': '语言长期以来一直被认为是人类推理的关键工具。大型语言模型（LLMs）的突破引发了利用这些模型解决复杂推理任务的研究兴趣。研究人员已经超越了简单的自回归标记生成，引入了“思考”的概念——一系列表示推理过程中间步骤的标记序列。这一创新范式使LLMs能够模仿复杂的类人推理过程，例如树搜索和反思性思考。最近，一种新兴的学习推理趋势应用强化学习（RL）来训练LLMs掌握推理过程。这种方法通过试错搜索算法自动生成高质量的推理轨迹，大大扩展了LLMs的推理能力，提供更多的训练数据。此外，最近的研究表明，在测试时促使LLMs生成更多的标记进行推理可以进一步显著提高推理准确性。因此，训练时间和测试时间的扩展共同展示了一条新的研究前沿——通往大型推理模型的道路。OpenAI的o1系列的推出标志着这一研究方向的重要里程碑。在本文综述中，我们将全面回顾近年来LLMs推理的发展。首先，我们介绍LLMs的基础背景，然后探讨推动大型推理模型发展的关键技术组件，重点在于自动化数据构建、学习推理技术以及测试时的扩展。我们还将分析用于构建大型推理模型的流行开源项目，并总结存在的挑战和未来的研究方向。', 'title_zh': '面向大规模推理模型：大规模语言模型强化推理综述'}
{'arxiv_id': 'arXiv:2501.09685', 'title': 'Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review', 'authors': 'Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani', 'link': 'https://arxiv.org/abs/2501.09685', 'abstract': 'This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models. While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures). In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning. This tutorial explores the foundational aspects of such inference-time algorithms. We review these methods from a unified perspective, demonstrating that current techniques -- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance -- aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards. Within this framework, we present several novel algorithms not yet covered in the literature. Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models. The code of this tutorial on protein design is available at this https URL', 'abstract_zh': '本教程提供了有关在微分模型中优化下游奖励函数的推理时指导和对齐方法的深入指南。虽然微分模型以其生成建模能力而闻名，但在生物学等领域的实际应用中，往往需要生成最大化特定指标（如稳定性、蛋白质亲和性、接近目标结构）的样本。在这种情况下，微分模型不仅可以生成现实的样本，还可以在推理时显式地最大化所需的度量，而无需微调。本教程探讨了此类推理时算法的基础方面。我们从统一的角度回顾了这些方法，表明当前的技术——如基于顺序蒙特卡洛（SMC）的指导、基于价值的采样和分类器指导——旨在近似软最优去噪过程（即RL中的策略），这些过程将预训练的去噪过程与作为超前函数的价值函数相结合，预测从中间状态到终端奖励。在这一框架内，我们介绍了几项文献中尚未涵盖的新算法。此外，我们讨论了（1）结合推理时技术的微调方法，（2）基于搜索算法（如蒙特卡洛树搜索）的推理时算法，这些算法在当前研究中受到了较少的关注，以及（3）语言模型和微分模型中推理时算法之间的联系。本教程中有关蛋白质设计的代码可在以下链接获取：[此处链接]', 'title_zh': '基于奖励引导的控制生成方法在扩散模型推断时序对齐：教程与综述'}
{'arxiv_id': 'arXiv:2501.09649', 'title': 'Monte Carlo Tree Search with Velocity Obstacles for safe and efficient motion planning in dynamic environments', 'authors': 'Lorenzo Bonanni, Daniele Meli, Alberto Castellini, Alessandro Farinelli', 'link': 'https://arxiv.org/abs/2501.09649', 'abstract': 'Online motion planning is a challenging problem for intelligent robots moving in dense environments with dynamic obstacles, e.g., crowds. In this work, we propose a novel approach for optimal and safe online motion planning with minimal information about dynamic obstacles. Specifically, our approach requires only the current position of the obstacles and their maximum speed, but it does not need any information about their exact trajectories or dynamic model. The proposed methodology combines Monte Carlo Tree Search (MCTS), for online optimal planning via model simulations, with Velocity Obstacles (VO), for obstacle avoidance. We perform experiments in a cluttered simulated environment with walls, and up to 40 dynamic obstacles moving with random velocities and directions. With an ablation study, we show the key contribution of VO in scaling up the efficiency of MCTS, selecting the safest and most rewarding actions in the tree of simulations. Moreover, we show the superiority of our methodology with respect to state-of-the-art planners, including Non-linear Model Predictive Control (NMPC), in terms of improved collision rate, computational and task performance.', 'abstract_zh': '在线运动规划是智能机器人在稠密环境中有动态障碍物（例如人群）的情况下面临的一个具有挑战性的问题。在本文中，我们提出了一种新颖的方法，用于在对动态障碍物了解最少的情况下实现最优且安全的在线运动规划。具体而言，我们的方法仅需要障碍物的当前位置及其最大速度信息，而不需要任何关于其准确轨迹或动力学模型的信息。我们提出的方法结合了蒙特卡洛树搜索（MCTS），通过模型仿真进行在线最优规划，以及速度障碍（VO），用于避免障碍物。我们在一个包含墙壁和最多40个动态障碍物的杂乱模拟环境中进行了实验，这些动态障碍物具有随机速度和方向。通过消融研究，我们展示了VO在扩展MCTS效率方面的关键贡献，特别是在仿真树中选择最安全和最具奖励性的动作。此外，我们展示了与最先进的规划方法（包括非线性模型预测控制（NMPC））相比，我们的方法在碰撞率、计算能力和任务性能方面的优越性。', 'title_zh': '使用速度障碍的蒙特卡洛树搜索在动态环境中的安全高效运动规划'}
{'arxiv_id': 'arXiv:2501.09646', 'title': 'NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary Markov Decision Processes', 'authors': 'Nathaniel S. Keplinger, Baiting Luo, Iliyas Bektas, Yunuo Zhang, Kyle Hollins Wray, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay', 'link': 'https://arxiv.org/abs/2501.09646', 'abstract': "In many real-world applications, agents must make sequential decisions in environments where conditions are subject to change due to various exogenous factors. These non-stationary environments pose significant challenges to traditional decision-making models, which typically assume stationary dynamics. Non-stationary Markov decision processes (NS-MDPs) offer a framework to model and solve decision problems under such changing conditions. However, the lack of standardized benchmarks and simulation tools has hindered systematic evaluation and advance in this field. We present NS-Gym, the first simulation toolkit designed explicitly for NS-MDPs, integrated within the popular Gymnasium framework. In NS-Gym, we segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations to dynamic environments. We review prior work in this domain and present a toolkit encapsulating key problem characteristics and types in NS-MDPs. This toolkit is the first effort to develop a set of standardized interfaces and benchmark problems to enable consistent and reproducible evaluation of algorithms under non-stationary conditions. We also benchmark six algorithmic approaches from prior work on NS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to assess the adaptability and robustness of their decision-making algorithms to non-stationary conditions.", 'abstract_zh': '在许多实际应用中，代理必须在条件因各种外生因素而发生变化的环境中做出顺序决策。这些非平稳环境对传统决策模型构成了重大挑战，后者通常假设动态是稳定的。非平稳马尔可夫决策过程（NS-MDP）为在这种变化条件下的决策问题提供了一个建模和求解框架。然而，缺乏标准化基准和仿真工具阻碍了该领域系统的评估和进展。我们提出了NS-Gym，这是一个专门为此设计的仿真工具包，并集成在流行的Gymnasium框架中。在NS-Gym中，我们分离了表征非平稳性的环境参数的演化过程与代理的决策模块，从而允许灵活且模块化的适应动态环境。我们回顾了该领域的先前工作，并提供了一个封装关键问题特性和类型的工具包。该工具包是首个尝试开发标准化接口和基准问题集的努力，以在非平稳条件下实现一致性和可再现性的算法评估。我们还使用NS-Gym对先前工作的六种算法方法进行了基准测试。我们的愿景是NS-Gym将使研究人员能够评估其决策算法在非平稳条件下的适应性和鲁棒性。', 'title_zh': 'NS-Gym: 开源非平稳马尔可夫决策过程的模拟环境和基准测试平台'}
{'arxiv_id': 'arXiv:2501.09645', 'title': 'CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding', 'authors': 'Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, Elisabeth André', 'link': 'https://arxiv.org/abs/2501.09645', 'abstract': "In today's assistant landscape, personalisation enhances interactions, fosters long-term relationships, and deepens engagement. However, many systems struggle with retaining user preferences, leading to repetitive user requests and disengagement. Furthermore, the unregulated and opaque extraction of user preferences in industry applications raises significant concerns about privacy and trust, especially in regions with stringent regulations like Europe. In response to these challenges, we propose a long-term memory system for voice assistants, structured around predefined categories. This approach leverages Large Language Models to efficiently extract, store, and retrieve preferences within these categories, ensuring both personalisation and transparency. We also introduce a synthetic multi-turn, multi-session conversation dataset (CarMem), grounded in real industry data, tailored to an in-car voice assistant setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to .95 in preference extraction, depending on category granularity. Our maintenance strategy reduces redundant preferences by 95% and contradictory ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively, the results demonstrate the system's suitability for industrial applications.", 'abstract_zh': '在当今的辅助助理环境中，个性化可以增强互动、促进长期关系，并加深参与度。然而，许多系统在保留用户偏好方面存在困难，导致用户重复请求和参与度下降。此外，在行业应用中未经规范和透明地提取用户偏好引发了关于隐私和信任的重大关切，特别是在像欧洲这样有严格法规的地区。为应对这些挑战，我们提出了一种基于预定义类别的长期记忆系统，该系统利用大型语言模型高效地提取、存储和检索这些类别的偏好，确保了个性化和透明度。我们还引入了一个基于实际行业数据的合成多轮多会话对话数据集（CarMem），适用于车载语音助手的环境。在该数据集上测试，我们的系统在偏好提取上的F1分数范围为0.78到0.95，具体取决于类别的细度。我们的维护策略将重复的偏好减少了95%，矛盾的偏好减少了92%，而最优检索的准确性达到了0.87。整体而言，实验结果表明该系统适用于工业应用。', 'title_zh': 'CarMem：通过类别限定增强大型语言模型语音助手的长期记忆'}
{'arxiv_id': 'arXiv:2501.09640', 'title': 'Electronic Health Records: Towards Digital Twins in Healthcare', 'authors': 'Muhammet Alkan, Hester Huijsdens, Yola Jones, Fani Deligianni', 'link': 'https://arxiv.org/abs/2501.09640', 'abstract': "The pivotal shift from traditional paper-based records to sophisticated Electronic Health Records (EHR), enabled systematic collection and analysis of patient data through descriptive statistics, providing insight into patterns and trends across patient populations. This evolution continued toward predictive analytics, allowing healthcare providers to anticipate patient outcomes and potential complications before they occur. This progression from basic digital record-keeping to sophisticated predictive modelling and digital twins reflects healthcare's broader evolution toward more integrated, patient-centred approaches that combine data-driven insights with personalized care delivery. This chapter explores the evolution and significance of healthcare information systems, beginning with an examination of the implementation of EHR in the UK and the USA. It provides a comprehensive overview of the International Classification of Diseases (ICD) system, tracing its development from ICD-9 to ICD-10. Central to this discussion is the MIMIC-III database, a landmark achievement in healthcare data sharing and arguably the most comprehensive critical care database freely available to researchers worldwide. MIMIC-III has democratized access to high-quality healthcare data, enabling unprecedented opportunities for research and analysis. The chapter examines its structure, clinical outcome analysis capabilities, and practical applications through case studies, with a particular focus on mortality and length of stay metrics, vital signs extraction, and ICD coding. Through detailed entity-relationship diagrams and practical examples, the text illustrates MIMIC's complex data structure and demonstrates how different querying approaches can lead to subtly different results, emphasizing the critical importance of understanding the database's architecture for accurate data extraction.", 'abstract_zh': '从传统的纸质记录向 sophisticated 的电子健康记录（EHR）的转变标志着系统性收集和分析患者数据的里程碑，通过描述性统计提供患者人群模式和趋势的见解。这种演变进一步发展到预测分析，使医疗服务提供者能够提前预测患者结果和潜在并发症。从基本的电子记录保持到复杂的预测建模和数字孪生反映了医疗保健向更整合、以患者为中心的方法的演变，这些方法结合了数据驱动的见解和个性化护理的交付。本章探讨了医疗信息系统的发展及其意义，从英国和美国的电子健康记录（EHR）实施开始。该章提供了一个关于国际疾病分类（ICD）系统的全面概述，追溯了从ICD-9到ICD-10的发展。在此讨论的核心是MIMIC-III数据库——这个在医疗数据共享方面的里程碑成就，可能是提供给全球研究人员最全面的重症监护数据库之一。MIMIC-III实现了高质量医疗数据的民主化访问，为研究和分析提供了前所未有的机会。本章考察了其结构、临床结果分析能力和通过案例研究的实际应用，尤其关注死亡率和住院时间指标、生命体征提取和ICD编码。通过详细的关系图和实用示例，文本展示了MIMIC复杂的数据结构，并说明了不同的查询方法如何导致略有不同的结果，强调了理解和掌握数据库架构对于准确提取数据的重要性。', 'title_zh': '电子健康记录：迈向医疗领域的数字孪生技术'}
{'arxiv_id': 'arXiv:2501.09632', 'title': 'Platform-Aware Mission Planning', 'authors': 'Stefan Panjkovic, Alessandro Cimatti, Andrea Micheli, Stefano Tonetta', 'link': 'https://arxiv.org/abs/2501.09632', 'abstract': 'Planning for autonomous systems typically requires reasoning with models at different levels of abstraction, and the harmonization of two competing sets of objectives: high-level mission goals that refer to an interaction of the system with the external environment, and low-level platform constraints that aim to preserve the integrity and the correct interaction of the subsystems. The complicated interplay between these two models makes it very hard to reason on the system as a whole, especially when the objective is to find plans with robustness guarantees, considering the non-deterministic behavior of the lower layers of the system.\nIn this paper, we introduce the problem of Platform-Aware Mission Planning (PAMP), addressing it in the setting of temporal durative actions. The PAMP problem differs from standard temporal planning for its exists-forall nature: the high-level plan dealing with mission goals is required to satisfy safety and executability constraints, for all the possible non-deterministic executions of the low-level model of the platform and the environment. We propose two approaches for solving PAMP. The first baseline approach amalgamates the mission and platform levels, while the second is based on an abstraction-refinement loop that leverages the combination of a planner and a verification engine. We prove the soundness and completeness of the proposed approaches and validate them experimentally, demonstrating the importance of heterogeneous modeling and the superiority of the technique based on abstraction-refinement.', 'abstract_zh': '以下是将英文内容翻译成中文的版本，符合学术规范：\n\n自动系统的规划通常需要在不同的抽象级别上推理模型，并解决两个相互竞争的目标集之间的协调问题：高层次的任务目标指的是系统与外部环境的交互，而低层次的平台约束旨在保持子系统的完整性和正确的交互。这两种模型之间的复杂交互使得系统整体推理变得非常困难，尤其是在寻找具有鲁棒性保证的计划时，这种困难更加明显，考虑到系统的较低层具有非确定性行为。\n\n本文提出了一种平台感知的任务规划（Platform-Aware Mission Planning, PAMP）问题，并在时间持续动作的背景下进行研究。与标准的时间规划不同，PAMP问题具有存在-普遍存在（exists-forall）的性质：高层次计划必须满足所有可能的低层次平台和环境非确定性执行的安全性和可执行性约束。我们提出了两种解决PAMP问题的方法。第一种基线方法将任务和平台级别结合起来，第二种方法基于一种抽象细化循环，该循环结合了一个计划器和一个验证引擎。我们证明了所提出方法的有效性，并通过实验验证了它们，强调了异构建模的重要性，并展示了基于抽象细化的技术的优势。', 'title_zh': '平台感知任务规划'}
{'arxiv_id': 'arXiv:2501.09628', 'title': 'Artificial Intelligence-Driven Clinical Decision Support Systems', 'authors': 'Muhammet Alkan, Idris Zakariyya, Samuel Leighton, Kaushik Bhargav Sivangi, Christos Anagnostopoulos, Fani Deligianni', 'link': 'https://arxiv.org/abs/2501.09628', 'abstract': 'As artificial intelligence (AI) becomes increasingly embedded in healthcare delivery, this chapter explores the critical aspects of developing reliable and ethical Clinical Decision Support Systems (CDSS). Beginning with the fundamental transition from traditional statistical models to sophisticated machine learning approaches, this work examines rigorous validation strategies and performance assessment methods, including the crucial role of model calibration and decision curve analysis. The chapter emphasizes that creating trustworthy AI systems in healthcare requires more than just technical accuracy; it demands careful consideration of fairness, explainability, and privacy. The challenge of ensuring equitable healthcare delivery through AI is stressed, discussing methods to identify and mitigate bias in clinical predictive models. The chapter then delves into explainability as a cornerstone of human-centered CDSS. This focus reflects the understanding that healthcare professionals must not only trust AI recommendations but also comprehend their underlying reasoning. The discussion advances in an analysis of privacy vulnerabilities in medical AI systems, from data leakage in deep learning models to sophisticated attacks against model explanations. The text explores privacy-preservation strategies such as differential privacy and federated learning, while acknowledging the inherent trade-offs between privacy protection and model performance. This progression, from technical validation to ethical considerations, reflects the multifaceted challenges of developing AI systems that can be seamlessly and reliably integrated into daily clinical practice while maintaining the highest standards of patient care and data protection.', 'abstract_zh': '随着人工智能（AI）在医疗保健领域的日益嵌入，本章探讨了开发可靠且伦理的临床决策支持系统（CDSS）的关键方面。从传统的统计模型过渡到复杂的机器学习方法入手，本研究探讨了严格的验证策略和性能评估方法，包括模型校准和决策曲线分析中至关重要的作用。本章强调，在医疗保健中创建可信赖的AI系统不仅需要技术准确性，还需要仔细考虑公平性、可解释性和隐私保护。确保通过AI实现医疗服务的公平性是一项挑战，本章讨论了如何识别和缓解临床预测模型中的偏差。随后，本章深入探讨了以人为中心的CDSS中的可解释性作为基石。这一重点反映了这样一种理解：医疗保健专业人员不仅应信任AI推荐，还应理解其背后的推理。讨论进一步深入到对医疗AI系统中隐私漏洞的分析，从深度学习模型中的数据泄露到针对模型解释的复杂攻击。本文探讨了差异隐私和联邦学习等隐私保护策略，同时认识到隐私保护与模型性能之间固有的权衡。从技术验证到伦理考量的逐步推进，反映了在保持最高标准的患者护理和数据保护的同时，无缝且可靠地将AI系统整合到临床实践中所面临的多方面挑战。', 'title_zh': '人工智能驱动的临床决策支持系统'}
{'arxiv_id': 'arXiv:2501.09534', 'title': 'AI in Support of Diversity and Inclusion', 'authors': 'Çiçek Güven, Afra Alishahi, Henry Brighton, Gonzalo Nápoles, Juan Sebastian Olier, Marie Šafář, Eric Postma, Dimitar Shterionov, Mirella De Sisto, Eva Vanmassenhove', 'link': 'https://arxiv.org/abs/2501.09534', 'abstract': "In this paper, we elaborate on how AI can support diversity and inclusion and exemplify research projects conducted in that direction. We start by looking at the challenges and progress in making large language models (LLMs) more transparent, inclusive, and aware of social biases. Even though LLMs like ChatGPT have impressive abilities, they struggle to understand different cultural contexts and engage in meaningful, human like conversations. A key issue is that biases in language processing, especially in machine translation, can reinforce inequality. Tackling these biases requires a multidisciplinary approach to ensure AI promotes diversity, fairness, and inclusion. We also highlight AI's role in identifying biased content in media, which is important for improving representation. By detecting unequal portrayals of social groups, AI can help challenge stereotypes and create more inclusive technologies. Transparent AI algorithms, which clearly explain their decisions, are essential for building trust and reducing bias in AI systems. We also stress AI systems need diverse and inclusive training data. Projects like the Child Growth Monitor show how using a wide range of data can help address real world problems like malnutrition and poverty. We present a project that demonstrates how AI can be applied to monitor the role of search engines in spreading disinformation about the LGBTQ+ community. Moreover, we discuss the SignON project as an example of how technology can bridge communication gaps between hearing and deaf people, emphasizing the importance of collaboration and mutual trust in developing inclusive AI. Overall, with this paper, we advocate for AI systems that are not only effective but also socially responsible, promoting fair and inclusive interactions between humans and machines.", 'abstract_zh': '在本文中，我们详细探讨了人工智能如何支持多样性和包容性，并展示了在该方向上进行的研究项目。我们首先分析了使大规模语言模型（LLMs）更加透明、包容以及意识到社会偏见所面临的挑战和取得的进步。尽管像ChatGPT这样的大规模语言模型具有令人印象深刻的性能，但它们在理解不同文化背景和进行有意义的、类人对话方面仍存在问题。一个关键问题在于，特别是在机器翻译中的语言处理偏见会强化不平等。解决这些偏见需要多学科的方法来确保人工智能促进多样性、公平性和包容性。我们还强调了人工智能在识别媒体中的偏见性内容方面的作用，这对于改善代表性至关重要。通过检测对社会群体不平等的描绘，人工智能可以帮助挑战刻板印象并创造更具包容性的技术。透明的人工智能算法，能够清晰解释其决策过程，对于建立对人工智能系统的信任并减少偏见至关重要。我们还强调了需要多样和包容性的训练数据来构建人工智能系统。例如，儿童成长监测项目展示了使用广泛数据如何有助于解决实际问题，如营养不良和贫困。我们提出了一个项目，展示了人工智能如何应用于监测搜索引擎在传播关于LGBTQ+社区的误导信息方面的角色。此外，我们讨论了SignON项目，作为如何通过技术弥合听力和聋哑人之间的交流差距的例子，强调了在开发包容性人工智能时合作和互信的重要性。总体而言，通过本文，我们倡导不仅有效而且具有社会责任感的人工智能系统，促进人机之间的公平和包容性交互。', 'title_zh': 'AI 在促进多样性和包容性方面的应用'}
{'arxiv_id': 'arXiv:2501.09431', 'title': 'A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy', 'authors': 'Huandong Wang, Wenjie Fu, Yingzhou Tang, Zhilong Chen, Yuxi Huang, Jinghua Piao, Chen Gao, Fengli Xu, Tao Jiang, Yong Li', 'link': 'https://arxiv.org/abs/2501.09431', 'abstract': 'While large language models (LLMs) present significant potential for supporting numerous real-world applica- tions and delivering positive social impacts, they still face significant challenges in terms of the inherent risk of privacy leakage, hallucinated outputs, and value misalignment, and can be maliciously used for generating toxic content and unethical purposes after been jailbroken. Therefore, in this survey, we present a comprehensive review of recent advancements aimed at mitigating these issues, organized across the four phases of LLM development and usage: data collecting and pre-training, fine-tuning and alignment, prompting and reasoning, and post-processing and auditing. We elaborate on the recent advances for enhancing the performance of LLMs in terms of privacy protection, hallucination reduction, value alignment, toxicity elimination, and jailbreak defenses. In contrast to previous surveys that focus on a single dimension of responsible LLMs, this survey presents a unified framework that encompasses these diverse dimensions, providing a comprehensive view of enhancing LLMs to better serve real-world applications.', 'abstract_zh': '尽管大型语言模型（LLMs）具有支持众多实际应用并带来积极社会影响的潜力，但它们仍然面临着隐私泄露风险、幻觉输出以及价值偏差等固有的挑战，且在被破解后可能会被恶意利用以生成有毒内容及不道德的目的。因此，在这项综述中，我们对旨在缓解这些问题的近期进展进行了全面回顾，并按照LLM开发和使用的四个阶段进行了组织：数据收集与预训练、微调与对齐、提示与推理，以及后处理与审计。我们详细介绍了为提高LLMs在隐私保护、减少幻觉、价值对齐、消除有毒内容以及破解防御方面的性能所取得的最新进展。与以往专注于负责使用LLMs单一方面的综述不同，本综述提供了一个统一的框架，涵盖了这些多方面的内容，为改善LLMs以更好地服务于实际应用提供了全面的视角。', 'title_zh': '负责任的人工语言模型综述：固有风险、恶意使用及缓解策略'}
{'arxiv_id': 'arXiv:2501.09368', 'title': 'Aligning Instruction Tuning with Pre-training', 'authors': 'Yiming Liang, Tianyu Zheng, Xinrun Du, Ge Zhang, Xingwei Qu, Xiang Yue, Chujie Zheng, Jiaheng Liu, Lei Ma, Wenhu Chen, Guoyin Wang, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang', 'link': 'https://arxiv.org/abs/2501.09368', 'abstract': 'Instruction tuning enhances large language models (LLMs) to follow human instructions across diverse tasks, relying on high-quality datasets to guide behavior. However, these datasets, whether manually curated or synthetically generated, are often narrowly focused and misaligned with the broad distributions captured during pre-training, limiting LLM generalization and effective use of pre-trained knowledge. We propose *Aligning Instruction Tuning with Pre-training* (AITP), a method that bridges this gap by identifying coverage shortfalls in instruction-tuning datasets and rewriting underrepresented pre-training data into high-quality instruction-response pairs. This approach enriches dataset diversity while preserving task-specific objectives. Evaluations on three fully open LLMs across eight benchmarks demonstrate consistent performance improvements with AITP. Ablations highlight the benefits of adaptive data selection, controlled rewriting, and balanced integration, emphasizing the importance of aligning instruction tuning with pre-training distributions to unlock the full potential of LLMs.', 'abstract_zh': '指令微调增强了大规模语言模型（LLMs）在各种任务中遵循人类指令的能力，依赖于高质量数据集的指导。然而，这些数据集，无论是手工策划的还是合成生成的，往往聚焦狭窄且与预训练期间捕捉到的广泛分布不一致，限制了LLM的泛化能力和预训练知识的有效利用。我们提出了一种名为*预训练与指令微调对齐*（AITP，Alignment of Instruction Tuning with Pre-training）的方法，该方法通过识别指令微调数据集中的覆盖率不足，并将未充分代表的预训练数据改写为高质量的指令-响应对，从而弥合了这一差距。这种方法丰富了数据集的多样性，同时保留了特定任务的目标。在三个完全开放的LLM上，针对八个基准进行的评估表明，AITP能够一致地提高性能。消融实验突显了自适应数据选择、可控改写和平衡整合的好处，强调了将指令微调与预训练分布对齐的重要性，以充分利用LLM的潜力。', 'title_zh': '将指令调优与预训练对齐'}
{'arxiv_id': 'arXiv:2501.09355', 'title': 'YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents in Augmented Reality Tasks', 'authors': 'Saptarashmi Bandyopadhyay, Vikas Bahirwani, Lavisha Aggarwal, Bhanu Guda, Lin Li, Andrea Colaco', 'link': 'https://arxiv.org/abs/2501.09355', 'abstract': "Multimodal AI Agents are AI models that have the capability of interactively and cooperatively assisting human users to solve day-to-day tasks. Augmented Reality (AR) head worn devices can uniquely improve the user experience of solving procedural day-to-day tasks by providing egocentric multimodal (audio and video) observational capabilities to AI Agents. Such AR capabilities can help AI Agents see and listen to actions that users take which can relate to multimodal capabilities of human users. Existing AI Agents, either Large Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive in nature, which means that models cannot take an action without reading or listening to the human user's prompts. Proactivity of AI Agents on the other hand can help the human user detect and correct any mistakes in agent observed tasks, encourage users when they do tasks correctly or simply engage in conversation with the user - akin to a human teaching or assisting a user. Our proposed YET to Intervene (YETI) multimodal agent focuses on the research question of identifying circumstances that may require the agent to intervene proactively. This allows the agent to understand when it can intervene in a conversation with human users that can help the user correct mistakes on tasks, like cooking, using AR. Our YETI Agent learns scene understanding signals based on interpretable notions of Structural Similarity (SSIM) on consecutive video frames. We also define the alignment signal which the AI Agent can learn to identify if the video frames corresponding to the user's actions on the task are consistent with expected actions. These signals are used by our AI Agent to determine when it should proactively intervene. We compare our results on the instances of proactive intervention in the HoloAssist multimodal benchmark for an expert agent guiding a user to complete procedural tasks.", 'abstract_zh': '多模态AI代理是能够交互和协作帮助人类用户解决日常任务的AI模型。增强现实（AR）头戴设备通过为AI代理提供以自我为中心的多模态（音频和视频）观察能力，能够独特地提升解决程序性日常任务的用户体验。这些AR能力可以帮助AI代理看到并听到用户进行的动作，从而与人类用户的多模态能力相关联。现有的AI代理，无论是大型语言模型（LLM）还是多模态视觉语言模型（VLM），本质上是反应性的，这意味着模型在执行动作之前需要读取或听取人类用户的提示。另一方面，AI代理的主动性可以帮助人类用户检测并纠正代理观察到的任务中的错误、鼓励用户正确完成任务，或者简单地与用户进行对话，如同人类在指导用户或帮助用户一样。我们提出的YET to Intervene（YETI）多模态代理专注于研究识别需要代理主动干预的情况。这使代理能够理解何时在与人类用户的对话中可以主动干预，以帮助用户纠正任务中的错误，比如烹饪，在AR的帮助下。我们的YETI代理基于连续视频帧的可解释结构相似性（SSIM）学习场景理解信号。我们还定义了对齐信号，该信号使AI代理能够识别出与任务中用户动作对应的视频帧是否与预期动作一致。这些信号被我们的AI代理用于决定何时应主动干预。我们在HoloAssist多模态基准数据集上比较了主动干预实例的实验结果，该基准数据集用于专家代理指导用户完成程序性任务。', 'title_zh': 'YETI（尚未干预）：多模态AI代理在增强现实任务中的主动干预'}
{'arxiv_id': 'arXiv:2501.09316', 'title': 'SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs', 'authors': 'Anbang Ye, Qianran Ma, Jia Chen, Muqi Li, Tong Li, Fujiao Liu, Siqi Mai, Meichen Lu, Haitao Bao, Yang You', 'link': 'https://arxiv.org/abs/2501.09316', 'abstract': 'Despite significant advancements in general-purpose AI agents, several challenges still hinder their practical application in real-world scenarios. First, the limited planning capabilities of Large Language Models (LLM) restrict AI agents from effectively solving complex tasks that require long-horizon planning. Second, general-purpose AI agents struggle to efficiently utilize domain-specific knowledge and human expertise. In this paper, we introduce the Standard Operational Procedure-guided Agent (SOP-agent), a novel framework for constructing domain-specific agents through pseudocode-style Standard Operational Procedures (SOPs) written in natural language. Formally, we represent a SOP as a decision graph, which is traversed to guide the agent in completing tasks specified by the SOP. We conduct extensive experiments across tasks in multiple domains, including decision-making, search and reasoning, code generation, data cleaning, and grounded customer service. The SOP-agent demonstrates excellent versatility, achieving performance superior to general-purpose agent frameworks and comparable to domain-specific agent systems. Additionally, we introduce the Grounded Customer Service Benchmark, the first benchmark designed to evaluate the grounded decision-making capabilities of AI agents in customer service scenarios based on SOPs.', 'abstract_zh': '尽管通用人工智能代理在技术上取得了显著进展，但在实际应用场景中仍面临着几个挑战。首先，大型语言模型（LLM）的有限规划能力限制了代理在需要长期规划的复杂任务中的有效解决。其次，通用人工智能代理难以高效利用特定领域的知识和人类的专业技能。在本文中，我们提出了标准操作规程引导代理（SOP-agent），这是一种通过自然语言编写的伪代码形式的标准操作规程（SOPs）来构建特定领域代理的新框架。具体而言，我们将SOP表示为决策图，该图在执行过程中引导代理完成由SOP指定的任务。我们在多个领域的任务中进行了广泛的实验，包括决策制定、搜索与推理、代码生成、数据清洗和基于SOP的地面客服。SOP-agent表现出了极高的灵活性，其性能优于通用代理框架，并与特定领域代理系统相当。此外，我们还提出了基于SOP的地面客服基准，这是首个旨在评估AI代理在客户服务场景中基于SOP的决策能力的标准。', 'title_zh': 'SOP-Agent：赋予通用人工智能代理领域特定的工作流程规范'}
{'arxiv_id': 'arXiv:2501.09284', 'title': 'SEAL: Entangled White-box Watermarks on Low-Rank Adaptation', 'authors': 'Giyeong Oh, Seajin Kim, Woohyun Cho, Sangkyu Lee, Jiwan Chung, Dokyung Song, Youngjae Yu', 'link': 'https://arxiv.org/abs/2501.09284', 'abstract': 'Recently, LoRA and its variants have become the de facto strategy for training and sharing task-specific versions of large pretrained models, thanks to their efficiency and simplicity. However, the issue of copyright protection for LoRA weights, especially through watermark-based techniques, remains underexplored. To address this gap, we propose SEAL (SEcure wAtermarking on LoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a secret, non-trainable matrix between trainable LoRA weights, serving as a passport to claim ownership. SEAL then entangles the passport with the LoRA weights through training, without extra loss for entanglement, and distributes the finetuned weights after hiding the passport. When applying SEAL, we observed no performance degradation across commonsense reasoning, textual/visual instruction tuning, and text-to-image synthesis tasks. We demonstrate that SEAL is robust against a variety of known attacks: removal, obfuscation, and ambiguity attacks.', 'abstract_zh': '近年来，LoRA及其变体已成为训练和共享特定任务大型预训练模型版本的事实标准策略，这得益于它们的高效性和简洁性。然而，LoRA权重的版权保护问题，特别是通过水印技术，仍未得到充分探索。为解决这一问题，我们提出了一种名为SEAL（SEcure wAtermarking on LoRA weights）的通用白盒水印技术，适用于LoRA。SEAL在可训练的LoRA权重之间嵌入一个秘密且不可训练的矩阵，作为所有权的证明。SEAL在训练过程中将此证明与LoRA权重缠结在一起，而无需额外的损失，然后在隐藏证明后分发微调后的权重。在应用SEAL时，我们发现其在常识推理、文本/视觉指令调优以及文本到图像合成任务中均未对性能产生任何下降。我们证明了SEAL在多种已知攻击（如移除、混淆和含糊性攻击）面前具有鲁棒性。', 'title_zh': 'SEAL：低秩适应中的纠缠白盒水印'}
{'arxiv_id': 'arXiv:2501.09279', 'title': 'Text Semantics to Flexible Design: A Residential Layout Generation Method Based on Stable Diffusion Model', 'authors': 'Zijin Qiu, Jiepeng Liu, Yi Xia, Hongtuo Qi, Pengkun Liu', 'link': 'https://arxiv.org/abs/2501.09279', 'abstract': 'Flexibility in the AI-based residential layout design remains a significant challenge, as traditional methods like rule-based heuristics and graph-based generation often lack flexibility and require substantial design knowledge from users. To address these limitations, we propose a cross-modal design approach based on the Stable Diffusion model for generating flexible residential layouts. The method offers multiple input types for learning objectives, allowing users to specify both boundaries and layouts. It incorporates natural language as design constraints and introduces ControlNet to enable stable layout generation through two distinct pathways. We also present a scheme that encapsulates design expertise within a knowledge graph and translates it into natural language, providing an interpretable representation of design knowledge. This comprehensibility and diversity of input options enable professionals and non-professionals to directly express design requirements, enhancing flexibility and controllability. Finally, experiments verify the flexibility of the proposed methods under multimodal constraints better than state-of-the-art models, even when specific semantic information about room areas or connections is incomplete.', 'abstract_zh': '基于人工智能的住宅布局设计灵活性仍然是一个显著挑战，传统的规则基启发式方法和图基生成方法往往缺乏灵活性，并且需要用户具备大量的设计知识。为了解决这些限制，我们提出了一种基于Stable Diffusion模型的跨模态设计方法，以生成灵活的住宅布局。该方法支持多种输入类型的学习目标，允许用户指定边界和布局。它将自然语言作为设计约束，并引入ControlNet，通过两条不同的路径实现稳定的布局生成。我们还提出了一种方案，将设计专业知识封装到知识图谱中，并将其转换为自然语言，提供设计知识的可解释表示。这种可解释性及多样化的输入选项使专业人士和非专业人士可以直接表达设计要求，从而增强灵活性和可控性。最后，实验结果表明，在多模态约束条件下，所提出的方法在灵活性方面优于最先进的模型，即使缺乏关于房间面积或连接的具体语义信息也是如此。', 'title_zh': '文本含义到灵活设计：基于稳定扩散模型的住宅布局生成方法'}
{'arxiv_id': 'arXiv:2501.09239', 'title': 'AI-based Identity Fraud Detection: A Systematic Review', 'authors': 'Chuo Jun Zhang, Asif Q. Gill, Bo Liu, Memoona J. Anwar', 'link': 'https://arxiv.org/abs/2501.09239', 'abstract': 'With the rapid development of digital services, a large volume of personally identifiable information (PII) is stored online and is subject to cyberattacks such as Identity fraud. Most recently, the use of Artificial Intelligence (AI) enabled deep fake technologies has significantly increased the complexity of identity fraud. Fraudsters may use these technologies to create highly sophisticated counterfeit personal identification documents, photos and videos. These advancements in the identity fraud landscape pose challenges for identity fraud detection and society at large. There is a pressing need to review and understand identity fraud detection methods, their limitations and potential solutions. This research aims to address this important need by using the well-known systematic literature review method. This paper reviewed a selected set of 43 papers across 4 major academic literature databases. In particular, the review results highlight the two types of identity fraud prevention and detection methods, in-depth and open challenges. The results were also consolidated into a taxonomy of AI-based identity fraud detection and prevention methods including key insights and trends. Overall, this paper provides a foundational knowledge base to researchers and practitioners for further research and development in this important area of digital identity fraud.', 'abstract_zh': '随着数字服务的快速进步，大量的个人可识别信息（PII）被存储在网络上，并且面临身份诈骗等网络攻击。最近，人工智能（AI）赋能的深度伪造技术的使用极大地增加了身份诈骗的复杂性。诈骗者可能利用这些技术创建极为精巧的伪造个人身份文件、照片和视频。这些身份诈骗领域的发展进步对身份诈骗检测和整个社会提出了挑战。迫切需要审视和理解身份诈骗检测方法、它们的局限性和潜在解决方案。本研究旨在通过使用广为人知的系统文献综述方法满足这一重要需求。本文回顾了四大主要学术文献数据库中的43篇精选论文。尤其是，综述结果突显了身份诈骗预防和检测方法的两大类型，深入探讨了具体的挑战。此外，结果还被整理成基于AI的身份诈骗检测和预防方法的分类体系，包括关键见解和趋势。总体而言，本文为研究人员和实践者提供了该重要领域数字身份诈骗进一步研究和开发的基础知识库。', 'title_zh': '基于AI的身份欺诈检测：一项系统性综述'}
{'arxiv_id': 'arXiv:2501.09182', 'title': 'A Blockchain-Enabled Approach to Cross-Border Compliance and Trust', 'authors': 'Vikram Kulothungan', 'link': 'https://arxiv.org/abs/2501.09182', 'abstract': 'As artificial intelligence (AI) systems become increasingly integral to critical infrastructure and global operations, the need for a unified, trustworthy governance framework is more urgent that ever. This paper proposes a novel approach to AI governance, utilizing blockchain and distributed ledger technologies (DLT) to establish a decentralized, globally recognized framework that ensures security, privacy, and trustworthiness of AI systems across borders. The paper presents specific implementation scenarios within the financial sector, outlines a phased deployment timeline over the next decade, and addresses potential challenges with solutions grounded in current research. By synthesizing advancements in blockchain, AI ethics, and cybersecurity, this paper offers a comprehensive roadmap for a decentralized AI governance framework capable of adapting to the complex and evolving landscape of global AI regulation.', 'abstract_zh': '随着人工智能（AI）系统在关键基础设施和全球运营中发挥越来越核心的作用，建立一个统一、值得信赖的治理框架的需求变得前所未有的迫切。本文提出了一种新的AI治理方法，利用区块链和分布式账本技术（DLT）建立一个去中心化、全球认可的框架，以确保跨越国界的人工智能系统的安全、隐私和可信度。本文在金融领域具体呈现了实施场景，概述了未来十年分阶段部署的时间表，并通过当前研究成果提出的解决方案来应对潜在挑战。通过整合区块链技术、AI伦理和网络安全领域的最新进展，本文提供了一整套路线图，旨在构建一个能够适应全球人工智能监管复杂多变环境的去中心化AI治理框架。', 'title_zh': '基于区块链的跨境合规与信任实现方法'}
{'arxiv_id': 'arXiv:2501.09136', 'title': 'Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG', 'authors': 'Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei', 'link': 'https://arxiv.org/abs/2501.09136', 'abstract': 'Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management.\nAgentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications.\nThis survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG', 'abstract_zh': '大型语言模型（LLMs）通过实现类人的文本生成和自然语言理解，极大地革新了人工智能（AI）。然而，它们依赖于静态训练数据，限制了它们对动态、实时查询的响应能力，导致输出过时或不准确。检索增强生成（RAG）作为一种解决方案已经出现，通过集成实时数据检索来增强LLMs，提供上下文相关且最新的响应。尽管具有潜力，传统的RAG系统受到了静态工作流程的限制，缺乏为多步骤推理和复杂任务管理所需的适应性。\n\n自主检索增强生成（Agentic RAG）超越了这些限制，通过将自主AI代理嵌入到RAG管道中来实现。这些代理利用自主设计模式中的反思、规划、工具使用和多智能体协作，动态管理检索策略，迭代细化上下文理解，并根据复杂任务需求适应工作流程。这种整合使得Agentic RAG系统在各种应用中提供无与伦比的灵活性、可扩展性和上下文感知能力。\n\n本文综述了Agentic RAG的全面探索，从其基础原理和RAG范式的演变开始。详细介绍了Agentic RAG架构的分类，突出了其在医疗、金融和教育等行业中的关键应用，并探讨了实施策略。此外，本文还讨论了这些系统扩展时面临的挑战、确保伦理决策以及优化实际应用中的性能，同时提供了实施Agentic RAG的框架和工具的详细见解。', 'title_zh': '代理检索增强生成：关于代理RAG的研究综述\n\n在这个翻译中，“Agentic Retrieval-Augmented Generation”被翻译为“代理检索增强生成”，“Survey on Agentic RAG”翻译为“关于代理RAG的研究综述”。这样的翻译既保留了原文的意思，又符合中文的表达习惯和学术规范。'}
{'arxiv_id': 'arXiv:2501.09755', 'title': 'Learnings from Scaling Visual Tokenizers for Reconstruction and Generation', 'authors': 'Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen', 'link': 'https://arxiv.org/abs/2501.09755', 'abstract': "Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.", 'abstract_zh': '通过自编码器实现视觉标记化能够增强最先进的图像和视频生成模型，将其像素压缩到潜在空间。尽管基于Transformer的生成器的扩展在最近的进步中占有重要地位，但标记化组件本身很少被扩展，这留下了关于自编码器设计选择如何影响其重构目标以及下游生成性能的问题。我们的工作旨在探索自编码器的扩展，以填补这一空白。为了促进这一探索，我们用增强的Vision Transformer架构（ViTok）代替典型的卷积骨干网络进行标记化。我们使用远超ImageNet-1K的大规模图像和视频数据集对ViTok进行训练，从而消除了对标记化组件扩展的数据限制。我们首先研究自编码器瓶颈扩展如何影响重构和生成——发现它与重构高度相关，但与生成的关系更为复杂。接下来，我们探讨了分别扩展自编码器的编码器和解码器对重构和生成性能的影响。关键的是，我们发现扩展编码器在重构和生成方面几乎没有提高，而扩展解码器可以提高重构性能，但对于生成的益处是混合的。基于我们的探索，我们设计了ViTok，这是一种轻量级自编码器，在ImageNet-1K和COCO重构任务（分辨率分别为256p和512p）上获得了与最先进的自编码器相当的性能，同时在UCF-101的16帧128p视频重构任务上超过了现有自编码器，且计算量减少2-5倍。当与扩散Transformer结合时，ViTok在ImageNet-1K图像生成任务中表现出竞争力，并在UCF-101上的类条件视频生成任务中建立了新的性能基准。', 'title_zh': '面向重构与生成的视觉标记化扩展学习'}
{'arxiv_id': 'arXiv:2501.09725', 'title': 'Parallel multi-objective metaheuristics for smart communications in vehicular networks', 'authors': 'Jamal Toutouh, Enrique Alba', 'link': 'https://arxiv.org/abs/2501.09725', 'abstract': 'This article analyzes the use of two parallel multi-objective soft computing algorithms to automatically search for high-quality settings of the Ad hoc On Demand Vector routing protocol for vehicular networks. These methods are based on an evolutionary algorithm and on a swarm intelligence approach. The experimental analysis demonstrates that the configurations computed by our optimization algorithms outperform other state-of-the-art optimized ones. In turn, the computational efficiency achieved by all the parallel versions is greater than 87 %. Therefore, the line of work presented in this article represents an efficient framework to improve vehicular communications.', 'abstract_zh': '本文分析了使用两种并行多目标软计算算法自动搜索车载网络中按需矢量路由协议（Ad hoc On Demand Vector Routing, AODV）高质量配置的方法。这两种方法分别基于进化算法和群体智能方法。实验分析表明，由我们的优化算法计算出的配置优于其他最先进的优化配置。此外，所有并行版本所实现的计算效率均超过87%。因此，本文提出的研究方向代表了一个高效框架，用于提升车载通信性能。', 'title_zh': '智能车辆网络中并行多目标元启发式算法的应用'}
{'arxiv_id': 'arXiv:2501.09720', 'title': 'A Simple Aerial Detection Baseline of Multimodal Language Models', 'authors': 'Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang', 'link': 'https://arxiv.org/abs/2501.09720', 'abstract': 'The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at this https URL.', 'abstract_zh': '基于生成预训练Transformer的多模态语言模型（MLMs）被认为是统一各种领域和任务的强大候选者。专为遥感（RS）开发的MLMs在多个任务中表现出色，例如视觉问答和视觉定位。除了能够检测给定指令对应的具体对象的视觉定位任务外，能够检测多种类别的所有对象的空中检测也是一个有价值的挑战性任务，为RS基础模型提供服务。然而，现有的RS MLMs尚未探索空中检测任务，因为MLMs的自回归预测机制与检测输出存在显著差异。在本文中，我们首次提出了一个简单的基线模型LMMRotate，将MLMs应用于空中检测。具体而言，我们首先介绍了一种归一化方法，将检测输出转换为文本输出，使其与MLM框架兼容。然后，我们提出了一种评估方法，以确保MLMs和传统目标检测模型之间的公平比较。我们通过微调开源的通用MLM模型构建基线，并实现了可与传统检测器相媲美的检测性能。我们希望这个基线能够为未来MLM的发展提供参考，从而使MLM能够更好地理解RS图像的能力。代码可在此网址获得：this https URL。', 'title_zh': '一种基于多模态语言模型的简单航空目标检测基准方法'}
{'arxiv_id': 'arXiv:2501.09709', 'title': 'CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education', 'authors': 'Tianyu Wang, Nianjun Zhou, Zhixiong Chen', 'link': 'https://arxiv.org/abs/2501.09709', 'abstract': "Many non-traditional students in cybersecurity programs often lack access to advice from peers, family members and professors, which can hinder their educational experiences. Additionally, these students may not fully benefit from various LLM-powered AI assistants due to issues like content relevance, locality of advice, minimum expertise, and timing. This paper addresses these challenges by introducing an application designed to provide comprehensive support by answering questions related to knowledge, skills, and career preparation advice tailored to the needs of these students. We developed a learning tool platform, CyberMentor, to address the diverse needs and pain points of students majoring in cybersecurity. Powered by agentic workflow and Generative Large Language Models (LLMs), the platform leverages Retrieval-Augmented Generation (RAG) for accurate and contextually relevant information retrieval to achieve accessibility and personalization. We demonstrated its value in addressing knowledge requirements for cybersecurity education and for career marketability, in tackling skill requirements for analytical and programming assignments, and in delivering real time on demand learning support. Using three use scenarios, we showcased CyberMentor in facilitating knowledge acquisition and career preparation and providing seamless skill-based guidance and support. We also employed the LangChain prompt-based evaluation methodology to evaluate the platform's impact, confirming its strong performance in helpfulness, correctness, and completeness. These results underscore the system's ability to support students in developing practical cybersecurity skills while improving equity and sustainability within higher education. Furthermore, CyberMentor's open-source design allows for adaptation across other disciplines, fostering educational innovation and broadening its potential impact.", 'abstract_zh': '在网络安全项目中，许多非传统学生往往缺乏来自同学、家庭成员和导师的建议，这可能会阻碍他们的学习体验。此外，这些学生可能无法充分利用各种基于LLM的AI助手，原因包括内容相关性不足、建议的地域性限制、最低专业要求和时间性因素。本文通过引入一个旨在为这些学生提供全面支持的应用程序来解决这些问题。该应用能够回答与知识、技能和职业准备建议相关的问题，提供符合这些学生需求的定制化建议。我们开发了一个名为CyberMentor的学习工具平台，以满足网络安全专业学生多样化的需求和痛点。该平台借助代理型工作流程和生成型大规模语言模型（LLMs），利用检索增强生成（RAG）实现准确且上下文相关的信息检索，从而实现可访问性和个性化。我们通过网络安全教育的知识需求、职业市场可塑性、技能需求解决分析和编程作业、以及实时按需学习支持等方面的价值展示，证明了CyberMentor的有效性。我们通过三个使用场景展示了CyberMentor在促进知识获取、职业准备和提供无缝技能导向支持方面的应用。同时，我们采用LangChain基于提示的评估方法来评估平台的影响，确认了其在有用性、准确性、完整性的强大表现。这些结果强调了该系统的支持能力，能够在提高教育公平性和可持续性方面帮助学生发展实用的网络安全技能。此外，CyberMentor的开源设计使其能够在其他学科中进行调整，从而促进教育创新并扩大其潜在影响。', 'title_zh': 'CyberMentor：一种基于人工智能的学习工具平台，旨在满足网络安全教育中多样化学生需求'}
{'arxiv_id': 'arXiv:2501.09705', 'title': 'Practical Continual Forgetting for Pre-trained Vision Models', 'authors': 'Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang', 'link': 'https://arxiv.org/abs/2501.09705', 'abstract': 'For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. (iii) In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LoRA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes. Codes have been released on this https URL.', 'abstract_zh': '由于隐私和安全方面的考虑，从预训练的视觉模型中清除不需要的信息的需求越来越明显。在实际场景中，删除请求可能随时随地来自用户和模型所有者，这些请求通常会形成一个序列。因此，在这种情况下，需要以连续的方式从预训练模型中逐渐删除不必要的信息，同时保持其余部分的数据不变。我们把这个问题定义为持续遗忘，并识别出三个关键挑战。  \n(i) 对于不需要的知识，高效且有效的删除至关重要。  \n(ii) 对于剩余的知识，遗忘过程带来的影响应尽可能小。  \n(iii) 在实际场景中，遗忘过程中可用的训练样本可能稀缺或部分缺失。  \n为了解决这些问题，我们首先提出了分组稀疏LoRA (GS-LoRA)。具体来说，为了解决 (i)，我们引入了LoRA模块，单独对Transformer块中的每个遗忘任务进行细调FFN层，并通过采用 (ii) 的简单分组稀疏正则化，实现特定LoRA组的选择和其余组的置零。为了进一步将GS-LoRA扩展到更多实际应用场景，我们加入了原型信息作为额外监督，并引入了更为实用的方法，即GS-LoRA++。对于忘记的类别，我们将logits移离其原型；对于剩余的类别，我们使logits向相应的原型靠拢。我们在人脸认证、目标检测和图像分类等方面进行了广泛的实验，并证明我们的方法能够在对其他类别影响最小的情况下，成功遗忘特定类别。代码已发布在此 <https://> 地址。\n\n请注意，这里的网址显示为占位符，您可以根据实际情况填写具体的网址链接。', 'title_zh': '预训练视觉模型的实用持续遗忘方法'}
{'arxiv_id': 'arXiv:2501.09700', 'title': 'Cueless EEG imagined speech for subject identification: dataset and benchmarks', 'authors': 'Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee', 'link': 'https://arxiv.org/abs/2501.09700', 'abstract': 'Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).', 'abstract_zh': '脑电图（EEG）信号已成为生物特征识别的一种有前途的模态。虽然以往的研究已经探索了使用具有语义意义词语的想象语音进行个体识别的方法，但大多数研究依赖额外的视觉或听觉提示。本研究引入了一种无需额外提示的EEG基想象语音范式，被试在没有任何外部提示的情况下想象这些语义意义词语的发音。这一创新的方法通过要求被试从预先定义的列表中自然地选择和想象词语，克服了先前方法的局限性。数据集包括来自11名被试的跨越五次会话的超过4,350次试验。我们评估了多种分类方法，包括传统的机器学习技术，如支持向量机（SVM）和XGBoost，以及时间序列基础模型和专门为EEG分类设计的深度学习架构，如EEG Conformer和浅层卷积网络（Shallow ConvNet）。采用基于会话的留一验证策略以确保可靠的评估并防止数据泄漏。我们的结果显示了卓越的分类准确性，达到了97.93%。这些发现强调了无额外提示的EEG范式在实际应用中的潜在价值，如脑机接口（BCIs），可实现安全可靠的个体识别。', 'title_zh': '无指导的脑电图想象言语用于主体识别：数据集与基准'}
{'arxiv_id': 'arXiv:2501.09682', 'title': 'Incorporating Quantum Advantage in Quantum Circuit Generation through Genetic Programming', 'authors': 'Christoph Stein, Michael Färber', 'link': 'https://arxiv.org/abs/2501.09682', 'abstract': 'Designing efficient quantum circuits that leverage quantum advantage compared to classical computing has become increasingly critical. Genetic algorithms have shown potential in generating such circuits through artificial evolution. However, integrating quantum advantage into the fitness function of these algorithms remains unexplored. In this paper, we aim to enhance the efficiency of quantum circuit design by proposing two novel approaches for incorporating quantum advantage metrics into the fitness function of genetic algorithms.1 We evaluate our approaches based on the Bernstein-Vazirani Problem and the Unstructured Database Search Problem as test cases. The results demonstrate that our approaches not only improve the convergence speed of the genetic algorithm but also produce circuits comparable to expert-designed solutions. Our findings suggest that automated quantum circuit design using genetic algorithms that incorporate a measure of quantum advantage is a promising approach to accelerating the development of quantum algorithms.', 'abstract_zh': '利用量子优势超越经典计算的高效量子电路设计已成为愈发重要的研究内容。遗传算法在通过人工进化生成此类电路方面展现了潜力。然而，将量子优势整合到这些算法的适应度函数中仍未得到探索。本文旨在通过提出两种新颖的方法，将量子优势指标整合到遗传算法的适应度函数中，提高量子电路设计的效率。我们以伯恩斯坦-瓦齐里问题和无结构数据库搜索问题作为测试案例，评估我们的方法。结果表明，我们的方法不仅可以加速遗传算法的收敛速度，还能生成与专家设计的解决方案相当的电路。我们的研究发现表明，利用包含量子优势度量的遗传算法实现自动化量子电路设计可能是加速量子算法开发的一个有 promise 的方法。', 'title_zh': '通过遗传编程在量子电路生成中融入量子优势'}
{'arxiv_id': 'arXiv:2501.09674', 'title': 'Authenticated Delegation and Authorized AI Agents', 'authors': 'Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Deslandes Whitney, Dazza Greenwood, Alan Chan, Alex Pentland', 'link': 'https://arxiv.org/abs/2501.09674', 'abstract': 'The rapid deployment of autonomous AI agents creates urgent challenges around authorization, accountability, and access control in digital spaces. New standards are needed to know whom AI agents act on behalf of and guide their use appropriately, protecting online spaces while unlocking the value of task delegation to autonomous agents. We introduce a novel framework for authenticated, authorized, and auditable delegation of authority to AI agents, where human users can securely delegate and restrict the permissions and scope of agents while maintaining clear chains of accountability. This framework builds on existing identification and access management protocols, extending OAuth 2.0 and OpenID Connect with agent-specific credentials and metadata, maintaining compatibility with established authentication and web infrastructure. Further, we propose a framework for translating flexible, natural language permissions into auditable access control configurations, enabling robust scoping of AI agent capabilities across diverse interaction modalities. Taken together, this practical approach facilitates immediate deployment of AI agents while addressing key security and accountability concerns, working toward ensuring agentic AI systems perform only appropriate actions and providing a tool for digital service providers to enable AI agent interactions without risking harm from scalable interaction.', 'abstract_zh': '自主AI代理的快速部署在数字空间中引发了授权、问责和访问控制方面的紧迫挑战。为了了解AI代理代表谁行事，并适当指导其使用，同时保护在线空间并充分利用任务委派给自主代理的价值，需要新的标准。我们提出了一种新的框架，用于对AI代理进行身份验证、授权和审计委托，使人类用户能够安全地分配并限制代理的权限和范围，同时保持清晰的责任链条。该框架基于现有的身份验证和访问管理协议，扩展了OAuth 2.0和OpenID Connect，添加了针对代理特定的凭证和元数据，保持了与现有认证和网络基础设施的兼容性。此外，我们提出了一种框架，用于将灵活、自然语言的权限翻译成可审计的访问控制配置，从而使AI代理的能力能够在多种交互模式下进行有效的限定。综合起来，这一实用方法可促进AI代理的即时部署，同时解决关键的安全和问责问题，确保自主AI系统仅执行适当的操作，并为数字服务提供商提供工具，使其能够在不冒大规模交互风险的情况下实现AI代理的交互。', 'title_zh': '认证委托与授权人工智能代理'}
{'arxiv_id': 'arXiv:2501.09672', 'title': 'Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark', 'authors': 'Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish', 'link': 'https://arxiv.org/abs/2501.09672', 'abstract': 'The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.', 'abstract_zh': '过去几年中视觉-语言模型（VLMs）的快速发展需要严格的和全面的评估方法和基准。本文分析了现有的VLM评估技术，包括自动度量、基于AI的评估以及在多种任务上的手工评估。首先，我们介绍了Robin——一种新型的VLM套件，我们通过结合大规模语言模型（LLMs）和视觉编码器（VEs）构建，多尺度地使用Robin来识别当前评估方法在不同尺度上的不足。接下来，为了克服这些已识别的局限性，我们引入了CHIRP——一种新的长条目响应基准，旨在进行更加稳健和全面的VLM评估。我们免费提供了Robin的训练代码、模型套件和CHIRP基准，以促进可再现性并推动VLM研究的发展。', 'title_zh': 'Robin：多尺度视觉-语言模型套件及CHIRP评估基准'}
{'arxiv_id': 'arXiv:2501.09653', 'title': 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models', 'authors': 'Jonathan Katzy, Razvan Mihai Popescu, Arie van Deursen, Maliheh Izadi', 'link': 'https://arxiv.org/abs/2501.09653', 'abstract': 'The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.', 'abstract_zh': '近年来，大型语言模型的 popularity 升高促进了对它们进行训练所需的广泛代码数据集的发展。这导致可用于收集和用于研究特定行为或评估大型语言模型的代码资源变得有限，且容易受到数据污染的影响。为了解决这一问题，我们发布了《The Heap》，这是一个包含 57 种编程语言的大规模多语言数据集，该数据集在与其他开源代码数据集去重后发布，使研究人员能够公平地评估大型语言模型，而无需承担显著的数据清理开销。', 'title_zh': '堆：一种无污染的多语言代码数据集，用于评估大型语言模型'}
{'arxiv_id': 'arXiv:2501.09620', 'title': 'Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment', 'authors': 'Chaoqi Wang, Zhuokai Zhao, Yibo Jiang, Zhaorun Chen, Chen Zhu, Yuxin Chen, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hao Ma, Sinong Wang', 'link': 'https://arxiv.org/abs/2501.09620', 'abstract': "Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causal inference to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.", 'abstract_zh': '近年来，大型语言模型（LLMs）在执行复杂任务方面取得了显著进展。尽管基于人类反馈的强化学习（RLHF）在使LLMs与人类偏好对齐方面效果显著，但它在奖励建模中容易受到虚假相关性的影响。因此，它常常引入一些偏差，如长度偏差、奉承、概念偏差和歧视，这些偏差妨碍了模型捕捉真实因果关系的能力。为解决这一问题，我们提出了一种新的因果奖励建模方法，该方法结合因果推断以减轻这些虚假相关性的影响。我们的方法确保了反事实不变性，即在改变无关变量时，奖励预测保持一致。通过在合成数据集和真实世界数据集上的实验，我们展示了我们的方法有效地减轻了各种类型的虚假相关性，从而更可靠和公平地使LLMs与人类偏好对齐。作为一种针对现有RLHF工作流的即插即用增强方法，我们的因果奖励建模提供了提高LLM微调可信度和公平性的实用途径。', 'title_zh': '超越奖励作弊：大型语言模型对齐的因果奖励'}
{'arxiv_id': 'arXiv:2501.09608', 'title': 'Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning', 'authors': 'Donghuo Zeng, Kazushi Ikeda', 'link': 'https://arxiv.org/abs/2501.09608', 'abstract': 'Metric learning projects samples into an embedded space, where similarities and dissimilarities are quantified based on their learned representations. However, existing methods often rely on label-guided representation learning, where representations of different modalities, such as audio and visual data, are aligned based on annotated labels. This approach tends to underutilize latent complex features and potential relationships inherent in the distributions of audio and visual data that are not directly tied to the labels, resulting in suboptimal performance in audio-visual embedding learning. To address this issue, we propose a novel architecture that integrates cross-modal triplet loss with progressive self-distillation. Our method enhances representation learning by leveraging inherent distributions and dynamically refining soft audio-visual alignments -- probabilistic alignments between audio and visual data that capture the inherent relationships beyond explicit labels. Specifically, the model distills audio-visual distribution-based knowledge from annotated labels in a subset of each batch. This self-distilled knowledge is used t', 'abstract_zh': '度量学习将样本映射到一个嵌入空间，在该空间中，样本的相似性和差异性基于其学习到的表示进行量化。然而，现有的方法往往依赖于标签引导的表示学习，不同模态（如音频和视觉数据）的表示根据标注标签进行对齐。这种方法往往会忽略音频和视觉数据内在分布中那些未直接与标签相关联的潜在复杂特征和关系，从而在音频-视觉嵌入学习中导致性能不佳。为了解决这一问题，我们提出了一种新的架构，它将跨模态三元组损失与逐步自我蒸馏相结合。我们的方法通过利用内在分布并动态精炼软音频-视觉对齐来增强表示学习——这些对齐捕捉了超越显性标签的内在关系。具体而言，模型从每个批次的子集中的标注标签中提取基于分布的知识进行自我蒸馏。这些自我提取的知识被用来改进表示学习，从而增强音频-视觉对齐，并更好地捕捉模态之间的内在联系。', 'title_zh': '面向音频-视觉嵌入学习的渐进自蒸馏度量学习'}
{'arxiv_id': 'arXiv:2501.09605', 'title': 'Managed-Retention Memory: A New Class of Memory for the AI Era', 'authors': 'Sergey Legtchenko, Ioan Stefanovici, Richard Black, Antony Rowstron, Junyi Liu, Paolo Costa, Burcu Canakci, Dushyanth Narayanan, Xingbo Wu', 'link': 'https://arxiv.org/abs/2501.09605', 'abstract': 'AI clusters today are one of the major uses of High Bandwidth Memory (HBM). However, HBM is suboptimal for AI workloads for several reasons. Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant energy per bit overheads. It is also expensive, with lower yield than DRAM due to manufacturing complexity. We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads. We believe that MRM may finally provide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM). These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance. MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data retention and write performance for better potential performance on the metrics important for these workloads.', 'abstract_zh': '当今，高性能计算内存（High Bandwidth Memory，HBM）是AI集群的主要应用之一。然而，HBM在处理AI负载时并不理想，原因有多个方面。研究表明，HBM在写性能方面存在过度配置的情况，但在密度和读带宽方面则存在不足，并且每比特功耗较高。此外，HBM的成本也相当高昂，这主要是由于其制造复杂度高于动态随机存取存储器（DRAM），导致良率较低。为此，我们提出了一种新的内存类别：管理保留内存（Managed-Retention Memory，MRM），这种内存更优化地用于存储AI推断所需的键数据结构。我们认为，MRM可能会为最初旨在支持存储类内存（Storage Class Memory，SCM）的技术提供一条可行的道路。这些技术传统上提供了长时间的数据持久性（10年以上），但在输入输出性能和耐久性方面表现不佳。MRM通过不同的权衡取舍，结合对工作负载输入输出模式的理解，放弃长期数据保留和写入性能，以提高这些工作负载所关注的重要性能上的潜在表现。', 'title_zh': '管理保留内存：AI时代的一种新内存类别'}
{'arxiv_id': 'arXiv:2501.09597', 'title': 'Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology via Pretraining', 'authors': 'Nathan Vaska, Justin Goodwin, Robin Walters, Rajmonda S. Caceres', 'link': 'https://arxiv.org/abs/2501.09597', 'abstract': 'Meshes are used to represent complex objects in high fidelity physics simulators across a variety of domains, such as radar sensing and aerodynamics. There is growing interest in using neural networks to accelerate physics simulations, and also a growing body of work on applying neural networks directly to irregular mesh data. Since multiple mesh topologies can represent the same object, mesh augmentation is typically required to handle topological variation when training neural networks. Due to the sensitivity of physics simulators to small changes in mesh shape, it is challenging to use these augmentations when training neural network-based physics simulators. In this work, we show that variations in mesh topology can significantly reduce the performance of neural network simulators. We evaluate whether pretraining can be used to address this issue, and find that employing an established autoencoder pretraining technique with graph embedding models reduces the sensitivity of neural network simulators to variations in mesh topology. Finally, we highlight future research directions that may further reduce neural simulator sensitivity to mesh topology.', 'abstract_zh': '在雷达检测和空气动力学等多个领域，网格用于在高保真物理模拟器中表示复杂对象。近年来，人们越来越有兴趣利用神经网络加速物理模拟，同时也出现了一系列直接将神经网络应用于不规则网格数据的研究工作。由于多种网格拓扑结构可以表示相同的对象，因此在训练神经网络时需要进行网格增强以处理拓扑变化。由于物理模拟器对网格形状的小变化非常敏感，在训练神经网络为基础的物理模拟器时，很难使用这些增强技术。在本文中，我们展示了网格拓扑变化会显著降低神经网络模拟器的性能。我们评估了在训练过程中预训练的方法是否可以解决这一问题，并发现采用现有的自动编码器预训练技术和图嵌入模型可以降低神经网络模拟器对网格拓扑变化的敏感度。最后，我们指出了未来可能进一步降低神经网络模拟器对网格拓扑敏感性的研究方向。', 'title_zh': '通过预训练减少网格拓扑对神经物理模拟器灵敏度的影响'}
{'arxiv_id': 'arXiv:2501.09595', 'title': 'IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale derived from Instrumented Timed Up and Go test in stroke patients', 'authors': 'Simone Macciò, Alessandro Carfì, Alessio Capitanelli, Peppino Tropea, Massimo Corbo, Fulvio Mastrogiovanni, Michela Picardi', 'link': 'https://arxiv.org/abs/2501.09595', 'abstract': "Effective fall risk assessment is critical for post-stroke patients. The present study proposes a novel, data-informed fall risk assessment method based on the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility measures that traditional clinical scales fail to capture. IFRA, which stands for Instrumented Fall Risk Assessment, has been developed using a two-step process: first, features with the highest predictive power among those collected in a ITUG test have been identified using machine learning techniques; then, a strategy is proposed to stratify patients into low, medium, or high-risk strata. The dataset used in our analysis consists of 142 participants, out of which 93 were used for training (15 synthetically generated), 17 for validation and 32 to test the resulting IFRA scale (22 non-fallers and 10 fallers). Features considered in the IFRA scale include gait speed, vertical acceleration during sit-to-walk transition, and turning angular velocity, which align well with established literature on the risk of fall in neurological patients. In a comparison with traditional clinical scales such as the traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates competitive performance, being the only scale to correctly assign more than half of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004). Despite the dataset's limited size, this is the first proof-of-concept study to pave the way for future evidence regarding the use of IFRA tool for continuous patient monitoring and fall prevention both in clinical stroke rehabilitation and at home post-discharge.", 'abstract_zh': '有效的跌倒风险评估对于中风后患者的治疗至关重要。本研究提出了一种基于仪态定时起立行走测试（Instrumented Timed Up and Go，ITUG）数据的新型数据驱动跌倒风险评估方法，该方法引入了许多传统临床量表未能捕捉到的移动性指标。该跌倒风险评估方法（IFRA）采用了两步过程进行开发：首先，使用机器学习技术从ITUG测试中收集的特征中识别出预测能力最强的特征；其次，提出了将患者分为低、中、高风险等级的策略。在我们的分析中，使用的数据集包含142名参与者，其中93人用于训练（包括15个合成生成的参与者），17人用于验证，32人用于测试最终的IFRA评分（其中22人没有跌倒，10人跌倒）。在IFRA评分中考虑的特征包括行走速度、从坐到走过程中的垂直加速度以及转向角速度，这些特征与神经病学患者跌倒风险的现有文献高度一致。与传统的临床量表（如传统的定时起立行走测试和简式BESTest）相比，IFRA表现出竞争力，是唯一能够正确将超过一半的跌倒患者分配到高风险等级的量表（Fisher精确检验p = 0.004）。尽管数据集规模有限，但这是首次关于如何使用IFRA工具实现临床中风康复和出院后家庭连续患者监测及跌倒预防的研究概念验证，为进一步的研究提供了可能的证据。', 'title_zh': 'IFRA：基于机器学习的卒中患者带仪器的跌倒风险评估量表，源自带仪器的起立行走测试'}
{'arxiv_id': 'arXiv:2501.09571', 'title': 'MatrixNet: Learning over symmetry groups using learned group representations', 'authors': 'Lucas Laird, Circe Hsu, Asilata Bapat, Robin Walters', 'link': 'https://arxiv.org/abs/2501.09571', 'abstract': 'Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.', 'abstract_zh': '群论已在机器学习中被用于提供一种在从机器人技术到蛋白质建模等任务中集成已知对称变换的理论基础方法。在这些应用中，对称神经网络使用预定义的表示来学习几何输入数据。我们提出了一种名为MatrixNet的神经网络架构，它学习群元素输入的矩阵表示，而不是使用预定义的表示。MatrixNet在多个有限群和阿廷辫群的预测任务中，相对于多个标准基线模型，表现出更高的样本效率和泛化能力。此外，我们还展示了MatrixNet能够遵守群关系，从而使其能够泛化到训练集中未出现的具有更长词长的群元素。', 'title_zh': 'MatrixNet：使用学习到的群表示在对称群上进行学习'}
{'arxiv_id': 'arXiv:2501.09555', 'title': 'Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis', 'authors': 'Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy', 'link': 'https://arxiv.org/abs/2501.09555', 'abstract': 'Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.\nMethods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.\nResults: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.\nConclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in this https URL.', 'abstract_zh': '目的：手术工作流程分析对于提高手术效率和安全性至关重要。然而，先前的研究主要依赖大规模标注数据集，这带来了成本、可扩展性和对专家注解的依赖性挑战。为了解决这些问题，我们提出了Surg-FTDA（少样本文本驱动适应）方法，旨在通过 minimal 配对图像标签数据处理各种手术工作流程分析任务。\n方法：我们的方法有两个关键组成部分。首先，少样本选择驱动模态对齐（Few-shot selection-based modality alignment）选择一小部分图像，并对其嵌入与下游任务的文本嵌入进行对齐，从而缩小模态差距。其次，文本驱动适应（Text-driven adaptation）仅利用文本数据来训练解码器，从而消除配对图像-文本数据的需求。该解码器随后应用于对齐的图像嵌入，使得在没有显式图像-文本对的情况下能够执行图像相关的任务。\n结果：我们评估了该方法在生成任务（图像字幕）和辨别任务（三元组识别和阶段识别）中的性能。结果显示，Surg-FTDA 在基准模型上表现更优，并在下游任务中表现出良好的泛化能力。\n结论：我们提出了一种文本驱动的适应方法，该方法缓解了模态差距，能够在最小依赖大型标注数据集的情况下处理多种手术工作流程分析任务。代码和数据集将发布在 [此处提供链接]。\n\n注：请根据实际情况提供具体的链接。', 'title_zh': '基于文本的微调方法在少样本手术工作流程分析中的应用'}
{'arxiv_id': 'arXiv:2501.09525', 'title': 'Class Incremental Fault Diagnosis under Limited Fault Data via Supervised Contrastive Knowledge Distillation', 'authors': 'Hanrong Zhang, Yifei Yao, Zixuan Wang, Jiayuan Su, Mengxuan Li, Peng Peng, Hongwei Wang', 'link': 'https://arxiv.org/abs/2501.09525', 'abstract': "Class-incremental fault diagnosis requires a model to adapt to new fault classes while retaining previous knowledge. However, limited research exists for imbalanced and long-tailed data. Extracting discriminative features from few-shot fault data is challenging, and adding new fault classes often demands costly model retraining. Moreover, incremental training of existing methods risks catastrophic forgetting, and severe class imbalance can bias the model's decisions toward normal classes. To tackle these issues, we introduce a Supervised Contrastive knowledge distiLlation for class Incremental Fault Diagnosis (SCLIFD) framework proposing supervised contrastive knowledge distillation for improved representation learning capability and less forgetting, a novel prioritized exemplar selection method for sample replay to alleviate catastrophic forgetting, and the Random Forest Classifier to address the class imbalance. Extensive experimentation on simulated and real-world industrial datasets across various imbalance ratios demonstrates the superiority of SCLIFD over existing approaches. Our code can be found at this https URL.", 'abstract_zh': '面向新类别的增量故障诊断需要模型能够在适应新故障类别同时保留先前知识。然而，现有的研究对于不平衡和长尾数据较少。从少量的故障数据中提取有区别的特征具有挑战性，而添加新的故障类别通常需要昂贵的模型重新训练。此外，现有方法的增量训练存在灾难性遗忘的风险，严重的类别不平衡也会使模型倾向于正常类别的决策。为解决这些问题，我们提出了一种监督对比知识蒸馏框架（SCLIFD）用于增量故障诊断，该框架通过监督对比知识蒸馏提高表示学习能力并减少遗忘，引入了一种新颖的优先示例选择方法以缓解灾难性遗忘，并采用随机森林分类器解决类别不平衡问题。在各种不平衡比率的模拟和实际工业数据集上的广泛实验表明，SCLIFD相比现有方法具有优势。我们的代码可在以下链接找到：[粘贴或提供链接]。', 'title_zh': '通过监督对比知识蒸馏进行有限故障数据下的类别增量故障诊断'}
{'arxiv_id': 'arXiv:2501.09481', 'title': 'MonoSOWA: Scalable monocular 3D Object detector Without human Annotations', 'authors': 'Jan Skvrna, Lukas Neumann', 'link': 'https://arxiv.org/abs/2501.09481', 'abstract': 'Detecting the three-dimensional position and orientation of objects using a single RGB camera is a foundational task in computer vision with many important applications. Traditionally, 3D object detection methods are trained in a fully-supervised setup, requiring vast amounts of human annotations, which are laborious, costly, and do not scale well with the ever-increasing amounts of data being captured.\nIn this paper, we present the first method to train 3D object detectors for monocular RGB cameras without domain-specific human annotations, thus making orders of magnitude more data available for training. Thanks to newly proposed Canonical Object Space, the method can not only exploit data across a variety of datasets and camera setups to train a single 3D detector, but unlike previous work it also works out of the box in previously unseen camera setups. All this is crucial for practical applications, where the data and cameras are extremely heterogeneous.\nThe method is evaluated on two standard autonomous driving datasets, where it outperforms previous works, which, unlike our method, still rely on 2D human annotations.', 'abstract_zh': '使用单个RGB相机检测物体的三维位置和姿态是计算机视觉中的一个基础任务，具有许多重要应用。传统上，三维物体检测方法是在完全监督的设置下训练的，需要大量的人工注释，这既耗时又昂贵，而且不适宜处理不断增长的数据量。\n\n在这篇论文中，我们提出了第一个无需特定领域人工注释即可训练单目RGB相机三维物体检测器的方法，从而使得训练数据量大幅增加。得益于新提出的标准物体空间，该方法不仅可以利用多种数据集和相机设置下的数据来训练一个单一的三维检测器，而且还能在未见过的相机设置下直接应用，这在实际应用中至关重要，因为数据和相机往往是异构的。\n\n该方法在两个标准的自动驾驶数据集上进行了评估，在这些数据集上，它超过了之前依赖于二维人工注释的工作。', 'title_zh': 'MonoSOWA：无需人工标注的可扩展单目3D对象检测器'}
{'arxiv_id': 'arXiv:2501.09469', 'title': 'Predicting Air Temperature from Volumetric Urban Morphology with Machine Learning', 'authors': 'Berk Kıvılcım, Patrick Erik Bradley', 'link': 'https://arxiv.org/abs/2501.09469', 'abstract': 'In this study, we firstly introduce a method that converts CityGML data into voxels which works efficiently and fast in high resolution for large scale datasets such as cities but by sacrificing some building details to overcome the limitations of previous voxelization methodologies that have been computationally intensive and inefficient at transforming large-scale urban areas into voxel representations for high resolution. Those voxelized 3D city data from multiple cities and corresponding air temperature data are used to develop a machine learning model. Before the model training, Gaussian blurring is implemented on input data to consider spatial relationships, as a result the correlation rate between air temperature and volumetric building morphology is also increased after the Gaussian blurring. After the model training, the prediction results are not just evaluated with Mean Square Error (MSE) but some image similarity metrics such as Structural Similarity Index Measure (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) that are able to detect and consider spatial relations during the evaluation process. This trained model is capable of predicting the spatial distribution of air temperature by using building volume information of corresponding pixel as input. By doing so, this research aims to assist urban planners in incorporating environmental parameters into their planning strategies, thereby facilitating more sustainable and inhabitable urban environments.', 'abstract_zh': '在本研究中，我们首先介绍了一种将 CityGML 数据转换为体素的方法，该方法在对大规模数据集（如城市）进行高分辨率处理时能高效且快速地工作，但会牺牲一些建筑细节，以克服之前体素化方法计算密集且低效的局限性。这些多城市体素化3D城市数据及其相应的气温数据被用于开发机器学习模型。在模型训练之前，我们对输入数据进行了高斯模糊处理，以考虑空间关系，结果表明，经过高斯模糊处理后，空气温度与体素化建筑形态间的相关度也得到了提高。模型训练完成后，预测结果不仅通过均方误差（MSE）进行评估，还使用了一些考虑空间关系的图像相似度指标，如结构相似性索引测量（SSIM）和学习感知图像块相似性（LPIPS），这些指标能够在此过程中检测和考虑空间关系。通过使用该训练好的模型，我们可以利用相应像素的建筑体积信息来预测空气温度的空间分布。本研究旨在帮助城市规划者在其规划策略中考虑环境参数，从而促进更加可持续和宜居的城市环境。', 'title_zh': '基于机器学习的体积城市形态对空气温度的预测'}
{'arxiv_id': 'arXiv:2501.09465', 'title': 'RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection', 'authors': 'Jianrui Shi, Yong Zhao, Zeyang Cui, Xiaoming Shen, Minhang Zeng, Xiaojie Liu', 'link': 'https://arxiv.org/abs/2501.09465', 'abstract': 'Object detection plays a crucial role in smart video analysis, with applications ranging from autonomous driving and security to smart cities. However, achieving real-time object detection on edge devices presents significant challenges due to their limited computational resources and the high demands of deep neural network (DNN)-based detection models, particularly when processing high-resolution video. Conventional strategies, such as input down-sampling and network up-scaling, often compromise detection accuracy for faster performance or lead to higher inference latency. To address these issues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven Partitioning and Edge Offloading framework designed to optimize the accuracy-latency trade-off in resource-constrained edge environments. Our approach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that partitions video frames into non-uniform blocks based on object distribution and the computational characteristics of DNNs. Furthermore, a parallel edge offloading scheme is implemented to distribute these blocks across multiple edge servers for concurrent processing. Experimental evaluations show that RE-POSE significantly enhances detection accuracy and reduces inference latency, surpassing existing methods.', 'abstract_zh': '对象检测在智能视频分析中发挥着关键作用，其应用范围从自动驾驶和安全到智慧城市。然而，由于边缘设备的计算资源有限以及基于深度神经网络(DNN)的对象检测模型对计算资源的高需求，实现实时对象检测面临着重大挑战，特别是在处理高分辨率视频时。传统策略，如输入下采样和网络放大，通常会以牺牲检测精度为代价来提高性能速度，或者导致更高的推断延迟。为了解决这些问题，本文提出了一种强化学习（Reinforcement Learning，RL）驱动的分割和边缘卸载框架——RE-POSE，旨在优化资源受限边缘环境中的精度-延迟权衡。我们的方法包括一个基于强化学习的动态聚类算法（RL-Based Dynamic Clustering Algorithm，RL-DCA），该算法根据物体分布和DNN的计算特性将视频帧分割为非均匀块。此外，我们还实现了一种并行的边缘卸载方案，将这些块分配到多个边缘服务器进行并发处理。实验评估表明，RE-POSE 显著提高了检测精度并减少了推断延迟，优于现有方法。', 'title_zh': 'RE-POSE：结合强化学习驱动的划分与卸载以提高边缘对象检测效率'}
{'arxiv_id': 'arXiv:2501.09444', 'title': 'Solving the unsolvable: Translating case law in Hong Kong', 'authors': 'King-kui Sin, Xi Xuan, Chunyu Kit, Clara Ho-yan Chan, Honic Ho-kin Ip', 'link': 'https://arxiv.org/abs/2501.09444', 'abstract': "This paper addresses the challenges translating case law under Hong Kong's bilingual legal system. It highlights the initial success of translating all written statutes into Chinese before the 1997 handover, a task mandated by the Basic Law. The effort involved significant collaboration among legal, linguistic, and translation experts, resulting in a comprehensive and culturally appropriate bilingual legal system. However, translating case law remains a significant challenge due to the sheer volume and continuous growth of judicial decisions. The paper critiques the governments and judiciarys sporadic and uncoordinated efforts to translate case law, contrasting it with the thorough approach previously taken for statute translation. Although the government acknowledges the importance of legal bilingualism, it lacks a sustainable strategy for translating case law. The Judiciarys position that translating all judgments is unnecessary, unrealistic, and not cost-effectiveis analyzed and critiqued for its impact on legal transparency and public trust. A proposed solution involves leveraging machine translation technology through a human-machine interactive translation platform, which undergoes two major transitions. Initially based on a neural model, the platform transitions to using a large language model for improved translation accuracy. Furthermore, it evolves from a single-agent system to a multi-agent system, incorporating Translator, Annotator, and Proofreader agents. This multi-agent approach, supported by a grant, aims to facilitate efficient, high-quality translation of judicial judgments by integrating advanced artificial intelligence and continuous feedback mechanisms, thus better meeting the needs of a bilingual legal system.", 'abstract_zh': '本文探讨了在港英文双语法律体系下进行案例法翻译所面临的挑战。在1997年主权移交前，所有书面法例均已成功翻译成中文，这是根据基本法的要求完成的任务。这项工作涉及法律、语言和翻译专家的密切合作，形成了全面且文化适应性的双语法律体系。然而，由于司法判决的数量庞大且持续增长，翻译案例法仍然是一项重大挑战。文章批评了政府和司法机构在翻译案例法方面反复无常且缺乏协调的努力，与早期对法例翻译的全面方法形成了对比。尽管政府承认法律双语化的重要性，但仍缺乏一种可持续的案例法翻译策略。司法机构认为，将所有判决翻译成中文是不必要的、不现实且不经济的做法，这一观点被分析并批评，认为这将对司法透明度和公众信任产生负面影响。本文提出的一项解决方案是通过人机交互翻译平台利用机器翻译技术，该平台经历了两大转变。最初基于神经模型，平台转向使用大型语言模型以提高翻译准确性。此外，它从单个代理系统演变为多代理系统，加入了翻译代理、注释代理和审校代理。在资助下，这种多代理方法旨在通过整合先进的人工智能技术和持续反馈机制，促进高质量的司法判决翻译，更好地满足双语法律体系的需求。', 'title_zh': '解决不可解之题：香港判例法的翻译'}
{'arxiv_id': 'arXiv:2501.09429', 'title': 'ADAGE: A generic two-layer framework for adaptive agent based modelling', 'authors': 'Benjamin Patrick Evans, Sihan Zeng, Sumitra Ganesh, Leo Ardon', 'link': 'https://arxiv.org/abs/2501.09429', 'abstract': 'Agent-based models (ABMs) are valuable for modelling complex, potentially out-of-equilibria scenarios. However, ABMs have long suffered from the Lucas critique, stating that agent behaviour should adapt to environmental changes. Furthermore, the environment itself often adapts to these behavioural changes, creating a complex bi-level adaptation problem. Recent progress integrating multi-agent reinforcement learning into ABMs introduces adaptive agent behaviour, beginning to address the first part of this critique, however, the approaches are still relatively ad hoc, lacking a general formulation, and furthermore, do not tackle the second aspect of simultaneously adapting environmental level characteristics in addition to the agent behaviours. In this work, we develop a generic two-layer framework for ADaptive AGEnt based modelling (ADAGE) for addressing these problems. This framework formalises the bi-level problem as a Stackelberg game with conditional behavioural policies, providing a consolidated framework for adaptive agent-based modelling based on solving a coupled set of non-linear equations. We demonstrate how this generic approach encapsulates several common (previously viewed as distinct) ABM tasks, such as policy design, calibration, scenario generation, and robust behavioural learning under one unified framework. We provide example simulations on multiple complex economic and financial environments, showing the strength of the novel framework under these canonical settings, addressing long-standing critiques of traditional ABMs.', 'abstract_zh': '基于代理的模型（Agent-Based Models，ABMs）在模拟复杂、可能偏离均衡的场景方面具有重要价值。然而，ABMs长期以来一直受到卢卡斯批判的影响，该批判指出代理行为应适应环境变化。此外，环境本身也往往会对这些行为变化作出调整，从而产生一个复杂的双向适应问题。近期通过将多代理强化学习整合到ABMs中取得了进展，引进入适应性代理行为，开始解决了这一批判的第一部分。然而，目前的方法仍相对粗糙，缺乏通用形式，并且不解决环境特征需要同时适应的问题。在本文中，我们开发了一个通用的双层框架，用于解决这些问题，该框架称为ADaptive AGEnt based modelling（ADAGE）。该框架将双向问题形式化为条件行为策略的斯塔克尔贝格博弈，提供了一个基于求解非线性方程组合的统一框架，用于适应性代理建模。我们展示了这种通用方法如何将几个常见的（曾视为独立的）ABM任务，如政策设计、参数校准、情景生成以及在统一框架下的鲁棒行为学习，统一起来。我们提供了在多种复杂经济和金融环境中的示例模拟，证明了该新型框架在这些经典设置下的优势，并解决了传统ABMs长期存在的批判。', 'title_zh': 'ADAGE：一种通用的两层框架，用于自适应代理基于的建模'}
{'arxiv_id': 'arXiv:2501.09420', 'title': 'Dynamic Neural Style Transfer for Artistic Image Generation using VGG19', 'authors': 'Kapil Kashyap, Mehak Garg, Sean Fargose, Sindhu Nair', 'link': 'https://arxiv.org/abs/2501.09420', 'abstract': 'Throughout history, humans have created remark- able works of art, but artificial intelligence has only recently started to make strides in generating visually compelling art. Breakthroughs in the past few years have focused on using convolutional neural networks (CNNs) to separate and manipulate the content and style of images, applying texture synthesis techniques. Nevertheless, a number of current techniques continue to encounter obstacles, including lengthy processing times, restricted choices of style images, and the inability to modify the weight ratio of styles. We proposed a neural style transfer system that can add various artistic styles to a desired image to address these constraints allowing flexible adjustments to style weight ratios and reducing processing time. The system uses the VGG19 model for feature extraction, ensuring high-quality, flexible stylization without compromising content integrity.', 'abstract_zh': '自古以来，人类创造了无数杰出的艺术作品，而人工智能在生成视觉上引人注目的艺术方面直到最近才开始取得进展。过去几年中的突破重点在于使用卷积神经网络（CNN）来分离和操控图像的内容和风格，并应用纹理合成技术。然而，当前许多方法仍然面临着诸多障碍，如处理时间长、可选风格图像限制和无法修改风格权重比。我们提出了一种神经风格迁移系统，可以将多种艺术风格添加到目标图像中，以解决这些约束问题，允许灵活调整风格权重比并减少处理时间。该系统使用VGG19模型进行特征提取，确保在不牺牲内容完整性的情况下实现高质量、灵活的艺术风格化。', 'title_zh': '使用VGG19的动态神经风格迁移生成艺术图像'}
{'arxiv_id': 'arXiv:2501.09410', 'title': 'MoE$^2$: Optimizing Collaborative Inference for Edge Large Language Models', 'authors': 'Lyudong Jin, Yanning Zhang, Yanhan Li, Shurong Wang, Howard H. Yang, Jian Wu, Meng Zhang', 'link': 'https://arxiv.org/abs/2501.09410', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. Exploiting the heterogeneous capabilities of edge LLMs is crucial for diverse emerging applications, as it enables greater cost-effectiveness and reduced latency. In this work, we introduce \\textit{Mixture-of-Edge-Experts (MoE$^2$)}, a novel collaborative inference framework for edge LLMs. We formulate the joint gating and expert selection problem to optimize inference performance under energy and latency constraints. Unlike conventional MoE problems, LLM expert selection is significantly more challenging due to the combinatorial nature and the heterogeneity of edge LLMs across various attributes. To this end, we propose a two-level expert selection mechanism through which we uncover an optimality-preserving property of gating parameters across expert selections. This property enables the decomposition of the training and selection processes, significantly reducing complexity. Furthermore, we leverage the objective's monotonicity and design a discrete monotonic optimization algorithm for optimal expert selection. We implement edge servers with NVIDIA Jetson AGX Orins and NVIDIA RTX 4090 GPUs, and perform extensive experiments. Our results validate that performance improvements of various LLM models and show that our MoE$^2$ method can achieve optimal trade-offs among different delay and energy budgets, and outperforms baselines under various system resource constraints.", 'abstract_zh': '大规模语言模型（LLMs）在广泛自然语言处理任务中展现了显著的能力。充分利用边缘端LLMs的异质能力对于多种新兴应用至关重要，因为它能提高成本效益并减少延迟。在这项工作中，我们介绍了**边缘端专家混合（MoE²）**，这是一种新颖的边缘端LLMs协作推理框架。我们提出了联合门控和专家选择问题的建模方法，以在能量和延迟约束下优化推理性能。与传统的MoE问题不同，由于边缘端LLMs在各个属性上的组合特性和异质性，LLM专家选择具有显著的挑战性。为了解决这一问题，我们提出了一种两层专家选择机制，通过该机制揭示了门控参数在不同专家选择下的保持最优性属性。这一属性使得训练和选择过程得以分解，显著降低了复杂度。此外，我们利用目标函数的单调性并设计了离散单调优化算法来进行最优专家选择。我们使用NVIDIA Jetson AGX Orin和NVIDIA RTX 4090 GPU构建边缘服务器，并进行了广泛实验。实验结果证明了各种LLM模型性能的提升，并表明我们的MoE²方法能够在不同的延迟和能量预算下实现最优权衡，并在各种系统资源约束下优于基准方法。', 'title_zh': 'MoE\\(^2\\): 优化边缘大型语言模型协作推理'}
{'arxiv_id': 'arXiv:2501.09395', 'title': 'ELM-DeepONets: Backpropagation-Free Training of Deep Operator Networks via Extreme Learning Machines', 'authors': 'Hwijae Son', 'link': 'https://arxiv.org/abs/2501.09395', 'abstract': 'Deep Operator Networks (DeepONets) are among the most prominent frameworks for operator learning, grounded in the universal approximation theorem for operators. However, training DeepONets typically requires significant computational resources. To address this limitation, we propose ELM-DeepONets, an Extreme Learning Machine (ELM) framework for DeepONets that leverages the backpropagation-free nature of ELM. By reformulating DeepONet training as a least-squares problem for newly introduced parameters, the ELM-DeepONet approach significantly reduces training complexity. Validation on benchmark problems, including nonlinear ODEs and PDEs, demonstrates that the proposed method not only achieves superior accuracy but also drastically reduces computational costs. This work offers a scalable and efficient alternative for operator learning in scientific computing.', 'abstract_zh': '深度操作网络（DeepONets）是操作学习中最突出的框架之一，基于操作的通用逼近定理。然而，训练DeepONets通常需要大量的计算资源。为了解决这一限制，我们提出了一种基于极端学习机（ELM）的ELM-DeepONets框架，利用ELM的反向传播特性。通过将DeepONet的训练重新表述为新引入参数的最小二乘问题，ELM-DeepONet方法显著降低了训练复杂度。在包括非线性微分方程（ODEs）和偏微分方程（PDEs）的基准问题上的验证表明，所提出的方法不仅实现了更高的准确性，而且还大幅减少了计算成本。这项工作为科学计算中的操作学习提供了可扩展且高效的替代方案。', 'title_zh': 'ELM-DeepONets：通过极端学习机实现的无反向传播深度算子网络的训练'}
{'arxiv_id': 'arXiv:2501.09394', 'title': 'Quantum-Enhanced Transformers for Robust Acoustic Scene Classification in IoT Environments', 'authors': 'Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana', 'link': 'https://arxiv.org/abs/2501.09394', 'abstract': 'The proliferation of Internet of Things (IoT) devices equipped with acoustic sensors necessitates robust acoustic scene classification (ASC) capabilities, even in noisy and data-limited environments. Traditional machine learning methods often struggle to generalize effectively under such conditions. To address this, we introduce Q-ASC, a novel Quantum-Inspired Acoustic Scene Classifier that leverages the power of quantum-inspired transformers. By integrating quantum concepts like superposition and entanglement, Q-ASC achieves superior feature learning and enhanced noise resilience compared to classical models. Furthermore, we introduce a Quantum Variational Autoencoder (QVAE) based data augmentation technique to mitigate the challenge of limited labeled data in IoT deployments. Extensive evaluations on the Tampere University of Technology (TUT) Acoustic Scenes 2016 benchmark dataset demonstrate that Q-ASC achieves remarkable accuracy between 68.3% and 88.5% under challenging conditions, outperforming state-of-the-art methods by over 5% in the best case. This research paves the way for deploying intelligent acoustic sensing in IoT networks, with potential applications in smart homes, industrial monitoring, and environmental surveillance, even in adverse acoustic environments.', 'abstract_zh': '互联网-of-事物（IoT）设备的普及，尤其是装备了声学传感器的设备，对声学场景分类（ASC）能力提出了新的要求，尤其是在嘈杂和数据有限的环境中。传统机器学习方法往往在这些条件下难以有效泛化。为了解决这一问题，我们引入了一种名为Q-ASC的新颖量子启发声学场景分类器，该分类器利用了量子启发变换器的强大功能。通过整合量子概念如叠加和纠缠，Q-ASC在特征学习和噪声鲁棒性方面优于经典模型。此外，我们提出了基于量子变分自编码器（QVAE）的数据增强技术，以应对物联网部署中有限的标注数据挑战。在坦佩雷大学（TUT）2016年声学场景基准数据集上的广泛评估表明，Q-ASC在挑战性条件下实现了68.3%至88.5%的显著准确率，最佳情况下比最先进的方法高出超过5%。这项研究为在物联网网络中部署智能声学传感铺平了道路，有可能应用于智能家居、工业监测和环境监控，甚至在不良声学环境中也适用。', 'title_zh': '物联网环境中增强的量子Transformer模型在鲁棒声场景分类中的应用'}
{'arxiv_id': 'arXiv:2501.09354', 'title': 'Style4Rec: Enhancing Transformer-based E-commerce Recommendation Systems with Style and Shopping Cart Information', 'authors': 'Berke Ugurlu, Ming-Yi Hong, Che Lin', 'link': 'https://arxiv.org/abs/2501.09354', 'abstract': "Understanding users' product preferences is essential to the efficacy of a recommendation system. Precision marketing leverages users' historical data to discern these preferences and recommends products that align with them. However, recent browsing and purchase records might better reflect current purchasing inclinations. Transformer-based recommendation systems have made strides in sequential recommendation tasks, but they often fall short in utilizing product image style information and shopping cart data effectively. In light of this, we propose Style4Rec, a transformer-based e-commerce recommendation system that harnesses style and shopping cart information to enhance existing transformer-based sequential product recommendation systems. Style4Rec represents a significant step forward in personalized e-commerce recommendations, outperforming benchmarks across various evaluation metrics. Style4Rec resulted in notable improvements: HR@5 increased from 0.681 to 0.735, NDCG@5 increased from 0.594 to 0.674, and MRR@5 increased from 0.559 to 0.654. We tested our model using an e-commerce dataset from our partnering company and found that it exceeded established transformer-based sequential recommendation benchmarks across various evaluation metrics. Thus, Style4Rec presents a significant step forward in personalized e-commerce recommendation systems.", 'abstract_zh': '理解用户的產品偏好對於推薦系統的效果至關重要。精准营销利用用戶歷史數據來識別這些偏好，並推薦與其偏好相符的產品。然而，最近的瀏覽和購買記錄可能更好地反映當前的購物傾向。基于变压器的推薦系統在序列表現任務方面取得了一定進展，但通常在有效地利用產品圖像風格信息和購物車數據方面存在不足。基於此，我們提出了Style4Rec，一種結合風格和購物車信息的基于变压器的電商推薦系統，旨在改進現有的基于变压器的序列表現產品推薦系統。Style4Rec 在個性化電商推薦方面是一個重要的進步，其表現超越了多種評估指標下的基準。Style4Rec 實現了顯著的改進：HR@5 從 0.681 提高到 0.735，NDCG@5 從 0.594 提高到 0.674，MRR@5 從 0.559 提高到 0.654。我們使用合作伙伴公司的電商數據集對該模型進行測試，並發現其在多種評估指標下超過了現有的基于变压器的序列表現推薦基準。因此，Style4Rec 代表了一個重要的進步，推動了個性化電商推薦系統的发展。', 'title_zh': 'Style4Rec：通过风格和购物车信息增强基于Transformer的电商推荐系统'}
{'arxiv_id': 'arXiv:2501.09345', 'title': 'Rational Tuning of LLM Cascades via Probabilistic Modeling', 'authors': 'Michael J. Zellinger, Matt Thomson', 'link': 'https://arxiv.org/abs/2501.09345', 'abstract': "Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using grid search, our parametric Markov-copula model significantly improves runtime scaling with respect to the length of the cascade and the desired resolution of the cost-error curve, turning them from intractable into low-order polynomial. In addition, the optimal thresholds computed using our continuous optimization-based algorithm increasingly outperform those found via grid search as cascade length grows, improving the area under the cost-error curve by 1.9% on average for cascades consisting of at least three models. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing LLM systems.", 'abstract_zh': '大型语言模型（LLMs）的可靠性近年来得到了广泛关注。鉴于LLMs具有生成幻觉的倾向，以及其对提示设计的高度敏感性，预测单个LLM的性能已经颇具挑战性。而在复合LLM系统，如级联系统中，问题变得更加复杂，我们需要不仅考虑每个模型的独立性能，还要理解不同模型错误率之间的相互作用。在这篇论文中，我们提出了一种概率模型来描述LLM序列的联合性能分布，该模型可以作为一种框架来有理由地使用连续优化调整LLM级联的置信阈值。与使用网格搜索选择置信阈值相比，我们的参数马尔可夫-柯皮亚模型在级联长度和所需的成本-错误曲线分辨率方面显著改善了运行时扩展性，使它们从不可解变成低次多项式。另外，基于连续优化算法计算的最佳阈值在级联长度增长时逐渐优于网格搜索找到的阈值，在长度至少为三个模型的级联中，平均提高了成本-错误曲线下的面积1.9%。总体而言，我们的马尔可夫-柯皮亚模型为调整LLM级联性能提供了一个理性的基础，并指出概率方法在分析LLM系统方面具有潜在优势。', 'title_zh': '通过概率建模对大规模语言模型级联进行合理的调优'}
{'arxiv_id': 'arXiv:2501.09333', 'title': 'Prompt-CAM: A Simpler Interpretable Transformer for Fine-Grained Analysis', 'authors': 'Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, Jianyang Gu, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G. Campolongo, Daniel Rubenstein, Charles V. Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao', 'link': 'https://arxiv.org/abs/2501.09333', 'abstract': "We present a simple usage of pre-trained Vision Transformers (ViTs) for fine-grained analysis, aiming to identify and localize the traits that distinguish visually similar categories, such as different bird species or dog breeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to extract localized, informative features. However, using saliency maps like Grad-CAM can hardly point out the traits: they often locate the whole object by a blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class Attention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific prompts to a pre-trained ViT and uses the corresponding outputs for classification. To classify an image correctly, the true-class prompt must attend to the unique image patches not seen in other classes' images, i.e., traits. As such, the true class's multi-head attention maps reveal traits and their locations. Implementation-wise, Prompt-CAM is almost a free lunch by simply modifying the prediction head of Visual Prompt Tuning (VPT). This makes Prompt-CAM fairly easy to train and apply, sharply contrasting other interpretable methods that design specific models and training processes. It is even simpler than the recently published INterpretable TRansformer (INTR), whose encoder-decoder architecture prevents it from leveraging pre-trained ViTs. Extensive empirical studies on a dozen datasets from various domains (e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate Prompt-CAM superior interpretation capability.", 'abstract_zh': '我们提出了一种简单使用预训练视觉变压器（ViTs）的方法，旨在对细粒度分析进行识别和定位，以区分视觉上相似的类别，例如不同种类的鸟类或狗的品种。预训练的ViTs，如DINO，展现了提取局部化、信息性特征的显著能力。然而，使用梯度CAM等显著性图往往难以指出这些特征：它们常通过模糊、粗略的热力图定位整个对象，而不是特定的特征。为此，我们提出了一种名为Prompt类注意力图（Prompt-CAM）的新方法。Prompt-CAM通过训练预训练ViT学习特定类别的提示，并利用相应的输出进行分类。为了正确分类一幅图像，真正的类提示必须关注在其他类图像中未见过的独特图像斑块，即特征。因此，真类的多头注意力图揭示了这些特征及其位置。\n\n从实现角度来看，只需简单修改视觉提示调优（VPT）的预测头，Prompt-CAM几乎可以免费获得。这使得Prompt-CAM相当容易训练和应用，与设计特定模型和训练过程的可解释方法相比形成鲜明对比。相比之下，最近发表的INterpretable TRansformer（INTR）因其编码器-解码器架构无法充分利用预训练ViTs。在多个领域（如鸟类、鱼类、昆虫、真菌、花卉、食物和汽车）的十余个数据集上的广泛实证研究表明，Prompt-CAM具有优越的解释能力。', 'title_zh': 'Prompt-CAM：一种更简洁可解释的变压器模型用于细粒度分析'}
{'arxiv_id': 'arXiv:2501.09328', 'title': 'Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks', 'authors': 'Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, Zhihong Tian', 'link': 'https://arxiv.org/abs/2501.09328', 'abstract': "Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks.\nIn this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from $12,000$ to $200$ with zero training cost.", 'abstract_zh': '开发高性能深度学习模型耗资巨大，因此模型所有者更倾向于使用机器学习即服务（MLaaS）平台，而不会公开发布其模型。然而，恶意用户可能会利用查询接口来执行模型提取攻击，从而使目标模型的功能在本地被重构。尽管先前的研究已经调查了可用于主张所有权的可触发水印技术，但现有的方法仍面临重大挑战：（1）大多数方法需要额外的训练，导致高开销和较低的灵活性；（2）它们往往未能考虑到高级别攻击者，使模型容易受到适应性攻击。\n\n在本文中，我们提出了一种名为Neural Honeytrace的稳健即插即用水印框架，用于抵御模型提取攻击。我们首先从信息论的角度制定了一个水印传输模型，为现有的可触发水印提供了可解释的原理和局限性概述。受该模型的指引，我们进一步引入了以下方法：（1）一种基于相似性的无需训练的水印方法，以实现即插即用和灵活的水印传输；（2）一种基于分布的多步水印信息传输策略，以实现稳健的水印。在四个数据集上的全面实验表明，Neural Honeytrace在效率和抵御适应性攻击方面优于先前的方法。Neural Honeytrace将最坏情况下的t-检验版权主张所需的样本平均数量从12,000减少到200，且无需训练成本。', 'title_zh': '神经蜂蜜追踪：一种针对模型提取攻击的稳健即插即用水印框架'}
{'arxiv_id': 'arXiv:2501.09327', 'title': 'On Learning Informative Trajectory Embeddings for Imitation, Classification and Regression', 'authors': 'Zichang Ge, Changyu Chen, Arunesh Sinha, Pradeep Varakantham', 'link': 'https://arxiv.org/abs/2501.09327', 'abstract': 'In real-world sequential decision making tasks like autonomous driving, robotics, and healthcare, learning from observed state-action trajectories is critical for tasks like imitation, classification, and clustering. For example, self-driving cars must replicate human driving behaviors, while robots and healthcare systems benefit from modeling decision sequences, whether or not they come from expert data. Existing trajectory encoding methods often focus on specific tasks or rely on reward signals, limiting their ability to generalize across domains and tasks. Inspired by the success of embedding models like CLIP and BERT in static domains, we propose a novel method for embedding state-action trajectories into a latent space that captures the skills and competencies in the dynamic underlying decision-making processes. This method operates without the need for reward labels, enabling better generalization across diverse domains and tasks. Our contributions are threefold: (1) We introduce a trajectory embedding approach that captures multiple abilities from state-action data. (2) The learned embeddings exhibit strong representational power across downstream tasks, including imitation, classification, clustering, and regression. (3) The embeddings demonstrate unique properties, such as controlling agent behaviors in IQ-Learn and an additive structure in the latent space. Experimental results confirm that our method outperforms traditional approaches, offering more flexible and powerful trajectory representations for various applications. Our code is available at this https URL.', 'abstract_zh': '在自动驾驶、机器人技术以及医疗健康等现实世界的顺序决策任务中，从观测到的状态-动作轨迹进行学习对于模仿、分类和聚类等任务至关重要。例如，自动驾驶汽车必须复制人类的驾驶行为，而机器人和医疗系统可以从建模决策序列中受益，无论这些序列是否来源于专家数据。现有的轨迹编码方法往往专注于特定任务或依赖于奖励信号，这限制了它们在不同领域和任务中的泛化能力。受到在静态领域中嵌入模型如CLIP和BERT取得成功的影响，我们提出了一种新颖的方法，将状态-动作轨迹嵌入到一个潜在空间中，以捕捉动态决策过程中所涉及的能力和技能。该方法无需使用奖励标签，从而能够在不同的领域和任务中实现更好的泛化能力。我们的贡献包括三个方面：(1) 我们介绍了一种轨迹嵌入方法，可以捕捉状态-动作数据中的多种能力。(2) 学习得到的嵌入在下游任务（包括模仿、分类、聚类和回归）中表现出强大的表征能力。(3) 嵌入表现出了独特的特性，例如在IQ-Learn中控制代理行为的能力以及潜在空间中的加性结构。实验结果表明，我们的方法在各项指标上均优于传统方法，提供了更加灵活和强大的轨迹表示形式，适用于多种应用场景。我们的代码可在以下链接获取：[此处插入链接]', 'title_zh': '关于学习信息丰富的轨迹嵌入用于模仿、分类和回归的研究'}
{'arxiv_id': 'arXiv:2501.09311', 'title': 'Shape-Based Single Object Classification Using Ensemble Method Classifiers', 'authors': 'Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli', 'link': 'https://arxiv.org/abs/2501.09311', 'abstract': 'Nowadays, more and more images are available. Annotation and retrieval of the images pose classification problems, where each class is defined as the group of database images labelled with a common semantic label. Various systems have been proposed for content-based retrieval, as well as for image classification and indexing. In this paper, a hierarchical classification framework has been proposed for bridging the semantic gap effectively and achieving multi-category image classification. A well known pre-processing and post-processing method was used and applied to three problems; image segmentation, object identification and image classification. The method was applied to classify single object images from Amazon and Google datasets. The classification was tested for four different classifiers; BayesNetwork (BN), Random Forest (RF), Bagging and Vote. The estimated classification accuracies ranged from 20% to 99% (using 10-fold cross validation). The Bagging classifier presents the best performance, followed by the Random Forest classifier.', 'abstract_zh': '如今，可供使用的图像越来越多。图像的标注和检索构成了分类问题，其中每个类别定义为一组具有共同语义标签的数据库图像。已提出了多种内容基于的检索系统，以及图像分类和索引系统。在本文中，我们提出了一种分层分类框架，以有效地弥合语义差距并实现多类图像分类。该框架采用了一种广为人知的预处理和后处理方法，并应用于图像分割、目标识别和图像分类三个问题。该方法应用于从Amazon和Google数据集中分类单个目标图像。分类结果使用四种不同的分类器进行测试：朴素贝叶斯网络（BN）、随机森林（RF）、Bagging和投票。估计的分类准确率范围从20%到99%（使用10折交叉验证）。袋装分类器表现出最佳性能，随后是随机森林分类器。', 'title_zh': '基于形状的单对象分类方法探究：集成分类器的应用'}
{'arxiv_id': 'arXiv:2501.09310', 'title': 'A Study of In-Context-Learning-Based Text-to-SQL Errors', 'authors': 'Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu', 'link': 'https://arxiv.org/abs/2501.09310', 'abstract': 'Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.', 'abstract_zh': '大规模语言模型（LLMs）已被用于执行文本转SQL任务，利用其上下文学习（ICL）能力将自然语言问题转换为结构化查询语言（SQL）。然而，这种方法面临着正确性问题，并需要有效的修复解决方案。本文我们首次对其进行全面研究，涵盖了四种代表性的ICL技术、五种基本修复方法、两个基准和两种LLM设置。我们发现文本转SQL错误普遍存在，并归纳总结了7类共29种错误类型。我们还发现，现有的修复尝试在高计算开销和大量误修复的情况下，对正确性的改进有限。基于这些发现，我们提出了一种名为MapleRepair的新型文本转SQL错误检测与修复框架。评估结果表明，MapleRepair优于现有解决方案，能够修复更多的查询（13.8%），同时误修复率几乎可以忽略，且计算开销降低了67.4%。', 'title_zh': '基于上下文学习的文本到SQL错误研究'}
{'arxiv_id': 'arXiv:2501.09309', 'title': 'Understanding Mental Health Content on Social Media and Its Effect Towards Suicidal Ideation', 'authors': 'Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail', 'link': 'https://arxiv.org/abs/2501.09309', 'abstract': 'This review underscores the critical need for effective strategies to identify and support individuals with suicidal ideation, exploiting technological innovations in ML and DL to further suicide prevention efforts. The study details the application of these technologies in analyzing vast amounts of unstructured social media data to detect linguistic patterns, keywords, phrases, tones, and contextual cues associated with suicidal thoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural networks, and their effectiveness in interpreting complex data patterns and emotional nuances within text data. The review discusses the potential of these technologies to serve as a life-saving tool by identifying at-risk individuals through their digital traces. Furthermore, it evaluates the real-world effectiveness, limitations, and ethical considerations of employing these technologies for suicide prevention, stressing the importance of responsible development and usage. The study aims to fill critical knowledge gaps by analyzing recent studies, methodologies, tools, and techniques in this field. It highlights the importance of synthesizing current literature to inform practical tools and suicide prevention efforts, guiding innovation in reliable, ethical systems for early intervention. This research synthesis evaluates the intersection of technology and mental health, advocating for the ethical and responsible application of ML, DL, and NLP to offer life-saving potential worldwide while addressing challenges like generalizability, biases, privacy, and the need for further research to ensure these technologies do not exacerbate existing inequities and harms.', 'abstract_zh': '本文概要强调了需要有效策略来识别和支持具有自杀倾向的个体，并利用机器学习和深度学习技术进一步推进自杀预防工作。研究详细探讨了这些技术如何应用于分析大量的非结构化社交媒体数据，以检测与自杀念头相关的语言模式、关键词、短语、语气和情境线索。研究探讨了支持向量机（SVMs）、卷积神经网络（CNNs）、长短期记忆网络（LSTM）、神经网络等多种机器学习和深度学习模型，并评估了它们在解读文本数据中的复杂模式和情感细微差别的有效性。本文讨论了这些技术作为救命工具的潜力，通过数字踪迹识别风险个体。此外，本文还评估了这些技术在现实世界中的有效性、限制和道德考虑，强调了负责任开发和使用的必要性。研究旨在通过分析该领域的最新研究、方法、工具和技术，填补关键知识缺口。本文强调综合现有文献以指导实践工具和自杀预防工作的必要性，引导建立可靠、符合伦理的早期干预系统。该研究综合评估了技术和心理健康之间的交叉点，倡导在推广和应用机器学习、深度学习和自然语言处理时确保其伦理和负责任的使用，以在全球范围内提供挽救生命的潜力，并解决可推广性、偏差、隐私等挑战，以及进一步研究的紧迫性，以确保这些技术不会加剧现有的不平等和伤害。', 'title_zh': '理解社交媒体上的心理健康内容及其对自杀意念的影响'}
{'arxiv_id': 'arXiv:2501.09292', 'title': 'To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic Retrieval Augmented Generation', 'authors': 'Kaustubh D. Dhole', 'link': 'https://arxiv.org/abs/2501.09292', 'abstract': 'Retrieval-Augmented Generation equips large language models with the capability to retrieve external knowledge, thereby mitigating hallucinations by incorporating information beyond the model\'s intrinsic abilities. However, most prior works have focused on invoking retrieval deterministically, which makes it unsuitable for tasks such as long-form question answering. Instead, dynamically performing retrieval by invoking it only when the underlying LLM lacks the required knowledge can be more efficient. In this context, we delve deeper into the question, "To Retrieve or Not to Retrieve?" by exploring multiple uncertainty detection methods. We evaluate these methods for the task of long-form question answering, employing dynamic retrieval, and present our comparisons. Our findings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy.', 'abstract_zh': '检索增强生成使大规模语言模型具备检索外部知识的能力，从而通过结合模型内在能力之外的信息来减轻幻觉现象。然而，大多数前期工作都集中在以确定性方式触发检索上，这使其不适合长文本问答等任务。相反，仅在基础语言模型缺乏所需知识时动态地触发检索可能是更有效的。在此背景下，我们深入探讨了“检索还是不检索？”这一问题，探索了多种不确定性检测方法。我们使用动态检索对长文本问答任务进行了评估，并展示了我们的比较结果。我们的研究发现，诸如度矩阵雅卡尔距离和偏心率等不确定性检测指标，可以将检索调用次数减少近一半，同时仅轻微降低问答准确性。', 'title_zh': '是取用还是不取用？动态检索增强生成中的不确定性检测'}
{'arxiv_id': 'arXiv:2501.09291', 'title': 'LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport', 'authors': 'Kyeongha Rho, Hyeongkeun Lee, Valentio Iverson, Joon Son Chung', 'link': 'https://arxiv.org/abs/2501.09291', 'abstract': 'Automated audio captioning is a task that generates textual descriptions for audio content, and recent studies have explored using visual information to enhance captioning quality. However, current methods often fail to effectively fuse audio and visual data, missing important semantic cues from each modality. To address this, we introduce LAVCap, a large language model (LLM)-based audio-visual captioning framework that effectively integrates visual information with audio to improve audio captioning performance. LAVCap employs an optimal transport-based alignment loss to bridge the modality gap between audio and visual features, enabling more effective semantic extraction. Additionally, we propose an optimal transport attention module that enhances audio-visual fusion using an optimal transport assignment map. Combined with the optimal training strategy, experimental results demonstrate that each component of our framework is effective. LAVCap outperforms existing state-of-the-art methods on the AudioCaps dataset, without relying on large datasets or post-processing. Code is available at this https URL.', 'abstract_zh': '自动音频字幕生成是一项生成音频内容文本描述的任务，近年来的研究探讨了利用视觉信息来提高字幕质量的方法。然而，当前的方法往往无法有效地融合音频和视觉数据，从而错过了每种模态中的重要语义线索。为解决这一问题，我们引入了LAVCap，这是一种基于大规模语言模型（LLM）的音频-视觉字幕生成框架，能够有效地将视觉信息与音频融合，以提高音频字幕生成性能。LAVCap 采用基于运筹学的对齐损失来弥合音频和视觉特征之间的模态差异，从而更好地提取语义信息。此外，我们提出了一个基于运筹学注意力模块，该模块使用运筹学分配图增强了音频-视觉融合能力。结合最佳训练策略，实验结果表明，框架中的每个组件都是有效的。LAVCap 在 AudioCaps 数据集上的性能超过了现有最先进的方法，且无需依赖大规模数据集或后续处理。相关代码可从此链接访问：[点击访问代码]', 'title_zh': 'LAVCap：基于最优 transport 的大型语言模型驱动的音视频描述生成'}
{'arxiv_id': 'arXiv:2501.09274', 'title': 'Large Language Model is Secretly a Protein Sequence Optimizer', 'authors': 'Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun', 'link': 'https://arxiv.org/abs/2501.09274', 'abstract': 'We consider the protein sequence engineering problem, which aims to find protein sequences with high fitness levels, starting from a given wild-type sequence. Directed evolution has been a dominating paradigm in this field which has an iterative process to generate variants and select via experimental feedback. We demonstrate large language models (LLMs), despite being trained on massive texts, are secretly protein sequence optimizers. With a directed evolutionary method, LLM can perform protein engineering through Pareto and experiment-budget constrained optimization, demonstrating success on both synthetic and experimental fitness landscapes.', 'abstract_zh': '我们将研究蛋白质序列工程问题，其目标是从给定的野生型序列出发，找到具有高适应度水平的蛋白质序列。定向进化一直是该领域的主导范式，它通过迭代过程生成变异体，并通过实验反馈进行选择。我们证明，尽管大型语言模型（LLMs）是通过大量文本训练的，但它们实际上是蛋白质序列优化器。通过使用定向进化方法，LLMs可以通过帕累托优化和实验预算约束进行蛋白质工程，展示出在合成性和实验性适应度景观中均取得成功的能力。', 'title_zh': '大型语言模型实际上是蛋白质序列优化器'}
{'arxiv_id': 'arXiv:2501.09265', 'title': 'Perspective Transition of Large Language Models for Solving Subjective Tasks', 'authors': 'Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu', 'link': 'https://arxiv.org/abs/2501.09265', 'abstract': 'Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.', 'abstract_zh': '大规模语言模型（LLMs）在自然语言处理领域引发了一场革命，推动了各种任务取得了显著的进步。与常识推理和算术问答等客观任务相比，LLMs 在处理主观任务时的表现仍然有限，特定问题的视角对于更好地解释上下文和给予适当回应起着关键作用。例如，在某些场景中，当LLMs 以专家视角回答问题时，可能会表现出更好的性能，潜在地利用其相关领域的知识。相反，在某些场景中，以第三方视角回答问题可能会使LLMs 提供更准确的答案，从而实现对问题的更全面理解，并有可能减轻固有的偏见。在这篇论文中，我们提出了一种基于上下文学习的方法——视角转换推理（RPT），该方法使LLMs 能够动态选择直接、角色和第三方视角，以最适合的方式解决相应的主观问题。通过在包括GPT-4、GPT-3.5、Llama-3 和 Qwen-2在内的12项主观任务上进行广泛的实验，我们的方法在使用闭源和开源LLMs时均优于广泛使用的基于单一固定视角的方法，如链式思维提示和专家提示。我们的研究突显了LLMs 如何灵活地调整视角以提供精细且上下文适配的回复，以应对不同问题的复杂方式。', 'title_zh': '大型语言模型视角转换在解决主观任务中的应用研究'}
{'arxiv_id': 'arXiv:2501.09254', 'title': 'Clone-Robust AI Alignment', 'authors': 'Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang', 'link': 'https://arxiv.org/abs/2501.09254', 'abstract': 'A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.', 'abstract_zh': '训练大规模语言模型（Large Language Models, LLMs）的一个关键挑战是确保模型与人类偏好正确对齐。强化学习带有人类反馈（Reinforcement Learning with Human Feedback, RLHF）通过使用人类标注者进行成对比较来训练奖励函数，并已成为一种流行的对齐方法。然而，在RLHF中，输入数据集在包括的问题和答案类型上并不一定是平衡的。因此，我们希望RLHF算法即使在可选方案分布不均匀时也能表现良好。借鉴社会选择理论的见解，我们引入了对近似克隆的鲁棒性这一RLHF算法的理想特性，该特性要求添加几乎相同的可选方案不会显著改变学到的奖励函数。首先，我们证明了基于标准化最大似然估计（MLE）的标准RLHF算法未能满足这一特性。然后，我们提出了一种加权MLE算法，这是一种修改标准标准化MLE的RLHF算法，通过对其他可选方案的相似性加权来调整可选方案的权重。该新算法在保证对近似克隆的鲁棒性的同时，保留了理想的理论性质。', 'title_zh': '克隆鲁棒的AI对齐'}
{'arxiv_id': 'arXiv:2501.09223', 'title': 'Foundations of Large Language Models', 'authors': 'Tong Xiao, Jingbo Zhu', 'link': 'https://arxiv.org/abs/2501.09223', 'abstract': 'This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.', 'abstract_zh': '这是一本关于大型语言模型的书籍。如标题所示，本书主要侧重于基础知识而非全面涵盖所有前沿技术。本书分为四大部分，分别探讨了四个关键领域：预训练、生成模型、提示技术以及对齐方法。本书旨在为自然语言处理及相关领域的大学生、专业人员和从业人员提供参考，并适合任何对大型语言模型感兴趣的人士阅读。', 'title_zh': '大型语言模型的理论基础'}
{'arxiv_id': 'arXiv:2501.09218', 'title': 'Interpretable Droplet Digital PCR Assay for Trustworthy Molecular Diagnostics', 'authors': 'Yuanyuan Wei, Yucheng Wu, Fuyang Qu, Yao Mu, Yi-Ping Ho, Ho-Pui Ho, Wu Yuan, Mingkun Xu', 'link': 'https://arxiv.org/abs/2501.09218', 'abstract': "Accurate molecular quantification is essential for advancing research and diagnostics in fields such as infectious diseases, cancer biology, and genetic disorders. Droplet digital PCR (ddPCR) has emerged as a gold standard for achieving absolute quantification. While computational ddPCR technologies have advanced significantly, achieving automatic interpretation and consistent adaptability across diverse operational environments remains a challenge. To address these limitations, we introduce the intelligent interpretable droplet digital PCR (I2ddPCR) assay, a comprehensive framework integrating front-end predictive models (for droplet segmentation and classification) with GPT-4o multimodal large language model (MLLM, for context-aware explanations and recommendations) to automate and enhance ddPCR image analysis. This approach surpasses the state-of-the-art models, affording 99.05% accuracy in processing complex ddPCR images containing over 300 droplets per image with varying signal-to-noise ratios (SNRs). By combining specialized neural networks and large language models, the I2ddPCR assay offers a robust and adaptable solution for absolute molecular quantification, achieving a sensitivity capable of detecting low-abundance targets as low as 90.32 copies/{\\mu}L. Furthermore, it improves model's transparency through detailed explanation and troubleshooting guidance, empowering users to make informed decisions. This innovative framework has the potential to benefit molecular diagnostics, disease research, and clinical applications, especially in resource-constrained settings.", 'abstract_zh': '准确的分子量化是推进感染性疾病、癌症生物学和遗传疾病等领域研究和诊断的关键。滴液数字聚合酶链反应（ddPCR）已成为实现绝对定量的黄金标准。尽管计算滴液ddPCR技术取得了显著进步，但在跨各种操作环境实现自动解释和一致适应性方面仍面临挑战。为解决这些限制，我们提出了智能可解释滴液ddPCR（I2ddPCR）实验法，该综合框架集成了前端预测模型（用于滴液分割和分类）以及基于GPT-4o多模态大型语言模型（MLLM，用于情境感知解释和建议），以自动化和增强ddPCR图像分析。这种方法超越了现有最先进的模型，在处理包含超过300个滴液（信号与噪声比SNR各异）的复杂ddPCR图像时，准确率达到99.05%。通过结合专用神经网络和大型语言模型，I2ddPCR实验法提供了一种稳健且可适应的绝对分子量化解决方案，能够检测低至90.32拷贝/μL的低丰度目标。此外，它通过详细的解释和故障排除指导提高了模型的透明度，使用户能够做出知情决策。这一创新框架有望在分子诊断、疾病研究和临床应用中发挥作用，尤其是在资源受限的环境中。', 'title_zh': '可解释的微滴数字PCR assay及其在可信赖的分子诊断中的应用'}
{'arxiv_id': 'arXiv:2501.09217', 'title': 'Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification', 'authors': 'Marcell T. Kurbucz, Balázs Hajós, Balázs P. Halmos, Vince Á. Molnár, Antal Jakovác', 'link': 'https://arxiv.org/abs/2501.09217', 'abstract': 'Time series classification (TSC) is fundamental in numerous domains, including finance, healthcare, and environmental monitoring. However, traditional TSC methods often struggle with the inherent complexity and variability of time series data. Building on our previous work with the linear law-based transformation (LLT) - which improved classification accuracy by transforming the feature space based on key data patterns - we introduce adaptive law-based transformation (ALT). ALT enhances LLT by incorporating variable-length shifted time windows, enabling it to capture distinguishing patterns of various lengths and thereby handle complex time series more effectively. By mapping features into a linearly separable space, ALT provides a fast, robust, and transparent solution that achieves state-of-the-art performance with only a few hyperparameters.', 'abstract_zh': '时间序列分类（TSC）在金融、医疗保健和环境监测等多个领域中具有基础性意义。然而，传统的TSC方法往往难以应对时间序列数据固有的复杂性和变异性。在此前基于线性法则变换（LLT）的基础上，我们进一步引入了自适应法则变换（ALT）。ALT通过结合可变长度的时间窗口平移，增强了LLT的功能，使其能够捕捉不同长度的特征模式，从而更有效地处理复杂的时间序列。通过将特征映射到线性可分的空间中，ALT提供了一个快速、稳健且透明的解决方案，仅需少量超参数即可达到当前最佳性能。', 'title_zh': '基于适应性法律的转换（ALT）：一种轻量级时间序列分类的特征表示'}
{'arxiv_id': 'arXiv:2501.09194', 'title': 'Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation', 'authors': 'Ahmad Süleyman, Göksel Biricik', 'link': 'https://arxiv.org/abs/2501.09194', 'abstract': 'Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLIGEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations.', 'abstract_zh': '大规模文字到图像（T2I）扩散模型已经在从自然语言文本描述生成多样化的高质量视觉效果方面展示了出色的表现。多种布局到图像模型已被开发，通过利用分割图、边缘和人体关键点等多种布局来控制生成过程。在此工作中，我们提出了一种名为 ObjectDiffusion 的模型，该模型借鉴了顶级的图像生成框架，通过引入新的边界框能力，无缝地条件化 T2I 模型。具体来说，我们对 ContorlNet 引入的网络架构进行了重大修改，并将其与 GLIGEN 提出的条件处理和注入技术结合。ObjectDiffusion 通过利用在大规模数据集上训练获得的生成知识进行预训练。我们对 COCO2017 训练集进行了微调，并在 COCO2017 验证集上进行了评估。该模型在三个方面均超过了基于开源数据集训练的当前最优模型，取得了 AP$_{50}$ 46.6、AR 44.5 和 FID 19.8 的成绩。ObjectDiffusion 在合成多样、高质量、高保真图像方面显示出了独特的能力，这些图像能够无缝地符合语义和空间控制布局。在定性和定量测试中，ObjectDiffusion 在各种上下文中展示了出色的封闭集和开放集场景的基础能力。定性评估验证了 ObjectDiffusion 生成不同大小和位置的多个对象的能力。', 'title_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\nGrounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation\n\n扎根于文本的地对图扩散模型，用于可控的高质量图像生成'}
{'arxiv_id': 'arXiv:2501.09187', 'title': 'Patch-aware Vector Quantized Codebook Learning for Unsupervised Visual Defect Detection', 'authors': 'Qisen Cheng, Shuhui Qu, Janghwan Lee', 'link': 'https://arxiv.org/abs/2501.09187', 'abstract': 'Unsupervised visual defect detection is critical in industrial applications, requiring a representation space that captures normal data features while detecting deviations. Achieving a balance between expressiveness and compactness is challenging; an overly expressive space risks inefficiency and mode collapse, impairing detection accuracy. We propose a novel approach using an enhanced VQ-VAE framework optimized for unsupervised defect detection. Our model introduces a patch-aware dynamic code assignment scheme, enabling context-sensitive code allocation to optimize spatial representation. This strategy enhances normal-defect distinction and improves detection accuracy during inference. Experiments on MVTecAD, BTAD, and MTSD datasets show our method achieves state-of-the-art performance.', 'abstract_zh': '无监督视觉缺陷检测在工业应用中至关重要，需要一个能够捕获正常数据特征并检测偏差的表示空间。在表达能力和紧凑性之间取得平衡是具有挑战性的；过于表达的表示空间可能导致效率低下和模式崩溃，从而影响检测准确性。我们提出了一种新的方法，基于增强的VQ-VAE框架，专为无监督缺陷检测而优化。该模型引入了一种基于补丁感知的动态码分配方案，能够根据上下文敏感地分配编码，优化空间表示。这种策略增强了正常与缺陷之间的区分，并提高了推理过程中的检测准确性。在MVTecAD、BTAD和MTSD数据集上的实验表明，我们的方法达到了最先进的性能。', 'title_zh': '基于斑块意识的向量量化代码本学习在无监督视觉缺陷检测中的应用'}
{'arxiv_id': 'arXiv:2501.09186', 'title': 'Guiding Retrieval using LLM-based Listwise Rankers', 'authors': 'Mandeep Rathee, Sean MacAvaney, Avishek Anand', 'link': 'https://arxiv.org/abs/2501.09186', 'abstract': "Large Language Models (LLMs) have shown strong promise as rerankers, especially in ``listwise'' settings where an LLM is prompted to rerank several search results at once. However, this ``cascading'' retrieve-and-rerank approach is limited by the bounded recall problem: relevant documents not retrieved initially are permanently excluded from the final ranking. Adaptive retrieval techniques address this problem, but do not work with listwise rerankers because they assume a document's score is computed independently from other documents. In this paper, we propose an adaptation of an existing adaptive retrieval method that supports the listwise setting and helps guide the retrieval process itself (thereby overcoming the bounded recall problem for LLM rerankers). Specifically, our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point. Through extensive experiments across diverse LLM rerankers, first stage retrievers, and feedback sources, we demonstrate that our method can improve nDCG@10 by up to 13.23% and recall by 28.02%--all while keeping the total number of LLM inferences constant and overheads due to the adaptive process minimal. The work opens the door to leveraging LLM-based search in settings where the initial pool of results is limited, e.g., by legacy systems, or by the cost of deploying a semantic first-stage.", 'abstract_zh': '大语言模型（LLM）在重新排序方面表现出强大的潜力，尤其是在“列表级”设置中，即LLM被提示一次性对多个搜索结果进行重新排序。然而，这种“级联”检索和重新排序的方法受到“回忆限制”问题的限制：未在最初检索到的相关文档将被永久排除在最终排名之外。自适应检索技术可以解决这一问题，但它们不能与列表级重新排序器一起使用，因为它们假设一个文档的分数与其他文档独立计算。本文中，我们提出了一种现有自适应检索方法的改进版本，该版本适用于列表级设置，并有助于引导检索过程本身（从而克服LLM重新排序器的回忆限制问题）。具体来说，我们提出的算法将初始排序结果与迄今为止最相关的文档提供的反馈文档结果合并。通过在各种LLM重新排序器、第一阶段检索器和反馈源上的广泛实验，我们证明了我们的方法可以在保持LLM推断次数不变的同时，将nDCG@10提高多达13.23%，召回率提高28.02%，并且自适应过程导致的额外开销极少。这项工作为在初始结果池受限的场景下利用基于LLM的搜索打开了大门，例如由传统系统限制或部署语义第一阶段的成本限制的场景。', 'title_zh': '使用基于LLM的列表式排名器指导检索'}
{'arxiv_id': 'arXiv:2501.09166', 'title': 'Attention is All You Need Until You Need Retention', 'authors': 'M. Murat Yaslioglu', 'link': 'https://arxiv.org/abs/2501.09166', 'abstract': 'This work introduces a novel Retention Layer mechanism for Transformer based architectures, addressing their inherent lack of intrinsic retention capabilities. Unlike human cognition, which can encode and dynamically recall symbolic templates, Generative Pretrained Transformers rely solely on fixed pretrained weights and ephemeral context windows, limiting their adaptability. The proposed Retention Layer incorporates a persistent memory module capable of real time data population, dynamic recall, and guided output generation. This enhancement allows models to store, update, and reuse observed patterns across sessions, enabling incremental learning and bridging the gap between static pretraining and dynamic, context sensitive adaptation. The Retention Layer design parallels social learning processes, encompassing attention, retention, reproduction, and motivation stages. Technically, it integrates a memory attention mechanism and episodic buffers to manage memory scalability, mitigate overfitting, and ensure efficient recall. Applications span adaptive personal assistants, real time fraud detection, autonomous robotics, content moderation, and healthcare diagnostics. In each domain, the retention mechanism enables systems to learn incrementally, personalize outputs, and respond to evolving real world challenges effectively. By emulating key aspects of human learning, this retention enhanced architecture fosters a more fluid and responsive AI paradigm, paving the way for dynamic, session aware models that extend the capabilities of traditional Transformers into domains requiring continual adaptation.', 'abstract_zh': '本文介绍了一种新颖的保留层机制，用于基于Transformer的架构中，解决了它们固有的内在保留能力不足的问题。与人类认知能够编码并动态回忆符号模板不同，生成预训练Transformer模型依赖于固定的预训练权重和短暂的上下文窗口，这限制了它们的适应性。所提出的保留层集成了一个持久性记忆模块，该模块能够实时数据填充、动态回忆以及引导输出生成。这种增强使得模型能够在不同会话中存储、更新和重用观察到的模式，从而实现增量学习，并弥补静态预训练与动态、上下文敏感适应之间的差距。保留层的设计借鉴了社会学习过程，包括注意力、保留、再现和动机阶段。技术上，该设计集成了记忆注意力机制和情境缓冲区，以管理内存的扩展性、缓解过拟合并确保高效回忆。其应用范围广泛，包括自适应个人助手、实时欺诈检测、自主机器人、内容审核以及医疗诊断。在每个领域中，保留机制使系统能够实现增量学习，个性化输出，并有效应对动态变化的实际挑战。通过模拟人类学习的关键方面，这种增强保留能力的架构促进了更具流动性和响应性的AI范式，开辟了动态、会话感知模型的道路，这些模型将传统Transformer的能力扩展到需要持续适应的领域。', 'title_zh': '"直到需要保留注意力之前，一切只需注意力"'}
{'arxiv_id': 'arXiv:2501.09164', 'title': 'The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching', 'authors': 'Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian', 'link': 'https://arxiv.org/abs/2501.09164', 'abstract': 'In this work, we address the challenge of evaluating large language models (LLMs) on the short answer matching task for Latvian and Lithuanian languages. We introduce novel datasets consisting of 502 Latvian and 690 Lithuanian question-answer pairs. For each question-answer pair, we generated matched and non-matched answers using a set of alteration rules specifically designed to introduce small but meaningful changes in the text. These generated answers serve as test cases to assess the ability of LLMs to detect subtle differences in matching of the original answers. A subset of the datasets was manually verified for quality and accuracy. Our results show that while larger LLMs, such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in distinguishing matched and non-matched answers, smaller models show more variance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot examples, while Mistral Nemo 12b underperformed on detection of subtle text alteration, particularly in Lithuanian, even with additional examples. QWEN2.5 7b and Mistral 7b were able to obtain a strong and comparable performance to the larger 70b models in zero and few shot experiments. Moreover, the performance of Mistral 7b was weaker in few shot experiments.', 'abstract_zh': '在本研究中，我们针对拉脱维亚语和立陶宛语的短答案匹配任务，探讨了评估大规模语言模型（LLMs）的挑战。我们引入了新的数据集，其中包含502个拉脱维亚语和690个立陶宛语的问题-答案对。对于每个问题-答案对，我们使用一组特定设计的修改规则生成了匹配和不匹配的答案，以引入细微但有意义的文本更改。这些生成的答案作为测试案例，用于评估LLMs检测原始答案细微差异的能力。部分数据集的高质量和准确性由人工验证。结果显示，尽管较大的LLMs（如QWEN2.5 72b和LLaMa3.1 70b）在区分匹配和不匹配的答案方面表现出近乎完美的性能，较小的模型则显示出更大的差异。例如，LLaMa3.1 8b和EuroLLM 9b受益于少量示例，而Mistral Nemo 12b在检测细微文本更改方面表现不佳，尤其是在立陶宛语中，即使有额外的示例也是如此。QWEN2.5 7b和Mistral 7b在零样本和少量样本实验中能够取得与较大的70b模型相当甚至更强的表现。此外，Mistral 7b在少量样本实验中的表现较弱。', 'title_zh': 'velnis(ia) 的细节决定一切：评估大规模语言模型对拉脱维亚语和立陶宛语文本简答题匹配判断的能力'}
{'arxiv_id': 'arXiv:2501.09163', 'title': 'Towards Understanding Extrapolation: a Causal Lens', 'authors': 'Lingjing Kong, Guangyi Chen, Petar Stojanov, Haoxuan Li, Eric P. Xing, Kun Zhang', 'link': 'https://arxiv.org/abs/2501.09163', 'abstract': "Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution. However, practical scenarios often involve only a handful of target samples, potentially lying outside the training support, which requires the capability of extrapolation. In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution. To this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms. Under this formulation, we cast the extrapolation problem into a latent-variable identification problem. We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios. Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties. We showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.", 'abstract_zh': '经典的分布转移处理通常需要一个目标分布，该分布位于训练分布之内。然而，在实践中，目标样本可能只有少量几个，且这些样本可能位于训练分布范围之外，这就需要泛化能力来进行外推。在本项研究中，我们旨在提供关于何时可以进行外推的理论理解，并提出在不依赖于支持集内目标分布的条件下的原则性方法。为此，我们使用隐变量模型来表述因果机制中的最小变化原则，将外推问题转化为隐变量识别问题。我们为仅有一个非支持集中的目标样本的情况提供了现实条件，以便即使在这种最具挑战性的场景下也能够进行识别。我们的理论揭示了底层流形的光滑性与分布转移属性之间错综复杂的相互作用。我们展示了我们的理论结果如何指导其实用适应算法的设计。通过在合成数据和真实数据上的实验，我们验证了我们的理论成果及其实际意义。', 'title_zh': '《通过因果视角理解外推：一种分析框架》'}
{'arxiv_id': 'arXiv:2501.09160', 'title': 'AutoLoop: Fast Visual SLAM Fine-tuning through Agentic Curriculum Learning', 'authors': 'Assaf Lahiany, Oren Gal', 'link': 'https://arxiv.org/abs/2501.09160', 'abstract': "Current visual SLAM systems face significant challenges in balancing computational efficiency with robust loop closure handling. Traditional approaches require careful manual tuning and incur substantial computational overhead, while learning-based methods either lack explicit loop closure capabilities or implement them through computationally expensive methods. We present AutoLoop, a novel approach that combines automated curriculum learning with efficient fine-tuning for visual SLAM systems. Our method employs a DDPG (Deep Deterministic Policy Gradient) agent to dynamically adjust loop closure weights during training, eliminating the need for manual hyperparameter search while significantly reducing the required training steps. The approach pre-computes potential loop closure pairs offline and leverages them through an agent-guided curriculum, allowing the model to adapt efficiently to new scenarios. Experiments conducted on TartanAir for training and validated across multiple benchmarks including KITTI, EuRoC, ICL-NUIM and TUM RGB-D demonstrate that AutoLoop achieves comparable or superior performance while reducing training time by an order of magnitude compared to traditional approaches. AutoLoop provides a practical solution for rapid adaptation of visual SLAM systems, automating the weight tuning process that traditionally requires multiple manual iterations. Our results show that this automated curriculum strategy not only accelerates training but also maintains or improves the model's performance across diverse environmental conditions.", 'abstract_zh': '当前的视觉SLAM系统在平衡计算效率与鲁棒环视匹配处理方面面临重大挑战。传统方法需要精心的手动调参，并且会带来显著的计算开销，而基于学习的方法要么缺乏明确的环视匹配能力，要么通过计算密集型的方法实现。我们提出了AutoLoop，这是一种结合自动化课程学习与高效微调的新型方法，适用于视觉SLAM系统。该方法采用DDPG（深度确定策略梯度）代理，在训练过程中动态调整环视匹配权重，从而消除手动超参数搜索的需要，同时显著减少所需的训练步骤。该方法预先计算潜在的环视匹配对，并通过代理引导的课程学习利用这些对，使模型能够高效适应新的场景。在TartanAir上进行训练，并在多种基准测试中进行验证，包括KITTI、EuRoC、ICL-NUIM和TUM RGB-D，实验结果显示，AutoLoop在训练时间降低一个数量级的同时，可实现类似或优于传统方法的性能。AutoLoop为视觉SLAM系统的快速适应提供了一个实用的解决方案，自动化了传统方法中需要多次手动迭代的权重调整过程。我们的结果表明，这种自动化的课程学习策略不仅加速了训练过程，还能够在各种环境条件下维持或提高模型的性能。', 'title_zh': '自引导环：通过代理性和课程学习快速进行视觉SLAM微调'}
{'arxiv_id': 'arXiv:2501.09154', 'title': 'Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A study on Lithuanian History', 'authors': 'Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian', 'link': 'https://arxiv.org/abs/2501.09154', 'abstract': 'In this work, we evaluated Lithuanian and general history knowledge of multilingual Large Language Models (LLMs) on a multiple-choice question-answering task. The models were tested on a dataset of Lithuanian national and general history questions translated into Baltic, Nordic, and other languages (English, Ukrainian, Arabic) to assess the knowledge sharing from culturally and historically connected groups. We evaluated GPT-4o, LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral 7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).\nOur results show that GPT-4o consistently outperformed all other models across language groups, with slightly better results for Baltic and Nordic languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b performed well but showed weaker alignment with Baltic languages. Smaller models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b) demonstrated gaps with LT-related alignment with Baltic languages while performing better on Nordic and other languages. The Nordic fine-tuned models did not surpass multilingual models, indicating that shared cultural or historical context alone does not guarantee better performance.', 'abstract_zh': '在本研究中，我们通过多项选择题作答任务评估了多语言大型语言模型（LLMs）对立陶宛历史和一般历史知识的理解。模型被测试在一个包含立陶宛国家历史和一般历史问题的数据集中，这些问题已经被翻译成波罗的海语系、北欧语系以及其他语言（英语、乌克兰语、阿拉伯语），以此来评估文化或历史联系紧密的群体之间的知识共享情况。我们评估了GPT-4o、LLaMa3.1 8b和70b、QWEN2.5 7b和72b、Mistral Nemo 12b、LLaMa3 8b、Mistral 7b、LLaMa3.2 3b以及北欧语系微调的模型（GPT-SW3和LLaMa3 8b）。\n\n研究结果显示，GPT-4o在各个语言组中始终保持领先地位，特别是在波罗的海语系和北欧语系的语言中表现更佳。大型开源模型如QWEN2.5 72b和LLaMa3.1 70b表现良好，但与波罗的海语系语言的匹配度较弱。较小的模型（Mistral Nemo 12b、LLaMa3.2 3b、QWEN 7B、LLaMa3.1 8B和LLaMa3 8b）在立陶宛语相关性匹配度上存在差距，但在这北欧语系和其他语言中的表现较好。北欧语系微调的模型未能超越多语言模型，这表明共享的文化或历史背景并不足以确保更好的模型性能。', 'title_zh': '面向波罗的海和北欧语言的多语言LLM评估：立陶宛历史研究'}
{'arxiv_id': 'arXiv:2501.09134', 'title': 'Benchmarking Robustness of Contrastive Learning Models for Medical Image-Report Retrieval', 'authors': 'Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang', 'link': 'https://arxiv.org/abs/2501.09134', 'abstract': 'Medical images and reports offer invaluable insights into patient health. The heterogeneity and complexity of these data hinder effective analysis. To bridge this gap, we investigate contrastive learning models for cross-domain retrieval, which associates medical images with their corresponding clinical reports. This study benchmarks the robustness of four state-of-the-art contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We introduce an occlusion retrieval task to evaluate model performance under varying levels of image corruption. Our findings reveal that all evaluated models are highly sensitive to out-of-distribution data, as evidenced by the proportional decrease in performance with increasing occlusion levels. While MedCLIP exhibits slightly more robustness, its overall performance remains significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a general-purpose dataset, struggles with medical image-report retrieval, highlighting the importance of domain-specific training data. The evaluation of this work suggests that more effort needs to be spent on improving the robustness of these models. By addressing these limitations, we can develop more reliable cross-domain retrieval models for medical applications.', 'abstract_zh': '医学影像和报告提供了宝贵的信息，以了解患者的健康状况。这些数据的异质性和复杂性阻碍了有效的分析。为解决这一问题，我们研究了对比学习模型在跨域检索中的应用，旨在将医学影像与其相应的临床报告关联起来。本研究对比了四种最先进的对比学习模型——CLIP、CXR-RePaiR、MedCLIP 和 CXR-CLIP 的稳健性。我们引入了一种遮蔽检索任务，以评估模型在不同程度图像失真的情况下的性能。研究发现，在增加遮蔽程度时，所有评估的模型都表现出高度的敏感性，这体现在其性能的相应下降。虽然 MedCLIP 表现出略微更好的稳健性，但其整体性能仍显著落后于CXR-CLIP 和 CXR-RePaiR。由通用数据集训练的 CLIP 在医学影像-报告检索方面表现不佳，突显了领域特定训练数据的重要性。本研究的评估表明，需要更加重视提高这些模型的稳健性。通过解决这些局限性，我们可以开发出更加可靠的跨域检索模型，应用于医学领域。', 'title_zh': '对比学习模型在医疗图像-报告检索中鲁棒性基准测试'}
{'arxiv_id': 'arXiv:2501.09114', 'title': 'Generative Medical Image Anonymization Based on Latent Code Projection and Optimization', 'authors': 'Huiyu Li, Nicholas Ayache, Hervé Delingette', 'link': 'https://arxiv.org/abs/2501.09114', 'abstract': 'Medical image anonymization aims to protect patient privacy by removing identifying information, while preserving the data utility to solve downstream tasks. In this paper, we address the medical image anonymization problem with a two-stage solution: latent code projection and optimization. In the projection stage, we design a streamlined encoder to project input images into a latent space and propose a co-training scheme to enhance the projection process. In the optimization stage, we refine the latent code using two deep loss functions designed to address the trade-off between identity protection and data utility dedicated to medical images. Through a comprehensive set of qualitative and quantitative experiments, we showcase the effectiveness of our approach on the MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that can serve as training set for detecting lung pathologies. Source codes are available at this https URL.', 'abstract_zh': '医学图像匿名化旨在通过去除患者身份信息来保护患者隐私，同时保留数据的用途以解决下游任务。本文提出了一种两阶段解决方案来解决医学图像匿名化问题：潜在代码投影与优化。在投影阶段，我们设计了一种简洁的编码器将输入图像投影到潜在空间，并提出了一种协同训练方案以提高投影过程的效果。在优化阶段，我们使用两种深层损失函数精炼潜在代码，这些损失函数专门针对医学图像处理身份保护与数据用途之间的权衡问题。通过一系列全面的定性和定量实验，我们在MIMIC-CXR胸部X射线数据集上展示了我们方法的有效性，生成了可用于检测肺部病变的匿名合成图像作为训练集。源代码可在以下链接处获取：this https URL。', 'title_zh': '基于潜在代码投影与优化的生成医学图像匿名化方法'}
{'arxiv_id': 'arXiv:2501.09112', 'title': 'Mantis Shrimp: Exploring Photometric Band Utilization in Computer Vision Networks for Photometric Redshift Estimation', 'authors': 'Andrew Engel, Nell Byler, Adam Tsou, Gautham Narayan, Emmanuel Bonilla, Ian Smith', 'link': 'https://arxiv.org/abs/2501.09112', 'abstract': 'We present Mantis Shrimp, a multi-survey deep learning model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. Machine learning is now an established approach for photometric redshift estimation, with generally acknowledged higher performance in areas with a high density of spectroscopically identified galaxies over template-based methods. Multiple works have shown that image-based convolutional neural networks can outperform tabular-based color/magnitude models. In comparison to tabular models, image models have additional design complexities: it is largely unknown how to fuse inputs from different instruments which have different resolutions or noise properties. The Mantis Shrimp model estimates the conditional density estimate of redshift using cutout images. The density estimates are well calibrated and the point estimates perform well in the distribution of available spectroscopically confirmed galaxies with (bias = 1e-2), scatter (NMAD = 2.44e-2) and catastrophic outlier rate ($\\eta$=17.53$\\%$). We find that early fusion approaches (e.g., resampling and stacking images from different instruments) match the performance of late fusion approaches (e.g., concatenating latent space representations), so that the design choice ultimately is left to the user. Finally, we study how the models learn to use information across bands, finding evidence that our models successfully incorporates information from all surveys. The applicability of our model to the analysis of large populations of galaxies is limited by the speed of downloading cutouts from external servers; however, our model could be useful in smaller studies such as generating priors over redshift for stellar population synthesis.', 'abstract_zh': '我们提出了一种名为Mantis Shrimp的多源深度学习模型，该模型融合了紫外波段（GALEX）、光学波段（PanSTARRS）和红外波段（UnWISE）的影像，用于光谱红移估计。机器学习现在已成为光谱红移估计的成熟方法，尤其是在星系光谱识别密度高的区域，基于机器学习的方法通常优于基于模板的方法，其性能更优。多篇文献已经表明，基于图像的卷积神经网络可以超越基于表格的颜色/光度模型。与基于表格的模型相比，基于图像的模型具有额外的设计复杂性：目前尚不清楚如何融合不同仪器（具有不同的分辨率或噪声特性）提供的输入。Mantis Shrimp模型使用切片影像估计条件密度分布。密度估计已很好地校准，且点估计在现有光谱确认星系的分布中表现出色（偏差=1e-2，散射=2.44e-2，灾难性异常点率=$\\eta$=17.53%）。我们发现早期融合方法（例如，重采样和堆叠不同仪器的影像）在性能上可与晚期融合方法（例如，连接潜空间表示）相媲美，最终的设计选择仍留给用户。此外，我们研究了模型如何学习跨波段使用信息，发现证据表明我们的模型成功地整合了所有调查的数据。该模型在处理大量星系群体分析的应用中受到从外部服务器下载影像的速度限制；然而，在生成星族合成中的红移先验值的小规模研究中，我们的模型仍然可能具有应用价值。', 'title_zh': 'mantis虾：探索计算机视觉网络中光谱带利用以进行光谱红移估计'}
{'arxiv_id': 'arXiv:2501.09104', 'title': 'A Non-autoregressive Model for Joint STT and TTS', 'authors': 'Vishal Sunder, Brian Kingsbury, George Saon, Samuel Thomas, Slava Shechtman Hagai Aronowitz, Eric Fosler-Lussier, Luis Lastras', 'link': 'https://arxiv.org/abs/2501.09104', 'abstract': 'In this paper, we take a step towards jointly modeling automatic speech recognition (STT) and speech synthesis (TTS) in a fully non-autoregressive way. We develop a novel multimodal framework capable of handling the speech and text modalities as input either individually or together. The proposed model can also be trained with unpaired speech or text data owing to its multimodal nature. We further propose an iterative refinement strategy to improve the STT and TTS performance of our model such that the partial hypothesis at the output can be fed back to the input of our model, thus iteratively improving both STT and TTS predictions. We show that our joint model can effectively perform both STT and TTS tasks, outperforming the STT-specific baseline in all tasks and performing competitively with the TTS-specific baseline across a wide range of evaluation metrics.', 'abstract_zh': '在本文中，我们朝着以完全非自回归的方式联合建模自动语音识别（ASR）和语音合成（TTS）迈出了一步。我们开发了一种新颖的多模态框架，能够单独或同时处理语音和文本模态作为输入。由于该模型的多模态特性，它还可以使用未配对的语音或文本数据进行训练。此外，我们提出了一种迭代优化策略，以提高我们模型的ASR和TTS性能，从而使部分假设可以在输出处被反馈到模型的输入中，从而逐步提高ASR和TTS的预测性能。我们展示了我们的联合模型能够有效地执行ASR和TTS任务，在所有任务中均优于ASR特定的基线，并在广泛使用的评估指标上与其他TTS特定的基线模型表现相当。', 'title_zh': '一种非自回归模型用于联合语音识别和语音合成'}
{'arxiv_id': 'arXiv:2501.09102', 'title': 'Tracking the Takes and Trajectories of English-Language News Narratives across Trustworthy and Worrisome Websites', 'authors': 'Hans W. A. Hanley, Emily Okabe, Zakir Durumeric', 'link': 'https://arxiv.org/abs/2501.09102', 'abstract': 'Understanding how misleading and outright false information enters news ecosystems remains a difficult challenge that requires tracking how narratives spread across thousands of fringe and mainstream news websites. To do this, we introduce a system that utilizes encoder-based large language models and zero-shot stance detection to scalably identify and track news narratives and their attitudes across over 4,000 factually unreliable, mixed-reliability, and factually reliable English-language news websites. Running our system over an 18 month period, we track the spread of 146K news stories. Using network-based interference via the NETINF algorithm, we show that the paths of news narratives and the stances of websites toward particular entities can be used to uncover slanted propaganda networks (e.g., anti-vaccine and anti-Ukraine) and to identify the most influential websites in spreading these attitudes in the broader news ecosystem. We hope that increased visibility into our distributed news ecosystem can help with the reporting and fact-checking of propaganda and disinformation.', 'abstract_zh': '理解误导性和完全虚假信息如何进入新闻生态系统仍然是一个艰巨的挑战，这需要追踪叙事如何在数千个边缘和主流新闻网站上传播。为此，我们提出了一种系统，该系统利用基于编码器的大规模语言模型和零样本立场检测，以可扩展的方式识别和跟踪在超过4000个事实不可靠、混合可靠性和事实可靠的英文新闻网站上的新闻叙事及其态度。在长达18个月的运行期间，我们追踪了146,000篇新闻故事的传播路径。通过使用基于网络的干扰（如NETINF算法），我们证明可以通过分析新闻叙事的传播路径及其网站对特定实体的态度，来揭露偏向性宣传网络（例如反疫苗和反乌克兰），并识别在更广泛的新闻生态系统中传播这些态度的最有影响力网站。我们希望通过增强对分布式新闻生态系统的透明度，有助于宣传和伪信息的报道与事实核查。', 'title_zh': '《追踪英语新闻叙事在可信赖与令人担忧网站上的演变轨迹》'}
{'arxiv_id': 'arXiv:2501.09092', 'title': 'SteLLA: A Structured Grading System Using LLMs with RAG', 'authors': 'Hefei Qiu, Brian White, Ashley Ding, Reinaldo Costa, Ali Hachem, Wei Ding, Ping Chen', 'link': 'https://arxiv.org/abs/2501.09092', 'abstract': "Large Language Models (LLMs) have shown strong general capabilities in many applications. However, how to make them reliable tools for some specific tasks such as automated short answer grading (ASAG) remains a challenge. We present SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval Augmented Generation (RAG) approach is used to empower LLMs specifically on the ASAG task by extracting structured information from the highly relevant and reliable external knowledge based on the instructor-provided reference answer and rubric, b) an LLM performs a structured and question-answering-based evaluation of student answers to provide analytical grades and feedback. A real-world dataset that contains students' answers in an exam was collected from a college-level Biology course. Experiments show that our proposed system can achieve substantial agreement with the human grader while providing break-down grades and feedback on all the knowledge points examined in the problem. A qualitative and error analysis of the feedback generated by GPT4 shows that GPT4 is good at capturing facts while may be prone to inferring too much implication from the given text in the grading task which provides insights into the usage of LLMs in the ASAG system.", 'abstract_zh': '大规模语言模型（LLMs）在许多应用中展示了强大的通用能力。然而，如何使它们成为某些特定任务（如自动化简答评分，ASAG）的可靠工具仍然是一个挑战。我们提出了SteLLA（基于LLM与RAG的结构化评分系统），其中：\na) 使用检索增强生成（RAG）方法，通过从与参考答案和评分标准高度相关且可靠的外部知识中提取结构化信息，具体提升LLM在ASAG任务上的能力；\nb) 一个LLM通过结构化和基于问题的答案评估来对学生答案进行评分和反馈，从而提供分析性评分和反馈。\n我们从一门大学生物学课程的考试中收集了一个包含学生答案的现实世界数据集。实验结果表明，我们提出系统的评分结果与人类评分者之间具有显著的一致性，同时提供了所有知识点的详细评分和反馈。对GPT4生成的反馈进行定性和错误分析后发现，GPT4在捕捉事实方面表现良好，但在评分任务中可能会从给定文本中过分推断含义，这为LLMs在ASAG系统中的应用提供了见解。', 'title_zh': 'SteLLA：一种基于LLM和RAG的结构化评分系统\n\n注释：在这句话中，“SteLLA”看起来像是一个专有名词或系统名称，因此保持不变。"LLM"是指大型语言模型（Large Language Model），"RAG"是指检索增强生成（Retrieval-Augmented Generation），这些都是当前学术和工业界讨论的热点话题。'}
{'arxiv_id': 'arXiv:2501.09081', 'title': 'Inferring Transition Dynamics from Value Functions', 'authors': 'Jacob Adamczyk', 'link': 'https://arxiv.org/abs/2501.09081', 'abstract': "In reinforcement learning, the value function is typically trained to solve the Bellman equation, which connects the current value to future values. This temporal dependency hints that the value function may contain implicit information about the environment's transition dynamics. By rearranging the Bellman equation, we show that a converged value function encodes a model of the underlying dynamics of the environment. We build on this insight to propose a simple method for inferring dynamics models directly from the value function, potentially mitigating the need for explicit model learning. Furthermore, we explore the challenges of next-state identifiability, discussing conditions under which the inferred dynamics model is well-defined. Our work provides a theoretical foundation for leveraging value functions in dynamics modeling and opens a new avenue for bridging model-free and model-based reinforcement learning.", 'abstract_zh': '在强化学习中，价值函数通常被训练以解决贝尔曼方程，该方程将当前值与未来值联系起来。这种时间依赖性暗示价值函数可能包含关于环境转换动态的隐含信息。通过重新排列贝尔曼方程，我们表明，收敛的价值函数编码了环境底层动态的模型。基于这一洞察，我们提出了一种直接从价值函数推断动力学模型的简单方法，这可能减轻显式动力学学习的需要。此外，我们探讨了下一个状态可识别性的挑战，讨论了在哪些条件下推断出的动力学模型是良好定义的。我们的工作为利用价值函数进行动力学建模提供了理论基础，并开辟了一条将无模型与基于模型的强化学习联系起来的新途径。', 'title_zh': '从价值函数推断转换动态'}
{'arxiv_id': 'arXiv:2501.09080', 'title': 'Average-Reward Reinforcement Learning with Entropy Regularization', 'authors': 'Jacob Adamczyk, Volodymyr Makarenko, Stas Tiomkin, Rahul V. Kulkarni', 'link': 'https://arxiv.org/abs/2501.09080', 'abstract': 'The average-reward formulation of reinforcement learning (RL) has drawn increased interest in recent years due to its ability to solve temporally-extended problems without discounting. Independently, RL algorithms have benefited from entropy-regularization: an approach used to make the optimal policy stochastic, thereby more robust to noise. Despite the distinct benefits of the two approaches, the combination of entropy regularization with an average-reward objective is not well-studied in the literature and there has been limited development of algorithms for this setting. To address this gap in the field, we develop algorithms for solving entropy-regularized average-reward RL problems with function approximation. We experimentally validate our method, comparing it with existing algorithms on standard benchmarks for RL.', 'abstract_zh': '近年来，平均回报形式的强化学习（RL）因其能够解决非贴现的时空延伸问题而引起了越来越多的关注。独立地，RL算法也受益于熵正则化：一种使最优策略具有随机性的方法，从而使其更具噪声鲁棒性。尽管这两种方法各自具有不同的优势，但有关将熵正则化与平均回报目标相结合的研究在文献中较少，针对这一设置的算法开发也相对有限。为了弥补这一研究空白，我们开发了一种基于函数逼近解决熵正则化平均回报RL问题的算法。并通过实验验证了该方法，并将其与其他现有算法在标准的RL基准测试中进行了比较。', 'title_zh': '带熵正则化的平均奖励强化学习'}
{'arxiv_id': 'arXiv:2501.09056', 'title': 'Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language Models through Simulation and Task Decomposition', 'authors': 'Sneheel Sarangi, Maha Elgarf, Hanan Salam', 'link': 'https://arxiv.org/abs/2501.09056', 'abstract': 'Theory of Mind (ToM) is the ability to understand and reflect on the mental states of others. Although this capability is crucial for human interaction, testing on Large Language Models (LLMs) reveals that they possess only a rudimentary understanding of it. Although the most capable closed-source LLMs have come close to human performance on some ToM tasks, they still perform poorly on complex variations of the task that involve more structured reasoning. In this work, we utilize the concept of "pretend-play", or ``Simulation Theory\'\' from cognitive psychology to propose ``Decompose-ToM\'\': an LLM-based inference algorithm that improves model performance on complex ToM tasks. We recursively simulate user perspectives and decompose the ToM task into a simpler set of functions: subject identification, question-reframing, world model updation, and knowledge availability. We test the algorithm on higher-order ToM tasks and a task testing for ToM capabilities in a conversational setting, demonstrating that our approach shows significant improvement across models compared to baseline methods while requiring minimal prompt tuning across tasks and no additional model training.', 'abstract_zh': '理论心理（ToM）是指理解并反思他人心理状态的能力。尽管这一能力对于人类的互动至关重要，但在对大型语言模型（LLMs）的测试中显示，它们仅具备初级的ToM理解能力。尽管最强大的闭源LLMs已经在某些ToM任务上接近了人类的表现，但在涉及更复杂推理的结构化任务中仍然表现不佳。在本研究中，我们利用认知心理学中的“假装游戏”概念，即“模拟理论”，提出了一种基于LLM的推理算法——“分解ToM”：该算法能够提高模型在复杂ToM任务中的性能。我们递归地模拟用户视角，并将ToM任务分解为更简单的一组功能：主体识别、问题重新表述、世界模型更新以及知识可用性。我们测试了该算法在高阶ToM任务和对话情境中测试ToM能力的表现，结果显示，相较于基线方法，我们的方法在各个模型上表现出了显著的改进，同时仅需最少的提示调整，并且无需额外的模型训练。', 'title_zh': 'Decompose-ToM：通过模拟和任务分解增强大型语言模型的理论心智推理能力'}
{'arxiv_id': 'arXiv:2501.09051', 'title': 'Polyp detection in colonoscopy images using YOLOv11', 'authors': 'Alok Ranjan Sahoo, Satya Sangram Sahoo, Pavan Chakraborty', 'link': 'https://arxiv.org/abs/2501.09051', 'abstract': 'Colorectal cancer (CRC) is one of the most commonly diagnosed cancers all over the world. It starts as a polyp in the inner lining of the colon. To prevent CRC, early polyp detection is required. Colonosopy is used for the inspection of the colon. Generally, the images taken by the camera placed at the tip of the endoscope are analyzed by the experts manually. Various traditional machine learning models have been used with the rise of machine learning. Recently, deep learning models have shown more effectiveness in polyp detection due to their superiority in generalizing and learning small features. These deep learning models for object detection can be segregated into two different types: single-stage and two-stage. Generally, two stage models have higher accuracy than single stage ones but the single stage models have low inference time. Hence, single stage models are easy to use for quick object detection. YOLO is one of the singlestage models used successfully for polyp detection. It has drawn the attention of researchers because of its lower inference time. The researchers have used Different versions of YOLO so far, and with each newer version, the accuracy of the model is increasing. This paper aims to see the effectiveness of the recently released YOLOv11 to detect polyp. We analyzed the performance for all five models of YOLOv11 (YOLO11n, YOLO11s, YOLO11m, YOLO11l, YOLO11x) with Kvasir dataset for the training and testing. Two different versions of the dataset were used. The first consisted of the original dataset, and the other was created using augmentation techniques. The performance of all the models with these two versions of the dataset have been analysed.', 'abstract_zh': '结直肠癌（CRC）是全球最常见的诊断癌症之一。它起始于结肠内壁的息肉。为了预防CRC，早期息肉检测是必要的。结肠镜检查用于检查结肠。通常，端oscope头部放置的摄像头所拍摄的影像由专家手动分析。随着机器学习的发展，各种传统的机器学习模型被应用于图像分析。最近，由于其在泛化和学习小特征方面的优势，深度学习模型在息肉检测方面显示出更高的效果。这些用于对象检测的深度学习模型可以分为两种类型：单阶段和双阶段。通常，双阶段模型比单阶段模型具有更高的精度，但单阶段模型的推理时间较短。因此，单阶段模型更适合快速对象检测。YOLO是一种成功的单阶段模型，因其较低的推理时间而吸引了研究人员的关注。研究人员过去已经使用了不同版本的YOLO，并且随着每个新版本的推出，模型的精度不断提高。本文旨在评估最近发布的YOLOv11在息肉检测方面的有效性。我们使用Kvasir数据集对YOLOv11的所有五个模型（YOLOv11n、YOLOv11s、YOLOv11m、YOLOv11l、YOLOv11x）进行了训练和测试分析。我们使用了两种不同的数据集版本：一种是原始数据集，另一种是通过数据增强技术创建的数据集。我们分析了所有模型在这两个数据集版本上的表现。', 'title_zh': '使用YOLOv1算法进行结肠镜图像中的息肉检测'}
{'arxiv_id': 'arXiv:2501.09050', 'title': 'Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning', 'authors': 'Jakob Struye, Filip Lemic, Jeroen Famaey', 'link': 'https://arxiv.org/abs/2501.09050', 'abstract': 'Extended Reality is a revolutionary method of delivering multimedia content to users. A large contributor to its popularity is the sense of immersion and interactivity enabled by having real-world motion reflected in the virtual experience accurately and immediately. This user motion, mainly caused by head rotations, induces several technical challenges. For instance, which content is generated and transmitted depends heavily on where the user is looking. Seamless systems, taking user motion into account proactively, will therefore require accurate predictions of upcoming rotations. Training and evaluating such predictors requires vast amounts of orientational input data, which is expensive to gather, as it requires human test subjects. A more feasible approach is to gather a modest dataset through test subjects, and then extend it to a more sizeable set using synthetic data generation methods. In this work, we present a head rotation time series generator based on TimeGAN, an extension of the well-known Generative Adversarial Network, designed specifically for generating time series. This approach is able to extend a dataset of head rotations with new samples closely matching the distribution of the measured time series.', 'abstract_zh': '扩展现实（Extended Reality）是一种革命性的多媒体内容交付方法，能够通过准确而即时地反映现实世界的运动增强用户的沉浸感和互动性。这种用户运动，主要是头部旋转引起的，带来了多种技术挑战。例如，生成和传输的内容很大程度上取决于用户的视角。考虑到用户运动的无缝系统将需要准确预测即将发生的旋转。训练和评估这类预测器需要大量方位输入数据，而这会产生高昂的成本，因为需要通过人类测试对象来收集。一种更为可行的方法是从测试对象中收集少量数据集，然后使用合成数据生成方法将其扩展到更大的数据集。在本研究中，我们提出了一种基于TimeGAN的头部旋转时间序列生成器，TimeGAN是广为人知的生成对抗网络的一个扩展，专门用于生成时间序列数据。这种方法能够对头部旋转数据集扩展新的样本，使其与测量时间序列的分布接近。', 'title_zh': '使用深度学习生成扩展现实中的真实合成头部旋转数据'}
{'arxiv_id': 'arXiv:2501.09049', 'title': 'Dynamic-Aware Spatio-temporal Representation Learning for Dynamic MRI Reconstruction', 'authors': 'Dayoung Baik, Jaejun Yoo', 'link': 'https://arxiv.org/abs/2501.09049', 'abstract': 'Dynamic MRI reconstruction, one of inverse problems, has seen a surge by the use of deep learning techniques. Especially, the practical difficulty of obtaining ground truth data has led to the emergence of unsupervised learning approaches. A recent promising method among them is implicit neural representation (INR), which defines the data as a continuous function that maps coordinate values to the corresponding signal values. This allows for filling in missing information only with incomplete measurements and solving the inverse problem effectively. Nevertheless, previous works incorporating this method have faced drawbacks such as long optimization time and the need for extensive hyperparameter tuning. To address these issues, we propose Dynamic-Aware INR (DA-INR), an INR-based model for dynamic MRI reconstruction that captures the spatial and temporal continuity of dynamic MRI data in the image domain and explicitly incorporates the temporal redundancy of the data into the model structure. As a result, DA-INR outperforms other models in reconstruction quality even at extreme undersampling ratios while significantly reducing optimization time and requiring minimal hyperparameter tuning.', 'abstract_zh': '动态MRI重建是逆问题的一种，通过深度学习技术的应用，其近年来得到了飞速发展。特别是在获取真实数据方面存在实际困难，促进了无监督学习方法的出现。在这些方法中，最近一种颇有前景的方法是隐式神经表示（INR），它将数据定义为一个连续函数，该函数将坐标值映射到相应的信号值。这使得仅凭不完整的测量数据就可以填充缺失信息并有效解决逆问题。然而，先前使用这种方法的研究也面临一些缺点，如优化时间较长以及需要大量超参数调整。为了解决这些问题，我们提出了一种基于INR的动态MRI重建模型——动态感知INR（DA-INR），该模型可以在图像域中捕捉动态MRI数据的空间和时间连续性，并明确地将数据的时间冗余性纳入模型结构中。结果表明，DA-INR 在极低采样比例下仍能显著提高重建质量，同时大大减少优化时间并需要较少的超参数调整。', 'title_zh': '动态感知空时表示学习在动态MRI重建中的应用'}
{'arxiv_id': 'arXiv:2501.09045', 'title': 'Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities', 'authors': 'Adam Goodge, Wee Siong Ng, Bryan Hooi, See Kiong Ng', 'link': 'https://arxiv.org/abs/2501.09045', 'abstract': 'Foundation models have revolutionized artificial intelligence, setting new benchmarks in performance and enabling transformative capabilities across a wide range of vision and language tasks. However, despite the prevalence of spatio-temporal data in critical domains such as transportation, public health, and environmental monitoring, spatio-temporal foundation models (STFMs) have not yet achieved comparable success. In this paper, we articulate a vision for the future of STFMs, outlining their essential characteristics and the generalization capabilities necessary for broad applicability. We critically assess the current state of research, identifying gaps relative to these ideal traits, and highlight key challenges that impede their progress. Finally, we explore potential opportunities and directions to advance research towards the aim of effective and broadly applicable STFMs.', 'abstract_zh': '基础模型已经革新了人工智能领域，设立了性能的新标准，并在一系列视觉和语言任务中实现了变革性的能力。然而，尽管在诸如交通、公共健康和环境监测等关键领域中空间-时间数据普遍存在，空间-时间基础模型（STFMs）尚未取得相媲美的成功。本文阐述了STFMs未来的愿景，详述了它们的核心特征以及实现广泛应用所需的泛化能力。我们对当前的研究状态进行了批判性评估，指出了与这些理想特质相比存在的差距，并突显了阻碍其发展的关键挑战。最后，我们探讨了促进有效且广泛应用的STFMs研究的潜在机会和方向。', 'title_zh': '空间-时间基础模型：视觉、挑战与机遇'}
{'arxiv_id': 'arXiv:2501.09044', 'title': 'TCMM: Token Constraint and Multi-Scale Memory Bank of Contrastive Learning for Unsupervised Person Re-identification', 'authors': 'Zheng-An Zhu, Hsin-Che Chien, Chen-Kuo Chiang', 'link': 'https://arxiv.org/abs/2501.09044', 'abstract': 'This paper proposes the ViT Token Constraint and Multi-scale Memory bank (TCMM) method to address the patch noises and feature inconsistency in unsupervised person re-identification works. Many excellent methods use ViT features to obtain pseudo labels and clustering prototypes, then train the model with contrastive learning. However, ViT processes images by performing patch embedding, which inevitably introduces noise in patches and may compromise the performance of the re-identification model. On the other hand, previous memory bank based contrastive methods may lead data inconsistency due to the limitation of batch size. Furthermore, existing pseudo label methods often discard outlier samples that are difficult to cluster. It sacrifices the potential value of outlier samples, leading to limited model diversity and robustness. This paper introduces the ViT Token Constraint to mitigate the damage caused by patch noises to the ViT architecture. The proposed Multi-scale Memory enhances the exploration of outlier samples and maintains feature consistency. Experimental results demonstrate that our system achieves state-of-the-art performance on common benchmarks. The project is available at \\href{this https URL}{this https URL}.', 'abstract_zh': '本文提出了一种ViT Token Constraint和多尺度记忆库（TCMM）方法，以解决无监督行人再识别工作中由于patches噪声和特征不一致性所带来的问题。许多优秀的无监督行人再识别方法利用ViT特征获取伪标签和聚类原型，然后通过对比学习训练模型。然而，ViT通过patch嵌入处理图像，不可避免地在patches中引入噪声，进而损害再识别模型的性能。另一方面，基于记忆库的对比学习方法因批次大小的限制可能引发数据不一致的问题。此外，现有的伪标签方法往往舍弃难以聚类的离群样本，这牺牲了离群样本的潜在价值，导致模型的多样性和鲁棒性受限。本文引入了ViT Token Constraint以减轻patch噪声对ViT架构的损害。提出的多尺度记忆库增强了对离群样本的探索，并保持了特征的一致性。实验结果表明，我们的系统在常见基准上的性能达到了最先进的水平。该项目的源代码可在以下链接获取：\\href{this https URL}{this https URL}。', 'title_zh': 'TCMM：对比学习的 token 约束和多尺度记忆库无监督行人重识别'}
{'arxiv_id': 'arXiv:2501.09039', 'title': "Playing Devil's Advocate: Unmasking Toxicity and Vulnerabilities in Large Vision-Language Models", 'authors': 'Abdulkadir Erol, Trilok Padhi, Agnik Saha, Ugur Kursuncu, Mehmet Emin Aktas', 'link': 'https://arxiv.org/abs/2501.09039', 'abstract': 'The rapid advancement of Large Vision-Language Models (LVLMs) has enhanced capabilities offering potential applications from content creation to productivity enhancement. Despite their innovative potential, LVLMs exhibit vulnerabilities, especially in generating potentially toxic or unsafe responses. Malicious actors can exploit these vulnerabilities to propagate toxic content in an automated (or semi-) manner, leveraging the susceptibility of LVLMs to deception via strategically crafted prompts without fine-tuning or compute-intensive procedures. Despite the red-teaming efforts and inherent potential risks associated with the LVLMs, exploring vulnerabilities of LVLMs remains nascent and yet to be fully addressed in a systematic manner. This study systematically examines the vulnerabilities of open-source LVLMs, including LLaVA, InstructBLIP, Fuyu, and Qwen, using adversarial prompt strategies that simulate real-world social manipulation tactics informed by social theories. Our findings show that (i) toxicity and insulting are the most prevalent behaviors, with the mean rates of 16.13% and 9.75%, respectively; (ii) Qwen-VL-Chat, LLaVA-v1.6-Vicuna-7b, and InstructBLIP-Vicuna-7b are the most vulnerable models, exhibiting toxic response rates of 21.50%, 18.30% and 17.90%, and insulting responses of 13.40%, 11.70% and 10.10%, respectively; (iii) prompting strategies incorporating dark humor and multimodal toxic prompt completion significantly elevated these vulnerabilities. Despite being fine-tuned for safety, these models still generate content with varying degrees of toxicity when prompted with adversarial inputs, highlighting the urgent need for enhanced safety mechanisms and robust guardrails in LVLM development.', 'abstract_zh': '大型视觉语言模型（LVLMs）的迅速发展提升了其能力，从而使内容创作和生产力提升等方面的应用成为可能。尽管这些模型具有创新潜力，但它们也表现出某些脆弱性，特别是在生成潜在有害或不安全的回应方面尤为突出。恶意行为者可以通过利用这些脆弱性，以自动化（或半自动化）的方式传播有害内容，并通过精心设计的提示来欺骗LVLMs，无需进行微调或密集计算的过程。尽管对LVLMs进行了红队测试，并存在固有的潜在风险，但探索LVLMs的脆弱性仍处于起步阶段，尚未以系统化的方式得到充分解决。本研究系统地考察了开源LVLMs（包括LLaVA、InstructBLIP、Fuyu和Qwen）的脆弱性，使用通过社会理论启发的对抗性提示策略模拟现实生活中的社会操纵技巧。研究结果表明：（i）毒性和侮辱是最常见的行为，平均率为16.13%和9.75%；（ii）Qwen-VL-Chat、LLaVA-v1.6-Vicuna-7b和InstructBLIP-Vicuna-7b是表现最为脆弱的模型，展现出的有害回应率为21.50%、18.30%和17.90%，侮辱回应率为13.40%、11.70%和10.10%；（iii）包含黑色幽默和多模态有害提示的提示策略显著增强了这些脆弱性。尽管这些模型经过了安全微调，但在受到对手输入提示时，仍然会生成不同程度的有害内容，这突显了在LVLM开发中增强安全机制和强化护栏的紧迫性。', 'title_zh': '扮演魔鬼代言人：揭示大型视觉-语言模型中的毒性与脆弱性'}
{'arxiv_id': 'arXiv:2501.09038', 'title': 'Do generative video models learn physical principles from watching videos?', 'authors': 'Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos', 'link': 'https://arxiv.org/abs/2501.09038', 'abstract': "AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn ``world models'' that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at this https URL; code at this https URL.", 'abstract_zh': 'AI视频生成正处于一场革命之中，其质量和现实感正迅速提升。这些进步引发了激烈的科学辩论：视频模型是否学会了“世界模型”，从而发现了物理定律——或者仅仅只是高级的像素预测器，能够在不理解现实物理原理的情况下达到视觉现实感？我们通过开发Physics-IQ，一个只能通过深刻理解各种物理原理（如流体力学、光学、固体力学、磁学和热力学）来解决的综合性基准数据集，来探讨这个问题。我们发现，当前一系列模型（Sora、Runway、Pika、Lumière、Stable Video Diffusion和VideoPoet）对物理原理的理解极为有限，与视觉现实感无关。然而，一些测试案例已经可以成功解决，这表明仅通过观察来获取某些物理原理可能是可能的，但仍面临重大挑战。虽然我们预计未来将有迅速的发展，但我们的研究表明，视觉现实感并不意味着物理理解。我们的项目页面在此 [此链接]；代码在此 [此链接]。', 'title_zh': '生成式视频模型在观看视频时是否学习了物理原理？'}
{'arxiv_id': 'arXiv:2501.09031', 'title': 'Synthetic Data and Health Privacy', 'authors': 'Gwénolé Abgrall, Xavier Monnet, Anmol Arora', 'link': 'https://arxiv.org/abs/2501.09031', 'abstract': 'This Viewpoint discusses generative artificial intelligence and safeguarding privacy by using synthetic data as a substitute for private health data.', 'abstract_zh': '本文观点探讨了使用合成数据作为私人健康数据替代品以保障隐私的生成型人工智能技术。', 'title_zh': '合成数据与健康隐私'}
{'arxiv_id': 'arXiv:2501.09029', 'title': 'Enhancing Data Integrity through Provenance Tracking in Semantic Web Frameworks', 'authors': 'Nilesh Jain', 'link': 'https://arxiv.org/abs/2501.09029', 'abstract': "This paper explores the integration of provenance tracking systems within the context of Semantic Web technologies to enhance data integrity in diverse operational environments. SURROUND Australia Pty Ltd demonstrates innovative applica-tions of the PROV Data Model (PROV-DM) and its Semantic Web variant, PROV-O, to systematically record and manage provenance information across multiple data processing domains. By employing RDF and Knowledge Graphs, SURROUND ad-dresses the critical challenges of shared entity identification and provenance granularity. The paper highlights the company's architecture for capturing comprehensive provenance data, en-abling robust validation, traceability, and knowledge inference. Through the examination of two projects, we illustrate how provenance mechanisms not only improve data reliability but also facilitate seamless integration across heterogeneous systems. Our findings underscore the importance of sophisticated provenance solutions in maintaining data integrity, serving as a reference for industry peers and academics engaged in provenance research and implementation.", 'abstract_zh': '本文探讨了在语义网技术的背景下集成来源追踪系统，以增强不同操作环境中的数据完整性。SURROUND Australia Pty Ltd展示了PROV数据模型（PROV-DM）及其语义网变体PROV-O在多个数据处理领域的创新应用，通过系统地记录和管理来源信息，解决了共享实体标识和来源细粒度的关键挑战。文章强调了该公司捕获全面来源数据的架构，能够实现稳健的验证、可追踪性和知识推断。通过两个项目的案例分析，我们说明了来源机制不仅提高了数据可靠性，还促进了异构系统之间的无缝集成。我们的研究结果突显了复杂来源解决方案在维护数据完整方面的重要性，并为从事来源研究与实施的业界同行和学术界提供了参考。', 'title_zh': '通过语义网框架中的来源追踪提高数据完整性'}
{'arxiv_id': 'arXiv:2501.09026', 'title': 'Intelligent Anti-Money Laundering Solution Based upon Novel Community Detection in Massive Transaction Networks on Spark', 'authors': 'Xurui Li, Xiang Cao, Xuetao Qiu, Jintao Zhao, Jianbin Zheng', 'link': 'https://arxiv.org/abs/2501.09026', 'abstract': 'Criminals are using every means available to launder the profits from their illegal activities into ostensibly legitimate assets. Meanwhile, most commercial anti-money laundering systems are still rule-based, which cannot adapt to the ever-changing tricks. Although some machine learning methods have been proposed, they are mainly focused on the perspective of abnormal behavior for single accounts. Considering money laundering activities are often involved in gang criminals, these methods are still not intelligent enough to crack down on criminal gangs all-sidedly. In this paper, a systematic solution is presented to find suspicious money laundering gangs. A temporal-directed Louvain algorithm has been proposed to detect communities according to relevant anti-money laundering patterns. All processes are implemented and optimized on Spark platform. This solution can greatly improve the efficiency of anti-money laundering work for financial regulation agencies.', 'abstract_zh': '犯罪分子正在利用所有可用的方法，将非法活动所获得的利润洗白为表面上合法的资产。与此同时，大多数商业反洗钱系统仍然基于规则，无法适应不断变化的手法。尽管已经提出了一些机器学习方法，但这些方法主要侧重于单一账户的异常行为视角。考虑到洗钱活动通常涉及团伙犯罪，这些方法仍然不够智能，无法全方位打击犯罪团伙。在此论文中，我们提出了一种系统性的解决方案，用于发现可疑的洗钱团伙。我们提出了一种时间导向的Louvain算法，根据相关的反洗钱模式检测社区。所有过程都在Spark平台上实现和优化。该解决方案可以大大提高金融监管机构反洗钱工作的效率。', 'title_zh': '基于新型社区检测的_spark大交易网络智能反洗钱解决方案'}
{'arxiv_id': 'arXiv:2501.09025', 'title': 'Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy Measures', 'authors': 'Marc Schmitt, Pantelis Koutroumpis', 'link': 'https://arxiv.org/abs/2501.09025', 'abstract': 'The digital age, driven by the AI revolution, brings significant opportunities but also conceals security threats, which we refer to as cyber shadows. These threats pose risks at individual, organizational, and societal levels. This paper examines the systemic impact of these cyber threats and proposes a comprehensive cybersecurity strategy that integrates AI-driven solutions, such as Intrusion Detection Systems (IDS), with targeted policy interventions. By combining technological and regulatory measures, we create a multilevel defense capable of addressing both direct threats and indirect negative externalities. We emphasize that the synergy between AI-driven solutions and policy interventions is essential for neutralizing cyber threats and mitigating their negative impact on the digital economy. Finally, we underscore the need for continuous adaptation of these strategies, especially in response to the rapid advancement of autonomous AI-driven attacks, to ensure the creation of secure and resilient digital ecosystems.', 'abstract_zh': '数字化时代，受人工智能革命的推动，带来了巨大的机遇，同时也潜藏了安全威胁，我们称之为网络阴影。这些威胁在个人、组织和社会层面均构成风险。本文探讨了这些网络威胁的系统影响，并提出了一种综合性的网络安全策略，该策略结合了基于人工智能的解决方案，如入侵检测系统（IDS），并结合了有针对性的政策干预措施。通过结合技术和监管措施，我们构建了一种多层次的防御体系，能够应对直接威胁和间接的负面外溢效应。我们强调，基于人工智能的解决方案与政策干预措施的协同作用对于抑制网络威胁及其对数字经济的负面影响至关重要。最后，我们强调有必要根据自主人工智能驱动攻击的快速进展不断调整这些策略，以确保建立安全和韧性并存的数字生态系统。', 'title_zh': '网络影子：利用人工智能与目标导向政策措施消除安全威胁'}
{'arxiv_id': 'arXiv:2501.09021', 'title': 'Navigating Ethical Challenges in Generative AI-Enhanced Research: The ETHICAL Framework for Responsible Generative AI Use', 'authors': 'Douglas Eacersall, Lynette Pretorius, Ivan Smirnov, Erika Spray, Sam Illingworth, Ritesh Chugh, Sonja Strydom, Dianne Stratton-Maher, Jonathan Simmons, Isaac Jennings, Rian Roux, Ruth Kamrowski, Abigail Downie, Chee Ling Thong, Katharine A. Howell', 'link': 'https://arxiv.org/abs/2501.09021', 'abstract': 'The rapid adoption of generative artificial intelligence (GenAI) in research presents both opportunities and ethical challenges that should be carefully navigated. Although GenAI tools can enhance research efficiency through automation of tasks such as literature review and data analysis, their use raises concerns about aspects such as data accuracy, privacy, bias, and research integrity. This paper develops the ETHICAL framework, which is a practical guide for responsible GenAI use in research. Employing a constructivist case study examining multiple GenAI tools in real research contexts, the framework consists of seven key principles: Examine policies and guidelines, Think about social impacts, Harness understanding of the technology, Indicate use, Critically engage with outputs, Access secure versions, and Look at user agreements. Applying these principles will enable researchers to uphold research integrity while leveraging GenAI benefits. The framework addresses a critical gap between awareness of ethical issues and practical action steps, providing researchers with concrete guidance for ethical GenAI integration. This work has implications for research practice, institutional policy development, and the broader academic community while adapting to an AI-enhanced research landscape. The ETHICAL framework can serve as a foundation for developing AI literacy in academic settings and promoting responsible innovation in research methodologies.', 'abstract_zh': '生成式人工智能（GenAI）在研究中的快速应用既带来了机遇也引发了伦理挑战，需要谨慎应对。虽然GenAI工具通过自动化文献回顾和数据分析等任务可以提高研究效率，但其使用也引发了关于数据准确性、隐私、偏见和研究诚信等方面的担忧。本文提出了一个名为ETHICAL框架，这是一个可行的指南，旨在促进负责任的GenAI研究应用。该框架通过构建主义案例研究，以实际研究情境中的多种GenAI工具为对象，包含七个核心原则：审视政策和指导方针、考虑社会影响、掌握技术理解、表明使用、批判性参与输出、访问安全版本和查看用户协议。遵循这些原则可以确保研究人员在利用GenAI优势的同时，也能够维护研究诚信。该框架填补了伦理问题意识与实际行动步骤之间的关键空白，为研究人员提供了具体指导，以实现伦理的GenAI集成。这项工作对研究实践、机构政策制定以及更广泛的学术社区具有重要意义，尤其是面对AI增强的研究环境时。ETHICAL框架可以作为在学术环境中培养AI素养的基础，并促进研究方法中的负责任创新。', 'title_zh': '生成式人工智能增强研究中的伦理挑战导航：负责任使用生成式人工智能的ETHICAL框架'}
{'arxiv_id': 'arXiv:2407.12471', 'title': 'Characterization of Political Polarized Users Attacked by Language Toxicity on Twitter', 'authors': 'Wentao Xu', 'link': 'https://arxiv.org/abs/2407.12471', 'abstract': 'Understanding the dynamics of language toxicity on social media is important for us to investigate the propagation of misinformation and the development of echo chambers for political scenarios such as U.S. presidential elections. Recent research has used large-scale data to investigate the dynamics across social media platforms. However, research on the toxicity dynamics is not enough. This study aims to provide a first exploration of the potential language toxicity flow among Left, Right and Center users. Specifically, we aim to examine whether Left users were easier to be attacked by language toxicity. In this study, more than 500M Twitter posts were examined. It was discovered that Left users received much more toxic replies than Right and Center users.', 'abstract_zh': '研究社交媒体上的语言毒性动态对于我们调查错误信息的传播以及政治情境（如美国大选）中的回声室现象具有重要意义。近年来，研究已经利用大规模数据跨平台研究这些动态。然而，有关语言毒性动态的研究仍然不足。本研究旨在首次探索左翼、右翼和中间派用户之间潜在的语言毒性流动。具体而言，我们旨在探讨左翼用户是否更容易受到语言毒性攻击。本研究共分析了超过5亿条推特帖子，发现左翼用户收到的毒害回复远多于右翼和中间派用户。', 'title_zh': 'Twitter上受语言毒性影响的极化用户特征分析'}
