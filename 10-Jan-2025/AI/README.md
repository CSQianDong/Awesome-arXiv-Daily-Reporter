# Neuro-Symbolic AI in 2024: A Systematic Review 

**Title (ZH)**: 2024年神经符号AI综述：系统评价 

**Authors**: Brandon C. Colelough, William Regli  

**Link**: [PDF](https://arxiv.org/pdf/2501.05435)  

**Abstract**: Background: The field of Artificial Intelligence has undergone cyclical periods of growth and decline, known as AI summers and winters. Currently, we are in the third AI summer, characterized by significant advancements and commercialization, particularly in the integration of Symbolic AI and Sub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.
Methods: The review followed the PRISMA methodology, utilizing databases such as IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion criteria targeted peer-reviewed papers published between 2020 and 2024. Papers were screened for relevance to Neuro-Symbolic AI, with further inclusion based on the availability of associated codebases to ensure reproducibility.
Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria and were analyzed in detail. The majority of research efforts are concentrated in the areas of learning and inference (63%), logic and reasoning (35%), and knowledge representation (44%). Explainability and trustworthiness are less represented (28%), with Meta-Cognition being the least explored area (5%). The review identifies significant interdisciplinary opportunities, particularly in integrating explainability and trustworthiness with other research areas.
Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with concentrated efforts in learning and inference. Significant gaps remain in explainability, trustworthiness, and Meta-Cognition. Addressing these gaps through interdisciplinary research will be crucial for advancing the field towards more intelligent, reliable, and context-aware AI systems. 

**Abstract (ZH)**: 背景：人工智能领域经历了周期性的繁荣与衰退，分别被称为AI夏和AI冬。当前，我们正处于第三个AI夏，特点是取得了重要的进步和商业化，特别是在符号AI与亚符号AI的融合方面，催生了神经符号AI。
方法：本次回顾采用了PRISMA方法，并利用IEEE Explore、Google Scholar、arXiv、ACM和SpringerLink等数据库。纳入标准为2020年至2024年期间发表的同行评审论文。筛选出与神经符号AI相关的论文，并进一步纳入具有相关代码库的论文以确保可重复性。
结果：从最初的1,428篇论文中，有167篇符合纳入标准并进行了详细分析。多数研究集中在学习和推理（63%）、逻辑和推理（35%）以及知识表示（44%）领域。可解释性和可信度的代表比例较低（28%），元认知是最少研究的领域（5%）。回顾发现，特别是在将可解释性和可信度与其他研究领域相结合方面存在重要的跨学科机会。
结论：自2020年以来，神经符号AI研究取得了快速发展，尤其集中在学习和推理方面。然而，在可解释性、可信度和元认知方面仍存在较大差距。通过跨学科研究来填补这些差距将是推动该领域向更加智能、可靠、和上下文感知的AI系统发展的关键。 

---
# Developing a Foundation of Vector Symbolic Architectures Using Category Theory 

**Title (ZH)**: 使用范畴论构建向量符号架构的基础 

**Authors**: Nolan P Shaw, P Michael Furlong, Britt Anderson, Jeff Orchard  

**Link**: [PDF](https://arxiv.org/pdf/2501.05368)  

**Abstract**: At the risk of overstating the case, connectionist approaches to machine learning, i.e. neural networks, are enjoying a small vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, we conduct a brief literature survey demonstrating the lacking intersection of these two topics, provide a list of desiderata for VSAs, and propose that VSAs may be understood as a (division) rig in a category enriched over a monoid in Met (the category of Lawvere metric spaces). This final contribution suggests that VSAs may be generalised beyond current implementations. It is our hope that grounding VSAs in category theory will lead to more rigorous connections with other research, both within and beyond, learning and cognition. 

**Abstract (ZH)**: 冒夸大其词之嫌，连接主义方法，即神经网络，在当前正经历一个小规模的流行。然而，这些方法需要大量的数据，并且生成的模型对人类来说是难以理解的。一种与神经网络和基于梯度的学习方法兼容，但能够明确建模组合性的替代框架是向量符号架构（VSA）。VSAs 是一类在高维向量表示上的代数运算。它们起源于认知科学，为了解决神经处理与人类进行的符号推理的统一问题。尽管机器学习方法已经受益于范畴论的分析，但 VSAs 尚未受到类似待遇。在本文中，我们首次尝试将范畴论应用于 VSAs。具体而言，我们进行了简要的文献综述，证明了这两个领域间的交集不足，列出了 VSAs 的期望特性，并提出 VSAs 可以被视为一个在类 Monoid（一类小品类）上丰富化的（除法）rig 在 Met（Lawvere 拓扑度量空间类别）中的类别中的对象。这一最后的贡献暗示 VSAs 也许可以超越当前的实现进行进一步的泛化。我们希望将 VSAs 基础奠定在范畴论之上，将促进 VSAs 与其他研究领域内的研究更为严谨的联系，无论是学习领域还是认知领域。 

---
# Search-o1: Agentic Search-Enhanced Large Reasoning Models 

**Title (ZH)**: Search-o1: 行动力搜寻增强的大规模推理模型 

**Authors**: Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou  

**Link**: [PDF](https://arxiv.org/pdf/2501.05366)  

**Abstract**: Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \url{this https URL}. 

**Abstract (ZH)**: 类似于OpenAI-o1的大型推理模型（LRMs）通过大规模强化学习展示了令人印象深刻的渐进推理能力。然而，它们的扩展推理过程常常受到知识不足的影响，导致频繁的不确定性及潜在的错误。为解决这一限制，我们引入了**Search-o1**，一种增强LRMs的框架，该框架结合了具有代理检索增强生成（RAG）机制的推理文档模块（Reason-in-Documents）。Search-o1将代理式的搜索工作流整合到推理过程中，使 LRMs 在遇到不确定的知识点时能够动态检索外部知识。此外，由于检索到的文档内容冗长，我们设计了一个独立的推理文档模块来深入分析检索到的信息，然后再将其注入推理链中，从而减少噪音并保持清晰的推理流程。在科学、数学和编程等复杂推理任务以及六个开放式QA基准测试中的大量实验表明，Search-o1表现出色。这种方法增强了LRMs在复杂推理任务中的可信度和应用性，为更可靠且多功能的智能系统铺平了道路。代码可在以下链接获取：\url{this https URL}。 

---
# Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments 

**Title (ZH)**: 动态拍卖环境中离线政策评估及反事实方法的研究 

**Authors**: Ritam Guha, Nilavra Pathak  

**Link**: [PDF](https://arxiv.org/pdf/2501.05278)  

**Abstract**: Counterfactual estimators are critical for learning and refining policies using logged data, a process known as Off-Policy Evaluation (OPE). OPE allows researchers to assess new policies without costly experiments, speeding up the evaluation process. Online experimental methods, such as A/B tests, are effective but often slow, thus delaying the policy selection and optimization process.
In this work, we explore the application of OPE methods in the context of resource allocation in dynamic auction environments. Given the competitive nature of environments where rapid decision-making is crucial for gaining a competitive edge, the ability to quickly and accurately assess algorithmic performance is essential. By utilizing counterfactual estimators as a preliminary step before conducting A/B tests, we aim to streamline the evaluation process, reduce the time and resources required for experimentation, and enhance confidence in the chosen policies. Our investigation focuses on the feasibility and effectiveness of using these estimators to predict the outcomes of potential resource allocation strategies, evaluate their performance, and facilitate more informed decision-making in policy selection. Motivated by the outcomes of our initial study, we envision an advanced analytics system designed to seamlessly and dynamically assess new resource allocation strategies and policies. 

**Abstract (ZH)**: 反事实估计器对于利用日志数据学习和优化策略至关重要，这一过程被称为离策评估（Off-Policy Evaluation, OPE）。OPE 允许研究人员通过低成本的方法评估新的策略，从而加快评估过程。在线实验方法，如 A/B 测试，虽然有效但通常效率较低，因此可能会延迟策略的选择和优化过程。

在本项研究中，我们探讨了将 OPE 方法应用于动态拍卖环境中资源分配的情境。考虑到这些环境中快速决策对于取得竞争优势的重要性，快速而准确地评估算法性能是必不可少的。通过在进行 A/B 测试之前利用反事实估计器作为初步步骤，我们旨在简化评估过程，减少实验所需的时间和资源，并提高对所选策略的信心。我们的研究重点关注使用这些估计器来预测潜在资源分配策略的结果、评估其性能，并促进更明智的政策选择。基于我们初步研究的成果，我们构想了一个先进的分析系统，旨在无缝且动态地评估新的资源分配策略和政策。 

---
# Online Prompt and Solver Selection for Program Synthesis 

**Title (ZH)**: 程序合成中的在线提示和求解器选择 

**Authors**: Yixuan Li, Lewis Frampton, Federico Mora, Elizabeth Polgreen  

**Link**: [PDF](https://arxiv.org/pdf/2501.05247)  

**Abstract**: Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2\% more queries than the best single solver and achieves results within 4\% of the virtual best solver. 

**Abstract (ZH)**: 大型语言模型（LLMs）在程序合成领域展现出了令人印象深刻的性能。然而，这种水平的性能并非在所有任务、所有LLMs和所有提示风格中都普遍存在。在许多领域中，一个LLM表现占优，一个提示风格占优，或者调用符号求解器比使用LLM更为合适。对于用户来说，一个关键挑战在于不仅要识别何时使用LLM作为求解器是正确的选择，并确定针对给定合成任务应该使用哪种特定的LLM及其恰当的提示方式，还要确定正确的调用方法。非专家用户如果做出了错误的选择，将在结果（解决任务的数量和解决时间）和财务成本（如果通过商用API使用封闭源代码的LLM）方面蒙受损失。我们将这一选择过程视为在线学习问题。我们采用多臂老虎机算法来选择部署符号求解器、LLM及其提示组合的方式，以最大化给定的奖励函数（该奖励函数可能侧重于解决时间、合成任务的数量或解决问题的成本）。我们实现了一个名为CYANEA的方法，并在文献中的函数合成查询、语法引导合成竞赛中的查询以及从SMT问题生成的新颖未知查询上对其进行了评估。CYANEA解决了37.2%更多的查询，并且其结果与理想的虚拟最佳求解器相差不超过4%。 

---
# Multimodal-to-Text Prompt Engineering in Large Language Models Using Feature Embeddings for GNSS Interference Characterization 

**Title (ZH)**: 使用特征嵌入进行全球导航卫星系统干扰特征化的大语言模型多模态到文本提示工程 

**Authors**: Harshith Manjunath, Lucas Heublein, Tobias Feigl, Felix Ott  

**Link**: [PDF](https://arxiv.org/pdf/2501.05079)  

**Abstract**: Large language models (LLMs) are advanced AI systems applied across various domains, including NLP, information retrieval, and recommendation systems. Despite their adaptability and efficiency, LLMs have not been extensively explored for signal processing tasks, particularly in the domain of global navigation satellite system (GNSS) interference monitoring. GNSS interference monitoring is essential to ensure the reliability of vehicle localization on roads, a critical requirement for numerous applications. However, GNSS-based positioning is vulnerable to interference from jamming devices, which can compromise its accuracy. The primary objective is to identify, classify, and mitigate these interferences. Interpreting GNSS snapshots and the associated interferences presents significant challenges due to the inherent complexity, including multipath effects, diverse interference types, varying sensor characteristics, and satellite constellations. In this paper, we extract features from a large GNSS dataset and employ LLaVA to retrieve relevant information from an extensive knowledge base. We employ prompt engineering to interpret the interferences and environmental factors, and utilize t-SNE to analyze the feature embeddings. Our findings demonstrate that the proposed method is capable of visual and logical reasoning within the GNSS context. Furthermore, our pipeline outperforms state-of-the-art machine learning models in interference classification tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）是应用于多个领域，包括自然语言处理（NLP）、信息检索和推荐系统等的高级人工智能系统。尽管它们具有高度适应性和高效性，但LLMs在信号处理任务中的应用尚不广泛，特别是在全球导航卫星系统（GNSS）干扰监测领域。GNSS干扰监测对于确保道路车辆定位的可靠性至关重要，这是许多应用的关键需求。然而，基于GNSS的定位系统容易受到干扰设备（如宽带干扰器）的影响，这些干扰会损害其精度。主要目标是识别、分类和减轻这些干扰。解读GNSS快照及其相关的干扰具有显著的挑战性，这些挑战源于其固有的复杂性，包括多路径效应、多种类型的干扰、差异化的传感器特性以及卫星星座的多样性。在本文中，我们从大规模GNSS数据集中提取特征，并使用LLaVA从广泛的知识库中检索相关信息。我们采用提示工程技术来解释干扰和环境因素，并利用t-SNE分析特征嵌入。我们的研究结果表明，所提出的方法能够在GNSS上下文中进行视觉和逻辑推理。此外，我们的流程在干扰分类任务中优于现有的机器学习模型。 

---
# A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for General Industrial Process Tasks Based on Large Language Model 

**Title (ZH)**: 基于大型语言模型的文本嵌入知识软传感建模方法：适用于通用工业过程任务 

**Authors**: Shuo Tong, Han Liu, Runyuan Guo, Xueqiong Tian, Wenqing Wang, Ding Liu, Youmin Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05075)  

**Abstract**: Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, DDSSs' limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLM's pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance. 

**Abstract (ZH)**: 基于数据驱动的软传感器（Data-Driven Soft Sensors, DDSS）已成为过程工业中预测关键性能指标的主要方法。然而，在建模过程中，DDSS的发展需要复杂且昂贵的定制设计，以适应各种任务。此外，DDSS受到单一结构化数据模态的限制，限制了它们整合额外上下文知识的能力。进一步地，DDSS有限的表征学习导致在数据稀缺的情况下预测性能较弱。为了应对这些挑战，我们提出了一种名为LLM-TKESS（大型语言模型用于文本嵌入知识的软感知）的通用框架，利用大型语言模型（LLM）的强大通用问题解决能力、跨模态知识迁移能力和少量样本学习能力，增强软感知建模。具体而言，我们提出了辅助变量系列编码器（Auxiliary Variable Series Encoder, AVS Encoder），以释放LLM捕捉序列内部的时间关系和辅助变量之间空间语义关系的潜力。接着，我们提出了一种两阶段微调对齐策略：在第一阶段，通过自回归训练进行参数高效微调，使LLM能够快速适应过程变量数据，从而形成软传感器基础模型（Soft Sensing Foundation Model, SSFM）。随后，通过训练适配器，我们无需修改其架构即可将SSFM适应各种下游任务。然后，我们提出了两种基于文本嵌入知识的软传感器，将新的自然语言模态集成进来，以克服纯粹结构化数据模型的局限。此外，得益于LLM已有的世界知识，我们的模型在小样本条件下展示了出色的预测能力。以空气预热器转子的热变形为例，通过广泛的实验验证，我们证明了LLM-TKESS表现出色。 

---
# A General Retrieval-Augmented Generation Framework for Multimodal Case-Based Reasoning Applications 

**Title (ZH)**: 一种用于多模态案例推理应用的通用检索增强生成框架 

**Authors**: Ofir Marom  

**Link**: [PDF](https://arxiv.org/pdf/2501.05030)  

**Abstract**: Case-based reasoning (CBR) is an experience-based approach to problem solving, where a repository of solved cases is adapted to solve new cases. Recent research shows that Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) can support the Retrieve and Reuse stages of the CBR pipeline by retrieving similar cases and using them as additional context to an LLM query. Most studies have focused on text-only applications, however, in many real-world problems the components of a case are multimodal. In this paper we present MCBR-RAG, a general RAG framework for multimodal CBR applications. The MCBR-RAG framework converts non-text case components into text-based representations, allowing it to: 1) learn application-specific latent representations that can be indexed for retrieval, and 2) enrich the query provided to the LLM by incorporating all case components for better context. We demonstrate MCBR-RAG's effectiveness through experiments conducted on a simplified Math-24 application and a more complex Backgammon application. Our empirical results show that MCBR-RAG improves generation quality compared to a baseline LLM with no contextual information provided. 

**Abstract (ZH)**: 案例基于推理（CBR）是一种基于经验的解决问题方法，在这种方法中，通过调整已解决的案例库来解决新问题。最近的研究表明，附有检索增强生成（RAG）的大语言模型（LLMs）可以支持CBR工作流中的检索和重用阶段，通过检索相似的案例，并将其作为附加上下文提供给LLM查询。大多数研究集中在纯文本应用上，然而，在许多实际问题中，案例的组件是多模态的。在本文中，我们提出了MCBR-RAG，这是一种适用于多模态CBR应用的一般RAG框架。MCBR-RAG框架将非文本案例组件转换为文本表示，使其能够：1）学习特定应用的潜在表示，这些表示可以进行索引以供检索，2）通过结合所有案例组件来丰富对LLM的查询，提供更好的上下文。我们通过在简化版的Math-24应用和更复杂的背投棋应用中进行的实验，证明了MCBR-RAG的有效性。我们的实证结果表明，MCBR-RAG在提供上下文信息的情况下相比没有提供上下文信息的基线LLM，提高了生成质量。 

---
# ActPC-Geom: Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms 

**Title (ZH)**: ActPC-Geom：通过加速基于信息几何和多元认知机制的主动预测编码，迈向可扩展的在线神经-符号学习 

**Authors**: Ben Goertzel  

**Link**: [PDF](https://arxiv.org/pdf/2501.04832)  

**Abstract**: This paper introduces ActPC-Geom, an approach to accelerate Active Predictive Coding (ActPC) in neural networks by integrating information geometry, specifically using Wasserstein-metric-based methods for measure-dependent gradient flows. We propose replacing KL-divergence in ActPC's predictive error assessment with the Wasserstein metric, suggesting this may enhance network robustness.
To make this computationally feasible, we present strategies including: (1) neural approximators for inverse measure-dependent Laplacians, (2) approximate kernel PCA embeddings for low-rank approximations feeding into these approximators, and (3) compositional hypervector embeddings derived from kPCA outputs, with algebra optimized for fuzzy FCA lattices learned through neural architectures analyzing network states.
This results in an ActPC architecture capable of real-time online learning and integrating continuous (e.g., transformer-like or Hopfield-net-like) and discrete symbolic ActPC networks, including frameworks like OpenCog Hyperon or ActPC-Chem for algorithmic chemistry evolution. Shared probabilistic, concept-lattice, and hypervector models enable symbolic-subsymbolic integration.
Key features include (1) compositional reasoning via hypervector embeddings in transformer-like architectures for tasks like commonsense reasoning, and (2) Hopfield-net dynamics enabling associative long-term memory and attractor-driven cognitive features.
We outline how ActPC-Geom combines few-shot learning with online weight updates, enabling deliberative thinking and seamless symbolic-subsymbolic reasoning. Ideas from Galois connections are explored for efficient hybrid ActPC/ActPC-Chem processing. Finally, we propose a specialized HPC design optimized for real-time focused attention and deliberative reasoning tailored to ActPC-Geom's demands. 

**Abstract (ZH)**: 本文介绍了ActPC-Geom方法，这是一种通过整合信息几何来加速神经网络中主动预测编码（ActPC）的方法，具体而言，利用基于Wasserstein度量的方法进行依赖度量的梯度流计算。我们提出将ActPC预测误差评估中的KL散度替换为Wasserstein度量，这可能增强网络的鲁棒性。

为了使这种计算变得可行，我们提出了以下策略：（1）神经近似器来估计逆依赖度量的拉普拉斯算子，（2）近似核PCA嵌入，用作这些近似器的低秩近似输入，以及（3）从核PCA输出派生的复合超向量嵌入，利用神经架构分析网络状态，优化代数运算以学习模糊的概念格矩阵。

这使得ActPC架构能够进行实时在线学习，并能够整合连续（例如，类似于变换器或霍普フィール德网络）和离散符号ActPC网络，包括OpenCog Hyperon或ActPC-Chem等框架，用于算法化学演变。共享的概率模型、概念格模型和超向量模型支持符号与次符号方法的集成。

关键特点包括：（1）在类似于变换器的架构中通过超向量嵌入来实现组合推理，用于常识推理等任务；（2）霍普菲尔德网络动力学支持关联的长期记忆及吸引子驱动的认知特征。

我们探讨了ActPC-Geom如何结合网络更新与少样本学习，实现有意识的思考和无缝符号与次符号推理。我们研究了格连接的概念，以实现高效混合ActPC/ActPC-Chem处理。最后，我们提出了一种专门的高性能计算设计，旨在针对ActPC-Geom的需求，优化实时集中注意力和有意识推理。 

---
# AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions and Interpretation to Transform Earth System Modeling 

**Title (ZH)**: 基于AI驱动的水文 modeling 重塑，以实现精确预测和解释，从而转变地球系统 modeling 

**Authors**: Cuihui Xia, Lei Yue, Deliang Chen, Yuyang Li, Hongqiang Yang, Ancheng Xue, Zhiqiang Li, Qing He, Guoqing Zhang, Dambaru Ballab Kattel, Lei Lei, Ming Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2501.04733)  

**Abstract**: Traditional equation-driven hydrological models often struggle to accurately predict streamflow in challenging regional Earth systems like the Tibetan Plateau, while hybrid and existing algorithm-driven models face difficulties in interpreting hydrological behaviors. This work introduces HydroTrace, an algorithm-driven, data-agnostic model that substantially outperforms these approaches, achieving a Nash-Sutcliffe Efficiency of 98% and demonstrating strong generalization on unseen data. Moreover, HydroTrace leverages advanced attention mechanisms to capture spatial-temporal variations and feature-specific impacts, enabling the quantification and spatial resolution of streamflow partitioning as well as the interpretation of hydrological behaviors such as glacier-snow-streamflow interactions and monsoon dynamics. Additionally, a large language model (LLM)-based application allows users to easily understand and apply HydroTrace's insights for practical purposes. These advancements position HydroTrace as a transformative tool in hydrological and broader Earth system modeling, offering enhanced prediction accuracy and interpretability. 

**Abstract (ZH)**: 传统由方程驱动的水文模型在挑战性的地理区域（如青藏高原）中往往难以准确预测径流，而现有的算法驱动模型则在解释水文行为方面遇到了困难。本研究介绍了一种名为HydroTrace的算法驱动、数据无关模型，该模型显著优于现有方法，实现了Nash-Sutcliffe 效率为98%的优异性能，并且在未见数据上表现出强大的泛化能力。此外，HydroTrace通过利用先进的注意力机制来捕捉空间-时间变化和特征特定影响，从而能够定量并空间细化径流分配，并解释冰川-雪-径流相互作用和季风动态等水文行为。此外，基于大型语言模型（LLM）的应用使用户能够轻松理解和应用于实际目的HydroTrace的见解。这些进步将HydroTrace定位为水文及更广泛地球系统建模领域的变革性工具，提供更高的预测准确性和可解释性。 

---
# An Empirical Study of Autoregressive Pre-training from Videos 

**Title (ZH)**: 来自视频的自回归预训练的实证研究 

**Authors**: Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, Jitendra Malik  

**Link**: [PDF](https://arxiv.org/pdf/2501.05453)  

**Abstract**: We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at this https URL 

**Abstract (ZH)**: 我们实证研究了基于视频的自回归预训练。为了进行这项研究，我们构建了一系列自回归视频模型，称为Toto。我们将视频视为视觉标记的序列，并训练变换器模型以自回归方式预测未来标记。我们的模型在包含超过十万亿个视觉标记的多样化视频和图像数据集上进行预训练。我们探索了不同的架构、训练和推理设计选择。我们评估了所学习的视觉表示在一系列下游任务中的性能，包括图像识别、视频分类、对象跟踪和机器人技术。我们的结果显示，尽管自回归预训练的归纳偏置相对较少，但其在所有基准测试中的性能依然具有竞争力。最后，我们发现，扩展我们的视频模型的性能曲线与语言模型的扩展曲线类似，尽管比率不同。更多细节请参考<https://...> 

---
# Consistent Flow Distillation for Text-to-3D Generation 

**Title (ZH)**: 文本到3D生成的一致流蒸馏 

**Authors**: Runjie Yan, Yinbo Chen, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05445)  

**Abstract**: Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose Consistent Flow Distillation (CFD), which addresses these limitations. We begin by leveraging the gradient of the diffusion ODE or SDE sampling process to guide the 3D generation. From the gradient-based sampling perspective, we find that the consistency of 2D image flows across different viewpoints is important for high-quality 3D generation. To achieve this, we introduce multi-view consistent Gaussian noise on the 3D object, which can be rendered from various viewpoints to compute the flow gradient. Our experiments demonstrate that CFD, through consistent flows, significantly outperforms previous methods in text-to-3D generation. 

**Abstract (ZH)**: Score Distillation Sampling (SDS) 在从图像生成模型提取 3D 生成模型方面取得了显著进展。然而，其最大似然性追求行为往往导致视觉质量下降和多样性不足，限制了其在 3D 应用中的效果。本文中，我们提出了一致流动蒸馏（CFD），以解决这些限制。我们首先利用扩散 ODE 或 SDE 采样过程的梯度来引导 3D 生成。从基于梯度的采样角度来看，不同视点下 2D 图像流动的一致性对于高质量的 3D 生成至关重要。为此，我们在 3D 对象上引入多视角一致的高斯噪声，可以从不同视角对其进行渲染以计算流动梯度。实验结果表明，通过一致流动，CFD 在文本到 3D 生成中明显优于先前的方法。 

---
# A survey of textual cyber abuse detection using cutting-edge language models and large language models 

**Title (ZH)**: 使用先进语言模型和大规模语言模型的文本网络欺凌检测综述 

**Authors**: Jose A. Diaz-Garcia, Joao Paulo Carvalho  

**Link**: [PDF](https://arxiv.org/pdf/2501.05443)  

**Abstract**: The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it. 

**Abstract (ZH)**: 社交媒体平台的成功促进了数字社区中各种形式网络欺凌现象的出现。这种欺凌表现为多种形式，包括仇恨言论、网络欺凌、情绪虐待、诱骗和裸聊。本文对社交媒体上普遍存在的不同形式的欺凌进行了全面分析，特别关注新兴技术（如语言模型和大规模语言模型）如何重塑这些网络中欺凌内容的检测与生成方式。我们探究了社交媒体欺凌蔓延的机制，分析了其心理和社会影响。此外，我们还探讨了先进语言模型的双重作用——既强调其增强自动检测系统以识别有害行为的潜力，也承认其生成有害内容的能力。本文旨在为网络安全性与伦理的持续讨论做出贡献，提供对网络欺凌不断演变的景观及其技术进步如何减少和加剧其影响的见解。 

---
# Progressive Growing of Video Tokenizers for Highly Compressed Latent Spaces 

**Title (ZH)**: 逐步增长的视频词嵌入器以实现高度压缩的潜在空间 

**Authors**: Aniruddha Mahapatra, Long Mai, Yitian Zhang, David Bourgin, Feng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05442)  

**Abstract**: Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-compression encoder surpasses that of high-compression encoders applied to original videos. This indicates that high-compression models can leverage representations from lower-compression models. Building on this insight, we develop a bootstrapped high-temporal-compression model that progressively trains high-compression blocks atop well-trained lower-compression models. Our method includes a cross-level feature-mixing module to retain information from the pretrained low-compression model and guide higher-compression blocks to capture the remaining details from the full video sequence. Evaluation of video benchmarks shows that our method significantly improves reconstruction quality while increasing temporal compression compared to direct extensions of existing video tokenizers. Furthermore, the resulting compact latent space effectively trains a video diffusion model for high-quality video generation with a reduced token budget. 

**Abstract (ZH)**: 视频分词器对于潜在视频扩散模型至关重要，它们将原始视频数据转换为时空压缩的潜在空间，以实现高效的训练。然而，要在不增加通道容量的情况下将最先进的视频分词器扩展到超过4倍的时序压缩比，面临着重大挑战。在本研究中，我们提出了一种替代方法来增强时序压缩。我们发现，使用低压缩比编码器对时序下采样视频进行重构的质量优于对原始视频应用高压缩比编码器的效果。这表明高压缩比模型可以从低压缩比模型中借用表示。基于这一洞察，我们开发了一种逐步训练高时序压缩模块的方法，该模块在良好训练的低时序压缩模块之上进行训练。我们的方法包含一个跨级别特征混合模块，用于保留预训练低压缩比模型中的信息，并引导高压缩比模块捕捉完整视频序列中的剩余细节。在视频基准评估中，我们的方法在提高重构质量的同时，相较于现有视频分词器的直接扩展提高了时序压缩比。此外，由此产生的紧凑潜在空间有效训练了一个视频扩散模型，用于高质量视频生成，而所需的分词预算减少。 

---
# From Simple to Complex Skills: The Case of In-Hand Object Reorientation 

**Title (ZH)**: 从简单技能到复杂技能：手持物体重新定向案例研究 

**Authors**: Haozhi Qi, Brent Yi, Mike Lambeta, Yi Ma, Roberto Calandra, Jitendra Malik  

**Link**: [PDF](https://arxiv.org/pdf/2501.05439)  

**Abstract**: Learning policies in simulation and transferring them to the real world has become a promising approach in dexterous manipulation. However, bridging the sim-to-real gap for each new task requires substantial human effort, such as careful reward engineering, hyperparameter tuning, and system identification. In this work, we present a system that leverages low-level skills to address these challenges for more complex tasks. Specifically, we introduce a hierarchical policy for in-hand object reorientation based on previously acquired rotation skills. This hierarchical policy learns to select which low-level skill to execute based on feedback from both the environment and the low-level skill policies themselves. Compared to learning from scratch, the hierarchical policy is more robust to out-of-distribution changes and transfers easily from simulation to real-world environments. Additionally, we propose a generalizable object pose estimator that uses proprioceptive information, low-level skill predictions, and control errors as inputs to estimate the object pose over time. We demonstrate that our system can reorient objects, including symmetrical and textureless ones, to a desired pose. 

**Abstract (ZH)**: 在模拟环境中学习策略并将其转移到现实世界已成为灵巧操作中一种有前景的方法。然而，为每个新任务桥接模拟与现实之间的差距仍需大量的人工努力，包括精细的奖励工程、超参数调优和系统识别。在此项工作中，我们提出了一种系统，利用低级技能来解决更复杂任务中遇到的挑战。具体而言，我们引入了一种基于先前获得的旋转技能的分层策略，用于手内物体重定向。该分层策略能够根据来自环境及其自身低级技能策略的反馈来选择执行哪个低级技能。与从头开始学习相比，分层策略对于分布外变化更具鲁棒性，并且能够更容易地从模拟环境转移到现实世界环境。此外，我们提出了一种可泛化的物体姿态估计器，该估计器将自我感知信息、低级技能预测和控制误差作为输入，用于估计物体随时间的动态姿态。我们展示了该系统可以将对称或无纹理的物体重定向至期望姿态。 

---
# A Novel Pathology Foundation Model by Mayo Clinic, Charit\'e, and Aignostics 

**Title (ZH)**: 由 mayo clinic、Charité 和 Aignostics 提出的一种新型病理学基础模型 

**Authors**: Maximilian Alber, Stephan Tietz, Jonas Dippel, Timo Milbich, Timothée Lesort, Panos Korfiatis, Moritz Krügener, Beatriz Perez Cancer, Neelay Shah, Alexander Möllers, Philipp Seegerer, Alexandra Carpen-Amarie, Kai Standvoss, Gabriel Dernbach, Edwin de Jong, Simon Schallenberg, Andreas Kunft, Helmut Hoffer von Ankershoffen, Gavin Schaeferle, Patrick Duffy, Matt Redlon, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Andrew Norgan  

**Link**: [PDF](https://arxiv.org/pdf/2501.05409)  

**Abstract**: Recent advances in digital pathology have demonstrated the effectiveness of foundation models across diverse applications. In this report, we present a novel vision foundation model based on the RudolfV approach. Our model was trained on a dataset comprising 1.2 million histopathology whole slide images, collected from two medical institutions: Mayo Clinic and Charité - Universtätsmedizin Berlin. Comprehensive evaluations show that our model achieves state-of-the-art performance across twenty-one public benchmark datasets, even though it is neither the largest model by parameter count nor by training dataset size. 

**Abstract (ZH)**: 近年来，数字病理学的发展表明基础模型在多种应用中的有效性。本报告中，我们提出了一种基于RudolfV方法的新型视觉基础模型。该模型是在来自Mayo Clinic和Charité - Universtätsmedizin Berlin两家医疗机构的120万张组织病理学全切片图像数据集上进行训练的。全面的评估结果显示，尽管该模型在参数量和训练数据集大小上并非最大，但其在二十一个公开基准数据集上仍达到了最先进的性能。 

---
# TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs 

**Title (ZH)**: TimeRL：基于多面体依赖图的高效深度强化学习 

**Authors**: Pedro F. Silvestre, Peter Pietzuch  

**Link**: [PDF](https://arxiv.org/pdf/2501.05408)  

**Abstract**: Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop. This results in programs with several nested loops and dynamic data dependencies between tensors. While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution. Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs. Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.
We describe TimeRL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution. TimeRL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations. TimeRL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions. Through simple PDG transformations, TimeRL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion. The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show that TimeRL executes current DRL algorithms up to 47$\times$ faster than existing DRL systems, while using 16$\times$ less GPU peak memory. 

**Abstract (ZH)**: 现代深度学习（DL）工作负载越来越多地使用复杂的深度强化学习（DRL）算法，在学习循环中生成训练数据。这导致了具有多层嵌套循环和张量之间动态数据依赖的程序。虽然带有即时执行支持的DL系统可以处理这种动态性，但它们缺乏基于图执行的优化和智能调度。然而，基于图的执行无法表达动态的张量形状，需要使用多个静态子图。这两种执行模型都会导致冗余计算、减少并行性和不高效的内存管理。

我们介绍了一种名为TimeRL的系统，该系统结合了即时执行的动态性和基于图执行的整体优化和调度。通过引入递归张量的声明式编程模型，使用户能够使用直观的递归方程定义动态依赖关系。TimeRL将递归张量翻译成具有动态依赖关系的多面体依赖图（PDG），其中依赖关系作为符号表达式。通过简单的PDG转换，TimeRL可以实现整体优化，如自动向量化、渐增性优化和操作符融合。PDG还允许计算一个高效的程序级执行调度，决定缓存分配、缓存捐赠以及GPU/CPU内存交换。研究表明，与现有的DRL系统相比，TimeRL能够将当前的DRL算法加速多达47倍，并且使用16倍少的GPU峰值内存。 

---
# On-line Policy Improvement using Monte-Carlo Search 

**Title (ZH)**: 使用蒙特卡洛搜索进行在线策略改进 

**Authors**: Gerald Tesauro, Gregory R. Galperin  

**Link**: [PDF](https://arxiv.org/pdf/2501.05407)  

**Abstract**: We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers.
We have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. In each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment. 

**Abstract (ZH)**: 我们提出了一种蒙特卡洛模拟算法，用于实时改进自适应控制器的策略。在蒙特卡洛模拟中，通过初始策略在每次模拟步骤中进行决策，统计测量每个可能行动的长期期望奖励。然后采取测量出的期望奖励最大的行动，从而实现策略的改进。该算法易于并行化，并已在IBM SP1和SP2并行RISC超级计算机上实现。

我们在国际象棋变种游戏塔克伦（Backgammon）领域应用该算法取得了初步的令人鼓舞的结果。对于从随机策略到TD-Gammon（一种极其强大的多层神经网络）的各种初始策略，我们进行了模拟并报告了结果。在每种情况下，蒙特卡洛算法都显著降低了基础玩家的错误率，降幅可达5倍以上。该算法在能够模拟环境的许多其他自适应控制应用中也具有潜在的应用价值。 

---
# TimeDP: Learning to Generate Multi-Domain Time Series with Domain Prompts 

**Title (ZH)**: TimeDP：通过领域提示生成多领域时间序列 

**Authors**: Yu-Hao Huang, Chang Xu, Yueying Wu, Wu-Jun Li, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2501.05403)  

**Abstract**: Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as "word" representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract "domain prompt" with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability. 

**Abstract (ZH)**: 时间序列生成模型对于数据增强和隐私保护等应用至关重要。大多数现有的时间序列生成模型通常设计为从一个特定领域生成数据。尽管利用其他领域数据以提高泛化能力的方法已在其他应用领域得到了证实，但对于时间序列建模而言，这一方法仍然具有挑战性，因为不同现实世界时间序列类别的模式差异较大。本文提出了一种基于领域提示的多领域时间序列扩散模型，命名为TimeDP。在TimeDP中，我们利用时间序列语义原型模块定义时间序列原型来表示时间序列的基本结构，每个原型向量作为“单词”，代表某种基本的时间序列特征。应用原型分配模块提取特定领域原型权重，用于学习生成条件中的领域提示。在采样过程中，我们从目标领域中使用少量示例提取“领域提示”，并将其作为条件生成时间序列样本。实验表明，我们的方法在领域内生成质量和对未见领域的生成能力方面均优于基线方法，提供了最先进的性能。 

---
# BRATI: Bidirectional Recurrent Attention for Time-Series Imputation 

**Title (ZH)**: BRATI：双向递归注意力时间序列插补 

**Authors**: Armando Collado-Villaverde, Pablo Muñoz, Maria D. R-Moreno  

**Link**: [PDF](https://arxiv.org/pdf/2501.05401)  

**Abstract**: Missing data in time-series analysis poses significant challenges, affecting the reliability of downstream applications. Imputation, the process of estimating missing values, has emerged as a key solution. This paper introduces BRATI, a novel deep-learning model designed to address multivariate time-series imputation by combining Bidirectional Recurrent Networks and Attention mechanisms. BRATI processes temporal dependencies and feature correlations across long and short time horizons, utilizing two imputation blocks that operate in opposite temporal directions. Each block integrates recurrent layers and attention mechanisms to effectively resolve long-term dependencies.
We evaluate BRATI on three real-world datasets under diverse missing-data scenarios: randomly missing values, fixed-length missing sequences, and variable-length missing sequences. Our findings demonstrate that BRATI consistently outperforms state-of-the-art models, delivering superior accuracy and robustness in imputing multivariate time-series data. 

**Abstract (ZH)**: 时间序列分析中的缺失数据会带来重大挑战，影响下游应用的可靠性。通过估计缺失值的插补过程已成为关键解决方案之一。本文介绍了一种名为BRATI的新颖深度学习模型，该模型结合了双向循环网络和注意力机制，旨在解决多变量时间序列插补问题。BRATI通过两个沿相反时间方向操作的插补块来处理长时间和短时间范围内的时间依赖性和特征相关性。每个插补块均整合了循环层和注意力机制，以有效解决长期依赖关系。

本文在三种具有不同缺失数据场景的真实世界数据集上评估了BRATI：随机缺失值、固定长度缺失序列和变长度缺失序列。我们的研究结果表明，BRATI在多变量时间序列数据插补方面始终优于现有最先进的模型，提供更高的准确性和鲁棒性。 

---
# Mechanistic understanding and validation of large AI models with SemanticLens 

**Title (ZH)**: 使用SemanticLens对大规模AI模型进行机制理解与验证 

**Authors**: Maximilian Dreyer, Jim Berend, Tobias Labarta, Johanna Vielhaben, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek  

**Link**: [PDF](https://arxiv.org/pdf/2501.05398)  

**Abstract**: Unlike human-engineered systems such as aeroplanes, where each component's role and dependencies are well understood, the inner workings of AI models remain largely opaque, hindering verifiability and undermining trust. This paper introduces SemanticLens, a universal explanation method for neural networks that maps hidden knowledge encoded by components (e.g., individual neurons) into the semantically structured, multimodal space of a foundation model such as CLIP. In this space, unique operations become possible, including (i) textual search to identify neurons encoding specific concepts, (ii) systematic analysis and comparison of model representations, (iii) automated labelling of neurons and explanation of their functional roles, and (iv) audits to validate decision-making against requirements. Fully scalable and operating without human input, SemanticLens is shown to be effective for debugging and validation, summarizing model knowledge, aligning reasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification), and detecting components tied to spurious correlations and their associated training data. By enabling component-level understanding and validation, the proposed approach helps bridge the "trust gap" between AI models and traditional engineered systems. We provide code for SemanticLens on this https URL and a demo on this https URL. 

**Abstract (ZH)**: 与飞机等由人类工程设计的系统不同，在这些系统中每个组件的作用和依赖关系都已明确理解，人工智能模型的内部工作原理仍然很大程度上是不可见的，这阻碍了验证过程并削弱了信任度。本文介绍了一种名为SemanticLens的通用解释方法，用于将神经网络组件（例如单个神经元）编码的隐含知识映射到如CLIP这样的基础模型的语义结构多模态空间。在这一空间中，可以实现多种独特的操作，包括（i）通过文本搜索识别编码特定概念的神经元，（ii）系统地分析和比较模型表示，（iii）自动标记神经元及其功能作用的解释，以及（iv）审核以验证决策是否符合要求。SemanticLens完全具有可扩展性，并且不需要人工干预，已被证明对调试和验证、总结模型知识、使推理与期望一致（例如，在黑色素瘤分类中遵循ABCDE规则）以及检测与错误相关联的组件及其相关训练数据非常有效。通过在组件级别实现理解和验证，该提出的方法有助于弥合人工智能模型与传统工程系统之间的“信任差距”。我们在此提供了SemanticLens的代码（此 https URL）以及演示（此 https URL）。 

---
# The global consensus on the risk management of autonomous driving 

**Title (ZH)**: 自动驾驶风险管理体系的全球共识 

**Authors**: Sebastian Krügel, Matthias Uhl  

**Link**: [PDF](https://arxiv.org/pdf/2501.05391)  

**Abstract**: Every maneuver of a vehicle redistributes risks between road users. While human drivers do this intuitively, autonomous vehicles allow and require deliberative algorithmic risk management. But how should traffic risks be distributed among road users? In a global experimental study in eight countries with different cultural backgrounds and almost 11,000 participants, we compared risk distribution preferences. It turns out that risk preferences in road traffic are strikingly similar between the cultural zones. The vast majority of participants in all countries deviates from a guiding principle of minimizing accident probabilities in favor of weighing up the probability and severity of accidents. At the national level, the consideration of accident probability and severity hardly differs between countries. The social dilemma of autonomous vehicles detected in deterministic crash scenarios disappears in risk assessments of everyday traffic situations in all countries. In no country do cyclists receive a risk bonus that goes beyond their higher vulnerability. In sum, our results suggest that a global consensus on the risk ethics of autonomous driving is easier to establish than on the ethics of crashing. 

**Abstract (ZH)**: 车辆每次操作都会重新分配道路使用者之间的风险。虽然人类司机会直观地进行这种操作，但自动驾驶车辆允许并要求进行有意识的算法风险管理。但如何在道路使用者之间分配交通风险？在全球一项涉及八个不同文化背景的国家、近11,000名参与者的实验研究中，我们比较了不同群体的风险分配偏好。结果表明，在不同文化区域内，道路交通中的风险偏好惊人地相似。所有国家的大多数参与者更倾向于权衡事故的概率和严重性，而不是单纯地减少事故发生概率。在各个国家的层面，关于事故概率和严重性的考量几乎没有差异。在确定性碰撞情景中发现的自动驾驶车辆的社会困境在所有国家的日常交通风险评估中消失。在没有任何一个国家中，骑自行车的人都获得了一个超越其较高脆弱性的风险系数。总的来说，我们的研究结果表明，在自动驾驶风险伦理方面达成全球共识比在碰撞伦理方面达成共识更容易。 

---
# Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models 

**Title (ZH)**: 大型物理模型：与大型语言模型和基础模型合作的方法研究 

**Authors**: Kristian G. Barman, Sascha Caron, Emily Sullivan, Henk W. de Regt, Roberto Ruiz de Austri, Mieke Boon, Michael Färber, Stefan Fröse, Faegheh Hasibi, Andreas Ipp, Rukshak Kapoor, Gregor Kasieczka, Daniel Kostić, Michael Krämer, Tobias Golling, Luis G. Lopez, Jesus Marco, Sydney Otten, Pawel Pawlowski, Pietro Vischia, Erik Weber, Christoph Weniger  

**Link**: [PDF](https://arxiv.org/pdf/2501.05382)  

**Abstract**: This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we call Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data - are tailored to address the demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability by testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models. 

**Abstract (ZH)**: 本文探讨了物理专用大型人工智能模型（我们称之为大型物理模型LPMs）的发展和评估理念，并提供了一个潜在的发展路线图。这些模型基于大型语言模型（LLMs）——通过广泛数据训练而来——并将针对物理研究的需求进行定制。LPMs可以独立运行，也可以作为集成框架的一部分。这个框架可以整合专门的工具，包括用于数学运算的符号推理模块，用于分析特定实验和模拟数据的框架，以及用于合成理论和科学文献的机制。首先，我们将探讨物理学界是否应该积极开发和优化专用模型，而不仅仅是依赖商业LLMs。然后，我们概述了LPMs可以通过物理学、计算机科学和科学哲学专家之间的跨学科合作来实现的方法。为了有效地整合这些模型，我们确定了三个关键支柱：发展、评估和哲学反思。发展侧重于构建能够处理物理文本、数学公式和各种物理数据的模型。评估通过测试和基准评估来评估准确性和可靠性。最后，哲学反思涵盖了对LLMs在物理学中的更广泛影响的分析，包括它们产生新的科学理解的潜力，以及可能在研究中出现的新协作模式。借鉴粒子物理实验合作的组织结构，我们提出了一种跨学科和协作的方法来构建和优化大型物理模型。这份路线图为实现这些特定于物理的大型AI模型设定了具体目标，定义了实现这些目标的途径，并识别了必须解决的挑战。 

---
# On Corrigibility and Alignment in Multi Agent Games 

**Title (ZH)**: 多智能体游戏中可纠正性和对齐性的研究 

**Authors**: Edmund Dable-Heath, Boyko Vodenicharski, James Bishop  

**Link**: [PDF](https://arxiv.org/pdf/2501.05360)  

**Abstract**: Corrigibility of autonomous agents is an under explored part of system design, with previous work focusing on single agent systems. It has been suggested that uncertainty over the human preferences acts to keep the agents corrigible, even in the face of human irrationality. We present a general framework for modelling corrigibility in a multi-agent setting as a 2 player game in which the agents always have a move in which they can ask the human for supervision. This is formulated as a Bayesian game for the purpose of introducing uncertainty over the human beliefs. We further analyse two specific cases. First, a two player corrigibility game, in which we want corrigibility displayed in both agents for both common payoff (monotone) games and harmonic games. Then we investigate an adversary setting, in which one agent is considered to be a `defending' agent and the other an `adversary'. A general result is provided for what belief over the games and human rationality the defending agent is required to have to induce corrigibility. 

**Abstract (ZH)**: 自主智能体的可纠正性是系统设计中的一个尚未充分探索的领域，先前的工作主要集中在单智能体系统上。有人建议，对人类偏好不确定性的存在可以使智能体保持可纠正性，即使在面对人类的非理性行为时也是如此。本文提供了一个通用框架，将多智能体环境中的可纠正性模型化为一个两人博弈，其中智能体始终可以采取行动请求人类监督。该框架被表述为贝叶斯博弈，以引入对人类信念的不确定性。我们进一步分析了两种特定情况。首先，一种两人可纠正性博弈，我们希望两个智能体在共付（单调）博弈和和声博弈中都能表现出可纠正性。然后，我们探讨了一种对抗设置，其中一个智能体被视为“防守型”智能体，另一个为“攻击型”智能体。本文提供了防守型智能体所需的关于博弈及其对人类理性行为的信念的一般结果，以诱导可纠正性。 

---
# Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction 

**Title (ZH)**: 流式对齐器：通过分布诱导实现高效的句子级对齐 

**Authors**: Hantao Lou, Jiaming Ji, Kaile Wang, Yaodong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05336)  

**Abstract**: The rapid advancement of large language models (LLMs) has led to significant improvements in their capabilities, but also to increased concerns about their alignment with human values and intentions. Current alignment strategies, including adaptive training and inference-time methods, have demonstrated potential in this area. However, these approaches still struggle to balance deployment complexity and capability across various tasks and difficulties. In this work, we introduce the Streaming Distribution Induce Aligner (Stream Aligner), a novel alignment paradigm that combines efficiency with enhanced performance in various tasks throughout the generation process. Stream Aligner achieves dynamic sentence-level correction by using a small model to learn the preferences of the suffix sentence, iteratively correcting the suffix sentence output by the upstream model, and then using the corrected sentence to replace the suffix sentence in subsequent generations. Compared to Aligner, our experiments demonstrate that Stream Aligner reduces reliance on the capabilities of additional models, enhances the reasoning abilities of LLMs, and decreases latency during user interaction. Specifically, Stream Aligner-2B model has achieved an improvement of 76.1% in helpfulness, 36.0% in harmlessness on the tested Llama2-70B-chat model, and Stream Aligner-8B has achieved an improvement of 3.5% on the math ability of the tested Llama3-70B-Instruct model. 

**Abstract (ZH)**: 大型语言模型（LLMs）的快速进步显著提高了其能力，但也引发了对它们与人类价值观和意图一致性问题的关注。当前的对齐策略，包括自适应训练和推理时的方法，在这一领域展现了一定的潜力。然而，这些方法仍然难以在不同任务和难度级别上平衡部署复杂性和能力。在此项研究中，我们引入了一种新的对齐范式——流式分布诱导对齐器（Stream Aligner），该范式结合了效率和在生成过程中的增强性能。Stream Aligner 通过使用一个小模型学习后续句子的偏好，并逐步修正上游模型输出的后续句子，然后将修正后的句子用于后续生成过程。与传统的对齐器相比，我们的实验表明，Stream Aligner 减少了对额外模型能力的依赖，增强了语言模型的推理能力，并降低了用户交互时的延迟。具体而言，Stream Aligner-2B 模型在测试的 Llama2-70B-chat 模型上实现了帮助性提升76.1%，无害性提升36.0%，而 Stream Aligner-8B 模型在测试的 Llama3-70B-Instruct 模型上实现了数学能力提升3.5%。 

---
# The Bakers and Millers Game with Restricted Locations 

**Title (ZH)**: 《有限地点限制下的磨坊主与面包师博弈》 

**Authors**: Simon Krogmann, Pascal Lenzner, Alexander Skopalik  

**Link**: [PDF](https://arxiv.org/pdf/2501.05334)  

**Abstract**: We study strategic location choice by customers and sellers, termed the Bakers and Millers Game in the literature. In our generalized setting, each miller can freely choose any location for setting up a mill, while each baker is restricted in the choice of location for setting up a bakery. For optimal bargaining power, a baker would like to select a location with many millers to buy flour from and with little competition from other bakers. Likewise, a miller aims for a location with many bakers and few competing millers. Thus, both types of agents choose locations to optimize the ratio of agents of opposite type divided by agents of the same type at their chosen location. Originally raised in the context of Fractional Hedonic Games, the Bakers and Millers Game has applications that range from commerce to product design.
We study the impact of location restrictions on the properties of the game. While pure Nash equilibria trivially exist in the setting without location restrictions, we show via a sophisticated, efficient algorithm that even the more challenging restricted setting admits equilibria. Moreover, the computed equilibrium approximates the optimal social welfare by a factor of at most $2\left(\frac{e}{e-1}\right)$. Furthermore, we give tight bounds on the price of anarchy/stability.
On the conceptual side, the location choice feature adds a new layer to the standard setting of Hedonic Games, in the sense that agents that select the same location form a coalition. This allows to naturally restrict the possible coalitions that can be formed. With this, our model generalizes simple symmetric Fractional Hedonic Games on complete bipartite valuation graphs and also Hedonic Diversity Games with utilities single-peaked at 0. We believe that this generalization is also a very interesting direction for other types of Hedonic Games. 

**Abstract (ZH)**: 本文研究了客户和卖家的战略选址选择问题，文献中将其称为制粉商和磨坊商游戏。在我们扩展的设定中，每个磨坊商可以自由选择任何位置来设立磨坊，而每个面包师在设立面包房的位置选择上受到一定的限制。为了获得最佳的谈判权，面包师倾向于选择拥有许多磨坊商供应面粉，同时竞争较少的其他面包师的位置。同样，磨坊商则希望选择一个有许多面包师但对手磨坊商较少的位置。因此，两类代理人都会选择一个使同一类型代理人在其选择的位置上的比例与不同类型代理人的比例优化的位置。最初在分数式报怨博弈的背景下提出的制粉商和磨坊商游戏，其应用场景从商业到产品设计均有涉及。
我们研究了位置限制对游戏性质的影响。尽管在没有位置限制的设定中，纯纳什均衡显然存在，我们通过一个复杂而高效的算法证明，即使在更加具挑战性的限制设定中，该游戏也存在均衡。此外，所计算出的均衡可以将最优社会福利近似到最多 $2\left(\frac{e}{e-1}\right)$ 的因子。同时，我们给出了价格失真/稳定性的确切界限。
从概念上讲，选址特征为标准的报怨博弈模型增添了一个新的维度，即选择了同一位置的代理形成一个联盟。这自然限制了可能形成的联盟。我们的模型不仅推广了完全二分图上的简单对称分数报怨博弈模型，还推广了零处单峰效用的报怨多样性博弈模型。我们相信，这种推广也是对其他类型报怨博弈的一个非常有趣的方向。 

---
# AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder 

**Title (ZH)**: AnCoGen：基于掩码自编码器的语音分析、控制与生成 

**Authors**: Samir Sadok, Simon Leglaive, Laurent Girin, Gaël Richard, Xavier Alameda-Pineda  

**Link**: [PDF](https://arxiv.org/pdf/2501.05332)  

**Abstract**: This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement. 

**Abstract (ZH)**: 本文介绍了AnCoGen，这是一种创新的方法，利用掩码自编码器在单模型内统一实现了语音信号的分析、控制和生成。AnCoGen能够通过估计关键属性（如说话人身份、音高、内容、响度、信噪比和清晰度指数）来分析语音。此外，它还能根据这些属性生成语音，并通过修改这些属性来实现对合成语音的精确控制。广泛的实验表明，AnCoGen在语音分析-复原、音高估计、音高修改和噪声抑制方面均表现出色。 

---
# Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation 

**Title (ZH)**: 面向人体姿态估计的均衡持续多模态学习研究 

**Authors**: Jiaxuan Peng, Mengshi Qi, Dong Zhao, Huadong Ma  

**Link**: [PDF](https://arxiv.org/pdf/2501.05264)  

**Abstract**: 3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods. However, RGB images are susceptible to limitations such as sensitivity to lighting conditions and potential user discomfort. Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention. Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance and the imperative for continual learning. In this work, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based contribution algorithm to quantify the contribution of each modality and identify modality imbalance. To address this imbalance, we employ a re-learning strategy. Furthermore, recognizing that raw data is prone to noise contamination, we develop a novel denoising continual learning approach. This approach incorporates a noise identification and separation module to mitigate the adverse effects of noise and collaborates with the balanced learning strategy to enhance optimization. Additionally, an adaptive EWC mechanism is employed to alleviate catastrophic forgetting. We conduct extensive experiments on the widely-adopted multi-modal dataset, MM-Fi, which demonstrate the superiority of our approach in boosting 3D pose estimation and mitigating catastrophic forgetting in complex scenarios. We will release our codes. 

**Abstract (ZH)**: 基于RGB的方法在三维人体姿态估计（3D HPE）领域中已成为一个重要的研究课题。然而，RGB图像存在一些局限性，如对光照条件的敏感性和潜在的用户不适。因此，多模态传感技术逐渐受到重视，这种方法利用非侵入性传感器。尽管如此，多模态3D HPE仍然面临着模态不平衡和持续学习的迫切需求。在本研究中，我们介绍了一种新的平衡持续多模态学习方法，以增强三维人体姿态估计，并解决模态不平衡的问题。该方法结合了RGB、LiDAR、毫米波（mmWave）和WiFi等多种模态。具体来说，我们提出了一种基于Shapley值的贡献算法，用于量化每个模态的贡献并识别模态不平衡。为解决这一不平衡问题，我们采用了重训练策略。此外，考虑到原始数据容易受到噪声污染的影响，我们开发了一种新的去噪持续学习方法。该方法包含了噪声识别和分离模块，以减轻噪声的负面影响，并与平衡学习策略协作，以增强优化效果。此外，我们采用了自适应EWC机制来缓解灾难性遗忘的问题。我们在广泛应用的多模态数据集MM-Fi上进行了广泛的实验，结果表明，我们的方法在提高三维姿态估计性能和在复杂场景中减轻灾难性遗忘方面具有明显的优势。我们还将发布我们的代码。 

---
# Enhancing Plagiarism Detection in Marathi with a Weighted Ensemble of TF-IDF and BERT Embeddings for Low-Resource Language Processing 

**Title (ZH)**: 增强马拉地语中的抄袭检测：基于低资源语言处理的TF-IDF和BERT嵌入加权集合方法 

**Authors**: Atharva Mutsaddi, Aditya Choudhary  

**Link**: [PDF](https://arxiv.org/pdf/2501.05260)  

**Abstract**: Plagiarism involves using another person's work or concepts without proper attribution, presenting them as original creations. With the growing amount of data communicated in regional languages such as Marathi -- one of India's regional languages -- it is crucial to design robust plagiarism detection systems tailored for low-resource languages. Language models like Bidirectional Encoder Representations from Transformers (BERT) have demonstrated exceptional capability in text representation and feature extraction, making them essential tools for semantic analysis and plagiarism detection. However, the application of BERT for low-resource languages remains under-explored, particularly in the context of plagiarism detection. This paper presents a method to enhance the accuracy of plagiarism detection for Marathi texts using BERT sentence embeddings in conjunction with Term Frequency-Inverse Document Frequency (TF-IDF) feature representation. This approach effectively captures statistical, semantic, and syntactic aspects of text features through a weighted voting ensemble of machine learning models. 

**Abstract (ZH)**: 抄袭是指在未给予适当引证的情况下使用他人的作品或概念，并将其呈现为原创创作。随着用马拉地语（印度的一种地方语言）等地方语言交流的数据量不断增加，设计针对低资源语言的稳健抄袭检测系统变得至关重要。像双向Transformer编码表示（BERT）这样的语言模型展示了卓越的文本表示和特征提取能力，使其成为语义分析和抄袭检测的重要工具。然而，BERT在低资源语言中的应用仍然相对较少，特别是在抄袭检测中的应用更为有限。本文提出了一种方法，使用BERT句子嵌入结合词频-逆文档频率（TF-IDF）特征表示，以提高马拉地语文本抄袭检测的准确性。该方法通过加权投票集成机器学习模型来有效地捕捉文本特征的统计学、语义学和句法学方面。 

---
# Automating the Detection of Code Vulnerabilities by Analyzing GitHub Issues 

**Title (ZH)**: 通过分析GitHub问题自动检测代码漏洞 

**Authors**: Daniele Cipollone, Changjie Wang, Mariano Scazzariello, Simone Ferlin, Maliheh Izadi, Dejan Kostic, Marco Chiesa  

**Link**: [PDF](https://arxiv.org/pdf/2501.05258)  

**Abstract**: In today's digital landscape, the importance of timely and accurate vulnerability detection has significantly increased. This paper presents a novel approach that leverages transformer-based models and machine learning techniques to automate the identification of software vulnerabilities by analyzing GitHub issues. We introduce a new dataset specifically designed for classifying GitHub issues relevant to vulnerability detection. We then examine various classification techniques to determine their effectiveness. The results demonstrate the potential of this approach for real-world application in early vulnerability detection, which could substantially reduce the window of exploitation for software vulnerabilities. This research makes a key contribution to the field by providing a scalable and computationally efficient framework for automated detection, enabling the prevention of compromised software usage before official notifications. This work has the potential to enhance the security of open-source software ecosystems. 

**Abstract (ZH)**: 在当今数字化环境中，及时准确地检测软件漏洞的重要性显著提升。本文提出了一种新颖的方法，利用基于变压器的模型和机器学习技术自动化软件漏洞的识别，通过分析GitHub问题进行。我们引入了一个专门用于分类与漏洞检测相关GitHub问题的新数据集。随后，我们研究了各种分类技术，以评估其有效性。结果表明，这种方法在实际应用中具有早期漏洞检测的潜力，可以显著减少软件漏洞被利用的窗口期。本研究通过对自动化检测提供一个可扩展且计算效率高的框架，在正式通知之前预防被篡改的软件使用，从而为该领域做出了重要贡献。这项工作有可能增强开源软件生态系统的安全性。 

---
# From Scientific Texts to Verifiable Code: Automating the Process with Transformers 

**Title (ZH)**: 从科学文本到可验证代码：利用变换器自动化这一过程 

**Authors**: Changjie Wang, Mariano Scazzariello, Marco Chiesa  

**Link**: [PDF](https://arxiv.org/pdf/2501.05252)  

**Abstract**: Despite the vast body of research literature proposing algorithms with formal guarantees, the amount of verifiable code in today's systems remains minimal. This discrepancy stems from the inherent difficulty of verifying code, particularly due to the time-consuming nature and strict formalism of proof details that formal verification tools require. However, the emergence of transformers in Large Language Models presents a promising solution to this challenge. In this position paper, we believe that transformers have the potential to read research papers that propose algorithms with formal proofs and translate these proofs into verifiable code. We leverage transformers to first build a formal structure of the proof using the original text from the paper, and then to handle the tedious, low-level aspects of proofs that are often omitted by humans. We argue that this approach can significantly reduce the barrier to formal verification. The above idea of reading papers to write verifiable code opens new avenues for automating the verification of complex systems, enabling a future where formally verified algorithms from academic research can more seamlessly transition into real-world software systems, thereby improving code reliability and security. 

**Abstract (ZH)**: 尽管有大量的研究文献提出了具有正式保证的算法，但在当今系统中可验证的代码数量仍然很少。这种差异源于验证代码的固有困难，尤其是由于形式验证工具所需的证明细节既耗时又严格。然而，大型语言模型中 transformer 的出现为解决这一挑战提供了有希望的解决方案。在这篇立场论文中，我们认为 transformer 有可能阅读提出具有形式证明的算法的研究论文，并将这些证明转换为可验证的代码。我们利用 transformer 首先利用论文中的原始文本构建证明的正式结构，然后处理人类常常忽略的繁琐、低级的证明方面。我们主张这种方法可以显著降低形式验证的门槛。通过阅读论文以编写可验证代码的想法开辟了自动化复杂系统验证的新途径，使未来学术研究中的形式验证算法能够更无缝地过渡到实际软件系统中，从而提高代码的可靠性和安全性。 

---
# RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models 

**Title (ZH)**: RAG-WM：一种高效的黑盒水印方法，用于大型语言模型的检索增强生成 

**Authors**: Peizhuo Lv, Mengjie Sun, Hao Wang, Xiaofeng Wang, Shengzhi Zhang, Yuxuan Chen, Kai Chen, Limin Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.05249)  

**Abstract**: In recent years, tremendous success has been witnessed in Retrieval-Augmented Generation (RAG), widely used to enhance Large Language Models (LLMs) in domain-specific, knowledge-intensive, and privacy-sensitive tasks. However, attackers may steal those valuable RAGs and deploy or commercialize them, making it essential to detect Intellectual Property (IP) infringement. Most existing ownership protection solutions, such as watermarks, are designed for relational databases and texts. They cannot be directly applied to RAGs because relational database watermarks require white-box access to detect IP infringement, which is unrealistic for the knowledge base in RAGs. Meanwhile, post-processing by the adversary's deployed LLMs typically destructs text watermark information. To address those problems, we propose a novel black-box "knowledge watermark" approach, named RAG-WM, to detect IP infringement of RAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark Generator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark texts based on watermark entity-relationship tuples and inject them into the target RAG. We evaluate RAG-WM across three domain-specific and two privacy-sensitive tasks on four benchmark LLMs. Experimental results show that RAG-WM effectively detects the stolen RAGs in various deployed LLMs. Furthermore, RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also evade watermark detection approaches, highlighting its promising application in detecting IP infringement of RAG systems. 

**Abstract (ZH)**: 近年来，检索增强生成（RAG）技术在特定领域、知识密集和隐私敏感任务中大幅提升了大型语言模型（LLMs）的表现，取得了巨大成功。然而，攻击者可能盗取这些有价值的RAG并进行部署或商业化，因此对知识产权（IP）侵权的检测变得至关重要。目前大多数现有的所有权保护解决方案，如水印技术，主要适用于关系型数据库和文本，无法直接应用于RAG。因为关系型数据库水印需要白盒访问以检测IP侵权，而在RAG的知识库中实现这一点并不现实。同时，攻击者部署的LLM的后处理通常会破坏文本水印信息。为解决这些问题，我们提出了一种新的黑盒“知识水印”方法，称为RAG-WM，用于检测RAG的IP侵权。RAG-WM采用多LLM交互框架，包括水印生成器、影子LLM和RAG以及水印鉴别器，基于水印实体-关系元组生成水印文本，并将其注入目标RAG。我们分别在四个基准LLM上进行了三种特定领域和两种隐私敏感任务的评估。实验结果表明，RAG-WM能够有效检测各种部署中被盗的RAG。此外，RAG-WM对重述、无关内容删除、知识插入和知识扩展攻击具有鲁棒性。最后，RAG-WM还可以避免其他水印检测方法的检测，凸显了它在检测RAG系统IP侵权方面的潜在应用价值。 

---
# Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning 

**Title (ZH)**: 从大规模语言模型中高效裁剪衍生出编码专用子模型 

**Authors**: Laura Puccioni, Alireza Farshin, Mariano Scazzariello, Changjie Wang, Marco Chiesa, Dejan Kostic  

**Link**: [PDF](https://arxiv.org/pdf/2501.05248)  

**Abstract**: Large Language Models (LLMs) have demonstrated their exceptional performance in various complex code generation tasks. However, their broader adoption is limited by significant computational demands and high resource requirements, particularly memory and processing power. To mitigate such requirements, model pruning techniques are used to create more compact models with significantly fewer parameters. However, current approaches do not focus on the efficient extraction of programming-language-specific sub-models. In this work, we explore the idea of efficiently deriving coding-specific sub-models through unstructured pruning (i.e., Wanda). We investigate the impact of different domain-specific calibration datasets on pruning outcomes across three distinct domains and extend our analysis to extracting four language-specific sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently extract programming-language-specific sub-models using appropriate calibration datasets while maintaining acceptable accuracy w.r.t. full models. We are also the first to provide analytical evidence that domain-specific tasks activate distinct regions within LLMs, supporting the creation of specialized sub-models through unstructured pruning. We believe that this work has significant potential to enhance LLM accessibility for coding by reducing computational requirements to enable local execution on consumer-grade hardware, and supporting faster inference times critical for real-time development feedback. 

**Abstract (ZH)**: 大语言模型（LLMs）在各种复杂的代码生成任务中展现了其卓越的表现。然而，其更广泛的采用受到显著的计算需求和高资源要求的限制，尤其是内存和处理能力的需要。为了减轻这种需求，通过模型剪枝技术可以创建更加紧凑的模型，具有显著减少的参数量。然而，当前的方法并未专注于高效提取编程语言特定的子模型。在本研究中，我们探索了通过无结构剪枝（例如Wanda）高效提取编程特定的子模型的想法。我们研究了不同领域特定校准数据集对剪枝结果的影响，并跨越三个不同的领域进行了分析。我们还进一步分析了提取出四种语言特定的子模型：Python、Java、C++ 和 JavaScript。我们首次通过合适的校准数据集高效地提取出了编程语言特定的子模型，同时在与完整模型相比较的情况下保持了可接受的准确性。我们也是首次提供实证分析证据，证明领域特定的任务在大语言模型中激活了不同的区域，支持通过无结构剪枝创建专门的子模型。我们相信，这项工作具有显著的潜力，可以通过减少计算需求来增强LLM在编程中的可访问性，使其能够在消费级硬件上进行局部执行，并支持实时开发反馈至关重要的快速推断时间。 

---
# Optimizing Estonian TV Subtitles with Semi-supervised Learning and LLMs 

**Title (ZH)**: 使用半监督学习和大规模语言模型优化爱沙尼亚电视字幕 

**Authors**: Artem Fedorchenko, Tanel Alumäe  

**Link**: [PDF](https://arxiv.org/pdf/2501.05234)  

**Abstract**: This paper presents an approach for generating high-quality, same-language subtitles for Estonian TV content. We fine-tune the Whisper model on human-generated Estonian subtitles and enhance it with iterative pseudo-labeling and large language model (LLM) based post-editing. Our experiments demonstrate notable subtitle quality improvement through pseudo-labeling with an unlabeled dataset. We find that applying LLM-based editing at test time enhances subtitle accuracy, while its use during training does not yield further gains. This approach holds promise for creating subtitle quality close to human standard and could be extended to real-time applications. 

**Abstract (ZH)**: 本文提出了一个生成高质量同语言字幕的方法，适用于爱沙尼亚电视内容。我们对 Whisper 模型进行了细调，使其适用于人类生成的爱沙尼亚语字幕，并通过迭代伪标签和基于大型语言模型（LLM）的后编辑增强了该模型。实验结果显示，通过在未标记的数据集上进行伪标签处理，可以显著提高字幕质量。我们发现，在测试时应用基于 LLM 的编辑可以提高字幕准确性，而在训练时使用该方法则不会带来额外的改进。该方法有望创建接近人类标准的字幕质量，并且可以扩展到实时应用。 

---
# A Novel Approach to Scalable and Automatic Topic-Controlled Question Generation in Education 

**Title (ZH)**: 一种新型的可扩展且自动的基于主题的问答生成方法在教育中的应用 

**Authors**: Ziqing Li, Mutlu Cukurova, Sahan Bulathwela  

**Link**: [PDF](https://arxiv.org/pdf/2501.05220)  

**Abstract**: The development of Automatic Question Generation (QG) models has the potential to significantly improve educational practices by reducing the teacher workload associated with creating educational content. This paper introduces a novel approach to educational question generation that controls the topical focus of questions. The proposed Topic-Controlled Question Generation (T-CQG) method enhances the relevance and effectiveness of the generated content for educational purposes. Our approach uses fine-tuning on a pre-trained T5-small model, employing specially created datasets tailored to educational needs. The research further explores the impacts of pre-training strategies, quantisation, and data augmentation on the model's performance. We specifically address the challenge of generating semantically aligned questions with paragraph-level contexts, thereby improving the topic specificity of the generated questions. In addition, we introduce and explore novel evaluation methods to assess the topical relatedness of the generated questions. Our results, validated through rigorous offline and human-backed evaluations, demonstrate that the proposed models effectively generate high-quality, topic-focused questions. These models have the potential to reduce teacher workload and support personalised tutoring systems by serving as bespoke question generators. With its relatively small number of parameters, the proposals not only advance the capabilities of question generation models for handling specific educational topics but also offer a scalable solution that reduces infrastructure costs. This scalability makes them feasible for widespread use in education without reliance on proprietary large language models like ChatGPT. 

**Abstract (ZH)**: 自动问答生成（QG）模型的发展有望通过减少教师创建教育资源的工作量来显著提升教育实践。本文介绍了一种新颖的教育问题生成方法，该方法可以控制问题的主题焦点。提出的议题控制问题生成（T-CQG）方法增强了生成内容的相关性和教育有效性。我们采用在预训练的T5-small模型上进行微调的方法，并利用专门针对教育需求定制的数据集。研究进一步探讨了预训练策略、量化和数据增强对模型性能的影响。我们特别解决了生成与段落级背景文本语义一致的问题的挑战，从而提高了生成问题的主题特定性。此外，我们引入了新的评估方法来评估生成问题的主题相关性。通过严格的离线和人为评估验证，研究结果表明，提出的模型能够有效生成高质量、主题导向的问题。这些模型有潜力减轻教师的工作负担，并通过作为定制问题生成器来支持个性化辅导系统。凭借相对较少的参数数量，这些提案不仅提升了处理特定教育主题的问答生成模型的能力，还提供了一种可扩展的解决方案，降低了基础设施成本。这种可扩展性使它们能够在教育领域广泛使用，而不依赖于像ChatGPT这样的专有大规模语言模型。 

---
# GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign Language Accessibility 

**Title (ZH)**: GLaM-Sign：面向手语无障碍的希腊语多模态唇读系统 

**Authors**: Dimitris Kouremenos, Klimis Ntalianis  

**Link**: [PDF](https://arxiv.org/pdf/2501.05213)  

**Abstract**: The Greek Language Multimodal Lip Reading with Integrated Sign Language Accessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and multimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals. Developed from the FEELIT project [2], it integrates high-resolution audio, video, textual transcriptions, and Greek Sign Language translations for applications like real-time sign language translation and enhanced subtitle synchronization. While its primary focus is on promoting inclusivity in the Greek tourism sector, its adaptability extends to education, healthcare, and public services. Future advancements will enhance word-level precision and scalability to additional languages, supported by advanced AI methodologies and collaborations with diverse stakeholders. This dataset underscores the transformative potential of multimodal resources in bridging communication gaps, fostering innovation, and setting a benchmark for ethical AI and inclusive technologies. 

**Abstract (ZH)**: 《GLaM-Sign：用于希腊语多媒体唇读与集成手语无障碍的资源》[1] 是无障碍技术与多模态人工智能领域的突破性资源，旨在支持听障和听力障碍者（Deaf and Hard-of-Hearing, DHH）群体。该资源源自 FEELIT 项目[2]，整合了高分辨率音频、视频、文本转录和希腊手语翻译，适用于实时手语翻译和字幕同步等应用。尽管其主要重点在于促进希腊旅游业的包容性，但其适应性扩展到了教育、医疗和公共服务领域。未来的发展将增强单词级别的精度，并支持更多语言的扩展，借助先进的人工智能方法和与多元利益相关方的合作。该数据集突显了多媒体资源在弥合沟通障碍、促进创新以及为伦理人工智能和包容性技术设定标准方面的潜力。

注释：
[1] GLaM-Sign：该名称似乎是基于项目的描述创建的，结合了“希腊语”、“多媒体”、“唇读”和“手语无障碍”等要素。
[2] FEELIT项目：项目名称是根据描述创造的示例，实际项目名称可能有所不同。 

---
# Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning 

**Title (ZH)**: 婴儿学习中超越语言输入的隐藏视觉概念发现 

**Authors**: Xueyi Ke, Satoshi Tsutsui, Yayun Zhang, Bihan Wen  

**Link**: [PDF](https://arxiv.org/pdf/2501.05205)  

**Abstract**: Infants develop complex visual understanding rapidly, even preceding of the acquisition of linguistic inputs. As computer vision seeks to replicate the human vision system, understanding infant visual development may offer valuable insights. In this paper, we present an interdisciplinary study exploring this question: can a computational model that imitates the infant learning process develop broader visual concepts that extend beyond the vocabulary it has heard, similar to how infants naturally learn? To investigate this, we analyze a recently published model in Science by Vong et al.,which is trained on longitudinal, egocentric images of a single child paired with transcribed parental speech. We introduce a training-free framework that can discover visual concept neurons hidden in the model's internal representations. Our findings show that these neurons can classify objects outside its original vocabulary. Furthermore, we compare the visual representations in infant-like models with those in moder computer vision models, such as CLIP or ImageNet pre-trained model, highlighting key similarities and differences. Ultimately, our work bridges cognitive science and computer vision by analyzing the internal representations of a computational model trained on an infant's visual and linguistic inputs. 

**Abstract (ZH)**: 婴儿能够快速发展复杂的视觉理解能力，甚至在获得语言输入之前。鉴于计算机视觉旨在模拟人类视觉系统，理解婴儿的视觉发展可能提供宝贵的见解。在本文中，我们进行了一项跨学科研究，探讨以下问题：能否构建一种模仿婴儿学习过程的计算模型，发展出超越其听到词汇范围的更广泛视觉概念，类似于婴儿自然学习的过程？为探究这一问题，我们分析了Vong等人在《科学》杂志上发表的一项最近的研究，该研究基于单一儿童的纵向主观图像及其语录父母的转录进行训练。我们引入了一种无需训练的框架，可以发现模型内部表示中隐藏的视觉概念神经元。研究结果表明，这些神经元能够对模型原始词汇之外的物体进行分类。此外，我们将婴儿样式的模型的视觉表示与现代计算机视觉模型（如CLIP或ImageNet预训练模型）进行比较，突显了关键的相似性和差异性。最终，通过分析接受婴儿视觉和语言输入训练的计算模型的内部表示，我们的工作将认知科学与计算机视觉联系了起来。 

---
# An Algorithmic Approach for Causal Health Equity: A Look at Race Differentials in Intensive Care Unit (ICU) Outcomes 

**Title (ZH)**: 因果健康公平的算法化方法研究：重症监护单元（ICU）结果中的种族差异分析 

**Authors**: Drago Plecko, Paul Secombe, Andrea Clarke, Amelia Fiske, Samarra Toby, Donisha Duff, David Pilcher, Leo Anthony Celi, Rinaldo Bellomo, Elias Bareinboim  

**Link**: [PDF](https://arxiv.org/pdf/2501.05197)  

**Abstract**: The new era of large-scale data collection and analysis presents an opportunity for diagnosing and understanding the causes of health inequities. In this study, we describe a framework for systematically analyzing health disparities using causal inference. The framework is illustrated by investigating racial and ethnic disparities in intensive care unit (ICU) outcome between majority and minority groups in Australia (Indigenous vs. Non-Indigenous) and the United States (African-American vs. White). We demonstrate that commonly used statistical measures for quantifying inequity are insufficient, and focus on attributing the observed disparity to the causal mechanisms that generate it. We find that minority patients are younger at admission, have worse chronic health, are more likely to be admitted for urgent and non-elective reasons, and have higher illness severity. At the same time, however, we find a protective direct effect of belonging to a minority group, with minority patients showing improved survival compared to their majority counterparts, with all other variables kept equal. We demonstrate that this protective effect is related to the increased probability of being admitted to ICU, with minority patients having an increased risk of ICU admission. We also find that minority patients, while showing improved survival, are more likely to be readmitted to ICU. Thus, due to worse access to primary health care, minority patients are more likely to end up in ICU for preventable conditions, causing a reduction in the mortality rates and creating an effect that appears to be protective. Since the baseline risk of ICU admission may serve as proxy for lack of access to primary care, we developed the Indigenous Intensive Care Equity (IICE) Radar, a monitoring system for tracking the over-utilization of ICU resources by the Indigenous population of Australia across geographical areas. 

**Abstract (ZH)**: 大规模数据收集与分析的新时代为诊断和理解健康不平等的原因提供了机会。本研究描述了一种系统分析卫生差异的框架，并利用因果推断进行了说明。该框架通过对比分析澳大利亚（土著人与非土著人）和美国（非洲裔美国人与白人）群体在重症监护室（ICU）结局中的种族和族裔差异来说明其应用。我们证明了常用来量化不平等的统计指标不足，并着重于将观察到的差距归因于产生这种差距的因果机制。结果显示，少数群体患者的入院年龄较小，慢性健康状况较差，更容易因紧急和非择期原因入院，并且病情更严重。然而，与此同时，我们发现属于少数群体身份具有保护性直接效应，少数群体患者的生存率优于多数群体的患者，保持其他变量不变的情况下。我们说明了这种保护性效应与ICU入院概率增加有关，少数群体患者的ICU入院风险更高。此外，尽管少数群体患者表现出更佳的生存率，但更有可能再次入住ICU。因此，由于对初级医疗服务的可及性较差，少数群体患者更可能因可预防的条件而入住ICU，导致死亡率降低并产生了看似保护性的效果。考虑到ICU入院的基线风险可能作为缺乏初级医疗服务的替代指标，我们开发了“土著重症监护公平雷达”（IICE Radar），这是一种监测系统，用于跟踪澳大利亚各个地理区域土著人群重症监护资源的过度使用。 

---
# Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering 

**Title (ZH)**: 在混乱中带来秩序：人工智能在安全软件工程中的作用 

**Authors**: Matteo Esposito  

**Link**: [PDF](https://arxiv.org/pdf/2501.05165)  

**Abstract**: Context. Developing secure and reliable software remains a key challenge in software engineering (SE). The ever-evolving technological landscape offers both opportunities and threats, creating a dynamic space where chaos and order compete. Secure software engineering (SSE) must continuously address vulnerabilities that endanger software systems and carry broader socio-economic risks, such as compromising critical national infrastructure and causing significant financial losses. Researchers and practitioners have explored methodologies like Static Application Security Testing Tools (SASTTs) and artificial intelligence (AI) approaches, including machine learning (ML) and large language models (LLMs), to detect and mitigate these vulnerabilities. Each method has unique strengths and limitations.
Aim. This thesis seeks to bring order to the chaos in SSE by addressing domain-specific differences that impact AI accuracy.
Methodology. The research employs a mix of empirical strategies, such as evaluating effort-aware metrics, analyzing SASTTs, conducting method-level analysis, and leveraging evidence-based techniques like systematic dataset reviews. These approaches help characterize vulnerability prediction datasets.
Results. Key findings include limitations in static analysis tools for identifying vulnerabilities, gaps in SASTT coverage of vulnerability types, weak relationships among vulnerability severity scores, improved defect prediction accuracy using just-in-time modeling, and threats posed by untouched methods.
Conclusions. This thesis highlights the complexity of SSE and the importance of contextual knowledge in improving AI-driven vulnerability and defect prediction. The comprehensive analysis advances effective prediction models, benefiting both researchers and practitioners. 

**Abstract (ZH)**: 背景. 在软件工程（SE）中，开发安全可靠的软件仍然是一个关键挑战。不断演变的技术环境既带来了机遇也带来了威胁，创造了一个动态的空间，在这个空间中，混乱与秩序共存。安全软件工程（SSE）必须不断应对威胁软件系统的漏洞，这些漏洞不仅会带来更广泛的社会经济风险，如损害关键的国家基础设施，还会导致重大经济损失。研究人员和实践者探索了诸如静态应用安全性测试工具（SASTTs）和人工智能（AI）方法，包括机器学习（ML）和大规模语言模型（LLMs），来检测和缓解这些漏洞。每种方法都有其独特的优势和局限性。

目标. 本论文旨在通过解决影响AI准确性的领域特定差异，为SSE中的混乱带来秩序。

方法. 研究采用了多种实证策略，包括评估效率感知度量、分析SASTTs、进行方法层面的分析以及利用基于证据的技术，如系统性数据集审查。这些方法有助于表征漏洞预测数据集的特点。

结果. 关键发现包括静态分析工具在识别漏洞方面的局限性、SASTT对漏洞类型的覆盖不足、严重程度评分之间关系较弱、使用即时建模提升缺陷预测准确性以及未处理的代码方法带来的威胁。

结论. 本论文强调了SSE的复杂性以及在改善基于AI的漏洞和缺陷预测方面背景知识的重要性。全面的分析推进了有效的预测模型的发展，这些模型既造福于研究人员，也造福于实践者。 

---
# Explainable AI based System for Supply Air Temperature Forecast 

**Title (ZH)**: 基于可解释人工智能的供风温度预测系统 

**Authors**: Marika Eik, Ahmet Kose, Hossein Nourollahi Hokmabad, Juri Belikov  

**Link**: [PDF](https://arxiv.org/pdf/2501.05163)  

**Abstract**: This paper explores the application of Explainable AI (XAI) techniques to improve the transparency and understanding of predictive models in control of automated supply air temperature (ASAT) of Air Handling Unit (AHU). The study focuses on forecasting of ASAT using a linear regression with Huber loss. However, having only a control curve without semantic and/or physical explanation is often not enough. The present study employs one of the XAI methods: Shapley values, which allows to reveal the reasoning and highlight the contribution of each feature to the final ASAT forecast. In comparison to other XAI methods, Shapley values have solid mathematical background, resulting in interpretation transparency. The study demonstrates the contrastive explanations--slices, for each control value of ASAT, which makes it possible to give the client objective justifications for curve changes. 

**Abstract (ZH)**: 本文探讨了可解释人工智能（XAI）技术在提高自动空调送风温度（ASAT）控制系统的预测模型透明度和理解能力中的应用。研究重点在于使用带Huber损失的线性回归进行ASAT的预测。然而，仅提供一个控制曲线而没有语义或物理解释往往不够。本研究采用了一种XAI方法——Shapley值，这种方法能够揭示推理过程并突出显示每个特征对最终ASAT预测结果的贡献。与其他XAI方法相比，Shapley值具有坚实的数学背景，从而提高了解释的透明度。研究通过为每个ASAT控制值提供对比性解释来展示曲线变化的客观理由，使得可以为客户提供具体的解释。 

---
# Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Identifier 

**Title (ZH)**: 基于自适应文档-关系交叉映射和概念唯一标识符的生物医学关系提取 

**Authors**: Yufei Shang, Yanrong Guo, Shijie Hao, Richang Hong  

**Link**: [PDF](https://arxiv.org/pdf/2501.05155)  

**Abstract**: Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensive texts, serving as a crucial subfield of biomedical text mining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works. 

**Abstract (ZH)**: 生物医学文档级关系提取（Bio-RE）旨在识别广泛文本中的生物医学实体关系，是生物医学文本挖掘中的关键子领域。现有的Bio-RE方法在跨句推断方面存在困难，这是捕捉跨越多个句子的关系所必需的。此外，之前的大多数方法忽视了文档的不完整性，并且缺乏外部知识的整合，限制了语境的丰富性。同时，标注数据的稀缺进一步妨碍了模型的训练。近期大型语言模型（LLMs）的发展激发了我们探索这些所有问题以解决文档级Bio-RE问题。具体来说，我们提出了一种通过LLM自适应文档-关系跨映射（ADRCM）微调和概念唯一标识符（CUI）检索增强生成（RAG）来解决文档级Bio-RE问题的框架。首先，我们介绍了迭代的实体关系摘要（IoRs）提示，以解决数据稀缺问题。通过这种方式，可以指导ChatGPT专注于实体关系，并通过迭代改进合成数据来生成与Bio-RE任务相关的合成数据。其次，我们提出了ADRCM微调，这是一种新颖的微调方法，建立了不同文档和关系之间的映射，增强了模型的语境理解和跨句推理能力。最后，在推理过程中，设计了一种针对生物医学的RAG方法，称为CUI RAG，利用CUI作为实体的索引，缩小检索范围并丰富相关文档的上下文。我们在三个Bio-RE数据集（GDA、CDR和BioRED）上的实验表明，与相关工作相比，我们提出的方法达到了最先进的性能。 

---
# A Systematic Literature Review on Deep Learning-based Depth Estimation in Computer Vision 

**Title (ZH)**: 基于深度学习的深度估计在计算机视觉中的系统文献综述 

**Authors**: Ali Rohan, Md Junayed Hasan, Andrei Petrovski  

**Link**: [PDF](https://arxiv.org/pdf/2501.05147)  

**Abstract**: Depth estimation (DE) provides spatial information about a scene and enables tasks such as 3D reconstruction, object detection, and scene understanding. Recently, there has been an increasing interest in using deep learning (DL)-based methods for DE. Traditional techniques rely on handcrafted features that often struggle to generalise to diverse scenes and require extensive manual tuning. However, DL models for DE can automatically extract relevant features from input data, adapt to various scene conditions, and generalise well to unseen environments. Numerous DL-based methods have been developed, making it necessary to survey and synthesize the state-of-the-art (SOTA). Previous reviews on DE have mainly focused on either monocular or stereo-based techniques, rather than comprehensively reviewing DE. Furthermore, to the best of our knowledge, there is no systematic literature review (SLR) that comprehensively focuses on DE. Therefore, this SLR study is being conducted. Initially, electronic databases were searched for relevant publications, resulting in 1284 publications. Using defined exclusion and quality criteria, 128 publications were shortlisted and further filtered to select 59 high-quality primary studies. These studies were analysed to extract data and answer defined research questions. Based on the results, DL methods were developed for mainly three different types of DE: monocular, stereo, and multi-view. 20 publicly available datasets were used to train, test, and evaluate DL models for DE, with KITTI, NYU Depth V2, and Make 3D being the most used datasets. 29 evaluation metrics were used to assess the performance of DE. 35 base models were reported in the primary studies, and the top five most-used base models were ResNet-50, ResNet-18, ResNet-101, U-Net, and VGG-16. Finally, the lack of ground truth data was among the most significant challenges reported by primary studies. 

**Abstract (ZH)**: 深度估计（DE）提供了场景的空间信息，并能够支持三维重建、物体检测和场景理解等任务。近年来，人们越来越关注使用深度学习（DL）方法来进行深度估计。传统技术依赖手工特征工程，这些方法往往难以适应多样化的场景，并且需要大量的手动调整。然而，DL模型能够自动从输入数据中提取相关特征，适应各种场景条件，并在未见过的环境中表现出良好的泛化能力。目前开发了大量基于DL的方法，因此有必要对其进行调查和总结，以反映最新的进展。此前关于深度估计的综述主要集中在单目或多目方法上，未能全面覆盖深度估计领域。此外，据我们所知，尚未有系统性的文献综述（SLR）全面关注深度估计领域。因此，本文将进行这样的SLR研究。首先，通过电子数据库搜索相关文献，共检索到1284篇文献。经过界定排除和质量标准筛选后，最终选定128篇文献，并进一步筛选出59篇高质量的原始研究作为分析对象。这些研究被分析以提取数据并回答定义的研究问题。根据研究结果，DL方法主要为三种不同类型的深度估计开发：单目、立体和多视图。共有20个公开可用的数据库用于训练、测试和评价深度估计的DL模型，其中KITTIdataset、NYU Depth V2以及Make 3D是最常使用的数据库。共使用了29种性能评估指标来评估深度估计的表现。在原始研究中报告了35种基础模型，其中使用次数最多的前五种基础模型分别是ResNet-50、ResNet-18、ResNet-101、U-Net和VGG-16。最后，初级研究表明数据匮乏是最大的挑战之一。 

---
# Constrained Optimization of Charged Particle Tracking with Multi-Agent Reinforcement Learning 

**Title (ZH)**: 多代理强化学习约束优化带电粒子跟踪 

**Authors**: Tobias Kortus, Ralf Keidel, Nicolas R. Gauger, Jan Kieseler  

**Link**: [PDF](https://arxiv.org/pdf/2501.05113)  

**Abstract**: Reinforcement learning demonstrated immense success in modelling complex physics-driven systems, providing end-to-end trainable solutions by interacting with a simulated or real environment, maximizing a scalar reward signal. In this work, we propose, building upon previous work, a multi-agent reinforcement learning approach with assignment constraints for reconstructing particle tracks in pixelated particle detectors. Our approach optimizes collaboratively a parametrized policy, functioning as a heuristic to a multidimensional assignment problem, by jointly minimizing the total amount of particle scattering over the reconstructed tracks in a readout frame. To satisfy constraints, guaranteeing a unique assignment of particle hits, we propose a safety layer solving a linear assignment problem for every joint action. Further, to enforce cost margins, increasing the distance of the local policies predictions to the decision boundaries of the optimizer mappings, we recommend the use of an additional component in the blackbox gradient estimation, forcing the policy to solutions with lower total assignment costs. We empirically show on simulated data, generated for a particle detector developed for proton imaging, the effectiveness of our approach, compared to multiple single- and multi-agent baselines. We further demonstrate the effectiveness of constraints with cost margins for both optimization and generalization, introduced by wider regions with high reconstruction performance as well as reduced predictive instabilities. Our results form the basis for further developments in RL-based tracking, offering both enhanced performance with constrained policies and greater flexibility in optimizing tracking algorithms through the option for individual and team rewards. 

**Abstract (ZH)**: 强化学习在建模复杂物理驱动系统方面展现了巨大的成功，通过与模拟或真实环境互动，提供端到端可训练的解决方案，最大化标量奖励信号。在这项工作中，我们在此前工作的基础上，提出了一种带分配约束的多智能体强化学习方法，用于在像素化粒子探测器中重构粒子轨迹。该方法通过联合最小化重建轨迹在读出帧中的总散射粒子量，协作优化一个参数化策略，该策略作为多维分配问题的启发式算法。为了满足约束要求，确保粒子击中点的唯一分配，我们提出了一种安全层，用于为每个联合动作解决线性分配问题。此外，为了增加局部策略预测与优化映射决策边界之间的成本差距，我们建议在黑盒梯度估计中增加一个额外组件，迫使策略向总分配成本较低的解决方案倾斜。我们通过模拟数据（模拟用于质子成像的粒子探测器生成的数据）的实验证明了我们方法的有效性，相比多个单智能体和多智能体基线，我们方法显示出更高的效果。我们还展示了带成本差距约束的有效性，这些约束在优化和泛化方面引入了更宽的重建性能高区域，以及减少了预测不稳定性的效果。我们的结果为基于RL的跟踪技术的进一步发展奠定了基础，提供了受限策略增强性能和通过个体和团队奖励优化跟踪算法更大灵活性的可能。 

---
# Advancing ALS Applications with Large-Scale Pre-training: Dataset Development and Downstream Assessment 

**Title (ZH)**: 大规模预训练推动ALS应用的发展：数据集开发与下游评估 

**Authors**: Haoyi Xiu, Xin Liu, Taehoon Kim, Kyoung-Sook Kim  

**Link**: [PDF](https://arxiv.org/pdf/2501.05095)  

**Abstract**: The pre-training and fine-tuning paradigm has revolutionized satellite remote sensing applications. However, this approach remains largely underexplored for airborne laser scanning (ALS), an important technology for applications such as forest management and urban planning. In this study, we address this gap by constructing a large-scale ALS point cloud dataset and evaluating its impact on downstream applications. Our dataset comprises ALS point clouds collected across the contiguous United States, provided by the United States Geological Survey's 3D Elevation Program. To ensure efficient data collection while capturing diverse land cover and terrain types, we introduce a geospatial sampling method that selects point cloud tiles based on land cover maps and digital elevation models. As a baseline self-supervised learning model, we adopt BEV-MAE, a state-of-the-art masked autoencoder for 3D outdoor point clouds, and pre-train it on the constructed dataset. The pre-trained models are subsequently fine-tuned for downstream tasks, including tree species classification, terrain scene recognition, and point cloud semantic segmentation. Our results show that the pre-trained models significantly outperform their scratch counterparts across all downstream tasks, demonstrating the transferability of the representations learned from the proposed dataset. Furthermore, we observe that scaling the dataset using our geospatial sampling method consistently enhances performance, whereas pre-training on datasets constructed with random sampling fails to achieve similar improvements. These findings highlight the utility of the constructed dataset and the effectiveness of our sampling strategy in the pre-training and fine-tuning paradigm. The source code and pre-trained models will be made publicly available at \url{this https URL}. 

**Abstract (ZH)**: 预训练和微调范式已经彻底改变了卫星遥感应用。然而，该方法在机载激光扫描（ALS）领域中的应用仍然相对未被探索，ALS 是森林管理和城市规划等应用中的一项重要技术。本研究旨在填补这一空白，通过构建大规模的 ALS 点云数据集并评估其对下游应用的影响。我们的数据集涵盖了美国连续地区的 ALS 点云数据，这些数据由美国地质调查局的 3D 地形计划提供。为了确保高效的数据采集同时捕获多种地表覆盖类型和地形类型，我们引入了一种地理空间采样方法，该方法基于土地覆盖图和数字高程模型来选择点云瓦片。作为基准的自监督学习模型，我们采用了最新的带有掩码的三维室外点云自编码器（BEV-MAE），并将其预训练在构建的数据集上。预训练模型随后用于下游任务的微调，包括树木种类分类、地形场景识别和点云语义分割。结果表明，预训练模型在所有下游任务中显著优于从零开始训练的模型，证明了所提出数据集中学习的表示的迁移性。此外，我们发现使用地理空间采样方法扩展数据集始终能够提高性能，而使用随机采样构造的数据集预训练则无法达到类似的改进效果。这些发现突显了所构建数据集的实用性以及我们在预训练和微调范式中的采样策略的有效性。源代码和预训练模型将在 \url{此网址} 公开。 

---
# Analyzing Memorization in Large Language Models through the Lens of Model Attribution 

**Title (ZH)**: 通过模型归因的视角分析大规模语言模型中的记忆现象 

**Authors**: Tarun Ram Menta, Susmit Agrawal, Chirag Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2501.05078)  

**Abstract**: Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on posthoc analyses, such as extracting memorized content or developing memorization metrics, without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the models generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPTNeo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real world applications. 

**Abstract (ZH)**: 大语言模型（LLMs）在现代应用中非常普遍，但往往会记住训练数据，这导致了隐私泄露和版权问题。现有研究主要集中在事后分析上，如提取记忆的内容或开发记忆度量标准，而未探索潜在的架构因素对记忆机制的影响。在本研究中，我们从架构的角度出发，分析不同层级的注意力模块如何影响记忆和泛化性能。通过归因技术，我们系统地干预LLM架构，绕过特定块的注意力模块，同时保持其他组件如层标准化和MLP变换不变。我们从数学角度提供了定理对我们的干预机制进行分析，界定了有和没有我们归因的层输出之间的差异。我们的理论和实证分析表明，较深的转子块中的注意力模块主要负责记忆现象，而较早的块对模型的泛化能力和推理能力至关重要。我们通过在不同LLM家族（Pythia和GPTNeo）和五个基准数据集上进行全面实验来验证这些发现。我们的洞见提供了一种实用的方法来缓解LLM中的记忆现象，同时保持其性能，从而有助于在实际应用中实现更安全和更伦理的部署。 

---
# Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning 

**Title (ZH)**: 通过视频支撑的蕴含树推理实现常识视频问题解答 

**Authors**: Huabin Liu, Filip Ilievski, Cees G. M. Snoek  

**Link**: [PDF](https://arxiv.org/pdf/2501.05069)  

**Abstract**: This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video and image-based VLMs across reasoning types. To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrites VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types. 

**Abstract (ZH)**: 本文提出了一种第一个基于视频的蕴含树推理方法，用于常识视频问答（VQA）。尽管大规模视觉语言模型（VLMs）取得了显著的进步，但人们越来越担心它们在视频和可能答案之间学习了虚假的相关性，这种相关性是由它们的黑盒性质和持续存在的基准测试偏差所强化的。我们的方法通过四个步骤显式地将VQA任务与视频片段进行关联：构建蕴含树、视频语言蕴含验证、树推理和动态树扩展。该方法的一个重要优势在于其可以跨不同类型的推理任务将通用性推广到当前的视频和图像基视觉语言模型。为了支持公平的评估，我们基于大型语言模型设计了一种去偏差程序，重新编写VQA基准答案集，以确保模型推理的正确性。对现有基准和去偏差基准进行系统的实验，突显了本方法在不同基准、视觉语言模型和推理类型上的影响。 

---
# D3RM: A Discrete Denoising Diffusion Refinement Model for Piano Transcription 

**Title (ZH)**: D3RM：一种用于钢琴转谱的离散去噪扩散精炼模型 

**Authors**: Hounsu Kim, Taegyun Kwon, Juhan Nam  

**Link**: [PDF](https://arxiv.org/pdf/2501.05068)  

**Abstract**: Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion model's refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in this https URL. 

**Abstract (ZH)**: 扩散模型因其在建模复杂数据分布方面表现出色而被广泛应用于生成领域。此外，它们在区分任务中也展现了竞争力，如图像分割。虽然扩散模型在自动音乐转录领域也受到了探索，但其性能尚未达到与其它方法竞争的水平。本文中，我们专注于离散扩散模型的细化能力，并提出了一种新型的钢琴转录架构。我们的模型将局部注意层作为去噪模块，基于预训练声学模型的细调特征，逐步预测目标高分辨率钢琴卷。为了进一步增强细化能力，我们设计了一种新颖的策略，在离散扩散模型的训练和推理阶段应用不同的过渡状态。在MAESTRO数据集上的实验表明，我们的方法在F1评分上优于之前的基于扩散模型的钢琴转录模型和基线模型。我们的代码可以在以下链接中获取：这个 https URL。 

---
# LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding 

**Title (ZH)**: LLaVA-Octopus：解锁基于指令的自适应投影融合以实现视频理解 

**Authors**: Jiaxing Zhao, Boyuan Sun, Xiang Chen, Xihan Wei, Qibin Hou  

**Link**: [PDF](https://arxiv.org/pdf/2501.05067)  

**Abstract**: In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as multimodal understanding, visual question answering, and video understanding, highlighting its broad application potential. 

**Abstract (ZH)**: 在本文中，我们介绍了一种新的视频多模态大型语言模型LLaVA-Octopus。LLaVA-Octopus根据用户指令自适应地加权来自不同视觉投影器的特征，从而能够利用每个投影器的互补优势。我们观察到，不同的视觉投影器在处理特定任务时表现出不同的特性。例如，有些投影器擅长捕捉静态细节，而另一些则更擅长处理时间信息，还有一些则更适合需要时间连贯性的任务。通过根据用户指令动态调整特征权重，LLaVA-Octopus能够动态选择和组合最适合的特征，显著增强了模型在多模态任务中的性能。实验结果表明，LLaVA-Octopus在多个基准测试中表现出色，尤其是在多模态理解、视觉问答和视频理解等任务中，突显了其广泛的应用潜力。 

---
# Improving Skeleton-based Action Recognition with Interactive Object Information 

**Title (ZH)**: 基于骨架的动作识别中的交互式物体信息改进方法 

**Authors**: Hao Wen, Ziqian Lu, Fengli Shen, Zhe-Ming Lu, Jialin Cui  

**Link**: [PDF](https://arxiv.org/pdf/2501.05066)  

**Abstract**: Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7\%, and on cross-view split, it is 99.2\%. 

**Abstract (ZH)**: 人体骨架信息在基于骨架的动作识别中非常重要，它提供了一种简单且高效的人体姿态描述方式。然而，现有的基于骨架的方法更多关注骨架本身，忽略与人类互动的物体，导致在涉及物体交互的动作识别上表现不佳。我们提出了一种新的动作识别框架，引入物体节点来补充缺失的互动物体信息。同时，我们提出了空间-temporal 变量图卷积网络（ST-VGCN）以有效地建模包含物体节点的变量图（Variable Graph, VG）。具体而言，为了验证互动物体信息的作用，我们通过利用简单的自训练方法建立了新的数据集 JXGC 24 和扩展数据集 NTU RGB+D+Object 60，并包含超过200万的额外物体节点。同时，我们设计了变量图构建方法以适应图形结构中节点数的变化。此外，我们首次探讨了引入额外物体信息导致的过拟合问题，并提出了一种基于变量图的数据增强方法，称为随机节点攻击（Random Node Attack）。在网络结构方面，我们引入了两种融合模块 CAF 和 WNPool，以及一种新型的节点平衡损失（Node Balance Loss），通过有效地融合和平衡骨架节点和物体节点信息，提升综合性能。我们的方法在多个基于骨架的动作识别基准测试中超过了之前的状态-of-the-art。在 NTU RGB+D 60 的跨个体分割上，我们的方法的准确率为96.7%，在跨视角分割上，准确率为99.2%。 

---
# Simultaneous emulation and downscaling with physically-consistent deep learning-based regional ocean emulators 

**Title (ZH)**: 基于物理一致的深度学习区域海洋模拟器的同时仿真的降尺度方法 

**Authors**: Leonard Lupin-Jimenez, Moein Darman, Subhashis Hazarika, Tianning Wu, Michael Gray, Ruyoing He, Anthony Wong, Ashesh Chattopadhyay  

**Link**: [PDF](https://arxiv.org/pdf/2501.05058)  

**Abstract**: Building on top of the success in AI-based atmospheric emulation, we propose an AI-based ocean emulation and downscaling framework focusing on the high-resolution regional ocean over Gulf of Mexico. Regional ocean emulation presents unique challenges owing to the complex bathymetry and lateral boundary conditions as well as from fundamental biases in deep learning-based frameworks, such as instability and hallucinations. In this paper, we develop a deep learning-based framework to autoregressively integrate ocean-surface variables over the Gulf of Mexico at $8$ Km spatial resolution without unphysical drifts over decadal time scales and simulataneously downscale and bias-correct it to $4$ Km resolution using a physics-constrained generative model. The framework shows both short-term skills as well as accurate long-term statistics in terms of mean and variability. 

**Abstract (ZH)**: 在基于人工智能的大气模拟取得成功的基础上，我们提出了一种针对墨西哥湾高分辨率区域海洋的人工智能海洋模拟和缩尺度框架。区域海洋模拟由于复杂的海底地形、侧向边界条件以及深度学习框架中的根本性偏差（如不稳定性与幻觉）而面临着独特的挑战。在本文中，我们开发了一种基于深度学习的方法，以自回归的方式整合墨西哥湾的海洋表层变量，空间分辨率为8公里，同时在长达数十年的时间尺度上避免了不可物理的漂移，并将其缩尺度和偏差校正至4公里分辨率，利用一个受到物理约束的生成模型。该框架在短期技能和长期统计特性（包括均值和变异性）方面均表现出色。 

---
# TAPFed: Threshold Secure Aggregation for Privacy-Preserving Federated Learning 

**Title (ZH)**: TAPFed：阈值安全聚合在隐私保护联邦学习中的应用 

**Authors**: Runhua Xu, Bo Li, Chao Li, James B.D. Joshi, Shuai Ma, Jianxin Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.05053)  

**Abstract**: Federated learning is a computing paradigm that enhances privacy by enabling multiple parties to collaboratively train a machine learning model without revealing personal data. However, current research indicates that traditional federated learning platforms are unable to ensure privacy due to privacy leaks caused by the interchange of gradients. To achieve privacy-preserving federated learning, integrating secure aggregation mechanisms is essential. Unfortunately, existing solutions are vulnerable to recently demonstrated inference attacks such as the disaggregation attack. This paper proposes TAPFed, an approach for achieving privacy-preserving federated learning in the context of multiple decentralized aggregators with malicious actors. TAPFed uses a proposed threshold functional encryption scheme and allows for a certain number of malicious aggregators while maintaining security and privacy. We provide formal security and privacy analyses of TAPFed and compare it to various baselines through experimental evaluation. Our results show that TAPFed offers equivalent performance in terms of model quality compared to state-of-the-art approaches while reducing transmission overhead by 29%-45% across different model training scenarios. Most importantly, TAPFed can defend against recently demonstrated inference attacks caused by curious aggregators, which the majority of existing approaches are susceptible to. 

**Abstract (ZH)**: 联邦学习是一种计算范式，通过使多个参与方能够协作训练机器学习模型而不泄露个人数据来增强隐私。然而，当前的研究表明，由于在梯度交换过程中出现的隐私泄露问题，传统的联邦学习平台无法保证隐私。为了实现隐私保护的联邦学习，集成安全聚合机制是必不可少的。不幸的是，现有的解决方案对最近展示的推理攻击（如拆分攻击）仍然容易受到攻击。本文提出了一种名为TAPFed的方法，用于在存在恶意聚合器的多个去中心化聚合器环境下实现隐私保护的联邦学习。TAPFed利用了一个提出的门限功能加密方案，并能够在存在一定数量恶意聚合器的情况下保持安全和隐私。我们对TAPFed进行了形式化的安全性与隐私性分析，并通过实验评估将其与各种基线进行比较。结果表明，在模型质量方面，TAPFed与现有的最先进的方法相当，同时在不同模型训练场景中将传输开销减少了29%-45%。更为重要的是，TAPFed能够抵御由好奇聚合器引起的最近展示出的推理攻击，而大多数现有的方法对此类攻击都较为脆弱。 

---
# Enhancing Human-Like Responses in Large Language Models 

**Title (ZH)**: 增强大型语言模型的人类like响应 

**Authors**: Ethem Yağız Çalık, Talha Rüzgar Akkuş  

**Link**: [PDF](https://arxiv.org/pdf/2501.05032)  

**Abstract**: This paper explores the advancements in making large language models (LLMs) more human-like. We focus on techniques that enhance natural language understanding, conversational coherence, and emotional intelligence in AI systems. The study evaluates various approaches, including fine-tuning with diverse datasets, incorporating psychological principles, and designing models that better mimic human reasoning patterns. Our findings demonstrate that these enhancements not only improve user interactions but also open new possibilities for AI applications across different domains. Future work will address the ethical implications and potential biases introduced by these human-like attributes. 

**Abstract (ZH)**: 本文探讨了使大型语言模型（LLMs）更加接近人类发展的进步。我们专注于提高AI系统在自然语言理解、对话连贯性和情感智能方面的技术。研究评估了各种方法，包括使用多样化的数据集进行微调、整合心理学原理以及设计更好地模仿人类推理模式的模型。研究结果表明，这些改进不仅提高了用户互动的质量，还为AI在不同领域的应用开辟了新的可能性。未来的工作将关注这些人性化特征所带来的伦理影响和潜在偏见。 

---
# Finding Needles in Emb(a)dding Haystacks: Legal Document Retrieval via Bagging and SVR Ensembles 

**Title (ZH)**: 在嵌入式干草堆中寻找针：基于Bootstrap aggregating和SVR集成的法律文档检索 

**Authors**: Kevin Bönisch, Alexander Mehler  

**Link**: [PDF](https://arxiv.org/pdf/2501.05018)  

**Abstract**: We introduce a retrieval approach leveraging Support Vector Regression (SVR) ensembles, bootstrap aggregation (bagging), and embedding spaces on the German Dataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the retrieval task in terms of multiple binary needle-in-a-haystack subtasks, we show improved recall over the baselines (0.849 > 0.803 | 0.829) using our voting ensemble, suggesting promising initial results, without training or fine-tuning any deep learning models. Our approach holds potential for further enhancement, particularly through refining the encoding models and optimizing hyperparameters. 

**Abstract (ZH)**: 我们提出了一种利用支持向量回归（SVR）集成、自助聚合（bagging）以及嵌入空间的方法，应用于德语法律信息检索数据集（GerDaLIR）。通过将检索任务转化为多个二元“针扎草堆”子任务，我们使用投票集成方法在德语数据集上取得了优于基线模型的召回率（0.849 > 0.803 | 0.829），表明了初始的有希望的结果，而无需任何深度学习模型的训练或微调。我们的方法具有进一步改进的潜力，特别是在改进编码模型和优化超参数方面。 

---
# On Measuring Unnoticeability of Graph Adversarial Attacks: Observations, New Measure, and Applications 

**Title (ZH)**: 关于衡量图 adversarial 攻击不可察觉性的评估：观察、新度量及其应用 

**Authors**: Hyeonsoo Jo, Hyunjin Hwang, Fanchen Bu, Soo Yong Lee, Chanyoung Park, Kijung Shin  

**Link**: [PDF](https://arxiv.org/pdf/2501.05015)  

**Abstract**: Adversarial attacks are allegedly unnoticeable. Prior studies have designed attack noticeability measures on graphs, primarily using statistical tests to compare the topology of original and (possibly) attacked graphs. However, we observe two critical limitations in the existing measures. First, because the measures rely on simple rules, attackers can readily enhance their attacks to bypass them, reducing their attack "noticeability" and, yet, maintaining their attack performance. Second, because the measures naively leverage global statistics, such as degree distributions, they may entirely overlook attacks until severe perturbations occur, letting the attacks be almost "totally unnoticeable." To address the limitations, we introduce HideNSeek, a learnable measure for graph attack noticeability. First, to mitigate the bypass problem, HideNSeek learns to distinguish the original and (potential) attack edges using a learnable edge scorer (LEO), which scores each edge on its likelihood of being an attack. Second, to mitigate the overlooking problem, HideNSeek conducts imbalance-aware aggregation of all the edge scores to obtain the final noticeability score. Using six real-world graphs, we empirically demonstrate that HideNSeek effectively alleviates the observed limitations, and LEO (i.e., our learnable edge scorer) outperforms eleven competitors in distinguishing attack edges under five different attack methods. For an additional application, we show that LEO boost the performance of robust GNNs by removing attack-like edges. 

**Abstract (ZH)**: adversarial 攻击据说无法被察觉。先前的研究在图上设计了攻击可察觉性度量，主要使用统计检验来比较原始图和可能被攻击后的图的拓扑结构。然而，我们发现现有的度量存在两个关键限制。首先，由于这些度量依赖于简单的规则，攻击者可以很容易地增强他们的攻击以避开这些度量，从而减少攻击的“察觉度”但仍然保持其攻击性能。其次，由于这些度量天真地利用了全局统计特征，如度分布，可能会在严重扰动发生之前完全忽略攻击，使得攻击几乎“完全不可察觉”。为了解决这些限制，我们引入了 HideNSeek，这是一种学习可调节的图攻击察觉度量。首先，为了缓解绕过问题，HideNSeek 学习使用可学习边评分器（LEO）来区分原始边和（潜在的）攻击边，LEO 评估每条边成为攻击边的概率。其次，为了缓解忽略问题，HideNSeek 采用不平衡感知聚合所有边评分，以获得最终的察觉度分数。通过使用六个真实世界的图，我们实证展示了 HideNSeek 有效缓解了观察到的限制，并且 LEO（即我们的可学习边评分器）在五种不同攻击方法下区分攻击边的能力优于十一个竞争对手。作为额外的应用，我们展示了LEO能够提升鲁棒图神经网络（GNN）的性能，通过去除类似攻击的边。 

---
# UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation 

**Title (ZH)**: UAV-VLA：大规模空中任务生成的视觉-语言-行动系统 

**Authors**: Oleg Sautenkov, Yasheerah Yaqoot, Artem Lykov, Muhammad Ahsan Mustafa, Grik Tadevosyan, Aibek Akhmetkazy, Miguel Altamirano Cabrera, Mikhail Martynov, Sausar Karaf, Dzmitry Tsetserukou  

**Link**: [PDF](https://arxiv.org/pdf/2501.05014)  

**Abstract**: The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach. 

**Abstract (ZH)**: UAV-VLA（视觉-语言-动作）系统是一种旨在促进与空中机器人通信的工具。通过将卫星影像处理与视觉语言模型（VLM）以及GPT的强大能力相结合，UAV-VLA允许用户通过简单的文本请求生成通用的飞行路径和行动计划。该系统利用卫星影像提供的丰富上下文信息，增强了决策能力和任务规划。VLM的视觉分析与GPT的自然语言处理相结合，使用户能够获得路径和行动集，从而提高空中操作的效率和 accessibility。新开发的方法在K-近邻（KNN）方法中显示了创建轨迹长度的差异为22%，以及通过欧几里得距离在寻找地图上感兴趣的目标时的平均误差为34.22米。 

---
# Quantum-enhanced causal discovery for a small number of samples 

**Title (ZH)**: 量子增强的因果发现方法用于少量样本 

**Authors**: Yota Maeda, Ken Arai, Yu Tanaka, Yu Terada, Hiroshi Ueno, Hiroyuki Tezuka  

**Link**: [PDF](https://arxiv.org/pdf/2501.05007)  

**Abstract**: The discovery of causal relationships from observed data has attracted significant interest from disciplines such as economics, social sciences, epidemiology, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are often associated with nonlinear causal structures, which make the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not assume any underlying model structures. Based on the independence conditional tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed qPC algorithm can explore causal relationships from the observed data drawn from arbitrary distributions. We conducted systematic experiments on fundamental graph parts of causal structures, demonstrating that the qPC algorithm exhibits a significantly better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the proposed quantum algorithm can empower classical algorithms for robust and accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. Additionally, the effectiveness of this method was validated using the Boston Housing dataset as a real-world application. These findings demonstrate the new potential of quantum circuit-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios where traditional approaches have shown limitations. 

**Abstract (ZH)**: 从观测数据中发现因果关系的研究吸引了经济学、社会科学、流行病学和生物学等多个领域的广泛关注。在实际应用中，往往缺乏对底层系统的充分了解，实际数据通常与非线性因果结构相关联，这使得大多数传统的因果分析方法直接应用变得困难。为了应对这一挑战，本研究提出了一种新型的量子彼得-克拉克（qPC）算法，该算法不需要假设任何底层的模型结构。基于由量子电路定义的一类再生核希尔伯特空间中的条件独立性检验，qPC算法可以从任意分布的数据中探索因果关系。我们对因果结构的基本图部分进行了系统的实验研究，结果表明qPC算法在小样本情况下相对于其经典对应物表现出明显更好的性能。此外，我们基于内核目标对齐（KTA）提出了一种新的优化方法，用于确定量子内核的超参数，这种方法有效降低了因果发现中误报的风险，提高了推断的可靠性。我们的理论和实验结果表明，所提出的量子算法可以增强经典算法在因果发现中的鲁棒性和准确性，在经典算法通常失效的情境下发挥重要作用。同时，我们使用波士顿住房数据集作为实际应用的实例验证了该方法的有效性。这些发现展示了基于量子电路的因果发现方法在解决实际问题中的新潜力，特别是当传统方法在小样本场景中表现出局限性时。 

---
# GiNet: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction 

**Title (ZH)**: GiNet：结合序列学习和上下文感知学习的电池容量预测 

**Authors**: Sara Sameer, Wei Zhang, Xin Lou, Qingyu Yan, Terence Goh, Yulin Gao  

**Link**: [PDF](https://arxiv.org/pdf/2501.04997)  

**Abstract**: The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality. In this paper, we aim to achieve accurate battery capacity prediction by learning from historical measurements of battery dynamics. We propose GiNet, a gated recurrent units enhanced Informer network, for predicting battery's capacity. The novelty and competitiveness of GiNet lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery's complex behaviors with both temporal dynamics and long-term dependencies. We conducted an experimental study based on a publicly available dataset to showcase GiNet's strength of gaining a holistic understanding of battery behavior and predicting battery capacity accurately. GiNet achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity. It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer. The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well. 

**Abstract (ZH)**: 电池需求的激增推动了先进电池管理系统的发展，其中电池容量建模是关键功能之一。本文旨在通过学习电池动力学的历史测量数据来实现准确的电池容量预测。我们提出了一种增强的基于门控循环单元的Informer网络（GiNet），用于预测电池的容量。GiNet 的创新性和竞争力在于其能够从原始电池数据中捕获序列和上下文信息，并通过时间动态和长期依赖来反映电池的复杂行为。我们基于一个公开可用的数据集进行了实验研究，以展示GiNet 对电池行为的全面理解和准确预测能力。在不依赖历史电池容量的情况下，GiNet 对未来时间槽的电池容量预测实现了0.11的平均绝对误差。相较于Informer，平均误差降低了27%，显著优于最新算法。这一有希望的结果突显了算法和电池知识定制化和优化集成的重要性，并为其他行业应用提供了启示。 

---
# IPDN: Image-enhanced Prompt Decoding Network for 3D Referring Expression Segmentation 

**Title (ZH)**: IPDN：增强提示解码网络在3D引用表达分割中的应用 

**Authors**: Qi Chen, Changli Wu, Jiayi Ji, Yiwei Ma, Danni Yang, Xiaoshuai Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.04995)  

**Abstract**: 3D Referring Expression Segmentation (3D-RES) aims to segment point cloud scenes based on a given expression. However, existing 3D-RES approaches face two major challenges: feature ambiguity and intent ambiguity. Feature ambiguity arises from information loss or distortion during point cloud acquisition due to limitations such as lighting and viewpoint. Intent ambiguity refers to the model's equal treatment of all queries during the decoding process, lacking top-down task-specific guidance. In this paper, we introduce an Image enhanced Prompt Decoding Network (IPDN), which leverages multi-view images and task-driven information to enhance the model's reasoning capabilities. To address feature ambiguity, we propose the Multi-view Semantic Embedding (MSE) module, which injects multi-view 2D image information into the 3D scene and compensates for potential spatial information loss. To tackle intent ambiguity, we designed a Prompt-Aware Decoder (PAD) that guides the decoding process by deriving task-driven signals from the interaction between the expression and visual features. Comprehensive experiments demonstrate that IPDN outperforms the state-ofthe-art by 1.9 and 4.2 points in mIoU metrics on the 3D-RES and 3D-GRES tasks, respectively. 

**Abstract (ZH)**: 3D参考表达分割（3D-RES）旨在根据给定的表达对点云场景进行分割。然而，现有的3D-RES方法面临两大挑战：特征模糊和意图模糊。特征模糊源于点云采集过程中由于光照和视角等限制导致的信息损失或失真。意图模糊指的是模型在解码过程中对所有查询给予同等处理，缺乏自上而下的特定任务指导。在本文中，我们引入了图像增强提示解码网络（IPDN），该网络利用多视角图像和任务驱动的信息来增强模型的推理能力。为解决特征模糊问题，我们提出了多视图语义嵌入（MSE）模块，该模块将多视角2D图像信息注入3D场景中，以补偿潜在的空间信息损失。为解决意图模糊问题，我们设计了一个提示感知解码器（PAD），通过表达与视觉特征的交互生成任务驱动信号以指导解码过程。全面的实验表明，与3D-RES和3D-GRES任务中的现有方法相比，IPDN分别在mIoU指标上表现出了1.9和4.2的提高。 

---
# CuRLA: Curriculum Learning Based Deep Reinforcement Learning for Autonomous Driving 

**Title (ZH)**: CuRLA：基于课程学习的深度强化学习在自动驾驶中的应用 

**Authors**: Bhargava Uppuluri, Anjel Patel, Neil Mehta, Sridhar Kamath, Pratyush Chakraborty  

**Link**: [PDF](https://arxiv.org/pdf/2501.04982)  

**Abstract**: In autonomous driving, traditional Computer Vision (CV) agents often struggle in unfamiliar situations due to biases in the training data. Deep Reinforcement Learning (DRL) agents address this by learning from experience and maximizing rewards, which helps them adapt to dynamic environments. However, ensuring their generalization remains challenging, especially with static training environments. Additionally, DRL models lack transparency, making it difficult to guarantee safety in all scenarios, particularly those not seen during training. To tackle these issues, we propose a method that combines DRL with Curriculum Learning for autonomous driving. Our approach uses a Proximal Policy Optimization (PPO) agent and a Variational Autoencoder (VAE) to learn safe driving in the CARLA simulator. The agent is trained using two-fold curriculum learning, progressively increasing environment difficulty and incorporating a collision penalty in the reward function to promote safety. This method improves the agent's adaptability and reliability in complex environments, and understand the nuances of balancing multiple reward components from different feedback signals in a single scalar reward function. Keywords: Computer Vision, Deep Reinforcement Learning, Variational Autoencoder, Proximal Policy Optimization, Curriculum Learning, Autonomous Driving. 

**Abstract (ZH)**: 在自动驾驶领域，传统的计算机视觉（CV）代理往往在不熟悉的情况下因训练数据中的偏差而表现不佳。深度强化学习（DRL）代理通过从经验中学习并最大化奖励来解决这一问题，这有助于它们适应动态环境。然而，确保其泛化仍然具有挑战性，尤其是在静态的训练环境中。此外，DRL模型缺乏透明度，这使得在所有场景中保证其安全性变得困难，尤其是那些在训练期间未见过的场景。为了应对这些问题，我们提出了一种结合DRL与课程学习的方法，用于自动驾驶。我们的方法使用Proximal Policy Optimization（PPO）代理和Variational Autoencoder（VAE）在CARLA模拟器中学习安全驾驶。代理通过两阶段的课程学习进行训练，逐步增加环境难度，并在奖励函数中引入碰撞惩罚以促进安全。该方法提高了代理在复杂环境中的适应性和可靠性，并能够理解将来自多种反馈信号的多个奖励组件平衡到单个标量奖励函数中的细微差别。关键词：计算机视觉、深度强化学习、变分自编码器、 proximal 政策优化、课程学习、自动驾驶。 

---
# SensorQA: A Question Answering Benchmark for Daily-Life Monitoring 

**Title (ZH)**: SensorQA：日常生活监控领域的问答基准数据集 

**Authors**: Benjamin Reichman, Xiaofan Yu, Lanxiang Hu, Jack Truxal, Atishay Jain, Rushil Chandrupatla, Tajana Šimunić Rosing, Larry Heck  

**Link**: [PDF](https://arxiv.org/pdf/2501.04974)  

**Abstract**: With the rapid growth in sensor data, effectively interpreting and interfacing with these data in a human-understandable way has become crucial. While existing research primarily focuses on learning classification models, fewer studies have explored how end users can actively extract useful insights from sensor data, often hindered by the lack of a proper dataset. To address this gap, we introduce \Dataset, the first human-created question-answering (QA) dataset for long-term time-series sensor data for daily life monitoring. \Dataset is created by human workers and includes 5.6K diverse and practical queries that reflect genuine human interests, paired with accurate answers derived from sensor data. We further establish benchmarks for state-of-the-art AI models on this dataset and evaluate their performance on typical edge devices. Our results reveal a gap between current models and optimal QA performance and efficiency, highlighting the need for new contributions. The dataset and code are available at: \url{this https URL}. 

**Abstract (ZH)**: 随着传感器数据的迅速增长，有效地以人类可理解的方式解释和交互这些数据变得至关重要。目前，现有研究主要集中在学习分类模型上，较少的研究探讨了终端用户如何主动从传感器数据中提取有用见解，这通常受到合适数据集缺乏的限制。为了解决这一问题，我们引入了\Dataset，这是首个用于日常生活监测中长期时间序列传感器数据的人工创建的问答（QA）数据集。\Dataset由人类工作者创建，包含5,600个多样且实用的问题和准确的答案，这些答案反映了真实的人类兴趣。我们还在此数据集上建立了最先进的AI模型基准，并评估了其在典型边缘设备上的性能。结果显示，当前模型在问答性能和效率方面存在差距，突显了需要新贡献的需求。数据集和代码可在以下链接获取：\url{this https URL}。 

---
# Battling the Non-stationarity in Time Series Forecasting via Test-time Adaptation 

**Title (ZH)**: 通过测试时自适应对抗时间序列预测中的非平稳性 

**Authors**: HyunGi Kim, Siwon Kim, Jisoo Mok, Sungroh Yoon  

**Link**: [PDF](https://arxiv.org/pdf/2501.04970)  

**Abstract**: Deep Neural Networks have spearheaded remarkable advancements in time series forecasting (TSF), one of the major tasks in time series modeling. Nonetheless, the non-stationarity of time series undermines the reliability of pre-trained source time series forecasters in mission-critical deployment settings. In this study, we introduce a pioneering test-time adaptation framework tailored for TSF (TSF-TTA). TAFAS, the proposed approach to TSF-TTA, flexibly adapts source forecasters to continuously shifting test distributions while preserving the core semantic information learned during pre-training. The novel utilization of partially-observed ground truth and gated calibration module enables proactive, robust, and model-agnostic adaptation of source forecasters. Experiments on diverse benchmark datasets and cutting-edge architectures demonstrate the efficacy and generality of TAFAS, especially in long-term forecasting scenarios that suffer from significant distribution shifts. The code is available at this https URL. 

**Abstract (ZH)**: 深度神经网络在时间序列预测（TSF）任务中取得了显著进展，这是时间序列建模中的主要任务之一。然而，时间序列的非平稳性在关键任务部署环境中削弱了预训练时间序列预测器的可靠性。本研究提出了一种针对TSF的首创测试时自适应框架（TSF-TTA）。所提出的TSF-TTA框架中的方法TAFAS能够灵活地调整源预测器以适应不断变化的测试分布，同时保留预训练期间学到的核心语义信息。部分观察到的真实地面标签的新利用和门控校准模块使源预测器能够实现主动、稳健且模型无关的自适应。在多种基准数据集和前沿架构上的实验结果表明，TAFAS 在长期预测场景中尤其是在面对显著分布变化的情况下，具有有效性和普适性。代码可在以下链接获取：this https URL。 

---
# Demystifying Domain-adaptive Post-training for Financial LLMs 

**Title (ZH)**: 揭开金融LLM训练后适应领域的神秘面纱 

**Authors**: Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty  

**Link**: [PDF](https://arxiv.org/pdf/2501.04961)  

**Abstract**: Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: this https URL 

**Abstract (ZH)**: 面向金融领域的大语言模型（LLMs）的自适应后训练已成为一种有前途的方法。然而，在不同数据和模型配置下识别最优自适应标准和训练策略仍面临显著挑战。为应对这些挑战，我们提出了FINDAP，这是一种系统性的、细致入微的研究，旨在探索适用于金融领域的LLMs的自适应后训练方法。我们的方法首先确定目标领域所需的核心能力，并设计一个与这些需求相一致的全面评估套件。然后，我们分析了关键后训练阶段的有效性，包括持续预训练、指令调优和偏好对齐。基于这些洞察，我们提出了一种有效的训练配方，集中于一种新的偏好数据蒸馏方法，该方法利用生成奖励模型的过程信号。所得到的模型Llama-Fin在一系列金融任务中取得了最先进的性能。我们的分析还揭示了每个后训练阶段如何贡献不同的能力，并具体指出了相关挑战和有效解决方案，为LLMs领域的自适应提供了宝贵的见解。项目页面：[这个链接](this https URL) 

---
# Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo Development Assessment 

**Title (ZH)**: 通过考虑不平衡的领域适应解决胚胎发育评估中的领域偏移问题 

**Authors**: Lei Li, Xinglin Zhang, Jun Liang, Tao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.04958)  

**Abstract**: Deep learning models in medical imaging face dual challenges: domain shift, where models perform poorly when deployed in settings different from their training environment, and class imbalance, where certain disease conditions are naturally underrepresented. We present Imbalance-Aware Domain Adaptation (IADA), a novel framework that simultaneously tackles both challenges through three key components: (1) adaptive feature learning with class-specific attention mechanisms, (2) balanced domain alignment with dynamic weighting, and (3) adaptive threshold optimization. Our theoretical analysis establishes convergence guarantees and complexity bounds. Through extensive experiments on embryo development assessment across four imaging modalities, IADA demonstrates significant improvements over existing methods, achieving up to 25.19\% higher accuracy while maintaining balanced performance across classes. In challenging scenarios with low-quality imaging systems, IADA shows robust generalization with AUC improvements of up to 12.56\%. These results demonstrate IADA's potential for developing reliable and equitable medical imaging systems for diverse clinical settings. The code is made public available at \url{this https URL} 

**Abstract (ZH)**: 深度学习模型在医学影像领域面临双重挑战：领域转移（domain shift），即当模型在与训练环境不同的设置中部署时表现不佳；以及类别不平衡（class imbalance），即某些疾病状况在自然状态下被严重低估。我们提出了平衡感知领域适应（Imbalance-Aware Domain Adaptation, IADA），这是一种新颖的框架，通过三个关键组件同时解决这两个问题：（1）具有类别特异性注意力机制的自适应特征学习；（2）动态加权平衡领域对齐；（3）自适应阈值优化。我们的理论分析提供了收敛性保证并设定了复杂性界限。通过对四个成像模态下胚胎发育评估进行广泛实验，IADA在多种类别的基础上显著提高了性能，相对现有方法达到了高达25.19%的准确率提升。在低质量成像系统的挑战性场景中，IADA表现出稳健的泛化能力，AUC提高了12.56%。这些结果证明了IADA在开发适用于各种临床环境的可靠且公平的医学影像系统方面的潜力。相关代码已在以下网址公开发布：\url{this https URL} 

---
# Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models 

**Title (ZH)**: 逐步掌握：提高大型语言模型遵循软约束的能力 

**Authors**: Qingyu Ren, Jie Zeng, Qianyu He, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu  

**Link**: [PDF](https://arxiv.org/pdf/2501.04945)  

**Abstract**: It is crucial for large language models (LLMs) to follow instructions that involve multiple constraints. However, soft constraints are semantically related and difficult to verify through automated methods. These constraints remain a significant challenge for LLMs. To enhance the ability of LLMs to follow soft constraints, we initially design a pipeline to obtain high-quality outputs automatically. Additionally, to fully utilize the acquired data, we introduce a training paradigm based on curriculum learning. We experimentally evaluate the effectiveness of our methods in improving LLMs' soft constraint following ability and analyze the factors driving the improvements. The datasets and code are publicly available at this https URL. 

**Abstract (ZH)**: 对于大型语言模型（LLMs）来说，遵循涉及多重约束的指令至关重要。然而，软约束是语义相关的，并且难以通过自动化方法进行验证。这些约束仍然是LLMs面临的一个重要挑战。为了增强LLMs遵循软约束的能力，我们首先设计了一个管道来自动化获取高质量的输出。此外，为了充分利用获得的数据，我们引入了一种基于渐进学习的教学范式。我们实证评估了方法在提高LLMs遵循软约束能力方面的有效性，并分析了驱动改进的因素。相关数据集和代码可在此网址访问：https://your-link-here.com。 

---
# Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency 

**Title (ZH)**: 通过洗牌不一致性破解多模态大型语言模型 

**Authors**: Shiji Zhao, Ranjie Duan, Fengxiang Wang, Chi Chen, Caixin Kang, Jialing Tao, YueFeng Chen, Hui Xue, Xingxing Wei  

**Link**: [PDF](https://arxiv.org/pdf/2501.04931)  

**Abstract**: Multimodal Large Language Models (MLLMs) have achieved impressive performance and have been put into practical use in commercial applications, but they still have potential safety mechanism vulnerabilities. Jailbreak attacks are red teaming methods that aim to bypass safety mechanisms and discover MLLMs' potential risks. Existing MLLMs' jailbreak methods often bypass the model's safety mechanism through complex optimization methods or carefully designed image and text prompts. Despite achieving some progress, they have a low attack success rate on commercial closed-source MLLMs. Unlike previous research, we empirically find that there exists a Shuffle Inconsistency between MLLMs' comprehension ability and safety ability for the shuffled harmful instruction. That is, from the perspective of comprehension ability, MLLMs can understand the shuffled harmful text-image instructions well. However, they can be easily bypassed by the shuffled harmful instructions from the perspective of safety ability, leading to harmful responses. Then we innovatively propose a text-image jailbreak attack named SI-Attack. Specifically, to fully utilize the Shuffle Inconsistency and overcome the shuffle randomness, we apply a query-based black-box optimization method to select the most harmful shuffled inputs based on the feedback of the toxic judge model. A series of experiments show that SI-Attack can improve the attack's performance on three benchmarks. In particular, SI-Attack can obviously improve the attack success rate for commercial MLLMs such as GPT-4o or Claude-3.5-Sonnet. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在商业应用中实现了令人印象深刻的表现并已投入实际使用，但它们仍然存在潜在的安全机制漏洞。Jailbreak攻击是一种红队测试方法，旨在绕过安全机制并发现MLLMs的潜在风险。现有的MLLMs的Jailbreak方法通常通过复杂的优化方法或精心设计的图像和文本提示来规避模型的安全机制。尽管取得了一些进展，但它们在商业闭源的MLLMs上的攻击成功率相对较低。与以往研究不同，我们通过实证研究发现，MLLMs在处理打乱的有害指令时存在推理能力和安全能力之间的混淆一致性（Shuffle Inconsistency）。具体来说，从推理能力的角度来看，MLLMs能够很好地理解打乱的有害文本-图像指令。然而，从安全能力的角度来看，它们很容易被这些打乱的有害指令绕过，从而产生有害响应。随后，我们创新地提出了一个名为SI-Attack的文本-图像Jailbreak攻击方法。具体而言，为了充分利用混淆一致性并克服打乱的随机性，我们应用基于查询的黑盒优化方法，根据有害判断模型的反馈来选择最具有危害性的打乱输入。一系列实验表明，SI-Attack能够提高在三个基准上的攻击性能。特别是，SI-Attack能够明显提高针对如GPT-4或Claude-3.5-Sonnet等商业MLLMs的攻击成功率。 

---
# Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images 

**Title (ZH)**: Image2CADSeq：从产品图像推断计算机辅助设计序列与知识推理 

**Authors**: Xingang Li, Zhenghui Sha  

**Link**: [PDF](https://arxiv.org/pdf/2501.04928)  

**Abstract**: Computer-aided design (CAD) tools empower designers to design and modify 3D models through a series of CAD operations, commonly referred to as a CAD sequence. In scenarios where digital CAD files are not accessible, reverse engineering (RE) has been used to reconstruct 3D CAD models. Recent advances have seen the rise of data-driven approaches for RE, with a primary focus on converting 3D data, such as point clouds, into 3D models in boundary representation (B-rep) format. However, obtaining 3D data poses significant challenges, and B-rep models do not reveal knowledge about the 3D modeling process of designs. To this end, our research introduces a novel data-driven approach with an Image2CADSeq neural network model. This model aims to reverse engineer CAD models by processing images as input and generating CAD sequences. These sequences can then be translated into B-rep models using a solid modeling kernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify individual steps of model creation, providing a deeper understanding of the construction process of CAD models. To quantitatively and rigorously evaluate the predictive performance of the Image2CADSeq model, we have developed a multi-level evaluation framework for model assessment. The model was trained on a specially synthesized dataset, and various network architectures were explored to optimize the performance. The experimental and validation results show great potential for the model in generating CAD sequences from 2D image data. 

**Abstract (ZH)**: 计算机辅助设计（CAD）工具使设计者能够通过一系列CAD操作设计和修改3D模型，这些操作通常被称为CAD序列。在无法访问数字CAD文件的情况下，逆向工程（RE）已被用于重建3D CAD模型。近年来，基于数据驱动的方法在逆向工程中取得进展，主要关注将点云等3D数据转换为边界表示（B-rep）格式的3D模型。然而，获取3D数据存在着显著的挑战，且B-rep模型无法揭示设计的3D建模过程知识。为此，我们的研究提出了一种新的数据驱动方法，即使用Image2CADSeq神经网络模型。该模型旨在通过处理图像作为输入并生成CAD序列来实现逆向工程。然后，这些序列可以通过实体建模内核转换为B-rep模型。与B-rep模型不同，CAD序列提供了对模型创建步骤进行单独修改的增强灵活性，从而更深入地理解CAD模型的构建过程。为了定量和严谨地评估Image2CADSeq模型的预测性能，我们开发了一种多级评估框架来进行模型评估。该模型是在特制合成的数据集上进行训练的，并探索了多种网络架构以优化性能。实验和验证结果表明，该模型在从2D图像数据生成CAD序列方面具有巨大的潜力。 

---
# FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with Single-Step Flow Matching 

**Title (ZH)**: FlowHigh: 向量高效且高质量的单步流匹配音频超分辨率 

**Authors**: Jun-Hak Yun, Seung-Bin Kim, Seong-Whan Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.04926)  

**Abstract**: Audio super-resolution is challenging owing to its ill-posed nature. Recently, the application of diffusion models in audio super-resolution has shown promising results in alleviating this challenge. However, diffusion-based models have limitations, primarily the necessity for numerous sampling steps, which causes significantly increased latency when synthesizing high-quality audio samples. In this paper, we propose FLowHigh, a novel approach that integrates flow matching, a highly efficient generative model, into audio super-resolution. We also explore probability paths specially tailored for audio super-resolution, which effectively capture high-resolution audio distributions, thereby enhancing reconstruction quality. The proposed method generates high-fidelity, high-resolution audio through a single-step sampling process across various input sampling rates. The experimental results on the VCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art performance in audio super-resolution, as evaluated by log-spectral distance and ViSQOL while maintaining computational efficiency with only a single-step sampling process. 

**Abstract (ZH)**: 音频超分辨率由于其病态性质而具有挑战性。最近，将扩散模型应用于音频超分辨率已显示出缓解这一挑战的前景。然而，基于扩散的模型存在局限性，主要在于需要大量的采样步骤，从而在合成高质量音频样本时导致显著增加的延迟。本文中，我们提出了一种名为FLowHigh的新方法，该方法将具有高效率的生成模型流匹配技术整合到音频超分辨率中。我们还探索了特别为音频超分辨率设计的概率路径，这些路径能够有效地捕捉到高分辨率音频分布，从而提高重构质量。提出的方法通过单一采样步骤生成高质量、高分辨率音频，适用于不同的输入采样率。在VCTK基准数据集上的实验结果表明，FLowHigh在音频超分辨率方面取得了最先进的性能，根据对数谱距离和ViSQOL评估，同时仅使用单一采样步骤保持了计算效率。 

---
# SUGAR: Leveraging Contextual Confidence for Smarter Retrieval 

**Title (ZH)**: SUGAR：利用上下文置信度提升智能检索 

**Authors**: Hanna Zubkova, Ji-Hoon Park, Seong-Whan Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.04899)  

**Abstract**: Bearing in mind the limited parametric knowledge of Large Language Models (LLMs), retrieval-augmented generation (RAG) which supplies them with the relevant external knowledge has served as an approach to mitigate the issue of hallucinations to a certain extent. However, uniformly retrieving supporting context makes response generation source-inefficient, as triggering the retriever is not always necessary, or even inaccurate, when a model gets distracted by noisy retrieved content and produces an unhelpful answer. Motivated by these issues, we introduce Semantic Uncertainty Guided Adaptive Retrieval (SUGAR), where we leverage context-based entropy to actively decide whether to retrieve and to further determine between single-step and multi-step retrieval. Our empirical results show that selective retrieval guided by semantic uncertainty estimation improves the performance across diverse question answering tasks, as well as achieves a more efficient inference. 

**Abstract (ZH)**: 考虑到大型语言模型（LLMs）参数知识的有限性，检索增强生成（RAG）通过为它们提供相关的外部知识，已在一定程度上缓解了幻觉问题。然而，均匀地检索支持性背景会使得响应生成在来源效率方面受到影响，因为当模型因嘈杂的检索内容而分心并生成无用答案时，并不是总是需要触发检索器，甚至可能是不准确的。为了解决这些问题，我们引入了基于语义不确定性自适应检索（SUGAR），该方法通过利用基于上下文的熵来主动决定是否进行检索，并进一步确定是采用单步还是多步检索。我们的实验证据表明，通过语义不确定性估计进行的选择性检索在多种问答任务中提高了性能，并且实现了更高效的推理。 

---
# Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals 

**Title (ZH)**: 使用机器学习和射频信号量化瘙痒及其对睡眠的影响 

**Authors**: Michail Ouroutzoglou, Mingmin Zhao, Joshua Hellerstein, Hariharan Rahul, Asima Badic, Brian S. Kim, Dina Katabi  

**Link**: [PDF](https://arxiv.org/pdf/2501.04896)  

**Abstract**: Chronic itch affects 13% of the US population, is highly debilitating, and underlies many medical conditions. A major challenge in clinical care and new therapeutics development is the lack of an objective measure for quantifying itch, leading to reliance on subjective measures like patients' self-assessment of itch severity. In this paper, we show that a home radio device paired with artificial intelligence (AI) can concurrently capture scratching and evaluate its impact on sleep quality by analyzing radio signals bouncing in the environment. The device eliminates the need for wearable sensors or skin contact, enabling monitoring of chronic itch over extended periods at home without burdening patients or interfering with their skin condition. To validate the technology, we conducted an observational clinical study of chronic pruritus patients, monitored at home for one month using both the radio device and an infrared camera. Comparing the output of the device to ground truth data from the camera demonstrates its feasibility and accuracy (ROC AUC = 0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a significant correlation between scratching and low sleep quality, manifested as a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep latency (R = 0.68, p < 0.001). Our study underscores the potential of passive, long-term, at-home monitoring of chronic scratching and its sleep implications, offering a valuable tool for both clinical care of chronic itch patients and pharmaceutical clinical trials. 

**Abstract (ZH)**: 慢性瘙痒影响美国约13%的人口，严重影响患者生活质量，并且是多种医学状况的基础。在临床护理和新疗法开发中，一个主要的挑战是对瘙痒缺乏客观度量标准，导致依赖患者的主观评价来评估瘙痒的严重程度。本文展示了将家庭无线设备与人工智能（AI）相结合，可以通过分析环境中反射的无线电波信号同时捕捉抓挠行为，并评估其对睡眠质量的影响。该设备消除了穿戴式传感器或皮肤接触的需要，从而可以在家中长时间监测慢性瘙痒而不给患者带来负担或干扰其皮肤状况。为了验证该技术，我们在一个月时间内在家中对慢性瘙痒患者进行了观察性临床研究，使用无线电设备和红外摄像头同时进行监测。将设备输出的数据与摄像头的真实数据进行比较，展示了其可行性和准确性（ROC AUC = 0.997，灵敏度 = 0.825，特异性 = 0.997）。结果表明，抓挠与低睡眠质量之间存在显著相关性，表现为睡眠效率降低（R = 0.6，p < 0.001）和睡眠潜伏期增加（R = 0.68，p < 0.001）。本研究突显了被动式、长期家庭监测慢性抓痒及其对睡眠影响的潜力，为慢性瘙痒患者的临床护理以及制药临床试验提供了有价值的工具。 

---
# Reach Measurement, Optimization and Frequency Capping In Targeted Online Advertising Under k-Anonymity 

**Title (ZH)**: 在k-匿名性下的目标在线广告中的到达量测量、优化与频次封顶 

**Authors**: Yuan Gao, Mu Qiao  

**Link**: [PDF](https://arxiv.org/pdf/2501.04882)  

**Abstract**: The growth in the use of online advertising to foster brand awareness over recent years is largely attributable to the ubiquity of social media. One pivotal technology contributing to the success of online brand advertising is frequency capping, a mechanism that enables marketers to control the number of times an ad is shown to a specific user. However, the very foundation of this technology is being scrutinized as the industry gravitates towards advertising solutions that prioritize user privacy. This paper delves into the issue of reach measurement and optimization within the context of $k$-anonymity, a privacy-preserving model gaining traction across major online advertising platforms. We outline how to report reach within this new privacy landscape and demonstrate how probabilistic discounting, a probabilistic adaptation of traditional frequency capping, can be employed to optimize campaign performance. Experiments are performed to assess the trade-off between user privacy and the efficacy of online brand advertising. Notably, we discern a significant dip in performance as long as privacy is introduced, yet this comes with a limited additional cost for advertising platforms to offer their users more privacy. 

**Abstract (ZH)**: 近年来，随着社交媒体的普及，网络广告在提升品牌认知方面的作用日益显著。一种对在线品牌广告成功至关重要的关键技术是频次控制（frequency capping），它使营销人员能够控制特定用户看到广告的次数。然而，随着行业转向优先考虑用户隐私的广告解决方案，这一技术的基础正受到质疑。本文探讨了在 $k$-匿名性（一种受到主要在线广告平台青睐的隐私保护模型）这一隐私保护环境中，触达度测量与优化的问题。我们详细阐述了如何在这一新的隐私环境中报告触达度，并展示了概率折扣（一种概率化的传统频次控制的适应形式）如何被用于优化广告活动的效果。我们进行了实验以评估用户隐私与在线品牌广告效果之间的权衡。值得注意的是，我们发现只要引入隐私，广告效果就会显著下降，但这对广告平台为用户提供更多隐私所付出的额外成本却相对有限。 

---
# Real-Time Textless Dialogue Generation 

**Title (ZH)**: 实时无文本对话生成 

**Authors**: Long Mai, Julie Carson-Berndsen  

**Link**: [PDF](https://arxiv.org/pdf/2501.04877)  

**Abstract**: Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: this https URL 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的发展显著推动了基于文本的对话系统的发展。这些系统现在可以生成高质量、准确且连贯的响应，涵盖广泛的主题和任务。然而，在自然性方面，口语对话系统仍然落后。它们往往产生机械化交互，存在响应时间过长、回答过于通用或谨慎、缺乏自然节奏和流畅的话语交接等问题。这些不足主要归因于对传统级联设计的过度依赖，这种设计包含多个分离且顺序执行的组件，以及对文本作为中间表示的使用。本文提出了一种实时、无文本的口语对话生成模型（RTTL-DG），旨在克服这些挑战。我们的系统通过直接处理流式语音对话，实现自然的话语交接，并在响应生成时尽量减少延迟。此外，我们的模型整合了回声、过滤器、笑声和其他副语言信号，这些信号在级联对话系统中通常缺失，从而创建更加自然和类似人类的交互。实施细节和生成样例可以在我们的代码库中找到：this https URL 

---
# Back Home: A Machine Learning Approach to Seashell Classification and Ecosystem Restoration 

**Title (ZH)**: 回家之路：一种机器学习 Approach 在海螺分类与生态系统恢复中的应用 

**Authors**: Alexander Valverde, Luis Solano  

**Link**: [PDF](https://arxiv.org/pdf/2501.04873)  

**Abstract**: In Costa Rica, an average of 5 tons of seashells are extracted from ecosystems annually. Confiscated seashells, cannot be returned to their ecosystems due to the lack of origin recognition. To address this issue, we developed a convolutional neural network (CNN) specifically for seashell identification. We built a dataset from scratch, consisting of approximately 19000 images from the Pacific and Caribbean coasts. Using this dataset, the model achieved a classification accuracy exceeding 85%. The model has been integrated into a user-friendly application, which has classified over 36,000 seashells to date, delivering real-time results within 3 seconds per image. To further enhance the system's accuracy, an anomaly detection mechanism was incorporated to filter out irrelevant or anomalous inputs, ensuring only valid seashell images are processed. 

**Abstract (ZH)**: 在哥斯达黎加，每年从生态系统中平均提取5吨海贝壳。收缴的海贝壳由于缺乏原产地识别，无法被放归其原生态系统。为解决这一问题，我们开发了一种专门用于海贝壳识别的卷积神经网络（CNN）。我们从头构建了一个数据集，其中包括来自太平洋和加勒比海岸约19000张图片。利用这个数据集，模型的分类准确率超过了85%。该模型已集成到一个用户友好的应用程序中，至今已分类超过36,000个海贝壳，并在每张图片上实现了3秒内的实时结果。为进一步提高系统的准确性，我们还引入了一种异常检测机制，以过滤掉无关或异常输入，确保仅处理有效的海贝壳图片。 

---
# Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware 

**Title (ZH)**: 探索大型语言模型在Android恶意软件语义分析与分类中的应用 

**Authors**: Brandon J Walton, Mst Eshita Khatun, James M Ghawaly, Aisha Ali-Gombe  

**Link**: [PDF](https://arxiv.org/pdf/2501.04848)  

**Abstract**: Malware analysis is a complex process of examining and evaluating malicious software's functionality, origin, and potential impact. This arduous process typically involves dissecting the software to understand its components, infection vector, propagation mechanism, and payload. Over the years, deep reverse engineering of malware has become increasingly tedious, mainly due to modern malicious codebases' fast evolution and sophistication. Essentially, analysts are tasked with identifying the elusive needle in the haystack within the complexities of zero-day malware, all while under tight time constraints. Thus, in this paper, we explore leveraging Large Language Models (LLMs) for semantic malware analysis to expedite the analysis of known and novel samples. Built on GPT-4o-mini model, \msp is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering. Additionally, \msp performs malware categorization, distinguishing potential malware from benign applications, thereby saving time during the malware reverse engineering process. Despite not being fine-tuned for Android malware analysis, we demonstrate that through optimized and advanced prompt engineering \msp can achieve up to 77% classification accuracy while providing highly robust summaries at functional, class, and package levels. In addition, leveraging the backward tracing of the summaries from package to function levels allowed us to pinpoint the precise code snippets responsible for malicious behavior. 

**Abstract (ZH)**: 恶意软件分析是一个复杂的过程，涉及对恶意软件的功能、来源和潜在影响进行检查和评估。这个艰巨的过程通常包括拆解软件，以理解其组件、感染途径、传播机制和载荷。近年来，恶意软件的深度逆向工程变得越来越繁琐，这主要是由于现代恶意代码的快速进化和复杂性。分析师的任务是识别在零日恶意软件的复杂性中的隐秘威胁，同时还要在时间紧迫的情况下完成这项工作。因此，本文探索利用大型语言模型（LLMs）进行语义恶意软件分析，以加速已知和新型样本的分析过程。基于GPT-4o-mini模型，\msp 设计用于通过分层级次的总结链和策略性提示工程增强Android恶意软件分析。此外，\msp 还能进行恶意软件分类，将潜在的恶意软件与良性应用程序区分开来，从而在逆向工程恶意软件的过程中节省时间。尽管没有针对Android恶意软件分析进行微调，我们通过优化和先进的提示工程展示了\msp 可以达到高达77%的分类准确率，并且还能提供功能、类和包三个层次上的高度稳健的总结。此外，利用总结自包级到功能级的回溯跟踪，我们能够精确定位负责恶意行为的代码片段。 

---
# Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction 

**Title (ZH)**: 通过并行音素序列预测提升基于EEG的聆听语音解码 

**Authors**: Jihwan Lee, Tiantian Feng, Aditya Kommineni, Sudarsana Reddy Kadiri, Shrikanth Narayanan  

**Link**: [PDF](https://arxiv.org/pdf/2501.04844)  

**Abstract**: Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available. 

**Abstract (ZH)**: 脑-机接口（BCI）提供了众多以人类为中心的应用可能性，尤其对患有神经性疾病的人士产生了影响。通过脑活动解码文本或语音是相关领域之一，能够增强语言感知受损人士的生活质量。我们提出了一种新颖的方法，通过利用辅助音素预测器来同时解码文本音素序列，从而增强基于脑电图（EEG）信号的听觉语音解码。所提出模型架构包含三个主要部分：EEG模块、语音生成模块和音素预测器。EEG模块负责将EEG信号学习转换为EEG嵌入。语音生成模块从EEG嵌入生成语音波形。音素预测器输出文本格式的解码音素序列。我们提出的方法使得用户能够同时通过两种模态（语音波形和文本音素序列）从EEG信号中获得解码的听觉语音，从而消除了为每个模态单独构建串联的顺序流水线的需要。此外，该提出的方法在两种模态下的性能也优于之前的方案。源代码和语音样本已公开提供。 

---
# Do Code LLMs Understand Design Patterns? 

**Title (ZH)**: 代码LLM是否理解设计模式？ 

**Authors**: Zhenyu Pan, Xuefeng Song, Yunkun Wang, Rongyu Cao, Binhua Li, Yongbin Li, Han Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.04835)  

**Abstract**: Code Large Language Models (LLMs) demonstrate great versatility in adapting to various downstream tasks, including code generation and completion, as well as bug detection and fixing. However, Code LLMs often fail to capture existing coding standards, leading to the generation of code that conflicts with the required design patterns for a given project. As a result, developers must post-process to adapt the generated code to the project's design norms. In this work, we empirically investigate the biases of Code LLMs in software development. Through carefully designed experiments, we assess the models' understanding of design patterns across recognition, comprehension, and generation. Our findings reveal that biases in Code LLMs significantly affect the reliability of downstream tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在适应各种下游任务方面表现出极大的灵活性，包括代码生成和补全，以及错误检测和修复。然而，代码LLMs往往无法捕捉现有的编码标准，导致生成的代码与给定项目的特定设计模式冲突。因此，开发人员必须对生成的代码进行后处理，以适应项目的规范。在本研究中，我们通过实证研究考察了代码LLMs在软件开发中的偏见。通过精心设计的实验，我们评估了模型在识别、理解和生成阶段对设计模式的理解能力。我们的研究发现，代码LLMs中的偏见显著影响下游任务的可靠性。 

---
# Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil 

**Title (ZH)**: 改进亚基础土壤强度估算的智能梯度提升算法 

**Authors**: Ismail B. Mustapha, Muyideen Abdulkareem, Shafaatunnur Hasan, Abideen Ganiyu, Hatem Nabus, Jin Chai Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.04826)  

**Abstract**: The performance of pavement under loading depends on the strength of the subgrade. However, experimental estimation of properties of pavement strengths such as California bearing ratio (CBR), unconfined compressive strength (UCS) and resistance value (R) are often tedious, time-consuming and costly, thereby inspiring a growing interest in machine learning based tools which are simple, cheap and fast alternatives. Thus, the potential application of two boosting techniques; categorical boosting (CatBoost) and extreme gradient boosting (XGBoost) and support vector regression (SVR), is similarly explored in this study for estimation of properties of subgrade soil modified with hydrated lime activated rice husk ash (HARSH). Using 121 experimental data samples of varying proportions of HARSH, plastic limit, liquid limit, plasticity index, clay activity, optimum moisture content, and maximum dry density as input for CBR, UCS and R estimation, four evaluation metrics namely coefficient of determination (R2), root mean squared error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) are used to evaluate the models' performance. The results indicate that XGBoost outperformed CatBoost and SVR in estimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in estimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in estimating the CBR and R with R2 of 0.9997 respectively. On the other hand, CatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature sensitivity analysis shows that the three machine learning techniques are unanimous that increasing HARSH proportion lead to values of the estimated properties respectively. A comparison with previous results also shows superiority of XGBoost in estimating subgrade properties. 

**Abstract (ZH)**: 加载条件下路面的性能取决于路基的强度。然而，实验估计路面强度（如California承载比（CBR）、无侧限抗压强度（UCS）和阻力值（R））的性质往往是耗时、费力且成本高昂的，因此越来越多的研究者对基于机器学习的工具产生了兴趣，这些工具可以提供简单、便宜且快速的替代方案。因此，在本研究中，探讨了两种提升技术——类别提升（CatBoost）和极端梯度提升（XGBoost）以及支持向量回归（SVR）在含水活化稻壳灰（HARSH）改性路基土性质估计中的潜在应用。

使用含有不同比例HARSH的121个实验数据样本，以及塑限、液限、塑性指数、黏土活性、最佳含水量和最大干燥密度作为输入变量，用于估计CBR、UCS和R的性质，本研究采用了四项评估标准：决定系数（R²）、均方根误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）来评估模型的性能。研究结果表明，XGBoost在估计这些性质方面优于CatBoost和SVR，分别为CBR、UCS和R的R²分别为0.9994、0.9995和0.9999。另外，SVR在估计CBR和R方面的R²分别为0.9997。另一方面，CatBoost在估计UCS方面优于SVR，R²为0.9994。特征灵敏度分析显示，三种机器学习技术一致认为，HARSH含量的增加会导致估计指标值的提升。与先前的研究结果相比，XGBoost在估计路基性质方面也表现出了优势。 

---
# Planing It by Ear: Convolutional Neural Networks for Acoustic Anomaly Detection in Industrial Wood Planers 

**Title (ZH)**: 依耳规划：用于工业木料刨削机声学异常检测的卷积神经网络 

**Authors**: Anthony Deschênes, Rémi Georges, Cem Subakan, Bruna Ugulino, Antoine Henry, Michael Morin  

**Link**: [PDF](https://arxiv.org/pdf/2501.04819)  

**Abstract**: In recent years, the wood product industry has been facing a skilled labor shortage. The result is more frequent sudden failures, resulting in additional costs for these companies already operating in a very competitive market. Moreover, sawmills are challenging environments for machinery and sensors. Given that experienced machine operators may be able to diagnose defects or malfunctions, one possible way of assisting novice operators is through acoustic monitoring. As a step towards the automation of wood-processing equipment and decision support systems for machine operators, in this paper, we explore using a deep convolutional autoencoder for acoustic anomaly detection of wood planers on a new real-life dataset. Specifically, our convolutional autoencoder with skip connections (Skip-CAE) and our Skip-CAE transformer outperform the DCASE autoencoder baseline, one-class SVM, isolation forest and a published convolutional autoencoder architecture, respectively obtaining an area under the ROC curve of 0.846 and 0.875 on a dataset of real-factory planer sounds. Moreover, we show that adding skip connections and attention mechanism under the form of a transformer encoder-decoder helps to further improve the anomaly detection capabilities. 

**Abstract (ZH)**: 近年来，木材加工业面临技术人员短缺的问题。这导致了更频繁的突发故障，这些公司本已在非常激烈的市场竞争中运营，因此增加了额外的成本。此外，锯木厂的环境对机械设备和传感器构成挑战。鉴于有经验的机械操作员可能能够诊断缺陷或故障，通过声学监测为新手操作员提供辅助是一种可能的方法。为了朝着木材加工设备的自动化和为机械操作员提供决策支持系统的目标前进，在本文中，我们通过一个深度卷积自编码器探索在新现实数据集上进行木材刨床声学异常检测的可能性。具体而言，我们的卷积自编码器（Skip-CAE）和卷积自编码器变压器在数据集上的 roc 曲线下面积分别为 0.846 和 0.875，分别优于 dcase 自编码器基线、one-class SVM、孤立森林以及一套已发布的卷积自编码器结构。此外，我们展示了在模型中添加跳跃连接和以变压器编码器-解码器形式的注意力机制有助于进一步提高异常检测能力。 

---
# Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning 

**Title (ZH)**: 针对TinyML中的分散资源共享：无线双层萌 gossip 并行SGD协作学习 

**Authors**: Ziyuan Bao, Eiman Kanjo, Soumya Banerjee, Hasib-Al Rashid, Tinoosh Mohsenin  

**Link**: [PDF](https://arxiv.org/pdf/2501.04817)  

**Abstract**: With the growing computational capabilities of microcontroller units (MCUs), edge devices can now support machine learning models. However, deploying decentralised federated learning (DFL) on such devices presents key challenges, including intermittent connectivity, limited communication range, and dynamic network topologies. This paper proposes a novel framework, bilayer Gossip Decentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to address these issues in resource-constrained environments. The framework incorporates a hierarchical communication structure using Distributed Kmeans (DKmeans) clustering for geographic grouping and a gossip protocol for efficient model aggregation across two layers: intra-cluster and inter-cluster. We evaluate the framework's performance against the Centralised Federated Learning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under IID and Non-IID conditions. Results demonstrate that the proposed method achieves comparable accuracy to CFL on IID datasets, requiring only 1.8 additional rounds for convergence. On Non-IID datasets, the accuracy loss remains under 8\% for moderate data imbalance. These findings highlight the framework's potential to support scalable and privacy-preserving learning on edge devices with minimal performance trade-offs. 

**Abstract (ZH)**: 随着微控制器单元（MCU）计算能力的不断提高，边缘设备现在可以支持机器学习模型。然而，在这些设备上部署去中心化的联邦学习（Distributed Federated Learning，DFL）带来了诸多关键挑战，包括间歇性连接、有限的通信范围和动态网络拓扑结构。本文提出了一种新的框架——双层Gossip去中心化并行随机梯度下降（Bilayer Gossip Decentralised Parallel Stochastic Gradient Descent，BGP-PSGD），旨在在资源受限的环境中解决这些问题。该框架采用分层通信结构，通过分布式K均值（DKmeans）聚类进行地理分组，并通过Gossip协议在两个层面上高效地进行模型聚合：簇内和簇间。我们使用MCUNet模型在CIFAR-10数据集上，与集中式联邦学习（Centralised Federated Learning，CFL）基线进行对比，分别在同分布（IID）和不同分布（Non-IID）条件下评估了该框架的性能。结果表明，在同分布数据集上，所提出的方法在准确性方面与CFL相当，仅需额外1.8个收敛轮次。在不同分布数据集上，在中等数据不均衡的情况下，准确性损失保持在8%以下。这些发现突显了该框架在边缘设备上支持可扩展且隐私保护的学习方面的潜力，同时具有最小的性能权衡。 

---
# TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training 

**Title (ZH)**: TREAD：面向高效架构无关扩散训练的令牌路由方法 

**Authors**: Felix Krause, Timy Phan, Vincent Tao Hu, Björn Ommer  

**Link**: [PDF](https://arxiv.org/pdf/2501.04765)  

**Abstract**: Diffusion models have emerged as the mainstream approach for visual generation. However, these models usually suffer from sample inefficiency and high training costs. This issue is particularly pronounced in the standard diffusion transformer architecture due to its quadratic complexity relative to input length. Recent works have addressed this by reducing the number of tokens processed in the model, often through masking. In contrast, this work aims to improve the training efficiency of the diffusion backbone by using predefined routes that store this information until it is reintroduced to deeper layers of the model, rather than discarding these tokens entirely. Further, we combine multiple routes and introduce an adapted auxiliary loss that accounts for all applied routes. Our method is not limited to the common transformer-based model - it can also be applied to state-space models. Unlike most current approaches, TREAD achieves this without architectural modifications. Finally, we show that our method reduces the computational cost and simultaneously boosts model performance on the standard benchmark ImageNet-1K 256 x 256 in class-conditional synthesis. Both of these benefits multiply to a convergence speedup of 9.55x at 400K training iterations compared to DiT and 25.39x compared to the best benchmark performance of DiT at 7M training iterations. 

**Abstract (ZH)**: 扩散模型已成为视觉生成的主要方法。然而，这些模型通常存在样本效率低和高训练成本的问题。在标准的扩散转换器架构中，这一问题尤为突出，因为该架构的复杂性与其输入长度呈二次增长。最近的研究通过减少模型处理的标记数量来缓解这一问题，通常通过掩码实现。相比之下，本研究旨在通过使用预定义的路径（这些路径保留这些信息直到将其重新引入模型的更深层面，而不是完全丢弃这些标记）来提高扩散主干的训练效率。此外，我们结合了多种路径，并引入了一种适应性的辅助损失，该损失涵盖了所有应用的路径。我们的方法不仅适用于常见的基于转换器的模型，还可以应用于状态空间模型。与大多数当前方法不同，TREAD 在不进行架构修改的情况下实现了这一点。最后，我们证明，与标准基准 ImageNet-1K 256x256 的类别条件合成相比，我们的方法降低了计算成本并同时提升了模型性能。这些好处在 40 万训练迭代下使训练速度提升了 9.55 倍，相较于 DiT；在 700 万训练迭代下，与 DiT 最佳基准性能相比，速度提高了 25.39 倍。 

---
# Discovering new robust local search algorithms with neuro-evolution 

**Title (ZH)**: 使用神经进化发现新的稳健局部搜索算法 

**Authors**: Mohamed Salim Amri Sakhri, Adrien Goëffon, Olivier Goudet, Frédéric Saubion, Chaïmaâ Touhami  

**Link**: [PDF](https://arxiv.org/pdf/2501.04747)  

**Abstract**: This paper explores a novel approach aimed at overcoming existing challenges in the realm of local search algorithms. Our aim is to improve the decision process that takes place within a local search algorithm so as to make the best possible transitions in the neighborhood at each iteration. To improve this process, we propose to use a neural network that has the same input information as conventional local search algorithms. In this paper, which is an extension of the work [Goudet et al. 2024] presented at EvoCOP2024, we investigate different ways of representing this information so as to make the algorithm as efficient as possible but also robust to monotonic transformations of the problem objective function. To assess the efficiency of this approach, we develop an experimental setup centered around NK landscape problems, offering the flexibility to adjust problem size and ruggedness. This approach offers a promising avenue for the emergence of new local search algorithms and the improvement of their problem-solving capabilities for black-box problems. 

**Abstract (ZH)**: 本文探讨了一种新型方法，旨在克服局部搜索算法领域中的现有挑战。我们的目标是改进局部搜索算法中的决策过程，以便在每次迭代中能够在邻域中做出最佳过渡。为了改进这一过程，我们提出了一种神经网络，其输入信息与传统的局部搜索算法相同。在此论文中，作为我们在EvoCOP2024会议上提交的工作[Goudet等人，2024]的扩展，我们研究了不同的信息表示方法，以使算法尽可能高效且对问题目标函数的单调变换具有鲁棒性。为了评估该方法的效率，我们围绕NK景观问题设计了一个实验框架，提供了调整问题规模和复杂性的灵活性。这种方法为新局部搜索算法的出现及其对黑盒问题的求解能力的改进提供了一个有前景的途径。 

---
# Generative Style Transfer for MRI Image Segmentation: A Case of Glioma Segmentation in Sub-Saharan Africa 

**Title (ZH)**: 生成式风格迁移在MRI图像分割中的应用：以撒哈拉以南非洲胶质瘤分割为例 

**Authors**: Rancy Chepchirchir, Jill Sunday, Raymond Confidence, Dong Zhang, Talha Chaudhry, Udunna C. Anazodo, Kendi Muchungi, Yujing Zou  

**Link**: [PDF](https://arxiv.org/pdf/2501.04734)  

**Abstract**: In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic Resonance Imaging (MRI) technology raises questions about the applicability of machine learning methods for clinical tasks. This study aims to provide a robust deep learning-based brain tumor segmentation (BraTS) method tailored for the SSA population using a threefold approach. Firstly, the impact of domain shift from the SSA training data on model efficacy was examined, revealing no significant effect. Secondly, a comparative analysis of 3D and 2D full-resolution models using the nnU-Net framework indicates similar performance of both the models trained for 300 epochs achieving a five-fold cross-validation score of 0.93. Lastly, addressing the performance gap observed in SSA validation as opposed to the relatively larger BraTS glioma (GLI) validation set, two strategies are proposed: fine-tuning SSA cases using the GLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel neural style transfer-based data augmentation technique for the SSA cases. This investigation underscores the potential of enhancing brain tumor prediction within SSA's unique healthcare landscape. 

**Abstract (ZH)**: 在撒哈拉以南非洲（SSA），使用较低质量的磁共振成像（MRI）技术引发了关于机器学习方法在临床任务中适用性的疑问。本研究旨在通过三重方法提供一个专门针对SSA人群的基于深度学习的大脑肿瘤分割（BraTS）方法。首先，研究了SSA训练数据领域迁移对模型效果的影响，发现没有显著影响。其次，使用nnU-Net框架对3D和2D全分辨率模型进行了比较分析，结果显示两种模型在训练300个epochs后表现相当，交叉验证得分为0.93。最后，为了解决在SSA验证集上观察到的性能差距，相对于相对较大的BraTS胶质瘤（GLI）验证集，提出了两种策略：使用GLI+SSA最佳预训练的2D全分辨率模型在300个epochs进行微调SSA案例，并引入一种新型的基于神经风格迁移的数据增强技术用于SSA病例。这项研究强调了在SSA独特医疗保健环境中增强大脑肿瘤预测的潜力。 

---
# SNR-EQ-JSCC: Joint Source-Channel Coding with SNR-Based Embedding and Query 

**Title (ZH)**: SNR-EQ-JSCC: 基于信噪比嵌入与查询的联合源信道编码 

**Authors**: Hongwei Zhang, Meixia Tao  

**Link**: [PDF](https://arxiv.org/pdf/2501.04732)  

**Abstract**: Coping with the impact of dynamic channels is a critical issue in joint source-channel coding (JSCC)-based semantic communication systems. In this paper, we propose a lightweight channel-adaptive semantic coding architecture called SNR-EQ-JSCC. It is built upon the generic Transformer model and achieves channel adaptation (CA) by Embedding the signal-to-noise ratio (SNR) into the attention blocks and dynamically adjusting attention scores through channel-adaptive Queries. Meanwhile, penalty terms are introduced in the loss function to stabilize the training process. Considering that instantaneous SNR feedback may be imperfect, we propose an alternative method that uses only the average SNR, which requires no retraining of SNR-EQ-JSCC. Simulation results conducted on image transmission demonstrate that the proposed SNR-EQJSCC outperforms the state-of-the-art SwinJSCC in peak signal-to-noise ratio (PSNR) and perception metrics while only requiring 0.05% of the storage overhead and 6.38% of the computational complexity for CA. Moreover, the channel-adaptive query method demonstrates significant improvements in perception metrics. When instantaneous SNR feedback is imperfect, SNR-EQ-JSCC using only the average SNR still surpasses baseline schemes. 

**Abstract (ZH)**: 在基于语义通信的联合源-信道编码（JSCC）系统中，应对动态信道的影响是关键问题之一。本文提出了一种轻量级的自适应信道语义编码架构，命名为SNR-EQ-JSCC。该架构基于通用的Transformer模型，并通过将信噪比（SNR）嵌入到注意力模块中，并通过自适应查询动态调整注意力分数来实现信道适应（CA）。同时，在损失函数中引入惩罚项以稳定训练过程。考虑到瞬时SNR反馈可能不完善，我们提出了一种替代方法，仅使用平均SNR，而无需重新训练SNR-EQ-JSCC。在图像传输的仿真结果中，所提出的SNR-EQ-JSCC在峰值信噪比（PSNR）和感知度量方面均优于现有最先进的SwinJSCC，同时只有其0.05%的存储开销和6.38%的计算复杂度用于CA。此外，自适应查询方法在感知度量方面显示出显著改进。当瞬时SNR反馈不完善时，仅使用平均SNR的SNR-EQ-JSCC仍优于基线方案。 

---
# Calculating Customer Lifetime Value and Churn using Beta Geometric Negative Binomial and Gamma-Gamma Distribution in a NFT based setting 

**Title (ZH)**: 基于NFT环境下的贝塔几何负二项分布与伽马-伽马分布计算客户终身价值和流失率 

**Authors**: Sagarnil Das  

**Link**: [PDF](https://arxiv.org/pdf/2501.04719)  

**Abstract**: Customer Lifetime Value (CLV) is an important metric that measures the total value a customer will bring to a business over their lifetime. The Beta Geometric Negative Binomial Distribution (BGNBD) and Gamma Gamma Distribution are two models that can be used to calculate CLV, taking into account both the frequency and value of customer transactions. This article explains the BGNBD and Gamma Gamma Distribution models, and how they can be used to calculate CLV for NFT (Non-Fungible Token) transaction data in a blockchain setting. By estimating the parameters of these models using historical transaction data, businesses can gain insights into the lifetime value of their customers and make data-driven decisions about marketing and customer retention strategies. 

**Abstract (ZH)**: 顾客终身价值（CLV）是衡量客户在其整个生命周期内为业务带来的总价值的重要指标。Beta Geometric Negative Binomial Distribution（BGNBD）模型和Gamma Gamma分布是两种可用于同时考虑客户交易频率和价值的模型，以计算CLV。本文将解释BGNBD模型和Gamma Gamma分布模型，并探讨它们在区块链环境中的NFT（非同质化代币）交易数据中计算CLV的应用。通过使用历史交易数据估计这些模型的参数，企业可以获得关于顾客终身价值的洞察，并基于数据做出关于营销和客户保留战略的决策。 

---
# Knowledge-Guided Biomarker Identification for Label-Free Single-Cell RNA-Seq Data: A Reinforcement Learning Perspective 

**Title (ZH)**: 基于知识指导的无标签单细胞RNA测序数据生物标志物识别：一种强化学习视角 

**Authors**: Meng Xiao, Weiliang Zhang, Xiaohan Huang, Hengshu Zhu, Min Wu, Xiaoli Li, Yuanchun Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2501.04718)  

**Abstract**: Gene panel selection aims to identify the most informative genomic biomarkers in label-free genomic datasets. Traditional approaches, which rely on domain expertise, embedded machine learning models, or heuristic-based iterative optimization, often introduce biases and inefficiencies, potentially obscuring critical biological signals. To address these challenges, we present an iterative gene panel selection strategy that harnesses ensemble knowledge from existing gene selection algorithms to establish preliminary boundaries or prior knowledge, which guide the initial search space. Subsequently, we incorporate reinforcement learning through a reward function shaped by expert behavior, enabling dynamic refinement and targeted selection of gene panels. This integration mitigates biases stemming from initial boundaries while capitalizing on RL's stochastic adaptability. Comprehensive comparative experiments, case studies, and downstream analyses demonstrate the effectiveness of our method, highlighting its improved precision and efficiency for label-free biomarker discovery. Our results underscore the potential of this approach to advance single-cell genomics data analysis. 

**Abstract (ZH)**: 基因面板选择旨在从无标签的基因组数据集中识别最具有信息性的基因组生物标志物。传统方法依赖领域专业知识、嵌入式机器学习模型或基于启发式的迭代优化，往往引入偏差和低效性，可能掩盖关键的生物信号。为应对这些挑战，我们提出了一种迭代的基因面板选择策略，该策略利用现有基因选择算法的集成知识来建立初步边界或先验知识，从而引导初始搜索空间。随后，我们通过由专家行为塑造的奖励函数引入强化学习，实现动态细化和针对性的基因面板选择。这种集成减少了由初始边界引起的偏差，同时利用了强化学习的随机适应性。通过全面的比较实验、案例研究和下游分析，我们证明了该方法的有效性，突显了其在无标签生物标志物发现中的较高精度和效率。我们的结果表明，此方法有潜力推动单细胞基因组数据的分析。 

---
# One Node One Model: Featuring the Missing-Half for Graph Clustering 

**Title (ZH)**: 一种节点一模型方法：针对图聚类中的缺失半边特征 

**Authors**: Xuanting Xie, Bingheng Li, Erlin Pan, Zhaochen Guo, Zhao Kang, Wenyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.09902)  

**Abstract**: Most existing graph clustering methods primarily focus on exploiting topological structure, often neglecting the ``missing-half" node feature information, especially how these features can enhance clustering performance. This issue is further compounded by the challenges associated with high-dimensional features. Feature selection in graph clustering is particularly difficult because it requires simultaneously discovering clusters and identifying the relevant features for these clusters. To address this gap, we introduce a novel paradigm called ``one node one model", which builds an exclusive model for each node and defines the node label as a combination of predictions for node groups. Specifically, the proposed ``Feature Personalized Graph Clustering (FPGC)" method identifies cluster-relevant features for each node using a squeeze-and-excitation block, integrating these features into each model to form the final representations. Additionally, the concept of feature cross is developed as a data augmentation technique to learn low-order feature interactions. Extensive experimental results demonstrate that FPGC outperforms state-of-the-art clustering methods. Moreover, the plug-and-play nature of our method provides a versatile solution to enhance GNN-based models from a feature perspective. 

**Abstract (ZH)**: 现有的大多数图聚类方法主要集中在利用拓扑结构，往往会忽略“缺失一半”的节点特征信息，特别是在这些特征如何能提升聚类性能方面。这种问题进一步受到高维特征挑战的加剧。在图聚类中进行特征选择尤为困难，因为它需要同时发现簇并识别这些簇的相关特征。为解决这一问题，我们引入了一种新的范式，称为“每个节点一个模型”，为每个节点建立专属模型，并将节点标签定义为节点组预测的组合。具体来说，提出的“特征个性化图聚类（FPGC）”方法使用压缩-激励模块识别每个节点的簇相关特征，将这些特征整合到每个模型中，以形成最终表示。此外，我们还开发了特征交叉的概念作为一种数据增强技术来学习低阶特征交互。广泛的实验结果表明，FPGC在聚类方法中表现出色。此外，我们方法的即插即用性质提供了一种从特征视角增强基于GNN的模型的通用解决方案。 

---
