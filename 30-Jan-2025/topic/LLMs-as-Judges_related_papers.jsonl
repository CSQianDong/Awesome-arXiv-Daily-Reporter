{'arxiv_id': 'arXiv:2501.17195', 'title': 'Atla Selene Mini: A General Purpose Evaluation Model', 'authors': 'Andrei Alexandru, Antonia Calvi, Henry Broomfield, Jackson Golden, Kyle Dai, Mathias Leys, Maurice Burger, Max Bartolo, Roman Engeler, Sashank Pisupati, Toby Drane, Young Sun Park', 'link': 'https://arxiv.org/abs/2501.17195', 'abstract': 'We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (this https URL) and Ollama to encourage widespread community adoption.', 'abstract_zh': '我们介绍了Atla Selene Mini，这是一个最先进的小型语言模型作为法官（SLMJ）。Selene Mini 是一种通用的评估器，在11个跨分布基准测试中，在绝对评分、分类和成对偏好任务上总体表现优于现有的最佳SLMJ和GPT-4o-mini。它在RewardBench中以8B生成模型的身份获得了最高分，超过了包括GPT-4o和专门的评估器在内的强基线模型。为了实现这一目标，我们开发了一种原则性的数据策展策略，通过添加合成生成的批评意见来扩充公共数据集，并通过过滤和数据集裁剪确保高质量。我们在直接偏好优化（DPO）和监督微调（SFT）损失的基础上训练了我们的模型，生成了一个在现实世界场景中表现出色、响应能力极强的评估器。Selene Mini 在金融和医疗行业数据集上的零-shot一致性与人类专家评估相比显著提高，同时对提示格式的变化具有鲁棒性。初步结果显示，Selene Mini 在一个由社区驱动的实时法官赛场上排名最高。我们已在HuggingFace（https://）和Ollama上发布了模型权重，以促进社区广泛采用。', 'title_zh': '阿特拉塞勒内迷你：一个通用评估模型'}
{'arxiv_id': 'arXiv:2501.17178', 'title': 'Tuning LLM Judges Hyperparameters', 'authors': 'David Salinas, Omar Swelam, Frank Hutter', 'link': 'https://arxiv.org/abs/2501.17178', 'abstract': 'Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention. While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging. In this paper, we propose to systematically analyze and tune hyperparameter of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trades accuracy for cost and also reduce significantly the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility.', 'abstract_zh': '评估大型语言模型（LLMs）通常需要成本高昂的人工注释。为了解决这一问题，提出了基于LLM的裁判系统，这种系统可以通过比较两个LLM的输出来对模型进行排名，而无需人工干预。尽管已经提出了一些方法，但不同论文之间存在诸多混淆因素。例如，模型、提示和超参数通常同时变化，这使得直接对比变得困难。本文中，我们提出了一种系统的方法来分析和调整LLM裁判的超参数。为了减轻评估裁判的成本，我们提出了一种多目标多保真度方法，该方法可以在准确性和成本之间进行权衡，同时也大大降低了搜索成本。我们的方法不仅在准确性和成本效益上超越了现有基准，还利用了开放权重模型，从而确保了更大的可访问性和可重复性。', 'title_zh': '调整大语言模型法官的超参数'}
{'arxiv_id': 'arXiv:2501.17581', 'title': 'CSEval: Towards Automated, Multi-Dimensional, and Reference-Free Counterspeech Evaluation using Auto-Calibrated LLMs', 'authors': 'Amey Hengle, Aswini Kumar, Anil Bandhakavi, Tanmoy Chakraborty', 'link': 'https://arxiv.org/abs/2501.17581', 'abstract': 'Counterspeech has been popular as an effective approach to counter online hate speech, leading to increasing research interest in automated counterspeech generation using language models. However, this field lacks standardised evaluation protocols and robust automated evaluation metrics that align with human judgement. Current automatic evaluation methods, primarily based on similarity metrics, do not effectively capture the complex and independent attributes of counterspeech quality, such as contextual relevance, aggressiveness, or argumentative coherence. This has led to an increased dependency on labor-intensive human evaluations to assess automated counter-speech generation methods. To address these challenges, we introduce CSEval, a novel dataset and framework for evaluating counterspeech quality across four dimensions: contextual-relevance, aggressiveness, argument-coherence, and suitableness. Furthermore, we propose Auto-Calibrated COT for Counterspeech Evaluation (ACE), a prompt-based method with auto-calibrated chain-of-thoughts (CoT) for scoring counterspeech using large language models. Our experiments show that ACE outperforms traditional metrics like ROUGE, METEOR, and BertScore in correlating with human judgement, indicating a significant advancement in automated counterspeech evaluation.', 'abstract_zh': '对抗言论作为一种有效的方法，已经被广泛用于反击网络仇恨言论，这促进了对使用语言模型进行自动化对抗言论生成的研究兴趣增加。然而，这一领域缺乏标准化的评估协议和与人类判断相一致的稳健自动评估指标。当前的自动评估方法主要基于相似性指标，无法有效捕捉对抗言论质量的复杂且独立的属性，如上下文相关性、攻击性或说理连贯性。这导致了对劳动密集型的人工评估的依赖，以评估自动化对抗言论生成方法。为了解决这些挑战，我们提出了一种新的对抗言论评估数据集和框架CSEval，它从四个维度评估对抗言论质量：上下文相关性、攻击性、说理连贯性以及适用性。此外，我们还提出了基于提示的对抗言论评估自动校准链式思维（Auto-Calibrated Chain-of-Thoughts，简称ACE）方法，这是一种使用大型语言模型评分对抗言论的方法，结合了自动校准的链式思维（CoT）。实验结果表明，ACE在与人类判断相关性方面优于传统的ROUGE、METEOR和BertScore等指标，表明在自动化对抗言论评估方面取得了重要进展。', 'title_zh': 'CSEval：面向自动化、多维度且无需参考标准的反言论评估系统，利用自动标定的大语言模型'}
