{'arxiv_id': 'arXiv:2501.17496', 'title': 'SemML: Enhancing Automata-Theoretic LTL Synthesis with Machine Learning', 'authors': 'Jan Kretinsky, Tobias Meggendorfer, Maximilian Prokop, Ashkan Zarkhah', 'link': 'https://arxiv.org/abs/2501.17496', 'abstract': "Synthesizing a reactive system from specifications given in linear temporal logic (LTL) is a classical problem, finding its applications in safety-critical systems design. We present our tool SemML, which won this year's LTL realizability tracks of SYNTCOMP, after years of domination by Strix. While both tools are based on the automata-theoretic approach, ours relies heavily on (i) Semantic labelling, additional information of logical nature, coming from recent LTL-to-automata translations and decorating the resulting parity game, and (ii) Machine Learning approaches turning this information into a guidance oracle for on-the-fly exploration of the parity game (whence the name SemML). Our tool fills the missing gaps of previous suggestions to use such an oracle and provides an efficeint implementation with additional algorithmic improvements. We evaluate SemML both on the entire set of SYNTCOMP as well as a synthetic data set, compare it to Strix, and analyze the advantages and limitations. As SemML solves more instances on SYNTCOMP and does so significantly faster on larger instances, this demonstrates for the first time that machine-learning-aided approaches can out-perform state-of-the-art tools in real LTL synthesis.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\n从线性时序逻辑（LTL）规范合成反应系统是一个经典问题，其在关键安全系统设计领域得到了广泛应用。我们介绍了我们的工具SemML，该工具在今年的SYNTCOMP的LTL实现赛道上获胜，打破了Strix长期以来的垄断。尽管这两种工具都基于自动机理论方法，但我们的工具主要依赖于以下两点：(i) 语义标签，这种来自最近的LTL到自动机转换的额外逻辑信息，并通过这种信息对最终的优先级游戏进行装饰，以及(ii) 使用机器学习方法将这些信息转化为优先级游戏中即时探索的指导预言机（因此名为SemML）。我们的工具填补了以往建议使用此类预言机时存在的空白，并提供了高效的实现，同时包含了一些算法改进。我们在SYNTCOMP的整个数据集以及一个合成数据集上评估了SemML，并将其与Strix进行比较，分析了其优势与局限性。由于SemML在SYNTCOMP上解决更多实例，并且在解决更大规模问题时显著更快，这首次证明机器学习辅助方法在实际LTL合成中可以优于最先进的工具。', 'title_zh': 'SemML：增强基于自动机的LTL综合的机器学习方法'}
{'arxiv_id': 'arXiv:2501.17315', 'title': 'A sketch of an AI control safety case', 'authors': 'Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving', 'link': 'https://arxiv.org/abs/2501.17315', 'abstract': 'As LLM agents gain a greater capacity to cause harm, AI developers might increasingly rely on control measures such as monitoring to justify that they are safe. We sketch how developers could construct a "control safety case", which is a structured argument that models are incapable of subverting control measures in order to cause unacceptable outcomes. As a case study, we sketch an argument that a hypothetical LLM agent deployed internally at an AI company won\'t exfiltrate sensitive information. The sketch relies on evidence from a "control evaluation,"\' where a red team deliberately designs models to exfiltrate data in a proxy for the deployment environment. The safety case then hinges on several claims: (1) the red team adequately elicits model capabilities to exfiltrate data, (2) control measures remain at least as effective in deployment, and (3) developers conservatively extrapolate model performance to predict the probability of data exfiltration in deployment. This safety case sketch is a step toward more concrete arguments that can be used to show that a dangerously capable LLM agent is safe to deploy.', 'abstract_zh': '随着大规模语言模型（LLM）代理的能力增强，导致潜在危害的风险增加，AI开发者可能会越来越多地依赖控制措施（如监控），以证明模型是安全的。我们勾勒了开发者如何构建一个“控制安全性论证”，这是一种结构化的论据，表明模型无法通过规避控制措施导致不可接受的结果。作为案例研究，我们勾勒了一个论点，即一家AI公司在内部部署的假设LLM代理不会泄露敏感信息。这一论点的基础在于“控制评估”的证据，其中红队故意设计模型以泄露数据，作为部署环境的代理。然后，安全性论证依赖于以下几个主张：（1）红队充分揭示了模型泄露数据的能力；（2）在部署中，控制措施至少与评估环境一样有效；（3）开发者保守地外推模型在评估中的表现，以预测在部署中数据泄露的概率。这种安全性论证草图是朝着更具可操作性的论据发展的重要一步，这些论据可以用来显示一个功能强大的LLM代理是安全的，可以部署。', 'title_zh': '人工智能控制安全性论证概要'}
{'arxiv_id': 'arXiv:2501.17206', 'title': 'Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care', 'authors': 'Fengpei Yuan, Nehal Hasnaeen, Ran Zhang, Bryce Bible, Joseph Riley Taylor, Hairong Qi, Fenghui Yao, Xiaopeng Zhao', 'link': 'https://arxiv.org/abs/2501.17206', 'abstract': "This study explores a novel approach to advancing dementia care by integrating socially assistive robotics, reinforcement learning (RL), large language models (LLMs), and clinical domain expertise within a simulated environment. This integration addresses the critical challenge of limited experimental data in socially assistive robotics for dementia care, providing a dynamic simulation environment that realistically models interactions between persons living with dementia (PLWDs) and robotic caregivers. The proposed framework introduces a probabilistic model to represent the cognitive and emotional states of PLWDs, combined with an LLM-based behavior simulation to emulate their responses. We further develop and train an adaptive RL system enabling humanoid robots, such as Pepper, to deliver context-aware and personalized interactions and assistance based on PLWDs' cognitive and emotional states. The framework also generalizes to computer-based agents, highlighting its versatility. Results demonstrate that the RL system, enhanced by LLMs, effectively interprets and responds to the complex needs of PLWDs, providing tailored caregiving strategies. This research contributes to human-computer and human-robot interaction by offering a customizable AI-driven caregiving platform, advancing understanding of dementia-related challenges, and fostering collaborative innovation in assistive technologies. The proposed approach has the potential to enhance the independence and quality of life for PLWDs while alleviating caregiver burden, underscoring the transformative role of interaction-focused AI systems in dementia care.", 'abstract_zh': '本研究探索了一种将社会辅助机器人、强化学习（RL）、大型语言模型（LLMs）以及临床领域专业知识整合应用于痴呆症护理的新方法，该研究在模拟环境中进行。这种整合解决了社会辅助机器人在痴呆症护理中面临的实验数据有限的关键挑战，提供了一个动态模拟环境，能够真实地模拟患有痴呆症的人（PLWD）与机器人护理人员之间的互动。提出的框架引入了一个概率模型来表示PLWD的认知和情感状态，并结合基于LLM的行为模拟来模拟他们的反应。进一步开发并训练了一个自适应RL系统，使Pepper等类人机器人能够根据PLWD的认知和情感状态提供情境感知和个性化的互动和辅助。该框架还能够应用于基于计算机的代理，突显其实用性。研究结果表明，通过LLM增强的RL系统能够有效地解释并响应PLWD的复杂需求，提供量身定制的护理策略。本研究通过提供一个可定制的人工智能驱动护理平台，促进了人机交互和人机互动中对痴呆相关挑战的理解，并促进了辅助技术的协同创新。提出的这一方法有望增强PLWD的独立性和生活质量，同时减轻护理人员的负担，突出以交互为中心的人工智能系统在痴呆症护理中的变革作用。', 'title_zh': '将以下论文内容或标题翻译成中文，并确保符合学术规范：\n\nOriginal Title: Integrating Reinforcement Learning and AI Agents for Adaptive Robotic Interaction and Assistance in Dementia Care\n\nTranslated Title: 结合强化学习和人工智能代理实现痴呆护理中的自适应机器人互动与辅助\n\nThis translation maintains the academic tone and accurately conveys the meaning of the original title.'}
{'arxiv_id': 'arXiv:2501.17855', 'title': 'GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings', 'authors': 'Ziang Liu, Yuanchen Ju, Yu Da, Tom Silver, Pranav N. Thakkar, Jenna Li, Justin Guo, Katherine Dimitropoulou, Tapomayukh Bhattacharjee', 'link': 'https://arxiv.org/abs/2501.17855', 'abstract': "Robot caregiving should be personalized to meet the diverse needs of care recipients -- assisting with tasks as needed, while taking user agency in action into account. In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals. In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks. We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy. We develop a neural model that learns to embed functional assessment scores into a latent representation of the user's physical function. The model is trained using motion capture data collected from users with emulated mobility limitations. After training, the model predicts personalized fROM for new users without motion capture. Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user's agency in action. See our website for more visualizations: this https URL.", 'abstract_zh': '机器人护理应个性化以满足护理对象多样化的需求——在执行任务时考虑用户的行动自主权。在如传递物品、洗澡、穿衣和康复等物理任务中，这种多样性的一个关键方面是功能活动范围（fROM），不同个体之间的差异性很大。在这项研究中，我们通过预测个性化的fROM来推广机器人的决策过程，以应对广泛类型的护理任务。我们提出了一种基于数据的新方法，利用职业治疗中获得的功能评估分数来预测个性化的fROM。我们开发了一种神经网络模型，该模型能够将功能评估分数嵌入到用户的物理功能的潜在表示中。该模型使用模拟移动限制的用户的动捕数据进行训练。经过训练后，该模型可以预测新用户的个性化fROM，而无需使用动捕数据。通过模拟实验和实际机器人用户研究，我们展示了模型生成的个性化fROM预测使机器人能够提供个性化且有效的帮助，同时提高用户的行动自主权。更多信息和可视化请参见我们的网站：this https URL。', 'title_zh': 'GRACE：基于用户功能嵌入的机器人辅助护理通用化'}
{'arxiv_id': 'arXiv:2501.17842', 'title': 'From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning', 'authors': 'Junseok Park, Hyeonseo Yang, Min Whoo Lee, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang', 'link': 'https://arxiv.org/abs/2501.17842', 'abstract': "Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning. Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural progression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving optimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models. In addition, we reinterpret Tolman's maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards.", 'abstract_zh': 'reinforcement 学习（RL）代理经常面临着在探索与利用之间取得平衡的挑战，特别是在奖励稀疏或密集时容易导致学习偏向的情况。生物系统，如人类婴儿，自然地通过从稀疏奖励的自由探索过渡到由越来越密集奖励引导的目标导向行为来解决这一平衡问题。受这一自然进程的启发，我们研究了在目标导向的 RL 任务中借鉴婴儿探索的奖励过渡方法。我们的研究主要集中在从稀疏奖励过渡到基于潜能的密集奖励（S2D）的过程中，同时保持最优策略不变。通过在动态机械臂操作和以自我为中心的3D导航任务中的实验，我们证明了有效的S2D奖励过渡显著提高了学习性能和样本效率。此外，利用交叉密度可视化工具，我们展示了S2D过渡平滑了策略损失景观，产生了更宽的极小值，从而提高了RL模型的一般性。此外，我们重新解读了托尔曼迷宫实验，突显了在S2D奖励的背景下早期自由探索学习的关键作用。', 'title_zh': '从稀疏到密集：基于学步儿的奖励过渡在目标导向强化学习中的应用'}
{'arxiv_id': 'arXiv:2501.17420', 'title': 'Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models', 'authors': 'Yuxuan Li, Hirokazu Shirado, Sauvik Das', 'link': 'https://arxiv.org/abs/2501.17420', 'abstract': 'While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.', 'abstract_zh': '尽管在公平性与一致性的进步有助于减轻大型语言模型（LLMs）在明确提示下表现出的明显偏见，我们假设这些模型在模拟人类行为时仍可能表现出隐性偏见。为了检验这一假设，我们提出了一种技术，该技术可以系统地揭示这些偏见，涉及广泛的社会人口统计类别，并通过评估具有社会人口统计信息的LLM生成的个性的代理在决策中的差异来进行评估。使用该技术，我们在三个社会人口统计群体和四个决策情景中测试了六种LLM。结果表明，最先进的LLM在几乎所有的模拟中都表现出显著的社会人口统计差异，尽管高级模型减少了明确的偏见，但隐性偏见却更加明显。此外，当我们将我们的发现与实证研究中报告的实际差异进行比较时，发现我们揭示的偏见方向一致，但明显放大。这种方向一致表明，我们的技术有助于揭示LLM中的系统性偏差，而不是随机变化；同时，隐性偏见的存在及其放大强调了需要采用新的策略来解决这些偏见的必要性。', 'title_zh': '言行胜于言语：智能体决策揭示语言模型中的隐含偏见'}
{'arxiv_id': 'arXiv:2501.17384', 'title': 'A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning', 'authors': 'Zhengpeng Xie, Jiahang Cao, Yulong Zhang, Qiang Zhang, Renjing Xu', 'link': 'https://arxiv.org/abs/2501.17384', 'abstract': "Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.", 'abstract_zh': '近年来，得益于神经网络的强大能力，强化学习（RL）已成功解决了许多具有挑战性的任务。然而，随着这些模型展示出增强的决策能力，它们也越来越容易发生过拟合。例如，训练好的RL模型往往难以泛化到同一任务的细微变化，如背景颜色的改变或其他细微的语义差异。为应对这一问题，我们提出了一种双智能体对抗策略学习框架，该框架允许智能体自发地学习底层语义，而无需引入任何先验的人类知识。具体来说，我们的框架涉及两个智能体之间的游戏过程：每个智能体都力求通过为同一状态生成不同的表示来最大化对对手策略的影响，同时保持自己对这种扰动的稳定性。这种互动促使智能体学习出具备泛化能力的策略，能够处理高维观测中的无关特征。在Procgen基准测试上的广泛实验结果表明，通过对抗过程显著提高了两个智能体的泛化性能，同时该框架也能应用于各种强化学习算法，例如近端策略优化（PPO）。利用对抗框架，RL智能体在基线方法上表现出显著的优越性，尤其是在高级任务中，这标志着深度强化学习泛化能力的重要进展。', 'title_zh': '一种双代理对抗框架，用于深度强化学习中的稳健泛化'}
{'arxiv_id': 'arXiv:2501.17329', 'title': 'Anomaly Detection in Cooperative Vehicle Perception Systems under Imperfect Communication', 'authors': 'Ashish Bastola, Hao Wang, Abolfazl Razi', 'link': 'https://arxiv.org/abs/2501.17329', 'abstract': 'Anomaly detection is a critical requirement for ensuring safety in autonomous driving. In this work, we leverage Cooperative Perception to share information across nearby vehicles, enabling more accurate identification and consensus of anomalous behaviors in complex traffic scenarios. To account for the real-world challenge of imperfect communication, we propose a cooperative-perception-based anomaly detection framework (CPAD), which is a robust architecture that remains effective under communication interruptions, thereby facilitating reliable performance even in low-bandwidth settings. Since no multi-agent anomaly detection dataset exists for vehicle trajectories, we introduce 15,000 different scenarios with a 90,000 trajectories benchmark dataset generated through rule-based vehicle dynamics analysis. Empirical results demonstrate that our approach outperforms standard anomaly classification methods in F1-score, AUC and showcase strong robustness to agent connection interruptions.', 'abstract_zh': '异常检测是确保自动驾驶安全性的一个关键需求。本研究利用协同感知（Cooperative Perception，CP）技术，在相邻车辆之间共享信息，这使得在复杂交通场景中更准确地识别和达成一致的异常行为成为可能。为了应对通信不完善的实际挑战，我们提出了一个基于协同感知的异常检测框架（CPAD），该框架具有鲁棒性，能在通信中断的情况下保持有效性，从而在带宽较低的环境下也能实现可靠的性能。由于目前没有适用于车辆轨迹的多智能体异常检测数据集，我们通过基于规则的车辆动力学分析生成了一个包含15,000种不同场景和90,000条轨迹的基准数据集。通过实验证明，我们的方法在F1分数、AUC值上优于标准的异常分类方法，并且在智能体连接中断时展现出很强的鲁棒性。', 'title_zh': '在 Imperfect 通信条件下的协同车辆感知系统中的异常检测'}
{'arxiv_id': 'arXiv:2501.17176', 'title': 'Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant', 'authors': 'Marc Ballestero-Ribó, Daniel Ortiz-Martínez', 'link': 'https://arxiv.org/abs/2501.17176', 'abstract': "The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.", 'abstract_zh': '由于大型语言模型（LLMs）的出现，实现师生比1:1的梦想比以往任何时候都更加接近。这些模型在教育领域的潜在应用之一是在大学入门级编程课程中为学生提供反馈。这样，遇到基础实现问题的学生就可以随时寻求24/7可用的语言模型的帮助。本文专注于探讨这种应用的三个方面。首先，评估了两种广为人知的模型——GPT-3.5T和GPT-4T——在为学生提供反馈方面的表现。实证结果显示，GPT-4T的表现远优于GPT-3.5T，但目前还不适合在实际场景中使用。这是因为生成错误信息的可能性导致潜在用户可能无法始终检测到这些错误。其次，本文提出了一种精心设计的提示，利用上下文学习技术，不仅允许自动化评价过程的重要部分，还能提供反馈中包含错误信息比例的下限，从而节省时间和精力。这是因为生成的反馈具有可编程分析的结构，能够反映语言模型在完成指定任务时的表现诊断信息。第三，本文还提出了一种基于语言模型实现实用学习工具的可能策略，该策略基于提出的提示技术。这一策略从教学角度来看，开辟了一系列有趣的可能性。', 'title_zh': '基于提示的低成本评估与运营ChatGPT作为计算机程序设计辅导助手'}
{'arxiv_id': 'arXiv:2501.17167', 'title': 'QualityFlow: An Agentic Workflow for Program Synthesis Controlled by LLM Quality Checks', 'authors': 'Yaojie Hu, Qiang Zhou, Qihong Chen, Xiaopeng Li, Linbo Liu, Dejiao Zhang, Amit Kachroo, Talha Oz, Omer Tripp', 'link': 'https://arxiv.org/abs/2501.17167', 'abstract': 'We introduce QualityFlow, a dynamic agentic workflow for program synthesis. Given the English description of a programming problem and a set of unit tests, the model\'s goal is to synthesize the correct program that solves the problem and passes the tests. QualityFlow consists of multiple large language model (LLM) agents that resemble a software development team, including code generation, testing, and self-debugging. Existing program synthesis methods face three major limitations: assumption of visible unit test conformity, bottleneck of synthesized test quality, and deviation of self-debugging trajectory. To address them, we propose the LLM Quality Checker, which explicitly "imagines" whether the synthesized programs\' execution would conform to the unit tests. The Quality Checks dynamically control the workflow, including actions to submit the final answer, clarify the problem statement, and revert previous workflow steps. As a result, our Quality Checker can precisely accept any correct program, mitigate faulty synthesized tests, and prevent potential workflow deviation. The success of the Quality Checker further enables Diversified Prompting, which encourages variations in LLM responses to maximize the possibility that a correct program appears and passes the quality check. In experiments, QualityFlow establishes the state-of-the-art results on four program synthesis benchmarks: MBPP, HumanEval, and the stricter evaluations of both MBPP and HumanEval from EvalPlus. Our systematic analysis shows that the dynamic workflow controlled by LLM quality checks can outperform static workflows and single-attempt zero-shot synthesis. The Quality Checker is the center of our investigation, and we dissect its individual performance and integrated impact on the workflow accuracy, as well as other ablations experiments to justify our workflow design.', 'abstract_zh': '我们引入了QualityFlow，这是一种动态代理工作流，用于程序合成。给定编程问题的英文描述和一组单元测试，模型的目标是合成正确的程序以解决问题并通过这些测试。QualityFlow 包含多个大的语言模型（LLM）代理，这些代理类似于软件开发团队，包括代码生成、测试和自我调试。现有的程序合成方法面临三大限制：单元测试一致性可见性的假设、合成测试质量的瓶颈以及自我调试轨迹的偏差。为了解决这些问题，我们提出了一种LLM质量检查器，明确“设想”合成的程序执行是否符合单元测试。质量检查动态控制工作流程，包括提交最终答案、澄清问题描述以及回滚先前的工作流程步骤。因此，我们的质量检查器可以精确接受任何正确的程序，减轻错误的合成测试，并防止潜在的工作流程偏移。质量检查的成功进一步使多样化提示成为可能，鼓励LLM响应的变化以最大化正确程序出现并通过质量检查的可能性。在实验中，QualityFlow 在四个程序合成基准测试（MBPP、HumanEval 及从EvalPlus的更严格的评估）中达到了最先进的结果。我们的系统分析表明，由LLM质量检查控制的动态工作流程可以优于静态工作流程和单一尝试的零次合成。质量检查器是我们研究的核心，我们剖析了其单独的性能和综合影响对工作流程准确性的影响，并进行了其他消融实验以证明我们的工作流程设计的有效性。', 'title_zh': 'QualityFlow：由LLM质量检查控制的代理工作流程程序合成'}
{'arxiv_id': 'arXiv:2501.17403', 'title': 'General Scene Adaptation for Vision-and-Language Navigation', 'authors': 'Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu', 'link': 'https://arxiv.org/abs/2501.17403', 'abstract': 'Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.', 'abstract_zh': '视觉语言导航（VLN）任务主要评估代理在多个环境中一次性执行个别指令的能力，旨在开发能够在任何环境中以零样本方式进行工作的代理。然而，现实世界中的导航机器人通常在物理布局相对一致且视觉观察和指导语言风格也相对一致的持久环境中运行。这种任务设置之间的差距为通过持续适应特定环境来改进VLN代理提供了机会。为了更好地反映这些现实条件，我们引入了GSA-VLN，这是一种新颖的任务，要求代理在特定场景中执行导航指令并同时适应该场景，以在时间上提高性能。为了评估该任务，必须解决现有VLN数据集中两个关键挑战：OOD（out-of-distribution）数据的缺失，以及每个场景中指令数量和风格的有限多样性。因此，我们提出了一种新的数据集GSA-R2R，该数据集显著扩展了R2R数据集中的环境和指令的多样性和数量，以评估代理在ID（in-distribution）和OOD（out-of-distribution）环境中的适应性。此外，我们设计了一个三阶段指令协调流水线，利用大模型（LLM）来优化生成的指令，并运用角色扮演技术将其重新表述为不同的话语风格。这一设计受到观察的启发，即每个用户往往在指令中表现出一致的特征或偏好。我们在GSA-R2R上进行了广泛实验，以全面评估我们的数据集并基准多种方法。根据我们的研究发现，我们提出了一种新颖的方法——GR-DUET，该方法结合了基于记忆的导航图与环境特定的训练策略，实现了在所有GSA-R2R分割上的前沿结果。', 'title_zh': '视觉语言导航中的通用场景适应'}
