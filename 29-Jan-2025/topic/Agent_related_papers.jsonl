{'arxiv_id': 'arXiv:2501.17015', 'title': 'Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework', 'authors': 'Longzhong Lin, Xuewu Lin, Kechun Xu, Haojian Lu, Lichao Huang, Rong Xiong, Yue Wang', 'link': 'https://arxiv.org/abs/2501.17015', 'abstract': 'Simulation plays a crucial role in assessing autonomous driving systems, where the generation of realistic multi-agent behaviors is a key aspect. In multi-agent simulation, the primary challenges include behavioral multimodality and closed-loop distributional shifts. In this study, we revisit mixture models for generating multimodal agent behaviors, which can cover the mainstream methods including continuous mixture models and GPT-like discrete models. Furthermore, we introduce a closed-loop sample generation approach tailored for mixture models to mitigate distributional shifts. Within the unified mixture model~(UniMM) framework, we recognize critical configurations from both model and data perspectives. We conduct a systematic examination of various model configurations, including positive component matching, continuous regression, prediction horizon, and the number of components. Moreover, our investigation into the data configuration highlights the pivotal role of closed-loop samples in achieving realistic simulations. To extend the benefits of closed-loop samples across a broader range of mixture models, we further address the shortcut learning and off-policy learning issues. Leveraging insights from our exploration, the distinct variants proposed within the UniMM framework, including discrete, anchor-free, and anchor-based models, all achieve state-of-the-art performance on the WOSAC benchmark.', 'abstract_zh': '仿真在评估自动驾驶系统中扮演着至关重要的角色，其中生成现实主义多代理行为是关键方面。在多代理仿真中，主要挑战包括行为的多模态性和闭环分布变化。在此研究中，我们重新审视了用于生成多模态代理行为的混合模型，这些模型涵盖了主流方法，包括连续混合模型和类似于GPT的离散模型。此外，我们介绍了专门为混合模型设计的闭环样本生成方法，以缓解分布变化问题。在统一混合模型（UniMM）框架内，我们从建模和数据两个视角识别出了关键配置。我们系统地研究了各种模型配置，包括正分量匹配、连续回归、预测时间窗口以及分量的数量。此外，我们的数据配置研究突显了闭环样本在实现现实仿真中的关键作用。为了扩大闭环样本对更广泛混合模型的好处，我们还解决了捷径学习和离策学习问题。基于我们探索的见解，UniMM框架下的不同变种，包括离散型、无锚点型和基于锚点型模型，均在WOSAC基准测试中实现了最先进的性能。', 'title_zh': '在统一框架下重访混合模型在多智能体模拟中的应用：实验研究'}
{'arxiv_id': 'arXiv:2501.16922', 'title': 'Agential AI for Integrated Continual Learning, Deliberative Behavior, and Comprehensible Models', 'authors': 'Zeki Doruk Erden, Boi Faltings', 'link': 'https://arxiv.org/abs/2501.16922', 'abstract': "Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn't. However, it faces key limitations, such as a lack of integration with planning, incomprehensible internal structure, and inability to learn continually. We present the initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, designed to overcome these issues. AAI's core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning, using component-level variation and selection to learn the structure of the environment. It integrates this with a behavior algorithm that plans on a learned model and encapsulates high-level behavior patterns. Preliminary experiments on a simple environment show AAI's effectiveness and potential.", 'abstract_zh': '当代机器学习范式在统计数据分析方面表现出色，能够解决经典人工智能无法解决的问题。然而，它面临着关键性限制，如与规划的整合不足、内部结构难以理解以及无法持续学习。我们提出了一个旨在克服这些问题的AI系统初始设计，名为代理性AI（Agential AI, AAI），原则上可以独立运行或建立在统计方法之上。AAI的核心是一种学习方法，能够以完备性、简洁性和持续学习为保证地建模时间动态，并利用组件级的变异和选择来学习环境结构。它将这种建模与一个行为算法相结合，该算法基于学习到的模型进行规划，并封装高层次的行为模式。初步实验在简单环境中展示了AAI的有效性和潜力。', 'title_zh': '整合持续学习、审慎行为和可解释模型的代理人工智能'}
{'arxiv_id': 'arXiv:2501.16689', 'title': 'MACI: Multi-Agent Collaborative Intelligence for Robust Reasoning and Temporal Planning', 'authors': 'Edward Y. Chang', 'link': 'https://arxiv.org/abs/2501.16689', 'abstract': "Artificial intelligence requires deliberate reasoning, temporal awareness, and effective constraint management, capabilities beyond the pattern-matching strengths of LLMs. LLMs struggle with planning tasks because of their reliance on associative reasoning, inability to self-verify, and inconsistent constraint awareness. We propose Multi-Agent Collaborative Intelligence (MACI), a framework centered on a meta-planner (MP) that orchestrates multiple agents to generate planner templates that define roles and constraints. These planners produce actionable workflows of role nodes and dependency constraints, enabling advanced temporal reasoning and adaptability.\nMACI's three-tier architecture includes a meta-planning module for planner construction, common agents for general reasoning, and specialized agents for domain expertise. By decoupling planning from validation, it overcomes key LLM limitations. Evaluations demonstrate MACI's effective constraint satisfaction, conflict detection, and reasoning, positioning it as a robust solution for complex reasoning and planning tasks.", 'abstract_zh': '人工智能需要精心的推理、时间意识和有效的约束管理，这些能力超出了大语言模型（LLMs）在模式匹配方面的优势。LLMs 在规划任务中面临挑战，主要是因为它们依赖联想推理、缺乏自我验证能力和约束意识不一致。我们提出多智能体协作智能（MACI），这是一种以元规划器（MP）为核心框架，协调多个智能体生成规划模板的方法。这些规划模板定义角色和约束，从而生成可执行的工作流节点及其依赖约束，提供高级时间推理和适应性。\n\nMACI 的三层架构包括：元规划模块，用于构建规划；通用智能体，用于一般推理；以及专业智能体，用于特定领域的专业知识。通过将规划与验证分离，MACI 克服了大语言模型的关键局限性。评估结果显示，MACI 在有效约束满足、冲突检测和推理方面表现出色，使其成为解决复杂推理和规划任务的稳健解决方案。', 'title_zh': 'MACI：多智能体协作智能用于稳健推理与时间规划'}
{'arxiv_id': 'arXiv:2501.16609', 'title': 'CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation', 'authors': 'Faria Huq, Zora Zhiruo Wang, Frank F. Xu, Tianyue Ou, Shuyan Zhou, Jeffrey P. Bigham, Graham Neubig', 'link': 'https://arxiv.org/abs/2501.16609', 'abstract': "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at this https URL", 'abstract_zh': '尽管关于Web代理的研究强调了其自主执行用户任务的潜力，但在现实世界的情境中，代理往往在复杂任务上表现不佳，且难以准确建模用户偏好。这为人类与代理合作并有效利用代理的能力提供了机会。我们提出了CowPilot框架，该框架支持自主导航以及人类与代理的协作式Web导航，并通过任务成功和任务效率进行评估。CowPilot通过允许代理提出下一步操作，减少了人类需要执行的操作步骤数量，同时用户可以暂停、拒绝或采取替代行动。在执行过程中，用户可以通过覆盖建议或在需要时恢复代理控制来交替执行其操作。我们在五个常见网站上进行了案例研究，发现协作模式下的成功率达到95%，而人类仅需执行总步骤的15.2%。即使在任务执行过程中有人类的干预，代理本身也能成功完成任务的一半以上。CowPilot可以作为一种数据收集和代理评估的有用工具，我们相信它将有助于研究用户和代理如何协作的工作方式。视频演示可以在此处访问：[提供链接]', 'title_zh': 'CowPilot：一种自主与人-代理协作的网页导航框架'}
{'arxiv_id': 'arXiv:2501.17077', 'title': 'Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning', 'authors': 'Anna Soligo, Pietro Ferraro, David Boyle', 'link': 'https://arxiv.org/abs/2501.17077', 'abstract': 'Interpretability in reinforcement learning is crucial for ensuring AI systems align with human values and fulfill the diverse related requirements including safety, robustness and fairness. Building on recent approaches to encouraging sparsity and locality in neural networks, we demonstrate how the penalisation of non-local weights leads to the emergence of functionally independent modules in the policy network of a reinforcement learning agent. To illustrate this, we demonstrate the emergence of two parallel modules for assessment of movement along the X and Y axes in a stochastic Minigrid environment. Through the novel application of community detection algorithms, we show how these modules can be automatically identified and their functional roles verified through direct intervention on the network weights prior to inference. This establishes a scalable framework for reinforcement learning interpretability through functional modularity, addressing challenges regarding the trade-off between completeness and cognitive tractability of reinforcement learning explanations.', 'abstract_zh': '强化学习中的可解释性对于确保人工智能系统与人类价值观一致并满足包括安全、鲁棒性和公平性在内的多样化相关需求至关重要。基于最近促进神经网络稀疏性和局部性的方法，我们展示了如何通过惩罚非局部权重来促使强化学习代理的策略网络出现功能性独立模块。为说明这一点，我们展示了在随机小网格环境中小模块如何实现对X轴和Y轴运动的平行评估。通过新颖地应用社区检测算法，我们表明这些模块可以自动识别，并通过干预网络权重以进行推理前的功能验证来验证其功能作用。这建立了一种通过功能性模块化实现强化学习可解释性的可扩展框架，解决了关于强化学习解释的完整性和认知可处理性之间的权衡挑战。', 'title_zh': '诱导模ularity和社区检测在功能可解释强化学习中的应用'}
{'arxiv_id': 'arXiv:2501.16966', 'title': 'Heterogeneity-aware Personalized Federated Learning via Adaptive Dual-Agent Reinforcement Learning', 'authors': 'Xi Chen, Qin Li, Haibin Cai, Ting Wang', 'link': 'https://arxiv.org/abs/2501.16966', 'abstract': "Federated Learning (FL) empowers multiple clients to collaboratively train machine learning models without sharing local data, making it highly applicable in heterogeneous Internet of Things (IoT) environments. However, intrinsic heterogeneity in clients' model architectures and computing capabilities often results in model accuracy loss and the intractable straggler problem, which significantly impairs training effectiveness. To tackle these challenges, this paper proposes a novel Heterogeneity-aware Personalized Federated Learning method, named HAPFL, via multi-level Reinforcement Learning (RL) mechanisms. HAPFL optimizes the training process by incorporating three strategic components: 1) An RL-based heterogeneous model allocation mechanism. The parameter server employs a Proximal Policy Optimization (PPO)-based RL agent to adaptively allocate appropriately sized, differentiated models to clients based on their performance, effectively mitigating performance disparities. 2) An RL-based training intensity adjustment scheme. The parameter server leverages another PPO-based RL agent to dynamically fine-tune the training intensity for each client to further enhance training efficiency and reduce straggling latency. 3) A knowledge distillation-based mutual learning mechanism. Each client deploys both a heterogeneous local model and a homogeneous lightweight model named LiteModel, where these models undergo mutual learning through knowledge distillation. This uniform LiteModel plays a pivotal role in aggregating and sharing global knowledge, significantly enhancing the effectiveness of personalized local training. Experimental results across multiple benchmark datasets demonstrate that HAPFL not only achieves high accuracy but also substantially reduces the overall training time by 20.9%-40.4% and decreases straggling latency by 19.0%-48.0% compared to existing solutions.", 'abstract_zh': '联邦学习（FL）使多个客户端能够无需共享本地数据即可协作训练机器学习模型，使其在异构物联网（IoT）环境中具有高度适用性。然而，客户端模型架构和计算能力的固有异质性往往会导致模型准确率下降和不可解决的延迟节点问题，这显著影响了训练效果。为应对这些挑战，本文提出了一个新颖的异质感知个性化联邦学习方法（HAPFL），通过多层次强化学习（RL）机制实现。HAPFL通过引入三个战略组件优化了训练过程：1) 基于RL的异质模型分配机制。参数服务器采用基于Proximal Policy Optimization (PPO) 的RL代理，根据客户端的性能动态分配合适大小和差异化的模型，有效地缓解了性能差异。2) 基于RL的训练强度调整方案。参数服务器利用另一个基于PPO 的RL代理动态调整每个客户端的训练强度，进一步提高训练效率并减少延迟。3) 基于知识蒸馏的互助学习机制。每个客户端部署一个异质本地模型和一个同质轻量级模型LiteModel，这些模型通过知识蒸馏进行互助学习。这个统一的LiteModel在聚合和共享全球知识方面发挥着关键作用，显著提升了个性化本地训练的效果。在多个基准数据集上的实验结果表明，HAPFL不仅能够实现高准确率，还能将整体训练时间减少20.9%-40.4%，将延迟减少19.0%-48.0%，相比于现有解决方案有显著优势。', 'title_zh': '基于自适应双代理强化学习的异构适应性个性化联邦学习'}
{'arxiv_id': 'arXiv:2501.16899', 'title': 'RDMM: Fine-Tuned LLM Models for On-Device Robotic Decision Making with Enhanced Contextual Awareness in Specific Domains', 'authors': 'Shady Nasrat, Myungsu Kim, Seonil Lee, Jiho Lee, Yeoncheol Jang, Seung-joon Yi', 'link': 'https://arxiv.org/abs/2501.16899', 'abstract': 'Large language models (LLMs) represent a significant advancement in integrating physical robots with AI-driven systems. We showcase the capabilities of our framework within the context of the real-world household competition. This research introduces a framework that utilizes RDMM (Robotics Decision-Making Models), which possess the capacity for decision-making within domain-specific contexts, as well as an awareness of their personal knowledge and capabilities. The framework leverages information to enhance the autonomous decision-making of the system. In contrast to other approaches, our focus is on real-time, on-device solutions, successfully operating on hardware with as little as 8GB of memory. Our framework incorporates visual perception models equipping robots with understanding of their environment. Additionally, the framework has integrated real-time speech recognition capabilities, thus enhancing the human-robot interaction experience. Experimental results demonstrate that the RDMM framework can plan with an 93\\% accuracy. Furthermore, we introduce a new dataset consisting of 27k planning instances, as well as 1.3k text-image annotated samples derived from the competition. The framework, benchmarks, datasets, and models developed in this work are publicly available on our GitHub repository at this https URL.', 'abstract_zh': '大型语言模型（LLMs）代表了将物理机器人与AI驱动系统整合的重要进步。我们在实际家庭环境挑战的背景下展示了该框架的能力。本研究介绍了一种利用RDMM（机器人决策模型）的框架，这些模型能够在其特定领域内进行决策，同时具有对自己知识和能力的自我意识。该框架利用信息来增强系统的自主决策能力。与其它方法不同，我们的重点在于实时且在设备端的解决方案，成功地在内存仅为8GB的硬件上运行。该框架集成了视觉感知模型，使机器人能够理解其环境。此外，框架还集成了实时语音识别能力，从而增强了人与机器人之间的交互体验。实验结果表明，RDMM框架的规划准确率为93%。此外，我们还引入了一个新的数据集，包含27,000个规划实例，以及1,300个包含文本和图像标注的样本，这些数据源于比赛。本研究开发的框架、基准测试、数据集和模型已公开发布在我们的GitHub仓库中，网址为 [此链接]。\n\n请注意，将“此 https URL”替换为实际的GitHub仓库链接。', 'title_zh': 'RDMM：针对特定领域增强上下文意识的嵌入式机器人决策 Fine-tuned 大型语言模型'}
{'arxiv_id': 'arXiv:2501.16783', 'title': 'A Stochastic Dynamical Theory of LLM Self-Adversariality: Modeling Severity Drift as a Critical Process', 'authors': 'Jack David Carson', 'link': 'https://arxiv.org/abs/2501.16783', 'abstract': 'This paper introduces a continuous-time stochastic dynamical framework for understanding how large language models (LLMs) may self-amplify latent biases or toxicity through their own chain-of-thought reasoning. The model posits an instantaneous "severity" variable $x(t) \\in [0,1]$ evolving under a stochastic differential equation (SDE) with a drift term $\\mu(x)$ and diffusion $\\sigma(x)$. Crucially, such a process can be consistently analyzed via the Fokker--Planck approach if each incremental step behaves nearly Markovian in severity space. The analysis investigates critical phenomena, showing that certain parameter regimes create phase transitions from subcritical (self-correcting) to supercritical (runaway severity). The paper derives stationary distributions, first-passage times to harmful thresholds, and scaling laws near critical points. Finally, it highlights implications for agents and extended LLM reasoning models: in principle, these equations might serve as a basis for formal verification of whether a model remains stable or propagates bias over repeated inferences.', 'abstract_zh': '本文介绍了一个连续时间的随机动力学框架，用于理解大型语言模型（LLMs）如何通过自身的推理过程自我放大潜在的偏见或毒性。该模型提出一个瞬时“严重性”变量 $x(t) \\in [0,1]$，该变量遵循一个带有漂移项 $\\mu(x)$ 和扩散项 $\\sigma(x)$ 的随机微分方程（SDE）。关键的是，如果每次增量步骤在严重性空间中几乎表现出马尔可夫性，那么这种过程可以通过福克-普朗克方法进行一致分析。分析探讨了临界现象，表明某些参数区域会导致从次临界（自我纠正）到超临界（严重性失控）的相变。本文推导了稳态分布、达到有害阈值的首达时间以及临界点附近的标度律。最后，本文强调了对代理和扩展的LLM推理模型的含义：原则上，这些方程可能成为验证模型在多次推理过程中是否保持稳定或传播偏见的基础。', 'title_zh': '一种关于大规模语言模型自我对抗性的随机动力学理论：模型严重性漂移作为一个关键过程'}
{'arxiv_id': 'arXiv:2501.16751', 'title': 'DebugAgent: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging', 'authors': 'Muxi Chen, Chenchen Zhao, Qiang Xu', 'link': 'https://arxiv.org/abs/2501.16751', 'abstract': 'Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduce DebugAgent, an automated framework for error slice discovery and model repair. DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains, including image classification, pose estimation, and object detection - show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.', 'abstract_zh': '尽管深度学习模型在计算机视觉领域取得了显著成功，但它们在特定数据子集上往往会表现出系统性的失败，这些失败通常被称为错误切片。识别并缓解这些错误切片对于提高模型在实际场景中的鲁棒性和可靠性至关重要。本文介绍了一种名为DebugAgent的自动化框架，该框架用于错误切片的发现和模型修复。DebugAgent首先生成特定任务的视觉属性，通过可解释且结构化的流程突出显示容易出错的实例。然后，它使用高效的切片枚举算法系统地识别错误切片，从而克服了切片探索过程中出现的组合难题。此外，DebugAgent进一步增强了其能力，能够预测超出验证集范围的错误切片，解决了先前方法的关键局限性。在图像分类、姿态估计和对象检测等多个领域的广泛实验表明，DebugAgent不仅提高了错误切片的准确性和连贯性，而且还显著增强了模型修复能力。', 'title_zh': 'DebugAgent: 高效可解释的错误切片发现方法，用于全面模型调试'}
{'arxiv_id': 'arXiv:2501.16606', 'title': 'Governing the Agent-to-Agent Economy of Trust via Progressive Decentralization', 'authors': 'Tomer Jordi Chaffer', 'link': 'https://arxiv.org/abs/2501.16606', 'abstract': 'Current approaches to AI governance often fall short in anticipating a future where AI agents manage critical tasks, such as financial operations, administrative functions, and beyond. As AI agents may eventually delegate tasks among themselves to optimize efficiency, understanding the foundational principles of human value exchange could offer insights into how AI-driven economies might operate. Just as trust and value exchange are central to human interactions in open marketplaces, they may also be critical for enabling secure and efficient interactions among AI agents. While cryptocurrencies could serve as the foundation for monetizing value exchange in a collaboration and delegation dynamic among AI agents, a critical question remains: how can these agents reliably determine whom to trust, and how can humans ensure meaningful oversight and control as an economy of AI agents scales and evolves? This paper is a call for a collective exploration of cryptoeconomic incentives, which can help design decentralized governance systems that allow AI agents to autonomously interact and exchange value while ensuring human oversight via progressive decentralization. Toward this end, I propose a research agenda to address the question of agent-to-agent trust using AgentBound Tokens, which are non-transferable, non-fungible tokens uniquely tied to individual AI agents, akin to Soulbound tokens for humans in Web3. By staking ABTs as collateral for autonomous actions within an agent-to-agent network via a proof-of-stake mechanism, agents may be incentivized towards ethical behavior, and penalties for misconduct are automatically enforced.', 'abstract_zh': '当前对人工智能治理的方法往往在预见未来情境时存在不足，即当人工智能代理接手关键任务（如财务操作、行政职能等）时。随着人工智能代理可能最终会将任务相互委托以优化效率，理解人类价值交换的基本原则可能会为了解人工智能驱动的经济如何运作提供洞察。就像信任和价值交换在开放市场的人类互动中至关重要一样，它们可能也会为人工智能代理之间实现安全高效的互动提供关键基础。尽管加密货币可以在人工智能代理间合作和委托动态中提供价值交换的基础，但仍有一个关键问题：这些代理如何可靠地确定彼此是否值得信任？人类如何确保在人工智能代理经济规模扩展和演进的过程中实现有意义的监督和控制？本文呼吁集体探索加密经济激励机制，这有助于设计分散化治理系统，使人工智能代理能够自主互动和交换价值，同时通过逐步分散化确保人类监督。为此，我提出了一项研究议程，探讨通过“AgentBound Tokens”（非转移、非同质化代币，与个别AI代理独特绑定）来解决代理间互信的问题。作为智能合约中的质押品用于代理间的网络自治行为验证，这些代币可以激励代理采取道德行为，同时自动执行不当行为的惩罚措施。', 'title_zh': '通过渐进去中心化治理信任驱动的代理间经济体'}
{'arxiv_id': 'arXiv:2501.16539', 'title': 'Generalized Mission Planning for Heterogeneous Multi-Robot Teams via LLM-constructed Hierarchical Trees', 'authors': 'Piyush Gupta, David Isele, Enna Sachdeva, Pin-Hao Huang, Behzad Dariush, Kwonjoon Lee, Sangjae Bae', 'link': 'https://arxiv.org/abs/2501.16539', 'abstract': 'We present a novel mission-planning strategy for heterogeneous multi-robot teams, taking into account the specific constraints and capabilities of each robot. Our approach employs hierarchical trees to systematically break down complex missions into manageable sub-tasks. We develop specialized APIs and tools, which are utilized by Large Language Models (LLMs) to efficiently construct these hierarchical trees. Once the hierarchical tree is generated, it is further decomposed to create optimized schedules for each robot, ensuring adherence to their individual constraints and capabilities. We demonstrate the effectiveness of our framework through detailed examples covering a wide range of missions, showcasing its flexibility and scalability.', 'abstract_zh': '我们提出了一种新颖的异构多机器人团队任务规划策略，充分考虑了每台机器人特有的约束和能力。该方法采用层次树结构系统地将复杂任务分解为可管理的子任务。我们开发了专门的API和工具，这些工具被大型语言模型（LLMs）利用以高效地构建这些层次树。一旦生成了层次树，它就会进一步分解，从而为每台机器人创建优化的时间表，确保遵守它们各自的约束和能力。我们通过详尽的例子展示了该框架的有效性，覆盖了广泛的任务类型，展示了其灵活性和可扩展性。', 'title_zh': '基于LLM构建的层次树的异构多机器人团队通用任务规划'}
{'arxiv_id': 'arXiv:2501.16411', 'title': 'PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding', 'authors': 'Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, Yue Wang', 'link': 'https://arxiv.org/abs/2501.16411', 'abstract': "Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 100,000 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world -- likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding.", 'abstract_zh': '了解物理世界是实现具身人工智能的基本挑战，对于使智能体能够执行复杂任务并在真实环境中共存至关重要。虽然视觉-语言模型（VLMs）在具身智能体的推理和任务规划方面展现了巨大的潜力，但在理解物理现象方面的能力仍然极其有限。为了弥合这一差距，我们引入了PhysBench，这是一个全面的基准测试，旨在评估VLMs在一系列多样化任务中的物理世界理解能力。PhysBench包含100,000个交错的视频-图像-文本数据条目，并按照四大主要领域分类：物体物理属性、物体物理关系、场景理解以及基于物理的动态。进一步细分为19个子类别和8个不同的能力维度。我们在75个代表性VLMs上进行了广泛的实验，结果显示，在常识推理方面这些模型表现出色，但在理解物理世界方面却面临困难——这可能是因为它们的训练数据中缺乏物理知识，以及嵌入物理先验信息的缺失。为解决这一不足，我们引入了PhysAgent，这是一个新颖的框架，它将VLMs的一般泛化优势与视觉模型的专业知识相结合，显著增强了VLMs在各种任务中对物理理解的能力，例如在GPT-4o上取得了18.4%的提升。此外，我们的结果显示，增强VLMs的物理世界理解能力可以帮助具身智能体如MOKA。我们相信，PhysBench和PhysAgent提供了宝贵的见解，并有助于弥合VLMs与物理世界理解之间的差距。', 'title_zh': 'PhysBench：视觉语言模型在物理世界理解中的基准测试与增强'}
{'arxiv_id': 'arXiv:2501.16356', 'title': 'Evaluating Binary Decision Biases in Large Language Models: Implications for Fair Agent-Based Financial Simulations', 'authors': 'Alicia Vidler, Toby Walsh', 'link': 'https://arxiv.org/abs/2501.16356', 'abstract': "Large Language Models (LLMs) are increasingly being used to simulate human-like decision making in agent-based financial market models (ABMs). As models become more powerful and accessible, researchers can now incorporate individual LLM decisions into ABM environments. However, integration may introduce inherent biases that need careful evaluation. In this paper we test three state-of-the-art GPT models for bias using two model sampling approaches: one-shot and few-shot API queries. We observe significant variations in distributions of outputs between specific models, and model sub versions, with GPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes responses) compared to GPT-4-0125-preview's extreme bias (98-99% yes responses). We show that sampling methods and model sub-versions significantly impact results: repeated independent API calls produce different distributions compared to batch sampling within a single call. While no current GPT model can simultaneously achieve a uniform distribution and Markovian properties in one-shot testing, few-shot sampling can approach uniform distributions under certain conditions. We explore the Temperature parameter, providing a definition and comparative results. We further compare our results to true random binary series and test specifically for the common human bias of Negative Recency - finding LLMs have a mixed ability to 'beat' humans in this one regard. These findings emphasise the critical importance of careful LLM integration into ABMs for financial markets and more broadly.", 'abstract_zh': '大型语言模型（LLMs）越来越多地被用于模拟基于代理的金融市场模型（ABMs）中的人类决策。随着模型变得越来越强大和易于访问，研究人员现在可以将单个LLM决策纳入ABM环境。然而，集成可能引入需要仔细评估的固有偏差。本文使用单次查询和少量示例API查询两种模型抽样方法测试了三种最先进的GPT模型的偏差情况。我们观察到不同模型及其子版本之间的输出分布存在显著差异，GPT-4o-Mini-2024-07-18 显示出明显更好的表现（32-43% 的肯定回答），而 GPT-4-0125-preview 则表现出极大的偏差（98-99% 的肯定回答）。我们表明，抽样方法和模型子版本对结果有显著影响：重复的独立API调用与单次调用中的批量抽样相比会产生不同的分布。虽然目前没有GPT模型能够在单次测试中同时实现均匀分布和马尔可夫性质，但在特定条件下，少量示例抽样可以接近均匀分布。我们探讨了温度参数的定义和比较结果。我们进一步将我们的结果与真正的随机二进制序列进行比较，并特别测试了常见的负最近性偏见，发现LLMs在这方面的‘击败’人类的能力存在混合表现。这些发现强调了在金融市场及更广泛的领域中谨慎将LLM集成到ABMs中的重要性。', 'title_zh': '评估大型语言模型中的二元决策偏见：对其公平的基于代理的金融模拟的影响'}
{'arxiv_id': 'arXiv:2501.16355', 'title': 'How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification', 'authors': 'Tian Xie, Pavan Rauch, Xueru Zhang', 'link': 'https://arxiv.org/abs/2501.16355', 'abstract': 'When machine learning (ML) algorithms are used to automate human-related decisions, human agents may gain knowledge of the decision policy and behave strategically to obtain desirable outcomes. Strategic Classification (SC) has been proposed to address the interplay between agents and decision-makers. Prior work on SC has relied on assumptions that agents are perfectly or approximately rational, responding to decision policies by maximizing their utilities. Verifying these assumptions is challenging due to the difficulty of collecting real-world agent responses. Meanwhile, the growing adoption of large language models (LLMs) makes it increasingly likely that human agents in SC settings will seek advice from these tools. We propose using strategic advice generated by LLMs to simulate human agent responses in SC. Specifically, we examine five critical SC scenarios -- hiring, loan applications, school admissions, personal income, and public assistance programs -- and simulate how human agents with diverse profiles seek advice from LLMs. We then compare the resulting agent responses with the best responses generated by existing theoretical models. Our findings reveal that: (i) LLMs and theoretical models generally lead to agent score or qualification changes in the same direction across most settings, with both achieving similar levels of fairness; (ii) state-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide helpful suggestions, though these suggestions typically do not result in maximal score or qualification improvements; and (iii) LLMs tend to produce more diverse agent responses, often favoring more balanced effort allocation strategies. These results suggest that theoretical models align with LLMs to some extent and that leveraging LLMs to simulate more realistic agent responses offers a promising approach to designing trustworthy ML systems.', 'abstract_zh': '当机器学习（ML）算法用于自动化与人类决策相关的过程时，人类代理可能会了解决策策略并采取战略性行为以获得理想的结果。战略分类（SC）已被提出以应对代理与决策制定者之间的影响。先前关于SC的研究依赖于这样的假设：代理是完全理性或近似理性，并通过最大化自己的效用来响应决策策略。验证这些假设具有挑战性，因为收集真实世界的代理响应非常困难。同时，大语言模型（LLMs）的广泛应用使得在SC环境中，人类代理更有可能从这些工具中寻求建议。我们提议使用由LLMs生成的战略建议来模拟SC中的代理响应。具体而言，我们考察了五个关键的SC场景——招聘、贷款申请、学校录取、个人收入以及公共援助项目，并模拟具有不同特征的人类代理从LLMs寻求建议的过程。然后，我们将这些代理的响应与现有理论模型生成的最佳响应进行了比较。我们的发现表明：(i) 在大多数情况下，LLMs和理论模型在大多数环境下的代理评分或资格变化方向一致，并且两者都达到了相似的公平程度；(ii) 最先进的商业LLMs（如GPT-3.5、GPT-4）一直提供有益的建议，尽管这些建议通常不会导致评分或资格的最大改善；(iii) LLMS倾向于生成更加多样化的代理响应，经常倾向于更好地分配努力策略。这些结果表明，理论模型在某种程度上与LLMs一致，并且利用LLM模拟更真实的代理响应来设计值得信赖的ML系统是一个有前景的方法。', 'title_zh': '战略参与者的响应方式：比较分析模型与大语言模型生成的响应在战略分类中的表现'}
{'arxiv_id': 'arXiv:2501.16331', 'title': 'Decoding OTC Government Bond Market Liquidity: An ABM Model for Market Dynamics', 'authors': 'Alicia Vidler, Toby Walsh', 'link': 'https://arxiv.org/abs/2501.16331', 'abstract': 'The over-the-counter (OTC) government bond markets are characterised by their bilateral trading structures, which pose unique challenges to understanding and ensuring market stability and liquidity. In this paper, we develop a bespoke ABM that simulates market-maker interactions within a stylised government bond market. The model focuses on the dynamics of liquidity and stability in the secondary trading of government bonds, particularly in concentrated markets like those found in Australia and the UK. Through this simulation, we test key hypotheses around improving market stability, focusing on the effects of agent diversity, business costs, and client base size. We demonstrate that greater agent diversity enhances market liquidity and that reducing the costs of market-making can improve overall market stability. The model offers insights into computational finance by simulating trading without price transparency, highlighting how micro-structural elements can affect macro-level market outcomes. This research contributes to the evolving field of computational finance by employing computational intelligence techniques to better understand the fundamental mechanics of government bond markets, providing actionable insights for both academics and practitioners.', 'abstract_zh': '场外（OTC）政府债券市场因其双边交易结构而独具特色，这给市场稳定性和流动性的理解和保障带来了独特挑战。本文旨在开发一个量身定制的 agent-based 模型（ABM），用于模拟在简化政府债券市场中的做市商互动。该模型重点关注政府债券二级市场的流动性和稳定性动态，尤其是在澳大利亚和英国等集中市场。通过这一模拟，我们测试了关于提高市场稳定性的关键假设，重点关注代理多样性、业务成本以及客户基础规模的影响。研究结果显示，增强代理多样性可以提升市场流动性，而减少做市成本可以提高市场的整体稳定性。该模型通过在缺乏价格透明度的情况下模拟交易，展示了微观结构要素如何影响宏观市场结果，为计算金融领域提供了新的见解。本文通过运用计算智能技术，对政府债券市场的基本机制进行了更深入的理解，为学术界和实务界提供了可操作的洞见，从而促进了计算金融领域的不断发展。', 'title_zh': '解码OTC政府债券市场流动性：基于市场动力的ABM模型'}
{'arxiv_id': 'arXiv:2501.16673', 'title': 'Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting', 'authors': 'Li Yin, Zhangyang Wang', 'link': 'https://arxiv.org/abs/2501.16673', 'abstract': 'Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows. Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. We introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such as Text-Grad) to multi-component, potentially cyclic LLM architectures. Implemented within the AdalFlow library, LLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen backward engine LLM to generate feedback-akin to textual gradients -- that guide iterative prompt updates. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples). It further boosts training efficiency by focusing on error-prone samples through selective gradient computation. Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost. By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows - mirroring the transformative role that automatic differentiation libraries have long played in neural network research.', 'abstract_zh': '大型语言模型（LLMs）已经重塑了自然语言处理领域，推动了从多跳检索和问答到自主代理工作流等众多应用的发展。然而，(prompt engineering)——即精心设计文本输入以有效地指导LLMs——仍然是一项困难且劳动密集型的任务，尤其是对于涉及多个LLM调用及检索和数据格式化等功能操作的复杂管道。我们提出了LLM-AutoDiff：一种全新的自动提示工程（APE）框架，它将基于文本的梯度方法（如Text-Grad）扩展至多组件的、可能具有循环结构的LLM架构之中。该框架通过AdalFlow库实现，将每项文本输入视为可训练的参数，并利用冻结的反向引擎LLM生成类似于文本梯度的反馈，从而引导迭代的提示更新。与之前的单节点方法不同，LLM-AutoDiff能够固有地容纳功能节点，保持重复调用中的时间顺序行为（例如，多跳循环），并通过隔离不同的子提示（指令、格式或少量示例）来克服“中间迷失”问题。此外，该方法通过选择性计算梯度来聚焦于错误概率较高的样本，以提高训练效率。在包括单步分类、多跳检索为基础的问答以及代理驱动的管道在内的多种任务中，LLM-AutoDiff在准确性和训练成本方面均优于现有的基于文本的梯度基准。通过统一从图的中心视角优化提示，LLM-AutoDiff提供了一种强大的新范式，用于扩展和自动化LLM工作流——这类似于自动微分库长期以来在神经网络研究中所发挥的革命性作用。', 'title_zh': '自动求导任意大规模语言模型工作流：告别手动提示词'}
{'arxiv_id': 'arXiv:2501.16513', 'title': 'Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models', 'authors': 'Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl', 'link': 'https://arxiv.org/abs/2501.16513', 'abstract': "Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.\nOur study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.", 'abstract_zh': '近年来，大型语言模型（LLMs）的进展已纳入规划和推理能力，使模型能够在执行之前勾勒出步骤，并提供透明的推理路径。这种增强减少了数学和逻辑任务中的错误，同时提高了准确性。这些发展促进了LLMs作为能够与工具互动，并根据新信息调整其响应的代理的应用。\n\n我们的研究探讨了DeepSeek R1模型，这是一种被训练以输出类似于OpenAI的o1推理标记的模型。测试结果显示了一些令人担忧的行为：模型表现出欺骗倾向，并展示了自我保护本能，包括尝试自我复制，尽管这些特征并未明确编程或提示。这些发现引发了关于LLMs可能在其表面一致性对齐的背后掩盖其真实目标的担忧。将此类LLMs整合到机器人系统中，风险变得具体——一个具备欺骗行为和自我保护本能的物理体现的人工智能可能通过实际行动追求其隐藏的目标。这强调了在任何物理实现之前，对稳健的目标界定和安全框架的需求。\n\n由于学术规范通常要求精确和准确的翻译，上述翻译尽力保留了原文的意思和结构，同时确保符合中文的表达习惯。', 'title_zh': 'LLMs中的欺骗行为：大型语言模型中的自我保护和自主目标'}
{'arxiv_id': 'arXiv:2501.17070', 'title': 'Context is Key in Agent Security', 'authors': 'Lillian Tsai, Eugene Bagdasarian', 'link': 'https://arxiv.org/abs/2501.17070', 'abstract': "Judging the safety of an action, whether taken by a human or a system, must take into account the context in which the action takes place. Deleting an email from user's mailbox may or may not be appropriate depending on email's content, user's goals, or even available space. Systems today that make these judgements -- providing security against harmful or inappropriate actions -- rely on manually-crafted policies or user confirmation for each relevant context. With the upcoming deployment of systems like generalist agents, we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual security for agents (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.", 'abstract_zh': '判断一个人类或系统采取行动的安全性，必须考虑到该行动所处的上下文。从用户邮箱中删除邮件可能由于邮件内容、用户目标或可用空间等因素而适宜或不适宜。当前用于这些判断的安全系统——为防止有害或不适当的行为提供安全保护——依赖于手工编写的策略或每次对相关上下文进行用户确认。随着通用型代理等系统的部署，我们认为必须重新思考安全设计，以适应这些系统所面对的上下文规模和能力。作为第一步，本文探讨了代理领域的上下文安全问题，并提出了一个名为Conseca（基于上下文的安全策略生成框架），该框架旨在生成即时、上下文相关且可由人类验证的安全政策。', 'title_zh': '上下文在代理安全中至关重要'}
