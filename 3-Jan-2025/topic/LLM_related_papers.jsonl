{'arxiv_id': 'arXiv:2501.01336', 'title': 'Aligning Large Language Models for Faithful Integrity Against Opposing Argument', 'authors': 'Yong Zhao, Yang Deng, See-Kiong Ng, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2501.01336', 'abstract': "Large Language Models (LLMs) have demonstrated impressive capabilities in complex reasoning tasks. However, they can be easily misled by unfaithful arguments during conversations, even when their original statements are correct. To this end, we investigate the problem of maintaining faithful integrity in LLMs. This involves ensuring that LLMs adhere to their faithful statements in the face of opposing arguments and are able to correct their incorrect statements when presented with faithful arguments. In this work, we propose a novel framework, named Alignment for Faithful Integrity with Confidence Estimation (AFICE), which aims to align the LLM responses with faithful integrity. Specifically, AFICE first designs a Bilateral Confidence Estimation (BCE) approach for estimating the uncertainty of each response generated by the LLM given a specific context, which simultaneously estimate the model's confidence to the question based on the internal states during decoding as well as to the answer based on cumulative probability ratios. With the BCE, we construct a conversational preference dataset composed of context, original statement, and argument, which is adopted for aligning the LLM for faithful integrity using Direct Preference Optimization (DPO). Extensive experimental results on a wide range of benchmarks demonstrate significant improvements in the LLM's ability to maintain faithful responses when encountering opposing arguments, ensuring both the practical utility and trustworthiness of LLMs in complex interactive settings. Code and data will be released via this https URL", 'abstract_zh': '大型语言模型（LLMs）在复杂推理任务中展现了令人印象深刻的性能。然而，在对话中，它们可能会被不忠实的论证误导，即使它们的原始声明是正确的。为解决这一问题，我们探讨了保持LLM忠实完整性的挑战。这涉及到确保在面对反对方论点时，LLM能够坚守其忠实声明，并在被忠实论点纠正时能够修正其错误声明。在此项研究中，我们提出了一个新颖的框架，称为“基于置信估计的忠实完整性对齐”（AFICE，Alignment for Faithful Integrity with Confidence Estimation），旨在使LLM响应与忠实完整性保持一致。具体来说，AFICE 首先设计了一种双边置信估计（BCE，Bilateral Confidence Estimation）方法，用于估计给定特定上下文中LLM生成的每个响应的不确定性，并同时根据解码过程中模型对问题的置信度和累积概率比来估计模型对答案的置信度。通过BCE，我们构建了一个对话偏好数据集，包含上下文、原始声明和论点，该数据集用于利用直接偏好优化（DPO，Direct Preference Optimization）对齐LLM以保持忠实完整性。广泛基准上的大量实验结果表明，当遇到反对论点时，该框架显著提高了LLM维护忠实响应的能力，确保了LLM在复杂交互环境中的实用性和可信度。代码和数据将在此处发布：https://...', 'title_zh': '将大型语言模型与反对论点的忠实完整性对齐'}
{'arxiv_id': 'arXiv:2501.01332', 'title': 'Decoding Knowledge in Large Language Models: A Framework for Categorization and Comprehension', 'authors': 'Yanbo Fang, Ruixiang Tang', 'link': 'https://arxiv.org/abs/2501.01332', 'abstract': 'Understanding how large language models (LLMs) acquire, retain, and apply knowledge remains an open challenge. This paper introduces a novel framework, K-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness and confidence. The framework defines six categories of knowledge, ranging from highly confident correctness to confidently held misconceptions, enabling a nuanced evaluation of model comprehension beyond binary accuracy. Using this framework, we demonstrate how techniques like chain-of-thought prompting and reinforcement learning with human feedback fundamentally alter the knowledge structures of internal (pre-trained) and external (context-dependent) knowledge in LLMs. CoT particularly enhances base model performance and shows synergistic benefits when applied to aligned LLMs. Moreover, our layer-wise analysis reveals that higher layers in LLMs encode more high-confidence knowledge, while low-confidence knowledge tends to emerge in middle-to-lower layers.', 'abstract_zh': '理解大型语言模型（LLMs）如何获取、保留和应用知识仍然是一项开放性的挑战。本文介绍了一种新的框架，K-(CSA)²，该框架按照正确性和信心两个维度对LLM的知识进行分类。该框架定义了六个知识类别，从高度自信的正确性到有把握的误解，从而实现对模型理解程度的细腻评估，超越了二元准确性的评价。通过使用这种框架，我们展示了诸如思维链提示和带有人类反馈的强化学习等技术如何根本性地改变内部（预训练的）和外部（上下文相关的）知识结构。思维链特别增强了基础模型的表现，并且在应用于对齐的语言模型时显示出协同效益。此外，我们的逐层分析表明，LLM中的较高层编码了更多的高信心知识，而低信心知识容易在中间到较低层出现。', 'title_zh': '大型语言模型中的知识解码：一种分类和理解的框架'}
{'arxiv_id': 'arXiv:2501.01306', 'title': 'Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking', 'authors': 'Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2501.01306', 'abstract': 'Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g. MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches.', 'abstract_zh': '大型语言模型（LLMs）展现出卓越的能力，但仍面临幻觉问题。典型的文字生成方法采用自动回归生成而不进行刻意的推理，这往往导致不可靠且事实不准确的回答。在本文中，我们提出了一种名为HaluSearch的新型框架，该框架结合了基于树搜索的算法（例如MCTS），以在推理过程中通过显式的慢思考生成过程来减轻LLMs的幻觉问题。具体而言，HaluSearch将文本生成构想为逐步推理过程，使用自我评估奖励模型来评估每一步生成，并引导树搜索通往最可靠的生成路径，以便充分利用LLMs的内部知识。为了平衡效率和质量，我们引入了一种受认知科学中的双重过程理论启发的分层思考系统切换机制，在实例和步骤层面上动态地交替使用快速和慢速思考模式，以适应问题和推理状态的复杂性。我们对英文和中文数据集进行了广泛的实验，并且结果显示，我们的方法显著优于基线方法。', 'title_zh': '思考更多，幻觉更少：通过快速思维和慢速思维双重过程减轻幻觉问题'}
{'arxiv_id': 'arXiv:2501.01305', 'title': 'Large Language Models for Mental Health Diagnostic Assessments: Exploring The Potential of Large Language Models for Assisting with Mental Health Diagnostic Assessments -- The Depression and Anxiety Case', 'authors': 'Kaushik Roy, Harshul Surana, Darssan Eswaramoorthi, Yuxin Zi, Vedant Palit, Ritvik Garimella, Amit Sheth', 'link': 'https://arxiv.org/abs/2501.01305', 'abstract': 'Large language models (LLMs) are increasingly attracting the attention of healthcare professionals for their potential to assist in diagnostic assessments, which could alleviate the strain on the healthcare system caused by a high patient load and a shortage of providers. For LLMs to be effective in supporting diagnostic assessments, it is essential that they closely replicate the standard diagnostic procedures used by clinicians. In this paper, we specifically examine the diagnostic assessment processes described in the Patient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and the Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized anxiety disorder (GAD). We investigate various prompting and fine-tuning techniques to guide both proprietary and open-source LLMs in adhering to these processes, and we evaluate the agreement between LLM-generated diagnostic outcomes and expert-validated ground truth. For fine-tuning, we utilize the Mentalllama and Llama models, while for prompting, we experiment with proprietary models like GPT-3.5 and GPT-4o, as well as open-source models such as llama-3.1-8b and mixtral-8x7b.', 'abstract_zh': '大型语言模型（LLMs）日益受到医疗专业人员的关注，它们有可能在诊断评估中提供帮助，从而缓解由于患者数量过多和医疗提供者短缺而导致的医疗系统压力。为了使LLMs在支持诊断评估方面有效，它们必须紧密复制临床医生使用的标准诊断程序。本文具体研究了用于重度抑郁症（MDD）的患者健康问卷-9（PHQ-9）和用于广泛性焦虑障碍（GAD）的一般化焦虑问卷-7（GAD-7）中的诊断评估过程。我们探讨了各种提示和微调技术，以引导自有的和开源的LLMs遵守这些过程，并且评估了LLM生成的诊断结果与专家验证的黄金标准之间的一致性。在微调方面，我们使用了Mentalllama和Llama模型，而在提示方面，我们尝试了诸如GPT-3.5和GPT-4o等自有模型，以及诸如llama-3.1-8b和mixtral-8x7b等开源模型。', 'title_zh': '大型语言模型在心理健康诊断评估中的应用：探索大型语言模型在辅助心理健康诊断评估中的潜力——以抑郁和焦虑为例'}
{'arxiv_id': 'arXiv:2501.01264', 'title': 'ProgCo: Program Helps Self-Correction of Large Language Models', 'authors': 'Xiaoshuai Song, Yanan Wu, Weixun Wang, Jiaheng Liu, Wenbo Su, Bo Zheng', 'link': 'https://arxiv.org/abs/2501.01264', 'abstract': 'Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools.', 'abstract_zh': '自我修正旨在使大型语言模型（LLMs）能够在没有外部反馈的情况下自我验证和自我完善其初始响应。然而，LLMs 经常无法有效地自我验证并生成正确的反馈，进一步导致调整失误，使得自我修正失败，特别是在复杂推理任务中。本文提出了一种程序驱动的自我修正方法（ProgCo）。首先，程序驱动的验证（ProgVe）利用自动生成并执行的验证伪程序来实现复杂的验证逻辑和广泛的验证。然后，程序驱动的调整（ProgRe）从 ProgVe 获取反馈，对响应和验证程序进行双重反思和调整，以减轻不正确的反馈对复杂推理任务的影响。在三个指令跟随和数学基准测试中的实验表明，ProgCo 能够实现有效的自我修正，并且在与实际程序工具结合使用时可以进一步提高性能。', 'title_zh': 'ProgCo: 程序辅助大型语言模型的自我修正'}
{'arxiv_id': 'arXiv:2501.01256', 'title': 'Digital Guardians: Can GPT-4, Perspective API, and Moderation API reliably detect hate speech in reader comments of German online newspapers?', 'authors': 'Manuel Weber, Moritz Huber, Maximilian Auch, Alexander Döschl, Max-Emanuel Keller, Peter Mandl', 'link': 'https://arxiv.org/abs/2501.01256', 'abstract': "In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score.", 'abstract_zh': '近年来，有毒内容和仇恨言论在网络空间中变得越来越普遍。在线报纸和论坛的管理员，部分由于法律法规的要求，现在需要仔细审查读者评论，并在必要时删除有害内容。这一过程是劳动密集型的。一些大型语言模型的提供商已经提出了自动检测仇恨言论或有毒内容的解决方案。这些解决方案包括OpenAI的GPT-4o、Jigsaw（Google）的Perspective API和OpenAI的 Moderation API。基于专门用于开发检测在线报纸读者评论中仇恨言论的工具的德语测试数据集HOCON34k，这些解决方案进行了比较，并与HOCON34k基线进行了对比。测试数据集包含1,592个标注过的文本样本。对于GPT-4o，采用了三种不同的提示方法，分别是零样本（Zero-Shot）、单样本（One-Shot）和少样本（Few-Shot）方法。实验结果表明，GPT-4o在综合MCC和F2分数的衡量标准下，优于Perspective API和Moderation API，并且比HOCON34k基线高出约5个百分点。', 'title_zh': '数字守护者：GPT-4、Perspective API和Moderation API能否可靠地检测德国在线报纸读者评论中的仇恨言论？'}
{'arxiv_id': 'arXiv:2501.01246', 'title': 'Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base Completion', 'authors': 'Qiyuan He, Jianfei Yu, Wenya Wang', 'link': 'https://arxiv.org/abs/2501.01246', 'abstract': "Integrating large language models (LLMs) with rule-based reasoning offers a powerful solution for improving the flexibility and reliability of Knowledge Base Completion (KBC). Traditional rule-based KBC methods offer verifiable reasoning yet lack flexibility, while LLMs provide strong semantic understanding yet suffer from hallucinations. With the aim of combining LLMs' understanding capability with the logical and rigor of rule-based approaches, we propose a novel framework consisting of a Subgraph Extractor, an LLM Proposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs from the KB. Then, the LLM uses these subgraphs to propose diverse and meaningful rules that are helpful for inferring missing facts. To effectively avoid hallucination in LLMs' generations, these proposed rules are further refined by a Rule Reasoner to pinpoint the most significant rules in the KB for Knowledge Base Completion. Our approach offers several key benefits: the utilization of LLMs to enhance the richness and diversity of the proposed rules and the integration with rule-based reasoning to improve reliability. Our method also demonstrates strong performance across diverse KB datasets, highlighting the robustness and generalizability of the proposed framework.", 'abstract_zh': '将大型语言模型（LLM）与基于规则的推理相结合为提高知识库完成（KBC）的灵活性和可靠性提供了一种强大的解决方案。传统的基于规则的KBC方法能够验证推理过程但缺乏灵活性，而LLM则提供了强大的语义理解能力，但却容易出现幻觉。为了解决这一点，我们旨在结合LLM的理解能力和基于规则方法的逻辑性和严谨性，提出了一种新的框架，该框架包括子图提取器、LLM 提出者和规则推理器。首先，子图提取器从知识库中抽取子图。然后，LLM 使用这些子图提出多样且有意义的规则，这些规则有助于推断缺失的事实。为了有效避免LLM生成过程中出现的幻觉，这些提出的规则将由规则推理器进一步精炼，以指出知识库中最关键的规则，从而促进知识库完成。我们的方法提供了几个关键优势：通过利用LLM增加所提出规则的丰富性和多样性，并与基于规则的推理相结合以提高可靠性。此外，我们的方法在多种不同的知识库数据集上表现出强大性能，这凸显了提出框架的稳健性和通用性。', 'title_zh': '大型语言模型增强的符号推理在知识库补全中的应用'}
{'arxiv_id': 'arXiv:2501.01237', 'title': 'Automated Self-Refinement and Self-Correction for LLM-based Product Attribute Value Extraction', 'authors': 'Alexander Brinkmann, Christian Bizer', 'link': 'https://arxiv.org/abs/2501.01237', 'abstract': "Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques, error-based prompt rewriting and self-correction, to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques have only a marginal impact on the model's performance across the different scenarios, while significantly increasing processing costs. For scenarios with training data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases.", 'abstract_zh': '结构化产品数据，以属性-值对的形式存在，对于电商平台支持功能例如分类产品搜索和基于属性的产品比较至关重要。然而，供应商通常提供的是非结构化产品描述，因此需要从非结构化描述中提取属性值以确保数据的一致性和可用性。大规模语言模型（LLMs）已经在少量示例场景下展示了其在产品属性值提取方面的潜力。近期的研究表明，自我修正技术可以提高LLMs在代码生成和文本到SQL翻译等任务上的性能。对于其他任务，这些技术的应用却导致了成本增加，因为处理额外的标记并没有提升性能。本文探讨了应用两种自我修正技术——基于错误的提示重写和自我修正——到产品属性值提取任务中。通过使用GPT-4o的零样本、少量上下文学习和微调场景评估这两大自我修正技术。实验结果显示，在不同的场景中，这两种自我修正技术对模型性能的影响微乎其微，但显著增加了处理成本。在有训练数据的情况下，微调场景获得了最高的性能，而且随着产品描述数量的增加，微调的成本递增效应逐渐被平衡。', 'title_zh': '基于LLM的产品属性值自动化自我精炼与自我修正'}
{'arxiv_id': 'arXiv:2501.01059', 'title': 'Dynamic Attention-Guided Context Decoding for Mitigating Context Faithfulness Hallucinations in Large Language Models', 'authors': 'Yanwen Huang, Yong Zhang, Ning Cheng, Zhitao Li, Shaojun Wang, Jing Xiao', 'link': 'https://arxiv.org/abs/2501.01059', 'abstract': "Large language models (LLMs) often suffer from context faithfulness hallucinations, where outputs deviate from retrieved information due to insufficient context utilization and high output uncertainty. Our uncertainty evaluation experiments reveal a strong correlation between high uncertainty and hallucinations. We hypothesize that attention mechanisms encode signals indicative of contextual utilization, validated through probing analysis. Based on these insights, we propose Dynamic Attention-Guided Context Decoding (DAGCD), a lightweight framework that integrates attention distributions and uncertainty signals in a single-pass decoding process. Experiments across QA datasets demonstrate DAGCD's effectiveness, achieving significant improvements in faithfulness and robustness while maintaining computational efficiency.", 'abstract_zh': '大型语言模型（LLMs）往往会遭受语境忠实性幻觉的问题，即输出与检索到的信息相偏离，这可能是由于上下文利用不足和高输出不确定性所致。我们的不确定性评估实验揭示了高不确定性与幻觉之间存在强烈的相关性。我们假设注意力机制编码了与上下文利用相关的信号，并通过探查分析进行了验证。基于这些见解，我们提出了动态注意力引导的上下文解码（DAGCD），这是一个轻量级框架，将注意力分布和不确定性信号整合到单次解码过程中。通过跨多种问答数据集的实验，证明了DAGCD的有效性，不仅在忠实性和鲁棒性方面取得了显著改进，同时保持了计算效率。', 'title_zh': '面向大型语言模型减轻上下文忠实性幻觉的动态注意力导向上下文解码'}
{'arxiv_id': 'arXiv:2501.01014', 'title': 'MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based on Large language Model', 'authors': 'Chengze Zhang, Changshan Li, Shiyang Gao', 'link': 'https://arxiv.org/abs/2501.01014', 'abstract': "The exponential growth of data and advancements in big data technologies have created a demand for more efficient and automated approaches to data analysis and storytelling. However, automated data analysis systems still face challenges in leveraging large language models (LLMs) for data insight discovery, augmented analysis, and data storytelling. This paper introduces the Multidimensional Data Storytelling Framework (MDSF) based on large language models for automated insight generation and context-aware storytelling. The framework incorporates advanced preprocessing techniques, augmented analysis algorithms, and a unique scoring mechanism to identify and prioritize actionable insights. The use of fine-tuned LLMs enhances contextual understanding and generates narratives with minimal manual intervention. The architecture also includes an agent-based mechanism for real-time storytelling continuation control. Key findings reveal that MDSF outperforms existing methods across various datasets in terms of insight ranking accuracy, descriptive quality, and narrative coherence. The experimental evaluation demonstrates MDSF's ability to automate complex analytical tasks, reduce interpretive biases, and improve user satisfaction. User studies further underscore its practical utility in enhancing content structure, conclusion extraction, and richness of detail.", 'abstract_zh': '大数据的指数级增长和大数据技术的进步促使对更高效和自动化的数据分析与叙述方法的需求。然而，自动数据分析系统在利用大型语言模型（LLM）进行数据洞察发现、增强分析和数据叙述方面仍面临挑战。本文介绍了一种基于大型语言模型的多维数据叙述框架（MDSF），该框架用于自动化洞察生成和上下文感知叙述。该框架整合了先进的预处理技术、增强分析算法以及独特的评分机制来识别和优先处理可操作的洞察。微调的LLM增强了对上下文的理解，并生成了需要最少手动干预的故事叙述。该架构还包括一种基于代理的机制，用于实时叙述延续控制。关键发现表明，MDSF在各类数据集上的洞察排名准确性、描述质量和叙述连贯性方面优于现有方法。实验评估展示了MDSF自动化复杂分析任务、减少解释偏见和提高用户满意度的能力。用户研究进一步突显了其在增强内容结构、结论提取和细节 richness 方面的实用价值。', 'title_zh': 'MDSF：基于大型语言模型的上下文感知多维数据叙事框架\n\n这个标题翻译符合学术规范，保留了原文的含义和结构。其中，“MDSF”被译为“MDSF”，保持了原文的简称形式。“基于大型语言模型”准确地翻译了“based on Large language Model”，确保术语的专业性和准确性。'}
{'arxiv_id': 'arXiv:2501.00999', 'title': 'Exploring Information Processing in Large Language Models: Insights from Information Bottleneck Theory', 'authors': 'Zhou Yang, Zhengyu Qi, Zhaochun Ren, Zhikai Jia, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao', 'link': 'https://arxiv.org/abs/2501.00999', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks by understanding input information and predicting corresponding outputs. However, the internal mechanisms by which LLMs comprehend input and make effective predictions remain poorly understood. In this paper, we explore the working mechanism of LLMs in information processing from the perspective of Information Bottleneck Theory. We propose a non-training construction strategy to define a task space and identify the following key findings: (1) LLMs compress input information into specific task spaces (e.g., sentiment space, topic space) to facilitate task understanding; (2) they then extract and utilize relevant information from the task space at critical moments to generate accurate predictions. Based on these insights, we introduce two novel approaches: an Information Compression-based Context Learning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances reasoning performance and inference efficiency by compressing retrieved example information into the task space. TS-FT employs a space-guided loss to fine-tune LLMs, encouraging the learning of more effective compression and selection mechanisms. Experiments across multiple datasets validate the effectiveness of task space construction. Additionally, IC-ICL not only improves performance but also accelerates inference speed by over 40\\%, while TS-FT achieves superior results with a minimal strategy adjustment.', 'abstract_zh': '大型语言模型（LLMs）已在广泛的任务中展示了卓越的性能，通过理解输入信息并预测相应的输出。然而，LLMs 在处理输入信息和作出有效预测时的具体工作机制仍然不甚明了。本文从信息瓶颈理论的角度探讨了LLMs在信息处理过程中的工作机制。我们提出了一种非训练的构建策略来定义任务空间，并发现以下关键发现：（1）LLMs 将输入信息压缩到特定的任务空间（例如情感空间、主题空间），以利于任务理解；（2）然后在关键时刻从任务空间中提取和利用相关信息来生成准确的预测。基于这些见解，我们引入了两种新的方法：信息压缩为基础的上下文学习（IC-ICL）和任务空间导向的微调（TS-FT）。IC-ICL 通过将检索到的示例信息压缩到任务空间中，增强推理性能和推理效率。TS-FT 利用空间导向的损失对LLMs 进行微调，促使模型学习更有效的压缩和选择机制。通过对多个数据集的实验验证了任务空间构建的有效性。此外，IC-ICL 不仅提高了性能，还通过超过40%的加速提升了推理速度，而TS-FT 在几乎没有策略调整的情况下实现了更优的结果。', 'title_zh': '探索大型语言模型中的信息处理：信息瓶颈理论的洞见'}
{'arxiv_id': 'arXiv:2501.00982', 'title': 'Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice', 'authors': 'Federico Ravenda, Seyed Ali Bahrainian, Andrea Raballo, Antonietta Mira, Noriko Kando', 'link': 'https://arxiv.org/abs/2501.00982', 'abstract': "In psychological practice, standardized questionnaires serve as essential tools for assessing mental constructs (e.g., attitudes, traits, and emotions) through structured questions (aka items). With the increasing prevalence of social media platforms where users share personal experiences and emotions, researchers are exploring computational methods to leverage this data for rapid mental health screening. In this study, we propose a novel adaptive Retrieval-Augmented Generation (RAG) approach that completes psychological questionnaires by analyzing social media posts. Our method retrieves the most relevant user posts for each question in a psychological survey and uses Large Language Models (LLMs) to predict questionnaire scores in a zero-shot setting. Our findings are twofold. First we demonstrate that this approach can effectively predict users' responses to psychological questionnaires, such as the Beck Depression Inventory II (BDI-II), achieving performance comparable to or surpassing state-of-the-art models on Reddit-based benchmark datasets without relying on training data. Second, we show how this methodology can be generalized as a scalable screening tool, as the final assessment is systematically derived by completing standardized questionnaires and tracking how individual item responses contribute to the diagnosis, aligning with established psychometric practices.", 'abstract_zh': '在心理学实践中，标准化问卷是通过结构化问题（即项目）来评估心理构念（如态度、特质和情绪）的重要工具。随着社交媒体平台的普及，用户在此平台上分享个人经历和情绪，研究人员正在探索利用这些数据进行快速心理健康筛查的计算方法。在本研究中，我们提出了一种新颖的自适应检索增强生成（RAG）方法，通过分析社交媒体帖子来完成心理学问卷。我们的方法会为心理调查问卷中的每个问题检索最相关的用户帖子，并利用大语言模型（LLMs）在零样本设置下预测问卷得分。我们的发现包括两个方面：首先，我们表明该方法能有效预测用户对心理问卷的反应，如贝克抑郁量表第二版（BDI-II），在基于Reddit的基准数据集上的性能达到了或超越了最先进的模型，且无需依赖训练数据。其次，我们展示了该方法作为可扩展筛查工具的普适性，通过完成标准化问卷并追踪各个项目反应对诊断的影响，系统地得出最终评估结果，这与传统的心理测量实践相一致。', 'title_zh': '大规模语言模型在心理健康评估中的有效性：利用自适应检索增强技术实现可解释的心理测验筛查'}
{'arxiv_id': 'arXiv:2501.00885', 'title': 'Representation in large language models', 'authors': 'Cameron C. Yetman', 'link': 'https://arxiv.org/abs/2501.00885', 'abstract': 'The extraordinary success of recent Large Language Models (LLMs) on a diverse array of tasks has led to an explosion of scientific and philosophical theorizing aimed at explaining how they do what they do. Unfortunately, disagreement over fundamental theoretical issues has led to stalemate, with entrenched camps of LLM optimists and pessimists often committed to very different views of how these systems work. Overcoming stalemate requires agreement on fundamental questions, and the goal of this paper is to address one such question, namely: is LLM behavior driven partly by representation-based information processing of the sort implicated in biological cognition, or is it driven entirely by processes of memorization and stochastic table look-up? This is a question about what kind of algorithm LLMs implement, and the answer carries serious implications for higher level questions about whether these systems have beliefs, intentions, concepts, knowledge, and understanding. I argue that LLM behavior is partially driven by representation-based information processing, and then I describe and defend a series of practical techniques for investigating these representations and developing explanations on their basis. The resulting account provides a groundwork for future theorizing about language models and their successors.', 'abstract_zh': '近年来，大型语言模型（LLMs）在多样性的任务上取得的非凡成功，导致了在解释它们如何执行任务方面产生了大量的科学和哲学理论。不幸的是，对基本理论问题的分歧导致了僵局，LLM 悲观派和乐观派阵营各有其固有的观点，这使得双方之间的共识难以达成。克服僵局需要在基本问题上达成一致，本文的目标是解决这样一个问题：即 LLMS 的行为是否部分地由类似于生物认知的认知表示信息处理驱动，还是完全由记忆和随机查找表的过程驱动？这是一个关于这些系统执行何种算法的问题，而答案对其它高层次问题，例如这些系统是否有信念、意图、概念、知识和理解，具有重要的影响。我认为 LLMS 的行为部分地由认知表示信息处理驱动，并且我将描述并捍卫一系列用于研究这些表示及其基础解释的实用技术。由此得出的描述为未来对语言模型及其后续版本的理论研究奠定了基础。', 'title_zh': '大型语言模型中的表示学习'}
{'arxiv_id': 'arXiv:2501.00874', 'title': 'LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models', 'authors': 'Hieu Man, Nghia Trung Ngo, Viet Dac Lai, Ryan A. Rossi, Franck Dernoncourt, Thien Huu Nguyen', 'link': 'https://arxiv.org/abs/2501.00874', 'abstract': "Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.", 'abstract_zh': '基于大型语言模型（LLMs）的嵌入模型的最新进展已经为文本嵌入任务，特别是在密集向量检索中，设定了新的最先进基准。然而，这些模型主要集中在英语上，使得多语言嵌入能力的探索相对有限。为了解决这一局限性，我们提出LUSIFER，这是一种新颖的零样本方法，可以在无需多语言监督的情况下，将基于LLM的嵌入模型适应多语言任务。LUSIFER的架构结合了一个多语言编码器，作为一种语言通用的学习器，以及一个针对嵌入特定任务优化的基于LLM的嵌入模型。这些组件通过少量可训练参数无缝集成，这些参数充当连接器，有效地将多语言编码器的语言理解能力转移到专门的嵌入模型上。此外，为了全面评估多语言嵌入性能，我们引入了一个新的基准，该基准包括5项主要嵌入任务，123个多样化的数据集，并涵盖14种语言。广泛的实验结果表明，LUSIFER在各种嵌入任务中显著提高了多语言性能，特别是在中低资源语言方面，无需使用显式多语言训练数据。', 'title_zh': 'LUSIFER：语言通用空间集成以增强大型语言模型的多语言嵌入'}
{'arxiv_id': 'arXiv:2501.00868', 'title': 'Large Language Models Are Read/Write Policy-Makers for Simultaneous Generation', 'authors': 'Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Yang Feng', 'link': 'https://arxiv.org/abs/2501.00868', 'abstract': 'Simultaneous generation models write generation results while reading streaming inputs, necessitating a policy-maker to determine the appropriate output timing. Existing simultaneous generation methods generally adopt the traditional encoder-decoder architecture and learn the generation and policy-making capabilities through complex dynamic programming techniques. Although LLMs excel at text generation, they face challenges in taking on the role of policy-makers through traditional training methods, limiting their exploration in simultaneous generation. To overcome these limitations, we propose a novel LLM-driven Simultaneous Generation (LSG) framework, which allows the off-the-shelf LLM to decide the generation timing and produce output concurrently. Specifically, LSG selects the generation policy that minimizes latency as the baseline policy. Referring to the baseline policy, LSG enables the LLM to devise an improved generation policy that better balances latency and generation quality, and writes generation results accordingly. Experiments on simultaneous translation and streaming automatic speech recognition tasks show that our method can achieve state-of-the-art performance utilizing the open-source LLMs and demonstrate practicality in real-world scenarios.', 'abstract_zh': '同时生成模型在读取流式输入的同时生成结果，需要决策者确定适当的输出时机。现有的同时生成方法通常采用传统的编码器-解码器架构，并通过复杂的动态规划技术来学习生成和决策制定的能力。尽管大型语言模型（LLMs）在文本生成方面表现出色，但在传统的训练方法下承担决策者的角色面临挑战，限制了它们在同时生成方面的探索。为了克服这些限制，我们提出了一种新颖的LLM驱动的即刻生成（LSG）框架，该框架允许即用型LLM决定生成时机并同时生成结果。具体来说，LSG 选择减少延迟的策略作为基准策略。参照基准策略，LSG 使LLM能够制定一个能更好地平衡延迟和生成质量的改进策略，并据此生成结果。在同时翻译和流式自动语音识别任务上的实验表明，我们的方法可以利用开源的LLMs达到最先进的性能，并在实际场景中具有实用价值。', 'title_zh': '大规模语言模型是同时生成的读写政策制定者'}
{'arxiv_id': 'arXiv:2501.00830', 'title': 'LLM+AL: Bridging Large Language Models and Action Languages for Complex Reasoning about Actions', 'authors': 'Adam Ishay, Joohyung Lee', 'link': 'https://arxiv.org/abs/2501.00830', 'abstract': 'Large Language Models (LLMs) have made significant strides in various intelligent tasks but still struggle with complex action reasoning tasks that require systematic search. To address this limitation, we propose a method that bridges the natural language understanding capabilities of LLMs with the symbolic reasoning strengths of action languages. Our approach, termed "LLM+AL," leverages the LLM\'s strengths in semantic parsing and commonsense knowledge generation alongside the action language\'s proficiency in automated reasoning based on encoded knowledge. We compare LLM+AL against state-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0, and o1-preview, using benchmarks for complex reasoning about actions. Our findings indicate that, although all methods exhibit errors, LLM+AL, with relatively minimal human corrections, consistently leads to correct answers, whereas standalone LLMs fail to improve even with human feedback. LLM+AL also contributes to automated generation of action languages.', 'abstract_zh': '大语言模型（LLMs）在各种智能任务中取得了显著进展，但在涉及系统搜索的复杂动作推理任务中仍然存在局限性。为了解决这一限制，我们提出了一种方法，将LLMs的自然语言理解能力与动作语言的符号推理优势相结合。我们的方法称为“LLM+AL”，它利用了LLMs在语义解析和常识知识生成方面的优势，同时利用了动作语言基于编码知识进行自动推理的能力。我们使用针对复杂动作推理的基准测试来比较LLM+AL与最新的LLMs，包括ChatGPT-4、Claude 3 Opus、Gemini Ultra 1.0和o1-preview。研究结果表明，尽管所有方法都存在错误，但在最小的人工修正下，LLM+AL始终能够得到正确的答案，而单独的LLMs即使在获得人类反馈后也无法改进。此外，LLM+AL也有助于自动生成动作语言。', 'title_zh': '基于LLM和AL的桥梁：大语言模型与操作语言在复杂动作推理中的融合'}
{'arxiv_id': 'arXiv:2501.00778', 'title': 'Decoding the Flow: CauseMotion for Emotional Causality Analysis in Long-form Conversations', 'authors': 'Yuxuan Zhang, Yulong Li, Zichen Yu, Feilong Tang, Zhixiang Lu, Chong Li, Kang Dang, Jionglong Su', 'link': 'https://arxiv.org/abs/2501.00778', 'abstract': 'Long-sequence causal reasoning seeks to uncover causal relationships within extended time series data but is hindered by complex dependencies and the challenges of validating causal links. To address the limitations of large-scale language models (e.g., GPT-4) in capturing intricate emotional causality within extended dialogues, we propose CauseMotion, a long-sequence emotional causal reasoning framework grounded in Retrieval-Augmented Generation (RAG) and multimodal fusion. Unlike conventional methods relying only on textual information, CauseMotion enriches semantic representations by incorporating audio-derived features-vocal emotion, emotional intensity, and speech rate-into textual modalities. By integrating RAG with a sliding window mechanism, it effectively retrieves and leverages contextually relevant dialogue segments, thus enabling the inference of complex emotional causal chains spanning multiple conversational turns. To evaluate its effectiveness, we constructed the first benchmark dataset dedicated to long-sequence emotional causal reasoning, featuring dialogues with over 70 turns. Experimental results demonstrate that the proposed RAG-based multimodal integrated approach, the efficacy of substantially enhances both the depth of emotional understanding and the causal inference capabilities of large-scale language models. A GLM-4 integrated with CauseMotion achieves an 8.7% improvement in causal accuracy over the original model and surpasses GPT-4o by 1.2%. Additionally, on the publicly available DiaASQ dataset, CauseMotion-GLM-4 achieves state-of-the-art results in accuracy, F1 score, and causal reasoning accuracy.', 'abstract_zh': '长序列因果推理旨在揭示扩展时间序列数据中的因果关系，但由于复杂的依赖关系和验证因果链接的挑战而受到限制。为了克服大型语言模型（如GPT-4）在捕捉扩展对话中复杂情感因果关系方面的局限性，我们提出了CauseMotion，这是一种基于检索增强生成（RAG）和多模态融合的长序列情感因果推理框架。与仅依赖文本信息的传统方法不同，CauseMotion通过将音频提取的特征（即声情、情感强度和语速）融入文本模态中，丰富了语义表示。通过将RAG与滑动窗口机制结合，它有效地检索和利用上下文相关的话语片段，从而能够推断跨越多个对话回合的复杂情感因果链。为了评估其有效性，我们构建了首个专注于长序列情感因果推理的基准数据集，其中包含超过70个回合的对话。实验结果表明，基于RAG的多模态集成方法显著增强了大型语言模型的情感理解深度和因果推理能力。集成CauseMotion的GLM-4在因果准确性上比原模型提升了8.7%，并且超越了GPT-4o 1.2%。此外，在公开可用的DiaASQ数据集上，CauseMotion-GLM-4在准确率、F1分数和因果推理准确性方面达到了最先进的性能。', 'title_zh': '解码流变：CauseMotion在长篇对话中情感因果关系分析中的应用'}
{'arxiv_id': 'arXiv:2501.00747', 'title': 'DIVE: Diversified Iterative Self-Improvement', 'authors': 'Yiwei Qin, Yixiu Liu, Pengfei Liu', 'link': 'https://arxiv.org/abs/2501.00747', 'abstract': "Recent advances in large language models (LLMs) have demonstrated the effectiveness of Iterative Self-Improvement (ISI) techniques. However, continuous training on self-generated data leads to reduced output diversity, a limitation particularly critical in reasoning tasks where diverse solution paths are essential. We present DIVE (Diversified Iterative Self-Improvement), a novel framework that addresses this challenge through two key components: Sample Pool Expansion for broader solution exploration, and Data Selection for balancing diversity and quality in preference pairs. Experiments on MATH and GSM8k datasets show that DIVE achieves a 10% to 45% relative increase in output diversity metrics while maintaining performance quality compared to vanilla ISI. Our ablation studies confirm both components' significance in achieving these improvements. Code is available at this https URL.", 'abstract_zh': '近年来，大规模语言模型（LLMs）的最新进展证明了迭代自我改进（Iterative Self-Improvement, ISI）技术的有效性。然而，持续使用自动生成的数据进行训练会导致输出多样性降低，这一限制在需要探索多种解题路径的推理任务中尤为关键。我们提出了一种新的框架DIVE（Diversified Iterative Self-Improvement），通过两个关键组件来解决这一挑战：样本池扩展以进行更广泛的解决方案探索，以及数据选择以在偏好配对中平衡多样性和质量。在MATH和GSM8k数据集上的实验表明，DIVE在保持性能质量的同时，使输出多样性的指标提高了10%到45%。我们的消融研究证实了这两个组件在实现这些改进中的重要性。代码可供参考，地址为：[这个链接](https://)。', 'title_zh': 'DIVE：多样化迭代自我提升'}
{'arxiv_id': 'arXiv:2501.00745', 'title': 'Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines', 'authors': 'Xiyang Hu', 'link': 'https://arxiv.org/abs/2501.00745', 'abstract': "The increasing integration of Large Language Model (LLM) based search engines has transformed the landscape of information retrieval. However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors. In this paper, we study the dynamics of ranking manipulation attacks. We frame this problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players strategically decide whether to cooperate or attack. We analyze the conditions under which cooperation can be sustained, identifying key factors such as attack costs, discount rates, attack success rates, and trigger strategies that influence player behavior. We identify tipping points in the system dynamics, demonstrating that cooperation is more likely to be sustained when players are forward-looking. However, from a defense perspective, we find that simply reducing attack success probabilities can, paradoxically, incentivize attacks under certain conditions. Furthermore, defensive measures to cap the upper bound of attack success rates may prove futile in some scenarios. These insights highlight the complexity of securing LLM-based systems. Our work provides a theoretical foundation and practical insights for understanding and mitigating their vulnerabilities, while emphasizing the importance of adaptive security strategies and thoughtful ecosystem design.", 'abstract_zh': '基于大语言模型（LLM）的搜索引擎集成日益增加，已经极大地改变了信息检索的格局。然而，这些系统容易遭受恶意攻击，尤其是排名操控攻击，攻击者可以通过精心构建网页内容来操控LLM的排名并推广特定内容，从而在竞争中获得不公平的优势。本文研究了排名操控攻击的动力学。我们将这个问题框架化为无限重复的囚徒困境，多个参与者战略性地决定是合作还是攻击。我们分析了确保合作能够持续的条件，识别了包括攻击成本、贴现率、攻击成功率和触发策略在内的关键因素，这些因素会影响玩家的行为。我们确定了系统动力学中的临界点，表明当玩家具有前瞻性时，合作更有可能持续。然而，从防御角度来看，我们发现简单降低攻击成功率在某些条件下反而可能激励攻击。此外，在某些情况下，限制攻击成功率的上限的防御措施可能无效。这些洞察突显了确保基于LLM系统的复杂性。我们的研究为理解并减轻其脆弱性提供了理论基础和实践见解，并强调了适应性安全策略和精心设计生态系统的重要性。', 'title_zh': '基于大型语言模型的搜索引擎中的对抗攻击动态研究'}
{'arxiv_id': 'arXiv:2501.00697', 'title': 'PANDA -- Paired Anti-hate Narratives Dataset from Asia: Using an LLM-as-a-Judge to Create the First Chinese Counterspeech Dataset', 'authors': 'Michael Bennie, Demi Zhang, Bushi Xiao, Jing Cao, Chryseis Xinyi Liu, Jian Meng, Alayo Tripp', 'link': 'https://arxiv.org/abs/2501.00697', 'abstract': 'Despite the global prevalence of Modern Standard Chinese language, counterspeech (CS) resources for Chinese remain virtually nonexistent. To address this gap in East Asian counterspeech research we introduce the a corpus of Modern Standard Mandarin counterspeech that focuses on combating hate speech in Mainland China. This paper proposes a novel approach of generating CS by using an LLM-as-a-Judge, simulated annealing, LLMs zero-shot CN generation and a round-robin algorithm. This is followed by manual verification for quality and contextual relevance. This paper details the methodology for creating effective counterspeech in Chinese and other non-Eurocentric languages, including unique cultural patterns of which groups are maligned and linguistic patterns in what kinds of discourse markers are programmatically marked as hate speech (HS). Analysis of the generated corpora, we provide strong evidence for the lack of open-source, properly labeled Chinese hate speech data and the limitations of using an LLM-as-Judge to score possible answers in Chinese. Moreover, the present corpus serves as the first East Asian language based CS corpus and provides an essential resource for future research on counterspeech generation and evaluation.', 'abstract_zh': '尽管现代标准汉语在全球范围内普遍存在，但汉语的对抗性言论（Counterspeech, CS）资源几乎不存在。为弥补东亚地区对抗性言论研究的空白，我们引入了一项针对中国大陆地区恶意言论的现代标准普通话CS语料库。本文提出了一种创新的方法，通过使用“语言模型作为法官”（LLM-as-a-Judge）、模拟退火算法、零样本汉语生成以及轮换算法来生成CS。在此之后，进行人工验证以确保质量和上下文相关性。本文详细说明了如何在汉语及其他非欧罗巴中心语言中创建有效的CS，包括不同的文化模式，哪些群体受到诋毁，以及哪种类型的语用标记在编程过程中被标记为恶意言论（Harassment Statements, HS）。通过分析生成的语料库，本文提供了有力证据，表明公开来源、正确标注的汉语恶意言论数据缺乏，并且仅使用“语言模型作为法官”来评分汉语潜在答案存在局限性。此外，本文提供的语料库是首个基于东亚语言的CS语料库，为今后的CS生成和评估研究提供了宝贵资源。', 'title_zh': 'PANDA——源自亚洲的配对反 Hate 叙述数据集：使用大语言模型作为裁判创建首个中文反仇恨言论数据集'}
{'arxiv_id': 'arXiv:2501.00691', 'title': "Labels Generated by Large Language Model Helps Measuring People's Empathy in Vitro", 'authors': 'Md Rakibul Hasan, Yue Yao, Md Zakir Hossain, Aneesh Krishna, Imre Rudas, Shafin Rahman, Tom Gedeon', 'link': 'https://arxiv.org/abs/2501.00691', 'abstract': 'Large language models (LLMs) have revolutionised numerous fields, with LLM-as-a-service (LLMSaaS) having a strong generalisation ability that offers accessible solutions directly without the need for costly training. In contrast to the widely studied prompt engineering for task solving directly (in vivo), this paper explores its potential in in-vitro applications. These involve using LLM to generate labels to help the supervised training of mainstream models by (1) noisy label correction and (2) training data augmentation with LLM-generated labels. In this paper, we evaluate this approach in the emerging field of empathy computing -- automating the prediction of psychological questionnaire outcomes from inputs like text sequences. Specifically, crowdsourced datasets in this domain often suffer from noisy labels that misrepresent underlying empathy. By leveraging LLM-generated labels to train pre-trained language models (PLMs) like RoBERTa, we achieve statistically significant accuracy improvements over baselines, achieving a state-of-the-art Pearson correlation coefficient of 0.648 on NewsEmp benchmarks. In addition, we bring insightful discussions, including current challenges in empathy computing, data biases in training data and evaluation metric selection. Code and LLM-generated data are available at this https URL (available once the paper is accepted).', 'abstract_zh': '大型语言模型（LLMs）已经革新了众多领域，LLM即服务（LLMSaaS）因其强大的泛化能力，能够提供无需昂贵训练成本的直接可访问解决方案。与广泛研究的任务直接解决方法（in vivo）的提示工程不同，本文探讨了其在体外（in-vitro）应用中的潜在价值。这些应用包括使用LLM生成标签以辅助主流模型的监督训练，具体表现在（1）嘈杂标签修正和（2）通过LLM生成的标签进行训练数据增强。在本文中，我们评估了该方法在新兴领域的情感计算中的应用——从如文本序列等输入自动预测心理问卷结果。具体而言，领域内的众包数据集常常存在难以代表真实同理心的嘈杂标签。通过利用LLM生成的标签训练像RoBERTa等预训练语言模型（PLMs），我们实现了显著的准确性改进，在NewsEmp基准测试中达到了0.648的最高皮尔逊相关系数。此外，我们还进行了深刻的讨论，包括情感计算领域的现有挑战、训练数据中的数据偏差以及评估指标的选择。相关代码和LLM生成的数据将在论文被接受后公布，请访问以下网址：[这个网址]。', 'title_zh': '大型语言模型生成的标签有助于体外测量人们的情绪共感能力'}
{'arxiv_id': 'arXiv:2501.00562', 'title': 'An Overview and Discussion on Using Large Language Models for Implementation Generation of Solutions to Open-Ended Problems', 'authors': 'Hashmath Shaik, Alex Doboli', 'link': 'https://arxiv.org/abs/2501.00562', 'abstract': 'Large Language Models offer new opportunities to devise automated implementation generation methods that can tackle problem solving activities beyond traditional methods, which require algorithmic specifications and can use only static domain knowledge, like performance metrics and libraries of basic building blocks. Large Language Models could support creating new methods to support problem solving activities for open-ended problems, like problem framing, exploring possible solving approaches, feature elaboration and combination, more advanced implementation assessment, and handling unexpected situations. This report summarized the current work on Large Language Models, including model prompting, Reinforcement Learning, and Retrieval-Augmented Generation. Future research requirements were also discussed.', 'abstract_zh': '大型语言模型为设计自动化实现生成方法提供了新的机遇，这些方法可以解决传统方法无法处理的问题，而传统方法需要算法规范且只能利用静态领域知识，例如性能指标和基本构建块的库。大型语言模型可以支持创建新的方法来解决开放型问题，例如问题框架、探索可能的解决方法、特征细化和组合、更高级的实现评估以及处理意外情况。本报告总结了当前大型语言模型的相关工作，包括模型提示、强化学习和检索增强生成技术，并讨论了未来的研究需求。', 'title_zh': '大型语言模型在解决开放式问题解决方案生成中的应用综述与讨论'}
{'arxiv_id': 'arXiv:2501.00560', 'title': 'Re-evaluating Automatic LLM System Ranking for Alignment with Human Preference', 'authors': 'Mingqi Gao, Yixin Liu, Xinyu Hu, Xiaojun Wan, Jonathan Bragg, Arman Cohan', 'link': 'https://arxiv.org/abs/2501.00560', 'abstract': "Evaluating and ranking the capabilities of different LLMs is crucial for understanding their performance and alignment with human preferences. Due to the high cost and time-consuming nature of human evaluations, an automatic LLM bencher (i.e., an automatic evaluation framework that aims to rank LLMs based on their alignment with human preferences) is indispensable. An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system). However, previous work has not thoroughly explored how to select these components or how their different combinations influence the results. In this work, through controlled experiments, we provide a series of recommendations on how to choose each component to better automate the evaluation of LLMs. Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work. Lastly, we found that the evaluation models' performance at the instance level (e.g., the accuracy of selecting the best output) does not always align with their effectiveness when used as a component of a bencher, highlighting the importance of dedicated system-level evaluation of benchers.", 'abstract_zh': '评估和排名不同大型语言模型（LLM）的能力对于理解它们的性能和与人类偏好的一致性至关重要。由于人工评估成本高且耗时，因此需要一种自动的LLM评判器（即基于人类偏好排名LLM的自动评估框架）。一个自动LLM评判器包括四个组成部分：输入集（例如，用户指令）、评估模型（例如，LLM）、评估类型（例如，成对比较）以及聚合方法（例如，ELO评价系统）。然而，以往的研究并未详细探讨如何选择这些组成部分，或不同组合如何影响结果。在本研究中，通过控制实验，我们提供了一系列如何选择每个组成部分以更好地自动化LLM评估的建议。此外，我们发现，在评估具有相似性能的LLM时，自动LLM评判器的表现急剧下降，突显了当前评判器的局限性，并呼吁未来的研究工作。最后，我们发现，在实例层次上（例如，选择最佳输出的准确性）评估模型的表现并不总与其用作评判器组件时的有效性一致，强调需要专门进行系统层面的评判器评估的重要性。', 'title_zh': '重新评估自动大型语言模型系统排名与人类偏好的一致性'}
{'arxiv_id': 'arXiv:2501.00430', 'title': 'Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and Reflection agents', 'authors': 'Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, Huimin Ma', 'link': 'https://arxiv.org/abs/2501.00430', 'abstract': 'Agents have demonstrated their potential in scientific reasoning tasks through large language models. However, they often face challenges such as insufficient accuracy and degeneration of thought when handling complex reasoning tasks, which impede their performance. To overcome these issues, we propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing the reasoning capabilities of LLMs. Our approach improves scientific reasoning accuracy by employing a multi-path reasoning mechanism where each path consists of a reactive agent and a reflection agent that collaborate to prevent degeneration of thought inherent in single-agent reliance. Additionally, the RR-MP framework does not require additional training; it utilizes multiple dialogue instances for each reasoning path and a separate summarizer to consolidate insights from all paths. This design integrates diverse perspectives and strengthens reasoning across each path. We conducted zero-shot and few-shot evaluations on tasks involving moral scenarios, college-level physics, and mathematics. Experimental results demonstrate that our method outperforms baseline approaches, highlighting the effectiveness and advantages of the RR-MP framework in managing complex scientific reasoning tasks.', 'abstract_zh': '大型语言模型已经在科学推理任务中展示了其潜力，但当处理复杂推理任务时，代理常常面临准确性不足和思维退化等挑战，这限制了它们的表现。为了解决这些问题，我们提出了一种反应性和反思性多路径推理框架（RR-MP框架），旨在增强大模型的推理能力。我们的方法通过采用多路径推理机制来提高科学推理的准确性，其中每条路径由一个反应性代理和一个反思性代理组成，二者协作以防止单代理依赖性所固有的思维退化。此外，RR-MP框架不需要额外的训练，它利用每条推理路径中的多个对话实例以及一个单独的总结器来整合所有路径所获得的洞见。这种设计整合了多角度的观点，并增强了每条路径的推理能力。我们在涉及道德场景、大学物理和数学的任务上进行了零样本和少样本评估。实验结果表明，我们的方法优于基线方法，突显了RR-MP框架在管理复杂科学推理任务方面的有效性和优势。', 'title_zh': '使用多路径协作反应与反思代理增强生成式预训练语言模型的推理能力'}
{'arxiv_id': 'arXiv:2501.00353', 'title': 'RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions', 'authors': 'Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, Benyou Wang', 'link': 'https://arxiv.org/abs/2501.00353', 'abstract': "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for enhancing large language models (LLMs) by incorporating external knowledge. However, current RAG methods face two limitations: (1) they only cover limited RAG scenarios. (2) They suffer from limited task diversity due to the lack of a general RAG dataset. To address these limitations, we propose RAG-Instruct, a general method for synthesizing diverse and high-quality RAG instruction data based on any source corpus. Our approach leverages (1) five RAG paradigms, which encompass diverse query-document relationships, and (2) instruction simulation, which enhances instruction diversity and quality by utilizing the strengths of existing instruction datasets. Using this method, we construct a 40K instruction dataset from Wikipedia, comprehensively covering diverse RAG scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively enhances LLMs' RAG capabilities, achieving strong zero-shot performance and significantly outperforming various RAG baselines across a diverse set of tasks. RAG-Instruct is publicly available at this https URL.", 'abstract_zh': '检索增强生成（RAG）已经成为通过引入外部知识来增强大语言模型（LLMs）的关键范式。然而，现有的RAG方法面临两个局限：（1）它们仅涵盖有限的RAG场景。（2）由于缺乏通用的RAG数据集，导致任务多样性有限。为了克服这些局限，我们提出了RAG-Instruct，这是一种基于任意来源语料库合成多样且高质量RAG指令数据的一般方法。我们的方法利用了以下两点：（1）五种RAG范式，涵盖了多样化的查询-文档关系；（2）指令模拟，通过利用现有指令数据集的长处来增强指令的多样性和质量。利用这种方法，我们从Wikipedia中构建了一个包含40,000条指令的数据集，全面覆盖了多样化的RAG场景和任务。实验表明，RAG-Instruct有效地增强了LLMs的RAG能力，实现了强大的零样本性能，并且在一系列任务上显著优于各种RAG基线方法。RAG-Instruct已在以下网址公开提供：[这里](this https URL)。', 'title_zh': 'RAG-Instruct：通过多样化检索增强指令提升大型语言模型'}
{'arxiv_id': 'arXiv:2501.00343', 'title': 'Chunk-Distilled Language Modeling', 'authors': 'Yanhong Li, Karen Livescu, Jiawei Zhou', 'link': 'https://arxiv.org/abs/2501.00343', 'abstract': "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream tasks. Code and data will be made publicly available.", 'abstract_zh': '我们将介绍片段提取语言模型（CD-LM），这是一种解决当前大型语言模型（LLMs）中的两个挑战的方法： TOKEN 级别生成的低效率，以及适应新数据和知识的难度。我们的方法结合了基于深层网络的大型语言模型和一个简单的检索模块，该模块允许在单个解码步骤中生成多TOKEN 的文本片段。我们的检索框架允许灵活构建模型特定或领域特定的数据存储，无论是利用现有模型的内部知识，还是整合来自标注语料库的人类专家意见。这种可适应性使得对语言模型的分布进行增强控制成为可能，而无需额外训练。我们将展示 CD-LM 的建模框架，并提供性能指标来证明其在多种下游任务中提高语言模型性能和效率的能力。代码和数据将公开提供。', 'title_zh': '句块蒸馏语言建模'}
{'arxiv_id': 'arXiv:2501.00274', 'title': 'LLM-Rubric: A Multidimensional, Calibrated Approach to Automated Evaluation of Natural Language Texts', 'authors': 'Helia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, Chris Kedzie', 'link': 'https://arxiv.org/abs/2501.00274', 'abstract': "This paper introduces a framework for the automated evaluation of natural language texts. A manually constructed rubric describes how to assess multiple dimensions of interest. To evaluate a text, a large language model (LLM) is prompted with each rubric question and produces a distribution over potential responses. The LLM predictions often fail to agree well with human judges -- indeed, the humans do not fully agree with one another. However, the multiple LLM distributions can be $\\textit{combined}$ to $\\textit{predict}$ each human judge's annotations on all questions, including a summary question that assesses overall quality or relevance. LLM-Rubric accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters. When evaluating dialogue systems in a human-AI information-seeking task, we find that LLM-Rubric with 9 questions (assessing dimensions such as naturalness, conciseness, and citation quality) predicts human judges' assessment of overall user satisfaction, on a scale of 1--4, with RMS error $< 0.5$, a $2\\times$ improvement over the uncalibrated baseline.", 'abstract_zh': '本文介绍了一个自然语言文本自动化评估的框架。一个手工构建的评价标准描述了如何评估多个感兴趣维度的方法。为了评估一篇文本，大型语言模型（LLM）被每次被提示每个评价标准的问题，并生成潜在回应的概率分布。LLM的预测往往不能很好地与人类评委的评判匹配——实际上，人类评委之间也并不完全一致。然而，多个LLM的分布可以通过某种方式**结合**，用于**预测**每位人类评委在所有问题上的标注，包括一个评估整体质量和相关性的总结问题。通过训练一个包含评委特定参数和通用参数的小型前馈神经网络，LLM-Rubric实现了这一目标。在一项评估对话系统的实验中，当任务涉及人类与AI的信息寻求对话时，使用包含9个问题（评估自然性、简洁性和引文质量等维度）的LLM-Rubric，可以预测人类评委对整体用户满意度的评价（采用1到4的评分标准），均方根误差小于0.5，相较于未校准的基线模型，性能提高了两倍。', 'title_zh': 'LLM-评分标准：一种多维度、校准化的自然语言文本自动化评估方法'}
{'arxiv_id': 'arXiv:2501.00269', 'title': 'A review of faithfulness metrics for hallucination assessment in Large Language Models', 'authors': 'Ben Malin, Tatiana Kalganova, Nikoloas Boulgouris', 'link': 'https://arxiv.org/abs/2501.00269', 'abstract': 'This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.', 'abstract_zh': '本文回顾了在开放式摘要、问答和机器翻译任务中对忠实性进行评估的方法。我们发现，使用大规模语言模型（LLM）作为忠实性评估工具通常是与人类判断最高度相关的指标。文中还讨论了其他研究如何减轻幻觉的方法，包括检索增强生成（RAG）和提示框架方法已被证明能提高忠实性，同时提供了其他减轻幻觉的建议。对忠实性的研究对于LLM的持续广泛应用至关重要，因为不忠实的回答可能会在许多原本适合应用LLM的领域带来重大风险。此外，评估开放式生成提供了比常用的多项选择基准测试更为全面的LLM性能衡量标准，有助于增强对LLM的信任度。', 'title_zh': '大型语言模型中幻觉评估忠实度度量综述'}
{'arxiv_id': 'arXiv:2501.00257', 'title': 'EQUATOR: A Deterministic Framework for Evaluating LLM Reasoning with Open-Ended Questions. # v1.0.0-beta', 'authors': 'Raymond Bernard, Shaina Raza, Subhabrata Das, Rahul Murugan', 'link': 'https://arxiv.org/abs/2501.00257', 'abstract': "Despite the remarkable coherence of Large Language Models (LLMs), existing evaluation methods often suffer from fluency bias and rely heavily on multiple-choice formats, making it difficult to assess factual accuracy and complex reasoning effectively. LLMs thus frequently generate factually inaccurate responses, especially in complex reasoning tasks, highlighting two prominent challenges: (1) the inadequacy of existing methods to evaluate reasoning and factual accuracy effectively, and (2) the reliance on human evaluators for nuanced judgment, as illustrated by Williams and Huckle (2024)[1], who found manual grading indispensable despite automated grading advancements.\nTo address evaluation gaps in open-ended reasoning tasks, we introduce the EQUATOR Evaluator (Evaluation of Question Answering Thoroughness in Open-ended Reasoning). This framework combines deterministic scoring with a focus on factual accuracy and robust reasoning assessment. Using a vector database, EQUATOR pairs open-ended questions with human-evaluated answers, enabling more precise and scalable evaluations. In practice, EQUATOR significantly reduces reliance on human evaluators for scoring and improves scalability compared to Williams and Huckle's (2004)[1] methods.\nOur results demonstrate that this framework significantly outperforms traditional multiple-choice evaluations while maintaining high accuracy standards. Additionally, we introduce an automated evaluation process leveraging smaller, locally hosted LLMs. We used LLaMA 3.2B, running on the Ollama binaries to streamline our assessments. This work establishes a new paradigm for evaluating LLM performance, emphasizing factual accuracy and reasoning ability, and provides a robust methodological foundation for future research.", 'abstract_zh': '尽管大型语言模型（LLMs）具有显著的连贯性，现有的评估方法常常存在流畅性偏见，并且高度依赖多项选择格式，这使得评估事实正确性和复杂推理能力变得困难。因此，LLMs 经常生成事实不准确的回答，尤其是在复杂推理任务中，这突显了两个主要挑战：（1）现有方法评估推理和事实正确性能力的不足，以及（2）对依靠人类评估者的细致判断的依赖性，正如 Williams 和 Huckle（2024）[1] 所指出的，即便在自动化评分技术取得进展的情况下，人工评分依然是必不可少的。\n\n为了填补开放性推理任务评估中的空白，我们引入了 EQUATOR 评估器（开放性推理中问题回答全面性的评估）。该框架结合了确定性评分，并着重于事实正确性和稳健推理的评估。通过使用向量数据库，EQUATOR 将开放式问题与人类评估的回答相关联，从而实现更精确和可扩展的评估。实际上，EQUATOR 显著减少了评分对人类评估者的依赖，并在可扩展性方面优于 Williams 和 Huckle（2004）[1] 的方法。\n\n我们的结果显示，该框架显著优于传统的多项选择评估方法，同时保持了高准确度标准。此外，我们还引入了一种利用本地部署的小型语言模型的自动化评估过程。我们使用了 LLaMA 3.2B，运行在 Ollama 二进制文件上，以简化我们的评估过程。这项工作为评估 LLM 性能确立了新的范式，强调了事实正确性和推理能力，并为未来的研究提供了坚实的方法论基础。', 'title_zh': 'EQUATOR：一种评估大模型在解答开放性问题时推理能力的确定性框架。# v1.0.0-beta'}
{'arxiv_id': 'arXiv:2501.00244', 'title': 'Have We Designed Generalizable Structural Knowledge Promptings? Systematic Evaluation and Rethinking', 'authors': 'Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Shaokai Chen, Mengshu Sun, Binbin Hu, Zhiqiang Zhang, Lei Liang, Wen Zhang, Huajun Chen', 'link': 'https://arxiv.org/abs/2501.00244', 'abstract': 'Large language models (LLMs) have demonstrated exceptional performance in text generation within current NLP research. However, the lack of factual accuracy is still a dark cloud hanging over the LLM skyscraper. Structural knowledge prompting (SKP) is a prominent paradigm to integrate external knowledge into LLMs by incorporating structural representations, achieving state-of-the-art results in many knowledge-intensive tasks. However, existing methods often focus on specific problems, lacking a comprehensive exploration of the generalization and capability boundaries of SKP. This paper aims to evaluate and rethink the generalization capability of the SKP paradigm from four perspectives including Granularity, Transferability, Scalability, and Universality. To provide a thorough evaluation, we introduce a novel multi-granular, multi-level benchmark called SUBARU, consisting of 9 different tasks with varying levels of granularity and difficulty.', 'abstract_zh': '大型语言模型（LLMs）在当前自然语言处理（NLP）研究中展示了卓越的文本生成性能。然而，事实准确性的缺乏仍然是悬在LLMs头顶的乌云。结构化知识提示（SKP）是一种将外部知识整合到LLMs中的显著范式，通过融合结构化表示，已在许多知识密集型任务上取得了最先进的成果。然而，现有方法往往专注于特定问题，缺乏对SKP的一般化能力和边界进行全面探索。本文旨在从粒度、可迁移性、可扩展性和通用性四个维度评估和重新思考SKP范式的推广能力。为了进行全面评估，我们引入了一个新颖的多粒度、多层次基准SUBARU，该基准由9种不同粒度和难度的任务组成。', 'title_zh': '我们设计的结构化知识提示具有普适性吗？系统评估与重新思考'}
{'arxiv_id': 'arXiv:2501.00233', 'title': 'Zero-Shot Strategies for Length-Controllable Summarization', 'authors': 'Fabian Retkowski, Alexander Waibel', 'link': 'https://arxiv.org/abs/2501.00233', 'abstract': "Large language models (LLMs) struggle with precise length control, particularly in zero-shot settings. We conduct a comprehensive study evaluating LLMs' length control capabilities across multiple measures and propose practical methods to improve controllability. Our experiments with LLaMA 3 reveal stark differences in length adherence across measures and highlight inherent biases of the model. To address these challenges, we introduce a set of methods: length approximation, target adjustment, sample filtering, and automated revisions. By combining these methods, we demonstrate substantial improvements in length compliance while maintaining or enhancing summary quality, providing highly effective zero-shot strategies for precise length control without the need for model fine-tuning or architectural changes. With our work, we not only advance our understanding of LLM behavior in controlled text generation but also pave the way for more reliable and adaptable summarization systems in real-world applications.", 'abstract_zh': '大型语言模型（LLMs）在精确长度控制方面存在挑战，特别是在零样本设置中。我们进行了一项全面的研究，评估LLMs在多个指标上的长度控制能力，并提出了一些实际方法来提高可控性。我们使用LaMA 3进行的实验揭示了在不同指标下长度遵守情况的巨大差异，并突显了该模型的固有偏差。为了应对这些挑战，我们介绍了一套方法：长度近似、目标调整、样本过滤和自动化修正。通过结合这些方法，我们展示了在保持或提升摘要质量的同时显著提高长度遵守度，为无需模型微调或架构更改即可实现精确长度控制提供了有效的零样本策略。通过我们的研究工作，我们不仅深化了对LLM在受控文本生成中行为的理解，还为在实际应用中提供更可靠和适应性强的摘要系统铺平了道路。', 'title_zh': '长度可控的零样本总结策略'}
{'arxiv_id': 'arXiv:2501.00224', 'title': 'Extracting effective solutions hidden in large language models via generated comprehensive specialists: case studies in developing electronic devices', 'authors': 'Hikari Tomita, Nobuhiro Nakamura, Shoichi Ishida, Toshio Kamiya, Kei Terayama', 'link': 'https://arxiv.org/abs/2501.00224', 'abstract': "Recently, many studies have increasingly explored the use of large language models (LLMs) to generate research ideas and scientific hypotheses. However, real-world research and development often require solving complex, interdisciplinary challenges where solutions may not be readily found through existing knowledge related to the problem. Therefore, it is desirable to leverage the vast, comprehensive knowledge of LLMs to generate effective, breakthrough solutions by integrating various perspectives from other disciplines. Here, we propose SELLM (Solution Enumeration via comprehensive List and LLM), a framework leveraging LLMs and structured guidance using MECE (Mutually Exclusive, Collectively Exhaustive) principles, such as International Patent Classification (IPC) and the periodic table of elements. SELLM systematically constructs comprehensive expert agents from the list to generate cross-disciplinary and effective solutions. To evaluate SELLM's practicality, we applied it to two challenges: improving light extraction in organic light-emitting diode (OLED) lighting and developing electrodes for next-generation memory materials. The results demonstrate that SELLM significantly facilitates the generation of effective solutions compared to cases without specific customization or effort, showcasing the potential of SELLM to enable LLMs to generate effective solutions even for challenging problems.", 'abstract_zh': '近年来，许多研究越来越多地探讨了使用大型语言模型（LLMs）来生成研究想法和科学假设。然而，现实世界的研究与发展往往需要解决复杂的、跨学科的挑战，而这些问题的解决方案可能无法通过现有与问题相关的知识轻易找到。因此，有必要充分利用LLMs广泛而综合的知识，通过整合其他学科的各种视角来生成有效且突破性的解决方案。在此，我们提出了SELLM（基于MECE原则的综合列表和LLM解决方案列举框架），该框架利用LLMs和MECE（互斥且详尽）原则进行结构化的指导，如国际专利分类（IPC）和元素周期表。SELLM系统地从列表中构建综合专家代理，以生成跨学科且有效的解决方案。为了评估SELLM的实际应用性，我们将其应用于两个挑战：改善有机发光二极管（OLED）照明中的光提取和开发下一代存储材料的电极。结果表明，与没有特定定制或努力的情况相比，SELLM显著促进了有效解决方案的生成，展示了SELLM能够使LLMs为棘手问题生成有效解决方案的潜力。', 'title_zh': '通过生成综合专家提取隐藏在大规模语言模型中的有效解决方案：电子产品开发中的案例研究'}
{'arxiv_id': 'arXiv:2501.00208', 'title': 'An Empirical Evaluation of Large Language Models on Consumer Health Questions', 'authors': 'Moaiz Abrar, Yusuf Sermet, Ibrahim Demir', 'link': 'https://arxiv.org/abs/2501.00208', 'abstract': "This study evaluates the performance of several Large Language Models (LLMs) on MedRedQA, a dataset of consumer-based medical questions and answers by verified experts extracted from the AskDocs subreddit. While LLMs have shown proficiency in clinical question answering (QA) benchmarks, their effectiveness on real-world, consumer-based, medical questions remains less understood. MedRedQA presents unique challenges, such as informal language and the need for precise responses suited to non-specialist queries. To assess model performance, responses were generated using five LLMs: GPT-4o mini, Llama 3.1: 70B, Mistral-123B, Mistral-7B, and Gemini-Flash. A cross-evaluation method was used, where each model evaluated its responses as well as those of others to minimize bias. The results indicated that GPT-4o mini achieved the highest alignment with expert responses according to four out of the five models' judges, while Mistral-7B scored lowest according to three out of five models' judges. This study highlights the potential and limitations of current LLMs for consumer health medical question answering, indicating avenues for further development.", 'abstract_zh': '本研究评估了多种大型语言模型（LLM）在MedRedQA数据集上的表现，该数据集包含从AskDocs子版块提取的由经过验证的专家解答的消费导向型医学问题和答案。尽管LLM已经在临床问答基准测试中展示了其能力，但它们在真实世界、消费导向型医学问题上的效果尚不完全清楚。MedRedQA数据集提出了独特的挑战，包括非正式的语言和需要针对非专科查询给出精确的回答。为了评估模型性能，使用了五种LLM生成了响应：GPT-4o mini、Llama 3.1: 70B、Mistral-123B、Mistral-7B 和 Gemini-Flash。采用了一种交叉评估方法，即每种模型不仅评估自己的响应，还评估其他模型的响应，以减少偏差。结果显示，GPT-4o mini 根据四款模型的评判员得到了与专家响应最高的一致性评价，而Mistral-7B则根据三款模型的评判员得到了最低的评价。本研究突显了当前LLM在消费健康医学问答方面的潜力和局限性，并指出了进一步发展的途径。', 'title_zh': '大型语言模型在消费者健康问题上的实证评估'}
{'arxiv_id': 'arXiv:2501.00199', 'title': 'GPT-4 on Clinic Depression Assessment: An LLM-Based Pilot Study', 'authors': 'Giuliano Lorenzoni, Pedro Elkind Velmovitsky, Paulo Alencar, Donald Cowan', 'link': 'https://arxiv.org/abs/2501.00199', 'abstract': "Depression has impacted millions of people worldwide and has become one of the most prevalent mental disorders. Early mental disorder detection can lead to cost savings for public health agencies and avoid the onset of other major comorbidities. Additionally, the shortage of specialized personnel is a critical issue because clinical depression diagnosis is highly dependent on expert professionals and is time consuming.\nIn this study, we explore the use of GPT-4 for clinical depression assessment based on transcript analysis. We examine the model's ability to classify patient interviews into binary categories: depressed and not depressed. A comparative analysis is conducted considering prompt complexity (e.g., using both simple and complex prompts) as well as varied temperature settings to assess the impact of prompt complexity and randomness on the model's performance.\nResults indicate that GPT-4 exhibits considerable variability in accuracy and F1-Score across configurations, with optimal performance observed at lower temperature values (0.0-0.2) for complex prompts. However, beyond a certain threshold (temperature >= 0.3), the relationship between randomness and performance becomes unpredictable, diminishing the gains from prompt complexity.\nThese findings suggest that, while GPT-4 shows promise for clinical assessment, the configuration of the prompts and model parameters requires careful calibration to ensure consistent results. This preliminary study contributes to understanding the dynamics between prompt engineering and large language models, offering insights for future development of AI-powered tools in clinical settings.", 'abstract_zh': '抑郁症影响了全世界数百万人口，并已成为最常见的精神障碍之一。早期的精神障碍检测可以为公共卫生机构节省成本，并避免其他严重共病的出现。此外，专业人员的短缺也是一个关键问题，因为临床抑郁诊断高度依赖于专业人员，并且耗时较长。\n\n在本研究中，我们探讨了使用GPT-4进行临床抑郁评估的方法，基于对话转录分析。我们考察了模型将患者访谈分类为两类（抑郁和非抑郁）的能力。我们进行了比较分析，考虑了提示复杂性（例如，使用简单和复杂的提示）以及不同的温度设置，以评估提示复杂性与随机性对模型性能的影响。\n\n结果表明，在不同配置下，GPT-4在准确性与F1分数方面表现出较大的波动性，对于复杂提示，最优性能出现在较低的温度值（0.0-0.2）。然而，当温度超过一定阈值（温度>=0.3）时，随机性和性能之间的关系变得不可预测，提示复杂性带来的收益开始减弱。\n\n这些发现表明，尽管GPT-4在临床评估方面显示出潜力，但提示和模型参数的配置需要仔细调整以确保结果的一致性。本初步研究为理解提提示工程与大规模语言模型之间的动态关系提供了见解，为未来临床环境下AI驱动工具的发展提供了参考。\n\n（注：GPT-4作为最新的模型，原文可能使用GPT-4或其他相关模型名称，这里根据上下文翻译为GPT-4。）', 'title_zh': '基于语言模型的初步研究：GPT-4在临床抑郁评估中的应用'}
{'arxiv_id': 'arXiv:2501.00164', 'title': 'Measuring Large Language Models Capacity to Annotate Journalistic Sourcing', 'authors': 'Subramaniam Vincent, Phoebe Wang, Zhan Shi, Sahas Koka, Yi Fang', 'link': 'https://arxiv.org/abs/2501.00164', 'abstract': 'Since the launch of ChatGPT in late 2022, the capacities of Large Language Models and their evaluation have been in constant discussion and evaluation both in academic research and in the industry. Scenarios and benchmarks have been developed in several areas such as law, medicine and math (Bommasani et al., 2023) and there is continuous evaluation of model variants. One area that has not received sufficient scenario development attention is journalism, and in particular journalistic sourcing and ethics. Journalism is a crucial truth-determination function in democracy (Vincent, 2023), and sourcing is a crucial pillar to all original journalistic output. Evaluating the capacities of LLMs to annotate stories for the different signals of sourcing and how reporters justify them is a crucial scenario that warrants a benchmark approach. It offers potential to build automated systems to contrast more transparent and ethically rigorous forms of journalism with everyday fare. In this paper we lay out a scenario to evaluate LLM performance on identifying and annotating sourcing in news stories on a five-category schema inspired from journalism studies (Gans, 2004). We offer the use case, our dataset and metrics and as the first step towards systematic benchmarking. Our accuracy findings indicate LLM-based approaches have more catching to do in identifying all the sourced statements in a story, and equally, in matching the type of sources. An even harder task is spotting source justifications.', 'abstract_zh': '自2022年底ChatGPT发布以来，大型语言模型及其评估能力一直在学术界和工业界持续讨论和评估。已在法律、医学和数学等多个领域开发了情景和基准测试（Bommasani等，2023），并且不断对模型变体进行评估。然而，尚未在新闻业，尤其是新闻来源和伦理方面获得足够的情景发展关注。新闻业在民主制度中是决定事实的关键功能（Vincent，2023），而来源是所有原创新闻输出的关键支柱。评估大型语言模型在识别和注释新闻故事中不同来源信号的能力及其记者如何证明这些信号的能力是至关重要的一个情景，需要采用基准测试方法。这对于构建自动化系统，以更透明和伦理严谨的方式来对比日常新闻内容具有潜在价值。本文旨在提出一个情景，利用来自新闻学研究的五类架构（Gans，2004）评估大型语言模型在识别和注释新闻故事来源方面的性能。我们提供了应用场景、数据集和评估指标，并迈出了系统基准测试的第一步。我们的准确度结果显示，基于大型语言模型的方法在识别故事中所有来源声明方面还有待改进，同样在匹配不同类型的来源方面亦有不足。更困难的任务在于识别来源证明。', 'title_zh': '测量大型语言模型在标注新闻来源方面的能力'}
{'arxiv_id': 'arXiv:2501.00152', 'title': 'Temporal reasoning for timeline summarisation in social media', 'authors': 'Jiayu Song, Mahmud Akhter, Dana Atzil Slonim, Maria Liakata', 'link': 'https://arxiv.org/abs/2501.00152', 'abstract': 'This paper explores whether enhancing temporal reasoning capabilities in Large Language Models (LLMs) can improve the quality of timeline summarization, the task of summarising long texts containing sequences of events, particularly social media threads . We introduce \\textit{NarrativeReason}, a novel dataset focused on temporal relationships among sequential events within narratives, distinguishing it from existing temporal reasoning datasets that primarily address pair-wise event relationships. Our approach then combines temporal reasoning with timeline summarization through a knowledge distillation framework, where we first fine-tune a teacher model on temporal reasoning tasks and then distill this knowledge into a student model while simultaneously training it for the task of timeline summarization. Experimental results demonstrate that our model achieves superior performance on mental health-related timeline summarization tasks, which involve long social media threads with repetitions of events and a mix of emotions, highlighting the importance of leveraging temporal reasoning to improve timeline summarisation.', 'abstract_zh': '本文探讨了在大型语言模型（LLMs）中增强时间推理能力是否能够提高事件序列（尤其是社交媒体帖文）长文本总结的质量。我们引入了《NarrativeReason》，这是一个专注于叙事中序列事件之间时间关系的新型数据集，与现有主要关注事件对之间关系的时间推理数据集不同。我们的方法通过知识蒸馏框架将时间推理与时间线总结结合起来，首先对教师模型进行针对时间推理任务的微调，然后将这些知识传送给学生模型，同时训练学生模型进行时间线总结任务。实验结果表明，我们的模型在涉及情感混合和事件重复的心理健康相关的长社交媒体帖文总结任务中表现更佳，强调了利用时间推理提高时间线总结质量的重要性。', 'title_zh': '社交媒体中的时间线总结中的时间推理'}
{'arxiv_id': 'arXiv:2501.00059', 'title': 'Large Language Models for Mathematical Analysis', 'authors': 'Ziye Chen, Hao Qi', 'link': 'https://arxiv.org/abs/2501.00059', 'abstract': "Mathematical problem-solving is a key field in artificial intelligence (AI) and a critical benchmark for evaluating the capabilities of large language models (LLMs). While extensive research has focused on mathematical problem-solving, most existing work and datasets concentrate on computational tasks, leaving gaps in areas like mathematical analysis, which demands rigorous proofs and formal reasoning. We developed the DEMI-MathAnalysis dataset, comprising proof-based problems from mathematical analysis topics such as Sequences and Limits, Infinite Series, and Convex Functions. We also designed a guiding framework to rigorously enhance LLMs' ability to solve these problems. Through fine-tuning LLMs on this dataset and employing our framework, we observed significant improvements in their capability to generate logical, complete, and elegant proofs. This work addresses critical gaps in mathematical reasoning and contributes to advancing trustworthy AI capable of handling formalized mathematical language. The code is publicly accessible at LLMs for Mathematical Analysis.", 'abstract_zh': '数学问题求解是人工智能（AI）中的一个关键领域，也是评估大型语言模型（LLMs）能力的重要基准。尽管大量研究集中在数学问题求解上，但现有工作和数据集大多集中在计算任务上，而在需要严谨证明和形式推理的数学分析领域存在空白。为此，我们开发了DEMI-MathAnalysis数据集，该数据集包含源自数学分析主题的问题，如数列和极限、无穷级数以及凸函数的基于证明的问题。我们还设计了一套指导框架，以严谨地提升LLMs解决这些问题的能力。通过在该数据集上对LLMs进行微调并使用我们的框架，我们观察到它们生成逻辑严密、完整且精美的证明的能力有了显著提升。这项工作填补了数学推理中的关键空白，并有助于推进能够处理形式化数学语言的可信AI的发展。相关代码已在LLMs for Mathematical Analysis中公开。', 'title_zh': '大规模语言模型在数学分析中的应用'}
{'arxiv_id': 'arXiv:2501.00031', 'title': 'Distilling Large Language Models for Efficient Clinical Information Extraction', 'authors': 'Karthik S. Vedula, Annika Gupta, Akshay Swaminathan, Ivan Lopez, Suhana Bedi, Nigam H. Shah', 'link': 'https://arxiv.org/abs/2501.00031', 'abstract': 'Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential solution. We evaluate the performance of distilled BERT models, which are approximately 1,000 times smaller than modern LLMs, for clinical named entity recognition (NER) tasks. We leveraged state-of-the-art LLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as teacher labelers for medication, disease, and symptom extraction. We applied our approach to over 3,300 clinical notes spanning five publicly available datasets, comparing distilled BERT models against both their teacher labelers and BERT models fine-tuned on human labels. External validation was conducted using clinical notes from the MedAlign dataset. For disease extraction, F1 scores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and 0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model), 0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of 0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had faster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash respectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and Gemini Flash respectively). On the external validation dataset, the distilled BERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699 (symptom). Distilled BERT models were up to 101x cheaper and 12x faster than state-of-the-art LLMs while achieving similar performance on NER tasks. Distillation offers a computationally efficient and scalable alternative to large LLMs for clinical information extraction.', 'abstract_zh': '大规模语言模型（LLMs）在临床信息提取方面表现出色，但其计算需求限制了其实用部署。知识蒸馏——即将大模型的知识转移到小模型的过程——提供了一种潜在的解决方案。我们评估了蒸馏后的BERT模型在临床命名实体识别（NER）任务中的性能，这些模型大约比现代LLMs小1,000倍。我们利用最先进的LLMs（Gemini和OpenAI模型）和医学本体（RxNorm和SNOMED）作为教师标签器，对药物、疾病和症状进行提取。我们将这种方法应用于超过3,300份临床笔记，并跨越五个公开可用的数据集，将蒸馏后的BERT模型与教师标签器和基于人类标签微调的BERT模型进行了对比。外部验证是在MedAlign数据集的临床笔记中进行的。对于疾病提取，F1分数分别为0.82（教师模型）、0.89（BioBERT基于人类标签训练）和0.84（蒸馏后的BioBERT）。对于药物提取，F1分数分别为0.84（教师模型）、0.91（BioBERT基于人类标签训练）和0.87（蒸馏后的BioBERT）。对于症状提取，F1分数分别为0.73（教师模型）和0.68（蒸馏后的BioBERT）。蒸馏后的BERT模型具有更快的推理速度（分别快于GPT-4o、o1-mini和Gemini Flash的12倍、4倍和8倍），并具有更低的成本（分别比GPT-4o、o1-mini和Gemini Flash便宜85倍、101倍和2倍）。在外部分区验证数据集中，蒸馏后的BERT模型的F1分数分别为0.883（药物）、0.726（疾病）和0.699（症状）。蒸馏后的BERT模型在成本和推理速度上分别比最先进的LLMs便宜101倍和12倍，同时在命名实体识别任务上实现了相似的性能。知识蒸馏为临床信息提取提供了计算有效且可扩展的替代方案，与大规模LLMs相比。', 'title_zh': '高效临床信息提取的大语言模型精简方法'}
{'arxiv_id': 'arXiv:2501.01329', 'title': 'The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation', 'authors': 'Shuzheng Gao, Chaozheng Wang, Cuiyun Gao, Xiaoqian Jiao, Chun Yong Chong, Shan Gao, Michael Lyu', 'link': 'https://arxiv.org/abs/2501.01329', 'abstract': "Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.", 'abstract_zh': '测试用例对于验证软件应用的可靠性和质量至关重要。最近的研究表明，大型语言模型（LLMs）能够生成适用于给定源代码的有用测试用例。然而，现有的工作主要依赖于人工编写的简单提示，这往往会导致结果不尽如人意，因为LLMs的表现会受到提示的影响。此外，这些方法使用相同的提示来处理所有LLMs，忽视了不同LLMs可能最适合不同的提示这一事实。由于可能存在的提示形式种类繁多，自动发现每个LLMs的最佳提示是一个重大挑战。尽管在自然语言处理领域存在一些自动提示优化的方法，但它们对于生成测试用例的任务来说往往无效。首先，这些方法通过简单地组合和变异现有的提示来逐次优化提示，缺乏适当的引导，导致生成的提示缺乏多样性，并且倾向于在生成的测试用例中重复相同的错误。其次，这些提示通常缺乏领域上下文知识，限制了LLMs在该任务中的表现。', 'title_zh': '《Prompt 魔法师：面向测试案例生成的自动化大型语言模型自适应提示优化》'}
{'arxiv_id': 'arXiv:2501.01205', 'title': 'Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects', 'authors': 'Abdullah Mushtaq, Muhammad Rafay Naeem, Ibrahim Ghaznavi, Muhammad Imran Taj, Imran Hashmi, Junaid Qadir', 'link': 'https://arxiv.org/abs/2501.01205', 'abstract': 'Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...', 'abstract_zh': '多智能体大型语言模型（Multi-Agent Large Language Models, MALLMs）因其在复杂问题解决、决策制定和规划任务中的能力而获得了广泛关注，这种能力与群体智慧的概念相契合。群体智慧概念指的是，各种不同的个体共同努力，产生有效的解决方案，这特别适用于教育环境。在工程教育中，高级设计项目（Senior Design Projects，也称为毕业设计或毕业综合项目）非常重要，因为它们将理论知识与实践应用相结合，培养批判性思维、团队合作以及解决实际问题的能力。在本文中，我们探讨了在进行这些通常涉及多学科考虑和冲突目标（如优化技术性能同时解决伦理、社会和环境问题）的高级设计项目时，如何利用多智能体大型语言模型进行支持。我们提出了一种框架，其中不同的LLM代理代表不同的专家视角，如问题表述代理、系统复杂性代理、社会与伦理代理或项目经理，从而促进整体的问题解决方法。该实现利用了多智能体系统（Multi-Agent System, MAS）的基本概念，如协作、合作和谈判，并通过提示工程开发出各种各样的代理个性。这些代理通过富有成效的协作对话来模拟人类工程团队，受群智AI原则的指导，能够高效地平衡个体贡献以形成统一的解决方案。我们采用这些技术来创建LLM代理之间的协作框架，促进跨学科的推理和谈判，这类似于现实世界中的高级设计项目。为了评估该框架的有效性，我们收集了六个来自工程和计算机科学领域的...', 'title_zh': '利用多代理大型语言模型解决复杂工程问题：高级设计项目框架'}
{'arxiv_id': 'arXiv:2501.00805', 'title': 'SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation', 'authors': 'Haitian Lu, Gaofeng Cheng, Liuping Luo, Leying Zhang, Yanmin Qian, Pengyuan Zhang', 'link': 'https://arxiv.org/abs/2501.00805', 'abstract': 'Recently, ``textless" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.', 'abstract_zh': '近年来，“无文本”语音语言模型（SLMs）基于语音单元在生成自然语音方面取得了巨大进展，包括非言语声化。然而，生成的语音样本往往缺乏语义连贯性。在本文中，我们提出了一种将SLMs和LLMs集成用于自发性口语对话生成的方法（SLIDE）。具体而言，我们首先利用一个LLM生成口语对话的内容。接下来，我们将文本对话转换为音素序列，并使用基于Transformer的双塔模型预测每个音素的持续时间。最后，一个基于口语音素序列的SLM用于语音化文本对话。在Fisher数据集上的实验结果表明，我们的系统可以生成既自然又保持高语义连贯性的口语对话。', 'title_zh': 'SLIDE：将语音语言模型与大规模语言模型集成以生成自发口语对话'}
{'arxiv_id': 'arXiv:2501.00684', 'title': 'IGC: Integrating a Gated Calculator into an LLM to Solve Arithmetic Tasks Reliably and Efficiently', 'authors': 'Florian Dietz, Dietrich Klakow', 'link': 'https://arxiv.org/abs/2501.00684', 'abstract': 'Solving arithmetic tasks is a simple and fundamental skill, yet modern Large Language Models (LLMs) have great difficulty with them. We introduce the Integrated Gated Calculator (IGC), a module that enables LLMs to perform arithmetic by emulating a calculator on the GPU. We finetune a Llama model with our module and test it on the BigBench Arithmetic benchmark, where it beats the State of the Art, outperforming all models on the benchmark, including models almost two orders of magnitude larger. Our approach takes only a single iteration to run and requires no external tools. It performs arithmetic operations entirely inside the LLM without the need to produce intermediate tokens. It is computationally efficient, interpretable, and avoids side-effects on tasks that do not require arithmetic operations. It reliably achieves 98\\% to 99\\% accuracy across multiple training runs and for all subtasks, including the substantially harder subtask of multiplication, which was previously unsolved.', 'abstract_zh': '解决算术任务是一项简单而基本的技能，然而现代大型语言模型（LLMs）在处理这类任务时却面临巨大困难。我们引入了综合门控计算器（IGC），这是一种模块，使LLMs能够通过在GPU上模拟计算器的方式执行算术计算。我们使用该模块对Llama模型进行了微调，并在BigBench Arithmetic基准测试中进行测试，结果显示该模型比现有最佳方法表现更优，即使在基准测试中，它也超过了所有其他模型，包括规模大了近两个数量级的模型。我们的方法只需要单次迭代即可运行，并不需要外部工具。该方法完全在LLM内部进行算术操作，无需生成中间词元。这种方法计算效率高、可解释性强，对于不需要进行算术操作的任务不会产生副作用。多次训练运行和所有子任务中，该方法都能可靠地达到98%到99%的准确性，甚至包括先前未被解决的乘法子任务。', 'title_zh': 'IGC：将门控计算器集成到大规模语言模型中，以可靠且高效地完成算术任务'}
{'arxiv_id': 'arXiv:2501.00539', 'title': 'MCP-Solver: Integrating Language Models with Constraint Programming Systems', 'authors': 'Stefan Szeider', 'link': 'https://arxiv.org/abs/2501.00539', 'abstract': "While Large Language Models (LLMs) perform exceptionally well at natural language tasks, they often struggle with precise formal reasoning and the rigorous specification of problems. We present MCP-Solver, a prototype implementation of the Model Context Protocol that demonstrates the potential for systematic integration between LLMs and constraint programming systems. Our implementation provides interfaces for the creation, editing, and validation of a constraint model. Through an item-based editing approach with integrated validation, the system ensures model consistency at every modification step and enables structured iterative refinement. The system handles concurrent solving sessions and maintains a persistent knowledge base of modeling insights. Initial experiments suggest that this integration can effectively combine LLMs' natural language understanding with constraint-solving capabilities. Our open-source implementation is proof of concept for integrating formal reasoning systems with LLMs through standardized protocols. While further research is needed to establish comprehensive formal guarantees, this work takes a first step toward principled integration of natural language processing with constraint-based reasoning.", 'abstract_zh': '尽管大规模语言模型（LLMs）在自然语言任务上表现出色，但它们往往在精确的形式推理和问题的严格规范上存在困难。我们提出了一种模型上下文协议（MCP-Solver）的原型实现，展示了LLMs与约束编程系统系统化集成的潜力。我们的实现提供了创建、编辑和验证约束模型的接口。通过基于项目的编辑方法并集成验证，系统在每次修改步骤中确保模型一致性，并支持结构化的迭代细化。系统能够处理并发求解会话，并保持一个持久的知识库，记录建模洞察。初步实验表明，这种集成能够有效结合LLMs的自然语言理解和求解约束问题的能力。我们开源的实现证明了通过标准化协议集成正式推理系统与LLMs的可能性。尽管还需要进一步的研究来建立全面的形式保证，但这项工作为自然语言处理与基于约束的推理的原理化集成迈出了第一步。', 'title_zh': 'MCP-Solver：将语言模型与约束编程系统集成'}
{'arxiv_id': 'arXiv:2501.00226', 'title': 'Generative Emergent Communication: Large Language Model is a Collective World Model', 'authors': 'Tadahiro Taniguchi, Ryo Ueda, Tomoaki Nakamura, Masahiro Suzuki, Akira Taniguchi', 'link': 'https://arxiv.org/abs/2501.00226', 'abstract': "This study proposes a unifying theoretical framework called generative emergent communication (generative EmCom) that bridges emergent communication, world models, and large language models (LLMs) through the lens of collective predictive coding (CPC). The proposed framework formalizes the emergence of language and symbol systems through decentralized Bayesian inference across multiple agents, extending beyond conventional discriminative model-based approaches to emergent communication. This study makes the following two key contributions: First, we propose generative EmCom as a novel framework for understanding emergent communication, demonstrating how communication emergence in multi-agent reinforcement learning (MARL) can be derived from control as inference while clarifying its relationship to conventional discriminative approaches. Second, we propose a mathematical formulation showing the interpretation of LLMs as collective world models that integrate multiple agents' experiences through CPC. The framework provides a unified theoretical foundation for understanding how shared symbol systems emerge through collective predictive coding processes, bridging individual cognitive development and societal language evolution. Through mathematical formulations and discussion on prior works, we demonstrate how this framework explains fundamental aspects of language emergence and offers practical insights for understanding LLMs and developing sophisticated AI systems for improving human-AI interaction and multi-agent systems.", 'abstract_zh': '本文提出了一种统一的理论框架，称为生成性涌现通信（Generative Emergent Communication，简称Generative EmCom），该框架通过集体预测编码（Collective Predictive Coding, CPC）的视角，将涌现通信、世界模型和大型语言模型（LLMs）三者联系起来。该提出的框架通过多个代理的分布式贝叶斯推理形式化了语言和符号系统的涌现，超越了传统的判别模型方法，应用于涌现通信领域。本文做出了以下两个关键贡献：首先，我们提出生成性EmCom作为一种新的框架，用于理解涌现通信，展示了多智能体强化学习（MARL）中的通信涌现如何从控制即推理的角度推导出来，并阐明其与传统判别性方法的关系。其次，我们提供了一个数学形式化表达，将LLMs视为通过CPC整合多个代理经验的集体世界模型。该框架为理解共享符号系统如何通过集体预测编码过程涌现提供了一个统一的理论基础，将个体认知发展与社会语言进化联系起来。通过数学形式化表达和对先前工作的讨论，我们展示了该框架如何解释语言涌现的基本方面，并为理解LLMs和开发提高人机交互和多智能体系统复杂AI系统提供了实用见解。', 'title_zh': '生成性 emergent 通信：大型语言模型是一个集体世界模型'}
{'arxiv_id': 'arXiv:2501.00055', 'title': 'LLM-Virus: Evolutionary Jailbreak Attack on Large Language Models', 'authors': 'Miao Yu, Junfeng Fang, Yingjie Zhou, Xing Fan, Kun Wang, Shirui Pan, Qingsong Wen', 'link': 'https://arxiv.org/abs/2501.00055', 'abstract': 'While safety-aligned large language models (LLMs) are increasingly used as the cornerstone for powerful systems such as multi-agent frameworks to solve complex real-world problems, they still suffer from potential adversarial queries, such as jailbreak attacks, which attempt to induce harmful content. Researching attack methods allows us to better understand the limitations of LLM and make trade-offs between helpfulness and safety. However, existing jailbreak attacks are primarily based on opaque optimization techniques (e.g. token-level gradient descent) and heuristic search methods like LLM refinement, which fall short in terms of transparency, transferability, and computational cost. In light of these limitations, we draw inspiration from the evolution and infection processes of biological viruses and propose LLM-Virus, a jailbreak attack method based on evolutionary algorithm, termed evolutionary jailbreak. LLM-Virus treats jailbreak attacks as both an evolutionary and transfer learning problem, utilizing LLMs as heuristic evolutionary operators to ensure high attack efficiency, transferability, and low time cost. Our experimental results on multiple safety benchmarks show that LLM-Virus achieves competitive or even superior performance compared to existing attack methods.', 'abstract_zh': '尽管安全对齐的大语言模型（LLMs）在诸如多智能体框架等强大系统中被越来越广泛地用作基础支柱，以解决复杂的现实世界问题，但它们仍然面临潜在的对抗查询，例如脱逃攻击（jailbreak attacks），这些攻击试图诱导有害内容。研究攻击方法有助于我们更好地了解LLM的局限性，并在帮助性和安全性之间做出权衡。然而，现有的脱逃攻击主要基于不透明的优化技术（例如，标记级梯度下降）和启发式搜索方法（如LLM优化），这些方法在透明性、转移性和计算成本方面存在不足。鉴于这些局限性，我们从生物病毒的进化和感染过程中汲取灵感，提出了一种基于进化算法的脱逃攻击方法，称为进化脱逃（evolutionary jailbreak）。LLM-Virus将脱逃攻击视为一种进化和迁移学习问题，利用LLM作为启发式的进化操作符，以确保高攻击效率、良好的转移性和低时间成本。我们在多个安全基准上的实验结果表明，LLM-Virus在性能上具有竞争力，甚至优于现有的攻击方法。', 'title_zh': 'LLM-Virus：大型语言模型的演化型突破攻击'}
{'arxiv_id': 'arXiv:2501.00039', 'title': 'Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning', 'authors': 'Chirag Nagpal, Subhashini Venugopalan, Jimmy Tobin, Marilyn Ladewig, Katherine Heller, Katrin Tomanek', 'link': 'https://arxiv.org/abs/2501.00039', 'abstract': "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.", 'abstract_zh': '我们介绍了一种大型语言模型（LLM），该模型能够处理语音输入，并表明通过强化学习（RLHF）进一步调整该模型使其能够更好地适应乱序语音，而传统的微调效果则较差。我们的方法用音频令牌替换LLM词汇表中低频的文字令牌，从而使模型能够在带有转录的语音数据上进行微调，进而识别语音。随后，我们使用基于语法和语义准确性的奖励进行RL，进一步推广LLM以识别乱序语音。尽管最终得到的LLM在语音识别方面并未胜过现有的系统，但我们发现使用自定义奖励进行强化学习的微调，与监督下的语言模型微调相比，其性能显著提升，尤其是在不同场景下的语音适应方面。这为使用大型语言模型进行语音识别提供了一种有吸引力的替代调优策略。', 'title_zh': '使用强化学习适应非规范语音的大型语言模型的语音识别'}
{'arxiv_id': 'arXiv:2501.00223', 'title': 'CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care', 'authors': 'Michael Gubanov, Anna Pyayt, Aleksandra Karolak', 'link': 'https://arxiv.org/abs/2501.00223', 'abstract': 'Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user.', 'abstract_zh': '在这里，我们描述了一个早期的Web规模混合知识图谱（KG）-大型语言模型（LLM）系统，该系统包含了最新的同行评审的结直肠癌医学知识。目前，该系统正在莫菲特癌症中心进行评估，以辅助医学研究和临床信息检索任务。莫菲特癌症中心是美国乃至世界领先的癌症中心之一。我们的混合系统与众不同，因为它能够更好地满足用户需求，而不仅仅是单一的LLM、KG或搜索引擎。现成的LLM已知存在幻觉和灾难性遗忘问题，并且训练数据可能已过时。当今最先进的KG，如PrimeKG、cBioPortal、ChEMBL、NCBI等，也需要人工整理，因此会很快变得过时。CancerKG 是一个无需监督的系统，能够自动吸收和组织最新的医学发现。为了缓解LLM的缺点，经过验证的KG充当了检索增强生成（RAG）的护栏。CancerKG 展示了5种不同的高级用户界面，每种界面都针对不同的数据模态进行了优化，以提供更方便的用户体验。', 'title_zh': 'CancerKG.ORG：一个面向Web、交互式、可验证的知识图谱-大语言模型混合系统，用于辅助优化癌症治疗和护理'}
{'arxiv_id': 'arXiv:2501.01335', 'title': 'CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for Benchmarking Large Language Models', 'authors': 'Johan Wahréus, Ahmed Mohamed Hussain, Panos Papadimitratos', 'link': 'https://arxiv.org/abs/2501.01335', 'abstract': 'Numerous studies have investigated methods for jailbreaking Large Language Models (LLMs) to generate harmful content. Typically, these methods are evaluated using datasets of malicious prompts designed to bypass security policies established by LLM providers. However, the generally broad scope and open-ended nature of existing datasets can complicate the assessment of jailbreaking effectiveness, particularly in specific domains, notably cybersecurity. To address this issue, we present and publicly release CySecBench, a comprehensive dataset containing 12662 prompts specifically designed to evaluate jailbreaking techniques in the cybersecurity domain. The dataset is organized into 10 distinct attack-type categories, featuring close-ended prompts to enable a more consistent and accurate assessment of jailbreaking attempts. Furthermore, we detail our methodology for dataset generation and filtration, which can be adapted to create similar datasets in other domains. To demonstrate the utility of CySecBench, we propose and evaluate a jailbreaking approach based on prompt obfuscation. Our experimental results show that this method successfully elicits harmful content from commercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT and 88% with Gemini; in contrast, Claude demonstrated greater resilience with a jailbreaking SR of 17%. Compared to existing benchmark approaches, our method shows superior performance, highlighting the value of domain-specific evaluation datasets for assessing LLM security measures. Moreover, when evaluated using prompts from a widely used dataset (i.e., AdvBench), it achieved an SR of 78.5%, higher than the state-of-the-art methods.', 'abstract_zh': 'Numerous 研究已经探讨了破解大型语言模型 (LLMs) 以生成有害内容的方法。通常，这些方法是通过使用设计来绕过LLM提供商制定的安全政策的恶意提示数据集进行评估的。然而，现有数据集通常范围广泛且开放式的特点，可能会在特定领域（尤其在网络安全领域）评估破解有效性方面造成复杂性。为解决这一问题，我们提出了并公开发布了CySecBench，这是一个包含12,662个专门设计用于评估网络安全领域破解技术的有效性的提示数据集。数据集被组织成10个不同的攻击类别，采用封闭式提示，以实现更一致和准确的破解尝试评估。此外，我们详细描述了数据集生成和过滤的方法，这些方法可以适应其他领域的数据集创建。为了展示CySecBench的实用性，我们提出并评估了一种基于提示混淆的破解方法。实验结果表明，该方法成功从商业黑盒LLM中引出了有害内容，使用ChatGPT的成功率为65%，使用Gemini的成功率为88%；相比之下，Claude的破解成功率仅为17%，显示出更强的抗破解能力。与现有的基准方法相比，我们的方法表现出更优的性能，强调了针对特定领域的评估数据集对于评估LLM安全措施的价值。此外，在使用广泛使用的数据集（如AdvBench）进行评估时，其成功率达到了78.5%，超过了最先进的方法。', 'title_zh': 'CySecBench：基于生成AI的旨在网络安全的大语言模型基准测试提示数据集'}
{'arxiv_id': 'arXiv:2501.00906', 'title': 'Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things', 'authors': 'Talha Zeeshan, Abhishek Kumar, Susanna Pirttikangas, Sasu Tarkoma', 'link': 'https://arxiv.org/abs/2501.00906', 'abstract': "This paper presents the development and evaluation of a Large Language Model (LLM), also known as foundation models, based multi-agent system framework for complex event processing (CEP) with a focus on video query processing use cases. The primary goal is to create a proof-of-concept (POC) that integrates state-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub) tools to address the integration of LLMs with current CEP systems. Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows. Extensive experiments evaluate the system's performance across varying configurations, complexities, and video resolutions, revealing the trade-offs between functionality and latency. The results show that while higher agent count and video complexities increase latency, the system maintains high consistency in narrative coherence. This research builds upon and contributes to, existing novel approaches to distributed AI systems, offering detailed insights into integrating such systems into existing infrastructures.", 'abstract_zh': '本文介绍了一种基于大规模语言模型（LLM）或称为基础模型的多Agent系统框架，该框架旨在处理复杂事件处理（CEP）中的视频查询应用场景。主要目标是创建一个概念验证（Proof of Concept, POC），将最先进的LLM编排框架与发布/订阅（Pub/Sub）工具集成，以解决将LLM与当前CEP系统集成的问题。通过结合使用Autogen框架和Kafka消息代理，系统展示了能够处理复杂工作流程的自主CEP管道。进行了广泛实验，评估了系统在不同配置、复杂性和视频分辨率下的性能，揭示了功能性和延迟之间的权衡。结果表明，尽管更高的Agent数量和视频复杂性增加了延迟，但系统在叙事连贯性方面保持了高度一致性。该研究建立在并贡献于现有分布式AI系统的新型方法之上，提供了将此类系统集成到现有基础设施中的详细见解。', 'title_zh': '基于大型语言模型的多代理系统增强复杂事件处理管道在多媒体事物互联网中的应用'}
{'arxiv_id': 'arXiv:2501.00829', 'title': 'An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component Deep Learning Systems', 'authors': 'Haoxiang Tian, Xingshuo Han, Guoquan Wu, An Guo, Yuan Zhou. Jie Zhang, Shuo Li, Jun Wei, Tianwei Zhang', 'link': 'https://arxiv.org/abs/2501.00829', 'abstract': "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching optimal solutions in complex multi-component applications. Traditional MOEAs for multi-component deep learning (MCDL) systems face challenges in enhancing the search efficiency while maintaining the diversity. To combat these, this paper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search algorithm to detect safety violations in MCDL systems. Inspired by the context-understanding ability of Large Language Models (LLMs), $\\mu$MOEA promotes the LLM to comprehend the optimization problem and generate an initial population tailed to evolutionary objectives. Subsequently, it employs adaptive selection and variation to iteratively produce offspring, balancing the evolutionary efficiency and diversity. During the evolutionary process, to navigate away from the local optima, $\\mu$MOEA integrates the evolutionary experience back into the LLM. This utilization harnesses the LLM's quantitative reasoning prowess to generate differential seeds, breaking away from current optimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL systems, and compare its performance with state-of-the-art MOEA methods. Experimental results show that $\\mu$MOEA can significantly improve the efficiency and diversity of the evolutionary search.", 'abstract_zh': '多目标进化算法（MOEAs）被广泛应用于复杂多组件应用中搜索最优解决方案。传统的多组件深度学习（MCDL）系统的MOEAs在提高搜索效率的同时保持多样性方面面临挑战。为了应对这些挑战，本文提出了$\\mu$MOEA，这是一种受大型语言模型（LLMs）赋能的自适应演化搜索算法，用于检测MCDL系统的安全性违规行为。受大型语言模型上下文理解能力的启发，$\\mu$MOEA促使LLMs理解优化问题，并生成适应进化目标的初始种群。随后，该算法采用自适应选择和变异，逐步产生新的后代，平衡演化效率和多样性。在演化过程中，为了摆脱局部最优解，$\\mu$MOEA将演化经验反馈给LLMs。这一过程利用了LLMs的定量推理能力来生成差异性种子，从而打破现有的最优解。我们评估了$\\mu$MOEA在其在检测MCDL系统安全性违规行为中的表现，并将其性能与最先进的MOEA方法进行了比较。实验结果表明，$\\mu$MOEA显著提高了进化搜索的效率和多样性。', 'title_zh': '一种基于大规模语言模型的自适应进化算法用于多组件深度学习系统'}
{'arxiv_id': 'arXiv:2501.00826', 'title': 'LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management', 'authors': 'Yichen Luo, Yebo Feng, Jiahua Xu, Paolo Tasca, Yang Liu', 'link': 'https://arxiv.org/abs/2501.00826', 'abstract': 'Cryptocurrency investment is inherently difficult due to its shorter history compared to traditional assets, the need to integrate vast amounts of data from various modalities, and the requirement for complex reasoning. While deep learning approaches have been applied to address these challenges, their black-box nature raises concerns about trust and explainability. Recently, large language models (LLMs) have shown promise in financial applications due to their ability to understand multi-modal data and generate explainable decisions. However, single LLM faces limitations in complex, comprehensive tasks such as asset investment. These limitations are even more pronounced in cryptocurrency investment, where LLMs have less domain-specific knowledge in their training corpora.\nTo overcome these challenges, we propose an explainable, multi-modal, multi-agent framework for cryptocurrency investment. Our framework uses specialized agents that collaborate within and across teams to handle subtasks such as data analysis, literature integration, and investment decision-making for the top 30 cryptocurrencies by market capitalization. The expert training module fine-tunes agents using multi-modal historical data and professional investment literature, while the multi-agent investment module employs real-time data to make informed cryptocurrency investment decisions. Unique intrateam and interteam collaboration mechanisms enhance prediction accuracy by adjusting final predictions based on confidence levels within agent teams and facilitating information sharing between teams. Empirical evaluation using data from November 2023 to September 2024 demonstrates that our framework outperforms single-agent models and market benchmarks in classification, asset pricing, portfolio, and explainability performance.', 'abstract_zh': '与传统资产相比，数字货币的投资因其较短的历史、需要整合多种数据模态以及复杂推理解释的要求而固有地具有挑战性。尽管深度学习方法已被应用于解决这些挑战，但其黑箱特性引发了关于信任与解释性的担忧。最近，大规模语言模型（LLMs）在金融应用中显示出潜力，这得益于它们能够理解多模态数据并生成可解释的决策。然而，单一的大规模语言模型在处理复杂且综合性的任务，如资产投资时存在局限性。在加密货币投资中，由于训练语料库中的领域特定知识较少，这种局限性更为明显。\n\n为了克服这些挑战，我们提出了一种可用于加密货币投资的可解释、多模态、多智能体框架。该框架中使用专门的智能体，在团队内部及跨团队进行协作，以处理数据分析、文献整合和市值前30位的加密货币的投资决策等子任务。专家训练模块通过多模态历史数据和专业的投资文献对智能体进行微调，而多智能体投资模块则利用实时数据进行加密货币投资决策。团队内部和跨团队的协作机制通过调整智能体团队内的置信水平来提高预测准确性，并促进团队之间的信息共享。通过从2023年11月至2024年9月的数据进行实证评估，我们的框架在分类、资产定价、投资组合和解释性性能方面均优于单一智能体模型和市场基准。', 'title_zh': '基于LLM的强大多代理系统自动加密货币投资组合管理'}
{'arxiv_id': 'arXiv:2501.00555', 'title': 'Monty Hall and Optimized Conformal Prediction to Improve Decision-Making with LLMs', 'authors': 'Harit Vishwakarma, Alan Mishler, Thomas Cook, Niccolò Dalmasso, Natraj Raman, Sumitra Ganesh', 'link': 'https://arxiv.org/abs/2501.00555', 'abstract': "Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, they often make overconfident, incorrect predictions, which can be risky in high-stakes settings like healthcare and finance. To mitigate these risks, recent works have used conformal prediction (CP), a model-agnostic framework for distribution-free uncertainty quantification. CP transforms a \\emph{score function} into prediction sets that contain the true answer with high probability. While CP provides this coverage guarantee for arbitrary scores, the score quality significantly impacts prediction set sizes. Prior works have relied on LLM logits or other heuristic scores, lacking quality guarantees. We address this limitation by introducing CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Furthermore, inspired by the Monty Hall problem, we extend CP's utility beyond uncertainty quantification to improve accuracy. We propose \\emph{conformal revision of questions} (CROQ) to revise the problem by narrowing down the available choices to those in the prediction set. The coverage guarantee of CP ensures that the correct choice is in the revised question prompt with high probability, while the smaller number of choices increases the LLM's chances of answering it correctly. Experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with Gemma-2, Llama-3 and Phi-3 models show that CP-OPT significantly reduces set sizes while maintaining coverage, and CROQ improves accuracy over the standard inference, especially when paired with CP-OPT scores. Together, CP-OPT and CROQ offer a robust framework for improving both the safety and accuracy of LLM-driven decision-making.", 'abstract_zh': '大规模语言模型（LLMs）正在赋能多种应用中的决策过程，包括工具或API的使用以及回答多项选择题（MCQs）。然而，这些模型在高风险场合（如医疗和金融）中往往会做出过于自信且错误的预测，这存在一定的风险。为减轻这些风险，最近的研究采用了区间预测（CP，Conformal Prediction）这一不依赖特定模型的框架，以实现无分布假设下的不确定性量化。CP 把一个 **评分函数** 转化为包含正确答案的概率较高的预测集。尽管 CP 能为任意评分提供这一覆盖保证，但评分的质量显著影响预测集的大小。先前的研究依赖于语言模型的输出 logits 或其他启发式评分，但缺乏质量保证。我们通过引入 CP-OPT（一种优化框架，用于学习能够最小化预测集大小同时保持覆盖范围的评分函数），来解决这一局限性。此外，受蒙提霍尔问题的启发，我们将 CP 的适用范围扩展到超越不确定性量化领域，以提高预测的准确性。我们提出了 **区间预测修订问题**（CROQ, Conformal Revision of Questions），通过将问题选项限定在预测集中以缩小选择范围。CP 的覆盖保证确保修订后的提示中正确答案的概率较高，而选择范围的缩小增加了语言模型正确回答问题的机会。我们在 MMLU、ToolAlpaca 和 TruthfulQA 数据集上的实验显示，CP-OPT 显著减小了预测集的大小并保持了覆盖范围，而 CROQ 能在标准推理之上提高准确性，尤其是在与 CP-OPT 评分结合使用时。共同使用 CP-OPT 和 CROQ 能为 LLM 驱动的决策提供一个提升其安全性和准确性的强大框架。', 'title_zh': '蒙提霍尔问题与优化一致预测在改善基于LLM的决策制定中的应用'}
{'arxiv_id': 'arXiv:2501.00418', 'title': 'Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models', 'authors': 'Martin Pawelczyk, Lillian Sun, Zhenting Qi, Aounon Kumar, Himabindu Lakkaraju', 'link': 'https://arxiv.org/abs/2501.00418', 'abstract': "The rapid proliferation of generative AI, especially large language models, has led to their integration into a variety of applications. A key phenomenon known as weak-to-strong generalization - where a strong model trained on a weak model's outputs surpasses the weak model in task performance - has gained significant attention. Yet, whether critical trustworthiness properties such as robustness, fairness, and privacy can generalize similarly remains an open question. In this work, we study this question by examining if a stronger model can inherit trustworthiness properties when fine-tuned on a weaker model's outputs, a process we term weak-to-strong trustworthiness generalization. To address this, we introduce two foundational training strategies: 1) Weak Trustworthiness Finetuning (Weak TFT), which leverages trustworthiness regularization during the fine-tuning of the weak model, and 2) Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT), which extends regularization to both weak and strong models. Our experimental evaluation on real-world datasets reveals that while some trustworthiness properties, such as fairness, adversarial, and OOD robustness, show significant improvement in transfer when both models were regularized, others like privacy do not exhibit signs of weak-to-strong trustworthiness. As the first study to explore trustworthiness generalization via weak-to-strong generalization, our work provides valuable insights into the potential and limitations of weak-to-strong generalization.", 'abstract_zh': '生成式AI的迅速发展，特别是大型语言模型的应用日益广泛。一个被称为“弱到强泛化”的关键现象逐渐受到关注，即一个基于较弱模型输出训练的强大模型在任务性能上超越了较弱模型。然而，诸如鲁棒性、公平性和隐私等关键信任属性是否也能类似地泛化，仍然是一个开放的问题。在本文中，我们通过研究是否一个较强模型在基于较弱模型输出进行微调时能够继承这些信任属性来探讨这一问题，这一过程我们称之为弱到强的信任属性泛化。为了应对这一挑战，我们引入了两种基本的训练策略：1) 轻量化信任属性微调（弱TFT），该策略利用信任属性正则化来微调较弱模型；2) 轻量化及弱到强信任属性微调（弱+WTS TFT），该策略将正则化扩展到较弱和较强模型。我们在真实数据集上的实验评价表明，尽管一些信任属性，如公平性、对抗性和OOD鲁棒性，在两个模型均进行正则化的情况下表现出显著的转移改进，但其他属性如隐私并未显示出明显的弱到强信任属性泛化的迹象。作为首次探讨通过弱到强泛化来实现信任属性泛化的研究，本文为我们提供了关于弱到强泛化的潜力和局限性的宝贵见解。', 'title_zh': '泛化信任：语言模型中的弱可信性到强可信性'}
{'arxiv_id': 'arXiv:2501.00217', 'title': 'The Potential of LLMs in Automating Software Testing: From Generation to Reporting', 'authors': 'Betim Sherifi, Khaled Slhoub, Fitzroy Nembhard', 'link': 'https://arxiv.org/abs/2501.00217', 'abstract': "Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy.", 'abstract_zh': '高质量的软件在软件工程中至关重要，这需要在测试活动中采用强大的验证和验证流程。虽然人工测试效果显著，但其耗时和成本较高，因此对自动化方法的需求也在增加。近年来，大型语言模型（LLMs）的进步对软件工程领域产生了重大影响，特别是在需求分析、测试自动化和调试等方面。本文探索了一种面向代理的自动软件测试方法，利用LLMs减少人工干预并提高测试效率。所提出的框架结合使用LLMs生成单元测试、可视化调用图并自动化测试执行和报告。跨多个Python和Java应用程序的评估表明，该系统的测试覆盖率高且运行高效。这项研究强调了LLM驱动代理在简化软件测试工作流方面的作用，同时也应对扩展性和准确性方面的挑战。', 'title_zh': '大型语言模型在自动化软件测试中的潜力：从生成到报告'}
{'arxiv_id': 'arXiv:2501.00051', 'title': 'DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework', 'authors': 'Yu-Zheng Lin, Qinxuan Shi, Zhanglong Yang, Banafsheh Saber Latibari, Sicong Shao, Soheil Salehi, Pratik Satam', 'link': 'https://arxiv.org/abs/2501.00051', 'abstract': 'Digital twin (DT) technology has emerged as a transformative approach to simulate, predict, and optimize the behavior of physical systems, with applications that span manufacturing, healthcare, climate science, and more. However, the development of DT models often faces challenges such as high data requirements, integration complexity, and limited adaptability to dynamic changes in physical systems. This paper presents a new method inspired by dynamic data-driven applications systems (DDDAS), called the dynamic data-driven generative of digital twins framework (DDD-GenDT), which combines the physical system with LLM, allowing LLM to act as DT to interact with the physical system operating status and generate the corresponding physical behaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining process, and we use the spindle current measurement data in the NASA milling wear data set as an example to enable LLMs to forecast the physical behavior from historical data and interact with current observations. Experimental results show that in the zero-shot prediction setting, the LLM-based DT can adapt to the change in the system, and the average RMSE of the GPT-4 prediction is 0.479A, which is 4.79% of the maximum spindle motor current measurement of 10A, with little training data and instructions required. Furthermore, we analyze the performance of DDD-GenDT in this specific application and their potential to construct digital twins. We also discuss the limitations and challenges that may arise in practical implementations.', 'abstract_zh': '数字孪生（Digital Twin, DT）技术作为一种变革性的方法，用于模拟、预测和优化物理系统的行为，其应用领域涵盖了制造、医疗、气候变化等众多方面。然而，DT 模型的开发常常面临高数据需求、集成复杂性和对物理系统动态变化的适应性有限等挑战。本文提出了一种受动态数据驱动应用系统（Dynamic Data-Driven Applications Systems, DDDAS）启发的新方法，称为动态数据驱动生成数字孪生框架（Dynamic Data-Driven Generative of Digital Twins, DDD-GenDT），该方法将物理系统与大型语言模型（LLM）结合，使得 LLM 能够充当数字孪生，与物理系统运行状态互动并生成相应的物理行为。我们将 DDD-GenDT 应用于计算机数控（CNC）加工过程，并使用NASA铣削磨损数据集中主轴电流测量数据作为示例，使 LLM 能够从历史数据中预测物理行为并与当前观测互动。实验结果表明，在零样本预测设置下，基于LLM的数字孪生能够适应系统的变化，GPT-4 的预测平均绝对均方根误差（RMSE）为0.479A，占最大主轴电机电流测量值10A的4.79%，且所需训练数据和指令较少。此外，我们分析了该特定应用中DDD-GenDT的效果及其构建数字孪生的潜力，并讨论了实际实施中可能出现的限制和挑战。', 'title_zh': 'DDD-GenDT：动态数据驱动的生成型数字孪生框架'}
