# Finding A Voice: Evaluating African American Dialect Generation for Chatbot Technology 

**Title (ZH)**: 寻找声音：评估非裔美国人口音生成技术在聊天机器人中的应用 

**Authors**: Sarah E. Finch, Ellie S. Paek, Sejung Kwon, Ikseon Choi, Jessica Wells, Rasheeta Chandler, Jinho D. Choi  

**Link**: [PDF](https://arxiv.org/pdf/2501.03441)  

**Abstract**: As chatbots become increasingly integrated into everyday tasks, designing systems that accommodate diverse user populations is crucial for fostering trust, engagement, and inclusivity. This study investigates the ability of contemporary Large Language Models (LLMs) to generate African American Vernacular English (AAVE) and evaluates the impact of AAVE usage on user experiences in chatbot applications. We analyze the performance of three LLM families (Llama, GPT, and Claude) in producing AAVE-like utterances at varying dialect intensities and assess user preferences across multiple domains, including healthcare and education. Despite LLMs' proficiency in generating AAVE-like language, findings indicate that AAVE-speaking users prefer Standard American English (SAE) chatbots, with higher levels of AAVE correlating with lower ratings for a variety of characteristics, including chatbot trustworthiness and role appropriateness. These results highlight the complexities of creating inclusive AI systems and underscore the need for further exploration of diversity to enhance human-computer interactions. 

**Abstract (ZH)**: 随着聊天机器人的日常任务集成程度不断提高，设计能够适应多样用户群体的系统对于培养信任、增强参与度和促进包容性至关重要。本研究探讨了当前大型语言模型（LLMs）生成非标准英语（如非洲裔美国人方言英语AAVE）的能力，并评估AAVE在聊天机器人应用中的使用对其用户体验的影响。我们分析了三种LLM家族（Llama、GPT和Claude）在不同方言强度下生成AAVE样态表达的表现，并评估了用户在医疗保健和教育等多个领域的偏好。尽管LLMs在生成AAVE样态语言方面表现出了较高的能力，但研究结果表明，AAVE使用者更偏好标准美国英语(SAE)聊天机器人，且方言的使用程度越高，聊天机器人的可信度和角色适宜性等各方面评价越低。这些结果突显了创建包容性AI系统的复杂性，并强调了进一步探究多样性以增强人机互动的重要性。 

---
# Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks 

**Title (ZH)**: 基于在线强化学习的动态自适应评价函数实时策略任务研究 

**Authors**: Weilong Yang, Jie Zhang, Xunyun Liu, Yanqing Ye  

**Link**: [PDF](https://arxiv.org/pdf/2501.03824)  

**Abstract**: Effective evaluation of real-time strategy tasks requires adaptive mechanisms to cope with dynamic and unpredictable environments. This study proposes a method to improve evaluation functions for real-time responsiveness to battle-field situation changes, utilizing an online reinforcement learning-based dynam-ic weight adjustment mechanism within the real-time strategy game. Building on traditional static evaluation functions, the method employs gradient descent in online reinforcement learning to update weights dynamically, incorporating weight decay techniques to ensure stability. Additionally, the AdamW optimizer is integrated to adjust the learning rate and decay rate of online reinforcement learning in real time, further reducing the dependency on manual parameter tun-ing. Round-robin competition experiments demonstrate that this method signifi-cantly enhances the application effectiveness of the Lanchester combat model evaluation function, Simple evaluation function, and Simple Sqrt evaluation function in planning algorithms including IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notable improvement in scores, with the en-hancement becoming more pronounced as the map size increases. Furthermore, the increase in evaluation function computation time induced by this method is kept below 6% for all evaluation functions and planning algorithms. The pro-posed dynamic adaptive evaluation function demonstrates a promising approach for real-time strategy task evaluation. 

**Abstract (ZH)**: 实时策略任务的有效评估需要适应性机制以应对动态和不可预测的环境。本研究提出了一种方法，通过在实时策略游戏中利用基于在线强化学习的动态权重调整机制，以改进评价函数，提高对战场情况变化的实时响应能力。在传统静态评价函数的基础上，该方法利用在线强化学习中的梯度下降技术动态更新权重，并结合权重衰减技术确保稳定性。此外，该方法还整合了AdamW优化器来实时调整在线强化学习的学习率和衰减速率，进一步减少对外部人工参数调整的依赖。轮循对战实验表明，该方法显著提高了Lanchester战斗模型评价函数、Simple评价函数和Simple Sqrt评价函数在IDABCD、IDRTMinimax和Portfolio AI等规划算法中的应用效果。随着地图规模的增加，评价函数得分的提升更为明显。同时，该方法引入的评价函数计算时间增加保持在所有评价函数和规划算法的6%以下。提出的动态适应性评价函数为实时策略任务的评估提供了一种有前景的方法。 

---
# SenseRAG: Constructing Environmental Knowledge Bases with Proactive Querying for LLM-Based Autonomous Driving 

**Title (ZH)**: SenseRAG：通过主动查询构建环境知识库以支持基于大语言模型的自动驾驶 

**Authors**: Xuewen Luo, Fan Ding, Fengze Yang, Yang Zhou, Junnyong Loo, Hwa Hui Tew, Chenxi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.03535)  

**Abstract**: This study addresses the critical need for enhanced situational awareness in autonomous driving (AD) by leveraging the contextual reasoning capabilities of large language models (LLMs). Unlike traditional perception systems that rely on rigid, label-based annotations, it integrates real-time, multimodal sensor data into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically understand and respond to complex driving environments. To overcome the inherent latency and modality limitations of LLMs, a proactive Retrieval-Augmented Generation (RAG) is designed for AD, combined with a chain-of-thought prompting mechanism, ensuring rapid and context-rich understanding. Experimental results using real-world Vehicle-to-everything (V2X) datasets demonstrate significant improvements in perception and prediction performance, highlighting the potential of this framework to enhance safety, adaptability, and decision-making in next-generation AD systems. 

**Abstract (ZH)**: 本文通过利用大规模语言模型（LLMs）的上下文推理能力，解决了自动驾驶（AD）中增强态势感知的关键需求。不同于依赖于刚性标签注释的传统感知系统，本文将实时的多模态传感器数据整合进一个统一的、LLMs可读的知识库中，使LLMs能够动态地理解和应对复杂的驾驶环境。为了克服LLMs固有的延迟和模态限制，本文为AD设计了一种前瞻性的检索增强生成（RAG）方法，并结合了一种链式思考提示机制，确保快速且富有上下文的理解。使用实际的车辆到一切（V2X）数据集的实验结果表明，在感知和预测性能方面取得了显著的改进，突显了该框架在提升下一代AD系统的安全性、适应性和决策能力方面的潜力。 

---
# VLM-driven Behavior Tree for Context-aware Task Planning 

**Title (ZH)**: 基于VLM的上下文感知任务规划行为树 

**Authors**: Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi  

**Link**: [PDF](https://arxiv.org/pdf/2501.03968)  

**Abstract**: The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLMs）用于生成行为树（BTs）在机器人领域引起了关注，但这一领域仍处于早期发展阶段。本文提出了一种新颖的框架，该框架利用视觉语言模型（VLMs）交互生成和编辑能够处理视觉条件的行为树，从而在视觉复杂环境中实现具有情境感知能力的机器人操作。我们方法的关键特点在于通过自我提示的视觉条件进行条件控制。具体来说，VLM 生成带有视觉条件节点的行为树，其中条件以自由形式的文本表达。另一个 VLM 过程将文本融入其提示中，并在机器人执行过程中针对真实世界的图像评估条件。我们已经在现实世界的咖啡店场景中验证了该框架，展示了其可行性及其局限性。 

---
# Rethinking Adversarial Attacks in Reinforcement Learning from Policy Distribution Perspective 

**Title (ZH)**: 从策略分布视角重新思考强化学习中的对抗攻击 

**Authors**: Tianyang Duan, Zongyuan Zhang, Zheng Lin, Yue Gao, Ling Xiong, Yong Cui, Hongbin Liang, Xianhao Chen, Heming Cui, Dong Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.03562)  

**Abstract**: Deep Reinforcement Learning (DRL) suffers from uncertainties and inaccuracies in the observation signal in realworld applications. Adversarial attack is an effective method for evaluating the robustness of DRL agents. However, existing attack methods targeting individual sampled actions have limited impacts on the overall policy distribution, particularly in continuous action spaces. To address these limitations, we propose the Distribution-Aware Projected Gradient Descent attack (DAPGD). DAPGD uses distribution similarity as the gradient perturbation input to attack the policy network, which leverages the entire policy distribution rather than relying on individual samples. We utilize the Bhattacharyya distance in DAPGD to measure policy similarity, enabling sensitive detection of subtle but critical differences between probability distributions. Our experiment results demonstrate that DAPGD achieves SOTA results compared to the baselines in three robot navigation tasks, achieving an average 22.03% higher reward drop compared to the best baseline. 

**Abstract (ZH)**: 深度强化学习（DRL）在实际应用中面临着观察信号的不确定性与不准确性。对抗攻击是一种评估DRL智能体鲁棒性的有效方法。然而，现有的针对单个采样动作的攻击方法对整体策略分布的影响有限，特别是在连续动作空间中。为解决这些问题，我们提出了分布感知的投影梯度下降攻击（DAPGD）。DAPGD使用策略分布相似性作为梯度扰动输入，攻击策略网络，而不是依赖于个别样本。我们利用Bhattacharyya距离在DAPGD中衡量策略相似性，能够敏感地检测概率分布之间细微但关键的差异。我们的实验结果表明，与基线方法相比，DAPGD在三个机器人导航任务中取得了SOTA结果，平均实现了22.03%更高的奖励下降。 

---
