{'arxiv_id': 'arXiv:2501.03930', 'title': 'Towards Reliable Testing for Multiple Information Retrieval System Comparisons', 'authors': 'David Otero, Javier Parapar, Álvaro Barreiro', 'link': 'https://arxiv.org/abs/2501.03930', 'abstract': 'Null Hypothesis Significance Testing is the \\textit{de facto} tool for assessing effectiveness differences between Information Retrieval systems. Researchers use statistical tests to check whether those differences will generalise to online settings or are just due to the samples observed in the laboratory. Much work has been devoted to studying which test is the most reliable when comparing a pair of systems, but most of the IR real-world experiments involve more than two. In the multiple comparisons scenario, testing several systems simultaneously may inflate the errors committed by the tests. In this paper, we use a new approach to assess the reliability of multiple comparison procedures using simulated and real TREC data. Experiments show that Wilcoxon plus the Benjamini-Hochberg correction yields Type I error rates according to the significance level for typical sample sizes while being the best test in terms of statistical power.', 'abstract_zh': 'null 假设显著性检验是评估信息检索系统效果差异的实际标准工具。研究人员使用统计测试来检查这些差异是否能够在在线环境中泛化，还是仅仅源于实验室中观察到的样本。许多研究致力于探讨在比较一对系统时哪种测试最可靠，但大多数信息检索的实际实验涉及超过两个系统。在多重比较的情况下，同时测试多个系统可能会导致测试中犯错误的概率增加。在这篇论文中，我们使用了一种新的方法，通过模拟和真实的TREC数据来评估多重比较程序的可靠性。实验表明，Wilcoxon检验结合Benjamini-Hochberg校正可以按照显著性水平产生适当的第I类错误率，并且在统计功效方面是最佳的选择。', 'title_zh': '面向多种信息检索系统比较的可靠测试方法'}
{'arxiv_id': 'arXiv:2501.03843', 'title': 'BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study', 'authors': 'Atharva Mutsaddi, Anvi Jamkhande, Aryan Thakre, Yashodhara Haribhakta', 'link': 'https://arxiv.org/abs/2501.03843', 'abstract': 'As short text data in native languages like Hindi increasingly appear in modern media, robust methods for topic modeling on such data have gained importance. This study investigates the performance of BERTopic in modeling Hindi short texts, an area that has been under-explored in existing research. Using contextual embeddings, BERTopic can capture semantic relationships in data, making it potentially more effective than traditional models, especially for short and diverse texts. We evaluate BERTopic using 6 different document embedding models and compare its performance against 8 established topic modeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive Regularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis (PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec. The models are assessed using coherence scores across a range of topic counts. Our results reveal that BERTopic consistently outperforms other models in capturing coherent topics from short Hindi texts.', 'abstract_zh': '随着如印地语这类本地语言的短文本数据在现代媒体中越来越常见，针对此类数据的稳健主题建模方法变得日益重要。本研究调查了BERTopic在建模印地语短文本方面的性能，而这一领域目前在现有研究中较少被探究。利用上下文嵌入，BERTopic可以捕捉数据中的语义关系，使其在主题建模方面可能比传统模型更有效，尤其是在处理短且多样的文本时。我们使用6种不同的文档嵌入模型评估了BERTopic，并将其性能与8种已建立的主题建模技术进行了比较，包括潜在狄利克雷分配（LDA）、非负矩阵分解（NMF）、潜在语义索引（LSI）、主题模型正则化（ARTM）、概率潜在语义分析（PLSA）、嵌入主题模型（ETM）、结合主题模型（CTM）和Top2Vec。这些模型通过一系列主题数量的连贯性分数进行了评估。我们的结果显示，BERTopic在捕捉印地语短文本中的连贯主题方面始终优于其他模型。', 'title_zh': '基于BERTopic的 Hindi 短文本主题建模：一项比较研究'}
{'arxiv_id': 'arXiv:2501.03811', 'title': 'Extending ChatGPT with a Browserless System for Web Product Price Extraction', 'authors': 'Jorge Lloret-Gazo', 'link': 'https://arxiv.org/abs/2501.03811', 'abstract': "With the advenement of ChatGPT, we can find very clean, precise answers to a varied amount of questions. However, for questions such as 'find the price of the lemon cake at zingerman's', the answer looks like 'I can't browse the web right now'. In this paper, we propose a system, called Wextractor, which extends ChatGPT to answer questions as the one mentioned before. Obviously, our system cannot be labeled as `artificial intelligence'. Simply, it offers to cover a kind of transactional search that is not included in the current version of ChatGPT. Moreover, Wextractor includes two improvements with respect to the initial version: social extraction and pointing pattern extraction to improve the answer speed.", 'abstract_zh': '随着ChatGPT的出现，我们可以找到各种问题非常干净、准确的答案。然而，对于诸如“在Zingerman’s柠檬蛋糕的价格是多少”的问题，回答却更像是“我现在不能上网浏览”。在本文中，我们提出了一种名为Wextractor的系统，该系统扩展了ChatGPT以回答上述类型的问题。显然，我们的系统不应被划分为“人工智能”。简单地说，它提供了一种当前版本ChatGPT中未包括的交易性搜索覆盖。此外，与最初的版本相比，Wextractor还包括了两项改进：社会提取和指针模式提取，以提高答案速度。', 'title_zh': '将ChatGPT扩展为无浏览器系统以提取网页产品价格'}
{'arxiv_id': 'arXiv:2501.03769', 'title': 'Multi-label Cross-lingual automatic music genre classification from lyrics with Sentence BERT', 'authors': 'Tiago Fernandes Tavares, Fabio José Ayres', 'link': 'https://arxiv.org/abs/2501.03769', 'abstract': "Music genres are shaped by both the stylistic features of songs and the cultural preferences of artists' audiences. Automatic classification of music genres using lyrics can be useful in several applications such as recommendation systems, playlist creation, and library organization. We present a multi-label, cross-lingual genre classification system based on multilingual sentence embeddings generated by sBERT. Using a bilingual Portuguese-English dataset with eight overlapping genres, we demonstrate the system's ability to train on lyrics in one language and predict genres in another. Our approach outperforms the baseline approach of translating lyrics and using a bag-of-words representation, improving the genrewise average F1-Score from 0.35 to 0.69. The classifier uses a one-vs-all architecture, enabling it to assign multiple genre labels to a single lyric. Experimental results reveal that dataset centralization notably improves cross-lingual performance. This approach offers a scalable solution for genre classification across underrepresented languages and cultural domains, advancing the capabilities of music information retrieval systems.", 'abstract_zh': '音乐流派既受歌曲风格特征的影响，也受艺术家听众文化偏好的影响。基于歌词的自动音乐流派分类在推荐系统、播放列表创建和图书馆组织等多个应用中具有重要作用。本文提出了一种基于多语言句子嵌入的多标签跨语言流派分类系统，所使用的嵌入由sBERT生成。利用一个双语葡萄牙-英语数据集，其中包含八个重叠流派，我们展示了该系统能够在一种语言下训练，并在另一种语言下预测流派的能力。与先将歌词翻译后再使用词袋表示的方法相比，我们的方法大幅提高了类别的平均F1-分数，从0.35提高到0.69。分类器采用了逐类别对抗的架构，能够为一首歌词分配多个流派标签。实验结果表明，数据集集中化显著提升了跨语言性能。该方法为低覆盖率语言和文化领域的流派分类提供了一种可扩展的解决方案，进而推动了音乐信息检索系统的进步。', 'title_zh': '基于句子BERT的跨语言歌词多标签自动音乐流派分类'}
{'arxiv_id': 'arXiv:2501.03598', 'title': 'RecKG: Knowledge Graph for Recommender Systems', 'authors': 'Junhyuk Kwon, Seokho Ahn, Young-Duk Seo', 'link': 'https://arxiv.org/abs/2501.03598', 'abstract': "Knowledge graphs have proven successful in integrating heterogeneous data across various domains. However, there remains a noticeable dearth of research on their seamless integration among heterogeneous recommender systems, despite knowledge graph-based recommender systems garnering extensive research attention. This study aims to fill this gap by proposing RecKG, a standardized knowledge graph for recommender systems. RecKG ensures the consistent representation of entities across different datasets, accommodating diverse attribute types for effective data integration. Through a meticulous examination of various recommender system datasets, we select attributes for RecKG, ensuring standardized formatting through consistent naming conventions. By these characteristics, RecKG can seamlessly integrate heterogeneous data sources, enabling the discovery of additional semantic information within the integrated knowledge graph. We apply RecKG to standardize real-world datasets, subsequently developing an application for RecKG using a graph database. Finally, we validate RecKG's achievement in interoperability through a qualitative evaluation between RecKG and other studies.", 'abstract_zh': '知识图谱在多个领域整合异构数据方面已经被证明是有效的。然而，尽管基于知识图谱的推荐系统吸引了广泛的研究关注，但在不同类型的推荐系统之间无缝整合知识图谱的研究依然相对匮乏。本研究旨在填补这一空白，提出一种标准化的知识图谱 RecKG，用于推荐系统。RecKG 确保了在不同数据集中的实体具有一致的表示方式，并通过兼容多种属性类型来实现有效的数据集成。通过对各种推荐系统数据集进行仔细的审查，我们选择了合适的属性，并通过一致的命名约定确保标准格式化。凭借这些特性，RecKG 可以无缝集成异构数据源，在整合的知识图谱中发现更多的语义信息。我们应用 RecKG 标准化实际数据集，并使用图数据库开发了 RecKG 的实际应用。最后，我们通过定性评估 RecKG 和其他研究之间的互操作性，验证了 RecKG 的这一成果。', 'title_zh': 'RecKG：推荐系统中的知识图谱'}
{'arxiv_id': 'arXiv:2501.03995', 'title': 'RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance', 'authors': 'Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus', 'link': 'https://arxiv.org/abs/2501.03995', 'abstract': "Retrieval-augmented generation (RAG) improves large language models (LLMs) by using external knowledge to guide response generation, reducing hallucinations. However, RAG, particularly multi-modal RAG, can introduce new hallucination sources: (i) the retrieval process may select irrelevant pieces (e.g., documents, images) as raw context from the database, and (ii) retrieved images are processed into text-based context via vision-language models (VLMs) or directly used by multi-modal language models (MLLMs) like GPT-4o, which may hallucinate. To address this, we propose a novel framework to evaluate the reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve ~88% accuracy on test data. Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements. Our RS model aligns with human preferences 20% more often than CLIP in retrieval, and our CS model matches human preferences ~91% of the time. Finally, we assess various RAG systems' selection and generation performances using RS and CS.", 'abstract_zh': '检索增强生成（Retrieval-Augmented Generation，RAG）通过利用外部知识引导响应生成，从而改进了大规模语言模型（Large Language Models，LLMs），减少幻觉现象。然而，RAG，尤其是多模态RAG，可能会引入新的幻觉来源：（i）检索过程可能会从数据库中选择与查询无关的片段（如文档、图像）作为原始上下文；（ii）检索到的图像通过视觉语言模型（Vision-Language Models，VLMs）转换为基于文本的上下文，或者直接被多模态语言模型（Multimodal Language Models，MLLMs）如GPT-4o使用，可能导致幻觉。为应对这一问题，我们提出了一种新的框架来评估多模态RAG的可靠性，使用两种性能指标：（i）相关性评分（Relevance Score，RS），评估检索条目与查询的相关性；（ii）正确性评分（Correctness Score，CS），评估生成响应的准确性。我们使用从ChatGPT派生的数据库和人工评估样本训练RS和CS模型。结果显示，这两种模型在测试数据上的准确率均达到约88%。此外，我们构建了一个包含5000个样本的人工标注数据库，用于评估检索片段的相关性及响应陈述的准确性。我们的RS模型在检索中比CLIP更符合人类偏好20%，而我们的CS模型约91%的情况下与人类偏好一致。最后，我们使用RS和CS评估了各种RAG系统的选择和生成性能。', 'title_zh': 'RAG-Check：评估多模态检索增强生成性能'}
{'arxiv_id': 'arXiv:2501.03989', 'title': '(De)-Indexing and the Right to be Forgotten', 'authors': 'Salvatore Vilella, Giancarlo Ruffo', 'link': 'https://arxiv.org/abs/2501.03989', 'abstract': 'In the digital age, the challenge of forgetfulness has emerged as a significant concern, particularly regarding the management of personal data and its accessibility online. The right to be forgotten (RTBF) allows individuals to request the removal of outdated or harmful information from public access, yet implementing this right poses substantial technical difficulties for search engines. This paper aims to introduce non-experts to the foundational concepts of information retrieval (IR) and de-indexing, which are critical for understanding how search engines can effectively "forget" certain content. We will explore various IR models, including boolean, probabilistic, vector space, and embedding-based approaches, as well as the role of Large Language Models (LLMs) in enhancing data processing capabilities. By providing this overview, we seek to highlight the complexities involved in balancing individual privacy rights with the operational challenges faced by search engines in managing information visibility.', 'abstract_zh': '在数字时代，遗忘的挑战已成为一个重要问题，特别是在个人数据管理及其在线可访问性方面。被遗忘权（Right to Be Forgotten, RTBF）赋予个人请求从公共访问中删除过时或有害信息的权利，然而，这为搜索引擎实施这一权利带来了巨大的技术难题。本文旨在为非专业人士介绍信息检索（Information Retrieval, IR）和去索引的基础概念，这对于理解搜索引擎如何有效“忘记”某些内容至关重要。我们将探讨各种IR模型，包括布尔模型、概率模型、向量空间模型和嵌入式方法，以及大型语言模型（Large Language Models, LLMs）在增强数据处理能力方面的作用。通过提供这一概述，我们希望能够揭示平衡个人隐私权与搜索引擎在管理信息可见性方面所面临的运营挑战的复杂性。', 'title_zh': '(去)索引与被遗忘权'}
{'arxiv_id': 'arXiv:2501.03904', 'title': 'Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study', 'authors': 'Ramya Jonnala, Gongbo Liang, Jeong Yang, Izzat Alsmadi', 'link': 'https://arxiv.org/abs/2501.03904', 'abstract': "The integration of large language models (LLMs) into public transit systems presents a transformative opportunity to enhance urban mobility. This study explores the potential of LLMs to revolutionize public transportation management within the context of San Antonio's transit system. Leveraging the capabilities of LLMs in natural language processing and data analysis, we investigate their capabilities to optimize route planning, reduce wait times, and provide personalized travel assistance. By utilizing the General Transit Feed Specification (GTFS) and other relevant data, this research aims to demonstrate how LLMs can potentially improve resource allocation, elevate passenger satisfaction, and inform data-driven decision-making in transit operations. A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses. Findings from this study suggest that while LLMs hold immense promise for public transit, careful engineering and fine-tuning are essential to realizing their full potential. San Antonio serves as a case study to inform the development of LLM-powered transit systems in other urban environments.", 'abstract_zh': '将大型语言模型（LLMs）集成到公共交通系统中，为城市交通的变革性提升提供了机会。本研究探讨了LLMs在圣安东尼奥公交系统中重塑公共交通管理的潜在可能性。借助LLMs在自然语言处理和数据分析方面的优势，我们研究了其提高路线规划效率、减少等待时间并提供个性化出行协助的能力。通过利用通用公共交通喂给规范（GTFS）及其他相关数据，本研究旨在展示LLMs如何潜在地优化资源配置、提升乘客满意度，并支持基于数据的决策制定。我们还对不同的ChatGPT模型进行了比较分析，评估了它们理解交通信息、检索相关数据和提供综合回答的能力。本研究的发现表明，虽然LLMs在公共交通领域具有巨大的潜力，但精细的设计和调整对于充分发挥其潜力至关重要。圣安东尼奥作为案例研究，为其他城市环境中LLM驱动的公交系统的发展提供了参考。', 'title_zh': '探索大规模语言模型在公共交通领域的潜力：桑托卢斯案例研究\n\n注：此处将“San Antonio”翻译为了“桑托卢斯”，因为在中文语境中，直接使用地名的英文名可能不如其汉化名称更为人所熟知。如果需要更准确的翻译，可以保留“San Antonio”，并在括号中注释英文原名，如“探索大规模语言模型在公共交通领域的潜力：桑托卢斯（San Antonio）案例研究”。'}
{'arxiv_id': 'arXiv:2501.03835', 'title': 'TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification', 'authors': 'Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen', 'link': 'https://arxiv.org/abs/2501.03835', 'abstract': 'Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendations, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity to the item embedding. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial scenarios. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Moreover, it has been successfully deployed in a real-world e-commerce platform, processing millions of product listings daily while supporting dynamic, large-scale attribute taxonomies.', 'abstract_zh': '产品属性值识别（PAVI）涉及从产品概要中识别属性值，这是提高电子商务平台上产品搜索、推荐和业务分析的关键任务。然而，现有的PAVI方法面临着诸多挑战，如推断隐式值、处理未见过分布（OOD）值以及生成规范化输出等。为解决这些局限性，我们引入了分类体系意识对比学习检索（TACLR）——首个基于检索的PAVI方法。TACLR将PAVI建模为一个信息检索任务，通过对产品概要和候选值进行编码，并基于它们与项目嵌入的相似性进行检索。它利用带有分类体系意识的困难负样本对比训练，并采用动态阈值的自适应推理。TACLR具有三个关键优势：（1）它能够有效处理隐式值和OOD值，并生成规范化输出；（2）它可以扩展到数千个类别、数万个属性和数百万个值；（3）它支持高负载工业场景下的高效推理。大量在专用和公开数据集上的实验验证了TACLR的有效性和效率。此外，TACLR已在实际的电子商务平台上成功部署，每天处理数百万个产品列表，同时支持动态、大规模的属性分类体系。', 'title_zh': 'TACLR：一种可扩展且高效的基于检索的方法，用于工业产品属性值识别'}
{'arxiv_id': 'arXiv:2501.03276', 'title': 'ComMer: a Framework for Compressing and Merging User Data for Personalization', 'authors': 'Yoel Zeldes, Amir Zait, Ilia Labzovsky, Danny Karmon, Efrat Farkash', 'link': 'https://arxiv.org/abs/2501.03276', 'abstract': "Large Language Models (LLMs) excel at a wide range of tasks, but adapting them to new data, particularly for personalized applications, poses significant challenges due to resource and computational constraints. Existing methods either rely on exposing fresh data to the model through the prompt, which is limited by context size and computationally expensive at inference time, or fine-tuning, which incurs substantial training and update costs. In this paper, we introduce ComMer - Compress and Merge - a novel framework that efficiently personalizes LLMs by compressing users' documents into compact representations, which are then merged and fed into a frozen LLM. We evaluate ComMer on two types of personalization tasks - personalized skill learning, using the tweet paraphrasing dataset and the personalized news headline generation dataset from the LaMP benchmark, and knowledge-intensive, using the PerLTQA dataset. Our experiments demonstrate that in constrained inference budget scenarios ComMer achieves superior quality in skill learning tasks, while highlighting limitations in knowledge-intensive settings due to the loss of detailed information. These results offer insights into trade-offs and potential optimizations in multi-document compression for personalization.", 'abstract_zh': '大型语言模型（LLMs）在多种任务中表现出色，但将它们适应新的数据，特别是个性化应用的数据，由于资源和计算限制，面临着重大挑战。现有方法要么依赖于通过提示将新鲜数据暴露给模型，这受限于上下文大小并在推理时计算成本高昂，要么依赖于微调，这会带来显著的训练和更新成本。在本文中，我们提出了一种新的框架——ComMer（Compress and Merge），能够高效地个性化LLMs。ComMer通过压缩用户的文档生成紧凑的表示，然后将这些表示合并并输入到冻结的LLM中。我们通过对两种类型的个性化任务——使用LaMP基准中的推文改写数据集和个人化新闻标题生成数据集进行个性化技能学习评估，以及使用PerLTQA数据集进行知识密集型评估，来评估ComMer的性能。实验结果表明，在受限的推理预算场景下，ComMer在技能学习任务中取得了更高质量的结果，但仍存在在知识密集型设置中因详细信息的丢失而出现的局限性。这些结果为多文档压缩在个性化中的权衡和潜在优化提供了见解。', 'title_zh': 'ComMer：一个用于个性化推荐的用户数据压缩与合并框架'}
