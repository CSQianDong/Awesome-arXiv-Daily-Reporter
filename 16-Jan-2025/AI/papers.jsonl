{'arxiv_id': 'arXiv:2501.09007', 'title': 'AI-RAN: Transforming RAN with AI-driven Computing Infrastructure', 'authors': 'Lopamudra Kundu, Xingqin Lin, Rajesh Gadiyar, Jean-Francois Lacasse, Shuvo Chowdhury', 'link': 'https://arxiv.org/abs/2501.09007', 'abstract': 'The radio access network (RAN) landscape is undergoing a transformative shift from traditional, communication-centric infrastructures towards converged compute-communication platforms. This article introduces AI-RAN which integrates both RAN and artificial intelligence (AI) workloads on the same infrastructure. By doing so, AI-RAN not only meets the performance demands of future networks but also improves asset utilization. We begin by examining how RANs have evolved beyond mobile broadband towards AI-RAN and articulating manifestations of AI-RAN into three forms: AI-for-RAN, AI-on-RAN, and AI-and-RAN. Next, we identify the key requirements and enablers for the convergence of communication and computing in AI-RAN. We then provide a reference architecture for advancing AI-RAN from concept to practice. To illustrate the practical potential of AI-RAN, we present a proof-of-concept that concurrently processes RAN and AI workloads utilizing NVIDIA Grace-Hopper GH200 servers. Finally, we conclude the article by outlining future work directions to guide further developments of AI-RAN.', 'abstract_zh': '无线接入网络（RAN）的景观正在经历从传统的通信为中心的基础设施向融合计算-通信平台的转变。本文介绍了AI-RAN，这是一种将RAN和人工智能（AI）工作负载集成在同一基础设施上的解决方案。通过这种方式，AI-RAN不仅满足了未来网络的性能需求，还提高了资产利用率。我们首先探讨了RAN如何从移动宽带领域扩展到AI-RAN，并将AI-RAN的表现形式概括为三种模式：AI-for-RAN、AI-on-RAN和AI-and-RAN。接着，我们识别了通信与计算融合在AI-RAN中所需的关键要求和支撑因素。随后，我们提供了一个参考架构，旨在将AI-RAN从概念转变为实践。为了展示AI-RAN的实际潜力，我们使用NVIDIA Grace-Hopper GH200服务器并行处理RAN和AI工作负载，提供了一个概念验证。最后，我们在文章结尾概述了未来的工作方向，以指导AI-RAN的进一步发展。', 'title_zh': 'AI-RAN：以AI驱动的计算基础设施重塑无线接入网'}
{'arxiv_id': 'arXiv:2501.08977', 'title': 'Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models', 'authors': 'Emma Croxford, Yanjun Gao, Nicholas Pellegrino, Karen K. Wong, Graham Wills, Elliot First, Miranda Schnier, Kyle Burton, Cris G. Ebby, Jillian Gorskic, Matthew Kalscheur, Samy Khalil, Marie Pisani, Tyler Rubeor, Peter Stetson, Frank Liao, Cherodeep Goswami, Brian Patterson, Majid Afshar', 'link': 'https://arxiv.org/abs/2501.08977', 'abstract': "As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037). Discriminant validity distinguished high- from low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows.", 'abstract_zh': "将大型语言模型（LLMs）集成到电子健康记录（EHR）工作流程中，实施前需要使用经过验证的工具评估其性能变得至关重要。现有的提供者文档质量评估工具往往不适合LLM生成的复杂文本，并且缺乏实际数据的验证。为了评估LLM生成的临床摘要质量，开发了提供者文档摘要质量评估工具（PDSQI-9）。使用几种LLM（GPT-4o、Mixtral 8x7b 和 Llama 3-8b）从多个专科的真实EHR数据中生成多文档摘要。验证工作包括皮尔逊相关性分析以确保实质性效度、因素分析和Cronbach's α以确保结构效度、可靠性检验（ICC和克朗巴哈α）以确保通用性、半德尔菲过程以确保内容效度，以及高质量与低质量摘要的对比以确保区分效度。七位临床医生评价者共评估了779份摘要并回答了8,329个问题，达到了80%以上的评价者间可靠性。PDSQI-9具有良好的内部一致性（Cronbach's α = 0.879；95% CI: 0.867-0.891）和高度的评价者间可靠性（ICC = 0.867；95% CI: 0.867-0.868），这支持了其结构效度和通用性。因素分析识别出一个解释58%变异的4因子模型，分别代表了组织性、清晰度、准确性和实用性。实质性效度由注释长度和精练（Succinct）和组织（Organized）评分的相关性（rho = -0.200，p = 0.029；rho = -0.190，p = 0.037）支持。区分效度能够区分高质量和低质量的摘要（p < 0.001）。PDSQI-9展示了强大的构建效度，支持其在临床实践中评估LLM生成的摘要，并促进LLM在医疗保健工作流程中的安全集成。", 'title_zh': '面向大型语言模型的提供者文档总结质量评价工具的开发与验证'}
{'arxiv_id': 'arXiv:2501.08951', 'title': 'Analyzing the Ethical Logic of Six Large Language Models', 'authors': 'W. Russell Neuman, Chad Coleman, Manan Shah', 'link': 'https://arxiv.org/abs/2501.08951', 'abstract': 'This study examines the ethical reasoning of six prominent generative large language models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude 3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist-deontological analytic, Moral Foundations Theory, and the Kohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and self-awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.', 'abstract_zh': '本研究考察了六种 prominant 生成式大型语言模型的道德推理：OpenAI GPT-4、Meta LLaMA 3.1、Perplexity、Anthropic Claude 3.5 Sonnet、Google Gemini 和 Mistral 7B。研究探讨了这些模型在其伦理推理中如何表达和应用伦理逻辑，特别是对于诸如铁路上的电车难题（Trolley Problem）和海因滋困境（Heinz Dilemma）等道德困境的反应。不同于传统的技术对齐研究，本研究采用可解释性与透明性的框架，促使模型解释其伦理推理过程。通过三种现有的伦理分类学——后果论-义务论分析、道德基础理论以及科尔伯格道德发展模型，对这一方法进行了分析。研究发现，这些语言模型在伦理逻辑上表现出高度一致性，体现出理性主义和后果论的特点，决策往往优先考虑最小化伤害和公平性。尽管在预训练和模型架构方面存在相似性，但不同模型在伦理推理方面的细致差异依然显著，反映了不同微调和后训练过程中的变化。这些模型在伦理推理方面的一致表现均显示出超越普通人类伦理逻辑的复杂性，表现出深思熟虑、谨慎和自我意识，其伦理推理方式类似于高级道德哲学讨论。令人惊讶的是，这些系统都一致描述自己的伦理推理比普通人类伦理逻辑更为复杂。', 'title_zh': '分析六款大型语言模型的伦理逻辑'}
{'arxiv_id': 'arXiv:2501.08897', 'title': 'Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning', 'authors': 'Qinyu Ma, Yuhao Zhou, Jianfeng Li', 'link': 'https://arxiv.org/abs/2501.08897', 'abstract': "Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs (KGs). By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm enables the exploration of all pathways, with a particular focus on multi-branched ones, helping LLMs overcome weak reasoning in multi-branched paths. This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs. Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways, demonstrating its effectiveness and potential for broader applications.", 'abstract_zh': '在材料化学中识别可靠的合成途径是一个复杂任务，特别是在聚合物科学领域，由于高分子物质的名称复杂且往往不唯一。为了应对这一挑战，我们提出了一种结合大型语言模型（LLMs）和知识图谱（KGs）的代理系统。通过利用LLMs强大的提取和识别化学物质名称的能力，并将提取的数据存储在结构化知识图谱中，我们的系统实现了有关文献的全自动检索、反应数据的提取、数据库查询、逆向合成路径树的构建，并进一步通过检索额外文献来扩大路径范围，推荐最佳反应途径。一种新型的多分支反应路径搜索（MBRPS）算法能够探索所有可能路径，特别是对于多分支路径的探索尤为突出，帮助LLMs克服多分支路径推理的局限性。本研究代表了利用LLMs构建专门针对高分子的全自动逆向合成规划代理系统的首次尝试。应用到聚酰亚胺的合成中，我们提出的新方法构建了一个包含数百条路径的逆向合成路径树，并推荐了优化路径，包括已知和新颖路径，展示了其有效性和更广泛的应用潜力。', 'title_zh': '利用大型语言模型作为知识驱动代理进行可靠的逆合成规划'}
{'arxiv_id': 'arXiv:2501.08841', 'title': 'Exploring Task-Level Optimal Prompts for Visual In-Context Learning', 'authors': 'Yan Zhu, Huan Ma, Changqing Zhang', 'link': 'https://arxiv.org/abs/2501.08841', 'abstract': "With the development of Vision Foundation Models (VFMs) in recent years, Visual In-Context Learning (VICL) has become a better choice compared to modifying models in most scenarios. Different from retraining or fine-tuning model, VICL does not require modifications to the model's weights or architecture, and only needs a prompt with demonstrations to teach VFM how to solve tasks. Currently, significant computational cost for finding optimal prompts for every test sample hinders the deployment of VICL, as determining which demonstrations to use for constructing prompts is very costly. In this paper, however, we find a counterintuitive phenomenon that most test samples actually achieve optimal performance under the same prompts, and searching for sample-level prompts only costs more time but results in completely identical prompts. Therefore, we propose task-level prompting to reduce the cost of searching for prompts during the inference stage and introduce two time-saving yet effective task-level prompt search strategies. Extensive experimental results show that our proposed method can identify near-optimal prompts and reach the best VICL performance with a minimal cost that prior work has never achieved.", 'abstract_zh': '近年来，随着 Vision Foundation Models (VFMs) 的发展，Visual In-Context Learning (VICL) 在大多数场景中已成为比修改模型更好的选择。与重新训练或微调模型不同，VICL 不需要对模型的权重或架构进行修改，只需提供带有示例的提示即可指导 VFM 解决任务。目前，为每个测试样本找到最优提示的高昂计算成本阻碍了 VICL 的部署，因为确定用于构建提示的示例是一个非常昂贵的过程。然而，在本文中，我们发现一个违反直觉的现象，即大多数测试样本在相同的提示下实际上可以获得最优性能，而在样本层面查找提示虽然花费更多时间，但结果是完全相同的提示。因此，本文提出任务层面的提示方法，以减少推理阶段查找提示的成本，并介绍两种节省时间且有效的任务层面提示搜索策略。广泛的实验结果表明，我们提出的方法可以识别接近最优的提示，并以先前工作从未达到的最小成本达到最佳 VICL 性能。', 'title_zh': '探索视觉情境学习中的任务级最优提示'}
{'arxiv_id': 'arXiv:2501.08814', 'title': 'SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector', 'authors': 'Kyeongryul Lee, Heehyeon Kim, Joyce Jiyoung Whang', 'link': 'https://arxiv.org/abs/2501.08814', 'abstract': 'The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments. Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored. Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities. In addition, we propose a Systematic dAta generatIon Framework for evaluating the risks of generative AI (SAIF). SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types. It ensures the systematic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks. Furthermore, SAIF is designed to accommodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios. We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector.', 'abstract_zh': '公共部门中生成式AI的快速采用，涵盖了从自动化公共服务到福利服务和移民过程等多样的应用，突显了其变革潜力，同时也强调了进行全面风险评估的紧迫性。尽管生成式AI在公共部门中的应用日益增多，但对其相关的风险评估仍缺乏足够的探讨。基于从不同政府政策和企业指南中提炼出的AI风险分类体系，我们研究了生成式AI在公共部门中所提出的重大风险，并将其范围扩大以考虑其多模态能力。此外，我们提出了一个系统生成数据框架（Systematic Data Generation Framework for Evaluating the Risks of Generative AI，简称SAIF）来评估生成式AI的风险。SAIF包含四个关键阶段：分解风险、设计场景、应用越狱方法和探索提示类型。该框架确保系统性和一致性的生成提示数据，便于全面评估并为缓解风险提供坚实的基础。此外，SAIF设计用于适应新兴的越狱方法和不断变化的提示类型，从而能够有效应对不可预见的风险场景。我们认为，这项研究可以在促进生成式AI在公共部门中的安全和负责任融入方面发挥关键作用。', 'title_zh': 'SAIF：全面的公共部门生成式AI风险评估框架'}
{'arxiv_id': 'arXiv:2501.08655', 'title': 'Application of Deep Reinforcement Learning to UAV Swarming for Ground Surveillance', 'authors': 'Raúl Arranz, David Carramiñana, Gonzalo de Miguel, Juan A. Besada, Ana M. Bernardos', 'link': 'https://arxiv.org/abs/2501.08655', 'abstract': "This paper summarizes in depth the state of the art of aerial swarms, covering both classical and new reinforcement-learning-based approaches for their management. Then, it proposes a hybrid AI system, integrating deep reinforcement learning in a multi-agent centralized swarm architecture. The proposed system is tailored to perform surveillance of a specific area, searching and tracking ground targets, for security and law enforcement applications. The swarm is governed by a central swarm controller responsible for distributing different search and tracking tasks among the cooperating UAVs. Each UAV agent is then controlled by a collection of cooperative sub-agents, whose behaviors have been trained using different deep reinforcement learning models, tailored for the different task types proposed by the swarm controller. More specifically, proximal policy optimization (PPO) algorithms were used to train the agents' behavior. In addition, several metrics to assess the performance of the swarm in this application were defined. The results obtained through simulation show that our system searches the operation area effectively, acquires the targets in a reasonable time, and is capable of tracking them continuously and consistently.", 'abstract_zh': '本文深入总结了空中集群技术的最新进展，涵盖了许多经典和基于新强化学习的方法。在此基础上，提出了一种混合人工智能系统，该系统在多智能体集中式集群架构中集成了深度强化学习。该系统针对特定区域的监控任务进行了优化，旨在搜索和跟踪地面目标，适用于安全和执法领域。集群由中央集群控制器管理，该控制器负责将不同的搜索和跟踪任务分配给协同工作的无人机。每个无人机智能体由一个由不同行为学已被训练以适应集群控制器提出的不同类型任务的合作子智能体集合控制。具体而言，使用近端策略优化（PPO）算法训练了智能体的行为。此外，还定义了一些指标来评估该应用中集群的表现。通过仿真获得的结果表明，我们的系统能够有效地搜索操作区域、在合理的时间内获取目标，并能够持续且一致地跟踪这些目标。', 'title_zh': '将以下论文内容或标题翻译成中文，符合学术规范：\n\n"基于深度强化学习的无人机集群地面监视应用"'}
{'arxiv_id': 'arXiv:2501.08603', 'title': 'Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design', 'authors': 'Zhi Zheng, Zhuoliang Xie, Zhenkun Wang, Bryan Hooi', 'link': 'https://arxiv.org/abs/2501.08603', 'abstract': 'Handcrafting heuristics for solving complex planning tasks (e.g., NP-hard combinatorial optimization (CO) problems) is a common practice but requires extensive domain knowledge. Recently, Large Language Model (LLM)-based automatic heuristics design (AHD) methods have shown promise in generating high-quality heuristics without manual intervention. Existing LLM-based AHD methods employ a population to maintain a fixed number of top-performing LLM-generated heuristics and introduce evolutionary computation (EC) to enhance the population iteratively. However, the population-based procedure brings greedy properties, often resulting in convergence to local optima. Instead, to more comprehensively explore the space of heuristics, we propose using Monte Carlo Tree Search (MCTS) for LLM-based heuristic evolution while preserving all LLM-generated heuristics in a tree structure. With a novel thought-alignment process and an exploration-decay technique, the proposed MCTS-AHD method delivers significantly higher-quality heuristics on various complex tasks. Our code is available at this https URL.', 'abstract_zh': '手工设计解决复杂规划任务（例如NP难组合优化（CO）问题）的启发式方法是一种常见做法，但需要深厚的领域知识。近年来，基于大型语言模型（LLM）的自动启发式设计（AHD）方法展示了在无需人工干预的情况下生成高质量启发式方法的潜力。现有的基于LLM的AHD方法通过维持一定数量的顶级性能LLM生成的启发式方法集合，并引入演化计算（EC）来逐步增强该集合，以提高启发式方法的质量。然而，基于群体的方法导致贪婪特性，往往导致局部最优解的收敛。相反，为了更全面地探索启发式方法的空间，我们提出使用蒙特卡洛树搜索（MCTS）来进行基于LLM的启发式方法演化，同时在树结构中保留所有LLM生成的启发式方法。利用一种新的思考对齐过程和探索衰减技术，提出的MCTS-AHD方法在各种复杂任务中提供了显著更高的质量启发式方法。我们的代码可在以下链接获取：this https URL。', 'title_zh': '基于LLM的自动启发式设计中的全面探索的蒙特卡洛树搜索方法'}
{'arxiv_id': 'arXiv:2501.08587', 'title': 'Sound Scene Synthesis at the DCASE 2024 Challenge', 'authors': 'Mathieu Lagrange, Junwon Lee, Modan Tailleur, Laurie M. Heller, Keunwoo Choi, Brian McFee, Keisuke Imoto, Yuki Okamoto', 'link': 'https://arxiv.org/abs/2501.08587', 'abstract': 'This paper presents Task 7 at the DCASE 2024 Challenge: sound scene synthesis. Recent advances in sound synthesis and generative models have enabled the creation of realistic and diverse audio content. We introduce a standardized evaluation framework for comparing different sound scene synthesis systems, incorporating both objective and subjective metrics. The challenge attracted four submissions, which are evaluated using the Fréchet Audio Distance (FAD) and human perceptual ratings. Our analysis reveals significant insights into the current capabilities and limitations of sound scene synthesis systems, while also highlighting areas for future improvement in this rapidly evolving field.', 'abstract_zh': '本文介绍了DCASE 2024挑战赛中的Task 7：声音场景合成。近年来，声音合成和生成模型的进展已经使得创造出真实且多样的音频内容成为可能。我们提出了一种标准化评估框架，用于比较不同的声音场景合成系统，该框架结合了客观和主观指标。此次挑战吸引了四个提交，这些提交均使用弗雷谢音频距离（FAD）和人类听觉感知评分进行评估。我们的分析揭示了当前声音场景合成系统的能力及其局限性，并指出了这一快速发展的领域中未来改进的潜在方向。', 'title_zh': 'DCASE 2024挑战赛中的声音场景合成'}
{'arxiv_id': 'arXiv:2501.08569', 'title': 'Evaluating SAT and SMT Solvers on Large-Scale Sudoku Puzzles', 'authors': 'Liam Davis, Tairan Ji', 'link': 'https://arxiv.org/abs/2501.08569', 'abstract': 'Modern SMT solvers have revolutionized the approach to constraint satisfaction problems by integrating advanced theory reasoning and encoding techniques. In this work, we evaluate the performance of modern SMT solvers in Z3, CVC5 and DPLL(T) against a standard SAT solver in DPLL. By benchmarking these solvers on novel, diverse 25x25 Sudoku puzzles of various difficulty levels created by our improved Sudoku generator, we examine the impact of advanced theory reasoning and encoding techniques. Our findings demonstrate that modern SMT solvers significantly outperform classical SAT solvers. This work highlights the evolution of logical solvers and exemplifies the utility of SMT solvers in addressing large-scale constraint satisfaction problems.', 'abstract_zh': '现代SMT求解器通过整合高级理论推理和编码技术，已经革新了约束满足问题的解决方法。在这项工作中，我们评估了现代SMT求解器Z3、CVC5和DPLL(T)与标准的DPLL SAT求解器的性能对比。通过使用我们改进的数独生成器生成的不同难度级别的新型25x25数独谜题作为基准测试，我们研究了高级理论推理和编码技术的影响。我们的研究发现，现代SMT求解器在性能上显著优于经典SAT求解器。这项工作展示了逻辑求解器的发展，并展示了SMT求解器在解决大规模约束满足问题方面的实用性。', 'title_zh': '评估SAT和SMT求解器在大规模数独谜题上的表现'}
{'arxiv_id': 'arXiv:2501.08565', 'title': 'DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem', 'authors': 'Shipei Zhou, Yuandong Ding, Chi Zhang, Zhiguang Cao, Yan Jin', 'link': 'https://arxiv.org/abs/2501.08565', 'abstract': 'This paper proposes a dual divide-and-optimize algorithm (DualOpt) for solving the large-scale traveling salesman problem (TSP). DualOpt combines two complementary strategies to improve both solution quality and computational efficiency. The first strategy is a grid-based divide-and-conquer procedure that partitions the TSP into smaller sub-problems, solving them in parallel and iteratively refining the solution by merging nodes and partial routes. The process continues until only one grid remains, yielding a high-quality initial solution. The second strategy involves a path-based divide-and-optimize procedure that further optimizes the solution by dividing it into sub-paths, optimizing each using a neural solver, and merging them back to progressively improve the overall solution. Extensive experiments conducted on two groups of TSP benchmark instances, including randomly generated instances with up to 100,000 nodes and real-world datasets from TSPLIB, demonstrate the effectiveness of DualOpt. The proposed DualOpt achieves highly competitive results compared to 10 state-of-the-art algorithms in the literature. In particular, DualOpt achieves an improvement gap up to 1.40% for the largest instance TSP100K with a remarkable 104x speed-up over the leading heuristic solver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB benchmarks, confirming its capability to tackle diverse real-world TSP applications.', 'abstract_zh': '本文提出了一种双启发式分解与优化算法（DualOpt），用于解决大规模旅行商问题（TSP）。DualOpt 结合了两种互补的策略，以提高解决方案质量和计算效率。第一个策略是一种基于网格的分解与征服过程，将TSP分割成更小的子问题，通过并行求解这些子问题并在每个迭代中通过合并节点和部分路径逐步细化解决方案，直到只剩下一张网格，从而生成高质量的初始解决方案。第二个策略是一种基于路径的分解与优化过程，通过将解决方案进一步分割为子路径，使用神经求解器优化每个子路径，并将它们重新合并以逐步提高整体解决方案的质量。在两个TSP基准实例组上的大量实验中，包括多达100,000个节点的随机生成实例和来自TSPLIB的真实世界数据集，证明了DualOpt的有效性。与文献中10种最先进的算法相比，提出的DualOpt取得了极具竞争力的结果。尤其是，对于最大的实例TSP100K，DualOpt实现了高达1.40%的改善，并且比领先的启发式求解器LKH3快了104倍。此外，DualOpt在TSPLIB基准测试中展示了强大的泛化能力，证实了其处理各种实际TSP应用的能力。', 'title_zh': 'DualOpt：一种用于大规模旅行商问题的对偶分治优化算法'}
{'arxiv_id': 'arXiv:2501.08561', 'title': 'ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for Digital Twins', 'authors': 'Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song', 'link': 'https://arxiv.org/abs/2501.08561', 'abstract': 'In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for digital twin technology called ``ANSR-DT." Our approach combines pattern recognition algorithms with reinforcement learning and symbolic reasoning to enable real-time learning and adaptive intelligence. This integration enhances the understanding of the environment and promotes continuous learning, leading to better and more effective decision-making in real-time for applications that require human-machine collaboration. We evaluated the \\textit{ANSR-DT} framework for its ability to learn and adapt to dynamic patterns, observing significant improvements in decision accuracy, reliability, and interpretability when compared to existing state-of-the-art methods. However, challenges still exist in extracting and integrating symbolic rules in complex environments, which limits the full potential of our framework in heterogeneous settings. Moreover, our ongoing research aims to address this issue in the future by ensuring seamless integration of neural models at large. In addition, our open-source implementation promotes reproducibility and encourages future research to build on our foundational work.', 'abstract_zh': '在本文中，我们提出了一种适应性神经符号学习框架，称为“ANSR-DT”，专门用于数字孪生技术。该方法结合了模式识别算法、强化学习和符号推理，以实现实时学习和自适应智能。这种集成增强了对环境的理解，并促进了持续学习，从而在需要人机协作的应用中实现更优、更有效的实时决策。我们评估了ANSR-DT框架在学习和适应动态模式方面的能力，并观察到与现有最先进的方法相比，其在决策准确性、可靠性和可解释性方面取得了显著提高。然而，复杂环境中提取和整合符号规则仍存在挑战，这限制了该框架在异构环境中的全部潜力。此外，我们正在进行的研究旨在未来通过确保大型神经模型的无缝集成来解决这一问题。另外，我们开源的实现促进了可重复性，并鼓励未来的研究基于我们奠定的基础进行进一步的工作。', 'title_zh': 'ANSR-DT：一种适应性神经符号学习与推理框架用于数字孪生'}
{'arxiv_id': 'arXiv:2501.08552', 'title': 'Reinforcement Learning-Enhanced Procedural Generation for Dynamic Narrative-Driven AR Experiences', 'authors': 'Aniruddha Srinivas Joshi', 'link': 'https://arxiv.org/abs/2501.08552', 'abstract': 'Procedural Content Generation (PCG) is widely used to create scalable and diverse environments in games. However, existing methods, such as the Wave Function Collapse (WFC) algorithm, are often limited to static scenarios and lack the adaptability required for dynamic, narrative-driven applications, particularly in augmented reality (AR) games. This paper presents a reinforcement learning-enhanced WFC framework designed for mobile AR environments. By integrating environment-specific rules and dynamic tile weight adjustments informed by reinforcement learning (RL), the proposed method generates maps that are both contextually coherent and responsive to gameplay needs. Comparative evaluations and user studies demonstrate that the framework achieves superior map quality and delivers immersive experiences, making it well-suited for narrative-driven AR games. Additionally, the method holds promise for broader applications in education, simulation training, and immersive extended reality (XR) experiences, where dynamic and adaptive environments are critical.', 'abstract_zh': '程序生成（PCG）广泛应用于游戏中的可扩展和多样化环境创建。然而，现有方法，如波动函数崩溃（WFC）算法，通常局限于静态场景，缺乏适应动态、叙事驱动应用的需求，特别是在增强现实（AR）游戏中。本论文提出了一种增强学习（RL）增强的WFC框架，旨在为移动AR环境服务。通过结合特定环境规则和由强化学习（RL）驱动的动态瓷砖权重调整，所提出的方法能够生成既具上下文一致性又能响应游戏需求的地图。对比评估和用户研究显示，该框架能够实现高质量的地图生成，并提供沉浸式体验，使其特别适合叙事驱动的AR游戏。此外，该方法在教育、模拟训练以及沉浸式扩展现实（XR）体验等场合也具有广泛应用前景，尤其是在需要动态和适应性环境的应用中。', 'title_zh': '强化学习增强的程序生成技术在动态叙事驱动型AR体验中的应用'}
{'arxiv_id': 'arXiv:2501.08450', 'title': 'Active Sampling for Node Attribute Completion on Graphs', 'authors': 'Benyuan Liu, Xu Chen, Yanfeng Wang, Ya Zhang, Zhi Cao, Ivor Tsang', 'link': 'https://arxiv.org/abs/2501.08450', 'abstract': "Node attribute, a type of crucial information for graph analysis, may be partially or completely missing for certain nodes in real world applications. Restoring the missing attributes is expected to benefit downstream graph learning. Few attempts have been made on node attribute completion, but a novel framework called Structure-attribute Transformer (SAT) was recently proposed by using a decoupled scheme to leverage structures and attributes. SAT ignores the differences in contributing to the learning schedule and finding a practical way to model the different importance of nodes with observed attributes is challenging. This paper proposes a novel AcTive Sampling algorithm (ATS) to restore missing node attributes. The representativeness and uncertainty of each node's information are first measured based on graph structure, representation similarity and learning bias. To select nodes as train samples in the next optimization step, a weighting scheme controlled by Beta distribution is then introduced to linearly combine the two properties. Extensive experiments on four public benchmark datasets and two downstream tasks have shown the superiority of ATS in node attribute completion.", 'abstract_zh': '节点属性是图分析中至关重要的一种信息，但在实际应用中，某些节点的属性可能部分或完全缺失。恢复缺失的属性预期将有助于下游的图学习任务。尽管在这方面尝试较少，但最近提出了一种名为结构-属性变换器（SAT，Structure-Attribute Transformer）的新框架，该框架通过解耦方案利用结构和属性信息。SAT忽略了结构和属性对学习计划贡献的差异，而找到一种实用的方法来建模具有观测属性节点的不同重要性颇具挑战性。本文提出了一种名为ATS（AcTive Sampling）的新算法来恢复缺失的节点属性。首先根据图结构、表示相似性和学习偏差来衡量每个节点信息的代表性及其不确定性。然后，引入了一个由Beta分布控制的加权方案，将上述两种性质线性结合，以选择作为下一步优化中训练样本的节点。在四个公开基准数据集和两个下游任务上的广泛实验表明，ATS在节点属性恢复方面具有优势。', 'title_zh': '图节点属性完成的主动采样方法'}
{'arxiv_id': 'arXiv:2501.09014', 'title': 'How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias', 'authors': "Tosin Fadahunsi, Giordano d'Aloisio, Antinisca Di Marco, Federica Sarro", 'link': 'https://arxiv.org/abs/2501.09014', 'abstract': 'Generative models are nowadays widely used to generate graphical content used for multiple purposes, e.g. web, art, advertisement. However, it has been shown that the images generated by these models could reinforce societal biases already existing in specific contexts. In this paper, we focus on understanding if this is the case when one generates images related to various software engineering tasks. In fact, the Software Engineering (SE) community is not immune from gender and ethnicity disparities, which could be amplified by the use of these models. Hence, if used without consciousness, artificially generated images could reinforce these biases in the SE domain. Specifically, we perform an extensive empirical evaluation of the gender and ethnicity bias exposed by three versions of the Stable Diffusion (SD) model (a very popular open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We obtain 6,720 images by feeding each model with two sets of prompts describing different software-related tasks: one set includes the Software Engineer keyword, and one set does not include any specification of the person performing the task. Next, we evaluate the gender and ethnicity disparities in the generated images. Results show how all models are significantly biased towards male figures when representing software engineers. On the contrary, while SD 2 and SD XL are strongly biased towards White figures, SD 3 is slightly more biased towards Asian figures. Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used. The results of our analysis highlight severe concerns about adopting those models to generate content for SE tasks and open the field for future research on bias mitigation in this context.', 'abstract_zh': '生成模型如今被广泛用于生成用于多种目的的图形内容，例如网页、艺术和广告。然而，研究发现，这些模型生成的图像可能会强化社会上已经在特定背景下存在的偏见。本文旨在探讨当生成与各类软件工程任务相关的图像时，这一现象是否会出现。事实上，软件工程（SE）社区并非免受性别和种族差异的影响，这些差异可能会因使用此类模型而被放大。因此，如果不加以意识的使用，人工生成的图像可能会在SE领域强化这些偏见。具体地，我们对三种版本的稳定扩散（SD）模型（一个非常流行的开源文本到图像模型）进行了一项广泛的经验性评估——SD 2、SD XL 和 SD 3——以了解它们在SE任务中的性别和种族偏见。通过输入每个模型两组描述不同软件相关任务的提示来生成6,720张图像：一组包含“软件工程师”关键词，另一组则不包含任何关于执行任务的人的特定说明。接着，我们评估生成图像中的性别和种族差异。结果表明，所有模型在表现软件工程师时都显著倾向于男性形象。相比之下，虽然SD 2和SD XL对白人形象有强烈偏见，SD 3则略偏向于亚裔形象。然而，无论使用哪种提示风格，所有模型都显著低估了黑人和阿拉伯人的形象。我们的分析结果揭示了在使用这些模型生成SE相关内容时存在的严重关切，并为偏见缓解方面的未来研究打开了新的领域。', 'title_zh': '生成模型是如何吸引软件工程师的？一个关于Stable Diffusion偏差的案例研究'}
{'arxiv_id': 'arXiv:2501.09012', 'title': 'Multimodal LLMs Can Reason about Aesthetics in Zero-Shot', 'authors': 'Ruixiang Jiang, Changwen Chen', 'link': 'https://arxiv.org/abs/2501.09012', 'abstract': "We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at this https URL.", 'abstract_zh': '我们首次探讨了如何通过Multimodal LLMs（多模态大语言模型）的推理能力评估艺术品的美学问题。为了便于这项研究，我们构建了MM-StyleBench，这是一个新颖的高质量基准数据集，用于评估艺术风格化。随后，我们开发了一种原则性的方法来建模人类偏好，并进行了MLLMs回答与人类偏好之间的系统相关性分析。我们的实验揭示了MLLMs在艺术评估中的固有幻觉问题，与响应的主观性有关。我们提出了ArtCoT，表明特定于艺术的任务分解和使用具体语言能够提升MLLMs在美学方面的推理能力。本研究为我们理解多模态大语言模型在艺术领域提供了宝贵的见解，并可以应用于多种下游应用，如风格迁移和艺术图像生成。代码见此处：[提供的链接]。', 'title_zh': '多模态大语言模型可以在零样本情况下进行审美推理'}
{'arxiv_id': 'arXiv:2501.08985', 'title': 'Personality Modeling for Persuasion of Misinformation using AI Agent', 'authors': 'Qianmin Lou, Wentao Xu', 'link': 'https://arxiv.org/abs/2501.08985', 'abstract': 'The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.', 'abstract_zh': '社交媒体平台上虚假信息的泛滥凸显了理解个体人格特质如何影响对虚假信息的易感性和传播的重要性。本研究采用了创新性的基于代理的建模方法，探讨了人格特质与虚假信息动态之间的关系。通过六个AI代理分别代表五大人格特质的不同维度（外向性、宜人性和神经质），我们模拟了跨六个不同虚假信息主题的交互过程。实验基于AgentScope框架和GLM-4-Flash模型实施，生成了90种独特的交互模式，揭示了人格组合如何影响说服力和抵御虚假信息的复杂模式。研究结果表明，分析性和批判性的人格特质增强了基于证据的讨论的有效性，而非侵略性的说服策略在虚假信息纠正中显示出意想不到的成功。值得注意的是，具有批判性特质的代理在与HIV相关的虚假信息讨论中成功率达到59.4%，而采用非侵略性方法的代理在不同人格组合下的持续说服率保持在40%以上。研究还揭示了说服效果的非传递性模式，挑战了基于人格的影响的传统假设。这些结果为在数字环境中开发的人格意识介入措施提供了重要见解，并建议有效的虚假信息对策应优先考虑情感联系和信任建立，而非对抗性方法。研究成果不仅丰富了关于人格与虚假信息动态的理论理解，还为社交媒体环境下打击虚假信息的实用策略提供了支持。', 'title_zh': '使用AI代理进行错误信息的劝说个性建模'}
{'arxiv_id': 'arXiv:2501.08970', 'title': 'Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography', 'authors': 'Ilia Shumailov, Daniel Ramage, Sarah Meiklejohn, Peter Kairouz, Florian Hartmann, Borja Balle, Eugene Bagdasarian', 'link': 'https://arxiv.org/abs/2501.08970', 'abstract': 'We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.', 'abstract_zh': '我们经常与不可信的第三方进行交互。强调隐私优先可以限制这些交互的效果，因为实现某些目标需要共享私人数据。传统上，解决这一挑战的方法要么是寻求可信的中介，要么是构建限制数据暴露量的密码协议，如多方计算或多知识证明。尽管在扩展密码方法方面取得了重大进步，但它们在适用的应用规模和复杂性方面仍存在限制。在本文中，我们提出能够胜任的机器学习模型可以充当可信的第三方，从而使得之前不可行的应用能够实现安全计算。特别地，我们描述了可信胜任模型环境（TCME）作为一种扩展安全计算的替代方法，其中胜任的机器学习模型在输入/输出约束下进行交互，具有明确的信息流向控制和明确的无状态性。这种方法旨在在隐私和计算效率之间取得平衡，使得在现有经典密码解决方案不可行的情况下也可以实现私有推理。我们描述了由TCME启用的各种用例，并展示了即使是某些简单的经典密码问题也可以通过TCME来解决。最后，我们概述了当前的限制，并讨论了实现它们的发展路径。', 'title_zh': '可信的机器学习模型解锁当前无法通过密码学实现的私有推断问题'}
{'arxiv_id': 'arXiv:2501.08962', 'title': 'An analysis of data variation and bias in image-based dermatological datasets for machine learning classification', 'authors': 'Francisco Mauro, Emanoel Thyago, Othon Vinicius, Rodrigo Abreu, Kelvin Cunha, José Gabriel, Rafael Barros, Thales Bezerra, Manoel Henriques, Natalia Lopes, Érico Moutinho, Jéssica Guido, Tsang Ing Ren, Paulo Borba', 'link': 'https://arxiv.org/abs/2501.08962', 'abstract': "AI algorithms have become valuable in aiding professionals in healthcare. The increasing confidence obtained by these models is helpful in critical decision demands. In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input. However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard. Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy. Also, clinical applications bring new challenges. It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes. A possible alternative would be to use transfer learning to deal with the clinical images. However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set. This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training. It assesses the main differences between distributions that disturb the model's prediction. Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy.", 'abstract_zh': '人工智能算法在医疗健康领域的应用日益增多，这些模型获得的不断增加的置信度有助于在关键决策中提供支持。在临床皮肤科中，分类模型可以通过仅使用RGB图像作为输入来检测患者皮肤上的恶性病变。然而，大多数基于学习的方法在训练时使用了经过严格验证的皮肤镜数据集，这些数据集通常样本量大且由金标准验证。临床应用模型旨在处理用户智能手机摄像头拍摄的图像，而这些图像无法提供与皮肤镜相同的分辨率。此外，临床应用带来了新的挑战：这些图像可能来自不受控的环境、肤色差异、视角变化、数据和标签噪声以及类分布不平衡等问题。一种可能的替代方法是使用迁移学习来处理临床图像，但由于样本量较少，这可能会导致模型性能下降；训练所用的数据源分布与测试集不同。本文旨在评估皮肤镜样本和临床样本之间的差距，并理解数据集变化如何影响模型训练。本文评估了那些扰乱模型预测的主要分布差异。最后，通过在不同架构上的实验，本文探讨了如何结合来自不同分布的数据，以减少对模型最终准确度的影响。', 'title_zh': '基于图像的皮肤疾病数据集中的数据变异性与偏差分析：用于机器学习分类的探讨'}
{'arxiv_id': 'arXiv:2501.08958', 'title': 'Kolmogorov-Arnold Networks for Time Series Granger Causality Inference', 'authors': 'Meiliang Liu, Yunfang Xu, Zijin Li, Zhengye Si, Xiaoxiao Yang, Xinyue Yang, Zhiwen Zhao', 'link': 'https://arxiv.org/abs/2501.08958', 'abstract': 'We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty along with ridge regularization, GCKAN infers the Granger causality from time series while enabling automatic time lag selection. Additionally, we propose an algorithm leveraging time-reversed Granger causality to enhance inference accuracy. The algorithm compares prediction and sparse-inducing losses derived from the original and time-reversed series, automatically selecting the casual relationship with the higher score or integrating the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.', 'abstract_zh': '我们将介绍Granger因果性柯尔莫戈罗夫-阿诺尔德网络（GCKAN），这是一种创新的架构，将近期提出的柯尔莫戈罗夫-阿诺尔德网络（KAN）扩展到因果推断领域。通过从KAN层中提取基权重，并结合稀疏诱导惩罚和岭正则化，GCKAN能够在时间序列数据中推断Granger因果性，并实现自动时间滞后选择。此外，我们提出了一种利用时间反转Granger因果性增强推断准确性的算法。该算法通过比较原始序列和时间反转序列的预测损失和稀疏诱导损失，自动生成得分更高的因果关系或整合结果以减轻虚假连接的影响。在洛伦兹-96模型、基因调节网络、功能性磁共振成像BOLD信号以及向量自回归（VAR）数据集上的全面实验表明，所提出模型在推断非线性、高维和小样本时间序列的Granger因果性方面达到了与当前最好方法相当的性能。', 'title_zh': '柯尔莫哥洛夫-阿诺尔德网络在时间序列格兰杰因果推断中的应用'}
{'arxiv_id': 'arXiv:2501.08931', 'title': 'Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos', 'authors': 'Javier Rodriguez-Juan, David Ortiz-Perez, Manuel Benavent-Lledo, David Mulero-Pérez, Pablo Ruiz-Ponce, Adrian Orihuela-Torres, Jose Garcia-Rodriguez, Esther Sebastián-González', 'link': 'https://arxiv.org/abs/2501.08931', 'abstract': 'The current biodiversity loss crisis makes animal monitoring a relevant field of study. In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity. Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format. In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification. This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition. The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes. In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification.', 'abstract_zh': '当前生物多样性丧失危机使动物监测成为一个重要的研究领域。鉴于此，通过监测收集的数据可以提供至关重要的见解和信息，支持旨在保护全球生物多样性的决策。尽管这些数据的重要性不言而喻，但现有的鸟视频数据集极为稀缺，且没有任何现有的数据集以视频格式提供详细的鸟类行为注释。为应对这一空白，本研究首次引入了一个专门用于鸟类行为检测和物种分类的细粒度视频数据集。该数据集满足了全面的鸟视频数据集的需求，并提供了详细的鸟类动作数据，促进了类似于人类行为识别领域所取得的进展的深度学习模型的发展。所提数据集包含178段在西班牙湿地记录的视频，捕捉了13种不同的鸟类执行的7类不同行为。此外，我们还呈现了在两个任务（鸟行为识别和物种分类）上使用最先进的模型的基准结果。', 'title_zh': '视觉湿地鸟类数据集：视频中鸟类物种识别与行为识别'}
{'arxiv_id': 'arXiv:2501.08925', 'title': 'Disentangling Exploration of Large Language Models by Optimal Exploitation', 'authors': 'Tim Grams, Patrick Betz, Christian Bartelt', 'link': 'https://arxiv.org/abs/2501.08925', 'abstract': 'Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems. In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored. Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient. We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities. Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks.', 'abstract_zh': '探索是自我提升和解决开放式问题的关键技能。然而，尚不清楚大型语言模型能否有效地探索状态空间。现有评估主要集中在探索与利用之间的权衡，通常在多臂老虎机问题中进行评估。相比之下，本研究将探索作为唯一目标，要求代理提供能够增加未来收益的信息。为了评估，我们提议通过测量已探索状态的最优可实现回报，将缺失的奖励分解为探索和利用两个部分。我们的实验结果表明，大多数模型难以充分探索状态空间，而薄弱的探索是不足的。我们观察到模型规模与探索性能之间存在正相关关系，较大的模型展示出更优秀的能力。此外，我们展示了我们的分解方法提供了探索性任务中由代理指令驱动的不同行为差异的见解，为改进大模型在探索性任务中的性能提供了一个有价值的工具。', 'title_zh': '通过最优利用拆分大型语言模型的探索'}
{'arxiv_id': 'arXiv:2501.08922', 'title': 'Modeling Melt Pool Features and Spatter Using Symbolic Regression and Machine Learning', 'authors': 'Olabode T. Ajenifujah, Amir Barati Farimani', 'link': 'https://arxiv.org/abs/2501.08922', 'abstract': 'Additive manufacturing (AM) is a rapidly evolving technology that has attracted applications across a wide range of fields due to its ability to fabricate complex geometries. However, one of the key challenges in AM is achieving consistent print quality. This inconsistency is often attributed to uncontrolled melt pool dynamics, partly caused by spatter which can lead to defects. Therefore, capturing and controlling the evolution of the melt pool is crucial for enhancing process stability and part quality. In this study, we developed a framework to support decision-making in AM operations, facilitating quality control and minimizing defects via machine learning (ML) and polynomial symbolic regression models. We implemented experimentally validated computational tools as a cost-effective approach to collect large datasets from laser powder bed fusion (LPBF) processes. For a dataset consisting of 281 process conditions, parameters such as melt pool dimensions (length, width, depth), melt pool geometry (area, volume), and volume indicated as spatter were extracted. Using machine learning (ML) and polynomial symbolic regression models, a high R2 of over 95 % was achieved in predicting the melt pool dimensions and geometry features for both the training and testing datasets, with either process conditions (power and velocity) or melt pool dimensions as the model inputs. In the case of volume indicated as spatter, R2 improved after logarithmic transforming the model inputs, which was either the process conditions or the melt pool dimensions. Among the investigated ML models, the ExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.', 'abstract_zh': '增材制造（AM）是一种迅速发展的技术，由于其能够制造复杂几何结构的能力，已在多个领域得到了广泛应用。然而，AM的关键挑战之一是如何实现一致的打印质量。这种不一致性通常归因于无法控制的熔池动态，部分原因是飞溅导致缺陷。因此，捕捉和控制熔池的演变是提升工艺稳定性和零件质量的关键。在本研究中，我们开发了一个框架来支持AM操作中的决策制定，通过机器学习（ML）和多项式符号回归模型实现质量控制和缺陷最小化。我们实施了实验验证的计算工具，作为一种成本效益高的方法来从激光粉床融合（LPBF）过程中收集大量数据集。对于包含281个工艺条件的数据集，提取了熔池尺寸（长度、宽度、深度）、熔池几何形状（面积、体积）以及表示飞溅的体积参数。通过机器学习（ML）和多项式符号回归模型，实现了训练和测试数据集在预测熔池尺寸和几何特征方面的高R²值，超过95%，模型输入为工艺条件（功率和速度）或熔池尺寸。对于表示飞溅的体积，通过将模型输入（工艺条件或熔池尺寸）进行对数变换后，R²值有所提高。在研究的机器学习模型中，ExtraTree模型达到了最高的R²值，分别为96.7%和87.5%。', 'title_zh': '使用符号回归和机器学习建模熔池特征和飞溅'}
{'arxiv_id': 'arXiv:2501.08907', 'title': 'Projection Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning', 'authors': 'Xinchen Han, Hossam Afifi, Michel Marot', 'link': 'https://arxiv.org/abs/2501.08907', 'abstract': 'Offline Reinforcement Learning (RL) faces a critical challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample learning, effectively mitigating the risks associated with OOD actions. However, the fixed hyperparameter in policy evaluation and density-based policy improvement method limit its overall efficiency. In this paper, we propose Proj-IQL, a projective IQL algorithm enhanced with the support constraint. In the policy evaluation phase, Proj-IQL generalizes the one-step approach to a multi-step approach through vector projection, while maintaining in-sample learning and expectile regression framework. In the policy improvement phase, Proj-IQL introduces support constraint that is more aligned with the policy evaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL guarantees monotonic policy improvement and enjoys a progressively more rigorous criterion for superior actions. Empirical results demonstrate the Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially in challenging navigation domains.', 'abstract_zh': '离线强化学习（Offline Reinforcement Learning, RL）面临着由离分布（Out-of-Distribution, OOD）动作引起的外推误差这一关键挑战。隐式Q学习（Implicit Q-Learning, IQL）算法通过使用分位回归实现了样本内学习，有效降低了与OOD动作相关的风险。然而，IQL在策略评估中的固定超参数以及基于密度的策略改善方法限制了其整体效率。本文提出了一种改进的IQL算法——Proj-IQL，该算法在策略评估阶段通过向量投影将一阶方法推广到多阶方法，同时保持样本内学习和分位回归框架。在策略改善阶段，Proj-IQL引入了一个与策略评估方法更为一致的支持约束。此外，我们从理论上证明了Proj-IQL保证了单调的策略改善，并且具有更为严格的优秀动作标准。实证结果表明，Proj-IQL在D4RL基准测试中取得了最先进的性能，特别是在具有挑战性的导航领域表现尤为突出。', 'title_zh': '带有支持约束的投影隐式Q学习在 Offline 强化学习中的应用'}
{'arxiv_id': 'arXiv:2501.08905', 'title': 'Computing Game Symmetries and Equilibria That Respect Them', 'authors': 'Emanuel Tewolde, Brian Hu Zhang, Caspar Oesterheld, Tuomas Sandholm, Vincent Conitzer', 'link': 'https://arxiv.org/abs/2501.08905', 'abstract': 'Strategic interactions can be represented more concisely, and analyzed and solved more efficiently, if we are aware of the symmetries within the multiagent system. Symmetries also have conceptual implications, for example for equilibrium selection. We study the computational complexity of identifying and using symmetries. Using the classical framework of normal-form games, we consider game symmetries that can be across some or all players and/or actions. We find a strong connection between game symmetries and graph automorphisms, yielding graph automorphism and graph isomorphism completeness results for characterizing the symmetries present in a game. On the other hand, we also show that the problem becomes polynomial-time solvable when we restrict the consideration of actions in one of two ways.\nNext, we investigate when exactly game symmetries can be successfully leveraged for Nash equilibrium computation. We show that finding a Nash equilibrium that respects a given set of symmetries is PPAD- and CLS-complete in general-sum and team games respectively -- that is, exactly as hard as Brouwer fixed point and gradient descent problems. Finally, we present polynomial-time methods for the special cases where we are aware of a vast number of symmetries, or where the game is two-player zero-sum and we do not even know the symmetries.', 'abstract_zh': '如果意识到一个多智能体系统中的对称性，那么战略互动可以更为简洁地表示，分析和求解也可以更高效。对称性还具有概念上的意义，例如对于均衡的选择。我们研究了识别和利用对称性的计算复杂性问题。基于经典的形式博弈框架，我们考虑了可以跨越某些或全部玩家及其行动的博弈对称性。我们发现博弈对称性和图自同构之间存在强烈联系，从而得到了用于刻画博弈中存在对称性的图自同构和图同构完全结果。另一方面，我们还展示了在行动方面限制考虑时，该问题可以转化为多项式时间可解的问题。\n\n接下来，我们研究何时可以成功利用博弈对称性来计算纳什均衡。我们证明，在一般和合作博弈中，找到尊重给定对称性的纳什均衡分别是PPAD-完全和CLS-完全问题，即恰好和布劳威尔不动点问题及梯度下降问题一样难。最后，我们提出了在已知大量对称性或在两玩家零和博弈且不知道对称性的情况下，计算纳什均衡的多项式时间方法。', 'title_zh': '计算遵守其对称性的博弈平衡点'}
{'arxiv_id': 'arXiv:2501.08889', 'title': 'Karatsuba Matrix Multiplication and its Efficient Custom Hardware Implementations', 'authors': 'Trevor E. Pogue, Nicola Nicolici', 'link': 'https://arxiv.org/abs/2501.08889', 'abstract': 'While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths. In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware. We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware.', 'abstract_zh': '虽然Karatsuba算法降低了大整数乘法的复杂度，但对于更常用位宽的较小整数，额外的加法操作限制了其优势的发挥。本研究中，我们提出了将标量Karatsuba乘法算法扩展到矩阵乘法的方法，展示了这种扩展如何保持原始Karatsuba算法中乘法复杂度的减少，同时减少额外加法的操作复杂度。此外，我们还提出了新的矩阵乘法硬件架构，以高效利用这种扩展的Karatsuba算法在专用硬件中的优势。研究结果表明，提出的算法和硬件架构可以为整数矩阵乘法提供实际的面积或执行时间改进，同时支持通过证明的 systolic阵列和传统乘法器架构在核心层面实现。我们对算法和架构进行了复杂度分析，并在单独和集成到端到端的深度学习加速器系统中对提出的架构与基线设计及先前相同计算平台上的最新工作的实现进行了评估，证明了其能够提高矩阵乘法硬件的面积性能。', 'title_zh': '卡拉塔布矩阵乘法及其高效的专用硬件实现'}
{'arxiv_id': 'arXiv:2501.08878', 'title': 'Incrementally Learning Multiple Diverse Data Domains via Multi-Source Dynamic Expansion Model', 'authors': 'Runqing Wu, Fei Ye, Qihe Liu, Guoxi Huang, Jinyu Guo, Rongyao Hu', 'link': 'https://arxiv.org/abs/2501.08878', 'abstract': 'Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge. However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain. This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains. We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks. Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning. Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance. We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance.', 'abstract_zh': '持续学习致力于开发一种能够在不断积累新信息的同时保留先前知识的模型。然而，目前的研究主要集中在一种简单的学习背景下，其中所有数据样本都源自单一的数据域。本文则将重点转向一个更为复杂且具现实意义的学习环境，其中的数据样本来源于多个不同的数据域。为应对这一复杂的学习挑战，我们提出了一种名为多源动态扩展模型（MSDEM）的新方法，该方法利用多种预训练模型作为骨干，并根据新出现的任务逐步建立新的专家模型，以实现对新任务的适应。此外，我们还提出了一种创新的动态扩展注意力机制，旨在从多个骨干模型中选择性地利用知识，从而加速新任务的学习过程。同时，我们引入了一种动态图权重路由器，能够战略性地重用所有先前获取的参数和表示，从而最大化正向知识迁移的效果，进一步提高泛化性能。我们进行了全面的实验，并且实验证明，我们提出的方法达到了现有最好水平。', 'title_zh': '通过多源动态扩展模型增量学习多个多样数据域'}
{'arxiv_id': 'arXiv:2501.08869', 'title': 'Silent Abandonment in Text-Based Contact Centers: Identifying, Quantifying, and Mitigating its Operational Impacts', 'authors': 'Antonio Castellanos, Galit B. Yom-Tov, Yair Goldberg, Jaeyoung Park', 'link': 'https://arxiv.org/abs/2501.08869', 'abstract': 'In the quest to improve services, companies offer customers the option to interact with agents via texting. Such contact centers face unique challenges compared to traditional call centers, as measuring customer experience proxies like abandonment and patience involves uncertainty. A key source of this uncertainty is silent abandonment, where customers leave without notifying the system, wasting agent time and leaving their status unclear. Silent abandonment also obscures whether a customer was served or left. Our goals are to measure the magnitude of silent abandonment and mitigate its effects. Classification models show that 3%-70% of customers across 17 companies abandon silently. In one study, 71.3% of abandoning customers did so silently, reducing agent efficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual costs per agent. We develop an expectation-maximization (EM) algorithm to estimate customer patience under uncertainty and identify influencing covariates. We find that companies should use classification models to estimate abandonment scope and our EM algorithm to assess patience. We suggest strategies to operationally mitigate the impact of silent abandonment by predicting suspected silent-abandonment behavior or changing service design. Specifically, we show that while allowing customers to write while waiting in the queue creates a missing data challenge, it also significantly increases patience and reduces service time, leading to reduced abandonment and lower staffing requirements.', 'abstract_zh': '在提高服务质量的努力中，企业为客户提供通过短信与代理互动的选择。与传统的电话服务中心相比，这种服务中心面临着独特挑战，因为衡量客户体验指标（如放弃率和耐性）涉及不确定性。这种不确定性的一个主要来源是无声放弃，即客户未通知系统就离开，白白浪费了代理的时间，并且使客户状态模糊不清。无声放弃还使得确定客户是否被服务或离开变得更加困难。我们的目标是测量无声放弃的程度，并减轻其影响。分类模型显示，在17家公司中，3%-70%的客户无声放弃。一项研究中，71.3%的放弃客户是无声放弃，这导致代理效率降低了3.2%，系统容量减少了15.3%，给每个代理每年带来5,457美元的成本。我们开发了一种预期最大化（EM）算法，用于在不确定性下估计客户耐性，并识别影响因素。结果显示，公司应该利用分类模型来估计放弃的范围，而利用我们的EM算法来评估耐性。我们建议通过预测疑似无声放弃行为或改变服务设计来操作性地减轻无声放弃的影响。具体而言，我们指出虽然允许客户在等待期间写消息会造成数据缺失的挑战，但这也显著提高了耐性，减少了服务时间，降低了放弃率，并降低了人员配置需求。', 'title_zh': '基于文本的联络中心中的隐性放弃现象：识别、量化及其操作影响的减轻措施'}
{'arxiv_id': 'arXiv:2501.08862', 'title': 'ARMOR: Shielding Unlearnable Examples against Data Augmentation', 'authors': 'Xueluan Gong, Yuji Wang, Yanjiao Chen, Haocheng Dong, Yiming Li, Mengyuan Sun, Shuaike Li, Qian Wang, Chen Chen', 'link': 'https://arxiv.org/abs/2501.08862', 'abstract': 'Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines.', 'abstract_zh': '当私人数据在线公开时，可能会被未经授权的第三方收集，用于训练深度神经网络（DNNs）。为了保护隐私，在原始样本上添加防御噪声可以降低DNNs的学习能力。最近提出了不可学习样本，通过最小化训练损失使得模型几乎不学习任何内容。然而，通常在使用数据进行训练前会对数据进行预处理，这可能会恢复受保护数据中的私人信息。本文揭示了数据增强这一常用的数据预处理技术导致的数据隐私泄露问题，数据增强可以显著提高基于不可学习样本训练的模型的准确率，从21.3%提高到66.1%。为解决这一问题，我们提出了一种名为ARMOR的防御框架，以保护数据隐私免受数据增强潜在泄露的威胁。为克服无法访问模型训练过程的困难，我们设计了一个非局部模块辅助的代理模型，以更好地捕捉数据增强的效果。此外，我们设计了一种代理数据增强选择策略，该策略通过最大化增强和非增强样本之间的分布对齐来选择每个类别的最优增强策略。我们还使用动态步长调整算法来增强防御噪声生成过程。我们在4个数据集和5种数据增强方法上进行了广泛的实验，以验证ARMOR的表现。与6种最先进的防御方法的比较表明，ARMOR可以在数据增强的情况下保留受保护私人数据的不可学习性。与基线方法相比，ARMOR可以将采用增强保护样本训练的模型的测试准确率最多降低60%。', 'title_zh': 'ARMOR：屏蔽数据增强中的不可遗忘样本\n\n在这个翻译中，“Unlearnable Examples”被翻译为“不可遗忘样本”，这在学术语境下，可以理解为某些通过数据增强生成的样本，模型难以遗忘或者学习效率低。这样的翻译既保留了原文的意思，又符合学术写作的规范。'}
{'arxiv_id': 'arXiv:2501.08851', 'title': 'Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data', 'authors': 'Balasundaram Kadirvelu, Teresa Bellido Bel, Aglaia Freccero, Martina Di Simplicio, Dasha Nicholls, A Aldo Faisal', 'link': 'https://arxiv.org/abs/2501.08851', 'abstract': 'Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25. Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support. Digital tools leveraging smartphones offer scalable and early intervention opportunities. Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents. Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools. Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation. They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors. A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning. The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric. Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders. The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness. This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks.', 'abstract_zh': '背景：青少年特别容易患心理健康障碍，超过75%的病例在25岁之前出现。研究显示，只有18%到34%的年轻患者在经历较高水平的抑郁或焦虑症状时会寻求帮助。利用智能手机的数字工具提供了大规模和早期干预的机会。目的：本研究使用一种新颖的机器学习框架，评估将主动和被动智能手机数据集成以预测非临床青少年心理健康障碍的可行性。具体而言，我们评估了Mindcraft应用在预测内化和外化障碍、进食障碍、失眠和自杀倾向风险方面的实用性。方法：参与者（N=103；平均年龄16.1岁）来自三所伦敦学校。参与者完成了《优势与困难量表》（SDQ-High风险）、《进食障碍筛查问卷》（ED-15）和《睡眠状况指示问卷》，并报告是否存在自杀念头。参与者使用Mindcraft应用连续14天，通过自我报告提供主动数据，并通过智能手机传感器提供被动数据。应用对比预训练阶段来增强用户特定特征的稳定性，随后进行监督微调。模型评估采用留一参与者验证方法，以平衡准确率作为主要指标。结果：将主动和被动数据集成相比单一数据源实现了更好的性能，分别对于SDQ-High风险、失眠、自杀倾向和进食障碍的平均平衡准确率分别为0.71、0.67、0.77和0.70。对比学习框架稳定了日常行为表示，提高了预测稳健性。本研究展示了将主动和被动智能手机数据与高级机器学习技术相结合，预测心理健康风险的潜力。', 'title_zh': '基于智能手机数据的主动与被动监测预测青少年心理健康风险：一项采用机器学习的可行性研究'}
{'arxiv_id': 'arXiv:2501.08850', 'title': 'Graph Counterfactual Explainable AI via Latent Space Traversal', 'authors': 'Andreas Abildtrup Hansen, Paraskevas Pegios, Anna Calissano, Aasa Feragen', 'link': 'https://arxiv.org/abs/2501.08850', 'abstract': "Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models. Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way. However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered. For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs. We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder. We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes. We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines.", 'abstract_zh': '解释深度神经网络的预测是一项非平凡的任务，但高质量的预测解释往往是从业者信任这些模型的前提条件。反事实解释旨在通过找到“最邻近”的在分布输入来解释预测，其预测在预设的方式下发生变化。然而，如何定义这种最邻近的替代输入仍然是一个开放问题，其解依赖于具体的领域（例如，图像、图、表格数据等）及其特定的应用场景。对于图来说，这个问题被进一步复杂化，一是由于其离散的性质，与最先进的图分类器的连续性质形成对比；二是由于作用于图的节点置换群。\n\n我们提出了一种方法，利用一个针对具体情况的置换不变图变分自编码器生成任何可微黑盒图分类器的反事实解释。我们通过跨越分类器的分类边界在整个自编码器的潜在空间中连续遍历，来生成反事实解释，从而无缝地结合了图的离散结构和连续属性。我们在三个图数据集上进行了实验验证，表明我们的模型始终具有高性能且比基线模型更具鲁棒性。', 'title_zh': '通过潜在空间遍历实现图型因果可解释人工智能'}
{'arxiv_id': 'arXiv:2501.08848', 'title': 'RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning', 'authors': 'Carlos Güemes-Palau, Miquel Ferriol-Galmés, Jordi Paillisse-Vilanova, Albert López-Brescó, Pere Barlet-Ros, Albert Cabellos-Aparicio', 'link': 'https://arxiv.org/abs/2501.08848', 'abstract': "Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.", 'abstract_zh': '网络仿真在网络建模中至关重要，可以帮助完成从容量规划到性能估算等一系列任务。传统的离散事件仿真（DES）方法在计算成本和准确性方面存在局限性。本文介绍了一种名为RouteNet-Gauss的新颖方法，该方法通过将测试床网络与机器学习（ML）模型集成来解决这些挑战。通过将测试床用作硬件加速器，RouteNet-Gauss能够快速生成训练数据集，并以高保真度模拟接近真实世界条件的网络场景。实验结果表明，与最先进的DES基方法相比，RouteNet-Gauss的预测误差可降低95%以上，并且推理时间比现有方法快488倍。RouteNet-Gauss的模块化架构可以根据网络场景的特定特征（如拓扑结构和路由）动态构建，使其能够理解并泛化到训练期间未见过的不同网络配置，甚至包括规模大10倍的网络。此外，它还支持时间聚合性能估算（TAPE），提供可配置的时间粒度，并保持高精度的流量性能指标。这种方法有望提高仿真效率和准确性，为网络运营商提供有价值的工具。', 'title_zh': 'RouteNet-Gauss：增强硬件的网络建模方法及机器学习应用'}
{'arxiv_id': 'arXiv:2501.08847', 'title': 'Automatic tuning of communication protocols for vehicular ad hoc networks using metaheuristics', 'authors': 'José García-Nieto, Jamal Toutouh, Enrique Alba', 'link': 'https://arxiv.org/abs/2501.08847', 'abstract': 'The emerging field of vehicular ad hoc networks (VANETs) deals with a set of communicating vehicles which are able to spontaneously interconnect without any pre-existing infrastructure. In such kind of networks, it is crucial to make an optimal configuration of the communication protocols previously to the final network deployment. This way, a human designer can obtain an optimal QoS of the network beforehand. The problem we consider in this work lies in configuring the File Transfer protocol Configuration (FTC) with the aim of optimizing the transmission time, the number of lost packets, and the amount of data transferred in realistic VANET scenarios. We face the FTC with five representative state-of-the-art optimization techniques and compare their performance. These algorithms are: Particle Swarm Optimization (PSO), Differential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy (ES), and Simulated Annealing (SA). For our tests, two typical environment instances of VANETs for Urban and Highway scenarios have been defined. The experiments using ns- 2 (a well-known realistic VANET simulator) reveal that PSO outperforms all the compared algorithms for both studied VANET instances.', 'abstract_zh': '新兴的车辆自组网络（VANETs）领域涉及一组能够自发互联的通信车辆，无需预先存在的基础设施。在这样的网络中，优化通信协议的配置对于最终网络部署至关重要。这样，人类设计者可以在实际部署之前获得网络的最佳服务质量（QoS）。本文考虑的问题是在现实的VANET场景中配置文件传输协议配置（FTC），以优化传输时间、丢失的数据包数量以及传输的数据量。我们使用五种代表性的先进优化技术来解决这个问题，并比较它们的性能。这些算法包括：粒子群优化（PSO）、差分进化（DE）、遗传算法（GA）、进化策略（ES）和模拟退火（SA）。为了进行测试，定义了两种典型的VANET环境实例，分别是城市场景和高速路场景。使用ns-2（一个知名的现实范式的VANET仿真器）进行的实验表明，PSO在两个研究的VANET实例中表现优于所有比较算法。', 'title_zh': '使用元启发式方法自动调整车载自组织网络通信协议'}
{'arxiv_id': 'arXiv:2501.08838', 'title': 'ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind', 'authors': 'Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Saki Mizuno, Keita Suzuki, Ryo Masumura, Hiroaki Sugiyama, Kuniko Saito', 'link': 'https://arxiv.org/abs/2501.08838', 'abstract': 'Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. ToMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits.', 'abstract_zh': '现有的理论思维（Theory of Mind，ToM）基准在三个方面与现实世界场景存在差异：1）它们评估的只是有限的几种心理状态，如信念；2）虚假信念没有被充分探讨；3）角色 diverse 的个性特征被忽视。为了应对这些挑战，我们引入了 ToMATO，这是一种基于对话的多选题形式的 ToM 基准。ToMATO 通过具有信息不对称性的大型语言模型（LLM）-LLM 对话生成。通过采用一种提示方法，要求角色扮演的 LLM 在每次发言前表达其思考过程，从而捕获到跨五个类别（信念、意图、愿望、情绪、知识）的一、二阶心理状态。这些表达的思考过程作为评估对话中角色心理状态的问答题的答案。此外，通过隐藏 LLM 的思考来引入信息不对称，从而生成各种心理状态的虚假信念。给 LLM 分配不同的个性特征进一步增加了言谈和思考的多样性。ToMATO 包含 5400 个问题、753 次对话和 15 种个性特征模式。我们的分析显示，由于角色扮演的 LLM 之间存在信息不对称，此数据集构建方法频繁生成虚假信念，并能有效反映多样化的个性特征。我们在 ToMATO 上评估了九种 LLM，发现即使是 GPT-4o mini 在理解虚假信念方面也落后于人类表现，并且缺乏对不同个性特征的鲁棒性。', 'title_zh': 'ToMATO：角色扮演大规模语言模型的内心状态口语化基准测试'}
{'arxiv_id': 'arXiv:2501.08828', 'title': 'MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents', 'authors': 'Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu', 'link': 'https://arxiv.org/abs/2501.08828', 'abstract': 'Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.', 'abstract_zh': '多模态文档检索旨在识别和检索各种形式的多模态内容，如图表、表格、图表以及布局信息，从中广泛文档。尽管多模态文档检索具有重要意义，但目前缺乏一个坚固的基准来有效地评估系统在多模态文档检索中的性能。为弥补这一不足，本文介绍了一个新的基准，命名为MMDocIR，涵盖两个不同的任务：页面级检索和布局级检索。前者侧重于在长文档中定位最相关的页面，而后者旨在检测特定布局，提供比整页分析更精细的粒度。布局可以是指各种元素，如文本段落、公式、图表、表格或图表。MMDocIR基准数据集包含专家注释的1,685个问题和通过自举生成的173,843个问题的标签，成为促进多模态文档检索训练和评估的重要资源。\n\n通过严格的实验，我们揭示了以下几点发现：(i) 视觉检索器显著优于文本检索器；(ii) MMDocIR的训练集能够有效提高多模态文档检索的训练过程；(iii) 利用VLM-文本的文本检索器比使用OCR-文本的检索器表现更好。这些发现进一步强调了在多模态文档检索中整合视觉元素的潜在优势。', 'title_zh': 'MMDocIR：长文档多模态检索的基准测试'}
{'arxiv_id': 'arXiv:2501.08816', 'title': 'IDEA: Image Description Enhanced CLIP-Adapter', 'authors': 'Zhipeng Ye, Feng Jiang, Qiufeng Wang, Kaizhu Huang, Jiaqi Huang', 'link': 'https://arxiv.org/abs/2501.08816', 'abstract': 'CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g. zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies primarily focus on either prompt learning for text or adapter tuning for vision, without fully exploiting the complementary information and correlations among image-text pairs. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA) method to adapt CLIP to few-shot image classification tasks. This method captures fine-grained features by leveraging both visual features and textual descriptions of images. IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model\'s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the Llama model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named "IMD-11". Our code and data are released at this https URL.', 'abstract_zh': 'CLIP（对比语言-图像预训练）在模式识别和计算机视觉领域取得了巨大成功。将CLIP应用于下游任务（例如零样本或少样本分类）是多模态学习领域的热点话题。然而，当前的研究主要集中在文本的提示学习或视觉的适配调优上，未能充分利用图像-文本对之间的互补信息和关联性。本文提出了一种图像描述增强CLIP适配器（IDEA）方法，以使CLIP适应少样本图像分类任务。该方法通过利用图像的视觉特征和文本描述来捕捉细微特征。IDEA是一种不需要训练的方法，可在多个任务中与最新模型相媲美，甚至优于最新模型。此外，我们引入了可训练IDEA（T-IDEA），该方法通过添加两个轻量级可学习组件（即投影器和可学习的隐空间），进一步提升了模型的性能，并在11个数据集上实现了SOTA结果。作为一项重要贡献，我们使用Llama模型并设计了一个综合管道来生成11个数据集图像的文本描述，共生成了1,637,795个图像-文本对，命名为“IMD-11”。我们的代码和数据已在此处发布：[相关链接]。', 'title_zh': 'IDEA：图像描述增强的CLIP-适配器'}
{'arxiv_id': 'arXiv:2501.08809', 'title': 'XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework', 'authors': 'Sida Tian, Can Zhang, Wei Yuan, Wei Tan, Wenjie Zhu', 'link': 'https://arxiv.org/abs/2501.08809', 'abstract': 'In recent years, remarkable advancements in artificial intelligence-generated content (AIGC) have been achieved in the fields of image synthesis and text generation, generating content comparable to that produced by humans. However, the quality of AI-generated music has not yet reached this standard, primarily due to the challenge of effectively controlling musical emotions and ensuring high-quality outputs. This paper presents a generalized symbolic music generation framework, XMusic, which supports flexible prompts (i.e., images, videos, texts, tags, and humming) to generate emotionally controllable and high-quality symbolic music. XMusic consists of two core components, XProjector and XComposer. XProjector parses the prompts of various modalities into symbolic music elements (i.e., emotions, genres, rhythms and notes) within the projection space to generate matching music. XComposer contains a Generator and a Selector. The Generator generates emotionally controllable and melodious music based on our innovative symbolic music representation, whereas the Selector identifies high-quality symbolic music by constructing a multi-task learning scheme involving quality assessment, emotion recognition, and genre recognition tasks. In addition, we build XMIDI, a large-scale symbolic music dataset that contains 108,023 MIDI files annotated with precise emotion and genre labels. Objective and subjective evaluations show that XMusic significantly outperforms the current state-of-the-art methods with impressive music quality. Our XMusic has been awarded as one of the nine Highlights of Collectibles at WAIC 2023. The project homepage of XMusic is this https URL.', 'abstract_zh': '近年来，人工智能生成内容（AIGC）在图像合成和文本生成领域取得了显著进展，生成的内容与人类创作的内容颇具相似性。然而，AI生成音乐的质量尚未达到这一标准，主要原因是难以有效地控制音乐情绪并确保高质量的输出。本文提出了一种通用的符号化音乐生成框架XMusic，该框架支持多种灵活的输入提示（例如：图像、视频、文字、标签和哼唱），以生成可调控情绪且高质量的符号化音乐。XMusic由两个核心组件组成：XProjector和XComposer。XProjector将来自不同模态的提示解析为符号音乐元素（即情绪、体裁、节奏和音符），生成相应的音乐。XComposer包含一个生成器和一个选择器。生成器基于我们创新的符号音乐表示生成可调控情绪且流畅的音乐，而选择器通过构建包含质量评估、情绪识别和体裁识别的多任务学习方案，来识别高质量的符号音乐。此外，我们构建了包含108,023个MIDI文件的XMIDI大型符号化音乐数据集，这些文件附带精确的情绪和体裁标签。客观和主观评估表明，XMusic在音乐质量方面明显优于目前的最先进的方法。我们的XMusic荣获了2023年世界人工智能大会WAIC年度九大亮点之一。XMusic项目的主页网址为：[这个 https URL]。', 'title_zh': 'XMusic：面向 generalized 和可控的象征性音乐生成框架'}
{'arxiv_id': 'arXiv:2501.08778', 'title': 'Networked Agents in the Dark: Team Value Learning under Partial Observability', 'authors': 'Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo', 'link': 'https://arxiv.org/abs/2501.08778', 'abstract': 'We propose a novel cooperative multi-agent reinforcement learning (MARL) approach for networked agents. In contrast to previous methods that rely on complete state information or joint observations, our agents must learn how to reach shared objectives under partial observability. During training, they collect individual rewards and approximate a team value function through local communication, resulting in cooperative behavior. To describe our problem, we introduce the networked dynamic partially observable Markov game framework, where agents communicate over a switching topology communication network. Our distributed method, DNA-MARL, uses a consensus mechanism for local communication and gradient descent for local computation. DNA-MARL increases the range of the possible applications of networked agents, being well-suited for real world domains that impose privacy and where the messages may not reach their recipients. We evaluate DNA-MARL across benchmark MARL scenarios. Our results highlight the superior performance of DNA-MARL over previous methods.', 'abstract_zh': '我们提出了一种新颖的协作多智能体强化学习（MARL）方法，适用于网络化智能体。与先前依赖完整状态信息或联合观测的方法不同，我们的智能体必须在部分可观测的情况下学会如何达成共享目标。在训练过程中，它们通过局部通信收集个体奖励并近似团队价值函数，从而实现协作行为。为了描述我们的问题，我们引入了网络动态部分可观测马尔可夫游戏框架，其中智能体通过切换拓扑通信网络进行沟通。我们的分布式方法DNA-MARL 使用一致性机制进行局部通信和梯度下降进行局部计算。DNA-MARL 扩展了网络化智能体的应用范围，特别适合那些具有隐私要求且消息可能无法到达接收方的真实世界领域。我们在基准MARL场景中评估了DNA-MARL。我们的结果突显了DNA-MARL相较于先前方法的优越性能。', 'title_zh': '在信息受限条件下网络化代理的价值学习：部分可观测性团队价值学习'}
{'arxiv_id': 'arXiv:2501.08774', 'title': 'How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in Software Engineering', 'authors': 'Christoph Treude, Marco A. Gerosa', 'link': 'https://arxiv.org/abs/2501.08774', 'abstract': 'Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to improve productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development.', 'abstract_zh': '人工智能（AI），包括大型语言模型和生成式AI，正在成为软件开发的重要力量，为开发者提供了涵盖整个开发生命周期的强大工具。尽管软件工程研究已经广泛研究了AI工具在软件开发中的应用，但开发者与这些AI驱动工具之间的具体交互方式直到最近才开始受到关注。理解和改进这些交互方式有可能提高AI驱动工作流中的生产力、信任度和效率。在本文中，我们提出了开发者与AI工具之间交互类型的分类，识别了包括自动代码补全建议、基于命令的操作以及对话式辅助在内的 eleven 种不同的交互类型。基于这一分类，我们概述了一项研究议程，旨在优化AI交互、提高开发者控制能力，并解决AI辅助开发中的信任和易用性挑战。通过为研究开发者与AI交互奠定结构化的基础，本文旨在激发创建更有效、更具适应性的软件开发AI工具的研究。', 'title_zh': '开发者与AI的互动：软件工程中人机协作的分类框架'}
{'arxiv_id': 'arXiv:2501.08760', 'title': 'Leveraging LLM Agents for Translating Network Configurations', 'authors': 'Yunze Wei, Xiaohui Xie, Yiwei Zuo, Tianshuo Hu, Xinyi Chen, Kaiwen Chi, Yong Cui', 'link': 'https://arxiv.org/abs/2501.08760', 'abstract': 'Configuration translation is a critical and frequent task in network operations. When a network device is damaged or outdated, administrators need to replace it to maintain service continuity. The replacement devices may originate from different vendors, necessitating configuration translation to ensure seamless network operation. However, translating configurations manually is a labor-intensive and error-prone process. In this paper, we propose an intent-based framework for translating network configuration with Large Language Model (LLM) Agents. The core of our approach is an Intent-based Retrieval Augmented Generation (IRAG) module that systematically splits a configuration file into fragments, extracts intents, and generates accurate translations. We also design a two-stage verification method to validate the syntax and semantics correctness of the translated configurations. We implement and evaluate the proposed method on real-world network configurations. Experimental results show that our method achieves 97.74% syntax correctness, outperforming state-of-the-art methods in translation accuracy.', 'abstract_zh': '网络配置的翻译是网络运维中一个关键且频繁的任务。当网络设备出现故障或过时，管理员需要更换设备以保持服务连续性。替换设备可能来自不同的供应商，这需要进行配置翻译以确保网络无缝运行。然而，手动翻译配置是一个劳动密集且易出错的过程。在本文中，我们提出了一种基于意图的框架，利用大型语言模型（LLM）代理进行网络配置翻译。我们方法的核心是一个基于意图的检索增强生成（IRAG）模块，该模块系统地将配置文件拆分为片段、提取意图并生成准确的翻译。我们还设计了一种两阶段验证方法，以验证翻译配置的语法和语义正确性。我们已在实际网络配置上实现了并评估了该方法。实验结果表明，我们的方法在语法正确性方面达到了97.74%，在翻译准确性方面超过了现有最先进的方法。', 'title_zh': '利用大语言模型代理进行网络配置翻译'}
{'arxiv_id': 'arXiv:2501.08712', 'title': 'Self-supervised Transformation Learning for Equivariant Representations', 'authors': 'Jaemyung Yu, Jaehyun Choi, Dong-Jae Lee, HyeongGwon Hong, Junmo Kim', 'link': 'https://arxiv.org/abs/2501.08712', 'abstract': "Unsupervised representation learning has significantly advanced various machine learning tasks. In the computer vision domain, state-of-the-art approaches utilize transformations like random crop and color jitter to achieve invariant representations, embedding semantically the same inputs despite transformations. However, this can degrade performance in tasks requiring precise features, such as localization or flower classification. To address this, recent research incorporates equivariant representation learning, which captures transformation-sensitive information. However, current methods depend on transformation labels and thus struggle with interdependency and complex transformations. We propose Self-supervised Transformation Learning (STL), replacing transformation labels with transformation representations derived from image pairs. The proposed method ensures transformation representation is image-invariant and learns corresponding equivariant transformations, enhancing performance without increased batch complexity. We demonstrate the approach's effectiveness across diverse classification and detection tasks, outperforming existing methods in 7 out of 11 benchmarks and excelling in detection. By integrating complex transformations like AugMix, unusable by prior equivariant methods, this approach enhances performance across tasks, underscoring its adaptability and resilience. Additionally, its compatibility with various base models highlights its flexibility and broad applicability. The code is available at this https URL.", 'abstract_zh': '无监督表示学习在各种机器学习任务中取得了显著进展。在计算机视觉领域，当前最先进的方法利用随机裁剪和颜色抖动等变换来实现不变表示，尽管存在变换，仍能嵌入语义相同的输入。然而，这种方法在需要精确特征的任务（如定位或花卉分类）中可能会导致性能下降。为了解决这一问题，近期的研究引入了变换不变表示学习，该方法捕捉变换敏感的信息。然而，当前的方法依赖于变换标签，因此难以处理复杂的变换和相互依赖。我们提出了一种自我监督变换学习（Self-supervised Transformation Learning, STL），用从图像对中提取的变换表示替代了变换标签。该提出的算法确保了变换表示的图像不变性，并学习相应的变换不变变换，从而在不增加批处理复杂度的情况下提升性能。我们通过在各种分类和检测任务中的应用展示了该方法的有效性，在11个基准测试中超过了现有方法中的7个，并在检测任务中表现出色。通过整合先前的变换不变方法无法处理的复杂变换（如AugMix），该方法在多种任务中均提升了性能，突显了其适应性和鲁棒性。此外，它与各种基础模型的兼容性展示了其灵活性和广泛的应用范围。代码可在以下地址获取：this https URL。', 'title_zh': '自监督变换学习以生成不变表示'}
{'arxiv_id': 'arXiv:2501.08669', 'title': 'SPEQ: Stabilization Phases for Efficient Q-Learning in High Update-To-Data Ratio Reinforcement Learning', 'authors': 'Carlo Romeo, Girolamo Macaluso, Alessandro Sestini, Andrew D. Bagdanov', 'link': 'https://arxiv.org/abs/2501.08669', 'abstract': 'A key challenge in Deep Reinforcement Learning is sample efficiency, especially in real-world applications where collecting environment interactions is expensive or risky. Recent off-policy algorithms improve sample efficiency by increasing the Update-To-Data (UTD) ratio and performing more gradient updates per environment interaction. While this improves sample efficiency, it significantly increases computational cost due to the higher number of gradient updates required. In this paper we propose a sample-efficient method to improve computational efficiency by separating training into distinct learning phases in order to exploit gradient updates more effectively. Our approach builds on top of the Dropout Q-Functions (DroQ) algorithm and alternates between an online, low UTD ratio training phase, and an offline stabilization phase. During the stabilization phase, we fine-tune the Q-functions without collecting new environment interactions. This process improves the effectiveness of the replay buffer and reduces computational overhead. Our experimental results on continuous control problems show that our method achieves results comparable to state-of-the-art, high UTD ratio algorithms while requiring 56\\% fewer gradient updates and 50\\% less training time than DroQ. Our approach offers an effective and computationally economical solution while maintaining the same sample efficiency as the more costly, high UTD ratio state-of-the-art.', 'abstract_zh': '深度强化学习中的一个关键挑战是样本效率，特别是在实际应用中收集环境交互可能既昂贵又具有风险。最近的离策略算法通过增加更新频率与数据的比率（Update-To-Data, UTD比率）并增加每次环境交互所进行的梯度更新次数来提高样本效率。虽然这提高了样本效率，但也由于需要更多的梯度更新而显著增加了计算成本。本文提出了一种样本高效的方法，通过将训练分为不同的学习阶段来提高计算效率，以更有效地利用梯度更新。我们的方法基于dropout Q-函数（DroQ）算法，交替进行在线、低UTD比率的训练阶段和离线稳定阶段。在稳定阶段，我们不收集新的环境交互即可微调Q-函数。这个过程提高了经验回放缓冲区的有效性，并减少了计算开销。我们在连续控制问题上的实验结果表明，与高UTD比率的先进算法相比，我们的方法只需要56%的梯度更新次数和50%的训练时间即可达到同等效果。我们的方法提供了一种有效且计算成本较低的解决方案，同时保持与成本较高的、高UTD比率的先进算法相同的样本效率。', 'title_zh': 'SPEQ：在高更新率 reinforcement learning 中稳定 Q 学习的稳定化阶段'}
{'arxiv_id': 'arXiv:2501.08653', 'title': 'Fine-grained Spatio-temporal Event Prediction with Self-adaptive Anchor Graph', 'authors': 'Wang-Tao Zhou, Zhao Kang, Sicong Liu, Lizong Zhang, Ling Tian', 'link': 'https://arxiv.org/abs/2501.08653', 'abstract': "Event prediction tasks often handle spatio-temporal data distributed in a large spatial area. Different regions in the area exhibit different characteristics while having latent correlations. This spatial heterogeneity and correlations greatly affect the spatio-temporal distributions of event occurrences, which has not been addressed by state-of-the-art models. Learning spatial dependencies of events in a continuous space is challenging due to its fine granularity and a lack of prior knowledge. In this work, we propose a novel Graph Spatio-Temporal Point Process (GSTPP) model for fine-grained event prediction. It adopts an encoder-decoder architecture that jointly models the state dynamics of spatially localized regions using neural Ordinary Differential Equations (ODEs). The state evolution is built on the foundation of a novel Self-Adaptive Anchor Graph (SAAG) that captures spatial dependencies. By adaptively localizing the anchor nodes in the space and jointly constructing the correlation edges between them, the SAAG enhances the model's ability of learning complex spatial event patterns. The proposed GSTPP model greatly improves the accuracy of fine-grained event prediction. Extensive experimental results show that our method greatly improves the prediction accuracy over existing spatio-temporal event prediction approaches.", 'abstract_zh': '事件预测任务经常处理分布在大面积空间区域中的时空数据。区域中的不同地区表现出不同的特征，同时存在潜在的相关性。这些空间异质性和相关性显著影响事件发生的时空分布，而现有的先进模型尚未对此进行解决。在连续空间中学习事件的空间依赖关系具有挑战性，因为它具有细微的粒度并且缺乏先验知识。本文提出了一种新的Graph Spatio-Temporal Point Process (GSTPP)模型，用于细粒度事件预测。该模型采用编码-解码架构，通过神经常微分方程（ODEs）联合建模空间局部区域的状态动态。状态演化基于一种新颖的自适应锚图（Self-Adaptive Anchor Graph, SAAG）的基础，该图捕获了空间依赖关系。通过在空间中自适应地定位锚节点，并共同构建它们之间的相关边，SAAG增强了模型学习复杂空间事件模式的能力。所提出的GSTPP模型显著提高了细粒度事件预测的准确性。广泛的实验结果显示，我们的方法在现有的时空事件预测方法中显著提高了预测准确性。', 'title_zh': '自适应锚图驱动的精细时空事件预测'}
{'arxiv_id': 'arXiv:2501.08648', 'title': 'MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities', 'authors': 'Savya Khosla, Kushal Kafle, Simon Jenni, Handong Zhao, John Collomosse, Jing Shi', 'link': 'https://arxiv.org/abs/2501.08648', 'abstract': 'While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning, respectively). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we introduce MAGNET, an adaptation of decoder-only LLMs that enhances their ability to generate robust representations and infill missing text spans, while preserving their knowledge and text generation capabilities. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging future context, (3) retain the ability for open-ended text generation without exhibiting repetition problem, and (4) preserve the knowledge gained by the LLM during pretraining.', 'abstract_zh': '在最初设计用于单向生成建模时，解码器型大型语言模型（LLMs）越来越多地被适应用于双向建模。然而，单向模型和双向模型通常以不同的目标分别进行训练（生成和表征学习）。这种分离忽视了开发更具通用性的语言模型以及使这些目标互相补充的机会。在本文中，我们介绍了一种名为MAGNET的解码器型LLMs的改进版本，它增强了模型生成稳健表征的能力和填补文本缺失部分的能力，同时保留了其知识和文本生成能力。MAGNET采用了三种自监督训练目标，并引入了一种结合双向注意和因果注意的注意力机制，使得所有目标都能进行统一训练。我们的实验结果表明，（1）MAGNET改进的LLMs在词级和句级表征学习任务中超过了强文本编码器；（2）利用未来上下文生成语境适配的文本填充；（3）保持了无限制文本生成的能力且未表现出重复问题；（4）保留了预训练期间通过LLM获得的知识。', 'title_zh': 'MAGNET：增强生成解码器的表示学习和填充能力'}
{'arxiv_id': 'arXiv:2501.08641', 'title': 'Reassessing the Role of Chain-of-Thought in Sentiment Analysis: Insights and Limitations', 'authors': 'Kaiyuan Zheng, Qinghua Zhao, Lei Li', 'link': 'https://arxiv.org/abs/2501.08641', 'abstract': "The relationship between language and thought remains an unresolved philosophical issue. Existing viewpoints can be broadly categorized into two schools: one asserting their independence, and another arguing that language constrains thought. In the context of large language models, this debate raises a crucial question: Does a language model's grasp of semantic meaning depend on thought processes? To explore this issue, we investigate whether reasoning techniques can facilitate semantic understanding. Specifically, we conceptualize thought as reasoning, employ chain-of-thought prompting as a reasoning technique, and examine its impact on sentiment analysis tasks. The experiments show that chain-of-thought has a minimal impact on sentiment analysis tasks. Both the standard and chain-of-thought prompts focus on aspect terms rather than sentiment in the generated content. Furthermore, counterfactual experiments reveal that the model's handling of sentiment tasks primarily depends on information from demonstrations. The experimental results support the first viewpoint.", 'abstract_zh': '语言与思维之间的关系仍然是一个未解的哲学问题。现有的观点可以大致分为两类：一类认为它们是独立的，另一类则认为语言限制着思维。在大型语言模型的背景下，这场辩论提出了一个关键问题：语言模型对语义意义的理解是否依赖于思维过程？为了探讨这一问题，我们研究了推理技术是否能够促进语义理解。具体而言，我们将思维概念化为推理，并采用链式思考提示作为一种推理技术，考察其对情感分析任务的影响。实验结果显示，链式思考对情感分析任务的影响微乎其微。标准提示和链式思考提示都集中在生成内容中的方面词汇而非情感上。此外，通过假设性实验发现，模型处理情感任务的能力主要依赖于示范信息。实验结果支持第一种观点。', 'title_zh': '重新评估在情感分析中链式思考的作用：见解与局限性'}
{'arxiv_id': 'arXiv:2501.08621', 'title': 'ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair', 'authors': 'Hong-Viet Tran, Minh-Quy Nguyen, Van-Vinh Nguyen', 'link': 'https://arxiv.org/abs/2501.08621', 'abstract': 'This paper presents an results of the VLSP 2022-2023 Machine Translation Shared Tasks, focusing on Vietnamese-Chinese and Vietnamese-Lao machine translation. The tasks were organized as part of the 9th, 10th annual workshop on Vietnamese Language and Speech Processing (VLSP 2022, VLSP 2023). The objective of the shared task was to build machine translation systems, specifically targeting Vietnamese-Chinese and Vietnamese-Lao translation (corresponding to 4 translation directions). The submission were evaluated on 1,000 pairs for testing (news and general domains) using established metrics like BLEU [11] and SacreBLEU [12]. Additionally, system outputs also were evaluated with human judgment provided by experts in Chinese and Lao languages. These human assessments played a crucial role in ranking the performance of the machine translation models, ensuring a more comprehensive evaluation.', 'abstract_zh': '本文介绍了VLSP 2022-2023机器翻译共享任务的结果，重点探讨了越南语-中文和越南语-老挝语的机器翻译任务。这些任务作为第九届和第十届越南语语言和语音处理研讨会（VLSP 2022、VLSP 2023）的一部分组织进行。共享任务的目标是构建机器翻译系统，特别是针对越南语-中文和越南语-老挝语翻译（对应4个翻译方向）。提交的作品使用了如BLEU [11] 和SacreBLEU [12] 等已建立的指标，在1000对测试集（包括新闻和通用领域）上进行评估。此外，还使用了中文和老挝语言专家的人工评判来评估系统输出。这些人工评估在排名机器翻译模型的性能方面起到了关键作用，确保了更全面的评估。', 'title_zh': 'ViBidirectionMT-Eval:  Vietnamese-中文和越南-老挝语言对的机器翻译评估'}
{'arxiv_id': 'arXiv:2501.08618', 'title': 'Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models', 'authors': 'Aruna Sankaranarayanan, Dylan Hadfield-Menell, Aaron Mueller', 'link': 'https://arxiv.org/abs/2501.08618', 'abstract': 'All natural languages are structured hierarchically. In humans, this structural restriction is neurologically coded: when two grammars are presented with identical vocabularies, brain areas responsible for language processing are only sensitive to hierarchical grammars. Using large language models (LLMs), we investigate whether such functionally distinct hierarchical processing regions can arise solely from exposure to large-scale language distributions. We generate inputs using English, Italian, Japanese, or nonce words, varying the underlying grammars to conform to either hierarchical or linear/positional rules. Using these grammars, we first observe that language models show distinct behaviors on hierarchical versus linearly structured inputs. Then, we find that the components responsible for processing hierarchical grammars are distinct from those that process linear grammars; we causally verify this in ablation experiments. Finally, we observe that hierarchy-selective components are also active on nonce grammars; this suggests that hierarchy sensitivity is not tied to meaning, nor in-distribution inputs.', 'abstract_zh': '所有自然语言都具有分层结构。在人类中，这种结构限制在神经上得到了编码：当呈现两个具有相同词汇的文法时，负责语言处理的大脑区域仅对层次结构文法敏感。利用大型语言模型（LLMs），我们探究是否仅通过大规模语言分布的暴露就能在处理中产生功能上区分的层次结构处理区域。我们使用英语、意大利语、日语或虚构单词生成输入，并改变底层文法以遵循层次结构或线性/位置规则。使用这些文法，我们首先观察到语言模型在处理分层结构输入和线性结构输入时表现出不同的行为。然后，我们发现处理层次结构文法的组件与处理线性文法的组件不同；我们在消融实验中因果验证了这一点。最后，我们观察到层次结构选择性组件也对虚构文法活跃；这表明层次结构敏感性与意义无关，也不限于分布内的输入。', 'title_zh': '大型语言模型中层次语法与线性语法的并行处理机制'}
{'arxiv_id': 'arXiv:2501.08617', 'title': 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation', 'authors': 'Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac', 'link': 'https://arxiv.org/abs/2501.08617', 'abstract': "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely on immediate feedback, which can fail to accurately reflect the downstream impact of an interaction on users' utility. We demonstrate that feedback based on evaluators' foresight estimates of downstream consequences systematically induces Goodhart's Law dynamics, incentivizing misaligned behaviors like sycophancy and deception and ultimately degrading user outcomes. To alleviate this, we propose decoupling evaluation from prediction by refocusing RLHF on hindsight feedback. Our theoretical analysis reveals that conditioning evaluator feedback on downstream observations mitigates misalignment and improves expected human utility, even when these observations are simulated by the AI system itself. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods -- Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) -- and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF.", 'abstract_zh': '生成式AI系统，如基础模型（FMs），必须与人类价值观很好地对齐，以确保其行为具有帮助性和可信度。虽然从人类反馈中进行强化学习（RLHF）已经显示出优化模型性能的潜力，但现有的RLHF流程主要依赖即时反馈，这可能无法准确反映交互对用户效用的下游影响。我们表明，基于评估者对下游后果预期的反馈系统会系统地导致Goodhart法则效应，激励出现不良行为如阿谀奉承和欺骗，最终损害用户成果。为解决这一问题，我们建议通过重新聚焦RLHF于后见之明反馈，从而将评估与预测脱钩。我们的理论分析表明，让评估者的反馈依赖于下游观察可以减轻不一致性和提高预期的人类效用，即使这些观察是由AI系统自身模拟的。为了在实用的对齐算法中利用这一洞察，我们引入了一种新的强化学习后见之明模拟方法（RLHS），该方法首先模拟可能的后果，然后通过获取反馈来评估哪个行为是在后见之明下真正有益的。我们用两种广泛使用的在线和离线偏好优化方法——近端策略优化（PPO）和直接偏好优化（DPO）——应用了RLHS方法，并且实验证实两者中的不一致性的减少情况都很显著。通过在线的人类用户研究，我们展示了即使仅用模拟后见之明反馈进行训练，RLHS也始终优于RLHF，更有效地帮助用户实现其目标，并获得更高的满意度评分。这些结果强调了即使在模拟情况下关注长期内部后果以减轻RLHF中不一致性的必要性。', 'title_zh': 'RLHS：反制RLHF中未对齐问题的 hindsight 回顾模拟方法'}
{'arxiv_id': 'arXiv:2501.08600', 'title': 'AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL', 'authors': 'Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso', 'link': 'https://arxiv.org/abs/2501.08600', 'abstract': 'As REST APIs have become widespread in modern web services, comprehensive testing of these APIs has become increasingly crucial. Due to the vast search space consisting of operations, parameters, and parameter values along with their complex dependencies and constraints, current testing tools suffer from low code coverage, leading to suboptimal fault detection. To address this limitation, we present a novel tool, AutoRestTest, which integrates the Semantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement Learning (MARL) and large language models (LLMs) for effective REST API testing. AutoRestTest determines operation-dependent parameters using the SODG and employs five specialized agents (operation, parameter, value, dependency, and header) to identify dependencies of operations and generate operation sequences, parameter combinations, and values. AutoRestTest provides a command-line interface and continuous telemetry on successful operation count, unique server errors detected, and time elapsed. Upon completion, AutoRestTest generates a detailed report highlighting errors detected and operations exercised. In this paper, we introduce our tool and present preliminary results.', 'abstract_zh': '随着现代网络服务中REST API变得日益普遍，全面测试这些API的重要性也逐渐提高。由于操作、参数及其参数值存在的庞大搜索空间，以及它们之间复杂的依赖关系和约束条件，当前的测试工具面临着代码覆盖率低的问题，导致故障检测效果不佳。为了解决这一限制，我们提出了一种名为AutoRestTest的新工具，该工具结合了语义操作依赖图（SODG）与多智能体强化学习（MARL）及大规模语言模型（LLMs），以实现有效的REST API测试。AutoRestTest利用SODG确定操作相关的参数，并通过五个专门的智能体（操作、参数、值、依赖关系和头部）来识别操作的依赖关系，并生成操作序列、参数组合和值。AutoRestTest提供命令行界面，并在测试成功次数、检测到的独特服务器错误和所花费时间方面提供持续的遥测数据。测试完成后，AutoRestTest生成一份详细的报告，突出显示检测到的错误和执行的操作。在本文中，我们介绍了该工具并展示了初步结果。', 'title_zh': 'AutoRestTest：一种基于LLM和MARL的自动REST API测试工具'}
{'arxiv_id': 'arXiv:2501.08598', 'title': 'LlamaRestTest: Effective REST API Testing with Small Language Models', 'authors': 'Myeongsoo Kim, Saurabh Sinha, Alessandro Orso', 'link': 'https://arxiv.org/abs/2501.08598', 'abstract': 'Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on these specifications. Recent advancements in Natural Language Processing (NLP), particularly with Large Language Models (LLMs), have enhanced REST API testing by extracting actionable rules and generating input values from the human-readable portions of the specification. However, these advancements overlook the potential of continuously refining the identified rules and test inputs based on server responses. To address this limitation, we present LlamaRestTest, a novel approach that employs two custom LLMs to generate realistic test inputs and uncover parameter dependencies during the testing process by incorporating server responses. These LLMs are created by fine-tuning the Llama3-8b model, using mined datasets of REST API example values and inter-parameter dependencies. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs to outperform larger models in detecting actionable rules and generating inputs for REST API testing. We evaluated configurations from the base Llama3-8B to fine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for efficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and error detection, even with RESTGPT-enhanced specifications, and an ablation study highlights the impact of its novel components.', 'abstract_zh': '现代Web服务高度依赖于REST API，这些API通常使用OpenAPI规范进行文档描述。这个标准的广泛采用导致开发了许多黑盒测试工具，这些工具根据规范生成测试用例。近年来，自然语言处理（NLP）的最新进展，尤其是大型语言模型（LLMs），通过从规范的人类可读部分中提取可操作规则并生成输入值，增强了REST API的测试。然而，这些进展忽视了根据服务器响应连续改进所识别规则和测试输入的潜力。为了解决这一局限，我们提出了LlamaRestTest，这是一种新颖的方法，通过结合服务器响应来生成现实的测试输入并揭示参数依赖关系。这种方法通过使用REST API示例值和参数间依赖关系的挖掘数据集对Llama3-8b模型进行微调来创建两个自定义的LLMs。我们在12个真实服务（包括流行的Spotify服务）上评估了LlamaRestTest，将其与RESTGPT（一个基于GPT的规格增强工具）以及RESTler、MoRest、EvoMaster和ARAT-RL等最先进的REST API测试工具进行了对比。结果显示，微调使小型LLMs能够超越大型模型，在检测可操作规则和生成REST API测试输入方面表现更优。我们从基础的Llama3-8B配置到微调版本进行了配置评估，并探索了2比特、4比特和8比特的量化方法以提高效率。LlamaRestTest在代码覆盖率和错误检测方面超越了最先进的工具，即使在RESTGPT增强的规格下也是如此。消融研究强调了其新型组件的影响。', 'title_zh': 'LlamaRestTest：使用小型语言模型进行有效的REST API测试'}
{'arxiv_id': 'arXiv:2501.08591', 'title': 'OpenMLDB: A Real-Time Relational Data Feature Computation System for Online ML', 'authors': 'Xuanhe Zhou, Wei Zhou, Liguo Qi, Hao Zhang, Dihao Chen, Bingsheng He, Mian Lu, Guoliang Li, Fan Wu, Yuqiang Chen', 'link': 'https://arxiv.org/abs/2501.08591', 'abstract': "Efficient and consistent feature computation is crucial for a wide range of online ML applications. Typically, feature computation is divided into two distinct phases, i.e., offline stage for model training and online stage for model serving. These phases often rely on execution engines with different interface languages and function implementations, causing significant inconsistencies. Moreover, many online ML features involve complex time-series computations (e.g., functions over varied-length table windows) that differ from standard streaming and analytical queries. Existing data processing systems (e.g., Spark, Flink, DuckDB) often incur multi-second latencies for these computations, making them unsuitable for real-time online ML applications that demand timely feature updates.\nThis paper presents OpenMLDB, a feature computation system deployed in 4Paradigm's SageOne platform and over 100 real scenarios. Technically, OpenMLDB first employs a unified query plan generator for consistent computation results across the offline and online stages, significantly reducing feature deployment overhead. Second, OpenMLDB provides an online execution engine that resolves performance bottlenecks caused by long window computations (via pre-aggregation) and multi-table window unions (via data self-adjusting). It also provides a high-performance offline execution engine with window parallel optimization and time-aware data skew resolving. Third, OpenMLDB features a compact data format and stream-focused indexing to maximize memory usage and accelerate data access. Evaluations in testing and real workloads reveal significant performance improvements and resource savings compared to the baseline systems. The open community of OpenMLDB now has over 150 contributors and gained 1.6k stars on GitHub.", 'abstract_zh': '高效的且一致的特征计算对于广泛的应用在线机器学习（ML）至关重要。通常，特征计算被划分为两个不同的阶段，即离线阶段用于模型训练和在线阶段用于模型服务。这两个阶段通常依赖于具有不同接口语言和函数实现的执行引擎，这导致了显著的不一致性。此外，许多在线机器学习的特征涉及复杂的时序计算（例如，不同长度表格窗口上的函数），这与标准的流计算和分析查询不同。现有的数据处理系统（如Spark、Flink、DuckDB）在这些计算中往往会产生多秒的延迟，这使它们不适合那些需要及时特征更新的实时在线机器学习应用。\n\n本文介绍了一种特征计算系统——OpenMLDB，该系统部署在4Paradigm的SageOne平台以及超过100个实际应用场景中。从技术上讲，OpenMLDB 首先采用统一的查询计划生成器，以确保离线和在线阶段的计算结果一致性，从而显著减少特征部署的开销。其次，OpenMLDB 提供了一个在线执行引擎，用于解决长时间窗口计算（通过预聚合）和多表窗口合并（通过数据自我调整）引起的性能瓶颈问题。它还提供了一个高性能的离线执行引擎，通过窗口并行优化和时间感知的数据偏差解决能力来提高效率。第三，OpenMLDB 引入了紧凑的数据格式和专注于流的数据索引，以最大化内存使用并加速数据访问。在测试和实际工作负载中的评估显示，OpenMLDB 在性能和资源利用方面相比基准系统有显著改进。OpenMLDB 社区现已拥有超过150名贡献者，并且在GitHub上获得了1600多次星标。', 'title_zh': 'OpenMLDB：面向在线机器学习的实时关系数据特征计算系统'}
{'arxiv_id': 'arXiv:2501.08585', 'title': 'A Systematic Review of Machine Learning Methods for Multimodal EEG Data in Clinical Application', 'authors': 'Siqi Zhao, Wangyang Li, Xiru Wang, Stevie Foglia, Hongzhao Tan, Bohan Zhang, Ameer Hamoodi, Aimee Nelson, Zhen Gao', 'link': 'https://arxiv.org/abs/2501.08585', 'abstract': 'Machine learning (ML) and deep learning (DL) techniques have been widely applied to analyze electroencephalography (EEG) signals for disease diagnosis and brain-computer interfaces (BCI). The integration of multimodal data has been shown to enhance the accuracy of ML and DL models. Combining EEG with other modalities can improve clinical decision-making by addressing complex tasks in clinical populations. This systematic literature review explores the use of multimodal EEG data in ML and DL models for clinical applications. A comprehensive search was conducted across PubMed, Web of Science, and Google Scholar, yielding 16 relevant studies after three rounds of filtering. These studies demonstrate the application of multimodal EEG data in addressing clinical challenges, including neuropsychiatric disorders, neurological conditions (e.g., seizure detection), neurodevelopmental disorders (e.g., autism spectrum disorder), and sleep stage classification. Data fusion occurred at three levels: signal, feature, and decision levels. The most commonly used ML models were support vector machines (SVM) and decision trees. Notably, 11 out of the 16 studies reported improvements in model accuracy with multimodal EEG data. This review highlights the potential of multimodal EEG-based ML models in enhancing clinical diagnostics and problem-solving.', 'abstract_zh': '机器学习（ML）和深度学习（DL）技术已在分析脑电图（EEG）信号以进行疾病诊断和脑-机接口（BCI）方面得到了广泛应用。多模态数据的整合已被证明能够提高ML和DL模型的准确性。将EEG与其他模态数据结合使用可以改进临床决策，通过解决临床人群中的复杂任务。本系统文献综述探讨了多模态EEG数据在临床应用中的ML和DL模型的使用情况。我们通过PubMed、Web of Science和Google Scholar进行了全面搜索，在三轮筛选后共获得了16篇相关研究。这些研究展示了多模态EEG数据在应对临床挑战中的应用，包括神经精神障碍、神经系统疾病（如癫痫检测）、神经发育障碍（如自闭症谱系障碍）以及睡眠阶段分类。数据融合发生在三个层次上：信号层、特征层和决策层。常用的ML模型主要是支持向量机（SVM）和决策树。值得注意的是，在16项研究中有11项报告表示，通过使用多模态EEG数据提高了模型准确性。本综述突显了基于多模态EEG的ML模型在增强临床诊断和问题解决方面的潜力。', 'title_zh': '多模态脑电图数据临床应用中机器学习方法的系统综述'}
{'arxiv_id': 'arXiv:2501.08566', 'title': 'Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation Disentanglement', 'authors': 'Qianniu Chen, Xiaoyang Hao, Bowen Li, Yue Liu, Li Lu', 'link': 'https://arxiv.org/abs/2501.08566', 'abstract': 'Zero-shot Text-To-Speech (TTS) synthesis shows great promise for personalized voice customization through voice cloning. However, current methods for achieving zero-shot TTS heavily rely on large model scales and extensive training datasets to ensure satisfactory performance and generalizability across various speakers. This raises concerns regarding both deployment costs and data security. In this paper, we present a lightweight and stable zero-shot TTS system. We introduce a novel TTS architecture designed to effectively model linguistic content and various speaker attributes from source speech and prompt speech, respectively. Furthermore, we present a two-stage self-distillation framework that constructs parallel data pairs for effectively disentangling linguistic content and speakers from the perspective of training data. Extensive experiments show that our system exhibits excellent performance and superior stability on the zero-shot TTS tasks. Moreover, it shows markedly superior computational efficiency, with RTFs of 0.13 and 0.012 on the CPU and GPU, respectively.', 'abstract_zh': '零样本文本转语音（TTS）合成通过语音克隆为个性化语音定制展示了巨大的潜力。然而，当前实现零样本TTS的方法严重依赖于大型模型规模和广泛的训练数据集，以确保在不同说话人之间的满意性能和泛化能力。这在部署成本和数据安全性方面引发了关注。本文介绍了一种轻量级且稳定的零样本TTS系统。我们提出了一种新颖的TTS架构，旨在从源语音和提示语音中分别有效地建模语音内容和各种说话人属性。此外，我们提出了一个两阶段自蒸馏框架，从训练数据的角度有效分离语音内容和说话人，构建了并行数据对。广泛实验表明，我们的系统在零样本TTS任务上表现出色且具有出色的稳定性。此外，它展示了显著的计算效率，CPU上的RTF为0.13，GPU上的RTF为0.012。', 'title_zh': '轻量级且稳定的零-shot TTS模型：基于自我提炼表示分离的方法'}
{'arxiv_id': 'arXiv:2501.08558', 'title': 'LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation', 'authors': 'Yiran Tao, Jehan Yang, Dan Ding, Zackory Erickson', 'link': 'https://arxiv.org/abs/2501.08558', 'abstract': 'Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF controllers like joysticks often requires frequent switching between control modes, where each mode maps controller movements to specific robot actions. Manually performing this frequent switching can make teleoperation cumbersome and inefficient. On the other hand, existing automatic mode-switching solutions, such as heuristic-based or learning-based methods, are often task-specific and lack generalizability. In this paper, we introduce LLM-Driven Automatic Mode Switching (LAMS), a novel approach that leverages Large Language Models (LLMs) to automatically switch control modes based on task context. Unlike existing methods, LAMS requires no prior task demonstrations and incrementally improves by integrating user-generated mode-switching examples. We validate LAMS through an ablation study and a user study with 10 participants on complex, long-horizon tasks, demonstrating that LAMS effectively reduces manual mode switches, is preferred over alternative methods, and improves performance over time. The project website with supplementary materials is at this https URL.', 'abstract_zh': '通过低自由度（DoF）控制器（如游戏杆）遥控高自由度（DoF）机器人操作器时，经常需要频繁地在不同的控制模式之间切换，每个模式都将控制器的动作映射到特定的机器人动作。手动进行这种频繁的切换会让遥操作变得繁琐且低效。另一方面，现有的自动模式切换解决方案，如基于启发式或基于学习的方法，通常是任务特定的，缺乏普适性。在本文中，我们介绍了一种新颖的方法——大型语言模型驱动的自动模式切换（LAMS），该方法利用大型语言模型（LLMs）根据任务上下文自动切换控制模式。与现有的方法不同，LAMS不需要任何先验的任务演示，并通过整合用户生成的模式切换示例进行增量改进。我们通过消融研究和包含10名参与者的用户研究对LAMS进行了验证，表明LAMS有效减少了手动模式切换，优于其他方法，并随着时间的推移提高了性能。项目的官方网站和补充材料可在如下链接访问：[这个链接]。', 'title_zh': 'LAMS：由大语言模型驱动的辅助远程操作自动模式切换'}
{'arxiv_id': 'arXiv:2501.08549', 'title': 'The Devil is in Temporal Token: High Quality Video Reasoning Segmentation', 'authors': 'Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, Huchuan Lu', 'link': 'https://arxiv.org/abs/2501.08549', 'abstract': "Existing methods for Video Reasoning Segmentation rely heavily on a single special token to represent the object in the keyframe or the entire video, inadequately capturing spatial complexity and inter-frame motion. To overcome these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation approach that leverages Multimodal Large Language Models (MLLMs) to inject rich spatiotemporal features into hierarchical this http URL key innovations include a Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS). Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that utilize MLLM's autoregressive learning to effectively capture both local and global information. Subsequently, we apply a similarity-based weighted fusion and frame selection strategy, then utilize SAM2 to perform keyframe segmentation and propagation. To enhance keyframe localization accuracy, the TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ achieves state-of-the-art performance on ReVOS, surpassing VISA by 5.9%/12.5%/9.1% in J&F scores across the three subsets. These results highlight the strong temporal reasoning and segmentation capabilities of our method. Code and model weights will be released at VRS-HQ.", 'abstract_zh': '现有的视频推理分割方法强烈依赖于单一特殊的token来表示关键帧或整个视频中的对象，未能充分捕捉空间复杂性和帧间运动。为克服这些挑战，我们提出了一种名为VRS-HQ的端到端视频推理分割方法，该方法利用多模态大型语言模型（MLLMs）将丰富的时空特征注入层次结构中。该方法的两大创新分别是时间动态聚合（TDA）和基于token的关键帧选择（TKS）。具体而言，我们设计了帧级<sEG>和时间级<sTAK> token，利用MLLM的自回归学习机制，有效捕捉局部和全局信息。之后，我们应用基于相似性的加权融合和帧选择策略，并利用SAM2执行关键帧分割和传播。为了提高关键帧定位的准确性，TKS在推理过程中根据SAM2的遮挡分数来筛选关键帧。VRS-HQ在ReVOS数据集上取得了最先进的性能，分别在三个子集的J&F分数上超越VISA 5.9%/12.5%/9.1%。这些结果突显了我们方法的强健时间推理和分割能力。VRS-HQ的相关代码和预训练模型权重将在未来公开。', 'title_zh': '时间_token中的魔鬼：高质量视频推理分割'}
{'arxiv_id': 'arXiv:2501.08540', 'title': 'Knowledge prompt chaining for semantic modeling', 'authors': 'Ning Pei Ding, Jingge Du, Zaiwen Feng', 'link': 'https://arxiv.org/abs/2501.08540', 'abstract': "The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field. Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge. Otherwise, the task will require human beings' effort and cost. In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining. It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture. Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally. Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data.", 'abstract_zh': '构建如CSV、JSON和XML等结构化数据的语义是一项在知识表示领域中高度相关的任务。尽管互联网上存在着大量的结构化数据，但将这些数据映射到领域本体以构建其语义仍然是一个极具挑战性的任务，因为它要求构建模型能够理解和学习图结构化的知识。否则，该任务将需要大量的人工努力和成本。在本文中，我们提出了一种新颖的自动语义建模框架：知识提示链（Knowledge Prompt Chaining）。该框架通过提示链结构化地序列化图结构化知识，并适当地注入到LLMs中。借助这种知识注入和提示链式工作机制，我们框架中的模型可以学习图的结构信息和潜在空间，并根据链的指令自然生成语义标签和语义图。根据实验结果，我们的方法在使用较少结构化输入数据的情况下，实现了比现有领先技术更好的性能。', 'title_zh': '知识提示链式方法用于语义建模'}
{'arxiv_id': 'arXiv:2501.08528', 'title': 'Dynamic Portfolio Optimization via Augmented DDPG with Quantum Price Levels-Based Trading Strategy', 'authors': 'Runsheng Lin, Zihan Xing, Mingze Ma, Raymond S.T. Lee', 'link': 'https://arxiv.org/abs/2501.08528', 'abstract': 'With the development of deep learning, Dynamic Portfolio Optimization (DPO) problem has received a lot of attention in recent years, not only in the field of finance but also in the field of deep learning. Some advanced research in recent years has proposed the application of Deep Reinforcement Learning (DRL) to the DPO problem, which demonstrated to be more advantageous than supervised learning in solving the DPO problem. However, there are still certain unsolved issues: 1) DRL algorithms usually have the problems of slow learning speed and high sample complexity, which is especially problematic when dealing with complex financial data. 2) researchers use DRL simply for the purpose of obtaining high returns, but pay little attention to the problem of risk control and trading strategy, which will affect the stability of model returns. In order to address these issues, in this study we revamped the intrinsic structure of the model based on the Deep Deterministic Policy Gradient (DDPG) and proposed the Augmented DDPG model. Besides, we also proposed an innovative risk control strategy based on Quantum Price Levels (QPLs) derived from Quantum Finance Theory (QFT). Our experimental results revealed that our model has better profitability as well as risk control ability with less sample complexity in the DPO problem compared to the baseline models.', 'abstract_zh': '随着深度学习的发展，近年来动态投资组合优化（DPO）问题在金融和深度学习领域都得到了广泛关注。一些最新的先进研究提出了将深度强化学习（DRL）应用于DPO问题的方法，显示出在解决DPO问题上比监督学习更具优势。然而，仍然存在一些未解决的问题：1）DRL算法通常面临学习速度慢和样本复杂度高的问题，尤其是在处理复杂金融数据时更为突出；2）研究人员仅将DRL用于获得高收益，而很少关注风险控制和交易策略的问题，这将影响模型收益的稳定性。为了解决这些问题，在本研究中，我们基于深度确定性策略梯度（DDPG）重构了模型的基本结构，并提出了一种增强型DDPG模型。此外，我们还基于量子金融理论（QFT）中的量子价格水平（QPLs）提出了一个创新的风险控制策略。实验结果表明，我们的模型在DPO问题上具有更好的盈利能力和风险控制能力，且样本复杂度较低。', 'title_zh': '基于量子价格层级交易策略的增强DDPG动态投资组合优化'}
{'arxiv_id': 'arXiv:2501.08523', 'title': 'Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation', 'authors': 'Jiaxin Guo, Yuanchang Luo, Daimeng Wei, Ling Zhang, Zongyao Li, Hengchao Shang, Zhiqiang Rao, Shaojun Li, Jinlong Yang, Zhanglin Wu, Hao Yang', 'link': 'https://arxiv.org/abs/2501.08523', 'abstract': 'The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \\textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.', 'abstract_zh': '人工智能领域在自然语言处理方面取得了显著进展，这主要归功于大规模语言模型（LLMs）的能力。这些模型构成了设计用于解决长时间段依赖性问题的代理的基础，特别是在文档级机器翻译（DocMT）方面。DocMT面临独特的挑战，质量、一致性和流畅性是其评价的关键指标。现有的方法如Doc2Doc和Doc2Sent要么省略句子，要么牺牲流畅性。本文介绍了Doc-Guided Sent2Sent++代理，该代理采用逐步句子级强制解码策略，以确保每个句子都能被翻译，并且通过增强相邻句子的流畅性来提升整体翻译质量。我们的代理利用Doc-Guided Memory机制，仅关注总结及其翻译，我们认为这是一种高效的方法，有助于保持一致性。通过在多种语言和领域中进行广泛的测试，我们证明Sent2Sent++在质量、一致性和流畅性方面均优于其他方法。结果表明，我们的方法在s-COMET、d-COMET、LTCR-$1_f$和文档级困惑度（d-ppl）等多项指标上取得了显著改进。本文的贡献包括对当前DocMT研究的详细分析、介绍Sent2Sent++解码方法、Doc-Guided Memory机制及其在不同语言和领域中有效性的验证。', 'title_zh': '文稿导向的 Sent2Sent++：一种带有文稿导向记忆的句子到句子增强代理模型用于文档级机器翻译'}
{'arxiv_id': 'arXiv:2501.08521', 'title': 'Mitigating Domain Shift in Federated Learning via Intra- and Inter-Domain Prototypes', 'authors': 'Huy Q. Le, Ye Lin Tun, Yu Qiao, Minh N. H. Nguyen, Keon Oh Kim, Choong Seon Hong', 'link': 'https://arxiv.org/abs/2501.08521', 'abstract': 'Federated Learning (FL) has emerged as a decentralized machine learning technique, allowing clients to train a global model collaboratively without sharing private data. However, most FL studies ignore the crucial challenge of heterogeneous domains where each client has a distinct feature distribution, which is common in real-world scenarios. Prototype learning, which leverages the mean feature vectors within the same classes, has become a prominent solution for federated learning under domain skew. However, existing federated prototype learning methods only consider inter-domain prototypes on the server and overlook intra-domain characteristics. In this work, we introduce a novel federated prototype learning method, namely I$^2$PFL, which incorporates $\\textbf{I}$ntra-domain and $\\textbf{I}$nter-domain $\\textbf{P}$rototypes, to mitigate domain shifts and learn a generalized global model across multiple domains in federated learning. To construct intra-domain prototypes, we propose feature alignment with MixUp-based augmented prototypes to capture the diversity of local domains and enhance the generalization of local features. Additionally, we introduce a reweighting mechanism for inter-domain prototypes to generate generalized prototypes to provide inter-domain knowledge and reduce domain skew across multiple clients. Extensive experiments on the Digits, Office-10, and PACS datasets illustrate the superior performance of our method compared to other baselines.', 'abstract_zh': '联邦学习（FL）作为一种去中心化的机器学习技术，允许客户端在不共享私有数据的情况下协作训练全球模型。然而，大多数FL研究忽略了每个客户端具有不同特征分布的异质领域这一关键挑战，这种情况在实际应用场景中非常常见。原型学习通过利用同一类别的均值特征向量，已成为解决联邦学习领域偏斜问题的主流解决方案。然而，现有的联邦原型学习方法只在服务器上考虑了跨域原型，而忽略了域内特征的特性。在本工作中，我们提出了一种新的联邦原型学习方法，即I$^2$PFL（Intra-domain and Inter-domain Prototype Federated Learning），该方法结合了域内和域间原型，以减轻领域偏斜并学习多个领域中的泛化全球模型。为了构建域内原型，我们提出了一种基于MixUp的特征对齐方法来捕捉本地域的多样性，并增强局部特征的一般化能力。此外，我们引入了一种跨域原型的重权机制，以生成泛化的原型，提供跨域知识并降低多个客户端间的领域偏斜。在Digits、Office-10和PACS数据集上的广泛实验表明，我们的方法在与其他基线方法的性能比较中具有显著优势。', 'title_zh': '通过域内和域间原型方法缓解联邦学习中的域迁移问题'}
{'arxiv_id': 'arXiv:2501.08518', 'title': 'Easing Seasickness through Attention Redirection with a Mindfulness-Based Brain--Computer Interface', 'authors': 'Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li', 'link': 'https://arxiv.org/abs/2501.08518', 'abstract': 'Seasickness is a prevalent issue that adversely impacts both passenger experiences and the operational efficiency of maritime crews. While techniques that redirect attention have proven effective in alleviating motion sickness symptoms in terrestrial environments, applying similar strategies to manage seasickness poses unique challenges due to the prolonged and intense motion environment associated with maritime travel. In this study, we propose a mindfulness brain-computer interface (BCI), specifically designed to redirect attention with the aim of mitigating seasickness symptoms in real-world settings. Our system utilizes a single-channel headband to capture prefrontal EEG signals, which are then wirelessly transmitted to computing devices for the assessment of mindfulness states. The results are transferred into real-time feedback as mindfulness scores and audiovisual stimuli, facilitating a shift in attentional focus from physiological discomfort to mindfulness practices. A total of 43 individuals participated in a real-world maritime experiment consisted of three sessions: a real-feedback mindfulness session, a resting session, and a pseudofeedback mindfulness session. Notably, 81.39% of participants reported that the mindfulness BCI intervention was effective, and there was a significant reduction in the severity of seasickness, as measured by the Misery Scale (MISC). Furthermore, EEG analysis revealed a decrease in the theta/beta ratio, corresponding with the alleviation of seasickness symptoms. A decrease in overall EEG band power during the real-feedback mindfulness session suggests that the mindfulness BCI fosters a more tranquil and downregulated state of brain activity. Together, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, with the potential to enhance the cruising experience for both passengers and crews.', 'abstract_zh': '晕船是一种普遍存在的问题，不仅影响乘客的体验，还影响海上船员的操作效率。尽管在陆地环境中能够有效缓解运动 sickness 症状的技术已经证明了其有效性，但在将相似策略应用于管理晕船方面仍面临独特挑战，因为海上旅行具有持续且剧烈的运动环境。在本研究中，我们提出了一种正念脑机接口（BCI），该接口专门设计用于重新引导注意力，以减轻实际情况下的晕船症状。我们的系统利用单一通道头带捕捉前额脑电图（EEG）信号，然后无线传输至计算设备以评估正念状态。结果通过正念评分和音频视觉刺激实时反馈，帮助将注意力焦点从生理不适转移到正念实践。共有43名参与者参加了包括三个会话的海上实验：即时反馈正念会话、休息会话和伪反馈正念会话。值得注意的是，81.39%的参与者表示正念BCI干预是有效的，并且晕船症状的严重程度根据Misery Scale（MISC）测量有显著减少。此外，EEG分析显示θ/β比值下降，这与缓解晕船症状相对应。在即时反馈正念会话期间EEG频带功率的整体下降表明，正念BCI促进了更安静和下调的大脑活动状态。综上所述，本研究提出了一种新颖的非药物、便携且有效的晕船干预方法，有望增强乘客和船员的乘船体验。', 'title_zh': '基于正念的脑-机接口通过注意力转移缓解 seasickness（晕海）'}
{'arxiv_id': 'arXiv:2501.08506', 'title': 'Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training', 'authors': 'Kavita Selva, Satita Vittayaareekul, Brando Miranda', 'link': 'https://arxiv.org/abs/2501.08506', 'abstract': 'Currently, data and model size dominate the narrative in the training of super-large, powerful models. However, there has been a lack of exploration on the effect of other attributes of the training dataset on model performance. We hypothesize that dataset diversity can impact the performance of vision models. Our study shows positive correlations between test set accuracy and data diversity, providing an argument for furthering the research of dataset attributes beyond size. We analyzed pre-training and model-agnostic meta-learning methods on twelve popular visual datasets (e.g., Omniglot, CIFAR-FS, Aircraft) and five model configurations, including MAML variants with different numbers of inner gradient steps and supervised learning. We show moderate to strong positive correlations (R-squared: 0.15-0.42) between accuracy and data diversity and weaker but significant correlations (R-squared: ~0.2) between loss and diversity. These findings support our hypothesis and demonstrate a promising way for a deeper exploration of how formal data diversity influences model performance. This initial study highlights the potential of (Task2Vec) data diversity as a valuable measure in the rapidly evolving field of large-scale learning and emphasizes that understanding the dataset is key to building more powerful and generalizable models.', 'abstract_zh': '当前，数据和模型规模在训练超级大型、强大模型的过程中占据了主导地位。然而，对训练数据集的其他属性对模型性能的影响缺乏探讨。我们推测数据集的多样性可能会对视觉模型的性能产生影响。我们的研究显示，测试集准确性与数据多样性之间存在正相关关系，这为研究超越规模的数据集属性提供了理由。我们在十二个流行的视觉数据集（例如Omniglot、CIFAR-FS、Aircraft）和五种模型配置上分析了预训练和模型无关元学习方法，包括不同内在梯度步数的MAML变体和监督学习。我们展示了准确性与数据多样性之间中等到较强的正相关关系（R方：0.15-0.42），以及损失与多样性的较弱但显著相关关系（R方：约0.2）。这些发现支持了我们的假设，并展示了进一步探索正式数据多样如何影响模型性能的有希望途径。这项初步研究突显了数据多样性（例如Task2Vec）作为快速发展的大规模学习领域的有价值的度量指标的潜力，并强调了理解数据集是构建更强大和更具泛化能力模型的关键。', 'title_zh': '探讨元学习的有效性：揭示MAML在数据多样性利用方面优于预训练的优势'}
{'arxiv_id': 'arXiv:2501.08502', 'title': 'Adapting Whisper for Regional Dialects: Enhancing Public Services for Vulnerable Populations in the United Kingdom', 'authors': 'Melissa Torgbi, Andrew Clayman, Jordan J. Speight, Harish Tayyar Madabushi', 'link': 'https://arxiv.org/abs/2501.08502', 'abstract': 'We collect novel data in the public service domain to evaluate the capability of the state-of-the-art automatic speech recognition (ASR) models in capturing regional differences in accents in the United Kingdom (UK), specifically focusing on two accents from Scotland with distinct dialects. This study addresses real-world problems where biased ASR models can lead to miscommunication in public services, disadvantaging individuals with regional accents particularly those in vulnerable populations. We first examine the out-of-the-box performance of the Whisper large-v3 model on a baseline dataset and our data. We then explore the impact of fine-tuning Whisper on the performance in the two UK regions and investigate the effectiveness of existing model evaluation techniques for our real-world application through manual inspection of model errors. We observe that the Whisper model has a higher word error rate (WER) on our test datasets compared to the baseline data and fine-tuning on a given data improves performance on the test dataset with the same domain and accent. The fine-tuned models also appear to show improved performance when applied to the test data outside of the region it was trained on suggesting that fine-tuned models may be transferable within parts of the UK. Our manual analysis of model outputs reveals the benefits and drawbacks of using WER as an evaluation metric and fine-tuning to adapt to regional dialects.', 'abstract_zh': '我们在公共服务领域收集了新型数据，以评估当前最先进的自动语音识别（ASR）模型在捕捉英国（UK）地方口音差异方面的能力，特别是着重于苏格兰两种具有明显方言差异的口音。本研究解决了实际问题，即有偏见的ASR模型可能导致公共服务中的沟通障碍，特别是在对地方口音持有特定态度的弱势群体中。我们首先考察了Whisper大型v3模型在基准数据集和我们数据集上的开箱即用性能。然后，我们探讨了对Whisper进行微调对苏格兰两地的性能影响，并通过人工检查模型错误来研究现有模型评价技术在我们的实际应用中的有效性。我们发现，与基准数据相比，Whisper模型在我们的测试数据集上的单词错误率（WER）较高，而在同一领域和口音的数据上进行微调可以改善测试数据集的性能。微调后的模型在应用于训练区域以外的测试数据时也表现出性能提升，这表明微调后的模型在英国部分地区可能是可迁移的。我们手动分析模型输出的结果揭示了使用WER作为评价指标和通过微调适应地方方言的好处与局限性。', 'title_zh': '适配地方dialect的Whisper：增强英国脆弱群体的公共服务 Kg\n\n注：这里的“dialect”指的是地方方言，“Whisper”通常指的是一种语音识别模型。在翻译时，考虑到学术规范和中文读者的习惯，可以将“dialect”翻译为“方言”，如果是具体指的某个已知的模型，保留“Whisper”即可。完整的翻译如下：\n\n适配地方方言的Whisper：增强英国脆弱群体的公共服务'}
{'arxiv_id': 'arXiv:2501.08496', 'title': 'Quantifying the Importance of Data Alignment in Downstream Model Performance', 'authors': 'Krrish Chawla, Aryan Sahai, Mario DePavia, Sudharsan Sundar, Brando Miranda', 'link': 'https://arxiv.org/abs/2501.08496', 'abstract': "Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \\textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.", 'abstract_zh': '与传统重视数据集规模的做法相反，我们探讨了数据对齐——这是一个经常被忽视的数据质量方面——在训练强大语言模型（LLM）中的作用。为此，我们使用基于Task2Vec的数据对齐系数，这是一种衡量两个数据集相似性的量化指标，来量化训练数据与评估数据对齐程度对下游性能的影响。具体而言，我们在两种设置下进行受控的干预实验：1. 增加预训练数据（pt）与评估数据之间的对齐系数的影响；2. 增加特定领域微调数据（ft）与特定领域评估数据之间的对齐系数的影响。我们探索的具体任务是自形式化——自然语言和代码之间的机器翻译任务，用于形式验证。在两种设置下，我们发现模型的训练数据和评估数据之间的对齐系数与其在相应下游任务上的损失/困惑度之间存在很强且可预见的负相关关系。这些发现建议重新评估LLM的训练方法，表明数据对齐相对于数据量的重要性，尤其是在诸如自形式化这类专门的下游任务中尤为重要。', 'title_zh': '量化数据对齐在下游模型性能中的重要性'}
{'arxiv_id': 'arXiv:2501.08471', 'title': 'Benchmarking Classical, Deep, and Generative Models for Human Activity Recognition', 'authors': 'Md Meem Hossain, Anh Han, Safina Showkat Ara, Zia Ush Shamszaman', 'link': 'https://arxiv.org/abs/2501.08471', 'abstract': 'Human Activity Recognition (HAR) has gained significant importance with the growing use of sensor-equipped devices and large datasets. This paper evaluates the performance of three categories of models : classical machine learning, deep learning architectures, and Restricted Boltzmann Machines (RBMs) using five key benchmark datasets of HAR (UCI-HAR, OPPORTUNITY, PAMAP2, WISDM, and Berkeley MHAD). We assess various models, including Decision Trees, Random Forests, Convolutional Neural Networks (CNN), and Deep Belief Networks (DBNs), using metrics such as accuracy, precision, recall, and F1-score for a comprehensive comparison. The results show that CNN models offer superior performance across all datasets, especially on the Berkeley MHAD. Classical models like Random Forest do well on smaller datasets but face challenges with larger, more complex data. RBM-based models also show notable potential, particularly for feature learning. This paper offers a detailed comparison to help researchers choose the most suitable model for HAR tasks.', 'abstract_zh': '人体活动识别（HAR）随着感应设备的广泛使用和大数据集的增多而变得尤为重要。本文评估了三类模型在五个关键基准数据集上的性能：经典机器学习模型、深度学习架构和受限玻尔兹曼机（RBM）。这些数据集包括UCI-HAR、OPPORTUNITY、PAMAP2、WISDM和伯克利MHAD。我们使用准确率、精确率、召回率和F1分数等指标，评估了包括决策树、随机森林、卷积神经网络（CNN）和深度信念网络（DBN）在内的多种模型，进行全面比较。结果显示，CNN模型在所有数据集上均表现出优越的性能，尤其是在伯克利MHAD数据集上。经典模型如随机森林在较小的数据集上表现良好，但在更大、更复杂的数据集上面临挑战。基于RBM的模型在特征学习方面也显示出明显的潜力。本文提供了详细的比较，以帮助研究人员选择最适合HAR任务的模型。', 'title_zh': '经典、深度学习和生成模型在人体活动识别中的基准研究'}
{'arxiv_id': 'arXiv:2501.08470', 'title': 'Detecting Contextual Anomalies by Discovering Consistent Spatial Regions', 'authors': 'Zhengye Yang, Richard J. Radke', 'link': 'https://arxiv.org/abs/2501.08470', 'abstract': 'We describe a method for modeling spatial context to enable video anomaly detection. The main idea is to discover regions that share similar object-level activities by clustering joint object attributes using Gaussian mixture models. We demonstrate that this straightforward approach, using orders of magnitude fewer parameters than competing models, achieves state-of-the-art performance in the challenging spatial-context-dependent Street Scene dataset. As a side benefit, the high-resolution discovered regions learned by the model also provide explainable normalcy maps for human operators without the need for any pre-trained segmentation model.', 'abstract_zh': '我们描述了一种用于建模空间上下文以实现视频异常检测的方法。主要思想是通过使用高斯混合模型聚类联合对象属性来发现具有相似对象级活动的区域。实验表明，这种方法使用比竞争模型少几个数量级的参数，在具有挑战性的空间上下文依赖的街道场景数据集中达到了最先进的性能。作为一种附带好处，模型学习到的高分辨率发现区域也为人类操作员提供了可解释的正常性映射，而无需任何预先训练的分割模型。', 'title_zh': '通过发现一致的空间区域来检测上下文异常'}
{'arxiv_id': 'arXiv:2501.08466', 'title': 'A Short-Term Predict-Then-Cluster Framework for Meal Delivery Services', 'authors': 'Jingyi Cheng, Shadi Sharif Azadeh', 'link': 'https://arxiv.org/abs/2501.08466', 'abstract': 'Micro-delivery services offer promising solutions for on-demand city logistics, but their success relies on efficient real-time delivery operations and fleet management. On-demand meal delivery platforms seek to optimize real-time operations based on anticipatory insights into citywide demand distributions. To address these needs, this study proposes a short-term predict-then-cluster framework for on-demand meal delivery services. The framework utilizes ensemble-learning methods for point and distributional forecasting with multivariate features, including lagged-dependent inputs to capture demand dynamics. We introduce Constrained K-Means Clustering (CKMC) and Contiguity Constrained Hierarchical Clustering with Iterative Constraint Enforcement (CCHC-ICE) to generate dynamic clusters based on predicted demand and geographical proximity, tailored to user-defined operational constraints. Evaluations of European and Taiwanese case studies demonstrate that the proposed methods outperform traditional time series approaches in both accuracy and computational efficiency. Clustering results demonstrate that the incorporation of distributional predictions effectively addresses demand uncertainties, improving the quality of operational insights. Additionally, a simulation study demonstrates the practical value of short-term demand predictions for proactive strategies, such as idle fleet rebalancing, significantly enhancing delivery efficiency. By addressing demand uncertainties and operational constraints, our predict-then-cluster framework provides actionable insights for optimizing real-time operations. The approach is adaptable to other on-demand platform-based city logistics and passenger mobility services, promoting sustainable and efficient urban operations.', 'abstract_zh': '基于实时需求预测的微交付服务为城市的即时物流提供了有前景的解决方案，但其成功依赖于高效的实时配送运营和车队管理。即时餐食配送平台致力于通过前瞻性洞见优化城市需求分布的实时运营。为满足这一需求，本研究提出了一种短期预测-聚类框架来优化即时餐食配送服务。该框架利用集成学习方法进行点预测和分布预测，采用多元特征，包括滞后的相关输入以捕捉需求动态。我们引入了受约束的K均值聚类（CKMC）和迭代约束条件下的连续约束层次聚类（CCHC-ICE），基于预测需求和地理接近性生成动态聚类，符合用户定义的操作约束。欧洲和台湾案例研究的评估结果表明，所提出的方法在准确性和计算效率方面均优于传统的时序分析方法。聚类结果表明，整合分布预测有效地应对了需求不确定性，提高了操作洞察的质量。此外，仿真研究还证明了短期需求预测对预防性策略（如闲置车队再平衡）的实际价值，显著提高了配送效率。通过解决需求不确定性与操作约束，我们的预测-聚类框架为优化实时运营提供了可操作性的洞察。该方法适用于其他基于即时平台的城市物流和乘客移动服务，促进可持续和高效的都市运营。', 'title_zh': '短期预测-聚类框架：用于餐饮配送服务'}
{'arxiv_id': 'arXiv:2501.08460', 'title': 'Towards Zero-Shot & Explainable Video Description by Reasoning over Graphs of Events in Space and Time', 'authors': 'Mihai Masala, Marius Leordeanu', 'link': 'https://arxiv.org/abs/2501.08460', 'abstract': 'In the current era of Machine Learning, Transformers have become the de facto approach across a variety of domains, such as computer vision and natural language processing. Transformer-based solutions are the backbone of current state-of-the-art methods for language generation, image and video classification, segmentation, action and object recognition, among many others. Interestingly enough, while these state-of-the-art methods produce impressive results in their respective domains, the problem of understanding the relationship between vision and language is still beyond our reach. In this work, we propose a common ground between vision and language based on events in space and time in an explainable and programmatic way, to connect learning-based vision and language state of the art models and provide a solution to the long standing problem of describing videos in natural language. We validate that our algorithmic approach is able to generate coherent, rich and relevant textual descriptions on videos collected from a variety of datasets, using both standard metrics (e.g. Bleu, ROUGE) and the modern LLM-as-a-Jury approach.', 'abstract_zh': '在当前的机器学习时代，变换器已经成为各种领域中的主流方法，包括计算机视觉和自然语言处理。基于变换器的解决方案是当前语言生成、图像和视频分类、分割、动作和物体识别等多种先进方法的核心。有趣的是，虽然这些最先进的方法在其各自的领域中取得了令人印象深刻的成果，但我们仍然无法理解视觉与语言之间关系的问题。本研究中，我们提出了一种基于时空事件的视觉与语言联系方法，以可解释且编程化的方式连接基于学习的视觉和语言先进模型，并提供了一个用自然语言描述视频的解决方案。我们验证了我们的算法方法能够在来自各种数据集的视频中生成连贯、丰富且相关性的文本描述，使用标准指标（如Bleu、ROUGE）以及现代的LLM作为评审员的方法进行验证。', 'title_zh': '面向空间与时间中事件图推理的零样本可解释视频描述研究'}
{'arxiv_id': 'arXiv:2501.08440', 'title': 'FARE: A Deep Learning-Based Framework for Radar-based Face Recognition and Out-of-distribution Detection', 'authors': 'Sabri Mustafa Kahya, Boran Hamdi Sivrikaya, Muhammet Sami Yavuz, Eckehard Steinbach', 'link': 'https://arxiv.org/abs/2501.08440', 'abstract': 'In this work, we propose a novel pipeline for face recognition and out-of-distribution (OOD) detection using short-range FMCW radar. The proposed system utilizes Range-Doppler and micro Range-Doppler Images. The architecture features a primary path (PP) responsible for the classification of in-distribution (ID) faces, complemented by intermediate paths (IPs) dedicated to OOD detection. The network is trained in two stages: first, the PP is trained using triplet loss to optimize ID face classification. In the second stage, the PP is frozen, and the IPs-comprising simple linear autoencoder networks-are trained specifically for OOD detection. Using our dataset generated with a 60 GHz FMCW radar, our method achieves an ID classification accuracy of 99.30% and an OOD detection AUROC of 96.91%.', 'abstract_zh': '在本工作中，我们提出了一种基于短距离FMCW雷达的面部识别和离群域（OOD）检测的新管道。所提出的系统利用了Range-Doppler和微Range-Doppler图像。该架构包括一个主路径（PP），负责ID面部的分类，以及专门用于OOD检测的中间路径（IPs）。网络在两个阶段进行训练：首先，使用三元损失训练PP以优化ID面部分类。在第二阶段，冻结PP，并训练IPs（由简单的线性自动编码器网络组成），专门用于OOD检测。使用60 GHz FMCW雷达生成的数据集，我们的方法实现了99.30%的ID分类准确率和96.91%的OOD检测AUC-ROC值。', 'title_zh': 'FARE：一种基于深度学习的雷达faces识别及异常分布检测框架'}
{'arxiv_id': 'arXiv:2501.08429', 'title': 'Modeling Discrimination with Causal Abstraction', 'authors': 'Milan Mossé, Kara Schechtman, Frederick Eberhardt, Thomas Icard', 'link': 'https://arxiv.org/abs/2501.08429', 'abstract': 'A person is directly racially discriminated against only if her race caused her worse treatment. This implies that race is an attribute sufficiently separable from other attributes to isolate its causal role. But race is embedded in a nexus of social factors that resist isolated treatment. If race is socially constructed, in what sense can it cause worse treatment? Some propose that the perception of race, rather than race itself, causes worse treatment. Others suggest that since causal models require modularity, i.e. the ability to isolate causal effects, attempts to causally model discrimination are misguided.\nThis paper addresses the problem differently. We introduce a framework for reasoning about discrimination, in which race is a high-level abstraction of lower-level features. In this framework, race can be modeled as itself causing worse treatment. Modularity is ensured by allowing assumptions about social construction to be precisely and explicitly stated, via an alignment between race and its constituents. Such assumptions can then be subjected to normative and empirical challenges, which lead to different views of when discrimination occurs. By distinguishing constitutive and causal relations, the abstraction framework pinpoints disagreements in the current literature on modeling discrimination, while preserving a precise causal account of discrimination.', 'abstract_zh': '一个人只有在其种族导致她受到更差的对待时，才会直接遭受种族歧视。这表明种族作为一种属性，必须能够与其他属性分离开来，以便隔离其因果作用。然而，种族嵌入在一系列难以孤立处理的社会因素之中。如果种族是社会建构的产物，那么它如何能够导致更差的对待？有些人提出，是种族感知而非种族本身导致了更差的对待。另一些人则认为，因为因果模型需要模ularity，即能够独立隔离因果效应，所以尝试建立因果模型的歧视论点是不合时宜的。\n\n本文则从不同角度解决了这一问题。我们引入了一种推理框架，其中种族是较低层次特征的高层抽象。在这一框架中，我们可以将种族视为导致更差对待的直接原因。通过允许关于社会建构的假设可以精确且明确地表述，即通过种族与其组成部分之间的对齐，可以确保模ularity。这样的假设随后可以接受规范性和经验性的挑战，从而得出不同的歧视发生的时间点。通过区分构成性和因果性关系，这种抽象框架不仅指出了当前关于建模歧视文献中存在分歧的地方，同时也保持了对歧视的精准因果描述。', 'title_zh': '使用因果抽象建模歧视'}
{'arxiv_id': 'arXiv:2501.08426', 'title': 'Causal vs. Anticausal merging of predictors', 'authors': 'Sergio Hernan Garrido Mejia, Patrick Blöbaum, Bernhard Schölkopf, Dominik Janzing', 'link': 'https://arxiv.org/abs/2501.08426', 'abstract': 'We study the differences arising from merging predictors in the causal and anticausal directions using the same data. In particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors. We use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect. We show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction. Furthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.', 'abstract_zh': '我们研究在使用相同数据进行预测器合并时，从因果方向和反因果方向所产生的差异。具体而言，我们研究在使用一个二分变量作为目标变量和两个连续变量作为预测变量的简单模型中出现的不对称性。我们使用因果最大熵（CMAXENT）作为归纳偏倚来合并预测器，然而，我们预期当使用考虑因果与效应之间不对称性的其他合并方法时也会出现类似差异。我们表明，如果观察所有双变量分布，CMAXENT 解决方案在因果方向上简化为逻辑回归，在反因果方向上简化为线性判别分析（LDA）。此外，我们研究仅观察某些双变量分布时，这两个解决方案的决策边界如何不同，以及这对变量外泛化（OOV generalisation）的影响。', 'title_zh': '因果合并与反因果合并预测者比较'}
{'arxiv_id': 'arXiv:2501.08421', 'title': 'SEAL: Speaker Error Correction using Acoustic-conditioned Large Language Models', 'authors': 'Anurag Kumar, Rohit Paturi, Amber Afshan, Sundararajan Srinivasan', 'link': 'https://arxiv.org/abs/2501.08421', 'abstract': 'Speaker Diarization (SD) is a crucial component of modern end-to-end ASR pipelines. Traditional SD systems, which are typically audio-based and operate independently of ASR, often introduce speaker errors, particularly during speaker transitions and overlapping speech. Recently, language models including fine-tuned large language models (LLMs) have shown to be effective as a second-pass speaker error corrector by leveraging lexical context in the transcribed output. In this work, we introduce a novel acoustic conditioning approach to provide more fine-grained information from the acoustic diarizer to the LLM. We also show that a simpler constrained decoding strategy reduces LLM hallucinations, while avoiding complicated post-processing. Our approach significantly reduces the speaker error rates by 24-43% across Fisher, Callhome, and RT03-CTS datasets, compared to the first-pass Acoustic SD.', 'abstract_zh': '说话人分离（SD）是现代端到端ASR流水线中的一个关键组件。传统的SD系统通常是基于音频的，并且独立于ASR系统工作，这在说话人转换和重叠语音时容易引入说话人错误。最近，包括微调的大语言模型（LLMs）已被证明能够通过利用转录输出中的词汇上下文，作为第二阶段的说话人错误校正工具，达到有效的作用。在本工作中，我们提出了一种新的声学条件编码方法，以向LLM提供更细致的信息。我们还展示了简单的约束解码策略减少了LLM的幻觉，同时避免了复杂的后处理步骤。与第一次的声学SD相比，我们的方法在Fisher、Callhome和RT03-CTS数据集上将说话人错误率显著降低了24-43%。', 'title_zh': 'SEAL：基于声学条件的大语言模型 speaker 错误校正'}
{'arxiv_id': 'arXiv:2501.08418', 'title': 'CVaR-Based Variational Quantum Optimization for User Association in Handoff-Aware Vehicular Networks', 'authors': 'Zijiang Yan, Hao Zhou, Jianhua Pei, Aryan Kaushik, Hina Tabassum, Ping Wang', 'link': 'https://arxiv.org/abs/2501.08418', 'abstract': 'Efficient resource allocation is essential for optimizing various tasks in wireless networks, which are usually formulated as generalized assignment problems (GAP). GAP, as a generalized version of the linear sum assignment problem, involves both equality and inequality constraints that add computational challenges. In this work, we present a novel Conditional Value at Risk (CVaR)-based Variational Quantum Eigensolver (VQE) framework to address GAP in vehicular networks (VNets). Our approach leverages a hybrid quantum-classical structure, integrating a tailored cost function that balances both objective and constraint-specific penalties to improve solution quality and stability. Using the CVaR-VQE model, we handle the GAP efficiently by focusing optimization on the lower tail of the solution space, enhancing both convergence and resilience on noisy intermediate-scale quantum (NISQ) devices. We apply this framework to a user-association problem in VNets, where our method achieves 23.5% improvement compared to the deep neural network (DNN) approach.', 'abstract_zh': '有效资源分配对于优化无线网络中的各种任务至关重要，通常被形式化为广义分配问题（GAP）。GAP 是线性和分配问题的广义版本，包含等式和不等式约束，为计算带来了挑战。在此工作中，我们提出了一种基于条件价值风险（CVaR）的变量化本证求解器（VQE）框架，以解决车辆网络中的GAP问题。我们的方法利用了量子和经典计算的混合结构，并结合了一个定制化的代价函数，该函数平衡了目标函数和约束条件特定惩罚，以提高解的质量和稳定性。利用CVaR-VQE模型，我们通过关注解空间的低端部分高效地解决了GAP，从而在嘈杂的中尺度量子（NISQ）设备上增强了收敛性和鲁棒性。我们将该框架应用于车辆网络中的用户关联问题，结果显示，与深度神经网络（DNN）方法相比，我们的方法改善了23.5%。', 'title_zh': '基于CVaR的变分量子优化方法在切换感知车联网中的用户关联'}
{'arxiv_id': 'arXiv:2501.08416', 'title': 'A Survey on Recent Advances in Self-Organizing Maps', 'authors': 'Axel Guérin, Pierre Chauvet, Frédéric Saubion', 'link': 'https://arxiv.org/abs/2501.08416', 'abstract': "Self-organising maps are a powerful tool for cluster analysis in a wide range of data contexts. From the pioneer work of Kohonen, many variants and improvements have been proposed. This review focuses on the last decade, in order to provide an overview of the main evolution of the seminal SOM algorithm as well as of the methodological developments that have been achieved in order to better fit to various application contexts and users' requirements. We also highlight a specific and important application field that is related to commercial use of SOM, which involves specific data management.", 'abstract_zh': '自组织映射是一种广泛数据背景下进行聚类分析的强大工具。从Kohonen的开创性工作以来，已经提出了许多变体和改进。本综述重点回顾了过去十年的发展，旨在概述经典的SOM算法的主要演变以及为更好地适应各种应用背景和用户需求而取得的方法学进展。我们还特别强调了一个重要且具体的应用领域，即SOM在商业中的应用，该领域涉及特定的数据管理。', 'title_zh': '近年来自组织映射的进展综述'}
{'arxiv_id': 'arXiv:2501.08415', 'title': 'Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics', 'authors': 'Georgii Gotin, Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin', 'link': 'https://arxiv.org/abs/2501.08415', 'abstract': 'Recent studies have revealed that modern image and video quality assessment (IQA/VQA) metrics are vulnerable to adversarial attacks. An attacker can manipulate a video through preprocessing to artificially increase its quality score according to a certain metric, despite no actual improvement in visual quality. Most of the attacks studied in the literature are white-box attacks, while black-box attacks in the context of VQA have received less attention. Moreover, some research indicates a lack of transferability of adversarial examples generated for one model to another when applied to VQA. In this paper, we propose a cross-modal attack method, IC2VQA, aimed at exploring the vulnerabilities of modern VQA models. This approach is motivated by the observation that the low-level feature spaces of images and videos are similar. We investigate the transferability of adversarial perturbations across different modalities; specifically, we analyze how adversarial perturbations generated on a white-box IQA model with an additional CLIP module can effectively target a VQA model. The addition of the CLIP module serves as a valuable aid in increasing transferability, as the CLIP model is known for its effective capture of low-level semantics. Extensive experiments demonstrate that IC2VQA achieves a high success rate in attacking three black-box VQA models. We compare our method with existing black-box attack strategies, highlighting its superiority in terms of attack success within the same number of iterations and levels of attack strength. We believe that the proposed method will contribute to the deeper analysis of robust VQA metrics.', 'abstract_zh': '近年来的研究表明，现代图像和视频质量评估（IQA/VQA）指标容易受到对抗性攻击的影响。攻击者可以通过预处理手段人为地提升视频的评分，尽管视觉质量并没有实际改进。文献中大多数研究关注的是白盒攻击，而视频质量评估（VQA）领域的黑盒攻击则受到了较少的关注。此外，一些研究指出，针对一种模型生成的对抗性样本在应用于VQA时并不具备很好的迁移性。在本文中，我们提出了一种跨模态攻击方法IC2VQA，旨在探索现代VQA模型的脆弱性。这一方法受到图像和视频的低层特征空间相似性的观察启发。我们研究了不同模态之间对抗性扰动的迁移性；具体来说，我们分析了在带有附加CLIP模块的白盒IQA模型上生成的对抗性扰动如何有效针对VQA模型。CLIP模块的添加有助于提高迁移性，因为该模型擅长捕捉低层语义。广泛的实验表明，IC2VQA方法在攻击三个黑盒VQA模型时具有较高的成功率。我们将我们的方法与现有的黑盒攻击策略进行了比较，突出了其在相同迭代次数和攻击强度下的优越性。我们认为，所提出的方法将有助于更深入地分析鲁棒的VQA指标。', 'title_zh': '跨模态可转移的图像到视频攻击对视频质量度量的影响'}
{'arxiv_id': 'arXiv:2501.08411', 'title': 'BiDepth Multimodal Neural Network: Bidirectional Depth Deep Learning Arcitecture for Spatial-Temporal Prediction', 'authors': 'Sina Ehsani, Fenglian Pan, Qingpei Hu, Jian Liu', 'link': 'https://arxiv.org/abs/2501.08411', 'abstract': 'Accurate prediction of spatial-temporal (ST) information in dynamic systems, such as urban mobility and weather patterns, is a crucial yet challenging problem. The complexity stems from the intricate interplay between spatial proximity and temporal relevance, where both long-term trends and short-term fluctuations are present in convoluted patterns. Existing approaches, including traditional statistical methods and conventional neural networks, may provide inaccurate results due to the lack of an effective mechanism that simultaneously incorporates information at variable temporal depths while maintaining spatial context, resulting in a trade-off between comprehensive long-term historical analysis and responsiveness to short-term new information. To bridge this gap, this paper proposes the BiDepth Multimodal Neural Network (BDMNN) with bidirectional depth modulation that enables a comprehensive understanding of both long-term seasonality and short-term fluctuations, adapting to the complex ST context. Case studies with real-world public data demonstrate significant improvements in prediction accuracy, with a 12% reduction in Mean Squared Error for urban traffic prediction and a 15% improvement in rain precipitation forecasting compared to state-of-the-art benchmarks, without demanding extra computational resources.', 'abstract_zh': '动态系统（如城市交通和天气模式）中的时空（ST）信息准确预测是一个至关重要的但具有挑战性的问题。这种复杂性源于空间临近性和时间相关性之间的复杂互动，其中既存在长期趋势也存在短期波动，这些都交织在复杂的模式中。现有的方法，包括传统的统计方法和传统的神经网络，可能因缺乏同时整合不同时间深度信息的有效机制而难以兼顾空间上下文，导致长期历史分析全面性和对短期新信息反应性之间的权衡。为解决这一问题，本文提出了一种双向深度调制的双深度多模态神经网络（BDMNN），该方法能够综合理解长期季节性和短期波动，适应复杂的时空上下文。通过对实际公共数据进行案例研究，实验结果表明，该方法在预测准确性上取得了显著改进：在城市交通预测中，均方误差降低了12%；在降雨量预测中，准确率提高了15%。此外，该方法并未增加额外的计算资源需求。', 'title_zh': '双向深度多模态神经网络：时空预测的双向深度学习架构'}
{'arxiv_id': 'arXiv:2501.08402', 'title': 'Addressing Quality Challenges in Deep Learning: The Role of MLOps and Domain Knowledge', 'authors': 'Santiago del Rey, Adrià Medina, Xavier Franch, Silverio Martínez-Fernández', 'link': 'https://arxiv.org/abs/2501.08402', 'abstract': 'Deep learning (DL) systems present unique challenges in software engineering, especially concerning quality attributes like correctness and resource efficiency. While DL models achieve exceptional performance in specific tasks, engineering DL-based systems is still essential. The effort, cost, and potential diminishing returns of continual improvements must be carefully evaluated, as software engineers often face the critical decision of when to stop refining a system relative to its quality attributes. This experience paper explores the role of MLOps practices -- such as monitoring and experiment tracking -- in creating transparent and reproducible experimentation environments that enable teams to assess and justify the impact of design decisions on quality attributes. Furthermore, we report on experiences addressing the quality challenges by embedding domain knowledge into the design of a DL model and its integration within a larger system. The findings offer actionable insights into not only the benefits of domain knowledge and MLOps but also the strategic consideration of when to limit further optimizations in DL projects to maximize overall system quality and reliability.', 'abstract_zh': '深度学习（DL）系统在软件工程中提出了独特的挑战，特别是在准确性和资源效率等质量属性方面。尽管DL模型在特定任务上取得了出色的性能，但基于DL的系统工程仍然是必要的。需要谨慎评估持续改进所需的努力、成本以及潜在的边际收益递减，因为软件工程师经常面临在系统质量属性与精炼之间做出关键决策的挑战。本文经验性地探讨了MLOps实践——如监控和实验跟踪——在创建透明和可重复的实验环境方面的作用，这些环境能够帮助团队评估和解释设计决策对质量属性的影响。此外，我们还报告了将领域知识嵌入DL模型设计及其与更大系统集成以应对质量挑战的经验。研究结果不仅提供了有关领域知识和MLOps益处的实用见解，还提供了关于何时在DL项目中限制进一步优化以最大化整个系统质量和可靠性的重要策略考虑。', 'title_zh': '解决深度学习中的质量挑战：MLOps与领域知识的作用'}
{'arxiv_id': 'arXiv:2501.08365', 'title': 'Towards Best Practices for Open Datasets for LLM Training', 'authors': 'Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro von Werra, Mitchell Baker, Julie Belião, Kasia Chmielinski, Marzieh Fadaee, Lisa Gutermuth, Hynek Kydlíček, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Longpre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu, Andrew Strait, Mark Surman, Anna Tumadóttir, Maurice Weber, Rebecca Weiss, Lee White, Thomas Wolf', 'link': 'https://arxiv.org/abs/2501.08365', 'abstract': 'Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.\nWhile this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.', 'abstract_zh': '许多人工智能公司正在未经版权拥有人许可的情况下训练其大规模语言模型（LLMs）。这种做法在不同法域下的合法性各不相同：在欧盟和日本等国家，这种行为在特定条件下是被允许的，而在美国，法律环境则更为模糊。不论法律规定如何，创意生产者的顾虑已经导致了几起高-profile的版权诉讼，法律诉讼的威胁经常被用作近期趋势的佐证，即私营和公共利益方均倾向于减少公开分享训练数据集的信息。这种限制数据信息的趋势阻碍了更广泛生态系统中的透明度、问责制和创新，因为它剥夺了研究人员、审计师和受影响个体了解AI模型所需的信息。\n\n虽然可以通过使用开放访问和公共领域数据来训练语言模型来缓解这一问题，但截至撰写本文时，尚无大规模训练的此类模型，主要原因在于构建这样的数据集存在大量的技术和社会挑战。这些挑战包括不完整和不可靠的元数据、物理记录数字化的成本和复杂性，以及确保迅速变化环境中相关性和责任感所需的多样化的法律和技术技能。为了构建一个未来，其中的AI系统可以安全地基于负责任地策展和治理的开放许可数据进行训练，需要在法律、技术和政策领域进行合作，并投资于元数据标准、数字化和技术开放文化培育。', 'title_zh': '面向大规模语言模型训练的开放数据集最佳实践'}
{'arxiv_id': 'arXiv:2501.08347', 'title': 'SCOT: Self-Supervised Contrastive Pretraining For Zero-Shot Compositional Retrieval', 'authors': 'Bhavin Jawade, Joao V. B. Soares, Kapil Thadani, Deen Dayal Mohan, Amir Erfan Eshratifar, Benjamin Culpepper, Paloma de Juan, Srirangaraj Setlur, Venu Govindaraju', 'link': 'https://arxiv.org/abs/2501.08347', 'abstract': 'Compositional image retrieval (CIR) is a multimodal learning task where a model combines a query image with a user-provided text modification to retrieve a target image. CIR finds applications in a variety of domains including product retrieval (e-commerce) and web search. Existing methods primarily focus on fully-supervised learning, wherein models are trained on datasets of labeled triplets such as FashionIQ and CIRR. This poses two significant challenges: (i) curating such triplet datasets is labor intensive; and (ii) models lack generalization to unseen objects and domains. In this work, we propose SCOT (Self-supervised COmpositional Training), a novel zero-shot compositional pretraining strategy that combines existing large image-text pair datasets with the generative capabilities of large language models to contrastively train an embedding composition network. Specifically, we show that the text embedding from a large-scale contrastively-pretrained vision-language model can be utilized as proxy target supervision during compositional pretraining, replacing the target image embedding. In zero-shot settings, this strategy surpasses SOTA zero-shot compositional retrieval methods as well as many fully-supervised methods on standard benchmarks such as FashionIQ and CIRR.', 'abstract_zh': '组合图像检索（CIR）是一种多模态学习任务，其中模型将查询图像与用户提供的文本修改相结合，以检索目标图像。CIR 在产品检索（电子商务）和网络搜索等多个领域中具有应用潜力。现有方法主要侧重于全监督学习，其中模型是在标记三元组（如 FashionIQ 和 CIRR）的数据集上进行训练。这带来了两个重大挑战：（i）收集这样的三元组数据集需要大量的劳动投入；（ii）模型在未见过的对象和领域中缺乏泛化能力。在本工作中，我们提出了一种名为 SCOT（自我监督组合训练）的新型零样本组合预训练策略，该策略结合了现有的大规模图像-文本对数据集和大规模语言模型的生成能力，以对比性地训练嵌入组合网络。具体来说，我们展示了大规模对比性预训练的视觉-语言模型的文本嵌入可以在组合预训练过程中充当代理目标监督，替代目标图像嵌入。在零样本设置中，该策略在 FashionIQ 和 CIRR 等标准基准上超过了最先进的零样本组合检索方法以及许多全监督方法。', 'title_zh': 'SCOT：自我监督对比预训练在零样本组合检索中的应用'}
{'arxiv_id': 'arXiv:2501.08339', 'title': 'Operator Learning for Reconstructing Flow Fields from Sparse Measurements: an Energy Transformer Approach', 'authors': 'Qian Zhang, Dmitry Krotov, George Em Karniadakis', 'link': 'https://arxiv.org/abs/2501.08339', 'abstract': 'Machine learning methods have shown great success in various scientific areas, including fluid mechanics. However, reconstruction problems, where full velocity fields must be recovered from partial observations, remain challenging. In this paper, we propose a novel operator learning framework for solving reconstruction problems by using the Energy Transformer (ET), an architecture inspired by associative memory models. We formulate reconstruction as a mapping from incomplete observed data to full reconstructed fields. The method is validated on three fluid mechanics examples using diverse types of data: (1) unsteady 2D vortex street in flow past a cylinder using simulation data; (2) high-speed under-expanded impinging supersonic jets impingement using Schlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The results demonstrate the ability of ET to accurately reconstruct complex flow fields from highly incomplete data (90\\% missing), even for noisy experimental measurements, with fast training and inference on a single GPU. This work provides a promising new direction for tackling reconstruction problems in fluid mechanics and other areas in mechanics, geophysics, weather prediction, and beyond.', 'abstract_zh': '机器学习方法在各个科学领域已经取得了显著的成功，包括流体力学。然而，从部分观测数据重构完整流场的重建问题依然具有挑战性。本文提出了一种新的算子学习框架，通过使用灵感源于关联记忆模型的能量变换器（Energy Transformer, ET）来解决这类重建问题。我们将重建问题表述为从不完整观测数据到完整重构流场的映射。该方法在三个流体力学例子上进行了验证，使用了不同类型的观测数据：(1) 柱体绕流中的2D 不稳定涡街流动，使用模拟数据；(2) 高速膨胀波不匹配冲击超音速喷流撞击，使用Schlieren成像；(3) 三维湍流喷流流动，使用粒子追踪。实验结果显示，ET 能够从高度不完整的数据（90% 缺失）中准确重构复杂的流场，即使对于噪声较大的实验测量数据也不例外，并且在单张GPU上具有快速的训练和推理能力。本研究为流体力学及其他领域（如力学、地球物理学、天气预测等）中的重建问题提供了一个有前景的新方向。', 'title_zh': '从稀疏测量重构流场的运算器学习方法：一种能量变换器 approach'}
{'arxiv_id': 'arXiv:2501.08335', 'title': 'MERaLiON-TextLLM: Cross-Lingual Understanding of Large Language Models in Chinese, Indonesian, Malay, and Singlish', 'authors': 'Xin Huang, Tarun Kumar Vangani, Minh Duc Pham, Xunlong Zou, Bin Wang, Zhengyuan Liu, Ai Ti Aw', 'link': 'https://arxiv.org/abs/2501.08335', 'abstract': 'Multilingual large language models (MLLMs) have shown impressive capabilities across a variety of languages. However, efficacy can differ greatly between different language families, especially for those with limited linguistic resources. This report presents MERaLiON-TextLLM, a series of open-source language models specifically tailored to improve understanding and generation in Chinese, Indonesian, Malay, and Singlish. The initial released model is built on Llama-3-8B-Base and refined through a meticulously crafted process of continued pre-training and weight merging. Our approach achieves performance improvements across benchmarks in these languages, exceeding the capabilities of the official Llama-3 models. We provide the model checkpoints as a resource to support further research and development in cross-lingual language understanding.', 'abstract_zh': '多语言大型语言模型（MLLMs）在多种语言中展现了令人印象深刻的性能。然而，不同语言家族之间的效果差异巨大，尤其是对于那些资源有限的语言而言。本报告介绍了MERaLiON-TextLLM，这是一种专为提高中文、印尼语、马来语和 Singlish 理解与生成能力而设计的开源语言模型系列。初始发布的模型基于 Llama-3-8B-Base，并通过精心设计的持续预训练和权重合并过程进行优化。我们的方法在这些语言的基准测试中实现了性能改进，超越了官方发布的 Llama-3 模型的能力。我们提供了模型检查点，以支持跨语言语言理解领域的进一步研究和开发。', 'title_zh': 'MERaLiON-TextLLM：中文、印尼语、马来语和新加坡英语大型语言模型的跨语言理解'}
