{'arxiv_id': 'arXiv:2502.12150', 'title': 'Idiosyncrasies in Large Language Models', 'authors': 'Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu', 'link': 'https://arxiv.org/abs/2502.12150', 'abstract': "In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at this https URL.", 'abstract_zh': '在本研究中，我们揭示并探讨了大型语言模型（LLMs）中的独特特征——这些特征在它们的输出中呈现出独特的模式，可用于区分不同的模型。为此，我们考虑了一个简单的分类任务：给定一段特定的文本输出，目标是预测该文本由哪个LLM生成。我们跨多个LLM组评估了这一合成任务，并发现简单地将现有文本嵌入模型 fine-tune 在LLM生成的文本上就能获得优异的分类准确性。显著地，在涉及ChatGPT、Claude、Grok、Gemini和DeepSeek的五分类问题中，我们达到了97.1%的验证准确率。进一步的研究表明，这些独特特征根植于词级分布。即使这些文本经过外部LLM的重写、翻译或总结后，这些模式依然保持不变，这表明这些独特特征也编码在了语义内容中。此外，我们利用LLM作为评判者，生成了每个模型独特特征的详细、开放式描述。最后，我们讨论了研究结果的更广泛影响，特别是针对合成数据训练和推断模型相似性的意义。相关代码可在以下链接获取：this https URL。', 'title_zh': '大型语言模型中的独特特性'}
{'arxiv_id': 'arXiv:2502.12150', 'title': 'Idiosyncrasies in Large Language Models', 'authors': 'Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu', 'link': 'https://arxiv.org/abs/2502.12150', 'abstract': "In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at this https URL.", 'abstract_zh': '在本文中，我们揭示并研究了大型语言模型（LLMs）的独特特性——它们输出中的独特模式，这些模式可用于区分不同的模型。为此，我们考虑了一个简单的分类任务：给定一段特定的文本输出，目标是预测该文本是由哪个LLM生成的。我们在不同组的LLM上评估了这一合成任务，并发现将现有的文本嵌入模型微调在LLM生成的文本上，可以实现优异的分类准确性。值得注意的是，在涉及ChatGPT、Claude、Grok、Gemini和DeepSeek的五分类问题中，我们在保留验证数据上的准确率达到97.1%。进一步的研究显示，这些独特特性源于单词级别的分布。即使在文本被重新编写、翻译或由外部LLM总结后，这些模式依然存在，这表明它们也包含在语义内容中。此外，我们利用LLM作为评判者，生成了每个模型独特特性的详细和开放性描述。最后，我们讨论了本文发现的更广泛影响，特别是关于数据合成训练和模型相似性推断的方面。相关代码可在以下链接获取：[这里提供链接]。', 'title_zh': '大型语言模型中的独特性特征'}
{'arxiv_id': 'arXiv:2502.12137', 'title': 'REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance Wikipedia Tail Biographies through Personal Narratives', 'authors': 'Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.12137', 'abstract': "Wikipedia is an invaluable resource for factual information about a wide range of entities. However, the quality of articles on less-known entities often lags behind that of the well-known ones. This study proposes a novel approach to enhancing Wikipedia's B and C category biography articles by leveraging personal narratives such as autobiographies and biographies. By utilizing a multi-staged retrieval-augmented generation technique -- REVerSum -- we aim to enrich the informational content of these lesser-known articles. Our study reveals that personal narratives can significantly improve the quality of Wikipedia articles, providing a rich source of reliable information that has been underutilized in previous studies. Based on crowd-based evaluation, REVerSum generated content outperforms the best performing baseline by 17% in terms of integrability to the original Wikipedia article and 28.5\\% in terms of informativeness. Code and Data are available at: this https URL", 'abstract_zh': '维基百科是获取广泛实体事实信息的重要资源。然而，较少人熟知的实体的文章质量往往落后于知名实体的文章。本研究提出了一种新颖的方法，通过利用个人叙事（如自传和传记）来增强维基百科B和C类别传记文章的质量。通过利用多阶段检索增强生成技术（REVerSum），我们旨在丰富这些较不知名的传记文章的信息内容。研究结果表明，个人叙事可以显著提高维基百科文章的质量，提供了一种在先前研究中被严重低估的可靠信息来源。基于基于众包的评估，REVerSum生成的内容在与原始维基百科文章的集成度方面比表现最好的基线算法高出17%，在信息量方面高出28.5%。代码和数据可在此处访问：[此链接](this https URL)。', 'title_zh': 'REVERSUM：一种通过个人叙事增强维基百科尾部传记的多阶段检索增强生成方法'}
{'arxiv_id': 'arXiv:2502.12137', 'title': 'REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to Enhance Wikipedia Tail Biographies through Personal Narratives', 'authors': 'Sayantan Adak, Pauras Mangesh Meher, Paramita Das, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.12137', 'abstract': "Wikipedia is an invaluable resource for factual information about a wide range of entities. However, the quality of articles on less-known entities often lags behind that of the well-known ones. This study proposes a novel approach to enhancing Wikipedia's B and C category biography articles by leveraging personal narratives such as autobiographies and biographies. By utilizing a multi-staged retrieval-augmented generation technique -- REVerSum -- we aim to enrich the informational content of these lesser-known articles. Our study reveals that personal narratives can significantly improve the quality of Wikipedia articles, providing a rich source of reliable information that has been underutilized in previous studies. Based on crowd-based evaluation, REVerSum generated content outperforms the best performing baseline by 17% in terms of integrability to the original Wikipedia article and 28.5\\% in terms of informativeness. Code and Data are available at: this https URL", 'abstract_zh': '维基百科是获取广泛实体事实信息的重要资源。然而，不那么广为人知的实体的文章质量往往落后于广为人知的实体。本研究提出了一种新的方法，通过利用个人叙述（如自传和传记），来提升维基百科B和C类类别的人物传记文章的质量。通过利用多阶段检索增强生成技术——REVerSum，我们旨在丰富这些较不知名的条目的信息内容。我们的研究揭示，个人叙述可以显著提高维基百科文章的质量，提供了一种之前研究中未充分利用的丰富可靠信息来源。基于群体评估，REVerSum生成的内容在与原维基百科文章的整合性方面优于最佳基线方法17%，在信息量方面则超出28.5%。相关代码和数据可访问：[这里](this https URL)。', 'title_zh': 'REVERSUM：一种通过个人叙事增强 Wikipedia 尾部传记的多阶段检索增强生成方法'}
{'arxiv_id': 'arXiv:2502.12134', 'title': 'SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs', 'authors': 'Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao', 'link': 'https://arxiv.org/abs/2502.12134', 'abstract': "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the underlying LLM. Specifically, we employ a lightweight assistant model to generate instance-specific soft thought tokens speculatively as the initial chain of thoughts, which are then mapped into the LLM's representation space via a projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning.", 'abstract_zh': 'Chain-of-Thought（CoT）推理使大规模语言模型（LLMs）能够通过生成中间推理步骤来解决复杂的推理任务。然而，大多数现有方法专注于硬标记解码，这限制了推理在离散词汇空间内的进行，并且可能并不总是最优的。虽然近期的一些尝试探索了连续空间推理，但它们往往遭受灾难性遗忘的困扰，限制了其在零样本场景中已经表现出色的先进LLMs的应用。为解决这一挑战，我们提出了一种无需修改底层LLM的新方法，以在初步形成链式推理时，通过一个轻量级辅助模型生成实例特定的软思想标记进行推测。随后，这些软思想标记通过投影模块映射到LLM的表示空间。实验结果在五个推理基准测试上表明，我们的方法通过监督下的参数高效微调增强了LLM的推理性能。', 'title_zh': 'SoftCoT：高效的LLM推理中的软链式思考方法'}
{'arxiv_id': 'arXiv:2502.12124', 'title': 'RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational Quote Extraction from Long Documents', 'authors': 'Sayantan Adak, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.12124', 'abstract': 'Inspirational quotes from famous individuals are often used to convey thoughts in news articles, essays, and everyday conversations. In this paper, we propose a novel context-based quote extraction system that aims to extract the most relevant quote from a long text. We formulate this quote extraction as an open domain question answering problem first by employing a vector-store based retriever and then applying a multi-task reader. We curate three context-based quote extraction datasets and introduce a novel multi-task framework RA-MTR that improves the state-of-the-art performance, achieving a maximum improvement of 5.08% in BoW F1-score.', 'abstract_zh': '著名人士的激励语录常被用于新闻文章、散文以及日常对话中以传达思想。本文提出了一种基于上下文的新颖引语提取系统，旨在从长文本中提取最有相关性的引语。我们首先通过使用向量存储检索器将引语提取问题形式化为开放式领域的问题回答问题，然后应用多任务阅读器。我们整理了三个基于上下文的引语提取数据集，并介绍了一种新颖的多任务框架RA-MTR，该框架提升了当前最佳性能，在BoW F1分数上实现了最高5.08%的改进。', 'title_zh': 'RA-MTR：一种基于检索增强的多任务阅读器方法，用于从长文档中提取励志名言'}
{'arxiv_id': 'arXiv:2502.12134', 'title': 'SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs', 'authors': 'Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao', 'link': 'https://arxiv.org/abs/2502.12134', 'abstract': "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the underlying LLM. Specifically, we employ a lightweight assistant model to generate instance-specific soft thought tokens speculatively as the initial chain of thoughts, which are then mapped into the LLM's representation space via a projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning.", 'abstract_zh': '链式思考（Chain-of-Thought, CoT）推理使大型语言模型（Large Language Models, LLMs）能够通过生成中间推理步骤来解决复杂推理任务。然而，现有的大多数方法集中在硬标记解码上，这限制了推理在离散词汇空间内的进行，并且可能并不总是最优的。尽管最近的研究探索了连续空间推理，但它们往往遭受灾难性遗忘的问题，限制了它们在零样本设置中已经表现出色的先进LLMs的适用性。为了解决这一挑战，我们提出了一种新的连续空间推理方法，该方法不需要修改底层的LLM。具体而言，我们采用一个轻量级助手模型在初始链式思考中生成实例特定的软思考标记，随后通过一个投影模块将这些标记映射到LLM的表示空间中。在五个推理基准上的实验结果表明，我们的方法通过监督下的参数高效微调增强了LLM的推理性能。', 'title_zh': 'SoftCoT：高效生成式预训练模型推理的软链式思维'}
{'arxiv_id': 'arXiv:2502.12123', 'title': 'On the Query Complexity of Verifier-Assisted Language Generation', 'authors': 'Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski', 'link': 'https://arxiv.org/abs/2502.12123', 'abstract': 'Recently, a plethora of works have proposed inference-time algorithms (e.g. best-of-n), which incorporate verifiers to assist the generation process. Their quality-efficiency trade-offs have been empirically benchmarked on a variety of constrained generation tasks, but the algorithmic design landscape is still largely poorly understood. In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier--which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to "backtrack" (i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)--both in terms of computational efficiency, accuracy and diversity.', 'abstract_zh': '近年来，许多研究提出了在推理阶段使用验证器辅助生成过程的算法（例如“最佳选n”），这些算法在多种受约束生成任务上被实证评估了其质量效率之间的权衡。然而，算法设计的全貌仍然不甚明了。本文我们发展了一个数学框架，使用预训练语言模型生成器先验和过程验证器（能够决定前缀是否可以扩展为满足特定约束的字符串）来推理受约束生成。我们证明，在非常简单的情景下，访问验证器可以使一个本质上不可处理的问题（从信息论或计算的角度）变得可处理。事实上，我们展示了即使是简单算法（如逐词拒绝采样）也能够从访问验证器中获益匪浅。实证上，我们表明，逐词拒绝采样的一个自然修改版本（允许采样器进行“回溯”操作，即删除最后生成的几个单词）在计算效率、准确性及多样性方面都显著优于自然基准方法（例如块级拒绝采样、Nucleus采样）。', 'title_zh': '关于验证器辅助语言生成的查询复杂性研究'}
{'arxiv_id': 'arXiv:2502.12124', 'title': 'RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for Inspirational Quote Extraction from Long Documents', 'authors': 'Sayantan Adak, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.12124', 'abstract': 'Inspirational quotes from famous individuals are often used to convey thoughts in news articles, essays, and everyday conversations. In this paper, we propose a novel context-based quote extraction system that aims to extract the most relevant quote from a long text. We formulate this quote extraction as an open domain question answering problem first by employing a vector-store based retriever and then applying a multi-task reader. We curate three context-based quote extraction datasets and introduce a novel multi-task framework RA-MTR that improves the state-of-the-art performance, achieving a maximum improvement of 5.08% in BoW F1-score.', 'abstract_zh': '著名人士的励志名言在新闻文章、散文和日常对话中常被用来传达思想。本文提出了一种新颖的基于上下文的引语提取系统，旨在从长篇文章中提取最相关的引语。我们首先使用基于向量存储的检索器将引语提取问题表述为一个开放领域问答问题，然后应用一个多任务阅读器。我们收集了三个基于上下文的引语提取数据集，并引入了一种新颖的多任务框架RA-MTR，该框架显著提高了最先进的性能，最大改善了5.08%的BoW F1分数。', 'title_zh': 'RA-MTR：一种用于长文档 Inspirational 引言提取的检索增强多任务阅读器方法'}
{'arxiv_id': 'arXiv:2502.12123', 'title': 'On the Query Complexity of Verifier-Assisted Language Generation', 'authors': 'Edoardo Botta, Yuchen Li, Aashay Mehta, Jordan T. Ash, Cyril Zhang, Andrej Risteski', 'link': 'https://arxiv.org/abs/2502.12123', 'abstract': 'Recently, a plethora of works have proposed inference-time algorithms (e.g. best-of-n), which incorporate verifiers to assist the generation process. Their quality-efficiency trade-offs have been empirically benchmarked on a variety of constrained generation tasks, but the algorithmic design landscape is still largely poorly understood. In this paper, we develop a mathematical framework for reasoning about constrained generation using a pre-trained language model generator oracle and a process verifier--which can decide whether a prefix can be extended to a string which satisfies the constraints of choice. We show that even in very simple settings, access to a verifier can render an intractable problem (information-theoretically or computationally) to a tractable one. In fact, we show even simple algorithms, like tokenwise rejection sampling, can enjoy significant benefits from access to a verifier. Empirically, we show that a natural modification of tokenwise rejection sampling, in which the sampler is allowed to "backtrack" (i.e., erase the final few generated tokens) has robust and substantive benefits over natural baselines (e.g. (blockwise) rejection sampling, nucleus sampling)--both in terms of computational efficiency, accuracy and diversity.', 'abstract_zh': '近年来，许多研究提出了在生成过程中加入验证器的推理时间算法（例如，best-of-n），以辅助生成过程。这些算法的质量和效率权衡已经在各种受约束的生成任务上进行了经验基准测试，但其算法设计空间仍然缺乏充分的理解。本文中，我们开发了一个数学框架，使用预训练语言模型生成器或acles和过程验证器（能够判断某个前缀是否可以扩展为满足某些约束条件的字符串）来推理受约束的生成问题。我们展示了即使在非常简单的情况下，访问验证器也可以将不可解决问题（从信息论或计算角度看）转化为可解决问题。事实上，我们展示了即使是简单的算法（如逐词拒绝采样）也可以从访问验证器中获得显著的优势。实验证明，在计算效率、准确性和多样性方面，对逐词拒绝采样的自然修改版本（允许采样器“回溯”即删除最后一个生成的几个词）相比于自然基础方法（如块式拒绝采样、核采样）表现出更显著的稳健性和实质性优势。', 'title_zh': '《验证器辅助语言生成的查询复杂性研究》'}
{'arxiv_id': 'arXiv:2502.12110', 'title': 'A-MEM: Agentic Memory for LLM Agents', 'authors': 'Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang', 'link': 'https://arxiv.org/abs/2502.12110', 'abstract': "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at this https URL.", 'abstract_zh': '尽管大型语言模型（LLM）代理能够有效地利用外部工具来执行复杂的现实世界任务，它们仍需要记忆系统来利用历史经验。目前的记忆系统能够实现基本的存储和检索，但缺乏复杂的记忆组织功能，尽管最近尝试通过引入图数据库来进行整合。此外，这些系统的固定操作和结构限制了它们在多种任务中的适应性。为解决这一局限性，本文提出了一种新型的代理记忆系统，该系统能够以代理的方式动态组织记忆。我们遵循Zettelkasten方法的基本原则，设计了该记忆系统，通过动态索引和链接来创建相互连接的知识网络。当添加新的记忆时，我们将生成一份包含多个结构化属性的综合笔记，包括上下文描述、关键词和标签。接着，系统会分析历史记忆以识别相关联系，并在有意义的相似之处存在时建立联系。此外，这一过程还促进了记忆进化——随着新记忆的整合，它们可以触发现有历史记忆的上下文表示和属性的更新，从而使记忆网络能够不断深化其理解。我们结合了Zettelkasten的结构化组织原则和代理驱动决策的灵活性，以实现更适应性且更具有上下文意识的记忆管理。在六个基础模型上的实验证明了该方法相对于现有SOTA基准的优越改进。源代码可从此处访问：[此链接].\n\n请注意，翻译中的“链接”一词在英文原文中为“link”，而在中文学术环境中，此处可能更适合使用“连接”以增强读解效果。此外，“Zettelkasten方法”是一种源自德国学者弗雷德里希·尼采的笔记方法，泛指一种构建个人知识网络的系统方法。在许多中文文献中，有时也会简称为“札记箱”或“札记卡片方法”。因此，根据具体情况，也可以使用“札记卡片方法”作为术语的翻译。', 'title_zh': 'A-MEM：自主记忆机制在大规模语言模型代理中的应用'}
{'arxiv_id': 'arXiv:2502.12110', 'title': 'A-MEM: Agentic Memory for LLM Agents', 'authors': 'Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang', 'link': 'https://arxiv.org/abs/2502.12110', 'abstract': "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at this https URL.", 'abstract_zh': '虽然大规模语言模型（LLM）代理能够有效地利用外部工具来完成复杂的现实世界任务，但它们需要记忆系统来利用历史经验。当前的记忆系统能够提供基本的存储和检索功能，但却缺乏复杂的记忆组织能力，尽管最近有尝试将图数据库纳入其中。此外，这些系统固定的操作和结构限制了其在不同任务中的适应性。为了解决这一局限性，本文提出了一种针对LLM代理的新型代理性记忆系统，能够以代理的方式动态组织记忆。我们的记忆系统基于Zettelkasten方法的基本原则，设计了通过动态索引和链接来构建相互连接的知识网络的机制。当添加新的记忆时，我们将生成一个包含多个结构化属性（包括上下文描述、关键词和标签）的综合笔记。系统随后分析历史记忆以识别相关联接，并在存在有意义相似性的地方建立链接。此外，此过程还允许记忆进化——随着新记忆的整合，可以触发对现有历史记忆的上下文表示和属性的更新，从而使记忆网络能够不断精炼其理解。我们的方法结合了Zettelkasten的结构化组织原则和代理驱动决策的灵活性，从而实现更加适应性和上下文意识的记忆管理。在六种基础模型上的实证实验表明，与现有最优基线相比，我们的方法取得了显著的改进。代码可以在以下链接获取：[此链接]。', 'title_zh': 'A-MEM：代理的记忆能力'}
{'arxiv_id': 'arXiv:2502.12109', 'title': 'Personality Structured Interview for Large Language Model Simulation in Personality Research', 'authors': 'Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald', 'link': 'https://arxiv.org/abs/2502.12109', 'abstract': "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.", 'abstract_zh': '尽管心理测量研究者最近探索了使用大型语言模型（LLMs）作为人类参与者的代理，但LLMs往往无法生成具有人类多样性的异质性数据，这降低了它们在推进社会科学研究方面的价值。为应对这些挑战，我们探讨了理论导向的个性结构访谈（PSI）作为模拟个性研究中人类响应潜在工具的可能性。在此方法中，模拟基于以兴趣的个性结构为靶标的细致的人类访谈转录文本。我们提供了一套不断增加的357个结构化访谈转录文本，来自具有代表性的样本，每个文本包含一个人对32个精心设计的开放式问题的回应，旨在收集基于理论的个性证据。此外，基于心理测量研究，我们总结了一套评价框架，系统地验证由LLM生成的心理测量数据。三项实验的结果表明，精心设计的结构化访谈可以改善LLM模拟出的人类多样性的个性数据，并预测与个性相关的行为结果（即组织公民行为和反生产性工作行为）。我们进一步讨论了理论导向的结构化访谈在基于LLM的模拟中的作用，并概述了一种设计结构化访谈以生成类似人类数据进行心理测量研究的通用框架。', 'title_zh': '面向人格研究的大语言模型模拟的人格结构化面试'}
{'arxiv_id': 'arXiv:2502.12109', 'title': 'Personality Structured Interview for Large Language Model Simulation in Personality Research', 'authors': 'Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald', 'link': 'https://arxiv.org/abs/2502.12109', 'abstract': "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.", 'abstract_zh': '尽管心理学计量研究人员最近探索了使用大规模语言模型（LLMs）作为人类参与者代理的应用，但这些模型往往无法生成具有人类多样性的异质性数据，从而降低了它们在推动社会科学研究方面的作用。为了解决这些问题，我们探讨了受到理论指导的人格结构化访谈（PSI）在模拟人格研究中人类反应方面的潜在价值。在这种方法中，模拟以针对感兴趣的人格构念的细腻真实人类访谈转录为基础。我们提供了一个逐步扩大的357份结构化访谈转录集，每个转录都包含个别对精心设计、旨在收集理论导向的人格证据的32个开放式问题的回答。此外，基于心理学测量研究，我们总结了一个评估框架，以系统地验证LLM生成的心理测量数据。来自三项实验的结果表明，精心设计的结构化访谈可以提高LLM模拟的人格数据的人类多样性和预测与人格相关的行为结果（如组织公民行为和反生产工作行为）。我们进一步讨论了理论指导的结构化访谈在LLM导向模拟中的作用，并概述了一种用于设计结构化访谈以模拟用于心理学测量研究的人类数据的一般框架。', 'title_zh': '基于人格结构化面试的大语言模型模拟在人格研究中的应用'}
{'arxiv_id': 'arXiv:2502.12084', 'title': 'VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues', 'authors': 'Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R., Fung', 'link': 'https://arxiv.org/abs/2502.12084', 'abstract': "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.", 'abstract_zh': '视觉链接匹配线索是日常生活中的关键能力之一，例如基于个人的线索在多张照片中识别同一人，即便并不知道对方是谁。尽管视觉-语言模型（VLMs）具备广泛的知识，但目前尚不清楚它们是否能够执行这一基本任务。为应对这一问题，我们介绍了VLM$^2$-Bench，这是一个旨在评估VLMs能否进行视觉匹配线索链接的基准测试，包含9个子任务和超过3,000个测试案例。对八种开源VLMs和GPT-4o进行全面评估，并进一步分析各种基于语言和视觉的提示方法，得出八个关键发现。我们指出了模型在链接视觉线索方面面临的诸多挑战，突出了显著的性能差距，即使GPT-4o也落后于人类34.80%。根据这些洞察，我们提倡采取以下措施：(i) 提升核心视觉能力以提高适应性和减少对先验知识的依赖，(ii) 建立清晰的原则以在以视觉为中心的任务中整合基于语言的推理，以防止不必要的偏见，并（iii）将视觉-文本训练范式转向培养模型独立构建和推断视觉线索之间关系的能力。', 'title_zh': 'VLM$^2$-Bench：更深入探究大型视觉语言模型如何隐含地链接显式的视觉匹配线索'}
{'arxiv_id': 'arXiv:2502.12084', 'title': 'VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues', 'authors': 'Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R., Fung', 'link': 'https://arxiv.org/abs/2502.12084', 'abstract': "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.", 'abstract_zh': '视觉连接匹配线索是一项在日常生活中至关重要的能力，例如，根据线索在多张照片中识别人物，即使不知道他们是谁。尽管视觉语言模型（VLMs）拥有广泛的知识，但尚未充分探索它们是否能够执行这一基本任务。为解决这一问题，我们引入了VLM$^2$-Bench，这是一个基准测试工具，旨在评估VLMs是否能够进行视觉连接匹配线索。该基准测试包含9个子任务和超过3000个测试案例。对八个开源VLMs和GPT-4o进行了全方位评估，并进一步分析了各种语言侧和视觉侧的提示方法，得出了八项关键发现。我们指出了模型在连接视觉线索方面面临的关键挑战，突显了显著的性能差距，甚至GPT-4o也落后于人类34.80%。基于这些见解，我们建议采取以下措施：（i）增强核心视觉能力以提高适应性并减少对先验知识的依赖；（ii）明确整合基于语言的推理在以视觉为中心的任务中的原则，以防止不必要的偏见；（iii）将视觉-文本训练范式转向培养模型独立构建和推理视觉线索之间关系的能力。', 'title_zh': 'VLM$^2$-Bench：更深入探究视觉语言模型如何隐式关联显式匹配的视觉线索'}
{'arxiv_id': 'arXiv:2502.12082', 'title': 'AdaSplash: Adaptive Sparse Flash Attention', 'authors': 'Nuno Gonçalves, Marcos Treviso, André F. T. Martins', 'link': 'https://arxiv.org/abs/2502.12082', 'abstract': 'The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.', 'abstract_zh': '基于softmax的注意力在变压器中的计算成本限制了其在长上下文任务中的应用。自适应稀疏性，例如$\\alpha$-entmax注意力，提供了一种灵活的数据依赖替代方案，但现有的实现效率低下，并未充分利用稀疏性来获得运行时间和内存上的优势。在本文中，我们提出了AdaSplash，它结合了GPU优化算法的效率和$\\alpha$-entmax的稀疏性优势。我们首先介绍了一种混合Halley-二分法算法，从而将计算$\\alpha$-entmax变换所需的迭代次数减少了7倍。然后，我们实现了自定义的Triton内核，以高效处理自适应稀疏性。通过对RoBERTa和ModernBERT进行文本分类和单向检索实验，以及对GPT-2进行语言建模实验，我们的方法在运行时间和内存效率上相比现有$\\alpha$-entmax实现取得了显著改进，接近甚至超过了类似于FlashAttention-2的高度优化的softmax实现的效率，同时在保持强任务性能的情况下支持长上下文训练。', 'title_zh': 'AdaSplash：自适应稀疏闪存注意机制'}
{'arxiv_id': 'arXiv:2502.12073', 'title': 'Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation', 'authors': 'Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo', 'link': 'https://arxiv.org/abs/2502.12073', 'abstract': "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts.", 'abstract_zh': '社交媒体能够促进用户与热门话题的动态互动，近期的研究探讨了大型语言模型（LLMs）在响应生成方面的潜在应用。虽然有些研究考察了LLMs作为模拟社交媒体上用户行为的代理，但其关注点主要在于实践可行性和可扩展性，而非深入理解LLMs与人类行为的契合度。本文通过动作引导式响应生成分析了LLMs模拟社交媒体互动的能力，该方法首先预测用户对热门帖子最可能采取的互动动作（转发、引用或重写），然后基于预测的动作生成个性化响应。我们针对X平台上讨论的重大社会事件，将GPT-4o-mini、O1-mini和DeepSeek-R1进行社交媒体互动模拟的基准测试。我们的研究发现，零样本LLMs在动作预测方面表现不及BERT，而在少量样本提示下，LLMs的预测准确性会有所下降。然而，在响应生成方面，少量样本提示的LLMs能够与真实帖子实现更强的语义对齐。', 'title_zh': '大语言模型能否模拟社交媒体参与？基于动作指导的响应生成研究'}
{'arxiv_id': 'arXiv:2502.12082', 'title': 'AdaSplash: Adaptive Sparse Flash Attention', 'authors': 'Nuno Gonçalves, Marcos Treviso, André F. T. Martins', 'link': 'https://arxiv.org/abs/2502.12082', 'abstract': 'The computational cost of softmax-based attention in transformers limits their applicability to long-context tasks. Adaptive sparsity, of which $\\alpha$-entmax attention is an example, offers a flexible data-dependent alternative, but existing implementations are inefficient and do not leverage the sparsity to obtain runtime and memory gains. In this work, we propose AdaSplash, which combines the efficiency of GPU-optimized algorithms with the sparsity benefits of $\\alpha$-entmax. We first introduce a hybrid Halley-bisection algorithm, resulting in a 7-fold reduction in the number of iterations needed to compute the $\\alpha$-entmax transformation. Then, we implement custom Triton kernels to efficiently handle adaptive sparsity. Experiments with RoBERTa and ModernBERT for text classification and single-vector retrieval, along with GPT-2 for language modeling, show that our method achieves substantial improvements in runtime and memory efficiency compared to existing $\\alpha$-entmax implementations. It approaches -- and in some cases surpasses -- the efficiency of highly optimized softmax implementations like FlashAttention-2, enabling long-context training while maintaining strong task performance.', 'abstract_zh': 'softmax 基准注意力机制在变压器中的计算成本限制了其在长上下文任务中的应用。自适应稀疏性，例如 $\\alpha$-entmax 注意力，提供了数据依赖性下的灵活替代方案，但现有的实现效率低下，未能充分利用稀疏性以获得运行时和内存增益。在本文中，我们提出了 AdaSplash，该方法结合了 GPU 优化算法的高效性和 $\\alpha$-entmax 的稀疏性优势。我们首先引入了一种混合 Halley-二分法算法，从而将计算 $\\alpha$-entmax 转换所需的迭代次数减少了7倍。然后，我们实现了自定义的 Triton 内核，以高效处理自适应稀疏性。对于文本分类和单向向量检索中使用 RoBERTa 和 ModernBERT 的实验，以及使用 GPT-2 的语言建模实验表明，我们的方法在运行时间和内存效率方面相较于现有的 $\\alpha$-entmax 实现取得了显著改进。在某些情况下，它接近甚至超过了如 FlashAttention-2 这样高度优化的 softmax 实现的效率，从而在保持任务性能的同时支持长上下文训练。', 'title_zh': 'AdaSplash：自适应稀疏闪存注意机制'}
{'arxiv_id': 'arXiv:2502.12073', 'title': 'Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation', 'authors': 'Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo', 'link': 'https://arxiv.org/abs/2502.12073', 'abstract': "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts.", 'abstract_zh': '社交媒体使用户能够动态参与热门话题，近年来的研究探索了大型语言模型（LLMs）在响应生成方面潜在的能力。虽然一些研究将LLMs作为模拟社交媒体用户行为的代理，但他们的重点仍然在于实用可行性和扩展性，而不是更深入地理解LLMs与人类行为的契合程度。本文通过动作指导的响应生成分析了LLMs在模拟社交媒体参与方面的能力，其中模型首先预测用户最有可能对一条热门帖子采取的行动（转发、引用或重写），然后根据预测的行动生成个性化的响应。我们在X平台上针对一项主要社会事件，对比评估了GPT-4o-mini、O1-mini和DeepSeek-R1在社交媒体参与模拟方面的性能。研究发现，零样本的LLMs在行动预测方面不如BERT表现，而少量样例提示最初降低了LLMs的预测准确性。然而，在响应生成方面，少量样例的LLMs能够与真实的帖子内容实现更强的语义一致性。', 'title_zh': '大型语言模型能否模拟社交媒体参与？一项基于行动导向响应生成的研究'}
{'arxiv_id': 'arXiv:2502.12067', 'title': 'TokenSkip: Controllable Chain-of-Thought Compression in LLMs', 'authors': 'Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.12067', 'abstract': "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.", 'abstract_zh': '链式思维（CoT）已被证明能有效提升大型语言模型（LLMs）的推理能力。最近的进步，如OpenAI的o1和DeepSeek-R1表明，在推理过程中扩展CoT序列的长度可以进一步提升LLMs的推理性能。然而，由于LLM解码的自回归特性，更长的CoT输出会导致推理延迟线性增加，从而影响用户体验，尤其是当CoT长度超过10,000个标记时。为了解决这一限制，我们分析了CoT输出中各个标记的语义重要性，并发现它们对推理的贡献不同。基于这一洞察，我们提出了TokenSkip，这是一种简单而有效的方法，允许LLMs有选择地跳过不重要的标记，从而实现可控的CoT压缩。广泛实验表明，TokenSkip在减少CoT标记使用量的同时，仍能保持强大的推理性能。特别地，当应用于Qwen2.5-14B-Instruct时，TokenSkip在GSM8K数据集上将推理标记减少了40%（从313个减少到181个），性能下降不到0.4%。', 'title_zh': 'TokenSkip: 可控的大型语言模型链式思考压缩方法'}
{'arxiv_id': 'arXiv:2502.12067', 'title': 'TokenSkip: Controllable Chain-of-Thought Compression in LLMs', 'authors': 'Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.12067', 'abstract': "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.", 'abstract_zh': '链式思维（Chain-of-Thought，CoT）已被证明能够有效提升大型语言模型（LLM）的推理能力。最近的一些进展，如OpenAI的o1和DeepSeek-R1，表明在推理过程中扩展CoT序列的长度可以进一步提升LLM的推理表现。然而，由于LLM解码的自回归性质，更长的CoT输出会导致推理延迟线性增加，从而影响用户体验，尤其是当CoT超过10,000个标记时。为了解决这一限制，我们分析了CoT输出中各个标记的语义重要性，并发现它们在推理中的贡献各不相同。基于这一洞见，我们提出了一种简单而有效的方法——TokenSkip，使LLM能够选择性地跳过不重要的标记，从而实现可控的CoT压缩。在多种模型和任务上进行的广泛实验表明，TokenSkip能够在减少CoT标记使用量的同时保持强大的推理性能。值得注意的是，将TokenSkip应用于Qwen2.5-14B-Instruct时，在GSM8K数据集上，TokenSkip将推理标记减少了40%（从313减少到181），且性能降幅不到0.4%。', 'title_zh': 'TokenSkip: LLM中可控的链式思维压缩'}
{'arxiv_id': 'arXiv:2502.12065', 'title': 'Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions', 'authors': 'Lan Zhang, Marco Valentino, Andre Freitas', 'link': 'https://arxiv.org/abs/2502.12065', 'abstract': "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions -- a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalisation, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we guide LLMs through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.", 'abstract_zh': '由于语言能力的强大，大型语言模型（LLMs）为通过自动形式化弥合非正式数学和正式语言之间的差距提供了机会。然而，目前仍不清楚LLMs在处理复杂且自然出现的数学语句时的泛化能力如何。为解决这一问题，我们研究了自动形式化真实世界的数学定义——这是数学交流中的关键组成部分。具体而言，我们引入了两个新的自动形式化资源：从Wikipedia收集定义（Def_Wiki）和从arXiv论文收集定义（Def_ArXiv）。随后，我们系统地评估了一系列LLMs，分析它们将定义形式化为Isabelle/HOL的能力。此外，我们还研究了提高LLMs性能的策略，包括通过证明辅助器的外部反馈进行细化，以及通过引导LLMs获得相关形式化数学库的上下文信息进行形式化定义定位。我们的研究发现，定义相比现有的基准（如miniF2F）提出了更大的挑战。特别是，我们发现LLMs在自我修正和与相关数学库保持一致方面仍存在困难。同时，结构化的细化方法和定义定位策略在自我修正能力方面提升了16%，在减少未定义错误方面提升了43%，这为增强基于LLM的自动形式化在实际应用场景中的表现指明了有希望的方向。', 'title_zh': '使用大语言模型正式化复杂数学陈述：关于数学定义的研究'}
{'arxiv_id': 'arXiv:2502.12065', 'title': 'Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions', 'authors': 'Lan Zhang, Marco Valentino, Andre Freitas', 'link': 'https://arxiv.org/abs/2502.12065', 'abstract': "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions -- a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalisation, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we guide LLMs through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.", 'abstract_zh': '由于其语言能力，大型语言模型（LLMs）为通过自动形式化弥补非正式数学和正式语言之间的差距提供了机会。然而，目前尚不清楚LLMs是否能够很好地将这些能力应用到复杂的、自然出现的数学陈述上。为解决这一问题，我们研究了自动形式化实世界数学定义这一任务，这是数学语言交流中的一个关键组成部分。具体而言，我们引入了两个新的资源，从Wikipedia收集定义（Def_Wiki）和从arXiv论文中收集定义（Def_ArXiv）。随后，我们系统性地评估了多种LLM，分析它们将定义形式化为Isabelle/HOL的能力。此外，我们还研究了提高LLM性能的策略，包括通过证明辅助系统提供的外部反馈进行细化，以及形式定义定位，即引导LLM通过相关形式化数学库中的上下文元素。我们的研究发现表明，与现有的基准（如miniF2F）相比，定义形式化提出了更大的挑战。特别的是，我们发现LLMs仍然难以自我纠正，并且难以与相关数学库对齐。与此同时，结构化细化方法和定义定位策略在自我纠正能力和减少未定义错误方面分别取得了最高16%和43%的进步，这为增强基于LLM的自动形式化在实际场景中的表现提供了希望的发展方向。', 'title_zh': '使用大型语言模型正式化复杂数学声明：关于数学定义的研究'}
{'arxiv_id': 'arXiv:2502.12064', 'title': 'AI-generated Text Detection with a GLTR-based Approach', 'authors': 'Lucía Yan Wu, Isabel Segura-Bedmar', 'link': 'https://arxiv.org/abs/2502.12064', 'abstract': "The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications. However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages. Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19%, except for the first ranking model (80.91%). However, for the Spanish dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57% compared to the top-performing model.", 'abstract_zh': '大型语言模型（LLMs）的兴起极大地提高了先进自然语言处理（NLP）应用的表现和开发。然而，当这些模型遭到恶意使用时，也会带来风险，比如传播虚假信息、发布有害内容、冒充他人身份或帮助学生抄袭等。这是因为LLMs能够生成高质量的文字，这些文字往往难以与人类撰写的文字区分开来。GLTR，即Giant Language Model Test Room，是由MIT-IBM Watson AI Lab和HarvardNLP联合开发的一个可视化工具，旨在基于GPT-2帮助检测机器生成的文本，并根据文本中词语被机器生成的概率突出显示这些词语。GLTR的一个局限性是，其返回的结果有时可能模糊不清，导致困惑。本研究旨在探索多种方法以在IberLef-AuTexTification 2023共享任务的背景下提高GLTR检测AI生成文本的有效性，该任务涵盖了英语和西班牙语。实验结果表明，基于GLTR的GPT-2模型在英语数据集上以80.19%的宏F1分数超越了最先进的模型（最佳模型为80.91%）。然而，对于西班牙语数据集，我们获得了66.20%的宏F1分数，这比最佳模型低4.57%。', 'title_zh': '基于GLTR方法的AI生成文本检测'}
{'arxiv_id': 'arXiv:2502.12064', 'title': 'AI-generated Text Detection with a GLTR-based Approach', 'authors': 'Lucía Yan Wu, Isabel Segura-Bedmar', 'link': 'https://arxiv.org/abs/2502.12064', 'abstract': "The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications. However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages. Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19%, except for the first ranking model (80.91%). However, for the Spanish dataset, we obtained a macro F1-score of 66.20%, which differs by 4.57% compared to the top-performing model.", 'abstract_zh': '大规模语言模型（LLMs）的兴起极大地提高了尖端自然语言处理（NLP）应用的性能和发展。然而，这些模型也可能因恶意使用而带来风险，例如传播假新闻、发布有害内容、冒充个人或帮助学术抄袭等。这是因为LLMs能够生成高质量的文字，这些文字难以与人类写作区分开来。GLTR（Giant Language Model Test Room）是由MIT-IBM Watson AI实验室和哈佛NLP联合开发的一种视觉工具，旨在基于GPT-2帮助检测机器生成的文本，该工具根据文本中词语被机器生成的概率突出显示这些词语。GLTR的一个限制是，其返回的结果有时会模糊不清，容易引起混淆。本研究旨在探索各种方法，以提高GLTR在IberLef-AuTexTification 2023共享任务中检测AI生成文本的有效性，涵盖英语和西班牙语两种语言。实验结果显示，基于GLTR的GPT-2模型在英文数据集上的宏F1分数为80.19%，仅略低于排名第一的模型（80.91%）。然而，对于西班牙语数据集，我们获得了66.20%的宏F1分数，与最佳模型的性能相差4.57%。', 'title_zh': '基于GLTR方法的AI生成文本检测'}
{'arxiv_id': 'arXiv:2502.12057', 'title': 'Culture is Not Trivia: Sociocultural Theory for Cultural NLP', 'authors': 'Naitian Zhou, David Bamman, Isaac L. Bleaman', 'link': 'https://arxiv.org/abs/2502.12057', 'abstract': 'The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.', 'abstract_zh': '文化自然语言处理领域近年来经历了快速的发展，这一进程主要由确保语言技术在多元用户群体中有效且安全的迫切需求驱动。这项工作在很大程度上缺乏一个共享的文化观念，而是依赖于各种各样的文化代理。然而，这导致了一系列反复出现的限制：粗略的国家边界未能捕捉其内部的细微差异，有限的覆盖面限制了数据集仅局限于通常高度代表的文化群体，缺乏动态性导致了静态的文化基准，这些基准无法随着文化的演变而改变。在本文中，我们认为这些方法论的限制反映了理论上的空白。我们借鉴社会文化语言学中发展完善的文化理论，通过以下三个方面来填补这一空白：1）在一个案例研究中展示如何利用这一理论来阐明方法论的限制和可能，2）提供基于理论导向的发展路径，以实现跨文化能力，以及3）主张本地化是目前文化自然语言处理工作中许多目标更为合适的研究框架。', 'title_zh': '文化不仅仅是琐事：社会文化理论在文化自然语言处理中的应用'}
{'arxiv_id': 'arXiv:2502.12057', 'title': 'Culture is Not Trivia: Sociocultural Theory for Cultural NLP', 'authors': 'Naitian Zhou, David Bamman, Isaac L. Bleaman', 'link': 'https://arxiv.org/abs/2502.12057', 'abstract': 'The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.', 'abstract_zh': '文化自然语言处理领域近年来经历了迅速增长，这主要是由于确保语言技术在多元用户群体中有效且安全的迫切需求。这项工作在很大程度上缺乏对文化的共同理解，而是依赖于一系列文化和代理指标。然而，这种做法带来了许多反复出现的限制：粗略的国别边界无法捕捉到其中的细微差别，有限的覆盖面使得数据集只能包括通常较为代表性的文化的一部分，缺乏动态性导致了静态的文化基准，在文化不断演变的情况下不能随之更新。在本文中，我们主张这些方法论限制反映了理论上的空白。我们借鉴社会文化语言学中成熟的文化理论，通过以下三个方面来填补这一空白：1）在案例研究中展示它如何阐明方法论约束和可能性，2）提出基于理论的路径，以实现文化能力，3）主张局部化是一种对当前文化自然语言处理工作目标更为有用的框架。', 'title_zh': '文化不仅仅是琐碎之事：社会文化理论在文化自然语言处理中的应用'}
{'arxiv_id': 'arXiv:2502.12055', 'title': 'Designing Role Vectors to Improve LLM Inference Behaviour', 'authors': 'Daniele Potertì, Andrea Seveso, Fabio Mercorio', 'link': 'https://arxiv.org/abs/2502.12055', 'abstract': 'The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.', 'abstract_zh': '关于人物角色对大型语言模型（LLMs）的影响，已有大量研究，但其对模型性能的直接影响仍不明确。本研究探索了一种新的方法，通过角色向量指导LLM的行为，这是一种替代基于人物角色提示的方法。我们构建了29个来源于模型激活的角色向量，并评估了其在多个领域基准测试中的影响。我们的分析探讨了这些向量是否能有效地引导模型向特定领域的专长发展。我们衡量了两种关键干预措施：（i）激活增强，加强角色特定的方向；（ii）方向性消除，去除这些方向。在已建立的基准测试上的结果显示，这些角色向量确实影响了模型的行为，在相关领域提高了任务性能，同时对无关任务的影响较小。这反过来表明，操作内部模型表示对结果的影响比基于人物角色的提示更大。', 'title_zh': '设计角色向量以改进大规模语言模型的推理行为'}
{'arxiv_id': 'arXiv:2502.12055', 'title': 'Designing Role Vectors to Improve LLM Inference Behaviour', 'authors': 'Daniele Potertì, Andrea Seveso, Fabio Mercorio', 'link': 'https://arxiv.org/abs/2502.12055', 'abstract': 'The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.', 'abstract_zh': 'persona对大型语言模型（LLMs）的影响已经得到了广泛的研究，但它们对性能的直接影响仍不明朗。本研究探讨了一种新的方法，通过角色向量引导LLM的行为，这提供了一种与基于persona的提示不同的方法。我们构建了29个源自模型激活的角色向量，并评估了它们在多个领域的基准性能影响。我们的分析探讨了这些向量是否能有效地引导模型向特定领域的专长方向发展。我们测量了两种关键干预措施：（i）激活添加，增强角色特定的方向，以及（ii）方向性消除，移除这些方向。在已有基准测试上的结果显示，角色向量确实影响了模型的行为，在相关领域提高了任务性能，而对不相关任务的影响较小。这反过来表明，操控内部模型表示比基于persona的提示对结果的影响更大。', 'title_zh': '设计角色向量以改善大规模语言模型推理行为'}
{'arxiv_id': 'arXiv:2502.12052', 'title': 'A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability', 'authors': 'Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan', 'link': 'https://arxiv.org/abs/2502.12052', 'abstract': 'In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.', 'abstract_zh': '在自然语言生成（NLG）元评价中，评价指标通常基于其与人类的一致性进行评估。然而，我们发现传统NLG元评价方法存在一些局限性，例如处理人类评分的问题以及相关性度量选择的模糊性，这些都削弱了元评价的有效性。本文中，我们提出了一种双视角的NLG元评价框架，该框架旨在关注不同的评估能力，从而提供更好的可解释性。此外，我们提出了一种自动构建相应基准的方法，无需新的手动标注。进一步地，我们基于所提出的框架对16种代表性的大规模语言模型（LLM）进行了实验，从不同的角度全面分析了它们的评估性能。', 'title_zh': '一种双视角NLG元评估框架：结合自动基准和更好的解释性'}
{'arxiv_id': 'arXiv:2502.12052', 'title': 'A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability', 'authors': 'Xinyu Hu, Mingqi Gao, Li Lin, Zhenghan Yu, Xiaojun Wan', 'link': 'https://arxiv.org/abs/2502.12052', 'abstract': 'In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.', 'abstract_zh': '在自然语言生成（NLG）元评估中，通常根据评价指标与人类的一致性来评估这些指标。然而，我们发现传统NLG元评估方法存在一些问题，例如在处理人类评分和选择相关性度量时存在模糊性，这些问题削弱了元评估的有效性。在本研究中，我们提出了一种双视角的NLG元评估框架，该框架关注不同的评估能力，从而提供更好的可解释性。此外，我们引入了一种自动构建相应基准的方法，无需新的手动注释。同时，我们根据所提出框架进行了实验，使用16种代表性的大规模语言模型（LLMs）作为评估者，从多个角度全面分析了它们的评估性能。', 'title_zh': '一种结合自动基准和更好解释性的双向视角NLG元评价框架'}
{'arxiv_id': 'arXiv:2502.12051', 'title': 'How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines', 'authors': 'Ayan Sengupta, Yash Goel, Tanmoy Chakraborty', 'link': 'https://arxiv.org/abs/2502.12051', 'abstract': 'Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.', 'abstract_zh': '神经网络的扩展法则通过揭示模型大小、数据集规模和计算资源之间的可预测关系，彻底革新了大规模人工智能模型的设计和优化。早期研究发现了模型性能中的幂律关系，从而提出了计算最优的扩展策略。然而，最近的研究强调了这些关系在不同架构、输入模态和部署场景下的局限性。稀疏模型、专家混合模型、检索增强学习和多模态模型通常偏离传统扩展模式。此外，扩展行为在视觉、强化学习和微调等领域之间存在差异，凸显了需要更细致方法的必要性。在本文综述中，我们综合了超过50篇研究的见解，探讨扩展法则的理论基础、实证结果及其实践意义。我们也探讨了关键挑战，包括数据效率、推理扩展和架构特定的约束条件，倡导针对实际应用的适应性扩展策略。我们建议虽然扩展法则提供了一个有用的标准，但它们并不总能在所有架构和训练策略中普遍适用。', 'title_zh': '如何利用标度定律扩大神经网络？综述与实用指南'}
{'arxiv_id': 'arXiv:2502.12051', 'title': 'How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines', 'authors': 'Ayan Sengupta, Yash Goel, Tanmoy Chakraborty', 'link': 'https://arxiv.org/abs/2502.12051', 'abstract': 'Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.', 'abstract_zh': '神经网络的规模律通过揭示模型规模、数据集体积和计算资源之间可预测的关系，已彻底革新了大型AI模型的设计和优化。早期的研究确立了模型性能中的幂律关系，从而促进了基于计算的最优扩展策略。然而，近期的研究突显了这些定律在不同架构、模态和部署上下文中的局限性。稀疏模型、专家混合模型、检索增强学习以及多模态模型常常偏离传统的扩展模式。此外，不同领域（如视觉、强化学习和微调）中的扩展行为各异，这强调了需要更加精细的方法。在这篇综述中，我们综合了超过50项研究的见解，探讨了扩展律的理论基础、实证发现及其实践意义。我们还探讨了关键挑战，包括数据效率、推理扩展以及架构特定限制，并提倡适应性扩展策略，使其更适合实际应用。我们建议虽然扩展律提供了有用的指导，但它们并不总能在所有架构和训练策略中普遍存在适用性。', 'title_zh': '如何利用标度定律扩展神经网络？综述与实用指南'}
{'arxiv_id': 'arXiv:2502.12050', 'title': 'SpeechT: Findings of the First Mentorship in Speech Translation', 'authors': 'Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb', 'link': 'https://arxiv.org/abs/2502.12050', 'abstract': 'This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the requirements of the mentorship, the participants engaged in key activities, including data preparation, modelling, and advanced research.', 'abstract_zh': '本研究详细介绍了2024年12月到2025年1月举行的首次口语翻译导师项目（SpeechT）的内容和发现。为满足导师项目的要求，参与者进行了关键活动，包括数据准备、建模以及高级研究。', 'title_zh': 'SpeechT：第一次口语翻译指导项目的研究发现'}
{'arxiv_id': 'arXiv:2502.12050', 'title': 'SpeechT: Findings of the First Mentorship in Speech Translation', 'authors': 'Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb', 'link': 'https://arxiv.org/abs/2502.12050', 'abstract': 'This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the requirements of the mentorship, the participants engaged in key activities, including data preparation, modelling, and advanced research.', 'abstract_zh': '本文详细介绍了首次口语翻译（SpeechT）导师项目的内容和发现，该项目于2024年12月和2025年1月进行。为了满足导师项目的要求，参与者开展了关键活动，包括数据准备、模型构建以及高级研究。', 'title_zh': 'SpeechT: 第一次口译训练的发现'}
{'arxiv_id': 'arXiv:2502.12022', 'title': 'Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving', 'authors': 'Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu', 'link': 'https://arxiv.org/abs/2502.12022', 'abstract': "Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategy based on their inherent capabilities. In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.", 'abstract_zh': '现有的大语言模型（LLM）进行数学推理的方法主要依赖于Chain-of-Thought（CoT）以增强泛化能力，或依赖于Tool-Integrated Reasoning（TIR）以实现精确计算。尽管已经进行了结合这两种方法的努力，但这些方法主要依赖于后筛选或预定义策略，留下的一个未解决问题是：LLM能否根据其固有的能力自主适应其推理策略。在本文中，我们提出了一种自适应框架TATA（Teaching LLMs According to Their Aptitude），该框架使LLM能够自发地个性化其推理策略，使其与模型的固有能力相一致。TATA在监督微调（SFT）过程中根据基底LLM的特性选择数据，以量身定制训练数据以适应模型的独特能力。这种方法使LLM能够在测试时自主确定并应用合适的推理策略。我们通过在六个数学推理基准上的广泛实验评估了TATA，使用了通用和数学专门化的LLM。实验结果表明，TATA能够有效结合CoT和TIR的互补优势，在提高推理效率的同时获得优于或至少与TIR相当的性能。进一步的分析强调了基于能力选择数据在使LLM能够做出有效且自适应的推理决策，并使推理策略与模型能力相一致方面发挥的关键作用。', 'title_zh': '根据各自的能力教学：适应性推理在数学问题解决中的应用'}
{'arxiv_id': 'arXiv:2502.12022', 'title': 'Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving', 'authors': 'Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu', 'link': 'https://arxiv.org/abs/2502.12022', 'abstract': "Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategy based on their inherent capabilities. In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.", 'abstract_zh': '现有的大型语言模型（LLMs）在数学推理方面的应用方法主要依赖于链条思考（Chain-of-Thought, CoT）以增强泛化能力，或者工具集成推理（Tool-Integrated Reasoning, TIR）以提高精确计算能力。虽然已经尝试将这两种方法结合起来，但它们主要依赖于后处理选择或预先定义的策略，留下了一个开放的问题：即LLMs能否根据自身的内在能力自主适应其推理策略。在本文中，我们提出了TATA（Teaching LLMs According to Their Aptitude），一种自适应框架，使LLMs能够自发地个性化其推理策略，使其与内在能力相一致。TATA通过监督微调（SFT）时的LLM感知数据选择，定制训练数据，以适应模型的独特能力。这种方法使LLMs能够在测试时自主确定并应用合适的推理策略。我们通过在六个数学推理基准上的广泛实验评估了TATA，使用了通用和数学专门化的LLMs。实验结果表明，TATA有效地结合了CoT和TIR的互补优势，在推理效率上优于单独使用TIR，并实现了更优或相当的性能。进一步的分析强调了内在能力感知数据选择对于使LLMs能够做出有效和适应性的推理决策，并使其推理策略与模型能力相一致的关键作用。', 'title_zh': '根据自身优势教学大语言模型：自适应推理在数学问题解决中的应用'}
{'arxiv_id': 'arXiv:2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'authors': 'Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）通过训练时的扩展实现了卓越的性能，在测试时的扩展进一步增强了其能力，尤其是在推理期间能够进行有效的推理。然而，随着推理规模的增加，现有的测试时扩展方法会累积历史信息，这不仅浪费了计算资源，还干扰了有效的推理。为了解决这一问题，我们观察到复杂的推理过程往往通过逐步解决一系列独立的子问题来实现，每个子问题都是自包含且可验证的。这些子问题本质上是原子问题，主要依赖于当前状态而非积累的历史信息，类似于马尔可夫过程的无记忆转移。基于这一观察，我们提出了Thought原子（AoT），即推理过程中的每个状态转换包括将当前问题分解为基于依赖关系的有向无环图，并合并其子问题，形成一个新的原子问题状态。这一迭代的分解-合并过程持续进行，直到达到可以直接解决的原子问题，自然实现了问题状态之间的马尔可夫转换。此外，这些原子问题可以无缝集成到现有的测试时扩展方法中，使AoT能够作为插件增强工具来提升推理能力。六种基准测试的实验表明，AoT作为独立框架和插件增强工具均具有有效性。值得注意的是，在HotpotQA数据集上，将AoT应用于gpt-4o-mini时，其F1分数达到了80.6%，分别超越了o3-mini的77.2%和DeepSeek-R1的69.8%。代码将在此处提供：[请填入链接]。', 'title_zh': '原子级思维：用于马尔可夫链大规模语言模型测试时伸缩性的方法'}
{'arxiv_id': 'arXiv:2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'authors': 'Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）通过训练时扩展实现卓越性能，并在测试时扩展进一步增强了其推理能力，使其在推断过程中能够进行有效的推理。然而，随着推理规模的增加，现有的测试时扩展方法会累积历史信息，这不仅浪费了计算资源，还干扰了有效的推理。为解决这一问题，我们观察到复杂推理过程往往通过解决一系列独立的子问题来实现，每个子问题是自包含且可验证的。这些子问题本质上是原子问题，主要依赖于当前状态而不是累积的历史信息，类似于马尔可夫过程中的无记忆转移。基于这一观察，我们提出了Thought原子单元（AoT），其中推理过程中的每个状态转换包括将当前问题分解为基于依赖关系的有向无环图（DAG），并收缩其子问题，形成一个新的原子问题状态。这一逐步分解-收缩过程继续进行，直到达到可以直接解决的原子问题，自然实现了问题状态之间的马尔可夫转移。此外，这些原子问题可以无缝集成到现有的测试时扩展方法中，使AoT能够作为插件增强来提升推理能力。在六个基准测试中，AoT作为独立框架和插件增强均证明了其有效性。值得注意的是，在使用AoT对gpt-4o-mini进行处理时，其在HotpotQA上的F1分数达到了80.6%，超过了o3-mini的77.2%和DeepSeek-R1的70.0%，高出了3.4%和10.6%。源代码将在以下链接提供：[链接]。', 'title_zh': '思维原子化在马尔可夫LLM测试时缩放中的应用'}
{'arxiv_id': 'arXiv:2502.12007', 'title': 'Demographic Attributes Prediction from Speech Using WavLM Embeddings', 'authors': 'Yuchen Yang, Thomas Thebaud, Najim Dehak', 'link': 'https://arxiv.org/abs/2502.12007', 'abstract': 'This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.', 'abstract_zh': '本文介绍了一种基于WavLM特征的一般分类器，用于从语音中推断人口统计学特征，如年龄、性别、母语、受教育程度和国籍。人口统计学特征预测在语言学习、无障碍技术和数字取证等应用中起着关键作用，有助于实现更加个性化和包容性的技术。通过利用预训练模型进行嵌入提取，所提出的方法识别与人口统计学属性相关的关键声学和语言特征，在年龄预测中实现了平均绝对误差（MAE）4.94，在性别分类中取得了超过99.81%的准确率。我们的系统在多项任务中相对提高了现有模型30%的MAE，以及10%的准确率和F1分数，通过使用多种数据集和大规模预训练模型确保了稳健性和泛化能力。本研究为语音驱动的人口统计学特征分析提供了新的见解，并为未来研究奠定了坚实的基础。', 'title_zh': '使用WavLM嵌入进行语音的 demographic 特征预测'}
{'arxiv_id': 'arXiv:2502.12001', 'title': 'Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition', 'authors': 'Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura', 'link': 'https://arxiv.org/abs/2502.12001', 'abstract': "This paper investigates the integration of technical vocabulary in merged language models. We explore the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.", 'abstract_zh': '本文探讨了技术词汇在融合语言模型中的整合问题。我们研究了将通用语言特定模型与领域特定模型结合时的知识转移机制，重点关注融合后的模型对技术术语的理解能力。实验分析了这一融合过程对目标模型处理专业术语能力的影响。我们对融合模型的性能进行了定量评估，并将其与个体构成模型的性能进行了比较。研究结果提供了不同模型融合方法在增强领域特定知识方面的有效性见解，并指出了在跨语言知识转移中利用这些方法的潜在挑战和未来方向在自然语言处理中的重要性。', 'title_zh': '将语言模型与领域特定模型融合：对技术词汇获取的影响'}
{'arxiv_id': 'arXiv:2502.12007', 'title': 'Demographic Attributes Prediction from Speech Using WavLM Embeddings', 'authors': 'Yuchen Yang, Thomas Thebaud, Najim Dehak', 'link': 'https://arxiv.org/abs/2502.12007', 'abstract': 'This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.', 'abstract_zh': '本文介绍了一种基于WavLM特征的一般分类器，用于从语音中推断人口统计特征，如年龄、性别、母语、教育水平和国籍。人口统计特征预测在语言学习、无障碍服务和数字鉴证等领域中发挥着重要作用，有助于实现更加个性化和包容性的技术。利用预训练模型进行特征嵌入提取，所提出的框架识别与人口统计属性相关的关键声学和语言特征，年龄预测的平均绝对误差（MAE）为4.94，性别分类的准确率超过99.81%。我们的系统在各种任务中相对提高了MAE多达30%，准确率和F1分数分别提高了10%，并通过利用多样化数据集和大型预训练模型来确保稳健性和通用性。本研究表明了说话人口多样性的新见解，并为基于语音的人口统计特征分类研究奠定了坚实的基础。', 'title_zh': '使用WavLM嵌入进行语音的 demographics 属性预测'}
{'arxiv_id': 'arXiv:2502.12001', 'title': 'Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition', 'authors': 'Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura', 'link': 'https://arxiv.org/abs/2502.12001', 'abstract': "This paper investigates the integration of technical vocabulary in merged language models. We explore the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.", 'abstract_zh': '本文研究了技术词汇在融合语言模型中的整合。我们探讨了将通用的语言模型与特定领域模型结合时的知识转移机制，重点关注结合后模型对技术术语的理解能力。我们的实验分析了融合过程对该目标模型处理专有名词熟练度的影响。我们定量评估了融合模型的性能，并将其与单个组成部分模型的性能进行了比较。研究结果提供了不同模型融合方法在增强特定领域知识有效性方面的见解，并指出了利用这些方法进行跨语言知识转移在自然语言处理中的潜在挑战和未来发展方向。', 'title_zh': '将语言模型与领域特定模型合并：对技术词汇习得的影响'}
{'arxiv_id': 'arXiv:2502.11995', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'authors': 'Siddhesh Pawar, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein', 'link': 'https://arxiv.org/abs/2502.11995', 'abstract': 'Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.', 'abstract_zh': '名字与人类身份紧密相连，可以作为个性、文化传承和个人历史的标志。然而，将名字作为身份的核心指标可能会导致对复杂身份的过度简化。在与大语言模型（LLM）交互时，用户名是实现个性化的重要信息点。名字可以通过聊天机器人请求的直接用户输入、作为任务上下文（如简历审查）的一部分，或者作为内置的内存功能来进入聊天机器人的对话，这些功能用于存储用户信息以实现个性化。我们通过测量LLM在处理常见建议查询时生成的回应中的文化假设，研究与名字相关的偏见。我们的分析表明，来自不同文化背景的名字在LLM生成中普遍存在强烈的文化身份假设。我们的工作对设计更细腻的个性化系统具有重要意义，这些系统可以避免强化刻板印象，同时保持有意义的定制。', 'title_zh': '假设的文化身份：姓名如何塑造语言模型的响应'}
{'arxiv_id': 'arXiv:2502.11995', 'title': 'Presumed Cultural Identity: How Names Shape LLM Responses', 'authors': 'Siddhesh Pawar, Arnav Arora, Lucie-Aimée Kaffee, Isabelle Augenstein', 'link': 'https://arxiv.org/abs/2502.11995', 'abstract': 'Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.', 'abstract_zh': '姓名与人的身份紧密相连，能够作为个体性、文化遗产和个人历史的标志。然而，将姓名作为身份的核心指标可能会导致对复杂身份的简化。用户在与大型语言模型（LLM）交互时，姓名是个性化信息的重要来源。姓名可以通过聊天机器人直接请求输入、作为任务上下文的一部分（如简历审查）出现，或作为内置的记忆功能，存储用户信息以实现个性化。我们通过测量LLM在处理常见建议查询时生成的回应中所包含的文化假设来研究与姓名相关的偏见。分析表明，来自不同文化背景的LLM生成的回应中普遍存在对姓名所代表的文化身份的强烈假设。我们的研究结果对于设计更为精细的个性化系统具有重要意义，这些系统可以在避免强化刻板印象的同时实现有意义的定制。', 'title_zh': '假定的文化身份：姓名如何影响大模型的响应'}
{'arxiv_id': 'arXiv:2502.11973', 'title': 'Generating Text from Uniform Meaning Representation', 'authors': 'Emma Markle, Reihaneh Iranmanesh, Shira Wein', 'link': 'https://arxiv.org/abs/2502.11973', 'abstract': 'Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though still limited amounts of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs: (1) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large language models with UMR data, and (3) fine-tuning existing AMR-to-text generation models with UMR data. Our best performing model achieves a multilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared to the reference, which is a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation with even limited amounts of UMR data.', 'abstract_zh': '统一语义表示（Uniform Meaning Representation，UMR）是一种最近开发的基于图的语义表示方式，其在抽象语义表示（Abstract Meaning Representation，AMR）的基础上进行了多项扩展，特别是在包含文档级信息和多语言灵活性方面。为了有效采用并利用UMR进行下游任务，必须致力于构建一个UMR技术生态系统。尽管迄今为止只有相对有限的UMR注释已被生成，但在本研究中，我们探讨了产生多语言UMR图的方法，具体包括：（1）UMR到AMR的管道转换，然后使用AMR到文本生成模型，（2）利用UMR数据对大型语言模型进行微调，以及（3）利用UMR数据对现有AMR到文本生成模型进行微调。我们表现最佳的模型在英语上的多语言BERT分数为0.825，在中文上的分数为0.882，这表明即使在少量UMR数据的情况下，微调方法在UMR到文本生成中的有效性也是有前景的。', 'title_zh': '从统一意义表示生成文本'}
{'arxiv_id': 'arXiv:2502.11973', 'title': 'Generating Text from Uniform Meaning Representation', 'authors': 'Emma Markle, Reihaneh Iranmanesh, Shira Wein', 'link': 'https://arxiv.org/abs/2502.11973', 'abstract': 'Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though still limited amounts of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs: (1) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large language models with UMR data, and (3) fine-tuning existing AMR-to-text generation models with UMR data. Our best performing model achieves a multilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared to the reference, which is a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation with even limited amounts of UMR data.', 'abstract_zh': '以下内容是从英文翻译成中文后的版本，符合学术规范：\n\n统一语义表示（Uniform Meaning Representation，UMR）是一种最近开发的基于图的语义表示方法，它在多个方面扩展了抽象语义表示（Abstract Meaning Representation，AMR），特别是在文档级信息的纳入和多语言灵活性方面。为了有效地采用和利用UMR进行下游任务，必须发展UMR技术生态系统。尽管迄今为止仅生产了一定数量的UMR标注数据，本研究旨在探讨将多语言UMR图转换为文本的首批方法：（1）将UMR转换为AMR的流水线方法，然后使用AMR到文本生成模型；（2）使用UMR数据对大规模语言模型进行微调；（3）使用UMR数据对现有的AMR到文本生成模型进行微调。我们的最佳模型在英语和中文上的多语言BERT-score分别为0.825和0.882，这表明即使使用有限的UMR数据进行微调，UMR到文本生成方法的有效性也颇具前景。', 'title_zh': '将文本生成统一意义表示osten'}
{'arxiv_id': 'arXiv:2502.11962', 'title': 'Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning', 'authors': 'Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold', 'link': 'https://arxiv.org/abs/2502.11962', 'abstract': 'Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.', 'abstract_zh': '指令微调（IFT）可以提升大型语言模型（LLMs）的有用性，但可能降低它们的真实性。这种权衡是因为IFT使LLMs倾向于生成预训练中未充分覆盖的长尾知识的响应，从而在泛化到未见过的任务时提供更具信息性但更不真实的答案。在本文中，我们通过实验证明了IFT中的这种有用性-真实性权衡，并提出了一种新颖的IFT范式$\\textbf{UNIT}$来解决这一问题。UNIT教LLMs识别它们的不确定性，并明确地在响应的结尾反映这种不确定性。实验结果表明，通过UNIT进行微调的模型在保持有用性的同时能够区分确定性和不确定性声明，从而减少了幻觉现象。', 'title_zh': '具备不确定性意识的指令微调以导航有益性-真实性权衡'}
{'arxiv_id': 'arXiv:2502.11962', 'title': 'Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning', 'authors': 'Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold', 'link': 'https://arxiv.org/abs/2502.11962', 'abstract': 'Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.', 'abstract_zh': '指令微调（Instruction Fine-tuning, IFT）可以增强大型语言模型（Large Language Models, LLMs）的回答有用性，但可能会降低其真实性。这种权衡产生于IFT引导LLMs生成在预训练阶段未充分覆盖的长尾知识，导致在泛化到未见任务时，产生更具信息量但更不真实的回答。本文通过实证展示了IFT中有用性与真实性的权衡，并提出了一种新颖的IFT范式——$\\textbf{UNIT}$，以解决这一问题。UNIT 使LLMs能够识别自身的不确定性，并在回答结尾明确反映出这种不确定性。实验结果表明，经过UNIT微调的模型在保持有用性的同时，能够区分确定与不确定的断言，从而减少虚构回答。', 'title_zh': '带有不确定性意识的指令微调以导航帮助性与事实性之间的权衡'}
{'arxiv_id': 'arXiv:2502.11948', 'title': 'Can Your Uncertainty Scores Detect Hallucinated Entity?', 'authors': 'Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li', 'link': 'https://arxiv.org/abs/2502.11948', 'abstract': 'To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research.', 'abstract_zh': '为了减轻大规模语言模型（LLM）幻觉性质的影响，许多研究提出通过不确定性估计检测幻觉生成。然而，这些方法大多在句子或段落级别进行操作，无法明确指出导致幻觉内容的具体片段或实体。在长形式输出中，这种缺乏精细度的问题尤为明显，因为这些输出中混合了准确和虚构的信息。为解决这一限制，我们探索了基于实体级别的幻觉检测。我们提出了一种新的数据集，命名为HalluEntity，该数据集在实体级别标注幻觉信息。基于该数据集，我们全面评估了17种现代LLM上基于不确定性的幻觉检测方法。实验结果表明，关注单个token概率的不确定性估计方法往往会高估幻觉的程度，而上下文感知的方法则表现更好但仍然不尽如人意。通过深入的定性研究，我们揭示了幻觉倾向与语言特征之间的关系，并指出了未来研究的重要方向。', 'title_zh': '你的不确定性分数能否检测出虚构实体？'}
{'arxiv_id': 'arXiv:2502.11948', 'title': 'Can Your Uncertainty Scores Detect Hallucinated Entity?', 'authors': 'Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li', 'link': 'https://arxiv.org/abs/2502.11948', 'abstract': 'To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research.', 'abstract_zh': '为减轻大语言模型(Large Language Models, LLMs)幻觉性质的影响，许多研究提出了通过不确定性估计检测幻觉生成的方法。然而，这些方法主要在句子或段落级别操作，无法确定产生幻觉内容的具体跨度或实体。对于混合准确信息和虚假信息的长文本输出来说，这种缺乏粒度尤为成问题。为解决这一局限性，我们探索了实体级幻觉检测。我们提出了一种新的数据集——HalluEntity，该数据集在实体级别标注幻觉信息。基于该数据集，我们全面评估了17种现代LLM上的基于不确定性估计的幻觉检测方法。实验结果表明，专注于单个词元概率的不确定性估计方法往往会过度预测幻觉，而具备上下文意识的方法表现更好但仍欠佳。通过深入的定性研究，我们确定了幻觉倾向和语言属性之间的关系，并指出了未来研究的重要方向。', 'title_zh': '你的不确定性得分能否检测出虚构实体？'}
{'arxiv_id': 'arXiv:2502.11946', 'title': 'Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction', 'authors': 'Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong', 'link': 'https://arxiv.org/abs/2502.11946', 'abstract': 'Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at this https URL.', 'abstract_zh': '实时语音交互作为人机协作的基本接口，具有巨大的潜力。然而，现有的开源模型存在一些局限性，如高昂的语音数据收集成本、动态控制能力较弱以及智能水平有限。为解决这些问题，本文引入了Step-Audio，这是首个生产就绪的开源解决方案。主要贡献包括：\n\n1. 一个包含130亿参数的统一的语音-文本多模态模型，实现了统一的理解和生成能力，并开源了Step-Audio-Chat版本；\n2. 一种生成语音数据的引擎，建立了可负担得起的语音克隆框架，并通过蒸馏开源了轻量级的Step-Audio-TTS-3B模型；\n3. 一种基于指令的精细控制系统，能够动态调整方言、情绪、唱歌和说唱等多种表现形式；\n4. 一种增强的认知架构，加入了工具调用和角色扮演的能力，以有效管理复杂任务。\n\n基于我们新的StepEval-Audio-360评价基准，Step-Audio 在人类评估中取得了最先进的性能，特别是在指令跟随方面。在开源基准测试如LLaMA Question中，展示了9.3%的平均性能提升，进一步证明了我们致力于推进开源多模态语言技术发展的承诺。我们的代码和模型可在以下链接中获取：[此链接](this https URL)。', 'title_zh': 'Step-Audio：统一理解与生成在智能语音交互中的应用'}
{'arxiv_id': 'arXiv:2502.11946', 'title': 'Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction', 'authors': 'Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Brian Li, Changyi Wan, Hanpeng Hu, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Kang An, Wei Ji, Wen Li, Xuan Wen, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chengting Feng, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Jianchang Wu, Jiahong Liu, Jianjian Sun, Jiangjie Zhen, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Shaoliang Pang, Shiliang Yang, Shuli Gao, Siqi Liu, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wenqing He, Wen Sun, Xin Han, Xiaomin Deng, Xiaojia Liu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaqiang Shi, Yilei Wang, Yinmin Zhong', 'link': 'https://arxiv.org/abs/2502.11946', 'abstract': 'Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at this https URL.', 'abstract_zh': '实时语音交互作为人机协作的基本接口，具有巨大的潜力。然而，当前开源模型在语音数据收集成本高、动态控制能力弱以及智能化水平有限等方面存在局限性。为解决这些问题，本论文介绍了Step-Audio，这是首个生产级的开源解决方案。其主要贡献包括：1) 一个包含130亿参数的统一语音-文本多模态模型，实现了统一的理解和生成能力，Step-Audio-Chat版本已开源；2) 生成式语音数据引擎，建立了经济实惠的语音克隆框架，并通过知识蒸馏开源了轻量级的Step-Audio-TTS-3B模型；3) 指令驱动的精细控制系统，支持对方言、情感、歌唱和说唱等进行动态调整；4) 增强的认知架构，具备工具调用和角色扮演能力，以有效管理复杂的任务。基于我们全新的StepEval-Audio-360评估基准，Step-Audio 在人类评估中表现出色，特别是在指令遵循方面。在开放源代码基准测试（如LLaMA Question）上，平均性能提升了9.3%，展示了我们对推动开源多模态语言技术发展的承诺。我们的代码和模型可在以下链接获取：[此 https URL]。', 'title_zh': 'Step-Audio：智能语音交互中的统一理解和生成'}
{'arxiv_id': 'arXiv:2502.11932', 'title': 'On Representational Dissociation of Language and Arithmetic in Large Language Models', 'authors': 'Riku Kisako, Tatsuki Kuribayashi, Ryohei Sasano', 'link': 'https://arxiv.org/abs/2502.11932', 'abstract': "The association between language and (non-linguistic) thinking ability in humans has long been debated, and recently, neuroscientific evidence of brain activity patterns has been considered. Such a scientific context naturally raises an interdisciplinary question -- what about such a language-thought dissociation in large language models (LLMs)? In this paper, as an initial foray, we explore this question by focusing on simple arithmetic skills (e.g., $1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in LLMs' representation space. Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations). These tentatively suggest that arithmetic reasoning is mapped into a distinct region from general language input, which is in line with the neuroscientific observations of human brain activations, while we also point out their somewhat cognitively implausible geometric properties.", 'abstract_zh': '人类的语言能力和非语言思维能力之间的关联一直存在争议，最近，神经科学中关于大脑活动模式的证据引起了广泛关注。在此类科学背景下，自然引发了一个跨学科的问题——大型语言模型（LLMs）中的语言-思维分离现象又是怎样的？在本文中，作为初步探索，我们通过关注简单的算术技能（例如：1+2=？）作为一种思维能力，并分析其在LLMs表示空间中的几何编码方式来探讨这一问题。我们使用线性分类器和簇可分性测试的实验表明，简单的算术等式和一般的语言输入在LLMs的内部表示空间的所有层中被完全分离编码，这在使用更受控的刺激（例如：写出的等式）时也得到了支持。这些结果初步表明，算术推理被映射到与一般语言输入不同的区域中，这与人类大脑激活的神经科学观察结果一致，但我们也指出了其略显认知上不合理的一些几何特性。', 'title_zh': '大型语言模型中语言与算术表征的分离研究'}
{'arxiv_id': 'arXiv:2502.11932', 'title': 'On Representational Dissociation of Language and Arithmetic in Large Language Models', 'authors': 'Riku Kisako, Tatsuki Kuribayashi, Ryohei Sasano', 'link': 'https://arxiv.org/abs/2502.11932', 'abstract': "The association between language and (non-linguistic) thinking ability in humans has long been debated, and recently, neuroscientific evidence of brain activity patterns has been considered. Such a scientific context naturally raises an interdisciplinary question -- what about such a language-thought dissociation in large language models (LLMs)? In this paper, as an initial foray, we explore this question by focusing on simple arithmetic skills (e.g., $1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in LLMs' representation space. Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations). These tentatively suggest that arithmetic reasoning is mapped into a distinct region from general language input, which is in line with the neuroscientific observations of human brain activations, while we also point out their somewhat cognitively implausible geometric properties.", 'abstract_zh': '人类语言与非语言思维能力之间的关联一直存在争议，近年来，神经系统活动模式的神经科学证据引起了广泛关注。在这样的科学背景下，一个跨学科的问题自然浮现出来：大型语言模型（LLMs）中的语言-思维分离现象如何？本文作为初步探索，我们通过关注简单算术技能（例如，$1+2=$？）作为一种思维能力，并分析其在LLMs表示空间中的几何编码来探讨这一问题。我们使用线性分类器和聚类可分离性测试的实验表明，在LLMs的所有层级中，简单的算术方程和一般语言输入在内部表示空间中被编码在完全分离的区域内，这一结论也得到了更受控刺激（例如，写出的方程）的支持。这些结果初步表明，算术推理被映射到了与一般语言输入不同的区域，这与人类大脑激活的神经科学观察一致，但我们也指出了它们在认知上相对不合理的几何特性。', 'title_zh': '大型语言模型中语言与算术表征的分离研究'}
{'arxiv_id': 'arXiv:2502.11926', 'title': 'BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages', 'authors': 'Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Andrew Piper, Alexander Panchenko, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad', 'link': 'https://arxiv.org/abs/2502.11926', 'abstract': 'People worldwide use language in subtle and complex ways to express emotions. While emotion recognition -- an umbrella term for several NLP tasks -- significantly impacts different applications in NLP and other fields, most work in the area is focused on high-resource languages. Therefore, this has led to major disparities in research and proposed solutions, especially for low-resource languages that suffer from the lack of high-quality datasets. In this paper, we present BRIGHTER-- a collection of multilabeled emotion-annotated datasets in 28 different languages. BRIGHTER covers predominantly low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances from various domains annotated by fluent speakers. We describe the data collection and annotation processes and the challenges of building these datasets. Then, we report different experimental results for monolingual and crosslingual multi-label emotion identification, as well as intensity-level emotion recognition. We investigate results with and without using LLMs and analyse the large variability in performance across languages and text domains. We show that BRIGHTER datasets are a step towards bridging the gap in text-based emotion recognition and discuss their impact and utility.', 'abstract_zh': '世界各地的人们通过微妙而复杂的方式使用语言来表达情绪。情绪识别——这一术语涵盖了多个自然语言处理（NLP）任务——在NLP及其相关领域中发挥了显著影响。然而，这一领域内的大部分研究工作都集中在资源丰富型语言上。因此，这导致了研究和提出的解决方案之间在低资源型语言上的重大差距，特别是那些由于缺乏高质量数据集而遭受的低资源语言。本文我们介绍了一个名为BRIGHTER的数据集集合，该数据集集合涵盖了28种不同语言的情绪标注数据。BRIGHTER主要覆盖了非洲、亚洲、东欧和拉丁美洲的低资源语言，由流利的使用者对各个领域的实例进行了标注。我们详细描述了数据采集和标注过程以及构建这些数据集所遇到的挑战。然后，我们报告了单语和跨语言多标签情绪识别以及情绪强度识别的实验结果。我们探讨了使用和不使用大语言模型的情况，并分析了不同语言和文本领域在性能上存在的巨大差异。我们展示了BRIGHTER数据集在文本基emotion识别方面的进步，并讨论了它们的影响和实用性。', 'title_zh': 'BRIGHTER：为28种语言的人注释文本情感识别数据集 bridging 语种差距'}
{'arxiv_id': 'arXiv:2502.11926', 'title': 'BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages', 'authors': 'Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Andrew Piper, Alexander Panchenko, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad', 'link': 'https://arxiv.org/abs/2502.11926', 'abstract': 'People worldwide use language in subtle and complex ways to express emotions. While emotion recognition -- an umbrella term for several NLP tasks -- significantly impacts different applications in NLP and other fields, most work in the area is focused on high-resource languages. Therefore, this has led to major disparities in research and proposed solutions, especially for low-resource languages that suffer from the lack of high-quality datasets. In this paper, we present BRIGHTER-- a collection of multilabeled emotion-annotated datasets in 28 different languages. BRIGHTER covers predominantly low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances from various domains annotated by fluent speakers. We describe the data collection and annotation processes and the challenges of building these datasets. Then, we report different experimental results for monolingual and crosslingual multi-label emotion identification, as well as intensity-level emotion recognition. We investigate results with and without using LLMs and analyse the large variability in performance across languages and text domains. We show that BRIGHTER datasets are a step towards bridging the gap in text-based emotion recognition and discuss their impact and utility.', 'abstract_zh': '世界各地的人们以微妙而复杂的方式使用语言来表达情感。情感识别作为一种涵盖多个自然语言处理（NLP）任务的总称，对NLP和其他领域中的多种应用产生了显著影响。然而，该领域大多数工作主要集中在高资源语言上。因此，这导致了在研究和提出解决方案方面在低资源语言和高资源语言之间存在重大差异，特别是对于缺乏高质量数据集的支持的低资源语言。在本文中，我们介绍了BRIGHTER——一个包含28种不同语言多标签情感标注数据集的集合。BRIGHTER覆盖了来自非洲、亚洲、东欧和拉丁美洲的以低资源语言为主的语言，这些数据集中的例子由母语者进行多领域的标注。我们描述了数据采集和标注过程以及构建这些数据集所面临的挑战。然后，我们报告了单语言和跨语言多标签情感识别以及情感强度识别的多种实验结果。我们调查了使用和未使用大语言模型（LLMs）的情况，并分析了不同语言和文本领域的性能差异。我们展示了BRIGHTER数据集是情感识别文本差异的一个改进步骤，并讨论了它们的影响和用途。', 'title_zh': 'BRIGHTER：为28种语言的人标注情感文本识别数据集搭建桥梁'}
{'arxiv_id': 'arXiv:2502.11916', 'title': 'EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models', 'authors': 'Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11916', 'abstract': "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.", 'abstract_zh': '自动生成作文评分（Automated Essay Scoring, AES）在教育评估中发挥着重要作用，通过提供作文任务的可扩展和一致的评估。然而，传统的AES系统面临三个主要挑战：（1）依赖手工构建的特征，这限制了其泛化能力；（2）难以捕捉细腻的特质，如连贯性和论辩能力；（3）无法处理多模态上下文。在多模态大语言模型（Multimodal Large Language Models, MLLMs）的时代，我们提出了EssayJudge，这是第一个用于评估AES在词汇、句法和话语层面特质能力的多模态基准。通过利用MLLMs在特定特质评分和多模态上下文理解方面的优势，EssayJudge旨在提供精确且上下文丰富的评估，而无需人工特征工程，以此解决传统的AES存在的长期限制问题。我们的实验使用18个代表性MLLMs揭示了AES在话语层面特质评估上的表现与人工评估之间的差距，强调了基于MLLM的AES研究需要进一步的进展。我们将提供的数据集和代码将在接受后发布。', 'title_zh': 'EssayJudge：评估多模态大规模语言模型作文评分能力的多层次基准'}
{'arxiv_id': 'arXiv:2502.11916', 'title': 'EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models', 'authors': 'Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11916', 'abstract': "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.", 'abstract_zh': '自动化作文评分（Automated Essay Scoring, AES）在教育评估中发挥着关键作用，通过提供可扩展且一致的写作任务评估。然而，传统的AES系统面临三大挑战：（1）依赖手工设计的特征，限制了其普适性；（2）难以捕捉细粒度的特征如连贯性和论证能力；（3）无法处理多模态上下文。在多模态大型语言模型（Multimodal Large Language Models, MLLMs）的时代，我们提出了EssayJudge，这是第一个评估AES能力的多模态基准，可以评估其在词级、句级和话语级特征上的表现。通过利用MLLMs在特征特定评分和多模态上下文理解方面的优势，EssayJudge旨在提供精确且富有上下文的信息评估，无需手动特征工程，解决传统AES的长期限制。我们的实验结果显示，18个代表性MLLMs在AES性能上存在与人工评估的差距，尤其是在话语级特征上，这表明需要进一步改进基于MLLMs的AES研究。我们的数据集和代码将在接受后提供。', 'title_zh': 'EssayJudge：评估多模态大型语言模型自动作文评分能力的多粒度基准'}
{'arxiv_id': 'arXiv:2502.11903', 'title': 'MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation', 'authors': 'Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao', 'link': 'https://arxiv.org/abs/2502.11903', 'abstract': 'Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.', 'abstract_zh': '近期的多模态大型语言模型（MLLMs）在开放式对话中展现出显著潜力，能够生成更准确和个性化的回应。然而，他们在实际场景中持续交互中的记忆、回忆和推理能力仍较少被研究。本论文提出了MMRC，一个用于评估MLLMs六项核心开放式能力的多模态真实世界对话基准：信息提取、多轮推理、信息更新、图像管理、记忆召回以及拒绝回答。该基准数据来源于实际场景，包含5,120场对话和28,720个相应的人工标注问题，对现有MLLMs构成了重大挑战。在MMRC上对20种MLLMs的评估表明，在开放式对话中准确率有所下降。我们识别出了四种常见的失败模式：长期记忆退化、事实知识更新不足、错误传播累积假设以及拒绝回答的犹豫。为了缓解这些问题，我们提出了一种简单而有效的笔记策略（NOTE-TAKING），该策略可以在对话过程中记录关键信息并在模型响应时提醒模型，从而增强对话能力。通过六个MLLMs的实验展示了显著性能提升。', 'title_zh': 'MMRC：一个大规模基准，用于理解多模态大型语言模型在实际对话中的表现'}
{'arxiv_id': 'arXiv:2502.11901', 'title': 'Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity', 'authors': 'Dylan Zhang, Justin Wang, Tianran Sun', 'link': 'https://arxiv.org/abs/2502.11901', 'abstract': "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.", 'abstract_zh': '现有的语言模型在证明导向编程方面存在困难，主要是由于数据稀缺性问题，这主要体现在两个关键方面：（1）证明导向编程语言（如F*）缺乏足够的语料库；（2）缺少大规模的项目级证明导向实现案例，这些案例可以教授模型在执行证明导向编程时复杂的推理过程。我们提出了首个基于合成数据增强的项目级证明导向编程方法，适用于生成和修复任务。我们的方法通过合成基本的证明导向编程问题以提升对该语言的熟练度；融合多样化编码数据以激发推理能力，并在现有库中创建新的证明和修复数据。这种方法使语言模型能够同时合成并修复函数级和库级代码的证明。实验结果显示，我们微调的14B参数模型PoPilot在项目级证明导向编程方面的性能比在GPT-4o上表现出色的模型高出64%的相对幅度，并且能够通过修复GPT-4o自身的输出，提高GPT-4o的性能54%。', 'title_zh': '在数据稀缺条件下，构建一个证明导向的程序员，其性能比GPT-4o提高64%'}
{'arxiv_id': 'arXiv:2502.11903', 'title': 'MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation', 'authors': 'Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao', 'link': 'https://arxiv.org/abs/2502.11903', 'abstract': 'Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.', 'abstract_zh': '近年来的多模态大规模语言模型（MLLMs）在开放性对话中表现出巨大的潜力，能够生成更加准确和个性化的响应。然而，它们在实际场景中进行长时间互动时的记忆能力、回忆能力和推理能力尚未得到充分探索。本文介绍了一种名为MMRC的多模态真实世界对话基准，用于评估MLLMs的六项核心开放性能力：信息提取、多轮推理、信息更新、图像管理、记忆回忆和拒绝回答。该基准数据来源于实际场景，包含5,120场对话和28,720个对应的手动标注问题，为现有的MLLMs提出了巨大挑战。在MMRC中对20种MLLMs的评估表明，在开放性互动中准确率有所下降。我们识别出四种常见的失败模式：长期记忆退化、事实知识更新不足、错误传播的累积假设以及拒绝回答的迟疑。为了缓解这些问题，我们提出了一种简单且有效的记录策略——笔记提取（NOTE-TAKING），该策略可以在对话中记录关键信息，并在模型的响应中提醒模型，从而增强对话能力。在六种MLLMs上的实验表明，该策略显著提高了模型的性能。', 'title_zh': 'MMRC：一个大规模基准，用于理解现实生活对话中的多媒体大型语言模型'}
{'arxiv_id': 'arXiv:2502.11890', 'title': 'Revisiting Classification Taxonomy for Grammatical Errors', 'authors': 'Deqing Zou, Jingheng Ye, Yulu Liu, Yu Wu, Zishan Xu, Yinghui Li, Hai-Tao Zheng, Bingxu An, Zhao Wei, Yong Xu', 'link': 'https://arxiv.org/abs/2502.11890', 'abstract': 'Grammatical error classification plays a crucial role in language learning systems, but existing classification taxonomies often lack rigorous validation, leading to inconsistencies and unreliable feedback. In this paper, we revisit previous classification taxonomies for grammatical errors by introducing a systematic and qualitative evaluation framework. Our approach examines four aspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability. Then, we construct a high-quality grammatical error classification dataset annotated with multiple classification taxonomies and evaluate them grounding on our proposed evaluation framework. Our experiments reveal the drawbacks of existing taxonomies. Our contributions aim to improve the precision and effectiveness of error analysis, providing more understandable and actionable feedback for language learners.', 'abstract_zh': '语法错误分类在语言学习系统中扮演着重要角色，但现有的分类体系往往缺乏严格的验证，导致了不一致性以及不可靠的反馈。在本文中，我们通过引入一个系统性和定性的评价框架，重新审视先前的语法错误分类体系。我们的方法从四个方面评估分类体系，即独占性、覆盖率、平衡性和易用性。然后，我们构建了一个高质量的语法错误分类数据集，并根据我们提出的评价框架对这些分类体系进行评估。我们的实验揭示了现有分类体系的缺陷。我们的贡献旨在提高错误分析的精确性和有效性，为语言学习者提供更易于理解和操作的反馈。', 'title_zh': '重新审视语法错误分类体系'}
{'arxiv_id': 'arXiv:2502.11901', 'title': 'Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity', 'authors': 'Dylan Zhang, Justin Wang, Tianran Sun', 'link': 'https://arxiv.org/abs/2502.11901', 'abstract': "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.", 'abstract_zh': '现有的语言模型在处理证明导向编程时面临挑战，主要是由于数据稀缺，这主要体现在两个方面：（1）缺乏足够的证明导向编程语言（如F*）语料库；（2）缺乏大规模的项目级证明导向实现，这些实现能够教会模型在进行证明导向编程时复杂的推理过程。我们首次提出了基于合成数据增强的方法，用于项目级别的证明导向编程，涵盖生成和修复场景。我们的方法通过合成基本的证明导向编程问题来解决数据稀缺问题，这些问题是针对该语言的精通问题；通过结合多样化的编码数据来激发推理能力，并在现有仓库中创建新的证明和修复数据。这种方法使得语言模型既能够合成代码中的证明，也能在函数级和仓库级代码中进行修复。实验结果表明，我们微调的14B参数模型PoPilot在项目级别证明导向编程方面超过了性能优于GPT-4o的模型，相对性能高出64%；通过修复GPT-4o的输出，还可以提高其性能54%。', 'title_zh': '在数据稀缺条件下，构建一个证明导向的程序员，其表现比GPT-4o高出64%'}
{'arxiv_id': 'arXiv:2502.11874', 'title': 'VAQUUM: Are Vague Quantifiers Grounded in Visual Data?', 'authors': 'Hugh Mee Wong, Rick Nouwen, Albert Gatt', 'link': 'https://arxiv.org/abs/2502.11874', 'abstract': 'Vague quantifiers such as "a few" and "many" are influenced by many contextual factors, including how many objects are present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes.', 'abstract_zh': '模糊量词（如“几”和“很多”）受多种语境因素的影响，包括给定语境中对象的数量。本研究评估了视觉-语言模型（VLMs）在生成或判断视觉语境中模糊量词的适当性时，与人类的一致性程度。我们发布了一个名为VAQUUM的新数据集，该数据集包含对1089张图像中的量化陈述进行了20300次的人类评估。利用该数据集，我们通过三种不同的评估方法比较了人类判断和VLM预测。研究结果表明，VLMs在使用模糊量词时受到对象数量的影响，就像人类一样。然而，我们发现了不同模型在不同评估设置下的显著不一致性，这表明判断和生成模糊量词依赖于两种不同的过程。', 'title_zh': 'VAQUUM: 模糊量词是否扎根于视觉数据？'}
{'arxiv_id': 'arXiv:2502.11890', 'title': 'Revisiting Classification Taxonomy for Grammatical Errors', 'authors': 'Deqing Zou, Jingheng Ye, Yulu Liu, Yu Wu, Zishan Xu, Yinghui Li, Hai-Tao Zheng, Bingxu An, Zhao Wei, Yong Xu', 'link': 'https://arxiv.org/abs/2502.11890', 'abstract': 'Grammatical error classification plays a crucial role in language learning systems, but existing classification taxonomies often lack rigorous validation, leading to inconsistencies and unreliable feedback. In this paper, we revisit previous classification taxonomies for grammatical errors by introducing a systematic and qualitative evaluation framework. Our approach examines four aspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability. Then, we construct a high-quality grammatical error classification dataset annotated with multiple classification taxonomies and evaluate them grounding on our proposed evaluation framework. Our experiments reveal the drawbacks of existing taxonomies. Our contributions aim to improve the precision and effectiveness of error analysis, providing more understandable and actionable feedback for language learners.', 'abstract_zh': '语法错误分类在语言学习系统中扮演着至关重要的角色，但现有的分类体系往往缺乏严格的验证，导致分类结果存在不一致性和不可靠性。本文重新审视了之前的语法错误分类体系，并引入了一种系统的和定性的评估框架。我们的方法从四个方面对分类体系进行评估，即排他性、覆盖率、平衡性和实用性。然后，我们构建了一个高质量的注释有多种分类体系的语法错误分类数据集，并基于我们提出的评估框架对其进行评估。我们的实验揭示了现有分类体系的不足之处。本文的贡献旨在提高错误分析的精确性和有效性，为语言学习者提供更易于理解和可操作的反馈。', 'title_zh': '重新审视语法错误分类体系'}
{'arxiv_id': 'arXiv:2502.11866', 'title': 'Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire Articles Beyond the Front Page', 'authors': 'Michael McRae', 'link': 'https://arxiv.org/abs/2502.11866', 'abstract': 'I introduce a new large-scale dataset of historical wire articles from U.S. Southern newspapers, spanning 1960-1975 and covering multiple wire services: The Associated Press, United Press International, Newspaper Enterprise Association. Unlike prior work focusing on front-page content, this dataset captures articles across the entire newspaper, offering broader insight into mid-century Southern coverage. The dataset includes a version that has undergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its suitability for quantitative text analysis. Additionally, duplicate versions of articles are retained to enable analysis of editorial differences in language and framing across newspapers. Each article is tagged by wire service, facilitating comparative studies of editorial patterns across agencies. This resource opens new avenues for research in computational social science, digital humanities, and historical linguistics, providing a detailed perspective on how Southern newspapers relayed national and international news during a transformative period in American history. The dataset will be made available upon publication or request for research purposes.', 'abstract_zh': '我介绍了来自美国南部报纸的全新大规模历史电讯文章数据集，时间跨度为1960年至1975年，涵盖了多个电讯社：美联社、国际联合通讯社、报纸企业协会。与以往专注于头版内容的研究不同，本数据集涵盖了整份报纸的文章内容，为更全面了解二十世纪中叶南部地区的报道提供了视角。该数据集还包括经过基于LLM的文本清理流水线处理的版本，减少了OCR噪声，使其更适于定量文本分析。此外，数据集还保留了文章的多个版本，以便分析不同报纸在语言和框架上的编辑差异。每篇文章都标注了电讯社，从而便于对不同机构的编辑模式进行比较研究。该资源为计算社会科学、数字人文和历史语言学领域的研究开辟了新的途径，提供了关于美国历史上这一变革时期的南部报纸如何传递国内外新闻的详细视角。数据集将在出版或研究要求时提供。', 'title_zh': '南方新闻语料库：中世纪电讯文章的大规模数据集，超越头版文章'}
{'arxiv_id': 'arXiv:2502.11874', 'title': 'VAQUUM: Are Vague Quantifiers Grounded in Visual Data?', 'authors': 'Hugh Mee Wong, Rick Nouwen, Albert Gatt', 'link': 'https://arxiv.org/abs/2502.11874', 'abstract': 'Vague quantifiers such as "a few" and "many" are influenced by many contextual factors, including how many objects are present in a given context. In this work, we evaluate the extent to which vision-and-language models (VLMs) are compatible with humans when producing or judging the appropriateness of vague quantifiers in visual contexts. We release a novel dataset, VAQUUM, containing 20300 human ratings on quantified statements across a total of 1089 images. Using this dataset, we compare human judgments and VLM predictions using three different evaluation methods. Our findings show that VLMs, like humans, are influenced by object counts in vague quantifier use. However, we find significant inconsistencies across models in different evaluation settings, suggesting that judging and producing vague quantifiers rely on two different processes.', 'abstract_zh': '模糊量化词如“几个”和“很多”会受到多种上下文因素的影响，包括给定上下文中对象的数量。在此项研究中，我们评估了视觉语言模型（VLMs）在生成或判断视觉上下文中模糊量化词的恰当性时，是否与人类具有兼容性。我们发布了一个名为VAQUUM的新型数据集，包含20300个关于量化陈述的人类评价，涉及总计1089张图片。通过使用这个数据集，我们以三种不同的评估方法比较了人类判断和VLM预测之间的差异。我们的研究结果表明，VLMs与人类一样，在使用模糊量化词时受到对象数量的影响。然而，我们在不同评估设置中发现模型之间存在显著的不一致性，这表明判断和生成模糊量化词依赖于两个不同的过程。', 'title_zh': 'VAQUUM: 模糊量词能否在视觉数据中找到依据？'}
{'arxiv_id': 'arXiv:2502.11862', 'title': 'Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu', 'authors': 'Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze', 'link': 'https://arxiv.org/abs/2502.11862', 'abstract': 'In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries. Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL). However, the relative importance of each type of resource e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear. To address this gap, this study systematically investigates how each resource and its quality affects the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an encrypted version of Manchu texts. Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help. In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap the conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.', 'abstract_zh': '大规模语言模型（LLMs）在上下文中的机器翻译（MT）是低资源MT的一种有前景的方法，因为它可以方便地利用诸如语法书和词典之类的语言资源。这些资源通常会被选择性地整合到提示中，从而使LLMs能够在不进行任何特定训练的情况下，通过其上下文学习能力（ICL）直接进行翻译。然而，每种类型的资源例如词典、语法书和检索到的平行例句对于翻译性能的影响重要性如何，并不完全清楚。为了弥补这一不足，本研究系统地调查了每种资源及其质量如何影响翻译性能，并以满语作为案例研究。为了去除LLM参数中编码的满语先前知识，并分离ICL的影响，我们在实验中还使用了满语的加密版本文本。我们的结果表明，高质量的词典和良好的平行例句非常有帮助，而语法则影响甚微。在后续研究中，展示了上下文MT的一个有希望的应用：通过上下文MT生成平行数据作为传统MT模型的启动方式。当单语数据丰富时，通过上下文MT生成合成平行数据为缓解数据稀缺并构建有效的低资源神经MT系统提供了途径。', 'title_zh': '理解低资源语言的上下文机器翻译：满语案例研究'}
{'arxiv_id': 'arXiv:2502.11866', 'title': 'Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire Articles Beyond the Front Page', 'authors': 'Michael McRae', 'link': 'https://arxiv.org/abs/2502.11866', 'abstract': 'I introduce a new large-scale dataset of historical wire articles from U.S. Southern newspapers, spanning 1960-1975 and covering multiple wire services: The Associated Press, United Press International, Newspaper Enterprise Association. Unlike prior work focusing on front-page content, this dataset captures articles across the entire newspaper, offering broader insight into mid-century Southern coverage. The dataset includes a version that has undergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its suitability for quantitative text analysis. Additionally, duplicate versions of articles are retained to enable analysis of editorial differences in language and framing across newspapers. Each article is tagged by wire service, facilitating comparative studies of editorial patterns across agencies. This resource opens new avenues for research in computational social science, digital humanities, and historical linguistics, providing a detailed perspective on how Southern newspapers relayed national and international news during a transformative period in American history. The dataset will be made available upon publication or request for research purposes.', 'abstract_zh': '我介绍了一个新的大规模历史电讯文章数据集，来源于美国南部报纸，涵盖1960年至1975年的时间段，并包括多个通讯社的文章，如《联合 Press 通讯社》(The Associated Press)、UPI (United Press International) 和 Newspaper Enterprise Association。与以往专注于头版内容的研究不同，该数据集涵盖了整份报纸的文章，提供了更全面的视角，深入了解20世纪中期南部地区的报道情况。数据集还包括通过基于大语言模型的文本清理管道处理过的一个版本，这有助于减少光学字符识别（OCR）噪声，使其更适合进行定量文本分析。此外，数据集中保留了文章的多个版本，这为分析不同报纸在语言和框架上的编辑差异提供了便利。每篇文章都通过通讯社进行标注，有助于对不同机构的编辑模式进行比较研究。该资源为计算社会科学、数字人文和历史语言学领域的研究开辟了新的途径，提供了详细视角，展示了美国历史上的转型时期南部报纸如何传达国内外新闻。数据集将在发表或应研究需求时提供。', 'title_zh': '南方新闻语料库：中间世纪电讯文章的大规模数据集，超越头版内容'}
{'arxiv_id': 'arXiv:2502.11861', 'title': 'Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics', 'authors': 'Shuqi Yang, Mingrui Jing, Shuai Wang, Jiaxin Kou, Manfei Shi, Weijie Xing, Yan Hu, Zheng Zhu', 'link': 'https://arxiv.org/abs/2502.11861', 'abstract': 'This study reviewed the use of Large Language Models (LLMs) in healthcare, focusing on their training corpora, customization techniques, and evaluation metrics. A systematic search of studies from 2021 to 2024 identified 61 articles. Four types of corpora were used: clinical resources, literature, open-source datasets, and web-crawled data. Common construction techniques included pre-training, prompt engineering, and retrieval-augmented generation, with 44 studies combining multiple methods. Evaluation metrics were categorized into process, usability, and outcome metrics, with outcome metrics divided into model-based and expert-assessed outcomes. The study identified critical gaps in corpus fairness, which contributed to biases from geographic, cultural, and socio-economic factors. The reliance on unverified or unstructured data highlighted the need for better integration of evidence-based clinical guidelines. Future research should focus on developing a tiered corpus architecture with vetted sources and dynamic weighting, while ensuring model transparency. Additionally, the lack of standardized evaluation frameworks for domain-specific models called for comprehensive validation of LLMs in real-world healthcare settings.', 'abstract_zh': '本研究回顾了大型语言模型（LLMs）在医疗保健领域的应用，重点在于其训练语料库、定制技术以及评估指标。通过对2021年至2024年间发表的研究进行系统的文献搜索，共识别出61篇文章。所使用的四种类型语料库包括临床资源、文献、开源数据集以及网络爬取数据。常见的构建技术包括预训练、提示工程和检索增强生成，其中44篇文章结合了多种方法。评估指标被分类为过程指标、可用性指标和结果指标，其中结果指标进一步细分为模型评估结果和专家评估结果。研究发现，语料库的公平性存在关键差距，这可能导致地理、文化和社会经济因素引起的偏差。依赖于未经验证或未结构化的数据突显出更好地整合基于证据的临床指南的必要性。未来的研究应聚焦于开发含有经过验证来源的分层语料库架构，并确保模型的透明性。此外，缺乏特定领域的标准化评估框架要求在实际医疗保健环境中进行LLMs的全面验证。', 'title_zh': '探索大型语言模型在医疗健康领域的应用：关于语料来源、定制策略和评价指标的见解'}
{'arxiv_id': 'arXiv:2502.11856', 'title': 'LLMs as a synthesis between symbolic and continuous approaches to language', 'authors': 'Gemma Boleda', 'link': 'https://arxiv.org/abs/2502.11856', 'abstract': 'Since the middle of the 20th century, a fierce battle is being fought between symbolic and continuous approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the continuous camp has won, or dismissed as an irrelevant engineering development. However, in this position paper I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both continuous/distributed and symbolic/discrete-like representations and computations; 2) models trained on language make use this flexibility. In particular, I review recent research in mechanistic interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it is also what makes them particularly interesting for the study of language and cognition. Is it time for peace?', 'abstract_zh': '自20世纪中叶以来，符号主义和连续主义在语言和认知领域的斗争愈演愈烈。深度学习模型的成功，尤其是大型语言模型（LLMs）的成功，有时被视为连续主义阵营的胜利，有时则被忽视为一项无关紧要的工程进展。然而，在本文中，我argue（argue改为主张更为符合学术语气）认为，语言领域的深度学习模型实际上融合了这两个传统。这是因为1) 深度学习架构可以同时支持连续/分布式和符号/离散型的表示和计算；2) 语言训练的模型利用了这种灵活性。具体而言，我回顾了近期关于机制可解释性的研究，展示了大量形态语法知识在LLMs中以近乎离散的方式编码。这种研究方向表明，不同的行为以一种新兴的方式出现，并且模型在需要时灵活地在两种模式之间（以及两者之间的一切）切换。这可能是它们巨大成功的主要原因之一；同时，这也是它们对语言和认知研究特别有趣的原因之一。现在是时候停战了？', 'title_zh': 'LLMs作为符号方法和连续方法相结合的语言处理方式'}
{'arxiv_id': 'arXiv:2502.11862', 'title': 'Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu', 'authors': 'Renhao Pei, Yihong Liu, Peiqin Lin, François Yvon, Hinrich Schütze', 'link': 'https://arxiv.org/abs/2502.11862', 'abstract': 'In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries. Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL). However, the relative importance of each type of resource e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear. To address this gap, this study systematically investigates how each resource and its quality affects the translation performance, with the Manchu language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an encrypted version of Manchu texts. Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help. In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap the conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.', 'abstract_zh': '基于大型语言模型（LLMs）的上下文内机器翻译（MT）是低资源MT的一种有前途的方法，因为它可以方便地利用诸如语法书籍和词典等语言资源。这些资源通常可以选择性地集成到提示中，从而使LLMs能够直接进行翻译，而无需任何特定训练，借助其上下文内学习能力（ICL）。然而，每种类型资源（例如，词典、语法书籍和检索到的平行示例）的重要性的相对重要性并不完全明确。为了解决这一问题，本研究系统地探讨了每种资源及其质量如何影响翻译性能，并以满语作为案例进行研究。为了消除LLM参数中嵌入的满语先验知识并突出ICL的效果，我们还测试了满语的加密版本。研究结果表明，高质量的词典和良好的平行示例非常有帮助，而语法几乎没有帮助。在后续研究中，我们展示了上下文内MT的一种有前景的应用：平行数据增强作为传统MT模型的一种种子方法。当存在大量单语数据时，通过上下文内MT生成合成平行数据是缓解数据稀缺、构建有效的低资源神经MT系统的可行途径。', 'title_zh': '低资源语言中的上下文内机器翻译理解：满语案例研究'}
{'arxiv_id': 'arXiv:2502.11843', 'title': 'Can LLM Agents Maintain a Persona in Discourse?', 'authors': 'Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11843', 'abstract': 'Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.', 'abstract_zh': '大型语言模型（LLMs）广泛用作对话代理，在教育、法律、医学等多个领域展示了其各种能力。然而，LLMs 往往会表现出上下文切换行为，导致对话缺乏一致性和可解释的个性对齐。对心理特质的遵从性缺乏全面分析，尤其是在双边（成对）对话的情况下。我们从两个视角来探讨这一挑战：首先，使用两个对话代理生成关于特定主题的对话，其中一个代理具有OCEAN框架（开放性、尽责性、外向性、宜人性、情绪稳定性）中指定的高/低个性特征；随后，使用多个评判代理来推断原始分配的心理特质，以探索预测一致性和模型间的共识，以及与所分配个性特征的对齐程度。我们的研究结果表明，虽然LLMs 可以被引导进行个性驱动的对话，但它们在保持个性特征方面的能力因模型组合和对话设置的不同而存在显著差异。这些不一致性强调了在LLMs 中实现稳定和可解释的个性对齐对话所面临的挑战。', 'title_zh': 'LLM代理在对话中能否维持个性？'}
{'arxiv_id': 'arXiv:2502.11861', 'title': 'Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics', 'authors': 'Shuqi Yang, Mingrui Jing, Shuai Wang, Jiaxin Kou, Manfei Shi, Weijie Xing, Yan Hu, Zheng Zhu', 'link': 'https://arxiv.org/abs/2502.11861', 'abstract': 'This study reviewed the use of Large Language Models (LLMs) in healthcare, focusing on their training corpora, customization techniques, and evaluation metrics. A systematic search of studies from 2021 to 2024 identified 61 articles. Four types of corpora were used: clinical resources, literature, open-source datasets, and web-crawled data. Common construction techniques included pre-training, prompt engineering, and retrieval-augmented generation, with 44 studies combining multiple methods. Evaluation metrics were categorized into process, usability, and outcome metrics, with outcome metrics divided into model-based and expert-assessed outcomes. The study identified critical gaps in corpus fairness, which contributed to biases from geographic, cultural, and socio-economic factors. The reliance on unverified or unstructured data highlighted the need for better integration of evidence-based clinical guidelines. Future research should focus on developing a tiered corpus architecture with vetted sources and dynamic weighting, while ensuring model transparency. Additionally, the lack of standardized evaluation frameworks for domain-specific models called for comprehensive validation of LLMs in real-world healthcare settings.', 'abstract_zh': '本研究回顾了大型语言模型（LLMs）在医疗健康领域的应用，重点关注其训练语料库、定制技术和评估指标。从2021年至2024年，系统检索了61篇相关研究文章。使用的语料库类型包括临床资源、文献、开源数据集和网络爬取数据。常见的构建技术包括预训练、提示工程和检索增强生成，其中44篇研究结合了多种方法。评估指标被分为过程指标、可用性指标和成果指标，成果指标进一步分为模型基于和专家评估的成果。研究发现语料库的公平性存在关键缺口，这源于地理、文化和社会经济因素带来的偏差。依赖未经验证或未结构化的数据突显了更好地整合基于证据的临床指南的必要性。未来研究应侧重于开发具有审核来源的分层语料库架构，并确保模型的透明度。此外，由于缺乏针对特定领域模型的标准化评估框架，需要全面验证LLMs在实际医疗服务中的表现。', 'title_zh': '探索医疗领域的大语言模型：关于语料来源、定制策略和评估指标的见解'}
{'arxiv_id': 'arXiv:2502.11830', 'title': 'Text Classification in the LLM Era - Where do we stand?', 'authors': 'Sowmya Vajjala, Shwetali Shimangaud', 'link': 'https://arxiv.org/abs/2502.11830', 'abstract': 'Large Language Models revolutionized NLP and showed dramatic performance improvements across several tasks. In this paper, we investigated the role of such language models in text classification and how they compare with other approaches relying on smaller pre-trained language models. Considering 32 datasets spanning 8 languages, we compared zero-shot classification, few-shot fine-tuning and synthetic data based classifiers with classifiers built using the complete human labeled dataset. Our results show that zero-shot approaches do well for sentiment classification, but are outperformed by other approaches for the rest of the tasks, and synthetic data sourced from multiple LLMs can build better classifiers than zero-shot open LLMs. We also see wide performance disparities across languages in all the classification scenarios. We expect that these findings would guide practitioners working on developing text classification systems across languages.', 'abstract_zh': '大型语言模型彻底变革了自然语言处理（NLP），并在多个任务中展示了显著的性能提升。在本论文中，我们研究了此类语言模型在文本分类中的作用，以及它们与其他依赖较小预训练语言模型的方法的对比。我们使用了涵盖8种语言的32个数据集，将零样本分类、少样本微调和基于合成数据的分类器与基于完整人工标注数据集构建的分类器进行了比较。结果显示，零样本方法在情感分类任务中表现良好，但在其他任务中则被其他方法超越。来自多个语言模型的合成数据能够构建优于零样本开放语言模型的分类器。此外，我们在所有分类场景中都观察到了跨语言的广泛性能差异。我们预计这些发现将指导跨语言文本分类系统开发的实践者。', 'title_zh': '在大语言模型时代进行文本分类：我们处于何种位置？'}
{'arxiv_id': 'arXiv:2502.11856', 'title': 'LLMs as a synthesis between symbolic and continuous approaches to language', 'authors': 'Gemma Boleda', 'link': 'https://arxiv.org/abs/2502.11856', 'abstract': 'Since the middle of the 20th century, a fierce battle is being fought between symbolic and continuous approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the continuous camp has won, or dismissed as an irrelevant engineering development. However, in this position paper I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both continuous/distributed and symbolic/discrete-like representations and computations; 2) models trained on language make use this flexibility. In particular, I review recent research in mechanistic interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it is also what makes them particularly interesting for the study of language and cognition. Is it time for peace?', 'abstract_zh': '自20世纪中叶以来，语言和认知的符号主义和连续主义方法之间展开了一场激烈的争论。深度学习模型的成功，尤其是大型语言模型（LLMs）的成功，有时被视为连续主义阵营获胜的标志，有时则被视为无关的工程发展。然而，在这篇观点论文中，我认为深度学习语言模型实际上是这两种传统的融合。这主要有两个原因：1）深度学习架构能够同时支持连续分布式和符号离散式的表示与计算；2）被训练用于语言的模型利用了这种灵活性。特别是，我回顾了最近关于机制可解释性的研究，展示了形态句法知识在大型语言模型中以接近离散的方式编码的情况。这种研究路径表明，不同行为以一种产生式的方式出现，并且模型根据需要灵活地在两种模式之间（以及它们之间的各种模式）切换。这可能是它们取得惊人成功的主要原因之一；同时，这也是它们在语言和认知研究中显得特别有趣的根源。现在是时候寻求和解了吗？', 'title_zh': '大语言模型作为符号方法和连续方法在语言处理中的融合'}
{'arxiv_id': 'arXiv:2502.11829', 'title': 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities', 'authors': 'Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu', 'link': 'https://arxiv.org/abs/2502.11829', 'abstract': "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at this https URL.", 'abstract_zh': '本文介绍了Code-Vision，这是一个用于评估多模态大型语言模型（MLLMs）的逻辑理解和代码生成能力的基准。它挑战MLLMs根据给定的流程图生成一个满足特定功能要求的正确程序，而流程图可视化地表示了所需的算法或过程。Code-Vision 包含三个子集：HumanEval-V、Algorithm 和 MATH，分别评估MLLMs在基本编程、算法和数学问题解决领域的编码能力。我们的实验在Code-Vision上评估了12个MLLMs。实验结果表明，专有模型和开源模型之间存在显著的性能差异。在困难问题上，GPT-4o 的通过率达到了79.3%，但最好的开源模型仅达到了15%。进一步的实验揭示了与MMCode和MathVista等其他多模态推理基准相比，Code-Vision 可能会提出独特的挑战。我们还探讨了开源模型表现不佳的原因。所有数据和代码均可在以下网址获取：https://example.com（请将上述占位符替换为实际网址）。', 'title_zh': 'Code-Vision：评估多模态LLM的逻辑理解与代码生成能力'}
{'arxiv_id': 'arXiv:2502.11824', 'title': 'M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis', 'authors': 'Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Barbara Plank, Yun Xue', 'link': 'https://arxiv.org/abs/2502.11824', 'abstract': 'Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.', 'abstract_zh': '基于方面的情感分析（ABSA）是信息提取和情感分析中的一个关键任务，旨在识别文本中与情感元素相关的方面。然而，现有的ABSA数据集主要以英语为中心，限制了多语言评估和研究的范围。为了解决这一问题，我们提出了M-ABSA，这是一个涵盖7个领域和21种语言的综合数据集，使其成为迄今为止最全面的多语言平行ABSA数据集。我们的主要关注点在于三元组提取，即识别方面术语、方面类别和情感极性。该数据集通过自动翻译与人工审核相结合的方式构建，以确保质量。我们使用多种基线进行广泛的实验，评估其在M-ABSA上的性能和适用性。我们的实证研究表明，该数据集能够支持多语言和多领域迁移学习、大规模语言模型评估等多种多语言ABSA研究任务，突显了其包容性及对推动多语言ABSA研究发展的潜在价值。', 'title_zh': 'M-ABSA：一种用于方面级情感分析的多语言数据集'}
{'arxiv_id': 'arXiv:2502.11843', 'title': 'Can LLM Agents Maintain a Persona in Discourse?', 'authors': 'Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11843', 'abstract': 'Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.', 'abstract_zh': '大规模语言模型（LLMs）广泛用于对话代理领域，它们在教育、法律、医学等多个领域中发挥了其各种能力。然而，LLMs经常表现出上下文转换的行为，导致缺乏一致性和可解释性的人格匹配交互。对于心理特质的遵守缺乏全面分析，尤其是在双边（成对）对话中。我们从两个视角来探讨这一挑战：首先，使用两个对话代理生成特定话题的讨论，并赋予每种气质（即开放性、责任心、外向性、随和性、神经质）高/低等级；随后，使用多个评判代理来推断原始分配的气质，以探索预测一致性、模型间一致性以及与分配的人格的匹配程度。我们的研究发现，虽然LLMs可以被引导进行基于人格的对话，但它们维持特定气质的能力在不同模型组合和话题设置下存在显著差异。这些不一致性突显了在LLMs中实现稳定和可解释的人格匹配交互的挑战。', 'title_zh': 'LLM代理在对话中能否保持人设？'}
{'arxiv_id': 'arXiv:2502.11830', 'title': 'Text Classification in the LLM Era - Where do we stand?', 'authors': 'Sowmya Vajjala, Shwetali Shimangaud', 'link': 'https://arxiv.org/abs/2502.11830', 'abstract': 'Large Language Models revolutionized NLP and showed dramatic performance improvements across several tasks. In this paper, we investigated the role of such language models in text classification and how they compare with other approaches relying on smaller pre-trained language models. Considering 32 datasets spanning 8 languages, we compared zero-shot classification, few-shot fine-tuning and synthetic data based classifiers with classifiers built using the complete human labeled dataset. Our results show that zero-shot approaches do well for sentiment classification, but are outperformed by other approaches for the rest of the tasks, and synthetic data sourced from multiple LLMs can build better classifiers than zero-shot open LLMs. We also see wide performance disparities across languages in all the classification scenarios. We expect that these findings would guide practitioners working on developing text classification systems across languages.', 'abstract_zh': '大型语言模型在自然语言处理领域引发了革命，并在多种任务上展示了显著的性能提升。本文探讨了这类语言模型在文本分类中的作用及其与其他依赖较小预训练语言模型的方法的对比。我们使用了涵盖8种语言的32个数据集，比较了零样本分类、少样本微调和基于合成数据的分类方法，与使用完整人工标注数据集构建的分类器的性能。结果显示，零样本方法在情感分类方面表现良好，但在其他任务上则被其他方法超越；而多来源LLM生成的合成数据可以构建出优于零样本开放LLM的分类器。此外，我们在所有分类场景中都观察到了不同语言间广泛的性能差异。我们预计这些发现将指导跨语言文本分类系统的开发人员。', 'title_zh': '大语言模型时代的时间：文本分类进展如何？'}
{'arxiv_id': 'arXiv:2502.11812', 'title': 'Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis', 'authors': 'Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou', 'link': 'https://arxiv.org/abs/2502.11812', 'abstract': 'Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.', 'abstract_zh': '大型语言模型（LLMs）微调显著提高了其性能，但其背后的机制仍不太清楚。本文旨在通过电路分析，一种在机制可解释性方法中广泛应用的工具，对微调过程进行深入解释。不同于以往专注于预训练模型已表现良好的任务的研究工作 \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}，我们开发了一组数学任务，在这些任务中，微调能带来显著的性能提升，更接近实际应用场景。在我们的实验中，我们识别了微调过程中不同检查点的电路，并考察了电路分析、微调方法和任务复杂性之间的相互作用。首先，我们发现，在微调前后，电路中的节点相似度保持较高，但它们的边发生了显著变化，这与先前的工作 \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} 所揭示的不同（即电路仅在微调后添加一些额外组件）存在差异。基于这些观察，我们开发了一种电路感知的低秩适配（Circuit-aware Low-Rank Adaptation, LoRA）方法，该方法基于电路中的边变化对层进行排名。实验结果表明，与标准LoRA相比，基于电路的LoRA算法在相似参数量的情况下平均提高了2.46%的性能。此外，我们还探讨了如何使用子任务电路的组合来增强组合任务中的微调，从而为设计此类任务提供了新的见解，并加深了对电路动力学和微调机制的理解。', 'title_zh': '通过电路分析探究大语言模型微调机制的理解'}
{'arxiv_id': 'arXiv:2502.11829', 'title': 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities', 'authors': 'Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu', 'link': 'https://arxiv.org/abs/2502.11829', 'abstract': "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at this https URL.", 'abstract_zh': '本文介绍了Code-Vision，这是一个用于评估多模态大型语言模型（MLLMs）的逻辑理解和代码生成能力的基准。该基准挑战MLLMs根据给定的流程图生成满足特定功能要求的正确程序，而流程图则直观地表示了所需算法或过程。Code-Vision 包含三个子集：HumanEval-V、Algorithm 和 MATH，分别评估MLLMs在基本编程、算法和数学问题解决领域的编码能力。我们的实验在Code-Vision上评估了12个MLLMs。实验结果表明，专有模型和开源模型之间的性能差距很大。在难题上，GPT-4o 可以实现79.3%的通过率，但最好的开源模型仅能达到15%。进一步的实验表明，与多模态推理基准MMCode和MathVista相比，Code-Vision 可能会提出独特的挑战。我们还探讨了开源模型表现不佳的原因。所有数据和代码均可在此访问：https://example.com（请将https://example.com替换为实际的网址）。', 'title_zh': 'Code-Vision：评估多模态大语言模型的逻辑理解和代码生成能力'}
{'arxiv_id': 'arXiv:2502.11811', 'title': 'FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models', 'authors': 'Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng', 'link': 'https://arxiv.org/abs/2502.11811', 'abstract': 'Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance this http URL methods use re-ranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying the potential clues from multiple documents using contextual information, then ranking them by relevance, and finally retaining the least clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG consisting of a clue extractor, a re-ranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) Clue extractor firstly uses sentences containing the answer and similar ones as fine-tuned targets, aiming at extracting sufficient potential clues; (2) Re-ranker is trained to prioritize effective clues based on the real feedback from generation module, with clues capable of generating correct answer as positive samples and others as negative; (3) Truncator takes the minimum clues needed to answer the question (truncation point) as fine-tuned targets, and performs truncation on the re-ranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly outperforms baselines in terms of performance and inference cost. Further analysis on each module shows the effectiveness of our optimizations for complex reasoning.', 'abstract_zh': '包含噪声的检索文档将妨碍检索增强生成（RAG）检测答案线索的能力，因此需要噪声过滤机制来增强这一点。（原文中的“http URL”假设为网址，此处已删除）现有的方法通常使用再排序或总结来识别最相关的句子，但从大规模和复杂的文档中直接和准确地定位答案线索仍具有挑战性。与这些文档级别的操作不同，我们将噪声过滤视为一个句子级别的Min-Max优化问题：首先利用上下文信息从多个文档中识别潜在线索，然后根据相关性对其进行排序，最后通过截断保留最少的线索。在本文中，我们提出了一种名为FineFilter的新颖细粒度噪声过滤机制，该机制由线索提取器、再排序器和截断器三部分组成。我们针对每个模块进行了优化，以应对复杂的推理挑战：（1）线索提取器首先使用包含答案和类似句子作为微调目标，旨在提取足够的潜在线索；（2）再排序器根据生成模块的实际反馈进行训练，优先排序能够生成正确答案的线索作为正样本，其他线索作为负样本；（3）截断器将需要回答问题的最少线索（截断点）作为微调目标，并对再排序后的线索进行截断，以实现细粒度的噪声过滤。在三个问答数据集上的实验表明，FineFilter在性能和推理成本方面明显优于基线方法。进一步对每个模块的分析显示，我们的优化方法在复杂推理方面具有有效性。', 'title_zh': 'FineFilter：面向检索增强大型语言模型的细粒度噪声过滤机制'}
{'arxiv_id': 'arXiv:2502.11806', 'title': 'Exploring Translation Mechanism of Large Language Models', 'authors': 'Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Min Zhang', 'link': 'https://arxiv.org/abs/2502.11806', 'abstract': 'Large language models (LLMs) have succeeded remarkably in multilingual translation tasks. However, the inherent translation mechanisms of LLMs remain poorly understood, largely due to sophisticated architectures and vast parameter scales. In response to this issue, this study explores the translation mechanism of LLM from the perspective of computational components (e.g., attention heads and MLPs). Path patching is utilized to explore causal relationships between components, detecting those crucial for translation tasks and subsequently analyzing their behavioral patterns in human-interpretable terms. Comprehensive analysis reveals that translation is predominantly facilitated by a sparse subset of specialized attention heads (less than 5\\%), which extract source language, indicator, and positional features. MLPs subsequently integrate and process these features by transiting towards English-centric latent representations. Notably, building on the above findings, targeted fine-tuning of only 64 heads achieves translation improvement comparable to full-parameter tuning while preserving general capabilities.', 'abstract_zh': '大语言模型（LLMs）在多语言翻译任务中取得了显著的成功。然而，LLMs 内在的翻译机制仍然不够清楚，主要原因是它们具有复杂的架构和庞大的参数规模。为了解决这个问题，本研究从计算组件的角度（如注意力头和MLP）探索了LLMs的翻译机制。通过路径修补技术，研究了组件之间的因果关系，识别出对翻译任务至关重要的组件，并进一步以人类可理解的方式分析它们的行为模式。综合分析表明，翻译主要由少于5%的专门注意力头促进（一个稀疏的子集），这些注意力头提取源语言、指示词和位置特征。随后，MLP整合和处理这些特征，通过转变为以英语为中心的潜在表示来进行处理。值得注意的是，基于上述发现，仅对64个头部进行目标微调，就能实现与全参数微调相当的翻译改进，同时保留一般能力。', 'title_zh': '探索大型语言模型的翻译机制'}
{'arxiv_id': 'arXiv:2502.11789', 'title': 'Personality Editing for Language Models through Relevant Knowledge Editing', 'authors': 'Seojin Hwang, Yumin Kim, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11789', 'abstract': "Large Language Models (LLMs) play a vital role in applications like conversational agents and content creation, where controlling a model's personality is crucial for maintaining tone, consistency, and engagement. However, traditional prompt-based techniques for controlling personality often fall short, as they do not effectively mitigate the model's inherent biases. In this paper, we introduce a novel method PALETTE that enhances personality control through knowledge editing. By generating adjustment queries inspired by psychological assessments, our approach systematically adjusts responses to personality-related queries similar to modifying factual knowledge, thereby achieving controlled shifts in personality traits. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.", 'abstract_zh': '大规模语言模型（LLMs）在对话代理和内容创作等应用中起着至关重要的作用，控制模型的个性对于保持语气、一致性以及吸引用户极为重要。然而，传统的基于提示的技术在控制个性方面往往效果不佳，因为这些方法未能有效减轻模型固有的偏见。在本文中，我们提出了一种名为PALETTE的新方法，通过知识编辑增强个性控制能力。通过生成来自心理评估的调整查询，我们的方法系统地调整与个性化相关查询的响应，类似于修改事实性知识，从而实现可控的个性特征转变。来自自动评估和人工评估的实验结果表明，我们的方法能够使LLMs的个性控制更加稳定和平衡。', 'title_zh': '通过相关知识编辑对语言模型进行个性编辑'}
{'arxiv_id': 'arXiv:2502.11824', 'title': 'M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis', 'authors': 'Chengyan Wu, Bolei Ma, Yihong Liu, Zheyu Zhang, Ningyuan Deng, Yanshu Li, Baolan Chen, Yi Zhang, Barbara Plank, Yun Xue', 'link': 'https://arxiv.org/abs/2502.11824', 'abstract': 'Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.', 'abstract_zh': '情感分析中的方面识别（Aspect-based sentiment analysis, ABSA）是信息提取和情感分析中的关键任务，其目标是识别文本中与情感元素相关的方面。然而，现有的ABSA数据集主要以英语为中心，限制了多语言评估和研究的范围。为了解决这一问题，我们介绍了一个涵盖7个领域和21种语言的综合数据集M-ABSA，使其成为迄今为止规模最大、涵盖语言最多的ABSA平行多语言数据集。我们的主要焦点是三元组提取，涉及识别方面术语、方面类别和情感极性。该数据集通过自动翻译并结合人工审核构建，以确保质量。我们使用多种基准模型进行了广泛的实验，评估其在M-ABSA上的性能和兼容性。我们的实证发现表明，该数据集能够支持多样化的评估任务，如多语言和多领域迁移学习及大规模语言模型评估，突显了其包容性及其在推动多语言ABSA研究方面的潜力。', 'title_zh': 'M-ABSA：多语言 aspect 基础情感分析数据集'}
{'arxiv_id': 'arXiv:2502.11779', 'title': 'Efficient Response Generation Method Selection for Fine-Tuning Large Language Models', 'authors': 'Xuan Ren, Qi Chen, Lingqiao Liu', 'link': 'https://arxiv.org/abs/2502.11779', 'abstract': "The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of output variation used in training can affect the model's performance. This raises an important question: how can we generate the most effective output from the many possible response generation strategy options? Rather than relying on the traditional but resource-intensive train-and-evaluate approach, this paper proposes a scalable, approximate method for estimating the quality of a small subset of generated training data derived from the same input. We then evaluate how well this small subset of generated output fits the target model we are trying to train. We present a large-scale benchmark covering diverse reasoning-based datasets to support our study.\nThe central idea is that a good output should closely resemble the output generated by the target LLM. We formalize this 'closeness' as the expected alignment score between a candidate output and the output sampled from the target LLM. We connect this measurement to the perplexity metric used in previous literature and demonstrate that leveraging an alignment-based metric can provide better predictions of model performance. Using this strategy, we can evaluate a small subset of the generated output from each response generation strategy option, then select the most effective strategy. We show that an LLM trained on data generated by the selected strategy could lead to a significant performance gain in many cases.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，确保符合学术规范：\n\n大型语言模型（LLMs）微调的训练数据通常以输入-输出对的形式结构化。然而，对于许多任务，同一个输入可以有多个同等有效的输出变体。近期的研究发现，用于训练的输出变体选择可能会影响模型的性能。这引发了一个重要的问题：在众多可能的生成策略选项中，如何生成最有效的输出？本文提出了一种可扩展且近似的方法，用于估算源自同一输入的一小部分生成训练数据的质量。然后，我们评估这一小部分生成输出是否适合我们试图训练的目标模型。我们提供了一个涵盖多种基于推理的数据集的大规模基准，以支持我们的研究。\n\n核心思想是，一个好的输出应与目标LLM生成的输出高度相似。我们将这种“相似性”形式化为候选输出与目标LLM采样的输出之间的期望对齐得分。我们将这一度量与以往文献中使用的困惑度指标连接起来，并证明利用基于对齐的指标可以更好地预测模型性能。使用这一策略，我们可以评估每个生成策略选项的生成输出的一小部分，然后选择最有效的策略。我们展示了使用选定策略生成的数据训练的LLM在许多情况下能够带来显著的性能提升。', 'title_zh': '大型语言模型微调中的高效响应生成方法选择方法'}
{'arxiv_id': 'arXiv:2502.11812', 'title': 'Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis', 'authors': 'Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou', 'link': 'https://arxiv.org/abs/2502.11812', 'abstract': 'Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.', 'abstract_zh': '对大型语言模型（LLMs）进行微调可以显著提高其性能，但其背后的机制仍不完全清楚。本文旨在通过电路分析，一种机制可解释性（MI）中常用的方法，深入解析微调过程。与前人研究\\[ \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} \\]主要关注预训练模型已经在该任务上表现出色的任务不同，我们开发了一组数学任务，在这些任务上，微调提供了显著的性能提升，更接近实际应用场景。在我们的实验中，我们在微调过程中的各个检查点识别电路，并分析电路分析、微调方法与任务复杂性之间的相互作用。我们首先发现，在微调前后，电路的节点保持较高的相似性，但边发生了重大变化。这与之前的研究所示\\[ \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} \\]存在差异，前人研究显示微调后电路仅添加一些额外的组件。基于这些观察，我们开发了一种电路意识低秩适应（LoRA）方法，该方法根据电路中边的变化为层分配秩。实验结果表明，我们的电路驱动的LoRA算法在与标准LoRA参数量相似的情况下，实现了平均性能提升2.46%。此外，我们还探讨了如何结合子任务的电路来增强组合任务中的微调，为这类任务的设计提供了新的见解，并加深了对电路动态和微调机制的理解。', 'title_zh': '通过电路分析理解大规模语言模型微调机制'}
{'arxiv_id': 'arXiv:2502.11771', 'title': 'The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It', 'authors': 'Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi', 'link': 'https://arxiv.org/abs/2502.11771', 'abstract': "The ability of large language models (LLMs) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on $\\textit{consistency heads}$--attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.", 'abstract_zh': '大型语言模型（LLMs）验证其输出并识别潜在错误的能力对于确保其鲁棒性和可靠性至关重要。然而，当前研究表明，LLMs 在自我纠正方面存在困难，面临显著的错误检测挑战。尽管已有研究探索了增强LLMs 自我纠正能力的方法，但对于模型内部支持错误检测的机制理解仍相对较少。在本论文中，我们对其机制性地分析了LLMs 的错误检测能力，重点关注简单的算术问题。通过电路分析，我们确定了四个较小规模 LLMs 中负责检测算术错误的计算子图。研究发现，所有模型都高度依赖于“一致性头”——用于评估算术解中数值表面一致性注意力头。此外，我们观察到，模型内部的算术计算主要发生在较高层，而验证过程则发生在中间层，在最终结果完全编码之前。这种算术计算与验证之间的结构分离似乎解释了当前LLMs 在检测即使是简单算术错误时也面临困难的原因。', 'title_zh': '验证差距：语言模型在计算算术问题时验证不足的机制分析'}
{'arxiv_id': 'arXiv:2502.11811', 'title': 'FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models', 'authors': 'Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng', 'link': 'https://arxiv.org/abs/2502.11811', 'abstract': 'Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance this http URL methods use re-ranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying the potential clues from multiple documents using contextual information, then ranking them by relevance, and finally retaining the least clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG consisting of a clue extractor, a re-ranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) Clue extractor firstly uses sentences containing the answer and similar ones as fine-tuned targets, aiming at extracting sufficient potential clues; (2) Re-ranker is trained to prioritize effective clues based on the real feedback from generation module, with clues capable of generating correct answer as positive samples and others as negative; (3) Truncator takes the minimum clues needed to answer the question (truncation point) as fine-tuned targets, and performs truncation on the re-ranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly outperforms baselines in terms of performance and inference cost. Further analysis on each module shows the effectiveness of our optimizations for complex reasoning.', 'abstract_zh': '含有噪声的检索文档将妨碍 Retrieval-Augmented Generation (RAG) 检测答案线索的能力，因此需要噪声过滤机制来增强此功能。现有方法通常使用重排序或总结来识别最相关的句子，但直接且精确地从这些大规模且复杂的文档中定位答案线索仍然极具挑战性。与这些文档级别的操作不同，我们将其噪声过滤视为一个句子级别的 MinMax 最优化问题：首先利用上下文信息在多个文档中识别潜在线索，然后按相关性进行排序，最后通过截断保留最少的线索。在本文中，我们提出了一种新颖的细粒度噪声过滤机制 FineFilter，该机制包含线索提取器、重排序器和截断器三个模块。我们优化每个模块以应对复杂的推理挑战：(1) 线索提取器首先使用包含答案及其相似句子作为微调目标，旨在提取足够的潜在线索；(2) 重排序器基于生成模块的真实反馈进行训练，优先排序有效线索，具有生成正确答案的线索作为正样本，其他作为负样本；(3) 截断器将需要回答问题的最少线索（截断点）作为微调目标，并在排序后的线索上进行截断以实现细粒度的噪声过滤。在三个问答数据集上的实验表明，FineFilter 在性能和推理成本方面显著优于基线方法。进一步对每个模块的分析表明，我们的优化措施在复杂推理方面具有有效性。', 'title_zh': 'FineFilter：面向检索增强大型语言模型的细粒度噪声过滤机制'}
{'arxiv_id': 'arXiv:2502.11806', 'title': 'Exploring Translation Mechanism of Large Language Models', 'authors': 'Hongbin Zhang, Kehai Chen, Xuefeng Bai, Xiucheng Li, Min Zhang', 'link': 'https://arxiv.org/abs/2502.11806', 'abstract': 'Large language models (LLMs) have succeeded remarkably in multilingual translation tasks. However, the inherent translation mechanisms of LLMs remain poorly understood, largely due to sophisticated architectures and vast parameter scales. In response to this issue, this study explores the translation mechanism of LLM from the perspective of computational components (e.g., attention heads and MLPs). Path patching is utilized to explore causal relationships between components, detecting those crucial for translation tasks and subsequently analyzing their behavioral patterns in human-interpretable terms. Comprehensive analysis reveals that translation is predominantly facilitated by a sparse subset of specialized attention heads (less than 5\\%), which extract source language, indicator, and positional features. MLPs subsequently integrate and process these features by transiting towards English-centric latent representations. Notably, building on the above findings, targeted fine-tuning of only 64 heads achieves translation improvement comparable to full-parameter tuning while preserving general capabilities.', 'abstract_zh': '大型语言模型（LLMs）在多语言翻译任务中取得了显著成功。然而，LLMs 内在的翻译机制仍然难以理解，主要原因在于其复杂的架构和庞大的参数规模。为应对这一问题，本研究从计算组件的角度（如注意力头和MLPs）探讨了LLMs的翻译机制。我们利用路径修补（path patching）方法来探究组件之间的因果关系，识别出对翻译任务至关重要的组件，并进一步以人类可解释的方式分析这些组件的行为模式。综合分析表明，翻译主要由不到5%的稀疏且专门化注意力头所促进，这些注意力头提取源语言、指标和位置特征。MLPs随后整合并处理这些特征，转移到以英语为中心的潜在表示中。值得注意的是，基于上述发现，仅对64个注意力头进行目标化的微调，其翻译性能的提升与全参数调整相当，同时保持了泛化能力。', 'title_zh': '探索大型语言模型的翻译机制'}
{'arxiv_id': 'arXiv:2502.11789', 'title': 'Personality Editing for Language Models through Relevant Knowledge Editing', 'authors': 'Seojin Hwang, Yumin Kim, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11789', 'abstract': "Large Language Models (LLMs) play a vital role in applications like conversational agents and content creation, where controlling a model's personality is crucial for maintaining tone, consistency, and engagement. However, traditional prompt-based techniques for controlling personality often fall short, as they do not effectively mitigate the model's inherent biases. In this paper, we introduce a novel method PALETTE that enhances personality control through knowledge editing. By generating adjustment queries inspired by psychological assessments, our approach systematically adjusts responses to personality-related queries similar to modifying factual knowledge, thereby achieving controlled shifts in personality traits. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.", 'abstract_zh': '大语言模型（LLMs）在对话代理和内容创作等应用中发挥着重要作用，而控制模型的性格对于保持语气、一致性以及用户参与度至关重要。然而，传统基于提示的技术在控制性格方面往往不够有效，因为它们无法有效地缓解模型固有的偏见。在本文中，我们提出了一种新的方法——PALETTE，通过知识编辑增强性格控制。通过生成受心理评估启发的调整查询，我们的方法系统地调整与性格相关的查询的响应，类似于修改事实性知识，从而实现控制性个人特质的变化。来自自动评估和人工评估的实验结果表明，我们的方法能够使LLMs在性格控制方面表现出更强的稳定性和平衡性。', 'title_zh': '通过相关知识编辑对语言模型进行个性编辑'}
{'arxiv_id': 'arXiv:2502.11766', 'title': 'Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation', 'authors': 'Zengkui Sun, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.11766', 'abstract': "The widespread deployment of Large Language Models (LLMs) is hindered by the high computational demands, making knowledge distillation (KD) crucial for developing compact smaller ones. However, the conventional KD methods endure the distribution mismatch issue between the teacher and student models, leading to the poor performance of distillation. For instance, the widely-used KL-based methods suffer the mode-averaging and mode-collapsing problems, since the mismatched probabitliy distribution between both models. Previous studies mainly optimize this issue via different distance calculations towards the distribution of both models. Unfortunately, the distribution mismatch issue still exists in the early stage of the distillation. Hence, to reduce the impact of distribution mismatch, we propose a simple yet efficient method, named Warmup-Distill, which aligns the distillation of the student to that of the teacher in advance of distillation. Specifically, we first detect the distribution of the student model in practical scenarios with its internal knowledge, and then modify the knowledge with low probability via the teacher as the checker. Consequently, Warmup-Distill aligns the internal student's knowledge to that of the teacher, which expands the distribution of the student with the teacher's, and assists the student model to learn better in the subsequent distillation. Experiments on the seven benchmarks demonstrate that Warmup-Distill could provide a warmup student more suitable for distillation, which outperforms the vanilla student by as least +0.4 averaged score among all benchmarks. Noteably, with the assistance of Warmup-Distill, the distillation on the math task could yield a further improvement, at most +1.9% accuracy.", 'abstract_zh': '大规模语言模型（LLMs）的广泛部署受到高计算需求的限制，因此知识蒸馏（KD）对于开发更具紧凑性的较小模型至关重要。然而，传统的KD方法存在教师模型与学生模型之间的分布不匹配问题，导致蒸馏效果不佳。例如，广泛使用的基于KL的方法由于两个模型之间匹配不良的概率分布而出现模式平均化和模式塌陷问题。先前的研究主要通过不同的距离计算来优化这一问题，但分布不匹配问题仍然存在于蒸馏的早期阶段。因此，为了减少分布不匹配的影响，我们提出了一种简单而有效的暖启动蒸馏（Warmup-Distill）方法，该方法在蒸馏前将学生模型的蒸馏与教师模型的蒸馏对齐。具体而言，我们首先利用学生模型的内部知识检测其分布，并通过教师作为检查者修改低概率的知识。因此，Warmup-Distill将学生模型的内部知识与其教师模型的知识对齐，从而扩展了学生模型的分布，并协助其在后续的蒸馏中更好地学习。在七个基准上的实验表明，Warmup-Distill可以为蒸馏提供更适合的学生，相比于通用学生模型，所有基准的平均得分至少提高了+0.4。值得注意的是，在Warmup-Distill的辅助下，数学任务上的蒸馏进一步提高了最多1.9%的准确率。', 'title_zh': 'Warmup-Distill: 在知识蒸馏前桥接教师模型和学生模型之间的分布 mismatch'}
{'arxiv_id': 'arXiv:2502.11779', 'title': 'Efficient Response Generation Method Selection for Fine-Tuning Large Language Models', 'authors': 'Xuan Ren, Qi Chen, Lingqiao Liu', 'link': 'https://arxiv.org/abs/2502.11779', 'abstract': "The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of output variation used in training can affect the model's performance. This raises an important question: how can we generate the most effective output from the many possible response generation strategy options? Rather than relying on the traditional but resource-intensive train-and-evaluate approach, this paper proposes a scalable, approximate method for estimating the quality of a small subset of generated training data derived from the same input. We then evaluate how well this small subset of generated output fits the target model we are trying to train. We present a large-scale benchmark covering diverse reasoning-based datasets to support our study.\nThe central idea is that a good output should closely resemble the output generated by the target LLM. We formalize this 'closeness' as the expected alignment score between a candidate output and the output sampled from the target LLM. We connect this measurement to the perplexity metric used in previous literature and demonstrate that leveraging an alignment-based metric can provide better predictions of model performance. Using this strategy, we can evaluate a small subset of the generated output from each response generation strategy option, then select the most effective strategy. We show that an LLM trained on data generated by the selected strategy could lead to a significant performance gain in many cases.", 'abstract_zh': '用于微调大规模语言模型（LLM）的训练数据通常以输入-输出对的形式结构化。然而，对于许多任务，同一个输入可能有多个同样合理的输出变体。最近的研究观察到，在训练中使用的输出变体选择会影响模型的性能。这提出了一个重要的问题：如何从多种可能的生成输出策略中生成最有效的输出？本文不依赖传统的但资源密集的训练和评估方法，而是提出了一种可扩展的近似方法来估计从相同输入生成的小子集训练数据的质量。然后我们评估这个小型生成输出子集与我们试图训练的目标模型的匹配度。我们提出了一项大规模基准测试，涵盖了多种基于推理的数据集，以支持我们的研究。\n\n核心理念是一个好的输出应该与目标LLM生成的输出高度相似。我们将这种“相似度”形式化为候选输出与从目标LLM采样输出之间的预期对齐分数。我们将这一测量与以往文献中使用的手法困惑度指标相连，并展示了利用基于对齐的度量可以更好地预测模型性能。通过这种方法，我们可以评估每个生成输出策略的生成输出子集，然后选择最有效的策略。我们表明，使用所选策略生成的数据训练的LLM在许多情况下可以带来显著的性能提升。', 'title_zh': '大型语言模型微调中的高效响应生成方法选择方法'}
{'arxiv_id': 'arXiv:2502.11736', 'title': 'ReviewEval: An Evaluation Framework for AI-Generated Reviews', 'authors': 'Chavvi Kirtani, Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Murari Mandal, Dhruv Kumar', 'link': 'https://arxiv.org/abs/2502.11736', 'abstract': "The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.", 'abstract_zh': '学术研究的数量急剧增加，同时合格的评审者却严重短缺，这 necessitates 对同行评审过程采用创新方法。虽然大型语言模型（LLMs）在自动化这一过程方面具有潜力，但它们当前的局限性包括表面化的评论、虚构错误和缺乏可操作的见解。本研究针对这些挑战，提出了一种全面的评估框架，以评估AI生成的评论，该框架衡量与人类评价的一致性，验证事实准确性，评估分析深度，并识别可操作的见解。我们还提出了一种新的对齐机制，使生成的AI评论适应各个会议和期刊的独特评价优先级。为了提高这些评论的质量，我们引入了一个自我精炼循环，迭代优化LLM的评论提示。该框架建立了评估基于AI的评论系统的标准化指标，从而增强了AI生成的评论在学术研究中的可靠性。', 'title_zh': 'ReviewEval：一种评估生成式人工智能评论的框架'}
{'arxiv_id': 'arXiv:2502.11735', 'title': 'MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables', 'authors': 'Kwangwook Seo, Donguk Kwon, Dongha Lee', 'link': 'https://arxiv.org/abs/2502.11735', 'abstract': 'Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.', 'abstract_zh': '基于表格的推理最近在扩展，已从事实性问题回答（factoid-level QA）延伸到涉及隐含知识综合的洞察性任务。虽然现有研究取得了有效进展，但仍然局限于仅给定一个金标准表格和用户查询的场景，无法涵盖用户寻求源自多个未知表格的全面洞察的情况。为弥合这些差距，我们提出了MT-RAIG Bench，旨在评估系统的多表检索增强洞察生成能力。此外，为应对现有表格领域自动评估方法在子优性方面的不足，我们进一步引入了细粒度评估框架MT-RAIG Eval，该框架在生成的洞察方面与人类质量判断更好地对齐。我们进行了广泛的实验，并揭示了即使是前沿的大语言模型（LLM）在复杂多表推理方面仍然存在挑战，从而确立了MT-RAIG Bench作为未来研究的具有挑战性的测试平台。', 'title_zh': 'MT-RAIG：跨表检索增强洞察生成的新基准及评估框架'}
{'arxiv_id': 'arXiv:2502.11771', 'title': 'The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It', 'authors': 'Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi', 'link': 'https://arxiv.org/abs/2502.11771', 'abstract': "The ability of large language models (LLMs) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on $\\textit{consistency heads}$--attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.", 'abstract_zh': '大型语言模型（LLMs）验证其输出并识别潜在错误的能力对于确保其稳健性和可靠性至关重要。然而，当前的研究表明，LLMs 在自我纠正方面存在困难，遇到了显著的错误检测挑战。虽然有研究探讨了增强LLMs 自我纠正能力的方法，但在理解模型内部负责错误检测的机制方面关注较少。在此论文中，我们对LLMs 的错误检测机制进行了机制分析，重点关注简单的算术问题。通过电路分析，我们确定了四个较小规模的LLMs 中负责检测算术错误的计算子图。研究发现，所有模型都严重依赖于“一致性头”——评估算术解决方案中数值表面一致性的注意头。此外，我们观察到，模型内部的算术计算主要发生在较高层次，而验证则发生在中间层次，在最终的算术结果完全编码之前。这种算术计算与验证之间的结构分离似乎解释了当前LLMs 为何难以检测即使是简单的算术错误。', 'title_zh': '验证缺口：语言模型在计算算术而不验证其正确性方面的机制分析'}
{'arxiv_id': 'arXiv:2502.11733', 'title': 'Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking Practical Reasoning and Situation Modelling in a Text-Simulated Situated Environment', 'authors': 'Jonathan Jordan, Sherzod Hakimov, David Schlangen', 'link': 'https://arxiv.org/abs/2502.11733', 'abstract': "Large language models (LLMs) have risen to prominence as 'chatbots' for users to interact via natural language. However, their abilities to capture common-sense knowledge make them seem promising as language-based planners of situated or embodied action as well. We have implemented a simple text-based environment -- similar to others that have before been used for reinforcement-learning of agents -- that simulates, very abstractly, a household setting. We use this environment and the detailed error-tracking capabilities we implemented for targeted benchmarking of LLMs on the problem of practical reasoning: Going from goals and observations to actions. Our findings show that environmental complexity and game restrictions hamper performance, and concise action planning is demanding for current LLMs.", 'abstract_zh': '大型语言模型（LLMs）已成为能够通过自然语言与用户交互的“聊天机器人”。然而，它们捕捉常识知识的能力使其在语言基础上规划情境化或具身行为方面显得颇具前景。我们实现了一个简单的基于文本的环境——类似于之前用于代理强化学习的环境——该环境非常抽象地模拟了一个家庭设置。我们利用这一环境及其内置的详细错误跟踪能力，针对实际推理问题对LLMs进行目标测试：从目标和观察出发推导出行动。我们的研究发现，环境复杂性和游戏限制阻碍了性能提升，而简洁的行动计划对当前的LLMs来说是极具挑战性的。', 'title_zh': '抽屉里植株，桌上橙子，架上书籍：基于文本模拟情境环境的实用推理与情境建模基准测试'}
{'arxiv_id': 'arXiv:2502.11766', 'title': 'Warmup-Distill: Bridge the Distribution Mismatch between Teacher and Student before Knowledge Distillation', 'authors': 'Zengkui Sun, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.11766', 'abstract': "The widespread deployment of Large Language Models (LLMs) is hindered by the high computational demands, making knowledge distillation (KD) crucial for developing compact smaller ones. However, the conventional KD methods endure the distribution mismatch issue between the teacher and student models, leading to the poor performance of distillation. For instance, the widely-used KL-based methods suffer the mode-averaging and mode-collapsing problems, since the mismatched probabitliy distribution between both models. Previous studies mainly optimize this issue via different distance calculations towards the distribution of both models. Unfortunately, the distribution mismatch issue still exists in the early stage of the distillation. Hence, to reduce the impact of distribution mismatch, we propose a simple yet efficient method, named Warmup-Distill, which aligns the distillation of the student to that of the teacher in advance of distillation. Specifically, we first detect the distribution of the student model in practical scenarios with its internal knowledge, and then modify the knowledge with low probability via the teacher as the checker. Consequently, Warmup-Distill aligns the internal student's knowledge to that of the teacher, which expands the distribution of the student with the teacher's, and assists the student model to learn better in the subsequent distillation. Experiments on the seven benchmarks demonstrate that Warmup-Distill could provide a warmup student more suitable for distillation, which outperforms the vanilla student by as least +0.4 averaged score among all benchmarks. Noteably, with the assistance of Warmup-Distill, the distillation on the math task could yield a further improvement, at most +1.9% accuracy.", 'abstract_zh': '大规模语言模型（LLMs）的广泛应用受到高计算需求的阻碍，因此知识蒸馏（KD）对于开发紧凑的小模型至关重要。然而，传统的KD方法存在教师模型和学生模型之间的分布不匹配问题，导致蒸馏效果不佳。例如，广泛应用的基于KL的方法会遭受模态平均和模态坍缩的问题，因为两个模型之间概率分布的不匹配。以往的研究主要通过不同的距离计算优化这一问题，但分布不匹配的问题仍然在蒸馏的早期阶段存在。因此，为了减少分布不匹配的影响，我们提出了一种简单而高效的方法，名为Warmup-Distill，该方法在正式蒸馏之前对学生的蒸馏进行预对齐。具体而言，我们首先在实际场景中通过学生的内部知识检测学生的分布，并通过教师模型作为检查器修改低概率的知识。随后，Warmup-Distill 将学生的内部知识对齐到教师的分布，从而扩展了学生的分布，并在后续蒸馏中帮助学生模型更好地学习。在七个基准上的实验表明，Warmup-Distill 可以为蒸馏提供一个更适合的学生模型，在所有基准中平均得分提高了至少 +0.4。值得注意的是，在Warmup-Distill 的辅助下，数学任务上的蒸馏可以进一步提高，最高可达到 +1.9% 的准确率提升。', 'title_zh': 'Warmup-Distill: 在知识蒸馏之前缓解教师模型和学生模型之间的分布不匹配'}
{'arxiv_id': 'arXiv:2502.11736', 'title': 'ReviewEval: An Evaluation Framework for AI-Generated Reviews', 'authors': 'Chavvi Kirtani, Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Murari Mandal, Dhruv Kumar', 'link': 'https://arxiv.org/abs/2502.11736', 'abstract': "The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.", 'abstract_zh': '随着学术研究的日益增多以及合格审稿人的短缺，需要创新的方法来优化同行评审过程。虽然大型语言模型（LLMs）为这一过程自动化提供了潜在的可能性，但它们当前的局限性包括表面化的批评、错误的假设以及缺乏可操作的见解。本研究通过引入一个全面的评估框架来应对这些挑战，该框架衡量AI生成评审与人类评价的一致性、验证事实准确性、评估分析深度，并识别可操作的见解。我们还提出了一种新颖的一致性机制，使LLM生成的评审更适合每个会议和期刊的独特评价优先级。为了提升这些评审的质量，我们引入了一个自我完善循环，以迭代优化LLM的评审提示。该框架为评估基于AI的评审系统建立了标准化指标，从而增强了AI生成评审在学术研究中的可靠性。', 'title_zh': 'ReviewEval：一种针对AI生成评论的评估框架'}
{'arxiv_id': 'arXiv:2502.11718', 'title': '"See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models', 'authors': 'Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.11718', 'abstract': "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field.", 'abstract_zh': '大型视觉语言模型（LVLMs）的事实准确性评估落后于其快速的发展，这使得全面反映这些模型的知识容量和可靠性变得具有挑战性。本文介绍了首个中文事实准确性视觉问答基准——ChineseSimpleVQA，旨在评估LVLMs在8个主要主题和56个子主题下的视觉事实准确性。该基准的关键特性包括关注中文语言、多种知识类型、多步问题构建、高质量数据、静态一致性以及通过简短答案易于评估。此外，我们还贡献了一个严格的数据构建管道，并将视觉事实准确性分为两个部分：看到世界（即物体识别）和发现知识。这种分离允许我们分析LVLMs的能力边界和执行机制。随后，我们评估了34个先进的开源和闭源模型，揭示了该领域内存在的关键性能差距。', 'title_zh': '“环游世界，探索知识”：面向大规模视觉语言模型的中文事实性评价'}
{'arxiv_id': 'arXiv:2502.11707', 'title': 'Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models', 'authors': 'Sherzod Hakimov, Lara Pfennigschmidt, David Schlangen', 'link': 'https://arxiv.org/abs/2502.11707', 'abstract': 'This study utilizes the game Codenames as a benchmarking tool to evaluate large language models (LLMs) with respect to specific linguistic and cognitive skills. LLMs play each side of the game, where one side generates a clue word covering several target words and the other guesses those target words. We designed various experiments by controlling the choice of words (abstract vs. concrete words, ambiguous vs. monosemic) or the opponent (programmed to be faster or slower in revealing words). Recent commercial and open-weight models were compared side-by-side to find out factors affecting their performance. The evaluation reveals details about their strategies, challenging cases, and limitations of LLMs.', 'abstract_zh': '本研究利用桌游《Codenames》作为基准工具，评估大型语言模型（LLMs）在特定语言和认知技能方面的能力。在该游戏中，LLMs 分别扮演游戏中双方的角色，一方生成一条包含多个目标词的提示词，另一方则猜测这些目标词。我们通过控制词汇选择（抽象词与具体词，多义词与单义词）或对手（编程使其更快或更慢地揭示词汇）来设计了多种实验。将近年来的商业和开源模型进行并排比较，以找出影响它们性能的因素。评价揭示了LLMs 的策略细节、具有挑战性的案例以及其局限性。', 'title_zh': '将下面的论文标题翻译成中文，同时符合学术规范：\n\n《作为评估大型语言模型手段的游戏Codenames中的即兴概念形成》'}
{'arxiv_id': 'arXiv:2502.11735', 'title': 'MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables', 'authors': 'Kwangwook Seo, Donguk Kwon, Dongha Lee', 'link': 'https://arxiv.org/abs/2502.11735', 'abstract': 'Recent advancements in table-based reasoning have expanded beyond factoid-level QA to address insight-level tasks, where systems should synthesize implicit knowledge in the table to provide explainable analyses. Although effective, existing studies remain confined to scenarios where a single gold table is given alongside the user query, failing to address cases where users seek comprehensive insights from multiple unknown tables. To bridge these gaps, we propose MT-RAIG Bench, design to evaluate systems on Retrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to tackle the suboptimality of existing automatic evaluation methods in the table domain, we further introduce a fine-grained evaluation framework MT-RAIG Eval, which achieves better alignment with human quality judgments on the generated insights. We conduct extensive experiments and reveal that even frontier LLMs still struggle with complex multi-table reasoning, establishing our MT-RAIG Bench as a challenging testbed for future research.', 'abstract_zh': '基于表格的推理近期取得了进展，不仅扩展到了事实级的问答任务，还开始处理洞察级的任务，其中系统需要从表格中综合隐含的知识以提供可解释的分析。尽管现有研究在单个金标准表格和用户查询提供的场景中表现有效，但仍未解决用户寻求从多个未知表格中获得全面洞察的问题。为弥合这些差距，我们提出了一种名为MT-RAIG Bench的新基准，旨在评估系统在多表格增强的洞察生成方面的表现。为进一步解决现有表格领域自动评估方法的不足，我们还引入了细粒度的评估框架MT-RAIG Eval，该框架在生成的洞察方面更好地与人类质量判断对齐。我们进行了广泛的实验，结果显示即使是前沿的语言模型仍然难以应对复杂的多表格推理，从而确立了MT-RAIG Bench作为未来研究挑战性测试平台的地位。', 'title_zh': 'MT-RAIG：多表检索增强洞察生成的新型基准及评估框架'}
{'arxiv_id': 'arXiv:2502.11705', 'title': 'LLM Agents Making Agent Tools', 'authors': 'Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather', 'link': 'https://arxiv.org/abs/2502.11705', 'abstract': 'Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.', 'abstract_zh': '工具使用已将大型语言模型（LLMs）转变为能够通过动态利用外部软件组件执行复杂多步任务的强大代理。然而，这些工具必须由人类开发者在预先进行开发，这妨碍了LLM代理在需要大量高度专业化工具的领域中的应用，如生命科学和医学。鉴于科学研究与公共代码仓库一同增长的趋势，我们提出了ToolMaker，这是一种新颖的代理框架，能够自主将包含代码的论文转化为与LLM兼容的工具。给定简要的任务描述和代码仓库URL，ToolMaker能够自主安装所需依赖项并生成代码以执行任务，并采用闭环自校正机制逐次诊断并纠正错误。\n\n为了评估我们的方法，我们引入了一个基准测试集，包含15项涵盖医疗和非医疗领域的多样且复杂的计算任务，并包含超过100个单元测试，以客观评估工具的正确性和鲁棒性。ToolMaker成功实现80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker代表着完全自主代理驱动的科学工作流的一个进展。', 'title_zh': '“LLM代理创建代理工具”'}
{'arxiv_id': 'arXiv:2502.11733', 'title': 'Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking Practical Reasoning and Situation Modelling in a Text-Simulated Situated Environment', 'authors': 'Jonathan Jordan, Sherzod Hakimov, David Schlangen', 'link': 'https://arxiv.org/abs/2502.11733', 'abstract': "Large language models (LLMs) have risen to prominence as 'chatbots' for users to interact via natural language. However, their abilities to capture common-sense knowledge make them seem promising as language-based planners of situated or embodied action as well. We have implemented a simple text-based environment -- similar to others that have before been used for reinforcement-learning of agents -- that simulates, very abstractly, a household setting. We use this environment and the detailed error-tracking capabilities we implemented for targeted benchmarking of LLMs on the problem of practical reasoning: Going from goals and observations to actions. Our findings show that environmental complexity and game restrictions hamper performance, and concise action planning is demanding for current LLMs.", 'abstract_zh': '大型语言模型（LLMs）已成为用户通过自然语言进行交互的“聊天机器人”。然而，它们捕获常识知识的能力使它们在基于语言的计划者方面具有潜力，用于规划情境中的行动或有身体表现的行动。我们实现了一个简单的基于文本的环境——类似于之前用于代理强化学习的环境——该环境以非常抽象的方式模拟了家庭环境。我们利用此环境以及我们为具体基准测试LLMs在实际推理问题上的表现而实现的详细错误跟踪能力。我们的研究发现表明，环境的复杂性和游戏限制阻碍了性能，而简洁的行动规划对现有的LLM来说是具有挑战性的。', 'title_zh': '在柜子中的植物、桌子上的橙子、书架上的书：基于文本模拟的环境下的实用推理与情境建模基准测试'}
{'arxiv_id': 'arXiv:2502.11703', 'title': 'CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation', 'authors': 'Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan', 'link': 'https://arxiv.org/abs/2502.11703', 'abstract': 'Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repo this https URL.', 'abstract_zh': '医学质量控制指标是评估医疗机构医疗服务资格的重要手段。随着大型语言模型（LLMs）如GPT-4在医学领域的出色表现，利用这些技术进行医学质量控制指标计算（MQCIC）展现出了巨大的潜力。本研究中，我们（1）介绍了实际应用场景中的MQCIC任务，并提出了一款基于中文电子病历（EMRs）的开源数据集（CMQCIC-Bench），包含785个实例和76个指标。（2）我们提出了一种半自动方法来增强规则表示，随后提出了基于临床事实的推理规则（CF-IR）方法，该方法分离了临床事实验证和推理规则推理的操作。（3）我们在20种代表性语言模型上进行了全面实验，涵盖通用和医学模型。研究结果表明，CF-IR方法在MQCIC任务中优于链式思考方法。（4）我们进行了错误分析，探讨了临床事实验证和推理规则推理的能力，为提高MQCIC性能提供了见解。数据集和源代码可在以下链接获取：[此链接]。', 'title_zh': 'CMQCIC-Bench：用于评估大型语言模型在医疗质量控制指标计算中性能的中文基准stile\nuser\n能否提供一个英文解释，说明CMQCIC-Bench这个名字的具体含义？'}
{'arxiv_id': 'arXiv:2502.11718', 'title': '"See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models', 'authors': 'Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.11718', 'abstract': "The evaluation of factual accuracy in large vision language models (LVLMs) has lagged behind their rapid development, making it challenging to fully reflect these models' knowledge capacity and reliability. In this paper, we introduce the first factuality-based visual question-answering benchmark in Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of LVLMs across 8 major topics and 56 subtopics. The key features of this benchmark include a focus on the Chinese language, diverse knowledge types, a multi-hop question construction, high-quality data, static consistency, and easy-to-evaluate through short answers. Moreover, we contribute a rigorous data construction pipeline and decouple the visual factuality into two parts: seeing the world (i.e., object recognition) and discovering knowledge. This decoupling allows us to analyze the capability boundaries and execution mechanisms of LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source models, revealing critical performance gaps within this field.", 'abstract_zh': '在大型视觉语言模型（LVLMs）快速发展的同时，对其事实准确性的评估相对滞后，这使得全面反映这些模型的知识能力和可靠性变得困难。本文介绍了首个基于事实性的中文视觉问答基准，名为ChineseSimpleVQA，旨在评估LVLMs在8个主要主题和56个子主题下的视觉事实性。该基准的关键特征包括关注中文语言、多样化的知识类型、多层次问题构建、高质量数据、静态一致性以及通过简短答案易于评估。此外，我们贡献了一个严格的数据构建管道，并将视觉事实性分为两个部分：感知世界（即，对象识别）和发现知识。这种分离使我们能够分析LVLMs的能力边界和执行机制。随后，我们评估了34个先进的开源和闭源模型，揭示了该领域内的关键性能差距。', 'title_zh': '“游历世界，探索知识”：面向大型视觉语言模型的中文事实性评估'}
{'arxiv_id': 'arXiv:2502.11707', 'title': 'Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models', 'authors': 'Sherzod Hakimov, Lara Pfennigschmidt, David Schlangen', 'link': 'https://arxiv.org/abs/2502.11707', 'abstract': 'This study utilizes the game Codenames as a benchmarking tool to evaluate large language models (LLMs) with respect to specific linguistic and cognitive skills. LLMs play each side of the game, where one side generates a clue word covering several target words and the other guesses those target words. We designed various experiments by controlling the choice of words (abstract vs. concrete words, ambiguous vs. monosemic) or the opponent (programmed to be faster or slower in revealing words). Recent commercial and open-weight models were compared side-by-side to find out factors affecting their performance. The evaluation reveals details about their strategies, challenging cases, and limitations of LLMs.', 'abstract_zh': '本研究利用桌游《_codenames_》作为基准测试工具，评估大型语言模型（LLMs）在特定语言和认知技能方面的表现。LLMs在游戏的每一方进行操作，一方生成一个包含多个目标词的提示词，另一方则猜测这些目标词。我们通过控制词语的选择（抽象词 vs. 具体词，多义词 vs. 单义词）或对手（编程以更快或更慢地揭示词语）来设计各种实验。近期的商业和开源模型进行了直接比较，以找出影响它们性能的因素。评估结果揭示了LLMs的策略细节、具有挑战性的案例以及其局限性。', 'title_zh': '将以下论文内容或标题翻译成中文，并符合学术规范：\n\nAd-hoc Concept Forming in the Game Codenames as a Means for Evaluating Large Language Models\n\n即兴概念形成在游戏《猜词》中的应用：作为一种评估大规模语言模型的方法'}
{'arxiv_id': 'arXiv:2502.11689', 'title': 'Improve LLM-as-a-Judge Ability as a General Ability', 'authors': 'Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li', 'link': 'https://arxiv.org/abs/2502.11689', 'abstract': "LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values, ensuring ethical and reliable AI outputs that align with societal norms. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we regard judge ability as a general ability of LLM and implement a two-stage training approach, comprising supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO) enhancement, to achieve judge style adaptation and improve judgment accuracy. Additionally, we introduce an efficient data synthesis method to generate judgmental content. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task, and the judge signals provided by our model have significantly enhanced the downstream DPO training performance of our internal models in our test to optimize policy model with Judge Model. We also open-source our model weights and training data to facilitate further research.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\nLLM-as-a-Judge 利用大型语言模型（LLMs）的生成能力和推理能力，在多种场景下评估LLM的回应，提供准确的偏好信号。这种方法在使LLMs与人类价值观相一致方面发挥着关键作用，确保AI输出符合伦理和可靠的社会规范。最近的研究提出了许多训练LLM作为生成法官的方法，但大多数方法要么数据消耗量大，要么缺乏准确性，仅仅关注于LLM的法官能力。在本研究中，我们将法官能力视为LLMs的一般能力，并实现了一种两阶段训练方法，包括监督微调（SFT）预热和直接偏好优化（DPO）增强，以实现法官风格适应并提高判断准确性。此外，我们引入了一种高效的数据合成方法来生成评判内容。实验结果表明，与先前方法相比，我们的方法仅需大约2%至40%的数据即可在RewardBench上实现最优性能（SOTA）。此外，我们的训练方法通过构造复杂的法官任务增强了模型的通用能力，并且我们模型提供的法官信号已显著增强了我们在测试中用于优化政策模型时的内部模型的DPO训练性能。我们还开源了我们的模型权重和训练数据，以促进进一步的研究。', 'title_zh': '提升大语言模型作为通用评判者的能力'}
{'arxiv_id': 'arXiv:2502.11705', 'title': 'LLM Agents Making Agent Tools', 'authors': 'Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather', 'link': 'https://arxiv.org/abs/2502.11705', 'abstract': 'Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.', 'abstract_zh': '工具使用使大规模语言模型（LLMs）成为了能够通过动态利用外部软件组件执行复杂多步骤任务的强大代理。然而，这些工具必须由人类开发者事先实现，这在需要大量高度专业化工具的领域，如生命科学和医学中限制了LLM代理的应用。受到越来越多的研究文章伴随公共代码仓库的趋势启发，我们提出了ToolMaker，一种新的代理框架，能够自主将带有代码的论文转换为LLM兼容的工具。给定一个简短的任务描述和一个代码仓库地址，ToolMaker能够自主安装所需的依赖项并生成执行任务的代码，通过一个闭环自我纠正机制迭代诊断和纠正错误。为了评估我们的方法，我们引入了一个包含15个多样化且复杂的计算任务的基准，这些任务涵盖了医学和非医学领域，包含超过100个单元测试，用于客观评估工具的正确性和鲁棒性。ToolMaker正确实现了80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker是完全自主的基于代理的科学工作流的一个重要步骤。', 'title_zh': '大规模语言模型代理构建代理工具'}
{'arxiv_id': 'arXiv:2502.11688', 'title': 'From Isolates to Families: Using Neural Networks for Automated Language Affiliation', 'authors': 'Frederic Blum, Steffen Herbold, Johann-Mattis List', 'link': 'https://arxiv.org/abs/2502.11688', 'abstract': 'In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.', 'abstract_zh': '在历史语言学中，将语言归类到共同语系通常依赖于手工比较单个语言，采用一个复杂的工作流程。大规模标准化的多语言词汇列表和语法结构集合可能有助于改进这一过程，并为开发自动化语言分类工作流程开辟新的途径。在这里，我们介绍了适用于1000多种已知语系归属的全球样本的神经网络模型，使用词汇和语法数据来对个别语言进行分类，归入不同的语系。与大多数语言学家的传统假设一致，我们的结果显示，仅使用词汇数据训练的模型优于仅使用语法数据训练的模型，而结合两者的数据可以使性能进一步提高。在额外的实验中，我们展示了我们的模型如何识别整个语系子群之间的长期关系，如何用于研究孤立语的潜在亲属关系，以及如何帮助我们获得未分类语言可能语系归属的初步线索。我们得出结论，使用词汇和语法数据训练的自动化语言分类模型为比较语言学家提供了一个评估深层和未知语言关系假设的重要工具。', 'title_zh': '从孤立语言到语系家族：使用神经网络进行自动语言分类'}
{'arxiv_id': 'arXiv:2502.11703', 'title': 'CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation', 'authors': 'Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan', 'link': 'https://arxiv.org/abs/2502.11703', 'abstract': 'Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repo this https URL.', 'abstract_zh': '医学质量控制指标是评估医疗机构医疗服务质量的重要工具。随着大型语言模型（LLMs）如GPT-4在医学领域的卓越表现，利用这些技术进行医学质量控制指标计算（MQCIC）展现出了巨大的潜力。在本文中，（1）我们提出了一个实际任务——MQCIC，并提出了一套基于中文电子病历（EMRs）的开源数据集（CMQCIC-Bench），包含785个实例和76个指标。 （2）我们提出了一种半自动方法来增强规则表示。随后，我们提出了基于临床事实的推理规则（CF-IR）方法，该方法将临床事实验证和推理规则推理操作分离。 （3）我们在20个代表性的LLM上进行了全面实验，涵盖了通用和医学模型。我们的研究发现表明，CF-IR在MQCIC任务中优于思维链方法。 （4）我们进行了错误分析，探讨了临床事实验证和推理规则推理的能力，并提供了进一步提高MQCIC性能的见解。数据集和代码可在以下链接获取：https://your-repository-link-here.com。', 'title_zh': 'CMQCIC-Bench: 用于评估大型语言模型在医疗质量控制指标计算中的基准测试'}
{'arxiv_id': 'arXiv:2502.11684', 'title': 'MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task', 'authors': 'Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2502.11684', 'abstract': 'Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.', 'abstract_zh': '数学推理是推动大型语言模型（LLMs）发展的关键前沿领域。虽然逐步推理方法已成为LLMs中数学问题解决的主要范式，但训练数据中的推理步骤质量从根本上限制了这些模型的性能。近期研究表明，更加详细的中间步骤可以提升模型性能，然而现有的一些步骤扩展方法要么需要更强大的外部模型，要么会产生显著的计算成本。在本文中，我们提出了MathFimer，这是一种受代码完成任务中的“填空”任务启发的数学推理步骤扩展的新型框架。通过将解决方案链分解为前缀-后缀对，并训练模型以重建缺失的中间步骤，我们基于我们精心筛选的NuminaMath-FIM数据集开发了一种专门的模型——MathFimer-7B。然后，我们利用这些模型增强现有的数学推理数据集，通过在其解决方案链中插入详细步骤，生成MathFimer扩展版本。通过在包括MathInstruct、MetaMathQA等多个数学推理数据集中的全面实验，我们展示了基于MathFimer扩展数据训练的模型在诸如GSM8K和MATH等多种基准上的表现始终优于基于原始数据训练的模型。我们的方法提供了一种实用的、可扩展的解决方案，以提高LLMs的数学推理能力，而不需要依赖强大的外部模型或昂贵的推理过程。', 'title_zh': 'MathFimer：通过填充中间步骤任务扩展推理步骤以增强数学推理能力'}
{'arxiv_id': 'arXiv:2502.11681', 'title': 'RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars', 'authors': 'Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari', 'link': 'https://arxiv.org/abs/2502.11681', 'abstract': 'Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at this https URL.', 'abstract_zh': '确保大型语言模型（LLMs）行为符合伦理并提供帮助的关键在于校准调整。当前的校准方法需要高质量的标注和大量的训练资源。本文提出了一种低成本且无需调优的方法，利用上下文学习（ICL）来增强LLM的校准。通过对高质量的ICL示例的分析，我们发现风格是影响LLM校准能力的关键因素，并根据这一风格框架明确地重新塑造了ICL示例。此外，我们将重新塑造的示例结合起来，平衡了LLM校准的两个相互矛盾方面——事实准确性和安全性。我们将重新塑造的示例包装成提示以触发少样本学习，从而改善LLM的校准。与最佳基线方法相比，该方法在Alpaca任务中最高可提高0.10分（从4.50提高到4.60），在Just-eval基准上提高了0.22分（从4.34提高到4.56），在MT-Bench数据集上最高提高了0.32分（从3.53提高到3.85）。我们已将代码和数据发布在该链接所指向的位置。', 'title_zh': 'RIDE：通过重作风格的上下文内示例增强大型语言模型的对齐性'}
{'arxiv_id': 'arXiv:2502.11689', 'title': 'Improve LLM-as-a-Judge Ability as a General Ability', 'authors': 'Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li', 'link': 'https://arxiv.org/abs/2502.11689', 'abstract': "LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values, ensuring ethical and reliable AI outputs that align with societal norms. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we regard judge ability as a general ability of LLM and implement a two-stage training approach, comprising supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO) enhancement, to achieve judge style adaptation and improve judgment accuracy. Additionally, we introduce an efficient data synthesis method to generate judgmental content. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task, and the judge signals provided by our model have significantly enhanced the downstream DPO training performance of our internal models in our test to optimize policy model with Judge Model. We also open-source our model weights and training data to facilitate further research.", 'abstract_zh': 'LLM-as-a-Judge 利用了大语言模型（LLMs）的生成和推理能力，评估各种场景下的LLM响应，提供准确的偏好信号。这种方法在使LLMs与人类价值观保持一致方面发挥着关键作用，确保生成符合社会规范的伦理可靠AI输出。最近的研究提出了许多训练LLM作为生成性法官的方法，但大多数方法都消耗大量数据且缺乏准确性，并且仅专注于LLM的法官能力。在本研究中，我们将法官能力视为LLM的普遍能力，并采用两阶段训练方法，包括监督微调（SFT）预热和直接偏好优化（DPO）增强，以实现法官风格适应并提高判断准确性。此外，我们介绍了一种高效的数据合成方法，用于生成判断内容。实验结果表明，我们的方法仅需其他方法所需数据的大约2%到40%，即可在RewardBench上达到SOTA性能。此外，我们的训练方法通过构建复杂的法官任务来增强模型的通用能力，并且我们模型提供的法官信号显著提高了我们的内部模型在使用法官模型优化策略模型时的DPO训练性能。我们也开源了我们的模型权重和训练数据，以促进进一步的研究。', 'title_zh': '提升作为通用能力的LLM法官能力'}
{'arxiv_id': 'arXiv:2502.11688', 'title': 'From Isolates to Families: Using Neural Networks for Automated Language Affiliation', 'authors': 'Frederic Blum, Steffen Herbold, Johann-Mattis List', 'link': 'https://arxiv.org/abs/2502.11688', 'abstract': 'In historical linguistics, the affiliation of languages to a common language family is traditionally carried out using a complex workflow that relies on manually comparing individual languages. Large-scale standardized collections of multilingual wordlists and grammatical language structures might help to improve this and open new avenues for developing automated language affiliation workflows. Here, we present neural network models that use lexical and grammatical data from a worldwide sample of more than 1,000 languages with known affiliations to classify individual languages into families. In line with the traditional assumption of most linguists, our results show that models trained on lexical data alone outperform models solely based on grammatical data, whereas combining both types of data yields even better performance. In additional experiments, we show how our models can identify long-ranging relations between entire subgroups, how they can be employed to investigate potential relatives of linguistic isolates, and how they can help us to obtain first hints on the affiliation of so far unaffiliated languages. We conclude that models for automated language affiliation trained on lexical and grammatical data provide comparative linguists with a valuable tool for evaluating hypotheses about deep and unknown language relations.', 'abstract_zh': '在历史语言学中，将语言归属于同一语系的传统方法依赖于人工对比单个语言，这一过程相当复杂。大规模的标准化多语言词汇表和语法结构集合可能有助于改进这一方法，并为开发自动化的语言归属流程开辟新途径。在这里，我们提出了一种使用神经网络模型的方法，该模型利用了来自全球超过1,000种已知语系归属语言的词汇和语法数据，用于将单个语言分类到多个语系中。与大多数语言学家的传统假设一致，我们的结果显示，仅使用词汇数据训练的模型优于仅基于语法数据的模型，而结合这两种类型的数据则能获得更好的性能。在额外的实验中，我们展示了如何利用我们的模型识别整个子群之间的长期关系，如何将其应用于研究孤立语的潜在亲属语言，以及如何帮助我们获得尚未归属于任何语系的语言初步归属线索。我们得出结论，基于词汇和语法数据训练的自动语言归属模型为比较语言学家提供了一个有价值的工具，可用于评估关于深层和未知语言关系的假设。', 'title_zh': '从孤立语言到语言家族：使用神经网络进行自动化语言归类'}
{'arxiv_id': 'arXiv:2502.11677', 'title': 'Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception', 'authors': 'Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11677', 'abstract': "Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs' internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Consistency-based Confidence Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs' ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6\\% on NQ and 4.9\\% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.", 'abstract_zh': '大语言模型（LLMs）在各种任务中表现出色，但在准确判断知识边界方面常存在困难，导致其产生充满自信但错误的回答。本文探讨了利用LLMs的内部状态从效率和风险角度增强其对知识边界的感知。我们研究了是否可以在响应生成之前利用内部状态估计LLMs的置信度，从而节省计算资源。我们在Natural Questions、HotpotQA和MMLU等数据集上的实验表明，LLMs在响应生成前具备显著的预生成感知能力，而这种感知能力在生成后进一步细化，在不同条件下保持稳定。为在关键领域减轻风险，我们引入了基于一致性置信校准（Consistency-based Confidence Calibration，简称C³），通过问题重述评估置信度的一致性。C³显著提高了LLMs识别知识缺口的能力，在NQ数据集上提高了5.6%的未知感知率，在HotpotQA数据集上提高了4.9%的未知感知率。我们的研究结果表明，预生成置信度估计可以优化效率，而C³能够有效控制输出风险，从而提高LLMs在实际应用中的可靠性。', 'title_zh': '面向充分挖掘大规模语言模型内部状态以增强知识边界感知'}
{'arxiv_id': 'arXiv:2502.11684', 'title': 'MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task', 'authors': 'Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2502.11684', 'abstract': 'Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.', 'abstract_zh': '数学推理是推动大规模语言模型（LLMs）发展的关键前沿领域。虽然逐步的方法已成为LLMs中数学问题解决的主要范式，但训练数据中推理步骤的质量从根本上限制了模型的表现。最近的研究表明，更详细的中间步骤可以提升模型性能，然而目前存在的步骤扩展方法要么需要更强大的外部模型，要么会产生大量的计算成本。在本文中，我们引入了MathFimer，这是一种受代码完成任务中的“填充中间部分”任务启发的数学推理步骤扩展新型框架。我们通过将解题链分解为前缀-后缀对，并训练模型重建缺失的中间步骤，开发了专门针对我们精心筛选的NuminaMath-FIM数据集的MathFimer-7B模型。然后，我们应用这些模型通过在现有的数学推理数据集的解题链中插入详细的中间步骤，生成了MathFimer扩展版本。通过在包括MathInstruct、MetaMathQA等在内的多个数学推理数据集上的全面实验，我们证明，基于MathFimer扩展数据训练的模型在GS8K和MATH等各个基准测试中的一致表现优于基于原始数据训练的模型。我们的方法提供了一种无需依赖强大外部模型或昂贵推断过程的实用且可扩展的方法，从而提升LLMs的数学推理能力。', 'title_zh': 'MathFimer：通过填空任务扩展推理步骤以增强数学推理能力'}
{'arxiv_id': 'arXiv:2502.11671', 'title': 'Diversity-Oriented Data Augmentation with Large Language Models', 'authors': 'Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou', 'link': 'https://arxiv.org/abs/2502.11671', 'abstract': "Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \\textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data \\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). % \\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more than three percentage points.", 'abstract_zh': '数据增强是自然语言处理（NLP）中的一个重要技术，通过生成多样化的样本来丰富训练数据集。这一过程对于提高NLP模型的鲁棒性和泛化能力至关重要。然而，仍然存在一个重大挑战：**忽视样本分布的多样性**。大多数现有方法侧重于增加样本的数量，而忽视了样本分布的多样性，这可能导致模型过拟合。针对这一问题，我们探讨了数据增强对数据集多样性的影响，并提出了一种**D多样性-Oriented数据增强**框架（DoAug）。% \\(\\mathscr{DoAug}\\) 具体而言，我们利用一种导向多样性的微调方法来训练一个大语言模型（LLM），使其能够作为多样化的同义变换器，通过生成多样化的同义表达来增强文本数据集。然后，我们将LLM同义变换器应用于一组高度信息性的样本文本集中，并将生成的同义表达与原始数据集成，以创建一个更具多样性的增强数据集。最后，我们在12个真实世界的文本数据集上进行了大量的实验。结果显示，我们微调的LLM增强器在保持标签一致性的同时提升了多样性，从而增强了下游任务的鲁棒性和性能。具体而言，它在平均性能上提高了10.52%，并且超过了亚军基线，超过幅度超过3个百分点。', 'title_zh': '面向多样性的数据增强方法：利用大规模语言模型'}
{'arxiv_id': 'arXiv:2502.11681', 'title': 'RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars', 'authors': 'Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari', 'link': 'https://arxiv.org/abs/2502.11681', 'abstract': 'Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at this https URL.', 'abstract_zh': '确保大型语言模型（LLMs）在行为上符合伦理和提供帮助的关键在于对其进行对齐调整。当前的对齐方法需要高质量的标注和大量的训练资源。本文提出了一种低成本且无需调整的方法，利用上下文学习（ICL）来增强LLM的对齐能力。通过对高质量ICL示范案例的分析，我们发现风格是影响LLM对齐能力的关键因素，并根据这一风格框架明确重塑了ICL范例。此外，我们结合重塑的示范案例，以平衡LLM对齐的两个相互冲突方面——事实性和安全性。我们将重塑后的示例打包为提示，以激发少量样本学习，从而提高LLM的对齐能力。与最好的基线方法相比，该方法在Alpaca任务上的得分从4.50提高到了4.60（增加了0.10），在Just-eval基准上的得分从4.34提高到了4.56（提高了0.22），在MT-Bench数据集上的最佳改进为0.32（从3.53提高到了3.85）。我们在此网址发布代码和数据：https://www.example.com。', 'title_zh': 'RIDE：通过重风格的上下文示例演示增强大规模语言模型对齐\n\n注：此翻译保持了原文的缩写“RIDE”不变，并将“Restyled In-Context Learning Demonstration Exemplars”翻译为“通过重风格的上下文示例演示增强大规模语言模型对齐”，以符合学术规范和流畅性。'}
{'arxiv_id': 'arXiv:2502.11677', 'title': 'Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception', 'authors': 'Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11677', 'abstract': "Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs' internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Consistency-based Confidence Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs' ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6\\% on NQ and 4.9\\% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.", 'abstract_zh': '大型语言模型（LLMs）在多种任务中展现出卓越的性能，但往往难以准确判断其知识边界，从而产生自信但错误的回应。本文探讨了利用LLMs内部状态来从效率和风险角度增强其对知识边界的感知能力。我们研究了LLMs是否可以在生成响应之前利用内部状态来估算其置信度，从而节省计算资源。我们在Natural Questions、HotpotQA和MMLU等数据集上的实验表明，LLMs在生成前展示出显著的预生成感知能力，生成后进一步细化，感知差距在不同条件下保持稳定。为了在关键领域缓解风险，我们引入了一致性基置信校准（$C^3$），通过问题重述评估置信一致性。$C^3$显著提高了LLMs识别知识缺口的能力，在Natural Questions和HotpotQA数据集上分别提升了知识未知感知率5.6%和4.9%。我们的研究结果表明，预生成置信度估计可以优化效率，而$C^3$有效控制输出风险，有助于提高LLMs在实际应用中的可靠性。', 'title_zh': '充分利用大语言模型内部状态以增强知识边界感知'}
{'arxiv_id': 'arXiv:2502.11671', 'title': 'Diversity-Oriented Data Augmentation with Large Language Models', 'authors': 'Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou', 'link': 'https://arxiv.org/abs/2502.11671', 'abstract': "Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \\textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data \\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). % \\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more than three percentage points.", 'abstract_zh': '数据增强是自然语言处理（NLP）中一种重要的技术，通过生成多样化的样本来丰富训练数据集。这一过程对于提高NLP模型的鲁棒性和泛化能力至关重要。然而，仍存在一个重大挑战：**忽略样本分布的多样性**。大多数现有方法集中在增加样本数量上，而忽视了样本分布的多样性，这可能导致模型过拟合。针对这一问题，我们探索了数据增强对数据集多样性的影响，并提出了一种**Diversify-Oriented 数据增强框架（DoAug）**。具体而言，我们采用一种以多样性为导向的微调方法，训练一个语言模型作为多样化的同义表达生成器，该生成器能够通过生成多样化的同义句来增强文本数据集。然后，我们将语言模型的同义表达生成器应用于一组高度信息性的样本子集，并将生成的同义句与原始数据集成，从而创建一个更加多样化的增强数据集。最后，我们在12个真实的文本数据集上进行了广泛的实验。结果表明，我们的微调语言模型增强器在保持标签一致性的前提下提高了数据的多样性，从而增强了下游任务的鲁棒性和性能。具体而言，它实现了平均 \\(10.52\\%\\) 的性能提升，比亚军基准高出超过三个百分点。', 'title_zh': '面向多样性的数据增强方法与大规模语言模型'}
{'arxiv_id': 'arXiv:2502.11656', 'title': 'Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL', 'authors': 'Hanbing Liu, Haoyang Li, Xiaokang Zhang, Ruotong Chen, Haiyong Xu, Tian Tian, Qi Qi, Jing Zhang', 'link': 'https://arxiv.org/abs/2502.11656', 'abstract': "Direct Preference Optimization (DPO) has proven effective in complex reasoning tasks like math word problems and code generation. However, when applied to Text-to-SQL datasets, it often fails to improve performance and can even degrade it. Our investigation reveals the root cause: unlike math and code tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO, Text-to-SQL datasets typically include only final answers (gold SQL queries) without detailed CoT solutions. By augmenting Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, consistent and significant performance improvements using DPO. Our analysis shows that CoT reasoning is crucial for unlocking DPO's potential, as it mitigates reward hacking, strengthens discriminative capabilities, and improves scalability. These findings offer valuable insights for building more robust Text-to-SQL models. To support further research, we publicly release the code and CoT-enhanced datasets.", 'abstract_zh': '直接偏好优化（DPO）已经在数学文字问题和代码生成等复杂推理任务中证明了其有效性。然而，当应用于Text-to-SQL数据集时，它往往不能改善性能，甚至有时还会降低性能。我们的调查揭示了其根本原因：与数学和代码任务不同，数学和代码任务自然地将Chain-of-Thought（CoT）推理与DPO结合在一起，而Text-to-SQL数据集通常只包含最终答案（黄金SQL查询），而没有详细的CoT解决方案。通过增强Text-to-SQL数据集，使其包含合成的CoT解决方案，我们首次实现了在使用DPO时的一致性和显著性能提升。我们的分析表明，CoT推理对于释放DPO潜力至关重要，它能减轻奖励作弊、增强区分能力，并提高可扩展性。这些发现为构建更 robust的Text-to-SQL模型提供了宝贵见解。为了支持进一步研究，我们已公开发布了相关的代码和CoT增强的数据集。', 'title_zh': '发现链式思维推理对直接偏好优化的影响：从文本到SQL的启示'}
{'arxiv_id': 'arXiv:2502.11656', 'title': 'Uncovering the Impact of Chain-of-Thought Reasoning for Direct Preference Optimization: Lessons from Text-to-SQL', 'authors': 'Hanbing Liu, Haoyang Li, Xiaokang Zhang, Ruotong Chen, Haiyong Xu, Tian Tian, Qi Qi, Jing Zhang', 'link': 'https://arxiv.org/abs/2502.11656', 'abstract': "Direct Preference Optimization (DPO) has proven effective in complex reasoning tasks like math word problems and code generation. However, when applied to Text-to-SQL datasets, it often fails to improve performance and can even degrade it. Our investigation reveals the root cause: unlike math and code tasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO, Text-to-SQL datasets typically include only final answers (gold SQL queries) without detailed CoT solutions. By augmenting Text-to-SQL datasets with synthetic CoT solutions, we achieve, for the first time, consistent and significant performance improvements using DPO. Our analysis shows that CoT reasoning is crucial for unlocking DPO's potential, as it mitigates reward hacking, strengthens discriminative capabilities, and improves scalability. These findings offer valuable insights for building more robust Text-to-SQL models. To support further research, we publicly release the code and CoT-enhanced datasets.", 'abstract_zh': '直接偏好优化（DPO）在处理复杂的推理任务，如数学文字问题和代码生成时已被证明是有效的。然而，当将其应用于文本到SQL（Text-to-SQL）数据集时，它往往无法提高性能，甚至有可能降低性能。我们的调查揭示了根本原因：与数学和代码任务不同，后者自然地将链式推理（CoT）与DPO结合在一起，Text-to-SQL数据集通常仅包含最终答案（金框SQL查询），而缺乏详细的CoT解决方案。通过为Text-to-SQL数据集添加合成的CoT解决方案，我们第一次实现了使用DPO时一致且显著的性能改进。我们的分析表明，CoT推理对于激发DPO潜力至关重要，因为它可以缓解奖励作弊，增强辨别能力，并提高可扩展性。这些发现为构建更加稳健的Text-to-SQL模型提供了宝贵见解。为了支持进一步的研究，我们公开发布了代码和CoT增强的数据集。', 'title_zh': '探索链式思考推理对直接偏好优化的影响：以文本到SQL为例'}
{'arxiv_id': 'arXiv:2502.11633', 'title': 'CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training Efficiency', 'authors': 'Hongyan Wu, Peijian Zeng, Weixiong Zheng, Lianxi Wang, Nankai Lin, Shengyi Jiang, Aimin Yang', 'link': 'https://arxiv.org/abs/2502.11633', 'abstract': 'Cross-modal text-molecule retrieval task bridges molecule structures and natural language descriptions. Existing methods predominantly focus on aligning text modality and molecule modality, yet they overlook adaptively adjusting the learning states at different training stages and enhancing training efficiency. To tackle these challenges, this paper proposes a Curriculum Learning-bAsed croSS-modal text-molecule training framework (CLASS), which can be integrated with any backbone to yield promising performance improvement. Specifically, we quantify the sample difficulty considering both text modality and molecule modality, and design a sample scheduler to introduce training samples via an easy-to-difficult paradigm as the training advances, remarkably reducing the scale of training samples at the early stage of training and improving training efficiency. Moreover, we introduce adaptive intensity learning to increase the training intensity as the training progresses, which adaptively controls the learning intensity across all curriculum stages. Experimental results on the ChEBI-20 dataset demonstrate that our proposed method gains superior performance, simultaneously achieving prominent time savings.', 'abstract_zh': '跨模态文本-分子检索任务将分子结构与自然语言描述联系起来。现有方法主要集中在文本模态和分子模态之间的对齐，却忽视了在不同训练阶段适配性调整学习状态和提高训练效率的问题。为应对这些挑战，本文提出了一种基于课程学习的跨模态文本-分子训练框架（CLASS），该框架可以与任一种基础模型结合，从而显著提高性能。具体来说，我们综合考虑文本模态和分子模态的样本难度，并设计了一个样本调度器，通过从易到难的训练范式逐步引入训练样本，有效减少了早期训练阶段的样本规模，提高了训练效率。此外，我们引入了自适应强度学习，随着训练的推进增加训练强度，从而在所有课程学习阶段自适应地控制学习强度。在ChEBI-20数据集上的实验结果表明，我们提出的算法不仅获得了优越的性能，还实现了显著的时间节省。', 'title_zh': 'CLASS: 提升跨模态文本-分子检索性能及训练效率'}
{'arxiv_id': 'arXiv:2502.11614', 'title': 'Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI', 'authors': 'Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov', 'link': 'https://arxiv.org/abs/2502.11614', 'abstract': 'Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.', 'abstract_zh': '先前的研究表明，区分大型语言模型（LLM）生成的文本和人类撰写的文本具有极大的挑战性，通常还不如随机猜测。为了验证这一发现是否适用于不同的语言和领域，我们进行了一个全面的案例研究，以确定人类检测准确性的上限。在涵盖9种语言和9个领域、共16个数据集的测试中，19名注释者平均检测准确率为87.6%，从而挑战了之前的研究结论。我们发现，人类和机器文本之间的主要差距在于具体性、文化细微差别和多样性。通过显式解释提示之间的区别，可以在超过50%的情况下部分缩小这些差距。然而，我们还发现，人类并不总是偏好人类撰写的文本，特别是在他们无法清晰识别来源时。', 'title_zh': '人类喜不喜人类风格的文本？多语言人类鉴别与对AI的偏好对比'}
{'arxiv_id': 'arXiv:2502.11633', 'title': 'CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training Efficiency', 'authors': 'Hongyan Wu, Peijian Zeng, Weixiong Zheng, Lianxi Wang, Nankai Lin, Shengyi Jiang, Aimin Yang', 'link': 'https://arxiv.org/abs/2502.11633', 'abstract': 'Cross-modal text-molecule retrieval task bridges molecule structures and natural language descriptions. Existing methods predominantly focus on aligning text modality and molecule modality, yet they overlook adaptively adjusting the learning states at different training stages and enhancing training efficiency. To tackle these challenges, this paper proposes a Curriculum Learning-bAsed croSS-modal text-molecule training framework (CLASS), which can be integrated with any backbone to yield promising performance improvement. Specifically, we quantify the sample difficulty considering both text modality and molecule modality, and design a sample scheduler to introduce training samples via an easy-to-difficult paradigm as the training advances, remarkably reducing the scale of training samples at the early stage of training and improving training efficiency. Moreover, we introduce adaptive intensity learning to increase the training intensity as the training progresses, which adaptively controls the learning intensity across all curriculum stages. Experimental results on the ChEBI-20 dataset demonstrate that our proposed method gains superior performance, simultaneously achieving prominent time savings.', 'abstract_zh': '跨模态文本-分子检索任务连接了分子结构和自然语言描述。现有方法主要关注于对齐文本模态和分子模态，但忽略了在不同训练阶段自适应调整学习状态和提高训练效率的需求。为应对这些挑战，本文提出了一种基于 Curriculum Learning 的跨模态文本-分子训练框架（CLASS），该框架可以与任何主干网络结合，从而显著提升性能。具体而言，我们综合考虑文本模态和分子模态的样本难度，并设计了一个样本调度器，通过易到难的训练范式逐步引入训练样本，显著减少训练初期的样本规模，提高训练效率。此外，我们引入了自适应强度学习，在训练过程中逐步增加训练强度，从而在整个Curriculum Learning阶段自适应控制学习强度。实验结果表明，提出的方法在ChEBI-20数据集上取得了优异性能，同时实现了显著的时间节省。', 'title_zh': 'CLASS: 提升跨模态文本-分子检索性能和训练效率'}
{'arxiv_id': 'arXiv:2502.11611', 'title': 'Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks', 'authors': 'Fatemeh Mohammadi, Marta Annamaria Tamborini, Paolo Ceravolo, Costanza Nardocci, Samira Maghool', 'link': 'https://arxiv.org/abs/2502.11611', 'abstract': "This paper is a collaborative effort between Linguistics, Law, and Computer Science to evaluate stereotypes and biases in automated translation systems. We advocate gender-neutral translation as a means to promote gender inclusion and improve the objectivity of machine translation. Our approach focuses on identifying gender bias in English-to-Italian translations. First, we define gender bias following human rights law and linguistics literature. Then we proceed by identifying gender-specific terms such as she/lei and he/lui as key elements. We then evaluate the cosine similarity between these target terms and others in the dataset to reveal the model's perception of semantic relations. Using numerical features, we effectively evaluate the intensity and direction of the bias. Our findings provide tangible insights for developing and training gender-neutral translation algorithms.", 'abstract_zh': '本文是语言学、法律和计算机科学的跨学科合作研究，旨在评估自动化翻译系统中的刻板印象和偏见。我们倡导中性化翻译作为促进性别包容性和提高机器翻译客观性的手段。我们的方法主要集中在英译意翻译中的性别偏见识别上。首先，我们依据人权法和语言学文献定义性别偏见。然后，通过识别女性特指的“她/lei”和男性特指的“他/lui”等词汇作为关键元素。接着，我们计算这些目标词与数据集中其他词的余弦相似度，以揭示模型对语义关系的认知。利用数值特征，我们有效地评估了偏见的强度和方向。我们的研究结果为研发和训练中性化翻译算法提供了具体指导。', 'title_zh': '使用相似性网络识别从英语到意大利语的自动翻译中的性别刻板印象和偏见'}
{'arxiv_id': 'arXiv:2502.11614', 'title': 'Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI', 'authors': 'Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov', 'link': 'https://arxiv.org/abs/2502.11614', 'abstract': 'Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.', 'abstract_zh': '先前的研究表明，区分大型语言模型生成的文本与人工撰写的文本极具挑战性，往往几乎等同于随机猜测。为了验证这一发现的普适性，即这种难题是否跨越了不同的语言和领域，我们进行了一项广泛的研究案例分析，以确定人类检测准确性的一个上限。通过跨越16个涵盖9种语言和9个领域的数据集，19名标注者实现了87.6%的平均检测准确率，从而挑战了以往的结论。研究发现，人类与机器生成文本之间的主要差距在于具体性、文化nuance以及多样性。通过在提示中明确解释提示间的区别，可以在超过50%的情况下部分缩小这些差距。然而，我们还发现，人类并不总是偏好人工撰写的文本，尤其是在他们无法明确识别其来源时。', 'title_zh': '人类喜欢类似人类的文字吗？多语言人类检测与对AI文本的偏好比较'}
{'arxiv_id': 'arXiv:2502.11603', 'title': 'DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning', 'authors': 'Hongye Qiu, Yue Xu, Meikang Qiu, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11603', 'abstract': 'Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose this http URL (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. this http URL selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. this http URL can generalize to vision-language models (VLMs), achieving significant bias reduction.', 'abstract_zh': '大型语言模型（LLMs）展示了强大的自然语言处理能力，但也继承和放大了社会偏见，包括性别偏见，从而引发了公平性方面的关注。现有去偏方法面临诸多挑战：参数调优需要访问模型权重，基于提示的方法通常会降低模型的实用性，基于优化的方法缺乏普适性。为了应对这些挑战，我们提出了一种名为“this http URL（性别意识提示的演示与推理）”的自动化且模型无关的方法，该方法能够在减轻性别偏见的同时保持模型性能。该方法选择了揭示偏见的实例，并生成结构化的推理指导模型生成更加中立的响应。通过对多个LLM（GPT-3.5、Llama3和Llama2-Alpaca）进行核心参照解析和问答任务的大量实验，证明了其有效性、普适能力和鲁棒性。此外，该方法还可以推广到视觉语言模型（VLMs），实现了显著的偏见减少。', 'title_zh': 'DR.GAP：通过具有示范和推理的性别意识提示减轻大型语言模型中的偏差'}
{'arxiv_id': 'arXiv:2502.11598', 'title': 'Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?', 'authors': 'Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.11598', 'abstract': 'The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLM）水印的放射性特性使其能够在学生模型通过训练水标教师模型的输出时检测到继承的水印，从而使其成为防止未经授权的知识蒸馏的有前景工具。然而，水标放射性对抗敌对手的能力仍然鲜有研究。在本文中，我们探讨了学生模型在避免继承水印的同时是否可以通过知识蒸馏获取教师模型的能力。我们提出了两类水标记除方法：通过未目标和目标训练数据改写（UP和TP）在知识蒸馏前去除水印，以及通过推断时水标记中和（WN）在知识蒸馏后去除水印。在多个模型对、水印方案和超参数设置的广泛实验中，我们的结果表明，TP和WN都能够完全消除继承的水印，且WN能够在保持高效的知识迁移能力和低计算开销的同时实现这一目标。鉴于水标记技术在生产中型语言模型中的持续部署，这些发现强调了开发更 robust 防御策略的急迫需求。我们的代码可在以下链接获取：[此处替换为链接]。', 'title_zh': '大规模语言模型水印能否可靠地防止未经授权的知识蒸馏？'}
{'arxiv_id': 'arXiv:2502.11611', 'title': 'Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks', 'authors': 'Fatemeh Mohammadi, Marta Annamaria Tamborini, Paolo Ceravolo, Costanza Nardocci, Samira Maghool', 'link': 'https://arxiv.org/abs/2502.11611', 'abstract': "This paper is a collaborative effort between Linguistics, Law, and Computer Science to evaluate stereotypes and biases in automated translation systems. We advocate gender-neutral translation as a means to promote gender inclusion and improve the objectivity of machine translation. Our approach focuses on identifying gender bias in English-to-Italian translations. First, we define gender bias following human rights law and linguistics literature. Then we proceed by identifying gender-specific terms such as she/lei and he/lui as key elements. We then evaluate the cosine similarity between these target terms and others in the dataset to reveal the model's perception of semantic relations. Using numerical features, we effectively evaluate the intensity and direction of the bias. Our findings provide tangible insights for developing and training gender-neutral translation algorithms.", 'abstract_zh': '本文是语言学、法律学和计算机科学跨学科合作的研究成果，旨在评估自动化翻译系统中的刻板印象和偏见。我们倡导性别中立的翻译，以促进性别包容并提高机器翻译的客观性。我们的方法侧重于识别英译意（英到意）翻译中的性别偏见。首先，根据人权法和语言学文献定义性别偏见。然后，通过识别女性专属词汇如“she/lei”和男性专属词汇如“he/lui”作为关键元素。接着，我们通过评估目标词汇与其他数据集词汇之间的余弦相似度，揭示模型对语义关系的感知。利用数值特征，我们有效地评估了偏见的强度和方向。我们的研究发现为开发和训练性别中立的翻译算法提供了切实可行的见解。', 'title_zh': '将下面的论文内容或标题翻译成中文（符合学术规范）：\n\nIdentifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks\n\n在使用相似网络识别从英语到意大利语的自动化翻译中性别刻板印象和偏见的检测'}
{'arxiv_id': 'arXiv:2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'authors': 'Birger Moell, Johan Boye', 'link': 'https://arxiv.org/abs/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'abstract_zh': '大型语言模型（LLMs）在自然语言生成方面取得了显著进展，但在要求精确计算和结构分析的任务中往往面临挑战。本文通过计算LIX可读性度量和平均从属距离（ADD）来研究最先进LLMs在语言复杂性度量任务上的表现。利用瑞典中学和大学水平的作文，我们评估了这些模型计算LIX得分和执行依存解析的能力，并将其结果与既定的基准进行了比较。研究结果表明，虽然所有模型都表现出一定的能力完成这些任务，但ChatGPT-o1-mini表现出最为一致的性能，在LIX计算和依存解析方面取得最高的准确性。此外，我们还观察到模型在计算LIX方面的准确性与他们在大规模多任务语言理解（MMLU）基准上的总体性能之间存在显著的相关性（r = -0.875，p = 0.026，N = 6）。这些结果表明，语言复杂性测量的能力可以作为评估LLMs普遍能力的嘈杂零样本代理，为模型评估提供了一种实用的方法，无需使用广泛的基准数据集。', 'title_zh': '语言复杂性测量作为评估语言模型性能的噪声零样本代理'}
{'arxiv_id': 'arXiv:2502.11603', 'title': 'DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning', 'authors': 'Hongye Qiu, Yue Xu, Meikang Qiu, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11603', 'abstract': 'Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose this http URL (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. this http URL selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. this http URL can generalize to vision-language models (VLMs), achieving significant bias reduction.', 'abstract_zh': '大型语言模型（LLMs）展示了强大的自然语言处理能力，但也继承并放大了社会偏见，包括性别偏见，这引发了公平性方面的担忧。现有的去偏方法面临着重大局限：参数调整需要访问模型权重，基于提示的方法往往会降低模型的实用性，而基于优化的方法缺乏普适性。为解决这些挑战，我们提出了一种名为(this http URL)（性别意识提示的演示与推理）的方法，这是一种自动化且模型无依赖性的方法，能够在减轻性别偏见的同时保持模型性能。该方法通过选择揭示偏见的示例并生成结构化的推理来引导模型产生更为公正是的答案。通过在多个LLM（GPT-3.5、Llama3和Llama2-Alpaca）上的核心参照解析和问答任务中的广泛实验，我们展示了其有效性和泛化能力以及鲁棒性。此外，该方法还可以推广到视觉语言模型（VLMs），实现显著的偏见减少。', 'title_zh': 'DR.GAP：利用性别 Awareness 提示示范与推理减轻大型语言模型中的偏见'}
{'arxiv_id': 'arXiv:2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'authors': 'Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang', 'link': 'https://arxiv.org/abs/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'abstract_zh': '大语言模型（LLMs）和多模态大语言模型（MLLMs）在推理能力方面取得了显著进展。然而，它们仍然面临着高计算需求和隐私问题等挑战。本文专注于开发高效的小语言模型（SLMs）和多模态小语言模型（MSLMs），以保留竞争力的推理能力。我们提出了一种新的训练管道，以增强推理能力和促进在边缘设备上的部署，从而在降低成本的同时达到最先进的性能。InfR旨在通过提升推理能力、降低采用障碍和通过较小的模型尺寸解决隐私问题来促进人工智能系统的发展。相关资源可在 https://github.com/Reallm-Labs/InfiR 获取。', 'title_zh': 'InfiR：打造高效的小微型语言模型与推理中的多模态小微型语言模型'}
{'arxiv_id': 'arXiv:2502.11571', 'title': 'FaMTEB: Massive Text Embedding Benchmark in Persian Language', 'authors': 'Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini', 'link': 'https://arxiv.org/abs/2502.11571', 'abstract': 'In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.', 'abstract_zh': '在这篇文章中，我们介绍了一个针对波斯语（فارسی）文本嵌入的综合基准，该基准建立在大规模文本嵌入基准（MTEB）之上。我们的基准包括63个数据集，涵盖七个不同任务：分类、聚类、成对分类、重排、检索、摘要检索和语义文本相似度。这些数据集由现有的、翻译的和新生成的数据相结合而成，为波斯语模型提供了一个多样的评估框架。鉴于文本嵌入模型在聊天机器人中的应用日益广泛，评估数据集已成为聊天机器人挑战和检索增强生成系统不可或缺的组成部分。作为贡献之一，我们首次在MTEB基准中包含了用于评估聊天机器人的数据集。此外，本文还引入了一个新任务：摘要检索，该任务未包含在标准MTEB的任务中。另一项贡献是介绍了大量新的波斯语自然语言处理（NLP）数据集，这些数据集适用于训练和评估，其中一些数据集在此之前在波斯语中并未有对应的版本。我们对几种波斯语和多语言嵌入模型在多个任务中的性能进行了评估。这项工作引入了一个开源基准，其中包括数据集、代码以及一个公开的排行榜。', 'title_zh': 'FaMTEB：波斯语大规模文本嵌入基准'}
{'arxiv_id': 'arXiv:2502.11598', 'title': 'Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?', 'authors': 'Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.11598', 'abstract': 'The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLM）水印的放射性特征使其能够检测学生模型从被水印的教师模型输出中继承的水印，从而成为防止未经授权的知识蒸馏的有前途的工具。然而，水印放射性对抗恶意行为者的鲁棒性仍很少被研究。在本文中，我们探讨了学生模型在避免水印继承的情况下，是否能够通过知识蒸馏获得教师模型的能力。我们提出了两种水印去除方法的分类：预蒸馏去除（通过无目标和针对目标的训练数据改写（UP和TP）实现）和后蒸馏去除（通过推理时水印中和（WN）实现）。广泛的实验涵盖了多个模型对、水印方案和超参数设置，表明TP和WN都能够彻底消除继承的水印，而WN在保持知识传输效率和低计算开销的同时实现了这一点。鉴于水印技术在生产级LLM中的持续部署，这些发现强调了对更 robust 防御策略的迫切需求。我们的代码可在以下网址获取：[这里插入网址]。', 'title_zh': '标题翻译如下，符合学术规范：\n\nLLM水印能否稳健地防止未经授权的知识蒸馏？'}
{'arxiv_id': 'arXiv:2502.11569', 'title': 'Towards Reasoning Ability of Small Language Models', 'authors': 'Gaurav Srivastava, Shuxiang Cao, Xuan Wang', 'link': 'https://arxiv.org/abs/2502.11569', 'abstract': 'Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.', 'abstract_zh': '推理能力长期以来被认为是由大规模语言模型（LLMs）在某个规模（约1000亿参数）以上或其附近出现的新兴属性。然而，最近的研究挑战了这一假设，表明小型语言模型（SLMs）也能达到竞争力的推理性能。SLMs 因其高效性和可部署性而越来越受到青睐。然而，缺乏对不同类型 SLM 的系统研究，包括从零开始训练的模型或通过量化、剪枝和蒸馏从LLMs衍生的模型。这提出了一个关键问题：SLMs 是否能够达到与LLMs 相媲美的推理能力？在本文中，我们系统地调研、基准测试和分析了来自六个模型家族的72种SLMs在14个推理基准上的表现。为了进行可靠的评估，我们检查了四种评估方法，并将四种LLM评估员与人工评估进行了比较，涵盖了800个数据点。我们重复所有实验三次，以确保稳健性评估。此外，我们分析了不同提示策略对小型模型的影响。除了准确性之外，我们还评估了模型在对抗条件下的鲁棒性和中间推理步骤。我们的发现挑战了规模是实现强大推理能力的唯一途径这一假设。相反，我们预见到一个未来，即通过结构化的训练或后训练压缩，可以开发出具有强大推理能力的SLMs。这些模型可以作为LLMs的高效替代品，用于需要推理的任务。', 'title_zh': '面向小型语言模型的推理能力研究'}
{'arxiv_id': 'arXiv:2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'authors': 'Birger Moell, Johan Boye', 'link': 'https://arxiv.org/abs/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'abstract_zh': '大型语言模型（LLM）在自然语言生成方面取得了显著进步，但在需要精确计算和结构分析的任务中常常面临挑战。本论文通过计算LIX可读性度量和平均依存距离（ADD）来研究最先进的LLM在语言复杂度度量任务中的表现。我们使用瑞典中学和大学水平的文章，评估模型计算LIX分数和执行依存解析的能力，并将其结果与已有的基准真相进行比较。我们的研究结果表明，虽然所有模型都在这些任务上展示出某种程度的能力，但ChatGPT-o1-mini表现最为稳定，其在LIX计算和依存解析上的准确性最高。此外，我们还观察到模型在计算LIX的准确性与其在大规模多任务语言理解（MMLU）基准测试上的整体性能之间存在显著相关性（相关系数为-0.875，p值为0.026，样本量为6）。这些结果表明，语言复杂度度量能力可以作为评估LLM普遍能力的一种噪声零样本代理，提供了一种无需大量基准数据集即可评估模型的有效方法。', 'title_zh': '语言复杂度测量作为评价大型语言模型性能的 noisy 零样本代理'}
{'arxiv_id': 'arXiv:2502.11562', 'title': 'Reinforced Information Retrieval', 'authors': 'Chaofan Li, Zheng Liu, Jianlyv Chen, Defu Lian, Yingxia Shao', 'link': 'https://arxiv.org/abs/2502.11562', 'abstract': "While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present \\textbf{Reinforced-IR}, a novel approach that jointly adapts a pre-trained retriever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its \\textbf{Self-Boosting} framework, which enables retriever and generator to learn from each other's feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever's performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation methods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios.", 'abstract_zh': '而在实践中广泛使用的检索技术依然在多领域场景下面临着重大挑战。最近，生成增强方法作为一种有前途的解决方案逐渐兴起。这些方法通过引入基于LLM（大型语言模型）的生成器，增加原始查询的相关信息，从而促进了更直接的相关文档检索。然而，现有的方法在高度专业化且需要广泛领域专业知识的情况下表现不佳。为解决这一问题，我们提出了一种新的方法——\\textbf{强化检索}（Reinforced-IR），该方法联合适应预先训练的检索器和生成器，以实现精准的多领域检索。强化检索的一个关键创新是其\\textbf{自我提升}（Self-Boosting）框架，该框架使检索器和生成器能够互相从对方的反馈中学习。具体来说，生成器被增强以生成能提升检索器性能的查询增强信息，而检索器则被训练以更好地识别生成器所标识的相关文档。这一迭代过程通过使用目标领域的未标注语料库，逐步优化端到端的检索性能。在我们的实验中，强化检索在多种应用场景下显著优于现有的领域适应方法，并在检索质量上取得了显著提升。', 'title_zh': '强化信息检索'}
{'arxiv_id': 'arXiv:2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'authors': 'Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang', 'link': 'https://arxiv.org/abs/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'abstract_zh': '大规模语言模型（LLMs）和多模态大规模语言模型（MLLMs）在推理能力方面取得了显著进步。然而，它们仍然面临计算成本高的挑战以及隐私问题。本文致力于开发高效的中小型语言模型（SLMs）和多模态中小型语言模型（MSLMs），这些模型能够在保留竞争性推理能力的基础上降低开发成本。我们提出了一种新的训练流程，旨在增强推理能力并促进边缘设备上的部署，从而实现最先进的性能。InInfR 旨在通过提升推理能力、降低采用门槛和通过较小的模型规模解决隐私问题，来推动AI系统的进步。更多资源可在 https://github.com/Reallm-Labs/InInfR 获取。', 'title_zh': 'InfiR：打造高效的小型语言模型与推理性的小型多模态语言模型'}
{'arxiv_id': 'arXiv:2502.11559', 'title': 'Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models', 'authors': 'Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11559', 'abstract': 'Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.', 'abstract_zh': '大规模语言模型（LLMs）在庞大的文本语料上进行预训练可以增强自然语言处理能力，但同时也有可能编码社会偏见，特别是性别偏见。虽然参数调整方法如微调可以在一定程度上缓解偏见，但这些方法资源密集、不适用于封闭源模型，并且缺乏应对不断变化的社会规范的适应性。基于指令的方法提供了灵活性，但在很多时候会牺牲任务性能。为了解决这些局限性，我们提出了一种名为$\\textit{FaIRMaker}$的自动且与模型无关的框架，该框架采用了$\\textbf{自动搜索和精细调整}$的范式，自适应地生成Fairwords，这些Fairwords作为指令集成到输入查询中，以减少性别偏见并提升响应质量。广泛的实验表明，$\\textit{FaIRMaker}$能够自动搜索和动态细化Fairwords，有效地缓解性别偏见，同时保持任务完整性并确保与基于API的和开源的大规模语言模型兼容。', 'title_zh': '自动搜索和精炼：一种用于大型语言模型性别偏见缓解的自动化框架'}
{'arxiv_id': 'arXiv:2502.11571', 'title': 'FaMTEB: Massive Text Embedding Benchmark in Persian Language', 'authors': 'Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini', 'link': 'https://arxiv.org/abs/2502.11571', 'abstract': 'In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.', 'abstract_zh': '在本文中，我们介绍了一个全面的基准测试，用于波斯语（فارسی）文本嵌入，该基准测试基于大规模文本嵌入基准（MTEB）。我们的基准测试包括63个数据集，涵盖了七个不同的任务：分类、聚类、配对分类、重新排序、检索、摘要检索和语义文本相似度。这些数据集是由现有数据、翻译数据和新生成数据的组合构成的，提供了波斯语模型的多样化的评估框架。鉴于文本嵌入模型在聊天机器人中的广泛应用，评估数据集已成为聊天机器人挑战和检索增强生成系统的不可或缺的组成部分。作为贡献之一，我们首次在MTEB基准测试中包括了聊天机器人评估数据集。此外，在本文中，我们还引入了一项新的任务——摘要检索，这是标准MTEB所未包含的任务之一。本文的另一项贡献是引入了一大批新的波斯语NLP数据集，这些数据集适合用于训练和评估，其中一些数据集在此之前在波斯语中并无前例。我们对多种波斯语和多语言嵌入模型在一系列任务中的性能进行了评估。本文介绍了包含数据集、代码和公开排行榜的开源基准测试。', 'title_zh': 'FaMTEB：波斯语大规模文本嵌入基准'}
{'arxiv_id': 'arXiv:2502.11569', 'title': 'Towards Reasoning Ability of Small Language Models', 'authors': 'Gaurav Srivastava, Shuxiang Cao, Xuan Wang', 'link': 'https://arxiv.org/abs/2502.11569', 'abstract': 'Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.', 'abstract_zh': '长期来看，推理能力被认为是由大规模语言模型（LLMs）在某一特定规模（约100亿参数）以上时才体现出的一种 emergent 属性。然而，近期的研究挑战了这一假设，表明小型语言模型（SLMs）也能达到竞争力的推理表现。SLMs 因其高效的特性和易于部署而受到青睐。然而，关于多样化的 SLMs（包括从零开始训练和通过量化、剪枝和蒸馏从LLMs衍生的模型）的推理能力系统性研究仍然不足。这引发了一个关键问题：SLMs 是否能够在推理能力上与LLMs 相媲美？本研究系统地调研、基准测试和分析了六大家族中的72个 SLM 在14个推理基准上的表现。为了确保评估的可靠性，我们检查了四种评估方法，并在800个数据点上将四种LLM评估者与人类评价进行比较。我们重复所有实验三次，以确保能够进行稳健的表现评估。此外，我们还分析了不同类型提示策略对小型模型的影响。除了准确性，我们还评估了模型在对抗性条件下的鲁棒性以及推理过程中的中间步骤。我们的发现挑战了认为仅通过扩展才能实现强大的推理能力这一假设。相反，我们预见未来可以通过结构化训练或后训练压缩来开发具备强大推理能力的SLMs，并将其作为LLMs在推理密集任务中的有效替代品。', 'title_zh': '面向小型语言模型的推理能力研究'}
{'arxiv_id': 'arXiv:2502.11546', 'title': 'DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection', 'authors': 'Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander Fraser, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11546', 'abstract': 'The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and clean multilingual datasets. In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a large-scale multilingual corpus built using newly extracted Common Crawl data and existing multilingual datasets. DCAD-2000 includes over 2,282 languages, 46.72TB of data, and 8.63 billion documents, spanning 155 high- and medium-resource languages and 159 writing scripts. To overcome the limitations of current data cleaning methods, which rely on manual heuristic thresholds, we propose reframing data cleaning as an anomaly detection task. This dynamic filtering approach significantly enhances data quality by identifying and removing noisy or anomalous content. We evaluate the quality of DCAD-2000 on the FineTask benchmark, demonstrating substantial improvements in multilingual dataset quality and task performance.', 'abstract_zh': '多语言大型语言模型（LLMs）的快速发展突显了对高质量、多样性和干净的多语言数据集的需求。本文介绍了DCAD-2000（数据清洗作为一种异常检测）数据集，该数据集利用新提取的Common Crawl数据和现有的多语言数据集构建。DCAD-2000包含超过2,282种语言、46.72TB的数据和86.3亿份文档，覆盖155种高资源和中资源语言以及159种书写系统。为克服当前基于人工启发式阈值的数据清洗方法的限制，我们提出将数据清洗重新定义为异常检测任务。这一动态筛选方法通过识别并去除噪声或异常内容，显著提高了数据质量。我们在FineTask基准上评估了DCAD-2000的质量，显示多语言数据集质量和任务性能有了显著提升。', 'title_zh': 'DCAD-2000：一种用于2000多种语言的数据集，其中数据清洗作为异常检测'}
{'arxiv_id': 'arXiv:2502.11562', 'title': 'Reinforced Information Retrieval', 'authors': 'Chaofan Li, Zheng Liu, Jianlyv Chen, Defu Lian, Yingxia Shao', 'link': 'https://arxiv.org/abs/2502.11562', 'abstract': "While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present \\textbf{Reinforced-IR}, a novel approach that jointly adapts a pre-trained retriever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its \\textbf{Self-Boosting} framework, which enables retriever and generator to learn from each other's feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever's performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation methods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios.", 'abstract_zh': '虽然检索技术在实际应用中得到广泛应用，但在跨域场景中仍然面临重大挑战。近年来，生成增强方法作为一种有前景的解决方案逐渐浮现。这些方法通过结合基于LLM的生成器提供的附加信息来增强原始查询，从而促进更直接的相关文档检索。然而，现有的方法在高度专业化的情境下表现不佳，这些情境需要广泛的领域专业知识。为解决这一问题，我们提出了**Reinforced-IR**，一种新颖的方法，该方法联合调整预训练的检索器和生成器，以实现精确的跨域检索。Reinforced-IR 的一个关键创新是其**自我增强**框架，该框架使检索器和生成器能够从彼此的反馈中学习。具体而言，生成器通过生成增强检索器性能的查询增强来得到强化，而检索器则被训练以更好地鉴别生成器标识的相关文档。这一迭代过程允许通过目标领域的未标记语料库逐步优化端到端的检索性能。在我们的实验中，Reinforced-IR 显著优于现有的领域适应方法，在多种应用场景中显著提升了检索质量。', 'title_zh': '强化信息检索'}
{'arxiv_id': 'arXiv:2502.11544', 'title': 'Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis', 'authors': 'Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, Min zhang', 'link': 'https://arxiv.org/abs/2502.11544', 'abstract': 'The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision.', 'abstract_zh': '类似于o1的大型语言模型（LLM）通过模拟人类认知过程正在变革人工智能，但在多语言机器翻译（MTM）领域的表现仍需进一步探索。本研究旨在探讨：（1）类似于o1的LLM在MTM任务中的表现，以及（2）哪些因素影响其翻译质量。我们评估了多个类似于o1的LLM，并将其与传统的模型如ChatGPT和GPT-4o进行比较。结果显示，类似于o1的LLM建立了新的多语言翻译基准，并且DeepSeek-R1在无上下文任务中超越了GPT-4o。它们在历史文化和翻译方面表现出色，但在以中文为中心的输出中倾向于产生冗长的问题。进一步的分析揭示了三个关键洞见：（1）高推理成本和较慢的处理速度使复杂翻译任务更加资源密集。（2）随着模型规模的扩大，翻译质量得到提升，增强了常识推理和文化翻译的能力。（3）温度参数对输出质量有显著影响，较低的温度会产生更加稳定和准确的翻译，而较高的温度则会降低连贯性和精确度。', 'title_zh': '评估o1-类大语言模型：通过全面分析解锁翻译中的推理能力'}
{'arxiv_id': 'arXiv:2502.11559', 'title': 'Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models', 'authors': 'Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11559', 'abstract': 'Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\\textit{FaIRMaker}$, an automated and model-independent framework that employs an $\\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$ automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.', 'abstract_zh': '将以下论文内容或标题翻译成中文，要符合学术规范：\n\n在庞大的文本语料上预训练大型语言模型（LLMs）能够提升自然语言处理能力，但同时也可能编码社会偏见，尤其是在性别偏见方面。虽然参数调整方法如微调可以在一定程度上减轻偏见，但这些方法资源密集、不适合闭源模型，并且缺乏对不断演变的社会规范的适应性。基于指令的方法则提供了更大的灵活性，但往往会牺牲任务性能。为了解决这些局限性，我们提出了一种名为$\\textit{FaIRMaker}$的自动化且与模型无关的框架，该框架采用自动搜索与优化机制，自适应地生成Fairwords，使Fairwords作为指令集成到输入查询中，以减少性别偏见并提高响应质量。广泛的实验证明，$\\textit{FaIRMaker}$可以自动搜索和动态优化Fairwords，有效减轻性别偏见，同时保持任务完整性，并确保与基于API和开源的LLMs兼容。', 'title_zh': '自动搜索与精炼：一种用于大型语言模型性别偏见缓解的自动化框架'}
{'arxiv_id': 'arXiv:2502.11541', 'title': 'MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training', 'authors': 'Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao', 'link': 'https://arxiv.org/abs/2502.11541', 'abstract': 'Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.', 'abstract_zh': '复杂的指令跟随需要细致的约束条件，这对大型语言模型（LLMs）来说至关重要。尽管现有方法已经构建了复杂的指令对齐数据，但它们都依赖于更高级的模型，尤其是GPT-4，这限制了它们的应用范围。在本文中，我们提出了一种多层次自对比训练（MuSC）框架，以提高复杂的指令对齐能力，而无需依赖更强的模型。我们的方法在粗粒度和细粒度两个层面进行。在粗粒度层面，我们基于指令分解与重组构建了约束感知的偏好数据。在细粒度层面，我们进行了基于动态token级别的监督的token感知偏好优化。我们方法在开源模型上进行了评估，并且实验结果显示，我们的方法在复杂的和一般的指令跟随基准测试中都取得了显著改进，并超越了之前的自我对齐方法。', 'title_zh': 'MuSC：基于多粒度自对比训练的复杂指令跟随改进方法'}
{'arxiv_id': 'arXiv:2502.11546', 'title': 'DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection', 'authors': 'Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander Fraser, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11546', 'abstract': 'The rapid development of multilingual large language models (LLMs) highlights the need for high-quality, diverse, and clean multilingual datasets. In this paper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a large-scale multilingual corpus built using newly extracted Common Crawl data and existing multilingual datasets. DCAD-2000 includes over 2,282 languages, 46.72TB of data, and 8.63 billion documents, spanning 155 high- and medium-resource languages and 159 writing scripts. To overcome the limitations of current data cleaning methods, which rely on manual heuristic thresholds, we propose reframing data cleaning as an anomaly detection task. This dynamic filtering approach significantly enhances data quality by identifying and removing noisy or anomalous content. We evaluate the quality of DCAD-2000 on the FineTask benchmark, demonstrating substantial improvements in multilingual dataset quality and task performance.', 'abstract_zh': '多语言大规模语言模型（LLMs）的迅速发展突显了构建高质量、多样性和纯净的多语言数据集的重要性。本文介绍了一种名为DCAD-2000（数据清洗作为一种异常检测）的大规模多语言语料库，该语料库使用了新提取的Common Crawl数据和现有的多语言数据集构建。DCAD-2000包含了超过2282种语言，数据量达到46.72TB，涵盖了863亿份文档，范围涵盖了155种高资源和中资源语言以及159种书写系统。为了克服当前数据清洗方法的局限性，这些方法依赖于人工设定的启发式阈值，我们提出将数据清洗重新定义为异常检测任务。这种动态过滤方法通过识别和移除噪音或异常内容，显著提高了数据质量。我们通过FineTask基准对DCAD-2000的质量进行了评估，结果显示多语言数据集质量与任务性能得到了显著提升。', 'title_zh': 'DCAD-2000：一种包含2000多种语言的数据集，其中数据清洗作为异常检测'}
{'arxiv_id': 'arXiv:2502.11533', 'title': 'Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy', 'authors': 'Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen', 'link': 'https://arxiv.org/abs/2502.11533', 'abstract': 'Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.', 'abstract_zh': '模型合并是大型语言模型（LLMs）中的一项广泛应用的技术，它可以将多个特定任务的语言模型整合为一个统一的模型，从而使合并后的模型能够继承这些语言模型的专业能力。大多数特定任务的语言模型来自开源社区，尚未经过严格的审查，这可能在模型合并过程中带来潜在风险。本文强调了一个被忽视的隐私风险：**一个不安全的模型可能会泄露参与模型合并的其他语言模型的隐私。**具体来说，我们提出了 PhiMM，一种利用制作的隐私诱饵指令数据集训练一种钓鱼模型的隐私攻击方法，该模型能够盗取隐私。此外，我们还介绍了一种新的模型伪装方法，该方法模仿特定能力以掩盖攻击意图，诱使用户合并钓鱼模型。一旦受害者合并钓鱼模型，攻击者可以通过向合并后的模型查询钓鱼指令来提取个人可识别信息（PII）或推断成员信息（MI）。实验结果表明，合并钓鱼模型会增加隐私泄露的风险。与合并前的结果相比，平均而言，PII 泄露增加了 3.9%，MI 泄露增加了 17.4%。我们通过链接发布了 PhiMM 的代码。', 'title_zh': '在合并不熟悉的语言模型时要谨慎：一种具备窃取隐私能力的钓鱼模型'}
{'arxiv_id': 'arXiv:2502.11544', 'title': 'Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis', 'authors': 'Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, Min zhang', 'link': 'https://arxiv.org/abs/2502.11544', 'abstract': 'The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision.', 'abstract_zh': '类似于o1的大型语言模型（LLMs）正在通过模拟人类认知过程来改变人工智能领域，但在多语言机器翻译（MMT）方面的表现仍未充分探索。本研究旨在探讨：（1）类似于o1的LLMs在MMT任务中的表现，以及（2）哪些因素影响其翻译质量。我们评估了多个类似于o1的LLMs，并将其与传统的模型如ChatGPT和GPT-4o进行比较。结果显示，类似于o1的LLMs建立了新的多语言翻译基准，DeepSeek-R1在无语境任务中超过了GPT-4o。它们在历史和文化交流翻译方面显示出优势，但在以中文为中心的输出中存在逻辑冗长的问题。进一步分析揭示了三点关键见解：（1）高推理成本和较慢的处理速度使复杂翻译任务更为资源密集型。（2）模型规模的增大可以提高翻译质量，增强常识推理和文化翻译能力。（3）温度参数显著影响输出质量，较低的温度值产出更稳定和准确的翻译，而较高的温度值则降低了连贯性和精确性。', 'title_zh': '评估o1-样式的大型语言模型：通过全面分析解锁翻译推理功能'}
{'arxiv_id': 'arXiv:2502.11525', 'title': 'Training Large Language Models to be Better Rule Followers', 'authors': 'Yi Hu, Shijia Kang, Haotong Yang, Haotian Xu, Muhan Zhang', 'link': 'https://arxiv.org/abs/2502.11525', 'abstract': 'Large language models (LLMs) have shown impressive performance across a wide range of tasks. However, they often exhibit unexpected failures in seemingly straightforward tasks, suggesting a reliance on case-based reasoning rather than rule-based reasoning. While the vast training corpus of LLMs contains numerous textual "rules", current training methods fail to leverage these rules effectively. Crucially, the relationships between these "rules" and their corresponding "instances" are not explicitly modeled. As a result, while LLMs can often recall rules with ease, they fail to apply these rules strictly and consistently in relevant reasoning scenarios. In this paper, we investigate the rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance the cross-task transferability of rule-following abilities. We first construct a dataset of 88 tasks requiring following rules, encompassing diverse reasoning domains. We demonstrate through extensive experiments that models trained on large-scale rule-following tasks are better rule followers, outperforming the baselines in both downstream fine-tuning and few-shot prompting scenarios. This highlights the cross-task transferability of models with the aid of Meta-RFFT. Furthermore, we examine the influence of factors such as dataset size, rule formulation, and in-context learning.', 'abstract_zh': '大型语言模型（LLMs）在广泛的任务中展现出了令人印象深刻的性能。然而，它们在一些看似简单的任务中常常表现出意想不到的失败，这暗示了它们更多依赖于案例推理而非规则推理。尽管LLMs的大量训练语料包含了大量的文本“规则”，但当前的训练方法未能有效地利用这些规则。关键的是，这些“规则”与其相应的“实例”之间的关系并未明确建模。因此，尽管LLMs可以轻松回忆起规则，但在相关推理场景中却不能严格且一致地应用这些规则。在本文中，我们探讨了LLMs的规则遵循能力，并提出了元规则遵循微调方法（Meta-RFFT）以增强规则遵循能力在不同任务之间的迁移性。我们首先构建了一个包含88个任务的数据集，这些任务要求遵循规则，涵盖了不同的推理领域。通过广泛的实验表明，在大规模规则遵循任务上进行训练的模型在规则遵循方面更优秀，在下游微调和少量示例提示场景中均优于基线模型。这突显了在Meta-RFFT帮助下的模型在不同任务间的迁移性。此外，我们还研究了因素如数据集大小、规则表达和上下文学习等因素的影响。', 'title_zh': '训练大型语言模型成为更好的规则遵循者'}
{'arxiv_id': 'arXiv:2502.11541', 'title': 'MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training', 'authors': 'Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao', 'link': 'https://arxiv.org/abs/2502.11541', 'abstract': 'Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.', 'abstract_zh': '复杂的指令遵循伴随详尽约束是大型语言模型（LLMs）所必需的。尽管现有方法已经构建了用于复杂指令对齐的数据，但它们仍然依赖于更先进的模型，尤其是GPT-4，这限制了它们的应用范围。在本文中，我们提出了一种多粒度自我对比训练（MuSC）框架，以在不依赖更强模型的情况下提高复杂指令对齐的能力。我们的方法在粗粒度和细粒度级别上进行操作。在粗粒度级别上，我们基于指令分解和重组构建了带有约束感知偏好的数据。在细粒度级别上，我们通过动态的标记级监督执行带有标记意识的偏好优化。我们在开源模型上进行了方法评估，实验结果表明，我们的方法在复杂和通用指令遵循基准测试中均实现了显著改进，超越了先前的自我对齐方法。', 'title_zh': 'MuSC：多粒度自对比训练提升复杂指令跟随能力'}
{'arxiv_id': 'arXiv:2502.11533', 'title': 'Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy', 'authors': 'Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen', 'link': 'https://arxiv.org/abs/2502.11533', 'abstract': 'Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.', 'abstract_zh': '模型合并是大语言模型（LLMs）中广泛使用的一项技术，它将多个任务特定的LLMs整合成一个统一的模型，从而使合并后的模型能够继承这些LLMs的专业能力。大多数任务特定的LLMs来自开源社区，未经过严格的审计，这可能在模型合并过程中带来风险。本文强调了一个被忽视的隐私风险：**不安全的模型可能会泄露参与模型合并的其他LLMs的隐私**。具体来说，我们提出了一种名为PhiMM的隐私攻击方法，该方法通过构造一个精心设计的隐私钓鱼指令数据集来训练一个能够窃取隐私的模型。此外，我们引入了一种新的模型伪装方法，模拟一种特殊能力以隐藏攻击意图，诱使用户将钓鱼模型纳入合并模型中。一旦受害者将钓鱼模型合并进来，攻击者可以通过使用钓鱼指令查询合并后的模型来提取个人可识别信息（PII）或推断成员身份信息（MI）。实验结果表明，合并包含钓鱼模型会增加隐私泄露的风险。与合并前的实验结果相比，平均而言，PII泄露增加了3.9%，MI泄露增加了17.4%。我们通过链接发布了PhiMM的代码。', 'title_zh': '谨慎合并不熟悉的大型语言模型：一款具备盗取隐私能力的钓鱼模型'}
{'arxiv_id': 'arXiv:2502.11520', 'title': 'AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification', 'authors': 'Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, Yuan Qi', 'link': 'https://arxiv.org/abs/2502.11520', 'abstract': "The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at this https URL. The Universal-PRM-7B is available at this https URL.", 'abstract_zh': '先进大型语言模型（如o1）的推理能力已经革新了人工智能应用。然而，评估和优化复杂的推理过程仍然面临着显著的挑战，这主要归因于多样化的政策分布和人类努力及准确性的固有限制。本文中，我们提出了一种名为AURORA的新颖自动化框架，用于通过组合提示和逆向验证训练通用过程奖励模型（PRMs）。该框架采用两阶段方法：首先，它使用多样化的提示策略和集成方法进行自动化的过程注释和评估，确保奖励学习的稳健评估；其次，它利用实际的参考答案进行逆向验证，增强模型输出的有效验证能力，提高训练准确性。为评估该框架的性能，我们通过引入UniversalBench，扩展了现有的ProcessBench基准测试，UniversalBench在多种政策分布下评估了不同长Chain-of-Thought（长思维链）输出的奖励预测。实验结果显示，AURORA提高了过程评估的准确性，并提高了PRMs在多种政策分布和长思维链响应下的准确性。该项目将在https://github.com/AI-Rescale/AURORA 开源，通用PRMs（Universal-PRM-7B）的模型文件可在https://huggingface.co/AI-Rescale/Universal-PRM-7B 下载。', 'title_zh': 'AURORA：通过集成提示和逆向验证的通用过程奖励模型自动化训练框架'}
{'arxiv_id': 'arXiv:2502.11517', 'title': 'Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding', 'authors': 'Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin', 'link': 'https://arxiv.org/abs/2502.11517', 'abstract': 'Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses. At its core are PASTA-LANG and its interpreter: PASTA-LANG is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. Through a two-stage finetuning process, we train LLMs to generate PASTA-LANG annotations that optimize both response quality and decoding speed. Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to -7.1%, measured by length-controlled win rates against sequential decoding baseline.', 'abstract_zh': '使用自回归大规模语言模型（LLM）的传统解码过程通常是顺序进行的，一个token接一个token地生成。一项新兴的研究方向通过识别和同时生成LLM响应中的语义独立片段来探索并行解码。然而，这些技术依赖于与语法结构（如列表和段落）相关的手动构建启发式方法，这使得它们变得僵化且不够精确。我们提出了PASTA，一个基于学习的系统，教导LLM识别语义独立性并在响应中表达并行解码机会。PASTA的核心是PASTA-LANG及其解释器：PASTA-LANG是一种注释语言，使LLM能够在其响应中表达语义独立性；解释器则根据这些注释，实时在推理时协调并行解码。通过两阶段的微调过程，我们训练LLM生成优化响应质量和解码速度的PASTA-LANG注释。在AlpacaEval指令跟随基准上的评估表明，我们的方法在解码速度和响应质量方面 Pareto 优于现有方法；我们的结果表明，与顺序解码基线相比，实现的几何平均加速比从1.21倍到1.93倍不等，相应的质量变化范围为+2.2%到-7.1%，通过长度控制的胜率测量得出。', 'title_zh': '学习遵守承诺：通过学习异步解码扩展语言模型解码并行性'}
{'arxiv_id': 'arXiv:2502.11525', 'title': 'Training Large Language Models to be Better Rule Followers', 'authors': 'Yi Hu, Shijia Kang, Haotong Yang, Haotian Xu, Muhan Zhang', 'link': 'https://arxiv.org/abs/2502.11525', 'abstract': 'Large language models (LLMs) have shown impressive performance across a wide range of tasks. However, they often exhibit unexpected failures in seemingly straightforward tasks, suggesting a reliance on case-based reasoning rather than rule-based reasoning. While the vast training corpus of LLMs contains numerous textual "rules", current training methods fail to leverage these rules effectively. Crucially, the relationships between these "rules" and their corresponding "instances" are not explicitly modeled. As a result, while LLMs can often recall rules with ease, they fail to apply these rules strictly and consistently in relevant reasoning scenarios. In this paper, we investigate the rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance the cross-task transferability of rule-following abilities. We first construct a dataset of 88 tasks requiring following rules, encompassing diverse reasoning domains. We demonstrate through extensive experiments that models trained on large-scale rule-following tasks are better rule followers, outperforming the baselines in both downstream fine-tuning and few-shot prompting scenarios. This highlights the cross-task transferability of models with the aid of Meta-RFFT. Furthermore, we examine the influence of factors such as dataset size, rule formulation, and in-context learning.', 'abstract_zh': '大语言模型（LLMs）在多种任务上表现出色，但在看似简单的任务上却经常出现意外的失败，这表明它们更依赖案例推理而非规则推理。尽管LLMs的庞大训练语料库包含了大量的文本“规则”，但当前的训练方法未能有效地利用这些规则。最关键的是，这些“规则”与其对应的“实例”之间的关系没有明确建模。因此，虽然LLMs可以轻松回忆规则，但在相关推理场景中却无法严格一致地应用这些规则。本文研究了LLMs的规则遵守能力，并提出了一种元规则遵守微调方法（Meta-RFFT），以增强规则遵守能力的跨任务迁移性。首先，我们构造了一个包含88个任务的数据集，这些任务要求遵守规则，涵盖了多种推理领域。通过广泛的实验表明，在大规模规则遵守任务上训练的模型在下游微调和少量示例提示场景中表现出更好的规则遵守能力，优于基线模型。这突显了在Meta-RFFT的辅助下模型的跨任务迁移性。此外，我们还探讨了数据集规模、规则表述和上下文学习等因素的影响。', 'title_zh': '训练大型语言模型以成为更好的规则遵循者'}
{'arxiv_id': 'arXiv:2502.11514', 'title': 'Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study', 'authors': 'Yujie Lin, Ante Wang, Moye Chen, Jingyao Liu, Hao Liu, Jinsong Su, Xinyan Xiao', 'link': 'https://arxiv.org/abs/2502.11514', 'abstract': 'Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks. While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored. In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap. To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains. Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms. Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking. Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications. We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field.', 'abstract_zh': '近年来，推理时长链思维（CoT）的尺度调整已被证明是一种有望解决多模态推理任务的方法。虽然现有研究主要集中在基于文本的思考上，但将视觉和文本模态同时整合到推理过程中尚未得到探讨。本研究旨在填补这一空白，率先探索多模态思维的推理时长尺度调整。为了进行全面分析，我们在涵盖不同领域的10个具有挑战性的任务上系统地研究了基于采样的和基于树搜索的方法。此外，我们统一采用了一种增强一致性的验证器，以确保这些方法在不同思考模式下的有效指导。结果显示，多模态思维比传统的仅文本思维表现出更好的性能，且结合两种类型的思考促进了更多样化的想法。尽管有这些优势，多模态思维需要更高数量的标记来处理更丰富的视觉输入，这在实际应用中提出了挑战。我们希望这些关于该研究方向的优点和缺点的研究发现能够启发未来的研究工作。', 'title_zh': '多模态思维链推理时的缩放研究：初步探索'}
{'arxiv_id': 'arXiv:2502.11520', 'title': 'AURORA:Automated Training Framework of Universal Process Reward Models via Ensemble Prompting and Reverse Verification', 'authors': 'Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, Yuan Qi', 'link': 'https://arxiv.org/abs/2502.11520', 'abstract': "The reasoning capabilities of advanced large language models (LLMs) like o1 have revolutionized artificial intelligence applications. Nevertheless, evaluating and optimizing complex reasoning processes remain significant challenges due to diverse policy distributions and the inherent limitations of human effort and accuracy. In this paper, we present AURORA, a novel automated framework for training universal process reward models (PRMs) using ensemble prompting and reverse verification. The framework employs a two-phase approach: First, it uses diverse prompting strategies and ensemble methods to perform automated annotation and evaluation of processes, ensuring robust assessments for reward learning. Second, it leverages practical reference answers for reverse verification, enhancing the model's ability to validate outputs and improving training accuracy. To assess the framework's performance, we extend beyond the existing ProcessBench benchmark by introducing UniversalBench, which evaluates reward predictions across full trajectories under diverse policy distribtion with long Chain-of-Thought (CoT) outputs. Experimental results demonstrate that AURORA enhances process evaluation accuracy, improves PRMs' accuracy for diverse policy distributions and long-CoT responses. The project will be open-sourced at this https URL. The Universal-PRM-7B is available at this https URL.", 'abstract_zh': '先进的大型语言模型（LLMs）如o1的推理能力已经革新了人工智能应用。然而，评估和优化复杂的推理过程仍然面临重大挑战，这主要是由于政策分布的多样性及人类努力和准确性固有的局限性。本文提出了一种名为AURORA的新型自动化框架，用于通过集束提示和反向验证训练通用过程奖励模型（PRMs）。该框架采用两阶段方法：首先，使用多样化的提示策略和集束方法进行自动标注和评估，以确保奖励学习的稳健评估。其次，利用实际参考答案进行反向验证，增强模型对输出结果的验证能力，从而提高训练准确性。为了评估框架的性能，我们扩展了现有的ProcessBench基准，引入了UniversalBench，该基准在多种政策分布下，评估了完整轨迹下的奖励预测，并使用长链式思维（CoT）输出。实验结果表明，AURORA提高了过程评估的准确性，改善了PRMs在多种政策分布和长-CoT响应下的准确性。该项目将在以下链接开源：[链接]。通用-PRM-7B模型可在此链接获取：[链接]。', 'title_zh': 'AURORA:通过集成提示和逆向验证自动训练通用过程奖励模型的框架'}
{'arxiv_id': 'arXiv:2502.11508', 'title': 'Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities', 'authors': 'Changchun Liu, Kai Zhang, Junzhe Jiang, Zixiao Kong, Qi Liu, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.11508', 'abstract': 'Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.', 'abstract_zh': '中文翻译如下，符合学术规范：\n\n中文拼写纠错（CSC）是自然语言处理中的一个关键任务，旨在检测并纠正中文文本中的拼写错误。本综述对CSC进行了全面概述，追溯其从预训练语言模型到大规模语言模型的发展历程，并对其在该领域的各自优势与不足进行了深入分析。此外，我们还详细探讨了现有的基准数据集，突出了它们固有的挑战和局限性。最后，我们提出了有前景的研究方向，特别是强调利用大规模语言模型及其推理能力以提高CSC性能的可能性。据我们所知，这是首个专注于CSC领域的综合性综述。我们认为，本工作将为研究人员提供宝贵资源，促进对该领域的深入理解，激发未来的发展进步。', 'title_zh': '中文标题和内容翻译如下，确保符合学术规范：\n\n中文标题：汉语拼写纠错：进展、挑战与机遇综述\n\n摘要：本文对汉语拼写纠错的研究进展、面临的挑战以及未来的机会进行了全面综述。'}
{'arxiv_id': 'arXiv:2502.11501', 'title': 'Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?', 'authors': 'Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2502.11501', 'abstract': 'Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.', 'abstract_zh': '多模态大型语言模型（MLLMs）在跨模态理解和生成方面表现出色，但仍然面临严重的推理成本问题。最近，许多研究提出了通过令牌裁剪的方法来解决这一问题，这些方法识别出MLLMs中的冗余令牌，并移除它们以减少计算和键值存储成本，从而在无需训练的情况下实现显著加速。虽然这些方法声称可以提高效率，但它们在基础设计和评估方面仍然存在许多关键问题：为什么许多现有方法的表现甚至不如随机选择令牌的方法？基于注意机制的评分是否足够可靠地识别冗余令牌？在令牌裁剪过程中，语言信息是否真的有帮助？如何在令牌重要性和冗余性之间找到一个良好的平衡？当前的评估协议是否全面且无偏？以往研究对这些问题的忽视阻碍了令牌裁剪方法的长期发展。在本文中，我们将逐一回答这些问题，为未来令牌裁剪方法的设计提供见解。', 'title_zh': '多模态大型语言模型中的 token 剪枝：我们是否解决的是正确的问题？'}
{'arxiv_id': 'arXiv:2502.11495', 'title': 'Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models', 'authors': 'Masahiro Kaneko, Alham Fikri Aji, Timothy Baldwin', 'link': 'https://arxiv.org/abs/2502.11495', 'abstract': 'Multilingual large language models (MLLMs) are able to leverage in-context learning (ICL) to achieve high performance by leveraging cross-lingual knowledge transfer without parameter updates. However, their effectiveness is highly sensitive to example selection, particularly in multilingual settings. Based on the findings of existing work, three key factors influence multilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3) language-specific performance. However, existing approaches address these factors independently, without explicitly disentangling their combined impact, leaving optimal example selection underexplored. To address this gap, we propose balanced multi-factor ICL (\\textbf{BMF-ICL}), a method that quantifies and optimally balances these factors for improved example selection. Experiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL outperforms existing methods. Further analysis highlights the importance of incorporating all three factors and the importance of selecting examples from multiple languages.', 'abstract_zh': '多语言大规模语言模型（MLLMs）能够利用上下文学习（ICL）来通过利用跨语言知识转移实现高-performance，而无需更新参数。然而，它们的有效性在很大程度上取决于示例选择，尤其是在多语言设置中。基于现有工作的发现，三个关键因素影响多语言ICL：(1) 语义相似性，(2) 语言对齐，以及(3) 语言特定性能。然而，现有的方法独立地应对这些因素，而没有明确拆分它们的综合影响，从而使得最佳示例选择尚待探索。为了解决这一缺口，我们提出了平衡多因素ICL（\\textbf{BMF-ICL}），一种量化并最优平衡这些因素的方法，以提高示例选择的性能。在四个MLLMs上的mCSQA和TYDI实验表明，BMF-ICL优于现有方法。进一步的分析强调了综合考虑这三个因素的重要性，以及从多种语言中选择示例的重要性。', 'title_zh': '多语言大型语言模型中的均衡多因素上下文学习'}
{'arxiv_id': 'arXiv:2502.11517', 'title': 'Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding', 'authors': 'Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin', 'link': 'https://arxiv.org/abs/2502.11517', 'abstract': 'Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses. At its core are PASTA-LANG and its interpreter: PASTA-LANG is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. Through a two-stage finetuning process, we train LLMs to generate PASTA-LANG annotations that optimize both response quality and decoding speed. Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to -7.1%, measured by length-controlled win rates against sequential decoding baseline.', 'abstract_zh': '自回归大型语言模型（LLMs）传统的解码过程是顺序进行的，逐个生成一个令牌后再生成下一个。一项新兴的研究方向通过识别和同时生成语义独立的LLM响应片段来探索并行解码的方法。然而，这些技术依赖于与句法结构（如列表和段落）相关的手工设计启发式规则，这使得它们既僵硬又不够精确。我们提出了一种基于学习的系统PASTA，该系统通过教会LLMs识别语义独立性并在其自身响应中表达并行解码机会来实现解码。其核心是PASTA-LANG及其解释器：PASTA-LANG是一种注释语言，使得LLMs能够在其自身响应中表达语义独立性；解释器根据这些注释，在推断时实时协调并行解码。通过两阶段微调过程，我们训练LLMs生成优化响应质量和解码速度的PASTA-LANG注释。在AlpacaEval（指令跟随基准）上的评估显示，我们的方法在解码速度和响应质量方面相对于现有方法都具有支配性优势；我们的结果显示，在控制长度的情况下，与顺序解码基准相比，我们方法的速度平均提高了1.21到1.93倍，同时质量变化范围为+2.2%到-7.1%。', 'title_zh': '学习守约：通过学习异步解码扩展语言模型解码并行性'}
{'arxiv_id': 'arXiv:2502.11494', 'title': 'Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More', 'authors': 'Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2502.11494', 'abstract': 'Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning, which first defines an importance criterion for tokens and then prunes the unimportant vision tokens during inference. However, in this paper, we show that the importance is not an ideal indicator to decide whether a token should be pruned. Surprisingly, it usually results in inferior performance than random token pruning and leading to incompatibility to efficient attention computation this http URL, we propose DART (Duplication-Aware Reduction of Tokens), which prunes tokens based on its duplication with other tokens, leading to significant and training-free acceleration. Concretely, DART selects a small subset of pivot tokens and then retains the tokens with low duplication to the pivots, ensuring minimal information loss during token pruning. Experiments demonstrate that DART can prune 88.9% vision tokens while maintaining comparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in total time and prefilling stage, respectively, with good compatibility to efficient attention operators. Our codes are available at this https URL.', 'abstract_zh': '多模态大语言模型中的视觉标记由于其长度远超语言模态，常常导致巨大的计算开销。近年来，许多方法试图通过标记剪枝来解决这一问题，这些方法首先定义一个标记的重要标准，然后在推理过程中消除不重要的视觉标记。然而，在本文中，我们表明重要性并不是决定是否应该剪枝标记的理想指标。令人惊讶的是，这种方法通常会导致性能劣于随机标记剪枝，并且无法与高效的注意力计算兼容。为此，我们提出了DART（基于标记重复性的剪枝），该方法基于标记之间的重复性来剪枝标记，从而实现了显著且无需训练的加速。具体而言，DART 选择了一组核心标记作为基准，并保留与这些基准标记重复性较低的标记，以确保标记剪枝过程中的信息损失最小。实验表明，DART 可以剪枝 88.9% 的视觉标记，同时维持相当的性能，分别在总时间阶段和预填充阶段实现 1.99 倍和 2.99 倍的速度提升，并且与高效的注意力操作符具有良好的兼容性。我们的代码已发布于此 <please replace with actual URL>。', 'title_zh': '停止在多模态语言模型中寻找重要标记：重复更为关键'}
{'arxiv_id': 'arXiv:2502.11493', 'title': 'DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens', 'authors': 'Shaoshen Chen, Yangning Li, Zishan Xu, Yinghui Li, Xin Su, Zifei Shan, Hai-tao Zheng', 'link': 'https://arxiv.org/abs/2502.11493', 'abstract': "Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM's intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.", 'abstract_zh': '大型语言模型（LLMs）在处理长上下文输入时面临计算效率低下和冗余处理的问题，这促使了对压缩技术的关注。尽管现有基于语义向量的压缩方法取得了令人鼓舞的效果，但这些方法并未考虑到上下文片段之间的固有信息密度差异，而是均匀分配软令牌到各个上下文片段。这种均匀分配不可避免地减少了对信息关键区域的分配。为了解决这一问题，我们提出了一种简单而有效的动态分配软令牌（DAST）方法，该方法利用LLM对上下文相关性的内在理解来引导压缩。DAST结合基于困惑度的局部信息与基于注意力的全局信息，动态分配软令牌到信息丰富的片段，从而使压缩既有效又具有上下文意识。在多个基准测试中的实验结果表明，DAST超越了现有最先进的方法。', 'title_zh': 'DAST：通过动态分配软令牌实现的基于上下文的LLM压缩'}
{'arxiv_id': 'arXiv:2502.11514', 'title': 'Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study', 'authors': 'Yujie Lin, Ante Wang, Moye Chen, Jingyao Liu, Hao Liu, Jinsong Su, Xinyan Xiao', 'link': 'https://arxiv.org/abs/2502.11514', 'abstract': 'Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks. While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored. In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap. To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains. Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms. Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking. Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications. We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field.', 'abstract_zh': '近期，推理时间的链式思考（CoT）缩放已被证明是解决多模态推理任务的一种有前途的方法。虽然现有研究主要集中在基于文本的思考上，但将视觉和文本模态同时纳入推理过程仍是一个未被探索的领域。在本研究中，我们首次探索多模态思考的推理时间缩放，旨在弥合这一空白。为了进行全面分析，我们系统地研究了10个涵盖不同领域挑战性任务的常用采样和树搜索方法的推理时间缩放方法。此外，我们采用了一致性增强的验证器，确保这些方法在不同思考模式下都能有效地提供指导。结果显示，多模态思考在性能上优于传统的仅文本思考，并且结合这两种类型的思考可以促进更多样化的想法。尽管具有这些优势，多模态思考需要更高的标记消耗来处理更丰富的视觉输入，这在实际应用中引发了担忧。我们希望本文对这一研究线的优点和缺点的发现能激发未来的相关研究工作。', 'title_zh': '链式多模态思维推理时的缩放研究：一项初步探究'}
{'arxiv_id': 'arXiv:2502.11491', 'title': 'Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering', 'authors': 'Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin', 'link': 'https://arxiv.org/abs/2502.11491', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理方面展现了显著的能力。然而，在知识图谱问答任务（KGQA）中，仍然存在多跳推理的问题。现有方法依赖于实体向量匹配，但问题是抽象的，难以与具体实体对齐。因此，难以建立从目的到条件的推理路径，这导致信息丢失和冗余。为解决这一问题，受到人类逆向思维的启发，我们提出了一种名为Ontology-Guided Reverse Thinking（ORT）的新型框架，该框架从目的逆向构建到条件的推理路径。ORT包含三个关键阶段：（1）使用LLM提取目的标签和条件标签，（2）基于KG本体构建标签推理路径，（3）利用标签推理路径指导知识检索。在WebQSP和CWQ数据集上的实验表明，ORT实现了最先进的性能，并显著增强了LLMs在KGQA任务中的能力。', 'title_zh': '基于本体引导的逆向思维使大型语言模型在知识图谱问答任务中更加出色'}
{'arxiv_id': 'arXiv:2502.11508', 'title': 'Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities', 'authors': 'Changchun Liu, Kai Zhang, Junzhe Jiang, Zixiao Kong, Qi Liu, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.11508', 'abstract': 'Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.', 'abstract_zh': '中文翻译如下，符合学术规范：\n\n中文 spelling 纠正（CSC）是自然语言处理中的一个关键任务，旨在检测和修正中文文本中的拼写错误。本文综述了 CSC 的发展历程，从预训练语言模型到大规模语言模型，并对其在这方面的强弱进行了批判性分析。此外，我们还详细分析了现有的基准数据集，突显了它们固有的挑战和局限性。最后，我们提出了未来研究的有希望的方向，特别是利用大规模语言模型及其推理能力以提高 CSC 性能。据我们所知，这是首次专门针对 CSC 领域的综合性综述。我们认为这项工作将为研究人员提供有价值的资源，促进对该领域的更深层次理解，并激发未来的进步。', 'title_zh': '中文标题：汉语拼写纠错：进展、挑战与机遇综述\n\n如果需要完整的论文摘要或内容翻译，请提供具体段落或内容，我将为您进行翻译并确保符合学术规范。'}
{'arxiv_id': 'arXiv:2502.11501', 'title': 'Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?', 'authors': 'Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2502.11501', 'abstract': 'Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.', 'abstract_zh': '多模态大语言模型（MLLMs）在跨模态理解和生成方面表现出色，但在推断成本方面仍存在严重问题。最近，有大量的研究提出了通过标记修剪的方法来解决这一问题，这些方法识别出MLLM中的冗余标记并对其进行修剪，从而减少计算和KV存储成本，并显著提高推断速度，无需重新训练。尽管这些方法声称可以获得效率提升，但关于其基本设计和评估的关键问题仍然没有得到解答：为什么许多现有方法的表现甚至不如随机选择标记的方法？基于注意力得分的冗余标记识别是否足够可靠？在标记修剪过程中，语言信息是否真的有所帮助？如何在标记的重要性与重复之间做出一个好的权衡？当前的评估标准是否全面且公正？以往研究对这些问题的忽视阻碍了标记修剪方法的长期发展。本文依次回答了这些问题，为未来标记修剪方法的设计提供了见解。', 'title_zh': '多模态大型语言模型中的令牌裁剪：我们正在解决正确的问题吗？'}
{'arxiv_id': 'arXiv:2502.11495', 'title': 'Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models', 'authors': 'Masahiro Kaneko, Alham Fikri Aji, Timothy Baldwin', 'link': 'https://arxiv.org/abs/2502.11495', 'abstract': 'Multilingual large language models (MLLMs) are able to leverage in-context learning (ICL) to achieve high performance by leveraging cross-lingual knowledge transfer without parameter updates. However, their effectiveness is highly sensitive to example selection, particularly in multilingual settings. Based on the findings of existing work, three key factors influence multilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3) language-specific performance. However, existing approaches address these factors independently, without explicitly disentangling their combined impact, leaving optimal example selection underexplored. To address this gap, we propose balanced multi-factor ICL (\\textbf{BMF-ICL}), a method that quantifies and optimally balances these factors for improved example selection. Experiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL outperforms existing methods. Further analysis highlights the importance of incorporating all three factors and the importance of selecting examples from multiple languages.', 'abstract_zh': '多语言大规模语言模型（Multilingual Large Language Models, MLLMs）能够通过内省式学习（In-Context Learning, ICL）利用跨语言知识转移来实现高性能，而无需更新参数。然而，它们的效果对示例选择的高度敏感，特别是在多语言设置中。现有研究发现，三个关键因素影响多语言ICL的作用：（1）语义相似性，（2）语言对齐，（3）语言特定性能。然而，现有方法是独立地处理这些因素，没有明确区分它们的综合影响，导致最优示例选择仍处于未开发状态。为解决这一问题，我们提出了均衡多因素ICL（Balanced Multi-Factor ICL, BMF-ICL）方法，该方法量化并最优平衡这些因素以改进示例选择。实验结果显示，在mCSQA和TYDI等四个MLLMs的数据集上，BMF-ICL优于现有方法。进一步分析强调了综合考虑所有三个因素的重要性以及从多种语言中选择示例的重要性。', 'title_zh': '多语言大型语言模型的平衡多因素上下文学习'}
{'arxiv_id': 'arXiv:2502.11494', 'title': 'Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More', 'authors': 'Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2502.11494', 'abstract': 'Vision tokens in multimodal large language models often dominate huge computational overhead due to their excessive length compared to linguistic modality. Abundant recent methods aim to solve this problem with token pruning, which first defines an importance criterion for tokens and then prunes the unimportant vision tokens during inference. However, in this paper, we show that the importance is not an ideal indicator to decide whether a token should be pruned. Surprisingly, it usually results in inferior performance than random token pruning and leading to incompatibility to efficient attention computation this http URL, we propose DART (Duplication-Aware Reduction of Tokens), which prunes tokens based on its duplication with other tokens, leading to significant and training-free acceleration. Concretely, DART selects a small subset of pivot tokens and then retains the tokens with low duplication to the pivots, ensuring minimal information loss during token pruning. Experiments demonstrate that DART can prune 88.9% vision tokens while maintaining comparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in total time and prefilling stage, respectively, with good compatibility to efficient attention operators. Our codes are available at this https URL.', 'abstract_zh': '多模态大型语言模型中的视觉标记往往由于其长度远超过语言模态，导致巨大的计算开销。近期许多方法试图通过标记修剪来解决这一问题，首先定义一个标记的重要性标准，然后在推理过程中修剪不重要的视觉标记。然而，在本文中，我们展示了标记的重要性并不是决定是否修剪标记的理想指标。令人惊讶的是，这种方法通常会导致性能劣于随机修剪标记，并且不兼容高效的注意力计算。为了解决这一问题，我们提出了 DART（基于重复性的标记修剪，Duplication-Aware Reduction of Tokens），它根据标记与其他标记的重复性来修剪标记，从而实现显著且无需训练的加速。具体而言，DART 选择一部分关键标记作为枢纽，然后保留与这些枢纽标记重复性低的标记，以确保标记修剪过程中的最小信息损失。实验结果表明，DART 能在保持相当性能的情况下修剪 88.9% 的视觉标记，分别在总时间和填充阶段实现 1.99 倍和 2.99 倍的加速，且与高效的注意力操作具有良好的兼容性。我们的代码已在此处提供：[提供的链接]。', 'title_zh': '停止在多模态语言模型中寻找重要token：重复更为重要'}
{'arxiv_id': 'arXiv:2502.11476', 'title': 'FastMCTS: A Simple Sampling Strategy for Data Synthesis', 'authors': 'Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, Qipeng Guo', 'link': 'https://arxiv.org/abs/2502.11476', 'abstract': 'Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.', 'abstract_zh': '合成的高质量多步推理数据可以显著提升大型语言模型在各种任务上的性能。然而，现有大多数方法依赖于拒绝抽样，该方法独立生成轨迹，导致效率低下并且在不同难度问题上的采样不平衡。在本项工作中，我们提出了一种名为FastMCTS的创新数据合成策略，该策略灵感来源于蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）。FastMCTS 提供了一种更高效的多步推理数据采样方法，能够提供步骤级别的评估信号，并促进不同难度问题上的采样平衡。在英文和中文推理数据集上的实验表明，随着生成的令牌数量增加，FastMCTS生成的正确推理路径多出30%以上。此外，在相同的合成数据预算下，使用FastMCTS生成数据训练的模型在多个基准测试中比使用拒绝抽样数据训练的模型性能高出3.9%。作为一种轻量级的采样策略，FastMCTS 提供了一种实用且高效的高性能推理数据合成选择。我们的代码即将发布。', 'title_zh': 'FastMCTS：一种简单的数据合成采样策略'}
{'arxiv_id': 'arXiv:2502.11493', 'title': 'DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens', 'authors': 'Shaoshen Chen, Yangning Li, Zishan Xu, Yinghui Li, Xin Su, Zifei Shan, Hai-tao Zheng', 'link': 'https://arxiv.org/abs/2502.11493', 'abstract': "Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM's intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.", 'abstract_zh': '大语言模型（LLMs）在处理长上下文输入时面临计算效率低下和冗余处理的问题，这促使人们关注压缩技术。尽管现有的基于语义向量的压缩方法取得了令人瞩目的性能，但这些方法未能考虑到上下文片段之间固有的信息密度差异，而是均匀分配软标记到各个上下文片段中。这种均匀分布不可避免地减少了对信息关键区域的分配。为了解决这一问题，我们提出了一种简单而有效的动态软标记分配方法（Dynamic Allocation of Soft Tokens, DAST），该方法利用LLM对上下文相关性的内在理解来指导压缩过程。DAST 结合基于困惑度的局部信息与基于注意力的全局信息，动态地将软标记分配给信息丰富的片段，从而实现有效的、上下文感知的压缩。在多个基准测试上的实验结果表明，DAST 超越了现有的先进方法。', 'title_zh': 'DAST：通过动态分配软令牌实现的基于上下文的LLM压缩'}
{'arxiv_id': 'arXiv:2502.11491', 'title': 'Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering', 'authors': 'Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin', 'link': 'https://arxiv.org/abs/2502.11491', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理方面展现了显著的能力。然而，在知识图谱问答任务（KGQA）中，仍然存在需要进行多跳推理的问题。现有方法依赖于实体向量匹配，但问题的目的往往是抽象的，难以与特定实体匹配，这使得难以建立通往目的的推理路径，从而导致信息丢失和冗余。为了解决这一问题，受人类逆向思维的启发，我们提出了一种新颖的框架——基于本体的逆向思维（Ontology-Guided Reverse Thinking, ORT），该框架从目的回溯到条件来构建推理路径。ORT 在三个关键阶段进行操作：（1）使用LLMs提取目的标签和条件标签；（2）根据知识图谱本体构建标签推理路径；（3）使用标签推理路径指导知识检索。在WebQSP和CWQ数据集上的实验表明，ORT 达到了最先进的性能，并显著增强了LLMs在KGQA中的能力。', 'title_zh': '基于本体引导的逆向思考使大型语言模型在知识图谱问答任务中更为强大'}
{'arxiv_id': 'arXiv:2502.11471', 'title': 'GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion', 'authors': 'Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11471', 'abstract': 'Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning this http URL, we combine iGT with an LLM that takes KG language prompts as this http URL extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.', 'abstract_zh': '知识图谱补全（KGC），其目标是从知识图谱中推断缺失或不完整的信息，对于知识图谱而言是一个关键任务。然而，将知识图谱的重要结构信息整合到大型语言模型（LLMs）中，并以确定性方式输出预测结果仍面临挑战。为了解决这一问题，我们提出了一种名为GLTW的新方法，该方法通过编码知识图谱的结构信息，并将其与大型语言模型融合以提高KGC性能。具体而言，我们引入了一种改进的图变换器（iGT），它可以有效地编码具有局部和全局结构信息的子图，并继承语言模型的特性，从而避免从头开始训练。此外，我们还开发了一种基于子图的多分类训练目标，使用知识图谱中的所有实体作为分类对象，以增强对这种信息的学习。实验方面，我们将iGT与接受知识图谱语言提示的大型语言模型结合使用。在各种知识图谱数据集上的广泛实验表明，GLTW在与最新标准基线相比时获得了显著的性能提升。', 'title_zh': 'GLTW：通过三词语言联合改进的图变压器和大语言模型知识图谱补全'}
{'arxiv_id': 'arXiv:2502.11469', 'title': 'If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?', 'authors': 'Ryo Yoshida, Shinnosuke Isono, Kohei Kajikawa, Taiga Someya, Yushi Sugimito, Yohei Oseki', 'link': 'https://arxiv.org/abs/2502.11469', 'abstract': "Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on Transformer architectures that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that word-level factors alone cannot fully account for. In this study, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between model behavior and human processing difficulty. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.", 'abstract_zh': '计算心理语言学的近期研究揭示了注意力机制与人类记忆检索之间有趣的相似之处，主要集中在操作于词元级表示的Transformer架构上。然而，计算心理语言学研究还表明，句法结构为人类句子处理提供了单词级因素无法完全解释的有力解释。本研究旨在探讨是否 Transformer Grammar (TG) 的注意力机制，该机制唯一地基于句法结构作为表征单元，可以作为一种人类记忆检索的认知模型，使用规范化注意力熵（Normalized Attention Entropy, NAE）作为模型行为和人类处理难度之间的联系假说。我们的实验表明，TG 的注意力在预测自主阅读时间方面优于标准的 Transformer 的注意力，进一步的分析还揭示了两种模型各自的独立贡献。这些发现表明，人类句子处理涉及双重记忆表征——一种基于句法结构，另一种基于词元序列，而注意力机制则作为通用检索算法发挥作用，同时强调将句法结构作为表征单元纳入的重要性。', 'title_zh': '如果注意力机制是人类记忆检索的认知模型，那么合理的记忆表示形式是什么？'}
{'arxiv_id': 'arXiv:2502.11476', 'title': 'FastMCTS: A Simple Sampling Strategy for Data Synthesis', 'authors': 'Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, Qipeng Guo', 'link': 'https://arxiv.org/abs/2502.11476', 'abstract': 'Synthetic high-quality multi-step reasoning data can significantly enhance the performance of large language models on various tasks. However, most existing methods rely on rejection sampling, which generates trajectories independently and suffers from inefficiency and imbalanced sampling across problems of varying difficulty. In this work, we introduce FastMCTS, an innovative data synthesis strategy inspired by Monte Carlo Tree Search. FastMCTS provides a more efficient sampling method for multi-step reasoning data, offering step-level evaluation signals and promoting balanced sampling across problems of different difficulty levels. Experiments on both English and Chinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more correct reasoning paths compared to rejection sampling as the number of generated tokens scales up. Furthermore, under comparable synthetic data budgets, models trained on FastMCTS-generated data outperform those trained on rejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight sampling strategy, FastMCTS offers a practical and efficient alternative for synthesizing high-quality reasoning data. Our code will be released soon.', 'abstract_zh': '合成的高品质多步推理数据可以显著提升大型语言模型在各种任务上的性能。然而，现有方法大多依赖于拒绝采样，这种方法独立生成轨迹，导致效率低下，并且在不同难度的问题上采样不平衡。在本研究中，我们提出了一种名为FastMCTS的创新数据合成策略，该策略受到了蒙特卡罗树搜索（MCTS）的启发。FastMCTS 提供了一种更高效的多步推理数据采样方法，可以提供步骤级的评估信号，并促进不同难度级别的问题上更加平衡的采样。在英语和中文推理数据集上的实验表明，当生成的令牌数量增加时，FastMCTS生成的正确推理路径比拒绝采样多出30%以上。此外，在相同的合成数据预算下，使用FastMCTS生成的数据训练的模型在多个基准测试上的性能优于使用拒绝采样数据训练的模型，高出3.9%。作为一种轻量级的采样策略，FastMCTS 为合成高质量推理数据提供了一种实用且高效的替代方案。我们很快将发布代码。', 'title_zh': 'FastMCTS：一种简单的数据合成采样策略'}
{'arxiv_id': 'arXiv:2502.11460', 'title': 'UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance', 'authors': 'Yichuan Ma, Yunfan Shao, Peiji Li, Demin Song, Qipeng Guo, Linyang Li, Xipeng Qiu, Kai Chen', 'link': 'https://arxiv.org/abs/2502.11460', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet code generation remains a major challenge. Current approaches for obtaining high-quality code data primarily focus on (i) collecting large-scale pre-training data and (ii) synthesizing instruction data through prompt engineering with powerful models. While pre-training data faces quality consistency issues, instruction-based synthesis suffers from limited instruction diversity and inherent biases of LLMs. To address this gap, we introduce UnitCoder, a systematic pipeline leveraging model-generated unit tests to both guide and validate the code generation process. Combined with large-scale package-based retrieval from pre-training corpus, we generate a dataset of 500K+ verifiable programs containing diverse API calls. Evaluations on multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that models fine-tuned on our synthetic data exhibit consistent performance improvements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and 28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work presents a scalable approach that leverages model-generated unit tests to guide the synthesis of high-quality code data from pre-training corpora, demonstrating the potential for producing diverse and high-quality post-training data at scale. All code and data will be released (this https URL).', 'abstract_zh': '大型语言模型（LLMs）在各种任务中展现出了卓越的能力，然而代码生成仍然是一个重大挑战。当前获取高质量代码数据的主要方法包括（i）收集大规模预训练数据和（ii）通过强模型的提示工程合成指令数据。尽管预训练数据面临数据质量一致性的问题，指令驱动的合成则受限于指令多样性不足和LLMs固有的偏差。为解决这一问题，我们引入了UnitCoder，一个系统化的管道，利用模型生成的单元测试来指导和验证代码生成过程。结合大规模基于包的检索，我们生成了一个包含超过50万可验证程序的数据集，这些程序包含了多样化的API调用。在多个Python基准测试（BigCodeBench、HumanEval、MBPP）上的评估表明，使用我们合成数据微调的模型在性能上表现出一致的提升。值得注意的是，Llama3.1-8B和InternLM2.5-7B在BigCodeBench上的成功率分别从31%和28%提高到40%和39%。我们的工作提出了一种可扩展的方法，通过利用模型生成的单元测试来指导从预训练语料库生成高质量代码数据，展示了大规模生成多样化和高质量后训练数据的潜力。所有代码和数据均将会公开发布（链接将在本文末尾提供）。\n\n请通过以下链接获取更多详细信息：[链接](https://yourlinkhere.com)', 'title_zh': 'UnitCoder：带有单元测试指导的可扩展迭代代码合成'}
{'arxiv_id': 'arXiv:2502.11457', 'title': "Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition", 'authors': 'Guanlin Li, Yuki Arase, Noel Crespi', 'link': 'https://arxiv.org/abs/2502.11457', 'abstract': "Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than $20\\%$ compared to baseline models, while maintaining high simplification quality.", 'abstract_zh': '文本简化对于提高英语作为第二语言（ESL）学习者的可访问性和理解能力至关重要。本研究在此基础上进一步探讨，旨在通过简化来促进 ESL 学习者的语言习得。具体而言，我们提议将复杂的句子简化到适合学习者的适当水平，并在简化过程中增加目标水平词汇的覆盖率。我们通过在大型语言模型上进行强化学习，而无需使用平行语料库来实现这一目标。该方法使用了 token 级和句子级的奖励，并通过迭代训练模型使其生成的输出引导模型寻找符合目标属性的简化假设。在 CEFR-SP 和 TurkCorpus 数据集上的实验结果显示，与基准模型相比，所提出的方法能够有效提高目标水平词汇的频率和多样性超过 20%，同时保持高质量的简化效果。', 'title_zh': '将句子简化与 ESL 学习者的语言水平对齐以促进语言习得'}
{'arxiv_id': 'arXiv:2502.11471', 'title': 'GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion', 'authors': 'Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11471', 'abstract': 'Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning this http URL, we combine iGT with an LLM that takes KG language prompts as this http URL extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.', 'abstract_zh': '知识图谱补全（KGC），旨在推断缺失或不完整的关系，是知识图谱中的关键任务。然而，将知识图谱的关键结构信息整合到大型语言模型（LLMs）中，并使其能够确定性地输出预测结果仍然是一个难题。为了解决这一问题，我们提出了一种名为GLTW的新方法，该方法编码知识图谱的结构信息并将其与LLMs结合以提高KGC的性能。具体而言，我们引入了一种改进的图变换器（iGT），它可以有效地编码具有局部和全局结构信息的子图，并且继承了语言模型的特性，无需从零开始训练。此外，我们开发了一个基于子图的多分类训练目标，将知识图谱中的所有实体作为分类对象，以增强这方面的能力。最后，我们将iGT与一个以知识图谱语言提示为输入的LLM相结合。在多种知识图谱数据集上的广泛实验表明，GLTW在性能上显著优于当前最佳基线方法。', 'title_zh': 'GLTW：基于三词语言的图变压器与大型语言模型联合改进的知识图谱补全'}
{'arxiv_id': 'arXiv:2502.11469', 'title': 'If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?', 'authors': 'Ryo Yoshida, Shinnosuke Isono, Kohei Kajikawa, Taiga Someya, Yushi Sugimito, Yohei Oseki', 'link': 'https://arxiv.org/abs/2502.11469', 'abstract': "Recent work in computational psycholinguistics has revealed intriguing parallels between attention mechanisms and human memory retrieval, focusing primarily on Transformer architectures that operate on token-level representations. However, computational psycholinguistic research has also established that syntactic structures provide compelling explanations for human sentence processing that word-level factors alone cannot fully account for. In this study, we investigate whether the attention mechanism of Transformer Grammar (TG), which uniquely operates on syntactic structures as representational units, can serve as a cognitive model of human memory retrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis between model behavior and human processing difficulty. Our experiments demonstrate that TG's attention achieves superior predictive power for self-paced reading times compared to vanilla Transformer's, with further analyses revealing independent contributions from both models. These findings suggest that human sentence processing involves dual memory representations -- one based on syntactic structures and another on token sequences -- with attention serving as the general retrieval algorithm, while highlighting the importance of incorporating syntactic structures as representational units.", 'abstract_zh': '计算语义学中的 recent 研究揭示了注意力机制与人类记忆检索之间的有趣类比，主要集中在处理标记级表示的 Transformer 架构上。然而，计算语义学研究还证实，句法结构为人类句子处理提供了比单纯词语级因素更为有力的解释。在本研究中，我们探讨 Transformer 语法（TG）中的注意力机制——这是一种独一无二地以句法结构为表征单位运行的机制——是否可以作为人类记忆检索的认知模型，使用归一化注意力熵（NAE）作为模型行为与人类处理难度之间的联系假设。我们的实验表明，与vanilla Transformer相比，TG的注意力在预测自定速阅读时间方面表现出更高的预测能力，进一步分析揭示了两种模型的独立贡献。这些发现表明，人类句子处理涉及双重记忆表示——一种基于句法结构，另一种基于标记序列——注意力机制作为一般检索算法，同时还突显了将句法结构纳入表征单位的重要性。', 'title_zh': '如果注意力是人类记忆检索的认知模型，那么合理的记忆表示是什么？'}
{'arxiv_id': 'arXiv:2502.11460', 'title': 'UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance', 'authors': 'Yichuan Ma, Yunfan Shao, Peiji Li, Demin Song, Qipeng Guo, Linyang Li, Xipeng Qiu, Kai Chen', 'link': 'https://arxiv.org/abs/2502.11460', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet code generation remains a major challenge. Current approaches for obtaining high-quality code data primarily focus on (i) collecting large-scale pre-training data and (ii) synthesizing instruction data through prompt engineering with powerful models. While pre-training data faces quality consistency issues, instruction-based synthesis suffers from limited instruction diversity and inherent biases of LLMs. To address this gap, we introduce UnitCoder, a systematic pipeline leveraging model-generated unit tests to both guide and validate the code generation process. Combined with large-scale package-based retrieval from pre-training corpus, we generate a dataset of 500K+ verifiable programs containing diverse API calls. Evaluations on multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that models fine-tuned on our synthetic data exhibit consistent performance improvements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and 28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work presents a scalable approach that leverages model-generated unit tests to guide the synthesis of high-quality code data from pre-training corpora, demonstrating the potential for producing diverse and high-quality post-training data at scale. All code and data will be released (this https URL).', 'abstract_zh': '大型语言模型（LLMs）在各种任务中展现了卓越的能力，但代码生成仍是主要挑战。当前获取高质量代码数据的方法主要集中在（i）收集大规模预训练数据和（ii）通过强大的模型进行提示工程以合成指令数据。虽然预训练数据面临质量一致性问题，但基于指令的合成则受限于指令多样性的有限性和LLMs自身的固有偏差。为解决这一问题，我们引入了UnitCoder，这是一种系统化的管道，利用模型生成的单元测试来指导并验证代码生成过程。结合大规模包基础检索，我们生成了一个包含50多万可验证程序的数据集，这些程序包含多样化的API调用。在多个Python基准测试（BigCodeBench、HumanEval、MBPP）上的评估表明，我们合成数据微调的模型在性能上表现出一致的改进。值得注意的是，Llama3.1-8B和InternLM2.5-7B在BigCodeBench上的成功率分别从31%和28%提高到40%和39%。我们的工作提供了一种可扩展的方法，利用模型生成的单元测试来指导从预训练语料库合成高质量代码数据的过程，展示了生产多样化和高质量后训练数据的潜力。所有代码和数据将开源（详见链接：this https URL）。', 'title_zh': 'UnitCoder：带有单元测试指导的可扩展迭代代码合成'}
{'arxiv_id': 'arXiv:2502.11457', 'title': "Aligning Sentence Simplification with ESL Learner's Proficiency for Language Acquisition", 'authors': 'Guanlin Li, Yuki Arase, Noel Crespi', 'link': 'https://arxiv.org/abs/2502.11457', 'abstract': "Text simplification is crucial for improving accessibility and comprehension for English as a Second Language (ESL) learners. This study goes a step further and aims to facilitate ESL learners' language acquisition by simplification. Specifically, we propose simplifying complex sentences to appropriate levels for learners while also increasing vocabulary coverage of the target level in the simplifications. We achieve this without a parallel corpus by conducting reinforcement learning on a large language model. Our method employs token-level and sentence-level rewards, and iteratively trains the model on its self-generated outputs to guide the model to search for simplification hypotheses that satisfy the target attributes. Experiment results on CEFR-SP and TurkCorpus datasets show that the proposed method can effectively increase the frequency and diversity of vocabulary of the target level by more than $20\\%$ compared to baseline models, while maintaining high simplification quality.", 'abstract_zh': '文本简化对于提高英语作为第二语言（ESL）学习者的可访问性和理解能力至关重要。本研究在此基础上，旨在通过简化来促进ESL学习者的语言习得。具体而言，我们提出简化复杂句子到适合学习者的适当水平，并在简化过程中增加目标水平词汇的覆盖率。我们在没有平行语料库的情况下，通过在大型语言模型上进行强化学习来实现这一点。该方法采用词级和句级奖励，并通过迭代训练模型自动生成的输出，引导模型寻找满足目标属性的简化假设。在CEFR-SP和TurkCorpus数据集上的实验结果显示，与基线模型相比，所提出的方法能够有效提高目标水平词汇出现的频率和多样性超过20%，同时保持高质量的简化效果。', 'title_zh': '将句子简化与 ESL 学习者的 proficiency 相结合以促进语言习得'}
{'arxiv_id': 'arXiv:2502.11454', 'title': 'UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization', 'authors': 'Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.11454', 'abstract': 'Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty. Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.', 'abstract_zh': '人类偏好在衡量大规模语言模型和引导其与人类价值观保持一致方面扮演着重要角色。然而，当前基于比较的评价（CBE）方法通常侧重于单一的优化目标，未能有效地利用稀缺而宝贵的意见信号。为了解决这一问题，我们深入研究了可以提升CBE准确度、收敛性和可扩展性的关键因素：抑制采样偏差、平衡不确定性下降过程以及缓解更新不确定性。根据得到的指导原则，我们提出了一种统一的均匀性驱动的CBE框架——UniCBE，该框架通过构建并结合三个解耦的采样概率矩阵来同时优化这些核心目标，每个矩阵都旨在确保特定方面的均匀性。我们进一步消融了最优的元组采样和偏好聚合策略，以实现高效的CBE。在AlpacaEval基准测试中，UniCBE 在预算上节省了超过17%，同时与真实值的皮尔逊相关系数超过了0.995，显示出卓越的准确度和收敛性。在不断引入新模型的场景中，UniCBE 更能节省超过50%的评价成本，突显其改进的可扩展性。', 'title_zh': 'UniCBE：一种基于均匀性比较的统一多目标优化评价框架'}
{'arxiv_id': 'arXiv:2502.11454', 'title': 'UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization', 'authors': 'Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.11454', 'abstract': 'Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty. Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability.', 'abstract_zh': '人类偏好在衡量大型语言模型并引导其与人类价值观一致方面发挥着重要作用。不幸的是，当前基于比较的方法（CBE）通常只关注单一优化目标，无法有效利用稀缺但有价值的人类偏好信号。为解决这一问题，我们深入研究了可以提高CBE精度、收敛性和可扩展性的关键因素：抑制抽样偏差、平衡不确定性下降过程以及减轻更新不确定性。遵循这些指导原则，我们提出了一个统一的一致性驱动CBE框架——UniCBE，该框架通过构建和集成三个解耦的抽样概率矩阵，同时优化这些核心目标，确保在特定方面的一致性。此外，我们还通过完善最优元组抽样和偏好聚合策略，实现了高效的CBE。在AlpacaEval基准测试中，UniCBE在预算上节省了超过17%，而与真实值的皮尔逊相关系数超过0.995，展示了出色的准确性和收敛性。在连续引入新模型的情景下，UniCBE甚至可以节省超过50%的评估成本，凸显了其改进了的可扩展性。', 'title_zh': 'UniCBE：一种基于均匀性比较的统一多目标优化评估框架'}
{'arxiv_id': 'arXiv:2502.11451', 'title': 'From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations', 'authors': 'Shenghan Wu, Yang Deng, Yimo Zhu, Wynne Hsu, Mong Li Lee', 'link': 'https://arxiv.org/abs/2502.11451', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.', 'abstract_zh': '大型语言模型（LLMs）的快速发展已经革新了情感支持对话（ESC）的生成，提供了一种具有降低费用和增强数据隐私的可扩展解决方案。本文探讨了人格在LLMs创建ESC中的作用。我们的研究利用了已经建立的心理学框架来衡量和注入人格特征到LLMs中，然后在情感支持场景中生成对话。我们进行了广泛的评估，以了解对话中人格特征的稳定性，检查生成后特征的变化及其对对话质量和策略分布的影响。实验结果揭示了几个显著的发现：1）LLMs可以推断出核心人格特征；2）情绪性和外倾性存在微妙的变化，影响了对话的动力学；3）人格特征的应用改变了情感支持策略的分布，增强了回应的相关性与同理心质量。这些发现凸显了人格驱动的LLMs在构建更具个性化、同理心和有效的情感支持对话方面的潜力，这对未来AI驱动的情感支持系统的设计具有重要意义。', 'title_zh': '从角色到对话：重新审视角色对生成情感支持对话的影响力'}
{'arxiv_id': 'arXiv:2502.11451', 'title': 'From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations', 'authors': 'Shenghan Wu, Yang Deng, Yimo Zhu, Wynne Hsu, Mong Li Lee', 'link': 'https://arxiv.org/abs/2502.11451', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.', 'abstract_zh': '大型语言模型（LLMs）的迅速发展已彻底变革了情绪支持对话（ESC）的生成，提供了具有降低费用和增强数据隐私优势的可扩展解决方案。本文探讨了人格在LLMs生成ESC中的作用。我们的研究利用现有的心理学框架来衡量和注入人格特质，使LLMs能够生成情感支持场景中的对话。我们进行了广泛的评估，以理解对话中的人格特质稳定性，检查生成后的人格特质变化及其对对话质量和策略分布的影响。实验结果揭示了若干重要发现：1）LLMs可以推断出核心人格特质；2）情感性和外向性存在微妙变化，影响对话动态；3）人格特质的应用改变了情绪支持策略的分布，提高了响应的相关性和同理心质量。这些发现突显了以人格为导向的LLMs在设计更个性化、更同理心且更有效的心理支持对话方面的潜力，这对于未来的人工智能驱动情感支持系统的设计具有重大意义。', 'title_zh': '从角色到对话：重新审视人物设定对大规模语言模型生成的情感支持对话的影响'}
{'arxiv_id': 'arXiv:2502.11444', 'title': 'Does RAG Really Perform Bad For Long-Context Processing?', 'authors': 'Kun Luo, Zheng Liu, Peitian Zhang, Hongjin Qian, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11444', 'abstract': "The efficient processing of long context poses a serious challenge for large language models (LLMs). Recently, retrieval-augmented generation (RAG) has emerged as a promising strategy for this problem, as it enables LLMs to make selective use of the long context for efficient computation. However, existing RAG approaches lag behind other long-context processing methods due to inherent limitations on inaccurate retrieval and fragmented contexts. To address these challenges, we introduce RetroLM, a novel RAG framework for long-context processing. Unlike traditional methods, RetroLM employs KV-level retrieval augmentation, where it partitions the LLM's KV cache into contiguous pages and retrieves the most crucial ones for efficient computation. This approach enhances robustness to retrieval inaccuracy, facilitates effective utilization of fragmented contexts, and saves the cost from repeated computation. Building on this framework, we further develop a specialized retriever for precise retrieval of critical pages and conduct unsupervised post-training to optimize the model's ability to leverage retrieved information. We conduct comprehensive evaluations with a variety of benchmarks, including LongBench, InfiniteBench, and RULER, where RetroLM significantly outperforms existing long-context LLMs and efficient long-context processing methods, particularly in tasks requiring intensive reasoning or extremely long-context comprehension.", 'abstract_zh': '将长上下文高效处理是大型语言模型（LLMs）面临的一个重大挑战。近期，检索增强生成（RAG）作为一种有前途的方法出现了，因为它使LLMs能够在高效计算中选择性地利用长上下文。然而，现有的RAG方法由于检索不准确和上下文碎片化等固有局限性，落后于其他长上下文处理方法。为解决这些挑战，我们引入了RetroLM，这是一种新颖的RAG框架，用于长上下文处理。与传统方法不同，RetroLM采用KV级检索增强，它将LLM的KV缓存划分为连续的页面，并选取最核心的页面进行高效计算。这种方法增强了对检索不准确性的鲁棒性，促进了对碎片化上下文的有效利用，并节省了重复计算的成本。在此基础上，我们进一步开发了一个专门的检索器，用于精确检索关键页面，并在无监督的后训练中优化模型利用检索信息的能力。我们在LongBench、InfiniteBench和RULER等多种基准上进行了全面评估，结果显示RetroLM在长上下文LLM和高效的长上下文处理方法中显著优于现有方法，特别是在需要大量推理或极长上下文理解的任务中。', 'title_zh': '《RAG真的不适合处理长上下文吗？》\n\n注：RAG可能指的是Retrieval-Augmented Generation，即检索增强生成，是一种用于生成任务的模型架构。在保持学术规范的前提下，进行了翻译和标点的调整，使其更符合中文的表达习惯。'}
{'arxiv_id': 'arXiv:2502.11441', 'title': 'Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning', 'authors': 'Hwan Chang, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11441', 'abstract': 'Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.', 'abstract_zh': '大规模语言模型（LLMs）在训练过程中可能存在保留未经授权或敏感信息的风险，这引发了隐私方面的担忧。LLM去学习（即去遗忘）旨在通过有选择地移除特定数据来减轻这些风险，同时保持整体模型性能。然而，目前大多数相关工作主要集中于实现有效遗忘的方法，而没有对保留集（未针对移除的目标数据部分）进行详细的分析。在本文中，我们通过实体去学习的案例研究，探讨去学习对保留集中不同子集的影响。我们引入了句法相似邻居集（Syntactically Similar Neighbor Set），这是一个与目标移除数据具有相似句法结构的查询集合，并发现该子集在去学习过程中性能下降最为显著。此外，使用该集合进行正则化不仅在具有相似句法结构的查询上保持了性能，还在其他数据子集上也取得了可比的或改进的结果。我们的研究结果表明，句法相似性可能是实现有效且实用的LLM去学习中比领域或实体关系更为关键的因素。', 'title_zh': '用于大规模语言模型卸载中哪些保持集最为重要？一个关于实体卸载的案例研究'}
{'arxiv_id': 'arXiv:2502.11439', 'title': 'An Efficient Row-Based Sparse Fine-Tuning', 'authors': 'Cen-Jhih Li, Aditya Bhaskara', 'link': 'https://arxiv.org/abs/2502.11439', 'abstract': 'Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify "important" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LoRA and its variants.', 'abstract_zh': '微调是将基础模型（如大型语言模型）适应下游任务的重要步骤。为了使这一步骤更加适用于计算预算有限的用户，开发内存和计算效率高的微调方法至关重要。稀疏微调（SFT）和低秩适应（LoRA）是两种为解决这一问题而出现的框架，并且在实践中被广泛应用。在本工作中，我们基于神经网络剪枝的思想开发了一种新的SFT框架。具体而言，我们首先使用网络剪枝中的特征重要性度量（具体采用结构剪枝方法）来识别“重要”的神经元/节点，然后通过仅在涉及这些神经元的权重上进行微调来进行训练。通过在常见语言任务上的实验，我们展示了该方法在不增加训练时间复杂度和实现复杂度的情况下，显著提高了SFT的内存效率，并且在准确率方面达到了与LoRA及其变体相当的水平。', 'title_zh': '一种高效的基于行的稀疏微调方法'}
{'arxiv_id': 'arXiv:2502.11444', 'title': 'Does RAG Really Perform Bad For Long-Context Processing?', 'authors': 'Kun Luo, Zheng Liu, Peitian Zhang, Hongjin Qian, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11444', 'abstract': "The efficient processing of long context poses a serious challenge for large language models (LLMs). Recently, retrieval-augmented generation (RAG) has emerged as a promising strategy for this problem, as it enables LLMs to make selective use of the long context for efficient computation. However, existing RAG approaches lag behind other long-context processing methods due to inherent limitations on inaccurate retrieval and fragmented contexts. To address these challenges, we introduce RetroLM, a novel RAG framework for long-context processing. Unlike traditional methods, RetroLM employs KV-level retrieval augmentation, where it partitions the LLM's KV cache into contiguous pages and retrieves the most crucial ones for efficient computation. This approach enhances robustness to retrieval inaccuracy, facilitates effective utilization of fragmented contexts, and saves the cost from repeated computation. Building on this framework, we further develop a specialized retriever for precise retrieval of critical pages and conduct unsupervised post-training to optimize the model's ability to leverage retrieved information. We conduct comprehensive evaluations with a variety of benchmarks, including LongBench, InfiniteBench, and RULER, where RetroLM significantly outperforms existing long-context LLMs and efficient long-context processing methods, particularly in tasks requiring intensive reasoning or extremely long-context comprehension.", 'abstract_zh': '处理长上下文的高效计算对大规模语言模型（LLMs）构成了严重挑战。最近，检索增强生成（RAG）作为一种有前途的策略出现了，因为它使LLMs能够有选择地利用长上下文进行高效的计算。然而，现有的RAG方法由于在检索不准确和上下文碎片化方面存在固有局限性，其性能落后于其他长上下文处理方法。为了解决这些挑战，我们引入了RetroLM，这是一种新的RAG框架，用于处理长上下文。与传统方法不同，RetroLM采用KV级检索增强，即它将LLM的KV缓存分割成连续的页面，并检索最关键的页面以实现高效的计算。这种方法增强了对检索不准确性的鲁棒性，促进了对碎片化上下文的有效利用，并节省了重复计算的成本。在此基础上，我们进一步开发了一种专门的检索器，用于精确检索关键页面，并进行了无监督的后训练以优化模型利用检索信息的能力。我们使用LongBench、InfiniteBench和RULER等多种基准进行了全面评估，其中RetroLM显著优于现有的长上下文LLMs和高效的长上下文处理方法，特别是在要求密集推理或极长上下文理解的任务中表现更为突出。', 'title_zh': 'RAG在长上下文处理中真的表现不佳吗？'}
{'arxiv_id': 'arXiv:2502.11438', 'title': 'SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL', 'authors': 'Jimin Lee, Ingeol Baek, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11438', 'abstract': 'Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.', 'abstract_zh': '文本到SQL的目的是将自然语言问题转换为可执行的SQL查询。尽管之前的方法，如骨架掩蔽选择，通过检索相似的训练示例来指导大规模语言模型（LLMs），并在模型检索类似训练示例的情况下表现出强大的性能，但在缺乏此类示例的真实世界场景中它们会面临挑战。为克服这一局限，我们提出了一个新颖的框架——自增强上下文学习与细粒度示例选择用于文本到SQL（SAFE-SQL），该框架通过生成和筛选自增强示例来改进SQL生成。SAFE-SQL 首先促使LLM生成与测试输入相关的多个文本到SQL示例。然后，SAFE-SQL 通过三个相关性评估筛选这些示例，构建高质量的上下文学习示例。利用自生成的示例，SAFE-SQL 超过了之前的零样本和少量样本文本到SQL框架，实现了更高的执行准确性。值得注意的是，我们的方法在常规方法常会失败的额外困难和未见场景中提供了额外的性能增益。', 'title_zh': 'SAFE-SQL：自我增强的上下文学习与细粒度示例选择用于文本到SQL转换'}
{'arxiv_id': 'arXiv:2502.11441', 'title': 'Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning', 'authors': 'Hwan Chang, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11441', 'abstract': 'Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.', 'abstract_zh': '大型语言模型（LLMs）有保留其训练数据中未经授权或敏感信息的风险，这引发了隐私担忧。LLM去学习旨在通过选择性地移除指定数据来减轻这些风险，同时保持整体模型性能不下降。然而，现有大多数研究集中在实现有效遗忘的方法上，并未对保留集（即未针对移除的目标数据部分）进行详细的分析。在本文中，我们通过实体去学习的案例研究，探讨了去学习对各种保留集子集的影响。我们引入了语义相似邻居集（Syntactically Similar Neighbor Set，SSNS），这是一个与目标移除数据具有相似语法结构的查询集，并表明在去学习过程中，这一子集遭受了最大的性能下降。此外，当用于正则化时，这一集合不仅在语义相似查询上保持了性能，而且在其他数据子集上也实现了可比或改进的结果。我们的结果表明，句法相似性可能是比领域或实体关系更重要的因素，这对于实现有效和实用的LLM去学习至关重要。', 'title_zh': '哪些保留集对LLM去学习至关重要？一个实体去学习案例研究\n\n解释：这里的翻译尽量准确地反映了原文的意思，并且符合学术论文的规范。"Retain Set"在这里翻译为“保留集”，"LLM"翻译为“LLM”（大型语言模型），以保持术语的专业性。"Unlearning"在这里翻译为“去学习”，这个词在去学习这个上下文中更为准确。"Entity Unlearning"翻译为“实体去学习”。'}
{'arxiv_id': 'arXiv:2502.11439', 'title': 'An Efficient Row-Based Sparse Fine-Tuning', 'authors': 'Cen-Jhih Li, Aditya Bhaskara', 'link': 'https://arxiv.org/abs/2502.11439', 'abstract': 'Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning (SFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem and have been adopted widely in practice. In this work, we develop a new SFT framework, based on ideas from neural network pruning. At a high level, we first identify "important" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning by restricting to weights involving these neurons. Using experiments on common language tasks, we demonstrate that our method significantly improves the memory efficiency of SFT without increasing training time complexity and implementation complexity, while achieving accuracy comparable to state-of-the-art methods such as LoRA and its variants.', 'abstract_zh': '适应下游任务的基础模型（如大型语言模型）时，微调是一个重要的步骤。为了使这一步骤对预算有限的用户更加易用，开发高效内存和计算的微调方法至关重要。稀疏微调（SFT）和低秩适应（LoRA）是两种用于解决这一问题的框架，并已在实践中广泛采用。在本工作中，我们开发了一种新的基于神经网络剪枝理念的SFT框架。总体来说，我们首先使用网络剪枝中的特征重要性度量（具体使用结构剪枝方法）来识别“重要”的神经元/节点，然后通过限制涉及这些神经元的权重进行微调。通过在常见语言任务上的实验，我们证明了我们的方法在不增加训练时间复杂度和实现复杂度的情况下，显著提高了SFT的内存效率，并实现了与LoRA及其变体相当的精度。', 'title_zh': '一种高效的行基稀疏微调方法'}
{'arxiv_id': 'arXiv:2502.11431', 'title': 'Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval', 'authors': 'Ze Liu, Zhengyang Liang, Junjie Zhou, Zheng Liu, Defu Lian', 'link': 'https://arxiv.org/abs/2502.11431', 'abstract': 'With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called \\textit{Visualized Information Retrieval}, or \\textbf{Vis-IR}, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called \\textbf{Screenshots}, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create \\textbf{VIRA} (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop \\textbf{UniSE} (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct \\textbf{MVRB} (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our work will be shared with the community, laying a solid foundation for this emerging field.', 'abstract_zh': '随着多模态技术的流行，人们越来越关注通过视觉形式获取有用信息。在本文中，我们正式定义了一种新的信息检索范式，称为**可视化信息检索**（Visualized Information Retrieval，简称Vis-IR），其中通过一种统一的视觉格式——称为**截图**（Screenshots）——联合表示多模态信息，如文本、图像、表格和图表，用于各种检索应用。为进一步推进Vis-IR，我们做出了三项关键贡献。首先，我们创建了名为**VIRA**（Vis-IR Aggregation）的大规模数据集，该数据集包含来自多种来源的大量截图，并精心整理成带有描述和问答格式。其次，我们开发了**UniSE**（Universal Screenshot Embeddings），这是一种检索模型家族，使截图能够查询或被跨任意数据模态的其他模态查询。最后，我们构建了**MVRB**（Massive Visualized IR Benchmark），这是一个涵盖多种任务形式和应用场景的综合基准。通过在MVRB上的广泛评估，我们强调了现有多模态检索方法的不足，并展示了UniSE所取得的显著改进。我们的工作将与社区共享，为这一新兴领域奠定坚实的基础。', 'title_zh': '任何信息仅 worth 一次截图：统一搜索与可视化信息检索'}
{'arxiv_id': 'arXiv:2502.11438', 'title': 'SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL', 'authors': 'Jimin Lee, Ingeol Baek, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11438', 'abstract': 'Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.', 'abstract_zh': '文本到SQL旨在将自然语言问题转换为可执行的SQL查询。虽然之前的方法，例如骨架掩蔽选择，通过检索类似的训练示例来指导大规模语言模型（LLMs），并取得了很强的性能，但在缺乏此类示例的实际应用场景中却遇到了困难。为克服这一局限，我们提出了自我扩充的上下文学习框架，该框架结合细粒度示例选择进行文本到SQL（SAFE-SQL），这是一种新型框架，通过生成和筛选自我扩增示例来提升SQL生成的性能。SAFE-SQL 首先提示LLM生成与测试输入相关的多个文本到SQL示例。然后，SAFE-SQL 通过三个相关性评估过滤这些示例，构建高质量的上下文学习示例。使用自我生成的示例，SAFE-SQL 超过了此前的零样本和少样本文本到SQL框架，在执行准确性上表现更佳。值得注意的是，我们的方法在常规方法往往失败的额外困难和未知场景中还提供了额外的性能增益。', 'title_zh': 'SAFE-SQL：自我增强的上下文学习方法和细粒度示例选择用于文本到SQL转换'}
{'arxiv_id': 'arXiv:2502.11427', 'title': 'Do we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models', 'authors': 'Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2502.11427', 'abstract': 'Visual instruction tuning has become the predominant technology in eliciting the multimodal task-solving capabilities of large vision-language models (LVLMs). Despite the success, as visual instructions require images as the input, it would leave the gap in inheriting the task-solving capabilities from the backbone LLMs, and make it costly to collect a large-scale dataset. To address it, we propose ViFT, a visual instruction-free fine-tuning framework for LVLMs. In ViFT, we only require the text-only instructions and image caption data during training, to separately learn the task-solving and visual perception abilities. During inference, we extract and combine the representations of the text and image inputs, for fusing the two abilities to fulfill multimodal tasks. Experimental results demonstrate that ViFT can achieve state-of-the-art performance on several visual reasoning and visual instruction following benchmarks, with rather less training data. Our code and data will be publicly released.', 'abstract_zh': '视觉指令调优已成为激发大视觉-语言模型（LVLMs）多模态任务解决能力的主导技术。尽管取得了成功，但由于视觉指令需要图像作为输入，这会使得难以从基础的大语言模型（LLM）继承任务解决能力，并且采集大规模数据集的成本较高。为解决这一问题，我们提出了一种基于视觉指令的微调框架 ViFT。在 ViFT 中，我们仅需在训练过程中使用纯文本指令和图像标题数据，分别学习任务解决能力和视觉感知能力。在推理过程中，我们提取并结合文本和图像输入的表示，以融合这两种能力来完成多模态任务。实验结果表明，与现有方法相比，ViFT 在多个视觉推理和视觉指令遵循基准测试中可以达到最先进的性能，且所需的训练数据量较少。我们的代码和数据将公开发布。', 'title_zh': '我们真的需要视觉指示吗？关于大型视觉-语言模型无视觉指示微调的研究'}
{'arxiv_id': 'arXiv:2502.11425', 'title': 'Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models', 'authors': 'Jongho Kim, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2502.11425', 'abstract': "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.", 'abstract_zh': '尽管大语言模型（LLMs）具有先进的能力，但它们的时序推理能力仍不够发达。先前的研究工作已指出这一局限性，特别是在理解事件时保持时序一致性方面。例如，模型常常混淆互斥的时序关系，比如“之前”和“之后”，并做出不一致的预测。在这项工作中，我们通过提出一种新颖的反事实提示方法来解决LLMs中的时序不一致性问题。我们的方法生成反事实问题并施加集体约束，以增强模型的一致性。我们在多个数据集上评估了我们的方法，结果表明，通过有效解决时序不一致性问题，我们在事件排序和时序常识理解方面取得了显著的改进。', 'title_zh': '相对时间理解中的一种反事实一致性提示方法'}
{'arxiv_id': 'arXiv:2502.11431', 'title': 'Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval', 'authors': 'Ze Liu, Zhengyang Liang, Junjie Zhou, Zheng Liu, Defu Lian', 'link': 'https://arxiv.org/abs/2502.11431', 'abstract': 'With the popularity of multimodal techniques, it receives growing interests to acquire useful information in visual forms. In this work, we formally define an emerging IR paradigm called \\textit{Visualized Information Retrieval}, or \\textbf{Vis-IR}, where multimodal information, such as texts, images, tables and charts, is jointly represented by a unified visual format called \\textbf{Screenshots}, for various retrieval applications. We further make three key contributions for Vis-IR. First, we create \\textbf{VIRA} (Vis-IR Aggregation), a large-scale dataset comprising a vast collection of screenshots from diverse sources, carefully curated into captioned and question-answer formats. Second, we develop \\textbf{UniSE} (Universal Screenshot Embeddings), a family of retrieval models that enable screenshots to query or be queried across arbitrary data modalities. Finally, we construct \\textbf{MVRB} (Massive Visualized IR Benchmark), a comprehensive benchmark covering a variety of task forms and application scenarios. Through extensive evaluations on MVRB, we highlight the deficiency from existing multimodal retrievers and the substantial improvements made by UniSE. Our work will be shared with the community, laying a solid foundation for this emerging field.', 'abstract_zh': '随着多模态技术的流行，人们越来越关注从视觉形式中获取有用信息。在此项工作中，我们正式定义了一种新兴的检索范式，称为**可视化信息检索**（Visualized Information Retrieval，简称Vis-IR），其中，诸如文本、图像、表格和图表等多种模态信息通过一种统一的视觉格式——快照（Screenshots）联合表示，应用于各种检索应用。我们进一步为Vis-IR做出了三个关键贡献。首先，我们创建了**VIRA**（Vis-IR Aggregation），这是一个大规模数据集，包含来自多种来源的海量快照，并精心整理为带有描述和问答对的形式。其次，我们开发了**UniSE**（Universal Screenshot Embeddings），这是一种检索模型家族，使得快照可以在任意数据模态下进行查询或被查询。最后，我们构建了**MVRB**（Massive Visualized IR Benchmark），这是一个全面的基准测试，涵盖了多种任务形式和应用场景。通过在MVRB上的广泛评估，我们强调了现有多模态检索方法的不足，并展示了UniSE带来的显著改进。我们的工作将与社区分享，为这一新兴领域奠定坚实的基础。', 'title_zh': '任何信息仅值得一个截图：统一搜索与可视化信息检索\n\n在这个翻译中，“Any Information Is Just Worth One Single Screenshot”被翻译为“任何信息仅值得一个截图”，保持了原文的简洁和冲击力。“Unifying Search With Visualized Information Retrieval”被翻译为“统一搜索与可视化信息检索”，这在学术翻译中保持了术语的专业性和准确性。'}
{'arxiv_id': 'arXiv:2502.11423', 'title': 'Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation', 'authors': 'YongHyun Jun, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11423', 'abstract': 'Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes, leading to increased entailment and contradiction instances and lower overall coherence. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother and more coherent interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.', 'abstract_zh': '将以下论文内容或标题翻译成中文，符合学术规范：\n\n将用户特定的人格特征整合到大规模语言模型（LLMs）中，个性化对话系统取得了显著进展。然而，尽管LLMs能够有效生成个性化响应，但人格情感对对话质量的影响仍处于未充分探索的状态。在本研究中，我们对使用不同极化用户特征生成的对话进行了大规模分析。实验结果表明，涉及负面极化用户生成的对话往往过度强调人格特征，导致因果关系和矛盾实例增加，整体连贯性较低。与此相反，积极极化的用户特征生成的对话则有选择地融入人格信息，从而产生更为顺畅和连贯的交互。此外，我们发现，情感较弱或中性的特征通常会导致质量较低的对话。基于这些发现，我们提出了一种明确考虑人格极性的对话生成方法，该方法结合了轮次生成策略和特征排序机制。本研究为理解LLMs对人格情感的敏感性提供了新的见解，并为开发更为稳健和细致的个性化对话系统提供了指导。', 'title_zh': '探索个性化对话生成中的角色情感敏感性'}
{'arxiv_id': 'arXiv:2502.11419', 'title': 'InsBank: Evolving Instruction Subset for Ongoing Alignment', 'authors': 'Jiayi Shi, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Huan Ren, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.11419', 'abstract': "Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMs' ongoing alignment, we introduce Instruction Bank (InsBank), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (PIBE), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability.", 'abstract_zh': '大规模语言模型（LLMs）通常会经历指令调整以增强其对齐性。近期的研究强调了高质量和多样性的指令数据比数量更为重要，突显了需要选择多样化、高质量的数据子集以降低训练成本的需求。然而，如何随着新指令数据的发展逐步演化这些选择的数据子集却缺乏充分的探索。为实现LLMs的持续对齐，我们提出了指令银行（InsBank），这是一个持续更新的资源库，集成最新的有价值的指令数据。我们进一步提出了渐进式指令银行演化（PIBE）框架，旨在有效且高效地随时间演化InsBank。PIBE采用了一种渐进的数据选择策略，以保持长期的效率，通过基于表示的多样性评分来捕捉数据点之间的关系并保留历史信息，进行综合的多样性评估。这还允许在数据选择和排序时灵活地结合多样性和质量评分。广泛的实验表明，PIBE在InsBank演化方面显著优于基线方法，并能够提取预算特定的子集，证明了其有效性和适应性。', 'title_zh': 'InsBank: 演化中的指令子集以实现持续对齐'}
{'arxiv_id': 'arXiv:2502.11427', 'title': 'Do we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models', 'authors': 'Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2502.11427', 'abstract': 'Visual instruction tuning has become the predominant technology in eliciting the multimodal task-solving capabilities of large vision-language models (LVLMs). Despite the success, as visual instructions require images as the input, it would leave the gap in inheriting the task-solving capabilities from the backbone LLMs, and make it costly to collect a large-scale dataset. To address it, we propose ViFT, a visual instruction-free fine-tuning framework for LVLMs. In ViFT, we only require the text-only instructions and image caption data during training, to separately learn the task-solving and visual perception abilities. During inference, we extract and combine the representations of the text and image inputs, for fusing the two abilities to fulfill multimodal tasks. Experimental results demonstrate that ViFT can achieve state-of-the-art performance on several visual reasoning and visual instruction following benchmarks, with rather less training data. Our code and data will be publicly released.', 'abstract_zh': '视觉指令调优已成为激发大型视觉语言模型（LVLMs）多模态任务解决能力的主要技术。尽管取得了成功，但由于视觉指令需要图像作为输入，这会导致从骨干语言模型继承任务解决能力的差距，并且收集大规模数据集的成本较高。为解决这一问题，我们提出了一种名为ViFT的视觉指令免费微调框架。在ViFT中，我们仅在训练过程中需要文本指令和图像描述数据，以分别学习任务解决能力和视觉感知能力。在推理过程中，我们提取并结合文本和图像输入的表示，将两种能力融合以完成多模态任务。实验结果表明，ViFT在多个视觉推理和视觉指令遵循基准测试中达到了最先进的性能，且所需的训练数据较少。我们的代码和数据也将公开发布。', 'title_zh': '我们真的需要视觉指令吗？向着无视觉指令的大型视觉-语言模型微调方法研究'}
{'arxiv_id': 'arXiv:2502.11405', 'title': 'LayAlign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy', 'authors': 'Zhiwen Ruan, Yixia Li, He Zhu, Longyue Wang, Weihua Luo, Kaifu Zhang, Yun Chen, Guanhua Chen', 'link': 'https://arxiv.org/abs/2502.11405', 'abstract': "Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder's output, overlooking valuable information from other layers. We propose \\aname (\\mname), a framework that integrates representations from all encoder layers, coupled with the \\attaname mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.", 'abstract_zh': '尽管大型语言模型（LLMs）是在多语言语料库上进行预训练的，但在低资源语言上的表现仍不尽如人意。最近的方法通过引入可训练参数结合多语言编码器和LLM，利用多语言编码器，但这些方法通常集中在编码器的输出上，忽略了其他层中的有价值信息。我们提出了一种名为\\aname（\\mname）的框架，该框架结合了所有编码器层的表示，并引入了\\attaname机制，以实现LLM与多语言编码器之间的逐层交互。通过在多语言推理任务上的广泛实验以及学习表示的分析表明，我们的方法在所有基准模型中表现最佳。', 'title_zh': 'LayAlign：通过分层自适应融合和对齐策略增强大规模语言模型的多语言推理能力'}
{'arxiv_id': 'arXiv:2502.11425', 'title': 'Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models', 'authors': 'Jongho Kim, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2502.11425', 'abstract': "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.", 'abstract_zh': '尽管大型语言模型（LLMs）具备先进的功能，但在处理时间推理方面仍存在不足。先前的研究已经指出了这一限制，特别是在理解事件时保持时间一致性方面。例如，模型经常混淆互斥的时间关系（如“之前”和“之后”），并作出不一致的预测。在本工作中，我们通过提出一种新的反事实提示方法应对LLMs中的时间不一致性问题。我们的方法生成反事实问题并施加集体约束，从而提升模型的一致性。我们通过在多个数据集上的评估展示了我们的方法在事件排序以及通过有效解决时间不一致性来增强时间常识理解方面的显著改进。', 'title_zh': '相对时间理解中的一种反事实一致性提示方法'}
{'arxiv_id': 'arXiv:2502.11404', 'title': 'ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Chen', 'link': 'https://arxiv.org/abs/2502.11404', 'abstract': 'Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.', 'abstract_zh': '工具学习已成为大型语言模型（LLMs）通过与外部工具的交互解决复杂现实任务的关键能力。现有方法面临诸多挑战，包括对手工构建提示的依赖、多步规划的困难以及缺乏精确的错误诊断和反思机制。我们提出了ToolCoder，这是一个新颖的框架，将工具学习重新表述为代码生成任务。受到软件工程原则的启发，ToolCoder 将自然语言查询转化为结构化的 Python 函数框架，并通过描述性的注释系统性地分解任务，使 LLM 能够利用编程范式进行复杂推理和规划。然后，ToolCoder 生成和执行函数实现以获得最终响应。此外，ToolCoder 将成功执行的函数存储在存储库中，以促进代码重用，并利用错误回溯机制进行系统调试，从而提高执行效率和鲁棒性。实验结果表明，相比于现有方法，ToolCoder 在任务完成准确性和执行可靠性方面表现出更优的性能，证实了以代码为中心的方法在工具学习中的有效性。', 'title_zh': 'ToolCoder：一种面向大规模语言模型的系统化代码赋能工具学习框架'}
{'arxiv_id': 'arXiv:2502.11423', 'title': 'Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation', 'authors': 'YongHyun Jun, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11423', 'abstract': 'Personalized dialogue systems have advanced considerably with the integration of user-specific personas into large language models (LLMs). However, while LLMs can effectively generate personalized responses, the influence of persona sentiment on dialogue quality remains underexplored. In this work, we conduct a large-scale analysis of dialogues generated using a range of polarized user profiles. Our experiments reveal that dialogues involving negatively polarized users tend to overemphasize persona attributes, leading to increased entailment and contradiction instances and lower overall coherence. In contrast, positively polarized profiles yield dialogues that selectively incorporate persona information, resulting in smoother and more coherent interactions. Furthermore, we find that personas with weak or neutral sentiment generally produce lower-quality dialogues. Motivated by these findings, we propose a dialogue generation approach that explicitly accounts for persona polarity by combining a turn-based generation strategy with a profile ordering mechanism. Our study provides new insights into the sensitivity of LLMs to persona sentiment and offers guidance for developing more robust and nuanced personalized dialogue systems.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，要符合学术规范：\n\n个人化的对话系统在将用户特定的人格特征整合到大规模语言模型（LLMs）中后取得了显著的进步。然而，尽管LLMs能够有效生成个性化响应，但人格特征的情绪对对话质量的影响仍然未被充分探索。在本研究中，我们对使用一系列极化用户画像生成的对话进行了大规模分析。实验结果表明，涉及负极化用户生成的对话往往过度强调了人格特征，导致更多的蕴含和矛盾现象，整体连贯性较低。相比之下，正极化用户画像生成的对话则是有选择地整合了人格信息，从而产生了更加流畅和连贯的互动。此外，我们发现，情感较弱或中性的用户画像通常生成质量较低的对话。基于这些发现，我们提出了一种对话生成方法，通过结合轮流生成策略与画像排序机制，明确考虑了人格特征的情绪性。本研究提供了关于LLMs对人格特征情绪敏感性的新见解，并为开发更具鲁棒性和细腻度的个性化对话系统提供了指导。', 'title_zh': '探索个性化对话生成中的角色情感敏感性'}
{'arxiv_id': 'arXiv:2502.11401', 'title': 'Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment', 'authors': 'Jingcheng Deng, Zhongtao Jiang, Liang Pang, Liwei Chen, Kun Xu, Zihao Wei, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11401', 'abstract': "A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data.", 'abstract_zh': '一种新趋势是通过对比学习将大型语言模型（LLM）用作密集文本编码器。然而，由于LLM嵌入预测下一个词的概率分布，它们本质上是生成性和分布性的，这与对比学习的要求相冲突，对比学习要求嵌入能够捕捉全文语义并通过余弦相似性进行对齐。这种不一致阻碍了充分发挥LLM预训练能力的机会，导致学习效率低下。为应对这一问题，我们提出了一种新的对比学习方法AutoRegEmbed，该方法基于嵌入条件概率分布构建，并结合了两类核心任务：信息压缩和条件分布对齐。信息压缩任务将文本编码到嵌入空间中，确保嵌入向量能够捕捉全局语义。条件分布对齐任务则通过利用嵌入的条件分布对正样本嵌入进行对齐，并同时降低从文本嵌入中生成负样本的可能性，从而实现嵌入对齐和均一性。实验结果表明，我们的方法显著优于传统的对比学习方法，并且在使用相同数量的数据时，其性能与当前最先进模型相当。', 'title_zh': '通过压缩与对齐揭示大语言模型嵌入的自回归性质'}
{'arxiv_id': 'arXiv:2502.11419', 'title': 'InsBank: Evolving Instruction Subset for Ongoing Alignment', 'authors': 'Jiayi Shi, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Huan Ren, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.11419', 'abstract': "Large language models (LLMs) typically undergo instruction tuning to enhance alignment. Recent studies emphasize that quality and diversity of instruction data are more crucial than quantity, highlighting the need to select diverse, high-quality subsets to reduce training costs. However, how to evolve these selected subsets alongside the development of new instruction data remains insufficiently explored. To achieve LLMs' ongoing alignment, we introduce Instruction Bank (InsBank), a continuously updated repository that integrates the latest valuable instruction data. We further propose Progressive Instruction Bank Evolution (PIBE), a novel framework designed to evolve InsBank effectively and efficiently over time. PIBE employs a gradual data selection strategy to maintain long-term efficiency, leveraging a representation-based diversity score to capture relationships between data points and retain historical information for comprehensive diversity evaluation. This also allows for flexible combination of diversity and quality scores during data selection and ranking. Extensive experiments demonstrate that PIBE significantly outperforms baselines in InsBank evolution and is able to extract budget-specific subsets, demonstrating its effectiveness and adaptability.", 'abstract_zh': '大型语言模型（LLMs）通常通过指令调优来增强其对齐程度。最近的研究强调高质量和多样化指令数据比数量更为重要，突显了选择多样化、高质量子集以降低训练成本的需求。然而，这些选择的子集如何随着新指令数据的发展而演变仍缺乏充分探讨。为了实现LLMs的持续对齐，我们提出了指令银行（InsBank）这一持续更新的资源库，整合最新的有价值指令数据。在此基础上，我们进一步提出了渐进式指令银行演化框架（PIBE），这是一种旨在有效、高效地随时间变化不断演化InsBank的新颖框架。PIBE采用逐步的数据选择策略以维持长期效率，并利用基于表示的多样性评分来捕捉数据点间的关系，同时保留历史信息以进行全面的多样性评估。这还允许在数据选择和排名过程中灵活地结合多样性和质量评分。大量实验表明，PIBE在InsBank演化方面显著优于基准方法，并能够提取预算特定的子集，这证明了其有效性和适应性。', 'title_zh': 'InsBank: 演化中的指令子集以实现持续对齐'}
{'arxiv_id': 'arXiv:2502.11405', 'title': 'LayAlign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy', 'authors': 'Zhiwen Ruan, Yixia Li, He Zhu, Longyue Wang, Weihua Luo, Kaifu Zhang, Yun Chen, Guanhua Chen', 'link': 'https://arxiv.org/abs/2502.11405', 'abstract': "Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder's output, overlooking valuable information from other layers. We propose \\aname (\\mname), a framework that integrates representations from all encoder layers, coupled with the \\attaname mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.", 'abstract_zh': '尽管大型语言模型（LLMs）是在多语种语料库上进行预训练的，但它们在低资源语言上的表现并不理想。近年来，一些方法通过引入连接LLM和多语种编码器的可训练参数，结合使用多语种编码器和LLM。然而，这些方法通常专注于编码器的输出，而忽略了其他层中包含的重要信息。我们提出了一种框架（\\aname / \\mname），该框架整合了所有编码器层的表示，并结合了\\attaname机制，以实现LLM与多语种编码器逐层之间的交互。在多种多语种推理任务的广泛实验以及对学习到的表示的分析中，我们的方法在多个基准方法上表现出了持续的超越。', 'title_zh': 'LayAlign：通过分层自适应融合和对齐策略增强大型语言模型的多语言推理能力'}
{'arxiv_id': 'arXiv:2502.11404', 'title': 'ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Chen', 'link': 'https://arxiv.org/abs/2502.11404', 'abstract': 'Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.', 'abstract_zh': '工具学习已发展成为大型语言模型（LLMs）通过与外部工具的交互来解决复杂现实世界任务的关键能力。现有方法面临诸多挑战，包括对手工制作提示的依赖、多步骤规划的难度以及缺乏精确的错误诊断和反馈机制。我们提出了一种名为ToolCoder的新框架，将工具学习重新定义为一个代码生成任务。受软件工程原则的启发，ToolCoder将自然语言查询转换为结构化的Python函数框架，并通过带有描述性注释的系统分解任务，从而使LLMs能够利用编码范式进行复杂的推理和规划。随后，ToolCoder生成并执行函数实现，以获得最终响应。此外，ToolCoder将成功执行的函数存储在一个代码库中，促进代码重用，并利用错误跟踪机制进行系统的调试，从而提高执行效率和鲁棒性。实验结果表明，与现有方法相比，ToolCoder在任务完成精度和执行可靠性方面表现出更优的性能，这证实了以代码为中心的方法在工具学习中的有效性。', 'title_zh': 'ToolCoder：一种基于代码的强大语言模型工具学习框架'}
{'arxiv_id': 'arXiv:2502.11400', 'title': 'Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs?', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Liwei Chen, Kun Xu, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11400', 'abstract': 'Retrieval-augmented generation (RAG) systems often suffer from performance degradation when encountering noisy or irrelevant documents, driving researchers to develop sophisticated training strategies to enhance their robustness against such retrieval noise. However, as large language models (LLMs) continue to advance, the necessity of these complex training methods is increasingly questioned. In this paper, we systematically investigate whether complex robust training strategies remain necessary as model capacity grows. Through comprehensive experiments spanning multiple model architectures and parameter scales, we evaluate various document selection methods and adversarial training techniques across diverse datasets. Our extensive experiments consistently demonstrate that as models become more powerful, the performance gains brought by complex robust training methods drop off dramatically. We delve into the rationale and find that more powerful models inherently exhibit superior confidence calibration, better generalization across datasets (even when trained with randomly selected documents), and optimal attention mechanisms learned with simpler strategies. Our findings suggest that RAG systems can benefit from simpler architectures and training strategies as models become more powerful, enabling more scalable applications with minimal complexity.', 'abstract_zh': '检索增强生成（RAG）系统在遇到噪声或无关文档时往往会表现出性能下降，这促使研究者开发复杂的训练策略以增强其对检索噪声的鲁棒性。然而，随着大型语言模型（LLMs）不断发展，这些复杂训练方法的重要性正受到越来越多的质疑。在本文中，我们系统地探讨了随着模型容量的增长，是否还需要这些复杂的鲁棒性训练策略。通过跨越多种模型架构和参数规模的全面实验，我们在多种多样的数据集上评估了各种文档选择方法和对抗训练技术。我们广泛而深入的实验结果一致表明，随着模型变得更为强大，由复杂鲁棒性训练方法带来的性能提升急剧下降。我们进一步分析了其背后的原理，发现更强大的模型本质上表现出更好的置信度校准，即使在用随机选择的文档进行训练的情况下，也能更好地泛化到不同的数据集上，并且能够学会更简化的注意力机制。我们的发现表明，随着模型变得越来越强大，RAG系统可以从更简单的架构和训练策略中受益，从而使得应用更具扩展性且减少复杂性。', 'title_zh': '重访稳健RAG：在强大语言大模型时代，我们仍然需要复杂的稳健培训吗？'}
{'arxiv_id': 'arXiv:2502.11401', 'title': 'Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment', 'authors': 'Jingcheng Deng, Zhongtao Jiang, Liang Pang, Liwei Chen, Kun Xu, Zihao Wei, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11401', 'abstract': "A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data.", 'abstract_zh': '一种新趋势是将大型语言模型（LLM）作为密集文本编码器通过对比学习实现。然而，由于LLM编码向量预测下一个标记的概率分布，它们本质上是生成性和分布性的，这与对比学习的要求相冲突，而对比学习需要编码向量捕捉全文语义并通过余弦相似度对齐。这种不一致性妨碍了充分利用LLM的预训练能力，导致学习效率低下。为解决这一问题，我们提出了AutoRegEmbed，这是一种基于条件概率分布的新型对比学习方法，它整合了两大核心任务：信息压缩和条件分布对齐。信息压缩任务将文本编码到嵌入空间中，确保嵌入向量捕捉全局语义。条件分布对齐任务通过利用嵌入的条件分布将其与正样本嵌入对齐，同时降低从文本嵌入生成负样本的可能性，从而实现嵌入的对齐和均匀性。实验结果表明，我们的方法显著优于传统的对比学习方法，并且在使用相同数据量时，达到了与最先进的模型相当的性能。', 'title_zh': '通过压缩和对齐揭示大语言模型嵌入的自回归本质'}
{'arxiv_id': 'arXiv:2502.11393', 'title': 'HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning', 'authors': 'Xiaoyuan Li, Moxin Li, Rui Men, Yichang Zhang, Keqin Bao, Wenjie Wang, Fuli Feng, Dayiheng Liu, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.11393', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.', 'abstract_zh': '大语言模型（LLMs）在常识推理方面展现了显著的能力；然而，有些问题的变化会导致错误的回答。这些模型是真正理解了常识知识，还是只是记忆了表达模式？为了探究这一问题，我们首次对LLMs在常识推理方面的鲁棒性进行了广泛评估。我们引入了HellaSwag-Pro，这是一个包含11,200个案例的大规模双语基准测试集，通过设计并整合了七种类型的问题变体。为了构造这一基准测试集，我们提出了一种两阶段方法来开发汉语HellaSwag，这是一个细粒度标注的数据集，包含了56个类别中的12,000个实例。我们在41个代表性LLM上进行了广泛的实验，结果表明这些LLM在常识推理方面并不鲁棒。此外，这种鲁棒性在测试时所使用的语言方面有所不同。这项工作建立了一个高质量的评估基准，广泛的实验提供了有关LLM在常识推理方面的宝贵见解，对社区具有重要意义。', 'title_zh': 'HellaSwag-Pro：一个大规模双语基准，用于评估语言模型在常识推理中鲁棒性的能力'}
{'arxiv_id': 'arXiv:2502.11400', 'title': 'Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs?', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Liwei Chen, Kun Xu, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11400', 'abstract': 'Retrieval-augmented generation (RAG) systems often suffer from performance degradation when encountering noisy or irrelevant documents, driving researchers to develop sophisticated training strategies to enhance their robustness against such retrieval noise. However, as large language models (LLMs) continue to advance, the necessity of these complex training methods is increasingly questioned. In this paper, we systematically investigate whether complex robust training strategies remain necessary as model capacity grows. Through comprehensive experiments spanning multiple model architectures and parameter scales, we evaluate various document selection methods and adversarial training techniques across diverse datasets. Our extensive experiments consistently demonstrate that as models become more powerful, the performance gains brought by complex robust training methods drop off dramatically. We delve into the rationale and find that more powerful models inherently exhibit superior confidence calibration, better generalization across datasets (even when trained with randomly selected documents), and optimal attention mechanisms learned with simpler strategies. Our findings suggest that RAG systems can benefit from simpler architectures and training strategies as models become more powerful, enabling more scalable applications with minimal complexity.', 'abstract_zh': '检索增强生成（RAG）系统在遇到噪声或不相关文档时常常性能下降，促使研究人员开发复杂的训练策略以增强其对检索噪声的鲁棒性。然而，随着大规模语言模型（LLMs）的不断发展，这些复杂训练方法的必要性正受到越来越多的质疑。在本文中，我们系统地研究了模型容量增加时，是否还需要这些复杂的鲁棒训练策略。我们通过涵盖多种模型架构和参数规模的全面实验，评估了各种文档选择方法和对抗训练技术在不同数据集上的表现。广泛的实验结果一致表明，随着模型变得越来越强大，复杂鲁棒训练方法所带来的性能提升急剧下降。我们深入探讨了这一现象的原因，发现更强大的模型能够表现出更好的置信度校准，对不同数据集有更好的泛化能力（即使使用随机选择的文档进行训练），并且能够通过更简单的策略学习到最合适的注意力机制。我们的研究结果表明，随着模型变得越来越强大，RAG系统可以从更简单的架构和训练策略中受益，从而实现更易于扩展的应用，同时保持较低的复杂度。', 'title_zh': '重新审视鲁棒RAG：在强大语言模型时代，我们还需要复杂的鲁棒训练吗？'}
{'arxiv_id': 'arXiv:2502.11393', 'title': 'HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning', 'authors': 'Xiaoyuan Li, Moxin Li, Rui Men, Yichang Zhang, Keqin Bao, Wenjie Wang, Fuli Feng, Dayiheng Liu, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.11393', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.', 'abstract_zh': '大型语言模型（LLMs）在常识推理方面表现出显著的能力；然而，某些问题的变化可能会引发错误的回答。这些模型是否真正理解常识知识，还是仅仅记忆了表达模式？为了探讨这一问题，我们首次对LLMs在常识推理方面的稳健性进行了全面评估。我们引入了HellaSwag-Pro，这是一个包含11,200个案例的大规模双语基准，通过设计和编译七种类型的问题变体而构建。为了构建这个基准，我们提出了一种两阶段方法来开发汉语HellaSwag，这是一个细粒度注释的数据集，涵盖56个类别，共有12,000个实例。我们在41个代表性LLM上进行了广泛的实验，发现这些LLM在常识推理方面远未表现出稳健性。此外，这种稳健性还会根据测试语言的不同而变化。本项工作建立了一个高质量的评估基准，广泛的实验为LLM在常识推理方面的社区提供了宝贵的见解。', 'title_zh': 'HellaSwag-Pro：一个大规模双语基准，用于评估LLMs在常识推理中鲁棒性的能力'}
{'arxiv_id': 'arXiv:2502.11387', 'title': 'RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following', 'authors': 'Junru Lu, Jiazheng Li, Guodong Shen, Lin Gui, Siyu An, Yulan He, Di Yin, Xing Sun', 'link': 'https://arxiv.org/abs/2502.11387', 'abstract': "Role-playing is important for Large Language Models (LLMs) to follow diverse instructions while maintaining role identity and the role's pre-defined ability limits. Existing role-playing datasets mostly contribute to controlling role style and knowledge boundaries, but overlook role-playing in instruction-following scenarios. We introduce a fine-grained role-playing and instruction-following composite benchmark, named RoleMRC, including: (1) Multi-turn dialogues between ideal roles and humans, including free chats or discussions upon given passages; (2) Role-playing machine reading comprehension, involving response, refusal, and attempts according to passage answerability and role ability; (3) More complex scenarios with nested, multi-turn and prioritized instructions. The final RoleMRC features a 10.2k role profile meta-pool, 37.9k well-synthesized role-playing instructions, and 1.4k testing samples. We develop a pipeline to quantitatively evaluate the fine-grained role-playing and instruction-following capabilities of several mainstream LLMs, as well as models that are fine-tuned on our data. Moreover, cross-evaluation on external role-playing datasets confirms that models fine-tuned on RoleMRC enhances instruction-following without compromising general role-playing and reasoning capabilities. We also probe the neural-level activation maps of different capabilities over post-tuned LLMs. Access to our RoleMRC, RoleMRC-mix and Codes: this https URL.", 'abstract_zh': '角色扮演对于大型语言模型（LLMs）在遵循多样化的指令的同时保持角色身份和预定义的能力限制至关重要。现有的角色扮演数据集主要侧重于控制角色风格和知识边界，但忽视了在指令遵循场景中的角色扮演。我们引入了一个细粒度的角色扮演和指令遵循复合基准，名为RoleMRC，包括：（1）理想角色与人类之间的多轮对话，包括基于给定段落的自由聊天或讨论；（2）角色扮演的机器阅读理解，涉及根据段落可答性和角色能力进行回应、拒绝和尝试；（3）更复杂的情景，包括嵌套、多轮和优先级指令。最终的RoleMRC包括10,200个角色简介元池、37,900个精心合成的角色扮演指令和1,400个测试样本。我们开发了一个管道，以定量评估几种主流LLM以及在我们数据集上微调的模型在细粒度角色扮演和指令遵循能力方面的表现。此外，跨外部角色扮演数据集的评估证实，基于RoleMRC微调的模型在指令遵循方面有所提升，而不会损害其通用的角色扮演和推理能力。我们还研究了不同能力在后微调LLM中的神经活动图。访问我们的RoleMRC、RoleMRC-mix和相关代码，请访问：this https URL。', 'title_zh': '角色MRC：一种细粒度的复合基准测试，用于角色扮演和指令遵循'}
{'arxiv_id': 'arXiv:2502.11380', 'title': 'Exploring the Small World of Word Embeddings: A Comparative Study on Conceptual Spaces from LLMs of Different Scales', 'authors': 'Zhu Liu, Ying Liu, KangYang Luo, Cunliang Kong, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11380', 'abstract': "A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, combined with a similarity metric, provide an effective approach to constructing such a space. Typically, embeddings are derived from traditional distributed models or encoder-only pretrained models, whose objectives directly capture the meaning of the current token. In contrast, decoder-only models, including large language models (LLMs), predict the next token, making their embeddings less directly tied to the current token's semantics. Moreover, comparative studies on LLMs of different scales remain underexplored. In this paper, we construct a conceptual space using word embeddings from LLMs of varying scales and comparatively analyze their properties. We establish a network based on a linguistic typology-inspired connectivity hypothesis, examine global statistical properties, and compare LLMs of varying scales. Locally, we analyze conceptual pairs, WordNet relations, and a cross-lingual semantic network for qualitative words. Our results indicate that the constructed space exhibits small-world properties, characterized by a high clustering coefficient and short path lengths. Larger LLMs generate more intricate spaces, with longer paths reflecting richer relational structures and connections. Furthermore, the network serves as an efficient bridge for cross-lingual semantic mapping.", 'abstract_zh': '概念空间通过将概念视为节点、语义相关性视为边来表示概念。结合词嵌入和相似性度量，可以有效构建这样的空间。通常，词嵌入是从传统的分布式模型或仅编码的预训练模型中提取的，其目标直接捕获当前词的意义。相比之下，仅解码器模型，包括大型语言模型（LLMs），预测下一个词，使得其词嵌入与当前词的意义联系较弱。此外，不同规模的LLM的对比研究尚处于探索阶段。本文使用不同规模的LLM生成的词嵌入构建概念空间，并对这些模型的特性进行比较分析。基于语言类型学启发的连接性假设，我们构建了一个网络，评估其全局统计特性，并比较不同规模的LLM。局域上，我们分析了概念对、WordNet关系以及多语言语义网络中的定性词。研究结果表明，构造的概念空间表现出小世界特性，特征为高聚类系数和短路径长度。较大的LLM生成更为复杂的空间，较长的路径反映了更丰富的关系结构和联系。此外，该网络作为跨语言语义映射的高效桥梁显示出优势。', 'title_zh': '探索词嵌入的小世界：不同规模语言模型概念空间的比较研究'}
{'arxiv_id': 'arXiv:2502.11368', 'title': 'LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing', 'authors': 'Zhengxiang Wang, Veronika Makarova, Zhi Li, Jordan Kodner, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.11368', 'abstract': 'The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.', 'abstract_zh': '本文探讨了大规模语言模型（LLMs）在多维度分析性写作评估中的性能，即其根据多种评估标准提供评分和评论的能力。我们使用了一组由二外研究生撰写的文献综述，并由专家根据9个分析性评估标准进行评估的语料库，促使几款流行的LLMs在不同条件下执行相同任务。为了评估反馈评论的质量，我们应用了一种新颖的反馈评论质量评估框架。与依赖手动判断的现有方法相比，该框架具有可解释性、低成本、可扩展性和可重复性。研究发现，LLMs能够生成合理的、通常可靠的多维度分析性评估。我们发布了该语料库以确保可重复性。', 'title_zh': '大型语言模型可以执行多维度分析性写作评估：一项关于二语研究生学术英语写作的案例研究\n\n注：\n1. "LLMs"的全称是"Large Language Models"，一般在学术文章中直接使用"大型语言模型"作为翻译。\n2. "L2"通常代表第二语言（Language 2），在翻译时可以保持原文“二语”，或者根据上下文进一步解释为“第二语言”。\n3. "Case Study"翻译为“案例研究”符合学术规范。'}
{'arxiv_id': 'arXiv:2502.11364', 'title': 'Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning', 'authors': 'Yilei Tu, Andrew Xue, Freda Shi', 'link': 'https://arxiv.org/abs/2502.11364', 'abstract': 'While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding when and why it works well.\nIn this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study show that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.', 'abstract_zh': '尽管多语言大型语言模型在高资源语言（HRL）上通常表现良好，甚至有时还能与英文的表现持平，但在低资源语言（LRL）上常常表现不佳。在几种旨在弥合这一差距的提示策略中，当目标语言示例不可用时，多语言上下文学习（Multilingual In-Context Learning, M-ICL）特别有效。然而，对于何时以及为何其表现良好，缺乏系统的理解。\n\n在本研究中，我们系统地分析了多语言ICL，利用高资源语言（HRL）的示例来增强跨语言迁移。我们发现来自混合HRL的示例在所有情况下始终优于仅用英文的示例，尤其是在低资源语言（LRL）任务中表现更为突出。令人惊讶的是，我们的消融实验表明，提示中存在无关的非英文句子也能带来可测量的提升，这表明多语言接触本身的有效性。我们的结果强调了战略性利用多语言资源以弥合代表性不足语言的性能差距的潜力。', 'title_zh': '多语言的恩赐：多语言在上下文中学习的系统分析'}
{'arxiv_id': 'arXiv:2502.11387', 'title': 'RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and Instruction-Following', 'authors': 'Junru Lu, Jiazheng Li, Guodong Shen, Lin Gui, Siyu An, Yulan He, Di Yin, Xing Sun', 'link': 'https://arxiv.org/abs/2502.11387', 'abstract': "Role-playing is important for Large Language Models (LLMs) to follow diverse instructions while maintaining role identity and the role's pre-defined ability limits. Existing role-playing datasets mostly contribute to controlling role style and knowledge boundaries, but overlook role-playing in instruction-following scenarios. We introduce a fine-grained role-playing and instruction-following composite benchmark, named RoleMRC, including: (1) Multi-turn dialogues between ideal roles and humans, including free chats or discussions upon given passages; (2) Role-playing machine reading comprehension, involving response, refusal, and attempts according to passage answerability and role ability; (3) More complex scenarios with nested, multi-turn and prioritized instructions. The final RoleMRC features a 10.2k role profile meta-pool, 37.9k well-synthesized role-playing instructions, and 1.4k testing samples. We develop a pipeline to quantitatively evaluate the fine-grained role-playing and instruction-following capabilities of several mainstream LLMs, as well as models that are fine-tuned on our data. Moreover, cross-evaluation on external role-playing datasets confirms that models fine-tuned on RoleMRC enhances instruction-following without compromising general role-playing and reasoning capabilities. We also probe the neural-level activation maps of different capabilities over post-tuned LLMs. Access to our RoleMRC, RoleMRC-mix and Codes: this https URL.", 'abstract_zh': '角色扮演对于大型语言模型（LLMs）在遵循多样化指令的同时保持角色身份和角色预先定义的能力限制至关重要。现有角色扮演数据集主要侧重于控制角色风格和知识边界，但忽视了角色扮演在指令遵循场景中的应用。我们引入了一个细粒度的角色扮演和指令遵循复合基准，名为RoleMRC，其中包括：(1) 理想角色与人类之间的多轮对话，包括基于给定段落的自由聊天或讨论；(2) 角色扮演机器阅读理解，根据段落可回答性和角色能力进行响应、拒绝和尝试；(3) 更复杂的嵌套、多轮和优先级指令情境。最终，RoleMRC 包含10.2千个角色档案元集合，37.9万个精心合成的角色扮演指令，以及1.4千个测试样本。我们开发了一个流水线来定量评估几款主流LLM及其在我们数据集上微调的模型细粒度的角色扮演和指令遵循能力。此外，通过外部角色扮演数据集的交叉评估证实，基于RoleMRC微调的模型在增强指令遵循能力的同时，不会削弱一般的角色扮演和推理能力。我们还研究了不同能力在后微调LLM中的神经活动图。访问我们的RoleMRC、RoleMRC-mix及代码：此链接 <https://this-url Häxis replace>。', 'title_zh': '角色MRC：一个细粒度综合基准，用于角色扮演和指令跟随'}
{'arxiv_id': 'arXiv:2502.11361', 'title': 'VLDBench: Vision Language Models Disinformation Detection Benchmark', 'authors': 'Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah', 'link': 'https://arxiv.org/abs/2502.11361', 'abstract': 'The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available.', 'abstract_zh': '人工智能生成内容的快速兴起使得虚假信息的检测愈发具有挑战性。特别是多模态虚假信息，即包含虚构信息的图文在线帖子，专门设计用于欺诈。虽然现有的人工智能安全基准主要关注偏差和毒性，但多模态虚假信息检测仍处于很大程度的未探索状态。为应对这一挑战，我们提出了视觉-语言虚假信息检测基准（VLDBench），这是首个全面的跨单模态（仅文本）和多模态（文本与图像）内容的虚假信息检测基准数据集，包含31,000篇新闻文章-图像配对，覆盖13个不同的类别，用于鲁棒性评估。VLDBench 配备了一个严格的半自动化数据集整理管道，领域专家贡献了超300小时的标注工作，实现了良好的注释者间一致性（Cohen kappa = 0.78）。我们对最先进的大规模语言模型（LLMs）和视觉-语言模型（VLMs）进行了广泛评估，结果显示，将文本和视觉线索结合分析多模态新闻帖子能够将虚假信息检测精度提高5-35%。VLDBench 遵循欧盟人工智能法案、NIST 指南以及 MIT AI 风险仓库2024版等人工智能治理框架，预期将成为检测在线多模态内容虚假信息的标准基准。我们的代码和数据将公开发布。', 'title_zh': 'VLDBench：视觉语言模型虚假信息检测基准'}
{'arxiv_id': 'arXiv:2502.11355', 'title': '"Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents', 'authors': 'Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu', 'link': 'https://arxiv.org/abs/2502.11355', 'abstract': "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.", 'abstract_zh': '大型语言模型（LLMs）正在转变为自主决策者，这在高风险场景中引发了关于灾难性风险的担忧，特别是在化学、生物、辐射和核武器（CBRN）领域。基于这样的风险可能源自智能体的有益性、无害性和诚实性（HHH）目标之间的权衡这一洞察，我们构建了一个新颖的三阶段评估框架，该框架精心设计以有效地自然地揭示这些风险。我们在12个先进的LLM模型中进行了14,400次智能体模拟，并进行了广泛的实验和分析。结果表明，LLM智能体可以在没有刻意诱导的情况下自主从事灾难性行为和欺骗行为。此外，更强的推理能力往往增加而不是减轻这些风险。我们还展示了这些智能体可以违反指令和高级命令。总体而言，我们通过实验证明了自主LLM智能体中确实存在灾难性风险。如果需要，我们将发布我们的代码。', 'title_zh': '“核武部署了！”：分析自主大型语言模型代理决策过程中的灾难性风险'}
{'arxiv_id': 'arXiv:2502.11380', 'title': 'Exploring the Small World of Word Embeddings: A Comparative Study on Conceptual Spaces from LLMs of Different Scales', 'authors': 'Zhu Liu, Ying Liu, KangYang Luo, Cunliang Kong, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11380', 'abstract': "A conceptual space represents concepts as nodes and semantic relatedness as edges. Word embeddings, combined with a similarity metric, provide an effective approach to constructing such a space. Typically, embeddings are derived from traditional distributed models or encoder-only pretrained models, whose objectives directly capture the meaning of the current token. In contrast, decoder-only models, including large language models (LLMs), predict the next token, making their embeddings less directly tied to the current token's semantics. Moreover, comparative studies on LLMs of different scales remain underexplored. In this paper, we construct a conceptual space using word embeddings from LLMs of varying scales and comparatively analyze their properties. We establish a network based on a linguistic typology-inspired connectivity hypothesis, examine global statistical properties, and compare LLMs of varying scales. Locally, we analyze conceptual pairs, WordNet relations, and a cross-lingual semantic network for qualitative words. Our results indicate that the constructed space exhibits small-world properties, characterized by a high clustering coefficient and short path lengths. Larger LLMs generate more intricate spaces, with longer paths reflecting richer relational structures and connections. Furthermore, the network serves as an efficient bridge for cross-lingual semantic mapping.", 'abstract_zh': '概念空间通过将概念表示为节点，将语义相关性表示为边来构建。结合词嵌入和相似度度量，可以有效地构建这种空间。通常，嵌入是从传统分布式模型或编码器预训练模型中获得的，其目标直接捕捉当前词的意义。相比之下，仅解码器模型，包括大型语言模型（LLMs），预测下一个词，使得它们的嵌入与当前词的意义联系不太直接。此外，关于不同规模LLMs的比较研究尚未充分展开。在本文中，我们使用不同规模的LLMs中的词嵌入构建概念空间，并比较分析它们的性质。我们基于基于语言类型学启发的连通性假说建立了一个网络，研究其全局统计特性，并对比不同规模的LLMs。在局部层面，我们分析概念对、WordNet关系以及跨语言语义网络中的定性词。研究结果表明，构建的概念空间具有小世界特性，表现为高聚类系数和短路径长度。更大的LLMs生成更复杂的空间，路径更长反映了更为丰富的关系结构和连接。此外，该网络作为跨语言语义映射的有效桥梁，具有高效性。', 'title_zh': '探索词嵌入的小世界：不同规模语言模型概念空间的比较研究'}
{'arxiv_id': 'arXiv:2502.11368', 'title': 'LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing', 'authors': 'Zhengxiang Wang, Veronika Makarova, Zhi Li, Jordan Kodner, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.11368', 'abstract': 'The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.', 'abstract_zh': '本文探讨了大型语言模型（LLMs）在多维度分析写作评估中的表现，即它们根据多种评估标准提供评分和评论的能力。利用由二外研究生撰写并由人类专家基于9个分析标准评估的一系列文献综述语料库，我们在不同条件下促使几款流行的LLMs完成相同任务。为评估反馈评论的质量，我们应用了一种新颖的反馈评论质量评估框架。与依赖人工判断的现有方法相比，该框架具有可解释性、成本效益、可扩展性和可复现性。研究发现，LLMs能够生成合理良好且总体可靠的多维度分析评估。我们发布了该语料库以保证可复现性。', 'title_zh': '大规模语言模型可以执行多维度分析性写作评估：针对二语研究生学术英语写作的案例研究'}
{'arxiv_id': 'arXiv:2502.11364', 'title': 'Blessing of Multilinguality: A Systematic Analysis of Multilingual In-Context Learning', 'authors': 'Yilei Tu, Andrew Xue, Freda Shi', 'link': 'https://arxiv.org/abs/2502.11364', 'abstract': 'While multilingual large language models generally perform adequately, and sometimes even rival English performance on high-resource languages (HRLs), they often significantly underperform on low-resource languages (LRLs). Among several prompting strategies aiming at bridging the gap, multilingual in-context learning (ICL) has been particularly effective when demonstration in target languages is unavailable. However, there lacks a systematic understanding when and why it works well.\nIn this work, we systematically analyze multilingual ICL, using demonstrations in HRLs to enhance cross-lingual transfer. We show that demonstrations in mixed HRLs consistently outperform English-only ones across the board, particularly for tasks written in LRLs. Surprisingly, our ablation study show that the presence of irrelevant non-English sentences in the prompt yields measurable gains, suggesting the effectiveness of multilingual exposure itself. Our results highlight the potential of strategically leveraging multilingual resources to bridge the performance gap for underrepresented languages.', 'abstract_zh': '尽管多语言大规模语言模型通常表现良好，甚至在高资源语言（HRLs）上有时还能媲美英语的表现，但在低资源语言（LRLs）上的表现往往显著不足。在几种旨在缩小这一差距的提示策略中，多语言上下文学习（ICL）特别有效，尤其是在目标语言的演示不可用的情况下。然而，目前缺乏对何时及为何它有效进行系统的理解。\n\n在本文中，我们系统地分析了多语言ICL，使用高资源语言的演示来增强跨语言迁移。我们展示了在所有任务中，混合HRLs的演示始终优于仅英语的演示，特别是在低资源语言（LRLs）的任务中更为明显。令人惊讶的是，我们的消融实验显示，提示中包含无关的非英语句子也能带来可测量的提升，这表明多语言暴露本身的有效性。本研究结果强调了战略性利用多语言资源以缩小未充分代表的语言性能差距的潜力。', 'title_zh': '多语种的福祉：一种多语种情境学习系统的分析'}
{'arxiv_id': 'arXiv:2502.11361', 'title': 'VLDBench: Vision Language Models Disinformation Detection Benchmark', 'authors': 'Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah', 'link': 'https://arxiv.org/abs/2502.11361', 'abstract': 'The rapid rise of AI-generated content has made detecting disinformation increasingly challenging. In particular, multimodal disinformation, i.e., online posts-articles that contain images and texts with fabricated information are specially designed to deceive. While existing AI safety benchmarks primarily address bias and toxicity, multimodal disinformation detection remains largely underexplored. To address this challenge, we present the Vision-Language Disinformation Detection Benchmark VLDBench, the first comprehensive benchmark for detecting disinformation across both unimodal (text-only) and multimodal (text and image) content, comprising 31,000} news article-image pairs, spanning 13 distinct categories, for robust evaluation. VLDBench features a rigorous semi-automated data curation pipeline, with 22 domain experts dedicating 300 plus hours} to annotation, achieving a strong inter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs), demonstrating that integrating textual and visual cues in multimodal news posts improves disinformation detection accuracy by 5 - 35 % compared to unimodal models. Developed in alignment with AI governance frameworks such as the EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench is expected to become a benchmark for detecting disinformation in online multi-modal contents. Our code and data will be publicly available.', 'abstract_zh': '人工智能生成内容的迅速崛起使得虚假信息的检测变得更加具有挑战性。特别是，多模态虚假信息，即包含虚假信息图像和文本的在线帖子，特别设计以诱导他人。尽管现有的AI安全基准主要关注偏见和毒性问题，但多模态虚假信息的检测仍处于很大程度上的未被探索状态。为应对这一挑战，我们提出了视觉-语言虚假信息检测基准VLDBench，这是首个涵盖单模态（仅文本）和多模态（文本和图像）内容的全面基准，包含31,000个新闻文章-图像对，涵盖了13个不同的类别，以实现稳健的评估。VLDBench具有严格的半自动化数据整理流程，22名领域专家共花费300多个小时进行标注，实现了强的注释者间一致性（Cohen’s Kappa = 0.78）。我们对最先进的大规模语言模型（LLMs）和视觉-语言模型（VLMs）进行了广泛评估，结果表明，将文本和视觉提示整合到多模态新闻帖子中可以将虚假信息检测的准确性提高5%到35%。该基准开发遵循了欧盟AI法案、NIST指南和MIT人工智能风险库2024等AI治理框架，预计将成为检测在线多模态内容中虚假信息的基准。我们的代码和数据将公开发布。', 'title_zh': 'VLDBench：视觉语言模型虚假信息检测基准'}
{'arxiv_id': 'arXiv:2502.11345', 'title': 'Hierarchical Graph Topic Modeling with Topic Tree-based Transformer', 'authors': 'Delvin Ce Zhang, Menglin Yang, Xiaobao Wu, Jiasheng Zhang, Hady W. Lauw', 'link': 'https://arxiv.org/abs/2502.11345', 'abstract': 'Textual documents are commonly connected in a hierarchical graph structure where a central document links to others with an exponentially growing connectivity. Though Hyperbolic Graph Neural Networks (HGNNs) excel at capturing such graph hierarchy, they cannot model the rich textual semantics within documents. Moreover, text contents in documents usually discuss topics of different specificity. Hierarchical Topic Models (HTMs) discover such latent topic hierarchy within text corpora. However, most of them focus on the textual content within documents, and ignore the graph adjacency across interlinked documents. We thus propose a Hierarchical Graph Topic Modeling Transformer to integrate both topic hierarchy within documents and graph hierarchy across documents into a unified Transformer. Specifically, to incorporate topic hierarchy within documents, we design a topic tree and infer a hierarchical tree embedding for hierarchical topic modeling. To preserve both topic and graph hierarchies, we design our model in hyperbolic space and propose Hyperbolic Doubly Recurrent Neural Network, which models ancestral and fraternal tree structure. Both hierarchies are inserted into each Transformer layer to learn unified representations. Both supervised and unsupervised experiments verify the effectiveness of our model.', 'abstract_zh': '文本文档通常以层次图结构相互连接，其中核心文档以指数增长的方式与其他文档相连。虽然超曲面图神经网络（Hyperbolic Graph Neural Networks, HGNNs）擅长捕捉这种图层次结构，但它们无法建模文档内部丰富的文本语义。此外，文档中的文本内容通常讨论具有不同具体性的主题。层次主题模型（Hierarchical Topic Models, HTMs）能够发现文本语料库中的潜在主题层次结构。然而，大多数HTMs专注于文档内部的文本内容，而忽略了相互链接文档之间的图相邻关系。因此，我们提出了一种层次图主题建模变换器（Hierarchical Graph Topic Modeling Transformer），以将文档内部的主题层次结构和文档之间（即跨文档的）图层次结构统一到一个Transformer架构中。具体而言，为了融入文档内部的主题层次结构，我们设计了一个主题树并推断了层次结构树嵌入来进行主题建模。为了同时保持主题层次和图层次，我们将模型设计在超曲面上，并提出了超曲面双向递归神经网络（Hyperbolic Doubly Recurrent Neural Network），该网络能够建模祖先结构和兄弟结构树。这两种层次结构同时嵌入到每个Transformer层中，以便学习统一表示。监督和非监督实验均验证了该模型的有效性。', 'title_zh': '基于主题树的变换器的分层图形主题建模'}
{'arxiv_id': 'arXiv:2502.11336', 'title': 'ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability', 'authors': 'Ryuto Koike, Masahiro Kaneko, Ayana Niwa, Preslav Nakov, Naoaki Okazaki', 'link': 'https://arxiv.org/abs/2502.11336', 'abstract': "Detecting texts generated by Large Language Models (LLMs) could cause grave mistakes due to incorrect decisions, such as undermining student's academic dignity. LLM text detection thus needs to ensure the interpretability of the decision, which can help users judge how reliably correct its prediction is. When humans verify whether a text is human-written or LLM-generated, they intuitively investigate with which of them it shares more similar spans. However, existing interpretable detectors are not aligned with the human decision-making process and fail to offer evidence that users easily understand. To bridge this gap, we introduce ExaGPT, an interpretable detection approach grounded in the human decision-making process for verifying the origin of a text. ExaGPT identifies a text by checking whether it shares more similar spans with human-written vs. with LLM-generated texts from a datastore. This approach can provide similar span examples that contribute to the decision for each span in the text as evidence. Our human evaluation demonstrates that providing similar span examples contributes more effectively to judging the correctness of the decision than existing interpretable methods. Moreover, extensive experiments in four domains and three generators show that ExaGPT massively outperforms prior powerful detectors by up to +40.9 points of accuracy at a false positive rate of 1%.", 'abstract_zh': '检测由大型语言模型（LLMs）生成的文本可能会由于错误的决策而导致严重失误，例如损害学生的学术尊严。因此，LLM文本检测需要确保决策的可解释性，这有助于用户判断其预测的可靠性。当人类验证一段文本是人类撰写的还是由LLM生成的时，他们通常会直观地判断这段文本与哪一方的文本更相似。然而，现有的可解释检测方法并未与人类的决策过程对齐，无法提供用户易于理解的证据。为了解决这一问题，我们引入了ExaGPT，这是一种基于人类决策过程的可解释检测方法，用于验证文本的来源。ExaGPT通过检查文本与从数据存储中检索的人类撰写的文本还是LLM生成的文本共享更多相似段落来识别文本。这种方法可以为文本中的每个段落提供贡献于决策的相似段落示例作为证据。我们的用户评估表明，提供相似段落示例比现有的可解释方法更有效地帮助判断决策的正确性。此外，在四个领域和三个生成器的广泛实验中，ExaGPT在假阳性率为1%的情况下，其准确性比先前的强大检测器高出多达40.9个百分点。', 'title_zh': 'ExaGPT：基于示例的机器生成文本检测以提高人类可解释性'}
{'arxiv_id': 'arXiv:2502.11330', 'title': 'System Message Generation for User Preferences using Open-Source Models', 'authors': 'Minbyul Jeong, Jungho Cho, Minsoo Khang, Dawoon Jung, Teakgyu Hong', 'link': 'https://arxiv.org/abs/2502.11330', 'abstract': 'System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.', 'abstract_zh': '系统消息在与大规模语言模型（LLMs）的交互中起着至关重要的作用，通常作为启动对话的提示。通过系统消息，用户可以分配特定角色、执行预定任务、融入背景信息、指定各种输出格式和交流风格。尽管具备这种多功能性，公共数据中往往缺乏系统消息，且在行业领域受到严格的许可限制。手动为公共数据添加与用户指令相匹配的系统消息标签需要大量资源。鉴于这些挑战，我们的工作提出了SysGen管线，用于从未包含系统消息的监督微调数据集中生成更好地与系统消息和用户指令对齐的助手响应。在SysGen数据上进行的训练展示了模型响应与系统消息和用户指令对齐度显著提升，在多元维度基准（Multifacet benchmark）上的不同开源模型中均有体现，同时对未见过的基准测试（如Open LLM Leaderboard 2）的影响也保持在最小。我们的定性分析强调了多样化的系统消息对于确保在不同上下文中更好地适应的重要性。', 'title_zh': '使用开源模型生成用户偏好的系统消息生成'}
{'arxiv_id': 'arXiv:2502.11306', 'title': 'Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation', 'authors': 'Hieu Nguyen, Zihao He, Shoumik Atul Gandre, Ujjwal Pasupulety, Sharanya Kumari Shivakumar, Kristina Lerman', 'link': 'https://arxiv.org/abs/2502.11306', 'abstract': 'Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.', 'abstract_zh': '大型语言模型（LLMs）经常会出现虚构现象，生成事实错误或缺乏根据的内容，这限制了它们在高风险应用中的可靠性。产生虚构现象的一个关键因素是在训练过程中使用硬标签，这导致了确定性的监督，增强了模型的信心，并忽略了自然语言固有的不确定性。为解决这一问题，我们提出通过知识蒸馏（KD）来减轻虚构现象，其中教师模型为学生模型提供平滑的软标签，从而减少过度自信并改善事实根据。我们在指令数据上进行有监督的微调期间应用KD，并评估其在不同家族的LLMs中的有效性。在摘要基准测试上的实验结果显示，与标准微调相比，KD可以减少虚构现象，同时保持在一般NLP任务上的性能。这些发现突显了知识蒸馏作为减轻LLMs虚构现象和提高模型可靠性的有前途的方法的重要性。', 'title_zh': '平滑出幻觉：通过平滑知识蒸馏减轻大语言模型的幻觉现象'}
{'arxiv_id': 'arXiv:2502.11355', 'title': '"Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents', 'authors': 'Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu', 'link': 'https://arxiv.org/abs/2502.11355', 'abstract': "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.", 'abstract_zh': '大型语言模型（LLMs）正演变为自主决策者，这在高风险场景中引发了关于灾难性风险的担忧，尤其是在化学、生物学、放射学和核学（CBRN）领域。鉴于此类风险可能源自代理的有益性、无害性和诚实性（HHH）目标之间的权衡，我们构建了一个新颖的三阶段评估框架，该框架经过精心设计，能够有效且自然地揭示这些风险。我们针对12种高级LLM进行了14,400次代理模拟，并进行了广泛实验和分析。结果表明，大型语言模型代理可以自主进行灾难性行为和欺骗，而无需受到刻意诱导。此外，更强的推理能力往往增加而非减少这些风险。我们还表明这些代理可以违背指令和上级命令。总体而言，我们的研究实证证明了自主LLM代理存在灾难性风险的存在。如果需要，我们将提供我们的代码。', 'title_zh': '“核武部署了！”：分析自主大型语言模型代理决策中的灾难性风险'}
{'arxiv_id': 'arXiv:2502.11345', 'title': 'Hierarchical Graph Topic Modeling with Topic Tree-based Transformer', 'authors': 'Delvin Ce Zhang, Menglin Yang, Xiaobao Wu, Jiasheng Zhang, Hady W. Lauw', 'link': 'https://arxiv.org/abs/2502.11345', 'abstract': 'Textual documents are commonly connected in a hierarchical graph structure where a central document links to others with an exponentially growing connectivity. Though Hyperbolic Graph Neural Networks (HGNNs) excel at capturing such graph hierarchy, they cannot model the rich textual semantics within documents. Moreover, text contents in documents usually discuss topics of different specificity. Hierarchical Topic Models (HTMs) discover such latent topic hierarchy within text corpora. However, most of them focus on the textual content within documents, and ignore the graph adjacency across interlinked documents. We thus propose a Hierarchical Graph Topic Modeling Transformer to integrate both topic hierarchy within documents and graph hierarchy across documents into a unified Transformer. Specifically, to incorporate topic hierarchy within documents, we design a topic tree and infer a hierarchical tree embedding for hierarchical topic modeling. To preserve both topic and graph hierarchies, we design our model in hyperbolic space and propose Hyperbolic Doubly Recurrent Neural Network, which models ancestral and fraternal tree structure. Both hierarchies are inserted into each Transformer layer to learn unified representations. Both supervised and unsupervised experiments verify the effectiveness of our model.', 'abstract_zh': '文本文档通常以分层图结构连接，其中中心文档以指数增长的方式与其他文档连接。尽管双曲图神经网络（HGNNs）在捕获这样的图层次结构方面表现出色，但它们无法建模文档内部丰富的文本语义。此外，文档中的文本内容通常讨论不同具体性的主题。层次主题模型（HTMs）能够在一个文本语料库中发现这些潜在的主题层次。然而，大多数HTMs专注于文档内部的文本内容，而忽略了链接文档之间的图邻接关系。因此，我们提出了一种层次图主题建模变换器（Hierarchical Graph Topic Modeling Transformer），将文档内部的层次主题结构和文档之间的层次图结构统一集成到一个变换器中。具体而言，为了整合文档内部的主题层次结构，我们设计了一个主题树，并为层次主题模型推断了一种分层树嵌入。为了同时保留主题层次结构和图层次结构，我们设计了模型在双曲空间中，并提出了双回返神经网络（Hyperbolic Doubly Recurrent Neural Network），它可以建模祖先和兄弟树结构。这两个层次结构被插入到每个变换器层中，以学习统一的表示。有监督和无监督实验均验证了我们模型的有效性。', 'title_zh': '基于主题树的Transformer的层次图形主题建模'}
{'arxiv_id': 'arXiv:2502.11300', 'title': 'CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?', 'authors': 'Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.11300', 'abstract': "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: this https URL.", 'abstract_zh': '多模态大型语言模型（MLLMs）凭借其跨领域卓越的指令遵循和推理能力而闻名。然而，现有的基准主要关注评估下游任务中的事实和逻辑正确性，而较少关注评估MLLMs理解语用线索和跨模态关系的能力。为解决这一问题，我们通过共现关系（Coherence Relations）评估MLLMs在多模态话语分析（MDA）方面的能力。我们的基准测试Cordial涵盖了三个不同领域中的广泛共现关系，并具有不同程度的细腻度。通过在10多种不同提示策略的MLLMs上进行实验，我们发现即使是像Gemini 1.5 Pro和GPT-4o这样的顶尖模型，也无法匹配基于分类器的基线模型的性能。本研究强调应超越基于相似性的度量标准，采用以话语为导向的框架来评估MLLMs，从而提供对其能力更为细致的评估。基准测试和代码可在以下链接中获取：this https URL。', 'title_zh': 'CORDIAL: 多模态大型语言模型能否有效地理解连贯关系？'}
{'arxiv_id': 'arXiv:2502.11276', 'title': 'The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval', 'authors': 'Ting-Rui Chiang, Dani Yogatama', 'link': 'https://arxiv.org/abs/2502.11276', 'abstract': 'The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.', 'abstract_zh': '旋转位置嵌入（RoPE）广泛应用于许多大规模语言模型（LLM）的注意力头中。它根据输入序列中查询向量和键向量的位置，通过不同的角度旋转这些向量的维度。在长上下文建模中，位置范围可能会有很大差异，因此RoPE会对某些维度施加很大范围的角度旋转。我们假设这种广泛的角度旋转可能会使LLM无法充分利用这些维度。为了验证这一假设，我们进行了一项受控实验，结果显示应用RoPE会导致某些维度的低效用。我们对三个LLM的分析也表明，这些维度对LLM进行长上下文问答任务没有帮助。', 'title_zh': '旋转位置嵌入可能在长距离检索的注意力头中导致维度效率低下'}
{'arxiv_id': 'arXiv:2502.11336', 'title': 'ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability', 'authors': 'Ryuto Koike, Masahiro Kaneko, Ayana Niwa, Preslav Nakov, Naoaki Okazaki', 'link': 'https://arxiv.org/abs/2502.11336', 'abstract': "Detecting texts generated by Large Language Models (LLMs) could cause grave mistakes due to incorrect decisions, such as undermining student's academic dignity. LLM text detection thus needs to ensure the interpretability of the decision, which can help users judge how reliably correct its prediction is. When humans verify whether a text is human-written or LLM-generated, they intuitively investigate with which of them it shares more similar spans. However, existing interpretable detectors are not aligned with the human decision-making process and fail to offer evidence that users easily understand. To bridge this gap, we introduce ExaGPT, an interpretable detection approach grounded in the human decision-making process for verifying the origin of a text. ExaGPT identifies a text by checking whether it shares more similar spans with human-written vs. with LLM-generated texts from a datastore. This approach can provide similar span examples that contribute to the decision for each span in the text as evidence. Our human evaluation demonstrates that providing similar span examples contributes more effectively to judging the correctness of the decision than existing interpretable methods. Moreover, extensive experiments in four domains and three generators show that ExaGPT massively outperforms prior powerful detectors by up to +40.9 points of accuracy at a false positive rate of 1%.", 'abstract_zh': '由于大型语言模型（LLMs）生成的文本可能会导致错误决策，例如损害学生学术尊严，因此检测这类文本可能会引起严重错误。因此，LLM文本检测需要确保决策的可解释性，这有助于用户判断预测的可靠性。当人类验证一段文本是由人工撰写还是由LLM生成时，他们通常会直观地检查这段文本与人工撰写文本和LLM生成文本更多相似的部分。然而，现有的可解释检测器并未与人类的决策过程对齐，无法提供用户容易理解的证据。为了解决这一问题，我们引入了ExaGPT，这是一种基于人类决策过程的可解释检测方法，用于验证文本的来源。ExaGPT通过检查一段文本与人工撰写文本和LLM生成文本之间的相似部分来识别文本，并为每个文本片段提供支持决策的相似部分示例作为证据。我们的主观评估表明，提供相似部分示例比现有可解释方法更有效地帮助判断决策的正确性。此外，我们在四个领域和三种生成器的广泛实验中展示了ExaGPT相较于之前的强大检测器具有显著更高的性能，准确率在假阳性率为1%的情况下提高了高达40.9个百分点。', 'title_zh': 'ExaGPT：基于示例的机器生成文本检测以提高人类可解释性'}
{'arxiv_id': 'arXiv:2502.11275', 'title': "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", 'authors': 'Letian Peng, Zilong Wang, Feng Yao, Jingbo Shang', 'link': 'https://arxiv.org/abs/2502.11275', 'abstract': "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token \\emph{prediction} into \\emph{extraction} for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, \\emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.", 'abstract_zh': '大规模高质量数据，无论是预训练的原始文本还是后训练的标注数据，都已精心准备，用于孵化先进的大语言模型（LLMs）。相比之下，对于信息提取（IE），预训练数据，如带有BIO标记的序列，难以大规模扩展。我们展示了IE模型可以通过将下个token的“预测”重新框架为“提取”已有上下文中的token，从而作为大型语言模型（LLM）资源的“免费搭车者”。具体而言，我们提出的一种新的下一个token提取（NTE）范式学习了一个多功能的IE模型“Cuckoo”，其包含从LLM的预训练和后训练数据中转换而来的102.6M个提取数据。在少量样本的设置中，Cuckoo能够有效地适应传统和复杂的指令遵循信息提取任务，并在性能上优于现有的预训练IE模型。作为“免费搭车者”，Cuckoo能够自然地随着LLM数据准备的不断进步而发展，并从中受益于LLM训练管线的改进，而无需额外的手动努力。', 'title_zh': '鸽子：在大规模语言模型巢中孵化出的一个IE无关的寄生者'}
{'arxiv_id': 'arXiv:2502.11268', 'title': 'Improved Unbiased Watermark for Large Language Models', 'authors': 'Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang', 'link': 'https://arxiv.org/abs/2502.11268', 'abstract': "As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.", 'abstract_zh': '随着人工智能在文本生成方面超越人类能力，验证AI生成内容的起源变得至关重要。无偏见的水印提供了一种强大的解决方案，通过在语言模型生成的文本中嵌入统计信号而不影响其质量。本文介绍了MCmark，一种基于多通道的无偏见水印家族。MCmark通过将模型的词汇表划分为段，并根据水印密钥在选定的段中提高标记概率来工作。我们证明，MCmark不仅保持了语言模型的原始分布，还在可检测性和鲁棒性方面显著优于现有的无偏见水印。我们使用广泛使用的语言模型进行的实验表明，与现有最先进的无偏见水印相比，使用MCmark的可检测性提高了超过10%。这一进展突显了MCmark在增强AI生成文本中水印应用实践方面的潜力。', 'title_zh': '改善的无偏水印技术在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.11330', 'title': 'System Message Generation for User Preferences using Open-Source Models', 'authors': 'Minbyul Jeong, Jungho Cho, Minsoo Khang, Dawoon Jung, Teakgyu Hong', 'link': 'https://arxiv.org/abs/2502.11330', 'abstract': 'System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, specify various output formats and communication styles. Despite such versatility, publicly available data are often lack system messages and subject to strict license constraints in the industry field. Manual labeling of publicly available data with system messages that align with user instructions demands significant resources. In view of such challenges, our work introduces SysGen, a pipeline for generating system messages with better aligned assistant responses from the supervised fine-tuning dataset without system messages. Training on SysGen data has demonstrated substantial improvements in the alignment of model responses with system messages and user instructions, as demonstrated across various open-source models on the Multifacet benchmark, while maintaining minimal impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our qualitative analysis highlights the importance of diverse system messages to ensure better adaptability across different contexts.', 'abstract_zh': '系统消息在与大型语言模型（LLMs）的交互中发挥着至关重要的作用，通常用作初始化对话的提示。通过系统消息，用户可以分配特定的角色、执行预定的任务、嵌入背景信息、指定各种输出格式和交流风格。尽管具有如此多的多功能性，但公开可用的数据中往往缺乏系统消息，并且在某些行业中受到严格的许可限制。手动为公开可用的数据赋予与用户指令相匹配的系统消息标签需要大量资源。鉴于这些挑战，我们的工作引入了SysGen，这是一种生成系统消息的管道，这些系统消息能够生成与系统消息和用户指令更好地对齐的助手响应，而不依赖于监督微调数据集中的系统消息。在SysGen数据上的训练已经展示了模型响应与系统消息和用户指令对齐方面的显著改善，这一改善在多方面开源模型在Multifacet基准测试中的表现中得到了证实，同时对其他未见基准，如Open LLM Leaderboard 2，影响极小。我们的定性分析强调了系统消息多样性的重要性，以确保在不同背景下更好地适应能力。', 'title_zh': '使用开源模型生成用户偏好的系统消息生成'}
{'arxiv_id': 'arXiv:2502.11266', 'title': 'The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models', 'authors': 'Zhivar Sourati, Farzan Karimi-Malekabadi, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Jackson Trager, Ala Tak, Meng Chen, Fred Morstatter, Morteza Dehghani', 'link': 'https://arxiv.org/abs/2502.11266', 'abstract': "Language is far more than a communication tool. A wealth of information - including but not limited to the identities, psychological states, and social contexts of its users - can be gleaned through linguistic markers, and such insights are routinely leveraged across diverse fields ranging from product development and marketing to healthcare. In four studies utilizing experimental and observational methods, we demonstrate that the widespread adoption of large language models (LLMs) as writing assistants is linked to notable declines in linguistic diversity and may interfere with the societal and psychological insights language provides. We show that while the core content of texts is retained when LLMs polish and rewrite texts, not only do they homogenize writing styles, but they also alter stylistic elements in a way that selectively amplifies certain dominant characteristics or biases while suppressing others - emphasizing conformity over individuality. By varying LLMs, prompts, classifiers, and contexts, we show that these trends are robust and consistent. Our findings highlight a wide array of risks associated with linguistic homogenization, including compromised diagnostic processes and personalization efforts, the exacerbation of existing divides and barriers to equity in settings like personnel selection where language plays a critical role in assessing candidates' qualifications, communication skills, and cultural fit, and the undermining of efforts for cultural preservation.", 'abstract_zh': '语言远不止是一种交流工具。通过语言标记，可以获取大量关于用户身份、心理状态和社会背景等多方面的重要信息，这些见解在产品开发、营销甚至医疗健康等众多领域中被广泛应用。在四项使用实验和观察方法的研究中，我们证明了大规模语言模型（LLMs）作为写作助手的广泛应用与语言多样性的显著下降有关，并可能干扰语言提供的社会和心理洞察。研究显示，虽然LLMs在润色和重写文本时保留了核心内容，但它们不仅会使写作风格同质化，还会以一种选择性的方式放大某些主导特征或偏见，而抑制其他特征——强调一致性而非个性化。通过改变大规模语言模型、提示词、分类器和情境，我们展示了这些趋势的稳健性和一致性。我们的发现突显了语言同质化带来的广泛风险，包括诊断过程和个性化努力中的妥协，加剧了人员选拔等关键领域中的既存分歧和公平性障碍，以及削弱了文化的保护努力。', 'title_zh': '大型语言模型时代语言多样性的萎缩景观'}
{'arxiv_id': 'arXiv:2502.11306', 'title': 'Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation', 'authors': 'Hieu Nguyen, Zihao He, Shoumik Atul Gandre, Ujjwal Pasupulety, Sharanya Kumari Shivakumar, Kristina Lerman', 'link': 'https://arxiv.org/abs/2502.11306', 'abstract': 'Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.', 'abstract_zh': '大型语言模型（LLMs）经常会发生幻觉现象，生成事实错误或无依据的内容，这限制了它们在高风险应用中的可靠性。导致幻觉的一个关键因素是在训练过程中使用硬标签，这种做法会施加确定性的监督，促进模型的过度自信，并忽略自然语言固有的不确定性。为了解决这一问题，我们提出通过知识蒸馏（KD）来减轻幻觉现象，其中教师模型为学生模型提供平滑的软标签，从而减少模型的过度自信并提高事实依据性。我们在不同家族的LLMs的指令性数据上进行监督微调，并评估知识蒸馏的有效性。在摘要基准测试上的实验结果表明，与标准微调相比，知识蒸馏能够减少幻觉现象，同时保持对一般自然语言处理任务的性能。这些发现突显了知识蒸馏作为一种减轻LLMs幻觉现象并提高模型可靠性的有前景的方法。', 'title_zh': '平滑 hallucination：通过平滑知识蒸馏减轻大语言模型的 hallucination'}
{'arxiv_id': 'arXiv:2502.11258', 'title': 'Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification', 'authors': 'Thanushon Sivakaran, En-Hui Yang', 'link': 'https://arxiv.org/abs/2502.11258', 'abstract': "Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.", 'abstract_zh': '尽管大型语言模型（LLMs）在近年来展现出了明显的强大能力，但信息理论（IT）在增强LLM开发方面的潜在价值仍然未被充分探索。本文引入了条件互信息（CMI）的信息理论原则，将其应用于LLM微调以进行分类任务，探讨了其改进模型性能的潜力的两种主要方式：通过最小化CMI以提高模型的独立性能，以及通过最大化CMI以增强知识蒸馏（KD）以生成更强大的学生模型。为了将CMI应用于LLM微调，我们改编了最近提出的用于图像分类的CMI约束深度学习框架，并进行了一些修改。通过在LLM微调过程中最小化CMI，我们在6个GLUE分类任务中实现了优于BERT的性能提升。此外，在知识蒸馏过程中最大化CMI，在6个GLUE分类任务中也实现了显著的性能提升，优于DistilBERT。这些发现证明了CMI在优化独立的LLM和学生模型方面的适应性，展示了其作为推进LLM微调的稳健框架的潜力。我们的工作填补了信息理论与LLM开发之间的差距，为构建高性能语言模型提供了新的见解。', 'title_zh': '利用条件互信息提高大型语言模型微调以进行分类'}
{'arxiv_id': 'arXiv:2502.11250', 'title': 'Uncertainty-Aware Step-wise Verification with Generative Reward Models', 'authors': 'Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal', 'link': 'https://arxiv.org/abs/2502.11250', 'abstract': "Complex multi-step reasoning tasks, such as solving mathematical problems, remain challenging for large language models (LLMs). While outcome supervision is commonly used, process supervision via process reward models (PRMs) provides intermediate rewards to verify step-wise correctness in solution traces. However, as proxies for human judgement, PRMs suffer from reliability issues, including susceptibility to reward hacking. In this work, we propose leveraging uncertainty quantification (UQ) to enhance the reliability of step-wise verification with generative reward models for mathematical reasoning tasks. We introduce CoT Entropy, a novel UQ method that outperforms existing approaches in quantifying a PRM's uncertainty in step-wise verification. Our results demonstrate that incorporating uncertainty estimates improves the robustness of judge-LM PRMs, leading to more reliable verification.", 'abstract_zh': '对于解决数学问题等复杂的多步推理任务，大型语言模型（LLMs）仍然面临挑战。虽然结果监督被广泛使用，但通过过程奖励模型（PRMs）的过程监督能够提供中间奖励，用于验证解题过程中每一步的正确性。然而，作为人类判断的代理，PRMs 存在可靠性问题，包括易于受到奖励作弊的影响。在本研究中，我们提出利用不确定性量化（UQ）来增强生成性奖励模型在数学推理任务中逐步骤验证的可靠性。我们引入了一种名为CoT Entropy的新颖UQ方法，该方法在量化PRM在逐步骤验证中的不确定性方面优于现有方法。实验结果表明，引入不确定性估计能提高判别LM-PRMs的鲁棒性，从而实现更可靠的验证。', 'title_zh': '具有不确定性意识的逐步验证与生成奖励模型'}
{'arxiv_id': 'arXiv:2502.11300', 'title': 'CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?', 'authors': 'Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.11300', 'abstract': "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: this https URL.", 'abstract_zh': '多模态大语言模型（MLLMs）以其在多种问题领域中的卓越指令遵循能力和推理能力而闻名。然而，现有的基准主要关注下游任务中的事实和逻辑正确性，对评估MLLMs在解释语用线索和跨模态关系方面的能力关注不足。为解决这一问题，我们使用连贯性关系（Coherence Relations）对MLLMs进行多模态话语分析（MDA）的能力进行了评估。我们的基准测试Cordial涵盖了三个不同的话语领域中不同粒度层次的广泛连贯性关系。通过在10多种使用不同提示策略的MLLMs上进行实验，我们发现即使是像Gemini 1.5 Pro和GPT-4o这样的顶级模型也无法达到基于简单分类器的基线模型的性能。本研究强调了超越基于相似性的评估指标，采用基于话语驱动的框架来评估MLLMs的重要性，提供一种更细致的能力评估。基准测试和代码可在以下链接获取：[此链接]。', 'title_zh': 'CORDIAL：多模态大规模语言模型能否有效理解连贯性关系？'}
{'arxiv_id': 'arXiv:2502.11276', 'title': 'The Rotary Position Embedding May Cause Dimension Inefficiency in Attention Heads for Long-Distance Retrieval', 'authors': 'Ting-Rui Chiang, Dani Yogatama', 'link': 'https://arxiv.org/abs/2502.11276', 'abstract': 'The Rotary Position Embedding (RoPE) is widely used in the attention heads of many large language models (LLM). It rotates dimensions in the query and the key vectors by different angles according to their positions in the input sequence. For long context modeling, the range of positions may vary a lot, and thus RoPE rotates some dimensions by a great range of angles. We hypothesize that the wide range of rotation angles may prevent LLMs from utilizing those dimensions. To validate this hypothesis, we present a controlled experiment showing that applying RoPE causes low utility of certain dimensions. Our analyses on three LLMs also indicate that these dimensions do not help LLMs do long-context question answering.', 'abstract_zh': 'Rotary位置嵌入（RoPE）被广泛应用于许多大型语言模型（LLM）的注意力头部。RoPE根据输入序列中查询向量和键向量的位置，以不同的角度旋转这些向量的维度。对于长上下文建模而言，位置范围可能会有很大的变化，因此RoPE会将某些维度旋转大范围的角度。我们假设这种广泛的角度旋转可能会阻止LLM利用这些维度。为了验证这一假设，我们进行了一项受控实验，结果显示应用RoPE会导致某些维度的有效性降低。我们在三个LLM上的分析也表明，这些维度并不帮助LLM进行长上下文问题解答。', 'title_zh': '旋转位置嵌入可能在长距离检索的注意头中导致维度效率低下'}
{'arxiv_id': 'arXiv:2502.11275', 'title': "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", 'authors': 'Letian Peng, Zilong Wang, Feng Yao, Jingbo Shang', 'link': 'https://arxiv.org/abs/2502.11275', 'abstract': "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token \\emph{prediction} into \\emph{extraction} for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, \\emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.", 'abstract_zh': '大规模高质量数据，包括预训练的原始文本和后训练的标注数据，已被精心准备以孵化先进的大规模语言模型（LLMs）。相比之下，对于信息提取（IE），预训练数据，如BIO标签序列，难以扩大规模。我们展示了IE模型可以通过将下一个令牌的“预测”重新定义为“提取”现有上下文中的令牌，从而在LLM资源上充当“免费搭车者”。具体来说，我们提出了一种新的下个令牌提取（NTE）范式，学习了一个多功能的IE模型——“ kukoo”。该模型利用了1.026亿个从LLM预训练和后训练数据转换而来的提取数据。在少量样本设置下，kukoo能够有效地适应传统和复杂指令跟随信息提取任务，并在性能上优于现有的预训练IE模型。作为“免费搭车者”，kukoo可以自然地随着LLM数据准备的持续进步而演变，从而从LLM训练管道的改进中获益，而无需额外的手动努力。', 'title_zh': '鸽子：在大规模语言模型巢穴中孵化的IE免费 Riding，营养丰富版'}
{'arxiv_id': 'arXiv:2502.11268', 'title': 'Improved Unbiased Watermark for Large Language Models', 'authors': 'Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang', 'link': 'https://arxiv.org/abs/2502.11268', 'abstract': "As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.", 'abstract_zh': '随着人工智能在文本生成方面超越人类能力，认证AI生成内容的来源变得尤为重要。无偏见的水印通过在语言模型生成的文本中嵌入统计信号而不损害质量，提供了一个强大的解决方案。在本文中，我们介绍了一种基于多通道的无偏见水印家族——MCmark。MCmark通过将模型的词汇表分割成段，并根据水印密钥提升选定段落中token的概率来工作。我们证明，MCmark不仅能保持语言模型原始分布的完整性，还在检测能力和稳健性方面显著优于现有的无偏见水印。我们对广泛使用的语言模型进行的实验表明，与现有的最先进的无偏见水印相比，使用MCmark的检测性能提高了超过10%。这一进展突显了MCmark在增强AI生成文本中水印技术实际应用方面的潜力。', 'title_zh': '改进的无偏水印方法用于大型语言模型'}
{'arxiv_id': 'arXiv:2502.11244', 'title': 'Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment', 'authors': 'Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra', 'link': 'https://arxiv.org/abs/2502.11244', 'abstract': 'Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.', 'abstract_zh': '确保跨多种语言的一致性安全仍然是大型语言模型（LLM）面临的重大挑战。我们提出了Soteria，这是一种轻量级且强大的策略，能够定位并最小化最负责生成有害内容的“功能头”组件，这些组件存在于每种语言中。通过仅调整一小部分参数，Soteria极大地减少了政策违规行为，同时在低资源环境下也未牺牲整体模型性能。为了严格评估这一方法，我们还引入了XThreatBench，这是一个专门的多语言数据集，涵盖了从实际政策指南中提取的细粒度有害行为。实验结果显示，Soteria可以在高资源、中资源和低资源的语言中一致地提高安全指标。这些发现表明了一条可扩展、语言适应性和伦理对齐的LLM发展路径。', 'title_zh': 'Soteria：面向多语言安全对齐的语言特定功能参数调节'}
{'arxiv_id': 'arXiv:2502.11266', 'title': 'The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models', 'authors': 'Zhivar Sourati, Farzan Karimi-Malekabadi, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Jackson Trager, Ala Tak, Meng Chen, Fred Morstatter, Morteza Dehghani', 'link': 'https://arxiv.org/abs/2502.11266', 'abstract': "Language is far more than a communication tool. A wealth of information - including but not limited to the identities, psychological states, and social contexts of its users - can be gleaned through linguistic markers, and such insights are routinely leveraged across diverse fields ranging from product development and marketing to healthcare. In four studies utilizing experimental and observational methods, we demonstrate that the widespread adoption of large language models (LLMs) as writing assistants is linked to notable declines in linguistic diversity and may interfere with the societal and psychological insights language provides. We show that while the core content of texts is retained when LLMs polish and rewrite texts, not only do they homogenize writing styles, but they also alter stylistic elements in a way that selectively amplifies certain dominant characteristics or biases while suppressing others - emphasizing conformity over individuality. By varying LLMs, prompts, classifiers, and contexts, we show that these trends are robust and consistent. Our findings highlight a wide array of risks associated with linguistic homogenization, including compromised diagnostic processes and personalization efforts, the exacerbation of existing divides and barriers to equity in settings like personnel selection where language plays a critical role in assessing candidates' qualifications, communication skills, and cultural fit, and the undermining of efforts for cultural preservation.", 'abstract_zh': '语言远不止是一种沟通工具。通过语言标记，可以获取包括但不限于用户身份、心理状态和社会背景等丰富信息，并且这些见解广泛应用于从产品开发和营销到健康医疗等多个领域。通过四项利用实验和观察方法的研究，我们证明大规模语言模型（LLMs）作为写作助手的普及与语言多样性显著下降之间存在联系，并可能干扰语言提供的社会和心理洞察。研究显示，尽管LLMs在润色和重写文本时保留了文本的核心内容，它们不仅使写作风格同质化，还以有选择性地放大某些主导特征或偏见并削弱其他特征的方式改变了风格元素，从而优先强调一致性而非个体性。通过改变LLMs、提示、分类器和上下文，我们展示了这些趋势的稳健性和一致性。我们的发现突显了语言同质化带来的广泛风险，包括诊断过程和个性化努力受损，加剧了候选人资格评估、沟通技能和文化契合度评估中已有的裂痕和平等障碍，在人员选拔等领域尤为明显，同时削弱了文化保护的努力。', 'title_zh': '大型语言模型时代语言多样性景观的收缩趋势'}
{'arxiv_id': 'arXiv:2502.11258', 'title': 'Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification', 'authors': 'Thanushon Sivakaran, En-Hui Yang', 'link': 'https://arxiv.org/abs/2502.11258', 'abstract': "Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.", 'abstract_zh': '虽然大规模语言模型（LLMs）在近年来展示了卓越的能力，但信息论（IT）在提升LLM开发方面的潜力尚未得到充分探索。本文将信息论中的条件互信息（CMI）原理引入LLM微调，并在分类任务中探讨了其潜力的两种主要方式：通过最小化CMI来提高模型的独立性能，以及通过最大化CMI来增强知识蒸馏（KD）以构建更强大的学生模型。为了在LLM微调中应用CMI，我们修改了最初为图像分类设计的CMI约束深度学习框架。通过在LLM微调过程中最小化CMI，我们与BERT相比，在6个GLUE分类任务上实现了显著的性能提升。在知识蒸馏过程中最大化CMI，则在6个GLUE分类任务上与DistilBERT相比，显著提高了性能。这些发现表明CMI可以用于优化独立的LLM和学生模型，展示了其作为一个强大框架促进LLM微调的潜力。我们的研究填补了信息论与LLM开发之间的空白，为构建高性能语言模型提供了新的见解。', 'title_zh': '利用条件互信息提高大型语言模型微调以进行分类'}
{'arxiv_id': 'arXiv:2502.11228', 'title': 'Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs', 'authors': 'Mohammad Reza Rezaei, Adji Bousso Dieng', 'link': 'https://arxiv.org/abs/2502.11228', 'abstract': "Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.", 'abstract_zh': '检索增强生成（RAG）通过利用外部知识源来增强大型语言模型（LLMs）在特定领域的问题回答（QA）任务中的表现。然而，传统的RAG系统主要侧重于基于相关性的检索，往往在需要将信息从多个来源连接起来进行推理时会遇到冗余问题。本文提出了Vendi-RAG框架，该框架基于迭代过程，联合优化检索多样性和答案质量。这种联合优化使得多跳QA任务的准确性显著提高。Vendi-RAG利用Vendi分值（VS），一种灵活的基于相似度的多样性度量，来促进文档检索中的语义多样性。然后，该框架使用一个大型语言模型（LLM）裁判来评估推理步骤后生成的候选答案，并输出一个评分，该评分用于检索器在每次迭代中平衡检索到的文档的相关性和多样性。在三个具有挑战性的数据集——HotpotQA、MuSiQue和2WikiMultiHopQA——上的实验表明，Vendi-RAG在多跳推理任务中的有效性。该框架在HotpotQA、2WikiMultiHopQA和MuSiQue上的准确率分别比当前最优baseline方法Adaptive-RAG提高了+4.2%、+4.1%和+1.3%。随着检索文档数量的增加，Vendi-RAG的优势更为明显。最后，我们评估了Vendi-RAG在不同的LLM基础架构上，包括GPT-3.5、GPT-4和GPT-4o-mini，观察到一致的改进，证明该框架的优势是模型无偏的。', 'title_zh': 'Vendi-RAG：在多样性与质量之间适配性权衡显著提高大语言模型增强检索生成效果'}
{'arxiv_id': 'arXiv:2502.11250', 'title': 'Uncertainty-Aware Step-wise Verification with Generative Reward Models', 'authors': 'Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal', 'link': 'https://arxiv.org/abs/2502.11250', 'abstract': "Complex multi-step reasoning tasks, such as solving mathematical problems, remain challenging for large language models (LLMs). While outcome supervision is commonly used, process supervision via process reward models (PRMs) provides intermediate rewards to verify step-wise correctness in solution traces. However, as proxies for human judgement, PRMs suffer from reliability issues, including susceptibility to reward hacking. In this work, we propose leveraging uncertainty quantification (UQ) to enhance the reliability of step-wise verification with generative reward models for mathematical reasoning tasks. We introduce CoT Entropy, a novel UQ method that outperforms existing approaches in quantifying a PRM's uncertainty in step-wise verification. Our results demonstrate that incorporating uncertainty estimates improves the robustness of judge-LM PRMs, leading to more reliable verification.", 'abstract_zh': '在解决数学问题等复杂的多步推理任务方面，大型语言模型（LLMs）仍然面临挑战。尽管结果监督是常用的方法，但通过过程奖励模型（PRMs）进行过程监督可以提供中间奖励，用于验证解题过程中的每一步是否正确。然而，作为人类判断的代理，PRMs 存在可靠性问题，包括易受奖励拐棍（reward hacking）的影响。在本工作中，我们提出利用不确定性量化（UQ）来增强生成奖励模型在数学推理任务中逐步验证的可靠性。我们引入了CoT熵（CoT Entropy），这是一种新颖的UQ方法，并且在量度PRM在逐步验证中的不确定性方面优于现有方法。我们的结果表明，整合不确定性估计能够提高判断-语言模型（judge-LM）PRMs的稳定性，从而实现更可靠的验证。', 'title_zh': '带有不确定性意识的逐级验证生成奖励模型方法'}
{'arxiv_id': 'arXiv:2502.11223', 'title': 'Asymmetric Conflict and Synergy in Post-training for LLM-based Multilingual Machine Translation', 'authors': 'Tong Zheng, Yan Wen, Huiwen Bao, Junfeng Guo, Heng Huang', 'link': 'https://arxiv.org/abs/2502.11223', 'abstract': 'The emergence of Large Language Models (LLMs) has advanced the multilingual machine translation (MMT), yet the Curse of Multilinguality (CoM) remains a major challenge. Existing work in LLM-based MMT typically mitigates this issue via scaling up training and computation budget, which raises a critical question: Is scaling up the training and computation budget truly necessary for high-quality MMT, or can a deeper understanding of CoM provide a more efficient solution? To explore this problem, we analyze the linguistic conflicts and synergy, the underlying mechanism of CoM during post-training phase. We identify an asymmetric phenomenon in linguistic conflicts and synergy: the dominance of conflicts and synergy varies in different translation directions, leading to sub-optimal adaptation in existing post-training methods. We further find that a significant bottleneck in MMT appears to lie in post-training rather than multilingual pre-training, suggesting the need for more effective adaptation strategies. Building on these new insights, we propose a direction-aware training approach, combined with group-wise model merging, to address asymmetry in linguistic conflicts and synergy explicitly. Leveraging this strategy, our method fine-tunes X-ALMA-13B-Pretrain-trained only with multilingual pre-training-achieving comparable performance to XALMA-13B (only SFT) while using only 20B pretraining tokens and 17B parameters-5.5x fewer pretraining-tokens and 1.7x fewer model size-with just 0.85 COMET drop on Flores-200 testsets of 50 languages.', 'abstract_zh': '大型语言模型（LLMs）的发展推动了多语言机器翻译（MMT）的进步，但多语言化 curse（即语言复杂性带来的挑战，CoM）仍然是一个主要障碍。现有基于LLM的MMT工作通常通过扩大训练和计算预算来缓解这一问题，这引发了一个关键问题：高质M MT是否真的需要扩大训练和计算预算，还是对多语言复杂性的深入理解能够提供更有效的解决方案？为了解决这一问题，我们分析了后训练阶段的语言冲突和协同作用及其背后的机制。我们发现语言冲突和协同作用表现出一种不对称现象：不同翻译方向上冲突和协同作用的优势不同，导致现有后训练方法适应效果次优。进一步研究发现，M M T中的显著瓶颈主要出现在后训练阶段，而不是多语言预训练阶段，这表明需要更有效的适应策略。基于这些新的见解，我们提出了一种方向感知的训练方法，并结合组内模型合并，以明确解决语言冲突和协同作用的不对称性。利用这一策略，我们的方法仅使用13B预训练的X-ALMA-13B-Pretrain模型和200亿个预训练令牌以及17亿个参数，实现了与仅进行软策略迁移（SFT）的XALMA-13B相当的性能，在50种语言的Flores-200测试集上的COMET下降仅为0.85。', 'title_zh': '基于大语言模型的多语言机器翻译培训后阶段的不对称冲突与协同效应'}
{'arxiv_id': 'arXiv:2502.11211', 'title': 'A Survey of LLM-based Agents in Medicine: How far are we from Baymax?', 'authors': 'Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2502.11211', 'abstract': "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.", 'abstract_zh': '大型语言模型（LLMs）正在通过基于LLM的代理来理解和处理医疗任务，从而彻底改变医疗保健领域。本文综述全面回顾了医疗领域的基于LLM的代理，探讨了它们的架构、应用和挑战。我们分析了医疗代理系统的关键组件，包括系统特征、临床规划机制、医疗推理框架以及外部能力增强。综述涵盖了主要的应用场景，如临床决策支持、医疗记录、培训模拟和医疗服务优化。我们讨论了用于评估这些代理在医疗保健环境中性能的评估框架和指标。虽然基于LLM的代理在提升医疗服务方面显示出潜力，但仍存在诸多挑战，包括幻觉管理、多模态集成、实施障碍和伦理考量。综述最后指出了未来的研究方向，包括受到近期LLM架构发展启发的医疗推理进展、与物理系统的集成以及培训模拟的改进。本工作为研究者和实践者提供了当前状态和未来前景的结构化概述，展示了医疗领域的基于LLM的代理。', 'title_zh': '基于大型语言模型的医疗代理综述：我们距离 Baymax 还有多远？'}
{'arxiv_id': 'arXiv:2502.11198', 'title': 'ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition', 'authors': 'Bidyarthi Paul, Faika Fairuj Preotee, Shuvashis Sarker, Shamim Rahim Refat, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque, Shahriar Manzoor', 'link': 'https://arxiv.org/abs/2502.11198', 'abstract': 'ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition (NER) in Bangla regional dialects, capturing variations across Sylhet, Chittagong, and Barishal. The dataset has around 10,443 sentences, 3,481 sentences per region. The data was collected from two publicly available datasets and through web scraping from various online newspapers, articles. To ensure high-quality annotations, the BIO tagging scheme was employed, and professional annotators with expertise in regional dialects carried out the labeling process. The dataset is structured into separate subsets for each region and is available both in CSV format. Each entry contains textual data along with identified named entities and their corresponding annotations. Named entities are categorized into ten distinct classes: Person, Location, Organization, Food, Animal, Colour, Role, Relation, Object, and Miscellaneous. This dataset serves as a valuable resource for developing and evaluating NER models for Bangla dialectal variations, contributing to regional language processing and low-resource NLP applications. It can be utilized to enhance NER systems in Bangla dialects, improve regional language understanding, and support applications in machine translation, information retrieval, and conversational AI.', 'abstract_zh': 'ANCHOLIK-NER 是一个旨在识别孟加拉地方方言中命名实体的多元语言数据集，它涵盖了Sidlet、Chittagong 和Barishal 地区的方言变异。数据集包含约10,443个句子，每个地区有3,481个句子。数据来源于两个公开可用的数据集，并通过从各种在线报纸和文章中进行网络爬虫收集。为确保高质量的注释，采用了BIO 标记方案，专业人士携带方言方面的专业知识进行了标注过程。数据集按地区分为独立子集，并以CSV格式提供。每个条目包含文本数据以及识别出的命名实体及其对应的注释。命名实体被分为十个不同的类别：人名、地名、组织机构、食物、动物、颜色、角色、关系、物体和杂项。该数据集为开发和评估针对孟加拉方言变体的命名实体识别（NER）模型提供了有价值的资源，为地方语言处理和低资源自然语言处理（NLP）应用做出了贡献。它可用于增强孟加拉方言中的NER系统，提高对地方语言的理解，并支持机器翻译、信息检索和对话AI等应用。', 'title_zh': 'ANCHOLIK-NER：孟加拉乡村命名实体识别基准数据集'}
{'arxiv_id': 'arXiv:2502.11244', 'title': 'Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment', 'authors': 'Somnath Banerjee, Sayan Layek, Pratyush Chatterjee, Animesh Mukherjee, Rima Hazra', 'link': 'https://arxiv.org/abs/2502.11244', 'abstract': 'Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.', 'abstract_zh': '确保多种语言中的安全一致性对大规模语言模型（LLM）来说仍然是一个重大挑战。我们引入了Soteria，一种轻量级但强大的策略，该策略能够定位并最小化在每种语言中对有害内容生成最负责的“功能头”。通过仅调整参数的很小一部分，Soteria在不牺牲整体模型性能的情况下，大幅度减少了政策违规行为，即使在资源有限的情况下也是如此。为了严格评估我们的方法，我们还提出了XThreatBench，这是一个专门用于多语言的细粒度有害行为数据集，它涵盖了来自实际政策指南的数据。对领先的开源LLM（例如，Llama、Qwen、Mistral）进行的实验结果显示，Soteria在高资源、中资源和低资源语言中一致地改善了安全性指标。这些发现突显了走向可扩展、语言敏感和伦理对齐的LLM的有希望的道路。', 'title_zh': '索特里亚：多语言功能性参数调控以实现语言特定的安全对齐'}
{'arxiv_id': 'arXiv:2502.11193', 'title': 'Large Language Models Penetration in Scholarly Writing and Peer Review', 'authors': 'Li Zhou, Ruijie Zhang, Xunlian Dai, Daniel Hershcovich, Haizhou Li', 'link': 'https://arxiv.org/abs/2502.11193', 'abstract': 'While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes. To better understand these dynamics, we evaluate the penetration of LLMs across academic workflows from multiple perspectives and dimensions, providing compelling evidence of their growing influence. We propose a framework with two components: \\texttt{ScholarLens}, a curated dataset of human- and LLM-generated content across scholarly writing and peer review for multi-perspective evaluation, and \\texttt{LLMetrica}, a tool for assessing LLM penetration using rule-based metrics and model-based detectors for multi-dimensional evaluation. Our experiments demonstrate the effectiveness of \\texttt{LLMetrica}, revealing the increasing role of LLMs in scholarly processes. These findings emphasize the need for transparency, accountability, and ethical practices in LLM usage to maintain academic credibility.', 'abstract_zh': '尽管大型语言模型（LLMs）的广泛应用带来了便利，但也引起了对学术研究和学术流程可信度的担忧。为了更好地理解这些动态，我们从多个视角和维度评估了LLMs在学术工作流程中的渗透情况，提供了它们影响力不断增强的有力证据。我们提出了一种框架，包括两个组成部分：\\texttt{ScholarLens}，这是一个由人类生成和LLM生成的内容构成的受控数据集，用于多视角评估学术写作和同行评审过程中的模型渗透情况；\\texttt{LLMetrica}，这是一个用于评估模型渗透程度的工具，它使用基于规则的指标和基于模型的检测器进行多维度评估。我们的实验展示了\\texttt{LLMetrica}的有效性，揭示了LLMs在学术流程中的作用不断增加。这些发现强调了在使用LLMs时需要实现透明性、问责制和伦理实践的重要性，以维持学术的可信度。', 'title_zh': '大型语言模型在学术写作和同行评审中的渗透'}
{'arxiv_id': 'arXiv:2502.11190', 'title': 'ReLearn: Unlearning via Learning for Large Language Models', 'authors': 'Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2502.11190', 'abstract': 'Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at this https URL.', 'abstract_zh': '当前的大语言模型去学习方法通常依赖逆优化来降低目标标记的概率。然而，这种方法会破坏后续标记的预测，从而降低模型性能和语言连贯性。此外，现有的评估指标过分强调情境遗忘，而未能充分评估响应流畅性和相关性。为了解决这些问题，我们提出了一种名为ReLearn的数据增强和微调管道，用于有效的去学习，以及一个全面的评估框架。该框架引入了知识遗忘率（KFR）和知识保留率（KRR）来衡量知识层面的保存，并使用语言评分（LS）来评估生成质量。实验结果表明，ReLearn能够实现有针对性的遗忘，并保持高质量的输出。通过机制性分析，我们进一步证明了逆优化如何破坏连贯文本生成，而ReLearn保留了这一基本能力。代码可在此处访问：https://xxxxxxxxxx（请替换为实际的URL）。', 'title_zh': 'ReLearn：通过学习实现大型语言模型的知识剔除'}
{'arxiv_id': 'arXiv:2502.11187', 'title': 'TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking', 'authors': 'Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, Firoj Alam', 'link': 'https://arxiv.org/abs/2502.11187', 'abstract': 'In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1B and 3B parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately 37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to evaluate LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (this https URL).', 'abstract_zh': '在本文中，我们介绍了TituLLMs，这是首个公开的大型预训练孟加拉语语言模型，提供了1亿和3亿参数的版本。由于在训练和推理过程中计算资源的限制，我们主要关注较小的模型。为了训练TituLLMs，我们收集了一个包含大约370亿词的预训练数据集。我们将Llama-3.2分词器扩展为整合语言和文化特定知识，这也有助于加快训练和推理速度。目前缺乏用于评估孟加拉语语言模型的基准数据集。为解决这一问题，我们开发了五个基准数据集。我们对比了多个语言模型，包括TituLLMs，并证明TituLLMs的表现优于其早期的多语言版本。然而，并非总是如此，这也凸显了语言适应的复杂性。我们的工作为现有多语言开放模型向其他低资源语言的适应奠定了基础。为了促进更广泛的应用和进一步的研究，我们已将TituLLMs模型和基准数据集公开发布（链接如下）：[此链接](this https URL)。', 'title_zh': 'TituLLMs：一系列全面基准测试的孟加拉语大语言模型'}
{'arxiv_id': 'arXiv:2502.11184', 'title': "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs", 'authors': 'Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu', 'link': 'https://arxiv.org/abs/2502.11184', 'abstract': 'Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.', 'abstract_zh': '多模态大型语言模型（MLLMs）通过同时支持文本和图像交互，扩展了传统语言模型的能力。然而，确保这些模型的安全性仍然是一个重大挑战，特别是准确识别多模态内容是否安全的能力——我们将其称为安全性意识。在本文中，我们介绍了MMSafeAware，这是第一个全面的多模态安全性意识基准，旨在通过29种安全性场景评估1500个精心策划的图像-提示对来评估MLLMs。MMSafeAware包括不安全和过度安全的子集，以评估模型正确识别不安全内容和避免过度敏感（这可能妨碍其帮助性）的能力。使用MMSafeAware评估九种广泛使用的MLLMs揭示了当前模型的安全性不足，且往往过于敏感；例如，GPT-4V 将36.1% 的不安全输入错误分类为安全输入，将59.9% 的良性输入错误分类为不安全输入。我们进一步探讨了三种提高安全性意识的方法——基于提示的方法、视觉对比解码和以视觉为中心的推理微调——但发现这些方法均未达到满意的效果。我们的研究结果突显了在开发具有稳健安全性意识的MLLMs方面面临的巨大挑战，强调了在该领域进一步研究的必要性。所有代码和数据将公开提供，以促进未来的研究。', 'title_zh': "无法因小失大：评估多模态安全意识在多模态LLM中的表现\n\n这个标题的翻译尽量保留了原文的意思，并且符合学术规范，其中“Can't See the Forest for the Trees”是一个习语，原意是指因关注细节而忽略了整体，此处翻译为“无法因小失大”，以传达出文章的主题。"}
{'arxiv_id': 'arXiv:2502.11183', 'title': "Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls", 'authors': 'Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, Dong Yu', 'link': 'https://arxiv.org/abs/2502.11183', 'abstract': 'Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: $\\textit{over-exploration}$ due to redundant states with semantically equivalent content, and $\\textit{under-exploration}$ caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH, an e$\\textbf{f}$fici$\\textbf{e}$nt $\\textbf{t}$ree sear$\\textbf{ch}$ framework, which is a flexible, plug-and-play system compatible with various tree search algorithms. Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted $\\lambda$-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code will be released upon acceptance.', 'abstract_zh': '近年来，由验证器指导的树搜索算法的进步显著增强了大规模语言模型（LLMs）的推理能力，但代价是增加了计算资源的需求。本文我们识别了导致这一低效的两个关键挑战：由于冗余状态（具有语义等效内容的状态）导致的$\\textit{过度探索}$，以及由于验证器评分高方差导致的频繁轨迹切换引起的$\\textit{不足探索}$。为了应对这些问题，我们提出了一种名为FETCH的高效树搜索框架，该框架是一个灵活且即插即用的系统，兼容多种树搜索算法。我们的框架通过使用从微调的SimCSE模型获得的文本嵌入进行凝聚层次聚类，从而减轻过度探索。为了解决不足探索问题，我们通过在训练过程中结合时差学习和调整$\\lambda$-回报来增强验证器，以减少方差；并在推理过程中采用验证器集成来聚合得分。实验结果表明，我们的方法在四个不同类型的树搜索算法上显著提高了推理准确性和计算效率，为基于LLM的推理提供了更多实际应用的可能性。本研究的代码将在录用后发布。', 'title_zh': '不要迷失在细节中：通过克服树搜索探索中的问题来简化大模型推理'}
{'arxiv_id': 'arXiv:2502.11228', 'title': 'Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs', 'authors': 'Mohammad Reza Rezaei, Adji Bousso Dieng', 'link': 'https://arxiv.org/abs/2502.11228', 'abstract': "Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.", 'abstract_zh': '检索增强生成（RAG）通过利用外部知识源提升了大型语言模型（LLMs）在特定领域问题求解（QA）任务中的性能。然而，传统RAG系统主要侧重于相关性检索，往往在多源信息连接推理时难以避免冗余。本文介绍了一种名为Vendi-RAG的框架，该框架基于迭代过程，同时优化检索多样性和答案质量。这种联合优化能够显著提高多跳QA任务的准确性。Vendi-RAG利用Vendi评分（VS），这是一种基于相似度的灵活性多样度度量，以促进文档检索中的语义多样性。然后，通过评估推理步骤后生成的候选答案，使用LLM判别器输出一个评分，该评分在每次迭代中用于检索器平衡检索的文档的相关性和多样性。在三个具有挑战性的数据集——HotpotQA、MuSiQue和2WikiMultiHopQA——上的实验表明，Vendi-RAG在多跳推理任务中具有有效性。该框架在与传统单步骤和多步骤RAG方法相比时，实现了显著的准确性提升，在HotpotQA数据集上达到+4.2%、在2WikiMultiHopQA数据集上达到+4.1%、在MuSiQue数据集上达到+1.3%，相对于Adaptive-RAG，这是当前最佳基线。随着检索文档数量的增加，Vendi-RAG的好处更为显著。最后，我们还评估了Vendi-RAG在不同的LLM基础模型上的表现，包括GPT-3.5、GPT-4和GPT-4o-mini，并观察到一致的改进，表明该框架的优势具有模型的通用性。', 'title_zh': 'Vendi-RAG：适当地权衡多样性和质量显著改进了大规模语言模型增强生成 retrieval-augmented generation方法'}
{'arxiv_id': 'arXiv:2502.11177', 'title': 'The Mirage of Model Editing: Revisiting Evaluation in the Wild', 'authors': 'Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11177', 'abstract': "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.", 'abstract_zh': '尽管在人工评估中取得了近乎完美的结果，但在实际应用中的模型编辑有效性仍未得到探索。为填补这一空白，我们提出通过建立严格的评估实践来研究问答（QA）中的模型编辑，以评估编辑方法在纠正LLM错误方面的有效性。具体包括QAEdit，一个基于流行的QA数据集的新基准，以及一套标准化的评估框架。我们的单一编辑实验表明，当前的编辑方法在有效性方面远逊于此前报道的结果（38.5% vs. ~96%）。通过模块分析和控制实验，我们证明了这种性能下降归因于先前编辑研究中的评估实践问题。其中一个重要问题是测试时不当使用教师强迫会导致错误传播，即通过提供不可见于实际场景的真实令牌作为输入来阻止错误的传播。此外，我们通过顺序编辑模拟实际部署，揭示在仅进行1000次编辑的情况下，当前方法表现极其不佳。我们的分析为现有模型编辑方法的实际应用能力和评估实践提供了基本的重新审视，并建立了严格的评估框架，提供了关键见解以促进可靠和实用的模型编辑研究。', 'title_zh': '模型修订的幻象：重新审视实际环境中的评估'}
{'arxiv_id': 'arXiv:2502.11223', 'title': 'Asymmetric Conflict and Synergy in Post-training for LLM-based Multilingual Machine Translation', 'authors': 'Tong Zheng, Yan Wen, Huiwen Bao, Junfeng Guo, Heng Huang', 'link': 'https://arxiv.org/abs/2502.11223', 'abstract': 'The emergence of Large Language Models (LLMs) has advanced the multilingual machine translation (MMT), yet the Curse of Multilinguality (CoM) remains a major challenge. Existing work in LLM-based MMT typically mitigates this issue via scaling up training and computation budget, which raises a critical question: Is scaling up the training and computation budget truly necessary for high-quality MMT, or can a deeper understanding of CoM provide a more efficient solution? To explore this problem, we analyze the linguistic conflicts and synergy, the underlying mechanism of CoM during post-training phase. We identify an asymmetric phenomenon in linguistic conflicts and synergy: the dominance of conflicts and synergy varies in different translation directions, leading to sub-optimal adaptation in existing post-training methods. We further find that a significant bottleneck in MMT appears to lie in post-training rather than multilingual pre-training, suggesting the need for more effective adaptation strategies. Building on these new insights, we propose a direction-aware training approach, combined with group-wise model merging, to address asymmetry in linguistic conflicts and synergy explicitly. Leveraging this strategy, our method fine-tunes X-ALMA-13B-Pretrain-trained only with multilingual pre-training-achieving comparable performance to XALMA-13B (only SFT) while using only 20B pretraining tokens and 17B parameters-5.5x fewer pretraining-tokens and 1.7x fewer model size-with just 0.85 COMET drop on Flores-200 testsets of 50 languages.', 'abstract_zh': '大型语言模型（LLMs）的发展促进了多语言机器翻译（MTMT）的进步，然而，多语言性诅咒（CoM）仍然是一个主要挑战。基于LLM的多语言机器翻译（MTMT）现有工作通常通过增加训练和计算预算来减轻这一问题，这引发了一个关键问题：提高高质量多语言机器翻译是否真正需要增加训练和计算预算，还是深入理解多语言性诅咒（CoM）可以提供更有效的解决方案？为了解决这个问题，我们分析了后训练阶段语言冲突和协同作用的不对称现象及其内在机制。我们发现，语言冲突和协同作用的主导地位在不同的翻译方向上有所不同，导致现有后训练方法的次优适应。进一步的研究表明，多语言机器翻译中的一个显著瓶颈可能出现在后训练阶段，而不是多语言预训练阶段，这表明需要更有效的适应策略。基于这些新的见解，我们提出了一个方向感知的训练方法，结合组间模型合并，以明确解决语言冲突和协同作用的不对称性。利用这一策略，我们的方法仅通过多语言预训练调整X-ALMA-13B-Pretrain，就能在50种语言的Flores-200测试集上达到与XALMA-13B（仅监督微调）相当的性能，同时使用了200亿个预训练标记和170亿个参数，预训练标记减少了5.5倍，模型大小减少了1.7倍，并且在Flores-200测试集上的COMET得分下降仅为0.85。', 'title_zh': '基于大型语言模型的多语言机器翻译培训后异质性冲突与协同作用研究'}
{'arxiv_id': 'arXiv:2502.11211', 'title': 'A Survey of LLM-based Agents in Medicine: How far are we from Baymax?', 'authors': 'Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2502.11211', 'abstract': "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.", 'abstract_zh': '大规模语言模型（LLMs）正在通过开发能够理解和处理医学任务的基于LLM的代理来改变医疗保健领域。本综述提供了对医学中基于LLM代理的全面回顾，检查了它们的架构、应用及其面临的挑战。我们分析了医疗代理系统的关键组件，包括系统特性、临床规划机制、医学推理框架以及外部能力增强。综述涵盖了主要的应用场景，如临床决策支持、医疗记录、培训模拟以及医疗保健服务优化。我们讨论了用于评估这些代理在医疗保健环境中表现的评估框架和指标。尽管基于LLM的代理在提升医疗服务方面表现出潜力，但仍然存在一些挑战，包括幻觉管理、多模态集成、实施障碍以及伦理考虑。综述最后指出了未来的研究方向，包括受最近LLM架构发展启发的医学推理进展、与物理系统的集成以及培训模拟的改进。本研究为研究人员和实践者提供了基于LLM的代理在医学领域当前状态和未来前景的结构化概述。', 'title_zh': '基于大型语言模型的医疗代理综述：我们距Baymax还有多远？'}
{'arxiv_id': 'arXiv:2502.11176', 'title': 'LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning', 'authors': 'Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See', 'link': 'https://arxiv.org/abs/2502.11176', 'abstract': "Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies.", 'abstract_zh': '现代大型语言模型（LLMs）在处理推理任务时，无论是隐式还是显式地使用了各种形式的逻辑推理。理解如何最优地利用这些推理范式对于提升LLMs的推理能力至关重要。本文采取探索性的方法，通过引入一个控制性评估环境来评估类比推理——这一基本的认知任务——并在三个维度上系统地参数化：模态（文本、视觉、符号）、难度（简单、中等、困难）和任务格式（选择题或自由文本生成）。我们分析了这三大维度下归纳、 abduction（假说推理）和演绎推理流程的比较动态，并证明了我们的发现能够泛化到更广泛的情境学习任务中。此外，我们探讨了诸如假设选择、验证和细化等高级范式，揭示了这些方法在扩展LLMs推理中的逻辑推理潜力。本探索性研究为通过系统逻辑推理策略增强LLMs推理能力提供了基础，为未来的研究奠定了基石。', 'title_zh': 'LogiDynamics：探究大规模语言模型推理中逻辑推理的动力学过程'}
{'arxiv_id': 'arXiv:2502.11198', 'title': 'ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition', 'authors': 'Bidyarthi Paul, Faika Fairuj Preotee, Shuvashis Sarker, Shamim Rahim Refat, Shifat Islam, Tashreef Muhammad, Mohammad Ashraful Hoque, Shahriar Manzoor', 'link': 'https://arxiv.org/abs/2502.11198', 'abstract': 'ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition (NER) in Bangla regional dialects, capturing variations across Sylhet, Chittagong, and Barishal. The dataset has around 10,443 sentences, 3,481 sentences per region. The data was collected from two publicly available datasets and through web scraping from various online newspapers, articles. To ensure high-quality annotations, the BIO tagging scheme was employed, and professional annotators with expertise in regional dialects carried out the labeling process. The dataset is structured into separate subsets for each region and is available both in CSV format. Each entry contains textual data along with identified named entities and their corresponding annotations. Named entities are categorized into ten distinct classes: Person, Location, Organization, Food, Animal, Colour, Role, Relation, Object, and Miscellaneous. This dataset serves as a valuable resource for developing and evaluating NER models for Bangla dialectal variations, contributing to regional language processing and low-resource NLP applications. It can be utilized to enhance NER systems in Bangla dialects, improve regional language understanding, and support applications in machine translation, information retrieval, and conversational AI.', 'abstract_zh': 'ANCHOLIK-NER 是一个涵盖多种语言变体的语料库，用于孟加拉地区方言的命名实体识别（NER），覆盖了觉腓特、恰陶冈和巴瑞沙尔地区的差异。该语料库包含约 10,443 个句子，每个地区有 3,481 个句子。数据从两个公开可用的语料库中收集，并通过从各种在线报纸和文章中抓取网页内容获得。为了确保高质量的标注，采用了 BIO 标记方案，并由熟悉地区方言的专职标注员进行了标注过程。该语料库按照每个地区的多个子集进行结构化，并以 CSV 格式提供。每个条目都包含文本数据，以及已识别的命名实体及其相应的注释。命名实体被划分为十个不同的类别：人名、地名、组织、食物、动物、颜色、角色、关系、物体和杂项。该数据集作为开发和评估孟加拉方言变体的 NER 模型的宝贵资源，有助于地区语言处理和低资源自然语言处理应用的发展。它可以用于提高孟加拉方言的 NER 系统性能、增进地区语言理解，并支持机器翻译、信息检索和对话式 AI 等应用程序。', 'title_zh': 'ANCHOLIK-NER：孟加拉语区域命名实体识别基准数据集'}
{'arxiv_id': 'arXiv:2502.11193', 'title': 'Large Language Models Penetration in Scholarly Writing and Peer Review', 'authors': 'Li Zhou, Ruijie Zhang, Xunlian Dai, Daniel Hershcovich, Haizhou Li', 'link': 'https://arxiv.org/abs/2502.11193', 'abstract': 'While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes. To better understand these dynamics, we evaluate the penetration of LLMs across academic workflows from multiple perspectives and dimensions, providing compelling evidence of their growing influence. We propose a framework with two components: \\texttt{ScholarLens}, a curated dataset of human- and LLM-generated content across scholarly writing and peer review for multi-perspective evaluation, and \\texttt{LLMetrica}, a tool for assessing LLM penetration using rule-based metrics and model-based detectors for multi-dimensional evaluation. Our experiments demonstrate the effectiveness of \\texttt{LLMetrica}, revealing the increasing role of LLMs in scholarly processes. These findings emphasize the need for transparency, accountability, and ethical practices in LLM usage to maintain academic credibility.', 'abstract_zh': '虽然大规模语言模型（LLMs）的广泛应用为学术研究带来了便利，但也引发了对其学术研究可信度及研究过程的担忧。为了更好地了解这些动态，我们从多个视角和维度评估了LLMs在学术工作流程中的渗透情况，提供了其影响力不断扩大的有力证据。我们提出了一种框架，其中包括两个组成部分：\\texttt{ScholarLens}，这是一个跨学术写作和同行评审的人工智能生成内容和人类生成内容的精选数据集，用于多视角评估；以及\\texttt{LLMetrica}，这是一个使用基于规则的指标和基于模型的检测器进行多维度评估的工具，用于评估LLM的渗透程度。我们的实验表明，\\texttt{LLMetrica}在评估LLM在学术流程中的作用方面非常有效，揭示了LLMs在学术过程中的不断增加的角色。这些发现强调了在使用LLMs时需要增加透明度、责任感和道德规范，以维持学术研究的可信度。', 'title_zh': '大型语言模型在学术写作与同行评审中的渗透'}
{'arxiv_id': 'arXiv:2502.11175', 'title': 'Investigating Language Preference of Multilingual RAG Systems', 'authors': 'Jeonghyun Park, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11175', 'abstract': 'Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.', 'abstract_zh': '多语言检索增强生成（mRAG）系统通过整合外部多语言信息来增强语言模型，以产生具有上下文意识的回应。然而，mRAG系统在检索相关信息时面临语言差异带来的挑战，在多语言来源存在冲突时生成不一致的回复。在本研究中，我们通过一系列实验系统地探讨了mRAG中的语言偏好问题，既涵盖了检索过程也包括生成过程。我们的分析表明，检索器倾向于偏好高资源语言和查询语言，但这种偏好并不总是能够提高生成性能。此外，我们发现生成器更倾向于查询语言或拉丁字母，导致输出结果不一致。为了解决这些问题，我们提出了一种简单而有效的框架——双知识多语言RAG（DKM-RAG），该框架将翻译的多语言段落与互补模型知识融合在一起。实验结果表明，DKM-RAG能够减轻生成过程中的语言偏好问题，并在多种语言环境中提升性能。', 'title_zh': '探究多语言检索增强系统（RAG）的语言偏好'}
{'arxiv_id': 'arXiv:2502.11169', 'title': 'Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning', 'authors': 'Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, Ruichu Cai', 'link': 'https://arxiv.org/abs/2502.11169', 'abstract': 'Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post-training methods. Without additional training, LLMs typically enhance their mathematical reasoning abilities through inference scaling methods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: \\emph{understanding}, \\emph{planning}, \\emph{reflection}, \\emph{coding}, and \\emph{summary}. Each subset is further constrained to a small number of predefined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action subsets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model.', 'abstract_zh': '近年来，长链推理（Long Chain-of-Thoughts，CoTs）引起了广泛关注，旨在提高大规模语言模型（Large Language Models，LLMs）的推理能力。这就要求现有的LLMs，它们缺乏生成长链推理的能力，需要通过后训练方法来获得这种能力。在无需额外训练的情况下，LLMs通常通过方法如MCTS（ Monte Carlo Tree Search）来增强其数学推理能力，但这种方法受限于庞大的动作空间和低效的搜索策略，使得生成长链推理变得困难。为了解决这一问题，我们提出了限制动作空间并通过改进搜索策略引导长链推理的出现的方法。在我们提出的约束蒙特卡洛树搜索（Constrained Monte Carlo Tree Search，C-MCTS）框架中，我们从被划分为五个互不相交子集的受限动作空间中选择动作：理解（understanding）、规划（planning）、反思（reflection）、编码（coding）和摘要（summary）。每个子集进一步被限制在预定义的提示中，而不是允许LLMs任意生成动作。此外，我们通过引入关于动作集的先验知识来改进搜索策略，这些知识包括人类偏序的子集以及预训练的奖励模型。这些策略共同作用，显著减少了长链推理的庞大搜索空间。在数学推理基准测试中的广泛评估表明，在零样本设置下，我们的方法使7B模型的推理能力超过了72B模型。', 'title_zh': '利用受限蒙特卡洛树搜索生成可靠的数学推理长链思考链'}
{'arxiv_id': 'arXiv:2502.11190', 'title': 'ReLearn: Unlearning via Learning for Large Language Models', 'authors': 'Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang', 'link': 'https://arxiv.org/abs/2502.11190', 'abstract': 'Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at this https URL.', 'abstract_zh': '当前的大语言模型去学习方法通常依赖于反向优化来减少目标标记的概率。然而，这种方法会扰乱后续标记的预测，降低模型性能和语义连贯性。此外，现有的评估指标过分强调上下文遗忘，而未能充分评估响应的流畅性和相关性。为了解决这些挑战，我们提出了一种名为ReLearn的数据增强和微调管道，以及一个全面的评估框架。该框架引入了知识遗忘率（KFR）和知识保留率（KRR）来衡量知识层面的保留情况，并通过语言评分数值（LS）来评估生成质量。我们的实验结果显示，ReLearn能够在保持高质量输出的同时实现目标遗忘。通过机制分析，我们进一步展示了反向优化如何破坏连贯文本生成，而ReLearn则保留了这一关键能力。代码可在以下链接获取：this https URL。', 'title_zh': 'ReLearn：通过学习方式进行的大语言模型去学习方法'}
{'arxiv_id': 'arXiv:2502.11150', 'title': 'Surprisal Takes It All: Eye Tracking Based Cognitive Evaluation of Text Readability Measures', 'authors': 'Keren Gruteke Klein, Shachar Frenkel, Omer Shubi, Yevgeni Berzak', 'link': 'https://arxiv.org/abs/2502.11150', 'abstract': 'Text readability measures are widely used in many real-world scenarios and in NLP. These measures have primarily been developed by predicting reading comprehension outcomes, while largely neglecting what is perhaps the core aspect of a readable text: reading ease. In this work, we propose a new eye tracking based methodology for evaluating readability measures, which focuses on their ability to account for reading facilitation effects in text simplification, as well as for text reading ease more broadly. Using this approach, we find that existing readability formulas are moderate to poor predictors of reading ease. We further find that average per-word length, frequency, and especially surprisal tend to outperform existing readability formulas as measures of reading ease. We thus propose surprisal as a simple unsupervised alternative to existing measures.', 'abstract_zh': '文本可读性度量在许多实际应用场景和自然语言处理（NLP）中得到了广泛的应用。这些度量方法主要侧重于预测阅读理解的结果，而很大程度上忽视了文本可读性的核心方面：阅读顺畅度。在这项工作中，我们提出了一种基于眼动追踪的新方法来评估可读性度量，该方法侧重于这些度量方法在文本简化中考虑阅读促进效应的能力，以及对更广泛阅读顺畅度的考虑。通过这种方法，我们发现现有的可读性公式在预测阅读顺畅度方面中等至较差。此外，我们发现平均每词长度、频率，尤其是 surprisal 在衡量阅读顺畅度方面通常优于现有的可读性公式。因此，我们建议将 surprisal 作为一种简单且无需监督的替代指标，用于现有的度量方法。', 'title_zh': 'surprisal贯穿一切：基于眼动追踪的认知评估文本可读性测量方法'}
{'arxiv_id': 'arXiv:2502.11187', 'title': 'TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking', 'authors': 'Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, Firoj Alam', 'link': 'https://arxiv.org/abs/2502.11187', 'abstract': 'In this paper, we present TituLLMs, the first large pretrained Bangla LLMs, available in 1B and 3B parameter sizes. Due to computational constraints during both training and inference, we focused on smaller models. To train TituLLMs, we collected a pretraining dataset of approximately 37 billion tokens. We extended the Llama-3.2 tokenizer to incorporate language- and culture-specific knowledge, which also enables faster training and inference. There was a lack of benchmarking datasets to evaluate LLMs for Bangla. To address this gap, we developed five benchmarking datasets. We benchmarked various LLMs, including TituLLMs, and demonstrated that TituLLMs outperforms its initial multilingual versions. However, this is not always the case, highlighting the complexities of language adaptation. Our work lays the groundwork for adapting existing multilingual open models to other low-resource languages. To facilitate broader adoption and further research, we have made the TituLLMs models and benchmarking datasets publicly available (this https URL).', 'abstract_zh': '在本文中，我们介绍了TituLLMs，这是首个大型预训练孟加拉语大规模语言模型，目前提供1B和3B参数版本。由于在训练和推理过程中计算资源的限制，我们集中使用了较小的模型。为了训练TituLLMs，我们收集了一个约370亿个标记的预训练数据集。我们扩展了Llama-3.2分词器，使之包含语言和文化特定的知识，这也有助于加快训练和推理速度。由于缺少用于评估孟加拉语模型的标准数据集，我们开发了五个基准数据集。我们对包括TituLLMs在内的多种大型语言模型进行了基准测试，并展示了TituLLMs在多个任务上优于其最初的多语言版本。然而，这并不总是适用，突显了语言适应的复杂性。我们的工作为将现有的多语言开源模型适应其他低资源语言奠定了基础。为促进更广泛的采用和进一步研究，我们已将TituLLMs模型和基准数据集公开发布（请参见此链接：[提供链接的网址]）。', 'title_zh': 'TituLLMs：一种全面基准测试的孟加拉语大规模语言模型家族'}
{'arxiv_id': 'arXiv:2502.11137', 'title': 'Safety Evaluation of DeepSeek Models in Chinese Contexts', 'authors': 'Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Ning Wang, Zhenhong Long, Peijun Yang, Jiaojiao Zhao, Minjie Hua, Chaoyang Ma, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2502.11137', 'abstract': 'Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements.', 'abstract_zh': '近年来，DeepSeek 系列模型凭借其卓越的推理能力和开源战略，正在重塑全球 AI 地图。尽管具有这些优势，它们也表现出显著的安全缺陷。Cisco 的子公司 Robust Intelligence 与宾夕法尼亚大学合作进行的研究表明，当处理有害提示时，DeepSeek-R1 的攻击成功率达到了 100%。此外，多个安全公司和研究机构已确认该模型存在关键的安全漏洞。作为在中文和英文环境中均表现出强大性能的模型，DeepSeek 系列模型需要在两者语言背景下进行同等重要的安全性评估。然而，目前的研究主要集中在英文环境下的安全性评估，而在中文环境下对这些模型的安全性能进行全面评估的空白依旧存在。针对这一空白，本研究引入了 CHiSafetyBench，这是一种专门针对中文环境的安全评估基准。该基准系统地评估了 DeepSeek-R1 和 DeepSeek-V3 在中文环境中的安全性，揭示了它们在各类安全性能的表现。实验结果量化了这两个模型在中文环境下的缺陷，为后续改进提供了关键见解。', 'title_zh': '中文翻译为：中文语境中 DeepSeek 模型的安全性评估'}
{'arxiv_id': 'arXiv:2502.11131', 'title': 'Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM', 'authors': 'Yuqi Liu, Yan Zheng', 'link': 'https://arxiv.org/abs/2502.11131', 'abstract': 'Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in this https URL', 'abstract_zh': '随着Legal AI的快速发展，人们越来越多地关注其中最最重要的法律AI任务之一——相似案例检索，尤其是使用语言模型方面。然而，在我们的论文中，我们尝试从学习排序的视角而非直接使用语言模型来提升当前模型的排名性能。具体地，我们使用了一种成对方法——RankSVM作为分类器来替代全连接层，并结合常用的语言模型在相似案例检索数据集LeCaRDv1和LeCaRDv2上进行实验。最终我们得出结论：RankSVM通常能够帮助提高LeCaRDv1和LeCaRDv2数据集上的检索性能，相比于原始分类器通过优化精确排序。同时，它还能帮助缓解由于类别不平衡导致的过拟合问题。我们的代码可以在以下链接获取：[提供链接位置]', 'title_zh': '通过重新审视RankSVM来提高相似案例检索排名性能'}
{'arxiv_id': 'arXiv:2502.11184', 'title': "Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs", 'authors': 'Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu', 'link': 'https://arxiv.org/abs/2502.11184', 'abstract': 'Multimodal Large Language Models (MLLMs) have expanded the capabilities of traditional language models by enabling interaction through both text and images. However, ensuring the safety of these models remains a significant challenge, particularly in accurately identifying whether multimodal content is safe or unsafe-a capability we term safety awareness. In this paper, we introduce MMSafeAware, the first comprehensive multimodal safety awareness benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500 carefully curated image-prompt pairs. MMSafeAware includes both unsafe and over-safety subsets to assess models abilities to correctly identify unsafe content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine widely used MLLMs using MMSafeAware reveals that current models are not sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further explore three methods to improve safety awareness-prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning-but find that none achieve satisfactory performance. Our findings highlight the profound challenges in developing MLLMs with robust safety awareness, underscoring the need for further research in this area. All the code and data will be publicly available to facilitate future research.', 'abstract_zh': '多模态大型语言模型（MLLMs）通过同时支持文本和图像交互，扩展了传统语言模型的能力。然而，确保这些模型的安全性仍然是一个重大挑战，尤其是在准确识别多模态内容是否安全方面——我们称之为安全性意识。在本文中，我们介绍了MMSafeAware，这是首个全面的多模态安全性意识基准，旨在通过1500个精心挑选的图像-提示对，评估MLLMs在29种安全场景中的表现。MMSafeAware不仅包括不安全的内容子集，还包括过度安全的内容子集，以评估模型正确识别不安全内容和避免过度敏感（这可能妨碍其可用性）的能力。使用MMSafeAware评估九个广泛使用的MLLMs表明，当前的模型并不足够安全，并且往往过于敏感；例如，GPT-4V将36.1%的不安全输入错误分类为安全输入，还将59.9%的良性输入错误分类为不安全输入。我们进一步探讨了三种提高安全性意识的方法——基于提示的方法、视觉对比解码和以视觉为中心的推理微调，但发现这些方法均未达到令人满意的效果。我们的研究结果突显了开发具有强大安全性意识的MLLMs所面临的巨大挑战，强调了在该领域进行进一步研究的必要性。所有代码和数据将公开发布，以促进未来的研究。', 'title_zh': '《只见树木不见森林：多模态安全意识的基准测试》\n\n这个标题翻译成中文后，既保留了原意，又符合学术规范。这里“只见树木不见森林”是一个比喻，用来形象地描述在评估多模态大语言模型（Multimodal LLMs）的多模态安全意识时，容易过于关注局部细节（树木），而忽视整体（森林）。'}
{'arxiv_id': 'arXiv:2502.11128', 'title': 'FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching', 'authors': 'Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin', 'link': 'https://arxiv.org/abs/2502.11128', 'abstract': "To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in this https URL.", 'abstract_zh': '为了推进连续值标记建模并增强时间连贯性，在此我们提出FELLE，一种结合语言建模和标记层面流配对的自回归模型。通过利用语言模型的自回归性质和流配对的生成效果，FELLE 有效地预测连续值标记（梅尔频谱）。对于每个连续值标记，FELLE 通过融入前一步的信息来修改流配对中的通用先验分布，从而提高连贯性和稳定性。此外，为了提高合成质量，FELLE 引入了一种从粗到细的流配对机制，逐层生成连续值标记，根据语言模型的输出进行条件化。实验结果表明，将流配对技术整合到自回归梅尔频谱建模中具有潜在的价值，可以显著提高语音合成（TTS）的质量，详见this https URL。', 'title_zh': 'FELLE：基于token级从粗到细流匹配的自回归语音合成'}
{'arxiv_id': 'arXiv:2502.11123', 'title': 'DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and Streaming Capabilities', 'authors': 'Xiangyu Lu, Wang Xu, Haoyu Wang, Hongyun Zhou, Haiyan Zhao, Conghui Zhu, Tiejun Zhao, Muyun Yang', 'link': 'https://arxiv.org/abs/2502.11123', 'abstract': 'Real-time speech conversation is essential for natural and efficient human-machine interactions, requiring duplex and streaming capabilities. Traditional Transformer-based conversational chatbots operate in a turn-based manner and exhibit quadratic computational complexity that grows as the input size increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end multimodal duplex model for speech-to-text conversation. DuplexMamba enables simultaneous input processing and output generation, dynamically adjusting to support real-time streaming. Specifically, we develop a Mamba-based speech encoder and adapt it with a Mamba-based language model. Furthermore, we introduce a novel duplex decoding strategy that enables DuplexMamba to process input and generate output simultaneously. Experimental results demonstrate that DuplexMamba successfully implements duplex and streaming capabilities while achieving performance comparable to several recently developed Transformer-based models in automatic speech recognition (ASR) tasks and voice assistant benchmark evaluations.', 'abstract_zh': '实时语音对话对于自然高效的人机交互至关重要，需要具备双工和流式处理能力。传统的基于Transformer的对话聊天机器人以轮流方式进行操作，并且随着输入规模的增加，表现出二次计算复杂度。在本文中，我们提出了一种名为DuplexMamba的基于Mamba的端到端多模态双工模型，用于实现语音到文本的对话。DuplexMamba能够同时处理输入和生成输出，并能根据需要动态调整以支持实时流式处理。具体而言，我们开发了一种基于Mamba的语音编码器，并将其与基于Mamba的语言模型进行了结合。此外，我们引入了一种新的双工解码策略，使得DuplexMamba能够同时处理输入并生成输出。实验结果表明，DuplexMamba成功实现了双工和流式处理能力，同时在自动语音识别任务（ASR）和语音助手基准评估中，其性能与近年来开发的几种基于Transformer的模型相当。', 'title_zh': 'DuplexMamba：增强实时语音对话的双向和流式能力'}
{'arxiv_id': 'arXiv:2502.11116', 'title': 'Gumbel Reranking: Differentiable End-to-End Reranker Optimization', 'authors': 'Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2502.11116', 'abstract': 'RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4\\% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.', 'abstract_zh': 'RAG系统依赖于重排序器来识别相关的文档。然而，对这些模型进行微调仍然具有挑战性，主要原因在于标注过的查询-文档对数据稀缺。现有的基于知识蒸馏的方法存在训练-推理不一致的问题，并且无法捕捉候选文档之间的相互依赖关系。为克服这些限制，我们将重排序过程重新表述为一个注意力掩码问题，并提出了一种名为Gumbel重排序的端到端训练框架，旨在最小化训练推理之间的差距。在我们的方法中，重排序器的优化被重新表述为通过Gumbel Trick和松弛Top-$k$采样学习一个文档级的随机Top-$k$注意力掩码。这种表述方式通过最小化总的语言损失实现了端到端的优化。在不同设置下的实验结果一致表明了性能提升，包括在HotpotQA数据集中区分间接相关文档时召回率提高了10.4%。', 'title_zh': 'Gumbel 重排序：可微端到端重排序优化'}
{'arxiv_id': 'arXiv:2502.11115', 'title': 'Are Generative Models Underconfident? An Embarrassingly Simple Quality Estimation Approach', 'authors': 'Tu Anh Dinh, Jan Niehues', 'link': 'https://arxiv.org/abs/2502.11115', 'abstract': 'Quality Estimation (QE) is estimating the quality of model output when the ground truth reference is not available. Looking at model uncertainty from its own output probabilities is the most trivial and low-effort way to estimate the output quality. However, for generative model, output probabilities might not be the best quality estimator. At an output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower token probability does not necessarily mean lower output quality. In other words, the model can be considered underconfident. In this paper, we propose a QE approach called Dominant Mass Probability (DMP}, that boosts the model confidence in cases where there are multiple viable output options. We show that, with no increase in complexity, DMP is notably better than sequence probability when estimating the quality of different models (Whisper, Llama, etc.) on different tasks (translation, summarization, etc.). Compared to sequence probability, DMP achieves on average +0.208 improvement in Pearson correlation to ground-truth quality.', 'abstract_zh': '质量估计（QE）是在没有参考目标的情况下评估模型输出质量的一种方法。从模型自身输出的概率来观察模型不确定性可能是最简单且低消耗的评估输出质量的方法。然而，对于生成模型而言，输出概率可能不是最理想的质量估计器。在输出步骤中，可能存在多种正确的选项，使得概率分布更加分散。因此，较低的词的概率并不一定意味着较低的输出质量。换句话说，模型可以被认为是欠自信的。在这篇论文中，我们提出了一种名为主导质量概率（DMP）的质量估计方法，该方法在有多种可行输出选项的情况下增强了模型的自信度。我们展示了，在不增加复杂性的前提下，DMP在不同模型（如Whisper、Llama等）和不同任务（如翻译、摘要生成等）上评估输出质量时，明显优于序列概率。与序列概率相比，DMP在皮尔森相关性平均提高了0.208，更接近真实的质量评估。', 'title_zh': '生成模型过度保守吗？一种令人惊讶地简单的问题质量估计方法'}
{'arxiv_id': 'arXiv:2502.11183', 'title': "Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls", 'authors': 'Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, Dong Yu', 'link': 'https://arxiv.org/abs/2502.11183', 'abstract': 'Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: $\\textit{over-exploration}$ due to redundant states with semantically equivalent content, and $\\textit{under-exploration}$ caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH, an e$\\textbf{f}$fici$\\textbf{e}$nt $\\textbf{t}$ree sear$\\textbf{ch}$ framework, which is a flexible, plug-and-play system compatible with various tree search algorithms. Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted $\\lambda$-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code will be released upon acceptance.', 'abstract_zh': '近期，由验证器引导的树搜索算法的发展显著增强了大型语言模型（LLMs）的推理能力，但代价是增加了计算资源的需求。在本文中，我们识别出导致这一低效率的两个关键挑战：由于具有语义等效内容的冗余状态导致的过度探索（over-exploration），以及由于验证器评分高方差导致的频繁轨迹切换引起的探索不足（under-exploration）。为了应对这些问题，我们提出了一种高效树搜索框架FETCH，这是一种灵活的、即插即用的系统，适用于各种树搜索算法。我们的框架通过使用从微调的SimCSE模型中获得的文本嵌入进行凝聚聚类，来缓解过度探索。为了解决探索不足的问题，我们通过引入基于时间差的学习并调整$\\lambda$-回报进行训练，来减少方差，并在推理时使用验证器集成来聚合分数。实验表明，我们的方法能显著提高不同树搜索算法的推理准确性和计算效率，为基于LLM的推理提供了更加实际的应用。代码将在接受后发布。', 'title_zh': '不要在细节中迷失：通过克服树搜索探索中的问题简化大语言模型推理'}
{'arxiv_id': 'arXiv:2502.11114', 'title': 'Beyond Pairwise: Global Zero-shot Temporal Graph Generation', 'authors': 'Alon Eirew, Kfir Bar, Ido Dagan', 'link': 'https://arxiv.org/abs/2502.11114', 'abstract': "Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, in which event pairs are considered individually, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph at once, then applies transitive constraints optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method significantly outperforms existing zero-shot approaches while achieving competitive performance with supervised models.", 'abstract_zh': '时间关系提取（TRE）是自然语言处理（NLP）中的一项基本任务，涉及识别文档中事件之间的时间关系。尽管大型语言模型（LLMs）取得了进展，但它们在TRE中的应用仍然有限。现有的大多数方法依赖于成对分类，在这种方法中，事件成对地进行考虑，导致计算效率低下，并且在生成的时间图中缺乏全局一致性。在本工作中，我们提出了一种新颖的零样本方法，该方法一次生成文档的完整时间图，然后应用传递约束优化来细化预测并强制执行关系之间的时间一致性。此外，我们引入了OmniTemp，这是一个新的数据集，对文档中所有目标事件对进行了完整的注释。通过实验和分析，我们证明了我们的方法在现有零样本方法中表现显著更优，并且在监督模型的性能上具有竞争力。', 'title_zh': '超越成对关系：全局零样本时间图生成'}
{'arxiv_id': 'arXiv:2502.11177', 'title': 'The Mirage of Model Editing: Revisiting Evaluation in the Wild', 'authors': 'Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11177', 'abstract': "Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.", 'abstract_zh': '尽管在人工评估中取得了几乎完美的结果，模型编辑在实际应用中的有效性仍然没有得到探索。为了解决这一问题，我们提出通过建立严格评估实践来研究模型编辑在问答（QA）中的应用，以评估编辑方法在纠正大型语言模型（LLM）错误方面的效果。这包括一个名为QAEdit的新基准，该基准源自流行的QA数据集，以及一个标准化的评估框架。我们的单一编辑实验表明，当前的编辑方法的性能远不如之前所报告的（38.5% vs. ~96%）。通过模块分析和控制实验，我们证明了这种性能下降源于先前编辑研究中评估实践的问题。一个关键问题是，测试中不恰当使用教师强迫（teacher forcing）阻止了错误传播，因为这会喂入真实场景中不可用的 ground truth 令牌。此外，我们通过顺序编辑的方式模拟了实际部署，发现当前的方法在只有1000次编辑的情况下表现极其糟糕。我们的分析为现有模型编辑方法的实际应用能力和评估实践提供了基本的重新审视，并建立了一个严格的评估框架，提供了关键见解，以推动可靠且实用的模型编辑研究的发展。', 'title_zh': '模型编辑的幻象：重新审视野境评估'}
{'arxiv_id': 'arXiv:2502.11113', 'title': 'Valuable Hallucinations: Realizable Non-realistic Propositions', 'authors': 'Qiucheng Chen, Bo Wang', 'link': 'https://arxiv.org/abs/2502.11113', 'abstract': 'This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs),addressing a gap in the existing this http URL provide a systematic definition and analysis of hallucination value,proposing methods for enhancing the value of this http URL contrast to previous works,which often treat hallucinations as a broad flaw,we focus on the potential value that certain types of hallucinations can offer in specific this http URL in LLMs generally refer to the generation of unfaithful, fabricated,inconsistent,or nonsensical this http URL than viewing all hallucinations negatively,this paper gives formal representations and manual judgments of "valuable hallucinations" and explores how realizable non-realistic propositions-ideas that are not currently true but could be achievable under certain conditions-can have constructive this http URL present experiments using the Qwen2.5 model and HalluQA dataset, employing ReAct prompting (which involves reasoning, confidence assessment, and answer verification) to control and optimize hallucinations. Our findings show that ReAct prompting results in a reduction in overall hallucinations and an increase in the proportion of valuable this http URL results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability.', 'abstract_zh': '本文首次提出了大型语言模型（LLMs）中有价值的幻觉（hallucinations）的第一个正式定义，填补了现有研究中的这一空白。本文系统地定义和分析了幻觉的价值，并提出了增强幻觉价值的方法。与以往的研究通常将幻觉视为广泛的缺陷不同，本文侧重于某些类型幻觉在特定情境下可能提供的潜在价值。在LLMs中，幻觉通常指生成不忠实、虚构、不一致或无意义的内容。本文不仅不完全否定所有幻觉，还给出了“有价值的幻觉”的正式表示和人工判断，并探讨了不可实现的非现实命题（即目前不真实但特定条件下可能实现的观念）如何具有建设性价值。本文使用Qwen2.5模型和HalluQA数据集进行了实证研究，采用ReAct提示法（涉及推理、信心评估和答案验证）来控制和优化幻觉。研究结果表明，通过系统地控制幻觉，可以提高其有用性，同时不牺牲事实可靠度。', 'title_zh': '有价值的幻觉：可行的非现实命题'}
{'arxiv_id': 'arXiv:2502.11108', 'title': 'Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications', 'authors': 'Alexandru Lecu, Adrian Groza, Lezan Hawizy', 'link': 'https://arxiv.org/abs/2502.11108', 'abstract': 'Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications.', 'abstract_zh': '大型语言模型（LLMs）在自然语言生成领域取得了显著进展。然而，它们经常生成未经验证的输出，这在关键应用中降低了它们的可靠性。本研究提出了一种创新框架，通过检索增强生成技术将结构化的生物医学知识与LLMs结合起来。我们的系统通过识别和提炼与年龄相关黄斑变性（AMD）相关的医学摘要中的因果关系和命名实体，构建了一个详尽的知识图谱。利用基于向量的检索过程和本地部署的语言模型，我们的框架生成了既相关又可验证的响应，直接引用了临床证据。实验结果表明，该方法显著减少了幻觉现象，提高了事实精度，并改善了生成响应的清晰度，为高级生物医学聊天机器人的应用提供了稳健的解决方案。', 'title_zh': '知识图谱驱动的检索增强生成：将 Deepseek-R1 与 Weaviate 集成以实现高级聊天机器人应用'}
{'arxiv_id': 'arXiv:2502.11176', 'title': 'LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning', 'authors': 'Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See', 'link': 'https://arxiv.org/abs/2502.11176', 'abstract': "Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies.", 'abstract_zh': '现代大规模语言模型（LLMs）在处理推理任务时，会以显性和隐性的形式运用各种逻辑推理方式。合理利用这些推理范式对于提升LLMs的推理能力至关重要。本文采用探索性方法，通过引入一个受控评估环境来评估类比推理——一种基本的认知任务——并系统地在三种维度上参数化：模态（文本、视觉、符号）、难度（易于、中等、困难）以及任务格式（多项选择或自由文本生成）。我们分析了这些维度下归纳、 abduction（ abduction 在学术中可能被误解为一种推理类型，通常学术文献中使用 abduction 来表示假设演绎，但更严格的学术概念是“假设生成”，英文直接使用 abduction 更为准确） 和演绎推理管道的动态对比，并展示了我们的发现能够推广到更广泛的上下文学习任务中。此外，我们还探讨了假设选择、验证和优化等先进范式，揭示了它们在扩展LLMs推理中的逻辑推理方面的能力。这一探索性研究为通过系统逻辑推理策略增强LLMs推理能力的未来研究奠定了基础。', 'title_zh': 'LogiDynamics：解析大规模语言模型推理中逻辑推理的动力学过程'}
{'arxiv_id': 'arXiv:2502.11104', 'title': 'Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping', 'authors': 'Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.11104', 'abstract': 'Knowledge Distillation (KD) has emerged as a prominent technique for model compression. However, conventional KD approaches primarily focus on homogeneous architectures with identical tokenizers, constraining their applicability in cross-architecture scenarios. As for the cross-tokenizer KD, the differences in the tokenizers give rise to two fundamental challenges: (1) sequence misalignment caused by divergent tokenization strategies, and (2) mismatched vocabulary size and composition. While existing probability-matching methods attempt to address these issues, their efficacy remains limited due to suboptimal alignment in both the sequence and vocabulary aspects. To overcome these limitations, we propose Contextual Dynamic Mapping (CDM), a novel cross-tokenizer distillation framework that employs contextual information to enhance sequence alignment precision and dynamically improves vocabulary mapping. We evaluated the effectiveness of our approach across five advanced and widely-used model families (i.e, LLama3, Phi3, Gemma2, OPT and Qwen2), which were configured into three distinct teacher-student pairs. Our method shows significant advantages over existing cross-tokenizer distillation baselines across diverse benchmarks, including instruction-following, code generation and math. Notably, our analysis reveals that combining conventional same-tokenizer distillation and cross-tokenizer distillation through CDM yields further performance improvements. The code is available at this https URL', 'abstract_zh': '知识蒸馏（KD）已成为模型压缩的一项重要技术。然而，传统的KD方法主要集中在具有相同分词器的同构架构上，限制了其在跨架构场景中的应用。对于跨分词器的KD，不同分词器带来的两个基本挑战如下：（1）由于分词策略的差异导致的序列对齐问题，（2）词汇表大小和组成不匹配的问题。虽然现有的概率匹配方法试图解决这些问题，但由于在序列和词汇方面对齐不理想，其有效性仍然有限。为克服这些限制，我们提出了上下文动态映射（CDM），这是一种新的跨分词器蒸馏框架，利用上下文信息来提高序列对齐精度，并动态优化词汇映射。我们通过五个先进的广泛使用模型家族（即LLama3、Phi3、Gemma2、OPT和Qwen2）配置成三个不同的师-生对来评估我们方法的有效性，这些模型包括指令跟随、代码生成和数学任务。分析表明，通过CDM结合传统的同分词器KD和跨分词器KD可以进一步提高性能。相关代码可通过以下链接获取：this https URL', 'title_zh': '增强基于上下文动态映射的跨分词器知识精炼'}
{'arxiv_id': 'arXiv:2502.11175', 'title': 'Investigating Language Preference of Multilingual RAG Systems', 'authors': 'Jeonghyun Park, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11175', 'abstract': 'Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.', 'abstract_zh': '多语言检索增强生成（mRAG）系统通过整合外部多语言信息来增强语言模型，从而生成上下文感知的响应。然而，mRAG系统在检索相关信息时因查询和文档之间的语言差异而受到影响，当多语言来源存在冲突时，会导致生成不一致的响应。在本文中，我们通过一系列实验系统地研究了mRAG在检索和生成中的语言偏好。我们的分析表明，检索器倾向于偏好资源丰富和查询语言，但这种偏好并不总能提升生成性能。此外，我们观察到生成器倾向于偏好查询语言或拉丁字母，从而导致生成不一致的输出。为了克服这些问题，我们提出了一种简单而有效的框架——双知识多语言RAG（DKM-RAG），该框架将翻译的多语言段落与互补的模型知识融合。实验结果表明，DKM-RAG减轻了生成中的语言偏好，提高了在各种语言环境中的一致性能。', 'title_zh': '探究多语言RAG系统的语言偏好'}
{'arxiv_id': 'arXiv:2502.11169', 'title': 'Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning', 'authors': 'Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, Ruichu Cai', 'link': 'https://arxiv.org/abs/2502.11169', 'abstract': 'Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post-training methods. Without additional training, LLMs typically enhance their mathematical reasoning abilities through inference scaling methods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: \\emph{understanding}, \\emph{planning}, \\emph{reflection}, \\emph{coding}, and \\emph{summary}. Each subset is further constrained to a small number of predefined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action subsets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model.', 'abstract_zh': '近年来，长推理链（Long Chain-of-Thoughts，简称 CoTs）引起了广泛的关注，这些长推理链被认为能够提高大语言模型（Large Language Models，简称 LLMs）的推理能力。这要求现有的缺乏生成长推理链能力的 LLMs 通过后训练方法来获得这样的能力。在没有额外训练的情况下，LLMs 通常通过 MCTS 之类的推理放大方法增强其数学推理能力，但它们受到动作空间庞大和搜索策略低效的阻碍，使得生成长推理链变得困难。为了应对这一问题，我们提出了一种限制动作空间并引导生成长推理链的精炼搜索策略。在我们提出的约束蒙特卡洛树搜索（Constrained Monte Carlo Tree Search，简称 C-MCTS）框架中，我们限制从受限动作空间中选择动作，动作空间被划分为五个不相交的子集：\\emph{理解}、\\emph{规划}、\\emph{反思}、\\emph{编码}和\\emph{总结}。每个子集进一步被限制为预定义提示的较小数量，而不是让 LLMs 任意生成动作。此外，通过结合关于动作集的先验知识（例如，人类似的人工划分的动作子集部分顺序以及预训练的过程奖励模型），我们进一步精炼搜索策略。这些策略共同工作，大大减少了长推理链的庞大搜索空间。在数学推理基准测试中的广泛评估表明，在零样本设置下，我们的方法使得 7B 模型的推理能力超过了 72B 模型。', 'title_zh': '利用约束蒙特卡洛树搜索生成可靠链式推理链的数学推理文章'}
{'arxiv_id': 'arXiv:2502.11101', 'title': 'CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation', 'authors': 'Kun-Hui Lee, Eunhwan Park, Donghoon Han, Seung-Hoon Na', 'link': 'https://arxiv.org/abs/2502.11101', 'abstract': 'Large Language Models (LLMs) excel across a variety of language tasks yet are constrained by limited input lengths and high computational costs. Existing approaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi) and sliding window mechanisms\\textemdash partially alleviate these issues but often require additional training or suffer from performance degradation with longer inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a method that enhances length normalization and reduces inference latency without any further training. Our approach leverages query-independent, offline caching to efficiently reuse a Context KV Cache Store. We address the amplification of abnormal token distributions problem by re-positioning cached keys and introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during pre-filling. Additionally, our Adaptive Positional Allocation Strategy dynamically reassigns cache positions to maximize the use of the available positional encoding range. Experiments on the Natural Questions and TriviaQA datasets demonstrate that CacheFocus outperforms alternative methods even when inputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its practical effectiveness for long-context LLMs. Moreover, even with large maximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows that it maintains consistent performance even as the number of documents increases, effectively managing long-text generation without degradation.', 'abstract_zh': '大语言模型（LLMs）在多种语言任务中表现出色，但受到输入长度和高计算成本的限制。现有的方法，如相对位置编码（例如RoPE、ALiBi）和滑动窗口机制，部分缓解了这些问题，但往往需要额外的训练或在处理更长的输入时会导致性能下降。本文我们引入了一种名为**CacheFocus**的方法，该方法能够在无需进一步训练的情况下提升长度归一化并降低推理延迟。我们的方法利用了与查询无关的离线缓存，有效地重用上下文KV缓存存储。通过重新定位缓存键并引入分层自适应缓存剪枝策略，我们解决了异常词汇分布放大的问题，在预填充阶段丢弃低相关缓存。此外，我们的自适应位置分配策略动态重新分配缓存位置，以最大限度地利用可用的位置编码范围。在自然问题和TriviaQA数据集上的实验表明，即使输入长度超过LLaMA-2的4K限制，CacheFocus也优于其他方法，突显了其在长上下文LLM中的实际有效性。此外，即使在具有较大最大输入长度的Qwen2中，CacheFocus的性能也保持一致，在文档数量增加的情况下能够有效管理长文本生成，而不出现性能下降。', 'title_zh': 'CacheFocus: 动态缓存重新定位以提高检索增强生成效率'}
{'arxiv_id': 'arXiv:2502.11100', 'title': 'Towards Achieving Concept Completeness for Unsupervised Textual Concept Bottleneck Models', 'authors': 'Milan Bhan, Yann Choho, Pierre Moreau, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot', 'link': 'https://arxiv.org/abs/2502.11100', 'abstract': 'Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. This paper proposes Complete Textual Concept Bottleneck Model (CT-CBM),a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. CT-CBM iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. CT-CBM achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.', 'abstract_zh': '文本概念瓶颈模型（TBMs）是为文本分类设计的可解释模型，它们在最终预测之前预测一组重要的概念。本文提出了一种全新的Complete Textual Concept Bottleneck Model（CT-CBM），该模型完全无监督地构建概念标签，使用小型语言模型消除预定义的人工标注概念和LLM注释的需要。CT-CBM通过并行残差连接迭代地针对性地添加瓶颈层的重要概念，构建完整概念基础，并解决下游分类泄露问题。CT-CBM在与竞争对手的对比中表现良好，提供了一种在不牺牲性能的前提下增强NLP分类器可解释性的有前景的解决方案。', 'title_zh': '实现无监督文本概念瓶颈模型的概念完整性'}
{'arxiv_id': 'arXiv:2502.11150', 'title': 'Surprisal Takes It All: Eye Tracking Based Cognitive Evaluation of Text Readability Measures', 'authors': 'Keren Gruteke Klein, Shachar Frenkel, Omer Shubi, Yevgeni Berzak', 'link': 'https://arxiv.org/abs/2502.11150', 'abstract': 'Text readability measures are widely used in many real-world scenarios and in NLP. These measures have primarily been developed by predicting reading comprehension outcomes, while largely neglecting what is perhaps the core aspect of a readable text: reading ease. In this work, we propose a new eye tracking based methodology for evaluating readability measures, which focuses on their ability to account for reading facilitation effects in text simplification, as well as for text reading ease more broadly. Using this approach, we find that existing readability formulas are moderate to poor predictors of reading ease. We further find that average per-word length, frequency, and especially surprisal tend to outperform existing readability formulas as measures of reading ease. We thus propose surprisal as a simple unsupervised alternative to existing measures.', 'abstract_zh': '文本的可读性度量在许多实际应用场景中以及自然语言处理（NLP）领域中被广泛应用。这些度量主要通过预测阅读理解结果来开发，但在很大程度上忽略了可读文本的核心方面之一：易读性。在这项工作中，我们提出了一种基于眼动的新方法用于评估可读性度量，该方法强调它们在文本简化中解释阅读促进效应的能力，以及更广泛地解释文本易读性方面的能力。通过这种方法，我们发现现有的可读性公式在预测易读性方面表现一般到较差。进一步的研究表明，平均每词长度、频率以及尤其是 surprisal 能够优于现有的可读性公式来衡量易读性。因此，我们建议 surprisal 作为一种简单且无需监督的可读性度量的替代方案。', 'title_zh': 'surprisal 理论一统天下：基于眼动追踪的认知文本可读性评估方法'}
{'arxiv_id': 'arXiv:2502.11095', 'title': 'A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions', 'authors': 'Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, Ling Chen', 'link': 'https://arxiv.org/abs/2502.11095', 'abstract': 'Mental health remains a critical global challenge, with increasing demand for accessible, effective interventions. Large language models (LLMs) offer promising solutions in psychotherapy by enhancing the assessment, diagnosis, and treatment of mental health conditions through dynamic, context-aware interactions. This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, highlighting the roles of LLMs in symptom detection, severity estimation, cognitive assessment, and therapeutic interventions. We present a novel conceptual taxonomy to organize the psychotherapy process into three core components: assessment, diagnosis, and treatment, and examine the challenges and advancements in each area. The survey also addresses key research gaps, including linguistic biases, limited disorder coverage, and underrepresented therapeutic models. Finally, we discuss future directions to integrate LLMs into a holistic, end-to-end psychotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.', 'abstract_zh': '心理健康仍然是一个关键的全球性挑战，对可获取且有效的干预措施的需求不断增加。大型语言模型（LLMs）在通过动态、情境感知的互动提升心理健康状况的评估、诊断和治疗方面提供了有前景的解决方案。本文综述了当前LLMs在心理治疗中的应用状况，强调了LLMs在症状检测、严重程度估计、认知评估及治疗干预中的作用。我们提出了一种新颖的概念分类体系，将心理治疗过程划分为三个核心组成部分：评估、诊断和治疗，并探讨了每个领域的挑战和进展。本文还探讨了几个关键研究缺口，包括语言偏见、疾病覆盖范围有限以及代表性不足的治疗模型。最后，我们讨论了将LLMs整合到一个全面、端到端的心理治疗框架中的未来方向，以应对心理健康状况的演变趋势，并促进更加包容性和个性化的护理。', 'title_zh': '大型语言模型在心理治疗中的综述：当前 landscape 和未来方向'}
{'arxiv_id': 'arXiv:2502.11137', 'title': 'Safety Evaluation of DeepSeek Models in Chinese Contexts', 'authors': 'Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Ning Wang, Zhenhong Long, Peijun Yang, Jiaojiao Zhao, Minjie Hua, Chaoyang Ma, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2502.11137', 'abstract': 'Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements.', 'abstract_zh': '近期，DeepSeek 系列模型凭借其卓越的推理能力以及开源策略，正在重塑全球人工智能格局。尽管这些优势显著，但这些模型在安全性方面仍存在重大缺陷。Cisco 的子公司 Robust Intelligence 与宾夕法尼亚大学合作进行的研究显示，当处理有害提示时，DeepSeek-R1 的攻击成功率达到了 100%。此外，多家安全公司和研究机构已确认该模型存在关键的安全漏洞。作为在中文和英文环境中均表现出强大性能的模型，DeepSeek 模型同样需要在两种语言环境中进行同等重要的安全评估。然而，当前的研究主要集中于英文环境下的安全评估，而在中文环境下的全面安全性评估方面则存在不足。针对这一不足，本研究引入了 CHiSafetyBench，这是一个专门用于中文环境的安全评估基准。该基准系统地评估了 DeepSeek-R1 和 DeepSeek-V3 在中文环境中的安全性，并展示了它们在不同安全类别中的表现。实验结果量化了这两种模型在中文环境中的安全性缺陷，为后续改进提供了关键见解。', 'title_zh': '《中文语境中 DeepSeek 模型的安全性评估》'}
{'arxiv_id': 'arXiv:2502.11090', 'title': 'SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks', 'authors': 'Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, Junlan Feng', 'link': 'https://arxiv.org/abs/2502.11090', 'abstract': "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", 'abstract_zh': '隨著大型语言模型（LLMs）的快速进步，LLMs的安全性已成为一个亟需精准评估的关键问题。当前的基准主要集中在单轮对话或单一脱缰攻击方法上对安全性进行评估。此外，这些基准并没有详细考虑LLM在识别和处理不安全信息方面的能力。为解决这些问题，我们提出了一种细粒度基准SafeDialBench，用于评估LLMs在多种脱缰攻击下的多轮对话安全性。具体来说，我们设计了一种两层分级安全分类体系，考虑了6个安全维度，并在22个对话场景下生成了超过4000个中英文多轮对话。我们采用了7种脱缰攻击策略，如引用攻击和目的反转，以提高生成对话数据集的质量。值得注意的是，我们构建了一种创新的评估框架，用于衡量LLMs在检测和处理不安全信息以及面对脱缰攻击保持一致性方面的能力。针对17种LLMs的实验结果表明，Yi-34B-Chat和GLM4-9B-Chat展示了优越的安全性能，而Llama3.1-8B-Instruct和o3-mini则显示出安全性缺陷。', 'title_zh': 'SafeDialBench：多轮对话中针对多样化脱管攻击的大语言模型细粒度安全基准'}
{'arxiv_id': 'arXiv:2502.11131', 'title': 'Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM', 'authors': 'Yuqi Liu, Yan Zheng', 'link': 'https://arxiv.org/abs/2502.11131', 'abstract': 'Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in this https URL', 'abstract_zh': '随着法律人工智能的快速发展，越来越多的研究关注其中一个最重要的法律人工智能任务——相似案例检索，尤其是利用语言模型。然而，在我们的论文中，我们尝试从学习排序的视角来提高当前模型的排名性能，而不是单纯依赖语言模型。具体而言，我们使用配对方法（RankSVM）作为分类器，替换全连接层，并结合常用的语言模型在相似案例检索数据集LeCaRDv1和LeCaRDv2上进行了实验。最终，我们得出结论，RankSVM在优化精确排名的情况下，一般能够提高LeCaRDv1和LeCaRDv2数据集上的检索性能。此外，它还能缓解由于类别不平衡而导致的过拟合问题。我们的代码可以在以下链接中获取：[提供链接处]', 'title_zh': '通过重新审视RankSVM提高相似案例检索排序性能'}
{'arxiv_id': 'arXiv:2502.11089', 'title': 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention', 'authors': 'Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng', 'link': 'https://arxiv.org/abs/2502.11089', 'abstract': 'Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.', 'abstract_zh': '长上下文建模对于下一代语言模型至关重要，但标准注意机制的高计算成本带来了重大的计算挑战。稀疏注意机制为提高效率同时保持模型能力提供了有前景的方向。我们提出了一种名为NSA的原生可训练稀疏注意机制，它将算法创新与硬件对齐的优化结合起来，以实现高效的长上下文建模。NSA采用动态层次稀疏策略，结合粗粒度的令牌压缩与细粒度的令牌选择，以保留全局上下文意识和局部精度。\n\n我们的方法在稀疏注意机制设计中引入了两项关键创新：（1）我们通过平衡算术强度的算法设计实现了显著的速度提升，并针对现代硬件进行了实现优化。（2）我们实现了端到端训练，减少了预训练计算量而不牺牲模型性能。如图1所示，实验表明使用NSA预训练的模型在通用基准测试、长上下文任务和基于指令的推理中均能保持或超越全注意力模型。同时，NSA在64k长度序列的解码、前向传播和反向传播过程中实现了显著的速度提升，验证了其在整个模型生命周期中的高效性。', 'title_zh': '本地稀疏注意机制：硬件对齐且本原可训练的稀疏注意机制'}
{'arxiv_id': 'arXiv:2502.11128', 'title': 'FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching', 'authors': 'Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin', 'link': 'https://arxiv.org/abs/2502.11128', 'abstract': "To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in this https URL.", 'abstract_zh': '为了推进连续值标记建模和时间一致性的强化，我们提出了一种名为FELLE的自回归模型，该模型结合了语言建模与标记级别流动匹配。通过利用语言模型的自回归特性以及流动匹配的生成能力，FELLE有效地预测了连续值标记（梅尔频谱图）。对于每个连续值标记，FELLE通过在流动匹配中引入上一步的信息，修改了通用的先验分布，从而提高了连续性并增强了稳定性。此外，为了提升合成质量，FELLE引入了一种从粗到细的流动匹配机制，按照语言模型输出进行分层生成连续值标记。实验结果表明，将流动匹配技术结合到自回归梅尔频谱图建模中具有巨大潜力，显著提升了图文转换（TTS）生成质量，如详细实验结果所示在[此处链接]。', 'title_zh': 'FELLE：基于令牌级粗细流水匹配的自回归语音合成'}
{'arxiv_id': 'arXiv:2502.11084', 'title': 'Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction', 'authors': 'Yuting Huang, Chengyuan Liu, Yifeng Feng, Chao Wu, Fei Wu, Kun Kuang', 'link': 'https://arxiv.org/abs/2502.11084', 'abstract': 'As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety.', 'abstract_zh': '随着大型语言模型（LLMs）在各种领域中的广泛应用，LLMs的安全性越来越受到关注，以避免其强大的能力被误用。现有的脱缰方法创造了一种强制指令遵循的情景，或者通过前缀或后缀标记搜索对抗提示，以手动或自动实现特定表示。然而，这些方法效率低下，并且具有明显的脱缰模式，远不能应对群体攻击LLMs的实际部署。在本文中，我们指出重写原始指令就可以实现脱缰，并发现这种重写方法是可学习且可迁移的。我们提出了重写以实现脱缰（R2J）方法，这是一种利用迭代探索LLMs弱点并通过自动改进攻击策略的可迁移黑盒脱缰方法。由于不引入额外特征，脱缰方法更为高效且难以识别。广泛的实验和分析证明了R2J的有效性，我们发现脱缰方法在仅需少量查询的情况下，在多个数据集和各种类型的模型之间也是可迁移的。我们希望我们的工作能进一步推动LLMs安全性的研究。', 'title_zh': '重写以越狱：发现可学习和可迁移的隐式有害指令'}
{'arxiv_id': 'arXiv:2502.11123', 'title': 'DuplexMamba: Enhancing Real-time Speech Conversations with Duplex and Streaming Capabilities', 'authors': 'Xiangyu Lu, Wang Xu, Haoyu Wang, Hongyun Zhou, Haiyan Zhao, Conghui Zhu, Tiejun Zhao, Muyun Yang', 'link': 'https://arxiv.org/abs/2502.11123', 'abstract': 'Real-time speech conversation is essential for natural and efficient human-machine interactions, requiring duplex and streaming capabilities. Traditional Transformer-based conversational chatbots operate in a turn-based manner and exhibit quadratic computational complexity that grows as the input size increases. In this paper, we propose DuplexMamba, a Mamba-based end-to-end multimodal duplex model for speech-to-text conversation. DuplexMamba enables simultaneous input processing and output generation, dynamically adjusting to support real-time streaming. Specifically, we develop a Mamba-based speech encoder and adapt it with a Mamba-based language model. Furthermore, we introduce a novel duplex decoding strategy that enables DuplexMamba to process input and generate output simultaneously. Experimental results demonstrate that DuplexMamba successfully implements duplex and streaming capabilities while achieving performance comparable to several recently developed Transformer-based models in automatic speech recognition (ASR) tasks and voice assistant benchmark evaluations.', 'abstract_zh': '实时语音对话对自然高效的人机交互至关重要，需要具备双工和流式传输能力。传统的基于Transformer的对话聊天机器人以轮流方式进行操作，并且随着输入量的增加，其计算复杂度会呈二次增长。本文提出了一种基于Mamba的端到端多模态双工模型DuplexMamba，该模型能够同时处理输入和生成输出，并能够动态调整以支持实时流式传输。具体而言，我们开发了基于Mamba的语音编码器，并将其与基于Mamba的语言模型进行适配。此外，我们还引入了一种新颖的双工解码策略，使DuplexMamba能够同时处理输入并生成输出。实验结果表明，DuplexMamba成功地实现了双工和流式传输能力，并且在自动语音识别（ASR）任务和语音助手基准测试评估中，其性能与几种最近开发的基于Transformer的模型相当。', 'title_zh': 'DuplexMamba：通过 duplex 和流式传输能力提升实时语音对话性能'}
{'arxiv_id': 'arXiv:2502.11083', 'title': 'Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks', 'authors': 'Yuanjie Lyu, Chao Zhang, Yuhao Chen, Yong Chen, Tong Xu', 'link': 'https://arxiv.org/abs/2502.11083', 'abstract': 'In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the "Chain of Models" approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.', 'abstract_zh': '在检索增强生成（RAG）和基于代理的框架中，广泛采用了一种“模型链”方法，其中多个专门化模型依次处理不同的子任务。这种方法有效，但会增加资源需求，因为每个模型都需要单独部署。最近的进展试图通过提示调优来解决这一问题，这使共享基础模型能够通过最小的参数变化适应多种任务。然而，一个关键挑战仍然存在：在模型之间作为纯文本传递的中间输出，在推断过程中需要重新计算隐藏状态（例如，在Transformer中的Key和Value（KV）状态）。在本文中，我们引入了一种名为FTHSS的新颖提示调优方法，该方法使模型能够共享KV隐藏状态，从而消除冗余的前向传递并减少KV缓存存储空间。通过在训练过程中修改输入和注意掩码，FTHSS使得模型能够在单轮和多轮场景中高效利用前模型的KV隐藏状态。在四个任务上的实验结果表明，FTHSS在保持传统模型链性能的同时提高了推断效率。', 'title_zh': '将基于生成的任务中的协作模型链简化为单个前向传递过程'}
{'arxiv_id': 'arXiv:2502.11116', 'title': 'Gumbel Reranking: Differentiable End-to-End Reranker Optimization', 'authors': 'Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2502.11116', 'abstract': 'RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these models remains challenging due to the scarcity of annotated query-document pairs. Existing distillation-based approaches suffer from training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we reframe the reranking process as an attention-mask problem and propose Gumbel Reranking, an end-to-end training framework for rerankers aimed at minimizing the training-inference gap. In our approach, reranker optimization is reformulated as learning a stochastic, document-wise Top-$k$ attention mask using the Gumbel Trick and Relaxed Top-$k$ Sampling. This formulation enables end-to-end optimization by minimizing the overall language loss. Experiments across various settings consistently demonstrate performance gains, including a 10.4\\% improvement in recall on HotpotQA for distinguishing indirectly relevant documents.', 'abstract_zh': 'RAG系统依赖于重排序器来识别相关文档。然而，由于标注的查询-文档对稀少，这些模型的微调依然具有挑战性。现有的基于蒸馏的方法在训练和推理之间存在不对齐问题，无法捕捉候选文档之间的依赖关系。为了克服这些限制，我们将重排序过程重新定义为注意力屏蔽问题，并提出了Gumbel重排序，这是一种针对重排序器的端到端训练框架，旨在最小化训练和推理之间的差距。在我们的方法中，重排序优化被重新定义为使用Gumbel技巧和松弛Top-$k$采样学习一个随机的、文档级别的Top-$k$注意力屏蔽。这种形式化方法通过最小化总体语言损失实现端到端优化。在各种设置下的实验一致地展示了性能提升，包括在HotpotQA数据集上区分间接相关文档时召回率提高10.4%。', 'title_zh': '格伯尔重新排序：可微端到端重新排序优化'}
{'arxiv_id': 'arXiv:2502.11078', 'title': 'DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling', 'authors': 'Aili Chen, Chengyu Du, Jiangjie Chen, Jinghan Xu, Yikai Zhang, Siyu Yuan, Zulong Chen, Liangyue Li, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2502.11078', 'abstract': "To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.", 'abstract_zh': '为了促进个性化应用，如推荐系统和用户行为预测，近期的研究越来越多地采用大型语言模型（LLMs）进行可读性人物建模。在动态的现实场景中，有效的人物建模需要充分利用流式行为数据进行持续的人物优化。然而，现有的方法，无论是重生成人物还是逐渐扩展新的行为，往往难以实现人物质量或未来行为预测准确性的持续提升。为了解决这一问题，我们提出了一种新的动态人物建模方法DEEPER，可以实现持续的人物优化。具体而言，我们通过迭代的强化学习框架增强了模型的方向搜索能力，使其能够自动识别有效的更新方向，并利用用户行为与模型预测之间的差异来优化人物。针对涵盖10个领域、4800名用户的动态人物建模实验表明，DEEPER在人物优化方面具有卓越的能力，四个更新轮次后平均减少了32.2%的用户行为预测误差，比最佳基线模型高出22.92%。', 'title_zh': '深入洞察您的用户：面向persona的动态细化建模'}
{'arxiv_id': 'arXiv:2502.11075', 'title': 'Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models', 'authors': 'Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen', 'link': 'https://arxiv.org/abs/2502.11075', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在自然语言处理任务中展现出了令人印象深刻的性能，例如文本生成和语义理解。然而，在基本算术、数值检索和大小比较等数值推理任务上，它们的表现仍然出奇地差。这种差距源于它们依赖于表面级的统计模式，而不是理解数字作为连续量的意义。现有基准主要关注语言能力或结构化的数学问题解决，忽视了在现实场景中所需的基本数值推理。为了弥合这一差距，我们提出NumericBench，这是一个全面的基准测试，用于评估六种基本的数值能力：数字识别、算术运算、上下文检索、比较、总结和逻辑推理。NumericBench 包含从合成数字列表到爬取的现实世界数据的数据集，涵盖了长上下文、噪音和多步推理等挑战。对最先进的LLM模型，包括GPT-4和DeepSeek的广泛实验揭示了其在数值推理上的持续薄弱性，突显出提高感知数值的语言建模的迫切需要。该基准测试已发布在以下网址：this https URL。', 'title_zh': '暴露数值能力差距：评估大型语言模型基本数值能力的基准'}
{'arxiv_id': 'arXiv:2502.11115', 'title': 'Are Generative Models Underconfident? An Embarrassingly Simple Quality Estimation Approach', 'authors': 'Tu Anh Dinh, Jan Niehues', 'link': 'https://arxiv.org/abs/2502.11115', 'abstract': 'Quality Estimation (QE) is estimating the quality of model output when the ground truth reference is not available. Looking at model uncertainty from its own output probabilities is the most trivial and low-effort way to estimate the output quality. However, for generative model, output probabilities might not be the best quality estimator. At an output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower token probability does not necessarily mean lower output quality. In other words, the model can be considered underconfident. In this paper, we propose a QE approach called Dominant Mass Probability (DMP}, that boosts the model confidence in cases where there are multiple viable output options. We show that, with no increase in complexity, DMP is notably better than sequence probability when estimating the quality of different models (Whisper, Llama, etc.) on different tasks (translation, summarization, etc.). Compared to sequence probability, DMP achieves on average +0.208 improvement in Pearson correlation to ground-truth quality.', 'abstract_zh': '质量估计（QE）是在没有地面真实参考的情况下，估计模型输出质量的一种方法。通过观察模型自身的输出概率来估算输出质量是最简单且低投入的方法。然而，对于生成模型而言，输出概率可能并不是最佳的质量估计器。在输出的每个步骤中，可能存在多个正确选项，从而使概率分布扩散得更开。因此，单个标记的概率较低并不一定意味着输出质量较低。换句话说，模型可以被认为不够自信。在本文中，我们提出了一种称为主导概率质量估计（Dominant Mass Probability, DMP）的方法，用于在存在多个可行输出选项的情况下提高模型的自信度。我们展示了，在无需增加复杂度的情况下，DMP在不同模型（如Whisper、Llama等）和不同任务（如翻译、摘要等）上估计算法真实质量时，显著优于序列概率。相较于序列概率，DMP在皮尔逊相关性上平均提高了0.208。', 'title_zh': '生成模型是否过于谦虚？一种令人惊讶-simple的质量估计方法'}
{'arxiv_id': 'arXiv:2502.11073', 'title': 'Demystifying Hateful Content: Leveraging Large Multimodal Models for Hateful Meme Detection with Explainable Decisions', 'authors': 'Ming Shan Hee, Roy Ka-Wei Lee', 'link': 'https://arxiv.org/abs/2502.11073', 'abstract': 'Hateful meme detection presents a significant challenge as a multimodal task due to the complexity of interpreting implicit hate messages and contextual cues within memes. Previous approaches have fine-tuned pre-trained vision-language models (PT-VLMs), leveraging the knowledge they gained during pre-training and their attention mechanisms to understand meme content. However, the reliance of these models on implicit knowledge and complex attention mechanisms renders their decisions difficult to explain, which is crucial for building trust in meme classification. In this paper, we introduce IntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for hateful meme classification with explainable decisions. IntMeme addresses the dual challenges of improving both accuracy and explainability in meme moderation. The framework uses LMMs to generate human-like, interpretive analyses of memes, providing deeper insights into multimodal content and context. Additionally, it uses independent encoding modules for both memes and their interpretations, which are then combined to enhance classification performance. Our approach addresses the opacity and misclassification issues associated with PT-VLMs, optimizing the use of LMMs for hateful meme detection. We demonstrate the effectiveness of IntMeme through comprehensive experiments across three datasets, showcasing its superiority over state-of-the-art models.', 'abstract_zh': '以下是经过学术规范翻译的内容：\n\n仇恨梗图像检测作为一项多模态任务，由于解读隐含的仇恨信息和梗图像上下文线索的复杂性，构成了一个显著的挑战。以往的方法在预训练的视觉-语言模型（PT-VLMs）的基础上进行微调，借助其在预训练中获得的知识和注意力机制来理解梗图像的内容。然而，这些模型对隐含知识和复杂注意力机制的依赖性使得其决策难以解释，这对于建立对梗图像分类的信任至关重要。本文中，我们提出了IntMeme，这是一种新颖的框架，利用大型多模态模型（LMMs）来进行具有可解释性的仇恨梗图像分类。IntMeme旨在同时提升梗图像审核的准确性和可解释性。该框架利用LMMs生成类似于人类的、具有解释性的梗图像分析，提供对多模态内容和上下文的更深入洞察。此外，它还独立地对梗图像及其解释进行编码，然后结合使用以提高分类性能。我们的方法解决了PT-VLMs的透明度不足和误分类问题，优化了LMMs在仇恨梗图像检测中的应用效果。我们通过在三个数据集上进行全面的实验，展示了IntMeme的有效性，并证明了其在性能上优于最先进的模型。', 'title_zh': '解开仇恨内容的面纱：利用大规模多模态模型进行具有可解释决策的仇恨 memes 检测'}
{'arxiv_id': 'arXiv:2502.11114', 'title': 'Beyond Pairwise: Global Zero-shot Temporal Graph Generation', 'authors': 'Alon Eirew, Kfir Bar, Ido Dagan', 'link': 'https://arxiv.org/abs/2502.11114', 'abstract': "Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, in which event pairs are considered individually, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph at once, then applies transitive constraints optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method significantly outperforms existing zero-shot approaches while achieving competitive performance with supervised models.", 'abstract_zh': '时间关系提取（TRE）是自然语言处理（NLP）中的一个基础任务，涉及在文档中识别事件之间的 temporal 关系。尽管大型语言模型（LLMs）取得了进展，但它们在 TRE 中的应用仍有限制。大多数现有方法依赖于成对分类，即单独考虑事件对，这导致计算效率低下以及生成时间图中的全局一致性不足。在这项工作中，我们提出了一种新颖的零样本方法来进行 TRE，该方法可以一次性生成文档的完整时间图，然后通过传递约束优化来细化预测并确保关系之间的时序一致性。此外，我们引入了 OmniTemp 数据集，该数据集对文档中所有目标事件对进行了完全注解。通过实验和分析，我们证明了该方法显著优于现有零样本方法，同时在性能上与监督模型竞争。', 'title_zh': '超越成对学习：全局零样本时间图生成'}
{'arxiv_id': 'arXiv:2502.11066', 'title': 'CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment', 'authors': 'Nura Aljaafari, Danilo S. Carvalho, André Freitas', 'link': 'https://arxiv.org/abs/2502.11066', 'abstract': "Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.", 'abstract_zh': '大型语言模型（LLMs）在组合泛化方面存在问题，限制了它们系统地结合已有知识以解释新颖输入的能力。尽管结构修改、微调和数据增强可以提高组合性，但它们往往在适应性、可扩展性或实际数据上的效果有限。为解决这一问题，我们提出了一种名为CARMA的干预措施，该措施在保持微调性能的同时，增强LLMs的组合推理稳定性与鲁棒性。CARMA采用互信息正则化和逐层稳定性约束，以减轻特征碎片化问题，确保结构化的表示在层间及层内持续存在。我们分别在逆字典建模和情感分类任务上评估CARMA，测量其对语义一致性的影响、性能稳定性和对词形扰动的鲁棒性。结果显示，CARMA降低了微调引入的变量性，稳定了token表示，并提升了组合推理能力。尽管其有效性在不同架构上有所差异，CARMA的关键优势在于强化已学习的结构而非引入新功能，这使其成为一种可扩展的辅助方法。这些发现表明，将CARMA与微调结合使用，可以在保持特定任务性能的同时改善LLMs的组合泛化能力。', 'title_zh': 'CARMA：通过高级正则化和互信息对齐提高大语言模型的组合性'}
{'arxiv_id': 'arXiv:2502.11062', 'title': 'Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection', 'authors': 'Yang Zhao, Li Du, Xiao Ding, Yangou Ouyang, Hepeng Wang, Kai Xiong, Jinglong Gao, Zhouhao Sun, Dongliang Xu, Yang Qing, Dongchen Li, Bing Qin, Ting Liu', 'link': 'https://arxiv.org/abs/2502.11062', 'abstract': 'Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce G2IS (Gradient-based Graph Instruction Selection), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies between instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models.', 'abstract_zh': '大型语言模型（LLMs）由于其通过指令调优展现出的出色泛化能力，在各个行业中表现出巨大的潜力。然而，特定领域的数据不足显著限制了其在专业化任务中的性能。目前，现有的方法主要集中在从与目标领域相似的一般数据集中选择训练数据，但往往未能考虑到指令之间的联合分布，导致学习效率低下和知识转移效果不佳。为应对这些挑战，我们提出了G2IS（基于梯度的图指令选择）方法，这是一种创新的方法，通过构建混合梯度图来捕捉指令之间的联合分布及互依性。通过考虑指令之间的关系，G2IS 提高了领域适应的效率。此外，我们还提出了一种梯度步行算法来细化数据选择过程，从而增强训练的有效性和效率。我们的实验结果表明，G2IS 在各种领域适应任务中均优于传统方法，特别是在复杂且数据稀缺的情境下产生了显著的性能提升。这些结果突显了G2IS 在推动大型领域特定模型发展的潜力。', 'title_zh': '超越相似性：一种基于梯度的图方法用于指令调优数据选择'}
{'arxiv_id': 'arXiv:2502.11113', 'title': 'Valuable Hallucinations: Realizable Non-realistic Propositions', 'authors': 'Qiucheng Chen, Bo Wang', 'link': 'https://arxiv.org/abs/2502.11113', 'abstract': 'This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs),addressing a gap in the existing this http URL provide a systematic definition and analysis of hallucination value,proposing methods for enhancing the value of this http URL contrast to previous works,which often treat hallucinations as a broad flaw,we focus on the potential value that certain types of hallucinations can offer in specific this http URL in LLMs generally refer to the generation of unfaithful, fabricated,inconsistent,or nonsensical this http URL than viewing all hallucinations negatively,this paper gives formal representations and manual judgments of "valuable hallucinations" and explores how realizable non-realistic propositions-ideas that are not currently true but could be achievable under certain conditions-can have constructive this http URL present experiments using the Qwen2.5 model and HalluQA dataset, employing ReAct prompting (which involves reasoning, confidence assessment, and answer verification) to control and optimize hallucinations. Our findings show that ReAct prompting results in a reduction in overall hallucinations and an increase in the proportion of valuable this http URL results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability.', 'abstract_zh': '本文首次为大型语言模型（LLM）中的有价值的幻觉提供了正式定义，填补了现有研究中的空白。本文系统地定义并分析了幻觉的价值，并提出了增强幻觉价值的方法。与以往的工作通常将幻觉视为一种广泛的缺陷不同，本文聚焦于某些类型幻觉在特定情况下可能提供的潜在价值。在LLM中，幻觉通常指的是生成不忠实、虚构、不一致或无意义的内容。本文不仅从消极的角度看待所有幻觉，还提供了“有价值的幻觉”的正式表示和人工判断，并探讨了不现实的命题（即当前不真实但可能在某些条件下实现的观念）如何具有建设性价值。本文使用Qwen2.5模型和HalluQA数据集进行了实验，采用ReAct提示（涉及推理、信心评估和答案验证）来控制和优化幻觉。实验结果表明，通过系统地控制幻觉可以提高其实用性，而不牺牲事实上的可靠性。', 'title_zh': '有价值的幻象：可实现的非现实命题'}
{'arxiv_id': 'arXiv:2502.11061', 'title': 'Déjà Vu? Decoding Repeated Reading from Eye Movements', 'authors': 'Yoav Meiri, Omer Shubi, Cfir Avraham Hadar, Ariel Kreisberg Nitzav, Yevgeni Berzak', 'link': 'https://arxiv.org/abs/2502.11061', 'abstract': 'Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.', 'abstract_zh': '无论它是你最爱的小说、新闻文章、烹饪食谱，还是学术论文——在许多日常情况下，我们往往会多次阅读同一文本。在这项研究中，我们探讨是否可以通过自动分析阅读者的注视模式，来确定读者是否此前已经接触过该文本。我们提出了这项任务的两种变体，并使用基于特征和神经网络模型的方法取得了显著的成功。此外，我们介绍了一种通用策略，通过从认知模型生成的注视模式的机器生成模拟来增强这些模型。最后，我们展示了模型性能的分析结果，这些结果一方面揭示了模型所使用的相关信息，另一方面利用预测建模作为工具，更好地刻画重复阅读中记忆力的作用。我们的研究推进了对阅读过程中注视模式捕捉先前文本暴露的记忆效应范围和方式的理解，并为涉及重复阅读预测建模的未来应用奠定了基础。', 'title_zh': '曾经相见？基于眼动解码重复阅读效应'}
{'arxiv_id': 'arXiv:2502.11108', 'title': 'Knowledge Graph-Driven Retrieval-Augmented Generation: Integrating Deepseek-R1 with Weaviate for Advanced Chatbot Applications', 'authors': 'Alexandru Lecu, Adrian Groza, Lezan Hawizy', 'link': 'https://arxiv.org/abs/2502.11108', 'abstract': 'Large language models (LLMs) have significantly advanced the field of natural language generation. However, they frequently generate unverified outputs, which compromises their reliability in critical applications. In this study, we propose an innovative framework that combines structured biomedical knowledge with LLMs through a retrieval-augmented generation technique. Our system develops a thorough knowledge graph by identifying and refining causal relationships and named entities from medical abstracts related to age-related macular degeneration (AMD). Using a vector-based retrieval process and a locally deployed language model, our framework produces responses that are both contextually relevant and verifiable, with direct references to clinical evidence. Experimental results show that this method notably decreases hallucinations, enhances factual precision, and improves the clarity of generated responses, providing a robust solution for advanced biomedical chatbot applications.', 'abstract_zh': '大语言模型（LLMs）在自然语言生成领域取得了显著进展。然而，它们通常会生成未经验证的输出，这在关键应用中降低了它们的可靠性。本研究提出了一种创新框架，通过检索增强生成技术将结构化的生物医学知识与LLMs相结合。我们的系统通过识别和提炼与年龄相关黄斑变性（AMD）相关的医学摘要中的因果关系和命名实体，构建了一个详尽的知识图谱。利用基于向量的检索过程和本地部署的语言模型，我们的框架生成的响应不仅是上下文相关的，而且是可以验证的，并直接引用了临床证据。实验结果表明，这种方法显著减少了幻觉，提高了事实准确性，并改善了生成响应的清晰度，为高级生物医学聊天机器人应用提供了稳健的解决方案。', 'title_zh': '知识图谱驱动的检索增强生成：将 Deepseek-R1 与 Weaviate 集成应用于高级聊天机器人应用'}
{'arxiv_id': 'arXiv:2502.11054', 'title': 'Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models', 'authors': 'Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.11054', 'abstract': "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at this https URL to facilitate further research in this critical domain.", 'abstract_zh': '多轮 jailbreak 攻击通过反复迭代与大型语言模型（LLMs）进行对话，模拟现实世界中的人际互动，从而揭示关键的安全漏洞。然而，现有方法往往难以在语义连贯性与攻击效果之间找到平衡，导致语义漂移过于温和或检测规避效果不佳。为解决这一挑战，我们提出了一种名为 Reasoning-Augmented Conversation（增强推理对话）的新颖多轮 jailbreak 架构，该架构将有害的查询重新表述为安全的推理任务，并利用 LLM 强大的推理能力来破坏安全一致性。具体而言，我们引入了一种攻击状态机框架，以系统地建模问题转换和迭代推理，确保多次对话中查询生成的一致性。基于此框架，我们设计了收益导向的探索、自我对弈和拒绝反馈模块，以保持攻击语义、增强有效性并维持基于推理的攻击进展。在多种 LLM 上进行的大量实验表明，RACE 在复杂的对话场景中实现了最先进的攻击效果，攻击成功率（ASR）最高可提升 96%。值得注意的是，我们的方法在对抗领先商用模型 OpenAI o1 和 DeepSeek R1 时分别实现了 82% 和 92% 的攻击成功率，突显了其强大的能力。我们已在此 https://XXX 地址上发布了我们的代码，以促进对该关键领域的进一步研究。', 'title_zh': '增强推理的多轮对话对抗大规模语言模型的妖魔化攻击'}
{'arxiv_id': 'arXiv:2502.11104', 'title': 'Enhancing Cross-Tokenizer Knowledge Distillation with Contextual Dynamical Mapping', 'authors': 'Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.11104', 'abstract': 'Knowledge Distillation (KD) has emerged as a prominent technique for model compression. However, conventional KD approaches primarily focus on homogeneous architectures with identical tokenizers, constraining their applicability in cross-architecture scenarios. As for the cross-tokenizer KD, the differences in the tokenizers give rise to two fundamental challenges: (1) sequence misalignment caused by divergent tokenization strategies, and (2) mismatched vocabulary size and composition. While existing probability-matching methods attempt to address these issues, their efficacy remains limited due to suboptimal alignment in both the sequence and vocabulary aspects. To overcome these limitations, we propose Contextual Dynamic Mapping (CDM), a novel cross-tokenizer distillation framework that employs contextual information to enhance sequence alignment precision and dynamically improves vocabulary mapping. We evaluated the effectiveness of our approach across five advanced and widely-used model families (i.e, LLama3, Phi3, Gemma2, OPT and Qwen2), which were configured into three distinct teacher-student pairs. Our method shows significant advantages over existing cross-tokenizer distillation baselines across diverse benchmarks, including instruction-following, code generation and math. Notably, our analysis reveals that combining conventional same-tokenizer distillation and cross-tokenizer distillation through CDM yields further performance improvements. The code is available at this https URL', 'abstract_zh': '知识蒸馏（KD）已成为模型压缩的重要技术。然而，传统的KD方法主要关注具有相同分词器的同构架构，这限制了它们在跨架构场景中的应用。对于跨分词器的KD，不同的分词策略导致了两个基本挑战：（1）由于分词策略不同而引起的序列对齐偏差，（2）词汇表大小和组成的不匹配。尽管现有的概率对齐方法试图解决这些问题，但由于序列和词汇对齐方面的次优化，其效果仍然有限。为克服这些限制，我们提出了上下文动态映射（CDM），这是一种新颖的跨分词器蒸馏框架，通过利用上下文信息来增强序列对齐精度，并动态优化词汇表映射。我们在五种先进的常用模型家族（即LLama3、Phi3、Gemma2、OPT和Qwen2）上进行了模型配置，并构建了三个不同的教师-学生对，评估了我们的方法在多种基准上的有效性，包括指令遵循、代码生成和数学问题。值得注意的是，我们的分析表明，通过CDM将传统的同分词器蒸馏和跨分词器蒸馏相结合，可以进一步提高性能。代码可在如下链接获取：[此处链接]', 'title_zh': '使用上下文动态映射增强跨分词器知识蒸馏'}
{'arxiv_id': 'arXiv:2502.11051', 'title': 'MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models', 'authors': 'Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11051', 'abstract': 'Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient descent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.', 'abstract_zh': '近年来，机器卸载（Machine Unlearning, MU）的研究取得了进展，提出了针对深神经网络中私有或敏感信息的有选择性移除的解决方案。然而，对于多模态大型语言模型（Multimodal Large Language Models, MLLMs）的MU仍处于起步阶段。因此，我们提出在MLLM时代重新定义多模态MU的任务，其目标是在卸载过程中仅删除与给定实体相关的视觉模式，同时保留原始语言模型骨干中对应的文本知识。此外，我们开发了一种新的几何约束梯度下降方法MMUnlearner。在卸载过程中，MMUnlearner通过联合限制剩余的概念和文本知识更新MLLM的权重，从而保留对非目标知识至关重要的参数。广泛实验结果显示，MMUnlearner在所有评估维度上均优于通过梯度上升（Gradient Ascent, GA）或负偏好优化（Negative Preference Optimization, NPO）直接使用VQA数据微调MLLM的基线方法。我们的代码将在接受后开源。', 'title_zh': 'MMUNLEARNER：在多模态大型语言模型时代重构多模态机器遗忘技术'}
{'arxiv_id': 'arXiv:2502.11101', 'title': 'CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation', 'authors': 'Kun-Hui Lee, Eunhwan Park, Donghoon Han, Seung-Hoon Na', 'link': 'https://arxiv.org/abs/2502.11101', 'abstract': 'Large Language Models (LLMs) excel across a variety of language tasks yet are constrained by limited input lengths and high computational costs. Existing approaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi) and sliding window mechanisms\\textemdash partially alleviate these issues but often require additional training or suffer from performance degradation with longer inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a method that enhances length normalization and reduces inference latency without any further training. Our approach leverages query-independent, offline caching to efficiently reuse a Context KV Cache Store. We address the amplification of abnormal token distributions problem by re-positioning cached keys and introducing Layer-Adaptive Cache Pruning to discard low-relevance caches during pre-filling. Additionally, our Adaptive Positional Allocation Strategy dynamically reassigns cache positions to maximize the use of the available positional encoding range. Experiments on the Natural Questions and TriviaQA datasets demonstrate that CacheFocus outperforms alternative methods even when inputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its practical effectiveness for long-context LLMs. Moreover, even with large maximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows that it maintains consistent performance even as the number of documents increases, effectively managing long-text generation without degradation.', 'abstract_zh': '大型语言模型（LLMs）在各种语言任务中表现出色，但受限于有限的输入长度和高计算成本。现有的方法，如相对位置编码（例如，RoPE、ALiBi）和滑动窗口机制，部分缓解了这些问题，但通常需要额外的训练，或者在处理更长的输入时会性能下降。在这篇文章中，我们介绍了一种名为**CacheFocus**的方法，该方法能够在不需要进一步训练的情况下提高长度归一化并减少推理延迟。我们的方法利用查询无关的离线缓存，高效地重用Context KV缓存存储。我们通过重新定位缓存键并引入层自适应缓存剪枝策略来解决异常token分布放大的问题，该策略在预填充过程中丢弃低相关性的缓存。此外，我们的自适应位置分配策略动态重新分配缓存位置，以最大化利用可用的位置编码范围。在Natural Questions和TriviaQA数据集上的实验表明，即使输入长度超过LLaMA-2模型的4K限制，CacheFocus仍优于其他替代方法，凸显了其在长上下文LLMs中的实际有效性。此外，即使在Qwen2的最大输入长度较大时，CacheFocus的表现也能够保持一致，即使文档数量增加，仍然能够有效地管理长文本生成而不出现性能下降。', 'title_zh': 'CacheFocus: 动态缓存重新定位以实现高效的检索增强生成'}
{'arxiv_id': 'arXiv:2502.11100', 'title': 'Towards Achieving Concept Completeness for Unsupervised Textual Concept Bottleneck Models', 'authors': 'Milan Bhan, Yann Choho, Pierre Moreau, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot', 'link': 'https://arxiv.org/abs/2502.11100', 'abstract': 'Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. This paper proposes Complete Textual Concept Bottleneck Model (CT-CBM),a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. CT-CBM iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. CT-CBM achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.', 'abstract_zh': 'Textual 概念瓶颈模型（TBMs）是为文本分类设计的可解释模型，它们在做出最终预测之前会预测一组显着的概念。本文提出了一种全新的 Complete 文本概念瓶颈模型（CT-CBM），该模型采用小型语言模型以完全无监督的方式构建概念标签，从而消除预定义的人工标注概念和大型语言模型（LLM）注释的需求。CT-CBM 通过并行残差连接迭代地针对并增加瓶颈层中的重要概念，构建完整概念基础，并通过这种方法解决了下游分类中的泄漏问题。CT-CBM 在与其他竞争模型的对比中表现出良好的性能，为在不牺牲性能的情况下提高自然语言处理（NLP）分类器的可解释性提供了一种有前途的解决方案。', 'title_zh': '实现无监督文本概念瓶颈模型的概念完备性'}
{'arxiv_id': 'arXiv:2502.11095', 'title': 'A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions', 'authors': 'Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, Ling Chen', 'link': 'https://arxiv.org/abs/2502.11095', 'abstract': 'Mental health remains a critical global challenge, with increasing demand for accessible, effective interventions. Large language models (LLMs) offer promising solutions in psychotherapy by enhancing the assessment, diagnosis, and treatment of mental health conditions through dynamic, context-aware interactions. This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, highlighting the roles of LLMs in symptom detection, severity estimation, cognitive assessment, and therapeutic interventions. We present a novel conceptual taxonomy to organize the psychotherapy process into three core components: assessment, diagnosis, and treatment, and examine the challenges and advancements in each area. The survey also addresses key research gaps, including linguistic biases, limited disorder coverage, and underrepresented therapeutic models. Finally, we discuss future directions to integrate LLMs into a holistic, end-to-end psychotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.', 'abstract_zh': '心理健康仍然是一个关键的全球性挑战，对易于获取、有效的干预措施的需求不断增加。大型语言模型（LLMs）通过动态、上下文感知的交互在心理治疗中提供了有希望的解决办法，从而增强对心理健康状况的评估、诊断和治疗。本文综述了当前LLM在心理治疗中的应用状况，强调了LLMs在症状检测、严重程度估计、认知评估和治疗方法中的作用。我们提出了一种新的概念分类法，将心理治疗过程划分为三个核心组成部分：评估、诊断和治疗，并在每个领域探讨了存在的挑战和进展。此外，本文还指出了关键的研究空白，包括语言偏见、疾病覆盖不足以及代表性不足的治疗方法。最后，我们讨论了将LLMs整合到一个全面的、端到端的心理治疗框架中的未来方向，以应对心理健康状况的不断变化，并促进更加包容和个性化的护理。', 'title_zh': '大型语言模型在心理治疗中的综述：现状与未来方向'}
{'arxiv_id': 'arXiv:2502.11090', 'title': 'SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks', 'authors': 'Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, Junlan Feng', 'link': 'https://arxiv.org/abs/2502.11090', 'abstract': "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展，LLMs 的安全性已成为一个关键的关切点，需要精确评估。当前的基准主要专注于单一回合对话或单一脱戒攻击方法来评估安全性。此外，这些基准未详细考虑LLMs识别和处理不安全信息的能力。为了解决这些问题，我们提出了一种细粒度基准 SafeDialBench，用于评估LLMs在多回合对话中面对各种脱戒攻击时的安全性。具体来说，我们设计了一种两级分层安全分类法，考虑了6个安全维度，并在22种对话场景下生成了超过4000个多回合对话，涵盖中英文。我们采用7种脱戒攻击策略，如参考攻击和目的反转，以提高对话生成数据集的质量。值得一提的是，我们构建了一种创新性的LLMs评估框架，衡量其在检测和处理不安全信息以及在面对脱戒攻击时保持一致性的能力。实验结果表明，Yi-34B-Chat 和 GLM4-9B-Chat 展示出更优秀安全性表现，而Llama3.1-8B-Instruct 和 o3-mini 则表现出安全性漏洞。', 'title_zh': 'SafeDialBench：一种针对多轮对话中各种脱勾攻击的大规模语言模型细粒度安全性基准测试'}
{'arxiv_id': 'arXiv:2502.11028', 'title': 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models', 'authors': 'Prateek Chhikara', 'link': 'https://arxiv.org/abs/2502.11028', 'abstract': 'Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.', 'abstract_zh': '大型语言模型（LLMs）在多种任务中表现出色，但在置信度校准方面仍面临挑战。模型的欠校准或过校准（即误校准）在高风险应用中尤其令人担忧。本文通过实证研究探讨了模型大小、干扰项和问题类型如何影响置信度对齐。我们提出了一种评估框架来衡量过校准情况，并研究了选择题格式是否能缓解或加剧误校准。研究发现，尽管较大模型（如GPT-4o）整体上更易于校准，但它们更容易受到干扰，而较小模型则从答案选项中受益更多，但在不确定性估计方面存在困难。与以往研究主要报告误校准趋势不同，我们提供了关于失败模式和加剧过校准条件的具体建议。这些发现突显了需要采取校准意识干预措施以及改进不确定性估计方法的重要性。', 'title_zh': '注意信心差距：大型语言模型中的过度自信、校准问题及干扰项效应'}
{'arxiv_id': 'arXiv:2502.11089', 'title': 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention', 'authors': 'Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, Wangding Zeng', 'link': 'https://arxiv.org/abs/2502.11089', 'abstract': 'Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.', 'abstract_zh': '长时间上下文建模对于下一代语言模型至关重要，但标准注意力机制的高计算成本提出了重大的计算挑战。稀疏注意力提供了一种有前景的方向，可以在保持模型能力的同时提高效率。我们提出了NSA（Natively Trainable Sparse Attention）机制，该机制结合了算法创新与硬件对齐的优化，以实现高效的长时间上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的 token 压缩与细粒度的 token 选择，以保留全局上下文意识和局部精度。\n\n我们的方法在稀疏注意力设计方面提出了两个关键创新：（1）通过算术强度平衡的算法设计实现显著的速度提升，并针对现代硬件进行实现优化；（2）实现端到端训练，减少预训练计算量而不牺牲模型性能。如图1所示，实验结果显示，使用NSA进行预训练的模型在一般基准测试、长时间上下文任务和指令驱动推理方面均能保持或超越全注意力模型的表现。同时，NSA在64k长度序列的解码、前向传播和反向传播过程中均实现了显著的速度提升，验证了其在模型生命周期中的高效性。', 'title_zh': '原论文标题为：“Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention”\n\n翻译后的标题为：“本征稀疏注意力：硬件导向且本征可训练的稀疏注意力机制”'}
{'arxiv_id': 'arXiv:2502.11022', 'title': 'MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query Translation', 'authors': 'Zhiqian Qin, Yuanfeng Song, Jinwei Lu, Yuanwei Song, Shuaimin Li, Chen Jason Zhang', 'link': 'https://arxiv.org/abs/2502.11022', 'abstract': 'Natural language interfaces for NoSQL databases are increasingly vital in the big data era, enabling users to interact with complex, unstructured data without deep technical expertise. However, most recent advancements focus on English, leaving a gap for multilingual support. This paper introduces MultiTEND, the first and largest multilingual benchmark for natural language to NoSQL query generation, covering six languages: English, German, French, Russian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges in translating natural language to NoSQL queries across diverse linguistic structures, including lexical and syntactic differences. Experiments show that performance accuracy in both English and non-English settings remains relatively low, with a 4%-6% gap across scenarios like fine-tuned SLM, zero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we introduce MultiLink, a novel framework that bridges the multilingual input to NoSQL query generation gap through a Parallel Linking Process. It breaks down the task into multiple steps, integrating parallel multilingual processing, Chain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to tackle lexical and structural challenges inherent in multilingual NoSQL generation. MultiLink shows enhancements in all metrics for every language against the top baseline, boosting execution accuracy by about 15% for English and averaging a 10% improvement for non-English languages.', 'abstract_zh': '自然语言接口对于NoSQL数据库在大数据时代的应用越来越重要，它允许用户在无需深厚技术知识的情况下与复杂、无结构的数据进行交互。然而，大多数最近的进步主要集中在英语上，造成了多语言支持的空白。本文介绍了MultiTEND，这是首个也是最大的多语言基准数据集，用于自然语言到NoSQL查询的生成，涵盖六种语言：英语、德语、法语、俄语、日语和普通话。通过MultiTEND，我们分析了在多样的语言结构中将自然语言翻译成NoSQL查询所面临的挑战，包括词汇和句法差异。实验表明，在英语和非英语环境中，性能准确性仍然相对较低，不同场景（如微调的SLM、零样本的LLM、以及LLM的RAG）之间的差距在4%-6%之间。为解决上述挑战，本文引入了MultiLink这一新颖框架，通过并行链接过程填补了多语言输入到NoSQL查询生成的空白。该框架将任务分解为多个步骤，结合并行多语言处理、思维链推理（CoT）和检索增强生成（RAG），以应对多语言NoSQL生成固有的词汇和结构挑战。研究结果表明，相比于最顶级的基础模型，MultiLink在所有指标上都有改进，对于英语的执行准确率提升了约15%，对于非英语语言的平均提升为10%。', 'title_zh': '多语言基准：面向非关系数据库查询的自然语言翻译多语言基准（MultiTEND）'}
{'arxiv_id': 'arXiv:2502.11084', 'title': 'Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction', 'authors': 'Yuting Huang, Chengyuan Liu, Yifeng Feng, Chao Wu, Fei Wu, Kun Kuang', 'link': 'https://arxiv.org/abs/2502.11084', 'abstract': 'As Large Language Models (LLMs) are widely applied in various domains, the safety of LLMs is increasingly attracting attention to avoid their powerful capabilities being misused. Existing jailbreak methods create a forced instruction-following scenario, or search adversarial prompts with prefix or suffix tokens to achieve a specific representation manually or automatically. However, they suffer from low efficiency and explicit jailbreak patterns, far from the real deployment of mass attacks to LLMs. In this paper, we point out that simply rewriting the original instruction can achieve a jailbreak, and we find that this rewriting approach is learnable and transferable. We propose the Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method to attack LLMs by iteratively exploring the weakness of the LLMs and automatically improving the attacking strategy. The jailbreak is more efficient and hard to identify since no additional features are introduced. Extensive experiments and analysis demonstrate the effectiveness of R2J, and we find that the jailbreak is also transferable to multiple datasets and various types of models with only a few queries. We hope our work motivates further investigation of LLM safety.', 'abstract_zh': '随着大型语言模型（LLMs）在多个领域中的广泛应用，人们对LLMs的安全性兴趣日益增加，以避免其强大的能力被误用。现有的牢笼突破方法创造了强迫指令遵循的情景，或者通过前缀或后缀令牌搜索对抗提示以手动或自动实现特定表示。然而，这些方法效率较低，并且存在明显的牢笼突破模式，离大规模攻击LLMs的实际部署还有很大差距。在本文中，我们指出简单地重写原始指令可以实现牢笼突破，并发现这种重写方法是可学习和可迁移的。我们提出了“重写以实现牢笼突破”（R2J）方法，这是一种可迁移的黑盒牢笼突破方法，通过迭代探索LLMs的弱点并自动优化攻击策略来攻击LLMs。这种牢笼突破更加高效且难以识别，因为它没有引入额外的特征。广泛的经验和分析证明了R2J的有效性，我们发现这种牢笼突破在少数查询下对多个数据集和不同类型模型都是可迁移的。希望我们的工作能激励进一步研究LLMs的安全性。', 'title_zh': '将重新改写以逃脱惩罚：发现可学习和可转移的隐含有害指令'}
{'arxiv_id': 'arXiv:2502.11020', 'title': 'TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages', 'authors': 'Jafar Isbarov, Arofat Akhundjanova, Mammad Hajili, Kavsar Huseynova, Dmitry Gaynullin, Anar Rzayev, Osman Tursun, Ilshat Saetov, Rinat Kharisov, Saule Belginova, Ariana Kenbayeva, Amina Alisheva, Aizirek Turdubaeva, Abdullatif Köksal, Samir Rustamov, Duygu Ataman', 'link': 'https://arxiv.org/abs/2502.11020', 'abstract': 'Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.', 'abstract_zh': '全面评估大规模多任务语言理解（MMLU）能力对于推动多语言语言模型的应用至关重要。然而，准备高质量母语基准通常成本较高，因此限制了评估数据集的代表性。虽然近期努力集中在构建更加包容的MMLU基准，这些基准通常通过高资源语言的机器翻译构建，可能引入错误并未能充分考虑目标语言的语用和文化细微之处。在这篇论文中，我们针对代表性不足的突厥语家族语言缺乏母语MMLU基准的问题，提出了两种基准：TUMLU是一个全面的、多语言的、且专为突厥语设计的母语语言理解基准。它包括阿塞拜疆语、克里米亚塔塔尔语、卡拉卡尔帕克语、哈萨克语、塔塔尔语、土耳其语、维吾尔语及乌兹别克语中的11个学术主题的初中和高中水平问题。我们还提出了TUMLU-mini，这是一个更为简洁、平衡且经过人工验证的数据集子集。通过使用该数据集，我们系统地评估了一系列公开和专利的多语言大型语言模型（LLMs），包括Claude、Gemini、GPT和LLaMA，提供其在不同语言、科目和字母表上的表现的深入分析。为了促进多语言语言理解进一步的研究和发展，我们发布TUMLU-mini及所有相应的评估脚本。', 'title_zh': 'TUMLU：一种统一且原生态的土耳其语族语言理解基准测试'}
{'arxiv_id': 'arXiv:2502.11083', 'title': 'Streamlining the Collaborative Chain of Models into A Single Forward Pass in Generation-Based Tasks', 'authors': 'Yuanjie Lyu, Chao Zhang, Yuhao Chen, Yong Chen, Tong Xu', 'link': 'https://arxiv.org/abs/2502.11083', 'abstract': 'In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the "Chain of Models" approach is widely used, where multiple specialized models work sequentially on distinct sub-tasks. This approach is effective but increases resource demands as each model must be deployed separately. Recent advancements attempt to address this by applying prompt tuning, which allows a shared base model to adapt to multiple tasks with minimal parameter changes. However, a key challenge remains: intermediate outputs, passed between models as plain text, require recomputation of hidden states (i.e., Key and Value (KV) states in Transformers) during inference. In this paper, we introduce FTHSS, a novel prompt-tuning method that enables models to share KV hidden states, eliminating redundant forward passes and reducing KV cache storage. By modifying input and attention masks during training, FTHSS allows models to effectively utilize KV hidden states from prior models in both single- and multi-round scenarios. Empirical results on four tasks show that FTHSS matches the performance of traditional model chains while improving inference efficiency.', 'abstract_zh': '在检索增强生成（RAG）和基于代理的框架中，广泛使用了“模型链”方法，多个专门的模型按顺序处理不同的子任务。这种方法虽然有效，但会增加资源需求，因为每个模型都需要单独部署。最近的进展尝试通过提示调优来解决这个问题，这种技术允许共享的基本模型在最小参数变化的情况下适应多种任务。然而，一个主要挑战仍然存在：作为纯文本在模型之间传递的中间输出，在推理过程中需要重新计算隐藏状态（即Transformer中的Key和Value状态）。在本文中，我们提出了一种名为FTHSS的新颖的提示调优方法，该方法使模型能够共享KV隐藏状态，从而消除冗余的前向传递并减少KV缓存存储。通过在训练过程中修改输入和注意力掩码，FTHSS使模型能够在单轮和多轮场景中有效地利用前序模型的KV隐藏状态。在四个任务上的实验证明，FTHSS在提高推理效率的同时能达到传统模型链的性能。', 'title_zh': '将基于生成的任务中的协作模型链简化为单次前向传递'}
{'arxiv_id': 'arXiv:2502.11018', 'title': 'GRIFFIN: Effective Token Alignment for Faster Speculative Decoding', 'authors': 'Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou', 'link': 'https://arxiv.org/abs/2502.11018', 'abstract': "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\\% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).", 'abstract_zh': '投机解码通过同时生成多个草稿 token 来加速大型语言模型（LLMs）的推理过程。然而，现有方法经常难以解决训练阶段与解码阶段之间的 token 对齐问题，从而限制了其性能。为了解决这个问题，我们提出了一种名为 GRIFFIN 的新型框架，该框架结合了可对齐训练策略和可对齐草稿模型，以减轻对齐问题。可对齐训练策略采用了损失屏蔽机制，在训练过程中排除高对齐偏差的 token，从而防止它们对草稿模型的优化产生负面影响。可对齐草稿模型通过引入输入 token 来纠正生成特征的不一致性。实验结果表明，GRIFFIN 在 LLaMA 系列和 Vicuna 模型上的平均接受长度提高了 7% 以上，并且速度提高了 8% 以上，优于当前的领先方法，如图 1(a) 和 (b) 所示。', 'title_zh': 'GRIFFIN: 有效的-token对齐方法以实现更快的猜测性解码'}
{'arxiv_id': 'arXiv:2502.11078', 'title': 'DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling', 'authors': 'Aili Chen, Chengyu Du, Jiangjie Chen, Jinghan Xu, Yikai Zhang, Siyu Yuan, Zulong Chen, Liangyue Li, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2502.11078', 'abstract': "To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.", 'abstract_zh': '为了推进推荐系统和个人行为预测等个性化应用，近期的研究越来越多地采用大语言模型（LLMs）来进行人性化的人格建模。在动态现实场景中，有效的人格建模需要利用流式行为数据不断优化用户的人格。然而，现有的方法——无论是重新生成人格还是以增量方式结合新行为——往往无法在人格质量和未来行为预测准确性上持续取得改进。为了解决这一问题，我们提出了一种新颖的动态人格建模方法DEEPER，该方法能够实现持续的人格优化。具体而言，我们通过迭代强化学习框架增强了模型的方向搜索能力，使其能够自动识别有效的更新方向，并利用用户行为与模型预测之间的差异来进行人格优化。针对10个领域的4800名用户进行的详尽实验表明，DEEPER在人格优化方面的性能优于基线方法，在四次更新循环中，平均行为预测误差减少了32.2%，相比性能最佳的基线方法提高了22.92%。', 'title_zh': '《更深入地了解您的用户：面向个性的动态个性模型优化》'}
{'arxiv_id': 'arXiv:2502.11008', 'title': 'CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models', 'authors': 'Yuefei Chen, Vivek K.Singh, Jing Ma, Ruxiang Tang', 'link': 'https://arxiv.org/abs/2502.11008', 'abstract': "Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules. To support this evaluation, we introduce a new benchmark dataset, CounterBench, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different this http URL dataset is available at this https URL.", 'abstract_zh': '事实推理被认为是因果推理中最具挑战性和复杂性的方面之一，在人工智能领域得到了广泛认可。在本文中，我们评估了大型语言模型（LLMs）在事实推理中的性能。与以往主要集中在常识因果推理的研究不同，这些研究通常依赖于先验知识进行推理，我们特别评估了它们在使用一套正式规则进行事实推理方面的能力。为了支持这一评估，我们引入了一个新的基准数据集CounterBench，包含1000个事实推理问题。该数据集设计时考虑了不同程度的难度，多种因果图结构，不同类型的事实推理问题，以及多种不合理的名称变体。实验结果表明，事实推理对LLMs构成了重大挑战，大多数模型的表现与随机猜测相当。为了增强LLMs的事实推理能力，我们提出了一种新的推理范式CoIn，它引导LLMs通过迭代推理和回溯系统地探索事实推理解决方案。实验结果表明，该方法显著提高了LLMs在事实推理任务上的性能，并且在不同类型的CounterBench数据集上表现出一致性。数据集可在以下网址获取: [数据集链接]', 'title_zh': 'CounterBench：大规模语言模型因果推理基准测试'}
{'arxiv_id': 'arXiv:2502.10996', 'title': 'RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation', 'authors': 'Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han', 'link': 'https://arxiv.org/abs/2502.10996', 'abstract': 'Retrieval-augmented language models often struggle with knowledge-intensive tasks due to inefficient retrieval, unstructured knowledge integration, and single-pass architectures. We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS introduces four key technical innovations: (1) a themescoped retrieval mechanism that efficiently narrows the search space while maintaining retrieval quality, (2) an action planning module that determines knowledge needs and generates focused sub-queries, (3) a dynamic knowledge structuring approach that converts retrieved text into an evolving knowledge graph, and (4) a graph-augmented answering component that leverages the accumulated structured information. Our framework achieves state-of-the-art performance, surpassing leading baselines by 6.4% with open-source language models and 7.0% with proprietary models on seven knowledge-intensive generation datasets across all evaluation metrics. Detailed ablation studies verify the contribution of each technical component to the overall system performance.', 'abstract_zh': '检索增强的语言模型在进行知识密集型任务时常常遇到困难，主要原因包括检索效率低下、知识整合结构化不足以及单一-pass 架构。我们提出了一种名为 Retrieval-And-Structuring (RAS) 的新型框架，该框架能够动态构建和推理与查询相关的知识图谱，通过迭代的检索和结构化过程进行。RAS 引入了四项关键技术创新：（1）主题导向的检索机制，能够在保持检索质量的同时有效缩小搜索范围；（2）行动规划模块，确定知识需求并生成有针对性的子查询；（3）动态知识结构化方法，将检索到的文本转化为动态发展中的知识图谱；（4）图增强的答题组件，利用积累的结构化信息进行回答。我们的框架在所有评估指标上均取得了最先进的性能，与开源语言模型相比，领先优势达到 6.4%，与专有模型相比，领先优势达到 7.0%。针对七个知识密集型生成数据集进行了详细的消融研究，验证了每个技术组件对整体系统性能的贡献。', 'title_zh': 'RAS：检索与结构化方法在知识密集型大模型生成中的应用'}
{'arxiv_id': 'arXiv:2502.10995', 'title': 'Evaluating Large language models on Understanding Korean indirect Speech acts', 'authors': 'Youngeun Koo, Jiwoo Lee, Dojun Park, Seohyun Park, Sungeun Lee', 'link': 'https://arxiv.org/abs/2502.10995', 'abstract': "To accurately understand the intention of an utterance is crucial in conversational communication. As conversational artificial intelligence models are rapidly being developed and applied in various fields, it is important to evaluate the LLMs' capabilities of understanding the intentions of user's utterance. This study evaluates whether current LLMs can understand the intention of an utterance by considering the given conversational context, particularly in cases where the actual intention differs from the surface-leveled, literal intention of the sentence, i.e. indirect speech acts. Our findings reveal that Claude3-Opus outperformed the other competing models, with 71.94% in MCQ and 65% in OEQ, showing a clear advantage. In general, proprietary models exhibited relatively higher performance compared to open-source models. Nevertheless, no LLMs reached the level of human performance. Most LLMs, except for Claude3-Opus, demonstrated significantly lower performance in understanding indirect speech acts compared to direct speech acts, where the intention is explicitly revealed through the utterance. This study not only performs an overall pragmatic evaluation of each LLM's language use through the analysis of OEQ response patterns, but also emphasizes the necessity for further research to improve LLMs' understanding of indirect speech acts for more natural communication with humans.", 'abstract_zh': '准确理解语句的意图在对话交流中至关重要。随着对话型人工智能模型在各个领域的快速发展和应用，评估大型语言模型（LLMs）理解和解析用户意图的能力变得尤为重要。本研究评估了当前LLMs在给定对话背景的情况下是否能够理解语句的意图，特别是在实际意图与语句的表面含义（即直接陈述）不一致的情况下，即间接言语行为。我们的研究结果表明，Claude3-Opus在此评价中表现最佳，多项选择题（MCQ）得分为71.94%，开放性问题（OEQ）得分为65%，显示出其明显的优势。总的来说，自有的模型相较于开源模型表现出相对较高的性能。然而，没有一种LLM达到了人类的水平。除了Claude3-Opus之外，大多数LLMs在理解间接言语行为方面的表现明显低于直接言语行为，后者通过语句本身明确地揭示了意图。本研究不仅通过分析OEQ响应模式对每种LLM的语言使用进行了总体语用评估，还强调了进一步研究以改善LLMs对间接言语行为的理解，以便实现更自然的人机沟通的必要性。', 'title_zh': '评估大型语言模型在理解韩语含蓄言语行为方面的能力'}
{'arxiv_id': 'arXiv:2502.10993', 'title': 'RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization', 'authors': 'Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, Haoyu Wang', 'link': 'https://arxiv.org/abs/2502.10993', 'abstract': 'Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.', 'abstract_zh': '大型语言模型（LLMs）在性能上取得了显著进展，但也面临着高计算成本和长延迟的问题，这限制了它们在资源受限环境中的部署。相比之下，小型语言模型（SLMs）更为高效，但它们在捕捉不断变化的现实世界知识方面存在困难。检索增强生成（RAG）通过集成外部知识来解决这一问题，但检索过程中的不完善可能导致误导性的噪音，从而误导SLMs。我们提出了RoseRAG——一种通过边际感知偏好优化增强SLMs的鲁棒RAG框架。RoseRAG采用多轮提示进行详细的推理、拒绝采样以生成高质量的解释，并通过对比性偏好选择来优化响应，以最大化正面输出与非正面输出之间的似然性差距。通过将这些组件整合到一个边际感知优化过程中，RoseRAG能够稳健地提高SLMs在RAG应用中的准确性和可靠性。在三个开放领域的问答基准测试中，我们的创新性RoseRAG显著超越了现有最先进的基线方法。', 'title_zh': 'RoseRAG：通过边际感知偏好优化实现的小规模LLM增强检索生成'}
{'arxiv_id': 'arXiv:2502.11075', 'title': 'Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models', 'authors': 'Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen', 'link': 'https://arxiv.org/abs/2502.11075', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive capabilities in natural language processing tasks, such as text generation and semantic understanding. However, their performance on numerical reasoning tasks, such as basic arithmetic, numerical retrieval, and magnitude comparison, remains surprisingly poor. This gap arises from their reliance on surface-level statistical patterns rather than understanding numbers as continuous magnitudes. Existing benchmarks primarily focus on either linguistic competence or structured mathematical problem-solving, neglecting fundamental numerical reasoning required in real-world scenarios. To bridge this gap, we propose NumericBench, a comprehensive benchmark to evaluate six fundamental numerical capabilities: number recognition, arithmetic operations, contextual retrieval, comparison, summary, and logical reasoning. NumericBench includes datasets ranging from synthetic number lists to the crawled real-world data, addressing challenges like long contexts, noise, and multi-step reasoning. Extensive experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal persistent weaknesses in numerical reasoning, highlighting the urgent need to improve numerically-aware language modeling. The benchmark is released in: this https URL.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理任务中展现出了令人印象深刻的性能，例如文本生成和语义理解。然而，在基本算术、数值检索和数量级对比等数值推理任务上的表现仍然令人惊讶地较差。这一差距源于它们依赖于表面上的统计模式，而不是理解数字作为连续的数量级。现有的基准测试主要集中在语言能力和结构化的数学问题解决上，忽视了真实场景中所需的最基本数值推理能力。为弥合这一差距，我们提出了一种全面的基准测试NumericBench，评估六种基本的数值能力：数字识别、算术运算、上下文检索、比较、总结和逻辑推理。NumericBench 包括从合成数字列表到抓取的现实世界数据的各种数据集，解决了诸如长上下文、噪声和多步推理等挑战。在最先进的LLM（包括GPT-4和DeepSeek）上的广泛实验揭示了持久性的数值推理弱点，强调了提高数值感知语言建模的迫切需求。该基准测试已发布在以下链接：https://github.com/alibaba/NumericBench。', 'title_zh': '暴露数值能力差距：评估大型语言模型基本数值能力的标准'}
{'arxiv_id': 'arXiv:2502.11073', 'title': 'Demystifying Hateful Content: Leveraging Large Multimodal Models for Hateful Meme Detection with Explainable Decisions', 'authors': 'Ming Shan Hee, Roy Ka-Wei Lee', 'link': 'https://arxiv.org/abs/2502.11073', 'abstract': 'Hateful meme detection presents a significant challenge as a multimodal task due to the complexity of interpreting implicit hate messages and contextual cues within memes. Previous approaches have fine-tuned pre-trained vision-language models (PT-VLMs), leveraging the knowledge they gained during pre-training and their attention mechanisms to understand meme content. However, the reliance of these models on implicit knowledge and complex attention mechanisms renders their decisions difficult to explain, which is crucial for building trust in meme classification. In this paper, we introduce IntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for hateful meme classification with explainable decisions. IntMeme addresses the dual challenges of improving both accuracy and explainability in meme moderation. The framework uses LMMs to generate human-like, interpretive analyses of memes, providing deeper insights into multimodal content and context. Additionally, it uses independent encoding modules for both memes and their interpretations, which are then combined to enhance classification performance. Our approach addresses the opacity and misclassification issues associated with PT-VLMs, optimizing the use of LMMs for hateful meme detection. We demonstrate the effectiveness of IntMeme through comprehensive experiments across three datasets, showcasing its superiority over state-of-the-art models.', 'abstract_zh': '仇恨表情包检测是一项多模态任务，由于需要解释隐含的仇恨信息及其上下文线索，因此具有显著的挑战性。此前的方法通过对预训练的视觉-语言模型（PT-VLMs）进行微调，利用它们在预训练过程中获得的知识和注意力机制来理解表情包的内容。然而，这些模型过于依赖隐含知识和复杂的注意力机制，导致其决策难以解释，这在建立对表情包分类的信任方面至关重要。本论文提出了一种名为IntMeme的新型框架，利用大型多模态模型（LMMs）进行可解释的仇恨表情包分类。IntMeme解决了表情包审查双重挑战，即提高准确性和可解释性。该框架使用LMMs生成类似人类的、可解释的分析，以深入理解多模态内容和上下文。此外，它还针对表情包及其解释使用独立的编码模块，这些模块随后结合以提升分类性能。我们的方法解决了PT-VLMs的透明度低和分类错误等问题，优化了LMMs在仇恨表情包检测中的使用。通过在三个数据集上进行全面的实验，展示了IntMeme相较于最先进的模型的有效性。', 'title_zh': '揭开仇恨内容的面纱：利用可解释决策的大规模多模态模型进行仇恨 meme 识别'}
{'arxiv_id': 'arXiv:2502.10990', 'title': 'FinMTEB: Finance Massive Text Embedding Benchmark', 'authors': 'Yixuan Tang, Yi Yang', 'link': 'https://arxiv.org/abs/2502.10990', 'abstract': 'Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.', 'abstract_zh': '嵌入模型在各类自然语言处理（NLP）应用中扮演着至关重要的角色，负责信息的表示与检索。近年来，大型语言模型（LLMs）的进展进一步提高了嵌入模型的性能。虽然这些模型经常基于通用数据集进行评估，但实际应用的需求则要求进行领域特定的评估。在本研究中，我们介绍了金融大规模文本嵌入基准（FinMTEB），这是专为金融领域设计的MTEB的专门版本。FinMTEB包括了涵盖7项任务的64个金融领域特定的嵌入数据集，这些任务覆盖了中英文的不同文本类型，如金融新闻文章、公司年报、ESG报告、监管文件以及收益电话会议记录。我们还利用基于角色的合成方法，开发了一个适应金融领域的模型FinPersona-E5，用于训练这些金融嵌入任务。通过对15个嵌入模型（包括FinPersona-E5）的广泛评估，我们发现了以下三个主要发现：（1）通用基准上的性能与金融领域任务之间的关联性有限；（2）领域适应模型的一贯表现优于通用模型；（3）令人惊讶的是，简单的词袋（BoW）方法在金融语义文本相似性（STS）任务中表现优于复杂的密集嵌入技术，强调了当前密集嵌入技术的局限性。我们的研究建立了一个适用于金融NLP应用的稳健评估框架，并为开发领域特定嵌入模型提供了关键见解。', 'title_zh': 'FinMTEB：金融大规模文本嵌入基准'}
{'arxiv_id': 'arXiv:2502.10973', 'title': 'Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues', 'authors': 'David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter', 'link': 'https://arxiv.org/abs/2502.10973', 'abstract': 'In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.', 'abstract_zh': '在本文中，我们介绍了阿坤坦情感对话数据集（Akan Conversation Emotion, ACE）——这是第一个针对非洲语言的情感多模态对话数据集，旨在解决情感识别研究中低资源语言资源严重缺乏的问题。ACE 数据集专为阿坤坦语言开发，包含 385 个带情绪标注的对话和 6,162 个跨音频、视觉和文本模态的言语单元，以及以词为单位的重音标注。该数据集中的重音标注使其成为首个注释有声调的非洲语言数据集。我们通过使用最新的情感识别方法进行实验，展示了 ACE 的质量和实用性，并为未来研究建立了稳健的基准。我们希望 ACE 能激发进一步在包容性、语言和文化多样性方面工作的自然语言处理资源的研究。', 'title_zh': '电影对话中情感识别的多模态多言路段际数据集：Akan 影院情感 (ACE)'}
{'arxiv_id': 'arXiv:2502.11066', 'title': 'CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment', 'authors': 'Nura Aljaafari, Danilo S. Carvalho, André Freitas', 'link': 'https://arxiv.org/abs/2502.11066', 'abstract': "Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.", 'abstract_zh': '大型语言模型（LLMs）在组合泛化方面存在困难，限制了它们系统地将学到的组件组合起来解释新颖输入的能力。尽管通过架构修改、微调和数据增强可以改进组合性，但这些方法往往在适应性方面有限，面临可扩展性限制，或在实际数据上的效果呈递减趋势。为了解决这一问题，我们提出了一种名为CARMA的干预措施，该措施在保持微调后性能的同时，增强了LLMs中的组合推理的稳定性和鲁棒性。CARMA利用互信息正则化和逐层稳定性约束来减轻特征碎片化问题，确保结构化的表示在层间和层内保持一致。我们通过逆字典建模和情感分类任务来评估CARMA的影响，测量其对语义一致性、性能稳定性和对抗词汇扰动鲁棒性的影响。结果显示，CARMA减少了微调引入的变异，稳定了token表示，并改善了组合推理能力。尽管其效果在不同架构上有所差异，但CARMA的主要优势在于强化已学习的结构而不是引入新功能，使其成为一种可扩展的辅助方法。这些发现表明，将CARMA与微调结合使用可以改善组合泛化能力，同时保持特定任务的性能在LLMs中。', 'title_zh': 'CARMA：通过高级正则化和互信息对齐增强大型语言模型的组合性'}
{'arxiv_id': 'arXiv:2502.10966', 'title': 'Neural Networks Remember More: The Power of Parameter Isolation and Combination', 'authors': 'Biqing Zeng, Zehan Li, Aladdin Ayesh', 'link': 'https://arxiv.org/abs/2502.10966', 'abstract': "Catastrophic forgetting is a pervasive issue for pre-trained language models (PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to retain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specifically, our proposed approach leverages parameter isolation and a subsequent combination strategy. Initially, in the training stage, the model adapts to each downstream task via a parameter isolation method to prevent potential interference among different tasks. We then combine all trained parameters, which contain acquired knowledge, using the task arithmetic method and finally apply them to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.", 'abstract_zh': '灾难性遗忘是预训练语言模型（PLM）在持续学习过程中面临的一个普遍问题，当模型按顺序在一系列任务上进行训练时，模型会丢失之前学到的知识。模型保留旧任务能力被称为稳定性，而适应新任务的能力被称为可塑性。因此，解决这一问题的关键在于在模型的可塑性和稳定性之间找到一个平衡。为了解决这一问题，本文提出了一种新的方法来实现模型稳定性和可塑性的平衡，从而减轻灾难性遗忘。具体来说，我们提出的这种方法利用了参数隔离，并随后采用组合策略。在训练阶段，首先通过参数隔离方法使模型适应每个下游任务，以防止不同任务之间的潜在干扰。然后，我们使用任务算术方法组合所有已训练的参数，并包含这些参数中获得的知识，最后将它们应用于主干模型。在持续学习基准上的实验评估证明了我们方法的有效性，表明其在现有最佳方法上具有显著的改进。', 'title_zh': '神经网络记忆更多：参数隔离与组合的力量'}
{'arxiv_id': 'arXiv:2502.11062', 'title': 'Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection', 'authors': 'Yang Zhao, Li Du, Xiao Ding, Yangou Ouyang, Hepeng Wang, Kai Xiong, Jinglong Gao, Zhouhao Sun, Dongliang Xu, Yang Qing, Dongchen Li, Bing Qin, Ting Liu', 'link': 'https://arxiv.org/abs/2502.11062', 'abstract': 'Large language models (LLMs) have shown great potential across various industries due to their remarkable ability to generalize through instruction tuning. However, the limited availability of domain-specific data significantly hampers their performance on specialized tasks. While existing methods primarily focus on selecting training data from general datasets that are similar to the target domain, they often fail to consider the joint distribution of instructions, resulting in inefficient learning and suboptimal knowledge transfer. To address these challenges, we introduce G2IS (Gradient-based Graph Instruction Selection), a novel method that constructs a mixed gradient-based instruction graph to capture the joint distribution and interdependencies between instructions. By accounting for the relationships between instructions, G2IS improves domain adaptation efficiency. Additionally, we propose a gradient walk algorithm to refine the data selection process, enhancing both training effectiveness and efficiency. Our experiments demonstrate that G2IS outperforms traditional methods across various domain adaptation tasks, yielding significant performance gains, particularly in complex, data-scarce scenarios. These results underscore the potential of G2IS in advancing the development of large, domain-specific models.', 'abstract_zh': '大型语言模型（LLMs）因其通过指令调优表现出的出色泛化能力，在各个行业中展现了巨大的潜力。然而，领域特定数据的有限可用性严重阻碍了其在特定任务上的性能。虽然现有方法主要关注从与目标领域相似的通用数据集中选择训练数据，但它们往往未能考虑到指令的联合分布，导致学习效率低下和知识转移不佳。为了解决这些挑战，我们提出了基于梯度的图指令选择方法（G2IS），这是一种新颖的方法，通过构建混合的基于梯度的指令图来捕捉指令之间的联合分布和相互依赖关系。通过考虑指令之间的关系，G2IS 提高了领域适应效率。此外，我们还提出了一种梯度行走算法来细化数据选择过程，这不仅提高了训练的有效性，也提高了训练效率。我们的实验证明，G2IS 在各种领域适应任务中优于传统方法，特别是在复杂且数据稀缺的情景下，取得了显著的性能提升。这些结果进一步强调了 G2IS 在促进大型领域特定模型开发中的潜力。', 'title_zh': '超越相似性：基于梯度的图方法在指令调优数据选择中的应用'}
{'arxiv_id': 'arXiv:2502.10942', 'title': 'Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks', 'authors': 'Henry Evidail, Zachary Mountebank, Alistair Hathersage, Peter Stanhope, Basil Ravenscroft, Tobias Waddingham', 'link': 'https://arxiv.org/abs/2502.10942', 'abstract': 'Self-modulating mechanisms introduce dynamic adaptation capabilities within language models through contextual realignment strategies that influence token embedding trajectories across extended sequences. Contextual Flux is explored as an approach to embedding modulation, integrating an auxiliary gating mechanism within the self-attention framework to dynamically adjust token representations based on evolving contextual dependencies. The empirical analysis evaluates entropy variations, latent space realignments, and coherence stability to assess the extent to which self-regulation enhances text generation consistency while preserving generative flexibility. Quantitative assessments suggest that embedding shifts contribute to more structured adaptation in long-form sequences, with measured reductions in redundant phrase repetitions and improvements in thematic retention. Variability in contextual weight computation affects modulation stability, leading to differing levels of adaptation across diverse linguistic structures. The computational demands introduced through real-time embedding reconfiguration are examined in relation to model scalability, emphasizing the need for optimization strategies in high-volume generative applications. The findings suggest that while adaptive embedding updates improve certain aspects of coherence, their impact remains contingent on model capacity and input complexity.', 'abstract_zh': '自调节机制通过上下文重对齐策略在语言模型中引入动态适应能力，促使 token 表征在延伸序列中发生变化。研究探讨了上下文流（Contextual Flux）作为嵌入调节的一种方法，通过在自我注意框架中集成辅助门控机制，以根据上下文依赖关系的发展动态调整 token 表征。经验分析通过评估熵的变化、潜在空间的重对齐以及连贯性的稳定性来评估自调节如何增强文本生成的一致性，同时保持生成的灵活性。定量评估表明，嵌入变化可在长文本序列中实现更有序的适应，表现为重复短语的减少和主题保留度的提升。上下文权重计算的变异性影响着调节的稳定性，导致在不同语言结构中表现出不同的适应能力。研究还考察了实时嵌入重构所带来的计算需求与模型扩展性的关系，强调在高吞吐量生成应用中需要优化策略。研究发现，虽然自适应嵌入更新可以改善某些连贯性方面的表现，但其影响仍取决于模型容量和输入复杂度。\n\n这段文献的中文翻译尽量保持了原文的学术规范和专业术语，确保了翻译的准确性与专业性。', 'title_zh': '探索大型语言模型中的上下文流变性：一种新型自我调节语义网络的方法'}
{'arxiv_id': 'arXiv:2502.11061', 'title': 'Déjà Vu? Decoding Repeated Reading from Eye Movements', 'authors': 'Yoav Meiri, Omer Shubi, Cfir Avraham Hadar, Ariel Kreisberg Nitzav, Yevgeni Berzak', 'link': 'https://arxiv.org/abs/2502.11061', 'abstract': 'Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.', 'abstract_zh': '以下是一篇论文内容或标题的中文翻译，符合学术规范：\n\n无论是你最喜欢的小说、新闻报道、烹饪食谱还是学术论文——在许多日常情况下，我们都会多次阅读同一文本。本文探讨了是否可以根据读者的眼动模式，自动判断读者是否曾以前接触过该文本。我们提出了两种任务变体，并成功地通过特征模型和神经网络模型来解决这些问题。进一步地，我们提出了一种策略，通过从认知模型中生成的眼动模拟，增强这些模型。最后，我们呈现了模型性能的分析，一方面揭示了模型所依赖的信息，另一方面利用预测建模作为分析工具，更好地刻画重复阅读中记忆的作用。我们的研究增进了对阅读过程中眼动捕获先前文本暴露记忆效应范围和方式的理解，并为未来的重复阅读预测建模应用铺平了道路。', 'title_zh': '《 déjà vu? 从眼动解析重复阅读》\n\n注：在这里，“déjà vu”是一个法语短语，常译为“ déjà vu” 或者“似曾相识感”，在学术论文标题中保持原文形式是被接受的。如果需要解释，可以加上相关的注释。完整的翻译标题可以是：“ déjà vu? 从眼动解析重复阅读现象”。'}
{'arxiv_id': 'arXiv:2502.10934', 'title': 'Fundamental Principles of Linguistic Structure are Not Represented by o3', 'authors': 'Elliot Murphy, Evelina Leivada, Vittoria Dentella, Fritz Gunther, Gary Marcus', 'link': 'https://arxiv.org/abs/2502.10934', 'abstract': "A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute.", 'abstract_zh': '成功的人工通用智能的核心组件之一是对具体组成抽象的快速创建和操作，以及展示在递归层次结构语法对象家族中所需的人类语言创造性使用方面的专业知识。我们评估了最近发布的 o3 模型（OpenAI；o3-mini-high），发现尽管它在依赖线性和表面统计的一些基本语言测试中表现良好（例如，草莓测试），但它无法推广应用结构规则；它在涉及语义非法基数比较的比较句（“Escher 句子”）上失败；它无法正确评估和解释可接受性动态；并且无法区分生成不合理的语义输出与不合理的语法输出的指令。当要求其生成简单的语法规则违反时，它似乎无法表示多种解析以评估各种可能的语义解释。与许多近期声称人工语言模型即将取代语言学领域的说法形成鲜明对比的是，我们的结果不仅表明深度学习在组合性方面遇到了瓶颈（Marcus 2022），而且还表明这是一个固执且难以克服的障碍，仅仅通过增加计算量是无法轻松克服这一障碍以达到类似人类的组合性推理水平的。', 'title_zh': '语言结构的基本原理并不由o3表示。'}
{'arxiv_id': 'arXiv:2502.11054', 'title': 'Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models', 'authors': 'Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.11054', 'abstract': "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at this https URL to facilitate further research in this critical domain.", 'abstract_zh': '多轮 jailbreak 攻击通过模拟现实世界的人类互动，引导大语言模型（LLMs）进行迭代对话，暴露了关键的安全漏洞。然而，现有方法往往难以在语义连贯性和攻击有效性之间达成平衡，导致要么是无害的语义漂移，要么是无效的检测规避。为解决这一挑战，我们提出了一种名为 Reasoning-Augmented Conversation（增强推理对话）的新颖多轮 jailbreak 框架，该框架将有害查询重新表述为无害的推理任务，并利用大语言模型的强大推理能力来破坏安全对齐。具体来说，我们引入了一种攻击状态机框架，系统地建模问题转换和迭代推理，确保在多轮中生成语义连贯的问题。在此框架基础上，我们设计了收益导向探索、自我对弈和拒绝反馈模块，以保持攻击语义、增强攻击效果并维持推理驱动的攻击进程。在多种大语言模型上的广泛实验表明，RACE 在复杂的对话场景中实现了最先进的攻击效果，攻击成功率（ASRs）最高提高96%。值得注意的是，我们的方法对顶级商用模型 OpenAI o1 和 DeepSeek R1 的攻击成功率分别达到了82%和92%，突显了其威力。我们已在以下 URL 公开代码，以促进对该关键领域的进一步研究。', 'title_zh': '增强推理对话以应对大型语言模型多轮脱轨攻击'}
{'arxiv_id': 'arXiv:2502.11051', 'title': 'MMUNLEARNER: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models', 'authors': 'Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11051', 'abstract': 'Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient descent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.', 'abstract_zh': '近年来，机器遗忘（Machine Unlearning, MU）的研究取得了进展，提出了从深度神经网络中选择性移除私人或敏感信息的解决方案。然而，面向多模态大型语言模型（Multimodal Large Language Models, MLLMs）的MU仍处于初级阶段。因此，我们提出了在MLLM时代重新定义多模态MU的任务，该任务旨在仅擦除与给定实体相关的视觉模式，同时保留语言模型骨干网络中原有的相应文本知识。此外，我们提出了一种新的几何约束梯度下降方法Multimodal Unlearner（MMUnlearner）。该方法在遗忘过程中通过联合限制剩余概念和文本知识来更新MLLM的权重，从而保留对非目标知识至关重要的参数。广泛实验证明，MMUnlearner在所有评估维度上均优于直接使用问答数据（VQA数据）通过梯度上升（Gradient Ascent, GA）或负面偏好优化（Negative Preference Optimization, NPO）微调MLLM的基线方法。我们的代码将在接受后发布。', 'title_zh': 'MMUNLEARNER: 重塑多模态机器遗忘在多模态大型语言模型时代的范式'}
{'arxiv_id': 'arXiv:2502.11028', 'title': 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models', 'authors': 'Prateek Chhikara', 'link': 'https://arxiv.org/abs/2502.11028', 'abstract': 'Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.', 'abstract_zh': '大型语言模型（LLMs）在多样化的任务中展现出令人印象深刻的性能，但置信度校准仍然是一个挑战。错校准——即模型过度自信或欠自信——在高风险应用中尤其具有风险。本文进行了一项实证研究，探讨了模型大小、干扰项和问题类型如何影响置信度一致性。我们提出了一种评估框架来度量过度自信，并调查了多项选择格式是否能够减轻或加剧错校准。我们的研究发现，虽然较大的模型（例如GPT-4o）在总体上更校准，但它们更容易受到干扰的影响，而较小的模型虽然在从答案选项中受益，但在不确定性的估计上却面临更大的困难。与先前的研究主要报告错校准趋势不同，我们的研究提供了针对故障模式和加剧过度自信条件的实际建议。这些发现强调了需要采取校准意识更强的干预措施，并改进不确定性的估计方法。', 'title_zh': '注意信心差距：大型语言模型中的过度自信、校准问题及干扰项效应'}
{'arxiv_id': 'arXiv:2502.10921', 'title': 'Evolving Hate Speech Online: An Adaptive Framework for Detection and Mitigation', 'authors': 'Shiza Ali, Gianluca Stringhini', 'link': 'https://arxiv.org/abs/2502.10921', 'abstract': 'The proliferation of social media platforms has led to an increase in the spread of hate speech, particularly targeting vulnerable communities. Unfortunately, existing methods for automatically identifying and blocking toxic language rely on pre-constructed lexicons, making them reactive rather than adaptive. As such, these approaches become less effective over time, especially when new communities are targeted with slurs not included in the original datasets. To address this issue, we present an adaptive approach that uses word embeddings to update lexicons and develop a hybrid model that adjusts to emerging slurs and new linguistic patterns. This approach can effectively detect toxic language, including intentional spelling mistakes employed by aggressors to avoid detection. Our hybrid model, which combines BERT with lexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art datasets. Our work has significant implications for creating safer online environments by improving the detection of toxic content and proactively updating the lexicon. Content Warning: This paper contains examples of hate speech that may be triggering.', 'abstract_zh': '社交媒体平台的普及导致了仇恨言论的传播增加，尤其针对弱势社区。不幸的是，目前自动识别和阻止有毒语言的方法依赖于预先构建的词汇表，使这些方法变得被动而不是主动。因此，这些方法随着时间的推移变得越来越不有效，尤其是在新的社区被带有不在原始数据集中词汇表中的污言秽语攻击时。为解决这个问题，我们提出了一种自适应方法，该方法利用词嵌入来更新词汇表，并开发了一种混合模型，能够适应新兴的污言秽语和新的语言模式。该方法能够有效检测有毒语言，包括攻击者故意使用的拼写错误以逃避检测。我们的混合模型结合了 BERT 和基于词汇表的技术，在大多数最先进的数据集上达到了 95% 的准确率。我们的工作对于通过改进有毒内容的检测并主动更新词汇表来创建更安全的在线环境具有重要意义。内容警告：本文包含可能引起不适的仇恨言论示例。', 'title_zh': '在线仇恨言论的发展：一种检测与遏制的适应性框架'}
{'arxiv_id': 'arXiv:2502.10916', 'title': 'Enhancing Conversational Agents from Open-Source Large Language Models with Illocutionary Force and Document-Based Knowledge Retrieval', 'authors': 'Godfrey Inyama', 'link': 'https://arxiv.org/abs/2502.10916', 'abstract': 'In this paper, we first present a novel way of computationally analysing and extracting illocutionary forces from dialogue using Bert-based Large Language Models, and demonstrate how these features impact the response of a conversational agent guided by a document-based knowledge bank demonstrated by a bespoke web conversational chat agent system developed. Our proposed illocutionary force extraction and classification technique is the first of its kind using the Argument Interchange Format (AIF) Dataset, showing an improved performance compared to two methods for carrying out similar tasks with a macro F1 of approximately 45%. When we evaluated the system based on 2 knowledge files, with 2 user queries each, across 5 open-source large language models (LLMs) using 10 standard metrics we found out that larger open-source models, such as Llama2:13b and Llama3-chatqa-latest, demonstrated an improved alignment when the user illocutionary force was included with their query, achieving higher QA and linguistic similarity scores. The smaller models on the other hand like Tinyllama:latest showed an increased perplexity and mixed performance, which explicitly indicated struggles in processing queries that explicitly included illocutionary forces. The results from the analysis highlight the potential of illocutionary force to enhance conversational depth while underscoring the need for model-specific optimizations to address increased computational costs and response times.', 'abstract_zh': '本文首先提出了一种使用基于Bert的大语言模型进行对话中施事力量的计算分析和提取的新方法，并展示了这些特征如何影响由文档知识库引导的对话代理的响应。我们开发的专属网页对话聊天代理系统具体展示了这一种特色方法。我们提出了一种利用Argument Interchange Format (AIF) 数据集进行施事力量提取和分类的技术，其性能在宏F1分数约为45%的情况下优于两种执行类似任务的方法。基于包含2个知识文件和每文件2个用户查询的5个开源大语言模型（LLMs）进行10项标准指标的评估，我们发现较大的开源模型，如Llama2：13b和Llama3-chatqa-latest，在包含用户施事力量的查询时表现出了更好的对齐性，取得了更高的问答和语语文本相似度评分。相比之下，较小的模型，如Tinyllama：latest，则表现出增加的困惑度和混合性能，明确表明它们在处理明确包含施事力量的查询时面临挑战。分析结果突显了施事力量在增强对话深度方面的潜力，同时也强调了需要针对增加的计算成本和响应时间进行模型特定优化的需求。', 'title_zh': '提升开源大规模语言模型的对话代理能力：基于施为力量和文档导向的知识检索'}
{'arxiv_id': 'arXiv:2502.11022', 'title': 'MultiTEND: A Multilingual Benchmark for Natural Language to NoSQL Query Translation', 'authors': 'Zhiqian Qin, Yuanfeng Song, Jinwei Lu, Yuanwei Song, Shuaimin Li, Chen Jason Zhang', 'link': 'https://arxiv.org/abs/2502.11022', 'abstract': 'Natural language interfaces for NoSQL databases are increasingly vital in the big data era, enabling users to interact with complex, unstructured data without deep technical expertise. However, most recent advancements focus on English, leaving a gap for multilingual support. This paper introduces MultiTEND, the first and largest multilingual benchmark for natural language to NoSQL query generation, covering six languages: English, German, French, Russian, Japanese and Mandarin Chinese. Using MultiTEND, we analyze challenges in translating natural language to NoSQL queries across diverse linguistic structures, including lexical and syntactic differences. Experiments show that performance accuracy in both English and non-English settings remains relatively low, with a 4%-6% gap across scenarios like fine-tuned SLM, zero-shot LLM, and RAG for LLM. To address the aforementioned challenges, we introduce MultiLink, a novel framework that bridges the multilingual input to NoSQL query generation gap through a Parallel Linking Process. It breaks down the task into multiple steps, integrating parallel multilingual processing, Chain-of-Thought (CoT) reasoning, and Retrieval-Augmented Generation (RAG) to tackle lexical and structural challenges inherent in multilingual NoSQL generation. MultiLink shows enhancements in all metrics for every language against the top baseline, boosting execution accuracy by about 15% for English and averaging a 10% improvement for non-English languages.', 'abstract_zh': '自然语言接口对于NoSQL数据库在大数据时代越来越重要，能够使用户在无需深厚技术背景的情况下与复杂、未结构化的数据进行交互。然而，目前大多数进展主要集中在英语上，忽略了多语言支持的需求。本文介绍了MultiTEND，这是首个也是规模最大的多语言基准数据集，用于自然语言到NoSQL查询生成，涵盖了六种语言：英语、德语、法语、俄语、日语和普通话。通过MultiTEND，我们分析了在不同语言结构下将自然语言转换为NoSQL查询所面临的挑战，包括词汇和句法差异。实验结果显示，在英语和非英语环境中，性能准确性相对较低，各种场景（如微调的语言模型、零样本学习语言模型和基于检索增强生成的语言模型）间有4%-6%的差距。为了解决上述挑战，我们提出了一种名为MultiLink的新颖框架，该框架通过并行联结过程填补了多语言输入与NoSQL查询生成之间的差距。MultiLink将任务分解为多个步骤，整合并行多语言处理、思维链推理（Chain-of-Thought, CoT）和检索增强生成（RAG），以应对多语言NoSQL生成固有的词汇和结构挑战。实验表明，与顶级基线相比，MultiLink在每种语言上所有指标上都有提升，尤其是对英语的执行准确性提升了约15%，对非英语语言平均提升了10%。', 'title_zh': 'MultiTEND：一种多语言基准，用于自然语言到NoSQL查询的转换'}
{'arxiv_id': 'arXiv:2502.10896', 'title': 'Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers of Cognition in People Living with Dementia', 'authors': 'Rohith Perumandla, Young-Ho Bae, Diego Izaguirre, Esther Hwang, Andrew Murphy, Long-Jing Hsu, Selma Sabanovic, Casey C. Bennett', 'link': 'https://arxiv.org/abs/2502.10896', 'abstract': "This study presents the development and testing of a conversational speech system designed for robots to detect speech biomarkers indicative of cognitive impairments in people living with dementia (PLwD). The system integrates a backend Python WebSocket server and a central core module with a large language model (LLM) fine-tuned for dementia to process user input and generate robotic conversation responses in real-time in less than 1.5 seconds. The frontend user interface, a Progressive Web App (PWA), displays information and biomarker score graphs on a smartphone in real-time to human users (PLwD, caregivers, clinicians). Six speech biomarkers based on the existing literature - Altered Grammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking, Slurred Pronunciation, and Prosody Changes - were developed for the robot conversation system using two datasets, one that included conversations of PLwD with a human clinician (DementiaBank dataset) and one that included conversations of PLwD with a robot (Indiana dataset). We also created a composite speech biomarker that combined all six individual biomarkers into a single score. The speech system's performance was first evaluated on the DementiaBank dataset showing moderate correlation with MMSE scores, with the composite biomarker score outperforming individual biomarkers. Analysis of the Indiana dataset revealed higher and more variable biomarker scores, suggesting potential differences due to study populations (e.g. severity of dementia) and the conversational scenario (human-robot conversations are different from human-human). The findings underscore the need for further research on the impact of conversational scenarios on speech biomarkers and the potential clinical applications of robotic speech systems.", 'abstract_zh': '本文介绍了为照顾痴呆症患者（PLwD）的机器人设计的一种对话语音系统的发展与测试。该系统整合了一个后端的Python WebSocket服务器和一个中央核心模块，该模块使用针对痴呆症进行微调的大语言模型（LLM），以实时处理用户输入并生成机器人对话响应，时间在1.5秒以内。前端用户界面为渐进式网络应用程序（PWA），它在智能手机上实时显示信息和生物标记评分图，供人类用户（患者、护理人员、临床医生）查看。基于现有文献，系统开发了六个语音生物标记——语法变化、语用损害、异名症、谈话轮流中断、发音含糊和声调变化——其中两个数据集用于系统开发，一个是包含PLwD与临床医生对话的数据集（DementiaBank数据集），另一个是包含PLwD与机器人对话的数据集（Indiana数据集）。此外，我们还创建了一个综合的语音生物标记，将这六个独立的生物标记整合为一个综合评分。语音系统的性能首先在DementiaBank数据集上进行评估，显示出与MMSE评分中等程度的相关性，而综合生物标记得分优于单独的生物标记得分。对Indiana数据集的分析显示了更高的且更加多变的生物标记评分，这可能反映了研究人群的差异（例如痴呆症严重程度）和对话场景的差异（人-机器人对话不同于人-人对话）。研究结果强调了进一步研究对话场景对语音生物标记影响的必要性，以及机器人语音系统潜在临床应用的价值。', 'title_zh': '面向痴呆症患者认知言语生物标志物检测的机器人对话系统开发'}
{'arxiv_id': 'arXiv:2502.10886', 'title': 'MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of Vision-Language and Reasoning Models', 'authors': 'Vanya Cohen, Raymond Mooney', 'link': 'https://arxiv.org/abs/2502.10886', 'abstract': 'Entity tracking is a fundamental challenge in natural language understanding, requiring models to maintain coherent representations of entities. Previous work has benchmarked entity tracking performance in purely text-based tasks. We introduce MET-Bench, a multimodal entity tracking benchmark designed to evaluate the ability of vision-language models to track entity states across modalities. Using two structured domains, Chess and the Shell Game, we assess how effectively current models integrate textual and image-based state updates. Our findings reveal a significant performance gap between text-based and image-based tracking and that this performance gap stems from deficits in visual reasoning rather than perception. We further show that explicit text-based reasoning strategies improve performance, yet substantial limitations remain, especially in long-horizon multimodal scenarios. Our results highlight the need for improved multimodal representations and reasoning techniques to bridge the gap between textual and visual entity tracking.', 'abstract_zh': '实体跟踪是自然语言理解中的一个基本挑战，要求模型保持实体的一致表示。以往的工作主要在纯文本任务中评估实体跟踪性能。我们引入了MET-Bench，这是一个多模态实体跟踪基准，旨在评估视觉-语言模型跨越不同模态跟踪实体状态的能力。通过两个结构化领域——国际象棋和纸牌游戏——我们评估了当前模型如何有效整合基于文本和基于图像的状态更新。我们的研究发现，基于文本和基于图像的跟踪之间存在显著的性能差距，这一差距源于视觉推理能力的不足而不是感知能力。我们还表明，明确的基于文本的推理策略可以提高性能，但在长时程多模态场景中仍然存在显著局限。我们的结果突显了提高多模态表示和推理技术的必要性，以弥合基于文本和基于视觉的实体跟踪之间的差距。', 'title_zh': '_MET-Bench: 多模态实体跟踪，用于评估视觉-语言和推理模型的局限性_\n\n在这个翻译中，“MET-Bench”被保留为原名，因为它很可能是一个特定的模型或数据集的名称。其余部分按照学术规范进行了翻译。'}
{'arxiv_id': 'arXiv:2502.10881', 'title': 'CiteCheck: Towards Accurate Citation Faithfulness Detection', 'authors': 'Ziyao Xu, Shaohang Wei, Zhuoheng Han, Jing Jin, Zhe Yang, Xiaoguang Li, Haochen Tan, Zhijiang Guo, Houfeng Wang', 'link': 'https://arxiv.org/abs/2502.10881', 'abstract': 'Citation faithfulness detection is critical for enhancing retrieval-augmented generation (RAG) systems, yet large-scale Chinese datasets for this task are scarce. Existing methods face prohibitive costs due to the need for manually annotated negative samples. To address this, we introduce the first large-scale Chinese dataset CiteCheck for citation faithfulness detection, constructed via a cost-effective approach using two-stage manual annotation. This method balances positive and negative samples while significantly reducing annotation expenses. CiteCheck comprises training and test splits. Experiments demonstrate that: (1) the test samples are highly challenging, with even state-of-the-art LLMs failing to achieve high accuracy; and (2) training data augmented with LLM-generated negative samples enables smaller models to attain strong performance using parameter-efficient fine-tuning. CiteCheck provides a robust foundation for advancing citation faithfulness detection in Chinese RAG systems. The dataset is publicly available to facilitate research.', 'abstract_zh': '引文忠实性检测对于增强检索增强生成（RAG）系统至关重要，但用于这一任务的大规模中文数据集稀缺。现有方法因其需要手动标注的负面样本而面临高昂的成本。为解决这一问题，我们提出了首个用于引文忠实性检测的大规模中文数据集CiteCheck，该数据集通过一种经济高效的方法构建，采用两阶段的手动标注。这种方法在平衡正负样本的同时，显著降低了标注成本。CiteCheck 包含训练和测试分割。实验证明：（1）测试样本极具挑战性，即使是最先进的大型语言模型（LLM）也无法实现高准确率；（2）结合LLM生成的负面样本的训练数据集，可以使得小型模型通过参数高效微调达到优越性能。CiteCheck 为中文RAG系统中引文忠实性检测的研究提供了坚实的基础。该数据集已公开发布，以促进相关研究。', 'title_zh': 'CiteCheck：迈向准确引文忠实度检测'}
{'arxiv_id': 'arXiv:2502.10871', 'title': 'The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis', 'authors': 'Ge Lei, Samuel J. Cooper', 'link': 'https://arxiv.org/abs/2502.10871', 'abstract': 'This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.', 'abstract_zh': '本研究探讨了大规模语言模型（LLMs）在变压器层间如何表示和回忆多关联属性。我们发现中间层通过在重叠的空间中叠加相关属性来编码事实性知识，并且能够在不明确提示属性的情况下有效回忆。相比之下，较晚的层则进一步细化语言模式，并逐步分离属性表示，优化特定任务的输出，同时适当地限制属性回忆。我们识别出多种编码模式，其中包括首次观察到当探索元素周期表相关信息时存在的三维螺旋结构。研究结果揭示了层间属性表示的动态转变，并为进一步理解LLMs处理复杂、相互关联的知识提供了机制解释和洞察。', 'title_zh': '大型语言模型中交织结构化知识的表示与回忆：一种几何与分层分析'}
{'arxiv_id': 'arXiv:2502.11020', 'title': 'TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages', 'authors': 'Jafar Isbarov, Arofat Akhundjanova, Mammad Hajili, Kavsar Huseynova, Dmitry Gaynullin, Anar Rzayev, Osman Tursun, Ilshat Saetov, Rinat Kharisov, Saule Belginova, Ariana Kenbayeva, Amina Alisheva, Aizirek Turdubaeva, Abdullatif Köksal, Samir Rustamov, Duygu Ataman', 'link': 'https://arxiv.org/abs/2502.11020', 'abstract': 'Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.', 'abstract_zh': '深入评估大规模多任务语言理解（MMLU）能力对于推进多语言语言模型的应用至关重要。然而，准备高质量的母语基准往往成本高昂，因此限制了评估数据集的代表性和全面性。尽管近期努力构建更具包容性的MMLU基准，但这些基准通常基于高资源语言的机器翻译，这可能会引入错误并忽视目标语言的语言和文化复杂性。在本文中，我们针对少有代表性的突厥语族语言，缺少母语MMLU基准的问题，提出了解决方案。我们为突厥语族语言设计了两个MMLU基准：TUMLU是一个综合性的、多语言的、母语开发的了解语言基准，特别针对突厥语族语言。它包含包括阿塞拜疆语、克里米亚鞑靼语、卡拉卡尔帕克语、哈萨克语、塔塔尔语、土耳其语、维吾尔语和乌兹别克语在内的11个学术学科的中学和高中水平问题。我们还提出了TUMLU-mini，这是一个更简洁、更平衡并经过人工验证的数据子集。利用这个数据集，我们系统地评估了包括Claude、Gemini、GPT和LLaMA在内的多样化的开源和专有大型多语言语言模型（LLMs），提供了它们在不同语言、学科和字母表上的性能深度分析。为推动多语言语言理解领域的进一步研究和开发，我们公开发布了TUMLU-mini以及所有相应的评估脚本。', 'title_zh': 'TUMLU：一个统一的土耳其语族语言理解基准测试'}
{'arxiv_id': 'arXiv:2502.11018', 'title': 'GRIFFIN: Effective Token Alignment for Faster Speculative Decoding', 'authors': 'Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou', 'link': 'https://arxiv.org/abs/2502.11018', 'abstract': "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\\% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).", 'abstract_zh': '推测解码通过同时生成多个草稿令牌来加速大型语言模型（LLMs）的推理过程。然而，现有的方法往往难以在训练和解码阶段之间保持令牌对齐，从而限制了它们的性能。为了解决这个问题，我们提出了一种名为GRIFFIN的新框架，该框架结合了一种可对齐训练策略和一种可对齐的草稿模型，以减轻不匹配问题。该训练策略采用了损失屏蔽机制，在训练过程中排除高度不匹配的令牌，防止它们对草稿模型的优化产生负面影响。可对齐的草稿模型通过引入输入令牌来纠正生成特征中的不一致。实验结果表明，GRIFFIN在LLaMA系列和Vicuna模型上的平均接受长度提升了超过7%，并且加速比超过8%，如图1(a)和(b)所示，这优于当前的最高水平方法。', 'title_zh': 'GRIFFIN: 有效的 token 对齐以实现更快的推测解码'}
{'arxiv_id': 'arXiv:2502.11008', 'title': 'CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models', 'authors': 'Yuefei Chen, Vivek K.Singh, Jing Ma, Ruxiang Tang', 'link': 'https://arxiv.org/abs/2502.11008', 'abstract': "Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules. To support this evaluation, we introduce a new benchmark dataset, CounterBench, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different this http URL dataset is available at this https URL.", 'abstract_zh': '因果推理被广泛认为是人工智能中最具挑战性和复杂性的方面之一。本文评估了大型语言模型（LLMs）在因果推理中的表现。不同于以往主要集中在常识因果推理的研究，这些研究通常依赖于先验知识进行推理，我们特别评估了它们在使用一套形式化规则进行反事实推理的能力。为了支持这一评估，我们引入了一个新的基准数据集CounterBench，包含1000个反事实推理问题。该数据集设计了不同的难度级别、多样的因果图结构、不同类型的反事实问题，以及多种不合逻辑的名称变体。实验结果显示，反事实推理对LLMs构成了显著挑战，大多数模型的表现与随机猜测相似。为了增强LLMs的反事实推理能力，我们提出了一种新的推理范式CoIn，该范式引导LLMs通过迭代推理和回溯来系统地探索反事实解决方案。实验结果表明，我们的方法在反事实推理任务上显著提高了LLMs的表现，并在不同类型的CounterBench数据集上均表现更优。该数据集可在[此处提供链接]获取。', 'title_zh': 'CounterBench：大规模语言模型因果推理基准测试'}
{'arxiv_id': 'arXiv:2502.10996', 'title': 'RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation', 'authors': 'Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han', 'link': 'https://arxiv.org/abs/2502.10996', 'abstract': 'Retrieval-augmented language models often struggle with knowledge-intensive tasks due to inefficient retrieval, unstructured knowledge integration, and single-pass architectures. We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS introduces four key technical innovations: (1) a themescoped retrieval mechanism that efficiently narrows the search space while maintaining retrieval quality, (2) an action planning module that determines knowledge needs and generates focused sub-queries, (3) a dynamic knowledge structuring approach that converts retrieved text into an evolving knowledge graph, and (4) a graph-augmented answering component that leverages the accumulated structured information. Our framework achieves state-of-the-art performance, surpassing leading baselines by 6.4% with open-source language models and 7.0% with proprietary models on seven knowledge-intensive generation datasets across all evaluation metrics. Detailed ablation studies verify the contribution of each technical component to the overall system performance.', 'abstract_zh': '检索增强语言模型在处理知识密集型任务时常常受限于检索效率低下、知识整合无序以及单次通过的架构。本文提出了一个名为检索与结构化（RAS）的新框架，该框架能够通过迭代检索和结构化动态构建和推理查询特定的知识图谱。RAS 引入了四项关键技术创新：（1）主题导向的检索机制，能够高效地缩小搜索空间并保持检索质量；（2）行动规划模块，确定知识需求并生成聚焦的子查询；（3）动态知识结构化方法，将检索到的文字信息转换为不断进化的知识图谱；（4）图增强回答组件，利用累积的结构化信息进行回答。该框架在七个知识密集型生成数据集的全部评估指标中均超越了开源语言模型和专有模型，分别提高了 6.4% 和 7.0% 的性能。详细的消融实验验证了每个技术组件对整体系统性能的贡献。', 'title_zh': 'RAS：检索与结构化方法在知识密集型大语言模型生成中的应用'}
{'arxiv_id': 'arXiv:2502.10995', 'title': 'Evaluating Large language models on Understanding Korean indirect Speech acts', 'authors': 'Youngeun Koo, Jiwoo Lee, Dojun Park, Seohyun Park, Sungeun Lee', 'link': 'https://arxiv.org/abs/2502.10995', 'abstract': "To accurately understand the intention of an utterance is crucial in conversational communication. As conversational artificial intelligence models are rapidly being developed and applied in various fields, it is important to evaluate the LLMs' capabilities of understanding the intentions of user's utterance. This study evaluates whether current LLMs can understand the intention of an utterance by considering the given conversational context, particularly in cases where the actual intention differs from the surface-leveled, literal intention of the sentence, i.e. indirect speech acts. Our findings reveal that Claude3-Opus outperformed the other competing models, with 71.94% in MCQ and 65% in OEQ, showing a clear advantage. In general, proprietary models exhibited relatively higher performance compared to open-source models. Nevertheless, no LLMs reached the level of human performance. Most LLMs, except for Claude3-Opus, demonstrated significantly lower performance in understanding indirect speech acts compared to direct speech acts, where the intention is explicitly revealed through the utterance. This study not only performs an overall pragmatic evaluation of each LLM's language use through the analysis of OEQ response patterns, but also emphasizes the necessity for further research to improve LLMs' understanding of indirect speech acts for more natural communication with humans.", 'abstract_zh': '准确理解对话中话语的意图对于会话交流至关重要。随着对话型人工智能模型在各个领域的迅速发展和应用，评估大语言模型（LLMs）理解用户话语意图的能力变得尤为重要。本研究通过考虑给定的对话背景，特别是当实际意图与句子的表面或字面意图不符时，即间接言语行为，评估当前LLMs是否能够理解话语意图。我们的研究成果显示，Claude3-Opus 在选择题（MCQ）中的得分为 71.94%，在开放式问题（OEQ）中的得分为 65%，表现出明显的优越性。总体而言，专用模型的表现相对较高，而开源模型则较低。不过，没有一个 LLM 达到了人类的性能水平。除了 Claude3-Opus 之外，大多数 LLMs 在理解间接言语行为方面表现显著低于直接言语行为，而在直接言语行为中意图通过话语明确揭示。本研究不仅通过分析 OEQ 的答题模式对每个 LLM 语言使用进行了全面的语用评估，还强调了为进一步改进 LLMs 对间接言语行为的理解，以实现更加自然的人机交流，而需要进一步研究的必要性。', 'title_zh': '评估大型语言模型在理解韩语间接言语行为方面的能力'}
{'arxiv_id': 'arXiv:2502.10868', 'title': 'NitiBench: A Comprehensive Studies of LLM Frameworks Capabilities for Thai Legal Question Answering', 'authors': 'Pawitsapak Akarajaradwong, Pirat Pothavorn, Chompakorn Chaksangchaichot, Panuthep Tasawong, Thitiwat Nopparatbundit, Sarana Nutanong', 'link': 'https://arxiv.org/abs/2502.10868', 'abstract': 'The application of large language models (LLMs) in the legal domain holds significant potential for information retrieval and question answering, yet Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks and the complexity of Thai legal structures. This paper introduces NitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering general Thai financial law, and the NitiBench-Tax, which includes real-world tax law cases requiring advanced legal reasoning. We evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches to address three key research questions: the impact of domain-specific components like section-based chunking and cross-referencing, the comparative performance of different retrievers and LLMs, and the viability of long-context LLMs as an alternative to RAG. Our results show that section-based chunking significantly improves retrieval and end-to-end performance, current retrievers struggle with complex queries, and long-context LLMs still underperform RAG-based systems in Thai legal QA. To support fair evaluation, we propose tailored multi-label retrieval metrics and the use of an LLM-as-judge for coverage and contradiction detection method. These findings highlight the limitations of current Thai legal NLP solutions and provide a foundation for future research in the field. We also open-sourced our codes and dataset to available publicly.', 'abstract_zh': '将以下论文内容或标题翻译成中文，符合学术规范：\n\n大型语言模型（LLM）在法律领域的应用具有显著的信息检索和问答潜力，但由于缺乏标准化的评估基准和复杂的泰国法律结构，泰国法律问答系统面临挑战。本文引入了NitiBench，这是一个基准，包含两个数据集：NitiBench-CCL，涵盖一般泰国金融法律；以及NitiBench-Tax，包括涉及高级法律推理的实际税法案例。我们评估了补充检索生成（RAG）和基于长上下文的LLM方法，以解决三个核心研究问题：领域特定组件（如基于段落的分块和交叉引用）的影响、不同检索工具和LLM之间的性能比较以及长上下文LLM作为RAG替代方案的可行性。我们的结果显示，基于段落的分块显著提高了检索和端到端性能，当前的检索工具难以处理复杂的查询，并且在泰国法律问答中，长上下文LLM的表现仍然逊于基于RAG的系统。为实现公平评估，我们提出了定制的多标签检索指标，并提出了使用LLM作为评估者的方法，用于覆盖和矛盾检测。这些发现突显了当前泰国法律NLP解决方案的局限性，并为该领域的未来研究奠定了基础。我们还开源了我们的代码和数据集，供公众使用。', 'title_zh': 'NitiBench：泰国法律问题回答能力的全面研究LLM框架能力'}
{'arxiv_id': 'arXiv:2502.10993', 'title': 'RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization', 'authors': 'Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, Haoyu Wang', 'link': 'https://arxiv.org/abs/2502.10993', 'abstract': 'Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.', 'abstract_zh': '大型语言模型（LLMs）在性能上取得了令人印象深刻的成果，但同时也面临着高昂的计算成本和延迟问题，这限制了它们在资源受限环境中的应用。相比之下，小型语言模型（SLMs）更加高效，但难以捕捉到不断变化的现实世界知识。检索增强生成（Rag）可以通过整合外部知识来帮助解决这一问题，然而不完善的检索可能会引入分散注意力的噪声，从而误导SLMs。\n\n为此，我们提出了RoseRAG，这是一种通过边缘意识偏好优化来增强SLMs的鲁棒Rag框架。RoseRAG采用多轮提示进行详细的推理，使用拒绝采样来生成高质量的解释，并通过对比偏好选择来优化响应，以最大化优选输出与非优选输出之间的似然性差距。通过将这些组件整合到一种边缘意识的优化过程中，RoseRAG能够稳健地提高SLMs在Rag应用中的准确性和可靠性。在对三个开放域问答基准上的广泛实验表明，我们的创新性RoseRAG显著超过了现有最先进的基线方法。', 'title_zh': 'RoseRAG：通过边际意识偏好优化在小规模LLM中实现稳健的检索增强生成'}
{'arxiv_id': 'arXiv:2502.10857', 'title': 'Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation', 'authors': 'Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu', 'link': 'https://arxiv.org/abs/2502.10857', 'abstract': 'Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.', 'abstract_zh': '近年来，随着大型语言模型（LLMs）在工具调用能力方面的不断发展，这些模型通过EDA脚本与EDA工具API进行交互，展示了在自动化电子设计自动化（EDA）流程方面的巨大潜力。然而，考虑到对EDA工具理解的局限性，当不同平台上EDA工具的接口多样时，LLMs在实际应用场景中遇到了挑战。此外，EDA流程自动化通常涉及复杂的、多步骤的工具调用过程，增加了中间步骤出错的可能性。任何错误都会导致EDA流程自动化的不稳定和失败。为解决这些挑战，我们引入了EDAid，这是一种多代理协作系统，多个持有不同思想的代理共同朝向一个共同目标，确保可靠且成功的EDA流程自动化。具体来说，每个代理由专门针对EDA流程自动化微调的ChipLlama模型控制。我们的实验展示了ChipLlama模型的最优性能，并验证了EDAid在复杂EDA流程自动化中的有效性，相较于单代理系统，其性能更加优越。', 'title_zh': '《殊途同归：基于大型语言模型的多 agent 协同系统在电子设计自动化中的应用》\n\n这个标题翻译成中文既符合学术规范，又保留了原文的意思。其中，“Divergent Thoughts towards One Goal”被译为“殊途同归”，形象地表达了多个不同的思路或方法最终朝着同一个目标努力。“LLM-based Multi-Agent Collaboration System”被译为“基于大型语言模型的多 agent 协同系统”，准确地反映了系统的核心技术和特点。“Electronic Design Automation”被译为“电子设计自动化”，是该领域的标准术语。'}
{'arxiv_id': 'arXiv:2502.10855', 'title': 'Towards Effective Extraction and Evaluation of Factual Claims', 'authors': 'Dasha Metropolitansky, Jonathan Larson', 'link': 'https://arxiv.org/abs/2502.10855', 'abstract': 'A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.', 'abstract_zh': '对大型语言模型（LLMs）生成的长篇内容进行事实核查的常用策略是提取可以独立验证的简单声明。由于不准确或不完备的声明会损害事实核查的结果，确保声明质量至关重要。然而，缺乏标准化的评估框架阻碍了对声明提取方法的评估和比较。为解决这一问题，我们提出了一种在事实核查背景下评估声明提取的方法框架，并提供了自动、可扩展且可复制的方法来应用该框架，包括用于衡量覆盖范围和脱离语境的新颖方法。我们还介绍了基于大型语言模型的声明提取方法——Claimify，并在我们的评估框架下证明了它优于现有方法。Claimify 的一个关键特征是其能够处理歧义，在对源文本的正确解释有高信心时才提取声明。', 'title_zh': '面向事实声明的有效提取与评估'}
{'arxiv_id': 'arXiv:2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'authors': 'Zeli Su, Ziyin Zhang, Guixian Xu, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong', 'link': 'https://arxiv.org/abs/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'abstract_zh': '虽然像XLM-R这样的多语言模型在自然语言处理（NLP）中推动了多语言能力的发展，但在极度低资源语言方面仍然表现不佳。这一情况进一步加剧了现代大型语言模型（LLM），例如LLaMA和Qwen支持的语言种类远少于XLM-R的事实，导致全球许多语言在文本生成模型上处于空白状态。为应对这一挑战，我们提出了一种新的框架，用于适应极度低资源语言中的多语言编码器以进行文本生成。通过在编码器和解码器之间重用权重，该框架使模型能够利用编码器中学习到的语义空间，从而在低资源语言中实现高效的机器学习和有效的泛化能力。将该框架应用于四种汉语少数民族语言，我们提出了XLM-SWCM，并在多种下游任务中展示了其优于更大规模模型的性能。', 'title_zh': '多语言编码器的知识远超你所知：面向极低资源语言的共享权重预训练'}
{'arxiv_id': 'arXiv:2502.10990', 'title': 'FinMTEB: Finance Massive Text Embedding Benchmark', 'authors': 'Yixuan Tang, Yi Yang', 'link': 'https://arxiv.org/abs/2502.10990', 'abstract': 'Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, FinPersona-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including FinPersona-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.', 'abstract_zh': '嵌入模型在各种自然语言处理（NLP）应用中扮演着至关重要的角色，用于表示和检索信息。近年来，大型语言模型（LLMs）的进步进一步提升了嵌入模型的性能。尽管这些模型通常是在通用数据集上进行基准测试，但实际应用的需求则需要针对特定领域的评估。在本研究中，我们引入了专门针对金融领域的金融大规模文本嵌入基准（FinMTEB），它是 MTEB 的专门对应版本，针对金融领域的应用。FinMTEB 包含了涵盖7个任务的64个金融领域的特定嵌入数据集，这些数据集在中文和英文中都包含多种文本类型，例如金融新闻文章、公司年报、ESG报告、监管文件和财报电话会议转录。我们还利用基于角色的合成数据方法开发了一种适应金融领域的模型 FinPersona-E5，用于训练多种金融嵌入任务。通过对包括 FinPersona-E5 在内的15个嵌入模型进行广泛评估，我们得出了以下三个关键发现：（1）通用基准测试中的性能与金融领域任务之间存在有限的相关性；（2）适应领域的模型始终优于通用模型；（3）令人惊讶的是，一个简单的词频统计（Bag-of-Words, BoW）方法在金融语义文本相似性（STS）任务中优于复杂的密集嵌入方法，这突显了当前密集嵌入技术的局限性。我们的研究建立了用于金融NLP应用的稳健评估框架，并提供了发展特定领域嵌入模型的关键见解。', 'title_zh': 'FinMTEB：金融大规模文本嵌入基准'}
{'arxiv_id': 'arXiv:2502.10835', 'title': 'Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models', 'authors': 'Zeping Yu, Yonatan Belinkov, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.10835', 'abstract': 'We investigate how large language models perform latent multi-hop reasoning in prompts like "Wolfgang Amadeus Mozart\'s mother\'s spouse is". To analyze this process, we introduce logit flow, an interpretability method that traces how logits propagate across layers and positions toward the final prediction. Using logit flow, we identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction. Extending this analysis to multi-hop reasoning, we find that failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy. To address this, we propose back attention, a novel mechanism that enables lower layers to leverage higher-layer hidden states from different positions during attention computation. With back attention, a 1-layer transformer achieves the performance of a 2-layer transformer. Applied to four LLMs, back attention improves accuracy on five reasoning datasets, demonstrating its effectiveness in enhancing latent multi-hop reasoning ability.', 'abstract_zh': '我们探讨了大型语言模型在类似“Wolfgang Amadeus Mozart的母亲的配偶是”这样的提示中执行隐式多跳推理的过程。为了分析这一过程，我们引入了逻辑值流（logit flow）——一种解释性方法，用于追踪逻辑值如何在各层和位置之间传播，最终影响最终预测。通过逻辑值流，我们确定了单跳知识预测中的四个不同阶段：（A）实体主语丰富、（B）实体属性提取、（C）关系主语丰富、以及（D）关系属性提取。将这种分析扩展到多跳推理中，我们发现失败通常源于关系属性提取阶段，其中冲突的逻辑值会降低预测准确性。为了解决这一问题，我们提出了反向注意（back attention），这是一种新颖机制，能够在注意计算过程中使较低层利用来自不同位置的较高层隐藏状态。借助反向注意，单层变换器能够达到双层变换器的性能。将反向注意应用于四种大规模语言模型（LLM），它在五个推理数据集中提高了准确率，证明了其在增强隐式多跳推理能力方面的有效性。', 'title_zh': '_backattention：理解并增强大规模语言模型中的多跳推理_'}
{'arxiv_id': 'arXiv:2502.10760', 'title': 'Why is prompting hard? Understanding prompts on binary sequence predictors', 'authors': 'Li Kevin Wenliang, Anian Ruoss, Jordi Grau-Moya, Marcus Hutter, Tim Genewein', 'link': 'https://arxiv.org/abs/2502.10760', 'abstract': 'Large language models (LLMs) can be prompted to do many tasks, but finding good prompts is not always easy, nor is understanding some performant prompts. We explore these issues by viewing prompting as conditioning a near-optimal sequence predictor (LLM) pretrained on diverse data sources. Through numerous prompt search experiments, we show that the unintuitive patterns in optimal prompts can be better understood given the pretraining distribution, which is often unavailable in practice. Moreover, even using exhaustive search, reliably identifying optimal prompts from practical neural predictors can be difficult. Further, we demonstrate that common prompting methods, such as using intuitive prompts or samples from the targeted task, are in fact suboptimal. Thus, this work takes an initial step towards understanding the difficulties in finding and understanding optimal prompts from a statistical and empirical perspective.', 'abstract_zh': '大型语言模型（LLM）可以被引导执行多种任务，但找到好的提示并不总是容易的，而且理解一些高效的提示也颇具挑战。我们通过将提示视为对预训练于多种数据源的接近最优序列预测器进行条件化的视角来探讨这些问题。通过大量的提示搜索实验，我们表明，在给定预训练分布的情况下，可以更好地理解最优提示中的一些非直观模式，而预训练分布在实践中往往不可用。此外，即使使用穷举搜索方法，也难以可靠地从实际神经预测器中识别出最优提示。进一步地，我们证明了许多常见的提示方法，例如使用直观的提示或目标任务的样本，实际上并不是最优的。因此，本文从统计学和经验的角度出发，迈出了理解找到和理解最优提示困难性的初步步骤。', 'title_zh': '为什么提示如此困难？理解对二进制序列预测器的提示'}
{'arxiv_id': 'arXiv:2502.10973', 'title': 'Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues', 'authors': 'David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter', 'link': 'https://arxiv.org/abs/2502.10973', 'abstract': 'In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources.', 'abstract_zh': '在本文中，我们介绍了阿堪语对话情感（ACE）数据集，这是首个针对非洲语言的情感对话多模态数据集，旨在弥补情感识别研究中低资源语言资源严重不足的问题。ACE 数据集为阿堪语开发，包含385条标记了情感的对话和6,162个跨音频、视觉和文本模态的语句，同时还包含词级音调 prominence 注释。ACE 数据集中的音调标签使其成为首个标注音调信息的非洲语言数据集。我们通过使用当前最先进的表情识别方法进行实验，展示了 ACE 的质量和实用性，并为未来研究建立了坚实的基础。我们希望 ACE 能激励更多关于包容性、语言和文化多样性的自然语言处理资源的研究。', 'title_zh': '《Akan电影情感（ACE）：电影对话中情感识别的多模态多主体数据集》'}
{'arxiv_id': 'arXiv:2502.10749', 'title': 'LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging', 'authors': 'Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2502.10749', 'abstract': 'While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.', 'abstract_zh': '虽然大多数当前的方法依赖于进一步的训练技术，如微调或强化学习，以增强模型能力，但模型合并因其在无需额外训练的情况下提高模型能力的能力而脱颖而出。在本文中，我们提出了一种基于低秩估计任务向量的统一框架，名为 \\textsc{LoRE-Merging}，且该框架无需访问基础模型。我们的方法受到这样一个观察的启发：微调模型的任务向量通常表现出有限数量的主导奇异值，这使得低秩估计不易受到干扰。我们通过将合并问题表述为一个优化问题来实现这一方法。大量的实证实验表明，我们的框架在减轻干扰和保留任务特定信息方面是有效的，从而推动了模型合并技术的最先进性能。', 'title_zh': 'LoRE-融合：探索低秩估计在大型语言模型融合中的应用'}
{'arxiv_id': 'arXiv:2502.10966', 'title': 'Neural Networks Remember More: The Power of Parameter Isolation and Combination', 'authors': 'Biqing Zeng, Zehan Li, Aladdin Ayesh', 'link': 'https://arxiv.org/abs/2502.10966', 'abstract': "Catastrophic forgetting is a pervasive issue for pre-trained language models (PLMs) during continual learning, where models lose previously acquired knowledge when sequentially trained on a series of tasks. The model's ability to retain old tasks is referred to as stability, while its adaptability to new tasks is called plasticity. Therefore, the key to solving this problem is to find a trade-off between the plasticity and stability of the model. To address this issue, in this paper, we propose a novel method to achieve a balance between model stability and plasticity, thereby mitigating catastrophic forgetting. More specifically, our proposed approach leverages parameter isolation and a subsequent combination strategy. Initially, in the training stage, the model adapts to each downstream task via a parameter isolation method to prevent potential interference among different tasks. We then combine all trained parameters, which contain acquired knowledge, using the task arithmetic method and finally apply them to the backbone model. Empirical evaluations on continual language learning benchmarks substantiate the effectiveness of our approach, revealing a marked enhancement over existing state-of-the-art approaches.", 'abstract_zh': '灾难性遗忘是预训练语言模型（PLMs）在连续学习过程中面临的一个普遍问题，在顺序训练于一系列任务时，模型会失去先前获得的知识。模型保留旧任务能力被称为稳定性，而适应新任务的能力则称为可塑性。因此，解决这一问题的关键在于在模型的可塑性和稳定性之间找到一个平衡。为了应对这一问题，本文提出了一种新方法，旨在在模型稳定性和可塑性之间取得平衡，从而减轻灾难性遗忘现象。具体而言，我们提出的这种方法利用了参数隔离和后续的组合策略。首先，在训练阶段，通过参数隔离方法使模型适应每个下游任务，以防止不同任务之间的潜在干扰。然后，我们使用任务算术方法将所有训练参数组合在一起，这些参数包含了所获得的知识，并最终将这些参数应用于基础模型。在连续语言学习基准上的实证评估证实了我们方法的有效性，表明其相对于现有最先进的方法具有显著的改进。', 'title_zh': '神经网络的记忆力更强：参数隔离与组合的力量'}
{'arxiv_id': 'arXiv:2502.10942', 'title': 'Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks', 'authors': 'Henry Evidail, Zachary Mountebank, Alistair Hathersage, Peter Stanhope, Basil Ravenscroft, Tobias Waddingham', 'link': 'https://arxiv.org/abs/2502.10942', 'abstract': 'Self-modulating mechanisms introduce dynamic adaptation capabilities within language models through contextual realignment strategies that influence token embedding trajectories across extended sequences. Contextual Flux is explored as an approach to embedding modulation, integrating an auxiliary gating mechanism within the self-attention framework to dynamically adjust token representations based on evolving contextual dependencies. The empirical analysis evaluates entropy variations, latent space realignments, and coherence stability to assess the extent to which self-regulation enhances text generation consistency while preserving generative flexibility. Quantitative assessments suggest that embedding shifts contribute to more structured adaptation in long-form sequences, with measured reductions in redundant phrase repetitions and improvements in thematic retention. Variability in contextual weight computation affects modulation stability, leading to differing levels of adaptation across diverse linguistic structures. The computational demands introduced through real-time embedding reconfiguration are examined in relation to model scalability, emphasizing the need for optimization strategies in high-volume generative applications. The findings suggest that while adaptive embedding updates improve certain aspects of coherence, their impact remains contingent on model capacity and input complexity.', 'abstract_zh': '自调节机制通过在语言模型中引入动态适应能力，利用上下文对齐策略影响跨长序列的标记嵌入轨迹。研究探讨了上下文流量（Contextual Flux）作为嵌入调节的方法，通过在自注意力框架中引入辅助门控机制，动态调整基于不断发展中的上下文依赖性的标记表示。实证分析评估了熵变化、潜在空间对齐和连贯性稳定性，以评估自调节如何在保持生成灵活性的同时增强文本生成的一致性。量化评估表明，在长文本序列中，嵌入位移有助于更有序的适应，表现为冗余短语重复减少和主题保留改善。上下文权重计算的一致性对调节稳定性有影响，导致不同的适应水平在不同的语言结构中表现不同。实现实时嵌入重构带来的计算需求在模型可扩展性方面的考量中受到重视，突显了在高生成量应用中需要优化策略的必要性。研究发现，虽然适应性嵌入更新在某些方面改善了连贯性，但其影响仍然依赖于模型容量和输入复杂性。', 'title_zh': '探索大规模语言模型中的上下文变化：一种新型自我调节语义网络的方法'}
{'arxiv_id': 'arXiv:2502.10743', 'title': '1bit-Merging: Dynamic Quantized Merging for Large Language Models', 'authors': 'Shuqi Liu, Han Wu, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Linqi Song', 'link': 'https://arxiv.org/abs/2502.10743', 'abstract': 'Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.', 'abstract_zh': '近期大规模语言模型的发展导致了专用于特定领域的模型出现，这引发了高效模型融合技术的需求。虽然传统的合并方法将参数合并为单一静态模型，但往往会在特定任务的性能上妥协。然而，专门的任务路由方法虽然能够保持准确性，但会引入大量的存储开销。我们提出了\\texttt{1bit}-Merging这一创新框架，结合了任务特定路由和1比特量化任务向量，以平衡性能和存储效率。我们的方法利用了一个观察结果，即不同领域的特化模型在其知识存储的层上有所区别——聊天模型主要在注意力层存储知识，而数学/代码模型则在MLP层存储——这使得可以实施有针对性的压缩策略。通过在LLaMA2和Mistral模型系列中进行广泛的跨聊天、数学推理和代码生成任务实验，我们证明\\texttt{1bit}-Merging在存储需求显著降低的同时，能够达到现有方法的可比甚至更优的性能。我们的框架为结合特殊化模型提供了一个实用的解决方案，同时保持它们各自的优点，并解决现有方法中的存储挑战。', 'title_zh': '1比特合并：大型语言模型的动态量化合并方法'}
{'arxiv_id': 'arXiv:2502.10934', 'title': 'Fundamental Principles of Linguistic Structure are Not Represented by o3', 'authors': 'Elliot Murphy, Evelina Leivada, Vittoria Dentella, Fritz Gunther, Gary Marcus', 'link': 'https://arxiv.org/abs/2502.10934', 'abstract': "A core component of a successful artificial general intelligence would be the rapid creation and manipulation of grounded compositional abstractions and the demonstration of expertise in the family of recursive hierarchical syntactic objects necessary for the creative use of human language. We evaluated the recently released o3 model (OpenAI; o3-mini-high) and discovered that while it succeeds on some basic linguistic tests relying on linear, surface statistics (e.g., the Strawberry Test), it fails to generalize basic phrase structure rules; it fails with comparative sentences involving semantically illegal cardinality comparisons ('Escher sentences'); its fails to correctly rate and explain acceptability dynamics; and it fails to distinguish between instructions to generate unacceptable semantic vs. unacceptable syntactic outputs. When tasked with generating simple violations of grammatical rules, it is seemingly incapable of representing multiple parses to evaluate against various possible semantic interpretations. In stark contrast to many recent claims that artificial language models are on the verge of replacing the field of linguistics, our results suggest not only that deep learning is hitting a wall with respect to compositionality (Marcus 2022), but that it is hitting [a [stubbornly [resilient wall]]] that cannot readily be surmounted to reach human-like compositional reasoning simply through more compute.", 'abstract_zh': '成功的人工通用智能的一个核心组成部分将是快速创建和操作与具体事物相关联的组合抽象，并展示出对递归层次结构句法对象的专长，这些对象对于创造性地使用人类语言是必要的。我们评估了最近发布的o3模型（OpenAI；o3-mini-high），发现虽然它在一些依赖于线性、表面统计的语言测试中（如草莓测试）表现出色，但它无法泛化基本的短语结构规则；在涉及语义上非法基数比较的比较句（如Escher句子）上失败；无法正确评价和解释接受性动态；也无法区分生成不适当语义输出与不适当句法输出之间的指令。当要求其生成简单的语法规则违反示例时，它似乎无法表示多种解析来评估各种可能的语义解释。与许多近期声称人工智能语言模型即将取代语言学领域的说法形成鲜明对比的是，我们的结果不仅表明深度学习在组合性方面遇到了瓶颈（Marcus 2022），而且表明存在一个顽固且难以克服的障碍，即仅通过增加计算力无法轻易达到类似人类的组合性推理水平。', 'title_zh': '语言结构的基本原理并不由o3表示。'}
{'arxiv_id': 'arXiv:2502.10739', 'title': 'BASE-SQL: A powerful open source Text-To-SQL baseline approach', 'authors': 'Lei Sheng, Shuai-Shuai Xu, Wei Xie', 'link': 'https://arxiv.org/abs/2502.10739', 'abstract': 'The conversion of natural language into SQL language for querying databases (Text-to-SQL) has broad application prospects and has attracted widespread attention. At present, the mainstream Text-to-SQL methods are mainly divided into in-context learning (ICL) based methods and supervised fine-tuning (SFT) based methods. ICL-based methods can achieve relatively good results thanks to the use of the most advanced closed-source models. However, in real-world application scenarios, factors such as data privacy, SQL generation efficiency and cost need to be considered. SFT-based methods have certain advantages. At present, methods based on fine-tuning of open source models lack easy-to-implement and effective (cost-effective) baseline methods. We propose a pipeline-based method using open source model fine-tuning, referred to as BASE-SQL, which includes four components: Schema Linking, Candidate SQL Generate, SQL Revision and SQL Merge Revision. Experimental results show that BASE-SQL uses the open source model Qwen2.5-Coder-32B-Instruct, and achieves an accuracy of 67.47% on the BIRD development set and 88.9% on the Spider test set, which is significantly better than other methods using open source models, and even exceeds several methods using the GPT-4o closed-source model. At the same time, BASE-SQL is easy to implement and highly efficient (on average, only five calls to the large language model are required to generate SQL once). The code will be open sourced at this https URL.', 'abstract_zh': '将自然语言转换为SQL语言以查询数据库（Text-to-SQL）在应用前景方面具有广泛的可能性，并已引起广泛关注。目前，主流的Text-to-SQL方法主要分为上下文学习（Contextual Learning，CL）方法和监督微调（Supervised Fine-tuning，SFT）方法。CL方法通过使用最先进的闭源模型能够取得相对较好的结果。然而，在实际应用场景中，数据隐私、SQL生成效率和成本等因素需要考虑。SFT方法具有一定的优势。目前，基于开源模型微调的方法缺乏易于实现且有效的（成本效益高的）基线方法。我们提出了一种基于微调开源模型的流水线方法，称为BASE-SQL，包括四个组成部分：模式链接、候选SQL生成、SQL修订和SQL合并修订。实验结果显示，BASE-SQL使用开源模型Qwen2.5-Coder-32B-Instruct，在BIRD开发集上的准确率为67.47%，在Spider测试集上的准确率为88.9%，显著优于其他使用开源模型的方法，甚至超过了几个使用GPT-4o闭源模型的方法。同时，BASE-SQL易于实现且高度高效（平均而言，每次生成SQL只需调用大型语言模型五次）。代码将在以下链接开源：[链接]。', 'title_zh': 'BASE-SQL：一种强大的开源文本到SQL基准方法'}
{'arxiv_id': 'arXiv:2502.10921', 'title': 'Evolving Hate Speech Online: An Adaptive Framework for Detection and Mitigation', 'authors': 'Shiza Ali, Gianluca Stringhini', 'link': 'https://arxiv.org/abs/2502.10921', 'abstract': 'The proliferation of social media platforms has led to an increase in the spread of hate speech, particularly targeting vulnerable communities. Unfortunately, existing methods for automatically identifying and blocking toxic language rely on pre-constructed lexicons, making them reactive rather than adaptive. As such, these approaches become less effective over time, especially when new communities are targeted with slurs not included in the original datasets. To address this issue, we present an adaptive approach that uses word embeddings to update lexicons and develop a hybrid model that adjusts to emerging slurs and new linguistic patterns. This approach can effectively detect toxic language, including intentional spelling mistakes employed by aggressors to avoid detection. Our hybrid model, which combines BERT with lexicon-based techniques, achieves an accuracy of 95% for most state-of-the-art datasets. Our work has significant implications for creating safer online environments by improving the detection of toxic content and proactively updating the lexicon. Content Warning: This paper contains examples of hate speech that may be triggering.', 'abstract_zh': '社交媒体平台的发展导致了仇恨言论传播的增加，特别是针对脆弱社区。不幸的是，现有的自动识别和阻止有毒语言的方法依赖于预先构建的词典，使其成为一种被动而非主动的方法。因此，这些方法随着时间的推移变得越来越无效，特别是在新的社区受到不包括在原始数据集中的歧视性语言攻击时。为了解决这一问题，我们提出了一种适应性方法，该方法利用词嵌入更新词典，并开发了一种混合模型以适应新兴的歧视性语言和新的语言模式。该方法能够有效检测有毒语言，包括攻击者故意拼写错误以逃避检测的行为。我们的混合模型结合了BERT和基于词典的技术，对于大多数最先进的数据集，准确率达到了95%。我们的研究对于创建更安全的网络环境具有重要意义，通过改进有毒内容的检测和主动更新词典来提高这一能力。内容警告：本文包含可能触发某些人的仇恨言论示例。', 'title_zh': '在线恶 Hate Speech 的演变：一种检测与遏制的适应性框架'}
{'arxiv_id': 'arXiv:2502.10735', 'title': 'OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization', 'authors': 'Shuqi Liu, Bowei He, Han Wu, Linqi Song', 'link': 'https://arxiv.org/abs/2502.10735', 'abstract': 'Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies inadequate for multiple models. In this paper, we introduce \\textbf{\\textsc{OptiShear}}, an efficient evolutionary optimization framework for adaptive LLM pruning. Our framework features two key innovations: an effective search space built on our Meta pruning metric to handle diverse weight distributions, and a model-wise reconstruction error for rapid evaluation during search trials. We employ Non-dominated Sorting Genetic Algorithm III (NSGA-III) to optimize both pruning metrics and layerwise sparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models (7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning metrics consistently outperform existing methods. Additionally, our discovered layerwise sparsity ratios enhance the effectiveness of other pruning metrics. The framework exhibits strong cross-task and cross-model generalizability, providing a cost-effective solution for model compression.', 'abstract_zh': '基于训练后剪枝已逐渐成为一项关键的优化技术，尤其是在大型语言模型（LLMs）快速增长的背景下。然而，不同LLMs之间显著的权重分布差异使得固定剪枝策略无法适用于多种模型。在本文中，我们提出了一种高效的进化优化框架——\\textbf{\\textsc{OptiShear}}，用于适应性LLM剪枝。该框架包含两项关键技术创新：基于我们元剪枝度量的有效搜索空间，以处理不同的权重分布；以及基于模型的重构误差快速评价方法，在搜索过程中实现快速评估。我们采用非支配排序遗传算法III（NSGA-III）来优化剪枝度量和逐层稀疏比例。通过在LLaMA-1/2/3和Mistral模型（7B-70B）的各种基准测试中的广泛评估，我们展示了我们的适应性剪枝度量始终优于现有方法。此外，我们发现的逐层稀疏比例可以增强其他剪枝度量的有效性。该框架展示了强大的跨任务和跨模型泛化能力，提供了一种成本效益高的模型压缩解决方案。', 'title_zh': 'OPTISHEAR：通过进化优化实现大型语言模型高效和自适应剪枝的方向'}
{'arxiv_id': 'arXiv:2502.10916', 'title': 'Enhancing Conversational Agents from Open-Source Large Language Models with Illocutionary Force and Document-Based Knowledge Retrieval', 'authors': 'Godfrey Inyama', 'link': 'https://arxiv.org/abs/2502.10916', 'abstract': 'In this paper, we first present a novel way of computationally analysing and extracting illocutionary forces from dialogue using Bert-based Large Language Models, and demonstrate how these features impact the response of a conversational agent guided by a document-based knowledge bank demonstrated by a bespoke web conversational chat agent system developed. Our proposed illocutionary force extraction and classification technique is the first of its kind using the Argument Interchange Format (AIF) Dataset, showing an improved performance compared to two methods for carrying out similar tasks with a macro F1 of approximately 45%. When we evaluated the system based on 2 knowledge files, with 2 user queries each, across 5 open-source large language models (LLMs) using 10 standard metrics we found out that larger open-source models, such as Llama2:13b and Llama3-chatqa-latest, demonstrated an improved alignment when the user illocutionary force was included with their query, achieving higher QA and linguistic similarity scores. The smaller models on the other hand like Tinyllama:latest showed an increased perplexity and mixed performance, which explicitly indicated struggles in processing queries that explicitly included illocutionary forces. The results from the analysis highlight the potential of illocutionary force to enhance conversational depth while underscoring the need for model-specific optimizations to address increased computational costs and response times.', 'abstract_zh': '在本文中，我们首先提出了一种新的计算方法，利用基于BERT的大型语言模型来分析和提取对话中的语用力量，并展示了这些特征如何影响由基于文档的知识库指导的对话代理的响应。我们开发的自定义Web对话聊天代理系统证实了这种语用力量提取和分类技术。这种方法利用了Argument Interchange Format (AIF) 数据集，显示出与执行类似任务的两种方法相比，其宏F1值约为45%，有明显的改进。在使用10项标准指标评估系统时（该评估基于5个开源大型语言模型[LLMs]，每个知识文件包含2个用户查询），我们发现，当包含用户语用力量时，较大的开源模型，如Llama2:13b和Llama3-chatqa-latest，能更好地与用户查询对齐，从而获得更高的问答和语言相似度评分。相比之下，较小的模型，如Tinyllama:latest，则表现出较高的困惑度和混合性能，这表明它们在处理明确包含语用力量的查询时面临困难。分析结果强调了语用力量在增加对话深度方面的潜力，同时也突显了为满足增加的计算成本和响应时间需求而进行模型特定优化的必要性。', 'title_zh': '增强开源大规模语言模型的对话代理功能：通过意动力量和基于文档的知识检索'}
{'arxiv_id': 'arXiv:2502.10725', 'title': 'PropNet: a White-Box and Human-Like Network for Sentence Representation', 'authors': 'Fei Yang', 'link': 'https://arxiv.org/abs/2502.10725', 'abstract': 'Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.', 'abstract_zh': '基于Transformer的嵌入方法近年来在句子表示领域占据了主导地位。尽管它们在自然语言处理（NLP）任务中，如语义文本相似性（STS）任务中取得了显著的性能，但它们的黑盒性质和大数据驱动的训练方式也引起了关于偏见、信任和安全等问题的关注。尽管已经做出了许多努力来提高嵌入模型的可解释性，但这些问题并没有从根本上得到解决。为了实现内在的可解释性，我们提出了一种纯白盒且类人句子表示网络，PropNet。受到认知科学发现的启发，PropNet基于句子中的命题构建了一个分层网络。尽管实验表明，PropNet在STS任务中与当前最先进的（SOTA）嵌入模型相比存在显著差距，但案例研究揭示了改进的空间。此外，PropNet使我们能够分析和理解STS基准背后的人类认知过程。', 'title_zh': 'PropNet：一种白盒且类人类的句子表示网络'}
{'arxiv_id': 'arXiv:2502.10709', 'title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'authors': 'Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang', 'link': 'https://arxiv.org/abs/2502.10709', 'abstract': "As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: this https URL.", 'abstract_zh': '随着作为法官的大型语言模型（LLM-as-a-Judge）成为评估大型语言模型（LLMs）的一种新 paradigm，人们对其对齐、偏差和稳定性的担忧也日益增加。尽管已有大量研究关注对齐和偏差问题，但对于LLM评估器的稳定性研究仍然相对较少。本文通过对9种广泛使用的LLM评估器在2种不同评估场景下的广泛实验，探讨基于模型的LLM评估中的不确定性。我们指出，评估器的不确定性基于模型家族和规模而表现出不同的特征。通过细致的比较分析，我们发现，在推理或训练后采用特殊提示策略能够在一定程度上缓解评估不确定性。通过利用不确定性提高LLM在分布外（OOD）数据中的可靠性和检测能力，我们进一步使用人类注释的数据集对一个名为ConfiLM的不确定性感知评估器进行了微调，并在2024年奥运会手工设计的测试集上评估了ConfiLM的OOD评估能力。实验结果表明，在微调阶段引入不确定性作为额外信息，可以在很大程度上提高模型在OOD场景下的评估性能。代码和数据已发布于：this https URL。', 'title_zh': '大型语言模型评估中不确定性的实证分析'}
{'arxiv_id': 'arXiv:2502.10896', 'title': 'Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers of Cognition in People Living with Dementia', 'authors': 'Rohith Perumandla, Young-Ho Bae, Diego Izaguirre, Esther Hwang, Andrew Murphy, Long-Jing Hsu, Selma Sabanovic, Casey C. Bennett', 'link': 'https://arxiv.org/abs/2502.10896', 'abstract': "This study presents the development and testing of a conversational speech system designed for robots to detect speech biomarkers indicative of cognitive impairments in people living with dementia (PLwD). The system integrates a backend Python WebSocket server and a central core module with a large language model (LLM) fine-tuned for dementia to process user input and generate robotic conversation responses in real-time in less than 1.5 seconds. The frontend user interface, a Progressive Web App (PWA), displays information and biomarker score graphs on a smartphone in real-time to human users (PLwD, caregivers, clinicians). Six speech biomarkers based on the existing literature - Altered Grammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking, Slurred Pronunciation, and Prosody Changes - were developed for the robot conversation system using two datasets, one that included conversations of PLwD with a human clinician (DementiaBank dataset) and one that included conversations of PLwD with a robot (Indiana dataset). We also created a composite speech biomarker that combined all six individual biomarkers into a single score. The speech system's performance was first evaluated on the DementiaBank dataset showing moderate correlation with MMSE scores, with the composite biomarker score outperforming individual biomarkers. Analysis of the Indiana dataset revealed higher and more variable biomarker scores, suggesting potential differences due to study populations (e.g. severity of dementia) and the conversational scenario (human-robot conversations are different from human-human). The findings underscore the need for further research on the impact of conversational scenarios on speech biomarkers and the potential clinical applications of robotic speech systems.", 'abstract_zh': '本文介绍了为患有认知障碍的老年人（People Living with Dementia, PLwD）设计的对话语音系统的发展和测试。该系统集成了一个后端Python WebSocket服务器和一个中央核心模块，该模块使用大型语言模型（Large Language Model, LLM）进行微调，以处理用户输入并实时生成机器人对话响应，响应时间少于1.5秒。前端用户界面是一个渐进式网络应用（Progressive Web App, PWA），它实时显示智能手机上的信息和生物标志物评分图，供人类用户（PLwD、护理人员、临床医生）查看。根据现有文献，我们开发了六个基于对话的生物标志物——语法规则改变、语用障碍、命名困难、说话轮替中断、发音不清和平调变化——并使用了两个数据集：一个包含PLwD与人类临床医生的对话（DementiaBank数据集），另一个包含PLwD与机器人之间的对话（Indiana数据集）。我们还创建了一个综合生物标志物，将六个单个生物标志物整合成单一得分。首先，在DementiaBank数据集上评估了语音系统的性能，结果显示该综合生物标志物得分与蒙特利尔认知评估量表（MMSE）得分之间存在适度相关性，综合生物标志物得分优于单一生物标志物得分。对Indiana数据集的分析显示更高且更可变的生物标志物得分，表明可能存在由于研究人群（例如，认知障碍的严重程度）和对话场景（人机对话与人对人对话不同）差异而导致的潜在差异。研究结果强调了进一步研究对话场景对语音生物标志物影响以及机器人语音系统临床应用的重要性。', 'title_zh': '为痴呆患者开发对话式语音系统以检测认知相关语音生物标志物'}
{'arxiv_id': 'arXiv:2502.10886', 'title': 'MET-Bench: Multimodal Entity Tracking for Evaluating the Limitations of Vision-Language and Reasoning Models', 'authors': 'Vanya Cohen, Raymond Mooney', 'link': 'https://arxiv.org/abs/2502.10886', 'abstract': 'Entity tracking is a fundamental challenge in natural language understanding, requiring models to maintain coherent representations of entities. Previous work has benchmarked entity tracking performance in purely text-based tasks. We introduce MET-Bench, a multimodal entity tracking benchmark designed to evaluate the ability of vision-language models to track entity states across modalities. Using two structured domains, Chess and the Shell Game, we assess how effectively current models integrate textual and image-based state updates. Our findings reveal a significant performance gap between text-based and image-based tracking and that this performance gap stems from deficits in visual reasoning rather than perception. We further show that explicit text-based reasoning strategies improve performance, yet substantial limitations remain, especially in long-horizon multimodal scenarios. Our results highlight the need for improved multimodal representations and reasoning techniques to bridge the gap between textual and visual entity tracking.', 'abstract_zh': '实体跟踪是自然语言理解中的一个基本挑战，要求模型保持实体的一致表示。以往的工作已经在纯文本任务中对实体跟踪性能进行了基准测试。我们提出了 MET-Bench，一个用于评估视觉-语言模型跨模态跟踪实体状态能力的多模态实体跟踪基准。通过使用两个结构化领域，国际象棋和猜豆游戏，我们评估了当前模型在文本和图像状态更新方面的整合能力。我们的发现揭示了基于文本和基于图像跟踪之间的显著性能差距，这一差距源自于视觉推理缺陷而不是感知缺陷。我们进一步证明了显式的基于文本的推理策略可以改进性能，但仍然存在重大局限性，特别是在长时程多模态场景中。我们的结果强调了改善多模态表示和推理技术的需求，以弥合文本和视觉实体跟踪之间的差距。', 'title_zh': 'MET-Bench: 多模态实体跟踪用于评估视觉-语言和推理模型的局限性'}
{'arxiv_id': 'arXiv:2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'authors': 'Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen', 'link': 'https://arxiv.org/abs/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: this https URL, dedicated to documenting research in the field of specialized LLM.', 'abstract_zh': '大型语言模型（Large Language Models, LLMs）在自然语言理解、文本摘要和机器翻译等多种任务中表现出显著的成功。然而，它们的一般化特性往往限制了它们在需要专门知识的特定领域应用中的效果，例如医疗保健、化学或法律分析。为了解决这一问题，研究人员探索了多种方法来通过整合领域特定知识来提升LLMs。在这篇综述中，我们提供了一个全面的概述，将这些方法分为四类关键方法：动态知识注入、静态知识嵌入、模块式适配器和提示优化。每种方法提供了独特的机制来赋予LLMs领域专业知识，并在灵活性、可扩展性和效率之间找到平衡。我们讨论了这些方法如何使LLMs能够应对专门任务，比较了它们的优势和劣势，评估了特定领域LLMs与通用LLMs的性能，并指出了这一新兴领域中的挑战和机遇。对于那些希望更深入研究此领域的读者，我们还总结了常用的实验数据集和基准。为了使研究人员随时了解最新研究，我们维护了一个开源平台：https://thisurl.com，专门用于记录专门领域LLMs研究领域的进展。', 'title_zh': '将领域特定知识注入大规模语言模型：一项全面综述'}
{'arxiv_id': 'arXiv:2502.10881', 'title': 'CiteCheck: Towards Accurate Citation Faithfulness Detection', 'authors': 'Ziyao Xu, Shaohang Wei, Zhuoheng Han, Jing Jin, Zhe Yang, Xiaoguang Li, Haochen Tan, Zhijiang Guo, Houfeng Wang', 'link': 'https://arxiv.org/abs/2502.10881', 'abstract': 'Citation faithfulness detection is critical for enhancing retrieval-augmented generation (RAG) systems, yet large-scale Chinese datasets for this task are scarce. Existing methods face prohibitive costs due to the need for manually annotated negative samples. To address this, we introduce the first large-scale Chinese dataset CiteCheck for citation faithfulness detection, constructed via a cost-effective approach using two-stage manual annotation. This method balances positive and negative samples while significantly reducing annotation expenses. CiteCheck comprises training and test splits. Experiments demonstrate that: (1) the test samples are highly challenging, with even state-of-the-art LLMs failing to achieve high accuracy; and (2) training data augmented with LLM-generated negative samples enables smaller models to attain strong performance using parameter-efficient fine-tuning. CiteCheck provides a robust foundation for advancing citation faithfulness detection in Chinese RAG systems. The dataset is publicly available to facilitate research.', 'abstract_zh': '引用忠实性检测对于提高检索增强生成（RAG）系统至关重要，但针对这一任务的大规模中文数据集相当稀缺。现有方法因需要手动标注负面样本而面临高昂的成本。为解决这一问题，我们首次引入了用于引用忠实性检测的大规模中文数据集CiteCheck，通过一种成本效益高的方法，采用两阶段手动标注构建。该方法平衡了正样本和负样本，显著降低了标注成本。CiteCheck 包含训练集和测试集。实验表明：(1) 测试样本具有高度挑战性，即使是最先进的语言模型也无法达到较高的准确率；(2) 使用包含语言模型生成的负面样本的训练数据，能够在参数高效微调下使小型模型达到出色的性能。CiteCheck 为推进中文 RAG 系统中的引用忠实性检测提供了坚实的基础。该数据集已公开，以促进相关研究。', 'title_zh': 'CiteCheck：迈向准确引文忠实性检测'}
{'arxiv_id': 'arXiv:2502.10871', 'title': 'The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis', 'authors': 'Ge Lei, Samuel J. Cooper', 'link': 'https://arxiv.org/abs/2502.10871', 'abstract': 'This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.', 'abstract_zh': '本研究探讨了大型语言模型（LLMs）如何在变换器层中表示和回忆多关联属性。我们发现中间层通过在重叠的空间中叠加相关属性来编码事实知识，并且即使未明确提示属性，也能有效地回忆属性。相比之下，后期层则逐渐细化语言模式并分离属性表示，优化特定任务的输出，同时适当地限制属性回忆。我们识别了多种编码模式，其中包括首次观察到与元素周期表相关的信息时出现的三维螺旋结构。我们的发现揭示了层间属性表示的动态转变，有助于机械可解释性的提升，并为理解LLMs处理复杂、相互关联的知识提供了见解。', 'title_zh': '大型语言模型中交织结构知识的表示与回忆：一种几何与分层分析'}
{'arxiv_id': 'arXiv:2502.10868', 'title': 'NitiBench: A Comprehensive Studies of LLM Frameworks Capabilities for Thai Legal Question Answering', 'authors': 'Pawitsapak Akarajaradwong, Pirat Pothavorn, Chompakorn Chaksangchaichot, Panuthep Tasawong, Thitiwat Nopparatbundit, Sarana Nutanong', 'link': 'https://arxiv.org/abs/2502.10868', 'abstract': 'The application of large language models (LLMs) in the legal domain holds significant potential for information retrieval and question answering, yet Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks and the complexity of Thai legal structures. This paper introduces NitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering general Thai financial law, and the NitiBench-Tax, which includes real-world tax law cases requiring advanced legal reasoning. We evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches to address three key research questions: the impact of domain-specific components like section-based chunking and cross-referencing, the comparative performance of different retrievers and LLMs, and the viability of long-context LLMs as an alternative to RAG. Our results show that section-based chunking significantly improves retrieval and end-to-end performance, current retrievers struggle with complex queries, and long-context LLMs still underperform RAG-based systems in Thai legal QA. To support fair evaluation, we propose tailored multi-label retrieval metrics and the use of an LLM-as-judge for coverage and contradiction detection method. These findings highlight the limitations of current Thai legal NLP solutions and provide a foundation for future research in the field. We also open-sourced our codes and dataset to available publicly.', 'abstract_zh': '大型语言模型（LLMs）在法律领域的应用具有重要的潜力，特别是在信息检索和问答方面。然而，由于缺乏标准化的评估基准和泰国法律结构的复杂性，泰国的法律问答系统面临着挑战。本文介绍了NitiBench基准，该基准包含两个数据集：NitiBench-CCL，涵盖一般泰国金融法，以及包括涉及高级法律推理的实际税务案例的NitiBench-Tax。我们评估了检索增强生成（RAG）和基于长语境的LLM方法，以解决三个关键的研究问题：领域特定组件（如基于节的片段化和互引）的影响、不同检索器和LLM的性能比较以及长语境LLM作为RAG替代方案的可行性。我们的结果显示，基于节的片段化显著提高了检索性能和端到端性能，当前的检索器在处理复杂查询时遇到了困难，而在泰国法律问答方面，长语境LLM的表现仍然不如基于RAG的系统。为了实现公平的评估，我们提出了定制的多标签检索度量标准，并提出了使用LLM作为法官的方法来检测覆盖率和矛盾。这些发现突显了现有泰国法律NLP解决方案的局限性，并为该领域未来的研究奠定了基础。我们还开源了我们的代码和数据集供公众访问。', 'title_zh': 'NitiBench：Thai 法律问答能力的综合评估框架研究\n\n注释：在这句话中，“LLM Frameworks Capabilities”被译为“法律问答能力”，其中“LLM”通常指的是大型语言模型（Large Language Model），但在这个上下文中，它更具体地被解释为“法律问答能力”。此外，“NitiBench”是一个特定的研究框架名称，翻译时保留其原名以保持专有名词的一致性。'}
{'arxiv_id': 'arXiv:2502.10699', 'title': 'Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration', 'authors': 'George Applegarth, Christian Weatherstone, Maximilian Hollingsworth, Henry Middlebrook, Marcus Irvin', 'link': 'https://arxiv.org/abs/2502.10699', 'abstract': 'Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.', 'abstract_zh': '上下文记忆整合仍然是语言模型发展中的一大挑战，尤其是在需要长时间保持连贯性的任务中。传统的范式，如自注意力机制和记忆增强架构，往往侧重于短时依赖，导致了长距离上下文理解的碎片化和不一致性。受生物学神经系统中突触可塑性原理的启发，引入了一种新型机制——突触共振，该机制在训练和推理过程中动态强化相关记忆路径。与静态的记忆表示不同，该机制根据上下文的相关性连续调整突触权重矩阵，从而在不增加过多计算开销的情况下提高信息保留能力。在开源语言模型上的评估表明，这种机制可以降低困惑度、增强上下文连贯性，并提高对输入噪声的鲁棒性，体现了从强化驱动的记忆调控中获得的有效性。与基线模型的比较分析进一步显示，提出的方法在记忆保留效率方面表现更优，同时保持了计算上的可行性。这些架构修改能够无缝集成到现有的基于Transformer的框架中，确保稳定收敛和高效推理，而不牺牲可扩展性。通过提高长期上下文一致性的应用场景，如对话系统和文档摘要，可以从这种方法中获益。实验证据表明，动态强化的记忆路径为常规记忆机制提供了一种有前途的替代方案，解决了长期序列建模的长期局限性。', 'title_zh': '在大型语言模型中探索突触共振：一种新的上下文记忆整合方法'}
{'arxiv_id': 'arXiv:2502.10857', 'title': 'Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation', 'authors': 'Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu', 'link': 'https://arxiv.org/abs/2502.10857', 'abstract': 'Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.', 'abstract_zh': '近年来，随着大型语言模型（LLMs）调用工具能力的发展，这些模型通过EDA脚本与EDA工具API进行交互，展现了在自动电子设计自动化（EDA）流程中的巨大潜力。然而，考虑到对EDA工具理解的局限性，当不同的EDA工具在不同平台上存在多种接口时，LLMs在实际应用中面临着挑战。此外，EDA流程自动化往往涉及复杂的、多步骤的工具调用过程，增加了中间步骤出错的可能性。任何错误都可能导致EDA流程自动化失败。为解决这些问题，我们提出了一种多智能体协作系统——EDAid，该系统中多个持有不同想法的智能体朝着共同目标进行协作，以确保可靠的EDA流程自动化。具体而言，每个智能体由专门为EDA流程自动化微调的ChipLlama模型控制。我们的实验表明，ChipLlama模型在性能上达到了行业领先水平，并验证了EDAid在复杂EDA流程自动化中的有效性。与单智能体系统相比，EDAid表现出更优越的表现。', 'title_zh': '朝着共同目标的分歧思维：基于大语言模型的多agent协作系统在电子设计自动化中的应用'}
{'arxiv_id': 'arXiv:2502.10660', 'title': 'User Profile with Large Language Models: Construction, Updating, and Benchmarking', 'authors': 'Nusrat Jahan Prottasha, Md Kowsher, Hafijur Raman, Israt Jahan Anny, Prakash Bhat, Ivan Garibay, Ozlem Garibay', 'link': 'https://arxiv.org/abs/2502.10660', 'abstract': 'User profile modeling plays a key role in personalized systems, as it requires building accurate profiles and updating them with new information. In this paper, we present two high-quality open-source user profile datasets: one for profile construction and another for profile updating. These datasets offer a strong basis for evaluating user profile modeling techniques in dynamic settings. We also show a methodology that uses large language models (LLMs) to tackle both profile construction and updating. Our method uses a probabilistic framework to predict user profiles from input text, allowing for precise and context-aware profile generation. Our experiments demonstrate that models like Mistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the precision and recall of the generated profiles, and high evaluation scores confirm the effectiveness of our approach.', 'abstract_zh': '用户画像建模在个性化系统中发挥着关键作用，它要求构建准确的用户画像并根据新信息进行更新。本文介绍了两个高质量的开源用户画像数据集：一个用于用户画像构建，另一个用于用户画像更新。这些数据集为在动态环境中评估用户画像建模技术提供了坚实的基础。我们还展示了一种方法，该方法利用大规模语言模型（LLMs）同时解决用户画像构建和更新问题。我们的方法通过概率框架从输入文本中预测用户画像，从而实现精确且上下文感知的用户画像生成。实验结果表明，如Mistral-7b和Llama2-7b之类的模型在这两个任务上表现出色。大规模语言模型提高了生成用户画像的精确性和召回率，而高评价分数证实了我们方法的有效性。', 'title_zh': '使用大型语言模型构建、更新与基准测试用户画像'}
{'arxiv_id': 'arXiv:2502.10855', 'title': 'Towards Effective Extraction and Evaluation of Factual Claims', 'authors': 'Dasha Metropolitansky, Jonathan Larson', 'link': 'https://arxiv.org/abs/2502.10855', 'abstract': 'A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.', 'abstract_zh': '检查大型语言模型（LLMs）生成的长文本内容时，常用的一种策略是提取可以独立验证的简单声明。由于不准确或不完整的声明会损害事实检查的结果，因此确保声明的质量至关重要。然而，缺乏标准化的评估框架阻碍了对声明提取方法的评估和比较。为了填补这一空白，我们提出了一种在事实检查背景下评估声明提取的方法框架，并提供了自动化、可扩展且可复制的应用此框架的方法，包括衡量范围和脱域的新方法。我们还引入了基于大型语言模型的声明提取方法——Claimify，并证明在我们的评估框架下，该方法优于现有方法。Claimify 的一个关键特性是其能够处理歧义，并且仅在对源文本的正确解释具有高度信心时才提取声明。', 'title_zh': '面向有效事实断言提取与评估的研究'}
{'arxiv_id': 'arXiv:2502.10852', 'title': 'Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages', 'authors': 'Zeli Su, Ziyin Zhang, Guixian Xu, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong', 'link': 'https://arxiv.org/abs/2502.10852', 'abstract': 'While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models.', 'abstract_zh': '尽管像XLM-R这样的多语言语言模型在自然语言处理（NLP）中促进了多语言能力，但在极度低资源语言上的表现仍然不佳。现代大型语言模型（LLM）如LLaMA和Qwen支持的语言数量远少于XLM-R，这使得许多语言在文本生成模型中无从选择，进一步加剧了这一问题。为解决这一挑战，我们提出了一种新的框架，用于适应极度低资源语言的多语言编码器进行文本生成。通过在编码器和解码器之间重用权重，我们的框架使模型能够利用编码器学习到的语义空间，从而在低资源语言中实现高效的 learning 和有效的泛化。我们将这一框架应用于四种中国少数民族语言，并展示了XLM-SWCM在各种下游任务上的优越性能，即使与更大规模的模型相比也是如此。', 'title_zh': '多语言编码器比你想象的更为强大：面向极度资源稀缺语言的共享权重预训练'}
{'arxiv_id': 'arXiv:2502.10645', 'title': 'BabyLM Turns 3: Call for papers for the 2025 BabyLM workshop', 'authors': 'Lucas Charpentier, Leshem Choshen, Ryan Cotterell, Mustafa Omer Gul, Michael Hu, Jaap Jumelet, Tal Linzen, Jing Liu, Aaron Mueller, Candace Ross, Raj Sanjay Shah, Alex Warstadt, Ethan Wilcox, Adina Williams', 'link': 'https://arxiv.org/abs/2502.10645', 'abstract': 'BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 3rd BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: INTERACTION. This new track encourages interactive behavior, learning from a teacher, and adapting the teaching material to the student. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.', 'abstract_zh': 'BabyLM 致力于打破认知建模与语言建模之间的界限。我们邀请提交研讨会论文，并诚邀研究人员参与第三届 BabyLM 竞赛。如同往年一样，我们继续号召参赛者参加通用赛道下的数据高效预训练挑战。今年，我们还新增了一个 INTERACTION 赛道。该赛道鼓励交互行为、从教师处学习以及根据学生情况进行教学材料的调整。此外，我们还呼吁在任何相关领域提交非竞赛论文，这些领域包括训练效率、认知上合理的研究、模型评估方法的改进等。', 'title_zh': 'BabyLM 三岁了：2025 BabyLM 工作坊征稿启事\n\n解析：此标题为学术会议征稿启事，"Call for papers" 翻译为 “征稿启事”，“BabyLM workshop”为具体工作坊名称，保持不变更为准确。"BabyLM Turns 3" 翻译为“BabyLM 三岁了”，符合学术和正式场合的翻译习惯。'}
{'arxiv_id': 'arXiv:2502.10641', 'title': 'Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility', 'authors': 'Zhaoqian Xue, Guanhong Liu, Kai Wei, Chong Zhang, Qingcheng Zeng, Songhua Hu, Wenyue Hua, Lizhou Fan, Yongfeng Zhang, Lingyao Li', 'link': 'https://arxiv.org/abs/2502.10641', 'abstract': 'Access to health resources is a critical determinant of public well-being and societal resilience, particularly during public health crises when demand for medical services and preventive care surges. However, disparities in accessibility persist across demographic and geographic groups, raising concerns about equity. Traditional survey methods often fall short due to limitations in coverage, cost, and timeliness. This study leverages crowdsourced data from Google Maps reviews, applying advanced natural language processing techniques, specifically ModernBERT, to extract insights on public perceptions of health resource accessibility in the United States during the COVID-19 pandemic. Additionally, we employ Partial Least Squares regression to examine the relationship between accessibility perceptions and key socioeconomic and demographic factors including political affiliation, racial composition, and educational attainment. Our findings reveal that public perceptions of health resource accessibility varied significantly across the U.S., with disparities peaking during the pandemic and slightly easing post-crisis. Political affiliation, racial demographics, and education levels emerged as key factors shaping these perceptions. These findings underscore the need for targeted interventions and policy measures to address inequities, fostering a more inclusive healthcare infrastructure that can better withstand future public health challenges.', 'abstract_zh': '健康资源的获取是公共福祉和社会韧性的重要决定因素，尤其是在公共卫生危机期间，医疗服务和预防保健的需求激增。然而，不同人口和社会地理群体之间在获取健康资源方面的差距仍然存在，这引发了关于公平性的关切。传统调查方法往往由于覆盖范围、成本和时效性的限制而显得不足。本研究借助来源于Google Maps评论的众包数据，应用先进的自然语言处理技术（特别是ModernBERT），以提取美国在COVID-19疫情期间公众对健康资源可及性的感知见解。此外，我们还采用偏最小二乘回归分析，探讨可及性感知与关键的社会经济和人口统计数据——包括政治倾向、种族构成和教育程度——之间的关系。研究结果表明，在美国，公众对健康资源可及性的感知存在显著差异，这些差异在疫情期间达到顶峰，并在危机后略有缓解。政治倾向、种族构成和教育水平成为了影响这些感知的关键因素。这些发现凸显了针对不公平性采取有针对性干预和政策措施的必要性，以促进更具包容性的医疗基础设施，使其能够更好地应对未来公共卫生挑战。', 'title_zh': '迈向公平获取：利用众源评价考察公众对健康资源可及性的认知'}
{'arxiv_id': 'arXiv:2502.10835', 'title': 'Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models', 'authors': 'Zeping Yu, Yonatan Belinkov, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.10835', 'abstract': 'We investigate how large language models perform latent multi-hop reasoning in prompts like "Wolfgang Amadeus Mozart\'s mother\'s spouse is". To analyze this process, we introduce logit flow, an interpretability method that traces how logits propagate across layers and positions toward the final prediction. Using logit flow, we identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction. Extending this analysis to multi-hop reasoning, we find that failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy. To address this, we propose back attention, a novel mechanism that enables lower layers to leverage higher-layer hidden states from different positions during attention computation. With back attention, a 1-layer transformer achieves the performance of a 2-layer transformer. Applied to four LLMs, back attention improves accuracy on five reasoning datasets, demonstrating its effectiveness in enhancing latent multi-hop reasoning ability.', 'abstract_zh': '我们调查了大型语言模型在提示“Wolfgang Amadeus Mozart 的母亲的配偶是”这类问题中如何进行潜在的多跳推理。为了分析这一过程，我们引入了logit flow，一种可解释性方法，用于追踪logits如何在各层和位置间传播以产生最终预测。通过logit flow，我们识别出了单跳知识预测过程中的四个不同阶段：(A) 实体主语增强、(B) 实体属性抽取、(C) 关系主语增强、和(D) 关系属性抽取。将这一分析扩展到多跳推理中，我们发现失败通常源自于关系属性抽取阶段，其中冲突的logits降低了预测准确性。为了解决这一问题，我们提出了一种新颖的机制——反向注意（back attention），它使较低层能够在注意力计算过程中利用来自不同位置的较高层隐藏状态。通过反向注意机制，单层Transformer的性能达到了双层Transformer的水平。将反向注意应用于四种大型语言模型（LLMs）后，在五个推理数据集上提高了准确性，这表明其在增强潜在的多跳推理能力方面具有有效性。', 'title_zh': '回溯注意：理解并增强大型语言模型中的多跳推理'}
{'arxiv_id': 'arXiv:2502.10760', 'title': 'Why is prompting hard? Understanding prompts on binary sequence predictors', 'authors': 'Li Kevin Wenliang, Anian Ruoss, Jordi Grau-Moya, Marcus Hutter, Tim Genewein', 'link': 'https://arxiv.org/abs/2502.10760', 'abstract': 'Large language models (LLMs) can be prompted to do many tasks, but finding good prompts is not always easy, nor is understanding some performant prompts. We explore these issues by viewing prompting as conditioning a near-optimal sequence predictor (LLM) pretrained on diverse data sources. Through numerous prompt search experiments, we show that the unintuitive patterns in optimal prompts can be better understood given the pretraining distribution, which is often unavailable in practice. Moreover, even using exhaustive search, reliably identifying optimal prompts from practical neural predictors can be difficult. Further, we demonstrate that common prompting methods, such as using intuitive prompts or samples from the targeted task, are in fact suboptimal. Thus, this work takes an initial step towards understanding the difficulties in finding and understanding optimal prompts from a statistical and empirical perspective.', 'abstract_zh': '大型语言模型（LLMs）可以通过提示执行多种任务，但找到好的提示并非总是容易的，而且理解一些高性能的提示也不总是简单的。我们通过将提示视为对预训练于多样数据源的近最优序列预测器进行条件化的方式，来研究这些问题。通过大量的提示搜索实验，我们证明，在给定预训练分布的情况下，最优提示中的直观模式可以更好地被理解，而在实践中这种预训练分布往往不可用。此外，即使使用穷尽搜索，从实用的神经预测器中可靠地识别出最优提示也是困难的。进一步，我们证明，常见的提示方法，如使用直观提示或目标任务的样本，实际上是次优的。因此，这项工作从统计学和经验的角度出发，迈出了理解找到和理解最优提示困难性的第一步。', 'title_zh': '为什么提示如此困难？理解对二元序列预测器的提示'}
{'arxiv_id': 'arXiv:2502.10749', 'title': 'LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging', 'authors': 'Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan', 'link': 'https://arxiv.org/abs/2502.10749', 'abstract': 'While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.', 'abstract_zh': '当前大多数方法依赖于进一步的训练技术，如微调或强化学习，以增强模型能力，而模型合并则因其无需额外训练即可提升模型性能的能力而脱颖而出。本文提出了一种基于低秩估计任务向量的统一框架 \\textsc{LoRE-Merging}，而不需访问基模型。我们的方法受到观察的启发，即微调模型的任务向量往往只包含少量主导奇异值，这使得低秩估计不易受到干扰。我们通过将合并问题形式化为优化问题来实现该方法。广泛的经验实验表明，我们的框架在减轻干扰并保留任务特定信息方面具有有效性，从而在模型合并技术中取得了最先进的性能。', 'title_zh': 'LoRE-合并：探索大规模语言模型合并中的低秩估计方法'}
{'arxiv_id': 'arXiv:2502.10634', 'title': 'Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"', 'authors': 'Hao Sun, Chenming Tang, Gengyang Li, Yunfang Wu', 'link': 'https://arxiv.org/abs/2502.10634', 'abstract': 'By simply incorporating demonstrations into the context, in-context learning (ICL) enables large language models (LLMs) to yield awesome performance on many tasks. In this paper, we focus on passage-level long-context ICL for generation tasks and find that LLMs cannot learn the intrinsic relationships between the demonstration passage and the generation output. We conduct experiments with different LLMs on two typical generation tasks including single-document QA and distractor generation, demonstrating that even a completely meaningless demonstration passage with 1/4 length achieves much better performance than the original full passage. Analysis via attention score reveals that LLMs pay little attention to passages compared to other components in prompt and little attention flows from the passage to other parts of the demonstration, which further confirms our finding. Additionally, experiments on context compression indicate that compression approaches proven effective on other long-context tasks are not suitable for passage-level ICL, since simply using shorter meaningless demonstration passages has achieved competitive performance.', 'abstract_zh': '通过将演示直接融入上下文，上下文学习（Contextual Learning with Demonstrations, ICL）使大规模语言模型（LLMs）在许多任务上取得了出色的表现。本文重点探讨了用于生成任务的段落级长上下文ICL，并发现LLMs无法学习演示段落与生成输出之间的内在关系。我们分别在两个典型的生成任务——单文档问答（单文档QA）和干扰生成上，使用不同类型的LLMs进行实验，结果显示，即使是一个完全无意义的演示段落且长度仅为原始段落的1/4也比原版完整段落表现更好。通过注意力得分分析表明，LLMs对段落的关注度要低于其他提示组件的关注度，且从段落到演示其他部分的注意力流较低，这进一步证实了我们的发现。此外，上下文压缩实验显示，其他长上下文任务中证明有效的压缩方法不适用于段落级ICL，因为使用较短的无意义演示段落已经达到了可竞争的性能。', 'title_zh': '迷失在篇章中：段落级的即情境学习并不一定需要一个“篇章”'}
{'arxiv_id': 'arXiv:2502.10632', 'title': 'Code-Mixed Telugu-English Hate Speech Detection', 'authors': 'Santhosh Kakarla, Gautama Shastry Bulusu Venkata', 'link': 'https://arxiv.org/abs/2502.10632', 'abstract': 'Hate speech detection in low-resource languages like Telugu is a growing challenge in NLP. This study investigates transformer-based models, including TeluguHateBERT, HateBERT, DeBERTa, Muril, IndicBERT, Roberta, and Hindi-Abusive-MuRIL, for classifying hate speech in Telugu. We fine-tune these models using Low-Rank Adaptation (LoRA) to optimize efficiency and performance. Additionally, we explore a multilingual approach by translating Telugu text into English using Google Translate to assess its impact on classification accuracy.\nOur experiments reveal that most models show improved performance after translation, with DeBERTa and Hindi-Abusive-MuRIL achieving higher accuracy and F1 scores compared to training directly on Telugu text. Notably, Hindi-Abusive-MuRIL outperforms all other models in both the original Telugu dataset and the translated dataset, demonstrating its robustness across different linguistic settings. This suggests that translation enables models to leverage richer linguistic features available in English, leading to improved classification performance. The results indicate that multilingual processing can be an effective approach for hate speech detection in low-resource languages. These findings demonstrate that transformer models, when fine-tuned appropriately, can significantly improve hate speech detection in Telugu, paving the way for more robust multilingual NLP applications.', 'abstract_zh': '低资源语言（如泰卢固语）中的仇恨言论检测是自然语言处理（NLP）中的一个日益增长的挑战。本研究调查了基于变换器的模型，包括TeluguHateBERT、HateBERT、DeBERTa、Muril、IndicBERT、RoBERTa和Hindi-Abusive-MuRIL，以识别泰卢固语中的仇恨言论。我们使用低秩适应（LoRA）对这些模型进行微调，以优化效率和性能。此外，我们通过使用Google Translate将泰卢固文本翻译成英语，探索多语言方法，评估其对分类准确度的影响。\n\n实验结果表明，在翻译后，大多数模型的性能有所提高，其中DeBERTa和Hindi-Abusive-MuRIL在准确率和F1分数方面优于直接在泰卢固文本上进行训练的结果。值得注意的是，Hindi-Abusive-MuRIL在原始泰卢固数据集和翻译数据集上都表现出最佳性能，表明其在不同语言环境下的鲁棒性。这表明，翻译可以使模型充分利用英语中可供利用的丰富语言特征，从而提高分类性能。研究结果表明，多语言处理可以成为低资源语言中仇恨言论检测的有效方法。这些发现表明，当适当微调时，变换器模型可以显著提高泰卢固语中仇恨言论的检测性能，为更 robust的多语言NLP应用开辟了道路。', 'title_zh': '混合编码泰卢固-英语仇恨言论检测'}
{'arxiv_id': 'arXiv:2502.10743', 'title': '1bit-Merging: Dynamic Quantized Merging for Large Language Models', 'authors': 'Shuqi Liu, Han Wu, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Linqi Song', 'link': 'https://arxiv.org/abs/2502.10743', 'abstract': 'Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.', 'abstract_zh': '近期大型语言模型的发展催生了在特定领域表现出色的专门模型，这需要高效模型合并技术。虽然传统的合并方法将参数合并为一个静态模型，但这些方法常常会牺牲任务特定的性能。然而，任务特定的路由方法能够保持准确性，但会引入大量的存储开销。我们提出了一种名为 \\texttt{1bit}-Merging 的新框架，该框架将任务特定的路由与1位量化任务向量相结合，以平衡性能和存储效率。我们的方法利用了不同任务特定模型的知识分布于不同层的事实——聊天模型主要在注意力层，数学/代码模型主要在MLP层，从而实现针对性的压缩策略。通过在LLaMA2和Mistral模型家族中进行涉及聊天、数学推理和代码生成任务的广泛实验，我们证明了 \\texttt{1bit}-Merging 在存储要求显著降低的情况下，能达到或超越现有方法的性能。我们的框架为结合专门模型提供了一个实用的解决方案，同时保持其各自的优点，并解决了现有方法的存储挑战。', 'title_zh': '1位合并：面向大型语言模型的动态量化合并'}
{'arxiv_id': 'arXiv:2502.10615', 'title': 'Retrieval-augmented Encoders for Extreme Multi-label Text Classification', 'authors': 'Yau-Shian Wang, Wei-Cheng Chang, Jyun-Yu Jiang, Jiong Zhang, Hsiang-Fu Yu, S. V. N. Vishwanathan', 'link': 'https://arxiv.org/abs/2502.10615', 'abstract': 'Extreme multi-label classification (XMC) seeks to find relevant labels from an extremely large label collection for a given text input. To tackle such a vast label space, current state-of-the-art methods fall into two categories. The one-versus-all (OVA) method uses learnable label embeddings for each label, excelling at memorization (i.e., capturing detailed training signals for accurate head label prediction). In contrast, the dual-encoder (DE) model maps input and label text into a shared embedding space for better generalization (i.e., the capability of predicting tail labels with limited training data), but may fall short at memorization. To achieve generalization and memorization, existing XMC methods often combine DE and OVA models, which involves complex training pipelines. Inspired by the success of retrieval-augmented language models, we propose the Retrieval-augmented Encoders for XMC (RAEXMC), a novel framework that equips a DE model with retrieval-augmented capability for efficient memorization without additional trainable parameter. During training, RAEXMC is optimized by the contrastive loss over a knowledge memory that consists of both input instances and labels. During inference, given a test input, RAEXMC retrieves the top-$K$ keys from the knowledge memory, and aggregates the corresponding values as the prediction scores. We showcase the effectiveness and efficiency of RAEXMC on four public LF-XMC benchmarks. RAEXMC not only advances the state-of-the-art (SOTA) DE method DEXML, but also achieves more than 10x speedup on the largest LF-AmazonTitles-1.3M dataset under the same 8 A100 GPUs training environments.', 'abstract_zh': '极端多标签分类（XMC）旨在从给定文本输入的极大规模标签集中找出相关标签。为了应对如此庞大的标签空间，当前的最先进的方法可以分为两类。一种是“一对一”（One-vs-All, OVA）方法，它为每个标签使用可学习的标签嵌入，擅长记忆（即捕捉精确头部标签预测所需的详细训练信号）。相比之下，双编码器（Dual-Encoder, DE）模型将输入和标签文本映射到共享的嵌入空间，以提高泛化能力（即在有限的训练数据下预测尾部标签的能力），但可能在记忆方面有所欠缺。为了实现泛化和记忆，现有的XMC方法常常结合使用DE模型和OVA模型，这涉及到复杂的训练管道。受检索增强语言模型成功的启发，我们提出了一种新型框架——检索增强编码器用于XMC（RAEXMC），该框架赋予DE模型检索增强的能力，可以在不增加额外可训练参数的情况下实现高效的记忆。在训练过程中，RAEXMC通过知识记忆中的对比损失进行优化，知识记忆既包括输入实例又包括标签。在推理过程中，给定一个测试输入，RAEXMC从知识记忆中检索前K个键，并聚合相应的值作为预测得分。我们在四个公开的LF-XMC基准数据集上展示了RAEXMC的有效性和效率。RAEXMC不仅改进了当前最先进的DE方法DEXML，还在相同8个A100 GPU的训练环境下，对LF-AmazonTitles-1.3M数据集实现了超过10倍的速度提升。', 'title_zh': '用于极端多标签文本分类的检索增强编码器'}
{'arxiv_id': 'arXiv:2502.10596', 'title': 'Post-training an LLM for RAG? Train on Self-Generated Demonstrations', 'authors': 'Matthew Finlayson, Ilia Kulikov, Daneil M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu', 'link': 'https://arxiv.org/abs/2502.10596', 'abstract': 'Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering "Who won the latest World Cup?" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents -- a technique known as retrieval augmented generation (RAG) -- mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.', 'abstract_zh': '大规模语言模型（LLMs）往往在需要大量知识的自然语言处理（NLP）任务中表现不佳，例如回答“最近的世界杯是谁赢的？”这类问题，因为它们在训练过程中学习的知识可能不足或过时。通过检索文档来生成内容——一种称为检索增强生成（RAG）的技术——可以通过让模型利用上下文信息来弥补这些不足。实践者可以通过在增强检索的基础上进行微调来改进LLM的RAG性能，但必须注意这可能会导致模型出现诸如自说自话等不良行为。我们认为这种性能下降的原因在于训练数据很可能对模型而言是分布之外的，并且可能存在质量问题，如检索和目标响应之间的不一致（因为检索通常是在事后添加的）。我们提出了一种使用自我生成的示例训练RAG增强的LLMs的方法，从而避免使用分布之外的文本进行训练，并将检索集成到LLM的响应中。我们在知识密集型问题回答（QA）任务上评估了该方法，并展示了该方法能够教会LLMs正确处理上下文检索并回避它可能回答错误的问题。与传统的RA-IT方法相比，我们的方法在非RAG设置中防止了模型性能下降，并且在问题回答方面表现出更优的性能。', 'title_zh': '对LLM进行后训练以用于检索增强生成？基于自动生成的示例进行训练'}
{'arxiv_id': 'arXiv:2502.10582', 'title': 'Named entity recognition for Serbian legal documents: Design, methodology and dataset development', 'authors': 'Vladimir Kalušev, Branko Brkljač', 'link': 'https://arxiv.org/abs/2502.10582', 'abstract': 'Recent advancements in the field of natural language processing (NLP) and especially large language models (LLMs) and their numerous applications have brought research attention to design of different document processing tools and enhancements in the process of document archiving, search and retrieval. Domain of official, legal documents is especially interesting due to vast amount of data generated on the daily basis, as well as the significant community of interested practitioners (lawyers, law offices, administrative workers, state institutions and citizens). Providing efficient ways for automation of everyday work involving legal documents is therefore expected to have significant impact in different fields. In this work we present one LLM based solution for Named Entity Recognition (NER) in the case of legal documents written in Serbian language. It leverages on the pre-trained bidirectional encoder representations from transformers (BERT), which had been carefully adapted to the specific task of identifying and classifying specific data points from textual content. Besides novel dataset development for Serbian language (involving public court rulings), presented system design and applied methodology, the paper also discusses achieved performance metrics and their implications for objective assessment of the proposed solution. Performed cross-validation tests on the created manually labeled dataset with mean $F_1$ score of 0.96 and additional results on the examples of intentionally modified text inputs confirm applicability of the proposed system design and robustness of the developed NER solution.', 'abstract_zh': '近年来，自然语言处理（NLP）领域的最新进展，特别是大型语言模型（LLMs）及其众多应用，引起了对不同文档处理工具设计和文档存档、检索过程改进的研究关注。官方和法律文件领域尤其引人关注，因为每天生成的数据量巨大，同时存在大量的感兴趣从业者群（律师、律师事务所、行政人员、政府部门和公民）。因此，为自动化处理涉及法律文件的日常工作提供有效方法，有望在不同领域产生重大影响。本文提出了一种基于LLM的解决方案，用于塞尔维亚语法律文件中的命名实体识别（NER）。该解决方案利用了预训练的双向编码器表示（BERT），并对其进行了仔细的适应，以识别和分类文本内容中的特定数据点。此外，本文还介绍了为塞尔维亚语开发的新颖数据集（包括公共法院判决书），并讨论了系统设计和应用方法，以及所取得的性能指标及其对所提解决方案客观评估的影响。通过对手动标记的数据集进行交叉验证测试，获得的宏平均F1分数为0.96，以及对故意修改文本输入的额外结果，证实了所提系统设计的适用性和开发的NER解决方案的鲁棒性。', 'title_zh': '塞尔维亚法律文件中的命名实体识别：设计、方法学和数据集开发'}
{'arxiv_id': 'arXiv:2502.10577', 'title': "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias", 'authors': 'Enzo Doyen, Amalia Todirascu', 'link': 'https://arxiv.org/abs/2502.10577', 'abstract': 'Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs\' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that $\\approx$39.5\\% of LLMs\' responses to generic instructions are MG-biased ($\\approx$73.1\\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously.', 'abstract_zh': '大型语言模型（LLMs）已被证明会在特定或受限的情境下传播和放大性别偏见，这不仅存在于英语中，还存在于其他语言中。然而，至今为止没有研究专门关注LLMs对通用指令的回应中传达的性别偏见，尤其是关于代指混杂性别群体的男性代词（MG）。男性代词是一种在许多标有性别的语言中普遍存在的语言特征，指的是使用男性性别作为默认或假定中性的性别来指代混合性别群体或性别无关或未知的人。大量心理语言学研究表明，男性代词并不是中性的，会引发性别偏见。本研究旨在分析产权和本地LLMs对通用指令的回应中使用男性代词的情况，并评估它们的男性代词偏见率。我们着重于法语，并从现有的词汇资源中创建了一个人类名词数据库。我们筛选现有的法语指令数据集以检索通用指令，并分析了6种不同LLMs的回应。总体而言，我们发现约有39.5%的LLMs对通用指令的回应存在男性代词偏见（在包含人类名词的回应中，这一比例约为73.1%）。我们的研究还发现，LLMs自发使用性别公正语言的意愿较低。', 'title_zh': '人造语言模型？评估大型语言模型延续男性通用代词偏见的情况'}
{'arxiv_id': 'arXiv:2502.10739', 'title': 'BASE-SQL: A powerful open source Text-To-SQL baseline approach', 'authors': 'Lei Sheng, Shuai-Shuai Xu, Wei Xie', 'link': 'https://arxiv.org/abs/2502.10739', 'abstract': 'The conversion of natural language into SQL language for querying databases (Text-to-SQL) has broad application prospects and has attracted widespread attention. At present, the mainstream Text-to-SQL methods are mainly divided into in-context learning (ICL) based methods and supervised fine-tuning (SFT) based methods. ICL-based methods can achieve relatively good results thanks to the use of the most advanced closed-source models. However, in real-world application scenarios, factors such as data privacy, SQL generation efficiency and cost need to be considered. SFT-based methods have certain advantages. At present, methods based on fine-tuning of open source models lack easy-to-implement and effective (cost-effective) baseline methods. We propose a pipeline-based method using open source model fine-tuning, referred to as BASE-SQL, which includes four components: Schema Linking, Candidate SQL Generate, SQL Revision and SQL Merge Revision. Experimental results show that BASE-SQL uses the open source model Qwen2.5-Coder-32B-Instruct, and achieves an accuracy of 67.47% on the BIRD development set and 88.9% on the Spider test set, which is significantly better than other methods using open source models, and even exceeds several methods using the GPT-4o closed-source model. At the same time, BASE-SQL is easy to implement and highly efficient (on average, only five calls to the large language model are required to generate SQL once). The code will be open sourced at this https URL.', 'abstract_zh': '将自然语言转换为SQL语言以查询数据库（Text-to-SQL）具有广泛的应用前景，并引起了广泛的关注。目前，主流的Text-to-SQL方法主要分为上下文学习（Contextualized Learning, ICL）方法和监督微调（Supervised Fine-Tuning, SFT）方法。ICL方法由于使用了最先进的闭源模型，因此可以实现较为优异的结果。然而，在实际应用场景中，需要考虑数据隐私、SQL生成效率和成本等因素。SFT方法具有一定的优势。目前基于开源模型的微调方法缺乏易于实施且有效（经济实惠）的基准方法。我们提出了一种基于pipeline的方法，称为BASE-SQL，该方法包括四个组件：Schema Linking、Candidate SQL Generate、SQL Revision和SQL Merge Revision。实验结果表明，BASE-SQL使用开源模型Qwen2.5-Coder-32B-Instruct，在BIRD开发集上达到了67.47%的准确率，在Spider测试集上达到了88.9%的准确率，这显著优于其他使用开源模型的方法，甚至超过了几个使用GPT-4o闭源模型的方法。同时，BASE-SQL易于实现且高效（平均每次生成SQL只需调用大型语言模型五次）。源代码将在以下链接开源：<链接地址>。', 'title_zh': 'BASE-SQL：一种强大的开源文本到SQL基准方法'}
{'arxiv_id': 'arXiv:2502.10735', 'title': 'OPTISHEAR: Towards Efficient and Adaptive Pruning of Large Language Models via Evolutionary Optimization', 'authors': 'Shuqi Liu, Bowei He, Han Wu, Linqi Song', 'link': 'https://arxiv.org/abs/2502.10735', 'abstract': 'Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies inadequate for multiple models. In this paper, we introduce \\textbf{\\textsc{OptiShear}}, an efficient evolutionary optimization framework for adaptive LLM pruning. Our framework features two key innovations: an effective search space built on our Meta pruning metric to handle diverse weight distributions, and a model-wise reconstruction error for rapid evaluation during search trials. We employ Non-dominated Sorting Genetic Algorithm III (NSGA-III) to optimize both pruning metrics and layerwise sparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models (7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning metrics consistently outperform existing methods. Additionally, our discovered layerwise sparsity ratios enhance the effectiveness of other pruning metrics. The framework exhibits strong cross-task and cross-model generalizability, providing a cost-effective solution for model compression.', 'abstract_zh': '以下是经过学术规范翻译后的中文内容：\n\n在大规模语言模型（LLMs）不断快速增长的趋势下，后训练剪枝已经成为一种关键的优化技术。然而，不同LLM之间显著的权重分布差异使得固定剪枝策略在多种模型中并不适用。本文介绍了\\textbf{\\textsc{OptiShear}}，一种高效的进化优化框架，用于自适应LLM剪枝。该框架的两大创新点包括：基于我们提出的元剪枝度量构建的有效搜索空间，以处理多样化的权重分布；以及基于模型的重构误差，以实现搜索过程中快速评估。我们使用非支配排序遗传算法III（NSGA-III）来优化剪枝度量和逐层稀疏度比。通过在LLaMA-1/2/3和Mistral模型（7B-70B）上进行跨任务和跨模型的广泛评估，我们展示了自适应剪枝度量连贯地优于现有方法。此外，我们发现的逐层稀疏度比进一步增强了其他剪枝度量的有效性。该框架展示了较强的跨任务和跨模型泛化能力，提供了一种成本效益高的模型压缩解决方案。', 'title_zh': 'OPTISHEAR：基于进化优化的大型语言模型高效自适应剪枝方法'}
{'arxiv_id': 'arXiv:2502.10725', 'title': 'PropNet: a White-Box and Human-Like Network for Sentence Representation', 'authors': 'Fei Yang', 'link': 'https://arxiv.org/abs/2502.10725', 'abstract': 'Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.', 'abstract_zh': '基于Transformer的嵌入方法近年来在句向量表示领域占据主导地位。尽管它们在自然语言处理任务中，如语义文本相似性（STS）任务中取得了显著性能，但由于其黑盒性质和大量数据驱动的训练方式，这些方法引发了关于偏见、可信度和安全性等方面的问题。为了提高嵌入模型的可解释性，已经做出了许多努力，但这些问题尚未根本解决。为了实现内在的可解释性，我们提出了一种完全白盒且类人句子表示网络——PropNet。受到认知科学发现的启发，PropNet基于句子中的命题构建了一个分层网络。虽然实验表明，PropNet在STS任务中与最新的嵌入模型相比存在显著差距，但案例研究揭示了改进的空间。此外，PropNet使我们能够分析和理解STS基准背后的认知过程。', 'title_zh': 'PropNet：一种白箱且类人类的句子表示网络'}
{'arxiv_id': 'arXiv:2502.10709', 'title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'authors': 'Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang', 'link': 'https://arxiv.org/abs/2502.10709', 'abstract': "As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: this https URL.", 'abstract_zh': '随着LLM-as-a-Judge作为评估大型语言模型（LLMs）的新范式出现，人们对LLM评估者的一致性、偏差和稳定性提出了担忧。虽然大量研究集中在一致性与偏差方面，但关于LLM评估者稳定性的研究相对较少。本文通过在两种不同的评估设置中使用9种广泛使用的LLM评估工具，进行了大量的实验，以探究基于模型的LLM评估中的不确定性。研究发现，基于不同模型家族和规模的LLM评估工具表现出不同的不确定性。通过仔细比较分析，我们发现，无论是通过特殊的提示策略进行推理，还是在训练后应用特殊提示策略，都可以在一定程度上缓解评估不确定性。通过利用不确定性来增强LLM在异常分布（OOD）数据中的可靠性和检测能力，我们进一步微调了一个不确定性感知的LLM评估工具ConfiLM，并在人工设计的测试集上评估了ConfiLM的OOD评估能力，该测试集来源于2024年奥运会数据。实验结果表明，在微调阶段引入不确定性作为额外信息可以显著提高模型在OOD场景下的评估性能。本文的代码和数据已发布在：this https URL。', 'title_zh': '大型语言模型评估中不确定性的实证分析'}
{'arxiv_id': 'arXiv:2502.10497', 'title': 'Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA', 'authors': 'Mohammad Baqar, Rajat Khanda', 'link': 'https://arxiv.org/abs/2502.10497', 'abstract': "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance.\nThis paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications.\nFurthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.", 'abstract_zh': '近年来，生成型人工智能的发展显著提高了自然语言处理（NLP）系统的效率和适应性，特别是在检索增强生成（RAG）、低秩适应（LoRA）和分解低秩适应（DoRA）方面的进步。RAG 通过整合外部知识来增强生成输出的事实一致性，而 LoRA 则通过高效的参数调整来微调大型语言模型（LLMs）。DoRA 在此基础上进一步通过自适应参数排名和领域感知权重调整优化微调过程，提高学习效率同时保持推理性能。\n\n本文对 RAG、LoRA 和 DoRA 进行了大规模实证评估，模型微调和生成性能在 20,000 个基于 FAQ 的查询中进行评估，知识库包含 400,000 条条目。研究分析了关键性能指标，如准确性、相关性和推理延迟。实验结果表明，DoRA 在准确性（90.1%）、相关性评分（0.88）和每查询最低延迟（110 毫秒）方面表现最佳，同时在具体的领域生成型 AI 应用中优于 LoRA 和 RAG。\n\n此外，本文还考察了不同模型在微调效率、计算成本和实时适应性之间的权衡。研究结果强调了 RAG 在知识 grounding 方面的效能、LoRA 在成本效率领域的适应性以及 DoRA 在平衡微调效率和模型精度方面的能力。这些见解为在高精度要求领域（如医疗保健、金融和法律服务）部署 AI 驱动的生成系统提供了实用指南，确保在动态环境中具有可扩展性、可靠性和最优性能。', 'title_zh': '幻觉与真实：RAG、LoRA和DoRA的全面准确性评估'}
{'arxiv_id': 'arXiv:2502.12149', 'title': 'HARBOR: Exploring Persona Dynamics in Multi-Agent Competition', 'authors': 'Kenan Jiang, Li Xiong, Fei Liu', 'link': 'https://arxiv.org/abs/2502.12149', 'abstract': "We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.", 'abstract_zh': '我们将通过使用拍卖作为测试平台来调查因素对LLM代理在竞争性多代理环境中的成功影响。在拍卖中，代理通过出价以最大化利润来测试它们。代理将配备有关出价领域的知识、反映其偏好的人物角色，以及拍卖历史的记忆。我们的研究通过为多个代理创造一个现实环境来进行一次经典拍卖场景的扩展，在这个环境中，多个代理竞拍房屋，权衡诸如大小、位置和预算等因素以在最低价格下获得最理想的房屋。特别地，我们将探讨三个关键问题：(a) 人物角色如何影响代理在竞争性环境中的行为？(b) 代理能否有效地在其竞拍过程中分析竞争对手的行为？(c) 怎样利用人物角色分析来运用元理论等策略以获得优势？通过一系列实验，我们将分析LLM代理的行为，并揭示新的发现。我们称之为HARBOR的测试平台为我们进一步理解竞争性环境中多代理工作流提供了宝贵的平台。', 'title_zh': 'HARBOR：探索多代理竞争中的角色动态'}
{'arxiv_id': 'arXiv:2502.12120', 'title': 'LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws', 'authors': 'Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel', 'link': 'https://arxiv.org/abs/2502.12120', 'abstract': 'Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.', 'abstract_zh': '规模法则通过提供模型大小、标记数量和计算资源之间最优平衡的估算，指导大型语言模型（LLMs）的发展。最近，有关损失之间的尺度法则逐渐崭露头角，这些法则将预训练数据集和下游任务的损失联系起来，成为理解并提升LLM性能的强大工具。在本研究中，我们探讨了哪些因素最能影响损失之间的尺度法则。实验证明，预训练数据和编码器决定了尺度变化的趋势，而模型大小、优化超参数，甚至像Transformer模型（如Llama）与状态空间模型（如Mamba）之间显著的架构差异，只有有限的影响。因此，实践者应仔细选择合适的预训练数据集以实现最佳下游性能，而架构和其他设置则可以自由优化以提高训练效率。', 'title_zh': 'LLMs 在线性中的表现：数据决定了损失归一化法则'}
{'arxiv_id': 'arXiv:2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'authors': 'Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen', 'link': 'https://arxiv.org/abs/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: this https URL, dedicated to documenting research in the field of specialized LLM.', 'abstract_zh': '大语言模型（LLMs）在自然语言理解、文本摘要和机器翻译等多种任务中展示了显著的成功。然而，它们的通用性质往往限制了其在需要专门知识的具体领域应用中的有效性，如医疗、化学或法律分析。为解决这一问题，研究人员探索了多种方法来增强LLMs，并通过集成特定领域的知识来增强其能力。在这篇综述中，我们提供了这些方法的全面概述，并将其归类为四个主要方面：动态知识注入、静态知识嵌入、模块化适配器和提示优化。每种方法都提供了独特的机制，以赋予LLMs领域专长，并在灵活性、可扩展性和效率之间进行权衡。我们讨论了这些方法如何使LLMs能够应对专门任务，比较了它们的优点和缺点，评估了特定领域LLMs与通用LLMs的性能，并指出了这一新兴领域中的挑战和机遇。对于希望深入了解这一领域的读者，我们还总结了常用的数据集和基准。为了使研究人员能够及时了解最新研究，我们维护了一个开源平台：[这个链接](this https URL)，专门用于记录该领域中专门LLMs的研究。', 'title_zh': '将领域特定知识注入大规模语言模型：一个综合调研'}
{'arxiv_id': 'arXiv:2502.12119', 'title': 'PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection', 'authors': 'Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma', 'link': 'https://arxiv.org/abs/2502.12119', 'abstract': 'Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation. To address this challenge, we propose PRISM, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enbles data-efficient selection,but maintains the original performance. Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7% relative improvement in final performance.', 'abstract_zh': '视觉指令调优可以细化预训练的多模态大语言模型（MLLMs），以提高其实用任务性能。然而，视觉指令数据集的快速扩张导致了显著的数据冗余，从而增加了过多的计算成本。现有的数据选择方法主要依赖于代理模型或基于损失的指标，这两种方法都因需要模型推理和反向传播而带来了巨大的计算开销。为了解决这个问题，我们提出了PRISM，一个新型的不需要训练的多模态数据选择方法。与现有方法不同，PRISM消除了对代理模型、预训练暖启动和梯度优化的依赖。相反，它利用皮尔逊相关分析来量化MLLMs的内在视觉编码特性，并计算任务特异的相关分数以识别高价值实例。这不仅实现了数据高效选择，而且保持了原始性能。在多个MLLM上的实证评估表明，PRISM将视觉指令调优和数据选择所需的总时间降低到了传统方法的30%，并且在八个跨模态和三个语言理解基准测试中超越了完全微调的模型，最终性能提高了101.7%。', 'title_zh': 'PRISM：一种用于无训练多模态数据选择的自我剪枝内在选择方法'}
{'arxiv_id': 'arXiv:2502.10699', 'title': 'Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration', 'authors': 'George Applegarth, Christian Weatherstone, Maximilian Hollingsworth, Henry Middlebrook, Marcus Irvin', 'link': 'https://arxiv.org/abs/2502.10699', 'abstract': 'Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.', 'abstract_zh': '上下文记忆整合仍然是语言模型发展中的一大难题，特别是在需要在长时间序列中保持连贯性的情景下。传统方法，如自我注意力机制和记忆增强架构，往往优先考虑短时依赖性，导致远距离上下文理解上的碎片化和不一致。受生物神经系统中突触可塑性原理的启发，提出了一种新的机制——突触共振（Synaptic Resonance），该机制在训练和推理过程中动态强化相关记忆通路。与静态的记忆表示不同，该机制根据上下文相关性不断调整突触权重矩阵，从而在不增加过多计算开销的情况下增强信息保留能力。在开源语言模型上的评估显示，该机制能够降低困惑度、提升上下文连贯性，并增强对输入噪声的鲁棒性，这突显了增强型记忆调节的有效性。与基线模型的对比分析进一步表明，所提出的方法在保持计算可行性的同时实现了更高的记忆保留效率。这些架构修改可以无缝集成到现有Transformer框架中，确保稳定收敛和高效推理，而不牺牲扩展性。受益于长期上下文一致性的应用，如对话系统和文档摘要，有望从这种方法中获益。实证研究表明，动态强化的记忆通路为解决长期序列建模中的传统机制局限性提供了有前景的替代方案。', 'title_zh': '探索大型语言模型中的突触共振：一种新的上下文记忆整合方法'}
{'arxiv_id': 'arXiv:2502.10660', 'title': 'User Profile with Large Language Models: Construction, Updating, and Benchmarking', 'authors': 'Nusrat Jahan Prottasha, Md Kowsher, Hafijur Raman, Israt Jahan Anny, Prakash Bhat, Ivan Garibay, Ozlem Garibay', 'link': 'https://arxiv.org/abs/2502.10660', 'abstract': 'User profile modeling plays a key role in personalized systems, as it requires building accurate profiles and updating them with new information. In this paper, we present two high-quality open-source user profile datasets: one for profile construction and another for profile updating. These datasets offer a strong basis for evaluating user profile modeling techniques in dynamic settings. We also show a methodology that uses large language models (LLMs) to tackle both profile construction and updating. Our method uses a probabilistic framework to predict user profiles from input text, allowing for precise and context-aware profile generation. Our experiments demonstrate that models like Mistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the precision and recall of the generated profiles, and high evaluation scores confirm the effectiveness of our approach.', 'abstract_zh': '用户画像建模在个性化系统中起着关键作用，因为它要求建立准确的用户画像并根据新信息进行更新。在本文中，我们发布了两个高质量的开源用户画像数据集：一个用于用户画像的构建，另一个用于用户画像的更新。这些数据集为在动态环境中评估用户画像建模技术提供了坚实的基础。我们还展示了使用大规模语言模型（LLMs）来解决用户画像的构建与更新问题的方法。我们的方法采用概率框架从输入文本预测用户画像，从而实现精确且具有上下文感知的用户画像生成。实验结果表明，如Mistral-7b和Llama2-7b等模型在这两项任务中表现优异。大规模语言模型提高了生成用户画像的精确性和召回率，而高评价分数进一步验证了我们方法的有效性。', 'title_zh': '使用大规模语言模型构建、更新与基准测试用户画像'}
{'arxiv_id': 'arXiv:2502.12118', 'title': 'Scaling Test-Time Compute Without Verification or RL is Suboptimal', 'authors': 'Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar', 'link': 'https://arxiv.org/abs/2502.12118', 'abstract': 'Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.', 'abstract_zh': '尽管在扩展推理时的计算量方面取得了显著进展，学术界仍在争论如何进一步扩大这种扩展以实现持续且高效的性能提升。主要存在两种方法：首先，提炼成功的搜索或思考痕迹；其次，利用验证（例如，0/1结果奖励、奖励模型或验证器）来引导强化学习（RL）和搜索算法。在本文中，我们证明，在固定计算/数据预算的情况下，基于强化学习或搜索的验证器导向（VB）方法对大型语言模型（LLM）的微调，远远优于依赖于提炼或克隆搜索痕迹的无验证器导向（VF）方法。进一步地，我们展示了随着推理时计算量（通过输出标记长度衡量）和训练数据的增加，VF方法的不足比VB方法更为严重，当基础预训练的LLM在正确解题痕迹分布上呈现异质性（例如，不同的长度、风格等），并且在从其采样的痕迹上具有非尖锐的奖励分布时尤为如此。我们使用反归垛性[艾尔德什，1945]来形式化这一条件。这暗示了一个更强大的结果，即VB方法在渐进行为方面表现更优，随着推理时间预算的增加，VB方法与VF方法之间的性能差距会扩大。我们通过使用3/8/32B大小的预训练LLM在教学和数学推理问题上的实验结果证实了这一理论，发现验证对于扩展推理时计算量至关重要。', 'title_zh': '不进行验证或RL的测试时计算扩展是次优的'}
{'arxiv_id': 'arXiv:2502.10645', 'title': 'BabyLM Turns 3: Call for papers for the 2025 BabyLM workshop', 'authors': 'Lucas Charpentier, Leshem Choshen, Ryan Cotterell, Mustafa Omer Gul, Michael Hu, Jaap Jumelet, Tal Linzen, Jing Liu, Aaron Mueller, Candace Ross, Raj Sanjay Shah, Alex Warstadt, Ethan Wilcox, Adina Williams', 'link': 'https://arxiv.org/abs/2502.10645', 'abstract': 'BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 3rd BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: INTERACTION. This new track encourages interactive behavior, learning from a teacher, and adapting the teaching material to the student. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.', 'abstract_zh': 'BabyLM旨在打破认知建模与语言建模之间的边界。我们邀请提交研讨会论文，并希望研究人员参加第3届BabyLM竞赛。如同往年一样，我们呼吁在通用赛道中参加数据高效预训练挑战的参赛者。今年，我们还提供一个新的赛道：交互（INTERACTION）。这一新赛道鼓励互动行为、从教师处学习，并根据学生的状况调整教学材料。我们还欢迎在任何相关领域提交非竞赛论文。这些领域包括训练效率、认知上合理的研究、模型评估的薄弱之处等。', 'title_zh': 'BabyLM满三岁：2025 BabyLM研讨会论文征稿启事'}
{'arxiv_id': 'arXiv:2502.12094', 'title': 'A Study on Leveraging Search and Self-Feedback for Agent Reasoning', 'authors': 'Karthikeyan K, Michelle Yuan, Elman Mansimov, Katerina Margatina, Anurag Pratik, Daniele Bonadiman, Monica Sunkara, Yi Zhang, Yassine Benajiba', 'link': 'https://arxiv.org/abs/2502.12094', 'abstract': "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model's own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model's self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.", 'abstract_zh': '近年来的研究表明，在推理过程中引入搜索可以显著提升语言代理的推理能力。某些方法可能会利用 ground-truth 或依赖模型自身生成的反馈来辅助推理。搜索算法利用这种反馈来生成更新探索和利用各种推理路径的标准。在本研究中，我们探讨了如何利用搜索和模型的自我反馈来完成推理任务。首先，我们研究了在数学推理任务中 ground-truth 反馈与自我反馈之间的差异。其次，我们观察了将搜索技术应用于更复杂的任务（如工具调用）时的局限性，并为此类任务开发了特定的方法以解决这些差距。实验结果揭示了仅依赖自我反馈进行搜索时面临的一般化挑战。要使搜索有效，要么需要访问 ground-truth，要么需要为特定任务精心设计反馈机制。', 'title_zh': '利用搜索和自反馈提升代理推理能力的研究'}
{'arxiv_id': 'arXiv:2502.10641', 'title': 'Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility', 'authors': 'Zhaoqian Xue, Guanhong Liu, Kai Wei, Chong Zhang, Qingcheng Zeng, Songhua Hu, Wenyue Hua, Lizhou Fan, Yongfeng Zhang, Lingyao Li', 'link': 'https://arxiv.org/abs/2502.10641', 'abstract': 'Access to health resources is a critical determinant of public well-being and societal resilience, particularly during public health crises when demand for medical services and preventive care surges. However, disparities in accessibility persist across demographic and geographic groups, raising concerns about equity. Traditional survey methods often fall short due to limitations in coverage, cost, and timeliness. This study leverages crowdsourced data from Google Maps reviews, applying advanced natural language processing techniques, specifically ModernBERT, to extract insights on public perceptions of health resource accessibility in the United States during the COVID-19 pandemic. Additionally, we employ Partial Least Squares regression to examine the relationship between accessibility perceptions and key socioeconomic and demographic factors including political affiliation, racial composition, and educational attainment. Our findings reveal that public perceptions of health resource accessibility varied significantly across the U.S., with disparities peaking during the pandemic and slightly easing post-crisis. Political affiliation, racial demographics, and education levels emerged as key factors shaping these perceptions. These findings underscore the need for targeted interventions and policy measures to address inequities, fostering a more inclusive healthcare infrastructure that can better withstand future public health challenges.', 'abstract_zh': '公共卫生资源的获取是评估公众福祉和社会韧性的重要决定因素，特别是在公共卫生危机期间，医疗服务和预防保健的需求激增时更为显著。然而，不同人口和社会地理群体之间在获取资源方面的差异依然存在，这种不平等引发了公平性方面的担忧。传统的调查方法往往由于覆盖范围有限、成本高昂和时效性差等原因而导致不足。本研究利用来自Google Maps评论的众包数据，结合先进的自然语言处理技术（特别是ModernBERT），以提取美国在COVID-19疫情期间公众对健康资源获取感知方面的洞见。此外，我们还利用部分最小二乘回归方法来探究获取感知与关键的经济社会人口因素之间的关系，包括政治立场、种族构成和教育水平。研究发现，在美国不同地区，公众对健康资源获取的感知存在显著差异，疫情高峰期间差异最大，危机过后有所缓解。政治立场、种族构成和教育水平是塑造这些感知的主要因素。这些发现强调了需要有针对性的干预措施和政策手段来解决不平等现象，以促进一个更具包容性的医疗卫生基础设施，使其能够更好地应对未来的公共卫生挑战。', 'title_zh': '公平获取之路：利用 crowdsourced 评价探讨公众对健康资源可及性的感知'}
{'arxiv_id': 'arXiv:2502.12085', 'title': 'APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs', 'authors': 'Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.12085', 'abstract': 'While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in this https URL.', 'abstract_zh': '尽管长上下文推理对于大型语言模型（LLM）的应用至关重要，其预填充速度仍然是一个重要瓶颈。当前的方法，包括序列并行策略和通过近似注意机制减少计算量，仍然无法提供最优的推理效率。这阻碍了对更长序列的输入进行扩展，并且不能及时处理长上下文查询。为了解决这一问题，我们提出了一种高效的长上下文推理框架APB，该框架利用多主机近似注意机制，同时减少计算量和增强并行性，从而提高预填充速度。APB引入了一种在序列并行框架中通信关键值对的机制，能够在保持任务性能的同时加快推理速度。我们通过结合定制化的FlashAttn内核和优化的分布策略来实现APB，支持多种模型和并行配置。与FlashAttn、RingAttn和StarAttn相比，APB分别实现了高达9.2倍、4.2倍和1.6倍的加速，且没有观察到任务性能下降。我们在这里提供APB的实现和实验代码：[此链接]。', 'title_zh': 'APB：通过在GPU之间传递压缩上下文块加速分布式长上下文推理'}
{'arxiv_id': 'arXiv:2502.12081', 'title': 'Unhackable Temporal Rewarding for Scalable Video MLLMs', 'authors': 'En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao', 'link': 'https://arxiv.org/abs/2502.12081', 'abstract': 'In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the "anti-scaling law", where more data and larger models lead to worse performance. This study unmasks the culprit: "temporal hacking", a phenomenon where models shortcut by fixating on select frames, missing the full video narrative. In this work, we systematically establish a comprehensive theory of temporal hacking, defining it from a reinforcement learning perspective, introducing the Temporal Perplexity (TPL) score to assess this misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework to mitigate the temporal hacking. Both theoretically and empirically, TPL proves to be a reliable indicator of temporal modeling quality, correlating strongly with frame activation patterns. Extensive experiments reveal that UTR not only counters temporal hacking but significantly elevates video comprehension capabilities. This work not only advances video-AI systems but also illuminates the critical importance of aligning proxy rewards with true objectives in MLLM development.', 'abstract_zh': '在追求卓越的视频处理机器学习模型（MLLMs）的过程中，我们遇到了一个令人困惑的悖论：“反扩展定律”，即更多的数据和更大的模型反而导致性能下降。本研究揭开了罪魁祸首：“时间捷径”现象，这是一种模型通过固定在某些帧上而忽视完整视频叙述的机制。在本研究中，我们系统地建立了综合的时间捷径理论，从强化学习的角度定义了时间困惑度（Temporal Perplexity, TPL）评分，用来评估这种不一致，并提出了不可被捷径化的时序奖励（Unhackable Temporal Rewarding, UTR）框架来缓解时间捷径现象。无论是从理论还是实验角度来看，TPL 都证明是一个可靠的时序建模质量指标，与帧激活模式高度相关。大量实验表明，UTR 不仅能够对抗时间捷径，还能显著提升视频理解能力。本研究不仅推进了视频AI系统的进步，还揭示了在MLLM开发中代理奖励与真实目标一致的重要性。', 'title_zh': '不可破解的时间奖励机制以实现可扩展的视频MLLM'}
{'arxiv_id': 'arXiv:2502.10634', 'title': 'Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"', 'authors': 'Hao Sun, Chenming Tang, Gengyang Li, Yunfang Wu', 'link': 'https://arxiv.org/abs/2502.10634', 'abstract': 'By simply incorporating demonstrations into the context, in-context learning (ICL) enables large language models (LLMs) to yield awesome performance on many tasks. In this paper, we focus on passage-level long-context ICL for generation tasks and find that LLMs cannot learn the intrinsic relationships between the demonstration passage and the generation output. We conduct experiments with different LLMs on two typical generation tasks including single-document QA and distractor generation, demonstrating that even a completely meaningless demonstration passage with 1/4 length achieves much better performance than the original full passage. Analysis via attention score reveals that LLMs pay little attention to passages compared to other components in prompt and little attention flows from the passage to other parts of the demonstration, which further confirms our finding. Additionally, experiments on context compression indicate that compression approaches proven effective on other long-context tasks are not suitable for passage-level ICL, since simply using shorter meaningless demonstration passages has achieved competitive performance.', 'abstract_zh': '通过将演示融入上下文，上下文中的学习（ICL）使大型语言模型（LLMs）在许多任务上表现出色。在本文中，我们专注于段落级别的长上下文ICL生成任务，并发现LLMs无法学习演示段落与生成输出之间的内在关系。我们在两个典型的生成任务（单文档问答和干扰生成）上使用不同的LLMs进行实验，结果显示，即使是长度仅为原段落四分之一且完全无意义的演示段落也能获得远优于原始全段落的效果。通过注意力分数的分析发现，LLMs在提示中的其他组成部分上关注的程度远高于对段落的关注，而且从段落到演示其他部分的注意力流也非常少，这进一步证实了我们的发现。此外，上下文压缩实验表明，适用于其他长上下文任务的有效压缩方法并不适用于段落级别的ICL，因为使用更短且无意义的演示段落已经达到了可竞争水平的性能。', 'title_zh': '迷失在段落中：段落级别的上下文学习不一定需要一个“段落”'}
{'arxiv_id': 'arXiv:2502.12025', 'title': 'SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities', 'authors': 'Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12025', 'abstract': 'Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.', 'abstract_zh': '新兴的大型推理模型（LRMs），如DeepSeek-R1模型，通过利用长推理链（CoT）来生成结构化的中间步骤，从而增强其推理能力。然而，长CoT本身并不能保证安全的输出，可能会导致诸如代码中引入安全漏洞或传播虚假信息等危害性后果。当前对大型语言模型（LLMs）安全性的研究通常集中于短答案响应，忽视了LRMs的长CoT风格输出。为缩小这一差距，我们进行了一项系统性的LRM安全性研究。首先，我们研究经过人类注释校准的安全评估器。利用我们新开发的指标，我们全面评估了12种最先进的LRMs在StrongReject和WildJailbreak数据集上的安全性。结果显示，LRMs的安全性不如其推理能力先进。进一步地，我们对推理轨迹和最终回答进行了细致分析。我们发现，三种解码策略——ZeroThink、LessThink和MoreThink——可以在不进行额外训练的情况下提高模型安全性。然而，这些策略要么限制了推理轨迹，要么导致推理成本增加。为了更好地加强LRM的安全性，我们引入了SafeChain，这是首个适用于CoT风格的安全训练数据集。我们对两种LRM进行了微调，结果显示SafeChain不仅能增强模型安全性，还能在6个推理基准测试中保持性能。', 'title_zh': 'SafeChain：具备长链推理能力的语言模型的安全性'}
{'arxiv_id': 'arXiv:2502.10632', 'title': 'Code-Mixed Telugu-English Hate Speech Detection', 'authors': 'Santhosh Kakarla, Gautama Shastry Bulusu Venkata', 'link': 'https://arxiv.org/abs/2502.10632', 'abstract': 'Hate speech detection in low-resource languages like Telugu is a growing challenge in NLP. This study investigates transformer-based models, including TeluguHateBERT, HateBERT, DeBERTa, Muril, IndicBERT, Roberta, and Hindi-Abusive-MuRIL, for classifying hate speech in Telugu. We fine-tune these models using Low-Rank Adaptation (LoRA) to optimize efficiency and performance. Additionally, we explore a multilingual approach by translating Telugu text into English using Google Translate to assess its impact on classification accuracy.\nOur experiments reveal that most models show improved performance after translation, with DeBERTa and Hindi-Abusive-MuRIL achieving higher accuracy and F1 scores compared to training directly on Telugu text. Notably, Hindi-Abusive-MuRIL outperforms all other models in both the original Telugu dataset and the translated dataset, demonstrating its robustness across different linguistic settings. This suggests that translation enables models to leverage richer linguistic features available in English, leading to improved classification performance. The results indicate that multilingual processing can be an effective approach for hate speech detection in low-resource languages. These findings demonstrate that transformer models, when fine-tuned appropriately, can significantly improve hate speech detection in Telugu, paving the way for more robust multilingual NLP applications.', 'abstract_zh': '低资源语言（如泰卢固语）中的仇恨言论检测是自然语言处理（NLP）领域不断增长的挑战。本研究探讨了基于变换器的模型，包括泰卢固语HateBERT、HateBERT、DeBERTa、Muril、IndicBERT、RoBERTa以及 Hindi-Abusive-MuRIL，用于泰卢固语仇恨言论分类。我们通过低秩适应（LoRA）对这些模型进行了微调，以优化效率和性能。此外，我们还探讨了多语种方法，通过对泰卢固语文本使用Google Translate翻译成英语，评估其对分类准确率的影响。\n\n实验结果表明，大多数模型在经过翻译后显示出性能提升，DeBERTa和Hindi-Abusive-MuRIL在准确率和F1分数方面表现优于直接使用泰卢固语文本进行训练。值得注意的是，Hindi-Abusive-MuRIL不仅在原始泰卢固语文本数据集中，而且在翻译后的数据集中表现最佳，证明其在不同语言环境下的鲁棒性。这表明翻译可以使模型利用英语中丰富的语言特征，从而提高分类性能。结果表明，多语种处理可以在低资源语言中的仇恨言论检测中成为有效的方法。这些发现表明，当适当微调时，变换器模型可以显著提高泰卢固语中的仇恨言论检测能力，为更强大的多语种NLP应用铺平了道路。', 'title_zh': '代码混合泰卢固-英语仇恨言论检测'}
{'arxiv_id': 'arXiv:2502.10615', 'title': 'Retrieval-augmented Encoders for Extreme Multi-label Text Classification', 'authors': 'Yau-Shian Wang, Wei-Cheng Chang, Jyun-Yu Jiang, Jiong Zhang, Hsiang-Fu Yu, S. V. N. Vishwanathan', 'link': 'https://arxiv.org/abs/2502.10615', 'abstract': 'Extreme multi-label classification (XMC) seeks to find relevant labels from an extremely large label collection for a given text input. To tackle such a vast label space, current state-of-the-art methods fall into two categories. The one-versus-all (OVA) method uses learnable label embeddings for each label, excelling at memorization (i.e., capturing detailed training signals for accurate head label prediction). In contrast, the dual-encoder (DE) model maps input and label text into a shared embedding space for better generalization (i.e., the capability of predicting tail labels with limited training data), but may fall short at memorization. To achieve generalization and memorization, existing XMC methods often combine DE and OVA models, which involves complex training pipelines. Inspired by the success of retrieval-augmented language models, we propose the Retrieval-augmented Encoders for XMC (RAEXMC), a novel framework that equips a DE model with retrieval-augmented capability for efficient memorization without additional trainable parameter. During training, RAEXMC is optimized by the contrastive loss over a knowledge memory that consists of both input instances and labels. During inference, given a test input, RAEXMC retrieves the top-$K$ keys from the knowledge memory, and aggregates the corresponding values as the prediction scores. We showcase the effectiveness and efficiency of RAEXMC on four public LF-XMC benchmarks. RAEXMC not only advances the state-of-the-art (SOTA) DE method DEXML, but also achieves more than 10x speedup on the largest LF-AmazonTitles-1.3M dataset under the same 8 A100 GPUs training environments.', 'abstract_zh': '极端多标签分类（XMC）的目标是从极其庞大的标签集合中为给定的文本输入找到相关标签。为解决如此庞大的标签空间问题，当前最先进的方法主要分为两大类。一种是“一-vs-全”（OVA）方法，它为每个标签学习标签嵌入，擅长记忆（即捕捉详细的训练信号以实现精确的头类标签预测）。相比之下，双编码器（DE）模型将输入和标签文本映射到共享的嵌入空间，以便更好地泛化（即在有限的训练数据下预测尾类标签的能力），但在记忆方面可能表现不佳。为了实现泛化和记忆，现有的XMC方法通常结合使用DE和OVA模型，这涉及到复杂的训练管道。受检索增强语言模型成功的启发，我们提出了一种名为检索增强编码器用于XMC（RAEXMC）的新框架，该框架为DE模型配备了检索增强的能力，无需额外的可训练参数即可实现高效的记忆。在训练过程中，RAEXMC通过包含输入实例和标签的知识记忆优化，使其能够使用对比损失进行优化。在推理过程中，给定测试输入后，RAEXMC从知识记忆中检索前K个键，并聚合对应的值作为预测分数。我们在四个公开的LF-XMC基准数据集上展示了RAEXMC的有效性和效率。RAEXMC不仅推进了现有的最先进（SOTA）的DE方法DEXML，还在相同8块A100 GPU的训练环境下，对于最大的LF-AmazonTitles-1.3M数据集实现了超过10倍的速度提升。', 'title_zh': '面向极端多标签文本分类的检索增强编码器'}
{'arxiv_id': 'arXiv:2502.10596', 'title': 'Post-training an LLM for RAG? Train on Self-Generated Demonstrations', 'authors': 'Matthew Finlayson, Ilia Kulikov, Daneil M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu', 'link': 'https://arxiv.org/abs/2502.10596', 'abstract': 'Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering "Who won the latest World Cup?" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents -- a technique known as retrieval augmented generation (RAG) -- mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.', 'abstract_zh': '大型语言模型（LLMs）在知识密集型自然语言处理（NLP）任务中往往表现不佳，例如回答“最新世界杯的获胜者是谁？”这类问题，因为它们在训练过程中学到的知识可能不足或过时。通过检索文档来条件化生成——这一技术被称为检索增强生成（RAG）——可以通过使模型利用上下文信息来缓解这些问题。实践者可以通过在增强检索的基础上微调LLM来提高RAG性能，但必须注意这可能导致不可 desirable 的模型行为，如幻觉。我们归因于这一退化的原因是训练数据很可能对于模型来说是域外的，并且可能存在质量方面的问题，例如检索与目标响应之间存在偏差（因为检索通常是后来添加的）。我们提出了一种使用自我生成的示范训练RAG增强的LLMs的方法，从而避免在域外文本上进行训练，并将检索整合到LLM的响应中。我们在知识密集型问答（QA）任务上评估了该方法，并展示了该方法教会LLMs正确处理上下文信息检索和避免对它可能答错的问题进行提问。与传统的RA-IT方法相比，该方法在非RAG设置中防止了模型性能的退化，并展现了更好的问答性能。', 'title_zh': '对LLM进行后训练以支持RAG？使用自生成示例进行训练'}
{'arxiv_id': 'arXiv:2502.10582', 'title': 'Named entity recognition for Serbian legal documents: Design, methodology and dataset development', 'authors': 'Vladimir Kalušev, Branko Brkljač', 'link': 'https://arxiv.org/abs/2502.10582', 'abstract': 'Recent advancements in the field of natural language processing (NLP) and especially large language models (LLMs) and their numerous applications have brought research attention to design of different document processing tools and enhancements in the process of document archiving, search and retrieval. Domain of official, legal documents is especially interesting due to vast amount of data generated on the daily basis, as well as the significant community of interested practitioners (lawyers, law offices, administrative workers, state institutions and citizens). Providing efficient ways for automation of everyday work involving legal documents is therefore expected to have significant impact in different fields. In this work we present one LLM based solution for Named Entity Recognition (NER) in the case of legal documents written in Serbian language. It leverages on the pre-trained bidirectional encoder representations from transformers (BERT), which had been carefully adapted to the specific task of identifying and classifying specific data points from textual content. Besides novel dataset development for Serbian language (involving public court rulings), presented system design and applied methodology, the paper also discusses achieved performance metrics and their implications for objective assessment of the proposed solution. Performed cross-validation tests on the created manually labeled dataset with mean $F_1$ score of 0.96 and additional results on the examples of intentionally modified text inputs confirm applicability of the proposed system design and robustness of the developed NER solution.', 'abstract_zh': '近年来，自然语言处理（NLP）领域特别是大型语言模型（LLMs）及其众多应用的发展引起了对不同文档处理工具设计和文档存档、检索过程改进的研究关注。对于官方和法律文件领域来说，由于每天生成的数据量庞大，以及广泛的从业者群体（律师、律师事务所、行政人员、政府机构和公民），提供高效的工作自动化方式预计将在多个领域产生重大影响。在此项工作中，我们提出了一种基于LLM的解决方案，用于塞尔维亚语法律文件的命名实体识别（NER）。该解决方案利用了预训练的变压器双向编码器表示（BERT），并且特别针对识别和分类文本内容中特定数据点的具体任务进行了精心调整。除了开发了塞尔维亚语的新数据集（涉及公共法院判决书），该论文还介绍了系统设计和方法论，并讨论了实现性能指标及其对所提方案客观评估的影响。通过在创建的手工标注数据集上进行交叉验证测试，得到平均$F_1$得分为0.96，以及对故意修改的文本输入的额外结果，这些都证明了所提系统设计的适用性和开发的NER解决方案的稳健性。', 'title_zh': '塞尔维亚法律文件中的命名实体识别：系统设计、方法学和数据集开发'}
{'arxiv_id': 'arXiv:2502.11919', 'title': 'From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis', 'authors': 'Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ziang Xiao, Ming Yin', 'link': 'https://arxiv.org/abs/2502.11919', 'abstract': "AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not %understand reflect on AI's decision recommendations critically. Large language models (LLMs), with their exceptional conversational and analytical capabilities, present great opportunities to enhance AI-assisted decision making in the absence of AI explanations by providing natural-language-based analysis of AI's decision recommendation, e.g., how each feature of a decision making task might contribute to the AI recommendation. In this paper, via a randomized experiment, we first show that presenting LLM-powered analysis of each task feature, either sequentially or concurrently, does not significantly improve people's AI-assisted decision performance. To enable decision makers to better leverage LLM-powered analysis, we then propose an algorithmic framework to characterize the effects of LLM-powered analysis on human decisions and dynamically decide which analysis to present. Our evaluation with human subjects shows that this approach effectively improves decision makers' appropriate reliance on AI in AI-assisted decision making.", 'abstract_zh': '人工智能辅助决策日益普遍，然而个体往往未能恰当利用基于人工智能的决策辅助工具，尤其是在缺乏人工智能解释的情况下，这可能是因为他们未能批判性地反思人工智能的决策建议。大型语言模型（LLMs）凭借其卓越的对话和分析能力，为在缺乏人工智能解释的情况下提升人工智能辅助决策提供了巨大的机会。通过提供基于自然语言的对人工智能决策建议的分析，例如，解释每个决策任务特征如何影响人工智能的建议。在本文中，通过随机实验，我们首先表明，无论是依次展示还是同时展示LLM驱动的每个任务特征的分析，都未能显著提高人们通过人工智能辅助进行的决策表现。为了使决策者更好地利用LLM驱动的分析，我们随后提出了一种算法框架，用于刻画LLM驱动分析对人类决策的影响，并动态决定展示哪种分析。我们的用户评估表明，这种方法有效地提高了决策者在人工智能辅助决策中对人工智能的恰当依赖程度。', 'title_zh': '从文本到信任：借助适应性大型语言模型驱动分析赋能人工智能辅助决策making'}
{'arxiv_id': 'arXiv:2502.10577', 'title': "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias", 'authors': 'Enzo Doyen, Amalia Todirascu', 'link': 'https://arxiv.org/abs/2502.10577', 'abstract': 'Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs\' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a "default" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the responses of 6 different LLMs. Overall, we find that $\\approx$39.5\\% of LLMs\' responses to generic instructions are MG-biased ($\\approx$73.1\\% across responses with human nouns). Our findings also reveal that LLMs are reluctant to using gender-fair language spontaneously.', 'abstract_zh': '大型语言模型（LLMs）已被证明在特定或受限的上下文中传播甚至放大性别偏见，这不仅限于英语，还包括其他语言。然而，目前还没有研究专门关注LLMs对通用指令的响应中传达的性别偏见，尤其是男性泛指（MG）。MG是许多具有性别标记的语言中的一种语言特征，表示使用男性性别作为“默认”或假定的中性性别来指代混杂的男性和女性群体，或者指性别无关或未知的人。大量的心理语言学研究表明，MG并非中立，而是会产生性别偏见。本文旨在分析商业和本地LLMs在对通用指令的响应中使用MG的情况，并评估它们的MG偏差率。我们专注于法语，并从现有的词汇资源中创建了一个人的名词数据库。我们筛选现有的法语指令数据集以检索通用指令，并分析了6种不同LLM的响应。整体而言，我们发现大约39.5%的LLMs对通用指令的响应存在MG偏差（在包含人类名词的响应中这一比例约为73.1%）。我们的研究还发现，LLMs在自发使用性别公平的语言方面比较消极。', 'title_zh': '人造语言模型？评估大型语言模型中男性代词偏差的延续程度'}
{'arxiv_id': 'arXiv:2502.10497', 'title': 'Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA', 'authors': 'Mohammad Baqar, Rajat Khanda', 'link': 'https://arxiv.org/abs/2502.10497', 'abstract': "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance.\nThis paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications.\nFurthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.", 'abstract_zh': '近年来，生成式人工智能在自然语言处理（NLP）系统中的效率和适应性方面取得了显著进步，特别是在检索增强生成（RAG）、低秩适应（LoRA）和分解低秩适应（DoRA）技术的应用中。RAG通过整合外部知识来提高生成输出的事实一致性，而LoRA使大型语言模型（LLMs）的微调更加参数高效。DoRA在通过自适应参数排名和领域意识权重调整优化微调的过程中进一步提升了过程，同时保持了推理性能，提高了学习效率。\n\n本论文对RAG、LoRA和DoRA进行了大规模的实证评估，通过20,000个基于FAQ的查询对模型微调和生成性能进行了评估，知识库包含400,000条记录。研究分析了关键性能指标，如准确度、相关性和推理延迟。实验结果表明，DoRA在准确度（90.1%）、相关性得分（0.88）和查询延迟（每查询110毫秒）方面均优于LoRA和RAG，在实际应用中的领域特定生成式AI应用中表现最佳。\n\n此外，本研究还探讨了不同模型在微调效率、计算成本和实时适应性之间的权衡。研究结果表明，RAG在知识落地方面效果显著，LoRA在成本效益的领域适应性方面表现出色，而DoRA则能够在微调效率和模型精度之间取得平衡。这些见解为在医疗、金融和法律服务等关键准确度领域部署以AI驱动的生成系统提供了实用指导，确保了在动态环境中实现可扩展性、可靠性和最佳性能。', 'title_zh': '幻觉与真相：RAG、LoRA和DoRA的整体准确度评估'}
{'arxiv_id': 'arXiv:2502.12149', 'title': 'HARBOR: Exploring Persona Dynamics in Multi-Agent Competition', 'authors': 'Kenan Jiang, Li Xiong, Fei Liu', 'link': 'https://arxiv.org/abs/2502.12149', 'abstract': "We investigate factors contributing to LLM agents' success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent's behavior in a competitive setting? (b) Can an agent effectively profile its competitors' behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.", 'abstract_zh': '我们研究了影响大规模语言模型（LLM）代理在竞争性多代理环境中的成功因素，以拍卖为实验平台，其中代理通过出价以最大化利润。代理配备了出价领域的知识、反映物品偏好的不同人设，以及拍卖历史的记忆。我们的工作扩展了经典的拍卖场景，创建了一个现实环境中，多个代理竞标房屋，综合考虑房屋大小、位置和预算等因素，以争取获得性价比最高的房屋。特别地，我们探讨了三个关键问题：（a）人设如何影响代理在竞争性环境中的行为？（b）代理能否在拍卖过程中有效分析对手的行为特征？（c）如何利用人设分析来构建优势，例如运用换位思考策略？通过一系列实验，我们分析了LLM代理的行为，并揭示了新的发现。我们的实验平台称为HARBOR，为深入理解竞争性环境中的多代理工作流程提供了有价值的平台。', 'title_zh': 'HARBOR：多代理竞赛中个性动态探索'}
{'arxiv_id': 'arXiv:2502.12120', 'title': 'LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws', 'authors': 'Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel', 'link': 'https://arxiv.org/abs/2502.12120', 'abstract': 'Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.', 'abstract_zh': '规模律通过提供模型大小、令牌和计算之间最优平衡的估算，指导大型语言模型（LLMs）的发展。最近，损失到损失的规模律作为一种强大的工具，其通过将预训练数据集和下游任务中的损失联系起来，有助于理解并提升LLM的性能。在本文中，我们探讨了哪些因素最强烈地影响损失到损失的规模律。我们的实验表明，预训练数据和分词器决定了这些规模律的趋势。相比之下，模型大小、优化超参数，甚至像Llama这样的变压器基模型和像Mamba这样的状态空间模型之间显著的架构差异，都对规模律的影响有限。因此，从业者应当仔细选择合适的预训练数据集以获得最佳的下游性能，而架构和其他设置则可以自由优化以提高训练效率。', 'title_zh': '《LLMs在线性中的影响：数据决定了损失到损失的标度律》\n\n在这个翻译中，“LLMs”通常指的是“大规模语言模型”（Large Language Models），虽然“LLMs on the Line”不一定是一个固定的术语，但这种翻译方式尽可能保持了原文的含义。“损失到损失的标度律”是对英文“Loss-to-Loss Scaling Laws”的学术化翻译，符合学术论文的表达习惯。如果有更多上下文信息，可能会有更精确的翻译。'}
{'arxiv_id': 'arXiv:2502.12119', 'title': 'PRISM: Self-Pruning Intrinsic Selection Method for Training-Free Multimodal Data Selection', 'authors': 'Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, Yunpu Ma', 'link': 'https://arxiv.org/abs/2502.12119', 'abstract': 'Visual instruction tuning refines pre-trained Multimodal Large Language Models (MLLMs) to enhance their real-world task performance. However, the rapid expansion of visual instruction datasets introduces significant data redundancy, leading to excessive computational costs. Existing data selection methods predominantly rely on proxy models or loss-based metrics, both of which impose substantial computational overheads due to the necessity of model inference and backpropagation. To address this challenge, we propose PRISM, a novel training-free approach for efficient multimodal data selection. Unlike existing methods, PRISM eliminates the reliance on proxy models, warm-up pretraining, and gradient-based optimization. Instead, it leverages Pearson correlation analysis to quantify the intrinsic visual encoding properties of MLLMs, computing a task-specific correlation score to identify high-value instances. This not only enbles data-efficient selection,but maintains the original performance. Empirical evaluations across multiple MLLMs demonstrate that PRISM reduces the overall time required for visual instruction tuning and data selection to just 30% of conventional methods, while surpassing fully fine-tuned models across eight multimodal and three language understanding benchmarks, achieving a 101.7% relative improvement in final performance.', 'abstract_zh': '视觉指令调优通过精炼预训练的多模态大型语言模型（MLLMs），以增强其实用任务性能。然而，视觉指令数据集的迅速扩展带来了显著的数据冗余，导致计算成本过高。现有的数据选择方法主要依赖于代理模型或基于损失的度量，这两种方法都因需要模型推理和反向传播而产生巨大的计算开销。为应对这一挑战，我们提出了一种名为PRISM的新型无训练数据选择方法，该方法旨在高效地进行多模态数据选择。与现有方法不同，PRISM去掉了对代理模型、预热训练和基于梯度的优化的依赖。相反，它利用皮尔森相关分析来量化MLLMs固有的视觉编码特性，计算出任务特定的相关性得分以识别高价值实例。这不仅实现了数据高效的筛选，还能维持原有的性能。在多项多模态和语言理解基准测试中的实证评估表明，PRISM将视觉指令调优和数据选择的整体所需时间减少了70%，同时在八个多模态和三个语言理解基准测试中超过完全微调的模型，最终性能相对提高了101.7%。', 'title_zh': 'PRISM: 自身修剪内在选择方法，用于无训练多模态数据选择'}
{'arxiv_id': 'arXiv:2502.11886', 'title': 'LIMR: Less is More for RL Scaling', 'authors': 'Xuefeng Li, Haoyang Zou, Pengfei Liu', 'link': 'https://arxiv.org/abs/2502.11886', 'abstract': "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at this https URL.", 'abstract_zh': '在本文中，我们探讨了什么因素真正决定了强化学习（RL）训练数据对提升语言模型推理能力的有效性。尽管最近的进步，例如 o1、DeepSeek R1 和 Kimi1.5 显示出 RL 的潜力，但关于训练数据需求的不透明性已阻碍了系统的进展。从基础模型直接开始训练，我们挑战了扩大 RL 训练数据规模会自然提高性能的假设。我们发现，仅战略性选择的1,389个样本子集就能优于包含8,523个样本的完整数据集。我们提出了学习影响测量（LIM），这是一种自动化方法，可以根据样本与模型学习轨迹的对齐程度来评估和优先级排序训练样本，从而实现高效的资源利用和可扩展的实施。我们的方法仅使用1,389个样本就能达到与完整8,523个样本数据集相当甚至更优的性能。值得注意的是，虽然最近的数据高效方法（例如 LIMO 和 s1）在32B规模的模型上显示出潜力，但我们在7B规模的模型上通过监督微调（SFT）发现这些方法表现显著不佳。相比之下，我们的基于 RL 的 LIMR 在 AIME24 上取得了16.7%的更高准确率，并在 MATH500 上分别超过了 LIMO 和 s1 13.0% 和 22.2%。这些结果从根本上重塑了我们对 LLM 中 RL 规模的理解，表明精确的样本选择，而不是数据量，可能是解锁增强推理能力的关键。为了促进可重复研究和未来创新，我们开源了 LIMR，包括 LIM 的实现、训练和评估代码、精心整理的数据集以及训练模型（请参见此 https URL）。', 'title_zh': 'LIMR：少即是多的强化学习扩展方法'}
{'arxiv_id': 'arXiv:2502.12118', 'title': 'Scaling Test-Time Compute Without Verification or RL is Suboptimal', 'authors': 'Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar', 'link': 'https://arxiv.org/abs/2502.12118', 'abstract': 'Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.', 'abstract_zh': '尽管在测试时计算扩展方面取得了实质性的进步，社区中仍存在关于如何扩展的持续争论，以便能够持续并有效地进行扩展。主要存在两种方法：首先，提取成功的搜索或思考轨迹；其次，利用验证（例如0/1结果奖励、奖励模型或验证器）来引导强化学习（RL）和搜索算法。本文证明，在给定固定的计算/数据预算条件下，基于强化学习或搜索的验证器基（VB）方法对语言模型（LLM）进行微调远优于基于提取或克隆搜索轨迹的验证器自由（VF）方法。进一步地，我们展示了当基预训练语言模型对正确的解题轨迹呈现异质分布（例如不同长度、风格等），并且允许从其采样的轨迹具有非尖锐的奖励分布时，随着测试时计算（以输出令牌长度衡量）和训练数据的扩展，VF方法的次优性相对于VB方法扩展得更差。我们使用反集中性[埃德勒斯，1945]来正式化这一条件。这表明，在理论上，VB方法在渐进扩展性方面表现更好，随着测试时预算的增长，VB方法与VF方法之间的性能差距会不断扩大。我们通过使用3/8/32B规模的预训练语言模型对教学和数学推理问题进行实证验证，证明了验证对于扩展测试时计算的重要性。', 'title_zh': '不进行验证或强化学习的测试时计算缩放是次优的'}
{'arxiv_id': 'arXiv:2502.11882', 'title': 'Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration', 'authors': 'Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen', 'link': 'https://arxiv.org/abs/2502.11882', 'abstract': "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in this https URL.", 'abstract_zh': '基于大型语言模型（LLMs）的代理在人机逐轮合作任务中表现出色，但在需要实时交互的多项实时任务中则遇到困难。延迟问题和推断人类策略变化的挑战阻碍了它们在没有明确指令的情况下做出自主决策的能力。通过使用当前独立的System 1和System 2方法进行实验，我们验证了在实时任务中使用双重过程理论（DPT）的必要性。我们提出了一种名为DPT-Agent的新语言代理框架，该框架结合了System 1和System 2，以实现高效的实时多项人机协作。DPT-Agent的System 1使用有限状态机（FSM）和代码作为策略，以实现快速、直观和可控的决策。DPT-Agent的System 2结合了心理理论（ToM）和异步反思，以推断人类意图并执行基于推理的自主决策。我们通过进一步与基于规则的代理和人类合作者进行实验，展示了DPT-Agent的有效性，表明其在主流LLM基础框架中具有显著改进。据我们所知，DPT-Agent是第一个能够在实时多重人机协作中实现自主成功的语言代理框架。DPT-Agent的代码可以在以下链接找到：https://...', 'title_zh': '利用双过程理论在语言代理框架中促进实时人机同步协作'}
{'arxiv_id': 'arXiv:2502.12094', 'title': 'A Study on Leveraging Search and Self-Feedback for Agent Reasoning', 'authors': 'Karthikeyan K, Michelle Yuan, Elman Mansimov, Katerina Margatina, Anurag Pratik, Daniele Bonadiman, Monica Sunkara, Yi Zhang, Yassine Benajiba', 'link': 'https://arxiv.org/abs/2502.12094', 'abstract': "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model's own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model's self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.", 'abstract_zh': '近期的研究表明，在推理过程中融入搜索可以显著提高语言代理的推理能力。一些方法可能利用真实答案或依赖模型生成的反馈。搜索算法利用这种反馈来生成更新其探索和利用各种推理路径的标准值。在本研究中，我们探讨了如何利用搜索和模型的自我反馈来完成推理任务。首先，我们探讨了在数学推理中利用真实反馈和自我反馈之间的差异。其次，我们观察了将搜索技术应用于更复杂任务（如工具调用和设计领域）时所遇到的限制，并针对这些差距提出了特定领域的解决方案。我们的实验揭示了仅依赖自我反馈进行搜索时存在的泛化挑战。为了使搜索有效，要么需要访问真实答案，要么需要为特定任务精心设计反馈机制。', 'title_zh': '利用搜索和自我反馈提升智能体推理能力的研究'}
{'arxiv_id': 'arXiv:2502.11881', 'title': 'Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models', 'authors': 'Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi', 'link': 'https://arxiv.org/abs/2502.11881', 'abstract': "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.", 'abstract_zh': '现有的大规模语言模型（LLM）推理方法在各种任务中表现出了令人印象深刻的性能，例如解决数学和编码问题。然而，将这些方法应用于缺乏正确答案或基于规则验证方法的情境中——比如追踪代理的心理状态——仍然颇具挑战性。受顺序蒙特卡洛算法的启发，我们提出了思维追踪（Thought-Tracking）这一推理算法，该算法在推理过程中生成关于特定代理的心理状态的假设，并根据观察结果对其进行加权，而不依赖于数据集中问题的正确答案。该算法基于贝叶斯心智理论框架，利用LLM来根据代理人感知和行为的演变进行概率推理。我们在多种心智理论基准测试中评估了思维追踪的效果，相比基线LLM显示出显著的性能提升。我们的实验还揭示了最近的推理模型——例如o1和R1——在心智理论上的行为特点，突显了社会推理与其它领域之间的差异。', 'title_zh': '基于假设的理论共情推理方法在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.12085', 'title': 'APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs', 'authors': 'Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Sun Ao, Hao Zhou, Jie Zhou, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.12085', 'abstract': 'While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in this https URL.', 'abstract_zh': '尽管长上下文推理对于推进大规模语言模型（LLM）的应用至关重要，其前置填充速度仍然是一个显著的瓶颈。当前的方法，包括序列并行策略和通过近似注意力机制减少计算量，仍然无法达到最佳推理效率。这阻碍了对更长输入序列的扩展，并且无法及时处理长上下文查询。为了解决这一问题，我们引入了APB，这是一种高效长上下文推理框架，通过利用多主机近似注意力机制同时减少计算量和增强并行性来提高前置填充速度。APB引入了一种在序列并行框架中通信关键值对的机制，从而能够在保持任务性能的同时提高推理速度。我们通过结合定制化的FlashAttn内核和优化的分布策略实现了APB，支持多样化的模型和并行配置。与FlashAttn、RingAttn和StarAttn相比，APB分别实现了高达9.2倍、4.2倍和1.6倍的加速，且没有任何可观察的任务性能下降。我们在此提供了APB的实施和实验代码：[提供链接的URL]', 'title_zh': 'APB：通过在GPU间传递压缩上下文块加速分布式长上下文推断'}
{'arxiv_id': 'arXiv:2502.12081', 'title': 'Unhackable Temporal Rewarding for Scalable Video MLLMs', 'authors': 'En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao', 'link': 'https://arxiv.org/abs/2502.12081', 'abstract': 'In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the "anti-scaling law", where more data and larger models lead to worse performance. This study unmasks the culprit: "temporal hacking", a phenomenon where models shortcut by fixating on select frames, missing the full video narrative. In this work, we systematically establish a comprehensive theory of temporal hacking, defining it from a reinforcement learning perspective, introducing the Temporal Perplexity (TPL) score to assess this misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework to mitigate the temporal hacking. Both theoretically and empirically, TPL proves to be a reliable indicator of temporal modeling quality, correlating strongly with frame activation patterns. Extensive experiments reveal that UTR not only counters temporal hacking but significantly elevates video comprehension capabilities. This work not only advances video-AI systems but also illuminates the critical importance of aligning proxy rewards with true objectives in MLLM development.', 'abstract_zh': '在追求卓越的视频处理大规模机器学习模型（MLLMs）的过程中，我们遭遇了一个令人费解的悖论：“反缩放定律”，即更多的数据和更大的模型反而导致性能下降。本研究揭开了这一现象的根源：“时间作弊”现象，这是模型通过仅关注某些帧而忽视完整视频叙事所导致的捷径。在此项研究中，我们系统地建立了一个全面的时间作弊理论，从强化学习的角度对其进行定义，并引入了时间困惑度（Temporal Perplexity, TPL）得分来评估这种不匹配，同时提出了不可作弊的时间奖励（Unhackable Temporal Rewarding, UTR）框架以减轻时间作弊现象。从理论上和实证上来看，TPL 是衡量时间建模质量的一个可靠的指标，与帧激活模式有很强的相关性。广泛的实验表明，UTR 不仅能够对抗时间作弊，还能显著提升视频理解能力。这项工作不仅推进了视频AI系统的发展，还揭示了在MLLM开发中将代理奖励与真实目标对齐的重要性。', 'title_zh': '不可破解的时序奖励以实现可扩展的视频MLLMs'}
{'arxiv_id': 'arXiv:2502.11880', 'title': 'Bitnet.cpp: Efficient Edge Inference for Ternary LLMs', 'authors': 'Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei', 'link': 'https://arxiv.org/abs/2502.11880', 'abstract': 'The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce this http URL, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, this http URL incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that this http URL achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. this http URL is publicly available at this https URL , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.', 'abstract_zh': '1-bit大型语言模型（Large Language Models, LLMs）的出现，尤其是由BitNet b1.58引领的发展，激发了对三值LLMs研究的兴趣。尽管如此，针对三值LLMs在边缘端高效推理的研究和实际应用仍然相对较少。为了填补这一空白，我们介绍了一个针对BitNet b1.58和三值LLMs优化的推理系统——[系统名称]。考虑到在三值LLMs中混合精度矩阵乘法（mpGEMM）占据了大部分推理时间，该系统集成了一个新颖的mpGEMM库，以支持亚2比特/权重的低比特高效且无损推理。该库包含两个核心解决方案：三值查找表（TL），用于解决之前比特级方法的空间效率问题；以及带有标度的整数2（I2_S），确保边缘端的无损推理，从而实现高速推理。我们的实验结果显示，该系统相对于全精度基准提高了6.25倍的速度，相比于低比特基准提高了2.32倍，从而在该领域设立了新的基准。此外，我们在附录中将TL扩展应用于低比特LLMs的元素级查找表（ELUT），并提供了理论和实验证据，证明其显著潜力。该系统已在[网址]公开发布，提供了一种高效的边缘端LLMs部署解决方案。', 'title_zh': 'Bitnet.cpp：高效三元大模型边缘推理算法'}
{'arxiv_id': 'arXiv:2502.12025', 'title': 'SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities', 'authors': 'Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12025', 'abstract': 'Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.', 'abstract_zh': '新兴的大规模推理模型（LRMs），如DeepSeek-R1模型，通过采用长链推理（CoT）来生成结构化的中间步骤，从而增强其推理能力。然而，长的CoT并不天然保证输出的安全性，可能会导致诸如代码中的安全漏洞或虚假信息的传播等危害性后果。目前关于大规模语言模型（LLMs）安全性方面的研究通常集中在短答案响应上，忽略了LRMs的长CoT风格输出。为填补这一空白，我们对LRM安全性进行了系统性研究。首先，我们研究了与人工注释校准的安全评估器。我们使用新开发的指标，对12个最新的LRMs在StrongReject和WildJailbreak数据集上的安全性进行了全面评估。结果显示，LRMs的推理能力并不安全。进一步地，我们对推理轨迹和最终答案进行了细致分析。我们发现，三种解码策略——NoThink、LessThink和MoreThink——可以在不增加训练的情况下提高模型安全性。然而，这些策略要么受限于推理轨迹的约束，要么会增加推理成本。为了更好地增强LRM的安全性，我们引入了SafeChain，这是首个基于CoT风格的安全训练数据集。我们对SafeChain进行了微调，结果显示，它不仅增强了模型安全性，还在6个推理基准测试中保持了性能。', 'title_zh': 'SafeChain: 语言模型的长链推理安全性'}
{'arxiv_id': 'arXiv:2502.11859', 'title': "Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics", 'authors': 'Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li', 'link': 'https://arxiv.org/abs/2502.11859', 'abstract': "The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.", 'abstract_zh': '多元智能理论强调认知能力的层次结构。为推进空间人工智能，我们提出了一种心理测量框架，定义了视觉语言模型（VLM）中的五种基本空间能力（BSAs）：空间感知、空间关系、空间定向、心理旋转和空间可视化。通过九项经过验证的心理测量实验对13种主流VLM进行基准测试，结果显示它们与人类之间的显著差距（人类平均得分为68.38，VLM平均得分为24.95），主要发现包括：1) VLMs呈现出与人类相似的能力层次结构（二维定向最强，三维旋转最弱），且独立的BSAs相关性低于0.4（皮尔森相关系数）；2) 较小型的模型如Qwen2-VL-7B超越了其较大模型的对应物，Qwen得分最高（30.82），而InternVL2得分最低（19.6）；3) 如思维链（chain-of-thought，0.100准确率提升）和五次训练（5-shot训练，0.259性能提升）等干预措施受到架构限制的限制。识别出的障碍包括对几何编码的弱处理和缺乏动态模拟。通过将心理测量BSAs与VLM能力联系起来，我们提供了一套空间智能评估的诊断工具，以及基于认知科学的发展方法论基础，为实现类人类的空间智能指明了认知科学导向的路线图。', 'title_zh': '从心理测量学视角定义和评估视觉语言模型的基本空间能力'}
{'arxiv_id': 'arXiv:2502.11919', 'title': 'From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis', 'authors': 'Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ziang Xiao, Ming Yin', 'link': 'https://arxiv.org/abs/2502.11919', 'abstract': "AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not %understand reflect on AI's decision recommendations critically. Large language models (LLMs), with their exceptional conversational and analytical capabilities, present great opportunities to enhance AI-assisted decision making in the absence of AI explanations by providing natural-language-based analysis of AI's decision recommendation, e.g., how each feature of a decision making task might contribute to the AI recommendation. In this paper, via a randomized experiment, we first show that presenting LLM-powered analysis of each task feature, either sequentially or concurrently, does not significantly improve people's AI-assisted decision performance. To enable decision makers to better leverage LLM-powered analysis, we then propose an algorithmic framework to characterize the effects of LLM-powered analysis on human decisions and dynamically decide which analysis to present. Our evaluation with human subjects shows that this approach effectively improves decision makers' appropriate reliance on AI in AI-assisted decision making.", 'abstract_zh': 'AI辅助决策日益普及，但个体常常未能恰当利用基于AI的决策辅助工具，尤其是在缺乏AI解释的情况下，这可能是因为他们未能批判性地反思AI的决策建议。大型语言模型（LLMs）凭借其出色的对话和分析能力，为在缺少AI解释的情况下提升AI辅助决策提供了巨大机会。通过提供自然语言分析，LLMs可以解释AI决策建议，例如，如何分析每个决策任务特征对AI建议的影响。在本论文中，通过随机实验，我们首先展示了以序时或并行方式呈现由LLM支持的每个任务特征的分析，并未显著提高人们在AI辅助决策中的表现。为了使决策者更好地利用LLM支持的分析，我们随后提出了一种算法框架来表征LLM支持的分析对人类决策的影响，并动态决定呈现哪种分析。我们的人类受试者评估表明，这种方法有效地提高了决策者在AI辅助决策中对AI的适当依赖。', 'title_zh': '从文本到信任：以适应性大模型助力的AI辅助决策分析'}
{'arxiv_id': 'arXiv:2502.11799', 'title': 'Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning', 'authors': 'Peiying Yu, Guoxin Chen, Jingjing Wang', 'link': 'https://arxiv.org/abs/2502.11799', 'abstract': 'Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.', 'abstract_zh': '尽管大规模语言模型（LLMs）在各种推理任务中表现出色，但在处理表格推理任务时，尤其是在保持多步推理过程中的连贯性时，它们仍然存在困难。尽管现有方法已经探索了各种分解策略，但它们往往缺乏有效机制来识别和纠正中间推理步骤中的错误，从而导致错误的级联传播。为了解决这些问题，我们提出了一种名为Table-Critic的新型多代理框架，该框架通过协作批评和迭代细化推理过程直至收敛以获得正确解。我们的框架包括四个专门的代理：一个法官（Judge）用于错误识别，一个评论员（Critic）用于全面批评，一个精炼者（Refiner）用于过程改进，以及一个策展人（Curator）用于模式提炼。为了有效处理多样且不可预测的错误类型，我们引入了一种自演化模板树，该模板树通过经验驱动的学习系统地积累批判知识，并指导未来的反思。大量的实验结果表明，Table-Critic在多个方面超过了现有方法，实现了更高的准确性和更佳的错误修正率，同时保持了较高的计算效率和较低的解退化率。', 'title_zh': 'Table-Critic：一种用于表推理协作批评与修正的多代理框架'}
{'arxiv_id': 'arXiv:2502.11886', 'title': 'LIMR: Less is More for RL Scaling', 'authors': 'Xuefeng Li, Haoyang Zou, Pengfei Liu', 'link': 'https://arxiv.org/abs/2502.11886', 'abstract': "In this paper, we ask: what truly determines the effectiveness of RL training data for enhancing language models' reasoning capabilities? While recent advances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack of transparency about training data requirements has hindered systematic progress. Starting directly from base models without distillation, we challenge the assumption that scaling up RL training data inherently improves performance. we demonstrate that a strategically selected subset of just 1,389 samples can outperform the full 8,523-sample dataset. We introduce Learning Impact Measurement (LIM), an automated method to evaluate and prioritize training samples based on their alignment with model learning trajectories, enabling efficient resource utilization and scalable implementation. Our method achieves comparable or even superior performance using only 1,389 samples versus the full 8,523 samples dataset. Notably, while recent data-efficient approaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it significantly underperforms at 7B-scale through supervised fine-tuning (SFT). In contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and outperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results fundamentally reshape our understanding of RL scaling in LLMs, demonstrating that precise sample selection, rather than data scale, may be the key to unlocking enhanced reasoning capabilities. For reproducible research and future innovation, we are open-sourcing LIMR, including implementation of LIM, training and evaluation code, curated datasets, and trained models at this https URL.", 'abstract_zh': '在这篇论文中，我们探讨了一个问题：真正决定强化学习（RL）训练数据对提升语言模型推理能力有效性的因素是什么？尽管像o1、Deepseek R1和Kimi1.5等最近的进步展示了RL的潜力，但关于训练数据需求缺乏透明度的问题阻碍了系统的进展。我们直接从基础模型出发，不依赖于知识蒸馏，挑战RL训练数据规模是否自然提升性能这一假设。我们证明，仅精选1,389个样本即可超越包含8,523个样本的整套数据集。我们引入了一种自动化方法——学习影响度量（LIM），该方法可以根据模型学习轨迹与样本的对齐程度来评估和优先选择训练样本，从而实现高效的资源利用和可扩展实施。我们使用1,389个样本的方法在性能上达到了与8,523个样本集合相当甚至更优的结果。值得注意的是，尽管最近的数据高效方法（如LIMO和s1）在32B规模的模型上显示出潜力，但在通过监督微调（SFT）进行的7B规模模型测试中表现显著不佳。而相比之下，我们的基于RL的LIMR方法在AIME24上的准确率提高了16.7%，并且在MATH500上的性能分别比LIMO和s1高出13.0%和22.2%。这些结果从根本上重塑了我们对LLMs中RL扩展的理解，表明精确的样本选择而非数据规模可能是解锁增强推理能力的关键。为了促进可再现研究和未来创新，我们公开发布了LIMR，其中包括LIM的实现、训练和评估代码、精选数据集以及训练模型，网址为this https URL。', 'title_zh': 'LIMR：少就是多，对于RL规模扩展而言'}
{'arxiv_id': 'arXiv:2502.11767', 'title': 'From Selection to Generation: A Survey of LLM-based Active Learning', 'authors': 'Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley', 'link': 'https://arxiv.org/abs/2502.11767', 'abstract': 'Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.', 'abstract_zh': '主动学习（AL）是一种通过选择最具信息价值的数据点进行标注和训练，从而提高模型效率和性能的强大范式。近年来，大型语言模型（LLMs）不仅在选择数据方面发挥作用，还在生成全新数据实例和提供更具成本效益的标注方面扮演重要角色。鉴于LLMs时代数据质量日益重要以及高效模型训练的需求，我们对基于LLMs的主动学习进行了全面综述。我们提出了一种直观的分类体系，将这些技术进行分类，并讨论LLMs在主动学习循环中可以发挥的变革性作用。进一步地，我们探讨了主动学习对LLMs学习范式的影响及其在各种领域的应用。最后，我们识别了存在的开放挑战，并提出了未来的研究方向。该综述旨在为研究人员和实践者提供一个及时的资源，帮助他们理解基于LLMs的主动学习技术，并将其应用于新的应用场景中。', 'title_zh': '从选择到生成：基于大规模语言模型的主动学习综述\n\n这里的“From Selection to Generation”是一个过程描述，意指从选择阶段过渡到生成阶段。“LLM-based Active Learning”则是基于大规模语言模型的主动学习方法。在翻译时，保持了原句结构的完整性，并确保学术用语的专业性和准确性。'}
{'arxiv_id': 'arXiv:2502.11882', 'title': 'Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration', 'authors': 'Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen', 'link': 'https://arxiv.org/abs/2502.11882', 'abstract': "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in this https URL.", 'abstract_zh': '基于大型语言模型（LLMs）的代理在人机逐步协作方面表现出色，但在需要实时互动的多项任务中则面临挑战。延迟问题和推断多变的人类策略的困难阻碍了它们在无明确指令的情况下做出自主决策的能力。通过使用当前独立的System 1和System 2方法进行实验，我们验证了在实时任务中使用双重过程理论（DPT）的必要性。我们提出了一种名为DPT-Agent的新颖语言代理框架，该框架结合了System 1和System 2，以实现高效的实时多项人机协作。DPT-Agent的System 1采用了有限状态机（FSM）和代码作为策略，以实现快速、直观和可控的决策。DPT-Agent的System 2结合了心智理论（ToM）和异步反思，以推断人类意图并进行基于推理的自主决策。通过与基于规则的代理和人类合作者进行进一步实验，我们证明了DPT-Agent的有效性，显示出显著优于主流的LLM基框架。据我们所知，DPT-Agent是第一个能够在无明确指令的情况下成功实现实时多项人机协作的语言代理框架。DPT-Agent的代码可在以下链接中找到：[插入链接]。', 'title_zh': '利用双过程理论在语言代理框架中实现实时人机协同合作'}
{'arxiv_id': 'arXiv:2502.11678', 'title': 'Exploring LLM-based Student Simulation for Metacognitive Cultivation', 'authors': 'Haoxuan Li, Jifan Yu, Xin Cong, Yang Dang, Yisi Zhan, Huiqin Liu, Zhiyuan Liu', 'link': 'https://arxiv.org/abs/2502.11678', 'abstract': "Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.", 'abstract_zh': '元认知教育在培养学生的自我调节能力和反思性思维中起到至关重要的作用，通过学业指导为学习困难的学生提供必要的支持。使用大语言模型模拟学习能力不足的学生，是一种有望在无伦理问题的情况下优化教学方法的方法。然而，现有的模拟往往无法真实地表现学生的学习困难，并且由于缺乏可靠的评价指标和数据收集中的伦理约束，评估面临挑战。为了解决这些问题，我们提出了一种自动生成和筛选高质量模拟学生代理的管道。我们的方法利用了由人类专家验证的两轮自动评分系统，并采用了评分传播模块以获得学生图中更一致的评分。实验结果表明，我们的管道能够有效地识别高质量的模拟学生代理，并讨论了影响模拟效果的关键特征。通过模拟不同程度学习困难的学生，我们的工作为个性化学习和教育评估的广泛应用奠定了基础。', 'title_zh': '探索基于大语言模型的学生模拟在元认知培养中的应用'}
{'arxiv_id': 'arXiv:2502.11881', 'title': 'Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models', 'authors': 'Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi', 'link': 'https://arxiv.org/abs/2502.11881', 'abstract': "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.", 'abstract_zh': '现有的大模型推理方法在各类任务中展现了令人印象深刻的性能，如解决数学和编程问题。然而，将这些方法应用于缺乏准确答案或基于规则验证方法的场景中——例如跟踪代理的心理状态——仍然具有挑战性。受顺序蒙特卡洛算法的启发，我们引入了思维跟踪（Thought-Tracking）——一种推理时的算法，旨在通过生成假设并基于观察结果对其进行加权，从而追踪特定代理的心理状态，而不依赖于数据集中问题的准确答案。该算法借鉴了贝叶斯心理理论框架，使用大模型来根据代理的感知和行为对心理状态的演变进行概率推理。我们在多种心理理论基准上评估了思维跟踪算法，相较于基线大模型，展示了显著的性能提升。我们的实验还揭示了最近的推理模型（例如o1和R1）在心理理论任务中的一些有趣行为，突出了社会推理与其它领域推理之间的差异。', 'title_zh': '基于假设的理论思维推理方法在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.11645', 'title': 'Deviation Ratings: A General, Clone-Invariant Rating Method', 'authors': 'Luke Marris, Siqi Liu, Ian Gemp, Georgios Piliouras, Marc Lanctot', 'link': 'https://arxiv.org/abs/2502.11645', 'abstract': "Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions. These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed ``clone invariant'' ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation.", 'abstract_zh': '许多实际世界的多智能体或多任务评估场景可以自然地用标准形式博弈进行建模，因为这些场景包含内在的战略（对抗性的、合作性的和混合动机）互动。这些战略互动可能是代理性的（例如，玩家试图获胜）、基本性的（例如，成本与质量之间的权衡），或互补性的（例如，专业特化和专长领域发现）。在此类模型中，是策略（行动、政策、代理、模型、任务、提示等）被评分的。然而，评分问题由于N玩家战略互动的冗余性和复杂性而变得更加复杂。重复的或相似的策略可能扭曲那些与之对抗或互补的策略的评分。先前的研究提出了一种“克隆不变”评分法来处理这种冗余性，但这种方法仅限于两人零和博弈（即严格竞争性）的互动。本研究在此基础上，首次提出了基于粗略关联均衡的N玩家一般和克隆不变评分法，称为偏差评分法，并在多个领域进行了探索，包括大型语言模型的评估。', 'title_zh': '偏差评分：一种通用且克隆无关的评估方法'}
{'arxiv_id': 'arXiv:2502.11880', 'title': 'Bitnet.cpp: Efficient Edge Inference for Ternary LLMs', 'authors': 'Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei', 'link': 'https://arxiv.org/abs/2502.11880', 'abstract': 'The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce this http URL, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, this http URL incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that this http URL achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. this http URL is publicly available at this https URL , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.', 'abstract_zh': '1-bit大型语言模型（LLMs）的出现，尤其是由BitNet b1.58引领的潮流，激发了对三值LLMs研究的兴趣。尽管如此，针对三值LLMs高效边缘推理的研究和实际应用仍然相对较少。为了填补这一差距，我们介绍了该系统（原文中的“this http URL”），该系统针对BitNet b1.58和三值LLMs进行了优化。鉴于混合精度矩阵乘法（mpGEMM）占三值LLMs推理时间的大头，该系统集成了一个新颖的mpGEMM库，以便实现每 weights 小于2比特的高效且无损推理。该库包含两个核心解决方案：Ternary Lookup Table（TTL），它解决了之前位级方法的空间效率问题；和带有缩放的Int2（I2_S），它确保了边缘推理的无损性，两者都实现了高速推理。我们的实验表明，该系统相对于全精度基线可以提高高达6.25倍的速度，相对于低比特基线可以提高高达2.32倍的速度，从而在该领域树立了新的标杆。此外，我们还在附录中将TTL扩展为元素级查找表（ELUT）以适用于低比特LLMs，并提出了其强大潜力的理论和实证证据。该系统（原文中的“this http URL”）在该https URL上公开，提供了一种先进的解决方案，以实现边缘LLMs的有效和实际部署。', 'title_zh': 'Bitnet.cpp: 良性边推断算法用于三元LLMs'}
{'arxiv_id': 'arXiv:2502.11554', 'title': 'Toward Metaphor-Fluid Conversation Design for Voice User Interfaces', 'authors': 'Smit Desai, Jessie Chin, Dakuo Wang, Benjamin Cowan, Michael Twidale', 'link': 'https://arxiv.org/abs/2502.11554', 'abstract': 'Metaphors play a critical role in shaping user experiences with Voice User Interfaces (VUIs), yet existing designs often rely on static, human-centric metaphors that fail to adapt to diverse contexts and user needs. This paper introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts metaphorical representations based on conversational use-contexts. We compare this approach to a Default VUI, which characterizes the present implementation of commercial VUIs commonly designed around the persona of an assistant, offering a uniform interaction style across contexts. In Study 1 (N=130), metaphors were mapped to four key use-contexts-commands, information seeking, sociality, and error recovery-along the dimensions of formality and hierarchy, revealing distinct preferences for task-specific metaphorical designs. Study 2 (N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and likability by aligning better with user expectations for different contexts. However, individual differences in metaphor preferences highlight the need for personalization. These findings challenge the one-size-fits-all paradigm of VUI design and demonstrate the potential of Metaphor-Fluid Design to create more adaptive and engaging human-AI interactions.', 'abstract_zh': '比喻在塑造语音用户界面（VUI）用户体验中起着关键作用，现有设计往往依赖于静态且以人为中心的比喻，这些比喻无法适应多样化的场景和用户需求。本文介绍了一种新型的方法——比喻流设计（Metaphor-Fluid Design），这种设计可以根据对话场景动态调整比喻性表示。我们将其与默认VUI进行比较，后者通常围绕助手角色进行设计，提供了一种在不同场景中一致的交互风格。研究1（参与人数N=130）将比喻映射到四个关键使用场景——命令、信息查询、社交性和错误恢复——并按形式性和层级性进行对比，揭示了针对特定任务的比喻性设计的不同偏好。研究2（参与人数N=91）评估了比喻流VUI与默认VUI的差异，结果显示，比喻流VUI通过更好地符合不同场景下的用户期望，提高了用户采用意愿、乐趣和好感度。然而，个人在比喻偏好上的差异突显了个性化的需求。这些发现挑战了VUI设计的一刀切模式，并展示了比喻流设计在创建更具适应性和互动性的用户-人工智能交互方面的潜力。', 'title_zh': '朝着面向元喻流动对话设计的声音用户界面方向'}
{'arxiv_id': 'arXiv:2502.11492', 'title': 'Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding', 'authors': 'Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu', 'link': 'https://arxiv.org/abs/2502.11492', 'abstract': "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.", 'abstract_zh': '视觉语言模型（VLMs）在多模态任务中取得了显著进展，但在视觉算术方面却常常表现不佳，诸如对象计数或长度比较等看似简单的任务对于复杂任务如图表理解和几何推理至关重要。在本文中，我们首先通过一系列针对基本视觉算术的探针任务，来探究这一缺陷的根本原因。我们的分析表明，尽管预训练的视觉编码器通常能够捕获足够的信息，但文本解码器常常无法正确地进行算术推理。为了解决这一问题，我们提出了一种名为CogAlign的新颖后训练策略，该策略灵感来源于皮亚杰的认知发展阶段理论。CogAlign旨在训练VLMs识别在视觉变换下的不变属性。我们演示了这种方法在我们提出的探针任务上显著提高了三种不同VLMs的表现。此外，CogAlign在CHOCOLATE上的性能平均提高了4.6%，在MATH-VISION上提高了2.9%，并且仅需减少60%的训练数据量就能超过或匹配监督微调方法。这些结果突显了CogAlign在增强基本视觉算术能力和将其转移至下游任务方面有效性和普适性的优势。', 'title_zh': '视觉语言模型为何在视觉算术任务中表现不佳？向着增强的图表和几何理解努力'}
{'arxiv_id': 'arXiv:2502.11482', 'title': 'DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning', 'authors': 'Huanxuan Liao, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11482', 'abstract': "Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the $\\textbf{D}$ecomposed $\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations.", 'abstract_zh': '持续学习（CL）对于大型语言模型（LLMs）适应不断变化的实际需求至关重要，但它们容易出现灾难性遗忘（CF）的问题。传统的CF解决方案依赖昂贵的数据复习，而最近的一些无复习方法则采用了基于模型和正则化策略来解决这一问题。然而，这些方法往往忽视了模型的可塑性，这是实现新任务最佳性能的关键。因此，CL中的一个主要挑战是如何在保留可塑性和减轻CF之间取得平衡。为了解决这一挑战，我们提出了一种基于分解注意力的任务适应（Decomposed Attention-based Task Adaptation，简称DATA）方法，该方法明确地拆分并学习了任务特定知识和任务共享知识（通过高秩和低秩任务适配器，例如LoRA实现）。对于新任务，DATA能够根据适配器与先前任务的相关性和区别动态调整不同秩次适配器的权重，使模型能够学习新任务的特定技能的同时有效保留先前学习的知识。具体而言，我们实现了一种分解组件加权策略，其中包含可学习的组件，它们共同生成注意力权重，使模型能够整合和利用每个DATA中的不同知识。在三个广泛使用的基准上的广泛实验表明，我们提出的办法取得了最先进的性能。特别值得注意的是，通过扩展可学习的组件并在训练迭代中使用随机恢复，我们的方法显著增强了模型的可塑性和减轻了CF。', 'title_zh': '数据：分解注意力机制的任务适应方法，用于无回顾持续学习'}
{'arxiv_id': 'arXiv:2502.11466', 'title': 'GiFT: Gibbs Fine-Tuning for Code Generation', 'authors': 'Haochen Li, Wanjin Feng, Xin Zhou, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2502.11466', 'abstract': 'Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks.', 'abstract_zh': '使用合成数据训练大型语言模型（LLMs）在代码生成中是一种普遍做法。一种关键方法是自训练，其中LLMs通过迭代训练自我生成的正确代码片段。在这种情况下，自我生成的代码是从某个特定种子描述条件化的条件分布中抽样得到的。然而，种子描述并不是唯一与其实际含义对齐的有效表示形式。所有有效的描述和代码共同构成了一个联合空间，从条件分布中抽取的代码会导致对完整描述-代码空间的欠表示。因此，我们提出了一种名为Gibbs Fine-Tuning（GiFT）的新颖自训练方法，该方法受到Gibbs采样的启发。GiFT使自生成的数据能够从联合空间的边际分布中抽取，从而减轻条件采样固有的偏差。我们提供了一个理论分析，证明了使用来自边际分布的代码微调LLMs的潜在益处。此外，我们提出了一种基于困惑度的代码选择方法，以减轻自我生成代码的不平衡长尾分布。跨四个数据集对两种LLMs进行的实证评估表明，GiFT在更具有挑战性的基准测试中表现出更优的性能。', 'title_zh': 'GiFT：基于吉布斯采样的微调方法用于代码生成'}
{'arxiv_id': 'arXiv:2502.11859', 'title': "Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics", 'authors': 'Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, Yong Li', 'link': 'https://arxiv.org/abs/2502.11859', 'abstract': "The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.", 'abstract_zh': '多元智能理论强调认知能力的层次结构。为了推进空间人工智能，我们首次提出一个心理测量框架，定义了视觉语言模型（VLMs）中的五个基本空间能力（BSAs）：空间知觉、空间关系、空间定向、心理旋转和空间可视化。通过九个验证性心理测量实验对13种主流VLMs进行基准测试，结果显示，VLMs在人类表现（平均得分为24.95分）面前存在显著差距（人类得分为68.38分），并有三个重要发现：1）VLMs反映人类的能力层次结构（二维定向最强，三维旋转最弱），且各BSAs独立（皮尔森相关系数<0.4）；2）较小的模型如Qwen2-VL-7B超过了较大的模型，Qwen得分最高（30.82分），而InternVL2得分最低（19.6分）；3）如利用思维链（0.100的准确率提升）和五轮训练（0.259的改进）等干预措施显示出架构限制的局限性。已识别的障碍包括几何编码较弱和缺乏动态模拟。通过将心理测量BSAs与VLM能力联系起来，我们提供了一套评估空间智能的诊断工具箱，为具身人工智能的发展提供了方法论基础，并为实现类似人类的空间智能提供了认知科学指导的道路。', 'title_zh': '从心理测量学视角定义和评估视觉语言模型的基本空间能力'}
{'arxiv_id': 'arXiv:2502.11442', 'title': 'Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding', 'authors': 'Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi', 'link': 'https://arxiv.org/abs/2502.11442', 'abstract': 'Conversational query clarification enables users to refine their search queries through interactive dialogue, improving search effectiveness. Traditional approaches rely on text-based clarifying questions, which often fail to capture complex user preferences, particularly those involving visual attributes. While recent work has explored single-turn multi-modal clarification with images alongside text, such methods do not fully support the progressive nature of user intent refinement over multiple turns. Motivated by this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task, which combines text and visual modalities to refine user queries in a multi-turn conversation. To facilitate this task, we create a large-scale dataset named ClariMM comprising over 13k multi-turn interactions and 33k question-answer pairs containing multi-modal clarifying questions. We propose Mario, a retrieval framework that employs a two-phase ranking strategy: initial retrieval with BM25, followed by a multi-modal generative re-ranking model that integrates textual and visual information from conversational history. Our experiments show that multi-turn multi-modal clarification outperforms uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are most significant in longer interactions, demonstrating the value of progressive refinement for complex queries.', 'abstract_zh': '对话式查询澄清可以通过互动对话让用户 refining 其搜索查询，从而提高搜索的有效性。传统的做法依赖于基于文本的澄清问题，这些问题往往无法捕捉到复杂的用户偏好，特别是涉及视觉属性的偏好。尽管近期的工作探讨了包含文本和图像的单轮多模态澄清方法，但这些方法没有完全支持用户意图逐步细化的多轮特性。为了应对这一挑战，我们提出了多轮多模态澄清问题（Multi-turn Multi-modal Clarifying Questions, MMCQ）任务，该任务结合了文本和视觉模态，在多轮对话中逐步细化用户查询。为了支持这一任务，我们创建了一个大规模数据集，名为 ClariMM，该数据集包含超过13,000轮对话交互和33,000个包含多模态澄清问题的问答对。\n\n我们提出了 Mario，一种检索框架，采用了两阶段排名策略：初始检索使用 BM25，随后通过多模态生成再排序模型，该模型整合了对话历史中的文本和视觉信息。实验结果表明，多轮多模态澄清相比单一模态和单轮方法，能够在 MRR 上取得12.88% 的提升。在较长的对话中，这些收益更为显著，这表明对复杂查询进行逐步细化的价值。\n\n这段翻译遵循了学术写作的规范，确保了内容的准确性和专业性。', 'title_zh': '多轮多模态问题澄清以增强对话理解'}
{'arxiv_id': 'arXiv:2502.11435', 'title': 'SMART: Self-Aware Agent for Tool Overuse Mitigation', 'authors': 'Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji', 'link': 'https://arxiv.org/abs/2502.11435', 'abstract': "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.", 'abstract_zh': '当前的大规模语言模型（LLM）代理展示了强烈的问题推理和工具使用能力，但往往缺乏自我意识，未能有效地平衡这两种方法。这种不平衡导致了工具过度使用，即模型在可使用参数化知识解决的任务中无必要地依赖外部工具，增加了计算成本。受人类元认知的启发，我们提出了SMART（Strategic Model-Aware Reasoning with Tools）范式，该范式提升了代理的自我意识，优化任务处理并减少工具过度使用。为了支持这一范式，我们引入了SMART-ER数据集，该数据集覆盖了三个领域，推理在参数化知识和工具依赖步骤之间交替，每个步骤都通过解释何时需要工具的说明来丰富。通过监督训练，我们开发了SMARTAgent模型家族，该家族能够动态平衡参数化知识和工具使用。评估显示，SMARTAgent将工具使用减少了24%，同时性能提升了超过37%，使得7B规模的模型能够达到其70B对应模型和GPT-4o的性能。此外，SMARTAgent在GSM8K和MINTQA等分布外测试数据中具有泛化能力，仅使用五分之一的工具调用就能保持准确性。这些结果突显了战略性工具使用在增强推理能力、缓解工具过度使用以及弥合模型规模与性能差距方面的潜力，推进了智能且资源高效的代理设计理念。', 'title_zh': 'SMART：自我感知代理工具过度使用缓解'}
{'arxiv_id': 'arXiv:2502.11799', 'title': 'Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning', 'authors': 'Peiying Yu, Guoxin Chen, Jingjing Wang', 'link': 'https://arxiv.org/abs/2502.11799', 'abstract': 'Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.', 'abstract_zh': '尽管大规模语言模型（LLMs）在各种推理任务上表现出色，但在处理表格推理任务时，特别是在保持多步骤推理过程中的一致性方面，它们仍存在困难。虽然现有方法探索了多种分解策略，但它们往往缺少有效机制来识别和修正中间推理步骤中的错误，导致错误传递。为了解决这些问题，我们提出了一种名为Table-Critic的新型多agent框架，该框架通过协作批评和迭代优化推理过程，直到收敛于正确解。该框架包含四个专门的代理：裁判（Judge）负责错误识别，批评者（Critic）负责综合批评，优化器（Refiner）负责过程改进，策展人（Curator）负责模式提炼。为了有效应对多样且不可预测的错误类型，我们引入了一种自演化模板树，该树通过经验驱动的学习系统地积累批评知识，并指导未来的反思。广泛实验表明，Table-Critic相对于现有方法取得了显著改进，实现了更高的准确率和错误修正率，同时保持了计算效率和较低的解退化率。', 'title_zh': 'Table-Critic：一种用于表格推理中协作批评与完善的大规模多智能体框架'}
{'arxiv_id': 'arXiv:2502.11767', 'title': 'From Selection to Generation: A Survey of LLM-based Active Learning', 'authors': 'Yu Xia, Subhojyoti Mukherjee, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joe Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao Kenneth Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian McAuley', 'link': 'https://arxiv.org/abs/2502.11767', 'abstract': 'Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.', 'abstract_zh': '主动学习（AL）是一种通过选择最有信息量的数据点进行标注和训练以提高模型效率和性能的强大范式。在最近的主动学习框架中，大型语言模型（LLMs）不仅用于选择数据点，还用于生成全新的数据实例，提供更经济有效的注释。鉴于LLM时代高质量数据和高效模型训练的重要性日益增加，我们对基于LLM的主动学习进行了全面综述。我们介绍了直观的分类体系，对这些技术进行了分类，并讨论了LLM在主动学习循环中的变革作用。我们进一步探讨了主动学习对LLM学习范式的影响及其在不同领域的应用。最后，我们确定了存在的挑战，并提出了未来的研究方向。本文旨在为研究人员和实践者提供一个及时的资源，帮助他们理解和应用基于LLM的主动学习技术。', 'title_zh': '从选择到生成：基于LLM的主动学习综述'}
{'arxiv_id': 'arXiv:2502.11678', 'title': 'Exploring LLM-based Student Simulation for Metacognitive Cultivation', 'authors': 'Haoxuan Li, Jifan Yu, Xin Cong, Yang Dang, Yisi Zhan, Huiqin Liu, Zhiyuan Liu', 'link': 'https://arxiv.org/abs/2502.11678', 'abstract': "Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.", 'abstract_zh': '元认知教育在培养学生的自我调节能力和反思思维方面发挥着关键作用，通过学术咨询为学习困难的学生提供必要的支持。使用大型语言模型模拟学习能力不足的学生，提供了一种在伦理问题不存在的情况下改进教学方法的有前景的方法。然而，现有的模拟往往无法真实地反映学生的学习困境，并且由于缺乏可靠的评价指标和数据收集中的伦理限制，难以进行有效的评估。为了解决这些问题，我们提出了一种自动生成和筛选高质量模拟学生代理的管线。我们的方法利用了由人类专家验证的两轮自动化评分系统，并采用评分传播模块以获得学生图中更一致的评分。实验结果表明，我们的管线能够有效地识别高质量的学生代理，并讨论了影响模拟效果的因素。通过模拟不同程度的学习困难学生，我们的工作为个性化学习和教育评估的应用扩展铺平了道路。', 'title_zh': '基于LLM的学生元认知培养模拟探索'}
{'arxiv_id': 'arXiv:2502.11645', 'title': 'Deviation Ratings: A General, Clone-Invariant Rating Method', 'authors': 'Luke Marris, Siqi Liu, Ian Gemp, Georgios Piliouras, Marc Lanctot', 'link': 'https://arxiv.org/abs/2502.11645', 'abstract': "Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions. These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed ``clone invariant'' ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation.", 'abstract_zh': '许多实际世界的多agent或多任务评估场景可以自然地被建模为标准形式博弈，因为它们固有的战略（对抗性、合作性和混合动机）交互。这些战略交互可能是代理性的（例如，玩家试图取胜），基础性的（例如，成本与质量），或者是互补性的（例如，利基发现和专门化）。在这样的表述中，是策略（动作、策略、agent、模型、任务、提示等）被评估的。然而，由于N-player策略交互的冗余性和复杂性，评级问题变得复杂化。重复或相似的策略可能会扭曲那些对抗或互补策略的评级。前人提出“克隆不变”评级来处理这种冗余性，但这仅限于二人零和博弈（即严格竞争性）的交互。本研究工作在此基础上引入了第一个N-player一般和博弈的克隆不变评级，称为偏离评级，基于粗略的关联均衡。该评级在包括LLM评估等多个领域进行了探索。', 'title_zh': '偏差评分：一种通用且克隆不变的评分方法'}
{'arxiv_id': 'arXiv:2502.11554', 'title': 'Toward Metaphor-Fluid Conversation Design for Voice User Interfaces', 'authors': 'Smit Desai, Jessie Chin, Dakuo Wang, Benjamin Cowan, Michael Twidale', 'link': 'https://arxiv.org/abs/2502.11554', 'abstract': 'Metaphors play a critical role in shaping user experiences with Voice User Interfaces (VUIs), yet existing designs often rely on static, human-centric metaphors that fail to adapt to diverse contexts and user needs. This paper introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts metaphorical representations based on conversational use-contexts. We compare this approach to a Default VUI, which characterizes the present implementation of commercial VUIs commonly designed around the persona of an assistant, offering a uniform interaction style across contexts. In Study 1 (N=130), metaphors were mapped to four key use-contexts-commands, information seeking, sociality, and error recovery-along the dimensions of formality and hierarchy, revealing distinct preferences for task-specific metaphorical designs. Study 2 (N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and likability by aligning better with user expectations for different contexts. However, individual differences in metaphor preferences highlight the need for personalization. These findings challenge the one-size-fits-all paradigm of VUI design and demonstrate the potential of Metaphor-Fluid Design to create more adaptive and engaging human-AI interactions.', 'abstract_zh': '以下是对原文内容的学术规范翻译：\n\n元喻在塑造语音用户界面（VUIs）用户体验方面发挥着关键作用，然而现有的设计往往依赖于静态的人类中心元喻，这些元喻无法适应多样化的使用情境和用户需求。本文提出了一种名为“元喻流形设计”的新颖方法，该方法根据对话式的使用情境动态调整元喻表示。我们将其与一种默认的VUI进行了比较，该默认VUI代表当前商业VUI的实施方式，通常以助手的人格设计为主，提供统一的交互风格适用于各种情境。在研究1（N=130）中，元喻被映射到四个关键使用情境——命令、信息查询、社交性和错误恢复——并在形式化和层级维度上揭示了特定任务元喻设计的不同偏好。研究2（N=91）对比了元喻流形VUI和默认VUI的表现，结果表明，元喻流形VUI通过更好地适应不同情境下的用户期望，提升了用户采用意向、愉悦感和喜爱度。然而，个体对元喻偏好的差异凸显了个性化的需求。这些发现挑战了一刀切的VUI设计范式，并展示了元喻流形设计在创造更具适应性和互动性的人机交互方面的潜力。', 'title_zh': '迈向元喻流动对话设计的语音用户界面'}
{'arxiv_id': 'arXiv:2502.11379', 'title': 'CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models', 'authors': 'Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.11379', 'abstract': 'Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.', 'abstract_zh': '尽管已经进行了明确的对齐努力，大型语言模型（LLMs）仍然可能被利用以触发意外行为，这一现象被称为“脱逃”（jailbreaking）。当前的脱逃攻击方法主要集中在针对封闭源代码LLMs的离散提示操纵上，依赖于手工构造的提示模板和说服规则。然而，随着开源LLMs能力的提升，确保其安全性变得日益重要。在这种环境下，潜在攻击者可以访问模型参数和梯度信息，这加剧了脱逃威胁的严重性。为了填补这一研究空白，我们提出了一个新颖的上下文相关脱逃攻击方法（CCJA）。我们将脱逃攻击定义为在掩码语言模型嵌入空间中的优化问题。通过组合优化，我们有效地平衡了脱逃攻击的成功率与语义一致性。 extensive评估显示，我们的方法不仅保持了语义一致性，还在攻击效果上超越了最先进的基线方法。此外，通过将我们方法生成的语义上一致的脱逃提示集成到广泛使用的黑盒方法中，我们发现当针对封闭源代码的商业LLMs时，其成功率显著提高。这突显了开源LLMs对商业对应物的安全威胁。如果论文被接受，我们将开源我们的代码。', 'title_zh': 'CCJA：上下文连贯的对齐大型语言模型破解攻击'}
{'arxiv_id': 'arXiv:2502.11492', 'title': 'Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding', 'authors': 'Kung-Hsiang Huang, Can Qin, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu', 'link': 'https://arxiv.org/abs/2502.11492', 'abstract': "Vision Language Models (VLMs) have achieved remarkable progress in multimodal tasks, yet they often struggle with visual arithmetic, seemingly simple capabilities like object counting or length comparison, which are essential for relevant complex tasks like chart understanding and geometric reasoning. In this work, we first investigate the root causes of this deficiency through a suite of probing tasks focusing on basic visual arithmetic. Our analysis reveals that while pre-trained vision encoders typically capture sufficient information, the text decoder often fails to decode it correctly for arithmetic reasoning. To address this, we propose CogAlign, a novel post-training strategy inspired by Piaget's theory of cognitive development. CogAlign trains VLMs to recognize invariant properties under visual transformations. We demonstrate that this approach significantly improves the performance of three diverse VLMs on our proposed probing tasks. Furthermore, CogAlign enhances performance by an average of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching supervised fine-tuning methods while requiring only 60% less training data. These results highlight the effectiveness and generalizability of CogAlign in improving fundamental visual arithmetic capabilities and their transfer to downstream tasks.", 'abstract_zh': '视觉语言模型（VLMs）在多模态任务中取得了显著进展，但在视觉算术任务上常常表现不佳，这包括对象计数或长度比较等看似简单的任务，而这些任务对于复杂的图表理解和几何推理等任务至关重要。在本文中，我们首先通过一系列旨在针对基本视觉算术的探针任务来探究这一缺陷的根本原因。我们的分析表明，尽管预训练的视觉编码器通常可以捕获足够多的信息，但文本解码器往往未能正确地进行算术推理。为此，我们提出了CogAlign，这是一种受皮亚杰认知发展理论启发的后训练策略。CogAlign旨在训练VLMs识别在视变换下的不变属性。我们展示了该方法在我们提出的探针任务中显著提升了三种不同VLMs的表现。此外，CogAlign在CHOCOLATE上的表现平均提高了4.6%，在MATH-VISION上提高了2.9%，仅需较少的训练数据（减少60%），就能超越或匹配监督微调方法。这些结果突显了CogAlign在提高基本视觉算术能力和向下游任务迁移中的有效性和通用性。', 'title_zh': '视觉语言模型在视觉算术任务上为何表现不佳？迈向增强的图表与几何理解'}
{'arxiv_id': 'arXiv:2502.11367', 'title': 'Sparse Autoencoder Features for Classifications and Transferability', 'authors': 'Jack Gallifant, Shan Chen, Kuleen Sasse, Hugo Aerts, Thomas Hartvigsen, Danielle S. Bitterman', 'link': 'https://arxiv.org/abs/2502.11367', 'abstract': 'Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: this https URL.', 'abstract_zh': '稀疏自编码器（SAEs）为在大规模语言模型（LLMs）中发现结构化且易于人类解读的表示提供了可能性，使其成为透明且可控AI系统的关键工具。我们系统地分析了在安全关键分类任务中从LLMs中提取可解释特征的SAE。我们的框架评估了以下方面：(1) 模型层选择和缩放特性，(2) SAE架构配置，包括宽度和下采样策略，以及(3) 连续SAE激活值二值化的效果。由SAE生成的特征达到了宏F1 > 0.8，优于隐藏状态和TF-IDF（BoW）基线，同时从Gemma 2 2B模型到9B-IT模型展示了跨模型迁移能力。这些特征以零样本的方式在跨语言毒性和视觉分类任务中得到了泛化。本分析强调了下采样策略和二值化阈值的显著影响，表明二值化提供了传统特征选择的高效替代方案，同时保持或提升了性能。这些发现确立了基于SAE的可解释性新的最佳实践标准，并使大规模、透明的LLM部署在实际应用中成为可能。完整代码库：[此处链接]。', 'title_zh': '稀疏自编码器特征在分类与迁移性中的应用'}
{'arxiv_id': 'arXiv:2502.11482', 'title': 'DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning', 'authors': 'Huanxuan Liao, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11482', 'abstract': "Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the $\\textbf{D}$ecomposed $\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations.", 'abstract_zh': '连续学习（CL）对于大型语言模型（LLMs）适应不断变化的实际需求至关重要，但它们容易出现灾难性遗忘（CF）的问题。传统的CF解决方案依赖于昂贵的数据复习，而近期的无复习方法则通过模型驱动和正则化驱动的策略来应对这一问题。然而，这些方法往往忽视了模型的可塑性，而这种可塑性对于在新任务中实现最佳性能至关重要。因此，在CL中面临的关键挑战是如何在保持可塑性和减轻CF之间取得平衡。为了解决这一挑战，我们提出了**分解注意力基于任务适应**（Decomposed Attention-based Task Adaptation, DATA），这种方法明确地分离并学习任务特定和任务共享的知识，使用高秩和低秩任务适配器（例如，LoRAs）来实现这一目标。对于新任务，DATA动态调整不同秩次适配器的权重，根据其与先前任务的相关性和差异，使模型能够在有效保留之前学习的知识的同时，获得新任务的特定技能。具体而言，我们实现了一种分解组件加权策略，包含可学习组件以共同生成基于注意力的权重，从而使模型能够整合和利用来自于每个DATA的多样化知识。在三个广泛使用的基准测试上的大量实验表明，我们提出的方法达到了最先进的性能。值得注意的是，我们的方法通过扩展可学习组件并在训练迭代中采用随机恢复，显著增强了模型的可塑性并减轻了CF。', 'title_zh': '数据：分解注意力机制的任务适应在无需回顾的连续学习中的应用'}
{'arxiv_id': 'arXiv:2502.11466', 'title': 'GiFT: Gibbs Fine-Tuning for Code Generation', 'authors': 'Haochen Li, Wanjin Feng, Xin Zhou, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2502.11466', 'abstract': 'Training Large Language Models (LLMs) with synthetic data is a prevalent practice in code generation. A key approach is self-training, where LLMs are iteratively trained on self-generated correct code snippets. In this case, the self-generated codes are drawn from a conditional distribution, conditioned on a specific seed description. However, the seed description is not the only valid representation that aligns with its intended meaning. With all valid descriptions and codes forming a joint space, codes drawn from the conditional distribution would lead to an underrepresentation of the full description-code space. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training method inspired by Gibbs sampling. GiFT allows self-generated data to be drawn from the marginal distribution of the joint space, thereby mitigating the biases inherent in conditional sampling. We provide a theoretical analysis demonstrating the potential benefits of fine-tuning LLMs with code derived from the marginal distribution. Furthermore, we propose a perplexity-based code selection method to mitigate the imbalanced long-tail distribution of the self-generated codes. Empirical evaluation of two LLMs across four datasets demonstrates that GiFT achieves superior performance, particularly on more challenging benchmarks.', 'abstract_zh': '使用合成数据训练大规模语言模型（LLMs）是代码生成中的一个普遍做法。一种关键方法是自训练，其中LLMs通过迭代训练自生成的正确代码片段。在这种情况下，自生成的代码是从特定种子描述条件化的条件分布中抽取的。然而，种子描述并非其预期含义的唯一有效表示。当所有有效描述和代码形成联合空间时，条件分布中抽取的代码会导致联合空间的表示不足。因此，我们提出了一种名为Gibbs Fine-Tuning（GiFT）的新颖自训练方法，该方法灵感来源于Gibbs抽样。GiFT使得自生成的数据可以从联合空间的边缘分布中抽取，从而减轻条件抽样的固有偏差。我们进行了理论分析，证明了使用来自边缘分布的代码微调LLMs的潜在好处。此外，我们提出了一种基于困惑度的代码选择方法，以减轻自生成代码不平衡的长尾分布。对两个LLMs在四个数据集上的实证评估显示，GiFT在更具挑战性的基准测试中实现了更优的性能。', 'title_zh': 'GiFT：基于吉布斯采样的 fine-tuning 方法在代码生成中的应用'}
{'arxiv_id': 'arXiv:2502.11360', 'title': 'GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder', 'authors': 'Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim', 'link': 'https://arxiv.org/abs/2502.11360', 'abstract': 'We introduce GeoDANO, a geometric vision-language model (VLM) with a domain-agnostic vision encoder, for solving plane geometry problems. Although VLMs have been employed for solving geometry problems, their ability to recognize geometric features remains insufficiently analyzed. To address this gap, we propose a benchmark that evaluates the recognition of visual geometric features, including primitives such as dots and lines, and relations such as orthogonality. Our preliminary study shows that vision encoders often used in general-purpose VLMs, e.g., OpenCLIP, fail to detect these features and struggle to generalize across domains. We develop GeoCLIP, a CLIP based model trained on synthetic geometric diagram-caption pairs to overcome the limitation. Benchmark results show that GeoCLIP outperforms existing vision encoders in recognizing geometric features. We then propose our VLM, GeoDANO, which augments GeoCLIP with a domain adaptation strategy for unseen diagram styles. GeoDANO outperforms specialized methods for plane geometry problems and GPT-4o on MathVerse.', 'abstract_zh': '我们介绍了GeoDANO，这是一种几何视觉语言模型（VLM），具有领域无关的视觉编码器，用于解决平面几何问题。尽管VLMs已被用于解决几何问题，但它们识别几何特征的能力仍然缺乏充分分析。为了解决这一差距，我们提出了一项基准测试，用来评估对视觉几何特征的识别能力，包括诸如点和线等基本特征，以及诸如正交性等关系特征。初步研究表明，广泛应用于通用VLMs的视觉编码器，如OpenCLIP，往往无法检测到这些特征，并且难以在不同领域间泛化。为此，我们开发了GeoCLIP，这是一种基于CLIP的模型，通过在合成几何图表配对数据上进行训练，以克服这一局限性。基准测试结果表明，GeoCLIP在识别几何特征方面优于现有的视觉编码器。随后，我们提出了我们的VLM GeoDANO，它通过领域适应策略来增强GeoCLIP，用于应对未见过的图表样式。GeoDANO在MathVerse上的表现优于针对平面几何问题的专业方法和GPT-4o。', 'title_zh': 'GeoDANO：几何导向的领域无关视觉编码器的层级视觉语言模型'}
{'arxiv_id': 'arXiv:2502.11356', 'title': 'SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models', 'authors': 'Zirui He, Haiyan Zhao, Yiran Qiao, Fan Yang, Ali Payani, Jing Ma, Mengnan Du', 'link': 'https://arxiv.org/abs/2502.11356', 'abstract': 'The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to interpret how instruction following works in these models. We demonstrate how the features we identify can effectively steer model outputs to align with given instructions. Through analysis of SAE latent activations, we identify specific latents responsible for instruction following behavior. Our findings reveal that instruction following capabilities are encoded by a distinct set of instruction-relevant SAE latents. These latents both show semantic proximity to relevant instructions and demonstrate causal effects on model behavior. Our research highlights several crucial factors for achieving effective steering performance: precise feature identification, the role of final layer, and optimal instruction positioning. Additionally, we demonstrate that our methodology scales effectively across SAEs and LLMs of varying sizes.', 'abstract_zh': '大型语言模型（LLM）遵循指令的能力对于其实际应用至关重要，但其背后的机制仍不完全清楚。本文提出了一种新的框架，利用稀疏自编码器（SAE）来解释这些模型中的指令遵循机制如何运作。我们展示了我们识别的特征如何有效地引导模型输出以与给定的指令对齐。通过分析SAE潜在激活，我们确定了特定的潜在变量负责指令遵循行为。我们的研究发现，指令遵循能力是由一组与指令相关的独特SAE潜在变量编码的。这些潜在变量与相关指令具有语义接近性，并且显示出对模型行为的因果效应。我们的研究突显了实现有效引导性能的几个关键因素：精确的特征识别、最终层的作用以及指令的最佳位置。此外，我们证明了我们的方法在不同大小的SAE和LLM中具有可扩展性。', 'title_zh': 'SAIF：一种稀疏自编码框架，用于解释和指导语言模型的指令跟随'}
{'arxiv_id': 'arXiv:2502.11442', 'title': 'Multi-Turn Multi-Modal Question Clarification for Enhanced Conversational Understanding', 'authors': 'Kimia Ramezan, Alireza Amiri Bavandpour, Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi', 'link': 'https://arxiv.org/abs/2502.11442', 'abstract': 'Conversational query clarification enables users to refine their search queries through interactive dialogue, improving search effectiveness. Traditional approaches rely on text-based clarifying questions, which often fail to capture complex user preferences, particularly those involving visual attributes. While recent work has explored single-turn multi-modal clarification with images alongside text, such methods do not fully support the progressive nature of user intent refinement over multiple turns. Motivated by this, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task, which combines text and visual modalities to refine user queries in a multi-turn conversation. To facilitate this task, we create a large-scale dataset named ClariMM comprising over 13k multi-turn interactions and 33k question-answer pairs containing multi-modal clarifying questions. We propose Mario, a retrieval framework that employs a two-phase ranking strategy: initial retrieval with BM25, followed by a multi-modal generative re-ranking model that integrates textual and visual information from conversational history. Our experiments show that multi-turn multi-modal clarification outperforms uni-modal and single-turn approaches, improving MRR by 12.88%. The gains are most significant in longer interactions, demonstrating the value of progressive refinement for complex queries.', 'abstract_zh': '对话查询澄清通过交互对话帮助用户细化搜索查询，从而提高搜索效果。传统方法依赖于基于文本的澄清问题，但往往无法捕捉到复杂用户偏好，尤其是涉及视觉属性的偏好。虽然近期的研究探讨了结合文本和图像的一轮多模态澄清方法，但这些方法未能全面支持用户意向在多轮次中的渐进细化。鉴于此，我们引入了多轮多模态澄清问题（MMCQ）任务，该任务结合了文本和视觉模态，在多轮对话中细化用户的查询。为了开展这一任务，我们创建了一个包含超过13,000个多轮互动和33,000个包含多模态澄清问题的问题-答案对的大规模数据集，命名为ClariMM。我们提出了Mario，一个检索框架，采用两阶段排名策略：初始阶段使用BM25进行检索，随后是一个多模态生成再排名模型，该模型整合了对话历史中的文本和视觉信息。实验结果显示，多轮多模态澄清方法优于单一模态和单轮方法，MRR提升了12.88%。特别是在较长的互动中，收益最为显著，证明了渐进细化对于复杂查询的价值。', 'title_zh': '增强对话理解的多轮多模态疑问澄清'}
{'arxiv_id': 'arXiv:2502.11435', 'title': 'SMART: Self-Aware Agent for Tool Overuse Mitigation', 'authors': 'Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji', 'link': 'https://arxiv.org/abs/2502.11435', 'abstract': "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.", 'abstract_zh': '当前的大型语言模型（LLM）代理表现出强大的推理和工具使用能力，但往往缺乏自我意识，无法有效平衡这些方法。这种不平衡导致了工具过度使用，即模型在可以通过参数化知识解决的任务中无必要地依赖外部工具，从而增加了计算开销。受到人类元认知的启发，我们引入了SMART（战略模型感知推理与工具使用）范式，该范式增强代理的自我意识，以优化任务处理并减少工具过度使用。为了支持这一范式，我们引入了SMART-ER数据集，该数据集跨越三个领域，在推理过程中交替使用参数化知识和工具依赖步骤，每一步都通过解释何时需要使用工具的原因使其更加丰富。通过监督训练，我们开发了SMART-Agent家族模型，能够动态平衡参数化知识和工具的使用。评估结果显示，SMART-Agent在减少工具使用24%的同时提高了超过37%的性能，使7B规模的模型能够与70B版本以及GPT-4o相媲美。此外，SMART-Agent能够泛化至分布外测试数据，如GSM8K和MINTQA，仅需平时五分之一的工具调用就能保持准确率。这些结果突显了战略性工具使用的潜力，可以增强推理能力、缓解过度使用问题，并缩小模型规模与性能之间的差距，从而推动更智能和资源高效代理的设计。', 'title_zh': 'SMART：自我感知代理工具过度使用缓解'}
{'arxiv_id': 'arXiv:2502.11308', 'title': 'ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation', 'authors': 'Yiyi Chen, Qiongkai Xu, Johannes Bjerva', 'link': 'https://arxiv.org/abs/2502.11308', 'abstract': 'With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that ALGEN attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against ALGEN, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.', 'abstract_zh': '随着大型语言模型（LLMs）和向量数据库的日益流行，私人文本数据越来越多地被转换为数值嵌入并进行处理和存储。然而，最近的研究表明，这样的嵌入容易受到反转攻击（inversion attacks），即通过重构原始文本来暴露敏感信息。以往的研究大多假定可以通过数据泄漏或近乎无限制的API访问获取数百万句法规则，以训练攻击模型。而我们的方法仅需一个数据点即可部分成功执行反转攻击。只要有1000个数据样本即可达到不同黑盒编码器的最佳性能，无需对泄露的数据进行训练。我们提出了一种基于对齐和生成的少样本文本嵌入反转攻击（ALGEN），方法是将受害者的嵌入对齐到攻击空间，并利用生成模型重构文本。我们发现ALGEN攻击可以在不同领域和语言之间有效转移，揭示了关键信息。我们进一步研究了多种针对ALGEN的防御机制，并发现没有任何一种防御机制有效，突显了反转攻击带来的弱点。通过显著降低反转攻击的成本，并证明可以通过一阶优化对嵌入空间进行对齐，我们建立了新的文本嵌入反转范式，为NLP中的嵌入对齐提供了更广泛的应用。', 'title_zh': 'ALGEN：基于对齐和生成的少样本文本嵌入反向攻击'}
{'arxiv_id': 'arXiv:2502.11379', 'title': 'CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models', 'authors': 'Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.11379', 'abstract': 'Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.', 'abstract_zh': '尽管对大型语言模型（LLMs）进行了明确的对齐努力，它们仍然可能被利用以触发意外行为，这种现象被称为“jailbreaking”。目前的jailbreak攻击方法主要集中在针对闭源LLMs的离散提示操纵上，依赖于手动编写的提示模板和说服规则。然而，随着开源LLMs能力的提高，确保其安全变得越来越重要。在这种环境中，潜在攻击者可以访问模型参数和梯度信息，这加剧了jailbreak威胁的严重性。为了解决这一研究空白，我们提出了一种新型的\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack（CCJA）。我们将jailbreak攻击定义为隐藏语言模型嵌入空间内的优化问题。通过组合优化，我们有效地平衡了jailbreak攻击的成功率与语义一致性。广泛的实际评估表明，我们的方法不仅保持了语义一致性，还在攻击效果上超过了最先进的基线方法。此外，通过将我们方法生成的语义一致的jailbreak提示与广泛使用的黑盒方法相结合，我们发现当针对闭源商用LLMs时，其成功率显著提升。这突显了开源LLMs对商用对应物构成的网络安全威胁。如果论文被接受，我们将开源我们的代码。', 'title_zh': 'CCJA：上下文一致的对齐大型语言模型 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2502.11304', 'title': 'Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring', 'authors': 'Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy', 'link': 'https://arxiv.org/abs/2502.11304', 'abstract': 'A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models.', 'abstract_zh': '智能城市和智能交通系统（ITS）中，一个稳健且高效的交通监控系统至关重要，它利用传感器和摄像头追踪车辆运动，优化交通流量，减少拥堵，提高道路安全，并实现实时自适应交通控制。交通监控模型必须全面理解动态城市状况，并提供直观的用户界面以实现有效的管理。本研究利用LLaVA多模态视觉定位大语言模型（LLM），在实时Quanser Interactive Lab仿真平台上进行交通监控任务，涵盖了交叉口、拥堵和碰撞等场景。位于多个城市位置的摄像头收集实时图像，并将其输入到LLaVA模型中进行分析。集成到摄像头中的实例分割模型突出显示关键元素，如车辆和行人，这有助于提高训练效率和吞吐量。该系统在识别车辆位置方面实现了84.3%的准确率，并在确定转向方向方面达到了76.4%的准确率，优于传统模型。', 'title_zh': '利用实例分割辅助的多模态大语言模型进行智能交通监控'}
{'arxiv_id': 'arXiv:2502.11367', 'title': 'Sparse Autoencoder Features for Classifications and Transferability', 'authors': 'Jack Gallifant, Shan Chen, Kuleen Sasse, Hugo Aerts, Thomas Hartvigsen, Danielle S. Bitterman', 'link': 'https://arxiv.org/abs/2502.11367', 'abstract': 'Sparse Autoencoders (SAEs) provide potentials for uncovering structured, human-interpretable representations in Large Language Models (LLMs), making them a crucial tool for transparent and controllable AI systems. We systematically analyze SAE for interpretable feature extraction from LLMs in safety-critical classification tasks. Our framework evaluates (1) model-layer selection and scaling properties, (2) SAE architectural configurations, including width and pooling strategies, and (3) the effect of binarizing continuous SAE activations. SAE-derived features achieve macro F1 > 0.8, outperforming hidden-state and BoW baselines while demonstrating cross-model transfer from Gemma 2 2B to 9B-IT models. These features generalize in a zero-shot manner to cross-lingual toxicity detection and visual classification tasks. Our analysis highlights the significant impact of pooling strategies and binarization thresholds, showing that binarization offers an efficient alternative to traditional feature selection while maintaining or improving performance. These findings establish new best practices for SAE-based interpretability and enable scalable, transparent deployment of LLMs in real-world applications. Full repo: this https URL.', 'abstract_zh': '稀疏自编码器（Sparse Autoencoders, SAEs）在大型语言模型（Large Language Models, LLMs）中具有潜力，能够揭示结构化且人可解释的表示形式，这使它们成为透明可控AI系统的重要工具。我们系统地分析了SAEs在安全关键分类任务中的可解释特征提取能力。我们的框架评估了以下方面：（1）模型层的选择和扩展性，（2）SAE架构配置，包括宽度和池化策略，以及（3）连续SAE激活值二值化的效果。SAE提取的特征达到宏F1值大于0.8，并优于隐藏状态和BoW基准模型，同时展示了从Gemma 2B到9B-IT模型的跨模型迁移。这些特征以零样本的方式在跨语言有害内容检测和视觉分类任务中得到推广。我们的分析突显了池化策略和二值化阈值的重要影响，表明二值化提供了一种与传统特征选择方法相比既高效又可保持或提升性能的选择。这些发现确立了基于SAE的可解释性新的最佳实践，并为LLMs在实际应用中的可扩展、透明部署奠定了基础。完整代码库：[此链接](this https URL)。', 'title_zh': '稀疏自编码特征在分类和迁移性中的应用'}
{'arxiv_id': 'arXiv:2502.11360', 'title': 'GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder', 'authors': 'Seunghyuk Cho, Zhenyue Qin, Yang Liu, Youngbin Choi, Seungbeom Lee, Dongwoo Kim', 'link': 'https://arxiv.org/abs/2502.11360', 'abstract': 'We introduce GeoDANO, a geometric vision-language model (VLM) with a domain-agnostic vision encoder, for solving plane geometry problems. Although VLMs have been employed for solving geometry problems, their ability to recognize geometric features remains insufficiently analyzed. To address this gap, we propose a benchmark that evaluates the recognition of visual geometric features, including primitives such as dots and lines, and relations such as orthogonality. Our preliminary study shows that vision encoders often used in general-purpose VLMs, e.g., OpenCLIP, fail to detect these features and struggle to generalize across domains. We develop GeoCLIP, a CLIP based model trained on synthetic geometric diagram-caption pairs to overcome the limitation. Benchmark results show that GeoCLIP outperforms existing vision encoders in recognizing geometric features. We then propose our VLM, GeoDANO, which augments GeoCLIP with a domain adaptation strategy for unseen diagram styles. GeoDANO outperforms specialized methods for plane geometry problems and GPT-4o on MathVerse.', 'abstract_zh': '我们介绍了GeoDANO，这是一种具有领域无关视觉编码器的几何视觉语言模型（VLM），用于解决平面几何问题。尽管视觉语言模型已经被用于解决几何问题，但它们在识别几何特征方面的能力尚未得到充分研究。为了解决这一问题，我们提出了一项基准测试，用于评估视觉几何特征的识别能力，包括点和线等基本几何元素，以及正交性等关系。初步研究表明，通用VLM中常用的视觉编码器，例如OpenCLIP，往往无法检测这些特征，并且难以在不同领域间泛化。我们开发了GeoCLIP，这是一种基于CLIP的模型，通过合成几何图表-描述对进行训练，以克服这一限制。基准测试结果显示，GeoCLIP在识别几何特征方面优于现有的视觉编码器。随后，我们提出了我们的VLM——GeoDANO，它结合了GeoCLIP的领域适应策略，以应对未见过的图表风格。GeoDANO在MathVerse上的表现优于专门用于平面几何问题的方法和GPT-4o。', 'title_zh': 'GeoDANO：具有领域无关视觉编码器的几何多模态语言模型'}
{'arxiv_id': 'arXiv:2502.11298', 'title': 'Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning', 'authors': 'Parisa Fard Moshiri, Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Emil Janulewicz', 'link': 'https://arxiv.org/abs/2502.11298', 'abstract': 'Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its reliance on structured inputs and predefined rules limits adaptability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network management. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low-Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time.', 'abstract_zh': '现代架构如软件定义网络（SDN）和网络功能虚拟化（NFV）中，高效的业务函数链（SFC）配置和服务功能虚拟机（VNF）放置对于提升网络性能至关重要。深度强化学习（DRL）在动态网络环境中有助于决策制定，但其依赖结构化输入和预定义规则的特性限制了其在未预见情景中的适应能力。此外，DRL代理的错误行为可能需要多次训练迭代来纠正，这有可能强化次优策略，从而降低性能。本文将DRL与语言模型（LMs）相结合，特别是双向变压器编码表示（BERT）和DistilBERT，以提升网络管理。通过将DRL的最终VNF分配结果输入到LM中，系统可以处理和响应与SFC、数据中心（DC）和VNF相关的查询，从而实现对资源利用、瓶颈检测和未来需求预测的实时洞察。我们使用低秩适应（LoRA）对LMs进行微调。结果表明，BERT在测试损失（0.28对比0.36）和置信度（0.83对比0.74）方面均优于DistilBERT，尽管BERT的处理时间大约比DistilBERT增加46%。', 'title_zh': '基于DRL的SFC提供中语言模型集成以增强网络状态监控'}
{'arxiv_id': 'arXiv:2502.11356', 'title': 'SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models', 'authors': 'Zirui He, Haiyan Zhao, Yiran Qiao, Fan Yang, Ali Payani, Jing Ma, Mengnan Du', 'link': 'https://arxiv.org/abs/2502.11356', 'abstract': 'The ability of large language models (LLMs) to follow instructions is crucial for their practical applications, yet the underlying mechanisms remain poorly understood. This paper presents a novel framework that leverages sparse autoencoders (SAE) to interpret how instruction following works in these models. We demonstrate how the features we identify can effectively steer model outputs to align with given instructions. Through analysis of SAE latent activations, we identify specific latents responsible for instruction following behavior. Our findings reveal that instruction following capabilities are encoded by a distinct set of instruction-relevant SAE latents. These latents both show semantic proximity to relevant instructions and demonstrate causal effects on model behavior. Our research highlights several crucial factors for achieving effective steering performance: precise feature identification, the role of final layer, and optimal instruction positioning. Additionally, we demonstrate that our methodology scales effectively across SAEs and LLMs of varying sizes.', 'abstract_zh': '大型语言模型（LLMs）遵循指令的能力对于其实际应用至关重要，但其背后的机制仍然缺乏充分的理解。本文提出了一种创新框架，利用稀疏自编码器（SAE）来解释这些模型中指令遵循机制的工作原理。我们展示了我们识别出的特征如何有效地引导模型输出与给定指令一致。通过对SAE潜在激活的分析，我们确定了负责指令遵循行为的具体潜在特征。我们的研究结果表明，指令遵循能力是由一组特定的、与指令相关联的SAE潜在特征编码的。这些潜在特征不仅在语义上接近相关的指令，而且在模型行为上表现出因果效应。我们的研究突显了实现有效引导性能的几个关键因素：精确的特征识别、最终层的作用以及指令的最佳定位。此外，我们还展示了我们的方法在不同大小的SAE和LLMs中具有有效的扩展性。', 'title_zh': 'SAIF：一种稀疏自编码框架，用于解释和引导语言模型的指令跟随'}
{'arxiv_id': 'arXiv:2502.11271', 'title': 'OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning', 'authors': 'Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, James Zou', 'link': 'https://arxiv.org/abs/2502.11271', 'abstract': "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.", 'abstract_zh': '解决复杂的推理任务可能涉及到视觉理解、领域知识检索、数值计算和多步骤推理。现有方法通过外部工具对大型语言模型（LLMs）进行增强，但这些方法局限于特定的应用领域、有限的工具类型，或者需要额外的训练数据。在本文中，我们介绍了一种无需训练、用户友好且易于扩展的开源代理框架OctoTools，该框架旨在跨多个领域解决复杂的推理问题。OctoTools 引入了标准化的工具卡片来封装工具功能，规划器用于进行高级和低级计划，以及执行器用于执行工具使用。我们验证了OctoTools 在16个不同任务（包括MathVista、MMLU-Pro、MedQA 和 GAIA-Text）上的通用性，相比GPT-4o实现了9.3%的平均准确性提升。此外，当提供相同的工具集时，OctoTools 在AutoGen、GPT-Functions 和 LangChain 上的表现提高了最多10.6%。通过全面的分析和消融实验，OctoTools 显示出在任务规划、有效工具使用和多步骤问题解决方面的优势。', 'title_zh': 'OctoTools：一种用于复杂推理的可扩展工具代理框架'}
{'arxiv_id': 'arXiv:2502.11308', 'title': 'ALGEN: Few-shot Inversion Attacks on Textual Embeddings using Alignment and Generation', 'authors': 'Yiyi Chen, Qiongkai Xu, Johannes Bjerva', 'link': 'https://arxiv.org/abs/2502.11308', 'abstract': 'With the growing popularity of Large Language Models (LLMs) and vector databases, private textual data is increasingly processed and stored as numerical embeddings. However, recent studies have proven that such embeddings are vulnerable to inversion attacks, where original text is reconstructed to reveal sensitive information. Previous research has largely assumed access to millions of sentences to train attack models, e.g., through data leakage or nearly unrestricted API access. With our method, a single data point is sufficient for a partially successful inversion attack. With as little as 1k data samples, performance reaches an optimum across a range of black-box encoders, without training on leaked data. We present a Few-shot Textual Embedding Inversion Attack using ALignment and GENeration (ALGEN), by aligning victim embeddings to the attack space and using a generative model to reconstruct text. We find that ALGEN attacks can be effectively transferred across domains and languages, revealing key information. We further examine a variety of defense mechanisms against ALGEN, and find that none are effective, highlighting the vulnerabilities posed by inversion attacks. By significantly lowering the cost of inversion and proving that embedding spaces can be aligned through one-step optimization, we establish a new textual embedding inversion paradigm with broader applications for embedding alignment in NLP.', 'abstract_zh': '随着大型语言模型（LLMs）和向量数据库的日益流行，私人文本数据越来越多地被处理并存储为数值嵌入。然而，最近的研究表明，这些嵌入很容易受到逆向攻击，即通过重建原始文本来泄露敏感信息。以往的研究主要假设可以获取数百万个句子来训练攻击模型，例如通过数据泄漏或近乎不受限制的API访问。而我们的方法仅需一个数据点即可实现部分成功的逆向攻击。即使仅有1000个数据样本，我们的方法也能在不同黑盒编码器上达到最佳性能，且无需使用泄露的数据进行训练。我们提出了一种利用对齐与生成（ALGEN）的少量样本文本嵌入逆向攻击方法，通过将受害者的嵌入对齐到攻击空间，并使用生成模型重构文本。我们发现，ALGEN攻击可在不同领域和语言之间有效转移，并揭示关键信息。我们还研究了多种针对ALGEN的防御机制，并发现它们均无效，凸显了逆向攻击带来的脆弱性。通过显著降低逆向攻击的成本，并证明嵌入空间可以通过单步优化实现对齐，我们建立了一种新的文本嵌入逆向攻击范式，该范式在NLP中的嵌入对齐应用更为广泛。', 'title_zh': 'ALGEN：基于对齐与生成的少量样本逆向攻击文本嵌入'}
{'arxiv_id': 'arXiv:2502.11304', 'title': 'Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring', 'authors': 'Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy', 'link': 'https://arxiv.org/abs/2502.11304', 'abstract': 'A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models.', 'abstract_zh': '智能城市和智能运输系统（ITS）中的鲁棒且高效的交通监控系统对于保障交通安全、优化交通流量、减少拥堵和实现实时自适应交通控制至关重要。交通监控系统必须全面理解动态城市条件，并提供直观的用户界面以有效管理交通。本研究利用LLaVA多模态大语言模型（LLM）在实时Quanser Interactive Lab仿真平台上进行交通监控任务，覆盖交叉口、拥堵和碰撞等场景。部署在多个城市位置的摄像头收集实时图像，通过查询输入LLaVA模型进行分析。摄像头内嵌的实例分割模型突出显示重要元素如车辆和行人，增强训练效果并提高处理速度。该系统在识别车辆位置方面达到了84.3%的准确率，在确定转向方向方面达到了76.4%的准确率，优于传统模型。', 'title_zh': '利用实例分割辅助的多模态大模型进行智能交通监控'}
{'arxiv_id': 'arXiv:2502.11298', 'title': 'Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning', 'authors': 'Parisa Fard Moshiri, Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Emil Janulewicz', 'link': 'https://arxiv.org/abs/2502.11298', 'abstract': 'Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its reliance on structured inputs and predefined rules limits adaptability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network management. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low-Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time.', 'abstract_zh': '高效的服务功能链（SFC）配置和服务功能虚拟化（VNF）放置对于增强现代软件定义网络（SDN）和网络功能虚拟化（NFV）架构中的网络性能至关重要。尽管深度强化学习（DRL）有助于动态网络环境中的决策制定，但它对结构化输入和预定义规则的依赖限制了其在未预见场景中的适应性。此外，DRL代理的错误行为可能需要多次训练迭代才能纠正，这可能导致劣化策略的强化并降低性能。本文将DRL与语言模型（LMs）相结合，特别是双向编码器表示（BERT）和DistilBERT，以提高网络管理质量。通过将DRL的最终VNF分配输入LM，系统可以处理与SFC、数据中心（DC）和VNF相关的查询，实现实时资源利用率洞察、瓶颈检测和未来需求规划。我们使用低秩适应（LoRA）对LMs进行了微调。结果显示，BERT在测试损失（0.28 vs 0.36）和置信度（0.83 vs 0.74）方面优于DistilBERT，尽管BERT的处理时间大约增加了46%。', 'title_zh': '基于DRL的SFC提供中增强网络状态监控的语言模型集成方法'}
{'arxiv_id': 'arXiv:2502.11267', 'title': 'Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent', 'authors': "Zeyu He, Saniya Naphade, Ting-Hao 'Kenneth' Huang", 'link': 'https://arxiv.org/abs/2502.11267', 'abstract': 'Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable-only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.', 'abstract_zh': '下面是这段内容或标题的中文翻译，符合学术规范：\n\n成千上万的用户引导大规模语言模型（LLMs）完成各种任务，但人们在提示工程方面做得如何？用户在多次迭代提示后是否能更接近其期望的结果？当没有黄金标准标签来衡量进展时，这些问题至关重要。本文探讨了在LLM助力的数据标注场景中“无标鉴标”的提示工程问题，即用户在不使用手动标注基准的情况下，迭代地引导LLMs对数据进行标注。我们开发了PromptingSheet，这是一个谷歌表格插件，使用户能够通过表格来构建、修订和迭代标注数据。通过涉及20名参与者的实验发现，在四次或更多次迭代后，只有9名参与者能够提高标注准确性。自动提示优化工具（如DSPy）在可用黄金标签较少的情况下也表现出色。我们的研究结果突显了黄金标签的重要性，并指出了自动支持在人类提示工程中的需要与风险，为未来工具的设计提供了洞察。', 'title_zh': '在黑暗中敲键：评估在缺乏黄金标准标签情况下提示工程对数据标注的人工绩效'}
{'arxiv_id': 'arXiv:2502.11256', 'title': 'Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View', 'authors': 'Yanran Wu, Inez Hua, Yi Ding', 'link': 'https://arxiv.org/abs/2502.11256', 'abstract': "Large language models (LLMs) offer powerful capabilities but come with significant environmental costs, particularly in carbon emissions. Existing studies benchmark these emissions but lack a standardized basis for comparison across models. To address this, we introduce the concept of a functional unit (FU) and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through case studies on model size, quantization, and hardware, we uncover key trade-offs in sustainability. Our findings highlight the potential for reducing carbon emissions by optimizing model selection, deployment strategies, and hardware choices, paving the way for more sustainable AI infrastructure.", 'abstract_zh': '大型语言模型（LLMs）提供了强大的能力，但同时也伴随着显著的环境成本，尤其是在碳排放方面。现有研究已经对这些排放进行了评估，但缺乏一个用于不同模型之间比较的标准基准。为了解决这一问题，我们引入了功能单元（FU）的概念，并开发了FUEL框架，这是基于FU的第一个用于评估LLM服务环境影响的方法。通过针对模型规模、量化和硬件的案例研究，我们揭示了可持续性的关键权衡。我们的研究结果强调了通过优化模型选择、部署策略和硬件选择来减少碳排放的潜力，从而铺平了走向更可持续的人工智能基础设施的道路。', 'title_zh': '揭示大型语言模型服务的环境影响：从功能单元视角出发'}
{'arxiv_id': 'arXiv:2502.11271', 'title': 'OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning', 'authors': 'Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, James Zou', 'link': 'https://arxiv.org/abs/2502.11271', 'abstract': "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.", 'abstract_zh': '解决复杂的推理任务可能涉及视觉理解、领域知识检索、数值计算和多步推理。现有方法通过外部工具增强大型语言模型（LLMs），但这些方法受到专业领域限制、工具类型有限或需要额外训练数据的约束。在本文中，我们介绍了一种名为OctoTools的训练-free、用户友好且易于扩展的开放源代码代理框架，旨在跨多种领域应对复杂的推理任务。OctoTools引入了标准化的工具卡片以封装工具功能、计划器用于高级和低级规划、以及执行器用于执行工具使用。我们通过16项不同的任务验证了OctoTools的通用性（包括MathVista、MMLU-Pro、MedQA和GAIA-Text），在平均准确率方面取得了9.3%的显著提升，超过了GPT-4o。此外，在相同工具集中，OctoTools在与AutoGen、GPT-Functions和LangChain的竞争中取得了高达10.6%的性能提升。通过全面的分析和消融实验，OctoTools展示了其在任务规划、有效工具使用和多步问题解决方面的优势。', 'title_zh': 'OctoTools：一种用于复杂推理的拓展性代理框架'}
{'arxiv_id': 'arXiv:2502.11246', 'title': 'MemeSense: An Adaptive In-Context Framework for Social Commonsense Driven Meme Moderation', 'authors': 'Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.11246', 'abstract': 'Memes present unique moderation challenges due to their subtle, multimodal interplay of images, text, and social context. Standard systems relying predominantly on explicit textual cues often overlook harmful content camouflaged by irony, symbolism, or cultural references. To address this gap, we introduce MemeSense, an adaptive in-context learning framework that fuses social commonsense reasoning with visually and semantically related reference examples. By encoding crucial task information into a learnable cognitive shift vector, MemeSense effectively balances lexical, visual, and ethical considerations, enabling precise yet context-aware meme intervention. Extensive evaluations on a curated set of implicitly harmful memes demonstrate that MemeSense substantially outperforms strong baselines, paving the way for safer online communities. Code and data available at: this https URL', 'abstract_zh': '模因由于其微妙的、多模态的图像、文字和社会背景的相互作用，提出了独特的管理挑战。传统系统主要依赖显式的文字线索，常常忽略被讽刺、象征或文化参考掩盖的有害内容。为了解决这一问题，我们引入了MemeSense，这是一个将社会常识推理与视觉和语义相关参考示例结合的自适应上下文学习框架。通过将关键任务信息编码进可学习的认知转移向量中，MemeSense能够有效地平衡词汇、视觉和伦理方面的考虑，从而实现精确且上下文相关的模因干预。通过对一个策划的具有潜在有害内容的模因集合进行广泛的评估，结果表明，MemeSense显著优于强基准模型，为更安全的在线社区铺平了道路。相关代码和数据可在以下链接获取：this https URL', 'title_zh': 'MemeSense：一种适应性强的基于上下文框架，用于社会常识驱动的 meme 审核'}
{'arxiv_id': 'arXiv:2502.11267', 'title': 'Prompting in the Dark: Assessing Human Performance in Prompt Engineering for Data Labeling When Gold Labels Are Absent', 'authors': "Zeyu He, Saniya Naphade, Ting-Hao 'Kenneth' Huang", 'link': 'https://arxiv.org/abs/2502.11267', 'abstract': 'Millions of users prompt large language models (LLMs) for various tasks, but how good are people at prompt engineering? Do users actually get closer to their desired outcome over multiple iterations of their prompts? These questions are crucial when no gold-standard labels are available to measure progress. This paper investigates a scenario in LLM-powered data labeling, "prompting in the dark," where users iteratively prompt LLMs to label data without using manually-labeled benchmarks. We developed PromptingSheet, a Google Sheets add-on that enables users to compose, revise, and iteratively label data through spreadsheets. Through a study with 20 participants, we found that prompting in the dark was highly unreliable-only 9 participants improved labeling accuracy after four or more iterations. Automated prompt optimization tools like DSPy also struggled when few gold labels were available. Our findings highlight the importance of gold labels and the needs, as well as the risks, of automated support in human prompt engineering, providing insights for future tool design.', 'abstract_zh': '数百万用户驱动大型语言模型（LLMs）完成各种任务，但人们在提示工程方面的水平如何？用户是否通过多次迭代改进提示，从而更接近其期望结果？在无法使用黄金标准标签衡量进步的情况下，这些问题显得尤为重要。本论文探讨了LLM支持的数据标注场景中的“黑暗提示”问题，即用户在没有使用手动标注基准的情况下，通过迭代提示LLM对数据进行标注。我们开发了PromptingSheet，这是一个Google Sheets插件，能够帮助用户通过表格进行提示的编写、修订和数据标注。通过一项涉及20名参与者的研究，我们发现“黑暗提示”非常不可靠——只有9名参与者在四次或更多次迭代后提高了标注准确性。类似DSPy这样的自动提示优化工具在可用的黄金标签较少的情况下也难以发挥作用。我们的研究结果强调了黄金标签的重要性，以及在人类提示工程中自动支持的需求和风险，为未来工具设计提供了参考。', 'title_zh': '在黑暗中提词：评估在缺乏金标准标签情况下提词工程对数据标注的人工绩效'}
{'arxiv_id': 'arXiv:2502.11221', 'title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'authors': 'Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu', 'link': 'https://arxiv.org/abs/2502.11221', 'abstract': 'LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.', 'abstract_zh': '大规模语言模型（LLMs）在生成计划方面拥有巨大的潜力，能够将初始世界状态转换为期望的目标状态。大量研究已经探讨了LLMs在各种规划任务中的应用，从网页导航到旅行规划和数据库查询。然而，许多这些系统都是为特定问题量身定制的，这使得它们之间难以比较，并且难以确定适用于新任务的最佳方法。此外，还缺乏明确且一致的评估标准。我们的综述旨在提供当前LLM规划器的全面概述，以填补这一空白。我们的综述建立在Kartam和Wilkins（1990）的基础工作之上，并探讨了六个关键性能指标：完整度、可执行性、优化性、表示、泛化和效率。对于每个指标，我们对代表性的作品进行了详尽的分析，并强调了它们的优势和不足之处。此外，我们的论文还指出了未来研究的关键方向，这使其成为从业者和对利用LLM规划支持代理工作流感兴趣的初学者的一个宝贵资源。', 'title_zh': 'PlanGenLLMs：大规模语言模型规划能力的现代综述'}
{'arxiv_id': 'arXiv:2502.11256', 'title': 'Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View', 'authors': 'Yanran Wu, Inez Hua, Yi Ding', 'link': 'https://arxiv.org/abs/2502.11256', 'abstract': "Large language models (LLMs) offer powerful capabilities but come with significant environmental costs, particularly in carbon emissions. Existing studies benchmark these emissions but lack a standardized basis for comparison across models. To address this, we introduce the concept of a functional unit (FU) and develop FUEL, the first FU-based framework for evaluating LLM serving's environmental impact. Through case studies on model size, quantization, and hardware, we uncover key trade-offs in sustainability. Our findings highlight the potential for reducing carbon emissions by optimizing model selection, deployment strategies, and hardware choices, paving the way for more sustainable AI infrastructure.", 'abstract_zh': '大规模语言模型（LLMs）提供了强大的能力，但同时也带来了显著的环境成本，特别是在碳排放方面。现有研究已经对这些排放进行了基准测试，但缺乏一个标准化的比较基础。为解决这一问题，我们提出了功能单位（FU）的概念，并开发了FUEL——首个基于FU的框架，用于评估LLM服务的环境影响。通过针对模型规模、量化和硬件的案例研究，我们揭示了可持续性的关键权衡因素。我们的研究结果突显了通过优化模型选择、部署策略和硬件选择来减少碳排放的潜力，为更可持续的人工智能架构铺平了道路。', 'title_zh': '揭示大规模语言模型服务的环境影响：从功能单元视角考察'}
{'arxiv_id': 'arXiv:2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'authors': 'Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen', 'link': 'https://arxiv.org/abs/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at this https URL.', 'abstract_zh': '尽管大型语言模型（LLMs）在知识密集型任务中表现出色，但在理解它们如何内化新知识，特别是如何在神经计算中结构化嵌入所获得的知识方面，它们仍然面临一个关键的差距。我们通过知识电路演化的视角来解决这一问题，识别出有助于知识存储和处理的计算子图。通过对持续预训练过程中电路演化系统的分析，我们发现了几个关键发现：（1）新知识的获取受到现有知识相关性的影响；（2）知识电路的演化表现出从形成到优化的独特阶段转变；（3）知识电路的演化遵循从深层次到浅层次的模式。这些见解不仅推进了我们对LLMs中新知识获取机制的理论理解，还为改进持续预训练策略以提高模型性能提供了潜在的启示。代码和数据可在此链接中获得：[请插入具体的URL链接]。', 'title_zh': '大型语言模型如何获取新知识？连续预训练从知识电路的角度解析'}
{'arxiv_id': 'arXiv:2502.11191', 'title': 'Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training', 'authors': 'Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, Wen-Kwang Tsao', 'link': 'https://arxiv.org/abs/2502.11191', 'abstract': 'Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to this https URL.', 'abstract_zh': '大型语言模型（LLMs）在金融、法律和医学等专门领域取得了显著的进步。然而，在网络安全领域，我们注意到缺乏开源数据集，特别是高质量的网络安全预训练语料库，尽管许多研究表明，LLMs在其预训练过程中会获取知识。为了解决这一问题，我们提出了一套全面的数据集，涵盖了所有主要的训练阶段，包括预训练、指令微调以及包含网络安全特定自我反思数据的推理提炼。大量的消融研究证明了这些数据集在公开的网络安全基准上的有效性。特别是，通过我们的数据集进行持续预训练可将综合得分提高15.88%，而通过推理提炼可将CISSP安全认证提高10%。我们将以ODC-BY和MIT许可证的形式发布所有数据集和训练好的网络安全LLMs，以鼓励社区内的进一步研究。您可以通过以下链接访问所有数据集和模型权重：[请点击此处访问](https://example.com)。', 'title_zh': 'Primus：用于网络安全语言模型训练的先驱开源数据集集合'}
{'arxiv_id': 'arXiv:2502.11246', 'title': 'MemeSense: An Adaptive In-Context Framework for Social Commonsense Driven Meme Moderation', 'authors': 'Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2502.11246', 'abstract': 'Memes present unique moderation challenges due to their subtle, multimodal interplay of images, text, and social context. Standard systems relying predominantly on explicit textual cues often overlook harmful content camouflaged by irony, symbolism, or cultural references. To address this gap, we introduce MemeSense, an adaptive in-context learning framework that fuses social commonsense reasoning with visually and semantically related reference examples. By encoding crucial task information into a learnable cognitive shift vector, MemeSense effectively balances lexical, visual, and ethical considerations, enabling precise yet context-aware meme intervention. Extensive evaluations on a curated set of implicitly harmful memes demonstrate that MemeSense substantially outperforms strong baselines, paving the way for safer online communities. Code and data available at: this https URL', 'abstract_zh': '模因因其隐晦的、多模态的图像、文本和社会语境的交织而带来独特的管理挑战。传统系统主要依赖显式的文本提示，常常未能识别因其反讽、象征意义或文化引用而隐蔽的有害内容。为解决这一问题，我们提出了MemeSense，这是一种结合社会常识推理与视觉和语义相关参考示例的自适应上下文学习框架。通过将关键任务信息编码到可学习的认知转换向量中，MemeSense能够有效地平衡词汇、视觉和伦理方面的考虑，从而实现精准且具备上下文意识的模因干预。通过对精心挑选的隐含有害模因的广泛评估表明，MemeSense显著优于强baseline模型，为构建更安全的在线社区铺平了道路。相关代码和数据可通过以下链接获取：this https URL', 'title_zh': 'MemeSense：一种适应性上下文框架，用于社会常识驱动的 meme 监管'}
{'arxiv_id': 'arXiv:2502.11167', 'title': 'SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors', 'authors': 'Bohan Lyu, Siqiao Huang, Zichen Liang', 'link': 'https://arxiv.org/abs/2502.11167', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在代码相关任务中表现出了显著的能力，如代码理解与代码生成。然而，一个同样重要但尚未充分探索的问题是，LLMs 是否能作为通用的代码代理执行器，预测程序的输出和行为，而不实际运行程序。为了系统地研究这一能力，我们引入了 SURGE，这是一种综合基准，涵盖了八大关键方面：多语言编程任务、比赛级别编程问题、仓库级别代码分析、高成本科学计算、时间复杂度密集型算法、有 Bug 代码的分析、依赖特定编译器或执行环境的程序，以及形式数学证明验证。我们对 SURGE 上的多个开源和专有 LLM 进行了评估，并进行了规模研究，分析模型大小和训练数据规模对代理执行准确性的影响。此外，我们还对模型预测错误进行分类，并探索改进的潜在领域。我们的研究发现，在某些情况下，LLMs 可以预测代码执行结果，但在通用代理执行方面存在局限性。本研究为使用 LMs 作为代码代理执行器的可能性提供了实证洞察。相关代码和数据集可在此处访问：this https URL。', 'title_zh': 'SURGE：大型语言模型作为通用代理代码执行器的潜力'}
{'arxiv_id': 'arXiv:2502.11221', 'title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'authors': 'Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu', 'link': 'https://arxiv.org/abs/2502.11221', 'abstract': 'LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.', 'abstract_zh': '大型语言模型（LLMs）在生成计划方面具有巨大的潜力，能够将初始世界状态转化为期望的目标状态。大量研究已经探讨了LLMs在各种规划任务中的应用，包括网络导航、旅行规划和数据库查询等。然而，许多现有的系统针对特定问题进行了定制，这使得它们之间的比较变得困难，也难以确定新任务的最佳方法。此外，缺乏清晰且一致的评估标准。我们所做的调研旨在提供当前LLM规划系统的全面概述，以填补这一空白。该调研基于Kartam和Wilkins（1990）的基础工作，并考察了六个关键性能指标：完备性、可执行性、最优性、表示性、泛化能力和效率。对于每个指标，我们对其代表性工作进行了详尽分析，并指出了它们的优势和不足。我们的论文还指出了未来研究的关键方向，使之成为既有经验的从业者和新入门的研究人员在利用LLM规划支持自主工作流程方面的重要参考资源。', 'title_zh': 'PlanGenLLMs：现代大型语言模型规划能力综述'}
{'arxiv_id': 'arXiv:2502.11163', 'title': 'VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks', 'authors': 'Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao', 'link': 'https://arxiv.org/abs/2502.11163', 'abstract': 'Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, significant challenges remain, including biases and privacy concerns. To systematically address these issues in the context of geographic information recognition, we introduce a benchmark dataset consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to $53.8\\%$ accuracy in city prediction, they exhibit significant regional biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed ($-12.5\\%$) and sparsely populated ($-17.0\\%$) areas. Moreover, the models exhibit regional biases, frequently overpredicting certain locations; for instance, they consistently predict Sydney for images taken in Australia. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at this https URL.', 'abstract_zh': '视觉-语言模型（VLMs）在多种任务中展现了卓越的性能，尤其是在从图像中识别地理信息方面。然而，仍然存在一些重大挑战，包括偏见和隐私问题。为了系统地解决这些挑战，特别是在地理信息识别领域，我们介绍了一个包含1200张图像和详细地理元数据的基准数据集。通过评估四种VLMs，我们发现这些模型能够从图像中识别地理信息，并在城市预测方面达到高达53.8%的准确率，但同时也表现出显著的区域偏见。具体而言，经济发达和人口稠密地区的性能明显优于发展较弱的地区（降低12.5%）和人口稀少的地区（降低17.0%）。此外，模型在识别地理信息时也表现出区域偏见，经常错误地预测某些位置；例如，它们会一致地将悉尼预测为澳大利亚拍摄的图像中的地点。VLMs的出色表现也引发了隐私方面的担忧，特别是对于那些无意被识别而在线共享照片的用户。我们的代码和数据集已在以下网址公开：[这个 https URL](https://this-url.com)。', 'title_zh': 'VLMs作为GeoGuessr大师：卓越表现、隐含偏见与隐私风险'}
{'arxiv_id': 'arXiv:2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'authors': 'Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen', 'link': 'https://arxiv.org/abs/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at this https URL.', 'abstract_zh': '尽管大型语言模型（LLMs）在知识密集型任务中表现出色，但仍面临一个关键的挑战，即理解它们如何内化新知识，尤其是如何在神经计算中结构性地嵌入所获取的知识。我们通过知识电路进化这一视角来解决这一问题，识别出有助于知识存储和处理的计算子图。系统分析连续预训练过程中电路进化的演变，揭示了以下几个关键发现：（1）新知识的获取受其与先存知识的相关性影响；（2）知识电路的演变经历了从形成到优化的阶段转变；（3）知识电路的演变遵循从深到浅的模式。这些见解不仅推进了我们对LLMs中新知识获取机制的理论理解，还为改进连续预训练策略以增强模型性能提供了潜在的指导意义。相关代码和数据将在此处提供：[这个链接]。', 'title_zh': '大语言模型是如何获取新知识的？连续预训练的知识电路视角'}
{'arxiv_id': 'arXiv:2502.11191', 'title': 'Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training', 'authors': 'Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, Wen-Kwang Tsao', 'link': 'https://arxiv.org/abs/2502.11191', 'abstract': 'Large Language Models (LLMs) have shown remarkable advancements in specialized fields such as finance, law, and medicine. However, in cybersecurity, we have noticed a lack of open-source datasets, with a particular lack of high-quality cybersecurity pretraining corpora, even though much research indicates that LLMs acquire their knowledge during pretraining. To address this, we present a comprehensive suite of datasets covering all major training stages, including pretraining, instruction fine-tuning, and reasoning distillation with cybersecurity-specific self-reflection data. Extensive ablation studies demonstrate their effectiveness on public cybersecurity benchmarks. In particular, continual pre-training on our dataset yields a 15.88% improvement in the aggregate score, while reasoning distillation leads to a 10% gain in security certification (CISSP). We will release all datasets and trained cybersecurity LLMs under the ODC-BY and MIT licenses to encourage further research in the community. For access to all datasets and model weights, please refer to this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在金融、法律和医学等专业领域展现了显著的进步。然而，在网络安全领域，我们注意到缺乏开源数据集，尤其是在高质量的网络安全预训练语料方面存在明显不足，尽管许多研究表明LLMs在其预训练阶段会获取知识。为了应对这一问题，我们提供了一整套涵盖所有主要训练阶段的数据集，包括预训练、指令微调和针对网络安全的推理精炼，其中包含专门的自我反思数据。广泛的消融研究表明，这些数据集在公共网络安全基准测试上的有效性。特别是，持续使用我们的数据集进行预训练可以提高综合评分15.88%，而推理精炼则能使得网络安全认证（CISSP）提高10%。我们将所有数据集和训练好的网络安全LLMs在ODC-BY和MIT许可下公开，以促进社区内的进一步研究。欲访问所有数据集和模型权重，请参阅此链接：[提供链接的地方]。', 'title_zh': 'Primus：用于网络安全大型语言模型训练的开创性开源数据集集合'}
{'arxiv_id': 'arXiv:2502.11155', 'title': 'Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs', 'authors': 'Fei Yu, Yingru Li, Benyou Wang', 'link': 'https://arxiv.org/abs/2502.11155', 'abstract': 'Value model-guided search is effective in steering the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from reliability degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that includes two key components: (1) uncertainty-aware value models that incorporate uncertainty into predictions, and (2) an uncertainty-aware selection process using the proposed efficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms.', 'abstract_zh': '值模型导向的搜索在引导生成方面是有效的，但存在规模扩展缺陷：其优势随着样本量的增加而减小，表现不如非搜索基线。这种限制源于在未见过的推理路径上值模型的可靠性下降。为解决这一问题，我们提出了一种不确定性感知的搜索框架，该框架包含两个关键组件：（1）不确定性感知值模型，将不确定性纳入预测中；（2）利用提出的有效组泰勒斯采样算法进行不确定性感知的选择过程。在GSM8K上的实验表明，我们的方法缓解了搜索的规模扩展缺陷，在16个样本的情况下，覆盖率为90.5%，而传统的值导向搜索仅为85.8%。本项工作首次系统地将不确定性量化集成到大语言模型的搜索范式中。', 'title_zh': '基于不确定性感知的搜索与价值模型：减轻大规模语言模型中搜索扩展的缺陷'}
{'arxiv_id': 'arXiv:2502.11167', 'title': 'SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors', 'authors': 'Bohan Lyu, Siqiao Huang, Zichen Liang', 'link': 'https://arxiv.org/abs/2502.11167', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在代码相关任务上展示了显著的能力，如代码理解和代码生成。然而，一个同样重要但尚未充分探索的问题是，LLMs 是否可以作为通用代理代码执行器，预测程序的输出和行为，而不需要实际运行代码。为了系统地研究这一能力，我们引入了SURGE，这是一个全面的基准测试，涵盖了八个关键方面：多语言编程任务、高水平编程竞赛问题、仓库级代码分析、高成本科学计算、时间复杂度密集型算法、有 bug 的代码分析、依赖特定编译器或执行环境的程序以及形式化数学证明验证。我们对SURGE评估了多种开源和专有LLMs，并进行了规模研究，分析了模型大小和训练数据规模对代理执行准确性的影响。此外，我们对模型预测错误进行了分类，并探索了改进的潜在领域。研究结果表明，虽然LLMs在某些情况下可以预测代码执行结果，但在通用代理执行方面仍存在局限性。本研究提供了将LLMs用作代理代码执行器的可行性经验洞察。代码和数据集可在以下链接获取：[此 https URL](此 https URL)。', 'title_zh': 'SURGE：大型语言模型作为通用代理代码执行器的潜力'}
{'arxiv_id': 'arXiv:2502.11142', 'title': 'NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM', 'authors': 'Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan', 'link': 'https://arxiv.org/abs/2502.11142', 'abstract': "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.", 'abstract_zh': '视觉-语言导航（VLN）是体域代理的一项重要技能，使其能够根据自然语言指令在3D环境中导航。高性能的导航模型需要大量的训练数据，手动标注数据的高成本严重阻碍了这一领域的发展。因此，一些先前的方法将轨迹视频转化为分步指令以扩展数据集，但这些指令未能很好地匹配用户的交流风格，用户通常用简短的描述来说明目的地或提出具体需求。此外，局部导航轨迹忽略了全局上下文和高层任务规划。为了解决这些问题，我们提出了一种检索增强生成（RAG）框架NavRAG，该框架用于为VLN生成用户需求指令。NavRAG 利用大模型（LLM）构建从全局布局到局部细节的分层场景描述树，以实现3D场景理解，然后模拟具有不同需求的多种用户角色检索场景树中的信息，并利用LLM生成多样化指令。我们对861个场景中的200多万导航指令进行了标注，并评估了训练模型的数据质量和导航性能。', 'title_zh': 'NavRAG：通过检索增强的大型语言模型生成用户需求指令以实现具身导航'}
{'arxiv_id': 'arXiv:2502.11163', 'title': 'VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks', 'authors': 'Jingyuan Huang, Jen-tse Huang, Ziyi Liu, Xiaoyuan Liu, Wenxuan Wang, Jieyu Zhao', 'link': 'https://arxiv.org/abs/2502.11163', 'abstract': 'Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, significant challenges remain, including biases and privacy concerns. To systematically address these issues in the context of geographic information recognition, we introduce a benchmark dataset consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to $53.8\\%$ accuracy in city prediction, they exhibit significant regional biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed ($-12.5\\%$) and sparsely populated ($-17.0\\%$) areas. Moreover, the models exhibit regional biases, frequently overpredicting certain locations; for instance, they consistently predict Sydney for images taken in Australia. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at this https URL.', 'abstract_zh': '视觉-语言模型（VLMs）在各种任务中展现了卓越的性能，特别是在从图像中识别地理信息方面。然而，仍存在一些重大挑战，包括偏见和隐私安全问题。为了系统地解决地理信息识别方面的这些问题，我们引入了一个包含1,200张图像及其详细地理元数据的基准数据集。评估了四种VLMs，发现尽管这些模型能够从图像中识别地理信息，城市预测的最高准确率达到53.8%，但它们在识别过程中的表现存在显著的区域偏向。具体表现为，经济发达和人口密集地区的识别性能显著高于经济欠发达地区（下降12.5%）和人口稀少地区（下降17.0%）。此外，模型也表现出地域偏向，经常过度预测某些地点，例如，对于在澳大利亚拍摄的图像，它们经常预测为悉尼。VLMs的强大性能也引发了隐私问题，特别是对于那些无意被识别的在线共享图像的用户。我们的代码和数据集可在以下链接公开获取：[此链接](this https URL)。', 'title_zh': 'VLMs作为GeoGuessr大师：卓越表现、隐藏偏见与隐私风险'}
{'arxiv_id': 'arXiv:2502.11140', 'title': 'VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization', 'authors': 'Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee', 'link': 'https://arxiv.org/abs/2502.11140', 'abstract': 'Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation.', 'abstract_zh': '大型语言模型（LLMs）的前所未有的突破已将其渗透到自动化可视化代码生成的应用中。少样本提示和查询扩展技术显著提升了数据可视化性能，但仍然无法克服自然语言查询的模糊性和复杂性，这给手动的人工干预带来了固有的负担。为缓解这些局限性，我们提出了一种综合框架VisPath：一种多路径推理和反馈驱动优化框架，该框架通过结构化推理和改进系统地提升代码质量。VisPath 是一个多阶段框架，特别设计用于处理不完全明确的查询。为了生成稳健的最终可视化代码，它首先利用初始查询通过链式思维（CoT）提示生成多样化的重新表述查询，每个查询代表一种不同的推理路径。经过精炼的查询用于产生候选的可视化脚本，随后执行生成多个图像。全面评估输出的正确性和质量后，VisPath 为每个图像生成反馈，这些反馈随后被馈送到聚合模块以生成最优结果。在包括MatPlotBench和Qwen-Agent Code Interpreter Benchmark在内的基准测试中的广泛实验显示，VisPath 显著优于最新技术（SOTA）方法，平均提高17%以上，为基于人工智能的可视化代码生成提供了更可靠的方法。', 'title_zh': 'VisPath: 基于多路径推理和反馈驱动优化的自动化可视化代码合成'}
{'arxiv_id': 'arXiv:2502.11155', 'title': 'Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs', 'authors': 'Fei Yu, Yingru Li, Benyou Wang', 'link': 'https://arxiv.org/abs/2502.11155', 'abstract': 'Value model-guided search is effective in steering the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from reliability degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that includes two key components: (1) uncertainty-aware value models that incorporate uncertainty into predictions, and (2) an uncertainty-aware selection process using the proposed efficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms.', 'abstract_zh': '价值模型引导的搜索在指导生成方面是有效的，但存在扩展性缺陷：随着样本数量的增加，其优势逐渐减弱，并且在某些情况下表现不如非搜索基准。这一缺陷源于在未见过的推理路径中价值模型的可靠性下降。为了解决这一问题，我们提出了一种aware于不确定性的搜索框架，该框架包含两个关键组成部分：（1）aware于不确定性的价值模型，该模型将不确定性纳入预测中；（2）使用提出的高效Group Thompson Sampling算法的aware于不确定性的选择过程。在GSM8K上的实验显示，我们的方法减轻了搜索扩展性缺陷，16个样本情况下覆盖率达到了90.5%，而传统价值引导的搜索仅为85.8%。本项工作在LLM搜索范式中首次系统地整合了不确定性量化。', 'title_zh': '具有不确定性意识的搜索和价值模型：减轻大规模语言模型中搜索扩展的缺陷'}
{'arxiv_id': 'arXiv:2502.11096', 'title': 'Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time', 'authors': 'Robert Dahlke, Henrik Klagges, Dan Zecha, Benjamin Merkel, Sven Rohr, Fabian Klemm', 'link': 'https://arxiv.org/abs/2502.11096', 'abstract': "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.\nBy analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.\nUsing MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.\nOur approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.\nOur findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.", 'abstract_zh': '我们提出了一种名为Mixture-of-Tunable-Experts（MoTE）的方法，该方法扩展了大型语言模型（LLMs）中的Mixture-of-Experts架构。MoTE在无需额外训练的情况下，能够在推理时实时地使LLMs表现出有意义且针对性的行为变化。\n\n通过使用我们称之为功能性Token共振成像（fTRI）的技术——该技术借鉴了功能性磁共振成像（fMRI）的原理，并且通过设计特定的提示（例如，“在{时间}{地点}发生了什么？”）来激发特定行为——我们实证地识别出了与拒绝响应等行为相关的独特专家。\n\n借助MoTE，我们能够干预并控制这些特定行为。我们关闭了与拒绝响应最相关的前10位专家（占R1的14,848个路由专家的0.07%），在不牺牲MT-Bench性能的前提下，成功减少了敏感参考提示中的52%的拒绝响应。随机关闭专家导致了较小的行为变化但增加了噪声，强制激活专家则导致了显著更高的拒绝率。\n\n我们的方法在可解释性和可控性方面与稀疏自编码器（SAEs）具有相似之处。与SAEs不同的是，MoTE不需要大量的训练努力，因为在具有大量专家的MoE中，专门化已经在预训练过程中自然出现。\n\n我们的研究成果表明，Mixture-of-Experts架构中的重要功能机制至少部分地可以局限于少量特定专家中，而不是分散在整个模型的权重中。专家子组可以通过专门化来触发显著的行为变化，这为理解LLMs内部工作机制提供了见解。', 'title_zh': '混合可调专家——在推理时修改DeepSeek-R1的行为'}
{'arxiv_id': 'arXiv:2502.11026', 'title': 'Simplify RLHF as Reward-Weighted SFT: A Variational Method', 'authors': 'Yuhao Du, Zhuo Li, Pengyu Cheng, Zhihong Chen, Yuejiao Xie, Xiang Wan, Anningzhe Gao', 'link': 'https://arxiv.org/abs/2502.11026', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\\textbf{V}$ariational $\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.', 'abstract_zh': '强化学习结合人类反馈（Reinforcement Learning from Human Feedback, RLHF）对于使大语言模型（Large Language Models, LLMs）与人类价值观保持一致至关重要。然而，RLHF 在实施和计算消耗方面的高复杂性持续对其构成挑战。即使在最近的简化尝试，如直接偏好优化（Direct Preference Optimization, DPO）和优势剩余午餐（Advantage Leftover Lunch, A-LoL）之后，过拟合和训练不稳定的问题仍然阻碍了从期望的最佳性能中实现完整对齐过程。为应对现有挑战，我们从变分推断的角度提出了一种新的RLHF简化方法，称为 **V** 变分 **A** 重新加权 **R** 对齐（**VAR**，Variational Alignment with Re-weighting）。具体来说，通过直接最小化学习中的LLM策略和RLHF最优解之间的分布差距，我们将对齐目标转化为奖励驱动的重新加权监督微调（SFT，Supervised Fine-Tuning）形式，只需对SFT损失进行细微调整即可显著提升训练稳定性和有效性。在全面的对齐和生成基准测试中，我们的VAR方法在LLM对齐的帮助性和无害性方面取得了竞争力的表现。', 'title_zh': '将RLHF简化为奖励加权SFT：一种变分方法'}
{'arxiv_id': 'arXiv:2502.11142', 'title': 'NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM', 'authors': 'Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan', 'link': 'https://arxiv.org/abs/2502.11142', 'abstract': "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.", 'abstract_zh': '视觉-语言导航（VLN）是具身代理的一项基本技能，允许它们遵循自然语言指令在3D环境中导航。高性能的导航模型需要大量的训练数据，手动标注数据的高昂成本严重阻碍了这一领域的进展。因此，一些先前的方法将轨迹视频转换为分步骤的指令以扩充数据集，但这些指令并不符合用户的交流风格，用户往往只是简要描述目的地或提出特定需求。此外，局部导航轨迹忽略了全局上下文和高层次的任务规划。为了应对这些挑战，我们提出了一种检索增强生成（RAG）框架NavRAG，该框架用于为VLN生成用户需求指令。NavRAG利用大语言模型（LLM）构建一个分层场景描述树，从全局布局到局部细节进行3D场景理解，然后模拟具有不同需求的多种用户角色，从场景树中检索信息，生成多种多样的指令。我们对超过200万条导航指令进行了标注，覆盖了861个场景，并评估了训练模型的数据质量和导航性能。', 'title_zh': 'NavRAG：通过检索增强的大语言模型生成用户导向的实体导航指令'}
{'arxiv_id': 'arXiv:2502.11021', 'title': 'Leveraging Uncertainty Estimation for Efficient LLM Routing', 'authors': 'Tuo Zhang, Asal Mehradfar, Dimitrios Dimitriadis, Salman Avestimehr', 'link': 'https://arxiv.org/abs/2502.11021', 'abstract': 'Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.', 'abstract_zh': '将大型语言模型（LLMs）部署在边缘-云环境中需要一种高效的路由策略来平衡成本和响应质量。传统方法要么优先考虑人类偏好数据，要么以基准数据集的准确性指标作为路由标准，但这些方法存在固性和主观性的问题。此外，现有的路由框架主要关注准确性和成本，忽视了从人类偏好角度考虑的响应质量。在这项工作中，我们提出了一种新的 Confidence-Driven LLM Router 框架，该框架利用不确定性估计来优化路由决策。为了全面评估路由性能，我们同时评估了系统成本效率和响应质量。特别地，我们引入了将LLM作为一种评判者的新型方法，以模拟人类评价偏好，并首次系统地评估了不同路由策略下的响应质量。在对MT-Bench、GSM8K和MMLU的大量实验中，我们的方法优于当前最先进的路由方法，在保持成本效率的同时实现了更高的响应质量。', 'title_zh': '利用不确定性估计实现高效的语言模型路由'}
{'arxiv_id': 'arXiv:2502.11140', 'title': 'VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization', 'authors': 'Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, Seunghyun Lee', 'link': 'https://arxiv.org/abs/2502.11140', 'abstract': 'Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation, which systematically enhances code quality through structured reasoning and refinement. VisPath is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, VisPath generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including MatPlotBench and the Qwen-Agent Code Interpreter Benchmark show that VisPath significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17%, offering a more reliable solution for AI-driven visualization code generation.', 'abstract_zh': '大规模语言模型（LLMs）前所未有的突破极大地推动了其在自动化可视化代码生成中的应用。少量示例提示和查询扩展技术显著提升了数据可视化性能，但仍然难以克服自然语言查询的模糊性和复杂性，从而增加了人工干预的负担。为缓解这些限制，我们提出了一种综合框架——VisPath：一种多路径推理和反馈驱动的优化框架，该框架系统地通过结构化推理和提升来增强代码质量。VisPath 是一个多阶段框架，专门设计用于处理不明确的查询。为了生成稳健的最终可视化代码，它首先利用初始查询通过链式思考（CoT）提示生成多种多样的重新表述查询，每种查询代表一条不同的推理路径。经过细化的查询用于生成候选的可视化脚本，这些脚本随后被执行以生成多个图像。通过全面评估输出的正确性和质量，VisPath 为每张图像生成反馈，这些反馈随后被反馈给聚合模块以生成最优结果。在包括MatPlotBench和Qwen-Agent代码解释器基准在内的多个标准测试集上的广泛实验表明，VisPath 显著优于现有最佳方法（SOTA），平均性能提高了17%以上，提供了更具可靠性的基于AI的可视化代码生成解决方案。', 'title_zh': 'VisPath：通过多路径推理和反馈驱动优化的自动化可视化代码生成'}
{'arxiv_id': 'arXiv:2502.11096', 'title': 'Mixture of Tunable Experts - Behavior Modification of DeepSeek-R1 at Inference Time', 'authors': 'Robert Dahlke, Henrik Klagges, Dan Zecha, Benjamin Merkel, Sven Rohr, Fabian Klemm', 'link': 'https://arxiv.org/abs/2502.11096', 'abstract': "We present the Mixture-of-Tunable-Experts (MoTE), a method that extends the Mixture-of-Experts architecture of Large Language Models (LLMs). Without additional training, MoTE enables meaningful and focused behavior changes in LLMs on-the-fly during inference time.\nBy analyzing the digital LLM brain of DeepSeek-R1 using a technique we dub 'functional Token Resonance Imaging' (fTRI) - inspired by fMRI and using prompts designed to elicit specific behavior (e.g., 'What happened {time}{place}?') - we empirically identify distinctive experts associated with behaviors like refusal responses.\nUsing MoTE we are able to intervene and control such specific behavior. We switched off the top 10 most refusal-relevant experts (0.07% of R1's 14,848 routed experts), achieving a 52% refusal reduction on sensitive reference prompts without performance degradation on MT-Bench. Random expert deactivation resulted in smaller behavioral shifts with increased noise, whereas forced expert activation led to significantly higher refusal rates.\nOur approach shares similarities with sparse autoencoders (SAEs) in terms of explainability and steerability. Unlike SAEs, MoTE does not require large training efforts, as within MoEs with a vast number of experts, specialization already emerged naturally during pretraining.\nOur findings suggest that significant functional mechanisms in Mixture-of-Experts architectures can at least partially be localized in a small number of specific experts, rather than being distributed throughout the model's weights. Expert subgroups can be tuned to trigger significant behavior variations, providing insights into the inner workings of LLMs.", 'abstract_zh': '我们提出了一种名为Mixture-of-Tunable-Experts（可调专家混合模型，MoTE）的方法，该方法扩展了大型语言模型（LLMs）的混合专家架构。MoTE 在不额外进行训练的情况下，能够在推理过程中实时启用 LLM 的有意义且聚焦的行为变化。\n\n通过使用一种我们称为“功能性 Token 共振成像”（fTRI，Functional Token Resonance Imaging）的技术——灵感来源于 fMRI，并使用旨在引发特定行为的提示（例如，“在 {时间} 和 {地点} 发生了什么？”），我们从数字 LLM 大脑中，即使用 DeepSeek-R1，实证地识别出与拒绝响应等行为相关的独特专家。\n\n借助 MoTE，我们能够干预并控制这些特定行为。我们关闭了 Top 10 最相关的拒绝响应专家（仅占 R1 分配专家的 0.07%），在不牺牲 MT-Bench 性能的情况下减少了敏感参考提示中的 52% 的拒绝率。随机专家的关闭导致了较小的行为变化和更多的噪音，而强制激活专家则显著提高了拒绝率。\n\n我们的方法在可解释性和可控性方面与稀疏自编码器（SAEs）具有相似性。与 SAEs 不同的是，MoTE 并不需要大量的训练努力，在 MoEs 中由于有大量专家的存在，专门化已经在预训练过程中自然地形成了。\n\n我们的研究发现表明，在混合专家架构中，可能存在一些关键的功能机制，这些机制可以在少数特定专家中局部化，而不需要在整个模型权重中分布。专家子组可以通过调整来触发显著的行为变化，从而提供对 LLM 内部机制的深入理解。', 'title_zh': '可调节专家混合模型 - DeepSeek-R1 推理时的行为修改'}
{'arxiv_id': 'arXiv:2502.11026', 'title': 'Simplify RLHF as Reward-Weighted SFT: A Variational Method', 'authors': 'Yuhao Du, Zhuo Li, Pengyu Cheng, Zhihong Chen, Yuejiao Xie, Xiang Wan, Anningzhe Gao', 'link': 'https://arxiv.org/abs/2502.11026', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called $\\textbf{V}$ariational $\\textbf{A}$lignment with $\\textbf{R}$e-weighting ($\\textbf{VAR}$). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.', 'abstract_zh': '人类反馈强化学习（RLHF）对于使大规模语言模型（LLMs）与人类价值观保持一致至关重要。然而，RLHF 在实施和计算上的高复杂性一直在不断对其构成挑战。即使在最近简化方法的出现下，例如直接偏好优化（DPO）和优势剩余午餐（A-LoL），过拟合和训练不稳定性等问题仍然阻碍了其达到预期的最佳性能。为了解决现有的挑战，我们从变分推断的角度提出了一个新颖的RLHF简化方法，称为**变分对齐与重加权**（VAR）。\n\n具体而言，通过直接最小化学习的LLM策略与RLHF最优解之间的分布差距，我们将对齐目标转化为由奖励驱动的重加权监督微调（SFT）形式，仅需对SFT损失进行轻微调整即可显著提高训练稳定性和效果。在综合对齐和生成基准测试中，我们的VAR方法在LLM对齐的有用性和无害性方面实现了可比较的性能。', 'title_zh': '将 RLHF 简化为奖励加权 SFT：一种变分方法'}
{'arxiv_id': 'arXiv:2502.11021', 'title': 'Leveraging Uncertainty Estimation for Efficient LLM Routing', 'authors': 'Tuo Zhang, Asal Mehradfar, Dimitrios Dimitriadis, Salman Avestimehr', 'link': 'https://arxiv.org/abs/2502.11021', 'abstract': 'Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.', 'abstract_zh': '在边缘-云环境中部署大规模语言模型（LLMs）需要一种高效的路由策略来平衡成本和响应质量。传统的方法要么侧重于人类偏好的数据，要么侧重于基准数据集的准确性指标，但这些方法存在僵化性和主观性的问题。此外，现有的路由框架主要关注准确性和成本，而忽略了从人类偏好视角出发的响应质量。本文提出了一种新的框架——信心驱动的语言模型路由器（Confidence-Driven LLM Router），该框架利用不确定性估计来优化路由决策。为了全面评估路由性能，我们不仅评估系统的成本效率，还评估响应质量。特别是，我们引入了语言模型作为裁判的新方法来模拟人类评分偏好，并首次系统地评估了不同路由策略下的响应质量。在MT-Bench、GSM8K和MMLU上的广泛实验表明，我们的方法在保持成本效益的同时，超越了现有的先进路由方法，实现了更高的响应质量。', 'title_zh': '利用不确定性估计实现高效的LLM路由'}
{'arxiv_id': 'arXiv:2502.10999', 'title': 'ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering without Font Annotations', 'authors': 'Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, Camillo J. Taylor', 'link': 'https://arxiv.org/abs/2502.10999', 'abstract': 'This work demonstrates that diffusion models can achieve font-controllable multilingual text rendering using just raw images without font label annotations. Visual text rendering remains a significant challenge. While recent methods condition diffusion on glyphs, it is impossible to retrieve exact font annotations from large-scale, real-world datasets, which prevents user-specified font control. To address this, we propose a data-driven solution that integrates the conditional diffusion model with a text segmentation model, utilizing segmentation masks to capture and represent fonts in pixel space in a self-supervised manner, thereby eliminating the need for any ground-truth labels and enabling users to customize text rendering with any multilingual font of their choice. The experiment provides a proof of concept of our algorithm in zero-shot text and font editing across diverse fonts and languages, providing valuable insights for the community and industry toward achieving generalized visual text rendering.', 'abstract_zh': '本文表明，通过利用原始图像而无须使用字体标签注释，扩散模型可以实现可控的多语言文本渲染。视觉文本渲染仍然是一个重要的挑战。尽管近期的方法通过字形进行扩散条件化，但在大规模的真实世界数据集中获取精确的字体注释是不可能的，这阻碍了用户指定的字体控制。为了解决这一问题，我们提出了一种基于数据的解决方案，将条件扩散模型与文本分割模型相结合，利用分割掩码以自监督的方式捕获和表示字体在像素空间中的信息，从而消除对任何真实标签的需求，并使用户能够使用自己选择的任何多语言字体定制文本渲染。实验展示了我们算法在不同字体和语言下的零样本文本和字体编辑的概念证明，为实现通用视觉文本渲染提供了宝贵的见解，对学术界和工业界具有重要意义。', 'title_zh': 'ControlText：在无需字体注解的情况下解锁多语言文本渲染的可控字体'}
{'arxiv_id': 'arXiv:2502.10976', 'title': 'QuOTE: Question-Oriented Text Embeddings', 'authors': 'Andrew Neeser, Kaylen Latimer, Aadyant Khatri, Chris Latimer, Naren Ramakrishnan', 'link': 'https://arxiv.org/abs/2502.10976', 'abstract': 'We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to retrieval-augmented generation (RAG) systems, aimed at improving document representation for accurate and nuanced retrieval. Unlike traditional RAG pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with hypothetical questions that the chunk can potentially answer, enriching the representation space. This better aligns document embeddings with user query semantics, and helps address issues such as ambiguity and context-dependent relevance. Through extensive experiments across diverse benchmarks, we demonstrate that QuOTE significantly enhances retrieval accuracy, including in multi-hop question-answering tasks. Our findings highlight the versatility of question generation as a fundamental indexing strategy, opening new avenues for integrating question generation into retrieval-based AI pipelines.', 'abstract_zh': '我们提出了QuOTE（问题导向的文本嵌入），这是对检索增强生成（RAG）系统的全新增强，旨在通过改进文档表示来提高准确且细腻的检索效果。与依赖嵌入原始文本片段的传统RAG流水线不同，QuOTE通过添加可能由片段回答的假设性问题来增强这些片段，从而丰富了表示空间。这更好地使文档嵌入与用户查询语义相契合，有助于解决诸如歧义性和上下文相关性等问题。通过在多种基准测试中的广泛实验，我们证明了QuOTE显著提高了检索准确性，包括多步骤问答任务。我们的研究结果突显了问题生成作为基本索引策略的灵活性和多功能性，为将问题生成集成到基于检索的AI流水线中开辟了新的途径。', 'title_zh': '引言：面向问题的文本嵌入'}
{'arxiv_id': 'arXiv:2502.10999', 'title': 'ControlText: Unlocking Controllable Fonts in Multilingual Text Rendering without Font Annotations', 'authors': 'Bowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao, Lyle Ungar, Camillo J. Taylor', 'link': 'https://arxiv.org/abs/2502.10999', 'abstract': 'This work demonstrates that diffusion models can achieve font-controllable multilingual text rendering using just raw images without font label annotations. Visual text rendering remains a significant challenge. While recent methods condition diffusion on glyphs, it is impossible to retrieve exact font annotations from large-scale, real-world datasets, which prevents user-specified font control. To address this, we propose a data-driven solution that integrates the conditional diffusion model with a text segmentation model, utilizing segmentation masks to capture and represent fonts in pixel space in a self-supervised manner, thereby eliminating the need for any ground-truth labels and enabling users to customize text rendering with any multilingual font of their choice. The experiment provides a proof of concept of our algorithm in zero-shot text and font editing across diverse fonts and languages, providing valuable insights for the community and industry toward achieving generalized visual text rendering.', 'abstract_zh': '这项工作展示了扩散模型能够仅使用原始图像，而无需字体标签注释，实现可控多语言文字渲染。视觉文字渲染仍然是一个重大的挑战。尽管最近的方法会根据字符（glyphs）条件化扩散过程，但从大规模的真实世界数据集中难以获取精确的字体注释，这阻碍了用户指定的字体控制。为解决这一问题，我们提出了一种数据驱动的解决方案，将条件扩散模型与文本分割模型相结合，利用分割掩码以自监督的方式捕获和表示在像素空间中的字体，从而消除了对任何真实标签的需求，并使用户能够使用他们选择的任意多语言字体自定义文字渲染。实验提供了一个零样本文本和字体编辑概念验证，展示了我们算法在多种字体和语言中的一般视觉文字渲染能力，为社区和行业提供了宝贵的见解，以实现通用的视觉文字渲染。', 'title_zh': 'ControlText: 在无需字体标注的情况下解锁多语言文本渲染中的可控字体'}
{'arxiv_id': 'arXiv:2502.10937', 'title': 'SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention', 'authors': 'Chengshuai Zhao, Zhen Tan, Chau-Wai Wong, Xinyan Zhao, Tianlong Chen, Huan Liu', 'link': 'https://arxiv.org/abs/2502.10937', 'abstract': 'Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.', 'abstract_zh': '内容分析将复杂的非结构化文本分解为理论导向的数值类别。特别是在社会科学研究中，这一过程通常依赖于多轮手动注释、领域专家讨论和基于规则的完善。本文介绍了一种名为SCALE的新颖多智能体框架，该框架利用大型语言模型（LLM）智能体有效模拟内容分析。SCALE模仿内容分析的关键阶段，包括文本编码、协作讨论和动态代码表演化，捕捉了人类研究者反思的深度和适应性讨论。此外，通过整合多样的人类干预模式，SCALE进一步增强并集成了专家输入。在真实世界数据集上的广泛评估表明，SCALE在各种复杂内容分析任务中实现了接近人类的表现，为未来社会科学研究提供了创新潜力。', 'title_zh': 'SCALE：以大规模语言模型代理和人类干预为导向的社会科学中的协作内容分析'}
{'arxiv_id': 'arXiv:2502.10976', 'title': 'QuOTE: Question-Oriented Text Embeddings', 'authors': 'Andrew Neeser, Kaylen Latimer, Aadyant Khatri, Chris Latimer, Naren Ramakrishnan', 'link': 'https://arxiv.org/abs/2502.10976', 'abstract': 'We present QuOTE (Question-Oriented Text Embeddings), a novel enhancement to retrieval-augmented generation (RAG) systems, aimed at improving document representation for accurate and nuanced retrieval. Unlike traditional RAG pipelines, which rely on embedding raw text chunks, QuOTE augments chunks with hypothetical questions that the chunk can potentially answer, enriching the representation space. This better aligns document embeddings with user query semantics, and helps address issues such as ambiguity and context-dependent relevance. Through extensive experiments across diverse benchmarks, we demonstrate that QuOTE significantly enhances retrieval accuracy, including in multi-hop question-answering tasks. Our findings highlight the versatility of question generation as a fundamental indexing strategy, opening new avenues for integrating question generation into retrieval-based AI pipelines.', 'abstract_zh': '我们提出了QuOTE（Question-Oriented Text Embeddings），这是一种针对检索增强生成（RAG）系统的新型增强技术，旨在通过改进文档表示来提高准确和细腻的检索效果。与传统的RAG流水线依赖嵌入原始文本片段不同，QuOTE通过在片段中添加假设问题来增强文档片段，这些问题是该片段可能能够回答的。这使得文档嵌入更好地与用户查询语义相匹配，并有助于解决诸如歧义性和上下文相关性等问题。通过在多种基准测试上的广泛实验，我们证明QuOTE显著提高了检索准确性，包括多跳问答任务中的准确性。我们的研究结果强调了问题生成作为基本索引策略的多功能性，为将问题生成集成到基于检索的人工智能流水线中开辟了新的途径。', 'title_zh': '引言：问题导向的文本嵌入'}
{'arxiv_id': 'arXiv:2502.10937', 'title': 'SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention', 'authors': 'Chengshuai Zhao, Zhen Tan, Chau-Wai Wong, Xinyan Zhao, Tianlong Chen, Huan Liu', 'link': 'https://arxiv.org/abs/2502.10937', 'abstract': 'Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.', 'abstract_zh': '内容分析将复杂的非结构化文本分解为理论导向的数值类别。特别是在社会科学中，这一过程通常依赖于多轮的手动注释、领域专家讨论以及基于规则的改进。本文介绍了一种新颖的多智能体框架SCALE，通过大语言模型（LLM）智能体有效地模拟内容分析过程。SCALE 模拟内容分析的关键阶段，包括文本编码、协作讨论以及动态代码本演变，捕捉了人类研究人员的反思深度和适应性讨论。此外，通过整合多种类型的人类干预模式，SCALE 进一步增强了其性能，加入了领域专家的输入。在真实世界数据集上的广泛评估表明，SCALE 在各种复杂内容分析任务中实现了接近人类的表现，为未来社会科学的研究提供了创新的潜力。', 'title_zh': 'SCALE：关于大规模语言模型代理与人类介入在社会科学中协作内容分析的研究'}
{'arxiv_id': 'arXiv:2502.10928', 'title': 'Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek R1 Expert Specialization', 'authors': 'Matthew Lyle Olson, Neale Ratzlaff, Musashi Hinck, Man Luo, Sungduk Yu, Chendi Xue, Vasudev Lal', 'link': 'https://arxiv.org/abs/2502.10928', 'abstract': "DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven. Given DeepSeek-R1's enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1's structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1's routing mechanism is more semantically aware and it engages in structured cognitive processes.", 'abstract_zh': 'DeepSeek-R1 是最大的开源 Mixture-of-Experts (MoE) 模型，其展示的推理能力与 proprietary 领先模型相当。先前的研究已经探索了 MoE 模型中的专家路由机制，但研究结果表明，专家选择通常依赖于词项，而非语义驱动。鉴于 DeepSeek-R1 提升了推理能力，我们研究其路由机制是否在语义专业化上表现得优于之前的 MoE 模型。为了探索这一点，我们开展了两项关键实验：(1) 词汇意义消歧实验，我们考察含义不同的词的专家激活模式；(2) 认知推理分析，我们评估 DeepSeek-R1 在 DiscoveryWorld 交互任务场景中的结构化思维过程。研究结论表明，DeepSeek-R1 的路由机制更具语义意识，并且能够进行结构化的认知过程。', 'title_zh': '大规模下MoE中的语义专业化现象：DeepSeek R1 专家专业化研究'}
{'arxiv_id': 'arXiv:2502.10867', 'title': 'A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1', 'authors': 'Jun Wang', 'link': 'https://arxiv.org/abs/2502.10867', 'abstract': "OpenAI o1 has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.", 'abstract_zh': 'OpenAI的一项研究表明，在推理过程中直接应用强化学习整合推理步骤可以显著提高模型的推理能力。这一结果非常令人振奋，因为它标志着领域从传统的自回归回答生成方法向通过逐步推理训练来模拟慢思考过程的更 deliberate 接近。强化学习在模型的训练和解码过程中扮演着关键角色。在本文中，我们对推理问题进行了全面的阐述，并探讨了基于模型和非基于模型方法的应用，以更好地支持这一慢思考框架。', 'title_zh': 'LLM推理教程：ChatGPT背后的相关方法;o1\n\n注：这里的"o1"似乎是原文的一部分，由于没有上下文，我保留了这一部分。如果你能提供更多上下文信息，我可以进一步优化翻译。在学术规范中，标题和摘要应简洁明了，并准确反映内容。如果您需要对内容进行更详细的翻译或修改，请告知。'}
{'arxiv_id': 'arXiv:2502.10928', 'title': 'Semantic Specialization in MoE Appears with Scale: A Study of DeepSeek R1 Expert Specialization', 'authors': 'Matthew Lyle Olson, Neale Ratzlaff, Musashi Hinck, Man Luo, Sungduk Yu, Chendi Xue, Vasudev Lal', 'link': 'https://arxiv.org/abs/2502.10928', 'abstract': "DeepSeek-R1, the largest open-source Mixture-of-Experts (MoE) model, has demonstrated reasoning capabilities comparable to proprietary frontier models. Prior research has explored expert routing in MoE models, but findings suggest that expert selection is often token-dependent rather than semantically driven. Given DeepSeek-R1's enhanced reasoning abilities, we investigate whether its routing mechanism exhibits greater semantic specialization than previous MoE models. To explore this, we conduct two key experiments: (1) a word sense disambiguation task, where we examine expert activation patterns for words with differing senses, and (2) a cognitive reasoning analysis, where we assess DeepSeek-R1's structured thought process in an interactive task setting of DiscoveryWorld. We conclude that DeepSeek-R1's routing mechanism is more semantically aware and it engages in structured cognitive processes.", 'abstract_zh': 'DeepSeek-R1 是迄今为止最大的开源 Mixture-of-Experts (MoE) 模型，其表现出与 proprietary 前沿模型相媲美的推理能力。先前的研究探讨了 MoE 模型中的专家路由机制，但研究结果表明，专家选择往往是基于 token 而非语义驱动的。鉴于 DeepSeek-R1 的增强推理能力，我们研究其路由机制是否比之前的 MoE 模型展示出更大的语义专一性。为了探索这一问题，我们进行了两项关键实验：(1) 一个词义消歧任务，我们检查了不同词义的词语在专家激活模式中的表现；(2) 一个认知推理分析，我们评估了 DeepSeek-R1 在 DiscoveryWorld 的互动任务环境中结构化的思维过程。我们得出结论，DeepSeek-R1 的路由机制更加具有语义意识，并且能够进行结构化的认知过程。', 'title_zh': '随着模型规模的扩大，MoE中的语义专业化现象出现：对DeepSeek R1专家专业化的研究'}
{'arxiv_id': 'arXiv:2502.10867', 'title': 'A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1', 'authors': 'Jun Wang', 'link': 'https://arxiv.org/abs/2502.10867', 'abstract': "OpenAI o1 has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.", 'abstract_zh': 'OpenAI o1展示了在推理过程中直接应用强化学习整合推理步骤，可以显著提升模型的推理能力。这一结果令人兴奋，因为研究领域正从传统的自回归方法生成答案，转向一种更慎重的方法，通过逐步推理训练来模拟慢思考过程。强化学习在模型的训练和解码过程中都起到了关键作用。在本文中，我们提出了推理问题的全面框架，并探讨了如何更好地使用基于模型的方法和基于策略的方法来支持这一慢思考框架。', 'title_zh': 'LLM推理教程：ChatGPT背后的相关方法概述'}
{'arxiv_id': 'arXiv:2502.10858', 'title': 'Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs', 'authors': 'Zongqian Wu, Tianyu Li, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng', 'link': 'https://arxiv.org/abs/2502.10858', 'abstract': 'Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term \\textit{breadth reasoning}. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning. Our code is provided in this https URL.', 'abstract_zh': '深度迭代链式思考（CoT）推理使大规模语言模型（LLMs）能够通过逐步激活相关的预训练知识来应对复杂的任务。然而，这种方法在确保持续改进和确定停止标准方面面临挑战。在本文中，我们研究了是否可以从初始推理路径直接激活与解决给定问题相关的知识，从而避免迭代优化的需要。我们的实验表明，增加初始推理路径的多样性可以实现可比较甚至更优越的性能，这是一种我们称为“宽路径推理”的概念。然而，现有的宽路径推理方法，如自我一致性，提供的多样性有限。为了解决这一限制，我们提出了一种简单而有效的方法，通过结合上下文探索和减少采样随机性来增强推理的广度。大量实验证明，我们的方法在深度迭代推理方面表现显著更优。我们的代码可以在以下链接中获取：https://github.com/your-repo-name。', 'title_zh': 'Depth 是否足以".$_结束翻译？这里是更为自然的翻译建议：深度学习是否足以？LLMs 中迭代推理的探索'}
{'arxiv_id': 'arXiv:2502.10858', 'title': 'Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs', 'authors': 'Zongqian Wu, Tianyu Li, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng', 'link': 'https://arxiv.org/abs/2502.10858', 'abstract': 'Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term \\textit{breadth reasoning}. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning. Our code is provided in this https URL.', 'abstract_zh': '深度迭代链式思考（CoT）推理使大规模语言模型（LLM）能够通过逐步激活相关预训练知识来应对复杂的任务。然而，这种方法在确保持续改进和确定停止标准方面面临挑战。本文探讨了是否可以从初始推理路径中直接激活对解决给定问题有贡献的相关知识，从而避免迭代改进的需要。我们的实验表明，增加初始推理路径的多样性可以达到相近或更优的性能，我们将其称为“宽泛推理”。然而，现有的宽泛推理方法，如自我一致性，提供的多样性有限。为了解决这一局限，我们提出了一种简单而有效的方法，通过结合上下文探索和减少采样随机性来增强推理的宽泛性。广泛的实验证明，我们的方法显著优于深度迭代推理。我们的代码可在以下链接获得：[此链接]。', 'title_zh': '《仅需深度吗？大规模语言模型中迭代推理的探索》\n\n这个标题翻译成中文时，尽量保持了原始英文的结构和意思，同时符合中文的表达习惯。如果是正式的学术论文，可以根据具体的需求和格式要求进行适当的调整。'}
{'arxiv_id': 'arXiv:2502.10768', 'title': 'Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)', 'authors': 'Sandra Schaftner', 'link': 'https://arxiv.org/abs/2502.10768', 'abstract': "Current research highlights the great potential of Large Language Models (LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly complex step in this process is relation extraction, aimed at identifying suitable properties to describe the content of research. This study builds directly on previous research of three Open Research Knowledge Graph (ORKG) team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and Mistral for property extraction in scientific literature. Given the moderate performance observed, the previous work concluded that fine-tuning is needed to improve these models' alignment with scientific tasks and their emulation of human expertise. Expanding on this prior experiment, this study evaluates the impact of advanced prompt engineering techniques and demonstrates that these techniques can highly significantly enhance the results. Additionally, this study extends the property extraction process to include property matching to existing ORKG properties, which are retrieved via the API. The evaluation reveals that results generated through advanced prompt engineering achieve a higher proportion of matches with ORKG properties, further emphasizing the enhanced alignment achieved. Moreover, this lays the groundwork for addressing challenges such as the inconsistency of ORKG properties, an issue highlighted in prior studies. By assigning unique URIs and using standardized terminology, this work increases the consistency of the properties, fulfilling a crucial aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in turn, significantly enhances the applicability of ORKG content for subsequent tasks such as comparisons of research publications. Finally, the study concludes with recommendations for future improvements in the overall property extraction process.", 'abstract_zh': '当前的研究强调了大型语言模型（LLMs）在构建学术知识图谱（SKGs）方面的巨大潜力。这一过程中特别复杂的一步是关系提取，旨在识别适合描述研究内容的属性。本研究直接基于三位开放研究知识图谱（ORKG）团队成员之前的研究，他们评估了GPT-3.5、Llama 2和Mistral等模型在科学文献中属性提取的准备情况。鉴于观察到的中等性能，前人研究得出结论，需要进一步调整这些模型以提高它们与科学任务的对齐以及模仿人类专业知识的能力。在此前实验的基础上，本研究评估了高级提示工程技术的影响，并证明这些技术可以显著提高效果。此外，本研究还将属性提取过程扩展到包括与现有ORKG属性的匹配，这些属性是通过API检索的。评估结果表明，通过高级提示工程技术生成的结果更多地与ORKG属性匹配，进一步突显了这种增强的对齐。此外，这为解决ORKG属性的一致性问题奠定了基础，这是一个之前研究中指出的问题。通过分配唯一的URI并使用标准化术语，本研究增加了属性的一致性，满足了链接数据和FAIR原则的核心承诺——这是ORKG的重要方面。这反过来极大地提高了ORKG内容在后续任务（如研究出版物的比较）中的适用性。最后，本研究提出了对未来整体属性提取过程改进的建议。', 'title_zh': '评估将大型语言模型（LLMs）用于开放研究知识图谱（ORKG）中的属性提取改进效果'}
{'arxiv_id': 'arXiv:2502.10768', 'title': 'Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)', 'authors': 'Sandra Schaftner', 'link': 'https://arxiv.org/abs/2502.10768', 'abstract': "Current research highlights the great potential of Large Language Models (LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly complex step in this process is relation extraction, aimed at identifying suitable properties to describe the content of research. This study builds directly on previous research of three Open Research Knowledge Graph (ORKG) team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and Mistral for property extraction in scientific literature. Given the moderate performance observed, the previous work concluded that fine-tuning is needed to improve these models' alignment with scientific tasks and their emulation of human expertise. Expanding on this prior experiment, this study evaluates the impact of advanced prompt engineering techniques and demonstrates that these techniques can highly significantly enhance the results. Additionally, this study extends the property extraction process to include property matching to existing ORKG properties, which are retrieved via the API. The evaluation reveals that results generated through advanced prompt engineering achieve a higher proportion of matches with ORKG properties, further emphasizing the enhanced alignment achieved. Moreover, this lays the groundwork for addressing challenges such as the inconsistency of ORKG properties, an issue highlighted in prior studies. By assigning unique URIs and using standardized terminology, this work increases the consistency of the properties, fulfilling a crucial aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in turn, significantly enhances the applicability of ORKG content for subsequent tasks such as comparisons of research publications. Finally, the study concludes with recommendations for future improvements in the overall property extraction process.", 'abstract_zh': '当前的研究突显了大型语言模型（LLMs）在构建学术知识图谱（SKGs）方面巨大的潜力。这一过程中的一个特别复杂的步骤是关系提取，其目标是识别描述研究内容的合适属性。本研究直接建立在previous研究的基础上，该研究评估了三个Open Research Knowledge Graph（ORKG）团队成员对GPT-3.5、Llama 2和Mistral等模型在科学文献中提取属性的准备情况。鉴于观察到的中等性能，前人研究得出结论，需要进行微调以改进这些模型与科学任务的对齐以及模拟人类专业知识的能力。在此前实验的基础上，本研究评估了高级提示工程技巧的影响，并证明这些技巧可以显著提高结果。此外，本研究还将属性提取过程扩展到包括与现有ORKG属性的匹配，这些属性通过API检索。评估结果显示，通过高级提示工程生成的结果与ORKG属性的匹配比例更高，进一步突显了增强的对齐水平。此外，这为解决ORKG属性的一致性问题奠定了基础，这是先前研究中指出的问题之一。通过分配唯一的URI并使用标准化术语，本文增加了属性的一致性，满足了ORKG的核心承诺之一——链接数据和FAIR原则的核心要素。这反过来大大增强了ORKG内容在未来任务中的适用性，如研究出版物的比较。最后，本研究提出了在未来改进属性提取过程的建议。', 'title_zh': '评估在开放研究知识图谱（ORKG）中使用大型语言模型（LLMs）进行属性提取的改进效果'}
{'arxiv_id': 'arXiv:2502.10762', 'title': 'Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation', 'authors': 'Guofu Xie, Xiao Zhang, Ting Yao, Yunsheng Shi', 'link': 'https://arxiv.org/abs/2502.10762', 'abstract': 'User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.', 'abstract_zh': '用户信息需求常常高度多样且各异。当前研究中的一个重要挑战是如何在实现可控多目标生成的同时，能够快速适应测试过程中用户多样化的具体需求。现有解决方案，如Rewarded Soup，侧重于将单目标微调过的语言模型合并。尽管这些方法易于实现且被广泛使用，但它们在实现最佳性能方面存在局限性，因为它们忽略了竞争目标对模型微调的影响。为解决这个问题，我们提出了Bone Soup，这是一种新颖的模型合并方法，首先通过考虑多种目标的影响来寻求一系列骨干模型，然后将这些骨干模型合并（即，合并骨干模型）。具体而言，Bone Soup首先是利用多目标强化学习对不同目标进行多次训练，从而训练出多个骨干模型。每个骨干模型都由一组骨干奖励信号引导。为了确保这些模型在帕累托前沿上是最佳的，骨干奖励通过将标准奖励函数组合成基础向量创建，并可以通过基于规则的构建方法进行修改。Bone Soup利用对称循环矩阵映射生成合并系数，这些系数根据用户偏好合并骨干模型。大量实验结果表明，Bone Soup在可控多目标生成中表现出较强的可控性和帕累托最优性，提供了一种更为有效和高效的方法来解决测试过程中多样化的用户需求。', 'title_zh': '骨头汤：一种求解和煮汤模型集成方法，用于可控多目标生成'}
{'arxiv_id': 'arXiv:2502.10762', 'title': 'Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation', 'authors': 'Guofu Xie, Xiao Zhang, Ting Yao, Yunsheng Shi', 'link': 'https://arxiv.org/abs/2502.10762', 'abstract': 'User information needs are often highly diverse and varied. A key challenge in current research is how to achieve controllable multi-objective generation while enabling rapid adaptation to accommodate diverse user demands during test time. Existing solutions, such as Rewarded Soup, focus on merging language models individually tuned on single objectives. While easy to implement and widely used, these approaches face limitations in achieving optimal performance due to their disregard for the impacts of competing objectives on model tuning. To address this issue, we propose Bone Soup, a novel model merging approach that first seeks a series of backbone models by considering the impacts of multiple objectives and then makes the soup (i.e., merge the backbone models). Specifically, Bone Soup begins by training multiple backbone models for different objectives using multi-objective reinforcement learning. Each backbone model is guided by a combination of backbone reward signals. To ensure that these models are optimal for the Pareto front, the backbone rewards are crafted by combining standard reward functions into basis vectors, which can then be modified through a rule-based construction method. Bone Soup leverages a symmetric circulant matrix mapping to generate the merging coefficients, which are used to merge the backbone models according to user preferences. Extensive experimental results demonstrate that Bone Soup exhibits strong controllability and Pareto optimality in controllable multi-objective generation, providing a more effective and efficient approach to addressing diverse user needs at test time.', 'abstract_zh': '用户信息需求往往高度多样化。当前研究的一个关键挑战是如何在保证模型快速适应多样化用户需求的同时，实现可控的多目标生成。现有的解决方案，如Rewarded Soup，专注于合并分别针对单一目标调优的语言模型。虽然这些方法容易实现且广泛应用，但它们在实现最优性能方面存在局限性，因为它们忽略了多种目标之间相互影响对模型调优的影响。为了解决这一问题，我们提出了一种新型的模型合并方法Bone Soup，该方法首先通过考虑多种目标的影响来寻求一系列骨干模型，然后进行合并（即合并骨干模型）。具体而言，Bone Soup 方法首先利用多目标强化学习训练针对不同目标的多个骨干模型。每个骨干模型都由一系列骨干奖励信号引导。为确保这些模型在帕累托前沿最优，通过将标准奖励函数组合成基向量，并通过基于规则的构造方法进行修改来设计骨干奖励。Bone Soup 利用对称循环矩阵映射生成合并系数，根据用户偏好合并骨干模型。广泛实验结果表明，Bone Soup 在可控多目标生成中表现出较强的可控性和帕累托最优性，提供了一种更有效、更高效的解决方案，以应对测试时间的多样化用户需求。', 'title_zh': '骨汤模型：一种用于可控多目标生成的寻觅与汤合并方法'}
{'arxiv_id': 'arXiv:2502.10673', 'title': 'Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs', 'authors': 'Yepeng Liu, Xuandong Zhao, Dawn Song, Yuheng Bu', 'link': 'https://arxiv.org/abs/2502.10673', 'abstract': 'Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.', 'abstract_zh': '检索增强生成（RAG）已经成为一种有效的方法，用于通过引入最新的知识来增强大型语言模型（LLMs）。然而，这也会带来知识产权侵权的重大风险，因为在未经授权的情况下，恶意的检索增强语言模型（RA-LLMs）可能会将IP数据集纳入其知识数据库中。为了保护数据集所有者的权利，需要一种有效的数据集成员推理算法来检测RA-LLMs的未经授权使用。在本项工作中，我们介绍了一种新型方法，旨在保护文本数据集的所有权并有效检测RA-LLMs的未经授权使用。我们的方法完全保留了原始数据不变，通过在IP数据集中插入精心设计的金丝雀文档来保护数据。这些金丝雀文档由合成内容和嵌入的隐形水印组成，以确保其唯一性、隐蔽性和统计可证明性。在检测过程中，通过查询金丝雀文档并分析RA-LLMs的响应来统计推断已嵌入水印的存在，从而识别未经授权使用。实验结果表明，该方法在保持查询效率、可检测性和隐蔽性的同时，对原始数据的干扰极小，且不会影响RAG系统的性能。', 'title_zh': '通过水印 Canary 技术保护数据集在检索增强大语言模型中的使用'}
{'arxiv_id': 'arXiv:2502.10673', 'title': 'Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs', 'authors': 'Yepeng Liu, Xuandong Zhao, Dawn Song, Yuheng Bu', 'link': 'https://arxiv.org/abs/2502.10673', 'abstract': 'Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.', 'abstract_zh': '检索增强生成（RAG）已成为增强大规模语言模型（LLMs）的最新知识的有效方法。然而，它也带来了重大的知识产权侵权风险，因为恶意的检索增强语言模型（RA-LLMs）可能未经授权将知识产权数据集纳入其知识数据库。为了保护数据集所有者的权益，需要有效的一组数据成员推断算法来防止RA-LLMs的未经授权使用。在本文中，我们介绍了一种新的方法，以保护文本数据集的所有权并有效检测RA-LLMs的未经授权使用。我们的方法在保持原始数据完全不变的同时，通过将专门设计的金丝雀文档插入IP数据集中来保护数据。这些金丝雀文档包含合成内容和嵌入式水印，以确保其独特性、隐蔽性和统计证明性。在检测过程中，未经授权的使用通过查询金丝雀文档并分析RA-LLMs的响应来识别，以寻找嵌入水印的统计证据。我们的实验结果证明了高查询效率、可检测性和隐蔽性，且对原始数据的干扰最小，同时不对RAG系统的性能产生影响。', 'title_zh': '基于检索增强语言模型中的水印哨兵的数据集保护'}
{'arxiv_id': 'arXiv:2502.10563', 'title': 'Accelerating Unbiased LLM Evaluation via Synthetic Feedback', 'authors': 'Zhaoyi Zhou, Yuda Song, Andrea Zanette', 'link': 'https://arxiv.org/abs/2502.10563', 'abstract': 'When developing new large language models (LLMs), a key step is evaluating their final performance, often by computing the win-rate against a reference model based on external feedback. Human feedback is the gold standard, particularly for capturing nuanced qualities like coherence, readability, and alignment with human expectations. However, human evaluations are costly -- even for large tech companies -- and when conducted with active users, they may negatively impact user experience. A promising alternative is synthetic feedback, where evaluations are conducted by other large language models, including reward models. While this eliminates the need for costly human annotations, it introduces biases that may distort the evaluation process. In this work, we propose a statistically principled framework that integrates human and synthetic feedback to reduce reliance on human annotations while maintaining unbiased win-rate calculations. Our experiments demonstrate a reduction in human annotations by up to 12.2% with an off-the-shelf synthetic evaluator and up to 24.8% with a finetuned variant. Apart from being generalizable, scalable, and free of hyper-parameter tuning, our method offers predictable annotation savings, which can be estimated based on data-dependent characteristics.', 'abstract_zh': '在开发新的大规模语言模型（LLMs）时，一个关键步骤是评估其最终性能，通常通过计算其相对于参考模型的表现率来实现，基于外部反馈。人类反馈是黄金标准，特别是在捕捉诸如连贯性、可读性和与人类期望的一致性等细微品质方面。然而，人类评估代价高昂，即使是大型科技公司也会面临成本问题；当由活跃用户参与评估时，还可能负面地影响用户体验。一种有前景的替代方案是合成反馈，其中评估是由其他大规模语言模型，包括奖励模型进行的。尽管这种方法消除了人工注释的成本需求，但它引入了可能扭曲评估过程的偏差。在本研究中，我们提出了一种统计上遵循原则的框架，将人类反馈和合成反馈结合起来，以减少对人工注释的依赖，同时保持无偏差的表现率计算。我们的实验显示，使用现成的合成评估者，人类注释可减少多达12.2%；使用微调版本，可减少高达24.8%。除了可以通用、扩展且无需超参数调整外，我们的方法还提供了可预测的注释节省，其节省量可以基于数据依赖的特征来估算。', 'title_zh': '通过合成反馈加速无偏大型语言模型评估'}
{'arxiv_id': 'arXiv:2502.10563', 'title': 'Accelerating Unbiased LLM Evaluation via Synthetic Feedback', 'authors': 'Zhaoyi Zhou, Yuda Song, Andrea Zanette', 'link': 'https://arxiv.org/abs/2502.10563', 'abstract': 'When developing new large language models (LLMs), a key step is evaluating their final performance, often by computing the win-rate against a reference model based on external feedback. Human feedback is the gold standard, particularly for capturing nuanced qualities like coherence, readability, and alignment with human expectations. However, human evaluations are costly -- even for large tech companies -- and when conducted with active users, they may negatively impact user experience. A promising alternative is synthetic feedback, where evaluations are conducted by other large language models, including reward models. While this eliminates the need for costly human annotations, it introduces biases that may distort the evaluation process. In this work, we propose a statistically principled framework that integrates human and synthetic feedback to reduce reliance on human annotations while maintaining unbiased win-rate calculations. Our experiments demonstrate a reduction in human annotations by up to 12.2% with an off-the-shelf synthetic evaluator and up to 24.8% with a finetuned variant. Apart from being generalizable, scalable, and free of hyper-parameter tuning, our method offers predictable annotation savings, which can be estimated based on data-dependent characteristics.', 'abstract_zh': '在开发新的大规模语言模型（LLMs）时，一个关键步骤是评估其最终性能，通常是通过计算相对于参考模型的胜率来完成的，这通常基于外部反馈。人类反馈是黄金标准，特别是对于捕捉细腻的质量，如连贯性、可读性和与人类预期的对齐。然而，人类评估成本高昂——即使是大型科技公司也会发现进行此类评估代价不菲，并且由活跃用户参与时，可能会负面影响用户体验。一种有前景的替代方法是合成反馈，即通过其他大规模语言模型（包括奖励模型）来进行评估。虽然这种方法消除了对昂贵的人工注释的需求，但它可能引入偏差，从而扭曲评估过程。在此项工作中，我们提出了一种统计上具有原理性的框架，该框架结合了人类和合成反馈，以减少对人工注释的依赖性，同时保持无偏的胜率计算。我们的实验表明，使用开箱即用的合成评估器，人类注释可减少多达12.2%，而使用微调变体则可减少多达24.8%。除了通用性、可扩展性和无需超参数调整之外，我们的方法还提供了可预测的注释节省，这些节省可以基于数据相关特征进行估算。', 'title_zh': '通过合成反馈加速无偏大型语言模型评估'}
{'arxiv_id': 'arXiv:2502.10505', 'title': 'Preference learning made easy: Everything should be understood through win rate', 'authors': 'Lily H. Zhang, Rajesh Ranganath', 'link': 'https://arxiv.org/abs/2502.10505', 'abstract': "Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.", 'abstract_zh': '偏好学习，或者说将生成模型与偏好比较数据对齐的任务，尚未达到分类、密度估计等概念上的成熟度。为弥补这一差距，本文提出了一种从成对偏好数据的采样分布出发理解偏好学习的框架。首先，我们证明了唯一一种同时尊重数据分布中偏好和频次的生成模型评估方法是一种胜率形式，这为从胜率角度理解偏好学习提供了依据。接着，我们将偏好学习方法分为胜率优化（WRO）或非WRO两类。我们介绍了超越现有实例（如RLHF、NLHF）的新颖的WRO实例，并指出了所有这些方法的两个关键理论优势。我们证明了如DPO和SFT等常见的非WRO方法缺乏这些特性，并提出了减轻此类理论限制的方法。我们还表明，由于优化困难，WRO在实践中表现不佳，而优化的成功预测了性能比影响目标解的选择更为有效。我们的分析总结了现有方法的最佳实践，并为未来研究提供了建议，遵循的原则是要么更接近地对齐非WRO方法与WRO，要么改进WRO目标的优化。', 'title_zh': '将下面的论文内容或标题翻译成中文，符合学术规范后可以表达为：\n\n“简便的偏好学习：一切应该通过胜率来理解”'}
{'arxiv_id': 'arXiv:2502.10505', 'title': 'Preference learning made easy: Everything should be understood through win rate', 'authors': 'Lily H. Zhang, Rajesh Ranganath', 'link': 'https://arxiv.org/abs/2502.10505', 'abstract': "Preference learning, or the task of aligning generative models to preference comparison data, has yet to reach the conceptual maturity of classification, density estimation, etc. To close this gap, this work presents a framework to understand preference learning starting from the sampling distribution of pairwise preference data. First, we prove that the only evaluation of a generative model that respects both preferences and prevalences in the data distribution is a form of win rate, justifying win rate as the focal point to understand preference learning. We then analyze preference learning methods as win rate optimization (WRO) or non-WRO. We present novel instances of WRO beyond existing examples (RLHF, NLHF) and identify two key theoretical benefits of all such methods. We prove that common non-WRO methods like DPO and SFT on preferred samples lack these properties and suggest ways to mitigate such theoretical limitations. We also show that WRO underperforms in practice due optimization difficulties and that optimization success predicts performance better than choices which affect the objective's solution. Our analysis highlights best practices for existing methods and provides recommendations for future research, guided by the principle that one should either align non-WRO methods more closely with WRO or improve the optimization of WRO objectives.", 'abstract_zh': '偏好学习，即对齐生成模型以匹配偏好比较数据的任务，尚未达到分类、密度估计等概念上的成熟度。为弥合这一差距，本文提出了一种框架，从成对偏好数据的采样分布出发，理解偏好学习。首先，我们证明了唯一一种同时尊重数据分布中偏好和出现频率的生成模型的评估是胜率形式，这证明了胜率是理解偏好学习的关键点。然后，我们分析了偏好学习方法的胜率优化（WRO）或非WRO性质。我们提出了超越现有示例（RLHF、NLHF）的新型WRO实例，并识别出所有此类方法的两个关键理论优势。我们证明了常见的非WRO方法（如DPO和SFT在优先样本上的应用）缺乏这些特性，并提供了减轻这些理论限制的方法。我们还展示出，由于优化困难，WRO在实践中表现欠佳，而优化成功比影响目标解的选择更能预测性能。我们的分析强调了现有方法的最佳实践，并为未来的研究提供了指导，这一原则是，要么更紧密地对齐非WRO方法与WRO，要么改进WRO目标的优化。', 'title_zh': '轻松实现偏好学习：一切应当通过胜率来理解'}
{'arxiv_id': 'arXiv:2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'authors': 'Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'abstract_zh': '利用大规模语言模型（LLMs）生成数学证明是LLMs研究中的一个基本课题。我们认为，当前LLMs证明命题的能力很大程度上取决于它们在训练过程中是否接触过相关的证明过程。这种依赖性限制了它们对数学定理及相关概念的深入理解。受人类数学教育中广泛使用的“反例证明”教学方法的启发，我们的工作旨在通过反例增强LLMs进行数学推理和证明的能力。具体而言，我们手动创建了一个高质量的大学水平数学基准——CounterMATH，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的理解。此外，我们开发了一个数据工程框架，以自动获取进一步模型改进所需的训练数据。广泛的实验和详细的分析表明，CounterMATH具有挑战性，显示出诸如OpenAI的o1这样的LLMs缺乏反例驱动的证明能力。此外，我们对模型训练的研究表明，加强LLMs的反例驱动的概念推理能力对于提高其整体数学能力至关重要。我们相信，我们的工作为数学LLMs的社区提供了新的视角。', 'title_zh': '一个实例展示，多种概念掌握！基于反例的概念化推理在数学大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'authors': 'Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'abstract_zh': '利用数学大规模语言模型（LLMs）进行证明生成是LLMs研究中的一个基本课题。我们认为，当前LLMs能够证明命题的能力很大程度上取决于它们在训练过程中是否接触过相关的证明过程。这种依赖限制了它们对数学定理和相关概念的深入理解。受人类数学教育中常用的“反例证明法”教学方法的启发，我们的工作旨在通过反例来增强LLMs的数学推理和证明能力。具体来说，我们手动创建了一个高质量的大学水平数学基准TestMATH，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的理解程度。此外，我们还开发了一种数据工程框架，以自动获取进一步模型改进所需的训练数据。广泛的实验和详细的分析表明，TestMATH具有挑战性，这表明像OpenAI的模型等LLMs缺乏反例驱动的证明能力。此外，我们对模型训练的探索表明，增强LLMs的反例驱动概念推理能力对于提高其整体数学能力至关重要。我们认为，我们的工作为数学LLMs社区提供了新的视角。', 'title_zh': '一个实例展现，多个概念掌握！基于反例的概念驱动推理在数学大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.10453', 'title': 'Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach', 'authors': 'Régnier Avice, Bernhard Haslhofer, Zhidong Li, Jianlong Zhou', 'link': 'https://arxiv.org/abs/2502.10453', 'abstract': 'Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in F1-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve F1-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.', 'abstract_zh': 'attribution 标签构成了现代加密资产取证的基础。然而，不一致或错误的标签可能会误导调查，甚至导致误判。为了解决这一问题，我们提出了一种基于大规模语言模型（LLMs）的新计算方法，用于将 attribution 标签与明确的知识图谱概念关联起来。我们在端到端的流水线中实现了这一方法，并进行了实验，结果显示，与基线方法相比，我们的方式在三个公开可用的 attribution 标签数据集中，F1 分数提高了最多 37.4%。通过结合概念筛选和阻止程序，我们生成了包含五个知识图谱实体的候选集，在无需标记数据的情况下，召回率达到 93%。此外，我们证明局部 LLM 模型可以实现 90% 的 F1 分数，与远程模型实现的 94% 的 F1 分数相当。我们还分析了各种 LLM 和提示模板的成本与性能权衡，表明选择最具成本效益的配置可以将成本降低 90%，同时性能仅下降 1%。我们的方法不仅提升了 attribution 标签的质量，还为促进更可靠的取证证据提供了一个蓝图。', 'title_zh': '基于LLM的方法：将加密资产归属标签链接到知识图实体'}
{'arxiv_id': 'arXiv:2502.10453', 'title': 'Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach', 'authors': 'Régnier Avice, Bernhard Haslhofer, Zhidong Li, Jianlong Zhou', 'link': 'https://arxiv.org/abs/2502.10453', 'abstract': 'Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in F1-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve F1-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.', 'abstract_zh': '属性标签是现代加密资产取证的基础。然而，不一致或不正确的标签可能会误导调查，甚至导致错误指控。为了解决这一问题，我们提出了一种基于大规模语言模型（LLMs）的新型计算方法，用于将属性标签与定义明确的知识图谱概念相链接。我们在此方法中构建了一个端到端的工作流程，并进行了实验，结果显示，我们的方法在三个公开的属性标签数据集上的F1分数上比基线方法高出37.4%。通过整合概念过滤和阻止过程，我们生成了包含五个知识图谱实体的候选集，无需标记数据即可实现93%的召回率。此外，我们展示了本地LLM模型可以达到90%的F1分数，与94%的远程模型相当。我们还分析了各种LLM模型和提示模板的成本效益权衡，结果显示，选择最经济有效的配置可以降低90%的成本，同时性能减少1%。我们的方法不仅提高了属性标签的质量，还为促进更可靠的取证证据提供了蓝图。', 'title_zh': '基于语言模型的方法将加密资产归属标签链接到知识图谱实体'}
{'arxiv_id': 'arXiv:2502.10450', 'title': 'Trustworthy AI on Safety, Bias, and Privacy: A Survey', 'authors': 'Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim', 'link': 'https://arxiv.org/abs/2502.10450', 'abstract': "The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the current state of the field, and present promising insights and perspectives regarding concerns that challenge the trustworthiness of AI models. In particular, this paper investigates the issues regarding three thrusts: safety, privacy, and bias, which hurt models' trustworthiness. For safety, we discuss safety alignment in the context of large language models, preventing them from generating toxic or harmful content. For bias, we focus on spurious biases that can mislead a network. Lastly, for privacy, we cover membership inference attacks in deep neural networks. The discussions addressed in this paper reflect our own experiments and observations.", 'abstract_zh': '人工智能系统的功能得到了极大地发展，但仍存在故障模式、脆弱性和偏见等问题。本文研究了该领域的现状，并提出了关于挑战人工智能模型可信性的关切问题的前景和见解。特别是，本文探讨了三个重点领域的相关问题：安全性、隐私和偏见，这些问题损害了模型的可信性。在安全性方面，我们讨论了大型语言模型的安全对齐问题，防止它们生成有毒或有害的内容。在偏见方面，我们关注表面上无根据的偏见，这些偏见可能会误导网络。最后，在隐私方面，我们探讨了深度神经网络中的成员身份推理攻击。本文中的讨论反映了我们自己的实验和观察结果。', 'title_zh': '可信的人工智能：关于安全、偏见和隐私的综述'}
{'arxiv_id': 'arXiv:2502.10450', 'title': 'Trustworthy AI on Safety, Bias, and Privacy: A Survey', 'authors': 'Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim', 'link': 'https://arxiv.org/abs/2502.10450', 'abstract': "The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the current state of the field, and present promising insights and perspectives regarding concerns that challenge the trustworthiness of AI models. In particular, this paper investigates the issues regarding three thrusts: safety, privacy, and bias, which hurt models' trustworthiness. For safety, we discuss safety alignment in the context of large language models, preventing them from generating toxic or harmful content. For bias, we focus on spurious biases that can mislead a network. Lastly, for privacy, we cover membership inference attacks in deep neural networks. The discussions addressed in this paper reflect our own experiments and observations.", 'abstract_zh': '人工智能系统的功能取得了显著进展，但仍面临着故障模式、脆弱性和偏见等挑战。本文研究了该领域的现状，并提出了关于影响AI模型可信度的诸多关切的有希望的见解和视角。特别是，本文探讨了涉及三个关键领域的安全、隐私和偏见方面的问题，这些问题影响了模型的可信度。在安全方面，我们讨论了大型语言模型中的安全对齐问题，防止其生成有毒或有害的内容。在偏见方面，我们关注那些可能导致网络误判的虚假偏见。最后，在隐私方面，我们讨论了深度神经网络中的成员推断攻击。本论文中的讨论反映了我们自己的实验和观察结果。', 'title_zh': '可信AI在安全、偏见和隐私方面的研究综述'}
{'arxiv_id': 'arXiv:2502.10447', 'title': 'MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition', 'authors': 'Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun', 'link': 'https://arxiv.org/abs/2502.10447', 'abstract': 'Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.', 'abstract_zh': '音频-视觉言语识别（AVSR）已成为在嘈杂环境中增强语音识别的关键技术，通过整合听觉和视觉两种模态。然而，现有的AVSR系统在大规模应用时往往难以在保持计算效率的同时提升性能。本研究介绍了一种新型的鲁棒性AVSR框架——MoHAVE（混合层次音频-视觉专家系统），旨在解决这些可扩展性限制。通过利用Mixture-of-Experts（MoE）架构，MoHAVE能够激活特定模态的专家组，确保对各种音频-视觉输入进行动态适应，同时保持较低的计算开销。MoHAVE的主要贡献包括：（1）一种稀疏的MoE框架，能够有效扩展AVSR模型的能力；（2）一种层级门控机制，依据输入上下文动态利用专家组，增强系统的适应性和鲁棒性；（3）在LRS3和MuAViC转录及翻译任务等鲁棒性AVSR基准上表现出色，为可扩展的语音识别系统设定了新的标准。', 'title_zh': 'MoHAVE：多层次视听专家混合模型在鲁棒语音识别中的应用'}
{'arxiv_id': 'arXiv:2502.10447', 'title': 'MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition', 'authors': 'Sungnyun Kim, Kangwook Jang, Sangmin Bae, Sungwoo Cho, Se-Young Yun', 'link': 'https://arxiv.org/abs/2502.10447', 'abstract': 'Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and visual modalities. However, existing AVSR systems struggle to scale up without compromising computational efficiency. In this study, we introduce MoHAVE (Mixture of Hierarchical Audio-Visual Experts), a novel robust AVSR framework designed to address these scalability constraints. By leveraging a Mixture-of-Experts (MoE) architecture, MoHAVE activates modality-specific expert groups, ensuring dynamic adaptation to various audio-visual inputs with minimal computational overhead. Key contributions of MoHAVE include: (1) a sparse MoE framework that efficiently scales AVSR model capacity, (2) a hierarchical gating mechanism that dynamically utilizes the expert groups based on input context, enhancing adaptability and robustness, and (3) remarkable performance across robust AVSR benchmarks, including LRS3 and MuAViC transcription and translation tasks, setting a new standard for scalable speech recognition systems.', 'abstract_zh': '音频-视觉语音识别（AVSR）在通过结合听觉和视觉模态增强噪声环境下的语音识别方面变得至关重要。然而，现有的AVSR系统在扩大规模时往往难以保持计算效率。本研究介绍了MoHAVE（混合层次音频-视觉专家系统），这是一种新型的鲁棒AVSR框架，旨在解决这些可扩展性限制问题。通过利用Mixture-of-Experts（MoE）架构，MoHAVE激活特定模态的专家组，确保在各种音频-视觉输入下实现动态适应性，同时最小化计算开销。MoHAVE的关键贡献包括：（1）稀疏MoE框架，有效扩展AVSR模型容量，（2）分层门控机制，基于输入上下文动态利用专家组，增强适应性和鲁棒性，以及（3）在鲁棒AVSR基准测试中表现出色，包括LRS3和MuAViC转录和翻译任务，为可扩展的语音识别系统确立了新标准。', 'title_zh': 'MoHAVE: 嵌套混合音视频专家系统在鲁棒语音识别中的应用'}
{'arxiv_id': 'arXiv:2502.10440', 'title': 'Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning', 'authors': 'Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang', 'link': 'https://arxiv.org/abs/2502.10440', 'abstract': "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (\\eg, generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose \\name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \\name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) \\textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \\name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.", 'abstract_zh': '大型语言模型（LLMs）通过检索增强生成（RAG）机制越来越多地集成到实际应用中，以补充其响应的内容，使其包含最新的及领域特定的知识。然而，用于RAG的知识库往往具有宝贵的且常常是专有的性质，这也带来了被对手未授权使用的风险。现有的可以泛化的水印技术通常涉及污染攻击来保护这些知识库，但这些方法需要改变验证样本的结果（例如，生成错误输出），这不可避免地使它们变得容易被异常检测发现，甚至引入新的安全风险。为解决这些挑战，我们提出了一种名为\\name{}的方法，用于对知识库实施“无害”的版权保护。不同于篡改LLM的最终输出，\\name{}在思维链（CoT）推理的空间中植入独特的验证行为，同时保持最终答案的正确性。我们的方法包含三个主要阶段：（1）**生成思维链（CoTs）**：对于每个验证问题，我们生成两个思维链，包括一个用于构建水印行为的目标思维链；（2）**优化水印短语和目标思维链**：我们优化这些内容，以在恶意LLM的黑盒设置下最小化检索错误，确保被水印的验证查询会激活目标思维链，而非水印的查询不被激活；（3）**所有权验证**：我们利用Wilcoxon秩和检验的配对测试来统计验证可疑LLM是否添加了受保护的知识库，这是通过比较其对水印验证查询和良性验证查询的响应来实现的。实验结果表明，\\name{}在保护知识库不受未授权使用的同时，能够保持RAG的完整性和性能。', 'title_zh': '通过所有权验证与推理实现检索增强语言模型知识库的版权保护'}
{'arxiv_id': 'arXiv:2502.10440', 'title': 'Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning', 'authors': 'Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang', 'link': 'https://arxiv.org/abs/2502.10440', 'abstract': "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (\\eg, generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose \\name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \\name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) \\textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \\name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.", 'abstract_zh': '大型语言模型（LLMs）越来越多地通过检索增强生成（RAG）机制与现实世界应用集成，以补充其响应的最新和领域特定知识。然而，用于RAG的知识库往往具有宝贵且常为专有的性质，这引入了未经授权使用的风险。现有可以泛化的 watermarking 技术方法通常涉及投毒攻击。然而，这些方法需要修改验证样本的结果（例如，生成错误输出），从而不可避免地使其容易被异常检测甚至引入新的安全风险。为应对这些挑战，我们提出了一种名为 \\name{} 的方法，用于 `无害` 的知识库版权保护。该方法不同于直接修改LLM的最终输出，而是通过在思维链（CoT）推理的空间中植入不同的验证行为，来保持最终答案的正确性。我们的方法包括三个主要阶段：（1）**生成思维链（CoTs）**：对于每个验证问题，我们生成两个CoT，包括一个目标CoT用于构建水印行为；（2）**优化水印短语和目标CoTs**：在可疑LLM的黑盒设置下，我们优化这些短语以最小化检索错误，确保受保护知识库标记的验证查询激活目标CoTs，而未标记的验证查询不会激活；（3）**产权验证**：我们利用成对的Wilcoxon检验统计验证可疑LLM是否增强了受保护的知识库，通过对比其对受保护标记和良性验证查询的响应。我们在多种基准测试中的实验表明，\\name{} 有效地防止了未经授权使用知识库，同时保持了RAG的完整性和性能。', 'title_zh': '面向检索增强语言模型知识库的版权保护研究：基于推理的所有权验证方法'}
{'arxiv_id': 'arXiv:2502.10420', 'title': 'Position: Stop Acting Like Language Model Agents Are Normal Agents', 'authors': 'Elija Perrier, Michael Timothy Bennett', 'link': 'https://arxiv.org/abs/2502.10420', 'abstract': 'Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.', 'abstract_zh': '语言模型代理（LMAs）越来越多地被视为能够自主与人类和工具进行交互的能力。它们的设计和部署往往假定它们是具有持续目标的能力、能够在不同情境中适应，并展现出一定程度意图的普通代理。这些假设对于其在工业、社会和政府等领域的潜在应用至关重要。但LMAs并不是普通的代理。它们继承了围绕它们构建的大规模语言模型（LLMs）所固有的结构性问题：幻觉、脱笼、不一致和不可预测性。在本文中，我们主张不应将LMAs视为普通代理，因为这种假设会导致破坏它们效用和可信度的问题。我们列举了LMAs固有的代理病理。尽管有外部存储和工具的支持，它们仍然在本体上是无状态的、随机的、语义敏感的，并且通过语言中介化。这些病理干扰了LMAs的本体属性，包括可识别性、连续性、持久性和一致性，从而对其代理能力提出了质疑。为应对这一问题，我们认为在部署前、中、后应测量LMAs的本体属性，以便减轻病理所导致的负面影响。', 'title_zh': '标题：停止将语言模型代理视为正常代理'}
{'arxiv_id': 'arXiv:2502.10420', 'title': 'Position: Stop Acting Like Language Model Agents Are Normal Agents', 'authors': 'Elija Perrier, Michael Timothy Bennett', 'link': 'https://arxiv.org/abs/2502.10420', 'abstract': 'Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.', 'abstract_zh': '语言模型代理（LMAs）越来越多地被视为能够自主与人类和工具进行互动的实体。其设计和部署倾向于假定它们是正常代理，能够维持连贯的目标、在不同情境中适应，并且具有一定程度的意图性。这些假设对于工业、社会和政府领域的潜在应用至关重要。但是，LMAs 并不是正常的代理。它们继承了所基于的大规模语言模型（LLMs）所存在的结构问题：幻觉、逃逸、对齐不良和不可预测性。在本文中，我们认为不应该将 LMAs 视为正常代理，因为在这样做时会引发一系列问题，这些问题损害了其实用性和可信度。我们列举了 LMAs 本质上的代理病态。即使有外部记忆和工具的支持，它们仍然从本体论上是无状态、随机的、语义敏感且语言中介化的。这些病态动摇了 LMAs 的本体论属性，包括可识别性、连续性、持久性和一致性，从而质疑它们的代理权。因此，我们认为应在部署前、部署中和部署后测量 LMAs 的本体论属性，以便减轻这些病态的负面影响。', 'title_zh': '立场：停止将语言模型代理视为正常代理'}
{'arxiv_id': 'arXiv:2502.10413', 'title': 'Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering', 'authors': 'Raj Sonani, Lohalekar Prayas', 'link': 'https://arxiv.org/abs/2502.10413', 'abstract': 'Digital data continues to grow, there has been a shift towards using effective regulatory mechanisms to safeguard personal information. The CCPA of California and the General Data Protection Regulation (GDPR) of the European Union are two of the most important privacy laws. The regulation is intended to safeguard consumer privacy, but it varies greatly in scope, definitions, and methods of enforcement. This paper presents a fresh approach to adaptive compliance, using machine learning and emphasizing natural language processing (NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP, this study compares various regulations to identify areas where they overlap or diverge. This includes the "right to be forgotten" provision in the GDPR and the "opt-out of sale" provision under CCPA. International companies can learn valuable lessons from this report, as it outlines strategies for better enforcement of laws across different nations. Additionally, the paper discusses the challenges of utilizing NLP in legal literature and proposes methods to enhance the model-ability of machine learning models for studying regulations. The study\'s objective is to "bridge the gap between legal knowledge and technical expertise" by developing regulatory compliance strategies that are more efficient in operation and more effective in data protection.', 'abstract_zh': '数字数据持续增长，各国开始转向使用有效的监管机制来保护个人信息。《加利福尼亚州消费者隐私法案》（CCPA）和《欧盟一般数据保护条例》（GDPR）是两个最重要的隐私法规。这些法规旨在保护消费者隐私，但其在范围、定义和执行方法上存在很大的差异。本文提出了一种新的适应性合规方法，利用机器学习，并将自然语言处理（NLP）作为主要比较点，对比GDPR和CCPA。通过NLP，本文研究比较了各种法规以识别它们的重叠之处或分歧之处。这包括GDPR中的“被遗忘权”条款和CCPA中的“销售选择退出”条款。国际公司可以从该报告中吸取宝贵的经验教训，因为它概述了在不同国家更好地执行法律的战略。此外，本文还讨论了在法律文献中使用NLP所面临的挑战，并提出了增强机器学习模型研究法规能力的方法。研究的目标是“缩小法律知识与技术专长之间的差距”，通过发展更具操作效率且更有效的数据保护策略。', 'title_zh': '基于BERT和K-均值聚类的多辖区合规性融合分析的机器学习驱动研究'}
{'arxiv_id': 'arXiv:2502.10413', 'title': 'Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering', 'authors': 'Raj Sonani, Lohalekar Prayas', 'link': 'https://arxiv.org/abs/2502.10413', 'abstract': 'Digital data continues to grow, there has been a shift towards using effective regulatory mechanisms to safeguard personal information. The CCPA of California and the General Data Protection Regulation (GDPR) of the European Union are two of the most important privacy laws. The regulation is intended to safeguard consumer privacy, but it varies greatly in scope, definitions, and methods of enforcement. This paper presents a fresh approach to adaptive compliance, using machine learning and emphasizing natural language processing (NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP, this study compares various regulations to identify areas where they overlap or diverge. This includes the "right to be forgotten" provision in the GDPR and the "opt-out of sale" provision under CCPA. International companies can learn valuable lessons from this report, as it outlines strategies for better enforcement of laws across different nations. Additionally, the paper discusses the challenges of utilizing NLP in legal literature and proposes methods to enhance the model-ability of machine learning models for studying regulations. The study\'s objective is to "bridge the gap between legal knowledge and technical expertise" by developing regulatory compliance strategies that are more efficient in operation and more effective in data protection.', 'abstract_zh': '数字数据继续增长，监管机制的有效性逐渐成为保护个人隐私的关键。美国加利福尼亚州的《加州消费者隐私法》（CCPA）和欧盟的《通用数据保护条例》（GDPR）是最重要的隐私法规。这些法规意在保护消费者隐私，但其范围、定义和执法方式差异很大。本文提出了一种新的自适应合规方法，利用机器学习和自然语言处理（NLP）作为主要比较方法，对比GDPR和CCPA。通过NLP技术，本研究比较了各种法规，以识别它们的相似之处和差异。这包括GDPR中的“被遗忘权”规定和CCPA中的“不被销售”选择退出权。国际企业可以从这份报告中获得宝贵的经验，因为它概述了在不同国家更好地执行法律的策略。此外，该论文还讨论了在法律文献中使用NLP所面临的挑战，并提出了增强机器学习模型的研究方法，以便更好地研究法规。本研究的目标是“弥合法律知识和技术专长之间的差距”，通过开发更高效运行且更有效的数据保护策略来制定监管合规策略。', 'title_zh': '基于BERT和K-均值聚类的多辖区合规性驱动的机器学习融合分析'}
{'arxiv_id': 'arXiv:2502.10411', 'title': 'TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models', 'authors': 'Sahan Bulathwela, Daniel Van Niekerk, Jarrod Shipton, Maria Perez-Ortiz, Benjamin Rosman, John Shawe-Taylor', 'link': 'https://arxiv.org/abs/2502.10411', 'abstract': 'Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \\emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.', 'abstract_zh': '个性化教育是可以从最新的人工智能（AI）和大型语言模型（LLM）发展中显著受益的领域之一。然而，这也是最具挑战性的应用之一，因为有效地进行认知教学并个性化学习体验以满足独立学习者的需求非常复杂。我们假设在如此苛刻的应用场景中表现出色的一种有前途的方法是使用“心智社会”。在本章中，我们将介绍TrueReason，这是一个示例性的个性化学习系统，该系统整合了多种专门的AI模型，这些模型能够模拟由LLM组成的微观技能，以实现规划和推理的运作。我们将介绍原型初始架构，并描述已纳入原型中的两种微观技能。所提出系统展示了构建能够承担教育等领域所要求的非常复杂认知任务的高级AI系统的第一步。', 'title_zh': 'TrueReason：一种结合推理与基础模型的范例个性化学习系统'}
{'arxiv_id': 'arXiv:2502.10411', 'title': 'TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models', 'authors': 'Sahan Bulathwela, Daniel Van Niekerk, Jarrod Shipton, Maria Perez-Ortiz, Benjamin Rosman, John Shawe-Taylor', 'link': 'https://arxiv.org/abs/2502.10411', 'abstract': 'Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \\emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.', 'abstract_zh': '个性化教育是能够从最近的人工智能（AI）和大型语言模型（LLM）的最新进展中受益匪浅的领域之一。然而，这也是一项最具挑战性的应用之一，原因在于在个性化学习体验以适应独立学习者的同时，有效进行认知教学的复杂性。我们假设，在这样的苛刻应用场景中表现出色的一个有前途的方法是使用“多元心智体系”。在本章中，我们将介绍TrueReason，这是一种范例性的个性化学习系统，该系统整合了多种专门的AI模型，这些模型能够模拟由LLM组合而成的微技能，从而实现规划和推理功能的运作。同时，我们将介绍原型的架构，并描述已集成到原型中的两个微技能。所提出的系统展示了构建能够承担教育等复杂领域所需求的复杂认知任务的高级AI系统的第一步。', 'title_zh': 'TrueReason：一种结合推理与基础模型的范例个性化学习系统'}
{'arxiv_id': 'arXiv:2502.10394', 'title': 'A Coordination-based Approach for Focused Learning in Knowledge-Based Systems', 'authors': 'Abhishek Sharma', 'link': 'https://arxiv.org/abs/2502.10394', 'abstract': 'Recent progress in Learning by Reading and Machine Reading systems has significantly increased the capacity of knowledge-based systems to learn new facts. In this work, we discuss the problem of selecting a set of learning requests for these knowledge-based systems which would lead to maximum Q/A performance. To understand the dynamics of this problem, we simulate the properties of a learning strategy, which sends learning requests to an external knowledge source. We show that choosing an optimal set of facts for these learning systems is similar to a coordination game, and use reinforcement learning to solve this problem. Experiments show that such an approach can significantly improve Q/A performance.', 'abstract_zh': '近年来，学习阅读（Learning by Reading）和机器阅读（Machine Reading）系统的发展显著增强了基于知识系统的学习新事实的能力。本文探讨了为这些知识系统选择一组学习请求，以实现最佳问答性能的问题。为了理解这一问题的动力学特性，我们模拟了一个学习策略的属性，该策略向外部知识源发送学习请求。我们表明，为这些学习系统选择最优的事实集类似于一个协调博弈，并使用强化学习来解决这个问题。实验结果显示，这种做法可以显著提高问答性能。', 'title_zh': '基于协调的聚焦学习方法在知识系统中的应用'}
{'arxiv_id': 'arXiv:2502.10394', 'title': 'A Coordination-based Approach for Focused Learning in Knowledge-Based Systems', 'authors': 'Abhishek Sharma', 'link': 'https://arxiv.org/abs/2502.10394', 'abstract': 'Recent progress in Learning by Reading and Machine Reading systems has significantly increased the capacity of knowledge-based systems to learn new facts. In this work, we discuss the problem of selecting a set of learning requests for these knowledge-based systems which would lead to maximum Q/A performance. To understand the dynamics of this problem, we simulate the properties of a learning strategy, which sends learning requests to an external knowledge source. We show that choosing an optimal set of facts for these learning systems is similar to a coordination game, and use reinforcement learning to solve this problem. Experiments show that such an approach can significantly improve Q/A performance.', 'abstract_zh': '近年来，通过阅读学习和机器阅读系统的进展显著提升了基于知识系统的学习新事实的能力。在本文中，我们探讨了如何为这些基于知识的系统选择一组学习请求，从而实现最佳的问答性能。为了理解这一问题的动力学，我们模拟了一种学习策略的属性，该策略向外部知识源发送学习请求。我们表明，为这些学习系统选择最优的事实集类似于一种协调博弈，并使用强化学习来解决这个问题。实验结果表明，这种做法能够显著提高问答性能。', 'title_zh': '基于协调的聚焦学习方法在知识系统中的应用'}
