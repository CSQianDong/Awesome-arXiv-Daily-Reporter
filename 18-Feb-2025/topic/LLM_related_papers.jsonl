{'arxiv_id': 'arXiv:2502.12143', 'title': 'Small Models Struggle to Learn from Strong Reasoners', 'authors': 'Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12143', 'abstract': 'Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.', 'abstract_zh': '大型语言模型（LLMs）在复杂的推理任务中表现出色，将其推理能力提炼到较小的模型中也展现出了潜力。然而，我们发现了一个有趣的现象，我们称之为小型模型学习差距：小型模型（$\\leq$3B参数）并不一致地从较长的链式思维（CoT）推理或从较大模型的提炼中受益。相反，当它们被微调在较短且更简单的推理链上，这些推理链更好地与它们的内在学习能力相一致时，它们的性能更好。为了解决这个问题，我们提出了一种简单而有效的策略——混合提炼（Mix Distillation），该策略通过结合较长和较短的CoT示例，或者从较大和较小模型中提取推理来平衡推理复杂性。我们的实验表明，与仅使用数据进行训练相比，混合提炼显著提高了小型模型的推理性能。这些发现突显了直接强模型提炼的局限性，并强调了适应推理复杂性对于有效推理能力转移的重要性。', 'title_zh': '小型模型难以从强大推理器中学习'}
{'arxiv_id': 'arXiv:2502.12029', 'title': 'KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs', 'authors': 'Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li', 'link': 'https://arxiv.org/abs/2502.12029', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities in various complex tasks, yet they still suffer from hallucinations. Introducing external knowledge, such as knowledge graph, can enhance the LLMs' ability to provide factual answers. LLMs have the ability to interactively explore knowledge graphs. However, most approaches have been affected by insufficient internal knowledge excavation in LLMs, limited generation of trustworthy knowledge reasoning paths, and a vague integration between internal and external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large model framework driven by the collaboration of internal and external knowledge. It relies on the internal knowledge of the LLM to guide the exploration of interpretable directed subgraphs in external knowledge graphs, better integrating the two knowledge sources for more accurate reasoning. Extensive experiments on multiple real-world datasets confirm the superiority of KnowPath.", 'abstract_zh': '大型语言模型（LLMs）在各种复杂任务中展现出了显著的能力，但仍然存在幻觉问题。引入外部知识，如知识图谱，可以增强LLMs提供事实性答案的能力。LLMs具有互动探索知识图谱的能力。然而，大多数方法受到了LLMs内部知识挖掘不足、可信知识推理路径生成有限以及内部和外部知识融合不畅的限制。因此，我们提出了一种名为KnowPath的知识增强大型模型框架，该框架依赖于LLMs的内部知识来引导对外部知识图谱中有解释性的有向子图的探索，从而更好地整合两种知识来源，以实现更准确的推理。多种真实世界数据集上的大量实验证明了KnowPath的优势。', 'title_zh': '知径：通过生成的推理路径增强的知识图谱推理'}
{'arxiv_id': 'arXiv:2502.12025', 'title': 'SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities', 'authors': 'Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendran', 'link': 'https://arxiv.org/abs/2502.12025', 'abstract': 'Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage long chain-of-thought (CoT) reasoning to generate structured intermediate steps, enhancing their reasoning capabilities. However, long CoT does not inherently guarantee safe outputs, potentially leading to harmful consequences such as the introduction of security vulnerabilities in code or the spread of misinformation. Current research on large language model (LLM) safety usually focuses on short-answer responses, overlooking the long CoT style outputs of LRMs. To bridge this gap, we conduct a systematic study of LRM safety. First, we investigate safety evaluators calibrated against human annotations. Using our newly developed metrics, we thoroughly assess the safety of 12 state-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results show that LRMs are not safe compared to their reasoning advance. Further, we perform a fine-grained analysis of the reasoning trace and final answer. We find that three decoding strategies-ZeroThink, LessThink, and MoreThink-can improve model safety without additional training. However, these strategies either use constrained reasoning traces or incur high inference costs. To better strengthen LRM safety, we introduce SafeChain, the first-of-its-kind safety training dataset in CoT style. We fine-tune two LRMs with SafeChain, showing that it not only enhances model safety but also preserves performance across 6 reasoning benchmarks.', 'abstract_zh': '新兴的大规模推理模型（LRMs），如DeepSeek-R1模型，通过采用长链推理（CoT）来生成结构化的中间步骤，从而增强其推理能力。然而，长的CoT并不天然保证输出的安全性，可能会导致诸如代码中的安全漏洞或虚假信息的传播等危害性后果。目前关于大规模语言模型（LLMs）安全性方面的研究通常集中在短答案响应上，忽略了LRMs的长CoT风格输出。为填补这一空白，我们对LRM安全性进行了系统性研究。首先，我们研究了与人工注释校准的安全评估器。我们使用新开发的指标，对12个最新的LRMs在StrongReject和WildJailbreak数据集上的安全性进行了全面评估。结果显示，LRMs的推理能力并不安全。进一步地，我们对推理轨迹和最终答案进行了细致分析。我们发现，三种解码策略——NoThink、LessThink和MoreThink——可以在不增加训练的情况下提高模型安全性。然而，这些策略要么受限于推理轨迹的约束，要么会增加推理成本。为了更好地增强LRM的安全性，我们引入了SafeChain，这是首个基于CoT风格的安全训练数据集。我们对SafeChain进行了微调，结果显示，它不仅增强了模型安全性，还在6个推理基准测试中保持了性能。', 'title_zh': 'SafeChain: 语言模型的长链推理安全性'}
{'arxiv_id': 'arXiv:2502.11881', 'title': 'Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models', 'authors': 'Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi', 'link': 'https://arxiv.org/abs/2502.11881', 'abstract': "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce thought-tracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate thought-tracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.", 'abstract_zh': '现有的大模型推理方法在各类任务中展现了令人印象深刻的性能，如解决数学和编程问题。然而，将这些方法应用于缺乏准确答案或基于规则验证方法的场景中——例如跟踪代理的心理状态——仍然具有挑战性。受顺序蒙特卡洛算法的启发，我们引入了思维跟踪（Thought-Tracking）——一种推理时的算法，旨在通过生成假设并基于观察结果对其进行加权，从而追踪特定代理的心理状态，而不依赖于数据集中问题的准确答案。该算法借鉴了贝叶斯心理理论框架，使用大模型来根据代理的感知和行为对心理状态的演变进行概率推理。我们在多种心理理论基准上评估了思维跟踪算法，相较于基线大模型，展示了显著的性能提升。我们的实验还揭示了最近的推理模型（例如o1和R1）在心理理论任务中的一些有趣行为，突出了社会推理与其它领域推理之间的差异。', 'title_zh': '基于假设的理论思维推理方法在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.11770', 'title': 'Cognitive-Aligned Document Selection for Retrieval-augmented Generation', 'authors': 'Bingyu Wan, Fuxi Zhang, Zhongpeng Qi, Jiayi Ding, Jijun Li, Baoshi Fan, Yijia Zhang, Jun Zhang', 'link': 'https://arxiv.org/abs/2502.11770', 'abstract': "Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment Re\\textbf{trieval} for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-of-the-art performance.", 'abstract_zh': '大规模语言模型（LLMs）固有地表现出幻觉现象，因为生成的文本精确度无法仅通过它们所包含的参数化知识来完全保证。尽管检索增强生成（RAG）系统通过引入外部文档来提高生成模型的准确性和可靠性，但在实际应用中，这些检索到的文档往往无法充分支持模型的响应。为了解决这一问题，我们提出了GGatrieval（细粒度、基于证据的检索以实现可验证生成），其利用LLM动态更新查询并筛选高质量、可靠的检索文档。具体来说，我们将用户查询解析为语法组件，并与检索到的文档进行细粒度的对齐。对于无法单独对齐的查询组件，我们提出了一种动态语义补偿机制，该机制会迭代地细化并重写查询，同时不断更新检索结果。这一迭代过程将持续到检索到的文档能够充分支持查询的响应为止。我们的方法引入了一种新的检索文档筛选标准，使其紧密模拟了人类获取目标信息的策略。这确保了检索内容能够有效地支持和验证生成的输出。在ALCE基准测试中，我们的方法显著超越了多种基线方法，达到了最先进的性能。', 'title_zh': '认知对齐的文档选择用于检索增强生成'}
{'arxiv_id': 'arXiv:2502.11649', 'title': 'Competing LLM Agents in a Non-Cooperative Game of Opinion Polarisation', 'authors': 'Amin Qasmi, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11649', 'abstract': "We introduce a novel non-cooperative game to analyse opinion formation and resistance, incorporating principles from social psychology such as confirmation bias, resource constraints, and influence penalties. Our simulation features Large Language Model (LLM) agents competing to influence a population, with penalties imposed for generating messages that propagate or counter misinformation. This framework integrates resource optimisation into the agents' decision-making process. Our findings demonstrate that while higher confirmation bias strengthens opinion alignment within groups, it also exacerbates overall polarisation. Conversely, lower confirmation bias leads to fragmented opinions and limited shifts in individual beliefs. Investing heavily in a high-resource debunking strategy can initially align the population with the debunking agent, but risks rapid resource depletion and diminished long-term influence.", 'abstract_zh': '我们提出了一种新颖的非合作博弈，用于分析意见形成和抵制行为，该博弈融合了社会心理学中的确认偏见、资源约束和影响惩罚等原则。我们的模拟中，大型语言模型（LLM）代理相互竞争，以影响人群，并对传播或反驳虚假信息的信息采取惩罚措施。该框架将资源优化融入代理的决策过程中。研究结果表明，较高的确认偏见虽然加强了群体内意见的一致性，但也加剧了整体极化。相反，较低的确认偏见会导致意见分化，并限制个人信念的转变。尽管投资高资源的驳斥策略可以初期使人群与驳斥代理保持一致，但也存在资源迅速耗尽和长期影响力减弱的风险。', 'title_zh': '非合作意见极化博弈中的竞争语言模型代理'}
{'arxiv_id': 'arXiv:2502.11574', 'title': 'Large Language Models and Mathematical Reasoning Failures', 'authors': 'Johan Boye, Birger Moell', 'link': 'https://arxiv.org/abs/2502.11574', 'abstract': "This paper investigates the mathematical reasoning capabilities of large language models (LLMs) using 50 newly constructed high-school-level word problems. Unlike prior studies that focus solely on answer correctness, we rigorously analyze both final answers and solution steps to identify reasoning failures. Evaluating eight state-of-the-art models - including Mixtral, Llama, Gemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models (e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors in spatial reasoning, strategic planning, and arithmetic, sometimes producing correct answers through flawed logic. Common failure modes include unwarranted assumptions, over-reliance on numerical patterns, and difficulty translating physical intuition into mathematical steps. Manual analysis reveals that models struggle with problems requiring multi-step deduction or real-world knowledge, despite possessing broad mathematical knowledge. Our results underscore the importance of evaluating reasoning processes, not just answers, and caution against overestimating LLMs' problem-solving proficiency. The study highlights persistent gaps in LLMs' generalization abilities, emphasizing the need for targeted improvements in structured reasoning and constraint handling.", 'abstract_zh': '本文使用50个新构建的高中级别文字题，探讨了大型语言模型（LLMs）的数学推理能力。不同于以往研究仅关注答案的正确性，我们对最终答案和解题步骤进行了严格的分析，以识别推理中的失败。评估了八种最先进的模型——包括Mixtral、Llama、Gemini、GPT-4o以及OpenAI的o1变体——结果发现，尽管较新的模型（例如o3-mini、deepseek-r1）获得了更高的准确性，但所有模型在空间推理、战略规划和算术方面都存在错误，有时通过错误的逻辑反而能得出正确的答案。常见的失败模式包括不合理的假设、过度依赖数字模式，以及难以将物理直觉转化为数学步骤。人工分析显示，尽管这些模型拥有广泛的数学知识，但在需要多步推理或现实世界知识的问题上仍存在困难。研究结果强调了评估推理过程的重要性，而不是仅仅关注答案，并提醒我们不要高估LLMs在问题解决方面的能力。该研究突显了LLMs在泛化能力方面的持续缺口，强调了在结构化推理和约束处理方面进行针对性改进的必要性。', 'title_zh': '大型语言模型在数学推理中的失败表现'}
{'arxiv_id': 'arXiv:2502.11528', 'title': 'A Survey of Personalized Large Language Models: Progress and Future Directions', 'authors': 'Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King', 'link': 'https://arxiv.org/abs/2502.11528', 'abstract': "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the this https URL.", 'abstract_zh': '大型语言模型（LLMs）在处理一般知识任务方面表现出色，但在处理用户特定的个性化需求方面存在困难，如理解个人情感、写作风格和偏好。个性化大型语言模型（PLLMs）通过利用个体用户数据（如用户档案、历史对话、内容和互动），提供与用户上下文相关且符合其特定需求的响应，来应对这些挑战。这一研究领域极具价值，因为PLLMs能够显著提升用户体验并在对话代理、推荐系统、情绪识别、医疗辅助等领域拥有广泛的应用前景。本文综述了从三个技术视角来看的PLLMs的最新进展：个性化上下文提示（输入层面）、个性化适配器微调（模型层面）和个性化偏好对齐（目标层面）。此外，本文还讨论了当前的局限性，并提出了未来研究的若干有前途的方向。有关本文综述的最新信息，请参阅 [此处](this https URL)。', 'title_zh': '个性化大型语言模型综述：进展与未来方向'}
{'arxiv_id': 'arXiv:2502.11435', 'title': 'SMART: Self-Aware Agent for Tool Overuse Mitigation', 'authors': 'Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji', 'link': 'https://arxiv.org/abs/2502.11435', 'abstract': "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.", 'abstract_zh': '当前的大型语言模型（LLM）代理表现出强大的推理和工具使用能力，但往往缺乏自我意识，无法有效平衡这些方法。这种不平衡导致了工具过度使用，即模型在可以通过参数化知识解决的任务中无必要地依赖外部工具，从而增加了计算开销。受到人类元认知的启发，我们引入了SMART（战略模型感知推理与工具使用）范式，该范式增强代理的自我意识，以优化任务处理并减少工具过度使用。为了支持这一范式，我们引入了SMART-ER数据集，该数据集跨越三个领域，在推理过程中交替使用参数化知识和工具依赖步骤，每一步都通过解释何时需要使用工具的原因使其更加丰富。通过监督训练，我们开发了SMART-Agent家族模型，能够动态平衡参数化知识和工具的使用。评估结果显示，SMART-Agent在减少工具使用24%的同时提高了超过37%的性能，使7B规模的模型能够与70B版本以及GPT-4o相媲美。此外，SMART-Agent能够泛化至分布外测试数据，如GSM8K和MINTQA，仅需平时五分之一的工具调用就能保持准确率。这些结果突显了战略性工具使用的潜力，可以增强推理能力、缓解过度使用问题，并缩小模型规模与性能之间的差距，从而推动更智能和资源高效代理的设计。', 'title_zh': 'SMART：自我感知代理工具过度使用缓解'}
{'arxiv_id': 'arXiv:2502.11433', 'title': '\\textsc{FLAG-Trader}: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading', 'authors': 'Guojun Xiong, Zhiyang Deng, Keyi Wang, Yupeng Cao, Haohang Li, Yangyang Yu, Xueqing Peng, Mingquan Lin, Kaleb E Smith, Xiao-Yang Liu, Jimin Huang, Sophia Ananiadou, Qianqian Xie', 'link': 'https://arxiv.org/abs/2502.11433', 'abstract': 'Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.', 'abstract_zh': '大型语言模型（LLMs）在多模态金融数据上进行微调后，在各种金融任务中展示了令人印象深刻的推理能力。然而，在交互式金融市场中的交易等多步、目标导向的情景中，它们往往难以应对复杂的代理性方法，以改进决策。为了解决这一问题，我们提出了一种名为 \\textsc{FLAG-Trader} 的统一架构，该架构将语言处理（通过LLMs进行）与基于梯度的强化学习（RL）策略优化相结合，在这种架构中，部分微调的LLM作为策略网络发挥作用，利用预训练的知识并通过参数高效的微调适应金融领域。通过由交易奖励驱动的策略梯度优化，我们的框架不仅提高了LLM在交易中的表现，还改善了其他金融领域任务的结果。我们提供了广泛的实验证据来验证这些增强效果。', 'title_zh': '\\textsc{FLAG-Trader}: 结合梯度强化学习的LLM-Agent融合模型在金融交易中的应用'}
{'arxiv_id': 'arXiv:2502.11418', 'title': 'TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents', 'authors': 'Geon Lee, Wenchao Yu, Kijung Shin, Wei Cheng, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.11418', 'abstract': 'Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score.', 'abstract_zh': '时间序列数据在各种应用中至关重要，包括气候建模、医疗监测和金融分析。理解与实际时间序列数据相关的时间背景信息通常对于准确可靠的事件预测至关重要。在本文中，我们介绍了一种名为TimeCAP的时间序列处理框架，该框架创造性地利用大型语言模型（LLMs）作为时间序列数据的背景补足者，而不仅仅是预测器。TimeCAP包括两个独立的LLM代理：一个生成文本摘要，捕捉时间序列的上下文，另一个利用这个丰富化的摘要做出更具信息量的预测。此外，TimeCAP采用了一种多模态编码器，该编码器与LLM代理协同工作，通过输入中的上下文示例增强其互增效果，从而提高预测性能。在实际数据集上的实验结果表明，TimeCAP在时间序列事件预测方面优于最先进的方法，包括利用LLMs作为预测器的方法，在F1分数上平均提高了28.75%。', 'title_zh': '时间上下文化预测：通过大型语言模型代理学习上下文化、增强和预测时间序列事件'}
{'arxiv_id': 'arXiv:2502.11304', 'title': 'Leveraging Multimodal-LLMs Assisted by Instance Segmentation for Intelligent Traffic Monitoring', 'authors': 'Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy', 'link': 'https://arxiv.org/abs/2502.11304', 'abstract': 'A robust and efficient traffic monitoring system is essential for smart cities and Intelligent Transportation Systems (ITS), using sensors and cameras to track vehicle movements, optimize traffic flow, reduce congestion, enhance road safety, and enable real-time adaptive traffic control. Traffic monitoring models must comprehensively understand dynamic urban conditions and provide an intuitive user interface for effective management. This research leverages the LLaVA visual grounding multimodal large language model (LLM) for traffic monitoring tasks on the real-time Quanser Interactive Lab simulation platform, covering scenarios like intersections, congestion, and collisions. Cameras placed at multiple urban locations collect real-time images from the simulation, which are fed into the LLaVA model with queries for analysis. An instance segmentation model integrated into the cameras highlights key elements such as vehicles and pedestrians, enhancing training and throughput. The system achieves 84.3% accuracy in recognizing vehicle locations and 76.4% in determining steering direction, outperforming traditional models.', 'abstract_zh': '智能城市和智能运输系统（ITS）中的鲁棒且高效的交通监控系统对于保障交通安全、优化交通流量、减少拥堵和实现实时自适应交通控制至关重要。交通监控系统必须全面理解动态城市条件，并提供直观的用户界面以有效管理交通。本研究利用LLaVA多模态大语言模型（LLM）在实时Quanser Interactive Lab仿真平台上进行交通监控任务，覆盖交叉口、拥堵和碰撞等场景。部署在多个城市位置的摄像头收集实时图像，通过查询输入LLaVA模型进行分析。摄像头内嵌的实例分割模型突出显示重要元素如车辆和行人，增强训练效果并提高处理速度。该系统在识别车辆位置方面达到了84.3%的准确率，在确定转向方向方面达到了76.4%的准确率，优于传统模型。', 'title_zh': '利用实例分割辅助的多模态大模型进行智能交通监控'}
{'arxiv_id': 'arXiv:2502.11221', 'title': 'PlanGenLLMs: A Modern Survey of LLM Planning Capabilities', 'authors': 'Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu', 'link': 'https://arxiv.org/abs/2502.11221', 'abstract': 'LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.', 'abstract_zh': '大型语言模型（LLMs）在生成计划方面具有巨大的潜力，能够将初始世界状态转化为期望的目标状态。大量研究已经探讨了LLMs在各种规划任务中的应用，包括网络导航、旅行规划和数据库查询等。然而，许多现有的系统针对特定问题进行了定制，这使得它们之间的比较变得困难，也难以确定新任务的最佳方法。此外，缺乏清晰且一致的评估标准。我们所做的调研旨在提供当前LLM规划系统的全面概述，以填补这一空白。该调研基于Kartam和Wilkins（1990）的基础工作，并考察了六个关键性能指标：完备性、可执行性、最优性、表示性、泛化能力和效率。对于每个指标，我们对其代表性工作进行了详尽分析，并指出了它们的优势和不足。我们的论文还指出了未来研究的关键方向，使之成为既有经验的从业者和新入门的研究人员在利用LLM规划支持自主工作流程方面的重要参考资源。', 'title_zh': 'PlanGenLLMs：现代大型语言模型规划能力综述'}
{'arxiv_id': 'arXiv:2502.11155', 'title': 'Uncertainty-Aware Search and Value Models: Mitigating Search Scaling Flaws in LLMs', 'authors': 'Fei Yu, Yingru Li, Benyou Wang', 'link': 'https://arxiv.org/abs/2502.11155', 'abstract': 'Value model-guided search is effective in steering the generation but suffers from scaling flaws: Its superiority diminishes with larger sample sizes, underperforming non-search baselines. This limitation arises from reliability degradation in value models in unseen reasoning paths. To address this, we propose an uncertainty-aware search framework that includes two key components: (1) uncertainty-aware value models that incorporate uncertainty into predictions, and (2) an uncertainty-aware selection process using the proposed efficient Group Thompson Sampling algorithm. Experiments on GSM8K show that our method mitigates search scaling flaws, achieving 90.5% coverage at 16 samples compared to 85.8% for conventional value-guided search. This work establishes the first systematic integration of uncertainty quantification in LLM search paradigms.', 'abstract_zh': '价值模型引导的搜索在指导生成方面是有效的，但存在扩展性缺陷：随着样本数量的增加，其优势逐渐减弱，并且在某些情况下表现不如非搜索基准。这一缺陷源于在未见过的推理路径中价值模型的可靠性下降。为了解决这一问题，我们提出了一种aware于不确定性的搜索框架，该框架包含两个关键组成部分：（1）aware于不确定性的价值模型，该模型将不确定性纳入预测中；（2）使用提出的高效Group Thompson Sampling算法的aware于不确定性的选择过程。在GSM8K上的实验显示，我们的方法减轻了搜索扩展性缺陷，16个样本情况下覆盖率达到了90.5%，而传统价值引导的搜索仅为85.8%。本项工作在LLM搜索范式中首次系统地整合了不确定性量化。', 'title_zh': '具有不确定性意识的搜索和价值模型：减轻大规模语言模型中搜索扩展的缺陷'}
{'arxiv_id': 'arXiv:2502.11142', 'title': 'NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM', 'authors': 'Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan', 'link': 'https://arxiv.org/abs/2502.11142', 'abstract': "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.", 'abstract_zh': '视觉-语言导航（VLN）是具身代理的一项基本技能，允许它们遵循自然语言指令在3D环境中导航。高性能的导航模型需要大量的训练数据，手动标注数据的高昂成本严重阻碍了这一领域的进展。因此，一些先前的方法将轨迹视频转换为分步骤的指令以扩充数据集，但这些指令并不符合用户的交流风格，用户往往只是简要描述目的地或提出特定需求。此外，局部导航轨迹忽略了全局上下文和高层次的任务规划。为了应对这些挑战，我们提出了一种检索增强生成（RAG）框架NavRAG，该框架用于为VLN生成用户需求指令。NavRAG利用大语言模型（LLM）构建一个分层场景描述树，从全局布局到局部细节进行3D场景理解，然后模拟具有不同需求的多种用户角色，从场景树中检索信息，生成多种多样的指令。我们对超过200万条导航指令进行了标注，覆盖了861个场景，并评估了训练模型的数据质量和导航性能。', 'title_zh': 'NavRAG：通过检索增强的大语言模型生成用户导向的实体导航指令'}
{'arxiv_id': 'arXiv:2502.10978', 'title': 'Agentic LLM Framework for Adaptive Decision Discourse', 'authors': 'Antoine Dolant, Praveen Kumar', 'link': 'https://arxiv.org/abs/2502.10978', 'abstract': "Effective decision-making in complex systems requires synthesizing diverse perspectives to address multifaceted challenges under uncertainty. This study introduces a real-world inspired agentic Large Language Models (LLMs) framework, to simulate and enhance decision discourse-the deliberative process through which actionable strategies are collaboratively developed. Unlike traditional decision-support tools, the framework emphasizes dialogue, trade-off exploration, and the emergent synergies generated by interactions among agents embodying distinct personas. These personas simulate diverse stakeholder roles, each bringing unique priorities, expertise, and value-driven reasoning to the table. The framework incorporates adaptive and self-governing mechanisms, enabling agents to dynamically summon additional expertise and refine their assembly to address evolving challenges. An illustrative hypothetical example focused on extreme flooding in a Midwestern township demonstrates the framework's ability to navigate uncertainty, balance competing priorities, and propose mitigation and adaptation strategies by considering social, economic, and environmental dimensions. Results reveal how the breadth-first exploration of alternatives fosters robust and equitable recommendation pathways. This framework transforms how decisions are approached in high-stakes scenarios and can be incorporated in digital environments. It not only augments decision-makers' capacity to tackle complexity but also sets a foundation for scalable and context-aware AI-driven recommendations. This research explores novel and alternate routes leveraging agentic LLMs for adaptive, collaborative, and equitable recommendation processes, with implications across domains where uncertainty and complexity converge.", 'abstract_zh': '在复杂系统中进行有效的决策需要综合多方面的视角以应对多维度的不确定性挑战。本研究提出了一种受现实启发的主动型大型语言模型（LLM）框架，用于模拟和增强决策对话——通过该过程，行动性策略得到协同开发。与传统的决策支持工具不同，该框架强调对话、权衡探索以及由不同人物特征的代理间交互产生的协同效应。这些人物模拟了不同的利益相关者角色，各自带来独特的优先级、专业知识和价值导向的推理。该框架整合了适应性和自我治理机制，使代理能够动态地召唤额外的专家并不断优化其组合以应对不断演变的挑战。一个示例假想场景，旨在解决中西部某一乡镇的极端洪水问题，展示了该框架在导航不确定性、平衡竞争性优先级以及综合社会、经济和环境维度提出减轻和适应策略方面的能力。研究结果表明，广度优先探索替代方案如何促进稳健且公正的推荐路径。该框架改变了在高风险场景中进行决策的方式，并可融入数字环境中。它不仅增强了决策者处理复杂性的能力，还为可扩展且情境感知的AI驱动推荐奠定了基础。本研究探讨了利用主动型LLM探索适应性、协作性和公平性的新型推荐过程，具有跨领域的重要意义，特别是在不确定性与复杂性交汇的地方。', 'title_zh': '代理型LLM框架：自适应决策对话'}
{'arxiv_id': 'arXiv:2502.10938', 'title': 'PEA: Enhancing LLM Performance on Computational-Reasoning Tasks', 'authors': 'Zi Wang, Shiwei Weng, Mohannad Alhanahnah, Somesh Jha, Tom Reps', 'link': 'https://arxiv.org/abs/2502.10938', 'abstract': "Large Language Models (LLMs) have exhibited remarkable capabilities across diverse domains, prompting investigations into their potential as generic reasoning engines. While recent studies have explored inference-time computation to enhance model performance on complex problems, current research lacks a formal framework to characterize the complexity of reasoning tasks. This study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a formal approach to describe and solve a class of important reasoning tasks termed computational reasoning problems. The PEA framework decomposes these problems into predicate and enumeration components, using LLMs to synthesize programs based on specified predicates, enumeration, and aggregation rules. These synthesized programs are then executed to obtain solutions to the computational tasks. We demonstrate the framework's efficacy on benchmark tasks including Boolean satisfiability problems, game of $24$, and planning problems. Empirical evaluation reveals that PEA substantially enhances the performance of underlying models on benchmark computational problems, yielding an average accuracy improvement of approximately $50\\%$, coupled with increased efficiency.", 'abstract_zh': '大型语言模型（LLMs）已在多个领域展现出卓越的能力，引发了将其作为通用推理引擎的研究。尽管近年来的研究探索了推理时的计算方法以提高模型在复杂问题上的表现，当前研究缺乏正式框架来表征推理任务的复杂性。本研究引入了谓词枚举聚合（PEA）框架，这是一种正式方法，用于描述和解决一类重要的推理任务——计算推理问题。PEA框架将这些任务分解为谓词和枚举两部分，并利用大规模语言模型（LLMs）根据指定的谓词、枚举和聚合规则生成程序。生成的程序随后被执行以求解计算任务。我们在基准任务，如布尔可满足性问题、24点游戏和规划问题上展示了该框架的有效性。实证评估表明，PEA显著提升了基础模型在基准计算问题上的性能，平均准确率提高了约50%，同时提高了效率。', 'title_zh': 'PEA：提高大型语言模型在计算推理任务性能的方法'}
{'arxiv_id': 'arXiv:2502.10937', 'title': 'SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention', 'authors': 'Chengshuai Zhao, Zhen Tan, Chau-Wai Wong, Xinyan Zhao, Tianlong Chen, Huan Liu', 'link': 'https://arxiv.org/abs/2502.10937', 'abstract': 'Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.', 'abstract_zh': '内容分析将复杂的非结构化文本分解为理论导向的数值类别。特别是在社会科学中，这一过程通常依赖于多轮的手动注释、领域专家讨论以及基于规则的改进。本文介绍了一种新颖的多智能体框架SCALE，通过大语言模型（LLM）智能体有效地模拟内容分析过程。SCALE 模拟内容分析的关键阶段，包括文本编码、协作讨论以及动态代码本演变，捕捉了人类研究人员的反思深度和适应性讨论。此外，通过整合多种类型的人类干预模式，SCALE 进一步增强了其性能，加入了领域专家的输入。在真实世界数据集上的广泛评估表明，SCALE 在各种复杂内容分析任务中实现了接近人类的表现，为未来社会科学的研究提供了创新的潜力。', 'title_zh': 'SCALE：关于大规模语言模型代理与人类介入在社会科学中协作内容分析的研究'}
{'arxiv_id': 'arXiv:2502.10906', 'title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'authors': 'In-Chang Baek, Sung-Hyun Kim, Sam Earle, Zehua Jiang, Noh Jin-Ha, Julian Togelius, Kyung-Joong Kim', 'link': 'https://arxiv.org/abs/2502.10906', 'abstract': 'Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort. In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs). In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators. This work introduces PCGRLLM, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques. We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach. Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks. The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model. Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.', 'abstract_zh': '奖励设计在游戏AI的训练中起到关键作用，需要大量的领域特定知识和人力投入。近年来，多项研究探讨了使用大型语言模型（LLM）生成奖励信号，以训练游戏代理和控制机器人。在内容生成文献中，早期就有关于生成奖励函数的工作，用于强化学习代理生成器。本文介绍了一种基于前期工作的扩展架构——PCGRLLM，该架构采用了反馈机制和多种基于推理的提示工程技术。我们使用两种最先进的LLM，在一个二维环境中评估所提出的方法，展示了我们方法的普适性。我们的实验提供了对LLM生成内容任务能力的有价值的评估，结果显示了显著的性能提升，分别高达415%和40%，这取决于语言模型的零样本能力。本文证明了在游戏AI开发中减少对人类依赖的可能性，同时支持并增强了创造性过程。', 'title_zh': 'PCGRLLM：基于大型语言模型的 procedural 内容生成强化学习奖励设计'}
{'arxiv_id': 'arXiv:2502.10867', 'title': 'A Tutorial on LLM Reasoning: Relevant Methods behind ChatGPT o1', 'authors': 'Jun Wang', 'link': 'https://arxiv.org/abs/2502.10867', 'abstract': "OpenAI o1 has shown that applying reinforcement learning to integrate reasoning steps directly during inference can significantly improve a model's reasoning capabilities. This result is exciting as the field transitions from the conventional autoregressive method of generating answers to a more deliberate approach that models the slow-thinking process through step-by-step reasoning training. Reinforcement learning plays a key role in both the model's training and decoding processes. In this article, we present a comprehensive formulation of reasoning problems and investigate the use of both model-based and model-free approaches to better support this slow-thinking framework.", 'abstract_zh': 'OpenAI o1展示了在推理过程中直接应用强化学习整合推理步骤，可以显著提升模型的推理能力。这一结果令人兴奋，因为研究领域正从传统的自回归方法生成答案，转向一种更慎重的方法，通过逐步推理训练来模拟慢思考过程。强化学习在模型的训练和解码过程中都起到了关键作用。在本文中，我们提出了推理问题的全面框架，并探讨了如何更好地使用基于模型的方法和基于策略的方法来支持这一慢思考框架。', 'title_zh': 'LLM推理教程：ChatGPT背后的相关方法概述'}
{'arxiv_id': 'arXiv:2502.10858', 'title': 'Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs', 'authors': 'Zongqian Wu, Tianyu Li, Jiaying Yang, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng', 'link': 'https://arxiv.org/abs/2502.10858', 'abstract': 'Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term \\textit{breadth reasoning}. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning. Our code is provided in this https URL.', 'abstract_zh': '深度迭代链式思考（CoT）推理使大规模语言模型（LLM）能够通过逐步激活相关预训练知识来应对复杂的任务。然而，这种方法在确保持续改进和确定停止标准方面面临挑战。本文探讨了是否可以从初始推理路径中直接激活对解决给定问题有贡献的相关知识，从而避免迭代改进的需要。我们的实验表明，增加初始推理路径的多样性可以达到相近或更优的性能，我们将其称为“宽泛推理”。然而，现有的宽泛推理方法，如自我一致性，提供的多样性有限。为了解决这一局限，我们提出了一种简单而有效的方法，通过结合上下文探索和减少采样随机性来增强推理的宽泛性。广泛的实验证明，我们的方法显著优于深度迭代推理。我们的代码可在以下链接获得：[此链接]。', 'title_zh': '《仅需深度吗？大规模语言模型中迭代推理的探索》\n\n这个标题翻译成中文时，尽量保持了原始英文的结构和意思，同时符合中文的表达习惯。如果是正式的学术论文，可以根据具体的需求和格式要求进行适当的调整。'}
{'arxiv_id': 'arXiv:2502.10522', 'title': 'GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs', 'authors': 'Shima Khoshraftar, Niaz Abedini, Amir Hajian', 'link': 'https://arxiv.org/abs/2502.10522', 'abstract': 'The application of large language models (LLMs) to graph data has attracted a lot of attention recently. LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text at- tributes of nodes. However, it is still challenging to efficiently en- code the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative man- ual adjustments that could be slow, tedious and difficult to replicate programmatically. In this paper, we propose GraphiT (Graphs in Text), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts us- ing the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task.', 'abstract_zh': '近年来，将大型语言模型（LLMs）应用于图数据引起了大量关注。LLMs 使我们能够利用预训练模型中的深度上下文嵌入来处理具有文本属性的图数据，而在这些图数据中，浅层嵌入通常用于节点的文本属性。然而，高效地将图结构和特征编码为顺序形式，以便用于LLMs，仍然具有挑战性。此外，单独使用LLM的效果高度依赖于输入提示的结构，这限制了它们作为可靠方法的有效性，并常常需要迭代的手动调整，这可能是缓慢、繁琐且难以程序化地重复的。在本文中，我们提出了一种名为GraphiT（图在文本中）的框架，用于将图编码为文本格式，并优化LLM提示以进行图预测任务。我们重点讨论具有文本属性的节点分类任务。我们为每个节点及其邻域编码图数据，以使LLMs能够更好地利用图中的信息。然后，我们使用DSPy框架进一步编程优化LLM提示，以自动化这一过程，使其更加高效和可重复。在三个数据集上，GraphiT的性能优于我们的LLM基线，我们展示了GraphiT中的优化步骤如何在无需手动调整提示的情况下获得可衡量的更好结果。此外，我们还证明了我们的图编码方法在竞争性方面与其他图编码方法相当，但成本较低，因为它可以使用显著较少的令牌完成相同任务。', 'title_zh': 'GraphiT: 采用提示优化的大语言模型高效进行文本属性图的节点分类'}
{'arxiv_id': 'arXiv:2502.10428', 'title': 'Dynamic Chain-of-Thought: Towards Adaptive Deep Reasoning', 'authors': 'Libo Wang', 'link': 'https://arxiv.org/abs/2502.10428', 'abstract': "To reduce the cost and consumption of computing resources caused by computational redundancy and delayed reward assignment in long CoT, this research proposes the dynamic chain-of-thought with adaptive reasoning time and steps. The researcher used simulation experiment to simulate the integration of D-CoT through Python 3.13 IDLE combined with a Python simulator based on GPTs. At the same time, the researcher used DeepSeek R1 as a control group to test and compare the performance of the D-CoT simulator in processing MIT OpenCourseWare's linear algebra exam questions. Experimental results show that D-CoT is better than DeepSeek R1 based on long CoT in three indicators: reasoning time, CoT length (reasoning steps) and token count, which achieves a significant reduction in computing resource consumption. In addition, this research has potential value in deep reasoning optimization and can be used as a reference for future dynamic deep reasoning frameworks.", 'abstract_zh': '为了减少由计算冗余和延迟奖励分配导致的长期CoT（Reasoning Chain）计算成本和资源消耗，本研究提出了一种具有自适应推理时间和步骤的动态CoT。研究者通过使用Python 3.13 IDLE结合基于GPTs的Python模拟器进行仿真实验，实现了D-CoT（Dynamic Chain-of-Thought）的集成。同时，研究者使用DeepSeek R1作为对照组，测试并比较了D-CoT模拟器在处理MIT OpenCourseWare线性代数考试题方面的能力。实验结果表明，与基于长期CoT的DeepSeek R1相比，D-CoT在推理时间、CoT长度（推理步骤）和标记计数三个指标上表现出显著的优势，实现了计算资源消耗的显著降低。此外，本研究在深入推理优化方面具有潜在价值，并可作为未来动态深层次推理框架的参考。', 'title_zh': '动态思维链：迈向自适应深度推理'}
{'arxiv_id': 'arXiv:2502.12145', 'title': 'Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control', 'authors': 'Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie', 'link': 'https://arxiv.org/abs/2502.12145', 'abstract': 'Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to mitigate large language model (LLM) hallucinations by incorporating external knowledge retrieval. However, existing RAG frameworks often apply retrieval indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or failing to retrieve iteratively when required for complex reasoning. Recent adaptive retrieval strategies, though adaptively navigates these retrieval strategies, predict only based on query complexity and lacks user-driven flexibility, making them infeasible for diverse user application needs. In this paper, we introduce a novel user-controllable RAG framework that enables dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two classifiers: one trained to prioritize accuracy and another to prioritize retrieval efficiency. Via an interpretable control parameter $\\alpha$, users can seamlessly navigate between minimal-cost retrieval and high-accuracy retrieval based on their specific requirements. We empirically demonstrate that our approach effectively balances accuracy, retrieval cost, and user controllability, making it a practical and adaptable solution for real-world applications.', 'abstract_zh': '检索增强生成（RAG）作为一种通过引入外部知识检索来减轻大型语言模型（LLM）幻觉的方法，已经逐渐成为一项强有力的技术。然而，现有的RAG框架通常不分青红皂白地进行检索，导致在不需要检索时过度检索，在需要进行复杂推理时又无法进行迭代检索。最近的自适应检索策略虽然能够在一定程度上适应这些检索策略，但仅基于查询复杂性进行预测，缺乏用户驱动的灵活性，无法满足多样化的用户应用需求。在本文中，我们提出了一种新型的用户可控的RAG框架，能够动态调整准确性和成本之间的权衡关系。该方法利用了两个分类器：一个用于优先考虑准确性，另一个用于优先考虑检索效率。通过一个可解释的控制参数 $\\alpha$，用户可以根据自身的具体需求，在低成本检索和高准确度检索之间无缝切换。我们通过实验证明，我们的方法能够有效平衡准确率、检索成本和用户可控性，成为一种实用且具有适应性的现实应用解决方案。', 'title_zh': '快速还是更好？在具有灵活用户控制的检索增强生成中权衡准确性和成本'}
{'arxiv_id': 'arXiv:2502.12109', 'title': 'Personality Structured Interview for Large Language Model Simulation in Personality Research', 'authors': 'Pengda Wang, Huiqi Zou, Hanjie Chen, Tianjun Sun, Ziang Xiao, Frederick L. Oswald', 'link': 'https://arxiv.org/abs/2502.12109', 'abstract': "Although psychometrics researchers have recently explored the use of large language models (LLMs) as proxies for human participants, LLMs often fail to generate heterogeneous data with human-like diversity, which diminishes their value in advancing social science research. To address these challenges, we explored the potential of the theory-informed Personality Structured Interview (PSI) as a tool for simulating human responses in personality research. In this approach, the simulation is grounded in nuanced real-human interview transcripts that target the personality construct of interest. We have provided a growing set of 357 structured interview transcripts from a representative sample, each containing an individual's response to 32 open-ended questions carefully designed to gather theory-based personality evidence. Additionally, grounded in psychometric research, we have summarized an evaluation framework to systematically validate LLM-generated psychometric data. Results from three experiments demonstrate that well-designed structured interviews could improve human-like heterogeneity in LLM-simulated personality data and predict personality-related behavioral outcomes (i.e., organizational citizenship behaviors and counterproductive work behavior). We further discuss the role of theory-informed structured interviews in LLM-based simulation and outline a general framework for designing structured interviews to simulate human-like data for psychometric research.", 'abstract_zh': '尽管心理测量学研究者最近已经开始探索使用大型语言模型（LLMs）作为人类参与者代理的可能性，但LLMs往往无法生成具有人类多样性的异质数据，这削弱了它们在促进社会科学研究方面的作用。为应对这些挑战，我们探讨了理论导向的性格结构化访谈（PSI）作为模拟性格研究中人类反应工具的潜在价值。在该方法中，模拟基于对目标性格构建的细致真实人类访谈记录。我们提供了一组不断增长的357份结构化访谈记录，其中每份记录包含一个个体对精心设计的32个开放性问题的回应，这些问题旨在收集基于理论的性格证据。此外，基于心理测量学研究，我们总结了一个评估框架，以系统验证LLM生成的心理测量数据。三项实验的结果表明，精心设计的结构化访谈可以提高LLM模拟性格数据中的拟人类异质性，并预测与性格相关的行为结果（如组织公民行为和破坏性工作行为）。最后，我们讨论了理论导向的结构化访谈在LLM基础上模拟中的作用，并概述了一个设计结构化访谈以模拟拟人类数据的通用框架，用于心理测量学研究。', 'title_zh': '个性结构化面试在人格研究中对大型语言模型模拟的应用'}
{'arxiv_id': 'arXiv:2502.12088', 'title': 'Meta-Statistical Learning: Supervised Learning of Statistical Inference', 'authors': 'Maxime Peyrard, Kyunghyun Cho', 'link': 'https://arxiv.org/abs/2502.12088', 'abstract': "This work demonstrates that the tools and principles driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than labels for individual datapoints. These tasks encompass statistical inference problems such as parameter estimation, hypothesis testing, or mutual information estimation. Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to individual datapoint. We propose meta-statistical learning, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this approach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architectures, without positional encoding, provide a natural fit due to their permutation-invariance properties. By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework's versatility with applications in hypothesis testing and mutual information estimation, showing strong performance, particularly for small datasets where traditional neural methods struggle.", 'abstract_zh': '本研究展示了可以将驱动大型语言模型（LLMs）成功的关键工具和技术应用于处理数据分布级别任务中，这些任务的目标是预测数据生成分布的性质，而不是单个数据点的标签。此类任务包括统计推断问题，如参数估计、假设检验或互信息估计。将这些任务嵌入传统机器学习管道中存在挑战，因为监督通常与单个数据点相关。我们提出了一种元统计学习框架，该框架借鉴了多实例学习的思想，将统计推断任务重新表述为监督学习问题。在该方法中，整个数据集被视为神经网络的单一输入，用于预测数据分布级别的参数。基于变压器的架构，由于其排他性不变性特性，非常适合这种应用场景。通过在大规模合成数据集上进行训练，元统计模型可以利用基于变压器的LLMs的可扩展性和优化基础设施。我们通过在假设检验和互信息估计中的应用展示了该框架的灵活性，并且在传统神经方法难以应对的小数据集上表现出强大性能。', 'title_zh': '元统计学习：监督学习中的统计推理学习'}
{'arxiv_id': 'arXiv:2502.12067', 'title': 'TokenSkip: Controllable Chain-of-Thought Compression in LLMs', 'authors': 'Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.12067', 'abstract': "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.", 'abstract_zh': '链式思维（Chain-of-Thought，CoT）已被证明能够有效提升大型语言模型（LLM）的推理能力。最近的一些进展，如OpenAI的o1和DeepSeek-R1，表明在推理过程中扩展CoT序列的长度可以进一步提升LLM的推理表现。然而，由于LLM解码的自回归性质，更长的CoT输出会导致推理延迟线性增加，从而影响用户体验，尤其是当CoT超过10,000个标记时。为了解决这一限制，我们分析了CoT输出中各个标记的语义重要性，并发现它们在推理中的贡献各不相同。基于这一洞见，我们提出了一种简单而有效的方法——TokenSkip，使LLM能够选择性地跳过不重要的标记，从而实现可控的CoT压缩。在多种模型和任务上进行的广泛实验表明，TokenSkip能够在减少CoT标记使用量的同时保持强大的推理性能。值得注意的是，将TokenSkip应用于Qwen2.5-14B-Instruct时，在GSM8K数据集上，TokenSkip将推理标记减少了40%（从313减少到181），且性能降幅不到0.4%。', 'title_zh': 'TokenSkip: LLM中可控的链式思维压缩'}
{'arxiv_id': 'arXiv:2502.12022', 'title': 'Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving', 'authors': 'Xin Xu, Yan Xu, Tianhao Chen, Yuchen Yan, Chengwu Liu, Zaoyu Chen, Yufei Wang, Yichun Yin, Yasheng Wang, Lifeng Shang, Qun Liu', 'link': 'https://arxiv.org/abs/2502.12022', 'abstract': "Existing approaches to mathematical reasoning with large language models (LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated Reasoning (TIR) for precise computation. While efforts have been made to combine these methods, they primarily rely on post-selection or predefined strategies, leaving an open question: whether LLMs can autonomously adapt their reasoning strategy based on their inherent capabilities. In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware data selection during supervised fine-tuning (SFT) to tailor training data to the model's unique abilities. This approach equips LLMs to autonomously determine and apply the appropriate reasoning strategy at test time. We evaluate TATA through extensive experiments on six mathematical reasoning benchmarks, using both general-purpose and math-specialized LLMs. Empirical results demonstrate that TATA effectively combines the complementary strengths of CoT and TIR, achieving superior or comparable performance with improved inference efficiency compared to TIR alone. Further analysis underscores the critical role of aptitude-aware data selection in enabling LLMs to make effective and adaptive reasoning decisions and align reasoning strategies with model capabilities.", 'abstract_zh': '现有的大型语言模型（LLMs）在数学推理方面的应用方法主要依赖于链条思考（Chain-of-Thought, CoT）以增强泛化能力，或者工具集成推理（Tool-Integrated Reasoning, TIR）以提高精确计算能力。虽然已经尝试将这两种方法结合起来，但它们主要依赖于后处理选择或预先定义的策略，留下了一个开放的问题：即LLMs能否根据自身的内在能力自主适应其推理策略。在本文中，我们提出了TATA（Teaching LLMs According to Their Aptitude），一种自适应框架，使LLMs能够自发地个性化其推理策略，使其与内在能力相一致。TATA通过监督微调（SFT）时的LLM感知数据选择，定制训练数据，以适应模型的独特能力。这种方法使LLMs能够在测试时自主确定并应用合适的推理策略。我们通过在六个数学推理基准上的广泛实验评估了TATA，使用了通用和数学专门化的LLMs。实验结果表明，TATA有效地结合了CoT和TIR的互补优势，在推理效率上优于单独使用TIR，并实现了更优或相当的性能。进一步的分析强调了内在能力感知数据选择对于使LLMs能够做出有效和适应性的推理决策，并使其推理策略与模型能力相一致的关键作用。', 'title_zh': '根据自身优势教学大语言模型：自适应推理在数学问题解决中的应用'}
{'arxiv_id': 'arXiv:2502.12018', 'title': 'Atom of Thoughts for Markov LLM Test-Time Scaling', 'authors': 'Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.12018', 'abstract': 'Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）通过训练时扩展实现卓越性能，并在测试时扩展进一步增强了其推理能力，使其在推断过程中能够进行有效的推理。然而，随着推理规模的增加，现有的测试时扩展方法会累积历史信息，这不仅浪费了计算资源，还干扰了有效的推理。为解决这一问题，我们观察到复杂推理过程往往通过解决一系列独立的子问题来实现，每个子问题是自包含且可验证的。这些子问题本质上是原子问题，主要依赖于当前状态而不是累积的历史信息，类似于马尔可夫过程中的无记忆转移。基于这一观察，我们提出了Thought原子单元（AoT），其中推理过程中的每个状态转换包括将当前问题分解为基于依赖关系的有向无环图（DAG），并收缩其子问题，形成一个新的原子问题状态。这一逐步分解-收缩过程继续进行，直到达到可以直接解决的原子问题，自然实现了问题状态之间的马尔可夫转移。此外，这些原子问题可以无缝集成到现有的测试时扩展方法中，使AoT能够作为插件增强来提升推理能力。在六个基准测试中，AoT作为独立框架和插件增强均证明了其有效性。值得注意的是，在使用AoT对gpt-4o-mini进行处理时，其在HotpotQA上的F1分数达到了80.6%，超过了o3-mini的77.2%和DeepSeek-R1的70.0%，高出了3.4%和10.6%。源代码将在以下链接提供：[链接]。', 'title_zh': '思维原子化在马尔可夫LLM测试时缩放中的应用'}
{'arxiv_id': 'arXiv:2502.11962', 'title': 'Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware Instruction Fine-Tuning', 'authors': 'Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold', 'link': 'https://arxiv.org/abs/2502.11962', 'abstract': 'Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language Models (LLMs), but it may lower their truthfulness. This trade-off arises because IFT steers LLMs to generate responses with long-tail knowledge that is not well covered during pre-training, leading to more informative but less truthful answers when generalizing to unseen tasks. In this paper, we empirically demonstrate this helpfulness-truthfulness trade-off in IFT and propose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs to recognize their uncertainty and explicitly reflect it at the end of their responses. Experimental results show that UNIT-tuned models maintain their helpfulness while distinguishing between certain and uncertain claims, thereby reducing hallucinations.', 'abstract_zh': '指令微调（Instruction Fine-tuning, IFT）可以增强大型语言模型（Large Language Models, LLMs）的回答有用性，但可能会降低其真实性。这种权衡产生于IFT引导LLMs生成在预训练阶段未充分覆盖的长尾知识，导致在泛化到未见任务时，产生更具信息量但更不真实的回答。本文通过实证展示了IFT中有用性与真实性的权衡，并提出了一种新颖的IFT范式——$\\textbf{UNIT}$，以解决这一问题。UNIT 使LLMs能够识别自身的不确定性，并在回答结尾明确反映出这种不确定性。实验结果表明，经过UNIT微调的模型在保持有用性的同时，能够区分确定与不确定的断言，从而减少虚构回答。', 'title_zh': '带有不确定性意识的指令微调以导航帮助性与事实性之间的权衡'}
{'arxiv_id': 'arXiv:2502.11916', 'title': 'EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models', 'authors': 'Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu', 'link': 'https://arxiv.org/abs/2502.11916', 'abstract': "Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research. Our dataset and code will be available upon acceptance.", 'abstract_zh': '自动化作文评分（Automated Essay Scoring, AES）在教育评估中发挥着关键作用，通过提供可扩展且一致的写作任务评估。然而，传统的AES系统面临三大挑战：（1）依赖手工设计的特征，限制了其普适性；（2）难以捕捉细粒度的特征如连贯性和论证能力；（3）无法处理多模态上下文。在多模态大型语言模型（Multimodal Large Language Models, MLLMs）的时代，我们提出了EssayJudge，这是第一个评估AES能力的多模态基准，可以评估其在词级、句级和话语级特征上的表现。通过利用MLLMs在特征特定评分和多模态上下文理解方面的优势，EssayJudge旨在提供精确且富有上下文的信息评估，无需手动特征工程，解决传统AES的长期限制。我们的实验结果显示，18个代表性MLLMs在AES性能上存在与人工评估的差距，尤其是在话语级特征上，这表明需要进一步改进基于MLLMs的AES研究。我们的数据集和代码将在接受后提供。', 'title_zh': 'EssayJudge：评估多模态大型语言模型自动作文评分能力的多粒度基准'}
{'arxiv_id': 'arXiv:2502.11896', 'title': 'CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning', 'authors': 'Yanxiao Zhao, Yangge Qian, Jingyang Shan, Xiaolin Qin', 'link': 'https://arxiv.org/abs/2502.11896', 'abstract': "Reinforcement learning (RL) in continuous action spaces encounters persistent challenges, such as inefficient exploration and convergence to suboptimal solutions. To address these limitations, we propose CAMEL, a novel framework integrating LLM-generated suboptimal policies into the RL training pipeline. CAMEL leverages dynamic action masking and an adaptive epsilon-masking mechanism to guide exploration during early training stages while gradually enabling agents to optimize policies independently. At the core of CAMEL lies the integration of Python-executable suboptimal policies generated by LLMs based on environment descriptions and task objectives. Although simplistic and hard-coded, these policies offer valuable initial guidance for RL agents. To effectively utilize these priors, CAMEL employs masking-aware optimization to dynamically constrain the action space based on LLM outputs. Additionally, epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling agents to transition from constrained exploration to autonomous policy refinement. Experimental validation on Gymnasium MuJoCo environments demonstrates the effectiveness of CAMEL. In Hopper-v4 and Ant-v4, LLM-generated policies significantly improve sample efficiency, achieving performance comparable to or surpassing expert masking baselines. For Walker2d-v4, where LLMs struggle to accurately model bipedal gait dynamics, CAMEL maintains robust RL performance without notable degradation, highlighting the framework's adaptability across diverse tasks. While CAMEL shows promise in enhancing sample efficiency and mitigating convergence challenges, these issues remain open for further research. Future work aims to generalize CAMEL to multimodal LLMs for broader observation-action spaces and automate policy evaluation, reducing human intervention and enhancing scalability in RL training pipelines.", 'abstract_zh': '在连续动作空间中运用强化学习（RL）一直面临诸多挑战，例如探索效率低下和收敛到次优解。为解决这些问题，我们提出了一种名为CAMEL的新框架，该框架将由大型语言模型（LLM）生成的次优策略集成到RL训练管道中。CAMEL利用动态动作遮蔽和自适应ε-遮蔽机制，在早期训练阶段引导探索，同时逐步允许智能体独立优化策略。CAMEL的核心在于将基于环境描述和任务目标生成的可执行Python策略集成到框架中。尽管这些策略简单且预先编码，它们仍为RL智能体提供有价值的第一指导。\n\n为了有效地利用这些先验知识，CAMEL采用了遮蔽感知优化（Masking-Aware Optimization）方法，在LSTM输出的基础上动态约束动作空间。此外，ε-遮蔽机制逐步减少对LLM生成指导的依赖，使智能体能够从受限探索过渡到自主策略优化。在Gymnasium MuJoCo环境中的实验验证显示了CAMEL的有效性。在Hopper-v4和Ant-v4环境中，由LLM生成的策略显著提高了样本效率，性能与或超过专家设计的遮蔽基线。对于Walker2d-v4，由于LLM难以准确建模双足步行动力学，CAMEL在保持稳健的RL性能方面表现出色，而无需显著下降，这显示出该框架在不同任务中的适应性。\n\n尽管CAMEL在提高样本效率和缓解收敛问题方面展现出潜力，这些问题仍然需要进一步研究。未来的工作将致力于将CAMEL扩展到多模态LLM，以处理更大的观察-动作空间，并自动评估策略，降低人工干预并增强RL训练管道的可扩展性。', 'title_zh': 'CAMEL：由大语言模型支持的连续动作遮蔽强化学习方法'}
{'arxiv_id': 'arXiv:2502.11863', 'title': 'FedEAT: A Robustness Optimization Framework for Federated LLMs', 'authors': 'Yahao Pang, Xingyuan Wu, Xiaojin Zhang, Wei Chen, Hai Jin', 'link': 'https://arxiv.org/abs/2502.11863', 'abstract': 'Significant advancements have been made by Large Language Models (LLMs) in the domains of natural language understanding and automated content creation. However, they still face persistent problems, including substantial computational costs and inadequate availability of training data. The combination of Federated Learning (FL) and LLMs (federated LLMs) offers a solution by leveraging distributed data while protecting privacy, which positions it as an ideal choice for sensitive domains. However, Federated LLMs still suffer from robustness challenges, including data heterogeneity, malicious clients, and adversarial attacks, which greatly hinder their applications. We first introduce the robustness problems in federated LLMs, to address these challenges, we propose FedEAT (Federated Embedding space Adversarial Training), a novel framework that applies adversarial training in the embedding space of client LLM and employs a robust aggregation approach, specifically geometric median aggregation, to enhance the robustness of Federated LLMs. Our experiments demonstrate that FedEAT effectively improves the robustness of Federated LLMs with minimal performance loss.', 'abstract_zh': '大语言模型（LLMs）在自然语言理解和自动化内容生成领域取得了显著进展。然而，它们仍然面临着持续存在的问题，包括巨大的计算成本和训练数据不足。通过结合联邦学习（FL）和LLMs（联邦LLMs），可以在利用分布式数据的同时保护隐私，这使它成为敏感领域理想的解决方案。然而，联邦LLMs仍然面临着鲁棒性挑战，包括数据异质性、恶意客户端和对抗攻击，这些挑战极大地阻碍了它们的应用。我们首先介绍了联邦LLMs中的鲁棒性问题，并为应对这些挑战，我们提出了一种名为FedEAT（联邦嵌入空间对抗训练）的新框架。该框架在客户端LLM的嵌入空间中应用对抗训练，并采用一种鲁棒聚合方法（特别是几何中位数聚合），以增强联邦LLMs的鲁棒性。我们的实验表明，FedEAT能够在不显著牺牲性能的情况下有效提高联邦LLMs的鲁棒性。', 'title_zh': 'FedEAT：联邦大规模语言模型的健壮性优化框架'}
{'arxiv_id': 'arXiv:2502.11844', 'title': 'BaxBench: Can LLMs Generate Correct and Secure Backends?', 'authors': 'Mark Vero, Niels Mündler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanović, Jingxuan He, Martin Vechev', 'link': 'https://arxiv.org/abs/2502.11844', 'abstract': 'The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.', 'abstract_zh': '程序的自动生成一直是计算机科学中的一个基本挑战。最近的基准测试显示，大型语言模型（LLMs）能够有效地在函数级别生成代码，进行代码编辑，并解决算法编程任务。然而，要实现完全自动化，LLMs 应该能够生成符合生产标准、自包含的应用模块。为了评估LLMs在解决这一挑战方面的能力，我们引入了BaxBench，这是一个新的评估基准，包含392个任务，用于生成后端应用。我们重点关注后端应用的三个原因：（i）它们实践相关，是大多数现代网络和云计算软件的核心组成部分；（ii）它们难以做到完美，需要多个函数和文件才能实现所需的功能；（iii）它们是安全性关键的，因为它们对外暴露给不可信的第三方，使防止部署时攻击的安全解决方案变得至关重要。BaxBench通过全面的测试案例验证生成应用程序的功能，并通过端到端的攻击执行来评估它们的安全风险。我们的实验揭示了当前LLMs在功能和安全方面的主要局限性：（i）即使是最好的模型，OpenAI O1，在代码正确性方面仅能达到60%；（ii）平均而言，我们能够在超过一半由每个LLM生成的正确程序上成功执行安全性攻击；（iii）在不太常用的后端框架中，模型进一步难以生成正确且安全的应用程序。BaxBench上的进展标志着使用LLMs实现自主和安全软件开发的重要步骤。', 'title_zh': 'BaxBench：大型语言模型能否生成正确的且安全的后端代码？'}
{'arxiv_id': 'arXiv:2502.11843', 'title': 'Can LLM Agents Maintain a Persona in Discourse?', 'authors': 'Pranav Bhandari, Nicolas Fay, Michael Wise, Amitava Datta, Stephanie Meek, Usman Naseem, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.11843', 'abstract': 'Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.', 'abstract_zh': '大规模语言模型（LLMs）广泛用于对话代理领域，它们在教育、法律、医学等多个领域中发挥了其各种能力。然而，LLMs经常表现出上下文转换的行为，导致缺乏一致性和可解释性的人格匹配交互。对于心理特质的遵守缺乏全面分析，尤其是在双边（成对）对话中。我们从两个视角来探讨这一挑战：首先，使用两个对话代理生成特定话题的讨论，并赋予每种气质（即开放性、责任心、外向性、随和性、神经质）高/低等级；随后，使用多个评判代理来推断原始分配的气质，以探索预测一致性、模型间一致性以及与分配的人格的匹配程度。我们的研究发现，虽然LLMs可以被引导进行基于人格的对话，但它们维持特定气质的能力在不同模型组合和话题设置下存在显著差异。这些不一致性突显了在LLMs中实现稳定和可解释的人格匹配交互的挑战。', 'title_zh': 'LLM代理在对话中能否保持人设？'}
{'arxiv_id': 'arXiv:2502.11829', 'title': 'Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities', 'authors': 'Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu', 'link': 'https://arxiv.org/abs/2502.11829', 'abstract': "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at this https URL.", 'abstract_zh': '本文介绍了Code-Vision，这是一个用于评估多模态大型语言模型（MLLMs）的逻辑理解和代码生成能力的基准。该基准挑战MLLMs根据给定的流程图生成满足特定功能要求的正确程序，而流程图则直观地表示了所需算法或过程。Code-Vision 包含三个子集：HumanEval-V、Algorithm 和 MATH，分别评估MLLMs在基本编程、算法和数学问题解决领域的编码能力。我们的实验在Code-Vision上评估了12个MLLMs。实验结果表明，专有模型和开源模型之间的性能差距很大。在难题上，GPT-4o 可以实现79.3%的通过率，但最好的开源模型仅能达到15%。进一步的实验表明，与多模态推理基准MMCode和MathVista相比，Code-Vision 可能会提出独特的挑战。我们还探讨了开源模型表现不佳的原因。所有数据和代码均可在此访问：https://example.com（请将https://example.com替换为实际的网址）。', 'title_zh': 'Code-Vision：评估多模态大语言模型的逻辑理解和代码生成能力'}
{'arxiv_id': 'arXiv:2502.11812', 'title': 'Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis', 'authors': 'Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou', 'link': 'https://arxiv.org/abs/2502.11812', 'abstract': 'Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.', 'abstract_zh': '对大型语言模型（LLMs）进行微调可以显著提高其性能，但其背后的机制仍不完全清楚。本文旨在通过电路分析，一种机制可解释性（MI）中常用的方法，深入解析微调过程。与前人研究\\[ \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} \\]主要关注预训练模型已经在该任务上表现出色的任务不同，我们开发了一组数学任务，在这些任务上，微调提供了显著的性能提升，更接近实际应用场景。在我们的实验中，我们在微调过程中的各个检查点识别电路，并分析电路分析、微调方法与任务复杂性之间的相互作用。我们首先发现，在微调前后，电路的节点保持较高的相似性，但边发生了重大变化。这与之前的研究所示\\[ \\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} \\]存在差异，前人研究显示微调后电路仅添加一些额外的组件。基于这些观察，我们开发了一种电路意识低秩适应（LoRA）方法，该方法根据电路中边的变化为层分配秩。实验结果表明，我们的电路驱动的LoRA算法在与标准LoRA参数量相似的情况下，实现了平均性能提升2.46%。此外，我们还探讨了如何结合子任务的电路来增强组合任务中的微调，为这类任务的设计提供了新的见解，并加深了对电路动态和微调机制的理解。', 'title_zh': '通过电路分析理解大规模语言模型微调机制'}
{'arxiv_id': 'arXiv:2502.11771', 'title': 'The Validation Gap: A Mechanistic Analysis of How Language Models Compute Arithmetic but Fail to Validate It', 'authors': 'Leonardo Bertolazzi, Philipp Mondorf, Barbara Plank, Raffaella Bernardi', 'link': 'https://arxiv.org/abs/2502.11771', 'abstract': "The ability of large language models (LLMs) to validate their output and identify potential errors is crucial for ensuring robustness and reliability. However, current research indicates that LLMs struggle with self-correction, encountering significant challenges in detecting errors. While studies have explored methods to enhance self-correction in LLMs, relatively little attention has been given to understanding the models' internal mechanisms underlying error detection. In this paper, we present a mechanistic analysis of error detection in LLMs, focusing on simple arithmetic problems. Through circuit analysis, we identify the computational subgraphs responsible for detecting arithmetic errors across four smaller-sized LLMs. Our findings reveal that all models heavily rely on $\\textit{consistency heads}$--attention heads that assess surface-level alignment of numerical values in arithmetic solutions. Moreover, we observe that the models' internal arithmetic computation primarily occurs in higher layers, whereas validation takes place in middle layers, before the final arithmetic results are fully encoded. This structural dissociation between arithmetic computation and validation seems to explain why current LLMs struggle to detect even simple arithmetic errors.", 'abstract_zh': '大型语言模型（LLMs）验证其输出并识别潜在错误的能力对于确保其稳健性和可靠性至关重要。然而，当前的研究表明，LLMs 在自我纠正方面存在困难，遇到了显著的错误检测挑战。虽然有研究探讨了增强LLMs 自我纠正能力的方法，但在理解模型内部负责错误检测的机制方面关注较少。在此论文中，我们对LLMs 的错误检测机制进行了机制分析，重点关注简单的算术问题。通过电路分析，我们确定了四个较小规模的LLMs 中负责检测算术错误的计算子图。研究发现，所有模型都严重依赖于“一致性头”——评估算术解决方案中数值表面一致性的注意头。此外，我们观察到，模型内部的算术计算主要发生在较高层次，而验证则发生在中间层次，在最终的算术结果完全编码之前。这种算术计算与验证之间的结构分离似乎解释了当前LLMs 为何难以检测即使是简单的算术错误。', 'title_zh': '验证缺口：语言模型在计算算术而不验证其正确性方面的机制分析'}
{'arxiv_id': 'arXiv:2502.11751', 'title': 'Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning', 'authors': 'Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang', 'link': 'https://arxiv.org/abs/2502.11751', 'abstract': "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multimodal Large Language Models (MLLMs), however, is resource-intensive and constrained by various training limitations. In this paper, we propose the Modular-based Visual Contrastive Decoding (MVCD) framework to move this obstacle. Our framework leverages LLMs' In-Context Learning (ICL) capability and the proposed visual contrastive-example decoding (CED), specifically tailored for this framework, without requiring any additional training. By converting visual signals into text and focusing on contrastive output distributions during decoding, we can highlight the new information introduced by contextual examples, explore their connections, and avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual perception to make it see and reason over the input visuals. To demonstrate MVCD's effectiveness, we conduct experiments with four LLMs across five question answering datasets. Our results not only show consistent improvement in model accuracy but well explain the effective components inside our decoding strategy. Our code will be available at this https URL.", 'abstract_zh': '尽管大型语言模型（LLMs）在语言任务中的推理和生成表现优异，但它们并没有专门为多模态挑战而设计。然而，训练多模态大型语言模型（MLLMs）需要大量资源，并受到各种训练限制。在本文中，我们提出了基于模块化视觉对比解码（MVCD）框架来克服这一障碍。我们的框架利用了LLMs的上下文学习（ICL）能力，并结合了专门为该框架设计的视觉对比解码（CED），而不需要额外的训练。通过将视觉信号转换为文本并在解码过程中关注对比输出分布，我们可以突出上下文示例引入的新信息，探索它们之间的关联，从而避免过度依赖先验编码知识。MVCD增强了LLMs的视觉感知能力，使其能够对输入视觉进行观察和推理。为了展示MVCD的有效性，我们在五个问答数据集中对四种LLMs进行了实验。我们的结果不仅展示了模型准确性的持续改进，还详细解释了解码策略中的有效组件。相关代码将在此网址中提供：this https URL。', 'title_zh': '语言模型视觉能力提升：用于大模型多模态推理的视觉对比解码'}
{'arxiv_id': 'arXiv:2502.11741', 'title': 'SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL', 'authors': 'Shuai Lyu, Haoran Luo, Zhonghong Ou, Yifan Zhu, Xiaoran Shang, Yang Qin, Meina Song', 'link': 'https://arxiv.org/abs/2502.11741', 'abstract': 'The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available at:this https URL.', 'abstract_zh': '文本到SQL（Text2SQL）任务旨在将自然语言查询转换为可执行的SQL查询。得益于大规模语言模型（LLMs）的应用，该领域取得了显著进展。然而，在SQL生成过程中，模型可扩展性、生成空间有限以及语义连贯性问题仍然存在。为了解决这些问题，我们提出了一种名为SQL-o1的基于自我奖励的启发式搜索方法，旨在增强LLMs在SQL查询生成中的推理能力。SQL-o1结合了蒙特卡洛树搜索（MCTS）作为启发式过程级搜索，并构建了一个模式感知的数据集，以帮助模型更好地理解数据库模式。在Bird和Spider数据集上的广泛实验显示，与最新的基线方法相比，SQL-o1在复杂数据集Bird上的执行准确率提高了10.8%，甚至优于基于GPT-4的方法。此外，SQL-o1在少样本学习场景中表现出色，并显示出了较强跨模型的迁移能力。我们的代码已在此处公开：[此链接]。', 'title_zh': 'SQL-o1：一种自我奖励启发式动态搜索方法用于文本到SQL'}
{'arxiv_id': 'arXiv:2502.11705', 'title': 'LLM Agents Making Agent Tools', 'authors': 'Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather', 'link': 'https://arxiv.org/abs/2502.11705', 'abstract': 'Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.', 'abstract_zh': '工具使用已将大型语言模型（LLMs）转变为能够通过动态利用外部软件组件执行复杂多步任务的强大代理。然而，这些工具必须由人类开发者在预先进行开发，这妨碍了LLM代理在需要大量高度专业化工具的领域中的应用，如生命科学和医学。鉴于科学研究与公共代码仓库一同增长的趋势，我们提出了ToolMaker，这是一种新颖的代理框架，能够自主将包含代码的论文转化为与LLM兼容的工具。给定简要的任务描述和代码仓库URL，ToolMaker能够自主安装所需依赖项并生成代码以执行任务，并采用闭环自校正机制逐次诊断并纠正错误。\n\n为了评估我们的方法，我们引入了一个基准测试集，包含15项涵盖医疗和非医疗领域的多样且复杂的计算任务，并包含超过100个单元测试，以客观评估工具的正确性和鲁棒性。ToolMaker成功实现80%的任务，显著优于当前最先进的软件工程代理。因此，ToolMaker代表着完全自主代理驱动的科学工作流的一个进展。', 'title_zh': '“LLM代理创建代理工具”'}
{'arxiv_id': 'arXiv:2502.11684', 'title': 'MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task', 'authors': 'Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Xin Xu, Mengdi Zhang, Jian Shao, Yueting Zhuang', 'link': 'https://arxiv.org/abs/2502.11684', 'abstract': 'Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.', 'abstract_zh': '数学推理是推动大规模语言模型（LLMs）发展的关键前沿领域。虽然逐步的方法已成为LLMs中数学问题解决的主要范式，但训练数据中推理步骤的质量从根本上限制了模型的表现。最近的研究表明，更详细的中间步骤可以提升模型性能，然而目前存在的步骤扩展方法要么需要更强大的外部模型，要么会产生大量的计算成本。在本文中，我们引入了MathFimer，这是一种受代码完成任务中的“填充中间部分”任务启发的数学推理步骤扩展新型框架。我们通过将解题链分解为前缀-后缀对，并训练模型重建缺失的中间步骤，开发了专门针对我们精心筛选的NuminaMath-FIM数据集的MathFimer-7B模型。然后，我们应用这些模型通过在现有的数学推理数据集的解题链中插入详细的中间步骤，生成了MathFimer扩展版本。通过在包括MathInstruct、MetaMathQA等在内的多个数学推理数据集上的全面实验，我们证明，基于MathFimer扩展数据训练的模型在GS8K和MATH等各个基准测试中的一致表现优于基于原始数据训练的模型。我们的方法提供了一种无需依赖强大外部模型或昂贵推断过程的实用且可扩展的方法，从而提升LLMs的数学推理能力。', 'title_zh': 'MathFimer：通过填空任务扩展推理步骤以增强数学推理能力'}
{'arxiv_id': 'arXiv:2502.11681', 'title': 'RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars', 'authors': 'Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari', 'link': 'https://arxiv.org/abs/2502.11681', 'abstract': 'Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at this https URL.', 'abstract_zh': '确保大型语言模型（LLMs）在行为上符合伦理和提供帮助的关键在于对其进行对齐调整。当前的对齐方法需要高质量的标注和大量的训练资源。本文提出了一种低成本且无需调整的方法，利用上下文学习（ICL）来增强LLM的对齐能力。通过对高质量ICL示范案例的分析，我们发现风格是影响LLM对齐能力的关键因素，并根据这一风格框架明确重塑了ICL范例。此外，我们结合重塑的示范案例，以平衡LLM对齐的两个相互冲突方面——事实性和安全性。我们将重塑后的示例打包为提示，以激发少量样本学习，从而提高LLM的对齐能力。与最好的基线方法相比，该方法在Alpaca任务上的得分从4.50提高到了4.60（增加了0.10），在Just-eval基准上的得分从4.34提高到了4.56（提高了0.22），在MT-Bench数据集上的最佳改进为0.32（从3.53提高到了3.85）。我们在此网址发布代码和数据：https://www.example.com。', 'title_zh': 'RIDE：通过重风格的上下文示例演示增强大规模语言模型对齐\n\n注：此翻译保持了原文的缩写“RIDE”不变，并将“Restyled In-Context Learning Demonstration Exemplars”翻译为“通过重风格的上下文示例演示增强大规模语言模型对齐”，以符合学术规范和流畅性。'}
{'arxiv_id': 'arXiv:2502.11671', 'title': 'Diversity-Oriented Data Augmentation with Large Language Models', 'authors': 'Zaitian Wang, Jinghan Zhang, Xinhao Zhang, Kunpeng Liu, Pengfei Wang, Yuanchun Zhou', 'link': 'https://arxiv.org/abs/2502.11671', 'abstract': "Data augmentation is an essential technique in natural language processing (NLP) for enriching training datasets by generating diverse samples. This process is crucial for improving the robustness and generalization capabilities of NLP models. However, a significant challenge remains: \\textit{Insufficient Attention to Sample Distribution Diversity}. Most existing methods focus on increasing the sample numbers while neglecting the sample distribution diversity, which can lead to model overfitting. In response, we explore data augmentation's impact on dataset diversity and propose a \\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data \\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). % \\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning approach to train an LLM as a diverse paraphraser, which is capable of augmenting textual datasets by generating diversified paraphrases. Then, we apply the LLM paraphraser to a selected coreset of highly informative samples and integrate the paraphrases with the original data to create a more diverse augmented dataset. Finally, we conduct extensive experiments on 12 real-world textual datasets. The results show that our fine-tuned LLM augmenter improves diversity while preserving label consistency, thereby enhancing the robustness and performance of downstream tasks. Specifically, it achieves an average performance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more than three percentage points.", 'abstract_zh': '数据增强是自然语言处理（NLP）中一种重要的技术，通过生成多样化的样本来丰富训练数据集。这一过程对于提高NLP模型的鲁棒性和泛化能力至关重要。然而，仍存在一个重大挑战：**忽略样本分布的多样性**。大多数现有方法集中在增加样本数量上，而忽视了样本分布的多样性，这可能导致模型过拟合。针对这一问题，我们探索了数据增强对数据集多样性的影响，并提出了一种**Diversify-Oriented 数据增强框架（DoAug）**。具体而言，我们采用一种以多样性为导向的微调方法，训练一个语言模型作为多样化的同义表达生成器，该生成器能够通过生成多样化的同义句来增强文本数据集。然后，我们将语言模型的同义表达生成器应用于一组高度信息性的样本子集，并将生成的同义句与原始数据集成，从而创建一个更加多样化的增强数据集。最后，我们在12个真实的文本数据集上进行了广泛的实验。结果表明，我们的微调语言模型增强器在保持标签一致性的前提下提高了数据的多样性，从而增强了下游任务的鲁棒性和性能。具体而言，它实现了平均 \\(10.52\\%\\) 的性能提升，比亚军基准高出超过三个百分点。', 'title_zh': '面向多样性的数据增强方法与大规模语言模型'}
{'arxiv_id': 'arXiv:2502.11603', 'title': 'DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware Prompting with Demonstration and Reasoning', 'authors': 'Hongye Qiu, Yue Xu, Meikang Qiu, Wenjie Wang', 'link': 'https://arxiv.org/abs/2502.11603', 'abstract': 'Large Language Models (LLMs) exhibit strong natural language processing capabilities but also inherit and amplify societal biases, including gender bias, raising fairness concerns. Existing debiasing methods face significant limitations: parameter tuning requires access to model weights, prompt-based approaches often degrade model utility, and optimization-based techniques lack generalizability. To address these challenges, we propose this http URL (Demonstration and Reasoning for Gender-Aware Prompting), an automated and model-agnostic approach that mitigates gender bias while preserving model performance. this http URL selects bias-revealing examples and generates structured reasoning to guide models toward more impartial responses. Extensive experiments on coreference resolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and Llama2-Alpaca) demonstrate its effectiveness, generalization ability, and robustness. this http URL can generalize to vision-language models (VLMs), achieving significant bias reduction.', 'abstract_zh': '大型语言模型（LLMs）展示了强大的自然语言处理能力，但也继承并放大了社会偏见，包括性别偏见，这引发了公平性方面的担忧。现有的去偏方法面临着重大局限：参数调整需要访问模型权重，基于提示的方法往往会降低模型的实用性，而基于优化的方法缺乏普适性。为解决这些挑战，我们提出了一种名为(this http URL)（性别意识提示的演示与推理）的方法，这是一种自动化且模型无依赖性的方法，能够在减轻性别偏见的同时保持模型性能。该方法通过选择揭示偏见的示例并生成结构化的推理来引导模型产生更为公正是的答案。通过在多个LLM（GPT-3.5、Llama3和Llama2-Alpaca）上的核心参照解析和问答任务中的广泛实验，我们展示了其有效性和泛化能力以及鲁棒性。此外，该方法还可以推广到视觉语言模型（VLMs），实现显著的偏见减少。', 'title_zh': 'DR.GAP：利用性别 Awareness 提示示范与推理减轻大型语言模型中的偏见'}
{'arxiv_id': 'arXiv:2502.11596', 'title': 'LLM Embeddings for Deep Learning on Tabular Data', 'authors': 'Boshko Koloski, Andrei Margeloiu, Xiangjian Jiang, Blaž Škrlj, Nikola Simidjievski, Mateja Jamnik', 'link': 'https://arxiv.org/abs/2502.11596', 'abstract': 'Tabular deep-learning methods require embedding numerical and categorical input features into high-dimensional spaces before processing them. Existing methods deal with this heterogeneous nature of tabular data by employing separate type-specific encoding approaches. This limits the cross-table transfer potential and the exploitation of pre-trained knowledge. We propose a novel approach that first transforms tabular data into text, and then leverages pre-trained representations from LLMs to encode this data, resulting in a plug-and-play solution to improv ing deep-learning tabular methods. We demonstrate that our approach improves accuracy over competitive models, such as MLP, ResNet and FT-Transformer, by validating on seven classification datasets.', 'abstract_zh': '表型深度学习方法在处理数据之前，需要将数值和分类输入特征嵌入到高维空间中。现有方法通过使用特定类型的数据编码方法来应对表型数据的异构性，这限制了跨表传输的潜力以及预训练知识的利用。我们提出了一种新的方法，首先将表型数据转换为文本，然后利用预训练的大语言模型（LLM）来编码这些数据，从而提供一种即插即用的解决方案，以改进深度学习表型方法。我们通过在七个分类数据集上进行验证，表明我们的方法在准确率上优于竞争模型，如MLP、ResNet和FT-Transformer。', 'title_zh': '大规模语言模型嵌入在表格数据深度学习中的应用'}
{'arxiv_id': 'arXiv:2502.11578', 'title': 'Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance', 'authors': 'Birger Moell, Johan Boye', 'link': 'https://arxiv.org/abs/2502.11578', 'abstract': "Large Language Models (LLMs) have made significant strides in natural language generation but often face challenges in tasks requiring precise calculations and structural analysis. This paper investigates the performance of state-of-the-art LLMs on language complexity measurement tasks, through the computation of the LIX readability metric and Average Dependency Distance (ADD). Using Swedish high school and university-level essays, we evaluate the models' abilities to compute LIX scores and perform dependency parsing, comparing their results to established ground truths. Our findings reveal that while all models demonstrate some capacity for these tasks, ChatGPT-o1-mini performs most consistently, achieving the highest accuracy in both LIX computation and dependency parsing. Additionally, we observe a strong significant correlation -0.875 p 0.026 (N=6) between the models' accuracy in computing LIX and their overall performance on the Massive Multitask Language Understanding (MMLU) benchmark. These results suggest that language complexity measurement abilities can serve as a noisy zero-shot proxies for assessing the general capabilities of LLMs, providing a practical method for model evaluation without the need for extensive benchmarking datasets.", 'abstract_zh': '大型语言模型（LLM）在自然语言生成方面取得了显著进步，但在需要精确计算和结构分析的任务中常常面临挑战。本论文通过计算LIX可读性度量和平均依存距离（ADD）来研究最先进的LLM在语言复杂度度量任务中的表现。我们使用瑞典中学和大学水平的文章，评估模型计算LIX分数和执行依存解析的能力，并将其结果与已有的基准真相进行比较。我们的研究结果表明，虽然所有模型都在这些任务上展示出某种程度的能力，但ChatGPT-o1-mini表现最为稳定，其在LIX计算和依存解析上的准确性最高。此外，我们还观察到模型在计算LIX的准确性与其在大规模多任务语言理解（MMLU）基准测试上的整体性能之间存在显著相关性（相关系数为-0.875，p值为0.026，样本量为6）。这些结果表明，语言复杂度度量能力可以作为评估LLM普遍能力的一种噪声零样本代理，提供了一种无需大量基准数据集即可评估模型的有效方法。', 'title_zh': '语言复杂度测量作为评价大型语言模型性能的 noisy 零样本代理'}
{'arxiv_id': 'arXiv:2502.11573', 'title': 'InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning', 'authors': 'Congkai Xie, Shuo Cai, Wenjun Wang, Pengxiang Li, Zhijie Sang, Kejing Yang, Yiming Zhang, Zhen Li, Guanghao Zhu, Zeyu Liu, Yang Yu, Yuhang Liu, Su Lu, Baoyi He, Qi Zhou, Xiaotian Han, Jianbo Yuan, Shengyu Zhang, Fei Wu, Hongxia Yang', 'link': 'https://arxiv.org/abs/2502.11573', 'abstract': 'Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \\InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.', 'abstract_zh': '大规模语言模型（LLMs）和多模态大规模语言模型（MLLMs）在推理能力方面取得了显著进步。然而，它们仍然面临计算成本高的挑战以及隐私问题。本文致力于开发高效的中小型语言模型（SLMs）和多模态中小型语言模型（MSLMs），这些模型能够在保留竞争性推理能力的基础上降低开发成本。我们提出了一种新的训练流程，旨在增强推理能力并促进边缘设备上的部署，从而实现最先进的性能。InInfR 旨在通过提升推理能力、降低采用门槛和通过较小的模型规模解决隐私问题，来推动AI系统的进步。更多资源可在 https://github.com/Reallm-Labs/InInfR 获取。', 'title_zh': 'InfiR：打造高效的小型语言模型与推理性的小型多模态语言模型'}
{'arxiv_id': 'arXiv:2502.11569', 'title': 'Towards Reasoning Ability of Small Language Models', 'authors': 'Gaurav Srivastava, Shuxiang Cao, Xuan Wang', 'link': 'https://arxiv.org/abs/2502.11569', 'abstract': 'Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.', 'abstract_zh': '长期来看，推理能力被认为是由大规模语言模型（LLMs）在某一特定规模（约100亿参数）以上时才体现出的一种 emergent 属性。然而，近期的研究挑战了这一假设，表明小型语言模型（SLMs）也能达到竞争力的推理表现。SLMs 因其高效的特性和易于部署而受到青睐。然而，关于多样化的 SLMs（包括从零开始训练和通过量化、剪枝和蒸馏从LLMs衍生的模型）的推理能力系统性研究仍然不足。这引发了一个关键问题：SLMs 是否能够在推理能力上与LLMs 相媲美？本研究系统地调研、基准测试和分析了六大家族中的72个 SLM 在14个推理基准上的表现。为了确保评估的可靠性，我们检查了四种评估方法，并在800个数据点上将四种LLM评估者与人类评价进行比较。我们重复所有实验三次，以确保能够进行稳健的表现评估。此外，我们还分析了不同类型提示策略对小型模型的影响。除了准确性，我们还评估了模型在对抗性条件下的鲁棒性以及推理过程中的中间步骤。我们的发现挑战了认为仅通过扩展才能实现强大的推理能力这一假设。相反，我们预见未来可以通过结构化训练或后训练压缩来开发具备强大推理能力的SLMs，并将其作为LLMs在推理密集任务中的有效替代品。', 'title_zh': '面向小型语言模型的推理能力研究'}
{'arxiv_id': 'arXiv:2502.11541', 'title': 'MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training', 'authors': 'Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao', 'link': 'https://arxiv.org/abs/2502.11541', 'abstract': 'Complex instruction-following with elaborate constraints is imperative for Large Language Models (LLMs). While existing methods have constructed data for complex instruction alignment, they all rely on a more advanced model, especially GPT-4, limiting their application. In this paper, we propose a Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the complex instruction alignment without relying on a stronger model. Our method is conducted on both coarse and fine granularity. On coarse-granularity, we construct constraint-aware preference data based on instruction decomposition and recombination. On fine-granularity, we perform token-aware preference optimization with dynamic token-level supervision. Our method is evaluated on open-sourced models, and experiment results show our method achieves significant improvement on both complex and general instruction-following benchmarks, surpassing previous self-alignment methods.', 'abstract_zh': '对于大型语言模型（LLMs），执行复杂指令并伴有详细约束是必不可少的。虽然现有方法已经构建了用于复杂指令对齐的数据，但它们都依赖于更为先进的模型，尤其是GPT-4，这限制了它们的应用范围。本文中，我们提出了一种多层次自我对比训练（MuSC）框架，以提高复杂指令对齐的能力，而不依赖于更强的模型。我们的方法在粗粒度和细粒度两个级别上进行。\n\n在粗粒度级别上，我们基于指令分解和重组构建了具有约束感知的偏好数据。在细粒度级别上，我们进行了具有动态词级监督的感知 token 偏好优化。我们的方法在开源模型上进行了评估，实验结果显示，我们的方法在复杂和通用指令执行基准上均取得了显著改进，超越了之前的自我对齐方法。', 'title_zh': 'MuSC：通过多粒度自对比训练改进复杂指令跟随'}
{'arxiv_id': 'arXiv:2502.11521', 'title': 'DeFiScope: Detecting Various DeFi Price Manipulations with LLM Reasoning', 'authors': 'Juantao Zhong, Daoyuan Wu, Ye Liu, Maoyi Xie, Yang Liu, Yi Li, Ning Liu', 'link': 'https://arxiv.org/abs/2502.11521', 'abstract': "DeFi (Decentralized Finance) is one of the most important applications of today's cryptocurrencies and smart contracts. It manages hundreds of billions in Total Value Locked (TVL) on-chain, yet it remains susceptible to common DeFi price manipulation attacks. Despite state-of-the-art (SOTA) systems like DeFiRanger and DeFort, we found that they are less effective to non-standard price models in custom DeFi protocols, which account for 44.2% of the 95 DeFi price manipulation attacks reported over the past three years.\nIn this paper, we introduce the first LLM-based approach, DeFiScope, for detecting DeFi price manipulation attacks in both standard and custom price models. Our insight is that large language models (LLMs) have certain intelligence to abstract price calculation from code and infer the trend of token price changes based on the extracted price models. To further strengthen LLMs in this aspect, we leverage Foundry to synthesize on-chain data and use it to fine-tune a DeFi price-specific LLM. Together with the high-level DeFi operations recovered from low-level transaction data, DeFiScope detects various DeFi price manipulations according to systematically mined patterns. Experimental results show that DeFiScope achieves a high precision of 96% and a recall rate of 80%, significantly outperforming SOTA approaches. Moreover, we evaluate DeFiScope's cost-effectiveness and demonstrate its practicality by helping our industry partner confirm 147 real-world price manipulation attacks, including discovering 81 previously unknown historical incidents.", 'abstract_zh': '去中心化金融（DeFi, Decentralized Finance）是当今加密货币和智能合约的重要应用之一。它在链上管理着数百亿美元的总资产（TVL），然而仍然容易受到常见的DeFi价格操控攻击。尽管有最先进的（SOTA）系统如DeFiRanger和DeFort，我们发现它们在处理定制DeFi协议中的非标准价格模型时效果较差，而这些定制协议在过去三年内报告的95起DeFi价格操控攻击中占44.2%。\n\n在本文中，我们提出了首个基于大语言模型（LLM, Large Language Model）的方法DeFiScope，用于检测标准和定制价格模型中的DeFi价格操控攻击。我们的见解在于，大语言模型具有一定的智能，能够从代码中抽象出价格计算，并根据提取的价格模型推断代币价格变化的趋势。为了进一步增强大语言模型在这方面的能力，我们利用Foundry合成了链上数据，并利用这些数据对特定于DeFi价格的大语言模型进行了微调。结合从低级别交易数据中恢复的高阶DeFi操作，DeFiScope根据系统挖掘出的模式检测各种DeFi价格操控。实验结果表明，DeFiScope的精度达到96%，召回率达到80%，显著优于SOTA方法。此外，我们评估了DeFiScope的成本效益，并通过帮助行业合作伙伴确认147起真实的DeFi价格操控攻击（其中包括发现81起以前未知的事件）展示了其实用性。', 'title_zh': 'DeFiScope：利用大语言模型推理检测各种DeFi价格操纵'}
{'arxiv_id': 'arXiv:2502.11491', 'title': 'Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering', 'authors': 'Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin', 'link': 'https://arxiv.org/abs/2502.11491', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.', 'abstract_zh': '大型语言模型（LLMs）在自然语言处理方面展现了显著的能力。然而，在知识图谱问答任务（KGQA）中，仍然存在需要进行多跳推理的问题。现有方法依赖于实体向量匹配，但问题的目的往往是抽象的，难以与特定实体匹配，这使得难以建立通往目的的推理路径，从而导致信息丢失和冗余。为了解决这一问题，受人类逆向思维的启发，我们提出了一种新颖的框架——基于本体的逆向思维（Ontology-Guided Reverse Thinking, ORT），该框架从目的回溯到条件来构建推理路径。ORT 在三个关键阶段进行操作：（1）使用LLMs提取目的标签和条件标签；（2）根据知识图谱本体构建标签推理路径；（3）使用标签推理路径指导知识检索。在WebQSP和CWQ数据集上的实验表明，ORT 达到了最先进的性能，并显著增强了LLMs在KGQA中的能力。', 'title_zh': '基于本体引导的逆向思考使大型语言模型在知识图谱问答任务中更为强大'}
{'arxiv_id': 'arXiv:2502.11458', 'title': 'Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models', 'authors': 'Jiecheng Zhou, Ding Tang, Rong Fu, Boni Hu, Haoran Xu, Yi Wang, Zhilin Pei, Zhongling Su, Liang Liu, Xingcheng Zhang, Weiming Zhang', 'link': 'https://arxiv.org/abs/2502.11458', 'abstract': 'The burgeoning computational demands for training large language models (LLMs) necessitate efficient methods, including quantized training, which leverages low-bit arithmetic operations to reduce costs. While FP8 precision has shown potential, leveraging FP4 remains challenging due to inherent quantization errors and limited representation capability. Based on the Transformer architecture, we present an FP4 training scheme for LLMs, overcoming these obstacles through mixed-precision quantization strategies tailed for different modules and training stages. This allows us to apply the precision level suitable to distinct components within the model, ensuring that multi-head attention and linear layers are handled appropriately. Our pretraining recipe ensures stability in backpropagation by incorporating fine-grained quantization methods with a target precision training schedule. Experimental results demonstrate that our FP4 training scheme achieves accuracy comparable to BF16 and FP8, with smaller theoretical computational cost. With the advent of next-generation hardware supporting FP4, our method sets the foundation for efficient ultra-low precision training.', 'abstract_zh': '随着训练大规模语言模型（LLMs）所需的计算需求日益增长，迫切需要高效的方法，其中包括量化训练，该方法通过使用低比特位数的算术运算来降低成本。尽管FP8精度显示出了潜力，但在利用FP4方面仍然面临挑战，这是由于固有的量化误差和有限的表示能力。基于Transformer架构，我们提出了一种适用于LLMs的FP4训练方案，通过针对不同模块和训练阶段的混合精度量化策略克服了这些障碍。这种方法允许我们根据不同模型组件适用的精度水平来调整精度，确保多头注意力机制和线性层能够得到适当处理。通过将精细量化方法与目标精度训练计划相结合，我们的预训练方案确保了反向传播过程的稳定性。实验结果表明，我们的FP4训练方案在理论计算成本更低的情况下，能够达到与BF16和FP8相当的准确性。在适用于FP4的下一代硬件即将问世的情况下，我们的方法为高效超低精度训练奠定了基础。', 'title_zh': '向量高效预训练：探索大语言模型中的FP4精度'}
{'arxiv_id': 'arXiv:2502.11425', 'title': 'Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models', 'authors': 'Jongho Kim, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2502.11425', 'abstract': "Despite the advanced capabilities of large language models (LLMs), their temporal reasoning ability remains underdeveloped. Prior works have highlighted this limitation, particularly in maintaining temporal consistency when understanding events. For example, models often confuse mutually exclusive temporal relations like ``before'' and ``after'' between events and make inconsistent predictions. In this work, we tackle the issue of temporal inconsistency in LLMs by proposing a novel counterfactual prompting approach. Our method generates counterfactual questions and enforces collective constraints, enhancing the model's consistency. We evaluate our method on multiple datasets, demonstrating significant improvements in event ordering for explicit and implicit events and temporal commonsense understanding by effectively addressing temporal inconsistencies.", 'abstract_zh': '尽管大型语言模型（LLMs）具备先进的功能，但在处理时间推理方面仍存在不足。先前的研究已经指出了这一限制，特别是在理解事件时保持时间一致性方面。例如，模型经常混淆互斥的时间关系（如“之前”和“之后”），并作出不一致的预测。在本工作中，我们通过提出一种新的反事实提示方法应对LLMs中的时间不一致性问题。我们的方法生成反事实问题并施加集体约束，从而提升模型的一致性。我们通过在多个数据集上的评估展示了我们的方法在事件排序以及通过有效解决时间不一致性来增强时间常识理解方面的显著改进。', 'title_zh': '相对时间理解中的一种反事实一致性提示方法'}
{'arxiv_id': 'arXiv:2502.11368', 'title': 'LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing', 'authors': 'Zhengxiang Wang, Veronika Makarova, Zhi Li, Jordan Kodner, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.11368', 'abstract': 'The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus for reproducibility.', 'abstract_zh': '本文探讨了大型语言模型（LLMs）在多维度分析写作评估中的表现，即它们根据多种评估标准提供评分和评论的能力。利用由二外研究生撰写并由人类专家基于9个分析标准评估的一系列文献综述语料库，我们在不同条件下促使几款流行的LLMs完成相同任务。为评估反馈评论的质量，我们应用了一种新颖的反馈评论质量评估框架。与依赖人工判断的现有方法相比，该框架具有可解释性、成本效益、可扩展性和可复现性。研究发现，LLMs能够生成合理良好且总体可靠的多维度分析评估。我们发布了该语料库以保证可复现性。', 'title_zh': '大规模语言模型可以执行多维度分析性写作评估：针对二语研究生学术英语写作的案例研究'}
{'arxiv_id': 'arXiv:2502.11300', 'title': 'CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?', 'authors': 'Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee', 'link': 'https://arxiv.org/abs/2502.11300', 'abstract': "Multimodal Large Language Models (MLLMs) are renowned for their superior instruction-following and reasoning capabilities across diverse problem domains. However, existing benchmarks primarily focus on assessing factual and logical correctness in downstream tasks, with limited emphasis on evaluating MLLMs' ability to interpret pragmatic cues and intermodal relationships. To address this gap, we assess the competency of MLLMs in performing Multimodal Discourse Analysis (MDA) using Coherence Relations. Our benchmark, CORDIAL, encompasses a broad spectrum of Coherence Relations across 3 different discourse domains at varying levels of granularity. Through our experiments on 10+ MLLMs employing different prompting strategies, we show that even top models like Gemini 1.5 Pro and GPT-4o fail to match the performance of simple classifier-based baselines. This study emphasizes the need to move beyond similarity-based metrics and adopt a discourse-driven framework for evaluating MLLMs, providing a more nuanced assessment of their capabilities. The benchmark and code are available at: this https URL.", 'abstract_zh': '多模态大语言模型（MLLMs）以其在多种问题领域中的卓越指令遵循能力和推理能力而闻名。然而，现有的基准主要关注下游任务中的事实和逻辑正确性，对评估MLLMs在解释语用线索和跨模态关系方面的能力关注不足。为解决这一问题，我们使用连贯性关系（Coherence Relations）对MLLMs进行多模态话语分析（MDA）的能力进行了评估。我们的基准测试Cordial涵盖了三个不同的话语领域中不同粒度层次的广泛连贯性关系。通过在10多种使用不同提示策略的MLLMs上进行实验，我们发现即使是像Gemini 1.5 Pro和GPT-4o这样的顶级模型也无法达到基于简单分类器的基线模型的性能。本研究强调了超越基于相似性的评估指标，采用基于话语驱动的框架来评估MLLMs的重要性，提供一种更细致的能力评估。基准测试和代码可在以下链接获取：[此链接]。', 'title_zh': 'CORDIAL：多模态大规模语言模型能否有效理解连贯性关系？'}
{'arxiv_id': 'arXiv:2502.11228', 'title': 'Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs', 'authors': 'Mohammad Reza Rezaei, Adji Bousso Dieng', 'link': 'https://arxiv.org/abs/2502.11228', 'abstract': "Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.", 'abstract_zh': '检索增强生成（RAG）通过利用外部知识源提升了大型语言模型（LLMs）在特定领域问题求解（QA）任务中的性能。然而，传统RAG系统主要侧重于相关性检索，往往在多源信息连接推理时难以避免冗余。本文介绍了一种名为Vendi-RAG的框架，该框架基于迭代过程，同时优化检索多样性和答案质量。这种联合优化能够显著提高多跳QA任务的准确性。Vendi-RAG利用Vendi评分（VS），这是一种基于相似度的灵活性多样度度量，以促进文档检索中的语义多样性。然后，通过评估推理步骤后生成的候选答案，使用LLM判别器输出一个评分，该评分在每次迭代中用于检索器平衡检索的文档的相关性和多样性。在三个具有挑战性的数据集——HotpotQA、MuSiQue和2WikiMultiHopQA——上的实验表明，Vendi-RAG在多跳推理任务中具有有效性。该框架在与传统单步骤和多步骤RAG方法相比时，实现了显著的准确性提升，在HotpotQA数据集上达到+4.2%、在2WikiMultiHopQA数据集上达到+4.1%、在MuSiQue数据集上达到+1.3%，相对于Adaptive-RAG，这是当前最佳基线。随着检索文档数量的增加，Vendi-RAG的好处更为显著。最后，我们还评估了Vendi-RAG在不同的LLM基础模型上的表现，包括GPT-3.5、GPT-4和GPT-4o-mini，并观察到一致的改进，表明该框架的优势具有模型的通用性。', 'title_zh': 'Vendi-RAG：适当地权衡多样性和质量显著改进了大规模语言模型增强生成 retrieval-augmented generation方法'}
{'arxiv_id': 'arXiv:2502.11211', 'title': 'A Survey of LLM-based Agents in Medicine: How far are we from Baymax?', 'authors': 'Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Wenting Chen, Xiang Li, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2502.11211', 'abstract': "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.", 'abstract_zh': '大规模语言模型（LLMs）正在通过开发能够理解和处理医学任务的基于LLM的代理来改变医疗保健领域。本综述提供了对医学中基于LLM代理的全面回顾，检查了它们的架构、应用及其面临的挑战。我们分析了医疗代理系统的关键组件，包括系统特性、临床规划机制、医学推理框架以及外部能力增强。综述涵盖了主要的应用场景，如临床决策支持、医疗记录、培训模拟以及医疗保健服务优化。我们讨论了用于评估这些代理在医疗保健环境中表现的评估框架和指标。尽管基于LLM的代理在提升医疗服务方面表现出潜力，但仍然存在一些挑战，包括幻觉管理、多模态集成、实施障碍以及伦理考虑。综述最后指出了未来的研究方向，包括受最近LLM架构发展启发的医学推理进展、与物理系统的集成以及培训模拟的改进。本研究为研究人员和实践者提供了基于LLM的代理在医学领域当前状态和未来前景的结构化概述。', 'title_zh': '基于大型语言模型的医疗代理综述：我们距Baymax还有多远？'}
{'arxiv_id': 'arXiv:2502.11196', 'title': 'How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training', 'authors': 'Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen', 'link': 'https://arxiv.org/abs/2502.11196', 'abstract': 'Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance. Code and data will be available at this https URL.', 'abstract_zh': '尽管大型语言模型（LLMs）在知识密集型任务中表现出色，但仍面临一个关键的挑战，即理解它们如何内化新知识，尤其是如何在神经计算中结构性地嵌入所获取的知识。我们通过知识电路进化这一视角来解决这一问题，识别出有助于知识存储和处理的计算子图。系统分析连续预训练过程中电路进化的演变，揭示了以下几个关键发现：（1）新知识的获取受其与先存知识的相关性影响；（2）知识电路的演变经历了从形成到优化的阶段转变；（3）知识电路的演变遵循从深到浅的模式。这些见解不仅推进了我们对LLMs中新知识获取机制的理论理解，还为改进连续预训练策略以增强模型性能提供了潜在的指导意义。相关代码和数据将在此处提供：[这个链接]。', 'title_zh': '大语言模型是如何获取新知识的？连续预训练的知识电路视角'}
{'arxiv_id': 'arXiv:2502.11149', 'title': 'Large Language-Geometry Model: When LLM meets Equivariance', 'authors': 'Zongzhao Li, Jiacheng Cen, Bing Su, Wenbing Huang, Tingyang Xu, Yu Rong, Deli Zhao', 'link': 'https://arxiv.org/abs/2502.11149', 'abstract': 'Accurately predicting 3D structures and dynamics of physical systems is crucial in scientific applications. Existing approaches that rely on geometric Graph Neural Networks (GNNs) effectively enforce $\\mathrm{E}(3)$-equivariance, but they often fall in leveraging extensive broader information. While direct application of Large Language Models (LLMs) can incorporate external knowledge, they lack the capability for spatial reasoning with guaranteed equivariance. In this paper, we propose EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates E(3)-equivariance with LLM capabilities. Specifically, EquiLLM comprises four key components: geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adaptor. Essentially, the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules. Experimental results demonstrate that EquiLLM delivers significant improvements over previous methods across molecular dynamics simulation, human motion simulation, and antibody design, highlighting its promising generalizability.', 'abstract_zh': '准确预测物理系统的三维结构和动态在科学研究中至关重要。现有的依赖几何图神经网络（GNNs）的方法能够有效强制 $\\mathrm{E}(3)$-酉变性，但它们往往未能充分利用广泛的信息。虽然直接应用大型语言模型（LLMs）可以整合外部知识，但它们缺乏保证酉变性的空间推理能力。在本文中，我们提出了一种新的框架EquiLLM，该框架能够无缝集成 $\\mathrm{E}(3)$-酉变性与LLM的能力。具体而言，EquiLLM 包含四个关键组件：几何感知提示、酉变编码器、LLM 和酉变适配器模块。通过指令性的提示引导的LLM 作为复杂的不变特征处理器，而3D方向信息则仅由酉变编码器和适配器模块处理。实验结果表明，EquiLLM 在分子动力学模拟、人体运动模拟和抗体设计等方面显著优于先前的方法，并突出显示了其潜在的良好泛化能力。', 'title_zh': '大型语言-几何模型：当大规模语言模型遇到等变性'}
{'arxiv_id': 'arXiv:2502.11147', 'title': 'Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity', 'authors': 'Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, Yizhou Shan', 'link': 'https://arxiv.org/abs/2502.11147', 'abstract': "Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur $O(N)$ time and memory consumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the ``impossible trinity'' of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with $O(L)$ time and $O(L)$ memory complexity.", 'abstract_zh': '大型语言模型（LLMs）在各个领域展现了强大的能力，尤其是在数学和编程等具有挑战性的推理任务中取得了显著进步。然而，解决推理任务通常需要长的解码链（思考链），这导致了$O(N)$的时间和内存消耗，其中$N$为链的长度。为了减少$O(N)$的时间和内存消耗，现有的稀疏性算法建议仅保留最关键的标记的中间数据（即键值缓存），而丢弃其余的数据。然而，现有的算法在准确率、时间和内存之间面临“不可能三角”。例如，最先进的算法Quest能够在$O(L)$时间内达到高准确率，但需要$O(N)$的内存（$L$是缓存预算，$L \\ll N$）。为了应对这一问题，在本文中，我们识别了一种新的推理任务解码阶段的注意力模式，其中标型标记（类似于数学证明中的引理）在使用后变得不再重要。基于这一模式，我们提出了一种名为RaaS的新算法，该算法仅在不需要时就识别并保留标型标记，从而能够以$O(L)$的时间复杂度和$O(L)$的内存复杂度实现高准确率。', 'title_zh': '高效长时解码推理与推理意识注意力稀疏性'}
{'arxiv_id': 'arXiv:2502.11090', 'title': 'SafeDialBench: A Fine-Grained Safety Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks', 'authors': 'Hongye Cao, Yanming Wang, Sijia Jing, Ziyue Peng, Zhixin Bai, Zhe Cao, Meng Fang, Fan Feng, Boyan Wang, Jiaheng Liu, Tianpei Yang, Jing Huo, Yang Gao, Fanyu Meng, Xi Yang, Chao Deng, Junlan Feng', 'link': 'https://arxiv.org/abs/2502.11090', 'abstract': "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability of identifying and handling unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 17 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展，LLMs 的安全性已成为一个关键的关切点，需要精确评估。当前的基准主要专注于单一回合对话或单一脱戒攻击方法来评估安全性。此外，这些基准未详细考虑LLMs识别和处理不安全信息的能力。为了解决这些问题，我们提出了一种细粒度基准 SafeDialBench，用于评估LLMs在多回合对话中面对各种脱戒攻击时的安全性。具体来说，我们设计了一种两级分层安全分类法，考虑了6个安全维度，并在22种对话场景下生成了超过4000个多回合对话，涵盖中英文。我们采用7种脱戒攻击策略，如参考攻击和目的反转，以提高对话生成数据集的质量。值得一提的是，我们构建了一种创新性的LLMs评估框架，衡量其在检测和处理不安全信息以及在面对脱戒攻击时保持一致性的能力。实验结果表明，Yi-34B-Chat 和 GLM4-9B-Chat 展示出更优秀安全性表现，而Llama3.1-8B-Instruct 和 o3-mini 则表现出安全性漏洞。', 'title_zh': 'SafeDialBench：一种针对多轮对话中各种脱勾攻击的大规模语言模型细粒度安全性基准测试'}
{'arxiv_id': 'arXiv:2502.11059', 'title': 'ClimateLLM: Efficient Weather Forecasting via Frequency-Aware Large Language Models', 'authors': 'Shixuan Li, Wei Yang, Peiyu Zhang, Xiongye Xiao, Defu Cao, Yuehan Qin, Xiaole Zhang, Yue Zhao, Paul Bogdan', 'link': 'https://arxiv.org/abs/2502.11059', 'abstract': 'Weather forecasting is crucial for public safety, disaster prevention and mitigation, agricultural production, and energy management, with global relevance. Although deep learning has significantly advanced weather prediction, current methods face critical limitations: (i) they often struggle to capture both dynamic temporal dependencies and short-term abrupt changes, making extreme weather modeling difficult; (ii) they incur high computational costs due to extensive training and resource requirements; (iii) they have limited adaptability to multi-scale frequencies, leading to challenges when separating global trends from local fluctuations. To address these issues, we propose ClimateLLM, a foundation model for weather forecasting. It captures spatiotemporal dependencies via a cross-temporal and cross-spatial collaborative modeling framework that integrates Fourier-based frequency decomposition with Large Language Models (LLMs) to strengthen spatial and temporal modeling. Our framework uses a Mixture-of-Experts (MoE) mechanism that adaptively processes different frequency components, enabling efficient handling of both global signals and localized extreme events. In addition, we introduce a cross-temporal and cross-spatial dynamic prompting mechanism, allowing LLMs to incorporate meteorological patterns across multiple scales effectively. Extensive experiments on real-world datasets show that ClimateLLM outperforms state-of-the-art approaches in accuracy and efficiency, as a scalable solution for global weather forecasting.', 'abstract_zh': '天气预报对于公共安全、灾害预防与减轻、农业生产及能源管理具有全球性的重大意义。尽管深度学习在天气预测方面取得了显著突破，但目前的方法仍面临关键限制：（i）它们往往难以捕捉动态的时间依赖性和短期突变，导致极端天气建模困难；（ii）由于广泛的训练和资源需求导致计算成本高；（iii）对多尺度频率的适应性有限，使得在区分全球趋势与局部波动时面临挑战。为了解决这些问题，我们提出了ClimateLLM，一种用于天气预报的基准模型。它通过一种跨时间和跨空间的协作建模框架来捕捉时空依赖性，该框架结合了基于傅里叶的频率分解与大型语言模型（LLMs），从而增强了空间和时间建模。我们的框架利用了一种混合专家机制（MoE），该机制能够自适应地处理不同频率的成分，从而有效地处理全局信号和局部极端事件。此外，我们还引入了一种跨时间和跨空间的动力提示机制，使得LLMs能够有效地纳入跨多尺度的气象模式。在真实世界数据集上的大量实验表明，ClimateLLM 在准确性和效率方面均优于当前最先进的方法，是全球天气预报的可扩展解决方案。', 'title_zh': 'ClimateLLM：基于频率意识的大语言模型高效天气预报'}
{'arxiv_id': 'arXiv:2502.11054', 'title': 'Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models', 'authors': 'Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.11054', 'abstract': "Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs' strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at this https URL to facilitate further research in this critical domain.", 'abstract_zh': '多轮 jailbreak 攻击通过模拟现实世界的人类互动，引导大语言模型（LLMs）进行迭代对话，暴露了关键的安全漏洞。然而，现有方法往往难以在语义连贯性和攻击有效性之间达成平衡，导致要么是无害的语义漂移，要么是无效的检测规避。为解决这一挑战，我们提出了一种名为 Reasoning-Augmented Conversation（增强推理对话）的新颖多轮 jailbreak 框架，该框架将有害查询重新表述为无害的推理任务，并利用大语言模型的强大推理能力来破坏安全对齐。具体来说，我们引入了一种攻击状态机框架，系统地建模问题转换和迭代推理，确保在多轮中生成语义连贯的问题。在此框架基础上，我们设计了收益导向探索、自我对弈和拒绝反馈模块，以保持攻击语义、增强攻击效果并维持推理驱动的攻击进程。在多种大语言模型上的广泛实验表明，RACE 在复杂的对话场景中实现了最先进的攻击效果，攻击成功率（ASRs）最高提高96%。值得注意的是，我们的方法对顶级商用模型 OpenAI o1 和 DeepSeek R1 的攻击成功率分别达到了82%和92%，突显了其威力。我们已在以下 URL 公开代码，以促进对该关键领域的进一步研究。', 'title_zh': '增强推理对话以应对大型语言模型多轮脱轨攻击'}
{'arxiv_id': 'arXiv:2502.11028', 'title': 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models', 'authors': 'Prateek Chhikara', 'link': 'https://arxiv.org/abs/2502.11028', 'abstract': 'Large Language Models (LLMs) demonstrate impressive performance across diverse tasks, yet confidence calibration remains a challenge. Miscalibration - where models are overconfident or underconfident - poses risks, particularly in high-stakes applications. This paper presents an empirical study on LLM calibration, examining how model size, distractors, and question types affect confidence alignment. We introduce an evaluation framework to measure overconfidence and investigate whether multiple-choice formats mitigate or worsen miscalibration. Our findings show that while larger models (e.g., GPT-4o) are better calibrated overall, they are more prone to distraction, whereas smaller models benefit more from answer choices but struggle with uncertainty estimation. Unlike prior work, which primarily reports miscalibration trends, we provide actionable insights into failure modes and conditions that worsen overconfidence. These findings highlight the need for calibration-aware interventions and improved uncertainty estimation methods.', 'abstract_zh': '大型语言模型（LLMs）在多样化的任务中展现出令人印象深刻的性能，但置信度校准仍然是一个挑战。错校准——即模型过度自信或欠自信——在高风险应用中尤其具有风险。本文进行了一项实证研究，探讨了模型大小、干扰项和问题类型如何影响置信度一致性。我们提出了一种评估框架来度量过度自信，并调查了多项选择格式是否能够减轻或加剧错校准。我们的研究发现，虽然较大的模型（例如GPT-4o）在总体上更校准，但它们更容易受到干扰的影响，而较小的模型虽然在从答案选项中受益，但在不确定性的估计上却面临更大的困难。与先前的研究主要报告错校准趋势不同，我们的研究提供了针对故障模式和加剧过度自信条件的实际建议。这些发现强调了需要采取校准意识更强的干预措施，并改进不确定性的估计方法。', 'title_zh': '注意信心差距：大型语言模型中的过度自信、校准问题及干扰项效应'}
{'arxiv_id': 'arXiv:2502.11018', 'title': 'GRIFFIN: Effective Token Alignment for Faster Speculative Decoding', 'authors': 'Shijing Hu, Jingyang Li, Xingyu Xie, Zhihui Lu, Kim-Chuan Toh, Pan Zhou', 'link': 'https://arxiv.org/abs/2502.11018', 'abstract': "Speculative decoding accelerates inference in large language models (LLMs) by generating multiple draft tokens simultaneously. However, existing methods often struggle with token misalignment between the training and decoding phases, limiting their performance. To address this, we propose GRIFFIN, a novel framework that incorporates a token-alignable training strategy and a token-alignable draft model to mitigate misalignment. The training strategy employs a loss masking mechanism to exclude highly misaligned tokens during training, preventing them from negatively impacting the draft model's optimization. The token-alignable draft model introduces input tokens to correct inconsistencies in generated features. Experiments on LLaMA-series and Vicuna models demonstrate that GRIFFIN achieves an average acceptance length improvement of over 7\\% and a speedup ratio exceeding 8%, outperforming current SoTAs as shown in Fig. 1 (a) and (b).", 'abstract_zh': '推测解码通过同时生成多个草稿令牌来加速大型语言模型（LLMs）的推理过程。然而，现有的方法往往难以在训练和解码阶段之间保持令牌对齐，从而限制了它们的性能。为了解决这个问题，我们提出了一种名为GRIFFIN的新框架，该框架结合了一种可对齐训练策略和一种可对齐的草稿模型，以减轻不匹配问题。该训练策略采用了损失屏蔽机制，在训练过程中排除高度不匹配的令牌，防止它们对草稿模型的优化产生负面影响。可对齐的草稿模型通过引入输入令牌来纠正生成特征中的不一致。实验结果表明，GRIFFIN在LLaMA系列和Vicuna模型上的平均接受长度提升了超过7%，并且加速比超过8%，如图1(a)和(b)所示，这优于当前的最高水平方法。', 'title_zh': 'GRIFFIN: 有效的 token 对齐以实现更快的推测解码'}
{'arxiv_id': 'arXiv:2502.11006', 'title': 'Prompt Inject Detection with Generative Explanation as an Investigative Tool', 'authors': 'Jonathan Pan, Swee Liang Wong, Yidi Yuan, Xin Wei Chia', 'link': 'https://arxiv.org/abs/2502.11006', 'abstract': 'Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be two-fold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.', 'abstract_zh': '大型语言模型（LLMs）容易受到基于恶意提示注入的攻击。这些注入可能导致模型脱逃或利用模型中的漏洞，产生不希望的响应。在调查提示注入时，面临的挑战是涉及的大量输入提示可能主要具有良性特征。此外，由于LLM对话中涉及的输入提示在语义和主观性上的复杂性，以及对话所进行的环境背景，这一调查挑战变得更加复杂。因此，AI安全调查面临的挑战将包括两个方面。首先，需要识别恶意提示注入，然后评估输入提示是否在上下文中是良性还是恶意。对于第一步，可以使用现有的AI安全解决方案，如防护栏来检测和保护LLM。防护栏已经通过多种方法开发。一种流行的方法是基于签名的方法，另一种流行的开发AI模型的方法包括使用基于NLP的语言模型。然而，在进行AI安全调查时，这些防护栏缺乏帮助调查人员分类或评估已识别的输入提示的能力。在这一应用研究探索中，我们将探讨利用LLM的文本生成能力来检测提示注入，并生成其检测的解释，以帮助AI安全调查人员评估和处理这些提示注入检测。这样的工具的应用价值在于简化对提示注入进行调查的任务。', 'title_zh': '使用生成解释作为调查工具的提示注入检测'}
{'arxiv_id': 'arXiv:2502.10961', 'title': 'Graders should cheat: privileged information enables expert-level automated evaluations', 'authors': 'Jin Peng Zhou, Sébastien M. R. Arnold, Nan Ding, Kilian Q. Weinberger, Nan Hua, Fei Sha', 'link': 'https://arxiv.org/abs/2502.10961', 'abstract': "Auto-evaluating language models (LMs), i.e., using a grader LM to evaluate the candidate LM, is an appealing way to accelerate the evaluation process and the cost associated with it. But this presents a paradox: how can we trust the grader LM, which is presumably weaker than the candidate LM, to assess problems that are beyond the frontier of the capabilities of either model or both? For instance, today's LMs struggle on graduate-level physics and Olympiad-level math, making them unreliable graders in these domains.\nWe show that providing privileged information -- such as ground-truth solutions or problem-specific guidelines -- improves automated evaluations on such frontier problems. This approach offers two key advantages. First, it expands the range of problems where LMs graders apply. Specifically, weaker models can now rate the predictions of stronger models. Second, privileged information can be used to devise easier variations of challenging problems which improves the separability of different LMs on tasks where their performance is generally low. With this approach, general-purpose LM graders match the state of the art performance on RewardBench, surpassing almost all the specially-tuned models. LM graders also outperform individual human raters on Vibe-Eval, and approach human expert graders on Olympiad-level math problems.", 'abstract_zh': '自动评估语言模型（LMs），即使用一个评判LM来评估候选LM，是一种加速评估过程及其相关成本的方法。但是这引出了一个悖论：我们如何能信任一个理论上比候选LM更弱的评判LM去评估超出这两个模型当前能力的问题？例如，当今的LM在研究生物理水平和奥林匹克数学水平上表现不佳，这使得它们在这些领域作为评判者是不可靠的。\n\n我们证明，提供特权信息（例如，真实答案或特定问题的指导方针）可以改善在前沿问题上的自动评估。这种方法有两大优势。首先，它扩展了LM评判器适用的问题范围。具体来说，较弱的模型现在可以评估较强模型的预测。其次，特权信息可以用来设计更具挑战性的问题的较简单的变体，从而提高了在这些任务上表现不佳的各个LM之间的可分辨性。借助这种方法，通用的LM评判器在RewardBench上达到了最先进的性能，超过了几乎所有的专门调整的模型。LM评判器在Vibe-Eval上也超过了单独的人类评判者，在奥林匹克数学问题上接近人类专家评判者。', 'title_zh': '评分老师应当作弊：特权信息使自动化高级评估成为可能\n\n解释：这句话讨论的是在评分过程中，如果评分老师能够获得一些特权信息（例如，学生的个人信息或作业的背景信息），那么他们可以利用这些信息进行更为精准和专业的评估。同样地，在自动化评估系统中，如果系统能够访问到类似的特权信息，那么它也有能力进行高级别的自动评估。这里“作弊”是指利用额外信息来获得更准确的结果，而不是指不正当的行为。'}
{'arxiv_id': 'arXiv:2502.10953', 'title': 'Empirical evaluation of LLMs in predicting fixes of Configuration bugs in Smart Home System', 'authors': 'Sheikh Moonwara Anjum Monisha, Atul Bharadwaj', 'link': 'https://arxiv.org/abs/2502.10953', 'abstract': 'This empirical study evaluates the effectiveness of Large Language Models (LLMs) in predicting fixes for configuration bugs in smart home systems. The research analyzes three prominent LLMs - GPT-4, GPT-4o (GPT-4 Turbo), and Claude 3.5 Sonnet - using four distinct prompt designs to assess their ability to identify appropriate fix strategies and generate correct solutions. The study utilized a dataset of 129 debugging issues from the Home Assistant Community, focusing on 21 randomly selected cases for in-depth analysis. Results demonstrate that GPT-4 and Claude 3.5 Sonnet achieved 80\\% accuracy in strategy prediction when provided with both bug descriptions and original scripts. GPT-4 exhibited consistent performance across different prompt types, while GPT-4o showed advantages in speed and cost-effectiveness despite slightly lower accuracy. The findings reveal that prompt design significantly impacts model performance, with comprehensive prompts containing both description and original script yielding the best results. This research provides valuable insights for improving automated bug fixing in smart home system configurations and demonstrates the potential of LLMs in addressing configuration-related challenges.', 'abstract_zh': '本实证研究评估了大型语言模型（LLMs）在预测智能家居系统配置错误修复方案方面的有效性。研究分析了三种知名的LLMs——GPT-4、GPT-4o（GPT-4 Turbo）和Claude 3.5 Sonnet，并使用四种不同的提示设计来评估它们识别合适修复策略和生成正确解决方案的能力。研究利用了来自Home Assistant Community的数据集，其中包含129个调试问题，并对21个随机选取的案例进行了深入分析。结果显示，当提供错误描述和原始脚本时，GPT-4和Claude 3.5 Sonnet在策略预测方面的准确率达到了80%。GPT-4在不同提示类型中表现出一致的性能，而GPT-4o在速度和成本效益方面表现出优势，尽管准确率略低一些。研究发现，提示设计对模型性能有显著影响，包含描述和原始脚本的全面提示能取得最佳结果。本研究为提高智能家居系统配置的自动错误修复提供了有价值的见解，并展示了LLMs在解决配置相关挑战方面的潜力。', 'title_zh': '智能家庭系统中配置错误修复预测的大型语言模型实证评估'}
{'arxiv_id': 'arXiv:2502.10940', 'title': 'CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation', 'authors': 'Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, Zheng Zhang', 'link': 'https://arxiv.org/abs/2502.10940', 'abstract': 'Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce CoLA and its memory-efficient implementation, CoLA-M. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by $\\bf 2\\pmb{\\times}$ and improves training throughput by $\\bf 1.86\\pmb{\\times}$ while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also $\\bf 2\\pmb{\\times}$ smaller, enabling faster inference with lower memory cost on resource-constrained platforms', 'abstract_zh': '大规模语言模型（LLMs）正在革新许多科学和技术领域。然而，它们巨大的模型规模在预训练阶段对计算资源提出了极高的需求。尽管低秩分解可以减少模型参数，但在LLM预训练中的直接应用往往会导致不可忽视的性能损失。为了解决这一基本挑战，我们引入了CoLA及其高效的实现CoLA-M。我们利用模型激活中广泛观察到的低秩结构，在因子化的权重矩阵之间引入非线性变换，以减少模型大小、提升模型容量和训练效率。在参数量从6000万到7亿的LLaMA模型上进行的实验表明，使用CoLA可以将计算成本减少$\\bf 2\\pmb{\\times}$，同时将训练吞吐量提高$\\bf 1.86\\pmb{\\times}$，同时仍然保持全秩级别的性能。CoLA-M进一步在不牺牲吞吐量的情况下压缩了内存成本，提供了在参数、计算和内存效率方面综合表现更优的预训练方法。生成的LLMs体积也减少了$\\bf 2\\pmb{\\times}$，这使得在资源受限的平台上实现更快的推理并降低内存成本成为可能。', 'title_zh': 'CoLA：通过低秩激活实现LLM的计算高效预训练'}
{'arxiv_id': 'arXiv:2502.10871', 'title': 'The Representation and Recall of Interwoven Structured Knowledge in LLMs: A Geometric and Layered Analysis', 'authors': 'Ge Lei, Samuel J. Cooper', 'link': 'https://arxiv.org/abs/2502.10871', 'abstract': 'This study investigates how large language models (LLMs) represent and recall multi-associated attributes across transformer layers. We show that intermediate layers encode factual knowledge by superimposing related attributes in overlapping spaces, along with effective recall even when attributes are not explicitly prompted. In contrast, later layers refine linguistic patterns and progressively separate attribute representations, optimizing task-specific outputs while appropriately narrowing attribute recall. We identify diverse encoding patterns including, for the first time, the observation of 3D spiral structures when exploring information related to the periodic table of elements. Our findings reveal a dynamic transition in attribute representations across layers, contributing to mechanistic interpretability and providing insights for understanding how LLMs handle complex, interrelated knowledge.', 'abstract_zh': '本研究探讨了大型语言模型（LLMs）如何在变换器层中表示和回忆多关联属性。我们发现中间层通过在重叠的空间中叠加相关属性来编码事实知识，并且即使未明确提示属性，也能有效地回忆属性。相比之下，后期层则逐渐细化语言模式并分离属性表示，优化特定任务的输出，同时适当地限制属性回忆。我们识别了多种编码模式，其中包括首次观察到与元素周期表相关的信息时出现的三维螺旋结构。我们的发现揭示了层间属性表示的动态转变，有助于机械可解释性的提升，并为理解LLMs处理复杂、相互关联的知识提供了见解。', 'title_zh': '大型语言模型中交织结构知识的表示与回忆：一种几何与分层分析'}
{'arxiv_id': 'arXiv:2502.10732', 'title': 'Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents', 'authors': 'Mauricio Tec, Guojun Xiong, Haichuan Wang, Francesca Dominici, Milind Tambe', 'link': 'https://arxiv.org/abs/2502.10732', 'abstract': "Deep Reinforcement Learning (RL) is remarkably effective in addressing sequential resource allocation problems in domains such as healthcare, public policy, and resource management. However, deep RL policies often lack transparency and adaptability, challenging their deployment alongside human decision-makers. In contrast, Language Agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement Learning (RBRL), a novel framework that jointly optimizes decision and explanations. At each step, RBRL generates candidate rules with an LLM, selects among them using an attention-based RL policy, and determines the environment action with an explanation via chain-of-thought reasoning. The RL rule selection is optimized using the environment rewards and an explainability metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's competitive performance with deep RL and efficiency gains over LLM fine-tuning. A survey further confirms the enhanced quality of its explanations.", 'abstract_zh': '深度强化学习（RL）在医疗保健、公共政策和资源管理等领域解决顺序资源分配问题方面表现出显著的效果。然而，深度RL策略往往缺乏透明性和适应性，这使其难以与人类决策者并行部署。相比之下，由大规模语言模型（LLMs）驱动的语言代理提供了易于人类理解的推理方式，但在有效决策方面可能存在挑战。为解决这一问题，我们提出了一种新颖框架——规则瓶颈强化学习（RBRL），旨在同时优化决策和解释。在每一步骤中，RBRL 使用LLM生成候选规则，使用基于注意力的RL策略从中选择，并通过链式推理进行解释，以确定环境动作。通过环境奖励和由LLM评估的解释性指标优化RL规则的选择。实地情景下的评估展示了RBRL在与深度RL相比时具有竞争力的表现，并在LLM微调方面提高了效率。进一步的调查还证实了其解释质量的提升。', 'title_zh': '规则瓶颈强化学习：语言代理参与的资源配置的联合解释与决策优化'}
{'arxiv_id': 'arXiv:2502.10709', 'title': 'An Empirical Analysis of Uncertainty in Large Language Model Evaluations', 'authors': 'Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang', 'link': 'https://arxiv.org/abs/2502.10709', 'abstract': "As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. The code and data are released at: this https URL.", 'abstract_zh': '随着LLM-as-a-Judge作为评估大型语言模型（LLMs）的新范式出现，人们对LLM评估者的一致性、偏差和稳定性提出了担忧。虽然大量研究集中在一致性与偏差方面，但关于LLM评估者稳定性的研究相对较少。本文通过在两种不同的评估设置中使用9种广泛使用的LLM评估工具，进行了大量的实验，以探究基于模型的LLM评估中的不确定性。研究发现，基于不同模型家族和规模的LLM评估工具表现出不同的不确定性。通过仔细比较分析，我们发现，无论是通过特殊的提示策略进行推理，还是在训练后应用特殊提示策略，都可以在一定程度上缓解评估不确定性。通过利用不确定性来增强LLM在异常分布（OOD）数据中的可靠性和检测能力，我们进一步微调了一个不确定性感知的LLM评估工具ConfiLM，并在人工设计的测试集上评估了ConfiLM的OOD评估能力，该测试集来源于2024年奥运会数据。实验结果表明，在微调阶段引入不确定性作为额外信息可以显著提高模型在OOD场景下的评估性能。本文的代码和数据已发布在：this https URL。', 'title_zh': '大型语言模型评估中不确定性的实证分析'}
{'arxiv_id': 'arXiv:2502.10626', 'title': 'K-Edit: Language Model Editing with Contextual Knowledge Awareness', 'authors': 'Elan Markowitz, Anil Ramakrishna, Ninareh Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, Aram Galstyan', 'link': 'https://arxiv.org/abs/2502.10626', 'abstract': 'As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approaches fail to produce edits that account for associated contextual information. We present K-Edit, an effective approach to generating contextually consistent knowledge edits. By using knowledge graphs, which maintain contextual consistency when an edge is edited, we are able to generate additional \\textit{contextual edits} that ensure consistency of related information in the language model. Our experiments demonstrate significant improvements in multi-hop question answering while maintaining the general effectiveness and scalability of model edits.', 'abstract_zh': '随着世界的变化，我们需要能够在不进行昂贵的重新训练的情况下更新我们的模型并纠正虚假信息。基于知识的模型编辑能使我们精确修改大规模语言模型的权重，从而修改其中编码的信息。近期的方法在一次性实现数千次编辑的召回方面取得了成功。然而，这些方法未能生成考虑到相关上下文信息的编辑。我们提出了K-Edit，这是一种生成上下文一致的知识编辑的有效方法。通过使用保持上下文一致性的知识图谱，我们能够生成额外的\\textit{上下文编辑}，以确保语言模型中相关信息的一致性。我们的实验结果显示，在多跳问答方面有显著改进，同时保持模型编辑的一般有效性和可扩展性。', 'title_zh': 'K-编辑：具有上下文知识意识的语言模型编辑'}
{'arxiv_id': 'arXiv:2502.10596', 'title': 'Post-training an LLM for RAG? Train on Self-Generated Demonstrations', 'authors': 'Matthew Finlayson, Ilia Kulikov, Daneil M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu', 'link': 'https://arxiv.org/abs/2502.10596', 'abstract': 'Large language models (LLMs) often struggle with knowledge intensive NLP tasks, such as answering "Who won the latest World Cup?" because the knowledge they learn during training may be insufficient or outdated. Conditioning generation on retrieved documents -- a technique known as retrieval augmented generation (RAG) -- mitigates these shortcomings by allowing the model to leverage in-context information. Practitioners can improve LLM RAG performance by fine-tuning on retrieval-augmented instructions, but must beware that this can cause undesirable model behaviors like hallucinations. We attribute this degradation to the fact that the training data is likely to be out-of-distribution for the model and may suffer from quality issues, such as misalignment between retrievals and target responses (since retrievals are frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs using self-generated demonstrations, thereby avoiding training on out-of-distribution text and integrating retrievals into the LLM responses. We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong. Compared to conventional RA-IT methods, our method prevents model degradation in non-RAG settings while exhibiting superior QA performance.', 'abstract_zh': '大型语言模型（LLMs）在知识密集型自然语言处理（NLP）任务中往往表现不佳，例如回答“最新世界杯的获胜者是谁？”这类问题，因为它们在训练过程中学到的知识可能不足或过时。通过检索文档来条件化生成——这一技术被称为检索增强生成（RAG）——可以通过使模型利用上下文信息来缓解这些问题。实践者可以通过在增强检索的基础上微调LLM来提高RAG性能，但必须注意这可能导致不可 desirable 的模型行为，如幻觉。我们归因于这一退化的原因是训练数据很可能对于模型来说是域外的，并且可能存在质量方面的问题，例如检索与目标响应之间存在偏差（因为检索通常是后来添加的）。我们提出了一种使用自我生成的示范训练RAG增强的LLMs的方法，从而避免在域外文本上进行训练，并将检索整合到LLM的响应中。我们在知识密集型问答（QA）任务上评估了该方法，并展示了该方法教会LLMs正确处理上下文信息检索和避免对它可能答错的问题进行提问。与传统的RA-IT方法相比，该方法在非RAG设置中防止了模型性能的退化，并展现了更好的问答性能。', 'title_zh': '对LLM进行后训练以支持RAG？使用自生成示例进行训练'}
{'arxiv_id': 'arXiv:2502.10536', 'title': 'PolyPath: Adapting a Large Multimodal Model for Multi-slide Pathology Report Generation', 'authors': 'Faruk Ahmed, Lin Yang, Tiam Jaroensri, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Greg S. Corrado, Dale R. Webster, Shravya Shetty, Shruthi Prabhakara, Yun Liu, Daniel Golden, Ellery Wulczyn, David F. Steiner', 'link': 'https://arxiv.org/abs/2502.10536', 'abstract': 'The interpretation of histopathology cases underlies many important diagnostic and treatment decisions in medicine. Notably, this process typically requires pathologists to integrate and summarize findings across multiple slides per case. Existing vision-language capabilities in computational pathology have so far been largely limited to small regions of interest, larger regions at low magnification, or single whole-slide images (WSIs). This limits interpretation of findings that span multiple high-magnification regions across multiple WSIs. By making use of Gemini 1.5 Flash, a large multimodal model (LMM) with a 1-million token context window, we demonstrate the ability to generate bottom-line diagnoses from up to 40,000 768x768 pixel image patches from multiple WSIs at 10X magnification. This is the equivalent of up to 11 hours of video at 1 fps. Expert pathologist evaluations demonstrate that the generated report text is clinically accurate and equivalent to or preferred over the original reporting for 68% (95% CI: [60%, 76%]) of multi-slide examples with up to 5 slides. While performance decreased for examples with 6 or more slides, this study demonstrates the promise of leveraging the long-context capabilities of modern LMMs for the uniquely challenging task of medical report generation where each case can contain thousands of image patches.', 'abstract_zh': '病理组织学案例的解释对于医学中的许多重要诊断和治疗决策至关重要。这一过程通常要求病理学家整合和总结多个切片中的各项发现。目前，在计算病理学中的视觉-语言能力主要局限于感兴趣的小区域、低放大倍数的大区域或单张全切片图像（WSI）。这限制了对跨越多个WSI的多个高倍率区域的发现进行解释的能力。通过利用Gemini 1.5 Flash，一种具有100万词上下文窗口的大规模多模态模型（LMM），我们展示了从多张10倍放大倍率的WSI中多达40,000个768x768像素的图像块生成最终诊断结果的能力。这相当于长达11小时的1 fps视频。专家病理学家的评估表明，生成的报告文本在临床准确性上与原报告相当或更优，对于包含至多5张切片的68%（95%置信区间：[60%, 76%]）的多切片示例，生成的报告文本更受偏好。尽管对于包含6张或更多切片的示例，性能有所下降，但本研究展示了利用现代LMM的长上下文能力来完成医学报告生成这一独特挑战任务的潜力，而每例病例可能包含数千个图像块。', 'title_zh': 'PolyPath: 调整大型多模态模型以生成多张病理切片报告'}
{'arxiv_id': 'arXiv:2502.10517', 'title': 'KernelBench: Can LLMs Write Efficient GPU Kernels?', 'authors': 'Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Ré, Azalia Mirhoseini', 'link': 'https://arxiv.org/abs/2502.10517', 'abstract': "Efficient GPU kernels are crucial for building performant machine learning architectures, but writing them is a time-consuming challenge that requires significant expertise; therefore, we explore using language models (LMs) to automate kernel generation. We introduce KernelBench, an open-source framework for evaluating LMs' ability to write fast and correct kernels on a suite of 250 carefully selected PyTorch ML workloads. KernelBench represents a real-world engineering environment and making progress on the introduced benchmark directly translates to faster practical kernels. We introduce a new evaluation metric fast_p, which measures the percentage of generated kernels that are functionally correct and offer a speedup greater than an adjustable threshold p over baseline. Our experiments across various state-of-the-art models and test-time methods show that frontier reasoning models perform the best out of the box but still fall short overall, matching the PyTorch baseline in less than 20% of the cases. While we show that results can improve by leveraging execution and profiling feedback during iterative refinement, KernelBench remains a challenging benchmark, with its difficulty increasing as we raise speedup threshold p.", 'abstract_zh': '高效的GPU内核对于构建高性能的机器学习架构至关重要，但编写这些内核是一个耗时且需要高度专业技能的挑战；因此，我们探讨了使用语言模型（LMs）来自动生成内核的可能性。我们介绍了KernelBench，这是一个开放源代码框架，用于评估LMs在一系列250个精心挑选的PyTorch机器学习工作负载上编写快速且正确的内核的能力。KernelBench代表了一个实际的工程环境，而在此引入的基准测试上取得的进步可以直接转化为更快的实际内核。我们引入了一个新的评估指标fast_p，该指标衡量生成内核中功能上正确且相比基线提供加速比阈值p更大的百分比。我们在各种最先进的模型和测试时方法下进行的实验表明，前沿推理模型开箱即用时表现最佳，但仍总体上表现不佳，在不到20%的情况下可与PyTorch基线匹敌。虽然我们展示了通过在迭代优化过程中利用执行和分析反馈来提高结果的可能性，但KernelBench仍然是一个具有挑战性的基准，随着加速比阈值p的提高，其难度也随之增加。', 'title_zh': 'KernelBench: 大型语言模型能够编写高效的GPU内核吗？'}
{'arxiv_id': 'arXiv:2502.10487', 'title': 'Fast Proxies for LLM Robustness Evaluation', 'authors': 'Tim Beyer, Jan Schuchardt, Leo Schwinn, Stephan Günnemann', 'link': 'https://arxiv.org/abs/2502.10487', 'abstract': "Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expensive. We compare the ability of fast proxy metrics to predict the real-world robustness of an LLM against a simulated attacker ensemble. This allows us to estimate a model's robustness to computationally expensive attacks without requiring runs of the attacks themselves. Specifically, we consider gradient-descent-based embedding-space attacks, prefilling attacks, and direct prompting. Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank) correlations with the full attack ensemble while reducing computational cost by three orders of magnitude.", 'abstract_zh': '评估大型语言模型（LLMs）在对抗性攻击下的鲁棒性对于安全部署至关重要，但现有的红队方法往往成本过高。我们比较了快速代理指标预测LLM在模拟攻击者集合下的实际鲁棒性的能力。这使得我们能够在不运行攻击本身的情况下估计模型对计算成本高昂的攻击的鲁棒性。具体来说，我们考虑了基于梯度下降的嵌入空间攻击、预填充攻击和直接提示。尽管直接提示尤其未能实现高误报率（ASR），但我们发现它和嵌入空间攻击能够很好地预测攻击的成功率， Achieving $r_p=0.87$（线性相关系数）和 $r_s=0.94$（斯皮尔曼秩相关系数）的关联性，同时将计算成本降低了三个数量级。', 'title_zh': '快速代理用于大规模语言模型鲁棒性评估'}
{'arxiv_id': 'arXiv:2502.10459', 'title': 'LLM4GNAS: A Large Language Model Based Toolkit for Graph Neural Architecture Search', 'authors': 'Yang Gao, Hong Yang, Yizhi Chen, Junxian Wu, Peng Zhang, Haishuai Wang', 'link': 'https://arxiv.org/abs/2502.10459', 'abstract': 'Graph Neural Architecture Search (GNAS) facilitates the automatic design of Graph Neural Networks (GNNs) tailored to specific downstream graph learning tasks. However, existing GNAS approaches often require manual adaptation to new graph search spaces, necessitating substantial code optimization and domain-specific knowledge. To address this challenge, we present LLM4GNAS, a toolkit for GNAS that leverages the generative capabilities of Large Language Models (LLMs). LLM4GNAS includes an algorithm library for graph neural architecture search algorithms based on LLMs, enabling the adaptation of GNAS methods to new search spaces through the modification of LLM prompts. This approach reduces the need for manual intervention in algorithm adaptation and code modification. The LLM4GNAS toolkit is extensible and robust, incorporating LLM-enhanced graph feature engineering, LLM-enhanced graph neural architecture search, and LLM-enhanced hyperparameter optimization. Experimental results indicate that LLM4GNAS outperforms existing GNAS methods on tasks involving both homogeneous and heterogeneous graphs.', 'abstract_zh': 'Graph神经架构搜索（GNAS）有助于自动设计特定下游图学习任务的图神经网络（GNNs）。然而，现有的GNAS方法通常需要手动适应新的图搜索空间，这需要大量的代码优化和领域特定知识。为了解决这一挑战，我们提出了LLM4GNAS，这是一个利用大型语言模型（LLMs）生成能力的GNAS工具包。LLM4GNAS包含基于LLMs的图神经架构搜索算法库，通过修改LLMs提示来使GNAS方法适应新的搜索空间。这种方法减少了算法适应和代码修改的手动干预需求。LLM4GNAS工具包是可扩展且稳定的，它结合了LLM增强的图特征工程、LLM增强的图神经架构搜索以及LLM增强的超参数优化。实验结果表明，LLM4GNAS在涉及同构和异构图的任务中优于现有的GNAS方法。', 'title_zh': 'LLM4GNAS：一种基于大型语言模型的图神经架构搜索工具包'}
{'arxiv_id': 'arXiv:2502.10454', 'title': 'One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs', 'authors': 'Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.10454', 'abstract': 'Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs\' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs\' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.', 'abstract_zh': '利用数学大规模语言模型（LLMs）进行证明生成是LLMs研究中的一个基本课题。我们认为，当前LLMs能够证明命题的能力很大程度上取决于它们在训练过程中是否接触过相关的证明过程。这种依赖限制了它们对数学定理和相关概念的深入理解。受人类数学教育中常用的“反例证明法”教学方法的启发，我们的工作旨在通过反例来增强LLMs的数学推理和证明能力。具体来说，我们手动创建了一个高质量的大学水平数学基准TestMATH，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的理解程度。此外，我们还开发了一种数据工程框架，以自动获取进一步模型改进所需的训练数据。广泛的实验和详细的分析表明，TestMATH具有挑战性，这表明像OpenAI的模型等LLMs缺乏反例驱动的证明能力。此外，我们对模型训练的探索表明，增强LLMs的反例驱动概念推理能力对于提高其整体数学能力至关重要。我们认为，我们的工作为数学LLMs社区提供了新的视角。', 'title_zh': '一个实例展现，多个概念掌握！基于反例的概念驱动推理在数学大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.10411', 'title': 'TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models', 'authors': 'Sahan Bulathwela, Daniel Van Niekerk, Jarrod Shipton, Maria Perez-Ortiz, Benjamin Rosman, John Shawe-Taylor', 'link': 'https://arxiv.org/abs/2502.10411', 'abstract': 'Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \\emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.', 'abstract_zh': '个性化教育是能够从最近的人工智能（AI）和大型语言模型（LLM）的最新进展中受益匪浅的领域之一。然而，这也是一项最具挑战性的应用之一，原因在于在个性化学习体验以适应独立学习者的同时，有效进行认知教学的复杂性。我们假设，在这样的苛刻应用场景中表现出色的一个有前途的方法是使用“多元心智体系”。在本章中，我们将介绍TrueReason，这是一种范例性的个性化学习系统，该系统整合了多种专门的AI模型，这些模型能够模拟由LLM组合而成的微技能，从而实现规划和推理功能的运作。同时，我们将介绍原型的架构，并描述已集成到原型中的两个微技能。所提出的系统展示了构建能够承担教育等复杂领域所需求的复杂认知任务的高级AI系统的第一步。', 'title_zh': 'TrueReason：一种结合推理与基础模型的范例个性化学习系统'}
{'arxiv_id': 'arXiv:2502.10406', 'title': 'FishBargain: An LLM-Empowered Bargaining Agent for Online Fleamarket Platform Sellers', 'authors': 'Dexin Kong, Xu Yan, Ming Chen, Shuguang Han, Jufeng Chen, Fei Huang', 'link': 'https://arxiv.org/abs/2502.10406', 'abstract': "Different from traditional Business-to-Consumer e-commerce platforms~(e.g., Amazon), online fleamarket platforms~(e.g., Craigslist) mainly focus on individual sellers who are lack of time investment and business proficiency. Individual sellers often struggle with the bargaining process and thus the deal is unaccomplished. Recent advancements in Large Language Models(LLMs) demonstrate huge potential in various dialogue tasks, but those tasks are mainly in the form of passively following user's instruction. Bargaining, as a form of proactive dialogue task, represents a distinct art of dialogue considering the dynamism of environment and uncertainty of adversary strategies. In this paper, we propose an LLM-empowered bargaining agent designed for online fleamarket platform sellers, named as FishBargain. Specifically, FishBargain understands the chat context and product information, chooses both action and language skill considering possible adversary actions and generates utterances. FishBargain has been tested by thousands of individual sellers on one of the largest online fleamarket platforms~(Xianyu) in China. Both qualitative and quantitative experiments demonstrate that FishBargain can effectively help sellers make more deals.", 'abstract_zh': '与传统的商家对消费者的电子商务平台（例如Amazon）不同，二手交易平台（例如Craigslist）主要侧重于缺乏时间和商业技能投入的个人卖家。个人卖家往往在谈判过程中遇到困难，导致交易未能完成。最近在大型语言模型（LLMs）方面的进展展示了其在各种对话任务中的巨大潜力，但这些任务大多是以被动遵循用户指令的形式出现。而谈判作为一种主动对话任务，由于环境的动态性和对手策略的不确定性，具有独特的对话艺术。在本文中，我们提出了一种为二手交易平台卖家设计的由大型语言模型支持的谈判代理，命名为FishBargain。具体而言，FishBargain理解聊天上下文和产品信息，根据可能的对手行动选择行动和语言技能，并生成相应的表达。FishBargain已经在最大的中国在线二手交易平台之一（闲鱼）上经过数千个个人卖家的测试。定性和定量实验均表明，FishBargain能够有效帮助卖家达成更多交易。', 'title_zh': 'FishBargain：一种基于大语言模型的在线地摊交易平台谈判代理'}
{'arxiv_id': 'arXiv:2312.02073', 'title': 'A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia', 'authors': 'Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary, Jason Eisner, Emre Kıcıman, Hamid Palangi, Barun Patra, Robert West', 'link': 'https://arxiv.org/abs/2312.02073', 'abstract': "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.", 'abstract_zh': '大型语言模型（LLMs）具备从其上下文环境中汲取新颖信息的能力，这是一个令人印象深刻的特性。然而，这些模型在上下文中获得信息的具体机制仍然未知，尤其是在上下文信息与模型存储的背景事实知识相矛盾的情况下更是如此。在具有一种增强检索方法的背景下，倾向于使用的上下文信息对于获取最新的信息至关重要，期望这种上下文信息可以纠正过时或噪声较大的存储知识。我们提出了一种新的方法来研究这些语言模型的语境相关能力，使用了一种名为Fakepedia的新型数据集，这是一种旨在与模型内部参数化知识产生矛盾的虚假文本数据集。本研究中，我们介绍了Fakepedia，这是一种特别设计的对抗性数据集，在其中内部参数化知识与上下文信息产生冲突时，用于评估模型的位置相关能力。我们使用Fakepedia对多种LLMs进行基准测试，并基于我们的掩码分组因果追踪（MGCT）方法，对LLM组件在回答Fakepedia查询时的作用进行因果中介分析。通过这种方法的分析，我们识别出了有位置依托和无位置依托响应之间不同的计算模式。最终，我们证明仅通过计算分析就可以区分有位置依托和无位置依托的响应。我们的结果与现有关于事实回忆机制的研究发现一起，提供了一个关于LLMs中的位置相关机制和事实回忆机制之间互动的连贯叙述。', 'title_zh': '《矩阵中的故障？寻找和检测语言模型_grounding_的假维基》\n\n注释：\n1. "Grounding" 在机器学习和人工智能领域通常指的是将抽象的概念或语言描述与具体的现实世界对象或实体关联起来的过程，此处保持了英文原词。\n2. "假维基" 是对 "Fakepedia" 这个专有名词的直译，用以指代模拟或伪造的信息库。\n3. 翻译中保持了原文的学术风格和结构，确保符合学术规范。'}
{'arxiv_id': 'arXiv:2502.10833', 'title': 'Order-agnostic Identifier for Large Language Model-based Generative Recommendation', 'authors': 'Xinyu Lin, Haihan Shi, Wenjie Wang, Fuli Feng, Qifan Wang, See-Kiong Ng, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2502.10833', 'abstract': "Leveraging Large Language Models (LLMs) for generative recommendation has attracted significant research interest, where item tokenization is a critical step. It involves assigning item identifiers for LLMs to encode user history and generate the next item. Existing approaches leverage either token-sequence identifiers, representing items as discrete token sequences, or single-token identifiers, using ID or semantic embeddings. Token-sequence identifiers face issues such as the local optima problem in beam search and low generation efficiency due to step-by-step generation. In contrast, single-token identifiers fail to capture rich semantics or encode Collaborative Filtering (CF) information, resulting in suboptimal performance.\nTo address these issues, we propose two fundamental principles for item identifier design: 1) integrating both CF and semantic information to fully capture multi-dimensional item information, and 2) designing order-agnostic identifiers without token dependency, mitigating the local optima issue and achieving simultaneous generation for generation efficiency. Accordingly, we introduce a novel set identifier paradigm for LLM-based generative recommendation, representing each item as a set of order-agnostic tokens. To implement this paradigm, we propose SETRec, which leverages CF and semantic tokenizers to obtain order-agnostic multi-dimensional tokens. To eliminate token dependency, SETRec uses a sparse attention mask for user history encoding and a query-guided generation mechanism for simultaneous token generation. We instantiate SETRec on T5 and Qwen (from 1.5B to 7B). Extensive experiments demonstrate its effectiveness under various scenarios (e.g., full ranking, warm- and cold-start ranking, and various item popularity groups). Moreover, results validate SETRec's superior efficiency and show promising scalability on cold-start items as model sizes increase.", 'abstract_zh': '利用大型语言模型（LLMs）进行生成性推荐已引起广泛的研究兴趣，其中项目标记化是关键步骤。它涉及为LLMs分配项目标识符，以便编码用户历史并生成下一个项目。现有方法要么利用标记序列标识符，将项目表示为离散的标记序列，要么利用单个标记标识符，使用ID或语义嵌入。标记序列标识符面临诸如在 beam 搜索中陷入局部最优问题以及由于逐步生成而导致的生成效率低下的问题。相比之下，单个标记标识符无法捕获丰富的语义或编码协同过滤（CF）信息，从而导致性能不佳。\n\n为了应对这些问题，我们提出了项目标识符设计的两个基本原则：1）结合CF和语义信息，充分捕获多维项目信息；2）设计无需依赖标记的顺序无关标识符，减轻局部最优问题，同时提高生成效率。因此，我们引入了一种新颖的集合标识符范式，用于基于LLM的生成性推荐，将每个项目表示为一组顺序无关标记。为实现这一范式，我们提出了SETRec，它利用CF和语义标记器获取顺序无关的多维标记。为了消除标记依赖性，SETRec 使用稀疏注意掩码对用户历史进行编码，并采用查询引导生成机制同时生成标记。我们在T5和Qwen（从1.5B到7B）上实现了SETRec。广泛的实验在各种场景下（例如完整排名、温启动和冷启动排名以及各种项目流行度组）证明了其有效性。此外，结果验证了SETRec在提高效率方面的优越性，并展示了随着模型规模增加，其在冷启动项目上的可扩展性前景。', 'title_zh': '基于大型语言模型的生成推荐中的无序依赖标识符'}
{'arxiv_id': 'arXiv:2502.10768', 'title': 'Evaluating improvements on using Large Language Models (LLMs) for property extraction in the Open Research Knowledge Graph (ORKG)', 'authors': 'Sandra Schaftner', 'link': 'https://arxiv.org/abs/2502.10768', 'abstract': "Current research highlights the great potential of Large Language Models (LLMs) for constructing Scholarly Knowledge Graphs (SKGs). One particularly complex step in this process is relation extraction, aimed at identifying suitable properties to describe the content of research. This study builds directly on previous research of three Open Research Knowledge Graph (ORKG) team members who assessed the readiness of LLMs such as GPT-3.5, Llama 2, and Mistral for property extraction in scientific literature. Given the moderate performance observed, the previous work concluded that fine-tuning is needed to improve these models' alignment with scientific tasks and their emulation of human expertise. Expanding on this prior experiment, this study evaluates the impact of advanced prompt engineering techniques and demonstrates that these techniques can highly significantly enhance the results. Additionally, this study extends the property extraction process to include property matching to existing ORKG properties, which are retrieved via the API. The evaluation reveals that results generated through advanced prompt engineering achieve a higher proportion of matches with ORKG properties, further emphasizing the enhanced alignment achieved. Moreover, this lays the groundwork for addressing challenges such as the inconsistency of ORKG properties, an issue highlighted in prior studies. By assigning unique URIs and using standardized terminology, this work increases the consistency of the properties, fulfilling a crucial aspect of Linked Data and FAIR principles - core commitments of ORKG. This, in turn, significantly enhances the applicability of ORKG content for subsequent tasks such as comparisons of research publications. Finally, the study concludes with recommendations for future improvements in the overall property extraction process.", 'abstract_zh': '当前的研究突显了大型语言模型（LLMs）在构建学术知识图谱（SKGs）方面巨大的潜力。这一过程中的一个特别复杂的步骤是关系提取，其目标是识别描述研究内容的合适属性。本研究直接建立在previous研究的基础上，该研究评估了三个Open Research Knowledge Graph（ORKG）团队成员对GPT-3.5、Llama 2和Mistral等模型在科学文献中提取属性的准备情况。鉴于观察到的中等性能，前人研究得出结论，需要进行微调以改进这些模型与科学任务的对齐以及模拟人类专业知识的能力。在此前实验的基础上，本研究评估了高级提示工程技巧的影响，并证明这些技巧可以显著提高结果。此外，本研究还将属性提取过程扩展到包括与现有ORKG属性的匹配，这些属性通过API检索。评估结果显示，通过高级提示工程生成的结果与ORKG属性的匹配比例更高，进一步突显了增强的对齐水平。此外，这为解决ORKG属性的一致性问题奠定了基础，这是先前研究中指出的问题之一。通过分配唯一的URI并使用标准化术语，本文增加了属性的一致性，满足了ORKG的核心承诺之一——链接数据和FAIR原则的核心要素。这反过来大大增强了ORKG内容在未来任务中的适用性，如研究出版物的比较。最后，本研究提出了在未来改进属性提取过程的建议。', 'title_zh': '评估在开放研究知识图谱（ORKG）中使用大型语言模型（LLMs）进行属性提取的改进效果'}
{'arxiv_id': 'arXiv:2502.11471', 'title': 'GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion', 'authors': 'Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.11471', 'abstract': 'Knowledge Graph Completion (KGC), which aims to infer missing or incomplete facts, is a crucial task for KGs. However, integrating the vital structural information of KGs into Large Language Models (LLMs) and outputting predictions deterministically remains challenging. To address this, we propose a new method called GLTW, which encodes the structural information of KGs and merges it with LLMs to enhance KGC performance. Specifically, we introduce an improved Graph Transformer (iGT) that effectively encodes subgraphs with both local and global structural information and inherits the characteristics of language model, bypassing training from scratch. Also, we develop a subgraph-based multi-classification training objective, using all entities within KG as classification objects, to boost learning this http URL, we combine iGT with an LLM that takes KG language prompts as this http URL extensive experiments on various KG datasets show that GLTW achieves significant performance gains compared to SOTA baselines.', 'abstract_zh': '知识图谱补全（KGC），旨在推断缺失或不完整的关系，是知识图谱中的关键任务。然而，将知识图谱的关键结构信息整合到大型语言模型（LLMs）中，并使其能够确定性地输出预测结果仍然是一个难题。为了解决这一问题，我们提出了一种名为GLTW的新方法，该方法编码知识图谱的结构信息并将其与LLMs结合以提高KGC的性能。具体而言，我们引入了一种改进的图变换器（iGT），它可以有效地编码具有局部和全局结构信息的子图，并且继承了语言模型的特性，无需从零开始训练。此外，我们开发了一个基于子图的多分类训练目标，将知识图谱中的所有实体作为分类对象，以增强这方面的能力。最后，我们将iGT与一个以知识图谱语言提示为输入的LLM相结合。在多种知识图谱数据集上的广泛实验表明，GLTW在性能上显著优于当前最佳基线方法。', 'title_zh': 'GLTW：基于三词语言的图变压器与大型语言模型联合改进的知识图谱补全'}
{'arxiv_id': 'arXiv:2502.10916', 'title': 'Enhancing Conversational Agents from Open-Source Large Language Models with Illocutionary Force and Document-Based Knowledge Retrieval', 'authors': 'Godfrey Inyama', 'link': 'https://arxiv.org/abs/2502.10916', 'abstract': 'In this paper, we first present a novel way of computationally analysing and extracting illocutionary forces from dialogue using Bert-based Large Language Models, and demonstrate how these features impact the response of a conversational agent guided by a document-based knowledge bank demonstrated by a bespoke web conversational chat agent system developed. Our proposed illocutionary force extraction and classification technique is the first of its kind using the Argument Interchange Format (AIF) Dataset, showing an improved performance compared to two methods for carrying out similar tasks with a macro F1 of approximately 45%. When we evaluated the system based on 2 knowledge files, with 2 user queries each, across 5 open-source large language models (LLMs) using 10 standard metrics we found out that larger open-source models, such as Llama2:13b and Llama3-chatqa-latest, demonstrated an improved alignment when the user illocutionary force was included with their query, achieving higher QA and linguistic similarity scores. The smaller models on the other hand like Tinyllama:latest showed an increased perplexity and mixed performance, which explicitly indicated struggles in processing queries that explicitly included illocutionary forces. The results from the analysis highlight the potential of illocutionary force to enhance conversational depth while underscoring the need for model-specific optimizations to address increased computational costs and response times.', 'abstract_zh': '在本文中，我们首先提出了一种新的计算方法，利用基于BERT的大型语言模型来分析和提取对话中的语用力量，并展示了这些特征如何影响由基于文档的知识库指导的对话代理的响应。我们开发的自定义Web对话聊天代理系统证实了这种语用力量提取和分类技术。这种方法利用了Argument Interchange Format (AIF) 数据集，显示出与执行类似任务的两种方法相比，其宏F1值约为45%，有明显的改进。在使用10项标准指标评估系统时（该评估基于5个开源大型语言模型[LLMs]，每个知识文件包含2个用户查询），我们发现，当包含用户语用力量时，较大的开源模型，如Llama2:13b和Llama3-chatqa-latest，能更好地与用户查询对齐，从而获得更高的问答和语言相似度评分。相比之下，较小的模型，如Tinyllama:latest，则表现出较高的困惑度和混合性能，这表明它们在处理明确包含语用力量的查询时面临困难。分析结果强调了语用力量在增加对话深度方面的潜力，同时也突显了为满足增加的计算成本和响应时间需求而进行模型特定优化的必要性。', 'title_zh': '增强开源大规模语言模型的对话代理功能：通过意动力量和基于文档的知识检索'}
{'arxiv_id': 'arXiv:2502.12150', 'title': 'Idiosyncrasies in Large Language Models', 'authors': 'Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu', 'link': 'https://arxiv.org/abs/2502.12150', 'abstract': "In this work, we unveil and study idiosyncrasies in Large Language Models (LLMs) -- unique patterns in their outputs that can be used to distinguish the models. To do so, we consider a simple classification task: given a particular text output, the objective is to predict the source LLM that generates the text. We evaluate this synthetic task across various groups of LLMs and find that simply fine-tuning existing text embedding models on LLM-generated texts yields excellent classification accuracy. Notably, we achieve 97.1% accuracy on held-out validation data in the five-way classification problem involving ChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals that these idiosyncrasies are rooted in word-level distributions. These patterns persist even when the texts are rewritten, translated, or summarized by an external LLM, suggesting that they are also encoded in the semantic content. Additionally, we leverage LLM as judges to generate detailed, open-ended descriptions of each model's idiosyncrasies. Finally, we discuss the broader implications of our findings, particularly for training on synthetic data and inferring model similarity. Code is available at this https URL.", 'abstract_zh': '在本文中，我们揭示并研究了大型语言模型（LLMs）的独特特性——它们输出中的独特模式，这些模式可用于区分不同的模型。为此，我们考虑了一个简单的分类任务：给定一段特定的文本输出，目标是预测该文本是由哪个LLM生成的。我们在不同组的LLM上评估了这一合成任务，并发现将现有的文本嵌入模型微调在LLM生成的文本上，可以实现优异的分类准确性。值得注意的是，在涉及ChatGPT、Claude、Grok、Gemini和DeepSeek的五分类问题中，我们在保留验证数据上的准确率达到97.1%。进一步的研究显示，这些独特特性源于单词级别的分布。即使在文本被重新编写、翻译或由外部LLM总结后，这些模式依然存在，这表明它们也包含在语义内容中。此外，我们利用LLM作为评判者，生成了每个模型独特特性的详细和开放性描述。最后，我们讨论了本文发现的更广泛影响，特别是关于数据合成训练和模型相似性推断的方面。相关代码可在以下链接获取：[这里提供链接]。', 'title_zh': '大型语言模型中的独特性特征'}
{'arxiv_id': 'arXiv:2502.12134', 'title': 'SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs', 'authors': 'Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao', 'link': 'https://arxiv.org/abs/2502.12134', 'abstract': "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the underlying LLM. Specifically, we employ a lightweight assistant model to generate instance-specific soft thought tokens speculatively as the initial chain of thoughts, which are then mapped into the LLM's representation space via a projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning.", 'abstract_zh': '链式思考（Chain-of-Thought, CoT）推理使大型语言模型（Large Language Models, LLMs）能够通过生成中间推理步骤来解决复杂推理任务。然而，现有的大多数方法集中在硬标记解码上，这限制了推理在离散词汇空间内的进行，并且可能并不总是最优的。尽管最近的研究探索了连续空间推理，但它们往往遭受灾难性遗忘的问题，限制了它们在零样本设置中已经表现出色的先进LLMs的适用性。为了解决这一挑战，我们提出了一种新的连续空间推理方法，该方法不需要修改底层的LLM。具体而言，我们采用一个轻量级助手模型在初始链式思考中生成实例特定的软思考标记，随后通过一个投影模块将这些标记映射到LLM的表示空间中。在五个推理基准上的实验结果表明，我们的方法通过监督下的参数高效微调增强了LLM的推理性能。', 'title_zh': 'SoftCoT：高效生成式预训练模型推理的软链式思维'}
{'arxiv_id': 'arXiv:2502.12110', 'title': 'A-MEM: Agentic Memory for LLM Agents', 'authors': 'Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang', 'link': 'https://arxiv.org/abs/2502.12110', 'abstract': "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at this https URL.", 'abstract_zh': '虽然大规模语言模型（LLM）代理能够有效地利用外部工具来完成复杂的现实世界任务，但它们需要记忆系统来利用历史经验。当前的记忆系统能够提供基本的存储和检索功能，但却缺乏复杂的记忆组织能力，尽管最近有尝试将图数据库纳入其中。此外，这些系统固定的操作和结构限制了其在不同任务中的适应性。为了解决这一局限性，本文提出了一种针对LLM代理的新型代理性记忆系统，能够以代理的方式动态组织记忆。我们的记忆系统基于Zettelkasten方法的基本原则，设计了通过动态索引和链接来构建相互连接的知识网络的机制。当添加新的记忆时，我们将生成一个包含多个结构化属性（包括上下文描述、关键词和标签）的综合笔记。系统随后分析历史记忆以识别相关联接，并在存在有意义相似性的地方建立链接。此外，此过程还允许记忆进化——随着新记忆的整合，可以触发对现有历史记忆的上下文表示和属性的更新，从而使记忆网络能够不断精炼其理解。我们的方法结合了Zettelkasten的结构化组织原则和代理驱动决策的灵活性，从而实现更加适应性和上下文意识的记忆管理。在六种基础模型上的实证实验表明，与现有最优基线相比，我们的方法取得了显著的改进。代码可以在以下链接获取：[此链接]。', 'title_zh': 'A-MEM：代理的记忆能力'}
{'arxiv_id': 'arXiv:2502.12073', 'title': 'Can LLMs Simulate Social Media Engagement? A Study on Action-Guided Response Generation', 'authors': 'Zhongyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo', 'link': 'https://arxiv.org/abs/2502.12073', 'abstract': "Social media enables dynamic user engagement with trending topics, and recent research has explored the potential of large language models (LLMs) for response generation. While some studies investigate LLMs as agents for simulating user behavior on social media, their focus remains on practical viability and scalability rather than a deeper understanding of how well LLM aligns with human behavior. This paper analyzes LLMs' ability to simulate social media engagement through action guided response generation, where a model first predicts a user's most likely engagement action-retweet, quote, or rewrite-towards a trending post before generating a personalized response conditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and DeepSeek-R1 in social media engagement simulation regarding a major societal event discussed on X. Our findings reveal that zero-shot LLMs underperform BERT in action prediction, while few-shot prompting initially degrades the prediction accuracy of LLMs with limited examples. However, in response generation, few-shot LLMs achieve stronger semantic alignment with ground truth posts.", 'abstract_zh': '社交媒体使用户能够动态参与热门话题，近年来的研究探索了大型语言模型（LLMs）在响应生成方面潜在的能力。虽然一些研究将LLMs作为模拟社交媒体用户行为的代理，但他们的重点仍然在于实用可行性和扩展性，而不是更深入地理解LLMs与人类行为的契合程度。本文通过动作指导的响应生成分析了LLMs在模拟社交媒体参与方面的能力，其中模型首先预测用户最有可能对一条热门帖子采取的行动（转发、引用或重写），然后根据预测的行动生成个性化的响应。我们在X平台上针对一项主要社会事件，对比评估了GPT-4o-mini、O1-mini和DeepSeek-R1在社交媒体参与模拟方面的性能。研究发现，零样本的LLMs在行动预测方面不如BERT表现，而少量样例提示最初降低了LLMs的预测准确性。然而，在响应生成方面，少量样例的LLMs能够与真实的帖子内容实现更强的语义一致性。', 'title_zh': '大型语言模型能否模拟社交媒体参与？一项基于行动导向响应生成的研究'}
{'arxiv_id': 'arXiv:2502.12065', 'title': 'Formalizing Complex Mathematical Statements with LLMs: A Study on Mathematical Definitions', 'authors': 'Lan Zhang, Marco Valentino, Andre Freitas', 'link': 'https://arxiv.org/abs/2502.12065', 'abstract': "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions -- a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalisation, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we guide LLMs through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.", 'abstract_zh': '由于语言能力的强大，大型语言模型（LLMs）为通过自动形式化弥合非正式数学和正式语言之间的差距提供了机会。然而，目前仍不清楚LLMs在处理复杂且自然出现的数学语句时的泛化能力如何。为解决这一问题，我们研究了自动形式化真实世界的数学定义——这是数学交流中的关键组成部分。具体而言，我们引入了两个新的自动形式化资源：从Wikipedia收集定义（Def_Wiki）和从arXiv论文收集定义（Def_ArXiv）。随后，我们系统地评估了一系列LLMs，分析它们将定义形式化为Isabelle/HOL的能力。此外，我们还研究了提高LLMs性能的策略，包括通过证明辅助器的外部反馈进行细化，以及通过引导LLMs获得相关形式化数学库的上下文信息进行形式化定义定位。我们的研究发现，定义相比现有的基准（如miniF2F）提出了更大的挑战。特别是，我们发现LLMs在自我修正和与相关数学库保持一致方面仍存在困难。同时，结构化的细化方法和定义定位策略在自我修正能力方面提升了16%，在减少未定义错误方面提升了43%，这为增强基于LLM的自动形式化在实际应用场景中的表现指明了有希望的方向。', 'title_zh': '使用大语言模型正式化复杂数学陈述：关于数学定义的研究'}
{'arxiv_id': 'arXiv:2502.12055', 'title': 'Designing Role Vectors to Improve LLM Inference Behaviour', 'authors': 'Daniele Potertì, Andrea Seveso, Fabio Mercorio', 'link': 'https://arxiv.org/abs/2502.12055', 'abstract': 'The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.', 'abstract_zh': 'persona对大型语言模型（LLMs）的影响已经得到了广泛的研究，但它们对性能的直接影响仍不明朗。本研究探讨了一种新的方法，通过角色向量引导LLM的行为，这提供了一种与基于persona的提示不同的方法。我们构建了29个源自模型激活的角色向量，并评估了它们在多个领域的基准性能影响。我们的分析探讨了这些向量是否能有效地引导模型向特定领域的专长方向发展。我们测量了两种关键干预措施：（i）激活添加，增强角色特定的方向，以及（ii）方向性消除，移除这些方向。在已有基准测试上的结果显示，角色向量确实影响了模型的行为，在相关领域提高了任务性能，而对不相关任务的影响较小。这反过来表明，操控内部模型表示比基于persona的提示对结果的影响更大。', 'title_zh': '设计角色向量以改善大规模语言模型推理行为'}
{'arxiv_id': 'arXiv:2502.11948', 'title': 'Can Your Uncertainty Scores Detect Hallucinated Entity?', 'authors': 'Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li', 'link': 'https://arxiv.org/abs/2502.11948', 'abstract': 'To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research.', 'abstract_zh': '为减轻大语言模型(Large Language Models, LLMs)幻觉性质的影响，许多研究提出了通过不确定性估计检测幻觉生成的方法。然而，这些方法主要在句子或段落级别操作，无法确定产生幻觉内容的具体跨度或实体。对于混合准确信息和虚假信息的长文本输出来说，这种缺乏粒度尤为成问题。为解决这一局限性，我们探索了实体级幻觉检测。我们提出了一种新的数据集——HalluEntity，该数据集在实体级别标注幻觉信息。基于该数据集，我们全面评估了17种现代LLM上的基于不确定性估计的幻觉检测方法。实验结果表明，专注于单个词元概率的不确定性估计方法往往会过度预测幻觉，而具备上下文意识的方法表现更好但仍欠佳。通过深入的定性研究，我们确定了幻觉倾向和语言属性之间的关系，并指出了未来研究的重要方向。', 'title_zh': '你的不确定性得分能否检测出虚构实体？'}
{'arxiv_id': 'arXiv:2502.11932', 'title': 'On Representational Dissociation of Language and Arithmetic in Large Language Models', 'authors': 'Riku Kisako, Tatsuki Kuribayashi, Ryohei Sasano', 'link': 'https://arxiv.org/abs/2502.11932', 'abstract': "The association between language and (non-linguistic) thinking ability in humans has long been debated, and recently, neuroscientific evidence of brain activity patterns has been considered. Such a scientific context naturally raises an interdisciplinary question -- what about such a language-thought dissociation in large language models (LLMs)? In this paper, as an initial foray, we explore this question by focusing on simple arithmetic skills (e.g., $1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in LLMs' representation space. Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations). These tentatively suggest that arithmetic reasoning is mapped into a distinct region from general language input, which is in line with the neuroscientific observations of human brain activations, while we also point out their somewhat cognitively implausible geometric properties.", 'abstract_zh': '人类的语言能力和非语言思维能力之间的关联一直存在争议，最近，神经科学中关于大脑活动模式的证据引起了广泛关注。在此类科学背景下，自然引发了一个跨学科的问题——大型语言模型（LLMs）中的语言-思维分离现象又是怎样的？在本文中，作为初步探索，我们通过关注简单的算术技能（例如：1+2=？）作为一种思维能力，并分析其在LLMs表示空间中的几何编码方式来探讨这一问题。我们使用线性分类器和簇可分性测试的实验表明，简单的算术等式和一般的语言输入在LLMs的内部表示空间的所有层中被完全分离编码，这在使用更受控的刺激（例如：写出的等式）时也得到了支持。这些结果初步表明，算术推理被映射到与一般语言输入不同的区域中，这与人类大脑激活的神经科学观察结果一致，但我们也指出了其略显认知上不合理的一些几何特性。', 'title_zh': '大型语言模型中语言与算术表征的分离研究'}
{'arxiv_id': 'arXiv:2502.11903', 'title': 'MMRC: A Large-Scale Benchmark for Understanding Multimodal Large Language Model in Real-World Conversation', 'authors': 'Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, Yutong Xie, Imran Razzak, Zongyuan Ge, Jionglong Su, Junjun He, Yu Qiao', 'link': 'https://arxiv.org/abs/2502.11903', 'abstract': 'Recent multimodal large language models (MLLMs) have demonstrated significant potential in open-ended conversation, generating more accurate and personalized responses. However, their abilities to memorize, recall, and reason in sustained interactions within real-world scenarios remain underexplored. This paper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for evaluating six core open-ended abilities of MLLMs: information extraction, multi-turn reasoning, information update, image management, memory recall, and answer refusal. With data collected from real-world scenarios, MMRC comprises 5,120 conversations and 28,720 corresponding manually labeled questions, posing a significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC indicate an accuracy drop during open-ended interactions. We identify four common failure patterns: long-term memory degradation, inadequacies in updating factual knowledge, accumulated assumption of error propagation, and reluctance to say no. To mitigate these issues, we propose a simple yet effective NOTE-TAKING strategy, which can record key information from the conversation and remind the model during its responses, enhancing conversational capabilities. Experiments across six MLLMs demonstrate significant performance improvements.', 'abstract_zh': '近期的多模态大型语言模型（MLLMs）在开放式对话中展现出显著潜力，能够生成更准确和个性化的回应。然而，他们在实际场景中持续交互中的记忆、回忆和推理能力仍较少被研究。本论文提出了MMRC，一个用于评估MLLMs六项核心开放式能力的多模态真实世界对话基准：信息提取、多轮推理、信息更新、图像管理、记忆召回以及拒绝回答。该基准数据来源于实际场景，包含5,120场对话和28,720个相应的人工标注问题，对现有MLLMs构成了重大挑战。在MMRC上对20种MLLMs的评估表明，在开放式对话中准确率有所下降。我们识别出了四种常见的失败模式：长期记忆退化、事实知识更新不足、错误传播累积假设以及拒绝回答的犹豫。为了缓解这些问题，我们提出了一种简单而有效的笔记策略（NOTE-TAKING），该策略可以在对话过程中记录关键信息并在模型响应时提醒模型，从而增强对话能力。通过六个MLLMs的实验展示了显著性能提升。', 'title_zh': 'MMRC：一个大规模基准，用于理解多模态大型语言模型在实际对话中的表现'}
{'arxiv_id': 'arXiv:2502.11901', 'title': 'Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity', 'authors': 'Dylan Zhang, Justin Wang, Tianran Sun', 'link': 'https://arxiv.org/abs/2502.11901', 'abstract': "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.", 'abstract_zh': '现有的语言模型在处理证明导向编程时面临挑战，主要是由于数据稀缺，这主要体现在两个方面：（1）缺乏足够的证明导向编程语言（如F*）语料库；（2）缺乏大规模的项目级证明导向实现，这些实现能够教会模型在进行证明导向编程时复杂的推理过程。我们首次提出了基于合成数据增强的方法，用于项目级别的证明导向编程，涵盖生成和修复场景。我们的方法通过合成基本的证明导向编程问题来解决数据稀缺问题，这些问题是针对该语言的精通问题；通过结合多样化的编码数据来激发推理能力，并在现有仓库中创建新的证明和修复数据。这种方法使得语言模型既能够合成代码中的证明，也能在函数级和仓库级代码中进行修复。实验结果表明，我们微调的14B参数模型PoPilot在项目级别证明导向编程方面超过了性能优于GPT-4o的模型，相对性能高出64%；通过修复GPT-4o的输出，还可以提高其性能54%。', 'title_zh': '在数据稀缺条件下，构建一个证明导向的程序员，其表现比GPT-4o高出64%'}
{'arxiv_id': 'arXiv:2502.11856', 'title': 'LLMs as a synthesis between symbolic and continuous approaches to language', 'authors': 'Gemma Boleda', 'link': 'https://arxiv.org/abs/2502.11856', 'abstract': 'Since the middle of the 20th century, a fierce battle is being fought between symbolic and continuous approaches to language and cognition. The success of deep learning models, and LLMs in particular, has been alternatively taken as showing that the continuous camp has won, or dismissed as an irrelevant engineering development. However, in this position paper I argue that deep learning models for language actually represent a synthesis between the two traditions. This is because 1) deep learning architectures allow for both continuous/distributed and symbolic/discrete-like representations and computations; 2) models trained on language make use this flexibility. In particular, I review recent research in mechanistic interpretability that showcases how a substantial part of morphosyntactic knowledge is encoded in a near-discrete fashion in LLMs. This line of research suggests that different behaviors arise in an emergent fashion, and models flexibly alternate between the two modes (and everything in between) as needed. This is possibly one of the main reasons for their wild success; and it is also what makes them particularly interesting for the study of language and cognition. Is it time for peace?', 'abstract_zh': '自20世纪中叶以来，语言和认知的符号主义和连续主义方法之间展开了一场激烈的争论。深度学习模型的成功，尤其是大型语言模型（LLMs）的成功，有时被视为连续主义阵营获胜的标志，有时则被视为无关的工程发展。然而，在这篇观点论文中，我认为深度学习语言模型实际上是这两种传统的融合。这主要有两个原因：1）深度学习架构能够同时支持连续分布式和符号离散式的表示与计算；2）被训练用于语言的模型利用了这种灵活性。特别是，我回顾了最近关于机制可解释性的研究，展示了形态句法知识在大型语言模型中以接近离散的方式编码的情况。这种研究路径表明，不同行为以一种产生式的方式出现，并且模型根据需要灵活地在两种模式之间（以及它们之间的各种模式）切换。这可能是它们取得惊人成功的主要原因之一；同时，这也是它们在语言和认知研究中显得特别有趣的根源。现在是时候寻求和解了吗？', 'title_zh': '大语言模型作为符号方法和连续方法在语言处理中的融合'}
{'arxiv_id': 'arXiv:2502.11811', 'title': 'FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models', 'authors': 'Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng', 'link': 'https://arxiv.org/abs/2502.11811', 'abstract': 'Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance this http URL methods use re-ranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying the potential clues from multiple documents using contextual information, then ranking them by relevance, and finally retaining the least clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG consisting of a clue extractor, a re-ranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) Clue extractor firstly uses sentences containing the answer and similar ones as fine-tuned targets, aiming at extracting sufficient potential clues; (2) Re-ranker is trained to prioritize effective clues based on the real feedback from generation module, with clues capable of generating correct answer as positive samples and others as negative; (3) Truncator takes the minimum clues needed to answer the question (truncation point) as fine-tuned targets, and performs truncation on the re-ranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly outperforms baselines in terms of performance and inference cost. Further analysis on each module shows the effectiveness of our optimizations for complex reasoning.', 'abstract_zh': '含有噪声的检索文档将妨碍 Retrieval-Augmented Generation (RAG) 检测答案线索的能力，因此需要噪声过滤机制来增强此功能。现有方法通常使用重排序或总结来识别最相关的句子，但直接且精确地从这些大规模且复杂的文档中定位答案线索仍然极具挑战性。与这些文档级别的操作不同，我们将其噪声过滤视为一个句子级别的 MinMax 最优化问题：首先利用上下文信息在多个文档中识别潜在线索，然后按相关性进行排序，最后通过截断保留最少的线索。在本文中，我们提出了一种新颖的细粒度噪声过滤机制 FineFilter，该机制包含线索提取器、重排序器和截断器三个模块。我们优化每个模块以应对复杂的推理挑战：(1) 线索提取器首先使用包含答案及其相似句子作为微调目标，旨在提取足够的潜在线索；(2) 重排序器基于生成模块的真实反馈进行训练，优先排序有效线索，具有生成正确答案的线索作为正样本，其他作为负样本；(3) 截断器将需要回答问题的最少线索（截断点）作为微调目标，并在排序后的线索上进行截断以实现细粒度的噪声过滤。在三个问答数据集上的实验表明，FineFilter 在性能和推理成本方面显著优于基线方法。进一步对每个模块的分析表明，我们的优化措施在复杂推理方面具有有效性。', 'title_zh': 'FineFilter：面向检索增强大型语言模型的细粒度噪声过滤机制'}
{'arxiv_id': 'arXiv:2502.11789', 'title': 'Personality Editing for Language Models through Relevant Knowledge Editing', 'authors': 'Seojin Hwang, Yumin Kim, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11789', 'abstract': "Large Language Models (LLMs) play a vital role in applications like conversational agents and content creation, where controlling a model's personality is crucial for maintaining tone, consistency, and engagement. However, traditional prompt-based techniques for controlling personality often fall short, as they do not effectively mitigate the model's inherent biases. In this paper, we introduce a novel method PALETTE that enhances personality control through knowledge editing. By generating adjustment queries inspired by psychological assessments, our approach systematically adjusts responses to personality-related queries similar to modifying factual knowledge, thereby achieving controlled shifts in personality traits. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.", 'abstract_zh': '大语言模型（LLMs）在对话代理和内容创作等应用中发挥着重要作用，而控制模型的性格对于保持语气、一致性以及用户参与度至关重要。然而，传统基于提示的技术在控制性格方面往往不够有效，因为它们无法有效地缓解模型固有的偏见。在本文中，我们提出了一种新的方法——PALETTE，通过知识编辑增强性格控制。通过生成受心理评估启发的调整查询，我们的方法系统地调整与性格相关的查询的响应，类似于修改事实性知识，从而实现控制性个人特质的变化。来自自动评估和人工评估的实验结果表明，我们的方法能够使LLMs在性格控制方面表现出更强的稳定性和平衡性。', 'title_zh': '通过相关知识编辑对语言模型进行个性编辑'}
{'arxiv_id': 'arXiv:2502.11779', 'title': 'Efficient Response Generation Method Selection for Fine-Tuning Large Language Models', 'authors': 'Xuan Ren, Qi Chen, Lingqiao Liu', 'link': 'https://arxiv.org/abs/2502.11779', 'abstract': "The training data for fine-tuning large language models (LLMs) is typically structured as input-output pairs. However, for many tasks, there can be multiple equally valid output variations for the same input. Recent studies have observed that the choice of output variation used in training can affect the model's performance. This raises an important question: how can we generate the most effective output from the many possible response generation strategy options? Rather than relying on the traditional but resource-intensive train-and-evaluate approach, this paper proposes a scalable, approximate method for estimating the quality of a small subset of generated training data derived from the same input. We then evaluate how well this small subset of generated output fits the target model we are trying to train. We present a large-scale benchmark covering diverse reasoning-based datasets to support our study.\nThe central idea is that a good output should closely resemble the output generated by the target LLM. We formalize this 'closeness' as the expected alignment score between a candidate output and the output sampled from the target LLM. We connect this measurement to the perplexity metric used in previous literature and demonstrate that leveraging an alignment-based metric can provide better predictions of model performance. Using this strategy, we can evaluate a small subset of the generated output from each response generation strategy option, then select the most effective strategy. We show that an LLM trained on data generated by the selected strategy could lead to a significant performance gain in many cases.", 'abstract_zh': '用于微调大规模语言模型（LLM）的训练数据通常以输入-输出对的形式结构化。然而，对于许多任务，同一个输入可能有多个同样合理的输出变体。最近的研究观察到，在训练中使用的输出变体选择会影响模型的性能。这提出了一个重要的问题：如何从多种可能的生成输出策略中生成最有效的输出？本文不依赖传统的但资源密集的训练和评估方法，而是提出了一种可扩展的近似方法来估计从相同输入生成的小子集训练数据的质量。然后我们评估这个小型生成输出子集与我们试图训练的目标模型的匹配度。我们提出了一项大规模基准测试，涵盖了多种基于推理的数据集，以支持我们的研究。\n\n核心理念是一个好的输出应该与目标LLM生成的输出高度相似。我们将这种“相似度”形式化为候选输出与从目标LLM采样输出之间的预期对齐分数。我们将这一测量与以往文献中使用的手法困惑度指标相连，并展示了利用基于对齐的度量可以更好地预测模型性能。通过这种方法，我们可以评估每个生成输出策略的生成输出子集，然后选择最有效的策略。我们表明，使用所选策略生成的数据训练的LLM在许多情况下可以带来显著的性能提升。', 'title_zh': '大型语言模型微调中的高效响应生成方法选择方法'}
{'arxiv_id': 'arXiv:2502.11733', 'title': 'Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking Practical Reasoning and Situation Modelling in a Text-Simulated Situated Environment', 'authors': 'Jonathan Jordan, Sherzod Hakimov, David Schlangen', 'link': 'https://arxiv.org/abs/2502.11733', 'abstract': "Large language models (LLMs) have risen to prominence as 'chatbots' for users to interact via natural language. However, their abilities to capture common-sense knowledge make them seem promising as language-based planners of situated or embodied action as well. We have implemented a simple text-based environment -- similar to others that have before been used for reinforcement-learning of agents -- that simulates, very abstractly, a household setting. We use this environment and the detailed error-tracking capabilities we implemented for targeted benchmarking of LLMs on the problem of practical reasoning: Going from goals and observations to actions. Our findings show that environmental complexity and game restrictions hamper performance, and concise action planning is demanding for current LLMs.", 'abstract_zh': '大型语言模型（LLMs）已成为用户通过自然语言进行交互的“聊天机器人”。然而，它们捕获常识知识的能力使它们在基于语言的计划者方面具有潜力，用于规划情境中的行动或有身体表现的行动。我们实现了一个简单的基于文本的环境——类似于之前用于代理强化学习的环境——该环境以非常抽象的方式模拟了家庭环境。我们利用此环境以及我们为具体基准测试LLMs在实际推理问题上的表现而实现的详细错误跟踪能力。我们的研究发现表明，环境的复杂性和游戏限制阻碍了性能，而简洁的行动规划对现有的LLM来说是具有挑战性的。', 'title_zh': '在柜子中的植物、桌子上的橙子、书架上的书：基于文本模拟的环境下的实用推理与情境建模基准测试'}
{'arxiv_id': 'arXiv:2502.11703', 'title': 'CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation', 'authors': 'Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan', 'link': 'https://arxiv.org/abs/2502.11703', 'abstract': 'Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repo this https URL.', 'abstract_zh': '医学质量控制指标是评估医疗机构医疗服务资格的重要手段。随着大型语言模型（LLMs）如GPT-4在医学领域的出色表现，利用这些技术进行医学质量控制指标计算（MQCIC）展现出了巨大的潜力。本研究中，我们（1）介绍了实际应用场景中的MQCIC任务，并提出了一款基于中文电子病历（EMRs）的开源数据集（CMQCIC-Bench），包含785个实例和76个指标。（2）我们提出了一种半自动方法来增强规则表示，随后提出了基于临床事实的推理规则（CF-IR）方法，该方法分离了临床事实验证和推理规则推理的操作。（3）我们在20种代表性语言模型上进行了全面实验，涵盖通用和医学模型。研究结果表明，CF-IR方法在MQCIC任务中优于链式思考方法。（4）我们进行了错误分析，探讨了临床事实验证和推理规则推理的能力，为提高MQCIC性能提供了见解。数据集和源代码可在以下链接获取：[此链接]。', 'title_zh': 'CMQCIC-Bench：用于评估大型语言模型在医疗质量控制指标计算中性能的中文基准stile\nuser\n能否提供一个英文解释，说明CMQCIC-Bench这个名字的具体含义？'}
{'arxiv_id': 'arXiv:2502.11689', 'title': 'Improve LLM-as-a-Judge Ability as a General Ability', 'authors': 'Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li', 'link': 'https://arxiv.org/abs/2502.11689', 'abstract': "LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values, ensuring ethical and reliable AI outputs that align with societal norms. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we regard judge ability as a general ability of LLM and implement a two-stage training approach, comprising supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO) enhancement, to achieve judge style adaptation and improve judgment accuracy. Additionally, we introduce an efficient data synthesis method to generate judgmental content. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task, and the judge signals provided by our model have significantly enhanced the downstream DPO training performance of our internal models in our test to optimize policy model with Judge Model. We also open-source our model weights and training data to facilitate further research.", 'abstract_zh': 'LLM-as-a-Judge 利用了大语言模型（LLMs）的生成和推理能力，评估各种场景下的LLM响应，提供准确的偏好信号。这种方法在使LLMs与人类价值观保持一致方面发挥着关键作用，确保生成符合社会规范的伦理可靠AI输出。最近的研究提出了许多训练LLM作为生成性法官的方法，但大多数方法都消耗大量数据且缺乏准确性，并且仅专注于LLM的法官能力。在本研究中，我们将法官能力视为LLM的普遍能力，并采用两阶段训练方法，包括监督微调（SFT）预热和直接偏好优化（DPO）增强，以实现法官风格适应并提高判断准确性。此外，我们介绍了一种高效的数据合成方法，用于生成判断内容。实验结果表明，我们的方法仅需其他方法所需数据的大约2%到40%，即可在RewardBench上达到SOTA性能。此外，我们的训练方法通过构建复杂的法官任务来增强模型的通用能力，并且我们模型提供的法官信号显著提高了我们的内部模型在使用法官模型优化策略模型时的DPO训练性能。我们也开源了我们的模型权重和训练数据，以促进进一步的研究。', 'title_zh': '提升作为通用能力的LLM法官能力'}
{'arxiv_id': 'arXiv:2502.11677', 'title': 'Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception', 'authors': 'Shiyu Ni, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11677', 'abstract': "Large language models (LLMs) exhibit impressive performance across diverse tasks but often struggle to accurately gauge their knowledge boundaries, leading to confident yet incorrect responses. This paper explores leveraging LLMs' internal states to enhance their perception of knowledge boundaries from efficiency and risk perspectives. We investigate whether LLMs can estimate their confidence using internal states before response generation, potentially saving computational resources. Our experiments on datasets like Natural Questions, HotpotQA, and MMLU reveal that LLMs demonstrate significant pre-generation perception, which is further refined post-generation, with perception gaps remaining stable across varying conditions. To mitigate risks in critical domains, we introduce Consistency-based Confidence Calibration ($C^3$), which assesses confidence consistency through question reformulation. $C^3$ significantly improves LLMs' ability to recognize their knowledge gaps, enhancing the unknown perception rate by 5.6\\% on NQ and 4.9\\% on HotpotQA. Our findings suggest that pre-generation confidence estimation can optimize efficiency, while $C^3$ effectively controls output risks, advancing the reliability of LLMs in practical applications.", 'abstract_zh': '大型语言模型（LLMs）在多种任务中展现出卓越的性能，但往往难以准确判断其知识边界，从而产生自信但错误的回应。本文探讨了利用LLMs内部状态来从效率和风险角度增强其对知识边界的感知能力。我们研究了LLMs是否可以在生成响应之前利用内部状态来估算其置信度，从而节省计算资源。我们在Natural Questions、HotpotQA和MMLU等数据集上的实验表明，LLMs在生成前展示出显著的预生成感知能力，生成后进一步细化，感知差距在不同条件下保持稳定。为了在关键领域缓解风险，我们引入了一致性基置信校准（$C^3$），通过问题重述评估置信一致性。$C^3$显著提高了LLMs识别知识缺口的能力，在Natural Questions和HotpotQA数据集上分别提升了知识未知感知率5.6%和4.9%。我们的研究结果表明，预生成置信度估计可以优化效率，而$C^3$有效控制输出风险，有助于提高LLMs在实际应用中的可靠性。', 'title_zh': '充分利用大语言模型内部状态以增强知识边界感知'}
{'arxiv_id': 'arXiv:2502.11598', 'title': 'Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?', 'authors': 'Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.11598', 'abstract': 'The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLM）水印的放射性特性使其能够在学生模型通过训练水标教师模型的输出时检测到继承的水印，从而使其成为防止未经授权的知识蒸馏的有前景工具。然而，水标放射性对抗敌对手的能力仍然鲜有研究。在本文中，我们探讨了学生模型在避免继承水印的同时是否可以通过知识蒸馏获取教师模型的能力。我们提出了两类水标记除方法：通过未目标和目标训练数据改写（UP和TP）在知识蒸馏前去除水印，以及通过推断时水标记中和（WN）在知识蒸馏后去除水印。在多个模型对、水印方案和超参数设置的广泛实验中，我们的结果表明，TP和WN都能够完全消除继承的水印，且WN能够在保持高效的知识迁移能力和低计算开销的同时实现这一目标。鉴于水标记技术在生产中型语言模型中的持续部署，这些发现强调了开发更 robust 防御策略的急迫需求。我们的代码可在以下链接获取：[此处替换为链接]。', 'title_zh': '大规模语言模型水印能否可靠地防止未经授权的知识蒸馏？'}
{'arxiv_id': 'arXiv:2502.11562', 'title': 'Reinforced Information Retrieval', 'authors': 'Chaofan Li, Zheng Liu, Jianlyv Chen, Defu Lian, Yingxia Shao', 'link': 'https://arxiv.org/abs/2502.11562', 'abstract': "While retrieval techniques are widely used in practice, they still face significant challenges in cross-domain scenarios. Recently, generation-augmented methods have emerged as a promising solution to this problem. These methods enhance raw queries by incorporating additional information from an LLM-based generator, facilitating more direct retrieval of relevant documents. However, existing methods struggle with highly specialized situations that require extensive domain expertise. To address this problem, we present \\textbf{Reinforced-IR}, a novel approach that jointly adapts a pre-trained retriever and generator for precise cross-domain retrieval. A key innovation of Reinforced-IR is its \\textbf{Self-Boosting} framework, which enables retriever and generator to learn from each other's feedback. Specifically, the generator is reinforced to generate query augmentations that enhance the retriever's performance, while the retriever is trained to better discriminate the relevant documents identified by the generator. This iterative process allows the end-to-end retrieval performance to be progressively optimized using an unlabeled corpus from the target domain. In our experiment, Reinforced-IR outperforms existing domain adaptation methods by a large margin, leading to substantial improvements in retrieval quality across a wide range of application scenarios.", 'abstract_zh': '虽然检索技术在实际应用中得到广泛应用，但在跨域场景中仍然面临重大挑战。近年来，生成增强方法作为一种有前景的解决方案逐渐浮现。这些方法通过结合基于LLM的生成器提供的附加信息来增强原始查询，从而促进更直接的相关文档检索。然而，现有的方法在高度专业化的情境下表现不佳，这些情境需要广泛的领域专业知识。为解决这一问题，我们提出了**Reinforced-IR**，一种新颖的方法，该方法联合调整预训练的检索器和生成器，以实现精确的跨域检索。Reinforced-IR 的一个关键创新是其**自我增强**框架，该框架使检索器和生成器能够从彼此的反馈中学习。具体而言，生成器通过生成增强检索器性能的查询增强来得到强化，而检索器则被训练以更好地鉴别生成器标识的相关文档。这一迭代过程允许通过目标领域的未标记语料库逐步优化端到端的检索性能。在我们的实验中，Reinforced-IR 显著优于现有的领域适应方法，在多种应用场景中显著提升了检索质量。', 'title_zh': '强化信息检索'}
{'arxiv_id': 'arXiv:2502.11544', 'title': 'Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through Comprehensive Analysis', 'authors': 'Andong Chen, Yuchen Song, Wenxin Zhu, Kehai Chen, Muyun Yang, Tiejun Zhao, Min zhang', 'link': 'https://arxiv.org/abs/2502.11544', 'abstract': 'The o1-Like LLMs are transforming AI by simulating human cognitive processes, but their performance in multilingual machine translation (MMT) remains underexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks and (2) what factors influence their translation quality. We evaluate multiple o1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o. Results show that o1-Like LLMs establish new multilingual translation benchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They demonstrate strengths in historical and cultural translation but exhibit a tendency for rambling issues in Chinese-centric outputs. Further analysis reveals three key insights: (1) High inference costs and slower processing speeds make complex translation tasks more resource-intensive. (2) Translation quality improves with model size, enhancing commonsense reasoning and cultural translation. (3) The temperature parameter significantly impacts output quality-lower temperatures yield more stable and accurate translations, while higher temperatures reduce coherence and precision.', 'abstract_zh': '类似于o1的大型语言模型（LLMs）正在通过模拟人类认知过程来改变人工智能领域，但在多语言机器翻译（MMT）方面的表现仍未充分探索。本研究旨在探讨：（1）类似于o1的LLMs在MMT任务中的表现，以及（2）哪些因素影响其翻译质量。我们评估了多个类似于o1的LLMs，并将其与传统的模型如ChatGPT和GPT-4o进行比较。结果显示，类似于o1的LLMs建立了新的多语言翻译基准，DeepSeek-R1在无语境任务中超过了GPT-4o。它们在历史和文化交流翻译方面显示出优势，但在以中文为中心的输出中存在逻辑冗长的问题。进一步分析揭示了三点关键见解：（1）高推理成本和较慢的处理速度使复杂翻译任务更为资源密集型。（2）模型规模的增大可以提高翻译质量，增强常识推理和文化翻译能力。（3）温度参数显著影响输出质量，较低的温度值产出更稳定和准确的翻译，而较高的温度值则降低了连贯性和精确性。', 'title_zh': '评估o1-样式的大型语言模型：通过全面分析解锁翻译推理功能'}
{'arxiv_id': 'arXiv:2502.11533', 'title': 'Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy', 'authors': 'Zhenyuan Guo, Yi Shi, Wenlong Meng, Chen Gong, Chengkun Wei, Wenzhi Chen', 'link': 'https://arxiv.org/abs/2502.11533', 'abstract': 'Model merging is a widespread technology in large language models (LLMs) that integrates multiple task-specific LLMs into a unified one, enabling the merged model to inherit the specialized capabilities of these LLMs. Most task-specific LLMs are sourced from open-source communities and have not undergone rigorous auditing, potentially imposing risks in model merging. This paper highlights an overlooked privacy risk: \\textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.} Specifically, we propose PhiMM, a privacy attack approach that trains a phishing model capable of stealing privacy using a crafted privacy phishing instruction dataset. Furthermore, we introduce a novel model cloaking method that mimics a specialized capability to conceal attack intent, luring users into merging the phishing model. Once victims merge the phishing model, the attacker can extract personally identifiable information (PII) or infer membership information (MI) by querying the merged model with the phishing instruction. Experimental results show that merging a phishing model increases the risk of privacy breaches. Compared to the results before merging, PII leakage increased by 3.9\\% and MI leakage increased by 17.4\\% on average. We release the code of PhiMM through a link.', 'abstract_zh': '模型合并是大语言模型（LLMs）中广泛使用的一项技术，它将多个任务特定的LLMs整合成一个统一的模型，从而使合并后的模型能够继承这些LLMs的专业能力。大多数任务特定的LLMs来自开源社区，未经过严格的审计，这可能在模型合并过程中带来风险。本文强调了一个被忽视的隐私风险：**不安全的模型可能会泄露参与模型合并的其他LLMs的隐私**。具体来说，我们提出了一种名为PhiMM的隐私攻击方法，该方法通过构造一个精心设计的隐私钓鱼指令数据集来训练一个能够窃取隐私的模型。此外，我们引入了一种新的模型伪装方法，模拟一种特殊能力以隐藏攻击意图，诱使用户将钓鱼模型纳入合并模型中。一旦受害者将钓鱼模型合并进来，攻击者可以通过使用钓鱼指令查询合并后的模型来提取个人可识别信息（PII）或推断成员身份信息（MI）。实验结果表明，合并包含钓鱼模型会增加隐私泄露的风险。与合并前的实验结果相比，平均而言，PII泄露增加了3.9%，MI泄露增加了17.4%。我们通过链接发布了PhiMM的代码。', 'title_zh': '谨慎合并不熟悉的大型语言模型：一款具备盗取隐私能力的钓鱼模型'}
{'arxiv_id': 'arXiv:2502.11525', 'title': 'Training Large Language Models to be Better Rule Followers', 'authors': 'Yi Hu, Shijia Kang, Haotong Yang, Haotian Xu, Muhan Zhang', 'link': 'https://arxiv.org/abs/2502.11525', 'abstract': 'Large language models (LLMs) have shown impressive performance across a wide range of tasks. However, they often exhibit unexpected failures in seemingly straightforward tasks, suggesting a reliance on case-based reasoning rather than rule-based reasoning. While the vast training corpus of LLMs contains numerous textual "rules", current training methods fail to leverage these rules effectively. Crucially, the relationships between these "rules" and their corresponding "instances" are not explicitly modeled. As a result, while LLMs can often recall rules with ease, they fail to apply these rules strictly and consistently in relevant reasoning scenarios. In this paper, we investigate the rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance the cross-task transferability of rule-following abilities. We first construct a dataset of 88 tasks requiring following rules, encompassing diverse reasoning domains. We demonstrate through extensive experiments that models trained on large-scale rule-following tasks are better rule followers, outperforming the baselines in both downstream fine-tuning and few-shot prompting scenarios. This highlights the cross-task transferability of models with the aid of Meta-RFFT. Furthermore, we examine the influence of factors such as dataset size, rule formulation, and in-context learning.', 'abstract_zh': '大语言模型（LLMs）在多种任务上表现出色，但在看似简单的任务上却经常出现意外的失败，这表明它们更依赖案例推理而非规则推理。尽管LLMs的庞大训练语料库包含了大量的文本“规则”，但当前的训练方法未能有效地利用这些规则。最关键的是，这些“规则”与其对应的“实例”之间的关系没有明确建模。因此，虽然LLMs可以轻松回忆规则，但在相关推理场景中却无法严格一致地应用这些规则。本文研究了LLMs的规则遵守能力，并提出了一种元规则遵守微调方法（Meta-RFFT），以增强规则遵守能力的跨任务迁移性。首先，我们构造了一个包含88个任务的数据集，这些任务要求遵守规则，涵盖了多种推理领域。通过广泛的实验表明，在大规模规则遵守任务上训练的模型在下游微调和少量示例提示场景中表现出更好的规则遵守能力，优于基线模型。这突显了在Meta-RFFT的辅助下模型的跨任务迁移性。此外，我们还探讨了数据集规模、规则表述和上下文学习等因素的影响。', 'title_zh': '训练大型语言模型以成为更好的规则遵循者'}
{'arxiv_id': 'arXiv:2502.11517', 'title': 'Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding', 'authors': 'Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, Michael Carbin', 'link': 'https://arxiv.org/abs/2502.11517', 'abstract': 'Decoding with autoregressive large language models (LLMs) traditionally occurs sequentially, generating one token after another. An emerging line of work explored parallel decoding by identifying and simultaneously generating semantically independent chunks of LLM responses. However, these techniques rely on hand-crafted heuristics tied to syntactic structures like lists and paragraphs, making them rigid and imprecise. We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses. At its core are PASTA-LANG and its interpreter: PASTA-LANG is an annotation language that enables LLMs to express semantic independence in their own responses; the language interpreter acts on these annotations to orchestrate parallel decoding on-the-fly at inference time. Through a two-stage finetuning process, we train LLMs to generate PASTA-LANG annotations that optimize both response quality and decoding speed. Evaluation on AlpacaEval, an instruction following benchmark, shows that our approach Pareto-dominates existing methods in terms of decoding speed and response quality; our results demonstrate geometric mean speedups ranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to -7.1%, measured by length-controlled win rates against sequential decoding baseline.', 'abstract_zh': '自回归大型语言模型（LLMs）传统的解码过程是顺序进行的，逐个生成一个令牌后再生成下一个。一项新兴的研究方向通过识别和同时生成语义独立的LLM响应片段来探索并行解码的方法。然而，这些技术依赖于与句法结构（如列表和段落）相关的手工设计启发式规则，这使得它们既僵硬又不够精确。我们提出了一种基于学习的系统PASTA，该系统通过教会LLMs识别语义独立性并在其自身响应中表达并行解码机会来实现解码。其核心是PASTA-LANG及其解释器：PASTA-LANG是一种注释语言，使得LLMs能够在其自身响应中表达语义独立性；解释器根据这些注释，在推断时实时协调并行解码。通过两阶段微调过程，我们训练LLMs生成优化响应质量和解码速度的PASTA-LANG注释。在AlpacaEval（指令跟随基准）上的评估显示，我们的方法在解码速度和响应质量方面相对于现有方法都具有支配性优势；我们的结果显示，在控制长度的情况下，与顺序解码基准相比，我们方法的速度平均提高了1.21到1.93倍，同时质量变化范围为+2.2%到-7.1%。', 'title_zh': '学习守约：通过学习异步解码扩展语言模型解码并行性'}
{'arxiv_id': 'arXiv:2502.11493', 'title': 'DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft Tokens', 'authors': 'Shaoshen Chen, Yangning Li, Zishan Xu, Yinghui Li, Xin Su, Zifei Shan, Hai-tao Zheng', 'link': 'https://arxiv.org/abs/2502.11493', 'abstract': "Large Language Models (LLMs) face computational inefficiencies and redundant processing when handling long context inputs, prompting a focus on compression techniques. While existing semantic vector-based compression methods achieve promising performance, these methods fail to account for the intrinsic information density variations between context chunks, instead allocating soft tokens uniformly across context chunks. This uniform distribution inevitably diminishes allocation to information-critical regions. To address this, we propose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method that leverages the LLM's intrinsic understanding of contextual relevance to guide compression. DAST combines perplexity-based local information with attention-driven global information to dynamically allocate soft tokens to the informative-rich chunks, enabling effective, context-aware compression. Experimental results across multiple benchmarks demonstrate that DAST surpasses state-of-the-art methods.", 'abstract_zh': '大语言模型（LLMs）在处理长上下文输入时面临计算效率低下和冗余处理的问题，这促使人们关注压缩技术。尽管现有的基于语义向量的压缩方法取得了令人瞩目的性能，但这些方法未能考虑到上下文片段之间固有的信息密度差异，而是均匀分配软标记到各个上下文片段中。这种均匀分布不可避免地减少了对信息关键区域的分配。为了解决这一问题，我们提出了一种简单而有效的动态软标记分配方法（Dynamic Allocation of Soft Tokens, DAST），该方法利用LLM对上下文相关性的内在理解来指导压缩过程。DAST 结合基于困惑度的局部信息与基于注意力的全局信息，动态地将软标记分配给信息丰富的片段，从而实现有效的、上下文感知的压缩。在多个基准测试上的实验结果表明，DAST 超越了现有的先进方法。', 'title_zh': 'DAST：通过动态分配软令牌实现的基于上下文的LLM压缩'}
{'arxiv_id': 'arXiv:2502.11444', 'title': 'Does RAG Really Perform Bad For Long-Context Processing?', 'authors': 'Kun Luo, Zheng Liu, Peitian Zhang, Hongjin Qian, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.11444', 'abstract': "The efficient processing of long context poses a serious challenge for large language models (LLMs). Recently, retrieval-augmented generation (RAG) has emerged as a promising strategy for this problem, as it enables LLMs to make selective use of the long context for efficient computation. However, existing RAG approaches lag behind other long-context processing methods due to inherent limitations on inaccurate retrieval and fragmented contexts. To address these challenges, we introduce RetroLM, a novel RAG framework for long-context processing. Unlike traditional methods, RetroLM employs KV-level retrieval augmentation, where it partitions the LLM's KV cache into contiguous pages and retrieves the most crucial ones for efficient computation. This approach enhances robustness to retrieval inaccuracy, facilitates effective utilization of fragmented contexts, and saves the cost from repeated computation. Building on this framework, we further develop a specialized retriever for precise retrieval of critical pages and conduct unsupervised post-training to optimize the model's ability to leverage retrieved information. We conduct comprehensive evaluations with a variety of benchmarks, including LongBench, InfiniteBench, and RULER, where RetroLM significantly outperforms existing long-context LLMs and efficient long-context processing methods, particularly in tasks requiring intensive reasoning or extremely long-context comprehension.", 'abstract_zh': '处理长上下文的高效计算对大规模语言模型（LLMs）构成了严重挑战。最近，检索增强生成（RAG）作为一种有前途的策略出现了，因为它使LLMs能够有选择地利用长上下文进行高效的计算。然而，现有的RAG方法由于在检索不准确和上下文碎片化方面存在固有局限性，其性能落后于其他长上下文处理方法。为了解决这些挑战，我们引入了RetroLM，这是一种新的RAG框架，用于处理长上下文。与传统方法不同，RetroLM采用KV级检索增强，即它将LLM的KV缓存分割成连续的页面，并检索最关键的页面以实现高效的计算。这种方法增强了对检索不准确性的鲁棒性，促进了对碎片化上下文的有效利用，并节省了重复计算的成本。在此基础上，我们进一步开发了一种专门的检索器，用于精确检索关键页面，并进行了无监督的后训练以优化模型利用检索信息的能力。我们使用LongBench、InfiniteBench和RULER等多种基准进行了全面评估，其中RetroLM显著优于现有的长上下文LLMs和高效的长上下文处理方法，特别是在要求密集推理或极长上下文理解的任务中表现更为突出。', 'title_zh': 'RAG在长上下文处理中真的表现不佳吗？'}
{'arxiv_id': 'arXiv:2502.11438', 'title': 'SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL', 'authors': 'Jimin Lee, Ingeol Baek, Byeongjeong Kim, Hwanhee Lee', 'link': 'https://arxiv.org/abs/2502.11438', 'abstract': 'Text-to-SQL aims to convert natural language questions into executable SQL queries. While previous approaches, such as skeleton-masked selection, have demonstrated strong performance by retrieving similar training examples to guide large language models (LLMs), they struggle in real-world scenarios where such examples are unavailable. To overcome this limitation, we propose Self-Augmentation in-context learning with Fine-grained Example selection for Text-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by generating and filtering self-augmented examples. SAFE-SQL first prompts an LLM to generate multiple Text-to-SQL examples relevant to the test input. Then SAFE-SQL filters these examples through three relevance assessments, constructing high-quality in-context learning examples. Using self-generated examples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL frameworks, achieving higher execution accuracy. Notably, our approach provides additional performance gains in extra hard and unseen scenarios, where conventional methods often fail.', 'abstract_zh': '文本到SQL旨在将自然语言问题转换为可执行的SQL查询。虽然之前的方法，例如骨架掩蔽选择，通过检索类似的训练示例来指导大规模语言模型（LLMs），并取得了很强的性能，但在缺乏此类示例的实际应用场景中却遇到了困难。为克服这一局限，我们提出了自我扩充的上下文学习框架，该框架结合细粒度示例选择进行文本到SQL（SAFE-SQL），这是一种新型框架，通过生成和筛选自我扩增示例来提升SQL生成的性能。SAFE-SQL 首先提示LLM生成与测试输入相关的多个文本到SQL示例。然后，SAFE-SQL 通过三个相关性评估过滤这些示例，构建高质量的上下文学习示例。使用自我生成的示例，SAFE-SQL 超过了此前的零样本和少样本文本到SQL框架，在执行准确性上表现更佳。值得注意的是，我们的方法在常规方法往往失败的额外困难和未知场景中还提供了额外的性能增益。', 'title_zh': 'SAFE-SQL：自我增强的上下文学习方法和细粒度示例选择用于文本到SQL转换'}
{'arxiv_id': 'arXiv:2502.11405', 'title': 'LayAlign: Enhancing Multilingual Reasoning in Large Language Models via Layer-Wise Adaptive Fusion and Alignment Strategy', 'authors': 'Zhiwen Ruan, Yixia Li, He Zhu, Longyue Wang, Weihua Luo, Kaifu Zhang, Yun Chen, Guanhua Chen', 'link': 'https://arxiv.org/abs/2502.11405', 'abstract': "Despite being pretrained on multilingual corpora, large language models (LLMs) exhibit suboptimal performance on low-resource languages. Recent approaches have leveraged multilingual encoders alongside LLMs by introducing trainable parameters connecting the two models. However, these methods typically focus on the encoder's output, overlooking valuable information from other layers. We propose \\aname (\\mname), a framework that integrates representations from all encoder layers, coupled with the \\attaname mechanism to enable layer-wise interaction between the LLM and the multilingual encoder. Extensive experiments on multilingual reasoning tasks, along with analyses of learned representations, show that our approach consistently outperforms existing baselines.", 'abstract_zh': '尽管大型语言模型（LLMs）是在多语种语料库上进行预训练的，但它们在低资源语言上的表现并不理想。近年来，一些方法通过引入连接LLM和多语种编码器的可训练参数，结合使用多语种编码器和LLM。然而，这些方法通常专注于编码器的输出，而忽略了其他层中包含的重要信息。我们提出了一种框架（\\aname / \\mname），该框架整合了所有编码器层的表示，并结合了\\attaname机制，以实现LLM与多语种编码器逐层之间的交互。在多种多语种推理任务的广泛实验以及对学习到的表示的分析中，我们的方法在多个基准方法上表现出了持续的超越。', 'title_zh': 'LayAlign：通过分层自适应融合和对齐策略增强大型语言模型的多语言推理能力'}
{'arxiv_id': 'arXiv:2502.11404', 'title': 'ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Chen', 'link': 'https://arxiv.org/abs/2502.11404', 'abstract': 'Tool learning has emerged as a crucial capability for large language models (LLMs) to solve complex real-world tasks through interaction with external tools. Existing approaches face significant challenges, including reliance on hand-crafted prompts, difficulty in multi-step planning, and lack of precise error diagnosis and reflection mechanisms. We propose ToolCoder, a novel framework that reformulates tool learning as a code generation task. Inspired by software engineering principles, ToolCoder transforms natural language queries into structured Python function scaffold and systematically breaks down tasks with descriptive comments, enabling LLMs to leverage coding paradigms for complex reasoning and planning. It then generates and executes function implementations to obtain final responses. Additionally, ToolCoder stores successfully executed functions in a repository to promote code reuse, while leveraging error traceback mechanisms for systematic debugging, optimizing both execution efficiency and robustness. Experiments demonstrate that ToolCoder achieves superior performance in task completion accuracy and execution reliability compared to existing approaches, establishing the effectiveness of code-centric approaches in tool learning.', 'abstract_zh': '工具学习已发展成为大型语言模型（LLMs）通过与外部工具的交互来解决复杂现实世界任务的关键能力。现有方法面临诸多挑战，包括对手工制作提示的依赖、多步骤规划的难度以及缺乏精确的错误诊断和反馈机制。我们提出了一种名为ToolCoder的新框架，将工具学习重新定义为一个代码生成任务。受软件工程原则的启发，ToolCoder将自然语言查询转换为结构化的Python函数框架，并通过带有描述性注释的系统分解任务，从而使LLMs能够利用编码范式进行复杂的推理和规划。随后，ToolCoder生成并执行函数实现，以获得最终响应。此外，ToolCoder将成功执行的函数存储在一个代码库中，促进代码重用，并利用错误跟踪机制进行系统的调试，从而提高执行效率和鲁棒性。实验结果表明，与现有方法相比，ToolCoder在任务完成精度和执行可靠性方面表现出更优的性能，这证实了以代码为中心的方法在工具学习中的有效性。', 'title_zh': 'ToolCoder：一种基于代码的强大语言模型工具学习框架'}
{'arxiv_id': 'arXiv:2502.11401', 'title': 'Following the Autoregressive Nature of LLM Embeddings via Compression and Alignment', 'authors': 'Jingcheng Deng, Zhongtao Jiang, Liang Pang, Liwei Chen, Kun Xu, Zihao Wei, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11401', 'abstract': "A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data.", 'abstract_zh': '一种新趋势是通过对比学习将大型语言模型（LLM）用作密集文本编码器。然而，由于LLM嵌入预测下一个词的概率分布，它们本质上是生成性和分布性的，这与对比学习的要求相冲突，对比学习要求嵌入能够捕捉全文语义并通过余弦相似性进行对齐。这种不一致阻碍了充分发挥LLM预训练能力的机会，导致学习效率低下。为应对这一问题，我们提出了一种新的对比学习方法AutoRegEmbed，该方法基于嵌入条件概率分布构建，并结合了两类核心任务：信息压缩和条件分布对齐。信息压缩任务将文本编码到嵌入空间中，确保嵌入向量能够捕捉全局语义。条件分布对齐任务则通过利用嵌入的条件分布对正样本嵌入进行对齐，并同时降低从文本嵌入中生成负样本的可能性，从而实现嵌入对齐和均一性。实验结果表明，我们的方法显著优于传统的对比学习方法，并且在使用相同数量的数据时，其性能与当前最先进模型相当。', 'title_zh': '通过压缩与对齐揭示大语言模型嵌入的自回归性质'}
{'arxiv_id': 'arXiv:2502.11400', 'title': 'Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs?', 'authors': 'Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Liwei Chen, Kun Xu, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.11400', 'abstract': 'Retrieval-augmented generation (RAG) systems often suffer from performance degradation when encountering noisy or irrelevant documents, driving researchers to develop sophisticated training strategies to enhance their robustness against such retrieval noise. However, as large language models (LLMs) continue to advance, the necessity of these complex training methods is increasingly questioned. In this paper, we systematically investigate whether complex robust training strategies remain necessary as model capacity grows. Through comprehensive experiments spanning multiple model architectures and parameter scales, we evaluate various document selection methods and adversarial training techniques across diverse datasets. Our extensive experiments consistently demonstrate that as models become more powerful, the performance gains brought by complex robust training methods drop off dramatically. We delve into the rationale and find that more powerful models inherently exhibit superior confidence calibration, better generalization across datasets (even when trained with randomly selected documents), and optimal attention mechanisms learned with simpler strategies. Our findings suggest that RAG systems can benefit from simpler architectures and training strategies as models become more powerful, enabling more scalable applications with minimal complexity.', 'abstract_zh': '检索增强生成（RAG）系统在遇到噪声或不相关文档时常常性能下降，促使研究人员开发复杂的训练策略以增强其对检索噪声的鲁棒性。然而，随着大规模语言模型（LLMs）的不断发展，这些复杂训练方法的必要性正受到越来越多的质疑。在本文中，我们系统地研究了模型容量增加时，是否还需要这些复杂的鲁棒训练策略。我们通过涵盖多种模型架构和参数规模的全面实验，评估了各种文档选择方法和对抗训练技术在不同数据集上的表现。广泛的实验结果一致表明，随着模型变得越来越强大，复杂鲁棒训练方法所带来的性能提升急剧下降。我们深入探讨了这一现象的原因，发现更强大的模型能够表现出更好的置信度校准，对不同数据集有更好的泛化能力（即使使用随机选择的文档进行训练），并且能够通过更简单的策略学习到最合适的注意力机制。我们的研究结果表明，随着模型变得越来越强大，RAG系统可以从更简单的架构和训练策略中受益，从而实现更易于扩展的应用，同时保持较低的复杂度。', 'title_zh': '重新审视鲁棒RAG：在强大语言模型时代，我们还需要复杂的鲁棒训练吗？'}
{'arxiv_id': 'arXiv:2502.11393', 'title': 'HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning', 'authors': 'Xiaoyuan Li, Moxin Li, Rui Men, Yichang Zhang, Keqin Bao, Wenjie Wang, Fuli Feng, Dayiheng Liu, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.11393', 'abstract': 'Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.', 'abstract_zh': '大型语言模型（LLMs）在常识推理方面表现出显著的能力；然而，某些问题的变化可能会引发错误的回答。这些模型是否真正理解常识知识，还是仅仅记忆了表达模式？为了探讨这一问题，我们首次对LLMs在常识推理方面的稳健性进行了全面评估。我们引入了HellaSwag-Pro，这是一个包含11,200个案例的大规模双语基准，通过设计和编译七种类型的问题变体而构建。为了构建这个基准，我们提出了一种两阶段方法来开发汉语HellaSwag，这是一个细粒度注释的数据集，涵盖56个类别，共有12,000个实例。我们在41个代表性LLM上进行了广泛的实验，发现这些LLM在常识推理方面远未表现出稳健性。此外，这种稳健性还会根据测试语言的不同而变化。本项工作建立了一个高质量的评估基准，广泛的实验为LLM在常识推理方面的社区提供了宝贵的见解。', 'title_zh': 'HellaSwag-Pro：一个大规模双语基准，用于评估LLMs在常识推理中鲁棒性的能力'}
{'arxiv_id': 'arXiv:2502.11306', 'title': 'Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation', 'authors': 'Hieu Nguyen, Zihao He, Shoumik Atul Gandre, Ujjwal Pasupulety, Sharanya Kumari Shivakumar, Kristina Lerman', 'link': 'https://arxiv.org/abs/2502.11306', 'abstract': 'Large language models (LLMs) often suffer from hallucination, generating factually incorrect or ungrounded content, which limits their reliability in high-stakes applications. A key factor contributing to hallucination is the use of hard labels during training, which enforce deterministic supervision, encourage overconfidence, and disregard the uncertainty inherent in natural language. To address this, we propose mitigating hallucination through knowledge distillation (KD), where a teacher model provides smoothed soft labels to a student model, reducing overconfidence and improving factual grounding. We apply KD during supervised finetuning on instructional data, evaluating its effectiveness across LLMs from different families. Experimental results on summarization benchmarks demonstrate that KD reduces hallucination compared to standard finetuning while preserving performance on general NLP tasks. These findings highlight KD as a promising approach for mitigating hallucination in LLMs and improving model reliability.', 'abstract_zh': '大型语言模型（LLMs）经常会发生幻觉现象，生成事实错误或无依据的内容，这限制了它们在高风险应用中的可靠性。导致幻觉的一个关键因素是在训练过程中使用硬标签，这种做法会施加确定性的监督，促进模型的过度自信，并忽略自然语言固有的不确定性。为了解决这一问题，我们提出通过知识蒸馏（KD）来减轻幻觉现象，其中教师模型为学生模型提供平滑的软标签，从而减少模型的过度自信并提高事实依据性。我们在不同家族的LLMs的指令性数据上进行监督微调，并评估知识蒸馏的有效性。在摘要基准测试上的实验结果表明，与标准微调相比，知识蒸馏能够减少幻觉现象，同时保持对一般自然语言处理任务的性能。这些发现突显了知识蒸馏作为一种减轻LLMs幻觉现象并提高模型可靠性的有前景的方法。', 'title_zh': '平滑 hallucination：通过平滑知识蒸馏减轻大语言模型的 hallucination'}
{'arxiv_id': 'arXiv:2502.11275', 'title': "Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest", 'authors': 'Letian Peng, Zilong Wang, Feng Yao, Jingbo Shang', 'link': 'https://arxiv.org/abs/2502.11275', 'abstract': "Massive high-quality data, both pre-training raw texts and post-training annotations, have been carefully prepared to incubate advanced large language models (LLMs). In contrast, for information extraction (IE), pre-training data, such as BIO-tagged sequences, are hard to scale up. We show that IE models can act as free riders on LLM resources by reframing next-token \\emph{prediction} into \\emph{extraction} for tokens already present in the context. Specifically, our proposed next tokens extraction (NTE) paradigm learns a versatile IE model, \\emph{Cuckoo}, with 102.6M extractive data converted from LLM's pre-training and post-training data. Under the few-shot setting, Cuckoo adapts effectively to traditional and complex instruction-following IE with better performance than existing pre-trained IE models. As a free rider, Cuckoo can naturally evolve with the ongoing advancements in LLM data preparation, benefiting from improvements in LLM training pipelines without additional manual effort.", 'abstract_zh': '大规模高质量数据，无论是预训练的原始文本还是后训练的标注数据，都已精心准备，用于孵化先进的大语言模型（LLMs）。相比之下，对于信息提取（IE），预训练数据，如带有BIO标记的序列，难以大规模扩展。我们展示了IE模型可以通过将下个token的“预测”重新框架为“提取”已有上下文中的token，从而作为大型语言模型（LLM）资源的“免费搭车者”。具体而言，我们提出的一种新的下一个token提取（NTE）范式学习了一个多功能的IE模型“Cuckoo”，其包含从LLM的预训练和后训练数据中转换而来的102.6M个提取数据。在少量样本的设置中，Cuckoo能够有效地适应传统和复杂的指令遵循信息提取任务，并在性能上优于现有的预训练IE模型。作为“免费搭车者”，Cuckoo能够自然地随着LLM数据准备的不断进步而发展，并从中受益于LLM训练管线的改进，而无需额外的手动努力。', 'title_zh': '鸽子：在大规模语言模型巢中孵化出的一个IE无关的寄生者'}
{'arxiv_id': 'arXiv:2502.11250', 'title': 'Uncertainty-Aware Step-wise Verification with Generative Reward Models', 'authors': 'Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal', 'link': 'https://arxiv.org/abs/2502.11250', 'abstract': "Complex multi-step reasoning tasks, such as solving mathematical problems, remain challenging for large language models (LLMs). While outcome supervision is commonly used, process supervision via process reward models (PRMs) provides intermediate rewards to verify step-wise correctness in solution traces. However, as proxies for human judgement, PRMs suffer from reliability issues, including susceptibility to reward hacking. In this work, we propose leveraging uncertainty quantification (UQ) to enhance the reliability of step-wise verification with generative reward models for mathematical reasoning tasks. We introduce CoT Entropy, a novel UQ method that outperforms existing approaches in quantifying a PRM's uncertainty in step-wise verification. Our results demonstrate that incorporating uncertainty estimates improves the robustness of judge-LM PRMs, leading to more reliable verification.", 'abstract_zh': '在解决数学问题等复杂的多步推理任务方面，大型语言模型（LLMs）仍然面临挑战。尽管结果监督是常用的方法，但通过过程奖励模型（PRMs）进行过程监督可以提供中间奖励，用于验证解题过程中的每一步是否正确。然而，作为人类判断的代理，PRMs 存在可靠性问题，包括易受奖励拐棍（reward hacking）的影响。在本工作中，我们提出利用不确定性量化（UQ）来增强生成奖励模型在数学推理任务中逐步验证的可靠性。我们引入了CoT熵（CoT Entropy），这是一种新颖的UQ方法，并且在量度PRM在逐步验证中的不确定性方面优于现有方法。我们的结果表明，整合不确定性估计能够提高判断-语言模型（judge-LM）PRMs的稳定性，从而实现更可靠的验证。', 'title_zh': '带有不确定性意识的逐级验证生成奖励模型方法'}
{'arxiv_id': 'arXiv:2502.11183', 'title': "Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls", 'authors': 'Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, Dong Yu', 'link': 'https://arxiv.org/abs/2502.11183', 'abstract': 'Recent advancements in tree search algorithms guided by verifiers have significantly enhanced the reasoning capabilities of large language models (LLMs), but at the cost of increased computational resources. In this work, we identify two key challenges contributing to this inefficiency: $\\textit{over-exploration}$ due to redundant states with semantically equivalent content, and $\\textit{under-exploration}$ caused by high variance in verifier scoring leading to frequent trajectory switching. To address these issues, we propose FETCH, an e$\\textbf{f}$fici$\\textbf{e}$nt $\\textbf{t}$ree sear$\\textbf{ch}$ framework, which is a flexible, plug-and-play system compatible with various tree search algorithms. Our framework mitigates over-exploration by merging semantically similar states using agglomerative clustering of text embeddings obtained from a fine-tuned SimCSE model. To tackle under-exploration, we enhance verifiers by incorporating temporal difference learning with adjusted $\\lambda$-returns during training to reduce variance, and employing a verifier ensemble to aggregate scores during inference. Experiments on GSM8K, GSM-Plus, and MATH datasets demonstrate that our methods significantly improve reasoning accuracy and computational efficiency across four different tree search algorithms, paving the way for more practical applications of LLM-based reasoning. The code will be released upon acceptance.', 'abstract_zh': '近期，由验证器引导的树搜索算法的发展显著增强了大型语言模型（LLMs）的推理能力，但代价是增加了计算资源的需求。在本文中，我们识别出导致这一低效率的两个关键挑战：由于具有语义等效内容的冗余状态导致的过度探索（over-exploration），以及由于验证器评分高方差导致的频繁轨迹切换引起的探索不足（under-exploration）。为了应对这些问题，我们提出了一种高效树搜索框架FETCH，这是一种灵活的、即插即用的系统，适用于各种树搜索算法。我们的框架通过使用从微调的SimCSE模型中获得的文本嵌入进行凝聚聚类，来缓解过度探索。为了解决探索不足的问题，我们通过引入基于时间差的学习并调整$\\lambda$-回报进行训练，来减少方差，并在推理时使用验证器集成来聚合分数。实验表明，我们的方法能显著提高不同树搜索算法的推理准确性和计算效率，为基于LLM的推理提供了更加实际的应用。代码将在接受后发布。', 'title_zh': '不要在细节中迷失：通过克服树搜索探索中的问题简化大语言模型推理'}
{'arxiv_id': 'arXiv:2502.11176', 'title': 'LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning', 'authors': 'Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See', 'link': 'https://arxiv.org/abs/2502.11176', 'abstract': "Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies.", 'abstract_zh': '现代大规模语言模型（LLMs）在处理推理任务时，会以显性和隐性的形式运用各种逻辑推理方式。合理利用这些推理范式对于提升LLMs的推理能力至关重要。本文采用探索性方法，通过引入一个受控评估环境来评估类比推理——一种基本的认知任务——并系统地在三种维度上参数化：模态（文本、视觉、符号）、难度（易于、中等、困难）以及任务格式（多项选择或自由文本生成）。我们分析了这些维度下归纳、 abduction（ abduction 在学术中可能被误解为一种推理类型，通常学术文献中使用 abduction 来表示假设演绎，但更严格的学术概念是“假设生成”，英文直接使用 abduction 更为准确） 和演绎推理管道的动态对比，并展示了我们的发现能够推广到更广泛的上下文学习任务中。此外，我们还探讨了假设选择、验证和优化等先进范式，揭示了它们在扩展LLMs推理中的逻辑推理方面的能力。这一探索性研究为通过系统逻辑推理策略增强LLMs推理能力的未来研究奠定了基础。', 'title_zh': 'LogiDynamics：解析大规模语言模型推理中逻辑推理的动力学过程'}
{'arxiv_id': 'arXiv:2502.11169', 'title': 'Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning', 'authors': 'Qingwen Lin, Boyan Xu, Zijian Li, Zhifeng Hao, Keli Zhang, Ruichu Cai', 'link': 'https://arxiv.org/abs/2502.11169', 'abstract': 'Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post-training methods. Without additional training, LLMs typically enhance their mathematical reasoning abilities through inference scaling methods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: \\emph{understanding}, \\emph{planning}, \\emph{reflection}, \\emph{coding}, and \\emph{summary}. Each subset is further constrained to a small number of predefined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action subsets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model.', 'abstract_zh': '近年来，长推理链（Long Chain-of-Thoughts，简称 CoTs）引起了广泛的关注，这些长推理链被认为能够提高大语言模型（Large Language Models，简称 LLMs）的推理能力。这要求现有的缺乏生成长推理链能力的 LLMs 通过后训练方法来获得这样的能力。在没有额外训练的情况下，LLMs 通常通过 MCTS 之类的推理放大方法增强其数学推理能力，但它们受到动作空间庞大和搜索策略低效的阻碍，使得生成长推理链变得困难。为了应对这一问题，我们提出了一种限制动作空间并引导生成长推理链的精炼搜索策略。在我们提出的约束蒙特卡洛树搜索（Constrained Monte Carlo Tree Search，简称 C-MCTS）框架中，我们限制从受限动作空间中选择动作，动作空间被划分为五个不相交的子集：\\emph{理解}、\\emph{规划}、\\emph{反思}、\\emph{编码}和\\emph{总结}。每个子集进一步被限制为预定义提示的较小数量，而不是让 LLMs 任意生成动作。此外，通过结合关于动作集的先验知识（例如，人类似的人工划分的动作子集部分顺序以及预训练的过程奖励模型），我们进一步精炼搜索策略。这些策略共同工作，大大减少了长推理链的庞大搜索空间。在数学推理基准测试中的广泛评估表明，在零样本设置下，我们的方法使得 7B 模型的推理能力超过了 72B 模型。', 'title_zh': '利用约束蒙特卡洛树搜索生成可靠链式推理链的数学推理文章'}
{'arxiv_id': 'arXiv:2502.11113', 'title': 'Valuable Hallucinations: Realizable Non-realistic Propositions', 'authors': 'Qiucheng Chen, Bo Wang', 'link': 'https://arxiv.org/abs/2502.11113', 'abstract': 'This paper introduces the first formal definition of valuable hallucinations in large language models (LLMs),addressing a gap in the existing this http URL provide a systematic definition and analysis of hallucination value,proposing methods for enhancing the value of this http URL contrast to previous works,which often treat hallucinations as a broad flaw,we focus on the potential value that certain types of hallucinations can offer in specific this http URL in LLMs generally refer to the generation of unfaithful, fabricated,inconsistent,or nonsensical this http URL than viewing all hallucinations negatively,this paper gives formal representations and manual judgments of "valuable hallucinations" and explores how realizable non-realistic propositions-ideas that are not currently true but could be achievable under certain conditions-can have constructive this http URL present experiments using the Qwen2.5 model and HalluQA dataset, employing ReAct prompting (which involves reasoning, confidence assessment, and answer verification) to control and optimize hallucinations. Our findings show that ReAct prompting results in a reduction in overall hallucinations and an increase in the proportion of valuable this http URL results demonstrate that systematically controlling hallucinations can improve their usefulness without compromising factual reliability.', 'abstract_zh': '本文首次为大型语言模型（LLM）中的有价值的幻觉提供了正式定义，填补了现有研究中的空白。本文系统地定义并分析了幻觉的价值，并提出了增强幻觉价值的方法。与以往的工作通常将幻觉视为一种广泛的缺陷不同，本文聚焦于某些类型幻觉在特定情况下可能提供的潜在价值。在LLM中，幻觉通常指的是生成不忠实、虚构、不一致或无意义的内容。本文不仅从消极的角度看待所有幻觉，还提供了“有价值的幻觉”的正式表示和人工判断，并探讨了不现实的命题（即当前不真实但可能在某些条件下实现的观念）如何具有建设性价值。本文使用Qwen2.5模型和HalluQA数据集进行了实验，采用ReAct提示（涉及推理、信心评估和答案验证）来控制和优化幻觉。实验结果表明，通过系统地控制幻觉可以提高其实用性，而不牺牲事实上的可靠性。', 'title_zh': '有价值的幻象：可实现的非现实命题'}
{'arxiv_id': 'arXiv:2502.11073', 'title': 'Demystifying Hateful Content: Leveraging Large Multimodal Models for Hateful Meme Detection with Explainable Decisions', 'authors': 'Ming Shan Hee, Roy Ka-Wei Lee', 'link': 'https://arxiv.org/abs/2502.11073', 'abstract': 'Hateful meme detection presents a significant challenge as a multimodal task due to the complexity of interpreting implicit hate messages and contextual cues within memes. Previous approaches have fine-tuned pre-trained vision-language models (PT-VLMs), leveraging the knowledge they gained during pre-training and their attention mechanisms to understand meme content. However, the reliance of these models on implicit knowledge and complex attention mechanisms renders their decisions difficult to explain, which is crucial for building trust in meme classification. In this paper, we introduce IntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for hateful meme classification with explainable decisions. IntMeme addresses the dual challenges of improving both accuracy and explainability in meme moderation. The framework uses LMMs to generate human-like, interpretive analyses of memes, providing deeper insights into multimodal content and context. Additionally, it uses independent encoding modules for both memes and their interpretations, which are then combined to enhance classification performance. Our approach addresses the opacity and misclassification issues associated with PT-VLMs, optimizing the use of LMMs for hateful meme detection. We demonstrate the effectiveness of IntMeme through comprehensive experiments across three datasets, showcasing its superiority over state-of-the-art models.', 'abstract_zh': '仇恨表情包检测是一项多模态任务，由于需要解释隐含的仇恨信息及其上下文线索，因此具有显著的挑战性。此前的方法通过对预训练的视觉-语言模型（PT-VLMs）进行微调，利用它们在预训练过程中获得的知识和注意力机制来理解表情包的内容。然而，这些模型过于依赖隐含知识和复杂的注意力机制，导致其决策难以解释，这在建立对表情包分类的信任方面至关重要。本论文提出了一种名为IntMeme的新型框架，利用大型多模态模型（LMMs）进行可解释的仇恨表情包分类。IntMeme解决了表情包审查双重挑战，即提高准确性和可解释性。该框架使用LMMs生成类似人类的、可解释的分析，以深入理解多模态内容和上下文。此外，它还针对表情包及其解释使用独立的编码模块，这些模块随后结合以提升分类性能。我们的方法解决了PT-VLMs的透明度低和分类错误等问题，优化了LMMs在仇恨表情包检测中的使用。通过在三个数据集上进行全面的实验，展示了IntMeme相较于最先进的模型的有效性。', 'title_zh': '揭开仇恨内容的面纱：利用可解释决策的大规模多模态模型进行仇恨 meme 识别'}
{'arxiv_id': 'arXiv:2502.11066', 'title': 'CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment', 'authors': 'Nura Aljaafari, Danilo S. Carvalho, André Freitas', 'link': 'https://arxiv.org/abs/2502.11066', 'abstract': "Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.", 'abstract_zh': '大型语言模型（LLMs）在组合泛化方面存在困难，限制了它们系统地将学到的组件组合起来解释新颖输入的能力。尽管通过架构修改、微调和数据增强可以改进组合性，但这些方法往往在适应性方面有限，面临可扩展性限制，或在实际数据上的效果呈递减趋势。为了解决这一问题，我们提出了一种名为CARMA的干预措施，该措施在保持微调后性能的同时，增强了LLMs中的组合推理的稳定性和鲁棒性。CARMA利用互信息正则化和逐层稳定性约束来减轻特征碎片化问题，确保结构化的表示在层间和层内保持一致。我们通过逆字典建模和情感分类任务来评估CARMA的影响，测量其对语义一致性、性能稳定性和对抗词汇扰动鲁棒性的影响。结果显示，CARMA减少了微调引入的变异，稳定了token表示，并改善了组合推理能力。尽管其效果在不同架构上有所差异，但CARMA的主要优势在于强化已学习的结构而不是引入新功能，使其成为一种可扩展的辅助方法。这些发现表明，将CARMA与微调结合使用可以改善组合泛化能力，同时保持特定任务的性能在LLMs中。', 'title_zh': 'CARMA：通过高级正则化和互信息对齐增强大型语言模型的组合性'}
{'arxiv_id': 'arXiv:2502.11095', 'title': 'A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions', 'authors': 'Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, Ling Chen', 'link': 'https://arxiv.org/abs/2502.11095', 'abstract': 'Mental health remains a critical global challenge, with increasing demand for accessible, effective interventions. Large language models (LLMs) offer promising solutions in psychotherapy by enhancing the assessment, diagnosis, and treatment of mental health conditions through dynamic, context-aware interactions. This survey provides a comprehensive overview of the current landscape of LLM applications in psychotherapy, highlighting the roles of LLMs in symptom detection, severity estimation, cognitive assessment, and therapeutic interventions. We present a novel conceptual taxonomy to organize the psychotherapy process into three core components: assessment, diagnosis, and treatment, and examine the challenges and advancements in each area. The survey also addresses key research gaps, including linguistic biases, limited disorder coverage, and underrepresented therapeutic models. Finally, we discuss future directions to integrate LLMs into a holistic, end-to-end psychotherapy framework, addressing the evolving nature of mental health conditions and fostering more inclusive, personalized care.', 'abstract_zh': '心理健康仍然是一个关键的全球性挑战，对易于获取、有效的干预措施的需求不断增加。大型语言模型（LLMs）通过动态、上下文感知的交互在心理治疗中提供了有希望的解决办法，从而增强对心理健康状况的评估、诊断和治疗。本文综述了当前LLM在心理治疗中的应用状况，强调了LLMs在症状检测、严重程度估计、认知评估和治疗方法中的作用。我们提出了一种新的概念分类法，将心理治疗过程划分为三个核心组成部分：评估、诊断和治疗，并在每个领域探讨了存在的挑战和进展。此外，本文还指出了关键的研究空白，包括语言偏见、疾病覆盖不足以及代表性不足的治疗方法。最后，我们讨论了将LLMs整合到一个全面的、端到端的心理治疗框架中的未来方向，以应对心理健康状况的不断变化，并促进更加包容和个性化的护理。', 'title_zh': '大型语言模型在心理治疗中的综述：现状与未来方向'}
{'arxiv_id': 'arXiv:2502.11078', 'title': 'DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling', 'authors': 'Aili Chen, Chengyu Du, Jiangjie Chen, Jinghan Xu, Yikai Zhang, Siyu Yuan, Zulong Chen, Liangyue Li, Yanghua Xiao', 'link': 'https://arxiv.org/abs/2502.11078', 'abstract': "To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.", 'abstract_zh': '为了推进推荐系统和个人行为预测等个性化应用，近期的研究越来越多地采用大语言模型（LLMs）来进行人性化的人格建模。在动态现实场景中，有效的人格建模需要利用流式行为数据不断优化用户的人格。然而，现有的方法——无论是重新生成人格还是以增量方式结合新行为——往往无法在人格质量和未来行为预测准确性上持续取得改进。为了解决这一问题，我们提出了一种新颖的动态人格建模方法DEEPER，该方法能够实现持续的人格优化。具体而言，我们通过迭代强化学习框架增强了模型的方向搜索能力，使其能够自动识别有效的更新方向，并利用用户行为与模型预测之间的差异来进行人格优化。针对10个领域的4800名用户进行的详尽实验表明，DEEPER在人格优化方面的性能优于基线方法，在四次更新循环中，平均行为预测误差减少了32.2%，相比性能最佳的基线方法提高了22.92%。', 'title_zh': '《更深入地了解您的用户：面向个性的动态个性模型优化》'}
{'arxiv_id': 'arXiv:2502.11008', 'title': 'CounterBench: A Benchmark for Counterfactuals Reasoning in Large Language Models', 'authors': 'Yuefei Chen, Vivek K.Singh, Jing Ma, Ruxiang Tang', 'link': 'https://arxiv.org/abs/2502.11008', 'abstract': "Counterfactual reasoning is widely recognized as one of the most challenging and intricate aspects of causality in artificial intelligence. In this paper, we evaluate the performance of large language models (LLMs) in counterfactual reasoning. In contrast to previous studies that primarily focus on commonsense causal reasoning, where LLMs often rely on prior knowledge for inference, we specifically assess their ability to perform counterfactual inference using a set of formal rules. To support this evaluation, we introduce a new benchmark dataset, CounterBench, comprising 1K counterfactual reasoning questions. The dataset is designed with varying levels of difficulty, diverse causal graph structures, distinct types of counterfactual questions, and multiple nonsensical name variants. Our experiments demonstrate that counterfactual reasoning poses a significant challenge for LLMs, with most models performing at levels comparable to random guessing. To enhance LLM's counterfactual reasoning ability, we propose a novel reasoning paradigm, CoIn, which guides LLMs through iterative reasoning and backtracking to systematically explore counterfactual solutions. Experimental results show that our method significantly improves LLM performance on counterfactual reasoning tasks and consistently enhances performance across different this http URL dataset is available at this https URL.", 'abstract_zh': '因果推理被广泛认为是人工智能中最具挑战性和复杂性的方面之一。本文评估了大型语言模型（LLMs）在因果推理中的表现。不同于以往主要集中在常识因果推理的研究，这些研究通常依赖于先验知识进行推理，我们特别评估了它们在使用一套形式化规则进行反事实推理的能力。为了支持这一评估，我们引入了一个新的基准数据集CounterBench，包含1000个反事实推理问题。该数据集设计了不同的难度级别、多样的因果图结构、不同类型的反事实问题，以及多种不合逻辑的名称变体。实验结果显示，反事实推理对LLMs构成了显著挑战，大多数模型的表现与随机猜测相似。为了增强LLMs的反事实推理能力，我们提出了一种新的推理范式CoIn，该范式引导LLMs通过迭代推理和回溯来系统地探索反事实解决方案。实验结果表明，我们的方法在反事实推理任务上显著提高了LLMs的表现，并在不同类型的CounterBench数据集上均表现更优。该数据集可在[此处提供链接]获取。', 'title_zh': 'CounterBench：大规模语言模型因果推理基准测试'}
{'arxiv_id': 'arXiv:2502.10996', 'title': 'RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation', 'authors': 'Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han', 'link': 'https://arxiv.org/abs/2502.10996', 'abstract': 'Retrieval-augmented language models often struggle with knowledge-intensive tasks due to inefficient retrieval, unstructured knowledge integration, and single-pass architectures. We present Retrieval-And-Structuring (RAS), a novel framework that dynamically constructs and reasons over query-specific knowledge graphs through iterative retrieval and structuring. RAS introduces four key technical innovations: (1) a themescoped retrieval mechanism that efficiently narrows the search space while maintaining retrieval quality, (2) an action planning module that determines knowledge needs and generates focused sub-queries, (3) a dynamic knowledge structuring approach that converts retrieved text into an evolving knowledge graph, and (4) a graph-augmented answering component that leverages the accumulated structured information. Our framework achieves state-of-the-art performance, surpassing leading baselines by 6.4% with open-source language models and 7.0% with proprietary models on seven knowledge-intensive generation datasets across all evaluation metrics. Detailed ablation studies verify the contribution of each technical component to the overall system performance.', 'abstract_zh': '检索增强语言模型在处理知识密集型任务时常常受限于检索效率低下、知识整合无序以及单次通过的架构。本文提出了一个名为检索与结构化（RAS）的新框架，该框架能够通过迭代检索和结构化动态构建和推理查询特定的知识图谱。RAS 引入了四项关键技术创新：（1）主题导向的检索机制，能够高效地缩小搜索空间并保持检索质量；（2）行动规划模块，确定知识需求并生成聚焦的子查询；（3）动态知识结构化方法，将检索到的文字信息转换为不断进化的知识图谱；（4）图增强回答组件，利用累积的结构化信息进行回答。该框架在七个知识密集型生成数据集的全部评估指标中均超越了开源语言模型和专有模型，分别提高了 6.4% 和 7.0% 的性能。详细的消融实验验证了每个技术组件对整体系统性能的贡献。', 'title_zh': 'RAS：检索与结构化方法在知识密集型大语言模型生成中的应用'}
{'arxiv_id': 'arXiv:2502.10995', 'title': 'Evaluating Large language models on Understanding Korean indirect Speech acts', 'authors': 'Youngeun Koo, Jiwoo Lee, Dojun Park, Seohyun Park, Sungeun Lee', 'link': 'https://arxiv.org/abs/2502.10995', 'abstract': "To accurately understand the intention of an utterance is crucial in conversational communication. As conversational artificial intelligence models are rapidly being developed and applied in various fields, it is important to evaluate the LLMs' capabilities of understanding the intentions of user's utterance. This study evaluates whether current LLMs can understand the intention of an utterance by considering the given conversational context, particularly in cases where the actual intention differs from the surface-leveled, literal intention of the sentence, i.e. indirect speech acts. Our findings reveal that Claude3-Opus outperformed the other competing models, with 71.94% in MCQ and 65% in OEQ, showing a clear advantage. In general, proprietary models exhibited relatively higher performance compared to open-source models. Nevertheless, no LLMs reached the level of human performance. Most LLMs, except for Claude3-Opus, demonstrated significantly lower performance in understanding indirect speech acts compared to direct speech acts, where the intention is explicitly revealed through the utterance. This study not only performs an overall pragmatic evaluation of each LLM's language use through the analysis of OEQ response patterns, but also emphasizes the necessity for further research to improve LLMs' understanding of indirect speech acts for more natural communication with humans.", 'abstract_zh': '准确理解对话中话语的意图对于会话交流至关重要。随着对话型人工智能模型在各个领域的迅速发展和应用，评估大语言模型（LLMs）理解用户话语意图的能力变得尤为重要。本研究通过考虑给定的对话背景，特别是当实际意图与句子的表面或字面意图不符时，即间接言语行为，评估当前LLMs是否能够理解话语意图。我们的研究成果显示，Claude3-Opus 在选择题（MCQ）中的得分为 71.94%，在开放式问题（OEQ）中的得分为 65%，表现出明显的优越性。总体而言，专用模型的表现相对较高，而开源模型则较低。不过，没有一个 LLM 达到了人类的性能水平。除了 Claude3-Opus 之外，大多数 LLMs 在理解间接言语行为方面表现显著低于直接言语行为，而在直接言语行为中意图通过话语明确揭示。本研究不仅通过分析 OEQ 的答题模式对每个 LLM 语言使用进行了全面的语用评估，还强调了为进一步改进 LLMs 对间接言语行为的理解，以实现更加自然的人机交流，而需要进一步研究的必要性。', 'title_zh': '评估大型语言模型在理解韩语间接言语行为方面的能力'}
{'arxiv_id': 'arXiv:2502.10993', 'title': 'RoseRAG: Robust Retrieval-augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization', 'authors': 'Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao, Haoyu Wang', 'link': 'https://arxiv.org/abs/2502.10993', 'abstract': 'Large language models (LLMs) have achieved impressive performance but face high computational costs and latency, limiting their deployment in resource-constrained settings. In contrast, small-scale LLMs (SLMs) are more efficient yet struggle to capture evolving real-world knowledge. Retrieval-augmented generation (RAG) helps by integrating external knowledge, but imperfect retrieval can introduce distracting noise that misleads SLMs. We propose RoseRAG, a robust RAG framework for SLMs via Margin-aware Preference Optimization. RoseRAG employs multi-turn prompting for detailed reasoning, rejection sampling for high-quality explanations, and contrastive preference selection to refine responses by maximizing the likelihood gap between preferred and non-preferred outputs. By integrating these components into a margin-aware optimization process, RoseRAG robustly enhances the accuracy and reliability of SLMs for RAG applications. Extensive experiments on three open-domain question answering benchmarks indicate that our innovative RoseRAG surpasses state-of-the-art baselines significantly.', 'abstract_zh': '大型语言模型（LLMs）在性能上取得了令人印象深刻的成果，但同时也面临着高昂的计算成本和延迟问题，这限制了它们在资源受限环境中的应用。相比之下，小型语言模型（SLMs）更加高效，但难以捕捉到不断变化的现实世界知识。检索增强生成（Rag）可以通过整合外部知识来帮助解决这一问题，然而不完善的检索可能会引入分散注意力的噪声，从而误导SLMs。\n\n为此，我们提出了RoseRAG，这是一种通过边缘意识偏好优化来增强SLMs的鲁棒Rag框架。RoseRAG采用多轮提示进行详细的推理，使用拒绝采样来生成高质量的解释，并通过对比偏好选择来优化响应，以最大化优选输出与非优选输出之间的似然性差距。通过将这些组件整合到一种边缘意识的优化过程中，RoseRAG能够稳健地提高SLMs在Rag应用中的准确性和可靠性。在对三个开放域问答基准上的广泛实验表明，我们的创新性RoseRAG显著超过了现有最先进的基线方法。', 'title_zh': 'RoseRAG：通过边际意识偏好优化在小规模LLM中实现稳健的检索增强生成'}
{'arxiv_id': 'arXiv:2502.10896', 'title': 'Developing Conversational Speech Systems for Robots to Detect Speech Biomarkers of Cognition in People Living with Dementia', 'authors': 'Rohith Perumandla, Young-Ho Bae, Diego Izaguirre, Esther Hwang, Andrew Murphy, Long-Jing Hsu, Selma Sabanovic, Casey C. Bennett', 'link': 'https://arxiv.org/abs/2502.10896', 'abstract': "This study presents the development and testing of a conversational speech system designed for robots to detect speech biomarkers indicative of cognitive impairments in people living with dementia (PLwD). The system integrates a backend Python WebSocket server and a central core module with a large language model (LLM) fine-tuned for dementia to process user input and generate robotic conversation responses in real-time in less than 1.5 seconds. The frontend user interface, a Progressive Web App (PWA), displays information and biomarker score graphs on a smartphone in real-time to human users (PLwD, caregivers, clinicians). Six speech biomarkers based on the existing literature - Altered Grammar, Pragmatic Impairments, Anomia, Disrupted Turn-Taking, Slurred Pronunciation, and Prosody Changes - were developed for the robot conversation system using two datasets, one that included conversations of PLwD with a human clinician (DementiaBank dataset) and one that included conversations of PLwD with a robot (Indiana dataset). We also created a composite speech biomarker that combined all six individual biomarkers into a single score. The speech system's performance was first evaluated on the DementiaBank dataset showing moderate correlation with MMSE scores, with the composite biomarker score outperforming individual biomarkers. Analysis of the Indiana dataset revealed higher and more variable biomarker scores, suggesting potential differences due to study populations (e.g. severity of dementia) and the conversational scenario (human-robot conversations are different from human-human). The findings underscore the need for further research on the impact of conversational scenarios on speech biomarkers and the potential clinical applications of robotic speech systems.", 'abstract_zh': '本文介绍了为患有认知障碍的老年人（People Living with Dementia, PLwD）设计的对话语音系统的发展和测试。该系统集成了一个后端Python WebSocket服务器和一个中央核心模块，该模块使用大型语言模型（Large Language Model, LLM）进行微调，以处理用户输入并实时生成机器人对话响应，响应时间少于1.5秒。前端用户界面是一个渐进式网络应用（Progressive Web App, PWA），它实时显示智能手机上的信息和生物标志物评分图，供人类用户（PLwD、护理人员、临床医生）查看。根据现有文献，我们开发了六个基于对话的生物标志物——语法规则改变、语用障碍、命名困难、说话轮替中断、发音不清和平调变化——并使用了两个数据集：一个包含PLwD与人类临床医生的对话（DementiaBank数据集），另一个包含PLwD与机器人之间的对话（Indiana数据集）。我们还创建了一个综合生物标志物，将六个单个生物标志物整合成单一得分。首先，在DementiaBank数据集上评估了语音系统的性能，结果显示该综合生物标志物得分与蒙特利尔认知评估量表（MMSE）得分之间存在适度相关性，综合生物标志物得分优于单一生物标志物得分。对Indiana数据集的分析显示更高且更可变的生物标志物得分，表明可能存在由于研究人群（例如，认知障碍的严重程度）和对话场景（人机对话与人对人对话不同）差异而导致的潜在差异。研究结果强调了进一步研究对话场景对语音生物标志物影响以及机器人语音系统临床应用的重要性。', 'title_zh': '为痴呆患者开发对话式语音系统以检测认知相关语音生物标志物'}
{'arxiv_id': 'arXiv:2502.10855', 'title': 'Towards Effective Extraction and Evaluation of Factual Claims', 'authors': 'Dasha Metropolitansky, Jonathan Larson', 'link': 'https://arxiv.org/abs/2502.10855', 'abstract': 'A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text.', 'abstract_zh': '检查大型语言模型（LLMs）生成的长文本内容时，常用的一种策略是提取可以独立验证的简单声明。由于不准确或不完整的声明会损害事实检查的结果，因此确保声明的质量至关重要。然而，缺乏标准化的评估框架阻碍了对声明提取方法的评估和比较。为了填补这一空白，我们提出了一种在事实检查背景下评估声明提取的方法框架，并提供了自动化、可扩展且可复制的应用此框架的方法，包括衡量范围和脱域的新方法。我们还引入了基于大型语言模型的声明提取方法——Claimify，并证明在我们的评估框架下，该方法优于现有方法。Claimify 的一个关键特性是其能够处理歧义，并且仅在对源文本的正确解释具有高度信心时才提取声明。', 'title_zh': '面向有效事实断言提取与评估的研究'}
{'arxiv_id': 'arXiv:2502.10835', 'title': 'Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models', 'authors': 'Zeping Yu, Yonatan Belinkov, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.10835', 'abstract': 'We investigate how large language models perform latent multi-hop reasoning in prompts like "Wolfgang Amadeus Mozart\'s mother\'s spouse is". To analyze this process, we introduce logit flow, an interpretability method that traces how logits propagate across layers and positions toward the final prediction. Using logit flow, we identify four distinct stages in single-hop knowledge prediction: (A) entity subject enrichment, (B) entity attribute extraction, (C) relation subject enrichment, and (D) relation attribute extraction. Extending this analysis to multi-hop reasoning, we find that failures often stem from the relation attribute extraction stage, where conflicting logits reduce prediction accuracy. To address this, we propose back attention, a novel mechanism that enables lower layers to leverage higher-layer hidden states from different positions during attention computation. With back attention, a 1-layer transformer achieves the performance of a 2-layer transformer. Applied to four LLMs, back attention improves accuracy on five reasoning datasets, demonstrating its effectiveness in enhancing latent multi-hop reasoning ability.', 'abstract_zh': '我们调查了大型语言模型在提示“Wolfgang Amadeus Mozart 的母亲的配偶是”这类问题中如何进行潜在的多跳推理。为了分析这一过程，我们引入了logit flow，一种可解释性方法，用于追踪logits如何在各层和位置间传播以产生最终预测。通过logit flow，我们识别出了单跳知识预测过程中的四个不同阶段：(A) 实体主语增强、(B) 实体属性抽取、(C) 关系主语增强、和(D) 关系属性抽取。将这一分析扩展到多跳推理中，我们发现失败通常源自于关系属性抽取阶段，其中冲突的logits降低了预测准确性。为了解决这一问题，我们提出了一种新颖的机制——反向注意（back attention），它使较低层能够在注意力计算过程中利用来自不同位置的较高层隐藏状态。通过反向注意机制，单层Transformer的性能达到了双层Transformer的水平。将反向注意应用于四种大型语言模型（LLMs）后，在五个推理数据集上提高了准确性，这表明其在增强潜在的多跳推理能力方面具有有效性。', 'title_zh': '回溯注意：理解并增强大型语言模型中的多跳推理'}
{'arxiv_id': 'arXiv:2502.10660', 'title': 'User Profile with Large Language Models: Construction, Updating, and Benchmarking', 'authors': 'Nusrat Jahan Prottasha, Md Kowsher, Hafijur Raman, Israt Jahan Anny, Prakash Bhat, Ivan Garibay, Ozlem Garibay', 'link': 'https://arxiv.org/abs/2502.10660', 'abstract': 'User profile modeling plays a key role in personalized systems, as it requires building accurate profiles and updating them with new information. In this paper, we present two high-quality open-source user profile datasets: one for profile construction and another for profile updating. These datasets offer a strong basis for evaluating user profile modeling techniques in dynamic settings. We also show a methodology that uses large language models (LLMs) to tackle both profile construction and updating. Our method uses a probabilistic framework to predict user profiles from input text, allowing for precise and context-aware profile generation. Our experiments demonstrate that models like Mistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the precision and recall of the generated profiles, and high evaluation scores confirm the effectiveness of our approach.', 'abstract_zh': '用户画像建模在个性化系统中起着关键作用，因为它要求建立准确的用户画像并根据新信息进行更新。在本文中，我们发布了两个高质量的开源用户画像数据集：一个用于用户画像的构建，另一个用于用户画像的更新。这些数据集为在动态环境中评估用户画像建模技术提供了坚实的基础。我们还展示了使用大规模语言模型（LLMs）来解决用户画像的构建与更新问题的方法。我们的方法采用概率框架从输入文本预测用户画像，从而实现精确且具有上下文感知的用户画像生成。实验结果表明，如Mistral-7b和Llama2-7b等模型在这两项任务中表现优异。大规模语言模型提高了生成用户画像的精确性和召回率，而高评价分数进一步验证了我们方法的有效性。', 'title_zh': '使用大规模语言模型构建、更新与基准测试用户画像'}
{'arxiv_id': 'arXiv:2502.10634', 'title': 'Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"', 'authors': 'Hao Sun, Chenming Tang, Gengyang Li, Yunfang Wu', 'link': 'https://arxiv.org/abs/2502.10634', 'abstract': 'By simply incorporating demonstrations into the context, in-context learning (ICL) enables large language models (LLMs) to yield awesome performance on many tasks. In this paper, we focus on passage-level long-context ICL for generation tasks and find that LLMs cannot learn the intrinsic relationships between the demonstration passage and the generation output. We conduct experiments with different LLMs on two typical generation tasks including single-document QA and distractor generation, demonstrating that even a completely meaningless demonstration passage with 1/4 length achieves much better performance than the original full passage. Analysis via attention score reveals that LLMs pay little attention to passages compared to other components in prompt and little attention flows from the passage to other parts of the demonstration, which further confirms our finding. Additionally, experiments on context compression indicate that compression approaches proven effective on other long-context tasks are not suitable for passage-level ICL, since simply using shorter meaningless demonstration passages has achieved competitive performance.', 'abstract_zh': '通过将演示直接融入上下文，上下文学习（Contextual Learning with Demonstrations, ICL）使大规模语言模型（LLMs）在许多任务上取得了出色的表现。本文重点探讨了用于生成任务的段落级长上下文ICL，并发现LLMs无法学习演示段落与生成输出之间的内在关系。我们分别在两个典型的生成任务——单文档问答（单文档QA）和干扰生成上，使用不同类型的LLMs进行实验，结果显示，即使是一个完全无意义的演示段落且长度仅为原始段落的1/4也比原版完整段落表现更好。通过注意力得分分析表明，LLMs对段落的关注度要低于其他提示组件的关注度，且从段落到演示其他部分的注意力流较低，这进一步证实了我们的发现。此外，上下文压缩实验显示，其他长上下文任务中证明有效的压缩方法不适用于段落级ICL，因为使用较短的无意义演示段落已经达到了可竞争的性能。', 'title_zh': '迷失在篇章中：段落级的即情境学习并不一定需要一个“篇章”'}
{'arxiv_id': 'arXiv:2502.10708', 'title': 'Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey', 'authors': 'Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen', 'link': 'https://arxiv.org/abs/2502.10708', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: this https URL, dedicated to documenting research in the field of specialized LLM.', 'abstract_zh': '大语言模型（LLMs）在自然语言理解、文本摘要和机器翻译等多种任务中展示了显著的成功。然而，它们的通用性质往往限制了其在需要专门知识的具体领域应用中的有效性，如医疗、化学或法律分析。为解决这一问题，研究人员探索了多种方法来增强LLMs，并通过集成特定领域的知识来增强其能力。在这篇综述中，我们提供了这些方法的全面概述，并将其归类为四个主要方面：动态知识注入、静态知识嵌入、模块化适配器和提示优化。每种方法都提供了独特的机制，以赋予LLMs领域专长，并在灵活性、可扩展性和效率之间进行权衡。我们讨论了这些方法如何使LLMs能够应对专门任务，比较了它们的优点和缺点，评估了特定领域LLMs与通用LLMs的性能，并指出了这一新兴领域中的挑战和机遇。对于希望深入了解这一领域的读者，我们还总结了常用的数据集和基准。为了使研究人员能够及时了解最新研究，我们维护了一个开源平台：[这个链接](this https URL)，专门用于记录该领域中专门LLMs的研究。', 'title_zh': '将领域特定知识注入大规模语言模型：一个综合调研'}
{'arxiv_id': 'arXiv:2502.12118', 'title': 'Scaling Test-Time Compute Without Verification or RL is Suboptimal', 'authors': 'Amrith Setlur, Nived Rajaraman, Sergey Levine, Aviral Kumar', 'link': 'https://arxiv.org/abs/2502.12118', 'abstract': 'Despite substantial advances in scaling test-time compute, an ongoing debate in the community is how it should be scaled up to enable continued and efficient improvements with scaling. There are largely two approaches: first, distilling successful search or thinking traces; and second, using verification (e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement learning (RL) and search algorithms. In this paper, we prove that finetuning LLMs with verifier-based (VB) methods based on RL or search is far superior to verifier-free (VF) approaches based on distilling or cloning search traces, given a fixed amount of compute/data budget. Further, we show that as we scale test-time compute (measured as the output token length) and training data, suboptimality of VF methods scales poorly compared to VB when the base pre-trained LLM presents a heterogeneous distribution over correct solution traces (e.g., different lengths, styles, etc.) and admits a non-sharp distribution over rewards on traces sampled from it. We formalize this condition using anti-concentration [Erdős, 1945]. This implies a stronger result that VB methods scale better asymptotically, with the performance gap between VB and VF methods widening as test-time budget grows. We corroborate our theory empirically on both didactic and math reasoning problems with 3/8/32B-sized pre-trained LLMs, where we find verification is crucial for scaling test-time compute.', 'abstract_zh': '尽管在测试时计算扩展方面取得了实质性的进步，社区中仍存在关于如何扩展的持续争论，以便能够持续并有效地进行扩展。主要存在两种方法：首先，提取成功的搜索或思考轨迹；其次，利用验证（例如0/1结果奖励、奖励模型或验证器）来引导强化学习（RL）和搜索算法。本文证明，在给定固定的计算/数据预算条件下，基于强化学习或搜索的验证器基（VB）方法对语言模型（LLM）进行微调远优于基于提取或克隆搜索轨迹的验证器自由（VF）方法。进一步地，我们展示了当基预训练语言模型对正确的解题轨迹呈现异质分布（例如不同长度、风格等），并且允许从其采样的轨迹具有非尖锐的奖励分布时，随着测试时计算（以输出令牌长度衡量）和训练数据的扩展，VF方法的次优性相对于VB方法扩展得更差。我们使用反集中性[埃德勒斯，1945]来正式化这一条件。这表明，在理论上，VB方法在渐进扩展性方面表现更好，随着测试时预算的增长，VB方法与VF方法之间的性能差距会不断扩大。我们通过使用3/8/32B规模的预训练语言模型对教学和数学推理问题进行实证验证，证明了验证对于扩展测试时计算的重要性。', 'title_zh': '不进行验证或强化学习的测试时计算缩放是次优的'}
{'arxiv_id': 'arXiv:2502.11919', 'title': 'From Text to Trust: Empowering AI-assisted Decision Making with Adaptive LLM-powered Analysis', 'authors': 'Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, Ziang Xiao, Ming Yin', 'link': 'https://arxiv.org/abs/2502.11919', 'abstract': "AI-assisted decision making becomes increasingly prevalent, yet individuals often fail to utilize AI-based decision aids appropriately especially when the AI explanations are absent, potentially as they do not %understand reflect on AI's decision recommendations critically. Large language models (LLMs), with their exceptional conversational and analytical capabilities, present great opportunities to enhance AI-assisted decision making in the absence of AI explanations by providing natural-language-based analysis of AI's decision recommendation, e.g., how each feature of a decision making task might contribute to the AI recommendation. In this paper, via a randomized experiment, we first show that presenting LLM-powered analysis of each task feature, either sequentially or concurrently, does not significantly improve people's AI-assisted decision performance. To enable decision makers to better leverage LLM-powered analysis, we then propose an algorithmic framework to characterize the effects of LLM-powered analysis on human decisions and dynamically decide which analysis to present. Our evaluation with human subjects shows that this approach effectively improves decision makers' appropriate reliance on AI in AI-assisted decision making.", 'abstract_zh': 'AI辅助决策日益普及，但个体常常未能恰当利用基于AI的决策辅助工具，尤其是在缺乏AI解释的情况下，这可能是因为他们未能批判性地反思AI的决策建议。大型语言模型（LLMs）凭借其出色的对话和分析能力，为在缺少AI解释的情况下提升AI辅助决策提供了巨大机会。通过提供自然语言分析，LLMs可以解释AI决策建议，例如，如何分析每个决策任务特征对AI建议的影响。在本论文中，通过随机实验，我们首先展示了以序时或并行方式呈现由LLM支持的每个任务特征的分析，并未显著提高人们在AI辅助决策中的表现。为了使决策者更好地利用LLM支持的分析，我们随后提出了一种算法框架来表征LLM支持的分析对人类决策的影响，并动态决定呈现哪种分析。我们的人类受试者评估表明，这种方法有效地提高了决策者在AI辅助决策中对AI的适当依赖。', 'title_zh': '从文本到信任：以适应性大模型助力的AI辅助决策分析'}
{'arxiv_id': 'arXiv:2502.11882', 'title': 'Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration', 'authors': 'Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen', 'link': 'https://arxiv.org/abs/2502.11882', 'abstract': "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in this https URL.", 'abstract_zh': '基于大型语言模型（LLMs）的代理在人机逐轮合作任务中表现出色，但在需要实时交互的多项实时任务中则遇到困难。延迟问题和推断人类策略变化的挑战阻碍了它们在没有明确指令的情况下做出自主决策的能力。通过使用当前独立的System 1和System 2方法进行实验，我们验证了在实时任务中使用双重过程理论（DPT）的必要性。我们提出了一种名为DPT-Agent的新语言代理框架，该框架结合了System 1和System 2，以实现高效的实时多项人机协作。DPT-Agent的System 1使用有限状态机（FSM）和代码作为策略，以实现快速、直观和可控的决策。DPT-Agent的System 2结合了心理理论（ToM）和异步反思，以推断人类意图并执行基于推理的自主决策。我们通过进一步与基于规则的代理和人类合作者进行实验，展示了DPT-Agent的有效性，表明其在主流LLM基础框架中具有显著改进。据我们所知，DPT-Agent是第一个能够在实时多重人机协作中实现自主成功的语言代理框架。DPT-Agent的代码可以在以下链接找到：https://...', 'title_zh': '利用双过程理论在语言代理框架中促进实时人机同步协作'}
{'arxiv_id': 'arXiv:2502.11645', 'title': 'Deviation Ratings: A General, Clone-Invariant Rating Method', 'authors': 'Luke Marris, Siqi Liu, Ian Gemp, Georgios Piliouras, Marc Lanctot', 'link': 'https://arxiv.org/abs/2502.11645', 'abstract': "Many real-world multi-agent or multi-task evaluation scenarios can be naturally modelled as normal-form games due to inherent strategic (adversarial, cooperative, and mixed motive) interactions. These strategic interactions may be agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or complementary (e.g. niche finding and specialization). In such a formulation, it is the strategies (actions, policies, agents, models, tasks, prompts, etc.) that are rated. However, the rating problem is complicated by redundancy and complexity of N-player strategic interactions. Repeated or similar strategies can distort ratings for those that counter or complement them. Previous work proposed ``clone invariant'' ratings to handle such redundancies, but this was limited to two-player zero-sum (i.e. strictly competitive) interactions. This work introduces the first N-player general-sum clone invariant rating, called deviation ratings, based on coarse correlated equilibria. The rating is explored on several domains including LLMs evaluation.", 'abstract_zh': '许多实际世界的多agent或多任务评估场景可以自然地被建模为标准形式博弈，因为它们固有的战略（对抗性、合作性和混合动机）交互。这些战略交互可能是代理性的（例如，玩家试图取胜），基础性的（例如，成本与质量），或者是互补性的（例如，利基发现和专门化）。在这样的表述中，是策略（动作、策略、agent、模型、任务、提示等）被评估的。然而，由于N-player策略交互的冗余性和复杂性，评级问题变得复杂化。重复或相似的策略可能会扭曲那些对抗或互补策略的评级。前人提出“克隆不变”评级来处理这种冗余性，但这仅限于二人零和博弈（即严格竞争性）的交互。本研究工作在此基础上引入了第一个N-player一般和博弈的克隆不变评级，称为偏离评级，基于粗略的关联均衡。该评级在包括LLM评估等多个领域进行了探索。', 'title_zh': '偏差评分：一种通用且克隆不变的评分方法'}
{'arxiv_id': 'arXiv:2502.11379', 'title': 'CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language Models', 'authors': 'Guanghao Zhou, Panjia Qiu, Mingyuan Fan, Cen Chen, Mingyuan Chu, Xin Zhang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.11379', 'abstract': 'Despite explicit alignment efforts for large language models (LLMs), they can still be exploited to trigger unintended behaviors, a phenomenon known as "jailbreaking." Current jailbreak attack methods mainly focus on discrete prompt manipulations targeting closed-source LLMs, relying on manually crafted prompt templates and persuasion rules. However, as the capabilities of open-source LLMs improve, ensuring their safety becomes increasingly crucial. In such an environment, the accessibility of model parameters and gradient information by potential attackers exacerbates the severity of jailbreak threats. To address this research gap, we propose a novel \\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization problem within the embedding space of masked language models. Through combinatorial optimization, we effectively balance the jailbreak attack success rate with semantic coherence. Extensive evaluations show that our method not only maintains semantic consistency but also surpasses state-of-the-art baselines in attack effectiveness. Additionally, by integrating semantically coherent jailbreak prompts generated by our method into widely used black-box methodologies, we observe a notable enhancement in their success rates when targeting closed-source commercial LLMs. This highlights the security threat posed by open-source LLMs to commercial counterparts. We will open-source our code if the paper is accepted.', 'abstract_zh': '尽管对大型语言模型（LLMs）进行了明确的对齐努力，它们仍然可能被利用以触发意外行为，这种现象被称为“jailbreaking”。目前的jailbreak攻击方法主要集中在针对闭源LLMs的离散提示操纵上，依赖于手动编写的提示模板和说服规则。然而，随着开源LLMs能力的提高，确保其安全变得越来越重要。在这种环境中，潜在攻击者可以访问模型参数和梯度信息，这加剧了jailbreak威胁的严重性。为了解决这一研究空白，我们提出了一种新型的\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak \\underline{A}ttack（CCJA）。我们将jailbreak攻击定义为隐藏语言模型嵌入空间内的优化问题。通过组合优化，我们有效地平衡了jailbreak攻击的成功率与语义一致性。广泛的实际评估表明，我们的方法不仅保持了语义一致性，还在攻击效果上超过了最先进的基线方法。此外，通过将我们方法生成的语义一致的jailbreak提示与广泛使用的黑盒方法相结合，我们发现当针对闭源商用LLMs时，其成功率显著提升。这突显了开源LLMs对商用对应物构成的网络安全威胁。如果论文被接受，我们将开源我们的代码。', 'title_zh': 'CCJA：上下文一致的对齐大型语言模型 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2502.11167', 'title': 'SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors', 'authors': 'Bohan Lyu, Siqiao Huang, Zichen Liang', 'link': 'https://arxiv.org/abs/2502.11167', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in code-related tasks, such as code understanding and code generation. However, an equally important yet underexplored question is whether LLMs can serve as general-purpose surrogate code executors, to predict the output and behavior of a program without actually running it. To systematically investigate this capability, we introduce SURGE, a comprehensive benchmark covering eight key aspects: multi-language programming tasks, competition-level programming problems, repository-level code analysis, high-cost scientific computing, time-complexity-intensive algorithms, buggy code analysis, programs dependent on specific compilers or execution environments, and formal mathematical proof verification. We evaluate multiple open-source and proprietary LLMs on SURGE and conduct a scaling study to analyze the impact of model size and training data scale on surrogate execution accuracy. Additionally, we categorize model prediction errors and explore potential areas for improvement. Our findings indicate that while LLMs can predict code execution results in certain cases, they exhibit limitations in general-purpose surrogate execution. This study provides empirical insights into the feasibility of using LLMs as surrogate code executors. Code and dataset are released at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在代码相关任务上展示了显著的能力，如代码理解和代码生成。然而，一个同样重要但尚未充分探索的问题是，LLMs 是否可以作为通用代理代码执行器，预测程序的输出和行为，而不需要实际运行代码。为了系统地研究这一能力，我们引入了SURGE，这是一个全面的基准测试，涵盖了八个关键方面：多语言编程任务、高水平编程竞赛问题、仓库级代码分析、高成本科学计算、时间复杂度密集型算法、有 bug 的代码分析、依赖特定编译器或执行环境的程序以及形式化数学证明验证。我们对SURGE评估了多种开源和专有LLMs，并进行了规模研究，分析了模型大小和训练数据规模对代理执行准确性的影响。此外，我们对模型预测错误进行了分类，并探索了改进的潜在领域。研究结果表明，虽然LLMs在某些情况下可以预测代码执行结果，但在通用代理执行方面仍存在局限性。本研究提供了将LLMs用作代理代码执行器的可行性经验洞察。代码和数据集可在以下链接获取：[此 https URL](此 https URL)。', 'title_zh': 'SURGE：大型语言模型作为通用代理代码执行器的潜力'}
{'arxiv_id': 'arXiv:2502.11021', 'title': 'Leveraging Uncertainty Estimation for Efficient LLM Routing', 'authors': 'Tuo Zhang, Asal Mehradfar, Dimitrios Dimitriadis, Salman Avestimehr', 'link': 'https://arxiv.org/abs/2502.11021', 'abstract': 'Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.', 'abstract_zh': '在边缘-云环境中部署大规模语言模型（LLMs）需要一种高效的路由策略来平衡成本和响应质量。传统的方法要么侧重于人类偏好的数据，要么侧重于基准数据集的准确性指标，但这些方法存在僵化性和主观性的问题。此外，现有的路由框架主要关注准确性和成本，而忽略了从人类偏好视角出发的响应质量。本文提出了一种新的框架——信心驱动的语言模型路由器（Confidence-Driven LLM Router），该框架利用不确定性估计来优化路由决策。为了全面评估路由性能，我们不仅评估系统的成本效率，还评估响应质量。特别是，我们引入了语言模型作为裁判的新方法来模拟人类评分偏好，并首次系统地评估了不同路由策略下的响应质量。在MT-Bench、GSM8K和MMLU上的广泛实验表明，我们的方法在保持成本效益的同时，超越了现有的先进路由方法，实现了更高的响应质量。', 'title_zh': '利用不确定性估计实现高效的LLM路由'}
{'arxiv_id': 'arXiv:2502.10563', 'title': 'Accelerating Unbiased LLM Evaluation via Synthetic Feedback', 'authors': 'Zhaoyi Zhou, Yuda Song, Andrea Zanette', 'link': 'https://arxiv.org/abs/2502.10563', 'abstract': 'When developing new large language models (LLMs), a key step is evaluating their final performance, often by computing the win-rate against a reference model based on external feedback. Human feedback is the gold standard, particularly for capturing nuanced qualities like coherence, readability, and alignment with human expectations. However, human evaluations are costly -- even for large tech companies -- and when conducted with active users, they may negatively impact user experience. A promising alternative is synthetic feedback, where evaluations are conducted by other large language models, including reward models. While this eliminates the need for costly human annotations, it introduces biases that may distort the evaluation process. In this work, we propose a statistically principled framework that integrates human and synthetic feedback to reduce reliance on human annotations while maintaining unbiased win-rate calculations. Our experiments demonstrate a reduction in human annotations by up to 12.2% with an off-the-shelf synthetic evaluator and up to 24.8% with a finetuned variant. Apart from being generalizable, scalable, and free of hyper-parameter tuning, our method offers predictable annotation savings, which can be estimated based on data-dependent characteristics.', 'abstract_zh': '在开发新的大规模语言模型（LLMs）时，一个关键步骤是评估其最终性能，通常是通过计算相对于参考模型的胜率来完成的，这通常基于外部反馈。人类反馈是黄金标准，特别是对于捕捉细腻的质量，如连贯性、可读性和与人类预期的对齐。然而，人类评估成本高昂——即使是大型科技公司也会发现进行此类评估代价不菲，并且由活跃用户参与时，可能会负面影响用户体验。一种有前景的替代方法是合成反馈，即通过其他大规模语言模型（包括奖励模型）来进行评估。虽然这种方法消除了对昂贵的人工注释的需求，但它可能引入偏差，从而扭曲评估过程。在此项工作中，我们提出了一种统计上具有原理性的框架，该框架结合了人类和合成反馈，以减少对人工注释的依赖性，同时保持无偏的胜率计算。我们的实验表明，使用开箱即用的合成评估器，人类注释可减少多达12.2%，而使用微调变体则可减少多达24.8%。除了通用性、可扩展性和无需超参数调整之外，我们的方法还提供了可预测的注释节省，这些节省可以基于数据相关特征进行估算。', 'title_zh': '通过合成反馈加速无偏大型语言模型评估'}
{'arxiv_id': 'arXiv:2502.10453', 'title': 'Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-based Approach', 'authors': 'Régnier Avice, Bernhard Haslhofer, Zhidong Li, Jianlong Zhou', 'link': 'https://arxiv.org/abs/2502.10453', 'abstract': 'Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based on Large Language Models (LLMs) to link attribution tags with well-defined knowledge graph concepts. We implemented this method in an end-to-end pipeline and conducted experiments showing that our approach outperforms baseline methods by up to 37.4% in F1-score across three publicly available attribution tag datasets. By integrating concept filtering and blocking procedures, we generate candidate sets containing five knowledge graph entities, achieving a recall of 93% without the need for labeled data. Additionally, we demonstrate that local LLM models can achieve F1-scores of 90%, comparable to remote models which achieve 94%. We also analyze the cost-performance trade-offs of various LLMs and prompt templates, showing that selecting the most cost-effective configuration can reduce costs by 90%, with only a 1% decrease in performance. Our method not only enhances attribution tag quality but also serves as a blueprint for fostering more reliable forensic evidence.', 'abstract_zh': '属性标签是现代加密资产取证的基础。然而，不一致或不正确的标签可能会误导调查，甚至导致错误指控。为了解决这一问题，我们提出了一种基于大规模语言模型（LLMs）的新型计算方法，用于将属性标签与定义明确的知识图谱概念相链接。我们在此方法中构建了一个端到端的工作流程，并进行了实验，结果显示，我们的方法在三个公开的属性标签数据集上的F1分数上比基线方法高出37.4%。通过整合概念过滤和阻止过程，我们生成了包含五个知识图谱实体的候选集，无需标记数据即可实现93%的召回率。此外，我们展示了本地LLM模型可以达到90%的F1分数，与94%的远程模型相当。我们还分析了各种LLM模型和提示模板的成本效益权衡，结果显示，选择最经济有效的配置可以降低90%的成本，同时性能减少1%。我们的方法不仅提高了属性标签的质量，还为促进更可靠的取证证据提供了蓝图。', 'title_zh': '基于语言模型的方法将加密资产归属标签链接到知识图谱实体'}
{'arxiv_id': 'arXiv:2502.10440', 'title': 'Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Ownership Verification with Reasoning', 'authors': 'Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang', 'link': 'https://arxiv.org/abs/2502.10440', 'abstract': "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning attacks. However, these methods require to alter the results of verification samples (\\eg, generating incorrect outputs), inevitably making them susceptible to anomaly detection and even introduce new security risks. To address these challenges, we propose \\name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \\name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) \\textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \\name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG.", 'abstract_zh': '大型语言模型（LLMs）通过检索增强生成（RAG）机制越来越多地集成到实际应用中，以补充其响应的内容，使其包含最新的及领域特定的知识。然而，用于RAG的知识库往往具有宝贵的且常常是专有的性质，这也带来了被对手未授权使用的风险。现有的可以泛化的水印技术通常涉及污染攻击来保护这些知识库，但这些方法需要改变验证样本的结果（例如，生成错误输出），这不可避免地使它们变得容易被异常检测发现，甚至引入新的安全风险。为解决这些挑战，我们提出了一种名为\\name{}的方法，用于对知识库实施“无害”的版权保护。不同于篡改LLM的最终输出，\\name{}在思维链（CoT）推理的空间中植入独特的验证行为，同时保持最终答案的正确性。我们的方法包含三个主要阶段：（1）**生成思维链（CoTs）**：对于每个验证问题，我们生成两个思维链，包括一个用于构建水印行为的目标思维链；（2）**优化水印短语和目标思维链**：我们优化这些内容，以在恶意LLM的黑盒设置下最小化检索错误，确保被水印的验证查询会激活目标思维链，而非水印的查询不被激活；（3）**所有权验证**：我们利用Wilcoxon秩和检验的配对测试来统计验证可疑LLM是否添加了受保护的知识库，这是通过比较其对水印验证查询和良性验证查询的响应来实现的。实验结果表明，\\name{}在保护知识库不受未授权使用的同时，能够保持RAG的完整性和性能。', 'title_zh': '通过所有权验证与推理实现检索增强语言模型知识库的版权保护'}
{'arxiv_id': 'arXiv:2502.10420', 'title': 'Position: Stop Acting Like Language Model Agents Are Normal Agents', 'authors': 'Elija Perrier, Michael Timothy Bennett', 'link': 'https://arxiv.org/abs/2502.10420', 'abstract': 'Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.', 'abstract_zh': '语言模型代理（LMAs）越来越多地被视为能够自主与人类和工具进行互动的实体。其设计和部署倾向于假定它们是正常代理，能够维持连贯的目标、在不同情境中适应，并且具有一定程度的意图性。这些假设对于工业、社会和政府领域的潜在应用至关重要。但是，LMAs 并不是正常的代理。它们继承了所基于的大规模语言模型（LLMs）所存在的结构问题：幻觉、逃逸、对齐不良和不可预测性。在本文中，我们认为不应该将 LMAs 视为正常代理，因为在这样做时会引发一系列问题，这些问题损害了其实用性和可信度。我们列举了 LMAs 本质上的代理病态。即使有外部记忆和工具的支持，它们仍然从本体论上是无状态、随机的、语义敏感且语言中介化的。这些病态动摇了 LMAs 的本体论属性，包括可识别性、连续性、持久性和一致性，从而质疑它们的代理权。因此，我们认为应在部署前、部署中和部署后测量 LMAs 的本体论属性，以便减轻这些病态的负面影响。', 'title_zh': '立场：停止将语言模型代理视为正常代理'}
