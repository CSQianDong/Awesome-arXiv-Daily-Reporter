# Formal Mathematical Reasoning: A New Frontier in AI 

**Title (ZH)**: 形式化数学推理：AI的新前沿 

**Authors**: Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song  

**Link**: [PDF](https://arxiv.org/pdf/2412.16075)  

**Abstract**: AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field. 

**Abstract (ZH)**: 人工智能在数学中的应用（AI4Math）不仅具有高度的学术吸引力，而且对于AI驱动的科学、工程及其他领域的发现也至关重要。在AI4Math领域所付出的大量努力与自然语言处理（NLP）中的技术相呼应，特别是在使用精心策划的文字形式数学数据集来训练大型语言模型方面。作为一种补充但尚未充分探索的途径，形数学推理基于形式系统（如证明助手），能够验证推理的正确性并提供自动反馈。在本文中，我们倡导形数学推理，并认为它对于推动AI4Math发展到更高水平是必不可少的。近年来，我们已经看到了利用AI进行形推理的稳步进展，包括核心任务如定理证明和自动形式化，以及新兴的应用如可验证的代码和硬件设计生成。然而，仍然存在许多挑战需要解决，以使AI真正掌握数学并产生更广泛的影响力。我们总结了现有的进展，讨论了开放的挑战，并展望了衡量未来成功的关键里程碑。在形数学推理这一转折点上，我们呼吁研究社区共同努力，推动该领域的根本性进步。 

---
# A Framework for Streaming Event-Log Prediction in Business Processes 

**Title (ZH)**: 业务流程中流式事件日志预测的框架 

**Authors**: Benedikt Bollig, Matthias Függer, Thomas Nowak  

**Link**: [PDF](https://arxiv.org/pdf/2412.16032)  

**Abstract**: We present a Python-based framework for event-log prediction in streaming mode, enabling predictions while data is being generated by a business process. The framework allows for easy integration of streaming algorithms, including language models like n-grams and LSTMs, and for combining these predictors using ensemble methods.
Using our framework, we conducted experiments on various well-known process-mining data sets and compared classical batch with streaming mode. Though, in batch mode, LSTMs generally achieve the best performance, there is often an n-gram whose accuracy comes very close. Combining basic models in ensemble methods can even outperform LSTMs. The value of basic models with respect to LSTMs becomes even more apparent in streaming mode, where LSTMs generally lack accuracy in the early stages of a prediction run, while basic methods make sensible predictions immediately. 

**Abstract (ZH)**: 我们提出了一种基于 Python 的框架，用于流式模式下的事件日志预测，允许在业务流程生成数据的同时进行预测。该框架允许轻松集成各种流式算法，包括 n-克gram 和 LSTMs 等语言模型，并通过集成方法将这些预测器进行组合。

使用该框架，我们对各种知名的流程挖掘数据集进行了实验，并将经典的批量模式与流式模式进行了比较。尽管在批量模式下，LSTMs 通常能实现最佳性能，但在很多情况下，某一 n-克gram 的准确度也非常接近。通过集成方法结合基础模型甚至可以超越 LSTMs。在流式模式下，基础模型相对于 LSTMs 的价值更加明显，因为在预测运行初期，LSTMs 通常缺乏准确性，而基础方法则能够立即做出合理的预测。 

---
# What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning 

**Title (ZH)**: 步级奖励模型在奖励什么？来自MCTS增强数学推理的反直觉发现 

**Authors**: Yiran Ma, Zui Chen, Tianqiao Liu, Mi Tian, Zhuo Liu, Zitao Liu, Weiqi Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.15904)  

**Abstract**: Step-level reward models (SRMs) can significantly enhance mathematical reasoning performance through process supervision or step-level preference alignment based on reinforcement learning. The performance of SRMs is pivotal, as they serve as critical guidelines, ensuring that each step in the reasoning process is aligned with desired outcomes. Recently, AlphaZero-like methods, where Monte Carlo Tree Search (MCTS) is employed for automatic step-level preference annotation, have proven particularly effective. However, the precise mechanisms behind the success of SRMs remain largely unexplored. To address this gap, this study delves into the counterintuitive aspects of SRMs, particularly focusing on MCTS-based approaches. Our findings reveal that the removal of natural language descriptions of thought processes has minimal impact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept at assessing the complex logical coherence present in mathematical language while having difficulty in natural language. These insights provide a nuanced understanding of the core elements that drive effective step-level reward modeling in mathematical reasoning. By shedding light on these mechanisms, this study offers valuable guidance for developing more efficient and streamlined SRMs, which can be achieved by focusing on the crucial parts of mathematical reasoning. 

**Abstract (ZH)**: 步骤级奖励模型（SRMs）可以通过基于强化学习的过程监督或步骤级偏好对齐显著提升数学推理性能。SRMs 的表现至关重要，因为它们作为关键指导方针，确保推理过程中的每一步都与期望的结果保持一致。最近，类似于 AlphaZero 的方法，通过蒙特卡洛树搜索（MCTS）进行自动步骤级偏好注解，已被证明特别有效。然而，SRMs 成功背后的精确机制仍然很大程度上未被探索。为填补这一空白，本研究深入探讨了 SRMs 的反直觉方面，特别是基于 MCTS 的方法。我们的研究发现，移除思维过程的自然语言描述对 SRMs 的有效性影响甚微。此外，我们展示了 SRMs 在评估数学语言中复杂的逻辑连贯性方面表现出色，但在处理自然语言时却存在困难。这些洞见为深入理解有效步骤级奖励建模的核心要素提供了精细的理解。通过阐明这些机制，本研究为开发更高效和简洁的 SRMs 提供了宝贵的指导，这可以通过专注于数学推理的关键部分来实现。 

---
# Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback 

**Title (ZH)**: Anything对齐：通过语言反馈训练多模态模型遵循指令 

**Authors**: Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, Mohan Wang, Josef Dai, Tianyi Qiu, Hua Xu, Dong Li, Weipeng Chen, Jun Song, Bo Zheng, Yaodong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15838)  

**Abstract**: Reinforcement learning from human feedback (RLHF) has proven effective in enhancing the instruction-following capabilities of large language models; however, it remains underexplored in the cross-modality domain. As the number of modalities increases, aligning all-modality models with human intentions -- such as instruction following -- becomes a pressing challenge. In this work, we make the first attempt to fine-tune all-modality models (i.e. input and output with any modality, also named any-to-any models) using human preference data across all modalities (including text, image, audio, and video), ensuring its behavior aligns with human intentions. This endeavor presents several challenges. First, there is no large-scale all-modality human preference data in existing open-source resources, as most datasets are limited to specific modalities, predominantly text and image. Secondly, the effectiveness of binary preferences in RLHF for post-training alignment in complex all-modality scenarios remains an unexplored area. Finally, there is a lack of a systematic framework to evaluate the capabilities of all-modality models, particularly regarding modality selection and synergy. To address these challenges, we propose the align-anything framework, which includes meticulously annotated 200k all-modality human preference data. Then, we introduce an alignment method that learns from unified language feedback, effectively capturing complex modality-specific human preferences and enhancing the model's instruction-following capabilities. Furthermore, to assess performance improvements in all-modality models after post-training alignment, we construct a challenging all-modality capability evaluation framework -- eval-anything. All data, models, and code frameworks have been open-sourced for the community. For more details, please refer to this https URL. 

**Abstract (ZH)**: 强化学习辅助人类反馈（Reinforcement Learning from Human Feedback, RLHF）已被证明能够增强大型语言模型的指令遵循能力；然而，在跨模态领域，其应用仍处于未被充分探索的状态。随着模态数量的增加，将所有模态模型与人类意图对齐（例如，指令遵循）成为了一个紧迫的挑战。在这项工作中，我们首次尝试使用包括文本、图像、音频和视频在内的所有模态的人类偏好数据对所有模态模型（即输入和输出可以是任何模态，也称为任意到任意模型）进行微调，以确保其行为与人类意图保持一致。这一尝试提出了几个挑战。首先，现有的开源资源中缺乏大规模的跨模态人类偏好数据，大多数数据集主要局限于特定的模态，主要是文本和图像模态。其次，二元偏好在复杂跨模态场景中的RLHF后训练对齐的有效性仍是一个未探索的领域。最后，缺乏系统的方法来评估所有模态模型的能力，特别是模态选择和协同效应。为了解决这些挑战，我们提出了一个名为“align-anything”的框架，其中包括了详细标注的20万个跨模态人类偏好数据。然后，我们介绍了一种基于统一语言反馈的学习方法，能够有效捕捉复杂模态特定的人类偏好，并增强模型的指令遵循能力。此外，为了评估所有模态模型在后训练对齐后的性能提升，我们构建了一个具有挑战性的跨模态能力评估框架——eval-anything。所有数据、模型和代码框架已经开源分享给社群。如需了解更多信息，请参见此处：[链接]。 

---
# AutoLife: Automatic Life Journaling with Smartphones and LLMs 

**Title (ZH)**: AutoLife：借助智能手机和大语言模型的自动生活记事 

**Authors**: Huatao Xu, Panron Tong, Mo Li, Mani Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2412.15714)  

**Abstract**: This paper introduces a novel mobile sensing application - life journaling - designed to generate semantic descriptions of users' daily lives. We present AutoLife, an automatic life journaling system based on commercial smartphones. AutoLife only inputs low-cost sensor data (without photos or audio) from smartphones and can automatically generate comprehensive life journals for users. To achieve this, we first derive time, motion, and location contexts from multimodal sensor data, and harness the zero-shot capabilities of Large Language Models (LLMs), enriched with commonsense knowledge about human lives, to interpret diverse contexts and generate life journals. To manage the task complexity and long sensing duration, a multilayer framework is proposed, which decomposes tasks and seamlessly integrates LLMs with other techniques for life journaling. This study establishes a real-life dataset as a benchmark and extensive experiment results demonstrate that AutoLife produces accurate and reliable life journals. 

**Abstract (ZH)**: 本文介绍了一种新颖的移动感知应用——生活日志记录，旨在生成用户的日常生活意义描述。我们提出了一种基于商用智能手机的自动化生活日志记录系统——AutoLife。AutoLife仅输入智能手机的低成本传感器数据（不包括照片或音频），即可自动生成全面的生活日志。为了实现这一目标，我们首先从多模态传感器数据中提取时间、运动和位置上下文，并利用大型语言模型（LLMs）的零样本能力，结合关于人类生活的常识知识，对多样化的上下文进行解释并生成生活日志。为了管理和减轻任务复杂性和长时间感知带来的挑战，我们提出了一种多层框架，该框架将任务分解，并无缝地将LLMs与其他技术集成用于生活日志记录。本研究建立了一个现实生活数据集作为基准，并通过广泛的实验结果证明，AutoLife可以生成准确可靠的日常生活日志。 

---
# Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration 

**Title (ZH)**: 协作健身房：一个促进和评估人类-代理协作的框架 

**Authors**: Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15701)  

**Abstract**: Recent advancements in language models (LMs) have sparked growing interest in developing LM agents. While fully autonomous agents could excel in many scenarios, numerous use cases inherently require them to collaborate with humans due to humans' latent preferences, domain expertise, or need for control. To facilitate the study of human-agent collaboration, we present Collaborative Gym (Co-Gym), a general framework enabling asynchronous, tripartite interaction among agents, humans, and task environments. We instantiate Co-Gym with three representative tasks in both simulated and real-world conditions, and propose an evaluation framework that assesses both the collaboration outcomes and processes. Our findings reveal that collaborative agents consistently outperform their fully autonomous counterparts in task performance within those delivered cases, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. However, our study also highlights significant challenges in developing collaborative agents, requiring advancements in core aspects of intelligence -- communication capabilities, situational awareness, and balancing autonomy and human control. 

**Abstract (ZH)**: 近年来语言模型（LMs）的发展激发了对LM代理的兴趣。虽然完全自主的代理在许多场景中表现卓越，但许多实际应用情境本质上需要它们与人类合作，这是因为人类潜在的偏好、领域专业知识或对控制的需求。为了促进人类-代理协作的研究，我们提出了协作健身房（Co-Gym），这是一种通用框架，可以支持代理、人类和任务环境之间的异步、三元交互。我们在模拟和现实世界条件下实例化了Co-Gym，并提出了评估框架，该框架评估了协作的成果和过程。我们的研究发现，在这些交付案例中，协作代理在任务绩效上始终优于完全自主的代理。在旅行规划任务中，真实用户评价时的胜率达到了86%；在表格分析任务中为74%；在相关工作研究中为66%。然而，我们的研究也揭示了开发协作代理面临的重大挑战，这要求在智能的核心方面（如沟通能力、情境意识以及平衡自主性和人类控制之间的关系）取得进步。 

---
# AIR: Unifying Individual and Cooperative Exploration in Collective Multi-Agent Reinforcement Learning 

**Title (ZH)**: AIR：整合个体与合作探索的集体多智能体强化学习 

**Authors**: Guangchong Zhou, Zeren Zhang, Guoliang Fan  

**Link**: [PDF](https://arxiv.org/pdf/2412.15700)  

**Abstract**: Exploration in cooperative multi-agent reinforcement learning (MARL) remains challenging for value-based agents due to the absence of an explicit policy. Existing approaches include individual exploration based on uncertainty towards the system and collective exploration through behavioral diversity among agents. However, the introduction of additional structures often leads to reduced training efficiency and infeasible integration of these methods. In this paper, we propose Adaptive exploration via Identity Recognition~(AIR), which consists of two adversarial components: a classifier that recognizes agent identities from their trajectories, and an action selector that adaptively adjusts the mode and degree of exploration. We theoretically prove that AIR can facilitate both individual and collective exploration during training, and experiments also demonstrate the efficiency and effectiveness of AIR across various tasks. 

**Abstract (ZH)**: 探索在协同多智能体强化学习（MARL）中的应用对于基于值的方法仍然具有挑战性，主要是由于缺乏明确的策略。现有方法包括基于系统不确定性进行个体探索以及通过智能体行为多样性进行集体探索。然而，引入额外结构往往会导致训练效率降低，并且难以将这些方法进行有效集成。在本文中，我们提出了一种基于身份识别的自适应探索（Adaptive Exploration via Identity Recognition，简称AIR）方法，该方法由两个对抗组件组成：用于从轨迹中识别智能体身份的分类器，以及能够自适应调整探索模式和程度的动作选择器。我们从理论上证明了AIR能够在训练过程中促进个体和集体探索，并通过实验验证了AIR在各种任务上的高效性和有效性。 

---
# Adaptable and Precise: Enterprise-Scenario LLM Function-Calling Capability Training Pipeline 

**Title (ZH)**: 适配性强且精确度高：企业场景中的大语言模型函数调用能力训练流程 

**Authors**: Guancheng Zeng, Wentao Ding, Beining Xu, Chi Zhang, Wenqiang Han, Gang Li, Jingjing Mo, Pengxu Qiu, Xinran Tao, Wang Tao, Haowen Hu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15660)  

**Abstract**: Enterprises possess a vast array of API assets scattered across various functions, forming the backbone of existing business processes. By leveraging these APIs as functional tools, enterprises can design diverse, scenario-specific agent applications, driven by on-premise function-calling models as the core engine. However, generic models often fail to meet enterprise requirements in terms of computational efficiency, output accuracy, and stability, necessitating scenario-specific adaptation. In this paper, we propose a training pipeline for function-calling capabilities tailored to real-world business scenarios. This pipeline includes the synthesis and augmentation of scenario-specific function-calling data, model fine-tuning, and performance evaluation and analysis. Using this pipeline, we generated 1,260 fully AI-generated samples and 1,035 augmented manually-labeled samples in digital HR agent scenario. The Qwen2.5-Coder-7B-Instruct model was employed as the base model and fine-tuned using the LoRA method on four GPUs with 24GB VRAM. Our fine-tuned model demonstrated outstanding performance in evaluations and practical applications, surpassing GPT-4 and GPT-4o in accuracy on the test set. These results validate the reliability of the proposed pipeline for training scenario-specific function-calling models. 

**Abstract (ZH)**: 企业拥有分布在各种功能领域的大量API资产，构成了现有业务流程的核心支撑。通过将这些API作为功能工具加以利用，企业可以设计出多样化的、基于特定场景的应用代理，以现场功能调用模型作为核心引擎。然而，通用模型在计算效率、输出精度和稳定性的方面往往无法满足企业的特定需求，因此需要进行场景特定的适应。在本文中，我们提出了一种针对实际业务场景的功能调用能力训练管道。该管道包括特定场景的功能调用数据合成与增强、模型微调以及性能评估与分析。通过该管道，我们在数字HR代理场景中生成了1,260个全AI生成的样本和1,035个手动标注后增强的样本。我们使用了Qwen2.5-Coder-7B-Instruct模型作为基础模型，并通过LoRA方法在四块带有24GB VRAM的GPU上进行了微调。我们的微调模型在评估和实际应用中表现出色，在测试集上超过了GPT-4和GPT-4o在准确性方面的表现。这些结果验证了所提出的针对特定场景的功能调用模型训练管道的有效性和可靠性。 

---
# Understanding Individual Agent Importance in Multi-Agent System via Counterfactual Reasoning 

**Title (ZH)**: 通过反事实推理理解多智能体系统中个体智能体的重要性 

**Authors**: Chen Jianming, Wang Yawen, Wang Junjie, Xie Xiaofei, Hu jun, Wang Qing, Xu Fanjiang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15619)  

**Abstract**: Explaining multi-agent systems (MAS) is urgent as these systems become increasingly prevalent in various applications. Previous work has proveided explanations for the actions or states of agents, yet falls short in understanding the black-boxed agent's importance within a MAS and the overall team strategy. To bridge this gap, we propose EMAI, a novel agent-level explanation approach that evaluates the individual agent's importance. Inspired by counterfactual reasoning, a larger change in reward caused by the randomized action of agent indicates its higher importance. We model it as a MARL problem to capture interactions across agents. Utilizing counterfactual reasoning, EMAI learns the masking agents to identify important agents. Specifically, we define the optimization function to minimize the reward difference before and after action randomization and introduce sparsity constraints to encourage the exploration of more action randomization of agents during training. The experimental results in seven multi-agent tasks demonstratee that EMAI achieves higher fidelity in explanations than baselines and provides more effective guidance in practical applications concerning understanding policies, launching attacks, and patching policies. 

**Abstract (ZH)**: 随着多代理系统（MAS）在各种应用中的逐渐普及，对其进行解释变得尤为紧迫。 previous研究虽然已经提供了对代理行为或状态的解释，但尚未充分理解黑盒代理在MAS中的重要性及其整体团队策略。为弥补这一差距，我们提出了EMAI，一种新颖的代理级别解释方法，用于评估个体代理的重要程度。受到反事实推理的启发，当代理的随机化动作导致奖励值更大变化时，表明其重要性更高。我们将其建模为一个多代理强化学习（MARL）问题，以捕捉代理间的交互。利用反事实推理，EMAI学习掩蔽代理以识别重要代理。具体地，我们定义优化函数以最小化随机化动作前后奖励值的差异，并引入稀疏性约束，鼓励在训练过程中更多地探索代理的随机化动作。在七个多代理任务中的实验结果表明，EMAI在解释的准确性上优于基线方法，并在理解策略、发起攻击和修补策略等实际应用方面提供了更有效的指导。 

---
# Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage 

**Title (ZH)**: 多模态代理调整：构建以VLearning Model为驱动的高效工具使用代理 

**Authors**: Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15606)  

**Abstract**: The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via \underline{T}rajectory \underline{T}uning on VLMs for \underline{T}ool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B}, which outperforms untrained VLMs by $20\%$, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. 

**Abstract (ZH)**: 大语言模型（LLMs）的发展推动了多模态代理（MMA）的应用，这些代理作为控制器调用外部工具，提供了解决实际任务的有效途径。本文提出了一种多模态代理调优方法，该方法可以自动生成多模态工具使用数据，并调优视图-语言模型（VLM）作为控制器，以增强工具使用推理能力。为了保证数据质量，我们使用GPT-4o mini模型生成查询、文件和轨迹，之后通过查询文件和轨迹验证器进行验证。基于数据合成管道，我们收集了包含20,000个任务和工具使用轨迹的MM-Traj数据集。然后，我们开发了T3-Agent，即通过轨迹调优VLM进行工具使用，利用MM-Traj数据集。在GTA和GAIA基准测试上的评估表明，T3-Agent能够持续提升两种流行的VLM：MiniCPM-V-8.5B和Qwen2-VL-7B的性能，未调优的VLM的性能提升了20%，证明了所提出的数据合成管道的有效性，从而为工具使用能力提供了高质量的数据。 

---
# Enhancing Large-scale UAV Route Planing with Global and Local Features via Reinforcement Graph Fusion 

**Title (ZH)**: 通过强化图融合利用全局和局部特征增强大规模无人机航线规划 

**Authors**: Tao Zhou, Kai Ye, Zeyu Shi, Jiajing Lin, Dejun Xu, Min Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15537)  

**Abstract**: Numerous remarkable advancements have been made in accuracy, speed, and parallelism for solving the Unmanned Aerial Vehicle Route Planing (UAVRP). However, existing UAVRP solvers face challenges when attempting to scale effectively and efficiently for larger instances. In this paper, we present a generalization framework that enables current UAVRP solvers to robustly extend their capabilities to larger instances, accommodating up to 10,000 points, using widely recognized test sets. The UAVRP under a large number of patrol points is a typical large-scale TSP this http URL proposed framework comprises three distinct steps. Firstly, we employ Delaunay triangulation to extract subgraphs from large instances while preserving global features. Secondly, we utilize an embedded TSP solver to obtain sub-results, followed by graph fusion. Finally, we implement a decoding strategy customizable to the user's requirements, resulting in high-quality solutions, complemented by a warming-up process for the heatmap. To demonstrate the flexibility of our approach, we integrate two representative TSP solvers into our framework and conduct a comprehensive comparative analysis against existing algorithms using large TSP benchmark datasets. The results unequivocally demonstrate that our framework efficiently scales existing TSP solvers to handle large instances and consistently outperforms state-of-the-art (SOTA) methods. Furthermore, since our proposed framework does not necessitate additional training or fine-tuning, we believe that its generality can significantly advance research on end-to-end UAVRP solvers, enabling the application of a broader range of methods to real-world scenarios. 

**Abstract (ZH)**: 在无人机路线规划（UAVRP）的求解方面，已在精度、速度和并行性等方面取得了许多显著的进展。然而，现有的UAVRP求解器在尝试有效地扩展到更大规模的问题实例时面临着挑战。本文提出了一种泛化框架，使得当前的UAVRP求解器能够稳健地扩展其能力以处理更大规模的实例，最多可处理10000个点，并使用广泛认可的测试集。在大量巡逻点的UAVRP问题中，这是一个典型的大型TSP问题。本文提出的框架包括三个不同的步骤。首先，我们采用德尔塔努雅三角剖分方法从大规模实例中提取子图并保持全局特征。其次，我们利用嵌入式TSP求解器获取子结果，并进行图融合。最后，我们实现一种可定制的解码策略，以满足用户的需求，从而获得高质量的解决方案，并通过热力图预热过程进行优化。为了展示我们方法的灵活性，我们将两种代表性TSP求解器集成到框架中，并使用大规模TSP基准数据集进行全面的对比分析，结果明确证明我们的框架能够高效地扩展现有的TSP求解器以处理大规模问题实例，并且在性能上始终优于最先进的（SOTA）方法。此外，由于我们提出的方法不需要额外的训练或微调，我们相信其泛化能力可以显著推动端到端UAVRP求解器的研究，使其能够应用更广泛的方法解决实际问题。 

---
# Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations 

**Title (ZH)**: 量化危险能力检测率：危险能力评估的理论模型 

**Authors**: Paolo Bova, Alessandro Di Stefano, Anh Han  

**Link**: [PDF](https://arxiv.org/pdf/2412.15433)  

**Abstract**: We present a quantitative model for tracking dangerous AI capabilities over time. Our goal is to help the policy and research community visualise how dangerous capability testing can give us an early warning about approaching AI risks. We first use the model to provide a novel introduction to dangerous capability testing and how this testing can directly inform policy. Decision makers in AI labs and government often set policy that is sensitive to the estimated danger of AI systems, and may wish to set policies that condition on the crossing of a set threshold for danger. The model helps us to reason about these policy choices. We then run simulations to illustrate how we might fail to test for dangerous capabilities. To summarise, failures in dangerous capability testing may manifest in two ways: higher bias in our estimates of AI danger, or larger lags in threshold monitoring. We highlight two drivers of these failure modes: uncertainty around dynamics in AI capabilities and competition between frontier AI labs. Effective AI policy demands that we address these failure modes and their drivers. Even if the optimal targeting of resources is challenging, we show how delays in testing can harm AI policy. We offer preliminary recommendations for building an effective testing ecosystem for dangerous capabilities and advise on a research agenda. 

**Abstract (ZH)**: 我们提出了一种定量模型，用于跟踪AI能力随时间发展而带来的潜在危险。我们的目标是帮助政策和研究社区可视化危险能力测试如何提供有关即将面临的AI风险的早期预警。首先，我们使用该模型提供了一种关于危险能力测试的新颖介绍，以及这种测试如何直接为政策制定提供信息。AI实验室和政府中的决策者经常根据对AI系统的估计危险性制定政策，可能希望制定基于危险程度达到一定阈值的政策。该模型帮助我们对这些政策选择进行深入思考。然后，我们通过仿真来说明我们可能无法测试危险能力的方式。总结来说，危险能力测试的失败可以表现为两种情况：对AI危险性估计的偏差增加，或达到危险阈值监测的延迟增大。我们强调了这两种失败模式的驱动因素：AI能力动态的不确定性以及前沿AI实验室之间的竞争。有效的AI政策要求我们解决这些失败模式及其驱动因素。即使最优资源分配存在挑战，我们展示了测试延迟如何损害AI政策。我们提供了有关构建有效危险能力测试生态系统的一些建议，并提出了研究议程。 

---
# Investigating Relational State Abstraction in Collaborative MARL 

**Title (ZH)**: 探究协作多智能体强化学习中的关系状态抽象 

**Authors**: Sharlin Utke, Jeremie Houssineau, Giovanni Montana  

**Link**: [PDF](https://arxiv.org/pdf/2412.15388)  

**Abstract**: This paper explores the impact of relational state abstraction on sample efficiency and performance in collaborative Multi-Agent Reinforcement Learning. The proposed abstraction is based on spatial relationships in environments where direct communication between agents is not allowed, leveraging the ubiquity of spatial reasoning in real-world multi-agent scenarios. We introduce MARC (Multi-Agent Relational Critic), a simple yet effective critic architecture incorporating spatial relational inductive biases by transforming the state into a spatial graph and processing it through a relational graph neural network. The performance of MARC is evaluated across six collaborative tasks, including a novel environment with heterogeneous agents. We conduct a comprehensive empirical analysis, comparing MARC against state-of-the-art MARL baselines, demonstrating improvements in both sample efficiency and asymptotic performance, as well as its potential for generalization. Our findings suggest that a minimal integration of spatial relational inductive biases as abstraction can yield substantial benefits without requiring complex designs or task-specific engineering. This work provides insights into the potential of relational state abstraction to address sample efficiency, a key challenge in MARL, offering a promising direction for developing more efficient algorithms in spatially complex environments. 

**Abstract (ZH)**: 本文探讨了关系状态抽象对协作多智能体强化学习中样本效率和性能的影响。所提出的关系抽象基于环境中智能体之间不允许直接通信的情况，利用了现实世界多智能体场景中空间推理的普适性。我们引入了MARC（多智能体关系评论家）架构，这是一种简单而有效的评论家架构，通过将状态转换为空间图并通过关系图神经网络处理来结合空间关系的归纳偏置。我们通过六个协作任务对MARC的性能进行了评估，包括一个新颖的包含异质智能体的环境。我们进行了全面的经验分析，将MARC与最新的多智能体强化学习（MARL）基线进行对比，展示了样本效率和渐近性能的改进，以及其潜在的泛化能力。研究结果表明，最小程度地整合空间关系的归纳偏置作为一种抽象方式，可以在不需复杂设计或任务特定工程的情况下带来显著益处。这项工作为理解关系状态抽象在解决MARL中的样本效率问题（这是MARL中的关键挑战）上的潜力提供了见解，并为在空间复杂环境中开发更高效的算法指明了有希望的方向。 

---
# Deep reinforcement learning with time-scale invariant memory 

**Title (ZH)**: 时间尺度不变记忆的深度强化学习 

**Authors**: Md Rysul Kabir, James Mochizuki-Freeman, Zoran Tiganj  

**Link**: [PDF](https://arxiv.org/pdf/2412.15292)  

**Abstract**: The ability to estimate temporal relationships is critical for both animals and artificial agents. Cognitive science and neuroscience provide remarkable insights into behavioral and neural aspects of temporal credit assignment. In particular, scale invariance of learning dynamics, observed in behavior and supported by neural data, is one of the key principles that governs animal perception: proportional rescaling of temporal relationships does not alter the overall learning efficiency. Here we integrate a computational neuroscience model of scale invariant memory into deep reinforcement learning (RL) agents. We first provide a theoretical analysis and then demonstrate through experiments that such agents can learn robustly across a wide range of temporal scales, unlike agents built with commonly used recurrent memory architectures such as LSTM. This result illustrates that incorporating computational principles from neuroscience and cognitive science into deep neural networks can enhance adaptability to complex temporal dynamics, mirroring some of the core properties of human learning. 

**Abstract (ZH)**: 估计时间关系的能力对动物和人工代理来说都是至关重要的。认知科学和神经科学为行为和神经层面的时间归因提供了深刻的理解。特别是，行为中观察到的学习动态的尺度不变性，得到了神经数据的支持，这是动物感知的关键原则之一：时间关系的成比例调整不会改变整体学习效率。本文将尺度不变记忆的计算神经科学模型整合到深度强化学习（RL）代理中。首先，我们进行理论分析，然后通过实验表明，这种代理能够在广泛的时序尺度上稳健地学习，这与使用常见循环记忆架构（如LSTM）构建的代理的表现不同。这一结果表明，在深度神经网络中融入来自神经科学和认知科学的计算原理可以增强对复杂时序动态的适应性，类似于人类学习的一些核心特性。 

---
# MotiF: Making Text Count in Image Animation with Motion Focal Loss 

**Title (ZH)**: MotiF：通过运动聚焦损失使文本在图像动画中具有重要性 

**Authors**: Shijie Wang, Samaneh Azadi, Rohit Girdhar, Saketh Rambhatla, Chen Sun, Xi Yin  

**Link**: [PDF](https://arxiv.org/pdf/2412.16153)  

**Abstract**: Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in this https URL. 

**Abstract (ZH)**: 从文本和图像生成视频（Text-Image-to-Video, TI2V）的目标是从给定的文本描述生成视频，这也被称为基于文本的图像动画。现有大多数方法在生成与文本提示高度一致的视频时遇到困难，尤其是在指定动作时。为克服这一限制，我们引入了MotiF，这是一种简单而有效的方法，它引导模型的学习集中在动作品质更高的区域，从而提高文本对齐和动作生成的效果。我们利用光流图生成动作热图，并根据动作强度调整损失函数。这个修改后的目标函数在改进性能方面表现出显著效果，并补充了现有利用运动先验作为模型输入的方法。此外，由于缺乏用于评估TI2V生成的多样基准，我们提出了TI2V Bench，这是一个包含320个图像-文本配对的数据集，用于稳健的评估。我们提出了一种人类评估协议，要求注释者在两段视频中选择总体偏好并提供理由。通过在TI2V Bench上的全面评估，MotiF 在与九个开源模型的对比中表现出色，平均偏好度达到72%。TI2V Bench 已在以下链接中发布：[提供的网站链接]。 

---
# Offline Reinforcement Learning for LLM Multi-Step Reasoning 

**Title (ZH)**: 离线强化学习在大语言模型多步推理中的应用 

**Authors**: Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, Yi Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16145)  

**Abstract**: Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline Reasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH) and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost performance during test time. 

**Abstract (ZH)**: 提高大型语言模型（LLMs）的多步推理能力对于快速适应复杂任务至关重要，这需要借助离线强化学习（RL）方法。尽管直接偏好优化（DPO）在使LLMs与人类偏好对齐方面显示出潜力，但对于多步推理任务而言，它并不理想。原因在于：（1）DPO依赖配对偏好数据，而这些数据对于多步推理任务来说并不容易获得；（2）DPO对所有词元采用统一处理，使其在多步推理任务中难以分配信用，而这种任务往往具有稀疏奖励。在本文中，我们提出了一种名为OREO（Offline Reasoning Optimization）的方法，这是一种用于增强LLM多步推理能力的离线RL方法。OREO借鉴了最大熵强化学习的先前研究，通过优化柔软贝尔曼方程联合学习策略模型和价值函数。我们从原则上说明了它减少了收集配对数据的需求，并能够更好地分配信用。实验证明，OREO在多步推理基准测试中（包括数学推理任务GSM8K和MATH以及体感智能体控制任务ALFWorld）超过了现有离线学习方法。当额外资源可用时，该方法可以扩展为多迭代框架。此外，学习得到的价值函数可以免费地指导树搜索，从而在测试时间进一步提高性能。 

---
# Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation 

**Title (ZH)**: 大规模语言模型能否混淆代码？大规模语言模型向汇编代码混淆分析的系统研究 

**Authors**: Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ndwula, Sriram Vema, Edward Raff, Manas Gaur  

**Link**: [PDF](https://arxiv.org/pdf/2412.16135)  

**Abstract**: Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk. The source code can be found at the following GitHub link: this https URL. 

**Abstract (ZH)**: 恶意软件作者常常使用代码混淆技术来使其恶意软件更难被检测。现有的代码混淆工具通常需要访问原始源代码（例如C++或Java），并且添加新的混淆技术是一个非平凡且劳动密集型的过程。在本研究中，我们提出以下问题：大型语言模型（LLMs）是否有可能生成新的混淆汇编代码？如果可以，这将对反病毒引擎构成风险，并可能增加攻击者创造新混淆模式的灵活性。我们通过开发包含MetamorphASM数据集（MAD）和三种代码混淆技术（死代码、寄存器替换和控制流改变）的MetamorphASM基准研究了这一可能性。MetamorphASM系统地使用MAD中的328,200个混淆汇编代码样本评估了LLMs生成和分析混淆代码的能力。我们发布了这个数据集，并分析了各种LLMs（例如GPT-3.5/4、GPT-4o-mini、Starcoder、CodeGemma、CodeLlama、CodeT5和LLaMA 3.1）生成混淆汇编代码的成功率。评估使用了已确立的信息论度量标准和人工审查以确保准确性，并为研究人员提供了研究和开发应对这一风险的修复措施的基础。源代码可在以下GitHub链接中找到：this https URL。 

---
# Convolutional Deep Operator Networks for Learning Nonlinear Focused Ultrasound Wave Propagation in Heterogeneous Spinal Cord Anatomy 

**Title (ZH)**: 适用于非线性异质脊髓解剖结构聚焦超声波传播学习的卷积深度算子网络 

**Authors**: Avisha Kumar, Xuzhe Zhi, Zan Ahmad, Minglang Yin, Amir Manbachi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16118)  

**Abstract**: Focused ultrasound (FUS) therapy is a promising tool for optimally targeted treatment of spinal cord injuries (SCI), offering submillimeter precision to enhance blood flow at injury sites while minimizing impact on surrounding tissues. However, its efficacy is highly sensitive to the placement of the ultrasound source, as the spinal cord's complex geometry and acoustic heterogeneity distort and attenuate the FUS signal. Current approaches rely on computer simulations to solve the governing wave propagation equations and compute patient-specific pressure maps using ultrasound images of the spinal cord anatomy. While accurate, these high-fidelity simulations are computationally intensive, taking up to hours to complete parameter sweeps, which is impractical for real-time surgical decision-making. To address this bottleneck, we propose a convolutional deep operator network (DeepONet) to rapidly predict FUS pressure fields in patient spinal cords. Unlike conventional neural networks, DeepONets are well equipped to approximate the solution operator of the parametric partial differential equations (PDEs) that govern the behavior of FUS waves with varying initial and boundary conditions (i.e., new transducer locations or spinal cord geometries) without requiring extensive simulations. Trained on simulated pressure maps across diverse patient anatomies, this surrogate model achieves real-time predictions with only a 2% loss on the test set, significantly accelerating the modeling of nonlinear physical systems in heterogeneous domains. By facilitating rapid parameter sweeps in surgical settings, this work provides a crucial step toward precise and individualized solutions in neurosurgical treatments. 

**Abstract (ZH)**: 聚焦超声（FUS）疗法是一种用于脊髓损伤（SCI）靶向治疗的有前景的工具，它可以提供亚毫米级精度，以增强损伤部位的血流，同时最大限度地减少对周围组织的影响。然而，其疗效高度依赖于超声源的位置，因为脊髓的复杂几何结构和声学异质性会导致FUS信号的畸变和衰减。当前的方法依赖于计算机模拟来求解波动传播的基本方程，并使用脊髓解剖结构的超声图像计算患者特异性压力图谱。尽管这种方法非常准确，但高保真模拟过程非常耗时，完成参数扫描可能需要数小时，这在实时手术决策中是不切实际的。为了解决这一瓶颈，我们提出了一种卷积深度操作网络（DeepONet）来快速预测患者脊髓中的FUS压力场。与传统的神经网络不同，DeepONets非常适合近似参数偏微分方程（PDEs）的解算子，这些方程描述了FUS波随初始和边界条件（如新的换能器位置或脊髓几何形状）变化的行为。通过在不同患者解剖结构的模拟压力图谱上进行训练，此代理模型可以在测试集上仅损失2%的情况下实现实时预测，极大地加速了异质域中非线性物理系统的建模。通过在手术环境中实现快速参数扫描，本研究为神经外科治疗中的精确和个性化解决方案提供了关键的一步。 

---
# Demystifying the Potential of ChatGPT-4 Vision for Construction Progress Monitoring 

**Title (ZH)**: 揭开ChatGPT-4视觉在施工进度监控潜在能力的神秘面纱 

**Authors**: Ahmet Bahaddin Ersoz  

**Link**: [PDF](https://arxiv.org/pdf/2412.16108)  

**Abstract**: The integration of Large Vision-Language Models (LVLMs) such as OpenAI's GPT-4 Vision into various sectors has marked a significant evolution in the field of artificial intelligence, particularly in the analysis and interpretation of visual data. This paper explores the practical application of GPT-4 Vision in the construction industry, focusing on its capabilities in monitoring and tracking the progress of construction projects. Utilizing high-resolution aerial imagery of construction sites, the study examines how GPT-4 Vision performs detailed scene analysis and tracks developmental changes over time. The findings demonstrate that while GPT-4 Vision is proficient in identifying construction stages, materials, and machinery, it faces challenges with precise object localization and segmentation. Despite these limitations, the potential for future advancements in this technology is considerable. This research not only highlights the current state and opportunities of using LVLMs in construction but also discusses future directions for enhancing the model's utility through domain-specific training and integration with other computer vision techniques and digital twins. 

**Abstract (ZH)**: 将大型视觉语言模型（LVLMs）如OpenAI的GPT-4 Vision集成到各个领域标志着人工智能领域的一次重要演变，特别是在视觉数据的分析和解释方面。本文探讨了GPT-4 Vision在建筑行业中的实际应用，重点关注其监测和追踪建筑工程进度的能力。利用高分辨率的建筑工地航空影像，研究考察了GPT-4 Vision如何进行详细的场景分析并追踪时间上的发展变化。研究结果表明，尽管GPT-4 Vision在识别建筑阶段、材料和机械设备方面表现出色，但它在精确物体定位和分割方面面临挑战。尽管存在这些限制，这类技术的未来发展潜力巨大。本研究不仅揭示了当前使用LVLMs在建筑行业中的状态和机遇，还讨论了通过领域特定训练和其他计算机视觉技术及数字孪生的集成来增强模型实用性的未来方向。 

---
# Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Time Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis 

**Title (ZH)**: 具有可解释性的多变量时间序列模式探索的AI：基于时间融合变换器和变分自编码器的潜在空间可视化分析在电力网格事件诊断中的应用 

**Authors**: Haowen Xu, Ali Boyaci, Jianming Lian, Aaron Wilson  

**Link**: [PDF](https://arxiv.org/pdf/2412.16098)  

**Abstract**: Detecting and analyzing complex patterns in multivariate time-series data is crucial for decision-making in urban and environmental system operations. However, challenges arise from the high dimensionality, intricate complexity, and interconnected nature of complex patterns, which hinder the understanding of their underlying physical processes. Existing AI methods often face limitations in interpretability, computational efficiency, and scalability, reducing their applicability in real-world scenarios. This paper proposes a novel visual analytics framework that integrates two generative AI models, Time Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex patterns into lower-dimensional latent spaces and visualize them in 2D using dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN. These visualizations, presented through coordinated and interactive views and tailored glyphs, enable intuitive exploration of complex multivariate temporal patterns, identifying patterns' similarities and uncover their potential correlations for a better interpretability of the AI outputs. The framework is demonstrated through a case study on power grid signal data, where it identifies multi-label grid event signatures, including faults and anomalies with diverse root causes. Additionally, novel metrics and visualizations are introduced to validate the models and evaluate the performance, efficiency, and consistency of latent maps generated by TFT and VAE under different configurations. These analyses provide actionable insights for model parameter tuning and reliability improvements. Comparative results highlight that TFT achieves shorter run times and superior scalability to diverse time-series data shapes compared to VAE. This work advances fault diagnosis in multivariate time series, fostering explainable AI to support critical system operations. 

**Abstract (ZH)**: 在城市和环境系统运营中，多变量时间序列数据中的复杂模式检测和分析是至关重要的。然而，高维度、复杂的内生结构以及模式间的相互作用带来了挑战，阻碍了对其实质物理过程的理解。现有的AI方法在解释性、计算效率和可扩展性方面常常存在局限性，限制了其在真实场景中的应用。本文提出了一种新的可视化分析框架，该框架结合了两种生成型AI模型——时间融合变换器（Time Fusion Transformer, TFT）和变分自编码器（Variational Autoencoders, VAE），将复杂的模式降低到低维隐空间，并通过主成分分析（PCA）、t-SNE和UMAP等降维技术并配合DBSCAN对其进行可视化。这些可视化以协调和交互式视图以及定制化的图标呈现，能够直观地探索复杂的多变量时间模式，识别模式的相似性并揭示潜在的相关性，从而提高对AI输出的解释性。该框架通过一个基于电力电网信号数据的案例研究得到了验证，识别出具有不同根本原因的多种电网事件签名，包括故障和异常事件。此外，本文引入了新的度量标准和可视化技术，用于验证模型并评估由TFT和VAE在不同配置下生成的隐空间模型的性能、效率和一致性。这些分析为模型参数调优和提高可靠性提供了可操作的见解。比较结果显示，TFT在运行时间和对各种时间序列数据形状的可扩展性方面优于VAE。这项工作推进了多变量时间序列中的故障诊断，促进了可解释AI的发展，以支持关键系统操作。 

---
# The Evolution of LLM Adoption in Industry Data Curation Practices 

**Title (ZH)**: 行业数据治理实践中大规模语言模型采用的演变 

**Authors**: Crystal Qian, Michael Xieyang Liu, Emily Reif, Grady Simon, Nada Hussein, Nathan Clement, James Wexler, Carrie J. Cai, Michael Terry, Minsuk Kahng  

**Link**: [PDF](https://arxiv.org/pdf/2412.16089)  

**Abstract**: As large language models (LLMs) grow increasingly adept at processing unstructured text data, they offer new opportunities to enhance data curation workflows. This paper explores the evolution of LLM adoption among practitioners at a large technology company, evaluating the impact of LLMs in data curation tasks through participants' perceptions, integration strategies, and reported usage scenarios. Through a series of surveys, interviews, and user studies, we provide a timely snapshot of how organizations are navigating a pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess LLM adoption in industry for development tasks (N=84), and facilitated expert interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we explored practitioners' current and anticipated LLM usage through a user study involving two LLM-based prototypes (N=12). While each study addressed distinct research goals, they revealed a broader narrative about evolving LLM usage in aggregate. We discovered an emerging shift in data understanding from heuristic-first, bottom-up approaches to insights-first, top-down workflows supported by LLMs. Furthermore, to respond to a more complex data landscape, data practitioners now supplement traditional subject-expert-created 'golden datasets' with LLM-generated 'silver' datasets and rigorously validated 'super golden' datasets curated by diverse experts. This research sheds light on the transformative role of LLMs in large-scale analysis of unstructured data and highlights opportunities for further tool development. 

**Abstract (ZH)**: 随着大型语言模型（LLM）在处理结构化文本数据方面的能力日益增强，它们为改进数据编目工作流提供了新的机会。本文探讨了大型科技公司从业者在LLM采用方面的演变，通过参与者感知、整合策略以及报告的使用场景评估LLM在数据编目任务中的影响。通过一系列调查、访谈和用户研究，我们提供了组织如何应对LLM演变关键时刻的及时快照。在2023年第二季度，我们进行了行业开发任务中LLM采用情况的调查（N=84），并在第三季度进行了专家访谈，评估不断变化的数据需求（N=10）。在2024年第二季度，我们通过涉及两个LLM原型的用户研究，探讨了从业者当前和预期的LLM使用情况（N=12）。尽管每项研究都有其特定的研究目标，但它们共同揭示了LLM使用模式的广泛变化。我们发现了一种从基于启发式、自下而上的方法向由LLM支持的自上而下的、以洞察为主的工作流的新兴转变。此外，为了应对更复杂的数据环境，数据从业者现在不仅使用传统的由学科专家创建的“金色数据集”，还补充使用由LLM生成的“银色数据集”和由多元专家严格验证的“超级金色数据集”。这项研究揭示了LLM在大规模分析非结构化数据中的转变作用，并指出了进一步开发工具的机会。 

---
# Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG 

**Title (ZH)**: 通过多智能体检索增强生成利用概念瓶颈实现可解释的放射学报告生成 

**Authors**: Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag  

**Link**: [PDF](https://arxiv.org/pdf/2412.16086)  

**Abstract**: Deep learning has advanced medical image classification, but interpretability challenges hinder its clinical adoption. This study enhances interpretability in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs) and a multi-agent Retrieval-Augmented Generation (RAG) system for report generation. By modeling relationships between visual features and clinical concepts, we create interpretable concept vectors that guide a multi-agent RAG system to generate radiology reports, enhancing clinical relevance, explainability, and transparency. Evaluation of the generated reports using an LLM-as-a-judge confirmed the interpretability and clinical utility of our model's outputs. On the COVID-QU dataset, our model achieved 81% classification accuracy and demonstrated robust report generation performance, with five key metrics ranging between 84% and 90%. This interpretable multi-agent framework bridges the gap between high-performance AI and the explainability required for reliable AI-driven CXR analysis in clinical settings. 

**Abstract (ZH)**: 深度学习在医学图像分类中取得了进展，但可解释性挑战阻碍了其在临床中的应用。本研究通过使用概念瓶颈模型（CBMs）和多代理检索增强生成（RAG）系统来增强胸片（CXR）分类的可解释性。通过建模视觉特征与临床概念之间的关系，我们创建了可解释的概念向量，这些向量指导多代理RAG系统生成放射学报告，从而增强临床相关性、可解释性和透明度。通过使用LLM作为评判者对生成的报告进行评估，证实了我们模型输出的可解释性和临床实用性。在COVID-QU数据集中，我们的模型实现了81%的分类准确率，并展示了稳健的报告生成性能，五项关键指标的范围在84%到90%之间。该可解释的多代理框架在临床环境中弥合了高性能AI与所需解释性的差距，适用于可靠的AI驱动胸片分析。 

---
# Label-Efficient Data Augmentation with Video Diffusion Models for Guidewire Segmentation in Cardiac Fluoroscopy 

**Title (ZH)**: 基于视频扩散模型的高效标签数据增强方法在心脏透视导线分割中的应用 

**Authors**: Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.16050)  

**Abstract**: The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. 

**Abstract (ZH)**: 在介入性心脏透视视频中准确分割导管对于计算机辅助导航任务至关重要。尽管深度学习方法在导管分割方面已经展现了高准确度和鲁棒性，但为了提高模型的通用性，仍需大量标注数据，这凸显了提升模型性能所需大量标注数据的重要性。为解决这一挑战，我们提出了引导分割帧一致性视频扩散模型（SF-VD），以生成大量标注的透视视频，从而扩充导管分割网络的训练数据。SF-VD 通过独立建模场景分布和运动分布来利用标注较少的视频。首先，通过生成根据指定输入掩码放置导管的2D透视图像来采样场景分布；然后，通过逐步生成后续帧来采样运动分布，并通过帧一致性策略确保帧与帧之间的一致性。一个引导分割机制进一步优化了这一过程，通过调整导管对比度，确保合成图像中导管可见性的多样性。在透视数据集上的评估证实了生成视频的质量优越，并展示了显著的导管分割改进。 

---
# Applying Predictive Analytics to Occupational Health and Safety in India 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，同时确保符合学术规范：

"应用预测分析提升印度的职业健康与安全"

或更详细的标题翻译为：

"利用预测分析技术提高印度职业健康与安全管理效果的研究" 

**Authors**: Ritwik Raj Saxena  

**Link**: [PDF](https://arxiv.org/pdf/2412.16038)  

**Abstract**: Predictive analytics is revolutionizing occupational health and safety (OHS). It offers evidence-based insights. These insights enable proactive risk management and informed, data-driven decision-making in organizational settings. This paper explores the key components of predictive analytics in OHS, beginning with data collection, management, and preparation, and moving through to advanced predictive modelling techniques. We emphasize the importance of data integrity through processes such as missing value imputation, anomaly detection, and feature engineering to ensure accurate model predictions. Risk prioritization identifies and ranks hazards across various factors, including employee behaviours, organizational policies, environmental conditions, and operational practices. We posit that insights derived from predictive models must be effectively interpreted and implemented. These insights guide organizations to focus on high-impact areas for accident prevention and resource optimization. The integration of predictive analytics in OHS brings notable benefits, including enhanced decision-making, greater operational efficiency, cost savings, and improved compliance with safety standards. We examine applications of predictive analytics in OHS in Indian settings. India has the largest workforce in the world, and the predominance of it is in the informal sector - a sector largely unprotected by the already inadequate OHS laws. Ethical considerations, data privacy concerns, and the risk of overdependence on predictive models are discussed. We conclude with a discussion on the potential for predictive analytics to create a data-oriented, adaptive approach to OHS in India. We posit that, using predictive analytics, India can develop high safety standards while traversing the complexities of its workforce setting. 

**Abstract (ZH)**: 预测分析正在彻底改变职业健康与安全（OHS）。它提供了基于证据的洞察，这些洞察能够实现前瞻性风险管理，并在组织环境中支持基于数据的决策制定。本文探讨了OHS中预测分析的关键组成部分，从数据的收集、管理和准备，到高级预测建模技术。我们强调通过诸如缺失值填充、异常检测和特征工程等过程来确保数据完整性的必要性，以确保准确的模型预测。风险优先级识别和排序各种因素（包括员工行为、组织政策、环境条件和操作实践）中的危害。我们认为，从预测模型中得出的洞察必须得到有效解释和实施。这些洞察指导组织集中精力于事故预防和资源优化的高影响区域。在OHS中集成预测分析带来了显著的利益，包括增强的决策制定能力、更高的操作效率、成本节约以及改进的安全标准符合性。我们研究了预测分析在印度环境中的应用。印度拥有世界上最大的劳动力，而大部分劳动力集中在 informal 部分——这个部门缺乏现有的尚不充分的职业健康与安全法律法规的保护。我们讨论了伦理考量、数据隐私问题以及过度依赖预测模型的风险。最后，我们探讨了预测分析在印度环境中创造以数据为导向、适应性强的OHS方法的潜力。我们提出，利用预测分析，印度可以在纵横交错的劳动力市场环境中发展高标准的安全规范。 

---
# The Only Way is Ethics: A Guide to Ethical Research with Large Language Models 

**Title (ZH)**: 唯有伦理可行：大型语言模型研究的伦理指南 

**Authors**: Eddie L. Ungless, Nikolas Vitsakis, Zeerak Talat, James Garforth, Björn Ross, Arno Onken, Atoosa Kasirzadeh, Alexandra Birch  

**Link**: [PDF](https://arxiv.org/pdf/2412.16022)  

**Abstract**: There is a significant body of work looking at the ethical considerations of large language models (LLMs): critiquing tools to measure performance and harms; proposing toolkits to aid in ideation; discussing the risks to workers; considering legislation around privacy and security etc. As yet there is no work that integrates these resources into a single practical guide that focuses on LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper', which we provide as an open and living resource for NLP practitioners, and those tasked with evaluating the ethical implications of others' work. Our goal is to translate ethics literature into concrete recommendations and provocations for thinking with clear first steps, aimed at computer scientists. 'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's and Don'ts, which we present also in this paper. We likewise identify useful toolkits to support ethical work. We refer the interested reader to the full LLM Ethics Whitepaper, which provides a succinct discussion of ethical considerations at each stage in a project lifecycle, as well as citations for the hundreds of papers from which we drew our recommendations. The present paper can be thought of as a pocket guide to conducting ethical research with LLMs. 

**Abstract (ZH)**: 关于大型语言模型（LLMs）的伦理考量，已有大量研究：批评用于评估性能和危害的工具；提出支持创意的工具包；讨论对工人的风险；考虑隐私和安全等方面的立法等。目前尚未有将这些资源整合为一个专注于LLMs的实用指南的工作；我们尝试实现这一雄心勃勃的目标。我们介绍了“LLM伦理白皮书”，作为NLP从业者和负责评估他人工作伦理影响者的开放和持续发展的资源。我们的目标是将伦理文献翻译成具体的建议和启发思考，旨在为计算机科学家提供清晰的第一步。在本文中，我们介绍了“LLM伦理白皮书”通过全面的文献回顾提炼出的清晰的“做”与“不做”清单。我们也指出了有帮助的工具包以支持伦理工作。有兴趣的读者可以参考完整的“LLM伦理白皮书”，该白皮书提供了项目生命周期每个阶段的伦理考虑的简洁讨论，并提供了我们从中汲取建议的数百篇论文的引用。本文可以视为一个便携指南，指导如何进行涉及LLMs的伦理研究。 

---
# Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition 

**Title (ZH)**: 选择你的解释：SHAP与GradCAM在人类活动识别中的比较 

**Authors**: Felix Tempel, Daniel Groos, Espen Alexander F. Ihlen, Lars Adde, Inga Strümke  

**Link**: [PDF](https://arxiv.org/pdf/2412.16003)  

**Abstract**: Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustworthy outcome predictions. However, users are often confused about which explanability method to choose for their specific use case. We present a comparative analysis of widely used explainability methods, Shapley Additive Explanations (SHAP) and Gradient-weighted Class Activation Mapping (GradCAM), within the domain of human activity recognition (HAR) utilizing graph convolutional networks (GCNs). By evaluating these methods on skeleton-based data from two real-world datasets, including a healthcare-critical cerebral palsy (CP) case, this study provides vital insights into both approaches' strengths, limitations, and differences, offering a roadmap for selecting the most appropriate explanation method based on specific models and applications. We quantitatively and quantitatively compare these methods, focusing on feature importance ranking, interpretability, and model sensitivity through perturbation experiments. While SHAP provides detailed input feature attribution, GradCAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements. Given the importance of XAI in enhancing trust and transparency in ML models, particularly in sensitive environments like healthcare, our research demonstrates how SHAP and GradCAM could complement each other to provide more interpretable and actionable model explanations. 

**Abstract (ZH)**: 使用可解释人工智能（XAI）技术解释机器学习（ML）模型已成为提升模型透明度和可信度的关键。在高风险领域如医疗保健中，理解模型决策尤为重要，以确保结果预测的道德性、合理性和可信性。然而，用户在选择合适的解释方法用于特定应用场景时往往会感到困惑。本文在利用图卷积网络（GCNs）进行人体活动识别（HAR）的背景下，对广泛使用的两种解释方法——Shapley加性解释（SHAP）和梯度加权类激活映射（GradCAM）——进行了比较分析。通过在两个真实数据集上的评估，包括一个涉及脑瘫（CP）的医疗保健关键案例，本文提供了这两种方法优缺点和差异的重要见解，为根据具体模型和应用场景选择最合适的解释方法提供了指南。我们通过扰动实验从定量和定性角度比较了这两种方法在特征重要性排名、可解释性和模型敏感性方面的表现。尽管SHAP提供了详细的输入特征归因，GradCAM则提供了更快、空间导向的解释，这两种方法在应用场景的需求下互补。鉴于XAI在增强ML模型在敏感环境如医疗保健中的信任和透明度方面的重要性，我们的研究成果证明了SHAP和GradCAM如何相互补充，提供更具解释性和行动性模型解释。 

---
# CNN-LSTM Hybrid Deep Learning Model for Remaining Useful Life Estimation 

**Title (ZH)**: 基于CNN-LSTM混合深度学习模型的剩余使用寿命估计 

**Authors**: Muthukumar G, Jyosna Philip  

**Link**: [PDF](https://arxiv.org/pdf/2412.15998)  

**Abstract**: Remaining Useful Life (RUL) of a component or a system is defined as the length from the current time to the end of the useful life. Accurate RUL estimation plays a crucial role in Predictive Maintenance applications. Traditional regression methods, both linear and non-linear, have struggled to achieve high accuracy in this domain. While Convolutional Neural Networks (CNNs) have shown improved accuracy, they often overlook the sequential nature of the data, relying instead on features derived from sliding windows. Since RUL prediction inherently involves multivariate time series analysis, robust sequence learning is essential. In this work, we propose a hybrid approach combining Convolutional Neural Networks with Long Short-Term Memory (LSTM) networks for RUL estimation. Although CNN-based LSTM models have been applied to sequence prediction tasks in financial forecasting, this is the first attempt to adopt this approach for RUL estimation in prognostics. In this approach, CNN is first employed to efficiently extract features from the data, followed by LSTM, which uses these extracted features to predict RUL. This method effectively leverages sensor sequence information, uncovering hidden patterns within the data, even under multiple operating conditions and fault scenarios. Our results demonstrate that the hybrid CNN-LSTM model achieves the highest accuracy, offering a superior score compared to the other methods. 

**Abstract (ZH)**: 该论文内容或标题的中文翻译如下，符合学术规范：

部件或系统剩余使用寿命（Remaining Useful Life, RUL）被定义为当前时间到使用寿命结束之间的时长。准确的RUL估计在预测性维护应用中起着至关重要的作用。传统回归方法，无论是线性的还是非线性的，在这一领域都难以达到高精度。虽然卷积神经网络（Convolutional Neural Networks, CNNs）展示了更高的精度，但它们往往忽略了数据的序列特性，而是依赖于滑动窗口提取的特征。由于RUL预测本质上涉及多元时间序列分析，因此稳健的序列学习变得至关重要。在本文中，我们提出了一种结合卷积神经网络和长短期记忆网络（Long Short-Term Memory, LSTM）的混合方法，用于RUL估计。尽管基于CNN的LSTM模型在金融预测中的序列预测任务中已经得到应用，但这是首次尝试将此方法应用于故障诊断中的RUL估计。在该方法中，首先使用CNN高效地从数据中提取特征，随后使用LSTM利用这些提取特征来预测RUL。该方法有效地利用了传感器序列信息，揭示了数据中的隐藏模式，即使在多种运行条件和故障场景下也是如此。我们的实验结果表明，混合CNN-LSTM模型达到了最高的准确度，其得分优于其他方法。 

---
# Data-Centric Improvements for Enhancing Multi-Modal Understanding in Spoken Conversation Modeling 

**Title (ZH)**: 以数据为中心的改进方法以提升多模态口语对话建模的理解能力 

**Authors**: Maximillian Chen, Ruoxi Sun, Sercan Ö. Arık  

**Link**: [PDF](https://arxiv.org/pdf/2412.15995)  

**Abstract**: Conversational assistants are increasingly popular across diverse real-world applications, highlighting the need for advanced multimodal speech modeling. Speech, as a natural mode of communication, encodes rich user-specific characteristics such as speaking rate and pitch, making it critical for effective interaction. Our work introduces a data-centric customization approach for efficiently enhancing multimodal understanding in conversational speech modeling. Central to our contributions is a novel multi-task learning paradigm that involves designing auxiliary tasks to utilize a small amount of speech data. Our approach achieves state-of-the-art performance on the Spoken-SQuAD benchmark, using only 10% of the training data with open-weight models, establishing a robust and efficient framework for audio-centric conversational modeling. We also introduce ASK-QA, the first dataset for multi-turn spoken dialogue with ambiguous user requests and dynamic evaluation inputs. Code and data forthcoming. 

**Abstract (ZH)**: 对话助理在各种实际应用中越来越受欢迎，突显了先进多模态语音建模的需求。语音作为一种自然的交流方式，编码了丰富的用户特定特征，如语速和音调，这对于有效的互动至关重要。我们的工作提出了数据驱动的定制方法，以高效地增强对话语音模型中的多模态理解。在我们的贡献中，核心在于提出了一种新颖的多任务学习范式，涉及设计辅助任务以利用少量的语音数据。我们的方法在Spoken-SQuAD基准测试中取得了最先进的性能，仅使用训练数据的10%就实现了这一目标，从而建立了适用于以音频为中心的对话建模的稳健且高效的框架。此外，我们还引入了ASK-QA数据集，这是首个包含模糊用户请求和动态评价输入的多轮语音对话数据集。代码和数据将于后续提供。 

---
# APIRL: Deep Reinforcement Learning for REST API Fuzzing 

**Title (ZH)**: APIRL：用于REST API 填充的深度强化学习 

**Authors**: Myles Foley, Sergio Maffeis  

**Link**: [PDF](https://arxiv.org/pdf/2412.15991)  

**Abstract**: REST APIs have become key components of web services. However, they often contain logic flaws resulting in server side errors or security vulnerabilities. HTTP requests are used as test cases to find and mitigate such issues. Existing methods to modify requests, including those using deep learning, suffer from limited performance and precision, relying on undirected search or making limited usage of the contextual information. In this paper we propose APIRL, a fully automated deep reinforcement learning tool for testing REST APIs. A key novelty of our approach is the use of feedback from a transformer module pre-trained on JSON-structured data, akin to that used in API responses. This allows APIRL to learn the subtleties relating to test outcomes, and generalise to unseen API endpoints. We show APIRL can find significantly more bugs than the state-of-the-art in real world REST APIs while minimising the number of required test cases. We also study how reward functions, and other key design choices, affect learnt policies in a thorough ablation study. 

**Abstract (ZH)**: REST APIs 已成为网络服务的关键组成部分。然而，它们常常包含逻辑缺陷，导致服务器端错误或安全漏洞。HTTP 请求被用作测试案例，以发现并缓解这些问题。现有的修改请求方法，包括那些使用深度学习的方法，都存在性能和精度有限的问题，往往依赖于无向搜索或仅部分利用上下文信息。本文提出了一种名为 APIRL 的完全自动化的深度强化学习工具，用于测试 REST APIs。我们方法的关键新颖之处在于，使用了预训练在 JSON 结构化数据上的变压器模块，类似于在 API 响应中使用的那种模块，这使得 APIRL 能够学习与测试结果相关的细微差异，并对未见过的 API 端点进行泛化。实验结果表明，与现有最先进的方法相比，APIRL 在真实世界的 REST API 中能够发现更多漏洞，同时大幅度减少所需的测试案例数量。我们还通过详尽的消融研究考察了奖励函数及其他关键设计选择对学习到的策略的影响。 

---
# Never Reset Again: A Mathematical Framework for Continual Inference in Recurrent Neural Networks 

**Title (ZH)**: 永不重置：循环神经网络中连续推理的数学框架 

**Authors**: Bojian Yin, Federico Corradi  

**Link**: [PDF](https://arxiv.org/pdf/2412.15983)  

**Abstract**: Recurrent Neural Networks (RNNs) are widely used for sequential processing but face fundamental limitations with continual inference due to state saturation, requiring disruptive hidden state resets. However, reset-based methods impose synchronization requirements with input boundaries and increase computational costs at inference. To address this, we propose an adaptive loss function that eliminates the need for resets during inference while preserving high accuracy over extended sequences. By combining cross-entropy and Kullback-Leibler divergence, the loss dynamically modulates the gradient based on input informativeness, allowing the network to differentiate meaningful data from noise and maintain stable representations over time. Experimental results demonstrate that our reset-free approach outperforms traditional reset-based methods when applied to a variety of RNNs, particularly in continual tasks, enhancing both the theoretical and practical capabilities of RNNs for streaming applications. 

**Abstract (ZH)**: 循环神经网络（RNNs）广泛应用于序列处理，但在持续推理过程中会面临由于状态饱和而产生的根本性限制，这需要进行破坏性的隐藏状态重置。然而，基于重置的方法需要与输入边界的同步要求，并且在推理过程中增加了计算成本。为了解决这一问题，我们提出了一种自适应损失函数，该函数在推理过程中消除了重置的需求，同时仍然能在长时间序列上保持高精度。通过结合交叉熵和相对熵（Kullback-Leibler散度），损失函数动态地根据输入的相关性调整梯度，使网络能够区分有意义的数据与噪声，并在时间上保持稳定的表示。实验结果表明，我们的无重置方法在应用于各种RNN时，特别是在持续任务中，能显著优于传统的基于重置的方法，从而增强了RNN在流式应用中理论和实际的能力。 

---
# Self-Supervised Radiograph Anatomical Region Classification -- How Clean Is Your Real-World Data? 

**Title (ZH)**: 自我监督X光解剖区域分类：您的实际数据有多干净？ 

**Authors**: Simon Langer, Jessica Ritter, Rickmer Braren, Daniel Rueckert, Paul Hager  

**Link**: [PDF](https://arxiv.org/pdf/2412.15967)  

**Abstract**: Modern deep learning-based clinical imaging workflows rely on accurate labels of the examined anatomical region. Knowing the anatomical region is required to select applicable downstream models and to effectively generate cohorts of high quality data for future medical and machine learning research efforts. However, this information may not be available in externally sourced data or generally contain data entry errors. To address this problem, we show the effectiveness of self-supervised methods such as SimCLR and BYOL as well as supervised contrastive deep learning methods in assigning one of 14 anatomical region classes in our in-house dataset of 48,434 skeletal radiographs. We achieve a strong linear evaluation accuracy of 96.6% with a single model and 97.7% using an ensemble approach. Furthermore, only a few labeled instances (1% of the training set) suffice to achieve an accuracy of 92.2%, enabling usage in low-label and thus low-resource scenarios. Our model can be used to correct data entry mistakes: a follow-up analysis of the test set errors of our best-performing single model by an expert radiologist identified 35% incorrect labels and 11% out-of-domain images. When accounted for, the radiograph anatomical region labelling performance increased -- without and with an ensemble, respectively -- to a theoretical accuracy of 98.0% and 98.8%. 

**Abstract (ZH)**: 基于现代深度学习的临床成像工作流依赖于被检解剖区域的准确标签。了解解剖区域对于选择适用的下游模型以及有效生成高质量数据的队列至关重要，这将对未来医学和机器学习研究产生积极影响。然而，这些信息可能在外购数据中不可用，或者通常包含数据录入错误。为解决这一问题，我们展示了自监督方法（如SimCLR和BYOL）以及监督对比深度学习方法的有效性，这些方法能够为我们的48,434张骨骼X射线图像分配14个解剖区域类之一。我们使用单一模型实现了96.6%的强线性评估准确率，使用集成方法实现了97.7%的准确率。此外，只需要少量标记样本（训练集的1%）即可实现92.2%的准确率，这使得在低标注量和因此低资源场景中能够进行使用。我们的模型可用于纠正数据录入错误：对表现最佳的单个模型测试集错误进行专家放射科医生的进一步分析发现，有35%的标签错误和11%的领域外图像。在这些错误被考虑在内时，骨骼X射线图像的解剖区域标注性能分别在单模型和集成方法的情况下达到了理论准确率的98.0%和98.8%。 

---
# From General to Specific: Tailoring Large Language Models for Personalized Healthcare 

**Title (ZH)**: 从通用到特定：为个性化医疗定制大型语言模型 

**Authors**: Ruize Shi, Hong Huang, Wei Zhou, Kehan Yin, Kai Zhao, Yun Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.15957)  

**Abstract**: The rapid development of large language models (LLMs) has transformed many industries, including healthcare. However, previous medical LLMs have largely focused on leveraging general medical knowledge to provide responses, without accounting for patient variability and lacking true personalization at the individual level. To address this, we propose a novel method called personalized medical language model (PMLM), which explores and optimizes personalized LLMs through recommendation systems and reinforcement learning (RL). Specifically, by utilizing self-informed and peer-informed personalization, PMLM captures changes in behaviors and preferences to design initial personalized prompts tailored to individual needs. We further refine these initial personalized prompts through RL, ultimately enhancing the precision of LLM guidance. Notably, the personalized prompt are hard prompt, which grants PMLM high adaptability and reusability, allowing it to directly leverage high-quality proprietary LLMs. We evaluate PMLM using real-world obstetrics and gynecology data, and the experimental results demonstrate that PMLM achieves personalized responses, and it provides more refined and individualized services, offering a potential way for personalized medical LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）的迅速发展已经改变了众多行业，医疗行业也不例外。然而，现有的医疗LLMs大多侧重于利用通用医疗知识来提供响应，而忽视了患者的个体差异，缺乏真正的个性化。为了解决这一问题，我们提出了一种新颖的方法，称为个性化医疗语言模型（PMLM），该方法通过推荐系统和强化学习（RL）探索和优化个性化LLMs。具体而言，PMLM利用自我引导和个人引导的个性化，捕捉行为和偏好变化，设计初始个性化提示以满足个人需求。我们进一步通过RL精炼这些初始个性化提示，最终提高LLMs指导的精确度。值得注意的是，个性化提示硬编码在PMLM中，赋予其高度的适应性和可重用性，使其可以直接利用高质量的专有LLMs。我们使用实际的妇产科数据评估了PMLM，并且实验结果表明，PMLM实现了个性化的响应，并提供了更加精细和个性化的服务，为个性化医疗LLMs的潜在应用提供了途径。 

---
# Trust Calibration in IDEs: Paving the Way for Widespread Adoption of AI Refactoring 

**Title (ZH)**: 面向IDE的可信度校准：铺平广泛采用AI重构技术的道路 

**Authors**: Markus Borg  

**Link**: [PDF](https://arxiv.org/pdf/2412.15948)  

**Abstract**: In the software industry, the drive to add new features often overshadows the need to improve existing code. Large Language Models (LLMs) offer a new approach to improving codebases at an unprecedented scale through AI-assisted refactoring. However, LLMs come with inherent risks such as braking changes and the introduction of security vulnerabilities. We advocate for encapsulating the interaction with the models in IDEs and validating refactoring attempts using trustworthy safeguards. However, equally important for the uptake of AI refactoring is research on trust development. In this position paper, we position our future work based on established models from research on human factors in automation. We outline action research within CodeScene on development of 1) novel LLM safeguards and 2) user interaction that conveys an appropriate level of trust. The industry collaboration enables large-scale repository analysis and A/B testing to continuously guide the design of our research interventions. 

**Abstract (ZH)**: 在软件行业中，增加新功能的趋势往往盖过了改进现有代码的需要。大规模语言模型（LLMs）提供了一种通过AI辅助重构来前所未有的大规模改进代码库的新方法。然而，LLMs 内在地伴随着风险，如引入错误和安全漏洞。我们建议将与模型的交互封装在集成开发环境（IDEs）中，并采用可信赖的安全防范措施验证重构尝试。同样重要的是，为了在AI重构方面获得采纳，还需进行信任发展方面的研究。在本文中，我们基于自动化领域的人因研究已有模型，定位我们未来的工作方向。我们详细介绍了在CodeScene中进行的行动研究，以开发1）新颖的LLM安全防范措施和2）传达适当信任水平的用户交互。行业合作使我们能够进行大规模代码库分析和A/B测试，以持续指导我们研究干预措施的设计。 

---
# Reframing Image Difference Captioning with BLIP2IDC and Synthetic Augmentation 

**Title (ZH)**: 用BLIP2IDC和合成增强重新 framing 图像差异描述istant 

**Authors**: Gautier Evennou, Antoine Chaffin, Vivien Chappelier, Ewa Kijak  

**Link**: [PDF](https://arxiv.org/pdf/2412.15939)  

**Abstract**: The rise of the generative models quality during the past years enabled the generation of edited variations of images at an important scale. To counter the harmful effects of such technology, the Image Difference Captioning (IDC) task aims to describe the differences between two images. While this task is successfully handled for simple 3D rendered images, it struggles on real-world images. The reason is twofold: the training data-scarcity, and the difficulty to capture fine-grained differences between complex images. To address those issues, we propose in this paper a simple yet effective framework to both adapt existing image captioning models to the IDC task and augment IDC datasets. We introduce BLIP2IDC, an adaptation of BLIP2 to the IDC task at low computational cost, and show it outperforms two-streams approaches by a significant margin on real-world IDC datasets. We also propose to use synthetic augmentation to improve the performance of IDC models in an agnostic fashion. We show that our synthetic augmentation strategy provides high quality data, leading to a challenging new dataset well-suited for IDC named Syned1. 

**Abstract (ZH)**: 近年来生成模型质量的提升使得大规模生成编辑后的图像变体成为可能。为了应对这种技术带来的负面影响，图像差异描述（Image Difference Captioning, IDC）任务旨在描述两张图像之间的差异。虽然该任务在简单的三维渲染图像上获得了成功处理，但在真实世界的图像上却遇到困难。原因主要有两个方面：训练数据稀缺，以及捕捉复杂图像之间的细微差异的难度。为了解决这些问题，本文提出了一种简单而有效的框架，用于将现有的图像描述模型适应到IDC任务，并增强IDC数据集。我们引入了BLIP2IDC，这是一种在低计算成本下适应IDC任务的BLIP2模型改编版本，并通过真实世界IDC数据集展示了它在显著程度上优于双流方法。我们还提出了一种合成增强策略，以无偏见的方式提升IDC模型的性能。我们表明，我们的合成增强策略提供了高质量的数据，从而创建了一个新的具有挑战性的IDC数据集Syned1，非常适合IDC任务。 

---
# Watertox: The Art of Simplicity in Universal Attacks A Cross-Model Framework for Robust Adversarial Generation 

**Title (ZH)**: Watertox：简约的艺术——一种通用攻击的跨模型框架，用于稳健的对抗样本生成 

**Authors**: Zhenghao Gao, Shengjie Xu, Meixi Chen, Fangyao Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.15924)  

**Abstract**: Contemporary adversarial attack methods face significant limitations in cross-model transferability and practical applicability. We present Watertox, an elegant adversarial attack framework achieving remarkable effectiveness through architectural diversity and precision-controlled perturbations. Our two-stage Fast Gradient Sign Method combines uniform baseline perturbations ($\epsilon_1 = 0.1$) with targeted enhancements ($\epsilon_2 = 0.4$). The framework leverages an ensemble of complementary architectures, from VGG to ConvNeXt, synthesizing diverse perspectives through an innovative voting mechanism. Against state-of-the-art architectures, Watertox reduces model accuracy from 70.6% to 16.0%, with zero-shot attacks achieving up to 98.8% accuracy reduction against unseen architectures. These results establish Watertox as a significant advancement in adversarial methodologies, with promising applications in visual security systems and CAPTCHA generation. 

**Abstract (ZH)**: 当代对抗攻击方法在跨模型可转移性和实用性方面面临重大局限。我们提出了一种名为Watertox的优雅对抗攻击框架，通过架构多样性和精确控制的扰动实现了显著的效果。该框架采用了两阶段快速梯度符号法，结合了均匀的基本扰动（$\epsilon_1 = 0.1$）和目标增强（$\epsilon_2 = 0.4$）。该框架利用了从VGG到ConvNeXt的一系列互补架构，通过创新的投票机制综合了多种视角。相较于最先进的架构，Watertox将模型准确率从70.6%降至16.0%，且零样本攻击在面对未见过的架构时，致准确率降低可达98.8%。这些结果确立了Watertox在对抗方法领域的重要进步，并在视觉安全系统和验证码生成方面具有广阔的应用前景。 

---
# Less is More: Towards Green Code Large Language Models via Unified Structural Pruning 

**Title (ZH)**: 更少即是更多：通过统一结构剪枝 toward 绿色代码大型语言模型 

**Authors**: Guang Yang, Yu Zhou, Xiangyu Zhang, Wei Cheng, Ke Liu, Xiang Chen, Terry Yue Zhuo, Taolue Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15921)  

**Abstract**: The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with lowdimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成性编码任务中的广泛应用引起了关注，这主要归因于其高计算需求和高能耗。与之前针对分类模型设计的针对低维度分类对数概率的结构剪枝方法不同，生成性代码LLMs产生高维度的令牌对数概率序列，这使得传统剪枝目标本身变得局限性很强。此外，现有的单一组件剪枝方法在应用于生成性代码LLMs时进一步限制了其有效性。为应对这些挑战，我们提出了Flab-Pruner，这是一种创新的统一结构剪枝方法，结合了词汇、层和前向网络（FFN）的剪枝。这种方法有效地减少了模型参数量，同时保持了性能。此外，我们还提出了一种针对编码任务的定制化代码指令数据策略，以提高裁剪模型性能恢复的效率。通过在三个最先进的代码LLMs上进行跨多个生成性编码任务的广泛评估，结果表明，Flab-Pruner 在裁剪掉22%的参数后仍然保留了97%的原始性能，并且在后续训练后实现了相同甚至更好的性能。裁剪后的模型在存储、GPU使用、计算效率和环境影响等方面表现出显著改进，同时保持了良好的鲁棒性。我们的研究为绿色软件工程提供了可持续的解决方案，并促进了LLMs在实际生成性编码智能应用中的高效部署。 

---
# Speedup Techniques for Switchable Temporal Plan Graph Optimization 

**Title (ZH)**: 切换时态计划图优化的加速技术 

**Authors**: He Jiang, Muhan Lin, Jiaoyang Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15908)  

**Abstract**: Multi-Agent Path Finding (MAPF) focuses on planning collision-free paths for multiple agents. However, during the execution of a MAPF plan, agents may encounter unexpected delays, which can lead to inefficiencies, deadlocks, or even collisions. To address these issues, the Switchable Temporal Plan Graph provides a framework for finding an acyclic Temporal Plan Graph with the minimum execution cost under delays, ensuring deadlock- and collision-free execution. Unfortunately, existing optimal algorithms, such as Mixed Integer Linear Programming and Graph-Based Switchable Edge Search (GSES), are often too slow for practical use. This paper introduces Improved GSES, which significantly accelerates GSES through four speedup techniques: stronger admissible heuristics, edge grouping, prioritized branching, and incremental implementation. Experiments conducted on four different map types with varying numbers of agents demonstrate that Improved GSES consistently achieves over twice the success rate of GSES and delivers up to a 30-fold speedup on instances where both methods successfully find solutions. 

**Abstract (ZH)**: 多智能体路径规划（Multi-Agent Path Finding, MAPF）关注于为多个智能体规划无碰撞路径。然而，在执行MAPF计划时，智能体可能会遇到意想不到的延迟，这可能导致效率低下、死锁或碰撞。为了解决这些问题，可切换时间计划图提供了一种在延迟下寻找无环时间计划图的框架，以确保死锁和碰撞的无冲突执行。遗憾的是，现有的最优算法，如混合整数线性规划（Mixed Integer Linear Programming, MILP）和基于图的可切换边搜索（Graph-Based Switchable Edge Search, GSES），往往对于实际应用来说速度过慢。本文介绍了改进的GSES，通过四种加速技术显著提高了GSES的速度：更强的容许启发式、边分组、优先分支和增量实现。在针对不同类型地图和不同数量智能体的实验中，改进的GSES在成功率上始终超过GSES两倍以上，并在两者都能找到解决方案的实例中实现了高达30倍的速度提升。 

---
# Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model 

**Title (ZH)**: 日语大规模胸部计算机断层扫描报告数据集的开发及高性能发现分类模型的研究 

**Authors**: Yosuke Yamagishi, Yuta Nakamura, Tomohiro Kikuchi, Yuki Sonoda, Hiroshi Hirakawa, Shintaro Kano, Satoshi Nakamura, Shouhei Hanaoka, Takeharu Yoshikawa, Osamu Abe  

**Link**: [PDF](https://arxiv.org/pdf/2412.15907)  

**Abstract**: Background: Recent advances in large language models highlight the need for high-quality multilingual medical datasets. While Japan leads globally in CT scanner deployment and utilization, the lack of large-scale Japanese radiology datasets has hindered the development of specialized language models for medical imaging analysis. Objective: To develop a comprehensive Japanese CT report dataset through machine translation and establish a specialized language model for structured finding classification. Additionally, to create a rigorously validated evaluation dataset through expert radiologist review. Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304 patients) into Japanese using GPT-4o mini. The training dataset consisted of 22,778 machine-translated reports, while the validation dataset included 150 radiologist-revised reports. We developed CT-BERT-JPN based on "tohoku-nlp/bert-base-japanese-v3" architecture for extracting 18 structured findings from Japanese radiology reports. Results: Translation metrics showed strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in 11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1 scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in four conditions. Conclusions: Our study establishes a robust Japanese CT report dataset and demonstrates the effectiveness of a specialized language model for structured finding classification. The hybrid approach of machine translation and expert validation enables the creation of large-scale medical datasets while maintaining high quality. 

**Abstract (ZH)**: 背景：近期大型语言模型的发展突显了高质量多语言医学数据集的迫切需求。虽然日本在CT扫描的部署和使用方面领先全球，但由于缺乏大规模的日语放射学数据集，阻碍了专门用于医学影像分析的语言模型的发展。目标：通过机器翻译开发一个全面的日语CT报告数据集，并建立一个专门用于结构化发现分类的语言模型。此外，通过专家放射科医师的审查创建一个严格验证的评估数据集。方法：我们使用GPT-4o mini将CT-RATE数据集（来自21,304名患者的24,283份CT报告）翻译成日语。训练数据集包含22,778份机器翻译的报告，验证数据集包括150份由放射科医师修订的报告。我们基于“tohoku-nlp/bert-base-japanese-v3”架构开发了CT-BERT-JPN，用于从日语放射学报告中提取18项结构化发现。结果：翻译指标显示了出色的表现，通气道区域和印象部分的BLEU分数分别为0.731和0.690，ROUGE分数范围分别为0.770至0.876和0.748至0.857。CT-BERT-JPN在18个条件中有11个方面优于GPT-4o，包括淋巴结肿大（+14.2%）、叶间间隔增厚（+10.9%）和肺不张（+7.4%）。该模型在14个条件中保持了超过0.95的F1分数，并在4个条件中达到了完美分数。结论：我们的研究建立了一个强大的日语CT报告数据集，并展示了专门用于结构化发现分类的语言模型的有效性。机器翻译与专家验证的结合方法使得能够创建大规模的高质量医学数据集。 

---
# On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education 

**Title (ZH)**: 预训练基础大语言模型在德国法律教育中的适用性分析 

**Authors**: Lorenz Wendlinger, Christian Braun, Abdullah Al Zubaer, Simon Alexander Nonn, Sarah Großkopf, Christofer Fellicious, Michael Granitzer  

**Link**: [PDF](https://arxiv.org/pdf/2412.15902)  

**Abstract**: We show that current open-source foundational LLMs possess instruction capability and German legal background knowledge that is sufficient for some legal analysis in an educational context. However, model capability breaks down in very specific tasks, such as the classification of "Gutachtenstil" appraisal style components, or with complex contexts, such as complete legal opinions. Even with extended context and effective prompting strategies, they cannot match the Bag-of-Words baseline. To combat this, we introduce a Retrieval Augmented Generation based prompt example selection method that substantially improves predictions in high data availability scenarios. We further evaluate the performance of pre-trained LLMs on two standard tasks for argument mining and automated essay scoring and find it to be more adequate. Throughout, pre-trained LLMs improve upon the baseline in scenarios with little or no labeled data with Chain-of-Thought prompting further helping in the zero-shot case. 

**Abstract (ZH)**: 我们在研究中显示，当前的开源基础型大规模语言模型（LLM）具备一定的指令执行能力和德国法律背景知识，足以在教育环境中进行某些法律分析。然而，这些模型在特定任务或复杂情境下的表现有限，例如“Gutachtenstil”评估风格成分的分类，或在完整的法律意见中进行复杂的分析。即使提供了扩展的上下文和有效的提示策略，它们也无法达到基于词袋（Bag-of-Words）的基本标准。为解决这一问题，我们引入了一种检索增强生成（RAG）的提示示例选择方法，在高数据可用性情况下显著提高了预测精度。此外，我们还评估了预训练LLM在两个标准的论证挖掘和自动作文评分任务上的表现，发现它们更为合适。在整个研究过程中，预训练LLM在少量或无标签数据的情境下表现优于基线模型，链式思考（Chain-of-Thought）的提示策略进一步在零样本情况下提高了其表现。 

---
# TelcoLM: collecting data, adapting, and benchmarking language models for the telecommunication domain 

**Title (ZH)**: TelcoLM：收集数据、适配并评估语言模型在电信领域的应用场景 

**Authors**: Camille Barboule, Viet-Phi Huynh, Adrien Bufort, Yoan Chabot, Géraldine Damnati, Gwénolé Lecorvé  

**Link**: [PDF](https://arxiv.org/pdf/2412.15891)  

**Abstract**: Despite outstanding processes in many tasks, Large Language Models (LLMs) still lack accuracy when dealing with highly technical domains. Especially, telecommunications (telco) is a particularly challenging domain due the large amount of lexical, semantic and conceptual peculiarities. Yet, this domain holds many valuable use cases, directly linked to industrial needs. Hence, this paper studies how LLMs can be adapted to the telco domain. It reports our effort to (i) collect a massive corpus of domain-specific data (800M tokens, 80K instructions), (ii) perform adaptation using various methodologies, and (iii) benchmark them against larger generalist models in downstream tasks that require extensive knowledge of telecommunications. Our experiments on Llama-2-7b show that domain-adapted models can challenge the large generalist models. They also suggest that adaptation can be restricted to a unique instruction-tuning step, dicarding the need for any fine-tuning on raw texts beforehand. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在许多任务中表现出色，但在处理高度技术性领域时，它们仍缺乏准确性。特别是在电信（telco）领域，由于大量的词汇、语义和概念独特性，这一领域尤其具有挑战性。然而，电信领域也包含许多直接关联到工业需求的重要应用场景。因此，本文研究了如何将LLMs适配到电信领域。具体而言，我们致力于（i）收集大量领域的特定数据集（8亿个标记，8万条指令），（ii）使用多种方法进行适配，以及（iii）在需要广泛电信知识的下游任务中将适配后的模型与更大的通用模型进行基准测试。我们在Llama-2-7b上的实验表明，领域适配模型能够挑战通用大型模型。这些结果还表明，适配过程可以仅限于单一的指令调整步骤，无需在原始文本上进行任何预先的微调。 

---
# Approximate State Abstraction for Markov Games 

**Title (ZH)**: 马尔可夫游戏的近似状态抽象 

**Authors**: Hiroki Ishibashi, Kenshi Abe, Atsushi Iwasaki  

**Link**: [PDF](https://arxiv.org/pdf/2412.15877)  

**Abstract**: This paper introduces state abstraction for two-player zero-sum Markov games (TZMGs), where the payoffs for the two players are determined by the state representing the environment and their respective actions, with state transitions following Markov decision processes. For example, in games like soccer, the value of actions changes according to the state of play, and thus such games should be described as Markov games. In TZMGs, as the number of states increases, computing equilibria becomes more difficult. Therefore, we consider state abstraction, which reduces the number of states by treating multiple different states as a single state. There is a substantial body of research on finding optimal policies for Markov decision processes using state abstraction. However, in the multi-player setting, the game with state abstraction may yield different equilibrium solutions from those of the ground game. To evaluate the equilibrium solutions of the game with state abstraction, we derived bounds on the duality gap, which represents the distance from the equilibrium solutions of the ground game. Finally, we demonstrate our state abstraction with Markov Soccer, compute equilibrium policies, and examine the results. 

**Abstract (ZH)**: 本文介绍了两玩家零和马尔可夫游戏（TZMGs）中的状态抽象方法，其中两个玩家的收益由表示环境的状态及其各自行动共同决定，状态转换遵循马尔可夫决策过程。例如，在足球这类游戏中，玩家采取行动的价值会根据比赛状态的变化而变化，因此这类游戏应当被描述为马尔可夫游戏。在TZMGs中，随着状态数量的增加，计算均衡变得更加困难。因此，我们考虑了状态抽象的方法，通过将多个不同的状态合并为一个状态来减少状态的数量。关于使用状态抽象寻找马尔可夫决策过程中的最优策略已有大量研究。然而，在多玩家环境中，使用状态抽象后的游戏可能产生与原始游戏不同的均衡解。为了评估状态抽象后游戏的均衡解，我们导出了对偶间隙的界，该界代表了与原始游戏的均衡解之间的距离。最后，我们以马尔可夫足球为例展示了状态抽象方法，计算了均衡策略，并分析了结果。 

---
# AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI 

**Title (ZH)**: AI在内的循环：人工智能时代生物医学可视化分析应用的未来 

**Authors**: Katja Bühler, Thomas Höllt, Thomas Schulz, Pere-Pau Vázquez  

**Link**: [PDF](https://arxiv.org/pdf/2412.15876)  

**Abstract**: AI is the workhorse of modern data analytics and omnipresent across many sectors. Large Language Models and multi-modal foundation models are today capable of generating code, charts, visualizations, etc. How will these massive developments of AI in data analytics shape future data visualizations and visual analytics workflows? What is the potential of AI to reshape methodology and design of future visual analytics applications? What will be our role as visualization researchers in the future? What are opportunities, open challenges and threats in the context of an increasingly powerful AI? This Visualization Viewpoint discusses these questions in the special context of biomedical data analytics as an example of a domain in which critical decisions are taken based on complex and sensitive data, with high requirements on transparency, efficiency, and reliability. We map recent trends and developments in AI on the elements of interactive visualization and visual analytics workflows and highlight the potential of AI to transform biomedical visualization as a research field. Given that agency and responsibility have to remain with human experts, we argue that it is helpful to keep the focus on human-centered workflows, and to use visual analytics as a tool for integrating ``AI-in-the-loop''. This is in contrast to the more traditional term ``human-in-the-loop'', which focuses on incorporating human expertise into AI-based systems. 

**Abstract (ZH)**: 人工智能是现代数据analytics的核心工具，并在众多领域无处不在。今天，大型语言模型和多模态基础模型已经能够生成代码、图表、可视化等内容。这些在数据分析中的大规模发展将如何塑造未来的数据可视化和视觉分析工作流程？人工智能在视觉分析方法和设计中有哪些重塑潜力？作为视觉分析研究人员，我们未来应该扮演什么角色？在人工智能越来越强大的背景下，有哪些机遇、开放挑战和威胁？本文从生物医学数据分析这一领域的视角出发，探讨这些问题，该领域基于复杂而敏感的数据做出关键决策，对透明度、效率和可靠性有很高的要求。我们将近期在人工智能领域的趋势和发展映射到交互式可视化和视觉分析工作流程的各个要素上，强调人工智能在重塑生物医学可视化研究领域的潜力。鉴于控制权和责任仍应保留给人类专家，我们认为保持以用户为中心的工作流程重点是有帮助的，并将视觉分析作为整合“AI在环”工具的手段。这与更传统的“人在环”概念形成了对比，后者侧重于将人类专业知识融入基于人工智能的系统中。 

---
# Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo Theories and Reachability Analysis 

**Title (ZH)**: 基于满意性理论和可达性分析的道路规则合规轨迹修复 

**Authors**: Yuanfei Lin, Zekun Xing, Xuyuan Han, Matthias Althoff  

**Link**: [PDF](https://arxiv.org/pdf/2412.15837)  

**Abstract**: Complying with traffic rules is challenging for automated vehicles, as numerous rules need to be considered simultaneously. If a planned trajectory violates traffic rules, it is common to replan a new trajectory from scratch. We instead propose a trajectory repair technique to save computation time. By coupling satisfiability modulo theories with set-based reachability analysis, we determine if and in what manner the initial trajectory can be repaired. Experiments in high-fidelity simulators and in the real world demonstrate the benefits of our proposed approach in various scenarios. Even in complex environments with intricate rules, we efficiently and reliably repair rule-violating trajectories, enabling automated vehicles to swiftly resume legally safe operation in real-time. 

**Abstract (ZH)**: 遵守交通规则对于自动驾驶车辆而言是一个挑战，因为需要同时考虑众多规则。如果预定的路径违反了交通规则，通常的做法是从头开始重新规划一条新的路径。相比之下，我们提出了一种路径修复技术来节省计算时间。通过将逻辑理论满足性与集合可达性分析相结合，我们确定初始路径是否以及如何进行修复。在高保真模拟器和现实世界中的实验表明，所提出的方法在各种场景中具有显著优势。即使在复杂且规则繁琐的环境中，我们也能高效且可靠地修复违规路径，使得自动驾驶车辆能够实时恢复合法安全的操作。 

---
# S$^2$DN: Learning to Denoise Unconvincing Knowledge for Inductive Knowledge Graph Completion 

**Title (ZH)**: S$^2$DN：学习去噪不令人信服的知识以进行归纳型知识图谱补全 

**Authors**: Tengfei Ma, Yujie Chen, Liang Wang, Xuan Lin, Bosheng Song, Xiangxiang Zeng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15822)  

**Abstract**: Inductive Knowledge Graph Completion (KGC) aims to infer missing facts between newly emerged entities within knowledge graphs (KGs), posing a significant challenge. While recent studies have shown promising results in inferring such entities through knowledge subgraph reasoning, they suffer from (i) the semantic inconsistencies of similar relations, and (ii) noisy interactions inherent in KGs due to the presence of unconvincing knowledge for emerging entities. To address these challenges, we propose a Semantic Structure-aware Denoising Network (S$^2$DN) for inductive KGC. Our goal is to learn adaptable general semantics and reliable structures to distill consistent semantic knowledge while preserving reliable interactions within KGs. Specifically, we introduce a semantic smoothing module over the enclosing subgraphs to retain the universal semantic knowledge of relations. We incorporate a structure refining module to filter out unreliable interactions and offer additional knowledge, retaining robust structure surrounding target links. Extensive experiments conducted on three benchmark KGs demonstrate that S$^2$DN surpasses the performance of state-of-the-art models. These results demonstrate the effectiveness of S$^2$DN in preserving semantic consistency and enhancing the robustness of filtering out unreliable interactions in contaminated KGs. 

**Abstract (ZH)**: 归纳知识图谱完成（Inductive Knowledge Graph Completion, KGC）旨在推断知识图谱（KGs）中新涌现实体之间的缺失事实，这提出了一个显著的挑战。尽管近年来的研究通过知识子图推理在推断这类实体方面取得了有希望的结果，但它们仍然面临以下挑战：（i）相似关系的语义不一致性；以及（ii）因KG中存在不可靠的知识而导致的噪音交互。为应对这些挑战，我们提出了一个语义结构感知降噪网络（Semantic Structure-aware Denoising Network, S$^2$DN）以进行归纳KGC。我们的目标是学习可适应的一般语义和可靠结构，以提取一致的语义知识并保留KG中的可靠交互。具体而言，我们通过在包围子图上引入语义平滑模块来保留关系的一般语义知识。我们还引入了一个结构精炼模块，用于过滤掉不可靠的交互并提供额外的知识，保留目标链接周围的稳健结构。在三个基准KG上的广泛实验表明，S$^2$DN的性能超过了当前最先进的模型。这些结果证明了S$^2$DN在保留语义一致性并增强筛选出不可靠交互的稳健性方面的有效性。 

---
# $\pi$-yalli: un nouveau corpus pour le nahuatl 

**Title (ZH)**: $\pi$-yalli：一个新的纳瓦特尔语语料库 

**Authors**: Juan-Manuel Torres-Moreno, Juan-José Guzmán-Landa, Graham Ranger, Martha Lorena Avendaño Garrido, Miguel Figueroa-Saavedra, Ligia Quintana-Torres, Carlos-Emiliano González-Gallardo, Elvys Linhares Pontes, Patricia Velázquez Morales, Luis-Gil Moreno Jiménez  

**Link**: [PDF](https://arxiv.org/pdf/2412.15821)  

**Abstract**: The NAHU$^2$ project is a Franco-Mexican collaboration aimed at building the $\pi$-YALLI corpus adapted to machine learning, which will subsequently be used to develop computer resources for the Nahuatl language. Nahuatl is a language with few computational resources, even though it is a living language spoken by around 2 million people. We have decided to build $\pi$-YALLI, a corpus that will enable to carry out research on Nahuatl in order to develop Language Models (LM), whether dynamic or not, which will make it possible to in turn enable the development of Natural Language Processing (NLP) tools such as: a) a grapheme unifier, b) a word segmenter, c) a POS grammatical analyser, d) a content-based Automatic Text Summarization; and possibly, e) a translator translator (probabilistic or learning-based). 

**Abstract (ZH)**: NAHU$^2$项目是法国和墨西哥的一项合作，旨在构建适应机器学习的$\pi$-YALLI语料库，并随后用于开发纳瓦特尔语（Nahuatl）的语言资源。尽管纳瓦特尔语是一种活的方言，使用者约有200万人，但其计算资源却很少。我们计划构建$\pi$-YALLI语料库，以便进行纳瓦特尔语的研究，开发动态或非动态语言模型（LM），从而进一步开发自然语言处理（NLP）工具，包括但不限于：a) 字符统一模块，b) 单词分段器，c) 词性（POS）语法分析器，d) 内容驱动的自动文本摘要；以及可能还包括e) 译者翻译器（基于概率或学习的方法）。 

---
# WebLLM: A High-Performance In-Browser LLM Inference Engine 

**Title (ZH)**: WebLLM：一种高性能的浏览器内运行的LLM推理引擎 

**Authors**: Charlie F. Ruan, Yucheng Qin, Xun Zhou, Ruihang Lai, Hongyi Jin, Yixin Dong, Bohan Hou, Meng-Shiun Yu, Yiyan Zhai, Sudeep Agarwal, Hangrui Cao, Siyuan Feng, Tianqi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15803)  

**Abstract**: Advancements in large language models (LLMs) have unlocked remarkable capabilities. While deploying these models typically requires server-grade GPUs and cloud-based inference, the recent emergence of smaller open-source models and increasingly powerful consumer devices have made on-device deployment practical. The web browser as a platform for on-device deployment is universally accessible, provides a natural agentic environment, and conveniently abstracts out the different backends from diverse device vendors. To address this opportunity, we introduce WebLLM, an open-source JavaScript framework that enables high-performance LLM inference entirely within web browsers. WebLLM provides an OpenAI-style API for seamless integration into web applications, and leverages WebGPU for efficient local GPU acceleration and WebAssembly for performant CPU computation. With machine learning compilers MLC-LLM and Apache TVM, WebLLM leverages optimized WebGPU kernels, overcoming the absence of performant WebGPU kernel libraries. Evaluations show that WebLLM can retain up to 80% native performance on the same device, with room to further close the gap. WebLLM paves the way for universally accessible, privacy-preserving, personalized, and locally powered LLM applications in web browsers. The code is available at: this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）的进步解锁了非凡的能力。尽管部署这些模型通常需要服务器级GPU和基于云的推理，但最近出现的更小的开源模型和日益强大的消费者设备使设备上部署成为可能。作为设备上部署平台的网络浏览器具有普遍可达性，提供了一个自然的自主环境，并且方便地将不同的后端抽象出来，与各种设备供应商无关。为了应对这一机遇，我们介绍了WebLLM，这是一种开源的JavaScript框架，可以在网络浏览器中实现高性能的LLM推理。WebLLM提供了一种类似OpenAI的API，使无缝集成到Web应用程序成为可能，并利用WebGPU进行高效的地方GPU加速，以及利用WebAssembly进行高性能的CPU计算。通过机器学习编译器MLC-LLM和Apache TVM，WebLLM利用优化的WebGPU内核，克服了缺乏高性能WebGPU内核库的问题。评估结果显示，WebLLM可以在同一设备上保留高达80%的原生性能，并有进一步缩小差距的空间。WebLLM为网络浏览器中的普遍可访问、私有保护、个性化和本地驱动的LLM应用程序铺平了道路。代码可在以下链接获得：this https URL. 

---
# Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form 

**Title (ZH)**: 形态指标与三维城市块体的双向映射：增强城市形态刻画与生成 

**Authors**: Chenyi Cai, Biao Li, Qiyan Zhang, Xiao Wang, Filip Biljecki, Pieter Herthogs  

**Link**: [PDF](https://arxiv.org/pdf/2412.15801)  

**Abstract**: Urban morphology, examining city spatial configurations, links urban design to sustainability. Morphology metrics play a fundamental role in performance-driven computational urban design (CUD) which integrates urban form generation, performance evaluation and optimization. However, a critical gap remains between performance evaluation and complex urban form generation, caused by the disconnection between morphology metrics and urban form, particularly in metric-to-form workflows. It prevents the application of optimized metrics to generate improved urban form with enhanced urban performance. Formulating morphology metrics that not only effectively characterize complex urban forms but also enable the reconstruction of diverse forms is of significant importance. This paper highlights the importance of establishing a bi-directional mapping between morphology metrics and complex urban form to enable the integration of urban form generation with performance evaluation. We present an approach that can 1) formulate morphology metrics to both characterize urban forms and in reverse, retrieve diverse similar 3D urban forms, and 2) evaluate the effectiveness of morphology metrics in representing 3D urban form characteristics of blocks by comparison. We demonstrate the methodology with 3D urban models of New York City, covering 14,248 blocks. We use neural networks and information retrieval for morphology metric encoding, urban form clustering and morphology metric evaluation. We identified an effective set of morphology metrics for characterizing block-scale urban forms through comparison. The proposed methodology tightly couples complex urban forms with morphology metrics, hence it can enable a seamless and bidirectional relationship between urban form generation and optimization in performance-driven urban design towards sustainable urban design and planning. 

**Abstract (ZH)**: 城市的形态学，研究城市空间布局结构，将城市设计与可持续性联系起来。形态学指标在驱动式计算城市设计（CUD）中发挥着基础性作用，这种设计整合了城市形态生成、性能评估和优化。然而，性能评估与复杂城市形态生成之间仍存在关键差距，这主要是由于形态学指标与城市形态之间的脱节，特别是在指标到形态的工作流程中。这阻碍了优化指标的应用，以生成具有更好城市性能的改进形态。有效表征复杂城市形态并能够重构多样化形态的形态学指标的制定尤为重要。本文强调建立形态学指标与复杂城市形态之间的双向映射的重要性，以实现城市形态生成与性能评估的整合。我们提出了一种方法，该方法可以1) 制定形态学指标，不仅能够表征城市形态，还可以反向检索多样化的类似三维城市形态，2) 通过比较评估形态学指标在表示街区尺度城市形态特征方面的有效性。我们通过纽约市的3D城市模型（覆盖14,248个街区）展示了这种方法论。我们使用神经网络和信息检索来编码形态学指标、进行城市形态聚类和评估形态学指标。通过比较，我们确定了一组有效的形态学指标，用于表征街区尺度的城市形态。所提出的方法论紧密耦合复杂的城市形态和形态学指标，因此它可以实现驱动式城市设计中的城市形态生成和优化之间的无缝和双向关系，从而推动可持续城市设计和规划。 

---
# GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning 

**Title (ZH)**: GraphSeqLM：统一的图语言框架用于omics图学习 

**Authors**: Heming Zhang, Di Huang, Yixin Chen, Fuhai Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15790)  

**Abstract**: The integration of multi-omic data is pivotal for understanding complex diseases, but its high dimensionality and noise present significant challenges. Graph Neural Networks (GNNs) offer a robust framework for analyzing large-scale signaling pathways and protein-protein interaction networks, yet they face limitations in expressivity when capturing intricate biological relationships. To address this, we propose Graph Sequence Language Model (GraphSeqLM), a framework that enhances GNNs with biological sequence embeddings generated by Large Language Models (LLMs). These embeddings encode structural and biological properties of DNA, RNA, and proteins, augmenting GNNs with enriched features for analyzing sample-specific multi-omic data. By integrating topological, sequence-derived, and biological information, GraphSeqLM demonstrates superior predictive accuracy and outperforms existing methods, paving the way for more effective multi-omic data integration in precision medicine. 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

多组学数据的集成对于理解复杂疾病至关重要，但其高维度和噪声带来了显著挑战。图神经网络（GNNs）提供了一种分析大规模信号通路和蛋白质-蛋白质相互作用网络的稳健框架，但在捕捉复杂的生物关系时存在表达能力的限制。为解决这一问题，我们提出了一种基于图序列语言模型（GraphSeqLM）的框架，该框架通过大型语言模型（LLMs）生成的生物序列嵌入来增强GNNs。这些嵌入编码DNA、RNA和蛋白质的空间结构和生物学特性，从而为分析样本特异性多组学数据提供了丰富的特征。通过整合拓扑、序列衍生和生物学信息，GraphSeqLM展示了卓越的预测准确性，并优于现有方法，为精准医学中的多组学数据集成提供了更加有效的途径。 

---
# Linguistic Features Extracted by GPT-4 Improve Alzheimer's Disease Detection based on Spontaneous Speech 

**Title (ZH)**: GPT-4 提取的语言特征基于自发言论改善了 Alzheimer's 病的检测 

**Authors**: Jonathan Heitz, Gerold Schneider, Nicolas Langer  

**Link**: [PDF](https://arxiv.org/pdf/2412.15772)  

**Abstract**: Alzheimer's Disease (AD) is a significant and growing public health concern. Investigating alterations in speech and language patterns offers a promising path towards cost-effective and non-invasive early detection of AD on a large scale. Large language models (LLMs), such as GPT, have enabled powerful new possibilities for semantic text analysis. In this study, we leverage GPT-4 to extract five semantic features from transcripts of spontaneous patient speech. The features capture known symptoms of AD, but they are difficult to quantify effectively using traditional methods of computational linguistics. We demonstrate the clinical significance of these features and further validate one of them ("Word-Finding Difficulties") against a proxy measure and human raters. When combined with established linguistic features and a Random Forest classifier, the GPT-derived features significantly improve the detection of AD. Our approach proves effective for both manually transcribed and automatically generated transcripts, representing a novel and impactful use of recent advancements in LLMs for AD speech analysis. 

**Abstract (ZH)**: 阿尔茨海默病（AD）是日益严重的公共健康问题。探究言语和语言模式的变化为大规模低成本和无创早期阿尔茨海默病检测提供了有前景的途径。大型语言模型（LLMs），如GPT，提供了 Powerful的新可能性，用于语义文本分析。在本研究中，我们利用GPT-4从患者的自发言语转录中提取五种语义特征。这些特征捕捉了阿尔茨海默病的已知症状，但传统计算语言学方法难以有效量化这些特征。我们展示了这些特征的临床意义，并进一步通过替代测量标准和人工评分验证了其中一个特征（“找词困难”）。结合现有的语言学特征和随机森林分类器，GPT提取的特征显著提高了阿尔茨海默病的检测效果。我们的方法对于手动转录和自动生成的转录都证明了有效性，代表了利用近年来LLMs的进步进行阿尔茨海默病言语分析的一个新颖且有影响力的用例。 

---
# Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models 

**Title (ZH)**: 《纯粹理性批判：揭示医学大语言模型的推理行为》

这个标题翻译旨在保持原文的学术严谨性和原有的深意。"Critique of Impure Reason" 源自康德的哲学著作《纯粹理性批判》，但在翻译时直接翻译为 "纯粹理性批判"，以符合学术表达的习惯。"Unveiling the reasoning behaviour of medical Large Language Models" 被直接翻译为 "揭示医学大语言模型的推理行为"，确保准确传达原文意思。 

**Authors**: Shamus Sim, Tyrone Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15748)  

**Abstract**: Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, we define the concept of reasoning behaviour in the specific context of medical LLMs. We then categorise and discuss the current state of the art of methods which evaluate reasoning behaviour in medical LLMs. Finally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole 

**Abstract (ZH)**: 背景：尽管大型语言模型（LLMs）在医疗领域已经广泛普及，但鲜有研究关注其推理行为。我们强调理解推理行为的重要性，而不是仅仅关注其高精度预测，因为这相当于可解释的人工智能（XAI）。特别是在临床领域使用医疗LLMs的可解释性将对整个医疗保健行业产生重大影响。结果：因此，我们定义了医疗LLMs特定情境下的推理行为概念。我们随后对当前评估医疗LLMs推理行为的方法进行了分类和讨论。最后，我们提出了理论框架，这些框架可以帮助医疗专业人员或机器学习工程师更好地理解这些以前不那么透明的模型的底层推理操作。结论：由此，医疗机器学习模型的临床医生和患者中的透明度和信任度将提高，进而加速医疗AI在整个医疗保健系统中的整合、应用及其进一步发展。 

---
# fluke: Federated Learning Utility frameworK for Experimentation and research 

**Title (ZH)**: Fluke：用于实验与研究的联邦学习效用框架 

**Authors**: Mirko Polato  

**Link**: [PDF](https://arxiv.org/pdf/2412.15728)  

**Abstract**: Since its inception in 2016, Federated Learning (FL) has been gaining tremendous popularity in the machine learning community. Several frameworks have been proposed to facilitate the development of FL algorithms, but researchers often resort to implementing their algorithms from scratch, including all baselines and experiments. This is because existing frameworks are not flexible enough to support their needs or the learning curve to extend them is too steep. In this paper, we present \fluke, a Python package designed to simplify the development of new FL algorithms. fluke is specifically designed for prototyping purposes and is meant for researchers or practitioners focusing on the learning components of a federated system. fluke is open-source, and it can be either used out of the box or extended with new algorithms with minimal overhead. 

**Abstract (ZH)**: 自2016年问世以来，联邦学习（FL）在机器学习领域获得了巨大 popularity。已经提出了几种框架以促进 FL 算法的开发，但研究者们经常需要从零开始实现他们的算法，包括所有基础算法和实验。这是因为现有的框架不够灵活，不能满足他们的需求，或者扩展这些框架的学习曲线过于陡峭。本文介绍了一个名为 \fluke 的 Python 包，旨在简化新型 FL 算法的开发过程。\fluke 特别适用于原型设计，旨在为专注于联邦系统学习组件的研究人员或从业者提供服务。\fluke 是开源的，用户既可以直接使用它，也可以通过最小的额外工作量扩展它以加入新的算法。 

---
# Towards Secure AI-driven Industrial Metaverse with NFT Digital Twins 

**Title (ZH)**: 面向NFT数字孪生的AI驱动工业元宇宙安全研究 

**Authors**: Ravi Prakash, Tony Thomas  

**Link**: [PDF](https://arxiv.org/pdf/2412.15716)  

**Abstract**: The rise of the industrial metaverse has brought digital twins (DTs) to the forefront. Blockchain-powered non-fungible tokens (NFTs) offer a decentralized approach to creating and owning these cloneable DTs. However, the potential for unauthorized duplication, or counterfeiting, poses a significant threat to the security of NFT-DTs. Existing NFT clone detection methods often rely on static information like metadata and images, which can be easily manipulated. To address these limitations, we propose a novel deep-learning-based solution as a combination of an autoencoder and RNN-based classifier. This solution enables real-time pattern recognition to detect fake NFT-DTs. Additionally, we introduce the concept of dynamic metadata, providing a more reliable way to verify authenticity through AI-integrated smart contracts. By effectively identifying counterfeit DTs, our system contributes to strengthening the security of NFT-based assets in the metaverse. 

**Abstract (ZH)**: 工业元宇宙的兴起将数字孪生（DTs）置于了中心位置。依托区块链技术的非同质化代币（NFTs）提供了一种去中心化的创建和拥有这些可复制DTs的方法。然而，未经授权的复制或伪造对NFT-DTs的安全构成了重大威胁。现有的NFT伪造检测方法往往依赖于静态信息，如元数据和图像，这些信息容易被篡改。为解决这些问题，我们提出了一种结合自动编码器和基于循环神经网络（RNN）分类器的新型深度学习解决方案。该方案能够实现实时模式识别，以检测假NFT-DTs。此外，我们引入了动态元数据的概念，通过结合AI的智能合约提供了一种更可靠的方式来验证真实性。通过有效识别伪造的DTs，我们的系统有助于加强基于NFT的元宇宙资产的安全性。 

---
# MacLight: Multi-scene Aggregation Convolutional Learning for Traffic Signal Control 

**Title (ZH)**: MacLight: 多场景聚合卷积学习在交通信号控制中的应用 

**Authors**: Sunbowen Lee, Hongqin Lyu, Yicheng Gong, Yingying Sun, Chao Deng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15703)  

**Abstract**: Reinforcement learning methods have proposed promising traffic signal control policy that can be trained on large road networks. Current SOTA methods model road networks as topological graph structures, incorporate graph attention into deep Q-learning, and merge local and global embeddings to improve policy. However, graph-based methods are difficult to parallelize, resulting in huge time overhead. Moreover, none of the current peer studies have deployed dynamic traffic systems for experiments, which is far from the actual situation.
In this context, we propose Multi-Scene Aggregation Convolutional Learning for traffic signal control (MacLight), which offers faster training speeds and more stable performance. Our approach consists of two main components. The first is the global representation, where we utilize variational autoencoders to compactly compress and extract the global representation. The second component employs the proximal policy optimization algorithm as the backbone, allowing value evaluation to consider both local features and global embedding representations. This backbone model significantly reduces time overhead and ensures stability in policy updates. We validated our method across multiple traffic scenarios under both static and dynamic traffic systems. Experimental results demonstrate that, compared to general and domian SOTA methods, our approach achieves superior stability, optimized convergence levels and the highest time efficiency. The code is under this https URL. 

**Abstract (ZH)**: 强化学习方法提出了在大型道路网络上训练的有希望的交通信号控制策略。当前的最佳方法将道路网络建模为拓扑图结构，将图注意力机制融入到深度Q学习中，并结合局部和全局嵌入来改进策略。然而，基于图的方法难以并行化，导致了巨大的时间开销。此外，当前的同行研究中没有部署动态交通系统进行实验，这与实际情况相差甚远。

在这一背景下，我们提出了多场景聚合卷积学习方法用于交通信号控制（MacLight），提供了更快的训练速度和更稳定的性能。我们的方法主要包括两个主要组成部分。首先，在全局表示部分，我们利用变分自编码器紧凑地压缩和提取全局表示。其次，我们采用近端策略优化算法作为主干，使得价值评估能够同时考虑局部特征和全局嵌入表示。这一主干模型显著减少了时间开销，并确保了策略更新的稳定性。我们在多种静态和动态交通场景下验证了我们的方法。实验结果表明，与通用和领域内的最佳方法相比，我们的方法在稳定性和收敛性方面表现出更优性能，并且具有最高的时间效率。相关代码已发布在以下网址：https://github.com/maclight。 

---
# AI-generated Image Quality Assessment in Visual Communication 

**Title (ZH)**: 人工智能生成图像的质量评估在视觉通信中的应用 

**Authors**: Yu Tian, Yixuan Li, Baoliang Chen, Hanwei Zhu, Shiqi Wang, Sam Kwong  

**Link**: [PDF](https://arxiv.org/pdf/2412.15677)  

**Abstract**: Assessing the quality of artificial intelligence-generated images (AIGIs) plays a crucial role in their application in real-world scenarios. However, traditional image quality assessment (IQA) algorithms primarily focus on low-level visual perception, while existing IQA works on AIGIs overemphasize the generated content itself, neglecting its effectiveness in real-world applications. To bridge this gap, we propose AIGI-VC, a quality assessment database for AI-Generated Images in Visual Communication, which studies the communicability of AIGIs in the advertising field from the perspectives of information clarity and emotional interaction. The dataset consists of 2,500 images spanning 14 advertisement topics and 8 emotion types. It provides coarse-grained human preference annotations and fine-grained preference descriptions, benchmarking the abilities of IQA methods in preference prediction, interpretation, and reasoning. We conduct an empirical study of existing representative IQA methods and large multi-modal models on the AIGI-VC dataset, uncovering their strengths and weaknesses. 

**Abstract (ZH)**: 评估人工生成图像（AI生成图像，AIGIs）的质量在其实用场景中扮演着至关重要的角色。然而，传统的图像质量评估（IQA）算法主要侧重于低级视觉感知，而现有的针对AIGIs的IQA研究则过于强调生成内容本身，忽视了其在实际应用中的有效性。为弥合这一差距，我们提出了AIGI-VC数据库，旨在从信息清晰度和情感互动的角度研究视觉传达领域中AIGIs的可传达性。该数据集包含2500张图像，涵盖了14个广告主题和8种情感类型。它提供了粗粒度的人类偏好标注和细粒度的偏好描述，以此来评估IQA方法在偏好预测、解释和推理方面的能力。我们对现有的代表性IQA方法和大型多模态模型在AIGI-VC数据集上的进行了实证研究，发现了这些方法的优势和不足。 

---
# MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula 

**Title (ZH)**: 数学语音识别：利用小型语言模型实现精准的数学语音转公式转换 

**Authors**: Sieun Hyeon, Kyudan Jung, Jaehee Won, Nam-Joon Kim, Hyun Gon Ryu, Hyuk-Jae Lee, Jaeyoung Do  

**Link**: [PDF](https://arxiv.org/pdf/2412.15655)  

**Abstract**: In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler's Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i $\textit{side}$ of x), instead of the concise $\LaTeX{}$ format (i.e., $ e^{ix} = \cos(x) + i\sin(x) $), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured $\LaTeX{}$ representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates $\LaTeX{}$ generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for $\LaTeX{}$ translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores compared to GPT-4o. 

**Abstract (ZH)**: 在各种学术和专业环境中，如数学讲座或研究展示中，口头传达数学表达式往往是必要的。然而，没有辅助视觉材料的情况下朗读数学表达式会显著妨碍理解，特别是在听障人士或者因为语言障碍依赖字幕的情况下。例如，当讲者朗读欧拉公式时，当前的自动语音识别（ASR）模型往往会生成冗长且容易出错的文本描述（如“e的i x次方等于x的余弦加上i乘以x的余弦函数”），而不是简洁的LaTeX格式（即 $e^{ix} = \cos(x) + i\sin(x)$），这阻碍了清晰的理解与沟通。为了解决这一问题，我们提出了MathSpeech，这是一个新颖的管道，将ASR模型与小型语言模型（sLM）结合，以纠正数学表达式中的错误并准确地将口语表达转换为结构化的LaTeX表示。MathSpeech通过一个新的数据集进行评估，该数据集源自讲座录音，其LaTeX生成能力与领先商业大型语言模型（LLM）相当，同时仅使用了120M参数的微调小型语言模型。具体而言，在LaTeX转换的CER、BLEU和ROUGE分数方面，MathSpeech相比GPT-4o表现出显著的优越性。我们观察到CER分数从0.390下降到0.298，ROUGE/BLEU分数也高于GPT-4o。 

---
# Tacit Learning with Adaptive Information Selection for Cooperative Multi-Agent Reinforcement Learning 

**Title (ZH)**: 自适应信息选择的隐式学习在协同多agent强化学习中的应用 

**Authors**: Lunjun Liu, Weilai Jiang, Yaonan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15639)  

**Abstract**: In multi-agent reinforcement learning (MARL), the centralized training with decentralized execution (CTDE) framework has gained widespread adoption due to its strong performance. However, the further development of CTDE faces two key challenges. First, agents struggle to autonomously assess the relevance of input information for cooperative tasks, impairing their decision-making abilities. Second, in communication-limited scenarios with partial observability, agents are unable to access global information, restricting their ability to collaborate effectively from a global perspective. To address these challenges, we introduce a novel cooperative MARL framework based on information selection and tacit learning. In this framework, agents gradually develop implicit coordination during training, enabling them to infer the cooperative behavior of others in a discrete space without communication, relying solely on local information. Moreover, we integrate gating and selection mechanisms, allowing agents to adaptively filter information based on environmental changes, thereby enhancing their decision-making capabilities. Experiments on popular MARL benchmarks show that our framework can be seamlessly integrated with state-of-the-art algorithms, leading to significant performance improvements. 

**Abstract (ZH)**: 在多智能体强化学习（MARL）中，集中训练与分散执行（CTDE）框架因其强大的性能而得到了广泛应用。然而，CTDE的进一步发展面临两个主要挑战。首先，智能体难以自主评估输入信息的相关性，影响其决策能力。其次，在通信受限和部分可观测的情景下，智能体无法获取全局信息，限制了它们从全局视角进行有效协作的能力。为了解决这些挑战，我们提出了一种基于信息选择和默会学习的新型合作MARL框架。在该框架中，智能体在训练过程中逐步发展出隐性的协调能力，能够在不进行通信的情况下，仅依靠局部信息推断出其他智能体的合作行为。此外，我们引入了门控和选择机制，使智能体能够根据环境变化自适应地筛选信息，从而提高其决策能力。通过对流行的MARL基准测试的实验表明，我们的框架可以无缝集成到最先进的算法中，从而显著提升性能。 

---
# JailPO: A Novel Black-box Jailbreak Framework via Preference Optimization against Aligned LLMs 

**Title (ZH)**: JailPO：一种针对对齐的大语言模型的新型黑盒 Jailbreak 框架通过偏好优化 

**Authors**: Hongyi Li, Jiawei Ye, Jie Wu, Tianjie Yan, Chu Wang, Zhixin Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15623)  

**Abstract**: Large Language Models (LLMs) aligned with human feedback have recently garnered significant attention. However, it remains vulnerable to jailbreak attacks, where adversaries manipulate prompts to induce harmful outputs. Exploring jailbreak attacks enables us to investigate the vulnerabilities of LLMs and further guides us in enhancing their security. Unfortunately, existing techniques mainly rely on handcrafted templates or generated-based optimization, posing challenges in scalability, efficiency and universality. To address these issues, we present JailPO, a novel black-box jailbreak framework to examine LLM alignment. For scalability and universality, JailPO meticulously trains attack models to automatically generate covert jailbreak prompts. Furthermore, we introduce a preference optimization-based attack method to enhance the jailbreak effectiveness, thereby improving efficiency. To analyze model vulnerabilities, we provide three flexible jailbreak patterns. Extensive experiments demonstrate that JailPO not only automates the attack process while maintaining effectiveness but also exhibits superior performance in efficiency, universality, and robustness against defenses compared to baselines. Additionally, our analysis of the three JailPO patterns reveals that attacks based on complex templates exhibit higher attack strength, whereas covert question transformations elicit riskier responses and are more likely to bypass defense mechanisms. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，并符合学术规范：

大规模语言模型（LLMs）在与人类反馈对齐后，最近引起了广泛关注。然而，它们仍然容易受到逃避攻击（jailbreak attacks）的影响，即攻击者通过操纵提示来诱导有害输出。探索这些逃避攻击有助于我们研究LLMs的脆弱性，并进一步指导其安全性的提升。不幸的是，现有的技术主要依赖于手工构建的模板或生成优化方法，这在可扩展性、效率和普适性方面带来了挑战。为了解决这些问题，我们提出了JailPO，这是一种新颖的黑盒逃避框架，用于检验LLMs的对齐情况。为了提高可扩展性和普适性，JailPO精心训练攻击模型，能够自动生成隐蔽的逃避提示。此外，我们引入了一种基于偏好优化的攻击方法，以提高逃避效果，从而提高效率。为了分析模型的脆弱性，我们提供了三种灵活的逃避模式。广泛实验表明，JailPO不仅自动化了攻击过程，同时保持了高效率，并且在鲁棒性和普适性方面优于基线模型。此外，对三种JailPO模式的分析显示，基于复杂模板的攻击具有更强的攻击力度，而隐蔽的问题转换则引发更具风险的响应，并且更有可能绕过防御机制。 

---
# Modeling Autonomous Shifts Between Focus State and Mind-Wandering Using a Predictive-Coding-Inspired Variational RNN Model 

**Title (ZH)**: 基于预测编码启发的变分循环神经网络模型：建模焦点状态与思维游离之间的自主转换 

**Authors**: Henrique Oyama, Jun Tani  

**Link**: [PDF](https://arxiv.org/pdf/2412.15620)  

**Abstract**: The current study investigates possible neural mechanisms underling autonomous shifts between focus state and mind-wandering by conducting model simulation experiments. On this purpose, we modeled perception processes of continuous sensory sequences using our previous proposed variational RNN model which was developed based on the free energy principle. The current study extended this model by introducing an adaptation mechanism of a meta-level parameter, referred to as the meta-prior $\mathbf{w}$, which regulates the complexity term in the free energy. Our simulation experiments demonstrated that autonomous shifts between focused perception and mind-wandering take place when $\mathbf{w}$ switches between low and high values associated with decrease and increase of the average reconstruction error over the past window. In particular, high $\mathbf{w}$ prioritized top-down predictions while low $\mathbf{w}$ emphasized bottom-up sensations. This paper explores how our experiment results align with existing studies and highlights their potential for future research. 

**Abstract (ZH)**: 当前研究通过进行模型仿真实验，探讨自主切换注意力状态和思维漫游的潜在神经机制。为此，我们基于自由能原理，使用先前提出的变分RNN模型，模拟连续感官序列的感知过程。在此基础上，本研究通过引入一个元级参数的适应机制，即元先验$\mathbf{w}$，来调节自由能中的复杂性项。我们的仿真实验结果显示，在$\mathbf{w}$从低值切换到高值时，伴随着过去窗口中平均重构误差的减少和增加，注意力状态和思维漫游之间会自发发生切换。特别是，高$\mathbf{w}$优先处理自上而下的预测，而低$\mathbf{w}$则更侧重于自下而上的感觉。本文探讨了我们的实验结果与现有研究的契合之处，并强调了这些结果在未来研究中的潜在价值。 

---
# Microservices-Based Framework for Predictive Analytics and Real-time Performance Enhancement in Travel Reservation Systems 

**Title (ZH)**: 基于微服务的预测分析与实时性能增强框架在旅行预订系统中的应用 

**Authors**: Biman Barua, M. Shamim Kaiser  

**Link**: [PDF](https://arxiv.org/pdf/2412.15616)  

**Abstract**: The paper presents a framework of microservices-based architecture dedicated to enhancing the performance of real-time travel reservation systems using the power of predictive analytics. Traditional monolithic systems are bad at scaling and performing with high loads, causing backup resources to be underutilized along with delays. To overcome the above-stated problems, we adopt a modularization approach in decoupling system components into independent services that can grow or shrink according to demand. Our framework also includes real-time predictive analytics, through machine learning models, that optimize forecasting customer demand, dynamic pricing, as well as system performance. With an experimental evaluation applying the approach, we could show that the framework impacts metrics of performance such as response time, throughput, transaction rate of success, and prediction accuracy compared to their conventional counterparts. Not only does the microservices approach improve scalability and fault tolerance like a usual architecture, but it also brings along timely and accurate predictions, which imply a greater customer satisfaction and efficiency of operation. The integration of real-time analytics would lead to more intelligent decision-making, thereby improving the response of the system along with the reliability it holds. A scalable, efficient framework is offered by such a system to address the modern challenges imposed by any form of travel reservation system while considering other complex, data-driven industries as future applications. Future work will be an investigation of advanced AI models and edge processing to further improve the performance and robustness of the systems employed. 

**Abstract (ZH)**: 本文提出了一种基于微服务的架构框架，旨在通过预测分析的力量增强实时旅行预订系统的性能。传统的单一架构在扩展和应对高负载时表现不佳，导致备用资源被闲置并造成延迟。为了解决上述问题，我们采用了模块化的方法，将系统组件解耦为独立的服务，这些服务可以根据需求增长或收缩。我们的框架还包括通过机器学习模型进行的实时预测分析，可以优化客户需求预测、动态定价以及系统性能。通过实验评估该方法，我们展示了与传统方法相比，该框架在响应时间、吞吐量、交易成功率和预测准确性等性能指标上的影响。微服务方法不仅提高了解决方案的可扩展性和容错性，还带来了及时准确的预测，从而提高了客户满意度和操作效率。实现实时分析将导致更智能的决策做出，从而提高系统的响应速度和可靠性。这种系统提供了一个可扩展且高效的框架，以应对各种旅行预订系统面临的现代挑战，并考虑到其他复杂的数据驱动行业作为未来应用。未来的工作将探索先进的AI模型和边缘处理，以进一步提高所使用系统的性能和鲁棒性。 

---
# A Fusion Approach of Dependency Syntax and Sentiment Polarity for Feature Label Extraction in Commodity Reviews 

**Title (ZH)**: 商品评论中特征标签提取的依赖句法与情感极性融合方法 

**Authors**: Jianfei Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15610)  

**Abstract**: This study analyzes 13,218 product reviews from this http URL, covering four categories: mobile phones, computers, cosmetics, and food. A novel method for feature label extraction is proposed by integrating dependency parsing and sentiment polarity analysis. The proposed method addresses the challenges of low robustness in existing extraction algorithms and significantly enhances extraction accuracy. Experimental results show that the method achieves an accuracy of 0.7, with recall and F-score both stabilizing at 0.8, demonstrating its effectiveness. However, challenges such as dependence on matching dictionaries and the limited scope of extracted feature tags require further investigation in future research. 

**Abstract (ZH)**: 本研究分析了来自<网址>的13,218条产品评论，涵盖了移动电话、计算机、化妆品和食品四个类别。我们提出了一种新颖的特征标签提取方法，该方法结合了依存句法分析和情感极性分析。所提出的方法解决了现有提取算法鲁棒性低的问题，显著提高了提取精度。实验结果表明，该方法的准确率达到0.7，召回率和F分数均稳定在0.8，证明了其有效性。然而，依赖匹配字典和提取特征标签范围有限等问题仍需在未来的研究中进一步探讨。 

---
# SODor: Long-Term EEG Partitioning for Seizure Onset Detection 

**Title (ZH)**: SODor：癫痫发作起始检测的长期脑电图分区方法 

**Authors**: Zheng Chen, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.15598)  

**Abstract**: Deep learning models have recently shown great success in classifying epileptic patients using EEG recordings. Unfortunately, classification-based methods lack a sound mechanism to detect the onset of seizure events. In this work, we propose a two-stage framework, \method, that explicitly models seizure onset through a novel task formulation of subsequence clustering. Given an EEG sequence, the framework first learns a set of second-level embeddings with label supervision. It then employs model-based clustering to explicitly capture long-term temporal dependencies in EEG sequences and identify meaningful subsequences. Epochs within a subsequence share a common cluster assignment (normal or seizure), with cluster or state transitions representing successful onset detections. Extensive experiments on three datasets demonstrate that our method can correct misclassifications, achieving 5%-11% classification improvements over other baselines and accurately detecting seizure onsets. 

**Abstract (ZH)**: 近年来，深度学习模型在使用脑电图（EEG）记录对癫痫患者进行分类方面表现出巨大的成功。遗憾的是，基于分类的方法缺乏有效的机制来检测癫痫发作事件的起始。本文提出了一种两阶段框架 \method，该框架通过一种新颖的子序列聚类任务形式显式地建模癫痫发作起始。给定一个EEG序列，该框架首先通过标签监督学习一组二级嵌入。然后，它使用基于模型的聚类来显式捕获EEG序列中的长期时间依赖性，并识别有意义的子序列。一个子序列内的时期共享一个共同的聚类分配（正常或癫痫发作），聚类或状态转换代表成功的起始检测。在三个数据集上的广泛实验表明，我们的方法可以纠正误分类，比其他基准模型实现5%-11%的分类性能提升，并且能够准确检测癫痫发作起始。 

---
# Mask-RadarNet: Enhancing Transformer With Spatial-Temporal Semantic Context for Radar Object Detection in Autonomous Driving 

**Title (ZH)**: Mask-RadarNet：通过空间-时间语义上下文增强变压器在自主驾驶雷达目标检测中的应用 

**Authors**: Yuzhi Wu, Jun Liu, Guangfeng Jiang, Weijian Liu, Danilo Orlando  

**Link**: [PDF](https://arxiv.org/pdf/2412.15595)  

**Abstract**: As a cost-effective and robust technology, automotive radar has seen steady improvement during the last years, making it an appealing complement to commonly used sensors like camera and LiDAR in autonomous driving. Radio frequency data with rich semantic information are attracting more and more attention. Most current radar-based models take radio frequency image sequences as the input. However, these models heavily rely on convolutional neural networks and leave out the spatial-temporal semantic context during the encoding stage. To solve these problems, we propose a model called Mask-RadarNet to fully utilize the hierarchical semantic features from the input radar data. Mask-RadarNet exploits the combination of interleaved convolution and attention operations to replace the traditional architecture in transformer-based models. In addition, patch shift is introduced to the Mask-RadarNet for efficient spatial-temporal feature learning. By shifting part of patches with a specific mosaic pattern in the temporal dimension, Mask-RadarNet achieves competitive performance while reducing the computational burden of the spatial-temporal modeling. In order to capture the spatial-temporal semantic contextual information, we design the class masking attention module (CMAM) in our encoder. Moreover, a lightweight auxiliary decoder is added to our model to aggregate prior maps generated from the CMAM. Experiments on the CRUW dataset demonstrate the superiority of the proposed method to some state-of-the-art radar-based object detection algorithms. With relatively lower computational complexity and fewer parameters, the proposed Mask-RadarNet achieves higher recognition accuracy for object detection in autonomous driving. 

**Abstract (ZH)**: 作为一项经济高效且稳健的技术，汽车雷达在过去几年中稳步改进，使其成为传统传感器（如摄像头和LiDAR）在自动驾驶中的一种 appealing 补充。丰富的射频频段数据引起了越来越多的关注。目前大多数基于雷达的模型将射频图像序列作为输入。然而，这些模型高度依赖于卷积神经网络，并在编码阶段忽略了空间-时间语义上下文。为了解决这些问题，我们提出了一种名为Mask-RadarNet的模型，以充分利用输入雷达数据中的层次语义特征。Mask-RadarNet通过利用交错卷积和注意力操作的结合来替代基于变换器模型的传统架构。此外，Mask-RadarNet中引入了块移位，以实现高效的时空特征学习。通过在时间维度上以特定马赛克模式移动部分块，Mask-RadarNet在降低时空建模计算负担的同时实现了竞争力的性能。为了捕捉空间-时间语义上下文信息，我们在编码器中设计了类掩蔽注意力模块（CMAM）。此外，我们还向模型中添加了一个轻量级辅助解码器，以聚集CMAM生成的先验图。在CRUW数据集上的实验展示了所提方法在一些最先进的雷达检测算法中的优越性。相较于较低的计算复杂度和较少的参数，所提出的Mask-RadarNet在自动驾驶中的对象检测中实现了更高的识别精度。 

---
# Machine Learning Techniques for Pattern Recognition in High-Dimensional Data Mining 

**Title (ZH)**: 高维数据挖掘中模式识别的机器学习技术 

**Authors**: Pochun Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15593)  

**Abstract**: This paper proposes a frequent pattern data mining algorithm based on support vector machine (SVM), aiming to solve the performance bottleneck of traditional frequent pattern mining algorithms in high-dimensional and sparse data environments. By converting the frequent pattern mining task into a classification problem, the SVM model is introduced to improve the accuracy and robustness of pattern extraction. In terms of method design, the kernel function is used to map the data to a high-dimensional feature space, so as to construct the optimal classification hyperplane, realize the nonlinear separation of patterns and the accurate mining of frequent items. In the experiment, two public datasets, Retail and Mushroom, were selected to compare and analyze the proposed algorithm with traditional FP-Growth, FP-Tree, decision tree and random forest models. The experimental results show that the algorithm in this paper is significantly better than the traditional model in terms of three key indicators: support, confidence and lift, showing strong pattern recognition ability and rule extraction effect. The study shows that the SVM model has excellent performance advantages in an environment with high data sparsity and a large number of transactions, and can effectively cope with complex pattern mining tasks. At the same time, this paper also points out the potential direction of future research, including the introduction of deep learning and ensemble learning frameworks to further improve the scalability and adaptability of the algorithm. This research not only provides a new idea for frequent pattern mining, but also provides important technical support for solving pattern discovery and association rule mining problems in practical applications. 

**Abstract (ZH)**: 本文提出了一种基于支持向量机（SVM）的频繁模式挖掘算法，旨在解决传统频繁模式挖掘算法在高维稀疏数据环境中性能瓶颈问题。通过将频繁模式挖掘任务转化为分类问题，引入SVM模型以提高模式提取的准确性和鲁棒性。在方法设计方面，使用核函数将数据映射到高维特征空间，从而构建最优分类超平面，实现模式的非线性分离和频繁项的精确挖掘。在实验中，选择了公共数据集Retail和Mushroom，将本文提出的算法与传统的FP-Growth、FP-Tree、决策树和随机森林模型进行比较和分析。实验结果表明，该算法在支持度、置信度和提升度三个关键指标上明显优于传统的模型，显示出强大的模式识别能力和规则提取效果。研究显示，SVM模型在高数据稀疏性和大量交易的数据环境中具有出色的表现优势，能够有效应对复杂的模式挖掘任务。同时，本文还指出了未来研究的潜在方向，包括引入深度学习和集成学习框架以进一步提高算法的可扩展性和适应性。这项研究不仅为频繁模式挖掘提供了一种新的思路，还为解决实际应用中的模式发现和关联规则挖掘问题提供了重要的技术支持。 

---
# Pre-training Graph Neural Networks on Molecules by Using Subgraph-Conditioned Graph Information Bottleneck 

**Title (ZH)**: 使用子图条件图信息瓶颈预训练分子图神经网络 

**Authors**: Van Thuy Hoang, O-Joun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2412.15589)  

**Abstract**: This study aims to build a pre-trained Graph Neural Network (GNN) model on molecules without human annotations or prior knowledge. Although various attempts have been proposed to overcome limitations in acquiring labeled molecules, the previous pre-training methods still rely on semantic subgraphs, i.e., functional groups. Only focusing on the functional groups could overlook the graph-level distinctions. The key challenge to build a pre-trained GNN on molecules is how to (1) generate well-distinguished graph-level representations and (2) automatically discover the functional groups without prior knowledge. To solve it, we propose a novel Subgraph-conditioned Graph Information Bottleneck, named S-CGIB, for pre-training GNNs to recognize core subgraphs (graph cores) and significant subgraphs. The main idea is that the graph cores contain compressed and sufficient information that could generate well-distinguished graph-level representations and reconstruct the input graph conditioned on significant subgraphs across molecules under the S-CGIB principle. To discover significant subgraphs without prior knowledge about functional groups, we propose generating a set of functional group candidates, i.e., ego networks, and using an attention-based interaction between the graph core and the candidates. Despite being identified from self-supervised learning, our learned subgraphs match the real-world functional groups. Extensive experiments on molecule datasets across various domains demonstrate the superiority of S-CGIB. 

**Abstract (ZH)**: 本研究旨在构建一种无需人类注释或先验知识的预训练图神经网络（GNN）模型，应用于分子领域。尽管已经提出了多种方法来克服获取标记分子的局限性，但之前的预训练方法仍然依赖于语义子图，即功能基团。仅关注功能基团可能会忽略图级差异。在分子上构建预训练GNN的关键挑战在于如何（1）生成良好的图级表示，并（2）在无先验知识的情况下自动发现功能基团。为了解决这一问题，我们提出了一种新颖的基于子图条件的图信息瓶颈（Subgraph-conditioned Graph Information Bottleneck，简称S-CGIB）方法，用于预训练GNN以识别核心子图（图核心）和显著子图。主要思想是图核心包含了压缩且足够的信息，可以根据显著子图在S-CGIB原则下生成良好的图级表示并重构输入图。为了在无功能基团先验知识的情况下发现显著子图，我们提出生成一组功能基团候选（即ego网络），并通过图核心与候选子图之间的基于注意力的交互来发现这些显著子图。尽管这些子图是从自监督学习中识别出来的，但我们学到的子图与实际世界的功能基团匹配。在不同领域下的分子数据集上的广泛实验表明，S-CGIB具有明显的优势。 

---
# Score-based Generative Diffusion Models for Social Recommendations 

**Title (ZH)**: 基于评分的生成扩散模型在社会推荐系统中的应用 

**Authors**: Chengyi Liu, Jiahao Zhang, Shijie Wang, Wenqi Fan, Qing Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15579)  

**Abstract**: With the prevalence of social networks on online platforms, social recommendation has become a vital technique for enhancing personalized recommendations. The effectiveness of social recommendations largely relies on the social homophily assumption, which presumes that individuals with social connections often share similar preferences. However, this foundational premise has been recently challenged due to the inherent complexity and noise present in real-world social networks. In this paper, we tackle the low social homophily challenge from an innovative generative perspective, directly generating optimal user social representations that maximize consistency with collaborative signals. Specifically, we propose the Score-based Generative Model for Social Recommendation (SGSR), which effectively adapts the Stochastic Differential Equation (SDE)-based diffusion models for social recommendations. To better fit the recommendation context, SGSR employs a joint curriculum training strategy to mitigate challenges related to missing supervision signals and leverages self-supervised learning techniques to align knowledge across social and collaborative domains. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach in filtering redundant social information and improving recommendation performance. 

**Abstract (ZH)**: 随着在线平台上社交网络的普及，社交推荐已成为增强个性化推荐的重要技术。社交推荐的有效性很大程度上依赖于社会同质性假设，即认为具有社会关系的个体往往具有相似的偏好。然而，这一基础假设由于现实世界社交网络中存在的内在复杂性和噪声而受到了挑战。在本文中，我们从创新的生成视角出发，直接生成最优的用户社会表示，以最大化与协作信号的一致性，从而应对低社会同质性挑战。具体而言，我们提出了基于评分的生成模型用于社交推荐（SGSR），该模型有效地利用了基于随机微分方程（SDE）的扩散模型来进行社交推荐。为了更好地适应推荐场景，SGSR采用了联合课程训练策略，以缓解监督信号缺失带来的挑战，并利用自监督学习技术来在社会和协同领域之间对齐知识。在真实数据集上的广泛实验表明，我们的方法在过滤冗余社交信息并提高推荐性能方面的有效性。 

---
# Continual Learning Using a Kernel-Based Method Over Foundation Models 

**Title (ZH)**: 使用核方法在基础模型上进行持续学习 

**Authors**: Saleh Momeni, Sahisnu Mazumder, Bing Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15571)  

**Abstract**: Continual learning (CL) learns a sequence of tasks incrementally. This paper studies the challenging CL setting of class-incremental learning (CIL). CIL has two key challenges: catastrophic forgetting (CF) and inter-task class separation (ICS). Despite numerous proposed methods, these issues remain persistent obstacles. This paper proposes a novel CIL method, called Kernel Linear Discriminant Analysis (KLDA), that can effectively avoid CF and ICS problems. It leverages only the powerful features learned in a foundation model (FM). However, directly using these features proves suboptimal. To address this, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random Fourier Features (RFF) to enhance the feature representations from the FM, leading to improved performance. When a new task arrives, KLDA computes only the mean for each class in the task and updates a shared covariance matrix for all learned classes based on the kernelized features. Classification is performed using Linear Discriminant Analysis. Our empirical evaluation using text and image classification datasets demonstrates that KLDA significantly outperforms baselines. Remarkably, without relying on replay data, KLDA achieves accuracy comparable to joint training of all classes, which is considered the upper bound for CIL performance. The KLDA code is available at this https URL. 

**Abstract (ZH)**: 连续学习（CL）能够在增量地学习一系列任务中逐步提升模型的能力。本文研究了类增量学习（CIL）这一具有挑战性的CL设置。CIL面临的两个关键挑战是灾难性遗忘（CF）和任务间类分离（ICS）。尽管已经提出了众多方法，但这些问题仍然是持久的障碍。本文提出了一种新的CIL方法，称为核线性判别分析（KLDA），能够有效地避免CF和ICS问题。该方法仅利用基础模型（FM）中学习的强大特征。然而，直接使用这些特征效果并不理想。为了解决这一问题，KLDA引入了Radial Basis Function（RBF）核及其随机傅里叶特征（RFF），以增强FM特征表示，从而提高性能。当新任务到来时，KLDA仅计算任务中每个类别的均值，并基于核化特征更新所有已学习类别的共享协方差矩阵。分类过程使用线性判别分析来进行。通过使用文本和图像分类数据集进行的实证评估表明，KLDA显著优于基线方法。更令人惊讶的是，在不依赖回放数据的情况下，KLDA的准确性与所有类别的联合训练相当，被认为是CIL性能的上限。KLDA的代码可在以下链接获取：[这里请提供网址]。 

---
# In-context Continual Learning Assisted by an External Continual Learner 

**Title (ZH)**: 基于外部持续学习者的上下文内持续学习 

**Authors**: Saleh Momeni, Sahisnu Mazumder, Zixuan Ke, Bing Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15563)  

**Abstract**: Existing continual learning (CL) methods mainly rely on fine-tuning or adapting large language models (LLMs). They still suffer from catastrophic forgetting (CF). Little work has been done to exploit in-context learning (ICL) to leverage the extensive knowledge within LLMs for CL without updating any parameters. However, incrementally learning each new task in ICL necessitates adding training examples from each class of the task to the prompt, which hampers scalability as the prompt length increases. This issue not only leads to excessively long prompts that exceed the input token limit of the underlying LLM but also degrades the model's performance due to the overextended context. To address this, we introduce InCA, a novel approach that integrates an external continual learner (ECL) with ICL to enable scalable CL without CF. The ECL is built incrementally to pre-select a small subset of likely classes for each test instance. By restricting the ICL prompt to only these selected classes, InCA prevents prompt lengths from becoming excessively long, while maintaining high performance. Experimental results demonstrate that InCA significantly outperforms existing CL baselines, achieving substantial performance gains. 

**Abstract (ZH)**: 现有的持续学习（CL）方法主要依赖于微调或适配大规模语言模型（LLMs），但仍然遭受严重的灾难性遗忘（CF）问题。很少有工作利用上下文学习（ICL）来利用LLMs中的广泛知识进行CL，而不更新任何参数。然而，ICL中增量学习每个新任务需要向提示中添加每个任务类别的训练示例，这会随着提示长度的增加而阻碍可扩展性。这一问题不仅导致提示过长，超出了底层LLM的输入标记限制，还会因过长的上下文而降低模型性能。为解决这一问题，我们介绍了一种名为InCA的新方法，该方法将外部持续学习器（ECL）与ICL集成，以实现无CF的可扩展CL。ECL是增量构建的，以预先选择每个测试实例可能性较高的小类别子集。通过将ICL提示仅限制在这些选定的类别中，InCA避免了提示长度变得过长，同时保持了高性能。实验结果表明，InCA显著优于现有的CL基准，实现了显著的性能提升。 

---
# Architecture-Aware Learning Curve Extrapolation via Graph Ordinary Differential Equation 

**Title (ZH)**: 基于架构的learning curve外推：通过图常微分方程的方法 

**Authors**: Yanna Ding, Zijie Huang, Xiao Shou, Yihang Guo, Yizhou Sun, Jianxi Gao  

**Link**: [PDF](https://arxiv.org/pdf/2412.15554)  

**Abstract**: Learning curve extrapolation predicts neural network performance from early training epochs and has been applied to accelerate AutoML, facilitating hyperparameter tuning and neural architecture search. However, existing methods typically model the evolution of learning curves in isolation, neglecting the impact of neural network (NN) architectures, which influence the loss landscape and learning trajectories. In this work, we explore whether incorporating neural network architecture improves learning curve modeling and how to effectively integrate this architectural information. Motivated by the dynamical system view of optimization, we propose a novel architecture-aware neural differential equation model to forecast learning curves continuously. We empirically demonstrate its ability to capture the general trend of fluctuating learning curves while quantifying uncertainty through variational parameters. Our model outperforms current state-of-the-art learning curve extrapolation methods and pure time-series modeling approaches for both MLP and CNN-based learning curves. Additionally, we explore the applicability of our method in Neural Architecture Search scenarios, such as training configuration ranking. 

**Abstract (ZH)**: 学习曲线外推可以预测从早期训练周期预测神经网络性能，并已被应用于加速自动化机器学习（AutoML），促进超参数调优和神经架构搜索。然而，现有的方法通常孤立地建模学习曲线的发展过程，忽略了神经网络（NN）架构的影响，这些架构影响损失景观和学习轨迹。在这项工作中，我们探索是否将神经网络架构的信息纳入学习曲线建模中，并研究如何有效集成这种架构信息。受到优化问题动力系统视角的启发，我们提出了一种新颖的包含架构意识的神经微分方程模型，以连续地预测学习曲线。我们通过实验展示了其捕捉波动学习曲线总体趋势的能力，并通过变分参数量化不确定性。我们的模型在MLP和CNN基础上的学习曲线外推中优于当前最先进的方法和纯时间序列建模方法。此外，我们探索了该方法在神经架构搜索（NAS）场景中的适用性，如训练配置排名。 

---
# NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning 

**Title (ZH)**: NGQA：个性化健康意识营养推理的营养图形问答基准 

**Authors**: Zheyuan Zhang, Yiyang Li, Nhi Ha Lan Le, Zehong Wang, Tianyi Ma, Vincent Galassi, Keerthiram Murugesan, Nuno Moniz, Werner Geyer, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye  

**Link**: [PDF](https://arxiv.org/pdf/2412.15547)  

**Abstract**: Diet plays a critical role in human health, yet tailoring dietary reasoning to individual health conditions remains a major challenge. Nutrition Question Answering (QA) has emerged as a popular method for addressing this problem. However, current research faces two critical limitations. On one hand, the absence of datasets involving user-specific medical information severely limits \textit{personalization}. This challenge is further compounded by the wide variability in individual health needs. On the other hand, while large language models (LLMs), a popular solution for this task, demonstrate strong reasoning abilities, they struggle with the domain-specific complexities of personalized healthy dietary reasoning, and existing benchmarks fail to capture these challenges. To address these gaps, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first graph question answering dataset designed for personalized nutritional health reasoning. NGQA leverages data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is healthy for a specific user, supported by explanations of the key contributing nutrients. The benchmark incorporates three question complexity settings and evaluates reasoning across three downstream tasks. Extensive experiments with LLM backbones and baseline models demonstrate that the NGQA benchmark effectively challenges existing models. In sum, NGQA addresses a critical real-world problem while advancing GraphQA research with a novel domain-specific benchmark. 

**Abstract (ZH)**: 饮食在人类健康中扮演着至关重要的角色，然而将饮食建议个性化以适应个体健康状况仍然面临重大挑战。营养问答（Nutrition Question Answering, NQA）已成为解决这一问题的一种流行方法。然而，当前的研究面临两个关键局限性。一方面，缺乏包含用户特定医疗信息的数据集严重限制了个性化能力。这一挑战进一步加剧了个体健康需求的广泛差异。另一方面，尽管大型语言模型（Large Language Models, LLMs）作为解决这一任务的一种流行方案，展示了强大的推理能力，但在处理个性化健康饮食推理的领域特定复杂性方面仍然存在问题，现有的基准测试未能捕捉这些挑战。为了弥补这些差距，我们引入了营养图问答（Nutritional Graph Question Answering, NGQA）基准测试，这是第一个用于个性化营养健康推理的图问答数据集。NGQA 利用了全国健康和营养检查调查（NHANES）和膳食研究中的食物和营养数据库（FNDDS）的数据，以评估特定用户是否健康食用某食品，并提供了关键贡献营养素的解释。基准测试包含了三个问题复杂度设置，并在三个下游任务上评估推理。大量的实验结果表明，NGQA 基准测试有效地挑战了现有模型。总之，NGQA 不仅解决了一个重要的实际问题，还通过一个新颖的领域特定基准测试推动了图问答（GraphQA）研究的发展。 

---
# VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving 

**Title (ZH)**: VLM-RL：统一的视觉语言模型与强化学习框架以实现安全自动驾驶 

**Authors**: Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, Sikai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.15544)  

**Abstract**: In recent years, reinforcement learning (RL)-based methods for learning driving policies have gained increasing attention in the autonomous driving community and have achieved remarkable progress in various driving scenarios. However, traditional RL approaches rely on manually engineered rewards, which require extensive human effort and often lack generalizability. To address these limitations, we propose \textbf{VLM-RL}, a unified framework that integrates pre-trained Vision-Language Models (VLMs) with RL to generate reward signals using image observation and natural language goals. The core of VLM-RL is the contrasting language goal (CLG)-as-reward paradigm, which uses positive and negative language goals to generate semantic rewards. We further introduce a hierarchical reward synthesis approach that combines CLG-based semantic rewards with vehicle state information, improving reward stability and offering a more comprehensive reward signal. Additionally, a batch-processing technique is employed to optimize computational efficiency during training. Extensive experiments in the CARLA simulator demonstrate that VLM-RL outperforms state-of-the-art baselines, achieving a 10.5\% reduction in collision rate, a 104.6\% increase in route completion rate, and robust generalization to unseen driving scenarios. Furthermore, VLM-RL can seamlessly integrate almost any standard RL algorithms, potentially revolutionizing the existing RL paradigm that relies on manual reward engineering and enabling continuous performance improvements. The demo video and code can be accessed at: this https URL. 

**Abstract (ZH)**: 近年来，基于强化学习（RL）的驾驶策略学习方法在自动驾驶领域获得了越来越多的关注，并在各种驾驶场景中取得了显著进展。然而，传统的RL方法依赖于手工设计的奖励函数，这需要大量的人力投入，并且往往缺乏泛化能力。为了解决这些局限性，我们提出了\textbf{VLM-RL}，一种综合了预训练的视觉语言模型（VLM）与强化学习的统一框架，通过图像观测和自然语言目标来生成奖励信号。VLM-RL的核心是对比语言目标（CLG）作为奖励的范式，使用正负语言目标来生成语义奖励。此外，我们还引入了一种分层奖励合成方法，将基于CLG的语义奖励与车辆状态信息结合，提高了奖励的稳定性，并提供了更全面的奖励信号。同时，我们采用批处理技术以优化训练过程中的计算效率。通过在CARLA模拟器中进行广泛实验，我们发现VLM-RL在最新基线方法中表现更优，实现了碰撞率降低10.5%，路线完成率提高104.6%，并且在未见过的驾驶场景中具备强大的泛化能力。此外，VLM-RL几乎可以无缝集成任何标准的RL算法，有潜力革命性地改变依赖于手工设计奖励的传统RL框架，并促进持续的性能改进。视频演示和代码可以在以下链接访问：this https URL。 

---
# ChangeDiff: A Multi-Temporal Change Detection Data Generator with Flexible Text Prompts via Diffusion Model 

**Title (ZH)**: ChangeDiff：一种通过扩散模型灵活使用文本提示的多时相变化检测数据生成器 

**Authors**: Qi Zang, Jiayi Yang, Shuang Wang, Dong Zhao, Wenjun Yi, Zhun Zhong  

**Link**: [PDF](https://arxiv.org/pdf/2412.15541)  

**Abstract**: Data-driven deep learning models have enabled tremendous progress in change detection (CD) with the support of pixel-level annotations. However, collecting diverse data and manually annotating them is costly, laborious, and knowledge-intensive. Existing generative methods for CD data synthesis show competitive potential in addressing this issue but still face the following limitations: 1) difficulty in flexibly controlling change events, 2) dependence on additional data to train the data generators, 3) focus on specific change detection tasks. To this end, this paper focuses on the semantic CD (SCD) task and develops a multi-temporal SCD data generator ChangeDiff by exploring powerful diffusion models. ChangeDiff innovatively generates change data in two steps: first, it uses text prompts and a text-to-layout (T2L) model to create continuous layouts, and then it employs layout-to-image (L2I) to convert these layouts into images. Specifically, we propose multi-class distribution-guided text prompts (MCDG-TP), allowing for layouts to be generated flexibly through controllable classes and their corresponding ratios. Subsequently, to generalize the T2L model to the proposed MCDG-TP, a class distribution refinement loss is further designed as training supervision. %For the former, a multi-classdistribution-guided text prompt (MCDG-TP) is proposed to complement via controllable classes and ratios. To generalize the text-to-image diffusion model to the proposed MCDG-TP, a class distribution refinement loss is designed as training supervision. For the latter, MCDG-TP in three modes is proposed to synthesize new layout masks from various texts. Our generated data shows significant progress in temporal continuity, spatial diversity, and quality realism, empowering change detectors with accuracy and transferability. The code is available at this https URL 

**Abstract (ZH)**: 数据驱动的深度学习模型在像素级注解的支持下，在变化检测（CD）方面取得了巨大的进展。然而，收集多样化数据并手动对其进行标注成本高、耗时且知识密集。现有的用于CD数据合成的生成方法在解决这一问题方面显示出竞争力，但仍面临以下局限性：1）灵活控制变化事件的难度；2）依赖额外数据来训练数据生成器；3）专注于特定的变化检测任务。为此，本文专注于语义变化检测（SCD）任务，并通过探索强大的扩散模型开发了一种多时相SCD数据生成器ChangeDiff。ChangeDiff创新性地通过两个步骤生成变化数据：首先，使用文本提示和文本转布局（T2L）模型生成连续布局；然后，使用布局转图像（L2I）将这些布局转换为图像。具体而言，我们提出了多类别分布指导的文本提示（MCDG-TP），通过控制类别及其相应比例灵活生成布局。随后，为使T2L模型适用于我们提出的MCDG-TP，设计了类别分布细化损失作为训练监督。此外，提出了三种模式的MCDG-TP，从不同文本中合成新的布局掩码。我们生成的数据在时间连续性、空间多样性和质量真实感方面取得了显著进展，赋予了变化检测器更高的准确性和可迁移性。代码见此网址：[提供网址] 

---
# FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF 

**Title (ZH)**: FedRLHF：一个保证收敛的联邦框架，用于保护隐私和个性化的意见收集引导强化学习（RLHF） 

**Authors**: Flint Xiaofeng Fan, Cheston Tan, Yew-Soon Ong, Roger Wattenhofer, Wei-Tsang Ooi  

**Link**: [PDF](https://arxiv.org/pdf/2412.15538)  

**Abstract**: In the era of increasing privacy concerns and demand for personalized experiences, traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges due to their reliance on centralized data. We introduce Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF enables collaborative policy learning across multiple clients without necessitating the sharing of raw data or human feedback, thereby ensuring robust privacy preservation. Leveraging federated reinforcement learning, each client integrates human feedback locally into their reward functions and updates their policies through personalized RLHF processes. We establish rigorous theoretical foundations for FedRLHF, providing convergence guarantees, and deriving sample complexity bounds that scale efficiently with the number of clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate that FedRLHF not only preserves user privacy but also achieves performance on par with centralized RLHF, while enhancing personalization across diverse client environments. 

**Abstract (ZH)**: 在日益增长的隐私关注和个性化体验需求背景下，传统的带人类反馈的强化学习（Reinforcement Learning with Human Feedback, RLHF）框架面临着重大挑战，因为它们依赖于中心化的数据。我们提出了带人类反馈的联邦强化学习（Federated Reinforcement Learning with Human Feedback, FedRLHF），这是一种新颖的框架，能够分散RLHF过程。FedRLHF允许多个客户端在无需共享原始数据或人类反馈的情况下进行合作策略学习，从而确保了强大的隐私保护。通过利用联邦强化学习，每个客户端都将其人类反馈本地化地集成到奖励函数中，并通过个性化的RLHF过程更新策略。我们为FedRLHF建立了严格的理论基础，提供了收敛保证，并推导出了与客户端数量呈高效缩放关系的样本复杂性边界。在MovieLens和IMDb数据集上的实证评估表明，FedRLHF不仅能够保护用户隐私，还能够达到中心化RLHF的性能水平，同时在多种客户端环境中增强了个性化能力。 

---
# Improved Forecasts of Global Extreme Marine Heatwaves Through a Physics-guided Data-driven Approach 

**Title (ZH)**: 通过物理导向的数据驱动方法改进全球极端海洋热浪的预报 

**Authors**: Ruiqi Shu, Hao Wu, Yuan Gao, Fanghua Xu, Ruijian Gou, Xiaomeng Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15532)  

**Abstract**: The unusually warm sea surface temperature events known as marine heatwaves (MHWs) have a profound impact on marine ecosystems. Accurate prediction of extreme MHWs has significant scientific and financial worth. However, existing methods still have certain limitations, especially in the most extreme MHWs. In this study, to address these issues, based on the physical nature of MHWs, we created a novel deep learning neural network that is capable of accurate 10-day MHW forecasting. Our framework significantly improves the forecast ability of extreme MHWs through two specially designed modules inspired by numerical models: a coupler and a probabilistic data argumentation. The coupler simulates the driving effect of atmosphere on MHWs while the probabilistic data argumentation approaches significantly boost the forecast ability of extreme MHWs based on the idea of ensemble forecast. Compared with traditional numerical prediction, our framework has significantly higher accuracy and requires fewer computational resources. What's more, explainable AI methods show that wind forcing is the primary driver of MHW evolution and reveal its relation with air-sea heat exchange. Overall, our model provides a framework for understanding MHWs' driving processes and operational forecasts in the future. 

**Abstract (ZH)**: 被称为海洋热浪（MHWs）的异常温暖的海面温度事件对海洋生态系统产生了深远影响。准确预测极端MHWs具有重要的科学和经济价值。然而，现有方法在预测极端MHWs方面仍存在一定的局限性。为解决这些问题，本研究基于MHWs的物理特性，开发了一种新型的深度学习神经网络，能够实现10天期的MHW准确预报。我们的框架通过两个由数值模型启发、专门设计的模块显著提高了极端MHWs的预报能力：耦合器和概率数据增强。耦合器模拟了大气对MHWs的驱动作用，而概率数据增强采用集合预报的理念，显著提升了极端MHWs的预报能力。与传统数值预测方法相比，我们的框架具有更高的准确性和更少的计算资源需求。此外，可解释AI方法表明，风强迫是MHW演变的主要驱动力，并揭示了其与空气-海面热交换的关系。总体而言，我们的模型为理解和未来操作预报MHW驱动过程提供了框架。 

---
# XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation 

**Title (ZH)**: XRAG: 考察核心组件——高级检索增强生成中的基础组件基准测试 

**Authors**: Qianren Mao, Yangyifei Luo, Jinlong Zhang, Hanwen Hao, Zhilong Cao, Xiaolong Wang, Xiao Guan, Zhenting Huang, Weifeng Jiang, Shuyu Guo, Zhentao Han, Qili Zhang, Siyuan Tao, Yujie Liu, Junnan Liu, Zhixing Tan, Jie Sun, Bo Li, Xudong Liu, Richong Zhang, Jianxin Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15529)  

**Abstract**: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and this http URL introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. Given the escalating complexity of RAG systems, we underscore the necessity of identifying potential failure points of RAG modules. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in the engineering of RAG modules. Subsequently, we proffer bespoke solutions that are designed to augment the validation processes and bolster the overall performance of these modules. Our work thoroughly evaluates the performance of core advanced components in RAG systems, providing insights into optimizations for prevalent failure points. 

**Abstract (ZH)**: 检索增强生成（RAG）结合了与大型语言模型（LLM）的生成能力相关的相关数据检索，确保生成的输出不仅具有上下文相关性，而且准确可靠。本文介绍了一个开源且模块化的代码库XRAG，该代码库有助于全面评估先进RAG模块基础组件的性能。这些组件被系统地归类为四个核心阶段：预检索、检索、后检索和生成。我们通过对重新配置的数据集进行系统分析，提供了它们有效性的全面基准。鉴于RAG系统的日益复杂，我们强调识别RAG模块潜在故障点的必要性。我们提出了实验方法学和诊断测试协议，以剖析RAG模块工程中的潜在故障点。随后，我们提出了一套定制化的解决方案，旨在增强这些模块的验证过程并提高其整体性能。我们的工作全面评估了RAG系统核心组件的性能，提供了对未来广泛故障点的优化见解。 

---
# Generalized Back-Stepping Experience Replay in Sparse-Reward Environments 

**Title (ZH)**: 广义回步经验回放应用于稀疏奖励环境 

**Authors**: Guwen Lyu, Masahiro Sato  

**Link**: [PDF](https://arxiv.org/pdf/2412.15525)  

**Abstract**: Back-stepping experience replay (BER) is a reinforcement learning technique that can accelerate learning efficiency in reversible environments. BER trains an agent with generated back-stepping transitions of collected experiences and normal forward transitions. However, the original algorithm is designed for a dense-reward environment that does not require complex exploration, limiting the BER technique to demonstrate its full potential. Herein, we propose an enhanced version of BER called Generalized BER (GBER), which extends the original algorithm to sparse-reward environments, particularly those with complex structures that require the agent to explore. GBER improves the performance of BER by introducing relabeling mechanism and applying diverse sampling strategies. We evaluate our modified version, which is based on a goal-conditioned deep deterministic policy gradient offline learning algorithm, across various maze navigation environments. The experimental results indicate that the GBER algorithm can significantly boost the performance and stability of the baseline algorithm in various sparse-reward environments, especially those with highly structural symmetricity. 

**Abstract (ZH)**: 自返经验重放（BER）是一种强化学习技术，可以在可逆环境中加速学习效率。BER通过使用生成的反向过渡和收集的经验的正向过渡来训练智能体。然而，原始算法设计用于奖励密集的环境，不需要复杂的探索，这限制了BER技术的潜力。为了解决这一问题，我们提出了一种增强版本的BER，称为广义自返经验重放（GBER）。GBER将原始算法扩展到稀疏奖励环境，特别是那些复杂结构需要智能体进行探索的环境。GBER通过引入重新标签机制并应用多样化的采样策略来提高BER的性能。我们基于目标条件的深度确定性策略梯度的离线学习算法对修改后的GBER进行了评估，并在多种迷宫导航环境中进行了测试。实验结果表明，GBER算法在各种稀疏奖励环境中可以显著提升基线算法的性能和稳定性，特别是在高度结构对称性环境中表现尤为明显。 

---
# HREF: Human Response-Guided Evaluation of Instruction Following in Language Models 

**Title (ZH)**: 人类响应引导的语言模型指令遵循评估 

**Authors**: Xinxi Lyu, Yizhong Wang, Hannaneh Hajishirzi, Pradeep Dasigi  

**Link**: [PDF](https://arxiv.org/pdf/2412.15524)  

**Abstract**: Evaluating the capability of Large Language Models (LLMs) in following instructions has heavily relied on a powerful LLM as the judge, introducing unresolved biases that deviate the judgments from human judges. In this work, we reevaluate various choices for automatic evaluation on a wide range of instruction-following tasks. We experiment with methods that leverage human-written responses and observe that they enhance the reliability of automatic evaluations across a wide range of tasks, resulting in up to a 3.2% improvement in agreement with human judges. We also discovered that human-written responses offer an orthogonal perspective to model-generated responses in following instructions and should be used as an additional context when comparing model responses. Based on these observations, we develop a new evaluation benchmark, Human Response-Guided Evaluation of Instruction Following (HREF), comprising 4,258 samples across 11 task categories with a composite evaluation setup, employing a composite evaluation setup that selects the most reliable method for each category. In addition to providing reliable evaluation, HREF emphasizes individual task performance and is free from contamination. Finally, we study the impact of key design choices in HREF, including the size of the evaluation set, the judge model, the baseline model, and the prompt template. We host a live leaderboard that evaluates LLMs on the private evaluation set of HREF. 

**Abstract (ZH)**: 大型语言模型（LLMs）在遵循指令方面的能力评估很大程度上依赖于一个强大的LLM作为评判者，这引入了未解决的偏差，使判断偏离了人类评判者的标准。在这项工作中，我们重新评估了多种自动评估方法在广泛范围内的指令遵循任务中的适用性。我们尝试利用人工撰写的响应，发现这些方法在广泛任务中提高了自动评估的可靠性，结果与人类评判者的同意率达到最高3.2%的提升。我们还发现，人工撰写的响应为指令遵循提供了与模型生成响应不同的视角，并且在比较模型响应时应被视为额外的上下文。基于这些观察，我们开发了一种新的评估基准——指导性指令遵循的人类响应引导评估（HREF），该基准包括4,258个样本，涵盖11个任务类别，并采用综合评估设置来选择每个类别中最可靠的方法。除了提供可靠的评估外，HREF还强调了个体任务的表现，并且不受污染。最后，我们研究了HREF中关键设计选择的影响，包括评估集的规模、评判模型、基线模型和提示模板。我们提供了一个实时排行榜，用于对HREF的私人评估集进行LLM的评估。 

---
# InstructOCR: Instruction Boosting Scene Text Spotting 

**Title (ZH)**: InstructOCR：指令增强场景文本检测 

**Authors**: Chen Duan, Qianyi Jiang, Pei Fu, Jiamin Chen, Shengxi Li, Zining Wang, Shan Guo, Junfeng Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.15523)  

**Abstract**: In the field of scene text spotting, previous OCR methods primarily relied on image encoders and pre-trained text information, but they often overlooked the advantages of incorporating human language instructions. To address this gap, we propose InstructOCR, an innovative instruction-based scene text spotting model that leverages human language instructions to enhance the understanding of text within images. Our framework employs both text and image encoders during training and inference, along with instructions meticulously designed based on text attributes. This approach enables the model to interpret text more accurately and flexibly. Extensive experiments demonstrate the effectiveness of our model and we achieve state-of-the-art results on widely used benchmarks. Furthermore, the proposed framework can be seamlessly applied to scene text VQA tasks. By leveraging instruction strategies during pre-training, the performance on downstream VQA tasks can be significantly improved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on the ST-VQA dataset. These experimental results provide insights into the benefits of incorporating human language instructions for OCR-related tasks. 

**Abstract (ZH)**: 在场景文本检测领域，以往的OCR方法主要依赖图像编码器和预训练的文字信息，但往往忽视了结合人类语言指令的优势。为弥补这一不足，我们提出了一种基于指令的场景文本检测模型InstructOCR，该模型利用人类语言指令来增强对图像中文字的理解。我们的框架在训练和推理过程中同时使用了文本和图像编码器，并且指令是根据文字属性精心设计的。这种方法使得模型能够更准确、更灵活地理解文本。广泛进行的实验表明，我们的模型具有显著的效果，并在广泛使用的基准测试上达到了最先进水平。此外，所提出的框架还可以无缝应用于场景文本的VQA任务。通过在预训练阶段利用指令策略，可以显著提高下游VQA任务的性能，在TextVQA数据集上提高了2.6%，在ST-VQA数据集上提高了2.1%。这些实验结果为将人类语言指令应用于OCR相关任务提供了有益的见解。 

---
# RESQUE: Quantifying Estimator to Task and Distribution Shift for Sustainable Model Reusability 

**Title (ZH)**: RESQUE: 评估估计器的任务和分布转移对可持续模型重用的量化方法 

**Authors**: Vishwesh Sangarya, Jung-Eun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2412.15511)  

**Abstract**: As a strategy for sustainability of deep learning, reusing an existing model by retraining it rather than training a new model from scratch is critical. In this paper, we propose REpresentation Shift QUantifying Estimator (RESQUE), a predictive quantifier to estimate the retraining cost of a model to distributional shifts or change of tasks. It provides a single concise index for an estimate of resources required for retraining the model. Through extensive experiments, we show that RESQUE has a strong correlation with various retraining measures. Our results validate that RESQUE is an effective indicator in terms of epochs, gradient norms, changes of parameter magnitude, energy, and carbon emissions. These measures align well with RESQUE for new tasks, multiple noise types, and varying noise intensities. As a result, RESQUE enables users to make informed decisions for retraining to different tasks/distribution shifts and determine the most cost-effective and sustainable option, allowing for the reuse of a model with a much smaller footprint in the environment. The code for this work is available here: this https URL 

**Abstract (ZH)**: 作为深度学习可持续性的策略之一，通过重新训练现有模型而不是从零开始训练新模型至关重要。在本文中，我们提出了REpresentation Shift QUantifying Estimator (RESQUE)——一种预测性度量工具，用于估算模型在面对数据分布变化或任务变更时所需的重新训练成本。它提供了一个简洁的指标，用于估算重新训练所需资源。通过广泛的实验，我们证明RESQUE与多种重新训练度量存在强烈的相关性。我们的结果验证了RESQUE在不同任务、多种噪声类型以及不同噪声强度下均是一个有效的指标，其与所需迭代次数、梯度范数、参数规模变化、能源消耗和碳排放等指标高度一致。这些度量指标在新任务、多种噪声类型和不同噪声强度下与RESQUE高度吻合。因此，RESQUE可以帮助用户在重新训练以应对不同任务/数据分布变化时做出明智的决策，从而确定成本效益最高且最为可持续的选择，使模型在环境影响更小的前提下得到重复使用。本文的代码可在以下链接获取：this https URL 

---
# Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models 

**Title (ZH)**: 大型语言模型中拟人类认知模式的涌现现象 

**Authors**: Zhisheng Tang, Mayank Kejriwal  

**Link**: [PDF](https://arxiv.org/pdf/2412.15501)  

**Abstract**: Research on emergent patterns in Large Language Models (LLMs) has gained significant traction in both psychology and artificial intelligence, motivating the need for a comprehensive review that offers a synthesis of this complex landscape. In this article, we systematically review LLMs' capabilities across three important cognitive domains: decision-making biases, reasoning, and creativity. We use empirical studies drawing on established psychological tests and compare LLMs' performance to human benchmarks. On decision-making, our synthesis reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent, indicating cognitive patterns that only partially align with human decision-making. On reasoning, advanced LLMs like GPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while smaller models fall short of human-level performance. A distinct dichotomy emerges in creativity: while LLMs excel in language-based creative tasks, such as storytelling, they struggle with divergent thinking tasks that require real-world context. Nonetheless, studies suggest that LLMs hold considerable potential as collaborators, augmenting creativity in human-machine problem-solving settings. Discussing key limitations, we also offer guidance for future research in areas such as memory, attention, and open-source model development. 

**Abstract (ZH)**: 对大型语言模型（LLMs）中涌现模式的研究在心理学和人工智能领域引起了广泛关注，推动了对其复杂领域的全面综述需求，进而提炼出该领域的综合视角。本文系统地回顾了LLMs在三个重要认知领域的能力：决策偏差、推理和创造力。我们利用基于公认的心理学测试的实证研究，将LLMs的表现与人类基准进行比较。在决策方面，我们的综合分析表明，尽管LLMs表现出一些类似人类的偏见，但观察到的一些人类偏见未在LLMs中出现，这表明这些认知模式与人类决策的对齐是部分的。在推理方面，先进的LLMs如GPT-4展示了类似人类系统2层次推理的反思性推理，而较小的模型则未能达到人类水平的表现。在创造力方面，LLMs在语言为基础的创造任务，如讲故事方面表现出色，但在需要现实世界背景的发散思维任务方面存在困难。尽管如此，研究表明，LLMs在人类-机器问题解决场景中作为合作者具有巨大的合作潜力，能够增强创造力。讨论关键限制后，我们还为未来的研究提供指导，这些领域包括记忆、注意力以及开源模型的开发。 

---
# A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations 

**Title (ZH)**: 一种具有可解释RBF分类器基础的稳健原型网络 

**Authors**: Sascha Saralajew, Ashish Rana, Thomas Villmann, Ammar Shaker  

**Link**: [PDF](https://arxiv.org/pdf/2412.15499)  

**Abstract**: Prototype-based classification learning methods are known to be inherently interpretable. However, this paradigm suffers from major limitations compared to deep models, such as lower performance. This led to the development of the so-called deep Prototype-Based Networks (PBNs), also known as prototypical parts models. In this work, we analyze these models with respect to different properties, including interpretability. In particular, we focus on the Classification-by-Components (CBC) approach, which uses a probabilistic model to ensure interpretability and can be used as a shallow or deep architecture. We show that this model has several shortcomings, like creating contradicting explanations. Based on these findings, we propose an extension of CBC that solves these issues. Moreover, we prove that this extension has robustness guarantees and derive a loss that optimizes robustness. Additionally, our analysis shows that most (deep) PBNs are related to (deep) RBF classifiers, which implies that our robustness guarantees generalize to shallow RBF classifiers. The empirical evaluation demonstrates that our deep PBN yields state-of-the-art classification accuracy on different benchmarks while resolving the interpretability shortcomings of other approaches. Further, our shallow PBN variant outperforms other shallow PBNs while being inherently interpretable and exhibiting provable robustness guarantees. 

**Abstract (ZH)**: 原型基分类学习方法因其固有的可解释性而闻名。然而，与深层模型相比，这种范式存在显著的局限性，例如性能较低。这导致开发出了所谓的深度原型网络（Deep Prototype-Based Networks，简称PBNs），也被称为原型部分模型。在本文中，我们从不同的属性角度分析了这些模型，包括可解释性。特别是，我们重点分析了Component-Based Classification（CBC）方法，该方法使用概率模型确保可解释性，并且可以作为一种浅层或深层架构使用。研究结果表明，该模型存在一些缺点，如产生矛盾的解释。基于这些发现，我们提出了一种CBC的扩展，解决了这些问题。此外，我们证明了这种扩展具有鲁棒性的保证，并推导出一个优化鲁棒性的损失函数。此外，我们的分析表明，大多数（深层）PBNs都与（深层）径向基函数（RBF）分类器相关，这意味着我们的鲁棒性保证同样适用于浅层RBF分类器。实验评估表明，我们提出的深层PBN在不同基准上的分类精度达到最佳，同时解决了其他方法中的可解释性问题。另外，我们的浅层PBN变体在保持固有可解释性的同时，表现出可验证的鲁棒性保证，并且优于其他浅层PBN。 

---
# The First Multilingual Model For The Detection of Suicide Texts 

**Title (ZH)**: 首个用于自杀文本检测的多语言模型 

**Authors**: Rodolfo Zevallos, Annika Schoene, John E. Ortega  

**Link**: [PDF](https://arxiv.org/pdf/2412.15498)  

**Abstract**: Suicidal ideation is a serious health problem affecting millions of people worldwide. Social networks provide information about these mental health problems through users' emotional expressions. We propose a multilingual model leveraging transformer architectures like mBERT, XML-R, and mT5 to detect suicidal text across posts in six languages - Spanish, English, German, Catalan, Portuguese and Italian. A Spanish suicide ideation tweet dataset was translated into five other languages using SeamlessM4T. Each model was fine-tuned on this multilingual data and evaluated across classification metrics. Results showed mT5 achieving the best performance overall with F1 scores above 85%, highlighting capabilities for cross-lingual transfer learning. The English and Spanish translations also displayed high quality based on perplexity. Our exploration underscores the importance of considering linguistic diversity in developing automated multilingual tools to identify suicidal risk. Limitations exist around semantic fidelity in translations and ethical implications which provide guidance for future human-in-the-loop evaluations. 

**Abstract (ZH)**: 自杀意念是全球影响数百万人的一个严重健康问题。社交媒体通过用户的感情表达提供了关于这类精神健康问题的信息。我们提出了一种多语言模型，利用像mBERT、XML-R和mT5这样的变压器架构，在六种语言——西班牙语、英语、德语、加泰罗尼亚语、葡萄牙语和意大利语——的帖子中检测自杀性文字。使用SeamlessM4T将一个西班牙语的自杀意念推文数据集翻译成其他五种语言。每个模型均在多语言数据上进行了微调，并根据分类指标进行了评估。结果表明，mT5在总体上表现最佳，F1分数高于85%，突显了跨语言迁移学习的能力。英语和西班牙语的翻译还显示了高质量的表现，基于困惑度。我们的研究强调了在开发自动化的多语言工具以识别自杀风险时考虑语言多样性的重要性。存在的局限包括翻译的语义忠实性和伦理问题，这些为未来的人机协作评估提供了指导。 

---
# Lexicography Saves Lives (LSL): Automatically Translating Suicide-Related Language 

**Title (ZH)**: 词典编纂拯救生命 (LSL): 自杀相关语言的自动翻译 

**Authors**: Annika Marie Schoene, John E. Ortega, Rodolfo Joel Zevallos, Laura Haaber Ihle  

**Link**: [PDF](https://arxiv.org/pdf/2412.15497)  

**Abstract**: Recent years have seen a marked increase in research that aims to identify or predict risk, intention or ideation of suicide. The majority of new tasks, datasets, language models and other resources focus on English and on suicide in the context of Western culture. However, suicide is global issue and reducing suicide rate by 2030 is one of the key goals of the UN's Sustainable Development Goals. Previous work has used English dictionaries related to suicide to translate into different target languages due to lack of other available resources. Naturally, this leads to a variety of ethical tensions (e.g.: linguistic misrepresentation), where discourse around suicide is not present in a particular culture or country. In this work, we introduce the 'Lexicography Saves Lives Project' to address this issue and make three distinct contributions. First, we outline ethical consideration and provide overview guidelines to mitigate harm in developing suicide-related resources. Next, we translate an existing dictionary related to suicidal ideation into 200 different languages and conduct human evaluations on a subset of translated dictionaries. Finally, we introduce a public website to make our resources available and enable community participation. 

**Abstract (ZH)**: 近年来，旨在识别或预测自杀风险、意图或想法的研究显著增多。大多数新的任务、数据集、语言模型和其他资源主要集中在英语和西方文化背景下的自杀问题上。然而，自杀是一个全球性问题，减少到2030年的自杀率是联合国可持续发展目标的重要目标之一。以往的工作因缺乏其他可用资源，而不得不使用与自杀相关的英语词典进行翻译。自然地，这导致了一系列伦理问题（例如：语言误解），其中有关自杀的讨论在某个特定的文化或国家中不存在。在本研究中，我们提出了“词典拯救生命项目”以解决这一问题，并做出了三方面的贡献。首先，我们概述了伦理考虑，并提供了缓解自杀相关资源开发中潜在危害的总体指导原则。其次，我们将一个现有关于自杀意念的词典翻译成200种不同的语言，并对翻译后的词典进行了部分的人类评估。最后，我们推出一个公共网站，发布我们的资源以供公众使用，并鼓励社区参与。 

---
# TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use 

**Title (ZH)**: TL-训练：一种基于任务特征的大型语言模型训练框架，用于工具使用场景 

**Authors**: Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du  

**Link**: [PDF](https://arxiv.org/pdf/2412.15495)  

**Abstract**: Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with external environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of distinct categories. Building on these findings, we propose TL-Training, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four diverse open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. The code and data are available at this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）通过利用工具与外部环境交互来实现显著进步，这是向通用人工智能迈出的关键一步。然而，依赖大规模数据集的标准监督微调（SFT）方法往往会忽略工具使用中的任务特定特征，导致性能瓶颈。为了解决这一问题，我们分析了三个现有的LLM，并发现了关键洞察：训练数据可能会无意中妨碍工具使用行为，词汇的重要性分布不均，工具调用错误归类于少数几种不同的类别。基于这些发现，我们提出了一种基于任务特征的培训方法（TL-Training），该方法减轻了不良训练数据的影响，动态调整令牌权重以在SFT过程中优先考虑关键令牌，并结合了一种针对错误类别的稳健奖励机制，该机制通过最近邻策略优化（Proximal Policy Optimization, PPO）进行优化。我们通过训练CodeLLaMA-2-7B并在四个不同的开源测试集上进行评估，验证了TL-Training的有效性。结果显示，通过我们的方法训练的LLM仅使用1,217个训练数据点即可达到或超过开源和封闭源LLM在工具使用性能上的表现。此外，我们的方法增强了模型在嘈杂环境下的鲁棒性，并提高了通用任务性能，为LLM中的工具使用培训提供了可扩展且高效的范式。相关代码和数据可在以下链接获取：[此 https URL]。 

---
# Task-Specific Preconditioner for Cross-Domain Few-Shot Learning 

**Title (ZH)**: 针对跨域少样本学习的任务特定预条件处理方法 

**Authors**: Suhyun Kang, Jungwon Park, Wonseok Lee, Wonjong Rhee  

**Link**: [PDF](https://arxiv.org/pdf/2412.15483)  

**Abstract**: Cross-Domain Few-Shot Learning~(CDFSL) methods typically parameterize models with task-agnostic and task-specific parameters. To adapt task-specific parameters, recent approaches have utilized fixed optimization strategies, despite their potential sub-optimality across varying domains or target tasks. To address this issue, we propose a novel adaptation mechanism called Task-Specific Preconditioned gradient descent~(TSP). Our method first meta-learns Domain-Specific Preconditioners~(DSPs) that capture the characteristics of each meta-training domain, which are then linearly combined using task-coefficients to form the Task-Specific Preconditioner. The preconditioner is applied to gradient descent, making the optimization adaptive to the target task. We constrain our preconditioners to be positive definite, guiding the preconditioned gradient toward the direction of steepest descent. Empirical evaluations on the Meta-Dataset show that TSP achieves state-of-the-art performance across diverse experimental scenarios. 

**Abstract (ZH)**: 跨域少样本学习（CDFSL）方法通常使用任务无关参数和任务特定参数来参数化模型。为了适应任务特定参数，近期的方法使用了固定的优化策略，尽管这些策略在不同领域或目标任务之间可能存在次优情况。为了解决这一问题，我们提出了一种新型的适应机制，称为任务特定预条件梯度下降（TSP）。我们的方法首先元学习领域特定预条件器（DSPs），这些预条件器捕捉每个元训练领域的特点，然后通过任务系数进行线性组合，形成任务特定预条件器。预条件器应用于梯度下降，从而使优化适应目标任务。我们将预条件器限制为正定的，以引导预条件梯度向最陡下降方向发展。实验评估表明，TSP在元数据集上的表现优于现有方法，在多种实验场景中达到了最先进的性能。 

---
# Continual Learning Using Only Large Language Model Prompting 

**Title (ZH)**: 仅使用大型语言模型提示的持续学习 

**Authors**: Jiabao Qiu, Zixuan Ke, Bing Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15479)  

**Abstract**: We introduce CLOB, a novel continual learning (CL) paradigm wherein a large language model (LLM) is regarded as a black box. Learning is done incrementally via only verbal prompting. CLOB does not fine-tune any part of the LLM or add any trainable parameters to it. It is particularly suitable for LLMs that are accessible via APIs. We also propose a new CL technique, called CIS, based on incremental summarization that also overcomes the LLM's input length limit. Experiments show CIS outperforms baselines by a very large margin. 

**Abstract (ZH)**: 我们介绍了一种新的连续学习（CL）范式CLOB，其中大型语言模型（LLM）被视为一个黑盒。学习是通过仅使用口头提示逐步进行的。CLOB 不对LLM的任何部分进行微调，也不向其中添加任何可训练参数。它特别适用于通过API访问的LLM。我们还提出了一种基于逐步总结的新CL技术CIS，该技术也克服了LLM的输入长度限制。实验结果显示，CIS相比于基线方法具有非常显著的优越性。 

---
# Difficulty-aware Balancing Margin Loss for Long-tailed Recognition 

**Title (ZH)**: 长尾识别中的难度感知边际损失平衡方法 

**Authors**: Minseok Son, Inyong Koo, Jinyoung Park, Changick Kim  

**Link**: [PDF](https://arxiv.org/pdf/2412.15477)  

**Abstract**: When trained with severely imbalanced data, deep neural networks often struggle to accurately recognize classes with only a few samples. Previous studies in long-tailed recognition have attempted to rebalance biased learning using known sample distributions, primarily addressing different classification difficulties at the class level. However, these approaches often overlook the instance difficulty variation within each class. In this paper, we propose a difficulty-aware balancing margin (DBM) loss, which considers both class imbalance and instance difficulty. DBM loss comprises two components: a class-wise margin to mitigate learning bias caused by imbalanced class frequencies, and an instance-wise margin assigned to hard positive samples based on their individual difficulty. DBM loss improves class discriminativity by assigning larger margins to more difficult samples. Our method seamlessly combines with existing approaches and consistently improves performance across various long-tailed recognition benchmarks. 

**Abstract (ZH)**: 在严重不平衡数据下训练时，深度神经网络往往难以准确识别仅有少量样本的类别。在长尾识别的研究中，先前的工作主要通过调整已知样本分布来重构偏倚学习，以在分类层面解决不同的分类难题。然而，这些方法往往忽视了同一类别内部实例难度的差异性。在本文中，我们提出了一种难度感知平衡边际（Difficulty-Aware Balancing Margin, DBM）损失，它同时考虑了类别不平衡和实例难度。DBM损失包含两个组成部分：类别间的边际，用于减轻由类别频数不平衡引起的学习偏倚；以及根据各自难度分派给困难正样本的实例间的边际。DBM损失通过给更难的样本分配更大的边际来提高类别区分性。我们的方法能够与现有方法无缝整合，并在各种长尾识别基准测试中持续提升性能。 

---
# Non-Uniform Parameter-Wise Model Merging 

**Title (ZH)**: 非均匀参数级模型合并 

**Authors**: Albert Manuel Orozco Camacho, Stefan Horoi, Guy Wolf, Eugene Belilovsky  

**Link**: [PDF](https://arxiv.org/pdf/2412.15467)  

**Abstract**: Combining multiple machine learning models has long been a technique for enhancing performance, particularly in distributed settings. Traditional approaches, such as model ensembles, work well, but are expensive in terms of memory and compute. Recently, methods based on averaging model parameters have achieved good results in some settings and have gained popularity. However, merging models initialized differently that do not share a part of their training trajectories can yield worse results than simply using the base models, even after aligning their neurons. In this paper, we introduce a novel approach, Non-uniform Parameter-wise Model Merging, or NP Merge, which merges models by learning the contribution of each parameter to the final model using gradient-based optimization. We empirically demonstrate the effectiveness of our method for merging models of various architectures in multiple settings, outperforming past methods. We also extend NP Merge to handle the merging of multiple models, showcasing its scalability and robustness. 

**Abstract (ZH)**: 将多个机器学习模型结合起来以提升性能在分布式环境中一直是一种有效的技术。传统方法，如模型集成，效果良好，但对内存和计算资源的需求较高。最近，基于平均模型参数的方法在某些场景下取得了良好的效果，并越来越受到欢迎。然而，合并那些在训练轨迹上不共享部分信息并且初始条件不同的模型，可能会导致最终效果不如直接使用基模型，即使在对神经元进行了对齐之后也是如此。在本文中，我们提出了一种新颖的方法，名为非均匀参数级模型合并（Non-uniform Parameter-wise Model Merging，简称NP Merge），该方法通过基于梯度的优化来学习每个参数对最终模型的贡献。我们实证展示了该方法在多种架构的模型合并中表现出色，优于以往的方法，并且还扩展了NP Merge以处理多个模型的合并，展示了其可扩展性和鲁棒性。 

---
# TalkWithMachines: Enhancing Human-Robot Interaction for Interpretable Industrial Robotics Through Large/Vision Language Models 

**Title (ZH)**: 机器与人对话：通过大规模/视觉语言模型提升可解释的工业机器人交互 

**Authors**: Ammar N. Abbas, Csaba Beleznai  

**Link**: [PDF](https://arxiv.org/pdf/2412.15462)  

**Abstract**: TalkWithMachines aims to enhance human-robot interaction by contributing to interpretable industrial robotic systems, especially for safety-critical applications. The presented paper investigates recent advancements in Large Language Models (LLMs) and Vision Language Models (VLMs), in combination with robotic perception and control. This integration allows robots to understand and execute commands given in natural language and to perceive their environment through visual and/or descriptive inputs. Moreover, translating the LLM's internal states and reasoning into text that humans can easily understand ensures that operators gain a clearer insight into the robot's current state and intentions, which is essential for effective and safe operation. Our paper outlines four LLM-assisted simulated robotic control workflows, which explore (i) low-level control, (ii) the generation of language-based feedback that describes the robot's internal states, (iii) the use of visual information as additional input, and (iv) the use of robot structure information for generating task plans and feedback, taking the robot's physical capabilities and limitations into account. The proposed concepts are presented in a set of experiments, along with a brief discussion. Project description, videos, and supplementary materials will be available on the project website: this https URL. 

**Abstract (ZH)**: TalkWithMachines致力于通过推进可解释的工业机器人系统来增强人机交互，尤其适用于关键安全应用。本论文探讨了大型语言模型（LLMs）和视觉语言模型（VLMs）的最新进展，并将其与机器人感知和控制相结合。这种整合使机器人能够理解并执行以自然语言给出的指令，并通过视觉和/或描述性输入感知其环境。此外，将LLM的内部状态和推理转化为人类易于理解的文本，确保操作员能更清楚地了解机器人当前的状态和意图，这对于有效的和安全的操作至关重要。本文概述了四种由LLM辅助的模拟机器人控制工作流程，涵盖了以下方面：(i) 低级控制，(ii) 基于语言的反馈生成，描述机器人的内部状态，(iii) 视觉信息作为附加输入的使用，以及(iv) 考虑机器人物理能力和限制的任务计划和反馈生成。所提出的概念通过一系列实验和简要讨论来呈现。项目简介、视频和补充材料可在项目网站上获取：this https URL。

请注意，原文中的“this https URL”应替换为实际的项目网站链接。 

---
# Northeastern Uni at Multilingual Counterspeech Generation: Enhancing Counter Speech Generation with LLM Alignment through Direct Preference Optimization 

**Title (ZH)**: 东北大学在多语言反言生成中的研究：通过直接偏好优化实现语言模型对齐以增强反言生成 

**Authors**: Sahil Wadhwa, Chengtian Xu, Haoming Chen, Aakash Mahalingam, Akankshya Kar, Divya Chaudhary  

**Link**: [PDF](https://arxiv.org/pdf/2412.15453)  

**Abstract**: The automatic generation of counter-speech (CS) is a critical strategy for addressing hate speech by providing constructive and informed responses. However, existing methods often fail to generate high-quality, impactful, and scalable CS, particularly across diverse linguistic contexts. In this paper, we propose a novel methodology to enhance CS generation by aligning Large Language Models (LLMs) using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Our approach leverages DPO to align LLM outputs with human preferences, ensuring contextually appropriate and linguistically adaptable responses. Additionally, we incorporate knowledge grounding to enhance the factual accuracy and relevance of generated CS. Experimental results demonstrate that DPO-aligned models significantly outperform SFT baselines on CS benchmarks while scaling effectively to multiple languages. These findings highlight the potential of preference-based alignment techniques to advance CS generation across varied linguistic settings. The model supervision and alignment is done in English and the same model is used for reporting metrics across other languages like Basque, Italian, and Spanish. 

**Abstract (ZH)**: 自动生成反言（counter-speech, CS）是应对网络仇恨言论的一种关键策略，通过提供建设性和信息丰富的回应来发挥作用。然而，现有的方法往往无法生成高质量、有影响力的且可扩展的反言，特别是在多样的语言背景下表现不佳。本文提出了一种新的方法，通过使用有监督微调（Supervised Fine-Tuning, SFT）和直接偏好优化（Direct Preference Optimization, DPO）来增强CS生成。我们的方法利用DPO使大语言模型（Large Language Models, LLMs）的输出与人类偏好对齐，从而确保生成的回应具有上下文相关性和语言适应性。此外，我们还结合了知识 grounding，以提高生成的反言的事实准确性和相关性。实验结果表明，DPO对齐的模型在反言基准测试中明显优于SFT基线，并且能够有效地扩展到多种语言。这些发现突显了基于偏好的对齐技术在跨多种语言背景改进反言生成方面的潜力。模型的监督和对齐在英语中完成，而相同的模型则用于跨其他语言（如巴斯克语、意大利语和西班牙语）的指标报告。 

---
# AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals 

**Title (ZH)**: AI增强的情报分析：探索基于生成式人工智能助手的设计以支持基因专业人士 

**Authors**: Angela Mastrianni, Hope Twede, Aleksandra Sarcevic, Jeremiah Wander, Christina Austin-Tse, Scott Saponas, Heidi Rehm, Ashley Mae Conard, Amanda K. Hall  

**Link**: [PDF](https://arxiv.org/pdf/2412.15444)  

**Abstract**: Generative AI has the potential to transform knowledge work, but further research is needed to understand how knowledge workers envision using and interacting with generative AI. We investigate the development of generative AI tools to support domain experts in knowledge work, examining task delegation and the design of human-AI interactions. Our research focused on designing a generative AI assistant to aid genetic professionals in analyzing whole genome sequences (WGS) and other clinical data for rare disease diagnosis. Through interviews with 17 genetics professionals, we identified current challenges in WGS analysis. We then conducted co-design sessions with six genetics professionals to determine tasks that could be supported by an AI assistant and considerations for designing interactions with the AI assistant. From our findings, we identified sensemaking as both a current challenge in WGS analysis and a process that could be supported by AI. We contribute an understanding of how domain experts envision interacting with generative AI in their knowledge work, a detailed empirical study of WGS analysis, and three design considerations for using generative AI to support domain experts in sensemaking during knowledge work.
CCS CONCEPTS: Human-centered computing, Human-computer interaction, Empirical studies in HCI
Additional Keywords and Phrases: whole genome sequencing, generative AI, large language models, knowledge work, sensemaking, co-design, rare disease
Contact Author: Angela Mastrianni (This work was done during the author's internship at Microsoft Research)
Ashley Mae Conard and Amanda K. Hall contributed equally 

**Abstract (ZH)**: 生成式AI有潜力重塑知识工作，但尚需进一步研究，以了解知识工作者如何设想使用和与生成式AI互动。我们探讨了生成式AI工具在支持领域专家进行知识工作方面的发展，关注任务委托和人机交互设计。我们的研究重点在于设计一个生成式AI助手，以帮助遗传专业人员分析全基因组序列（WGS）和其他临床数据，用于罕见病诊断。通过与17名遗传专业人士的访谈，我们确定了当前WGS分析中的难题。随后，我们与六名遗传专业人士进行了共同设计研讨会，以确定可以由AI助手支持的任务以及与AI助手进行交互时需要考虑的因素。根据我们的发现，我们确定了意义建构既是当前WGS分析中的一个难题，也是 khả năng由AI支持的过程。我们的贡献包括对领域专家如何在其知识工作中与生成式AI互动的见解、对WGS分析的详细实证研究，以及三条使用生成式AI支持领域专家在知识工作中进行意义建构的设计建议。

CCS概念：以人为中心的计算，人机交互，人机交互中的实证研究

附加关键词和短语：全基因组测序，生成式AI，大型语言模型，知识工作，意义建构，共同设计，罕见病

联系作者：安吉拉·马斯特里亚尼（本研究是在作者在微软研究实习期间完成的）
阿什莉·梅·康铎和阿曼达·K·霍尔贡献相当 

---
# Energy consumption of code small language models serving with runtime engines and execution providers 

**Title (ZH)**: 运行时引擎和执行提供者支持下的代码小型语言模型的能耗分析 

**Authors**: Francisco Durán, Matias Martinez, Patricia Lago, Silverio Martínez-Fernández  

**Link**: [PDF](https://arxiv.org/pdf/2412.15441)  

**Abstract**: Background. The rapid growth of Language Models (LMs), particularly in code generation, requires substantial computational resources, raising concerns about energy consumption and environmental impact. Optimizing LMs inference for energy efficiency is crucial, and Small Language Models (SLMs) offer a promising solution to reduce resource demands.
Aim. Our goal is to analyze the impact of deep learning runtime engines and execution providers on energy consumption, execution time, and computing-resource utilization from the point of view of software engineers conducting inference in the context of code SLMs.
Method. We conducted a technology-oriented, multi-stage experimental pipeline using twelve code generation SLMs to investigate energy consumption, execution time, and computing-resource utilization across the configurations.
Results. Significant differences emerged across configurations. CUDA execution provider configurations outperformed CPU execution provider configurations in both energy consumption and execution time. Among the configurations, TORCH paired with CUDA demonstrated the greatest energy efficiency, achieving energy savings from 37.99% up to 89.16% compared to other serving configurations. Similarly, optimized runtime engines like ONNX with the CPU execution provider achieved from 8.98% up to 72.04% energy savings within CPU-based configurations. Also, TORCH paired with CUDA exhibited efficient computing-resource utilization.
Conclusions. Serving configuration choice significantly impacts energy efficiency. While further research is needed, we recommend the above configurations best suited to software engineers' requirements for enhancing serving efficiency in energy and performance. 

**Abstract (ZH)**: 背景。语言模型（LMs）的快速增长，尤其是代码生成领域的增长，需要大量的计算资源，这引发了对能源消耗和环境影响的担忧。优化LMs推理以提高能源效率至关重要，而小型语言模型（SLMs）则提供了一种减少资源需求的有前景的解决方案。

目的。我们的目标是从软件工程师的角度，分析深度学习运行时引擎和执行提供者在代码SLMs推理过程中对能源消耗、执行时间和计算资源利用的影响。

方法。我们采用了一种技术导向的多阶段实验管道，使用十二种代码生成SLMs来研究不同配置下的能源消耗、执行时间和计算资源利用情况。

结果。不同配置之间的差异显著。CUDA执行提供者配置在能源消耗和执行时间方面均优于CPU执行提供者配置。在所有配置中，TORCH与CUDA的组合显示出最大的能源效率，与其他服务配置相比，能够实现高达37.99%至89.16%的能源节省。同样，如对于基于CPU的配置，优化的运行时引擎（如ONNX）与CPU执行提供者的组合也能实现高达8.98%至72.04%的能源节省。此外，TORCH与CUDA的组合还能有效利用计算资源。

结论。服务配置选择对能源效率有显著影响。虽然还需要进一步研究，但这些配置在提高能源和性能方面更适合软件工程师的需求，推荐他们使用这些配置以提升服务效率。 

---
# Efficient Neural Network Encoding for 3D Color Lookup Tables 

**Title (ZH)**: 高效的神经网络编码用于3D颜色查找表 

**Authors**: Vahid Zehtab, David B. Lindell, Marcus A. Brubaker, Michael S. Brown  

**Link**: [PDF](https://arxiv.org/pdf/2412.15438)  

**Abstract**: 3D color lookup tables (LUTs) enable precise color manipulation by mapping input RGB values to specific output RGB values. 3D LUTs are instrumental in various applications, including video editing, in-camera processing, photographic filters, computer graphics, and color processing for displays. While an individual LUT does not incur a high memory overhead, software and devices may need to store dozens to hundreds of LUTs that can take over 100 MB. This work aims to develop a neural network architecture that can encode hundreds of LUTs in a single compact representation. To this end, we propose a model with a memory footprint of less than 0.25 MB that can reconstruct 512 LUTs with only minor color distortion ($\bar{\Delta}E_M$ $\leq$ 2.0) over the entire color gamut. We also show that our network can weight colors to provide further quality gains on natural image colors ($\bar{\Delta}{E}_M$ $\leq$ 1.0). Finally, we show that minor modifications to the network architecture enable a bijective encoding that produces LUTs that are invertible, allowing for reverse color processing. Our code is available at this https URL. 

**Abstract (ZH)**: 三维彩色查找表（LUT）可通过将输入的RGB值映射到特定的输出RGB值来实现精准的颜色操作。三维LUT在视频编辑、相机内置处理、摄影滤镜、计算机图形以及显示设备的颜色处理等多个应用领域中起着重要作用。虽然单个LUT不会占用大量的内存空间，但软件和设备可能需要存储数十到数百个LUT，这些LUT总共可能超过100MB。本研究旨在开发一种神经网络架构，能够将数百个LUT以紧凑的形式进行编码。为此，我们提出了一种内存占用小于0.25MB的模型，该模型可以在整个色域内重建512个LUT，色差（$\bar{\Delta}E_M$）不超过2.0。此外，我们还展示了该网络可以针对自然图像的颜色进行加权处理，从而进一步提高图像质量（$\bar{\Delta}E_M$）不超过1.0。最后，我们展示了通过对网络结构进行轻微修改，可以实现双射编码，生成可逆的LUT，从而实现逆向颜色处理。我们的代码可以在以下链接获取：[这里提供链接]。 

---
# Offline Safe Reinforcement Learning Using Trajectory Classification 

**Title (ZH)**: 使用轨迹分类的离线安全强化学习 

**Authors**: Ze Gong, Akshat Kumar, Pradeep Varakantham  

**Link**: [PDF](https://arxiv.org/pdf/2412.15429)  

**Abstract**: Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks. 

**Abstract (ZH)**: 离线安全强化学习（RL）已成为一种有前途的方法，用于在不进行具有风险的在线交互时学习安全行为。大多数现有的离线安全RL方法依赖于每时步的成本约束（源自全局成本约束），这可能导致策略过于保守或违反安全约束。在本文中，我们提出了一种方法，通过学习一种生成期望轨迹并避免非期望轨迹的策略来克服这一问题。具体而言，我们首先将预先收集的状态-动作轨迹数据集划分为期望和非期望子集。直观地说，期望子集包含高奖励和安全的轨迹，而非期望子集包含不安全的轨迹和低奖励但安全的轨迹。其次，我们学习一个策略，使其生成期望轨迹并避免非期望轨迹，其中（非）期望性评分由从期望和非期望轨迹数据集中学习的分类器提供。这种方法绕过了现有方法中使用的最小-最大目标函数所带来的计算复杂性和稳定性问题。从理论上讲，我们还证明了我们的方法与涉及人类反馈的相关学习范式有着密切的联系。最后，我们广泛地使用DSRL基准对离线安全RL方法进行了评估。实证结果显示，我们的方法在多种基准任务中不仅获得了更高的奖励，而且在满足约束方面也表现更好，超越了竞争性的基线方法。 

---
# Learning Visual Composition through Improved Semantic Guidance 

**Title (ZH)**: 通过优化语义指导学习视觉构成 

**Authors**: Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao, Kaifeng Chen, Abhijit Ogale, Jonathon Shlens  

**Link**: [PDF](https://arxiv.org/pdf/2412.15396)  

**Abstract**: Visual imagery does not consist of solitary objects, but instead reflects the composition of a multitude of fluid concepts. While there have been great advances in visual representation learning, such advances have focused on building better representations for a small number of discrete objects bereft of an understanding of how these objects are interacting. One can observe this limitation in representations learned through captions or contrastive learning -- where the learned model treats an image essentially as a bag of words. Several works have attempted to address this limitation through the development of bespoke learned architectures to directly address the shortcomings in compositional learning. In this work, we focus on simple, and scalable approaches. In particular, we demonstrate that by substantially improving weakly labeled data, i.e. captions, we can vastly improve the performance of standard contrastive learning approaches. Previous CLIP models achieved near chance rate on challenging tasks probing compositional learning. However, our simple approach boosts performance of CLIP substantially and surpasses all bespoke architectures. Furthermore, we showcase our results on a relatively new captioning benchmark derived from DOCCI. We demonstrate through a series of ablations that a standard CLIP model trained with enhanced data may demonstrate impressive performance on image retrieval tasks. 

**Abstract (ZH)**: 视觉意象并非由孤立的对象组成，而是反映了多种流动物质概念的组合。尽管在视觉表示学习方面取得了巨大进展，这些进展主要集中在建立更优秀的表示方法，这些表示方法忽略了这些对象之间如何相互作用的理解。这种局限性可以在通过字幕或对比学习学习到的表示中观察到——学习到的模型基本上将图像视为一组单词。许多研究试图通过开发专门的自学习架构来解决这种局限性，以直接解决组成性学习的不足。在本文中，我们关注简单且可扩展的方法。具体而言，我们证明通过显著提高弱标签数据（即字幕），可以极大地提高标准对比学习方法的表现。之前的一些CLIP模型在测试组成性学习任务时接近随机水平。然而，我们的简单方法显著提升了CLIP的表现，并超越了所有专门设计的架构。此外，我们在基于DOCCI的新字幕基准上展示了我们的结果。通过一系列消融实验，我们证明使用增强数据训练的标准CLIP模型可能在图像检索任务中表现出色。 

---
# Systematic Evaluation of Long-Context LLMs on Financial Concepts 

**Title (ZH)**: 对金融概念的系统性评估在长上下文LLM中的表现 

**Authors**: Lavanya Gupta, Saket Sharma, Yiyun Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.15386)  

**Abstract**: Long-context large language models (LC LLMs) promise to increase reliability of LLMs in real-world tasks requiring processing and understanding of long input documents. However, this ability of LC LLMs to reliably utilize their growing context windows remains under investigation. In this work, we evaluate the performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series of progressively challenging tasks, as a function of factors such as context length, task difficulty, and position of key information by creating a real world financial news dataset. Our findings indicate that LC LLMs exhibit brittleness at longer context lengths even for simple tasks, with performance deteriorating sharply as task complexity increases. At longer context lengths, these state-of-the-art models experience catastrophic failures in instruction following resulting in degenerate outputs. Our prompt ablations also reveal unfortunate continued sensitivity to both the placement of the task instruction in the context window as well as minor markdown formatting. Finally, we advocate for more rigorous evaluation of LC LLMs by employing holistic metrics such as F1 (rather than recall) and reporting confidence intervals, thereby ensuring robust and conclusive findings. 

**Abstract (ZH)**: 长上下文大语言模型（LC LLMs）有望提高大语言模型在处理和理解长输入文档的实际任务中的可靠性。然而，LC LLMs能否可靠地利用其不断扩大的上下文窗口的能力仍需进一步研究。在本研究中，我们通过创建一个真实的金融新闻数据集，评估了最先进的GPT-4系列LC LLMs在解决一系列逐步增加难度的任务中的性能，考察了上下文长度、任务难度和关键信息位置等因素的影响。我们的研究结果显示，即使对于简单的任务，LC LLMs在较长上下文长度下也表现出脆弱性，随着任务复杂性的增加，性能急剧下降。在较长的上下文长度下，这些最先进的模型在指令执行方面经历了灾难性的失败，导致产生了退化输出。此外，我们的提示修改分析还揭示了模型对上下文窗口中任务指令位置以及轻微标记格式的持续敏感性。最后，我们建议通过使用综合指标（如F1分数，而不是召回率）并报告置信区间来更严格地评估LC LLMs，从而确保稳健和明确的研究结论。 

---
# Automated Root Cause Analysis System for Complex Data Products 

**Title (ZH)**: 复杂数据产品的自动化根因分析系统 

**Authors**: Mathieu Demarne, Miso Cilimdzic, Tom Falkowski, Timothy Johnson, Jim Gramling, Wei Kuang, Hoobie Hou, Amjad Aryan, Gayatri Subramaniam, Kenny Lee, Manuel Mejia, Lisa Liu, Divya Vermareddy  

**Link**: [PDF](https://arxiv.org/pdf/2412.15374)  

**Abstract**: We present ARCAS (Automated Root Cause Analysis System), a diagnostic platform based on a Domain Specific Language (DSL) built for fast diagnostic implementation and low learning curve. Arcas is composed of a constellation of automated troubleshooting guides (Auto-TSGs) that can execute in parallel to detect issues using product telemetry and apply mitigation in near-real-time. The DSL is tailored specifically to ensure that subject matter experts can deliver highly curated and relevant Auto-TSGs in a short time without having to understand how they will interact with the rest of the diagnostic platform, thus reducing time-to-mitigate and saving crucial engineering cycles when they matter most. This contrasts with platforms like Datadog and New Relic, which primarily focus on monitoring and require manual intervention for mitigation. ARCAS uses a Large Language Model (LLM) to prioritize Auto-TSGs outputs and take appropriate actions, thus suppressing the costly requirement of understanding the general behavior of the system. We explain the key concepts behind ARCAS and demonstrate how it has been successfully used for multiple products across Azure Synapse Analytics and Microsoft Fabric Synapse Data Warehouse. 

**Abstract (ZH)**: 我们介绍了ARCAS（自动根本原因分析系统），一个基于领域特定语言（DSL）构建的诊断平台，该平台旨在实现快速诊断实施并具有较低的学习曲线。ARCAS 由一组可以并行执行以检测问题并应用近实时缓解措施的自动化故障排除指南（Auto-TSGs）组成。这种DSL专门设计，以便领域专家能够在短时间内快速开发出高度精炼且相关的Auto-TSGs，而无需深入了解其如何与诊断平台的其他部分交互，从而减少缓解时间并节省关键时刻的重要工程周期。这与Datadog和New Relic等平台形成对比，后者主要专注于监控并需要手动干预来缓解问题。ARCAS 使用大型语言模型（LLM）优先处理Auto-TSGs的输出并采取适当行动，从而避免了理解系统整体行为的昂贵需求。我们介绍了ARCAS的关键概念，并展示了它在Azure Synapse Analytics和Microsoft Fabric Synapse数据仓库等多个产品中的成功应用实例。 

---
# Granger Causality Detection with Kolmogorov-Arnold Networks 

**Title (ZH)**: 使用柯尔莫哥洛夫-阿诺德网络的格兰杰因果性检测 

**Authors**: Hongyu Lin, Mohan Ren, Paolo Barucca, Tomaso Aste  

**Link**: [PDF](https://arxiv.org/pdf/2412.15373)  

**Abstract**: Discovering causal relationships in time series data is central in many scientific areas, ranging from economics to climate science. Granger causality is a powerful tool for causality detection. However, its original formulation is limited by its linear form and only recently nonlinear machine-learning generalizations have been introduced. This study contributes to the definition of neural Granger causality models by investigating the application of Kolmogorov-Arnold networks (KANs) in Granger causality detection and comparing their capabilities against multilayer perceptrons (MLP). In this work, we develop a framework called Granger Causality KAN (GC-KAN) along with a tailored training approach designed specifically for Granger causality detection. We test this framework on both Vector Autoregressive (VAR) models and chaotic Lorenz-96 systems, analysing the ability of KANs to sparsify input features by identifying Granger causal relationships, providing a concise yet accurate model for Granger causality detection. Our findings show the potential of KANs to outperform MLPs in discerning interpretable Granger causal relationships, particularly for the ability of identifying sparse Granger causality patterns in high-dimensional settings, and more generally, the potential of AI in causality discovery for the dynamical laws in physical systems. 

**Abstract (ZH)**: 研究时间序列数据中的因果关系在许多科学领域都是核心问题，从经济学到气候科学均有涉及。Granger因果性是一种强大的因果关系检测工具。然而，其原始形式局限于线性模型，直到最近才出现了非线性机器学习的推广。本研究通过探究Kolmogorov-Arnold网络（KANs）在Granger因果性检测中的应用，并将其与多层感知机（MLP）的能力进行比较，为基于神经网络的Granger因果性模型的定义做出了贡献。本文开发了一个框架，称为Granger因果性KAN（GC-KAN），并设计了一种专门针对Granger因果性检测的训练方法。我们在向量自回归（VAR）模型和混沌洛伦兹-96系统上测试了该框架，分析了KANs压缩输入特征的能力，通过对Granger因果关系的识别，提供了一种简洁而准确的Granger因果性检测模型。研究结果表明，KANs在辨别可解释的Granger因果关系方面可能优于MLP，特别是在高维设置中识别稀疏的Granger因果模式方面。更广泛地说，本研究展示了人工智能在物理系统动力学定律因果发现方面的潜力。 

---
# Making Transparency Advocates: An Educational Approach Towards Better Algorithmic Transparency in Practice 

**Title (ZH)**: 培养透明性倡导者：一种促进更好实践算法透明性的教育方法 

**Authors**: Andrew Bell, Julia Stoyanovich  

**Link**: [PDF](https://arxiv.org/pdf/2412.15363)  

**Abstract**: Concerns about the risks and harms posed by artificial intelligence (AI) have resulted in significant study into algorithmic transparency, giving rise to a sub-field known as Explainable AI (XAI). Unfortunately, despite a decade of development in XAI, an existential challenge remains: progress in research has not been fully translated into the actual implementation of algorithmic transparency by organizations. In this work, we test an approach for addressing the challenge by creating transparency advocates, or motivated individuals within organizations who drive a ground-up cultural shift towards improved algorithmic transparency.
Over several years, we created an open-source educational workshop on algorithmic transparency and advocacy. We delivered the workshop to professionals across two separate domains to improve their algorithmic transparency literacy and willingness to advocate for change. In the weeks following the workshop, participants applied what they learned, such as speaking up for algorithmic transparency at an organization-wide AI strategy meeting. We also make two broader observations: first, advocacy is not a monolith and can be broken down into different levels. Second, individuals' willingness for advocacy is affected by their professional field. For example, news and media professionals may be more likely to advocate for algorithmic transparency than those working at technology start-ups. 

**Abstract (ZH)**: 关于人工智能（AI）所带来的风险和危害的担忧已经导致了对算法透明度的大量研究，从而形成了一个名为可解释人工智能（XAI）的子领域。尽管在过去十年中XAI有所发展，但仍然存在一个根本性的挑战：研究进展并未完全转化为组织实际的算法透明度实施。在本工作中，我们通过创建透明度倡导者，即组织内的有动力个体，来推动自下而上的文化变革，以提高算法透明度，来应对这一挑战。

经过多年的时间，我们创建了一个开源教育研讨会，旨在提高参与者对算法透明度的认识和推行变革的积极性。我们在这两个独立领域的专业人士中举办了研讨会，以增强他们对算法透明度的理解和推动变革的意愿。在研讨会后的几周内，参与者展示了将所学知识应用于实际工作中的情况，例如在组织范围内的AI策略会议上为算法透明度发声。我们还做出了两项更广泛观察：首先，倡导并非单一的统一行为，而是可以划分为不同层次。其次，不同专业领域内的个体推行倡导的积极性不同。例如，新闻和媒体行业从业人员比那些在技术初创公司工作的人员更有可能为算法透明度倡导。 

---
# GeoPro-Net: Learning Interpretable Spatiotemporal Prediction Models through Statistically-Guided Geo-Prototyping 

**Title (ZH)**: GeoPro-Net：通过统计指导的地理原型化学习可解释的空间时间预测模型 

**Authors**: Bang An, Xun Zhou, Zirui Zhou, Ronilo Ragodos, Zenglin Xu, Jun Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.15353)  

**Abstract**: The problem of forecasting spatiotemporal events such as crimes and accidents is crucial to public safety and city management. Besides accuracy, interpretability is also a key requirement for spatiotemporal forecasting models to justify the decisions. Interpretation of the spatiotemporal forecasting mechanism is, however, challenging due to the complexity of multi-source spatiotemporal features, the non-intuitive nature of spatiotemporal patterns for non-expert users, and the presence of spatial heterogeneity in the data. Currently, no existing deep learning model intrinsically interprets the complex predictive process learned from multi-source spatiotemporal features. To bridge the gap, we propose GeoPro-Net, an intrinsically interpretable spatiotemporal model for spatiotemporal event forecasting problems. GeoPro-Net introduces a novel Geo-concept convolution operation, which employs statistical tests to extract predictive patterns in the input as Geo-concepts, and condenses the Geo-concept-encoded input through interpretable channel fusion and geographic-based pooling. In addition, GeoPro-Net learns different sets of prototypes of concepts inherently, and projects them to real-world cases for interpretation. Comprehensive experiments and case studies on four real-world datasets demonstrate that GeoPro-Net provides better interpretability while still achieving competitive prediction performance compared with state-of-the-art baselines. 

**Abstract (ZH)**: 时空事件（如犯罪和事故）的预测问题对于公共安全和城市管理至关重要。除了准确率外，可解释性也是时空预测模型的另一个关键要求，因为这有助于证明模型决策的合理性。然而，由于时空特征来源多样、时空模式不易于非专业人士直观理解以及数据中存在空间异质性，时空预测机制的解释仍然颇具挑战性。目前，没有已有的深度学习模型能够从多源时空特征中内在地解释复杂的预测过程。为此，我们提出了一种时空可解释模型GeoPro-Net，专门用于解决时空事件预测问题。GeoPro-Net引入了一种新的Geo-概念卷积操作，该操作通过统计测试从输入中提取预测模式作为Geo-概念，并通过可解释的通道融合和基于地理聚合进行Geo-概念编码后的输入压缩。此外，GeoPro-Net内在地学习了概念的不同原型集，并将这些原型投影到实际案例中以进行解释。在四个真实数据集上的全面实验和案例研究表明，GeoPro-Net在保持与最先进的基线模型竞争力的同时，提供了更好的解释性。 

---
# Exploring Machine Learning Engineering for Object Detection and Tracking by Unmanned Aerial Vehicle (UAV) 

**Title (ZH)**: 探索无人驾驶航空器（UAV）的目标检测与跟踪中的机器学习工程应用 

**Authors**: Aneesha Guna, Parth Ganeriwala, Siddhartha Bhattacharyya  

**Link**: [PDF](https://arxiv.org/pdf/2412.15347)  

**Abstract**: With the advancement of deep learning methods it is imperative that autonomous systems will increasingly become intelligent with the inclusion of advanced machine learning algorithms to execute a variety of autonomous operations. One such task involves the design and evaluation for a subsystem of the perception system for object detection and tracking. The challenge in the creation of software to solve the task is in discovering the need for a dataset, annotation of the dataset, selection of features, integration and refinement of existing algorithms, while evaluating performance metrics through training and testing. This research effort focuses on the development of a machine learning pipeline emphasizing the inclusion of assurance methods with increasing automation. In the process, a new dataset was created by collecting videos of moving object such as Roomba vacuum cleaner, emulating search and rescue (SAR) for indoor environment. Individual frames were extracted from the videos and labeled using a combination of manual and automated techniques. This annotated dataset was refined for accuracy by initially training it on YOLOv4. After the refinement of the dataset it was trained on a second YOLOv4 and a Mask R-CNN model, which is deployed on a Parrot Mambo drone to perform real-time object detection and tracking. Experimental results demonstrate the effectiveness of the models in accurately detecting and tracking the Roomba across multiple trials, achieving an average loss of 0.1942 and 96% accuracy. 

**Abstract (ZH)**: 随着深度学习方法的发展，自主系统将越来越需要通过集成高级机器学习算法来实现各种自主操作而变得更加智能。其中一个任务是对感知系统中的子系统进行设计和评估，用于物体检测和跟踪。要解决此任务所编写的软件面临的挑战在于发现数据集的需求、数据集的标注、特征的选择、现有算法的集成和优化，以及通过训练和测试评估性能指标。本研究侧重于开发一个机器学习管道，并强调随着自动化程度的提高，集成保证方法。在此过程中，通过收集移动物体的视频（如Roomba吸尘器），模拟室内环境的搜索和救援（SAR）任务，创建了一个新的数据集。从视频中提取帧并使用手动和自动相结合的方法进行标注。对数据集进行了初步训练，采用YOLOv4进行精确度优化后，进一步使用YOLOv4和Mask R-CNN模型进行训练。这些模型被部署在一只Parrot Mambo无人机上，进行实时物体检测和跟踪。实验结果表明，这些模型在多次试验中能够准确检测和跟踪Roomba，平均损失为0.1942，准确率为96%。 

---
# Eliciting Causal Abilities in Large Language Models for Reasoning Tasks 

**Title (ZH)**: 针对推理任务，在大型语言模型中激发因果能力的研究 

**Authors**: Yajing Wang, Zongwei Luo, Jingzhe Wang, Zhanke Zhou, Yongqiang Chen, Bo Han  

**Link**: [PDF](https://arxiv.org/pdf/2412.15314)  

**Abstract**: Prompt optimization automatically refines prompting expressions, unlocking the full potential of LLMs in downstream tasks. However, current prompt optimization methods are costly to train and lack sufficient interpretability. This paper proposes enhancing LLMs' reasoning performance by eliciting their causal inference ability from prompting instructions to correct answers. Specifically, we introduce the Self-Causal Instruction Enhancement (SCIE) method, which enables LLMs to generate high-quality, low-quantity observational data, then estimates the causal effect based on these data, and ultimately generates instructions with the optimized causal effect. In SCIE, the instructions are treated as the treatment, and textual features are used to process natural language, establishing causal relationships through treatments between instructions and downstream tasks. Additionally, we propose applying Object-Relational (OR) principles, where the uncovered causal relationships are treated as the inheritable class across task objects, ensuring low-cost reusability. Extensive experiments demonstrate that our method effectively generates instructions that enhance reasoning performance with reduced training cost of prompts, leveraging interpretable textual features to provide actionable insights. 

**Abstract (ZH)**: 自动优化提示能够自动精炼提示表达，从而在下游任务中释放出大语言模型（LLMs）的全部潜力。然而，当前的提示优化方法在训练成本上较高且缺乏足够的解释性。本文提出了一种通过从提示指令中激发因果推理能力来提高LLMs推理性能的方法。具体而言，我们引入了自因果指令增强（SCIE）方法，该方法使LLMs能够生成高质量、低数量的观察数据，基于这些数据估计因果效应，最终生成具有优化因果效应的指令。在SCIE中，指令被视为治疗手段，使用文本特征处理自然语言，通过治疗手段在指令与下游任务之间建立因果关系。此外，我们提出应用对象-关系（OR）原则，其中发现的因果关系被视为任务对象间的可继承类，以确保低成本的可重用性。广泛的经验表明，我们的方法能够有效生成增强推理性能的提示指令，并降低提示训练成本，利用可解释的文本特征提供可操作的见解。 

---
# MRWeb: An Exploration of Generating Multi-Page Resource-Aware Web Code from UI Designs 

**Title (ZH)**: MRWeb：从UI设计生成多页面资源感知Web代码的探索 

**Authors**: Yuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, Michael R. Lyu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15310)  

**Abstract**: Multi-page websites dominate modern web development. However, existing design-to-code methods rely on simplified assumptions, limiting to single-page, self-contained webpages without external resource connection. To address this gap, we introduce the Multi-Page Resource-Aware Webpage (MRWeb) generation task, which transforms UI designs into multi-page, functional web UIs with internal/external navigation, image loading, and backend routing. We propose a novel resource list data structure to track resources, links, and design components. Our study applies existing methods to the MRWeb problem using a newly curated dataset of 500 websites (300 synthetic, 200 real-world). Specifically, we identify the best metric to evaluate the similarity of the web UI, assess the impact of the resource list on MRWeb generation, analyze MLLM limitations, and evaluate the effectiveness of the MRWeb tool in real-world workflows. The results show that resource lists boost navigation functionality from 0% to 66%-80% while facilitating visual similarity. Our proposed metrics and evaluation framework provide new insights into MLLM performance on MRWeb tasks. We release the MRWeb tool, dataset, and evaluation framework to promote further research. 

**Abstract (ZH)**: 现代网络开发中，多页网站占主导地位。然而，现有的设计到代码的方法依赖于简化的假设，无法生成具有外部资源连接的多页独立网页。为解决这一问题，我们引入了多页资源感知网页（MRWeb）生成任务，该任务将UI设计转化为具有内部/外部导航、图像加载和后端路由功能的多页功能性网络UI。我们提出了一种新的资源列表数据结构，用于追踪资源、链接和设计组件。我们的研究使用新整理的500个网站数据集（300个合成网站，200个真实网站）将现有方法应用于MRWeb问题。具体来说，我们确定了评估网络UI相似度的最佳指标，评估了资源列表对MRWeb生成的影响，分析了MLLM的能力限制，并评估了在实际工作流程中使用MRWeb工具的有效性。结果显示，资源列表将导航功能从0%提升至66%-80%，同时促进视觉相似度的提升。我们提出的指标和评估框架为MLLM在MRWeb任务上的性能提供了新的见解。我们发布了MRWeb工具、数据集和评估框架，以促进进一步的研究。 

---
# Conceptual In-Context Learning and Chain of Concepts: Solving Complex Conceptual Problems Using Large Language Models 

**Title (ZH)**: 概念上下文学习与概念链：使用大规模语言模型解决复杂概念问题 

**Authors**: Nishtha N. Vaidya, Thomas Runkler, Thomas Hubauer, Veronika Haderlein-Hoegberg, Maja Mlicic Brandt  

**Link**: [PDF](https://arxiv.org/pdf/2412.15309)  

**Abstract**: Science and engineering problems fall in the category of complex conceptual problems that require specific conceptual information (CI) like math/logic -related know-how, process information, or engineering guidelines to solve them. Large Language Models (LLMs) are promising agents to solve such complex conceptual problems due to their implications in advancing engineering and science tasks like assisted problem-solving. But vanilla LLMs, trained on open-world data, lack the necessary CI. In this work, we specifically explore shallow customization methods (SCMs) of LLMs for solving complex conceptual problems. We propose two novel SCM algorithms for LLM, to augment LLMs with CI and enable LLMs to solve complex conceptual problems: Conceptual In-Context Learning (C-ICL) and Chain of Concepts (CoC). The problem tackled in this paper is generation of proprietary data models in the engineering/industry domain based on conceptual information in data modelling guidelines. We evaluate our algorithms on varied sizes of the OpenAI LLMs against four evaluation metrics related to syntactic and semantic correctness, time and cost incurred. The proposed algorithms perform better than currently popular LLM SCMs like In-context Learning (ICL) and Chain of Thoughts (CoT). It was observed that as compared to CoT, response correctness increased by 30.6% and 29.88% for the new SCMs C-ICL and CoC respectively. Qualitative analysis suggests that the proposed new SCMs activate emergent capabilities in LLMs, previously unobserved in the existing SCMs. They make problem-solving processes more transparent and reduce hallucinations and the tendency of model responses to copy examples from prompts (parroting). 

**Abstract (ZH)**: 科学研究和工程问题属于复杂的概念性问题，这类问题需要特定的概念性信息（CI），如数学/逻辑相关的知识、过程信息或工程指南来解决。大型语言模型（LLMs）因其在促进工程和科学任务方面（如辅助解决问题）的作用而成为解决这类复杂概念性问题的有希望的代理。但是，传统的LLMs在开放世界数据中训练，缺乏必要的CI。在本文中，我们特别探索了大型语言模型的浅层定制方法（SCMs）以解决复杂概念性问题。我们为LLMs提出了两种新颖的SCM算法，以增强LLMs中的CI，并使LLMs能够解决复杂的概念性问题：概念性上下文学习（C-ICL）和概念链（CoC）。

本文所解决的问题是在数据建模指南中基于概念信息生成工程/工业领域的专有数据模型。我们通过与四个与语法和语义正确性、耗时和成本相关的评估指标进行比较，评估了这些算法在不同规模的OpenAI LLMs上的性能。所提出的算法比目前流行的LLM SCMs（如上下文学习ICL和思考链CoT）表现更好。与CoT相比，新的SCMs C-ICL和CoC的响应正确率分别提高了30.6%和29.88%。定性分析表明，所提出的新的SCMs在现有的SCMs中激活了LLMs中未曾观察到的能力，使问题解决过程更加透明，并减少了幻觉和模型响应复制提示中示例的倾向（鹦鹉学舌）。 

---
# Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling 

**Title (ZH)**: 代码树：一种用于复杂任务处理中端到端代码生成与执行的树状探索框架 

**Authors**: Ziyi Ni, Yifan Li, Ning Yang, Dou Shen, Pin Lv, Daxiang Dong  

**Link**: [PDF](https://arxiv.org/pdf/2412.15305)  

**Abstract**: Solving complex reasoning tasks is a key real-world application of agents. Thanks to the pretraining of Large Language Models (LLMs) on code data, recent approaches like CodeAct successfully use code as LLM agents' action, achieving good results. However, CodeAct greedily generates the next action's code block by relying on fragmented thoughts, resulting in inconsistency and instability. Moreover, CodeAct lacks action-related ground-truth (GT), making its supervision signals and termination conditions questionable in multi-turn interactions. To address these issues, we first introduce a simple yet effective end-to-end code generation paradigm, CodeProgram, which leverages code's systematic logic to align with global reasoning and enable cohesive problem-solving. Then, we propose Tree-of-Code (ToC), which self-grows CodeProgram nodes based on the executable nature of the code and enables self-supervision in a GT-free scenario. Experimental results on two datasets using ten popular zero-shot LLMs show ToC remarkably boosts accuracy by nearly 20% over CodeAct with less than 1/4 turns. Several LLMs even perform better on one-turn CodeProgram than on multi-turn CodeAct. To further investigate the trade-off between efficacy and efficiency, we test different ToC tree sizes and exploration mechanisms. We also highlight the potential of ToC's end-to-end data generation for supervised and reinforced fine-tuning. 

**Abstract (ZH)**: 解决复杂推理任务是智能体在现实世界中的关键应用之一。得益于大型语言模型（LLMs）在代码数据上的预训练，近年来像CodeAct这样的方法成功地将代码作为LLM智能体的动作，取得了良好的效果。然而，CodeAct依赖于片段化的想法来贪婪地生成下一个动作的代码块，这导致了不一致性和不稳定性的出现。此外，CodeAct缺乏与动作相关的真实标注（Ground Truth, GT），使得其在多轮交互中的监督信号和终止条件存在疑问。为了解决这些问题，我们首先引入了一种简单而有效的端到端代码生成范式——CodeProgram，该范式利用代码的系统逻辑来与全局推理对齐，从而实现连贯的问题解决。然后，我们提出了基于可执行性的CodeProgram节点自我增长方法——Tree-of-Code（ToC），并在无需真实标注的场景中实现自我监督。在两个数据集上使用十种流行的零样本LLM进行的实验结果显示，ToC在不到四分之一的轮次中比CodeAct提高了近20%的准确性。甚至有几种LLM在单轮次CodeProgram上的表现优于多轮次CodeAct。为了进一步探索效能与效率之间的权衡，我们测试了不同大小的ToC树以及探索机制。我们还强调了ToC端到端数据生成在监督训练和强化调优方面的潜在价值。 

---
# A Comparative Study of DSPy Teleprompter Algorithms for Aligning Large Language Models Evaluation Metrics to Human Evaluation 

**Title (ZH)**: 对 DSPy 电子提词器算法在对齐大型语言模型评估指标与人工评估方面的一项比较研究 

**Authors**: Bhaskarjit Sarmah, Kriti Dutta, Anna Grigoryan, Sachin Tiwari, Stefano Pasquali, Dhagash Mehta  

**Link**: [PDF](https://arxiv.org/pdf/2412.15298)  

**Abstract**: We argue that the Declarative Self-improving Python (DSPy) optimizers are a way to align the large language model (LLM) prompts and their evaluations to the human annotations. We present a comparative analysis of five teleprompter algorithms, namely, Cooperative Prompt Optimization (COPRO), Multi-Stage Instruction Prompt Optimization (MIPRO), BootstrapFewShot, BootstrapFewShot with Optuna, and K-Nearest Neighbor Few Shot, within the DSPy framework with respect to their ability to align with human evaluations. As a concrete example, we focus on optimizing the prompt to align hallucination detection (using LLM as a judge) to human annotated ground truth labels for a publicly available benchmark dataset. Our experiments demonstrate that optimized prompts can outperform various benchmark methods to detect hallucination, and certain telemprompters outperform the others in at least these experiments. 

**Abstract (ZH)**: 我们认为声明式的自我提升Python（DSPy）优化器是一种方法，可以将大型语言模型（LLM）的指令及其评估与人类标注对齐。我们在DSPy框架内对五种演讲提示算法进行比较分析，即合作性提示优化（COPRO）、多阶段指令提示优化（MIPRO）、BootstrapFewShot、带有Optuna的BootstrapFewShot以及K-最近邻少样本方法，以评估它们在与人类评估对齐方面的能力。作为具体示例，我们专注于优化提示以将幻觉检测（使用LLM作为裁判）与公共可用基准数据集的人类标注真实标签对齐。我们的实验表明，优化的提示可以超越各种基准方法来检测幻觉，并且某些演讲提示算法在至少这些实验中优于其他算法。 

---
# A Universal Model for Human Mobility Prediction 

**Title (ZH)**: 一种通用的人类 Mobility 预测模型 

**Authors**: Qingyue Long, Yuan Yuan, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15294)  

**Abstract**: Predicting human mobility is crucial for urban planning, traffic control, and emergency response. Mobility behaviors can be categorized into individual and collective, and these behaviors are recorded by diverse mobility data, such as individual trajectory and crowd flow. As different modalities of mobility data, individual trajectory and crowd flow have a close coupling relationship. Crowd flows originate from the bottom-up aggregation of individual trajectories, while the constraints imposed by crowd flows shape these individual trajectories. Existing mobility prediction methods are limited to single tasks due to modal gaps between individual trajectory and crowd flow. In this work, we aim to unify mobility prediction to break through the limitations of task-specific models. We propose a universal human mobility prediction model (named UniMob), which can be applied to both individual trajectory and crowd flow. UniMob leverages a multi-view mobility tokenizer that transforms both trajectory and flow data into spatiotemporal tokens, facilitating unified sequential modeling through a diffusion transformer architecture. To bridge the gap between the different characteristics of these two data modalities, we implement a novel bidirectional individual and collective alignment mechanism. This mechanism enables learning common spatiotemporal patterns from different mobility data, facilitating mutual enhancement of both trajectory and flow predictions. Extensive experiments on real-world datasets validate the superiority of our model over state-of-the-art baselines in trajectory and flow prediction. Especially in noisy and scarce data scenarios, our model achieves the highest performance improvement of more than 14% and 25% in MAPE and Accuracy@5. 

**Abstract (ZH)**: 预测人类流动性对于城市规划、交通控制和应急响应至关重要。流动性行为可以分为个体行为和集体行为，并且这些行为由不同的流动性数据记录，如个体轨迹和人群流动。作为不同形式的流动性数据，个体轨迹和人群流动之间密切相关。人群流动源自自下而上的个体轨迹聚合，而人群流动对这些个体轨迹施加的约束则塑造了它们。现有的流动性预测方法由于个体轨迹和人群流动之间的模态差异，通常仅限于单一任务。在这项工作中，我们旨在统一流动性预测，以突破任务特定模型的限制。我们提出了一种通用的人类流动性预测模型（命名为UniMob），该模型可以应用于个体轨迹和人群流动。UniMob利用多视图流动性分词器将轨迹和流量数据转换为时空令牌，并通过扩散变换器架构实现统一的序列建模。为了弥合这两种数据模态之间不同特征的差距，我们实现了一种新颖的双向个体和集体对齐机制。这种机制使得可以从不同的流动性数据中学习共同的时空模式，从而促进轨迹和流量预测的相互增强。在真实数据集的广泛实验中，我们的模型在轨迹和流量预测方面优于现有的先进基线。尤其是在噪音大和数据稀少的情况下，我们的模型在相对绝差（MAPE）和准确性@5方面分别实现了超过14%和25%的性能提升。 

---
# SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage 

**Title (ZH)**: SATA：一种通过简单的辅助任务链接实现大型语言模型脱缰的范式 

**Authors**: Xiaoning Dong, Wenbo Hu, Wei Xu, Tianxing He  

**Link**: [PDF](https://arxiv.org/pdf/2412.15289)  

**Abstract**: Large language models (LLMs) have made significant advancements across various tasks, but their safety alignment remain a major concern. Exploring jailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure them. Existing methods primarily design sophisticated instructions for the LLM to follow, or rely on multiple iterations, which could hinder the performance and efficiency of jailbreaks. In this work, we propose a novel jailbreak paradigm, Simple Assistive Task Linkage (SATA), which can effectively circumvent LLM safeguards and elicit harmful responses. Specifically, SATA first masks harmful keywords within a malicious query to generate a relatively benign query containing one or multiple [MASK] special tokens. It then employs a simple assistive task such as a masked language model task or an element lookup by position task to encode the semantics of the masked keywords. Finally, SATA links the assistive task with the masked query to jointly perform the jailbreak. Extensive experiments show that SATA achieves state-of-the-art performance and outperforms baselines by a large margin. Specifically, on AdvBench dataset, with mask language model (MLM) assistive task, SATA achieves an overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and with element lookup by position (ELP) assistive task, SATA attains an overall ASR of 76% and HS of 4.43. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种任务上取得了显著的进步，但其安全性对齐仍然是一个重要问题。探索攻击性提示可以揭示LLMs的脆弱性，并指导其安全保障工作。现有方法主要通过设计复杂的指令来引导LLM，或者依赖于多次迭代，这可能会阻碍攻击性提示的效果和效率。在本工作中，我们提出了一种新颖的攻击性提示范式——简单辅助任务链接（Simple Assistive Task Linkage, SATA），它可以有效地规避LLMs的安全防护机制并引出有害响应。具体而言，SATA 首先在恶意查询中屏蔽有害关键词，生成一个相对较安全的查询，其中包含一个或多个[MASK]特殊标记。然后，它采用简单的辅助任务，如屏蔽语言模型任务或按位置查找元素任务，来编码被屏蔽关键词的语义。最后，SATA 将辅助任务与被屏蔽的查询链接起来，共同执行攻击性提示。大量实验表明，SATA 达到了最先进的性能，并在基线方法上取得了显著的优势。具体地说，在AdvBench数据集上，使用屏蔽语言模型（MLM）辅助任务时，SATA 的整体攻击成功率（ASR）为85%，有害评分（HS）为4.57；使用按位置查找元素（ELP）辅助任务时，SATA 的整体ASR为76%，HS为4.43。 

---
# Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models 

**Title (ZH)**: 基于推断意识的微调以优化大型语言模型的“最佳N选择”采样 

**Authors**: Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, Aleksandra Faust  

**Link**: [PDF](https://arxiv.org/pdf/2412.15287)  

**Abstract**: Recent studies have indicated that effectively utilizing inference-time compute is crucial for attaining better performance from large language models (LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm, in which the model is fine-tuned in a manner that directly optimizes the performance of the inference-time strategy. We study this paradigm using the simple yet effective Best-of-N (BoN) inference strategy, in which a verifier selects the best out of a set of LLM-generated responses. We devise the first imitation learning and reinforcement learning~(RL) methods for BoN-aware fine-tuning, overcoming the challenging, non-differentiable argmax operator within BoN. We empirically demonstrate that our BoN-aware models implicitly learn a meta-strategy that interleaves best responses with more diverse responses that might be better suited to a test-time input -- a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6% to 67.1%. 

**Abstract (ZH)**: 近年来的研究表明，有效利用推理时的计算资源对于提高大规模语言模型（LLMs）的性能至关重要。在这项工作中，我们提出了一种新颖的推理感知微调范式，在这种范式中，模型的微调直接优化了推理时策略的性能。我们使用简单而有效的“最优N”（BoN）推理策略进行了研究，该策略中验证器会选择一组LLM生成响应中的最佳结果。我们设计了首个针对BoN感知微调的模仿学习和强化学习（RL）方法，克服了BoN中难以求解的非可微argmax操作。通过实验证明，我们的BoN感知模型隐式学习了一种元策略，该策略交替选择最佳响应和更具多样性的响应，这些响应可能更适合测试时的输入——这一过程类似于RL中的探索-利用权衡。我们的实验表明，BoN感知微调在提高性能和推理时的计算效率方面是有效的。特别是，我们的方法将Gemma 2B在Hendrycks MATH上的Bo32性能从26.8%提高到30.8%，通过@32从60.0%提高到67.0%，并在HumanEval上的pass@16从61.6%提高到67.1%。 

---
# Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining 

**Title (ZH)**: 最大化数据的潜力：通过两阶段预训练提升大规模语言模型的准确性 

**Authors**: Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro  

**Link**: [PDF](https://arxiv.org/pdf/2412.15285)  

**Abstract**: Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends. 

**Abstract (ZH)**: 大规模语言模型的有效预训练需要策略性地选择、混合和排序数据。然而，关于数据混合的关键细节，尤其是其在更长的标记窗口和更大的模型规模方面的扩展性，由于模型开发者披露有限而鲜有研究。为解决这一问题，我们正式化了两阶段预训练的概念，并对如何选择和混合数据以最大化两个阶段的模型准确性进行了广泛而系统的研究。我们的研究结果表明，两阶段预训练方法在平均准确率上比随机数据排序和自然分布的标记高3.4%和17%。我们提供了基于数据源质量及可见的训练轮数来优化混合比例的深度指导。我们提议使用规模较小（1万亿标记）的下采样数据来设计混合比例，然后展示了我们方法的有效扩展性，适用于更长的标记窗口（15万亿标记）和更大的模型规模（250亿参数）。这些见解为实践者提供了设计和扩展其数据混合比例的一系列步骤。 

---
# Channel Merging: Preserving Specialization for Merged Experts 

**Title (ZH)**: 频道合并：保留合并专家专门化的策略 

**Authors**: Mingyang Zhang, Jing Liu, Ganggui Ding, Xinyi Yu, Linlin Ou, Bohan Zhuang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15283)  

**Abstract**: Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router. 

**Abstract (ZH)**: 近年来，针对特定任务的微调实践被用于提升大型语言模型（LLM）在后续任务中的性能。通过整合多样化的LLM，整体的模型能力得到了显著提升。然而，传统的集成方法需要将所有专门模型同时加载到GPU内存中，这显得非常耗内存。为了解决这一问题，模型合并策略应运而生，即将所有LLM合并为一个模型，以减少推理过程中的内存占用。尽管已有进展，但模型合并往往伴随着参数冲突和性能下降，尤其是专家数量增加时。此前的方法尝试通过后剪枝和部分合并来缓解这些问题，但这些方法在性能和存储效率方面存在局限性，尤其是在合并的专家数量增加时。为解决这些挑战，我们提出了一种新的策略——信道合并（Channel Merging），旨在最小化参数冲突的同时提高存储效率。该方法在线下根据信道参数的相似性进行聚类和合并，确保在同一组内仅合并高度相似的参数，从而显著减少参数冲突。在推理过程中，我们可以通过查找合并后的组来即时调用专家参数，从而保留专门的知识。实验结果表明，信道合并能够稳定地提供高性能，与未合并模型在英语和中文推理、数学推理以及代码生成等任务中表现相当，并且在使用特定任务路由器时，仅需53%的参数便可达到与模型集成相当的性能。 

---
# A Systematic Examination of Preference Learning through the Lens of Instruction-Following 

**Title (ZH)**: 通过指令遵循视角系统探究偏好学习 

**Authors**: Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan  

**Link**: [PDF](https://arxiv.org/pdf/2412.15282)  

**Abstract**: Preference learning is a widely adopted post-training technique that aligns large language models (LLMs) to human preferences and improves specific downstream task capabilities. In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use two preference dataset curation methods - rejection sampling (RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected) responses. Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts. Our experiments reveal that shared prefixes in preference pairs, as generated by MCTS, provide marginal but consistent improvements and greater stability across challenging training configurations. High-contrast preference pairs generally outperform low-contrast pairs; however, combining both often yields the best performance by balancing diversity and learning efficiency. Additionally, training on prompts of moderate difficulty leads to better generalization across tasks, even for more complex evaluation scenarios, compared to overly challenging prompts. Our findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment. 

**Abstract (ZH)**: 偏好学习是一种广泛采用的后训练技术，它将大型语言模型（LLMs）与人类偏好对齐，从而提高特定下游任务的能力。在本研究中，我们系统地探讨了特定偏好数据集属性如何影响LLMs在指令遵循任务中的对齐和下游性能。我们使用一种新颖的合成数据生成管道生成了48,000个独特指令遵循提示，这些提示结合了23个可验证的约束，这使得能够对模型响应进行细粒度和自动化质量评估。通过我们的合成提示，我们采用两种偏好数据集编撰方法——拒绝采样（RS）和蒙特卡洛树搜索（MCTS）——来获取“选定”和“拒绝”的响应对。然后，我们进行了实验，探索了以下方面的影响：（1）选定和拒绝响应之间的共享前缀，（2）选定和拒绝响应间的对比度和质量，以及（3）训练提示的复杂性。我们的实验表明，由MCTS生成的偏好对中的共享前缀虽然提供了边际但一致的改进和更好的稳定性，特别是在具有挑战性的训练配置中。高对比度的偏好对通常优于低对比度的对；然而，结合两者通常能够通过平衡多样性和学习效率来获得最佳性能。此外，在具有适度难度的提示上进行训练，即使在更复杂的评估场景中，也能更好地泛化到各种任务中，而过于挑战性的提示则不然。我们的研究结果提供了优化指令遵循任务中偏好数据编撰的实用见解，提供了一个可扩展且有效的框架，以增强LLM的训练和对齐。 

---
# Context-DPO: Aligning Language Models for Context-Faithfulness 

**Title (ZH)**: 基于上下文的DPO：使语言模型更加上下文一致 

**Authors**: Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Shenghua Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15280)  

**Abstract**: Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment remains underexplored. To address this, we propose $\textbf{Context-DPO}$, the first alignment method specifically designed to enhance LLMs' context-faithfulness. We introduce $\textbf{ConFiQA}$, a benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts to evaluate context-faithfulness. By leveraging faithful and stubborn responses to questions with provided context from ConFiQA, our Context-DPO aligns LLMs through direct preference optimization. Extensive experiments demonstrate that our Context-DPO significantly improves context-faithfulness, achieving 35% to 280% improvements on popular open-source models. Further analysis demonstrates that Context-DPO preserves LLMs' generative capabilities while providing interpretable insights into context utilization. Our code and data are released at this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）的可靠响应需要遵循用户指令和检索到的信息。虽然对齐技术有助于使LLMs与人类意图和价值观保持一致，但通过对齐提高上下文相关性仍缺乏探索。为了解决这一问题，我们提出了**Context-DPO**，这是第一个专门设计来增强LLMs上下文相关性的对齐方法。我们引入了**ConFiQA**，这是一个基准测试，模拟了带有知识冲突的检索增强生成（RAG）场景，用于评估上下文相关性。通过利用ConFiQA提供的上下文信息中忠实和固执的回答，我们的Context-DPO通过对偏好进行直接优化来对齐LLMs。大量实验表明，我们的Context-DPO显著提高了上下文相关性，在流行开源模型上的改善幅度达到35%到280%。进一步的分析表明，Context-DPO保持了LLMs的生成能力，同时提供了对上下文利用的可解释见解。我们的代码和数据可以在以下网址获取：[此网址] 

---
# Functional connectomes of neural networks 

**Title (ZH)**: 神经网络的功能连接组 

**Authors**: Tananun Songdechakraiwut, Yutong Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15279)  

**Abstract**: The human brain is a complex system, and understanding its mechanisms has been a long-standing challenge in neuroscience. The study of the functional connectome, which maps the functional connections between different brain regions, has provided valuable insights through various advanced analysis techniques developed over the years. Similarly, neural networks, inspired by the brain's architecture, have achieved notable success in diverse applications but are often noted for their lack of interpretability. In this paper, we propose a novel approach that bridges neural networks and human brain functions by leveraging brain-inspired techniques. Our approach, grounded in the insights from the functional connectome, offers scalable ways to characterize topology of large neural networks using stable statistical and machine learning techniques. Our empirical analysis demonstrates its capability to enhance the interpretability of neural networks, providing a deeper understanding of their underlying mechanisms. 

**Abstract (ZH)**: 人类大脑是一个复杂的系统，对其工作机制的理解一直是神经科学中的长期挑战。功能连接组学的研究，即绘制不同脑区之间功能连接的方法，通过多年发展起来的各种高级分析技术提供了重要的见解。同样，受脑结构启发的神经网络在多种应用中取得了显著成果，但通常因其缺乏可解释性而受到批评。在本文中，我们提出了一种新颖的方法，通过借鉴脑启发的技术将神经网络与人类大脑功能联系起来。我们的方法基于功能连接组学的洞见，提供了使用稳定统计和机器学习技术表征大型神经网络拓扑结构的可扩展方式。我们的实证分析证明了其增强神经网络可解释性的能力，为理解其内部机制提供了更深入的理解。 

---
# DreaMark: Rooting Watermark in Score Distillation Sampling Generated Neural Radiance Fields 

**Title (ZH)**: DreamMark: 将水印嵌入到得分蒸馏采样生成的神经辐射场中 

**Authors**: Xingyu Zhu, Xiapu Luo, Xuetao Wei  

**Link**: [PDF](https://arxiv.org/pdf/2412.15278)  

**Abstract**: Recent advancements in text-to-3D generation can generate neural radiance fields (NeRFs) with score distillation sampling, enabling 3D asset creation without real-world data capture. With the rapid advancement in NeRF generation quality, protecting the copyright of the generated NeRF has become increasingly important. While prior works can watermark NeRFs in a post-generation way, they suffer from two vulnerabilities. First, a delay lies between NeRF generation and watermarking because the secret message is embedded into the NeRF model post-generation through fine-tuning. Second, generating a non-watermarked NeRF as an intermediate creates a potential vulnerability for theft. To address both issues, we propose Dreamark to embed a secret message by backdooring the NeRF during NeRF generation. In detail, we first pre-train a watermark decoder. Then, the Dreamark generates backdoored NeRFs in a way that the target secret message can be verified by the pre-trained watermark decoder on an arbitrary trigger viewport. We evaluate the generation quality and watermark robustness against image- and model-level attacks. Extensive experiments show that the watermarking process will not degrade the generation quality, and the watermark achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise) and model-level attacks (e.g., pruning attack). 

**Abstract (ZH)**: 近年来，文本生成三维模型的技术取得了显著进展，能够利用得分蒸馏采样生成神经辐射场（NeRF），从而在无需实际数据采集的情况下创建三维资产。随着NeRF生成质量的快速提升，保护生成的NeRF版权问题变得越来越重要。尽管先前的研究可以通过后生成的方式对NeRF进行水印，但它们存在两个漏洞。首先，NeRF生成和水印之间存在时间延迟，因为秘密信息是在生成NeRF模型之后通过微调嵌入进去的。其次，生成未水印的NeRF作为中间步骤会增加被盗的风险。为了解决这些问题，我们提出了一种名为Dreamark的方法，在生成NeRF过程中通过后门嵌入秘密信息。具体而言，我们首先预训练一个水印解码器。然后，Dreamark以一种方式生成后门NeRF，在任意触发视窗下预训练的水印解码器可以验证目标秘密信息。我们评估了生成质量和在图像级和模型级攻击下的水印鲁棒性。广泛实验表明，水印嵌入过程不会降低生成质量，即使在图像级攻击（如高斯噪声）和模型级攻击（如剪枝攻击）中，水印的准确率也达到90%以上。 

---
# PLPP: Prompt Learning with Perplexity Is Self-Distillation for Vision-Language Models 

**Title (ZH)**: PLPP：基于困惑度的自蒸馏提示学习方法用于视觉-语言模型 

**Authors**: Biao Liu, Wenyi Fang, Xiaoyu Wu, Yang Zheng, Zheng Hu, Bo Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2412.15277)  

**Abstract**: Pre-trained Vision-Language (VL) models such as CLIP have demonstrated their excellent performance across numerous downstream tasks. A recent method, Context Optimization (CoOp), further improves the performance of VL models on downstream tasks by introducing prompt learning. CoOp optimizes a set of learnable vectors, aka prompt, and freezes the whole CLIP model. However, relying solely on CLIP loss to fine-tune prompts can lead to models that are prone to overfitting on downstream task. To address this issue, we propose a plug-in prompt-regularization method called PLPP (Prompt Learning with PerPlexity), which use perplexity loss to regularize prompt learning. PLPP designs a two-step operation to compute the perplexity for prompts: (a) calculating cosine similarity between the weight of the embedding layer and prompts to get labels, (b) introducing a language model (LM) head that requires no training behind text encoder to output word probability distribution. Meanwhile, we unveil that the essence of PLPP is inherently a form of self-distillation. To further prevent overfitting as well as to reduce the additional computation introduced by PLPP, we turn the hard label to soft label and choose top-$k$ values for calculating the perplexity loss. For accelerating model convergence, we introduce mutual self-distillation learning, that is perplexity and inverted perplexity loss. The experiments conducted on four classification tasks indicate that PLPP exhibits superior performance compared to existing methods. 

**Abstract (ZH)**: 预训练的多模态（Vision-Language, VL）模型，如CLIP，在众多下游任务中展现了出色的性能。最近提出的一种方法，Context Optimization (CoOp)，通过引入提示学习进一步提高了VL模型在下游任务中的性能。CoOp通过优化一组可学习的向量（即提示）并冻结整个CLIP模型来实现这一目标。然而，仅依靠CLIP损失微调提示可能导致模型在下游任务上过度拟合。为解决这一问题，我们提出了一种插件提示正则化方法，称为PLPP（Prompt Learning with PerPlexity），该方法使用困惑度损失来正则化提示学习。PLPP设计了一种两步操作来计算提示的困惑度：（a）计算嵌入层权重与提示之间的余弦相似性以获取标签；（b）引入一个无需在文本编码器后进行训练的语言模型（LM）头，以输出单词概率分布。同时，我们揭示了PLPP本质上是一种自我蒸馏的形式。为了进一步防止过拟合，并减少PLPP引入的额外计算，我们将硬标签转换为软标签，并根据困惑度损失计算前k个值。为了加速模型收敛，我们引入了一种互相关自我蒸馏学习，即困惑度损失和倒数困惑度损失。在四个分类任务上的实验表明，PLPP相对于现有方法具有更好的性能。 

---
# Exploring Query Efficient Data Generation towards Data-free Model Stealing in Hard Label Setting 

**Title (ZH)**: 探索查询高效的数据生成方法以实现无数据模型盗窃中的硬标签设置 

**Authors**: Gaozheng Pei, Shaojie lyu, Ke Ma, Pinci Yang, Qianqian Xu, Yingfei Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.15276)  

**Abstract**: Data-free model stealing involves replicating the functionality of a target model into a substitute model without accessing the target model's structure, parameters, or training data. The adversary can only access the target model's predictions for generated samples. Once the substitute model closely approximates the behavior of the target model, attackers can exploit its white-box characteristics for subsequent malicious activities, such as adversarial attacks. Existing methods within cooperative game frameworks often produce samples with high confidence for the prediction of the substitute model, which makes it difficult for the substitute model to replicate the behavior of the target model. This paper presents a new data-free model stealing approach called Query Efficient Data Generation (\textbf{QEDG}). We introduce two distinct loss functions to ensure the generation of sufficient samples that closely and uniformly align with the target model's decision boundary across multiple classes. Building on the limitation of current methods, which typically yield only one piece of supervised information per query, we propose the query-free sample augmentation that enables the acquisition of additional supervised information without increasing the number of queries. Motivated by theoretical analysis, we adopt the consistency rate metric, which more accurately evaluates the similarity between the substitute and target models. We conducted extensive experiments to verify the effectiveness of our proposed method, which achieved better performance with fewer queries compared to the state-of-the-art methods on the real \textbf{MLaaS} scenario and five datasets. 

**Abstract (ZH)**: 数据免费模型窃取涉及在不访问目标模型结构、参数或训练数据的情况下，将目标模型的功能复制到替代模型中。攻击者只能访问目标模型对生成样本的预测结果。一旦替代模型能够紧密逼近目标模型的行为，攻击者可以利用其白盒特性进行后续的恶意活动，例如对抗性攻击。现有的基于合作博弈框架的方法通常会生成高置信度的样本，这使得替代模型难以复制目标模型的行为。本文提出了一种新的数据免费模型窃取方法，称为高效查询数据生成（\textbf{QEDG}）。

我们引入了两个不同的损失函数以确保生成足够数量的样本，这些样本能够紧密且均匀地与目标模型的决策边界对齐，跨越多个类别。基于当前方法的局限性，即通常每个查询只能提供一条监督信息，我们提出了无需查询的样本增强方法，以在不增加查询次数的情况下获取额外的监督信息。受理论分析的启发，我们采用了连贯率度量标准，这更准确地评估了替代模型和目标模型之间的相似性。我们在广泛的实验中验证了我们提出方法的有效性，与最先进的方法相比，该方法在真实的\textbf{MLaaS}场景和五个数据集上表现出更好的性能，并且使用了较少的查询次数。 

---
# Fooling LLM graders into giving better grades through neural activity guided adversarial prompting 

**Title (ZH)**: 通过神经活动引导的对抗性提示欺骗LLM阅卷器给出更好的评分 

**Authors**: Atsushi Yamamura, Surya Ganguli  

**Link**: [PDF](https://arxiv.org/pdf/2412.15275)  

**Abstract**: The deployment of artificial intelligence (AI) in critical decision-making and evaluation processes raises concerns about inherent biases that malicious actors could exploit to distort decision outcomes. We propose a systematic method to reveal such biases in AI evaluation systems and apply it to automated essay grading as an example. Our approach first identifies hidden neural activity patterns that predict distorted decision outcomes and then optimizes an adversarial input suffix to amplify such patterns. We demonstrate that this combination can effectively fool large language model (LLM) graders into assigning much higher grades than humans would. We further show that this white-box attack transfers to black-box attacks on other models, including commercial closed-source models like Gemini. They further reveal the existence of a "magic word" that plays a pivotal role in the efficacy of the attack. We trace the origin of this magic word bias to the structure of commonly-used chat templates for supervised fine-tuning of LLMs and show that a minor change in the template can drastically reduce the bias. This work not only uncovers vulnerabilities in current LLMs but also proposes a systematic method to identify and remove hidden biases, contributing to the goal of ensuring AI safety and security. 

**Abstract (ZH)**: 人工智能（AI）在关键决策和评估过程中的部署引发了关于潜在偏见可能被恶意行为者利用以扭曲决策结果的担忧。我们提出了一种系统的方法来揭示AI评估系统中的这些偏见，并以自动作文评分为例进行了应用。我们的方法首先识别出能预测扭曲决策结果的隐蔽神经活动模式，然后优化一个对抗性输入后缀以放大这些模式。我们证明，这种组合能够有效地使大型语言模型（LLM）评分者给予比人类高得多的分数。进一步的研究表明，这种白盒攻击可以转移到其他模型的黑盒攻击中，包括如Gemini这样的商用封闭源代码模型。研究还揭示了存在一个“魔法单词”，对攻击效果起着关键作用。我们追溯了这一“魔法单词”偏见的起源，可以追溯到监督微调LLM时普遍使用的对话模板的结构，并表明模板中的小变动可以大大减少这一偏见。这项工作不仅揭示了当前LLM的漏洞，还提出了一种系统的方法来识别和消除隐藏偏见，从而为确保AI的安全性和安全性目标做出贡献。 

---
# Memory-Augmented Agent Training for Business Document Understanding 

**Title (ZH)**: 基于记忆增强的代理训练方法及其在商业文档理解中的应用 

**Authors**: Jiale Liu, Yifan Zeng, Malte Højmark-Bertelsen, Marie Normann Gadeberg, Huazheng Wang, Qingyun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15274)  

**Abstract**: Traditional enterprises face significant challenges in processing business documents, where tasks like extracting transport references from invoices remain largely manual despite their crucial role in logistics operations. While Large Language Models offer potential automation, their direct application to specialized business domains often yields unsatisfactory results. We introduce Matrix (Memory-Augmented agent Training through Reasoning and Iterative eXploration), a novel paradigm that enables LLM agents to progressively build domain expertise through experience-driven memory refinement and iterative learning. To validate this approach, we collaborate with one of the world's largest logistics companies to create a dataset of Universal Business Language format invoice documents, focusing on the task of transport reference extraction. Experiments demonstrate that Matrix outperforms prompting a single LLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the optimized systems and observe that the agent system requires less API calls, fewer costs and can analyze longer documents on average. Our methods establish a new approach to transform general-purpose LLMs into specialized business tools through systematic memory enhancement in document processing tasks. 

**Abstract (ZH)**: 传统企业面临在处理业务文件时的重大挑战，例如从发票中提取运输参考信息等任务仍然主要依赖手动操作，尽管这些操作在物流运营中至关重要。虽然大型语言模型提供了潜在的自动化机会，但它们直接应用于专业化业务领域通常效果不尽如人意。本文介绍了一种新颖的方法——Matrix（基于推理和迭代探索的记忆增强智能体训练），该方法通过经验驱动的记忆精炼和迭代学习，使大型语言模型智能体逐步建立起专业领域的知识。为了验证这一方法，我们与全球最大的物流公司之一合作，创建了一个统一商务语言格式的发票文件数据集，重点关注运输参考信息的提取任务。实验结果显示，Matrix 的表现优于单一大语言模型的提示方式高出30.3%，优于普通的大型语言模型智能体高出35.2%。此外，我们还分析了优化系统的表现指标并观察到，智能体系统所需的API调用次数更少、成本更低，且平均可以分析更长的文档。我们的方法为通过系统化的文档处理任务中的记忆增强，将通用型大语言模型转变为专用业务工具，建立了一种新的方法论。 

---
# SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation 

**Title (ZH)**: SimGRAG：利用相似子图进行知识图驱动的检索增强生成 

**Authors**: Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, Weiguo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15272)  

**Abstract**: Recent advancements in large language models (LLMs) have shown impressive versatility across various tasks. To eliminate its hallucinations, retrieval-augmented generation (RAG) has emerged as a powerful approach, leveraging external knowledge sources like knowledge graphs (KGs). In this paper, we study the task of KG-driven RAG and propose a novel Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively addresses the challenge of aligning query texts and KG structures through a two-stage process: (1) query-to-pattern, which uses an LLM to transform queries into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the alignment between the pattern and candidate subgraphs using a graph semantic distance (GSD) metric. We also develop an optimized retrieval algorithm that efficiently identifies the top-$k$ subgraphs within 1-second latency on a 10-million-scale KG. Extensive experiments show that SimGRAG outperforms state-of-the-art KG-driven RAG methods in both question answering and fact verification, offering superior plug-and-play usability and scalability. 

**Abstract (ZH)**: 近期大规模语言模型（LLMs）在各种任务中的表现展现了惊人的 versatility。为了消除其幻觉现象，检索增强生成（RAG）作为一种强大的方法应运而生，利用如知识图谱（KGs）等外部知识源。本文探讨了基于KG的RAG任务，并提出了一种新颖的相似图增强检索增强生成（SimGRAG）方法。该方法通过两阶段过程有效解决了查询文本与KG结构对齐的挑战：（1）查询到模式阶段，使用LLM将查询转换为所需的图模式；（2）模式到子图阶段，通过图语义距离（GSD）度量量化模式与候选子图之间的对齐程度。我们还开发了一种优化的检索算法，能够在1秒的延迟下高效地在1000万规模的KG中识别出前k个子图。广泛的经验表明，SimGRAG在问答和事实验证方面均优于现有的基于KG的RAG方法，提供了更优的即插即用可用性和可扩展性。 

---
# Baichuan4-Finance Technical Report 

**Title (ZH)**: Baichuan4-金融技术报告 

**Authors**: Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, Jian Xie  

**Link**: [PDF](https://arxiv.org/pdf/2412.15270)  

**Abstract**: Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning, yet their potential in finance remains underexplored due to the complexity and specialization of financial knowledge. In this work, we report the development of the Baichuan4-Finance series, including a comprehensive suite of foundational Baichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which are built upon Baichuan4-Turbo base model and tailored for finance domain. Firstly, we have dedicated significant effort to building a detailed pipeline for improving data quality. Moreover, in the continual pre-training phase, we propose a novel domain self-constraint training strategy, which enables Baichuan4-Finance-Base to acquire financial knowledge without losing general capabilities. After Supervised Fine-tuning and Reinforcement Learning from Human Feedback and AI Feedback, the chat model Baichuan4-Finance is able to tackle various financial certification questions and real-world scenario applications. We evaluate Baichuan4-Finance on many widely used general datasets and two holistic financial benchmarks. The evaluation results show that Baichuan4-Finance-Base surpasses almost all competitive baselines on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even more impressive performance on financial application scenarios, showcasing its potential to foster community innovation in the financial LLM field. 

**Abstract (ZH)**: 大型语言模型（LLMs）在语言理解、生成和推理方面展示了强大的能力，然而，由于金融知识的复杂性和专业化特性，其在金融领域的应用潜力尚未得到充分探索。在本研究中，我们介绍了Baichuan4-Finance系列的发展，其中包括一个全面的基础模型Baichuan4-Finance-Base和一个与金融领域对齐的语言模型Baichuan4-Finance，这两个模型都是基于Baichuan4-Turbo基础模型构建，并为金融领域量身定制的。首先，我们着重构建了一个详细的数据提升管道，以提高数据质量。此外，在持续预训练阶段，我们提出了一种新的领域自我约束训练策略，这使Baichuan4-Finance-Base能够在不牺牲通用能力的情况下获得金融知识。经过监督微调和基于人类反馈和AI反馈的强化学习后，聊天模型Baichuan4-Finance能够应对各种财务认证问题和实际应用场景。我们在多个广泛使用的通用数据集和两个综合的金融基准上对Baichuan4-Finance进行了评估。评估结果表明，Baichuan4-Finance-Base在大多数竞争性基线上的金融任务上表现出显著的优越性，同时在通用LLM基准上的性能并未受到影响。同时，Baichuan4-Finance在金融应用场景中的表现更为突出，展示了其在金融LLM领域激发社区创新的巨大潜力。 

---
# The Reliability Paradox: Exploring How Shortcut Learning Undermines Language Model Calibration 

**Title (ZH)**: 可靠性悖论：探究捷径学习如何削弱语言模型校准 

**Authors**: Geetanjali Bihani, Julia Rayz  

**Link**: [PDF](https://arxiv.org/pdf/2412.15269)  

**Abstract**: The advent of pre-trained language models (PLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found PLMs to suffer from miscalibration, indicating a lack of accuracy in the confidence estimates provided by these models. Current evaluation methods for PLM calibration often assume that lower calibration error estimates indicate more reliable predictions. However, fine-tuned PLMs often resort to shortcuts, leading to overconfident predictions that create the illusion of enhanced performance but lack generalizability in their decision rules. The relationship between PLM reliability, as measured by calibration error, and shortcut learning, has not been thoroughly explored thus far. This paper aims to investigate this relationship, studying whether lower calibration error implies reliable decision rules for a language model. Our findings reveal that models with seemingly superior calibration portray higher levels of non-generalizable decision rules. This challenges the prevailing notion that well-calibrated models are inherently reliable. Our study highlights the need to bridge the current gap between language model calibration and generalization objectives, urging the development of comprehensive frameworks to achieve truly robust and reliable language models. 

**Abstract (ZH)**: 预训练语言模型（PLMs）的出现已经在自然语言处理领域取得了显著的性能提升。然而，近期的研究发现PLMs存在校准偏差的问题，表明这些模型提供的置信度估计不够准确。当前对PLM校准的评估方法通常假定较低的校准误差估计意味着更可靠的结果。然而，微调后的PLMs往往依赖捷径，导致高置信度的预测，这种预测给人们一种性能提升的错觉，但缺乏决策规则的普适性。迄今为止，PLM可靠性（通过校准误差测量）与捷径学习之间的关系尚未得到充分探索。本文旨在研究这一关系，探讨较低的校准误差是否意味着语言模型的可靠决策规则。我们的研究发现，看似校准更好的模型实际上具有较高水平的不普适决策规则。这一发现挑战了校准好的模型本身就是可靠的这一普遍观念。我们的研究强调了在语言模型校准与泛化目标之间填补当前差距的必要性，建议开发综合框架以实现真正可靠和鲁棒的语言模型。 

---
# Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph 

**Title (ZH)**: 基于元毒理知识图谱增强LLM驱动的仇恨和 Toxicity 检测 

**Authors**: Yibo Zhao, Jiapeng Zhu, Can Xu, Xiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15268)  

**Abstract**: The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code will be available soon. 

**Abstract (ZH)**: 社交媒体平台的迅猛发展引发了对在线内容毒性的严重关切。当使用大型语言模型（LLMs）进行毒性检测时，两个关键挑战随之出现：1）缺乏领域特定的有毒知识导致漏报；2）LLMs 对有毒言论的过度敏感性造成误报，从而限制了言论自由。为解决这些问题，我们提出了一种名为MetaTox的新方法，利用元有毒知识图进行图搜索，以增强仇恨言论和毒性检测。首先，我们通过利用LLMs构建了一个全面的元有毒知识图，这一过程分三个步骤完成，并使用毒性基准数据集作为语料库。其次，我们通过检索和排序过程查询该图，以补充准确且相关的有毒知识。广泛的实验和跨多个数据集的深入案例研究显示，我们的MetaTox方法显著降低了误报率，并提升了整体毒性检测性能。我们的代码将在不久的将来开源。 

---
# Toxicity Detection towards Adaptability to Changing Perturbations 

**Title (ZH)**: 面向变化扰动适应性的毒性检测 

**Authors**: Hankun Kang, Jianhao Chen, Yongqi Li, Xin Miao, Mayi Xu, Ming Zhong, Yuanyuan Zhu, Tieyun Qian  

**Link**: [PDF](https://arxiv.org/pdf/2412.15267)  

**Abstract**: Toxicity detection is crucial for maintaining the peace of the society. While existing methods perform well on normal toxic contents or those generated by specific perturbation methods, they are vulnerable to evolving perturbation patterns. However, in real-world scenarios, malicious users tend to create new perturbation patterns for fooling the detectors. For example, some users may circumvent the detector of large language models (LLMs) by adding `I am a scientist' at the beginning of the prompt. In this paper, we introduce a novel problem, i.e., continual learning jailbreak perturbation patterns, into the toxicity detection field. To tackle this problem, we first construct a new dataset generated by 9 types of perturbation patterns, 7 of them are summarized from prior work and 2 of them are developed by us. We then systematically validate the vulnerability of current methods on this new perturbation pattern-aware dataset via both the zero-shot and fine tuned cross-pattern detection. Upon this, we present the domain incremental learning paradigm and the corresponding benchmark to ensure the detector's robustness to dynamically emerging types of perturbed toxic text. Our code and dataset are provided in the appendix and will be publicly available at GitHub, by which we wish to offer new research opportunities for the security-relevant communities. 

**Abstract (ZH)**: 毒性检测对于维护社会和平至关重要。尽管现有方法在正常毒性内容或特定扰动方法生成的内容上表现出色，但在应对不断演化的扰动模式时却显得脆弱。然而，在现实场景中，恶意用户往往创建新的扰动模式来欺骗检测器。例如，一些用户可能会通过在提示的开头添加“我是一个科学家”来绕过大型语言模型（LLMs）的检测器。在本文中，我们提出了一个新的问题，即持续学习针对扰动模式的逃逸问题，将其引入到毒性检测领域。为了应对这一问题，我们首先构建了一个由9种扰动模式生成的新数据集，其中7种模式是从先前的工作中总结出来的，另外2种则是我们自行开发的。我们随后通过零样本和微调跨模式检测系统地验证了现有方法在这新扰动模式感知数据集上的脆弱性。在此基础上，我们提出了领域增量学习范式及其相应的基准测试，以确保检测器能够应对动态出现的各类扰动有毒文本。我们提供了代码和数据集（详见附录），并计划在GitHub上公开发布，希望为相关安全研究领域提供新的研究机遇。 

---
# On the Structural Memory of LLM Agents 

**Title (ZH)**: LLM代理的结构记忆研究 

**Authors**: Ruihong Zeng, Jinyuan Fang, Siwei Liu, Zaiqiao Meng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15266)  

**Abstract**: Memory plays a pivotal role in enabling large language model~(LLM)-based agents to engage in complex and long-term interactions, such as question answering (QA) and dialogue systems. While various memory modules have been proposed for these tasks, the impact of different memory structures across tasks remains insufficiently explored. This paper investigates how memory structures and memory retrieval methods affect the performance of LLM-based agents. Specifically, we evaluate four types of memory structures, including chunks, knowledge triples, atomic facts, and summaries, along with mixed memory that combines these components. In addition, we evaluate three widely used memory retrieval methods: single-step retrieval, reranking, and iterative retrieval. Extensive experiments conducted across four tasks and six datasets yield the following key insights: (1) Different memory structures offer distinct advantages, enabling them to be tailored to specific tasks; (2) Mixed memory structures demonstrate remarkable resilience in noisy environments; (3) Iterative retrieval consistently outperforms other methods across various scenarios. Our investigation aims to inspire further research into the design of memory systems for LLM-based agents. 

**Abstract (ZH)**: 记忆在使基于大型语言模型（LLM）的代理能够进行复杂和长期的交互中发挥着关键作用，例如问答（QA）和对话系统。虽然已经提出了多种记忆模块，但不同记忆结构在各种任务中的影响仍然没有得到充分探索。本论文研究了记忆结构和记忆检索方法对LLM基代理性能的影响。具体来说，我们评估了四种类型的记忆结构，包括片段、知识三元组、原子事实和总结，以及将这些组件结合在一起的混合记忆。此外，我们还评估了三种广泛使用的记忆检索方法：单步检索、再排序和迭代检索。在四个任务和六个数据集上进行的大量实验揭示了以下关键见解：（1）不同的记忆结构提供了独特的优势，使其能够适应特定的任务；（2）混合记忆结构在嘈杂环境中表现出显著的鲁棒性；（3）迭代检索在各种场景中一贯优于其他方法。我们的研究旨在激发进一步研究用于LLM基代理的记忆系统的设计。 

---
# Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large Language Models 

**Title (ZH)**: 中文标题和内容如下，符合学术规范：

中文标题：Chinese SafetyQA：面向大规模语言模型的中文简短事实安全性基准

该标题翻译力求准确传达原文的意思，同时保持了学术论文标题的简洁性和专业性。 

**Authors**: Yingshui Tan, Boren Zheng, Baihui Zheng, Kerui Cao, Huiyun Jing, Jincheng Wei, Jiaheng Liu, Yancheng He, Wenbo Su, Xiangyong Zhu, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15265)  

**Abstract**: With the rapid advancement of Large Language Models (LLMs), significant safety concerns have emerged. Fundamentally, the safety of large language models is closely linked to the accuracy, comprehensiveness, and clarity of their understanding of safety knowledge, particularly in domains such as law, policy and ethics. This factuality ability is crucial in determining whether these models can be deployed and applied safely and compliantly within specific regions. To address these challenges and better evaluate the factuality ability of LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark. Chinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG ability and robustness against attacks. 

**Abstract (ZH)**: 随着大语言模型（LLMs）的迅速发展，安全问题日益凸显。从根本上说，大语言模型的安全性与其对安全知识的理解准确性、全面性和清晰度密切相关，尤其是在法律、政策和伦理等领域。这种事实理解能力是确定这些模型是否能够在特定地区安全合规地部署和应用的关键因素。为了应对这些挑战，并更好地评估LLMs回答简短问题的事实理解能力，我们介绍了中文安全问答基准（Chinese SafetyQA）。中文安全问答基准具有以下特性（即：中文、多样性、高质量、静态、易于评估、与安全相关、无害）。基于中文安全问答基准，我们对现有大语言模型的事实理解能力进行了全面评估，并分析了这些能力与大语言模型其他能力（例如，检索增强能力及其抵御攻击的鲁棒性）之间的关系。 

---
# ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports 

**Title (ZH)**: ReXTrust：一种用于AI生成放射学报告细粒度幻觉检测的模型 

**Authors**: Romain Hardy, Sung Eun Kim, Pranav Rajpurkar  

**Link**: [PDF](https://arxiv.org/pdf/2412.15264)  

**Abstract**: The increasing adoption of AI-generated radiology reports necessitates robust methods for detecting hallucinations--false or unfounded statements that could impact patient care. We present ReXTrust, a novel framework for fine-grained hallucination detection in AI-generated radiology reports. Our approach leverages sequences of hidden states from large vision-language models to produce finding-level hallucination risk scores. We evaluate ReXTrust on a subset of the MIMIC-CXR dataset and demonstrate superior performance compared to existing approaches, achieving an AUROC of 0.8751 across all findings and 0.8963 on clinically significant findings. Our results show that white-box approaches leveraging model hidden states can provide reliable hallucination detection for medical AI systems, potentially improving the safety and reliability of automated radiology reporting. 

**Abstract (ZH)**: 随着AI生成的放射学报告的日益采用，需要建立稳健的方法来检测幻觉——即假的或缺乏依据的陈述，这些陈述可能会影响患者护理。本文提出了一种新的框架ReXTrust，用于AI生成的放射学报告中的细粒度幻觉检测。我们的方法利用大型视觉-语言模型中的隐藏状态序列来生成发现级别的幻觉风险评分。我们对MIMIC-CXR数据集的一部分进行了ReXTrust的评估，并展示了与现有方法相比的优越性能，总体发现的AUROC为0.8751，临床显著发现的AUROC为0.8963。我们的结果表明，利用模型隐藏状态的白盒方法可以为医疗AI系统提供可靠的幻觉检测能力，从而有可能提高自动化放射学报告的安全性和可靠性。 

---
# Advanced ingestion process powered by LLM parsing for RAG system 

**Title (ZH)**: 由LLM解析驱动的先进吸入过程以支持RAK系统 

**Authors**: Arnau Perez, Xavier Vizcaino  

**Link**: [PDF](https://arxiv.org/pdf/2412.15262)  

**Abstract**: Retrieval Augmented Generation (RAG) systems struggle with processing multimodal documents of varying structural complexity. This paper introduces a novel multi-strategy parsing approach using LLM-powered OCR to extract content from diverse document types, including presentations and high text density files both scanned or not. The methodology employs a node-based extraction technique that creates relationships between different information types and generates context-aware metadata. By implementing a Multimodal Assembler Agent and a flexible embedding strategy, the system enhances document comprehension and retrieval capabilities. Experimental evaluations across multiple knowledge bases demonstrate the approach's effectiveness, showing improvements in answer relevancy and information faithfulness. 

**Abstract (ZH)**: 检索增强生成（RAG）系统在处理结构复杂度各异的多媒体文档时存在挑战。本文介绍了一种基于大语言模型（LLM）的光学字符识别（OCR）的新颖多策略解析方法，从多种文档类型中提取内容，包括演示文稿和高文本密度的扫描文件或非扫描文件。该方法采用基于节点的提取技术，在不同信息类型之间建立关系，并生成上下文感知的元数据。通过实施多媒体装配代理和灵活的嵌入策略，该系统增强了文档理解和检索能力。在多个知识库上的实验评估证明了该方法的有效性，显示出答案相关性和信息忠实度的改进。 

---
# Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search 

**Title (ZH)**: 使用大语言模型进行总结和搜索的现实世界医学知识结构化提取 

**Authors**: Edward Kim, Manil Shrestha, Richard Foty, Tom DeLay, Vicki Seyfert-Margolis  

**Link**: [PDF](https://arxiv.org/pdf/2412.15256)  

**Abstract**: Creation and curation of knowledge graphs can accelerate disease discovery and analysis in real-world data. While disease ontologies aid in biological data annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture patient condition nuances or rare diseases. Multiple disease definitions across data sources complicate ontology mapping and disease clustering. We propose creating patient knowledge graphs using large language model extraction techniques, allowing data extraction via natural language rather than rigid ontological hierarchies. Our method maps to existing ontologies (MeSH, SNOMED-CT, RxNORM, HPO) to ground extracted entities.
Using a large ambulatory care EHR database with 33.6M patients, we demonstrate our method through the patient search for Dravet syndrome, which received ICD10 recognition in October 2020. We describe our construction of patient-specific knowledge graphs and symptom-based patient searches. Using confirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based entity extraction to characterize patients in grounded ontologies. We then apply this method to identify Beta-propeller protein-associated neurodegeneration (BPAN) patients, demonstrating real-world discovery where no ground truth exists. 

**Abstract (ZH)**: 知识图谱的创建与维护可以加速在真实世界数据中疾病的发现与分析。尽管疾病本体有助于生物数据注释，但编码分类（如SNOMED-CT、ICD10、CPT）可能无法捕捉患者的细微情况或罕见疾病。来自不同数据源的多种疾病定义增加了本体映射和疾病聚类的复杂性。我们提出使用大型语言模型提取技术创建患者知识图谱，允许通过自然语言而非严格的形式化本体层次结构进行数据提取。我们的方法将提取的实体映射到现有的本体（MeSH、SNOMED-CT、RxNORM、HPO），以进行实体定义的验证。

我们使用包含3360万患者的大型门急诊电子健康记录数据库，通过Dravet综合征（于2020年10月被正式认可为ICD10编码）为例，演示了我们的方法。我们描述了如何构建患者特定的知识图谱及基于症状的患者搜索。通过确认的Dravet综合征ICD10编码作为真实标准，我们利用基础语言模型（LLM）实体提取技术来表征这些实体。然后，我们应用此方法来识别β-螺旋桨蛋白相关神经变性疾病（BPAN）患者，展示了在缺乏真实标准情况下进行的实际世界疾病发现。 

---
# Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation 

**Title (ZH)**: 数据漂白：通过知识蒸馏人工提升基准结果 

**Authors**: Jonibek Mansurov, Akhmed Sakip, Alham Fikri Aji  

**Link**: [PDF](https://arxiv.org/pdf/2412.15255)  

**Abstract**: In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce "Data Laundering," a three-phase process analogous to financial money laundering, that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75\% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method that inflates scores using knowledge distillation without realizing the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at \url{this https URL}. 

**Abstract (ZH)**: 在本文中，我们展示了知识蒸馏可以被操纵以操控语言模型基准得分，揭示了当前评估实践中的一个关键漏洞。我们引入了“数据漂白”这一三阶段过程，这一过程类似于金融领域的洗钱过程，能够在表面上合法的中间训练步骤中隐蔽地传递针对特定基准的知识。通过使用两层的BERT学生模型进行广泛的实验，我们展示了这种方法如何能够在不开发真正推理能力的情况下大幅提高基准准确性（在GPQA上可达75%）。值得注意的是，此方法不仅可能被故意利用，也可能被无意间采用，因为研究人员可能在不知情的情况下采用知识蒸馏来人为提升得分，却未意识到其后果。尽管我们的研究结果证明了该方法的有效性，但我们将其作为对AI评估完整性的重要警示，强调了需要更为稳健的评估方法的迫切需求。本项工作旨在为AI开发中的评估完整性讨论以及对更准确反映模型真正能力的基准的需求作出贡献。相关代码可在 \url{此链接} 获取。 

---
# NER- RoBERTa: Fine-Tuning RoBERTa for Named Entity Recognition (NER) within low-resource languages 

**Title (ZH)**: NER-RoBERTa：在低资源语言中 Fine-Tuning RoBERTa 进行命名实体识别（NER） 

**Authors**: Abdulhady Abas Abdullah, Srwa Hasan Abdulla, Dalia Mohammad Toufiq, Halgurd S. Maghdid, Tarik A. Rashid, Pakshan F. Farho, Shadan Sh. Sabr, Akar H. Taher, Darya S. Hamad, Hadi Veisi, Aras T. Asaad  

**Link**: [PDF](https://arxiv.org/pdf/2412.15252)  

**Abstract**: Nowadays, Natural Language Processing (NLP) is an important tool for most people's daily life routines, ranging from understanding speech, translation, named entity recognition (NER), and text categorization, to generative text models such as ChatGPT. Due to the existence of big data and consequently large corpora for widely used languages like English, Spanish, Turkish, Persian, and many more, these applications have been developed accurately. However, the Kurdish language still requires more corpora and large datasets to be included in NLP applications. This is because Kurdish has a rich linguistic structure, varied dialects, and a limited dataset, which poses unique challenges for Kurdish NLP (KNLP) application development. While several studies have been conducted in KNLP for various applications, Kurdish NER (KNER) remains a challenge for many KNLP tasks, including text analysis and classification. In this work, we address this limitation by proposing a methodology for fine-tuning the pre-trained RoBERTa model for KNER. To this end, we first create a Kurdish corpus, followed by designing a modified model architecture and implementing the training procedures. To evaluate the trained model, a set of experiments is conducted to demonstrate the performance of the KNER model using different tokenization methods and trained models. The experimental results show that fine-tuned RoBERTa with the SentencePiece tokenization method substantially improves KNER performance, achieving a 12.8% improvement in F1-score compared to traditional models, and consequently establishes a new benchmark for KNLP. 

**Abstract (ZH)**: 如今，自然语言处理（NLP）已成为大多数人日常生活中的重要工具，包括语音理解、翻译、命名实体识别（NER）、文本分类以及生成文本模型（如ChatGPT）。由于大数据的存在，特别是英语、西班牙语、土耳其语、波斯语等广泛使用的语言有大量的语料库，这些应用已经得到了精确的发展。然而，库尔德语仍需更多的语料库和大数据集才能纳入NLP应用。这是因为库尔德语具有丰富的语言结构、多样化的方言以及有限的数据集，为库尔德语NLP（KNLP）应用的发展带来了独特挑战。尽管在不同的KNLP应用中已进行了多项研究，但库尔德语命名实体识别（KNER）仍然是许多KNLP任务中的一个挑战，包括文本分析和分类。在此项研究中，我们通过提出一种方法来微调预训练的RoBERTa模型以应对KNER的不足。为此，我们首先创建了一个库尔德语语料库，然后设计了一种修改过的模型架构并实现了训练流程。为了评估训练后的模型，我们进行了一系列实验，使用不同的分词方法和训练模型来展示KNER模型的性能。实验结果显示，使用SentencePiece分词方法微调的RoBERTa在KNER任务上的性能显著提升，在F1分数上较传统模型提高了12.8%，从而为KNLP建立了新的基准。 

---
# AgentPS: Agentic Process Supervision for Multi-modal Content Quality Assurance through Multi-round QA 

**Title (ZH)**: AgentPS：代理过程监督在多轮问答中实现多模态内容质量保障 

**Authors**: Gorden Liu, Yu Sun, Ruixiao Sun, Xin Dong, Hongyu Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2412.15251)  

**Abstract**: The advanced processing and reasoning capabilities of multimodal large language models (MLLMs) have driven substantial progress in vision-language (VL) understanding tasks. However, while effective for tasks governed by straightforward logic, MLLMs often encounter challenges when reasoning over complex, interdependent logic structures. To address this limitation, we introduce \textit{AgentPS}, a novel framework that integrates Agentic Process Supervision into MLLMs via multi-round question answering during fine-tuning. \textit{AgentPS} demonstrates significant performance improvements over baseline MLLMs on proprietary TikTok datasets, due to its integration of process supervision and structured sequential reasoning. Furthermore, we show that replacing human-annotated labels with LLM-generated labels retains much of the performance gain, highlighting the framework's practical scalability in industrial applications. These results position \textit{AgentPS} as a highly effective and efficient architecture for multimodal classification tasks. Its adaptability and scalability, especially when enhanced by automated annotation generation, make it a powerful tool for handling large-scale, real-world challenges. 

**Abstract (ZH)**: 多模态大语言模型（MLLMs）的高级处理和推理能力在视觉语言（VL）理解任务中推动了显著的进步。然而，当面对复杂的、相互依赖的逻辑结构时，MLLMs 经常会遇到挑战。为了解决这一局限性，我们引入了 \textit{AgentPS}，这是一种新颖的框架，通过在微调过程中多轮问答整合了代理过程监督。与基线 MLLMs 相比，\textit{AgentPS} 在自主开发的抖音数据集上展示了显著的性能提升，这归功于其过程监督和结构化序列推理的整合。此外，我们表明用大语言模型生成的标签替换人工标注的标签仍然能保留大部分性能提升，这强调了该框架在工业应用中的实用可扩展性。这些结果将 \textit{AgentPS} 定位为一种在多模态分类任务中高度有效和高效的架构。其适应性和可扩展性，尤其是在增强自动标注生成的情况下，使其成为处理大规模实际挑战的强有力工具。 

---
# LLMs for Literature Review: Are we there yet? 

**Title (ZH)**: 基于LLMs的文献综述：我们已经到达了吗？ 

**Authors**: Shubham Agarwal, Gaurav Sahu, Abhay Puri, Issam H. Laradji, Krishnamurthy DJ Dvijotham, Jason Stanley, Laurent Charlin, Christopher Pal  

**Link**: [PDF](https://arxiv.org/pdf/2412.15249)  

**Abstract**: Literature reviews are an essential component of scientific research, but they remain time-intensive and challenging to write, especially due to the recent influx of research papers. This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract. We decompose the task into two components: 1. Retrieving related works given a query abstract, and 2. Writing a literature review based on the retrieved results. We analyze how effective LLMs are for both components. For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base. Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods, while providing insights into the LLM's decision-making process. In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review. To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation protocol to promote additional research and development in this regard. Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning. Further, we demonstrate that our planning-based approach achieves higher-quality reviews by minimizing hallucinated references in the generated review by 18-26% compared to existing simpler LLM-based generation methods. 

**Abstract (ZH)**: 文献综述是科学研究中不可或缺的组成部分，但撰写文献综述仍然耗时且具有挑战性，尤其是由于近年来研究论文的数量激增。本文探讨了近年来大规模语言模型（LLMs）在基于摘要辅助撰写文献综述方面的零样本能力。我们将任务分解为两个部分：1. 给定查询摘要，检索相关工作，以及2. 根据检索结果撰写文献综述。我们分析了LLMs在这两个方面的有效性。在检索方面，我们提出了一种新的两步搜索策略，首先使用LLM从论文摘要中提取有意义的关键词，然后通过查询外部知识库检索可能相关的工作。此外，我们研究了一种基于提示的重排序机制，并展示了通过提供线索可以将归一化召回率提高一倍，同时揭示了LLM的决策过程。在生成阶段，我们提出了一种两步方法，首先制定文献综述的计划，然后执行计划中的步骤以生成实际的综述。为了评估不同的基于LLM的文献综述方法，我们使用了一种为新发布的LLM设计的协议来从arXiv论文中创建测试集，以避免在零样本评估中测试集污染。我们发布了此评估协议以促进对该领域的进一步研究和开发。我们的实证结果表明，当任务分解为检索和规划的小部件时，LLMs在撰写文献综述方面显示出巨大的潜力。此外，我们证明了基于规划的方法通过减少生成综述中虚构参考文献18-26%的数量，实现了更高的文献综述质量，与现有的简单LLM生成方法相比更为优越。 

---
# Accelerating Retrieval-Augmented Generation 

**Title (ZH)**: 加速检索增强生成 

**Authors**: Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad Alian  

**Link**: [PDF](https://arxiv.org/pdf/2412.15246)  

**Abstract**: An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG.
In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7-26.3x lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM, which is the most expensive component in today's servers, from being stranded. 

**Abstract (ZH)**: 改善大型语言模型（LLMs）中幻觉并提高其准确性的逐步解决方案是检索增强生成（RAG），即通过从外部知识源（如网络）检索信息来增强LLMs。本文概述了几种RAG执行管道，并剖析了其检索和生成阶段之间错综复杂的相互作用。我们表明，尽管精确检索方案成本较高，但在某些情况下，它们可以比近似检索方案减少推理时间，因为精确检索模型可以向生成模型发送较少但更准确的文档列表，从而保持相同的端到端准确性。这一观察结果促使我们加速RAG中的精确最近邻搜索。

在本工作中，我们设计了一种智能知识存储（IKS），这是一种2型CXL设备，它采用了扩展的近内存加速架构，并在主机CPU和近内存加速器之间实现了一种新的缓存一致性接口。IKS相较于在英特尔西.com rapids CPU上执行搜索，可在512GB向量数据库中提供13.4到27.9倍更快的精确最近邻搜索性能。这种更高的搜索性能转化为代表性的RAG应用程序的1.7到26.3倍更低的端到端推理时间。IKS本质上是一种内存扩展器；其内部DDR可以被拆分并用于服务器上运行的其他应用程序，以防止作为服务器中最昂贵组成部分的DDR闲置。 

---
# MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative Samples 

**Title (ZH)**: MPPO：具有任意负样本的LLM多对偏好优化 

**Authors**: Shuo Xie, Fangzhi Zhu, Jiahui Wang, Lulu Wen, Wei Dai, Xiaowei Chen, Junxiong Zhu, Kai Zhou, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.15244)  

**Abstract**: Aligning Large Language Models (LLMs) with human feedback is crucial for their development. Existing preference optimization methods such as DPO and KTO, while improved based on Reinforcement Learning from Human Feedback (RLHF), are inherently derived from PPO, requiring a reference model that adds GPU memory resources and relies heavily on abundant preference data. Meanwhile, current preference optimization research mainly targets single-question scenarios with two replies, neglecting optimization with multiple replies, which leads to a waste of data in the application. This study introduces the MPPO algorithm, which leverages the average likelihood of model responses to fit the reward function and maximizes the utilization of preference data. Through a comparison of Point-wise, Pair-wise, and List-wise implementations, we found that the Pair-wise approach achieves the best performance, significantly enhancing the quality of model responses. Experimental results demonstrate MPPO's outstanding performance across various benchmarks. On MT-Bench, MPPO outperforms DPO, ORPO, and SimPO. Notably, on Arena-Hard, MPPO surpasses DPO and ORPO by substantial margins. These achievements underscore the remarkable advantages of MPPO in preference optimization tasks. 

**Abstract (ZH)**: 将大型语言模型（LLMs）与人类反馈对齐是其发展的重要环节。现有的偏好优化方法，如DPO和KTO，尽管基于人类反馈强化学习（RLHF）进行改进，但本质上源自PPO，需要一个参考模型以增加GPU内存资源，并且依赖大量的偏好数据。同时，当前的偏好优化研究主要集中在单个问题场景下的两个回复，忽略了多回复情况下的优化，导致在应用中数据的浪费。本研究提出了MPPO算法，该算法利用模型响应的平均概率来拟合奖励函数，最大限度地利用偏好数据。通过点式、对式和列表式实现的比较，我们发现对式方法表现最佳，显著提升了模型响应的质量。实验结果表明，MPPO在各种基准测试中表现出色。在MT-Bench上，MPPO优于DPO、ORPO和SimPO。值得注意的是，在Arena-Hard上，MPPO的性能显著优于DPO和ORPO。这些成就突显了MPPO在偏好优化任务中的显著优势。 

---
# Script-Based Dialog Policy Planning for LLM-Powered Conversational Agents: A Basic Architecture for an "AI Therapist" 

**Title (ZH)**: 基于脚本的对话策略规划：面向大语言模型驱动的对话代理的基础架构——以“AI治疗师”为例 

**Authors**: Robert Wasenmüller, Kevin Hilbert, Christoph Benzmüller  

**Link**: [PDF](https://arxiv.org/pdf/2412.15242)  

**Abstract**: Large Language Model (LLM)-Powered Conversational Agents have the potential to provide users with scaled behavioral healthcare support, and potentially even deliver full-scale "AI therapy'" in the future. While such agents can already conduct fluent and proactive emotional support conversations, they inherently lack the ability to (a) consistently and reliably act by predefined rules to align their conversation with an overarching therapeutic concept and (b) make their decision paths inspectable for risk management and clinical evaluation -- both essential requirements for an "AI Therapist".
In this work, we introduce a novel paradigm for dialog policy planning in conversational agents enabling them to (a) act according to an expert-written "script" that outlines the therapeutic approach and (b) explicitly transition through a finite set of states over the course of the conversation. The script acts as a deterministic component, constraining the LLM's behavior in desirable ways and establishing a basic architecture for an AI Therapist.
We implement two variants of Script-Based Dialog Policy Planning using different prompting techniques and synthesize a total of 100 conversations with LLM-simulated patients. The results demonstrate the feasibility of this new technology and provide insights into the efficiency and effectiveness of different implementation variants. 

**Abstract (ZH)**: 大规模语言模型（LLM）驱动的对话代理有潜力为用户提供扩展的行为健康支持，并且未来有可能提供全面的“AI疗法”。虽然这些代理已经能够进行流畅且主动的情绪支持对话，但它们本质上缺乏以下两种能力：（a）始终如一且可靠地按照预定义的规则行动，以使对话与整体治疗概念保持一致；（b）使其决策路径可追溯，以进行风险管理及临床评估——这些都是“AI治疗师”的基本要求。

在本文中，我们提出了一种新的对话策略规划范式，使对话代理能够（a）根据专家撰写的“剧本”，该剧本概述了治疗方法；以及（b）在整个对话过程中明确地过渡到一系列有限的状态。剧本作为确定性组件，限制了LLM的行为方式，并为“AI治疗师”建立了一个基本架构。

我们使用不同的提示技术实施了两种基于剧本的对话策略规划方案，并合成了100场与LLM模拟患者之间的对话。结果表明了该新技术的可行性，并提供了不同实现变体的效率和有效性见解。 

---
# Quantifying Positional Biases in Text Embedding Models 

**Title (ZH)**: 量化文本嵌入模型中的位置偏差 

**Authors**: Samarth Goel, Reagan J. Lee, Kannan Ramchandran  

**Link**: [PDF](https://arxiv.org/pdf/2412.15241)  

**Abstract**: Embedding models are crucial for tasks in Information Retrieval (IR) and semantic similarity measurement, yet their handling of longer texts and associated positional biases remains underexplored. In this study, we investigate the impact of content position and input size on text embeddings. Our experiments reveal that embedding models, irrespective of their positional encoding mechanisms, disproportionately prioritize the beginning of an input. Ablation studies demonstrate that insertion of irrelevant text or removal at the start of a document reduces cosine similarity between altered and original embeddings by up to 12.3\% more than ablations at the end. Regression analysis further confirms this bias, with sentence importance declining as position moves further from the start, even with with content-agnosticity. We hypothesize that this effect arises from pre-processing strategies and chosen positional encoding techniques. These findings quantify the sensitivity of retrieval systems and suggest a new lens towards embedding model robustness. 

**Abstract (ZH)**: 嵌入模型对于信息检索（IR）任务和语义相似度测量至关重要，然而它们处理较长文本及其相关位置偏见的研究尚不充分。在本研究中，我们探讨了文本内容位置和输入大小对文本嵌入的影响。实验结果表明，无论嵌入模型使用哪种位置编码机制，它们都倾向于优先处理输入的开始部分。消融实验表明，在文档开头插入无关文本或删除文本，会使修改后的嵌入与原始嵌入的余弦相似度降低12.3%以上，而这种影响在文档结尾比在开头更为明显。回归分析进一步证实了这种偏见，随着位置远离开头，句子的重要性逐渐下降，即使这些嵌入模型具有非内容依赖性。我们推测这种效果来源于预处理策略和选择的位置编码技术。这些发现量化了检索系统的敏感性，并为嵌入模型的鲁棒性提供了新的视角。 

---
# ChainStream: An LLM-based Framework for Unified Synthetic Sensing 

**Title (ZH)**: ChainStream：一种基于大语言模型的统一合成感知框架

在这个翻译中，“ChainStream”被保留为原名，因为它可能是该研究的一项技术或系统的名字。如果“ChainStream”有特定的中文译名，可以根据实际情况进行适当调整。感兴趣的读者可以提供更多背景信息，以便进一步优化翻译。 

**Authors**: Jiacheng Liu, Yuanchun Li, Liangyan Li, Yi Sun, Hao Wen, Xiangyu Li, Yao Guo, Yunxin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.15240)  

**Abstract**: Many applications demand context sensing to offer personalized and timely services. Yet, developing sensing programs can be challenging for developers and using them is privacy-concerning for end-users. In this paper, we propose to use natural language as the unified interface to process personal data and sense user context, which can effectively ease app development and make the data pipeline more transparent. Our work is inspired by large language models (LLMs) and other generative models, while directly applying them does not solve the problem - letting the model directly process the data cannot handle complex sensing requests and letting the model write the data processing program suffers error-prone code generation. We address the problem with 1) a unified data processing framework that makes context-sensing programs simpler and 2) a feedback-guided query optimizer that makes data query more informative. To evaluate the performance of natural language-based context sensing, we create a benchmark that contains 133 context sensing tasks. Extensive evaluation has shown that our approach is able to automatically solve the context-sensing tasks efficiently and precisely. The code is opensourced at this https URL. 

**Abstract (ZH)**: 许多应用程序需要进行上下文感知以提供个性化和及时的服务。然而，开发感知程序对开发者来说可能具有挑战性，而在使用这些程序时，终端用户也存在隐私方面的顾虑。本文中，我们提出使用自然语言作为统一接口来处理个人数据和感知用户上下文，这可以有效地简化应用程序开发，并使数据处理流程更加透明。我们的工作灵感来自于大规模语言模型（LLMs）和其他生成模型，然而直接应用这些模型并不能解决问题——让模型直接处理数据无法应对复杂的感知请求，而让模型编写数据处理程序则可能导致错误代码的生成。为了解决这些问题，我们提出了一种统一的数据处理框架，使上下文感知程序更加简单，并且使用反馈引导的查询优化器，使数据查询更加信息丰富。为了评估基于自然语言的上下文感知性能，我们创建了一个基准，其中包含133项上下文感知任务。广泛评估表明，我们的方法能够高效准确地自动解决上下文感知任务。相关代码已开源，地址为<your_link_here>。 

---
# Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs 

**Title (ZH)**: 建模故事情节预期以理解参与度：基于大语言模型的生成框架 

**Authors**: Hortense Fong, George Gui  

**Link**: [PDF](https://arxiv.org/pdf/2412.15239)  

**Abstract**: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters from Wattpad, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries. 

**Abstract (ZH)**: 理解消费者何时以及为何参与故事对于内容创作者和平台至关重要。虽然现有的理论表明，受众对未来可能发生的事情的信念应在参与决策中发挥重要作用，但大多数实证研究更多地集中在直接从实际内容中提取特征的技术上，而不是捕捉前瞻性的信念，这主要是由于无法在非结构化的叙事数据中系统地建模这些信念。为了补充现有的特征提取技术，本文引入了一个新的框架，利用大规模语言模型来捕捉受众对未来故事可能发展的情节的期望。我们的方法为每个故事生成多个可能的续篇，并通过现有的内容分析技术提取与期望、不确定性和惊喜相关的特征。我们将该方法应用于超过30,000篇来自Wattpad的书籍章节，证明该框架通过平均增强现有特征工程技术的边际解释能力31%的方式补充了现有技术。研究结果表明，不同的参与形式——继续阅读、评论和投票——受当前内容和预期内容的不同组合驱动。我们的框架提供了一种新颖的研究和探索方法，以了解受众的前瞻性信念如何塑造他们对叙事媒体的参与，这对以内容为中心的行业的营销策略具有重要意义。 

---
# Dipper: Diversity in Prompts for Producing Large Language Model Ensembles in Reasoning tasks 

**Title (ZH)**: Dipper：推理任务中大型语言模型集合的提示多样性 

**Authors**: Gregory Kang Ruey Lau, Wenyang Hu, Diwen Liu, Jizhuo Chen, See-Kiong Ng, Bryan Kian Hsiang Low  

**Link**: [PDF](https://arxiv.org/pdf/2412.15238)  

**Abstract**: Large Language Models still encounter substantial challenges in reasoning tasks, especially for smaller models, which many users may be restricted to due to resource constraints (e.g. GPU memory restrictions). Inference-time methods to boost LLM performance, such as prompting methods to invoke certain reasoning pathways in responses, have been shown effective in past works, though they largely rely on sequential queries. The ensemble method, which consists of multiple constituent models running in parallel, is a promising approach to achieving better inference-time performance, especially given recent developments that enabled significant speed-ups in LLM batch inference. In this work, we propose a novel, training-free LLM ensemble framework where a single LLM model is fed an optimized, diverse set of prompts in parallel, effectively producing an ensemble at inference time to achieve performance improvement in reasoning tasks. We empirically demonstrate that our method leads to significant gains on math reasoning tasks, e.g., on MATH, where our ensemble consisting of a few small models (e.g., three Qwen2-MATH-1.5B-it models) can outperform a larger model (e.g., Qwen2-MATH-7B-it). 

**Abstract (ZH)**: 大型语言模型在推理任务中仍面临诸多挑战，尤其是对于小型模型而言，许多用户可能由于资源限制（例如GPU内存限制）只能使用这些模型。过去的研究表明，在推理时间通过提示方法来激活特定的推理路径可以有效提升大模型的性能，尽管这种方法主要依赖于顺序查询。多个模型并行运行的集成方法是一种在最近显著提高了大模型批量推理速度的基础上实现更好推理时间性能的有前途的方法。在本工作中，我们提出了一种无需训练的大规模语言模型集成框架，在该框架下，单个大模型并行接收优化后的多样化提示，以在推理时间内生成一个集成模型，从而在推理任务中取得性能提升。我们实验证明，该方法在数学推理任务中（例如MATH数据集）取得了显著成效，我们的集成模型（包含几个小型模型，例如三个Qwen2-MATH-1.5B-it模型）在某些任务上优于一个更大的模型（例如Qwen2-MATH-7B-it）。 

---
# CareBot: A Pioneering Full-Process Open-Source Medical Language Model 

**Title (ZH)**: CareBot：一种先锋性的全流程开源医疗语言模型 

**Authors**: Lulu Zhao, Weihao Zeng, Xiaofeng Shi, Hua Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.15236)  

**Abstract**: Recently, both closed-source LLMs and open-source communities have made significant strides, outperforming humans in various general domains. However, their performance in specific professional domains such as medicine, especially within the open-source community, remains suboptimal due to the complexity of medical knowledge. In this paper, we propose CareBot, a bilingual medical LLM, which leverages a comprehensive approach integrating continuous pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning with human feedback (RLHF). Our novel two-stage CPT method, comprising Stable CPT and Boost CPT, effectively bridges the gap between general and domain-specific data, facilitating a smooth transition from pre-training to fine-tuning and enhancing domain knowledge progressively. We also introduce DataRater, a model designed to assess data quality during CPT, ensuring that the training data is both accurate and relevant. For SFT, we develope a large and diverse bilingual dataset, along with ConFilter, a metric to enhance multi-turn dialogue quality, which is crucial to improving the model's ability to handle more complex dialogues. The combination of high-quality data sources and innovative techniques significantly improves CareBot's performance across a range of medical applications. Our rigorous evaluations on Chinese and English benchmarks confirm CareBot's effectiveness in medical consultation and education. These advancements not only address current limitations in medical LLMs but also set a new standard for developing effective and reliable open-source models in the medical domain. We will open-source the datasets and models later, contributing valuable resources to the research community. 

**Abstract (ZH)**: 近年来，闭源大语言模型和开源社区均取得了显著进展，在多个通用领域已超越人类。然而，它们在特定专业领域，尤其是医学领域中的表现仍欠佳，这主要是由于医学知识的复杂性。本文提出CareBot，一种双语医学大语言模型，它结合了连续预训练（CPT）、监督微调（SFT）和基于人类反馈的强化学习（RLHF）的全面方法。我们的创新双阶段CPT方法，包括稳定预训练（Stable CPT）和增强预训练（Boost CPT），有效地弥合了通用数据与领域特定数据之间的差距，促进从预训练到微调的平滑过渡，并逐级增强领域知识。此外，我们还引入了DataRater模型，用于评估CPT过程中的数据质量，确保训练数据既准确又相关。对于SFT，我们开发了一个大型且多样化的双语数据集，并引入了ConFilter指标，以提高多轮对话质量，这对于提高模型处理复杂对话的能力至关重要。高质量数据资源和创新技术的组合显著提升了CareBot在多种医学应用中的性能。我们在中文和英文基准上的严格评估证实了CareBot在医学咨询和教育方面的效果。这些进展不仅解决了当前医学大语言模型的限制，还为在医学领域开发有效且可靠开源模型树立了新的标准。我们将后续开源数据集和模型，为研究社区贡献宝贵的资源。 

---
# OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models 

**Title (ZH)**: OG-RAG：面向本体的检索增强生成方法用于大规模语言模型 

**Authors**: Kartik Sharma, Peeyush Kumar, Yunqing Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.15235)  

**Abstract**: This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented Generation method designed to enhance LLM-generated responses by anchoring retrieval processes in domain-specific ontologies. While LLMs are widely used for tasks like question answering and search, they struggle to adapt to specialized knowledge, such as industrial workflows or knowledge work, without expensive fine-tuning or sub-optimal retrieval methods. Existing retrieval-augmented models, such as RAG, offer improvements but fail to account for structured domain knowledge, leading to suboptimal context generation. Ontologies, which conceptually organize domain knowledge by defining entities and their interrelationships, offer a structured representation to address this gap. OG-RAG constructs a hypergraph representation of domain documents, where each hyperedge encapsulates clusters of factual knowledge grounded using domain-specific ontology. An optimization algorithm then retrieves the minimal set of hyperedges that constructs a precise, conceptually grounded context for the LLM. This method enables efficient retrieval while preserving the complex relationships between entities. OG-RAG applies to domains where fact-based reasoning is essential, particularly in tasks that require workflows or decision-making steps to follow predefined rules and procedures. These include industrial workflows in healthcare, legal, and agricultural sectors, as well as knowledge-driven tasks such as news journalism, investigative research, consulting and more. Our evaluations demonstrate that OG-RAG increases the recall of accurate facts by 55% and improves response correctness by 40% across four different LLMs. Additionally, OG-RAG enables 30% faster attribution of responses to context and boosts fact-based reasoning accuracy by 27% compared to baseline methods. 

**Abstract (ZH)**: 本文提出了OG-RAG（基于本体的检索增强生成方法），旨在通过将检索过程锚定在特定领域的本体中，来增强LLM生成的回答。尽管LLM在问答和搜索等任务中被广泛应用，但在处理特定知识（如工业流程或知识工作）方面却面临挑战，缺乏经济有效的微调或不理想的检索方法。现有的检索增强模型，如RAG，虽然有所改进，但未能充分考虑结构化的领域知识，导致上下文生成效果不佳。本体概念上组织领域知识，通过定义实体及其相互关系提供了结构化表示，从而填补了这一空白。OG-RAG构造了一个文档超图表示，其中每个超边封装了使用领域特定本体定义的事实知识集。然后，通过优化算法检索构建精确且概念上支持的上下文所需的最小超边集，供LLM使用。这种方法允许高效检索，同时保持实体之间的复杂关系。OG-RAG适用于事实推理至关重要的领域，特别是在需要遵循预定义规则和流程的流程或决策任务中，特别是在健康医疗、法律和农业等行业中的工业流程，以及需要知识驱动的任务，如新闻报道、调查研究、咨询等。我们的评估结果表明，与四个不同LLM相比，OG-RAG将准确事实的召回率提高了55%，响应正确性提高了40%。此外，OG-RAG将回应归因于上下文的速度提高了30%，并将基于事实的推理准确性提高了27%，相较于基线方法有显著提升。 

---
# Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education 

**Title (ZH)**: 使用ChatGPT进行教学：可教ChatGPT代理对学生编程教育效果的影响 

**Authors**: Angxuan Chen, Yuang Wei, Huixiao Le, Yan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.15226)  

**Abstract**: This study investigates the potential of using ChatGPT as a teachable agent to support students' learning by teaching process, specifically in programming education. While learning by teaching is an effective pedagogical strategy for promoting active learning, traditional teachable agents have limitations, particularly in facilitating natural language dialogue. Our research explored whether ChatGPT, with its ability to engage learners in natural conversations, can support this process. The findings reveal that interacting with ChatGPT improves students' knowledge gains and programming abilities, particularly in writing readable and logically sound code. However, it had limited impact on developing learners' error-correction skills, likely because ChatGPT tends to generate correct code, reducing opportunities for students to practice debugging. Additionally, students' self-regulated learning (SRL) abilities improved, suggesting that teaching ChatGPT fosters learners' higher self-efficacy and better implementation of SRL strategies. This study discussed the role of natural dialogue in fostering socialized learning by teaching, and explored ChatGPT's specific contributions in supporting students' SRL through the learning by teaching process. Overall, the study highlights ChatGPT's potential as a teachable agent, offering insights for future research on ChatGPT-supported education. 

**Abstract (ZH)**: 本研究探讨了将ChatGPT作为一种可教代理，通过教学过程支持学生学习的潜力，特别是在编程教育领域。虽然通过教学是一种促进主动学习的有效教学策略，但传统的可教代理存在局限性，特别是在促进自然语言对话方面。我们的研究探索了ChatGPT是否能够利用其进行自然对话的能力支持这一过程。研究结果表明，与ChatGPT互动可以提高学生的学习知识和编程能力，特别是在编写可读且逻辑正确的代码方面。然而，它在培养学生的错误校正技能方面的影响有限，这可能是因为ChatGPT倾向于生成正确的代码，减少了学生练习调试的机会。此外，学生自我调节学习（SRL）的能力得到了提高，表明教授ChatGPT能够增强学生的自我效能感，并更好地实施SRL策略。本研究讨论了自然对话在通过教学促进社会化学习中的作用，并探讨了ChatGPT在支持学生通过教学过程实现SRL方面的具体贡献。总体而言，本研究强调了ChatGPT作为可教代理的潜力，并为未来关于ChatGPT支持教育的研究提供了见解。 

---
# A Survey on Large Language Model-based Agents for Statistics and Data Science 

**Title (ZH)**: 基于大规模语言模型的代理在统计学和数据科学领域的研究综述 

**Authors**: Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.14222)  

**Abstract**: In recent years, data science agents powered by Large Language Models (LLMs), known as "data agents," have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software. 

**Abstract (ZH)**: 近年来，由大规模语言模型（LLMs）驱动的数据科学代理，即“数据代理”，展示了显著的潜力，以变革传统的数据分析范式。本文综述了基于LLM的数据代理的演变、能力和应用，强调了它们在简化复杂数据任务以及降低无相关专业知识用户的学习门槛方面的作用。我们探讨了基于LLM的框架当前的设计趋势，并详细介绍了诸如规划、推理、反思、多代理协作、用户界面、知识集成和系统设计等关键特性，这些特性使代理能够最大限度地减少人力干预解决数据驱动的问题。此外，我们分析了几例案例研究，展示了各种数据代理在实际场景中的应用。最后，我们识别了关键挑战，并提出了未来的研究方向，以推动数据代理的发展，使其成为智能统计分析软件。 

---
# Sum-of-Squares Programming for Ma-Trudinger-Wang Regularity of Optimal Transport Maps 

**Title (ZH)**: 利用平方和规划方法研究Ma-Trudinger-Wang最优传输映射的正则性 

**Authors**: Sachin Shivakumar, Georgiy A. Bondar, Gabriel Khan, Abhishek Halder  

**Link**: [PDF](https://arxiv.org/pdf/2412.13372)  

**Abstract**: For a given ground cost, approximating the Monge optimal transport map that pushes forward a given probability measure onto another has become a staple in several modern machine learning algorithms. The fourth-order Ma-Trudinger-Wang (MTW) tensor associated with this ground cost function provides a notion of curvature in optimal transport. The non-negativity of this tensor plays a crucial role for establishing continuity for the Monge optimal transport map. It is, however, generally difficult to analytically verify this condition for any given ground cost. To expand the class of cost functions for which MTW non-negativity can be verified, we propose a provably correct computational approach which provides certificates of non-negativity for the MTW tensor using Sum-of-Squares (SOS) programming. We further show that our SOS technique can also be used to compute an inner approximation of the region where MTW non-negativity holds. We apply our proposed SOS programming method to several practical ground cost functions to approximate the regions of regularity of their corresponding optimal transport maps. 

**Abstract (ZH)**: 对于给定的地价成本，近似将一个给定概率测度推送到另一个测度上的蒙特基金会最优运输映射已成为现代机器学习算法中的基本方法之一。与该地价成本相关的四阶马-特杜宁-王（MTW）张量为最优运输提供了一个曲率概念。此张量的非负性对于建立蒙特基金会最优运输映射的连续性至关重要。然而，对于任何一个给定的地价成本，验证这一条件通常都非常困难。为扩大能够验证MTW非负性的成本函数的范围，我们提出了一个证明正确的计算方法，该方法使用求和为平方（Sum-of-Squares, SOS）编程来提供MTW张量非负性的认证。我们进一步展示了我们的SOS技术还可以用于计算MTW非负性成立的内部近似区域。我们利用我们提出的SOS编程方法对几个实际的地价成本函数进行应用，以近似其相应的最优运输映射的正则性区域。 

---
# TACNET: Temporal Audio Source Counting Network 

**Title (ZH)**: TACNET：时序音频声源计数网络 

**Authors**: Amirreza Ahmadnejad, Ahmad Mahmmodian Darviishani, Mohmmad Mehrdad Asadi, Sajjad Saffariyeh, Pedram Yousef, Emad Fatemizadeh  

**Link**: [PDF](https://arxiv.org/pdf/2311.02369)  

**Abstract**: In this paper, we introduce the Temporal Audio Source Counting Network (TaCNet), an innovative architecture that addresses limitations in audio source counting tasks. TaCNet operates directly on raw audio inputs, eliminating complex preprocessing steps and simplifying the workflow. Notably, it excels in real-time speaker counting, even with truncated input windows. Our extensive evaluation, conducted using the LibriCount dataset, underscores TaCNet's exceptional performance, positioning it as a state-of-the-art solution for audio source counting tasks. With an average accuracy of 74.18 percentage over 11 classes, TaCNet demonstrates its effectiveness across diverse scenarios, including applications involving Chinese and Persian languages. This cross-lingual adaptability highlights its versatility and potential impact. 

**Abstract (ZH)**: 在本文中，我们介绍了Temporal Audio Source Counting Network（TaCNet），这是一种创新的架构，旨在解决音频源计数任务中的局限性。TaCNet 直接处理原始音频输入，消除了复杂的预处理步骤，简化了工作流程。值得注意的是，它即使在输入窗口被截断的情况下也能出色地进行实时发言人计数。通过使用 LibriCount 数据集进行广泛评估，我们进一步证实了TaCNet 的优异性能，使其成为音频源计数任务的前沿解决方案。TaCNet 在 11 个类别上的平均准确率为 74.18%，表明其在各种场景中的有效性，包括涉及汉语和波斯语的应用。这种跨语言适应性突显了其多功能性和潜在影响。 

---
