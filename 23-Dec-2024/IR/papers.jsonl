{'arxiv_id': 'arXiv:2412.16086', 'title': 'Towards Interpretable Radiology Report Generation via Concept Bottlenecks using a Multi-Agentic RAG', 'authors': 'Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag', 'link': 'https://arxiv.org/abs/2412.16086', 'abstract': "Deep learning has advanced medical image classification, but interpretability challenges hinder its clinical adoption. This study enhances interpretability in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs) and a multi-agent Retrieval-Augmented Generation (RAG) system for report generation. By modeling relationships between visual features and clinical concepts, we create interpretable concept vectors that guide a multi-agent RAG system to generate radiology reports, enhancing clinical relevance, explainability, and transparency. Evaluation of the generated reports using an LLM-as-a-judge confirmed the interpretability and clinical utility of our model's outputs. On the COVID-QU dataset, our model achieved 81% classification accuracy and demonstrated robust report generation performance, with five key metrics ranging between 84% and 90%. This interpretable multi-agent framework bridges the gap between high-performance AI and the explainability required for reliable AI-driven CXR analysis in clinical settings.", 'abstract_zh': '深度学习技术的进步促进了医学图像分类，但其可解释性不足限制了其临床应用。本研究通过使用概念瓶颈模型（CBMs）和多智能体检索增强生成（RAG）系统，增强了胸部X光（CXR）分类的可解释性。通过建模视觉特征与临床概念之间的关系，我们创建了可解释的概念向量，这些向量指导多智能体RAG系统生成放射学报告，从而增强了临床相关性、可解释性和透明度。利用语言模型作为评委进行生成报告的评估，证实了我们模型输出的可解释性和临床实用性。在COVID-QU数据集中，我们的模型实现了81%的分类准确率，并展示了稳健的报告生成性能，五个关键指标的范围在84%到90%之间。这种可解释的多智能体框架填补了高性能AI与临床环境中可靠AI驱动CXR分析所需可解释性之间的空白。', 'title_zh': '通过多智能体 Retrieval-Augmented Generation (RAG) 中的概念瓶颈实现可解释的放射学报告生成'}
{'arxiv_id': 'arXiv:2412.15973', 'title': 'Legommenders: A Comprehensive Content-Based Recommendation Library with LLM Support', 'authors': 'Qijiong Liu, Lu Fan, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2412.15973', 'abstract': 'We present Legommenders, a unique library designed for content-based recommendation that enables the joint training of content encoders alongside behavior and interaction modules, thereby facilitating the seamless integration of content understanding directly into the recommendation pipeline. Legommenders allows researchers to effortlessly create and analyze over 1,000 distinct models across 15 diverse datasets. Further, it supports the incorporation of contemporary large language models, both as feature encoder and data generator, offering a robust platform for developing state-of-the-art recommendation models and enabling more personalized and effective content delivery.', 'abstract_zh': '我们介绍了一种名为Legommenders的独特图书管理系统，它专为基于内容的推荐设计，能够同时训练内容编码器和行为与交互模块，从而实现直接将内容理解无缝集成到推荐流程中。Legommenders允许研究人员轻松创建和分析超过1,000种不同的模型，并可在15个不同的数据集上进行实验。此外，它支持将当今的大型语言模型纳入其中，作为特征编码器和数据生成器，提供了一个强大的平台，用于开发最新的推荐模型，并促进更为个性化和有效的内容交付。', 'title_zh': 'Legommenders：一种具备LLM支持的全面内容基推荐库'}
{'arxiv_id': 'arXiv:2412.15759', 'title': 'ASPIRE: Assistive System for Performance Evaluation in IR', 'authors': 'Georgios Peikos, Wojciech Kusa, Symeon Symeonidis', 'link': 'https://arxiv.org/abs/2412.15759', 'abstract': 'Information Retrieval (IR) evaluation involves far more complexity than merely presenting performance measures in a table. Researchers often need to compare multiple models across various dimensions, such as the Precision-Recall trade-off and response time, to understand the reasons behind the varying performance of specific queries for different models. We introduce ASPIRE (Assistive System for Performance Evaluation in IR), a visual analytics tool designed to address these complexities by providing an extensive and user-friendly interface for in-depth analysis of IR experiments. ASPIRE supports four key aspects of IR experiment evaluation and analysis: single/multi-experiment comparisons, query-level analysis, query characteristics-performance interplay, and collection-based retrieval analysis. We showcase the functionality of ASPIRE using the TREC Clinical Trials collection. ASPIRE is an open-source toolkit available online: this https URL', 'abstract_zh': '信息检索（IR）评估远不止于仅仅将性能指标呈现在表中。研究人员通常需要在多个维度上比较多个模型，例如查准率-查全率权衡与响应时间，以理解不同模型在特定查询上的性能差异背后的原因。为此，我们介绍了ASPIRE（Assistive System for Performance Evaluation in IR），这是一种可视化分析工具，旨在通过提供深入分析IR实验的广泛且用户友好的界面来解决这些复杂性。ASPIRE支持IR实验评估和分析的四个关键方面：单实验/多实验比较、查询级别分析、查询特征-性能互动以及基于集合的检索分析。我们使用TREC临床试验集合展示了ASPIRE的功能。ASPIRE是一个开源工具包，可在以下网址在线获取：这个 https URL', 'title_zh': 'ASPIRE：辅助系统在信息检索中的绩效评估'}
{'arxiv_id': 'arXiv:2412.15494', 'title': 'PolySmart and VIREO @ TRECVid 2024 Ad-hoc Video Search', 'authors': 'Jiaxin Wu, Chong-Wah Ngo, Xiao-Yong Wei, Qing Li', 'link': 'https://arxiv.org/abs/2412.15494', 'abstract': 'This year, we explore generation-augmented retrieval for the TRECVid AVS task. Specifically, the understanding of textual query is enhanced by three generations, including Text2Text, Text2Image, and Image2Text, to address the out-of-vocabulary problem. Using different combinations of them and the rank list retrieved by the original query, we submitted four automatic runs. For manual runs, we use a large language model (LLM) (i.e., GPT4) to rephrase test queries based on the concept bank of the search engine, and we manually check again to ensure all the concepts used in the rephrased queries are in the bank. The result shows that the fusion of the original and generated queries outperforms the original query on TV24 query sets. The generated queries retrieve different rank lists from the original query.', 'abstract_zh': '今年，我们探索了增强检索技术在TRECVid AVS任务中的应用。具体而言，通过三次生成——包括文本到文本（Text2Text）、文本到图像（Text2Image）和图像到文本（Image2Text）——来增强对文本查询的理解，以解决词汇量不足的问题。利用不同组合的生成方法以及原始查询检索出的排名列表，我们提交了四个自动运行版本。对于手动运行版本，我们使用大型语言模型（LLM，例如GPT4）基于搜索引擎的概念库重新表述测试查询，并手动检查确保所有使用在重新表述查询中的概念都存在于概念库中。结果显示，在电视24（TV24）查询集上，原始查询和生成查询的融合表现优于原始查询。生成查询检索出与原始查询不同的排名列表。', 'title_zh': 'PolySmart 和 VIREO 参与的 TRECVid 2024 即时视频检索挑战赛'}
{'arxiv_id': 'arXiv:2412.15404', 'title': 'A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science', 'authors': 'Ahmet Yasin Aytar, Kemal Kilic, Kamer Kaya', 'link': 'https://arxiv.org/abs/2412.15404', 'abstract': "In the rapidly evolving field of data science, efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation. This paper presents an enhanced Retrieval-Augmented Generation (RAG) application, an artificial intelligence (AI)-based system designed to assist data scientists in accessing precise and contextually relevant academic resources. The AI-powered application integrates advanced techniques, including the GeneRation Of BIbliographic Data (GROBID) technique for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method, to significantly improve the relevance and accuracy of the retrieved information. This implementation of AI specifically addresses the challenge of academic literature navigation. A comprehensive evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS) framework demonstrates substantial improvements in key metrics, particularly Context Relevance, underscoring the system's effectiveness in reducing information overload and enhancing decision-making processes. Our findings highlight the potential of this enhanced Retrieval-Augmented Generation system to transform academic exploration within data science, ultimately advancing the workflow of research and innovation in the field.", 'abstract_zh': '在数据科学这一快速发展的领域中，有效地导航海量的学术文献对于做出明智的决策和创新至关重要。本文介绍了一种增强型检索增强生成（RAG）应用，这是一种基于人工智能（AI）的系统，旨在帮助数据科学家访问精确且上下文相关的学术资源。该AI驱动的应用程序集成了先进的技术，包括用于提取参考文献信息的GeneRation Of BIbliographic Data (GROBID) 技术、微调嵌入模型、语义切块，以及摘录优先检索方法，以显著提高检索信息的相关性和准确性。该AI 实现特别解决了学术文献导航的挑战。使用检索增强生成评估系统（RAGAS）框架进行的全面评估显示，在关键指标上取得了显著改进，特别是在上下文相关性方面，突显了该系统在减少信息过载和增强决策过程方面的有效性。我们的研究结果强调了这种增强型检索增强生成系统的潜在能力，以重塑数据科学中的学术探索，并最终推动该领域研究与创新的工作流程。', 'title_zh': '数据科学中学术文献导航的检索增强生成框架'}
{'arxiv_id': 'arXiv:2412.15232', 'title': 'Ranking Narrative Query Graphs for Biomedical Document Retrieval (Technical Report)', 'authors': 'Hermann Kroll, Pascal Sackhoff, Timo Breuer, Ralf Schenkel, Wolf-Tilo Balke', 'link': 'https://arxiv.org/abs/2412.15232', 'abstract': "Keyword-based searches are today's standard in digital libraries. Yet, complex retrieval scenarios like in scientific knowledge bases, need more sophisticated access paths. Although each document somewhat contributes to a domain's body of knowledge, the exact structure between keywords, i.e., their possible relationships, and the contexts spanned within each single document will be crucial for effective retrieval. Following this logic, individual documents can be seen as small-scale knowledge graphs on which graph queries can provide focused document retrieval. We implemented a full-fledged graph-based discovery system for the biomedical domain and demonstrated its benefits in the past. Unfortunately, graph-based retrieval methods generally follow an 'exact match' paradigm, which severely hampers search efficiency, since exact match results are hard to rank by relevance. This paper extends our existing discovery system and contributes effective graph-based unsupervised ranking methods, a new query relaxation paradigm, and ontological rewriting. These extensions improve the system further so that users can retrieve results with higher precision and higher recall due to partial matching and ontological rewriting.", 'abstract_zh': '基于关键词的检索是当今数字图书馆的标准方法。然而，在科学知识库中，复杂的检索场景需要更高级的访问路径。尽管每篇文档都对某一领域知识库有所贡献，关键词之间的具体结构，即它们可能存在的关系，以及文档内部所覆盖的具体语境对于有效的检索至关重要。按照这一逻辑，个体文档可以被视为小型的知识图，通过对这些图进行图形查询，可以实现对文档的聚焦检索。我们已经为生物医学领域实现了完整的图形基础发现系统，并在过去展示了其优势。不幸的是，基于图形的检索方法通常遵循“精确匹配”范式，这极大地影响了搜索效率，因为精确匹配的结果很难根据相关性进行排名。本文在现有发现系统的基础上进行了扩展，并贡献了有效的无监督图形基于的排名方法、新的查询宽松范式以及本体重写方法。这些扩展进一步改进了系统，使得用户可以通过部分匹配和本体重写获取更高精度和更高召回率的结果。', 'title_zh': '生物医学文档检索中基于叙事查询图的排名方法（技术报告）'}
{'arxiv_id': 'arXiv:2412.15229', 'title': 'Building an Explainable Graph-based Biomedical Paper Recommendation System (Technical Report)', 'authors': 'Hermann Kroll, Christin K. Kreutz, Bill Matthias Thang, Philipp Schaer, Wolf-Tilo Balke', 'link': 'https://arxiv.org/abs/2412.15229', 'abstract': 'Digital libraries provide different access paths, allowing users to explore their collections. For instance, paper recommendation suggests literature similar to some selected paper. Their implementation is often cost-intensive, especially if neural methods are applied. Additionally, it is hard for users to understand or guess why a recommendation should be relevant for them. That is why we tackled the problem from a different perspective. We propose XGPRec, a graph-based and thus explainable method which we integrate into our existing graph-based biomedical discovery system. Moreover, we show that XGPRec (1) can, in terms of computational costs, manage a real digital library collection with 37M documents from the biomedical domain, (2) performs well on established test collections and concept-centric information needs, and (3) generates explanations that proved to be beneficial in a preliminary user study. We share our code so that user libraries can build upon XGPRec.', 'abstract_zh': '数字图书馆提供了不同的访问途径，允许用户探索其收藏。例如，基于论文推荐可以建议与某些选定论文相似的文献。其实施往往成本高昂，尤其是如果应用了神经方法。此外，用户难以理解或推测出推荐为何与他们相关。因此，我们从不同的角度解决这一问题。我们提出了XGPRec，这是一种基于图的方法，因此具有可解释性，并将其集成到我们现有的基于图的生物医学发现系统中。此外，我们表明XGPRec（1）在计算成本方面，能够管理包含3700万份生物医学领域文档的真实数字图书馆收藏；（2）在现有测试集合和概念中心的信息需求上表现良好；（3）生成的解释已被初步用户研究证明是有益的。我们提供了代码以供用户图书馆借鉴和使用XGPRec。', 'title_zh': '构建一个可解释的图基生物医学论文推荐系统（技术报告）'}
{'arxiv_id': 'arXiv:2412.15957', 'title': 'From General to Specific: Tailoring Large Language Models for Personalized Healthcare', 'authors': 'Ruize Shi, Hong Huang, Wei Zhou, Kehan Yin, Kai Zhao, Yun Zhao', 'link': 'https://arxiv.org/abs/2412.15957', 'abstract': 'The rapid development of large language models (LLMs) has transformed many industries, including healthcare. However, previous medical LLMs have largely focused on leveraging general medical knowledge to provide responses, without accounting for patient variability and lacking true personalization at the individual level. To address this, we propose a novel method called personalized medical language model (PMLM), which explores and optimizes personalized LLMs through recommendation systems and reinforcement learning (RL). Specifically, by utilizing self-informed and peer-informed personalization, PMLM captures changes in behaviors and preferences to design initial personalized prompts tailored to individual needs. We further refine these initial personalized prompts through RL, ultimately enhancing the precision of LLM guidance. Notably, the personalized prompt are hard prompt, which grants PMLM high adaptability and reusability, allowing it to directly leverage high-quality proprietary LLMs. We evaluate PMLM using real-world obstetrics and gynecology data, and the experimental results demonstrate that PMLM achieves personalized responses, and it provides more refined and individualized services, offering a potential way for personalized medical LLMs.', 'abstract_zh': '大规模语言模型（LLMs）的迅速发展已经改变了众多行业，包括医疗保健行业。然而，之前的医疗LLMs主要侧重于利用通用的医学知识来提供响应，而没有考虑到患者的个体差异，并缺乏真正意义上的个性化服务。为了解决这一问题，我们提出了一种新颖的方法，称为个性化医疗语言模型（PMLM），该方法通过推荐系统和强化学习（RL）来探索和优化个性化LLMs。具体而言，PMLM利用自我指导和同伴指导的个性化方法，捕捉行为和偏好的变化，设计出针对个人需求的初始个性化提示。我们进一步通过RL来细化这些初始个性化提示，最终提升了LLM指导的精准度。值得注意的是，个性化提示作为一种硬提示，赋予PMLM高度的适应性和重复使用性，使其可以直接利用高质量的专有LLMs。我们使用实际的妇产科数据对PMLM进行了评估，实验结果表明PMLM能够实现个性化响应，提供更加精细和个性化的服务，为个性化医疗LLMs的实现提供了潜在途径。', 'title_zh': '从通用到具体：为个性化医疗定制大型语言模型'}
{'arxiv_id': 'arXiv:2412.15602', 'title': 'Music Genre Classification: Ensemble Learning with Subcomponents-level Attention', 'authors': 'Yichen Liu, Abhijit Dasgupta, Qiwei He', 'link': 'https://arxiv.org/abs/2412.15602', 'abstract': 'Music Genre Classification is one of the most popular topics in the fields of Music Information Retrieval (MIR) and digital signal processing. Deep Learning has emerged as the top performer for classifying music genres among various methods. The letter introduces a novel approach by combining ensemble learning with attention to sub-components, aiming to enhance the accuracy of identifying music genres. The core innovation of our work is the proposal to classify the subcomponents of the music pieces separately, allowing our model to capture distinct characteristics from those sub components. By applying ensemble learning techniques to these individual classifications, we make the final classification decision on the genre of the music. The proposed method has superior advantages in terms of accuracy compared to the other state-of-the-art techniques trained and tested on the GTZAN dataset.', 'abstract_zh': '音乐体裁分类是音乐信息检索（MIR）和数字信号处理领域中最热门的话题之一。深度学习已成为各种方法中分类音乐体裁的佼佼者。本文介绍了一种创新方法，结合了集成学习与对子组件的注意力机制，旨在提高识别音乐体裁的准确性。我们工作的核心创新是提出了单独对音乐片段的子组件进行分类的方案，使我们的模型能够捕捉到这些子组件的独特特征。通过将集成学习技术应用于这些个体分类，我们对音乐的体裁做出最终分类决策。与在GTZAN数据集上训练和测试的其他最新技术相比，所提出的方法在准确性方面具有明显优势。', 'title_zh': '音乐流派分类：基于子组件级注意力的集成学习'}
{'arxiv_id': 'arXiv:2412.15510', 'title': 'ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction using Sequence-To-Sequence Transformers', 'authors': 'Vinayak Arannil, Tomal Deb, Atanu Roy', 'link': 'https://arxiv.org/abs/2412.15510', 'abstract': "Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question-answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models don't require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94% on establishing the relationships between ADEs and the respective suspects.", 'abstract_zh': '早期识别不良药物事件（ADE）对于市场引入新药物时采取及时措施至关重要。这些ADE信息可以通过临床研究报告、患者健康记录、社交媒体帖子等多种非结构化数据源获得。利用机器学习从文本数据中提取ADE及其相关可疑药物是一项具有挑战性的任务，原因是药物-ADE对之间的复杂语言关系以及缺乏大量标注数据集。本文介绍了一种基于问答（QA）的方法——ADEQA，该方法使用准监督标注数据和序列到序列的变换器来提取ADE、可疑药物及其之间的关系。与传统问答模型不同，基于自然语言生成（NLG）的模型不需要 extensive 的标记粒度（token level labeling），从而大幅降低了采用门槛。在公共ADE数据集上，我们实现了94%的F1分数，成功建立了ADE与其相应可疑药物之间的关系，达到了当前的先进水平。', 'title_zh': 'ADEQA：一种基于问题回答的联合ADE-Suspect提取方法，使用序列到序列的变换器模型'}
{'arxiv_id': 'arXiv:2412.15396', 'title': 'Learning Visual Composition through Improved Semantic Guidance', 'authors': 'Austin Stone, Hagen Soltau, Robert Geirhos, Xi Yi, Ye Xia, Bingyi Cao, Kaifeng Chen, Abhijit Ogale, Jonathon Shlens', 'link': 'https://arxiv.org/abs/2412.15396', 'abstract': 'Visual imagery does not consist of solitary objects, but instead reflects the composition of a multitude of fluid concepts. While there have been great advances in visual representation learning, such advances have focused on building better representations for a small number of discrete objects bereft of an understanding of how these objects are interacting. One can observe this limitation in representations learned through captions or contrastive learning -- where the learned model treats an image essentially as a bag of words. Several works have attempted to address this limitation through the development of bespoke learned architectures to directly address the shortcomings in compositional learning. In this work, we focus on simple, and scalable approaches. In particular, we demonstrate that by substantially improving weakly labeled data, i.e. captions, we can vastly improve the performance of standard contrastive learning approaches. Previous CLIP models achieved near chance rate on challenging tasks probing compositional learning. However, our simple approach boosts performance of CLIP substantially and surpasses all bespoke architectures. Furthermore, we showcase our results on a relatively new captioning benchmark derived from DOCCI. We demonstrate through a series of ablations that a standard CLIP model trained with enhanced data may demonstrate impressive performance on image retrieval tasks.', 'abstract_zh': '视觉想象并不由孤立的对象构成，而是反映了多种流动概念的组合。尽管在视觉表示学习方面取得了巨大进步，但这些进展主要集中在构建更好对少数离散对象的表示，而忽略了这些对象之间相互作用的理解。这一局限性在通过描述词或对比学习学习到的表示中可以观察到——学习到的模型基本上将图像视为一组词语。已有的一些研究试图通过开发定制的学习架构来直接解决组合学习中的不足之处。在本文中，我们专注于简单且可扩展的方法。特别是在通过大幅提高弱标记数据（如描述词）来改进标准对比学习方法的效果方面，我们展示了这种方法的有效性。先前的CLIP模型在测试组合学习能力的具有挑战性的任务中仅能达到接近随机的水平。然而，我们简单的方法大大提升了CLIP的表现，并超越了所有定制的架构。此外，我们在一个新的基于DOCCI的描述词基准测试中展示了我们的结果。通过一系列消融实验表明，使用增强数据训练的标准CLIP模型可能在图像检索任务中表现出令人 impressive 的性能。', 'title_zh': '通过改进语义指导学习视觉 compositions'}
{'arxiv_id': 'arXiv:2412.15310', 'title': 'MRWeb: An Exploration of Generating Multi-Page Resource-Aware Web Code from UI Designs', 'authors': 'Yuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, Michael R. Lyu', 'link': 'https://arxiv.org/abs/2412.15310', 'abstract': 'Multi-page websites dominate modern web development. However, existing design-to-code methods rely on simplified assumptions, limiting to single-page, self-contained webpages without external resource connection. To address this gap, we introduce the Multi-Page Resource-Aware Webpage (MRWeb) generation task, which transforms UI designs into multi-page, functional web UIs with internal/external navigation, image loading, and backend routing. We propose a novel resource list data structure to track resources, links, and design components. Our study applies existing methods to the MRWeb problem using a newly curated dataset of 500 websites (300 synthetic, 200 real-world). Specifically, we identify the best metric to evaluate the similarity of the web UI, assess the impact of the resource list on MRWeb generation, analyze MLLM limitations, and evaluate the effectiveness of the MRWeb tool in real-world workflows. The results show that resource lists boost navigation functionality from 0% to 66%-80% while facilitating visual similarity. Our proposed metrics and evaluation framework provide new insights into MLLM performance on MRWeb tasks. We release the MRWeb tool, dataset, and evaluation framework to promote further research.', 'abstract_zh': '现代网页开发中，多页面网站占主导地位。然而，现有的从设计到代码的方法依赖于简化假设，无法处理具有外部资源连接的多页面、自包含网页。为解决这一问题，我们引入了多页面资源感知网页（MRWeb）生成任务，该任务将用户界面设计转换为具有内部/外部导航、图像加载和后端路由的多页面功能性网页UI。我们提出了一个新型资源列表数据结构，用于跟踪资源、链接和设计组件。我们的研究使用新编纂的500个网站数据集（300个合成网页，200个真实世界网站），将现有方法应用于MRWeb问题。具体而言，我们确定了评估网页UI相似性的最佳度量标准，评估了资源列表对MRWeb生成的影响，分析了大型语言模型（LLM）的局限性，并评估了MRWeb工具在实际工作流程中的有效性。结果显示，资源列表将导航功能从0%提升到66%-80%，同时促进了视觉相似性。我们提出的度量标准和评估框架为大规模语言模型在MRWeb任务中的性能提供了新的见解。我们发布了MRWeb工具、数据集和评估框架，以促进进一步的研究。', 'title_zh': 'MRWeb：从UI设计生成多页面资源感知型Web代码的探索'}
{'arxiv_id': 'arXiv:2412.15308', 'title': 'ViFactCheck: A New Benchmark Dataset and Methods for Multi-domain News Fact-Checking in Vietnamese', 'authors': 'Tran Thai Hoa, Tran Quang Duy, Khanh Quoc Tran, Kiet Van Nguyen', 'link': 'https://arxiv.org/abs/2412.15308', 'abstract': 'The rapid spread of information in the digital age highlights the critical need for effective fact-checking tools, particularly for languages with limited resources, such as Vietnamese. In response to this challenge, we introduce ViFactCheck, the first publicly available benchmark dataset designed specifically for Vietnamese fact-checking across multiple online news domains. This dataset contains 7,232 human-annotated pairs of claim-evidence combinations sourced from reputable Vietnamese online news, covering 12 diverse topics. It has been subjected to a meticulous annotation process to ensure high quality and reliability, achieving a Fleiss Kappa inter-annotator agreement score of 0.83. Our evaluation leverages state-of-the-art pre-trained and large language models, employing fine-tuning and prompting techniques to assess performance. Notably, the Gemma model demonstrated superior effectiveness, with an impressive macro F1 score of 89.90%, thereby establishing a new standard for fact-checking benchmarks. This result highlights the robust capabilities of Gemma in accurately identifying and verifying facts in Vietnamese. To further promote advances in fact-checking technology and improve the reliability of digital media, we have made the ViFactCheck dataset, model checkpoints, fact-checking pipelines, and source code freely available on GitHub. This initiative aims to inspire further research and enhance the accuracy of information in low-resource languages.', 'abstract_zh': '数字时代信息的迅速传播凸显了有效的事实核查工具的重要性，特别是对于资源有限的语言而言，如越南语。为应对这一挑战，我们介绍了ViFactCheck，这是首个专为越南语事实核查设计并公开提供的基准数据集，涵盖多个在线新闻领域。该数据集包含7,232个经过人工标注的声张-证据对，来源于可信的越南在线新闻，涵盖12个不同的主题。该数据集经过细致的标注过程，以确保高质量和可靠性，实现了Fleiss Kappa注释者间一致性评分0.83。我们通过使用最新的预训练和大型语言模型进行评估，采用了微调和提示技术来评估性能。值得注意的是，Gemma模型表现尤为出色，其宏F1分数达到89.90%，从而为事实核查基准设立了新的标准。该结果突显了Gemma在准确识别和验证越南语事实方面的坚实能力。为了进一步促进事实核查技术的进步和提高数字媒体的可靠性，我们已将ViFactCheck数据集、模型检查点、事实核查管道以及源代码免费发布在GitHub上。这一举措旨在激发更多的研究兴趣，并提高低资源语言中信息的准确性。', 'title_zh': 'ViFactCheck：一个多领域越南新闻事实核查的新基准数据集和方法'}
{'arxiv_id': 'arXiv:2412.15282', 'title': 'A Systematic Examination of Preference Learning through the Lens of Instruction-Following', 'authors': 'Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, Liang Tan', 'link': 'https://arxiv.org/abs/2412.15282', 'abstract': 'Preference learning is a widely adopted post-training technique that aligns large language models (LLMs) to human preferences and improves specific downstream task capabilities. In this work we systematically investigate how specific attributes of preference datasets affect the alignment and downstream performance of LLMs in instruction-following tasks. We use a novel synthetic data generation pipeline to generate 48,000 unique instruction-following prompts with combinations of 23 verifiable constraints that enable fine-grained and automated quality assessments of model responses. With our synthetic prompts, we use two preference dataset curation methods - rejection sampling (RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected) responses. Then, we perform experiments investigating the effects of (1) the presence of shared prefixes between the chosen and rejected responses, (2) the contrast and quality of the chosen, rejected responses and (3) the complexity of the training prompts. Our experiments reveal that shared prefixes in preference pairs, as generated by MCTS, provide marginal but consistent improvements and greater stability across challenging training configurations. High-contrast preference pairs generally outperform low-contrast pairs; however, combining both often yields the best performance by balancing diversity and learning efficiency. Additionally, training on prompts of moderate difficulty leads to better generalization across tasks, even for more complex evaluation scenarios, compared to overly challenging prompts. Our findings provide actionable insights into optimizing preference data curation for instruction-following tasks, offering a scalable and effective framework for enhancing LLM training and alignment.', 'abstract_zh': '偏好学习是一种广泛采用的后训练技术，用于使大型语言模型（LLMs）与人类偏好对齐，并提高特定下游任务的能力。在本项研究中，我们系统地探讨了偏好数据集特定属性如何影响LLMs在指令遵循任务中的对齐和下游性能。我们使用新颖的合成数据生成管道生成了48,000个唯一的指令遵循提示，这些提示结合了23个可验证的约束，能够进行细粒度和自动化的模型响应质量评估。使用我们的合成提示，我们采用了两种偏好数据集编纂方法——拒绝采样（RS）和蒙特卡洛树搜索（MCTS）——来获得（选择，拒绝）响应对。然后，我们进行了实验，以研究（1）选择和拒绝响应之间的共享前缀对；（2）选择和拒绝响应的对比度和质量；以及（3）训练提示的复杂性的影响。我们的实验表明，由MCTS生成的偏好对之间的共享前缀提供了边际但一致的改进和更高的稳定性，特别是在具有挑战性的训练配置中。高对比度的偏好对通常优于低对比度对；然而，将两者结合起来通常可以实现最佳性能，平衡多样性和学习效率。此外，在中等难度的提示上进行训练，即使在更复杂的评估场景中也能更好地促进任务泛化，相对于过于复杂的提示。我们的研究结果提供了可操作的见解，以优化指令遵循任务中的偏好数据编纂，提供了一种可扩展且有效的框架，以增强LLM的训练和对齐。', 'title_zh': '通过遵循指令的视角对偏好学习进行系统性研究'}
{'arxiv_id': 'arXiv:2412.15280', 'title': 'Context-DPO: Aligning Language Models for Context-Faithfulness', 'authors': 'Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Shenghua Liu', 'link': 'https://arxiv.org/abs/2412.15280', 'abstract': "Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment remains underexplored. To address this, we propose $\\textbf{Context-DPO}$, the first alignment method specifically designed to enhance LLMs' context-faithfulness. We introduce $\\textbf{ConFiQA}$, a benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts to evaluate context-faithfulness. By leveraging faithful and stubborn responses to questions with provided context from ConFiQA, our Context-DPO aligns LLMs through direct preference optimization. Extensive experiments demonstrate that our Context-DPO significantly improves context-faithfulness, achieving 35% to 280% improvements on popular open-source models. Further analysis demonstrates that Context-DPO preserves LLMs' generative capabilities while providing interpretable insights into context utilization. Our code and data are released at this https URL", 'abstract_zh': '大规模语言模型（LLMs）的可靠响应需要遵循用户指令并结合检索到的信息。虽然对齐技术有助于使LLMs与人类的意图和价值观保持一致，但通过对齐提高上下文保真度仍研究不足。为了应对这一挑战，我们提出了一种名为$\\textbf{Context-DPO}$的新方法，这是首个专门设计用于增强LLMs上下文保真度的对齐方法。我们引入了$\\textbf{ConFiQA}$基准，该基准模拟了包含知识冲突的检索增强生成（RAG）场景，用于评估上下文保真度。通过利用ConFiQA提供的上下文信息生成的忠实且固执的响应，我们的Context-DPO通过直接的偏好优化对齐LLMs。大量实验表明，我们的Context-DPO显著提高了上下文保真度，在流行的开源模型上实现了35%到280%的改进。进一步的分析表明，Context-DPO既保留了LLMs的生成能力，又提供了关于上下文利用的可解释见解。我们的代码和数据可以在以下链接获取：this https URL', 'title_zh': 'CONTEXT-DPO：提高语言模型背景一致性的对齐方法'}
{'arxiv_id': 'arXiv:2412.15272', 'title': 'SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation', 'authors': 'Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, Weiguo Zheng', 'link': 'https://arxiv.org/abs/2412.15272', 'abstract': 'Recent advancements in large language models (LLMs) have shown impressive versatility across various tasks. To eliminate its hallucinations, retrieval-augmented generation (RAG) has emerged as a powerful approach, leveraging external knowledge sources like knowledge graphs (KGs). In this paper, we study the task of KG-driven RAG and propose a novel Similar Graph Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively addresses the challenge of aligning query texts and KG structures through a two-stage process: (1) query-to-pattern, which uses an LLM to transform queries into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the alignment between the pattern and candidate subgraphs using a graph semantic distance (GSD) metric. We also develop an optimized retrieval algorithm that efficiently identifies the top-$k$ subgraphs within 1-second latency on a 10-million-scale KG. Extensive experiments show that SimGRAG outperforms state-of-the-art KG-driven RAG methods in both question answering and fact verification, offering superior plug-and-play usability and scalability.', 'abstract_zh': '近年来，大型语言模型（LLMs）在各种任务中展现了令人印象深刻的灵活性。为了消除其幻觉，检索增强生成（RAG）方法已经作为一种有力的手段出现了，它利用了知识图谱（KGs）等外部知识源。本文研究了基于KG的RAG任务，并提出了一种新颖的相似图增强检索增强生成（SimGRAG）方法。该方法通过一个两阶段过程有效解决了查询文本与KG结构对齐的挑战：（1）查询到模式，使用LLM将查询转换为所需的图模式；（2）模式到子图，使用图语义距离（GSD）度量来量化模式与候选子图之间的对齐程度。此外，我们还开发了一种优化的检索算法，该算法能在1秒钟的延迟内高效地在规模为1000万的知识图谱中识别出前k个子图。广泛的实验表明，SimGRAG在问题回答和事实验证方面均优于现有的基于KG的RAG方法，提供了卓越的即插即用可行性和可扩展性。', 'title_zh': 'SimGRAG：利用相似子图进行知识图驱动的检索增强生成'}
{'arxiv_id': 'arXiv:2412.15271', 'title': 'A MapReduce Approach to Effectively Utilize Long Context Information in Retrieval Augmented Language Models', 'authors': 'Gongbo Zhang, Zihan Xu, Qiao Jin, Fangyi Chen, Yilu Fang, Yi Liu, Justin F. Rousseau, Ziyang Xu, Zhiyong Lu, Chunhua Weng, Yifan Peng', 'link': 'https://arxiv.org/abs/2412.15271', 'abstract': 'While holding great promise for improving and facilitating healthcare, large language models (LLMs) struggle to produce up-to-date responses on evolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is a pivotal innovation that improves the accuracy and relevance of LLM responses by integrating LLMs with a search engine and external sources of knowledge. However, the quality of RAG responses can be largely impacted by the rank and density of key information in the retrieval results, such as the "lost-in-the-middle" problem. In this work, we aim to improve the robustness and reliability of the RAG workflow in the medical domain. Specifically, we propose a map-reduce strategy, BriefContext, to combat the "lost-in-the-middle" issue without modifying the model weights. We demonstrated the advantage of the workflow with various LLM backbones and on multiple QA datasets. This method promises to improve the safety and reliability of LLMs deployed in healthcare domains.', 'abstract_zh': '虽然大型语言模型（LLMs）在改善和促进医疗保健方面充满了潜力，但由于知识过时或幻觉，它们在生成关于不断演变的主题的最新回应方面面临挑战。检索增强生成（RAG，Retrieval-Augmented Generation）是一种关键创新，通过将LLMs与搜索引擎和外部知识来源结合使用，提高了LLMs响应的准确性和相关性。然而，RAG响应的质量往往受到检索结果中关键信息排名和密度的影响，如“迷失在中间”的问题。本文旨在增强医疗领域RAG工作流的稳健性和可靠性。具体而言，我们提出了一种映射-减少策略，称为简要上下文（BriefContext），以在不修改模型权重的情况下解决“迷失在中间”的问题。我们通过使用多种LLMs底座和多个问答数据集验证了该工作流的优势。该方法有望提高部署在医疗领域中的LLMs的安全性和可靠性。', 'title_zh': '一种利用MapReduce有效利用长上下文信息的检索增强语言模型方法'}
{'arxiv_id': 'arXiv:2412.15262', 'title': 'Advanced ingestion process powered by LLM parsing for RAG system', 'authors': 'Arnau Perez, Xavier Vizcaino', 'link': 'https://arxiv.org/abs/2412.15262', 'abstract': "Retrieval Augmented Generation (RAG) systems struggle with processing multimodal documents of varying structural complexity. This paper introduces a novel multi-strategy parsing approach using LLM-powered OCR to extract content from diverse document types, including presentations and high text density files both scanned or not. The methodology employs a node-based extraction technique that creates relationships between different information types and generates context-aware metadata. By implementing a Multimodal Assembler Agent and a flexible embedding strategy, the system enhances document comprehension and retrieval capabilities. Experimental evaluations across multiple knowledge bases demonstrate the approach's effectiveness, showing improvements in answer relevancy and information faithfulness.", 'abstract_zh': '基于检索增强生成（RAG）系统在处理结构复杂度各异的多模态文档方面存在挑战。本文介绍了一种新颖的多策略解析方法，利用大型语言模型（LLM）驱动的光学字符识别（OCR）技术，从不同类型的文件中提取内容，包括演示文稿和高文本密度文件（无论是扫描还是未扫描）。该方法采用基于节点的提取技术，创建不同类型信息之间的关系，并生成上下文感知的元数据。通过实施多模态组装代理和灵活的嵌入策略，系统增强了文档理解和检索能力。在多个知识库上的实验评估表明，该方法的有效性，在回答相关性和信息忠实性方面表现出改进。', 'title_zh': '基于LLM解析的高级检索过程用于 Retrieval-Augmented Generation 系统'}
{'arxiv_id': 'arXiv:2412.15247', 'title': 'Streamlining Systematic Reviews: A Novel Application of Large Language Models', 'authors': 'Fouad Trad, Ryan Yammine, Jana Charafeddine, Marlene Chakhtoura, Maya Rahme, Ghada El-Hajj Fuleihan, Ali Chehab', 'link': 'https://arxiv.org/abs/2412.15247', 'abstract': "Systematic reviews (SRs) are essential for evidence-based guidelines but are often limited by the time-consuming nature of literature screening. We propose and evaluate an in-house system based on Large Language Models (LLMs) for automating both title/abstract and full-text screening, addressing a critical gap in the literature. Using a completed SR on Vitamin D and falls (14,439 articles), the LLM-based system employed prompt engineering for title/abstract screening and Retrieval-Augmented Generation (RAG) for full-text screening. The system achieved an article exclusion rate (AER) of 99.5%, specificity of 99.6%, a false negative rate (FNR) of 0%, and a negative predictive value (NPV) of 100%. After screening, only 78 articles required manual review, including all 20 identified by traditional methods, reducing manual screening time by 95.5%. For comparison, Rayyan, a commercial tool for title/abstract screening, achieved an AER of 72.1% and FNR of 5% when including articles Rayyan considered as undecided or likely to include. Lowering Rayyan's inclusion thresholds improved FNR to 0% but increased screening time. By addressing both screening phases, the LLM-based system significantly outperformed Rayyan and traditional methods, reducing total screening time to 25.5 hours while maintaining high accuracy. These findings highlight the transformative potential of LLMs in SR workflows by offering a scalable, efficient, and accurate solution, particularly for the full-text screening phase, which has lacked automation tools.", 'abstract_zh': '系统评价（SRs）对于基于证据的指南至关重要，但文献筛选往往耗时较长，限制了其应用。我们提出并评估了一种基于大型语言模型（LLMs）的自研系统，用于自动化标题/摘要筛选和全文筛选，填补了文献中的一项关键空白。利用一项已完成的维生素D与跌倒相关的SR（共14,439篇文章），该基于LLM的系统使用提示工程进行标题/摘要筛选，并使用检索增强生成（RAG）进行全文筛选。系统实现了99.5%的文章排除率（AER）、99.6%的特异性、0%的假阴性率（FNR）和100%的阴性预测值（NPV）。在筛选之后，仅需人工复查78篇文章，其中包括传统方法识别出的全部20篇文章，减少了95.5%的手动筛选时间。相比之下，商业化工具Rayyan在包括Rayyan认为待决定或很可能包含的文章时，实现了72.1%的文章排除率（AER）和5%的假阴性率（FNR）。降低Rayyan的纳入阈值可将假阴性率降至0%，但增加了筛选时间。通过同时解决两个筛选阶段，基于LLM的系统显著优于Rayyan和传统方法，在保持高准确性的前提下将总体筛选时间缩短至25.5小时。这些发现突显了大型语言模型在SR工作流程中具有变革性潜力，提供了一种可扩展、高效且准确的解决方案，特别是在全文筛选阶段缺乏自动化工具的情况下。', 'title_zh': '简化系统评价：大型语言模型的一项新型应用'}
{'arxiv_id': 'arXiv:2412.15246', 'title': 'Accelerating Retrieval-Augmented Generation', 'authors': 'Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad Alian', 'link': 'https://arxiv.org/abs/2412.15246', 'abstract': "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG.\nIn this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7-26.3x lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM, which is the most expensive component in today's servers, from being stranded.", 'abstract_zh': '解决大型语言模型（LLMs）幻觉问题并提高其准确性的不断演化的解决方案是检索增强生成（RAG），这种技术通过从外部知识来源（例如网络）检索信息来增强LLMs。本文概述了几种RAG执行管道，并阐明了其检索和生成阶段之间的复杂交互关系。我们展示了虽然精确检索方案成本较高，但与近似检索变体相比，它们可以在保持相同端到端准确性的前提下减少推理时间，因为精确检索模型可以向生成模型发送更小但更准确的文档列表。这一观察结果促使我们加速RAG中的精确最近邻搜索。\n\n在本文中，我们设计了一种名为智能知识存储（IKS）的CXL类型2设备，它实现了扩展的近内存加速架构，并在主机CPU和近内存加速器之间采用了一种新颖的缓存一致性接口。相比于在Intel Sapphire Rapids CPU上执行搜索，IKS在512GB向量数据库上的精确最近邻搜索速度提高了13.4到27.9倍。这种更高的搜索性能转化为代表性的RAG应用程序中1.7到26.3倍的端到端推理时间降低。IKS本质上是一种内存扩展器；其内部DRAM可以分离并用于服务器上的其他应用程序，从而防止当今服务器中最昂贵的组件——DRAM——被闲置。', 'title_zh': '加速检索增强生成'}
{'arxiv_id': 'arXiv:2412.15241', 'title': 'Quantifying Positional Biases in Text Embedding Models', 'authors': 'Samarth Goel, Reagan J. Lee, Kannan Ramchandran', 'link': 'https://arxiv.org/abs/2412.15241', 'abstract': 'Embedding models are crucial for tasks in Information Retrieval (IR) and semantic similarity measurement, yet their handling of longer texts and associated positional biases remains underexplored. In this study, we investigate the impact of content position and input size on text embeddings. Our experiments reveal that embedding models, irrespective of their positional encoding mechanisms, disproportionately prioritize the beginning of an input. Ablation studies demonstrate that insertion of irrelevant text or removal at the start of a document reduces cosine similarity between altered and original embeddings by up to 12.3\\% more than ablations at the end. Regression analysis further confirms this bias, with sentence importance declining as position moves further from the start, even with with content-agnosticity. We hypothesize that this effect arises from pre-processing strategies and chosen positional encoding techniques. These findings quantify the sensitivity of retrieval systems and suggest a new lens towards embedding model robustness.', 'abstract_zh': '嵌入模型对于信息检索（IR）任务和语义相似度测量至关重要，但它们对长文本及其相关位置偏好的处理仍然未被充分探索。在这种研究中，我们探讨了内容位置和输入大小对文本嵌入的影响。我们的实验揭示，不论嵌入模型使用何种位置编码机制，它们都会过度优先处理输入的开头部分。消融实验表明，在文档开头插入无关文本或删除文本，会使修改后的嵌入与原始嵌入的余弦相似度降低高达12.3%以上，而这种影响在文档结尾处的消融实验中更为温和。进一步的回归分析也证实了这种偏见，随着位置远离开头，命题的重要性呈下降趋势，即使在的内容无关联性的情况下也是如此。我们认为这种效应源于预处理策略和选择的位置编码技术。这些发现量化了检索系统的敏感性，并为嵌入模型的稳健性提供了一个新的视角。', 'title_zh': '量化文本嵌入模型中的位置偏见'}
{'arxiv_id': 'arXiv:2412.15093', 'title': 'Nano-ESG: Extracting Corporate Sustainability Information from News Articles', 'authors': 'Fabian Billert, Stefan Conrad', 'link': 'https://arxiv.org/abs/2412.15093', 'abstract': 'Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.\nAn independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at this https URL.', 'abstract_zh': '评估企业的可持续性影响是一个高度复杂的问题，近年来引起了越来越多的关注。今天，投资者主要依赖于来自成熟评级机构的可持续性评级来分析企业的行为是否符合社会责任。然而，这些评级最近因难以理解且几乎无法复制而受到批评。\n\n一种独立的方法是利用丰富的新闻文章数据来了解企业的可持续性实践。在本文中，我们探索了一种不同的方法来识别企业在可持续性领域的关键机遇和挑战。我们提供了一个包含超过840,000篇新闻文章的新数据集，这些文章是在2023年1月至2024年9月期间为德国主要企业收集的。通过应用多种自然语言处理技术，我们首先识别出相关文章，然后对这些文章进行总结，并使用大型语言模型（LLMs）提取与可持续性相关的观点和方面。此外，我们对获得的数据进行了评估，并确定LLM生成的答案是准确的。我们在此共享这两个数据集：[此链接]。', 'title_zh': '纳米ESG：从新闻文章中提取企业 Sustainability 信息'}
{'arxiv_id': 'arXiv:2412.15005', 'title': 'DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain Recommendation', 'authors': 'Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju', 'link': 'https://arxiv.org/abs/2412.15005', 'abstract': 'Recommender systems are widely used in various real-world applications, but they often encounter the persistent challenge of the user cold-start problem. Cross-domain recommendation (CDR), which leverages user interactions from one domain to improve prediction performance in another, has emerged as a promising solution. However, users with similar preferences in the source domain may exhibit different interests in the target domain. Therefore, directly transferring embeddings may introduce irrelevant source-domain collaborative information. In this paper, we propose a novel graph-based disentangled contrastive learning framework to capture fine-grained user intent and filter out irrelevant collaborative information, thereby avoiding negative transfer. Specifically, for each domain, we use a multi-channel graph encoder to capture diverse user intents. We then construct the affinity graph in the embedding space and perform multi-step random walks to capture high-order user similarity relationships. Treating one domain as the target, we propose a disentangled intent-wise contrastive learning approach, guided by user similarity, to refine the bridging of user intents across domains. Extensive experiments on four benchmark CDR datasets demonstrate that DisCo consistently outperforms existing state-of-the-art baselines, thereby validating the effectiveness of both DisCo and its components.', 'abstract_zh': '推荐系统在各种实际应用场景中得到了广泛应用，但它们经常面临用户冷启动问题这一持续性的挑战。跨域推荐（CDR），通过利用一个领域中的用户交互来提高另一个领域预测性能，已经成为了潜在的解决方案。然而，源领域的用户可能在目标领域表现出不同的兴趣偏好，因此直接转移嵌入可能会引入无关的源领域协作信息。在本文中，我们提出了一种新颖的基于图的去纠缠对比学习框架，以捕捉精细的用户意图并过滤掉无关的协作信息，从而避免负迁移。具体而言，对于每一领域，我们使用多通道图编码器来捕获多样的用户意图。接着，我们构建了嵌入空间中的亲和图，并执行多步随机游走以捕捉高级用户的相似关系。将一个领域视为目标领域，我们提出了基于用户相似性的去纠缠意图对比学习方法，以改进跨领域用户意图的桥梁构建。在四个基准CDR数据集上的广泛实验表明，DisCo始终优于现有的先进基线方法，从而验证了DisCo及其组件的有效性。', 'title_zh': 'DisCo：基于图的解纠缠对比学习在冷启动跨域推荐中的应用'}
{'arxiv_id': 'arXiv:2412.14978', 'title': 'Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation', 'authors': 'Rongqing Kenneth Ong, Andy W. H. Khong', 'link': 'https://arxiv.org/abs/2412.14978', 'abstract': 'Incorporating multi-modal features as side information has recently become a trend in recommender systems. To elucidate user-item preferences, recent studies focus on fusing modalities via concatenation, element-wise sum, or attention mechanisms. Despite having notable success, existing approaches do not account for the modality-specific noise encapsulated within each modality. As a result, direct fusion of modalities will lead to the amplification of cross-modality noise. Moreover, the variation of noise that is unique within each modality results in noise alleviation and fusion being more challenging. In this work, we propose a new Spectrum-based Modality Representation (SMORE) fusion graph recommender that aims to capture both uni-modal and fusion preferences while simultaneously suppressing modality noise. Specifically, SMORE projects the multi-modal features into the frequency domain and leverages the spectral space for fusion. To reduce dynamic contamination that is unique to each modality, we introduce a filter to attenuate and suppress the modality noise adaptively while capturing the universal modality patterns effectively. Furthermore, we explore the item latent structures by designing a new multi-modal graph learning module to capture associative semantic correlations and universal fusion patterns among similar items. Finally, we formulate a new modality-aware preference module, which infuses behavioral features and balances the uni- and multi-modal features for precise preference modeling. This empowers SMORE with the ability to infer both user modality-specific and fusion preferences more accurately. Experiments on three real-world datasets show the efficacy of our proposed model. The source code for this work has been made publicly available at this https URL.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\n近年来，将多模态特征作为辅助信息已在推荐系统中成为一种趋势。为了阐明用户与项目之间的偏好，最近的研究侧重于通过连接、按元素求和或注意力机制来融合各类模态。尽管这些方法取得了显著的成功，但现有的方法并没有考虑到每种模态中特有的噪声。因此，直接融合模态会导致跨模态噪声的放大。此外，每种模态内独有的噪声变异使得噪声抑制和融合变得更加困难。在此项工作中，我们提出了一种基于频谱的模态表示（Spectrum-based Modality Representation, SMORE）融合图推荐算法，旨在同时捕捉单模态偏好和融合偏好，同时抑制模态噪声。具体来说，SMORE将多模态特征投影到频域，并利用频谱空间进行融合。为减少每种模态特有的动态污染，我们引入了一个滤波器，以适应性地衰减和抑制模态噪声，同时有效捕捉通用模态模式。此外，我们通过设计一个新的多模态图学习模块来探索项目的潜在结构，以捕捉相似项目之间关联的语义关系和通用融合模式。最后，我们构架了一个新的模态感知偏好模块，该模块融合了行为特征并平衡单模态和多模态特征，以实现精确的偏好建模。这一能力使SMORE能够更准确地推断出用户的模态特异偏好和融合偏好。在三个真实世界数据集上的实验结果表明，我们所提出的模型具有有效性。该工作的源代码已在此处公开 <https://link.to.source.code>。', 'title_zh': '基于谱的模态表示融合图卷积网络多模态推荐'}
{'arxiv_id': 'arXiv:2412.14967', 'title': 'ECLIPSE: Contrastive Dimension Importance Estimation with Pseudo-Irrelevance Feedback for Dense Retrieval', 'authors': "Giulio D'Erasmo, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri", 'link': 'https://arxiv.org/abs/2412.14967', 'abstract': 'Recent advances in Information Retrieval have leveraged high-dimensional embedding spaces to improve the retrieval of relevant documents. Moreover, the Manifold Clustering Hypothesis suggests that despite these high-dimensional representations, documents relevant to a query reside on a lower-dimensional, query-dependent manifold. While this hypothesis has inspired new retrieval methods, existing approaches still face challenges in effectively separating non-relevant information from relevant signals. We propose a novel methodology that addresses these limitations by leveraging information from both relevant and non-relevant documents. Our method, ECLIPSE, computes a centroid based on irrelevant documents as a reference to estimate noisy dimensions present in relevant ones, enhancing retrieval performance. Extensive experiments on three in-domain and one out-of-domain benchmarks demonstrate an average improvement of up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10 w.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our results pave the way for more robust, pseudo-irrelevance-based retrieval systems in future IR research.', 'abstract_zh': '近年来，信息检索领域的进展通过利用高维嵌入空间来提高相关文档的检索效率。此外，流形聚类假设表明，尽管存在这些高维表示，与查询相关的文档实际上位于一个由查询定义的低维流形上。尽管这一假设激发了新的检索方法，但现有方法仍面临有效分离噪声信息与相关信号的问题。我们提出了一种新的方法，通过利用来自相关和非相关文档的信息来解决这些限制。我们的方法ECLIPSE基于非相关文档计算一个质心，作为参考来估计相关文档中存在的噪声维度，从而提高检索性能。在三个领域内的和一个领域外的基准测试上的广泛实验表明，与基于DIME的方法（分别）相比，在mAP（AP）上平均改进了19.50%（分别22.35%），在nDCG@10上分别改进了11.42%（分别13.10%）。我们的结果铺平了未来信息检索研究中更稳健的伪不可靠性基检系统的发展之路。', 'title_zh': 'ECLIPSE：基于伪无关反馈的对比维度重要性估计在密集检索中的应用'}
