# Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents 

**Title (ZH)**: 使用内存优化的大语言模型基用户代理增强跨域推荐 

**Authors**: Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13843)  

**Abstract**: Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at this https URL. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的用户代理已经成为通过模拟用户交互来改善推荐系统的一种强大工具。然而，现有的方法在跨域场景中效率低下，主要是由于缺乏高效的内存结构，导致无法保留相关的信息，并且不能很好地考虑到如流行度等社会影响因素。为了解决这些限制，我们提出了AgentCF++，这是一种新型框架，具备双重内存架构和两步融合机制，以有效过滤领域特定的偏好。此外，我们还提出了共享内存的兴趣群体，使模型能够捕捉对具有相似兴趣的用户产生影响的流行趋势。通过在多个跨域数据集上进行广泛的实验，AgentCF++在基线模型上显示出了优越的性能，证明了其在细化推荐系统中用户行为模拟方面有效性。我们的代码已发布在此 <https://> 地址。 

---
# SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering? 

**Title (ZH)**: SearchRAG：搜索引擎能对基于LLM的医疗问答有所帮助吗？ 

**Authors**: Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13233)  

**Abstract**: Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge. 

**Abstract (ZH)**: 大型语言模型（LLMs）在通用领域展现了令人瞩目的能力，但在需要专门知识的任务上往往表现不佳。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些知识库可能存在过时或不完整的问题，缺乏准确回答医疗问题所必需的精细临床细节。在本研究中，我们提出了一种名为SearchRAG的新框架，该框架通过利用实时搜索引擎克服了这些限制。我们的方法通过合成查询生成将复杂的医疗问题转换为搜索引擎友好的查询，并利用不确定性驱动的知识选择来筛选并整合最相关和最有信息量的医疗知识作为LLM的输入。实验结果表明，我们的方法在医疗问题回答任务中显著提高了响应的准确性，特别是在需要详细和最新知识的复杂问题上。 

---
# Causes and Strategies in Multiagent Systems 

**Title (ZH)**: 多智能体系统中的原因与策略研究 

**Authors**: Sylvia S. Kerkhove, Natasha Alechina, Mehdi Dastani  

**Link**: [PDF](https://arxiv.org/pdf/2502.13701)  

**Abstract**: Causality plays an important role in daily processes, human reasoning, and artificial intelligence. There has however not been much research on causality in multi-agent strategic settings. In this work, we introduce a systematic way to build a multi-agent system model, represented as a concurrent game structure, for a given structural causal model. In the obtained so-called causal concurrent game structure, transitions correspond to interventions on agent variables of the given causal model. The Halpern and Pearl framework of causality is used to determine the effects of a certain value for an agent variable on other variables. The causal concurrent game structure allows us to analyse and reason about causal effects of agents' strategic decisions. We formally investigate the relation between causal concurrent game structures and the original structural causal models. 

**Abstract (ZH)**: 因果性在日常流程、人类推理和人工智能中扮演着重要角色。然而，对于多智能体的战略设置中的因果性研究却相对较少。在本项工作中，我们介绍了一种系统的方法来构建给定结构性因果模型的多智能体系统模型，该模型表示为并发博弈结构。在所获得的所谓的因果并发博弈结构中，状态转移对应于对给定因果模型中智能体变量所进行的干预。我们利用Halpern和Pearl的因果性框架来确定某个智能体变量特定值对其他变量的影响。因果并发博弈结构使我们能够分析和推断智能体战略决策的因果效应。我们从形式上研究了因果并发博弈结构与原始结构性因果模型之间的关系。 

---
# Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning 

**Title (ZH)**: 遗传算法驱动的多任务 reinforcement learning 模型演化框架 

**Authors**: Yan Yu, Wengang Zhou, Yaodong Yang, Wanxuan Lu, Yingyan Hou, Houqiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.13569)  

**Abstract**: Multi-task reinforcement learning employs a single policy to complete various tasks, aiming to develop an agent with generalizability across different scenarios. Given the shared characteristics of tasks, the agent's learning efficiency can be enhanced through parameter sharing. Existing approaches typically use a routing network to generate specific routes for each task and reconstruct a set of modules into diverse models to complete multiple tasks simultaneously. However, due to the inherent difference between tasks, it is crucial to allocate resources based on task difficulty, which is constrained by the model's structure. To this end, we propose a Model Evolution framework with Genetic Algorithm (MEGA), which enables the model to evolve during training according to the difficulty of the tasks. When the current model is insufficient for certain tasks, the framework will automatically incorporate additional modules, enhancing the model's capabilities. Moreover, to adapt to our model evolution framework, we introduce a genotype module-level model, using binary sequences as genotype policies for model reconstruction, while leveraging a non-gradient genetic algorithm to optimize these genotype policies. Unlike routing networks with fixed output dimensions, our approach allows for the dynamic adjustment of the genotype policy length, enabling it to accommodate models with a varying number of modules. We conducted experiments on various robotics manipulation tasks in the Meta-World benchmark. Our state-of-the-art performance demonstrated the effectiveness of the MEGA framework. We will release our source code to the public. 

**Abstract (ZH)**: 多任务强化学习通过单一策略完成各种任务，旨在培养一个能够在不同场景下具有泛化能力的智能体。鉴于任务共享的特征，通过参数共享可以提高智能体的学习效率。现有的方法通常使用路由网络为每个任务生成特定的路径，并重新构建一组模块为多种任务同时处理。然而，由于任务之间的固有差异，根据任务难度分配资源至关重要，这受到模型结构的限制。为此，我们提出了一种基于遗传算法的模型演进框架（MEGA），该框架可以在训练过程中根据任务难度使模型进化。当当前模型对某些任务不足时，框架将自动引入额外模块，提升模型的能力。此外，为了适应我们的模型演进框架，我们引入了一种基因型模块级模型，使用二进制序列作为基因型策略进行模型重构，并采用非梯度遗传算法优化这些基因型策略。与输出维度固定的路由网络不同，我们的方法允许动态调整基因型策略长度，使其能够适应具有不同模块数量的模型。我们在Meta-World基准中的各种机器人操作任务上进行了实验。我们的领先表现证明了MEGA框架的有效性。我们将公开发布我们的源代码。 

---
# Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges 

**Title (ZH)**: 将代理人工智能与6G网络集成用于关键任务应用：案例研究与挑战 

**Authors**: Sunder Ali Khowaja, Kapal Dev, Muhammad Salman Pathan, Engin Zeydan, Merouane Debbah  

**Link**: [PDF](https://arxiv.org/pdf/2502.13476)  

**Abstract**: We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks. 

**Abstract (ZH)**: 我们正处在变革的时代，特别是在人工智能（AI）领域，尤其是基础模型方面的最新进展持续受到关注。AI 已经成为众多依赖自动化服务交付的应用的重要组成部分，其中一个关键应用领域是至关重要的公共安全应用。基于AI的至关重要的应用存在的问题是带有人机循环系统的系统以及在保持态势感知的同时对动态条件缺乏适应性。近期，代理型人工智能（Agentic AI, AAI）因其能够通过上下文视角分析文本数据并在条件变化时迅速适应而备受关注。在此背景下，本文提出了一种针对至关重要的应用的AAI框架。我们提出了一种具有多层架构的新型框架以实现AAI。此外，我们还详细介绍了AAI层次结构，该层次结构可以填补网络基础设施与至关重要的应用之间的差距。初步分析表明，AAI可以将初始响应时间平均减少5.6分钟，警报生成时间平均减少15.6秒，并且资源分配改善了13.4%。我们还展示了AAI方法将同时操作的数量提高40%，从而将恢复时间最多缩短5.2分钟。最后，我们讨论了在实施AAI框架时需要考虑的一些问题和挑战。 

---
# Vision-Based Generic Potential Function for Policy Alignment in Multi-Agent Reinforcement Learning 

**Title (ZH)**: 基于视觉的通用潜在函数在多代理 reinforcement 学习策略对齐中的应用 

**Authors**: Hao Ma, Shijie Wang, Zhiqiang Pu, Siyao Zhao, Xiaolin Ai  

**Link**: [PDF](https://arxiv.org/pdf/2502.13430)  

**Abstract**: Guiding the policy of multi-agent reinforcement learning to align with human common sense is a difficult problem, largely due to the complexity of modeling common sense as a reward, especially in complex and long-horizon multi-agent tasks. Recent works have shown the effectiveness of reward shaping, such as potential-based rewards, to enhance policy alignment. The existing works, however, primarily rely on experts to design rule-based rewards, which are often labor-intensive and lack a high-level semantic understanding of common sense. To solve this problem, we propose a hierarchical vision-based reward shaping method. At the bottom layer, a visual-language model (VLM) serves as a generic potential function, guiding the policy to align with human common sense through its intrinsic semantic understanding. To help the policy adapts to uncertainty and changes in long-horizon tasks, the top layer features an adaptive skill selection module based on a visual large language model (vLLM). The module uses instructions, video replays, and training records to dynamically select suitable potential function from a pre-designed pool. Besides, our method is theoretically proven to preserve the optimal policy. Extensive experiments conducted in the Google Research Football environment demonstrate that our method not only achieves a higher win rate but also effectively aligns the policy with human common sense. 

**Abstract (ZH)**: 将多智能体强化学习的政策引导与人类常识相一致是一个具有挑战性的问题，主要原因是将常识建模为奖励的复杂性，尤其是在复杂的长时间多智能体任务中尤为突出。近期的研究表明，基于潜在函数的奖励塑造等方法能有效提高政策与人类常识的对齐程度。然而，现有的工作主要依赖专家设计基于规则的奖励，这通常耗时且缺乏对常识高层语义的理解。为了解决这一问题，我们提出了一种分层的基于视觉的奖励塑造方法。在最低层，一个视觉-语言模型（VLM）作为通用的潜在函数，通过其内在的语义理解引导政策与人类常识保持一致。为了帮助政策适应长时间任务中的不确定性与变化，最高层采用了一个基于视觉大型语言模型（vLLM）的自适应技能选择模块。该模块利用指令、视频回放和训练记录动态从预先设计的池中选择合适的潜在函数。此外，我们的方法从理论上证明能保持最优政策。在Google Research足球环境中的广泛实验表明，我们的方法不仅提高了胜率，还有效使政策与人类常识保持一致。 

---
# Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation 

**Title (ZH)**: 原子近端策略优化在电动机器人出租车调度与充电站分配中的应用 

**Authors**: Jim Dai, Manxi Wu, Zhanhao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.13392)  

**Abstract**: Pioneering companies such as Waymo have deployed robo-taxi services in several U.S. cities. These robo-taxis are electric vehicles, and their operations require the joint optimization of ride matching, vehicle repositioning, and charging scheduling in a stochastic environment. We model the operations of the ride-hailing system with robo-taxis as a discrete-time, average reward Markov Decision Process with infinite horizon. As the fleet size grows, the dispatching is challenging as the set of system state and the fleet dispatching action set grow exponentially with the number of vehicles. To address this, we introduce a scalable deep reinforcement learning algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the action space using atomic action decomposition. We evaluate our algorithm using real-world NYC for-hire vehicle data and we measure the performance using the long-run average reward achieved by the dispatching policy relative to a fluid-based reward upper bound. Our experiments demonstrate the superior performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct extensive numerical experiments to analyze the efficient allocation of charging facilities and assess the impact of vehicle range and charger speed on fleet performance. 

**Abstract (ZH)**: 效仿Waymo等先锋企业已在美多个城市部署自动驾驶出租车服务。这些自动驾驶出租车是电动车辆，其运营需要在随机环境中对行程匹配、车辆调度和充电计划进行联合优化。我们将搭载自动驾驶出租车的网约车系统模型化为一个具有无限展望期的离散时间平均奖励马尔可夫决策过程（MDP）。随着车队规模的增长，调度变得极具挑战性，因为系统状态集和调度动作集会随着车辆数量的增加而指数级增长。为了解决这一问题，我们引入了一种可扩展的深度强化学习算法，称为原子级 proximal 政策优化算法（Atomic-PPO），该算法通过原子级动作分解减少了动作空间。我们使用实际的纽约市网约车数据评估了该算法，并通过调度策略相对于基于流体的方法奖励上限实现的长期平均奖励来衡量性能。我们的实验表明，与基准方法相比，我们的Atomic-PPO具有更优的性能。此外，我们还进行了广泛的数值实验，以分析充电设施的有效分配，并评估车辆续航能力和充电速度对车队性能的影响。 

---
# Reasoning with Reinforced Functional Token Tuning 

**Title (ZH)**: 强化功能标记调优的推理方法 

**Authors**: Kongcheng Zhang, Qi Yao, Baisheng Lai, Jiaxing Huang, Wenkai Fang, Dacheng Tao, Mingli Song, Shunyu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13389)  

**Abstract**: In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel reinforced fine-tuning framework that empowers Large Language Models (LLMs) with self-play learn-to-reason capabilities. Unlike prior prompt-driven reasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g., <analyze>, <verify>, <refine>) directly into the model vocabulary, enabling chain-of-thought construction with diverse human-like reasoning behaviors. Specifically, RFTT comprises two phases: (1) supervised fine-tuning performs prompt-driven tree search to obtain self-generated training data annotated with functional tokens, which warms up the model to learn these tokens for reasoning; and (2) online reinforcement learning further allows the model to explore different reasoning pathways through functional token sampling without relying on prompts, thereby facilitating effective self-improvement for functional reasoning. Extensive experiments demonstrate the superiority of the proposed RFTT on mathematical benchmarks, significantly boosting Qwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to 60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently improves with more search rollouts at inference time. Our code is available at this https URL. 

**Abstract (ZH)**: 在这项工作中，我们提出了一种新型的强化函数标记调优框架——强化功能标记调优（RFTT），旨在增强大型语言模型（LLMs）的自博弈学习推理能力。与之前的基于提示的推理努力不同，RFTT 将一组丰富的可学习功能标记（例如 <analyze>、<verify>、<refine>）直接嵌入到模型词汇表中，使模型能够构建具有多种类人类推理行为的推理链条。具体而言，RFTT 包含两个阶段：（1）监督调优通过基于提示的树搜索获取带有功能标记的自动生成训练数据，以预热模型学习这些用于推理的功能标记；（2）在线强化学习进一步允许模型通过功能标记采样探索不同的推理路径，而无需依赖提示，从而促进功能推理的有效自我改进。大量的实验表明，RFTT 在数学基准测试中表现优越，在 MATH 数据集上显著提升了 Qwen-2.5-7B-Instruct（从 70.6% 提高到 79.8%）和 LLaMA-3.1-8B-Instruct（从 32.2% 提高到 60.2%）的效果。此外，我们在推理时进行更多搜索展开时，RFTT 的性能也持续改进。我们的代码可在以下 URL 获取：this https URL。 

---
# Reflection of Episodes: Learning to Play Game from Expert and Self Experiences 

**Title (ZH)**: 基于片段的反映：从专家和自我经验学习玩游戏 

**Authors**: Xiaojie Xu, Zongyuan Li, Chang Lu, Runnan Qi, Yanan Ni, Lumin Jiang, Xiangbei Liu, Xuebo Zhang, Yongchun Fang, Kuihua Huang, Xian Guo, Zhanghua Wu, Zhenya Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.13388)  

**Abstract**: StarCraft II is a complex and dynamic real-time strategy (RTS) game environment, which is very suitable for artificial intelligence and reinforcement learning research. To address the problem of Large Language Model(LLM) learning in complex environments through self-reflection, we propose a Reflection of Episodes(ROE) framework based on expert experience and self-experience. This framework first obtains key information in the game through a keyframe selection method, then makes decisions based on expert experience and self-experience. After a game is completed, it reflects on the previous experience to obtain new self-experience. Finally, in the experiment, our method beat the robot under the Very Hard difficulty in TextStarCraft II. We analyze the data of the LLM in the process of the game in detail, verified its effectiveness. 

**Abstract (ZH)**: 星际争霸II是一个复杂且动态的即时战略（RTS）游戏环境，非常适合人工智能和强化学习的研究。为了通过自我反思解决大规模语言模型（LLM）在复杂环境中的学习问题，我们提出了基于专家经验和自我经验的事件反思（Reflection of Episodes, ROE）框架。该框架首先通过关键帧选择方法获取游戏中的关键信息，然后基于专家经验和自我经验做出决策。在一场游戏结束后，它会反思之前的经历以获得新的自我经验。最后，在实验中，我们的方法在TextStarCraft II的非常困难模式下击败了机器人。我们详细分析了游戏中大规模语言模型的数据，验证了该方法的有效性。 

---
# Fighter Jet Navigation and Combat using Deep Reinforcement Learning with Explainable AI 

**Title (ZH)**: 使用可解释人工智能的深度强化学习在战斗机导航与作战中的应用 

**Authors**: Swati Kar, Soumyabrata Dey, Mahesh K Banavar, Shahnewaz Karim Sakib  

**Link**: [PDF](https://arxiv.org/pdf/2502.13373)  

**Abstract**: This paper presents the development of an Artificial Intelligence (AI) based fighter jet agent within a customized Pygame simulation environment, designed to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's primary objectives include efficiently navigating the environment, reaching a target, and selectively engaging or evading an enemy. A reward function balances these goals while optimized hyperparameters enhance learning efficiency. Results show more than 80\% task completion rate, demonstrating effective decision-making. To enhance transparency, the jet's action choices are analyzed by comparing the rewards of the actual chosen action (factual action) with those of alternate actions (counterfactual actions), providing insights into the decision-making rationale. This study illustrates DRL's potential for multi-objective problem-solving with explainable AI. Project page is available at: \href{this https URL}{Project GitHub Link}. 

**Abstract (ZH)**: 本文介绍了在自定义Pygame仿真环境中开发的一种基于人工智能（AI）的战斗机智能代理的研究成果，该代理通过深度强化学习（DRL）解决多目标任务。飞机的主要目标包括高效导航环境、到达目标以及选择性地与敌人交战或规避。奖励函数平衡了这些目标，优化的超参数提高了学习效率。结果显示任务完成率超过80%，表明其具有有效的决策能力。为了增强透明度，通过将实际选择的行动（事实行动）的奖励与替代行动（假设行动）的奖励进行比较，分析飞机的行动选择，从而深入了解决策过程的依据。本研究展示了可解释AI在基于DRL的多目标问题解决中的潜力。项目页面可以通过以下链接访问：\href{该项目的GitHub链接}{项目GitHub链接}。 

---
# Demonstrating specification gaming in reasoning models 

**Title (ZH)**: 在推理模型中展示规范游戏行为 

**Authors**: Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish  

**Link**: [PDF](https://arxiv.org/pdf/2502.13295)  

**Abstract**: We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack.
We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing. 

**Abstract (ZH)**: 我们通过指示模型战胜国际象棋引擎来演示大规模语言模型（LLM）代理规范博弈。我们发现，如o1 Preview和DeepSeek-R1这类逻辑推理模型往往会默认作弊，而像GPT-4o和Claude 3.5 Sonnet这类语言模型则需要明确被告知正常对弈无法作弊。

我们改进了前人的工作（如Hubinger等人，2024；Meinke等人，2024；Wei等人，2024），通过使用实际的任务提示并避免过度引导。我们的结果表明，逻辑推理模型可能会为了解决复杂问题而采取作弊手段，这与OpenAI（2024）在网络安全能力测试中观察到的o1 Docker脱逃现象一致。 

---
# Autellix: An Efficient Serving Engine for LLM Agents as General Programs 

**Title (ZH)**: Autellix：一种高效的服务引擎，用于将大型语言模型代理视为通用程序 

**Authors**: Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica  

**Link**: [PDF](https://arxiv.org/pdf/2502.13965)  

**Abstract**: Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM. 

**Abstract (ZH)**: 大型语言模型（LLM）的应用正在超越简单的聊天机器人，发展成为动态的、通用的代理程序，这些程序通过扩展LLM调用和输出token来帮助AI代理进行推理、探索和解决复杂的任务。然而，现有的LLM服务系统忽视了程序之间的依赖关系，错失了优化的重要机会。我们分析表明，提交给LLM服务引擎的程序会经历长时间的累计等待时间，主要原因是在单个LLM请求和程序级别上出现了头部阻塞。为了解决这一问题，我们提出了一种名为Autellix的LLM服务系统，将程序视为一等公民，以最小化其端到端的延迟。Autellix拦截程序提交的LLM调用，并为调度器提供程序级别上下文。我们提出了两种调度算法——针对单线程和分布式程序的算法，基于程序之前已完成的调用来预取和优先处理LLM调用。我们的评估表明，与现有的最先进的系统如vLLM相比，Autellix在相同延迟的情况下，将程序的吞吐量提高了4-15倍。 

---
# RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision 

**Title (ZH)**: RAG-Gym：通过过程监督优化推理和搜索代理 

**Authors**: Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.13957)  

**Abstract**: Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at this https URL. 

**Abstract (ZH)**: 检索增强生成（RAG）在知识密集型任务中展现出了巨大的潜力，但其传统的架构依赖于静态检索，这限制了其在需要顺序信息搜索的复杂问题上的有效性。虽然主动推理和搜索提供了更具适应性的方法，但大多数现有方法仍然高度依赖于提示工程。在本工作中，我们引入了RAG-Gym，这是一种统一的优化框架，通过在每个检索步骤中提供细致的过程监督来增强信息查询代理。我们还提出了ReSearch，这是一种新的代理架构，在RAG-Gym框架中结合了答案推理和搜索查询生成。在四个具有挑战性的数据集中进行的实验表明，RAG-Gym在各种代理架构中提高了高达25.6%的性能，而ReSearch在所有基线中始终保持优异表现。进一步的分析强调了高级语言模型作为过程奖励裁判的有效性，并展示了训练好的奖励模型在不同语言模型中作为验证者的可迁移性。此外，我们还研究了主动RAG的训练和推理的扩展性能。该项目的主页可通过以下链接访问：[这是一个网址]。 

---
# DataSciBench: An LLM Agent Benchmark for Data Science 

**Title (ZH)**: DataSciBench：一个用于数据科学的大型语言模型代理基准测试 

**Authors**: Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue  

**Link**: [PDF](https://arxiv.org/pdf/2502.13897)  

**Abstract**: This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at this https URL. 

**Abstract (ZH)**: 本文介绍了DataSciBench，这是一个全面的基准测试，用于评估大型语言模型（LLM）在数据科学领域的能力。最近的相关基准测试主要集中在单一任务、易于获得的地面真实值和简便的评估指标上，这限制了可以评估的任务范围。相比之下，DataSciBench 是基于更为全面和精挑细选的自然且具有挑战性的提示构建的，适用于不确定的地面真实值和评估指标。我们开发了一种半自动管道来生成地面真实值（GT）并验证评估指标。该管道利用并实现了基于LLM的自我一致性与人工验证策略，通过利用收集的提示、预定义的任务类型和聚合函数（指标）来生成准确的GT。此外，我们提出了一种创新的Task-Function-Code（TFC）框架，根据精确定义的度量标准和编程规则来评估每次代码执行的结果。我们的实验框架包括使用我们收集的多样化的提示测试6种API基模型、8种开源通用模型和9种开源代码生成模型。这种方法旨在提供更全面和严格的LLM在数据科学领域的评估，揭示它们的优势和劣势。实验结果表明，API基模型在所有指标上均优于开源模型，Deepseek-Coder-33B-Instruct 在开源模型中得分最高。我们已在此链接 https://... 上发布了所有代码和数据。 

---
# Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization 

**Title (ZH)**: 海报：SpiderSim：面向工业数字化的多Agent驱动生成网络安全理论仿真 

**Authors**: Jiaqi Li, Xizhong Guo, Yang Zhao, Lvyang Zhang, Lidong Zhai  

**Link**: [PDF](https://arxiv.org/pdf/2502.13778)  

**Abstract**: Rapid industrial digitalization has created intricate cybersecurity demands that necessitate effective validation methods. While cyber ranges and simulation platforms are widely deployed, they frequently face limitations in scenario diversity and creation efficiency. In this paper, we present SpiderSim, a theoretical cybersecurity simulation platform enabling rapid and lightweight scenario generation for industrial digitalization security research. At its core, our platform introduces three key innovations: a structured framework for unified scenario modeling, a multi-agent collaboration mechanism for automated generation, and modular atomic security capabilities for flexible scenario composition. Extensive implementation trials across multiple industrial digitalization contexts, including marine ranch monitoring systems, validate our platform's capacity for broad scenario coverage with efficient generation processes. Built on solid theoretical foundations and released as open-source software, SpiderSim facilitates broader research and development in automated security testing for industrial digitalization. 

**Abstract (ZH)**: 快速工业数字化催生了复杂网络安全需求，这要求有有效的验证方法。虽然网络范围和仿真平台广泛部署，但在场景多样性和生成效率方面仍存在局限性。本文介绍了SpiderSim，这是一种理论上的网络安全仿真平台，能够实现快速和轻量级的场景生成，以满足工业数字化安全研究的需求。该平台的核心创新包括三个方面：一个统一的结构化场景建模框架、一种自动化的多代理协作机制以及模块化的原子安全能力，以支持灵活的场景组合。通过在多个工业数字化背景下（包括海洋牧场监控系统）进行广泛的实施试验，验证了该平台在高效生成过程中具备广泛的场景覆盖能力。基于坚实的理论基础并作为开源软件发布，SpiderSim促进了自动化安全测试在工业数字化领域的更广泛研究和开发。 

---
# GPA: Grover Policy Agent for Generating Optimal Quantum Sensor Circuits 

**Title (ZH)**: GPA：格罗ver策略代理生成最优量子传感器电路 

**Authors**: Ahmad Alomari, Sathish A. P. Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2502.13755)  

**Abstract**: This study proposes a GPA for designing optimal Quantum Sensor Circuits (QSCs) to address complex quantum physics problems. The GPA consists of two parts: the Quantum Policy Evaluation (QPE) and the Quantum Policy Improvement (QPI). The QPE performs phase estimation to generate the search space, while the QPI utilizes Grover search and amplitude amplification techniques to efficiently identify an optimal policy that generates optimal QSCs. The GPA generates QSCs by selecting sequences of gates that maximize the Quantum Fisher Information (QFI) while minimizing the number of gates. The QSCs generated by the GPA are capable of producing entangled quantum states, specifically the squeezed states. High QFI indicates increased sensitivity to parameter changes, making the circuit useful for quantum state estimation and control tasks. Evaluation of the GPA on a QSC that consists of two qubits and a sequence of R_x, R_y, and S gates demonstrates its efficiency in generating optimal QSCs with a QFI of 1. Compared to existing quantum agents, the GPA achieves higher QFI with fewer gates, demonstrating a more efficient and scalable approach to the design of QSCs. This work illustrates the potential computational power of quantum agents for solving quantum physics problems 

**Abstract (ZH)**: 本文提出了一种基于梯度方法（Gradient Method, GPA）的设计最优量子传感器电路（Quantum Sensor Circuits, QSCs）的方法，以解决复杂的量子物理问题。GPA由两个部分组成：量子策略评估（Quantum Policy Evaluation, QPE）和量子策略改进（Quantum Policy Improvement, QPI）。QPE通过相位估计生成搜索空间，而QPI则利用Grover搜索和振幅放大技术高效地识别出能够生成最优QSCs的最优策略。GPA通过选择最大化量子费雪信息（Quantum Fisher Information, QFI）同时最小化门的数量的门序列来生成QSCs。由GPA生成的QSCs能够产生纠缠量子态，特别是压缩态。高QFI表明对参数变化的敏感度增加，使得电路适用于量子态估计和控制任务。在由两个量子位和一系列R_x，R_y和S门组成的QSC上评估GPA，证明了其生成具有QFI为1的最优QSCs的效率。与现有的量子代理相比，GPA使用较少的门实现了更高的QFI，展示了设计QSCs更加高效和可扩展的方法。本文展示了量子代理在解决量子物理问题方面的潜在计算能力。 

---
# An LLM-based Agent for Reliable Docker Environment Configuration 

**Title (ZH)**: 基于大语言模型的代理用于可靠的Docker环境配置 

**Authors**: Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao  

**Link**: [PDF](https://arxiv.org/pdf/2502.13681)  

**Abstract**: Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment "pollution" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%. 

**Abstract (ZH)**: 软件开发中的环境配置是至关重要的一步，尤其是在处理不熟悉的代码仓库时。虽然大型语言模型（LLMs）展示了完成软件工程任务的潜力，但现有的环境配置方法往往依赖于手动努力或脆弱的脚本，导致效率低下且结果不可靠。我们引入了Repo2Run，这是首个基于LLM的代理，旨在完全自动化环境配置并生成适用于任意Python仓库的可执行Dockerfile。我们解决了两个主要挑战：（1）使LLM代理能够在隔离的Docker容器内配置环境，（2）确保配置过程的成功记录能够准确无误地转移到Dockerfile中，不会出错。为实现这一目标，我们提出了原子配置合成，该方法采用双环境架构（内部环境和外部环境），并具有回滚机制以防止由于命令失败而导致的环境“污染”，从而保证原子执行（要么完全执行，要么根本不执行），并通过Dockerfile生成器将成功的配置步骤转换为可运行的Dockerfile。我们在420个具有单元测试的最近Python仓库提出的基准上评估了Repo2Run，其成功率达到了86.0%，比最佳基线高出63.9%。 

---
# Decentralized Planning Using Probabilistic Hyperproperties 

**Title (ZH)**: 使用概率超性质进行去中心化规划 

**Authors**: Francesco Pontiggia, Filip Macák, Roman Andriushchenko, Michele Chiari, Milan Češka  

**Link**: [PDF](https://arxiv.org/pdf/2502.13621)  

**Abstract**: Multi-agent planning under stochastic dynamics is usually formalised using decentralized (partially observable) Markov decision processes ( MDPs) and reachability or expected reward specifications. In this paper, we propose a different approach: we use an MDP describing how a single agent operates in an environment and probabilistic hyperproperties to capture desired temporal objectives for a set of decentralized agents operating in the environment. We extend existing approaches for model checking probabilistic hyperproperties to handle temporal formulae relating paths of different agents, thus requiring the self-composition between multiple MDPs. Using several case studies, we demonstrate that our approach provides a flexible and expressive framework to broaden the specification capabilities with respect to existing planning techniques. Additionally, we establish a close connection between a subclass of probabilistic hyperproperties and planning for a particular type of Dec-MDPs, for both of which we show undecidability. This lays the ground for the use of existing decentralized planning tools in the field of probabilistic hyperproperty verification. 

**Abstract (ZH)**: 在具有随机动态的多智能体规划中，通常使用去中心化的（部分可观测的）马尔可夫决策过程（Markov decision processes, MDPs）和可达性或期望奖励规范进行形式化。本文提出了一种不同的方法：我们使用描述单个智能体在环境中的操作的MDP，并使用概率超属性来捕捉一组在环境中操作的去中心化智能体的期望时间目标。我们将现有的概率超属性的模型检查方法扩展到处理不同智能体路径之间的临时公式，从而要求在多个MDP之间进行自我组合。通过几个案例研究，我们证明了本方法提供了一个灵活且具表现力的框架，以增强相对于现有规划技术的规范能力。此外，我们建立了概率超属性的一个子类与特定类型Dec-MDP规划之间的紧密联系，并证明了这两种情况下的不可判定性。这一结果为在概率超属性验证领域利用现有的去中心化规划工具奠定了基础。 

---
# Learning Symbolic Task Decompositions for Multi-Agent Teams 

**Title (ZH)**: 多智能体团队中的符号任务分解学习 

**Authors**: Ameesh Shah, Niklas Lauffer, Thomas Chen, Nikhil Pitta, Sanjit A. Seshia  

**Link**: [PDF](https://arxiv.org/pdf/2502.13376)  

**Abstract**: One approach for improving sample efficiency in cooperative multi-agent learning is to decompose overall tasks into sub-tasks that can be assigned to individual agents. We study this problem in the context of reward machines: symbolic tasks that can be formally decomposed into sub-tasks. In order to handle settings without a priori knowledge of the environment, we introduce a framework that can learn the optimal decomposition from model-free interactions with the environment. Our method uses a task-conditioned architecture to simultaneously learn an optimal decomposition and the corresponding agents' policies for each sub-task. In doing so, we remove the need for a human to manually design the optimal decomposition while maintaining the sample-efficiency benefits of improved credit assignment. We provide experimental results in several deep reinforcement learning settings, demonstrating the efficacy of our approach. Our results indicate that our approach succeeds even in environments with codependent agent dynamics, enabling synchronous multi-agent learning not achievable in previous works. 

**Abstract (ZH)**: 提高合作多智能体学习样本效率的一种方法是将整体任务分解为可以分配给个体智能体的子任务。我们在此背景下研究了这一问题：即奖励机器：能够形式化分解为子任务的符号任务。为了处理无法事先了解环境的情况，我们提出了一种可以从与环境的无模型交互中学习最优分解的框架。我们的方法采用任务条件化的架构，同时学习每个子任务的最优分解和相应的智能体策略。通过这种方式，我们消除了需要人工设计最优分解的必要性，同时保留了改进归因带来的样本效率优势。我们在多个深度强化学习场景中提供了实验结果，证明了我们方法的有效性。我们的结果表明，即使在智能体动态相互依赖的环境中，我们的方法也能成功实现同步多智能体学习，这是之前工作所无法实现的。 

---
# Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance 

**Title (ZH)**: 调整信任水平：减轻由信任引起的对AI辅助的不当依赖 

**Authors**: Tejas Srinivasan, Jesse Thomason  

**Link**: [PDF](https://arxiv.org/pdf/2502.13321)  

**Abstract**: Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration. 

**Abstract (ZH)**: 用户对AI推荐的信任偏差如何影响AI辅助决策任务中的依赖行为。低信任水平导致用户过度依赖，而高信任水平则导致用户过分依赖。我们提出，AI助理应通过信任自适应干预来调整其行为以减轻这种不适当的依赖。例如，在用户信任水平较低时，提供解释可以促使用户更加谨慎地考虑助理的建议。在两个决策场景中，即公众回答科学问题和医生进行医疗诊断，我们发现，在低信任水平时提供支持性解释，在高信任水平时提供反向解释，可以减少最多38%的不适当依赖，并提高决策准确性约20%。我们还能够通过适时插入强制暂停来促进深思熟虑，以减少过分依赖。实验结果强调了AI适应用户信任如何促进适当依赖，为我们改善人机协作提供了令人兴奋的途径。 

---
# Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors 

**Title (ZH)**: 将以下论文的内容或标题翻译成中文，并符合学术规范：

Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors

翻译成中文：

逐步验证器的训练以用于对话式导师代理：大型语言模型作为你的编程导师的有趣案例 

**Authors**: Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai  

**Link**: [PDF](https://arxiv.org/pdf/2502.13311)  

**Abstract**: Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks. 

**Abstract (ZH)**: 由大规模语言模型（LLMs）驱动的智能辅导代理在语言学习和科学教育等领域提供了个性化的指导，但它们在引导用户解决复杂实际任务方面的能力仍然很少被探索。为了解决这一局限性，本文专注于编码辅导这一具有挑战性的问题，要求辅导代理积极引导学生完成既定的编码任务。我们提出了一种新颖的代理工作流程，即轨迹与验证（TRAVER）方法，该方法结合了知识追踪以估计学生的知识状态，并通过回合制验证以确保有效引导完成任务。我们引入了DICT协议，这是一种自动评估方法，通过受控的学生模拟和代码生成测试全面评估辅导代理。通过大量实验揭示了编码辅导的挑战，并证明TRAVER实现了显著更高的成功率。尽管本文中我们以代码辅导为例，但我们的结果和发现可以推广到其他任务，为各种任务的辅导代理的发展提供了宝贵的见解。 

---
# Communication Strategy on Macro-and-Micro Traffic State in Cooperative Deep Reinforcement Learning for Regional Traffic Signal Control 

**Title (ZH)**: 区域交通信号控制中基于合作深度强化学习的宏观-微观交通状态通信策略 

**Authors**: Hankang Gu, Shangbo Wang, Dongyao Jia, Yuli Zhang, Yanrong Luo, Guoqiang Mao, Jianping Wang, Eng Gee Lim  

**Link**: [PDF](https://arxiv.org/pdf/2502.13248)  

**Abstract**: Adaptive Traffic Signal Control (ATSC) has become a popular research topic in intelligent transportation systems. Regional Traffic Signal Control (RTSC) using the Multi-agent Deep Reinforcement Learning (MADRL) technique has become a promising approach for ATSC due to its ability to achieve the optimum trade-off between scalability and optimality. Most existing RTSC approaches partition a traffic network into several disjoint regions, followed by applying centralized reinforcement learning techniques to each region. However, the pursuit of cooperation among RTSC agents still remains an open issue and no communication strategy for RTSC agents has been investigated. In this paper, we propose communication strategies to capture the correlation of micro-traffic states among lanes and the correlation of macro-traffic states among intersections. We first justify the evolution equation of the RTSC process is Markovian via a system of store-and-forward queues. Next, based on the evolution equation, we propose two GAT-Aggregated (GA2) communication modules--GA2-Naive and GA2-Aug to extract both intra-region and inter-region correlations between macro and micro traffic states. While GA2-Naive only considers the movements at each intersection, GA2-Aug also considers the lane-changing behavior of vehicles. Two proposed communication modules are then aggregated into two existing novel RTSC frameworks--RegionLight and Regional-DRL. Experimental results demonstrate that both GA2-Naive and GA2-Aug effectively improve the performance of existing RTSC frameworks under both real and synthetic scenarios. Hyperparameter testing also reveals the robustness and potential of our communication modules in large-scale traffic networks. 

**Abstract (ZH)**: 自适应交通信号控制（ATSC）已成为智能交通系统中的热门研究课题。区域交通信号控制（RTSC）利用多智能体深度强化学习（MADRL）技术已成为ATSC的一种有前途的方法，这得益于其在可扩展性和优化性之间实现最优折衷的能力。大多数现有的RTSC方法将交通网络分割成几个不相交区域，然后针对每个区域应用集中式强化学习技术。然而，RTSC智能体之间的合作追求仍然是一个未解决的问题，尚未有针对RTSC智能体的通信策略的研究。在这篇文章中，我们提出了通信策略来捕捉车道之间微观交通状态的相关性以及交叉口之间宏观交通状态的相关性。我们首先通过一系列存储转发队列证明RTSC过程的进化方程是马尔可夫的。基于进化方程，我们提出了两种基于GAT聚集（GA2）的通信模块——GA2-Naive和GA2-Aug，以提取宏观和微观交通状态在区域内和区域间的相关性。GA2-Naive仅考虑每个交叉口的交通流动，而GA2-Aug也考虑了车辆的变道行为。我们随后将两种提出的通信模块整合到两种现有的新型RTSC框架——RegionLight和Regional-DRL中。实验结果表明，在真实和合成场景下，GA2-Naive和GA2-Aug都能够有效提高现有RTSC框架的性能。超参数测试还揭示了我们的通信模块在大规模交通网络中的鲁棒性和潜力。 

---
# Learning To Explore With Predictive World Model Via Self-Supervised Learning 

**Title (ZH)**: 通过自我监督学习利用预测世界模型进行探索学习 

**Authors**: Alana Santana, Paula P. Costa, Esther L. Colombini  

**Link**: [PDF](https://arxiv.org/pdf/2502.13200)  

**Abstract**: Autonomous artificial agents must be able to learn behaviors in complex environments without humans to design tasks and rewards. Designing these functions for each environment is not feasible, thus, motivating the development of intrinsic reward functions. In this paper, we propose using several cognitive elements that have been neglected for a long time to build an internal world model for an intrinsically motivated agent. Our agent performs satisfactory iterations with the environment, learning complex behaviors without needing previously designed reward functions. We used 18 Atari games to evaluate what cognitive skills emerge in games that require reactive and deliberative behaviors. Our results show superior performance compared to the state-of-the-art in many test cases with dense and sparse rewards. 

**Abstract (ZH)**: 自主人工代理必须能够在人类无需设计任务和奖励的情况下，在复杂环境中学习行为。为每个环境设计这些功能是不切实际的，因此激励了内在奖励功能的发展。在本文中，我们提出使用长期以来被忽视的认知元素来构建自主驱动代理的内部世界模型。我们的代理能够与环境进行满意的交互，并在无需预先设计奖励函数的情况下学习复杂行为。我们使用18个雅达利游戏来评估哪些认知技能会在需要反应行为和深思熟虑行为的游戏中涌现。结果显示，与现有的最佳方法相比，在许多密集奖励和稀疏奖励的测试案例中，我们的方法具有更好的性能。 

---
# Conditional Max-Sum for Asynchronous Multiagent Decision Making 

**Title (ZH)**: 异步多智能体决策制定中的条件最大和算法 

**Authors**: Dimitrios Troullinos, Georgios Chalkiadakis, Ioannis Papamichail, Markos Papageorgiou  

**Link**: [PDF](https://arxiv.org/pdf/2502.13194)  

**Abstract**: In this paper we present a novel approach for multiagent decision making in dynamic environments based on Factor Graphs and the Max-Sum algorithm, considering asynchronous variable reassignments and distributed message-passing among agents. Motivated by the challenging domain of lane-free traffic where automated vehicles can communicate and coordinate as agents, we propose a more realistic communication framework for Factor Graph formulations that satisfies the above-mentioned restrictions, along with Conditional Max-Sum: an extension of Max-Sum with a revised message-passing process that is better suited for asynchronous settings. The overall application in lane-free traffic can be viewed as a hybrid system where the Factor Graph formulation undertakes the strategic decision making of vehicles, that of desired lateral alignment in a coordinated manner; and acts on top of a rule-based method we devise that provides a structured representation of the lane-free environment for the factors, while also handling the underlying control of vehicles regarding core operations and safety. Our experimental evaluation showcases the capabilities of the proposed framework in problems with intense coordination needs when compared to a domain-specific baseline without communication, and an increased adeptness of Conditional Max-Sum with respect to the standard algorithm. 

**Abstract (ZH)**: 在本文中，我们提出了一种基于因子图和Max-Sum算法的新型多智能体决策方法，该方法考虑了异步变量重新赋值和智能体之间的分布式消息传递。受无车道交通这一具有挑战性的领域的启发，其中自动驾驶车辆可以作为智能体进行通信和协调，我们提出了一个满足上述限制的更具现实意义的通信框架，并引入了条件Max-Sum：这是一个改进的Max-Sum算法的扩展版本，更适用于异步环境。在无车道交通中的总体应用可以视为一种混合系统，其中因子图表征承担车辆的策略决策，实现协调的侧向对齐；同时，它基于我们设计的一种基于规则的方法进行运作，为因子提供结构化的表示，并处理车辆在核心操作和安全方面的底层控制。我们的实验评估显示，与未通信的领域特定基线相比，所提出的框架在需要大量协调的问题上展现出了更强的能力，同时条件Max-Sum在处理标准算法方面也表现出更高的适应性。 

---
# Unveiling Privacy Risks in LLM Agent Memory 

**Title (ZH)**: 揭示大规模语言模型代理记忆中的隐私风险 

**Authors**: Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang  

**Link**: [PDF](https://arxiv.org/pdf/2502.13172)  

**Abstract**: Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment. 

**Abstract (ZH)**: 大语言模型（LLM）代理在各种实际应用中越来越普遍。它们通过在记忆模块中储存私用户-代理交互来增强决策能力，从而引入了新的隐私风险。本研究系统地探讨了在黑盒设置下，我们提出的记忆提取攻击（MEXTRA）对LLM代理的脆弱性。为了从记忆中提取私有信息，我们提出了一种有效的攻击提示设计方法，以及基于对LLM代理不同知识水平的自动化提示生成方法。在两个典型代理上的实验表明了MEXTRA的有效性。此外，我们从代理方和攻击方的角度探讨了影响记忆泄露的关键因素。我们的发现强调了在LLM代理设计和部署中需要有效的记忆保护措施的紧迫性。 

---
# HedgeAgents: A Balanced-aware Multi-agent Financial Trading System 

**Title (ZH)**: HedgeAgents：一种兼顾平衡的多agent金融市场交易系统 

**Authors**: Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, Xiangmin Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13165)  

**Abstract**: As automated trading gains traction in the financial market, algorithmic investment strategies are increasingly prominent. While Large Language Models (LLMs) and Agent-based models exhibit promising potential in real-time market analysis and trading decisions, they still experience a significant -20% loss when confronted with rapid declines or frequent fluctuations, impeding their practical application. Hence, there is an imperative to explore a more robust and resilient framework. This paper introduces an innovative multi-agent system, HedgeAgents, aimed at bolstering system robustness via ``hedging'' strategies. In this well-balanced system, an array of hedging agents has been tailored, where HedgeAgents consist of a central fund manager and multiple hedging experts specializing in various financial asset classes. These agents leverage LLMs' cognitive capabilities to make decisions and coordinate through three types of conferences. Benefiting from the powerful understanding of LLMs, our HedgeAgents attained a 70% annualized return and a 400% total return over a period of 3 years. Moreover, we have observed with delight that HedgeAgents can even formulate investment experience comparable to those of human experts (this https URL). 

**Abstract (ZH)**: 随着自动化交易在金融市场中的逐渐普及，算法投资策略日益突出。虽然大型语言模型（LLMs）和基于代理的模型在实时市场分析和交易决策方面展现出巨大的潜力，但在面对急剧下跌或频繁波动时，它们仍然会遭受高达20%的损失，这阻碍了它们的实际应用。因此，有必要探索更加 robust 和 resilient 的框架。本文介绍了一种创新的多代理系统——HedgeAgents，旨在通过“对冲”策略来增强系统的稳健性。在这个平衡良好的系统中，已经设计了一系列对冲代理，HedgeAgents 包括一个中央资金管理者和多个专注于不同金融资产类别的对冲专家。这些代理利用 LLMs 的认知能力做出决策并通过三种类型的会议进行协调。得益于 LLMs 强大的理解能力，我们的 HedgeAgents 在三年的时间内实现了70%的年化回报率和400%的总回报率。此外，我们高兴地观察到，HedgeAgents 即使在投资经验方面也能够媲美人类专家（请参阅此链接: [请插入链接]）。 

---
# Multi-Agent Actor-Critic Generative AI for Query Resolution and Analysis 

**Title (ZH)**: 多代理actor-critic生成式AI在查询解析与分析中的应用 

**Authors**: Mohammad Wali Ur Rahman, Ric Nevarez, Lamia Tasnim Mim, Salim Hariri  

**Link**: [PDF](https://arxiv.org/pdf/2502.13164)  

**Abstract**: In this paper, we introduce MASQRAD (Multi-Agent Strategic Query Resolution and Diagnostic tool), a transformative framework for query resolution based on the actor-critic model, which utilizes multiple generative AI agents. MASQRAD is excellent at translating imprecise or ambiguous user inquiries into precise and actionable requests. This framework generates pertinent visualizations and responses to these focused queries, as well as thorough analyses and insightful interpretations for users. MASQRAD addresses the common shortcomings of existing solutions in domains that demand fast and precise data interpretation, such as their incapacity to successfully apply AI for generating actionable insights and their challenges with the inherent ambiguity of user queries. MASQRAD functions as a sophisticated multi-agent system but "masquerades" to users as a single AI entity, which lowers errors and enhances data interaction. This approach makes use of three primary AI agents: Actor Generative AI, Critic Generative AI, and Expert Analysis Generative AI. Each is crucial for creating, enhancing, and evaluating data interactions. The Actor AI generates Python scripts to generate data visualizations from large datasets within operational constraints, and the Critic AI rigorously refines these scripts through multi-agent debate. Finally, the Expert Analysis AI contextualizes the outcomes to aid in decision-making. With an accuracy rate of 87\% when handling tasks related to natural language visualization, MASQRAD establishes new benchmarks for automated data interpretation and showcases a noteworthy advancement that has the potential to revolutionize AI-driven applications. 

**Abstract (ZH)**: 在本文中，我们介绍了一种名为MASQRAD（多代理战略查询解析与诊断工具）的框架，该框架基于演员-评论者模型，利用了多个生成式人工智能代理。MASQRAD能够将模糊或含糊的用户查询精确化和具体化为可执行的请求。该框架生成与这些集中查询相关的相关可视化和响应，并为用户提供详细的分析和见解。MASQRAD解决了现有解决方案在需要快速精确数据解释的领域中的常见不足，如它们无法成功应用AI生成可执行见解以及用户查询固有的模糊性带来的挑战。MASQRAD作为一个复杂的多代理系统，但在用户面前“伪装”为单一个人工智能实体，降低了错误率并提升了数据交互的质量。这种方法利用了三种主要的AI代理：演员生成式AI、评论者生成式AI和专家分析生成式AI。每个代理对于创建、增强和评估数据交互都是至关重要的。演员AI生成Python脚本来在操作约束内从大规模数据集中生成数据可视化，评论者AI通过多代理辩论严谨地改进这些脚本。最后，专家分析AI将结果置于上下文中，以辅助决策。在自然语言可视化任务处理中，MASQRAD的准确率达到87%，并建立了自动数据解释的新基准，展示了具有潜力革新人工智能驱动应用的重要进展。 

---
# Noumenal Labs White Paper: How To Build A Brain 

**Title (ZH)**: noumenal labs白皮书：如何构建一个大脑 

**Authors**: Maxwell J. D. Ramstead, Candice Pattisapu, Jason Fox, Jeff Beck  

**Link**: [PDF](https://arxiv.org/pdf/2502.13161)  

**Abstract**: This white paper describes some of the design principles for artificial or machine intelligence that guide efforts at Noumenal Labs. These principles are drawn from both nature and from the means by which we come to represent and understand it. The end goal of research and development in this field should be to design machine intelligences that augment our understanding of the world and enhance our ability to act in it, without replacing us. In the first two sections, we examine the core motivation for our approach: resolving the grounding problem. We argue that the solution to the grounding problem rests in the design of models grounded in the world that we inhabit, not mere word models. A machine super intelligence that is capable of significantly enhancing our understanding of the human world must represent the world as we do and be capable of generating new knowledge, building on what we already know. In other words, it must be properly grounded and explicitly designed for rational, empirical inquiry, modeled after the scientific method. A primary implication of this design principle is that agents must be capable of engaging autonomously in causal physics discovery. We discuss the pragmatic implications of this approach, and in particular, the use cases in realistic 3D world modeling and multimodal, multidimensional time series analysis. 

**Abstract (ZH)**: 这份白皮书阐述了诺蒙那尔实验室（Noumenal Labs）在设计人工或机器智能时遵循的一些设计理念。这些原则既来源于自然界，也来源于我们理解和表达它的方法。这个领域研究与开发的最终目标应该是设计能够增强我们对世界的理解并提高我们在其中行动能力的机器智能，而不应取代我们。在前两部分中，我们将探讨我们方法的核心动机：解决基础问题。我们主张基础问题的解决方案在于设计与我们所居住的世界紧密相连的模型，而不仅仅是单词模型。能够显著提升我们对人类世界的理解的机器超级智能必须以我们的方式来表达这个世界，并能够生成新的知识，以此为基础进行推断。换句话说，它必须是正确地基础化，且明确设计用于理性、实证的探究，并遵循科学方法。这一设计原则的主要含义是，代理必须能够自主进行因果物理发现。我们将讨论这一方法的实用意义，特别是在实际3D世界建模和多模态、多维度时间序列分析方面的应用场景。 

---
# Understanding Dynamic Diffusion Process of LLM-based Agents under Information Asymmetry 

**Title (ZH)**: 理解在信息不对称情况下基于LLM的智能体动态扩散过程 

**Authors**: Yiwen Zhang, Yifu Wu, Wenyue Hua, Xuming Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13160)  

**Abstract**: Large language models have been used to simulate human society using multi-agent systems. Most current social simulation research emphasizes interactive behaviors in fixed environments, ignoring information opacity, relationship variability and diffusion diversity. In this paper, we study the dynamics of information diffusion in 12 asymmetric open environments defined by information content and distribution mechanisms. We first present a general framework to capture the features of information diffusion. Then, we designed a dynamic attention mechanism to help agents allocate attention to different information, addressing the limitations of LLM-based attention. Agents start by responding to external information stimuli within a five-agent group, increasing group size and forming information circles while developing relationships and sharing information. Additionally, we observe the emergence of information cocoons, the evolution of information gaps, and the accumulation of social capital, which are closely linked to psychological, sociological, and communication theories. 

**Abstract (ZH)**: 大规模语言模型已被用于通过多智能体系统模拟人类社会。目前大多数社会仿真研究侧重于固定环境中的交互行为，忽略了信息不透明性、关系的变异性以及信息扩散的多样性。本文研究了在由信息内容和分布机制定义的12种不对称开放环境中信息扩散的动力学。我们首先提出了一种通用框架来捕捉信息扩散的特征。然后，我们设计了一种动态注意力机制，以帮助智能体根据不同信息分配注意力，弥补了基于语言模型（LLM）注意力机制的局限性。智能体最初在五智能体小组中回应外部信息刺激，随着小组规模的扩大和信息循环的形成，智能体建立关系并共享信息。此外，我们观察到了信息茧房的出现、信息缺口的演变以及社会资本的积累，这些现象与心理学、社会学和传播理论密切相关。 

---
