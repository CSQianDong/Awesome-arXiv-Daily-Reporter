{'arxiv_id': 'arXiv:2502.13912', 'title': 'Optimizing Research Portfolio For Semantic Impact', 'authors': 'Alexander V. Belikov', 'link': 'https://arxiv.org/abs/2502.13912', 'abstract': "Citation metrics are widely used to assess academic impact but suffer from social biases, including institutional prestige and journal visibility. Here we introduce rXiv Semantic Impact (XSI), a novel framework that predicts research impact by analyzing how scientific semantic graphs evolve in underlying fabric of science. Rather than counting citations, XSI tracks the evolution of research concepts in the academic knowledge graph (KG). Starting with a construction of a comprehensive KG from 324K biomedical publications (2003-2025), we demonstrate that XSI can predict a paper's future semantic impact (SI) with remarkable accuracy ($R^2$ = 0.69) three years in advance. We leverage these predictions to develop an optimization framework for research portfolio selection that systematically outperforms random allocation. We propose SI as a complementary metric to citations and present XSI as a tool to guide funding and publishing decisions, enhancing research impact while mitigating risk.", 'abstract_zh': '引文指标广泛用于评估学术影响力，但存在社交偏见，包括机构声望和期刊可见度。在此，我们介绍了一种新颖的方法——rXiv语义影响（XSI），该方法通过分析科学语义图如何在科学基础结构中演化来预测研究影响力，而不是仅仅依靠引用计数。XSI 跟踪学术知识图谱（KG）中研究概念的演变。通过从 2003 年到 2025 年的 324,000 篇生物医学出版物构建一个全面的知识图谱，我们展示了 XSI 在三年前就能以相当高的准确度（$R^2$ = 0.69）预测一篇文章的未来语义影响力 (SI)。我们利用这些预测开发了一个科研组合优化框架，该框架系统地优于随机分配。我们提议将 SI 作为引文指标的补充指标，并将 XSI 作为引导资金分配和出版决策的工具，旨在提升研究影响力并减轻风险。', 'title_zh': '优化研究组合以提升语义影响力'}
{'arxiv_id': 'arXiv:2502.13909', 'title': 'Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?', 'authors': 'Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park', 'link': 'https://arxiv.org/abs/2502.13909', 'abstract': "Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users' item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMs' ability to understand users' item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）最近因其先进的文本理解能力和情境敏感性而成为推荐系统中具有前景的工具。尽管目前实践大多数LLM4Rec模型在序列推荐场景下进行训练和评估，我们发现现有模型是否能够充分理解用户项目交互序列中存在的序列信息这一方面已经被很大程度上忽视了。本文首先通过一系列实验展示了现有LLM4Rec模型在训练和推理过程中未能充分捕捉序列信息。接着，我们提出了一种简单而有效的基于序列的LLM推荐方法，称为LLM-SRec，该方法通过从预训练的CF-SRec模型中提取用户表示并将其融入LLMs中，增强序列信息的整合。我们的广泛实验表明，LLM-SRec能够提高LLMs对用户项目交互序列的理解能力，从而提高推荐性能。此外，与现有需要对LLMs进行微调的LLM4Rec模型不同，LLM-SRec仅通过训练少量轻量级的MLP就能达到最先进的性能，突显了其在实际应用中的实用性。我们的代码可在以下链接访问：[请在此处添加链接]。', 'title_zh': '迷失在序列中：大型语言模型理解序列推荐吗？'}
{'arxiv_id': 'arXiv:2502.13908', 'title': 'Judging the Judges: A Collection of LLM-Generated Relevance Judgements', 'authors': 'Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz', 'link': 'https://arxiv.org/abs/2502.13908', 'abstract': 'Using Large Language Models (LLMs) for relevance assessments offers promising opportunities to improve Information Retrieval (IR), Natural Language Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing IR experimenters to build evaluation collections with a fraction of the manual human labor currently required. This could help with fresh topics on which there is still limited knowledge and could mitigate the challenges of evaluating ranking systems in low-resource scenarios, where it is challenging to find human annotators. Given the fast-paced recent developments in the domain, many questions concerning LLMs as assessors are yet to be answered. Among the aspects that require further investigation, we can list the impact of various components in a relevance judgment generation pipeline, such as the prompt used or the LLM chosen.\nThis paper benchmarks and reports on the results of a large-scale automatic relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where different relevance assessment approaches were proposed. In detail, we release and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track relevance judgments produced by eight international teams who participated in the challenge. Given their diverse nature, these automatically generated relevance judgments can help the community not only investigate systematic biases caused by LLMs but also explore the effectiveness of ensemble models, analyze the trade-offs between different models and human assessors, and advance methodologies for improving automated evaluation techniques. The released resource is available at the following link: this https URL', 'abstract_zh': '使用大规模语言模型（LLMs）进行相关性评估为改进信息检索（IR）、自然语言处理（NLP）及相关领域提供了令人 promise 的机会。确实，LLMs 有望使信息检索实验者能够使用较少的人工手动劳动构建评价集合。这有助于处理知识有限的新颖主题，并减轻在资源有限的场景中评估排名系统时寻找人工注释员的挑战。鉴于最近在该领域的快速发展，关于 LLMs 作为评估者的许多问题尚未得到解答。在需要进一步研究的方面中，可以列出影响相关性判断生成管道各种组件的影响，比如使用的提示或选择的LLM。\n\n本文基于SIGIR 2024的LLMJudge挑战，对大规模自动相关性评估进行了基准测试和报告。具体而言，我们发布了并基准测试了来自八个国际团队的42个LLM生成的标签，这些团队参加了该挑战并针对TREC 2023深度学习赛道的相关性判断进行了评估。由于这些自动生成的相关性判断具有多样性，它们不仅有助于社区研究由LLMs引起的系统性偏见，还可以探索集成模型的有效性，分析不同模型和人工评估者之间的权衡，并推动改进自动化评估技术的方法论。发布的资源可以通过以下链接访问：[这个链接](this https URL)', 'title_zh': '评判法官：一组由LLM生成的相关性判断'}
{'arxiv_id': 'arXiv:2502.13845', 'title': 'Enhancing LLM-Based Recommendations Through Personalized Reasoning', 'authors': 'Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13845', 'abstract': "Current recommendation systems powered by large language models (LLMs) often underutilize their reasoning capabilities due to a lack of explicit logical structuring. To address this limitation, we introduce CoT-Rec, a framework that integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by incorporating two crucial processes: user preference analysis and item perception evaluation. CoT-Rec operates in two key phases: (1) personalized data extraction, where user preferences and item perceptions are identified, and (2) personalized data application, where this information is leveraged to refine recommendations. Our experimental analysis demonstrates that CoT-Rec improves recommendation accuracy by making better use of LLMs' reasoning potential. The implementation is publicly available at this https URL.", 'abstract_zh': '当前由大规模语言模型（LLMs）驱动的推荐系统往往未能充分发挥其推理能力，原因在于缺乏明确的逻辑结构。为解决这一局限，我们提出了一种名为CoT-Rec的框架，该框架通过整合两种关键过程——用户偏好分析和物品感知评估——将链式推理（CoT）引入LLM驱动的推荐中。CoT-Rec在两个关键阶段运行：（1）个性化数据提取阶段，在此阶段识别用户偏好和物品感知；（2）个性化数据应用阶段，在此阶段利用这些信息改进推荐。我们的实验分析表明，CoT-Rec通过更有效地利用LLMs的推理潜力提高了推荐的准确性。该实现可以在以下地址访问：[此链接]。', 'title_zh': '通过个性化推理增强基于LLM的推荐系统'}
{'arxiv_id': 'arXiv:2502.13843', 'title': 'Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based User Agents', 'authors': 'Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13843', 'abstract': 'Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at this https URL.', 'abstract_zh': '基于大规模语言模型（LLM）的用户代理已作为一种增强推荐系统的方法而崭露头角，通过模拟用户交互来提升推荐效果。然而，现有方法在跨域场景中表现不佳，主要是由于不高效的内存结构导致无关信息留存，并无法考虑到如流行度等社会影响因素。为解决这些限制，我们提出了一种名为AgentCF++的新颖框架，该框架采用了双层记忆架构和两步融合机制，能够有效过滤领域特定的偏好。此外，我们还提出了一种共享记忆的兴趣群体模型，使得模型能够捕捉兴趣相似用户的流行趋势影响。在多个跨域数据集上的 extensive 实验中，AgentCF++ 显示出了比基线模型更优异的性能，突显了其在增强推荐系统中用户行为模拟精炼方面的有效性。我们的代码可在以下链接获取：this https URL。', 'title_zh': '使用内存优化的大语言模型基用户代理增强跨域推荐'}
{'arxiv_id': 'arXiv:2502.13840', 'title': 'Mitigating Popularity Bias in Collaborative Filtering through Fair Sampling', 'authors': 'Jiahao Liu, Dongsheng Li, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu', 'link': 'https://arxiv.org/abs/2502.13840', 'abstract': 'Recommender systems often suffer from popularity bias, where frequently interacted items are overrepresented in recommendations. This bias stems from propensity factors influencing training data, leading to imbalanced exposure. In this paper, we introduce a Fair Sampling (FS) approach to address this issue by ensuring that both users and items are selected with equal probability as positive and negative instances. Unlike traditional inverse propensity score (IPS) methods, FS does not require propensity estimation, eliminating errors associated with inaccurate calculations. Our theoretical analysis demonstrates that FS effectively neutralizes the influence of propensity factors, achieving unbiased learning. Experimental results validate that FS outperforms state-of-the-art methods in both point-wise and pair-wise recommendation tasks, enhancing recommendation fairness without sacrificing accuracy. The implementation is available at this https URL.', 'abstract_zh': '推荐系统通常会遭受流行性偏差的问题，即频繁互动的项目在推荐中被过度代表。这种偏差源于影响训练数据的倾向性因素，导致曝光不均衡。在本文中，我们提出了一种公平采样（Fair Sampling，FS）方法，通过确保用户和项目以相等的概率被选为正实例和负实例来解决这一问题。与传统的逆倾向性评分（Inverse Propensity Score, IPS）方法不同，FS 不需要估算倾向性，从而避免了因不准确计算而产生的误差。我们的理论分析表明，FS 有效地抵消了倾向性因素的影响，实现了无偏学习。实验结果验证了在点预测和对预测任务中，FS 都优于当前最先进的方法，同时提高了推荐的公平性而不牺牲准确性。代码实现可在以下链接获得：this https URL。', 'title_zh': '通过公平采样缓解协作过滤中的流行性偏见'}
{'arxiv_id': 'arXiv:2502.13826', 'title': 'In-Place Updates of a Graph Index for Streaming Approximate Nearest Neighbor Search', 'authors': 'Haike Xu, Magdalen Dobson Manohar, Philip A. Bernstein, Badrish Chandramouli, Richard Wen, Harsha Vardhan Simhadri', 'link': 'https://arxiv.org/abs/2502.13826', 'abstract': 'Indices for approximate nearest neighbor search (ANNS) are a basic component for information retrieval and widely used in database, search, recommendation and RAG systems. In these scenarios, documents or other objects are inserted into and deleted from the working set at a high rate, requiring a stream of updates to the vector index. Algorithms based on proximity graph indices are the most efficient indices for ANNS, winning many benchmark competitions. However, it is challenging to update such graph index at a high rate, while supporting stable recall after many updates. Since the graph is singly-linked, deletions are hard because there is no fast way to find in-neighbors of a deleted vertex. Therefore, to update the graph, state-of-the-art algorithms such as FreshDiskANN accumulate deletions in a batch and periodically consolidate, removing edges to deleted vertices and modifying the graph to ensure recall stability. In this paper, we present IP-DiskANN (InPlaceUpdate-DiskANN), the first algorithm to avoid batch consolidation by efficiently processing each insertion and deletion in-place. Our experiments using standard benchmarks show that IP-DiskANN has stable recall over various lengthy update patterns in both high-recall and low-recall regimes. Further, its query throughput and update speed are better than using the batch consolidation algorithm and HNSW.', 'abstract_zh': '近似最近邻搜索（ANNS）索引是信息检索的基本组成部分，在数据库、搜索、推荐和检索增强生成（RAG）系统中被广泛使用。在这些场景中，文档或其他对象以高速率插入和删除到工作集中，因此需要对向量索引进行相应的更新。基于临近图索引的算法是最高效的ANNS索引，赢得了多次基准竞赛。然而，以高速率更新这样的图索引并保持多次更新后的稳定召回率是一项挑战。由于图是单向链接的，删除操作困难，因为没有快速查找被删除顶点的入邻居的方法。因此，为了更新图，最先进的算法如FreshDiskANN通过对删除操作进行批量累积并在固定周期内进行合并来避免，删除到被删除顶点的边，并修改图以确保召回稳定性。在本文中，我们提出了InPlaceUpdate-DiskANN（IP-DiskANN），这是第一个通过高效处理每个插入和删除操作来避免批量合并的算法。我们的实验使用标准基准测试表明，在高召回率和低召回率的多种更新模式下，IP-DiskANN具有稳定的召回率。此外，其查询吞吐量和更新速度优于使用批量合并算法和HNSW。', 'title_zh': '流式近似最近邻搜索的图索引就地更新方法'}
{'arxiv_id': 'arXiv:2502.13783', 'title': 'Generative Large Recommendation Models: Emerging Trends in LLMs for Recommendation', 'authors': 'Hao Wang, Wei Guo, Luankang Zhang, Jin Yao Chin, Yufei Ye, Huifeng Guo, Yong Liu, Defu Lian, Ruiming Tang, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.13783', 'abstract': 'In the era of information overload, recommendation systems play a pivotal role in filtering data and delivering personalized content. Recent advancements in feature interaction and user behavior modeling have significantly enhanced the recall and ranking processes of these systems. With the rise of large language models (LLMs), new opportunities have emerged to further improve recommendation systems. This tutorial explores two primary approaches for integrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning capabilities of general LLMs, and generative large recommendation models, which focus on scaling and sophistication. While the former has been extensively covered in existing literature, the latter remains underexplored. This tutorial aims to fill this gap by providing a comprehensive overview of generative large recommendation models, including their recent advancements, challenges, and potential research directions. Key topics include data quality, scaling laws, user behavior mining, and efficiency in training and inference. By engaging with this tutorial, participants will gain insights into the latest developments and future opportunities in the field, aiding both academic research and practical applications. The timely nature of this exploration supports the rapid evolution of recommendation systems, offering valuable guidance for researchers and practitioners alike.', 'abstract_zh': '在信息过载的时代，推荐系统在过滤数据和提供个性化内容方面发挥着关键作用。最近在特征交互和用户行为建模方面的进展显著提升了这些系统的召回率和排名过程。随着大量语言模型（LLMs）的兴起，新的机会已出现，可进一步改进推荐系统。本教程探讨了两种主要的LLM集成方法：LLM增强推荐，利用通用LLM的推理能力，以及生成型大型推荐模型，后者侧重于扩展性和复杂性。尽管前者在现有文献中已被广泛讨论，后者却仍处于探索阶段。本教程旨在填补这一空白，通过全面总结生成型大型推荐模型的最新进展、面临的挑战和潜在的研究方向，为参与者提供宝贵的见解。关键议题包括数据质量、扩展法则、用户行为挖掘以及训练和推理的效率。通过参与本次教程，参与者将深入了解推荐系统领域的最新发展和未来机遇，从而有助于学术研究和实际应用。本探索的及时性支持了推荐系统领域的快速演变，为研究人员和从业者提供了宝贵的指导。', 'title_zh': '生成式大型推荐模型：推荐领域新兴的LLM趋势'}
{'arxiv_id': 'arXiv:2502.13763', 'title': 'Unsupervised Graph Embeddings for Session-based Recommendation with Item Features', 'authors': 'Andreas Peintner, Marta Moscati, Emilia Parada-Cabaleiro, Markus Schedl, Eva Zangerle', 'link': 'https://arxiv.org/abs/2502.13763', 'abstract': "In session-based recommender systems, predictions are based on the user's preceding behavior in the session. State-of-the-art sequential recommendation algorithms either use graph neural networks to model sessions in a graph or leverage the similarity of sessions by exploiting item features. In this paper, we combine these two approaches and propose a novel method, Graph Convolutional Network Extension (GCNext), which incorporates item features directly into the graph representation via graph convolutional networks. GCNext creates a feature-rich item co-occurrence graph and learns the corresponding item embeddings in an unsupervised manner. We show on three datasets that integrating GCNext into sequential recommendation algorithms significantly boosts the performance of nearest-neighbor methods as well as neural network models. Our flexible extension is easy to incorporate in state-of-the-art methods and increases the MRR@20 by up to 12.79%.", 'abstract_zh': '在基于会话的推荐系统中，预测基于用户在会话中的preceding行为。目前最先进的序列推荐算法要么使用图神经网络在图中建模会话，要么通过利用物品特征来利用会话之间的相似性。在本文中，我们将这两种方法结合起来，提出了一种新型方法——图卷积网络扩展（GCNext），该方法通过图卷积网络直接将物品特征纳入图表示中。GCNext构建了一个丰富的物品共现图，并以无监督的方式学习相应的物品嵌入。我们通过三个数据集的实验表明，将GCNext集成到序列推荐算法中可以显著提升最近邻方法和神经网络模型的性能。我们的灵活扩展易于集成到最先进的方法中，并将MRR@20的性能提升最大12.79%。', 'title_zh': '基于项目特征的会话推荐的无监督图嵌入方法'}
{'arxiv_id': 'arXiv:2502.13719', 'title': 'TrustRAG: An Information Assistant with Retrieval Augmented Generation', 'authors': 'Yixing Fan, Qiang Yan, Wenshan Wang, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.13719', 'abstract': '\\Ac{RAG} has emerged as a crucial technique for enhancing large models with real-time and domain-specific knowledge. While numerous improvements and open-source tools have been proposed to refine the \\ac{RAG} framework for accuracy, relatively little attention has been given to improving the trustworthiness of generated results. To address this gap, we introduce TrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives: indexing, retrieval, and generation. Specifically, in the indexing stage, we propose a semantic-enhanced chunking strategy that incorporates hierarchical indexing to supplement each chunk with contextual information, ensuring semantic completeness. In the retrieval stage, we introduce a utility-based filtering mechanism to identify high-quality information, supporting answer generation while reducing input length. In the generation stage, we propose fine-grained citation enhancement, which detects opinion-bearing sentences in responses and infers citation relationships at the sentence-level, thereby improving citation accuracy. We open-source the TrustRAG framework and provide a demonstration studio designed for excerpt-based question answering tasks \\footnote{this https URL}. Based on these, we aim to help researchers: 1) systematically enhancing the trustworthiness of \\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more reliable outputs.', 'abstract_zh': '以下内容或标题已经翻译成中文，并符合学术规范格式：\n\n\\Ac{RAG} 技术已经发展成为增强大型模型的实时和领域特定知识的关键手段。虽然已经提出了许多改进和开源工具来改进 \\ac{RAG} 框架的准确性，但相对较少关注生成结果的可信度问题。为了解决这一差距，我们提出了一种名为 TrustRAG 的新型框架，从三个角度增强 \\ac{RAG}：索引、检索和生成。具体而言，在索引阶段，我们提出了一种结合层次索引的语义增强分块策略，为每个分块补充上下文信息，确保语义完备性。在检索阶段，我们引入了一种基于效用的过滤机制，用于识别高质量的信息，支持答案生成同时减少输入长度。在生成阶段，我们提出了细粒度的引文增强策略，该策略在响应中检测观点表达的句子，并在句子级别推断引文关系，从而提高引文准确性。我们开源了 TrustRAG 框架，并提供了一个演示 studio，适用于引文基问题回答任务 \\footnote{参见此处: \\url{this https URL}}。基于以上内容，我们的目标是帮助研究人员：1) 系统地提高 \\ac{RAG} 系统的可信度；2) 开发具有更可靠输出的 \\ac{RAG} 系统。', 'title_zh': 'TrustRAG：一种基于检索增强生成的信息助手'}
{'arxiv_id': 'arXiv:2502.13713', 'title': 'TALKPLAY: Multimodal Music Recommendation with Large Language Models', 'authors': 'Seungheon Doh, Keunwoo Choi, Juhan Nam', 'link': 'https://arxiv.org/abs/2502.13713', 'abstract': "We present TalkPlay, a multimodal music recommendation system that reformulates the recommendation task as large language model token generation. TalkPlay represents music through an expanded token vocabulary that encodes multiple modalities - audio, lyrics, metadata, semantic tags, and playlist co-occurrence. Using these rich representations, the model learns to generate recommendations through next-token prediction on music recommendation conversations, that requires learning the associations natural language query and response, as well as music items. In other words, the formulation transforms music recommendation into a natural language understanding task, where the model's ability to predict conversation tokens directly optimizes query-item relevance. Our approach eliminates traditional recommendation-dialogue pipeline complexity, enabling end-to-end learning of query-aware music recommendations. In the experiment, TalkPlay is successfully trained and outperforms baseline methods in various aspects, demonstrating strong context understanding as a conversational music recommender.", 'abstract_zh': '我们提出了一种名为TalkPlay的多模态音乐推荐系统，将推荐任务重新定义为大型语言模型标记生成。TalkPlay 通过扩展的标记词汇表来表示音乐，该词汇表编码了多种模态的信息——包括音频、歌词、元数据、语义标签以及播放列表共现情况。利用这些丰富的表示形式，模型通过音乐推荐对话中下一个标记的预测来学习生成推荐，从而需要学习自然语言查询和响应之间的关联，以及音乐项目。换句话说，这种表述将音乐推荐转化为一个自然语言理解任务，模型预测对话标记的能力直接优化了查询与项目的相关性。我们的方法消除了传统的推荐对话流水线复杂性，从而实现基于查询的音乐推荐的端到端学习。在实验中，TalkPlay 成功地进行了训练，并在多个方面超越了基准方法，展示了其作为对话式音乐推荐系统强大的上下文理解能力。', 'title_zh': 'TALKPLAY：基于大型语言模型的多模态音乐推荐'}
{'arxiv_id': 'arXiv:2502.13581', 'title': 'ActionPiece: Contextually Tokenizing Action Sequences for Generative Recommendation', 'authors': 'Yupeng Hou, Jianmo Ni, Zhankui He, Noveen Sachdeva, Wang-Cheng Kang, Ed H. Chi, Julian McAuley, Derek Zhiyuan Cheng', 'link': 'https://arxiv.org/abs/2502.13581', 'abstract': 'Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.', 'abstract_zh': '生成推荐（GR）是一种新兴范式，其中用户行为被标记为离散的标记模式并自回归地生成为预测。然而，现有的GR模型将每个行为单独标记，对所有序列中的相同行为分配相同的固定标记，而不考虑上下文关系。这种缺乏上下文感知可能导致性能不佳，因为同一行为在其上下文环境中可能会有不同的含义。为解决这一问题，我们提出使用ActionPiece在标记行为序列时明确地引入上下文。在ActionPiece中，每个行为由一组项目特征表示，这些特征作为初始标记使用。给定行为序列语料库，我们基于其在个体集合内部和相邻集合之间同时共现频率构建词汇表，将特征模式合并为新的标记。考虑到特征集的无序性，我们进一步引入集合置换正则化，这生成了具有相同语义的多个行为序列片段。在公开数据集上的实验表明，ActionPiece在所有现有的行为标记方法中表现最佳，NDCG@$10$值提高了$6.00\\%$至$12.82\\%$。', 'title_zh': 'ActionPiece：基于上下文的动作序列分词生成推荐'}
{'arxiv_id': 'arXiv:2502.13539', 'title': 'Bursting Filter Bubble: Enhancing Serendipity Recommendations with Aligned Large Language Models', 'authors': 'Yunjia Xi, Muyan Weng, Wen Chen, Chao Yi, Dian Chen, Gaoyang Guo, Mao Zhang, Jian Wu, Yuning Jiang, Qingwen Liu, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2502.13539', 'abstract': 'Recommender systems (RSs) often suffer from the feedback loop phenomenon, e.g., RSs are trained on data biased by their recommendations. This leads to the filter bubble effect that reinforces homogeneous content and reduces user satisfaction. To this end, serendipity recommendations, which offer unexpected yet relevant items, are proposed. Recently, large language models (LLMs) have shown potential in serendipity prediction due to their extensive world knowledge and reasoning capabilities. However, they still face challenges in aligning serendipity judgments with human assessments, handling long user behavior sequences, and meeting the latency requirements of industrial RSs. To address these issues, we propose SERAL (Serendipity Recommendations with Aligned Large Language Models), a framework comprising three stages: (1) Cognition Profile Generation to compress user behavior into multi-level profiles; (2) SerenGPT Alignment to align serendipity judgments with human preferences using enriched training data; and (3) Nearline Adaptation to integrate SerenGPT into industrial RSs pipelines efficiently. Online experiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and transactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user experience without much impact on overall revenue. Now, it has been fully deployed in the "Guess What You Like" of the Taobao App homepage.', 'abstract_zh': '推荐系统（RSs）通常会遭受反馈循环现象的影响，例如，它们在受到自身推荐数据偏差训练后，会导致过滤泡泡效应，进一步强化同质化内容，降低用户体验。为了解决这一问题，提出了意外推荐（Serendipity Recommendations），这种推荐能够提供意外且相关的内容。最近，大型语言模型（LLMs）在意外推荐预测方面显示出潜力，因为它们具有广泛的世界知识和推理能力。然而，它们仍然面临着将意外推荐判断与人类评估对齐、处理长用户行为序列以及满足工业推荐系统延迟要求的挑战。为了解决这些问题，我们提出了一种名为SERAL（Serendipity Recommendations with Aligned Large Language Models）的框架，包括三个阶段：（1）认知概况生成（Cognition Profile Generation），将用户行为压缩成多级概况；（2）SerenGPT对齐（SerenGPT Alignment），使用丰富化的训练数据将意外推荐判断与人类偏好对齐；以及（3）离线适应（Nearline Adaptation），高效地将SerenGPT集成到工业推荐系统管道中。在线实验表明，SERAL可以将意外推荐项目的曝光率（PVR）、点击率和交易量分别提高5.7%、29.56%和27.6%，从而提升用户体验，同时对总收入的影响较小。目前，该框架已经全面部署在淘宝App主页的“猜你喜欢”功能中。', 'title_zh': '破除信息茧房：通过对齐的大语言模型增强偶然性推荐'}
{'arxiv_id': 'arXiv:2502.13530', 'title': 'Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential Recommendation', 'authors': 'Wuhan Chen, Zongwei Wang, Min Gao, Xin Xia, Feng Jiang, Junhao Wen', 'link': 'https://arxiv.org/abs/2502.13530', 'abstract': "Traditional sequential recommendation (SR) methods heavily rely on explicit item IDs to capture user preferences over time. This reliance introduces critical limitations in cold-start scenarios and domain transfer tasks, where unseen items and new contexts often lack established ID mappings. To overcome these limitations, recent studies have shifted towards leveraging text-only information for recommendation, thereby improving model generalization and adaptability across domains. Although promising, text-based SR faces unique difficulties: items' text descriptions often share semantic similarities that lead to clustered item representations, compromising their uniformity, a property essential for promoting diversity and enhancing generalization in recommendation systems. In this paper, we explore a novel framework to improve the uniformity of item representations in text-based SR. Our analysis reveals that items within a sequence exhibit marked semantic similarity, meaning they are closer in representation than items overall, and that this effect is more pronounced for less popular items, which form tighter clusters compared to their more popular counterparts. Based on these findings, we propose UniT, a framework that employs three pairwise item sampling strategies: Unified General Sampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven Sampling Strategy. Each strategy applies varying degrees of repulsion to selectively adjust the distances between item pairs, thereby refining representation uniformity while considering both sequence context and item popularity. Extensive experiments on multiple real-world datasets demonstrate that our proposed approach outperforms state-of-the-art models, validating the effectiveness of UniT in enhancing both representation uniformity and recommendation this http URL source code is available at this https URL.", 'abstract_zh': '以下是经过学术规范翻译后的中文内容：\n\n传统的序列推荐（SR）方法高度依赖于明确定义的项目ID，以捕捉用户随时间变化的偏好。这种依赖在冷启动场景和领域迁移任务中引入了关键限制，因为未见过的项目和新的上下文往往缺乏已建立的ID映射。为了克服这些限制，最近的研究转向利用仅文本信息来进行推荐，从而提高模型的泛化能力和跨领域适应性。尽管前景广阔，但基于文本的序列推荐面临独特的挑战：项目文本描述往往表现出语义相似性，导致项目表示聚集在一起，这破坏了必需的多样性，降低了推荐系统的泛化能力。\n\n本文探讨了一种新的框架，旨在提高基于文本的序列推荐中项目表示的一致性。我们的分析表明，在序列中展现项目时，项目之间的语义相似度明显，它们在表示空间中的距离比总体项目更近，而且这种效应对于更不受欢迎的项目更为显著，这些项目形成了更紧密的群集，相比之下，更受欢迎的项目则不然。基于这些发现，我们提出了一种名为UniT的新框架，它采用了三种成对项目采样策略：统一通用采样策略、序列驱动采样策略和受欢迎程度驱动采样策略。每种策略以不同程度的应用排斥力来选择性地调整项目对之间的距离，从而在考虑序列上下文和项目受欢迎程度的同时，逐步优化表示一致性。在多个真实世界数据集上的广泛实验表明，我们的方法优于最先进的模型，验证了UniT在增强表示一致性以及推荐性能方面的有效性。有关本文的源代码可以在以下链接获得：[提供链接]。', 'title_zh': '打破簇群：基于文本的序列推荐中的均匀性优化'}
{'arxiv_id': 'arXiv:2502.13506', 'title': 'Reproducing NevIR: Negation in Neural Information Retrieval', 'authors': 'Coen van Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Alliannejadi', 'link': 'https://arxiv.org/abs/2502.13506', 'abstract': "Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category - listwise Large Language Model (LLM) rerankers - outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalizability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM rerankers achieve reasonable performance across both negation tasks.", 'abstract_zh': '否定是人类交流的一个基本方面，但在信息检索（IR）中，语言模型（LMs）在处理否定时仍面临挑战。尽管现代神经IR系统高度依赖LMs，但很少有人关注它们对否定的处理方式。在此项研究中，我们重现并扩展了NevIR的研究成果，NevIR是一个基准研究，揭示了大多数IR模型在处理否定时的表现与随机排名相当。我们复制了NevIR的原始实验并评估了最新开发的一流IR模型。我们的研究结果表明，最近出现的一种类别——列表级大型语言模型（LLM）重排序器——在性能上优于其他模型，但仍低于人类表现。此外，我们利用ExcluIR数据集来评估否定理解的泛化能力。ExcluIR数据集专门针对包含广泛否定的排除查询设计。我们的研究结果表明，仅通过在一个数据集上进行微调并不能可靠地改进其他数据集上的表现，这表明它们的数据分布存在显著差异。此外，我们观察到只有交叉编码器和列表级大型语言模型重排序器能够在两种否定任务上实现合理的表现。', 'title_zh': '复现 NevIR：神经信息检索中的否定语处理'}
{'arxiv_id': 'arXiv:2502.13481', 'title': 'LLM4Tag: Automatic Tagging System for Information Retrieval via Large Language Models', 'authors': 'Ruiming Tang, Chenxu Zhu, Bo Chen, Weipeng Zhang, Menghui Zhu, Xinyi Dai, Huifeng Guo', 'link': 'https://arxiv.org/abs/2502.13481', 'abstract': 'Tagging systems play an essential role in various information retrieval applications such as search engines and recommender systems. Recently, Large Language Models (LLMs) have been applied in tagging systems due to their extensive world knowledge, semantic understanding, and reasoning capabilities. Despite achieving remarkable performance, existing methods still have limitations, including difficulties in retrieving relevant candidate tags comprehensively, challenges in adapting to emerging domain-specific knowledge, and the lack of reliable tag confidence quantification. To address these three limitations above, we propose an automatic tagging system LLM4Tag. First, a graph-based tag recall module is designed to effectively and comprehensively construct a small-scale highly relevant candidate tag set. Subsequently, a knowledge-enhanced tag generation module is employed to generate accurate tags with long-term and short-term knowledge injection. Finally, a tag confidence calibration module is introduced to generate reliable tag confidence scores. Extensive experiments over three large-scale industrial datasets show that LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag has been deployed online for content tagging to serve hundreds of millions of users.', 'abstract_zh': '标签系统在各种信息检索应用中（如搜索引擎和推荐系统）起着至关重要的作用。近年来，由于大型语言模型（LLMs）具有广泛的世界知识、语义理解和推理能力，它们被应用于标签系统中。尽管现有方法取得了显著的性能，但仍存在一些局限性，包括难以全面检索相关候选标签、难以适应新兴的域特定知识以及缺乏可靠标签置信度量化的问题。为了解决上述三个局限性，我们提出了一种自动标签系统——LLM4Tag。首先，设计了一种基于图的标签召回模块，以有效地并全面地构建一个小规模的相关候选标签集。随后，采用了知识增强的标签生成模块，通过长短期知识注入生成准确的标签。最后，引入了标签置信度校准模块，生成可靠的标签置信度分数。在三个大规模工业数据集上进行的广泛实验表明，LLM4Tag 显著优于现有最先进的基线方法，并且LLM4Tag 已在线部署用于内容标记，为数亿用户提供服务。', 'title_zh': 'LLM4Tag：通过大规模语言模型的自动标签系统用于信息检索'}
{'arxiv_id': 'arXiv:2502.13465', 'title': 'HawkBench: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks', 'authors': 'Hongjin Qian, Zheng Liu, Chao Gao, Yankai Wang, Defu Lian, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2502.13465', 'abstract': 'In real-world information-seeking scenarios, users have dynamic and diverse needs, requiring RAG systems to demonstrate adaptable resilience. To comprehensively evaluate the resilience of current RAG methods, we introduce HawkBench, a human-labeled, multi-domain benchmark designed to rigorously assess RAG performance across categorized task types. By stratifying tasks based on information-seeking behaviors, HawkBench provides a systematic evaluation of how well RAG systems adapt to diverse user needs.\nUnlike existing benchmarks, which focus primarily on specific task types (mostly factoid queries) and rely on varying knowledge bases, HawkBench offers: (1) systematic task stratification to cover a broad range of query types, including both factoid and rationale queries, (2) integration of multi-domain corpora across all task types to mitigate corpus bias, and (3) rigorous annotation for high-quality evaluation.\nHawkBench includes 1,600 high-quality test samples, evenly distributed across domains and task types. Using this benchmark, we evaluate representative RAG methods, analyzing their performance in terms of answer quality and response latency. Our findings highlight the need for dynamic task strategies that integrate decision-making, query interpretation, and global knowledge understanding to improve RAG generalizability. We believe HawkBench serves as a pivotal benchmark for advancing the resilience of RAG methods and their ability to achieve general-purpose information seeking.', 'abstract_zh': '在现实世界的信息查询场景中，用户的需求是动态多样的，这要求RAG系统能够展现出灵活的适应性和韧性。为了全面评估当前RAG方法的韧性，我们提出了HawkBench——一个由人工标注、跨领域设计的基准测试系统，旨在严格评估RAG在不同任务类型中的表现。通过根据信息查询行为对任务进行分层，HawkBench提供了一种系统性的评估方式，用以考察RAG系统如何适应多样化的用户需求。\n\n与现有的基准测试不同，后者主要聚焦于特定的任务类型（主要是事实查询），并且依赖于不同的知识库，HawkBench 提供了以下优势：（1）系统性的任务分层，以覆盖广泛的查询类型，包括事实查询和推理查询；（2）跨所有任务类型的多领域语料库集成，以减轻语料库偏见；（3）严格的标注，以确保高质量的评估。\n\nHawkBench 包含了1600个高质量的测试样本，这些样本在各个领域和任务类型中均匀分布。使用这个基准测试，我们评估了代表性的RAG方法，并从答案质量和响应延迟两个方面分析它们的性能。我们的研究结果强调了需要采用动态任务策略的必要性，这种策略应结合决策制定、查询解释和全局知识理解，以提高RAG的一般化能力。我们相信HawkBench 为推动RAG方法的韧性和其在实现广泛信息检索方面的潜力提供了关键的基准测试。', 'title_zh': 'HawkBench: 探究RAG方法在分层信息检索任务中的鲁棒性'}
{'arxiv_id': 'arXiv:2502.13245', 'title': 'Range Retrieval with Graph-Based Indices', 'authors': 'Magdalen Dobson Manohar, Taekseung Kim, Guy E. Belloch', 'link': 'https://arxiv.org/abs/2502.13245', 'abstract': 'Retrieving points based on proximity in a high-dimensional vector space is a crucial step in information retrieval applications. The approximate nearest neighbor search (ANNS) problem, which identifies the $k$ nearest neighbors for a query (approximately, since exactly is hard), has been extensively studied in recent years. However, comparatively little attention has been paid to the related problem of finding all points within a given distance of a query, the range retrieval problem, despite its applications in areas such as duplicate detection, plagiarism checking, and facial recognition. In this paper, we present a set of algorithms for range retrieval on graph-based vector indices, which are known to achieve excellent performance on ANNS queries. Since a range query may have anywhere from no matching results to thousands of matching results in the database, we introduce a set of range retrieval algorithms based on modifications of the standard graph search that adapt to terminate quickly on queries in the former group, and to put more resources into finding results for the latter group. Due to the lack of existing benchmarks for range retrieval, we also undertake a comprehensive study of range characteristics of existing embedding datasets, and select a suitable range retrieval radius for eight existing datasets with up to 100 million points in addition to the one existing benchmark. We test our algorithms on these datasets, and find up to 100x improvement in query throughput over a naive baseline approach, with 5-10x improvement on average, and strong performance up to 100 million data points.', 'abstract_zh': '在高维向量空间中基于邻近性检索点是信息检索应用中的关键步骤。近年来，近似最近邻搜索（ANNS）问题（即确定查询的近似最邻近点，由于精确查找困难）已被广泛研究。然而，相比之下，查询范围内所有点的检索问题（范围检索问题）尽管在重复检测、抄袭检查和面部识别等领域有着广泛的应用，却得到了较少的关注。在本文中，我们提出了一系列基于图基向量索引的范围检索算法，这类索引在处理ANNS查询时已知表现优异。由于范围查询可能在数据库中没有任何匹配结果或多达数千个匹配结果，我们引入了一系列基于标准图搜索修改的范围检索算法，这些算法能够在前者的情况中迅速终止查询，并在后者的情况中投入更多资源来找到更多结果。由于目前缺乏范围检索的基准测试，我们还对现有嵌入数据集的范围特性进行了全面研究，并选择了适合八个已有一个基准的数据集（这些数据集最多包含1亿个点）的范围检索半径。我们使用这些数据集测试了我们的算法，并发现与原始基准方法相比，查询吞吐量可提高100倍，且平均提高5-10倍，即使在数据点达到1亿时也能表现出色。', 'title_zh': '基于图索引的范围检索'}
{'arxiv_id': 'arXiv:2502.13881', 'title': 'PSCon: Toward Conversational Product Search', 'authors': 'Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen', 'link': 'https://arxiv.org/abs/2502.13881', 'abstract': 'Conversational Product Search (CPS) is confined to simulated conversations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.', 'abstract_zh': '基于对话的产品搜索（Conversational Product Search, CPS）受到实际数据集缺乏的限制，现有的CPS数据集由于未反映出人类语言的特点，仅限于模拟对话。此外，当前的对话数据集在支持跨市场和多语言使用方面存在局限性。本文介绍了一种新的CPS数据收集协议，并提出了一种新的CPS数据集PSCon，旨在通过类人类对话来辅助产品搜索。该数据集使用受指导的人对人数据收集协议构建，并支持两种语言和两个市场。此外，该数据集可以全面探索CPS的六个子任务：用户意图检测、关键词提取、系统动作预测、问题选择、项目排名和响应生成。进一步地，我们还对该数据集进行了分析，并在此提出的CPS数据集上提出了一个基准模型。', 'title_zh': 'PSCon: 向对话式产品搜索迈进'}
{'arxiv_id': 'arXiv:2502.13668', 'title': 'PeerQA: A Scientific Question Answering Dataset from Peer Reviews', 'authors': 'Tim Baumgärtner, Ted Briscoe, Iryna Gurevych', 'link': 'https://arxiv.org/abs/2502.13668', 'abstract': 'We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens. Our code and data is available at this https URL.', 'abstract_zh': '我们介绍了PeerQA，这是一个真实的、科学性的、面向文档级别的问答（QA）数据集。PeerQA的问题来源于同行评审，这些问题是在评审人员仔细审查科学文章时提出的。答案由每篇文章的原作者进行标注。该数据集包含来自208篇学术文章的579个QA配对，其中多数来自机器学习和自然语言处理领域，还包括地质科学和公共卫生等其他科学领域的子集。PeerQA支持开发实用QA系统中的三个关键任务：证据检索、无法回答的问题分类以及答案生成。我们对收集的数据集进行了详细的分析，并进行了实验，建立了针对所有三项任务的基础系统。我们的实验和分析揭示了在文档级别检索中需要去语境化，我们发现即使是简单的去语境化方法也能在各种架构中一致性地提升检索性能。在答案生成方面，PeerQA为长上下文建模提供了一个具有挑战性的基准，因为论文的平均大小为12,000个标记。我们的代码和数据可在以下链接中获取：this https URL。', 'title_zh': 'PeerQA：来自于同行评审的科学问答数据集'}
{'arxiv_id': 'arXiv:2502.13595', 'title': 'MMTEB: Massive Multilingual Text Embedding Benchmark', 'authors': 'Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff', 'link': 'https://arxiv.org/abs/2502.13595', 'abstract': 'Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.', 'abstract_zh': '文本嵌入通常仅在一组受限的任务上进行评估，这些任务受到语言、领域和任务多样性的限制。为了克服这些局限并提供更全面的评估，我们引入了大规模多语言文本嵌入基准（MMTEB）——这是对MTEB的大规模、社区驱动的扩展，涵盖了超过500个质量控制的评估任务，涉及250多种语言。MMTEB 包括一系列具有挑战性且新颖的任务，如指令跟随、长文档检索和代码检索，这些任务代表了迄今为止嵌入模型评估任务的最大多语言集合。借助该集合，我们开发了多个高度多语言基准，用于评估具有代表性的模型集合。我们发现，尽管拥有数十亿参数的大型语言模型（LLMs）在某些语言子集和任务类别上能够达到最先进的性能，但公开可用的最佳性能模型仅有5.6亿参数的multilingual-e5-large-instruct。为提高可访问性并降低计算成本，我们引入了一种基于任务间相关性的新颖下采样方法，确保选择的多样性同时保持相对模型排名。此外，我们通过采样困难的负例优化了检索等任务，从而创建了更小但有效的拆分。这些优化使我们能够引入大幅降低计算需求的基准。例如，我们新引入的零样本英语文本基准在保持与全规模版本相似的排名顺序的同时，显著降低了计算成本。', 'title_zh': '大规模多语言文本嵌入基准：MMTEB'}
{'arxiv_id': 'arXiv:2502.13233', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'authors': 'Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu', 'link': 'https://arxiv.org/abs/2502.13233', 'abstract': "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", 'abstract_zh': '大型语言模型（LLMs）在通用领域展现了令人瞩目的能力，但在需要专门知识的任务上往往表现不佳。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些知识库可能存在过时或不完整的问题，缺乏准确回答医疗问题所必需的精细临床细节。在本研究中，我们提出了一种名为SearchRAG的新框架，该框架通过利用实时搜索引擎克服了这些限制。我们的方法通过合成查询生成将复杂的医疗问题转换为搜索引擎友好的查询，并利用不确定性驱动的知识选择来筛选并整合最相关和最有信息量的医疗知识作为LLM的输入。实验结果表明，我们的方法在医疗问题回答任务中显著提高了响应的准确性，特别是在需要详细和最新知识的复杂问题上。', 'title_zh': 'SearchRAG：搜索引擎能对基于LLM的医疗问答有所帮助吗？'}
