{'arxiv_id': 'arXiv:2502.13963', 'title': 'MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads', 'authors': 'Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang', 'link': 'https://arxiv.org/abs/2502.13963', 'abstract': 'Large Language Models (LLMs) frequently show distracted attention due to irrelevant information in the input, which severely impairs their long-context capabilities. Inspired by recent studies on the effectiveness of retrieval heads in long-context factutality, we aim at addressing this distraction issue through improving such retrieval heads directly. We propose Multi-Document Attention Focusing (MuDAF), a novel method that explicitly optimizes the attention distribution at the head level through contrastive learning. According to the experimental results, MuDAF can significantly improve the long-context question answering performance of LLMs, especially in multi-document question answering. Extensive evaluations on retrieval scores and attention visualizations show that MuDAF possesses great potential in making attention heads more focused on relevant information and reducing attention distractions.', 'abstract_zh': '大型语言模型（LLMs）经常由于输入中的无关信息而表现出注意力分散的情况，这严重影响了它们处理长上下文的能力。受到有关长上下文事实性检索头部有效性研究的启发，我们旨在通过直接改进检索头部来解决这一注意力分散问题。我们提出了一种名为多文档注意力聚焦（MuDAF）的新方法，该方法通过对比学习显式优化头部的注意力分布。实验结果显示，MuDAF 能够显著提高LLMs 的长上下文问答性能，尤其是在多文档问答方面。广泛的检索得分评估和注意力可视化结果表明，MuDAF 具有使注意力头部更专注于相关信息并减少注意力分散的巨大潜力。', 'title_zh': 'MuDAF：通过对比学习在注意力头上的长上下文多文档注意力聚焦'}
{'arxiv_id': 'arXiv:2502.13962', 'title': 'Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering', 'authors': 'William Jurayj, Jeffrey Cheng, Benjamin Van Durme', 'link': 'https://arxiv.org/abs/2502.13962', 'abstract': 'Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.', 'abstract_zh': '在推理过程中扩大大型语言模型的测试时计算量已经展示了在推理基准上的出色性能。然而，现有对测试时缩放的评估过于假设推理系统应始终对提供的每个问题给出答案。这种做法忽略了模型对答案的信心问题，以及始终提供回应的适当性。为了解决这些问题，我们在推理过程中提取置信度分数以阈值化模型的响应。我们发现，增加推理时的计算预算不仅有助于模型更准确地回答更多问题，还能提高正确答案的置信度。然后，我们通过考虑具有非零响应风险级别的设置扩展当前的零风险响应评估范式，并建议在这些设置下报告评估结果的方案。', 'title_zh': '这是你的最终答案吗？测试时缩放可以提升选择性问答性能'}
{'arxiv_id': 'arXiv:2502.13959', 'title': 'LIDDIA: Language-based Intelligent Drug Discovery Agent', 'authors': 'Reza Averly, Frazier N. Baker, Xia Ning', 'link': 'https://arxiv.org/abs/2502.13959', 'abstract': 'Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.', 'abstract_zh': '药物发现是一个漫长、昂贵且复杂的过程，高度依赖于专业化的药物化学家，他们可能需要数年时间来探索潜在疗法的庞大空间。最近在化学领域的人工智能技术取得了进展，旨在加速药物发现的个别任务；然而，仍迫切需要一个智能代理来导航整个药物发现过程。为此，我们介绍了一种名为LIDDiA的自主智能体，能够在计算环境中智能地导航药物发现过程。通过利用大型语言模型的推理能力，LIDDiA 成为了低成本且高度适应的自主药物发现工具。我们对LIDDiA进行了全面的评测，证明了以下几点：(1) 在超过70%的30个临床相关目标上，它可以生成符合关键药理标准的分子；(2) 它在化学空间中智能地平衡了探索与利用；(3) 它能够在EGFR（一种对癌症至关重要的靶标）中识别出有前景的新药候选物。', 'title_zh': 'LIDDIA：基于语言的智能药物发现代理'}
{'arxiv_id': 'arXiv:2502.13957', 'title': 'RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision', 'authors': 'Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang', 'link': 'https://arxiv.org/abs/2502.13957', 'abstract': 'Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at this https URL.', 'abstract_zh': '检索增强生成（RAG）在知识密集型任务中展现出了巨大潜力，但其传统的架构依赖于静态检索，这限制了其在需要顺序信息获取的复杂问题上的有效性。尽管主动推理和搜索提供了一种更加适应的解决方案，但目前大多数方法依然高度依赖于提示工程。在本项工作中，我们提出了RAG-Gym，这是一种统一的优化框架，通过在每次搜索步骤中进行细粒度的过程监督来增强信息获取代理。我们还提出了一种名为ReSearch的新颖代理架构，其在RAG-Gym框架内实现了答案推理和搜索查询生成的协同。在四个具有挑战性的数据集上的实验表明，RAG-Gym能够在各种代理架构中提高性能多达25.6%，并且ReSearch持续优于现有基准。进一步的分析展示了高级大型语言模型作为过程奖励评判者的有效性，并揭示了训练好的奖励模型在不同大型语言模型中作为验证器的可转移性。此外，我们还研究了主动RAG在训练和推理方面的扩展性。该项目的主页可以通过以下链接访问：这个网址。', 'title_zh': 'RAG-Gym：通过过程监督优化推理和搜索智能体'}
{'arxiv_id': 'arXiv:2502.13954', 'title': 'Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition', 'authors': 'Jingwang Huang, Jiang Zhong, Qin Lei, Jinpeng Gao, Yuming Yang, Sirui Wang, Peiguang Li, Kaiwen Wei', 'link': 'https://arxiv.org/abs/2502.13954', 'abstract': 'Multimodal multi-label emotion recognition (MMER) aims to identify the concurrent presence of multiple emotions in multimodal data. Existing studies primarily focus on improving fusion strategies and modeling modality-to-label dependencies. However, they often overlook the impact of \\textbf{aleatoric uncertainty}, which is the inherent noise in the multimodal data and hinders the effectiveness of modality fusion by introducing ambiguity into feature representations. To address this issue and effectively model aleatoric uncertainty, this paper proposes Latent emotional Distribution Decomposition with Uncertainty perception (LDDU) framework from a novel perspective of latent emotional space probabilistic modeling. Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal method that accounts for the dispersed distribution of uncertainty and integrates distribution information. Experimental results show that LDDU achieves state-of-the-art performance on the CMU-MOSEI and M$^3$ED datasets, highlighting the importance of uncertainty modeling in MMER. Code is available at this https URL\\this http URL.', 'abstract_zh': '多模态多标签情绪识别（MMER）旨在同时识别多模态数据中存在的多种情绪。现有研究主要集中在改进融合策略和建模模态到标签的相关性上。然而，它们往往忽略了 aleatoric 不确定性的影响，即多模态数据中固有的噪声，这通过引入特征表示中的模糊性阻碍了模态融合的效果。为了解决这一问题并有效建模 aleatoric 不确定性，本文从潜在情绪空间的概率建模这一新颖角度，提出了潜在情绪分布分解与不确定性感知（LDDU）框架。具体而言，我们引入了一种在情绪空间内的对比性分离分布机制来建模多模态数据，从而使我们能够提取语义特征和不确定性。此外，我们设计了一种不确定性感知的多模态融合方法，能够考虑不确定性分布的分散性，并整合分布信息。实验结果显示，LDDU 在 CMU-MOSEI 和 M³ED 数据集上达到了最先进的性能，强调了在 MMER 中建模不确定性的重要性。代码可在以下网址获得：此链接 [此链接]。', 'title_zh': '潜在分布解耦：一种面向不确定性的多模态情感识别概率框架'}
{'arxiv_id': 'arXiv:2502.13946', 'title': "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region", 'authors': 'Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li', 'link': 'https://arxiv.org/abs/2502.13946', 'abstract': "The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.", 'abstract_zh': '大型语言模型（LLMs）的安全对齐仍然存在漏洞，因为它们的初始行为可以通过相对简单的攻击轻易被破解。鉴于现有LLMs在输入指令与初始模型输出之间填充固定模板是一个常见做法，我们假设模板是这些漏洞的关键因素：LLMs在安全相关决策上过度依赖模板区域的综合信息，这极大地影响了这些模型的安全行为。我们称这一问题为基于模板的安全对齐。在本文中，我们进行了广泛的实验并验证了基于模板的安全对齐在各种对齐后的LLMs中普遍存在。我们的机制分析显示了它在遇到推理时的破解攻击时导致模型的脆弱性。此外，我们表明从模板区域分离安全机制可以有效地缓解破解攻击的脆弱性。我们鼓励未来的研究开发减少对模板区域依赖的更 robust 的安全对齐技术。', 'title_zh': '为什么受保护的船舶会搁浅？对齐的大语言模型的安全机制往往会植根于模板区域'}
{'arxiv_id': 'arXiv:2502.13925', 'title': 'Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?', 'authors': 'Xiaochen Wang, Heming Xia, Jialin Song, Longyu Guan, Yixin Yang, Qingxiu Dong, Weiyao Luo, Yifan Pu, Yiru Wang, Xiangdi Meng, Wenjie Li, Zhifang Sui', 'link': 'https://arxiv.org/abs/2502.13925', 'abstract': 'Large Multimodal Models (LMMs) have achieved remarkable success across various visual-language tasks. However, existing benchmarks predominantly focus on single-image understanding, leaving the analysis of image sequences largely unexplored. To address this limitation, we introduce StripCipher, a comprehensive benchmark designed to evaluate capabilities of LMMs to comprehend and reason over sequential images. StripCipher comprises a human-annotated dataset and three challenging subtasks: visual narrative comprehension, contextual frame prediction, and temporal narrative reordering. Our evaluation of $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a significant performance gap compared to human capabilities, particularly in tasks that require reordering shuffled sequential images. For instance, GPT-4o achieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower than human performance. Further quantitative analysis discuss several factors, such as input format of images, affecting the performance of LLMs in sequential understanding, underscoring the fundamental challenges that remain in the development of LMMs.', 'abstract_zh': '大规模多模态模型（Large Multimodal Models, LMMs）在各种视觉-语言任务中取得了显著的成功。然而，现有的基准测试主要集中在单张图像的理解上，对图像序列的分析研究相对较少。为了弥补这一不足，我们提出了StripCipher，一个全面的基准测试，旨在评估LMMs在理解和推理图像序列方面的能力。StripCipher 包含一个人工标注的数据集，并设置了三个具有挑战性的子任务：视觉叙述理解、上下文帧预测和时间叙述重新排序。我们对16个最先进的LMMs（包括GPT-4o 和Qwen2.5VL）进行了评估，发现其性能与人类能力之间存在显著差距，特别是在需要重新排序打乱的图像序列的任务中。例如，GPT-4o 在重新排序子任务中的准确率仅为23.93%，比人类的表现低56.07%。进一步的定量分析讨论了一些因素，如图像输入格式对LMMs在序列理解方面的性能影响，突显了LMMs发展中仍存在的根本性挑战。', 'title_zh': '超越单一画面：LMMs 能够理解图像序列中的时间与上下文叙事吗？'}
{'arxiv_id': 'arXiv:2502.13922', 'title': 'LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization', 'authors': 'Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing', 'link': 'https://arxiv.org/abs/2502.13922', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \\ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales.', 'abstract_zh': '大型语言模型（LLMs）通过预训练和对齐展示了显著的能力。然而，表现优异的短上下文LLM在长上下文场景中可能表现不佳，这主要是由于它们在长上下文对齐方面不够充分。这一对齐过程因长时间人工标注的不切实际以及短上下文和长上下文性能平衡的难度而变得具有挑战性。为了解决这些问题，我们引入了LongPO方法，该方法使短上下文LLM能够在长上下文任务中自我进化，通过内部转移短上下文能力来优化自身。LongPO利用LLM学习自我生成的短到长偏好数据，这些数据由针对相同指令生成的长上下文响应及其压缩的短上下文对应版本组成。这种偏好揭示了在短上下文对齐过程中培养的LLM的能力和潜力，这些能力在不足对齐的长上下文场景中可能会被削弱。此外，LongPO还引入了一种短到长KL约束，以减轻在长上下文对齐过程中短上下文性能的下降。当应用于从128K到512K上下文长度的Mistral-7B-Instruct-v0.2时，LongPO能够在长上下文和短上下文任务中完全保留短上下文性能，并在长上下文和短上下文任务中显著优于简单的自训练（SFT）和DPO方法。特别是，由我们的方法训练的模型在长上下文基准测试中的表现可以与或超越那些涉及大量长上下文标注和更大参数规模的优于LLM（例如，GPT-4-128K）的表现。', 'title_zh': '长文脉：通过短到长偏好优化的大语言模型的长文脉自我进化'}
{'arxiv_id': 'arXiv:2502.13917', 'title': 'TESS 2: A Large-Scale Generalist Diffusion Language Model', 'authors': 'Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan', 'link': 'https://arxiv.org/abs/2502.13917', 'abstract': 'We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at this https URL.', 'abstract_zh': '我们介绍了一种名为TESS 2的通用指令遵循扩散语言模型，其表现优于当代的指令微调扩散模型，并且能够匹敌甚至超越强大的自回归（AR）模型。我们通过首先使用通常的交叉熵作为扩散损失进行连续预训练来适应一个强大的AR模型，然后进行进一步的指令微调来训练TESS 2。我们发现，适应训练以及基模型的选择对于训练出表现良好的指令遵循扩散模型至关重要。此外，我们还提出了一种新颖且模块化的推理时奖励指导程序，可以在无需重新训练底层模型的情况下对模型输出进行对齐。最后，我们展示了通过增加推理时的计算资源，TESS 2的性能进一步提升，突显了扩散语言模型在推理时具有精细控制计算资源使用的实用性。源代码和模型可以在以下链接获取：this https URL', 'title_zh': 'TESS 2：大规模通用扩散语言模型'}
{'arxiv_id': 'arXiv:2502.13913', 'title': 'How Do LLMs Perform Two-Hop Reasoning in Context?', 'authors': 'Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell', 'link': 'https://arxiv.org/abs/2502.13913', 'abstract': '"Socrates is human. All humans are mortal. Therefore, Socrates is mortal." This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language Models (LLMs) can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches $100%$ accuracy. Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training.', 'abstract_zh': '苏格拉底是人。所有人都会死亡。因此，苏格拉底会死亡。这个经典的推理例子展示了二阶推理，即结论从两个相连的前提中逻辑地得出。虽然基于变压器的大型语言模型（LLMs）可以进行二阶推理，但在面对分散注意力的前提时，它们往往会退化为随机猜测。为了理解背后的机制，我们在合成的二阶推理任务上训练了一个三层的变压器。训练动力学显示两个阶段：一个缓慢的学习阶段，在此阶段中，三层变压器像LLMs一样进行随机猜测，随后是一个突然的阶段转换，在此阶段中，三层变压器突然达到100%的准确率。通过逆向工程，我们解释了模型如何最初在分散注意力的情况下进行随机猜测，又如何最终学会忽略这些分散注意力的因素。此外，我们提出了一种三参数模型，支持这些机制对变压器训练动力学因果关系的说明。最后，对LLMs的实验表明，发现的机制具有相同的适用性。我们的方法为科学理解LLMs提供了新的视角，而我们的发现为理解训练过程中的推理如何出现提供了新的见解。', 'title_zh': 'LLMs在上下文中执行两跳推理的表现如何？'}
{'arxiv_id': 'arXiv:2502.13897', 'title': 'DataSciBench: An LLM Agent Benchmark for Data Science', 'authors': 'Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue', 'link': 'https://arxiv.org/abs/2502.13897', 'abstract': 'This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at this https URL.', 'abstract_zh': '本文介绍了DataSciBench，这是一个全面的基准测试框架，用于评估大型语言模型（LLM）在数据科学中的能力。近期的相关基准测试主要集中在单一任务、易于获取的基准 truth 和简洁的评估指标上，这限制了可以评估的任务范围。相比之下，DataSciBench 是基于更全面和精心挑选的自然且具有挑战性的提示构建的，适用于模糊的基准 truth 和评估指标。我们开发了一种半自动管道来生成基准 truth (GT) 并验证评估指标。该管道利用并实现了基于 LLM 的自一致性与人工验证策略，通过利用收集到的提示、预定义的任务类型以及聚合函数（指标）来生成准确的 GT。此外，我们提出了一个创新的任务-功能-代码（TFC）框架，用于根据精确定义的指标和编程规则评估每次代码执行的结果。我们的实验框架包括使用我们收集的多种提示对6个基于API的模型、8个开源通用模型和9个开源代码生成模型进行测试。这种方法旨在提供更全面和严格的 LLM 在数据科学中的评估，揭示其优缺点。实验结果表明，基于API的模型在所有指标上均优于开源模型，Deepseek-Coder-33B-Instruct 在开源模型中得分最高。我们已将所有代码和数据发布在该网址：this https URL。', 'title_zh': 'DataSciBench：一个用于数据科学的大语言模型代理基准测试'}
{'arxiv_id': 'arXiv:2502.13881', 'title': 'PSCon: Toward Conversational Product Search', 'authors': 'Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen', 'link': 'https://arxiv.org/abs/2502.13881', 'abstract': 'Conversational Product Search (CPS) is confined to simulated conversations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.', 'abstract_zh': '受限于缺乏反映人类语言特征的真实世界产品搜索对话数据集，当前的产品搜索对话（Conversational Product Search, CPS）研究主要局限于模拟对话。此外，现有对话数据集在支持跨市场和多语言使用方面存在局限。在本文中，我们介绍了一种新的CPS数据收集协议，并提出了PSCon数据集，该数据集旨在通过类似人类的对话来辅助产品搜索。该数据集使用指导下的真人对真人数据收集协议构建，支持两种语言和两个市场。此外，该数据集能够对CPS的六个子任务进行充分探索：用户意图检测、关键词提取、系统行动预测、问题选择、项目排序和响应生成。进一步地，我们还对数据集进行了分析，并在此基础上提出了基准模型。', 'title_zh': 'PSCon: 朝向对话型产品搜索'}
{'arxiv_id': 'arXiv:2502.13853', 'title': 'Fine-grained Fallacy Detection with Human Label Variation', 'authors': 'Alan Ramponi, Agnese Daffara, Sara Tonelli', 'link': 'https://arxiv.org/abs/2502.13853', 'abstract': 'We introduce Faina, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. Faina includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond "single ground truth" evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span matches, overlaps, and the varying severity of labeling errors. Our experiments across four fallacy detection setups show that multi-task and multi-label transformer-based approaches are strong baselines across all settings. We release our data, code, and annotation guidelines to foster research on fallacy detection and human label variation more broadly.', 'abstract_zh': '我们介绍了Faina数据集，这是首个既包含多元合理解释又包容自然分歧的谬误检测数据集。Faina 包含超过11,000个跨度级别的标注，覆盖了20种类型的谬误，这些标注是两位专家在关于移民、气候变化和公共卫生等主题的意大利社交媒体帖子上提供的。通过多轮深入的标注研究，我们最大限度地减少了标注错误，同时保留了人类标注标签变异的信号。此外，我们提出了一种框架，超越了单一地面真实值的评估方法，同时考虑了多种（同样可靠）的测试集以及该任务的独特之处，即部分跨度匹配、重叠和标注错误的不同严重程度。我们在四个谬误检测设置的实验中发现，多任务和多标签的变换器基线方法在所有设置中表现强劲。我们发布了我们的数据、代码和标注指南，以促进谬误检测和人类标注变异的研究。', 'title_zh': '细粒度谬误检测中的人工标注变异分析'}
{'arxiv_id': 'arXiv:2502.13847', 'title': 'DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue', 'authors': 'Feiyuan Zhang, Dezhi Zhu, James Ming, Yilun Jin, Di Chai, Liu Yang, Han Tian, Zhaoxin Fan, Kai Chen', 'link': 'https://arxiv.org/abs/2502.13847', 'abstract': 'Retrieval-Augmented Generation (RAG) systems have shown substantial benefits in applications such as question answering and multi-turn dialogue \\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging static knowledge bases, often overlook the potential of dynamic historical information in ongoing conversations. To bridge this gap, we introduce DH-RAG, a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that utilize both long-term memory and immediate historical context in conversational responses \\citep{stafford1987conversational}. DH-RAG is structured around two principal components: a History-Learning based Query Reconstruction Module, designed to generate effective queries by synthesizing current and prior interactions, and a Dynamic History Information Updating Module, which continually refreshes historical context throughout the dialogue. The center of DH-RAG is a Dynamic Historical Information database, which is further refined by three strategies within the Query Reconstruction Module: Historical Query Clustering, Hierarchical Matching, and Chain of Thought Tracking. Experimental evaluations show that DH-RAG significantly surpasses conventional models on several benchmarks, enhancing response relevance, coherence, and dialogue quality.', 'abstract_zh': '检索增强生成（RAG）系统在问答和多轮对话等应用中显示了显著的优势 \\citep{lewis2020retrieval}。然而，传统的RAG方法虽然利用了静态知识库，但在 ongoing 会话中往往忽视了动态历史信息的潜力。为弥补这一差距，我们引入了 DH-RAG，这是一种基于动态历史语境的检索增强生成方法，适用于多轮对话。DH-RAG 受启发于人类认知过程，该过程在对话响应中同时利用长期记忆和即时历史语境 \\citep{stafford1987conversational}。DH-RAG 包含两大主要组件：一个基于历史学习的查询重构模块，旨在通过综合当前和先前的交互生成有效的查询，以及一个动态历史信息更新模块，该模块在整个对话过程中不断刷新历史语境。DH-RAG 的核心是动态历史信息数据库，该数据库在查询重构模块内的三种策略的 refining 过程中进一步优化：历史查询聚类、分层匹配和思维链跟踪。实验评估表明，DH-RAG 在多个基准测试中显著优于常规模型，增强了响应的相关性、连贯性和对话质量。', 'title_zh': 'DH-RAG：一种基于动态历史语境的检索增强生成方法用于多轮对话'}
{'arxiv_id': 'arXiv:2502.13842', 'title': 'Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking', 'authors': 'Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang', 'link': 'https://arxiv.org/abs/2502.13842', 'abstract': 'Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5\\% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2\\%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.', 'abstract_zh': '大型语言模型（LLMs）在参数约束下面临着固有的性能瓶颈，特别是在处理需要复杂推理的关键令牌时尤为明显。实证分析显示，这些关键令牌会在各层之间引发陡峭的梯度跃变，揭示了标准Transformer架构中的关键压力点。基于这一洞察，我们提出了Inner Thinking Transformer（ITT），重新构想了各层计算为隐式的思考步骤。ITT通过自适应令牌路由动态分配计算资源，通过残差思考连接迭代细化表示，并使用思考步骤编码区分不同的推理阶段。ITT能够在不增加参数量的情况下，更深入地处理关键令牌。在参数规模从162M到466M的各种模型上的评估表明，ITT在使用仅162M参数的情况下实现了466M Transformer 96.5%的性能，减少了43.2%的训练数据，并在11个基准测试中优于Transformer/Loop变体。通过在推理过程中实现弹性计算分配，ITT通过架构感知优化隐式思考路径，实现了性能与效率的平衡。', 'title_zh': '内生思考变压器：利用动态深度缩放促进适应性内部思考'}
{'arxiv_id': 'arXiv:2502.13791', 'title': 'From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions', 'authors': 'Nathanaël Carraz Rakotonirina, Mohammed Hamdy, Jon Ander Campos, Lucas Weber, Alberto Testoni, Marzieh Fadaee, Sandro Pezzelle, Marco Del Tredici', 'link': 'https://arxiv.org/abs/2502.13791', 'abstract': "Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.", 'abstract_zh': '大型语言模型（LLMs）在工作环境中被广泛用于完成各种任务，表现出色于解决孤立的个人问题。然而，它们是否能够在长时间的交互中有效协作呢？为探究这一问题，我们引入了MemoryCode，这是一个合成的多会话数据集，旨在测试LLMs在面对大量无关信息时跟踪和执行简单编码指令的能力，模拟了现实环境。所有我们测试的模型在处理孤立指令方面表现良好，然而，即使是性能最先进的模型（如GPT-4o）在指令分布在多会话中的情况下表现下降。我们的分析表明，这是由于这些模型未能在长时间的指令链中检索和整合信息。我们的研究结果突显了当前LLMs的一个基本局限性，限制了它们在长时间交互中有效协作的能力。', 'title_zh': '从工具到同伴：评估多轮次编码交互中的大语言模型'}
{'arxiv_id': 'arXiv:2502.13780', 'title': 'Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions', 'authors': 'Beatrice Savoldi, Alan Ramponi, Matteo Negri, Luisa Bentivogli', 'link': 'https://arxiv.org/abs/2502.13780', 'abstract': "Converging societal and technical factors have transformed language technologies into user-facing applications employed across languages. Machine Translation (MT) has become a global tool, with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs). This accessibility has expanded MT's reach to a vast base of lay users, often with little to no expertise in the languages or the technology itself. Despite this, the understanding of MT consumed by this diverse group of users -- their needs, experiences, and interactions with these systems -- remains limited. This paper traces the shift in MT user profiles, focusing on non-expert users and how their engagement with these systems may change with LLMs. We identify three key factors -- usability, trust, and literacy -- that shape these interactions and must be addressed to align MT with user needs. By exploring these dimensions, we offer insights to guide future MT with a user-centered approach.", 'abstract_zh': '社会和技术因素的相互作用已将语言技术转变为跨语言使用的用户面向应用。机器翻译（MT）已成为一种全球工具，现在跨语言服务也得到了多语言大规模语言模型（LLMs）驱动的对话系统的支持。这种可访问性使MT扩展到了大量的非专业用户群体，这些用户通常对所使用的语言或技术缺少专业知识。尽管如此，用户（尤其是这一多样化群体）对MT的理解——包括他们对这些系统的需要、体验和互动——仍相当有限。本文探讨了MT用户群体的变化，重点关注非专家用户及其在LLMs支持下的系统互动如何变化。我们确定了三项关键因素——易用性、信任和识字能力——这些因素塑造了这些互动，并且必须解决以使MT更好地满足用户需求。通过探索这些维度，本文提供了指导未来用户中心化机器翻译的见解。', 'title_zh': '众手翻译：将普通用户置于机器翻译互动中的核心位置'}
{'arxiv_id': 'arXiv:2502.13776', 'title': 'EHOP: A Dataset of Everyday NP-Hard Optimization Problems', 'authors': 'Alex Duchnowski, Ellie Pavlick, Alexander Koller', 'link': 'https://arxiv.org/abs/2502.13776', 'abstract': 'We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a collection of NP-hard optimization problems expressed in natural language. EHOP includes problem formulations that could be found in computer science textbooks, versions that are dressed up as problems that could arise in real life, and variants of well-known problems with inverted rules. We find that state-of-the-art LLMs, across multiple prompting strategies, systematically solve textbook problems more accurately than their real-life and inverted counterparts. We argue that this constitutes evidence that LLMs adapt solutions seen during training, rather than leveraging reasoning abilities that would enable them to generalize to novel problems.', 'abstract_zh': '我们介绍了日常棘手优化问题（EHOP）数据集，这是一个用自然语言表达的NP难优化问题的集合。EHOP包含可以在计算机科学教科书中找到的问题表述，以及被包装成可能在现实生活中出现的问题版本，还包括一些具有反向规则的经典问题变体。我们发现，无论是采用哪种提示策略，最先进的语言模型（LLM）都系统地比它们的现实生活和反向对应版本更准确地解决了教科书中的问题。我们认为，这表明语言模型是通过适应训练期间看到的解决方案来进行工作的，而不是利用能够使它们泛化到新型问题的推理能力。', 'title_zh': 'EHOP：日常NP难优化问题数据集'}
{'arxiv_id': 'arXiv:2502.13775', 'title': 'VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare', 'authors': 'Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem', 'link': 'https://arxiv.org/abs/2502.13775', 'abstract': 'Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.', 'abstract_zh': '对齐技术已经成为确保大型语言模型（LLMs）生成与人类价值观一致的输出的核心。然而，现有的对齐范式通常建模的是平均或单一偏好，未能考虑到不同文化和社区中多元视角的多样性。在健康相关的情景中，这种局限性尤为重要，因为文化、宗教、个人价值观和分歧意见的影响导致了多样性的重要性。尽管在多元对齐方面取得了进展，但之前没有研究关注健康领域，这可能是因为缺乏公开可用的数据集。为解决这一问题，我们引入了VITAL，这是一个新的基准数据集，包含13100个价值观导向的情境和5400个关于健康的选择题，旨在评估和基准测试多元对齐方法。通过对包括不同大小的八种LLM的广泛评估，我们表明现有的多元对齐技术在有效容纳多元化的医疗保健信念方面存在不足，突显了在特定领域定制AI对齐的必要性。这项工作揭示了当前方法的局限性，并为开发健康特定领域的对齐解决方案奠定了基础。', 'title_zh': 'VITAL：用于医疗领域多元一致性的基准测试新数据集'}
{'arxiv_id': 'arXiv:2502.13766', 'title': 'GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking', 'authors': 'Florian Schneider, Carolin Holtermann, Chris Biemann, Anne Lauscher', 'link': 'https://arxiv.org/abs/2502.13766', 'abstract': 'Large Vision-Language Models (LVLMs) have recently gained attention due to their distinctive performance and broad applicability. While it has been previously shown that their efficacy in usage scenarios involving non-Western contexts falls short, existing studies are limited in scope, covering just a narrow range of cultures, focusing exclusively on a small number of cultural aspects, or evaluating a limited selection of models on a single task only. Towards globally inclusive LVLM research, we introduce GIMMICK, an extensive multimodal benchmark designed to assess a broad spectrum of cultural knowledge across 144 countries representing six global macro-regions. GIMMICK comprises six tasks built upon three new datasets that span 728 unique cultural events or facets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary and 26 open-weight models of all sizes. We systematically examine (1) regional cultural biases, (2) the influence of model size, (3) input modalities, and (4) external cues. Our analyses reveal strong biases toward Western cultures across models and tasks and highlight strong correlations between model size and performance, as well as the effectiveness of multimodal input and external geographic cues. We further find that models have more knowledge of tangible than intangible aspects (e.g., food vs. rituals) and that they excel in recognizing broad cultural origins but struggle with a more nuanced understanding.', 'abstract_zh': '大型多模态语言视觉模型（LVLMs）因其实现出色性能和广泛的适用性而最近受到了关注。尽管已有研究表明，这些模型在涉及非西方语境的应用场景中的效果不佳，但现有研究的范围有限，仅涵盖了少量的文化，专注于少数文化方面的内容，或者仅在一个任务上评估少量模型。为了实现全球包容性的LVLM研究，我们引入了GIMMICK——一个广泛的多模态基准，旨在评估来自六大全球宏观区域的144个国家的文化知识谱系。GIMMICK包含六个基于三个新数据集的任务，覆盖了728个独特的文化事件或方面，对20个LVLM和11个LLM（包括5个私有和26个开放权重模型，无论规模大小）进行了评估。我们系统地考察了以下方面的影响：(1) 地域文化偏见，(2) 模型规模的影响，(3) 输入模态，以及(4) 外部线索。我们的分析揭示了模型和任务中强烈偏向于西方文化的偏见，强调了模型规模与性能之间的强关联，以及多模态输入和外部地理线索的有效性。我们进一步发现，模型对有形文化（如食物）比无形文化（如仪式）了解更多，且在识别广泛的文化起源上表现出色，但在更细致的理解方面则表现欠佳。', 'title_zh': 'GIMMICK — 全球包容性多模态多任务文化知识基准测试'}
{'arxiv_id': 'arXiv:2502.13753', 'title': 'SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning', 'authors': 'Renxi Wang, Honglin Mu, Liqun Ma, Lizhi Lin, Yunlong Feng, Timothy Baldwin, Xudong Han, Haonan Li', 'link': 'https://arxiv.org/abs/2502.13753', 'abstract': "Evaluating large language models' (LLMs) long-context understanding capabilities remains challenging. We present SCALAR (Scientific Citation-based Live Assessment of Long-context Academic Reasoning), a novel benchmark that leverages academic papers and their citation networks. SCALAR features automatic generation of high-quality ground truth labels without human annotation, controllable difficulty levels, and a dynamic updating mechanism that prevents data contamination. Using ICLR 2025 papers, we evaluate 8 state-of-the-art LLMs, revealing key insights about their capabilities and limitations in processing long scientific documents across different context lengths and reasoning types. Our benchmark provides a reliable and sustainable way to track progress in long-context understanding as LLM capabilities evolve.", 'abstract_zh': '评估大型语言模型（LLMs）的长文理解能力仍然具有挑战性。我们提出了SCALAR（基于科学引文的长文学术推理实时评估），这是一种新颖的基准测试，利用了学术论文及其引文网络。SCALAR 特点在于其能够自动生成高质量的无标注真实标签，可控的难度等级，以及动态更新机制以防止数据污染。通过使用ICLR 2025论文，我们对8个最先进的LLMs进行了评估，揭示了它们在处理不同类型推理和不同上下文长度的长科学文档时的能力和局限性。我们的基准测试提供了一种可靠且可持续的方法，用于跟踪LLM能力演进过程中长文理解的进步。', 'title_zh': 'SCALAR：基于科学引用的长上下文学术推理实时评估'}
{'arxiv_id': 'arXiv:2502.13738', 'title': 'Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding', 'authors': 'Keqin Peng, Liang Ding, Yuanxin Ouyang, Meng Fang, Yancheng Yuan, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.13738', 'abstract': 'Large language models (LLMs) excel at a range of tasks through in-context learning (ICL), where only a few task examples guide their predictions. However, prior research highlights that LLMs often overlook input-label mapping information in ICL, relying more on their pre-trained knowledge. To address this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method that emphasizes input-label mapping by contrasting the output distributions between positive and negative in-context examples. Experiments on 7 natural language understanding (NLU) tasks show that our ICCD method brings consistent and significant improvement (up to +2.1 improvement on average) upon 6 different scales of LLMs without requiring additional training. Our approach is versatile, enhancing performance with various demonstration selection methods, demonstrating its broad applicability and effectiveness. The code and scripts will be publicly released.', 'abstract_zh': '大规模语言模型（LLMs）通过在上下文学习（ICL） excel 在各种任务中，其中只有少量的任务示例引导它们的预测。然而，先前的研究指出，LLMs 倾向于在 ICL 中忽略输入-标签映射信息，更多地依赖它们的预训练知识。为了解决这一问题，我们提出了在上下文对比解码（ICCD）方法，这是一种通过对比正向和负向在上下文示例的输出分布来强调输入-标签映射的新方法。我们在7项自然语言理解（NLU）任务上的实验表明，我们的ICCD方法在6种不同规模的LLMs上带来了持续且显著的改进（平均提高幅度高达+2.1%），而无需额外训练。我们的方法具有通用性，通过各种演示选择方法增强性能，展示了其广泛适用性和有效性。我们将公开发布代码和脚本。', 'title_zh': '在上下文学习中通过对比解码增强输入-标签映射'}
{'arxiv_id': 'arXiv:2502.13725', 'title': 'Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method', 'authors': 'Juyuan Zhang, Wei Zhu, Jiechao Gao', 'link': 'https://arxiv.org/abs/2502.13725', 'abstract': "Time series modeling holds significant importance in many real-world applications and has been extensively studied. While pre-trained foundation models have made impressive strides in the fields of natural language processing (NLP) and computer vision (CV), their development in time series domains has been constrained by data sparsity. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities, and (b) keeping the inference efficiency. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the LLM backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model's predictive capabilities. Our experimental results on an extensive collection of challenging real-world time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance.", 'abstract_zh': '时间序列建模在许多实际应用中具有重要意义并得到了广泛研究。虽然预训练基础模型已在自然语言处理（NLP）和计算机视觉（CV）领域取得了显著进展，但在时间序列领域的发展受到了数据稀疏性的限制。一系列近期研究表明，大型语言模型（LLMs）在处理复杂序列的模式识别和推理方面表现出强大的能力。然而，当前文献尚未在以下两个方面实现高质量的平衡：（a）有效地将时间序列模态与自然语言模态对齐，以及（b）保持推理效率。为了解决上述问题，我们现提出Time-LlaMA框架。Time-LlaMA首先通过线性标记化机制将时间序列输入转换为标记嵌入。其次，时间序列的标记嵌入与文本提示对齐。第三，为了进一步适应LLM基础模型的时间序列建模需求，我们开发了一种动态低秩适应技术（D-LoRA）。D-LoRA动态地在转换器基础模型的每一层为每一组时间序列输入选择最合适的LoRA模块，从而增强模型的预测能力。我们在一系列具有挑战性的实际时间序列任务上的实验结果证实，我们的方法取得了现有最佳（SOTA）的表现。', 'title_zh': '通过一种新型参数高效适应方法，将大型语言模型适应于时间序列建模'}
{'arxiv_id': 'arXiv:2502.13723', 'title': 'Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values', 'authors': 'Hongbo Zhang, Han Cui, Guangsheng Bao, Linyi Yang, Jun Wang, Yue Zhang', 'link': 'https://arxiv.org/abs/2502.13723', 'abstract': 'We introduce Direct Value Optimization (DVO), an innovative reinforcement learning framework for enhancing large language models in complex reasoning tasks. Unlike traditional methods relying on preference labels, DVO utilizes value signals at individual reasoning steps, optimizing models via a mean squared error loss. The key benefit of DVO lies in its fine-grained supervision, circumventing the need for labor-intensive human annotations. Target values within the DVO are estimated using either Monte Carlo Tree Search or an outcome value model. Our empirical analysis on both mathematical and commonsense reasoning tasks shows that DVO consistently outperforms existing offline preference optimization techniques, even with fewer training steps. These findings underscore the importance of value signals in advancing reasoning capabilities and highlight DVO as a superior methodology under scenarios lacking explicit human preference information.', 'abstract_zh': '我们将介绍直接值优化（Direct Value Optimization, DVO），这是一种增强大型语言模型在复杂推理任务中表现的创新强化学习框架。与依赖偏好标签的传统方法不同，DVO 利用每个推理步骤的价值信号，通过均方误差损失优化模型。DVO 的关键优势在于其精细的监督，从而绕过了劳动密集型的人工注释需求。在DVO中，目标值可以通过蒙特卡洛树搜索或结果值模型进行估计。我们在数学和常识推理任务上的实证分析表明，即使在更少的训练步数下，DVO 也一致地优于现有的离线偏好优化技术。这些发现强调了价值信号在提升推理能力方面的重要性，并突显了在缺乏明确人类偏好信息的情况下，DVO 是一种更优的方法。', 'title_zh': '直接价值优化：通过精炼的价值提升大规模语言模型的链式思维推理能力'}
{'arxiv_id': 'arXiv:2502.13718', 'title': 'Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis', 'authors': 'Chengyan Wu, Bolei Ma, Ningyuan Deng, Yanqing He, Yun Xue', 'link': 'https://arxiv.org/abs/2502.13718', 'abstract': "Aspect-based sentiment analysis (ABSA) is a sequence labeling task that has garnered growing research interest in multilingual contexts. However, recent studies lack more robust feature alignment and finer aspect-level alignment. In this paper, we propose a novel framework, Multi-Scale and Multi-Objective optimization (MSMO) for cross-lingual ABSA. During multi-scale alignment, we achieve cross-lingual sentence-level and aspect-level alignment, aligning features of aspect terms in different contextual environments. Specifically, we introduce code-switched bilingual sentences into the language discriminator and consistency training modules to enhance the model's robustness. During multi-objective optimization, we design two optimization objectives: supervised training and consistency training, aiming to enhance cross-lingual semantic alignment. To further improve model performance, we incorporate distilled knowledge of the target language into the model. Results show that MSMO significantly enhances cross-lingual ABSA by achieving state-of-the-art performance across multiple languages and models.", 'abstract_zh': '基于aspect的语义分析（ABSA）是一种序列标注任务，近年来在多语言环境中引起了越来越多的研究兴趣。然而，近期的研究缺乏更稳健的特征对齐和更精细的aspect级别对齐。在本文中，我们提出了一种新颖的框架——多尺度和多目标优化（MSMO）用于跨语言ABSA。在多尺度对齐过程中，我们实现了跨语言句子级别和aspect级别对齐，对不同上下文中aspect术语特征进行对齐。具体而言，我们在语言判别器和一致性训练模块中引入了代码混合的双语句子，以增强模型的鲁棒性。在多目标优化过程中，我们设计了两个优化目标：监督训练和一致性训练，旨在增强跨语言语义对齐。为进一步提高模型性能，我们还将目标语言的提炼知识融入到模型中。结果表明，MSMO显著提升了跨语言ABSA的表现，在多种语言和模型中取得了最先进的效果。', 'title_zh': '跨语言方面情感分析的多尺度与多目标优化'}
{'arxiv_id': 'arXiv:2502.13691', 'title': "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora", 'authors': 'Tristan Karch, Luca Engel, Philippe Schwaller, Frédéric Kaplan', 'link': 'https://arxiv.org/abs/2502.13691', 'abstract': "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using three strategically selected datasets: EPFL PhD manuscripts (likely containing novel specialized knowledge), Wikipedia articles (presumably part of training data), and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.", 'abstract_zh': '随着大规模语言模型（LLMs）的性能趋于相似，提高其性能的关键在于识别并整合有价值的新信息来源。然而，评估哪些文本集合值得投入大量资源进行数字化、预处理和集成到LLM系统中仍然是一个重大挑战。我们提出了一种新的应对策略：一种无需进行模型训练或微调的自动化工作流，用于评估文本集合的信息增益潜力。该方法从文本中生成多项选择题（MCQ），并测量LLM在有和没有访问原始材料情况下的表现。这两种情况下的表现差距作为集合信息潜力的代理指标。我们使用三个战略性选取的数据集进行了方法验证：EPFL博士论文（可能包含新颖的专业知识）、维基百科文章（假设是训练数据的一部分）和一个合成基线数据集。我们的结果显示，该方法能够有效识别包含有价值新颖信息的集合，为优先数据获取和整合努力提供了实用工具。', 'title_zh': '这个数据集值得我语言模型投入时间吗？自动测量文本语料库的信息潜力'}
{'arxiv_id': 'arXiv:2502.13685', 'title': 'MoM: Linear Sequence Modeling with Mixture-of-Memories', 'authors': 'Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.13685', 'abstract': 'Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain\'s ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at this https URL and is also released as a part of this https URL.', 'abstract_zh': '以下是从线性序列建模方法到Mixture-of-Memories (MoM)架构的翻译，符合学术规范：\n\n线性序列建模方法，如线性注意力、状态空间建模和线性RNNs，通过降低训练和推断的复杂性提供了显著的效率提升。然而，这类方法通常将整个输入序列压缩为单一的固定大小的内存状态，这在需要大量回忆的下游任务中会导致次优表现。受神经科学的启发，尤其是大脑在维持稳健长时记忆的同时减轻“记忆干扰”的能力，我们引入了一种新型架构——称为Mixture-of-Memories (MoM)。MoM 使用多个独立的内存状态，并通过路由器网络将输入词元导向特定的内存状态。这种方法大大增加了整体的内存容量，同时最大限度地减少了记忆干扰。因此，MoM 在回忆密集型任务上表现出色，超越了现有的线性序列建模技术。尽管 MoM 包含多个内存状态，但每个内存状态的计算仍然保持线性复杂度，使其在训练期间保留了线性复杂度优势，而在推断期间则保持常数复杂度。实验结果表明，MoM 在下游语言任务中显著优于当前的线性序列模型，特别是在需要大量回忆的任务上，其性能甚至可以与Transformer模型媲美。MoM的代码已在以下位置开源：[这里](https://example.com)，并且作为[这里](https://example.com)的一部分也进行了发布。', 'title_zh': 'MoM：混合记忆的线性序列建模'}
{'arxiv_id': 'arXiv:2502.13674', 'title': 'SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation', 'authors': 'Song Duong, Florian Le Bronnec, Alexandre Allauzen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari', 'link': 'https://arxiv.org/abs/2502.13674', 'abstract': "Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.", 'abstract_zh': '当大型语言模型（LLMs）用于条件文本生成时，往往会生成幻觉，即与输入上下文不符或缺乏根据的信息。这一问题在典型条件文本生成任务中尤为明显，如文本总结和数据到文本生成，其目标是基于上下文输入生成流畅的文本。当针对特定领域进行微调时，LLMs在提供与给定上下文相符的答案方面往往力不从心，经常添加信息或产生错误。这一问题的一个潜在原因是，LLMs依赖于其训练数据中学到的统计模式。这种依赖性可能会干扰模型保持对所提供上下文忠实度的能力，导致生成未根据的文本。我们进一步基于这一观察，提出了一种新颖的半监督方法，用于生成不合事实的样本集。随后，我们通过一种训练过程对模型进行改进，该过程鼓励生成现实依据的输出，而非不合事实的信息，借鉴偏好训练的方法。我们的方法显著提高了文本生成的现实依据程度，在忠实性方面优于现有半监督技术，这一结论通过自动评估指标、基于LLM的评估以及人机评估得到了验证。', 'title_zh': '标题：SCOPE：一种增强条件文本生成忠实性的自监督框架'}
{'arxiv_id': 'arXiv:2502.13668', 'title': 'PeerQA: A Scientific Question Answering Dataset from Peer Reviews', 'authors': 'Tim Baumgärtner, Ted Briscoe, Iryna Gurevych', 'link': 'https://arxiv.org/abs/2502.13668', 'abstract': 'We present PeerQA, a real-world, scientific, document-level Question Answering (QA) dataset. PeerQA questions have been sourced from peer reviews, which contain questions that reviewers raised while thoroughly examining the scientific article. Answers have been annotated by the original authors of each paper. The dataset contains 579 QA pairs from 208 academic articles, with a majority from ML and NLP, as well as a subset of other scientific communities like Geoscience and Public Health. PeerQA supports three critical tasks for developing practical QA systems: Evidence retrieval, unanswerable question classification, and answer generation. We provide a detailed analysis of the collected dataset and conduct experiments establishing baseline systems for all three tasks. Our experiments and analyses reveal the need for decontextualization in document-level retrieval, where we find that even simple decontextualization approaches consistently improve retrieval performance across architectures. On answer generation, PeerQA serves as a challenging benchmark for long-context modeling, as the papers have an average size of 12k tokens. Our code and data is available at this https URL.', 'abstract_zh': '我们介绍了PeerQA，这是一个现实世界中的科学文献级别的问答（QA）数据集。PeerQA的问题来源于同行评审，这些问题是评审人在详细审阅科学文章时提出的。这些问题的回答由每篇文章的原始作者进行标注。该数据集包含来自208篇学术论文的579个QA配对，大多数来自机器学习和自然语言处理领域，同时也包含来自地质科学和公共卫生等其他科学社区的部分论文。PeerQA支持开发实际QA系统中的三个关键任务：证据检索、无法回答的问题分类以及答案生成。我们对收集的数据集进行了详细的分析，并进行了实验以建立这三个任务的基线系统。我们的实验和分析揭示了在文献级别检索中需要去情境化的必要性，在此过程中，我们发现即使简单的去情境化方法也能在各种架构中持续提高检索性能。在答案生成方面，PeerQA 为长上下文建模提供了一个具有挑战性的基准，因为论文平均包含12k个标记。我们的代码和数据可在以下链接获得：this https URL。', 'title_zh': 'PeerQA：来自同行评审的科学问答数据集'}
{'arxiv_id': 'arXiv:2502.13656', 'title': 'Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models', 'authors': 'Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen', 'link': 'https://arxiv.org/abs/2502.13656', 'abstract': 'Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.', 'abstract_zh': '句子嵌入是许多自然语言处理（NLP）任务的基础，对比学习方法利用标注数据集（如语义一致性和矛盾性数据集，NLI）取得了优异性能。然而，这种方法对人工标签的高度依赖限制了其可扩展性。近期的研究利用大规模语言模型（LLMs）生成句子对，从而减少标注依赖。然而，这些方法忽视了对细微语义区分至关重要的排序信息。为了解决这一挑战，我们提出了一种方法，在潜在空间中控制LLMs的生成方向。与无约束生成不同，这种方法确保了有意义的语义差异。然后，我们通过整合排序信息和语义信息来细化现有的句子嵌入模型。在多个基准测试上的实验结果表明，我们的方法在合成排序句子方面成本较低，但仍能取得最新的最佳性能（SOTA）。', 'title_zh': '通过使用大型语言模型生成句子对进行排序以精炼句子嵌入模型'}
{'arxiv_id': 'arXiv:2502.13652', 'title': 'C2T: A Classifier-Based Tree Construction Method in Speculative Decoding', 'authors': 'Feiye Huo, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Shengli Sun', 'link': 'https://arxiv.org/abs/2502.13652', 'abstract': 'The growing scale of Large Language Models (LLMs) has exacerbated inference latency and computational costs. Speculative decoding methods, which aim to mitigate these issues, often face inefficiencies in the construction of token trees and the verification of candidate tokens. Existing strategies, including chain mode, static tree, and dynamic tree approaches, have limitations in accurately preparing candidate token trees for verification. We propose a novel method named C2T that adopts a lightweight classifier to generate and prune token trees dynamically. Our classifier considers additional feature variables beyond the commonly used joint probability to predict the confidence score for each draft token to determine whether it is the candidate token for verification. This method outperforms state-of-the-art (SOTA) methods such as EAGLE-2 on multiple benchmarks, by reducing the total number of candidate tokens by 25% while maintaining or even improving the acceptance length.', 'abstract_zh': '大型语言模型（LLMs）规模的不断扩大加剧了推理延迟和计算成本。推测解码方法旨在缓解这些问题，但在构建令牌树和验证候选令牌方面经常面临效率低下。现有的策略，包括链模式、静态树和动态树方法，在准确生成需要验证的候选令牌树方面存在局限性。我们提出了一种名为C2T的新方法，该方法采用轻量级分类器动态生成和修剪令牌树。我们的分类器不仅考虑常用联合概率等特征变量，还考虑其他特征变量来预测每个草稿令牌的信心评分，以确定其是否为需要验证的候选令牌。该方法在多个基准测试中优于现有最先进的方法（如EAGLE-2），在减少候选令牌总数25%的同时，能够保持甚至提高接受长度。', 'title_zh': 'C2T：推测解码中基于分类器的树构建方法'}
{'arxiv_id': 'arXiv:2502.13648', 'title': 'Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs', 'authors': 'Youna Kim, Minjoon Choi, Sungmin Cho, Hyuhng Joon Kim, Sang-goo Lee, Taeuk Kim', 'link': 'https://arxiv.org/abs/2502.13648', 'abstract': "Large Language Models (LLMs) enhance their problem-solving capability by leveraging both parametric and external knowledge. Beyond leveraging external knowledge to improve response accuracy, they require key capabilities for reliable knowledge-handling: resolving conflicts between knowledge sources, avoiding distraction from uninformative external knowledge, and abstaining when sufficient knowledge is unavailable. Prior studies have examined these scenarios in isolation or with limited scope. To systematically evaluate these capabilities, we introduce a comprehensive framework for analyzing knowledge-handling based on two key dimensions: the presence of parametric knowledge and the informativeness of external knowledge. Through analysis, we identify biases in knowledge utilization and examine how the ability to handle one scenario impacts performance in others. Furthermore, we demonstrate that training on data constructed based on the knowledge-handling scenarios improves LLMs' reliability in integrating and utilizing knowledge.", 'abstract_zh': '大型语言模型（LLMs）通过利用参数知识和外部知识来增强其问题解决能力。除了利用外部知识提高响应准确性外，它们还需要具备可靠的知识处理能力：解决不同知识来源之间的冲突、避免被不相关信息分散注意力，并在缺乏足够知识时保持沉默。以往研究在单一或有限的范围内探讨了这些场景。为了系统地评估这些能力，我们引入了一个基于两个关键维度的全面框架来分析知识处理能力：参数知识的存在和外部知识的相关性。通过分析，我们识别了知识利用中的偏差，并探讨了在一种场景中处理能力如何影响其他场景的性能。此外，我们还证明，基于知识处理场景构建的数据训练可以提高LLMs整合和利用知识的可靠性。', 'title_zh': '参数和外部知识下的可靠性：理解大语言模型中的知识处理'}
{'arxiv_id': 'arXiv:2502.13647', 'title': 'Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh', 'authors': 'Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto', 'link': 'https://arxiv.org/abs/2502.13647', 'abstract': "Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.", 'abstract_zh': '低资源语言的指令调优仍因文本数据有限而未得到充分探索，尤其是在政府和文化领域。为应对这一挑战，我们引入并开源了一个大规模（包含10,600样本）的指令遵循（Instruction-Following, IFT）数据集，涵盖了与哈萨克斯坦息息相关的关键机构和文化知识。该数据集增强了大规模语言模型（LLM）对程序性、法律法规和结构化治理主题的理解。我们采用LLM辅助数据生成方法，对比了开放权重和封闭权重模型在数据集构建中的表现，并选择了GPT-4o作为基础模型。数据集中每个实体都经过全面的手工验证，以确保质量。此外，我们还证明了在我们数据集上微调Qwen、Falcon和Gemma能够一致地提高多项选择和生成任务的性能，这表明LLM辅助指令调优对于低资源语言具有潜在价值。', 'title_zh': '面向低资源语言的公共政府与文化数据的指令调优：哈萨克语案例研究\n\n在这个翻译中，我确保了术语的专业性和准确性，同时保持了原文的学术规范。"Instruction Tuning" 被译为“指令调优”，“Public Government and Cultural Data” 被译为“公共政府与文化数据”，“Low-Resource Language” 被译为“低资源语言”，这些都是机器学习和自然语言处理领域的专业术语。同时，“Case Study” 被译为“案例研究”，这是一个常见的学术术语，适用于描述具体的实验或分析。'}
{'arxiv_id': 'arXiv:2502.13646', 'title': 'D.Va: Validate Your Demonstration First Before You Use It', 'authors': 'Qi Zhang, Zhiqing Xiao, Ruixuan Xiao, Lirong Gao, Junbo Zhao', 'link': 'https://arxiv.org/abs/2502.13646', 'abstract': "In-context learning (ICL) has demonstrated significant potential in enhancing the capabilities of large language models (LLMs) during inference. It's well-established that ICL heavily relies on selecting effective demonstrations to generate outputs that better align with the expected results. As for demonstration selection, previous approaches have typically relied on intuitive metrics to evaluate the effectiveness of demonstrations, which often results in limited robustness and poor cross-model generalization capabilities. To tackle these challenges, we propose a novel method, \\textbf{D}emonstration \\textbf{VA}lidation (\\textbf{this http URL}), which integrates a demonstration validation perspective into this field. By introducing the demonstration validation mechanism, our method effectively identifies demonstrations that are both effective and highly generalizable. \\textbf{this http URL} surpasses all existing demonstration selection techniques across both natural language understanding (NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate the robustness and generalizability of our approach across various language models with different retrieval models.", 'abstract_zh': '上下文学习（ICL）在增强大型语言模型（LLMs）推理能力方面展现了显著的潜力。已有研究确立了ICL高度依赖于选择有效的示例，以生成与预期结果更好的匹配输出。在示例选择方面，以往的方法通常依赖直观的指标来评估示例的有效性，但这往往导致了有限的鲁棒性和跨模型的一般化能力。为了解决这些问题，我们提出了一种新的方法，即**演示验证（Demonstration Validation, DV）**（请点击此链接: [this http URL]），该方法将演示验证视角整合进入这一领域。通过引入演示验证机制，我们的方法能够有效识别既有效又高度泛化的示例。**DV** 在自然语言理解（NLU）和自然语言生成（NLG）任务中均超越了所有现有的示例选择技术。此外，我们还展示了我们的方法在不同检索模型的多种语言模型上的鲁棒性和泛化能力。', 'title_zh': 'D.Va: 在使用演示之前，请先验证它。'}
{'arxiv_id': 'arXiv:2502.13645', 'title': 'Measuring the Effect of Transcription Noise on Downstream Language Understanding Tasks', 'authors': 'Ori Shapira, Shlomo E. Chazan, Amir DN Cohen', 'link': 'https://arxiv.org/abs/2502.13645', 'abstract': 'With the increasing prevalence of recorded human speech, spoken language understanding (SLU) is essential for its efficient processing. In order to process the speech, it is commonly transcribed using automatic speech recognition technology. This speech-to-text transition introduces errors into the transcripts, which subsequently propagate to downstream NLP tasks, such as dialogue summarization. While it is known that transcript noise affects downstream tasks, a systematic approach to analyzing its effects across different noise severities and types has not been addressed. We propose a configurable framework for assessing task models in diverse noisy settings, and for examining the impact of transcript-cleaning techniques. The framework facilitates the investigation of task model behavior, which can in turn support the development of effective SLU solutions. We exemplify the utility of our framework on three SLU tasks and four task models, offering insights regarding the effect of transcript noise on tasks in general and models in particular. For instance, we find that task models can tolerate a certain level of noise, and are affected differently by the types of errors in the transcript.', 'abstract_zh': '随着录音人类语音的普遍增多，口头语言理解（SLU）对于其高效处理变得至关重要。为了处理语音，通常会使用自动语音识别技术将其转换为文本。这一语音到文本的转换引入了错误，这些错误随后传播到下游自然语言处理（NLP）任务，如对话总结等。虽然已知转录中的噪声会干扰下游任务，但如何系统地分析其在不同噪声严重程度和类型下的影响尚未得到解决。我们提出了一个可配置框架，用于在各种噪声环境中评估任务模型，并考察转录清理技术的影响。该框架有助于研究任务模型的行为，从而支持开发有效的SLU解决方案。我们通过三个SLU任务和四种任务模型例证了该框架的应用价值，提供了关于噪声对任务和模型影响的一般性和具体性见解。例如，我们发现任务模型可以容忍一定程度的噪声，但不同类型的转录错误会对任务模型产生不同的影响。', 'title_zh': '测量转录噪声对下游语言理解任务的影响'}
{'arxiv_id': 'arXiv:2502.13640', 'title': 'Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts', 'authors': 'Maiya Goloburda, Nurkhan Laiyk, Diana Turmakhan, Yuxia Wang, Mukhammed Togmanov, Jonibek Mansurov, Askhat Sametov, Nurdaulet Mukhituly, Minghan Wang, Daniil Orel, Zain Muhammad Mujahid, Fajri Koto, Timothy Baldwin, Preslav Nakov', 'link': 'https://arxiv.org/abs/2502.13640', 'abstract': 'Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased.', 'abstract_zh': '大型语言模型（LLMs）具有生成有害内容的潜在风险，这给用户带来了安全威胁。尽管在开发LLM风险分类和安全性评估提示方面取得了显著进展，大多数研究主要集中在单一语言环境中，特别是英语中。然而，在双语环境中，语言和地区特有的风险经常被忽视，并且核心发现可能与单一语言环境下的发现有所不同。本文介绍了Qorgau，一个专门为卡斯哈克语和俄语设计的新数据集，反映了卡斯哈克斯坦独特的双语环境，在该国同时使用卡斯哈克语（资源较少的语言）和俄语（资源丰富语言）。实验结果显示，多语言和特定语言的LLM在安全性性能方面存在显著差异，强调了为了确保在像卡斯哈克斯坦这样的国家中负责任和安全地部署LLM而需要定制化和地区特定的数据集的重要性。警告：本文包含可能会令人不适、有害或偏颇的示例数据。', 'title_zh': 'Qorgau：评估哈萨克-俄语双语背景下的大型语言模型安全性'}
{'arxiv_id': 'arXiv:2502.13628', 'title': 'Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection', 'authors': 'Darpan Aswal, Manjira Sinha', 'link': 'https://arxiv.org/abs/2502.13628', 'abstract': 'Transformer-based models dominate NLP tasks like sentiment analysis, machine translation, and claim verification. However, their massive computational demands and lack of interpretability pose challenges for real-world applications requiring efficiency and transparency. In this work, we explore Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives for Environmental Claim Detection, reframing it as a graph classification problem. We construct dependency parsing graphs to explicitly model syntactic structures, using simple word embeddings (word2vec) for node features with dependency relations encoded as edge features. Our results demonstrate that these graph-based models achieve comparable or superior performance to state-of-the-art transformers while using 30x fewer parameters. This efficiency highlights the potential of structured, interpretable, and computationally efficient graph-based approaches.', 'abstract_zh': '基于Transformer的模型在情感分析、机器翻译和声明验证等自然语言处理任务中占据了主导地位。然而，它们巨大的计算需求及其缺乏可解释性为需要高效性和透明性的实际应用带来了挑战。本研究探索了图神经网络（GNNs）和双曲图神经网络（HGNNs）作为环境声明检测的轻量且有效的替代方案，将该任务重新定义为图分类问题。我们构建了依赖解析图以明确建模句法结构，并使用简单的词嵌入（如word2vec）作为节点特征，将依赖关系作为边特征进行编码。结果显示，这些图基模型在参数数量减少30倍的情况下，达到了与最先进的Transformer相当或更好的性能。这种效率突显了结构化、可解释性和计算高效的图基方法的潜力。', 'title_zh': '使用双曲图神经网络进行非欧几里得分层表示学习以赋能环境索赔检测'}
{'arxiv_id': 'arXiv:2502.13622', 'title': 'REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models', 'authors': 'DongGeon Lee, Hwanjo Yu', 'link': 'https://arxiv.org/abs/2502.13622', 'abstract': 'Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.', 'abstract_zh': '大型语言模型（LLM）输出中的幻觉严重限制了它们在知识密集型任务（如问答）中的可靠性。为解决这一挑战，我们引入了REFIND（Retrieval-augmented Factuality hallucINation Detection）框架，通过直接利用检索到的文档来检测LLM输出中的幻觉段落。作为REFIND的一部分，我们提出了上下文敏感性比率（CSR，Context Sensitivity Ratio），这是一种新颖的度量方法，用于量化LLM输出对检索到的证据的敏感性。这种创新的方法使得REFIND能够高效且准确地检测幻觉，使其区别于现有的方法。在评估中，REFIND在九种语言中表现出色，包括资源有限的设置，并显著优于基线模型，在识别幻觉段落方面获得了更高的IoU分数。本研究强调了量化上下文敏感性对于幻觉检测的有效性，从而为各种语言中的更可靠和可信赖的LLM应用开辟了新的途径。', 'title_zh': 'REFIND：大型语言模型中的检索增强事实幻觉检测'}
{'arxiv_id': 'arXiv:2502.13619', 'title': 'Complex Ontology Matching with Large Language Model Embeddings', 'authors': 'Guilherme Sousa, Rinaldo Lima, Cassia Trojahn', 'link': 'https://arxiv.org/abs/2502.13619', 'abstract': 'Ontology, and more broadly, Knowledge Graph Matching is a challenging task in which expressiveness has not been fully addressed. Despite the increasing use of embeddings and language models for this task, approaches for generating expressive correspondences still do not take full advantage of these models, in particular, large language models (LLMs). This paper proposes to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The generation of correspondences is performed by matching similar surroundings of instance sub-graphs. The integration of LLMs results in different architectural modifications, including label similarity, sub-graph matching, and entity matching. The performance word embeddings, sentence embeddings, and LLM-based embeddings, was compared. The results demonstrate that integrating LLMs surpasses all other models, enhancing the baseline version of the approach with a 45\\% increase in F-measure.', 'abstract_zh': '本论文的主题是概念匹配，更广泛地说，知识图谱匹配是一项具有挑战性的任务，其中表达能力尚未得到充分解决。尽管嵌入表示和语言模型在这项任务中的应用日益增多，但仍缺乏充分利用这些模型，特别是在大语言模型（LLMs）方面的利用。本文提出了一种将LLMs整合进基于对齐需求和ABox关系发现生成具有表现力的对应关系的方法中。通过匹配实例子图的相似环境来生成对应关系。将LLMs整合后，产生了不同的架构修改，包括标签相似性、子图匹配和实体匹配。还对比了词嵌入、句子嵌入和基于LLM的嵌入的表现。结果表明，将LLMs整合进方法中显著超越了所有其他模型，baselines方法的F-measure提高了45%。', 'title_zh': '使用大规模语言模型嵌入进行复杂的本体匹配'}
{'arxiv_id': 'arXiv:2502.13604', 'title': 'BeamLoRA: Beam-Constraint Low-Rank Adaptation', 'authors': 'Naibin Gu, Zhenyu Zhang, Xiyu Liu, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang', 'link': 'https://arxiv.org/abs/2502.13604', 'abstract': 'Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.', 'abstract_zh': '为了适应大规模语言模型高效微调的需求，低秩适应（LoRA）已成为最有效的参数高效微调方法之一。尽管LoRA在提高效率方面取得了显著进展，但在准确性方面仍有改进空间。在此基础上，我们从一个新颖的角度评估了LoRA秩的特性。研究结果表明，LoRA模块内的不同秩不仅具有不同的重要性水平，而且在整个微调过程中还会表现出动态变化，这可能会限制LoRA的性能。基于这些发现，我们提出了BeamLoRA，其将每个LoRA模块视作一个束，其中每个秩自然对应于潜在的子解，微调过程则成为寻找最优子解组合的搜索过程。BeamLoRA动态地消除了表现不佳的子解，同时扩大了有希望的子解的参数空间，从而在固定秩的情况下提高了性能。广泛的实验结果覆盖了三个基础模型和12个数据集，涵盖数学推理、代码生成和常识推理等多个领域，显示BeamLoRA能够一贯地提高LoRA的性能，超越了其他基准方法。', 'title_zh': 'BeamLoRA：带束约束的低秩适应'}
{'arxiv_id': 'arXiv:2502.13603', 'title': 'Efficient Safety Retrofitting Against Jailbreaking for LLMs', 'authors': 'Dario Garcia-Gasulla, Anna Arias-Duart, Adrian Tormos, Daniel Hinjos, Oscar Molina-Sedano, Ashwin Kumar Gururajan, Maria Eugenia Cardello', 'link': 'https://arxiv.org/abs/2502.13603', 'abstract': "Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research.", 'abstract_zh': '直接偏好优化（DPO）是一种高效对齐技术，通过在偏好数据上进行训练，引导大语言模型（LLM）生成更受欢迎的输出，从而避免了显式奖励模型的需求。它的简洁性使其易于适应各种领域和安全要求。本文探讨了DPO在对抗牢笼破解攻击方面对模型安全性的有效性，同时尽量减少数据需求和训练成本。我们引入了Egida，一个源自多个数据源的扩展数据集，其中包括27个不同的安全主题和18种不同的攻击类型，同时还包含合成和人类标签。这些数据被用于提升最先进的大语言模型（Llama-3.1-8B/70B-指令型、Qwen-2.5-7B/72B-指令型）在各个主题和攻击类型下的安全性。此外，我们还评估了它们在通用任务上的性能下降情况，以及它们在拒绝过度方面的倾向。按照提出的的方法论，经过训练的模型在使用少量样本（2,000 个）并具有较低计算成本（8B 模型为3元，72B 模型为20元）的情况下，将攻击成功率降低了10%-30%。安全对齐的模型能够泛化到未见过的主题和攻击类型，最成功的攻击类型的成功率约为5%。模型的大小和家族对其向安全性的可塑性影响巨大，表明预训练选择的重要性。为验证我们的发现，作者进行了一项独立的人类偏好一致性评估，并公开了与Llama-Guard-3-8B相关联的数据集Egida-HSafe。总体而言，本文展示了使用DPO增强大语言模型安全性的成本效益和可行性，同时也指出了其当前的局限性。所有数据集和模型均已公开，以便实现可重现性并促进进一步的研究。', 'title_zh': '针对LLMs的高效安全性加固以对抗克隆ware攻击'}
{'arxiv_id': 'arXiv:2502.13595', 'title': 'MMTEB: Massive Multilingual Text Embedding Benchmark', 'authors': 'Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff', 'link': 'https://arxiv.org/abs/2502.13595', 'abstract': 'Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.', 'abstract_zh': '文本嵌入通常仅在有限的任务集上进行评估，这些任务集受语言、领域和任务多样性的限制。为了解决这些限制并提供更全面的评估，我们引入了大规模多语言文本嵌入基准（MMTEB）——这是一个由社区推动的大规模扩展，涵盖了250多种语言中超过500个质量控制的任务。MMTEB包括一系列具有挑战性的新型任务，如指令跟随、长文档检索和代码检索，这是迄今为止涵盖嵌入模型评估任务最大的多语言集合。利用这一集合，我们开发了多个高度多语言基准，以此来评估代表性模型。我们发现，尽管具有数十亿参数的大型语言模型（LLMs）在某些语言子集和任务类别中可以达到最先进的性能，但性能最佳的公开可用模型是具有5.6亿参数的multilingual-e5-large-instruct。为了提高可访问性并降低计算成本，我们提出了一种基于任务间相关性的新颖降采样方法，确保选择的多样性同时保持相对模型排名的稳定性。此外，我们通过选择硬负例对检索任务进行了优化，从而创建了较小但有效的拆分。这些优化使我们能够提出一些大幅降低计算需求的基准。例如，我们新引入的零样本英语基准在计算成本大幅减少的情况下，仍然保持与全规模版本相似的排名顺序。', 'title_zh': 'MMTEB: 大规模多语言文本嵌入基准'}
{'arxiv_id': 'arXiv:2502.13592', 'title': "Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints", 'authors': 'Nicolò Penzo, Marco Guerini, Bruno Lepri, Goran Glavaš, Sara Tonelli', 'link': 'https://arxiv.org/abs/2502.13592', 'abstract': "Multi-Party Conversations (MPCs) are widely studied across disciplines, with social media as a primary data source due to their accessibility. However, these datasets raise privacy concerns and often reflect platform-specific properties. For example, interactions between speakers may be limited due to rigid platform structures (e.g., threads, tree-like discussions), which yield overly simplistic interaction patterns (e.g., as a consequence of ``reply-to'' links). This work explores the feasibility of generating diverse MPCs with instruction-tuned Large Language Models (LLMs) by providing deterministic constraints such as dialogue structure and participants' stance. We investigate two complementary strategies of leveraging LLMs in this context: (i.) LLMs as MPC generators, where we task the LLM to generate a whole MPC at once and (ii.) LLMs as MPC parties, where the LLM generates one turn of the conversation at a time, provided the conversation history. We next introduce an analytical framework to evaluate compliance with the constraints, content quality, and interaction complexity for both strategies. Finally, we assess the quality of obtained MPCs via human annotation and LLM-as-a-judge evaluations. We find stark differences among LLMs, with only some being able to generate high-quality MPCs. We also find that turn-by-turn generation yields better conformance to constraints and higher linguistic variability than generating MPCs in one pass. Nonetheless, our structural and qualitative evaluation indicates that both generation strategies can yield high-quality MPCs.", 'abstract_zh': '多party对话（MPCs）在跨学科领域得到了广泛研究，社交媒体因其可访问性成为了主要的数据来源。然而，这些数据集引发了隐私问题，并且常常反映出特定平台的特性。例如，由于平台结构的严格性（如帖子、树状讨论等），发言者之间的互动可能会受到限制，从而导致过于简单的交互模式（例如，由于“回复链接”机制）。本研究探讨了通过提供确定性约束（如对话结构和参与者立场）来使用指令调整的大语言模型（LLMs）生成多样化MPCs的可行性。我们在此情境中研究了两个互补的LLMs利用策略：（i）将LLMs作为MPC生成器，要求LLMs一次性生成整个MPC；（ii）将LLMs作为MPC参与者，根据对话历史逐步生成对话的每一回合。接下来，我们引入了一种分析框架来评估这两种策略在遵守约束、内容质量和交互复杂性方面的符合程度。最后，我们通过人工标注和LLM作为评判者的方式评估所获得的MPCs的质量。我们发现不同LLMs之间存在显著差异，只有部分LLMs能够生成高质量的MPCs。我们还发现，逐回合生成相比于一次性生成MPCs，在遵守约束和语言多样性方面表现更好。然而，我们的结构和定性评估表明，这两种生成策略都能够生成高质量的MPCs。', 'title_zh': '不要停止多方对话！关于带有约束条件生成合成多方对话的研究'}
{'arxiv_id': 'arXiv:2502.13566', 'title': 'Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs', 'authors': 'Joonatan Laato, Jenna Kanerva, John Loehr, Virpi Lummaa, Filip Ginter', 'link': 'https://arxiv.org/abs/2502.13566', 'abstract': 'We performed a zero-shot information extraction study on a historical collection of 89,339 brief Finnish-language interviews of refugee families relocated post-WWII from Finnish Eastern Karelia. Our research objective is two-fold. First, we aim to extract social organizations and hobbies from the free text of the interviews, separately for each family member. These can act as a proxy variable indicating the degree of social integration of refugees in their new environment. Second, we aim to evaluate several alternative ways to approach this task, comparing a number of generative models and a supervised learning approach, to gain a broader insight into the relative merits of these different approaches and their applicability in similar studies.\nWe find that the best generative model (GPT-4) is roughly on par with human performance, at an F-score of 88.8%. Interestingly, the best open generative model (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7% F-score, demonstrating that open models are becoming a viable alternative for some practical tasks even on non-English data. Additionally, we test a supervised learning alternative, where we fine-tune a Finnish BERT model (FinBERT) using GPT-4 generated training data. By this method, we achieved an F-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k interviews. Such an approach would be particularly appealing in cases where the computational resources are limited, or there is a substantial mass of data to process.', 'abstract_zh': '我们对二战后从芬兰东方卡累利阿迁出的89,339份难民家庭简短访谈进行了零样本信息提取研究。本研究有两个目标。首先，我们旨在从访谈的自由文本中提取每个家庭成员的社会组织和爱好，作为难民在新环境中社会融入程度的代理变量。其次，我们尝试了多种不同的方法来完成这一任务，比较了几种生成模型和监督学习方法，以深入了解这些不同方法的相对优势及其在类似研究中的适用性。\n\n我们的研究表明，最佳生成模型（GPT-4）的F分数为88.8%，表现大致与人类水平相当。有趣的是，最好的开源生成模型（Llama-3-70B-Instruct）的表现也非常接近，F分数为87.7%，这表明即使在非英语数据上，开源模型也开始成为一些实际任务的可行性替代方案。此外，我们还测试了一种监督学习的替代方法，使用GPT-4生成的数据对芬兰BERT模型（FinBERT）进行微调。通过这种方法，我们已经在6,000份访谈中实现了84.1%的F分数，并且随着访谈数量增加到30,000份，F分数提高到了86.3%。在计算资源有限或需要处理大量数据的情况下，这样的方法尤其具有吸引力。', 'title_zh': '使用大型语言模型从芬兰卡拉利难民访谈中提取社会联系'}
{'arxiv_id': 'arXiv:2502.13564', 'title': 'PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models', 'authors': 'Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei', 'link': 'https://arxiv.org/abs/2502.13564', 'abstract': "The rapid development of large language models (LLMs) is redefining the landscape of human-computer interaction, and their integration into various user-service applications is becoming increasingly prevalent. However, transmitting user data to cloud-based LLMs presents significant risks of data breaches and unauthorized access to personal identification information. In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios. We construct SensitiveQA, the first privacy open-ended question-answering dataset. It comprises 57k interactions in Chinese and English, encompassing a diverse range of user-sensitive information within the conversations. Our proposed solution employs a multi-stage strategy aimed at preemptively securing user information while simultaneously preserving the response quality of cloud-based LLMs. Experimental validation underscores our method's efficacy in balancing privacy protection with maintaining robust interaction quality. The code and dataset are available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）的快速发展正在重新定义人机交互的格局，并使其在各种用户服务应用中的集成越来越普遍。然而，将用户数据传输到基于云的LLMs带来了数据泄露和未经授权访问个人身份信息的重大风险。本文提出了一个隐私保护管道，旨在在实际使用大型语言模型的场景中保护用户和敏感信息的隐私。我们构建了SensitiveQA，这是首个开放性隐私问答数据集，包含57,000个中英双语的交互，涵盖了对话中的多种用户敏感信息。我们提出的方法采用多阶段策略，旨在预先保护用户信息的同时，同时保持基于云的LLMs响应质量。实验验证表明，我们的方法能够在保护隐私的同时维持稳健的交互质量。完整的代码和数据集可在此处访问：[提供链接]。', 'title_zh': 'PRIV-QA：面向云大规模语言模型的隐私保护问答系统'}
{'arxiv_id': 'arXiv:2502.13550', 'title': 'STaR-SQL: Self-Taught Reasoner for Text-to-SQL', 'authors': 'Mingqian He, Yongliang Shen, Wenqi Zhang, Qiuying Peng, Jun Wang, Weiming Lu', 'link': 'https://arxiv.org/abs/2502.13550', 'abstract': 'Generating step-by-step "chain-of-thought" rationales has proven effective for improving the performance of large language models on complex reasoning tasks. However, applying such techniques to structured tasks, such as text-to-SQL, remains largely unexplored. In this paper, we introduce Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes SQL query generation as a reasoning-driven process. Our method prompts the LLM to produce detailed reasoning steps for SQL queries and fine-tunes it on rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL dedicates additional test-time computation to reasoning, thereby positioning LLMs as spontaneous reasoners rather than mere prompt-based agents. To further scale the inference process, we incorporate an outcome-supervised reward model (ORM) as a verifier, which enhances SQL query accuracy. Experimental results on the challenging Spider benchmark demonstrate that STaR-SQL significantly improves text-to-SQL performance, achieving an execution accuracy of 86.6%. This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to predict answers directly by 18.0%. Additionally, STaR-SQL outperforms agent-like prompting methods that leverage more powerful yet closed-source models such as GPT-4. These findings underscore the potential of reasoning-augmented training for structured tasks and open the door to extending self-improving reasoning models to text-to-SQL generation and beyond.', 'abstract_zh': '生成逐步的“推理链”解释已被证明有助于提高大型语言模型在复杂推理任务上的性能。然而，将此类技术应用于结构化任务（如文本到SQL查询）方面仍存在大量未探索的空间。在本论文中，我们引入了一种名为Self-Taught Reasoner for text-to-SQL（STaR-SQL）的新颖方法，它将SQL查询生成重新构想为一个基于推理的过程。我们的方法促使大型语言模型（LLM）生成详细的SQL查询推理步骤，并在正确结果导向的解释上进行微调。与传统方法不同，STaR-SQL在推理过程中分配额外的测试时计算资源，从而将LLM定位为自发的推理者而非仅依赖于提示的代理。为了进一步扩大推理过程的规模，我们引入了一个基于结果监督的奖励模型（ORM）作为验证器，它提高了SQL查询的准确性。在考验性的Spider基准测试中，STaR-SQL在文本到SQL任务上的性能显著提升，实现了86.6%的执行准确率。这超过了一次性示例基线31.6%，也超过了专门微调以直接预测答案的基线微调模型18.0%。此外，STaR-SQL还超越了使用更强大闭源模型如GPT-4的其他代理式提示方法。这些发现强调了增强推理训练在结构化任务中的潜力，并为扩展自我改进的推理模型至文本到SQL生成等更广泛的领域打开了大门。', 'title_zh': 'STaR-SQL：自我学习推理器用于文本到SQL转换'}
{'arxiv_id': 'arXiv:2502.13548', 'title': 'Detecting Linguistic Bias in Government Documents Using Large language Models', 'authors': 'Milena de Swart, Floris den Hengst, Jieying Chen', 'link': 'https://arxiv.org/abs/2502.13548', 'abstract': "This paper addresses the critical need for detecting bias in government documents, an underexplored area with significant implications for governance. Existing methodologies often overlook the unique context and far-reaching impacts of governmental documents, potentially obscuring embedded biases that shape public policy and citizen-government interactions. To bridge this gap, we introduce the Dutch Government Data for Bias Detection (DGDB), a dataset sourced from the Dutch House of Representatives and annotated for bias by experts. We fine-tune several BERT-based models on this dataset and compare their performance with that of generative language models. Additionally, we conduct a comprehensive error analysis that includes explanations of the models' predictions. Our findings demonstrate that fine-tuned models achieve strong performance and significantly outperform generative language models, indicating the effectiveness of DGDB for bias detection. This work underscores the importance of labeled datasets for bias detection in various languages and contributes to more equitable governance practices.", 'abstract_zh': '本文探讨了在政府文件中检测偏见的迫切需求，这是一个尚未充分探索但具有重要治理意义的领域。现有的方法论往往忽略了政府文件的独特背景及其深远影响，这可能掩盖了嵌入的偏见，这些偏见塑造了公共政策和公民-政府互动。为弥补这一空白，我们引入了“荷兰政府数据用于偏见检测”（DGDB）数据集，该数据集来源于荷兰议会，并由专家进行了偏见标注。我们对几种基于BERT的模型进行了微调，并将其性能与生成型语言模型进行了比较。此外，我们还进行了全面的错误分析，包括模型预测的解释。研究结果表明，微调后的模型实现了强大的性能，并显著优于生成型语言模型，这表明DGDB对于偏见检测的有效性。本项研究突出了在不同语言中使用标注数据集对于偏见检测的重要性，并为更公平的治理实践做出了贡献。', 'title_zh': '使用大型语言模型检测政府文件中的语言偏见'}
{'arxiv_id': 'arXiv:2502.13544', 'title': 'From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN', 'authors': 'Peiwen Yuan, Chuyi Tan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.13544', 'abstract': 'Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further this http URL bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error this http URL this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content this http URL experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.', 'abstract_zh': '尽管大型语言模型（LLMs）取得了快速进展，但它们控制长度的文本生成（LCTG）能力仍然不令人满意，这对其实用应用构成了重大限制。现有方法主要侧重于端到端训练以加强遵守长度约束。然而，LCTG子能力的分解不足和针对性增强限制了这一进程。为了弥合这一差距，我们从人类模式的角度进行自底向上的LCTG子能力分解，并进行了详细的错误分析。基于这些分析，我们提出了一个简单而有效的插件式方法——MarkerGen：(1) 通过外部工具集成来缓解LLM的基本缺陷；(2) 通过动态插入标记进行显式的长度建模；(3) 采用三阶段生成方案以更好地适应长度约束并在保持内容完整性的同时提高生成质量。实验结果表明，MarkerGen 在不同场景下显著提高了LCTG能力，展现出卓越的有效性和普适性。', 'title_zh': '从亚能力诊断到与人类对齐的生成：通过MARKERGEN缩小文本长度控制的差距'}
{'arxiv_id': 'arXiv:2502.13542', 'title': 'Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference', 'authors': 'Qingfa Xiao, Jiachuan Wang, Haoyang Li, Cheng Deng, Jiaqi Tang, Shuangyin Li, Yongqi Zhang, Jun Wang, Lei Chen', 'link': 'https://arxiv.org/abs/2502.13542', 'abstract': 'Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \\textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as probe-Query in each sliding window to effectively represent the entire context, which is an approach overlooked by existing methods. Thus, we propose \\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that dynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the relevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of probe-Query for retrieval at pre-filling stage. To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency.', 'abstract_zh': '近年来，大语言模型（LLMs）在长上下文任务中的表现极为出色，但在有限的GPU内存下，推理效率方面面临着重大挑战。现有解决方案首先提出了滑动窗口方法，通过累积一个历史的键-值（KV）对集以供重复使用，随后进一步改进方案在每一步选择性地保留其子集。然而，由于长上下文中的注意力分布稀疏，难以识别和回忆相关的KV对，因为注意力容易被大量候选对所干扰。此外，我们发现，在每个滑动窗口中选择代表性标记作为探针查询以有效地代表整个上下文是有希望的方法，而这一方法在现有方法中被忽略了。因此，我们提出了ActQKV方法，这是一种无需训练、基于激活的动态方法，能够确定探针查询，并利用它在推理阶段检索相关的键-值对。具体而言，ActQKV在每个上下文窗口中监控一个标记级指标——激活偏差（Activation Bias），从而在预填充阶段实现探针查询的适当地构建。为了准确地回忆相关的键-值对并最小化无关的键-值对，我们在解码阶段通过跨层信息密度的引导设计了一种动态的键-值对截止机制。在Long-Bench和$\\infty$ Benchmarks上的实验表明，ActQKV方法在推理质量和资源效率方面达到了最先进的性能。', 'title_zh': '基于激活感知的探针查询：有效的大语言模型长上下文键值检索推理'}
{'arxiv_id': 'arXiv:2502.13520', 'title': 'A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment', 'authors': 'Khalid N. Elmadani, Nizar Habash, Hanada Taha-Thomure', 'link': 'https://arxiv.org/abs/2502.13520', 'abstract': 'This paper introduces the Balanced Arabic Readability Evaluation Corpus BAREC, a large-scale, fine-grained dataset for Arabic readability assessment. BAREC consists of 68,182 sentences spanning 1+ million words, carefully curated to cover 19 readability levels, from kindergarten to postgraduate comprehension. The corpus balances genre diversity, topical coverage, and target audiences, offering a comprehensive resource for evaluating Arabic text complexity. The corpus was fully manually annotated by a large team of annotators. The average pairwise inter-annotator agreement, measured by Quadratic Weighted Kappa, is 81.3%, reflecting a high level of substantial agreement. Beyond presenting the corpus, we benchmark automatic readability assessment across different granularity levels, comparing a range of techniques. Our results highlight the challenges and opportunities in Arabic readability modeling, demonstrating competitive performance across various methods. To support research and education, we will make BAREC openly available, along with detailed annotation guidelines and benchmark results.', 'abstract_zh': '本文介绍了平衡阿拉伯文本易读性评估语料库（BAREC），这是一个大规模、细粒度的数据集，用于阿拉伯文本易读性评估。BAREC 包含68,182个句子，总计超过100万词，经过精心筛选，涵盖了从幼儿园到研究生水平的19个易读性等级。该语料库平衡了文体多样性、主题覆盖面和目标受众，为评估阿拉伯文本复杂度提供了全面的资源。语料库由一个庞大的标注团队完全手工标注。通过Quadratic Weighted Kappa度量的一致性分析显示，平均双标注者两两之间的一致率为81.3%，反映了高实质一致性水平。除了介绍该语料库外，我们还对其在不同粒度水平上的自动易读性评估进行了基准测试，并比较了多种技术。我们的结果显示，阿拉伯文本易读性建模面临挑战和机遇，各种方法均表现出竞争力。为了支持研究和教育，我们将BAREC完全公开，并提供详细的标注指南和基准测试结果。', 'title_zh': '大规模且均衡的语料库用于阿拉伯语精细可读性评估'}
{'arxiv_id': 'arXiv:2502.13514', 'title': 'Shall Your Data Strategy Work? Perform a Swift Study', 'authors': 'Minlong Peng, Jingyi Yang, Zhongjun He, Hua Wu', 'link': 'https://arxiv.org/abs/2502.13514', 'abstract': 'This work presents a swift method to assess the efficacy of particular types of instruction-tuning data, utilizing just a handful of probe examples and eliminating the need for model retraining. This method employs the idea of gradient-based data influence estimation, analyzing the gradient projections of probe examples from the chosen strategy onto evaluation examples to assess its advantages. Building upon this method, we conducted three swift studies to investigate the potential of Chain-of-thought (CoT) data, query clarification data, and response evaluation data in enhancing model generalization. Subsequently, we embarked on a validation study to corroborate the findings of these swift studies. In this validation study, we developed training datasets tailored to each studied strategy and compared model performance with and without the use of these datasets. The results of the validation study aligned with the findings of the swift studies, validating the efficacy of our proposed method.', 'abstract_zh': '本研究提出了一种快速评估特定类型指令调整数据效果的方法，仅需少量探针示例即可实现，无需重新训练模型。该方法采用基于梯度的数据影响估计理念，通过分析所选策略下的探针示例在评估示例上的梯度投影来评估其优势。在此基础上，我们进行了三项快速研究，探讨链式思考数据、查询澄清数据和响应评估数据在提升模型泛化能力方面的潜力。随后，我们进行了一个验证研究，以验证上述快速研究的发现。在验证研究中，我们为每种研究策略开发了定制化的训练数据集，并将带有和不带有这些数据集的模型性能进行了比较。验证研究的结果与快速研究的发现一致，验证了我们提出方法的有效性。', 'title_zh': '你的数据策略有效吗？快速评估方法'}
{'arxiv_id': 'arXiv:2502.13509', 'title': 'Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion', 'authors': 'Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang', 'link': 'https://arxiv.org/abs/2502.13509', 'abstract': 'Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.', 'abstract_zh': '大型语言模型（LLMs）在视觉语言任务中展现了卓越的性能，但在医疗领域的应用仍然相对未被充分探索，尤其是在将结构化时间序列数据与非结构化临床笔记数据相结合方面。在临床实践中，动态时间序列数据（如实验室检测结果）捕捉到重要的时间模式，而临床笔记则提供了丰富的语义背景。由于连续信号与离散文本之间固有的差异，将这些模态融合在一起极具挑战性。为了解决这一问题，我们提出了一种名为ProMedTS的新型自监督多模态框架，该框架利用提示引导学习统一这些异构数据类型。我们的方法利用轻量级异常检测生成异常描述，这些描述作为提示，指导原始时间序列数据被编码成具信息性的嵌入。这些嵌入在共享的潜在空间中与文本表示对齐，同时保留了细微的时间动态特征及语义洞察。此外，我们的框架还融入了定制的自监督目标，以增强跨模态和跨模态内的对齐。我们在真实世界数据集上对ProMedTS进行了疾病诊断任务的评估，结果表明，我们的方法在所有评估指标上都优于最先进的方法。', 'title_zh': '《解锁多模态集成在电子健康记录中的潜力：一种语言和时间序列融合的提示学习框架》'}
{'arxiv_id': 'arXiv:2502.13502', 'title': 'PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference', 'authors': 'Burc Gokden', 'link': 'https://arxiv.org/abs/2502.13502', 'abstract': 'We show that Large Language Model from Power Law Decoder Representations (PLDR-LLM) is a foundational model whose deductive outputs are invariant tensors up to a small perturbation. PLDR-LLM learns a singularity condition for the deductive outputs that enable the once-inferred energy-curvature tensor $\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph attention (PLGA) generating the deductive outputs at inference. We demonstrate that a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in a straightforward manner to improve the inference time. The invariance and generalizable nature of deductive outputs is at a very high fidelity where deductive outputs have same RMSE and determinant values up to 15 decimal places after caching, and zero-shot benchmark scores remain unchanged. Ablation studies show that learned deductive outputs have distinct loss and accuracy characteristics from models pretrained with transferred, randomly initialized or identity tensors as a constant tensor operator and an LLM with scaled-dot product attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$ is predefined as identity. The observed invariance characteristic introduces a novel asymmetry between training and inference phases with caching. We outline observed common characteristics of the deductive outputs for the learned singularity condition. We provide an implementation of a training and inference framework for PLDR-LLM with KV-cache and G-cache.', 'abstract_zh': '我们展示了Power Law Decoder Representations (PLDR)的大语言模型（PLDR-LLM）是一个基础模型，其演绎输出在轻微扰动下是不变的张量。PLDR-LLM 学习了一个演绎输出的奇点条件，使得在推理过程中，通过Power Law Graph Attention (PLGA) 生成的演绎输出的能量曲率张量 $\\mathbf{G}_{LM}$ 能够被缓存起来替代。我们证明了 $\\mathbf{G}_{LM}$ (G缓存) 和密钥值缓存(KV缓存) 的实现可以以简单的方式提高推理时间。演绎输出的不变性和普适性质具有极高的保真度，其中在缓存后，RMSE和行列式值在15位小数点后依然相同，零样本基准测试分数也没有改变。消融研究显示，学习到的演绎输出具有与使用传递、随机初始化或恒等张量预训练模型不同的损失和准确率特性。一种特殊的 PLDR-LLM 是具有缩放点积注意机制 (SDPA) 的大语言模型，其中 $\\mathbf{G}_{LM}$ 已预定义为恒等张量。观察到的不变性特征在训练和推理阶段引入了一种新的不对称性，并且随着缓存的存在而显现。我们概述了学习奇点条件下的演绎输出的常见特性。我们还提供了一个带有密钥值缓存和G缓存的PLDR-LLM的训练和推理框架实现。', 'title_zh': 'PLDR-LLMs 学习一个可泛化的张量运算器，该运算器可以在推理时替代其自身的深度神经网络'}
{'arxiv_id': 'arXiv:2502.13497', 'title': 'Towards Geo-Culturally Grounded LLM Generations', 'authors': 'Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin, Sunipa Dev', 'link': 'https://arxiv.org/abs/2502.13497', 'abstract': "Generative large language models (LLMs) have been demonstrated to have gaps in diverse, cultural knowledge across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on the ability of LLMs to display familiarity with a diverse range of national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on a series of cultural familiarity benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., the norms, artifacts, and institutions of national cultures), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models, while failing to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional knowledge about a culture and open-ended cultural fluency when it comes to evaluating the cultural familiarity of generative LLMs.", 'abstract_zh': '生成型大型语言模型（LLMs）在全球范围内显示出在多元文化知识方面存在差距。我们研究了检索增强生成和搜索引擎强化技术对LLMs展示对多样化国家文化的熟悉程度能力的影响。具体来说，我们在一系列文化熟悉性基准测试中比较了标准LLMs、配备从定制知识库检索（即KB接地）的LLMs以及从网页检索增强的LLMs（即搜索引擎强化）的表现。我们发现，搜索引擎强化显著提高了LLMs在测试命题知识（如国家文化的规范、器物和制度）的多项选择基准测试中的表现，而知识库接地的效果受到知识覆盖范围不完整和检索器不理想的限制。然而，搜索引擎强化也增加了语言模型产生刻板印象判断的风险，而未能在充分统计功效的人类评估中改善评估者对文化熟悉度的判断。这些结果突显了在评估生成型LLMs的文化熟悉度时命题文化知识和开放性文化流畅度之间的区别。', 'title_zh': '面向地理文化基础的大型语言模型生成'}
{'arxiv_id': 'arXiv:2502.13490', 'title': 'What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis', 'authors': 'Peiran Wang, Yang Liu, Yunfei Lu, Jue Hong, Ye Wu', 'link': 'https://arxiv.org/abs/2502.13490', 'abstract': "Large language model (LLM) systems suffer from the models' unstable ability to generate valid and factual content, resulting in hallucination generation. Current hallucination detection methods heavily rely on out-of-model information sources, such as RAG to assist the detection, thus bringing heavy additional latency. Recently, internal states of LLMs' inference have been widely used in numerous research works, such as prompt injection detection, etc. Considering the interpretability of LLM internal states and the fact that they do not require external information sources, we introduce such states into LLM hallucination detection. In this paper, we systematically analyze different internal states' revealing features during inference forward and comprehensively evaluate their ability in hallucination detection. Specifically, we cut the forward process of a large language model into three stages: understanding, query, generation, and extracting the internal state from these stages. By analyzing these states, we provide a deep understanding of why the hallucinated content is generated and what happened in the internal state of the models. Then, we introduce these internal states into hallucination detection and conduct comprehensive experiments to discuss the advantages and limitations.", 'abstract_zh': '大型语言模型（LLM）系统在生成有效且准确的内容方面表现出不稳定的模型能力，导致它们产生幻觉。当前的幻觉检测方法严重依赖于模型外的信息源，如RAG等工具来辅助检测，这带来了额外的延迟。最近，LLM推理过程中的内部状态在众多研究工作中被广泛使用，例如提示注入检测等。考虑到LLM内部状态的可解释性和它们不需要外部信息源的特点，我们将这些内部状态引入到LLM幻觉检测中。本文系统分析了推理过程中不同内部状态的揭示特征，并全面评估了它们在幻觉检测中的能力。具体来说，我们将大型语言模型的前向过程划分为三个阶段：理解、查询和生成，并从中提取这些阶段的内部状态。通过分析这些状态，我们可以深入理解为什么会生成幻觉内容，以及模型内部状态发生了什么。随后，我们将这些内部状态引入幻觉检测中，并进行全面的实验讨论其优势和局限性。', 'title_zh': '《模型在思考些什么？通过模型内部状态分析理解大型语言模型的幻觉现象》'}
{'arxiv_id': 'arXiv:2502.13487', 'title': 'Transferring Textual Preferences to Vision-Language Understanding through Model Merging', 'authors': 'Chen-An Li, Tzu-Han Lin, Yun-Nung Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2502.13487', 'abstract': "Large vision-language models (LVLMs) perform outstandingly across various multimodal tasks. However, their ability to evaluate generated content remains limited, and training vision-language reward models (VLRMs) with preference data is computationally expensive. This paper explores a training-free alternative by merging text-based reward models (RMs) with LVLMs to create VLRMs. Our approach shows that integrating these models leads to improved performance over LVLMs' scoring and text-based RMs, offering an efficient method for incorporating textual preferences into LVLMs.", 'abstract_zh': '大规模多模态模型（LVLMs）在各种多模态任务中表现出色。然而，它们评估生成内容的能力仍然有限，使用偏好数据训练视觉-语言奖励模型（VLRMs）在计算上非常昂贵。本文通过将基于文本的奖励模型（RMs）与LVLMs结合，探索了一种无需训练的替代方案，以创建VLRMs。我们的方法表明，将这些模型集成可以提高VLRMs的性能，比单纯的LVLMs评分和基于文本的RM更高效，提供了一种将文本偏好融入LVLMs的有效方法。', 'title_zh': '通过模型合并将文本偏好转移至视觉-语言理解'}
{'arxiv_id': 'arXiv:2502.13475', 'title': 'LLM should think and action as a human', 'authors': 'Haun Leung, ZiNan Wang', 'link': 'https://arxiv.org/abs/2502.13475', 'abstract': 'It is popular lately to train large language models to be used as chat assistants, but in the conversation between the user and the chat assistant, there are prompts, require multi-turns between the chat assistant and the user. However, there are a number of issues with the multi-turns conversation: The response of the chat assistant is prone to errors and cannot help users achieve their goals; It is difficult for chat assistant to generate responses with different processes based on actual needs for the same command or request; Chat assistant require the use of tools, but the current approach is not elegant and efficient, and the number of tool calls that can be supported is limited. The main reason for these issues is that large language models do not have the thinking ability as a human, lack the reasoning ability and planning ability, and lack the ability to execute plans. To solve these issues, we propose a thinking method based on a built-in chain of thought: In the multi-turns conversation, for each user prompt, the large language model thinks based on elements such as chat history, thinking context, action calls, memory and knowledge, makes detailed reasoning and planning, and actions according to the plan. We also explored how the large language model enhances thinking ability through this thinking method: Collect training datasets according to the thinking method and fine tune the large language model through supervised learning; Train a consistency reward model and use it as a reward function to fine tune the large language model using reinforcement learning, and the reinforced large language model outputs according to this way of thinking. Our experimental results show that the reasoning ability and planning ability of the large language model are enhanced, and the issues in the multi-turns conversation are solved.', 'abstract_zh': '近年来，训练大规模语言模型作为聊天助理变得非常流行，但在用户与聊天助理的对话中，聊天助理需要与用户进行多轮互动。然而，多轮对话存在一些问题：聊天助理的回答容易出错，无法帮助用户实现目标；对于同一命令或请求，聊天助理难以根据不同实际需求生成不同的响应过程；聊天助理需要使用工具，但当前的方法不够优雅且效率低下，支持的工具调用数量也很有限。导致这些问题的主要原因是大型语言模型缺乏人类的思考能力，缺乏推理能力和策划能力，也缺乏执行计划的能力。为解决这些问题，我们提出了一种基于内置链式思考的方法：在多轮对话中，对于每个用户的提示，大型语言模型基于对话历史、思考背景、动作调用、记忆和知识等因素进行思考，进行详细的推理和规划，并根据计划采取行动。我们还探索了通过这种方法提升大型语言模型的思考能力的方式：根据此方法收集训练数据集，并通过监督学习微调大型语言模型；训练一致性奖励模型，并将其用作奖励函数，在强化学习中细调大型语言模型，强化的大型语言模型将按照这种方式进行思考。我们的实验结果表明，大型语言模型的推理能力和规划能力得到了增强，多轮对话中的问题得到了解决。', 'title_zh': 'LLM 应该像人类一样思考和行动。'}
{'arxiv_id': 'arXiv:2502.13474', 'title': 'Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect Controllable Text Generation with Large Language Models', 'authors': 'Chenyu Zhu, Yefeng Liu, Chenyang Lyu, Xue Yang, Guanhua Chen, Longyue Wang, Weihua Luo, Kaifu Zhang', 'link': 'https://arxiv.org/abs/2502.13474', 'abstract': 'Multi-aspect controllable text generation aims to control text generation in attributes from multiple aspects, making it a complex but powerful task in natural language processing. Supervised fine-tuning methods are often employed for this task due to their simplicity and effectiveness. However, they still have some limitations: low rank adaptation (LoRA) only fine-tunes a few parameters and has suboptimal control effects, while full fine-tuning (FFT) requires significant computational resources and is susceptible to overfitting, particularly when data is limited. Moreover, existing works typically train multi-aspect controllable text generation models using only single-aspect annotated data, which results in discrepancies in data distribution; at the same time, accurately generating text with specific attributes is a challenge that requires strong attribute-aware capabilities. To address these limitations, we propose a lightweight, adaptive and attribute-aware framework for multi-aspect controllable text generation. Our framework can dynamically adjust model parameters according to different aspects of data to achieve controllable text generation, aiming to optimize performance across multiple aspects. Experimental results show that our framework outperforms other strong baselines, achieves state-of-the-art performance, adapts well to data discrepancies, and is more accurate in attribute perception.', 'abstract_zh': '多方面可控文本生成旨在从多个方面控制文本生成，使其成为一个复杂但强大的自然语言处理任务。监督微调方法常被用于此任务，因其简单且有效。然而，它们仍然存在一些局限性：低秩适应（LoRA）仅微调少量参数，控制效果不佳；而全方位微调（FFT）需要大量计算资源，并且在数据有限时容易过拟合。此外，现有研究通常仅使用单方面标注数据来训练多方面可控文本生成模型，这导致了数据分布上的不一致；同时，准确生成具有特定属性的文本也是一项挑战，要求具有强大的属性感知能力。为解决这些问题，我们提出了一种轻量级、自适应和属性感知的框架，用于多方面可控文本生成。该框架可以根据数据的不同方面动态调整模型参数，以实现可控文本生成，并旨在在多个方面优化性能。实验结果表明，我们的框架优于其他强对照基准，达到最优性能，能够很好地适应数据分布差异，并在属性感知方面更为准确。', 'title_zh': '面向轻量级、适应性及属性感知的多方面可控文本生成——大型语言模型的方法'}
{'arxiv_id': 'arXiv:2502.13472', 'title': 'FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems', 'authors': 'Borui Liao, Yulong Xu, Jiao Ou, Kaiyuan Yang, Weihua Jian, Pengfei Wan, Di Zhang', 'link': 'https://arxiv.org/abs/2502.13472', 'abstract': 'Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.', 'abstract_zh': '全双工语音对话系统（全双工SDS）通过实现实时双向通信显著增强了人机交互的自然性。然而，现有的方法在高度耦合的体系结构设计和过于简化的二元状态建模下，面临着独立模块优化困难和上下文噪声干扰等挑战。本文提出了FlexDuo，这是一种灵活的全双工控制模块，通过模块化设计将全双工控制与语音对话系统解耦。此外，受到人类交流中信息过滤机制的启发，我们引入了一个显式的Idle状态。一方面，Idle状态过滤冗余噪声和无关音频以提高对话质量；另一方面，它建立了一种基于语义完整性的缓冲机制，降低相互干扰的风险，同时确保准确的响应转换。在Fisher语料库上的实验结果显示，与集成的全双工对话系统基线相比，FlexDuo将误中断率降低了24.9%，响应准确性提高了7.6%。此外，在汉语和英语对话质量方面，它还优于基于语音活动检测（VAD）的基线系统。提出的模块化架构和基于状态的对话模型为构建灵活且高效的全双工对话系统提供了新的技术路径。', 'title_zh': 'FlexDuo：一种 Enables 语音对话系统全双工能力的插件系统'}
{'arxiv_id': 'arXiv:2502.13464', 'title': 'Estimating Commonsense Plausibility through Semantic Shifts', 'authors': 'Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.13464', 'abstract': "Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches--reliant on likelihoods or verbalized judgments--struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models' ability to capture semantic nuances, thereby further enhancing ComPaSS.", 'abstract_zh': '常识合理性估计对于评估语言模型（LMs）至关重要，但现有的生成方法依赖于似然性或口头判断，难以进行细致的区分。在本文中，我们提出了一种新的区分性框架ComPaSS，通过测量在添加与常识相关的信息时语义的变化来量化常识合理性。合理的添加会导致语义微小的变化，而不合理的添加会导致显著的偏差。在包括大型语言模型（LLMs）和多模态语言视觉模型（VLMs）的不同骨干架构下的两种精细的常识合理性估计任务中进行的评估显示，ComPaSS 一贯优于基线方法。这表明区分性方法在精细的常识合理性评估中优于生成方法。实验还表明：（1）当与ComPaSS结合使用时，VLMs在视觉导向的常识任务中表现出优于LMs的性能。（2）对比预训练进一步增强了ComPaSS骨干模型捕捉语义细微差别的能力，从而进一步提高了ComPaSS的性能。', 'title_zh': '通过语义转换估算常识合理性'}
{'arxiv_id': 'arXiv:2502.13458', 'title': 'ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails', 'authors': 'Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen', 'link': 'https://arxiv.org/abs/2502.13458', 'abstract': "Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.", 'abstract_zh': '确保大型语言模型（LLMs）的安全性至关重要，因为它们正被应用于真实世界的应用中。现有的防护栏依赖于基于规则的过滤或单次分类，这限制了它们处理复杂安全违规的能力。为了解决这一问题，我们提出了一个名为ThinkGuard的批判增强防护栏模型，通过生成结构化的批判性反馈来提炼高容量LLMs的知识，同时标注安全标签。通过批判性反馈增强的数据微调，捕获的反思性思考能力极大地增强了防护栏的谨慎性和可解释性。在多个安全基准测试中，ThinkGuard在平均F1值和AUPRC（平均精准召回率曲线下面积）上表现出色，优于所有基线模型。与LLaMA Guard 3相比，ThinkGuard在准确率上提高了16.1%，在宏F1值上提高了27.0%。此外，ThinkGuard还超越了仅使用标签进行微调的模型，这表明结构化的批判性反馈不仅提升了分类的准确性，还增强了复杂安全性推理，同时保持了计算效率。', 'title_zh': 'ThinkGuard: 深思熟虑的慢思考促成谨慎的边界-guardrails 在这里特指一种谨慎的行为规范或限制，因此可以翻译为：\n\nThinkGuard: 深思熟虑的慢思考形成谨慎的界限'}
{'arxiv_id': 'arXiv:2502.13442', 'title': 'TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation', 'authors': 'Jialin Ouyang', 'link': 'https://arxiv.org/abs/2502.13442', 'abstract': 'Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems.', 'abstract_zh': '大型语言模型（LLMs）现在在标准数学应用题基准测试（如GSM8K）上的表现接近人类水平，但其真实的推理能力仍然存在争议。一个关键问题在于，模型往往会生成尽管自信但缺乏依据的答案，对无法解答的问题给出回答。我们提出了TreeCut，这是一个合成数据集，通过将每个问题表示为一棵树并移除选定的必要条件，系统地生成无限数量的无法解答的数学应用题及其可解答的对应问题。实验表明，在各自的最坏情况下，TreeCut成功地促使大型语言模型（包括GPT-4o和o3-mini）产生了61%和42%的幻觉。进一步的分析显示，更深或更复杂的树结构、复合项名称以及在路径中间移除必要条件都会增加幻觉的发生率，突显了大型语言模型在识别无法解答的数学问题方面持续存在的挑战。', 'title_zh': 'TreeCut: 用于大语言模型幻觉评估的合成不可回答数学文字问题数据集'}
{'arxiv_id': 'arXiv:2502.13441', 'title': 'The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?', 'authors': 'Yutao Sun, Mingshuai Chen, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Jianwei Yin', 'link': 'https://arxiv.org/abs/2502.13441', 'abstract': 'Self-improving large language models (LLMs) -- i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself -- is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent -- a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.', 'abstract_zh': '自提升大型语言模型（LLMs）——即通过使用模型自身生成的合成数据对其进行微调以提高其性能——是增进LLM能力的一种有前途的方法，同时避免了大量监督的需求。现有的自提升方法通常依赖于外部监督信号（如种子数据）和第三方模型的帮助。本文提出了Crescent——一种简单而有效的方法，能够在完全自主的情况下生成高质量的合成问答数据。Crescent首先通过诱饵提示促使LLM生成原始问题，然后通过基于拒绝采样的自我去重来多样化这些问题，最后将问题输入LLM，并通过多数投票收集相应的答案。我们表明，Crescent揭示了在零外部监督信号的情况下实现真正的自提升的潜力，特别是在数学推理方面；特别是，Crescent生成的问答对能够（i）在不牺牲其一般性能（尤其是在零样本设置中）的情况下提升LLM的推理能力；（ii）比现有基于种子数据集扩展的方法更有效地将LLM知识传递给较弱的模型。', 'title_zh': '自我提升悖论：语言模型能否在没有外部支撑的情况下自主提升推理能力？'}
{'arxiv_id': 'arXiv:2502.13428', 'title': 'MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering', 'authors': 'Guanming Xiong, Haochen Li, Wen Zhao', 'link': 'https://arxiv.org/abs/2502.13428', 'abstract': "This study explores how to enhance the reasoning capabilities of large language models (LLMs) in knowledge base question answering (KBQA) by leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods are particularly challenging as these approaches require locating elements from knowledge bases and generating logical forms, demanding not only extensive annotated data but also strong reasoning capabilities. Although recent approaches leveraging LLMs as agents have demonstrated considerable potential, these studies are inherently constrained by their linear decision-making processes. To address this limitation, we propose a MCTS-based framework that enhances LLMs' reasoning capabilities through tree search methodology. We design a carefully designed step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our approach significantly outperforms linear decision-making methods, particularly in low-resource scenarios. Additionally, we contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision. Experimental results on the extended dataset demonstrate that our method achieves comparable performance to fully supervised models while using significantly less training data.", 'abstract_zh': '本研究旨在通过利用蒙特卡洛树搜索（MCTS）来提升大型语言模型（LLMs）在知识库问答（KBQA）中的推理能力。基于语义解析的KBQA方法尤其具有挑战性，因为这些方法需要从知识库中定位元素并生成逻辑形式，不仅需要大量的标注数据，还需要强大的推理能力。尽管利用LLMs作为代理的最近一些方法展示了其巨大的潜力，但这些研究本质上受限于其线性的决策过程。为了解决这一限制，我们提出了一种基于MCTS的框架，通过树搜索方法来增强LLMs的推理能力。我们设计了一种精细设计的逐步奖励机制，仅需直接提示开源指令LLMs，无需额外的微调。实验结果表明，我们的方法在低资源场景下显著优于线性决策方法。此外，我们通过使用遥测监督对现有问题-SPARQL数据集中的中间推理过程进行标注，为KBQA社区贡献了新的数据资源。在扩展数据集上的实验结果表明，我们的方法在使用显著少的训练数据的情况下，实现了与完全监督模型相当的性能。', 'title_zh': 'MCTS-KBQA：基于蒙特卡洛树搜索的知识库问答'}
{'arxiv_id': 'arXiv:2502.13422', 'title': 'TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition', 'authors': 'Yuxiang Wang, Junhao Gan, Jianzhong Qi', 'link': 'https://arxiv.org/abs/2502.13422', 'abstract': "Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose TabSD, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. TabSD generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, SQL Verifier refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, SLQA and SEQA, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that TABSD outperforms the best-existing baseline models by 23.07%, 2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables.", 'abstract_zh': '自由格式表格上的问答（TableQA）由于缺乏预定义的模式和大型表格中的噪声而具有挑战性。虽然大型语言模型（LLMs）在TableQA中显示出潜力，但它们在处理大规模自由格式表格和噪声敏感性方面存在困难。为了解决这些挑战，我们提出了一种基于SQL的分解模型TabSD，以增强LLMs处理大规模自由格式表格的能力。TabSD生成SQL查询来指导表格分解、去除噪声并处理子表格以生成更好的答案。此外，SQL验证器会优化SQL输出以提高分解准确性。我们介绍了两个包含大规模自由格式表格的TableQA数据集，SLQA和SEQA，这些数据集仅包含大规模自由格式表格，并将公开提供。在四个基准数据集上的实验结果表明，TabSD在准确率方面分别优于现有最优基线模型23.07%、2.84%、23.24%和9.32%，突显了其在处理大规模和噪声丰富的自由格式表格方面的有效性。', 'title_zh': 'TabSD：基于SQL表分解的大型自由格式表格问答'}
{'arxiv_id': 'arXiv:2502.13417', 'title': 'RLTHF: Targeted Human Feedback for LLM Alignment', 'authors': 'Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra', 'link': 'https://arxiv.org/abs/2502.13417', 'abstract': "Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF's strategic data curation.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，需符合学术规范：\n\n对用户偏好进行大规模语言模型（LLMs）的调优是具有挑战性的，主要是因为在基于人类反馈的强化学习（RLHF）中高质量的人类注解成本高昂，而AI反馈的一般化能力有限。为了解决这些挑战，我们提出了一种人机混合框架RLTHF，该框架结合了基于LLM的初步对齐和选择性的人类注解，以实现最少努力的全人类注解对齐。RLTHF 使用奖励模型的奖励分布识别LLM误标注的难以注解样本，并通过结合战略性的人类修正持续增强对齐，同时利用LLM正确标注的样本。在HH-RLHF和TL;DR数据集上的评估表明，仅需6-7%的人类注解努力，RLTHF 即能达到全人类注解级别的对齐效果。此外，使用RLTHF 精选数据集训练的模型在下游任务中的表现优于使用完全人工标注数据集训练的模型，这进一步证明了RLTHF 战略性数据精选的有效性。', 'title_zh': 'RLTHF：针对LLM对齐的定向人类反馈'}
{'arxiv_id': 'arXiv:2502.13416', 'title': 'Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning', 'authors': 'Ningke Li, Yahui Song, Kailong Wang, Yuekang Li, Ling Shi, Yi Liu, Haoyu Wang', 'link': 'https://arxiv.org/abs/2502.13416', 'abstract': "Large language models (LLMs) face the challenge of hallucinations -- outputs that seem coherent but are actually incorrect. A particularly damaging type is fact-conflicting hallucination (FCH), where generated content contradicts established facts. Addressing FCH presents three main challenges: 1) Automatically constructing and maintaining large-scale benchmark datasets is difficult and resource-intensive; 2) Generating complex and efficient test cases that the LLM has not been trained on -- especially those involving intricate temporal features -- is challenging, yet crucial for eliciting hallucinations; and 3) Validating the reasoning behind LLM outputs is inherently difficult, particularly with complex logical relationships, as it requires transparency in the model's decision-making process.\nThis paper presents Drowzee, an innovative end-to-end metamorphic testing framework that utilizes temporal logic to identify fact-conflicting hallucinations (FCH) in large language models (LLMs). Drowzee builds a comprehensive factual knowledge base by crawling sources like Wikipedia and uses automated temporal-logic reasoning to convert this knowledge into a large, extensible set of test cases with ground truth answers. LLMs are tested using these cases through template-based prompts, which require them to generate both answers and reasoning steps. To validate the reasoning, we propose two semantic-aware oracles that compare the semantic structure of LLM outputs to the ground truths. Across nine LLMs in nine different knowledge domains, experimental results show that Drowzee effectively identifies rates of non-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates of temporal-related hallucinations ranging from 16.7% to 39.2%.", 'abstract_zh': '大型语言模型（LLMs）面临幻觉的挑战，即输出看似连贯但实际上是错误的。其中，特别是事实冲突的幻觉（FCH）尤为有害，这种幻觉是生成的内容与已确立的事实相矛盾。解决FCH的主要挑战包括：1）自动构建和维护大规模基准数据集既困难又耗资源；2）生成LLM未受训练且尤其涉及复杂时间特征的复杂测试案例具有挑战性，但这些测试案例对于引发幻觉至关重要；和3）验证LLM输出的推理过程本质上是困难的，尤其是当涉及复杂的逻辑关系时，因为这需要模型决策过程的透明度。\n本文提出了一种名为Drowzee的创新端到端变形测试框架，该框架利用时序逻辑来识别大型语言模型（LLMs）中的事实冲突幻觉（FCH）。Drowzee通过爬取如维基百科等源来构建全面的事实知识库，并使用自动化的时序逻辑推理将这些知识转化为大量可扩展的具有真实答案测试案例集。通过基于模板的提示对LLM进行测试，要求它们不仅生成答案，还要生成推理步骤。为了验证推理，我们提出了两种语义感知的验证机制，将LLM输出的语义结构与真实答案进行对比。在九个不同知识领域的九个LLM上进行的实验结果显示，Drowzee能够有效识别24.7%到59.8%的非时间相关幻觉率，以及16.7%到39.2%的时间相关幻觉率。', 'title_zh': '基于时间逻辑推理增强的LLM事实矛盾幻觉检测'}
{'arxiv_id': 'arXiv:2502.13396', 'title': 'Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study', 'authors': 'Wenwen Xie, Gray Gwizdz, Dongji Feng', 'link': 'https://arxiv.org/abs/2502.13396', 'abstract': 'While Large Language Models (LLMs) have emerged as promising tools for evaluating Natural Language Generation (NLG) tasks, their effectiveness is limited by their inability to appropriately weigh the importance of different topics, often overemphasizing minor details while undervaluing critical information, leading to misleading assessments. Our work proposes an efficient prompt design mechanism to address this specific limitation and provide a case study. Through strategic prompt engineering that incorporates explicit importance weighting mechanisms, we enhance using LLM-as-a-Judge ability to prioritize relevant information effectively, as demonstrated by an average improvement of 6% in the Human Alignment Rate (HAR) metric.', 'abstract_zh': '尽管大型语言模型（LLMs）已在自然语言生成（NLG）任务评估中展现出有前景的应用工具潜力，但它们的有效性受限于无法适当权衡不同主题的重要性，常常过度强调细枝末节而忽视关键信息，导致评估结果误导性。我们提出了一种高效的设计提示机制来解决这一特定限制，并进行了案例研究。通过战略性地进行提示工程，结合明确的重要权重机制，我们增强了使用LLM作为“裁判”的能力，以有效优先处理相关信息，这在人类一致性率（HAR）指标上平均提高了6%。', 'title_zh': '将权重机制提示引入两步流程中的LLM作为法官：一项案例研究'}
{'arxiv_id': 'arXiv:2502.13383', 'title': 'MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification', 'authors': 'Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, Wentao Zhang', 'link': 'https://arxiv.org/abs/2502.13383', 'abstract': 'According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.', 'abstract_zh': '根据测试时缩放（Test-Time Scaling）原则，外部慢思考与验证机制的集成已被证明能够增强大型语言模型（LLMs）的多轮推理能力。然而，在多模态（MM）领域中，仍缺乏一个强大的MM验证器。本文旨在通过引入MM验证器（MM-Verifier）和MM推理器（MM-Reasoner）来增强多模态推理，通过更长的推理时间和更稳健的验证。首先，我们提出了一种两步MM验证数据合成方法，该方法结合了基于模拟的树搜索和验证，并使用拒绝采样生成高质量的思维链（CoT）数据。这些数据用于微调验证模型MM-Verifier。此外，我们介绍了一种更高效的MMCOT数据合成方法，填补了基于文本和多模态推理之间的差距。合成的数据用于微调MM-Reasoner。我们的MM-Verifier在MathCheck、MathVista和MathVerse基准测试中均优于所有更大的模型。此外，MM-Reasoner展示了很强的有效性和可扩展性，随着数据量的增加，性能不断提升。最终，当结合MM-Reasoner和MM-Verifier时，我们的方法在MathVista基准测试中取得了65.3的高准确率，超过了GPT-4o（63.8），该结果是在12次采样（rollouts）下得出的。', 'title_zh': 'MM-Verify: 提高多模态推理能力的链式思考验证方法'}
{'arxiv_id': 'arXiv:2502.13374', 'title': 'Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor', 'authors': 'Barys Liskavets, Shuvendu Roy, Maxim Ushakov, Mark Klibanov, Ali Etemad, Shane Luke', 'link': 'https://arxiv.org/abs/2502.13374', 'abstract': 'The rise of Large Language Models (LLMs) has led to significant interest in prompt compression, a technique aimed at reducing the length of input prompts while preserving critical information. However, the prominent approaches in prompt compression often require explicit questions or handcrafted templates for compression, limiting their generalizability. We propose Task-agnostic Prompt Compression (TPC), a novel framework that generalizes compression across tasks and domains without requiring input questions or templates. TPC generates a context-relevant task description using a task descriptor trained on a curated dataset of context and query pairs, and fine-tuned via reinforcement learning with a reward function designed to capture the most relevant information. The task descriptor is then utilized to compute the relevance of each sentence in the prompt to generate the compressed prompt. We introduce 3 model sizes (Base, Large, and Huge), where the largest model outperforms the existing state-of-the-art methods on LongBench and ZeroSCROLLS benchmarks, and our smallest model performs comparable to the existing solutions while being considerably smaller.', 'abstract_zh': '大型语言模型（LLMs）的兴起引发了对指令压缩的广泛关注，这是一种旨在减少输入指令长度同时保留关键信息的技术。然而，当前的指令压缩方法往往需要明确的问题或人工设计的模板，这限制了它们的普适性。我们提出了一种名为任务无关指令压缩（Task-agnostic Prompt Compression, TPC）的新框架，该框架能够跨任务和领域进行通用的指令压缩，而无需输入的问题或模板。TPC 使用在一个编排好的上下文和查询对数据集上训练的任务描述器生成与上下文相关的目标描述，并通过强化学习进行微调，奖励函数旨在捕捉最相关的信息。任务描述器随后被用于计算指令中每一句话的相关性，以生成压缩后的指令。我们提供了三种模型规模（基础版、大型版和超大型版），其中最大的模型在 LongBench 和 ZeroSCROLLS 基准测试中优于现有最先进方法，而我们的最小模型则在规模上显著减小，且性能可与现有解决方案媲美。', 'title_zh': '基于上下文感知句子嵌入和奖励引导任务描述的无任务提示压缩'}
{'arxiv_id': 'arXiv:2502.13369', 'title': 'Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval', 'authors': 'Aditya Sharma, Luis Lara, Amal Zouaq, Christopher J. Pal', 'link': 'https://arxiv.org/abs/2502.13369', 'abstract': 'The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.', 'abstract_zh': '从自然语言问题生成SPARQL查询的能力对于确保从知识图谱（KG）中高效且准确地检索结构化数据至关重要。虽然大型语言模型（LLMs）已在SPARQL查询生成中得到了广泛应用，但在基于内部参数知识生成知识图谱元素（如统一资源标识符URIs）时，它们往往容易出现幻觉和脱离分布的错误。这经常导致看似合理但实际上错误的内容，从而对它们在实际信息检索（IR）应用中的使用构成了重大挑战。这促使了大量研究致力于检测和缓解这些错误。本文介绍了一种模块化框架PGMR（Post-Generation Memory Retrieval），该框架结合了一个非参数记忆模块，用于检索KG元素并增强基于LLM的SPARQL查询生成。实验结果表明，PGMR在多种数据集、数据分布和LLM中均表现出强劲的性能。值得注意的是，PGMR显著减少了URI幻觉，多个场景中几乎完全消除了这一问题。', 'title_zh': '基于生成后记忆检索的减轻语言模型驱动的SPARQL查询生成中幻觉的方法'}
{'arxiv_id': 'arXiv:2502.13361', 'title': 'RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering', 'authors': 'Sichu Liang, Linhai Zhang, Hongyu Zhu, Wenwen Wang, Yulan He, Deyu Zhou', 'link': 'https://arxiv.org/abs/2502.13361', 'abstract': 'Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.', 'abstract_zh': '医学问答需要广泛获取专一的概念知识。当前的范式，检索增强生成（RAG），通过大规模语料库检索获取医学专业知识，并利用这些知识来引导一个通用的大型语言模型（LLM）生成答案。然而，现有的检索方法往往忽视了事实性知识的重要性，这限制了检索到的概念知识的相关性，并在临床决策等实际场景中（如基于电子健康记录EHRs）限制了其应用。本文引入了RGAR，这是一种循环生成增强检索框架，可以从双重来源（即EHRs和语料库）检索相关的事实性和概念性知识，使它们能够相互作用和相互完善。通过在三个事实感知的医学问答基准测试中进行广泛的评估，RGAR在医学RAG系统中确立了新的最优性能。值得注意的是，使用RGAR的Llama-3.1-8B-Instruct模型超过了显著更大的RAG增强的GPT-3.5模型。我们的研究结果证明了提取事实性知识对于检索的益处，这始终能提高生成质量。', 'title_zh': 'RGAR：基于循环生成增强检索的事实感知医疗问答方法'}
{'arxiv_id': 'arXiv:2502.13358', 'title': 'Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications', 'authors': 'Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu', 'link': 'https://arxiv.org/abs/2502.13358', 'abstract': 'Large Language Models (LLMs) have transformed natural language processing, yet they still struggle with direct text editing tasks that demand precise, context-aware modifications. While models like ChatGPT excel in text generation and analysis, their editing abilities often fall short, addressing only superficial issues rather than deeper structural or logical inconsistencies. In this work, we introduce a dual approach to enhance LLMs editing performance. First, we present InstrEditBench, a high-quality benchmark dataset comprising over 20,000 structured editing tasks spanning Wiki articles, LaTeX documents, code, and database Domain-specific Languages (DSL). InstrEditBench is generated using an innovative automated workflow that accurately identifies and evaluates targeted edits, ensuring that modifications adhere strictly to specified instructions without altering unrelated content. Second, we propose FineEdit, a specialized model trained on this curated benchmark. Experimental results demonstrate that FineEdit achieves significant improvements around {10\\%} compared with Gemini on direct editing tasks, convincingly validating its effectiveness.', 'abstract_zh': '大型语言模型（LLMs）已经改变了自然语言处理领域，但它们仍难以直接应对需要精确且上下文相关的修改的文本编辑任务。虽然像ChatGPT这样的模型在文本生成和分析方面表现出色，但在编辑能力方面往往不尽如人意，它们通常只能解决表面问题，而忽视了深层次的结构性或逻辑一致性问题。在本文中，我们提出了一种双管齐下的方法来提升LLMs的编辑性能。首先，我们介绍了InstrEditBench，这是一个包含超过20,000个结构化编辑任务的高质量基准数据集，任务范围涵盖了维基百科文章、LaTeX文档、代码和数据库领域特定语言（DSL）。InstrEditBench 是通过一个创新的自动化工作流程生成的，此工作流程能够精准地识别和评估目标修改，确保修改严格符合指定的指示，而不影响无关的内容。其次，我们提出了FineEdit，这是一个专门为此精选基准数据集训练的模型。实验结果表明，FineEdit 在直接编辑任务上的表现相比于Gemini 提升了约10%，有力地验证了其有效性。', 'title_zh': '填补大语言模型编辑差距：FineEdit 用于精确和针对性的文字修改'}
{'arxiv_id': 'arXiv:2502.13349', 'title': 'Event Segmentation Applications in Large Language Model Enabled Automated Recall Assessments', 'authors': 'Ryan A. Panela, Alex J. Barnett, Morgan D. Barense, Björn Herrmann', 'link': 'https://arxiv.org/abs/2502.13349', 'abstract': 'Understanding how individuals perceive and recall information in their natural environments is critical to understanding potential failures in perception (e.g., sensory loss) and memory (e.g., dementia). Event segmentation, the process of identifying distinct events within dynamic environments, is central to how we perceive, encode, and recall experiences. This cognitive process not only influences moment-to-moment comprehension but also shapes event specific memory. Despite the importance of event segmentation and event memory, current research methodologies rely heavily on human judgements for assessing segmentation patterns and recall ability, which are subjective and time-consuming. A few approaches have been introduced to automate event segmentation and recall scoring, but validity with human responses and ease of implementation require further advancements. To address these concerns, we leverage Large Language Models (LLMs) to automate event segmentation and assess recall, employing chat completion and text-embedding models, respectively. We validated these models against human annotations and determined that LLMs can accurately identify event boundaries, and that human event segmentation is more consistent with LLMs than among humans themselves. Using this framework, we advanced an automated approach for recall assessments which revealed semantic similarity between segmented narrative events and participant recall can estimate recall performance. Our findings demonstrate that LLMs can effectively simulate human segmentation patterns and provide recall evaluations that are a scalable alternative to manual scoring. This research opens novel avenues for studying the intersection between perception, memory, and cognitive impairment using methodologies driven by artificial intelligence.', 'abstract_zh': '理解个体在自然环境中感知和回忆信息的方式对于理解感知潜在失败（例如感觉丧失）和记忆障碍（例如痴呆症）至关重要。事件分段，即在动态环境中识别出不同的事件的过程，是我们在感知、编码和回忆经验中的核心。这一认知过程不仅影响即时的理解，还塑造了特定事件的记忆。尽管事件分段和事件记忆的重要性日益凸显，但当前的研究方法主要依赖人类判断来评估分段模式和回忆能力，这往往主观且耗时。尽管已有少数方法用于自动化事件分段和回忆评分，但在人类响应的有效性和实施的简便性方面仍需要进一步改进。为应对这些问题，我们利用大型语言模型（LLMs）来自动执行事件分段和回忆评估，分别采用了聊天完成和文本嵌入模型。我们通过人类注释对这些模型进行了验证，并确定LLMs能够准确识别事件边界，人类事件分段的一致性甚至优于人与人之间的共识。利用这一框架，我们推进了自动化的回忆评估方法，表明分段叙述事件和参与者所回忆的内容在语义上的相似性可以估计回忆性能。我们的研究结果表明，LLMs能够有效地模拟人类的分段模式，并提供一种有扩展性的替代手动评分的回忆评估。这项研究为利用人工智能驱动的方法研究感知、记忆与认知障碍之间的交叉领域开拓了新的途径。', 'title_zh': '大型语言模型赋能的自动回忆评估中的事件段落化应用'}
{'arxiv_id': 'arXiv:2502.13347', 'title': 'Craw4LLM: Efficient Web Crawling for LLM Pretraining', 'authors': 'Shi Yu, Zhiyuan Liu, Chenyan Xiong', 'link': 'https://arxiv.org/abs/2502.13347', 'abstract': "Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at this https URL.", 'abstract_zh': '网页爬取是大语言模型（LLMs）预训练数据的主要来源，但大多数爬取的网页由于质量较低而在预训练中被丢弃。本文提出了一种名为Crawl4LLM的有效网络爬取方法，该方法基于LLM预训练的需求探索网络图。具体而言，它利用网页在LLM预训练中的影响力作为网络爬虫调度器的优先级评分，替代了基于标准图连接性的优先级评分方式。我们在包含9亿个网页的商业搜索引擎索引的网络图上进行的实验表明，Crawl4LLM能够在获得高质量预训练数据方面表现出高效性。使用仅21%的URL，基于Crawl4LLM数据进行预训练的语言模型在下游任务中达到了之前爬取的性能，显著减少了爬取的浪费并减轻了网站的负担。我们的代码将在以下网址公开：this https URL。', 'title_zh': 'Craw4LLM：用于大语言模型预训练的高效网页抓取方法'}
{'arxiv_id': 'arXiv:2502.13342', 'title': 'Beyond De-Identification: A Structured Approach for Defining and Detecting Indirect Identifiers in Medical Texts', 'authors': 'Ibrahim Baroud, Lisa Raithel, Sebastian Möller, Roland Roller', 'link': 'https://arxiv.org/abs/2502.13342', 'abstract': 'Sharing sensitive texts for scientific purposes requires appropriate techniques to protect the privacy of patients and healthcare personnel. Anonymizing textual data is particularly challenging due to the presence of diverse unstructured direct and indirect identifiers. To mitigate the risk of re-identification, this work introduces a schema of nine categories of indirect identifiers designed to account for different potential adversaries, including acquaintances, family members and medical staff. Using this schema, we annotate 100 MIMIC-III discharge summaries and propose baseline models for identifying indirect identifiers. We will release the annotation guidelines, annotation spans (6,199 annotations in total) and the corresponding MIMIC-III document IDs to support further research in this area.', 'abstract_zh': '为了科学研究目的共享敏感文字需要采用适当的技术来保护患者和医疗人员的隐私。匿名化文本数据尤为具有挑战性，因为其中包含多种多样的未结构化直接和间接标识符。为了减轻重新识别的风险，本研究引入了一个由九类间接标识符组成的方案，该方案考虑了不同的潜在对手，包括熟人、家庭成员和医疗人员。利用这一方案，我们对100万份MIMIC-III出院总结进行了标注，并提出了一种基准模型来识别间接标识符。我们将发布标注指南、标注区间（总计6,199个标注）以及相应的MIMIC-III文档ID，以支持该领域的进一步研究。', 'title_zh': '超越去标识化：一种结构化方法用于定义和检测医学文本中的间接标识符'}
{'arxiv_id': 'arXiv:2502.13337', 'title': 'Language Models are Few-Shot Graders', 'authors': 'Chenyan Zhao, Mariana Silva, Seth Poulsen', 'link': 'https://arxiv.org/abs/2502.13337', 'abstract': 'Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.', 'abstract_zh': '对学生作业提供评价是有效学习的关键组成部分，而自动化这一过程可以显著减轻人工评分者的负担。基于大型语言模型（LLMs）的进步，自动短回答评分（ASAG）系统为评估和提供即时反馈提供了有希望的解决方案，尤其适用于开放性学生回答。本文介绍了利用最先进的LLMs构建的ASAG管道。我们的LLM基ASAG管道在相同的数据集上比现有自定义模型表现更好。我们还比较了三种OpenAI模型：GPT-4、GPT-4o和o1-preview的评分性能。结果显示，GPT-4o在准确性和成本效益之间取得了最佳平衡。另一方面，尽管o1-preview的准确率更高，但由于误差波动较大，使其在教室环境中不太实用。我们研究了使用无示例、随机选择和基于检索增强生成（RAG）的选择策略将教师评分示例纳入提示中的影响。我们的研究结果表明，提供评分示例可以提高评分准确性，而基于RAG的选择策略优于随机选择。此外，将评分标准整合到系统中可以提高准确性，因为它提供了一个结构化的评价标准。', 'title_zh': '语言模型是少量示例评分器'}
{'arxiv_id': 'arXiv:2502.13329', 'title': 'Language Models Can Predict Their Own Behavior', 'authors': 'Dhananjay Ashok, Jonathan May', 'link': 'https://arxiv.org/abs/2502.13329', 'abstract': 'Autoregressive Language Models output text by sequentially predicting the next token to generate, with modern methods like Chain-of-Thought (CoT) prompting achieving state-of-the-art reasoning capabilities by scaling the number of generated tokens. However, are there times when we can infer how the model will behave (e.g. abstain from answering a question) early in the computation, making generation unnecessary? We show that internal representation of input tokens alone can often precisely predict, not just the next token, but eventual behavior over the entire output sequence. We leverage this capacity and learn probes on internal states to create early warning (and exit) systems. Specifically, if the probes can confidently estimate the way the LM is going to behave, then the system will avoid generating tokens altogether and return the estimated behavior instead. On 27 text classification datasets spanning five different tasks, we apply this method to estimate the eventual answer of an LM under CoT prompting, reducing inference costs by 65% (average) while suffering an accuracy loss of no more than 1.4% (worst case). We demonstrate the potential of this method to pre-emptively identify when a model will abstain from answering a question, fail to follow output format specifications, or give a low-confidence response. We explore the limits of this capability, showing that probes generalize to unseen datasets, but perform worse when LM outputs are longer and struggle to predict properties that require access to knowledge that the models themselves lack. Encouragingly, performance scales with model size, suggesting applicability to the largest of models', 'abstract_zh': '自回归语言模型通过顺序预测下一个标记来生成文本，现代方法如Chain-of-Thought (CoT) 提示可以扩展生成的标记数量，从而实现最先进的推理能力。然而，是否存在在计算早期就能推断出模型的行为（例如，在回答问题时选择不作答）从而避免生成的过程的情况？我们发现仅依靠输入标记的内部表示，通常可以准确预测整个输出序列的行为，而不仅仅是下一个标记。我们利用这一能力，通过学习内部状态的探针来创建早期预警（和退出）系统。具体来说，如果探针可以自信地估计自回归语言模型的行为，那么系统将避免生成任何标记，并返回估计的行为。我们在这涵盖五个不同任务的27个文本分类数据集上应用此方法，以估计在使用CoT提示下的自回归语言模型的最终答案，通过减少65％（平均值）的推理成本，同时仅损失最多1.4％的准确性（最坏情况）。我们展示了该方法预判模型何时选择不作答、未能遵循输出格式规范或给出低置信度响应的潜力。我们探索了该能力的限制，发现探针在未曾见过的数据集上表现出良好的泛化能力，但当语言模型的输出较长时表现较差，难以预测需要访问模型自身所缺乏知识的属性。令人鼓舞的是，性能与模型规模成正比，这表明该方法适用于最大的模型。', 'title_zh': '语言模型可以预测自身的行为'}
{'arxiv_id': 'arXiv:2502.13326', 'title': 'Capturing Human Cognitive Styles with Language: Towards an Experimental Evaluation Paradigm', 'authors': 'Vasudha Varadarajan, Syeda Mahwish, Xiaoran Liu, Julia Buffolino, Christian C. Luhmann, Ryan L. Boyd, H. Andrew Schwartz', 'link': 'https://arxiv.org/abs/2502.13326', 'abstract': "While NLP models often seek to capture cognitive states via language, the validity of predicted states is determined by comparing them to annotations created without access the cognitive states of the authors. In behavioral sciences, cognitive states are instead measured via experiments. Here, we introduce an experiment-based framework for evaluating language-based cognitive style models against human behavior. We explore the phenomenon of decision making, and its relationship to the linguistic style of an individual talking about a recent decision they made. The participants then follow a classical decision-making experiment that captures their cognitive style, determined by how preferences change during a decision exercise. We find that language features, intended to capture cognitive style, can predict participants' decision style with moderate-to-high accuracy (AUC ~ 0.8), demonstrating that cognitive style can be partly captured and revealed by discourse patterns.", 'abstract_zh': '尽管自然语言处理（NLP）模型常常试图通过语言来捕捉认知状态，但预测出的认知状态的有效性则是通过与作者的认知状态无法直接获取的标注进行比较来确定的。在行为科学中，认知状态则通过实验进行测量。在此基础上，我们提出了一种基于实验的框架，用于评估语言驱动的认知风格模型与人类行为的一致性。我们探讨了决策这一现象及其与个体讨论最近做出的决策时所采用的语言风格之间的关系。随后，参与者进行了一个经典的决策实验，该实验通过捕捉他们在决策过程中的偏好变化来衡量其认知风格。我们发现，旨在捕捉认知风格的语言特征能够以中等到高的准确率（AUC约0.8）预测参与者的决策风格，这表明认知风格部分可以通过话语模式来捕捉和揭示。', 'title_zh': '使用语言捕捉人类认知风格：一种实验评价范式的探索'}
{'arxiv_id': 'arXiv:2502.13319', 'title': 'Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare', 'authors': 'Hiba Ahsan, Arnab Sen Sharma, Silvio Amir, David Bau, Byron C. Wallace', 'link': 'https://arxiv.org/abs/2502.13319', 'abstract': 'We know from prior work that LLMs encode social biases, and that this manifests in clinical tasks. In this work we adopt tools from mechanistic interpretability to unveil sociodemographic representations and biases within LLMs in the context of healthcare. Specifically, we ask: Can we identify activations within LLMs that encode sociodemographic information (e.g., gender, race)? We find that gender information is highly localized in middle MLP layers and can be reliably manipulated at inference time via patching. Such interventions can surgically alter generated clinical vignettes for specific conditions, and also influence downstream clinical predictions which correlate with gender, e.g., patient risk of depression. We find that representation of patient race is somewhat more distributed, but can also be intervened upon, to a degree. To our knowledge, this is the first application of mechanistic interpretability methods to LLMs for healthcare.', 'abstract_zh': '从前人的研究中我们知道，大规模语言模型（LLMs）会编码社会偏见，并且这种偏见在临床任务中表现出来。在这项研究中，我们采用了机制可解释性工具来揭示LLMs在医疗保健背景下的社会人口统计特征和偏见。具体而言，我们提出以下问题：我们能否识别出LLMs中编码社会人口统计信息（如性别、种族）的激活？我们发现性别信息在中间的多层感知机（MLP）层中高度集中，并且可以通过修补技术在推理时可靠地进行操控。这些干预措施可以精确地改变特定疾病条件下的生成临床案例，并且还会对与性别相关的下游临床预测产生影响，例如患者抑郁风险。我们发现患者种族的表示相对分布更广泛，但也可以在一定程度上进行干预。据我们所知，这是首次将机制可解释性方法应用于医疗保健领域的LLMs中。', 'title_zh': '阐明LLMs在医疗健康领域中人口统计学偏见机制的研究'}
{'arxiv_id': 'arXiv:2502.13311', 'title': 'Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors', 'authors': 'Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai', 'link': 'https://arxiv.org/abs/2502.13311', 'abstract': "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.", 'abstract_zh': '由大规模语言模型（LLMs）驱动的智能辅导代理在语言学习和科学教育等领域提供了个性化的指导，并且被广泛关注。然而，它们在引导用户解决复杂现实任务方面的能力尚未充分探索。为了解决这一限制，本研究聚焦于编码辅导这一具有挑战性的问题，要求辅导代理主动引导学生完成预定义的编码任务。我们提出了一种新颖的代理工作流——追踪与验证（TRAVer），结合了知识追踪来估计学生的学习状态，并通过每步验证确保有效指导以完成任务。我们引入了DICT，这是一种自动评估协议，通过受控的学生模拟和代码生成测试综合评估辅导代理。广泛的实验揭示了编码辅导的挑战，并展示了TRAVer实现了显著更高的成功率。虽然我们在本文中以代码辅导为例，但我们的结果和发现可以延伸到其他任务，为多种任务的辅导代理进步提供有价值的见解。', 'title_zh': '将以下论文内容或标题翻译成中文，并符合学术规范：\n\nTraining Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors\n\n训练逐轮验证器以用于对话辅导代理：LLM作为你的编程辅导老师的有趣案例'}
{'arxiv_id': 'arXiv:2502.13310', 'title': 'Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations', 'authors': 'Adib Mosharrof, Moghis Fereidouni, A.B. Siddique', 'link': 'https://arxiv.org/abs/2502.13310', 'abstract': 'Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data. Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses. However, their task completion performance - measured by accurate execution of API calls - remains suboptimal, with the best models achieving only around 53% success in unseen domains. To improve task completion, we propose ZeroToD, a framework that incorporates a schema augmentation mechanism to enhance API call accuracy and overall task completion rates, particularly in out-of-domain settings. We also compare ZeroToD with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.', 'abstract_zh': '传统任务导向对话（ToD）系统在训练过程中 heavily 依赖于劳动密集型的回合级标注，如对话状态和策略标签。本文探讨了大型语言模型（LLMs）是否可以在仅使用自然语言对话数据的情况下进行微调以执行ToD任务，无需标注这些数据。我们评估了它们在未见过的领域中的泛化能力，并将其表现与在完全标注数据上训练的模型进行了比较。通过使用三种不同规模的开源LLMs和两个多样的ToD数据集进行广泛的实验，我们发现，在未使用回合级标注的情况下进行微调的模型能够生成连贯且在上下文中的适当回应。然而，它们的任务完成性能——通过准确执行API调用来衡量——仍然不尽如人意，即使是最优模型在未见过的领域中的成功率也只有约53%。为了提高任务完成率，我们提出了ZeroToD框架，该框架结合了一种模式扩充机制，以提高API调用准确性和总体任务完成率，尤其是在领域外设置中。我们还比较了ZeroToD与无需微调的替代方法，如利用现成的LLMs进行提示，发现我们的框架使得更小的、经过微调的模型能够在任务完成方面超越大规模的专有LLMs。此外，一个评估信息量、流畅性和任务完成的人类研究表明了我们的实证发现。这些发现表明，可以开发出在实际应用中成本低、可扩展且零样本泛化的ToD系统是可行的。', 'title_zh': '评估和增强面向任务对话系统在域外 généralization 的能力：无需基于轮次的对话标注完成任务'}
{'arxiv_id': 'arXiv:2502.13298', 'title': 'Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via Prompt Chaining and Fine-Grained Feedback', 'authors': 'Moghis Fereidouni, Md Sajid Ahmed, Adib Mosharrof, A.B. Siddique', 'link': 'https://arxiv.org/abs/2502.13298', 'abstract': 'Task-oriented dialog (TOD) systems facilitate users in accomplishing complex, multi-turn tasks through natural language. While traditional approaches rely on extensive fine-tuning and annotated data for each domain, instruction-tuned large language models (LLMs) offer a more flexible alternative. However, LLMs struggle to reliably handle multi-turn task completion, particularly with accurately generating API calls and adapting to new domains without explicit demonstrations. To address these challenges, we propose RealTOD, a novel framework that enhances TOD systems through prompt chaining and fine-grained feedback mechanisms. Prompt chaining enables zero-shot domain adaptation via a two-stage prompting strategy, eliminating the need for human-curated demonstrations. Meanwhile, the fine-grained feedback mechanism improves task completion by verifying API calls against domain schemas and providing precise corrective feedback when errors are detected. We conduct extensive experiments on the SGD and BiTOD benchmarks using four LLMs. RealTOD improves API accuracy, surpassing AutoTOD by 37.74% on SGD and SimpleTOD by 11.26% on BiTOD. Human evaluations further confirm that LLMs integrated with RealTOD achieve superior task completion, fluency, and informativeness compared to existing methods.', 'abstract_zh': '面向任务的对话（TOD）系统通过自然语言帮助用户完成复杂的多轮任务。传统的方法依赖于在每个领域进行广泛的微调和标注数据，而指令调优的大语言模型（LLMs）则提供了一种更具灵活性的替代方案。然而，LLMs 在处理多轮任务完成方面仍存在挑战，尤其是在准确生成 API 调用和在没有明确示范的情况下适应新领域方面。为了应对这些挑战，我们提出了 RealTOD，这是一种通过提示链和细粒度反馈机制增强TOD系统的新型框架。提示链通过两阶段提示策略实现零样本领域的适应，从而消除对人类编写的示范的依赖。同时，细粒度反馈机制通过验证 API 调用与领域模式的一致性并在检测到错误时提供精准的纠正反馈，从而改善任务完成。我们在使用四个大语言模型进行广泛的实验中，使用了 SGD 和 BiTOD 基准测试集。结果表明，RealTOD 在提高 API 准确性方面优于 AutoTOD，在 SGD 中提高了 37.74%，在 BiTOD 中提高了 11.26%。进一步的人类评估表明，与现有方法相比，结合了 RealTOD 的大语言模型在任务完成、流畅性和信息量方面表现更优。', 'title_zh': '通过提示链式运用和细粒度反馈提高面向任务对话系统中的多轮任务完成性能'}
{'arxiv_id': 'arXiv:2502.13297', 'title': 'Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding', 'authors': 'Yunpeng Xiao, Youpeng Zhao, Kai Shu', 'link': 'https://arxiv.org/abs/2502.13297', 'abstract': 'Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\\% and 23.3\\%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at this https URL', 'abstract_zh': '自然语言理解（NLU）是一项使机器能够理解人类语言的任务。一些任务，如立场检测和情感分析，与个人的主观观点密切相关，因此被称为个体层次的NLU。以往，这些任务往往被简化为文本层次的NLU任务，忽略了个人因素的影响。这不仅使得推断变得困难且难以解释，还常常导致大量标签错误。为了解决上述限制，我们提出了一种基于个体因素的新NLU标注指南。具体来说，我们综合考虑了同一个体发布的其他帖子，然后在考虑到所有个体帖子的基础上标注个体的主观观点。我们使用该指南来扩展并重新标注立场检测和基于话题的情感分析数据集。我们发现，样本文本中的错误率高达31.7%和23.3%。进一步地，我们使用大型语言模型在重新标注的数据集上进行了实验，并发现加入个体因素后，大型语言模型在这两个数据集上的表现良好。GPT-4O和Llama3-70B均在重新标注的数据集上实现了超过87%的准确率。我们还通过消融研究验证了个体因素的有效性。我们呼吁未来的研究人员在创建此类数据集时加入个体因素。我们的重新标注数据集可通过以下网址获取：[提供网址]', 'title_zh': '理解并解决个体水平自然语言理解中的标签错误'}
{'arxiv_id': 'arXiv:2502.13278', 'title': 'Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models', 'authors': 'Sirisha Velampalli, Chandrashekar Muniyappa, Ashutosh Saxena', 'link': 'https://arxiv.org/abs/2502.13278', 'abstract': 'Emojis are being frequently used in todays digital world to express from simple to complex thoughts more than ever before. Hence, they are also being used in sentiment analysis and targeted marketing campaigns. In this work, we performed sentiment analysis of Tweets as well as on emoji dataset from the Kaggle. Since tweets are sentences we have used Universal Sentence Encoder (USE) and Sentence Bidirectional Encoder Representations from Transformers (SBERT) end-to-end sentence embedding models to generate the embeddings which are used to train the Standard fully connected Neural Networks (NN), and LSTM NN models. We observe the text classification accuracy was almost the same for both the models around 98 percent. On the contrary, when the validation set was built using emojis that were not present in the training set then the accuracy of both the models reduced drastically to 70 percent. In addition, the models were also trained using the distributed training approach instead of a traditional singlethreaded model for better scalability. Using the distributed training approach, we were able to reduce the run-time by roughly 15% without compromising on accuracy. Finally, as part of explainable AI the Shap algorithm was used to explain the model behaviour and check for model biases for the given feature set.', 'abstract_zh': '在当今的数字世界中，表情符号被频繁用于表达从简单到复杂的各种想法，因此它们也被应用于情感分析和针对性的营销活动。在本研究中，我们对推文和来自Kaggle的表情符号数据集进行了情感分析。由于推文是句子形式，我们使用了全局句子编码器（USE）和双向Transformer编码器表示（SBERT）端到端的句子嵌入模型来生成嵌入，这些嵌入用于训练标准的全连接神经网络（NN）和LSTM NN模型。我们观察到，两种模型的文本分类准确性基本相同，大约在98%左右。相反，当使用在训练集中未出现的表情符号构建验证集时，两种模型的准确性大幅下降至70%左右。此外，我们还使用了分布式训练方法，而不是传统的单线程模型，提高了模型的可扩展性。采用分布式训练方法，我们在不牺牲准确性的前提下，将运行时间减少了约15%。最后，在可解释的人工智能方面，我们使用Shap算法来解释模型行为，并检查给定特征集下的模型偏见。', 'title_zh': '使用端到端、迁移学习、分布式和可解释人工智能模型对文本和 emoji 数据进行情感分析的性能评估'}
{'arxiv_id': 'arXiv:2502.13270', 'title': 'REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation', 'authors': 'Dong-Ho Lee, Adyasha Maharana, Jay Pujara, Xiang Ren, Francesco Barbieri', 'link': 'https://arxiv.org/abs/2502.13270', 'abstract': 'Long-term, open-domain dialogue capabilities are essential for chatbots aiming to recall past interactions and demonstrate emotional intelligence (EI). Yet, most existing research relies on synthetic, LLM-generated data, leaving open questions about real-world conversational patterns. To address this gap, we introduce REALTALK, a 21-day corpus of authentic messaging app dialogues, providing a direct benchmark against genuine human interactions.\nWe first conduct a dataset analysis, focusing on EI attributes and persona consistency to understand the unique challenges posed by real-world dialogues. By comparing with LLM-generated conversations, we highlight key differences, including diverse emotional expressions and variations in persona stability that synthetic dialogues often fail to capture.\nBuilding on these insights, we introduce two benchmark tasks: (1) persona simulation where a model continues a conversation on behalf of a specific user given prior dialogue context; and (2) memory probing where a model answers targeted questions requiring long-term memory of past interactions.\nOur findings reveal that models struggle to simulate a user solely from dialogue history, while fine-tuning on specific user chats improves persona emulation. Additionally, existing models face significant challenges in recalling and leveraging long-term context within real-world conversations.', 'abstract_zh': '长期、开放域的对话能力对于聊天机器人而言至关重要，因为这使其能够回忆过去的交互并展现情感智能（EI）。然而，现有的大多数研究依赖于合成的、由大语言模型生成的数据，留下了关于真实世界对话模式的疑问。为了解决这一问题，我们引入了REALTALK，这是一个为期21天的聊天应用对话语料库，它为直接与真实人类交互进行基准测试提供了可能。\n\n我们首先对数据集进行了分析，重点关注EI属性和角色一致性，以了解真实世界对话所提出的不同挑战。通过与大语言模型生成的对话进行比较，我们指出了关键差异，包括多样的情感表达和角色稳定性变化，而这些变化往往是合成对话所无法捕捉到的。\n\n基于这些见解，我们引入了两个基准任务：(1) 角色模拟，模型根据先前的对话背景代表特定用户继续对话；(2) 记忆探测，模型回答需要利用过去交互长期记忆的问题。\n\n我们的研究发现，模型难以仅凭对话历史来模拟用户，而特定用户对话的微调则能改善角色模仿。此外，现有模型在真实世界对话中回忆和利用长期背景方面面临着重大挑战。', 'title_zh': 'REALTALK：一个21天的真实世界长效对话数据集'}
{'arxiv_id': 'arXiv:2502.13260', 'title': 'Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models', 'authors': 'Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Yue Xing, Jiliang Tang, Qi He', 'link': 'https://arxiv.org/abs/2502.13260', 'abstract': 'Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.', 'abstract_zh': '链式推理（Chain-of-Thought, CoT），将复杂任务分解为中间推理步骤，极大地提高了大型语言模型（LLMs）在挑战性任务上的性能。然而，CoT 中详细推理过程往往导致生成时间较长和计算成本较高，部分原因是包含了不必要的步骤。为解决这一问题，我们提出了一种方法，使用困惑度作为其重要性的度量来识别关键推理步骤：如果移除某步骤会导致困惑度显著增加，则该步骤被认为是关键步骤。该方法使得模型能够专注于生成这些关键步骤。这一目标可以通过两种途径实现：在少样本链式推理中精细化演示示例，或通过选择仅包含关键步骤的示例对模型进行微调。全面的实验验证了我们方法的有效性，该方法在推理准确性和效率之间取得了更好的平衡。', 'title_zh': '逐步 perplexity 引导的精炼方法以提高大型语言模型中的高效思维链推理效率'}
{'arxiv_id': 'arXiv:2502.13259', 'title': 'HumT DumT: Measuring and controlling human-like language in LLMs', 'authors': 'Myra Cheng, Sunny Yu, Dan Jurafsky', 'link': 'https://arxiv.org/abs/2502.13259', 'abstract': 'Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to overreliance and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HumT and SocioT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HumT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs. HumT also offers insights into the impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DumT, a method using HumT to systematically control and reduce the degree of human-like tone while preserving model performance. DumT offers a practical approach for mitigating risks associated with anthropomorphic language generation.', 'abstract_zh': '以下是对论文内容或标题的中文翻译，符合学术规范：\n\n大语言模型（LLM）应该生成人类听起来的语言吗？人类语言可能会改善用户体验，但也可能导致过度依赖和刻板印象。评估这些潜在影响需要一种系统的方法来衡量LLM输出中的类人类语气。我们引入了HumT和SocioT，基于LLM相对概率的数据文本中类人类语气和社会认知的其他维度的度量标准。通过在偏好和使用数据集中测量HumT，我们发现用户更倾向于LLM生成的较少类人类的输出。HumT还揭示了拟人化的影响：类人类的LLM输出与温暖、社交亲近感、女性化和低地位高度相关，这些特征与前述伤害密切相关。我们引入了DumT，这是一种使用HumT系统地控制和减少类人类语气度数的方法，同时保持模型性能。DumT为缓解拟人化语言生成相关风险提供了实用方法。', 'title_zh': 'HumT DumT：评估和控制大型语言模型中的类人类语言能力'}
{'arxiv_id': 'arXiv:2502.13252', 'title': 'Multilingual Language Model Pretraining using Machine-translated Data', 'authors': 'Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, David Adelani, Yihong Chen, Raphael Tang, Pontus Stenetorp', 'link': 'https://arxiv.org/abs/2502.13252', 'abstract': 'High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the quality and diversity of the available multilingual pretraining corpora. In this work, we find that machine-translated texts from a single high-quality source language can contribute significantly to the pretraining quality of multilingual LLMs. We translate FineWeb-Edu, a high-quality English web dataset, into nine languages, resulting in a 1.7-trillion-token dataset, which we call TransWebEdu and pretrain a 1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine non-English reasoning tasks, we show that TransWebLLM matches or outperforms state-of-the-art multilingual models trained using closed data, such as Llama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We demonstrate that adding less than 5% of TransWebEdu as domain-specific pretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian, Swahili, and Welsh understanding and commonsense reasoning tasks. To promote reproducibility, we release our corpus, models, and training pipeline under Open Source Initiative-approved licenses.', 'abstract_zh': '高资源语言如英语，使得高质量大型语言模型（LLMs）的预训练成为可能。相比之下，大多数其他语言则不然，因为非英语语言的LLMs仍然表现不佳，这很可能是由于可用于多语言预训练语料库的质量和多样性不足导致的。本研究中，我们发现来自单一高质量源语言的机器翻译文本能够显著提高多语言LLMs的预训练质量。我们将一个高质量的英语网络数据集FineWeb-Edu翻译成九种语言，最终获得一个包含1.7万亿词汇单位的数据集，我们将其命名为TransWebEdu，并从头开始在该数据集上预训练了一个具有1.3亿参数的模型TransWebLLM。在九个非英语推理任务中，我们展示了尽管使用的数据量比闭源数据训练的最先进的多语言模型（如Llama3.2、Qwen2.5和Gemma）少一个数量级，TransWebLLM仍然能够匹敌或超越这些模型的表现。我们证明，添加不到5%的TransWebEdu作为领域特定的预训练数据集，在阿拉伯语、意大利语、印度尼西亚语、斯瓦希里语和威尔士语的理解以及常识推理任务上均达到了新的最先进水平。为促进可再现性，我们按照Open Source Initiative认可的许可证发布了我们的语料库、模型和训练管道。', 'title_zh': '使用机器翻译数据进行多语言语言模型预训练'}
{'arxiv_id': 'arXiv:2502.13251', 'title': 'Neural Attention Search', 'authors': 'Difan Deng, Marius Lindauer', 'link': 'https://arxiv.org/abs/2502.13251', 'abstract': "We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.", 'abstract_zh': '我们提出了 Neural Attention Search (NAtS)，这是一种框架，能够自动评估序列中每个token的重要性，并确定在几轮操作后是否可以删除相应的token。该方法可以在推理过程中高效地减少基于Transformer模型所需的KV缓存大小，从而降低推理成本。在本文中，我们设计了一个搜索空间，包含三种类型的token：（i）全局(token)将在后续所有token的查询中保留。 （ii）局部(token)将持续存在，直到下一个全局(token)出现为止。 （iii）滑动窗口(token)将对后续固定大小的token的推理产生影响。类似地，通过可学习的注意力掩码，这种token类型的信息可以与架构权重一同学习。实验表明，无论是从头训练新的Transformer模型还是对现有的大型语言模型进行微调，NAtS都能够高效地减少模型所需的KV缓存大小，同时保持模型的性能。', 'title_zh': '神经注意力搜索'}
{'arxiv_id': 'arXiv:2502.13247', 'title': 'Grounding LLM Reasoning with Knowledge Graphs', 'authors': 'Alfonso Amayuelas, Joy Sain, Simerjot Kaur, Charese Smiley', 'link': 'https://arxiv.org/abs/2502.13247', 'abstract': 'Knowledge Graphs (KGs) are valuable tools for representing relationships between entities in a structured format. Traditionally, these knowledge bases are queried to extract specific information. However, question-answering (QA) over such KGs poses a challenge due to the intrinsic complexity of natural language compared to the structured format and the size of these graphs. Despite these challenges, the structured nature of KGs can provide a solid foundation for grounding the outputs of Large Language Models (LLMs), offering organizations increased reliability and control.\nRecent advancements in LLMs have introduced reasoning methods at inference time to improve their performance and maximize their capabilities. In this work, we propose integrating these reasoning strategies with KGs to anchor every step or "thought" of the reasoning chains in KG data. Specifically, we evaluate both agentic and automated search methods across several reasoning strategies, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), using GRBench, a benchmark dataset for graph reasoning with domain-specific graphs. Our experiments demonstrate that this approach consistently outperforms baseline models, highlighting the benefits of grounding LLM reasoning processes in structured KG data.', 'abstract_zh': '知识图谱（Knowledge Graphs, KGs）是用于以结构化格式表示实体之间关系的重要工具。传统上，这些知识库被用来查询以提取特定信息。然而，针对此类KG进行问答（Question-Answering, QA）具有挑战性，因为自然语言的内在复杂性远高于结构化格式，并且这些图的规模庞大。尽管存在这些挑战，KG的结构化特性可以为大型语言模型（Large Language Models, LLMs）的输出提供坚实的基础，从而提高组织的可靠性和控制力。\n\n近年来，LLM的发展引入了推理方法，以便在推理过程中改善其性能并最大化其能力。在本工作中，我们提出将这些推理策略与KG集成，确保推理链中的每一“步骤”或“思考”都能在KG数据中找到依据。具体而言，我们利用GRBench基准数据集评估了针对特定领域图推理的不同推理策略，包括思维链（Chain-of-Thought, CoT）、思维树（Tree-of-Thought, ToT）和思维图（Graph-of-Thought, GoT）的自动搜索和代理搜索方法。我们的实验表明，这种方法在所有情况下都优于基准模型，突出了将LLM的推理过程锚定在结构化KG数据中的益处。', 'title_zh': '将知识图谱融合到大规模语言模型推理中'}
{'arxiv_id': 'arXiv:2502.13246', 'title': 'When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models', 'authors': 'Julia Mendelsohn, Ceren Budak', 'link': 'https://arxiv.org/abs/2502.13246', 'abstract': 'Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. "water" or "vermin"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.', 'abstract_zh': '比喻，即将一个概念描述为另一个概念，广泛存在于政治领域并能影响人们对重要问题的理解。我们提出了一种计算方法来衡量比喻语言，并专注于社交媒体上的移民话语。基于定性社会科学研究，我们识别出七种在移民话语中被唤起的概念（例如“水”或“害虫”）。我们提出了一个新颖的技术，该技术利用词级和文档级信号来衡量这些概念相关的比喻。随后，我们在40万条关于移民的美国推文中研究了比喻、政治意识形态和用户参与度之间的关系。虽然保守派比自由派更倾向于使用去人性化比喻，但这种差异因概念而异。此外，与生物体相关的比喻与更多的转发相关，特别是在自由派作者的作品中。我们的研究突显了计算方法在理解政治话语中微妙且隐含的语言方面的潜在价值，这些方法可以补充定性方法。', 'title_zh': '当人们变成洪水：使用大规模语言模型分析移民 discourse 中的去人性化隐喻'}
{'arxiv_id': 'arXiv:2502.13233', 'title': 'SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?', 'authors': 'Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu', 'link': 'https://arxiv.org/abs/2502.13233', 'abstract': "Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.", 'abstract_zh': '大型语言模型（LLMs）在通用领域展现了卓越的能力，但在需要特定知识的任务上 often struggle 往往表现出不足。传统的检索增强生成（RAG）技术通常从静态知识库中检索外部信息，这些知识库可能过时或不完整，无法提供准确医疗问答所需的精细临床细节。在本工作中，我们提出了一种名为SearchRAG的新框架，该框架通过利用实时搜索引擎来克服这些限制。我们的方法采用合成查询生成将复杂的医疗问题转换为搜索引擎友好的查询，并利用基于不确定性知识选择来过滤和整合与LLM输入最相关和最有信息价值的医疗知识。实验结果表明，我们的方法在医疗问答任务中显著提高了响应准确性，特别是在需要详细和最新知识的复杂问题上。', 'title_zh': 'SearchRAG：搜索引擎能在基于大规模语言模型的医疗问答中提供帮助吗？'}
{'arxiv_id': 'arXiv:2502.13207', 'title': 'Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation', 'authors': 'Giorgio Franceschelli, Mirco Musolesi', 'link': 'https://arxiv.org/abs/2502.13207', 'abstract': 'Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.', 'abstract_zh': '尽管大型语言模型在创意任务中的应用日益增多，但它们的输出往往缺乏多样性。常见的解决方案，如在更高的温度下采样，可能会损害结果的质量。基于信息论，我们提出一种基于上下文的评分方法，以定量评估价值和原创性。该评分方法激励准确性并遵循请求，同时鼓励偏离已学习的分布。我们提议将该评分方法作为奖励用于强化学习框架中，以细调大型语言模型以实现最佳性能。通过诗生成和数学问题求解实验验证了我们的策略，结果显示这种方法能够提升生成解决方案的价值和原创性。', 'title_zh': '超出（灰盒）之外的思考：一种基于上下文的评分方法，用于评估神经文本生成的价值与原创性'}
{'arxiv_id': 'arXiv:2502.13195', 'title': 'Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs', 'authors': 'Leonie Weissweiler, Kyle Mahowald, Adele Goldberg', 'link': 'https://arxiv.org/abs/2502.13195', 'abstract': "Linguistic evaluations of how well LMs generalize to produce or understand novel text often implicitly take for granted that natural languages are generated by symbolic rules. Grammaticality is thought to be determined by whether or not sentences obey such rules. Interpretation is believed to be compositionally generated by syntactic rules operating on meaningful words. Semantic parsing is intended to map sentences into formal logic. Failures of LMs to obey strict rules have been taken to reveal that LMs do not produce or understand language like humans. Here we suggest that LMs' failures to obey symbolic rules may be a feature rather than a bug, because natural languages are not based on rules. New utterances are produced and understood by a combination of flexible interrelated and context-dependent schemata or constructions. We encourage researchers to reimagine appropriate benchmarks and analyses that acknowledge the rich flexible generalizations that comprise natural languages.", 'abstract_zh': '对语言模型（LMs）生成或理解新文本能力的语言学评估往往默认自然语言是由符号规则生成的。句法正确性被认为是由句子是否遵循这些规则所决定的。语义解释通常被理解为由对有意义的词进行操作的句法规则按成分生成的结果。语义解析旨在将句子映射到形式逻辑。LMs未能严格遵守规则的情况被视为揭示了它们不像人类那样生成或理解语言。我们建议，人们不应将LMs违背符号规则的行为视为错误，而是认为LMs未能遵循规则可能是其特征之一，因为自然语言并非基于规则生成。新的话语产生和理解是由灵活的相关且依赖上下文的框架或构造组合实现的。我们鼓励研究者重新构想能反映自然语言丰富灵活的一般化的适当基准和分析方法。', 'title_zh': '语言概括不是规则：对语言模型评估的影响'}
{'arxiv_id': 'arXiv:2502.13193', 'title': 'Private Text Generation by Seeding Large Language Model Prompts', 'authors': 'Supriya Nagesh, Justin Y. Chen, Nina Mishra, Tal Wagner', 'link': 'https://arxiv.org/abs/2502.13193', 'abstract': 'We explore how private synthetic text can be generated by suitably prompting a large language model (LLM). This addresses a challenge for organizations like hospitals, which hold sensitive text data like patient medical records, and wish to share it in order to train machine learning models for medical tasks, while preserving patient privacy. Methods that rely on training or finetuning a model may be out of reach, either due to API limits of third-party LLMs, or due to ethical and legal prohibitions on sharing the private data with the LLM itself.\nWe propose Differentially Private Keyphrase Prompt Seeding (DP-KPS), a method that generates a private synthetic text corpus from a sensitive input corpus, by accessing an LLM only through privatized prompts. It is based on seeding the prompts with private samples from a distribution over phrase embeddings, thus capturing the input corpus while achieving requisite output diversity and maintaining differential privacy. We evaluate DP-KPS on downstream ML text classification tasks, and show that the corpora it generates preserve much of the predictive power of the original ones. Our findings offer hope that institutions can reap ML insights by privately sharing data with simple prompts and little compute.', 'abstract_zh': '我们探讨了如何通过适当提示大型语言模型（LLM）来生成私有合成文本。这解决了医院等机构的一个挑战，这些机构拥有敏感的文本数据，如患者医疗记录，并希望分享这些数据以训练医疗任务的机器学习模型，同时保护患者隐私。依赖于训练或微调模型的方法可能由于第三方LLM的API限制或伦理与法律禁止与LLM共享私有数据而难以实现。\n\n我们提出了一种基于差异隐私关键短语提示种子的生成方法（DP-KPS），该方法通过仅使用私有化提示访问LLM，从而从敏感输入语料库生成私有合成文本语料库。该方法基于将提示种子与短语嵌入的概率分布中的私有样本相结合，从而捕捉输入语料库并实现所需的输出多样性，同时保持差异隐私。我们在下游机器学习文本分类任务上评估了DP-KPS，并展示了它生成的语料库保留了原始语料库的大部分预测能力。我们的研究结果表明，机构可以通过简单的提示和少量计算来私有地共享数据并从中获得机器学习洞察。', 'title_zh': '通过种子大规模语言模型提示进行私有文本生成'}
{'arxiv_id': 'arXiv:2502.13943', 'title': 'AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence', 'authors': 'Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, Chuheng Zhang, Wei Shen, Zhouhan Lin', 'link': 'https://arxiv.org/abs/2502.13943', 'abstract': "Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size. These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities.", 'abstract_zh': '当前用于训练过程奖励模型（PRMs）的方法往往涉及使用基于规则的技术，如使用预定义的占位符标记或将推理步骤的长度固定为特定大小，将响应分解为多个推理步骤。这些方法忽视了特定单词通常不标志着文本中的实际决策点这一事实。为了解决这一问题，我们提出了一种名为AdaptiveStep的方法，该方法根据模型预测下一个单词的信心程度划分推理步骤。这种划分方法在每个步骤中提供了更多的决策信息，从而增强了下游任务，如奖励模型学习的效能。此外，我们的方法无需手动标注。我们通过在数学推理和代码生成任务中使用AdaptiveStep训练的PRMs进行实验，证明了其有效性。实验结果表明，所得PRM在Best-of-N性能上达到了最先进的水平，优于基于标记级价值引导解码的贪婪搜索策略，并且与现有的开源PRMs相比，构建成本降低了30%以上。此外，我们还对PRM的性能、可迁移性和泛化能力进行了详尽的分析和案例研究。', 'title_zh': 'AdaptiveStep: 通过模型自信度自动分割推理步骤'}
{'arxiv_id': 'arXiv:2502.13928', 'title': 'Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images', 'authors': 'Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber', 'link': 'https://arxiv.org/abs/2502.13928', 'abstract': "Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM's visually-dependent task performance while retaining or even improving the model's general abilities. We opensource our code at this https URL", 'abstract_zh': '近年来的研究表明，大型多模态模型（特别是视觉-语言模型，VLMs）往往忽视图像内容，过度依赖语言模型先验，导致视觉导向任务中的错误以及幻觉。我们认为这一问题的根源在于现有VLMs没有明确训练来生成准确反映细粒度图像细节的文本。为了在VLM训练期间增强视觉反馈，我们提出了一种新颖的微调目标S-VCO（对称视觉对比优化），该目标引导模型捕捉重要视觉细节并与其相应的文本标记对齐。为了进一步促进这种细粒度对齐，我们引入了MVC数据集，该数据集通过自动过滤和放大视觉逆事实数据来构建，以挑战模型解决涉及最小视觉对比的情况。实验表明，我们的方法在涵盖各种能力和领域的多种基准测试中一致地提高了VLM性能，实现了幻觉最多减少22%的效果，并在视觉导向和通用任务上取得了显著进展。值得注意的是，这些改进在具有更高视觉依赖性的基准测试中表现得更为明显。简言之，S-VCO显著提升了VLM在视觉导向任务中的表现，同时保持或甚至提升了模型的一般能力。我们已将代码开源，地址为：this https URL', 'title_zh': '对称视觉对比优化：通过最少对比图像对齐视觉语言模型'}
{'arxiv_id': 'arXiv:2502.13923', 'title': 'Qwen2.5-VL Technical Report', 'authors': 'Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.13923', 'abstract': 'We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.', 'abstract_zh': '我们介绍了Qwen2.5-VL，这是Qwen视觉-语言系列的最新旗舰模型，展示了在基础能力和创新功能方面的重要进步。Qwen2.5-VL 在通过增强的视觉识别、精确的对象定位、稳健的文档解析和长时间视频理解来理解和交互于世界方面取得了显著的进步。Qwen2.5-VL 的一个亮点在于其能够使用边界框或点精确地定位对象。它能够稳健地从发票、表格和图示中提取结构化数据，并对图表、示意图和布局进行详细的分析。为了处理复杂输入，Qwen2.5-VL 引入了动态分辨率处理和绝对时间编码，使其能够处理不同大小的图像和长达数小时的视频，并具备秒级事件定位的能力。这使得模型能够原生地感知空间比例和时间动态，而无需依赖传统的归一化技术。通过从零开始训练原生动态分辨率的视觉Transformer（ViT），并结合窗口注意机制，我们减少了计算开销同时保持了原生分辨率。因此，Qwen2.5-VL 不仅在静止图像和文档理解方面表现出色，还能够作为能够进行推理、工具使用和任务执行的交互式视觉代理，在诸如操作计算机和移动设备等实际场景中也能自如应对。Qwen2.5-VL 提供了三种不同的大小，适用于从边缘人工智能到高性能计算的各种应用场景。旗舰模型Qwen2.5-VL-72B 与最新的GPT-4o和Claude 3.5 Sonnet等模型表现出色，在文档和图示理解方面尤为突出。此外，Qwen2.5-VL 保持了出色的语言性能，保留了Qwen2.5语言模型的核心语言能力。', 'title_zh': 'Qwen2.5-VL 技术报告'}
{'arxiv_id': 'arXiv:2502.13920', 'title': 'Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health', 'authors': 'Xingbo Wang, Janessa Griffith, Daniel A. Adler, Joey Castillo, Tanzeem Choudhury, Fei Wang', 'link': 'https://arxiv.org/abs/2502.13920', 'abstract': "Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.", 'abstract_zh': '尽管睡眠追踪设备普及，许多个人难以将数据转化为实际改善睡眠健康的措施。当前的方法虽能提供数据驱动的建议，但在实际生活限制和个体背景适应性方面可能不可行。我们提出了 HealthGuru，这是一种新型的大规模语言模型驱动的聊天机器人，旨在通过数据驱动、理论指导和适应性强的建议，结合对话行为改变支持来提升睡眠健康。HealthGuru 的多代理框架整合了可穿戴设备数据、上下文信息以及上下文多臂bandit模型，以提供个性化的睡眠改善活动建议。该系统支持自然对话，同时结合数据驱动的见解和行为改变理论方法。我们的八周实地部署研究，涉及16名参与者，将HealthGuru与基准聊天机器人进行了比较。结果表明，与基准聊天机器人相比，HealthGuru 在睡眠时长和活动评分等指标上有所提升，且响应质量更高，用户行为改变的积极性也有所提高。我们还识别出健康聊天机器人中个性化和用户参与设计方面的挑战与考虑。', 'title_zh': '通过数据驱动和理论指导的大语言模型探索个性化健康支持：以睡眠健康为例的研究'}
{'arxiv_id': 'arXiv:2502.13898', 'title': 'GroundCap: A Visually Grounded Image Captioning Dataset', 'authors': 'Daniel A. P. Oliveira, Lourenço Teodoro, David Martins de Matos', 'link': 'https://arxiv.org/abs/2502.13898', 'abstract': "Current image captioning systems lack the ability to link descriptive text to specific visual elements, making their outputs difficult to verify. While recent approaches offer some grounding capabilities, they cannot track object identities across multiple references or ground both actions and objects simultaneously. We propose a novel ID-based grounding system that enables consistent object reference tracking and action-object linking, and present GroundCap, a dataset containing 52,016 images from 77 movies, with 344 human-annotated and 52,016 automatically generated captions. Each caption is grounded on detected objects (132 classes) and actions (51 classes) using a tag system that maintains object identity while linking actions to the corresponding objects. Our approach features persistent object IDs for reference tracking, explicit action-object linking, and segmentation of background elements through K-means clustering. We propose gMETEOR, a metric combining caption quality with grounding accuracy, and establish baseline performance by fine-tuning Pixtral-12B. Human evaluation demonstrates our approach's effectiveness in producing verifiable descriptions with coherent object references.", 'abstract_zh': '当前的图像描述系统缺乏将描述性文本与特定的视觉元素关联起来的能力，导致其输出难以验证。尽管最近的方法提供了一定程度的语义关联能力，但它们仍然无法跨多个引用跟踪对象身份，也无法同时关联动作和对象。我们提出了一种新型的身份基于的语义关联系统，该系统能够实现一致的对象引用跟踪和动作-对象关联，并介绍了包含52,016张来自77部电影的图像、344个人工标注和52,016个自动生成的描述的GroundCap数据集。每个描述均通过标签系统基于检测到的对象（132类）和动作（51类）进行语义关联，该系统维护了对象身份并建立了动作与相应对象之间的联系。我们的方法具备持久的对象ID以实现引用跟踪、明确的动作-对象关联，并通过K-means聚类分割背景元素。我们提出了一种结合描述质量和语义关联准确性的gMETEOR指标，并通过微调Pixtral-12B模型建立了基线性能。基于人工评价表明，该方法能够在产生可验证描述的同时具备一致的对象引用。', 'title_zh': 'GroundCap：一种基于视觉的地基图像 captioning 数据集'}
{'arxiv_id': 'arXiv:2502.13870', 'title': 'SPEX: Scaling Feature Interaction Explanations for LLMs', 'authors': 'Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu', 'link': 'https://arxiv.org/abs/2502.13870', 'abstract': 'Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among interactions -- common in real-world data -- and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models.', 'abstract_zh': '大规模语言模型（LLMs）由于能够捕捉输入特征之间的复杂交互，而彻底改变了机器学习领域。诸如SHAP之类的流行事后解释方法提供的是边际特征归因，而其交互重要性扩展仅适用于较短的输入长度（约20个输入项）。我们提出了一种名为光谱解释器（SPEX）的模型无关的交互归因算法，该算法能够高效地扩展到长输入长度（约1000个输入项）。SPEX 利用输入交互中的潜在自然稀疏性——在实际数据中很常见，并运用通道解码算法结合稀疏傅里叶变换来高效地识别重要交互。我们在三个需要LLM利用输入之间的交互来完成任务的困难的长语境数据集上进行了实验。对于长输入，SPEX 在忠实地重建LLM输出方面比边际归因方法可提高最多20%的性能。此外，SPEX 能成功识别对模型输出影响显著的关键特征和交互。对于我们的其中一个数据集HotpotQA，SPEX 提供的交互与人类标注一致。最后，我们使用我们的模型无关方法生成解释，展示了闭源LLM（GPT-4o mini）的抽象推理能力和视觉语言模型的组合推理能力。', 'title_zh': 'SPEX：扩展大型语言模型特征交互解释'}
{'arxiv_id': 'arXiv:2502.13820', 'title': 'Scoring Verifiers: Evaluating Synthetic Verification in Code and Reasoning', 'authors': 'Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg', 'link': 'https://arxiv.org/abs/2502.13820', 'abstract': 'Code verification has recently found great success as a critical component in training large scale reasoning models for coding. Synthetic techniques such as self-generated test cases and reward models provide a way to enhance code capabilities beyond predefined tests. Building on these advancements, we propose new benchmarks designed to systematically evaluate the impact of synthetic verification methods on assessing solution correctness. We introduce HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. Using these benchmarks, we analyze synthetic verification methods in standard, reasoning-based, and reward-based LLMs. Our results show that recent reasoning models significantly improve test case generation and that scaling test cases enhances verification accuracy.', 'abstract_zh': '代码验证近年来在训练大规模推理模型方面取得了巨大的成功，将其作为关键组件用于编程任务。合成技术，如自动生成的测试案例和奖励模型，为提高代码能力提供了超出预定义测试的新途径。在这些进展的基础上，我们提出了新的基准测试，旨在系统地评估合成验证方法对评估解决方案正确性的影响。我们引入了HE-R、HE-R+、MBPP-R和MBPP-R+，将现有的编程基准转换为评分和排名数据集，以评估合成验证器的效果。利用这些基准测试，我们分析了标准、推理基于和奖励基于的语言模型（LLMs）中的合成验证方法。研究结果表明，近期的推理模型显著提高了测试案例生成的质量，而扩展测试案例则提高了验证的准确性。', 'title_zh': '评分验证器：评估代码与推理中的合成验证'}
{'arxiv_id': 'arXiv:2502.13811', 'title': 'On the Duality between Gradient Transformations and Adapters', 'authors': 'Lucas Torroba-Hennigen, Hunter Lang, Han Guo, Yoon Kim', 'link': 'https://arxiv.org/abs/2502.13811', 'abstract': "We study memory-efficient optimization of neural networks with linear gradient transformations, where the gradients are linearly mapped to a lower dimensional space than the full parameter space, thus saving memory required for gradient accumulation and optimizer state persistence. The model parameters are updated by first performing an optimization step in the lower dimensional space and then going back into the original parameter space via the linear map's transpose. We show that optimizing the model in this transformed space is equivalent to reparameterizing the original model through a linear adapter that additively modifies the model parameters, and then only optimizing the adapter's parameters. When the transformation is Kronecker-factored, this establishes an equivalence between GaLore and one-sided LoRA. We show that this duality between gradient transformations and adapter-based reparameterizations unifies existing approaches to memory-efficient training and suggests new techniques for improving training efficiency and memory use.", 'abstract_zh': '我们研究了具有线性梯度变换的神经网络的记忆高效优化方法，其中梯度被线性映射到比完整参数空间低维的空间中，从而节省了梯度累积和优化器状态持久化的内存需求。模型参数首先在低维空间中进行优化步骤，然后通过线性映射的转置回到原始参数空间。我们表明，在变换空间中优化模型等效于通过线性适配器重新参数化原始模型，该适配器通过加性修改模型参数，仅优化适配器的参数。当变换是克罗内克分解时，这将GaLore与单侧LoRA建立了等价关系。我们展示了梯度变换与基于适配器的重新参数化之间的这种二元性，统一了现有的高效训练方法，并为提高训练效率和内存使用提出了新的技术建议。', 'title_zh': '《梯度变换与适配器之间的对偶关系》'}
{'arxiv_id': 'arXiv:2502.13794', 'title': 'LESA: Learnable LLM Layer Scaling-Up', 'authors': 'Yifei Yang, Zouying Cao, Xinbei Ma, Yao Yao, Libo Qin, Zhi Chen, Hai Zhao', 'link': 'https://arxiv.org/abs/2502.13794', 'abstract': 'Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose \\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.', 'abstract_zh': '从头开始训练大规模语言模型（LLMs）需要巨大的计算资源，使其成为一种成本高昂的解决方案。通过利用较小模型的参数来创建较大的模型，模型规模扩展提供了一个有前景的解决方案。然而，现有的深度扩展方法依赖于经验启发式规则进行层复制，这会导致较差的初始化并减慢持续预训练的收敛速度。我们提出了一种名为 \\textbf{LESA} 的新颖可学习深度扩展方法。通过将每一层的参数连接起来并应用奇异值分解，我们揭示了各层之间的潜在模式，表明层间参数可以进行学习。LESA 使用神经网络预测相邻层之间插入的参数，从而实现更好的初始化并加快训练速度。实验表明，相比于现有基线方法，LESA 在持续预训练时以不到一半的计算成本获得了更好的性能。广泛的分析结果表明，其在不同模型规模和任务中均表现出有效性。', 'title_zh': 'LESA：可学习的大语言模型层扩展方法'}
{'arxiv_id': 'arXiv:2502.13721', 'title': 'Learning Novel Transformer Architecture for Time-series Forecasting', 'authors': 'Juyuan Zhang, Wei Zhu, Jiechao Gao', 'link': 'https://arxiv.org/abs/2502.13721', 'abstract': 'Despite the success of Transformer-based models in the time-series prediction (TSP) tasks, the existing Transformer architecture still face limitations and the literature lacks comprehensive explorations into alternative architectures. To address these challenges, we propose AutoFormer-TS, a novel framework that leverages a comprehensive search space for Transformer architectures tailored to TSP tasks. Our framework introduces a differentiable neural architecture search (DNAS) method, AB-DARTS, which improves upon existing DNAS approaches by enhancing the identification of optimal operations within the architecture. AutoFormer-TS systematically explores alternative attention mechanisms, activation functions, and encoding operations, moving beyond the traditional Transformer design. Extensive experiments demonstrate that AutoFormer-TS consistently outperforms state-of-the-art baselines across various TSP benchmarks, achieving superior forecasting accuracy while maintaining reasonable training efficiency.', 'abstract_zh': '尽管基于Transformer的模型在时间序列预测（TSP）任务中取得了成功，现有Transformer架构仍然存在局限性，相关文献中缺乏对替代架构的全面探索。为解决这些挑战，我们提出了一种新的框架——AutoFormer-TS，该框架利用全面的搜索空间，针对TSP任务定制Transformer架构。我们的框架引入了一种可微神经架构搜索（Differentiable Neural Architecture Search, DNAS）方法——AB-DARTS，这种方法在现有DNAS方法的基础上，通过优化架构中最佳操作的识别能力，提高了搜索效率。AutoFormer-TS系统地探索了替代注意力机制、激活函数和编码操作，超越了传统的Transformer设计。大量的实验表明，AutoFormer-TS在各种TSP基准测试中均优于现有最先进的基线方法，实现了更出色的预测精度，同时保持了合理的训练效率。', 'title_zh': '学习新型Transformer架构的时间序列预测'}
{'arxiv_id': 'arXiv:2502.13681', 'title': 'An LLM-based Agent for Reliable Docker Environment Configuration', 'authors': 'Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao', 'link': 'https://arxiv.org/abs/2502.13681', 'abstract': 'Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment "pollution" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.', 'abstract_zh': '环境配置是软件开发中一个关键但耗时的步骤，尤其是在处理不熟悉的代码仓库时。尽管大规模语言模型（LLMs）展示了完成软件工程任务的潜力，但现有的环境配置方法通常依赖于手动努力或脆弱的脚本，这导致了效率低下和不可靠的结果。我们引入了Repo2Run，这是第一个基于LLM的代理，旨在完全自动化环境配置并生成可执行的Dockerfile，适用于任意的Python仓库。我们解决了两大主要挑战：（1）使LLM代理能够在隔离的Docker容器内配置环境，以及（2）确保成功的配置过程被记录，并准确地转移到Dockerfile中而不会出错。为实现这一目标，我们提出了原子配置合成，其特征是双重环境架构（内部环境和外部环境），并具有回滚机制来防止因失败命令而导致的环境“污染”，从而保证原子执行（完全执行或根本不执行），以及Dockerfile生成器将成功的配置步骤转移到可运行的Dockerfile中。我们在一个包含420个具有单元测试的近期Python仓库的基准测试中评估了Repo2Run，结果显示其成功率达到86.0%，比最佳基线高出63.9%。', 'title_zh': '基于LLM的代理模型用于可靠配置Docker环境'}
{'arxiv_id': 'arXiv:2502.13632', 'title': 'Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization', 'authors': 'Or Raphael Bidusa, Shaul Markovitch', 'link': 'https://arxiv.org/abs/2502.13632', 'abstract': "The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer both interpretability and intervenability by incorporating explicit concept representations. However, these methods suffer from key limitations, including reliance on labeled concept datasets and significant architectural modifications that challenges re-integration into existing system pipelines. In this work, we introduce a new methodology for incorporating interpretability and intervenability into an existing model by integrating Concept Layers (CLs) into its architecture. Our approach projects the model's internal vector representations into a conceptual, explainable vector space before reconstructing and feeding them back into the model. Furthermore, we eliminate the need for a human-selected concept set by algorithmically searching an ontology for a set of concepts that can be either task-specific or task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions. Additionally, we present a proof of concept showcasing an intervenability interface, allowing users to adjust model behavior dynamically, such as mitigating biases during inference.", 'abstract_zh': '大型语言模型（LLMs）的不透明性导致了大量研究努力，旨在增强其可解释性，主要通过事后分析方法。近年来，概念瓶颈模型（CBMs）等在建模过程中引入解释性和干预性的方法提供了兼具解释性和干预性的途径，通过引入显式的概念表示。然而，这些方法面临一些关键限制，包括对标记概念数据集的依赖以及对现有系统管道的重大架构更改，这构成了重新整合的挑战。在此项工作中，我们提出了一种新的方法，通过将概念层（CLs）集成到现有模型的架构中，将其可解释性和干预性融合到模型中。我们的方法将模型的内部向量表示投影到一个概念化和可解释的向量空间中，然后进行重建并返回到模型中。此外，我们通过算法搜索本体来消除人类选定概念集的需求，并为这些概念集提供任务特定或任务无关的选择。我们在多个任务中评估了CLs，证明它们能够保持原始模型的性能和一致性，同时允许进行有意义的干预。此外，我们展示了概念层的一个概念验证界面，使用户能够动态调整模型行为，例如在推断过程中减轻偏差。', 'title_zh': '概念层：通过语言模型的概念化增强可解释性和可干预性'}
{'arxiv_id': 'arXiv:2502.13606', 'title': 'LaVCa: LLM-assisted Visual Cortex Captioning', 'authors': 'Takuya Matsuyama, Shinji Nishimoto, Yu Takagi', 'link': 'https://arxiv.org/abs/2502.13606', 'abstract': 'Understanding the property of neural populations (or voxels) in the human brain can advance our comprehension of human perceptual and cognitive processing capabilities and contribute to developing brain-inspired computer models. Recent encoding models using deep neural networks (DNNs) have successfully predicted voxel-wise activity. However, interpreting the properties that explain voxel responses remains challenging because of the black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven approach that uses large language models (LLMs) to generate natural-language captions for images to which voxels are selective. By applying LaVCa for image-evoked brain activity, we demonstrate that LaVCa generates captions that describe voxel selectivity more accurately than the previously proposed method. Furthermore, the captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts. These findings offer profound insights into human visual representations by assigning detailed captions throughout the visual cortex while highlighting the potential of LLM-based methods in understanding brain representations. Please check out our webpage at this https URL', 'abstract_zh': '理解人类大脑中神经群体（或体素）的特性可以提升我们对人类感知和认知处理能力的理解，并有助于开发类脑计算模型。最近使用深度神经网络（DNNs）的编码模型已经成功预测了体素级别的活动。然而，由于DNNs的黑箱性质，解释解释体素反应的特性仍然具有挑战性。为了解决这一问题，我们提出了一种基于数据的方法——LLM辅助的视觉皮层描述（LaVCa），该方法利用大型语言模型（LLMs）为对体素具有选择性的图像生成自然语言描述。通过将LaVCa应用于由图像诱发的大脑活动，我们表明LaVCa生成的描述体素选择性的描述比之前提出的方法更加准确。此外，LaVCa生成的描述在体素间和体素内层次上比现有方法更详细地捕捉到更多定量属性。进一步对LaVCa生成的体素特异性属性进行详细分析，揭示了感兴趣区域（ROIs）在视觉皮层内的微尺度功能分化，并揭示了同时代表多个独特概念的体素。这些发现为全面理解人类视觉表征提供了深刻的见解，同时突显了基于LLM的方法在理解大脑表征方面的潜力。请访问我们的网页以获取更多信息：[该 URL 链接]', 'title_zh': 'LaVCa：LLM辅助的视觉皮层描述'}
{'arxiv_id': 'arXiv:2502.13568', 'title': 'LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation', 'authors': 'Xin Li, Anand Sarwate', 'link': 'https://arxiv.org/abs/2502.13568', 'abstract': 'Imposing an effective structural assumption on neural network weight matrices has been the major paradigm for designing Parameter-Efficient Fine-Tuning (PEFT) systems for adapting modern large pre-trained models to various downstream tasks. However, low rank based adaptation has become increasingly challenging due to the sheer scale of modern large language models. In this paper, we propose an effective kernelization to further reduce the number of parameters required for adaptation tasks. Specifically, from the classical idea in numerical analysis regarding matrix Low-Separation-Rank (LSR) representations, we develop a kernel using this representation for the low rank adapter matrices of the linear layers from large networks, named the Low Separation Rank Adaptation (LSR-Adapt) kernel. With the ultra-efficient kernel representation of the low rank adapter matrices, we manage to achieve state-of-the-art performance with even higher accuracy with almost half the number of parameters as compared to conventional low rank based methods. This structural assumption also opens the door to further GPU-side optimizations due to the highly parallelizable nature of Kronecker computations.', 'abstract_zh': '对神经网络权重矩阵施加有效的结构性假设一直是设计参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）系统以适应现代大规模预训练模型应用于各种下游任务的主要 paradigm。然而，由于现代大规模语言模型的规模庞大，基于低秩的适应变得越来越具有挑战性。在本文中，我们提出了一种有效的核化方法，以进一步减少适应任务所需的参数数量。具体来说，我们从数值分析中经典的矩阵低分离秩（Low-Separation-Rank，LSR）表示法的经典思想出发，为大规模网络中的线性层的低秩适配矩阵开发了一个核，命名为低分离秩适配（LSR-Adapt）核。凭借这种超高效的低秩适配矩阵核表示法，我们能够在比传统基于低秩的方法少近一半参数的情况下，仍然实现最先进的性能，并获得更高的准确率。这一结构性假设还由于克罗内克乘法的高度并行化特性，为进一步的GPU端优化打开了大门。', 'title_zh': 'LSR-Adapt: 矩阵低分离秩内核自适应的超高效参数调优'}
{'arxiv_id': 'arXiv:2502.13533', 'title': 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models', 'authors': 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou', 'link': 'https://arxiv.org/abs/2502.13533', 'abstract': 'Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).', 'abstract_zh': '大规模语言模型（LLMs）在自然语言处理领域取得了显著进展，展现了出色的任务泛化能力。低秩适应（LoRA）提供了一种成本效益高的微调解决方案，即冻结原始模型参数，仅训练轻量级的低秩适配器矩阵。然而，LoRA的内存占用主要由原始模型参数决定。为缓解这一问题，我们提出了一种名为LoRAM的记忆高效LoRA训练方案。该方案基于这样一个直觉：在高参数量的LLMs中，许多神经元在训练中的效用较低，但对推理至关重要。LoRAM的独特之处在于，它在一个精简的（小型）模型上进行训练以获得精简的低秩矩阵，然后使用这些精简的矩阵与原始（大型）模型进行推理。此外，先验进行的低成本连续预训练（由模型发布者提前完成）能够对齐精简模型和原始模型之间的知识差异。我们的广泛实验表明，LoRAM在各种剪枝策略和下游任务中具有良好的效果。对于一个拥有700亿参数的模型，LoRAM只需20G HBM的GPU内存即可进行训练，替代了用于LoRA训练的A100-80G GPU以及用于全量微调的15个GPU。具体而言，通过有结构的剪枝结合4比特量化实现的QLoRAM，在LLaMA-3.1-70B（LLaMA-2-70B）模型上，将低秩矩阵训练中占据内存使用的参数存储成本降低了15.81倍（16.95倍），同时在性能上优于原始的LLaMA-3.1-70B（LLaMA-2-70B）和LoRA训练的LLaMA-3.1-8B（LLaMA-2-13B）。', 'title_zh': '从小模型训练，到大模型推理：高效内存的LoRA训练方法'}
{'arxiv_id': 'arXiv:2502.13465', 'title': 'HawkBench: Investigating Resilience of RAG Methods on Stratified Information-Seeking Tasks', 'authors': 'Hongjin Qian, Zheng Liu, Chao Gao, Yankai Wang, Defu Lian, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2502.13465', 'abstract': 'In real-world information-seeking scenarios, users have dynamic and diverse needs, requiring RAG systems to demonstrate adaptable resilience. To comprehensively evaluate the resilience of current RAG methods, we introduce HawkBench, a human-labeled, multi-domain benchmark designed to rigorously assess RAG performance across categorized task types. By stratifying tasks based on information-seeking behaviors, HawkBench provides a systematic evaluation of how well RAG systems adapt to diverse user needs.\nUnlike existing benchmarks, which focus primarily on specific task types (mostly factoid queries) and rely on varying knowledge bases, HawkBench offers: (1) systematic task stratification to cover a broad range of query types, including both factoid and rationale queries, (2) integration of multi-domain corpora across all task types to mitigate corpus bias, and (3) rigorous annotation for high-quality evaluation.\nHawkBench includes 1,600 high-quality test samples, evenly distributed across domains and task types. Using this benchmark, we evaluate representative RAG methods, analyzing their performance in terms of answer quality and response latency. Our findings highlight the need for dynamic task strategies that integrate decision-making, query interpretation, and global knowledge understanding to improve RAG generalizability. We believe HawkBench serves as a pivotal benchmark for advancing the resilience of RAG methods and their ability to achieve general-purpose information seeking.', 'abstract_zh': '在现实生活中的信息查询场景中，用户的需求是动态和多样的，要求RAG系统具备强大的适应性。为了全面评估当前RAG方法的适应能力，我们引入了HawkBench，这是一种经过人工标注的多领域基准测试，可严格评估分类任务类型中RAG性能。通过基于信息查询行为对任务进行分层，HawkBench 提供了一种系统的方法来评估RAG系统如何适应多样的用户需求。\n\n不同于现有的基准测试主要集中在特定任务类型（主要是事实查询）且依赖于不同的知识库，HawkBench 提供了以下几点改进：\n1. 系统的任务分层，以涵盖广泛的查询类型，包括事实查询和推理查询。\n2. 在所有任务类型中整合多领域语料库，以减轻语料库偏差。\n3. 严格的标注以确保高质量的评估。\n\nHawkBench 包含1,600个高质量的测试样本，均匀分布在各个领域和任务类型中。利用该基准测试，我们评估了代表性的RAG方法，并从答案质量和响应延迟两个方面分析了它们的性能。我们的研究结果强调了在改善RAG泛化能力方面，需要动态的任务策略及其对决策制定、查询解释和全局知识理解的整合。我们相信，HawkBench 是推动RAG方法的鲁棒性及其实现通用信息检索能力的重要基准。', 'title_zh': 'HawkBench: 探究RAG方法在分层信息检索任务中的鲁棒性'}
{'arxiv_id': 'arXiv:2502.13447', 'title': 'Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning', 'authors': 'Yang Yan, Bingqing Yue, Qiaxuan Li, Man Huang, Jingyu Chen, Zhenzhong Lan', 'link': 'https://arxiv.org/abs/2502.13447', 'abstract': "The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly injecting medical knowledge into the learning process affects the performance of cross-modality classification, focusing on Chest X-ray (CXR) images. We introduce a novel Set Theory-based knowledge injection framework that generates captions for CXR images with controllable knowledge granularity. Using this framework, we fine-tune CLIP model on captions with varying levels of medical information. We evaluate the model's performance through zero-shot classification on the CheXpert dataset, a benchmark for CXR classification. Our results demonstrate that injecting fine-grained medical knowledge substantially improves classification accuracy, achieving 72.5\\% compared to 49.9\\% when using human-generated captions. This highlights the crucial role of domain-specific knowledge in medical cross-modality learning. Furthermore, we explore the influence of knowledge density and the use of domain-specific Large Language Models (LLMs) for caption generation, finding that denser knowledge and specialized LLMs contribute to enhanced performance. This research advances medical image analysis by demonstrating the effectiveness of knowledge injection for improving automated CXR classification, paving the way for more accurate and reliable diagnostic tools.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\n将人工智能集成到医学影像中展现了巨大的潜力，但预训练知识与跨模态学习性能之间的关系尚不明确。本研究探讨了明确将医学知识注入学习过程如何影响跨模态分类性能，重点关注胸部X光（CXR）图像。我们提出了一种基于集合论的新型知识注入框架，用于生成具有可控医学知识粒度的CXR图像描述。利用该框架，我们分别对具有不同医学信息量的描述进行了CLIP模型的微调。我们通过使用CheXpert数据集（一个CXR分类的基准）进行零样本分类来评估模型的性能。结果显示，注入详细的医学知识显著提高了分类准确性，使用含有医学信息描述时的准确率为72.5%，而使用人工生成描述时的准确率为49.9%。这突显了医学特定知识在医学跨模态学习中的关键作用。此外，我们探讨了知识密度以及使用领域特定的大规模语言模型（LLMs）对描述生成的影响，发现更密集的知识和专门化的LLMs有助于增强性能。这项研究通过证明知识注入对于提高自动化CXR分类的有效性，推进了医学影像分析，为更准确和可靠诊断工具的发展铺平了道路。', 'title_zh': '通过跨模态学习中的知识注入提高胸部X光分类效果'}
{'arxiv_id': 'arXiv:2502.13398', 'title': '$\\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization', 'authors': 'Vishal Dey, Xiao Hu, Xia Ning', 'link': 'https://arxiv.org/abs/2502.13398', 'abstract': "Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce $\\mathtt{MoMUInstruct}$, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging $\\mathtt{MoMUInstruct}$, we develop $\\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that $\\mathtt{GeLLM^3O}$s consistently outperform state-of-the-art baselines. $\\mathtt{GeLLM^3O}$s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of $\\mathtt{GeLLM^3O}$s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. $\\mathtt{MoMUInstruct}$, models, and code are accessible through this https URL.", 'abstract_zh': '尽管近年来取得了一定进展，大多数用于分子优化的计算方法仍然局限于单属性或双属性优化任务，并且在处理新的优化任务时表现出了较差的可扩展性和泛化能力。与此同时，大型语言模型（LLMs）展现了出域的强大泛化能力，可以应用于新的任务。为了展示LLMs在分子优化中的潜力，我们引入了$\\mathtt{MoMUInstruct}$，这是第一个专注于复杂多属性分子优化任务的高质量指令微调数据集。利用$\\mathtt{MoMUInstruct}$，我们开发了$\\mathtt{GeLLM^3O}$系列指令微调的LLMs，用于分子优化。跨5个领域任务和5个非领域任务的广泛评估表明，$\\mathtt{GeLLM^3O}$s在所有测试任务中都优于最先进的基线方法。$\\mathtt{GeLLM^3O}$s还展示了出色的零样本泛化能力，能够显著优于强大的闭源LLMs，面对未见过的任务时表现出色。这种强大的泛化能力表明$\\mathtt{GeLLM^3O}$s作为分子优化领域的基础模型具有巨大的潜力，可以有效应对新的优化任务而无需进行资源密集型的重新训练。$\\mathtt{MoMUInstruct}$数据集、模型和代码可以通过以下链接访问：[此处提供链接]。', 'title_zh': '$\\mathtt{GeLLM^3O}$：通用大型语言模型进行多属性分子优化'}
{'arxiv_id': 'arXiv:2502.13344', 'title': 'K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction', 'authors': 'Tassallah Abdullahi, Ioanna Gemou, Nihal V. Nayak, Ghulam Murtaza, Stephen H. Bach, Carsten Eickhoff, Ritambhara Singh', 'link': 'https://arxiv.org/abs/2502.13344', 'abstract': "Drug discovery is a complex and time-intensive process that requires identifying and validating new therapeutic candidates. Computational approaches using large-scale biomedical knowledge graphs (KGs) offer a promising solution to accelerate this process. However, extracting meaningful insights from large-scale KGs remains challenging due to the complexity of graph traversal. Existing subgraph-based methods are tailored to graph neural networks (GNNs), making them incompatible with other models, such as large language models (LLMs). We introduce K-Paths, a retrieval framework that extracts structured, diverse, and biologically meaningful paths from KGs. Integrating these paths enables LLMs and GNNs to effectively predict unobserved drug-drug and drug-disease interactions. Unlike traditional path-ranking approaches, K-Paths retrieves and transforms paths into a structured format that LLMs can directly process, facilitating explainable reasoning. K-Paths employs a diversity-aware adaptation of Yen's algorithm to retrieve the K shortest loopless paths between entities in an interaction query, prioritizing biologically relevant and diverse relationships. Our experiments on benchmark datasets show that K-Paths improves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on drug repurposing and 13.42 points on interaction severity prediction. We also show that Llama 70B achieves F1-score gains of 6.18 and 8.46 points, respectively. K-Paths also improves the supervised training efficiency of EmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining strong predictive performance. Beyond its scalability and efficiency, K-Paths uniquely bridges the gap between KGs and LLMs, providing explainable rationales for predicted interactions. These capabilities show that K-Paths is a valuable tool for efficient data-driven drug discovery.", 'abstract_zh': '药物发现是一个复杂且耗时的过程，需要识别和验证新的治疗候选物。利用大规模生物医学知识图谱（KGs）的计算方法为加速这一过程提供了一个有前途的解决方案。然而，从大规模KGs中提取有意义的见解仍然具有挑战性，因为图遍历的复杂性。现有的基于子图的方法是为图神经网络（GNNs）定制的，这使得它们无法与其他模型，如大规模语言模型（LLMs）兼容。我们介绍了一种名为K-Paths的检索框架，可以从KGs中提取结构化、多样且生物学相关的路径。将这些路径集成可以使得LLMs和GNNs有效地预测未观察到的药物-药物和药物-疾病相互作用。不同于传统的路径排名方法，K-Paths检索并转换路径为LLMs可以直接处理的结构化格式，从而促进可解释的推理。K-Paths采用一种多样化感知的Yen算法变种来检索查询中实体之间最短无环路径的K个，优先考虑生物学相关性和多样性关系。我们在基准数据集上的实验表明，K-Paths在药物重定位上将Llama 8.1B的F1分数提高了12.45分，在交互严重性预测上提高了13.42分。此外，Llama 70B分别提高了6.18分和8.46分。K-Paths还通过将KG大小减少90%同时保持强大的预测性能来提高EmerGNN这一最先进的GNN的监督训练效率。除了可扩展性和效率之外，K-Paths还独特地在KGs和LLMs之间架起了桥梁，为预测交互提供了可解释的依据。这些能力表明，K-Paths是一个高效数据驱动药物发现的有价值工具。', 'title_zh': 'K-路径：图路径推理在药物再利用和药物相互作用预测中的应用'}
{'arxiv_id': 'arXiv:2502.13321', 'title': 'Adjust for Trust: Mitigating Trust-Induced Inappropriate Reliance on AI Assistance', 'authors': 'Tejas Srinivasan, Jesse Thomason', 'link': 'https://arxiv.org/abs/2502.13321', 'abstract': "Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance. For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. In two decision-making scenarios -- laypeople answering science questions and doctors making medical diagnoses -- we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38% reduction in inappropriate reliance and 20% improvement in decision accuracy. We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration.", 'abstract_zh': '信任如何影响用户在人工智能辅助决策任务中的推荐依赖性：低信任和高信任分别导致过度依赖和不足依赖增加。我们提出，人工智能助手应通过信任自适应干预来调整其行为，以缓解这种不适当的依赖性。例如，当用户信任较低时，提供解释可以促使用户更仔细地考虑助手的建议。在两种决策场景——普通公众回答科学问题和医生进行医学诊断——中，我们发现，在低信任和高信任时刻分别提供支持性解释和反驳性解释，可将不适当的依赖减少多达38%，决策准确率提高20%。我们还通过适配性插入强制暂停来促进深思熟虑，从而减少过度依赖。研究结果表明，人工智能助手对用户信任的适应性调整有助于促进适当的依赖性，为改善人机协作提供了令人兴奋的发展方向。', 'title_zh': '调整信任：缓解由信任引发的对AI辅助的不当依赖'}
{'arxiv_id': 'arXiv:2502.13189', 'title': 'MoBA: Mixture of Block Attention for Long-Context LLMs', 'authors': 'Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu', 'link': 'https://arxiv.org/abs/2502.13189', 'abstract': "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\nIn this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at this https URL.", 'abstract_zh': '扩展有效的上下文长度是推动大型语言模型（LLM）向人工通用智能（AGI）发展的重要因素。然而，传统注意机制中固有的计算复杂度的二次增加构成了一个难以跨越的障碍。现有的方法要么引入强烈偏见的结构，如sink或窗口注意，这些结构通常是任务特定的，要么将注意机制大幅修改为线性近似，而后者在复杂推理任务中的性能尚待充分探索。\n\n在这项工作中，我们提出了一种遵循“减少结构”原则的解决方案，使得模型能够自主决定注意力的分配，而不是引入预定义的偏见。我们提出了混合块注意（MoBA，Mixture of Block Attention）这一创新方法，该方法将专家混合（MoE，Mixture of Experts）的原则应用于注意机制中。这种新型架构在处理长上下文任务时表现出优越的性能，并且其关键优势在于能够无缝地在全注意力和稀疏注意力之间切换，从而提高效率而不牺牲性能。MoBA 已被部署以支持 KIMI 的长上下文请求，并展示了在 LLM 中高效注意计算方面的显著进展。我们的代码可在以下链接获取：[提供链接网址]', 'title_zh': 'MoBA：长上下文LLM的混合块注意机制'}
{'arxiv_id': 'arXiv:2502.13166', 'title': 'Large Language Models Can Help Mitigate Barren Plateaus', 'authors': 'Jun Zhuang, Chaowen Guan', 'link': 'https://arxiv.org/abs/2502.13166', 'abstract': "In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum Neural Networks (QNNs) have emerged as a promising approach for various applications, yet their training is often hindered by barren plateaus (BPs), where gradient variance vanishes exponentially as the model size increases. To address this challenge, we propose a new Large Language Model (LLM)-driven search framework, AdaInit, that iteratively searches for optimal initial parameters of QNNs to maximize gradient variance and therefore mitigate BPs. Unlike conventional one-time initialization methods, AdaInit dynamically refines QNN's initialization using LLMs with adaptive prompting. Theoretical analysis of the Expected Improvement (EI) proves a supremum for the search, ensuring this process can eventually identify the optimal initial parameter of the QNN. Extensive experiments across four public datasets demonstrate that AdaInit significantly enhances QNN's trainability compared to classic initialization methods, validating its effectiveness in mitigating BPs.", 'abstract_zh': '在嘈杂的中等规模量子（NISQ）计算时代，量子神经网络（QNNs）已成为多种应用的有前途的方法，然而它们的训练常常受到荒漠平原（BPs，Barren Plateaus）的阻碍，其中梯度方差随着模型规模的增加而指数级消失。为了应对这一挑战，我们提出了一种新的基于大型语言模型（LLM）的搜索框架AdaInit，该框架逐次搜索QNNs的最佳初始参数，以最大化梯度方差，从而减轻BPs的影响。与传统的单一初始化方法不同，AdaInit通过使用自适应提示的LLMs动态细化QNN的初始化。通过预期改善（Expected Improvement，EI）的理论分析证明了搜索的上界，确保这一过程最终能够确定QNN的最佳初始参数。在四个公开数据集上的广泛实验表明，与经典的初始化方法相比，AdaInit 显著增强了QNN的可训练性，验证了其在减轻BPs方面的有效性。', 'title_zh': '大型语言模型可以帮助缓解 barren 平原问题'}
{'arxiv_id': 'arXiv:2502.13162', 'title': 'ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs', 'authors': 'Ziyi Ni, Hao Wang, Huacan Wang', 'link': 'https://arxiv.org/abs/2502.13162', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success in various domains but remain vulnerable to adversarial jailbreak attacks. Existing prompt-defense strategies, including parameter-modifying and parameter-free approaches, face limitations in adaptability, interpretability, and customization, constraining their effectiveness against evolving threats. To address these challenges, we propose ShieldLearner, a novel paradigm that mimics human learning in defense. Through trial and error, it autonomously distills attack signatures into a Pattern Atlas and synthesizes defense heuristics into a Meta-analysis Framework, enabling systematic and interpretable threat detection. Furthermore, we introduce Adaptive Adversarial Augmentation to generate adversarial variations of successfully defended prompts, enabling continuous self-improvement without model retraining. In addition to standard benchmarks, we create a hard test set by curating adversarial prompts from the Wildjailbreak dataset, emphasizing more concealed malicious intent. Experimental results show that ShieldLearner achieves a significantly higher defense success rate than existing baselines on both conventional and hard test sets, while also operating with lower computational overhead, making it a practical and efficient solution for real-world adversarial defense.', 'abstract_zh': '大型语言模型（LLMs）在多个领域取得了显著的成功，但仍然容易受到对抗性劫持攻击的影响。现有的提示防御策略，包括参数修改型和参数无关型方法，存在适应性、可解释性和个性化方面的局限性，限制了它们在应对不断演变的威胁方面的有效性。为了解决这些挑战，我们提出了一种名为ShieldLearner的新范式，模仿人类在防御中的学习过程。通过试错，它能够自主提炼攻击特征，构建模式图谱，并综合生成防御启发式规则，从而实现系统的和可解释的威胁检测。此外，我们引入了适应性对抗增广，通过生成成功防御的提示的对抗变体，实现持续的自我改进，无需重新训练模型。除了标准基准测试外，我们通过从Wildjailbreak数据集中精心筛选具有更隐蔽恶意意图的对抗性提示，创建了一个更难的测试集。实验结果表明，ShieldLearner在常规和更难的测试集上都实现了显著更高的防御成功率，同时由于较低的计算开销而更为实际和高效，使其成为应对实际对抗性攻击的一个可行且高效的解决方案。', 'title_zh': 'ShieldLearner：LLM中的一种新的Jailbreak攻击防护范式'}
{'arxiv_id': 'arXiv:2502.12215', 'title': 'Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?', 'authors': 'Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2502.12215', 'abstract': "The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.", 'abstract_zh': '测试时缩放在大型语言模型（LLMs）中的出现，如OpenAI的o1系列所体现的，通过在推理过程中调整计算资源分配来提升推理能力。尽管其后继者如QwQ、DeepSeek-R1（R1）和LIMO重现了这些进步，但这些模型是否真正具备测试时缩放能力尚待探究。本研究发现，这些o1类似模型的较长推理过程（CoT）并不一致地提高准确性；相反，对于相同的题目，正确答案的推理过程往往比错误答案更短。进一步的研究表明，这一现象与模型的自我校正能力密切相关——较长的推理过程包含更多的自我校正，这些自我校正往往导致性能下降。随后，我们对QwQ、R1和LIMO的串行和并行缩放策略进行了比较，发现并行缩放策略在覆盖率和可扩展性方面表现出色。基于这些见解，我们提出了一种名为“最短多数投票法”的方法，该方法结合了并行缩放策略和推理过程长度的特性，显著提高了模型的测试时可扩展性，相较于传统的多数投票方法更为有效。', 'title_zh': '重新审视o1-like模型的测试时缩放能力：它们真具备测试时缩放的能力吗？'}
