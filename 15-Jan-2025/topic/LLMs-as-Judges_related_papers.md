# Potential and Perils of Large Language Models as Judges of Unstructured Textual Data 

**Title (ZH)**: 大型语言模型作为无结构文本数据裁判的潜力与风险 

**Authors**: Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar  

**Link**: [PDF](https://arxiv.org/pdf/2501.08167)  

**Abstract**: Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases. 

**Abstract (ZH)**: 大规模语言模型的迅速发展解锁了处理和总结非结构化文本数据的惊人能力。这为分析丰富且开放式数据集（如调查回复）带来了重要影响，L大型语言模型（LLMs）有望高效地提炼关键主题和情绪。然而，随着组织越来越多地利用这些强大的人工智能系统来理解文本反馈，一个关键问题应运而生：我们能否信任LLMs准确地代表这些文本数据集中的视角？尽管LLMs在生成人类般自然的摘要方面表现出色，但它们的输出可能存在无意中的偏差，偏离原始回应的真实内容。LLMs生成的输出与实际数据主题之间的差异可能导致决策失误，对组织产生深远影响。本研究调查了LLMs作为评判模型评估由其他LLMs生成的摘要主题一致性的有效性。我们使用Anthropic的Claude模型从开放式调查回复中生成主题摘要，并使用Amazon的Titan Express、Nova Pro和Meta的Llama作为LLMs评判者。LLM评判者的方法与使用科恩κ、斯皮尔曼ρ和克里彭多夫α进行的人类评估进行了比较，验证了一种可扩展的人性化评估方法的替代方案。我们的研究发现，尽管作为评判者的LLMs提供了与人类评分者相比可扩展的解决方案，但人类评分者仍可能在检测细微的、情境特定的差异方面更为出色。本研究为AI辅助文本分析领域提供了更多信息。我们讨论了限制并提供了未来研究的建议，强调在不同背景和应用场景下一般化LLM评判模型时需要谨慎考虑。 

---
