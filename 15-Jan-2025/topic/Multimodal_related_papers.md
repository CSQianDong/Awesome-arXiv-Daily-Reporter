# CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition 

**Title (ZH)**: CG-MER：一种基于纸牌游戏的多模态情感识别数据集 

**Authors**: Nessrine Farhat, Amine Bohi, Leila Ben Letaifa, Rim Slama  

**Link**: [PDF](https://arxiv.org/pdf/2501.08182)  

**Abstract**: The field of affective computing has seen significant advancements in exploring the relationship between emotions and emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a comprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three primary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the dataset has the potential to incorporate additional modalities, such as Natural Language Processing (NLP) to expand the scope of emotion recognition research. The dataset was curated through engaging participants in card game sessions, where they were prompted to express a range of emotions while responding to diverse questions. The study included 10 sessions with 20 participants (9 females and 11 males). The dataset serves as a valuable resource for furthering research in emotion recognition and provides an avenue for exploring the intricate connections between human emotions and digital technologies. 

**Abstract (ZH)**: 情感计算领域在探索情感与新兴技术之间的关系方面取得了显著进展。本文在此领域中做出了一个创新且宝贵的贡献，通过引入一个全面的法语多模态数据集，专门用于情感识别。该数据集包含三个主要模态：面部表情、语音和手势，提供了对情感的全方位视角。此外，该数据集还具备通过整合自然语言处理（NLP）等其他模态来扩展情感识别研究范围的潜力。数据集是通过让参与者参与卡片游戏会话，并在回答不同问题的同时表达各种情感来收集的。研究包括了10个会话，共20名参与者（其中9名女性和11名男性）。该数据集为情感识别研究提供了宝贵的资源，并为探索人的情感与数字技术之间的复杂联系开辟了路径。 

---
# Visual Language Models as Operator Agents in the Space Domain 

**Title (ZH)**: 视觉语言模型在空间领域中的操作代理应用 

**Authors**: Alejandro Carrasco, Marco Nedungadi, Enrico M. Zucchelli, Amit Jain, Victor Rodriguez-Fernandez, Richard Linares  

**Link**: [PDF](https://arxiv.org/pdf/2501.07802)  

**Abstract**: This paper explores the application of Vision-Language Models (VLMs) as operator agents in the space domain, focusing on both software and hardware operational paradigms. Building on advances in Large Language Models (LLMs) and their multimodal extensions, we investigate how VLMs can enhance autonomous control and decision-making in space missions. In the software context, we employ VLMs within the Kerbal Space Program Differential Games (KSPDG) simulation environment, enabling the agent to interpret visual screenshots of the graphical user interface to perform complex orbital maneuvers. In the hardware context, we integrate VLMs with robotic systems equipped with cameras to inspect and diagnose physical space objects, such as satellites. Our results demonstrate that VLMs can effectively process visual and textual data to generate contextually appropriate actions, competing with traditional methods and non-multimodal LLMs in simulation tasks, and showing promise in real-world applications. 

**Abstract (ZH)**: 本文探讨了视觉语言模型（VLMs）作为操作代理在空间领域的应用，重点关注软件和硬件操作范式。基于大型语言模型（LLMs）及其多模态扩展的最新进展，我们研究了VLMs如何在空间任务中增强自主控制和决策能力。在软件背景下，我们利用VLMs在Kerbal Space Program Differential Games（KSPDG）仿真环境中，让代理能够解释图形用户界面的视觉截图，从而执行复杂的轨道机动。在硬件背景下，我们将VLMs整合到配备摄像头的机器人系统中，以检查和诊断物理空间物体，如卫星。研究结果表明，VLMs能够有效处理视觉和文本数据以生成上下文相关的行为，在仿真任务中可以与传统方法和非多模态的LLMs竞争，显示出在实际应用中的潜力。 

---
# A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following 

**Title (ZH)**: 单细胞分析中的指令跟随多模态AI协作员 

**Authors**: Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.08187)  

**Abstract**: Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language of cellular biology", capturing intricate gene expression patterns at the single-cell level. However, interacting with this "language" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights. 

**Abstract (ZH)**: 大型语言模型在解释复杂的自然语言指令方面表现出色，使其能够执行多种多样的任务。在生命科学领域，单细胞RNA测序（scRNA-seq）数据作为“细胞生物学的语言”，捕获了单细胞层面 intricate 的基因表达模式。然而，通过传统工具与这种“语言”互动往往是低效且不直观的，给研究人员带来了挑战。为解决这些局限性，我们提出了一种多模态AI协同助手InstructCell，它利用自然语言作为媒介进行更直接和灵活的单细胞分析。我们构建了一个全面的多模态指令数据集，将基于文本的指令与来自不同组织和物种的scRNA-seq谱型配对。在此基础上，我们开发了一种多模态细胞语言架构，能够同时解释和处理这两种模态。InstructCell使研究人员能够仅使用简单的自然语言命令完成关键任务，如细胞类型注释、条件伪细胞生成以及药物敏感性预测。广泛的评估表明，InstructCell在性能上往往与现有的单细胞基础模型相当甚至更优，且能够适应各种实验条件。更重要的是，InstructCell提供了一个易操作且直观的工具，用于探索复杂的单细胞数据，降低技术门槛并促进更深入的生物学洞察。 

---
# Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma 

**Title (ZH)**: 探索视觉语言模型在尤文氏肉瘤诊断中的强大工具作用 

**Authors**: Alvaro Pastor-Naranjo, Pablo Meseguer, Rocío del Amor, Jose Antonio Lopez-Guerrero, Samuel Navarro, Katia Scotlandi, Antonio Llombart-Bosch, Isidro Machado, Valery Naranjo  

**Link**: [PDF](https://arxiv.org/pdf/2501.08042)  

**Abstract**: Ewing's sarcoma (ES), characterized by a high density of small round blue cells without structural organization, presents a significant health concern, particularly among adolescents aged 10 to 19. Artificial intelligence-based systems for automated analysis of histopathological images are promising to contribute to an accurate diagnosis of ES. In this context, this study explores the feature extraction ability of different pre-training strategies for distinguishing ES from other soft tissue or bone sarcomas with similar morphology in digitized tissue microarrays for the first time, as far as we know. Vision-language supervision (VLS) is compared to fully-supervised ImageNet pre-training within a multiple instance learning paradigm. Our findings indicate a substantial improvement in diagnostic accuracy with the adaption of VLS using an in-domain dataset. Notably, these models not only enhance the accuracy of predicted classes but also drastically reduce the number of trainable parameters and computational costs. 

**Abstract (ZH)**: 尤因氏肉瘤（ES）由高密度的小圆蓝细胞组成，这些细胞缺乏结构组织，尤其对10至19岁的青少年构成了重要的健康威胁。基于人工 Intelligence 的系统对病理学图像进行自动化分析，有望为ES的确诊提供准确的诊断。在此背景下，本研究首次探索了不同预训练策略在区分具有相似形态的ES与其他软组织或骨肉瘤方面的能力，使用数字化组织微阵列。据我们所知，本研究通过视觉-语言监督（VLS）与完全监督的ImageNet预训练进行比较，研究在多实例学习框架下的表现。我们的研究结果表明，通过使用领域内数据集进行VLS能显著提高诊断准确性。值得注意的是，这些模型不仅提升了预测类别的准确性，还大幅减少了可训练参数的数量和计算成本。 

---
# Facial Dynamics in Video: Instruction Tuning for Improved Facial Expression Perception and Contextual Awareness 

**Title (ZH)**: 视频中面部动态的建模：指令调整以提高面部表情感知和上下文意识 

**Authors**: Jiaxing Zhao, Boyuan Sun, Xiang Chen, Xihan Wei  

**Link**: [PDF](https://arxiv.org/pdf/2501.07978)  

**Abstract**: Facial expression captioning has found widespread application across various domains. Recently, the emergence of video Multimodal Large Language Models (MLLMs) has shown promise in general video understanding tasks. However, describing facial expressions within videos poses two major challenges for these models: (1) the lack of adequate datasets and benchmarks, and (2) the limited visual token capacity of video MLLMs. To address these issues, this paper introduces a new instruction-following dataset tailored for dynamic facial expression caption. The dataset comprises 5,033 high-quality video clips annotated manually, containing over 700,000 tokens. Its purpose is to improve the capability of video MLLMs to discern subtle facial nuances. Furthermore, we propose FaceTrack-MM, which leverages a limited number of tokens to encode the main character's face. This model demonstrates superior performance in tracking faces and focusing on the facial expressions of the main characters, even in intricate multi-person scenarios. Additionally, we introduce a novel evaluation metric combining event extraction, relation classification, and the longest common subsequence (LCS) algorithm to assess the content consistency and temporal sequence consistency of generated text. Moreover, we present FEC-Bench, a benchmark designed to assess the performance of existing video MLLMs in this specific task. All data and source code will be made publicly available. 

**Abstract (ZH)**: 面部表情描述已经在多个领域找到了广泛的应用。近年来，视频多模态大型语言模型（MM-LLMs）在通用视频理解任务中展现出了潜力。然而，这些模型在描述视频中的面部表情时遇到了两大挑战：（1）缺乏充分的数据集和基准；（2）视频MM-LLMs的视觉标记容量有限。为了解决这些问题，本文引入了一个新的指令遵循数据集，专门用于动态面部表情描述。该数据集包含5,033个高质量的手动注释视频片段，包含超过700,000个标记，旨在提高视频MM-LLMs识别面部细微表情的能力。此外，我们提出了一种名为FaceTrack-MM的模型，该模型利用有限数量的标记来编码主要人物的脸部。该模型在追踪面部并在复杂多个人场景中聚焦主要人物的面部表情方面表现出优越性能。此外，我们还引入了一种新的评价指标，结合事件提取、关系分类以及最长公共子序列（LCS）算法来评估生成文本的内容一致性与时间顺序一致性。最后，我们提出了FEC-Bench基准，用于评估现有视频MM-LLMs在这一特定任务中的性能。所有数据和源代码都将公开发布。 

---
# Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding 

**Title (ZH)**: Tarsier2：从详细的视频描述到全面的视频理解，推进大型视觉-语言模型的发展 

**Authors**: Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin  

**Link**: [PDF](https://arxiv.org/pdf/2501.07888)  

**Abstract**: We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM) designed for generating detailed and accurate video descriptions, while also exhibiting superior general video understanding capabilities. Tarsier2 achieves significant advancements through three key upgrades: (1) Scaling pre-training data from 11M to 40M video-text pairs, enriching both volume and diversity; (2) Performing fine-grained temporal alignment during supervised fine-tuning; (3) Using model-based sampling to automatically construct preference data and applying DPO training for optimization. Extensive experiments show that Tarsier2-7B consistently outperforms leading proprietary models, including GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\% performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B also sets new state-of-the-art results across 15 public benchmarks, spanning tasks such as video question-answering, video grounding, hallucination test, and embodied question-answering, demonstrating its versatility as a robust generalist vision-language model. 

**Abstract (ZH)**: 我们介绍了Tarsier2，这是一个最先进的大型视觉-语言模型（LVLM），专门设计用于生成详细准确的视频描述，并具备出色的通用视频理解能力。Tarsier2 通过三项关键技术升级实现了重要进步：(1) 将预训练数据从 1100 万对视频-文本扩展到 4000 万对，丰富了数据的规模和多样性；(2) 在监督微调过程中进行精细粒度的时序对齐；(3) 使用基于模型的采样自动构建偏好数据，并采用 DPO 训练进行优化。大量实验表明，Tarsier2-7B 在细致的视频描述任务中始终优于包括 GPT-4o 和 Gemini 1.5 Pro 在内的领先商用模型。在 DREAM-1K 基准测试中，Tarsier2-7B 的 F1 得分分别比 GPT-4o 提高了 2.8% 和比 Gemini-1.5-Pro 提高了 5.8%。在人类旁观者评估中，Tarsier2-7B 在 GPT-4o 上表现出 +8.6% 的性能优势，在 Gemini-1.5-Pro 上则表现出 +24.9% 的显著优势。此外，Tarsier2-7B 在 15 个公开基准测试中设立了新的最佳性能记录，涵盖视频问答、视频定位、幻觉测试和体态问答等任务，证明了其作为稳健通用视觉-语言模型的灵活性和能力。 

---
# TriMod Fusion for Multimodal Named Entity Recognition in Social Media 

**Title (ZH)**: 社交媒体多模态命名实体识别的TriMod融合方法 

**Authors**: Mosab Alfaqeeh  

**Link**: [PDF](https://arxiv.org/pdf/2501.08267)  

**Abstract**: Social media platforms serve as invaluable sources of user-generated content, offering insights into various aspects of human behavior. Named Entity Recognition (NER) plays a crucial role in analyzing such content by identifying and categorizing named entities into predefined classes. However, traditional NER models often struggle with the informal, contextually sparse, and ambiguous nature of social media language. To address these challenges, recent research has focused on multimodal approaches that leverage both textual and visual cues for enhanced entity recognition. Despite advances, existing methods face limitations in capturing nuanced mappings between visual objects and textual entities and addressing distributional disparities between modalities. In this paper, we propose a novel approach that integrates textual, visual, and hashtag features (TriMod), utilizing Transformer-attention for effective modality fusion. The improvements exhibited by our model suggest that named entities can greatly benefit from the auxiliary context provided by multiple modalities, enabling more accurate recognition. Through the experiments on a multimodal social media dataset, we demonstrate the superiority of our approach over existing state-of-the-art methods, achieving significant improvements in precision, recall, and F1 score. 

**Abstract (ZH)**: 社交媒体平台是用户生成内容的重要来源，为人类行为的各个层面提供了见解。命名实体识别（NER）在分析此类内容时起着至关重要的作用，通过识别和将命名实体分类到预定义的类别中。然而，传统NER模型往往难以处理社交媒体语言的非正式性、上下文稀疏性和模糊性。为应对这些挑战，最近的研究集中于利用文本和视觉线索的多模态方法以增强实体识别效果。尽管取得了进展，但现有方法在捕捉视觉对象和文本实体之间的细微映射关系以及解决模态分布差异方面仍存在局限性。在本文中，我们提出了一种新颖的方法，即结合文本、视觉和标签特征的TriMod模型，利用Transformer-attention实现有效的模态融合。我们的模型表现改进表明，命名实体可以从多种模态提供的辅助上下文中显著受益，从而实现更准确的识别。通过在多模态社交媒体数据集上的实验，我们展示了该方法优于现有最先进的方法，实现了在精确度、召回率和F1分数上的显著提升。 

---
# Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention for Enabled Classification 

**Title (ZH)**: 动态多模态情感分析：利用跨模态注意力实现enabled分类 

**Authors**: Hui Lee, Singh Suniljit, Yong Siang Ong  

**Link**: [PDF](https://arxiv.org/pdf/2501.08085)  

**Abstract**: This paper explores the development of a multimodal sentiment analysis model that integrates text, audio, and visual data to enhance sentiment classification. The goal is to improve emotion detection by capturing the complex interactions between these modalities, thereby enabling more accurate and nuanced sentiment interpretation. The study evaluates three feature fusion strategies -- late stage fusion, early stage fusion, and multi-headed attention -- within a transformer-based architecture. Experiments were conducted using the CMU-MOSEI dataset, which includes synchronized text, audio, and visual inputs labeled with sentiment scores. Results show that early stage fusion significantly outperforms late stage fusion, achieving an accuracy of 71.87\%, while the multi-headed attention approach offers marginal improvement, reaching 72.39\%. The findings suggest that integrating modalities early in the process enhances sentiment classification, while attention mechanisms may have limited impact within the current framework. Future work will focus on refining feature fusion techniques, incorporating temporal data, and exploring dynamic feature weighting to further improve model performance. 

**Abstract (ZH)**: 本文探讨了一种多模态情感分析模型的发展，该模型整合了文本、音频和视觉数据，以提高情感分类的准确性。目标是通过捕捉这些模态之间的复杂交互来改进情绪检测，从而实现更为准确和细腻的情感解读。研究在基于变压器的架构中评估了三种特征融合策略——后期融合、早期融合和多头注意力机制。实验使用了CMU-MOSEI数据集，该数据集包含了同步的文本、音频和视觉输入，并用情感评分进行标注。结果显示，早期融合显著优于后期融合，准确率达到71.87%，而多头注意力机制仅带来微小的提升，准确率达到了72.39%。研究结果表明，在处理过程中早期整合模态可以增强情感分类的效果，而注意力机制在现有框架中的影响有限。未来的工作将集中在改进特征融合技术、引入时间数据和探索动态特征加权等方面，以进一步提高模型性能。 

---
# Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding 

**Title (ZH)**: 参数倒置图像金字塔网络在视觉感知与多模态理解中的应用 

**Authors**: Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai  

**Link**: [PDF](https://arxiv.org/pdf/2501.07783)  

**Abstract**: Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at this https URL. 

**Abstract (ZH)**: 图像金字塔在许多高性能方法中被广泛应用，用于获取多尺度特征以实现精确的视觉感知和理解。然而，目前的图像金字塔使用同一大型模型处理不同分辨率的图像，导致计算成本显著增加。为解决这一挑战，我们提出了一种新的网络架构，称为参数倒置图像金字塔网络（PIIP）。具体而言，PIIP 利用预训练模型（ViTs 或 CNNs）作为分支来处理多尺度图像，其中高分辨率图像由较小的网络分支处理，以平衡计算成本和性能。为了整合来自不同空间尺度的信息，我们进一步提出了一种新的跨分支特征交互机制。为了验证 PIIP 的有效性，我们将其应用于各种感知模型以及代表性的多模态大型语言模型 LLaVA，并在对象检测、分割、图像分类和多模态理解等不同任务上进行了广泛的实验。与单分支和现有多种分辨率方法相比，PIIP 在较低计算成本的基础上实现了更优的性能。当应用于大规模视觉基础模型 InternViT-6B 时，PIIP 在检测和分割任务上的性能可提高 1%-2%，计算量仅为原始计算量的 40%-60%，最终在 MS COCO 上达到 60.0 的框 AP，在 ADE20K 上达到 59.7 的 mIoU。在多模态理解方面，我们的 PIIP-LLaVA 在 TextVQA 上实现了 73.0% 的准确率，在 MMBench 上实现了 74.5% 的准确率，仅使用了 2.8 百万个训练数据。我们的代码已在此处发布：[this https URL]。 

---
# A Heterogeneous Multimodal Graph Learning Framework for Recognizing User Emotions in Social Networks 

**Title (ZH)**: 一种异构多模态图学习框架，用于识别社交网络中用户情感 

**Authors**: Sree Bhattacharyya, Shuhua Yang, James Z. Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.07746)  

**Abstract**: The rapid expansion of social media platforms has provided unprecedented access to massive amounts of multimodal user-generated content. Comprehending user emotions can provide valuable insights for improving communication and understanding of human behaviors. Despite significant advancements in Affective Computing, the diverse factors influencing user emotions in social networks remain relatively understudied. Moreover, there is a notable lack of deep learning-based methods for predicting user emotions in social networks, which could be addressed by leveraging the extensive multimodal data available. This work presents a novel formulation of personalized emotion prediction in social networks based on heterogeneous graph learning. Building upon this formulation, we design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that utilizes deep learning-based features for user emotion recognition. Additionally, we include a dynamic context fusion module in HMG-Emo that is capable of adaptively integrating the different modalities in social media data. Through extensive experiments, we demonstrate the effectiveness of HMG-Emo and verify the superiority of adopting a graph neural network-based approach, which outperforms existing baselines that use rich hand-crafted features. To the best of our knowledge, HMG-Emo is the first multimodal and deep-learning-based approach to predict personalized emotions within online social networks. Our work highlights the significance of exploiting advanced deep learning techniques for less-explored problems in Affective Computing. 

**Abstract (ZH)**: 社交媒体平台的迅速扩展为获取大量多模态用户生成内容提供了前所未有的机会。理解用户情绪可以为改善人际沟通和理解人类行为提供宝贵的见解。尽管在情感计算方面取得了显著进展，但在社交网络中影响用户情绪的多种因素仍相对未被充分研究。此外，对于基于深度学习预测社交网络中用户情绪的方法也存在明显不足，这可以通过利用广泛可用的多模态数据来解决。本文提出了一种基于异构图学习的个性化情绪预测新框架。在此基础上，我们设计了HMG-Emo异构多模态图学习框架，该框架利用深度学习特征进行用户情绪识别。此外，HMG-Emo中还包括一个动态上下文融合模块，该模块能够适应性地整合社交媒体数据中的不同模态。通过广泛实验，我们证明了HMG-Emo的有效性，并验证了基于图神经网络的方法优于使用丰富手工设计特征的现有基准方法。据我们所知，HMG-Emo是第一个基于多模态和深度学习的方法，用于预测在线社交网络中的个性化情绪。我们的研究突显了利用先进深度学习技术解决情感计算中未充分探索问题的重要性。 

---
