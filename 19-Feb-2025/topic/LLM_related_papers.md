# Learning More Effective Representations for Dense Retrieval through Deliberate Thinking Before Search 

**Title (ZH)**: 在搜索前进行精心思考以学习更有效表示方法的密集检索研究 

**Authors**: Yifan Ji, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan Liu, Yu Gu, Ge Yu, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.12974)  

**Abstract**: Recent dense retrievers usually thrive on the emergency capabilities of Large Language Models (LLMs), using them to encode queries and documents into an embedding space for retrieval. These LLM-based dense retrievers have shown promising performance across various retrieval scenarios. However, relying on a single embedding to represent documents proves less effective in capturing different perspectives of documents for matching. In this paper, we propose Deliberate Thinking based Dense Retriever (DEBATER), which enhances these LLM-based retrievers by enabling them to learn more effective document representations through a step-by-step thinking process. DEBATER introduces the Chain-of-Deliberation mechanism to iteratively optimize document representations using a continuous chain of thought. To consolidate information from various thinking steps, DEBATER also incorporates the Self Distillation mechanism, which identifies the most informative thinking steps and integrates them into a unified text embedding. Experimental results show that DEBATER significantly outperforms existing methods across several retrieval benchmarks, demonstrating superior accuracy and robustness. All codes are available at this https URL. 

**Abstract (ZH)**: 近年来，密集检索器通常依赖大型语言模型（LLMs）的应急能力，利用这些模型将查询和文档编码到一个嵌入空间中进行检索。这些基于LLM的密集检索器在各种检索场景中表现出有希望的性能。然而，仅依靠单一嵌入来表示文档在捕捉文档的多角度方面效果不佳。在本文中，我们提出了一种精细思考为基础的密集检索器（DEBATER），通过逐步的思考过程增强这些基于LLM的检索器，使其学习更有效的文档表示。DEBATER引入了链式思考机制，通过连续的思想链条迭代优化文档表示。为了整合来自不同思考步骤的信息，DEBATER还引入了自我蒸馏机制，该机制识别出最具信息量的思考步骤，并将其整合进一个统一的文本嵌入中。实验结果表明，DEBATER在多个检索基准上显著优于现有方法，显示出更高的准确性和鲁棒性。所有代码已在以下网址提供：此 https URL。 

---
# G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable Recommendation 

**Title (ZH)**: G-Refer：解释性推荐增强的图检索大语言模型 

**Authors**: Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.12586)  

**Abstract**: Explainable recommendation has demonstrated significant advantages in informing users about the logic behind recommendations, thereby increasing system transparency, effectiveness, and trustworthiness. To provide personalized and interpretable explanations, existing works often combine the generation capabilities of large language models (LLMs) with collaborative filtering (CF) information. CF information extracted from the user-item interaction graph captures the user behaviors and preferences, which is crucial for providing informative explanations. However, due to the complexity of graph structure, effectively extracting the CF information from graphs still remains a challenge. Moreover, existing methods often struggle with the integration of extracted CF information with LLMs due to its implicit representation and the modality gap between graph structures and natural language explanations. To address these challenges, we propose G-Refer, a framework using graph retrieval-augmented large language models (LLMs) for explainable recommendation. Specifically, we first employ a hybrid graph retrieval mechanism to retrieve explicit CF signals from both structural and semantic perspectives. The retrieved CF information is explicitly formulated as human-understandable text by the proposed graph translation and accounts for the explanations generated by LLMs. To bridge the modality gap, we introduce knowledge pruning and retrieval-augmented fine-tuning to enhance the ability of LLMs to process and utilize the retrieved CF information to generate explanations. Extensive experiments show that G-Refer achieves superior performance compared with existing methods in both explainability and stability. Codes and data are available at this https URL. 

**Abstract (ZH)**: 可解释推荐已经在告知用户推荐逻辑方面展现了显著优势，从而增加了系统的透明度、有效性和可信度。为了提供个性化且可解释的解释，现有研究通常结合大型语言模型（LLMs）的生成能力和基于协同过滤（CF）的信息。从用户-项交互图中提取的CF信息捕捉了用户行为和偏好，这是提供有用解释的关键。然而，由于图结构的复杂性，有效地从图中提取CF信息仍然是一项挑战。此外，现有方法在将提取的CF信息与LLMs集成时常常遇到困难，这主要是由于其隐式表示以及图结构与自然语言解释之间的模态差距。为了解决这些挑战，我们提出了一种基于图检索增强的大语言模型（LLMs）的框架——G-Refer。具体而言，我们首先采用一种混合图检索机制，从结构和语义两个视角检索显性的CF信号。通过提出的图翻译将检索到的CF信息明确定义为易于理解的文本，并用于支持LLMs生成解释。为了弥合模态差距，我们引入了知识精简和检索增强微调，以增强LLMs处理和利用检索到的CF信息生成解释的能力。大量的实验表明，G-Refer 在可解释性和稳定性方面均优于现有方法。相关代码和数据可以在此处获取：[提供链接]。 

---
# HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation 

**Title (ZH)**: HopRAG：基于逻辑感知的多跳推理检索增强生成 

**Authors**: Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.12442)  

**Abstract**: Retrieval-Augmented Generation (RAG) systems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical relevance. To address this, we propose HopRAG, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, HopRAG constructs a passage graph, with text chunks as vertices and logical connections established via LLM-generated pseudo-queries as edges. During retrieval, it employs a retrieve-reason-prune mechanism: starting with lexically or semantically similar passages, the system explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to identify truly relevant ones. Extensive experiments demonstrate HopRAG's superiority, achieving 76.78\% higher answer accuracy and 65.07\% improved retrieval F1 score compared to conventional methods. The repository is available at this https URL. 

**Abstract (ZH)**: 检索增强生成（RAG）系统常常难以应对不完美的检索问题，因为传统的检索器主要关注词汇或语义相似性，而忽视了逻辑相关性。为了解决这一问题，我们提出了HopRAG，一种新颖的RAG框架，通过图结构的知识探索引入逻辑推理来增强检索。在索引过程中，HopRAG 构造一个段落图，其中文本片段作为顶点，通过LLM生成的伪查询建立逻辑连接，作为边。在检索过程中，系统采用“检索-推理-修剪”的机制：从词汇或语义相似的段落开始，系统根据伪查询和LLM推理逐步探索多跳邻居，以识别真正相关的内容。广泛的实验结果证明了HopRAG 的优越性，其答案准确性比传统方法高76.78%，检索F1分数提高了65.07%。代码库可访问 [该链接]。 

---
# AIDE: AI-Driven Exploration in the Space of Code 

**Title (ZH)**: AIDE：AI 驱动的代码空间探索 

**Authors**: Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.13138)  

**Abstract**: Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-and-error as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench. 

**Abstract (ZH)**: 机器学习，现代人工智能的基础，已经推动了一系列创新，这些创新从根本上改变了世界。然而，这些进步背后隐藏着一个复杂且往往耗时的过程，需要大量的劳动和计算密集型迭代与实验。开发机器学习模型的工程师和科学家们大量时间花费在试错任务上，而不是构思创新的解决方案或研究假说。为解决这一挑战，我们引入了AI驱动探索（AIDE），这是一种由大规模语言模型（LLMs）驱动的机器学习工程代理。AIDE将机器学习工程视为代码优化问题，将试错过程视为在潜在解决方案空间中的树搜索问题。通过战略性地重用和改进有希望的解决方案，AIDE有效地用计算资源换取更好的性能，其结果在多个机器学习工程基准测试上达到了最先进的水平，包括我们在Kaggle的评估、OpenAI MLE-Bench和METRS RE-Bench的结果中得到了验证。 

---
# Interactive Agents to Overcome Ambiguity in Software Engineering 

**Title (ZH)**: 软件工程中克服模糊性的交互式代理 

**Authors**: Sanidhya Vijayvargiya, Xuhui Zhou, Akhila Yerukola, Maarten Sap, Graham Neubig  

**Link**: [PDF](https://arxiv.org/pdf/2502.13069)  

**Abstract**: AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements. 

**Abstract (ZH)**: 人工智能代理正越来越多地被部署以自动化任务，通常是基于含糊不清且未充分指定的用户指令。做出不适当的假设和未提出澄清问题可能导致次优结果、因工具误用而带来的安全风险以及计算资源的浪费。在这项研究中，我们通过评估专有和开源模型在三个关键步骤中的表现，研究了语言模型代理在这种交互式代码生成设置中处理含糊指令的能力：(a) 利用互动来改善含糊场景中的表现，(b) 检测含糊性，以及(c) 提出有针对性的问题。我们的研究结果表明，模型难以区分明确指定与未充分指定的指令。然而，对于未充分指定的输入，当模型进行互动时，能够有效地从用户那里获得关键信息，从而显著提高表现，进一步突出了高效互动的价值。我们的研究指出了当前最先进的模型在复杂软件工程任务中处理含糊性时存在的重要空白，并将评估结构化为不同的步骤，以促进有针对性的改进。 

---
# You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations 

**Title (ZH)**: 你需要模拟以获得名声：利用多代理对话解决会议纪要稀缺性问题 

**Authors**: Frederic Kirstein, Muneeb Khan, Jan Philip Wahle, Terry Ruas, Bela Gipp  

**Link**: [PDF](https://arxiv.org/pdf/2502.13001)  

**Abstract**: Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints. 

**Abstract (ZH)**: 会议总结因缺乏高质量数据而受限，主要原因是隐私限制和昂贵的收集过程。我们通过提出FAME数据集来填补这一空白，该数据集包含500个英语会议和300个德语会议，是由我们新开发的多智能体会议合成框架MIMIC生成的。MIMIC框架通过定义基于心理学原理的参与者角色、规划对话内容，并协调大规模语言模型（LLM）辩论来生成给定知识源的会议记录。一个模块化的后处理步骤进一步细化这些输出，减少了潜在的重复性和过于正式的语气，确保了大规模对话的连贯性和可信度。我们还提出了一种基于心理学的评估框架，评估自然性、社会行为的真实性以及记录的难度。人类评估结果显示，FAME在自然性（4.5/5）上接近真实的会议自发性，保留了以讲演者为中心的挑战（3/5在口语方面），并引入了更丰富的信息导向难点（4/5在难度上）。这些发现表明，FAME是一个良好的且可扩展的现实会议条件的代理。它为会议总结研究和需要对话数据或其他对话中心应用的任务提供了新的测试场景，特别是在行为约束下模拟社交场景时。 

---
# Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger 

**Title (ZH)**: 具有元认知触发的大型语言模型自适应工具使用研究 

**Authors**: Wenjun Li, Dexun Li, Kuicai Dong, Cong Zhang, Hao Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, Yong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.12961)  

**Abstract**: Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs' internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）展现出了显著的涌现能力，通过利用外部工具解决复杂问题，这些复杂问题需要特定处理或实时数据。尽管现有研究扩大了LLMs对各种工具（如程序解释器、搜索引擎、天气/地图应用程序）的访问，但在实际使用中，这些工具的必要性有时被忽视，导致了工具调用的随意性。这种简单粗放的方法带来了两个关键问题：（1）由于不必要的工具调用增加延迟，（2）由于与外部工具交互不当而可能导致潜在错误。在本文中，我们引入了元认知作为LLMs自我评估其能力的代理，代表了模型对其自身局限性的意识。基于此，我们提出了MeCo，即一种适应性外部工具使用决策策略。MeCo通过捕捉表示空间中的高层次认知信号来量化元认知得分，并指导何时调用工具。值得注意的是，MeCo不需要微调，成本也非常低。我们的实验表明，MeCo准确地检测到了LLMs的内部认知信号，并在多个基础模型和基准测试中显著改善了工具使用的决策过程。 

---
# Towards more Contextual Agents: An extractor-Generator Optimization Framework 

**Title (ZH)**: 更加贴近语境的智能代理：提取-生成优化框架 

**Authors**: Mourad Aouini, Jinan Loubani  

**Link**: [PDF](https://arxiv.org/pdf/2502.12926)  

**Abstract**: Large Language Model (LLM)-based agents have demonstrated remarkable success in solving complex tasks across a wide range of general-purpose applications. However, their performance often degrades in context-specific scenarios, such as specialized industries or research domains, where the absence of domain-relevant knowledge leads to imprecise or suboptimal outcomes. To address this challenge, our work introduces a systematic approach to enhance the contextual adaptability of LLM-based agents by optimizing their underlying prompts-critical components that govern agent behavior, roles, and interactions. Manually crafting optimized prompts for context-specific tasks is labor-intensive, error-prone, and lacks scalability. In this work, we introduce an Extractor-Generator framework designed to automate the optimization of contextual LLM-based agents. Our method operates through two key stages: (i) feature extraction from a dataset of gold-standard input-output examples, and (ii) prompt generation via a high-level optimization strategy that iteratively identifies underperforming cases and applies self-improvement techniques. This framework substantially improves prompt adaptability by enabling more precise generalization across diverse inputs, particularly in context-specific tasks where maintaining semantic consistency and minimizing error propagation are critical for reliable performance. Although developed with single-stage workflows in mind, the approach naturally extends to multi-stage workflows, offering broad applicability across various agent-based systems. Empirical evaluations demonstrate that our framework significantly enhances the performance of prompt-optimized agents, providing a structured and efficient approach to contextual LLM-based agents. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的代理已经在广泛的一般性应用领域中展示了解决复杂任务的显著成功。然而，在特定情境或专业领域中，这类代理的表现通常会因缺乏相关领域知识而下降，导致产出不够精确或次优。为解决这一挑战，我们的研究引入了一种系统的方法来增强基于LLM的代理的上下文适应性，通过优化它们的基本提示——这些提示是决定代理行为、角色和互动的关键组件。手工为特定情境下的任务定制优化提示既耗时、容易出错，又缺乏可扩展性。在这项研究中，我们提出了一种提取-生成框架，旨在自动化优化基于上下文的LLM代理。具体方法通过两个关键阶段运作：(i) 从优质输入-输出示例数据集中提取特征，(ii) 通过高层次的优化策略生成提示，该策略逐次识别表现不佳的情况并应用自我改进技术。该框架通过使提示能够更准确地泛化到多样化输入，特别是在那些需要保持语义一致性并最小化错误传播才能确保可靠性能的特定情境任务中，显著增强了提示的适应性。虽然该方法最初是为目标任务设计的，但自然地适用于多阶段流程，具有在各种代理系统中广泛应用的潜力。经验评估表明，我们的方法显著增强了提示优化代理的表现，为上下文敏感的LLM代理提供了一种结构化和高效的方法。 

---
# Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols 

**Title (ZH)**: 面向AI适应性反馈的探索：比较大型语言模型与教师在实验协议中的反馈质量 

**Authors**: Kathrin Seßler, Arne Bewersdorff, Claudia Nerdel, Enkelejda Kasneci  

**Link**: [PDF](https://arxiv.org/pdf/2502.12842)  

**Abstract**: Effective feedback is essential for fostering students' success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feedback often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback quality of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent's performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student's work context. Qualitative analysis highlighted the LLM agent's limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that combining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators. 

**Abstract (ZH)**: 有效的反馈对于培养学生的科学探究成功至关重要。随着人工智能的进步，大规模语言模型（LLMs）为即时和适应性反馈提供了新的可能性。然而，这种反馈往往缺乏实际教育从业者提供的教学生态验证。为了解决这一局限，本研究评估并比较了LLM代理与真人教师和科学教育专家对学生撰写的实验方案给出的反馈质量。四名盲评者，均为科学探究和科学教育领域的专业人士，根据有效的反馈六个标准（Feed Up、Feed Back、Feed Forward、建设性语气、语言清晰度、技术术语）使用五点李克特量表分别对LLM代理、教师和科学教育专家生成的反馈文本进行了评估。研究结果表明，LLM生成的反馈在整体质量上与教师和专家的反馈无显著差异。然而，LLM代理在Feed Back维度的表现较弱，该维度涉及在学生作品的具体情境中识别并解释错误。定性分析揭示了LLM代理在上下文理解和具体错误清晰传达方面的局限性。我们的研究结果表明，结合LLM生成的反馈与人类专业知识可以提升教育实践，利用LLMs的高效性和教育者的精细理解。 

---
# Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research 

**Title (ZH)**: Perovskite-LLM：用于钙钛矿太阳电池研究的知识增强型大规模语言模型 

**Authors**: Xiang Liu, Penglei Sun, Shuyan Chen, Longhan Zhang, Peijie Dong, Huajie You, Yongqi Zhang, Chang Yan, Xiaowen Chu, Tong-yi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.12669)  

**Abstract**: The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research. 

**Abstract (ZH)**: Perovskite 太阳能电池（PSCs）的快速进步导致了研究论文的指数级增长，迫切需要高效的知识管理和推理系统。我们提出了一种全面的知识增强系统，该系统集成了三个关键组件。首先，我们开发了 Perovskite-KG，这是一种从 1,517 篇研究论文中构建的特定领域的知识图谱，包含 23,789 个实体和 22,272 个关系。其次，我们创建了两个互补的数据集：Perovskite-Chat，包含 55,101 个高质量的问题-答案对，通过一种新型多代理框架生成，以及 Perovskite-Reasoning，包含 2,217 个精心策划的材料科学问题。第三，我们介绍了两个专门的大规模语言模型：Perovskite-Chat-LLM 用于领域特定知识辅助，Perovskite-Reasoning-LLM 用于科学研究任务。实验结果表明，我们的系统在领域特定知识检索和科学研究任务方面显著优于现有模型，为研究人员提供了有效的文献回顾、实验设计和 PSC 研究复杂问题解决的工具。 

---
# Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights 

**Title (ZH)**: 推理时计算在大规模语言模型推理与规划中的应用：基准与见解 

**Authors**: Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, Shuiwang Ji  

**Link**: [PDF](https://arxiv.org/pdf/2502.12521)  

**Abstract**: We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks. 

**Abstract (ZH)**: 我们探讨了大型语言模型（LLMs）在解决复杂任务中的推理和规划能力。最近推理时技术的发展表明，通过在推理过程中探索中间步骤，可以在无需额外训练的情况下增强LLM的推理能力。值得注意的是，OpenAI的o1模型通过其新颖的多步推理和验证方法展示了令人鼓舞的性能。在此基础上，我们研究了如何通过扩展推理时技术来提高推理和规划能力，并重点探讨了计算成本与性能之间的权衡。为此，我们构建了一个全面的基准测试，称为Sys2Bench，并在五个类别（算术推理、逻辑推理、常识推理、算法推理和规划）中的十一个不同任务上对现有的推理时技术进行了广泛的实验评估。我们的研究发现，单纯扩展推理时计算的局限性在于，没有任何一种推理时技术能够在所有推理和规划任务中表现出一致的优良性能。 

---
# Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation 

**Title (ZH)**: 增强、分离和个性化：一种稳健的系统2到系统1代码生成流水线 

**Authors**: Kounianhua Du, Hanjing Wang, Jianxing Liu, Jizheng Chen, Xinyi Dai, Yasheng Wang, Ruiming Tang, Yong Yu, Jun Wang, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.12492)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable capabilities in various domains, particularly in system 1 tasks, yet the intricacies of their problem-solving mechanisms in system 2 tasks are not sufficiently explored. Recent research on System2-to-System1 methods surge, exploring the System 2 reasoning knowledge via inference-time computation and compressing the explored knowledge into System 1 process. In this paper, we focus on code generation, which is a representative System 2 task, and identify two primary challenges: (1) the complex hidden reasoning processes and (2) the heterogeneous data distributions that complicate the exploration and training of robust LLM solvers. To tackle these issues, we propose a novel BDC framework that explores insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with mutual \textbf{B}oosting, \textbf{D}isentangles the heterogeneous training data for composable LoRA-experts, and obtain \textbf{C}ustomized problem solver for each data instance with an input-aware hypernetwork to weight over the LoRA-experts, offering effectiveness, flexibility, and robustness. This framework leverages multiple LLMs through mutual verification and boosting, integrated into a Monte-Carlo Tree Search process enhanced by reflection-based pruning and refinement. Additionally, we introduce the DisenLora algorithm, which clusters heterogeneous data to fine-tune LLMs into composable Lora experts, enabling the adaptive generation of customized problem solvers through an input-aware hypernetwork. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks, offering a novel System2-to-System1 solution. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经在多个领域展现出了出色的能力，特别是在系统1任务方面，而其在系统2任务中的问题解决机制则尚未得到充分探索。近年来，关于系统2到系统1方法的研究激增，通过推理时的计算来探索系统2的推理知识，并将这些知识压缩到系统1的过程中。本文关注代码生成这一典型的系统2任务，并识别出两个主要挑战：（1）复杂的隐藏推理过程和（2）异质数据分布，这些都使得探索和训练鲁棒的LLM求解器变得更加困难。为了解决这些问题，我们提出了一种新颖的BDC框架，该框架利用一种具有增强学习的多智能体MC-树算法探索LLM的洞察力系统2知识，通过彼此拆解异质训练数据形成可组合的LoRA专家，为每个数据实例定制问题求解器，并通过具有输入感知的超网络加权选择LoRA专家，从而实现高效、灵活和鲁棒性。该框架通过相互验证和提升的多个LLM进行集成，并在基于反射的剪枝和改进的蒙特卡洛树搜索过程中得到增强。此外，我们还引入了DisenLora算法，该算法将异质数据聚类并微调LLM形成可组合的LoRA专家，通过具有输入感知的超网络生成适应性的定制问题求解器。这项工作为推进LLM在复杂推理任务中的能力奠定了基础，并提供了一种新颖的系统2到系统1解决方案。 

---
# Integrating Expert Knowledge into Logical Programs via LLMs 

**Title (ZH)**: 通过大型语言模型将专家知识集成到逻辑程序中 

**Authors**: Franciszek Górski, Oskar Wysocki, Marco Valentino, Andre Freitas  

**Link**: [PDF](https://arxiv.org/pdf/2502.12275)  

**Abstract**: This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at GitHub. 

**Abstract (ZH)**: 本文介绍了ExKLoP，这是一种新型框架，旨在评估大型语言模型（LLMs）在将专家知识整合到逻辑推理系统中的有效性。在工程领域，这种能力尤为重要，因为供应商推荐的操作范围等专家知识可以直接嵌入到自动化监控系统中。通过模仿专家验证步骤，任务如范围检查和约束验证有助于确保系统的安全性和可靠性。我们的方法系统地评估了LLM生成的逻辑规则，不仅评估其句法流畅性，还评估其逻辑正确性。同时，我们还通过基于代码执行结果的迭代反馈循环探索模型的自我纠正能力。ExKLoP提供了一个可扩展的数据集，包含130个工程前提条件、950个提示以及相应的验证点。这使得全面基准测试成为可能，同时允许控制任务复杂性和实验的可扩展性。我们利用合成数据创建方法对包括Llama3、Gemma、Mixtral、Mistral和Qwen在内的多样化大型语言模型进行了广泛的实证评估。结果显示，尽管模型生成了接近完美的句法正确代码，但它们在将专家知识转换为逻辑表达时经常出现错误。此外，迭代自我纠正仅带来了微小的改进（最多3%）。总体而言，ExKLoP提供了一个稳健的评估平台，简化了选择用于自我纠正系统的有效模型的过程，同时明确地界定了遇到的不同类型的错误。完整的实现及所有相关数据均可在GitHub上获取。 

---
# Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context 

**Title (ZH)**: 适配心理语言学研究于大规模语言模型：共指语境中的性别包容性语言 

**Authors**: Marion Bartl, Thomas Brendan Murphy, Susan Leavy  

**Link**: [PDF](https://arxiv.org/pdf/2502.13120)  

**Abstract**: Gender-inclusive language is often used with the aim of ensuring that all individuals, regardless of gender, can be associated with certain concepts. While psycholinguistic studies have examined its effects in relation to human cognition, it remains unclear how Large Language Models (LLMs) process gender-inclusive language. Given that commercial LLMs are gaining an increasingly strong foothold in everyday applications, it is crucial to examine whether LLMs in fact interpret gender-inclusive language neutrally, because the language they generate has the potential to influence the language of their users. This study examines whether LLM-generated coreferent terms align with a given gender expression or reflect model biases. Adapting psycholinguistic methods from French to English and German, we find that in English, LLMs generally maintain the antecedent's gender but exhibit underlying masculine bias. In German, this bias is much stronger, overriding all tested gender-neutralization strategies. 

**Abstract (ZH)**: 性别中立语言通常用于确保所有个体，无论其性别，都可以与特定概念相关联。尽管心理语言学研究已经考察了性别中立语言对人类认知的影响，但关于大型语言模型（LLMs）如何处理性别中立语言的问题仍不清楚。鉴于商业LLMs在日常应用中的地位日益增强，重要的是要研究LLMs是否实际上以中立的方式解释性别中立语言，因为它们生成的语言有可能影响用户的语言表达。本研究探讨了LLMs生成的核心参照词是否与给定的性别表达一致，或反映模型偏见。我们将心理语言学方法从法语和德语中借鉴到英语中，发现LLMs在英语中通常保持先行词的性别，但表现出潜在的男性偏向。在德语中，这种偏向更为强烈，能够超越所有测试的性别中性化策略。 

---
# Performance Evaluation of Large Language Models in Statistical Programming 

**Title (ZH)**: 统计编程中大型语言模型的性能评估 

**Authors**: Xinyi Song, Kexin Xie, Lina Lee, Ruizhe Chen, Jared M. Clark, Hao He, Haoran He, Jie Min, Xinlei Zhang, Simin Zheng, Zhiyang Zhang, Xinwei Deng, Yili Hong  

**Link**: [PDF](https://arxiv.org/pdf/2502.13117)  

**Abstract**: The programming capabilities of large language models (LLMs) have revolutionized automatic code generation and opened new avenues for automatic statistical analysis. However, the validity and quality of these generated codes need to be systematically evaluated before they can be widely adopted. Despite their growing prominence, a comprehensive evaluation of statistical code generated by LLMs remains scarce in the literature. In this paper, we assess the performance of LLMs, including two versions of ChatGPT and one version of Llama, in the domain of SAS programming for statistical analysis. Our study utilizes a set of statistical analysis tasks encompassing diverse statistical topics and datasets. Each task includes a problem description, dataset information, and human-verified SAS code. We conduct a comprehensive assessment of the quality of SAS code generated by LLMs through human expert evaluation based on correctness, effectiveness, readability, executability, and the accuracy of output results. The analysis of rating scores reveals that while LLMs demonstrate usefulness in generating syntactically correct code, they struggle with tasks requiring deep domain understanding and may produce redundant or incorrect results. This study offers valuable insights into the capabilities and limitations of LLMs in statistical programming, providing guidance for future advancements in AI-assisted coding systems for statistical analysis. 

**Abstract (ZH)**: 大型语言模型（LLMs）的编程能力已经彻底改变了自动代码生成，并为自动统计分析开辟了新的途径。然而，在这些生成的代码可以广泛应用之前，它们的准确性和质量需要系统地评估。尽管LLMs的重要性在不断增加，但关于由LLMs生成的统计代码的全面评估在文献中仍然较少。本文评估了包括两个版本的ChatGPT和一个版本的Llama在内的LLMs在SAS编程领域的表现，特别是在统计分析方面。我们的研究利用了一组涵盖多样化统计主题和数据集的统计分析任务。每个任务包括问题描述、数据集信息以及由人工验证的SAS代码。我们通过对LLMs生成的SAS代码的质量进行基于正确性、有效性、可读性、可执行性和输出结果准确性的人工专家评估，进行全面评估。评级分数的分析表明，虽然LLMs在生成语法正确的代码方面显示出一定的有用性，但在需要深入领域理解的任务中表现不佳，可能会生成冗余或错误的结果。本研究为LLMs在统计编程中的能力和局限性提供了宝贵的见解，并为未来辅助编程系统的进步提供了指导。 

---
# Text2World: Benchmarking Large Language Models for Symbolic World Model Generation 

**Title (ZH)**: 文本到世界：符号世界模型生成的大语言模型基准测试 

**Authors**: Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo  

**Link**: [PDF](https://arxiv.org/pdf/2502.13092)  

**Abstract**: Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at this https URL. 

**Abstract (ZH)**: 近年来，利用大型语言模型（LLMs）从文本描述中生成符号世界模型的兴趣日益增加。虽然LLMs在世界建模的背景下已经被广泛研究，但之前的研究所遇到的挑战包括评价的随机性、依赖于间接指标以及领域范围有限。为了解决这些局限性，我们基于计划定义语言（PDDL）引入了一个新的基准，Text2World，并且采用了多种基于执行的评估指标以提供更稳健的评估。我们使用Text2World对标当前的LLMs，并发现大规模强化学习训练而成的推理模型表现最佳。然而，即使是最优秀的模型在世界建模方面仍表现出有限的能力。基于这些见解，我们探讨了几种有前景的策略来增强LLMs的世界建模能力，包括测试时扩展、智能体训练等。我们希望Text2World可以作为一个关键资源，为利用LLMs作为世界模型的未来研究奠定基础。该项目页面可从以下网址访问：[请在此处插入网址]。 

---
# LAMD: Context-driven Android Malware Detection and Classification with LLMs 

**Title (ZH)**: LAMD：基于上下文的Android恶意软件检测与分类方法（利用大型语言模型） 

**Authors**: Xingzhi Qian, Xinran Zheng, Yiling He, Shuo Yang, Lorenzo Cavallaro  

**Link**: [PDF](https://arxiv.org/pdf/2502.13055)  

**Abstract**: The rapid growth of mobile applications has escalated Android malware threats. Although there are numerous detection methods, they often struggle with evolving attacks, dataset biases, and limited explainability. Large Language Models (LLMs) offer a promising alternative with their zero-shot inference and reasoning capabilities. However, applying LLMs to Android malware detection presents two key challenges: (1)the extensive support code in Android applications, often spanning thousands of classes, exceeds LLMs' context limits and obscures malicious behavior within benign functionality; (2)the structural complexity and interdependencies of Android applications surpass LLMs' sequence-based reasoning, fragmenting code analysis and hindering malicious intent inference. To address these challenges, we propose LAMD, a practical context-driven framework to enable LLM-based Android malware detection. LAMD integrates key context extraction to isolate security-critical code regions and construct program structures, then applies tier-wise code reasoning to analyze application behavior progressively, from low-level instructions to high-level semantics, providing final prediction and explanation. A well-designed factual consistency verification mechanism is equipped to mitigate LLM hallucinations from the first tier. Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes. 

**Abstract (ZH)**: 移动应用的快速增长加剧了针对Android平台的恶意软件威胁。尽管存在众多检测方法，但它们经常难以应对不断演变的攻击、数据集偏见以及有限的解释性。大规模语言模型（LLMs）因其零-shot推理和推理论的能力提供了有希望的替代方案。然而，将LLMs应用于Android恶意软件检测存在两个关键挑战：（1）Android应用中的大量支持代码，往往包含数千个类，超出了LLMs的上下文限制，从而使恶意行为隐藏在良性功能之中；（2）Android应用的结构复杂性和相互依赖关系超过了LLMs基于序列的推理能力，导致代码分析碎片化，并妨碍恶意意图的推断。为了解决这些挑战，我们提出了一种实用的上下文驱动框架LAMD，以使基于LLM的Android恶意软件检测成为可能。LAMD 结合了关键上下文提取，以隔离安全关键代码区域并构建程序结构，然后采用逐级代码推理来逐步分析应用程序行为，从低级指令到高级语义，提供最终的预测和解释。还设计了一套有效的事实一致性验证机制，以减轻第一级LLM的混乱行为。在实际场景中的评估证明了LAMD 的有效性，为动态威胁环境中基于LLM的恶意软件分析奠定了可行的基础。 

---
# LLM-Powered Proactive Data Systems 

**Title (ZH)**: LLM驱动的主动型数据系统 

**Authors**: Sepanta Zeighami, Yiming Lin, Shreya Shankar, Aditya Parameswaran  

**Link**: [PDF](https://arxiv.org/pdf/2502.13016)  

**Abstract**: With the power of LLMs, we now have the ability to query data that was previously impossible to query, including text, images, and video. However, despite this enormous potential, most present-day data systems that leverage LLMs are reactive, reflecting our community's desire to map LLMs to known abstractions. Most data systems treat LLMs as an opaque black box that operates on user inputs and data as is, optimizing them much like any other approximate, expensive UDFs, in conjunction with other relational operators. Such data systems do as they are told, but fail to understand and leverage what the LLM is being asked to do (i.e. the underlying operations, which may be error-prone), the data the LLM is operating on (e.g., long, complex documents), or what the user really needs. They don't take advantage of the characteristics of the operations and/or the data at hand, or ensure correctness of results when there are imprecisions and ambiguities. We argue that data systems instead need to be proactive: they need to be given more agency -- armed with the power of LLMs -- to understand and rework the user inputs and the data and to make decisions on how the operations and the data should be represented and processed. By allowing the data system to parse, rewrite, and decompose user inputs and data, or to interact with the user in ways that go beyond the standard single-shot query-result paradigm, the data system is able to address user needs more efficiently and effectively. These new capabilities lead to a rich design space where the data system takes more initiative: they are empowered to perform optimization based on the transformation operations, data characteristics, and user intent. We discuss various successful examples of how this framework has been and can be applied in real-world tasks, and present future directions for this ambitious research agenda. 

**Abstract (ZH)**: 借助大语言模型（LLM）的力量，我们现在有能力查询之前无法查询的数据，包括文本、图像和视频。然而，尽管有如此巨大的潜力，当前大多数利用LLM的数据系统仍具有反应性，反映出我们的社区希望将LLM映射到已知的抽象概念。大多数数据系统将LLM视为一个不透明的黑盒，它按照用户的输入和数据进行操作，优化它们与现有的其他关系运算符类似，优化它们并和其他近似且昂贵的用户定义函数（UDF）一起工作。这样的数据系统会按指示行事，但无法理解或利用LLM需要执行的操作（即可能错误的操作），正在操作的数据（例如，长而复杂的文档）或者用户真正需要什么。它们没有利用手头操作或数据的特定特征，也没有确保在不精确和歧义情况下结果的正确性。我们认为，数据系统需要更为主动：它们应该获得更多的自主权——借助LLM的力量来理解和重构用户输入和数据，并决定如何表示和处理这些操作和数据。通过允许数据系统解析、重写和分解用户输入和数据，或以超越标准的单次查询-结果范式的与用户交互方式，数据系统能够更高效地满足用户需求。这些新能力开辟了一个丰富多彩的设计空间，在这个空间里，数据系统能够更加主动地进行优化，基于转换操作、数据特征和用户意图进行优化。我们讨论了这一框架在实际任务中成功应用的例子，并提出了这一雄心勃勃的研究议程的未来发展方向。 

---
# Personalized Top-k Set Queries Over Predicted Scores 

**Title (ZH)**: 预测评分下的个性化Top-k集合查询 

**Authors**: Sohrab Namazi Nia, Subhodeep Ghosh, Senjuti Basu Roy, Sihem Amer-Yahia  

**Link**: [PDF](https://arxiv.org/pdf/2502.12998)  

**Abstract**: This work studies the applicability of expensive external oracles such as large language models in answering top-k queries over predicted scores. Such scores are incurred by user-defined functions to answer personalized queries over multi-modal data. We propose a generic computational framework that handles arbitrary set-based scoring functions, as long as the functions could be decomposed into constructs, each of which sent to an oracle (in our case an LLM) to predict partial scores. At a given point in time, the framework assumes a set of responses and their partial predicted scores, and it maintains a collection of possible sets that are likely to be the true top-k. Since calling oracles is costly, our framework judiciously identifies the next construct, i.e., the next best question to ask the oracle so as to maximize the likelihood of identifying the true top-k. We present a principled probabilistic model that quantifies that likelihood. We study efficiency opportunities in designing algorithms. We run an evaluation with three large scale datasets, scoring functions, and baselines. Experiments indicate the efficacy of our framework, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring result accuracy. Scalability experiments further indicate that our framework could be used in large-scale applications. 

**Abstract (ZH)**: 本文研究了昂贵的外部或acles（如大型语言模型）在回答基于预测分数的个性化查询时的适用性。这些分数是由用户定义的函数为多模态数据生成的。我们提出了一种通用的计算框架，该框架能够处理任意的集基评分函数，只要这些函数可以分解为一些各自被发送给或acles（在这种情况下是LLM）预测部分分数的基本构建块。在某一时间点，该框架假设一组响应及其部分预测分数，并维护一个可能的集合，这些集合可能是真实的top-k结果。由于向或acles求解是比较昂贵的操作，我们的框架会精心选择下一个构建块，即向或acles提出下一个最佳问题，以最大化识别真实top-k结果的概率。我们提出了一种原理性的概率模型来量化这种概率。我们研究了算法设计中的效率提升机会。我们使用三个大规模数据集、评分函数和基线进行了评估。实验表明，与基线相比，我们的框架通过减少对LLM的调用次数来显著提高效果，同时保持结果准确性。进一步的可扩展性实验表明，我们的框架可以在大规模应用中使用。 

---
# Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options 

**Title (ZH)**: 选项流：通过考虑选项进行多样性和改进的语言模型推理 

**Authors**: Lakshmi Nair, Ian Trase, Mark Kim  

**Link**: [PDF](https://arxiv.org/pdf/2502.12929)  

**Abstract**: We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning. 

**Abstract (ZH)**: 我们提出了一种新的推理方法，称为“选项流（Flow-of-Options, FoO）”，旨在解决大型语言模型（LLMs）固有的偏差问题。FoO 方法使 LLMs 能够系统地探讨其推理过程中多样化的可能选项，这一能力通过基于 FoO 的自主系统（例如自主解决机器学习任务的 AutoML 系统）得到了验证。我们的框架在标准数据科学任务中优于最先进的基线方法，取得了38.2%至69.2%的性能提升，在治疗化学任务中则取得了37.4%至47.9%的性能提升。由于总体操作成本低于每任务1美元，该框架非常适合成本敏感的应用场景。除了分类和回归任务之外，我们还展示了基于 FoO 的自主系统在强化学习和图像生成等更广泛任务中的适用性。由于 FoO 在压缩和可解释表征中支持多样化的 LLM 解决方案，并且在结合基于案例的推理时支持长期记忆，因此我们的框架相比当前最先进的 AutoML 自主系统展现出了显著的进步。 

---
# Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models 

**Title (ZH)**: DeepSeek和GPT的推理与信任行为：一项揭示大型语言模型隐含裂痕的实验研究 

**Authors**: Rubing Lu, João Sedoc, Arun Sundararajan  

**Link**: [PDF](https://arxiv.org/pdf/2502.12825)  

**Abstract**: When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy. 

**Abstract (ZH)**: 当遇到由新大型语言模型（LLM）带来的日益频繁的功能改进或成本降低时，利用LLM的应用开发人员必须决定是否利用这些改进，还是继续使用较老但经过验证的模型。较低的感知转换障碍可能导致决策忽视了过渡过程中可能引发的更为微妙的行为变化。我们通过使用一个流行的博弈论行为经济学模型来信任行为来展示OpenAI和DeepSeek模型之间的明显差异。我们强调，当o1-mini和o3-mini模型将利润最大化和冒险与信任带来的未来收益相统一时，它们的信任行为出现了崩溃；而这也与DeepSeek模型更复杂且盈利的信任行为形成了对比，后者源自于能够整合更深层次概念如前瞻性规划和心智理论的能力。随着LLM成为高风险商业系统的基石，我们的研究结果突显了依赖于过于狭义定义的LLM性能基准的危害，并建议组织的AI战略应包括对它们隐含故障线的仔细分析。 

---
# How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild 

**Title (ZH)**: 多语言环境下大型语言模型的幻觉程度有多少？关于自然环境下的多语言大型语言模型幻觉估计的研究 

**Authors**: Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš  

**Link**: [PDF](https://arxiv.org/pdf/2502.12769)  

**Abstract**: In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models. 

**Abstract (ZH)**: 在信息误导的时代，幻觉——大型语言模型（LLMs）生成非事实性或不忠实响应的趋势——代表了它们全球效用的主要风险。尽管LLMs正在变得越来越多种语言，但关于检测和量化LLM幻觉的研究大多是（a）以英语为中心，并且（b）集中在机器翻译（MT）和摘要等任务上，而这些任务在现实世界中比开放的信息检索更少见。与此相反，我们的目标是跨语言评估知识密集型长格式问答中的LLM幻觉程度。为此，我们训练了一个多语言幻觉检测模型，并在30种语言和6种开源LLM家族中进行了大规模研究。我们从一个英语幻觉检测数据集开始，并依赖翻译（MT）来生成其他语言的（嘈杂）训练数据。我们还手动标注了五种高资源语言的黄金数据；然后我们证明，对于这些语言，在银色测试集（LLM生成的）和黄金测试集之间，幻觉率的估计值是相似的，这验证了使用银色数据来估计其他语言的幻觉率的有效性。在最终的幻觉率估计中，我们为30种语言构建了一个基于LLM生成提示和维基百科文章的知识密集型问答数据集。我们发现，虽然LLMs为高资源语言生成了更长的响应并包含了更多的幻觉性标记，但语言的数字化表示与其长度归一化后的幻觉率之间没有相关性。此外，我们发现较小的LLMs表现出比较大的模型更高的幻觉率。 

---
# R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs 

**Title (ZH)**: R2-KG：知识图上可靠推理的通用双代理框架 

**Authors**: Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.12767)  

**Abstract**: Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference. 

**Abstract (ZH)**: 近年来，研究者们将大型语言模型（LLMs）与知识图谱（KGs）相结合，以增强推理能力，同时在不进行额外训练的情况下提高推理准确性，减轻幻觉问题。然而，现有的框架往往较为僵化，难以适应知识图谱或任务的变化。它们还高度依赖强大的LLM来进行可靠的（即可信的）推理。为解决这些问题，我们提出了R2-KG，这是一种即插即用的双智能体框架，将推理分为两种角色：操作员（一种低容量的LLM），负责收集证据；监督者（一种高容量的LLM），负责做出最终判决。这种设计在保持强大推理准确性的同时，还最大限度地降低了LLM推理的成本。此外，R2-KG 还采用了一种回避机制（Abstention mechanism），仅在从知识图谱中收集到足够的证据后才生成答案，这极大地提升了推理的可靠性。在多个基于知识图谱的推理任务中的实验显示，无论作为操作员使用的LLM本身的能力如何，R2-KG 在准确性和可靠性方面都始终优于基线方法。进一步的实验表明，单智能体版本的R2-KG，结合严格的自我一致性策略，能够在降低推理成本的同时显著提高可靠性，但在复杂的知识图谱中会导致更高的回避率。我们的研究结果确立了R2-KG 作为一种灵活且成本效益高的知识图谱推理解决方案的地位。它减少了对高容量LLM的依赖，并确保了可信的推理。 

---
# HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading 

**Title (ZH)**: HeadInfer：按头卸载的内存高效大型语言模型推理方法 

**Authors**: Cheng Luo, Zefan Cai, Hanshi Sun, Jinqi Xiao, Bo Yuan, Wen Xiao, Junjie Hu, Jiawei Zhao, Beidi Chen, Anima Anandkumar  

**Link**: [PDF](https://arxiv.org/pdf/2502.12574)  

**Abstract**: Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods. 

**Abstract (ZH)**: 基于Transformer的大语言模型（LLMs）在长上下文生成方面表现出色。扩充上下文长度已经不正常地将LLMs在推理过程中的内存足迹主要转移到了键值缓存（KV缓存）中。本文中，我们提出了HEADINFER方法，该方法将KV缓存卸载到CPU RAM上，同时避免在任何Transformer层上完全存储KV缓存。HEADINFER采用细粒度、按头卸载的策略，在只在GPU上动态计算注意力输出的情况下，仅保留部分注意力头的KV缓存。通过roofline分析，我们展示了HEADINFER在保持计算效率的同时，显著减少了内存足迹。我们使用Llama-3-8B模型对100万 token的序列进行了评估，将KV缓存的GPU内存足迹从128 GB减少到1 GB，总GPU内存使用量从207 GB减少到17 GB，相比BF16基线推理实现了92%的减少。值得注意的是，HEADINFER可以在具有24 GB内存的单个消费级GPU（例如NVIDIA RTX 4090）上实现8B模型的400万token推理，而不需要使用近似方法。 

---
# GSCE: A Prompt Framework with Enhanced Reasoning for Reliable LLM-driven Drone Control 

**Title (ZH)**: GSCE：一种增强推理的提示框架，用于可靠的LLM驱动无人机控制 

**Authors**: Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2502.12531)  

**Abstract**: The integration of Large Language Models (LLMs) into robotic control, including drones, has the potential to revolutionize autonomous systems. Research studies have demonstrated that LLMs can be leveraged to support robotic operations. However, when facing tasks with complex reasoning, concerns and challenges are raised about the reliability of solutions produced by LLMs. In this paper, we propose a prompt framework with enhanced reasoning to enable reliable LLM-driven control for drones. Our framework consists of novel technical components designed using Guidelines, Skill APIs, Constraints, and Examples, namely GSCE. GSCE is featured by its reliable and constraint-compliant code generation. We performed thorough experiments using GSCE for the control of drones with a wide level of task complexities. Our experiment results demonstrate that GSCE can significantly improve task success rates and completeness compared to baseline approaches, highlighting its potential for reliable LLM-driven autonomous drone systems. 

**Abstract (ZH)**: 将大型语言模型（LLMs）集成到机器人控制中，包括无人机，有望革新自主系统。已有研究显示，LLMs可以在支持机器人操作方面发挥重要作用。然而，在面对需要复杂推理的任务时，关于LLMs生成解决方案的可靠性的担忧逐渐增加。本文提出了一种增强推理的提示框架，以实现可靠的大规模语言模型驱动的无人机控制。该框架包含使用指南、技能API、约束和示例（GSCE）设计的新技术组件。GSCE的特点是其可靠的代码生成和与约束的合规性。我们使用GSCE对不同复杂程度的任务进行了全面的无人机控制实验。实验结果显示，与基线方法相比，GSCE显著提高了任务的成功率和完整性，突显了其在可靠的大规模语言模型驱动的自主无人机系统中的潜力。 

---
# MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation 

**Title (ZH)**: MCTS-Judge：作为代码正确性评估的LLM法官的测试时缩放 

**Authors**: Yutong Wang, Pengliang Ji, Chaoqun Yang, Kaixin Li, Ming Hu, Jiaoyang Li, Guillaume Sartoretti  

**Link**: [PDF](https://arxiv.org/pdf/2502.12468)  

**Abstract**: The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm. 

**Abstract (ZH)**: LLM作为法官的范式在评估生成内容方面展现出潜力，但在编程等需要推理的场景中缺乏可靠性。借鉴最近推理模型的进步和规模法则的转变，我们率先将推理时的计算引入LLM作为法官的情景，提出了MCTS-Judge，这是一种资源高效的、适用于代码正确性评估的系统-2思维框架。MCTS-Judge利用蒙特卡洛树搜索（MCTS）将问题分解为更简单的多视角评估。通过结合基于当前轨迹历史行为的自我评估和基于先验展开的树的信心上限（UCB）的节点选择策略，MCTS-Judge平衡了全局优化和当前轨迹的细化。我们进一步设计了一种高精度、单元测试级别的奖励机制，鼓励大型语言模型（LLM）进行逐行分析。在三个基准和五种LLM的广泛实验中，MCTS-Judge展示了其有效性，将基线模型的准确率从41%提高到80%，并且在使用三分之一更少的令牌时超越了o1系列模型。进一步的评估验证了其推理轨迹在逻辑性、分析能力、覆盖面和整体质量上的优越性，同时揭示了LLM作为法官范式的推理时的规模法则。 

---
# EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking 

**Title (ZH)**: EquiBench：通过等价性检查评估大型语言模型的代码推理能力 

**Authors**: Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yaofeng Sun, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, Alex Aiken  

**Link**: [PDF](https://arxiv.org/pdf/2502.12466)  

**Abstract**: Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities. 

**Abstract (ZH)**: 程序等价检验是指确定两个程序在所有可能输入下的输出是否完全相同，这为软件重构、测试和优化等多种应用提供了基础。本文将程序等价检验任务视为评估大型语言模型（LLMs）代码推理能力的一种新方法。我们引入了EquiBench数据集，包含2400个程序对，覆盖了四种编程语言和六种等价类别。这些程序对通过程序分析、编译器调度和超优化系统性生成，涵盖了需要进行深层次语义推理而非仅仅简单的语法变化的复杂结构变换。对17个最先进的LLM的评估结果显示，OpenAI o3-mini 在整体准确率方面最高，达到了78.0%。在最具挑战性的类别中，最高的准确率为62.3%和68.8%，仅略高于二分类的50%随机基线，这表明当前模型在代码推理能力方面仍存在显著的提升空间。 

---
# Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance 

**Title (ZH)**: 压力测试泛化能力：细微修改如何削弱大型语言模型的性能 

**Authors**: Guangxiang Zhao, Saier Hu, Xiaoqi Jian, Jinzhu Wu, Yuhan Wu, Change Jia, Lin Sun, Xiangzheng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.12459)  

**Abstract**: This paper investigates the fragility of Large Language Models (LLMs) in generalizing to novel inputs, specifically focusing on minor perturbations in well-established benchmarks (e.g., slight changes in question format or distractor length). Despite high benchmark scores, LLMs exhibit significant accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT-4 experiences a 25-point accuracy loss when question types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts. This work aligns with the ACL 2025 theme track on the Generalization of NLP models, proposing a "Generalization Stress Test" to assess performance shifts under controlled perturbations. The study calls for reevaluating benchmarks and developing more reliable evaluation methodologies to capture LLM generalization abilities better. 

**Abstract (ZH)**: 本文探讨了大型语言模型（LLMs）在泛化到新颖输入时的脆弱性，特别关注于在成熟基准测试中细微扰动的影响（例如，问题格式的轻微变化或干扰项长度的变化）。尽管在基准测试中的得分很高，但LLMs在面对这些细微但内容保持不变的修改时，显示出显著的准确率下降和意想不到的偏差（例如，偏好更长的干扰项）。例如，当更改选项长度而不改变问题时，Qwen 2.5 1.5B的MMLU分数从60升至89，然后又从89降至36。即使GPT-4在更改问题类型时也经历了25点准确率的下降，且在所有三种修改类别中均降低了6点准确率。这些分析表明，LLMs依赖于表面特征而非形成能够跨越不同格式、词法变化和无关内容移位的稳健、抽象表示。本文与ACL 2025的主题研讨会相契合，该研讨会关注于自然语言处理模型的泛化能力，并提出了一种“泛化压力测试”来评估在受控扰动下性能的变化。研究呼吁重新评估基准测试并开发更可靠的评估方法，以更好地捕捉LLMs的泛化能力。 

---
# Multi-Attribute Steering of Language Models via Targeted Intervention 

**Title (ZH)**: 通过目标干预实现语言模型的多属性引导 

**Authors**: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2502.12446)  

**Abstract**: Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient finetuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline). 

**Abstract (ZH)**: 推理时干预（ITI）已经发展成为一种有前景的方法，在不需要对大型语言模型（LLM）参数进行昂贵更新的情况下，通过干预词元表示来引导LLM的行为朝特定方向发展（例如，提高有用性）。然而，现有的ITI方法难以处理具有冲突的多属性设置，例如在提高有用性的同时减少毒性。为了应对这一挑战，我们引入了多属性定向引导（MAT-Steer），这是一种新颖的引导框架，旨在进行多属性的、选择性的词元级干预。MAT-Steer 使用一种对齐目标来引导模型内部表示，使其不理想的输出接近理想的输出，并通过不同属性间向量的稀疏性和正交性约束来减少属性间的冲突。我们通过两种不同的设置对MAT-Steer进行评估：（i）在问答（QA）任务中，我们平衡真理性和偏见、毒性等属性；（ii）在生成任务中，我们同时提高有用性、正确性和连贯性等属性。在两种任务类型中，MAT-Steer 都优于现有的 ITI 方法和参数效率的微调方法，例如，在问答任务中平均准确率提高3%，并在对抗最好的ITI基线时胜率高达55.82%。 

---
# Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models 

**Title (ZH)**: 大规模语言模型中检测不安全提示的梯度共现分析 

**Authors**: Jingyuan Yang, Bowen Yan, Rongjun Li, Ziyu Zhou, Xin Chen, Zhiyong Feng, Wei Peng  

**Link**: [PDF](https://arxiv.org/pdf/2502.12411)  

**Abstract**: Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins. 

**Abstract (ZH)**: 不安全的提示对大型语言模型（LLMs）构成了显著的安全风险。现有检测不安全提示的方法依赖于数据驱动的微调来训练护栏模型，这需要大量的数据和计算资源。相比之下，最近出现了一些基于少量样本的梯度方法，仅需要少量的安全和不安全参考提示。基于梯度的方法通过分析大型语言模型（LLMs）中安全关键参数梯度的一致模式来识别不安全提示。尽管这些方法有效，但它们对方向相似性的限制（余弦相似性）导致了“方向偏差”，这限制了它们识别不安全提示的能力。为克服这一限制，我们引入了GradCoo，这是一种新颖的梯度共现分析方法，将安全关键参数识别的范围扩展到包括未加权梯度相似性，从而减少了“方向偏差”的影响并提高了不安全提示检测的准确性。在广泛使用的基准数据集ToxicChat和XStest上进行的全面实验表明，我们提出的方法在与现有方法相比时能实现最先进的（SOTA）性能。此外，我们证实了GradCoo在不同规模和来源的大型语言模型（LLM）基础模型中检测不安全提示的一般性。 

---
# QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models 

**Title (ZH)**: QuZO: 量化零阶微调方法应用于大型语言模型 

**Authors**: Jiajun Zhou, Yifan Yang, Kai Zhen, Ziyue Liu, Yequan Zhao, Ershad Banijamali, Athanasios Mouchtaris, Ngai Wong, Zheng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.12346)  

**Abstract**: Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantization often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimization require backpropagation, which are error-prone in the low-precision settings. To overcome these limitations, we propose the Quantized Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low-precision straight-through estimator, and utilizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the training process, while achieving results comparable to first-order methods in ${\rm FP}8$ and superior accuracy in ${\rm INT}8$ and ${\rm INT}4$ training. Experiments demonstrate that low-bit training QuZO achieves performance comparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks, while reducing memory cost by $2.94 \times$ in LLaMA2-7B fine-tuning compared to quantized first-order methods. 

**Abstract (ZH)**: 语言模型（大型语言模型，LLMs）通常被量化到较低的精度（例如32位浮点数降为8位或更低），以减少推理时的内存成本和延迟。然而，量化往往会降低模型性能，因此需要进行下游任务的微调。传统的微调方法，如随机梯度下降和Adam优化，需要进行反向传播，而在低精度设置中容易出错。为了克服这些限制，我们提出了一种名为Quantized Zeroth-Order（QuZO）的框架，专门用于通过低精度正向传播（例如4位或8位）来微调大型语言模型。该方法可以避免使用低精度下的不可靠直通估计器，并利用优化的随机化舍入来缓解增加的偏差。QuZO 简化了训练过程，同时在 ${\rm FP}8$ 中达到了与一阶方法相当的结果，并在 ${\rm INT}8$ 和 ${\rm INT}4$ 训练中取得了更高的准确性。实验结果表明，低位数训练的QuZO 在GLUE、多选题和生成任务上的表现与MeZO优化方法相当，而在对7B参数的LLaMA2进行微调时，相比于量化的一阶方法，内存成本降低了约 $2.94 \times$。 

---
# Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities? 

**Title (ZH)**: 重新审视 o1 类模型的测试时缩放能力：它们真具备测试时缩放的能力吗？ 

**Authors**: Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2502.12215)  

**Abstract**: The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches. 

**Abstract (ZH)**: 测试时缩放在大型语言模型（LLMs）中的出现，以OpenAI的o1系列为例，通过在推理过程中扩展计算资源分配，提高了模型的推理能力。虽然诸如QwQ、Deepseek-R1（R1）和LIMO等后续模型也复制了这些进展，但这些模型是否确实具备测试时缩放能力仍有待进一步探索。研究发现，这些类似o1的模型的较长推理路径并不总是提升准确率；实际上，对于相同问题，正确的答案往往比错误的答案更短。进一步的研究显示，这一现象与模型的自我修订能力密切相关——较长的推理路径包含更多的自我修订，这通常会导致性能下降。我们还对比了在QwQ、R1和LIMO上应用顺序和并行缩放策略的效果，发现并行缩放策略在覆盖范围和可扩展性上表现更好。基于这些发现，我们提出了一种名为“最短多数投票法”的方法，该方法结合了并行缩放策略和推理路径长度的特点，显著提高了模型的测试时可扩展性，相比传统多数投票方法具有显著优势。 

---
# Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement 

**Title (ZH)**: 零令牌驱动的深层思考在大语言模型中的实现：通过循环精炼激发现有参数的全部潜力 

**Authors**: Guanghao Li, Wenhao Jiang, Li Shen, Ming Tang, Chun Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2502.12214)  

**Abstract**: Resource limitations often constrain the parameter counts of Large Language Models (LLMs), hindering their performance. While existing methods employ parameter sharing to reuse the same parameter set under fixed budgets, such approaches typically force each layer to assume multiple roles with a predetermined number of iterations, restricting efficiency and adaptability. In this work, we propose the Zero Token Transformer (ZTT), which features a head-tail decoupled parameter cycling method. We disentangle the first (head) and last (tail) layers from parameter cycling and iteratively refine only the intermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an internal architectural component rather than an input token, to guide layer-specific computation. At each cycle, the model retrieves a zero token (with trainable key values) from a Zero-Token Pool, integrating it alongside regular tokens in the attention mechanism. The corresponding attention scores not only reflect each layer's computational importance but also enable dynamic early exits without sacrificing overall model accuracy. Our approach achieves superior performance under tight parameter budgets, effectively reduces computational overhead via early exits, and can be readily applied to fine-tune existing pre-trained models for enhanced efficiency and adaptability. 

**Abstract (ZH)**: 资源限制往往限制了大型语言模型（LLMs）的参数数量，影响其性能。现有的方法通过参数共享在固定预算下重用相同的参数集，但这类方法通常会强制每个层承担多种角色，并预先决定迭代次数，限制了效率和适应性。在本文中，我们提出了零令牌变换器（ZTT），其特征是一种头尾解耦的参数循环方法。我们将第一层（头层）和最后一层（尾层）从参数循环中分离出来，并仅迭代优化中间层。此外，我们引入了零令牌机制，这是一种内部架构组成部分而非输入令牌，用于引导层特定的计算。在每次循环中，模型从零令牌池中检索一个可训练的零令牌，并将其与常规令牌一起集成到注意机制中。相应的注意分数不仅反映了每层计算的重要性，还允许动态提前退出而不会牺牲整体模型的准确性。我们的方法在严格的参数预算下实现了更优的性能，通过提前退出有效减少了计算开销，并可以方便地应用于现有预训练模型的微调，以提高效率和适应性。 

---
# An Interpretable Automated Mechanism Design Framework with Large Language Models 

**Title (ZH)**: 具有大型语言模型的可解释自动化机制设计框架 

**Authors**: Jiayuan Liu, Mingyu Guo, Vincent Conitzer  

**Link**: [PDF](https://arxiv.org/pdf/2502.12203)  

**Abstract**: Mechanism design has long been a cornerstone of economic theory, with traditional approaches relying on mathematical derivations. Recently, automated approaches, including differentiable economics with neural networks, have emerged for designing payments and allocations. While both analytical and automated methods have advanced the field, they each face significant weaknesses: mathematical derivations are not automated and often struggle to scale to complex problems, while automated and especially neural-network-based approaches suffer from limited interpretability. To address these challenges, we introduce a novel framework that reformulates mechanism design as a code generation task. Using large language models (LLMs), we generate heuristic mechanisms described in code and evolve them to optimize over some evaluation metrics while ensuring key design criteria (e.g., strategy-proofness) through a problem-specific fixing process. This fixing process ensures any mechanism violating the design criteria is adjusted to satisfy them, albeit with some trade-offs in performance metrics. These trade-offs are factored in during the LLM-based evolution process. The code generation capabilities of LLMs enable the discovery of novel and interpretable solutions, bridging the symbolic logic of mechanism design and the generative power of modern AI. Through rigorous experimentation, we demonstrate that LLM-generated mechanisms achieve competitive performance while offering greater interpretability compared to previous approaches. Notably, our framework can rediscover existing manually designed mechanisms and provide insights into neural-network based solutions through Programming-by-Example. These results highlight the potential of LLMs to not only automate but also enhance the transparency and scalability of mechanism design, ensuring safe deployment of the mechanisms in society. 

**Abstract (ZH)**: 机制设计一直是经济理论的基石，传统的研究方法依赖于数学推导。近年来，自动化方法，包括使用神经网络的可微经济学，已经开始用于设计支付和分配机制。虽然分析方法和自动化方法都推动了领域的发展，但它们各自存在显著的局限性：数学推导方法无法自动化且难以处理复杂问题，而自动化方法尤其是基于神经网络的方法则缺乏可解释性。为了解决这些挑战，我们提出了一种新的框架，将机制设计重新表述为代码生成任务。利用大型语言模型（LLMs），我们生成描述为代码的启发式机制，并通过特定问题的校正过程来优化某些评价指标，同时确保关键设计标准（如策略不变性）。这一校正过程确保了任何违反设计标准的机制都会被调整以满足这些标准，尽管这可能会在性能指标上带来一些权衡。这些权衡因素将在基于LLM的进化过程中加以考虑。LLMs的代码生成能力使我们能够发现新颖且可解释的解决方案，从而弥合机制设计的符号逻辑与现代AI的生成能力之间的鸿沟。通过严格的实验证据，我们展示了基于LLM生成的机制能够达到竞争力的性能，同时提供比以往方法更大的可解释性。值得注意的是，我们的框架可以重新发现现有的手工设计机制，并通过示例编程（Programming-by-Example）为基于神经网络的解决方案提供见解。这些结果突显了LLMs不仅能够自动化机制设计，还能够增强其透明度和可扩展性，确保机制在社会中的安全部署。 

---
# Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model 

**Title (ZH)**: 通过大规模语言模型挖掘心力衰竭患者30天再住院的社会决定因素 

**Authors**: Mingchen Shao, Youjeong Kang, Xiao Hu, Hyunjung Gloria Kwak, Carl Yang, Jiaying Lu  

**Link**: [PDF](https://arxiv.org/pdf/2502.12158)  

**Abstract**: Heart Failure (HF) affects millions of Americans and leads to high readmission rates, posing significant healthcare challenges. While Social Determinants of Health (SDOH) such as socioeconomic status and housing stability play critical roles in health outcomes, they are often underrepresented in structured EHRs and hidden in unstructured clinical notes. This study leverages advanced large language models (LLMs) to extract SDOHs from clinical text and uses logistic regression to analyze their association with HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited transportation) linked to readmission risk, this work also offers actionable insights for reducing readmissions and improving patient care. 

**Abstract (ZH)**: 心力衰竭（HF）影响了数百万美国人，并导致较高的再住院率，给医疗保健带来了重大挑战。虽然社会决定因素（SDOH）如社会经济地位和住房稳定性在健康结果中起着关键作用，但它们往往在结构化的电子健康记录（EHRs）中被低估，并隐藏在非结构化的临床笔记中。本研究利用先进的大规模语言模型（LLMs）从临床文本中提取SDOH，并使用逻辑回归分析其与HF再住院之间的关联。通过识别与再住院风险相关的关键SDOH（例如吸烟、有限的交通手段），本研究还为减少再住院和改善患者护理提供了可行的见解。 

---
