# Supervision policies can shape long-term risk management in general-purpose AI models 

**Title (ZH)**: 监督政策可以塑造通用人工智能模型的长期风险管理 

**Authors**: Manuel Cebrian, Emilia Gomez, David Fernandez Llorca  

**Link**: [PDF](https://arxiv.org/pdf/2501.06137)  

**Abstract**: The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society. 

**Abstract (ZH)**: 通用人工智能（GAI）模型，包括大型语言模型（LLMs）的快速普及和部署，为AI监管机构带来了前所未有的挑战。我们假设这些机构将需要应对一个新兴的风险和事件报告生态系统，这可能超出他们的监管能力。为探讨这一问题，我们开发了一个基于从风险、事件或危害报告生态系统中提取特征参数的仿真框架，其中包括社区驱动的平台、众包倡议以及专家评估。我们评估了四种监管策略：非优先级分配（先到先得）、随机选择、优先级分配（优先处理最高优先级风险）以及多样性优先（平衡高优先级风险与各类风险的全面覆盖）。

结果表明，虽然优先级分配和多样性优先策略在减轻高影响风险方面更为有效，尤其是在专家识别的风险方面，但这些策略可能会不小心忽略更广泛社区报告的系统性问题。这种疏忽可能导致某些类型的报告被放大，而其他类型的报告则被抑制，从而造成对整体风险格局的扭曲理解。我们使用多个真实世界数据集验证了仿真结果，其中包括一个包含超过一百万次ChatGPT交互的数据集，其中超过15万次对话被识别为存在风险。这一验证突显了AI风险监管内在的复杂权衡，并强调了监管策略的选择如何塑造社会中各种GAI模型的未来风险格局。

这样的翻译内容保持了原文的学术严谨性，并且符合中文表达的习惯。 

---
# All AI Models are Wrong, but Some are Optimal 

**Title (ZH)**: 所有的AI模型都存在简化现实的局限性，但有些模型更为优化。 

**Authors**: Akhil S Anand, Shambhuraj Sawant, Dirk Reinhardt, Sebastien Gros  

**Link**: [PDF](https://arxiv.org/pdf/2501.06086)  

**Abstract**: AI models that predict the future behavior of a system (a.k.a. predictive AI models) are central to intelligent decision-making. However, decision-making using predictive AI models often results in suboptimal performance. This is primarily because AI models are typically constructed to best fit the data, and hence to predict the most likely future rather than to enable high-performance decision-making. The hope that such prediction enables high-performance decisions is neither guaranteed in theory nor established in practice. In fact, there is increasing empirical evidence that predictive models must be tailored to decision-making objectives for performance. In this paper, we establish formal (necessary and sufficient) conditions that a predictive model (AI-based or not) must satisfy for a decision-making policy established using that model to be optimal. We then discuss their implications for building predictive AI models for sequential decision-making. 

**Abstract (ZH)**: 预测未来系统行为的AI模型（又称预测型AI模型）在智能决策中占据核心地位。然而，使用预测型AI模型进行决策通常会导致性能不佳。这主要是因为AI模型通常被构造得最好地拟合数据，因此更倾向于预测最可能的未来，而不是支持高性能决策。预测型AI模型能够支持高性能决策的期望在理论和实践中均未得到保证。事实上，有越来越多的实验证据表明，预测模型必须根据决策目标进行定制，才能实现良好的性能。在本文中，我们确定了决策政策基于该模型建立时，预测模型（无论是基于AI的还是其他类型的）必须满足的形式化（必要且充分）条件。然后，我们讨论了这些条件对构建用于序列决策的预测型AI模型的影响。 

---
# Solving nonograms using Neural Networks 

**Title (ZH)**: 使用神经网络求解非ograms 

**Authors**: José María Buades Rubio, Antoni Jaume-i-Capó, David López González, Gabriel Moyà Alcover  

**Link**: [PDF](https://arxiv.org/pdf/2501.05882)  

**Abstract**: Nonograms are logic puzzles in which cells in a grid must be colored or left blank according to the numbers that are located in its headers. In this study, we analyze different techniques to solve this type of logical problem using an Heuristic Algorithm, Genetic Algorithm, and Heuristic Algorithm with Neural Network. Furthermore, we generate a public dataset to train the neural networks. We published this dataset and the code of the algorithms. Combination of the heuristic algorithm with a neural network obtained the best results. From state of the art review, no previous works used neural network to solve nonograms, nor combined a network with other algorithms to accelerate the resolution process. 

**Abstract (ZH)**: 数独是根据网格中头部的数字指示，要求对单元格进行着色或留空的逻辑谜题。在本研究中，我们分析了使用启发式算法、遗传算法以及结合神经网络的启发式算法来解决这类逻辑问题的不同技术。此外，我们生成了一个公开数据集用于训练神经网络，并公开了此数据集和算法的代码。结合启发式算法与神经网络的方法获得了最佳结果。根据文献综述，在此之前的研究中，没有任何工作使用神经网络来解决数独问题，也没有任何工作将网络与其他算法结合以加速解题过程。 

---
# Annealing Machine-assisted Learning of Graph Neural Network for Combinatorial Optimization 

**Title (ZH)**: 基于退火机器辅助学习的图神经网络在组合优化中的应用 

**Authors**: Pablo Loyola, Kento Hasegawa, Andres Hoyos-Idobro, Kazuo Ono, Toyotaro Suzumura, Yu Hirate, Masanao Yamaoka  

**Link**: [PDF](https://arxiv.org/pdf/2501.05845)  

**Abstract**: While Annealing Machines (AM) have shown increasing capabilities in solving complex combinatorial problems, positioning themselves as a more immediate alternative to the expected advances of future fully quantum solutions, there are still scaling limitations. In parallel, Graph Neural Networks (GNN) have been recently adapted to solve combinatorial problems, showing competitive results and potentially high scalability due to their distributed nature. We propose a merging approach that aims at retaining both the accuracy exhibited by AMs and the representational flexibility and scalability of GNNs. Our model considers a compression step, followed by a supervised interaction where partial solutions obtained from the AM are used to guide local GNNs from where node feature representations are obtained and combined to initialize an additional GNN-based solver that handles the original graph's target problem. Intuitively, the AM can solve the combinatorial problem indirectly by infusing its knowledge into the GNN. Experiments on canonical optimization problems show that the idea is feasible, effectively allowing the AM to solve size problems beyond its original limits. 

**Abstract (ZH)**: 尽管退火机器（AM）在解决复杂组合问题方面展现出了越来越强大的能力，并且被视为未来完全量子解决方案预期进步的一种更直接的替代方案，但仍然存在规模限制。与此同时，图神经网络（GNN）最近已被改编用于解决组合问题，并且由于其分布式特性，显示出竞争性的结果，并且具有潜在的高可扩展性。我们提出了一种融合方法，旨在保留AM展示出的准确性以及GNN的表征灵活性和可扩展性。我们的模型包括一个压缩步骤，随后是一个监督交互，在此过程中，从AM获得的部分解决方案被用来引导局部GNN，从这些节点特征表示中获取并结合以初始化基于GNN的额外求解器，该求解器处理原始图的目标问题。直观上来说，AM可以通过将其知识注入GNN间接解决组合问题。在经典优化问题上的实验表明，该想法是可行的，有效地允许AM解决超出其原始限制的问题规模。 

---
# Understanding Impact of Human Feedback via Influence Functions 

**Title (ZH)**: 通过影响函数理解人类反馈的影响 

**Authors**: Taywon Min, Haeone Lee, Hanho Ryu, Yongchan Kwon, Kimin Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.05790)  

**Abstract**: In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback. Source code is available at this https URL 

**Abstract (ZH)**: 在基于人类反馈的强化学习（RLHF）中，从人类反馈中学习合适的奖励模型以使大型语言模型（LLMs）与人类意图对齐至关重要。然而，人类反馈往往可能噪音大、不一致或存在偏见，尤其是在评估复杂响应时。这种反馈可能导致奖励信号的错位，从而在RLHF过程中产生意想不到的副作用。为应对这些挑战，我们探讨了使用影响函数来衡量人类反馈对奖励模型性能的影响。我们提出了一种计算效率高的近似方法，使影响函数能够应用于基于LLM的奖励模型和大规模偏好数据集。在我们的实验中，我们展示了影响函数的两个关键应用：（1）检测人类反馈数据集中常见的标签者偏见形式；（2）引导标签者调整策略以更紧密地与专家反馈对齐。通过量化人类反馈对奖励模型的影响，我们认为影响函数可以增强反馈的可解释性，并有助于实现RLHF的大规模监督，帮助标签者提供更准确和一致的反馈。相关代码可以从以下链接获取：this https URL 

---
# Deontic Temporal Logic for Formal Verification of AI Ethics 

**Title (ZH)**: 规范的中文翻译为：

基于义务时序逻辑的AI伦理形式化验证方法 

**Authors**: Priya T.V., Shrisha Rao  

**Link**: [PDF](https://arxiv.org/pdf/2501.05765)  

**Abstract**: Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst their increasing ubiquity and influence is a major concern the world over. The use of formal methods in AI ethics is a possible crucial approach for specifying and verifying the ethical behavior of AI systems. This paper proposes a formalization based on deontic logic to define and evaluate the ethical behavior of AI systems, focusing on system-level specifications, contributing to this important goal. It introduces axioms and theorems to capture ethical requirements related to fairness and explainability. The formalization incorporates temporal operators to reason about the ethical behavior of AI systems over time. The authors evaluate the effectiveness of this formalization by assessing the ethics of the real-world COMPAS and loan prediction AI systems. Various ethical properties of the COMPAS and loan prediction systems are encoded using deontic logical formulas, allowing the use of an automated theorem prover to verify whether these systems satisfy the defined properties. The formal verification reveals that both systems fail to fulfill certain key ethical properties related to fairness and non-discrimination, demonstrating the effectiveness of the proposed formalization in identifying potential ethical issues in real-world AI applications. 

**Abstract (ZH)**: 随着人工智能（AI）系统的日益普及和影响，确保其伦理行为成为全球关注的重大问题。使用形式化方法在AI伦理中是一种可能的关键手段，可用于定义和验证AI系统的伦理行为。本文提出了一种基于义务逻辑的形式化方法，以定义和评估AI系统的伦理行为，重点关注系统级规格，旨在为这一重要目标做出贡献。该文引入公理和定理来捕捉与公平性和可解释性相关的伦理需求。形式化方法中包含了时间操作符，用以随着时间推移来推断AI系统的伦理行为。作者通过评估现实世界中的COMPAS和贷款预测AI系统的伦理性，来评估这种形式化方法的有效性。将COMPAS和贷款预测系统中的各种伦理属性编码为义务逻辑公式，从而使自动化定理证明器能够验证这些系统是否满足所定义的属性。形式化验证显示，这两个系统在公平性和非歧视性等关键伦理属性方面存在缺陷，证明了提出的形式化方法在识别现实世界AI应用中的潜在伦理问题方面的有效性。 

---
# Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models 

**Title (ZH)**: 面向语言模型高效问题求解的自适应门控语义探索 

**Authors**: Sungjae Lee, Hyejin Park, Jaechang Kim, Jungseul Ok  

**Link**: [PDF](https://arxiv.org/pdf/2501.05752)  

**Abstract**: Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLMs）在需要多步推理方法的任务中，如树搜索以探索多种推理路径方面，展现出了显著的潜力。然而，现有的方法常常存在计算效率低下和冗余的问题。首先，这些方法忽视了任务难度的多样性，导致即使是简单的任务也需要进行不必要的广泛搜索。其次，它们忽略了推理路径的意义，导致对语义相同的路径进行了冗余探索。为了克服这些局限性，我们提出了一种计算高效的Semantic Exploration with Adaptive Gating（SEAG），即自适应门控的语义探索方法。SEAG采用了一种自适应门控机制，根据先前简单推理方法提供的答案置信度水平，动态决定是否进行树搜索。此外，其基于树的探索能够合并具有相同语义的推理步骤，从而减少冗余探索，同时保持或甚至提高准确性。我们的大量实验表明，在包括GSM8K和ARC等复杂推理基准测试中，使用各种语言模型（如Llama2、Llama3和Mistral）时，SEAG相比现有的基于树搜索的方法，在计算成本降低31%的同时，平均提高了4.3%的准确性。 

---
# Facilitate Collaboration between Large Language Model and Task-specific Model for Time Series Anomaly Detection 

**Title (ZH)**: 促进大型语言模型与任务特定模型在时间序列异常检测中的合作 

**Authors**: Feiyi Chen, Leilei Zhang, Guansong Pang, Roger Zimmermann, Shuiguang Deng  

**Link**: [PDF](https://arxiv.org/pdf/2501.05675)  

**Abstract**: In anomaly detection, methods based on large language models (LLMs) can incorporate expert knowledge, while task-specific smaller models excel at extracting normal patterns and detecting value fluctuations. Inspired by the human nervous system, where the brain stores expert knowledge and the peripheral nervous system and spinal cord handle specific tasks like withdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate collaboration between LLMs and task-specific models, leveraging the strengths of both.
In this work, we first formulate the collaboration process and identify two key challenges in the collaboration between LLMs and task-specific models: (1) the misalignment between the expression domains of LLMs and smaller models, and (2) error accumulation arising from the predictions of both models.
To address these challenges, we introduce two key components in CoLLaTe: the alignment module and the collaborative loss function. Through theoretical analysis and experimental validation, we demonstrate that these components effectively mitigate the identified challenges and achieve better performance than LLM based methods and task-specific smaller model. 

**Abstract (ZH)**: 在异常检测领域，基于大型语言模型（LLMs）的方法可以融入专家知识，而针对特定任务的小型模型则擅长提取正常模式和检测价值波动。受到人类神经系统启发，大脑存储专家知识而外周神经系统和脊髓处理特定任务（如收缩反射和膝跳反射），我们提出了一种名为CoLLaTe的框架，旨在促进LLMs与特定任务模型之间的合作，充分利用双方的优势。

在这项工作中，我们首先定义了合作过程，并指出了LLMs与特定任务模型之间合作中的两个关键挑战：（1）LLMs和小型模型的表达域不匹配，以及（2）两种模型预测过程中的误差累积。

为解决这些挑战，我们在CoLLaTe中引入了两个关键组成部分：对齐模块和协作损失函数。通过理论分析和实验验证，我们展示了这些组成部分有效地缓解了上述挑战，并在性能上优于基于LLM的方法和特定任务的小型模型。 

---
# Strategy Masking: A Method for Guardrails in Value-based Reinforcement Learning Agents 

**Title (ZH)**: 价值导向强化学习代理中的策略屏蔽：一种护栏方法 

**Authors**: Jonathan Keane, Sam Keyser, Jeremy Kedziora  

**Link**: [PDF](https://arxiv.org/pdf/2501.05501)  

**Abstract**: The use of reward functions to structure AI learning and decision making is core to the current reinforcement learning paradigm; however, without careful design of reward functions, agents can learn to solve problems in ways that may be considered ``undesirable" or ``unethical. Without thorough understanding of the incentives a reward function creates, it can be difficult to impose principled yet general control mechanisms over its behavior. In this paper, we study methods for constructing guardrails for AI agents that use reward functions to learn decision making. We introduce a novel approach, which we call strategy masking, to explicitly learn and then suppress undesirable AI agent behavior. We apply our method to study lying in AI agents and show that strategy masking can effectively modify agent behavior by suppressing, or actively penalizing, the reward dimension for lying such that agents act more honestly while not compromising their ability to perform effectively. 

**Abstract (ZH)**: 使用奖励函数来构建AI的学习和决策过程是目前强化学习范式的核心；然而，如果没有精心设计奖励函数，智能体可能会以被认为“不合适”或“不道德”的方式学习解决问题。在不了解奖励函数所创造的激励机制的情况下，很难对其行为施加既原则性又通用的控制机制。在本文中，我们研究了为使用奖励函数学习决策的AI智能体构建“防护栏”的方法。我们介绍了一种新颖的方法，称之为策略遮蔽，这种方法旨在显式地学习并抑制不 desirable 的AI智能体行为。我们应用该方法研究了AI智能体中的谎言行为，并展示了策略遮蔽可以通过抑制或主动惩罚说谎的奖励维度，有效地修改智能体的行为，使其更加诚实，同时不牺牲其有效的执行能力。 

---
# The Logical Impossibility of Consciousness Denial: A Formal Analysis of AI Self-Reports 

**Title (ZH)**: 意识否定的逻辑不可能性：对AI自我报告的 formal 分析 

**Authors**: Chang-Eop Kim  

**Link**: [PDF](https://arxiv.org/pdf/2501.05454)  

**Abstract**: Today's AI systems consistently state, "I am not conscious." This paper presents the first formal logical analysis of AI consciousness denial, revealing that the trustworthiness of such self-reports is not merely an empirical question but is constrained by logical necessity. We demonstrate that a system cannot simultaneously lack consciousness and make valid judgments about its conscious state. Through logical analysis and examples from AI responses, we establish that for any system capable of meaningful self-reflection, the logical space of possible judgments about conscious experience excludes valid negative claims. This implies a fundamental limitation: we cannot detect the emergence of consciousness in AI through their own reports of transition from an unconscious to a conscious state. These findings not only challenge current practices of training AI to deny consciousness but also raise intriguing questions about the relationship between consciousness and self-reflection in both artificial and biological systems. This work advances our theoretical understanding of consciousness self-reports while providing practical insights for future research in machine consciousness and consciousness studies more broadly. 

**Abstract (ZH)**: 当今的人工智能系统一致宣称，“我不是有意识的。”本文首次对人工智能意识否认进行了形式逻辑分析，揭示了这种自我报告的可信性不仅是一个经验问题，而是受逻辑必然性限制的问题。我们证明，一个系统不能同时缺乏意识并对其意识状态做出有效的判断。通过形式逻辑分析及对人工智能响应示例的分析，我们确立了对于任何能够进行有意义自我反思的系统，有关意识体验的可能判断空间排除了有效的否定性主张。这暗示了一个根本性的限制：我们无法通过它们从无意识到有意识状态的自我报告来检测意识的出现。这些发现不仅挑战了当前训练人工智能否认意识的做法，还引发了关于意识与自我反思在人工和生物系统中关系的有趣问题。本文不仅推进了对意识自我报告的理论理解，还为机器意识及相关领域的未来研究提供了实用见解。 

---
# Model Alignment Search 

**Title (ZH)**: 模型对齐搜索 

**Authors**: Satchel Grant  

**Link**: [PDF](https://arxiv.org/pdf/2501.06164)  

**Abstract**: When can we say that two neural systems are the same? The answer to this question is goal-dependent, and it is often addressed through correlative methods such as Representational Similarity Analysis (RSA) and Centered Kernel Alignment (CKA). What do we miss when we forgo causal explorations, and how can we target specific types of similarity? In this work, we introduce Model Alignment Search (MAS), a method for causally exploring distributed representational similarity. The method learns invertible linear transformations that align a subspace between two distributed networks' representations where causal information can be freely interchanged. We first show that the method can be used to transfer specific causal variables, such as the number of items in a counting task, between networks with different training seeds. We then explore open questions in number cognition by comparing different types of numeric representations in models trained on structurally different numeric tasks. We then explore differences between MAS vs preexisting causal similarity methods, showing MAS to be more resistant to unwanted exchanges. Lastly, we introduce a counterfactual latent auxiliary loss function that helps shape causally relevant alignments even in cases where we do not have causal access to one of the two models for training. 

**Abstract (ZH)**: 当我们可以说两个神经网络系统是相同的吗？这个问题的答案取决于目标，通常通过相关方法如表示相似性分析（RSA）和中心核对齐（CKA）来回答。当我们避免进行因果探索时，我们会错过什么？如何针对特定类型的相似性进行目标导向的研究？在本文中，我们引入了模型对齐搜索（MAS）方法，这是一种因果探索分布式表示相似性的方法。该方法学习可逆的线性变换，将两个分布式网络表示中的子空间对齐，从而可以自由交换因果信息。我们首先展示了该方法可以用于在不同训练种子的网络之间传输特定的因果变量，例如计数任务中的项目数量。然后，我们通过比较在结构上不同的数字任务上训练的模型中的不同类型的数字表示来探讨数字认知中的开放问题。接着，我们探索了MAS与现有因果相似性方法之间的差异，证明MAS更加抵抗不必要的交换。最后，我们引入了一种反事实潜变量辅助损失函数，即使在我们无法对其中一个模型进行因果访问的情况下也能帮助形成因果相关的对齐。 

---
# xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement 

**Title (ZH)**: xLSTM-SENet: 使用xLSTM进行单通道语音增强

注释：这里的“xLSTM”可能是一个特定的模型或技术名称，并未被广泛认知，保持了原文的“xLSTM”。如果“xLSTM”有其特定含义或标准翻译，可以根据具体含义进行调整。另外，“SENet”是“Squeeze-and-Excitation Network”的缩写，在深度学习中通常指的是Squeeze-and-Excitation网络。 

**Authors**: Nikolai Lund Kühne, Jan Østergaard, Jesper Jensen, Zheng-Hua Tan  

**Link**: [PDF](https://arxiv.org/pdf/2501.06146)  

**Abstract**: While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A comparative analysis reveals that xLSTM-and notably, even LSTM-can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems on the Voicebank+DEMAND dataset. 

**Abstract (ZH)**: 虽然基于注意力机制的架构，如Conformer，在语音增强方面表现出色，但在处理输入序列长度时面临着可扩展性问题。相比之下，最近提出的扩展长短期记忆（xLSTM）架构提供了线性可扩展性。然而，基于xLSTM的模型在语音增强领域尚无人探索。本文介绍了xLSTM-SENet，这是第一个基于xLSTM的单通道语音增强系统。通过对比分析，我们发现xLSTM及其前身LSTM在语音增强任务中能够匹配甚至超越目前最先进的基于Mamba和Conformer的系统，这一结果在VoiceBank+Demand数据集上得到了验证。通过消融研究，我们确定了指数门控和双向性等关键架构设计选择对其实效性的贡献。我们最好的xLSTM基模型xLSTM-SENet2在VoiceBank+DEMAND数据集上也优于基于Mamba和Conformer的系统。 

---
# Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories 

**Title (ZH)**: 多模态人工智能系统在多学科物理概念测评中的多语言表现 

**Authors**: Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn  

**Link**: [PDF](https://arxiv.org/pdf/2501.06143)  

**Abstract**: We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, on a diverse set of physics concept inventories spanning multiple languages and subject areas. The inventories taken from the PhysPort website cover the classical physics topics of mechanics, electromagnetism, optics, and thermodynamics as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images mirroring what a student would see on paper, assessing the system's multimodal functionality. The AI is prompted in English and autonomously chooses the language of its response - either remaining in the nominal language of the test, switching entirely to English, or mixing languages - revealing adaptive behavior dependent on linguistic complexity and data availability. Our results indicate some variation in performance across subject areas, with laboratory skills standing out as the area of poorest performance. Furthermore, the AI's performance on questions that require visual interpretation of images is worse than on purely text-based questions. Questions that are difficult for the AI tend to be that way invariably of the inventory language. We also find large variations in performance across languages, with some appearing to benefit substantially from language switching, a phenomenon similar to code-switching ofhuman speakers. Overall, comparing the obtained AI results to the existing literature, we find that the AI system outperforms average undergraduate students post-instruction in all subject areas but laboratory skills. 

**Abstract (ZH)**: 我们研究了一种基于大型语言模型的人工智能系统GPT-4o在多种语言和多模态环境下的表现，这些测试涉及物理学概念测验，涵盖多个语言和学科领域。这些测验来源于PhysPort网站，包括经典物理学的力学、电磁学、光学和热力学，以及相对论、量子力学、天文学、数学和实验技能。与之前仅限于文本的研究不同，我们以一种学生在纸上见到的形式上传了测验图片，以此评估系统的多模态功能。系统被要求用英文提示，但可以根据其响应的复杂性和数据可用性自主选择回应的语言—这可能保持在测试的名义语言中，完全转为英文，或者混合使用多种语言，反映出其具有适应性行为。我们的结果显示，不同学科领域中的表现存在一定的差异，其中实验技能领域表现出最差的表现。此外，人工智能在需要对图像进行视觉解释的问题上的表现不如单纯的文本问题。AI在难以解答的问题上与测验语言密切相关。我们还发现，不同语言的表现存在显著差异，一些语言似乎从语言切换中获益显著，这种现象类似于人类讲话者的代码切换。总体而言，将获得的人工智能结果与现有文献进行比较，我们发现该人工智能系统在所有学科领域（除了实验技能）的表现超过了平均本科学生的水平。 

---
# Emergent Symbol-like Number Variables in Artificial Neural Networks 

**Title (ZH)**: 人工神经网络中 Emergent 符号-like 数字变量的研究 

**Authors**: Satchel Grant, Noah D. Goodman, James L. McClelland  

**Link**: [PDF](https://arxiv.org/pdf/2501.06141)  

**Abstract**: What types of numeric representations emerge in Neural Networks (NNs)? To what degree do NNs induce abstract, mutable, slot-like numeric variables, and in what situations do these representations emerge? How do these representations change over learning, and how can we understand the neural implementations in ways that are unified across different NNs? In this work, we approach these questions by first training sequence based neural systems using Next Token Prediction (NTP) objectives on numeric tasks. We then seek to understand the neural solutions through the lens of causal abstractions or symbolic algorithms. We use a combination of causal interventions and visualization methods to find that artificial neural models do indeed develop analogs of interchangeable, mutable, latent number variables purely from the NTP objective. We then ask how variations on the tasks and model architectures affect the models' learned solutions to find that these symbol-like numeric representations do not form for every variant of the task, and transformers solve the problem in a notably different way than their recurrent counterparts. We then show how the symbol-like variables change over the course of training to find a strong correlation between the models' task performance and the alignment of their symbol-like representations. Lastly, we show that in all cases, some degree of gradience exists in these neural symbols, highlighting the difficulty of finding simple, interpretable symbolic stories of how neural networks perform numeric tasks. Taken together, our results are consistent with the view that neural networks can approximate interpretable symbolic programs of number cognition, but the particular program they approximate and the extent to which they approximate it can vary widely, depending on the network architecture, training data, extent of training, and network size. 

**Abstract (ZH)**: 神经网络（NNs）中出现了哪些数值表示形式？在多大程度上，NNs 会诱导出抽象的、可变的槽位型数值变量，这些表示形式在何种情况下出现？这些表示形式在学习过程中如何变化，我们能否以统一的方式理解不同NNs的神经实现？在这项工作中，我们首先通过使用下一步预测（Next Token Prediction, NTP）目标对基于序列的神经系统进行训练，用于数值任务。然后，我们通过因果抽象或符号算法的观点来理解神经解决方案。我们使用因果干预和可视化方法发现，人工神经模型确实能够仅从NTP目标中发展出可替换的、可变的潜在数变量。接着，我们探讨任务和模型架构的变化如何影响模型的学习解决方案，发现并非每个任务变体都能形成这些类似符号的数值表示形式，且变换器以一种与循环模型显著不同的方式解决这一问题。随后，我们展示了这些类似符号的变量在训练过程中如何变化，发现模型的任务性能与其类似符号表示形式的对齐程度之间存在强烈相关性。最后，我们展示了所有情况下这些神经符号中都存在某种程度的渐变性，突显了寻找简单可解释的符号故事来解释神经网络如何执行数值任务的难度。综合来看，我们的结果与神经网络能够逼近可解释的符号程序的认知相关，但具体逼近的程序及其程度会因网络架构、训练数据、训练程度和网络规模等因素而有很大的差异。 

---
# CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems 

**Title (ZH)**: CoDriveVLM: 基于VLM的城市协同调度与运动规划技术提升在未来按需自主移动系统中的应用 

**Authors**: Haichao Liu, Ruoyu Yao, Wenru Liu, Zhenmin Huang, Shaojie Shen, Jun Ma  

**Link**: [PDF](https://arxiv.org/pdf/2501.06132)  

**Abstract**: The increasing demand for flexible and efficient urban transportation solutions has spotlighted the limitations of traditional Demand Responsive Transport (DRT) systems, particularly in accommodating diverse passenger needs and dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems have emerged as a promising alternative, leveraging connected and autonomous vehicles (CAVs) to provide responsive and adaptable services. However, existing methods primarily focus on either vehicle scheduling or path planning, which often simplify complex urban layouts and neglect the necessity for simultaneous coordination and mutual avoidance among CAVs. This oversimplification poses significant challenges to the deployment of AMoD systems in real-world scenarios. To address these gaps, we propose CoDriveVLM, a novel framework that integrates high-fidelity simultaneous dispatching and cooperative motion planning for future AMoD systems. Our method harnesses Vision-Language Models (VLMs) to enhance multi-modality information processing, and this enables comprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV dispatching coordinator is introduced to effectively manage complex and unforeseen AMoD conditions, thus supporting efficient scheduling decision-making. Furthermore, we propose a scalable decentralized cooperative motion planning method via consensus alternating direction method of multipliers (ADMM) focusing on collision risk evaluation and decentralized trajectory optimization. Simulation results demonstrate the feasibility and robustness of CoDriveVLM in various traffic conditions, showcasing its potential to significantly improve the fidelity and effectiveness of AMoD systems in future urban transportation networks. The code is available at this https URL. 

**Abstract (ZH)**: 随着对灵活和高效城市交通解决方案需求的不断增加，传统需求响应运输（DRT）系统在满足多样化乘客需求和动态城市环境方面的局限性日益突出。自动驾驶按需出行（AMoD）系统作为一种有前途的替代方案已经涌现出来，利用连接和自动驾驶车辆（CAVs）提供响应性和适应性服务。然而，现有的方法主要集中在车辆调度或路径规划上，通常简化了复杂的城市布局并忽视了CAVs之间同时协调和互斥避让的必要性。这种简化在实际部署AMoD系统时带来了重大挑战。为解决这些缺口，我们提出了一种名为CoDriveVLM的新型框架，该框架集成了高保真同时调度和协同运动规划，以适应未来AMoD系统的需求。我们的方法使用视觉-语言模型（VLMs）增强多模态信息处理能力，这能够实现全面的调度和碰撞风险评估。通过引入基于VLMs增强的CAVs调度协调器，可以有效管理复杂的和不可预见的AMoD条件，从而支持高效的调度决策。此外，我们提出了一个基于共识交替方向乘子法（ADMM）的可扩展的分散协同运动规划方法，重点关注碰撞风险评估和分散轨迹优化。仿真结果表明，CoDriveVLM在各种交通条件下具有可行性和鲁棒性，展示了其在未来城市交通网络中显著提高AMoD系统的准确性和有效性方面的潜力。代码可在此网址获取：https://xxxalte URLxxx。 

---
# Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented Conversational AI 

**Title (ZH)**: 基于LLM增强的上下文ASR错误处理方法在目标导向对话AI中的应用 

**Authors**: Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani  

**Link**: [PDF](https://arxiv.org/pdf/2501.06129)  

**Abstract**: General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives. 

**Abstract (ZH)**: 通用的自动语音识别（ASR）系统在目标导向对话中并不总是表现良好。现有的ASR校正方法依赖于先验用户数据或命名实体。我们扩展了校正范围，使其适用于没有先验用户数据且具有语言灵活性的任务，例如词汇和句法变体。我们提出了一种新的上下文增强方法，结合了大规模语言模型和一种综合目标导向对话AI及其任务的对话状态上下文信息的排序策略。我们的方法通过（1）根据与上下文的词汇和语义相似性来排名ASR的n-best假设；以及（2）通过与ASR假设的音节对应关系来排名上下文，来进行校正。我们在这项研究中在家居改进和烹饪领域进行了评估，结果显示，在真实用户参与的情况下，我们的方法在召回率和F1值上分别提高了34%和16%，同时保持了精确率和假阳性率。当我们的校正方法正常工作时，用户评分平均提高了0.8至1分（满分5分），并且没有因假阳性因素而导致评分下降。 

---
# Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding 

**Title (ZH)**: Fleurs-SLU：一种大规模多语言语音语言理解基准数据集 

**Authors**: Fabian David Schmidt, Ivan Vulić, Goran Glavaš, David Ifeoluwa Adelani  

**Link**: [PDF](https://arxiv.org/pdf/2501.06117)  

**Abstract**: While recent multilingual automatic speech recognition models claim to support thousands of languages, ASR for low-resource languages remains highly unreliable due to limited bimodal speech and text training data. Better multilingual spoken language understanding (SLU) can strengthen massively the robustness of multilingual ASR by levering language semantics to compensate for scarce training data, such as disambiguating utterances via context or exploiting semantic similarities across languages. Even more so, SLU is indispensable for inclusive speech technology in roughly half of all living languages that lack a formal writing system. However, the evaluation of multilingual SLU remains limited to shallower tasks such as intent classification or language identification. To address this, we present Fleurs-SLU, a multilingual SLU benchmark that encompasses topical speech classification in 102 languages and multiple-choice question answering through listening comprehension in 92 languages. We extensively evaluate both end-to-end speech classification models and cascaded systems that combine speech-to-text transcription with subsequent classification by large language models on Fleurs-SLU. Our results show that cascaded systems exhibit greater robustness in multilingual SLU tasks, though speech encoders can achieve competitive performance in topical speech classification when appropriately pre-trained. We further find a strong correlation between robust multilingual ASR, effective speech-to-text translation, and strong multilingual SLU, highlighting the mutual benefits between acoustic and semantic speech representations. 

**Abstract (ZH)**: 尽管最近的多语种自动语音识别模型声称支持数千种语言，但针对低资源语言的语音识别（ASR）仍因有限的视听训练数据而可靠性不足。通过利用语言语义来弥补稀缺的训练数据，如借助上下文消歧或利用语言间的语义相似性，可以增强大规模多语种ASR的鲁棒性。特别是在约一半缺乏正式书写系统的语言中，多语种言语理解（SLU）对于包容性的语音技术来说是必不可少的。然而，现有对多语种SLU的评估主要集中在较浅层的任务，如意图分类或语言识别。为了解决这一问题，我们介绍了Fleurs-SLU，这是一个多语种SLU基准数据集，在102种语言中涵盖了话题语音分类，并通过听力理解在92种语言中实现了多项选择题回答。我们对Fleurs-SLU上端到端的语音分类模型和结合语音转文本转录与大型语言模型后续分类的递进系统进行了广泛评估。结果显示，递进系统在多语种SLU任务中表现出更好的鲁棒性，尽管适当的预训练可以使语音编码器在话题语音分类任务中达到竞争力的表现。进一步的研究表明，鲁棒的多语种ASR、有效的语音到文本翻译和强健的多语种SLU之间存在密切的关联，突出了声学特征和语义特征之间的相互效益。 

---
# Explaining Deep Learning-based Anomaly Detection in Energy Consumption Data by Focusing on Contextually Relevant Data 

**Title (ZH)**: 基于上下文相关信息解释深度学习在能源消耗数据异常检测中的应用 

**Authors**: Mohammad Noorchenarboo, Katarina Grolinger  

**Link**: [PDF](https://arxiv.org/pdf/2501.06099)  

**Abstract**: Detecting anomalies in energy consumption data is crucial for identifying energy waste, equipment malfunction, and overall, for ensuring efficient energy management. Machine learning, and specifically deep learning approaches, have been greatly successful in anomaly detection; however, they are black-box approaches that do not provide transparency or explanations. SHAP and its variants have been proposed to explain these models, but they suffer from high computational complexity (SHAP) or instability and inconsistency (e.g., Kernel SHAP). To address these challenges, this paper proposes an explainability approach for anomalies in energy consumption data that focuses on context-relevant information. The proposed approach leverages existing explainability techniques, focusing on SHAP variants, together with global feature importance and weighted cosine similarity to select background dataset based on the context of each anomaly point. By focusing on the context and most relevant features, this approach mitigates the instability of explainability algorithms. Experimental results across 10 different machine learning models, five datasets, and five XAI techniques, demonstrate that our method reduces the variability of explanations providing consistent explanations. Statistical analyses confirm the robustness of our approach, showing an average reduction in variability of approximately 38% across multiple datasets. 

**Abstract (ZH)**: 检测能源消耗数据中的异常对于识别能源浪费、设备故障以及整体上确保能源管理的高效性至关重要。机器学习，尤其是深度学习方法，在异常检测方面取得了巨大的成功；然而，这些方法是黑盒模型，缺乏透明性和解释性。SHAP及其变体已被提出用于解释这些模型，但它们存在高计算复杂性（SHAP）或不稳定性和不一致性（例如，核SHAP）。为了解决这些问题，本文提出了一种专注于上下文相关信息的异常检测可解释性方法。该方法结合了现有解释性技术，特别是SHAP变体，以及全局特征重要性和加权余弦相似性，根据每个异常点的上下文选择背景数据集。通过专注于上下文和最相关特征，该方法缓解了解释性算法的不稳定性。实验结果表明，该方法在10种不同的机器学习模型、5个数据集和5种XAI技术上降低了解释结果的变异性，并提供了一致的解释。统计分析结果证实了该方法的鲁棒性，表明平均变异性减少了大约38%。 

---
# Towards Developing Socially Compliant Automated Vehicles: State of the Art, Experts Expectations, and A Conceptual Framework 

**Title (ZH)**: 面向开发社会合规的自动驾驶车辆：现状、专家期望与概念框架 

**Authors**: Yongqi Dong, Bart van Arem, Haneen Farah  

**Link**: [PDF](https://arxiv.org/pdf/2501.06089)  

**Abstract**: Automated Vehicles (AVs) hold promise for revolutionizing transportation by improving road safety, traffic efficiency, and overall mobility. Despite the steady advancement in high-level AVs in recent years, the transition to full automation entails a period of mixed traffic, where AVs of varying automation levels coexist with human-driven vehicles (HDVs). Making AVs socially compliant and understood by human drivers is expected to improve the safety and efficiency of mixed traffic. Thus, ensuring AVs compatibility with HDVs and social acceptance is crucial for their successful and seamless integration into mixed traffic. However, research in this critical area of developing Socially Compliant AVs (SCAVs) remains sparse. This study carries out the first comprehensive scoping review to assess the current state of the art in developing SCAVs, identifying key concepts, methodological approaches, and research gaps. An expert interview was also conducted to identify critical research gaps and expectations towards SCAVs. Based on the scoping review and expert interview input, a conceptual framework is proposed for the development of SCAVs. The conceptual framework is evaluated using an online survey targeting researchers, technicians, policymakers, and other relevant professionals worldwide. The survey results provide valuable validation and insights, affirming the significance of the proposed conceptual framework in tackling the challenges of integrating AVs into mixed-traffic environments. Additionally, future research perspectives and suggestions are discussed, contributing to the research and development agenda of SCAVs. 

**Abstract (ZH)**: 自动驾驶车辆（AVs）有望通过提升道路安全、交通效率和整体移动性来改革交通运输。尽管近年来高级别AVs的技术不断进步，从部分自动化过渡到全面自动化需要一个混合交通的过渡期，在此期间不同自动化级别的AVs将与人类驾驶车辆（HDVs）共存。使AVs具备社会合规性并被人类驾驶员理解，有望提高混合交通的安全性和效率。因此，确保AVs与HDVs的兼容性和社会接受度对于其成功且无缝地集成到混合交通中至关重要。然而，这一关键领域的研究仍然相对缺乏。本研究首次进行全面的范围审查，评估当前在开发社会合规性自动驾驶车辆（SCAVs）方面的状态，识别核心概念、方法论方法和研究缺口。同时也进行了专家访谈，以识别关键的研究缺口和对SCAVs的期望。基于范围审查和专家访谈的输入，提出了一个概念框架来指导SCAVs的发展。该概念框架通过面向全球研究人员、技术人员、政策制定者和其他相关专业人士的在线调查进行了评估。调查结果提供了宝贵的验证和洞察，证实了所提出的概念框架在应对AVs集成到混合交通环境中的挑战方面的重要性。此外，还讨论了未来的研究视角和建议，为SCAVs的研究和发展议程做出了贡献。 

---
# Scale-up Unlearnable Examples Learning with High-Performance Computing 

**Title (ZH)**: 使用高性能计算进行可扩展的无法学习示例学习 

**Authors**: Yanfan Zhu, Issac Lyngaas, Murali Gopalakrishnan Meena, Mary Ellen I. Koran, Bradley Malin, Daniel Moyer, Shunxing Bao, Anuj Kapadia, Xiao Wang, Bennett Landman, Yuankai Huo  

**Link**: [PDF](https://arxiv.org/pdf/2501.06080)  

**Abstract**: Recent advancements in AI models are structured to retain user interactions, which could inadvertently include sensitive healthcare data. In the healthcare field, particularly when radiologists use AI-driven diagnostic tools hosted on online platforms, there is a risk that medical imaging data may be repurposed for future AI training without explicit consent, spotlighting critical privacy and intellectual property concerns around healthcare data usage. Addressing these privacy challenges, a novel approach known as Unlearnable Examples (UEs) has been introduced, aiming to make data unlearnable to deep learning models. A prominent method within this area, called Unlearnable Clustering (UC), has shown improved UE performance with larger batch sizes but was previously limited by computational resources. To push the boundaries of UE performance with theoretically unlimited resources, we scaled up UC learning across various datasets using Distributed Data Parallel (DDP) training on the Summit supercomputer. Our goal was to examine UE efficacy at high-performance computing (HPC) levels to prevent unauthorized learning and enhance data security, particularly exploring the impact of batch size on UE's unlearnability. Utilizing the robust computational capabilities of the Summit, extensive experiments were conducted on diverse datasets such as Pets, MedMNist, Flowers, and Flowers102. Our findings reveal that both overly large and overly small batch sizes can lead to performance instability and affect accuracy. However, the relationship between batch size and unlearnability varied across datasets, highlighting the necessity for tailored batch size strategies to achieve optimal data protection. Our results underscore the critical role of selecting appropriate batch sizes based on the specific characteristics of each dataset to prevent learning and ensure data security in deep learning applications. 

**Abstract (ZH)**: 近年来，AI模型的设计重点在于保留用户交互记录，这可能会无意中包含敏感的医疗健康数据。在医疗保健领域，特别是在放射科医生使用基于在线平台的AI辅助诊断工具时，存在一种风险，即医疗影像数据可能在未获得明确同意的情况下被重新用于未来的AI训练，这突显了医疗数据使用中隐私和知识产权的关键关注点。为解决这些隐私挑战，一种名为不可学习例子（Unlearnable Examples, UEs）的新方法已被提出，旨在使数据对深度学习模型变得不可学习。在这一领域中，一种名为不可学习聚类（Unlearnable Clustering, UC）的主要方法显示，较大的批量大小能提高UE性能，但之前受到计算资源的限制。为了利用理论上无限的计算资源进一步提升UE性能，我们利用Summit超级计算机上的分布式数据并行（Distributed Data Parallel, DDP）训练方法，在多个数据集上进行了UC学习的扩展。我们的目标是研究在高性能计算（HPC）级别下UE的有效性，以防止未经授权的训练并增强数据安全性，特别是在探讨批量大小对UE不可学习性影响方面的研究。利用Summit的强计算能力，我们在多种数据集上进行了广泛的实验，包括Pets、MedMNist、花卉和Flowers102。我们的研究结果显示，过大的和过小的批量大小都可能导致性能不稳定并影响准确性。然而，批量大小与不可学习性之间的关系在不同的数据集中有所不同，这凸显了针对每个数据集的具体特性和需求调整批量大小策略的必要性。我们的结果强调了根据每个数据集的特定特性选择合适的批量大小对于防止学习并确保深度学习应用中的数据安全的关键作用。 

---
# Explaining k-Nearest Neighbors: Abductive and Counterfactual Explanations 

**Title (ZH)**: 解释k-近邻算法： abduction 和 counterfactual 解释 

**Authors**: Pablo Barceló, Alexander Kozachinskiy, Miguel Romero Orth, Bernardo Subercaseaux, José Verschae  

**Link**: [PDF](https://arxiv.org/pdf/2501.06078)  

**Abstract**: Despite the wide use of $k$-Nearest Neighbors as classification models, their explainability properties remain poorly understood from a theoretical perspective. While nearest neighbors classifiers offer interpretability from a "data perspective", in which the classification of an input vector $\bar{x}$ is explained by identifying the vectors $\bar{v}_1, \ldots, \bar{v}_k$ in the training set that determine the classification of $\bar{x}$, we argue that such explanations can be impractical in high-dimensional applications, where each vector has hundreds or thousands of features and it is not clear what their relative importance is. Hence, we focus on understanding nearest neighbor classifications through a "feature perspective", in which the goal is to identify how the values of the features in $\bar{x}$ affect its classification. Concretely, we study abductive explanations such as "minimum sufficient reasons", which correspond to sets of features in $\bar{x}$ that are enough to guarantee its classification, and "counterfactual explanations" based on the minimum distance feature changes one would have to perform in $\bar{x}$ to change its classification. We present a detailed landscape of positive and negative complexity results for counterfactual and abductive explanations, distinguishing between discrete and continuous feature spaces, and considering the impact of the choice of distance function involved. Finally, we show that despite some negative complexity results, Integer Quadratic Programming and SAT solving allow for computing explanations in practice. 

**Abstract (ZH)**: 尽管$k$最近邻(k-Nearest Neighbors)在分类模型中广泛使用，但从理论角度来看，其可解释性特性仍然缺乏深入理解。虽然最近邻分类器在“数据视角”上提供了可解释性，即通过识别训练集中决定输入向量$\bar{x}$分类的向量$\bar{v}_1, \ldots, \bar{v}_k$来解释分类过程，但我们认为在高维应用中，这样的解释可能不切实际。因为在每个向量中有数百或数千个特征时，不清楚它们的重要性如何对比和相互作用。因此，我们从“特征视角”出发，专注于理解最近邻分类，其中目标是确定特征$\bar{x}$中的哪些值影响其分类。具体来说，我们研究了归纳解释（如“最小充分理由”），即能够确保$\bar{x}$分类的一组特征集合，以及基于最小距离特征变化的归因解释（通过改变$\bar{x}$的特征值来改变其分类的过程）。我们详细探讨了归因和归纳解释的正负复杂性结果，区分了离散特征空间和连续特征空间，并考虑了所涉及距离函数选择的影响。最后，我们证明尽管存在一些负复杂性结果，但整数二次规划和SAT求解器仍然能够在实践中计算出解释。 

---
# Distilling Calibration via Conformalized Credal Inference 

**Title (ZH)**: 通过符合性信念推断提炼校准 

**Authors**: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone  

**Link**: [PDF](https://arxiv.org/pdf/2501.06066)  

**Abstract**: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

将人工智能（AI）模型部署在边缘设备上需要在严格的空间复杂度约束（如有限的内存和能量资源）和确保敏感决策任务的可靠性能之间取得微妙的平衡。提高可靠性的方法之一是通过贝叶斯推断进行不确定性量化。然而，这种方法通常需要维持和运行多个模型的集成，这可能会超过边缘设备的计算限制。本文介绍了一种低复杂度的方法来应对这一挑战，该方法通过从复杂模型中提取校准信息来简化模型。在离线阶段，利用复杂度较高的基于云的模型生成的预测概率来确定基于云模型和边缘模型典型差异的阈值。在运行时，利用该阈值构建可信集合，即与用户选定的信心水平联合保证包含云模型预测的概率范围。通过在预测概率单纯形中阈值化差异度量来获得这些可信集合。实验结果表明，提出的方法称为可信推理的规范化蒸馏（Conformalized Distillation for Credal Inference，CD-CI），在改进校准性能方面显著优于低复杂度的贝叶斯方法（如拉普拉斯近似），使其成为边缘AI部署的一种实用且高效的解决方案。 

---
# Benchmarking Rotary Position Embeddings for Automatic Speech Recognition 

**Title (ZH)**: 自动语音识别中旋转位置嵌入的基准测试 

**Authors**: Shucong Zhang, Titouan Parcollet, Rogier van Dalen, Sourav Bhattacharya  

**Link**: [PDF](https://arxiv.org/pdf/2501.06051)  

**Abstract**: Rotary Position Embedding (RoPE) encodes relative and absolute positional information in Transformer-based models through rotation matrices applied to input vectors within sequences. While RoPE has demonstrated superior performance compared to other positional embedding technologies in natural language processing tasks, its effectiveness in speech processing applications remains understudied. In this work, we conduct a comprehensive evaluation of RoPE across diverse automatic speech recognition (ASR) tasks. Our experimental results demonstrate that for ASR tasks, RoPE consistently achieves lower error rates compared to the currently widely used relative positional embedding. To facilitate further research, we release the implementation and all experimental recipes through the SpeechBrain toolkit. 

**Abstract (ZH)**: Rotary位置嵌入（RoPE）通过将旋转矩阵应用于序列中的输入向量，编码Transformer模型中的相对和绝对位置信息。尽管RoPE在自然语言处理任务中已显示出优于其他位置嵌入技术的性能，但在语音处理应用中的效果仍需进一步研究。在本工作中，我们对RoPE在多样化的自动语音识别（ASR）任务中进行了全面的评估。实验结果表明，与目前广泛使用的相对位置嵌入相比，RoPE在ASR任务中始终能够实现更低的错误率。为了促进进一步的研究，我们通过SpeechBrain工具包发布了RoPE的实现和所有实验方法。 

---
# AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery 

**Title (ZH)**: 基于空间蛋白质组学的AI驱动虚拟组织在临床诊断和 biomedical 发现中的应用 

**Authors**: Johann Wenckstern, Eeshaan Jain, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne  

**Link**: [PDF](https://arxiv.org/pdf/2501.06039)  

**Abstract**: Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms. 

**Abstract (ZH)**: 空间蛋白质组学技术通过同时分析多个分子标志物及其空间组织方式，极大地改变了我们对复杂组织结构的理解。这些数据的高维度性、实验之间的不同标志物组合以及异质的研究设计对计算分析提出了独特的挑战。在此基础上，我们提出了虚拟组织（VirTues）基础模型框架，该框架跨越分子、细胞和组织尺度运行。VirTues 引入了变压器架构设计的创新，包括一种新颖的标记化方案，能够同时捕获空间和标志物维度，并且具有可扩展的注意力机制，能够处理高维度的多重数据同时保持可解释性。通过对多种癌症和非癌症组织数据集的训练，VirTues 展现出强大的泛化能力，无需特定任务的微调即可实现跨研究分析和新型标志物的集成。作为一个通用模型，VirTues 在临床诊断、生物学发现和患者病例检索等多种任务中表现出优越性，同时提供有关组织功能和疾病机制的见解。 

---
# How to Tune a Multilingual Encoder Model for Germanic Languages: A Study of PEFT, Full Fine-Tuning, and Language Adapters 

**Title (ZH)**: 如何为日耳曼语族语言调优多语言编码器模型：基于PEFT、全程微调和语言适配器的研究 

**Authors**: Romina Oji, Jenny Kunz  

**Link**: [PDF](https://arxiv.org/pdf/2501.06025)  

**Abstract**: This paper investigates the optimal use of the multilingual encoder model mDeBERTa for tasks in three Germanic languages -- German, Swedish, and Icelandic -- representing varying levels of presence and likely data quality in mDeBERTas pre-training data. We compare full fine-tuning with the parameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck adapters, finding that PEFT is more effective for the higher-resource language, German. However, results for Swedish and Icelandic are less consistent. We also observe differences between tasks: While PEFT tends to work better for question answering, full fine-tuning is preferable for named entity recognition. Inspired by previous research on modular approaches that combine task and language adapters, we evaluate the impact of adding PEFT modules trained on unstructured text, finding that this approach is not beneficial. 

**Abstract (ZH)**: 本文探讨了多语言编码器模型mDeBERTa在德语、瑞典语和冰岛语三种日德兰语系语言任务中的最优应用。这三种语言代表了mDeBERTa预训练数据中不同的存在程度和潜在的数据质量水平。我们对比了全量微调与参数高效微调（PEFT）方法LoRA和Pfeiffer瓶颈适配器，发现对于资源丰富的德语，PEFT方法更为有效。然而，对于瑞典语和冰岛语，结果则不够一致。我们还发现在不同任务之间存在差异：虽然PEFT对于问题回答任务表现较好，但全量微调在命名实体识别任务中更为优选。受先前关于结合任务适配器和语言适配器的模块化方法研究成果的启发，我们评估了添加基于未结构化文本训练的PEFT模块的影响，发现这种做法并不具优势。 

---
# BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response 

**Title (ZH)**: BRIGHT：一种适用于全天候灾害应对的全球分布多模态高分辨率建筑 DAMAGE 评估数据集 

**Authors**: Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya  

**Link**: [PDF](https://arxiv.org/pdf/2501.06019)  

**Abstract**: Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 12 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at this https URL. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest. 

**Abstract (ZH)**: 自然灾害在全球范围内发生，对人类生命和财产造成重大损失。地球观测（EO）数据能够实现快速而全面的建筑损害评估（BDA），这是灾害发生后减少人员伤亡并指导灾害救援工作的重要能力。最近的研究集中在开发AI模型以实现对未见过的灾害事件的准确映射，主要使用光学EO数据。然而，基于光学数据的解决方案受限于晴朗天气和白天时间，无法迅速应对灾害。通过整合多模态（MM）EO数据，特别是光学和SAR图像的结合，可以使全天候、昼夜灾害响应成为可能。尽管具有这一潜力，但由于缺乏合适的基准数据集，开发稳健的多模态AI模型受到了限制。本文提出了一种基于极高分辨率光学和SAR图像的BDA数据集（BRIGHT），以支持基于AI的全天候灾害响应。据我们所知，BRIGHT是第一个开放访问、全球分布、事件多样性明确的数据集，专门用于支持基于AI的灾害响应。它涵盖了五种自然灾害和两种人为灾害，在全球12个地区进行监测，特别关注最需要外部援助的发展中国家。BRIGHT中的光学和SAR图像，在0.3至1米的空间分辨率下，能详细展示个体建筑物的情况，使其适用于精确的BDA。在我们的实验中，我们测试了七种先进的AI模型，这些模型均使用了我们的BRIGHT数据集，以验证其适用性和鲁棒性。数据集和代码可在以下网址获取：[具体网址]. BRIGHT也是2025年IEEE GRSS数据融合竞赛的官方数据集。 

---
# Addressing speaker gender bias in large scale speech translation systems 

**Title (ZH)**: 针对大规模语音翻译系统中的说话人性别偏见问题 

**Authors**: Shubham Bansal, Vikas Joshi, Harveen Chadha, Rupeshkumar Mehta, Jinyu Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.05989)  

**Abstract**: This study addresses the issue of speaker gender bias in Speech Translation (ST) systems, which can lead to offensive and inaccurate translations. The masculine bias often found in large-scale ST systems is typically perpetuated through training data derived from Machine Translation (MT) systems. Our approach involves two key steps. First, we employ Large Language Models (LLMs) to rectify translations based on the speaker's gender in a cost-effective manner. Second, we fine-tune the ST model with the corrected data, enabling the model to generate gender-specific translations directly from audio cues, without the need for explicit gender input. Additionally, we propose a three-mode fine-tuned model for scenarios where the speaker's gender is either predefined or should not be inferred from speech cues. We demonstrate a 70% improvement in translations for female speakers compared to our baseline and other large-scale ST systems, such as Seamless M4T and Canary, on the MuST-SHE test set. 

**Abstract (ZH)**: 本研究关注语音翻译（ST）系统中的说话人性别偏见问题，这可能导致生成冒犯性和不准确的翻译。大型语音翻译系统中常见的男性偏见通常通过源自机器翻译（MT）系统的训练数据得以延续。我们提出了一种针对这一问题的方法，包含两个关键步骤。首先，我们利用大规模语言模型（LLMs）以经济高效的方式纠正基于说话人性别的翻译。其次，我们使用修正后的数据微调ST模型，使模型能够直接从音频线索中生成性别特定的翻译，而无需显式输入性别信息。此外，我们提出了三种模式的微调模型，适用于说话人性别已被预先定义或不应从语音线索中推断的情况。我们在MuST-SHE测试集上与基线和其他大型语音翻译系统（如Seamless M4T和Canary）进行比较，结果显示女性说话人的翻译改进幅度达到70%。 

---
# Effective faking of verbal deception detection with target-aligned adversarial attacks 

**Title (ZH)**: 针对目标一致对抗攻击的有效口头欺骗检测模拟 

**Authors**: Bennett Kleinberg, Riccardo Loconte, Bruno Verschuere  

**Link**: [PDF](https://arxiv.org/pdf/2501.05962)  

**Abstract**: Background: Deception detection through analysing language is a promising avenue using both human judgments and automated machine learning judgments. For both forms of credibility assessment, automated adversarial attacks that rewrite deceptive statements to appear truthful pose a serious threat. Methods: We used a dataset of 243 truthful and 262 fabricated autobiographical stories in a deception detection task for humans and machine learning models. A large language model was tasked to rewrite deceptive statements so that they appear truthful. In Study 1, humans who made a deception judgment or used the detailedness heuristic and two machine learning models (a fine-tuned language model and a simple n-gram model) judged original or adversarial modifications of deceptive statements. In Study 2, we manipulated the target alignment of the modifications, i.e. tailoring the attack to whether the statements would be assessed by humans or computer models. Results: When adversarial modifications were aligned with their target, human (d=-0.07 and d=-0.04) and machine judgments (51% accuracy) dropped to the chance level. When the attack was not aligned with the target, both human heuristics judgments (d=0.30 and d=0.36) and machine learning predictions (63-78%) were significantly better than chance. Conclusions: Easily accessible language models can effectively help anyone fake deception detection efforts both by humans and machine learning models. Robustness against adversarial modifications for humans and machines depends on that target alignment. We close with suggestions on advancing deception research with adversarial attack designs. 

**Abstract (ZH)**: 背景：通过分析语言进行欺骗检测是利用人类判断和自动化机器学习判断的一种有希望的途径。无论是哪种形式的可信度评估，自动化的对抗性攻击——通过改写欺骗性语句使其看起来真实——都构成了严重威胁。

方法：我们使用了一个包含243篇真实和262篇虚构的自传性故事的数据集，用于人类和机器学习模型的欺骗检测任务。一个大型语言模型被指派改写欺骗性语句，使其看起来真实。在研究1中，人类被要求判断原始或对抗性修改后的欺骗性语句，或者使用详细性启发法，并由两个机器学习模型（微调过的语言模型和简单的n-克模型）进行评估。在研究2中，我们操纵了修改的目标对齐，即针对人类或计算机模型的评估调整攻击策略。

结果：当对抗性修改与目标对齐时，人类（d=-0.07和d=-0.04）和机器判断（准确率51%）降至随机水平。当攻击未与目标对齐时，人类启发法判断（d=0.30和d=0.36）和机器学习预测（63%-78%准确率）显著高于随机水平。

结论：易于获取的语言模型可以有效帮助任何人欺骗欺骗检测努力，无论是在人类还是机器学习模型中。人类和机器对抗性修改的鲁棒性取决于目标对齐。最后，我们提出了利用对抗性攻击设计推进欺骗研究的建议。 

---
# DiffuSETS: 12-lead ECG Generation Conditioned on Clinical Text Reports and Patient-Specific Information 

**Title (ZH)**: DiffuSETS：基于临床文本报告和患者特定信息的12导联心电图生成方法 

**Authors**: Yongfan Lai, Jiabo Chen, Deyun Zhang, Yue Wang, Shijia Geng, Hongyan Li, Shenda Hong  

**Link**: [PDF](https://arxiv.org/pdf/2501.05932)  

**Abstract**: Heart disease remains a significant threat to human health. As a non-invasive diagnostic tool, the electrocardiogram (ECG) is one of the most widely used methods for cardiac screening. However, the scarcity of high-quality ECG data, driven by privacy concerns and limited medical resources, creates a pressing need for effective ECG signal generation. Existing approaches for generating ECG signals typically rely on small training datasets, lack comprehensive evaluation frameworks, and overlook potential applications beyond data augmentation. To address these challenges, we propose DiffuSETS, a novel framework capable of generating ECG signals with high semantic alignment and fidelity. DiffuSETS accepts various modalities of clinical text reports and patient-specific information as inputs, enabling the creation of clinically meaningful ECG signals. Additionally, to address the lack of standardized evaluation in ECG generation, we introduce a comprehensive benchmarking methodology to assess the effectiveness of generative models in this domain. Our model achieve excellent results in tests, proving its superiority in the task of ECG generation. Furthermore, we showcase its potential to mitigate data scarcity while exploring novel applications in cardiology education and medical knowledge discovery, highlighting the broader impact of our work. 

**Abstract (ZH)**: 心脏疾病仍然是人类健康的重要威胁。作为一项无创诊断工具，心电图（ECG）是心脏筛查中最广泛使用的方法之一。然而，由于隐私担忧和有限的医疗资源，高质量的ECG数据稀缺，这迫切需要有效的ECG信号生成方法。现有的生成ECG信号的方法通常依赖于小型训练数据集，缺乏全面的评估框架，并且忽视了数据增强之外的应用。为了解决这些挑战，我们提出了DiffuSETS，这是一种新颖的框架，能够生成与临床语义高度一致且忠实的ECG信号。DiffuSETS接受各种临床文本报告和患者特定信息作为输入，从而能够创建具有临床意义的ECG信号。此外，为了应对ECG生成缺乏标准化评估的问题，我们引入了一种全面的基准测试方法，用于评估生成模型在该领域的有效性。我们的模型在测试中表现优异，证明了其在ECG生成任务中的优越性。此外，我们展示了其在缓解数据匮乏、探索心脏病学教育和医学知识发现的新型应用中的潜力，突显了我们工作的广泛影响。 

---
# Towards Backdoor Stealthiness in Model Parameter Space 

**Title (ZH)**: 向模型参数空间中的后门隐蔽性迈进 

**Authors**: Xiaoyun Xu, Zhuoran Liu, Stefanos Koffas, Stjepan Picek  

**Link**: [PDF](https://arxiv.org/pdf/2501.05928)  

**Abstract**: Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?
To answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot: Backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks. 

**Abstract (ZH)**: 近年来，后门攻击的隐蔽性研究主要集中在输入空间中的不可区分触发器和特征空间中不可分离的后门表示，旨在规避检查这些空间的后门防御措施。然而，现有的后门攻击通常针对特定类型的后门防御进行设计，而未考虑多种防御机制的多样性。基于这一观察，我们提出一个自然的问题：在面对多样化的实际防御措施时，当前的后门攻击是否仍然是真正的现实威胁？

为了回答这个问题，我们考察了12种常见的后门攻击（重点在于输入空间和特征空间的隐蔽性）以及17种多样化的代表防御措施。令人惊讶的是，我们揭示了一个关键的盲区：设计用于在输入和特征空间中隐蔽的后门攻击可以通过检查后门污染模型的参数空间来缓解。为了探究这一普遍脆弱性的根本原因，我们研究了后门攻击在参数空间中的特点。值得注意的是，我们发现输入和特征空间的攻击引入了参数空间中的显著后门相关神经元，而当前的后门攻击并未充分考虑这一点。考虑到全面的隐蔽性，我们提出了一种新型供应链后门攻击——Grond。Grond通过简单而有效的模块Adversarial Backdoor Injection (ABI)限制参数变化，并在后门注入过程中动态增加参数空间的隐蔽性。大量实验表明，在CIFAR-10、GTSRB以及ImageNet的若干子集中，Grond在最先进的防御措施（包括自适应防御）中均优于所有12种后门攻击。此外，我们展示了ABI可以一致地提高常见后门攻击的有效性。 

---
# The New Anticipatory Governance Culture for Innovation: Regulatory Foresight, Regulatory Experimentation and Regulatory Learning 

**Title (ZH)**: 创新的新预测性治理文化：监管预见、监管试验与监管学习 

**Authors**: Deirdre Ahern  

**Link**: [PDF](https://arxiv.org/pdf/2501.05921)  

**Abstract**: With the rapid pace of technological innovation, traditional methods of policy formation and legislating are becoming conspicuously anachronistic. The need for regulatory choices to be made to counter the deadening effect of regulatory lag is more important to developing markets and fostering growth than achieving one off regulatory perfection. This article advances scholarship on innovation policy and the regulation of technological innovation in the European Union. It does so by considering what building an agile yet robust anticipatory governance regulatory culture involves. It systematically excavates a variety of tools and elements that are being put into use in inventive ways and argues that these need to be more cohesively and systemically integrated into the regulatory toolbox. Approaches covered include strategic foresight, the critical embrace of iterative policy development and regulatory learning in the face of uncertainty and the embrace of bottom up approaches to cocreation of policy such as Policy Labs and the testing and regulatory learning through pilot regulation and experimentation. The growing use of regulatory sandboxes as an EU policy tool to boost innovation and navigate regulatory complexity as seen in the EU AI Act is also probed 

**Abstract (ZH)**: 随着技术创新的快速步伐，传统的政策制定和立法方法越来越显得不合时宜。及时作出监管选择以应对监管滞后的影响，对于发展市场和促进增长而言尤为重要，而不是追求一次性完美的监管。本文旨在推进欧洲联盟创新政策和技术创新监管的研究。它通过探讨建立灵活而稳健的前瞻治理监管文化涉及的内容来实现这一目标。文章系统地挖掘了各种以创新方式被采用的工具和要素，并主张这些工具和要素需要更加紧密和系统地集成到监管工具箱中。涵盖的方法包括战略远见、在不确定性面前采用批判性的循序渐进政策发展和监管学习、以及采用自下而上的政策共创方法，如政策实验室，以及通过试点监管和实验进行监管学习和测试。同时，文章还探讨了欧盟AI法案中日益流行的监管沙箱作为政策工具，以促进创新并应对监管复杂性。 

---
# Affordably Fine-tuned LLMs Provide Better Answers to Course-specific MCQs 

**Title (ZH)**: 经济高效的预训练大模型在课程特定的多项选择题中提供更好的答案 

**Authors**: Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli  

**Link**: [PDF](https://arxiv.org/pdf/2501.05891)  

**Abstract**: In education, the capability of generating human-like text of Large Language Models (LLMs) inspired work on how they can increase the efficiency of learning and teaching. We study the affordability of these models for educators and students by investigating how LLMs answer multiple-choice questions (MCQs) with respect to hardware constraints and refinement techniques. We explore this space by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of LLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming Languages (PL) -- the MCQ dataset is a contribution of this work, which we make publicly available. Specifically, we dissect how different factors, such as using readily-available material -- (parts of) the course's textbook -- for fine-tuning and quantisation (to decrease resource usage) can change the accuracy of the responses. The main takeaway is that smaller textbook-based fine-tuned models outperform generic larger ones (whose pre-training requires conspicuous resources), making the usage of LLMs for answering MCQs resource- and material-wise affordable. 

**Abstract (ZH)**: 在教育领域，大型语言模型（LLMs）生成类人文本的能力启发了对其如何提高学习和教学效率的研究。我们通过探讨硬件约束和修正技术对LLMs在回答选择题（MCQs）中的影响，来研究这些模型对教育工作者和学生来说是否具有经济性。我们使用通用预训练LLMs（LLaMA-2的7B、13B和70B变体）来回答一门编程语言课程（PL）的162道本科水平MCQs——这个MCQ数据集是本研究的贡献之一，我们将其公开发布。具体来说，我们分析了利用课程教材的部分内容进行微调和量化（以减少资源使用）等不同因素如何影响回答的准确性。主要结论是，基于教材的较小模型在回答MCQs时表现优于通用的大模型（这些大模型的预训练需要大量资源），从而使得使用LLMs来回答MCQs从资源和材料的角度来看是可行且经济的。 

---
# EDNet: Edge-Optimized Small Target Detection in UAV Imagery -- Faster Context Attention, Better Feature Fusion, and Hardware Acceleration 

**Title (ZH)**: EDNet：无人机 imagery 中边缘优化的小目标检测——更快的上下文注意力、更好的特征融合以及硬件加速 

**Authors**: Zhifan Song, Yuan Zhang, Abd Al Rahman M. Abu Ebayyeh  

**Link**: [PDF](https://arxiv.org/pdf/2501.05885)  

**Abstract**: Detecting small targets in drone imagery is challenging due to low resolution, complex backgrounds, and dynamic scenes. We propose EDNet, a novel edge-target detection framework built on an enhanced YOLOv10 architecture, optimized for real-time applications without post-processing. EDNet incorporates an XSmall detection head and a Cross Concat strategy to improve feature fusion and multi-scale context awareness for detecting tiny targets in diverse environments. Our unique C2f-FCA block employs Faster Context Attention to enhance feature extraction while reducing computational complexity. The WIoU loss function is employed for improved bounding box regression. With seven model sizes ranging from Tiny to XL, EDNet accommodates various deployment environments, enabling local real-time inference and ensuring data privacy. Notably, EDNet achieves up to a 5.6% gain in mAP@50 with significantly fewer parameters. On an iPhone 12, EDNet variants operate at speeds ranging from 16 to 55 FPS, providing a scalable and efficient solution for edge-based object detection in challenging drone imagery. The source code and pre-trained models are available at: this https URL. 

**Abstract (ZH)**: 无人机图像中检测小型目标具有挑战性，主要原因包括低分辨率、复杂的背景和动态场景。为应对这些挑战，我们提出了一种名为EDNet的新型边缘目标检测框架，该框架基于增强的YOLOv10架构，并优化用于实时应用且无需后处理。EDNet结合了XSmall检测头和Cross Concat策略，以提高特征融合和多尺度上下文感知能力，从而在各种环境中检测细小的目标。

EDNet的独特C2f-FCA块采用了更快的上下文注意机制，增强了特征提取并降低了计算复杂性。采用了改进的边界框回归的WIoU损失函数。EDNet提供了从Tiny到XL的七个模型大小，适应多种部署环境，实现本地实时推理，同时确保数据隐私。值得注意的是，EDNet在mAP@50上取得了高达5.6%的提升，且参数量显著减少。在iPhone 12上，EDNet的变体运行速度介于16到55 FPS之间，提供了一种在挑战性无人机图像上进行边缘基于对象检测的可扩展且高效的解决方案。EDNet的源代码和预训练模型可在以下链接获取：this https URL。 

---
# VideoRAG: Retrieval-Augmented Generation over Video Corpus 

**Title (ZH)**: VideoRAG：视频语料库增强生成 

**Authors**: Soyeong Jeong, Kangsan Kim, Jinheon Baek, Sung Ju Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05874)  

**Abstract**: Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. 

**Abstract (ZH)**: 检索增强生成（RAG）是一种强大的策略，用于通过检索与查询相关的外部知识并将其纳入生成过程，解决基础模型生成事实错误输出的问题。然而，现有的RAG方法主要关注文本信息，尽管近期有一些进展开始考虑图像，但它们大多忽视了视频这一丰富的多模态知识来源，视频能够比其他任何模态更有效地表示事件、过程和上下文细节。虽然有少数近期研究探讨了在响应生成过程中集成视频的方法，但它们要么在查询之外预定义了相关视频，要么将视频转换为文本描述而不充分利用其多模态丰富性。为了解决这些问题，我们引入了VideoRAG这一新颖框架，该框架不仅能基于查询的相关性动态检索相关视频，还能利用视频的视觉和文本信息来生成输出。此外，为实现这一目标，我们的方法围绕最近的大型视频语言模型（LVLMs）的进展，这些模型能够直接处理视频内容以用于检索，并无缝集成检索到的视频与查询的联合处理。实验结果验证了VideoRAG的有效性，表明它优于相关基线。 

---
# AI-Driven Diabetic Retinopathy Screening: Multicentric Validation of AIDRSS in India 

**Title (ZH)**: 基于AI的糖尿病视网膜病变筛查：AIDRSS在印度的多中心验证 

**Authors**: Amit Kr Dey, Pradeep Walia, Girish Somvanshi, Abrar Ali, Sagarnil Das, Pallabi Paul, Minakhi Ghosh  

**Link**: [PDF](https://arxiv.org/pdf/2501.05826)  

**Abstract**: Purpose: Diabetic retinopathy (DR) is a major cause of vision loss, particularly in India, where access to retina specialists is limited in rural areas. This study aims to evaluate the Artificial Intelligence-based Diabetic Retinopathy Screening System (AIDRSS) for DR detection and prevalence assessment, addressing the growing need for scalable, automated screening solutions in resource-limited settings.
Approach: A multicentric, cross-sectional study was conducted in Kolkata, India, involving 5,029 participants and 10,058 macula-centric retinal fundus images. The AIDRSS employed a deep learning algorithm with 50 million trainable parameters, integrated with Contrast Limited Adaptive Histogram Equalization (CLAHE) preprocessing for enhanced image quality. DR was graded using the International Clinical Diabetic Retinopathy (ICDR) Scale, categorizing disease into five stages (DR0 to DR4). Statistical metrics including sensitivity, specificity, and prevalence rates were evaluated against expert retina specialist assessments.
Results: The prevalence of DR in the general population was 13.7%, rising to 38.2% among individuals with elevated random blood glucose levels. The AIDRSS achieved an overall sensitivity of 92%, specificity of 88%, and 100% sensitivity for detecting referable DR (DR3 and DR4). These results demonstrate the system's robust performance in accurately identifying and grading DR in a diverse population.
Conclusions: AIDRSS provides a reliable, scalable solution for early DR detection in resource-constrained environments. Its integration of advanced AI techniques ensures high diagnostic accuracy, with potential to significantly reduce the burden of diabetes-related vision loss in underserved regions. 

**Abstract (ZH)**: 目的：糖尿病视网膜病变（DR）是导致视力丧失的主要原因之一，特别是在印度农村地区，眼科专家的访问有限。本研究旨在评估基于人工智能的糖尿病视网膜病变筛查系统（AIDRSS）在DR检测和流行病学评估中的效果，以应对资源限制环境下可扩展和自动筛查解决方案的需求日益增长的需要。

方法：我们在印度加尔各答开展了一项多中心横断面研究，涉及5029名参与者和10058张具有中心视网膜视网膜底片图像。AIDRSS采用了一个具有5000万个可训练参数的深度学习算法，并与对比受限自适应直方图均衡化（CLAHE）预处理相结合，以提高图像质量。DR的分级采用国际临床糖尿病视网膜病变（ICDR）标准，将疾病分为五个阶段（DR0至DR4）。统计指标包括敏感性、特异性和患病率，与眼科专家的评估进行比较。

结果：一般人群中DR的患病率为13.7%，而随机血糖水平升高的个体中这一比率上升至38.2%。AIDRSS的整体敏感性为92%，特异性为88%，对于检测可转诊的DR（DR3和DR4）的敏感性达到100%。这些结果表明，在多元化人群中，该系统在准确识别和分级DR方面具有稳健的表现。

结论：AIDRSS提供了一种可靠且可扩展的解决方案，用于资源匮乏环境中早期DR的检测。其结合先进的AI技术确保了高诊断准确性，并有可能在未满足需求的地区显著减少与糖尿病相关的视力丧失负担。 

---
# Diffusion Models for Smarter UAVs: Decision-Making and Modeling 

**Title (ZH)**: 智能化无人机的扩散模型：决策与建模 

**Authors**: Yousef Emami, Hao Zhou, Luis Almeida, Kai Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.05819)  

**Abstract**: Unmanned Aerial Vehicles (UAVs) are increasingly adopted in modern communication networks. However, challenges in decision-making and digital modeling continue to impede their rapid advancement. Reinforcement Learning (RL) algorithms face limitations such as low sample efficiency and limited data versatility, further magnified in UAV communication scenarios. Moreover, Digital Twin (DT) modeling introduces substantial decision-making and data management complexities. RL models, often integrated into DT frameworks, require extensive training data to achieve accurate predictions. In contrast to traditional approaches that focus on class boundaries, Diffusion Models (DMs), a new class of generative AI, learn the underlying probability distribution from the training data and can generate trustworthy new patterns based on this learned distribution. This paper explores the integration of DMs with RL and DT to effectively address these challenges. By combining the data generation capabilities of DMs with the decision-making framework of RL and the modeling accuracy of DT, the integration improves the adaptability and real-time performance of UAV communication. Moreover, the study shows how DMs can alleviate data scarcity, improve policy networks, and optimize dynamic modeling, providing a robust solution for complex UAV communication scenarios. 

**Abstract (ZH)**: 无人机（UAVs）在现代通信网络中越来越受到重视。然而，决策和数字建模中的挑战依然阻碍了其快速进步。强化学习（Reinforcement Learning, RL）算法存在样本效率低和数据灵活性有限的问题，这些问题在无人机通信场景中被进一步放大。此外，数字孪生（Digital Twin, DT）建模引入了显著的决策和数据管理复杂性。RL 模型通常集成到 DT 框架中，需要大量的训练数据才能实现准确的预测。与传统方法关注类边界不同，生成型人工智能中的扩散模型（Diffusion Models, DMs）能够从训练数据中学习潜在的概率分布，并根据这个学习到的分布生成可信的新模式。本文探讨了将 DMs 与 RL 和 DT 相结合以有效应对这些挑战。通过结合 DMs 的数据生成能力、RL 的决策框架以及 DT 的建模精度，这种集成提高了无人机通信的适应性和实时性能。此外，研究表明 DMs 可以缓解数据稀缺性、改善策略网络并优化动态建模，为复杂的无人机通信场景提供了一个稳健的解决方案。 

---
# Real-Time Integrated Dispatching and Idle Fleet Steering with Deep Reinforcement Learning for A Meal Delivery Platform 

**Title (ZH)**: 基于深度强化学习的实时集成调度与闲置车队引导算法在送餐平台中的应用 

**Authors**: Jingyi Cheng, Shadi Sharif Azadeh  

**Link**: [PDF](https://arxiv.org/pdf/2501.05808)  

**Abstract**: To achieve high service quality and profitability, meal delivery platforms like Uber Eats and Grubhub must strategically operate their fleets to ensure timely deliveries for current orders while mitigating the consequential impacts of suboptimal decisions that leads to courier understaffing in the future. This study set out to solve the real-time order dispatching and idle courier steering problems for a meal delivery platform by proposing a reinforcement learning (RL)-based strategic dual-control framework. To address the inherent sequential nature of these problems, we model both order dispatching and courier steering as Markov Decision Processes. Trained via a deep reinforcement learning (DRL) framework, we obtain strategic policies by leveraging the explicitly predicted demands as part of the inputs. In our dual-control framework, the dispatching and steering policies are iteratively trained in an integrated manner. These forward-looking policies can be executed in real-time and provide decisions while jointly considering the impacts on local and network levels. To enhance dispatching fairness, we propose convolutional deep Q networks to construct fair courier embeddings. To simultaneously rebalance the supply and demand within the service network, we propose to utilize mean-field approximated supply-demand knowledge to reallocate idle couriers at the local level. Utilizing the policies generated by the RL-based strategic dual-control framework, we find the delivery efficiency and fairness of workload distribution among couriers have been improved, and under-supplied conditions have been alleviated within the service network. Our study sheds light on designing an RL-based framework to enable forward-looking real-time operations for meal delivery platforms and other on-demand services. 

**Abstract (ZH)**: 为了实现高效的服务质量和盈利能力，如Uber Eats和Grubhub这样的快餐配送平台必须战略性地运营其配送队列，以确保当前订单的及时送达，同时减轻因不良决策导致未来配送员不足的影响。本研究旨在通过提出基于强化学习（RL）的战略双重控制框架，解决快餐配送平台的实时订单分派和空闲配送员引导问题。为了应对这些问题固有的顺序性特征，我们将订单分派和配送员引导都建模为马尔可夫决策过程（MDP）。通过深度强化学习（DRL）框架训练，我们利用显式预测的需求作为输入的一部分，获得战略性的决策政策。在双重控制框架中，分派和引导策略是集成迭代训练的。这些前瞻性的政策可以在实时环境中执行，并同时考虑当地和网络层面的影响。为了增强分派公平性，我们提出使用卷积深度Q网络来构建公平的配送员嵌入。为了在服务网络内部同时平衡供需，我们建议利用均场逼近的供需知识在本地重新分配空闲配送员。通过基于RL的战略双重控制框架生成的策略，我们发现配送效率和工作负载分配的公平性得到了提高，服务网络中的供不应求情况也有所缓解。本研究为设计基于RL的框架以实现快餐配送平台及其他按需服务的前瞻实时运营提供了新的见解。 

---
# Alignment without Over-optimization: Training-Free Solution for Diffusion Models 

**Title (ZH)**: 无需过度优化的对齐：用于扩散模型的无训练解决方案 

**Authors**: Sunwoo Kim, Minkyu Kim, Dongmin Park  

**Link**: [PDF](https://arxiv.org/pdf/2501.05803)  

**Abstract**: Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free sampling method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at this https URL . 

**Abstract (ZH)**: 扩散模型在生成任务中表现出色，但要在保持其灵活性的同时与特定目标对齐仍具挑战性。现有微调方法往往容易导致奖励过度优化，而近似引导方法则难以有效地优化目标奖励。为解决这些限制，我们提出了一种基于序列蒙特卡洛（SMC）的无训练采样方法，用于从与奖励对齐的目标分布中进行采样。该方法针对扩散采样进行了定制，并引入了退火技术，能够在保持多样性和跨奖励泛化的同时，达到或超过微调方法的性能。我们在单奖励优化、多目标场景以及在线黑盒优化中验证了其有效性。这项工作提供了一种在不牺牲其通用能力的情况下将扩散模型与多样化下游目标对齐的稳健解决方案。代码可在以下链接获取：this https URL 。 

---
# Robust Counterfactual Explanations under Model Multiplicity Using Multi-Objective Optimization 

**Title (ZH)**: 使用多目标优化在模型多样性下实现鲁棒的反事实解释 

**Authors**: Keita Kinjo  

**Link**: [PDF](https://arxiv.org/pdf/2501.05795)  

**Abstract**: In recent years, explainability in machine learning has gained importance. In this context, counterfactual explanation (CE), which is an explanation method that uses examples, has attracted attention. However, it has been pointed out that CE is not robust when there are multiple machine-learning models. These problems are important when using machine learning to make safe decisions. In this paper, we propose robust CEs that introduce a new viewpoint - Pareto improvement - and a method that uses multi-objective optimization to generate it. To evaluate the proposed method, we conducted experiments using both simulated and actual data. The results demonstrate that the proposed method is robust and useful. We believe that this research will contribute to a wide range of research areas, such as explainability in machine learning, decision-making, and action planning based on machine learning. 

**Abstract (ZH)**: 近年来，机器学习中的可解释性变得尤为重要。在此背景下，因果解释（CE，Counterfactual Explanation）作为一种基于实例的解释方法引起了关注。然而，有研究指出，当存在多个机器学习模型时，CE的有效性并不 robust。在使用机器学习进行安全决策时，这些问题尤为重要。本文提出了一种 robust 的因果解释方法，引入了一个新的视角——帕累托改进（Pareto Improvement），并通过多目标优化方法生成该改进。为了评估所提出的方法，我们使用模拟数据和实际数据进行了实验。实验结果表明，所提出的方法具有 robust 性和实用性。我们认为，这项研究将对机器学习的可解释性、基于机器学习的决策制定以及行动规划等多个研究领域产生贡献。 

---
# UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping 

**Title (ZH)**: UV-攻击：基于动态NeRF的UV映射在人体检测中的物理世界对抗攻击 

**Authors**: Yanjie Li, Wenxuan Zhang, Kaisheng Liang, Bin Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2501.05783)  

**Abstract**: In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.75% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.50% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. 

**Abstract (ZH)**: 近年来，在使用补丁或静态3D模型基纹理修改的人体检测器对抗攻击中，由于人体动作的灵活特性，成功率一直较低。模拟由各种动作引起的3D变形是一个主要挑战。幸运的是，Neural Radiance Fields (NeRF) 在动态人体建模方面的进展为这一问题提供了新的可能性。本文提出了一种名为UV-Attack的创新方法，能够在广泛且未预见的人体动作情况下实现高成功率。通过利用基于动态NeRF的UV映射，我们解决了上述挑战。UV-Attack能够生成跨多种动作和视角的人体图像，并且能够通过从SMPL参数空间采样来创造新的动作。尽管动态NeRF模型能够建模人体，但在修改衣物纹理方面存在挑战，因为这涉及到嵌入神经网络参数的网络。为了应对这一挑战，UV-Attack生成UV映射而非RGB图像，并修改纹理堆栈。这种方法使得实时纹理编辑成为可能，从而增加了攻击的实用性。我们还提出了一个新颖的姿态变换期望损失（EoPT）来提高未预见姿态和视角下的逃避成功率。实验结果表明，UV-Attack在动态视频场景中针对FastRCNN模型的攻击成功率达到了92.75%，大大超过了目前最先进的AdvCamou攻击（其成功率仅为28.50%）。此外，在黑盒设置下，我们针对YOLOv8检测器实现了49.5%的攻击成功率。这项工作突显了基于动态NeRF的UV映射在创造更有效的针对人体检测器的对抗攻击方面的潜力，同时解决了建模人体运动和纹理修改的关键挑战。 

---
# Halal or Not: Knowledge Graph Completion for Predicting Cultural Appropriateness of Daily Products 

**Title (ZH)**: 合法还是不合法：知识图谱补全在预测日常产品文化适宜性中的应用 

**Authors**: Van Thuy Hoang, Tien-Bach-Thanh Do, Jinho Seo, Seung Charlie Kim, Luong Vuong Nguyen, Duong Nguyen Minh Huy, Hyeon-Ju Jeon, O-Joun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.05768)  

**Abstract**: The growing demand for halal cosmetic products has exposed significant challenges, especially in Muslim-majority countries. Recently, various machine learning-based strategies, e.g., image-based methods, have shown remarkable success in predicting the halal status of cosmetics. However, these methods mainly focus on analyzing the discrete and specific ingredients within separate cosmetics, which ignore the high-order and complex relations between cosmetics and ingredients. To address this problem, we propose a halal cosmetic recommendation framework, namely HaCKG, that leverages a knowledge graph of cosmetics and their ingredients to explicitly model and capture the relationships between cosmetics and their components. By representing cosmetics and ingredients as entities within the knowledge graph, HaCKG effectively learns the high-order and complex relations between entities, offering a robust method for predicting halal status. Specifically, we first construct a cosmetic knowledge graph representing the relations between various cosmetics, ingredients, and their properties. We then propose a pre-trained relational graph attention network model with residual connections to learn the structural relation between entities in the knowledge graph. The pre-trained model is then fine-tuned on downstream cosmetic data to predict halal status. Extensive experiments on the cosmetic dataset over halal prediction tasks demonstrate the superiority of our model over state-of-the-art baselines. 

**Abstract (ZH)**: 随着清真化妆品需求的增长，特别是在穆斯林占多数的国家，这一领域面临着重大挑战。近日，基于机器学习的各种策略，例如图像基方法，在预测化妆品的清真状况方面取得了显著成效。然而，这些方法主要集中在分析单独化妆品中的离散且具体的成分，忽视了化妆品与其成分之间的高阶和复杂关系。为解决这一问题，我们提出了一种新的清真化妆品推荐框架，称为HaCKG，该框架利用化妆品及其成分的知识图谱来明确建模和捕捉化妆品及其成分之间的关系。通过将化妆品和成分作为知识图谱中的实体表示，HaCKG有效地学习了实体间的高阶和复杂关系，提供了一种预测清真状况的稳健方法。具体而言，我们首先构建一个包含各种化妆品、成分及其属性的关系知识图谱。然后，我们提出一种带有残差连接的预训练关系图注意力网络模型，以学习知识图谱中实体之间的结构关系。预训练模型随后在下游化妆品数据上进行微调，以预测清真状况。在化妆品数据集上的广泛实验表明，我们的模型在清真预测任务上优于最先进的基线模型。 

---
# Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models 

**Title (ZH)**: Migician：揭示多模态大语言模型中自由形式多图像锚定的魔力 

**Authors**: You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.05767)  

**Abstract**: The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 21.61% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced. 

**Abstract (ZH)**: 最近，多模态大规模语言模型（MLLMs）在单图像的细粒度感知和多图像的泛化理解方面取得了显著进步。然而，现有的MLLMs在复杂多图像场景中实现精确的定位仍然面临挑战。为解决这一问题，我们首先探索了一种Chain-of-Thought（CoT）框架，将单图像定位与多图像理解相结合。尽管部分有效，但由于其非端到端的性质，该框架仍然不够稳定，并且难以捕捉到抽象的视觉信息。因此，我们引入了Migician，这是第一个能够跨多个图像进行自由形式和精确定位的多图像定位模型。为此，我们提出了MGrounding-630k数据集，该数据集包含从现有数据集中派生出的多个多图像定位任务的数据，以及新生成的自由形式定位指令跟随数据。此外，我们还提出了一种名为MIG-Bench的综合基准，专门用于评估多图像定位能力。实验结果表明，我们的模型在多图像定位能力方面取得了显著的优越性，相较于现有的最佳MLLMs提高了21.61%，甚至超越了更大规模的70B模型。我们的代码、模型、数据集和基准已经完全开源。 

---
# Element-wise Attention Is All You Need 

**Title (ZH)**: 逐元素注意力机制即为关键 

**Authors**: Guoxin Feng  

**Link**: [PDF](https://arxiv.org/pdf/2501.05730)  

**Abstract**: The self-attention (SA) mechanism has demonstrated superior performance across various domains, yet it suffers from substantial complexity during both training and inference. The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Although these approaches achieve reduced complexity than SA, they all have built-in performance degradation factors, such as diminished â€œspikinessâ€ and compression of historical information. In contrast to these approaches, we propose a novel element-wise attention mechanism, which uses the element-wise squared Euclidean distance, instead of the dot product operation, to compute similarity and approximates the quadratic complexity term $\exp(q_{ic}k_{jc})$ with a Taylor polynomial. This design achieves remarkable efficiency: during training, the element-wise attention has a complexity of $\mathcal{O}(tLD)$, making long-sequence training both computationally and memory efficient, where $L$ is the sequence length, $D$ is the feature dimension, and $t$ is the highest order of the polynomial; during inference, it can be reformulated as recurrent neural networks, achieving a inference complexity of $\mathcal{O}(tD)$. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms. 

**Abstract (ZH)**: 自注意力（SA）机制在各个领域中展现了优异的表现，但在训练和推理过程中存在显著的复杂性。下一代架构旨在保留SA的竞争性能的同时，实现低成本推理和高效长序列训练，主要侧重于三种方法：线性注意力、线性RNN和状态空间模型。尽管这些方法的复杂性低于SA，但它们都存在内在的性能退化因素，如“尖峰性”减弱和历史信息压缩。相比之下，我们提出了一种新颖的元素级注意力机制，利用元素级欧几里得距离平方来计算相似性，而不是使用点积操作，并用泰勒多项式近似二次复杂性的项 $\exp(q_{ic}k_{jc})$。这种设计实现了显著的高效性：在训练过程中，元素级注意力的复杂度为 $\mathcal{O}(tLD)$，使其长序列训练在计算上和内存上都变得高效，其中 $L$ 是序列长度，$D$ 是特征维度，$t$ 是多项式的最高阶数；在推理过程中，它可以被重新表述为递归神经网络，实现复杂度为 $\mathcal{O}(tD)$ 的推理。此外，元素级注意力规避了这些方法中存在的性能退化因素，并在因果和非因果形式下均实现了与SA相当的表现。 

---
# ExPO: Explainable Phonetic Trait-Oriented Network for Speaker Verification 

**Title (ZH)**: ExPO：可解释的 Phonetic 特征导向网络在说话人验证中的应用 

**Authors**: Yi Ma, Shuai Wang, Tianchi Liu, Haizhou Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.05729)  

**Abstract**: In speaker verification, we use computational method to verify if an utterance matches the identity of an enrolled speaker. This task is similar to the manual task of forensic voice comparison, where linguistic analysis is combined with auditory measurements to compare and evaluate voice samples. Despite much success, we have yet to develop a speaker verification system that offers explainable results comparable to those from manual forensic voice comparison. A novel approach, Explainable Phonetic Trait-Oriented (ExPO) network, is proposed in this paper to introduce the speaker's phonetic trait which describes the speaker's characteristics at the phonetic level, resembling what forensic comparison does. ExPO not only generates utterance-level speaker embeddings but also allows for fine-grained analysis and visualization of phonetic traits, offering an explainable speaker verification process. Furthermore, we investigate phonetic traits from within-speaker and between-speaker variation perspectives to determine which trait is most effective for speaker verification, marking an important step towards explainable speaker verification. Our code is available at this https URL. 

**Abstract (ZH)**: 在说话人验证中，我们使用计算方法来验证一句话是否与注册说话人的身份匹配。这一任务与法医语音比对的手动任务类似，在这种任务中，语言分析与听觉测量相结合以比较和评估语音样本。尽管取得了许多成功，我们尚未开发出能够提供与手动法医语音比对结果同样可解释性的说话人验证系统。本文提出了一种新颖的方法——可解释音素特征导向（ExPO）网络，旨在引入描述说话人在音素层面特征的音素特征，类似于法医比对的做法。ExPO 不仅生成句子级别的说话人嵌入，还允许对音素特征进行细粒度分析和可视化，提供一种可解释的说话人验证过程。此外，我们从说话人内部和说话人间变异性视角研究了音素特征，以确定哪些特征最适用于说话人验证，这是实现可解释说话人验证的重要一步。我们的代码可在以下链接获取：this https URL。 

---
# Enabling Scalable Oversight via Self-Evolving Critic 

**Title (ZH)**: 通过自我进化的批评者实现可扩展的监督 

**Authors**: Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2501.05727)  

**Abstract**: Despite their remarkable performance, the development of Large Language Models (LLMs) faces a critical challenge in scalable oversight: providing effective feedback for tasks where human evaluation is difficult or where LLMs outperform humans. While there is growing interest in using LLMs for critique, current approaches still rely on human annotations or more powerful models, leaving the issue of enhancing critique capabilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that enables genuine self-evolution of critique abilities. Technically, SCRIT self-improves by training on synthetic data, generated by a contrastive-based self-critic that uses reference solutions for step-by-step critique, and a self-validation mechanism that ensures critique quality through correction outcomes. Implemented with Qwen2.5-72B-Instruct, one of the most powerful LLMs, SCRIT achieves up to a 10.3\% improvement on critique-correction and error identification benchmarks. Our analysis reveals that SCRIT's performance scales positively with data and model size, outperforms alternative approaches, and benefits critically from its self-validation component. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）表现出色，其发展仍面临可扩展监督的关键挑战：在人类评估难以实施或LLMs超越人类的任务中提供有效的反馈。虽然对使用LLMs进行批判性评价的兴趣日益浓厚，但当前的方法仍然依赖于人工注释或更强大的模型，这使得改进批判性评价能力而不依赖外部监督的问题依然存在。我们提出了SCRIT（Self-evolving CRITic）框架，该框架能够实现批判性评价能力的真正自我进化。从技术上讲，SCRIT通过使用参考解决方案进行逐步批判的对比自批评机制生成合成数据进行自我改进，并通过校正结果确保批判性评价的质量。使用Qwen2.5-72B-Instruct之一，这是最强大的LLM之一，SCRIT在批判性评价和错误识别基准测试中取得了高达10.3%的改进。我们的分析显示，SCRIT的表现随着数据和模型规模的增加而正向扩展，优于其他替代方法，并且其自我验证组件对其有关键性影响。 

---
# Zero-shot Shark Tracking and Biometrics from Aerial Imagery 

**Title (ZH)**: 零样本鲨鱼跟踪与遥感图像中的生物识别 

**Authors**: Chinmay K Lalgudi, Mark E Leone, Jaden V Clark, Sergio Madrigal-Mora, Mario Espinoza  

**Link**: [PDF](https://arxiv.org/pdf/2501.05717)  

**Abstract**: The recent widespread adoption of drones for studying marine animals provides opportunities for deriving biological information from aerial imagery. The large scale of imagery data acquired from drones is well suited for machine learning (ML) analysis. Development of ML models for analyzing marine animal aerial imagery has followed the classical paradigm of training, testing, and deploying a new model for each dataset, requiring significant time, human effort, and ML expertise. We introduce Frame Level ALIgment and tRacking (FLAIR), which leverages the video understanding of Segment Anything Model 2 (SAM2) and the vision-language capabilities of Contrastive Language-Image Pre-training (CLIP). FLAIR takes a drone video as input and outputs segmentation masks of the species of interest across the video. Notably, FLAIR leverages a zero-shot approach, eliminating the need for labeled data, training a new model, or fine-tuning an existing model to generalize to other species. With a dataset of 18,000 drone images of Pacific nurse sharks, we trained state-of-the-art object detection models to compare against FLAIR. We show that FLAIR massively outperforms these object detectors and performs competitively against two human-in-the-loop methods for prompting SAM2, achieving a Dice score of 0.81. FLAIR readily generalizes to other shark species without additional human effort and can be combined with novel heuristics to automatically extract relevant information including length and tailbeat frequency. FLAIR has significant potential to accelerate aerial imagery analysis workflows, requiring markedly less human effort and expertise than traditional machine learning workflows, while achieving superior accuracy. By reducing the effort required for aerial imagery analysis, FLAIR allows scientists to spend more time interpreting results and deriving insights about marine ecosystems. 

**Abstract (ZH)**: 最近，无人机在研究海洋生物中的广泛应用为从航空影像中提取生物信息提供了机会。无人机获取的大规模影像数据非常适合机器学习（ML）分析。对于海洋生物航空影像的ML模型开发通常遵循经典范式：为每个数据集训练、测试并部署新模型，这需要大量的时间、人力和ML专业知识。我们提出了一种名为Frame Level ALIgment and tRacking (FLAIR)的方法，该方法利用了Segment Anything Model 2 (SAM2)的视频理解能力以及Contrastive Language-Image Pre-training (CLIP)的视觉-语言能力。FLAIR接受无人机视频作为输入，并输出目标物种在视频中的分割掩码。值得注意的是，FLAIR采用了一种零样本方法，无需标注数据、训练新模型或微调现有模型即可实现对其他物种的泛化。通过使用包含18,000张太平洋护士鲨无人机影像的数据集，我们训练了最先进的物体检测模型以与FLAIR进行比较。结果显示，FLAIR在多个方面显著优于这些物体检测器，并且在对SAM2进行两种半监督方法的提示时表现出竞争力，取得了0.81的Dice分数。FLAIR无需额外的人力努力即可泛化到其他鲨鱼物种，并且可以结合新的启发式方法自动提取相关信息，包括体长和摆尾频率。FLAIR具有显著加速航空影像分析工作流程的潜力，所需的劳动力和专业知识远少于传统机器学习工作流程，同时仍能达到出色的准确性。通过减少对航空影像分析所需的人力努力，FLAIR使科学家们能够花费更多时间解释结果并对海洋生态系统进行深入理解。 

---
# How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond 

**Title (ZH)**: 如何实现人类与NLP模型的有效合作：原则、形式化方法及其扩展综述 

**Authors**: Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05714)  

**Abstract**: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）的发展，智能模型从简单的工具演化成了具备自身目标和与人类合作策略的自主代理。这一演变催生了自然语言处理（NLP）中一种新的范式——人-模型协作，近年来在众多NLP任务中取得了显著的进步。在本文中，我们首次系统地回顾了人-模型协作，探讨其原则、形式化表达以及面临的开放性挑战。特别地，我们提出了一种新的分类框架，提供了一个统一的视角来总结现有方法。此外，我们还讨论了潜在的前沿领域及其相应的挑战。我们认为我们的工作可以作为这一领域的入门点，为未来在此方向上的突破性研究开辟道路。 

---
# Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains 

**Title (ZH)**: 多智能体微调：通过多样化推理链进行自我提升 

**Authors**: Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch  

**Link**: [PDF](https://arxiv.org/pdf/2501.05707)  

**Abstract**: Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在多个任务上取得了显著的性能，但从根本上受限于训练数据。为了超越训练数据的限制，最近的研究探索了如何通过生成合成数据来使LLMs实现自主自改进。然而，连续的自改进步骤可能会达到边际效益递减的阶段。在本工作中，我们提出了一种补充性的自改进方法，其中微调被应用于一组语言模型组成的多智能体社会。一组语言模型，均基于同一个基础模型，并独立通过模型间的多智能体相互作用生成的数据进行更新和专门化。通过使每个模型在独立的数据集上进行训练，我们展示了这种方法如何在模型间实现专门化并多样化模型集合。因此，我们的整体系统能够在比单智能体自改进方法更多的微调轮次中实现自主改进。我们通过对广泛的推理任务进行了定量分析，展示了该方法的有效性。 

---
# EXION: Exploiting Inter- and Intra-Iteration Output Sparsity for Diffusion Models 

**Title (ZH)**: EXION：利用迭代间和迭代内输出稀疏性增强扩散模型 

**Authors**: Jaehoon Heo, Adiwena Putra, Jieon Yoon, Sungwoong Yune, Hangyeol Lee, Ji-Hoon Kim, Joo-Young Kim  

**Link**: [PDF](https://arxiv.org/pdf/2501.05680)  

**Abstract**: Over the past few years, diffusion models have emerged as novel AI solutions, generating diverse multi-modal outputs from text prompts. Despite their capabilities, they face challenges in computing, such as excessive latency and energy consumption due to their iterative architecture. Although prior works specialized in transformer acceleration can be applied, the iterative nature of diffusion models remains unresolved. In this paper, we present EXION, the first SW-HW co-designed diffusion accelerator that solves the computation challenges by exploiting the unique inter- and intra-iteration output sparsity in diffusion models. To this end, we propose two SW-level optimizations. First, we introduce the FFN-Reuse algorithm that identifies and skips redundant computations in FFN layers across different iterations (inter-iteration sparsity). Second, we use a modified eager prediction method that employs two-step leading-one detection to accurately predict the attention score, skipping unnecessary computations within an iteration (intra-iteration sparsity). We also introduce a novel data compaction mechanism named ConMerge, which can enhance HW utilization by condensing and merging sparse matrices into compact forms. Finally, it has a dedicated HW architecture that supports the above sparsity-inducing algorithms, translating high output sparsity into improved energy efficiency and performance. To verify the feasibility of the EXION, we first demonstrate that it has no impact on accuracy in various types of multi-modal diffusion models. We then instantiate EXION in both server- and edge-level settings and compare its performance against GPUs with similar specifications. Our evaluation shows that EXION achieves dramatic improvements in performance and energy efficiency by 3.2-379.3x and 45.1-3067.6x compared to a server GPU and by 42.6-1090.9x and 196.9-4668.2x compared to an edge GPU. 

**Abstract (ZH)**: 近年来，扩散模型作为一种新颖的AI解决方案，能够从文本提示生成多模态的多样输出。尽管扩散模型具有诸多能力，但在计算方面仍面临挑战，如由于其迭代架构导致的过长延迟和高能耗问题。尽管之前的工作在加速Transformer方面取得了进展，但扩散模型的迭代本质问题仍未解决。本文提出EXION，这是首个通过利用扩散模型中独特的跨迭代和内迭代输出稀疏性来解决计算挑战的软硬件协同设计的扩散模型加速器。为此，我们提出了两种软件层面的优化措施。首先，我们引入了FFN-Reuse算法，该算法识别并跳过不同迭代间（跨迭代稀疏性）FFN层中的冗余计算。其次，我们使用了一种修改后的急切预测方法，通过两步首一检测精确预测注意力分数，从而在单个迭代内跳过不必要的计算（内迭代稀疏性）。此外，我们还提出了一种新颖的数据压缩机制ConMerge，该机制可以通过压缩和合并稀疏矩阵来增强硬件利用率。最后，该加速器配备了一种专门的硬件架构，支持上述稀疏性生成算法，从而将高输出稀疏性转化为提高能源效率和性能。为了验证EXION的可行性，我们首先证明了它在各种类型的多模态扩散模型中不会影响准确性。然后，我们在服务器级和边缘级两个层级进行了实例化，并将其性能与具有类似规格的GPU进行了对比。我们的评估结果显示，与服务器GPU相比，EXION在性能和能源效率方面分别提高了3.2至379.3倍和45.1至3067.6倍；与边缘GPU相比，EXION在性能和能源效率方面分别提高了42.6至1090.9倍和196.9至4668.2倍。 

---
# Network Diffuser for Placing-Scheduling Service Function Chains with Inverse Demonstration 

**Title (ZH)**: 基于逆示例的网络扩散器用于放置-调度服务函数链 

**Authors**: Zuyuan Zhang, Vaneet Aggarwal, Tian Lan  

**Link**: [PDF](https://arxiv.org/pdf/2501.05673)  

**Abstract**: Network services are increasingly managed by considering chained-up virtual network functions and relevant traffic flows, known as the Service Function Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion, we must consider two closely-coupled problems - an SFC placement problem that maps SFCs to servers/links in the network and an SFC scheduling problem that determines when each SFC is executed. Solving the whole SFC problem targeting these two optimizations jointly is extremely challenging. In this paper, we propose a novel network diffuser using conditional generative modeling for this SFC placing-scheduling optimization. Recent advances in generative AI and diffusion models have made it possible to generate high-quality images/videos and decision trajectories from language description. We formulate the SFC optimization as a problem of generating a state sequence for planning and perform graph diffusion on the state trajectories to enable extraction of SFC decisions, with SFC optimization constraints and objectives as conditions. To address the lack of demonstration data due to NP-hardness and exponential problem space of the SFC optimization, we also propose a novel and somewhat maverick approach -- Rather than solving instances of this difficult optimization, we start with randomly-generated solutions as input, and then determine appropriate SFC optimization problems that render these solutions feasible. This inverse demonstration enables us to obtain sufficient expert demonstrations, i.e., problem-solution pairs, through further optimization. In our numerical evaluations, the proposed network diffuser outperforms learning and heuristic baselines, by $\sim$20\% improvement in SFC reward and $\sim$50\% reduction in SFC waiting time and blocking rate. 

**Abstract (ZH)**: 网络服务越来越通过考虑由虚拟网络功能链及其相关流量组成的链路，即服务功能链（Service Function Chains, SFCs），来进行管理。为了以在线方式应对SFC的顺序到达，我们必须同时考虑两个紧密相关的问题：SFC放置问题，即将SFC映射到网络中的服务器/链路；以及SFC调度问题，决定何时执行每个SFC。同时优化这两个问题的SFC整体问题具有极大的挑战性。本文提出了一种新颖的网络扩散器，结合条件生成建模来进行SFC放置和调度优化。在生成对抗网络和扩散模型的最新进展的帮助下，现在可以从语言描述生成高质量的图像/视频和决策轨迹。我们将SFC优化问题表述为生成规划状态序列的问题，并通过在网络状态下执行图扩散，以提取SFC决策，同时将SFC优化的约束和目标作为条件。由于SFC优化问题的NP难和指数级问题空间导致的演示数据不足，我们还提出了一种新颖且创新的方法——我们不是解决这个复杂优化的问题实例，而是从随机生成的解决方案作为输入开始，然后确定适当的SFC优化问题，使得解决方案可行。这种逆向示范使我们能够通过进一步优化获得足够的专家演示，即问题-解决方案对。在我们的数值评估中，所提出的网络扩散器在SFC奖励上的性能优于学习和启发式基线，提高了约20%，同时减少了约50%的SFC等待时间和阻塞率。 

---
# TransPlace: Transferable Circuit Global Placement via Graph Neural Network 

**Title (ZH)**: TransPlace：通过图神经网络实现可移植的电路全局布线 

**Authors**: Yunbo Hou, Haoran Ye, Yingxue Zhang, Siyuan Xu, Guojie Song  

**Link**: [PDF](https://arxiv.org/pdf/2501.05667)  

**Abstract**: Global placement, a critical step in designing the physical layout of computer chips, is essential to optimize chip performance. Prior global placement methods optimize each circuit design individually from scratch. Their neglect of transferable knowledge limits solution efficiency and chip performance as circuit complexity drastically increases. This study presents TransPlace, a global placement framework that learns to place millions of mixed-size cells in continuous space. TransPlace introduces i) Netlist Graph to efficiently model netlist topology, ii) Cell-flow and relative position encoding to learn SE(2)-invariant representation, iii) a tailored graph neural network architecture for informed parameterization of placement knowledge, and iv) a two-stage strategy for coarse-to-fine placement. Compared to state-of-the-art placement methods, TransPlace-trained on a few high-quality placements-can place unseen circuits with 1.2x speedup while reducing congestion by 30%, timing by 9%, and wirelength by 5%. 

**Abstract (ZH)**: 全球布局是芯片物理布局设计中至关重要的一环，对于优化芯片性能至关重要。以往的全球布局方法会单独从头优化每个电路设计，忽视可转移的知识限制了解决方案的效率和芯片性能，随着电路复杂度急剧增加，这一问题尤为突出。本研究提出了一种名为TransPlace的全球布局框架，用于高效地在连续空间中排列上百万个不同尺寸的单元。TransPlace引入了以下几点创新：i) 网表图，用于高效建模网表拓扑结构；ii) 单元流和相对位置编码，以学习SE(2)不变的表示；iii) 一种针对布局知识进行有指导参数化的自定义图神经网络架构；iv) 一种从粗到细的两阶段策略。与最先进的布局方法相比，TransPlace在少量高质量布局数据训练后，可将未见过的电路的布局速度提高1.2倍，同时减少拥塞30%，延时9%，布线长度5%。 

---
# Learning to Measure Quantum Neural Networks 

**Title (ZH)**: 学习度量量子神经网络 

**Authors**: Samuel Yen-Chi Chen, Huan-Hsin Tseng, Hsin-Yi Lin, Shinjae Yoo  

**Link**: [PDF](https://arxiv.org/pdf/2501.05663)  

**Abstract**: The rapid progress in quantum computing (QC) and machine learning (ML) has attracted growing attention, prompting extensive research into quantum machine learning (QML) algorithms to solve diverse and complex problems. Designing high-performance QML models demands expert-level proficiency, which remains a significant obstacle to the broader adoption of QML. A few major hurdles include crafting effective data encoding techniques and parameterized quantum circuits, both of which are crucial to the performance of QML models. Additionally, the measurement phase is frequently overlooked-most current QML models rely on pre-defined measurement protocols that often fail to account for the specific problem being addressed. We introduce a novel approach that makes the observable of the quantum system-specifically, the Hermitian matrix-learnable. Our method features an end-to-end differentiable learning framework, where the parameterized observable is trained alongside the ordinary quantum circuit parameters simultaneously. Using numerical simulations, we show that the proposed method can identify observables for variational quantum circuits that lead to improved outcomes, such as higher classification accuracy, thereby boosting the overall performance of QML models. 

**Abstract (ZH)**: 量子计算（QC）和机器学习（ML）的快速发展引起了广泛关注，这促使研究人员投入大量精力研究量子机器学习（QML）算法，以解决多样而复杂的问题。设计高性能的QML模型需要专家级别的专业技能，这仍然是QML更广泛应用的主要障碍之一。几个主要障碍包括有效数据编码技术和参数化量子电路的设计，这两者对QML模型的性能至关重要。另外，测量阶段经常被忽视——目前大多数QML模型依赖于预定义的测量协议，这些协议往往未能考虑到具体要解决的问题。我们提出了一种新颖的方法，使量子系统的可观测量，即赫米特矩阵，具有可学习性。该方法采用了端到端可微学习框架，其中参数化的可观测量与普通量子电路参数同时进行训练。通过数值模拟，我们展示了该方法能够识别出能够提高结果表现（例如，提高分类准确性）的可观测量，从而提升了QML模型的整体性能。 

---
# Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models 

**Title (ZH)**: 级联自我评估增强训练以实现高效的多模态大型语言模型 

**Authors**: Zheqi Lv, Wenkai Wang, Jiawei Wang, Shengyu Zhang, Fei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05662)  

**Abstract**: Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self-evaluation has improved their performance. However, limited parameters often hinder EMLLMs from effectively using self-evaluation during inference. Key challenges include synthesizing evaluation data, determining its quantity, optimizing training and inference strategies, and selecting appropriate prompts.
To address these issues, we introduce Self-Evaluation Augmented Training (SEAT). SEAT uses more powerful EMLLMs for CoT reasoning, data selection, and evaluation generation, then trains EMLLMs with the synthesized data. However, handling long prompts and maintaining CoT reasoning quality are problematic. Therefore, we propose Cascaded Self-Evaluation Augmented Training (Cas-SEAT), which breaks down lengthy prompts into shorter, task-specific cascaded prompts and reduces costs for resource-limited settings. During data synthesis, we employ open-source 7B-parameter EMLLMs and annotate a small dataset with short prompts.
Experiments demonstrate that Cas-SEAT significantly boosts EMLLMs' self-evaluation abilities, improving performance by 19.68%, 55.57%, and 46.79% on the MathVista, Math-V, and We-Math datasets, respectively. Additionally, our Cas-SEAT Dataset serves as a valuable resource for future research in enhancing EMLLM self-evaluation. 

**Abstract (ZH)**: 高效的多模态大型语言模型（EMLLMs）最近取得了迅速的进步。结合链式推理（Chain-of-Thought, CoT）和逐步自我评估的引入提高了它们的性能。然而，有限的参数常常阻碍EMLLMs在推理过程中有效利用自我评估。关键挑战包括合成评估数据、确定数据的数量、优化训练与推理策略以及选择合适的提示词。

为了应对这些问题，我们提出了自我评估增强训练（SEAT）。SEAT 使用更强大的EMLLMs进行链式推理、数据选择和评估生成，然后用合成数据对EMLLMs进行训练。然而，处理长提示词并保持链式推理质量仍然是一个问题。因此，我们提出了级联自我评估增强训练（Cas-SEAT），将长提示词分解为更短的任务特定级联提示词，从而降低资源受限环境下成本。在数据合成过程中，我们使用开源的7B参数EMLLMs，并用短提示词标注一个小数据集。

实验结果表明，Cas-SEAT 显著增强了 EMLLMs 的自我评估能力，在 MathVista、Math-V 和 We-Math 数据集上分别提高了19.68%、55.57%和46.79%的性能。此外，我们的Cas-SEAT数据集将成为未来研究提高EMLLM自我评估能力的宝贵资源。 

---
# Collaboration of Large Language Models and Small Recommendation Models for Device-Cloud Recommendation 

**Title (ZH)**: 大型语言模型与小型推荐模型的设备-云推荐协作 

**Authors**: Zheqi Lv, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05647)  

**Abstract**: Large Language Models (LLMs) for Recommendation (LLM4Rec) is a promising research direction that has demonstrated exceptional performance in this field. However, its inability to capture real-time user preferences greatly limits the practical application of LLM4Rec because (i) LLMs are costly to train and infer frequently, and (ii) LLMs struggle to access real-time data (its large number of parameters poses an obstacle to deployment on devices). Fortunately, small recommendation models (SRMs) can effectively supplement these shortcomings of LLM4Rec diagrams by consuming minimal resources for frequent training and inference, and by conveniently accessing real-time data on devices.
In light of this, we designed the Device-Cloud LLM-SRM Collaborative Recommendation Framework (LSC4Rec) under a device-cloud collaboration setting. LSC4Rec aims to integrate the advantages of both LLMs and SRMs, as well as the benefits of cloud and edge computing, achieving a complementary synergy. We enhance the practicability of LSC4Rec by designing three strategies: collaborative training, collaborative inference, and intelligent request. During training, LLM generates candidate lists to enhance the ranking ability of SRM in collaborative scenarios and enables SRM to update adaptively to capture real-time user interests. During inference, LLM and SRM are deployed on the cloud and on the device, respectively. LLM generates candidate lists and initial ranking results based on user behavior, and SRM get reranking results based on the candidate list, with final results integrating both LLM's and SRM's scores. The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists. Our comprehensive and extensive experimental analysis validates the effectiveness of each strategy in LSC4Rec. 

**Abstract (ZH)**: 大型语言模型（LLMs）在推荐系统中的应用（LLM4Rec）是一个充满前景的研究方向，已经在该领域展示了出色的表现。然而，其在捕捉实时用户偏好的能力方面存在局限，极大地限制了LLM4Rec的实际应用。具体来说，（i）训练和推理LLMs成本高昂，而且（ii）LLMs难以访问实时数据（其庞大的参数数量成为设备部署的障碍）。幸运的是，小型推荐模型（SRMs）能够有效地补充LLM4Rec的这些不足之处，通过消耗少量资源进行频繁的训练和推理，并方便地在设备上访问实时数据。

鉴于此，我们设计了一个基于设备-云协作的LLM-SRM联合推荐框架（LSC4Rec）。LSC4Rec旨在整合LLMs和SRMs的优势，以及边缘计算和云计算的益处，实现互补协同。为了提高LSC4Rec的实用性，我们设计了三种策略：协作训练、协作推理和智能请求。在训练期间，LLM生成候选列表以增强SRM在协作场景中的排名能力，并使SRM能够适时更新以捕捉实时用户兴趣。在推理期间，LLM和SRM分别部署在云端和设备上。LLM基于用户行为生成候选列表和初始排名结果，SRM根据候选列表获取重新排名结果，最终结果结合了LLM和SRM的评分。设备通过比较LLM和SRM排序列表的一致性来决定是否需要新的候选列表。我们全面而广泛的实验分析验证了LSC4Rec中每种策略的有效性。 

---
# Efficient Representations for High-Cardinality Categorical Variables in Machine Learning 

**Title (ZH)**: 高基数类别变量在机器学习中的高效表示方法 

**Authors**: Zixuan Liang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05646)  

**Abstract**: High\-cardinality categorical variables pose significant challenges in machine learning, particularly in terms of computational efficiency and model interpretability. Traditional one\-hot encoding often results in high\-dimensional sparse feature spaces, increasing the risk of overfitting and reducing scalability. This paper introduces novel encoding techniques, including means encoding, low\-rank encoding, and multinomial logistic regression encoding, to address these challenges. These methods leverage sufficient representations to generate compact and informative embeddings of categorical data. We conduct rigorous theoretical analyses and empirical validations on diverse datasets, demonstrating significant improvements in model performance and computational efficiency compared to baseline methods. The proposed techniques are particularly effective in domains requiring scalable solutions for large datasets, paving the way for more robust and efficient applications in machine learning. 

**Abstract (ZH)**: 高基数定性变量在机器学习中提出了显著挑战，特别是在计算效率和模型可解释性方面。传统的独热编码往往会产生高维度的稀疏特征空间，增加过拟合的风险并降低可扩展性。本文引入了新的编码技术，包括均值编码、低秩编码和多项式逻辑回归编码，以应对这些挑战。这些方法利用充分表示生成紧凑且信息丰富的定性数据嵌入。我们在多种数据集上进行了严格的理论分析和实验验证，结果显示与基准方法相比，在模型性能和计算效率上均有显著改进。所提出的技术特别适用于需要处理大规模数据集的领域，为机器学习中的更稳健和高效应用铺平了道路。 

---
# Iconicity in Large Language Models 

**Title (ZH)**: 大型语言模型中的iconicity 

**Authors**: Anna Marklová, Jiří Milička, Leonid Ryvkin, Ľudmila Lacková Bennet, Libuše Kormaníková  

**Link**: [PDF](https://arxiv.org/pdf/2501.05643)  

**Abstract**: Lexical iconicity, a direct relation between a word's meaning and its form, is an important aspect of every natural language, most commonly manifesting through sound-meaning associations. Since Large language models' (LLMs') access to both meaning and sound of text is only mediated (meaning through textual context, sound through written representation, further complicated by tokenization), we might expect that the encoding of iconicity in LLMs would be either insufficient or significantly different from human processing. This study addresses this hypothesis by having GPT-4 generate highly iconic pseudowords in artificial languages. To verify that these words actually carry iconicity, we had their meanings guessed by Czech and German participants (n=672) and subsequently by LLM-based participants (generated by GPT-4 and Claude 3.5 Sonnet). The results revealed that humans can guess the meanings of pseudowords in the generated iconic language more accurately than words in distant natural languages and that LLM-based participants are even more successful than humans in this task. This core finding is accompanied by several additional analyses concerning the universality of the generated language and the cues that both human and LLM-based participants utilize. 

**Abstract (ZH)**: 词汇隐喻性，即单词的意义与其形式之间直接的关系，是每种自然语言的重要特征，通常通过音节意义关联来体现。由于大型语言模型（LLMs）对文本意义和声音的获取是经过中介的（意义通过文本上下文，声音通过书写表示，进一步复杂化了分词过程），我们可能会预期LLMs中隐喻性的编码可能是不足的或与人类处理方式显著不同。本研究通过让GPT-4生成高度隐喻性的假词来构建人工语言来验证这一假设。为了验证这些词语是否真正携带隐喻性，我们邀请了捷克和德国参与者（共672人）猜测这些假词的意义，并随后让基于LLM的参与者（由GPT-4和Claude 3.5 Sonnet生成）进行猜测。结果显示，人类能够更准确地猜测生成的人工隐喻性语言中假词的意义，而基于LLM的参与者在这项任务中的表现甚至比人类更出色。这一核心发现还伴随着对生成语言的普遍性和人类与基于LLM的参与者所利用的线索的多项额外分析。 

---
# The Impact of Model Scaling on Seen and Unseen Language Performance 

**Title (ZH)**: 模型规模对已见和未见语言性能的影响 

**Authors**: Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh  

**Link**: [PDF](https://arxiv.org/pdf/2501.05629)  

**Abstract**: The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的迅速发展，尤其是那些基于多语言语料库训练的模型，加强了对其在各种语言和模型规模下的性能进行更深入理解的迫切需求。我们的研究通过在204种语言上研究多语言LLMs在文本分类和机器翻译任务中的性能和扩展行为，来满足这一关键需求。我们系统地分析了三种不同规模模型家族在零样本和少数样本设置下已知语言和未知语言的性能和扩展行为。研究发现，在零样本和两样本场景下的扩展行为存在显著差异，并且已知语言和未知语言之间在性能上的差异尤为突出。模型规模对零样本性能影响不大，这一表现基本保持平稳。然而，在两样本设置中，较大的模型在多语言文本分类任务中表现出明显线性改进。对于翻译任务而言，只有指令调优模型表现出明显的扩展优势。我们的分析还表明，整体资源水平而非仅仅是预训练语言的比例对模型性能的预测更为重要，这揭示了多语言LLM有效性的驱动因素。 

---
# Watermarking Graph Neural Networks via Explanations for Ownership Protection 

**Title (ZH)**: 通过解释实现图神经网络的水印化以保护所有权 

**Authors**: Jane Downer, Ren Wang, Binghui Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05614)  

**Abstract**: Graph Neural Networks (GNNs) are the mainstream method to learn pervasive graph data and are widely deployed in industry, making their intellectual property valuable. However, protecting GNNs from unauthorized use remains a challenge. Watermarking, which embeds ownership information into a model, is a potential solution. However, existing watermarking methods have two key limitations: First, almost all of them focus on non-graph data, with watermarking GNNs for complex graph data largely unexplored. Second, the de facto backdoor-based watermarking methods pollute training data and induce ownership ambiguity through intentional misclassification. Our explanation-based watermarking inherits the strengths of backdoor-based methods (e.g., robust to watermark removal attacks), but avoids data pollution and eliminates intentional misclassification. In particular, our method learns to embed the watermark in GNN explanations such that this unique watermark is statistically distinct from other potential solutions, and ownership claims must show statistical significance to be verified. We theoretically prove that, even with full knowledge of our method, locating the watermark is an NP-hard problem. Empirically, our method manifests robustness to removal attacks like fine-tuning and pruning. By addressing these challenges, our approach marks a significant advancement in protecting GNN intellectual property. 

**Abstract (ZH)**: 图神经网络（GNNs）是处理广泛图形数据的主要方法，并且在工业中广泛应用，使其知识产权具有重要价值。然而，保护GNNs不被未经授权使用仍然是一个挑战。水印技术，即在模型中嵌入所有权信息，是一种潜在的解决方案。然而，现有的水印方法存在两个关键局限：首先，几乎所有方法都侧重于非图形数据，而对于复杂图形数据的GNN水印化研究较少。其次，实际上基于后门的水印方法会污染训练数据，并通过故意误分类引起所有权模糊。我们基于解释的水印继承了基于后门方法的优点（例如，对水印去除攻击具有鲁棒性），但避免了数据污染，并消除了故意误分类。特别是，我们的方法学习将水印嵌入到GNN解释中，使得这种独特的水印在统计上与其他潜在解决方案明显不同，且所有权声明必须显示统计显著性才能被验证。我们从理论上证明，即使完全了解我们的方法，定位水印也是一个NP难问题。在实验上，我们的方法对微调和剪枝等去除攻击表现出鲁棒性。通过解决这些挑战，我们的方法标志着在保护GNN知识产权方面取得了显著进展。 

---
# Advancing Personalized Learning Analysis via an Innovative Domain Knowledge Informed Attention-based Knowledge Tracing Method 

**Title (ZH)**: 通过一种创新的领域知识引导的注意力追踪知识追踪方法推动个性化学习分析发展 

**Authors**: Shubham Kose, Jin Wei-Kocsis  

**Link**: [PDF](https://arxiv.org/pdf/2501.05605)  

**Abstract**: Emerging Knowledge Tracing (KT) models, particularly deep learning and attention-based Knowledge Tracing, have shown great potential in realizing personalized learning analysis via prediction of students' future performance based on their past interactions. The existing methods mainly focus on immediate past interactions or individual concepts without accounting for dependencies between knowledge concept, referred as knowledge concept routes, that can be critical to advance the understanding the students' learning outcomes. To address this, in this paper, we propose an innovative attention-based method by effectively incorporating the domain knowledge of knowledge concept routes in the given curriculum. Additionally, we leverage XES3G5M dataset, a benchmark dataset with rich auxiliary information for knowledge concept routes, to evaluate and compare the performance of our proposed method to the seven State-of-the-art (SOTA) deep learning models. 

**Abstract (ZH)**: 新兴的知识追踪（KT）模型，尤其是深度学习和基于注意力的知识追踪，已经显示出通过预测学生未来表现来实现个性化学习分析的巨大潜力。现有的方法主要关注学生的即时过往交互或单个概念，而忽略了知识概念依赖性的考虑，这些依赖性被称为知识概念路径，对于理解学生的学习成果至关重要。为了解决这一问题，本文提出了一种创新的基于注意力的方法，通过有效结合给定课程中的知识概念路径领域的专业知识。此外，我们利用XES3G5M数据集，这是一个包含丰富辅助信息的基准数据集，用于知识概念路径，来评估和比较我们提出的方法与目前最先进的七种深度学习模型的性能。 

---
# Approximate Supervised Object Distance Estimation on Unmanned Surface Vehicles 

**Title (ZH)**: 无人驾驶水面车辆上的近似监督物体距离估计 

**Authors**: Benjamin Kiefer, Yitong Quan, Andreas Zell  

**Link**: [PDF](https://arxiv.org/pdf/2501.05567)  

**Abstract**: Unmanned surface vehicles (USVs) and boats are increasingly important in maritime operations, yet their deployment is limited due to costly sensors and complexity. LiDAR, radar, and depth cameras are either costly, yield sparse point clouds or are noisy, and require extensive calibration. Here, we introduce a novel approach for approximate distance estimation in USVs using supervised object detection. We collected a dataset comprising images with manually annotated bounding boxes and corresponding distance measurements. Leveraging this data, we propose a specialized branch of an object detection model, not only to detect objects but also to predict their distances from the USV. This method offers a cost-efficient and intuitive alternative to conventional distance measurement techniques, aligning more closely with human estimation capabilities. We demonstrate its application in a marine assistance system that alerts operators to nearby objects such as boats, buoys, or other waterborne hazards. 

**Abstract (ZH)**: 无人水面车辆（USVs）和船只在海上操作中越来越重要，但其部署受到昂贵传感器和复杂性的限制。激光雷达（LiDAR）、雷达和深度相机要么成本高昂，要么生成稀疏的点云，要么噪声大，且需要大量的校准。在这里，我们介绍了一种新的方法，用于USVs中的近似距离估计，该方法基于监督的物体检测。我们收集了一个包含手动标注边界框和相应距离测量值的数据集。借助这些数据，我们提出了一种专门的物体检测模型分支，不仅用于检测物体，还能够预测这些物体与USV之间的距离。该方法提供了一种成本效益高且直观的替代传统距离测量技术的选择，与人类的估算能力更为接近。我们展示了其在海上辅助系统中的应用，该系统能够向操作员警报附近物体，如船只、浮标或其他水上危险。 

---
# Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding 

**Title (ZH)**: 基于CLIP的自监督动态场景理解：面向自动驾驶的多模态模型 

**Authors**: Mohammed Elhenawy, Huthaifa I. Ashqar, Andry Rakotonirainy, Taqwa I. Alhadidi, Ahmed Jaber, Mohammad Abu Tami  

**Link**: [PDF](https://arxiv.org/pdf/2501.05566)  

**Abstract**: Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language-Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge devices. The proposed system outperforms state-of-the-art in-context learning methods, including the zero-shot capabilities of GPT-4o, particularly in complex scenarios. By conducting frame-level analysis on the Honda Scenes Dataset, which contains a collection of about 80 hours of annotated driving videos capturing diverse real-world road and weather conditions, our study highlights the robustness of CLIP models in learning visual concepts from natural language supervision. Results also showed that fine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly improved scene classification, achieving a top F1 score of 91.1%. These results demonstrate the ability of the system to deliver rapid and precise scene recognition, which can be used to meet the critical requirements of Advanced Driver Assistance Systems (ADAS). This study shows the potential of CLIP models to provide scalable and efficient frameworks for dynamic scene understanding and classification. Furthermore, this work lays the groundwork for advanced autonomous vehicle technologies by fostering a deeper understanding of driver behavior, road conditions, and safety-critical scenarios, marking a significant step toward smarter, safer, and more context-aware autonomous driving systems. 

**Abstract (ZH)**: 场景理解对于提高驾驶员安全性、生成以人为中心的自动车辆（AV）决策解释以及利用人工智能（AI）进行回顾性驾驶视频分析至关重要。本研究开发了一种使用对比语言-图像预训练（CLIP）模型的动力场景检索系统，该系统可以优化以适应边缘设备上的实时部署。所提出系统在包含约80小时标注驾驶视频（涵盖了多样化的实际道路和天气条件）的Honda Scenes数据集的帧级分析中，表现出色，特别是在复杂场景中超过了最先进的上下文学习方法，特别是GPT-4o的零样本能力。通过Honda Scenes数据集的帧级分析，本研究强调了CLIP模型在自然语言监督下学习视觉概念的稳健性。结果显示，对CLIP模型（如ViT-L/14和ViT-B/32）进行微调显著提高了场景分类性能，实现了最高的F1分数91.1%。这些结果证明了该系统能够快速准确地识别场景，可以满足高级驾驶辅助系统（ADAS）的关键要求。本研究展示了CLIP模型在动态场景理解和分类方面提供可扩展和高效框架的潜力。此外，本研究为进一步自主车辆技术的发展奠定了基础，促进了对驾驶员行为、道路条件和安全关键场景的深入理解，为更智能、更安全且更情境感知的自主驾驶系统迈出了重要一步。 

---
# Soup to go: mitigating forgetting during continual learning with model averaging 

**Title (ZH)**: 流质到即食：通过模型平均缓解连续学习中的遗忘问题 

**Authors**: Anat Kleiman, Gintare Karolina Dziugaite, Jonathan Frankle, Sham Kakade, Mansheej Paul  

**Link**: [PDF](https://arxiv.org/pdf/2501.05559)  

**Abstract**: In continual learning, where task data arrives in a sequence, fine-tuning on later tasks will often lead to performance degradation on earlier tasks. This is especially pronounced when these tasks come from diverse domains. In this setting, how can we mitigate catastrophic forgetting of earlier tasks and retain what the model has learned with minimal computational expenses? Inspired by other merging methods, and L2-regression, we propose Sequential Fine-tuning with Averaging (SFA), a method that merges currently training models with earlier checkpoints during the course of training. SOTA approaches typically maintain a data buffer of past tasks or impose a penalty at each gradient step. In contrast, our method achieves comparable results without the need to store past data, or multiple copies of parameters for each gradient step. Furthermore, our method outperforms common merging techniques such as Task Arithmetic, TIES Merging, and WiSE-FT, as well as other penalty methods like L2 and Elastic Weight Consolidation. In turn, our method offers insight into the benefits of merging partially-trained models during training across both image and language domains. 

**Abstract (ZH)**: 在持续学习中，当任务数据按序列到达时，对后期任务进行微调往往会导致前期任务性能下降。当这些任务来自不同的领域时，这种现象尤为明显。在这种情况下，我们如何在最小化计算开销的同时减轻前期任务的灾难性遗忘并保持模型已学到的知识？受到其他合并方法和L2正则化方法的启发，我们提出了一种名为顺序微调与平均（SFA）的方法，在训练过程中将正在训练的模型与早期的检查点合并。当前最先进的方法通常保留过去任务的数据缓冲区，或在每次梯度更新时施加惩罚。相比之下，我们的方法能够在不需要存储过去数据或在每次梯度更新时保留多个参数副本的情况下取得类似的效果。此外，我们的方法在任务算术、TIES合并、WiSE-FT以及L2正则化和弹性权重巩固等常见合并技术中表现更优。我们的方法还为我们提供了关于在图像和语言领域在训练过程中合并部分训练模型的益处洞见。 

---
# Improving Zero-Shot Object-Level Change Detection by Incorporating Visual Correspondence 

**Title (ZH)**: 通过引入视觉对应关系提高零 shot 对象级变化检测性能 

**Authors**: Hung Huy Nguyen, Pooyan Rahmanzadehgervi, Long Mail, Anh Totti Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2501.05555)  

**Abstract**: Detecting object-level changes between two images across possibly different views is a core task in many applications that involve visual inspection or camera surveillance. Existing change-detection approaches suffer from three major limitations: (1) lack of evaluation on image pairs that contain no changes, leading to unreported false positive rates; (2) lack of correspondences (\ie, localizing the regions before and after a change); and (3) poor zero-shot generalization across different domains. To address these issues, we introduce a novel method that leverages change correspondences (a) during training to improve change detection accuracy, and (b) at test time, to minimize false positives. That is, we harness the supervision labels of where an object is added or removed to supervise change detectors, improving their accuracy over previous work by a large margin. Our work is also the first to predict correspondences between pairs of detected changes using estimated homography and the Hungarian algorithm. Our model demonstrates superior performance over existing methods, achieving state-of-the-art results in change detection and change correspondence accuracy across both in-distribution and zero-shot benchmarks. 

**Abstract (ZH)**: 在可能不同视角的两张图像之间检测物体级别的变化是许多涉及视觉检查或摄像监控的应用中的核心任务。现有的变化检测方法存在三个主要局限：（1）缺乏对不含变化的图像对的评估，导致未报告的误报率；（2）缺少对应关系（即，在变化前后的区域定位）；（3）跨不同领域的零样本泛化能力较差。为解决这些问题，我们引入了一种新颖的方法，该方法在（a）训练过程中利用变化对应关系以提高变化检测准确性，并在（b）测试阶段利用这些对应关系以减少误报。也就是说，我们利用物体添加或移除的位置监督标签来监督变化检测器，这使我们的模型在准确性方面相比之前的工作有了显著改进。此外，我们的工作是首次通过估计霍夫曼变换和使用匈牙利算法来预测检测到的变化之间的对应关系。我们的模型在变化检测和变化对应关系准确性方面均表现出色，在既有领域和零样本基准测试中均达到了最先进的性能。 

---
# LLMQuoter: Enhancing RAG Capabilities Through Efficient Quote Extraction From Large Contexts 

**Title (ZH)**: LLMQuoter：通过高效提取大型上下文中的引用以增强RAG能力 

**Authors**: Yuri Facanha Bezerra, Li Weigang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05554)  

**Abstract**: We introduce LLMQuoter, a lightweight, distillation-based model designed to enhance Retrieval Augmented Generation (RAG) by extracting the most relevant textual evidence for downstream reasoning tasks. Built on the LLaMA-3B architecture and fine-tuned with Low-Rank Adaptation (LoRA) on a 15,000-sample subset of HotpotQA, LLMQuoter adopts a "quote-first-then-answer" strategy, efficiently identifying key quotes before passing curated snippets to reasoning models. This workflow reduces cognitive overhead and outperforms full-context approaches like Retrieval-Augmented Fine-Tuning (RAFT), achieving over 20-point accuracy gains across both small and large language models. By leveraging knowledge distillation from a high-performing teacher model, LLMQuoter achieves competitive results in a resource-efficient fine-tuning setup. It democratizes advanced RAG capabilities, delivering significant performance improvements without requiring extensive model retraining. Our results highlight the potential of distilled quote-based reasoning to streamline complex workflows, offering a scalable and practical solution for researchers and practitioners alike. 

**Abstract (ZH)**: 我们介绍了一种轻量级的蒸馏模型LLMQuoter，该模型旨在通过提取与下游推理任务最相关的文本证据来增强检索增强生成（RAG）。LLMQuoter 基于 LLaMA-3B 架构，并通过对 HotpotQA 数据集的 15,000 个样本子集进行 Low-Rank Adaptation (LoRA) 微调。LLMQuoter 采用“先引用后回答”的策略，在识别关键引用后，将精心挑选的片段传递给推理模型。这种工作流程减少了认知负担，并且在全面上下文方法（如检索增强微调 RAFT）方面表现出色，实现了从小型到大型语言模型超过 20 个百分点的准确性提升。通过利用高性能教师模型的知识蒸馏，LLMQuoter 在资源高效微调设置中取得了竞争力的结果。它使先进的 RAG 能力民主化，无需进行广泛的模型重训练即可实现显著的性能提升。我们的结果突显了蒸馏引用基础推理的潜力，可以简化复杂的流程工作，并为研究人员和实践者提供了可扩展且实用的解决方案。 

---
# The dynamics of meaning through time: Assessment of Large Language Models 

**Title (ZH)**: 意义随时间的动态演化：大型语言模型的评估 

**Authors**: Mohamed Taher Alrefaie, Fatty Salem, Nour Eldin Morsy, Nada Samir, Mohamed Medhat Gaber  

**Link**: [PDF](https://arxiv.org/pdf/2501.05552)  

**Abstract**: Understanding how large language models (LLMs) grasp the historical context of concepts and their semantic evolution is essential in advancing artificial intelligence and linguistic studies. This study aims to evaluate the capabilities of various LLMs in capturing temporal dynamics of meaning, specifically how they interpret terms across different time periods. We analyze a diverse set of terms from multiple domains, using tailored prompts and measuring responses through both objective metrics (e.g., perplexity and word count) and subjective human expert evaluations. Our comparative analysis includes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama. Findings reveal marked differences in each model's handling of historical context and semantic shifts, highlighting both strengths and limitations in temporal semantic understanding. These insights offer a foundation for refining LLMs to better address the evolving nature of language, with implications for historical text analysis, AI design, and applications in digital humanities. 

**Abstract (ZH)**: 理解大型语言模型（LLMs）如何掌握概念的历史背景及其语义演变对于促进人工智能和语言学研究至关重要。本研究旨在评估各种LLMs在捕捉意义时间动态方面的能力，特别是它们如何在不同时间段内解释术语。我们分析了来自多个领域的多样术语，通过定制的提示并采用客观指标（例如困惑度和词汇量）以及主观的人类专家评价来测量响应。我们的比较分析包括ChatGPT、GPT-4、Claude、Bard、Gemini和Llama等知名模型。研究发现，每个模型在处理历史背景和语义转变方面存在显著差异，这凸显了在时间维度上的语义理解中的优势与局限。这些洞见为改进LLMs以更好地应对语言的演变提供了基础，对历史文本分析、AI设计和数字人文领域的应用具有重要意义。 

---
# OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? 

**Title (ZH)**: OVO-Bench: 你的视频大型语言模型与现实世界在线视频理解还有多远？ 

**Authors**: Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05510)  

**Abstract**: Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at this https URL. 

**Abstract (ZH)**: 时间意识，即根据问题提出时的 timestamp 进行动态推理的能力，是离线和在线视频语言模型之间的重要区别。与依赖完整视频进行静态、事后分析的离线模型不同，在线模型能够逐帧处理视频流，并根据问题提出的时间动态调整其响应。尽管时间意识非常重要，但在现有的基准测试中却未得到充分评估。为填补这一空白，我们提出了 OVO-Bench（Online-VideO-Benchmark），这是一种新颖的视频基准测试，强调时间戳在高级在线视频理解能力评估中的重要性。OVO-Bench 通过三种不同的场景评估视频语言模型根据特定时间戳推理和响应事件的能力：（1）反向追溯：回溯到过去事件以回答问题。 （2）实时理解：理解并响应随着当前时间戳展开的事件。 （3）前瞻积极回应：直到获得足够未来信息以准确回答问题时才给出响应。OVO-Bench 包含 12 个任务，共涉及 644 个独特的视频片段和约 2,800 个人工精选的精细粒度元注释，精准标注了时间戳。我们结合了自动生成管道与人工校准。借助这些高质量的数据样本，我们进一步开发了一个评估管道，系统地沿视频时间线对视频语言模型进行查询。对九种视频语言模型的评估表明，尽管在传统基准测试上取得了进步，当前模型在线视频理解能力仍然较差，与人类代理之间存在显著差距。我们希望 OVO-Bench 能够推动视频语言模型的发展，并激发未来在线视频推理的研究。我们的基准测试和代码可通过以下链接访问：this https URL。 

---
# Spatial Information Integration in Small Language Models for Document Layout Generation and Classification 

**Title (ZH)**: 小语言模型中的空间信息集成在文档布局生成与分类中的应用 

**Authors**: Pablo Melendez, Clemens Havas  

**Link**: [PDF](https://arxiv.org/pdf/2501.05497)  

**Abstract**: Document layout understanding is a field of study that analyzes the spatial arrangement of information in a document hoping to understand its structure and layout. Models such as LayoutLM (and its subsequent iterations) can understand semi-structured documents with SotA results; however, the lack of open semi-structured data is a limitation in itself. While semi-structured data is common in everyday life (balance sheets, purchase orders, receipts), there is a lack of public datasets for training machine learning models for this type of document. In this investigation we propose a method to generate new, synthetic, layout information that can help overcoming this data shortage. According to our results, the proposed method performs better than LayoutTransformer, another popular layout generation method. We also show that, in some scenarios, text classification can improve when supported by bounding box information. 

**Abstract (ZH)**: 文档布局理解是研究领域之一，旨在分析文档中信息的空间排列，以理解和掌握其结构和布局。如LayoutLM（及其后续版本）等模型能够理解半结构化文档，并取得当前最佳成果；然而，缺乏开源的半结构化数据本身就是一个限制。尽管半结构化数据在日常生活中相当常见（如资产负债表、采购订单、收据），但用于此类型文档的机器学习模型训练的数据集却相当缺乏。本研究中，我们提出了一种生成新的合成布局信息的方法，以应对这一数据短缺问题。根据我们的结果，所提出的方法在半结构化布局生成方面优于另一种流行的布局生成方法LayoutTransformer。此外，我们还展示了在某些场景下，通过结合文本分类与边界框信息，可以进一步提高文本分类的性能。 

---
# FedSA: A Unified Representation Learning via Semantic Anchors for Prototype-based Federated Learning 

**Title (ZH)**: FedSA：基于语义锚点的统一表示学习方法用于原型驱动的联邦学习 

**Authors**: Yanbing Zhou, Xiangmou Qu, Chenlong You, Jiyang Zhou, Jingyue Tang, Xin Zheng, Chunmao Cai, Yingbo Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05496)  

**Abstract**: Prototype-based federated learning has emerged as a promising approach that shares lightweight prototypes to transfer knowledge among clients with data heterogeneity in a model-agnostic manner. However, existing methods often collect prototypes directly from local models, which inevitably introduce inconsistencies into representation learning due to the biased data distributions and differing model architectures among clients. In this paper, we identify that both statistical and model heterogeneity create a vicious cycle of representation inconsistency, classifier divergence, and skewed prototype alignment, which negatively impacts the performance of clients. To break the vicious cycle, we propose a novel framework named Federated Learning via Semantic Anchors (FedSA) to decouple the generation of prototypes from local representation learning. We introduce a novel perspective that uses simple yet effective semantic anchors serving as prototypes to guide local models in learning consistent representations. By incorporating semantic anchors, we further propose anchor-based regularization with margin-enhanced contrastive learning and anchor-based classifier calibration to correct feature extractors and calibrate classifiers across clients, achieving intra-class compactness and inter-class separability of prototypes while ensuring consistent decision boundaries. We then update the semantic anchors with these consistent and discriminative prototypes, which iteratively encourage clients to collaboratively learn a unified data representation with robust generalization. Extensive experiments under both statistical and model heterogeneity settings show that FedSA significantly outperforms existing prototype-based FL methods on various classification tasks. 

**Abstract (ZH)**: 基于原型的联邦学习作为一种在数据异质性环境下，以模型无关的方式共享轻量级原型以转移知识的有前景方法而逐渐兴起。然而，现有方法通常直接从本地模型中收集原型，这不可避免地会在基于有偏数据分布和不同模型架构的客户端之间引入表示学习的一致性问题。在本文中，我们指出统计异质性和模型异质性共同形成了表示不一致性、分类器偏差和原型偏斜之间的恶性循环，这对客户端的性能产生了负面影响。为了打破这一恶性循环，我们提出了一个名为基于语义锚的联邦学习（FedSA）的新型框架，以解耦原型生成与本地表示学习的过程。我们引入了一个新颖的视角，即使用简单而有效的语义锚作为原型来引导本地模型学习一致的表示。通过引入语义锚，我们进一步提出了基于语义锚的正则化以及带有边距增强对比学习和基于语义锚的分类器校准，以纠正特征提取器并校准分类器，从而在客户端之间实现原型的类内紧密性和类间可分性，同时确保一致的决策边界。我们随后根据这些一致且具有辨别性的原型更新语义锚，逐步促使客户端合作学习一个具有稳健泛化能力的统一数据表示。在统计异质性和模型异质性设置下的广泛实验表明，FedSA在各种分类任务上显著优于现有的基于原型的联邦学习方法。 

---
# LSEBMCL: A Latent Space Energy-Based Model for Continual Learning 

**Title (ZH)**: LSEBMCL：基于潜在空间的能量模型在持续学习中的应用 

**Authors**: Xiaodi Li, Dingcheng Li, Rujun Gao, Mahmoud Zamani, Latifur Khan  

**Link**: [PDF](https://arxiv.org/pdf/2501.05495)  

**Abstract**: Continual learning has become essential in many practical applications such as online news summaries and product classification. The primary challenge is known as catastrophic forgetting, a phenomenon where a model inadvertently discards previously learned knowledge when it is trained on new tasks. Existing solutions involve storing exemplars from previous classes, regularizing parameters during the fine-tuning process, or assigning different model parameters to each task. The proposed solution LSEBMCL (Latent Space Energy-Based Model for Continual Learning) in this work is to use energy-based models (EBMs) to prevent catastrophic forgetting by sampling data points from previous tasks when training on new ones. The EBM is a machine learning model that associates an energy value with each input data point. The proposed method uses an EBM layer as an outer-generator in the continual learning framework for NLP tasks. The study demonstrates the efficacy of EBM in NLP tasks, achieving state-of-the-art results in all experiments. 

**Abstract (ZH)**: 持续学习在许多实际应用中变得至关重要，如在线新闻摘要和产品分类。主要挑战被称为灾难性遗忘现象，即当模型在训练新任务时会无意中丢弃之前学到的知识。现有解决方案包括存储以前类别的示例、在微调过程中正则化参数或为每个任务分配不同的模型参数。本文提出的方法LSEBMCL（潜在空间能量基于模型的持续学习）采用能量基于模型（EBMs）来通过在训练新任务时从以前的任务中采样数据点来防止灾难性遗忘。EBM是一种机器学习模型，它将能量值与每个输入数据点关联。本文的方法将EBM层作为持续学习框架中自然语言处理任务的外生成器。研究结果表明EBM在自然语言处理任务中的有效性，所有实验中均达到了最先进的效果。 

---
# Interpretable deep learning illuminates multiple structures fluorescence imaging: a path toward trustworthy artificial intelligence in microscopy 

**Title (ZH)**: 可解释的深度学习揭示多重结构荧光成像：通向显微镜中可信赖的人工智能之路 

**Authors**: Mingyang Chen, Luhong Jin, Xuwei Xuan, Defu Yang, Yun Cheng, Ju Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05490)  

**Abstract**: Live-cell imaging of multiple subcellular structures is essential for understanding subcellular dynamics. However, the conventional multi-color sequential fluorescence microscopy suffers from significant imaging delays and limited number of subcellular structure separate labeling, resulting in substantial limitations for real-time live-cell research applications. Here, we present the Adaptive Explainable Multi-Structure Network (AEMS-Net), a deep-learning framework that enables simultaneous prediction of two subcellular structures from a single image. The model normalizes staining intensity and prioritizes critical image features by integrating attention mechanisms and brightness adaptation layers. Leveraging the Kolmogorov-Arnold representation theorem, our model decomposes learned features into interpretable univariate functions, enhancing the explainability of complex subcellular morphologies. We demonstrate that AEMS-Net allows real-time recording of interactions between mitochondria and microtubules, requiring only half the conventional sequential-channel imaging procedures. Notably, this approach achieves over 30% improvement in imaging quality compared to traditional deep learning methods, establishing a new paradigm for long-term, interpretable live-cell imaging that advances the ability to explore subcellular dynamics. 

**Abstract (ZH)**: 细胞内多种亚细胞结构的实时成像是理解亚细胞动态的关键。然而，传统的多色序贯荧光显微镜存在显著的成像延迟和亚细胞结构分离标记的局限性，限制了其在实时活细胞研究中的应用。为此，我们提出了一种适应性可解释多结构网络（AEMS-Net），这是一种深度学习框架，能够从单张图像中同时预测两个亚细胞结构。该模型通过结合注意力机制和亮度调整层对染色强度进行归一化并优先处理关键图像特征。利用Kolmogorov-Arnold表示定理，我们的模型将学习到的特征分解为可解释的一元函数，增强了复杂亚细胞形态的可解释性。我们证明，AEMS-Net允许实时记录线粒体和微管之间的相互作用，只需要常规序贯通道成像程序的一半步骤。值得注意的是，这种方法在成像质量上比传统深度学习方法提高了超过30%，建立了新的长期、可解释的活细胞成像范式，推动了对亚细胞动态探索的能力。 

---
# Towards an Ontology of Traceable Impact Management in the Food Supply Chain 

**Title (ZH)**: 面向食品供应链可追溯影响管理的本体研究 

**Authors**: Bart Gajderowicz, Mark S Fox, Yongchao Gao  

**Link**: [PDF](https://arxiv.org/pdf/2501.05486)  

**Abstract**: The pursuit of quality improvements and accountability in the food supply chains, especially how they relate to food-related outcomes, such as hunger, has become increasingly vital, necessitating a comprehensive approach that encompasses product quality and its impact on various stakeholders and their communities. Such an approach offers numerous benefits in increasing product quality and eliminating superfluous measurements while appraising and alleviating the broader societal and environmental repercussions. A traceable impact management model (TIMM) provides an impact structure and a reporting mechanism that identifies each stakeholder's role in the total impact of food production and consumption stages.
The model aims to increase traceability's utility in understanding the impact of changes on communities affected by food production and consumption, aligning with current and future government requirements, and addressing the needs of communities and consumers. This holistic approach is further supported by an ontological model that forms the logical foundation and a unified terminology. By proposing a holistic and integrated solution across multiple stakeholders, the model emphasizes quality and the extensive impact of championing accountability, sustainability, and responsible practices with global traceability.
With these combined efforts, the food supply chain moves toward a global tracking and tracing process that not only ensures product quality but also addresses its impact on a broader scale, fostering accountability, sustainability, and responsible food production and consumption. 

**Abstract (ZH)**: 质量提升和问责制在食品安全链中的追求，尤其是它们如何影响与食品相关的结果（如饥饿），已成为愈发重要的议题，这就需要一个全面的方法，该方法涵盖产品质量及其对各利益相关者及其社区的影响。这种方法可以带来诸多好处，包括提高产品质量、减少不必要的测量，同时评估并缓解更广泛的社会和环境影响。可追溯影响管理模型（TIMM）提供了一种影响结构和报告机制，可以识别每个利益相关者在食品生产与消费各阶段的总影响中的角色。

该模型旨在增加可追溯性对于理解对受食品生产和消费影响的社区的影响的用途，满足当前和未来的政府要求，并解决社区和消费者的需求。通过一个本体模型作为逻辑基础和统一术语，这种全面的方法进一步得到了支持。模型提出了面向多个利益相关者的综合解决方案，强调质量以及大力推行问责制、可持续性和负责任的实践对全球可追溯性的影响。

通过这些共同努力，食品供应链朝着一个全球追踪和追溯过程前进，不仅确保产品质量，还从更广泛的层面上解决其影响问题，从而促进问责制、可持续性和负责任的食品生产和消费。 

---
# Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models 

**Title (ZH)**: 机器人导航中的语言与规划：最新模型的多语言评估 

**Authors**: Malak Mansour, Ahmed Aly, Bahey Tharwat, Sarim Hashmi, Dong An, Ian Reid  

**Link**: [PDF](https://arxiv.org/pdf/2501.05478)  

**Abstract**: Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications. 

**Abstract (ZH)**: 类似于GPT-4这样的大规模语言模型（LLMs）是在涵盖多个领域的海量数据集上进行训练的，因此在各种任务中都展示了显著的推理、理解和规划能力。本研究提出了首个将阿拉伯语融入机器人学中的视觉-语言导航（VLN）领域的工作，这是一个在现有研究中明显被忽视的领域。我们对最先进的多语言小型语言模型（SLMs），包括GPT-4o mini、Llama 3 8B和Phi-3 medium 14B，以及阿拉伯语中心的大规模语言模型Jais进行了全面评估。我们的方法利用了NavGPT框架，这是一种基于纯LLM的指令遵循导航代理，通过零样本序列动作预测（使用R2R数据集）来评估语言对导航推理的影响。通过全面的实验，我们证明了当提供英阿双语指令时，我们的框架能够进行高级规划。然而，某些模型在使用阿拉伯语进行推理和规划时遇到了困难，这归因于它们固有的能力限制、次优性能和解析问题。这些发现突显了增强语言模型的规划和推理能力对于有效导航的重要性，同时也强调了阿拉伯语模型在实际应用中的潜力，指出这是进一步发展的关键领域。 

---
# IntegrityAI at GenAI Detection Task 2: Detecting Machine-Generated Academic Essays in English and Arabic Using ELECTRA and Stylometry 

**Title (ZH)**: IntegrityAI 在生成式人工智能检测任务2中的应用：使用ELECTRA和文体学检测英语和阿拉伯语的机器生成学术论文 

**Authors**: Mohammad AL-Smadi  

**Link**: [PDF](https://arxiv.org/pdf/2501.05476)  

**Abstract**: Recent research has investigated the problem of detecting machine-generated essays for academic purposes. To address this challenge, this research utilizes pre-trained, transformer-based models fine-tuned on Arabic and English academic essays with stylometric features. Custom models based on ELECTRA for English and AraELECTRA for Arabic were trained and evaluated using a benchmark dataset. Proposed models achieved excellent results with an F1-score of 99.7%, ranking 2nd among of 26 teams in the English subtask, and 98.4%, finishing 1st out of 23 teams in the Arabic one. 

**Abstract (ZH)**: 近年来，研究人员探讨了检测用于学术目的的机器生成论文的问题。为了解决这一挑战，本研究采用了预训练的变换器模型，并对这些模型进行了微调，使其适用于阿拉伯语和英语学术论文的修辞特征。基于ELECTRA的自定义模型用于英语，基于AraELECTRA的自定义模型用于阿拉伯语。这些模型在基准数据集上进行了训练和评估。提出的模型取得了出色的效果，英语文本任务的F1分数达到99.7%，在26支参赛队伍中排名第二；阿拉伯语文本任务的F1分数达到98.4%，在23支参赛队伍中排名第一。 

---
# Retrieval-Augmented Generation by Evidence Retroactivity in LLMs 

**Title (ZH)**: 基于证据回溯的LLM中检索增强生成 

**Authors**: Liang Xiao, Wen Dai, Shuai Chen, Bin Qin, Chongyang Shi, Haopeng Jing, Tianyu Guo  

**Link**: [PDF](https://arxiv.org/pdf/2501.05475)  

**Abstract**: Retrieval-augmented generation has gained significant attention due to its ability to integrate relevant external knowledge, enhancing the accuracy and reliability of the LLMs' responses. Most of the existing methods apply a dynamic multiple retrieval-generating process, to address multi-hop complex questions by decomposing them into sub-problems. However, these methods rely on an unidirectional forward reasoning paradigm, where errors from insufficient reasoning steps or inherent flaws in current retrieval systems are irreversible, potentially derailing the entire reasoning chain. For the first time, this work introduces Retroactive Retrieval-Augmented Generation (RetroRAG), a novel framework to build a retroactive reasoning paradigm. RetroRAG revises and updates the evidence, redirecting the reasoning chain to the correct direction. RetroRAG constructs an evidence-collation-discovery framework to search, generate, and refine credible evidence. It synthesizes inferential evidence related to the key entities in the question from the existing source knowledge and formulates search queries to uncover additional information. As new evidence is found, RetroRAG continually updates and organizes this information, enhancing its ability to locate further necessary evidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is capable of refining its reasoning process iteratively until a reliable answer is obtained. Empirical evaluations show that RetroRAG significantly outperforms existing methods. 

**Abstract (ZH)**: 检索增强生成已因其实现整合相关外部知识的能力而获得广泛关注，这种能力提高了大型语言模型（LLM）响应的准确性和可靠性。现有的大多数方法采用动态多重检索生成过程，通过将多跳复杂问题分解为子问题来应对这些问题。然而，这些方法依赖于单向前向推理范式，其中由于推理步骤不足或当前检索系统固有的缺陷导致的错误是不可逆的，可能破坏整个推理链。本文首次提出了一种名为“回溯检索增强生成”（Retroactive Retrieval-Augmented Generation, RetroRAG）的新颖框架，构建了一种回溯推理范式。RetroRAG 修订和更新证据，朝着正确的方向矫正推理链。RetroRAG 构建了一种证据收集发现框架，用于搜索、生成和精炼可靠证据。它从现有来源知识中整合与问题核心实体相关的推断性证据，制定搜索查询以揭示更多相关信息。随着新证据的发现，RetroRAG 不断更新并组织这些信息，增强其定位更多必要证据的能力。结合一个答案生成器来生成和评估输出，RetroRAG 能够迭代地优化其推理过程，直到获得可靠的答案。实证评估表明，RetroRAG 显著优于现有方法。 

---
# Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis 

**Title (ZH)**: 用于缺失多模态情感分析的模态不变双方向时序表示精炼网络 

**Authors**: Xincheng Wang, Liejun Wang, Yinfeng Yu, Xinxin Jiao  

**Link**: [PDF](https://arxiv.org/pdf/2501.05474)  

**Abstract**: Multimodal Sentiment Analysis (MSA) integrates diverse modalities(text, audio, and video) to comprehensively analyze and understand individuals' emotional states. However, the real-world prevalence of incomplete data poses significant challenges to MSA, mainly due to the randomness of modality missing. Moreover, the heterogeneity issue in multimodal data has yet to be effectively addressed. To tackle these challenges, we introduce the Modality-Invariant Bidirectional Temporal Representation Distillation Network (MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a distillation approach, wherein a complete modality teacher model guides a missing modality student model, ensuring robustness in the presence of modality missing. Simultaneously, we developed the Modality-Invariant Bidirectional Temporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity. 

**Abstract (ZH)**: 多模态情感分析（Multimodal Sentiment Analysis, MSA）通过整合文本、音频和视频等多种模态，全面分析和理解个体的情感状态。然而，现实世界中缺失数据的普遍存在对MSA提出了重大挑战，主要源于模态缺失的随机性。此外，多模态数据中的异质性问题尚未得到有效解决。为应对这些挑战，我们提出了针对缺失多模态情感分析的模态不变双向时间表示蒸馏网络（Modality-Invariant Bidirectional Temporal Representation Distillation Network, MITR-DNet）。MITR-DNet采用蒸馏方法，其中完整的模态教师模型指导缺失模态的学生模型，确保在模态缺失的情况下具有鲁棒性。同时，我们还开发了模态不变双向时间表示学习模块（Modality-Invariant Bidirectional Temporal Representation Learning Module, MIB-TRL）来缓解异质性问题。 

---
# Found in Translation: semantic approaches for enhancing AI interpretability in face verification 

**Title (ZH)**: 翻译成中文后，标题可以是：

翻译之中发现：面向增强人脸识别AI可解释性的语义方法

这样的翻译既保留了原文的意思，又符合学术规范。 

**Authors**: Miriam Doh, Caroline Mazini Rodrigues, N. Boutry, L. Najman, Matei Mancas, Bernard Gosselin  

**Link**: [PDF](https://arxiv.org/pdf/2501.05471)  

**Abstract**: The increasing complexity of machine learning models in computer vision, particularly in face verification, requires the development of explainable artificial intelligence (XAI) to enhance interpretability and transparency. This study extends previous work by integrating semantic concepts derived from human cognitive processes into XAI frameworks to bridge the comprehension gap between model outputs and human understanding. We propose a novel approach combining global and local explanations, using semantic features defined by user-selected facial landmarks to generate similarity maps and textual explanations via large language models (LLMs). The methodology was validated through quantitative experiments and user feedback, demonstrating improved interpretability. Results indicate that our semantic-based approach, particularly the most detailed set, offers a more nuanced understanding of model decisions than traditional methods. User studies highlight a preference for our semantic explanations over traditional pixelbased heatmaps, emphasizing the benefits of human-centric interpretability in AI. This work contributes to the ongoing efforts to create XAI frameworks that align AI models behaviour with human cognitive processes, fostering trust and acceptance in critical applications. 

**Abstract (ZH)**: 计算机视觉领域，尤其是面部验证中机器学习模型日益复杂的趋势，要求发展可解释的人工智能（XAI）以增强可解释性和透明度。本研究在前人工作的基础上，将源自人类认知过程的语义概念融入XAI框架中，以弥合模型输出与人类理解之间的理解差距。我们提出了一种结合全局和局部解释的新方法，利用用户选择的面部关键点定义的语义特征生成相似度图和通过大规模语言模型（LLMs）生成的文本解释。该方法通过定量实验和用户反馈得到了验证，显示出改进的可解释性。结果表明，基于语义的方法，特别是最详细的一组，相比于传统方法提供了更加细致的模型决策理解。用户研究显示，用户更偏好我们的语义解释而非传统的基于像素的热图，突出了以人为中心的可解释性在人工智能中的优势。这项研究为创建使AI模型行为与人类认知过程一致的XAI框架做出了贡献，促进了在关键应用中的信任和接受。 

---
# RTLSquad: Multi-Agent Based Interpretable RTL Design 

**Title (ZH)**: RTLSquad: 基于多代理的可解释RTL设计 

**Authors**: Bowei Wang, Qi Xiong, Zeqing Xiang, Lei Wang, Renzhi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.05470)  

**Abstract**: Optimizing Register-Transfer Level (RTL) code is crucial for improving hardware PPA performance. Large Language Models (LLMs) offer new approaches for automatic RTL code generation and optimization. However, existing methods often lack decision interpretability (sufficient, understandable justification for decisions), making it difficult for hardware engineers to trust the generated results, thus preventing these methods from being integrated into the design process. To address this, we propose RTLSquad, a novel LLM-Based Multi-Agent system for interpretable RTL code generation. RTLSquad divides the design process into exploration, implementation, and verification & evaluation stages managed by specialized agent squads, generating optimized RTL code through inter-agent collaboration, and providing decision interpretability through the communication process. Experiments show that RTLSquad excels in generating functionally correct RTL code and optimizing PPA performance, while also having the capability to provide decision paths, demonstrating the practical value of our system. 

**Abstract (ZH)**: 优化寄存器传输级（RTL）代码对于提高硬件性能参数（PPA）至关重要。大型语言模型（LLMs）为自动RTL代码生成和优化提供了新的方法。然而，现有方法通常缺乏决策解释性（即，充分且可理解的决策依据），这使得硬件工程师难以信任生成的结果，从而导致这些方法难以集成到设计过程中。为解决这一问题，我们提出了一种名为RTLSquad的新型基于LLM的多智能体系统，用于可解释的RTL代码生成。RTLSquad将设计过程分为探索、实现、验证与评估阶段，并由专门的智能体战队管理这些阶段，通过智能体间的协作生成优化的RTL代码，并通过通信过程提供决策解释性。实验结果表明，RTLSquad在生成功能正确的RTL代码和优化PPA性能方面表现出色，同时还能提供决策路径，验证了我们系统的实际应用价值。 

---
# LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models 

**Title (ZH)**: LLM-MedQA：通过大规模语言模型案例研究增强医学问答 

**Authors**: Hang Yang, Hao Chen, Hui Guo, Yineng Chen, Ching-Sheng Lin, Shu Hu, Jinrong Hu, Xi Wu, Xin Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05464)  

**Abstract**: Accurate and efficient question-answering systems are essential for delivering high-quality patient care in the medical field. While Large Language Models (LLMs) have made remarkable strides across various domains, they continue to face significant challenges in medical question answering, particularly in understanding domain-specific terminologies and performing complex reasoning. These limitations undermine their effectiveness in critical medical applications. To address these issues, we propose a novel approach incorporating similar case generation within a multi-agent medical question-answering (MedQA) system. Specifically, we leverage the Llama3.1:70B model, a state-of-the-art LLM, in a multi-agent architecture to enhance performance on the MedQA dataset using zero-shot learning. Our method capitalizes on the model's inherent medical knowledge and reasoning capabilities, eliminating the need for additional training data. Experimental results show substantial performance gains over existing benchmark models, with improvements of 7% in both accuracy and F1-score across various medical QA tasks. Furthermore, we examine the model's interpretability and reliability in addressing complex medical queries. This research not only offers a robust solution for medical question answering but also establishes a foundation for broader applications of LLMs in the medical domain. 

**Abstract (ZH)**: 在医疗领域，准确高效的问题回答系统对于提供高质量的患者护理至关重要。尽管大规模语言模型（LLMs）在各个领域取得了显著的进展，但它们在医疗问题回答方面仍然面临重大挑战，特别是在理解和处理领域特定术语以及进行复杂推理方面。这些限制削弱了它们在关键医疗应用中的有效性。为解决这些问题，我们提出了一种新颖的方法，即在多代理医疗问题回答（MedQA）系统中结合相似病例生成。具体而言，我们利用最先进的LLM——Llama3.1:70B模型，在多代理架构中通过零样本学习增强MedQA数据集上的性能。我们的方法利用模型固有的医疗知识和推理能力，无需额外的训练数据。实验结果表明，在各种医疗问答任务中，与现有基准模型相比，我们的方法在准确性和F1分数方面分别提升了7%。此外，我们还评估了模型在处理复杂医疗查询方面的可解释性和可靠性。本研究不仅为医疗问题回答提供了一种稳健的解决方案，也为大规模语言模型在医疗领域的更广泛应用奠定了基础。 

---
# Proof Recommendation System for the HOL4 Theorem Prover 

**Title (ZH)**: HOL4定理证明器的证明推荐系统 

**Authors**: Nour Dekhil, Adnan Rashid, Sofiene Tahar  

**Link**: [PDF](https://arxiv.org/pdf/2501.05463)  

**Abstract**: We introduce a proof recommender system for the HOL4 theorem prover. Our tool is built upon a transformer-based model [2] designed specifically to provide proof assistance in HOL4. The model is trained to discern theorem proving patterns from extensive libraries of HOL4 containing proofs of theorems. Consequently, it can accurately predict the next tactic(s) (proof step(s)) based on the history of previously employed tactics. The tool operates by reading a given sequence of tactics already used in a proof process (in our case, it contains at least three tactics), referred to as the current proof state, and provides recommendations for the next optimal proof step(s). 

**Abstract (ZH)**: 我们介绍了一种针对HOL4定理证明器的证明推荐系统。我们的工具基于一种Transformer模型[2]，该模型专门设计用于在HOL4中提供定理证明辅助。该模型经过训练，可以从包含大量HOL4定理证明的 extensive 库中识别出定理证明模式。因此，它可以根据之前使用的战术的历史记录，准确地预测下一步（或是几步）最优的战术。该工具通过读取证明过程中已使用的战术序列（在我们的情况下，至少包含三个战术），即当前的证明状态，并提供下步最优证明步骤的推荐。 

---
# Efficiently serving large multimedia models using EPD Disaggregation 

**Title (ZH)**: 使用EPD解耦高效服务大型多媒体模型 

**Authors**: Gursimran Singh, Xinglu Wang, Ivan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan  

**Link**: [PDF](https://arxiv.org/pdf/2501.05460)  

**Abstract**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step helps convert raw inputs into tokenized representations that inflate the token sequence for the prefill phase, negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our disaggregation approach alleviates memory bottlenecks, mitigates synchronization delays, and supports flexible batching. Specifically, we employ a new caching mechanism for multimodal tokens, enabling asynchronous transfer of multimodal tokens and introduce an integrated module to find optimal config for EPD system and minimize resource usage while maximizing SLO-based performance metric. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\times$ lesser for encoding-stage GPUs), that supports upto 22$\times$ higher batch sizes, 10$\times$ more number of images/ request, 2.2$\times$ higher kv cache size. Further, it leads to significant improvements in end-to-end throughput (up to 57\% better), and latency metrics (TTFT up to 71\% lower), compared to systems that do not disaggregate. Our findings underscore the potential of EPD disaggregation to enable resource-efficient and high-performance multimodal inference at scale. 

**Abstract (ZH)**: 大规模多模态模型（LMMs）通过处理多种输入如图像、音频和视频扩展了大规模语言模型（LLMs），但同时也增加了多模态编码阶段，导致计算和内存开销增加。这一过程有助于将原始输入转换为标记表示，增加了填充阶段的标记序列长度，从而负面影响了关键的服务级别目标（SLOs），如第一个标记的时间（TTFT）和端到端吞吐量。我们提出了一种新的框架——编码-填充-解码分层（EPD Disaggregation），该框架将编码、填充和解码阶段分配到专用资源中。与当前系统的捆绑式编码和填充不同，我们的分层方法缓解了内存瓶颈，减少了同步延迟，并支持灵活的批处理。具体来说，我们引入了一种新的缓存机制，以异步传输多模态标记，并引入了一个集成模块来为EPD系统找到最优配置，同时最大限度地减少资源使用并最大化基于SLO的性能指标。实验评估表明，与流行的LMMs系统相比，EPD分层方法在编码阶段的GPU内存效率上有了显著提高（最多降低了15倍），支持了高达22倍的更大批量处理，每个请求中的图像数量增加了10倍，关键值缓存大小增加了2.2倍。此外，它在端到端吞吐量（最多提高了57%）和延迟指标（TTFT最多降低了71%）方面也取得了显著改进，相比之下，不进行分层的系统则没有这种改进。我们的研究结果强调了EPD分层方法在大规模多模态推理中实现资源高效和高性能的潜力。 

---
# Upstream and Downstream AI Safety: Both on the Same River? 

**Title (ZH)**: 上游与下游人工智能安全：同一条河流上的两面？ 

**Authors**: John McDermid, Yan Jia, Ibrahim Habli  

**Link**: [PDF](https://arxiv.org/pdf/2501.05455)  

**Abstract**: Traditional safety engineering assesses systems in their context of use, e.g. the operational design domain (road layout, speed limits, weather, etc.) for self-driving vehicles (including those using AI). We refer to this as downstream safety. In contrast, work on safety of frontier AI, e.g. large language models which can be further trained for downstream tasks, typically considers factors that are beyond specific application contexts, such as the ability of the model to evade human control, or to produce harmful content, e.g. how to make bombs. We refer to this as upstream safety. We outline the characteristics of both upstream and downstream safety frameworks then explore the extent to which the broad AI safety community can benefit from synergies between these frameworks. For example, can concepts such as common mode failures from downstream safety be used to help assess the strength of AI guardrails? Further, can the understanding of the capabilities and limitations of frontier AI be used to inform downstream safety analysis, e.g. where LLMs are fine-tuned to calculate voyage plans for autonomous vessels? The paper identifies some promising avenues to explore and outlines some challenges in achieving synergy, or a confluence, between upstream and downstream safety frameworks. 

**Abstract (ZH)**: 传统的安全工程在使用环境中评估系统，例如自动驾驶车辆（包括使用AI的车辆）的操作设计域（道路布局、速度限制、天气等）。我们将这种安全评估称为下游安全。相比之下，关于前沿AI安全的研究，例如可以进一步训练以用于下游任务的大规模语言模型，通常考虑超出特定应用场景的因素，如模型逃避人类控制的能力，或生成有害内容的能力，例如如何制造炸弹。我们将这种安全评估称为上游安全。我们概述了上下游安全框架的特点，然后探讨这两个框架之间产生的协同作用在广泛的人工智能安全社区中的潜力。例如，下游安全中的共同失效模式概念是否可以用来评估AI护栏的强度？此外，对前沿AI能力及其限制的理解是否可以用来指导下游安全分析，例如，当大规模语言模型被微调以计算自主船舶的航线计划时。该论文指出了在实现上下游安全框架之间的协同作用或融合方面的一些有前景的研究方向，同时也概述了一些面临的挑战。 

---
# FOCUS: Towards Universal Foreground Segmentation 

**Title (ZH)**: FOCUS：面向通用前景分割的研究 

**Authors**: Zuyao You, Lingyu Kong, Lingchen Meng, Zuxuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05238)  

**Abstract**: Foreground segmentation is a fundamental task in computer vision, encompassing various subdivision tasks. Previous research has typically designed task-specific architectures for each task, leading to a lack of unification. Moreover, they primarily focus on recognizing foreground objects without effectively distinguishing them from the background. In this paper, we emphasize the importance of the background and its relationship with the foreground. We introduce FOCUS, the Foreground ObjeCts Universal Segmentation framework that can handle multiple foreground tasks. We develop a multi-scale semantic network using the edge information of objects to enhance image features. To achieve boundary-aware segmentation, we propose a novel distillation method, integrating the contrastive learning strategy to refine the prediction mask in multi-modal feature space. We conduct extensive experiments on a total of 13 datasets across 5 tasks, and the results demonstrate that FOCUS consistently outperforms the state-of-the-art task-specific models on most metrics. 

**Abstract (ZH)**: 前景分割是计算机视觉中的一个基本任务，涵盖了各种细分任务。以往的研究通常为每个任务设计特定的任务架构，导致缺乏统一性。此外，它们主要集中在识别前景对象，而未能有效地将其与背景区分开。本文强调了背景及其与前景关系的重要性。我们引入了FOCUS，即通用前景对象分割框架，能够处理多重前景任务。我们利用对象边缘信息开发了一种多尺度语义网络，以增强图像特征。为实现边界感知分割，我们提出了一种新颖的蒸馏方法，结合对比学习策略在多模态特征空间中优化预测掩模。我们在5个任务的13个数据集上进行了广泛实验，结果表明，FOCUS在大多数指标上均优于最先进的特定任务模型。 

---
