# VideoRAG: Retrieval-Augmented Generation over Video Corpus 

**Title (ZH)**: VideoRAG：基于视频语料的检索增强生成 

**Authors**: Soyeong Jeong, Kangsan Kim, Jinheon Baek, Sung Ju Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2501.05874)  

**Abstract**: Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. 

**Abstract (ZH)**: 检索增强生成（RAG）是一种强大的策略，用于解决基础模型生成事实性错误输出的问题。通过检索与查询相关的外部知识并将其纳入生成过程，RAG能够克服这一问题。然而，现有的RAG方法主要集中在文本信息上，近年来的一些进展开始考虑图像，但它们很大程度上忽略了视频这一丰富的多模态知识来源，视频能够比任何其他模态更有效地表示事件、过程和上下文细节。虽然有一些最近的研究探讨了在响应生成过程中整合视频的方法，但它们要么预先定义与查询相关的视频而不根据查询检索它们，要么将视频转换为文本描述而不充分利用其多模态丰富性。为了解决这些问题，我们引入了VideoRAG，这是一种新型框架，不仅能够根据查询的相关性动态检索相关视频，还能够在生成输出时利用视频的视觉和文本信息。此外，为了实现这一目标，我们的方法围绕最近的大型视频语言模型（LVLMs）的进展展开，这使得可以直接处理视频内容以用于检索，并无缝地将检索到的视频与查询联合集成。通过实验验证了VideoRAG的有效性，展示了它在与基线方法相比的优越性。 

---
# Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories 

**Title (ZH)**: 多模态人工智能系统在多学科物理概念测评中的多语言表现 

**Authors**: Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn  

**Link**: [PDF](https://arxiv.org/pdf/2501.06143)  

**Abstract**: We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, on a diverse set of physics concept inventories spanning multiple languages and subject areas. The inventories taken from the PhysPort website cover the classical physics topics of mechanics, electromagnetism, optics, and thermodynamics as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images mirroring what a student would see on paper, assessing the system's multimodal functionality. The AI is prompted in English and autonomously chooses the language of its response - either remaining in the nominal language of the test, switching entirely to English, or mixing languages - revealing adaptive behavior dependent on linguistic complexity and data availability. Our results indicate some variation in performance across subject areas, with laboratory skills standing out as the area of poorest performance. Furthermore, the AI's performance on questions that require visual interpretation of images is worse than on purely text-based questions. Questions that are difficult for the AI tend to be that way invariably of the inventory language. We also find large variations in performance across languages, with some appearing to benefit substantially from language switching, a phenomenon similar to code-switching ofhuman speakers. Overall, comparing the obtained AI results to the existing literature, we find that the AI system outperforms average undergraduate students post-instruction in all subject areas but laboratory skills. 

**Abstract (ZH)**: 我们研究了一种基于大型语言模型的人工智能系统GPT-4o在多种语言和多模态环境下的表现，这些测试涉及物理学概念测验，涵盖多个语言和学科领域。这些测验来源于PhysPort网站，包括经典物理学的力学、电磁学、光学和热力学，以及相对论、量子力学、天文学、数学和实验技能。与之前仅限于文本的研究不同，我们以一种学生在纸上见到的形式上传了测验图片，以此评估系统的多模态功能。系统被要求用英文提示，但可以根据其响应的复杂性和数据可用性自主选择回应的语言—这可能保持在测试的名义语言中，完全转为英文，或者混合使用多种语言，反映出其具有适应性行为。我们的结果显示，不同学科领域中的表现存在一定的差异，其中实验技能领域表现出最差的表现。此外，人工智能在需要对图像进行视觉解释的问题上的表现不如单纯的文本问题。AI在难以解答的问题上与测验语言密切相关。我们还发现，不同语言的表现存在显著差异，一些语言似乎从语言切换中获益显著，这种现象类似于人类讲话者的代码切换。总体而言，将获得的人工智能结果与现有文献进行比较，我们发现该人工智能系统在所有学科领域（除了实验技能）的表现超过了平均本科学生的水平。 

---
# CoDriveVLM: VLM-Enhanced Urban Cooperative Dispatching and Motion Planning for Future Autonomous Mobility on Demand Systems 

**Title (ZH)**: CoDriveVLM: 基于VLM的城市协同调度与运动规划技术提升在未来按需自主移动系统中的应用 

**Authors**: Haichao Liu, Ruoyu Yao, Wenru Liu, Zhenmin Huang, Shaojie Shen, Jun Ma  

**Link**: [PDF](https://arxiv.org/pdf/2501.06132)  

**Abstract**: The increasing demand for flexible and efficient urban transportation solutions has spotlighted the limitations of traditional Demand Responsive Transport (DRT) systems, particularly in accommodating diverse passenger needs and dynamic urban environments. Autonomous Mobility-on-Demand (AMoD) systems have emerged as a promising alternative, leveraging connected and autonomous vehicles (CAVs) to provide responsive and adaptable services. However, existing methods primarily focus on either vehicle scheduling or path planning, which often simplify complex urban layouts and neglect the necessity for simultaneous coordination and mutual avoidance among CAVs. This oversimplification poses significant challenges to the deployment of AMoD systems in real-world scenarios. To address these gaps, we propose CoDriveVLM, a novel framework that integrates high-fidelity simultaneous dispatching and cooperative motion planning for future AMoD systems. Our method harnesses Vision-Language Models (VLMs) to enhance multi-modality information processing, and this enables comprehensive dispatching and collision risk evaluation. The VLM-enhanced CAV dispatching coordinator is introduced to effectively manage complex and unforeseen AMoD conditions, thus supporting efficient scheduling decision-making. Furthermore, we propose a scalable decentralized cooperative motion planning method via consensus alternating direction method of multipliers (ADMM) focusing on collision risk evaluation and decentralized trajectory optimization. Simulation results demonstrate the feasibility and robustness of CoDriveVLM in various traffic conditions, showcasing its potential to significantly improve the fidelity and effectiveness of AMoD systems in future urban transportation networks. The code is available at this https URL. 

**Abstract (ZH)**: 随着对灵活和高效城市交通解决方案需求的不断增加，传统需求响应运输（DRT）系统在满足多样化乘客需求和动态城市环境方面的局限性日益突出。自动驾驶按需出行（AMoD）系统作为一种有前途的替代方案已经涌现出来，利用连接和自动驾驶车辆（CAVs）提供响应性和适应性服务。然而，现有的方法主要集中在车辆调度或路径规划上，通常简化了复杂的城市布局并忽视了CAVs之间同时协调和互斥避让的必要性。这种简化在实际部署AMoD系统时带来了重大挑战。为解决这些缺口，我们提出了一种名为CoDriveVLM的新型框架，该框架集成了高保真同时调度和协同运动规划，以适应未来AMoD系统的需求。我们的方法使用视觉-语言模型（VLMs）增强多模态信息处理能力，这能够实现全面的调度和碰撞风险评估。通过引入基于VLMs增强的CAVs调度协调器，可以有效管理复杂的和不可预见的AMoD条件，从而支持高效的调度决策。此外，我们提出了一个基于共识交替方向乘子法（ADMM）的可扩展的分散协同运动规划方法，重点关注碰撞风险评估和分散轨迹优化。仿真结果表明，CoDriveVLM在各种交通条件下具有可行性和鲁棒性，展示了其在未来城市交通网络中显著提高AMoD系统的准确性和有效性方面的潜力。代码可在此网址获取：https://xxxalte URLxxx。 

---
# BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response 

**Title (ZH)**: BRIGHT：一种适用于全天候灾害应对的全球分布多模态高分辨率建筑 DAMAGE 评估数据集 

**Authors**: Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya  

**Link**: [PDF](https://arxiv.org/pdf/2501.06019)  

**Abstract**: Disaster events occur around the world and cause significant damage to human life and property. Earth observation (EO) data enables rapid and comprehensive building damage assessment (BDA), an essential capability in the aftermath of a disaster to reduce human casualties and to inform disaster relief efforts. Recent research focuses on the development of AI models to achieve accurate mapping of unseen disaster events, mostly using optical EO data. However, solutions based on optical data are limited to clear skies and daylight hours, preventing a prompt response to disasters. Integrating multimodal (MM) EO data, particularly the combination of optical and SAR imagery, makes it possible to provide all-weather, day-and-night disaster responses. Despite this potential, the development of robust multimodal AI models has been constrained by the lack of suitable benchmark datasets. In this paper, we present a BDA dataset using veRy-hIGH-resoluTion optical and SAR imagery (BRIGHT) to support AI-based all-weather disaster response. To the best of our knowledge, BRIGHT is the first open-access, globally distributed, event-diverse MM dataset specifically curated to support AI-based disaster response. It covers five types of natural disasters and two types of man-made disasters across 12 regions worldwide, with a particular focus on developing countries where external assistance is most needed. The optical and SAR imagery in BRIGHT, with a spatial resolution between 0.3-1 meters, provides detailed representations of individual buildings, making it ideal for precise BDA. In our experiments, we have tested seven advanced AI models trained with our BRIGHT to validate the transferability and robustness. The dataset and code are available at this https URL. BRIGHT also serves as the official dataset for the 2025 IEEE GRSS Data Fusion Contest. 

**Abstract (ZH)**: 自然灾害在全球范围内发生，对人类生命和财产造成重大损失。地球观测（EO）数据能够实现快速而全面的建筑损害评估（BDA），这是灾害发生后减少人员伤亡并指导灾害救援工作的重要能力。最近的研究集中在开发AI模型以实现对未见过的灾害事件的准确映射，主要使用光学EO数据。然而，基于光学数据的解决方案受限于晴朗天气和白天时间，无法迅速应对灾害。通过整合多模态（MM）EO数据，特别是光学和SAR图像的结合，可以使全天候、昼夜灾害响应成为可能。尽管具有这一潜力，但由于缺乏合适的基准数据集，开发稳健的多模态AI模型受到了限制。本文提出了一种基于极高分辨率光学和SAR图像的BDA数据集（BRIGHT），以支持基于AI的全天候灾害响应。据我们所知，BRIGHT是第一个开放访问、全球分布、事件多样性明确的数据集，专门用于支持基于AI的灾害响应。它涵盖了五种自然灾害和两种人为灾害，在全球12个地区进行监测，特别关注最需要外部援助的发展中国家。BRIGHT中的光学和SAR图像，在0.3至1米的空间分辨率下，能详细展示个体建筑物的情况，使其适用于精确的BDA。在我们的实验中，我们测试了七种先进的AI模型，这些模型均使用了我们的BRIGHT数据集，以验证其适用性和鲁棒性。数据集和代码可在以下网址获取：[具体网址]. BRIGHT也是2025年IEEE GRSS数据融合竞赛的官方数据集。 

---
# Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models 

**Title (ZH)**: 《Migician：揭示多模态大型语言模型中自由形式多图像 grounding 的魔力》 

**Authors**: You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.05767)  

**Abstract**: The recent advancement of Multimodal Large Language Models (MLLMs) has significantly improved their fine-grained perception of single images and general comprehension across multiple images. However, existing MLLMs still face challenges in achieving precise grounding in complex multi-image scenarios. To address this, we first explore a Chain-of-Thought (CoT) framework that integrates single-image grounding with multi-image comprehension. While partially effective, it remains unstable and struggles to capture abstract visual information due to its non-end-to-end nature. Therefore, we introduce Migician, the first multi-image grounding model capable of performing free-form and accurate grounding across multiple images. To support this, we present the MGrounding-630k dataset, which comprises data for several multi-image grounding tasks derived from existing datasets, along with newly generated free-form grounding instruction-following data. Furthermore, we propose MIG-Bench, a comprehensive benchmark specifically designed for evaluating multi-image grounding capabilities. Experimental results demonstrate that our model achieves significantly superior multi-image grounding capabilities, outperforming the best existing MLLMs by 21.61% and even surpassing much larger 70B models. Our code, model, dataset, and benchmark are fully open-sourced. 

**Abstract (ZH)**: 近年来，多模态大规模语言模型（MLLMs）在单图像的细粒度感知和多图像的一般理解方面取得了显著进步。然而，现有的MLLMs在复杂多图像场景下的精准定位方面仍面临挑战。为解决这一问题，我们首先探索将单图像定位与多图像理解结合的链式思考（CoT）框架。虽然部分有效，但由于其非端到端的特性，该框架仍不稳定，并且难以捕捉抽象的视觉信息。因此，我们提出了Migician，这是首个能够跨多个图像进行自由形式和准确定位的多图像定位模型。为此，我们发布了MGrounding-630k数据集，该数据集包含来自现有数据集的多种多图像定位任务的数据，以及新生成的自由形式的定位指令遵循数据。此外，我们还提出了MIG-Bench，这是一个全面的基准，专门用于评估多图像定位能力。实验结果表明，我们的模型在多图像定位能力方面取得了显著优越的表现，相较于现有的最佳MLLMs提高了21.61%，甚至超越了更庞大的70B模型。我们的代码、模型、数据集和基准均已开源。 

---
# Zero-shot Shark Tracking and Biometrics from Aerial Imagery 

**Title (ZH)**: 零样本鲨鱼跟踪与遥感图像中的生物识别 

**Authors**: Chinmay K Lalgudi, Mark E Leone, Jaden V Clark, Sergio Madrigal-Mora, Mario Espinoza  

**Link**: [PDF](https://arxiv.org/pdf/2501.05717)  

**Abstract**: The recent widespread adoption of drones for studying marine animals provides opportunities for deriving biological information from aerial imagery. The large scale of imagery data acquired from drones is well suited for machine learning (ML) analysis. Development of ML models for analyzing marine animal aerial imagery has followed the classical paradigm of training, testing, and deploying a new model for each dataset, requiring significant time, human effort, and ML expertise. We introduce Frame Level ALIgment and tRacking (FLAIR), which leverages the video understanding of Segment Anything Model 2 (SAM2) and the vision-language capabilities of Contrastive Language-Image Pre-training (CLIP). FLAIR takes a drone video as input and outputs segmentation masks of the species of interest across the video. Notably, FLAIR leverages a zero-shot approach, eliminating the need for labeled data, training a new model, or fine-tuning an existing model to generalize to other species. With a dataset of 18,000 drone images of Pacific nurse sharks, we trained state-of-the-art object detection models to compare against FLAIR. We show that FLAIR massively outperforms these object detectors and performs competitively against two human-in-the-loop methods for prompting SAM2, achieving a Dice score of 0.81. FLAIR readily generalizes to other shark species without additional human effort and can be combined with novel heuristics to automatically extract relevant information including length and tailbeat frequency. FLAIR has significant potential to accelerate aerial imagery analysis workflows, requiring markedly less human effort and expertise than traditional machine learning workflows, while achieving superior accuracy. By reducing the effort required for aerial imagery analysis, FLAIR allows scientists to spend more time interpreting results and deriving insights about marine ecosystems. 

**Abstract (ZH)**: 最近，无人机在研究海洋生物中的广泛应用为从航空影像中提取生物信息提供了机会。无人机获取的大规模影像数据非常适合机器学习（ML）分析。对于海洋生物航空影像的ML模型开发通常遵循经典范式：为每个数据集训练、测试并部署新模型，这需要大量的时间、人力和ML专业知识。我们提出了一种名为Frame Level ALIgment and tRacking (FLAIR)的方法，该方法利用了Segment Anything Model 2 (SAM2)的视频理解能力以及Contrastive Language-Image Pre-training (CLIP)的视觉-语言能力。FLAIR接受无人机视频作为输入，并输出目标物种在视频中的分割掩码。值得注意的是，FLAIR采用了一种零样本方法，无需标注数据、训练新模型或微调现有模型即可实现对其他物种的泛化。通过使用包含18,000张太平洋护士鲨无人机影像的数据集，我们训练了最先进的物体检测模型以与FLAIR进行比较。结果显示，FLAIR在多个方面显著优于这些物体检测器，并且在对SAM2进行两种半监督方法的提示时表现出竞争力，取得了0.81的Dice分数。FLAIR无需额外的人力努力即可泛化到其他鲨鱼物种，并且可以结合新的启发式方法自动提取相关信息，包括体长和摆尾频率。FLAIR具有显著加速航空影像分析工作流程的潜力，所需的劳动力和专业知识远少于传统机器学习工作流程，同时仍能达到出色的准确性。通过减少对航空影像分析所需的人力努力，FLAIR使科学家们能够花费更多时间解释结果并对海洋生态系统进行深入理解。 

---
# Cascaded Self-Evaluation Augmented Training for Efficient Multimodal Large Language Models 

**Title (ZH)**: 级联自评估增强训练以提高高效多模态大型语言模型的效果 

**Authors**: Zheqi Lv, Wenkai Wang, Jiawei Wang, Shengyu Zhang, Fei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05662)  

**Abstract**: Efficient Multimodal Large Language Models (EMLLMs) have rapidly advanced recently. Incorporating Chain-of-Thought (CoT) reasoning and step-by-step self-evaluation has improved their performance. However, limited parameters often hinder EMLLMs from effectively using self-evaluation during inference. Key challenges include synthesizing evaluation data, determining its quantity, optimizing training and inference strategies, and selecting appropriate prompts.
To address these issues, we introduce Self-Evaluation Augmented Training (SEAT). SEAT uses more powerful EMLLMs for CoT reasoning, data selection, and evaluation generation, then trains EMLLMs with the synthesized data. However, handling long prompts and maintaining CoT reasoning quality are problematic. Therefore, we propose Cascaded Self-Evaluation Augmented Training (Cas-SEAT), which breaks down lengthy prompts into shorter, task-specific cascaded prompts and reduces costs for resource-limited settings. During data synthesis, we employ open-source 7B-parameter EMLLMs and annotate a small dataset with short prompts.
Experiments demonstrate that Cas-SEAT significantly boosts EMLLMs' self-evaluation abilities, improving performance by 19.68%, 55.57%, and 46.79% on the MathVista, Math-V, and We-Math datasets, respectively. Additionally, our Cas-SEAT Dataset serves as a valuable resource for future research in enhancing EMLLM self-evaluation. 

**Abstract (ZH)**: 高效多模态大型语言模型（EMLLMs）最近取得了迅速的进步。引入链式思维（CoT）推理和逐步自我评估提高了其性能。然而，有限的参数往往妨碍EMLLMs在推理过程中有效地利用自我评估。关键挑战包括合成评估数据、确定其数量、优化训练和推理策略，以及选择合适的提示。

为了解决这些问题，我们提出了自我评估增强训练（SEAT）。SEAT 使用更强大的EMLLMs进行CoT推理、数据选择和评估生成，然后使用合成的数据训练EMLLMs。然而，处理长提示并保持CoT推理质量存在困难。因此，我们提出了分步自我评估增强训练（Cas-SEAT），该方法将长提示分解为更短、任务特定的分步提示，以降低资源有限环境下成本。在数据合成过程中，我们使用开源的7B参数EMLLMs，并对短提示进行了标注。

实验表明，Cas-SEAT 显著提高了EMLLMs的自我评估能力，在MathVista、Math-V和We-Math数据集上的表现分别提高了19.68%、55.57%和46.79%。此外，我们提出的Cas-SEAT数据集为未来增强EMLLM自我评估的研究提供了宝贵的资源。 

---
# Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding 

**Title (ZH)**: 基于CLIP的自监督动态场景理解：面向自动驾驶的多模态模型 

**Authors**: Mohammed Elhenawy, Huthaifa I. Ashqar, Andry Rakotonirainy, Taqwa I. Alhadidi, Ahmed Jaber, Mohammad Abu Tami  

**Link**: [PDF](https://arxiv.org/pdf/2501.05566)  

**Abstract**: Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language-Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge devices. The proposed system outperforms state-of-the-art in-context learning methods, including the zero-shot capabilities of GPT-4o, particularly in complex scenarios. By conducting frame-level analysis on the Honda Scenes Dataset, which contains a collection of about 80 hours of annotated driving videos capturing diverse real-world road and weather conditions, our study highlights the robustness of CLIP models in learning visual concepts from natural language supervision. Results also showed that fine-tuning the CLIP models, such as ViT-L/14 and ViT-B/32, significantly improved scene classification, achieving a top F1 score of 91.1%. These results demonstrate the ability of the system to deliver rapid and precise scene recognition, which can be used to meet the critical requirements of Advanced Driver Assistance Systems (ADAS). This study shows the potential of CLIP models to provide scalable and efficient frameworks for dynamic scene understanding and classification. Furthermore, this work lays the groundwork for advanced autonomous vehicle technologies by fostering a deeper understanding of driver behavior, road conditions, and safety-critical scenarios, marking a significant step toward smarter, safer, and more context-aware autonomous driving systems. 

**Abstract (ZH)**: 场景理解对于提高驾驶员安全性、生成以人为中心的自动车辆（AV）决策解释以及利用人工智能（AI）进行回顾性驾驶视频分析至关重要。本研究开发了一种使用对比语言-图像预训练（CLIP）模型的动力场景检索系统，该系统可以优化以适应边缘设备上的实时部署。所提出系统在包含约80小时标注驾驶视频（涵盖了多样化的实际道路和天气条件）的Honda Scenes数据集的帧级分析中，表现出色，特别是在复杂场景中超过了最先进的上下文学习方法，特别是GPT-4o的零样本能力。通过Honda Scenes数据集的帧级分析，本研究强调了CLIP模型在自然语言监督下学习视觉概念的稳健性。结果显示，对CLIP模型（如ViT-L/14和ViT-B/32）进行微调显著提高了场景分类性能，实现了最高的F1分数91.1%。这些结果证明了该系统能够快速准确地识别场景，可以满足高级驾驶辅助系统（ADAS）的关键要求。本研究展示了CLIP模型在动态场景理解和分类方面提供可扩展和高效框架的潜力。此外，本研究为进一步自主车辆技术的发展奠定了基础，促进了对驾驶员行为、道路条件和安全关键场景的深入理解，为更智能、更安全且更情境感知的自主驾驶系统迈出了重要一步。 

---
# Modality-Invariant Bidirectional Temporal Representation Distillation Network for Missing Multimodal Sentiment Analysis 

**Title (ZH)**: 针对缺失多模态情感分析的模态不变双向时序表示蒸馏网络 

**Authors**: Xincheng Wang, Liejun Wang, Yinfeng Yu, Xinxin Jiao  

**Link**: [PDF](https://arxiv.org/pdf/2501.05474)  

**Abstract**: Multimodal Sentiment Analysis (MSA) integrates diverse modalities(text, audio, and video) to comprehensively analyze and understand individuals' emotional states. However, the real-world prevalence of incomplete data poses significant challenges to MSA, mainly due to the randomness of modality missing. Moreover, the heterogeneity issue in multimodal data has yet to be effectively addressed. To tackle these challenges, we introduce the Modality-Invariant Bidirectional Temporal Representation Distillation Network (MITR-DNet) for Missing Multimodal Sentiment Analysis. MITR-DNet employs a distillation approach, wherein a complete modality teacher model guides a missing modality student model, ensuring robustness in the presence of modality missing. Simultaneously, we developed the Modality-Invariant Bidirectional Temporal Representation Learning Module (MIB-TRL) to mitigate heterogeneity. 

**Abstract (ZH)**: 多模态情感分析（Multimodal Sentiment Analysis, MSA）结合了多种模态（文本、音频和视频），以全面分析和理解个体的情感状态。然而，现实世界中数据不完整的问题对MSA构成了重大挑战，主要原因是模态缺失的随机性。此外，多模态数据的异质性问题尚未得到有效解决。为应对这些挑战，我们提出了用于缺失多模态情感分析的模态不变双向时间表示蒸馏网络（Modality-Invariant Bidirectional Temporal Representation Distillation Network, MITR-DNet）。MITR-DNet 采用蒸馏方法，由完整的模态教师模型引导缺失模态的学生模型，确保在模态缺失情况下的鲁棒性。同时，我们开发了模态不变双向时间表示学习模块（Modality-Invariant Bidirectional Temporal Representation Learning Module, MIB-TRL），以减轻异质性问题。 

---
# Efficiently serving large multimedia models using EPD Disaggregation 

**Title (ZH)**: 使用EPD解耦高效服务大型多媒体模型 

**Authors**: Gursimran Singh, Xinglu Wang, Ivan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan  

**Link**: [PDF](https://arxiv.org/pdf/2501.05460)  

**Abstract**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step helps convert raw inputs into tokenized representations that inflate the token sequence for the prefill phase, negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our disaggregation approach alleviates memory bottlenecks, mitigates synchronization delays, and supports flexible batching. Specifically, we employ a new caching mechanism for multimodal tokens, enabling asynchronous transfer of multimodal tokens and introduce an integrated module to find optimal config for EPD system and minimize resource usage while maximizing SLO-based performance metric. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\times$ lesser for encoding-stage GPUs), that supports upto 22$\times$ higher batch sizes, 10$\times$ more number of images/ request, 2.2$\times$ higher kv cache size. Further, it leads to significant improvements in end-to-end throughput (up to 57\% better), and latency metrics (TTFT up to 71\% lower), compared to systems that do not disaggregate. Our findings underscore the potential of EPD disaggregation to enable resource-efficient and high-performance multimodal inference at scale. 

**Abstract (ZH)**: 大规模多模态模型（LMMs）通过处理多种输入如图像、音频和视频扩展了大规模语言模型（LLMs），但同时也增加了多模态编码阶段，导致计算和内存开销增加。这一过程有助于将原始输入转换为标记表示，增加了填充阶段的标记序列长度，从而负面影响了关键的服务级别目标（SLOs），如第一个标记的时间（TTFT）和端到端吞吐量。我们提出了一种新的框架——编码-填充-解码分层（EPD Disaggregation），该框架将编码、填充和解码阶段分配到专用资源中。与当前系统的捆绑式编码和填充不同，我们的分层方法缓解了内存瓶颈，减少了同步延迟，并支持灵活的批处理。具体来说，我们引入了一种新的缓存机制，以异步传输多模态标记，并引入了一个集成模块来为EPD系统找到最优配置，同时最大限度地减少资源使用并最大化基于SLO的性能指标。实验评估表明，与流行的LMMs系统相比，EPD分层方法在编码阶段的GPU内存效率上有了显著提高（最多降低了15倍），支持了高达22倍的更大批量处理，每个请求中的图像数量增加了10倍，关键值缓存大小增加了2.2倍。此外，它在端到端吞吐量（最多提高了57%）和延迟指标（TTFT最多降低了71%）方面也取得了显著改进，相比之下，不进行分层的系统则没有这种改进。我们的研究结果强调了EPD分层方法在大规模多模态推理中实现资源高效和高性能的潜力。 

---
# FOCUS: Towards Universal Foreground Segmentation 

**Title (ZH)**: FOCUS：面向通用前景分割的研究 

**Authors**: Zuyao You, Lingyu Kong, Lingchen Meng, Zuxuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.05238)  

**Abstract**: Foreground segmentation is a fundamental task in computer vision, encompassing various subdivision tasks. Previous research has typically designed task-specific architectures for each task, leading to a lack of unification. Moreover, they primarily focus on recognizing foreground objects without effectively distinguishing them from the background. In this paper, we emphasize the importance of the background and its relationship with the foreground. We introduce FOCUS, the Foreground ObjeCts Universal Segmentation framework that can handle multiple foreground tasks. We develop a multi-scale semantic network using the edge information of objects to enhance image features. To achieve boundary-aware segmentation, we propose a novel distillation method, integrating the contrastive learning strategy to refine the prediction mask in multi-modal feature space. We conduct extensive experiments on a total of 13 datasets across 5 tasks, and the results demonstrate that FOCUS consistently outperforms the state-of-the-art task-specific models on most metrics. 

**Abstract (ZH)**: 前景分割是计算机视觉中的一个基本任务，涵盖了各种细分任务。以往的研究通常为每个任务设计特定的任务架构，导致缺乏统一性。此外，它们主要集中在识别前景对象，而未能有效地将其与背景区分开。本文强调了背景及其与前景关系的重要性。我们引入了FOCUS，即通用前景对象分割框架，能够处理多重前景任务。我们利用对象边缘信息开发了一种多尺度语义网络，以增强图像特征。为实现边界感知分割，我们提出了一种新颖的蒸馏方法，结合对比学习策略在多模态特征空间中优化预测掩模。我们在5个任务的13个数据集上进行了广泛实验，结果表明，FOCUS在大多数指标上均优于最先进的特定任务模型。 

---
# Scalable Vision Language Model Training via High Quality Data Curation 

**Title (ZH)**: 通过高质量数据整理实现可扩展的视觉语言模型训练 

**Authors**: Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, Jiao Ran  

**Link**: [PDF](https://arxiv.org/pdf/2501.05952)  

**Abstract**: In this paper, we introduce SAIL-VL (ScAlable Vision Language Model TraIning via High QuaLity Data Curation), an open-source vision language model (VLM) of state-of-the-art (SOTA) performance with 2B parameters. We introduce three key improvements that contribute to SAIL-VL's leading performance: (1) Scalable high-quality visual understanding data construction: We implement a visual understanding data construction pipeline, which enables hundred-million-scale high-quality recaption data annotation. Equipped with this pipeline, we curate SAIL-Caption, a large-scale caption dataset with large quantity and the highest data quality compared with opensource caption datasets. (2) Scalable Pretraining with High-Quality Visual Understanding Data: We scale SAIL-VL's pretraining budget up to 131B tokens and show that even a 2B VLM benefits from scaled up training data sizes, exhibiting expected data size scaling laws in visual understanding and instruction following performance. (3) Scalable SFT via quantity and quality scaling: We introduce general guidance for instruction data curation to scale up instruction data continuously, allowing us to construct a large SFT dataset with the highest quality. To further improve SAIL-VL's performance, we propose quality scaling, a multi-stage training recipe with curriculum learning, to improve model performance scaling curves w.r.t. data sizes from logarithmic to be near-linear. SAIL-VL obtains the highest average score in 19 commonly used benchmarks in our evaluation and achieves top1 performance among VLMs of comparable sizes on OpenCompass (this https URL). We release our SAIL-VL-2B model at HuggingFace (this https URL). 

**Abstract (ZH)**: 在本文中，我们介绍了SAIL-VL（ScAlable Vision Language Model TraIning via High QuaLity Data Curation），这是一种开源的高性能视觉语言模型（VLM），参数量达到20亿。我们介绍了三项关键改进，这些改进共同推动了SAIL-VL的领先性能：（1）可扩展的高质量视觉理解数据构建：我们实现了一个视觉理解数据构建管道，能够生成超过亿级规模的高质量重描述数据标注。利用该管道，我们整理了SAIL-Caption这一大型数据集，其数据量大且数据质量超过开源重描述数据集。（2）基于高质量视觉理解数据的大规模预训练：我们将SAIL-VL的预训练预算扩展至1310亿个令牌，并证明即使参数量为20亿的模型也能从中受益，表现出随着数据规模增加，视觉理解和指令跟随性能的预期数据规模扩展规律。（3）通过数量和质量扩展的大规模精炼训练：我们提出了一种通用的指令数据整理指导原则，允许我们持续扩大指令数据集规模，从而构建高质量的大规模精炼训练数据集。为了进一步提升SAIL-VL的性能，我们提出了一种多层次的训练策略，结合了课程学习方法，使模型性能随数据规模增加的曲线从对数关系变为近似线性。SAIL-VL在我们的评估中获得了19个常用基准中最高的平均得分，并在与之参数量相近的其他VLM模型中表现最佳（这些信息可在以下链接查询：[请提供链接]）。我们已将SAIL-VL-2B模型发布在Hugging Face（请提供链接）。 

---
