{'arxiv_id': 'arXiv:2412.19312', 'title': 'From Interets to Insights: An LLM Approach to Course Recommendations Using Natural Language Queries', 'authors': 'Hugh Van Deventer, Mark Mills, August Evrard', 'link': 'https://arxiv.org/abs/2412.19312', 'abstract': "Most universities in the United States encourage their students to explore academic areas before declaring a major and to acquire academic breadth by satisfying a variety of requirements. Each term, students must choose among many thousands of offerings, spanning dozens of subject areas, a handful of courses to take. The curricular environment is also dynamic, and poor communication and search functions on campus can limit a student's ability to discover new courses of interest. To support both students and their advisers in such a setting, we explore a novel Large Language Model (LLM) course recommendation system that applies a Retrieval Augmented Generation (RAG) method to the corpus of course descriptions. The system first generates an 'ideal' course description based on the user's query. This description is converted into a search vector using embeddings, which is then used to find actual courses with similar content by comparing embedding similarities. We describe the method and assess the quality and fairness of some example prompts. Steps to deploy a pilot system on campus are discussed.", 'abstract_zh': '大多数美国大学鼓励学生在正式声明主修专业之前探索学术领域，并通过满足各种要求来获得学术广度。每一学期，学生都需要在数千门课程中选择，其中涵盖了几十个学科领域，需要挑选几门课程。课程环境也是动态的，校园内沟通不畅和搜索功能不足可能会限制学生发现新课程的兴趣。为了在这种环境中支持学生及其导师，我们探索了一种新型大规模语言模型（LLM）课程推荐系统，该系统采用检索增强生成（RAG）方法来处理课程描述的语料库。该系统首先根据用户的查询生成一个“理想”的课程描述。这一描述随后被转换成搜索向量，通过比较嵌入相似性来找到具有相似内容的实际情况中的课程。我们描述了该方法，并评估了部分示例提示的质量和公平性。还讨论了在校园中部署试点系统的步骤。', 'title_zh': '从兴趣到洞察：基于自然语言查询的大型语言模型在课程推荐中的应用'}
{'arxiv_id': 'arXiv:2412.19302', 'title': 'RecLM: Recommendation Instruction Tuning', 'authors': 'Yangqin Jiang, Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang', 'link': 'https://arxiv.org/abs/2412.19302', 'abstract': "Modern recommender systems aim to deeply understand users' complex preferences through their past interactions. While deep collaborative filtering approaches using Graph Neural Networks (GNNs) excel at capturing user-item relationships, their effectiveness is limited when handling sparse data or zero-shot scenarios, primarily due to constraints in ID-based embedding functions. To address these challenges, we propose a model-agnostic recommendation instruction-tuning paradigm that seamlessly integrates large language models with collaborative filtering. Our proposed Recommendation Language Model (RecLM) enhances the capture of user preference diversity through a carefully designed reinforcement learning reward function that facilitates self-augmentation of language models. Comprehensive evaluations demonstrate significant advantages of our approach across various settings, and its plug-and-play compatibility with state-of-the-art recommender systems results in notable performance enhancements.", 'abstract_zh': '现代推荐系统旨在通过用户的过去交互深入了解其复杂的偏好。虽然基于图神经网络（GNNs）的深度协同过滤方法在捕捉用户和项目之间的关系方面表现出色，但在处理稀疏数据或零样本场景时，其效果受限于基于ID嵌入函数的约束。为了解决这些挑战，我们提出了一种模型通用的推荐指令调优范式，将大规模语言模型与协同过滤无缝集成。我们提出的推荐语言模型（RecLM）通过精心设计的强化学习奖励函数来增强对用户偏好多样性的捕捉，该奖励函数促进了语言模型的自我增强。全面的评估结果表明，我们的方法在各种场景下具有显著优势，并且其即插即用兼容性与最先进的推荐系统相结合，带来了显著的性能提升。', 'title_zh': 'RecLM：推荐指令调优'}
{'arxiv_id': 'arXiv:2412.19265', 'title': 'Optimizing Multi-Stage Language Models for Effective Text Retrieval', 'authors': 'Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc', 'link': 'https://arxiv.org/abs/2412.19265', 'abstract': 'Efficient text retrieval is critical for applications such as legal document analysis, particularly in specialized contexts like Japanese legal systems. Existing retrieval methods often underperform in such domain-specific scenarios, necessitating tailored approaches. In this paper, we introduce a novel two-phase text retrieval pipeline optimized for Japanese legal datasets. Our method leverages advanced language models to achieve state-of-the-art performance, significantly improving retrieval efficiency and accuracy. To further enhance robustness and adaptability, we incorporate an ensemble model that integrates multiple retrieval strategies, resulting in superior outcomes across diverse tasks. Extensive experiments validate the effectiveness of our approach, demonstrating strong performance on both Japanese legal datasets and widely recognized benchmarks like MS-MARCO. Our work establishes new standards for text retrieval in domain-specific and general contexts, providing a comprehensive solution for addressing complex queries in legal and multilingual environments.', 'abstract_zh': '高效的文本检索对于法律文件分析等应用至关重要，特别是在如日本法律系统这类专门领域中尤为重要。现有的检索方法在这样的专门领域中往往表现不佳，因此需要有针对性的方法。在本文中，我们提出了一种优化的两阶段文本检索管道，专门适用于日本法律数据集。我们的方法利用先进的语言模型，实现了最先进的性能，显著提高了检索效率和准确性。为了进一步增强鲁棒性和适应性，我们引入了一个集成模型，整合了多种检索策略，从而在多任务中实现了更优的结果。广泛的实验验证了我们方法的有效性，在日本法律数据集和广泛认可的标准基准MS-MARCO上表现出强劲的性能。我们的工作为专门领域和通用领域中的文本检索设定了新的标准，提供了一套全面的解决方案，以应对法律和多语言环境中的复杂查询。', 'title_zh': '优化多阶段语言模型以实现有效的文本检索'}
{'arxiv_id': 'arXiv:2412.19172', 'title': 'Towards Popularity-Aware Recommendation: A Multi-Behavior Enhanced Framework with Orthogonality Constraint', 'authors': 'Yishan Han, Biao Xu, Yao Wang, Shanxing Gao', 'link': 'https://arxiv.org/abs/2412.19172', 'abstract': "Top-$K$ recommendation involves inferring latent user preferences and generating personalized recommendations accordingly, which is now ubiquitous in various decision systems. Nonetheless, recommender systems usually suffer from severe \\textit{popularity bias}, leading to the over-recommendation of popular items. Such a bias deviates from the central aim of reflecting user preference faithfully, compromising both customer satisfaction and retailer profits. Despite the prevalence, existing methods tackling popularity bias still have limitations due to the considerable accuracy-debias tradeoff and the sensitivity to extensive parameter selection, further exacerbated by the extreme sparsity in positive user-item interactions.\nIn this paper, we present a \\textbf{Pop}ularity-aware top-$K$ recommendation algorithm integrating multi-behavior \\textbf{S}ide \\textbf{I}nformation (PopSI), aiming to enhance recommendation accuracy and debias performance simultaneously. Specifically, by leveraging multiple user feedback that mirrors similar user preferences and formulating it as a three-dimensional tensor, PopSI can utilize all slices to capture the desiring user preferences effectively. Subsequently, we introduced a novel orthogonality constraint to refine the estimated item feature space, enforcing it to be invariant to item popularity features thereby addressing our model's sensitivity to popularity bias. Comprehensive experiments on real-world e-commerce datasets demonstrate the general improvements of PopSI over state-of-the-art debias methods with a marginal accuracy-debias tradeoff and scalability to practical applications. The source code for our algorithm and experiments is available at \\url{this https URL}.", 'abstract_zh': 'Top-$K$ 推荐涉及推断用户的潜在偏好并根据这些偏好生成个性化推荐，这在各种决策系统中已变得无处不在。然而，现有的推荐系统通常会遭受严重的“流行度偏差”，导致过于推荐流行项目。这种偏差背离了忠实反映用户偏好的中心目标，损害了客户的满意度和零售商的利润。尽管这种偏差普遍存在，但现有的缓解流行度偏差的方法仍然受到高精度与偏差校正之间的权衡以及参数选择高度敏感性的限制，进一步被积极的用户-项目交互极端稀疏性所加剧。\n\n本文提出了一种结合多行为补充信息（PopSI）的“流行度”感知Top-$K$ 推荐算法，旨在同时提升推荐准确性和偏倚校正性能。具体而言，通过利用反映相似用户偏好的多种用户反馈并将其形式化为三维张量，PopSI 可以利用所有切片来有效捕捉用户的期望偏好。随后，我们引入了一种新颖的正交约束来细化估算的项目特征空间，确保其相对于项目流行度特征保持不变，从而解决了模型对流行度偏差的高度敏感性问题。在真实世界电商数据集上的全面实验表明，与最先进的偏倚校正方法相比，PopSI 在精度与偏倚校正之间的权衡边际增加较少，并且适用于实际应用。我们的算法和实验代码可在 \\url{此链接} 获取。', 'title_zh': '面向流行性感知的推荐：一种具有正交约束的多行为增强框架'}
{'arxiv_id': 'arXiv:2412.19048', 'title': 'Jasper and Stella: distillation of SOTA embedding models', 'authors': 'Dun Zhang, FulongWang', 'link': 'https://arxiv.org/abs/2412.19048', 'abstract': 'A crucial component of many deep learning applications (such as FAQ and RAG) is dense retrieval, in which embedding models are used to convert raw text to numerical vectors and then get the most similar text by MIPS (Maximum Inner Product Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and AIR-Bench) have been established to evaluate embedding models accurately. Thanks to these benchmarks, we can use SOTA models; however, the deployment and application of these models in industry were hampered by their large vector dimensions and numerous parameters. To alleviate this problem, 1) we present a distillation technique that can enable a smaller student model to achieve good performance. 2) Inspired by MRL we present a training approach of reducing the vector dimensions based on its own vectors or its teacher vectors. 3) We do simple yet effective alignment training between images and text to make our model a multimodal encoder. We trained Stella and Jasper models using the technologies above and achieved high scores on the MTEB leaderboard. We release the model and data at Hugging Face Hub (this https URL) and the training logs are at this https URL.', 'abstract_zh': '许多深度学习应用（如FAQ和RAG）的关键组成部分是密集检索，其中嵌入模型被用于将原始文本转换为数值向量，然后通过MIPS（最大内积搜索）找到最相似的文本。一些文本嵌入基准（例如MTEB、BEIR和AIR-Bench）已经建立起来，用于准确评估嵌入模型。得益于这些基准，我们能够使用最先进的模型；然而，由于这些模型具有大量的向量维度和参数，它们在工业中的部署和应用受到了限制。为了解决这一问题，1）我们提出了一种蒸馏技术，可以使得一个较小的学生模型也能获得良好的性能。2）借鉴MRL的方法，我们提出了一种基于自身向量或教师向量减少向量维度的训练方法。3）我们进行了简单的多模态对齐训练，将图像与文本对齐，使我们的模型成为一个多模态编码器。我们使用上述技术训练了Stella和Jasper模型，并在MTEB排行榜上取得了高分。我们已在Hugging Face Hub（这个链接）发布了该模型和数据，并且训练日志位于这个链接。', 'title_zh': '《Jasper和Stella：领先嵌入模型的精简》\n\n在这里，“Jasper和Stella”被保留了原名，因为它们可能是指特定的模型或系统名称。如果需要进一步解释或提供更多背景信息，请告知我。'}
{'arxiv_id': 'arXiv:2412.18962', 'title': "Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing Node-neighbor Discrepancy in Graph Convolutional Network", 'authors': 'Zheyu Chen, Jinfeng Xu, Haibo Hu', 'link': 'https://arxiv.org/abs/2412.18962', 'abstract': 'The rapid expansion of multimedia contents has led to the emergence of multimodal recommendation systems. It has attracted increasing attention in recommendation systems because its full utilization of data from different modalities alleviates the persistent data sparsity problem. As such, multimodal recommendation models can learn personalized information about nodes in terms of visual and textual. To further alleviate the data sparsity problem, some previous works have introduced graph convolutional networks (GCNs) for multimodal recommendation systems, to enhance the semantic representation of users and items by capturing the potential relationships between them. However, adopting GCNs inevitably introduces the over-smoothing problem, which make nodes to be too similar. Unfortunately, incorporating multimodal information will exacerbate this challenge because nodes that are too similar will lose the personalized information learned through multimodal information. To address this problem, we propose a novel model that retains the personalized information of ego nodes during feature aggregation by Reducing Node-neighbor Discrepancy (RedN^nD). Extensive experiments on three public datasets show that RedN^nD achieves state-of-the-art performance on accuracy and robustness, with significant improvements over existing GCN-based multimodal frameworks.', 'abstract_zh': '多媒体内容的迅猛增长导致了多模态推荐系统的出现。由于充分利用了不同模态的数据，这些系统在缓解持续存在的数据稀疏性问题方面引起了越来越多的关注。因此，多模态推荐模型可以从视觉和文本两个方面学习个体的信息。为了进一步缓解数据稀疏性问题，一些先前的工作引入了图卷积网络（GCNs），通过捕捉用户和项目之间潜在的关系来增强语义表示。然而，采用GCNs不可避免地引入了过度平滑问题，使节点变得过于相似。不幸的是，结合多模态信息会加剧这一挑战，因为过于相似的节点会失去通过多模态信息学习到的个性化信息。为了解决这个问题，我们提出了一种新型模型，通过减少节点邻居差异（RedN^nD）在特征聚合过程中保留ego节点的个性化信息。在三个公开数据集上的广泛实验表明，RedN^nD在准确性和鲁棒性方面达到了现有基于GCN的多模态框架的最佳性能，并取得了显著改进。', 'title_zh': '不要迷失自我：通过减少图卷积网络中节点-邻节点差异来提升多模态推荐'}
{'arxiv_id': 'arXiv:2412.18956', 'title': 'Musings About the Future of Search: A Return to the Past?', 'authors': 'Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne', 'link': 'https://arxiv.org/abs/2412.18956', 'abstract': "When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.", 'abstract_zh': '当你有疑问时，最有效的方法是直接与该领域的专家进行交流并与其对话。在文字发明之前，这是唯一的途径。尽管这种方法有效，但它存在扩展性挑战。文字使得知识得以物化、保存和复制，促进了几个世纪以来技术的进步，将寻求信息的人与相关信息连接起来。这一进程最终导致了我们熟知的十蓝链接的搜索引擎模式，直到近期生成式AI的出现。然而，我们往往忽视了一点，即消费静态内容是一种不完美的解决方案。随着大规模语言模型的发展，通过让用户直接与专家进行交互，可以开发出更加优越的体验。这些交互不仅能满足信息需求，而且专家模型还能发挥更大的作用。这一即将到来的未来需要重新构想搜索方式。', 'title_zh': '关于搜索未来的思考：回归过去吗？'}
{'arxiv_id': 'arXiv:2412.18770', 'title': 'Attack-in-the-Chain: Bootstrapping Large Language Models for Attacks Against Black-box Neural Ranking Models', 'authors': 'Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2412.18770', 'abstract': 'Neural ranking models (NRMs) have been shown to be highly effective in terms of retrieval performance. Unfortunately, they have also displayed a higher degree of sensitivity to attacks than previous generation models. To help expose and address this lack of robustness, we introduce a novel ranking attack framework named Attack-in-the-Chain, which tracks interactions between large language models (LLMs) and NRMs based on chain-of-thought (CoT) prompting to generate adversarial examples under black-box settings. Our approach starts by identifying anchor documents with higher ranking positions than the target document as nodes in the reasoning chain. We then dynamically assign the number of perturbation words to each node and prompt LLMs to execute attacks. Finally, we verify the attack performance of all nodes at each reasoning step and proceed to generate the next reasoning step. Empirical results on two web search benchmarks show the effectiveness of our method.', 'abstract_zh': '神经排序模型（NRMs）在检索性能方面已被证明非常有效。然而，它们对攻击的敏感性也较前一代模型更高。为了解决这一鲁棒性不足的问题，我们提出了一种名为“链中攻击”（Attack-in-the-Chain）的新型攻击框架，该框架基于链式思维（CoT）提示跟踪大规模语言模型（LLMs）与NRMs之间的交互，以生成对抗样本。我们的方法首先识别出排名高于目标文档的锚文档，并将其作为推理链中的节点。然后，动态分配每个节点的扰动词数量，并提示LLMs执行攻击。最后，我们在每一步推理中验证所有节点的攻击性能，并继续生成下一步推理。在两个网页搜索基准数据集上的实证结果表明了我们方法的有效性。', 'title_zh': '链式攻击：用于黑盒神经排序模型攻击的大型语言模型的自我提升攻击方法'}
{'arxiv_id': 'arXiv:2412.18768', 'title': 'On the Robustness of Generative Information Retrieval Models', 'authors': 'Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Changjiang Zhou, Maarten de Rijke, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2412.18768', 'abstract': 'Generative information retrieval methods retrieve documents by directly generating their identifiers. Much effort has been devoted to developing effective generative IR models. Less attention has been paid to the robustness of these models. It is critical to assess the out-of-distribution (OOD) generalization of generative IR models, i.e., how would such models generalize to new distributions? To answer this question, we focus on OOD scenarios from four perspectives in retrieval problems: (i)query variations; (ii)unseen query types; (iii)unseen tasks; and (iv)corpus expansion. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of representative generative IR models against dense retrieval models. Our empirical results indicate that the OOD robustness of generative IR models is in need of improvement. By inspecting the OOD robustness of generative IR models we aim to contribute to the development of more reliable IR models. The code is available at \\url{this https URL}.', 'abstract_zh': '生成式信息检索方法通过直接生成文档标识符来检索文档。人们投入了大量努力来开发有效的生成式信息检索模型。但在这些模型的鲁棒性方面关注较少。评估生成式信息检索模型的分布外（OOD）泛化能力至关重要，即这些模型在面对新分布时会如何表现？为回答这一问题，我们从检索问题的四个角度关注分布外（OOD）场景：（i）查询变化；（ii）未见过的查询类型；（iii）未见过的任务；以及（iv）语料库扩展。基于这一分类体系，我们进行了实证研究，分析代表性生成式信息检索模型相对于密集检索模型的分布外鲁棒性。我们的实证结果显示，生成式信息检索模型的分布外鲁棒性需要改进。通过分析生成式信息检索模型的分布外鲁棒性，我们旨在促进对更可靠的信息检索模型的发展。代码可以在 \\url{此处填写URL} 获得。', 'title_zh': '生成式信息检索模型的稳健性研究'}
{'arxiv_id': 'arXiv:2412.18735', 'title': 'Adaptive Self-supervised Learning for Social Recommendations', 'authors': 'Xin He, Shanru Lin, Wenqi Fan, Mingchen Sun, Ying Wang, Xin Wang', 'link': 'https://arxiv.org/abs/2412.18735', 'abstract': 'In recent years, researchers have attempted to exploit social relations to improve the performance in recommendation systems. Generally, most existing social recommendation methods heavily depends on substantial domain knowledge and expertise in primary recommendation tasks for designing useful auxiliary tasks. Meanwhile, Self-Supervised Learning (SSL) recently has received considerable attention in the field of recommendation, since it can provide self-supervision signals in assisting the improvement of target recommendation systems by constructing self-supervised auxiliary tasks from raw data without human-annotated labels. Despite the great success, these SSL-based social recommendations are insufficient to adaptively balance various self-supervised auxiliary tasks, since assigning equal weights on various auxiliary tasks can result in sub-optimal recommendation performance, where different self-supervised auxiliary tasks may contribute differently to improving the primary social recommendation across different datasets. To address this issue, in this work, we propose Adaptive Self-supervised Learning for Social Recommendations (AdasRec) by taking advantage of various self-supervised auxiliary tasks. More specifically, an adaptive weighting mechanism is proposed to learn adaptive weights for various self-supervised auxiliary tasks, so as to balance the contribution of such self-supervised auxiliary tasks for enhancing representation learning in social recommendations. The adaptive weighting mechanism is used to assign different weights on auxiliary tasks to achieve an overall weighting of the entire auxiliary tasks and ultimately assist the primary recommendation task, achieved by a meta learning optimization problem with an adaptive weighting network. Comprehensive experiments on various real-world datasets are constructed to verify the effectiveness of our proposed method.', 'abstract_zh': '近年来，研究人员试图利用社交关系来提高推荐系统的性能。通常，现有的大多数社交推荐方法高度依赖于主要推荐任务中的领域知识和专业知识来设计有用的支持任务。与此同时，自监督学习（SSL）在推荐领域的应用近年来受到了广泛关注，因为它可以通过从原始数据中构造自监督辅助任务来提供自监督信号，从而在不需要人工标注标签的情况下辅助目标推荐系统的改进。尽管取得了巨大的成功，但基于SSL的社交推荐方法在适应性地平衡各种自监督辅助任务方面仍然不足，因为将不同辅助任务分配相同的权重可能导致推荐性能不佳，不同的自监督辅助任务在不同数据集上对提升主要社交推荐的贡献可能不同。为了解决这一问题，本研究通过利用各种自监督辅助任务的优势，提出了一种自适应的自监督学习方法，用于社交推荐（AdasRec）。具体来说，提出了一种自适应加权机制来学习各种自监督辅助任务的不同权重，以便平衡这些自监督辅助任务对提高社交推荐中表示学习的贡献。自适应加权机制用于为辅助任务分配不同权重，以实现整个辅助任务的综合权重，并最终帮助主要推荐任务，这一过程通过具有自适应加权网络的元学习优化问题来实现。通过在多种实际数据集上进行全面实验，验证了我们所提出方法的有效性。', 'title_zh': '适应性自监督学习在社会推荐中的应用'}
{'arxiv_id': 'arXiv:2412.18731', 'title': 'Position-aware Graph Transformer for Recommendation', 'authors': 'Jiajia Chen, Jiancan Wu, Jiawei Chen, Chongming Gao, Yong Li, Xiang Wang', 'link': 'https://arxiv.org/abs/2412.18731', 'abstract': 'Collaborative recommendation fundamentally involves learning high-quality user and item representations from interaction data. Recently, graph convolution networks (GCNs) have advanced the field by utilizing high-order connectivity patterns in interaction graphs, as evidenced by state-of-the-art methods like PinSage and LightGCN. However, one key limitation has not been well addressed in existing solutions: capturing long-range collaborative filtering signals, which are crucial for modeling user preference. In this work, we propose a new graph transformer (GT) framework -- \\textit{Position-aware Graph Transformer for Recommendation} (PGTR), which combines the global modeling capability of Transformer blocks with the local neighborhood feature extraction of GCNs. The key insight is to explicitly incorporate node position and structure information from the user-item interaction graph into GT architecture via several purpose-designed positional encodings. The long-range collaborative signals from the Transformer block are then combined linearly with the local neighborhood features from the GCN backbone to enhance node embeddings for final recommendations. Empirical studies demonstrate the effectiveness of the proposed PGTR method when implemented on various GCN-based backbones across four real-world datasets, and the robustness against interaction sparsity as well as noise.', 'abstract_zh': '协作推荐本质上涉及从交互数据中学习高质量的用户和项目表示。近年来，图卷积网络（GCNs）通过在交互图中利用高阶连接模式，极大地推动了该领域的发展，如PinSage和LightGCN等先进方法即是例证。然而，现有的解决方案中尚未很好地解决一个关键问题：捕捉长程协作过滤信号，这对建模用户偏好至关重要。在本文中，我们提出了一种新的图变换器（GT）框架——“基于位置感知的图变换器推荐”（PGTR），该框架结合了Transformers块的全局建模能力和GCNs的局部邻域特征提取能力。关键思路是通过几种专门设计的位置编码，显式地将用户-项目交互图中的节点位置和结构信息融入到变换器架构中。变换器块中的长程协作信号随后通过线性组合与GCN主干中的局部邻域特征相结合，以增强节点嵌入，最终提高推荐质量。实验研究表明，该提出的PGTR方法在四个实际数据集的各种GCN基背景下均显示出有效性，并且能够有效应对交互稀疏性和噪声等问题。', 'title_zh': '基于位置的图Transformer推荐方法'}
{'arxiv_id': 'arXiv:2412.19200', 'title': 'Personalized Dynamic Music Emotion Recognition with Dual-Scale Attention-Based Meta-Learning', 'authors': 'Dengming Zhang, Weitao You, Ziheng Liu, Lingyun Sun, Pei Chen', 'link': 'https://arxiv.org/abs/2412.19200', 'abstract': 'Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of different moments in music, playing a crucial role in music information retrieval. The existing DMER methods struggle to capture long-term dependencies when dealing with sequence data, which limits their performance. Furthermore, these methods often overlook the influence of individual differences on emotion perception, even though everyone has their own personalized emotional perception in the real world. Motivated by these issues, we explore more effective sequence processing methods and introduce the Personalized DMER (PDMER) problem, which requires models to predict emotions that align with personalized perception. Specifically, we propose a Dual-Scale Attention-Based Meta-Learning (DSAML) method. This method fuses features from a dual-scale feature extractor and captures both short and long-term dependencies using a dual-scale attention transformer, improving the performance in traditional DMER. To achieve PDMER, we design a novel task construction strategy that divides tasks by annotators. Samples in a task are annotated by the same annotator, ensuring consistent perception. Leveraging this strategy alongside meta-learning, DSAML can predict personalized perception of emotions with just one personalized annotation sample. Our objective and subjective experiments demonstrate that our method can achieve state-of-the-art performance in both traditional DMER and PDMER.', 'abstract_zh': '动态音乐情感识别（Dynamic Music Emotion Recognition, DMER）旨在预测音乐中不同时刻的情感，对于音乐信息检索具有重要意义。现有的DMER方法在处理序列数据时难以捕捉长期依赖关系，这限制了它们的性能。此外，这些方法通常忽略个体差异对情感感知的影响，尽管每个人在现实世界中有自己个性化的情感感知。针对这些问题，我们探索了更有效的序列处理方法，并引入了个性化DMER（Personalized DMER, PDMER）问题，要求模型预测与个性化感知相契合的情感。具体来说，我们提出了一个双尺度注意机制元学习（Dual-Scale Attention-Based Meta-Learning, DSAML）方法。该方法结合了双尺度特征提取器的特征，并通过双尺度注意变换器捕获短期和长期依赖关系，从而在传统DMER中提高性能。为实现PDMER，我们设计了一种新型的任务构造策略，通过注释者进行任务划分。任务中的样本由同一个注释者进行标注，确保感知一致性。利用这种策略并结合元学习，DSAML仅需一个个性化的标注样本即可预测个性化的情感感知。我们的客观和主观实验表明，该方法在传统DMER和PDMER中均能达到最先进的性能。', 'title_zh': '基于双尺度注意力元学习的个性化动态音乐情绪识别'}
{'arxiv_id': 'arXiv:2412.19178', 'title': 'Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval', 'authors': 'Yang Du, Yuqi Liu, Qin Jin', 'link': 'https://arxiv.org/abs/2412.19178', 'abstract': 'Cross-modal (e.g. image-text, video-text) retrieval is an important task in information retrieval and multimodal vision-language understanding field. Temporal understanding makes video-text retrieval more challenging than image-text retrieval. However, we find that the widely used video-text benchmarks have shortcomings in comprehensively assessing abilities of models, especially in temporal understanding, causing large-scale image-text pre-trained models can already achieve comparable zero-shot performance with video-text pre-trained models. In this paper, we introduce RTime, a novel temporal-emphasized video-text retrieval dataset. We first obtain videos of actions or events with significant temporality, and then reverse these videos to create harder negative samples. We then recruit annotators to judge the significance and reversibility of candidate videos, and write captions for qualified videos. We further adopt GPT-4 to extend more captions based on human-written captions. Our RTime dataset currently consists of 21k videos with 10 captions per video, totalling about 122 hours. Based on RTime, we propose three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We further enhance the use of harder-negatives in model training, and benchmark a variety of video-text models on RTime. Extensive experiment analysis proves that RTime indeed poses new and higher challenges to video-text retrieval. We release our RTime dataset\\footnote{\\url{this https URL}} to further advance video-text retrieval and multimodal understanding research.', 'abstract_zh': '在信息检索和多模态视觉语言理解领域，跨模态检索（如图像-文本、视频-文本）是一个重要的任务。时间理解使得视频-文本检索相比图像-文本检索更加具有挑战性。然而，我们发现广泛使用的视频-文本基准数据集在全面评估模型能力方面存在不足，尤其是在时间理解方面，导致大规模预训练的图像-文本模型已经在零样本性能上与视频-文本预训练模型达到了可比的水平。本文中，我们提出了RTime，一个新颖的时间重点视频-文本检索数据集。我们首先获取具有良好时间性的动作或事件视频，并将这些视频反转以创造更难的负样本。然后，我们招募标注者评价候选视频的显著性和可逆性，并为合格的视频撰写描述。在此基础上，我们使用GPT-4扩展了更多的人写描述。目前，我们的RTime数据集包含21,000个视频，每个视频有10个描述，总计约122小时。基于RTime，我们提出了三种检索基准任务：RTime-Origin、RTime-Hard 和 RTime-Binary。我们进一步增强了模型训练中使用更难负样本的方法，并在RTime上对多种视频-文本模型进行了基准测试。广泛的实验证明，RTime确实为视频-文本检索提出了新的和更高的挑战。我们发布了我们的RTime数据集\\footnote{\\url{this https URL}}，以进一步推动视频-文本检索和多模态理解研究。', 'title_zh': '反向时间：一种新的强调时间的跨模态视频-文本检索基准'}
{'arxiv_id': 'arXiv:2412.19069', 'title': 'Effective and secure federated online learning to rank', 'authors': 'Shuyi Wang', 'link': 'https://arxiv.org/abs/2412.19069', 'abstract': 'Online Learning to Rank (OLTR) optimises ranking models using implicit user feedback, such as clicks. Unlike traditional Learning to Rank (LTR) methods that rely on a static set of training data with relevance judgements to learn a ranking model, OLTR methods update the model continually as new data arrives. Thus, it addresses several drawbacks such as the high cost of human annotations, potential misalignment between user preferences and human judgments, and the rapid changes in user query intents. However, OLTR methods typically require the collection of searchable data, user queries, and clicks, which poses privacy concerns for users.\nFederated Online Learning to Rank (FOLTR) integrates OLTR within a Federated Learning (FL) framework to enhance privacy by not sharing raw data. While promising, FOLTR methods currently lag behind traditional centralised OLTR due to challenges in ranking effectiveness, robustness with respect to data distribution across clients, susceptibility to attacks, and the ability to unlearn client interactions and data. This thesis presents a comprehensive study on Federated Online Learning to Rank, addressing its effectiveness, robustness, security, and unlearning capabilities, thereby expanding the landscape of FOLTR.', 'abstract_zh': '在线学习排序（Online Learning to Rank, OLTR）使用隐式的用户反馈，例如点击，来优化排名模型。与传统的学习排序（Learning to Rank, LTR）方法不同，这些传统方法依赖静态的数据集和相关性判断来学习一个排名模型，OLTR方法能够随着新数据的不断到来持续更新模型。因此，它解决了人工注释成本高、用户偏好与人为判断可能不一致以及用户查询意图快速变化等问题。然而，OLTR方法通常需要收集可搜索的数据、用户查询和点击，这给用户隐私带来了担忧。\n\n联邦在线学习排序（Federated Online Learning to Rank, FOLTR）通过将OLTR整合到联邦学习（Federated Learning, FL）框架中，增强隐私保护，无需共享原始数据。尽管FOLTR具有潜力，但目前在排名效果、对客户端数据分布的鲁棒性、易受攻击性以及用户交互和数据的不可学习能力方面，仍然落后于传统的中央式OLTR方法。本论文对联邦在线学习排序进行了全面研究，探讨了其有效性、鲁棒性、安全性以及不可学习能力，从而扩展了FOLTR的研究领域。', 'title_zh': '有效的安全联邦在线学习排序'}
{'arxiv_id': 'arXiv:2412.18860', 'title': 'Bootstrap Your Own Context Length', 'authors': 'Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei', 'link': 'https://arxiv.org/abs/2412.18860', 'abstract': 'We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.', 'abstract_zh': '我们提出了一种通过利用语言模型的短语境能力来训练长语境语言模型的自举方法。该方法采用一个简单的代理工作流来合成多样化的长语境指令调优数据，从而消除手动数据收集和标注的必要性。所提出的合成数据工作流只需要一个短语境语言模型、一个文本检索器和一个文档集合，这些资源在开源生态系统中均易于获取。随后，使用合成数据对语言模型进行微调，以延长其语境长度。通过这种方式，我们有效地通过自举过程将语言模型的短语境能力转移到长语境场景中。我们使用开源的Llama-3家族模型进行了实验，并证明我们的方法能够成功将语境长度扩展到多达100万词，并在各种基准测试中取得了优越的性能。', 'title_zh': '自我训练上下文长度'}
{'arxiv_id': 'arXiv:2412.18819', 'title': 'LLM-assisted vector similarity search', 'authors': 'Md Riyadh, Muqi Li, Felix Haryanto Lie, Jia Long Loh, Haotian Mi, Sayam Bohra', 'link': 'https://arxiv.org/abs/2412.18819', 'abstract': 'As data retrieval demands become increasingly complex, traditional search methods often fall short in addressing nuanced and conceptual queries. Vector similarity search has emerged as a promising technique for finding semantically similar information efficiently. However, its effectiveness diminishes when handling intricate queries with contextual nuances. This paper explores a hybrid approach combining vector similarity search with Large Language Models (LLMs) to enhance search accuracy and relevance. The proposed two-step solution first employs vector similarity search to shortlist potential matches, followed by an LLM for context-aware ranking of the results. Experiments on structured datasets demonstrate that while vector similarity search alone performs well for straightforward queries, the LLM-assisted approach excels in processing complex queries involving constraints, negations, or conceptual requirements. By leveraging the natural language understanding capabilities of LLMs, this method improves the accuracy of search results for complex tasks without sacrificing efficiency. We also discuss real-world applications and propose directions for future research to refine and scale this technique for diverse datasets and use cases.\nOriginal article: this https URL', 'abstract_zh': '随着数据检索需求变得越来越复杂，传统检索方法在应对精致和概念性查询时常常力不从心。向量相似度检索已作为一种有效方法，能够在高效地找出语义相似的信息方面大放异彩。然而，当处理带有上下文细微差别的复杂查询时，其效果会有所减弱。本文探讨了一种结合向量相似度检索和大规模语言模型（Large Language Models, LLMs）的混合方法，以提高检索的准确性和相关性。所提出的一种两步解决方案首先利用向量相似度检索来筛选潜在匹配项，随后使用LLM对结果进行上下文感知的排序。实验表明，在结构化数据集上，仅使用向量相似度检索对于简单查询具有良好的性能，而LLM辅助的方法在处理涉及约束、否定或概念性要求的复杂查询方面表现出色。通过利用LLMs的语言理解能力，该方法能够在不牺牲效率的同时，提高复杂任务的检索结果准确性。此外，本文还讨论了实际应用场景，并提出了未来研究的方向，以细化并扩大此技术的应用范围，适用于多样化的数据集和应用场景。\n\n原始文献：this https URL', 'title_zh': 'LLM辅助向量相似性搜索'}
{'arxiv_id': 'arXiv:2412.18806', 'title': 'FOR: Finetuning for Object Level Open Vocabulary Image Retrieval', 'authors': 'Hila Levi, Guy Heller, Dan Levi', 'link': 'https://arxiv.org/abs/2412.18806', 'abstract': 'As working with large datasets becomes standard, the task of accurately retrieving images containing objects of interest by an open set textual query gains practical importance. The current leading approach utilizes a pre-trained CLIP model without any adaptation to the target domain, balancing accuracy and efficiency through additional post-processing. In this work, we propose FOR: Finetuning for Object-centric Open-vocabulary Image Retrieval, which allows finetuning on a target dataset using closed-set labels while keeping the visual-language association crucial for open vocabulary retrieval. FOR is based on two design elements: a specialized decoder variant of the CLIP head customized for the intended task, and its coupling within a multi-objective training framework. Together, these design choices result in a significant increase in accuracy, showcasing improvements of up to 8 mAP@50 points over SoTA across three datasets. Additionally, we demonstrate that FOR is also effective in a semi-supervised setting, achieving impressive results even when only a small portion of the dataset is labeled.', 'abstract_zh': '随着处理大规模数据集成为常态，使用开放词汇集的文字查询准确检索包含感兴趣对象的图像的任务获得了实际意义。当前领先的解决方案是在目标域上利用预训练的CLIP模型，通过额外的后处理来平衡准确性和效率。在此项工作中，我们提出了一种称为FOR的方法：面向对象的开放词汇图像检索的微调，该方法允许使用封闭集标签对目标数据集进行微调，同时保持对于开放词汇检索至关重要的视觉-语言关联。FOR基于两个设计元素：专为指定任务定制的CLIP头的特殊解码器变体，以及将其嵌入多目标训练框架。这些设计选择共同导致了显著的准确率提升，在三个数据集上展示了最高达8个mAP@50点的改进，从而超越了现有最佳方法。此外，我们还证明了FOR在半监督设置中也具有有效性，即使只有少量数据集被标注，其结果也非常令人印象深刻。', 'title_zh': 'FOR：对象级开放词汇图像检索的微调方法'}
{'arxiv_id': 'arXiv:2412.18784', 'title': 'Zema Dataset: A Comprehensive Study of Yaredawi Zema with a Focus on Horologium Chants', 'authors': 'Mequanent Argaw Muluneh, Yan-Tsung Peng, Worku Abebe Degife, Nigussie Abate Tadesse, Aknachew Mebreku Demeku, Li Su', 'link': 'https://arxiv.org/abs/2412.18784', 'abstract': 'Computational music research plays a critical role in advancing music production, distribution, and understanding across various musical styles worldwide. Despite the immense cultural and religious significance, the Ethiopian Orthodox Tewahedo Church (EOTC) chants are relatively underrepresented in computational music research. This paper contributes to this field by introducing a new dataset specifically tailored for analyzing EOTC chants, also known as Yaredawi Zema. This work provides a comprehensive overview of a 10-hour dataset, 369 instances, creation, and curation process, including rigorous quality assurance measures. Our dataset has a detailed word-level temporal boundary and reading tone annotation along with the corresponding chanting mode label of audios. Moreover, we have also identified the chanting options associated with multiple chanting notations in the manuscript by annotating them accordingly. Our goal in making this dataset available to the public 1 is to encourage more research and study of EOTC chants, including lyrics transcription, lyric-to-audio alignment, and music generation tasks. Such research work will advance knowledge and efforts to preserve this distinctive liturgical music, a priceless cultural artifact for the Ethiopian people.', 'abstract_zh': '计算音乐研究在推动世界各地各种音乐风格的音乐制作、分发和理解方面发挥着关键作用。尽管埃塞俄比亚正统提华多教会（EOTC）圣歌具有巨大的文化和宗教意义，但在计算音乐研究中，它们仍然相对未受到广泛关注。本文通过引入一个专门为分析EOTC圣歌（即亚利德瓦伊赞玛）定制的新数据集，为这一领域做出了贡献。本文提供了该10小时数据集的全面概述，包括369个实例，创建和编纂过程，以及严格的质量保证措施。我们的数据集包含单词级别的时间边界和读音音调标注，以及相应的唱诵模式标签。此外，我们还通过相应标注明确了手稿中多种唱诵符号所对应的唱诵选项。我们旨在将这一数据集提供给公众使用，以鼓励更多关于EOTC圣歌的研究和学习，包括歌词转写、歌词与音频对齐以及音乐生成任务等方面的工作。此类研究将进一步深化对这种独特的礼拜音乐的认知与保护，使其成为埃塞俄比亚人民宝贵的文化遗产。', 'title_zh': '札玛数据集：雅ry达瓦札玛的综合研究——以Horologium颂歌为重点'}
{'arxiv_id': 'arXiv:2412.18715', 'title': 'Optimization and Scalability of Collaborative Filtering Algorithms in Large Language Models', 'authors': 'Haowei Yang, Longfei Yun, Jinghan Cao, Qingyi Lu, Yuming Tu', 'link': 'https://arxiv.org/abs/2412.18715', 'abstract': 'With the rapid development of large language models (LLMs) and the growing demand for personalized content, recommendation systems have become critical in enhancing user experience and driving engagement. Collaborative filtering algorithms, being core to many recommendation systems, have garnered significant attention for their efficiency and interpretability. However, traditional collaborative filtering approaches face numerous challenges when integrated into large-scale LLM-based systems, including high computational costs, severe data sparsity, cold start problems, and lack of scalability. This paper investigates the optimization and scalability of collaborative filtering algorithms in large language models, addressing these limitations through advanced optimization strategies. Firstly, we analyze the fundamental principles of collaborative filtering algorithms and their limitations when applied in LLM-based contexts. Next, several optimization techniques such as matrix factorization, approximate nearest neighbor search, and parallel computing are proposed to enhance computational efficiency and model accuracy. Additionally, strategies such as distributed architecture and model compression are explored to facilitate dynamic updates and scalability in data-intensive environments.', 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展以及个性化内容需求的日益增长，推荐系统在提升用户体验和促进互动方面变得尤为重要。作为许多推荐系统的核心，协同过滤算法因其高效性和可解释性而受到广泛关注。然而，传统的协同过滤方法在集成到基于LLM的大规模系统中时面临诸多挑战，包括高计算成本、严重的数据稀疏性、冷启动问题以及缺乏扩展性。本文探讨了在大规模语言模型中优化和提升协同过滤算法的方法，通过先进的优化策略来解决这些问题。首先，我们分析了协同过滤算法的基本原理及其在LLM背景下的局限性。接着，提出了几种优化技术，如矩阵分解、近似最近邻搜索和并行计算，以提高计算效率和模型准确性。此外，我们还研究了分布式架构和模型压缩等策略，以促进数据密集环境中动态更新与扩展。', 'title_zh': '大型语言模型中协作过滤算法的优化与可扩展性研究'}
{'arxiv_id': 'arXiv:2412.18713', 'title': 'Enhanced Recommendation Combining Collaborative Filtering and Large Language Models', 'authors': 'Xueting Lin, Zhan Cheng, Longfei Yun, Qingyi Lu, Yuanshuai Luo', 'link': 'https://arxiv.org/abs/2412.18713', 'abstract': "With the advent of the information explosion era, the importance of recommendation systems in various applications is increasingly significant. Traditional collaborative filtering algorithms are widely used due to their effectiveness in capturing user behavior patterns, but they encounter limitations when dealing with cold start problems and data sparsity. Large Language Models (LLMs), with their strong natural language understanding and generation capabilities, provide a new breakthrough for recommendation systems. This study proposes an enhanced recommendation method that combines collaborative filtering and LLMs, aiming to leverage collaborative filtering's advantage in modeling user preferences while enhancing the understanding of textual information about users and items through LLMs to improve recommendation accuracy and diversity. This paper first introduces the fundamental theories of collaborative filtering and LLMs, then designs a recommendation system architecture that integrates both, and validates the system's effectiveness through experiments. The results show that the hybrid model based on collaborative filtering and LLMs significantly improves precision, recall, and user satisfaction, demonstrating its potential in complex recommendation scenarios.", 'abstract_zh': '随着信息爆炸时代的到来，推荐系统在各种应用中的重要性愈加显著。传统协作过滤算法因其在捕捉用户行为模式方面的有效性而被广泛应用，但在处理冷启动问题和数据稀疏性方面遇到了限制。大规模语言模型（LLMs）凭借其强大的自然语言理解和生成能力，为推荐系统提供了新的突破。本研究提出了一种结合协作过滤和LLMs的增强推荐方法，旨在利用协作过滤在建模用户偏好方面的优势，同时通过LLMs增强对用户和项目文本信息的理解，以提高推荐的准确性和多样性。本文首先介绍了协作过滤和LLMs的基础理论，然后设计了一个整合两者的推荐系统架构，并通过实验验证了该系统的有效性。结果表明，基于协作过滤和LLMs的混合模型显著提高了准确率、召回率和用户满意度，展示了其在复杂推荐场景中的潜力。', 'title_zh': '结合协同过滤和大型语言模型的增强推荐方法'}
