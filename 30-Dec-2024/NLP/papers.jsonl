{'arxiv_id': 'arXiv:2412.19785', 'title': "Enhancing Whisper's Accuracy and Speed for Indian Languages through Prompt-Tuning and Tokenization", 'authors': 'Kumud Tripathi, Raj Gothi, Pankaj Wasnik', 'link': 'https://arxiv.org/abs/2412.19785', 'abstract': "Automatic speech recognition has recently seen a significant advancement with large foundational models such as Whisper. However, these models often struggle to perform well in low-resource languages, such as Indian languages. This paper explores two novel approaches to enhance Whisper's multilingual speech recognition performance in Indian languages. First, we propose prompt-tuning with language family information, which enhances Whisper's accuracy in linguistically similar languages. Second, we introduce a novel tokenizer that reduces the number of generated tokens, thereby accelerating Whisper's inference speed. Our extensive experiments demonstrate that the tokenizer significantly reduces inference time, while prompt-tuning enhances accuracy across various Whisper model sizes, including Small, Medium, and Large. Together, these techniques achieve a balance between optimal WER and inference speed.", 'abstract_zh': '自动语音识别在大型基础模型如Whisper的推动下，近期取得了显著进展。然而，这些模型在资源稀缺的语言中表现往往不佳，如印度语等。本文探讨了两种新颖的方法来提升Whisper在印度语中的多语言语音识别性能。首先，我们提出了一种基于语言家族信息的提示调优方法，这种方法可以增强Whisper在语言相似语言中的准确性。其次，我们引入了一种新型的分词器，该分词器减少了生成的 token 数量，从而加速了Whisper的推理速度。我们进行的大量实验表明，该分词器显著减少了推理时间，而提示调优则在各种Whisper模型大小（包括小型、中型和大型）上增强了准确性。结合使用这些技术可以在最优词错误率(WER)和推理速度之间实现平衡。', 'title_zh': '通过提示调优和分词提高Whisper在印度语言中的准确性和速度'}
{'arxiv_id': 'arXiv:2412.19781', 'title': 'Machine Learning for Sentiment Analysis of Imported Food in Trinidad and Tobago', 'authors': 'Cassandra Daniels, Koffka Khan', 'link': 'https://arxiv.org/abs/2412.19781', 'abstract': 'This research investigates the performance of various machine learning algorithms (CNN, LSTM, VADER, and RoBERTa) for sentiment analysis of Twitter data related to imported food items in Trinidad and Tobago. The study addresses three primary research questions: the comparative accuracy and efficiency of the algorithms, the optimal configurations for each model, and the potential applications of the optimized models in a live system for monitoring public sentiment and its impact on the import bill. The dataset comprises tweets from 2018 to 2024, divided into imbalanced, balanced, and temporal subsets to assess the impact of data balancing and the COVID-19 pandemic on sentiment trends. Ten experiments were conducted to evaluate the models under various configurations. Results indicated that VADER outperformed the other models in both multi-class and binary sentiment classifications. The study highlights significant changes in sentiment trends pre- and post-COVID-19, with implications for import policies.', 'abstract_zh': '本研究探讨了各种机器学习算法（卷积神经网络CNN、长短期记忆网络LSTM、VADER和RoBERTa）在 Trinidad and Tobago进口食品相关推文情感分析中的性能。研究主要回答了三个研究问题：这些算法的比较准确性和效率、每个模型的最佳配置，以及优化模型在实际系统中监测公众舆论及其对进口账单影响方面的潜在应用。数据集涵盖了从2018年到2024年的推文，分为不平衡、平衡和时间分段子集，以评估数据平衡和COVID-19疫情对情感趋势的影响。进行了十项实验以在不同配置下评估这些模型。结果表明，VADER在多类和二元情感分类中均表现出色。本研究突显了COVID-19前后情感趋势的显著变化，并对进口政策具有重要影响意义。', 'title_zh': '用于分析特立尼达和多巴哥进口食品情绪的机器学习方法'}
{'arxiv_id': 'arXiv:2412.19610', 'title': 'Machine Generated Product Advertisements: Benchmarking LLMs Against Human Performance', 'authors': 'Sanjukta Ghosh', 'link': 'https://arxiv.org/abs/2412.19610', 'abstract': 'This study compares the performance of AI-generated and human-written product descriptions using a multifaceted evaluation model. We analyze descriptions for 100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4) with and without sample descriptions, against human-written descriptions. Our evaluation metrics include sentiment, readability, persuasiveness, Search Engine Optimization(SEO), clarity, emotional appeal, and call-to-action effectiveness. The results indicate that ChatGPT 4 performs the best. In contrast, other models demonstrate significant shortcomings, producing incoherent and illogical output that lacks logical structure and contextual relevance. These models struggle to maintain focus on the product being described, resulting in disjointed sentences that do not convey meaningful information. This research provides insights into the current capabilities and limitations of AI in the creation of content for e-Commerce.', 'abstract_zh': '本研究采用多维度评估模型比较了AI生成和人工撰写的商品描述性能。我们分析了由四款AI模型（Gemma 2B、LLAMA、GPT2和ChatGPT 4）生成的100种商品描述（有样本描述和无样本描述两种情况），并与人工撰写的描述进行了对比。我们的评估指标包括情感、可读性、说服力、搜索引擎优化（SEO）、清晰度、情感吸引力以及呼吁行动的有效性。结果表明，ChatGPT 4 表现最佳。相比之下，其他模型表现出明显的不足，生成的输出内容零散且缺乏逻辑结构和相关性。这些模型在描述商品本身时很难保持聚焦，导致生成的句子断片化，无法传达有意义的信息。本研究为了解AI在电商内容创作方面的当前能力和局限性提供了见解。', 'title_zh': '机器生成的产品广告：LLM与人类表现的基准比较'}
{'arxiv_id': 'arXiv:2412.19544', 'title': 'TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data', 'authors': 'Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu', 'link': 'https://arxiv.org/abs/2412.19544', 'abstract': 'Semantic parsing, which converts natural language questions into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (TARGA), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning. Experiments on multiple knowledge base question answering (KBQA) datasets demonstrate that TARGA, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA(+7.7) and KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.', 'abstract_zh': '语义解析，即将自然语言问题转换为逻辑形式，在结构化环境中进行推理中起着关键作用。然而，现有的方法面临两个重大挑战：对大量手动标注数据的依赖以及在未见示例上的有限泛化能力。为解决这些问题，我们提出了目标导向的合成数据生成（TARGA）框架，这是一种实用的框架，能够动态生成高相关性的合成数据，无需手动标注。从给定问题的相关实体和关系出发，通过逐层扩展和跨层组合，探测潜在的相关查询。然后，针对这些构建的查询生成相应的自然语言问题，共同作为上下文学习中的合成示范。在多个知识库问答（KBQA）数据集上的实验表明，TARGA仅使用一个7B参数的模型显著优于现有未经微调的方法，这些方法利用了闭源模型，在GrailQA和KBQA-Agent上的F1分数分别提高了7.7和12.2个百分点。此外，TARGA还在非独立同分布（non-I.I.D.）设置下展现出更高的样本效率、鲁棒性和泛化能力。', 'title_zh': 'TARGA：面向结构化数据实践推理的靶向合成数据生成方法'}
{'arxiv_id': 'arXiv:2412.19522', 'title': 'Exploiting Domain-Specific Parallel Data on Multilingual Language Models for Low-resource Language Translation', 'authors': 'Surangika Ranathungaa, Shravan Nayak, Shih-Ting Cindy Huang, Yanke Mao, Tong Su, Yun-Hsiang Ray Chan, Songchen Yuan, Anthony Rinaldi, Annie En-Shiun Lee', 'link': 'https://arxiv.org/abs/2412.19522', 'abstract': "Neural Machine Translation (NMT) systems built on multilingual sequence-to-sequence Language Models (msLMs) fail to deliver expected results when the amount of parallel data for a language, as well as the language's representation in the model are limited. This restricts the capabilities of domain-specific NMT systems for low-resource languages (LRLs). As a solution, parallel data from auxiliary domains can be used either to fine-tune or to further pre-train the msLM. We present an evaluation of the effectiveness of these two techniques in the context of domain-specific LRL-NMT. We also explore the impact of domain divergence on NMT model performance. We recommend several strategies for utilizing auxiliary parallel data in building domain-specific NMT models for LRLs.", 'abstract_zh': '基于多语言序列到序列语言模型（msLMs）的神经机器翻译（NMT）系统在目标语言的平行数据量有限且模型中该语言表示不足时，无法达到预期效果。这限制了低资源语言（LRLs）领域特定NMT系统的性能。为了解决这一问题，可以利用辅助领域的平行数据来 fine-tune 或进一步预训练 msLM。我们评估了这两种技术在低资源语言领域特定NMT中的有效性，并探讨了领域差异对NMT模型性能的影响。我们还提出了几条利用辅助平行数据构建LRLs领域特定NMT模型的策略。', 'title_zh': '利用多语言模型中的领域特定并行数据进行低资源语言翻译'}
{'arxiv_id': 'arXiv:2412.19513', 'title': 'Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs', 'authors': 'Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui', 'link': 'https://arxiv.org/abs/2412.19513', 'abstract': 'Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction. Our code will be publicly available on GitHub.', 'abstract_zh': '大型语言模型（LLMs）可以纠正它们自动生成的答案，但同时也观察到自我纠正后准确度会下降的现象。为了更深入地理解自我纠正机制，我们尝试分解、评估和分析LLMs的自我纠正行为。通过对比和分析自我纠正前后答案的正确性，我们将自我纠正能力分解为信心（敢于修正答案）和批判力（将错误的答案转变为正确答案）两个能力，并从概率论的角度提出了两个用于衡量这两种能力的度量标准，同时还提出了一种用于总体自我纠正能力评估的度量标准。基于我们的分解和评估指标，我们进行了广泛的实验并得出了若干实证结论。例如，我们发现不同模型可能表现出不同的行为：有些模型更自信，而另一些模型则更严谨。我们还发现，在通过提示或上下文学习操纵模型自我纠正行为时，这两种能力之间的权衡关系（即提高一种能力可能会导致另一种能力下降）。此外，我们发现了一种简单有效的策略，通过更改Supervision Fine-Tuning（SFT）数据格式来提高自我纠正能力，并且该策略在两种能力上都优于传统的SFT，且自我纠正后的准确度也显著提高。我们的代码将在GitHub上公开发布。', 'title_zh': '自信与批评：大型语言模型自我修正能力的分解'}
{'arxiv_id': 'arXiv:2412.19512', 'title': 'Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging', 'authors': 'Hua Farn, Hsuan Su, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee', 'link': 'https://arxiv.org/abs/2412.19512', 'abstract': 'Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，符合学术规范：\n\n对大型语言模型（LLM）进行微调以适应下游任务是一种广泛采用的方法，但在安全对齐的LLM中，这往往会导致安全性下降。目前，许多解决方案通过引入额外的安全数据来应对这一问题，但在许多情况下这并不实用。在本文中，我们探讨的问题是：如何在不依赖额外安全数据的情况下提高下游任务性能同时保持LLM的安全性？我们提出了一种简单且有效的方法，该方法保持了LLM的内在安全性并增强了其下游任务性能：合并预制和后微调的安全对齐模型的权重。在各种下游任务、模型和合并方法下的实验结果表明，这种做法有效地减轻了安全性下降的问题并提升了下游任务性能，提供了一种实用的方法来适应安全对齐的LLM。', 'title_zh': '通过预调和后调模型合并保障微调后的大型语言模型安全'}
{'arxiv_id': 'arXiv:2412.19490', 'title': 'User Willingness-aware Sales Talk Dataset', 'authors': 'Asahi Hentona, Jun Baba, Shiki Sato, Reina Akama', 'link': 'https://arxiv.org/abs/2412.19490', 'abstract': "User willingness is a crucial element in the sales talk process that affects the achievement of the salesperson's or sales system's objectives. Despite the importance of user willingness, to the best of our knowledge, no previous study has addressed the development of automated sales talk dialogue systems that explicitly consider user willingness. A major barrier is the lack of sales talk datasets with reliable user willingness data. Thus, in this study, we developed a user willingness-aware sales talk collection by leveraging the ecological validity concept, which is discussed in the field of human-computer interaction. Our approach focused on three types of user willingness essential in real sales interactions. We created a dialogue environment that closely resembles real-world scenarios to elicit natural user willingness, with participants evaluating their willingness at the utterance level from multiple perspectives. We analyzed the collected data to gain insights into practical user willingness-aware sales talk strategies. In addition, as a practical application of the constructed dataset, we developed and evaluated a sales dialogue system aimed at enhancing the user's intent to purchase.", 'abstract_zh': '用户意愿是销售谈话语过程中至关重要的一项要素，它影响销售人员或销售系统的目标实现。尽管用户意愿的重要性不言而喻，据我们所知，迄今为止，没有一项先前的研究专门探讨了在明确考虑用户意愿的前提下开发自动化销售谈话语系统的方法。主要障碍之一是缺乏包含可靠用户意愿数据的销售谈话语料库。因此，本研究通过利用人机交互领域中的生态效度概念，开发了一种考虑用户意愿的销售谈话语料库。我们的方法重点关注了三种在真实销售互动中至关重要的用户意愿类型。我们创建了一个接近真实场景的对话环境，以促发自然的用户意愿，并从多个视角让参与者在句子层面对自己的意愿进行评估。我们分析收集的数据，以了解实用的用户意愿意识型销售谈话语策略。此外，作为所构建数据集的实际应用，我们开发并评估了一个旨在增强用户购买意愿的销售对话系统。', 'title_zh': '用户意愿感知销售对话数据集'}
{'arxiv_id': 'arXiv:2412.19482', 'title': 'Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering', 'authors': 'Shiwen Ni, Hao Cheng, Min Yang', 'link': 'https://arxiv.org/abs/2412.19482', 'abstract': 'Legal question answering (QA) has attracted increasing attention from people seeking legal advice, which aims to retrieve the most applicable answers from a large-scale database of question-answer pairs. Previous methods mainly use a dual-encoder architecture to learn dense representations of both questions and answers. However, these methods could suffer from lacking domain knowledge and sufficient labeled training data. In this paper, we propose a three-stage (\\underline{p}re-training, \\underline{f}ine-tuning and \\underline{r}e-ranking) framework for \\underline{l}egal \\underline{QA} (called PFR-LQA), which promotes the fine-grained text representation learning and boosts the performance of dense retrieval with the dual-encoder architecture. Concretely, we first conduct domain-specific pre-training on legal questions and answers through a self-supervised training objective, allowing the pre-trained model to be adapted to the legal domain. Then, we perform task-specific fine-tuning of the dual-encoder on legal question-answer pairs by using the supervised learning objective, leading to a high-quality dual-encoder for the specific downstream QA task. Finally, we employ a contextual re-ranking objective to further refine the output representations of questions produced by the document encoder, which uses contextual similarity to increase the discrepancy between the anchor and hard negative samples for better question re-ranking. We conduct extensive experiments on a manually annotated legal QA dataset. Experimental results show that our PFR-LQA method achieves better performance than the strong competitors for legal question answering.', 'abstract_zh': '法律问答（Legal Question Answering, LQA）吸引了寻求法律咨询的人们日益增长的兴趣，其目标是从大规模的问题-答案对数据库中检索出最适用的答案。先前的方法主要采用双编码器架构来学习问题和答案的密集表示。然而，这些方法可能会因缺乏领域知识和充足的标注训练数据而遇到问题。在本文中，我们提出了一种三阶段（预训练、微调和重排）框架（称为PFR-LQA），以促进细粒度文本表示的学习，并通过双编码器架构提升密集检索的性能。具体而言，我们首先通过自监督训练目标在法律问题和答案上进行领域特定的预训练，使预训练模型能够适应法律领域。然后，我们通过监督学习目标对双编码器在法律问题-答案对上进行任务特定的微调，从而产生高质量的双编码器，专门针对下游的问答任务。最后，我们采用上下文重排目标进一步细化文档编码器生成的问题表示，利用上下文相似性增加锚点样本和难以否定样本之间的差异，从而更好地进行问题重排。我们在一个手工标注的法律问答数据集上进行了广泛的实验。实验结果表明，我们的PFR-LQA方法在法律问题回答方面优于强劲的竞争者。', 'title_zh': '预训练、微调和重排-ranked：法律问答的三阶段框架'}
{'arxiv_id': 'arXiv:2412.19449', 'title': 'Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models', 'authors': 'Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao', 'link': 'https://arxiv.org/abs/2412.19449', 'abstract': 'This study proposes a knowledge distillation algorithm based on large language models and feature alignment, aiming to effectively transfer the knowledge of large pre-trained models into lightweight student models, thereby reducing computational costs while maintaining high model performance. Different from the traditional soft label distillation method, this method introduces a multi-layer feature alignment strategy to deeply align the intermediate features and attention mechanisms of the teacher model and the student model, maximally retaining the semantic expression ability and context modeling ability of the teacher model. In terms of method design, a multi-task loss function is constructed, including feature matching loss, attention alignment loss, and output distribution matching loss, to ensure multi-level information transfer through joint optimization. The experiments were comprehensively evaluated on the GLUE data set and various natural language processing tasks. The results show that the proposed model performs very close to the state-of-the-art GPT-4 model in terms of evaluation indicators such as perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline models such as DeBERTa, XLNet, and GPT-3, showing significant performance improvements and computing efficiency advantages. Research results show that the feature alignment distillation strategy is an effective model compression method that can significantly reduce computational overhead and storage requirements while maintaining model capabilities. Future research can be further expanded in the directions of self-supervised learning, cross-modal feature alignment, and multi-task transfer learning to provide more flexible and efficient solutions for the deployment and optimization of deep learning models.', 'abstract_zh': '本研究提出了一种基于大规模语言模型和特征对齐的知识蒸馏算法，旨在有效地将大规模预训练模型的知识转移到轻量级的学生模型中，从而在降低计算成本的同时保持高模型性能。不同于传统的软标签蒸馏方法，本方法引入了多层次特征对齐策略，以深入对齐教师模型和学生模型的中间特征和注意力机制，最大程度地保留教师模型的语义表达能力和上下文建模能力。在方法设计方面，构建了一个多任务损失函数，包括特征匹配损失、注意力对齐损失和输出分布匹配损失，通过联合优化确保多级信息的传递。实验在GLUE数据集和各种自然语言处理任务上进行了综合评估。结果显示，所提出的模型在困惑度、BLEU、ROUGE和CER等评估指标上的表现与当前最先进的GPT-4模型非常接近。同时，它远超DeBERTa、XLNet和GPT-3等基线模型，显示出显著的性能改进和计算效率优势。研究结果表明，特征对齐蒸馏策略是一种有效的方法，可以在显著减少计算开销和存储需求的同时保持模型能力。未来的研究可以在自监督学习、跨模态特征对齐和多任务迁移学习的方向上进一步扩展，为深度学习模型的部署和优化提供更灵活和高效的解决方案。', 'title_zh': '基于特征对齐的知识蒸馏方法：用于大型语言模型高效压缩的研究'}
{'arxiv_id': 'arXiv:2412.19437', 'title': 'DeepSeek-V3 Technical Report', 'authors': 'DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng', 'link': 'https://arxiv.org/abs/2412.19437', 'abstract': 'We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at this https URL.', 'abstract_zh': '我们介绍了DeepSeek-V3，这是一个强大的混合专家（MoE）语言模型，总参数数量为671亿，每个词激活参数为37亿。为实现高效的推理和成本效益的训练，DeepSeek-V3采用了多头潜注意力（MLA）和DeepSeekMoE架构，这些架构在DeepSeek-V2中得到了充分验证。此外，DeepSeek-V3首次提出了一种无辅助损失的负载均衡策略，并设置了多词预测训练目标以提高性能。我们使用了14.8万亿个多样且高质量的词元对DeepSeek-V3进行了预训练，随后进行了监督微调和强化学习，以充分挖掘其潜力。综合评估表明，DeepSeek-V3在多项指标上优于其他开源模型，并达到了与主流闭源模型相当的性能。尽管其性能优秀，但DeepSeek-V3仅需2.788M H800 GPU小时即可完成全部训练。此外，其训练过程非常稳定，在整个训练过程中未出现任何不可恢复的损失突增或恢复操作。模型检查点可通过以下链接获得：[这里](https://)。', 'title_zh': 'DeepSeek-V3 技术报告'}
{'arxiv_id': 'arXiv:2412.19361', 'title': 'Dynamic Skill Adaptation for Large Language Models', 'authors': 'Jiaao Chen, Diyi Yang', 'link': 'https://arxiv.org/abs/2412.19361', 'abstract': 'We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework to adapt novel and complex skills to Large Language Models (LLMs). Compared with previous work which learns from human-curated and static data in random orders, we propose to first automatically generate and organize the training data by mimicking the learning pathways of human and then dynamically tailor the training data based on the training dynamics. Specifically, inspired by the learning structures and teaching strategies in the human education system, we first construct a skill graph by decomposing complex skills into sub-skills and arranging them based on their dependencies in human syllables. For every skill, we utilize LLMs to generate both textbook-like data which contains detailed descriptions of skills for pre-training and exercise-like data which targets at explicitly utilizing the skills to solve problems for instruction-tuning. Furthermore, during the instruction-tuning, we dynamically update the training data which down-weight easy-to-learn examples, generate more complex examples, and filter out data with errors. Experiments on large language models such as LLAMA and Mistral demonstrate the effectiveness of our proposed methods in adapting math reasoning skills and social study skills.', 'abstract_zh': '我们提出了动态技能适应（DSA），这是一种适应性和动态框架，用于将新颖且复杂的技能适应大型语言模型（LLMs）。与之前的工作相比，这些工作依赖于随机顺序的人工策划和静态数据进行学习，我们提出首先通过模仿人类的学习路径自动生成和组织训练数据，然后根据训练动态动态调整训练数据。具体而言，借鉴人类教育系统中的学习结构和教学策略，我们首先通过将复杂技能分解为子技能，并根据人类教科书中的依赖关系进行排列，构建了一个技能图。对于每个技能，我们利用LLMs生成类似教科书的数据，包含技能的详细描述以进行预训练，并生成类似于练习题的数据，以明确利用这些技能解决具体问题，用于指令调优。此外，在指令调优过程中，我们动态更新训练数据，降低容易学习的示例权重，生成更复杂的示例，并过滤掉错误数据。实验表明，在LLAMA和Mistral等大型语言模型上的实验验证了我们提出的方法在适应数学推理技能和社会研究技能方面的有效性。', 'title_zh': '大型语言模型的动态技能适应性'}
{'arxiv_id': 'arXiv:2412.19346', 'title': 'Semi-Supervised Learning from Small Annotated Data and Large Unlabeled Data for Fine-grained PICO Entity Recognition', 'authors': 'Fangyi Chen, Gongbo Zhang, Yilu Fang, Yifan Peng, Chunhua Weng', 'link': 'https://arxiv.org/abs/2412.19346', 'abstract': 'Objective: Extracting PICO elements -- Participants, Intervention, Comparison, and Outcomes -- from clinical trial literature is essential for clinical evidence retrieval, appraisal, and synthesis. Existing approaches do not distinguish the attributes of PICO entities. This study aims to develop a named entity recognition (NER) model to extract PICO entities with fine granularities.\nMaterials and Methods: Using a corpus of 2,511 abstracts with PICO mentions from 4 public datasets, we developed a semi-supervised method to facilitate the training of a NER model, FinePICO, by combining limited annotated data of PICO entities and abundant unlabeled data. For evaluation, we divided the entire dataset into two subsets: a smaller group with annotations and a larger group without annotations. We then established the theoretical lower and upper performance bounds based on the performance of supervised learning models trained solely on the small, annotated subset and on the entire set with complete annotations, respectively. Finally, we evaluated FinePICO on both the smaller annotated subset and the larger, initially unannotated subset. We measured the performance of FinePICO using precision, recall, and F1.\nResults: Our method achieved precision/recall/F1 of 0.567/0.636/0.60, respectively, using a small set of annotated samples, outperforming the baseline model (F1: 0.437) by more than 16\\%. The model demonstrates generalizability to a different PICO framework and to another corpus, which consistently outperforms the benchmark in diverse experimental settings (p-value \\textless0.001).\nConclusion: This study contributes a generalizable and effective semi-supervised approach to named entity recognition leveraging large unlabeled data together with small, annotated data. It also initially supports fine-grained PICO extraction.', 'abstract_zh': '目标：从临床试验文献中提取参与对象（Participants）、干预措施（Intervention）、对照组（Comparison）和结局指标（Outcomes）（PICO元素）是临床证据检索、评估和综合的关键。现有方法未能区分PICO实体的属性。本研究旨在开发一个命名实体识别（NER）模型，以精细粒度提取PICO实体。\n\n材料与方法：我们利用包含2,511篇带有PICO提及的摘要的4个公共数据集建立了一个半监督方法，通过结合少量标注的数据和大量未标注的数据来促进NER模型FinePICO的训练。为了评估效果，我们将整个数据集分为两个子集：一个较小的带有标注的子集和一个较大的未标注子集。基于仅使用较小的标注子集进行监督学习模型训练的表现和整个数据集完全标注子集进行训练的表现，我们分别设定理论下的较低和较高性能界限。最后，我们在较小的标注子集和较大的最初未标注子集上评估了FinePICO。我们使用精确率、召回率和F1值来衡量FinePICO的性能。\n\n结果：在使用少量标注样本的情况下，我们的方法达到了0.567/0.636/0.60的精确率/召回率/F1值，远超基线模型（F1: 0.437）16%以上。该模型展示了在不同的PICO框架和另一份语料库上的泛化能力，并在多种实验设置中表现优于基准模型（p值<0.001）。\n\n结论：本研究提出了一种利用大量未标注数据和少量标注数据结合的半监督方法来识别命名实体的一般化和有效方法，并初步支持了精细粒度的PICO提取。', 'title_zh': '基于少量标注数据和大量未标注数据的半监督学习方法在细粒度PICO实体识别中的应用'}
{'arxiv_id': 'arXiv:2412.19260', 'title': 'MEDEC: A Benchmark for Medical Error Detection and Correction in Clinical Notes', 'authors': 'Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, Thomas Lin', 'link': 'https://arxiv.org/abs/2412.19260', 'abstract': 'Several studies showed that Large Language Models (LLMs) can answer medical questions correctly, even outperforming the average human score in some medical exams. However, to our knowledge, no study has been conducted to assess the ability of language models to validate existing or generated medical text for correctness and consistency. In this paper, we introduce MEDEC (this https URL), the first publicly available benchmark for medical error detection and correction in clinical notes, covering five types of errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal Organism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes from three US hospital systems that were not previously seen by any LLM. The dataset has been used for the MEDIQA-CORR shared task to evaluate seventeen participating systems [Ben Abacha et al., 2024]. In this paper, we describe the data creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4, Claude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and correcting medical errors requiring both medical knowledge and reasoning capabilities. We also conducted a comparative study where two medical doctors performed the same task on the MEDEC test set. The results showed that MEDEC is a sufficiently challenging benchmark to assess the ability of models to validate existing or generated notes and to correct medical errors. We also found that although recent LLMs have a good performance in error detection and correction, they are still outperformed by medical doctors in these tasks. We discuss the potential factors behind this gap, the insights from our experiments, the limitations of current evaluation metrics, and share potential pointers for future research.', 'abstract_zh': '多项研究表明，大型语言模型（LLMs）能够准确回答医学问题，甚至在某些医学考试中还表现出优于普通人类的得分。然而，据我们所知，尚未有任何研究评估语言模型验证现有或生成的医学文本正确性和一致性的能力。本文介绍了MEDEC（该链接：this https URL），这是首个公开的用于临床笔记中医学错误检测与修正的基准数据集，涵盖了五种类型的错误（诊断、管理、治疗、药物治疗和病因）。MEDEC包含3,848份临床文本，其中包括来自三个未见过任何LLM的美国医院系统的488份临床笔记。该数据集已被用于MEDIQA-CORR共享任务，评估了17个参与系统的性能[Ben Abacha et al., 2024]。在本文中，我们描述了数据的创建方法，并评估了近期的LLM（如o1-preview、GPT-4、Claude 3.5 Sonnet和Gemini 2.0 Flash）在检测和修正医学错误任务中的性能，这些任务既需要医学知识也需要推理能力。我们还进行了一项对比研究，其中两位医学专家也在MEDEC测试集上执行相同的任务。结果表明，MEDEC是一个足够具有挑战性的基准，可以评估模型验证现有或生成的临床笔记、修正医学错误的能力。我们还发现，尽管近期的LLM在错误检测和修正方面表现出色，但在这些任务上仍然不如医学专家。本文讨论了这种差距背后可能的因素、实验所得的见解、当前评估指标的局限性，并分享了未来研究的潜在方向。', 'title_zh': 'MEDEC：临床笔记中医疗错误检测与修正的标准基准'}
{'arxiv_id': 'arXiv:2412.19168', 'title': 'GFG -- Gender-Fair Generation: A CALAMITA Challenge', 'authors': 'Simona Frenda, Andrea Piergentili, Beatrice Savoldi, Marco Madeddu, Martina Rosola, Silvia Casola, Chiara Ferrando, Viviana Patti, Matteo Negri, Luisa Bentivogli', 'link': 'https://arxiv.org/abs/2412.19168', 'abstract': 'Gender-fair language aims at promoting gender equality by using terms and expressions that include all identities and avoid reinforcing gender stereotypes. Implementing gender-fair strategies is particularly challenging in heavily gender-marked languages, such as Italian. To address this, the Gender-Fair Generation challenge intends to help shift toward gender-fair language in written communication. The challenge, designed to assess and monitor the recognition and generation of gender-fair language in both mono- and cross-lingual scenarios, includes three tasks: (1) the detection of gendered expressions in Italian sentences, (2) the reformulation of gendered expressions into gender-fair alternatives, and (3) the generation of gender-fair language in automatic translation from English to Italian. The challenge relies on three different annotated datasets: the GFL-it corpus, which contains Italian texts extracted from administrative documents provided by the University of Brescia; GeNTE, a bilingual test set for gender-neutral rewriting and translation built upon a subset of the Europarl dataset; and Neo-GATE, a bilingual test set designed to assess the use of non-binary neomorphemes in Italian for both fair formulation and translation tasks. Finally, each task is evaluated with specific metrics: average of F1-score obtained by means of BERTScore computed on each entry of the datasets for task 1, an accuracy measured with a gender-neutral classifier, and a coverage-weighted accuracy for tasks 2 and 3.', 'abstract_zh': '性别公平语言旨在通过使用包容所有身份的术语和表达方式，避免强化性别刻板印象，从而促进性别平等。在性别标记程度较高的语言中，如意大利语，实施性别公平策略尤为具有挑战性。为应对这一挑战，性别公平生成挑战旨在推动性别公平语言在书面交流中的应用。该挑战旨在评估和监控性别公平语言在单语和跨语情境下的识别和生成，包括三项任务：（1）检测意大利语句子中的性别化表达；（2）将性别化表达重新表述为性别公平的替代表达；（3）从英语自动翻译成意大利语的性别公平语言生成。该挑战依赖于三个不同的标注数据集：GFL-it语料库，包含从布雷西亚大学提供的行政文件中提取的意大利文本；GeNTE，一个基于Europarl数据集子集构建的双语测试集，用于性别中性重写和翻译；以及Neo-GATE，一个双语测试集，旨在评估在意大利语中使用非二元新形态词以进行公正表达和翻译任务的能力。最后，每一项任务都使用特定的指标进行评估：任务1中每个数据集条目通过BERTScore计算的平均F1分数、一种使用性别中性分类器衡量的准确性，以及任务2和任务3的覆盖率加权准确性。', 'title_zh': 'GFG —— 性别公正生成：一个 CALAMITA 挑战\n\n注：这里假设“CALAMITA”是一个特定的挑战或竞赛名称，保持了原文形式以确保专有名词的一致性。如果“CALAMITA”有特定的中文翻译或含义，建议根据具体情况做适当调整。'}
{'arxiv_id': 'arXiv:2412.19140', 'title': 'SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis', 'authors': 'Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng', 'link': 'https://arxiv.org/abs/2412.19140', 'abstract': 'In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at this https URL.', 'abstract_zh': '近年来，金融领域的细粒度情感分析获得了显著的关注，但实体级数据集的稀缺性仍然是一个关键挑战。为了解决这一问题，我们构建了迄今为止最大规模的英语和中文金融实体级情感分析数据集。在此基础上，我们提出了一种名为自意识上下文学习修正（SILC）的新型两阶段情感分析方法。第一阶段涉及微调一个基础的大语言模型，以生成适用于我们任务的伪标注数据。在第二阶段，我们使用基于GNN的示例检索器训练一个修正模型，该检索器利用了伪标注数据进行指导。这种两阶段策略使我们在新构建的数据集上取得了最先进的性能，推动了金融情感分析领域的进步。在一个案例研究中，我们展示了我们数据和方法在监测加密货币市场方面的增强实用性。我们的数据集和代码可以在此 URL 获取：[此 https URL]。', 'title_zh': 'SILC-EFSA: 具有实体级金融情感分析上下文自意识修正的自我监督学习方法'}
{'arxiv_id': 'arXiv:2412.19113', 'title': 'SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values', 'authors': 'Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang', 'link': 'https://arxiv.org/abs/2412.19113', 'abstract': 'Missing value is a critical issue in data science, significantly impacting the reliability of analyses and predictions. Missing value imputation (MVI) is a longstanding problem because it highly relies on domain knowledge. Large language models (LLMs) have emerged as a promising tool for data cleaning, including MVI for tabular data, offering advanced capabilities for understanding and generating content. However, despite their promise, existing LLM techniques such as in-context learning and Chain-of-Thought (CoT) often fall short in guiding LLMs to perform complex reasoning for MVI, particularly when imputing derived missing values, which require mathematical formulas and data relationships across rows and columns. This gap underscores the need for further advancements in LLM methodologies to enhance their reasoning capabilities for more reliable imputation outcomes. To fill this gap, we propose SketchFill, a novel sketch-based method to guide LLMs in generating accurate formulas to impute missing numerical values. Our experimental results demonstrate that SketchFill significantly outperforms state-of-the-art methods, achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher accuracy than MetaGPT. This sets a new standard for automated data cleaning and advances the field of MVI for numerical values.', 'abstract_zh': '缺失值是数据科学中的一个关键问题，严重影响了分析和预测的可靠性。缺失值插补（MVI）一直是长期存在的问题，因为它高度依赖领域知识。大型语言模型（LLMs）已经 emerged 作为数据清洗的有希望的工具，包括表数据的 MVI，提供了对内容理解和生成的高级能力。然而，尽管它们具有潜力，现有的 LLM 技术，如上下文学习和思维链（Chain-of-Thought, CoT），往往在引导 LLMs 进行复杂推理以进行 MVI 方面表现不佳，特别是在 imputing 派生的缺失值时，这些值需要跨行和列的数学公式和数据关系。这一差距突显了进一步在 LLM 方法上进行创新的需求，以增强其对更可靠插补结果的推理能力。为填补这一缺口，我们提出了一种名为 SketchFill 的新颖的基于草图的方法，以引导 LLMs 生成准确的公式来 impute 缺失的数值数据。我们的实验结果表明，SketchFill 显著优于现有方法，在基于 CoT 的方法上的准确率提高了 56.2%，在 MetaGPT 上的准确率提高了 78.8%。这为自动化数据清洗设立了新的标准，并推进了数值数据 MVI 的领域。', 'title_zh': 'SketchFill：基于草图的代码生成方法用于填充衍生缺失值'}
{'arxiv_id': 'arXiv:2412.19102', 'title': '"I\'ve Heard of You!": Generate Spoken Named Entity Recognition Data for Unseen Entities', 'authors': 'Jiawei Yu, Xiang Geng, Yuang Li, Mengxin Ren, Wei Tang, Jiahuan Li, Zhibin Lan, Min Zhang, Hao Yang, Shujian Huang, Jinsong Su', 'link': 'https://arxiv.org/abs/2412.19102', 'abstract': 'Spoken named entity recognition (NER) aims to identify named entities from speech, playing an important role in speech processing. New named entities appear every day, however, annotating their Spoken NER data is costly. In this paper, we demonstrate that existing Spoken NER systems perform poorly when dealing with previously unseen named entities. To tackle this challenge, we propose a method for generating Spoken NER data based on a named entity dictionary (NED) to reduce costs. Specifically, we first use a large language model (LLM) to generate sentences from the sampled named entities and then use a text-to-speech (TTS) system to generate the speech. Furthermore, we introduce a noise metric to filter out noisy data. To evaluate our approach, we release a novel Spoken NER benchmark along with a corresponding NED containing 8,853 entities. Experiment results show that our method achieves state-of-the-art (SOTA) performance in the in-domain, zero-shot domain adaptation, and fully zero-shot settings. Our data will be available at this https URL.', 'abstract_zh': '口语命名实体识别（Spoken Named Entity Recognition, Spoken NER）旨在从口语中识别命名实体，这在口语处理中扮演着重要角色。新的命名实体每天都在出现，然而标注其口语命名实体识别数据的成本较高。本文表明，现有的口语命名实体识别系统在处理之前未见过的命名实体时表现不佳。为应对这一挑战，我们提出了一种基于命名实体字典（Named Entity Dictionary, NED）生成口语命名实体识别数据的方法以降低标注成本。具体而言，我们首先使用大规模语言模型（Large Language Model, LLM）从采样的命名实体中生成句子，然后使用文本转语音（Text-to-Speech, TTS）系统生成口语。此外，我们引入了一个噪音度量方法以过滤掉嘈杂的数据。为了评估我们的方法，我们提供了一个新的口语命名实体识别基准数据集及其相应的包含8,853个实体的命名实体字典。实验结果表明，我们的方法在领域内、零样本领域自适应和完全零样本设置中均达到了目前最先进的性能（State-of-the-Art, SOTA）。我们的数据集将在以下网址提供：\\[此链接\\]。', 'title_zh': '《听过你！》：为未知实体生成语音命名实体识别数据'}
{'arxiv_id': 'arXiv:2412.19076', 'title': 'Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and Analysis', 'authors': 'Dima Galat', 'link': 'https://arxiv.org/abs/2412.19076', 'abstract': 'The recent proliferation of AI-generated content has prompted significant interest in developing reliable detection methods. This study explores techniques for identifying AI-generated text through sentence-level evaluation within hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits distinct, repetitive probability patterns that enable consistent in-domain detection. Empirical tests show that minor textual modifications, such as rewording, have minimal impact on detection accuracy. These results provide valuable insights for advancing AI detection methodologies, offering a pathway toward robust solutions to address the complexities of synthetic text identification.', 'abstract_zh': '近年来，AI生成内容的激增引发了对可靠检测方法的广泛关注。本研究探讨了通过混合文章内的句子级评估来识别AI生成文本的技术。研究发现，ChatGPT-3.5 Turbo 显示出独特的、重复的概率模式，这使其可以在领域内实现一致的检测。实证测试表明，轻微的文本修改，如重新润色，对检测准确性的影响很小。这些结果为推进AI检测方法学提供了宝贵的见解，为解决合成文本识别的复杂性提供了稳健的解决方案路径。', 'title_zh': 'ALTA 2024 共享任务中大语言模型检测的进展：技术与分析'}
{'arxiv_id': 'arXiv:2412.19070', 'title': 'Cross-Demographic Portability of Deep NLP-Based Depression Models', 'authors': 'Tomek Rutowski, Elizabeth Shriberg, Amir Harati, Yang Lu, Ricardo Oliveira, Piotr Chlebek', 'link': 'https://arxiv.org/abs/2412.19070', 'abstract': 'Deep learning models are rapidly gaining interest for real-world applications in behavioral health. An important gap in current literature is how well such models generalize over different populations. We study Natural Language Processing (NLP) based models to explore portability over two different corpora highly mismatched in age. The first and larger corpus contains younger speakers. It is used to train an NLP model to predict depression. When testing on unseen speakers from the same age distribution, this model performs at AUC=0.82. We then test this model on the second corpus, which comprises seniors from a retirement community. Despite the large demographic differences in the two corpora, we saw only modest degradation in performance for the senior-corpus data, achieving AUC=0.76. Interestingly, in the senior population, we find AUC=0.81 for the subset of patients whose health state is consistent over time. Implications for demographic portability of speech-based applications are discussed.', 'abstract_zh': '深度学习模型在行为健康领域的实际应用中正迅速获得关注。当前文献中的一个重要缺口是如何这些模型在不同人群中的一般化能力。我们研究了基于自然语言处理（NLP）的模型，以探索其在两个在年龄分布上高度不匹配的数据集之间的普适性。第一个也是规模更大的数据集包含年轻讲者，用于训练一个NLP模型以预测抑郁症状。在针对相同年龄分布的未见过的讲者进行测试时，该模型的性能为AUC=0.82。然后，我们用包含退休社区老年人的第二数据集对该模型进行测试。尽管两个数据集之间存在显著的人口统计学差异，但在老年人数据集中的性能下降相对轻微，AUC为0.76。有趣的是，在老年人群中，对于健康状况随时间保持一致的患者子集，我们得到了AUC=0.81的结果。讨论了基于语音的应用在不同人群中的普适性问题。', 'title_zh': '基于深度自然语言处理的抑郁症模型的跨人群迁移性研究'}
{'arxiv_id': 'arXiv:2412.19043', 'title': 'Indonesian-English Code-Switching Speech Synthesizer Utilizing Multilingual STEN-TTS and Bert LID', 'authors': 'Ahmad Alfani Handoyo, Chung Tran, Dessi Puji Lestari, Sakriani Sakti', 'link': 'https://arxiv.org/abs/2412.19043', 'abstract': 'Multilingual text-to-speech systems convert text into speech across multiple languages. In many cases, text sentences may contain segments in different languages, a phenomenon known as code-switching. This is particularly common in Indonesia, especially between Indonesian and English. Despite its significance, no research has yet developed a multilingual TTS system capable of handling code-switching between these two languages. This study addresses Indonesian-English code-switching in STEN-TTS. Key modifications include adding a language identification component to the text-to-phoneme conversion using finetuned BERT for per-word language identification, as well as removing language embedding from the base model. Experimental results demonstrate that the code-switching model achieves superior naturalness and improved speech intelligibility compared to the Indonesian and English baseline STEN-TTS models.', 'abstract_zh': '多语言文本到语音系统可以将文本转换为多个语言的语音。在许多情况下，文本句子中可能包含不同语言的段落，这种现象称为语码转换。在印度尼西亚，尤其是印度尼西亚语和英语之间，语码转换现象尤为普遍。尽管这一现象十分重要，但目前尚无研究开发出能够处理这两种语言之间语码转换的多语言TTS系统。本研究在STEN-TTS中解决了印度尼西亚语-英语语码转换的问题。关键的修改包括在文本到音素转换中添加一个语言识别组件，利用微调后的BERT进行逐词语言识别，并从基础模型中移除语言嵌入。实验结果表明，语码转换模型在自然度和语音可理解性方面优于印度尼西亚语和英语的基线STEN-TTS模型。', 'title_zh': '利用多语言STEN-TTS和Bert语言识别的印尼语-英语代码转换语音合成系统'}
{'arxiv_id': 'arXiv:2412.19018', 'title': 'Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability', 'authors': 'Ruixi Lin, Yang You', 'link': 'https://arxiv.org/abs/2412.19018', 'abstract': "In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, a tailored correction for per-sample, per-class's probability. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule Optimization based Debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.", 'abstract_zh': '上下文学习，使大型语言模型能够在少数示例下执行多种任务，但被发现在线性多类别文本分类任务中存在类别间预测准确度的不平衡现象。尽管已经开发出了显著的输出校正方法以解决这一问题并同时提高下游预测准确度，但它们可能无法回答核心解释性挑战，即为什么某些特定类别需要校正，以及更重要的是，如何针对每个样本和每个类别的概率进行定制校正。为应对这种解释性缺口，我们首先发现不平衡源于某些类别的ICL输出概率始终较高，而其他类别则较低或处于混合范围内，因此前者被更频繁地选择，导致了更高的准确率；更为关键的是，我们发现这些范围对准确度偏差的影响程度存在显著差异，这突显了进行精确且可解释的概率校正的必要性。基于这一认识，我们提出了FuRud，一种基于模糊规则优化的去偏差方法，该方法（1）检测哪些类别需要校正；（2）对于每个需要校正的类别，检测其概率范围，并对其应用不对称的放大或缩减，以解释性的方式进行校正。值得注意的是，在七个基准数据集上，FuRud将类间准确度偏差（COBias）降低了56%以上，同时准确率相对提高了21%，远超最先进的去偏差方法。此外，FuRud只需要10个优化示例就能优化下游任务。进一步地，FuRud能够处理导致预测高度偏斜的提示格式。例如，对于使用字母选项的ICL输出，FuRud实现了21%的相对准确率提升和36%的相对COBias减少。', 'title_zh': '让规则发声：增强基于语境的学习去偏见化 interpretaability\n\n注：在学术翻译中，我们需要注意一些细节。这里的翻译“增强基于语境的学习去偏见化 interpretaability”试图保持原文的结构和含义。不过，在学术论文中，“interpretaability”可能并不是一个标准术语。通常，“可解释性（Interpretability）”是一个更常见的概念，用来表示对模型决策过程的理解。因此，更合适的翻译可能是：\n\n让规则发声：增强基于语境的学习去偏见化并提高可解释性\n\n这样既保留了原文的意思，又符合学术规范。'}
{'arxiv_id': 'arXiv:2412.18947', 'title': 'MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models', 'authors': 'Kaiwen Zuo, Yirui Jiang', 'link': 'https://arxiv.org/abs/2412.18947', 'abstract': "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.", 'abstract_zh': '以下是翻译成中文的内容，符合学术规范：\n\n医学大型语言模型（MLLMs）在医疗应用中表现出潜力，但仍存在生成医学上不可靠或不准确信息的幻觉倾向，这给患者护理带来了重大风险。本文引入了MedHallBench，这是一个全面的基准框架，用于评估和减轻MLLMs中的幻觉问题。我们的方法整合了专家验证的医学案例场景与现有的医学数据库，以创建一个强大的评估数据集。该框架采用了一种复杂的测量系统，该系统结合了自动ACHMI（医学影像中的自动标题幻觉测量系统）评分与严格的临床专家评估，并利用强化学习方法实现自动标注。通过一种特别为医疗应用设计的优化强化学习从人类反馈（RLHF）训练管道，MedHallBench能够在保持严格准确标准的同时，全面评估MLLMs在各种临床环境中的表现。我们进行了涉及多种模型的对比实验，利用基准框架为广泛采用的大型语言模型（LLMs）建立了基线。研究发现，ACHMI比传统评估指标能提供更细致的幻觉影响理解，从而突显了其在幻觉评估中的优势。本研究为增强MLLMs在医疗环境中的可靠性奠定了基础，并提出了应对医疗应用中AI幻觉问题的可操作策略。', 'title_zh': 'MedHallBench：一种评估医疗大型语言模型幻觉的新基准'}
{'arxiv_id': 'arXiv:2412.18934', 'title': 'Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference', 'authors': 'Libo Zhang, Zhaoning Zhang, Baizhou Xu, Songzhu Mei, Dongsheng Li', 'link': 'https://arxiv.org/abs/2412.18934', 'abstract': 'Due to the high resource demands of Large Language Models (LLMs), achieving widespread deployment on consumer-grade devices presents significant challenges. Typically, personal or consumer-grade devices, including servers configured prior to the era of large-scale models, generally have relatively weak GPUs and relatively strong CPUs. However, most current methods primarily depend on GPUs for computation. Therefore, we propose Dovetail, an approach that deploys the draft model on the GPU to generate draft tokens while allowing the target model to perform parallel verification on the CPU, thereby improving the utilization of all available hardware resources and occupying less inter-device communication bandwidth. Accordingly, we have redesigned the draft model to better align with heterogeneous hardware characteristics. To this end, we implemented several optimizations: reducing the number of draft tokens to mitigate latency in parallel verification, increasing the depth of the draft model to enhance its predictive capacity, and introducing DGF (Dynamic Gating Fusion) to improve the integration of features and token embeddings. In the HumanEval benchmark, Dovetail achieved an inference speed of 5.86 tokens per second for LLaMA2-Chat-7B using 3GB of VRAM, representing an approximately 2.77x improvement over CPU-only inference. Furthermore, the inference speed was increased to 8 tokens per second when utilizing 7GB of VRAM.', 'abstract_zh': '由于大型语言模型（LLMs）对资源的需求很高，要在消费级设备上实现广泛部署面临着显著挑战。通常，个人或消费级设备，包括在大规模模型时代之前的配置的服务器，一般具有相对较弱的GPU和相对较强的CPU。然而，目前大多数方法主要依赖GPU进行计算。因此，我们提出了一种名为Dovetail的方法。该方法通过将在GPU上部署草稿模型以生成草稿令牌，同时在CPU上并行验证目标模型，从而提高了所有可用硬件资源的利用率，并减少了设备间通信带宽的占用。相应地，我们重新设计了草稿模型，以更好地适应异构硬件特性。为此，我们实施了多项优化措施：减少草稿令牌的数量以缓解并行验证中的延迟问题，增加草稿模型的深度以增强其预测能力，并引入DGF（Dynamic Gating Fusion）以提高特征和词嵌入的集成度。在HumanEval基准测试中，Dovetail使用3GB VRAM对LLaMA2-Chat-7B进行了推理，其推理速度达到了每秒5.86个令牌，相比于仅使用CPU的推理速度提高了约2.77倍。此外，当使用7GB VRAM时，推理速度提高到了每秒8个令牌。', 'title_zh': 'Dovetail：一种用于大规模语言模型推理的CPU/GPU异构猜测性解码方法'}
{'arxiv_id': 'arXiv:2412.18925', 'title': 'HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs', 'authors': 'Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, Benyou Wang', 'link': 'https://arxiv.org/abs/2412.18925', 'abstract': 'The breakthrough of OpenAI o1 highlights the potential of enhancing reasoning to improve LLM. Yet, most research in reasoning has focused on mathematical tasks, leaving domains like medicine underexplored. The medical domain, though distinct from mathematics, also demands robust reasoning to provide reliable answers, given the high standards of healthcare. However, verifying medical reasoning is challenging, unlike those in mathematics. To address this, we propose verifiable medical problems with a medical verifier to check the correctness of model outputs. This verifiable nature enables advancements in medical reasoning through a two-stage approach: (1) using the verifier to guide the search for a complex reasoning trajectory for fine-tuning LLMs, (2) applying reinforcement learning (RL) with verifier-based rewards to enhance complex reasoning further. Finally, we introduce HuatuoGPT-o1, a medical LLM capable of complex reasoning, which outperforms general and medical-specific baselines using only 40K verifiable problems. Experiments show complex reasoning improves medical problem-solving and benefits more from RL. We hope our approach inspires advancements in reasoning across medical and other specialized domains.', 'abstract_zh': 'OpenAI o1的突破凸显了增强推理能力以提高大语言模型性能的潜力。然而，大多数关于推理的研究主要集中在数学任务上，而医学领域等其他领域则未得到充分探索。尽管医学与数学领域不同，但医学同样需要强大的推理能力以提供可靠的答案，因为医疗服务的标准非常高。然而，医学推理的验证比数学领域的验证更具挑战性。为解决这一问题，我们提出了一种带有医学验证器的可验证医学问题，以检查模型输出的正确性。这种可验证性通过两阶段方法促进了医学推理的进步：（1）使用验证器指导复杂的推理轨迹搜索，以微调大语言模型；（2）利用基于验证器的奖励强化学习（RL）来进一步增强复杂的推理能力。最后，我们介绍了HuatuoGPT-o1，这是一种能够进行复杂推理的医学大语言模型，仅使用40,000个可验证问题便超越了一般和专门针对医学问题的基准模型。实验结果显示，复杂的推理能够提高医学问题的解决能力，并且更受益于强化学习。我们希望我们的方法能够激励跨医学和其他专门领域中的推理进步。', 'title_zh': 'HuatuoGPT-o1：面向医学复杂推理的大型语言模型探究'}
{'arxiv_id': 'arXiv:2412.18908', 'title': 'Research Experiment on Multi-Model Comparison for Chinese Text Classification Tasks', 'authors': 'JiaCheng Li', 'link': 'https://arxiv.org/abs/2412.18908', 'abstract': 'With the explosive growth of Chinese text data and advancements in natural language processing technologies, Chinese text classification has become one of the key techniques in fields such as information retrieval and sentiment analysis, attracting increasing attention. This paper conducts a comparative study on three deep learning models:TextCNN, TextRNN, and this http URL for Chinese text classification tasks. By conducting experiments on the THUCNews dataset, the performance of these models is evaluated, and their applicability in different scenarios is discussed.', 'abstract_zh': '随着中文文本数据的爆炸性增长和自然语言处理技术的进步，中文文本分类已成为信息检索和情感分析等领域中的关键技术之一，引起了越来越多的关注。本文对比研究了三种深度学习模型：TextCNN、TextRNN 以及this http URL 用于中文文本分类任务的效果。通过在THUCNews数据集上进行实验，评估了这些模型的性能，并讨论了它们在不同场景下的适用性。', 'title_zh': '中文翻译如下，符合学术规范：\n\n对中国文本分类任务的多模型比较研究实验'}
{'arxiv_id': 'arXiv:2412.18868', 'title': 'Overview of MWE history, challenges, and horizons: standing at the 20th anniversary of the MWE workshop series via MWE-UD2024', 'authors': 'Lifeng Han, Kilian Evang, Archna Bhatia, Gosse Bouma, A. Seza Doğruöz, Marcos Garcia, Voula Giouli, Joakim Nivre, Alexandre Rademacher', 'link': 'https://arxiv.org/abs/2412.18868', 'abstract': 'Starting in 2003 when the first MWE workshop was held with ACL in Sapporo, Japan, this year, the joint workshop of MWE-UD co-located with the LREC-COLING 2024 conference marked the 20th anniversary of MWE workshop events over the past nearly two decades. Standing at this milestone, we look back to this workshop series and summarise the research topics and methodologies researchers have carried out over the years. We also discuss the current challenges that we are facing and the broader impacts/synergies of MWE research within the CL and NLP fields. Finally, we give future research perspectives. We hope this position paper can help researchers, students, and industrial practitioners interested in MWE get a brief but easy understanding of its history, current, and possible future.', 'abstract_zh': '自2003年在日本札幌举行的第一次短语习得研讨会（MWE workshop）与ACL会议联合举办以来，今年在LREC-COLING 2024会议期间联合举办的MWE-UD研讨会庆祝了过去近二十年中MWE研讨会系列的第20个年头。站在这一里程碑上，我们回顾了这个研讨会系列的历史，总结了研究人员在过去几年中开展的研究主题和方法论。我们还讨论了当前面临的挑战以及MWE研究在计算语言学（CL）和自然语言处理（NLP）领域的更广泛影响/协同作用。最后，我们提出了未来研究的展望。我们希望这篇立场文件能帮助对MWE感兴趣的科研人员、学生和工业实践者获得对其历史、现状及其可能未来的简明易懂的理解。', 'title_zh': '基于MWE-UD2024，回顾词汇习语历史、挑战与展望：纪念词汇习语研讨会系列20周年'}
{'arxiv_id': 'arXiv:2412.18863', 'title': 'Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models', 'authors': 'Meltem Aksoy', 'link': 'https://arxiv.org/abs/2412.18863', 'abstract': "Large language models (LLMs) have become integral tools in diverse domains, yet their moral reasoning capabilities across cultural and linguistic contexts remain underexplored. This study investigates whether multilingual LLMs, such as GPT-3.5-Turbo, GPT-4o-mini, Llama 3.1, and MistralNeMo, reflect culturally specific moral values or impose dominant moral norms, particularly those rooted in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and Russian, the study analyzes the models' adherence to six core moral foundations: care, equality, proportionality, loyalty, authority, and purity. The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs. Although some models demonstrate adaptability to diverse contexts, others exhibit biases influenced by the composition of the training data. These findings underscore the need for culturally inclusive model development to improve fairness and trust in multilingual AI systems.", 'abstract_zh': '大型语言模型（LLMs）已经成为各个领域的核心工具，但它们在不同的文化和语言背景下是否能体现特定的道德价值观或强加主导的道德规范（尤其是根植于英语中的规范）仍然有待深入探索。本研究旨在调查多语言LLMs，如GPT-3.5-Turbo、GPT-4o-mini、Llama 3.1和MistralNeMo，是否能够反映特定文化中的道德价值观，还是倾向于强加主导的道德规范。本研究使用更新后的道德基础问卷（MFQ-2，更新至8种语言，即阿拉伯语、波斯语、英语、西班牙语、日语、汉语、法语和俄语），对模型在六个核心道德维度上的表现进行分析：关爱、平等、比例、忠诚、权威和纯净。研究结果表明，不同文化和语言背景下存在显著的差异，这挑战了LLMs在道德上具有一致性的假设。虽然有些模型在不同背景下表现出一定的适应性，但其他模型则显示出由训练数据组成影响的偏见。这些发现强调了在多语言AI系统中需要文化包容性开发的重要性，以提高公平性和用户的信任度。', 'title_zh': '他们的道德观由谁来代表？揭示多语言语言模型中的文化偏见\n\n这个标题翻译尽可能符合学术文章的规范，同时保持了原文的意思。如果有具体的论文内容需要翻译，请提供具体内容，以便进行更准确的翻译。'}
{'arxiv_id': 'arXiv:2412.18860', 'title': 'Bootstrap Your Own Context Length', 'authors': 'Liang Wang, Nan Yang, Xingxing Zhang, Xiaolong Huang, Furu Wei', 'link': 'https://arxiv.org/abs/2412.18860', 'abstract': 'We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.', 'abstract_zh': '我们提出了一种通过利用短语境语言模型的能力来训练长语境语言模型的bootstrapping方法。该方法利用简单的代理工作流合成长语境指令调整数据，从而消除了手动数据收集和标注的需求。所提出的数据合成工作流仅需要一个短语境语言模型、一个文本检索器和一个文档集合，这些资源在开源生态系统中都很容易获得。随后，使用合成数据对语言模型进行微调，以扩展其上下文长度。通过这种方式，我们有效地通过bootstrapping过程将语言模型的短语境能力转移到长语境场景中。我们使用开源Llama-3模型家族进行了实验，并展示了我们的方法能够将上下文长度扩展至多达100万词，且在各种基准测试中表现出更优的性能。', 'title_zh': '自我生成情境长度'}
{'arxiv_id': 'arXiv:2412.18826', 'title': 'RapGuard: Safeguarding Multimodal Large Language Models via Rationale-aware Defensive Prompting', 'authors': 'Yilei Jiang, Yingshui Tan, Xiangyu Yue', 'link': 'https://arxiv.org/abs/2412.18826', 'abstract': 'While Multimodal Large Language Models (MLLMs) have made remarkable progress in vision-language reasoning, they are also more susceptible to producing harmful content compared to models that focus solely on text. Existing defensive prompting techniques rely on a static, unified safety guideline that fails to account for the specific risks inherent in different multimodal contexts. To address these limitations, we propose RapGuard, a novel framework that uses multimodal chain-of-thought reasoning to dynamically generate scenario-specific safety prompts. RapGuard enhances safety by adapting its prompts to the unique risks of each input, effectively mitigating harmful outputs while maintaining high performance on benign tasks. Our experimental results across multiple MLLM benchmarks demonstrate that RapGuard achieves state-of-the-art safety performance, significantly reducing harmful content without degrading the quality of responses.', 'abstract_zh': '尽管多模态大型语言模型（MLLMs）在视觉语言推理方面取得了显著进展，但与仅专注于文本的模型相比，它们更容易生成有害内容。现有的防御性提示技术依靠的是静态的、统一的安全准则，无法考虑到不同多模态背景下固有的特定风险。为了解决这些限制，我们提出了一种名为RapGuard的新型框架，该框架利用多模态链式推理动态生成针对特定场景的安全提示。RapGuard通过适应每个输入的独特风险来提升安全性，有效地减轻有害输出，同时在无害任务上保持高水平的性能。我们在多个MLLM基准上的实验结果表明，RapGuard实现了最先进的安全性能，显著减少了有害内容，而不会降低响应的质量。', 'title_zh': 'RapGuard：通过理据意识防御型提示保护多模态大规模语言模型'}
{'arxiv_id': 'arXiv:2412.18811', 'title': 'DCIS: Efficient Length Extrapolation of LLMs via Divide-and-Conquer Scaling Factor Search', 'authors': 'Lei Yang, Shaoyang Xu, Deyi Xiong', 'link': 'https://arxiv.org/abs/2412.18811', 'abstract': 'Large language models (LLMs) based on the Transformer architecture usually have their context length limited due to the high training cost. Recent advancements extend the context window by adjusting the scaling factors of RoPE and fine-tuning. However, suboptimal initialization of these factors results in increased fine-tuning costs and reduced performance at target length. To address these challenges, we propose an innovative RoPE-based fine-tuning framework that diverges from conventional scaling factors search. Specifically, we present a Divide-and-Conquer Incremental Search (DCIS) algorithm that strategically determines the better scaling factors. Further fine-tuning with the identified scaling factors effectively extends the context window of LLMs. Empirical results demonstrate that our methodology not only mitigates performance decay at extended target lengths but also allows the model to fine-tune on short contexts and generalize to long contexts, thereby reducing the cost of fine-tuning. The scaling factors obtained through DCIS can even perform effectively without fine-tuning. Further analysis of the search space reveals that DCIS achieves twice the search efficiency compared to other methods. We also examine the impact of the non-strictly increasing scaling factors utilized in DCIS and evaluate the general capabilities of LLMs across various context lengths.', 'abstract_zh': '基于Transformer架构的大语言模型（LLMs）通常受限于上下文长度，因为训练成本较高。最近的进步通过调整RoPE（旋转位置编码）的缩放因子和微调来扩展上下文窗口。然而，这些因子的次优初始化会导致微调成本增加并降低目标长度上的性能。为应对这些挑战，我们提出了一种创新的基于RoPE的微调框架，该框架不同于传统的缩放因子搜索方式。具体而言，我们提出了一种分而治之增量搜索（DCIS）算法，该算法战略性地确定了更佳的缩放因子。使用识别出的缩放因子进行进一步微调可以有效扩展LLMs的上下文窗口。实验证明，我们的方法不仅能缓解在延长目标长度上的性能衰减，还能使模型在短上下文中进行微调并适应长上下文，从而降低微调成本。通过DCIS获得的缩放因子甚至可以在不微调的情况下有效工作。进一步对搜索空间的分析表明，DCIS的搜索效率是其他方法的两倍。我们还研究了DCIS中使用的非严格递增缩放因子的影响，并评估了LLMs在各种上下文长度下的泛化能力。', 'title_zh': 'DCIS: 分而治之缩放因子搜索下的高效长度外推大规模语言模型'}
{'arxiv_id': 'arXiv:2412.18800', 'title': 'Improving Generated and Retrieved Knowledge Combination Through Zero-shot Generation', 'authors': 'Xinkai Du, Quanjie Han, Chao Lv, Yan Liu, Yalin Sun, Hao Shu, Hongbo Shan, Maosong Sun', 'link': 'https://arxiv.org/abs/2412.18800', 'abstract': 'Open-domain Question Answering (QA) has garnered substantial interest by combining the advantages of faithfully retrieved passages and relevant passages generated through Large Language Models (LLMs). However, there is a lack of definitive labels available to pair these sources of knowledge. In order to address this issue, we propose an unsupervised and simple framework called Bi-Reranking for Merging Generated and Retrieved Knowledge (BRMGR), which utilizes re-ranking methods for both retrieved passages and LLM-generated passages. We pair the two types of passages using two separate re-ranking methods and then combine them through greedy matching. We demonstrate that BRMGR is equivalent to employing a bipartite matching loss when assigning each retrieved passage with a corresponding LLM-generated passage. The application of our model yielded experimental results from three datasets, improving their performance by +1.7 and +1.6 on NQ and WebQ datasets, respectively, and obtaining comparable result on TriviaQA dataset when compared to competitive baselines.', 'abstract_zh': '开放域问答（QA）通过结合忠实检索段落和大型语言模型（LLMs）生成的相关段落的优势，引起了广泛关注。然而，现有方法缺乏用于这些知识来源的标准标签进行配对。为了解决这一问题，我们提出了一种名为“Bi-Reranking for Merging Generated and Retrieved Knowledge（BRMGR）”的无监督简单框架，利用重排序方法对检索段落和LLM生成段落进行重排序。我们使用两种不同的重排序方法分别配对这两种类型的段落，并通过贪婪匹配将它们结合在一起。我们证明，当为每段检索段落配对相应的LLM生成段落时，BRMGR等效于使用二分匹配损失。实验结果表明，我们的模型在三个数据集上的应用提高了NQ和WebQ数据集的性能，分别提高了1.7和1.6个百分点，而在Trivqa数据集上与竞品基线相当。', 'title_zh': '通过零样本生成提高生成和检索知识的结合'}
{'arxiv_id': 'arXiv:2412.18733', 'title': 'Intra- and Inter-modal Context Interaction Modeling for Conversational Speech Synthesis', 'authors': 'Zhenqi Jia, Rui Liu', 'link': 'https://arxiv.org/abs/2412.18733', 'abstract': "Conversational Speech Synthesis (CSS) aims to effectively take the multimodal dialogue history (MDH) to generate speech with appropriate conversational prosody for target utterance. The key challenge of CSS is to model the interaction between the MDH and the target utterance. Note that text and speech modalities in MDH have their own unique influences, and they complement each other to produce a comprehensive impact on the target utterance. Previous works did not explicitly model such intra-modal and inter-modal interactions. To address this issue, we propose a new intra-modal and inter-modal context interaction scheme-based CSS system, termed III-CSS. Specifically, in the training phase, we combine the MDH with the text and speech modalities in the target utterance to obtain four modal combinations, including Historical Text-Next Text, Historical Speech-Next Speech, Historical Text-Next Speech, and Historical Speech-Next Text. Then, we design two contrastive learning-based intra-modal and two inter-modal interaction modules to deeply learn the intra-modal and inter-modal context interaction. In the inference phase, we take MDH and adopt trained interaction modules to fully infer the speech prosody of the target utterance's text content. Subjective and objective experiments on the DailyTalk dataset show that III-CSS outperforms the advanced baselines in terms of prosody expressiveness. Code and speech samples are available at this https URL.", 'abstract_zh': '对话式语音合成（Conversational Speech Synthesis, CSS）的目标是有效利用多模态对话历史（Multimodal Dialogue History, MDH），为目标话语生成具有适当会话语调的语音。CSS的关键挑战在于如何建模MDH与目标话语之间的相互作用。值得注意的是，MDH中的文本和语音模态各自具有独特的影响力，并相互补充，共同对目标话语产生综合影响。以往的工作没有明确建模这种模态内和模态间的相互作用。为解决这一问题，我们提出了一种基于模态内和模态间上下文交互方案的新CSS系统，称为III-CSS。具体而言，在训练阶段，我们将MDH与目标话语中的文本和语音模态结合，获得四种模态组合，包括历史文本-下一步文本、历史语音-下一步语音、历史文本-下一步语音、以及历史语音-下一步文本。然后，我们设计了两个基于对比学习的模态内交互模块和两个模态间交互模块，以深入学习模态内和模态间的上下文交互。在推理阶段，我们利用MDH和训练好的交互模块全面推断目标话语文本内容的语音语调。在DailyTalk数据集上进行的主观数字化和客观实验表明，III-CSS在语调表达能力方面优于先进的基线系统。代码和语音样本可在以下链接获取：<这个链接>。', 'title_zh': '跨模态和跨通道上下文交互建模在会话语音合成中的应用'}
{'arxiv_id': 'arXiv:2412.18729', 'title': 'Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks', 'authors': 'Jiacheng Hu, Xiaoxuan Liao, Jia Gao, Zhen Qi, Hongye Zheng, Chihang Wang', 'link': 'https://arxiv.org/abs/2412.18729', 'abstract': 'This study proposes a large language model optimization method based on the improved LoRA fine-tuning algorithm, aiming to improve the accuracy and computational efficiency of the model in natural language processing tasks. We fine-tune the large language model through a low-rank adaptation strategy, which significantly reduces the consumption of computing resources while maintaining the powerful capabilities of the pre-trained model. The experiment uses the QQP task as the evaluation scenario. The results show that the improved LoRA algorithm shows significant improvements in accuracy, F1 score, and MCC compared with traditional models such as BERT, Roberta, T5, and GPT-4. In particular, in terms of F1 score and MCC, our model shows stronger robustness and discrimination ability, which proves the potential of the improved LoRA algorithm in fine-tuning large-scale pre-trained models. In addition, this paper also discusses the application prospects of the improved LoRA algorithm in other natural language processing tasks, emphasizing its advantages in multi-task learning and scenarios with limited computing resources. Future research can further optimize the LoRA fine-tuning strategy and expand its application in larger-scale pre-trained models to improve the generalization ability and task adaptability of the model.', 'abstract_zh': '本研究提出了一种基于改进LoRA微调算法的大语言模型优化方法，旨在提高模型在自然语言处理任务中的准确性和计算效率。通过低秩适应策略对大规模语言模型进行微调，显著减少了计算资源的消耗，同时保持了预训练模型的强大能力。实验使用QQP任务作为评估场景，结果表明改进的LoRA算法在准确率、F1分数和MCC等方面相较于传统的BERT、RoBERTa、T5和GPT-4模型显示出显著的提升。特别是在F1分数和MCC方面，我们的模型显示出更强的稳健性和辨析能力，证明了改进的LoRA算法在微调大规模预训练模型方面的潜力。此外，本文还探讨了改进LoRA算法在其他自然语言处理任务中的应用前景，强调了其在多任务学习和计算资源有限场景中的优势。未来的研究可以进一步优化LoRA微调策略，并将其应用扩展到更大规模的预训练模型，以提高模型的泛化能力和任务适应性。', 'title_zh': '使用增强型LoRA微调算法提升大型语言模型在自然语言处理任务中的效率和稳健性'}
{'arxiv_id': 'arXiv:2412.18719', 'title': 'Using Large Language Models for Automated Grading of Student Writing about Science', 'authors': 'Chris Impey, Matthew Wenger, Nikhil Garuda, Shahriar Golchin, Sarah Stamer', 'link': 'https://arxiv.org/abs/2412.18719', 'abstract': 'Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.', 'abstract_zh': '大规模班级中对正式或非正式学习者的写作进行评估是一个重大挑战。因此，大多数大规模班级，特别是在科学领域，依赖于客观评估工具，如多项选择测验，这些测验通常只有一个正确答案。随着人工智能的迅速发展，利用大型语言模型（LLMs）来评估学生写作的可能性已经出现。一项实验使用了GPT-4，旨在确定基于LLMs的机器学习方法能否在评估天文学相关主题的短篇写作作业方面与教师评分相媲美或超越教师评分的可靠性。实验对象包括通过Coursera提供的三门大规模开放在线课程（MOOCs）中的成年学习者。其中一门课程为天文学，第二门课程为天体生物学，第三门课程为天文学的历史与哲学。实验结果也可以适用于大学环境中非科学专业的学生，其中课程内容和评估方式类似。数据包括三个课程中120名学生对12个问题的回答。GPT-4被提供了所有三个课程中的总成绩、模型答案和教师评分标准。除了评估LLM在重现教师评分方面的可靠度外，LLM还被要求生成自己的评分标准。总体而言，LLM在汇总和个体学生层面都比同伴评分更加可靠，并且在所有三个在线课程中大致与教师评分相当。这一结果意味着未来LLMs可能被用于自动、可靠且可扩展地评估学生的科学写作。', 'title_zh': '使用大型语言模型对学生科学写作的自动化评分应用'}
{'arxiv_id': 'arXiv:2412.18707', 'title': 'Multiple References with Meaningful Variations Improve Literary Machine Translation', 'authors': 'Si Wu, John Wieting, David A. Smith', 'link': 'https://arxiv.org/abs/2412.18707', 'abstract': 'While a source sentence can be translated in many ways, most machine translation (MT) models are trained with only a single reference. Previous work has shown that using synthetic paraphrases can improve MT. This paper investigates best practices for employing multiple references by analyzing the semantic similarity among different English translations of world literature in the Par3 dataset. We classify the semantic similarity between paraphrases into three groups: low, medium, and high, and fine-tune two different LLMs (mT5-large and LLaMA-2-7B) for downstream MT tasks. Across different models, holding the total training instances constant, single-reference but more source texts only marginally outperforms multiple-reference with half of the source texts. Moreover, using paraphrases of medium and high semantic similarity outperforms an unfiltered dataset (+BLEU 0.3-0.5, +COMET 0.2-0.9, +chrF++ 0.25-0.32). Our code is publicly available on GitHub.', 'abstract_zh': '尽管一个源句子可以有多种翻译方式，但大多数机器翻译（MT）模型通常仅使用单一参考进行训练。先前的研究表明，使用合成的同义词替换可以提升机器翻译的效果。本文通过分析Par3数据集中世界文学的不同英语翻译之间的语义相似性，探讨了使用多个参考的最佳实践。我们将同义词替换的语义相似性分为三类：低、中、高，并分别对两种不同的大型语言模型（mT5-large 和 LLaMA-2-7B）进行微调，用于下游的机器翻译任务。在不同的模型中，保持总训练实例数量不变的情况下，单一参考但更多源文本的表现仅略微优于使用一半源文本的多个参考。此外，使用中等和高水平语义相似性的同义词替换相较于未经筛选的数据集，在BLEU分数上提高了0.3-0.5，在COMET分数上提高了0.2-0.9，在chrF++分数上提高了0.25-0.32。我们的代码已在GitHub上公开。', 'title_zh': '多种富有意义的参考文献改善了文学机器翻译'}
{'arxiv_id': 'arXiv:2412.18702', 'title': 'CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era', 'authors': 'Yanlin Feng, Simone Papicchio, Sajjadur Rahman', 'link': 'https://arxiv.org/abs/2412.18702', 'abstract': 'Retrieval from graph data is crucial for augmenting large language models (LLM) with both open-domain knowledge and private enterprise data, and it is also a key component in the recent GraphRAG system (edge et al., 2024). Despite decades of research on knowledge graphs and knowledge base question answering, leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal support for retrieval from modern encyclopedic knowledge graphs like Wikidata. In this paper, we analyze the root cause and suggest that modern RDF knowledge graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly large schemas that far exceed the typical LLM context window, use of resource identifiers, overlapping relation types and lack of normalization. As a solution, we propose property graph views on top of the underlying RDF graph that can be efficiently queried by LLMs using Cypher. We instantiated this idea on Wikidata and introduced CypherBench, the first benchmark with 11 large-scale, multi-domain property graphs with 7.8 million entities and over 10,000 questions. To achieve this, we tackled several key challenges, including developing an RDF-to-property graph conversion engine, creating a systematic pipeline for text-to-Cypher task generation, and designing new evaluation metrics.', 'abstract_zh': '从图形数据中检索对于增强大型语言模型（LLM）的通用领域知识和企业私有数据至关重要，并且也是最近的GraphRAG系统（edge等，2024）中的关键组成部分。尽管在知识图谱和知识库问答方面进行了几十年的研究，主流的LLM框架（例如Langchain和LlamaIndex）对现代百科全书式知识图谱（如Wikidata）的检索支持仍然相对有限。在本论文中，我们分析了根本原因，并建议现代RDF知识图谱（如Wikidata、Freebase）因为其过大且远超标准LLM上下文窗口的模式定义、使用资源标识符、关系类型重叠以及缺乏规范化而导致效率低下。为此，我们提出了一种基于底层RDF图的属性图视图，使其可以使用Cypher高效地被LLM查询。我们在Wikidata上实现了这一想法，并引入了CypherBench，这是第一个包含11个大规模、多领域属性图的基准，拥有超过780万个实体和10000多个问题。为实现这一点，我们攻克了多个关键挑战，包括开发RDF到属性图的转换引擎、创建系统性的从文本到Cypher任务生成管道以及设计新的评估指标。', 'title_zh': 'CypherBench：面向大规模现代知识图谱在大语言模型时代精确检索的研究'}
{'arxiv_id': 'arXiv:2412.18690', 'title': 'AgreeMate: Teaching LLMs to Haggle', 'authors': 'Ainesh Chatterjee, Samuel Miller, Nithin Parepally', 'link': 'https://arxiv.org/abs/2412.18690', 'abstract': 'We introduce AgreeMate, a framework for training Large Language Models (LLMs) to perform strategic price negotiations through natural language. We apply recent advances to a negotiation setting where two agents (i.e. buyer or seller) use natural language to bargain on goods using coarse actions. Specifically, we present the performance of Large Language Models when used as agents within a decoupled (modular) bargaining architecture. We demonstrate that using prompt engineering, fine-tuning, and chain-of-thought prompting enhances model performance, as defined by novel metrics. We use attention probing to show model attention to semantic relationships between tokens during negotiations.', 'abstract_zh': '我们将介绍AgreeMate框架，该框架旨在通过自然语言训练大规模语言模型（LLMs）进行战略性价格谈判。我们应用了最近的进展，将这种谈判设置应用于两个代理（买家或卖家）使用自然语言通过粗略行动来进行商品交易的情境。具体而言，我们展示了在解耦（模块化）的协商架构中，当大规模语言模型作为代理使用时的性能表现。我们证明了通过提示工程、微调和链式思考提示可以提升模型性能，而这种提升利用了新的性能指标进行定义。我们采用注意力探针来展示模型在谈判过程中对词汇间语义关系的注意力。', 'title_zh': 'AgreeMate：教学超大规模语言模型进行讨价还价'}
{'arxiv_id': 'arXiv:2412.18672', 'title': 'From Hallucinations to Facts: Enhancing Language Models with Curated Knowledge Graphs', 'authors': 'Ratnesh Kumar Joshi, Sagnik Sengupta, Asif Ekbal', 'link': 'https://arxiv.org/abs/2412.18672', 'abstract': 'Hallucination, a persistent challenge plaguing language models, undermines their efficacy and trustworthiness in various natural language processing endeavors by generating responses that deviate from factual accuracy or coherence. This paper addresses language model hallucination by integrating curated knowledge graph (KG) triples to anchor responses in empirical data. We meticulously select and integrate relevant KG triples tailored to specific contexts, enhancing factual grounding and alignment with input. Our contribution involves constructing a comprehensive KG repository from Wikipedia and refining data to spotlight essential information for model training. By imbuing language models with access to this curated knowledge, we aim to generate both linguistically fluent responses and deeply rooted in factual accuracy and context relevance. This integration mitigates hallucinations by providing a robust foundation of information, enabling models to draw upon a rich reservoir of factual data during response generation. Experimental evaluations demonstrate the effectiveness of multiple approaches in reducing hallucinatory responses, underscoring the role of curated knowledge graphs in improving the reliability and trustworthiness of language model outputs.', 'abstract_zh': '幻觉，一直是语言模型面临的顽固挑战，削弱了其在各种自然语言处理任务中的效用和可信度，因为它会产生与事实准确性或连贯性相偏离的回应。本文通过整合精心筛选的知识图谱（KG）三元组来解决语言模型的幻觉问题，将回应锚定在实际数据上。我们仔细选择并整合了适应特定上下文的相关KG三元组，增强了事实 grounding 并与输入进行了对齐。我们贡献的一部分是构建了一个源自维基百科的全面KG仓库，并精炼数据以突出模型训练中相关信息的要点。通过赋予语言模型访问这些精心筛选的知识的能力，我们旨在生成既流畅又有事实准确性和上下文相关性的响应。这种整合通过提供一个坚实的信息基础，使模型在生成回应时能够利用丰富的事实数据资源，从而减轻幻觉现象。实验评估证明了多种方法在减少幻觉回应方面的有效性，突显了精心筛选的知识图谱在提高语言模型输出可靠性和可信度方面的作用。', 'title_zh': '从幻觉到事实：通过精心策划的知识图谱提升语言模型'}
{'arxiv_id': 'arXiv:2412.18655', 'title': 'Simple is not Enough: Document-level Text Simplification using Readability and Coherence', 'authors': 'Laura Vásquez-Rodríguez, Nhung T.H. Nguyen, Piotr Przybyła, Matthew Shardlow, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2412.18655', 'abstract': 'In this paper, we present the SimDoc system, a simplification model considering simplicity, readability, and discourse aspects, such as coherence. In the past decade, the progress of the Text Simplification (TS) field has been mostly shown at a sentence level, rather than considering paragraphs or documents, a setting from which most TS audiences would benefit. We propose a simplification system that is initially fine-tuned with professionally created corpora. Further, we include multiple objectives during training, considering simplicity, readability, and coherence altogether. Our contributions include the extension of professionally annotated simplification corpora by the association of existing annotations into (complex text, simple text, readability label) triples to benefit from readability during training. Also, we present a comparative analysis in which we evaluate our proposed models in a zero-shot, few-shot, and fine-tuning setting using document-level TS corpora, demonstrating novel methods for simplification. Finally, we show a detailed analysis of outputs, highlighting the difficulties of simplification at a document level.', 'abstract_zh': '在本文中，我们介绍了SimDoc系统，这是一种综合考虑简洁性、可读性和话语连贯性（coherence）的简化模型。在过去十年中，文本简化（Text Simplification, TS）领域的发展主要集中在句子层面，而忽略了段落或文档级的简化，这是大多数TS应用场景中受益最大的领域。我们提出了一种初始通过专业创建的语料库进行微调的简化系统。进一步地，在训练过程中，我们考虑了简洁性、可读性和连贯性三者之间的平衡。我们的贡献包括通过将现有的注释与（复杂文本、简单文本、可读性标签）三元组关联起来，扩展了专业标注的简化语料库，以便在训练过程中利用可读性信息。此外，我们还进行了对比分析，在文档级TS语料库上评估了我们提出的模型在无标签、少量标签和微调设置下的性能，展示了新的简化方法。最后，我们对输出进行了详细分析，突出了在文档级别简化过程中遇到的挑战。', 'title_zh': '简洁并不足够：基于可读性和连贯性的文档级文本简化'}
{'arxiv_id': 'arXiv:2412.18644', 'title': 'DynaGRAG: Improving Language Understanding and Generation through Dynamic Subgraph Representation in Graph Retrieval-Augmented Generation', 'authors': 'Karishma Thakrar', 'link': 'https://arxiv.org/abs/2412.18644', 'abstract': 'Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to enhance language understanding and generation by leveraging external knowledge. However, effectively capturing and integrating the rich semantic information present in textual and structured data remains a challenge. To address this, a novel GRAG framework is proposed to focus on enhancing subgraph representation and diversity within the knowledge graph. By improving graph density, capturing entity and relation information more effectively, and dynamically prioritizing relevant and diverse subgraphs, the proposed approach enables a more comprehensive understanding of the underlying semantic structure. This is achieved through a combination of de-duplication processes, two-step mean pooling of embeddings, query-aware retrieval considering unique nodes, and a Dynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph Convolutional Networks (GCNs) and Large Language Models (LLMs) through hard prompting further enhances the learning of rich node and edge representations while preserving the hierarchical subgraph structure. Experimental results on multiple benchmark datasets demonstrate the effectiveness of the proposed GRAG framework, showcasing the significance of enhanced subgraph representation and diversity for improved language understanding and generation.', 'abstract_zh': 'Graph检索增强生成（GRAG或Graph RAG）架构旨在通过利用外部知识来增强语言理解与生成。然而，有效地捕捉和整合文本和结构化数据中的丰富语义信息仍然是一个挑战。为了解决这一问题，提出了一个新颖的GRAG框架，以增强知识图中的子图表示和多样性。通过提高图的密度、更有效地捕捉实体和关系信息，并动态优先考虑相关和多样化的子图，所提出的方法能够更全面地理解潜在的语义结构。这通过去重过程的组合、两步嵌入聚类平均、考虑唯一节点的查询感知检索以及动态相似性感知BFS（DSA-BFS）遍历算法实现。通过将图卷积网络（GCNs）和大型语言模型（LLMs）结合使用硬提示，进一步增强了丰富的节点和边表示的学习能力，同时保留了层次化的子图结构。在多个基准数据集上的实验结果表明，提出的GRAG框架的有效性，展示了增强子图表示和多样性的显著意义，以提高语言理解和生成。', 'title_zh': 'DynaGRAG：通过图检索增强生成中的动态子图表示提高语言理解和生成'}
{'arxiv_id': 'arXiv:2412.18627', 'title': 'KRAIL: A Knowledge-Driven Framework for Base Human Reliability Analysis Integrating IDHEAS and Large Language Models', 'authors': 'Xingyu Xiao, Peng Chen, Ben Qi, Hongru Zhao, Jingang Liang, Jiejuan Tong, Haitao Wang', 'link': 'https://arxiv.org/abs/2412.18627', 'abstract': "Human reliability analysis (HRA) is crucial for evaluating and improving the safety of complex systems. Recent efforts have focused on estimating human error probability (HEP), but existing methods often rely heavily on expert knowledge,which can be subjective and time-consuming. Inspired by the success of large language models (LLMs) in natural language processing, this paper introduces a novel two-stage framework for knowledge-driven reliability analysis, integrating IDHEAS and LLMs (KRAIL). This innovative framework enables the semi-automated computation of base HEP values. Additionally, knowledge graphs are utilized as a form of retrieval-augmented generation (RAG) for enhancing the framework' s capability to retrieve and process relevant data efficiently. Experiments are systematically conducted and evaluated on authoritative datasets of human reliability. The experimental results of the proposed methodology demonstrate its superior performance on base HEP estimation under partial information for reliability assessment.", 'abstract_zh': '人类可靠性分析（HRA）对于评估和改善复杂系统的安全性至关重要。近期的研究主要集中在估计人类错误概率（HEP），但现有方法往往依赖于专家知识，这可能会导致主观性和耗时性。受到大型语言模型（LLMs）在自然语言处理领域取得成功的影响，本文提出了一种新的基于知识驱动的可靠性分析框架——KRAIL（Knowledge-driven Reliability Analysis using Integrated IDHEAS and Large Language Models），并采用两阶段流程实现这一框架。这个创新性框架能够半自动化地计算基础HEP值。此外，本文还利用知识图谱作为一种检索增强生成（RAG）方法，以提高框架在高效检索和处理相关数据方面的能力。我们系统地在权威的人类可靠性数据集上进行了实验并进行了评估。所提出方法的实验结果表明，在可靠性评估中基于部分信息对基础HEP进行估计时，该方法表现出卓越的性能。', 'title_zh': 'KRAIL：一种基于知识的框架，结合IDHEAS和大规模语言模型进行基础人类可靠性分析'}
{'arxiv_id': 'arXiv:2412.18626', 'title': 'Why Do Large Language Models (LLMs) Struggle to Count Letters?', 'authors': 'Tairan Fu, Raquel Ferrando, Javier Conde, Carlos Arriaga, Pedro Reviriego', 'link': 'https://arxiv.org/abs/2412.18626', 'abstract': 'Large Language Models (LLMs) have achieved unprecedented performance on many complex tasks, being able, for example, to answer questions on almost any topic. However, they struggle with other simple tasks, such as counting the occurrences of letters in a word, as illustrated by the inability of many LLMs to count the number of "r" letters in "strawberry". Several works have studied this problem and linked it to the tokenization used by LLMs, to the intrinsic limitations of the attention mechanism, or to the lack of character-level training data. In this paper, we conduct an experimental study to evaluate the relations between the LLM errors when counting letters with 1) the frequency of the word and its components in the training dataset and 2) the complexity of the counting operation. We present a comprehensive analysis of the errors of LLMs when counting letter occurrences by evaluating a representative group of models over a large number of words. The results show a number of consistent trends in the models evaluated: 1) models are capable of recognizing the letters but not counting them; 2) the frequency of the word and tokens in the word does not have a significant impact on the LLM errors; 3) there is a positive correlation of letter frequency with errors, more frequent letters tend to have more counting errors, 4) the errors show a strong correlation with the number of letters or tokens in a word and 5) the strongest correlation occurs with the number of letters with counts larger than one, with most models being unable to correctly count words in which letters appear more than twice.', 'abstract_zh': '大规模语言模型（LLMs）在许多复杂任务上取得了前所未有的性能，能够回答几乎任何主题的问题。然而，它们在一些简单的任务上表现不佳，例如计算单词中某个字母的出现次数。例如，许多LLMs无法准确计算“strawberry”中字母“r”的出现次数。已有研究将这一问题与LLMs所使用的分词、注意力机制的内在限制或字符级别训练数据的缺乏联系起来。在这项研究中，我们进行了实验性研究，评估了LLM在计算字母出现次数时的错误与1）训练数据集中单词及其组成部分的频率和2）计数操作的复杂性之间的关系。我们通过评估一组代表性模型在大量单词上的计字母出现次数，对LLM的错误进行了全面分析。研究结果表明，在评估的模型中存在几个一致的趋势：1）模型能够识别字母，但无法进行计数；2）单词及其组成部分在训练数据集中的频率对LLMs的错误影响不大；3）字母频率与错误之间存在正相关关系，更频繁出现的字母往往有更多的计数错误；4）错误与单词或其组成部分中的字母数或令牌数之间存在强烈的相关性；5）与字母数目的相关性最强的是那些计数超过一个的字母，大多数模型在字母出现次数超过两次的单词计数方面无法正确完成任务。', 'title_zh': '为什么大型语言模型（LLMs）在数字母时显得力不从心？'}
{'arxiv_id': 'arXiv:2412.18621', 'title': 'Investigating the Feasibility of Mitigating Potential Copyright Infringement via Large Language Model Unlearning', 'authors': 'Guangyao Dou', 'link': 'https://arxiv.org/abs/2412.18621', 'abstract': "Pre-trained Large Language Models (LLMs) have demonstrated remarkable capabilities but also pose risks by learning and generating copyrighted material, leading to significant legal and ethical concerns. In a potential real-world scenario, model owners may need to continuously address copyright infringement in order to address requests for content removal that emerge at different time points. One potential way of addressing this is via sequential unlearning, where copyrighted content is removed sequentially as new requests arise. Despite its practical relevance, sequential unlearning in the context of copyright infringement has not been rigorously explored in existing literature. To address this gap, we propose Stable Sequential Unlearning (SSU), a novel framework designed to unlearn copyrighted content from LLMs over multiple time steps. Our approach works by identifying and removing specific weight updates in the model's parameters that correspond to copyrighted content using task vectors. We improve unlearning efficacy by introducing random labeling loss and ensuring the model retains its general-purpose knowledge by adjusting targeted parameters with gradient-based weight saliency. Extensive experimental results show that SSU sometimes achieves an effective trade-off between unlearning efficacy and general-purpose language abilities, outperforming existing baselines, but it's not a cure-all for unlearning copyrighted material.", 'abstract_zh': '预训练大语言模型（LLMs）展现了卓越的能力，但也面临着因学习和生成受版权保护的内容而带来的风险，这引发了重大的法律和伦理问题。在潜在的现实世界场景中，模型所有者可能需要不断应对版权侵权问题，以处理不同时间点出现的内容移除请求。一种潜在的解决方案是通过顺序遗忘，即随着新的请求出现，逐步移除受版权保护的内容。尽管其具有实践相关性，但关于版权侵权的背景下顺序遗忘的问题在现有文献中尚未得到严格探讨。为解决这一空白，我们提出了一种新颖的框架——稳定顺序遗忘（SSU），旨在分阶段从LLMs中遗忘受版权保护的内容。我们的方法通过识别并移除对应受版权保护内容的特定权重更新来实现，这些更新可以通过任务向量来标识。为了提高遗忘效果，我们引入了随机标记损失，并通过基于梯度的权重敏感性调整目标参数以确保模型保留其通用语言知识。广泛的经验表明，SSU有时能够在遗忘效果和通用语言能力之间实现有效的权衡，其性能优于现有的基线方法，但并非治愈所有遗忘受版权保护内容的问题。', 'title_zh': '探究通过大规模语言模型脱忆缓解潜在版权侵权的可能性'}
{'arxiv_id': 'arXiv:2412.18619', 'title': 'Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey', 'authors': 'Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang', 'link': 'https://arxiv.org/abs/2412.18619', 'abstract': 'Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at this https URL', 'abstract_zh': '基于自然语言处理领域的语言建模基础，Next Token Prediction (NTP) 已发展成为一种在不同模态的机器学习任务中具备广泛训练目标的工具，并取得了显著的成功。随着大规模语言模型（LLMs）的进步，这些模型已经能够统一处理文本模态中的理解和生成任务，最近的研究显示，来自其他不同模态的任务也可以有效包含在 NTP 框架中，即将多模态信息转化为 tokens，并在给定上下文的情况下预测下一个 tokens。本文综述引入了一个综合分类法，通过 NTP 的视角，将理解与生成统一在多模态学习中。该提出的分类法涵盖了五个关键方面：多模态分词、MMNTP 模型架构、统一的任务表示、数据集与评估，以及开放性的挑战。这一新的分类法旨在帮助研究人员探索多模态智能。与此相关的一个 GitHub 代码库，收集了最新的论文和资源，可在以下链接找到：[此处链接](此 https URL)。', 'title_zh': '面向多模态智能的下一个令牌预测：一项综述'}
{'arxiv_id': 'arXiv:2412.18618', 'title': 'Exploring Text Representations for Online Misinformation', 'authors': 'Martins Samuel Dogo', 'link': 'https://arxiv.org/abs/2412.18618', 'abstract': 'Mis- and disinformation, commonly collectively called fake news, continue to menace society. Perhaps, the impact of this age-old problem is presently most plain in politics and healthcare. However, fake news is affecting an increasing number of domains. It takes many different forms and continues to shapeshift as technology advances. Though it arguably most widely spreads in textual form, e.g., through social media posts and blog articles. Thus, it is imperative to thwart the spread of textual misinformation, which necessitates its initial detection. This thesis contributes to the creation of representations that are useful for detecting misinformation. Firstly, it develops a novel method for extracting textual features from news articles for misinformation detection. These features harness the disparity between the thematic coherence of authentic and false news stories. In other words, the composition of themes discussed in both groups significantly differs as the story progresses. Secondly, it demonstrates the effectiveness of topic features for fake news detection, using classification and clustering. Clustering is particularly useful because it alleviates the need for a labelled dataset, which can be labour-intensive and time-consuming to amass. More generally, it contributes towards a better understanding of misinformation and ways of detecting it using Machine Learning and Natural Language Processing.', 'abstract_zh': '虚假信息和误导信息（统称为假信息）继续对社会构成威胁。或许，这一古老问题的影响在当今的政治和医疗健康领域最为明显。然而，假信息的影响范围正在逐渐扩展到越来越多的领域。它采取多种形式，并随着技术的进步不断演变。尽管假信息主要通过文本形式传播，例如通过社交媒体帖子和博客文章，因此，阻止文本信息的传播迫在眉睫，而这要求对信息进行初始检测。本论文旨在为识别虚假信息创建有用的表现形式。首先，它开发了一种新颖的方法从新闻文章中提取文本特征以进行虚假信息检测。这些特征利用了真实新闻和虚假新闻主题连贯性之间的差异。换句话说，两种分类中讨论的主题组成在故事发展过程中显著不同。其次，它通过分类和聚类展示了主题特征在检测假新闻的有效性。聚类特别有用，因为它减轻了需要标注数据集的负担，而后者往往是劳动密集型且耗时的。更广泛地说，它为通过机器学习和自然语言处理识别虚假信息提供了更好的理解和方法。', 'title_zh': '探究在线虚假信息的文本表示方法'}
{'arxiv_id': 'arXiv:2412.19792', 'title': 'InfAlign: Inference-aware language model alignment', 'authors': 'Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami', 'link': 'https://arxiv.org/abs/2412.19792', 'abstract': 'Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inference-time methods. We then modify the alignment objective and propose a framework for inference-aware alignment (IAPO). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-and-transform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and best-of-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform baselines that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.', 'abstract_zh': '语言模型对齐已成为培训现代生成语言模型的一个关键步骤。对齐的目标是微调一个参考模型，使得对齐模型的一个样本相对于参考模型的一个样本的胜率最大化，同时要满足KL散度约束。如今，我们越来越多地在推理时间使用算法（如Best-of-N、受控解码、树搜索）进行语言模型解码，而不仅仅是使用标准采样方法。然而，现有的对齐目标并不能捕捉到这样的推理时解码过程。我们显示，在这样的推理时方法方面，现有的对齐框架是次优的。我们随后修改了对齐目标，并提出了一个考虑推理时的对齐框架（IAPO，Inference-Aware Alignment）。我们证明，对于任何推理时解码算法，优化对齐策略相对于参考策略的推理时胜率的最佳解是转换后的奖励问题的标准RLHF问题的解。这促使我们提出了KL正则化的校准和转换强化学习（CTRL，KL-regularized Calibrate-and-Transform RL）算法来解决这一问题，该算法包括一个奖励校准步骤和一个转换后的校准奖励的KL正则化奖励最大化步骤。我们将研究具体应用到了两个重要的推理时策略：Best-of-N采样和Best-of-N破解，其中模型会采样N个响应，然后选择奖励最高或最低的响应。我们为这些策略提出了具体的转换方法，并证明我们的框架在语言模型对齐方面显著优于现有最先进的方法。在实验中，我们的框架在Anthropic友好性和无害性对话基准数据集上的推理时胜率上相较于不考虑推理时解码的基线方法，提高了8%-12%和4%-9%。', 'title_zh': 'InfAlign：基于推断的语言模型对齐'}
{'arxiv_id': 'arXiv:2412.19723', 'title': 'OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis', 'authors': 'Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu', 'link': 'https://arxiv.org/abs/2412.19723', 'abstract': "Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at \\href{this https URL}{OS-Genesis Homepage}.", 'abstract_zh': '由视觉语言模型（VLMs）驱动的图形用户界面（GUI）代理已经展示了与人类类似的计算机控制能力。尽管这些代理在推进数字自动化方面具有实用性，但仍然存在一个关键瓶颈：收集用于训练的高质量轨迹数据。目前收集此类数据的常见方法依赖于人类监督或通过执行预定义任务生成合成数据，这两种方法要么耗费资源，要么无法保证数据质量。此外，这些方法还存在数据多样性不足以及合成数据与现实环境之间差距较大的问题。为了解决这些挑战，我们提出了一种名为OS-Genesis的新型GUI数据合成管道，它逆转了传统的轨迹数据收集流程。OS-Genesis不再依赖于预定义任务，而是让代理首先感知环境并执行逐步交互，然后回顾性地推导出高质量的任务，以实现轨迹级别的探索。我们还使用轨迹奖励模型来确保生成轨迹的质量。实验结果表明，使用OS-Genesis训练GUI代理可以显著提高其在高度挑战性的在线基准测试中的性能。深入分析还验证了OS-Genesis在效率以及数据质量和多样性方面优于现有合成方法。我们的代码、数据和检查点可在OS-Genesis官方网站\\href{该链接}{OS-Genesis官网}获取。', 'title_zh': 'OS-Genesis: 通过反向任务合成自动构建GUI代理轨迹'}
{'arxiv_id': 'arXiv:2412.19707', 'title': 'Toward Adaptive Reasoning in Large Language Models with Thought Rollback', 'authors': 'Sijia Chen, Baochun Li', 'link': 'https://arxiv.org/abs/2412.19707', 'abstract': "Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or thoughts, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., ``hallucinations''. This paper proposes a new reasoning framework, called Thought Rollback (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under ``hallucinations''. The core mechanism of TR is rolling back thoughts, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset.", 'abstract_zh': '大规模语言模型（LLMs）通常被用于通过逐步推理来解决各种任务。然而，中间推理步骤的结构是刚性的且单向的，例如链条、树结构或无环有向图。因此，这种僵化且只能向前的推理可能无法应对复杂的任务，并且当LLM频繁给出错误响应（即“幻觉”）时，会失效。本文提出了一种新的推理框架，称为“思维回滚”（Thought Rollback, TR），允许LLMs在遇到“幻觉”时能够适应性地构建推理结构，并在解决问题过程中进行有效的推理。TR的核心机制是回滚思维，这使得LLMs能够对思维进行错误分析，并回滚到之前的任何错误思维进行修正。在问题解决过程中，通过将这种试错过程包含在提示中以引导LLM，每次回滚都会产生一条更可靠的推理路径。因此，从简单的初始提示且无需人工注释开始，带有TR的LLM能够适配性地逐步探索思维，以找到正确的解决方案。在数学问题和多任务推理的综合实验中，TR在问题解决率和交互成本方面表现出最先进的性能。例如，带有TR的GPT-4在MATH数据集上的解决率为9%优于当前最佳性能。', 'title_zh': '面向大型语言模型的自适应推理机制研究：反向思考方法探索'}
{'arxiv_id': 'arXiv:2412.19583', 'title': 'A Comparative Study of Machine Unlearning Techniques for Image and Text Classification Models', 'authors': 'Omar M. Safa, Mahmoud M. Abdelaziz, Mustafa Eltawy, Mohamed Mamdouh, Moamen Gharib, Salaheldin Eltenihy, Nagia M. Ghanem, Mohamed M. Ismail', 'link': 'https://arxiv.org/abs/2412.19583', 'abstract': 'Machine Unlearning has emerged as a critical area in artificial intelligence, addressing the need to selectively remove learned data from machine learning models in response to data privacy regulations. This paper provides a comprehensive comparative analysis of six state-of-theart unlearning techniques applied to image and text classification tasks. We evaluate their performance, efficiency, and compliance with regulatory requirements, highlighting their strengths and limitations in practical scenarios. By systematically analyzing these methods, we aim to provide insights into their applicability, challenges,and tradeoffs, fostering advancements in the field of ethical and adaptable machine learning.', 'abstract_zh': '机器遗忘作为人工智能中的一个关键领域，旨在响应数据隐私法规的要求，从机器学习模型中选择性地删除已学习的数据。本文对六种最新的遗忘技术在图像分类和文本分类任务中的应用进行了全面的比较分析。我们评估了这些技术的性能、效率以及与法规要求的符合性，强调了它们在实际场景中的优势和局限性。通过系统分析这些方法，我们旨在提供其适用性、挑战和权衡方面的见解，从而促进伦理和适应性机器学习领域的进步。', 'title_zh': '机器学习技术在图像和文本分类模型中的比较研究'}
{'arxiv_id': 'arXiv:2412.19351', 'title': 'ETTA: Elucidating the Design Space of Text-to-Audio Models', 'authors': 'Sang-gil Lee, Zhifeng Kong, Arushi Goel, Sungwon Kim, Rafael Valle, Bryan Catanzaro', 'link': 'https://arxiv.org/abs/2412.19351', 'abstract': "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we set up a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions -- a task that is more challenging than current benchmarks.", 'abstract_zh': '近年来，文本到语音（Text-To-Audio，TTA）合成领域取得了显著的进展，使用户能够通过自然语言提示生成的合成音频丰富其创意工作流程。尽管取得了这些进展，数据、模型架构、训练目标函数以及采样策略对目标基准的影响尚未得到充分理解。为了提供TTA模型设计空间的全面理解，我们设计了一个大规模的经验实验，重点关注扩散和流动匹配模型。我们的贡献包括：1）AF-Synthetic，一个高质量的合成音频描述数据集，由音频理解模型获得；2）对不同架构、训练和推理设计选择在TTA模型中的系统比较；3）对采样方法及其生成质量和推理速度的帕累托曲线进行分析。我们基于这一广泛分析所获得的知识，提出了我们最佳的TTA模型，命名为阐明文本到音频（Elucidated Text-To-Audio，ETTA）。在AudioCaps和MusicCaps上评估时，ETTA在使用公开数据训练的基线模型上提供了改进，同时在使用专有数据训练的模型中具有竞争力。最后，我们展示了ETTA生成复杂且富有想象力的音频描述的能力得到了提升——这是一个比当前基准更具有挑战性的任务。', 'title_zh': 'ETTA：揭示文本到语音模型的设计空间'}
{'arxiv_id': 'arXiv:2412.19350', 'title': 'On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages', 'authors': 'Aleksandar Terzić, Michael Hersche, Giacomo Camposampiero, Thomas Hofmann, Abu Sebastian, Abbas Rahimi', 'link': 'https://arxiv.org/abs/2412.19350', 'abstract': 'Selective state-space models (SSMs) are an emerging alternative to the Transformer, offering the unique advantage of parallel training and sequential inference. Although these models have shown promising performance on a variety of tasks, their formal expressiveness and length generalization properties remain underexplored. In this work, we provide insight into the workings of selective SSMs by analyzing their expressiveness and length generalization performance on regular language tasks, i.e., finite-state automaton (FSA) emulation. We address certain limitations of modern SSM-based architectures by introducing the Selective Dense State-Space Model (SD-SSM), the first selective SSM that exhibits perfect length generalization on a set of various regular language tasks using a single layer. It utilizes a dictionary of dense transition matrices, a softmax selection mechanism that creates a convex combination of dictionary matrices at each time step, and a readout consisting of layer normalization followed by a linear map. We then proceed to evaluate variants of diagonal selective SSMs by considering their empirical performance on commutative and non-commutative automata. We explain the experimental results with theoretical considerations. Our code is available at this https URL.', 'abstract_zh': '选择性状态空间模型（Selective State-Space Models, SSMs）是Transformer的一种新兴替代方案，提供了并行训练和顺序推断的独特优势。尽管这些模型在多种任务中展现出了有前途的性能，但它们的形式表达能力和长度泛化属性仍然未被充分探索。在本研究中，我们通过分析选择性SSM在有限状态自动机（FSA）模拟等常规语言任务上的表达能力和长度泛化性能，来揭示选择性SSM的工作原理。我们通过引入选择性密集状态空间模型（Selective Dense State-Space Model, SD-SSM），在单层结构下实现了对多种常规语言任务的完美长度泛化。该模型利用一个密集转换矩阵词典，以及在每个时间步创建词典矩阵凸组合的Softmax选择机制，同时包含一层归一化和线性映射的读出机制。我们随后通过考查其在可交换自动机和不可交换自动机上的经验性能，来评估对角选择性SSM的变体。我们用理论分析来解释实验结果。相关代码详见：this https URL。', 'title_zh': '选择性状态空间模型在正规语言上的表征能力和长度泛化能力研究'}
{'arxiv_id': 'arXiv:2412.19289', 'title': 'ViPCap: Retrieval Text-Based Visual Prompts for Lightweight Image Captioning', 'authors': 'Taewhan Kim, Soeun Lee, Si-Woo Kim, Dong-Jin Kim', 'link': 'https://arxiv.org/abs/2412.19289', 'abstract': 'Recent lightweight image captioning models using retrieved data mainly focus on text prompts. However, previous works only utilize the retrieved text as text prompts, and the visual information relies only on the CLIP visual embedding. Because of this issue, there is a limitation that the image descriptions inherent in the prompt are not sufficiently reflected in the visual embedding space. To tackle this issue, we propose ViPCap, a novel retrieval text-based visual prompt for lightweight image captioning. ViPCap leverages the retrieved text with image information as visual prompts to enhance the ability of the model to capture relevant visual information. By mapping text prompts into the CLIP space and generating multiple randomized Gaussian distributions, our method leverages sampling to explore randomly augmented distributions and effectively retrieves the semantic features that contain image information. These retrieved features are integrated into the image and designated as the visual prompt, leading to performance improvements on the datasets such as COCO, Flickr30k, and NoCaps. Experimental results demonstrate that ViPCap significantly outperforms prior lightweight captioning models in efficiency and effectiveness, demonstrating the potential for a plug-and-play solution.', 'abstract_zh': '近年来，使用检索数据的轻量级图像描述模型主要关注文本提示。然而，先前的工作仅利用检索到的文本作为文本提示，而视觉信息则依赖于CLIP视觉嵌入。由于这一局限性，图像描述并不充分反映在提示中的内在视觉信息。为了解决这一问题，我们提出了一种新颖的检索文本基视觉提示方法——ViPCap，用于轻量级图像描述。ViPCap 利用带有图像信息的检索到的文本作为视觉提示，以增强模型捕获相关视觉信息的能力。通过将文本提示映射到CLIP空间，并生成多个随机高斯分布，我们的方法利用采样探索随机增强的分布，并有效检索包含图像信息的语义特征。这些检索到的特征被整合到图像中并指定为视觉提示，从而在COCO、Flickr30k和NoCaps等数据集上提高了性能。实验结果表明，ViPCap 在效率和效果上显著优于先前的轻量级描述模型，展示了其作为即插即用解决方案的潜力。', 'title_zh': 'ViPCap：基于检索的文字视觉提示轻量级图像Captioning'}
{'arxiv_id': 'arXiv:2412.19265', 'title': 'Optimizing Multi-Stage Language Models for Effective Text Retrieval', 'authors': 'Quang Hoang Trung, Le Trung Hoang, Nguyen Van Hoang Phuc', 'link': 'https://arxiv.org/abs/2412.19265', 'abstract': 'Efficient text retrieval is critical for applications such as legal document analysis, particularly in specialized contexts like Japanese legal systems. Existing retrieval methods often underperform in such domain-specific scenarios, necessitating tailored approaches. In this paper, we introduce a novel two-phase text retrieval pipeline optimized for Japanese legal datasets. Our method leverages advanced language models to achieve state-of-the-art performance, significantly improving retrieval efficiency and accuracy. To further enhance robustness and adaptability, we incorporate an ensemble model that integrates multiple retrieval strategies, resulting in superior outcomes across diverse tasks. Extensive experiments validate the effectiveness of our approach, demonstrating strong performance on both Japanese legal datasets and widely recognized benchmarks like MS-MARCO. Our work establishes new standards for text retrieval in domain-specific and general contexts, providing a comprehensive solution for addressing complex queries in legal and multilingual environments.', 'abstract_zh': '高效的文本检索对于法律文件分析等应用至关重要，尤其是在诸如日本法律系统这样专门的语境中。现有检索方法往往在这些特定领域中表现不佳，因此需要定制的方法。本文介绍了一种针对日本法律数据集优化的两阶段文本检索流水线。我们的方法利用先进的语言模型，实现了最先进的性能，显著提高了检索效率和准确性。为了进一步增强鲁棒性和适应性，我们引入了一种集成模型，将多种检索策略整合在一起，从而在各种任务上取得了卓越的结果。广泛的实验验证了我们方法的有效性，不仅在日语法律数据集上表现出色，而且在MS-MARCO等广泛认可的基准上也表现出强大的性能。我们的研究为特定领域和通用环境下的文本检索奠定了新的标准，提供了解决法律和多语言环境中复杂查询的综合解决方案。', 'title_zh': '优化多阶段语言模型以实现有效的文本检索'}
{'arxiv_id': 'arXiv:2412.19255', 'title': 'Multi-matrix Factorization Attention', 'authors': 'Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum', 'link': 'https://arxiv.org/abs/2412.19255', 'abstract': "We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.", 'abstract_zh': '我们提出了一种新颖的注意力架构，即多矩阵分解注意力（MFA）和MFA关键值重用（MFA-KR）。现有的标准多头注意力机制（MHA）的高级变体，包括当前最佳方法（SOTA）如MLA，在严格的键值缓存（KV缓存）约束下无法保持与MHA相当的性能表现。MFA通过在查询-键（QK）电路中利用低秩矩阵分解高效地提升注意力头的数量和维度，从而增强模型容量。MFA-KR在此基础上进一步通过值投影重新参数化重用键缓存来减少内存需求。MFA的设计使其在严格的KV缓存预算下能够保持强大的模型容量，而MFA-KR即使在更苛刻的KV缓存限制下也能适用，尽管会有一些性能上的权衡。值得注意的是，在我们的大规模实验中，所提出的架构不仅在性能上优于MLA，还与MHA相当，并且分别将KV缓存使用量减少了最多56%和93.7%。', 'title_zh': '多矩阵因子化注意力'}
{'arxiv_id': 'arXiv:2412.19184', 'title': 'Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching', 'authors': 'Wenjing Chen', 'link': 'https://arxiv.org/abs/2412.19184', 'abstract': "With the rapid development of multimodal learning, the image-text matching task, as a bridge connecting vision and language, has become increasingly important. Based on existing research, this study proposes an innovative visual semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic Embedding (MH-CVSE). This model introduces a multi-head self-attention mechanism based on the consensus-aware visual semantic embedding model (CVSE) to capture information in multiple subspaces in parallel, significantly enhancing the model's ability to understand and represent the complex relationship between images and texts. In addition, we adopt a parameterized feature fusion strategy to flexibly integrate feature information at different levels, further improving the model's expressive power. In terms of loss function design, the MH-CVSE model adopts a dynamic weight adjustment strategy to dynamically adjust the weight according to the loss value itself, so that the model can better balance the contribution of different loss terms during training. At the same time, we introduce a cosine annealing learning rate strategy to help the model converge more stably in the later stages of training. Extensive experimental verification on the Flickr30k dataset shows that the MH-CVSE model achieves better performance than previous methods in both bidirectional image and text retrieval tasks, fully demonstrating its effectiveness and superiority.", 'abstract_zh': '随着多模态学习的迅速发展，图像-文本匹配任务作为连接视觉与语言的桥梁，已经变得越来越重要。基于现有的研究成果，本研究提出了一种创新的视觉语义嵌入模型——多头共识意识视觉语义嵌入（MH-CVSE）。该模型引入了基于共识意识视觉语义嵌入模型（CVSE）的多头自注意力机制，可以在多个子空间中并行捕捉信息，显著增强了模型对图像和文本之间复杂关系的理解和表示能力。此外，我们采用了参数化特征融合策略，灵活地整合不同层次的特征信息，进一步提升了模型的表达能力。在损失函数设计方面，MH-CVSE模型采用了动态权重调整策略，根据损失值本身动态调整权重，使模型在训练过程中能够更好地平衡不同的损失项贡献。同时，我们引入了余弦退火学习率策略，以帮助模型在训练后期更加稳定地收敛。在Flickr30k数据集上的大量实验验证表明，MH-CVSE模型在双向图像和文本检索任务中均优于以前的方法，充分展示了其有效性和优越性。', 'title_zh': '基于多头注意力的动态视觉-语义嵌入增强图像-文本匹配'}
{'arxiv_id': 'arXiv:2412.19178', 'title': 'Reversed in Time: A Novel Temporal-Emphasized Benchmark for Cross-Modal Video-Text Retrieval', 'authors': 'Yang Du, Yuqi Liu, Qin Jin', 'link': 'https://arxiv.org/abs/2412.19178', 'abstract': 'Cross-modal (e.g. image-text, video-text) retrieval is an important task in information retrieval and multimodal vision-language understanding field. Temporal understanding makes video-text retrieval more challenging than image-text retrieval. However, we find that the widely used video-text benchmarks have shortcomings in comprehensively assessing abilities of models, especially in temporal understanding, causing large-scale image-text pre-trained models can already achieve comparable zero-shot performance with video-text pre-trained models. In this paper, we introduce RTime, a novel temporal-emphasized video-text retrieval dataset. We first obtain videos of actions or events with significant temporality, and then reverse these videos to create harder negative samples. We then recruit annotators to judge the significance and reversibility of candidate videos, and write captions for qualified videos. We further adopt GPT-4 to extend more captions based on human-written captions. Our RTime dataset currently consists of 21k videos with 10 captions per video, totalling about 122 hours. Based on RTime, we propose three retrieval benchmark tasks: RTime-Origin, RTime-Hard, and RTime-Binary. We further enhance the use of harder-negatives in model training, and benchmark a variety of video-text models on RTime. Extensive experiment analysis proves that RTime indeed poses new and higher challenges to video-text retrieval. We release our RTime dataset\\footnote{\\url{this https URL}} to further advance video-text retrieval and multimodal understanding research.', 'abstract_zh': '跨模态检索（例如图像文本、视频文本）是信息检索和多模态视觉语言理解领域中的一个重要任务。时间理解使得视频文本检索相较于图像文本检索更具挑战性。然而，我们发现广泛使用的视频文本基准数据集在全面评估模型能力方面存在不足，特别是在时间理解方面，导致大规模的图像文本预训练模型已经能够与视频文本预训练模型实现可比拟的零样本性能。在本文中，我们引入了RTime，这是一种新的强调时间的视频文本检索数据集。我们首先获取具有显著时间性的动作或事件视频，然后反向这些视频以创建更难的负样本。接下来，我们招募标注员来判断候选视频的重要性及其可逆性，并为合格的视频编写描述性文字。我们进一步采用GPT-4基于人类编写的描述性文字扩展更多描述性文字。目前，我们的RTime数据集包含21,000个视频，每个视频有10个描述，总计大约122小时。基于RTime，我们提出了三个检索基准任务：RTime-Origin、RTime-Hard和RTime-Binary。我们进一步增强了模型训练中使用更具挑战性的负样本，并在RTime上对各种视频文本模型进行了基准测试。广泛的实验分析证明，RTime确实为视频文本检索提出了新的和更高的挑战。我们发布了RTime数据集\\footnote{\\url{https://}}以进一步推动视频文本检索和多模态理解的研究。', 'title_zh': '逆向时间：一种新的强调时间维度的跨模态视频-文本检索基准'}
{'arxiv_id': 'arXiv:2412.19155', 'title': 'Referencing Where to Focus: Improving VisualGrounding with Referential Query', 'authors': 'Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang', 'link': 'https://arxiv.org/abs/2412.19155', 'abstract': 'Visual Grounding aims to localize the referring object in an image given a natural language expression. Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes. However, existing research primarily focuses on designing stronger multi-modal decoder, which typically generates learnable queries by random initialization or by using linguistic embeddings. This vanilla query generation approach inevitably increases the learning difficulty for the model, as it does not involve any target-related information at the beginning of decoding. Furthermore, they only use the deepest image feature during the query learning process, overlooking the importance of features from other levels. To address these issues, we propose a novel approach, called RefFormer. It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder. By incorporating the referential query into the decoder, we can effectively mitigate the learning difficulty of the decoder, and accurately concentrate on the target object. Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.', 'abstract_zh': '视觉定位（Visual Grounding）的目标是给定自然语言表达，在图像中定位指涉的对象。基于DETR的方法在视觉定位领域的最新进展引起了广泛关注，因为这些方法可以直接预测目标对象的坐标，无需依赖预生成的候选提案或预定义的锚框。然而，现有的研究主要集中在设计更强的多模态解码器，通常通过随机初始化或使用语言嵌入生成可学习查询。这种默认的查询生成方法不可避免地增加了模型的学习难度，因为开始解码时并不包含任何与目标相关的信息。此外，在查询学习过程中，他们仅使用最深的图像特征，忽视了来自其他层级特征的重要性。为了解决这些问题，我们提出了一种新颖的方法，称为RefFormer。它包含一个可以无缝集成到CLIP中的查询适应模块，生成参考查询以为解码器提供先验上下文，同时包含一个特定任务的解码器。通过将参考查询融入解码器中，我们可以有效缓解解码器的学习难度，并准确集中于目标对象。此外，我们提出的查询适应模块还可以作为适配器，保留CLIP中的丰富知识，而不需要调整主干网络的参数。广泛实验表明，我们提出的方法在五个视觉定位基准数据集上优于现有最先进的方法，显示出其有效性和效率。', 'title_zh': '聚焦于何处：改进视觉定位的参照查询方法'}
{'arxiv_id': 'arXiv:2412.19087', 'title': 'MoPD: Mixture-of-Prompts Distillation for Vision-Language Models', 'authors': 'Yang Chen, Shuai Fu, Yu Zhang', 'link': 'https://arxiv.org/abs/2412.19087', 'abstract': 'Soft prompt learning methods are effective for adapting vision-language models (VLMs) to downstream tasks. Nevertheless, empirical evidence reveals a tendency of existing methods that they overfit seen classes and exhibit degraded performance on unseen classes. This limitation is due to the inherent bias in the training data towards the seen classes. To address this issue, we propose a novel soft prompt learning method, named Mixture-of-Prompts Distillation (MoPD), which can effectively transfer useful knowledge from hard prompts manually hand-crafted (a.k.a. teacher prompts) to the learnable soft prompt (a.k.a. student prompt), thereby enhancing the generalization ability of soft prompts on unseen classes. Moreover, the proposed MoPD method utilizes a gating network that learns to select hard prompts used for prompt distillation. Extensive experiments demonstrate that the proposed MoPD method outperforms state-of-the-art baselines especially on on unseen classes.', 'abstract_zh': '软提示学习方法可以有效地使视觉-语言模型（VLMs）适应下游任务。然而，实证研究表明，现有方法倾向于过拟合已见过的类别，并在未见过的类别上表现出较差的性能。这一限制是由于训练数据中存在对已见过类别的固有偏差。为了解决这个问题，我们提出了一种新的软提示学习方法，称为混合提示蒸馏（MoPD），它可以有效地将手工构建的硬提示（即教师提示）中的有用知识传递给可学习的软提示（即学生提示），从而增强软提示在未见过类别的泛化能力。此外，提出的MoPD方法利用了一个门控网络，用于学习选择用于提示蒸馏的硬提示。广泛实验证明，在未见过的类别上，所提出的MoPD方法明显优于最先进的基线方法。', 'title_zh': 'MoPD: 混合提示蒸馏方法在视觉语言模型中的应用'}
{'arxiv_id': 'arXiv:2412.19072', 'title': 'Robust Speech and Natural Language Processing Models for Depression Screening', 'authors': 'Y. Lu, A. Harati, T. Rutowski, R. Oliveira, P. Chlebek, E. Shriberg', 'link': 'https://arxiv.org/abs/2412.19072', 'abstract': 'Depression is a global health concern with a critical need for increased patient screening. Speech technology offers advantages for remote screening but must perform robustly across patients. We have described two deep learning models developed for this purpose. One model is based on acoustics; the other is based on natural language processing. Both models employ transfer learning. Data from a depression-labeled corpus in which 11,000 unique users interacted with a human-machine application using conversational speech is used. Results on binary depression classification have shown that both models perform at or above AUC=0.80 on unseen data with no speaker overlap. Performance is further analyzed as a function of test subset characteristics, finding that the models are generally robust over speaker and session variables. We conclude that models based on these approaches offer promise for generalized automated depression screening.', 'abstract_zh': '抑郁症是全球性的健康问题，亟需增加患者的筛查。语音技术为远程筛查提供了优势，但必须在不同患者中表现出色。我们描述了两种为这一目的开发的深度学习模型。一种模型基于声学特征；另一种模型基于自然语言处理。两种模型均采用了迁移学习。使用的数据来自一个标记了抑郁症的语料库，其中有11,000名独特用户通过对话语音与人机应用进行了交互。在二分类抑郁诊断中，两种模型在未见过的数据集上均实现了AUC≥0.80的性能，且使用者重合度为零。我们进一步分析了模型在测试子集特性上的性能，发现模型在讲者和会话变量方面通常表现出良好的稳健性。我们得出结论，基于这些方法的模型有望用于泛化的自动抑郁症筛查。', 'title_zh': '鲁棒性的语音和自然语言处理模型在抑郁症筛查中的应用'}
{'arxiv_id': 'arXiv:2412.18910', 'title': 'AdaEAGLE: Optimizing Speculative Decoding via Explicit Modeling of Adaptive Draft Structures', 'authors': 'Situo Zhang, Hankun Wang, Da Ma, Zichen Zhu, Lu Chen, Kunyao Lan, Kai Yu', 'link': 'https://arxiv.org/abs/2412.18910', 'abstract': 'Speculative Decoding (SD) is a popular lossless technique for accelerating the inference of Large Language Models (LLMs). We show that the decoding speed of SD frameworks with static draft structures can be significantly improved by incorporating context-aware adaptive draft structures. However, current studies on adaptive draft structures are limited by their performance, modeling approaches, and applicability. In this paper, we introduce AdaEAGLE, the first SD framework that explicitly models adaptive draft structures. AdaEAGLE leverages the Lightweight Draft Length Predictor (LDLP) module to explicitly predict the optimal number of draft tokens during inference to guide the draft model. It achieves comparable speedup results without manual thresholds and allows for deeper, more specialized optimizations. Moreover, together with threshold-based strategies, AdaEAGLE achieves a $1.62\\times$ speedup over the vanilla AR decoding and outperforms fixed-length SotA baseline while maintaining output quality.', 'abstract_zh': '猜测解码（Speculative Decoding, SD）是一种流行的无损加速技术，用于加速大型语言模型（LLMs）的推理过程。研究表明，通过结合上下文自适应草稿结构，具有静态草稿结构的SD框架的解码速度可以显著提高。然而，当前关于自适应草稿结构的研究受到了性能、建模方法和适用性等方面的限制。在本文中，我们介绍了AdaEAGLE，这是第一个明确建模自适应草稿结构的SD框架。AdaEAGLE利用轻量级草稿长度预测器（LDLP）模块，在推理过程中明确预测最优草稿令牌数，以引导草稿模型。它无需手动设置阈值即可实现类似的速度提升结果，并允许进行更深、更专门的优化。此外，结合基于阈值的策略，AdaEAGLE在无损AR解码的基础上实现了1.62倍的速度提升，并在保持输出质量的同时超越了固定长度的最新基线。', 'title_zh': 'AdaEAGLE：通过显式建模自适应草稿结构来优化投机解码'}
{'arxiv_id': 'arXiv:2412.18748', 'title': 'Towards Expressive Video Dubbing with Multiscale Multimodal Context Interaction', 'authors': 'Yuan Zhao, Rui Liu, Gaoxiang Cong', 'link': 'https://arxiv.org/abs/2412.18748', 'abstract': "Automatic Video Dubbing (AVD) generates speech aligned with lip motion and facial emotion from scripts. Recent research focuses on modeling multimodal context to enhance prosody expressiveness but overlooks two key issues: 1) Multiscale prosody expression attributes in the context influence the current sentence's prosody. 2) Prosody cues in context interact with the current sentence, impacting the final prosody expressiveness. To tackle these challenges, we propose M2CI-Dubber, a Multiscale Multimodal Context Interaction scheme for AVD. This scheme includes two shared M2CI encoders to model the multiscale multimodal context and facilitate its deep interaction with the current sentence. By extracting global and local features for each modality in the context, utilizing attention-based mechanisms for aggregation and interaction, and employing an interaction-based graph attention network for fusion, the proposed approach enhances the prosody expressiveness of synthesized speech for the current sentence. Experiments on the Chem dataset show our model outperforms baselines in dubbing expressiveness. The code and demos are available at \\textcolor[rgb]{0.93,0.0,0.47}{this https URL}.", 'abstract_zh': '自动视频配音（AVD）从脚本中生成与唇形和面部情感对齐的语音。近期的研究集中在通过建模多模态上下文来增强音调表现力，但忽略了两个关键问题：1）多尺度的音调表现属性对当前句子的音调有影响；2）多模态上下文中的音调线索与当前句子相互作用，影响最终的音调表现力。为解决这些问题，我们提出了一种多尺度多模态上下文交互方案（M2CI-Dubber）用于AVD。该方案包括两个共享的M2CI编码器，用于建模多尺度多模态上下文，并促进上下文与当前句子的深层次交互。通过提取上下文中每个模态的全局和局部特征，并利用基于注意力机制的聚合和交互方式，以及采用基于交互的图注意力网络进行融合，所提出的方法增强了当前句子生成语音的音调表现力。在Chem数据集上的实验表明，与基线模型相比，我们的模型在配音表现力方面表现更好。代码和演示可以在<此URL>获取。', 'title_zh': '面向多尺度多模态上下文交互的表达性视频配音研究'}
{'arxiv_id': 'arXiv:2412.18693', 'title': 'Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning', 'authors': 'Alex Beutel, Kai Xiao, Johannes Heidecke, Lilian Weng', 'link': 'https://arxiv.org/abs/2412.18693', 'abstract': 'Automated red teaming can discover rare model failures and generate challenging examples that can be used for training or evaluation. However, a core challenge in automated red teaming is ensuring that the attacks are both diverse and effective. Prior methods typically succeed in optimizing either for diversity or for effectiveness, but rarely both. In this paper, we provide methods that enable automated red teaming to generate a large number of diverse and successful attacks.\nOur approach decomposes the task into two steps: (1) automated methods for generating diverse attack goals and (2) generating effective attacks for those goals. While we provide multiple straightforward methods for generating diverse goals, our key contributions are to train an RL attacker that both follows those goals and generates diverse attacks for those goals. First, we demonstrate that it is easy to use a large language model (LLM) to generate diverse attacker goals with per-goal prompts and rewards, including rule-based rewards (RBRs) to grade whether the attacks are successful for the particular goal. Second, we demonstrate how training the attacker model with multi-step RL, where the model is rewarded for generating attacks that are different from past attempts further increases diversity while remaining effective. We use our approach to generate both prompt injection attacks and prompts that elicit unsafe responses. In both cases, we find that our approach is able to generate highly-effective and considerably more diverse attacks than past general red-teaming approaches.', 'abstract_zh': '自动化红队演练可以发现罕见的模型故障并生成具有挑战性的实例，这些实例可用于训练或评估。然而，自动化红队演练的核心挑战在于确保攻击既多样化又有效。以往的方法通常在优化多样性和有效性之间取得成功，但很少两者兼顾。在本文中，我们提供了一种方法，使自动化红队演练能够生成大量多样且成功的攻击。\n\n我们的方法将任务分解为两个步骤：（1）生成多样化的攻击目标的自动化方法；（2）为这些目标生成有效的攻击。尽管我们提供了多种简单的方法来生成多样化的目标，但我们的关键贡献在于训练一个深度强化学习（RL）攻击者，该攻击者不仅遵循这些目标，还为每个目标生成多样化的攻击。首先，我们证明使用大型语言模型（LLMs）通过每个目标的提示和奖励（包括基于规则的奖励RBR，以评估攻击是否针对特定目标成功）来生成多样化的攻击者目标是很容易的。其次，我们展示通过使用多步强化学习训练攻击模型，其中模型因其生成的攻击与过去尝试不同而得到奖励，在保持有效性的同时还能增加多样性。我们使用我们的方法生成了注入提示攻击以及引发不安全响应的提示。在两种情况下，我们发现我们的方法生成的攻击不仅更为有效，且在多样性方面也明显优于以往的一般红队方法。', 'title_zh': '使用自动生成奖励和多步强化学习的多样化和有效的红队演练'}
{'arxiv_id': 'arXiv:2412.18669', 'title': 'Advancing Explainability in Neural Machine Translation: Analytical Metrics for Attention and Alignment Consistency', 'authors': 'Anurag Mishra', 'link': 'https://arxiv.org/abs/2412.18669', 'abstract': 'Neural Machine Translation (NMT) models have shown remarkable performance but remain largely opaque in their decision making processes. The interpretability of these models, especially their internal attention mechanisms, is critical for building trust and verifying that these systems behave as intended. In this work, we introduce a systematic framework to quantitatively evaluate the explainability of an NMT model attention patterns by comparing them against statistical alignments and correlating them with standard machine translation quality metrics. We present a set of metrics attention entropy and alignment agreement and validate them on an English-German test subset from WMT14 using a pre trained mT5 model. Our results indicate that sharper attention distributions correlate with improved interpretability but do not always guarantee better translation quality. These findings advance our understanding of NMT explainability and guide future efforts toward building more transparent and reliable machine translation systems.', 'abstract_zh': '神经机器翻译（NMT）模型在性能上表现出色，但在决策过程方面仍显得较为不透明。这些模型的可解释性，特别是它们的内部注意力机制，对于建立信任并验证这些系统是否按预期行为至关重要。本研究中，我们引入了一个系统框架，通过将NMT模型的注意力模式与统计对齐进行比较，并与标准的机器翻译质量指标相关联，来定量评估这些模式的可解释性。我们提出了注意力熵和对齐一致性的度量标准，并使用WMT14的英德测试子集和预训练的mT5模型进行了验证。我们的结果表明，更尖锐的注意力分布与更好的解释性相关联，但并不总是确保更好的翻译质量。这些发现推进了我们对NMT解释性的理解，并为未来构建更透明和可靠的机器翻译系统指明了方向。', 'title_zh': '增强神经机器翻译的可解释性：注意力和对齐一致性分析性指标探讨'}
{'arxiv_id': 'arXiv:2412.18614', 'title': 'Investigating Acoustic-Textual Emotional Inconsistency Information for Automatic Depression Detection', 'authors': 'Rongfeng Su, Changqing Xu, Xinyi Wu, Feng Xu, Xie Chen, Lan Wangt, Nan Yan', 'link': 'https://arxiv.org/abs/2412.18614', 'abstract': "Previous studies have demonstrated that emotional features from a single acoustic sentiment label can enhance depression diagnosis accuracy. Additionally, according to the Emotion Context-Insensitivity theory and our pilot study, individuals with depression might convey negative emotional content in an unexpectedly calm manner, showing a high degree of inconsistency in emotional expressions during natural conversations. So far, few studies have recognized and leveraged the emotional expression inconsistency for depression detection. In this paper, a multimodal cross-attention method is presented to capture the Acoustic-Textual Emotional Inconsistency (ATEI) information. This is achieved by analyzing the intricate local and long-term dependencies of emotional expressions across acoustic and textual domains, as well as the mismatch between the emotional content within both domains. A Transformer-based model is then proposed to integrate this ATEI information with various fusion strategies for detecting depression. Furthermore, a scaling technique is employed to adjust the ATEI feature degree during the fusion process, thereby enhancing the model's ability to discern patients with depression across varying levels of severity. To best of our knowledge, this work is the first to incorporate emotional expression inconsistency information into depression detection. Experimental results on a counseling conversational dataset illustrate the effectiveness of our method.", 'abstract_zh': '先前的研究已经证明，单一声学情感标签中的情感特征能够提高抑郁症诊断的准确性。另外，根据情绪情景无关性理论以及我们的初步研究结果，抑郁症患者可能以出乎意料的平静方式传达负面情感内容，在自然对话中表现出情绪表达的高度不一致性。目前，很少有研究能够识别并利用这种情绪表达的不一致性来进行抑郁检测。本文提出了一种多模态跨注意力方法，旨在捕捉声学-文本情绪不一致性（ATEI）信息。这一方法通过分析声学和文本领域中情绪表达的复杂局部与长期依赖关系以及两个领域中情绪内容的不匹配情况来实现。随后提出了一种基于Transformer的模型，并结合多种融合策略将ATEI信息融入其中，以检测抑郁症。此外，在融合过程中采用了一种缩放技术来调整ATEI特征的强度，从而增强模型在不同严重程度患者中的识别能力。据我们所知，这是首次将情绪表达的不一致性信息引入抑郁症检测的研究。在咨询对话数据集上的实验结果表明了我们方法的有效性。', 'title_zh': '探究声学-文本情绪不一致性信息以实现自动抑郁检测'}
{'arxiv_id': 'arXiv:2412.18613', 'title': 'The Illusion-Illusion: Vision Language Models See Illusions Where There are None', 'authors': 'Tomer Ullman', 'link': 'https://arxiv.org/abs/2412.18613', 'abstract': 'Illusions are entertaining, but they are also a useful diagnostic tool in cognitive science, philosophy, and neuroscience. A typical illusion shows a gap between how something "really is" and how something "appears to be", and this gap helps us understand the mental processing that lead to how something appears to be. Illusions are also useful for investigating artificial systems, and much research has examined whether computational models of perceptions fall prey to the same illusions as people. Here, I invert the standard use of perceptual illusions to examine basic processing errors in current vision language models. I present these models with illusory-illusions, neighbors of common illusions that should not elicit processing errors. These include such things as perfectly reasonable ducks, crooked lines that truly are crooked, circles that seem to have different sizes because they are, in fact, of different sizes, and so on. I show that many current vision language systems mistakenly see these illusion-illusions as illusions. I suggest that such failures are part of broader failures already discussed in the literature.', 'abstract_zh': '幻觉虽然令人娱乐，但在认知科学、哲学和神经科学中也是一项有用的诊断工具。一个典型的幻觉展示了“实际情况”与“表面看起来的情况”之间的差距，而这种差距有助于我们理解导致某种事物看起来是某样东西的心理过程。幻觉另一个有用之处在于，它们可用于研究人工系统，许多研究探讨了感知计算模型是否也会受到与人类相同的幻觉影响。在此，我将反向利用感知幻觉，研究当前视觉语言模型的基本处理错误。我将这些模型暴露于幻觉类似物，这些幻觉类似物不应引起处理错误。这包括看起来非常合理的鸭子、实际歪曲的线、看似大小不同的实际上确实大小不同的圆等。我展示了当前许多视觉语言系统错误地将这些幻觉类似物视为幻觉。我建议，这些失败是文献中已经讨论的更广泛失败的一部分。', 'title_zh': '幻觉之幻：视觉语言模型在无任何实际幻觉存在的地方看到了幻觉'}
