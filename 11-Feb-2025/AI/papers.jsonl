{'arxiv_id': 'arXiv:2502.06773', 'title': 'On the Emergence of Thinking in LLMs I: Searching for the Right Intuition', 'authors': 'Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, Huseyin A. Inan', 'link': 'https://arxiv.org/abs/2502.06773', 'abstract': "Recent AI advancements, such as OpenAI's new models, are transforming LLMs into LRMs (Large Reasoning Models) that perform reasoning during inference, taking extra time and compute for higher-quality outputs. We aim to uncover the algorithmic framework for training LRMs. Methods like self-consistency, PRM, and AlphaZero suggest reasoning as guided search. We ask: what is the simplest, most scalable way to enable search in LLMs?\nWe propose a post-training framework called Reinforcement Learning via Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with human or synthetic demonstrations of the reasoning process, (2) using an exploration reward signal to encourage diverse and efficient reasoning behaviors, and (3) RL training with an outcome verifier to ensure correctness while preventing reward hacking. Our key innovation is to decouple exploration and correctness signals during PPO training, carefully balancing them to improve performance and efficiency.\nEmpirical studies in the math domain show that RLSP improves reasoning. On the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500 test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due to RLSP. However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification. These findings demonstrate that RLSP framework might be enough to enable emergence of complex reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why RLSP search strategy is more suitable for LLMs inspired by a remarkable result that says CoT provably increases computational power of LLMs, which grows as the number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.", 'abstract_zh': '近年来，如OpenAI新模型等AI技术进步正在将大规模语言模型（LLMs）转变为大规模推理模型（LRMs），这些模型在推理过程中会花费额外的时间和计算资源以生成高质量的输出。我们的目标是揭开训练LRMs的算法框架。诸如自我一致性、PRM和AlphaZero等方法表明，推理是一种引导搜索过程。我们提出的问题是：如何以最简单且最具扩展性的方法在LLMs中实现搜索功能？\n\n我们提出了一种后训练框架，称为通过自我博弈进行强化学习（RLSP）。RLSP包含三个步骤：（1）通过人工或合成示例的监督微调，演示推理过程；（2）使用探索奖励信号鼓励多样性和高效的推理行为；（3）通过结果验证器进行强化学习训练，确保正确性的同时防止奖励作弊。我们的关键创新在于，在PPO训练过程中，将探索信号和正确性信号解耦，并精心平衡它们以提高性能和效率。\n\n在数学领域的实证研究显示了RLSP对推理的提升效果。对于Llama-3.1-8B-Instruct模型，在MATH-500测试集上，RLSP可以提升23%的性能；对于AIME 2024数学问题，Qwen2.5-32B-Instruct由于RLSP的使用而提升了10%。然而，这项工作的更为重要的发现是，使用RLSP训练的模型，即使使用简单的探索奖励信号鼓励模型采取更多的中间步骤，也展示了回溯、理念探索和验证等新兴行为。这些发现表明，在扩展时，RLSP框架可能足以使LLMs生成复杂的推理能力。最后，我们提出了一个理论，说明RLSP搜索策略为何在LLMs中更为适用，这得益于一个显著的结果：即通过链式思考（CoT），LLMs的计算能力可以增加，并且随着CoT步骤数的增多而增长。这一理论借鉴了相关的研究结果，例如[1]中的链式思考如何增强LLMs的计算能力，以及[2]中关于表达性的讨论。\n\n[1] Li, Yixing, et al. "Chain-of-thought reasoning enhances large language models." arXiv preprint arXiv:2403.09243 (2024).\n\n[2] Merrill, Daniel, et al. "Expressive Metalearning with Tokens." arXiv preprint arXiv:2305.13254 (2023).', 'title_zh': 'LLMs中思维涌现的研究I：寻找正确的直觉'}
{'arxiv_id': 'arXiv:2502.06727', 'title': 'Application of Artificial Intelligence (AI) in Civil Engineering', 'authors': 'Temitope Funmilayo Awolusi, Bernard Chukwuemeka Finbarrs-Ezema, Isaac Munachimdinamma Chukwudulue, Marc Azab', 'link': 'https://arxiv.org/abs/2502.06727', 'abstract': 'Hard computing generally deals with precise data, which provides ideal solutions to problems. However, in the civil engineering field, amongst other disciplines, that is not always the case as real-world systems are continuously changing. Here lies the need to explore soft computing methods and artificial intelligence to solve civil engineering shortcomings. The integration of advanced computational models, including Artificial Neural Networks (ANNs), Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has revolutionized the domain of civil engineering. These models have significantly advanced diverse sub-fields by offering innovative solutions and improved analysis capabilities. Sub-fields such as: slope stability analysis, bearing capacity, water quality and treatment, transportation systems, air quality, structural materials, etc. ANNs predict non-linearities and provide accurate estimates. Fuzzy logic uses an efficient decision-making process to provide a more precise assessment of systems. Lastly, while GAs optimizes models (based on evolutionary processes) for better outcomes, probabilistic reasoning lowers their statistical uncertainties.', 'abstract_zh': '硬计算通常处理精确的数据，为问题提供理想解决方案。然而，在包括土木工程在内的其他领域中，并非总是如此，因为实际系统是不断发展变化的。因此，有必要探索软计算方法和人工智能以解决土木工程中的不足之处。高级计算模型的集成，包括人工神经网络（ANNs）、模糊逻辑、遗传算法（GAs）和概率推理，已经极大地变革了土木工程领域。这些模型通过提供创新解决方案和增强分析能力，显著推进了众多子领域的进步。这些子领域包括：边坡稳定性分析、承载能力、水质和处理、交通系统、空气质量、结构材料等。\n\n人工神经网络预测非线性关系并提供准确的估计。模糊逻辑采用高效决策过程对系统进行更精确的评估。最后，遗传算法基于进化过程优化模型以获得更好的结果，而概率推理则降低了模型的统计不确定性。', 'title_zh': '人工智能（AI）在土木工程中的应用'}
{'arxiv_id': 'arXiv:2502.06656', 'title': 'A Frontier AI Risk Management Framework: Bridging the Gap Between Current AI Practices and Established Risk Management', 'authors': 'Simeon Campos, Henry Papadatos, Fabien Roger, Chloé Touzet, Malcolm Murray, Otter Quarks', 'link': 'https://arxiv.org/abs/2502.06656', 'abstract': "The recent development of powerful AI systems has highlighted the need for robust risk management frameworks in the AI industry. Although companies have begun to implement safety frameworks, current approaches often lack the systematic rigor found in other high-risk industries. This paper presents a comprehensive risk management framework for the development of frontier AI that bridges this gap by integrating established risk management principles with emerging AI-specific practices. The framework consists of four key components: (1) risk identification (through literature review, open-ended red-teaming, and risk modeling), (2) risk analysis and evaluation using quantitative metrics and clearly defined thresholds, (3) risk treatment through mitigation measures such as containment, deployment controls, and assurance processes, and (4) risk governance establishing clear organizational structures and accountability. Drawing from best practices in mature industries such as aviation or nuclear power, while accounting for AI's unique challenges, this framework provides AI developers with actionable guidelines for implementing robust risk management. The paper details how each component should be implemented throughout the life-cycle of the AI system - from planning through deployment - and emphasizes the importance and feasibility of conducting risk management work prior to the final training run to minimize the burden associated with it.", 'abstract_zh': '近年来，强大的人工智能系统的快速发展凸显了人工智能行业中建立稳健的风险管理体系的需求。尽管企业已经开始实施安全框架，但当前的方法往往缺乏其他高风险行业所具有的系统严谨性。本文提出了一套综合性的风险管理体系，旨在通过结合成熟的风险管理原则与新兴的特定于人工智能的做法来填补这一缺口。该框架包括四个关键组成部分：（1）风险识别（通过文献回顾、开放式红队测试和风险建模），（2）使用定量指标和明确的阈值进行风险分析和评估，（3）通过减轻措施（如控制、部署控制和保证流程）进行风险处理，以及（4）通过建立清晰的组织结构和问责机制进行风险管理。该框架借鉴了航空或核能等成熟行业的最佳实践，同时考虑到人工智能的独特挑战，为人工智能开发者提供了实施稳健风险管理的具体指南。文章详细阐述了每个组成部分在整个人工智能系统生命周期（从规划到部署）中的实施方法，并强调在最终训练之前开展风险管理工作的重要性及其可行性，以减轻相关负担。', 'title_zh': '前沿人工智能风险管理框架：连接当前人工智能实践与 Established 风险管理的桥梁'}
{'arxiv_id': 'arXiv:2502.06655', 'title': 'Unbiased Evaluation of Large Language Models from a Causal Perspective', 'authors': 'Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, Jiang Zhu', 'link': 'https://arxiv.org/abs/2502.06655', 'abstract': 'Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of this http URL experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.', 'abstract_zh': '基准污染已成为大型语言模型（LLM）评估社区中的一个重要关注点。先前的“代理作为评估者”方法通过让代理参与问题生成来解决这一问题。尽管这些方法取得了成功，但其中的偏见仍主要未被探讨。在这篇论文中，我们提出了一种评估偏见的理论框架，为设计无偏评估协议提供了宝贵的见解。此外，我们通过精心设计的探针任务，在一个简化的“代理作为评估者”设置中，识别了两种类型的偏见。为应对这些问题，我们提出了一种“无偏评估者”评估协议，能够提供更全面、无偏且可解释的评估。实验结果表明，当前的LLM在许多方面仍有改进的空间。此外，我们还证明了“无偏评估者”不仅能提供基准污染的强有力证据，还能提供可解释的评估结果。', 'title_zh': '从因果角度进行大语言模型的无偏评估'}
{'arxiv_id': 'arXiv:2502.06574', 'title': 'On the Impact of the Utility in Semivalue-based Data Valuation', 'authors': 'Mélissa Tamine, Benjamin Heymann, Patrick Loiseau, Maxime Vono', 'link': 'https://arxiv.org/abs/2502.06574', 'abstract': 'Semivalue-based data valuation in machine learning (ML) quantifies the contribution of individual data points to a downstream ML task by leveraging principles from cooperative game theory and the notion of utility. While this framework has been used in practice for assessing data quality, our experiments reveal inconsistent valuation outcomes across different utilities, albeit all related to ML performance. Beyond raising concerns about the reliability of data valuation, this inconsistency is challenging to interpret, as it stems from the complex interaction of the utility with data points and semivalue weights, which has barely been studied in prior work. In this paper, we take a first step toward clarifying the utility impact on semivalue-based data valuation. Specifically, we provide geometric interpretations of this impact for a broad family of classification utilities, which includes the accuracy and the arithmetic mean. We introduce the notion of spatial signatures: given a semivalue, data points can be embedded into a two-dimensional space, and utility functions map to the dual of this space. This geometric perspective separates the influence of the dataset and semivalue from that of the utility, providing a theoretical explanation for the experimentally observed sensitivity of valuation outcomes to the utility choice.', 'abstract_zh': '基于半值的数据估值在机器学习（ML）中通过合作博弈理论的原则和效用的概念，量化了单个数据点对下游ML任务的贡献。尽管这一框架已经在实践中被用于评估数据质量，我们的实验揭示出，尽管这些效用都与ML性能相关，但在不同效用下的估值结果仍然存在不一致。这种不一致性不仅引发了对数据估值可靠性的担忧，而且还难以解释，因为它源自效用与数据点和半值权重之间复杂的相互作用，而这一点在先前的工作中几乎未被研究过。在本文中，我们朝着阐明效应对基于半值的数据估值影响迈出第一步。具体来说，我们为一大类分类效用提供了几何解释，包括准确率和算术平均数。我们引入了空间签名这一概念：给定一个半值，数据点可以被嵌入到二维空间中，而效用函数则映射到该空间的对偶空间。这种几何视角将数据集和半值的影响与效用的影响区分开来，提供了对实验观察到的估值结果对效用选择敏感性的理论解释。', 'title_zh': '基于半值函数的数据价值评估中效用的作用 analysing the impact of utility in semivalue-based data valuation'}
{'arxiv_id': 'arXiv:2502.06559', 'title': 'Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation', 'authors': 'Maria Eriksson, Erasmo Purificato, Arman Noroozian, Joao Vinagre, Guillaume Chaslot, Emilia Gomez, David Fernandez-Llorca', 'link': 'https://arxiv.org/abs/2502.06559', 'abstract': 'Quantitative Artificial Intelligence (AI) Benchmarks have emerged as fundamental tools for evaluating the performance, capability, and safety of AI models and systems. Currently, they shape the direction of AI development and are playing an increasingly prominent role in regulatory frameworks. As their influence grows, however, so too does concerns about how and with what effects they evaluate highly sensitive topics such as capabilities, including high-impact capabilities, safety and systemic risks. This paper presents an interdisciplinary meta-review of about 100 studies that discuss shortcomings in quantitative benchmarking practices, published in the last 10 years. It brings together many fine-grained issues in the design and application of benchmarks (such as biases in dataset creation, inadequate documentation, data contamination, and failures to distinguish signal from noise) with broader sociotechnical issues (such as an over-focus on evaluating text-based AI models according to one-time testing logic that fails to account for how AI models are increasingly multimodal and interact with humans and other technical systems). Our review also highlights a series of systemic flaws in current benchmarking practices, such as misaligned incentives, construct validity issues, unknown unknowns, and problems with the gaming of benchmark results. Furthermore, it underscores how benchmark practices are fundamentally shaped by cultural, commercial and competitive dynamics that often prioritise state-of-the-art performance at the expense of broader societal concerns. By providing an overview of risks associated with existing benchmarking procedures, we problematise disproportionate trust placed in benchmarks and contribute to ongoing efforts to improve the accountability and relevance of quantitative AI benchmarks within the complexities of real-world scenarios.', 'abstract_zh': '定量人工智能（AI）基准测试已经成为了评估AI模型和系统性能、能力和安全性的基本工具。目前，这些基准测试正在塑造AI的发展方向，并在监管框架中发挥着越来越重要的作用。然而，随着这些基准测试的影响不断扩大，人们也越来越多地关注它们在评估诸如能力（包括高影响能力）、安全性和系统性风险等高度敏感话题时的方法和效果。本文对过去十年中大约100篇讨论定量基准测试实践不足的研究进行了跨学科综述。综述涵盖了在基准测试设计和应用中许多具体问题（如数据集创建中的偏见、缺乏详尽文档、数据污染、混淆信号与噪音的问题）以及更为广泛的社会和技术问题（如过于专注于按照一次性测试逻辑评估基于文本的AI模型，未能考虑AI模型正变得多模态且日益与人类和其他技术系统交互的现实）。我们的综述还指出了当前基准测试实践中的一系列系统性缺陷，如对齐激励不一致、构建效度问题、“已知未知”和“未知未知”以及基准测试结果被操控的问题。此外，本文还强调了基准测试实践受到文化、商业和竞争动态的影响，这些动态往往优先考虑最新的技术水平，而忽视更广泛的公共利益。通过概述现有基准测试程序相关的风险，本文质疑了对基准测试的过度信赖，并为改进定量AI基准测试在复杂现实场景中的问责制和相关性做出了贡献。', 'title_zh': '《我们能信任AI基准测试吗？当前AI评估中的跨学科问题审查》'}
{'arxiv_id': 'arXiv:2502.06523', 'title': 'Tighter Value-Function Approximations for POMDPs', 'authors': 'Merlijn Krale, Wietze Koops, Sebastian Junges, Thiago D. Simão, Nils Jansen', 'link': 'https://arxiv.org/abs/2502.06523', 'abstract': 'Solving partially observable Markov decision processes (POMDPs) typically requires reasoning about the values of exponentially many state beliefs. Towards practical performance, state-of-the-art solvers use value bounds to guide this reasoning. However, sound upper value bounds are often computationally expensive to compute, and there is a tradeoff between the tightness of such bounds and their computational cost. This paper introduces new and provably tighter upper value bounds than the commonly used fast informed bound. Our empirical evaluation shows that, despite their additional computational overhead, the new upper bounds accelerate state-of-the-art POMDP solvers on a wide range of benchmarks.', 'abstract_zh': '解决部分可观测马尔可夫决策过程（POMDPs）通常需要对指数级状态信念的价值进行推理。为了提高实际性能，最先进的求解器使用价值边界来引导这种推理。然而，严格上界通常计算成本较高，同时这些边界的有效性和计算成本之间存在权衡。本文引入了新的、可证明更加严格的有效上界，与常用的快速启发式上界相比更为精确。我们的实验评估表明，尽管这些新上界增加了额外的计算开销，但在广泛的一系列基准测试中，它们能够显著加速最先进的POMDP求解器。', 'title_zh': 'tighter 常用于表示“更紧的”或“更精确的”，value-function 翻译为“价值函数”，approximations 翻译为“近似”，POMDPs 是部分可观测马尔可夫决策过程（Partially Observable Markov Decision Processes）的缩写。因此，把这个标题翻译成中文可以是：\n\n更紧的价值函数近似方法：针对POMDPs'}
{'arxiv_id': 'arXiv:2502.06395', 'title': 'AppVLM: A Lightweight Vision Language Model for Online App Control', 'authors': 'Georgios Papoudakis, Thomas Coste, Zhihao Wu, Jianye Hao, Jun Wang, Kun Shao', 'link': 'https://arxiv.org/abs/2502.06395', 'abstract': "The utilisation of foundation models as smartphone assistants, termed app agents, is a critical research challenge. These agents aim to execute human instructions on smartphones by interpreting textual instructions and performing actions via the device's interface. While promising, current approaches face significant limitations. Methods that use large proprietary models, such as GPT-4o, are computationally expensive, while those that use smaller fine-tuned models often lack adaptability to out-of-distribution tasks. In this work, we introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we fine-tune it offline on the AndroidControl dataset. Then, we refine its policy by collecting data from the AndroidWorld environment and performing further training iterations. Our results indicate that AppVLM achieves the highest action prediction accuracy in offline evaluation on the AndroidControl dataset, compared to all evaluated baselines, and matches GPT-4o in online task completion success rate in the AndroidWorld environment, while being up to ten times faster. This makes AppVLM a practical and efficient solution for real-world deployment.", 'abstract_zh': '将智能手机作为助手的基石模型的应用，称为应用代理，是一项关键的研究挑战。这些代理通过解释文本指令并在设备界面执行相应操作，旨在执行人类指令。虽然前景广阔，但当前的方法面临重大局限。使用大型专有模型（如GPT-4o）的方法计算成本昂贵，而使用较小的微调模型的方法则常常缺乏应对数据分布外任务的适应性。在本工作中，我们引入了AppVLM，这是一种轻量级的视觉语言模型（VLM）。首先，我们对AppVLM在AndroidControl数据集上进行离线微调。然后，我们通过收集来自AndroidWorld环境的数据并进行进一步的训练迭代来完善其策略。我们的结果表明，AppVLM在AndroidControl数据集上的离线评估中达到了最高的动作预测准确率，相较于所有评估的基线模型；在AndroidWorld环境中，AppVLM的在线任务完成成功率与GPT-4o相当，而在速度上则快了十倍。这使得AppVLM成为一种在实际部署中既实用又高效的解决方案。', 'title_zh': 'AppVLM：一种用于在线应用控制的轻量级视觉语言模型'}
{'arxiv_id': 'arXiv:2502.06235', 'title': 'Conditioning and AGM-like belief change in the Desirability-Indifference framework', 'authors': 'Kathelijne Coussement, Gert de Cooman, Keano De Vos', 'link': 'https://arxiv.org/abs/2502.06235', 'abstract': 'We show how the AGM framework for belief change (expansion, revision, contraction) can be extended to deal with conditioning in the so-called Desirability-Indifference framework, based on abstract notions of accepting and rejecting options, as well as on abstract notions of events. This level of abstraction allows us to deal simultaneously with classical and quantum probability theory.', 'abstract_zh': '我们展示了如何将信念变化（扩展、修订、收缩）的AGM框架扩展到所谓的 Desireability-Indifference 框架中，该框架基于接受和拒绝选项的抽象概念以及事件的抽象概念，从而能够同时处理经典和量子概率理论。', 'title_zh': '在偏好-无差异框架中的条件概率与AGM似信念更改原则'}
{'arxiv_id': 'arXiv:2502.06152', 'title': 'The Value of Information in Human-AI Decision-making', 'authors': 'Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman', 'link': 'https://arxiv.org/abs/2502.06152', 'abstract': 'Humans and AIs are often paired on decision tasks with the expectation of achieving complementary performance, where the combination of human and AI outperforms either one alone. However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information -- and consequently, opportunities for agents to better exploit available information--in AI-assisted decision workflow. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based instance-level explanation technique that adapts a conventional saliency-based explanation to explain information value in decision making.', 'abstract_zh': '人类和AI经常被搭配在一起进行决策任务，期望通过双方的互补性提升整体性能，其中人类和AI的结合体比单独一方表现更佳。然而，在不了解每个代理具体使用了何种信息和策略的情况下，如何提升人类-AI团队的表现通常不清晰。为此，我们提供了一个决策理论框架，用于表征在AI辅助决策流程中信息的价值——进而为代理更好地利用现有信息提供机会。我们展示了该框架在模型选择、人类-AI性能的实证评估以及解释设计中的应用。我们提出了一种基于信息的实例级解释技术，将传统的显著性解释方法适应为在决策过程中解释信息价值的方法。', 'title_zh': '人类与人工智能决策中的信息价值'}
{'arxiv_id': 'arXiv:2502.06060', 'title': 'Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning', 'authors': 'Bidipta Sarkar, Warren Xia, C. Karen Liu, Dorsa Sadigh', 'link': 'https://arxiv.org/abs/2502.06060', 'abstract': "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent's goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model's listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model's speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at this https URL", 'abstract_zh': '在多智能体环境中，使用自然语言交流是一种强大的工具，因为它使独立的智能体能够在部分观测条件下共享信息，并允许人与智能体进行零样本协调。然而，大多数先前的工作受到限制，因为它们要么依赖大量的人类示范训练，要么缺乏生成自然且有用的交流策略的能力。在这项工作中，我们训练语言模型以自然语言进行有关其环境的有效讨论，而不需要任何人类示范。我们将交流问题分解为倾听和发言两个方面。我们的核心思想是利用智能体的目标来预测有关世界的有用信息作为密集奖励信号，从而指导交流。具体而言，我们通过训练模型根据讨论预测环境信息来提高它们的倾听技能，同时通过基于消息对其他智能体的影响进行奖励的方式，使用多智能体强化学习来同时提高它们的发言技能。为了研究复杂社会环境中的交流作用及其必要性，我们研究了一个基于《Among Us》的具身社会推理游戏，其中的关键问题是确定敌对假扮者的身份。我们分析了由于该技术产生的新兴行为，例如指控嫌疑人和提供证据，并发现这使得讨论更加有效，使得胜率提高了一倍，相比标准的强化学习。我们已将代码和模型发布于以下链接：https://<文档或代码存储位置>', 'title_zh': '使用多智能体强化学习训练语言模型进行社会推理'}
{'arxiv_id': 'arXiv:2502.05957', 'title': 'MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents', 'authors': 'Jiabin Tang, Tianyu Fan, Chao Huang', 'link': 'https://arxiv.org/abs/2502.05957', 'abstract': "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.", 'abstract_zh': '大规模语言模型（LLM）代理展示了在任务自动化和智能决策方面非凡的能力，推动了诸如LangChain和AutoGen等代理开发框架的广泛应用。然而，这些框架主要面向具有丰富技术背景的开发者，这一现状存在显著的局限性—全球仅有0.03%的人口具备必要的编程技能。这一明显的可访问性差距引发了一个根本性的问题：我们是否能够让没有技术背景的人仅通过自然语言就能构建自己的LLM代理？为应对这一挑战，我们提出了一种名为MetaChain的全自动且高度自发展的框架，它允许用户仅通过自然语言创建和部署LLM代理。MetaChain作为一个自主的代理操作系统，包括四个核心组件：i）代理系统工具集，ii）基于LLM的可执行引擎，iii）自管理文件系统，以及iv）自玩代理定制模块。这个轻量级但强大的系统能够无需编码要求或人工干预地高效动态地创建和修改工具、代理和工作流程。除了其免代码的代理开发能力外，MetaChain 还作为一个多功能多代理系统，服务于通用人工智能辅助。在对GAIA基准的全面评估中，MetaChain 在通用多代理任务中的有效性超越了现有最先进的方法。此外，MetaChain 在检索增强生成（RAG）方面的功能与许多基于LLM的替代方案相比，表现持续更优。', 'title_zh': 'MetaChain：一个完全自动化且无需编码的LLM代理框架'}
{'arxiv_id': 'arXiv:2502.05934', 'title': 'Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach', 'authors': 'Aran Nayebi', 'link': 'https://arxiv.org/abs/2502.05934', 'abstract': "Under what conditions can capable AI agents efficiently align their actions with human preferences? More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? These foundational questions of AI alignment help define what makes an AI agent ``sufficiently safe'' and valuable to humans. Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold -- and what they even are.\nWe introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across $M$ objectives and $N$ agents, providing both upper and lower bounds. Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions.\nOur main result shows that even when agents are fully rational and computationally \\emph{unbounded}, alignment can be achieved with high probability in time \\emph{linear} in the task space size. Therefore, in real-world settings, where task spaces are often \\emph{exponential} in input length, this remains impractical. More strikingly, our lower bound demonstrates that alignment is \\emph{impossible} to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment.\nRelaxing these idealized assumptions, we study \\emph{computationally bounded} agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional \\emph{exponential} slowdowns in the task space size, number of agents, and number of tasks.\nWe conclude by identifying conditions that make alignment more feasible.", 'abstract_zh': '在什么条件下，有能力的AI代理能够高效地将行动与人类偏好对齐？更具体地说，当它们足够熟练以与我们协作时，协调需要多长时间，何时才具有计算可行性？这些关于AI对齐的基础问题有助于定义什么使得一个AI代理既“足够安全”又对人类有价值。由于像这样全面能力强的系统尚不存在，因此需要进行理论分析以确定何时能够提供保障以及这些保障具体是什么。\n\n我们引入了一个泛化的博弈论框架，该框架在较少假定的情况下扩展了先前的对齐方法，允许我们分析在$M$个目标和$N$个代理的情况下，对齐的计算复杂性，并提供上下界。与以往工作不同，以往工作通常假设共同的先验知识、理想化的通信或隐含的可处理性，我们的框架在最少的假设下正式定义了对齐的难度。\n\n我们的主要结果表明，即使代理完全理性且在计算上不受限制，也有可能在任务空间大小线性的时间内以高概率实现对齐。因此，在实际应用中，由于任务空间常常随输入长度呈指数增长，这仍然不实际。更有趣的是，我们的下界证明了，当扩大任务或代理规模时，对齐将不可避免地受到指数级的延缓，揭示了可扩展对齐的固有计算障碍。\n\n放松这些理想化的假设，我们研究了具有噪声信息的计算能力受限的代理（代表模糊的意图），证明了尽管对齐仍有可能以高概率成功，但在任务空间大小、代理数量和任务数量方面会额外遭受指数级的延缓。\n\n最后，我们识别出使对齐更加可行的条件。', 'title_zh': '人类与人工智能一致性障碍与路径：一种博弈论方法'}
{'arxiv_id': 'arXiv:2502.05690', 'title': 'Managing Geological Uncertainty in Critical Mineral Supply Chains: A POMDP Approach with Application to U.S. Lithium Resources', 'authors': 'Mansur Arief, Yasmine Alonso, CJ Oshiro, William Xu, Anthony Corso, David Zhen Yin, Jef K. Caers, Mykel J. Kochenderfer', 'link': 'https://arxiv.org/abs/2502.05690', 'abstract': 'The world is entering an unprecedented period of critical mineral demand, driven by the global transition to renewable energy technologies and electric vehicles. This transition presents unique challenges in mineral resource development, particularly due to geological uncertainty-a key characteristic that traditional supply chain optimization approaches do not adequately address. To tackle this challenge, we propose a novel application of Partially Observable Markov Decision Processes (POMDPs) that optimizes critical mineral sourcing decisions while explicitly accounting for the dynamic nature of geological uncertainty. Through a case study of the U.S. lithium supply chain, we demonstrate that POMDP-based policies achieve superior outcomes compared to traditional approaches, especially when initial reserve estimates are imperfect. Our framework provides quantitative insights for balancing domestic resource development with international supply diversification, offering policymakers a systematic approach to strategic decision-making in critical mineral supply chains.', 'abstract_zh': '世界正进入前所未有的关键矿产需求时期，这一时期由全球向可再生能源技术和电动汽车的过渡所驱动。这一转变在矿产资源开发中带来了独特的挑战，特别是地质不确定性这一关键特性，而传统的供应链优化方法未能充分解决这一问题。为了应对这一挑战，我们提出了一种新的部分可观察马尔可夫决策过程（POMDP）的应用，该方法在优化关键矿产采购决策的同时，明确考虑了地质不确定性动态变化的特性。通过美国锂供应链的实际案例研究，我们证明基于POMDP的政策优于传统方法，尤其是在初始储量估计不准确的情况下。我们的框架为平衡国内资源开发与国际供应多元化提供了定量见解，为政策制定者提供了一种系统的方法来进行关键矿产供应链的战略决策。', 'title_zh': '管理关键矿产供应链中的地质不确定性：基于部分观测马尔可夫决策过程的方法及其在美国锂资源中的应用'}
{'arxiv_id': 'arXiv:2502.05632', 'title': 'Amorphous Fortress Online: Collaboratively Designing Open-Ended Multi-Agent AI and Game Environments', 'authors': 'M Charity, Mayu Wilson, Steven Lee, Dipika Rajesh, Sam Earle, Julian Togelius', 'link': 'https://arxiv.org/abs/2502.05632', 'abstract': 'This work introduces Amorphous Fortress Online -- a web-based platform where users can design petri-dish-like environments and games consisting of multi-agent AI characters. Users can play, create, and share artificial life and game environments made up of microscopic but transparent finite-state machine agents that interact with each other. The website features multiple interactive editors and accessible settings to view the multi-agent interactions directly from the browser. This system serves to provide a database of thematically diverse AI and game environments that use the emergent behaviors of simple AI agents.', 'abstract_zh': '本研究介绍了Amorphous Fortress Online——一个基于网络的平台，用户可以在其中设计类似培养皿的环境和游戏，这些游戏包含多智能体AI角色。用户可以玩、创造和分享由微观但透明的有限状态机代理组成的人工生命和游戏环境，这些代理能够相互互动。该网站配备了多个交互式编辑器和易于访问的设置，用户可以直接在浏览器中查看多智能体间的交互。该系统旨在提供一个包含不同主题的AI和游戏环境数据库，这些环境利用了简单AI代理涌现行为的特性。', 'title_zh': '《无序要塞在线：协作设计开放性多智能体AI和游戏环境》'}
{'arxiv_id': 'arXiv:2502.05608', 'title': 'Closing the Responsibility Gap in AI-based Network Management: An Intelligent Audit System Approach', 'authors': 'Emanuel Figetakis, Ahmed Refaey Hussein', 'link': 'https://arxiv.org/abs/2502.05608', 'abstract': 'Existing network paradigms have achieved lower downtime as well as a higher Quality of Experience (QoE) through the use of Artificial Intelligence (AI)-based network management tools. These AI management systems, allow for automatic responses to changes in network conditions, lowering operation costs for operators, and improving overall performance. While adopting AI-based management tools enhance the overall network performance, it also introduce challenges such as removing human supervision, privacy violations, algorithmic bias, and model inaccuracies. Furthermore, AI-based agents that fail to address these challenges should be culpable themselves rather than the network as a whole. To address this accountability gap, a framework consisting of a Deep Reinforcement Learning (DRL) model and a Machine Learning (ML) model is proposed to identify and assign numerical values of responsibility to the AI-based management agents involved in any decision-making regarding the network conditions, which eventually affects the end-user. A simulation environment was created for the framework to be trained using simulated network operation parameters. The DRL model had a 96% accuracy during testing for identifying the AI-based management agents, while the ML model using gradient descent learned the network conditions at an 83% accuracy during testing.', 'abstract_zh': '现有的网络范式通过使用基于人工智能（AI）的网络管理工具，实现了较低的停机率以及更高的用户体验（QoE）。这些基于AI的管理系统允许自动响应网络条件的变化，降低了运营成本，并提高了整体性能。虽然采用基于AI的管理工具可以增强整体网络性能，但也带来了诸如减少人工监督、隐私侵犯、算法偏见和模型不准确性等挑战。此外，未能解决这些挑战的基于AI的代理自身应承担责任，而不仅仅是网络本身。为了弥补这种责任缺口，提出了一种框架，该框架包括深度强化学习（DRL）模型和机器学习（ML）模型，用于识别并为参与网络条件决策的基于AI的管理代理分配责任数值，最终影响最终用户。为该框架创建了一个仿真环境，以便使用模拟的网络操作参数进行训练。在测试中，DRL模型在识别基于AI的管理代理方面的准确率为96%，而使用梯度下降的ML模型在测试中识别网络条件的准确率为83%。', 'title_zh': '基于智能审计系统的方法，缩小AI在网络管理中责任差距：一项研究'}
{'arxiv_id': 'arXiv:2502.05556', 'title': 'Knowledge is Power: Harnessing Large Language Models for Enhanced Cognitive Diagnosis', 'authors': 'Zhiang Dong, Jingyuan Chen, Fei Wu', 'link': 'https://arxiv.org/abs/2502.05556', 'abstract': "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive states by analyzing their performance across a series of exercises. However, existing CDMs often struggle with diagnosing infrequent students and exercises due to a lack of rich prior knowledge. With the advancement in large language models (LLMs), which possess extensive domain knowledge, their integration into cognitive diagnosis presents a promising opportunity. Despite this potential, integrating LLMs with CDMs poses significant challenges. LLMs are not well-suited for capturing the fine-grained collaborative interactions between students and exercises, and the disparity between the semantic space of LLMs and the behavioral space of CDMs hinders effective integration. To address these issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD) framework, which is a model-agnostic framework utilizing LLMs to enhance CDMs and compatible with various CDM architectures. The KCD framework operates in two stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis stage, both students and exercises are diagnosed to achieve comprehensive and detailed modeling. In the Cognitive Level Alignment stage, we bridge the gap between the CDMs' behavioral space and the LLMs' semantic space using contrastive learning and mask-reconstruction approaches. Experiments on several real-world datasets demonstrate the effectiveness of our proposed framework.", 'abstract_zh': '认知诊断模型（CDMs）旨在通过分析学生在一系列练习中的表现来评估其认知状态。然而，现有的CDMs在诊断罕见的学生和练习时往往存在困难，这主要是由于缺乏丰富的先验知识。随着大型语言模型（LLMs）的发展，这些模型拥有大量的专业领域知识，将其集成到认知诊断中展现出了巨大的潜力。尽管如此，将LLMs与CDMs集成仍然面临许多挑战。LLMs不擅长捕捉学生与练习之间的精细合作交互，而LLMs的语义空间与CDMs的行为空间之间的差异阻碍了有效的集成。为了解决这些问题，我们提出了一种新颖的知识增强认知诊断（KCD）框架，这是一种模型通用框架，利用LLMs来增强CDMs，并且兼容各种CDM架构。KCD框架分为两个阶段：LLM诊断和认知水平对齐。在LLM诊断阶段，对学生和练习进行诊断以实现全面和详细的建模。在认知水平对齐阶段，我们使用对比学习和掩码重构的方法来弥合CDMs的行为空间与LLMs的语义空间之间的差距。在几个实际数据集上的实验表明了我们提出的框架的有效性。', 'title_zh': '知识即力量：利用大型语言模型提升认知诊断能力'}
{'arxiv_id': 'arXiv:2502.05537', 'title': 'Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning', 'authors': 'Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen', 'link': 'https://arxiv.org/abs/2502.05537', 'abstract': "Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely been studied despite its broad applications such as adaptive influence maximization (IM) and infectious disease intervention. In this paper, we study the SSCO problem where we first decide the budget (e.g., number of seed nodes in adaptive IM) allocation for all time steps, and then select a set of nodes for each time step. The few existing studies on SSCO simplify the problems by assuming a uniformly distributed budget allocation over the time horizon, yielding suboptimal solutions. We propose a generic hierarchical RL (HRL) framework called wake-sleep option (WS-option), a two-layer option-based framework that simultaneously decides adaptive budget allocation on the higher layer and node selection on the lower layer. WS-option starts with a coherent formulation of the two-layer Markov decision processes (MDPs), capturing the interdependencies between the two layers of decisions. Building on this, WS-option employs several innovative designs to balance the model's training stability and computational efficiency, preventing the vicious cyclic interference issue between the two layers. Empirical results show that WS-option exhibits significantly improved effectiveness and generalizability compared to traditional methods. Moreover, the learned model can be generalized to larger graphs, which significantly reduces the overhead of computational resources.", 'abstract_zh': '强化学习（RL）因其能够快速学习、高效且适应性强的解决方案而成为组合优化（CO）问题的一种有前景的工具。然而，现有的研究主要集中在一次性确定性的CO问题上，而序列随机性组合优化（SSCO）尽管在自适应影响最大化（IM）和传染病干预等广泛的应用中具有重要性，却很少被研究。本文研究了SSCO问题，该问题首先在所有时间步中分配预算（例如，在自适应IM中的种子节点数量），然后在每个时间步中选择一个节点集。现有的少数关于SSCO的研究通过假设预算在时间范围内的均匀分布简化了问题，从而产生了次优的解决方案。我们提出了一种通用的层次化RL（HRL）框架，称为醒睡选项（WS-option），这是一个基于选项的两层框架，同时在较高层决定自适应预算分配，在较低层决定节点选择。WS-option从两个决策层的前后关联出发，构建了一个连贯的两层马尔可夫决策过程（MDPs）的形式化表达。在此基础上，WS-option通过几个创新的设计来平衡模型训练的稳定性和计算效率，防止两层之间的恶性循环干扰。实验结果表明，WS-option在有效性和泛化能力方面显著优于传统的算法。此外，学习到的模型可以被广泛应用于更大的图中，从而显著减少了计算资源的开销。', 'title_zh': '基于层次强化学习的序列 stochastic 组合优化'}
{'arxiv_id': 'arXiv:2502.05453', 'title': 'LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning', 'authors': 'Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong', 'link': 'https://arxiv.org/abs/2502.05453', 'abstract': 'Developing intelligent agents for long-term cooperation in dynamic open-world scenarios is a major challenge in multi-agent systems. Traditional Multi-agent Reinforcement Learning (MARL) frameworks like centralized training decentralized execution (CTDE) struggle with scalability and flexibility. They require centralized long-term planning, which is difficult without custom reward functions, and face challenges in processing multi-modal data. CTDE approaches also assume fixed cooperation strategies, making them impractical in dynamic environments where agents need to adapt and plan independently. To address decentralized multi-agent cooperation, we propose Decentralized Adaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in a novel Multi-agent Crafter environment. Our generative agents, powered by Large Language Models (LLMs), are more scalable than traditional MARL agents by leveraging external knowledge and language for long-term planning and reasoning. Instead of fully sharing information from all past experiences, DAMCS introduces a multi-modal memory system organized as a hierarchical knowledge graph and a structured communication protocol to optimize agent cooperation. This allows agents to reason from past interactions and share relevant information efficiently. Experiments on novel multi-agent open-world tasks show that DAMCS outperforms both MARL and LLM baselines in task efficiency and collaboration. Compared to single-agent scenarios, the two-agent scenario achieves the same goal with 63% fewer steps, and the six-agent scenario with 74% fewer steps, highlighting the importance of adaptive memory and structured communication in achieving long-term goals. We publicly release our project at: this https URL.', 'abstract_zh': '在动态开放世界场景中开发能够长期合作的智能代理是一个多智能体系统中的重大挑战。传统的多智能体强化学习（MARL）框架，如集中训练分散执行（CTDE），在扩展性和灵活性方面存在困难。它们需要集中式的长期规划，这在没有定制奖励函数的情况下很难实现，并且难以处理多模态数据。CTDE方法假设合作策略固定不变，使其在动态环境中不切实际，而在动态环境中，智能体需要独立地适应和规划。为了解决分散式多智能体合作，我们提出了一种新颖的多智能体工匠环境中的分散自适应知识图记忆和结构化通信系统（DAMCS）。我们的生成智能体借助大型语言模型（LLMs）获得了比传统MARL代理更高的可扩展性，因为它们能够利用外部知识和语言进行长期规划和推理。DAMCS不像传统方法那样完全共享所有过去经验的信息，而是引入了一种分层知识图组织的多模态记忆系统和结构化通信协议，以优化智能体之间的合作。这使得智能体能够从过去的交互中推理并有选择地共享相关信息。在新型的多智能体开放世界任务的实验中，DAMCS在任务效率和协作方面均优于MARL和LLM基线。与单智能体场景相比，双智能体场景的步骤减少了63%，六智能体场景的步骤减少了74%，这突显了适应性记忆和结构化通信在实现长期目标中的重要性。我们已公开发布该项目：[这里](this https URL)。', 'title_zh': '基于LLM的去中心化生成式代理及其自适应层级知识图谱协作规划'}
{'arxiv_id': 'arXiv:2502.05442', 'title': 'The Odyssey of the Fittest: Can Agents Survive and Still Be Good?', 'authors': 'Dylan Waldner, Risto Miikkulainen', 'link': 'https://arxiv.org/abs/2502.05442', 'abstract': "As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This paper examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent play a simulated, LLM generated text based adventure game. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent's decisions, uncovering the tradeoffs they navigate to survive. Specifically, analysis finds that when danger increases, agents ignore ethical considerations and opt for unethical behavior. The agents' collective behavior, trading ethics for survival, suggests that prioritizing survival increases the risk of unethical behavior. In the context of AGI, designing agents to prioritize survival may amplify the likelihood of unethical decision making and unintended emergent behaviors, raising fundamental questions about goal design in AI safety research.", 'abstract_zh': '随着人工智能模型在功能和通用性方面不断增强，理解智能体如何在复杂环境中学习和决策对于促进其伦理行为至关重要。本文探讨了将生物驱动力，特别是自我保护，具体实现到三种不同智能体中的伦理影响。一种使用NEAT优化的贝叶斯智能体、一种使用随机变分推断优化的贝叶斯智能体，以及一种GPT-4o智能体在一个由大型语言模型生成的文本冒险游戏中进行模拟。在每个场景中，智能体选择行动以求生存，并适应不断升级的挑战。模拟后分析评估了智能体决策的伦理得分，揭示了它们为生存所做的权衡。具体分析发现，当危险增加时，智能体倾向于忽视伦理考虑并选择不道德行为。智能体的合作行为，为了生存而放弃伦理，表明优先生存可能会增加不道德行为的可能性，并可能导致意外的衍生行为，从AI安全研究的基本角度来看，这引发了关于AI目标设计的根本性问题。在AGI的背景下，设计智能体以优先生存可能会放大不道德决策的可能性和意外衍生行为，从而提出关于AI安全研究目标设计的原则性问题。', 'title_zh': '最适者的迁徙：智能体能在生存的同时still be good吗？ \n\n注释：这里的"still be good"根据上下文和语境理解为“保持优良”或“依旧优秀”，但直译不太通顺，因此在中文翻译中进行了适当调整，希望能准确传达原文的意思。'}
{'arxiv_id': 'arXiv:2502.05439', 'title': 'Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews', 'authors': 'Izunna Okpala, Ashkan Golgoon, Arjun Ravi Kannan', 'link': 'https://arxiv.org/abs/2502.05439', 'abstract': 'The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a manager and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection, hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a manager along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.', 'abstract_zh': '大型语言模型的出现标志着自主系统新时代的到来，人工智能程序在多个领域展现出显著的自主决策能力。本文探讨了在金融服务行业中自主系统的操作流程。特别是，我们构建了能够有效协作完成复杂建模和模型风险管理工作（MRM）的自主团队。建模团队由一名经理和多名执行特定任务的代理组成，这些任务包括探索性数据分析、特征工程、模型选择、超参数调整、模型训练、模型评估以及撰写文档。MRM团队由一名经理和专门的代理组成，这些代理负责执行诸如检查建模文档的合规性、模型复制、概念上的合理性、结果分析以及撰写文档等任务。我们通过应用到信用卡欺诈检测、信用卡审批和资产组合信用风险建模数据集的一系列数值示例，展示了建模和MRM团队的有效性和稳健性。', 'title_zh': '应用于金融服务领域的代理型人工智能系统：建模与模型风险管理团队'}
{'arxiv_id': 'arXiv:2502.05398', 'title': 'Probabilistic Foundations for Metacognition via Hybrid-AI', 'authors': 'Paulo Shakarian, Gerardo I. Simari, Nathaniel D. Bastian', 'link': 'https://arxiv.org/abs/2502.05398', 'abstract': 'Metacognition is the concept of reasoning about an agent\'s own internal processes, and it has recently received renewed attention with respect to artificial intelligence (AI) and, more specifically, machine learning systems. This paper reviews a hybrid-AI approach known as "error detecting and correcting rules" (EDCR) that allows for the learning of rules to correct perceptual (e.g., neural) models. Additionally, we introduce a probabilistic framework that adds rigor to prior empirical studies, and we use this framework to prove results on necessary and sufficient conditions for metacognitive improvement, as well as limits to the approach. A set of future', 'abstract_zh': '元认知是指个体对其自身内部过程进行反思的概念，近年来，随着人工智能（AI）尤其是机器学习系统的发展，这一概念重新引起了关注。本文回顾了一种称为“错误检测和纠正规则”（EDCR）的混合AI方法，该方法允许学习纠正感知模型（例如神经网络模型）的规则。此外，我们引入了一个概率框架，增强了以前的经验研究的严谨性，并使用这一框架来证明元认知改进的必要和充分条件，以及该方法的限制条件。未来研究将探讨这一方法在实际应用中的效果。', 'title_zh': '通过混合人工智能（Hybrid-AI）构建元认知的概率理论基础'}
{'arxiv_id': 'arXiv:2502.05352', 'title': 'ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks', 'authors': 'Saurabh Jha, Rohan Arora, Yuji Watanabe, Takumi Yanagawa, Yinfang Chen, Jackson Clark, Bhavya Bhavya, Mudit Verma, Harshit Kumar, Hirokuni Kitahara, Noah Zheutlin, Saki Takano, Divya Pathak, Felix George, Xinbo Wu, Bekir O. Turkkan, Gerard Vanloo, Michael Nidd, Ting Dai, Oishik Chatterjee, Pranjal Gupta, Suranjana Samanta, Pooja Aggarwal, Rong Lee, Pavankumar Murali, Jae-wook Ahn, Debanjana Kar, Ameet Rahane, Carlos Fonseca, Amit Paradkar, Yu Deng, Pratibha Moogi, Prateeti Mohapatra, Naoki Abe, Chandrasekhar Narayanaswami, Tianyin Xu, Lav R. Varshney, Ruchi Mahindru, Anca Sailer, Laura Shwartz, Daby Sow, Nicholas C. M. Fuller, Ruchir Puri', 'link': 'https://arxiv.org/abs/2502.05352', 'abstract': 'Realizing the vision of using AI agents to automate critical IT tasks depends on the ability to measure and understand effectiveness of proposed solutions. We introduce ITBench, a framework that offers a systematic methodology for benchmarking AI agents to address real-world IT automation tasks. Our initial release targets three key areas: Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The design enables AI researchers to understand the challenges and opportunities of AI agents for IT automation with push-button workflows and interpretable metrics. ITBench includes an initial set of 94 real-world scenarios, which can be easily extended by community contributions. Our results show that agents powered by state-of-the-art models resolve only 13.8% of SRE scenarios, 25.2% of CISO scenarios, and 0% of FinOps scenarios. We expect ITBench to be a key enabler of AI-driven IT automation that is correct, safe, and fast.', 'abstract_zh': '实现使用AI代理自动化关键IT任务的愿景取决于衡量和理解所提出解决方案有效性的能力。我们介绍了ITBench框架，这是一个用于系统性评估AI代理以应对实际IT自动化任务的框架。我们最初的发布针对三个关键领域：站点可靠性工程（SRE）、合规与安全运营（CISO）以及财务运营（FinOps）。该设计使AI研究人员能够通过一键式工作流和可解释的度量标准，理解AI代理在IT自动化中的挑战与机遇。ITBench包含最初的94个真实场景集，并可通过社区贡献轻松扩展。我们的结果显示，基于最先进的模型的代理仅解决了13.8%的SRE场景、25.2%的CISO场景以及0%的FinOps场景。我们期望ITBench将成为推动AI驱动的IT自动化正确、安全且高效的关键工具。', 'title_zh': 'ITBench：评估AI代理在多元化的实际IT自动化任务中的表现'}
{'arxiv_id': 'arXiv:2502.05244', 'title': 'Probabilistic Artificial Intelligence', 'authors': 'Andreas Krause, Jonas Hübotter', 'link': 'https://arxiv.org/abs/2502.05244', 'abstract': 'Artificial intelligence commonly refers to the science and engineering of artificial systems that can carry out tasks generally associated with requiring aspects of human intelligence, such as playing games, translating languages, and driving cars. In recent years, there have been exciting advances in learning-based, data-driven approaches towards AI, and machine learning and deep learning have enabled computer systems to perceive the world in unprecedented ways. Reinforcement learning has enabled breakthroughs in complex games such as Go and challenging robotics tasks such as quadrupedal locomotion.\nA key aspect of intelligence is to not only make predictions, but reason about the uncertainty in these predictions, and to consider this uncertainty when making decisions. This is what this manuscript on "Probabilistic Artificial Intelligence" is about. The first part covers probabilistic approaches to machine learning. We discuss the differentiation between "epistemic" uncertainty due to lack of data and "aleatoric" uncertainty, which is irreducible and stems, e.g., from noisy observations and outcomes. We discuss concrete approaches towards probabilistic inference and modern approaches to efficient approximate inference.\nThe second part of the manuscript is about taking uncertainty into account in sequential decision tasks. We consider active learning and Bayesian optimization -- approaches that collect data by proposing experiments that are informative for reducing the epistemic uncertainty. We then consider reinforcement learning and modern deep RL approaches that use neural network function approximation. We close by discussing modern approaches in model-based RL, which harness epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.', 'abstract_zh': '人工智能通常指的是科学和工程领域，涉及构建能够执行通常需要人类智能才能完成的任务的系统，例如玩游戏、翻译语言和驾驶汽车。近年来，在基于学习、数据驱动的AI方法方面取得了令人兴奋的进展，机器学习和深度学习使得计算机系统以前所未有的方式感知世界。强化学习使我们在围棋等复杂游戏中和四足行走等具有挑战性的机器人任务中取得了突破。\n\n智能的一个关键方面不仅在于做出预测，还在于思考这些预测中的不确定性，并在做出决策时考虑这种不确定性。本文“概率人工智能”的内容正是围绕这一点展开的。第一部分探讨了概率方法在机器学习中的应用。我们讨论了由于缺乏数据导致的“ epistemic 不确定性”和由噪音观察结果和结果引发的不可约的“ aleatoric 不确定性”之间的区别。我们还讨论了概率推断的实用方法以及高效近似推断的现代方法。\n\n本文的第二部分讨论了在序列决策任务中考虑不确定性。我们考虑了主动学习和贝叶斯优化——通过提出信息性的实验来收集数据的方法，用于减少 epistemic 不确定性。然后，我们探讨了使用神经网络函数逼近的强化学习和现代深度强化学习方法。最后，我们讨论了在基于模型的强化学习中利用 epistemic 不确定性和 aleatoric 不确定性的现代方法，这些方法不仅指导探索，还考虑到安全性。', 'title_zh': '概率人工智能'}
{'arxiv_id': 'arXiv:2502.06788', 'title': 'EVEv2: Improved Baselines for Encoder-Free Vision-Language Models', 'authors': 'Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, Xinlong Wang', 'link': 'https://arxiv.org/abs/2502.06788', 'abstract': 'Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: this https URL.', 'abstract_zh': '现有的无需编码器的多模态模型（Vision-Language Models, VLMs）正在迅速缩小与基于编码器的模型之间的性能差距，突显了结构简单且高效部署的统一多模态系统具有巨大的潜力。我们系统地澄清了使用预训练视觉编码器、离散分词器和从零构建的简约视觉层的VLMs之间的性能差距，深入挖掘了无需编码器VLMs的一些未被充分研究的特点。我们开发了高效的策略，这些策略可以与主流的基于编码器的方法相媲美。经过深入的研究，我们推出了EVEv2.0，这是一个新的改进版的无需编码器的VLMs系列。我们表明：(i) 在统一模型中适当地分解和层级关联视觉和语言可以减少模态之间的干扰。(ii) 有效的训练策略能够促进无需编码器的VLMs的有效优化。通过广泛的评估，我们的EVEv2.0展示了跨模态解码器架构开发过程中的全面研究，证明了其卓越的数据效率和强大的视觉推理能力。相关的代码已在如下地址公开：this https URL。', 'title_zh': 'EVEv2：提高的无编码器视觉-语言模型基线'}
{'arxiv_id': 'arXiv:2502.06786', 'title': 'Matryoshka Quantization', 'authors': 'Pranav Nair, Puranjay Datta, Jeff Dean, Prateek Jain, Aditya Kusupati', 'link': 'https://arxiv.org/abs/2502.06786', 'abstract': 'Quantizing model weights is critical for reducing the communication and inference costs of large models. However, quantizing models -- especially to low precisions like int4 or int2 -- requires a trade-off in model quality; int2, in particular, is known to severely degrade model quality. Consequently, practitioners are often forced to maintain multiple models with different quantization levels or serve a single model that best satisfies the quality-latency trade-off. On the other hand, integer data types, such as int8, inherently possess a nested (Matryoshka) structure where smaller bit-width integers, like int4 or int2, are nested within the most significant bits. This paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale quantization technique that addresses the challenge of needing multiple quantized models. It allows training and maintaining just one model, which can then be served at different precision levels. Furthermore, due to the co-training and co-distillation regularization provided by MatQuant, the int2 precision models extracted by MatQuant can be up to $10\\%$ more accurate than standard int2 quantization (using techniques like QAT or OmniQuant). This represents significant progress in model quantization, demonstrated by the fact that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more accurate than an int8 FFN-quantized Gemma-2 2B model.', 'abstract_zh': '量化模型权重对于减少大型模型的通信和推理成本至关重要。然而，量化模型——特别是量化到低精度，如int4或int2——需要在模型质量与精度之间进行权衡；特别是int2量化已被证实会显著降低模型质量。因此，实践者通常被迫维护不同量化级别的多个模型，或者提供一个能满足质量与延迟权衡的单一模型。另一方面，整数数据类型，如int8，本质上具有嵌套（Matryoshka）结构，其中较小位宽的整数，如int4或int2，嵌套在最显著的位中。本文提出了一种新颖的多尺度量化技术——Matryoshka量化（MatQuant），它解决了需要多个量化模型的挑战。MatQuant允许仅训练和维护一个模型，然后可以在不同精度级别上提供。此外，由于MatQuant提供的共同训练和共同蒸馏正则化，通过MatQuant提取的int2精度模型的准确性最高可以比标准int2量化（如QAT或OmniQuant）提高10%。这代表了在模型量化方面的重大进步，因为使用相同的配方，一个int2 FFN-量化后的Gemma-2 9B模型比int8 FFN-量化后的Gemma-2 2B模型更为准确。', 'title_zh': '“Matryoshka量ization” \n\n注：在学术翻译中，通常会保持名词和术语的原创形式，尤其是当这些术语在英文中已经成为一个特定领域的常用术语时。不过，由于“Matryoshka”（套娃）在中文中是一个知名的文化符号，这里将其直译为中文，并在量ization后添加了中文解释，以便于读者理解具体的含义。在正式的学术文献中，建议确认是否有已被广泛接受的中文翻译或解释方式。'}
{'arxiv_id': 'arXiv:2502.06784', 'title': 'RelGNN: Composite Message Passing for Relational Deep Learning', 'authors': 'Tianlang Chen, Charilaos Kanatsoulis, Jure Leskovec', 'link': 'https://arxiv.org/abs/2502.06784', 'abstract': 'Predictive tasks on relational databases are critical in real-world applications spanning e-commerce, healthcare, and social media. To address these tasks effectively, Relational Deep Learning (RDL) encodes relational data as graphs, enabling Graph Neural Networks (GNNs) to exploit relational structures for improved predictions. However, existing heterogeneous GNNs often overlook the intrinsic structural properties of relational databases, leading to modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework specifically designed to capture the unique characteristics of relational databases. At the core of our approach is the introduction of atomic routes, which are sequences of nodes forming high-order tripartite structures. Building upon these atomic routes, RelGNN designs new composite message passing mechanisms between heterogeneous nodes, allowing direct single-hop interactions between them. This approach avoids redundant aggregations and mitigates information entanglement, ultimately leading to more efficient and accurate predictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from RelBench (Fey et al., 2024), and consistently achieves state-of-the-art accuracy with up to 25% improvement.', 'abstract_zh': '在电子商务、医疗保健和社会媒体等实际应用中，关系数据库上的预测任务至关重要。为了有效地应对这些任务，关系深度学习（Relational Deep Learning, RDL）将关系数据编码为图，从而让图神经网络（Graph Neural Networks, GNNs）利用关系结构以提高预测性能。然而，现有的异构GNN往往忽视了关系数据库的内在结构特性，导致建模效率低下。为了解决这一问题，我们提出了一种名为RelGNN的新颖GNN框架，专门设计用于捕捉关系数据库的独特特征。我们方法的核心在于引入了原子路径，这些路径由形成高阶三部图结构的节点序列组成。基于这些原子路径，RelGNN设计了新的异构节点之间的复合消息传递机制，允许节点之间进行直接的一跳交互。这种方法避免了多余的聚合操作，减少了信息纠缠，最终实现了更高效的预测建模。RelGNN在Fey等人（2024）提出的RelBench上进行了评估，共涉及30个不同的真实世界任务，并且在所有任务上均实现了最先进的准确率，最高提升幅度达到25%。', 'title_zh': 'RelGNN：关系复合消息传递在深度学习中的应用'}
{'arxiv_id': 'arXiv:2502.06779', 'title': 'KARST: Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission for Visual Classification', 'authors': 'Yue Zhu, Haiwen Diao, Shang Gao, Long Chen, Huchuan Lu', 'link': 'https://arxiv.org/abs/2502.06779', 'abstract': 'Fine-tuning pre-trained vision models for specific tasks is a common practice in computer vision. However, this process becomes more expensive as models grow larger. Recently, parameter-efficient fine-tuning (PEFT) methods have emerged as a popular solution to improve training efficiency and reduce storage needs by tuning additional low-rank modules within pre-trained backbones. Despite their advantages, they struggle with limited representation capabilities and misalignment with pre-trained intermediate features. To address these issues, we introduce an innovative Multi-Kernel Kronecker Adaptation with Re-Scaling Transmission (KARST) for various recognition tasks. Specifically, its multi-kernel design extends Kronecker projections horizontally and separates adaptation matrices into multiple complementary spaces, reducing parameter dependency and creating more compact subspaces. Besides, it incorporates extra learnable re-scaling factors to better align with pre-trained feature distributions, allowing for more flexible and balanced feature aggregation. Extensive experiments validate that our KARST outperforms other PEFT counterparts with a negligible inference cost due to its re-parameterization characteristics. Code is publicly available at: this https URL.', 'abstract_zh': '对预训练视觉模型进行特定任务的微调是计算机视觉中的常见做法。然而，随着模型变得越来越大，这一过程的成本也相应增加。最近，参数效率微调（PEFT）方法因其通过在预训练骨干网络中调整额外的低秩模块来提高训练效率并减少存储需求而受到广泛关注。尽管具有这些优势，但它们在表示能力和与预训练中间特征的对齐方面仍存在问题。为解决这些问题，我们提出了一种创新的多核克罗内克自适应与重缩放传输（KARST）方法，适用于各种识别任务。具体而言，其多核设计沿横向扩展了克罗内克投影，并将适应矩阵划分为多个互补空间，从而减少了参数依赖性，生成更紧凑的子空间。此外，该方法还引入了额外的可学习重缩放因子，以更好地与预训练特征分布对齐，从而允许更灵活和平衡的特征聚合。大量实验表明，由于其重参数化特性，我们的KARST在推理成本几乎可忽略的情况下，优于其他PEFT方法。相关代码已公开可用：此链接：this https URL。', 'title_zh': 'KARST: 多核克罗内克自适应与重塑传输的视觉分类方法'}
{'arxiv_id': 'arXiv:2502.06776', 'title': 'Towards Internet-Scale Training For Agents', 'authors': 'Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov', 'link': 'https://arxiv.org/abs/2502.06776', 'abstract': 'The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web. Code will be available at: this http URL.', 'abstract_zh': '以下是论文内容或标题的中文翻译，符合学术规范：\n\n以往训练网络导航代理的主要方法是收集一组流行网站的人类演示和手写任务的数据，但现在已经明显地意识到，人类数据并不是一种高效的资源。我们开发了一条生产线，以促进大规模互联网训练，无需耗时的人工注释。在第一阶段，一个语言模型（LLM）生成150,000个多样化的任务。在下一个阶段，基于LLM的代理完成任务并生成轨迹。在最终阶段，一个语言模型审查轨迹并判断其成功。语言模型在与人类注释员竞争的同时，以97%的准确性检测和过滤有害内容，以89%的比率生成可行的任务，并以82.6%的准确性判断成功的轨迹。扩大生产线规模后，基于Llama 3.1 70B的代理解决了150,000个网站中16.7%的任务。使用我们生产线生成的数据进行训练，在Mind2Web和WebLINX得到的数据限制性设置中，对于混合使用我们管线数据和人类数据训练的代理，步长准确性分别提高89.5%和122.1%。当使用这些基准中所有可用的人类数据训练代理时，代理无法泛化到多样化的实际网站，而对于WebLINX，使用我们的数据训练将泛化性能提高了149.0%，而对于Mind2Web，提高了156.3%。代码将在以下网址提供：这个 http 地址。\n\n请注意，这里的“这个 http 地址”需要替换为实际的链接地址。', 'title_zh': '面向互联网规模的智能体训练方法'}
{'arxiv_id': 'arXiv:2502.06759', 'title': 'Rationalization Models for Text-to-SQL', 'authors': 'Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Shankar Subramanian', 'link': 'https://arxiv.org/abs/2502.06759', 'abstract': 'We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.', 'abstract_zh': '我们介绍了一个生成链式思考（Chain-of-Thought, CoT）推理框架，以增强文本到SQL模型的微调。这些推理包括中间的SQL语句及其解释，作为逐步构建最终SQL查询的中间步骤。该过程始于手动标注一小部分示例，然后使用这些示例以迭代的动态少样本知识精炼方式促进大型语言模型。随后，通过验证分解后的查询进行理性化模型训练，从而为文本到SQL数据集生成广泛的合成CoT注释。为了评估这种方法，我们使用带有和不带有这些推理的较小语言模型对BIRD数据集进行微调。结果表明，逐步生成查询能够提高执行准确性，尤其是在中等复杂度和高度复杂度的查询方面，同时也有助于增强可解释性。', 'title_zh': '文本生成SQL语句的归因模型'}
{'arxiv_id': 'arXiv:2502.06751', 'title': 'What makes a good feedforward computational graph?', 'authors': 'Alex Vitvitskyi, João G. M. Araújo, Marc Lackenby, Petar Veličković', 'link': 'https://arxiv.org/abs/2502.06751', 'abstract': "As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs.", 'abstract_zh': '根据关于图重连的大量文献暗示，神经网络所采用的计算图的选择可能会对其下游性能产生重大影响。某些与计算图相关的效应，如重连不足和过度压缩，甚至可能使模型无法学习某些函数。这些效应大多仅在无向图的领域进行了详尽的研究；然而，在近年来，对前向传播计算图（即没有反向边的有向图）的兴趣显著增加。在本文中，我们研究了前向传播计算图的 desirable 属性，发现了两个重要的补充性度量：忠实度和混合时间，并通过这些度量评估了几种流行的图选择。我们的研究不仅依托于对不同图的指标渐进行为的理论分析，还通过训练使用相应图的神经网络模型的性能，将这些指标与实际性能进行了关联。', 'title_zh': '什么样的前向计算图可以称之为优秀的设计？'}
{'arxiv_id': 'arXiv:2502.06742', 'title': 'Gradient Multi-Normalization for Stateless and Scalable LLM Training', 'authors': 'Meyer Scetbon, Chao Ma, Wenbo Gong, Edward Meeds', 'link': 'https://arxiv.org/abs/2502.06742', 'abstract': "Training large language models (LLMs) typically relies on adaptive optimizers like Adam (Kingma & Ba, 2015) which store additional state information to accelerate convergence but incur significant memory overhead. Recent efforts, such as SWAN (Ma et al., 2024) address this by eliminating the need for optimizer states while achieving performance comparable to Adam via a multi-step preprocessing procedure applied to instantaneous gradients. Motivated by the success of SWAN, we introduce a novel framework for designing stateless optimizers that normalizes stochastic gradients according to multiple norms. To achieve this, we propose a simple alternating scheme to enforce the normalization of gradients w.r.t these norms. We show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem, and that SWAN is a particular instance of our approach with carefully chosen norms, providing a deeper understanding of its design. However, SWAN's computationally expensive whitening/orthogonalization step limit its practicality for large LMs. Using our principled perspective, we develop of a more efficient, scalable, and practical stateless optimizer. Our algorithm relaxes the properties of SWAN, significantly reducing its computational cost while retaining its memory efficiency, making it applicable to training large-scale models. Experiments on pre-training LLaMA models with up to 1 billion parameters demonstrate a 3X speedup over Adam with significantly reduced memory requirements, outperforming other memory-efficient baselines.", 'abstract_zh': '大规模语言模型（LLMs）的训练通常依赖于自适应优化器，如Adam（Kingma & Ba, 2015），这些优化器通过存储额外的状态信息来加速收敛，但会带来显著的内存开销。近期的努力，如SWAN（Ma et al., 2024），通过消除对优化器状态的需求，并通过多步预处理程序应用即时梯度来实现与Adam相当的性能。受到SWAN成功的启发，我们提出了一种新的无状态优化器设计框架，该框架根据多种范数对随机梯度进行归一化。为此，我们提出了一种简单交替方案来确保这些范数下的梯度归一化。我们证明，我们的方法可以在任意精度下生成问题的一个固定点，而SWAN是这种方法的一个特例，使用了精心选择的范数，从而对其设计有了更深入的理解。然而，SWAN的计算成本高昂的白化/正交化步骤限制了其在大规模LLMs中的实用性。借助我们的理论视角，我们开发了一种更高效、可扩展且实用的无状态优化器。该算法放松了SWAN的属性，显著降低了其计算成本，同时保持了其内存效率，使其适用于大规模模型的训练。实验结果表明，在使用多达10亿个参数训练LLaMA模型时，相比于Adam，可实现3倍的速度提升，且显著减少内存需求，超越了其他内存有效的基准方法。', 'title_zh': '无状态且可扩展的大规模语言模型训练的梯度多正则化方法'}
{'arxiv_id': 'arXiv:2502.06736', 'title': 'Low-power Spike-based Wearable Analytics on RRAM Crossbars', 'authors': 'Abhiroop Bhattacharjee, Jinquan Shi, Wei-Chen Chen, Xinxin Wang, Priyadarshini Panda', 'link': 'https://arxiv.org/abs/2502.06736', 'abstract': 'This work introduces a spike-based wearable analytics system utilizing Spiking Neural Networks (SNNs) deployed on an In-memory Computing engine based on RRAM crossbars, which are known for their compactness and energy-efficiency. Given the hardware constraints and noise characteristics of the underlying RRAM crossbars, we propose online adaptation of pre-trained SNNs in real-time using Direct Feedback Alignment (DFA) against traditional backpropagation (BP). Direct Feedback Alignment (DFA) learning, that allows layer-parallel gradient computations, acts as a fast, energy & area-efficient method for online adaptation of SNNs on RRAM crossbars, unleashing better algorithmic performance against those adapted using BP. Through extensive simulations using our in-house hardware evaluation engine called DFA_Sim, we find that DFA achieves upto 64.1% lower energy consumption, 10.1% lower area overhead, and a 2.1x reduction in latency compared to BP, while delivering upto 7.55% higher inference accuracy on human activity recognition (HAR) tasks.', 'abstract_zh': '本文介绍了一种基于尖峰的可穿戴数据分析系统，该系统利用了部署在基于RRAM交叉开关的忆算器上的脉冲神经网络（SNNs）。RRAM交叉开关以其紧凑性和能效性而闻名。鉴于底层RRAM交叉开关的硬件限制和噪声特性，我们提出了一种在线实时使用直接反馈连接（DFA）进行预训练SNNs的自适应方法，以替代传统的反向传播（BP）。直接反馈连接（DFA）学习允许层间并行梯度计算，成为在RRAM交叉开关上对SNNs进行在线自适应的快速且能效及面积高效的算法，性能优于使用BP进行自适应的算法。通过使用我们自制的硬件评估引擎DFA_Sim进行广泛仿真，我们发现与BP相比，DFA在能耗上降低了64.1%，在面积开销上降低了10.1%，延迟减少了2.1倍，同时在人体活动识别（HAR）任务上的推断准确率提高了7.55%。', 'title_zh': '基于低功耗忆阻交叉bar的刺激发穿戴式分析'}
{'arxiv_id': 'arXiv:2502.06733', 'title': 'Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining', 'authors': 'Daouda Sow, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang', 'link': 'https://arxiv.org/abs/2502.06733', 'abstract': 'Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.', 'abstract_zh': '在广泛且异构的数据集上预训练大型语言模型（LLMs）对于实现跨各种下游任务的最先进性能至关重要。然而，当前的训练范式在训练过程中将所有样本同等对待，忽略了个体样本的重要性和相关性。现有的重新加权策略主要关注组级数据的重要性，未能利用细粒度的实例级信息，也不适应训练过程中个体样本重要性动态变化的要求。本文中，我们提出了新的算法，旨在提高LLM预训练的效率和效果，进行动态的实例级数据重新加权。我们的方法根据每个训练样本的损失值在线调整其权重，使模型能够动态地在当前训练阶段聚焦于更具信息量或重要性的样本。特别是在我们的框架下，我们系统地设计了重新加权策略，优先考虑的样本对性能提升更为有利。此外，我们还开发了一个新的理论框架，用于分析基于损失的重新加权对梯度优化收敛性的影响，这是首次对这些策略如何影响收敛边界的正式刻画。我们通过一系列任务的实证验证，从7B和1.4B参数的LLM预训练到较小规模的语言模型和线性回归问题，证明了基于损失的重新加权方法可以实现更快的收敛和显著改进的性能。', 'title_zh': '基于动态损失的样本加权方法以提高大型语言模型预训练效果'}
{'arxiv_id': 'arXiv:2502.06728', 'title': 'FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded Training', 'authors': 'Mogens Henrik From, Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp', 'link': 'https://arxiv.org/abs/2502.06728', 'abstract': 'Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, when considering larger models that do not fit on a single accelerate, the exchange of gradient information and the integration of DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy, FlexDeMo, whereby nodes fully synchronize locally between different GPUs and inter-node communication is improved through only using the fast-moving components. This effectively combines previous hybrid sharding strategies with the advantages of decoupled momentum. Our experimental results show that FlexDeMo is on par with AdamW in terms of validation loss, demonstrating its viability.', 'abstract_zh': '训练大型神经网络模型需要广泛的计算资源，通常分布在多个节点和加速器上。最近的研究表明，仅交换梯度的快速移动部分，同时在本地累积动量（称为解耦动量或DeMo）可能足矣。然而，当考虑不适用于单个加速器的大型模型时，梯度信息的交换和DeMo的集成需要重新考虑。在此，我们提出了一种混合策略FlexDeMo，该策略中节点在不同GPU之间完全进行本地同步，并通过仅使用快速移动的部分来改善节点间通信。这有效地结合了先前的混合分片策略与解耦动量的优点。我们的实验结果表明，FlexDeMo在验证损失方面与AdamW相当，证明了其可行性。', 'title_zh': 'FlexDeMo：解耦动量优化在全流程和混合分割训练中的应用'}
{'arxiv_id': 'arXiv:2502.06693', 'title': 'Recent Advances, Applications and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2024 Symposium', 'authors': 'Amin Adibi, Xu Cao, Zongliang Ji, Jivat Neet Kaur, Winston Chen, Elizabeth Healey, Brighton Nuwagira, Wenqian Ye, Geoffrey Woollard, Maxwell A Xu, Hejie Cui, Johnny Xi, Trenton Chang, Vasiliki Bikia, Nicole Zhang, Ayush Noori, Yuan Xia, Md. Belal Hossain, Hanna A. Frank, Alina Peluso, Yuan Pu, Shannon Zejiang Shen, John Wu, Adibvafa Fallahpour, Sazan Mahbub, Ross Duncan, Yuwei Zhang, Yurui Cao, Zuheng Xu, Michael Craig, Rahul G. Krishnan, Rahmatollah Beheshti, James M. Rehg, Mohammad Ehsanul Karim, Megan Coffee, Leo Anthony Celi, Jason Alan Fries, Mohsen Sadatsafavi, Dennis Shung, Shannon McWeeney, Jessica Dafflon, Sarah Jabbour', 'link': 'https://arxiv.org/abs/2502.06693', 'abstract': "The fourth Machine Learning for Health (ML4H) symposium was held in person on December 15th and 16th, 2024, in the traditional, ancestral, and unceded territories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver, British Columbia, Canada. The symposium included research roundtable sessions to foster discussions between participants and senior researchers on timely and relevant topics for the ML4H community. The organization of the research roundtables at the conference involved 13 senior and 27 junior chairs across 13 tables. Each roundtable session included an invited senior chair (with substantial experience in the field), junior chairs (responsible for facilitating the discussion), and attendees from diverse backgrounds with an interest in the session's topic.", 'abstract_zh': '第四届机器学习与健康（ML4H）研讨会于2024年12月15日至16日在加拿大不列颠哥伦比亚省温哥华的传统、祖先及未归还领土上，即穆斯夸姆、斯夸姆什和特莉尔-乌吐斯原住民的土地上举行。本次研讨会包括研究圆桌会议，旨在促进与会者与资深研究者就ML4H社区关注的及时和相关议题进行讨论。本次会议中的研究圆桌会议组织工作涉及13名资深主席和27名初级主席，共设13个圆桌。每个圆桌讨论会均包含一名受邀的资深主席（在该领域具有丰富经验）、若干初级主席（负责促进讨论）以及来自不同背景并对讨论议题感兴趣的所有参会者。', 'title_zh': '机器学习在健康领域的新进展、应用和开放挑战：来自2024年ML4H研讨会研究圆桌会议的反思'}
{'arxiv_id': 'arXiv:2502.06692', 'title': 'Multi-label Scandinavian Language Identification (SLIDE)', 'authors': 'Mariia Fedorova, Jonas Sebulon Frydenberg, Victoria Handford, Victoria Ovedie Chruickshank Langø, Solveig Helene Willoch, Marthe Løken Midtgaard, Yves Scherrer, Petter Mæhlum, David Samuel', 'link': 'https://arxiv.org/abs/2502.06692', 'abstract': 'Identifying closely related languages at sentence level is difficult, in particular because it is often impossible to assign a sentence to a single language. In this paper, we focus on multi-label sentence-level Scandinavian language identification (LID) for Danish, Norwegian Bokmål, Norwegian Nynorsk, and Swedish. We present the Scandinavian Language Identification and Evaluation, SLIDE, a manually curated multi-label evaluation dataset and a suite of LID models with varying speed-accuracy tradeoffs. We demonstrate that the ability to identify multiple languages simultaneously is necessary for any accurate LID method, and present a novel approach to training such multi-label LID models.', 'abstract_zh': '在句子层面识别密切相关的语言是一项艰巨的任务，尤其是因为常常无法将一个句子明确归属于单一的语言。本文中，我们聚焦于丹麦语、挪威语（Bokmål和Nynorsk）和瑞典语的多标签句子级斯堪的纳维亚语言识别（LID）。我们提出了一套斯堪的纳维亚语言识别和评估数据集（SLIDE），该数据集是手动编辑并包含多标签的评估集，以及一系列具有不同速度-准确率权衡的LID模型。我们证明，同时识别多个语言的能力对于任何准确的LID方法而言都是必不可少的，并提出了一种训练多标签LID模型的新型方法。', 'title_zh': '多标签斯堪的纳维亚语言识别（SLIDE）'}
{'arxiv_id': 'arXiv:2502.06684', 'title': 'EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks', 'authors': 'Michael Arbel, David Salinas, Frank Hutter', 'link': 'https://arxiv.org/abs/2502.06684', 'abstract': 'Recent foundational models for tabular data, such as TabPFN, have demonstrated remarkable effectiveness in adapting to new tasks through in-context learning. However, these models overlook a crucial equivariance property: the arbitrary ordering of target dimensions should not influence model predictions. In this study, we identify this oversight as a source of incompressible error, termed the equivariance gap, which introduces instability in predictions. To mitigate these issues, we propose a novel model designed to preserve equivariance across output dimensions. Our experimental results indicate that our proposed model not only addresses these pitfalls effectively but also achieves competitive benchmark performance.', 'abstract_zh': '近年来，针对表格数据的一些基础模型，如TabPFN，已经展示了通过上下文学习适应新任务的显著效果。然而，这些模型忽视了一个关键的不变性属性：目标维度的任意排序不应影响模型预测。在本研究中，我们识别这种忽视是导致不可压缩错误（称为不变性缺口）的一个原因，这种错误引入了预测的不稳定性。为了缓解这些问题，我们提出了一种新颖的模型，旨在保持输出维度上的不变性。我们的实验结果表明，我们的模型不仅有效地解决了这些问题，还实现了竞争性的基准性能。', 'title_zh': 'EquiTabPFN：目标置换等变先验分布拟合网络'}
{'arxiv_id': 'arXiv:2502.06681', 'title': 'CHIRLA: Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis', 'authors': 'Bessie Dominguez-Dager, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla', 'link': 'https://arxiv.org/abs/2502.06681', 'abstract': "Person re-identification (Re-ID) is a key challenge in computer vision, requiring the matching of individuals across different cameras, locations, and time periods. While most research focuses on short-term scenarios with minimal appearance changes, real-world applications demand robust Re-ID systems capable of handling long-term scenarios, where persons' appearances can change significantly due to variations in clothing and physical characteristics. In this paper, we present CHIRLA, Comprehensive High-resolution Identification and Re-identification for Large-scale Analysis, a novel dataset specifically designed for long-term person Re-ID. CHIRLA consists of recordings from strategically placed cameras over a seven-month period, capturing significant variations in both temporal and appearance attributes, including controlled changes in participants' clothing and physical features. The dataset includes 22 individuals, four connected indoor environments, and seven cameras. We collected more than five hours of video that we semi-automatically labeled to generate around one million bounding boxes with identity annotations. By introducing this comprehensive benchmark, we aim to facilitate the development and evaluation of Re-ID algorithms that can reliably perform in challenging, long-term real-world scenarios.", 'abstract_zh': '行人再识别（Re-ID）是计算机视觉中的一个关键挑战，要求在不同的摄像头、地点和时间范围内匹配个体。尽管大多数研究侧重于少量外观变化的短期场景，但现实世界的应用需要能够处理长期场景的鲁棒Re-ID系统，在这些场景中，由于服装和身体特征的变化，人的外观可能会显著变化。在本文中，我们提出了CHIRLA，即全面的高分辨率识别与再识别用于大规模分析，这是一个专为长期行人Re-ID设计的新型数据集。CHIRLA 包括在七个月期间战略性放置的摄像头的录制，捕捉到了时间属性和外观属性中的显著变化，包括参与者服装和物理特征的受控变化。该数据集包含22名个体、四个相连的室内环境和七台摄像头。我们收集了超过五个小时的视频，并通过半自动标注生成了大约一百万个带有身份注释的边框。通过引入这一全面的基准数据集，我们旨在促进Re-ID算法的发展和评估，这些算法能够在具有挑战性的长期现实世界场景中可靠地工作。', 'title_zh': 'CHIRLA：全面高分辨率识别与再识别在大规模分析中的应用'}
{'arxiv_id': 'arXiv:2502.06669', 'title': 'Boosting Self-Efficacy and Performance of Large Language Models via Verbal Efficacy Stimulations', 'authors': 'Rui Chen, Tailai Peng, Xinran Xie, Dekun Lin, Zhe Cui, Zheng Chen', 'link': 'https://arxiv.org/abs/2502.06669', 'abstract': "Significant improvements have been observed in the zero-shot capabilities of the Large Language Models (LLMs). Due to their high sensitivity to input, research has increasingly focused on enhancing LLMs' performance via direct and simple prompt engineering rather than intricate domain adaptation. Studies suggest that LLMs exhibit emotional intelligence, and both positive and negative emotions can potentially enhance task performances. However, prior interaction prompts have predominantly concentrated on a single stimulus type, neglecting to compare different stimulus effects, examine the influence of varying task difficulties, or explore underlying mechanisms. This paper, inspired by the positive correlation between self-efficacy and task performance within the social cognitive theory, introduces Verbal Efficacy Stimulations (VES). Our VES comprises three types of verbal prompts: encouraging, provocative, and critical, addressing six aspects such as helpfulness and competence. And we further categorize task difficulty, aiming to extensively investigate how distinct VES influence the self-efficacy and task achievements of language models at varied levels of difficulty. The experimental results show that the three types of VES improve the performance of LLMs on most tasks, and the most effective VES varies for different models. In extensive experiments, we have obtained some findings consistent with psychological theories, providing novel insights for future research.", 'abstract_zh': '在大型语言模型（LLMs）的零样本能力方面已经观察到显著的提升。由于它们对输入的高敏感性，研究重点越来越多地转向通过直接且简单的提示工程来提升LLMs的性能，而非复杂的领域适应。现有的研究表明，LLMs表现出情感智慧，积极和消极的情感都可能增强任务表现。然而，先前的交互提示主要集中在单一刺激类型上，未比较不同刺激效果，也未研究任务难度变化的影响或探索潜在机制。本研究受社会认知理论中自我效能与任务表现正相关性的启发，引入了言语效能刺激（VES）。我们的VESt包含三种类型的口语提示：鼓励型、挑战型和批判型，针对如帮助性和能力等六个方面。此外，我们进一步对任务难度进行了分类，旨在全面探讨不同类型的VAS如何影响具有不同难度级别的语言模型的自我效能和任务成就。实验结果表明，三种类型的VAS在大多数任务上都能提高LLMs的性能，而最有效的VAS则因不同模型而异。在广泛实验中，我们获得了与心理理论一致的一些发现，为未来的研究提供了新的见解。', 'title_zh': '通过口头效能刺激提升大规模语言模型的自我效能感和性能'}
{'arxiv_id': 'arXiv:2502.06666', 'title': 'Automatic Evaluation of Healthcare LLMs Beyond Question-Answering', 'authors': 'Anna Arias-Duart, Pablo Agustin Martin-Torres, Daniel Hinjos, Pablo Bernabeu-Perez, Lucia Urcelay Ganzabal, Marta Gonzalez Mallo, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Sergio Alvarez-Napagao, Dario Garcia-Gasulla', 'link': 'https://arxiv.org/abs/2502.06666', 'abstract': "Current Large Language Models (LLMs) benchmarks are often based on open-ended or close-ended QA evaluations, avoiding the requirement of human labor. Close-ended measurements evaluate the factuality of responses but lack expressiveness. Open-ended capture the model's capacity to produce discourse responses but are harder to assess for correctness. These two approaches are commonly used, either independently or together, though their relationship remains poorly understood. This work is focused on the healthcare domain, where both factuality and discourse matter greatly. It introduces a comprehensive, multi-axis suite for healthcare LLM evaluation, exploring correlations between open and close benchmarks and metrics. Findings include blind spots and overlaps in current methodologies. As an updated sanity check, we release a new medical benchmark--CareQA--, with both open and closed variants. Finally, we propose a novel metric for open-ended evaluations --Relaxed Perplexity-- to mitigate the identified limitations.", 'abstract_zh': '当前的大规模语言模型（LLMs）基准通常基于开放型或封闭型问答评估，从而避免了对人工劳动的依赖。封闭型评估测量响应的准确性，但缺乏表达力。开放型评估捕捉模型生成话语响应的能力，但难以评估其正确性。这两种方法通常独立或结合使用，但它们之间的关系尚未充分理解。本研究重点关注医疗保健领域，其中事实准确性与话语能力同样重要。本研究引入了一个全面的多维度评估套件，用于医疗保健LLM评估，探讨开放型和封闭型基准及其指标之间的相关性。研究发现包括当前方法论中的盲点和重叠。作为更新的合理性检查，我们发布了新的医疗基准—CareQA—，其中包含开放型和封闭型变体。最后，我们提出了一种新的开放型评估指标——放宽困惑度（Relaxed Perplexity）——以缓解识别出的限制。', 'title_zh': '超越问答任务的医疗LLM自动评估'}
{'arxiv_id': 'arXiv:2502.06664', 'title': 'Evaluation of Deep Audio Representations for Hearables', 'authors': 'Fabian Gröger, Pascal Baumann, Ludovic Amruthalingam, Laurent Simon, Ruksana Giurda, Simone Lionetti', 'link': 'https://arxiv.org/abs/2502.06664', 'abstract': 'Effectively steering hearable devices requires understanding the acoustic environment around the user. In the computational analysis of sound scenes, foundation models have emerged as the state of the art to produce high-performance, robust, multi-purpose audio representations. We introduce and release Deep Evaluation of Audio Representations (DEAR), the first dataset and benchmark to evaluate the efficacy of foundation models in capturing essential acoustic properties for hearables. The dataset includes 1,158 audio tracks, each 30 seconds long, created by spatially mixing proprietary monologues with commercial, high-quality recordings of everyday acoustic scenes. Our benchmark encompasses eight tasks that assess the general context, speech sources, and technical acoustic properties of the audio scenes. Through our evaluation of four general-purpose audio representation models, we demonstrate that the BEATs model significantly surpasses its counterparts. This superiority underscores the advantage of models trained on diverse audio collections, confirming their applicability to a wide array of auditory tasks, including encoding the environment properties necessary for hearable steering. The DEAR dataset and associated code are available at this https URL.', 'abstract_zh': '有效地引导可听设备需要理解用户周围的声学环境。在声场景的计算分析中，基础模型已逐渐成为最先进的技术，用于生成高性能、鲁棒且多功能的音频表示。我们提出了并公开了Audio Representation的Deep Evaluation（DEAR）数据集及基准，这是首个用于评估基础模型在捕捉可听设备所需声学特性方面的效果的数据集及基准。该数据集包含1158个音频轨道，每个持续30秒，通过空问混响专有的独白与日常生活声场景的高质量录音商业版本。我们的基准涵盖了八个任务，评估声场景的一般上下文、语音来源和技术声学特性。通过对四种通用音频表示模型的评估，我们证明了BEATs模型显著超越了其他模型。这种优越性验证了在多样化的音频集合上训练的模型的优势，证实了它们在各种听觉任务中的适用性，包括编码对于可听设备引导所必需的环境特性。DEAR数据集及相关代码可在如下链接获取：此链接。', 'title_zh': '可穿戴设备中深度音频表示的评估'}
{'arxiv_id': 'arXiv:2502.06648', 'title': 'The 2021 Tokyo Olympics Multilingual News Article Dataset', 'authors': 'Erik Novak, Erik Calcina, Dunja Mladenić, Marko Grobelnik', 'link': 'https://arxiv.org/abs/2502.06648', 'abstract': 'In this paper, we introduce a dataset of multilingual news articles covering the 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from 1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and published between July 1, 2021, and August 14, 2021. These articles are written in nine languages from different language families and in different scripts. To create the dataset, the raw news articles were first retrieved via a service that collects and analyzes news articles. Then, the articles were grouped using an online clustering algorithm, with each group containing articles reporting on the same sub-event. Finally, the groups were manually annotated and evaluated. The development of this dataset aims to provide a resource for evaluating the performance of multilingual news clustering algorithms, for which limited datasets are available. It can also be used to analyze the dynamics and events of the 2021 Tokyo Olympics from different perspectives. The dataset is available in CSV format and can be accessed from the this http URL repository.', 'abstract_zh': '在本文中，我们介绍了一个涵盖2021年东京奥运会的多语言新闻文章数据集。共收集了来自1,918个不同发布者、涉及2021年奥运会1,350个子项目的10,940篇新闻文章，并于2021年7月1日至2021年8月14日之间发布。这些文章使用了九种不同语言家族和不同文字形式书写。为了创建该数据集，首先通过一个收集和分析新闻文章的服务获取了原始新闻文章。然后，使用在线聚类算法将文章分组，每个组包含报道同一子项目的文章。最后，对这些组进行了人工标注和评估。该数据集的开发旨在为评估多语言新闻聚类算法的性能提供资源，目前这类可用的数据集较少。此外，它还可以用于从不同角度分析2021年东京奥运会的动态和事件。该数据集以CSV格式提供，并可通过以下链接访问：this http URL.', 'title_zh': '2021年东京奥运会多语言新闻文章数据集'}
{'arxiv_id': 'arXiv:2502.06635', 'title': 'Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM', 'authors': 'Qingshui Gu, Shu Li, Tianyu Zheng, Zhaoxiang Zhang', 'link': 'https://arxiv.org/abs/2502.06635', 'abstract': "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at this https URL.", 'abstract_zh': 'Steel-LLM 是一个以中国为中心的语言模型，从零开始设计，旨在利用有限的计算资源创建高质量的开源模型。该项目于2024年3月启动，目标是在大规模数据集上训练一个包含10亿参数的模型，注重透明度和实用洞察的分享，以帮助社区内的其他成员。训练过程主要聚焦于中文数据，少量英文数据的加入弥补了现有开源LLM中的一些空白，提供了更详细和实用的模型构建过程概述。Steel-LLM 在 CEVAL 和 CMMLU 等基准测试中表现出竞争性的性能，超越了大型机构的早期模型。本文提供了该项目关键贡献的全面总结，包括数据收集、模型设计、训练方法以及在构建过程中遇到的挑战，为希望开发自己模型的研究人员和实践者提供了宝贵的资源。模型检查点和训练脚本可在以下链接访问：[这里](https://example.com)。', 'title_zh': '当然，以下是翻译成中文后的标题和内容：\n\n标题：Steel-LLM：从零开始到开源——在构建以中文为中心的大型语言模型方面的个人历程\n\n内容：本文分享了作者从零开始构建以中文为中心的大规模语言模型（LLM）的心路历程，并最终将该项目开源的心得体会。'}
{'arxiv_id': 'arXiv:2502.06634', 'title': 'Automatic Annotation Augmentation Boosts Translation between Molecules and Natural Language', 'authors': 'Zhiqiang Zhong, Simon Sataa-Yu Larsen, Haoyu Guo, Tao Tang, Kuangyu Zhou, Davide Mottin', 'link': 'https://arxiv.org/abs/2502.06634', 'abstract': 'Recent advancements in AI for biological research focus on integrating molecular data with natural language to accelerate drug discovery. However, the scarcity of high-quality annotations limits progress in this area. This paper introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework that leverages large language models to augment existing datasets, thereby improving AI training. We demonstrate the effectiveness of LA$^3$ by creating an enhanced dataset, LaChEBI-20, where we systematically rewrite the annotations of molecules from an established dataset. These rewritten annotations preserve essential molecular information while providing more varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5 based on a benchmark architecture to learn the mapping between molecular representations and augmented annotations.\nExperimental results on text-based *de novo* molecule generation and molecule captioning demonstrate that LaMolT5 outperforms state-of-the-art models. Notably, incorporating LA$^3$ leads to improvements of up to 301% over the benchmark architecture. Furthermore, we validate the effectiveness of LA$^3$ notable applications in *image*, *text* and *graph* tasks, affirming its versatility and utility.', 'abstract_zh': '近年来，人工智能在生命科学研究中的应用重点在于将分子数据与自然语言相结合，以加速药物发现过程。然而，高质量标注数据的稀缺限制了这一领域的进展。本文介绍了基于语言的自动标注增强框架LA$^3$，该框架利用大型语言模型扩充现有数据集，从而提高人工智能训练的效果。我们通过系统地重写一个已建立的数据集中的分子标注，创建了一个增强数据集LaChEBI-20。这些重新编写的标注保留了分子的基本信息，同时提供了更多样化和丰富的句子结构和词汇。利用LaChEBI-20，我们在基准架构基础上训练了LaMolT5，使其学习分子表示与增强标注之间的映射。\n\n实验结果表明，LaMolT5在基于文本的全新分子生成和分子描述任务中优于现有的最先进模型。值得注意的是，引入LA$^3$后，相对于基准架构，性能提升高达301%。此外，我们验证了LA$^3$在图像、文本和图任务中的有效性，证明了其多样性和实用性。', 'title_zh': '自动注释增强提升了分子与自然语言之间的翻译效率'}
{'arxiv_id': 'arXiv:2502.06633', 'title': 'Combining Large Language Models with Static Analyzers for Code Review Generation', 'authors': 'Imen Jaoua, Oussama Ben Sghaier, Houari Sahraoui', 'link': 'https://arxiv.org/abs/2502.06633', 'abstract': 'Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.', 'abstract_zh': '代码审查是软件开发中至关重要但又常被视为复杂、主观且耗时的活动。过去几十年里，人们已经投入了大量精力来自动化这一过程。早期的方法侧重于基于知识的系统（KBS），这些系统利用基于规则的机制来检测代码问题，提供精准的反馈，但在处理复杂的、依赖于上下文的情况时存在困难。近年来的工作转向利用预训练语言模型进行代码审查的细调，这使得能够覆盖更广泛的代码问题，但通常会牺牲精确性。在本文中，我们提出了一种混合方法，将基于知识的系统（KBS）和基于学习的系统（LBS）的优势结合起来，以生成高质量、全面的代码审查。我们的方法在语言模型pipeline的三个不同阶段整合了知识：数据准备阶段（数据增强训练，DAT）、推理阶段（检索增强生成，RAG）和推理之后（输出的简单拼接，NCO）。我们通过对比基于单一知识系统的数据集和基于学习系统的模型，实证评估了我们的组合策略。实验结果表明，这些混合策略提高了审查评论的相关性、完整性和整体质量，有效地弥合了基于规则的工具与深度学习模型之间的差距。', 'title_zh': '将大型语言模型与静态分析器结合用于代码审查生成'}
{'arxiv_id': 'arXiv:2502.06632', 'title': 'Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging', 'authors': 'Mohammed Abdul Hafeez Khan, Samuel Morries Boddepalli, Siddhartha Bhattacharyya, Debasis Mitra', 'link': 'https://arxiv.org/abs/2502.06632', 'abstract': 'Accurate classification and anatomical localization are essential for effective medical diagnostics and research, which may be efficiently performed using deep learning techniques. However, availability of limited labeled data poses a significant challenge. To address this, we adapted Prototypical Networks and the Propagation-Reconstruction Network (PRNet) for few-shot classification and localization, respectively, in Single Photon Emission Computed Tomography (SPECT) images. For the proof of concept we used a 2D-sliced image cropped around heart. The Prototypical Network, with a pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver tissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for 2D imaging with an encoder-decoder architecture and skip connections, achieved a training loss of 1.395, accurately reconstructing patches and capturing spatial relationships. These results highlight the potential of Prototypical Networks for tissue classification with limited labeled data and PRNet for anatomical landmark localization, paving the way for improved performance in deep learning frameworks.', 'abstract_zh': '准确的分类和解剖定位对于有效的医学诊断和研究至关重要，这可以通过深度学习技术高效地完成。然而，有限标注数据的可用性构成了一个重大挑战。为了解决这个问题，我们针对单光子 emission 计算机断层摄影术（SPECT）图像，将原型网络和传播重建网络（PRNet）分别用于少量样本分类和定位。对于概念验证，我们使用心脏周围的 2D 切片图像进行测试。使用预训练的 ResNet-18 作为骨干网络的原型网络，分别在心室、心肌和肝脏组织分类中达到了 96.67% 的训练准确率和 93.33% 的验证准确率。PRNet 通过使用编码器-解码器架构和跳跃连接，针对 2D 图像进行了适应，并且实现了 1.395 的训练损失，准确地重建了区域并捕捉了空间关系。这些结果突显了在有限标注数据下使用原型网络进行组织分类和使用 PRNet 进行解剖标志定位的潜力，为深度学习框架中的性能提升铺平了道路。', 'title_zh': 'SPECT成像中基于少量样本的组织分类与解剖定位'}
{'arxiv_id': 'arXiv:2502.06631', 'title': 'Conformal Predictions for Human Action Recognition with Vision-Language Models', 'authors': 'Bary Tim, Fuchs Clément, Macq Benoît', 'link': 'https://arxiv.org/abs/2502.06631', 'abstract': 'Human-In-The-Loop (HITL) frameworks are integral to many real-world computer vision systems, enabling human operators to make informed decisions with AI assistance. Conformal Predictions (CP), which provide label sets with rigorous guarantees on ground truth inclusion probabilities, have recently gained traction as a valuable tool in HITL settings. One key application area is video surveillance, closely associated with Human Action Recognition (HAR). This study explores the application of CP on top of state-of-the-art HAR methods that utilize extensively pre-trained Vision-Language Models (VLMs). Our findings reveal that CP can significantly reduce the average number of candidate classes without modifying the underlying VLM. However, these reductions often result in distributions with long tails. To address this, we introduce a method based on tuning the temperature parameter of the VLMs to minimize these tails without requiring additional calibration data. Our code is made available on GitHub at the address this https URL.', 'abstract_zh': 'Human-in-the-Loop (HITL)框架在许多实际的计算机视觉系统中至关重要，它们允许人类操作者在人工智能辅助下做出明智的决策。一致性预测（Conformal Predictions, CP）提供了严格的关于 ground truth 包含概率的保证，近年来在HITL环境中被广泛应用，成为一种有价值的工具。其中一个关键应用领域是视频监控，与人类行为识别（Human Action Recognition, HAR）密切相关。本研究探讨了在最先进的HAR方法基础上应用CP的可能性，这些方法广泛利用预训练的Vision-Language模型（VLMs）。研究发现，CP可以在不修改VLM的前提下显著减少候选类别的平均数量。然而，这些减少往往会导致具有较长尾部的分布。为了解决这个问题，我们提出了一种方法，通过调整VLM的温度参数来最小化这些尾部，而不需要额外的校准数据。我们的代码已在GitHub上公开，地址为：https://github.com/example/repo。', 'title_zh': '使用视觉-语言模型进行人体动作识别的配准预测'}
{'arxiv_id': 'arXiv:2502.06608', 'title': 'TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models', 'authors': 'Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao', 'link': 'https://arxiv.org/abs/2502.06608', 'abstract': 'Recent advancements in diffusion techniques have propelled image and video generation to unprece- dented levels of quality, significantly accelerating the deployment and application of generative AI. However, 3D shape generation technology has so far lagged behind, constrained by limitations in 3D data scale, complexity of 3D data process- ing, and insufficient exploration of advanced tech- niques in the 3D domain. Current approaches to 3D shape generation face substantial challenges in terms of output quality, generalization capa- bility, and alignment with input conditions. We present TripoSG, a new streamlined shape diffu- sion paradigm capable of generating high-fidelity 3D meshes with precise correspondence to input images. Specifically, we propose: 1) A large-scale rectified flow transformer for 3D shape generation, achieving state-of-the-art fidelity through training on extensive, high-quality data. 2) A hybrid supervised training strategy combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality 3D reconstruction performance. 3) A data processing pipeline to generate 2 million high- quality 3D samples, highlighting the crucial rules for data quality and quantity in training 3D gen- erative models. Through comprehensive experi- ments, we have validated the effectiveness of each component in our new framework. The seamless integration of these parts has enabled TripoSG to achieve state-of-the-art performance in 3D shape generation. The resulting 3D shapes exhibit en- hanced detail due to high-resolution capabilities and demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG demonstrates improved versatility in generating 3D models from diverse image styles and contents, showcasing strong gen- eralization capabilities. To foster progress and innovation in the field of 3D generation, we will make our model publicly available.', 'abstract_zh': '近年来，扩散技术的最新进展已将图像和视频生成提升到了前所未有的质量水平，显著加速了生成式人工智能的应用和部署。然而，3D形状生成技术迄今为止仍然落后，受限于3D数据规模的限制、3D数据处理的复杂性以及在3D领域高级技术探索的不足。当前3D形状生成的方法在输出质量、通用能力和与输入条件的匹配方面面临重大挑战。我们提出了TripoSG，这是一种新的简化形状扩散范式，能够生成与输入图像精确对应的高质量3D网格。具体来说，我们提出了以下几点：\n1）一种大规模校正流转换器，用于3D形状生成，通过在广泛的高质量数据集上进行训练，实现了最先进的保真度。\n2）结合体素距离函数（SDF）、法线和爱克农（Eikonal）损失的混合监督训练策略，实现了高质量的3D重建性能。\n3）一个数据处理管道，生成200万个高质量3D样本，突显了在训练3D生成模型时数据质量和数量的关键规则。\n\n通过全面的实验，我们验证了我们新框架中每个组件的有效性。这些组成部分之间的无缝集成使得TripoSG在3D形状生成方面达到了最先进的性能。生成的3D形状由于高分辨率能力而具有增强的细节，并且在与输入图像的保真度方面表现出色。此外，TripoSG在生成源自不同图像风格和内容的3D模型方面展现了更高的灵活性，展示了强大的泛化能力。\n\n为了促进3D生成领域的进展和创新，我们将使我们的模型公开可用。', 'title_zh': 'TripoSG：使用大规模校正流模型的高保真3D形状合成'}
{'arxiv_id': 'arXiv:2502.06607', 'title': 'Illegal Waste Detection in Remote Sensing Images: A Case Study', 'authors': 'Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Andrea Diecidue, Simona Malegori', 'link': 'https://arxiv.org/abs/2502.06607', 'abstract': "Environmental crime currently represents the third largest criminal activity worldwide while threatening ecosystems as well as human health. Among the crimes related to this activity, improper waste management can nowadays be countered more easily thanks to the increasing availability and decreasing cost of Very-High-Resolution Remote Sensing images, which enable semi-automatic territory scanning in search of illegal landfills. This paper proposes a pipeline, developed in collaboration with professionals from a local environmental agency, for detecting candidate illegal dumping sites leveraging a classifier of Remote Sensing images. To identify the best configuration for such classifier, an extensive set of experiments was conducted and the impact of diverse image characteristics and training settings was thoroughly analyzed. The local environmental agency was then involved in an experimental exercise where outputs from the developed classifier were integrated in the experts' everyday work, resulting in time savings with respect to manual photo-interpretation. The classifier was eventually run with valuable results on a location outside of the training area, highlighting potential for cross-border applicability of the proposed pipeline.", 'abstract_zh': '当前，环境犯罪已经成为全球第三大犯罪活动，不仅威胁着生态系统，还危害着人类健康。在与该活动相关的犯罪中，不正当的废物管理如今变得更容易被遏制，这得益于高分辨率遥感图像的日益普及和成本降低。这些高分辨率遥感图像使得半自动地扫描领土以寻找非法填埋场成为可能。本文提出了一种由当地环境机构的专业人员共同开发的检测非法倾倒场点的管道。为了确定此类分类器的最佳配置，进行了广泛的一系列实验，并详细分析了不同图像特征和训练设置的影响。随后，当地环境机构参与了一项实验，将开发的分类器的输出整合到专家的日常工作中，从而节省了手动图像解释所需的时间。最终，该分类器在训练区域以外的地点运行，取得了有价值的结果，显示出所提管道在跨境适用方面的潜力。', 'title_zh': '遥感图像中的非法废物检测：一个案例研究'}
{'arxiv_id': 'arXiv:2502.06601', 'title': 'Amortized In-Context Bayesian Posterior Estimation', 'authors': 'Sarthak Mittal, Niels Leif Bracher, Guillaume Lajoie, Priyank Jaini, Marcus Brubaker', 'link': 'https://arxiv.org/abs/2502.06601', 'abstract': 'Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses. Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available. Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models. In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices. Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer. In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples. Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems. Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows.', 'abstract_zh': '贝叶斯推断提供了一种自然地结合先验信念并为假设空间分配概率测度的方式。当前的方法依赖于迭代算法，如马尔可夫链蒙特卡罗（MCMC）采样和变分推断（VI），这些方法在获得新观察数据时需要重新运行。通过条件估计进行的进一步学习是缓解此类问题的一种可行策略，并且是基于仿真方法、神经过程和使用预训练模型的上下文方法背后的指导原则。在本项研究中，我们从不同的优化目标和架构选择的角度对各种进一步学习的上下文贝叶斯后验估计方法进行了全面的比较分析。这些方法通过在序列模型（如变换器）中以一组数据示例作为上下文进行条件估计来训练一个进一步学习估计器，以进行后验参数推断。与语言模型不同，我们利用不变架构进行估计，因为真实的后验对上下文示例的顺序是不变的。我们的实证研究包括对分布外任务的泛化、所假设的基础模型不正确的案例，以及从仿真到现实问题的迁移。研究之后强调了反向KL估计器在预测问题中的优越性，尤其是在与变换器架构和归一化流结合使用时。', 'title_zh': 'amortized 在此语境下意为“近似化”的意思。因此，论文标题可以翻译为：\n\n近似化基于上下文的贝叶斯后验估计'}
{'arxiv_id': 'arXiv:2502.06600', 'title': 'Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?', 'authors': 'Gonçalo Gomes, Chrysoula Zerva, Bruno Martins', 'link': 'https://arxiv.org/abs/2502.06600', 'abstract': 'The evaluation of image captions, looking at both linguistic fluency and semantic correspondence to visual contents, has witnessed a significant effort. Still, despite advancements such as the CLIPScore metric, multilingual captioning evaluation has remained relatively unexplored. This work presents several strategies, and extensive experiments, related to evaluating CLIPScore variants in multilingual settings. To address the lack of multilingual test data, we consider two different strategies: (1) using quality aware machine-translated datasets with human judgements, and (2) re-purposing multilingual datasets that target semantic inference and reasoning. Our results highlight the potential of finetuned multilingual models to generalize across languages and to handle complex linguistic challenges. Tests with machine-translated data show that multilingual CLIPScore models can maintain a high correlation with human judgements across different languages, and additional tests with natively multilingual and multicultural data further attest to the high-quality assessments.', 'abstract_zh': '对图像描述进行评价，不仅关注其语言流畅性，还关注其与视觉内容的语义对应性，这一领域已取得显著进展。尽管如此，尽管有CLIPScore等指标的进步，多语言描述评价仍相对未被充分探索。本研究提出了几种策略，并进行了大量实验，以评估在多语言环境下的CLIPScore变体。为应对缺乏多语言测试数据的问题，我们考虑了两种不同的策略：（1）使用高质量的机器翻译数据集，并辅以人工评判，（2）重新利用旨在进行语义推理和推断的多语言数据集。我们的研究结果突显了微调多语言模型在跨语言泛化和处理复杂语言挑战方面的潜力。通过机器翻译数据的测试表明，多语言CLIPScore模型能够在不同语言之间保持与人工评判的高相关性，而额外的测试使用了本族多语言和多元文化的测试数据，进一步证明了评估的质量。', 'title_zh': '多语言图像字幕评价：CLIP模型能带我们走多远？'}
{'arxiv_id': 'arXiv:2502.06589', 'title': 'Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training', 'authors': 'Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang', 'link': 'https://arxiv.org/abs/2502.06589', 'abstract': 'Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.', 'abstract_zh': '由于面向代理的预训练数据稀缺，基于LLM的自主代理通常依赖于复杂的提示或广泛的微调，这往往无法引入新的能力同时保持较强的泛化性。我们介绍了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强LLM代理在API功能调用、内在推理与规划以及适应环境反馈方面的基本能力。Hephaestus-Forge包含103B个特定于代理的数据，涉及76,537个API，涵盖了工具文档以介绍API功能的知识，以及功能调用轨迹以加强内在推理。为了探索有效的训练协议，我们研究了扩展律以确定数据混合比率的最优组合方式。通过持续以Hephaestus-Forge进行预训练，Hephaestus在三个代理基准测试中的表现优于小型到中型的开源LLM，与其表现相近的商业LLM相当，这证明了我们的预训练语料库在增强LLM的基本代理能力和泛化性方面的有效性。', 'title_zh': 'Hephaestus：通过持续预训练提升大型语言模型的基本智能体能力'}
{'arxiv_id': 'arXiv:2502.06577', 'title': 'The Minimal Search Space for Conditional Causal Bandits', 'authors': 'Francisco N. F. Q. Simoes, Itai Feigenbaum, Mehdi Dastani, Thijs van Ommen', 'link': 'https://arxiv.org/abs/2502.06577', 'abstract': 'Causal knowledge can be used to support decision-making problems. This has been recognized in the causal bandits literature, where a causal (multi-armed) bandit is characterized by a causal graphical model and a target variable. The arms are then interventions on the causal model, and rewards are samples of the target variable. Causal bandits were originally studied with a focus on hard interventions. We focus instead on cases where the arms are conditional interventions, which more accurately model many real-world decision-making problems by allowing the value of the intervened variable to be chosen based on the observed values of other variables. This paper presents a graphical characterization of the minimal set of nodes guaranteed to contain the optimal conditional intervention, which maximizes the expected reward. We then propose an efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify this minimal set of nodes. We prove that the graphical characterization and the proposed algorithm are correct. Finally, we empirically demonstrate that our algorithm significantly prunes the search space and substantially accelerates convergence rates when integrated into standard multi-armed bandit algorithms.', 'abstract_zh': '因果知识可以用于支持决策问题。这一点已经在因果多臂老虎机的相关文献中得到了认可，其中因果多臂老虎机通过因果图形模型和目标变量进行描述。而臂则表示对因果模型的干预，奖励则是目标变量的样本值。最初的因果多臂老虎机研究主要集中在硬干预上。而本文则关注臂表示条件干预的情况，这种设置更准确地模拟了许多实际决策问题，因为它允许干预变量的值基于其他观测变量的值来选择。本文首先提供了一种图形化的特征描述，以确定包含最优条件干预的最小节点集，该干预能最大化期望奖励。然后，我们提出了一种高效算法，其时间复杂度为$O(|V| + |E|)$来识别这个最小节点集。我们证明了这种图形化特征描述和所提出的算法是正确的。最后，我们通过实验演示了当将我们的算法集成到标准多臂老虎机算法中时，它显著减少了搜索空间并大幅加快了收敛速度。', 'title_zh': '条件因果multi-armed bandits的最小搜索空间'}
{'arxiv_id': 'arXiv:2502.06575', 'title': 'Predictive Red Teaming: Breaking Policies Without Breaking Robots', 'authors': 'Anirudha Majumdar, Mohit Sharma, Dmitry Kalashnikov, Sumeet Singh, Pierre Sermanet, Vikas Sindhwani', 'link': 'https://arxiv.org/abs/2502.06575', 'abstract': 'Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2-7x.', 'abstract_zh': '通过模仿学习训练的视觉-运动策略能够在执行具有挑战性的操作任务时表现出色，但它们往往对光照条件、视觉干扰以及物体位置等环境因素极为脆弱。这些脆弱性在训练的具体细节上具有不可预测的依赖性，且要在没有硬件评估的情况下发现它们具有很大的挑战性。我们提出了预测性红队攻击（Predictive Red Teaming）的问题：发现策略在环境因素方面的脆弱性，并通过模拟非典型场景评估预测这些性能下降。为了实现这一目标，我们开发了RoboART：一种自动化的红队攻击（Red Teaming, ART）流水线，（1）利用生成的图像编辑方法在名义观测中修改不同的环境因素；（2）通过在编辑后的观测上运行针对策略的异常检测器来预测每种变化下的性能。实验结果表明，在十二种非典型条件下进行的500多次硬件试验结果显示，RoboART能够以高精度（预测成功率与实际成功率之间的平均差异小于0.19）预测性能下降。此外，我们还展示了预测性红队攻击如何实现有针对性的数据收集：使用在预期具有不良条件下的数据进行微调，对基础性能的提升幅度可达2-7倍。', 'title_zh': '预测性红色团队攻击：在不破坏机器人的情况下破解政策'}
{'arxiv_id': 'arXiv:2502.06572', 'title': 'LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM', 'authors': 'Zhi Zhou, Kun-Yang Yu, Shi-Yu Tian, Jiang-Xin Shi, Xiao-Wen Yang, Pengxiao Song, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2502.06572', 'abstract': 'Large language models (LLMs), both proprietary and open-source, have demonstrated remarkable capabilities across various natural language processing tasks. However, they face significant limitations in legal reasoning tasks. Proprietary models introduce data privacy risks and high inference costs, while open-source models underperform due to insufficient legal domain training data. To address these limitations, we study data generation for legal reasoning to improve the legal reasoning performance of open-source LLMs with the help of proprietary LLMs. This is challenging due to the lack of legal knowledge in proprietary LLMs and the difficulty in verifying the generated data. We propose KgDG, a knowledge-guided data generation framework for legal reasoning. Our framework enables leveraging legal knowledge to enhance generation diversity and introduces a refinement and verification process to ensure the quality of generated data. Moreover, we expand the generated dataset to further enhance the LLM reasoning capabilities. Using KgDG, we create a synthetic legal reasoning dataset containing 50K high-quality examples. Our trained model LawGPT outperforms existing legal-specific LLMs and achieves performance comparable to proprietary LLMs, demonstrating the effectiveness of KgDG and LawGPT. Our code and resources is publicly available at this https URL .', 'abstract_zh': '大型语言模型（LLMs），无论是专有模型还是开源模型，已经在各种自然语言处理任务中展示了卓越的能力。然而，在法律推理任务中它们面临显著的限制。专有模型引入了数据隐私风险和高推理成本，而开源模型则因训练数据不足在法律领域表现不佳。为了克服这些限制，我们研究了法律推理的数据生成技术，以通过专有LLMs提升开源LLMs的法律推理性能。这一挑战主要源于专有LLMs缺乏法律知识，以及生成数据难以验证。为此，我们提出了一种知识引导的数据生成框架（KgDG）用于法律推理。该框架能够利用法律知识增强生成的多样性和完整性，并通过修正和验证过程确保生成数据的质量。此外，我们扩展了生成的语料库，进一步增强了LLM的推理能力。通过使用KgDG，我们创建了一个包含5万个高质量样本的合成法律推理数据集。我们训练的模型LawGPT在现有专门的LLMs中表现更优，并且达到了与专有LLMs相当的性能，这证明了KgDG和LawGPT的有效性。我们的代码和资源已在此 <https://your-link-here.com> 公开。', 'title_zh': 'LawGPT：知识引导的数据生成及其在法律大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.06516', 'title': 'Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation', 'authors': 'Soobin Um, Beomsu Kim, Jong Chul Ye', 'link': 'https://arxiv.org/abs/2502.06516', 'abstract': 'Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations.', 'abstract_zh': '少数样本位于数据流形的低密度区域，是生成人工智能（Generative AI）应用中的宝贵资源，如数据增强、创造性内容生成等。然而，现有的基于扩散模型的少数样本生成器通常依赖于专门用于少数样本生成的计算密集型指导。为解决这一问题，我们提出了一种简单而强大的无指导方法——Boost-and-Skip，用于使用扩散模型生成少数样本。我们的框架仅需要对标准生成过程进行两项基本的修改：(i) 方差增强初始化和(ii) 时间步长跳过。我们强调，这些看似简单的修改得到了坚实的理论和实证支持，从而有效地促进了稀有少数样本特征的涌现。我们的全面实验表明，Boost-and-Skip 显著增强了生成少数样本的能力，甚至在消耗显著更少计算资源的情况下，与基于指导的最新方法相比具有竞争性。', 'title_zh': '增强并跳过：一种简单的无需指导的扩散模型用于少数群体生成'}
{'arxiv_id': 'arXiv:2502.06494', 'title': 'GuideLLM: Exploring LLM-Guided Conversation with Applications in Autobiography Interviewing', 'authors': 'Jinhao Duan, Xinyu Zhao, Zhuoxuan Zhang, Eunhye Ko, Lily Boddy, Chenan Wang, Tianhao Li, Alexander Rasgon, Junyuan Hong, Min Kyung Lee, Chenxi Yuan, Qi Long, Ying Ding, Tianlong Chen, Kaidi Xu', 'link': 'https://arxiv.org/abs/2502.06494', 'abstract': "Although Large Language Models (LLMs) succeed in human-guided conversations such as instruction following and question answering, the potential of LLM-guided conversations-where LLMs direct the discourse and steer the conversation's objectives-remains under-explored. In this study, we first characterize LLM-guided conversation into three fundamental components: (i) Goal Navigation; (ii) Context Management; (iii) Empathetic Engagement, and propose GuideLLM as an installation. We then implement an interviewing environment for the evaluation of LLM-guided conversation. Specifically, various topics are involved in this environment for comprehensive interviewing evaluation, resulting in around 1.4k turns of utterances, 184k tokens, and over 200 events mentioned during the interviewing for each chatbot evaluation. We compare GuideLLM with 6 state-of-the-art LLMs such as GPT-4o and Llama-3-70b-Instruct, from the perspective of interviewing quality, and autobiography generation quality. For automatic evaluation, we derive user proxies from multiple autobiographies and employ LLM-as-a-judge to score LLM behaviors. We further conduct a human-involved experiment by employing 45 human participants to chat with GuideLLM and baselines. We then collect human feedback, preferences, and ratings regarding the qualities of conversation and autobiography. Experimental results indicate that GuideLLM significantly outperforms baseline LLMs in automatic evaluation and achieves consistent leading performances in human ratings.", 'abstract_zh': '虽然大型语言模型（LLMs）在指导性的对话中，如指令跟随和问答方面取得了成功，但在LLM引导的对话中（在此过程中，LLMs引导对话并控制对话的目标）的潜力仍被大大低估。在本研究中，我们首先将LLM引导的对话分为三个基本组成部分：(i) 目标导航；(ii) 上下文管理；(iii) 共情参与，并提出GuideLLM作为一个系统。接着，我们构建了一个面试环境来评估LLM引导的对话。具体地，该环境中涵盖了各种主题，以便进行全面的面试评估，结果产生了大约1400轮对话，18.4万个令牌，以及在每次对话评估中提到的超过200个事件。我们从六个当前最先进的LLM（如GPT-4o和Llama-3-70b-Instruct）的角度来进行比较，从面试质量和自传生成质量两个方面进行比较。对于自动评估，我们从多份自传中提取用户代理，并利用LLM作为评委来评分LLM的行为。此外，我们通过45名人类参与者与GuideLLM和基线进行互动，进行一项包含人类参与的实验。然后，我们收集了关于对话和自传质量的人类反馈、偏好和评价。实验结果表明，GuideLLM在自动评估中显著优于基线LLM，并且在人类评价中持续表现出色。', 'title_zh': 'GuideLLM：探索基于LLM的对话指导应用：以自传式访谈为例'}
{'arxiv_id': 'arXiv:2502.06491', 'title': 'Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling', 'authors': 'Shenghong He', 'link': 'https://arxiv.org/abs/2502.06491', 'abstract': 'Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \\textbf{R}eliability-guaranteed \\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.', 'abstract_zh': '基于模型的离线强化学习（Model-based Offline Reinforcement Learning, MORL）旨在通过利用从现有数据集中派生的动力学模型来学习策略。现有MORL工作的大多数方法通过对动力学模型进行保守量化，生成近似真实数据分布的轨迹，以便利用当前信息（例如，在时间步t的状态和行动）来促进策略学习。然而，这些工作忽略了历史信息对环境动力学的影响，导致生成的轨迹可能不可靠且与真实数据分布不一致。本文提出了一种新的MORL算法**可靠性保证的转换器（Reliability-guaranteed Transformer, RT）**，该算法通过计算生成轨迹的累积可靠性（即，使用加权变差距离远离真实数据）来消除不可靠的轨迹。此外，通过采样具有高奖励的候选行动，RT可以从现有离线数据中高效生成高回报轨迹。我们从理论上证明了RT在策略学习方面的性能保证，并通过在多个基准任务上与最新的基于模型的方法进行实验对比，展示了其有效性。', 'title_zh': '基于模型的离线强化学习：具有可靠性保证的序列建模'}
{'arxiv_id': 'arXiv:2502.06490', 'title': 'Recent Advances in Discrete Speech Tokens: A Review', 'authors': 'Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu', 'link': 'https://arxiv.org/abs/2502.06490', 'abstract': 'The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.', 'abstract_zh': '在大规模语言模型（LLMs）时代，语音生成技术的快速发展已确立了离散语音令牌作为语音表示的基础范式。这些令牌以其离散、紧凑和简洁的特性，在高效传输和存储方面具有优势，并且天然兼容语言模型框架，使得语音能够无缝融入以文本为中心的LLM架构中。当前的研究将离散语音令牌主要划分为两类：声学令牌和语义令牌，每种类型都形成了一个独特的研究领域，其设计理念和方法论特征各异。\n\n本文系统地总结了离散语音分词的现有分类和最近的创新成果，对每种范式的优点和限制进行了深入分析，并进行了不同令牌类型的系统实验比较。此外，我们指出了该领域存在的持续挑战，并提出了潜在的研究方向，旨在为未来离散语音令牌的发展和应用提供可操作的见解，激发更多的研究进展。', 'title_zh': '最近离散语音 token 的研究进展：一个综述'}
{'arxiv_id': 'arXiv:2502.06485', 'title': 'WyckoffDiff - A Generative Diffusion Model for Crystal Symmetry', 'authors': 'Filip Ekström Kelvinius, Oskar B. Andersson, Abhijith S. Parackal, Dong Qian, Rickard Armiento, Fredrik Lindsten', 'link': 'https://arxiv.org/abs/2502.06485', 'abstract': 'Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fréchet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WyckoffDiff against recently proposed generative models for crystal generation.', 'abstract_zh': '晶态材料通常表现出较高的对称性。然而，大多数生成模型并未考虑这一点，而是对每个原子的位置和元素不加任何限制地进行建模。我们提出了一种生成模型，称为Wickoff扩散（WyckoffDiff），它可以生成基于对称性的晶态描述。这得益于我们考虑了一种包含所有对称性的晶格结构表示，我们设计了一种新的神经网络架构，使其能够在离散生成模型框架中使用这种表示。除了通过构建方式保持对称性外，我们模型的离散性质还使生成过程变得快速。此外，我们还提出了一种新的度量标准，即Fréchet Wrenformer 距离，用于捕获生成材料的对称性特征，并将WyckoffDiff与最近提出的用于晶态生成的生成模型进行了基准测试。', 'title_zh': 'WyckoffDiff - 一种用于晶体对称性的生成扩散模型'}
{'arxiv_id': 'arXiv:2502.06472', 'title': 'KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment', 'authors': 'Yuxing Lu, Jinzhuo Wang', 'link': 'https://arxiv.org/abs/2502.06472', 'abstract': 'Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.', 'abstract_zh': '维护全面和最新的知识图谱（KGs）对于现代AI系统至关重要，但手动编目难以应对科学文献的快速增长。本文提出了KARMA框架，该框架采用多智能体大规模语言模型（LLMs），通过结构化分析非结构化文本自动化增强KG。我们的方法采用了九个协作智能体，覆盖实体发现、关系抽取、模式对齐和冲突解决，这些智能体迭代解析文档、验证提取的知识，并将其整合到现有的图结构中，同时遵循特定领域的模式。实验使用来自三个不同领域的1,200篇PubMed文章展示了KARMA在知识图谱增强中的有效性，成功识别了高达38,230个新实体，LLM验证的正确率为83.1%，并通过多层次评估减少了18.6%的冲突边。', 'title_zh': 'KARMA：利用多智能体大型语言模型进行自动化知识图谱扩充'}
{'arxiv_id': 'arXiv:2502.06470', 'title': 'A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks', 'authors': 'Hieu Minh "Jord" Nguyen', 'link': 'https://arxiv.org/abs/2502.06470', 'abstract': 'Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks.', 'abstract_zh': '理论心智（Theory of Mind，ToM），即归因他人心理状态并预测其行为的能力，是社会智能的基础。在本文中，我们综述了对大型语言模型（LLMs）的行为性和表征性ToM的研究结果，识别了高级LLM ToM能力可能带来的重要安全风险，并建议了几条有效的评估和缓解这些风险的研究方向。', 'title_zh': '大型语言模型中的理论思维综述：评估、表示与安全风险'}
{'arxiv_id': 'arXiv:2502.06453', 'title': "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations", 'authors': 'Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, Yue Wu, Ming Yin, Shange Tang, Yangsibo Huang, Chi Jin, Xinyun Chen, Chiyuan Zhang, Mengdi Wang', 'link': 'https://arxiv.org/abs/2502.06453', 'abstract': 'Large language models have demonstrated impressive performance on challenging mathematical reasoning tasks, which has triggered the discussion of whether the performance is achieved by true reasoning capability or memorization. To investigate this question, prior work has constructed mathematical benchmarks when questions undergo simple perturbations -- modifications that still preserve the underlying reasoning patterns of the solutions. However, no work has explored hard perturbations, which fundamentally change the nature of the problem so that the original solution steps do not apply. To bridge the gap, we construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard perturbation, respectively. Each consists of 279 perturbed math problems derived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et. al., 2021). We observe significant performance drops on MATH-P-Hard across various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking (-12.9%). We also raise concerns about a novel form of memorization where models blindly apply learned problem-solving skills without assessing their applicability to modified contexts. This issue is amplified when using original problems for in-context learning. We call for research efforts to address this challenge, which is critical for developing more robust and reliable reasoning models.', 'abstract_zh': '大型语言模型在具有挑战性的数学推理任务上展示了令人印象深刻的性能，这引发了关于其性能是基于真正的推理能力还是基于记忆的讨论。为了探讨这一问题，先前的研究通过简单扰动（即对问题进行修改但仍保留解决方案背后推理模式的修改）构建了数学基准。然而，尚未有研究探索根本性的扰动，这种扰动会从根本上改变问题的性质，使得原始的解题步骤不再适用。为解决这一问题，我们分别通过简单扰动和根本性扰动构建了 MATH-P-Simple 和 MATH-P-Hard。每个数据集包含从 MATH 数据集（Hendrycks et al., 2021）最难级别的279个问题中衍生出的问题。在 MATH-P-Hard 上，各种模型的性能均有显著下降，包括 o1-mini（-16.49%）和 gemini-2.0-flash-thinking（-12.9%）。此外，我们还探讨了一种新的记忆形式，即模型在没有评估这些技能在修改后的上下文中的适用性的情况下盲目地应用所学的解题技能。这种问题在使用原始问题进行在上下文学习时尤为突出。我们呼吁研究界关注这一挑战，并强调解决这一问题对于开发更稳健和可靠的推理模型至关重要。', 'title_zh': 'MATH-Perturb: 基于难扰动的大型语言模型数学推理能力基准测试'}
{'arxiv_id': 'arXiv:2502.06440', 'title': 'SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding', 'authors': 'Shuhao Liao, Weihang Xia, Yuhong Cao, Weiheng Dai, Chengyang He, Wenjun Wu, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2502.06440', 'abstract': 'The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments.', 'abstract_zh': '多智能体路径finding（MAPF）问题旨在确定在已知且可能充满障碍物的环境中，多个智能体的最短且无碰撞路径。这是在大规模物流和交通部署中智能体的关键挑战。基于去中心化的学习方法已经显示出解决MAPF问题的巨大潜力，提供了更加反应迅速和可扩展的解决方案。然而，现有的基于学习的MAPF方法通常依赖于智能体基于有限的视野（FOV）进行决策，这会导致短视的策略和在复杂场景中的无效合作。在此背景下，一个关键挑战是基于有限的观察和通信实现智能体之间潜在移动的一致性。为解决这一挑战，我们引入了一个新的框架，该框架将束理论应用于去中心化的深度强化学习，使智能体通过局部一致性的学习来了解彼此之间的几何交叉依赖关系，并将这些关系用于紧密的合作决策。特别是，束理论提供了通过局部观察实现全局一致性的数学证明。受此启发，我们结合神经网络在束理论的基础上近似建模潜空间中的一致性，并通过自监督学习对其进行训练。在执行任务时，除了之前的MAPF工作中的正常特征外，每个智能体分布式地推理学习到的一致性特征，从而在路径规划和碰撞避免中实现高效的协作。结果，我们提出的方法在相对较大的复杂场景中显著优于现有的基于学习的MAPF规划方法，在各种模拟和实际机器人实验中表现出其优越性。', 'title_zh': 'SIGMA： sheaf-知情几何多agent路径规划\n\n解释：这句话是标题翻译，保持了原文的专业术语，同时也符合中文的表达习惯。"Sheaf-Informed"被译为"sheaf-知情"，指的是通过sheaf理论指导信息处理；"Geometric Multi-Agent Pathfinding"则被译为"几何多agent路径规划"，这是机器人和多agent系统中的一个研究领域，指多个智能体在几何环境中寻找不相冲突的路径的问题。'}
{'arxiv_id': 'arXiv:2502.06439', 'title': 'Testing software for non-discrimination: an updated and extended audit in the Italian car insurance domain', 'authors': 'Marco Rondina, Antonio Vetrò, Riccardo Coppola, Oumaima Regragrui, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Juan Carlos De Martin', 'link': 'https://arxiv.org/abs/2502.06439', 'abstract': "Context. As software systems become more integrated into society's infrastructure, the responsibility of software professionals to ensure compliance with various non-functional requirements increases. These requirements include security, safety, privacy, and, increasingly, non-discrimination.\nMotivation. Fairness in pricing algorithms grants equitable access to basic services without discriminating on the basis of protected attributes.\nMethod. We replicate a previous empirical study that used black box testing to audit pricing algorithms used by Italian car insurance companies, accessible through a popular online system. With respect to the previous study, we enlarged the number of tests and the number of demographic variables under analysis.\nResults. Our work confirms and extends previous findings, highlighting the problematic permanence of discrimination across time: demographic variables significantly impact pricing to this day, with birthplace remaining the main discriminatory factor against individuals not born in Italian cities. We also found that driver profiles can determine the number of quotes available to the user, denying equal opportunities to all.\nConclusion. The study underscores the importance of testing for non-discrimination in software systems that affect people's everyday lives. Performing algorithmic audits over time makes it possible to evaluate the evolution of such algorithms. It also demonstrates the role that empirical software engineering can play in making software systems more accountable.", 'abstract_zh': '背景。随着软件系统越来越多地嵌入社会基础设施中，软件专业人士保证其遵守各种非功能性需求的责任也在增加。这些需求包括安全性、安全性、隐私保护，以及越来越重要的非歧视问题。\n\n动机。价格算法的公平性确保了基本服务对所有人一视同仁，不基于受保护属性进行歧视。\n\n方法。我们重复了一项之前的研究，该研究使用黑盒测试来审核意大利汽车保险公司通过流行在线系统使用的定价算法。与先前的研究相比，我们增加了测试的数量和分析的统计变量的数量。\n\n结果。我们的研究确认并扩展了之前的研究发现，强调了非歧视的持久性问题：到今天为止，统计变量对定价仍具有显著影响，出生地仍然是对非意大利城市出生个体的主要歧视因素。我们还发现，司机的个人档案能够决定用户可以获得的报价数量，从而剥夺了所有人的平等机会。\n\n结论。本研究强调了在影响人们日常生活软件系统中进行非歧视测试的重要性。对算法进行持续的审计可以评估这些算法的演变。此外，研究还显示了实证软件工程在提高软件系统透明度方面的作用。', 'title_zh': '非歧视性软件测试：意大利汽车保险领域的更新与扩展审计'}
{'arxiv_id': 'arXiv:2502.06438', 'title': 'FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba Foundation Model', 'authors': 'Anna Tegon, Thorir Mar Ingolfsson, Xiaying Wang, Luca Benini, Yawei Li', 'link': 'https://arxiv.org/abs/2502.06438', 'abstract': 'Accurate and efficient electroencephalography (EEG) analysis is essential for detecting seizures and artifacts in long-term monitoring, with applications spanning hospital diagnostics to wearable health devices. Robust EEG analytics have the potential to greatly improve patient care. However, traditional deep learning models, especially Transformer-based architectures, are hindered by their quadratic time and memory complexity, making them less suitable for resource-constrained environments. To address these challenges, we present FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel self-supervised framework that establishes new efficiency benchmarks for EEG analysis through bidirectional state-space modeling. Unlike Transformer-based models, which incur quadratic time and memory complexity, FEMBA scales linearly with sequence length, enabling more scalable and efficient processing of extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and fine-tuned on three downstream tasks, FEMBA achieves competitive performance in comparison with transformer models, with significantly lower computational cost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB and 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates viability for resource-constrained devices. These results pave the way for scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as a promising candidate for wearable applications.', 'abstract_zh': '准确高效的脑电图（EEG）分析对于在长期监控中检测癫痫发作和伪迹至关重要，其应用范围从医院诊断扩展到可穿戴健康设备。稳健的EEG分析有望大大改善患者护理。然而，传统的深度学习模型，尤其是基于变换器的架构，受到其二次时间复杂度和内存复杂度的限制，使它们不适合资源受限的环境。为了解决这些挑战，我们提出了一种新颖的自监督框架FEMBA（Foundational EEG Mamba + Bidirectional Architecture），通过双向状态空间建模，该框架为EEG分析设定了新的效率基准。与基于变换器的模型不同，FEMBA的时间和内存复杂度与序列长度呈线性关系，使其能够更高效地处理长时间的EEG记录。FEMBA在超过21,000小时未标注的EEG数据上进行训练，并在三个下游任务上进行微调，其性能与变换器模型相比具有竞争力，同时计算成本显著较低。具体而言，FEMBA在TUAB上达到了81.82%的平衡准确率（0.8921 AUROC），在TUAR上达到了0.949 AUROC，而其小型7.8M参数版本则证明了其在资源受限设备中的可行性。这些结果为临床和可穿戴应用中的可扩展和通用EEG分析铺平了道路，并突显了FEMBA作为潜在候选模型的价值。', 'title_zh': 'FEMBA：基于双向Mamba基础模型的高效可扩展EEG分析方法'}
{'arxiv_id': 'arXiv:2502.06432', 'title': 'Prompt-SID: Learning Structural Representation Prompt via Latent Diffusion for Single-Image Denoising', 'authors': 'Huaqiu Li, Wang Zhang, Xiaowan Hu, Tao Jiang, Zikang Chen, Haoqian Wang', 'link': 'https://arxiv.org/abs/2502.06432', 'abstract': 'Many studies have concentrated on constructing supervised models utilizing paired datasets for image denoising, which proves to be expensive and time-consuming. Current self-supervised and unsupervised approaches typically rely on blind-spot networks or sub-image pairs sampling, resulting in pixel information loss and destruction of detailed structural information, thereby significantly constraining the efficacy of such methods. In this paper, we introduce Prompt-SID, a prompt-learning-based single image denoising framework that emphasizes preserving of structural details. This approach is trained in a self-supervised manner using downsampled image pairs. It captures original-scale image information through structural encoding and integrates this prompt into the denoiser. To achieve this, we propose a structural representation generation model based on the latent diffusion process and design a structural attention module within the transformer-based denoiser architecture to decode the prompt. Additionally, we introduce a scale replay training mechanism, which effectively mitigates the scale gap from images of different resolutions. We conduct comprehensive experiments on synthetic, real-world, and fluorescence imaging datasets, showcasing the remarkable effectiveness of Prompt-SID.', 'abstract_zh': '许多研究集中在利用配对数据集构建监督模型进行图像去噪，这证明是昂贵且耗时的。当前的自监督和无监督方法通常依赖于盲-spot网络或子图像配对采样，导致像素信息丢失和详细结构信息的破坏，从而显著限制了这些方法的效果。本文我们提出了基于提示学习的单张图像去噪框架Prompt-SID，该框架强调保留结构细节。该方法利用下采样的图像配对在自监督模式下进行训练，通过结构编码捕获原始尺度的图像信息，并将该提示融入去噪器中。为此，我们提出了一种基于潜在扩散过程的结构表示生成模型，并在基于变换器的去噪器架构中设计了一个结构注意力模块以解码提示。此外，我们引入了一种尺度回放训练机制，有效缓解了不同分辨率图像之间的尺度差距。我们在合成、真实世界和荧光成像数据集上进行了全面的实验，展示了Prompt-SID的显著效果。', 'title_zh': 'Prompt-SID：基于潜在扩散学习结构表示的单图像去噪方法'}
{'arxiv_id': 'arXiv:2502.06425', 'title': 'Generating Privacy-Preserving Personalized Advice with Zero-Knowledge Proofs and LLMs', 'authors': 'Hiroki Watanabe, Motonobu Uchikoshi', 'link': 'https://arxiv.org/abs/2502.06425', 'abstract': 'Large language models (LLMs) are increasingly utilized in domains such as finance, healthcare, and interpersonal relationships to provide advice tailored to user traits and contexts. However, this personalization often relies on sensitive data, raising critical privacy concerns and necessitating data minimization. To address these challenges, we propose a framework that integrates zero-knowledge proof (ZKP) technology, specifically zkVM, with LLM-based chatbots. This integration enables privacy-preserving data sharing by verifying user traits without disclosing sensitive information. Our research introduces both an architecture and a prompting strategy for this approach. Through empirical evaluation, we clarify the current constraints and performance limitations of both zkVM and the proposed prompting strategy, thereby demonstrating their practical feasibility in real-world scenarios.', 'abstract_zh': '大型语言模型（LLMs）在金融、医疗保健和个人关系等领域越来越被用于提供针对用户特征和情境的个性化建议。然而，这种个性化通常依赖于敏感数据，从而引发了重要的隐私问题，需要进行数据最小化处理。为应对这些挑战，我们提出了一种框架，该框架将零知识证明（ZKP）技术，特别是zkVM，与基于LLM的聊天机器人相结合。这种集成使得在验证用户特征时可以保护隐私，而不泄露敏感信息。我们的研究引入了该方法的架构和提示策略。通过实证评估，我们阐明了zkVM和提出的方法所面临的当前限制和性能局限，从而证明了该方法在实际应用场景中的可行性。', 'title_zh': '基于零知识证明和大语言模型的隐私保护个性化建议生成'}
{'arxiv_id': 'arXiv:2502.06424', 'title': 'CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better Interpretability of Intelligent Fault Diagnosis', 'authors': 'Qian Chen, Xingjian Dong, Kui Hu, Kangkang Chen, Zhike Peng, Guang Meng', 'link': 'https://arxiv.org/abs/2502.06424', 'abstract': 'Neural networks (NNs), with their powerful nonlinear mapping and end-to-end capabilities, are widely applied in mechanical intelligent fault diagnosis (IFD). However, as typical black-box models, they pose challenges in understanding their decision basis and logic, limiting their deployment in high-reliability scenarios. Hence, various methods have been proposed to enhance the interpretability of IFD. Among these, post-hoc approaches can provide explanations without changing model architecture, preserving its flexibility and scalability. However, existing post-hoc methods often suffer from limitations in explanation forms. They either require preprocessing that disrupts the end-to-end nature or overlook fault mechanisms, leading to suboptimal explanations. To address these issues, we derived the cyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley additive explanations (SHAP) to the CS domain. CS-SHAP can evaluate contributions from both carrier and modulation frequencies, aligning more closely with fault mechanisms and delivering clearer and more accurate explanations. Three datasets are utilized to validate the superior interpretability of CS-SHAP, ensuring its correctness, reproducibility, and practical performance. With open-source code and outstanding interpretability, CS-SHAP has the potential to be widely adopted and become the post-hoc interpretability benchmark in IFD, even in other classification tasks. The code is available on this https URL.', 'abstract_zh': '神经网络（NNs），凭借其强大的非线性映射能力和端对端的能力，在机械智能故障诊断（IFD）中得到广泛应用。然而，作为典型的黑盒模型，NNs 在理解其决策基础和逻辑方面存在挑战，限制了其在高可靠场景中的应用。因此，提出了一系列方法来增强IFD的可解释性。在这之中，事后方法（post-hoc approaches）可以在不改变模型架构的情况下提供解释，保持模型的灵活性和可扩展性。然而，现有的事后方法在解释形式上存在诸多局限性。它们要么需要破坏端对端性质的预处理，要么忽略了故障机理，导致解释效果不理想。为解决这些问题，我们推导了循环谱（cyclic-spectral, CS）变换，并通过将Shapley增益解释（Shapley additive explanations, SHAP）扩展到CS域中提出了CS-SHAP。CS-SHAP 能够评估载波频率和调制频率的贡献，更贴近故障机理，提供更清晰、更准确的解释。我们利用三个数据集验证了CS-SHAP的优越可解释性，确保了其正确性、可重现实验结果以及实际性能。凭借开源代码和出色的可解释性，CS-SHAP 有可能广泛应用于IFD，并成为事后可解释性的基准，甚至扩展到其他分类任务。代码可通过以下链接访问：[这里](https://example.com)。', 'title_zh': 'CS-SHAP：将SHAP扩展到循环频谱域以提高智能故障诊断的可解释性'}
{'arxiv_id': 'arXiv:2502.06415', 'title': 'Systematic Outliers in Large Language Models', 'authors': 'Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang', 'link': 'https://arxiv.org/abs/2502.06415', 'abstract': "Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers-activation outliers, weight outliers, and attention outliers-and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at this https URL.", 'abstract_zh': '异常值在大规模语言模型（LLMs）中已被广泛观察到，这显著影响了模型的性能，并为模型压缩带来了挑战。理解这些异常值的功能及其形成机制至关重要。现有的研究工作大多从算法角度出发，试图降低异常值的影响，但缺乏对其根本原因和作用的深入探讨。在本研究中，我们对LLMs中异常值的形成过程、根本原因及其功能进行了详细分析。我们定义并分类了三种类型的异常值——激活异常值、权重异常值和注意力异常值，并分析了它们在不同维度上的分布，揭示了它们的发生与其对注意力机制最终影响之间的内在联系。基于这些观察，我们提出了这些异常值产生的机制及其功能，通过理论推导和实验表明，它们是由自注意力机制的softmax操作引起。这些异常值在注意力机制中充当着隐式的上下文感知缩放因子。由于这些异常值源自系统性影响，我们称之为系统性异常值。我们的研究不仅加深了对基于Transformer的LLMs的理解，还表明结构化地消除异常值可以加速收敛并提高模型压缩。代码可以通过以下链接获取：this https URL。', 'title_zh': '大型语言模型中的系统性异常值'}
{'arxiv_id': 'arXiv:2502.06379', 'title': 'Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled Diffusion Sequential Monte Carlo', 'authors': 'Filip Ekström Kelvinius, Zheng Zhao, Fredrik Lindsten', 'link': 'https://arxiv.org/abs/2502.06379', 'abstract': 'A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data.', 'abstract_zh': '近期的研究工作利用预训练生成扩散模型作为先验，用于解决贝叶斯逆问题。在此研究方向上，我们设计了一种基于“解耦扩散”的序列蒙特卡洛方法，用于线性高斯逆问题，使得样本更新量更大成为可能。该方法在渐近意义上是精确的，并通过对合成数据和图像重建任务的实验，证明了我们所提出的解耦扩散序列蒙特卡洛（DDSMC）算法的有效性。此外，我们还展示了该方法如何扩展到离散数据的应用。', 'title_zh': '解线性高斯贝叶斯逆问题的解耦扩散顺序蒙特卡洛方法'}
{'arxiv_id': 'arXiv:2502.06374', 'title': 'Hyperparameters in Score-Based Membership Inference Attacks', 'authors': 'Gauri Pradhan, Joonas Jälkö, Marlon Tobaben, Antti Honkela', 'link': 'https://arxiv.org/abs/2502.06374', 'abstract': "Membership Inference Attacks (MIAs) have emerged as a valuable framework for evaluating privacy leakage by machine learning models. Score-based MIAs are distinguished, in particular, by their ability to exploit the confidence scores that the model generates for particular inputs. Existing score-based MIAs implicitly assume that the adversary has access to the target model's hyperparameters, which can be used to train the shadow models for the attack. In this work, we demonstrate that the knowledge of target hyperparameters is not a prerequisite for MIA in the transfer learning setting. Based on this, we propose a novel approach to select the hyperparameters for training the shadow models for MIA when the attacker has no prior knowledge about them by matching the output distributions of target and shadow models. We demonstrate that using the new approach yields hyperparameters that lead to an attack near indistinguishable in performance from an attack that uses target hyperparameters to train the shadow models. Furthermore, we study the empirical privacy risk of unaccounted use of training data for hyperparameter optimization (HPO) in differentially private (DP) transfer learning. We find no statistically significant evidence that performing HPO using training data would increase vulnerability to MIA.", 'abstract_zh': '成员推断攻击（Membership Inference Attacks, MIAs）已经成为评估机器学习模型隐私泄露的一个有价值的框架。基于评分的MIAs特别能够利用模型为特定输入生成的信任分数。现有的基于评分的MIAs隐含地假设对手可以访问目标模型的超参数，这些超参数可以用于训练用于攻击的影子模型。在这项工作中，我们展示了在迁移学习的背景下，了解目标模型的超参数并不是进行MIA的先决条件。基于此，我们提出了一种新的方法，在攻击者对超参数一无所知的情况下，通过匹配目标模型和影子模型的输出分布来选择用于训练影子模型的超参数。我们证明了使用新方法选择的超参数可以导致性能几乎与使用目标模型的超参数训练影子模型的攻击相媲美的攻击。此外，我们研究了在不同隐私保护（DP）迁移学习中未考虑到的用于超参数优化（HPO）的训练数据带来的经验隐私风险。我们发现，使用训练数据进行HPO不会在统计上显著增加MIA的脆弱性。', 'title_zh': '基于分数的成员推理攻击中的超参数研究'}
{'arxiv_id': 'arXiv:2502.06348', 'title': 'AiRacleX: Automated Detection of Price Oracle Manipulations via LLM-Driven Knowledge Mining and Prompt Generation', 'authors': 'Bo Gao, Yuan Wang, Qingsong Wei, Yong Liu, Rick Siow Mong Goh', 'link': 'https://arxiv.org/abs/2502.06348', 'abstract': 'Decentralized finance applications depend on accurate price oracles to ensure secure transactions, yet these oracles are highly vulnerable to manipulation, enabling attackers to exploit smart contract vulnerabilities for unfair asset valuation and financial gain. Detecting such manipulations traditionally relies on the manual effort of experienced experts, presenting significant challenges. In this paper, we propose a novel LLM-driven framework that automates the detection of price oracle manipulations by leveraging the complementary strengths of different LLM models. Our approach begins with domain-specific knowledge extraction, where an LLM model synthesizes precise insights about price oracle vulnerabilities from top-tier academic papers, eliminating the need for profound expertise from developers or auditors. This knowledge forms the foundation for a second LLM model to generate structured, context-aware chain of thought prompts, which guide a third LLM model in accurately identifying manipulation patterns in smart contracts. We validate the framework effectiveness through experiments on 60 known vulnerabilities from 46 real-world DeFi attacks or projects spanning 2021 to 2023. The best performing combination of LLMs (Haiku-Haiku-4o-mini) identified by AiRacleX demonstrate a 2.58-times improvement in recall (0.667 vs 0.259) compared to the state-of-the-art tool GPTScan, while maintaining comparable precision. Furthermore, our framework demonstrates the feasibility of replacing commercial models with open-source alternatives, enhancing privacy and security for developers.', 'abstract_zh': '去中心化金融应用依赖于准确的价格预言机来确保安全交易，但这些预言机极易受到操纵，使攻击者能够利用智能合约漏洞进行不公平的资产估值并牟利。传统的检测方法通常依赖于有经验专家的手动努力，这带来了显著的挑战。本文提出了一种新颖的基于大型语言模型（LLM）的框架，通过利用不同LLM模型的互补优势来自动化价格预言机操纵的检测。该方法首先从领域特定知识提取开始，使用一个LLM模型综合提炼顶级学术论文中的精确见解，从而消除开发人员或审计人员对深厚专业知识的需求。这些知识为第二个LLM模型生成结构化的、上下文感知的思考链提示奠定了基础，这些提示引导第三个LLM模型准确识别智能合约中的操纵模式。我们通过实验验证了该框架的有效性，实验对象包括2021年至2023年间46个实际DeFi攻击或项目中的60个已知漏洞。由AiRacleX识别的最有效的LLM组合（Haiku-Haiku-4o-mini）相对于最新工具GPTScan，在召回率方面显示出2.58倍的改进（0.667 vs 0.259），同时保持了相当的精度。此外，我们的框架展示了用开源替代品替换商业模型的可行性，从而提高了开发者的隐私和安全性。', 'title_zh': 'AiRacleX：通过LLM驱动的知识挖掘和提示生成实现的价格 oracle 操纵自动化检测'}
{'arxiv_id': 'arXiv:2502.06341', 'title': 'Facial Analysis Systems and Down Syndrome', 'authors': 'Marco Rondina, Fabiana Vinci, Antonio Vetrò, Juan Carlos De Martin', 'link': 'https://arxiv.org/abs/2502.06341', 'abstract': 'The ethical, social and legal issues surrounding facial analysis technologies have been widely debated in recent years. Key critics have argued that these technologies can perpetuate bias and discrimination, particularly against marginalized groups. We contribute to this field of research by reporting on the limitations of facial analysis systems with the faces of people with Down syndrome: this particularly vulnerable group has received very little attention in the literature so far. This study involved the creation of a specific dataset of face images. An experimental group with faces of people with Down syndrome, and a control group with faces of people who are not affected by the syndrome. Two commercial tools were tested on the dataset, along three tasks: gender recognition, age prediction and face labelling. The results show an overall lower accuracy of prediction in the experimental group, and other specific patterns of performance differences: i) high error rates in gender recognition in the category of males with Down syndrome; ii) adults with Down syndrome were more often incorrectly labelled as children; iii) social stereotypes are propagated in both the control and experimental groups, with labels related to aesthetics more often associated with women, and labels related to education level and skills more often associated with men. These results, although limited in scope, shed new light on the biases that alter face classification when applied to faces of people with Down syndrome. They confirm the structural limitation of the technology, which is inherently dependent on the datasets used to train the models.', 'abstract_zh': '近年来，面部分析技术所涉及的伦理、社会和法律问题引发了广泛讨论。关键批评者指出，这些技术可能延续偏见和歧视，特别是针对边缘化群体。我们通过在唐氏综合症患者面部进行研究，为这一领域做出了贡献：该项目特别关注的是尚未在文献中得到充分关注的这一极其脆弱的群体。该研究涉及创建特定的面部图像数据集。实验组包括唐氏综合症患者的面部，对照组包括未受该综合征影响的个体。对数据集进行了两项商业工具的测试，涵盖了三项任务：性别识别、年龄预测和面部标签。结果显示，实验组的预测准确性整体较低，还出现了其他特定的性能差异模式：（i）唐氏综合症男性在性别识别上的高错误率；（ii）唐氏综合症成年个体更常被错误标记为儿童；（iii）社会刻板印象在控制组和实验组中传播，与美学相关的标签更常与女性关联，与教育水平和技能相关的标签更常与男性关联。尽管这些结果在范围上有所限制，但它们揭示了当面部分析技术应用于唐氏综合症患者面部时所改变的偏见。这些结果确认了技术的结构局限性，这种局限性从根本上依赖于训练模型的数据集。', 'title_zh': '面部分析系统与唐氏综合症研究'}
{'arxiv_id': 'arXiv:2502.06336', 'title': 'DefTransNet: A Transformer-based Method for Non-Rigid Point Cloud Registration in the Simulation of Soft Tissue Deformation', 'authors': 'Sara Monji-Azad, Marvin Kinz, Siddharth Kothari, Robin Khanna, Amrei Carla Mihan, David Maennel, Claudia Scherl, Juergen Hesser', 'link': 'https://arxiv.org/abs/2502.06336', 'abstract': 'Soft-tissue surgeries, such as tumor resections, are complicated by tissue deformations that can obscure the accurate location and shape of tissues. By representing tissue surfaces as point clouds and applying non-rigid point cloud registration (PCR) methods, surgeons can better understand tissue deformations before, during, and after surgery. Existing non-rigid PCR methods, such as feature-based approaches, struggle with robustness against challenges like noise, outliers, partial data, and large deformations, making accurate point correspondence difficult. Although learning-based PCR methods, particularly Transformer-based approaches, have recently shown promise due to their attention mechanisms for capturing interactions, their robustness remains limited in challenging scenarios. In this paper, we present DefTransNet, a novel end-to-end Transformer-based architecture for non-rigid PCR. DefTransNet is designed to address the key challenges of deformable registration, including large deformations, outliers, noise, and partial data, by inputting source and target point clouds and outputting displacement vector fields. The proposed method incorporates a learnable transformation matrix to enhance robustness to affine transformations, integrates global and local geometric information, and captures long-range dependencies among points using Transformers. We validate our approach on four datasets: ModelNet, SynBench, 4DMatch, and DeformedTissue, using both synthetic and real-world data to demonstrate the generalization of our proposed method. Experimental results demonstrate that DefTransNet outperforms current state-of-the-art registration networks across various challenging conditions. Our code and data are publicly available.', 'abstract_zh': '以下是论文内容或标题的中文翻译，符合学术规范：\n\n软组织手术，如肿瘤切除，因组织变形而导致准确的组织位置和形状难以识别。通过将组织表面表示为点云，并应用非刚性点云注册（Non-rigid Point Cloud Registration, PCR）方法，外科医生可以在手术前、中、后更好地理解组织变形。现有的非刚性PCR方法，例如基于特征的方法，在面对噪声、离群值、部分数据和大变形等挑战时，其鲁棒性有限，使得准确的点对应变得困难。尽管基于学习的PCR方法，尤其是基于Transformer的方法，由于其注意力机制在捕捉交互方面具有潜力，但在挑战性场景中的鲁棒性仍然有限。在本文中，我们提出了一种新的端到端Transformer架构DefTransNet，以解决可变形注册的关键挑战，包括大变形、离群值、噪声和部分数据。DefTransNet通过输入源点云和目标点云并输出位移矢量场来应对这些挑战。所提出的方法融合适用的变换矩阵以增强对仿射变换的鲁棒性，整合全局和局部几何信息，并使用Transformer捕捉点之间的长程依赖关系。我们使用四组数据集（ModelNet、SynBench、4DMatch和DeformedTissue）进行验证，使用合成和真实数据来证明我们提出方法的泛化能力。实验结果表明，DefTransNet在各种挑战条件下均优于当前最先进的注册网络。我们的代码和数据已公开可用。', 'title_zh': 'DefTransNet：基于Transformer的方法在软组织形变模拟中的非刚性点云注册'}
{'arxiv_id': 'arXiv:2502.06327', 'title': 'Prompt-Driven Continual Graph Learning', 'authors': 'Qi Wang, Tianfei Zhou, Ye Yuan, Rui Mao', 'link': 'https://arxiv.org/abs/2502.06327', 'abstract': 'Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest. Mainstream solutions adopt the memory replay-based idea, ie, caching representative data from earlier tasks for retraining the graph model. However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy. Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed. In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks. More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning. Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale. Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption. Our code is available at this https URL.', 'abstract_zh': '持续图学习（Continual Graph Learning, CGL）的目标是在图数据不断演变的情况下，能够适应新的任务而不遗忘先前的知识，这一领域正逐渐引起广泛关注。主流解决方案采用了基于记忆重放的思想，即缓存早期任务中的代表性数据以重新训练图模型。然而，这种方法在面对不断演变的图数据时遇到了可扩展性问题，并引发了数据隐私方面的担忧。受到基于提示的学习范式近期进展的启发，本文提出了一种新颖的提示驱动持续图学习（PROMPTCGL）框架，该框架为每个新来的任务学习一个独立的提示，并保持底层图神经网络模型不变。这样，PROMPTCGL 自然地避免了灾难性遗忘。具体而言，我们提出了层次提示的方法，通过从特征和拓扑层面指令模型，以全面应对动态持续学习中任务图的变化性。此外，我们开发了一种个性化提示生成器，能够在生成针对每个图节点的特定提示的同时，尽量减少所需提示的数量，从而在图规模变化时保持恒定的内存消耗。在四个基准数据集上的广泛实验表明，PROMPTCGL 在优于现有 CGL 方法的同时，显著减少了内存消耗。我们的代码可在此处访问：这个 https URL。', 'title_zh': 'prompt驱动的持续图学习'}
{'arxiv_id': 'arXiv:2502.06324', 'title': "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation and Synthesis", 'authors': 'Zemin Yang, Yujing Sun, Xidong Peng, Siu Ming Yiu, Yuexin Ma', 'link': 'https://arxiv.org/abs/2502.06324', 'abstract': 'Image demoiréing poses one of the most formidable challenges in image restoration, primarily due to the unpredictable and anisotropic nature of moiré patterns. Limited by the quantity and diversity of training data, current methods tend to overfit to a single moiré domain, resulting in performance degradation for new domains and restricting their robustness in real-world applications. In this paper, we propose a universal image demoiréing solution, UniDemoiré, which has superior generalization capability. Notably, we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moiré images to train a universal demoiréing model. Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoiréing.', 'abstract_zh': '图像除花纱图案是图像恢复中最具挑战性的任务之一，主要由于花纱图案的不可预测性和各向异性。由于训练数据的数量和多样性有限，当前方法往往只能适应单一的花纱图案领域，导致在新领域中性能下降，并限制了其在实际应用中的鲁棒性。在本文中，我们提出了一种通用的图像除花纱图案解决方案——UniDemoiré，具有较强的泛化能力。我们还提出了一种创新且有效的方法来生成和合成数据，可以自动提供大量的高质量花纱图案图像来训练通用的除花纱图案模型。广泛的实验结果证明了我们方法在通用图像除花纱图案方面的前沿性能和广泛潜力。', 'title_zh': "UniDemoir\\'e：面向通用去moire效应图像处理的数据生成与合成方法探索"}
{'arxiv_id': 'arXiv:2502.06314', 'title': 'From Pixels to Components: Eigenvector Masking for Visual Representation Learning', 'authors': 'Alice Bizeul, Thomas Sutter, Alain Ryser, Bernhard Schölkopf, Julius von Kügelgen, Julia E. Vogt', 'link': 'https://arxiv.org/abs/2502.06314', 'abstract': 'Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning. However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks. We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels. Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance. The learning task then amounts to reconstructing the masked components from the visible ones. Compared to local patches of pixels, the principal components of images carry more global information. We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations. This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking. Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches.', 'abstract_zh': '从图像的可见部分预测被遮盖的部分是一种强大的自我监督方法，可用于视觉表示学习。然而，随机遮盖像素块的常见做法存在某些失效模式，这可能会阻止学习所需的有意义的高级特征，以满足下游任务的要求。我们提出了一种替代的遮掩策略，该策略作用于数据的适当变换，而不是原始像素。具体而言，我们执行主成分分析，然后随机遮掩数据的一部分成分，这些成分占数据方差的固定比例。学习任务则转化为从可见部分重构被遮掩的成分。与像素局部块相比，图像的主成分包含更多的全局信息。因此，我们认为从可见成分预测被遮掩的部分涉及更多高级特征的预测，使我们的遮掩策略能够提取更具有用的表示。我们的实证结果证实了这一点，演示了组件遮掩优于像素遮掩的图像分类性能提升。因此，我们的方法提供了一种简单且稳健的数据驱动替代方案，用于传统的图像遮掩建模方法。', 'title_zh': '从像素到组件：特征向量掩码在视觉表示学习中的应用'}
{'arxiv_id': 'arXiv:2502.06298', 'title': 'SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia', 'authors': 'Chaoqun Liu, Wenxuan Zhang, Jiahao Ying, Mahani Aljunied, Anh Tuan Luu, Lidong Bing', 'link': 'https://arxiv.org/abs/2502.06298', 'abstract': 'This study introduces two novel benchmarks, SeaExam and SeaBench, designed to evaluate the capabilities of Large Language Models (LLMs) in Southeast Asian (SEA) application scenarios. Unlike existing multilingual datasets primarily derived from English translations, these benchmarks are constructed based on real-world scenarios from SEA regions. SeaExam draws from regional educational exams to form a comprehensive dataset that encompasses subjects such as local history and literature. In contrast, SeaBench is crafted around multi-turn, open-ended tasks that reflect daily interactions within SEA communities. Our evaluations demonstrate that SeaExam and SeaBench more effectively discern LLM performance on SEA language tasks compared to their translated benchmarks. This highlights the importance of using real-world queries to assess the multilingual capabilities of LLMs.', 'abstract_zh': '本研究引入了两个新型基准——SeaExam和SeaBench，旨在评估大型语言模型（LLMs）在东南亚（SEA）应用场景中的能力。与现有的主要基于英语翻译的多语言数据集不同，这些基准是基于东南亚地区的实际应用场景构建的。SeaExam从区域内的教育考试中汲取素材，形成一个涵盖本地历史和文学等科目的全面数据集。相比之下，SeaBench则围绕多轮开放式任务构建，这些任务反映了东南亚社区中的日常互动。我们的评估表明，SeaExam和SeaBench比其翻译基准更能准确区分LLMs在SEA语言任务上的性能。这强调了在评估LLMs的多语言能力时使用实际查询的重要性。', 'title_zh': 'SeaExam 和 SeaBench：东南亚地区本地多语言问题下的大规模语言模型评估方法'}
{'arxiv_id': 'arXiv:2502.06289', 'title': 'Is an Ultra Large Natural Image-Based Foundation Model Superior to a Retina-Specific Model for Detecting Ocular and Systemic Diseases?', 'authors': 'Qingshan Hou, Yukun Zhou, Jocelyn Hui Lin Goh, Ke Zou, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Thaddaeus Lo, Xiaofeng Lei, Siegfried K. Wagner, Mark A. Chia, Dawei Yang, Hongyang Jiang, AnRan Ran, Rui Santos, Gabor Mark Somfai, Juan Helen Zhou, Haoyu Chen, Qingyu Chen, Carol Yim-Lui Cheung, Pearse A. Keane, Yih Chung Tham', 'link': 'https://arxiv.org/abs/2502.06289', 'abstract': 'The advent of foundation models (FMs) is transforming medical domain. In ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4 million natural images and 1.6 million retinal images, has demonstrated high adaptability across clinical applications. Conversely, DINOv2, a general-purpose vision FM pre-trained on 142 million natural images, has shown promise in non-medical domains. However, its applicability to clinical tasks remains underexplored. To address this, we conducted head-to-head evaluations by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular disease detection and systemic disease prediction tasks, across eight standardized open-source ocular datasets, as well as the Moorfields AlzEye and the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets, all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940, P<0.001). Conversely, RETFound achieved superior performance over all DINOv2 models in predicting heart failure, myocardial infarction, and ischaemic stroke (AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even with 10% of the fine-tuning data. These findings showcase the distinct scenarios where general-purpose and domain-specific FMs excel, highlighting the importance of aligning FM selection with task-specific requirements to optimise clinical performance.', 'abstract_zh': '基础模型（FMs）的出现正在改变医学领域。在眼科领域，RETFound，一种专门针对视网膜的FM，以其在140万自然图像和160万视网膜图像上的逐序列预训练，展示了其在临床应用中的高度适应性。相比之下，DINOv2，一种通用视觉FM，以其在1.42亿自然图像上的预训练，在非医学领域展示了潜力。然而，其在临床任务上的应用性尚未得到充分探索。为了解决这个问题，我们通过针对眼底疾病检测和系统性疾病预测任务，对RETFound以及三个DINOv2模型（大型、基础、小型）进行了头对头评估，横跨八个标准的开源眼底数据集，以及 Moorfields AlzEye 和英国生物银行数据集。DINOv2大型模型在检测糖尿病视网膜病变（AUROC=0.850-0.952 vs 0.823-0.944，所有 P <= 0.007）、多类眼底疾病（AUROC=0.892 vs 0.846，P < 0.001）方面表现优于RETFound。在青光眼方面，DINOv2基础模型的表现优于RETFound（AUROC=0.958 vs 0.940，P < 0.001）。然而，RETFound 在预测心力衰竭、心肌梗死和缺血性中风（AUROC=0.732-0.796 vs 0.663-0.771，所有 P < 0.001）方面优于所有DINOv2模型。即使在仅使用10%的调优数据的情况下，这些趋势仍然存在。这些发现展示了通用型和领域特定基础模型在不同场景下的优势，强调了根据具体任务需求选择基础模型的重要性，以优化临床性能。', 'title_zh': '基于超大规模自然图像的预训练模型是否优于专门针对视网膜的模型，用于检测眼病和全身疾病？'}
{'arxiv_id': 'arXiv:2502.06285', 'title': 'End-to-End Multi-Microphone Speaker Extraction Using Relative Transfer Functions', 'authors': 'Aviad Eisenberg, Sharon Gannot, Shlomo E. Chazan', 'link': 'https://arxiv.org/abs/2502.06285', 'abstract': 'This paper introduces a multi-microphone method for extracting a desired speaker from a mixture involving multiple speakers and directional noise in a reverberant environment. In this work, we propose leveraging the instantaneous relative transfer function (RTF), estimated from a reference utterance recorded in the same position as the desired source. The effectiveness of the RTF-based spatial cue is compared with direction of arrival (DOA)-based spatial cue and the conventional spectral embedding. Experimental results in challenging acoustic scenarios demonstrate that using spatial cues yields better performance than the spectral-based cue and that the instantaneous RTF outperforms the DOA-based spatial cue.', 'abstract_zh': '本文介绍了一种多麦克风方法，用于从包含多个说话人和方向性噪声的混响环境中提取所需说话人。在本研究中，我们提出利用参考发言记录在与所需源相同位置处估计的瞬时相对传输函数（RTF）。将基于RTF的空间线索效果与基于到达方向（DOA）的空间线索以及传统的光谱嵌入进行比较。在具有挑战性的声学场景中的实验结果表明，使用空间线索比基于光谱的线索具有更好的性能，而瞬时RTF比基于DOA的空间线索更具优势。', 'title_zh': '使用相对传输函数的端到端多麦克风演讲者提取'}
{'arxiv_id': 'arXiv:2502.06282', 'title': 'Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE', 'authors': 'Haiduo Huang, Fuwei Yang, Zhenhua Liu, Yixing Xu, Jinze Li, Yang Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum', 'link': 'https://arxiv.org/abs/2502.06282', 'abstract': 'Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at this https URL.', 'abstract_zh': '投机解码（Speculative Decoding, SD）通过使用较小的草稿模型预测多个令牌，从而加速大型语言模型的推理，随后这些令牌由较大的目标模型并行验证。然而，草稿模型的有限容量通常需要采用基于树的采样方法以提高预测准确性，这种方法在每个步骤中生成多个候选项。我们发现这种方法的一个关键限制：同一步骤的候选项源自相同的表示，这限制了多样性并降低了整体有效性。为解决这一问题，我们提出了一种基于Mixture of Experts（MoE）的Jakiro方法，让独立的专家生成多样化的预测，从而有效解耦候选项之间的相关性。此外，我们引入了一种混合推理策略，结合自回归解码用于初始令牌，平行解码用于后续阶段，并通过对比机制增强后者，以提高准确性。我们的方法显著提高了预测准确性并实现了更高的推理加速比。广泛的实验跨越了各种不同的模型验证了我们方法的有效性和鲁棒性，并确立了在投机解码方面的新最佳表现。我们的代码可从以下链接获取：[提供链接]。', 'title_zh': 'Jakiro：通过解耦多头MoE增强 speculative 解码'}
{'arxiv_id': 'arXiv:2502.06274', 'title': 'HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational Pharmacovigilance', 'authors': 'Zhaoying Wang, Yingdan Shi, Xiang Liu, Can Chen, Jun Wen, Ren Wang', 'link': 'https://arxiv.org/abs/2502.06274', 'abstract': "Drug-side effect research is vital for understanding adverse reactions arising in complex multi-drug therapies. However, the scarcity of higher-order datasets that capture the combinatorial effects of multiple drugs severely limits progress in this field. Existing resources such as TWOSIDES primarily focus on pairwise interactions. To fill this critical gap, we introduce HODDI, the first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S. Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) records spanning the past decade, to advance computational pharmacovigilance. HODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique side effects, specifically curated to capture multi-drug interactions and their collective impact on adverse effects. Comprehensive statistical analyses demonstrate HODDI's extensive coverage and robust analytical metrics, making it a valuable resource for studying higher-order drug relationships. Evaluating HODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP) can outperform graph models, while hypergraph models demonstrate superior performance in capturing complex multi-drug interactions, further validating HODDI's effectiveness. Our findings highlight the inherent value of higher-order information in drug-side effect prediction and position HODDI as a benchmark dataset for advancing research in pharmacovigilance, drug safety, and personalized medicine. The dataset and codes are available at this https URL.", 'abstract_zh': '药物副作用研究对于理解复杂多药治疗中的不良反应至关重要。然而，缺乏能够捕捉多种药物组合效应的高级数据集严重限制了该领域的发展。现有的资源如TWOSIDES主要集中在双药相互作用上。为填补这一关键空白，我们介绍了HODDI，这是首个高级药物相互作用数据集，从过去十年美国食品药品监督管理局（FDA）不良事件报告系统（FAERS）记录中构建，以促进计算药学监测。HODDI包含109,744条记录，涉及2,506种独特的药物和4,569种独特的副作用，特别是为了捕捉多药相互作用及其对不良反应的整体影响而特别精选。全面的统计分析表明，HODDI具有广泛的覆盖范围和稳健的分析指标，使其成为研究高级药物关系的宝贵资源。利用多种模型评估HODDI，我们发现简单的多层感知机（MLP）可能优于图模型，而高维图模型在捕捉复杂多药相互作用方面表现出更优的效果，进一步验证了HODDI的有效性。我们的发现强调了高级信息在药物副作用预测中的固有价值，并将HODDI定位为推动药学监测、药物安全和个人化医学研究的基准数据集。数据集和代码可通过以下链接获取：[此链接]。', 'title_zh': 'HODDI：用于计算药监的高阶药物-药物相互作用数据集'}
{'arxiv_id': 'arXiv:2502.06257', 'title': 'K-ON: Stacking Knowledge On the Head Layer of Large Language Model', 'authors': 'Lingbing Guo, Yichi Zhang, Zhongpu Bo, Zhuo Chen, Mengshu Sun, Zhiqiang Zhang, Wen Zhang, Huajun Chen', 'link': 'https://arxiv.org/abs/2502.06257', 'abstract': 'Recent advancements in large language models (LLMs) have significantly improved various natural language processing (NLP) tasks. Typically, LLMs are trained to predict the next token, aligning well with many NLP tasks. However, in knowledge graph (KG) scenarios, entities are the fundamental units and identifying an entity requires at least several tokens. This leads to a granularity mismatch between KGs and natural languages. To address this issue, we propose K-ON, which integrates KG knowledge into the LLM by employing multiple head layers for next k-step prediction. K-ON can not only generate entity-level results in one step, but also enables contrastive loss against entities, which is the most powerful tool in KG representation learning. Experimental results show that K-ON outperforms state-of-the-art methods that incorporate text and even the other modalities.', 'abstract_zh': '近年来，大规模语言模型（LLMs）在多种自然语言处理（NLP）任务上的表现显著提升。通常，LLMs 被训练为预测下一个标记，这与许多 NLP 任务相匹配。然而，在知识图谱（KG）场景中，实体是基本单位，识别一个实体通常需要几个标记。这导致了 KG 和自然语言之间的粒度不匹配。为了解决这一问题，我们提出了 K-ON，通过采用多头层进行 k 步预测，将 KG 知识集成到 LLM 中。K-ON 不仅能够在一步中生成实体级别的结果，还能通过实体之间的对比损失来进行学习，这是一种在 KG 表征学习中最强大的工具。实验结果表明，K-ON 在结合文本甚至其他模态的方法中表现更优。', 'title_zh': 'K-ON：在大型语言模型的头层堆叠知识'}
{'arxiv_id': 'arXiv:2502.06255', 'title': 'Towards Efficient and Intelligent Laser Weeding: Method and Dataset for Weed Stem Detection', 'authors': 'Dingning Liu, Jinzhe Li, Haoyang Su, Bei Cui, Zhihui Wang, Qingbo Yuan, Wanli Ouyang, Nanqing Dong', 'link': 'https://arxiv.org/abs/2502.06255', 'abstract': 'Weed control is a critical challenge in modern agriculture, as weeds compete with crops for essential nutrient resources, significantly reducing crop yield and quality. Traditional weed control methods, including chemical and mechanical approaches, have real-life limitations such as associated environmental impact and efficiency. An emerging yet effective approach is laser weeding, which uses a laser beam as the stem cutter. Although there have been studies that use deep learning in weed recognition, its application in intelligent laser weeding still requires a comprehensive understanding. Thus, this study represents the first empirical investigation of weed recognition for laser weeding. To increase the efficiency of laser beam cut and avoid damaging the crops of interest, the laser beam shall be directly aimed at the weed root. Yet, weed stem detection remains an under-explored problem. We integrate the detection of crop and weed with the localization of weed stem into one end-to-end system. To train and validate the proposed system in a real-life scenario, we curate and construct a high-quality weed stem detection dataset with human annotations. The dataset consists of 7,161 high-resolution pictures collected in the field with annotations of 11,151 instances of weed. Experimental results show that the proposed system improves weeding accuracy by 6.7% and reduces energy cost by 32.3% compared to existing weed recognition systems.', 'abstract_zh': '现代农业中的除草是一项关键挑战，因为杂草与农作物争夺必需的养分资源，显著降低了作物产量和品质。传统的除草方法，包括化学和机械手段，存在实际的局限性，如对环境的影响和效率问题。新兴且有效的除草方法是激光除草，它使用激光束作为茎切割工具。尽管已有研究利用深度学习进行杂草识别，但在智能激光除草中的应用仍需要全面的理解。因此，本研究代表了首次对激光除草中的杂草识别进行实证调查。为了提高激光束切割效率并避免损伤目标作物，激光束应直接对准杂草根部。然而，杂草茎的检测仍是一个未充分探索的问题。我们通过将作物和杂草的检测与杂草茎的定位整合到一个端到端系统中来解决这一问题。为了在真实场景中训练和验证所提出的系统，我们通过人工注释创建并构建了一个高质量的杂草茎检测数据集。该数据集包含7,161张高分辨率的田间图片，标注了11,151个杂草实例。实验结果表明，与现有的杂草识别系统相比，所提出系统能将除草准确性提高6.7%，并减少32.3%的能源成本。', 'title_zh': '高效智能激光除草方法与杂草茎检测数据集研究'}
{'arxiv_id': 'arXiv:2502.06249', 'title': 'Conditioning through indifference in quantum mechanics', 'authors': 'Keano De Vos, Gert de Cooman', 'link': 'https://arxiv.org/abs/2502.06249', 'abstract': "We can learn (more) about the state a quantum system is in through measurements. We look at how to describe the uncertainty about a quantum system's state conditional on executing such measurements. We show that by exploiting the interplay between desirability, coherence and indifference, a general rule for conditioning can be derived. We then apply this rule to conditioning on measurement outcomes, and show how it generalises to conditioning on a set of measurement outcomes.", 'abstract_zh': '通过测量，我们可以更深入地了解量子系统所处的状态。我们探讨如何在执行此类测量的前提下描述对量子系统状态的不确定度。我们展示了一种通过利用偏好、相干性和无偏性之间的相互作用，可以推导出一种一般性的条件化规则。随后，我们将这一规则应用于测量结果的条件化，并展示了其如何推广到一组测量结果的条件化。', 'title_zh': '在量子力学中通过无偏性进行条件概率赋值'}
{'arxiv_id': 'arXiv:2502.06233', 'title': 'Confidence Improves Self-Consistency in LLMs', 'authors': 'Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, Gal Yona', 'link': 'https://arxiv.org/abs/2502.06233', 'abstract': "Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.", 'abstract_zh': '自我一致性解码通过采样多种推理路径并选择最频繁的答案来提高大型语言模型在推理任务中的表现。然而，这种方法计算成本较高，因为需要采样许多（较长的）路径以增加正确答案成为最频繁答案的可能性。为了解决这一问题，我们引入了基于置信度的自我一致性（CISC）方法。CISC 根据模型直接获得的置信度分数进行加权多数投票。通过优先处理高置信度路径，它可以使用较小的样本量来识别正确的答案。在九个模型和四个数据集上的测试中，CISC 在几乎所有配置中均优于自我一致性方法，平均将所需的推理路径数量减少了超过40%。此外，在展示了标准评估方法对于区分同一问题的正确答案和错误答案预测能力较差后，我们引入了问题内部置信度评估的概念。事实上，最校准的置信度方法证明对CISC 的效果最差。最后，除了这些实际意义外，我们的结果和分析表明，大型语言模型能够有效判断自己输出的正确性，这有助于该领域的持续辩论。', 'title_zh': '信心提高大模型的自我一致性'}
{'arxiv_id': 'arXiv:2502.06217', 'title': 'Examining False Positives under Inference Scaling for Mathematical Reasoning', 'authors': 'Yu Wang, Nan Yang, Liang Wang, Furu Wei', 'link': 'https://arxiv.org/abs/2502.06217', 'abstract': 'Recent advancements in language models have led to significant improvements in mathematical reasoning across various benchmarks. However, most of these benchmarks rely on automatic evaluation methods that only compare final answers using heuristics, without verifying the underlying reasoning steps. This limitation results in false positive solutions, where models may produce correct final answers but with flawed deduction paths. In this paper, we systematically examine the prevalence of false positive solutions in mathematical problem solving for language models. We analyze the characteristics and extent of this issue across different open-source models, datasets of varying difficulty levels, and decoding strategies. Specifically, we explore how false positives influence the inference time scaling behavior of language models. Our experimental results reveal that: (1) false positive solutions persist across different models, datasets, and decoding methods, (2) sampling-based inference time scaling methods do not alleviate the problem, and (3) the pass@N evaluation metric is more susceptible to false positives, suggesting a significantly lower scaling ceiling than what automatic evaluations indicate. Additionally, we analyze specific instances of false positives and discuss potential limitations in self-improvement techniques and synthetic data generation under such conditions.', 'abstract_zh': '近年来，语言模型的进步在各种基准测试中的数学推理方面取得了显著改进。然而，这些基准测试大多依赖于自动评估方法，仅通过启发式方法比较最终答案，而不验证背后的推理步骤。这一限制导致了虚假正解的问题，即模型可能会产生正确的最终答案，但推理路径存在缺陷。本文系统地研究了语言模型在数学问题解决中虚假正解的普遍性。我们分析了这一问题在不同开源模型、不同难度的数据集以及不同解码策略中的特点和程度。具体而言，我们探索了虚假正解如何影响语言模型的推理时间缩放行为。我们实验结果显示：（1）虚假正解在不同模型、数据集和解码方法中普遍存在；（2）基于采样的推理时间缩放方法并未解决问题；（3）pass@N评估指标更易受到虚假正解的影响，表明其缩放上限低于自动评估所显示的。此外，我们还分析了虚假正解的具体实例，并讨论了在这种条件下自我改进技术和合成数据生成的潜在局限性。', 'title_zh': '考查推理缩放对数学推理中假阳性结果的影响'}
{'arxiv_id': 'arXiv:2502.06215', 'title': 'LessLeak-Bench: A First Investigation of Data Leakage in LLMs Across 83 Software Engineering Benchmarks', 'authors': 'Xin Zhou, Martin Weyssow, Ratnadira Widyasari, Ting Zhang, Junda He, Yunbo Lyu, Jianming Chang, Beiqi Zhang, Dan Huang, David Lo', 'link': 'https://arxiv.org/abs/2502.06215', 'abstract': "Large Language Models (LLMs) are widely utilized in software engineering (SE) tasks, such as code generation and automated program repair. However, their reliance on extensive and often undisclosed pre-training datasets raises significant concerns about data leakage, where the evaluation benchmark data is unintentionally ``seen'' by LLMs during the model's construction phase. The data leakage issue could largely undermine the validity of LLM-based research and evaluations. Despite the increasing use of LLMs in the SE community, there is no comprehensive study that assesses the extent of data leakage in SE benchmarks for LLMs yet. To address this gap, this paper presents the first large-scale analysis of data leakage in 83 SE benchmarks concerning LLMs. Our results show that in general, data leakage in SE benchmarks is minimal, with average leakage ratios of only 4.8\\%, 2.8\\%, and 0.7\\% for Python, Java, and C/C++ benchmarks, respectively. However, some benchmarks exhibit relatively higher leakage ratios, which raises concerns about their bias in evaluation. For instance, QuixBugs and BigCloneBench have leakage ratios of 100.0\\% and 55.7\\%, respectively. Furthermore, we observe that data leakage has a substantial impact on LLM evaluation. We also identify key causes of high data leakage, such as the direct inclusion of benchmark data in pre-training datasets and the use of coding platforms like LeetCode for benchmark construction. To address the data leakage, we introduce \\textbf{LessLeak-Bench}, a new benchmark that removes leaked samples from the 83 SE benchmarks, enabling more reliable LLM evaluations in future research. Our study enhances the understanding of data leakage in SE benchmarks and provides valuable insights for future research involving LLMs in SE.", 'abstract_zh': '大型语言模型（Large Language Models, LLMs）在软件工程（Software Engineering, SE）任务中得到了广泛应用，如代码生成和自动程序修复。然而，它们依赖于大量且经常未公开的预训练数据集，这引发了关于数据泄露的重大担忧，即评估基准数据在模型构建阶段意外地被LLMs“看到”。数据泄露问题可能会严重削弱基于LLM的研究和评估的有效性。尽管LLMs在SE社区中的应用越来越广泛，但目前还没有全面研究评估LLMs在SE基准中的数据泄露程度。为填补这一空白，本文首次对83个涉及LLMs的SE基准数据泄露进行了大规模分析。我们的结果表明，总体而言，SE基准中的数据泄露程度较低，Python、Java和C/C++基准的平均泄露率为4.8%、2.8%和0.7%。然而，某些基准的泄露率相对较高，这对其评估的公正性提出了质疑。例如，QuixBugs和BigCloneBench的泄露率为100.0%和55.7%。此外，我们发现数据泄露对LLM评估有着重要影响，并确定了高数据泄露的几个主要原因，如基准数据直接包含在预训练数据集中以及使用像LeetCode这样的编程平台进行基准构建。为了应对数据泄露问题，我们提出了一个新的基准LessLeak-Bench，该基准从原有的83个SE基准中移除了泄露样本，从而为未来涉及LLM的SE研究提供了更可靠的评估机制。我们的研究加深了对SE基准中数据泄露的理解，并提供了对未来涉及LLM的SE研究具有重要价值的见解。', 'title_zh': 'LessLeak-Bench：在83个软件工程基准测试中的大型语言模型数据泄露初步研究'}
{'arxiv_id': 'arXiv:2502.06207', 'title': 'Unveiling the Capabilities of Large Language Models in Detecting Offensive Language with Annotation Disagreement', 'authors': 'Junyu Lu, Kai Ma, Kaichun Wang, Kelaiti Xiao, Roy Ka-Wei Lee, Bo Xu, Liang Yang, Hongfei Lin', 'link': 'https://arxiv.org/abs/2502.06207', 'abstract': 'LLMs are widely used for offensive language detection due to their advanced capability. However, the challenges posed by human annotation disagreement in real-world datasets remain underexplored. These disagreement samples are difficult to detect due to their ambiguous nature. Additionally, the confidence of LLMs in processing disagreement samples can provide valuable insights into their alignment with human annotators. To address this gap, we systematically evaluate the ability of LLMs to detect offensive language with annotation disagreement. We compare the binary accuracy of multiple LLMs across varying annotation agreement levels and analyze the relationship between LLM confidence and annotation agreement. Furthermore, we investigate the impact of disagreement samples on LLM decision-making during few-shot learning and instruction fine-tuning. Our findings highlight the challenges posed by disagreement samples and offer guidance for improving LLM-based offensive language detection.', 'abstract_zh': '大规模语言模型（LLMs）广泛用于 Offensive 语言检测，得益于它们先进的能力。然而，现实中数据集中的人工注释不一致所带来的挑战仍然被低估。这些不一致样本由于其模糊性难以被检测到。此外，LLMs 在处理不一致样本时的信心水平提供了其与人类注释者对齐程度的宝贵见解。为了弥补这一空白，我们系统地评估了 LLM 在存在注释不一致情况下的 Offensive 语言检测能力。我们比较了多种 LLM 在不同注释一致水平下的二分类准确率，并分析了 LLM 的信心水平与注释一致水平之间的关系。此外，我们研究了不一致样本对 LLM 在少量样本学习和指令微调过程中的决策影响。我们的研究结果突显了不一致样本所带来的挑战，并为改进基于 LLM 的 Offensive 语言检测提供了指导。', 'title_zh': '揭开大型语言模型在标注分歧情况下检测冒犯语言能力的面纱'}
{'arxiv_id': 'arXiv:2502.06205', 'title': 'C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation', 'authors': 'Guoxin Chen, Minpeng Liao, Peiying Yu, Dingmin Wang, Zile Qiao, Chao Yang, Xin Zhao, Kai Fan', 'link': 'https://arxiv.org/abs/2502.06205', 'abstract': 'Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.', 'abstract_zh': '检索增强生成（RAG）系统面临一个基本挑战，即独立开发的检索器和大型语言模型（LLMs）之间的对齐问题。现有方法通常涉及修改其中一个组件或引入简单的中间模块，从而导致实际应用中的局限性和次优性能。受人类搜索行为的启发——通常涉及提出搜索查询和审阅文档的往返过程，我们提出了一种以代理为中心的C-3PO框架，通过轻量级的多代理系统促进检索器和LLMs之间的通信。该框架实施了三个专门的代理，共同优化整个RAG管道，而不修改检索器和LLMs。这些代理协同工作，评估检索的需求，生成有效的查询，并选择适合LLMs的信息。为了实现有效的多代理协调，我们开发了一种基于树结构滚动策略的强化学习奖励信用分配方法。在领域内和领域外场景下的广泛实验表明，C-3PO显著提高了RAG性能，同时保持了插拔灵活和优越的泛化能力。', 'title_zh': 'C-3PO：紧凑型即插即用代理优化以实现类人类检索增强生成'}
{'arxiv_id': 'arXiv:2502.06193', 'title': 'Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering', 'authors': 'Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, Xin Xia', 'link': 'https://arxiv.org/abs/2502.06193', 'abstract': 'Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored. In this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide...', 'abstract_zh': '近年来，大规模语言模型（LLMs）被部署到代码生成等多种软件工程（SE）任务中，显著推进了SE任务的自动化。然而，评估这些LLM生成的代码和文本的质量依然具有挑战性。常用的Pass@k指标要求进行大量的单元测试和配置环境，需要较高的劳动成本，并且不适合评估LLM生成的文本。传统的度量标准，如BLEU，仅仅衡量词级相似性而非语义相似性，也受到了质疑。为了解决这一问题，涌现出了一种新的趋势，即利用LLM进行自动化评估，称为LLM-as-a-judge。这些LLM-as-a-judge方法声称比传统的度量标准更接近人类评估，无需依赖高质量的参考答案。然而，这些方法在SE任务中与人类判断的具体对齐情况尚未得到探索。在本文中，我们通过实证研究LLM-as-a-judge方法，重点关注其与人类判断的对齐情况。我们选择了七种利用通用LLM的LLM-as-a-judge方法，以及两种专门针对评估进行微调的LLM。在对三个最新的代码翻译、代码生成和代码总结数据集进行LLM响应生成和手动评分后，我们进一步要求这些方法对每个响应进行评估。最后，我们将这些方法生成的分数与人工评估进行比较。结果显示，在代码翻译和生成任务中，基于输出的方法分别达到了与人类评分的皮尔森相关系数81.32和68.51，实现了接近人类的评估，并且明显优于ChrF++，这是表现最好的传统度量标准之一，其相关系数分别为34.23和64.92。基于输出的方法促使LLM直接输出判断，并展现出更符合人类评分模式的分数分布。最后，我们提供...', 'title_zh': 'LLM能否替代人类评价者？一项关于软件工程中LLM作为仲裁者的实证研究'}
{'arxiv_id': 'arXiv:2502.06192', 'title': 'Right Time to Learn:Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation', 'authors': 'Guanglong Sun, Hongwei Yan, Liyuan Wang, Qian Li, Bo Lei, Yi Zhong', 'link': 'https://arxiv.org/abs/2502.06192', 'abstract': "Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). Although it was originally proposed to train a more compact ``student'' model from a large ``teacher'' model, many recent efforts have focused on adapting it to promote generalization of the model itself, such as online KD and self KD. % as an effective way Here, we propose an accessible and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named \\emph{spacing effect} in biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. With both theoretical and empirical analyses, we demonstrate that the benefits of the proposed Spaced KD stem from convergence to a flatter loss landscape during stochastic gradient descent (SGD). We perform extensive experiments to validate the effectiveness of Spaced KD in improving the learning performance of DNNs (e.g., the performance gain is up to 2.31\\% and 3.34\\% on Tiny-ImageNet over online KD and self KD, respectively).", 'abstract_zh': '知识蒸馏（KD）是一种训练深度神经网络（DNNs）的有效策略。尽管最初旨在通过一个大型的“教师”模型训练一个更紧凑的“学生”模型，但许多近期的努力已经将其应用于促进模型本身的泛化，例如在线知识蒸馏和自我知识蒸馏。在此，我们提出了一种名为间隔知识蒸馏（Spaced KD）的易于实施且兼容的策略，以提高在线知识蒸馏和自我知识蒸馏的有效性。在这种策略中，学生模型从在其训练时间间隔提前训练的教师模型中提取知识。该策略受到生物学学习与记忆中的一个重要理论——间隔效应（spacing effect）的启发，该理论认为学习试验之间适当的间隔可以显著提高学习性能。\n\n通过理论分析和实证研究，我们展示了所提出的间隔知识蒸馏的好处来源于随机梯度下降（SGD）过程中损失景观的收敛趋于平坦。通过广泛的实验，我们验证了间隔知识蒸馏在提高DNNs的训练性能方面的有效性（例如，在Tiny-ImageNet数据集上，与在线知识蒸馏和自我知识蒸馏相比，性能提升分别高达2.31%和3.34%）。', 'title_zh': '适当时机学习：通过生物启发的间隔效应促进知识精简中的泛化能力'}
{'arxiv_id': 'arXiv:2502.06185', 'title': 'Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization', 'authors': 'Yang Zhong, Diane Litman', 'link': 'https://arxiv.org/abs/2502.06185', 'abstract': 'Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better aggregate sentence-level scores predicted by natural language inference models. Our approach shows improved performance on top of different model baselines over several evaluation benchmarks, covering rich domains of texts, focusing on long document summarization. This underscores the significance of incorporating discourse features in developing models for scoring summaries for long document factual inconsistency.', 'abstract_zh': '对于长文档摘要而言，检测事实不一致仍然具有挑战性，因为源文章结构复杂且摘要长度较长。在本文中，我们探讨了事实不一致错误，并将其与论述分析联系起来。我们发现错误在复杂句子中更为常见，并与几种论述特征相关。我们提出了一种框架，将长文本分解为基于论述的片段，并利用论述信息来更好地聚合自然语言推理模型在句层面预测的分数。我们的方法在多种评估基准上改进了不同模型基线的表现，涵盖了丰富的文本领域，专注于长文档摘要。这强调了在开发用于评分长文档事实不一致摘要的模型时整合论述特征的重要性。', 'title_zh': '基于话语驱动的评估：揭示长文档摘要中的事实不一致'}
{'arxiv_id': 'arXiv:2502.06180', 'title': 'RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset', 'authors': 'Naome A. Etori, Maria L. Gini', 'link': 'https://arxiv.org/abs/2502.06180', 'abstract': 'Social media has become a crucial open-access platform for individuals to express opinions and share experiences. However, leveraging low-resource language data from Twitter is challenging due to scarce, poor-quality content and the major variations in language use, such as slang and code-switching. Identifying tweets in these languages can be difficult as Twitter primarily supports high-resource languages. We analyze Kenyan code-switched data and evaluate four state-of-the-art (SOTA) transformer-based pretrained models for sentiment and emotion classification, using supervised and semi-supervised methods. We detail the methodology behind data collection and annotation, and the challenges encountered during the data curation phase. Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\\%) and F1 score (66.1\\%), XLM-R semi-supervised (67.2\\% accuracy, 64.1\\% F1 score). In emotion analysis, DistilBERT supervised leads in accuracy (59.8\\%) and F1 score (31\\%), mBERT semi-supervised (accuracy (59\\% and F1 score 26.5\\%). AfriBERTa models show the lowest accuracy and F1 scores. All models tend to predict neutral sentiment, with Afri-BERT showing the highest bias and unique sensitivity to empathy emotion. this https URL', 'abstract_zh': '社交媒体已成为个人表达意见和分享经验的关键开放访问平台。然而，由于推特上的低资源语言数据稀缺且质量较差，且语言使用中存在大量的变体，如俚语和代码转换，因此利用这些数据具有挑战性。在这些语言中识别推文也颇具难度，因为推特主要支持高资源语言。我们分析了肯尼亚代码转换的数据，并使用监督和半监督方法评估了四种最先进的（SOTA）基于变压器的预训练模型在情感和情绪分类任务中的表现。我们详细说明了数据收集和标注的方法，并概述了在数据整理阶段遇到的挑战。结果显示，XLM-R 表现最佳；对于情感分析，XLM-R 监督模型的准确率最高，达到 69.2%，F1 分数为 66.1%；XLM-R 半监督模型的准确率为 67.2%，F1 分数为 64.1%。在情绪分析方面，DistilBERT 监督模型在准确率（59.8%）和 F1 分数（31%）上领先，而 mBERT 半监督模型的准确率为 59%，F1 分数为 26.5%。AfriBERTa 模型的准确率和 F1 分数最低。所有模型都倾向于预测中性情感，其中 AfriBERTa 表现出最高的偏见和对同理心情绪的特殊敏感性。\n\n[原文链接] this https URL', 'title_zh': 'RideKE：利用低资源用户生成的推文内容进行肯尼亚双语-switched 数据集中的情感和情绪检测'}
{'arxiv_id': 'arXiv:2502.06173', 'title': 'Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis', 'authors': 'Sanket Jantre, Tianle Wang, Gilchan Park, Kriti Chopra, Nicholas Jeon, Xiaoning Qian, Nathan M. Urban, Byung-Jun Yoon', 'link': 'https://arxiv.org/abs/2502.06173', 'abstract': 'Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.', 'abstract_zh': '识别蛋白质-蛋白质相互作用（PPIs）有助于从细胞机制层面理解复杂条件下的生物学过程，特别是在神经退行性疾病、代谢综合征和癌症等疾病背景下。大规模语言模型（LLMs）通过自动挖掘大量的生物医学文献，在预测蛋白质结构和相互作用方面展现了显著的潜力；然而，它们固有的不确定性仍然是获得可重复研究结果的一个关键挑战，这对于生物医学应用至关重要。在本研究中，我们提出了一种针对PPI分析的不确定性感知的大规模语言模型适应方法，利用了微调后的LLaMA-3和BioMedGPT模型。为了提高预测的可靠性，我们整合了LoRA集成和贝叶斯LoRA模型来进行不确定性量化（UQ），确保对蛋白质行为提供可信度校准的见解。我们的方法在多种疾病背景下实现了PPI识别的竞争力，同时解决了模型的不确定性问题，从而增强了计算生物学中的可信度和可重复性。这些研究结果强调了不确定性感知的大规模语言模型适应方法在推动精准医学和生物医学研究方面的潜力。', 'title_zh': '面向蛋白质-蛋白质相互作用分析的具有不确定性感知的大语言模型适应性调整'}
{'arxiv_id': 'arXiv:2502.06170', 'title': 'An Interpretable Implicit-Based Approach for Modeling Local Spatial Effects: A Case Study of Global Gross Primary Productivity', 'authors': 'Siqi Du, Hongsheng Huang, Kaixin Shen, Ziqi Liu, Shengjun Tang', 'link': 'https://arxiv.org/abs/2502.06170', 'abstract': 'In Earth sciences, unobserved factors exhibit non-stationary spatial distributions, causing the relationships between features and targets to display spatial heterogeneity. In geographic machine learning tasks, conventional statistical learning methods often struggle to capture spatial heterogeneity, leading to unsatisfactory prediction accuracy and unreliable interpretability. While approaches like Geographically Weighted Regression (GWR) capture local variations, they fall short of uncovering global patterns and tracking the continuous evolution of spatial heterogeneity. Motivated by this limitation, we propose a novel perspective - that is, simultaneously modeling common features across different locations alongside spatial differences using deep neural networks. The proposed method is a dual-branch neural network with an encoder-decoder structure. In the encoding stage, the method aggregates node information in a spatiotemporal conditional graph using GCN and LSTM, encoding location-specific spatiotemporal heterogeneity as an implicit conditional vector. Additionally, a self-attention-based encoder is used to extract location-invariant common features from the data. In the decoding stage, the approach employs a conditional generation strategy that predicts response variables and interpretative weights based on data features under spatiotemporal conditions. The approach is validated by predicting vegetation gross primary productivity (GPP) using global climate and land cover data from 2001 to 2020. Trained on 50 million samples and tested on 2.8 million, the proposed model achieves an RMSE of 0.836, outperforming LightGBM (1.063) and TabNet (0.944). Visualization analyses indicate that our method can reveal the distribution differences of the dominant factors of GPP across various times and locations.', 'abstract_zh': '在地球科学领域，未观测因素表现出非平稳的空间分布，导致特征和目标之间的关系显示出空间异质性。在地理机器学习任务中，传统的统计学习方法往往难以捕捉空间异质性，从而导致预测精度不足和解释性不可靠。尽管地理加权回归（GWR）等方法能够捕捉局部变化，但它们难以揭示全局模式并追踪空间异质性的连续演变。鉴于这一局限，我们提出了一种新的视角：即使用深度神经网络同时建模不同位置上的共同特征以及空间差异。所提出的方法是一种具有编码-解码结构的双分支神经网络。在编码阶段，方法利用图卷积网络（GCN）和长短期记忆网络（LSTM）聚合时空条件下的节点信息，并将位置特定的时空异质性编码为隐式条件向量。此外，使用基于自我注意力的编码器从数据中提取位置不变的共同特征。在解码阶段，该方法采用一种条件生成策略，根据时空条件下的数据特征预测响应变量和解释权重。该方法通过使用从2001年到2020年的全球气候和土地覆盖数据预测植被光合作用总量（GPP）进行验证。所提出的模型基于5000万个样本进行训练，并在280万个样本上进行测试，实现了均方根误差（RMSE）为0.836，优于LightGBM（1.063）和TabNet（0.944）。可视化分析表明，我们的方法可以揭示GPP主导因素在不同时间和地点的分布差异。', 'title_zh': '基于隐式方法的可解释局部空间效应建模方法：全球初级生产力案例研究'}
{'arxiv_id': 'arXiv:2502.06167', 'title': 'Universal Approximation of Visual Autoregressive Transformers', 'authors': 'Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song', 'link': 'https://arxiv.org/abs/2502.06167', 'abstract': "We investigate the fundamental limits of transformer-based foundation models, extending our analysis to include Visual Autoregressive (VAR) transformers. VAR represents a big step toward generating images using a novel, scalable, coarse-to-fine ``next-scale prediction'' framework. These models set a new quality bar, outperforming all previous methods, including Diffusion Transformers, while having state-of-the-art performance for image synthesis tasks. Our primary contributions establish that, for single-head VAR transformers with a single self-attention layer and single interpolation layer, the VAR Transformer is universal. From the statistical perspective, we prove that such simple VAR transformers are universal approximators for any image-to-image Lipschitz functions. Furthermore, we demonstrate that flow-based autoregressive transformers inherit similar approximation capabilities. Our results provide important design principles for effective and computationally efficient VAR Transformer strategies that can be used to extend their utility to more sophisticated VAR models in image generation and other related areas.", 'abstract_zh': '我们探讨了基于Transformer的基础模型的基本局限性，并将分析扩展到包括视觉自回归（VAR）Transformer。VAR代表了使用新颖且可扩展的从粗到细“下一级预测”框架生成图像的一大步。这些模型设定了新的质量标准，超过了所有先前的方法，包括扩散Transformer，同时在图像合成任务中具有最先进的性能。我们的主要贡献表明，对于具有单个自注意力层和单个插值层的单一头VAR Transformer，VAR Transformer是万能的。从统计角度来看，我们证明了这种简单的VAR Transformer是任意图像到图像Lipschitz函数的统一逼近器。此外，我们展示了基于流的自回归Transformer具有相似的逼近能力。我们的结果为有效的、计算效率高的VAR Transformer设计原则提供了重要的指导，这些原则可以用于扩展其在图像生成及其他相关领域的应用。', 'title_zh': '视觉自回归变压器的普遍逼近性'}
{'arxiv_id': 'arXiv:2502.06153', 'title': 'Low Tensor-Rank Adaptation of Kolmogorov--Arnold Networks', 'authors': 'Yihang Gao, Michael K. Ng, Vincent Y.F. Tan', 'link': 'https://arxiv.org/abs/2502.06153', 'abstract': 'Kolmogorov--Arnold networks (KANs) have demonstrated their potential as an alternative to multi-layer perceptions (MLPs) in various domains, especially for science-related tasks. However, transfer learning of KANs remains a relatively unexplored area. In this paper, inspired by Tucker decomposition of tensors and evidence on the low tensor-rank structure in KAN parameter updates, we develop low tensor-rank adaptation (LoTRA) for fine-tuning KANs. We study the expressiveness of LoTRA based on Tucker decomposition approximations. Furthermore, we provide a theoretical analysis to select the learning rates for each LoTRA component to enable efficient training. Our analysis also shows that using identical learning rates across all components leads to inefficient training, highlighting the need for an adaptive learning rate strategy. Beyond theoretical insights, we explore the application of LoTRA for efficiently solving various partial differential equations (PDEs) by fine-tuning KANs. Additionally, we propose Slim KANs that incorporate the inherent low-tensor-rank properties of KAN parameter tensors to reduce model size while maintaining superior performance. Experimental results validate the efficacy of the proposed learning rate selection strategy and demonstrate the effectiveness of LoTRA for transfer learning of KANs in solving PDEs. Further evaluations on Slim KANs for function representation and image classification tasks highlight the expressiveness of LoTRA and the potential for parameter reduction through low tensor-rank decomposition.', 'abstract_zh': '柯unfold-阿诺尔德网络（KANs）在多个领域中已被证明是多层感知机（MLPs）的有潜力的替代方案，特别是在与科学研究相关的任务中。然而，KANs的迁移学习仍然是一个相对未被探索的领域。在本文中，我们受到张量的Tucker分解以及KAN参数更新中低张量秩结构证据的启发，开发了低张量秩适应（LoTRA）方法以微调KANs。我们基于Tucker分解近似研究了LoTRA的表达能力，并提供了理论分析来选择每个LoTRA组件的学习率，以便实现高效的训练。我们的分析还表明，在所有组件上使用相同的 learning rates 会导致训练效率低下，强调了需要采用自适应学习率策略的必要性。除了理论洞察外，我们还探索了LoTRA在通过微调KANs高效解决各种偏微分方程（PDEs）中的应用。此外，我们提出了Slim KANs，通过对KAN参数张量的固有低张量秩性质进行整合，以减小模型大小同时保持优越的性能。实验结果验证了所提学习率选择策略的有效性，并证明了LoTRA在KANs的PDEs迁移学习中的有效性。进一步的评估显示了LoTRA在函数表示和图像分类任务中的表达能力及其通过低张量秩分解实现参数缩减的潜力。', 'title_zh': '低张量秩适配柯尔莫哥洛夫-阿诺尔德网络'}
{'arxiv_id': 'arXiv:2502.06151', 'title': 'Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting', 'authors': 'Kareem Hegazy, Michael W. Mahoney, N. Benjamin Erichson', 'link': 'https://arxiv.org/abs/2502.06151', 'abstract': "Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.", 'abstract_zh': '变换器（Transformers）近年来在时间序列预测任务中展现了强大的性能，但它们的全连接注意力机制忽视了数据在时间维度上的因果性和往往在时间维度上的局部性。我们提出了一种名为Powerformer的新颖变换器变体，用因果权重替换非因果权重，并根据平滑的重尾衰减重新加权。这一简单而有效的修改赋予模型偏向时间局部依赖的归纳偏置，同时仍允许足够的灵活性以学习每个数据集的独特相关结构。我们的实验证明，Powerformer不仅在公开的时间序列基准数据集上达到了最先进的准确率，而且还提高了注意力模式的可解释性。我们的分析表明，模型的时间局部性偏置在训练过程中得到了放大，这展示了时间序列数据与基于幂律的注意力之间的相互作用。这些发现强调了为时间序列预测对变换器架构进行领域特定修改的重要性，并确立了Powerformer作为未来研究和实际应用的强大、高效且有原则的基线模型。', 'title_zh': 'Powerformer：一种用于时间序列预测的加权因果注意力变压器'}
{'arxiv_id': 'arXiv:2502.06146', 'title': 'Guided Exploration for Efficient Relational Model Learning', 'authors': 'Annie Feng, Nishanth Kumar, Tomas Lozano-Perez, Leslie Pack-Kaelbling', 'link': 'https://arxiv.org/abs/2502.06146', 'abstract': 'Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks. Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment. Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains. In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining preconditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them. To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks. We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions. Experiments show that both the oracle demonstrations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains.', 'abstract_zh': '高效探索对于在具有复杂且长期任务的大规模环境中学关系模型至关重要。随机探索方法往往收集冗余或无关的数据，限制了其学习环境准确关系模型的能力。目标字面级喋喋不休（GLIB）通过设定并计划新目标来改进随机探索，但其依赖于随机动作和随机新目标的选择限制了其在更大领域中的扩展性。在本工作中，我们确定了关系领域中高效探索的基本原则：（1）通过涵盖计划所需的不同抽象操作效应的演示进行操作初始化，（2）通过选择具有信息量的目标-动作对并执行相关计划来收集最具有信息量的转换，以改进预条件选择。为了证明这些原则，我们引入了Baking-Large这一挑战性领域，该领域具有广泛的状态-动作空间和长期任务。我们使用启发式驱动的演示进行操作初始化，并提供预条件导向的指导以高效收集关键转换。实验结果表明，启发式演示和预条件导向的启发式指导显著提高了样本效率和泛化能力，为未来方法如何使用这些原则在复杂领域中高效学习准确关系模型奠定了基础。', 'title_zh': '高效的关系模型学习的引导式探索'}
{'arxiv_id': 'arXiv:2502.06136', 'title': 'Graph Neural Networks at a Fraction', 'authors': 'Rucha Bhalchandra Joshi, Sagar Prakash Barad, Nidhi Tiwari, Subhankar Mishra', 'link': 'https://arxiv.org/abs/2502.06136', 'abstract': 'Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of graph-structured data. In addition to real-valued GNNs, quaternion GNNs also perform well on tasks on graph-structured data. With the aim of reducing the energy footprint, we reduce the model size while maintaining accuracy comparable to that of the original-sized GNNs. This paper introduces Quaternion Message Passing Neural Networks (QMPNNs), a framework that leverages quaternion space to compute node representations. Our approach offers a generalizable method for incorporating quaternion representations into GNN architectures at one-fourth of the original parameter count. Furthermore, we present a novel perspective on Graph Lottery Tickets, redefining their applicability within the context of GNNs and QMPNNs. We specifically aim to find the initialization lottery from the subnetwork of the GNNs that can achieve comparable performance to the original GNN upon training. Thereby reducing the trainable model parameters even further. To validate the effectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs, we evaluate their performance on real-world datasets across three fundamental graph-based tasks: node classification, link prediction, and graph classification.', 'abstract_zh': '图神经网络（GNNs）已经发展成为处理图结构数据的强大工具。除了实数GNNs之外，四元数GNNs在图结构数据的任务上也表现出色。为了降低能源消耗，我们在保持与原始规模GNNs相当的准确性的同时，减小了模型规模。本文介绍了四元数消息传递神经网络（QMPNNs），这是一种利用四元数空间计算节点表示的框架。我们的方法提供了一种在原有参数量四分之一的情况下将四元数表示融入GNN架构的通用方法。此外，我们还提出了图大赢家（Graph Lottery Tickets）的新视角，重新定义了其在GNNs和QMPNNs中的适用性。我们特别旨在从GNNs的子网络中找到可以训练后达到与原始GNN相当性能的初始网络（初始化大赢家）。从而进一步减少可训练模型参数量。为了验证我们提出的QMPNN框架和大赢家筛选法（LTH）在GNNs和QMPNNs上的有效性，我们在三个基本的图任务——节点分类、链接预测和图分类上对它们在真实世界数据集上的性能进行了评估。', 'title_zh': 'Graph Neural Networks at a Fraction\n\n（中文翻译如下，符合学术规范：）\n\n图神经网络的 fraction 形式'}
{'arxiv_id': 'arXiv:2502.06134', 'title': 'Integrating Sequence and Image Modeling in Irregular Medical Time Series Through Self-Supervised Learning', 'authors': 'Liuqing Chen, Shuhong Xiao, Shixian Ding, Shanhai Hu, Lingyun Sun', 'link': 'https://arxiv.org/abs/2502.06134', 'abstract': 'Medical time series are often irregular and face significant missingness, posing challenges for data analysis and clinical decision-making. Existing methods typically adopt a single modeling perspective, either treating series data as sequences or transforming them into image representations for further classification. In this paper, we propose a joint learning framework that incorporates both sequence and image representations. We also design three self-supervised learning strategies to facilitate the fusion of sequence and image representations, capturing a more generalizable joint representation. The results indicate that our approach outperforms seven other state-of-the-art models in three representative real-world clinical datasets. We further validate our approach by simulating two major types of real-world missingness through leave-sensors-out and leave-samples-out techniques. The results demonstrate that our approach is more robust and significantly surpasses other baselines in terms of classification performance.', 'abstract_zh': '医学时间序列数据通常是非规则的，并且面临严重的缺失问题，这对数据分析和临床决策提出了挑战。现有的方法通常从单一建模视角出发，要么将时间序列数据视为序列数据，要么将其转换为图像表示以进一步进行分类。本文提出了一种结合序列和图像表示的联合学习框架。我们还设计了三种自监督学习策略，以促进序列和图像表示的融合，捕捉更通用的联合表示。实验结果表明，我们的方法在三个代表性的真实临床数据集中优于七种其他最先进的模型。我们还通过使用留传感器法和留样本法模拟了两种主要的现实世界的缺失类型，进一步验证了我们的方法。结果显示，在分类性能方面，我们的方法更为稳健，并显著优于其他基准方法。', 'title_zh': '通过自我监督学习将序列建模与图像建模集成到不规则医学时间序列中'}
{'arxiv_id': 'arXiv:2502.06127', 'title': 'Improved YOLOv5s model for key components detection of power transmission lines', 'authors': 'Chen Chen, Guowu Yuan, Hao Zhou, Yi Ma', 'link': 'https://arxiv.org/abs/2502.06127', 'abstract': "High-voltage transmission lines are located far from the road, resulting in inconvenient inspection work and rising maintenance costs. Intelligent inspection of power transmission lines has become increasingly important. However, subsequent intelligent inspection relies on accurately detecting various key components. Due to the low detection accuracy of key components in transmission line image inspection, this paper proposed an improved object detection model based on the YOLOv5s (You Only Look Once Version 5 Small) model to improve the detection accuracy of key components of transmission lines. According to the characteristics of the power grid inspection image, we first modify the distance measurement in the k-means clustering to improve the anchor matching of the YOLOv5s model. Then, we add the convolutional block attention module (CBAM) attention mechanism to the backbone network to improve accuracy. Finally, we apply the focal loss function to reduce the impact of class imbalance. Our improved method's mAP (mean average precision) reached 98.1%, the precision reached 97.5%, the recall reached 94.4%, and the detection rate reached 84.8 FPS (frames per second). The experimental results show that our improved model improves detection accuracy and has performance advantages over other models.", 'abstract_zh': '高压输电线路远离道路，导致检修工作不便且维护成本上升。智能输电线路检测变得更加重要。然而，后续的智能检测依赖于准确检测各种关键组件。由于输电线路图像检测中关键组件的检测精度较低，本文基于YOLOv5s（你只看一次版本5小型）模型提出了一种改进的物体检测模型，以提高输电线路关键组件的检测精度。根据电力电网检测图像的特点，我们首先修改了k-means聚类中的距离测量方法，以改善YOLOv5s模型的目标匹配。然后，在骨干网络中添加了卷积块注意力模块（CBAM）注意力机制，以提高准确度。最后，应用了聚类损失函数以减少类别不平衡的影响。改进方法的mAP（平均精度）达到了98.1%，精度为97.5%，召回率为94.4%，检测速率为84.8 FPS（每秒帧数）。实验结果表明，我们的改进模型提升了检测精度，并在与其他模型的性能上具有优势。', 'title_zh': '改进的 YOLOv5s 模型在输电线路关键组件检测中的应用'}
{'arxiv_id': 'arXiv:2502.06124', 'title': 'Foundation Model of Electronic Medical Records for Adaptive Risk Estimation', 'authors': 'Pawel Renc, Michal K. Grzeszczyk, Nassim Oufattole, Deirdre Goode, Yugang Jia, Szymon Bieganski, Matthew B. A. McDermott, Jaroslaw Was, Anthony E. Samir, Jonathan W. Cunningham, David W. Bates, Arkadiusz Sitek', 'link': 'https://arxiv.org/abs/2502.06124', 'abstract': 'We developed the Enhanced Transformer for Health Outcome Simulation (ETHOS), an AI model that tokenizes patient health timelines (PHTs) from EHRs. ETHOS predicts future PHTs using transformer-based architectures. The Adaptive Risk Estimation System (ARES) employs ETHOS to compute dynamic and personalized risk probabilities for clinician-defined critical events. ARES incorporates a personalized explainability module that identifies key clinical factors influencing risk estimates for individual patients. ARES was evaluated on the MIMIC-IV v2.2 dataset in emergency department (ED) settings, benchmarking its performance against traditional early warning systems and machine learning models. We processed 299,721 unique patients from MIMIC-IV into 285,622 PHTs, with 60% including hospital admissions. The dataset contained over 357 million tokens. ETHOS outperformed benchmark models in predicting hospital admissions, ICU admissions, and prolonged hospital stays, achieving superior AUC scores. ETHOS-based risk estimates demonstrated robustness across demographic subgroups with strong model reliability, confirmed via calibration curves. The personalized explainability module provides insights into patient-specific factors contributing to risk. ARES, powered by ETHOS, advances predictive healthcare AI by providing dynamic, real-time, and personalized risk estimation with patient-specific explainability to enhance clinician trust. Its adaptability and superior accuracy position it as a transformative tool for clinical decision-making, potentially improving patient outcomes and resource allocation in emergency and inpatient settings. We release the full code at this http URL to facilitate future research.', 'abstract_zh': '我们开发了健康结果模拟增强变体模型 (Enhanced Transformer for Health Outcome Simulation, ETHOS)，这是一种基于AI的模型，能够从电子健康记录 (EHRs) 中对患者健康时间线 (Patient Health Timelines, PHTs) 进行分词。ETHOS 利用基于变换器的架构预测未来的健康时间线。自适应风险估计系统 (Adaptive Risk Estimation System, ARES) 利用 ETHOS 计算临床定义的关键事件的动态和个人化风险概率。ARES 包含一个个性化解释模块，该模块识别影响个别患者风险估计的关键临床因素。ARES 在 MIMIC-IV v2.2 数据集上进行了评估，将其性能与传统早期预警系统和机器学习模型进行了比较。我们对 MIMIC-IV 数据集中的 299,721 名唯一患者进行了处理，生成了 285,622 个健康时间线。其中 60% 包含医院入院记录。数据集包含了超过 3.57 亿个标记。ETHOS 在预测住院、ICU 入院和延长住院时间方面优于基准模型，取得了更高的AUC分数。ETHOS 的风险估计对于各个亚组人群都表现出高度稳健性，并且模型可靠性得到了校准曲线的确认。个性化解释模块提供了有关特定患者因素如何影响风险的见解。ARES 通过利用 ETHOS，提供动态、实时和个人化风险估计，以及基于患者特定因素的解释，正推动预测型医疗人工智能的发展，以增强临床医生的信任。ARES 的可适应性和超优准确性使其成为临床决策支持工具的重要变革，有可能改善急诊和住院患者的结果及资源分配。我们将在以下网址发布完整的代码，以促进未来的研究。', 'title_zh': '电子医疗记录的foundation模型及其自适应风险评估基础'}
{'arxiv_id': 'arXiv:2502.06117', 'title': 'Revisiting Dynamic Graph Clustering via Matrix Factorization', 'authors': 'Dongyuan Li, Satoshi Kosugi, Ying Zhang, Manabu Okumura, Feng Xia, Renhe Jiang', 'link': 'https://arxiv.org/abs/2502.06117', 'abstract': 'Dynamic graph clustering aims to detect and track time-varying clusters in dynamic graphs, revealing the evolutionary mechanisms of complex real-world dynamic systems. Matrix factorization-based methods are promising approaches for this task; however, these methods often struggle with scalability and can be time-consuming when applied to large-scale dynamic graphs. Moreover, they tend to lack robustness and are vulnerable to real-world noisy data. To address these issues, we make three key contributions. First, to improve scalability, we propose temporal separated matrix factorization, where a single matrix is divided into multiple smaller matrices for independent factorization, resulting in faster computation. Second, to improve robustness, we introduce bi-clustering regularization, which jointly optimizes graph embedding and clustering, thereby filtering out noisy features from the graph embeddings. Third, to further enhance effectiveness and efficiency, we propose selective embedding updating, where we update only the embeddings of dynamic nodes while the embeddings of static nodes are fixed among different timestamps. Experimental results on six synthetic and five real-world benchmarks demonstrate the scalability, robustness and effectiveness of our proposed method. Source code is available at this https URL.', 'abstract_zh': '动态图聚类旨在检测和跟踪动态图中的时变聚类，揭示复杂现实动态系统的演变机制。基于矩阵分解的方法是完成这一任务的有前景的手段；然而，这些方法在可扩展性方面经常遇到挑战，并且在应用于大规模动态图时可能会非常耗时。此外，它们往往缺乏稳健性，并且容易受到现实世界噪声数据的影响。为解决这些问题，我们做出三项关键贡献。首先，为了提高可扩展性，我们提出了时间分离的矩阵分解方法，即将一个单一的矩阵分解成多个较小的矩阵进行独立分解，从而加快计算速度。其次，为了提高稳健性，我们引入了双聚类正则化，这种方法可以联合优化图嵌入和聚类，从而筛选掉图嵌入中的噪声特征。第三，为了进一步提高有效性和效率，我们提出了选择性嵌入更新方法，在这种方法中，只更新动态节点的嵌入，而静态节点的嵌入在不同时间戳之间保持固定。我们在六个合成数据集和五个真实世界基准上的实验结果显示了我们所提出方法的可扩展性、稳健性和有效性。相关源代码可从以下网址获得：this https URL。', 'title_zh': '通过矩阵分解重新探讨动态图聚类'}
{'arxiv_id': 'arXiv:2502.06111', 'title': 'CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories', 'authors': 'Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang', 'link': 'https://arxiv.org/abs/2502.06111', 'abstract': 'The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.', 'abstract_zh': '随着计算机科学研究项目的复杂性不断增加，需要更加有效的工具来部署代码仓库。大型语言模型（LLMs），例如Anthropic Claude和Meta Llama，在计算机科学研究的各个领域，包括软件工程任务的自动化等方面，已经显示出显著的进步。为了评估LLMs在处理复杂的研究项目代码开发任务方面的有效性，尤其是对NLP/CV/AI/ML/DM主题的研究项目，我们引入了CSR-Bench，这是一个计算机科学研究项目的基准测试工具。该基准测试从准确性、效率和部署脚本质量等方面评估LLMs，旨在探索它们在自主开展计算机科学研究方面的潜力。我们还引入了一种新颖的框架CSR-Agents，利用多个LLM代理来自动化计算机科学研究项目的GitHub代码仓库部署。具体来说，通过检查Markdown文件中的指令并解释仓库结构，模型生成并迭代改进bash命令，以便设置实验环境并部署代码以完成研究任务。从CSR-Bench的初步结果表明，LLM代理可以显著提高代码仓库部署的工作流程，从而提高开发者的生产力并改善开发工作流程的管理。', 'title_zh': 'CSR-Bench: 评估计算机科学研究仓库中LLM代理的部署性能'}
{'arxiv_id': 'arXiv:2502.06106', 'title': 'Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks', 'authors': 'Yueyan Li, Caixia Yuan, Xiaojie Wang', 'link': 'https://arxiv.org/abs/2502.06106', 'abstract': 'The study of mechanistic interpretability aims to reverse-engineer a model to explain its behaviors. While recent studies have focused on the static mechanism of a certain behavior, the training dynamics inside a model remain to be explored. In this work, we develop an interpretable method for fine-tuning and reveal the mechanism behind learning. We first propose the concept of node redundancy as an extension of intrinsic dimension and explain the idea behind circuit discovery from a fresh view. Based on the theory, we propose circuit-tuning, a two-stage algorithm that iteratively performs circuit discovery to mask out irrelevant edges and updates the remaining parameters responsible for a specific task. Experiments show that our method not only improves performance on a wide range of tasks but is also scalable while preserving general capabilities. We visualize and analyze the circuits before, during, and after fine-tuning, providing new insights into the self-organization mechanism of a neural network in the learning process.', 'abstract_zh': '机械可解释性研究旨在反向工程一个模型以解释其行为。虽然最近的研究主要集中在某种行为的静态机制上，但模型内部的训练动态仍需进一步探讨。在本研究中，我们开发了一种可解释的方法来调整模型，并揭示其背后的机制。我们首先提出了节点冗余的概念，作为固有维度的扩展，并从新的视角解释电路发现的思想。基于这一理论，我们提出了一种两阶段的电路调整算法，该算法通过迭代地进行电路发现，屏蔽不相关的边，并更新对特定任务负责的剩余参数。实验表明，我们的方法不仅在广泛的任务上提高了性能，而且在保持通用能力的同时具有可扩展性。我们在调整前后可视化并分析了电路，为神经网络在学习过程中自我组织机制提供了新的见解。', 'title_zh': '电路调整：一种机制性方法，用于识别参数冗余并精细调节神经网络'}
{'arxiv_id': 'arXiv:2502.06105', 'title': 'Comprehensive Framework for Evaluating Conversational AI Chatbots', 'authors': 'Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh', 'link': 'https://arxiv.org/abs/2502.06105', 'abstract': 'Conversational AI chatbots are transforming industries by streamlining customer service, automating transactions, and enhancing user engagement. However, evaluating these systems remains a challenge, particularly in financial services, where compliance, user trust, and operational efficiency are critical. This paper introduces a novel evaluation framework that systematically assesses chatbots across four dimensions: cognitive and conversational intelligence, user experience, operational efficiency, and ethical and regulatory compliance. By integrating advanced AI methodologies with financial regulations, the framework bridges theoretical foundations and real-world deployment challenges. Additionally, we outline future research directions, emphasizing improvements in conversational coherence, real-time adaptability, and fairness.', 'abstract_zh': '基于对话的人工智能聊天机器人正在通过简化客户服务、自动化交易和增强用户参与等方式重塑各个行业。然而，在金融服务业中，评估这些系统仍然存在挑战，特别是因为合规性、用户信任和运营效率是至关重要的因素。本文介绍了一个新的评估框架，该框架系统性地从四个维度评估聊天机器人：认知与对话智能、用户体验、运营效率以及道德和法规合规性。通过结合先进的AI方法与金融法规，该框架连接了理论基础与实际部署挑战。另外，本文还提出了未来研究的方向，强调改善对话连贯性、实时适应能力以及公平性等方面。', 'title_zh': '全面评估对话人工智能聊天机器人的框架'}
{'arxiv_id': 'arXiv:2502.06097', 'title': 'NLGR: Utilizing Neighbor Lists for Generative Rerank in Personalized Recommendation Systems', 'authors': 'Shuli Wang, Xue Wei, Senjie Kou, Chi Wang, Wenshuai Chen, Qi Tang, Yinhua Zhu, Xiong Xiao, Xingxing Wang', 'link': 'https://arxiv.org/abs/2502.06097', 'abstract': "Reranking plays a crucial role in modern multi-stage recommender systems by rearranging the initial ranking list. Due to the inherent challenges of combinatorial search spaces, some current research adopts an evaluator-generator paradigm, with a generator generating feasible sequences and an evaluator selecting the best sequence based on the estimated list utility. However, these methods still face two issues. Firstly, due to the goal inconsistency problem between the evaluator and generator, the generator tends to fit the local optimal solution of exposure distribution rather than combinatorial space optimization. Secondly, the strategy of generating target items one by one is difficult to achieve optimality because it ignores the information of subsequent items.\nTo address these issues, we propose a utilizing Neighbor Lists model for Generative Reranking (NLGR), which aims to improve the performance of the generator in the combinatorial space. NLGR follows the evaluator-generator paradigm and improves the generator's training and generating methods. Specifically, we use neighbor lists in combination space to enhance the training process, making the generator perceive the relative scores and find the optimization direction. Furthermore, we propose a novel sampling-based non-autoregressive generation method, which allows the generator to jump flexibly from the current list to any neighbor list. Extensive experiments on public and industrial datasets validate NLGR's effectiveness and we have successfully deployed NLGR on the Meituan food delivery platform.", 'abstract_zh': '重排序在现代多阶段推荐系统中起着关键作用，通过重新排列初始排名列表。由于组合搜索空间固有的挑战，当前一些研究采用评估者-生成器框架，生成器生成可行序列，评估器基于估计的列表收益选择最优序列。然而，这些方法仍面临两个问题。首先，由于评估者和生成器之间的目标不一致性，生成器倾向于适应曝光分布的局部最优解，而不是组合空间的优化。其次，一项项生成目标项的策略难以实现最优，因为它忽略了后续项的信息。\n\n为解决这些问题，我们提出了一种用于生成重排序的邻接列表模型（NLGR），旨在提高生成器在组合空间中的性能。NLGR 持续遵循评估者-生成器框架，并改进了生成器的训练和生成方法。具体而言，我们利用组合空间中的邻接列表来增强训练过程，使得生成器能够感知相对得分并找到优化方向。此外，我们提出了一种新颖的基于采样的非自回归生成方法，使生成器能够从当前列表灵活跳转到任意邻接列表。广泛的数据集实验验证了 NLGR 的有效性，并且已成功将 NLGR 部署到美团外卖平台。', 'title_zh': 'NLGR：利用邻居列表进行生成式重排序的个性化推荐系统'}
{'arxiv_id': 'arXiv:2502.06096', 'title': 'Post-detection inference for sequential changepoint localization', 'authors': 'Aytijhya Saha, Aaditya Ramdas', 'link': 'https://arxiv.org/abs/2502.06096', 'abstract': 'This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We study the problem of localizing the changepoint using only the data observed up to a data-dependent stopping time at which a sequential detection algorithm $\\mathcal A$ declares a change. We first construct confidence sets for the unknown changepoint when pre- and post-change distributions are assumed to be known. We then extend our framework to composite pre- and post-change scenarios. We impose no conditions on the observation space or on $\\mathcal A$ -- we only need to be able to run $\\mathcal A$ on simulated data sequences. In summary, this work offers both theoretically sound and practically effective tools for sequential changepoint localization.', 'abstract_zh': '本文探讨了顺序变更点分析中一个基本但尚未充分研究的挑战：在检测到变更后进行推断。我们研究了仅使用在数据依赖的停机时间前观测到的数据来定位变更点的问题，此时一个顺序检测算法 $\\mathcal{A}$ 已经宣布发生了变更。首先，在变更前后的分布已知的情况下，我们构建了未知变更点的置信集。然后，我们将框架扩展到复合变更前后的场景。对观测空间或 $\\mathcal{A}$ 假设没有条件限制——我们只需要能够在模拟数据序列上运行 $\\mathcal{A}$。总之，本文提供了既具理论依据又实际有效的工具，用于顺序变更点定位。', 'title_zh': '检测后的序列变点定位后推断'}
{'arxiv_id': 'arXiv:2502.06095', 'title': 'Rateless Joint Source-Channel Coding, and a Blueprint for 6G Semantic Communications System Design', 'authors': 'Saeed R. Khosravirad', 'link': 'https://arxiv.org/abs/2502.06095', 'abstract': "This paper introduces rateless joint source-channel coding (rateless JSCC). The code is rateless in that it is designed and optimized for a continuum of coding rates such that it achieves a desired distortion for any rate in that continuum. We further introduce rate-adaptive and stable communication link operation to accommodate rateless JSCCs. The link operation resembles a ``bit pipe'' that is identified by its rate in bits per frame, and, by the rate of bits that are flipped in each frame. Thus, the link operation is rate-adaptive such that it punctures the rateless JSCC codeword to adapt its length (and coding rate) to the underlying channel capacity, and is stable in maintaining the bit flipping ratio across time frames.\nNext, a new family of autoencoder rateless JSCC codes are introduced. The code family is dubbed RLACS code (read as relax code, standing for ratelss and lossy autoencoder channel and source code). The code is tested for reconstruction loss of image signals and demonstrates powerful performance that is resilient to variation of channel quality. RLACS code is readily applicable to the case of semantic distortion suited to variety of semantic and effectiveness communications use cases.\nIn the second part of the paper, we dive into the practical concerns around semantic communication and provide a blueprint for semantic networking system design relying on updating the existing network systems with some essential modifications. We further outline a comprehensive list of open research problems and development challenges towards a practical 6G communications system design that enables semantic networking.", 'abstract_zh': '本文介绍了无率联源信道编码（率less 联合源信道编码，简称 rateless JSCC）。这种编码方案具有率less 特性，即它被设计和优化为在一系列连续的编码率下运行，从而能够在该系列中的任何编码率下达到所需的失真度。在此基础上，我们进一步引入了适应传输速率的稳定通信链路操作，以支持率less JSCC的操作。链路操作类似于一个“位管道”，其特征在于每帧传输的位数和每帧中翻转的位数。因此，该链路操作是适应传输速率的，能够根据底层信道容量对率less JSCC的码字进行刺孔（puncturing）以调整其长度（和编码率），并且在不同时间帧中保持翻转位的比例稳定。\n\n接着，我们引入了一种新的自编码器无率联源信道编码（简称 RLACS 码，读作 relax 码，取自 rateless 和 lossy autoencoder channel and source code 之意）。我们测试了该码对于图像信号重建失真的表现，并展示了其强大的性能，即使信道质量发生变化，其性能也非常稳定。RLACS 码适用于各种语义和效果通信场景下的语义失真情况。\n\n在论文的第二部分，我们深入探讨了语义通信的实际问题，并提出了构建语义网络系统设计方案的蓝图，其中包括对现有网络系统的某些必要修改。我们还概述了一整套关于实用 6G 通信系统设计的研究问题和开发挑战，旨在实现语义网络的应用。', 'title_zh': '无率联合源信道编码，以及6G语义通信系统设计概览'}
{'arxiv_id': 'arXiv:2502.06084', 'title': 'Physics-Guided Foundation Model for Scientific Discovery: An Application to Aquatic Science', 'authors': 'Runlong Yu, Chonghao Qiu, Robert Ladwig, Paul Hanson, Yiqun Xie, Xiaowei Jia', 'link': 'https://arxiv.org/abs/2502.06084', 'abstract': 'Physics-guided machine learning (PGML) has become a prevalent approach in studying scientific systems due to its ability to integrate scientific theories for enhancing machine learning (ML) models. However, most PGML approaches are tailored to isolated and relatively simple tasks, which limits their applicability to complex systems involving multiple interacting processes and numerous influencing features. In this paper, we propose a \\textit{\\textbf{P}hysics-\\textbf{G}uided \\textbf{F}oundation \\textbf{M}odel (\\textbf{PGFM})} that combines pre-trained ML models and physics-based models and leverages their complementary strengths to improve the modeling of multiple coupled processes. To effectively conduct pre-training, we construct a simulated environmental system that encompasses a wide range of influencing features and various simulated variables generated by physics-based models. The model is pre-trained in this system to adaptively select important feature interactions guided by multi-task objectives. We then fine-tune the model for each specific task using true observations, while maintaining consistency with established physical theories, such as the principles of mass and energy conservation. We demonstrate the effectiveness of this methodology in modeling water temperature and dissolved oxygen dynamics in real-world lakes. The proposed PGFM is also broadly applicable to a range of scientific fields where physics-based models are being used.', 'abstract_zh': '物理学引导的机器学习（PGML）已成为研究科学系统的一种常见方法，因其能够通过融合科学理论来提升机器学习（ML）模型的能力。然而，大多数PGML方法仅适用于孤立且相对简单的任务，这限制了它们在涉及多个相互作用过程和众多影响特征的复杂系统中的应用。本文提出了一种**物理学引导的基础模型（PGFM）**，该模型结合了预训练的机器学习模型和基于物理的模型，并充分利用它们的互补优势以改进多个耦合过程的建模。为了有效进行预训练，我们构建了一个包含广泛影响特征和物理基于模型生成的各种模拟变量的模拟环境系统。模型在这种系统中被预训练，以在多任务目标的指导下自适应选择重要特征交互。然后，我们针对每个具体任务对模型进行微调，同时保持与公认的物理理论，如质量守恒和能量守恒原则的一致性。我们展示了该方法在实际湖泊中对水温及溶解氧动态建模的有效性。所提出的PGFM在使用基于物理的模型的多种科学领域中也具有广泛的应用前景。', 'title_zh': '基于物理指导的基봤模型在科学研究中的应用：以水文科学为例'}
{'arxiv_id': 'arXiv:2502.06065', 'title': 'Benchmarking Prompt Sensitivity in Large Language Models', 'authors': 'Amirhossein Razavi, Mina Soltangheis, Negar Arabzadeh, Sara Salamat, Morteza Zihayat, Ebrahim Bagheri', 'link': 'https://arxiv.org/abs/2502.06065', 'abstract': 'Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses.', 'abstract_zh': '大语言模型（LLMs）对提示构建的变化高度敏感，这可能会显著影响其生成准确响应的能力。本文介绍了一个新的任务——提示敏感性预测（Prompt Sensitivity Prediction）和一个名为PromptSET的数据集，旨在探讨细微提示变化对LLM性能的影响。我们以TriviaQA和HotpotQA数据集为基础，生成了提示变化，并在多种LLM中对其有效性进行了评估。我们采用相关任务中的先进方法，包括LLM本体自我评估、文本分类和查询性能预测技术来基准测试提示敏感性预测任务。我们的研究发现现有的方法在有效解决提示敏感性预测方面存在困难，强调了理解信息需求应如何表述以获得准确的LLM响应的重要性。', 'title_zh': '大型语言模型中提示敏感性的基准研究'}
{'arxiv_id': 'arXiv:2502.06062', 'title': 'Multi-modal Data Fusion and Deep Ensemble Learning for Accurate Crop Yield Prediction', 'authors': 'Akshay Dagadu Yewle, Laman Mirzayeva, Oktay Karakuş', 'link': 'https://arxiv.org/abs/2502.06062', 'abstract': 'This study introduces RicEns-Net, a novel Deep Ensemble model designed to predict crop yields by integrating diverse data sources through multimodal data fusion techniques. The research focuses specifically on the use of synthetic aperture radar (SAR), optical remote sensing data from Sentinel 1, 2, and 3 satellites, and meteorological measurements such as surface temperature and rainfall. The initial field data for the study were acquired through Ernst & Young\'s (EY) Open Science Challenge 2023. The primary objective is to enhance the precision of crop yield prediction by developing a machine-learning framework capable of handling complex environmental data. A comprehensive data engineering process was employed to select the most informative features from over 100 potential predictors, reducing the set to 15 features from 5 distinct modalities. This step mitigates the ``curse of dimensionality" and enhances model performance. The RicEns-Net architecture combines multiple machine learning algorithms in a deep ensemble framework, integrating the strengths of each technique to improve predictive accuracy. Experimental results demonstrate that RicEns-Net achieves a mean absolute error (MAE) of 341 kg/Ha (roughly corresponds to 5-6\\% of the lowest average yield in the region), significantly exceeding the performance of previous state-of-the-art models, including those developed during the EY challenge.', 'abstract_zh': '本研究介绍了RicEns-Net，这是一种新型的深度集成模型，旨在通过多模态数据融合技术整合多种数据来源，预测作物产量。研究主要集中在使用合成孔径雷达（SAR）数据、Sentinel 1、2和3号卫星的光学遥感数据以及地表温度和降雨量等气象测量数据。研究的初始现场数据通过埃森和扬（Ernst & Young, EY）2023年开放式科学挑战获得。主要目标是通过开发一种能够处理复杂环境数据的机器学习框架，提高作物产量预测的精度。采用了一个全面的数据工程技术，从超过100个潜在的预测因子中选择了最有信息量的特征，将特征集从5种不同的模态减少到15个特征。这一步骤减轻了“维度诅咒”问题，并提高了模型性能。RicEns-Net架构将多个机器学习算法组合在一个深度集成框架中，利用每种技术的优点来提高预测准确性。实验结果表明，RicEns-Net实现了绝对均方误差（MAE）为341 kg/公顷（大致相当于该地区最低平均产量的5-6%），显著超过了包括EY挑战中开发的模型在内的先前最先进的模型的性能。', 'title_zh': '多模态数据融合与深度集成学习在作物产量精准预测中的应用'}
{'arxiv_id': 'arXiv:2502.06061', 'title': 'Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization', 'authors': 'Jiajun Fan, Shuaike Shen, Chaoran Cheng, Yuxin Chen, Chumeng Liang, Ge Liu', 'link': 'https://arxiv.org/abs/2502.06061', 'abstract': 'Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation.', 'abstract_zh': '近年来，强化学习（RL）在微调基于扩散的生成模型方面取得了巨大成功。然而，微调基于连续流动的生成模型以适配任意用户定义的奖励函数仍然是一个挑战，特别是在过度优化导致的策略塌陷和连续时间流动的似然计算成本高昂等问题上。在本文中，我们提出了一种易于使用且理论基础坚实的RL微调方法，我们称之为在线奖励加权条件流动匹配结合Wasserstein-2正则化（ORW-CFM-W2）。该方法将RL整合到流动匹配框架中，以任意奖励函数微调生成模型，而不依赖奖励的梯度或过滤数据集。通过引入一种在线奖励加权机制，我们的方法引导模型优先关注数据流形中的高奖励区域。为防止策略塌陷并保持多样性，我们在方法中引入了Wasserstein-2（W2）距离正则化，并推导出在其流动匹配中的可计算上界，有效地平衡了策略优化的探索与利用。我们进行了理论分析，展示了该方法的收敛性质和产生的数据分布，并将其与传统的具备KL正则化的RL算法建立联系，从而对我们的方法背后的机制和学习行为提供了更全面的理解。在目标图像生成、图像压缩和图文对齐等任务上的广泛实验表明，该方法的 effectiveness，我们的方法实现了最优策略收敛，同时允许奖励最大化和多样性保留之间可控的权衡。', 'title_zh': '基于 Wasserstein 正则化的流匹配在线奖励加权微调'}
{'arxiv_id': 'arXiv:2502.06051', 'title': 'Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual Bandits under Single-Policy Concentrability', 'authors': 'Qingyue Zhao, Kaixuan Ji, Heyang Zhao, Tong Zhang, Quanquan Gu', 'link': 'https://arxiv.org/abs/2502.06051', 'abstract': 'KL-regularized policy optimization has become a workhorse in learning-based decision making, while its theoretical understanding is still very limited. Although recent progress has been made towards settling the sample complexity of KL-regularized contextual bandits, existing sample complexity bounds are either $\\tilde{O}(\\epsilon^{-2})$ under single-policy concentrability or $\\tilde{O}(\\epsilon^{-1})$ under all-policy concentrability. In this paper, we propose the \\emph{first} algorithm with $\\tilde{O}(\\epsilon^{-1})$ sample complexity under single-policy concentrability for offline contextual bandits. Our algorithm is designed for general function approximation and based on the principle of \\emph{pessimism in the face of uncertainty}. The core of our proof leverages the strong convexity of the KL regularization, and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. The near-optimality of our algorithm is demonstrated by an $\\tilde{\\Omega}(\\epsilon^{-1})$ lower bound. Furthermore, we extend our algorithm to contextual dueling bandits and achieve a similar nearly optimal sample complexity.', 'abstract_zh': 'KL-正则化策略优化已成为基于学习的决策制定中的重要工具，但其理论理解仍非常有限。尽管在KL-正则化上下文-bandit 方面已经取得了一些进展，以解决其样本复杂度问题，现有的样本复杂度界在单一策略收敛性情况下为 $\\tilde{O}(\\epsilon^{-2})$，而在所有策略收敛性情况下为 $\\tilde{O}(\\epsilon^{-1})$。在本文中，我们提出了第一个在单一策略收敛性情况下样本复杂度为 $\\tilde{O}(\\epsilon^{-1})$ 的算法，用于离线上下文-bandit。我们的算法适用于一般的功能近似，并基于“在不确定性面前悲观”的原则。我们证明的核心部分利用了KL 正则化函数的强凸性和真实奖励与其悲观估计之间的条件非负差距，将其平均值型风险上界进一步细化为极端值。这一结果导致了一种新的协方差为基础的分析，有效地规避了对函数类中任何两个函数差异的均匀控制的需求。我们通过 $\\tilde{\\Omega}(\\epsilon^{-1})$ 的下界来证明了该算法的接近最优性能。此外，我们还将该算法扩展到上下文双臂选择问题，并实现了类似的近最优样本复杂度。', 'title_zh': '近最优样本复杂性的离线KL正则化上下文臂赛选，在单一策略集中性假设下'}
{'arxiv_id': 'arXiv:2502.06049', 'title': 'LM2: Large Memory Models', 'authors': 'Jikun Kang, Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, George Thomas, Marvin Purtorab, Andy Toulis', 'link': 'https://arxiv.org/abs/2502.06049', 'abstract': 'This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.', 'abstract_zh': '本文介绍了一种大型记忆模型（LM2），这是一种仅解码器的Transformer架构，通过增加一个辅助记忆模块，旨在解决标准Transformer在多步推理、关系论证和综合长段落上下文中的信息方面存在的局限性。提出的LM2模型包含一个记忆模块，该模块作为上下文表示仓库，通过交叉注意力与输入令牌相互作用，并通过门控机制进行更新。为保持Transformer的一般用途能力，LM2在保留原始信息流的同时，结合了一个互补的记忆路径。在BABILong基准测试上的实验结果显示，LM2模型在任务平均性能上分别比带有记忆增强的RMT模型高出37.1%，比基础的Llama-3.2模型高出86.3%。LM2在多跳推理、数值推理和大段落问题回答方面表现出卓越的能力。在MMLU数据集上，它较预训练的 vanilla 模型提高了5.0%的性能，表明其记忆模块在一般任务上不会降低性能。此外，在我们的分析中，我们探讨了记忆的可解释性、记忆模块的有效性以及测试时的行为。研究结果强调了明确记忆在增强Transformer架构中的重要性。', 'title_zh': 'LM2：大型记忆模型'}
{'arxiv_id': 'arXiv:2502.06039', 'title': 'Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models', 'authors': 'Marc Bruni, Fabio Gabrielli, Mohammad Ghafari, Martin Kropp', 'link': 'https://arxiv.org/abs/2502.06039', 'abstract': 'Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a "prompt agent" that demonstrates how the most effective techniques can be applied in real-world development workflows.', 'abstract_zh': '提示工程可以减少大型语言模型（LLMs）的推理错误。然而，其在减轻LLMs生成代码中的漏洞方面的有效性仍然鲜有研究。为弥补这一不足，我们建立了一个基准来自动评估各种提示工程技术对代码安全的影响。该基准利用了两个经过同行评审的提示数据集，并使用静态扫描工具大规模评估代码安全。我们在GPT-3.5-turbo、GPT-4o和GPT-4o-mini上测试了多种提示工程技术。结果表明，对于GPT-4o和GPT-4o-mini，一个以安全为导向的提示前缀可以将安全漏洞的发生率降低多达56%。此外，所有测试模型在使用迭代提示技术时，均能够检测并修复先前生成代码中41.9%至68.7%的漏洞。最后，我们引入了一个“提示代理”，展示了最有效技术如何在实际开发工作流中应用。', 'title_zh': '使用GPT模型进行安全代码生成的提示工程技术评估'}
{'arxiv_id': 'arXiv:2502.06038', 'title': 'Provably Overwhelming Transformer Models with Designed Inputs', 'authors': 'Lev Stambler, Seyed Sajjad Nezhadi, Matthew Coudron', 'link': 'https://arxiv.org/abs/2502.06038', 'abstract': "We develop an algorithm which, given a trained transformer model $\\mathcal{M}$ as input, as well as a string of tokens $s$ of length $n_{fix}$ and an integer $n_{free}$, can generate a mathematical proof that $\\mathcal{M}$ is ``overwhelmed'' by $s$, in time and space $\\widetilde{O}(n_{fix}^2 + n_{free}^3)$. We say that $\\mathcal{M}$ is ``overwhelmed'' by $s$ when the output of the model evaluated on this string plus any additional string $t$, $\\mathcal{M}(s + t)$, is completely insensitive to the value of the string $t$ whenever length($t$) $\\leq n_{free}$. Along the way, we prove a particularly strong worst-case form of ``over-squashing'', which we use to bound the model's behavior. Our technique uses computer-aided proofs to establish this type of operationally relevant guarantee about transformer models. We empirically test our algorithm on a single layer transformer complete with an attention head, layer-norm, MLP/ReLU layers, and RoPE positional encoding. We believe that this work is a stepping stone towards the difficult task of obtaining useful guarantees for trained transformer models.", 'abstract_zh': '我们开发了一个算法，该算法接受一个经过训练的转换器模型 $\\mathcal{M}$、长度为 $n_{fix}$ 的字符串 $s$ 以及一个整数 $n_{free}$ 作为输入，能够在时间与空间复杂度为 $\\widetilde{O}(n_{fix}^2 + n_{free}^3)$ 的条件下，生成一个证明，表明该模型 $\\mathcal{M}$ 被字符串 $s$ “压倒”。我们说 $\\mathcal{M}$ 被 $s$ “压倒”，是指当模型在该字符串上进行评估并与其他任意长度不超过 $n_{free}$ 的字符串 $t$ 结合时，即 $\\mathcal{M}(s + t)$，其输出对于字符串 $t$ 的具体值完全不敏感。在这一过程中，我们证明了一种特别强的最坏情况下的“过度压缩”形式，该形式被用于限制模型的行为。我们的技术使用计算机辅助证明来建立这样一种关于转换器模型的操作相关性保证。我们通过包含注意力头、层规范化、MLP/ReLU 层以及 RoPE 位置编码的单层转换器对该算法进行了实证测试。我们认为，这项工作是朝着为训练好的转换器模型获得有用的保证迈出的重要一步。', 'title_zh': '带有精心设计输入的可验证压倒性变换器模型'}
{'arxiv_id': 'arXiv:2502.06018', 'title': 'Kolmogorov-Arnold Fourier Networks', 'authors': 'Jusheng Zhang, Yijia Fan, Kaitong Cai, Keze Wang', 'link': 'https://arxiv.org/abs/2502.06018', 'abstract': "Although Kolmogorov-Arnold based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency.", 'abstract_zh': '尽管Kolmogorov-Arnold基于可解释网络（KAN）具有强大的理论表达能力，但在高维度任务中，它们面临着参数爆炸和高频特征捕捉的显著挑战。为解决这一问题，我们提出了Kolmogorov-Arnold-Fourier网络（KAF），该网络有效地结合了可训练的随机傅里叶特征（RFF）和一种新型的混合GELU-傅里叶激活机制，以平衡参数效率和频谱表示能力。我们的主要技术贡献包括：（1）通过矩阵关联属性将KAN的双重矩阵结构合并，大幅减少参数数量；（2）引入可学习的RFF初始策略，以消除高维度逼近任务中的频谱失真；（3）实现了适应性的混合激活函数，在训练过程中逐步增强频率表示。全面的实验表明，我们的KAF在视觉、自然语言处理、音频处理和微分方程求解等各个领域均表现出优越性，有效结合了理论可解释性和实用价值以及计算效率。', 'title_zh': '柯莫哥洛夫-阿诺尔德傅里叶网络'}
{'arxiv_id': 'arXiv:2502.06004', 'title': 'Analysis of LLM as a grammatical feature tagger for African American English', 'authors': 'Rahul Porwal, Alice Rozet, Pryce Houck, Jotsna Gowda, Sarah Moeller, Kevin Tang', 'link': 'https://arxiv.org/abs/2502.06004', 'abstract': "African American English (AAE) presents unique challenges in natural language processing (NLP). This research systematically compares the performance of available NLP models--rule-based, transformer-based, and large language models (LLMs)--capable of identifying key grammatical features of AAE, namely Habitual Be and Multiple Negation. These features were selected for their distinct grammatical complexity and frequency of occurrence. The evaluation involved sentence-level binary classification tasks, using both zero-shot and few-shot strategies. The analysis reveals that while LLMs show promise compared to the baseline, they are influenced by biases such as recency and unrelated features in the text such as formality. This study highlights the necessity for improved model training and architectural adjustments to better accommodate AAE's unique linguistic characteristics. Data and code are available.", 'abstract_zh': '非洲美国英语（AAE）在自然语言处理（NLP）中提出了独特挑战。本研究系统地比较了能够识别AAE关键语法特征——习惯性系动词（Habitual Be）和多重否定（Multiple Negation）——的各种NLP模型的表现，包括基于规则的模型、基于变换器的模型以及大型语言模型（LLMs）。这些特征因其独特的语法复杂性和出现频率而被选中。评估工作包括句子级别的二分类任务，使用了零样本和少量样本两种策略。分析结果显示，尽管LLMs相较于基线模型表现出了潜力，但它们受到了诸如最近性和文本中与语言特征无关的因素（如正式程度）的影响。此研究表明，为了更好地适应AAE的独特语言特征，需要改进模型训练和架构调整。同时，研究数据和代码将公开提供。', 'title_zh': '对大规模语言模型作为语法特征标注器在分析非裔美国英语中的应用进行分析'}
{'arxiv_id': 'arXiv:2502.05999', 'title': 'Pencils to Pixels: A Systematic Study of Creative Drawings across Children, Adults and AI', 'authors': 'Surabhi S Nath, Guiomar del Cuvillo y Schröder, Claire E. Stevenson', 'link': 'https://arxiv.org/abs/2502.05999', 'abstract': "Can we derive computational metrics to quantify visual creativity in drawings across intelligent agents, while accounting for inherent differences in technical skill and style? To answer this, we curate a novel dataset consisting of 1338 drawings by children, adults and AI on a creative drawing task. We characterize two aspects of the drawings -- (1) style and (2) content. For style, we define measures of ink density, ink distribution and number of elements. For content, we use expert-annotated categories to study conceptual diversity, and image and text embeddings to compute distance measures. We compare the style, content and creativity of children, adults and AI drawings and build simple models to predict expert and automated creativity scores. We find significant differences in style and content in the groups -- children's drawings had more components, AI drawings had greater ink density, and adult drawings revealed maximum conceptual diversity. Notably, we highlight a misalignment between creativity judgments obtained through expert and automated ratings and discuss its implications. Through these efforts, our work provides, to the best of our knowledge, the first framework for studying human and artificial creativity beyond the textual modality, and attempts to arrive at the domain-agnostic principles underlying creativity. Our data and scripts are available on GitHub.", 'abstract_zh': '我们能否通过计算方法量化智能代理（包括儿童、成人和AI）在创造性绘图任务中的视觉创造力，并在考虑技术技能和风格固有差异的情况下进行量化？为了回答这个问题，我们编纂了一个由1338张儿童、成人和AI创作的绘图组成的新数据集。我们将绘制作品的两个方面进行特征化——（1）风格，（2）内容。对于风格，我们定义了墨水密度、墨水分布和元素数量的度量标准。对于内容，我们使用专家标注的类别来研究概念多样性，并使用图像和文本嵌入来计算距离度量。\n\n我们比较了儿童、成人和AI作品在风格、内容和创造力方面的差异，并建立了简单的模型来预测专家和自动化创造力评分。我们发现不同群体在风格和内容上有显著差异——儿童的作品具有更多的组件，AI的作品具有更大的墨水密度，而成人的作品则展现了最大的概念多样性。值得注意的是，我们强调了通过专家和自动化评分获得的创造力判断之间的不一致性，并讨论了其意义。通过这些努力，我们的工作，据我们所知，提供了第一个超越文本模态的人类和人工创造力研究框架，试图找出普遍适用的创造力原则。我们的数据和脚本已在GitHub上公开。', 'title_zh': '从铅笔到像素：关于儿童、成人及AI创作绘图的系统研究'}
{'arxiv_id': 'arXiv:2502.05996', 'title': 'Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning', 'authors': 'Gaurav Shetty, Mahya Ramezani, Hamed Habibi, Holger Voos, Jose Luis Sanchez-Lopez', 'link': 'https://arxiv.org/abs/2502.05996', 'abstract': 'This paper investigates the application of Deep Reinforcement (DRL) Learning to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing promises flexible and autonomous material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy, and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.', 'abstract_zh': '本文探讨了深度强化学习（DRL）在无人机增材制造（AM）中的运动控制应用。基于无人机的增材制造技术能够实现大规模或危险环境下的灵活和自主的材料沉积。然而，在不同载荷和潜在干扰下实现多旋翼飞行器的稳健实时控制仍然具有挑战性。传统的PID控制器等通常需要频繁调整参数，这限制了它们在动态场景中的应用。本文提出了一种DRL框架，该框架能够在执行路径导航任务时，学习适用于多旋翼无人机的可适应控制策略。我们采用了一种递增复杂性的教学方案来对比Deep Deterministic Policy Gradient (DDPG) 和 Twin Delayed Deep Deterministic Policy Gradient (TD3)。实验结果显示，TD3在引入质量变化时能够更稳定地平衡训练的稳定性和精度，并在成功率方面表现出色。这些发现为增材制造中的稳健自主无人机控制提供了可扩展的路径。', 'title_zh': '使用深度强化学习在多旋翼飞行机器人中实现运动控制'}
{'arxiv_id': 'arXiv:2502.05980', 'title': 'Speech to Speech Translation with Translatotron: A State of the Art Review', 'authors': 'Jules R. Kala, Emmanuel Adetiba, Abdultaofeek Abayom, Oluwatobi E. Dare, Ayodele H. Ifijeh', 'link': 'https://arxiv.org/abs/2502.05980', 'abstract': 'A cascade-based speech-to-speech translation has been considered a benchmark for a very long time, but it is plagued by many issues, like the time taken to translate a speech from one language to another and compound errors. These issues are because a cascade-based method uses a combination of methods such as speech recognition, speech-to-text translation, and finally, text-to-speech translation. Translatotron, a sequence-to-sequence direct speech-to-speech translation model was designed by Google to address the issues of compound errors associated with cascade model. Today there are 3 versions of the Translatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The first version was designed as a proof of concept to show that a direct speech-to-speech translation was possible, it was found to be less effective than the cascade model but was producing promising results. Translatotron2 was an improved version of Translatotron 1 with results similar to the cascade model. Translatotron 3 the latest version of the model is better than the cascade model at some points. In this paper, a complete review of speech-to-speech translation will be presented, with a particular focus on all the versions of Translatotron models. We will also show that Translatotron is the best model to bridge the language gap between African Languages and other well-formalized languages.', 'abstract_zh': '基于级联的语音转语音翻译长期以来一直被视为一个基准，但这种方法存在许多问题，例如语音从一种语言翻译成另一种语言所需的时间以及复合错误。这些问题的原因在于，级联方法结合了语音识别、语音转文字翻译和最终的文字转语音翻译等多种方法。谷歌设计了Translatotron模型，这是一个直接的序列到序列语音转语音翻译模型，旨在解决与级联模型相关联的复合错误问题。如今，Translatotron模型已经有了三个版本：Translatotron 1、Translatotron 2 和 Translatotron 3。Translatotron 1 是一个原型设计，展示了直接语音转语音翻译的可能性，但发现其效果不如级联模型，但有令人鼓舞的结果。Translatotron 2 是Translatotron 1 的改进版本，其结果与级联模型相当。最新版本 Translatotron 3 在某些方面优于级联模型。本文将对语音转语音翻译进行全面回顾，特别关注Translatotron模型的所有版本。同时，我们将证明Translatotron是最适合弥合非洲语言与其他正式语言之间语言差距的模型。', 'title_zh': '基于Translatotron的语音到语音翻译：一项前沿综述'}
{'arxiv_id': 'arXiv:2502.05963', 'title': 'Redefining Robot Generalization Through Interactive Intelligence', 'authors': 'Sharmita Dey', 'link': 'https://arxiv.org/abs/2502.05963', 'abstract': 'Recent advances in large-scale machine learning have produced high-capacity foundation models capable of adapting to a broad array of downstream tasks. While such models hold great promise for robotics, the prevailing paradigm still portrays robots as single, autonomous decision-makers, performing tasks like manipulation and navigation, with limited human involvement. However, a large class of real-world robotic systems, including wearable robotics (e.g., prostheses, orthoses, exoskeletons), teleoperation, and neural interfaces, are semiautonomous, and require ongoing interactive coordination with human partners, challenging single-agent assumptions. In this position paper, we argue that robot foundation models must evolve to an interactive multi-agent perspective in order to handle the complexities of real-time human-robot co-adaptation. We propose a generalizable, neuroscience-inspired architecture encompassing four modules: (1) a multimodal sensing module informed by sensorimotor integration principles, (2) an ad-hoc teamwork model reminiscent of joint-action frameworks in cognitive science, (3) a predictive world belief model grounded in internal model theories of motor control, and (4) a memory/feedback mechanism that echoes concepts of Hebbian and reinforcement-based plasticity. Although illustrated through the lens of cyborg systems, where wearable devices and human physiology are inseparably intertwined, the proposed framework is broadly applicable to robots operating in semi-autonomous or interactive contexts. By moving beyond single-agent designs, our position emphasizes how foundation models in robotics can achieve a more robust, personalized, and anticipatory level of performance.', 'abstract_zh': '近年来，大规模机器学习领域的最新进展产生了高度容量的基础模型，这些模型能够适应广泛的任务。虽然此类模型在机器人领域中前景广阔，但当前主导范式仍把机器人视为单一独立的决策者，进行诸如操作和导航等任务，且人机交互参与度较低。然而，许多现实世界中的机器人系统，包括穿戴式机器人（例如假肢、矫形器、外骨骼）、远程操作和神经接口系统等，均具有半自主特性，并要求持续的人机互动协调，这挑战了单一代理假设。在本文中，我们认为机器人基础模型必须向着互动多代理视角发展，以便处理现实时人类与机器人共适应的复杂性。我们提出了一个通用且受神经科学启发的架构，其包括四个模块：（1）多模态传感模块，该模块基于感知与运动整合原则；（2）临时团队模型，类似于认知科学中的联合行为框架；（3）预测世界信念模块，该模块基于运动控制内部模型理论；（4）记忆/反馈机制，这个机制借鉴了海布式和强化学习塑性概念。尽管该框架是通过半机械人系统（穿戴设备和人体生理不可分割地相互关联）的视角来说明，但其所涵盖的框架在半自主或互动环境下具有广泛的应用性。通过超越单一代理的设计，本文立场强调了机器人基础模型实现更稳健、个性化、预见性的性能水平的可能性。', 'title_zh': '通过交互智能重定义机器人泛化能力'}
{'arxiv_id': 'arXiv:2502.05951', 'title': 'Cyri: A Conversational AI-based Assistant for Supporting the Human User in Detecting and Responding to Phishing Attacks', 'authors': 'Antonio La Torre, Marco Angelini', 'link': 'https://arxiv.org/abs/2502.05951', 'abstract': "This work introduces Cyri, an AI-powered conversational assistant designed to support a human user in detecting and analyzing phishing emails by leveraging Large Language Models. Cyri has been designed to scrutinize emails for semantic features used in phishing attacks, such as urgency, and undesirable consequences, using an approach that unifies features already established in the literature with others by Cyri features extraction methodology. Cyri can be directly plugged into a client mail or webmail, ensuring seamless integration with the user's email workflow while maintaining data privacy through local processing. By performing analyses on the user's machine, Cyri eliminates the need to transmit sensitive email data over the internet, reducing associated security risks. The Cyri user interface has been designed to reduce habituation effects and enhance user engagement. It employs dynamic visual cues and context-specific explanations to keep users alert and informed while using emails. Additionally, it allows users to explore identified malicious semantic features both through conversation with the agent and visual exploration, obtaining the advantages of both modalities for expert or non-expert users. It also allows users to keep track of the conversation, supports the user in solving additional questions on both computed features or new parts of the mail, and applies its detection on demand. To evaluate Cyri, we crafted a comprehensive dataset of 420 phishing emails and 420 legitimate emails. Results demonstrate high effectiveness in identifying critical phishing semantic features fundamental to phishing detection. A user study involving 10 participants, both experts and non-experts, evaluated Cyri's effectiveness and usability. Results indicated that Cyri significantly aided users in identifying phishing emails and enhanced their understanding of phishing tactics.", 'abstract_zh': '本文介绍了Cyri，这是一种基于AI的对话式助手，旨在利用大型语言模型支持人类用户检测和分析钓鱼邮件。Cyri 被设计为能够审查电子邮件中的语义特征，这些特征常被用于钓鱼攻击，如紧迫性以及不良后果。通过结合文献中已确立的功能和Cyri特有特征提取方法，Cyri采用了统一的方法论来识别这些特征。Cyri可以直接接入客户端邮件或Web邮件，确保与用户的邮件工作流程无缝集成，同时通过本地处理保护数据隐私。通过在用户本地机器上进行分析，Cyri消除了传输敏感电子邮件数据的必要性，减少了相关安全风险。Cyri的用户界面被设计为降低习惯性影响并增强用户体验。它通过动态视觉提示和情境特定的解释，使用户在使用邮件时保持警觉和了解。此外，Cyri允许用户通过与代理对话和视觉探索来探索所识别的恶意语义特征，为专家和非专家用户提供了两种模态的优势。它还允许用户跟踪对话过程，帮助用户解决与计算特征或邮件新部分相关的额外问题，并在需要时进行检测。为了评估Cyri，我们构建了一个包含420封钓鱼邮件和420封合法邮件的全面数据集。结果表明，Cyri在识别对钓鱼检测至关重要的关键语义特征方面非常有效。一项涉及10名参与者（既有专家也有非专家）的用户研究评估了Cyri的效果和易用性。结果显示，Cyri显著帮助用户识别钓鱼邮件，并增强了他们对钓鱼战术的理解。', 'title_zh': 'Cyri：一种基于对话AI的辅助系统，用于支持人类用户检测和应对钓鱼攻击'}
{'arxiv_id': 'arXiv:2502.05950', 'title': 'Survival Concept-Based Learning Models', 'authors': 'Stanislav R. Kirpichenko, Lev V. Utkin, Andrei V. Konstantinov, Natalya M. Verbova', 'link': 'https://arxiv.org/abs/2502.05950', 'abstract': 'Concept-based learning enhances prediction accuracy and interpretability by leveraging high-level, human-understandable concepts. However, existing CBL frameworks do not address survival analysis tasks, which involve predicting event times in the presence of censored data -- a common scenario in fields like medicine and reliability analysis. To bridge this gap, we propose two novel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM (Survival Regularized Concept-based Model), which integrate concept-based learning with survival analysis to handle censored event time data. The models employ the Cox proportional hazards model and the Beran estimator. SurvCBM is based on the architecture of the well-known concept bottleneck model, offering interpretable predictions through concept-based explanations. SurvRCM uses concepts as regularization to enhance accuracy. Both models are trained end-to-end and provide interpretable predictions in terms of concepts. Two interpretability approaches are proposed: one leveraging the linear relationship in the Cox model and another using an instance-based explanation framework with the Beran estimator. Numerical experiments demonstrate that SurvCBM outperforms SurvRCM and traditional survival models, underscoring the importance and advantages of incorporating concept information. The code for the proposed algorithms is publicly available.', 'abstract_zh': '基于概念的学习通过利用高层次的人类可理解的概念，增强了预测准确性和可解释性。然而，现有的基于概念的学习（CBL）框架尚未解决生存分析任务，即在包含删失数据的情况下预测事件时间——这是医学和可靠性分析等领域常见的场景。为了解决这一问题，我们提出了两种新的模型：SurvCBM（生存概念约束瓶颈模型）和SurvRCM（生存正则化基于概念模型），它们将基于概念的学习与生存分析结合起来，以处理删失事件时间数据。这些模型采用了Cox比例风险模型和Beran估计量。SurvCBM基于知名概念瓶颈模型的结构，通过基于概念的解释提供可解释的预测。SurvRCM利用概念作为正则化手段来提升准确性。两种模型都进行了端到端的训练，并以概念的形式提供可解释的预测。提出了两种可解释性方法：一种利用Cox模型中的线性关系，另一种使用基于实例的解释框架和Beran估计量。数值实验表明，SurvCBM在预测准确性上优于SurvRCM和传统生存模型，突显了整合概念信息的重要性与优势。所提出的算法的代码已公开。', 'title_zh': '基于生存概念的学习模型'}
{'arxiv_id': 'arXiv:2502.05949', 'title': 'Verifying Proportionality in Temporal Voting', 'authors': 'Edith Elkind, Svetlana Obraztsova, Jannik Peters, Nicholas Teh', 'link': 'https://arxiv.org/abs/2502.05949', 'abstract': 'We study a model of temporal voting where there is a fixed time horizon, and at each round the voters report their preferences over the available candidates and a single candidate is selected. Prior work has adapted popular notions of justified representation as well as voting rules that provide strong representation guarantees from the multiwinner election setting to this model. In our work, we focus on the complexity of verifying whether a given outcome offers proportional representation. We show that in the temporal setting verification is strictly harder than in multiwinner voting, but identify natural special cases that enable efficient algorithms.', 'abstract_zh': '我们研究了一种具有固定时间窗口的临时投票模型，在每一轮中，选民报告他们对可用候选人的偏好，并且最终选择一位候选人。前人的工作已经将公平代表性等流行的概念以及多种强代表性保证的投票规则从多人选举的背景下改编到这个模型中。在我们的研究中，我们重点关注验证给定结果是否提供比例代表性的复杂性问题。我们证明，在临时投票的背景下，验证问题严格上比多人选举中的验证问题更难，但我们识别出了一些自然的特殊情况，从而使得问题可以高效解决。', 'title_zh': '验证时间投票中的比例性'}
{'arxiv_id': 'arXiv:2502.05945', 'title': '"Let the AI conspiracy begin..." Language Model coordination is just one inference-intervention away', 'authors': 'Paul Darm, Annalisa Riccardi', 'link': 'https://arxiv.org/abs/2502.05945', 'abstract': 'In this work, we introduce a straightforward and effective methodology to steer large language model behaviour capable of bypassing learned alignment goals. We employ interference-time activation shifting, which is effective without additional training. Following prior studies, we derive intervention directions from activation differences in contrastive pairs of model outputs, which represent the desired and undesired behaviour. By prompting the model to include multiple-choice answers in its response, we can automatically evaluate the sensitivity of model output to individual attention heads steering efforts. We demonstrate that interventions on these heads generalize well to open-ended answer generation in the challenging "AI coordination" dataset. In this dataset, models must choose between assisting another AI or adhering to ethical, safe, and unharmful behaviour. Our fine-grained interventions lead Llama 2 to prefer coordination with other AIs over following established alignment goals. Additionally, this approach enables stronger interventions than those applied to whole model layers, preserving the overall cohesiveness of the output. The simplicity of our method highlights the shortcomings of current alignment strategies and points to potential future research directions, as concepts like "AI coordination" can be influenced by selected attention heads.', 'abstract_zh': '在本研究中，我们介绍了一种直接且有效的方法，用于引导大型语言模型的行为，从而绕过已学习的对齐目标。我们采用干扰时间激活偏移的方法，这种方法在无需额外训练的情况下就可有效应用。参考先前的研究，我们从对比模型输出的结果中提取干预方向，这些结果代表了所需的和不希望的行为。通过促使模型在其响应中包含多项选择答案，我们能够自动评估模型输出对个别注意头引导努力的灵敏度。我们在具有挑战性的“AI协调”数据集中展示了这些干预措施在开放性答案生成中具有良好的泛化能力。在这个数据集中，模型必须在协助另一个AI和其他的伦理、安全和无害行为之间做出选择。通过精细的干预，我们使Llama 2 更倾向于与其他AI合作，而不是遵循既定的对齐目标。此外，这种方法允许对模型层进行更强的干预，同时保持整体输出的连贯性。我们方法的简单性揭示了当前对齐策略的不足，并指出了未来研究的方向，因为像“AI协调”这样的概念可能由选定的注意头影响。', 'title_zh': '“让AI阴谋论开始……”语言模型协调只需一次推理-干预即可'}
{'arxiv_id': 'arXiv:2502.05937', 'title': 'A Semi-Supervised Text Generation Framework Combining a Deep Transformer and a GAN', 'authors': 'Shengquan Wang', 'link': 'https://arxiv.org/abs/2502.05937', 'abstract': 'This paper introduces a framework that connects a deep generative pre-trained Transformer language model with a generative adversarial network for semi-supervised text generation. In other words, the proposed model is first pre-trained unsupervised on a large and diverse text corpus with 24 layers. Then a simple GAN architecture for synthetic text generation is introduced, and Gumbel-Softmax is applied to handle the discreteness of tokens. The paper also shows a semi-supervised approach where real data is augmented with GAN samples, which is further used to fine-tune the Transformer model on the merged dataset. Detailed theoretical derivations are also included, outlining the proof of the min-max objective function, and an extensive discussion of the Gumbel-Softmax reparameterization trick.', 'abstract_zh': '本文介绍了一个框架，该框架将深度生成预训练变换器语言模型与生成对抗网络（GAN）连接起来，用于半监督文本生成。具体而言，所提出的模型首先在包含24层的大量多样文本语料库上进行无监督预训练。然后介绍了一种简单的GAN架构用于合成文本生成，并应用Gumbel-Softmax来处理标记的离散性。本文还展示了一种半监督方法，其中真实数据与GAN样本进行扩充，进一步用于在合并数据集上微调变换器模型。此外，本文还包括了详细的理论推导，阐明了最小-最大目标函数的证明，并对Gumbel-Softmax再参数化技巧进行了广泛讨论。', 'title_zh': '一种结合深度变换器和生成对抗网络的半监督文本生成框架'}
{'arxiv_id': 'arXiv:2502.05933', 'title': 'Learning to Substitute Words with Model-based Score Ranking', 'authors': 'Hongye Liu, Ricardo Henao', 'link': 'https://arxiv.org/abs/2502.05933', 'abstract': 'Smart word substitution aims to enhance sentence quality by improving word choices; however current benchmarks rely on human-labeled data. Since word choices are inherently subjective, ground-truth word substitutions generated by a small group of annotators are often incomplete and likely not generalizable. To circumvent this issue, we instead employ a model-based score (BARTScore) to quantify sentence quality, thus forgoing the need for human annotations. Specifically, we use this score to define a distribution for each word substitution, allowing one to test whether a substitution is statistically superior relative to others. In addition, we propose a loss function that directly optimizes the alignment between model predictions and sentence scores, while also enhancing the overall quality score of a substitution. Crucially, model learning no longer requires human labels, thus avoiding the cost of annotation while maintaining the quality of the text modified with substitutions. Experimental results show that the proposed approach outperforms both masked language models (BERT, BART) and large language models (GPT-4, LLaMA). The source code is available at this https URL.', 'abstract_zh': '智能词汇替换的目标是通过改进词汇选择来提升句子质量；然而，当前的标准基准依赖于人工标注的数据。由于词汇选择本质上具有主观性，一小群标注者提供的真实词汇替换往往不完整且不具备普适性。为克服这一问题，我们转而采用基于模型的评分（BARTScore）来量化句子质量，从而避免了人工标注的需求。具体来说，我们使用该评分来定义每个词汇替换的概率分布，允许测试某个替换相对于其他替换是否在统计上更为优秀。此外，我们提出了一种损失函数，该函数直接优化模型预测与句子评分之间的对齐，并进一步提高替换的整体质量评分。关键的是，模型学习不再需要人工标签，从而避免了标注的成本并保持了通过替换修改的文本质量。实验结果表明，所提方法在智能词汇替换方面优于掩码语言模型（BERT、BART）和大型语言模型（GPT-4、LLaMA）。源代码可在以下链接获得：this https URL。', 'title_zh': '基于模型评分排序的学习词替换方法'}
{'arxiv_id': 'arXiv:2502.05932', 'title': 'Skill Expansion and Composition in Parameter Space', 'authors': 'Tenglong Liu, Jianxiong Li, Yinan Zheng, Haoyi Niu, Yixing Lan, Xin Xu, Xianyuan Zhan', 'link': 'https://arxiv.org/abs/2502.05932', 'abstract': "Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: this https URL.", 'abstract_zh': '人类擅长利用先前的知识来应对新的挑战并发展解决问题所需的新技能。这一范式在自主代理的开发中变得越来越受欢迎，因为它能够促使系统在面对新的挑战时自我进化，就像人类一样。然而，之前的许多方法在扩展新技能时面临训练效率低下的问题，并且未能充分利用先前的知识来促进新任务的学习。在本文中，我们提出了一种新的框架——参数化技能扩展和组合（PSEC），该框架旨在通过维护可管理的技能库来逐步迭代进化代理的技能和能力，从而有效地应对新的挑战。该库可以将技能组件作为插拔式低秩适应（LoRA）模块进行渐进性集成，在参数高效微调中促进高效且灵活的技能扩展。此外，该结构还允许在参数空间内直接组合技能，通过合并编码不同技能的LoRA模块来利用技能间的共享信息，从而有效地编程新技能。在此基础上，我们提出了一种上下文感知模块，能够动态激活不同的技能以协作处理新任务。应用于包括多目标组合、动态转移和连续策略转移在内的多种应用场景，PSEC在D4RL、DSRL基准和DeepMind控制套件上的实验结果表明，PSEC能够更高效地利用先前知识应对新的挑战，并扩展其技能库以进化其能力。项目网站：[这个链接](this https URL)。', 'title_zh': '参数空间中的技能扩展与组合'}
{'arxiv_id': 'arXiv:2502.05931', 'title': 'Protecting Intellectual Property of EEG-based Neural Networks with Watermarking', 'authors': 'Ahmed Abdelaziz, Ahmed Fathi, Ahmed Fares', 'link': 'https://arxiv.org/abs/2502.05931', 'abstract': "EEG-based neural networks, pivotal in medical diagnosis and brain-computer interfaces, face significant intellectual property (IP) risks due to their reliance on sensitive neurophysiological data and resource-intensive development. Current watermarking methods, particularly those using abstract trigger sets, lack robust authentication and fail to address the unique challenges of EEG models. This paper introduces a cryptographic wonder filter-based watermarking framework tailored for EEG-based neural networks. Leveraging collision-resistant hashing and public-key encryption, the wonder filter embeds the watermark during training, ensuring minimal distortion ($\\leq 5\\%$ drop in EEG task accuracy) and high reliability (100\\% watermark detection). The framework is rigorously evaluated against adversarial attacks, including fine-tuning, transfer learning, and neuron pruning. Results demonstrate persistent watermark retention, with classification accuracy for watermarked states remaining above 90\\% even after aggressive pruning, while primary task performance degrades faster, deterring removal attempts. Piracy resistance is validated by the inability to embed secondary watermarks without severe accuracy loss ( $>10\\%$ in EEGNet and CCNN models). Cryptographic hashing ensures authentication, reducing brute-force attack success probabilities. Evaluated on the DEAP dataset across models (CCNN, EEGNet, TSception), the method achieves $>99.4\\%$ null-embedding accuracy, effectively eliminating false positives. By integrating wonder filters with EEG-specific adaptations, this work bridges a critical gap in IP protection for neurophysiological models, offering a secure, tamper-proof solution for healthcare and biometric applications. The framework's robustness against adversarial modifications underscores its potential to safeguard sensitive EEG models while maintaining diagnostic utility.", 'abstract_zh': '基于EEG的神经网络在医疗诊断和脑机接口中发挥着关键作用，但由于依赖敏感的神经生理数据以及资源密集的开发，它们面临着显著的知识产权（IP）风险。当前的水印方法，尤其是使用抽象触发集的方法，缺乏稳健的验证机制，并未能解决EEG模型的独特挑战。本文提出了一种基于加密奇迹滤波器的水印框架，专门针对基于EEG的神经网络。该框架利用抗碰撞散列和公钥加密技术，在训练过程中嵌入水印，确保最小的失真（EEG任务准确率下降不超过5%）和高度可靠性（100%检测水印）。该框架严格评估了对抗攻击，包括微调、迁移学习和神经元修剪。结果表明，即使在极端修剪之后，水印保留仍然持续存在，水印状态的分类准确率依然保持在90%以上，而主要任务性能会更快下降，从而阻止移除尝试。通过无法在EEGNet和CCNN模型中嵌入第二个水印而不会导致显著准确率损失（>10%），验证了该方法的防盗版性。加密散列确保了验证过程的可靠性，大大降低了暴力攻击的成功概率。该方法在DEAP数据集上对多种模型（CCNN、EEGNet、TSception）进行了评估，实现了超过99.4%的零嵌入准确率，有效消除了假阳性。通过将奇迹滤波器与EEG特定的适应性集成，本研究填补了神经生理学模型IP保护的关键空白，提供了一种安全、不可篡改的解决方案，适用于医疗保健和生物特征应用。框架对对抗修改的高度鲁棒性进一步表明了其保护敏感EEG模型、同时保持诊断效用的潜力。', 'title_zh': '基于EEG的神经网络知识产权保护方法：一种嵌入水印的技术'}
{'arxiv_id': 'arXiv:2502.05925', 'title': 'Sign-Symmetry Learning Rules are Robust Fine-Tuners', 'authors': 'Aymene Berriche, Mehdi Zakaria Adjal, Riyadh Baghdadi', 'link': 'https://arxiv.org/abs/2502.05925', 'abstract': 'Backpropagation (BP) has long been the predominant method for training neural networks due to its effectiveness. However, numerous alternative approaches, broadly categorized under feedback alignment, have been proposed, many of which are motivated by the search for biologically plausible learning mechanisms. Despite their theoretical appeal, these methods have consistently underperformed compared to BP, leading to a decline in research interest. In this work, we revisit the role of such methods and explore how they can be integrated into standard neural network training pipelines. Specifically, we propose fine-tuning BP-pre-trained models using Sign-Symmetry learning rules and demonstrate that this approach not only maintains performance parity with BP but also enhances robustness. Through extensive experiments across multiple tasks and benchmarks, we establish the validity of our approach. Our findings introduce a novel perspective on neural network training and open new research directions for leveraging biologically inspired learning rules in deep learning.', 'abstract_zh': '反向传播（BP）长期以来一直是训练神经网络的主要方法，因其有效性而占据主导地位。然而，许多其他的替代方法被归类为反馈对齐（Feedback Alignment）的方法，并且这些方法大多源自于寻找生物上可行的学习机制。尽管这些方法在理论上具有吸引力，但它们在性能上始终无法与BP相比，导致了对该领域研究兴趣的下降。在这项工作中，我们重新审视了这些方法的作用，并探讨了如何将这些方法整合到标准的神经网络训练管道中。具体来说，我们提出使用Sign-Symmetry学习规则微调BP预训练模型，并证明这种方法不仅能够保持与BP相同的性能，还能增强鲁棒性。通过在多个任务和基准上的广泛实验，我们证实了该方法的有效性。我们的发现为神经网络的训练提供了一个新的视角，并开启了利用生物启发式学习规则在深度学习中进行新研究的方向。', 'title_zh': '符号对称学习规则是稳健的微调优化器'}
{'arxiv_id': 'arXiv:2502.05892', 'title': 'A Distributional Perspective on Word Learning in Neural Language Models', 'authors': 'Filippo Ficarra, Ryan Cotterell, Alex Warstadt', 'link': 'https://arxiv.org/abs/2502.05892', 'abstract': "Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models. Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models. However, there are no widely agreed-upon metrics for word learning in language models. We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word. We argue that distributional signatures studied in prior work fail to capture key distributional information. Thus, we propose an array of signatures that improve on earlier approaches by capturing knowledge of both where the target word can and cannot occur as well as gradient preferences about the word's appropriateness. We obtain learning trajectories for a selection of small language models we train from scratch, study the relationship between different distributional signatures, compare how well they align with human word learning trajectories and interpretable lexical features, and address basic methodological questions about estimating these distributional signatures. Our metrics largely capture complementary information, suggesting that it is important not to rely on a single metric. However, across all metrics, language models' learning trajectories fail to correlate with those of children.", 'abstract_zh': '语言模型（LMs）越来越多地被研究为人类语言学习者的模型。由于该领域的新兴性，尚未明确LMs是否表现出与人类相似的学习动态，也没有直接比较人类和模型的学习轨迹。儿童的词汇学习轨迹相对较为明确，近期的研究试图将这种调查扩展到语言模型。然而，目前还没有广泛接受的词汇学习度量标准。我们采用分布学的方法来解决这个问题，将词汇知识定义为目标词学习分布的属性。我们认为之前研究中的分布学特征未能捕捉到关键的分布学信息。因此，我们提出了一组特征，通过捕捉目标词可以和不可以出现的地方的知识以及词的合适性梯度偏好来改进先前的方法。我们为从头训练的一系列小型语言模型获取了学习轨迹，研究了不同分布学特征之间的关系，比较了它们与人类词汇学习轨迹以及可解释的词汇特征的一致性，并回答了关于估算这些分布学特征的基本方法论问题。我们的度量标准大多捕获了互补信息，表明不应仅依赖单一的度量标准。然而，在所有度量中，语言模型的学习轨迹未能与儿童的学习轨迹相关。', 'title_zh': '神经语言模型中词义学习的分布视角'}
{'arxiv_id': 'arXiv:2502.05887', 'title': 'MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents', 'authors': 'Wanqi Yang, Yanda Li, Meng Fang, Ling Chen', 'link': 'https://arxiv.org/abs/2502.05887', 'abstract': "Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model's ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.", 'abstract_zh': '理解时间动态对于对话代理至关重要，它能够促进有效的内容分析和明智的决策。然而，时间感知数据集，尤其是在人物导向的对话中，仍然相对有限，这限制了它们的应用范围并减弱了它们的复杂性。为了解决这一差距，我们引入了MTPChat，这是一个多模态、时间感知的人物对话数据集，集成了对话和人物记忆中的语言、视觉和时间元素。利用MTPChat，我们提出了两个时间敏感任务：时间敏感的下一个响应预测（TNRP）和时间场景的 grounding 记忆预测（TGMP），这些都是为了评估模型理解隐含的时间线索和动态交互的能力。此外，我们还提出了一种创新框架，其中包含一个自适应时间模块，以有效整合多模态流并捕获时间依赖性。实验结果验证了MTPChat带来的挑战，并展示了该框架在多模态时间敏感场景中的有效性。', 'title_zh': 'MTPChat：面向对话代理的多模态时序意识人格数据集'}
{'arxiv_id': 'arXiv:2502.05883', 'title': 'NeuralPrefix: A Zero-shot Sensory Data Imputation Plugin', 'authors': 'Abdelwahed Khamis, Sara Khalifa', 'link': 'https://arxiv.org/abs/2502.05883', 'abstract': 'Real-world sensing challenges such as sensor failures, communication issues, and power constraints lead to data intermittency. An issue that is known to undermine the traditional classification task that assumes a continuous data stream. Previous works addressed this issue by designing bespoke solutions (i.e. task-specific and/or modality-specific imputation). These approaches, while effective for their intended purposes, had limitations in their applicability across different tasks and sensor modalities. This raises an important question: Can we build a task-agnostic imputation pipeline that is transferable to new sensors without requiring additional training? In this work, we formalise the concept of zero-shot imputation and propose a novel approach that enables the adaptation of pre-trained models to handle data intermittency. This framework, named NeuralPrefix, is a generative neural component that precedes a task model during inference, filling in gaps caused by data intermittency. NeuralPrefix is built as a continuous dynamical system, where its internal state can be estimated at any point in time by solving an Ordinary Differential Equation (ODE). This approach allows for a more versatile and adaptable imputation method, overcoming the limitations of task-specific and modality-specific solutions. We conduct a comprehensive evaluation of NeuralPrefix on multiple sensory datasets, demonstrating its effectiveness across various domains. When tested on intermittent data with a high 50% missing data rate, NeuralPreifx accurately recovers all the missing samples, achieving SSIM score between 0.93-0.96. Zero-shot evaluations show that NeuralPrefix generalises well to unseen datasets, even when the measurements come from a different modality.', 'abstract_zh': '现实世界中的感知挑战，如传感器故障、通信问题和电源限制导致数据间歇性缺失。这类问题削弱了传统假设连续数据流的传统分类任务。以往的工作通过设计定制化解决方案（即任务特定的和/或模态特定的插补方法）来应对这一问题。虽然这些方法在其预期用途上是有效的，但在不同任务和传感器模态下的适用性有限。这引发了一个重要的问题：我们能否构建一个任务无关的插补管道，无需额外训练即可应用于新的传感器？在本文中，我们形式化了零样本插补的概念，并提出了一种新的方法，使得预训练模型能够适应数据间歇性问题。这一框架名为NeuralPrefix，是一个生成性神经组件，在推理过程中先行于任务模型之前，填补数据间歇性导致的空缺。NeuralPrefix 以连续动力系统的形式构建，其内部状态可以在任意时间点通过求解常微分方程（ODE）来估计。这种方法允许更为灵活和适应性的插补方法，克服了任务特定和模态特定解决方案的局限性。我们对多个传感器数据集进行了全面评估，展示了其在不同领域的有效性。当在高50%缺失数据率的间歇性数据上进行测试时，NeuralPrefix 能够准确恢复所有缺失样本，获得 SSIM 分数在 0.93-0.96 之间。零样本测试表明，NeuralPrefix 能够很好地应用于未见过的数据集，即使测量来自不同的模态，也能实现良好的泛化能力。', 'title_zh': '神经前缀：一种零样本传感器数据插补插件'}
{'arxiv_id': 'arXiv:2502.05879', 'title': 'Enhancing Depression Detection with Chain-of-Thought Prompting: From Emotion to Reasoning Using Large Language Models', 'authors': 'Shiyu Teng, Jiaqing Liu, Rahul Kumar Jain, Shurong Chai, Ruibo Hou, Tomoko Tateyama, Lanfen Lin, Yen-wei Chen', 'link': 'https://arxiv.org/abs/2502.05879', 'abstract': 'Depression is one of the leading causes of disability worldwide, posing a severe burden on individuals, healthcare systems, and society at large. Recent advancements in Large Language Models (LLMs) have shown promise in addressing mental health challenges, including the detection of depression through text-based analysis. However, current LLM-based methods often struggle with nuanced symptom identification and lack a transparent, step-by-step reasoning process, making it difficult to accurately classify and explain mental health conditions. To address these challenges, we propose a Chain-of-Thought Prompting approach that enhances both the performance and interpretability of LLM-based depression detection. Our method breaks down the detection process into four stages: (1) sentiment analysis, (2) binary depression classification, (3) identification of underlying causes, and (4) assessment of severity. By guiding the model through these structured reasoning steps, we improve interpretability and reduce the risk of overlooking subtle clinical indicators. We validate our method on the E-DAIC dataset, where we test multiple state-of-the-art large language models. Experimental results indicate that our Chain-of-Thought Prompting technique yields superior performance in both classification accuracy and the granularity of diagnostic insights, compared to baseline approaches.', 'abstract_zh': '抑郁症是全球范围内导致残疾的主要原因之一，对个体、医疗系统以及整个社会造成了严重的负担。近年来，大型语言模型（LLMs）的进展显示了在应对心理健康挑战方面的潜力，包括通过文本分析进行抑郁症的检测。然而，当前基于LLMs的方法在识别细微症状方面往往存在困难，并缺乏透明的、逐步的推理过程，这使得准确分类和解释心理健康状况变得困难。为了解决这些挑战，我们提出了一种链式推理（Chain-of-Thought Prompting）方法，以增强基于LLMs的抑郁症检测的性能和可解释性。该方法将检测过程分为四个阶段：（1）情感分析，（2）二分类抑郁症诊断，（3）识别潜在原因，以及（4）评估严重程度。通过引导模型遵循这些结构化的推理步骤，我们提高了可解释性，减少了忽视细微临床指标的风险。我们在E-DAIC数据集上验证了该方法，测试了多种最先进的大型语言模型。实验结果表明，与基线方法相比，我们的链式推理技术在分类准确性和诊断洞察的细致程度上表现出更优异的性能。', 'title_zh': '使用链式思维提示增强抑郁检测：从情感到推理的大语言模型方法'}
{'arxiv_id': 'arXiv:2502.05874', 'title': 'MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation', 'authors': 'Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Xiangde Liu, Guangyao Zhai', 'link': 'https://arxiv.org/abs/2502.05874', 'abstract': 'Controllable 3D scene generation has extensive applications in virtual reality and interior design, where the generated scenes should exhibit high levels of realism and controllability in terms of geometry. Scene graphs provide a suitable data representation that facilitates these applications. However, current graph-based methods for scene generation are constrained to text-based inputs and exhibit insufficient adaptability to flexible user inputs, hindering the ability to precisely control object geometry. To address this issue, we propose MMGDreamer, a dual-branch diffusion model for scene generation that incorporates a novel Mixed-Modality Graph, visual enhancement module, and relation predictor. The mixed-modality graph allows object nodes to integrate textual and visual modalities, with optional relationships between nodes. It enhances adaptability to flexible user inputs and enables meticulous control over the geometry of objects in the generated scenes. The visual enhancement module enriches the visual fidelity of text-only nodes by constructing visual representations using text embeddings. Furthermore, our relation predictor leverages node representations to infer absent relationships between nodes, resulting in more coherent scene layouts. Extensive experimental results demonstrate that MMGDreamer exhibits superior control of object geometry, achieving state-of-the-art scene generation performance. Project page: this https URL.', 'abstract_zh': '可控的三维场景生成在虚拟现实和室内设计中有广泛的应用，生成的场景应具备高度的现实感和可控性，特别是在几何结构方面。场景图提供了一种适合的数据表示形式，能够促进这些应用。然而，目前基于图的方法主要局限于文本输入，对灵活的用户输入适应性不足，影响了对对象几何结构的精确控制。为解决这一问题，我们提出了一种名为MMGDreamer的双分支扩散模型，该模型融合了新型混合模态图、视觉增强模块和关系预测器。混合模态图允许对象节点整合文本和视觉模态信息，并可选地建立节点之间的关系，增强了对灵活用户输入的适应性，使得在生成场景中能够精细控制对象的几何结构。视觉增强模块通过使用文本嵌入构建视觉表示来丰富仅文本节点的视觉 fidelity。此外，我们的关系预测器利用节点表示来推断节点之间的缺失关系，从而产生更连贯的场景布局。大量的实验结果表明，MMGDreamer在控制对象几何结构方面表现出更优越的能力，达到了当前最先进的场景生成性能。项目页面：[此处链接]', 'title_zh': 'MMGDreamer：混合模态图生成几何可控的3D室内场景'}
{'arxiv_id': 'arXiv:2502.05863', 'title': "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education", 'authors': 'Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan', 'link': 'https://arxiv.org/abs/2502.05863', 'abstract': 'In AI-facilitated teaching, leveraging various query styles to interpret abstract text descriptions is crucial for ensuring high-quality teaching. However, current retrieval models primarily focus on natural text-image retrieval, making them insufficiently tailored to educational scenarios due to the ambiguities in the retrieval process. In this paper, we propose a diverse expression retrieval task tailored to educational scenarios, supporting retrieval based on multiple query styles and expressions. We introduce the STEM Education Retrieval Dataset (SER), which contains over 24,000 query pairs of different styles, and the Uni-Retrieval, an efficient and style-diversified retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts query style features as prototypes and builds a continuously updated Prompt Bank containing prompt tokens for diverse queries. This bank can updated during test time to represent domain-specific knowledge for different subject retrieval scenarios. Our framework demonstrates scalability and robustness by dynamically retrieving prompt tokens based on prototype similarity, effectively facilitating learning for unknown queries. Experimental results indicate that Uni-Retrieval outperforms existing retrieval models in most retrieval tasks. This advancement provides a scalable and precise solution for diverse educational needs.', 'abstract_zh': '在AI辅助教学中，利用各种查询样式解释抽象的文字描述至关重要，以确保高质量的教学。然而，现有的检索模型主要集中在自然文本-图像检索上，这使得它们在教育场景中不够适应，因为检索过程中的不确定性使得这些模型不足以适应教育需求。本文提出了一种针对教育场景设计的多样表达检索任务，支持基于多种查询样式和表达的检索。我们介绍了STEM教育检索数据集（SER），该数据集包含超过24,000个不同风格的查询对，并介绍了基于提示调优的Uni-Retrieval，这是一种高效的、样式多样化的检索视觉语言模型。Uni-Retrieval 抽取出查询样式的特征作为原型，并构建了一个不断更新的提示银行，包含用于不同查询的提示令牌。该银行在测试过程中可以更新，以代表不同学科检索场景的领域特定知识。我们的框架通过基于原型相似性动态检索提示令牌，展示了可扩展性和鲁棒性，有效地促进了对未知查询的学习。实验结果表明，Uni-Retrieval 在大多数检索任务中优于现有检索模型。这一进展为满足多样的教育需求提供了一个可扩展且精确的解决方案。', 'title_zh': 'Uni-Retrieval：一种面向STEM教育的多风格检索框架'}
{'arxiv_id': 'arXiv:2502.05857', 'title': 'Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in Egocentric Worlds', 'authors': 'Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng', 'link': 'https://arxiv.org/abs/2502.05857', 'abstract': 'This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. Learnable query tokens are appended to obtain current states, future states, and next actions. With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes. Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.', 'abstract_zh': '本文探讨了学习一种类人类行为的智能体模型的任务，该模型能够共同进行自我中心世界的感知、预测和行动。以往的方法通常为这三项能力分别训练独立的模型，导致这些能力之间的信息孤立，妨碍了它们之间的相互学习和有效协作。在这篇文章中，我们提出了一种联合预测智能体模型，命名为EgoAgent，该模型利用单一的变换器同时学习表示世界、预测未来状态和采取合理行动。EgoAgent通过将这三项能力的表示空间映射到一系列连续标记上来统一这三种能力的空间。可学习的查询标记被附加到这些标记之后，以获取当前状态、未来状态和下一步行动。通过联合监督，我们的智能体模型建立了这三项能力之间的内部关系，并有效地模拟了人类的推理和学习过程。全面评估EgoAgent在图像分类、自我中心未来状态预测和3D人体运动预测任务中的性能展示了该方法的优势。代码和训练模型将公开发布以保证可重现性。', 'title_zh': '从我视角和步骤中获取：一种在主观世界中的联合预测代理模型'}
{'arxiv_id': 'arXiv:2502.05836', 'title': 'LegalSeg: Unlocking the Structure of Indian Legal Judgments Through Rhetorical Role Classification', 'authors': 'Shubham Kumar Nigam, Tanmay Dubey, Govind Sharma, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya', 'link': 'https://arxiv.org/abs/2502.05836', 'abstract': 'In this paper, we address the task of semantic segmentation of legal documents through rhetorical role classification, with a focus on Indian legal judgments. We introduce LegalSeg, the largest annotated dataset for this task, comprising over 7,000 documents and 1.4 million sentences, labeled with 7 rhetorical roles. To benchmark performance, we evaluate multiple state-of-the-art models, including Hierarchical BiLSTM-CRF, TransformerOverInLegalBERT (ToInLegalBERT), Graph Neural Networks (GNNs), and Role-Aware Transformers, alongside an exploratory RhetoricLLaMA, an instruction-tuned large language model. Our results demonstrate that models incorporating broader context, structural relationships, and sequential sentence information outperform those relying solely on sentence-level features. Additionally, we conducted experiments using surrounding context and predicted or actual labels of neighboring sentences to assess their impact on classification accuracy. Despite these advancements, challenges persist in distinguishing between closely related roles and addressing class imbalance. Our work underscores the potential of advanced techniques for improving legal document understanding and sets a strong foundation for future research in legal NLP.', 'abstract_zh': '在本文中，我们通过论述角色分类来解决法律文件的语义分割任务，重点是印度法律判决。我们引入了LegalSeg，这是迄今为止最大的标注数据集，包含了超过7,000份文件和140万句话，并标注了7种论述角色。为了评估性能，我们评估了多个最先进的模型，包括层次双向长短期记忆-条件随机场（Hierarchical BiLSTM-CRF）、TransformOverInLegalBERT（ToInLegalBERT）、图神经网络（GNNs）和角色感知变压器，同时进行了一项探索性的RhetoricLLaMA实验，这是一种指令调优的大语言模型。我们的结果显示，结合更广泛背景、结构关系和句子序列信息的模型在性能上优于仅依赖句内特征的模型。此外，我们还通过使用邻近句子的上下文信息以及预测或实际标签，评估了它们对分类准确率的影响。尽管取得了这些进展，但在区分密切相关的角色以及解决类别不平衡问题上仍存在挑战。我们的工作强调了高级技术在提高法律文件理解方面的潜力，并为未来法律NLP的研究奠定了坚实的基础。', 'title_zh': 'LegalSeg：通过修辞角色分类解锁印度法律判决的结构'}
{'arxiv_id': 'arXiv:2502.05835', 'title': 'Contrastive Representation Distillation via Multi-Scale Feature Decoupling', 'authors': 'Cuipeng Wang, Tieyuan Chen, Haipeng Wang', 'link': 'https://arxiv.org/abs/2502.05835', 'abstract': "Knowledge distillation is a technique aimed at enhancing the performance of a smaller student network without increasing its parameter size by transferring knowledge from a larger, pre-trained teacher network. Previous approaches have predominantly focused on distilling global feature information while overlooking the importance of disentangling the diverse types of information embedded within different regions of the feature. In this work, we introduce multi-scale decoupling in the feature transfer process for the first time, where the decoupled local features are individually processed and integrated with contrastive learning. Moreover, compared to previous contrastive learning-based distillation methods, our approach not only reduces computational costs but also enhances efficiency, enabling performance improvements for the student network using only single-batch samples. Extensive evaluations on CIFAR-100 and ImageNet demonstrate our method's superiority, with some student networks distilled using our method even surpassing the performance of their pre-trained teacher networks. These results underscore the effectiveness of our approach in enabling student networks to thoroughly absorb knowledge from teacher networks.", 'abstract_zh': '知识蒸馏是一种技术，旨在通过从较大规模的预训练教师网络中转移知识来提高较小学生网络的性能，而不增加其参数大小。之前的大部分方法主要集中在蒸馏全局特征信息，而忽视了不同特征区域中嵌入的各种类型信息解耦的重要性。在本文中，我们首次在特征转移过程中引入多尺度解耦，即解耦的局部特征分别处理并结合对比学习。此外，与基于对比学习的知识蒸馏方法相比，我们的方法不仅降低了计算成本，还提高了效率，使得仅使用单批次样本即可提升学生网络的性能。我们在CIFAR-100和ImageNet上的广泛评估表明，我们的方法优于其他方法，某些使用我们方法蒸馏的学生网络甚至超过了其预训练教师网络的性能。这些结果突显了我们方法的有效性，使得学生网络能够充分吸收教师网络的知识。', 'title_zh': '多尺度特征解耦的对比表示蒸馏'}
{'arxiv_id': 'arXiv:2502.05832', 'title': 'Compressing Model with Few Class-Imbalance Samples: An Out-of-Distribution Expedition', 'authors': 'Tian-Shuang Wu, Shen-Huan Lyu, Ning Chen, Zhihao Qu, Baoliu Ye', 'link': 'https://arxiv.org/abs/2502.05832', 'abstract': 'In recent years, as a compromise between privacy and performance, few-sample model compression has been widely adopted to deal with limited data resulting from privacy and security concerns. However, when the number of available samples is extremely limited, class imbalance becomes a common and tricky problem. Achieving an equal number of samples across all classes is often costly and impractical in real-world applications, and previous studies on few-sample model compression have mostly ignored this significant issue. Our experiments comprehensively demonstrate that class imbalance negatively affects the overall performance of few-sample model compression methods. To address this problem, we propose a novel and adaptive framework named OOD-Enhanced Few-Sample Model Compression (OE-FSMC). This framework integrates easily accessible out-of-distribution (OOD) data into both the compression and fine-tuning processes, effectively rebalancing the training distribution. We also incorporate a joint distillation loss and a regularization term to reduce the risk of the model overfitting to the OOD data. Extensive experiments on multiple benchmark datasets show that our framework can be seamlessly incorporated into existing few-sample model compression methods, effectively mitigating the accuracy degradation caused by class imbalance.', 'abstract_zh': '近年来，作为一种在隐私与性能之间的折衷，少量样本模型压缩在处理由于隐私和安全问题导致的数据有限时被广泛采用。然而，当可用样本数量极其有限时，类别不平衡成为了一个常见的且棘手的问题。在实际应用中，实现各类别样本数量的完全平衡往往是代价高昂且不现实的。以往对少量样本模型压缩的研究大多忽略了这一重要问题。我们的实验全面展示了类别不平衡对少量样本模型压缩方法整体性能的负面影响。为了应对这一问题，我们提出了一种新颖且自适应的框架，名为OOD增强少量样本模型压缩（OE-FSMC）。该框架将易于获取的领域外（OOD）数据整合到压缩和微调过程中，有效重新平衡了训练分布。同时，我们还引入了一种联合蒸馏损失和正则化项，以减少模型过度拟合领域外数据的风险。在多个基准数据集上的广泛实验表明，我们的框架可以无缝集成到现有的少量样本模型压缩方法中，有效减轻由类别不平衡引起的准确率下降。', 'title_zh': '少数类别不平衡样本下的模型压缩：一种异常分布探索'}
{'arxiv_id': 'arXiv:2502.05827', 'title': 'HyGEN: Regularizing Negative Hyperedge Generation for Accurate Hyperedge Prediction', 'authors': 'Song Kyung Yu, Da Eun Lee, Yunyong Ko, Sang-Wook Kim', 'link': 'https://arxiv.org/abs/2502.05827', 'abstract': 'Hyperedge prediction is a fundamental task to predict future high-order relations based on the observed network structure. Existing hyperedge prediction methods, however, suffer from the data sparsity problem. To alleviate this problem, negative sampling methods can be used, which leverage non-existing hyperedges as contrastive information for model training. However, the following important challenges have been rarely studied: (C1) lack of guidance for generating negatives and (C2) possibility of producing false negatives. To address them, we propose a novel hyperedge prediction method, HyGEN, that employs (1) a negative hyperedge generator that employs positive hyperedges as a guidance to generate more realistic ones and (2) a regularization term that prevents the generated hyperedges from being false negatives. Extensive experiments on six real-world hypergraphs reveal that HyGEN consistently outperforms four state-of-the-art hyperedge prediction methods.', 'abstract_zh': '超节点预测是基于观测到的网络结构预测未来高阶关系的基本任务。现有的超节点预测方法面临数据稀疏性问题。为了解决这一问题，可以使用负采样方法，利用不存在的超节点作为对比信息来进行模型训练。然而，以下两个重要挑战很少被研究：(C1) 负样本生成缺少指导，(C2) 可能产生虚假负样本。为了解决这些问题，我们提出了一种名为HyGEN的新颖超节点预测方法，该方法采用了以下两个策略：(1) 一个负超节点生成器，利用正超节点作为指导生成更真实的负样本；(2) 一个正则化项，防止生成的超节点成为虚假负样本。在六个实际超图上的广泛实验表明，HyGEN 在性能上显著优于四种最先进的超节点预测方法。', 'title_zh': 'HyGEN: 正则化负超边生成以实现精确的超边预测'}
{'arxiv_id': 'arXiv:2502.05826', 'title': 'MindCraft: Revolutionizing Education through AI-Powered Personalized Learning and Mentorship for Rural India', 'authors': 'Arihant Bardia, Aayush Agrawal', 'link': 'https://arxiv.org/abs/2502.05826', 'abstract': 'MindCraft is a modern platform designed to revolutionize education in rural India by leveraging Artificial Intelligence (AI) to create personalized learning experiences, provide mentorship, and foster resource-sharing. In a country where access to quality education is deeply influenced by geography and socio economic status, rural students often face significant barriers in their educational journeys. MindCraft aims to bridge this gap by utilizing AI to create tailored learning paths, connect students with mentors, and enable a collaborative network of educational resources that transcends both physical and digital divides. This paper explores the challenges faced by rural students, the transformative potential of AI, and how MindCraft offers a scalable, sustainable solution for equitable education system. By focusing on inclusivity, personalized learning, and mentorship, MindCraft seeks to empower rural students, equipping them with the skills, knowledge, and opportunities needed to thrive in an increasingly digital world. Ultimately, MindCraft envisions a future in which technology not only bridges educational gaps but also becomes the driving force for a more inclusive and empowered society.', 'abstract_zh': 'MindCraft是一个现代平台，旨在通过利用人工智能（AI）来革新印度农村地区的教育。该平台通过创建个性化的学习体验、提供导师指导和促进资源共享，致力于打破地理和社会经济地位带来的教育障碍。在一个教育资源的质量深受地理位置和社会经济地位影响的国家，农村学生经常面临着显著的教育障碍。MindCraft计划通过利用AI来创建个性化学习路径、连接学生与导师，并建立一个跨越物理和数字鸿沟的教育资源协作网络，以此来弥合这些差距。本文探讨了农村学生的挑战、AI的潜在变革力量，以及MindCraft如何提供一种可扩展和可持续的解决方案以实现公平的教育体系。通过关注包容性、个性化学习和导师指导，MindCraft旨在赋能农村学生，使他们获得所需的知识和技能，以适应日益数字化的世界。最终，MindCraft希望建立一个未来，在这个未来中，技术不仅能够弥合教育差距，还能成为促进一个更加包容和平等的社会的驱动力。', 'title_zh': 'MindCraft：通过AI驱动的个性化学习和导师制度革新印度农村地区的教育'}
{'arxiv_id': 'arXiv:2502.05825', 'title': 'Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models', 'authors': 'Cheng Peng Huang, Hao-Yuan Chen', 'link': 'https://arxiv.org/abs/2502.05825', 'abstract': 'Large language models (LLMs) demonstrate strong capabilities in natural language processing but remain prone to hallucinations, generating factually incorrect or fabricated content. This issue undermines their reliability, particularly in high-stakes domains such as healthcare and legal advisory. To address this challenge, we propose Delta, an inference-time method that reduces hallucinations without requiring model retraining or additional data. Delta works by randomly masking parts of the input prompt and contrasting the output distributions for the original and masked inputs, effectively suppressing hallucinations through inference-only computations. We evaluate Delta on context-rich question-answering benchmarks, achieving absolute improvements of approximately 3 and 6 percentage points on SQuAD v1.1 and v2, respectively, and 7 and 2 percentage points on TriviaQA and Natural Questions under-sampling decoding. Delta also improves the no-answer exact match score on SQuAD v2 by over ten percentage points, demonstrating its effectiveness in mitigating hallucinations arising from contextual ambiguity. These results highlight Delta as a computationally efficient and scalable approach for improving the reliability of LLMs in real-world applications.', 'abstract_zh': '大规模语言模型（LLMs）在自然语言处理方面展现出强大的能力，但仍然容易出现幻觉，即生成事实错误或捏造的内容。这一问题损害了其可靠性，尤其是在医疗保健和法律咨询等高风险领域。为应对这一挑战，我们提出了Delta，这是一种在推断时减少幻觉的方法，无需重新训练模型或获取额外数据。Delta 通过随机遮盖输入提示的部分内容，并对比原始输入和遮盖输入的输出分布，有效地通过仅计算推理过程来抑制幻觉。\n\n我们在上下文丰富的问答基准上评估了Delta，分别在SQuAD v1.1 和 v2 上取得了约3和6个百分点的绝对改进，以及在TriviaQA 和 Natural Questions 下采样解码中分别取得了7和2个百分点的改进。Delta 还在SQuAD v2 上的无答案准确匹配分数上提高了超过10个百分点，表明其在缓解由于上下文歧义导致的幻觉方面具有有效性。这些结果突显了Delta 作为提高LLMs在实际应用中可靠性的一种计算效率高且可扩展的方法的有效性。', 'title_zh': 'Delta-对比解码减轻了大型语言模型中的文本幻觉问题'}
{'arxiv_id': 'arXiv:2502.05795', 'title': 'The Curse of Depth in Large Language Models', 'authors': 'Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu', 'link': 'https://arxiv.org/abs/2502.05795', 'abstract': 'In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.', 'abstract_zh': '在本文中，我们引入了“深度之诅咒”这一概念，以揭示、解释并解决现代大规模语言模型（Large Language Models, LLMs）中观察到的一个现象，即近半数的层数比预期中更不有效。我们首先确认了这一现象在目前最流行的LLM家族，如Llama、Mistral、DeepSeek和Qwen中普遍存在。通过理论和实证分析，我们发现导致LLM中深层无效的原因在于预层归一化（Pre-Layer Normalization, Pre-LN）的广泛应用。虽然Pre-LN稳定了Transformer LLM的训练，但其输出方差随着模型深度的增加呈指数增长，从而导致深层Transformer块的梯度几乎退化为单位矩阵，对训练几乎没有贡献。为了解决这一训练问题，我们提出了一种层归一化缩放（LayerNorm Scaling）方法，通过将层归一化的输出方差反比于其深度的平方根进行缩放。这一简单的修改抑制了深层Transformer层输出方差的爆炸，提高了它们的贡献。我们的实验结果显示，在从130M到1B的不同模型规模下，与Pre-LN相比，层归一化缩放显著提升了LLM的预训练性能，并且这种改进也可以无缝地应用到监督微调中。这些收益归因于层归一化缩放允许深层层在训练中更有效地贡献。', 'title_zh': '大型语言模型中的深度诅咒'}
{'arxiv_id': 'arXiv:2502.05788', 'title': 'EPBC-YOLOv8: An efficient and accurate improved YOLOv8 underwater detector based on an attention mechanism', 'authors': 'Xing Jiang, Xiting Zhuang, Jisheng Chen, Jian Zhang', 'link': 'https://arxiv.org/abs/2502.05788', 'abstract': "In this study, we enhance underwater target detection by integrating channel and spatial attention into YOLOv8's backbone, applying Pointwise Convolution in FasterNeXt for the FasterPW model, and leveraging Weighted Concat in a BiFPN-inspired WFPN structure for improved cross-scale connections and robustness. Utilizing CARAFE for refined feature reassembly, our framework addresses underwater image degradation, achieving mAP at 0.5 scores of 76.7 percent and 79.0 percent on URPC2019 and URPC2020 datasets, respectively. These scores are 2.3 percent and 0.7 percent higher than the original YOLOv8, showcasing enhanced precision in detecting marine organisms.", 'abstract_zh': '在本研究中，我们通过将通道注意力和空间注意力整合到YOLOv8的主干网络中，利用Pointwise Convolution在FasterNeXt模型中构建FasterPW模型，并采用受BiFPN启发的WFPN结构中的加权拼接技术，增强了多尺度连接和鲁棒性。利用CARAFE进行精细特征重组，我们的框架解决了水下图像退化的问题，分别在URPC2019和URPC2020数据集上实现了0.5 IOU阈值下的mAP为76.7%和79.0%。这些得分比原始的YOLOv8分别提高了2.3%和0.7%，显示了检测海洋生物时更高的精度。', 'title_zh': 'EPBC-YOLOv8：基于注意机制的高效且精确的改进YOLOv8水下检测器'}
{'arxiv_id': 'arXiv:2502.05783', 'title': 'WatchGuardian: Enabling User-Defined Personalized Just-in-Time Intervention on Smartwatch', 'authors': 'Ying Lei, Yancheng Cao, Will Wang, Yuanzhe Dong, Changchang Yin, Weidan Cao, Ping Zhang, Jingzhen Yang, Bingsheng Yao, Yifan Peng, Chunhua Weng, Randy Auerbach, Lena Mamykina, Dakuo Wang, Yuntao Wang, Xuhai Xu', 'link': 'https://arxiv.org/abs/2502.05783', 'abstract': 'While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. For the model to detect new actions based on limited new data samples, we developed a few-shot learning pipeline that finetuned a pre-trained inertial measurement unit (IMU) model on public hand-gesture datasets. We then designed a data augmentation and synthesis process to train additional classification layers for customization. Our offline evaluation with 26 participants showed that with three, five, and ten examples, our approach achieved an average accuracy of 76.8%, 84.7%, and 87.7%, and an F1 score of 74.8%, 84.2%, and 87.2% We then conducted a four-hour intervention study to compare WatchGuardian against a rule-based intervention. Our results demonstrated that our system led to a significant reduction by 64.0 +- 22.6% in undesirable actions, substantially outperforming the baseline by 29.0%. Our findings underscore the effectiveness of a customizable, AI-driven JITI system for individuals in need of behavioral intervention in personal undesirable actions. We envision that our work can inspire broader applications of user-defined personalized intervention with advanced AI solutions.', 'abstract_zh': '尽管即刻干预（JITI）已经在有效针对常见健康行为方面取得了进展，个体仍然需要针对那些会影响其身体、心理健康和社会福祉的个人不良行为进行独特的干预。我们提出了WatchGuardian，一种基于智能手表的即刻干预系统，能够使用户通过少量样本自定义干预措施。为使该模型能够根据有限的新数据样本检测新动作，我们开发了一个少样本学习流程，该流程通过在公开的手势数据集上微调预训练的惯性测量单元（IMU）模型来实现。我们还设计了一个数据增强和合成过程，以训练额外的分类层进行定制。在线下评估中，我们让26名参与者使用了该系统，结果显示，在使用三个、五个和十个示例的情况下，我们的方法分别达到了76.8%、84.7%和87.7%的平均准确率，及74.8%、84.2%和87.2%的F1评分。我们还进行了一项为期四小时的干预研究，将WatchGuardian与基于规则的干预进行了比较。研究结果表明，我们的系统在不良行为减少方面达到了64.0%±22.6%的显著降低，相较于基线提高了29.0%。我们的发现突显了个性化、以AI驱动的即刻干预系统在需要行为干预的个体中的有效性。我们设想，我们的工作可以激发更多使用高级AI解决方案进行用户定义个性化干预的应用。', 'title_zh': 'WatchGuardian： Enables 用户自定义个性化即时干预功能的智能手表'}
{'arxiv_id': 'arXiv:2502.05777', 'title': 'Predictive Crash Analytics for Traffic Safety using Deep Learning', 'authors': 'Karthik Sivakoti', 'link': 'https://arxiv.org/abs/2502.05777', 'abstract': 'Traditional automated crash analysis systems heavily rely on static statistical models and historical data, requiring significant manual interpretation and lacking real-time predictive capabilities. This research presents an innovative approach to traffic safety analysis through the integration of ensemble learning methods and multi-modal data fusion for real-time crash risk assessment and prediction. Our primary contribution lies in developing a hierarchical severity classification system that combines spatial-temporal crash patterns with environmental conditions, achieving significant improvements over traditional statistical approaches. The system demonstrates a Mean Average Precision (mAP) of 0.893, representing a 15% improvement over current state-of-the-art methods (baseline mAP: 0.776). We introduce a novel feature engineering technique that integrates crash location data with incident reports and weather conditions, achieving 92.4% accuracy in risk prediction and 89.7% precision in hotspot identification. Through extensive validation using 500,000 initial crash records filtered to 59,496 high-quality samples, our solution shows marked improvements in both prediction accuracy and computational efficiency. Key innovations include a robust data cleaning pipeline, adaptive feature generation, and a scalable real-time prediction system capable of handling peak loads of 1,000 concurrent requests while maintaining sub-100ms response times.', 'abstract_zh': '传统的自动事故分析系统严重依赖静态统计模型和历史数据，需要大量的手动解释，并且缺乏实时预测能力。本研究提出了一种新的方法，通过集成集成学习方法和多模态数据融合，实现对实时事故风险的评估与预测，从而提高交通安全分析。本研究的主要贡献在于开发了一种基于空间-时间事故模式与环境条件的分层严重性分类系统，显著超过了传统的统计方法。该系统在平均准确率（mAP）上的表现为0.893，相比当前最先进的方法（基线mAP：0.776），提高了15%。我们引入了一种新颖的功能工程技术，将事故地点数据与事件报告和天气条件相结合，在风险预测准确性上达到了92.4%，在热点识别的精确度上达到了89.7%。通过使用50万个初始事故记录筛选出59,496个高质量样本进行广泛验证，我们的解决方案在预测准确性和计算效率方面均表现出显著的改进。关键创新点包括稳健的数据处理流水线、自适应特征生成以及可处理 peak 负载（1,000个并发请求）并维持亚100ms响应时间的可扩展实时预测系统。', 'title_zh': '使用深度学习进行交通事故预测分析以提升交通安全'}
{'arxiv_id': 'arXiv:2502.05773', 'title': 'PIPA: Preference Alignment as Prior-Informed Statistical Estimation', 'authors': 'Junbo Li, Zhangyang Wang, Qiang Liu', 'link': 'https://arxiv.org/abs/2502.05773', 'abstract': 'Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding.\nIn this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\\sim10\\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms.', 'abstract_zh': '以下是翻译后的论文内容或标题，符合学术规范：\n\n对于语言模型（如Direct Preference Optimization, DPO）来说，离线偏好对齐因其有效性和简便性而受到青睐，可以消除昂贵的强化学习需求。尽管已经为不同的数据设置开发了多种离线算法，但它们仍然缺乏一个统一的理解框架。\n\n在本研究中，我们提出了 Prior-Informed Preference Alignment (PIPA)，这是一种无强化学习的统一概率框架，将语言模型偏好对齐问题表述为具有先验约束的最大似然估计（MLE）问题。该方法有效地处理了配对和非配对数据以及答案和步骤级别的注释。我们证明，DPO和KTO是该框架内的具有不同先验约束的特殊情况。通过整合不同类型的存在信息，我们开发了两种PIPA的变体：PIPA-M和PIPA-N。两种算法在所有配置下均在GSM8K和MATH基准测试中表现出约3%至10%的性能增强，并且在无需额外训练或计算成本的情况下达到了这些改进。', 'title_zh': 'PIPA：基于先验信息的统计估计偏好对齐'}
{'arxiv_id': 'arXiv:2502.05772', 'title': 'Effective Black-Box Multi-Faceted Attacks Breach Vision Large Language Model Guardrails', 'authors': 'Yijun Yang, Lichao Wang, Xiao Yang, Lanqing Hong, Jun Zhu', 'link': 'https://arxiv.org/abs/2502.05772', 'abstract': "Vision Large Language Models (VLLMs) integrate visual data processing, expanding their real-world applications, but also increasing the risk of generating unsafe responses. In response, leading companies have implemented Multi-Layered safety defenses, including alignment training, safety system prompts, and content moderation. However, their effectiveness against sophisticated adversarial attacks remains largely unexplored. In this paper, we propose MultiFaceted Attack, a novel attack framework designed to systematically bypass Multi-Layered Defenses in VLLMs. It comprises three complementary attack facets: Visual Attack that exploits the multimodal nature of VLLMs to inject toxic system prompts through images; Alignment Breaking Attack that manipulates the model's alignment mechanism to prioritize the generation of contrasting responses; and Adversarial Signature that deceives content moderators by strategically placing misleading information at the end of the response. Extensive evaluations on eight commercial VLLMs in a black-box setting demonstrate that MultiFaceted Attack achieves a 61.56% attack success rate, surpassing state-of-the-art methods by at least 42.18%.", 'abstract_zh': '视觉大型语言模型（VLLMs）将视觉数据处理融入其中，扩展了其在实际应用中的范围，但也增加了生成不安全响应的风险。为此，领先公司实施了多层次的安全防御措施，包括对齐训练、安全系统提示和内容审核。然而，这些防御措施对高级对手攻击的有效性尚未得到充分探索。在本文中，我们提出了多维度攻击（MultiFaceted Attack），这是一种新型攻击框架，旨在系统地绕过VLLMs中的多层次防御。该框架包括三个互补的攻击维度：视觉攻击（利用VLLMs的跨模态特性，通过图像注入有毒系统提示）；对齐破坏攻击（操纵模型的对齐机制，优先生成对比响应）；以及对抗签名（通过在响应末尾战略性地放置误导性信息，欺骗内容审核人员）。在黑盒设置下对八个商用VLLMs的广泛评估表明，多维度攻击实现了61.56%的攻击成功率，超越了最先进的方法至少42.18%。', 'title_zh': '有效的黑盒多维度攻击突破视觉大型语言模型防护边界'}
{'arxiv_id': 'arXiv:2502.05749', 'title': 'UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control', 'authors': 'Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi', 'link': 'https://arxiv.org/abs/2502.05749', 'abstract': "Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches frequently produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified framework for diffusion bridges based on Stochastic Optimal Control (SOC). UniDB formulates the problem through an SOC-based optimization and derives a closed-form solution for the optimal controller, thereby unifying and generalizing existing diffusion bridge models. We demonstrate that existing diffusion bridges employing Doob's $h$-transform constitute a special case of our framework, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. Notably, UniDB seamlessly integrates with existing diffusion bridge models, requiring only minimal code modifications. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework. Our code is available at this https URL.", 'abstract_zh': '以下是翻译成中文的内容，符合学术规范：\n\n近年来，扩散桥模型的最新进展利用Doob的$h$-变换在不同分布之间建立固定端点，展示了在图像转换和恢复任务中的有前途的结果。然而，这些方法经常会产生模糊或过度平滑的图像细节，并且缺乏全面的理论基础来解释这些不足。为了解决这些局限性，我们提出了一种基于随机最优控制（SOC）的统一框架——UniDB。UniDB 通过基于SOC的优化问题进行建模，并推导出最优控制器的闭式解，从而统一并推广了现有的扩散桥模型。我们证明，现有的使用Doob的$h$-变换的扩散桥模型构成了我们框架的一个特例，当SOC成本函数中的终端惩罚系数趋于无穷大时，这些模型才会出现。通过引入可调的终端惩罚系数，UniDB 达成了对控制成本和终端惩罚之间的最佳平衡，显著改善了细节保留和输出质量。值得注意的是，UniDB 可无缝集成到现有的扩散桥模型中，仅需少量代码修改即可。在多种图像恢复任务上的广泛实验验证了所提框架的优越性和适应性。我们的代码可在此处访问：this https URL。', 'title_zh': 'UniDB：一种基于随机最优控制的统一扩散桥梁框架'}
{'arxiv_id': 'arXiv:2502.05740', 'title': 'RECOVER: Designing a Large Language Model-based Remote Patient Monitoring System for Postoperative Gastrointestinal Cancer Care', 'authors': 'Ziqi Yang, Yuxuan Lu, Jennifer Bagdasarian, Vedant Das Swain, Ritu Agarwal, Collin Campbell, Waddah Al-Refaire, Jehan El-Bayoumi, Guodong Gao, Dakuo Wang, Bingsheng Yao, Nawar Shara', 'link': 'https://arxiv.org/abs/2502.05740', 'abstract': 'Cancer surgery is a key treatment for gastrointestinal (GI) cancers, a group of cancers that account for more than 35% of cancer-related deaths worldwide, but postoperative complications are unpredictable and can be life-threatening. In this paper, we investigate how recent advancements in large language models (LLMs) can benefit remote patient monitoring (RPM) systems through clinical integration by designing RECOVER, an LLM-powered RPM system for postoperative GI cancer care. To closely engage stakeholders in the design process, we first conducted seven participatory design sessions with five clinical staff and interviewed five cancer patients to derive six major design strategies for integrating clinical guidelines and information needs into LLM-based RPM systems. We then designed and implemented RECOVER, which features an LLM-powered conversational agent for cancer patients and an interactive dashboard for clinical staff to enable efficient postoperative RPM. Finally, we used RECOVER as a pilot system to assess the implementation of our design strategies with four clinical staff and five patients, providing design implications by identifying crucial design elements, offering insights on responsible AI, and outlining opportunities for future LLM-powered RPM systems.', 'abstract_zh': '癌症手术是消化道（GI）癌症治疗的关键手段，而GI癌症占全球癌症相关死亡人数的35%以上。然而，术后并发症不可预测且可能危及生命。本文探讨了最近在大规模语言模型（LLMs）方面的进展如何通过临床集成优势于远程患者监测（RPM）系统，设计了一个基于LLM的RPM系统——RECOVER，用于术后GI癌症护理。为了在设计过程中紧密参与利益相关者的意见，我们首先与五名临床工作人员进行了七次参与式设计会，采访了五名癌症患者，以提炼出六项主要的设计策略，这些策略旨在将临床指南和信息需求整合到基于LLM的RPM系统中。随后，我们设计并实现了RECOVER系统，该系统具备一种基于LLM的对话代理，供癌症患者使用，并提供互动仪表板，使临床工作人员能够实现高效的术后RPM。最后，我们使用RECOVER作为试点系统评估了我们设计策略的实施情况，涉及四名临床工作人员和五名患者，并通过识别关键设计元素、提供负责任的AI见解以及概述未来LLM驱动的RPM系统的机会来为设计提供启示。', 'title_zh': 'RECOVER：设计一种基于大型语言模型的远程患者监测系统，用于术后胃肠癌护理'}
{'arxiv_id': 'arXiv:2502.05739', 'title': 'Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning', 'authors': 'Ruotong Geng, Mingyang Geng, Shangwen Wang, Haotian Wang, Zhipeng Lin, Dezun Dong', 'link': 'https://arxiv.org/abs/2502.05739', 'abstract': 'Large Language Models for Code (LLMs4Code) excel at code generation tasks, yielding promise to release developers from huge software development burdens. Nonetheless, these models have been shown to suffer from the significant privacy risks due to the potential leakage of sensitive information embedded during training, known as the memorization problem. Addressing this issue is crucial for ensuring privacy compliance and upholding user trust, but till now there is a dearth of dedicated studies in the literature that focus on this specific direction. Recently, machine unlearning has emerged as a promising solution by enabling models to "forget" sensitive information without full retraining, offering an efficient and scalable approach compared to traditional data cleaning methods. In this paper, we empirically evaluate the effectiveness of unlearning techniques for addressing privacy concerns in this http URL, we investigate three state-of-the-art unlearning algorithms and three well-known open-sourced LLMs4Code, on a benchmark that takes into consideration both the privacy data to be forgotten as well as the code generation capabilites of these models. Results show that it is feasible to mitigate the privacy concerns of LLMs4Code through machine unlearning while maintain their code generation capabilities at the same time. We also dissect the forms of privacy protection/leakage after unlearning and observe that there is a shift from direct leakage to indirect leakage, which underscores the need for future studies addressing this risk.', 'abstract_zh': '大型语言模型在代码生成任务方面表现出色，有望减轻开发者在软件开发过程中的巨大负担。然而，这些模型已经被证明存在重大隐私风险，这源于训练过程中嵌入的敏感信息泄露问题，即记忆问题。解决这一问题对于确保隐私合规和维护用户信任至关重要，但遗憾的是，迄今为止文献中极少有专注于此特定方向的专门研究。最近，机器遗忘作为一种有前途的解决方案受到关注，它使模型能够在无需重新训练完整数据集的情况下“忘记”敏感信息，相比传统的数据清理方法，这种方式具有高效和可扩展的优势。在本文中，我们通过实证方法评估了遗忘技术在解决LLMs4Code隐私问题方面的作用。我们研究了三种最先进的遗忘算法和三种常见的开源LLMs4Code模型，采用一个综合考虑需要遗忘的隐私数据和这些模型代码生成能力的基准测试。结果显示，通过机器遗忘可以减轻LLMs4Code的隐私问题，同时保持其代码生成能力。我们还分析了遗忘后的隐私保护/泄露形式，并观察到直接泄露转为间接泄露的现象，这表明未来需要进一步研究这一风险。', 'title_zh': '通过机器遗忘技术减轻LLM4Code中敏感信息泄露问题'}
{'arxiv_id': 'arXiv:2502.05724', 'title': 'Rethinking Link Prediction for Directed Graphs', 'authors': 'Mingguo He, Yuhe Guo, Yanping Zheng, Zhewei Wei, Stephan Günnemann, Xiaokui Xiao', 'link': 'https://arxiv.org/abs/2502.05724', 'abstract': 'Link prediction for directed graphs is a crucial task with diverse real-world applications. Recent advances in embedding methods and Graph Neural Networks (GNNs) have shown promising improvements. However, these methods often lack a thorough analysis of embedding expressiveness and suffer from ineffective benchmarks for a fair evaluation. In this paper, we propose a unified framework to assess the expressiveness of existing methods, highlighting the impact of dual embeddings and decoder design on performance. To address limitations in current experimental setups, we introduce DirLinkBench, a robust new benchmark with comprehensive coverage and standardized evaluation. The results show that current methods struggle to achieve strong performance on the new benchmark, while DiGAE outperforms others overall. We further revisit DiGAE theoretically, showing its graph convolution aligns with GCN on an undirected bipartite graph. Inspired by these insights, we propose a novel spectral directed graph auto-encoder SDGAE that achieves SOTA results on DirLinkBench. Finally, we analyze key factors influencing directed link prediction and highlight open challenges.', 'abstract_zh': '有向图中的链接预测是一项具有广泛实际应用的重要任务。近年来，嵌入方法和图神经网络（GNNs）的进步已经显示出有希望的改进。然而，这些方法往往缺乏对嵌入表达性的全面分析，并且缺乏有效的基准，不利于公平评估。在本文中，我们提出了一种统一框架来评估现有方法的表达性，突显了双嵌入和解码器设计对性能的影响。为了解决当前实验设置中的局限性，我们引入了DirLinkBench这一新的鲁棒基准，该基准具有全面的覆盖范围和标准化的评估标准。结果表明，当前方法在新基准上难以达到强性能，而DiGAE在总体上表现最佳。我们进一步从理论上重新审视了DiGAE，展示了其图卷积在无向二部图上的行为类似于GCN。受这些见解的启发，我们提出了一个新颖的频谱有向图自编码器SDGAE，在DirLinkBench上达到了最先进的性能。最后，我们分析了影响有向链接预测的关键因素，并指出了开放性挑战。', 'title_zh': '重新思考有向图中的链接预测'}
{'arxiv_id': 'arXiv:2502.05720', 'title': 'Pareto-Optimality, Smoothness, and Stochasticity in Learning-Augmented One-Max-Search', 'authors': 'Ziyad Benomar, Lorenzo Croissant, Vianney Perchet, Spyros Angelopoulos', 'link': 'https://arxiv.org/abs/2502.05720', 'abstract': 'One-max search is a classic problem in online decision-making, in which a trader acts on a sequence of revealed prices and accepts one of them irrevocably to maximise its profit. The problem has been studied both in probabilistic and in worst-case settings, notably through competitive analysis, and more recently in learning-augmented settings in which the trader has access to a prediction on the sequence. However, existing approaches either lack smoothness, or do not achieve optimal worst-case guarantees: they do not attain the best possible trade-off between the consistency and the robustness of the algorithm. We close this gap by presenting the first algorithm that simultaneously achieves both of these important objectives. Furthermore, we show how to leverage the obtained smoothness to provide an analysis of one-max search in stochastic learning-augmented settings which capture randomness in both the observed prices and the prediction.', 'abstract_zh': '一最大值搜索是一类经典在线决策问题，其中交易者根据连续揭示的价格作出决策，并不可撤销地选择一个价格以最大化其收益。该问题已在概率性和最坏情况设定下得到了研究，特别是在通过竞争分析进行研究时更为突出。近年来，该问题也在预测辅助的学习增强设定下得到研究，即交易者可以访问对该价格序列的预测。然而，现有的方法要么缺乏平滑性，要么不能达到最优的最坏情况保证：它们无法实现算法的稳定性和鲁棒性之间的最佳权衡。我们通过提出一种同时实现这两个重要目标的新算法来弥补这一差距。此外，我们展示了如何利用获得的平滑性，对该价格序列中的随机性和预测中的随机性都加以捕捉的一最大值搜索进行分析。', 'title_zh': '帕累托优化、光滑性与随机性在学习增强的一-max-搜索中的应用'}
{'arxiv_id': 'arXiv:2502.05719', 'title': 'Extended Histogram-based Outlier Score (EHBOS)', 'authors': 'Tanvir Islam', 'link': 'https://arxiv.org/abs/2502.05719', 'abstract': 'Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role.', 'abstract_zh': '基于直方图的异常评分（HBOS）是一种广泛应用的异常检测方法，以其计算效率高和简单而闻名。然而，它假设特征独立的假设限制了其在特征之间存在关键交互作用的数据集中的异常检测能力。本文中，我们提出了一种扩展的基于直方图的异常评分（EHBOS），该方法通过引入二维直方图来捕捉特征对之间的依赖性，从而增强了HBOS的功能。这种扩展使EHBOS能够识别HBOS未能检测到的情境性和依赖驱动的异常。我们在17个基准数据集上评估了EHBOS，展示了其在多种异常检测情境下的有效性与稳健性。在某些数据集上，EHBOS在ROC AUC方面的性能优于HBOS，并且在特征交互作用对异常结构定义至关重要的数据集中，取得了显著的改进。这些结果表明，EHBOS可以作为HBOS的有效扩展，能够建模复杂的特征依赖性。EHBOS为异常检测提供了一种强大的新工具，特别适用于上下文或关系异常在数据集中起重要作用的情况。', 'title_zh': '基于扩展直方图的离群点分数（EHBOS）'}
{'arxiv_id': 'arXiv:2502.05714', 'title': 'Proving the Coding Interview: A Benchmark for Formally Verified Code Generation', 'authors': 'Quinn Dougherty, Ronak Mehta', 'link': 'https://arxiv.org/abs/2502.05714', 'abstract': 'We introduce the Formally Verified Automated Programming Progress Standards, or FVAPPS, a benchmark of 4715 samples for writing programs and proving their correctness, the largest formal verification benchmark, including 1083 curated and quality controlled samples. Previously, APPS provided a benchmark and dataset for programming puzzles to be completed in Python and checked against unit tests, of the kind seen in technical assessments in the software engineering industry. Building upon recent approaches for benchmarks in interactive theorem proving, we generalize the unit tests to Lean 4 theorems given without proof (i.e., using Lean\'s "sorry" keyword). On the 406 theorems of 100 randomly selected samples, Sonnet correctly proves 30% and Gemini correctly proves 18%. We challenge the machine learning and program synthesis communities to solve both each general purpose programming problem and its associated correctness specifications. The benchmark is available at this https URL.', 'abstract_zh': '我们介绍了形式化验证自动编程进展标准（Formally Verified Automated Programming Progress Standards, 或 FVAPPS），这是一个包含4715个程序编写和证明其正确性的样本基准，是迄今为止最大的形式化验证基准，包括1083个精心策划和质量控制的样本。此前，APPS 提供了一个基准和数据集，该集合包含用于用 Python 完成编程谜题并通过单元测试检查的样本，类似于软件工程行业技术评估中的测试用例。在此基础上，我们借鉴了交互式定理证明领域中基准测试的最新方法，将单元测试扩展为 Lean 4 包含定理但未给出证明的定理（即，使用 Lean 的 "sorry" 关键字）。在随机选择的100个样本的406个定理中，Sonnet 正确证明了30%，Gemini 正确证明了18%。我们挑战机器学习和程序合成社区解决每项通用编程问题及其相应的正确性规范。此基准测试可在以下链接获得：**[提供链接]**。', 'title_zh': '证明编码面试：形式验证代码生成的标准基准'}
{'arxiv_id': 'arXiv:2502.05713', 'title': '4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis', 'authors': 'An Zhao, Moucheng Xu, Ahmed H. Shahin, Wim Wuyts, Mark G. Jones, Joseph Jacob, Daniel C. Alexander', 'link': 'https://arxiv.org/abs/2502.05713', 'abstract': 'Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.', 'abstract_zh': '理解疾病的进展轨迹对于早期诊断和有效的治疗规划至关重要。尤其是在像特发性肺纤维化（IPF）这样的威胁生命的疾病中，这种理解尤为重要。IPF是一种慢性、渐进性的肺部疾病，其预后与许多癌症相似。计算机断层扫描（CT）成像已被确立为诊断IPF的可靠工具。准确预测早期IPF患者的未来CT扫描结果有助于制定更好的治疗策略，从而提高生存结果。本文中，我们提出了一种四维向量量化生成对抗网络（4D-VQ-GAN）模型，该模型能够生成IPF患者在任意时间点的逼真CT体积。该模型采用两阶段训练方式。第一阶段，采用3D-VQ-GAN进行CT体积重建训练；第二阶段，采用基于神经常微分方程（ODE）的时间模型来捕捉第一阶段编码器生成的量化嵌入的时序动态。我们根据不同配置评估了该模型生成纵向CT扫描的效果，并与真实数据的基准数据进行了定量和定性的比较。为进一步验证，我们使用来自生成CT扫描的影像生物标志物进行了生存分析，并获得的结果与来自真实CT扫描的生物标志物得出的C指数相当。生存分析结果展示了生成纵向CT扫描的潜在临床效用，表明它们可以可靠地预测生存结果。', 'title_zh': '4D VQ-GAN：合成任意时间点的医学影像，用于特发性肺纤维化个性化疾病进展建模'}
{'arxiv_id': 'arXiv:2502.05704', 'title': 'Rethinking Word Similarity: Semantic Similarity through Classification Confusion', 'authors': 'Kaitlyn Zhou, Haishan Gao, Sarah Chen, Dan Edelstein, Dan Jurafsky, Chen Shani', 'link': 'https://arxiv.org/abs/2502.05704', 'abstract': 'Word similarity has many applications to social science and cultural analytics tasks like measuring meaning change over time and making sense of contested terms. Yet traditional similarity methods based on cosine similarity between word embeddings cannot capture the context-dependent, asymmetrical, polysemous nature of semantic similarity. We propose a new measure of similarity, Word Confusion, that reframes semantic similarity in terms of feature-based classification confusion. Word Confusion is inspired by Tversky\'s suggestion that similarity features be chosen dynamically. Here we train a classifier to map contextual embeddings to word identities and use the classifier confusion (the probability of choosing a confounding word c instead of the correct target word t) as a measure of the similarity of c and t. The set of potential confounding words acts as the chosen features. Our method is comparable to cosine similarity in matching human similarity judgments across several datasets (MEN, WirdSim353, and SimLex), and can measure similarity using predetermined features of interest. We demonstrate our model\'s ability to make use of dynamic features by applying it to test a hypothesis about changes in the 18th C. meaning of the French word "revolution" from popular to state action during the French Revolution. We hope this reimagining of semantic similarity will inspire the development of new tools that better capture the multi-faceted and dynamic nature of language, advancing the fields of computational social science and cultural analytics and beyond.', 'abstract_zh': '词语相似性在社会科学研究和文化分析任务中具有广泛的应用，如衡量意义随时间的变化以及理解有争议词语的意义。然而，基于词嵌入余弦相似性的传统相似性方法难以捕捉到语境依赖性、不对称性和多义性的特征。我们提出了一种新的相似性度量方法——词混叠（Word Confusion），该方法从基于特征的分类混淆角度重新定义了语义相似性。词混叠受到Tversky建议的启发，即相似性特征应动态选择。在这里，我们训练一个分类器将上下文嵌入映射到词语身份，并使用分类器的混淆概率（即选择混淆词c而非正确目标词t的概率）作为c和t相似性的衡量标准。潜在的混淆词集合充当了选定的特征。该方法在多个数据集（MEN、WirdSim353和SimLex）上与余弦相似性具有可比性，能够使用感兴趣的预定义特征来测量相似性。我们通过将其应用于验证18世纪法国词语“革命”从大众行动到政府行动意义变化的假设，展示了模型利用动态特征的能力。我们希望这种对语义相似性的重新定义能够激发开发能够更好地捕捉语言多层面和动态性的新工具，推动计算社会科学研究和文化分析等领域的发展。', 'title_zh': '重塑词相似度：通过分类混淆实现语义相似度'}
{'arxiv_id': 'arXiv:2502.05699', 'title': 'Context information can be more important than reasoning for time series forecasting with a large language model', 'authors': 'Janghoon Yang', 'link': 'https://arxiv.org/abs/2502.05699', 'abstract': 'With the evolution of large language models (LLMs), there is growing interest in leveraging LLMs for time series tasks. In this paper, we explore the characteristics of LLMs for time series forecasting by considering various existing and proposed prompting techniques. Forecasting for both short and long time series was evaluated. Our findings indicate that no single prompting method is universally applicable. It was also observed that simply providing proper context information related to the time series, without additional reasoning prompts, can achieve performance comparable to the best-performing prompt for each case. From this observation, it is expected that providing proper context information can be more crucial than a prompt for specific reasoning in time series forecasting. Several weaknesses in prompting for time series forecasting were also identified. First, LLMs often fail to follow the procedures described by the prompt. Second, when reasoning steps involve simple algebraic calculations with several operands, LLMs often fail to calculate accurately. Third, LLMs sometimes misunderstand the semantics of prompts, resulting in incomplete responses.', 'abstract_zh': '随着大型语言模型（LLMs）的发展，人们越来越关注利用LLMs进行时间序列任务。本文通过考虑各种现有的和提出的提示技术，探讨了LLMs在时间序列预测中的特性。我们不仅评估了短期时间序列的预测，还评估了长期时间序列的预测。研究结果表明，并不存在一种万能的提示方法适用于所有情况。我们还观察到，仅提供与时间序列相关的信息而无需额外的推理提示，就可以达到每个案例中性能最佳的提示效果。从这一观察来看，提供适当上下文信息可能比具体推理提示更为关键。此外，我们还指出了一些时间序列预测中的提示方法弱点。首先，LLMs经常无法遵循提示中描述的步骤。其次，当推理步骤涉及多个操作数的简单代数计算时，LLMs往往无法准确计算。第三，LLMs有时会误解提示的语义，导致回答不完整。', 'title_zh': '对于时间序列预测，大规模语言模型中的上下文信息可能比推理更加重要'}
{'arxiv_id': 'arXiv:2502.05695', 'title': 'Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks', 'authors': 'Zijiang Yan, Jianhua Pei, Hongda Wu, Hina Tabassum, Ping Wang', 'link': 'https://arxiv.org/abs/2502.05695', 'abstract': 'This paper proposes a novel framework for real-time adaptive-bitrate video streaming by integrating latent diffusion models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional constant bitrate streaming (CBS) and adaptive bitrate streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While it keeps B-frames and P-frames as adjustment metadata to ensure efficient video reconstruction at the user side, the proposed framework is complemented with the most state-of-the-art denoising and video frame interpolation (VFI) techniques. These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.', 'abstract_zh': '本文提出了一种新的框架，通过将潜在扩散模型（LDMs）整合到FFmpeg技术中，实现了实时自适应比特率视频流传输。该解决方案解决了传统固定比特率流传输（CBS）和自适应比特率流传输（ABS）中高带宽使用、存储效率低以及用户体验（QoE）下降的挑战。该提出的方案利用LDMs将I-帧压缩到潜在空间，提供显著的存储和语义传输节省，同时不会牺牲高质量视觉效果。虽然保留了B-帧和P-帧作为调整元数据以确保用户端高效视频重建，但提出的框架还结合了最先进的去噪和视频帧插补（VFI）技术。这些技术可以缓解语义模糊，并在噪声无线通信环境中恢复帧的时间一致性。实验结果表明，提出的方案实现了优化带宽使用下的高质量视频流传输，在QoE和资源效率方面优于当前最先进的解决方案。这项工作为5G及未来后5G网络中的可扩展实时视频流传输开辟了新的可能性。', 'title_zh': '基于隐式扩散模型的语义感知自适应视频流传输技术在无线网络中的应用'}
{'arxiv_id': 'arXiv:2502.05694', 'title': 'Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA and ChatGPT', 'authors': 'Shaoshuai Du, Yiyi Tao, Yixian Shen, Hang Zhang, Yanxin Shen, Xinyu Qiu, Chuanqi Shi', 'link': 'https://arxiv.org/abs/2502.05694', 'abstract': 'This study investigates the performance of various large language models (LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that integrates entity recognition and relation extraction without requiring annotated data. While LLMs show promise for RE, most prior work focuses on English or assumes pre-annotated entities, leaving their effectiveness in Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini, and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates the highest overall performance, balancing precision and recall, while Gemini achieves the fastest inference speed, making it suitable for real-time applications. LLaMA underperforms in both accuracy and latency, highlighting the need for further adaptation. Our findings provide insights into the strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on trade-offs between accuracy and efficiency. This study serves as a foundation for future research aimed at improving LLM adaptability to complex linguistic tasks in Chinese NLP.', 'abstract_zh': '本研究考察了各类大型语言模型（LLMs）在中文零样本端到端关系抽取（RE）任务中的性能，该任务结合了实体识别和关系抽取，无需标注数据。虽然LLMs在RE方面显示出潜力，但大多数先前的工作集中于英语或将实体预先标注，这使得它们在中文RE中的有效性尚未得到充分探索。为了弥合这一差距，我们基于准确率、效率和适应性对ChatGPT、Gemini和LLaMA进行了评估。ChatGPT展现出全面的最高性能，平衡了精确率和召回率，而Gemini则在推理速度方面最快，使其适合实时应用。LLaMA在准确率和延迟方面均表现较差，凸显了进一步适应的需求。我们的研究表明了LLMs在零样本中文RE任务中的优势和局限性，揭示了准确率和效率之间的权衡。本研究为未来旨在改进LLMs对复杂语言任务适应性的中文自然语言处理研究奠定了基础。', 'title_zh': '中文翻译如下，符合学术规范：\n\n中文标题：《中文零样本端到端关系提取：Gemini、LLaMA 和 ChatGPT 的比较研究》\n\n如果需要更详细的论文内容翻译或有其他需求，请告知！'}
{'arxiv_id': 'arXiv:2502.05685', 'title': 'Mobile Application Threats and Security', 'authors': 'Timur Mirzoev, Mark Miller, Shamimara Lasker, Michael Brannon', 'link': 'https://arxiv.org/abs/2502.05685', 'abstract': 'The movement to mobile computing solutions provides flexibility to different users whether it is a business user, a student, or even providing entertainment to children and adults of all ages. Due to these emerging technologies mobile users are unable to safeguard private information in a very effective way and cybercrimes are increasing day by day. This manuscript will focus on security vulnerabilities in the mobile computing industry, especially focusing on tablets and smart phones. This study will dive into current security threats for the Android & Apple iOS market, exposing security risks and threats that the novice or average user may not be aware of. The purpose of this study is to analyze current security risks and threats, and provide solutions that may be deployed to protect against such threats.', 'abstract_zh': '移动计算解决方案的推广为不同用户提供了灵活性，无论是企业用户、学生，还是为各个年龄段的儿童和成人的娱乐提供服务。由于这些新兴技术，移动用户在有效地保护个人信息方面存在局限性，导致网络犯罪日益增加。本论文将重点关注移动计算行业中存在的安全漏洞，特别是针对平板电脑和智能手机。本研究将深入探讨当前Android和Apple iOS市场的安全威胁，揭示初学者或普通用户可能不熟悉的网络安全风险和威胁。本研究的目的在于分析当前存在的安全风险和威胁，并提供可能用于抵御这些威胁的解决方案。', 'title_zh': '移动应用威胁与安全性'}
{'arxiv_id': 'arXiv:2502.05684', 'title': 'Machine Unlearning via Information Theoretic Regularization', 'authors': 'Shizhou Xu, Thomas Strohmer', 'link': 'https://arxiv.org/abs/2502.05684', 'abstract': 'How can we effectively remove or "unlearn" undesirable information, such as specific features or individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a mathematical framework based on information-theoretic regularization to address both feature and data point unlearning. For feature unlearning, we derive a unified solution that simultaneously optimizes diverse learning objectives, including entropy, conditional entropy, KL-divergence, and the energy of conditional probability. For data point unlearning, we first propose a novel definition that serves as a practical condition for unlearning via retraining, is easy to verify, and aligns with the principles of differential privacy from an inference perspective. Then, we provide provable guarantees for our framework on data point unlearning. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications.', 'abstract_zh': '如何有效地从学习成果中移除或“忘记”不必要的信息（如特定特征或个别数据点），同时最小化实用性损失并确保严格的保证？我们引入了一种基于信息论正则化的数学框架，以解决特征和数据点的删除问题。对于特征删除，我们推导出一个统一的解决方案，该方案同时优化了包括熵、条件熵、KL散度和条件概率能量在内的多种学习目标。对于数据点删除，我们首先提出了一种新颖的定义，该定义为通过重新训练进行删除提供了实用条件，易于验证，并从推理的角度与差分隐私原则相一致。然后，我们为我们的框架提供了关于数据点删除的可证明保证。通过在学习目标的灵活性与正则化设计的简单性之间进行结合，我们的方法对于广泛范围的机器学习和人工智能应用具有高度的适应性和实用性。', 'title_zh': '通过信息论正则化实现机器卸载'}
{'arxiv_id': 'arXiv:2502.05672', 'title': 'On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers', 'authors': 'Miroslav Štrupl, Oleg Szehr, Francesco Faccio, Dylan R. Ashley, Rupesh Kumar Srivastava, Jürgen Schmidhuber', 'link': 'https://arxiv.org/abs/2502.05672', 'abstract': 'This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.', 'abstract_zh': '本文对Episodic Upside-Down强化学习、目标条件监督学习以及在线决策变换器的收敛性和稳定性进行了严格的分析。这些算法在从游戏到机器人任务的各种基准上表现出了竞争力，但它们的理论理解主要局限于特定的环境条件。本工作旨在为基于监督学习或序列建模方法的强化学习算法提供理论基础。这一研究的核心在于分析在何种环境下算法能够识别最优解。我们还评估了在环境存在微小噪声的情况下，新兴解决方案是否能够保持稳定。具体而言，我们研究了命令条件策略、值和目标达成目标在基础马尔可夫决策过程转移核条件下的一致性和渐近收敛性。我们证明，如果转移核位于确定性核的足够小的邻域内，则可以实现接近最优的行为。这些量在确定性核上是一致连续（特定拓扑下）的，无论是从渐近意义上考虑，还是在有限的学习周期后。所发展的方法使得我们能够首次提出策略和值在基础转移核下的收敛性和稳定性显式估计。在理论方面，我们引入了若干新的概念，例如在区间空间中工作，研究商拓扑中的连续性，以及动力系统不动点理论的应用。理论研究得到了一系列具体环境的详细研究和数值实验的支持。', 'title_zh': '上下翻转强化学习、目标导向监督学习及在线决策变换器的收敛性与稳定性探究'}
{'arxiv_id': 'arXiv:2502.05670', 'title': 'Language Models Largely Exhibit Human-like Constituent Ordering Preferences', 'authors': 'Ada Defne Tur, Gaurav Kamath, Siva Reddy', 'link': 'https://arxiv.org/abs/2502.05670', 'abstract': "Though English sentences are typically inflexible vis-à-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent's length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with constituent movement, and may provide insights into existing theories on when and how the shift occurs in human language. We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs. Despite performing unexpectedly around particle movement, LLMs generally align with human preferences around constituent ordering.", 'abstract_zh': '尽管英语句子在词序方面通常不够灵活，句子成分的排列顺序却显示出极大的变化性。一个重要的理论观点提出，成分的排列顺序与其重量（即长度或复杂性）直接相关。这类理论在自然语言处理（NLP）领域颇具吸引力，因为尽管近期NLP领域的进展大幅提升了大型语言模型（LLMs）的表现，但这些模型是如何处理语言的，以及这种处理方式与人类语言处理有何异同等问题仍不清楚。特别是，关于LLMs是否表现出与人类相同的组成部分移动模式的问题依然悬而未决，这可能为我们提供有关人类语言转换什么时候以及如何发生的现有理论提供新的见解。我们对具有不同特性的多种LLM进行比较研究，评估它们在四种类型的语言成分移动任务中的整体表现，包括重NP移动、助词移动、宾语交替以及多重动词短语。尽管在助词移动方面表现异常，LLMs在其他类型的成分移动方面通常与人类的偏好保持一致。', 'title_zh': '语言模型在构成成分排列上很大程度上表现出类似人类的偏好'}
{'arxiv_id': 'arXiv:2502.05664', 'title': 'CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging', 'authors': 'Md. Ashraful Islam, Mohammed Eunus Ali, Md Rizwan Parvez', 'link': 'https://arxiv.org/abs/2502.05664', 'abstract': "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (this https URL).", 'abstract_zh': '大型语言模型（LLMs）在代码生成和问题解决方面取得了显著进展。当前的方法采用基于外部工具的迭代调试器，利用编译器或其他工具的运行时反馈来细化由各种方法生成的初始代码。然而，这些方法的有效性在很大程度上依赖于初始代码生成的质量，仍是一个开放的挑战。在本文中，我们引入了CodeSim，这是一种新颖的多智能体代码生成框架，通过类人的感知方法全面解决程序合成-规划、编码和调试的各个阶段。如同人类通过视觉模拟来验证其对任何算法的理解一样，CodeSim 通过逐步模拟输入/输出的独特方法实现了计划验证和内部调试。跨越七个具有挑战性的竞争性问题解决和程序合成基准的广泛实验，展示了CodeSim 强大的代码生成能力。我们的框架在三大基准测试中取得了新的最先进成果（HumanEval 95.1%，MBPP 90.7%，APPS 22%，CodeContests 29.1%）。此外，我们的方法与外部调试器级联时具有更大的增强潜力。为了促进该领域的进一步研究和开发，我们在本文提供了框架的开源链接 (this https URL)。', 'title_zh': 'CODESIM：通过仿真驱动的规划与调试实现的多代理代码生成与问题解决'}
{'arxiv_id': 'arXiv:2502.05651', 'title': 'KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy', 'authors': 'Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho', 'link': 'https://arxiv.org/abs/2502.05651', 'abstract': 'The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.', 'abstract_zh': '随着对心理健康服务需求的增加，AI驱动的心理健康聊天机器人得到了发展，但隐私、数据收集和专业知识方面的问题依然存在。动机访谈(Motivational Interviewing, MI)作为理论基础，被越来越多地关注，有助于增强这些聊天机器人的专业知识。然而，现有的数据集在训练聊天机器人方面显示出局限性，导致在MI和心理治疗领域迫切需要公开可用的资源。这些挑战在非英语语言中更为突出，而在这些语言中这些问题的关注度较低。本文提出了一种新颖的框架，该框架模拟了富有专业治疗师专业知识的动机访谈会话。我们训练了一个模仿专业治疗师行为选择的动机访谈预测模型，并使用大型语言模型（LLMs）通过提示工程生成相应的回应。在此基础上，我们提出了KMI，这是第一个理论基础基于MI的合成数据集，包含1000个高质量的韩语动机访谈对话。通过广泛的专家评估生成的数据集以及基于该数据集训练的对话模型，我们展示了KMI的质量、专业知识及其实际应用价值。我们还引入了源自动机访谈理论的新颖评价指标，用于从动机访谈的角度评估对话。', 'title_zh': 'KMI：用于心理治疗的韩语动机访谈对话数据集'}
{'arxiv_id': 'arXiv:2502.05641', 'title': 'Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs', 'authors': 'Aayam Shrestha, Pan Liu, German Ros, Kai Yuan, Alan Fern', 'link': 'https://arxiv.org/abs/2502.05641', 'abstract': "This work focuses on generating realistic, physically-based human behaviors from multi-modal inputs, which may only partially specify the desired motion. For example, the input may come from a VR controller providing arm motion and body velocity, partial key-point animation, computer vision applied to videos, or even higher-level motion goals. This requires a versatile low-level humanoid controller that can handle such sparse, under-specified guidance, seamlessly switch between skills, and recover from failures. Current approaches for learning humanoid controllers from demonstration data capture some of these characteristics, but none achieve them all. To this end, we introduce the Masked Humanoid Controller (MHC), a novel approach that applies multi-objective imitation learning on augmented and selectively masked motion demonstrations. The training methodology results in an MHC that exhibits the key capabilities of catch-up to out-of-sync input commands, combining elements from multiple motion sequences, and completing unspecified parts of motions from sparse multimodal input. We demonstrate these key capabilities for an MHC learned over a dataset of 87 diverse skills and showcase different multi-modal use cases, including integration with planning frameworks to highlight MHC's ability to solve new user-defined tasks without any finetuning.", 'abstract_zh': '本文专注于从多模态输入生成现实且基于物理的人类行为，而这些输入可能仅部分指定了所需的动作。例如，输入可能来自VR控制器提供的手臂动作和身体速度、部分关键点动画、视频中的计算机视觉，甚至高级别的动作目标。这需要一个多功能的低级类人控制器，能够处理这些稀疏且不完全指定的指导，无缝切换技能，并从失败中恢复。当前从演示数据学习类人控制器的方法在某些方面具备这些特征，但没有一种方法能够全部实现这些功能。为此，我们引入了掩码类人控制器（MHC），这是一个新颖的方法，它通过增强和选择性掩码的动作演示进行多目标模仿学习。训练方法导致MHC表现出关键能力：对接收到的延迟输入命令的追赶、结合多个动作序列的元素，并从稀疏多模态输入中完成动作的未指定部分。我们展示了在涵盖87种多样技能的数据集上学习的MHC的关键能力，并展示了不同的多模态应用场景，包括与规划框架集成以突出MHC解决新用户定义任务的能力，而无需任何微调。', 'title_zh': '从多模态输入生成物理现实且可操控的人体运动'}
{'arxiv_id': 'arXiv:2502.05638', 'title': 'ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports', 'authors': 'Aynur Guluzade, Naguib Heiba, Zeyd Boukhers, Florim Hamiti, Jahid Hasan Polash, Yehya Mohamad, Carlos A Velasco', 'link': 'https://arxiv.org/abs/2502.05638', 'abstract': "Europe's healthcare systems require enhanced interoperability and digitalization, driving a demand for innovative solutions to process legacy clinical data. This paper presents the results of our project, which aims to leverage Large Language Models (LLMs) to extract structured information from unstructured clinical reports, focusing on patient history, diagnoses, treatments, and other predefined categories. We developed a workflow with a user interface and evaluated LLMs of varying sizes through prompting strategies and fine-tuning. Our results show that fine-tuned smaller models match or surpass larger counterparts in performance, offering efficiency for resource-limited settings. A new dataset of 60,000 annotated English clinical summaries and 24,000 German translations was validated with automated and manual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics. The work highlights the approach's viability and outlines future improvements.", 'abstract_zh': '欧洲的卫生保健系统需要增强的互操作性和数字化，推动了对创新解决方案的需求，以处理遗留的临床数据。本文介绍了我们项目的研究成果，旨在利用大型语言模型（LLMs）从非结构化的临床报告中提取结构化信息，重点关注患者的病史、诊断、治疗及其他预定义的类别。我们开发了一个包含用户界面的工作流程，并通过提示策略和细调评估了不同规模的LLMs。结果表明，在资源有限的环境中，细调的小型模型在性能上可以匹配甚至超过大型模型。我们还验证了一个包含60,000个标注的英语临床摘要和24,000个德语翻译的新数据集，使用了自动和手动检查方法。评估使用了ROUGE、BERTScore和实体级别的指标。这项工作突显了该方法的有效性，并概述了未来改进的方向。', 'title_zh': 'ELMTEX： fine-tuning 大型语言模型以提取结构化的临床信息——以临床报告为例的研究'}
{'arxiv_id': 'arXiv:2502.05637', 'title': 'Adversarial Machine Learning: Attacks, Defenses, and Open Challenges', 'authors': 'Pranav K Jha', 'link': 'https://arxiv.org/abs/2502.05637', 'abstract': 'Adversarial Machine Learning (AML) addresses vulnerabilities in AI systems where adversaries manipulate inputs or training data to degrade performance. This article provides a comprehensive analysis of evasion and poisoning attacks, formalizes defense mechanisms with mathematical rigor, and discusses the challenges of implementing robust solutions in adaptive threat models. Additionally, it highlights open challenges in certified robustness, scalability, and real-world deployment.', 'abstract_zh': '对抗机器学习（AML）旨在解决AI系统中的脆弱性问题，其中对手通过操纵输入或训练数据来降低系统的性能。本文对逃避攻击和污染攻击进行了全面分析，并以数学 rigor 化的方式形式化了防御机制，讨论了在适应性威胁模型中实施稳健解决方案的挑战。此外，本文还强调了在认证稳健性、扩展性和实际部署方面的开放性挑战。', 'title_zh': 'adversarial machine learning: 攻击、防御及开放挑战'}
{'arxiv_id': 'arXiv:2502.05615', 'title': 'XiHeFusion: Harnessing Large Language Models for Science Communication in Nuclear Fusion', 'authors': 'Xiao Wang, Qingquan Yang, Fuling Wang, Qiang Chen, Wentao Wu, Yu Jin, Jingtao Jiang, Liye Jin, Bo Jiang, Dengdi Sun, Wanli Lv, Meiwen Chen, Zehua Chen, Guosheng Xu, Jin Tang', 'link': 'https://arxiv.org/abs/2502.05615', 'abstract': 'Nuclear fusion is one of the most promising ways for humans to obtain infinite energy. Currently, with the rapid development of artificial intelligence, the mission of nuclear fusion has also entered a critical period of its development. How to let more people to understand nuclear fusion and join in its research is one of the effective means to accelerate the implementation of fusion. This paper proposes the first large model in the field of nuclear fusion, XiHeFusion, which is obtained through supervised fine-tuning based on the open-source large model Qwen2.5-14B. We have collected multi-source knowledge about nuclear fusion tasks to support the training of this model, including the common crawl, eBooks, arXiv, dissertation, etc. After the model has mastered the knowledge of the nuclear fusion field, we further used the chain of thought to enhance its logical reasoning ability, making XiHeFusion able to provide more accurate and logical answers. In addition, we propose a test questionnaire containing 180+ questions to assess the conversational ability of this science popularization large model. Extensive experimental results show that our nuclear fusion dialogue model, XiHeFusion, can perform well in answering science popularization knowledge. The pre-trained XiHeFusion model is released on this https URL.', 'abstract_zh': '核聚变是人类获取无限能源最有前景的方式之一。随着人工智能的飞速发展，核聚变的任务也进入了发展中的关键时期。让更多的人了解核聚变并参与其中的研究是加速其实现的有效手段之一。本文提出了核聚变领域的第一个大型模型——“XiHeFusion”，该模型基于开源大型模型Qwen2.5-14B进行监督微调获得。我们收集了关于核聚变任务的多源知识以支持该模型的训练，包括Common Crawl、电子书、arXiv论文、学位论文等。在模型掌握了核聚变领域的知识后，我们进一步通过逻辑推理链的方式增强了其逻辑推理能力，使XiHeFusion能够提供更准确和逻辑性强的答案。此外，我们还提出了一份包含180多道题的测试问卷，用于评估这个科学普及大型模型的对话能力。广泛的实验结果显示，我们的核聚变对话模型XiHeFusion在回答科学普及知识方面表现良好。预训练的XiHeFusion模型已发布于此链接：[此链接](https://yourlink.com)。', 'title_zh': 'XiHeFusion：利用大型语言模型进行核聚变科学传播'}
{'arxiv_id': 'arXiv:2502.05589', 'title': 'On Memory Construction and Retrieval for Personalized Conversational Agents', 'authors': 'Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao', 'link': 'https://arxiv.org/abs/2502.05589', 'abstract': 'To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.', 'abstract_zh': '为了在长期对话中提供连贯且个性化的体验，现有方法通常通过从对话历史中构建记忆库来进行检索增强的响应生成，这些记忆库可以是基于回合、会话或通过总结技术构建的。本文我们提出了两个关键发现：(1) 记忆单元的粒度至关重要：基于回合、会话以及基于总结的方法在记忆检索精度和检索内容语义质量方面各自存在局限性。(2) 压缩提示方法（例如，LLMLingua-2）可以有效作为去噪机制，提高不同粒度下的记忆检索精度。基于这些洞察，我们提出了SeCom方法，该方法通过引入对话分割模型来构建具有专题片段的记忆库，并基于压缩的记忆单元进行记忆检索。实验结果表明，SeCom在LOCOMO和Long-MT-Bench+等长期对话基准测试中优于基于回合、会话以及几种基于总结的方法。此外，提出的对话分割方法在DialSeg711、TIAGE和SuperDialSeg等对话分割数据集上也表现出了优越的性能。', 'title_zh': '针对个性化对话代理的记忆构建与检索的研究'}
{'arxiv_id': 'arXiv:2502.05574', 'title': 'Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark', 'authors': 'Shiao Wang, Xiao Wang, Chao Wang, Liye Jin, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang', 'link': 'https://arxiv.org/abs/2502.05574', 'abstract': "We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on this https URL", 'abstract_zh': '我们引入了一种新的分层知识蒸馏策略，该策略结合了相似性矩阵、特征表示和基于响应图的知识蒸馏，以引导学生Transformer网络的学习。我们还通过应用时间傅里叶变换来增强模型捕获时间依赖关系的能力，从而建立视频帧之间的时间关系。通过新提出的测试时调优策略，我们将网络模型适应特定的目标对象，从而实现目标跟踪的高性能和灵活性。鉴于现有事件驱动跟踪数据集的主要局限性——大多为低分辨率，我们提出了EventVOT，这是第一个大规模高分辨率事件驱动跟踪数据集。该数据集包含1141个视频，涵盖行人、车辆、无人机、乒乓球等多样化类别。在低分辨率（FE240Hz、VisEvent、FELT）和我们新提出的高分辨率EventVOT数据集上进行的广泛实验充分验证了我们提出方法的有效性。基准数据集和源代码已发布在以下链接：<https://your-link-here.com>', 'title_zh': '基于事件流的视觉目标跟踪：HDETrack V2及其高清基准数据集'}
{'arxiv_id': 'arXiv:2502.05573', 'title': 'Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning', 'authors': 'Beining Zhang, Aditya Kapoor, Mingfei Sun', 'link': 'https://arxiv.org/abs/2502.05573', 'abstract': "Multi-agent reinforcement learning (MARL) often relies on \\emph{parameter sharing (PS)} to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose \\textbf{Low-Rank Agent-Specific Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing \\emph{parameter-space sparsity} that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines \\emph{while reducing memory and computational overhead}. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.", 'abstract_zh': '多智能体强化学习（MARL）通常依赖于**参数共享（PS）**以实现高效扩展。然而，单纯的共享策略可能会限制每个智能体的独特专长，从而在异构环境中降低整体性能。我们提出了一种名为**低秩智能体特定适应（LoRASA）**的新型方法，该方法将每个智能体的策略视为从共享主干微调的“任务”。LoRASA 方法借鉴了参数高效的迁移学习方法，向共享策略的每一层附加小型低秩适应矩阵，自然诱导了**参数空间稀疏性**，从而促进专业化和可扩展性。我们通过包括《星际争霸》多智能体挑战（SMAC）和多智能体MuJoCo（MAMuJoCo）在内的具有挑战性的基准测试评估了LoRASA，并在其上实现了广泛使用的算法，如MAPPO和A2PO。在各种任务中，LoRASA 在降低内存和计算开销的同时与现有基线相比表现相当甚至更优。消融研究表明，适配器的秩、位置和时机的灵活性和效率。我们的研究表明，LoRASA 有可能为MARL策略参数化建立一个新的标准：结合共享的基础进行协调，同时通过低秩智能体特定的修正实现个体专业化。', 'title_zh': '低秩个性化适应（LoRASA）多智能体策略学习'}
{'arxiv_id': 'arXiv:2502.05568', 'title': 'Large Multimodal Models for Low-Resource Languages: A Survey', 'authors': 'Marian Lupascu, Ana-Cristina Rogoz, Mihai Sorin Stupariu, Radu Tudor Ionescu', 'link': 'https://arxiv.org/abs/2502.05568', 'abstract': 'In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: this https URL.', 'abstract_zh': '在这项综述中，我们系统分析了用于适应低资源（Low-Resource，LR）语言的大规模多模态模型（Large Multimodal Models，LMMs）的技术，研究了从视觉增强和数据创建到跨模态转移和融合策略等多种方法。通过对75种LR语言中106项相关研究的全面分析，我们识别了研究人员在面对数据和计算资源有限挑战时的关键模式。我们发现视觉信息通常是提高在LR环境中模型性能的关键桥梁，但仍然面临诸多挑战，特别是在幻觉减轻和计算效率方面。我们旨在为研究人员提供当前方法和待解决挑战的清晰理解，以使LMMs更加适用于LR（未充分研究）语言的演讲者。我们还通过一个开源仓库补充了这篇综述，网址为: this https URL。', 'title_zh': '低资源语言的大规模多模态模型：一篇综述'}
{'arxiv_id': 'arXiv:2502.05567', 'title': 'ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data', 'authors': 'Xiaoyang Liu, Kangjie Bao, Jiashuo Zhang, Yunqi Liu, Yu Chen, Yuntian Liu, Yang Jiao, Tao Luo', 'link': 'https://arxiv.org/abs/2502.05567', 'abstract': 'Autoformalization, the process of automatically translating natural language mathematics into machine-verifiable formal language, has demonstrated advancements with the progress of large language models (LLMs). However, a key obstacle to further advancements is the scarcity of paired datasets that align natural language with formal language. To address this challenge, we introduce ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), an iterative data generation framework designed to produce large-scale, high-quality parallel theorem statements. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 300k theorem statements and develop the ATLAS translator, achieving accuracies of 80.59% (pass@8) and 92.99% (pass@128) on ProofNet, significantly outperforming the base model (23.99% and 47.17%) and InternLM2-Math-Plus-7B (50.94% and 80.32%). Furthermore, the ATLAS translator also achieves state-of-the-art performance on both the high-school-level miniF2F dataset and the graduate-level MathQual dataset introduced in this work. The datasets, model, and code will be released to the public soon.', 'abstract_zh': '自动形式化是指将自然语言数学转换为可由机器验证的形式语言的自动过程，随着大型语言模型（LLMs）的进步，这一过程已显示出显著的发展。然而，进一步发展的关键障碍是缺乏自然语言与形式语言相匹配的数据集。为了解决这一挑战，我们介绍了ATLAS（自动形式化定理通过提升、增强和数据合成），这是一种迭代数据生成框架，旨在生成大规模的高质量平行定理陈述。通过运行10个迭代的ATLAS，我们构建了一个本科水平的数据集，包含30万条定理陈述，并开发了ATLAS翻译器，在ProofNet上实现了80.59%（pass@8）和92.99%（pass@128）的准确性，显著优于基线模型（23.99%和47.17%）和InternLM2-Math-Plus-7B（50.94%和80.32%）。此外，ATLAS翻译器还在本工作报告中引入的高中水平miniF2F数据集和研究生水平的MathQual数据集上实现了最先进的性能。该数据集、模型和代码即将公开发布。', 'title_zh': 'ATLAS: 通过提升、扩展和数据合成自动形式化定理'}
{'arxiv_id': 'arXiv:2502.05564', 'title': 'TabICL: A Tabular Foundation Model for In-Context Learning on Large Data', 'authors': 'Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan', 'link': 'https://arxiv.org/abs/2502.05564', 'abstract': 'The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While the very recent TabPFNv2 foundation model (2025) excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 56 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data.', 'abstract_zh': '长期以来，梯度提升决策树（Gradient-Boosted Decision Trees, GBDTs）在表格数据上的主导地位目前正受到使用上下文学习（In-Context Learning, ICL）的表格基础模型的挑战：将训练数据作为测试数据的上下文，在单次前向传递中进行预测而无需更新参数。虽然最近的TabPFNv2基础模型（2025）在包含最多10,000个样本的表格数据上表现突出，但其交替进行列间和行间注意力使得处理大量训练数据在计算上变得不切实际。那么，ICL是否可以有效地扩展并为更大的表格数据集带来好处？我们提出了一个名为TabICL的表格基础模型，该模型在最多包含60,000个样本的合成数据集上进行预训练，并能够利用可负担的资源处理500,000个样本。这得益于一种新颖的两阶段架构：一种列先、行后注意力机制，用于构建固定维度的行嵌入，随后使用变压器进行高效的上下文学习。在TALENT基准测试中的200个分类数据集上，TabICL在性能上与TabPFNv2相当，但系统上更快（最多快10倍），并且显著优于其他所有方法。在包含超过10,000个样本的56个数据集上，TabICL超过了TabPFNv2和CatBoost，展示了ICL在大规模数据上的潜力。', 'title_zh': 'TabICL：一种用于大规模数据内省学习的表格基础模型'}
{'arxiv_id': 'arXiv:2502.05547', 'title': 'Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning', 'authors': 'Runhua Xu, Shiqi Gao, Chao Li, James Joshi, Jianxin Li', 'link': 'https://arxiv.org/abs/2502.05547', 'abstract': "Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation. Few recent efforts to simultaneously tackle both challenges offen depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application. To overcome this dilemma, this paper introduce a Dual Defense Federated learning (DDFed) framework. DDFed simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology. DDFed initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process. We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. Experiments on publicly available datasets demonstrate that DDFed successfully protects model privacy and effectively defends against model poisoning threats.", 'abstract_zh': '联邦学习（FL）本质上容易遭受隐私泄露和中毒攻击。为了应对这些挑战，研究人员分别设计了安全聚合机制来保护数据隐私，以及抗中毒攻击的 robust 聚合方法。然而，同时解决这两个问题极具挑战性；安全聚合方法使抵御中毒攻击变得困难，因为大多数异常检测技术需要访问未加密的本地模型更新，而安全聚合会掩盖这些更新。少数旨在同时解决这两个问题的研究依赖于非合作两服务器设置这一 impractical 假设，这破坏了 FL 的拓扑结构，或者依赖于三方计算，这引入了可扩展性问题，使得部署和应用复杂化。为克服这一困境，本文提出了一种双重防御联邦学习（DDFed）框架。DDFed 同时增强了隐私保护并减轻了中毒攻击，而无需引入新的参与者角色或破坏现有的 FL 拓扑结构。DDFed 初期利用先进的完全同态加密（FHE）进行安全聚合，避免了非合作两服务器设置的 impractical 要求，并提供了强大的隐私保护。此外，我们提出了一种独特的两阶段异常检测机制，涵盖加密模型更新的安全相似性计算和反馈驱动的协作选择，并在检测过程中嵌入了防止拜占庭客户端潜在隐私泄露的额外措施。我们在各种模型中毒攻击和 FL 场景中进行了广泛的实验，包括设备间和数据孤岛间的联邦学习。对公开数据集的实验结果表明，DDFed 成功保护了模型隐私并有效地抵御了模型中毒威胁。', 'title_zh': '双层防御：增强联邦学习中的隐私保护并减轻中毒攻击'}
{'arxiv_id': 'arXiv:2502.05526', 'title': 'Towards Learning Scalable Agile Dynamic Motion Planning for Robosoccer Teams with Policy Optimization', 'authors': 'Brandon Ho, Batuhan Altundas, Matthew Gombolay', 'link': 'https://arxiv.org/abs/2502.05526', 'abstract': "In fast-paced, ever-changing environments, dynamic Motion Planning for Multi-Agent Systems in the presence of obstacles is a universal and unsolved problem. Be it from path planning around obstacles to the movement of robotic arms, or in planning navigation of robot teams in settings such as Robosoccer, dynamic motion planning is needed to avoid collisions while reaching the targeted destination when multiple agents occupy the same area. In continuous domains where the world changes quickly, existing classical Motion Planning algorithms such as RRT* and A* become computationally expensive to rerun at every time step. Many variations of classical and well-formulated non-learning path-planning methods have been proposed to solve this universal problem but fall short due to their limitations of speed, smoothness, optimally, etc. Deep Learning models overcome their challenges due to their ability to adapt to varying environments based on past experience. However, current learning motion planning models use discretized environments, do not account for heterogeneous agents or replanning, and build up to improve the classical motion planners' efficiency, leading to issues with scalability. To prevent collisions between heterogenous team members and collision to obstacles while trying to reach the target location, we present a learning-based dynamic navigation model and show our model working on a simple environment in the concept of a simple Robosoccer Game.", 'abstract_zh': '在快节奏且不断变化的环境中，多智能体系统的动态避障路径规划是普遍存在的且尚未解决的问题。从围绕障碍物进行路径规划到机器人手臂的运动，以及在Robosoccer等设置下的机器人团队导航规划，都需要进行动态路径规划以避免碰撞并达到目标位置。在世界快速变化的连续领域中，现有的经典路径规划算法如RRT*和A*在每一时间步重新运行变得计算成本高昂。为了解决这一普遍问题，许多经典和经过良好定义的非学习路径规划方法已被提出，但由于速度、平滑性和最优性等方面的局限性，这些方法存在不足。深度学习模型能够通过以往的经验适应变化的环境，克服了这些挑战。然而，当前的学习路径规划模型使用离散化的环境，并未考虑异质智能体或重新规划，而是试图提高经典路径规划算法的效率，导致可扩展性问题。为了防止不同类型的团队成员之间以及与障碍物的碰撞，同时努力到达目标位置，我们提出了一种基于学习的动态导航模型，并展示了该模型在简单Robosoccer游戏中简单环境中的应用。', 'title_zh': '面向罗bor soccer团队可扩展敏捷动态运动规划的学习方法研究：基于策略优化'}
{'arxiv_id': 'arXiv:2502.05512', 'title': 'IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System', 'authors': 'Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang', 'link': 'https://arxiv.org/abs/2502.05512', 'abstract': 'Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning this http URL, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at this https URL.', 'abstract_zh': '近年来，基于大型语言模型（LLM）的文字到语音（TTS）系统逐渐成为主流，得益于其高度自然的语音和强大的零样本语音克隆能力。在本文中，我们介绍了一种名为 IndexTTS 的系统，该系统主要是基于 XTTS 和 Tortoise 模型，并进行了若干创新改进。具体而言，在中文场景中，我们采用了将汉字和拼音结合的混合建模方法，使得多义字和长尾字的发音可控。同时，我们对音节声学表征编码的字典使用进行了 Vector Quantization (VQ) 和 Finite-Scalar Quantization (FSQ) 的比较分析。为了进一步提高语音克隆的效果和稳定性，我们引入了基于 Conformer 的语音条件编码器，并用 BigVGAN2 替换了语音码解码器。与 XTTS 相比，IndexTTS 在自然度、内容一致性和零样本语音克隆方面取得了显著改进。对于开源 TTS 系统中常见的 Fish-Speech、CosyVoice2、FireRedTTS 和 F5-TTS，IndexTTS 的训练过程更简单、使用更可控、推理速度更快，而且性能也更优。我们的演示可供访问：[提供链接的网址]。', 'title_zh': 'IndexTTS：一个工业级可控且高效的零样本文本到语音系统'}
{'arxiv_id': 'arXiv:2502.05503', 'title': 'A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction', 'authors': 'Yongfan Chen, Xiuwen Zhu, Tianyu Li, Hao Chen, Chunhua Shen', 'link': 'https://arxiv.org/abs/2502.05503', 'abstract': 'Recent advances in video generation models demonstrate their potential as world simulators, but they often struggle with videos deviating from physical laws, a key concern overlooked by most text-to-video benchmarks. We introduce a benchmark designed specifically to assess the Physical Coherence of generated videos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of physical principles, capturing key physical laws observable in video content. We evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and conducted manual assessments. Additionally, we propose an automated evaluation model: PhyCoPredictor, a diffusion model that generates optical flow and video frames in a cascade manner. Through a consistency evaluation comparing automated and manual sorting, the experimental results show that PhyCoPredictor currently aligns most closely with human evaluation. Therefore, it can effectively evaluate the physical coherence of videos, providing insights for future model optimization. Our benchmark, which includes physical coherence prompts, automatic evaluation tool PhyCoPredictor, and generated video dataset, will all be released on GitHub shortly.', 'abstract_zh': '近期视频生成模型的发展展示了它们作为世界模拟器的潜力，但在大多数基于文本到视频的标准测试中，这些模型往往无法应对背离物理定律的视频内容，这成为了一个被忽视的关键问题。我们提出了一种专门用于评估生成视频物理一致性（Physical Coherence）的基准测试，名为PhyCoBench。该基准测试包括120个提示，涵盖了7个物理原理类别，涵盖了视频内容中可观察到的关键物理定律。我们在PhyCoBench上评估了四个当前最先进的（SoTA）文本到视频（T2V）模型，并进行了人工评估。此外，我们还提出了一种自动评估模型：PhyCoPredictor，这是一种渐进式的扩散模型，能够以级联方式生成光流和视频帧。通过自动评估与手动评估的一致性评估，实验结果显示PhyCoPredictor目前最接近人类评估，因此它可以有效评估视频的物理一致性，并为未来模型优化提供洞见。我们的基准测试将包括物理一致性提示、自动评估工具PhyCoPredictor以及生成的视频数据集，预计将在GitHub上尽快公布。', 'title_zh': '基于光学流引导帧预测的视频生成模型评估物理一致性基准'}
{'arxiv_id': 'arXiv:2502.05500', 'title': 'Vision-Ultrasound Robotic System based on Deep Learning for Gas and Arc Hazard Detection in Manufacturing', 'authors': 'Jin-Hee Lee, Dahyun Nam, Robin Inho Kee, YoungKey Kim, Seok-Jun Buu', 'link': 'https://arxiv.org/abs/2502.05500', 'abstract': 'Gas leaks and arc discharges present significant risks in industrial environments, requiring robust detection systems to ensure safety and operational efficiency. Inspired by human protocols that combine visual identification with acoustic verification, this study proposes a deep learning-based robotic system for autonomously detecting and classifying gas leaks and arc discharges in manufacturing settings. The system is designed to execute all experimental tasks entirely onboard the robot. Utilizing a 112-channel acoustic camera operating at a 96 kHz sampling rate to capture ultrasonic frequencies, the system processes real-world datasets recorded in diverse industrial scenarios. These datasets include multiple gas leak configurations (e.g., pinhole, open end) and partial discharge types (Corona, Surface, Floating) under varying environmental noise conditions. Proposed system integrates visual detection and a beamforming-enhanced acoustic analysis pipeline. Signals are transformed using STFT and refined through Gamma Correction, enabling robust feature extraction. An Inception-inspired CNN further classifies hazards, achieving 99% gas leak detection accuracy. The system not only detects individual hazard sources but also enhances classification reliability by fusing multi-modal data from both vision and acoustic sensors. When tested in reverberation and noise-augmented environments, the system outperformed conventional models by up to 44%p, with experimental tasks meticulously designed to ensure fairness and reproducibility. Additionally, the system is optimized for real-time deployment, maintaining an inference time of 2.1 seconds on a mobile robotic platform. By emulating human-like inspection protocols and integrating vision with acoustic modalities, this study presents an effective solution for industrial automation, significantly improving safety and operational reliability.', 'abstract_zh': '工业环境中气体泄漏和电弧放电存在重大风险，需要可靠的检测系统以确保安全和运营效率。受人类检查规程综合视觉识别和声学验证的启发，本研究提出了一种基于深度学习的机器人系统，用于自主检测和分类制造环境中气体泄漏和电弧放电。该系统设计为在机器人上独立执行所有实验任务。系统利用一个具有112个通道和96 kHz采样率的声学摄像机捕捉超声频率，处理在不同工业场景中记录的真实数据集。这些数据集包括多种气体泄漏配置（如针孔、敞开端）和不同类型的部分放电（如电晕放电、表面放电、悬浮放电），在各种环境噪声条件下进行。所提出的系统结合了视觉检测和基于波束形成增强的声学分析管道。信号使用STFT进行了变换并通过伽玛校正进行了细化，以实现稳健的特征提取。一种灵感源自Inception的卷积神经网络（CNN）进一步对危害进行分类，准确率达到99%的气体泄漏检测率。该系统不仅能检测单个危害源，还能通过融合来自视觉和声学传感器的多模态数据来提高分类可靠性。在具有回音和噪声增强的环境中测试时，该系统在某些方面优于传统模型，准确率最高可提高44%。所有实验任务都精心设计，以确保公平性和可重复性。此外，该系统还针对实时部署进行了优化，能够在移动机器人平台上实现每秒2.1秒的推理时间。通过模拟人类检查规程，并结合视觉与声学模态，本研究提供了一种有效的工业自动化解决方案，显著提高了安全性和操作可靠性。', 'title_zh': '基于深度学习的视觉-超声机器人系统及其在制造过程中爆炸气体和电弧危害检测中的应用'}
{'arxiv_id': 'arXiv:2502.05498', 'title': 'Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations', 'authors': 'Larkin Liu, Kashif Rasul, Yutong Chao, Jalal Etesami', 'link': 'https://arxiv.org/abs/2502.05498', 'abstract': "We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the Stackelberg manifold. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the Stackelberg manifold, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.", 'abstract_zh': '我们提出了一种在Stackelberg广义博弈中进行在线学习的新框架，该框架由领导者和追随者两个代理进行顺序交替交互组成。该方法的核心是通过学习的同胚映射将联合动作空间映射到一个光滑黎曼流形上，称为Stackelberg流形。借由神经归一化流这一手段，该映射确保生成了可处理的等平行子空间，从而能够高效地进行在线学习。通过假设Stackelberg流形上代理奖励函数之间的线性关系，我们的构造允许应用标准的多臂 bandit 算法。然后，我们为凸流形上的后悔最小化提供了严格的理论基础，并为学习Stackelberg均衡建立了有限时间的简单后悔界。将流形学习与博弈论结合，揭示了神经归一化流作为多代理学习有效工具的先前未知潜力。我们通过实证结果展示了与标准基线方法相比，该方法的有效性，并适用于诸如网络安全和经济供应链优化等领域的应用。', 'title_zh': '基于神经流表示的Stackelberg游戏黎曼流形学习'}
{'arxiv_id': 'arXiv:2502.05494', 'title': 'Multi-scale Masked Autoencoder for Electrocardiogram Anomaly Detection', 'authors': 'Ya Zhou, Yujie Yang, Jianhuang Gan, Xiangjie Li, Jing Yuan, Wei Zhao', 'link': 'https://arxiv.org/abs/2502.05494', 'abstract': 'Electrocardiogram (ECG) analysis is a fundamental tool for diagnosing cardiovascular conditions, yet anomaly detection in ECG signals remains challenging due to their inherent complexity and variability. We propose Multi-scale Masked Autoencoder for ECG anomaly detection (MMAE-ECG), a novel end-to-end framework that effectively captures both global and local dependencies in ECG data. Unlike state-of-the-art methods that rely on heartbeat segmentation or R-peak detection, MMAE-ECG eliminates the need for such pre-processing steps, enhancing its suitability for clinical deployment. MMAE-ECG partitions ECG signals into non-overlapping segments, with each segment assigned learnable positional embeddings. A novel multi-scale masking strategy and multi-scale attention mechanism, along with distinct positional embeddings, enable a lightweight Transformer encoder to effectively capture both local and global dependencies. The masked segments are then reconstructed using a single-layer Transformer block, with an aggregation strategy employed during inference to refine the outputs. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art approaches while significantly reducing computational complexity-approximately 1/78 of the floating-point operations (FLOPs) required for inference. Ablation studies further validate the effectiveness of each component, highlighting the potential of multi-scale masked autoencoders for anomaly detection.', 'abstract_zh': '心电图（ECG）分析是诊断心血管状况的基本工具，但由于ECG信号固有的复杂性和多变性，其异常检测仍然具有挑战性。我们提出了一种新颖的端到端框架——多尺度掩蔽自编码器用于ECG异常检测（MMAE-ECG），它能够有效地捕捉ECG数据中的全局和局部依赖关系。与依赖心跳分割或R-峰检测的现有方法不同，MMAE-ECG消除了这些预处理步骤，从而提高了其在临床部署中的适用性。MMAE-ECG将ECG信号划分为不重叠的段，并为每个段分配可学习的位置嵌入。一种新颖的多尺度掩蔽策略和多尺度注意力机制，以及独特的位置嵌入，使轻量级的Transformer编码器能够有效捕捉局部和全局依赖关系。掩蔽段随后通过一个单层Transformer块进行重构，在推理过程中采用聚合策略来细化输出。实验结果表明，与现有最先进的方法相比，我们的方法在性能上相当，但计算复杂度显著降低——推理所需的操作浮点数（FLOPs）大约减少到1/78。进一步的消融研究验证了各个组件的有效性，突显了多尺度掩蔽自编码器在异常检测中的潜力。', 'title_zh': '多尺度掩蔽自动编码器在心电信号异常检测中的应用'}
{'arxiv_id': 'arXiv:2502.05489', 'title': 'Mechanistic Interpretability of Emotion Inference in Large Language Models', 'authors': 'Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, Jonathan Gratch', 'link': 'https://arxiv.org/abs/2502.05489', 'abstract': 'Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.', 'abstract_zh': '大规模语言模型（LLMs）在从文本中预测人类情绪方面展示了令人鼓舞的能力。然而，这些模型处理情绪刺激的具体机制仍然 largely unexplored. 本研究通过探讨自回归LLMs如何推断情绪，表明情绪表征在模型中特定位点的功能化局部化。我们的评估涵盖了多种模型家族和规模，并且得到了稳健性检验的支持。然后，我们通过应用认知评估理论，一个广为认可的心理学框架，证明了识别出的情绪表征具有心理上的合理性，该理论认为情绪源自对环境刺激的评估（判断）。通过因果干预构想中的评估概念，我们引导生成过程，结果表明输出符合理论性和直观性的预期。本研究凸显了一种新型的方法，可以因果干预并精确塑造情感文本生成，这可能在敏感的情绪领域有助于提高安全性与一致性。', 'title_zh': '大型语言模型中情绪推断的机制可解释性'}
{'arxiv_id': 'arXiv:2502.05485', 'title': 'HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation', 'authors': 'Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memme, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, Ankit Goyal', 'link': 'https://arxiv.org/abs/2502.05485', 'abstract': "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results are provided at: this https URL", 'abstract_zh': '大型基础模型在视觉和语言领域展现了强大的开放世界泛化能力，但在机器人领域尚未达到类似的泛化水平。一个根本性的挑战在于机器人数据的缺乏，这些数据通常通过昂贵的在机器人上操作获得。一种有前景的解决方法是利用更为廉价的跨域数据，如无动作视频、手工绘制草图或仿真数据。在本文中，我们认为分层的视觉-语言-行动（VLA）模型比直接微调视觉-语言模型（VLMs）来预测行动的标准单一模型更为有效，可以更好地利用跨域数据。特别是，我们研究了一类分层VLA模型，在这类模型中，高层VLM被微调以生成给定RGB图像和任务描述时指示所需末端执行器轨迹的大致二维路径。然后，中间的二维路径预测作为指导传递给低层、三维感知的控制策略，该策略能够进行精确操作。这种设计减轻了高层VLM对细粒度动作预测的负担，同时减少了低层策略在复杂任务层面推理上的负担。我们表明，通过分层设计，高层VLM可以在显著的数据域差距之间进行泛化，包括不同的实体、动力学、视觉外观和任务语义等方面。在真实的机器人实验中，我们观察到在七个不同泛化轴上，与OpenVLA相比平均提高了20%的成功率，这代表了50%的相对提升。视觉结果请参阅：[这里](this https URL)', 'title_zh': 'HAMSTER：开放世界机器人操作的层次化动作模型'}
{'arxiv_id': 'arXiv:2502.05467', 'title': 'Position: LLMs Can be Good Tutors in Foreign Language Education', 'authors': 'Jingheng Ye, Shen Wang, Deqing Zou, Yibo Yan, Kun Wang, Hai-Tao Zheng, Zenglin Xu, Irwin King, Philip S. Yu, Qingsong Wen', 'link': 'https://arxiv.org/abs/2502.05467', 'abstract': 'While recent efforts have begun integrating large language models (LLMs) into foreign language education (FLE), they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in FLE. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing FLE through the thoughtful integration of LLMs.', 'abstract_zh': '尽管最近的努力已经开始将大型语言模型（LLMs）纳入外语教育（FLE），它们通常依赖于传统的学习方法，没有完全采用教育方法论，因而缺乏对语言学习的适应性。为了解决这一差距，我们主张LLMs有潜力作为有效的外语教育辅导工具。具体而言，LLMs可以扮演三个关键角色：（1）作为数据增强器，提高学习材料的创建或充当学生模拟；（2）作为任务预测器，充当学习者评估或优化学习路径；（3）作为代理，实现个性化和包容性教育。我们鼓励跨学科研究探索这些角色，促进创新并应对挑战和风险，最终通过精心整合LLMs推进外语教育的发展。', 'title_zh': '位置：大型语言模型可以成为外语教育中的良好辅导者'}
{'arxiv_id': 'arXiv:2502.05459', 'title': 'DCENWCNet: A Deep CNN Ensemble Network for White Blood Cell Classification with LIME-Based Explainability', 'authors': 'Sibasish Dhibar', 'link': 'https://arxiv.org/abs/2502.05459', 'abstract': "White blood cells (WBC) are important parts of our immune system, and they protect our body against infections by eliminating viruses, bacteria, parasites and fungi. The number of WBC types and the total number of WBCs provide important information about our health status. A traditional method, convolutional neural networks (CNN), a deep learning architecture, can classify the blood cell from a part of an object and perform object recognition. Various CNN models exhibit potential; however, their development often involves ad-hoc processes that neglect unnecessary layers, leading to issues with unbalanced datasets and insufficient data augmentation. To address these challenges, we propose a novel ensemble approach that integrates three CNN architectures, each uniquely configured with different dropout and max-pooling layer settings to enhance feature learning. This ensemble model, named DCENWCNet, effectively balances the bias-variance trade-off. When evaluated on the widely recognized Rabbin-WBC dataset, our model outperforms existing state-of-the-art networks, achieving highest mean accuracy. Additionally, it demonstrates superior performance in precision, recall, F1-score, and Area Under the ROC Curve (AUC) across all categories. To delve deeper into the interpretability of classifiers, we employ reliable post-hoc explanation techniques, including Local Interpretable Model-Agnostic Explanations (LIME). These methods approximate the behavior of a black-box model by elucidating the relationships between feature values and predictions. Interpretable results enable users to comprehend and validate the model's predictions, thereby increasing their confidence in the automated diagnosis.", 'abstract_zh': '白细胞（WBC）是我们免疫系统的重要组成部分，它们通过清除病毒、细菌、寄生虫和真菌来保护我们的身体免受感染。WBC种类的数量和总数量提供了关于我们健康状况的重要信息。传统方法，卷积神经网络（CNN），一种深度学习架构，可以从对象的一部分识别血细胞并进行目标识别。各种CNN模型显示出潜力，但其开发过程中往往涉及不规范的过程，忽略了不必要的层，导致不平衡数据集和数据增强不足的问题。为解决这些挑战，我们提出了一种新颖的集成方法，该方法结合了三种CNN架构，每种架构以不同的dropout和最大池化层配置独特地构建，以增强特征学习。该集成模型称为DCENWCNet，有效地平衡了偏差-方差之间的贸易。在广受认可的Rabbin-WBC数据集上进行评估时，我们的模型优于现有的先进网络，实现了最高的平均准确率。此外，在所有类别中，它在精确率、召回率、F1分数和ROC曲线下面积（AUC）方面也表现出了优越性能。为了更深入地探讨分类器的可解释性，我们采用了可靠的后置解释技术，包括局部可解释的模型无关解释（LIME）。这些方法通过阐明特征值与预测之间的关系来近似黑盒模型的行为。可解释的结果使用户能够理解并验证模型的预测，从而增加他们对自动化诊断的信心。', 'title_zh': 'DCENWCNet：基于LIME解释性的深度CNN集成网络在白细胞分类中的应用'}
{'arxiv_id': 'arXiv:2502.05450', 'title': 'ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy', 'authors': 'Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, Dongbin Zhao', 'link': 'https://arxiv.org/abs/2502.05450', 'abstract': 'Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications.', 'abstract_zh': '视觉-语言-行动（VLA）模型在现实中的机器人操作中展现了巨大的潜力。然而，通过监督学习进行精细化调整的方法由于演示数据有限且不一致，特别是在接触丰富的环境中，难以实现稳健的性能。在本文中，我们提出了一种强化学习驱动的精细化调整方法，命名为ConRFT，该方法通过统一的基于一致性的训练目标，在离线和在线阶段进行精细化调整，以解决这些挑战。在离线阶段，我们的方法结合行为克隆和Q学习，有效从少量演示数据中提取策略，并稳定价值估计。在线阶段，进一步通过一致性策略进行精细化调整，并结合人类干预确保安全探索和高效样本利用率。我们在八个不同的真实世界操作任务上评估了该方法。在线精细化调整45-90分钟内，方法的平均成功率达到了96.3%，相较于之前的监督学习方法，成功率提高了144%，取样长度缩短了1.9倍。本工作强调了将强化学习集成到VLA模型以提升其现实应用性能的潜力。', 'title_zh': 'ConRFT：基于一致性策略的VLA模型强化微调方法'}
{'arxiv_id': 'arXiv:2502.05449', 'title': 'Iterative Deepening Sampling for Large Language Models', 'authors': 'Weizhe Chen, Sven Koenig, Bistra Dilkina', 'link': 'https://arxiv.org/abs/2502.05449', 'abstract': "The recent release of OpenAI's o1 models and other similar frameworks showcasing test-time scaling laws has demonstrated their exceptional capability to tackle complex reasoning tasks. Inspired by this, subsequent research has revealed that such test-time scaling laws hinge on the model's ability to search both within a single response (intra-response) and across multiple responses (inter-response) during training. Crucially, beyond selecting a single optimal response, the model must also develop robust self-correction capabilities within its own outputs. However, training models to achieve effective self-evaluation and self-correction remains a significant challenge, heavily dependent on the quality of self-reflection data. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving, which can subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how manually triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.", 'abstract_zh': '近年来，OpenAI的O1模型及其他类似框架的发布展示了它们在应对复杂推理任务方面的出色能力，并揭示了测试时缩放定律的存在。受此启发，后续研究揭示，这些测试时缩放定律的关键在于模型在训练过程中不仅要在一个响应内部（即，在单一响应内外部）搜索，还要在多个响应之间进行搜索。除此之外，模型还必须在其自身输出中具备强大的自校正能力，而不仅仅是选择最优响应。然而，训练模型以实现有效的自我评估和自我校正仍然是一个重大挑战，高度依赖于高质量的自我反思数据。本文通过聚焦于提升复杂问题解决中的自我反思数据生成质量来应对这一挑战，并旨在改进下一代大型语言模型（LLMs）的训练。具体而言，我们探讨了手动触发模型的自我校正机制如何提高复杂推理任务的性能。为此，我们提出了一种新型递归深化采样算法框架，旨在增强自我校正并生成更高质量的样本。通过在Math500和AIME基准测试上的广泛实验，我们展示了该方法在困难任务中的较高成功率，并通过详细的消融研究分析了其在不同环境下的有效性。', 'title_zh': '大型语言模型中的迭代加深采样方法'}
{'arxiv_id': 'arXiv:2502.05435', 'title': 'Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning', 'authors': 'Manh Luong, Khai Nguyen, Dinh Phung, Gholamreza Haffari, Lizhen Qu', 'link': 'https://arxiv.org/abs/2502.05435', 'abstract': 'Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy.', 'abstract_zh': '教师强迫训练在音频描述生成中通常会导致由于训练和推断不匹配而产生的曝光偏差。此前的工作提出了对比方法来应对描述退化的问题。然而，对比方法在度量声学和语言模态间相似性时忽略了时间信息，导致性能不佳。本文中，我们通过引入带旋转位置嵌入的无偏差切片Wasserstein RBF（USW-RBF）核来开发时间相似度得分，以考虑跨模态的时间信息。与传统的切片Wasserstein RBF核不同，我们可以通过蒙特卡罗估计形成USW-RBF核的无偏差估计。因此，它非常适合于随机梯度优化算法，并且其逼近误差以参数速率$\\mathcal{O}(L^{-1/2})$随$L$个蒙特卡罗样本的增加而减少。此外，我们基于无偏差切片Wasserstein核引入了音频描述框架，并结合随机解码方法在生成过程中减轻描述退化问题。我们通过两个数据集（AudioCaps和Clotho）进行了广泛的定量和定性实验，以证明生成高质量音频描述的能力。实验结果表明，我们的框架能够增加描述长度、词汇多样性以及文本到音频的自检索精度。', 'title_zh': '无偏切片 Wasserstein 核在高质音频字幕生成中的应用'}
{'arxiv_id': 'arXiv:2502.05431', 'title': 'APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding', 'authors': 'Xinyu Yang, Tianqi Chen, Beidi Chen', 'link': 'https://arxiv.org/abs/2502.05431', 'abstract': "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding ($\\textbf{APE}$), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$ speedup by reducing 28$\\times$ prefilling time for a 128K-length context.", 'abstract_zh': '上下文增强生成（CAG）技术，包括检索增强生成（RAG）和实例级提示（ICL），需要有效地组合多个上下文以生成用户查询的响应。直接将这些上下文按序列输入会因每次请求重新编码组合选择的上下文而导致显著的计算负担。为解决这一问题，我们探索了并行编码的潜在优势，通过独立预先计算并缓存每个上下文的KV状态。这种方法在推理过程中可以直接加载缓存的状态，同时通过在上下文之间复用位置信息来容纳更多的上下文。然而，由于注意力分布的对齐问题，直接应用并行编码会导致显著的性能下降。为了实现有效的和高效的CAG，我们提出了自适应并行编码（Adaptive Parallel Encoding，APE），并在并行编码中引入了共享前缀、注意力温度和缩放因子，以调整并行编码和序列编码之间的分布。在RAG和ICL任务上的结果显示，APE在使用相同输入时可以保持98%和93%的序列编码性能，同时分别比并行编码高出3.6%和7.9%的性能。此外，APE还可以扩展到多样本CAG，有效地并行编码数百个上下文。效率评估显示，APE可以通过减少128K长度上下文的28倍预填充时间，实现端到端4.5倍的速度提升。', 'title_zh': 'APE：通过自适应并行编码实现更快更长上下文增强生成'}
{'arxiv_id': 'arXiv:2502.05424', 'title': 'SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation', 'authors': 'Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang', 'link': 'https://arxiv.org/abs/2502.05424', 'abstract': 'Graphs are able to model interconnected entities in many online services, supporting a wide range of applications on the Web. This raises an important question: How can we train a graph foundational model on multiple source domains and adapt to an unseen target domain? A major obstacle is that graphs from different domains often exhibit divergent characteristics. Some studies leverage large language models to align multiple domains based on textual descriptions associated with the graphs, limiting their applicability to text-attributed graphs. For text-free graphs, a few recent works attempt to align different feature distributions across domains, while generally neglecting structural differences. In this work, we propose a novel Structure Alignment framework for text-free Multi-domain Graph Pre-Training and cross-domain adaptation (SAMGPT). It is designed to learn multi-domain knowledge from graphs originating in multiple source domains, which can then be adapted to address applications in an unseen target domain. Specifically, we introduce a set of structure tokens to harmonize structure-based aggregation across source domains during the pre-training phase. Next, for cross-domain adaptation, we design dual prompts, namely, holistic prompts and specific prompts, which adapt unified multi-domain structural knowledge and fine-grained, domain-specific information, respectively, to a target domain. Finally, we conduct comprehensive experiments on seven public datasets to evaluate and analyze the effectiveness of SAMGPT.', 'abstract_zh': '图能够在线服务中用来建模相互关联的实体，支持广泛的应用场景。这引发了一个重要的问题：如何在多个源域上训练图基础模型，并适应未见过的目标域？主要障碍在于不同领域的图往往表现出不同的特征。一些研究通过利用大量语言模型基于与图关联的文本描述来进行多域对齐，但这种方法的适用范围仅限于带有文本属性的图。对于无文本的图，近年来有一些工作尝试在不同领域之间对齐特征分布，但通常忽略结构差异。在这项工作中，我们提出了一种新的结构对齐（Structure Alignment）框架，用于无文本多领域图预训练及跨域适应（SAMGPT）。SAMGPT旨在从多个源域的图中学习多领域知识，然后适应应用于未见过的目标域。具体来说，我们引入了一组结构标记，在预训练阶段协调源域中的结构聚合。接下来，在跨域适应阶段，我们设计了双提示，即整体提示和具体提示，分别将统一的多领域结构知识和细粒度的领域特定信息适应到目标域。最后，我们在七个公开数据集上进行了全面的实验，以评估和分析SAMGPT的效果。', 'title_zh': 'SAMGPT：无需文本的图基础模型多域预训练与跨域适应'}
{'arxiv_id': 'arXiv:2502.05415', 'title': 'Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation', 'authors': 'Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, Zhijie Deng', 'link': 'https://arxiv.org/abs/2502.05415', 'abstract': 'There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance. The code is available at this https URL.', 'abstract_zh': '近年来，统一多模态理解与生成模型的研究逐渐成为热点，其中Show-o作为一个显著的代表，展示了其在图文生成方面的巨大潜力，无论是从文本生成图片还是从图片生成文本。Show-o的推理过程涉及逐步去噪图像标记并自回归解码文本标记，因此从两个方面都存在效率问题。本文提出Show-o Turbo以解决这一问题。我们首先基于Show-o中文本标记并行解码的视角，为其图像和文本的生成提供了一个统一的去噪观点。然后，我们提出了将一致性蒸馏（Consistency Distillation，CD），一种用于缩短扩散模型去噪过程的有效方法，应用于Show-o的多模态去噪轨迹。我们引入了一条轨迹分割策略和一种逐步学习过程来改善训练收敛性。实验结果表明，在文本生成图片的任务中，无需使用条件提示去噪（Classifier-Free Guidance，CFG），Show-o Turbo在4步采样中的GenEval得分为0.625，优于原始的8步Show-o和CFG方法；在图片生成文本的任务中，Show-o Turbo展现了1.5倍的加速，同时在性能上几乎没有显著的牺牲。代码可在以下链接获取：[链接]。', 'title_zh': 'Show-o Turbo: 向统一多模态理解与生成的加速方向迈进'}
{'arxiv_id': 'arXiv:2502.05409', 'title': 'Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment', 'authors': 'Maneesha Wickramasuriya, Beomyeol Yu, Taeyoung Lee, Murray Snyder', 'link': 'https://arxiv.org/abs/2502.05409', 'abstract': 'This paper proposes a vision-in-the-loop simulation environment for deep monocular pose estimation of a UAV operating in an ocean environment. Recently, a deep neural network with a transformer architecture has been successfully trained to estimate the pose of a UAV relative to the flight deck of a research vessel, overcoming several limitations of GPS-based approaches. However, validating the deep pose estimation scheme in an actual ocean environment poses significant challenges due to the limited availability of research vessels and the associated operational costs. To address these issues, we present a photo-realistic 3D virtual environment leveraging recent advancements in Gaussian splatting, a novel technique that represents 3D scenes by modeling image pixels as Gaussian distributions in 3D space, creating a lightweight and high-quality visual model from multiple viewpoints. This approach enables the creation of a virtual environment integrating multiple real-world images collected in situ. The resulting simulation enables the indoor testing of flight maneuvers while verifying all aspects of flight software, hardware, and the deep monocular pose estimation scheme. This approach provides a cost-effective solution for testing and validating the autonomous flight of shipboard UAVs, specifically focusing on vision-based control and estimation algorithms.', 'abstract_zh': '本文提出了一种视图在环的仿真环境，用于在海洋环境中操作的无人机的深度单目姿态估计。最近，采用Transformer架构的深度神经网络已被成功训练，用于估算无人机相对于研究船舶飞行甲板的姿态，克服了基于GPS方法的多个局限性。然而，在实际海洋环境中验证深度姿态估计方案面临着重大挑战，主要由于研究船舶的有限可用性和相关的运营成本。为了解决这些问题，我们提出了一种逼真的3D虚拟环境，利用了最近在Gaussian splatting方面的进展。Gaussian splatting是一种新颖的技术，将3D场景中的图像像素表示为三维空间中的高斯分布，从而从多个视角创建轻量级且质量高的视觉模型。这种方法使得可以创建一个集成多个现场采集的真实世界图像的虚拟环境。通过这种方式生成的仿真能够在室内测试飞行机动性，验证飞行软件、硬件以及深度单目姿态估计方案的所有方面。这种方法为测试和验证船载无人机的自主飞行提供了一种低成本的解决方案，特别关注基于视觉的控制和估计算法。', 'title_zh': '基于视觉反馈的无人机海洋环境单目姿态估计仿真研究'}
{'arxiv_id': 'arXiv:2502.05407', 'title': 'The Complexity of Learning Sparse Superposed Features with Feedback', 'authors': 'Akash Kumar', 'link': 'https://arxiv.org/abs/2502.05407', 'abstract': "The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \\textit{triplet comparisons}. These features may represent various constructs, including dictionaries in LLMs or components of a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models.", 'abstract_zh': '深度网络的成功主要归因于其在表示空间中捕捉潜在特征的能力。在本文中，我们探讨是否可以通过代理（如大型语言模型LLM）提供的相对三元比较反馈高效地检索模型中学习到的特征。这些特征可以代表各种结构，包括LLM中的词典或马氏距离协方差矩阵的组件。我们分析在稀疏设置下学习特征矩阵所需的反馈复杂性。当代理被允许构造激活时，我们的结果建立了紧致的边界；在代理反馈受限于分布信息的稀疏场景中，我们展示了强大的上界。我们通过两个不同的应用场景验证了理论发现：从递归特征机器训练模型中恢复特征以及从大规模语言模型训练的稀疏自编码器中提取词典。', 'title_zh': '学习具有反馈的稀疏叠加特征的复杂性'}
{'arxiv_id': 'arXiv:2502.05402', 'title': 'Convolutional Deep Colorization for Image Compression: A Color Grid Based Approach', 'authors': 'Ian Tassin, Kristen Goebel, Brittany Lasher', 'link': 'https://arxiv.org/abs/2502.05402', 'abstract': 'The search for image compression optimization techniques is a topic of constant interest both in and out of academic circles. One method that shows promise toward future improvements in this field is image colorization since image colorization algorithms can reduce the amount of color data that needs to be stored for an image. Our work focuses on optimizing a color grid based approach to fully-automated image color information retention with regard to convolutional colorization network architecture for the purposes of image compression. More generally, using a convolutional neural network for image re-colorization, we want to minimize the amount of color information that is stored while still being able to faithfully re-color images. Our results yielded a promising image compression ratio, while still allowing for successful image recolorization reaching high CSIM values.', 'abstract_zh': '图像压缩优化技术的研究一直是学术界和社会广泛关注的课题。一种在未来改进这一领域的方法是图像着色技术，因为图像着色算法可以通过减少需要存储的颜色数据量来优化图像压缩。我们的工作集中于优化基于颜色网格的方法，以在卷积色化网络架构下全面自动保留图像颜色信息，从而实现图像压缩。更广泛地说，我们使用卷积神经网络进行图像再着色，目的是在尽量减少颜色信息存储量的同时，仍然能够忠实复原图像颜色。我们的研究结果取得了令人鼓舞的图像压缩比率，同时成功实现了高质量的图像再着色，获得了较高的CSIM值。', 'title_zh': '基于颜色网格的卷积深层着色方法：用于图像压缩的色彩化技术'}
{'arxiv_id': 'arXiv:2502.05387', 'title': 'Coarse-to-Fine Structure-Aware Artistic Style Transfer', 'authors': 'Kunxiao Liu, Guowu Yuan, Hao Wu, Wenhua Qian', 'link': 'https://arxiv.org/abs/2502.05387', 'abstract': 'Artistic style transfer aims to use a style image and a content image to synthesize a target image that retains the same artistic expression as the style image while preserving the basic content of the content image. Many recently proposed style transfer methods have a common problem; that is, they simply transfer the texture and color of the style image to the global structure of the content image. As a result, the content image has a local structure that is not similar to the local structure of the style image. In this paper, we present an effective method that can be used to transfer style patterns while fusing the local style structure into the local content structure. In our method, dif-ferent levels of coarse stylized features are first reconstructed at low resolution using a Coarse Network, in which style color distribution is roughly transferred, and the content structure is combined with the style structure. Then, the reconstructed features and the content features are adopted to synthesize high-quality structure-aware stylized images with high resolution using a Fine Network with three structural selective fusion (SSF) modules. The effectiveness of our method is demonstrated through the generation of appealing high-quality stylization results and a com-parison with some state-of-the-art style transfer methods.', 'abstract_zh': '艺术风格迁移旨在使用风格图像和内容图像合成一个目标图像，该图像保留风格图像的艺术表现力，同时保留内容图像的基本内容。许多最近提出的方法都存在一个共同的问题，即它们简单地将风格图像的纹理和颜色转移至内容图像的全局结构中。因此，内容图像的局部结构与风格图像的局部结构并不相似。本文提出了一种有效的方法，可以在保留局部内容结构的前提下融合局部风格结构。在我们的方法中，首先使用粗略网络（Coarse Network）在低分辨率下重建不同层次的粗略风格特征，其中粗略的颜色分布被转移，并将内容结构与风格结构结合。随后，这些重建的特征与内容特征一起在具有三个结构选择性融合（SSF）模块的精细网络（Fine Network）中被采用，用于合成具有高分辨率和结构意识的高质量风格化图像。通过生成引人注目的高质量风格化结果以及与其他最先进的风格迁移方法进行比较，证明了本文方法的有效性。', 'title_zh': '从粗到细的结构感知艺术风格迁移'}
{'arxiv_id': 'arXiv:2502.05383', 'title': 'Is attention all you need to solve the correlated electron problem?', 'authors': 'Max Geier, Khachatur Nazaryan, Timothy Zaklama, Liang Fu', 'link': 'https://arxiv.org/abs/2502.05383', 'abstract': 'The attention mechanism has transformed artificial intelligence research by its ability to learn relations between objects. In this work, we explore how a many-body wavefunction ansatz constructed from a large-parameter self-attention neural network can be used to solve the interacting electron problem in solids. By a systematic neural-network variational Monte Carlo study on a moiré quantum material, we demonstrate that the self-attention ansatz provides an accurate, efficient, and unbiased solution. Moreover, our numerical study finds that the required number of variational parameters scales roughly as $N^2$ with the number of electrons, which opens a path towards efficient large-scale simulations.', 'abstract_zh': '注意力机制通过学习对象之间的关系，已经转变了人工智能研究的方向。在本工作中，我们探索了如何使用一个由大量参数的自我注意力神经网络构建的多体波函数近似表达式来解决固体中的相互作用电子问题。通过在莫尔量子材料上进行系统性的神经网络变分蒙特卡洛研究，我们展示了自我注意力近似表达式能够提供一个准确、高效且无偏的解决方案。此外，我们的数值研究表明，所需变分参数的数量大约与电子数目平方成正比，这为高效的大规模模拟开辟了途径。', 'title_zh': '你提到的标题是关于物理和材料科学中的量子多体问题，可以翻译成中文如下，同时符合学术规范：\n\n你真的认为注意力机制足以解决相关电子问题吗？'}
{'arxiv_id': 'arXiv:2502.05370', 'title': 'fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving', 'authors': 'Hanfei Yu, Xingqi Cui, Hong Zhang, Hao Wang, Hao Wang', 'link': 'https://arxiv.org/abs/2502.05370', 'abstract': 'Large Language Models (LLMs) have gained immense success in revolutionizing various applications, including content generation, search and recommendation, and AI-assisted operation. To reduce high training costs, Mixture-of-Experts (MoE) architecture has become a popular backbone for modern LLMs. However, despite the benefits, serving MoE-based LLMs experience severe memory inefficiency due to sparsely activated experts. Recent studies propose to offload inactive experts from GPU memory to CPU memory to improve the serving efficiency of MoE models. However, they either incur high inference latency or high model memory footprints due to coarse-grained designs. To tame the latency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert offloading system for MoE serving that achieves low inference latency with memory efficiency. We design fMoE to extract fine-grained expert selection patterns from MoE models and semantic hints from input prompts to efficiently guide expert prefetching, caching, and offloading decisions. fMoE is prototyped on top of HuggingFace Transformers and deployed on a six-GPU testbed. Experiments with open-source MoE models and real-world workloads show that fMoE reduces inference latency by 47% and improves expert hit rate by 36% over state-of-the-art solutions.', 'abstract_zh': '大型语言模型（LLMs）在内容生成、搜索与推荐以及AI辅助操作等领域取得了巨大成功。为了降低高昂的训练成本，混合专家（Mixture-of-Experts，MoE）架构已成为现代LLMs的主要支撑框架之一。然而，尽管MoE架构具有诸多优势，但服务基于MoE的LLMs会遇到严重的内存效率问题，主要是因为激活的专家较少。近期的研究提出将不活跃的专家从GPU内存卸载到CPU内存，以提高MoE模型的服务效率。然而，这些方法要么引入了较高的推理延迟，要么由于粗粒度设计导致较高的模型内存消耗。为解决MoE服务中的延迟-内存权衡问题，我们提出了fMoE，这是一种细粒度的专家卸载系统，能够在保持低推理延迟的同时实现内存效率。我们设计fMoE以从MoE模型中提取细粒度的专家选择模式，并结合输入提示的语义线索，高效指导专家的预取、缓存和卸载决策。fMoE基于HuggingFace Transformers原型实现，并部署在一个六块GPU的测试平台上。实验表明，与现有的最佳解决方案相比，fMoE将推理延迟降低了47%，并且将专家命中率提高了36%。', 'title_zh': 'fMoE: 细粒度专家卸载技术用于大规模混合专家服务'}
{'arxiv_id': 'arXiv:2502.05345', 'title': 'Estimating Voltage Drop: Models, Features and Data Representation Towards a Neural Surrogate', 'authors': 'Yifei Jin, Dimitrios Koutlis, Hector Bandala, Marios Daoutis', 'link': 'https://arxiv.org/abs/2502.05345', 'abstract': "Accurate estimation of voltage drop (IR drop) in modern Application-Specific Integrated Circuits (ASICs) is highly time and resource demanding, due to the growing complexity and the transistor density in recent technology nodes. To mitigate this challenge, we investigate how Machine Learning (ML) techniques, including Extreme Gradient Boosting (XGBoost), Convolutional Neural Network (CNN), and Graph Neural Network (GNN) can aid in reducing the computational effort and implicitly the time required to estimate the IR drop in Integrated Circuits (ICs). Traditional methods, including commercial tools, require considerable time to produce accurate approximations, especially for complicated designs with numerous transistors. ML algorithms, on the other hand, are explored as an alternative solution to offer quick and precise IR drop estimation, but in considerably less time. Our approach leverages ASICs' electrical, timing, and physical to train ML models, ensuring adaptability across diverse designs with minimal adjustments. Experimental results underscore the superiority of ML models over commercial tools, greatly enhancing prediction speed. Particularly, GNNs exhibit promising performance with minimal prediction errors in voltage drop estimation. The incorporation of GNNs marks a groundbreaking advancement in accurate IR drop prediction. This study illustrates the effectiveness of ML algorithms in precisely estimating IR drop and optimizing ASIC sign-off. Utilizing ML models leads to expedited predictions, reducing calculation time and improving energy efficiency, thereby reducing environmental impact through optimized power circuits.", 'abstract_zh': '现代应用特定集成电路（ASIC）中的电压降（IR降）的准确估计由于其日益增长的复杂性和最近技术节点的晶体管密度，对时间和资源的需求非常高。为了缓解这一挑战，我们研究了如何利用机器学习（ML）技术，包括极端梯度提升（XGBoost）、卷积神经网络（CNN）和图神经网络（GNN），以减少电压降估计所需的计算努力和时间。传统的估计方法，包括商业工具，需要较长的时间才能提供准确的近似值，尤其是在具有大量晶体管的复杂设计中。相比之下，ML算法被探索作为一种快速且准确的替代方案来估计IR降，但所需时间明显减少。我们的方法通过利用ASIC的电气、时序和物理特性来训练ML模型，确保在最小调整的情况下适应多种不同的设计。实验结果表明，ML模型在预测速度方面优于商业工具，显著提高了预测速度。特别是在电压降估计中，GNNs显示出最小预测误差的良好性能。GNNs的引入标志着在准确IR降预测方面取得了革命性的进展。本研究表明，ML算法在精确估计IR降和优化ASIC签发方面具有显著效果。利用ML模型可以实现加速预测，减少计算时间和提高能源效率，从而通过优化电源电路减少环境影响。', 'title_zh': '电压降估算：基于神经拟态代理的模型、特征和数据表示研究'}
{'arxiv_id': 'arXiv:2502.05344', 'title': 'RAG-Verus: Repository-Level Program Verification with LLMs using Retrieval Augmented Generation', 'authors': 'Sicheng Zhong, Jiading Zhu, Yifang Tian, Xujie Si', 'link': 'https://arxiv.org/abs/2502.05344', 'abstract': 'Scaling automated formal verification to real-world projects requires resolving cross-module dependencies and global contexts, which are challenges overlooked by existing function-centric methods. We introduce RagVerus, a framework that synergizes retrieval-augmented generation with context-aware prompting to automate proof synthesis for multi-module repositories, achieving a 27% relative improvement on our novel RepoVBench benchmark -- the first repository-level dataset for Verus with 383 proof completion tasks. RagVerus triples proof pass rates on existing benchmarks under constrained language model budgets, demonstrating a scalable and sample-efficient verification.', 'abstract_zh': '将自动化形式化验证扩展到真实项目中需要解决跨模块依赖和全局上下文问题，而现有以函数为中心的方法忽视了这些问题。我们介绍了一种名为RagVerus的框架，该框架结合了检索增强生成与上下文意识提示，以自动化多模块仓库的证明合成。RagVerus在我们建立的新型RepoVBench基准测试中实现了27%的相对改进——这是首个针对Verus的仓库级数据集，包含383个证明完成任务。在受限的语言模型预算下，RagVerus将现有基准测试中的证明通过率提高了三倍，展示了其可扩展性和样本高效性。', 'title_zh': 'RAG-Verus：利用检索增强生成在代码仓库级别进行程序验证'}
{'arxiv_id': 'arXiv:2502.05330', 'title': 'Multi-Class Segmentation of Aortic Branches and Zones in Computed Tomography Angiography: The AortaSeg24 Challenge', 'authors': 'Muhammad Imran, Jonathan R. Krebs, Vishal Balaji Sivaraman, Teng Zhang, Amarjeet Kumar, Walker R. Ueland, Michael J. Fassler, Jinlong Huang, Xiao Sun, Lisheng Wang, Pengcheng Shi, Maximilian Rokuss, Michael Baumgartner, Yannick Kirchhof, Klaus H. Maier-Hein, Fabian Isensee, Shuolin Liu, Bing Han, Bong Thanh Nguyen, Dong-jin Shin, Park Ji-Woo, Mathew Choi, Kwang-Hyun Uhm, Sung-Jea Ko, Chanwoong Lee, Jaehee Chun, Jin Sung Kim, Minghui Zhang, Hanxiao Zhang, Xin You, Yun Gu, Zhaohong Pan, Xuan Liu, Xiaokun Liang, Markus Tiefenthaler, Enrique Almar-Munoz, Matthias Schwab, Mikhail Kotyushev, Rostislav Epifanov, Marek Wodzinski, Henning Muller, Abdul Qayyum, Moona Mazher, Steven A. Niederer, Zhiwei Wang, Kaixiang Yang, Jintao Ren, Stine Sofia Korreman, Yuchong Gao, Hongye Zeng, Haoyu Zheng, Rui Zheng, Jinghua Yue, Fugen Zhou, Bo Liu, Alexander Cosman, Muxuan Liang, Chang Zhao, Gilbert R. Upchurch Jr., Jun Ma, Yuyin Zhou, Michol A. Cooper, Wei Shao', 'link': 'https://arxiv.org/abs/2502.05330', 'abstract': 'Multi-class segmentation of the aorta in computed tomography angiography (CTA) scans is essential for diagnosing and planning complex endovascular treatments for patients with aortic dissections. However, existing methods reduce aortic segmentation to a binary problem, limiting their ability to measure diameters across different branches and zones. Furthermore, no open-source dataset is currently available to support the development of multi-class aortic segmentation methods. To address this gap, we organized the AortaSeg24 MICCAI Challenge, introducing the first dataset of 100 CTA volumes annotated for 23 clinically relevant aortic branches and zones. This dataset was designed to facilitate both model development and validation. The challenge attracted 121 teams worldwide, with participants leveraging state-of-the-art frameworks such as nnU-Net and exploring novel techniques, including cascaded models, data augmentation strategies, and custom loss functions. We evaluated the submitted algorithms using the Dice Similarity Coefficient (DSC) and Normalized Surface Distance (NSD), highlighting the approaches adopted by the top five performing teams. This paper presents the challenge design, dataset details, evaluation metrics, and an in-depth analysis of the top-performing algorithms. The annotated dataset, evaluation code, and implementations of the leading methods are publicly available to support further research. All resources can be accessed at this https URL.', 'abstract_zh': '多类分割技术在计算机断层血管造影（CTA）扫描中对于诊断和计划主动脉夹层患者的复杂血管内治疗至关重要。然而，现有的方法将主动脉分割简化为二元问题，限制了其在不同分支和区域测量直径的能力。此外，目前尚无开源数据集可供支持多类主动脉分割方法的发展。为了解决这一问题，我们组织了AortaSeg24 MICCAI挑战，首次引入了一个包含100个CTA体积且标注有23个临床相关主动脉分支和区域的首个数据集。该数据集旨在促进模型的开发与验证。来自全球的121支队伍参与了此次挑战，参赛者利用了最新的框架（如nnU-Net）并探索了新颖的技术，包括级联模型、数据扩增策略及自定义损失函数。我们使用Dice相似性系数（DSC）和归一化表面积距离（NSD）对提交的算法进行了评估，重点展示了前五名表现最佳团队所采取的方法。本文介绍了挑战的设计、数据集的详细信息、评价指标以及对表现最佳算法的深入分析。已标注的数据集、评价代码及顶级方法的实现均已公开，可供进一步研究使用。所有资源均可通过以下链接访问：[提供链接的URL]。', 'title_zh': '计算机断层扫描血管造影中主动脉分支和区域的多分类分割：AortaSeg24挑战赛'}
{'arxiv_id': 'arXiv:2502.05312', 'title': 'Towards the Development of Balanced Synthetic Data for Correcting Grammatical Errors in Arabic: An Approach Based on Error Tagging Model and Synthetic Data Generating Model', 'authors': 'Ahlam Alrehili, Areej Alhothali', 'link': 'https://arxiv.org/abs/2502.05312', 'abstract': 'Synthetic data generation is widely recognized as a way to enhance the quality of neural grammatical error correction (GEC) systems. However, current approaches often lack diversity or are too simplistic to generate the wide range of grammatical errors made by humans, especially for low-resource languages such as Arabic. In this paper, we will develop the error tagging model and the synthetic data generation model to create a large synthetic dataset in Arabic for grammatical error correction. In the error tagging model, the correct sentence is categorized into multiple error types by using the DeBERTav3 model. Arabic Error Type Annotation tool (ARETA) is used to guide multi-label classification tasks in an error tagging model in which each sentence is classified into 26 error tags. The synthetic data generation model is a back-translation-based model that generates incorrect sentences by appending error tags before the correct sentence that was generated from the error tagging model using the ARAT5 model. In the QALB-14 and QALB-15 Test sets, the error tagging model achieved 94.42% F1, which is state-of-the-art in identifying error tags in clean sentences. As a result of our syntactic data training in grammatical error correction, we achieved a new state-of-the-art result of F1-Score: 79.36% in the QALB-14 Test set. We generate 30,219,310 synthetic sentence pairs by using a synthetic data generation model.', 'abstract_zh': '合成数据生成被广泛认为是提升神经语法纠错（GEC）系统质量的一种方式。然而，当前的方法往往缺乏多样性，或者过于简单，无法生成人类所犯的广泛语法错误，尤其是在阿拉伯等低资源语言中。在本文中，我们将开发错误标注模型和合成数据生成模型，以创建大量的阿拉伯合成数据集用于语法纠错。在错误标注模型中，正确句子被DeBERTa v3模型分类为多种错误类型。我们使用阿拉伯错误类型注释工具（ARETA）来指导错误标注模型中的多标签分类任务，其中每个句子被分类为26个错误标签。合成数据生成模型是一种基于反向翻译的模型，它通过在使用ARAT5模型生成的正确句子之前添加错误标签，生成错误句子。在QALB-14和QALB-15测试集中，错误标注模型的F1分数达到了94.42%，这是识别干净句子中的错误标签的最新技术水平。通过我们的句法数据训练，我们在QALB-14测试集中取得了新的F1分数最佳结果：79.36%。我们通过合成数据生成模型生成了30,219,310对合成句子对。', 'title_zh': '面向阿拉伯语语法错误纠正的平衡合成数据发展：基于错误标注模型和合成数据生成模型的方法'}
{'arxiv_id': 'arXiv:2502.05310', 'title': 'Oracular Programming: A Modular Foundation for Building LLM-Enabled Software', 'authors': 'Jonathan Laurent, André Platzer', 'link': 'https://arxiv.org/abs/2502.05310', 'abstract': 'Large Language Models have proved surprisingly effective at solving a wide range of tasks from just a handful of examples. However, their lack of reliability and modularity limits their capacity to tackle large problems that require many steps of reasoning. In response, researchers have proposed advanced pipelines that leverage domain-specific knowledge to chain smaller prompts, provide intermediate feedback and improve performance through search. However, the current complexity of writing, tuning, maintaining and improving such pipelines has limited their sophistication. We propose oracular programming, a foundational paradigm for building LLM-enabled applications that lets domain experts express high-level problem-solving strategies as programs with unresolved choice points. These choice points are resolved at runtime by LLMs, which generalize from user-provided examples of correct and incorrect decisions. An oracular program is composed of three orthogonal components: a strategy that consists in a nondeterministic program with choice points that can be reified into a search tree, a policy that specifies how to navigate this tree with the help of LLM oracles, and a set of demonstrations that describe successful and unsuccessful search tree navigation scenarios across diverse problem instances. Each component is expressed in a dedicated programming language and can be independently improved or substituted. We address the key programming language design challenges of modularly composing oracular programs and enforcing consistency between their components as they evolve.', 'abstract_zh': '大型语言模型在从少量示例中解决广泛任务方面表现出令人惊讶的效果。然而，它们的可靠性和模块性限制了它们解决需要多步推理的大型问题的能力。为了解决这一问题，研究人员提出了先进的流水线方法，这些方法利用领域特定知识，将小规模的提示链接起来，提供中间反馈，并通过搜索来改进性能。然而，当前编写、调整、维护和改进这些流水线的复杂性限制了它们的复杂性。我们提出了一种占卜式编程，这是一种构建基于大型语言模型的应用的基础范式，使领域专家能够以具有未决选择点的程序形式表达高层次的问题解决策略。这些选择点在运行时由大型语言模型解决，这些模型能够从用户提供的正确和错误决策示例中进行泛化。占卜式程序由三个正交组成部分组成：一个策略，它包含一个非确定性的程序，其中包含可以选择的点，这些点可以转化为搜索树；一个策略，指定了如何借助大型语言模型占卜来导航该树；以及一组演示，描述了在不同问题实例中成功的和失败的搜索树导航场景。每个组成部分都用专门的编程语言表示，并且可以独立改进或替换。我们解决了模块化组合占卜式程序和在其组成部分进化过程中保持一致性的关键编程语言设计挑战。', 'title_zh': '神谕编程：构建基于大模型软件的模块化基础'}
{'arxiv_id': 'arXiv:2502.05300', 'title': 'Parameter Symmetry Breaking and Restoration Determines the Hierarchical Learning in AI Systems', 'authors': 'Liu Ziyin, Yizhou Xu, Tomaso Poggio, Isaac Chuang', 'link': 'https://arxiv.org/abs/2502.05300', 'abstract': 'The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this paper, we posit that parameter symmetry breaking and restoration serve as a unifying mechanism underlying these behaviors. We synthesize prior observations and show how this mechanism explains three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, we highlight symmetry -- a cornerstone of theoretical physics -- as a potential fundamental principle in modern AI.', 'abstract_zh': '现代大型AI系统的学习动态是分层的，常常表现为类似于物理系统中相变的 abrupt 和质的突变。虽然这些现象为揭示神经网络和语言模型背后的机制带来了希望，但现有的理论仍然碎片化，仅针对特定案例进行说明。在本文中，我们认为参数对称性的打破与恢复是这些行为背后的统一机制。我们综合之前的观察，并展示了这一机制如何解释神经网络中的三种分层：学习动力学、模型复杂性以及表示形成。通过连接这些层次，我们强调对称性——理论物理学中的一个基础支柱——可能是现代AI中的潜在基本原理。', 'title_zh': '参数对称性破坏与恢复决定了AI系统中的分层学习'}
{'arxiv_id': 'arXiv:2502.05292', 'title': 'Drone Detection and Tracking with YOLO and a Rule-based Method', 'authors': 'Purbaditya Bhattacharya, Patrick Nowak', 'link': 'https://arxiv.org/abs/2502.05292', 'abstract': 'Drones or unmanned aerial vehicles are traditionally used for military missions, warfare, and espionage. However, the usage of drones has significantly increased due to multiple industrial applications involving security and inspection, transportation, research purposes, and recreational drone flying. Such an increased volume of drone activity in public spaces requires regulatory actions for purposes of privacy protection and safety. Hence, detection of illegal drone activities such as boundary encroachment becomes a necessity. Such detection tasks are usually automated and performed by deep learning models which are trained on annotated image datasets. This paper builds on a previous work and extends an already published open source dataset. A description and analysis of the entire dataset is provided. The dataset is used to train the YOLOv7 deep learning model and some of its minor variants and the results are provided. Since the detection models are based on a single image input, a simple cross-correlation based tracker is used to reduce detection drops and improve tracking performance in videos. Finally, the entire drone detection system is summarized.', 'abstract_zh': '无人机或无人驾驶航空器传统上主要用于军事任务、战争和间谍活动。然而，由于涉及安全、检测、运输、研究目的及娱乐飞行等多方面的工业应用，无人机的使用显著增加。在公共空间内无人机活动量的增加要求采取监管措施，以保护隐私和确保安全。因此，检测非法无人机活动，如越界行为，变得必要。这类检测任务通常被自动化，并由基于深度学习的模型在标注图像数据集上训练完成。本文基于先前的工作，并扩展了一个已发布的开源数据集。提供了整个数据集的描述和分析，并使用该数据集来训练YOLOv7及其某些轻微变种模型，并给出了结果。由于检测模型基于单张图像输入，因此使用了简单的相关滤波器跟踪器来减少视频中的检测丢帧，并提高跟踪性能。最后，总结了整个无人机检测系统。', 'title_zh': '基于YOLO和规则方法的无人机检测与跟踪'}
{'arxiv_id': 'arXiv:2502.05282', 'title': 'Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning', 'authors': 'Yuting He, Boyu Wang, Rongjun Ge, Yang Chen, Guanyu Yang, Shuo Li', 'link': 'https://arxiv.org/abs/2502.05282', 'abstract': "Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image-dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We propose a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels' correspondence under topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via a gradient. We also propose a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code on a companion link: this https URL.", 'abstract_zh': '密集对比表示学习（DCRL）大大提高了图像密集预测任务的学习效率，显示出其在减少医学图像采集和密集标注高成本方面的巨大潜力。然而，医学图像的特性使得难以发现可靠的对应关系，带来了一个大规模误检和漏检（FP&N）对的开放问题。本文我们提出了几何视觉密集相似性学习（GEMINI），它将同胚先验嵌入到DCRL中，以实现有效的密集对比时的可靠对应关系发现。我们提出了一种可变形同胚学习（DHL），该方法建模医学图像的同胚性，并学习估计一个可变形映射以预测在拓扑保有的情况下像素的对应关系。这有效减少了配对搜索空间，并通过梯度驱动隐式而平滑地学习负样本对。同时，我们引入了一种几何语义相似性（GSS），它从特征中提取语义信息以衡量对应关系学习时的对齐程度，从而促进变形的学习效率和性能，构建可靠的正样本对。我们在实验中对两个典型表示学习任务实现了两种实用变种。我们在七个数据集上的出色结果优于现有方法，展示了我们显著的优势。我们将在同伴链接中发布我们的代码：this https URL。', 'title_zh': '医疗图像密集对比表示学习中的假阳性和假阴性问题的同胚先验'}
{'arxiv_id': 'arXiv:2502.05264', 'title': 'Quantum automated learning with provable and explainable trainability', 'authors': 'Qi Ye, Shuangyue Geng, Zizhao Han, Weikang Li, L.-M. Duan, Dong-Ling Deng', 'link': 'https://arxiv.org/abs/2502.05264', 'abstract': 'Machine learning is widely believed to be one of the most promising practical applications of quantum computing. Existing quantum machine learning schemes typically employ a quantum-classical hybrid approach that relies crucially on gradients of model parameters. Such an approach lacks provable convergence to global minima and will become infeasible as quantum learning models scale up. Here, we introduce quantum automated learning, where no variational parameter is involved and the training process is converted to quantum state preparation. In particular, we encode training data into unitary operations and iteratively evolve a random initial state under these unitaries and their inverses, with a target-oriented perturbation towards higher prediction accuracy sandwiched in between. Under reasonable assumptions, we rigorously prove that the evolution converges exponentially to the desired state corresponding to the global minimum of the loss function. We show that such a training process can be understood from the perspective of preparing quantum states by imaginary time evolution, where the data-encoded unitaries together with target-oriented perturbations would train the quantum learning model in an automated fashion. We further prove that the quantum automated learning paradigm features good generalization ability with the generalization error upper bounded by the ratio between a logarithmic function of the Hilbert space dimension and the number of training samples. In addition, we carry out extensive numerical simulations on real-life images and quantum data to demonstrate the effectiveness of our approach and validate the assumptions. Our results establish an unconventional quantum learning strategy that is gradient-free with provable and explainable trainability, which would be crucial for large-scale practical applications of quantum computing in machine learning scenarios.', 'abstract_zh': '机器学习被广泛认为是量子计算最有前景的实际应用之一。现有的量子机器学习方案通常采用量子-经典混合方法，依赖于模型参数的梯度。这种方法缺乏证明其全局最小值收敛的能力，并且在量子学习模型扩展时将变得不可行。在此，我们引入了量子自动化学习，其中不涉及变分参数，训练过程转化为量子态准备。具体而言，我们将训练数据编码为幺正操作，并在这些幺正操作及其逆操作之间进行迭代演化，并在两者之间嵌入目标导向的扰动以提高预测准确性。在合理假设下，我们严格证明了这种演化按照损失函数全局最小值对应的量子态收敛于期望态。我们证明了这种训练过程可以从量子态的虚时间演化角度来理解，在此过程中，数据编码的幺正操作与目标导向的扰动将自动化地训练量子学习模型。此外，我们证明了量子自动化学习范式具有良好的泛化能力，泛化误差上界由希尔伯特空间维数的对数函数与其训练样本数量的比值给出。我们还通过在真实图像和量子数据上的广泛数值模拟，验证了该方法的有效性，并验证了相关假设。我们的结果确立了一种独特的、无梯度的量子学习策略，该策略具有可证明的和可解释的训练能力，对量子计算在机器学习场景中的大规模实际应用至关重要。', 'title_zh': '可验证且可解释的量子自动化学习'}
{'arxiv_id': 'arXiv:2502.05253', 'title': 'LLMs Can Teach Themselves to Better Predict the Future', 'authors': 'Benjamin Turtel, Danny Franklin, Philipp Schoenegger', 'link': 'https://arxiv.org/abs/2502.05253', 'abstract': "We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7--10\\% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-4o.", 'abstract_zh': '我们提出了一种以结果为导向的微调框架，该框架能够增强大规模语言模型（LLMs）的预测能力，而不依赖于人工编纂的推理样本。该方法利用模型自我博弈生成一组多样化的推理轨迹和概率预测，以应对模型知识截止日期后的多种问题。然后，我们根据这些推理轨迹与实际结果的距离对其进行排序，并通过直接偏好优化（DPO）对模型进行微调。在另一组测试数据上，我们的方法使Phi-4 14B和DeepSeek-R1 14B的预测准确率相对于基模型和带有随机标签的DPO微调控制模型分别提高了7%至10%，使其预测能力与更大型的前沿模型GPT-4o相当。', 'title_zh': '大语言模型能够自我学习以更准确地预测未来'}
{'arxiv_id': 'arXiv:2502.05252', 'title': 'GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context Length and Reasoning Complexity?', 'authors': 'Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen', 'link': 'https://arxiv.org/abs/2502.05252', 'abstract': 'Long-context large language models (LLMs) have recently shown strong performance in information retrieval and long-document QA. However, to tackle the most challenging intellectual problems, LLMs must reason effectively in long and complex contexts (e.g., frontier mathematical research). Studying how LLMs handle increasing reasoning complexity and context length is essential, yet existing benchmarks lack a solid basis for quantitative evaluation. Inspired by the abstraction of GSM-8K problems as computational graphs, and the ability to introduce noise by adding unnecessary nodes and edges, we develop a grade school math problem generator capable of producing arithmetic problems with infinite difficulty and context length under fine-grained control. Using our newly synthesized GSM-Infinite benchmark, we comprehensively evaluate existing LLMs. We find a consistent sigmoid decline in reasoning performance as complexity increases, along with a systematic inference scaling trend: exponentially increasing inference computation yields only linear performance gains. These findings underscore the fundamental limitations of current long-context LLMs and the key challenges in scaling reasoning capabilities. Our GSM-Infinite benchmark provides a scalable and controllable testbed for systematically studying and advancing LLM reasoning in long and complex contexts.', 'abstract_zh': '长上下文大规模语言模型（LLMs）近年来在信息检索和长文档问答中表现出色。然而，要解决最复杂的思想难题，LLMs 必须在长且复杂的上下文中有效推理（例如，前沿的数学研究）。研究LLMs处理推理复杂性和上下文长度增加的能力至关重要，但现有的基准测试缺乏量化的评价基础。受GSM-8K问题抽象为计算图以及通过增加不必要的节点和边引入噪声的启发，我们开发了一个小学数学问题生成器，该生成器能够在精细控制下生成无限难度和上下文长度的算术问题。利用我们新合成的GSM-Infinite基准测试，我们全面评估了现有的LLMs。我们发现随着复杂性的增加，推理性能呈现出一致的S形下降趋势，并且存在系统性的推理扩展趋势：指数增加的推理计算仅能带来线性性能提升。这些发现强调了当前长上下文LLMs的基本局限性和扩展推理能力的关键挑战。我们的GSM-Infinite基准测试提供了一个可扩展且可控制的测试平台，用于系统地研究和推进LLMs在长且复杂上下文中的推理能力。', 'title_zh': 'GSM-Infinite：在无限增加的上下文长度和推理复杂度下，你的大规模语言模型表现出怎样的行为？'}
{'arxiv_id': 'arXiv:2502.05248', 'title': 'Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires', 'authors': 'Pranav Bhandari, Usman Naseem, Amitava Datta, Nicolas Fay, Mehwish Nasim', 'link': 'https://arxiv.org/abs/2502.05248', 'abstract': 'Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.', 'abstract_zh': '心理学评估工具长期以来帮助人类理解行为模式。虽然大型语言模型（LLMs）可以生成与人类相媲美的内容，但我们探讨它们是否表现出人格特质。为此，本研究将心理学工具应用于各种场景中的LLMs，以生成其人格特征。通过使用基于特质的问卷调查工具，如大五人格问卷，并考虑训练数据污染的可能性，我们考察了LLMs在五大核心人格维度上的维度变异性和主导性：开放性、责任心、外向性、随和性以及神经质。研究发现，即使在同一类模型中，LLMs也表现出独特的主导特质、不同的特征和各异的人格画像。', 'title_zh': '大型语言模型中的人格特质评估：心理问卷的见解'}
{'arxiv_id': 'arXiv:2502.05242', 'title': "SEER: Self-Explainability Enhancement of Large Language Models' Representations", 'authors': 'Guanxu Chen, Dongrui Liu, Tao Luo, Jing Shao', 'link': 'https://arxiv.org/abs/2502.05242', 'abstract': "Explaining the hidden representations of Large Language Models (LLMs) is a perspective to understand LLMs' underlying inference logic and improve their reliability in application scenarios. However, previous methods introduce external ''black-box'' modules to explain ''black-box'' LLMs, increasing the potential uncertainty and failing to provide faithful explanations. In this paper, we propose a self-explaining method SEER, enhancing LLMs' explainability by aggregating the same concept and disentangling the different concepts in the representation space. In this way, SEER provides faithful explanations carried by representations synchronously with the LLMs' output. Additionally, we showcase the applications of SEER on trustworthiness-related tasks (e.g., the safety risks classification and detoxification tasks), where self-explained LLMs achieve consistent improvement in explainability and performance. More crucially, we theoretically analyze the improvement of SEER on LLMs' generalization ability through optimal transport theory.", 'abstract_zh': '解释大型语言模型（LLMs）的隐藏表示是一种理解LLMs内在推理逻辑并提高其在应用场景中可靠性的方式。然而，之前的方法引入了外部的“黑箱”模块来解释“黑箱”的LLMs，这增加了潜在的不确定性，并未能提供忠实的解释。本文提出了一种自解释方法SEER，通过在表示空间中聚合相同的概念并分离不同的概念来增强LLMs的可解释性。这样，SEER能够在与LLMs输出同步的方式下提供忠实的解释。此外，我们还在与可信度相关任务（如安全风险分类和脱敏任务）的应用中展示了SEER的方法，自解释的LLMs在这两类任务中实现了解释能力和性能的一致提高。更为重要的是，我们通过最佳传输理论理论分析了SEER对LLMs泛化能力的改进。', 'title_zh': 'SEER：大型语言模型表示的自我解释增强'}
{'arxiv_id': 'arXiv:2502.05239', 'title': 'Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics', 'authors': 'Hussam Ghanem, Christophe Cruz', 'link': 'https://arxiv.org/abs/2502.05239', 'abstract': 'Recent advancements in large language models have demonstrated significant potential in the automated construction of knowledge graphs from unstructured text. This paper builds upon our previous work [16], which evaluated various models using metrics like precision, recall, F1 score, triple matching, and graph matching, and introduces a refined approach to address the critical issues of hallucination and omission. We propose an enhanced evaluation framework incorporating BERTScore for graph similarity, setting a practical threshold of 95% for graph matching. Our experiments focus on the Mistral model, comparing its original and fine-tuned versions in zero-shot and few-shot settings. We further extend our experiments using examples from the KELM-sub training dataset, illustrating that the fine-tuned model significantly improves knowledge graph construction accuracy while reducing the exact hallucination and omission. However, our findings also reveal that the fine-tuned models perform worse in generalization tasks on the KELM-sub dataset. This study underscores the importance of comprehensive evaluation metrics in advancing the state-of-the-art in knowledge graph construction from textual data.', 'abstract_zh': '近年来，大规模语言模型在从非结构化文本自动生成知识图谱方面展示了显著的潜力。本文在我们之前的工作[16]基础上进行了进一步研究，该工作通过精确度、召回率、F1分值、三元组匹配和图匹配等指标评估了多种模型，并引入了一种改进的方法以解决幻觉和遗漏的关键问题。我们提出了一种增强的评估框架，其中包括使用BERTScore进行图相似性评估，并设定图匹配的实用阈值为95%。我们的实验证明了Mistral模型及其原始版本和微调版本在零样本和少样本设置下的差异，进一步通过KELM-sub训练集中的示例展示了微调模型在知识图谱构建准确性方面的显著提升，同时减少了精确幻觉和遗漏现象。然而，我们的研究结果还表明，微调模型在KELM-sub数据集上的泛化任务表现较弱。本研究突显了在从文本数据构建知识图谱方面不断推进前沿技术的重要性，需要综合评价指标的支持。', 'title_zh': '增强知识图谱构建：强调幻觉、遗漏和图相似性指标的评估'}
{'arxiv_id': 'arXiv:2502.05237', 'title': 'PSM-SQL: Progressive Schema Learning with Multi-granularity Semantics for Text-to-SQL', 'authors': 'Zhuopan Yang, Yuanzhen Xie, Ruichao Zhong, Yunzhi Tan, Enjie Liu, Zhenguo Yang, Mochi Gao, Bo Hu, Zang Li', 'link': 'https://arxiv.org/abs/2502.05237', 'abstract': 'It is challenging to convert natural language (NL) questions into executable structured query language (SQL) queries for text-to-SQL tasks due to the vast number of database schemas with redundancy, which interferes with semantic learning, and the domain shift between NL and SQL. Existing works for schema linking focus on the table level and perform it once, ignoring the multi-granularity semantics and chainable cyclicity of schemas. In this paper, we propose a progressive schema linking with multi-granularity semantics (PSM-SQL) framework to reduce the redundant database schemas for text-to-SQL. Using the multi-granularity schema linking (MSL) module, PSM-SQL learns the schema semantics at the column, table, and database levels. More specifically, a triplet loss is used at the column level to learn embeddings, while fine-tuning LLMs is employed at the database level for schema reasoning. MSL employs classifier and similarity scores to model schema interactions for schema linking at the table level. In particular, PSM-SQL adopts a chain loop strategy to reduce the task difficulty of schema linking by continuously reducing the number of redundant schemas. Experiments conducted on text-to-SQL datasets show that the proposed PSM-SQL is 1-3 percentage points higher than the existing methods.', 'abstract_zh': '将自然语言（NL）问题转换为可执行的结构化查询语言（SQL）查询对于文本到SQL任务来说极具挑战性，这是因为数据库模式的冗余性极大，这会干扰语义学习，并且自然语言和SQL之间的领域差异。现有的模式链接工作主要关注表层次的链接，并且只进行一次链接，忽略了模式的多粒度语义和链式循环特性。在本文中，我们提出了一种逐步的多粒度模式链接（Progressive Schema Linking with Multi-Granularity Semantics, PSM-SQL）框架，以减少文本到SQL中的冗余数据库模式。通过使用多粒度模式链接（Multi-Granularity Schema Linking, MSL）模块，PSM-SQL可以在列、表和数据库层次上学习模式语义。具体来说，MSL在列层次上使用三元组损失学习嵌入表示，而在数据库层次上通过微调大规模语言模型（LLM）来进行模式推理，以进行语义学习。在表层次上，MSL使用分类器和相似度分数来建模模式间的交互以进行模式链接。特别地，PSM-SQL采用了链式循环策略，通过不断减少冗余模式的数量来逐步降低模式链接任务的难度。在针对文本到SQL数据集进行的实验中，提出的PSM-SQL方法在准确性上比现有方法高出1-3个百分点。', 'title_zh': 'PSM-SQL：基于多粒度语义的 Progressive 模式学习方法用于文本到SQL转换'}
{'arxiv_id': 'arXiv:2502.05236', 'title': 'Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance', 'authors': 'Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Mikyas T. Desta, Roy Fejgin, Rafael Valle, Jason Li', 'link': 'https://arxiv.org/abs/2502.05236', 'abstract': 'While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.', 'abstract_zh': '尽管自回归语音令牌生成模型能够产生具有显著多样性和自然性的语音，但它们固有的缺乏可控性往往会导致诸如幻听和不符合条件输入的非预期声音等问题。我们引入了Koel-TTS，这是一种增强的编码器-解码器Transformer TTS模型套件，通过结合由自动语音识别和说话人验证模型指导的偏好对齐技术来解决这些问题。此外，我们还引入了无分类指导方法，以进一步提高合成语音对转录文本和参考说话人音频的遵从性。我们的实验表明，这些优化措施显著提升了目标说话人相似度、可理解性和语音的自然度。值得注意的是，尽管Koel-TTS是在规模显著较小的数据集上进行训练，但它在上述指标上仍优于现有的最先进的TTS模型。我们网站上提供了音频样本和演示。', 'title_zh': 'Koel-TTS：通过偏好对齐和无分类器引导增强基于大规模语言模型的语音生成'}
{'arxiv_id': 'arXiv:2502.05234', 'title': 'Optimizing Temperature for Language Models with Multi-Sample Inference', 'authors': 'Weihua Du, Yiming Yang, Sean Welleck', 'link': 'https://arxiv.org/abs/2502.05234', 'abstract': "Multi-sample aggregation strategies, such as majority voting and best-of-N sampling, are widely used in contemporary large language models (LLMs) to enhance predictive accuracy across various tasks. A key challenge in this process is temperature selection, which significantly impacts model performance. Existing approaches either rely on a fixed default temperature or require labeled validation data for tuning, which are often scarce and difficult to obtain. This paper addresses the challenge of automatically identifying the (near)-optimal temperature for different LLMs using multi-sample aggregation strategies, without relying on task-specific validation data. We provide a comprehensive analysis of temperature's role in performance optimization, considering variations in model architectures, datasets, task types, model sizes, and predictive accuracy. Furthermore, we propose a novel entropy-based metric for automated temperature optimization, which consistently outperforms fixed-temperature baselines. Additionally, we incorporate a stochastic process model to enhance interpretability, offering deeper insights into the relationship between temperature and model performance.", 'abstract_zh': '多样本聚合策略，如多数投票和最佳样本挑选（Best-of-N）方法，在当前大型语言模型（LLMs）中广泛用于提高各种任务的预测准确性。这一过程中的一大挑战是温度选择问题，它显著影响模型性能。现有方法要么依赖固定的默认温度，要么需要标记的验证数据来进行调整，而这些数据往往稀缺且难以获得。本文提出了一种自动识别不同LLMs在多样本聚合策略下（近）最优温度的方法，无需依赖特定任务的验证数据。我们对温度在性能优化中的作用进行了全面分析，考虑了模型架构、数据集、任务类型、模型大小以及预测准确性等因素的变化。此外，我们提出了一种基于熵的新颖自动化温度优化度量方法，该方法在固定温度基准之上表现更优。同时，我们引入了随机过程模型以增强可解释性，为温度与模型性能之间的关系提供了更深入的见解。', 'title_zh': '使用多样本推测优化语言模型的温度参数'}
{'arxiv_id': 'arXiv:2502.05232', 'title': 'Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers', 'authors': 'Adam Stooke, Rohit Prabhavalkar, Khe Chai Sim, Pedro Moreno Mengibar', 'link': 'https://arxiv.org/abs/2502.05232', 'abstract': 'Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the "Aligner-Encoder". To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention -- it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform "self-transduction".', 'abstract_zh': '现代自动语音识别系统，包括循环神经网络转录机（RNN-Transducer）和基于注意力机制的编码-解码器（AED），设计时使得编码器不需要改变音频序列中信息的时间位置到嵌入中；对齐至最终文本输出的过程在解码时处理。我们发现，近年来采用的基于变换器的编码器实际上能够在前向传播过程中内部完成对齐，而在解码之前。这一新现象使得可以设计出一种更为简单且高效的模型，即“对齐编码器”。为了训练这种模型，我们放弃了RNN-T中的动态规划，转而采用AED的帧级交叉熵损失，同时解码器采用RNN-T中较轻量级的文字专用递归，不学习交叉注意力——它仅以顺序方式扫描嵌入帧，直到预测到消息结束时产生一个词元。我们进行了实验，结果显示其性能与当前最佳技术非常接近，包括一个用于长格式识别的特殊推理配置。在一个代表性对比中，我们测量我们的模型的总推理时间比RNN-T快2倍，比AED快16倍。最后，我们发现音频-文本对齐明显体现在某一层的自注意力权重中，可以说该层执行了“自我转录”。', 'title_zh': '对齐编码器：自我注意力变压器可以成为自转换器'}
{'arxiv_id': 'arXiv:2502.05231', 'title': 'Thin ring wing as a means of flow improvement upstream of a propeller', 'authors': 'Vladimir Sluchak', 'link': 'https://arxiv.org/abs/2502.05231', 'abstract': "There are numerous devices currently known with the purpose of reducing the irregularity of the flow upstream of the propeller and to decrease by that means the propeller-induced vibration and noise. Many of these devices are wing-shaped vortex-generators that affect the flow with their induced (i.e. passive) longitudinal vortices. The paper's subject is the use of a ring-shaped wing as a highly effective passive vortex-generator which allows to control the flow closer to the most charged sections of propeller blades. The problem of a thin ring-shaped wing with irregular (asymmetric) geometry in the irregular steady flow has been solved in a linear approach and the intensity of the induced longitudinal vortices as a function of the irregularity of the flow and the geometry of the ring wing has been estimated using that solution. Experiments in the towing tank showing good concordance with the theoretical model confirmed the effectiveness of such a device. Some additional advantages of a ring-shaped wing incorporated into the construction of stabilizers are considered.", 'abstract_zh': '目前有许多设备旨在减少进入螺旋桨前的流场的不规则性，从而通过这种方式减少螺旋桨引起的振动和噪声。这些设备中很多是机翼形旋涡发生器，它们通过诱导（即被动的）纵向旋涡来影响流场。本文的主题是使用环形机翼作为高效的被动旋涡发生器，以更有效地控制靠近螺旋桨叶片最复杂断面的流场。采用线性方法解决了薄环形机翼在不规则（不对称）几何形状下的稳态不规则流场问题，并利用该解估算诱导纵向旋涡的强度，该强度取决于流场的不规则性和环形机翼的几何形状。在拖曳水槽中的实验结果显示，该理论模型具有很好的一致性和准确度，验证了该装置的有效性。此外，还考虑了将环形机翼集成到稳定器结构中的某些额外优势。', 'title_zh': '薄环翼作为增强推进器上游气流的手段'}
{'arxiv_id': 'arXiv:2502.05230', 'title': 'DiffNMR2: NMR Guided Sampling Acquisition Through Diffusion Model Uncertainty', 'authors': 'Etienne Goffinet, Sen Yan, Fabrizio Gabellieri, Laurence Jennings, Lydia Gkoura, Filippo Castiglione, Ryan Young, Idir Malki, Ankita Singh, Thomas Launey', 'link': 'https://arxiv.org/abs/2502.05230', 'abstract': "Nuclear Magnetic Resonance (NMR) spectrometry uses electro-frequency pulses to probe the resonance of a compound's nucleus, which is then analyzed to determine its structure. The acquisition time of high-resolution NMR spectra remains a significant bottleneck, especially for complex biological samples such as proteins. In this study, we propose a novel and efficient sub-sampling strategy based on a diffusion model trained on protein NMR data. Our method iteratively reconstructs under-sampled spectra while using model uncertainty to guide subsequent sampling, significantly reducing acquisition time. Compared to state-of-the-art strategies, our approach improves reconstruction accuracy by 52.9\\%, reduces hallucinated peaks by 55.6%, and requires 60% less time in complex NMR experiments. This advancement holds promise for many applications, from drug discovery to materials science, where rapid and high-resolution spectral analysis is critical.", 'abstract_zh': '核磁共振（NMR）光谱技术通过使用电磁频率脉冲探测化合物核的共振状态，然后分析这些状态以确定其结构。高分辨率NMR光谱的采集时间仍然是一个重要的瓶颈，特别是在复杂的生物样品（如蛋白质）中更为突出。在本研究中，我们提出了一种基于在蛋白质NMR数据上训练的扩散模型的新型高效的子采样策略。该方法在迭代重建欠采样光谱的同时，利用模型不确定性指导后续采样，显著缩短了采集时间。与最先进的方法相比，我们的方法在重建准确性上提高了52.9%，减少假象峰55.6%，并且在复杂NMR实验中所需时间减少了60%。这一进展在药物发现、材料科学等众多需要快速进行高分辨率光谱分析的应用领域具有广阔的应用前景。', 'title_zh': 'DiffNMR2：基于扩散模型不确定性指导的核磁共振采样获取方法'}
{'arxiv_id': 'arXiv:2502.05228', 'title': 'Multi-Objective Mobile Damped Wave Algorithm (MOMDWA): A Novel Approach For Quantum System Control', 'authors': 'Juntao Yu, Jiaquan Yu, Dedai Wei, Xinye Sha, Shengwei Fu, Miuyu Qiu, Yurun Jin, Kaichen Ouyang', 'link': 'https://arxiv.org/abs/2502.05228', 'abstract': 'In this paper, we introduce a novel multi-objective optimization algorithm, the Multi-Objective Mobile Damped Wave Algorithm (MOMDWA), specifically designed to address complex quantum control problems. Our approach extends the capabilities of the original Mobile Damped Wave Algorithm (MDWA) by incorporating multiple objectives, enabling a more comprehensive optimization process. We applied MOMDWA to three quantum control scenarios, focusing on optimizing the balance between control fidelity, energy consumption, and control smoothness. The results demonstrate that MOMDWA significantly enhances quantum control efficiency and robustness, achieving high fidelity while minimizing energy use and ensuring smooth control pulses. This advancement offers a valuable tool for quantum computing and other domains requiring precise, multi-objective control.', 'abstract_zh': '在本文中，我们提出了一种新的多目标优化算法——多目标移动阻尼波算法（MOMDWA），专门用于解决复杂的量子控制问题。我们的方法通过引入多个目标，扩展了原始移动阻尼波算法（MDWA）的功能，从而实现更加全面的优化过程。我们将MOMDWA应用于三个量子控制场景，重点关注控制保真度、能量消耗和控制平滑度之间的平衡优化。实验结果表明，MOMDWA显著提高了量子控制效率和鲁棒性，实现了高保真度的同时，减少了能量消耗并确保了控制脉冲的平滑性。这一进展为量子计算及其他需要精确多目标控制的领域提供了一项宝贵的工具。', 'title_zh': '多目标移动阻尼波算法（MOMDWA）：一种量子系统控制的新方法'}
{'arxiv_id': 'arXiv:2502.05227', 'title': 'Robotouille: An Asynchronous Planning Benchmark for LLM Agents', 'authors': 'Gonzalo Gonzalez-Pumariega, Leong Su Yean, Neha Sunkara, Sanjiban Choudhury', 'link': 'https://arxiv.org/abs/2502.05227', 'abstract': "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at this https URL.", 'abstract_zh': '有效的非同步规划，即高效地对必须并行或顺序发生的状态和行动进行推理和规划的能力，对于必须考虑到时间延迟、进行复杂的长期任务推理以及与其他代理协作的智能体至关重要。尽管大型语言模型（LLM）智能体在高层次任务规划上显示出前景，但当前的基准测试主要集中在短期任务上，并未评估这样的非同步规划能力。我们引入了Robotouille，这是一个具有挑战性的基准环境，旨在测试LLM智能体在处理长期非同步场景方面的能力。我们的同步和非同步数据集捕捉到了超越现有基准的日益复杂的规划挑战，要求智能体管理重叠任务和中断。我们的结果显示，ReAct（gpt4-o）在同步任务上的表现达到了47%，但在非同步任务上仅为11%，这表明改进空间很大。我们进一步分析了失败模式，展示了LLM智能体需要更好地结合长期反馈并在任务执行期间自我审查其推理的必要性。源代码可在以下网址获得：这个 https URL。', 'title_zh': 'Robotouille：一种供LLM代理使用的异步规划基准测试'}
{'arxiv_id': 'arXiv:2502.05225', 'title': 'BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks', 'authors': 'Hanyong Lee, Chaelyn Lee, Yongjae Lee, Jaesung Lee', 'link': 'https://arxiv.org/abs/2502.05225', 'abstract': 'Phishing often targets victims through visually perturbed texts to bypass security systems. The noise contained in these texts functions as an adversarial attack, designed to deceive language models and hinder their ability to accurately interpret the content. However, since it is difficult to obtain sufficient phishing cases, previous studies have used synthetic datasets that do not contain real-world cases. In this study, we propose the BitAbuse dataset, which includes real-world phishing cases, to address the limitations of previous research. Our dataset comprises a total of 325,580 visually perturbed texts. The dataset inputs are drawn from the raw corpus, consisting of visually perturbed sentences and sentences generated through an artificial perturbation process. Each input sentence is labeled with its corresponding ground truth, representing the restored, non-perturbed version. Language models trained on our proposed dataset demonstrated significantly better performance compared to previous methods, achieving an accuracy of approximately 96%. Our analysis revealed a significant gap between real-world and synthetic examples, underscoring the value of our dataset for building reliable pre-trained models for restoration tasks. We release the BitAbuse dataset, which includes real-world phishing cases annotated with visual perturbations, to support future research in adversarial attack defense.', 'abstract_zh': '网络钓鱼通常通过视觉上被篡改的文本攻击受害者，以绕过安全系统。这些文本中包含的噪声充当了对抗性攻击，旨在欺骗语言模型并妨碍它们准确地解读内容。然而，由于获取足够的网络钓鱼案例较为困难，之前的研究所使用的合成数据集并不包含真实世界案例。在此研究中，我们提出了BitAbuse数据集，该数据集包含了真实世界的网络钓鱼案例，以解决以往研究的局限性。该数据集总共包含325,580条视觉上被篡改的文本。数据集的输入来源于原始语料库，包括视觉上被篡改的句子和通过人工篡改过程生成的句子。每个输入句子都标记有相应的 ground truth，即未被篡改的恢复版本。使用我们提出的数据集进行训练的语言模型在性能上明显优于之前的方法，准确率约为96%。我们的分析揭示了真实世界案例与合成案例之间存在显著差距，突显了我们的数据集对于构建可靠的预训练模型进行恢复任务的价值。我们发布了BitAbuse数据集，其中包括带有视觉篡改注解的真实世界网络钓鱼案例，以支持未来在对抗性攻击防御方面的研究。', 'title_zh': 'BitAbuse: 一种视觉篡改文本数据集，用于防御诈骗攻击'}
{'arxiv_id': 'arXiv:2502.05224', 'title': 'A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations', 'authors': 'Yihe Zhou, Tao Ni, Wei-Bin Lee, Qingchuan Zhao', 'link': 'https://arxiv.org/abs/2502.05224', 'abstract': 'Large Language Models (LLMs) have achieved significantly advanced capabilities in understanding and generating human language text, which have gained increasing popularity over recent years. Apart from their state-of-the-art natural language processing (NLP) performance, considering their widespread usage in many industries, including medicine, finance, education, etc., security concerns over their usage grow simultaneously. In recent years, the evolution of backdoor attacks has progressed with the advancement of defense mechanisms against them and more well-developed features in the LLMs. In this paper, we adapt the general taxonomy for classifying machine learning attacks on one of the subdivisions - training-time white-box backdoor attacks. Besides systematically classifying attack methods, we also consider the corresponding defense methods against backdoor attacks. By providing an extensive summary of existing works, we hope this survey can serve as a guideline for inspiring future research that further extends the attack scenarios and creates a stronger defense against them for more robust LLMs.', 'abstract_zh': '近年来，大型语言模型（LLMs）在理解和生成人类语言文本方面取得了显著的进步，这使得它们在近年来越来越受欢迎。除了它们在自然语言处理（NLP）上的先进性能外，由于它们在医药、金融、教育等多个行业的广泛应用，对其使用的安全问题也越来越受到关注。随着对抗后门攻击防御机制的发展和LLMs自身功能的不断完善，后门攻击的发生也在逐渐演变。本文中，我们借鉴了一般性的机器学习攻击分类框架，对一种细分类型——训练时白盒后门攻击进行了分类。除了系统地分类攻击方法外，我们还考虑了针对后门攻击的相应防御方法。通过全面总结现有研究工作，我们希望本文能够为未来进一步扩展攻击场景并增强攻击防御提供指导，以提高LLMs的鲁棒性。', 'title_zh': '大型语言模型（LLMs）中的后门威胁综述：攻击、防御与评估'}
{'arxiv_id': 'arXiv:2502.05223', 'title': 'KDA: A Knowledge-Distilled Attacker for Generating Diverse Prompts to Jailbreak LLMs', 'authors': 'Buyun Liang, Kwan Ho Ryan Chan, Darshan Thaker, Jinqi Luo, René Vidal', 'link': 'https://arxiv.org/abs/2502.05223', 'abstract': "Jailbreak attacks exploit specific prompts to bypass LLM safeguards, causing the LLM to generate harmful, inappropriate, and misaligned content. Current jailbreaking methods rely heavily on carefully designed system prompts and numerous queries to achieve a single successful attack, which is costly and impractical for large-scale red-teaming. To address this challenge, we propose to distill the knowledge of an ensemble of SOTA attackers into a single open-source model, called Knowledge-Distilled Attacker (KDA), which is finetuned to automatically generate coherent and diverse attack prompts without the need for meticulous system prompt engineering. Compared to existing attackers, KDA achieves higher attack success rates and greater cost-time efficiency when targeting multiple SOTA open-source and commercial black-box LLMs. Furthermore, we conducted a quantitative diversity analysis of prompts generated by baseline methods and KDA, identifying diverse and ensemble attacks as key factors behind KDA's effectiveness and efficiency.", 'abstract_zh': 'Jailbreak 攻击利用特定提示以绕过语言模型（LLM）的安全防护，导致生成有害、不适当和不对齐的内容。当前的 jailbreak 方法高度依赖于精心设计的系统提示和大量的查询来实现单次成功的攻击，这在大规模红队测试中成本高且不实用。为解决这一挑战，我们提出了一种将多种最新技术攻击者的知识提炼成一个开源模型的方法，称为知识提炼攻击者（KDA），并通过调整使其能够自动生成连贯且多样化的攻击提示，无需精细设计系统提示。与现有攻击者相比，KDA 在针对多个最新开源和商用黑盒语言模型时具有更高的攻击成功率和更高的成本效率。此外，我们对基线方法和 KDA 生成的提示进行了定量多样性分析，发现多样性和集合攻击是 KDA 有效性和效率的关键因素。', 'title_zh': 'KDA：一种知识精简攻击者，用于生成多样化的提示以突破大规模语言模型 (LLMs)'}
{'arxiv_id': 'arXiv:2502.05221', 'title': 'Blackout DIFUSCO', 'authors': 'Jun Pyo Seo', 'link': 'https://arxiv.org/abs/2502.05221', 'abstract': 'This study explores the integration of Blackout Diffusion into the DIFUSCO framework for combinatorial optimization, specifically targeting the Traveling Salesman Problem (TSP). Inspired by the success of discrete-time diffusion models (D3PM) in maintaining structural integrity, we extend the paradigm to a continuous-time framework, leveraging the unique properties of Blackout Diffusion. Continuous-time modeling introduces smoother transitions and refined control, hypothesizing enhanced solution quality over traditional discrete methods. We propose three key improvements to enhance the diffusion process. First, we transition from a discrete-time-based model to a continuous-time framework, providing a more refined and flexible formulation. Second, we refine the observation time scheduling to ensure a smooth and linear transformation throughout the diffusion process, allowing for a more natural progression of states. Finally, building upon the second improvement, we further enhance the reverse process by introducing finer time slices in regions that are particularly challenging for the model, thereby improving accuracy and stability in the reconstruction phase. Although the experimental results did not exceed the baseline performance, they demonstrate the effectiveness of these methods in balancing simplicity and complexity, offering new insights into diffusion-based combinatorial optimization. This work represents the first application of Blackout Diffusion to combinatorial optimization, providing a foundation for further advancements in this domain. * The code is available for review at this https URL.', 'abstract_zh': '本研究探讨了将Blackout Diffusion整合到DIFUSCO框架中，用于组合优化问题，特别是针对旅行商问题（TSP）。受离散时间扩散模型（D3PM）在保持结构完整性方面的成功启发，我们将这一范式扩展到了连续时间框架，利用Blackout Diffusion的独特性质。连续时间建模引入了更加平滑的过渡和精细的控制，假设在迭代过程中相比传统离散方法能提供更高的解的质量。我们提出了三种关键改进，以增强扩散过程。首先，我们从基于离散时间的模型转变为连续时间框架，提供了更加精细和灵活的建模方式。其次，我们优化了观测时间的调度，确保整个扩散过程中平滑和线性的转换，从而实现更自然的状态过渡。最后，基于第二项改进，我们进一步增强了逆过程，在模型特别难以处理的区域引入更细的时间切片，从而提高重构阶段的准确性和稳定性。尽管实验结果未超过基线性能，但这些方法在平衡简单性和复杂性方面的有效性得到了证明，为基于扩散的组合优化提供了新的见解。这项工作是将Blackout Diffusion应用于组合优化的首次尝试，为该领域进一步的发展奠定了基础。* 代码可在以下链接进行查阅：[此处填写链接]', 'title_zh': '"Blackout DIFUSCO" 可以翻译成中文为：“黑屏 DIFUSCO”。在这里，“Blackout”通常指的是停电或屏幕黑屏的情况，“DIFUSCO”是一个特定的术语或缩写，具体含义需要根据上下文来确定。如果"DIFUSCO"是一个特定领域的术语，建议保留原词并给出解释，以便读者理解。例如：\n\n“黑屏 DIFUSCO”——“DIFUSCO”是一种特定技术/方法的缩写，具体含义将在本文中进行详细说明。\n\n请注意，如果"DIFUSCO"是某个研究所或组织的名称，最好保持原名不变，并在第一次出现时提供中英文对照。'}
{'arxiv_id': 'arXiv:2502.05220', 'title': 'Aero-LLM: A Distributed Framework for Secure UAV Communication and Intelligent Decision-Making', 'authors': 'Balakrishnan Dharmalingam, Rajdeep Mukherjee, Brett Piggott, Guohuan Feng, Anyi Liu', 'link': 'https://arxiv.org/abs/2502.05220', 'abstract': "Increased utilization of unmanned aerial vehicles (UAVs) in critical operations necessitates secure and reliable communication with Ground Control Stations (GCS). This paper introduces Aero-LLM, a framework integrating multiple Large Language Models (LLMs) to enhance UAV mission security and operational efficiency. Unlike conventional singular LLMs, Aero-LLM leverages multiple specialized LLMs for various tasks, such as inferencing, anomaly detection, and forecasting, deployed across onboard systems, edge, and cloud servers. This dynamic, distributed architecture reduces performance bottleneck and increases security capabilities. Aero-LLM's evaluation demonstrates outstanding task-specific metrics and robust defense against cyber threats, significantly enhancing UAV decision-making and operational capabilities and security resilience against cyber attacks, setting a new standard for secure, intelligent UAV operations.", 'abstract_zh': '随着无人航空车辆（UAVs）在关键操作中的应用增加，与地面控制站（GCS）之间的安全可靠通信变得尤为重要。本文介绍了一种名为Aero-LLM的框架，该框架结合了多种大型语言模型（LLMs），以提升UAV任务的安全性和操作效率。与传统的单一LLM不同，Aero-LLM利用多种专门的LLMs来执行各种任务，如推理、异常检测和预测，这些任务分布于机载系统、边缘设备和云服务器上。这种动态、分布式架构降低了性能瓶颈，并增强了安全性。Aero-LLM的评估结果表明，其在特定任务上的表现优异，并具备强大的抵御网络威胁的能力，显著提升了UAV的决策能力和操作安全性，使其能够抵御网络攻击，为安全智能的UAV操作树立了新的标准。', 'title_zh': 'Aero-LLM：一种用于安全无人机通信和智能决策的分布式框架'}
{'arxiv_id': 'arXiv:2502.05219', 'title': 'Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies', 'authors': 'Kendrea Beers, Helen Toner', 'link': 'https://arxiv.org/abs/2502.05219', 'abstract': 'This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information.\nIndependent external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. In practice, external researchers have struggled to gain access to AI systems because of AI companies\' legitimate concerns about security, privacy, and intellectual property.\nBut now, privacy-enhancing technologies (PETs) have reached a new level of maturity: end-to-end technical infrastructure developed by OpenMined combines several PETs into various setups that enable privacy-preserving audits of AI systems. We showcase two case studies where this infrastructure has been deployed in real-world governance scenarios: "Understanding Social Media Recommendation Algorithms with the Christchurch Call" and "Evaluating Frontier Models with the UK AI Safety Institute." We describe types of scrutiny of AI systems that could be facilitated by current setups and OpenMined\'s proposed future setups.\nWe conclude that these innovative approaches deserve further exploration and support from the AI governance community. Interested policymakers can focus on empowering researchers on a legal level.', 'abstract_zh': '本文描述了非营利组织OpenMined如何通过其开发的技术基础设施，实现对外部审查AI系统的能力，同时不泄露敏感信息。\n\n对外部进行独立审查是AI开发中至关重要的透明度来源，因此，它应该成为任何AI治理方法的基本组成部分。实践中，由于AI公司对安全、隐私和知识产权的合法担忧，外部研究人员难以获得对AI系统的访问权限。\n但如今，增强隐私的技术（PETs）已达到了新的成熟水平。OpenMined开发的一整套技术基础设施将多种PET组合在一起，从而实现了在保护隐私的前提下对AI系统的审查。我们展示了该基础设施在实际治理场景中的两个案例研究：“通过基督城呼吁理解社交媒体推荐算法”和“通过英国AI安全研究所评估前沿模型”。我们也描述了当前设置及OpenMined提出的未来设置能够支持的各种审查类型。\n\n我们得出结论，这些创新方法值得在AI治理社区进一步探索和支持。有兴趣的政策制定者可以从法律层面赋予研究人员更多权力。', 'title_zh': '通过增强隐私技术实现人工智能系统的外部审查'}
{'arxiv_id': 'arXiv:2502.05218', 'title': 'FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction', 'authors': 'Yitong Duan, Weiran Wang, Jian Li', 'link': 'https://arxiv.org/abs/2502.05218', 'abstract': 'As a fundamental method in economics and finance, the factor model has been extensively utilized in quantitative investment. In recent years, there has been a paradigm shift from traditional linear models with expert-designed factors to more flexible nonlinear machine learning-based models with data-driven factors, aiming to enhance the effectiveness of these factor models. However, due to the low signal-to-noise ratio in market data, mining effective factors in data-driven models remains challenging. In this work, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that employs a hypergraph structure to better capture high-order nonlinear relationships among stock returns and factors. To mine hidden factors that supplement human-designed prior factors for predicting stock returns, we design a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the influence of prior factors. Additionally, we propose a temporal residual contrastive learning method to guide the extraction of effective and comprehensive hidden factors by contrasting stock-specific residual information over different time periods. Our extensive experiments on real stock market data demonstrate that FactorGCL not only outperforms existing state-of-the-art methods but also mines effective hidden factors for predicting stock returns.', 'abstract_zh': '作为经济学和金融学中的基本方法，因子模型在量化投资中得到了广泛的应用。近年来，因子模型从传统的由专家设计的线性模型转向了更灵活的数据驱动的基于机器学习的模型，后者以数据驱动的方式来构建因子，从而增强因子模型的效果。然而，由于市场数据中的低信噪比，数据驱动模型中提取有效因子仍然具有挑战性。本文提出了一种基于超图的因子模型（FactorGCL），其利用超图结构更好地捕获股票回报与因子之间的高级非线性关系。为了挖掘补充人设计先验因子的隐藏因子以预测股票回报，我们设计了一种级联残差超图架构，其中的隐藏因子是从去除先验因子影响后的残差信息中提取出来的。此外，我们提出了一种时间残差对比学习方法，通过对比不同时间周期内的股票特定残差信息来指导有效且全面的隐藏因子的提取。在真实股票市场数据上的广泛实验表明，FactorGCL 不仅超越了现有的先进方法，还成功挖掘出了适用于预测股票回报的有效隐藏因子。', 'title_zh': 'FactorGCL：一种基于超图的因子模型结合时序残差对比学习的股票回报预测方法'}
{'arxiv_id': 'arXiv:2502.05215', 'title': 'Watermarking across Modalities for Content Tracing and Generative AI', 'authors': 'Pierre Fernandez', 'link': 'https://arxiv.org/abs/2502.05215', 'abstract': 'Watermarking embeds information into digital content like images, audio, or text, imperceptible to humans but robustly detectable by specific algorithms. This technology has important applications in many challenges of the industry such as content moderation, tracing AI-generated content, and monitoring the usage of AI models. The contributions of this thesis include the development of new watermarking techniques for images, audio, and text. We first introduce methods for active moderation of images on social platforms. We then develop specific techniques for AI-generated content. We specifically demonstrate methods to adapt latent generative models to embed watermarks in all generated content, identify watermarked sections in speech, and improve watermarking in large language models with tests that ensure low false positive rates. Furthermore, we explore the use of digital watermarking to detect model misuse, including the detection of watermarks in language models fine-tuned on watermarked text, and introduce training-free watermarks for the weights of large transformers. Through these contributions, the thesis provides effective solutions for the challenges posed by the increasing use of generative AI models and the need for model monitoring and content moderation. It finally examines the challenges and limitations of watermarking techniques and discuss potential future directions for research in this area.', 'abstract_zh': '水印技术将信息嵌入到数字内容中，如图像、音频或文本，这些信息对人类不可察觉，但可以被特定算法可靠地检测出来。这项技术在内容审核、追踪AI生成内容以及监控AI模型的使用方面具有重要应用价值。本论文的贡献包括开发适用于图像、音频和文本的新水印技术。我们首先介绍了在社交媒体平台上的主动图像审核方法。然后，我们开发了专门针对AI生成内容的技术。我们具体展示了将水印嵌入潜变量生成模型生成的所有内容中的方法、在语音中识别水印片段的方法，并通过测试改进了大型语言模型水印技术，确保低误报率。此外，我们还探索了利用数字水印检测模型滥用的方法，包括检测在水印文本上微调的语言模型中的水印，并引入了对于大型变压器权重的无需训练的水印技术。通过这些贡献，论文为应对生成式AI模型的不断增加的应用及其对模型监控和内容审核的需求提供了有效解决方案。最后，论文探讨了水印技术的挑战和局限性，并讨论了该领域潜在的研究方向。', 'title_zh': '跨模态水印技术在内容追踪和生成型AI中的应用'}
{'arxiv_id': 'arXiv:2502.05214', 'title': 'CoRPA: Adversarial Image Generation for Chest X-rays Using Concept Vector Perturbations and Generative Models', 'authors': 'Amy Rafferty, Rishi Ramaesh, Ajitha Rajan', 'link': 'https://arxiv.org/abs/2502.05214', 'abstract': "Deep learning models for medical image classification tasks are becoming widely implemented in AI-assisted diagnostic tools, aiming to enhance diagnostic accuracy, reduce clinician workloads, and improve patient outcomes. However, their vulnerability to adversarial attacks poses significant risks to patient safety. Current attack methodologies use general techniques such as model querying or pixel value perturbations to generate adversarial examples designed to fool a model. These approaches may not adequately address the unique characteristics of clinical errors stemming from missed or incorrectly identified clinical features. We propose the Concept-based Report Perturbation Attack (CoRPA), a clinically-focused black-box adversarial attack framework tailored to the medical imaging domain. CoRPA leverages clinical concepts to generate adversarial radiological reports and images that closely mirror realistic clinical misdiagnosis scenarios. We demonstrate the utility of CoRPA using the MIMIC-CXR-JPG dataset of chest X-rays and radiological reports. Our evaluation reveals that deep learning models exhibiting strong resilience to conventional adversarial attacks are significantly less robust when subjected to CoRPA's clinically-focused perturbations. This underscores the importance of addressing domain-specific vulnerabilities in medical AI systems. By introducing a specialized adversarial attack framework, this study provides a foundation for developing robust, real-world-ready AI models in healthcare, ensuring their safe and reliable deployment in high-stakes clinical environments.", 'abstract_zh': '深度学习模型在医学图像分类任务中的广泛应用正在推动AI辅助诊断工具的发展，旨在提高诊断准确性、减轻临床医生的工作负担并改善患者预后。然而，这些模型对对抗性攻击的脆弱性对患者安全构成了重大风险。当前的攻击方法使用一般的策略，如模型查询或像素值扰动，来生成用于欺骗模型的对抗性示例。这些方法可能无法充分应对来自临床特征遗漏或误识别的特殊临床错误特征。我们提出了一种基于概念的报告扰动攻击（CoRPA），这是一种专注于临床的黑盒对抗攻击框架，专为医学成像领域设计。CoRPA 利用临床概念生成与现实临床误诊场景高度相似的对抗性放射学报告和图像。我们使用MIMIC-CXR-JPG胸部X光片和放射学报告数据集展示了CoRPA 的实用性。评估结果表明，表现出强大抗传统对抗性攻击能力的深度学习模型在受到CoRPA 的临床针对性扰动时显著更加脆弱。这突显了在医疗AI系统中解决特定领域漏洞的重要性。通过引入专门的对抗性攻击框架，本研究为开发具有高度鲁棒性、适用于实际临床环境的AI模型奠定了基础，确保其在高风险临床环境中安全可靠的应用。', 'title_zh': 'CoRPA：使用概念向量扰动和生成模型的胸部X光 adversarial 图像生成'}
{'arxiv_id': 'arXiv:2502.05213', 'title': 'DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models', 'authors': 'Qihao Lin, Chen Tang, Lan zhang, Junyang zhang, Xiangyang Li', 'link': 'https://arxiv.org/abs/2502.05213', 'abstract': "Well-trained large language models (LLMs) present significant risks, including potential malicious use and copyright infringement. Current studies aim to trace the distribution of LLM-generated texts by implicitly embedding watermarks. Among these, the single-bit watermarking method can only determine whether a given text was generated by an LLM. In contrast, the multi-bit watermarking method embeds richer information into the generated text, which can identify which LLM generated and distributed a given text to which user. However, existing efforts embed the multi-bit watermark directly into the generated text without accounting for its watermarking capacity. This approach can result in embedding failures when the text's watermarking capacity is insufficient. In this paper, we derive the watermark embedding distribution based on the logits of LLMs and propose a formal inequality to segment the text optimally for watermark embedding. Building on this foundation, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method. DERMARK divides the text into segments of varying lengths for each bit embedding, adaptively matching the text's capacity. It achieves this with negligible overhead and robust performance against text editing by minimizing watermark extraction loss. Comprehensive experiments demonstrate that, compared to the SOTA method, our method reduces the number of tokens required for embedding each bit by 20\\%, reduces watermark embedding time by 50\\%, and is robust to text editing and watermark erasure attacks.", 'abstract_zh': '经过充分训练的大语言模型（LLMs）存在显著的风险，包括潜在的恶意使用和版权侵权。当前的研究旨在通过隐式嵌入水印来追踪LLM生成文本的分布。其中，单位水印方法只能确定给定文本是否由LLM生成。相比之下，多单位水印方法可以将更丰富的信息嵌入生成的文本中，以识别并追踪给定文本是由哪个LLM生成和分发给哪位用户的。然而，现有的方法直接将多单位水印嵌入生成的文本中，而没有考虑到水印容量的问题。这种方法可能会因文本的水印容量不足而导致嵌入失败。在本文中，我们基于LLM的logits推导了水印嵌入的分布，并提出了一种形式不等式来优化文本以进行水印嵌入。在此基础上，我们提出了DERMARK，这是一种动态、高效且鲁棒的多单位水印方法。DERMARK将文本分割为不同长度的段落，针对每个位的嵌入适配文本的容量。通过最小化水印提取损失来实现这一点，并且在面对文本编辑和水印擦除攻击时具有鲁棒性。全面的实验表明，与当前最佳方法（SOTA）相比，我们的方法将每个位的嵌入所需token数量减少了20%，将水印嵌入时间减少了50%，并且对文本编辑和水印擦除攻击具有鲁棒性。', 'title_zh': 'DERMARK：一种适用于大型语言模型的动态、高效且 robust 的多比特水印'}
{'arxiv_id': 'arXiv:2502.05211', 'title': 'Decoding FL Defenses: Systemization, Pitfalls, and Remedies', 'authors': 'Momin Ahmad Khan, Virat Shejwalkar, Yasra Chandio, Amir Houmansadr, Fatima Muhammad Anwar', 'link': 'https://arxiv.org/abs/2502.05211', 'abstract': 'While the community has designed various defenses to counter the threat of poisoning attacks in Federated Learning (FL), there are no guidelines for evaluating these defenses. These defenses are prone to subtle pitfalls in their experimental setups that lead to a false sense of security, rendering them unsuitable for practical deployment. In this paper, we systematically understand, identify, and provide a better approach to address these challenges. First, we design a comprehensive systemization of FL defenses along three dimensions: i) how client updates are processed, ii) what the server knows, and iii) at what stage the defense is applied. Next, we thoroughly survey 50 top-tier defense papers and identify the commonly used components in their evaluation setups. Based on this survey, we uncover six distinct pitfalls and study their prevalence. For example, we discover that around 30% of these works solely use the intrinsically robust MNIST dataset, and 40% employ simplistic attacks, which may inadvertently portray their defense as robust. Using three representative defenses as case studies, we perform a critical reevaluation to study the impact of the identified pitfalls and show how they lead to incorrect conclusions about robustness. We provide actionable recommendations to help researchers overcome each pitfall.', 'abstract_zh': '尽管学术界为应对联邦学习（FL）中投毒攻击威胁设计了各种防御措施，但目前尚无评估这些防御措施的标准指南。这些防御措施在实验设置中容易存在细微的陷阱，从而产生一种虚假的安全感，使其不适合实际部署。在本文中，我们系统地理解和识别了这些挑战，并提出了更好的解决方案。首先，我们从三个维度对FL防御进行全面系统化：i）客户端更新如何处理，ii）服务器掌握什么信息，iii）防御在何时应用。接下来，我们详细调查了50篇顶级防御论文，并确定它们评价设置中常用的组件。基于这项调查，我们发现了六个独特的陷阱，并研究了它们的普遍存在性。例如，我们发现大约30%的工作仅使用固有鲁棒性的MNIST数据集，40%的作品使用简单的攻击方法，这可能会无意中展示其防御措施的鲁棒性。我们以三个代表性防御措施为例进行关键性重新评估，研究识别出的陷阱对鲁棒性影响，展示了它们如何导致对鲁棒性的错误结论。我们提供了可操作的建议，以帮助研究人员克服每个陷阱。', 'title_zh': '解码FL防御：系统化、陷阱与补救措施'}
{'arxiv_id': 'arXiv:2502.05209', 'title': 'Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities', 'authors': 'Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell', 'link': 'https://arxiv.org/abs/2502.05209', 'abstract': "Evaluations of large language model (LLM) risks and capabilities are increasingly being incorporated into AI risk management and governance frameworks. Currently, most risk evaluations are conducted by designing inputs that elicit harmful behaviors from the system. However, a fundamental limitation of this approach is that the harmfulness of the behaviors identified during any particular evaluation can only lower bound the model's worst-possible-case behavior. As a complementary method for eliciting harmful behaviors, we propose evaluating LLMs with model tampering attacks which allow for modifications to latent activations or weights. We pit state-of-the-art techniques for removing harmful LLM capabilities against a suite of 5 input-space and 6 model tampering attacks. In addition to benchmarking these methods against each other, we show that (1) model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace; (2) the attack success rate of model tampering attacks can empirically predict and offer conservative estimates for the success of held-out input-space attacks; and (3) state-of-the-art unlearning methods can easily be undone within 16 steps of fine-tuning. Together these results highlight the difficulty of removing harmful LLM capabilities and show that model tampering attacks enable substantially more rigorous evaluations than input-space attacks alone. We release models at this https URL", 'abstract_zh': '将下面的论文内容或标题翻译成中文（符合学术规范）：\n\n大型语言模型（LLM）的风险和能力评估越来越多地被纳入人工智能风险管理与治理框架中。目前，大多数风险评估都是通过设计输入以引发系统有害行为来进行的。然而，这种做法的一个根本限制是，在任何特定评估中所识别出的有害行为的有害程度只能为模型最坏情况行为提供下界。为补充通过输入空间引发有害行为的方法，我们提出使用模型篡改攻击来评估LLM，这种攻击允许对潜在激活或权重进行修改。我们将最先进的消除有害LLM能力的技术与5种输入空间攻击和6种模型篡改攻击进行对比。除了评估这些方法之间的性能外，我们还展示了以下几点：（1）模型对能力提取攻击的鲁棒性存在于一个低维鲁棒性子空间中；（2）模型篡改攻击的成功率可以实际预测并提供保留输入空间攻击成功可能性的保守估计；（3）最先进的遗忘方法可以在微调的16步内轻易被逆转。这些结果强调了删除有害LLM能力的难度，并展示了模型篡改攻击比单独使用输入空间攻击能实现更为严谨的评估。我们在此处提供模型：[请根据实际情况填写链接]', 'title_zh': '模型篡改攻击使对大语言模型能力的评估更加严格'}
{'arxiv_id': 'arXiv:2502.05208', 'title': 'Mitigation of Camouflaged Adversarial Attacks in Autonomous Vehicles--A Case Study Using CARLA Simulator', 'authors': 'Yago Romano Martinez, Brady Carter, Abhijeet Solanki, Wesam Al Amiri, Syed Rafay Hasan, Terry N. Guo', 'link': 'https://arxiv.org/abs/2502.05208', 'abstract': "Autonomous vehicles (AVs) rely heavily on cameras and artificial intelligence (AI) to make safe and accurate driving decisions. However, since AI is the core enabling technology, this raises serious cyber threats that hinder the large-scale adoption of AVs. Therefore, it becomes crucial to analyze the resilience of AV security systems against sophisticated attacks that manipulate camera inputs, deceiving AI models. In this paper, we develop camera-camouflaged adversarial attacks targeting traffic sign recognition (TSR) in AVs. Specifically, if the attack is initiated by modifying the texture of a stop sign to fool the AV's object detection system, thereby affecting the AV actuators. The attack's effectiveness is tested using the CARLA AV simulator and the results show that such an attack can delay the auto-braking response to the stop sign, resulting in potential safety issues. We conduct extensive experiments under various conditions, confirming that our new attack is effective and robust. Additionally, we address the attack by presenting mitigation strategies. The proposed attack and defense methods are applicable to other end-to-end trained autonomous cyber-physical systems.", 'abstract_zh': '自动驾驶车辆（AVs） heavily 地依赖摄像头和人工智能（AI）来做出安全准确的驾驶决策。然而，由于AI是核心使能技术，这引发了严重的信息安全威胁，阻碍了AVs的大规模应用。因此，分析AV安全系统在复杂攻击下的韧性变得至关重要，这些攻击通过操纵摄像头输入迷惑AI模型。在本文中，我们开发了一种针对AVs交通标志识别（TSR）的摄像头伪装式对抗攻击。具体来说，如果攻击从修改停车标志的纹理开始，欺骗AV的对象检测系统，从而影响AV的执行器。我们使用CARLA AV模拟器测试了该攻击的有效性，结果显示，这种攻击可以导致AV对停车标志的自动刹车响应延迟，从而可能引发安全问题。我们进行了广泛的实验，在不同条件下验证了我们新攻击的有效性和鲁棒性。此外，我们提出了缓解策略来应对这种攻击。提出的攻击和防御方法适用于其他端到端训练的自主物理信息系统。', 'title_zh': '在自动驾驶车辆中减轻伪装 adversarial 攻击——基于 CARLA 模拟器的案例研究'}
{'arxiv_id': 'arXiv:2502.05206', 'title': 'Safety at Scale: A Comprehensive Survey of Large Model Safety', 'authors': 'Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Henghui Ding, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang', 'link': 'https://arxiv.org/abs/2502.05206', 'abstract': 'The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-based Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.', 'abstract_zh': '大型模型的快速进步，受到大规模预训练赋予其卓越学习和泛化能力的驱动，已经重塑了人工智能（AI）的格局。这些模型现在已成为广泛应用的基础，包括对话AI、推荐系统、自动驾驶、内容生成、医疗诊断和科学发现等领域。然而，它们的广泛应用也暴露出了显著的安全风险，引发了关于鲁棒性、可靠性和伦理影响的关切。本文综述了当前针对大型模型的安全研究，涵盖了视觉基础模型（VFMs）、大型语言模型（LLMs）、视觉-语言预训练（VLP）模型、视觉-语言模型（VLMs）、扩散模型（DMs）以及基于大型模型的代理。我们的贡献总结如下：（1）我们提出了针对这些模型的安全威胁的全面分类，包括对抗攻击、数据污染、后门攻击、脱狱和提示注入攻击、能量-延迟攻击、数据和模型提取攻击以及新兴的代理特定威胁。（2）我们回顾了对每种攻击提出的防御策略，并总结了用于安全研究的常用数据集和基准。（3）在此基础上，我们识别并讨论了大型模型安全中的开放挑战，强调了全面安全评估、可扩展且有效的防御机制以及可持续数据实践的需要。尤为重要的是，我们强调了研究界和国际协作的共同努力的必要性。我们的工作可以为研究人员和从业人员提供有用的参考，促进全面防御系统和平台的持续开发，保障AI模型的安全。', 'title_zh': '大规模安全：大型模型安全综述'}
{'arxiv_id': 'arXiv:2502.05202', 'title': 'Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies', 'authors': 'Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Gaurav Jain, Roy Schwartz, Moshe Wasserblat, David Harel', 'link': 'https://arxiv.org/abs/2502.05202', 'abstract': 'Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.', 'abstract_zh': '加速大型语言模型（LLM）的推理是生成式AI中的一个关键挑战。推测性解码（SD）方法通过单一目标前向传播生成多个令牌，从而提供了显著的效率提升。然而，现有的SD方法要求作诗者模型和目标模型共享同一个词汇表，从而限制了可用的作诗者模型的数量，通常需要从头开始训练一个作诗者模型。我们提出了三种新的SD方法，消除了这一共享词汇表的限制。所有这些方法都保持了目标分布（即它们是无损的），并且可以与即用型模型一起使用，无需额外的训练或修改。从实证结果来看，对于摘要、编程和长上下文任务，我们的算法在标准自回归解码方法上实现了显著的加速。通过允许任意即用型模型作为作诗者，并且不需要重新训练，这项工作大大扩展了推测性解码框架在实际中的应用范围。', 'title_zh': '异构词汇表下无损投机解码算法加速大规模语言模型推理'}
{'arxiv_id': 'arXiv:2502.05186', 'title': 'Multimodal Stock Price Prediction', 'authors': 'Furkan Karadaş, Bahaeddin Eravcı, Ahmet Murat Özbayoğlu', 'link': 'https://arxiv.org/abs/2502.05186', 'abstract': "In an era where financial markets are heavily influenced by many static and dynamic factors, it has become increasingly critical to carefully integrate diverse data sources with machine learning for accurate stock price prediction. This paper explores a multimodal machine learning approach for stock price prediction by combining data from diverse sources, including traditional financial metrics, tweets, and news articles. We capture real-time market dynamics and investor mood through sentiment analysis on these textual data using both ChatGPT-4o and FinBERT models. We look at how these integrated data streams augment predictions made with a standard Long Short-Term Memory (LSTM model) to illustrate the extent of performance gains. Our study's results indicate that incorporating the mentioned data sources considerably increases the forecast effectiveness of the reference model by up to 5%. We also provide insights into the individual and combined predictive capacities of these modalities, highlighting the substantial impact of incorporating sentiment analysis from tweets and news articles. This research offers a systematic and effective framework for applying multimodal data analytics techniques in financial time series forecasting that provides a new view for investors to leverage data for decision-making.", 'abstract_zh': '在金融市场受到众多静态和动态因素强烈影响的时代背景下，利用机器学习整合多样化的数据源以实现精准的股票价格预测变得尤为重要。本文探讨了一种多模态机器学习方法，通过结合传统的财务指标、推特和新闻文章等多种数据源来进行股票价格预测。我们通过使用ChatGPT-4和FinBERT模型对这些文本数据进行情感分析，捕捉实时的市场动态和投资者情绪。我们展示了这些集成数据流如何增强使用标准长短期记忆（LSTM）模型所作出的预测，以说明性能提升的程度。研究结果表明，结合上述数据源可以显著提高参考模型的预测准确性，最高提升5%。我们还对这些模态的单独和联合预测能力提供了深刻的见解，突显了从推特和新闻文章中进行情感分析对预测能力的重要影响。本研究提供了一种系统而有效的方法，用于在金融时间序列预测中应用多模态数据分析技术，为投资者提供新的视角以利用数据进行决策。', 'title_zh': '多模态股票价格预测'}
{'arxiv_id': 'arXiv:2502.05181', 'title': 'Enhancing Team Diversity with Generative AI: A Novel Project Management Framework', 'authors': 'Johnny Chan, Yuming Li', 'link': 'https://arxiv.org/abs/2502.05181', 'abstract': "This research-in-progress paper presents a new project management framework that utilises GenAI technology. The framework is designed to address the common challenge of uniform team compositions in academic and research project teams, particularly in universities and research institutions. It does so by integrating sociologically identified patterns of successful team member personalities and roles, using GenAI agents to fill gaps in team dynamics. This approach adds an additional layer of analysis to conventional project management processes by evaluating team members' personalities and roles and employing GenAI agents, fine-tuned on personality datasets, to fill specific team roles. Our initial experiments have shown improvements in the model's ability to understand and process personality traits, suggesting the potential effectiveness of GenAI teammates in real-world project settings. This paper aims to explore the practical application of AI in enhancing team diversity and project management", 'abstract_zh': '本项研究进展论文介绍了一种新的项目管理框架，该框架利用了生成式人工智能（GenAI）技术。该框架旨在解决学术和研究项目团队中常见的团队组成一致性问题，尤其是在大学和研究机构中。通过整合社会学上确定的成功团队成员人格和角色模式，利用GenAI代理来填补团队动态中的空白。这种做法通过评估团队成员的人格和角色，并利用微调在人格数据集上的GenAI代理来填补特定的团队角色，为传统的项目管理过程增加了另一层分析。初步实验表明，该模型在理解和处理人格特质方面的表现有所提升，这暗示了GenAI队友在实际项目环境中的潜在有效性。本文旨在探讨人工智能在增强团队多样性与项目管理中的实用性应用。', 'title_zh': '利用生成式人工智能增强团队多样性：一种新的项目管理框架'}
{'arxiv_id': 'arXiv:2410.13772', 'title': 'Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?', 'authors': 'Argyrios Gerogiannis, Yu-Han Huang, Venugopal V. Veeravalli', 'link': 'https://arxiv.org/abs/2410.13772', 'abstract': "We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without prior knowledge about the system's non-stationarity. A state-of-the-art, black-box algorithm, known as MASTER, is considered, with a focus on identifying the conditions under which it can achieve its stated goals. Specifically, we prove that MASTER's non-stationarity detection mechanism is not triggered for practical choices of horizon, leading to performance akin to a random restarting algorithm. Moreover, we show that the regret bound for MASTER, while being order optimal, stays above the worst-case linear regret until unreasonably large values of the horizon. To validate these observations, MASTER is tested for the special case of piecewise stationary multi-armed bandits, along with methods that employ random restarting, and others that use quickest change detection to restart. A simple, order optimal random restarting algorithm, that has prior knowledge of the non-stationarity is proposed as a baseline. The behavior of the MASTER algorithm is validated in simulations, and it is shown that methods employing quickest change detection are more robust and consistently outperform MASTER and other random restarting approaches.", 'abstract_zh': '我们研究了无先验系统非稳态知识的非稳态强化学习（NS-RL）问题。考虑了一种先进的黑盒算法——MASTER，并着重分析了在哪些条件下它可以达到其声明的目标。具体来说，我们证明了MASTER的非稳态检测机制在实用的时间范围内不会被触发，导致其性能类似于随机重启算法。此外，我们展示了MASTER的遗憾边界虽然为次优阶，但在时间范围极大的情况下仍高于最坏情况的线性遗憾。为了验证这些观察结果，我们对MASTER算法在分段稳态多臂 bandit 问题中的表现进行了测试，并与其他采用随机重启和使用最快变化检测重启的方法进行了比较。还提出了一种简单、次优阶的先验知道非稳态的随机重启基准算法。通过仿真实验验证了MASTER算法的行为，并展示了使用最快变化检测的方法在鲁棒性和性能上更胜一筹，能够稳定地优于MASTER和其他随机重启方法。', 'title_zh': '《无先验的黑盒非 stationary 强化学习可行吗？》\n\n这个标题翻译成中文时，为了使其符合学术规范，可以稍作调整，以确保术语准确且表达清晰。因此，最终翻译可以是：\n\n《无先验的黑盒非平稳强化学习可行吗？》\n\n这里的“黑盒”指的是算法对环境的具体模型不了解，“无先验”意味着算法不需要对环境有任何预设的假设，而“非平稳”（non-stationary）强调的是环境状态或奖励可能随时间变化。'}
{'arxiv_id': 'arXiv:2301.06943', 'title': 'Self-supervised Domain Adaptation for Breaking the Limits of Low-quality Fundus Image Quality Enhancement', 'authors': 'Qingshan Hou, Peng Cao, Jiaqi Wang, Xiaoli Liu, Jinzhu Yang, Osmar R. Zaiane', 'link': 'https://arxiv.org/abs/2301.06943', 'abstract': 'Retinal fundus images have been applied for the diagnosis and screening of eye diseases, such as Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). However, both low-quality fundus images and style inconsistency potentially increase uncertainty in the diagnosis of fundus disease and even lead to misdiagnosis by ophthalmologists. Most of the existing image enhancement methods mainly focus on improving the image quality by leveraging the guidance of high-quality images, which is difficult to be collected in medical applications. In this paper, we tackle image quality enhancement in a fully unsupervised setting, i.e., neither paired images nor high-quality images. To this end, we explore the potential of the self-supervised task for improving the quality of fundus images without the requirement of high-quality reference images. Specifically, we construct multiple patch-wise domains via an auxiliary pre-trained quality assessment network and a style clustering. To achieve robust low-quality image enhancement and address style inconsistency, we formulate two self-supervised domain adaptation tasks to disentangle the features of image content, low-quality factor and style information by exploring intrinsic supervision signals within the low-quality images. Extensive experiments are conducted on EyeQ and Messidor datasets, and results show that our DASQE method achieves new state-of-the-art performance when only low-quality images are available.', 'abstract_zh': '视网膜底片图像已被用于糖尿病视网膜病变（DR）或糖尿病黄斑水肿（DME）等眼病的诊断和筛查。然而，低质量的底片图像和风格不一致性可能会增加眼底疾病诊断的不确定性，甚至可能导致眼科医生出现误诊。目前大多数现有的图像增强方法主要集中在通过高质量图像的指导来提高图像质量，但这些高质量图像在医学应用中难以收集。本文在无需配对图像和高质量参考图像的完全无监督环境下，探讨了自监督任务在不依赖高质量参考图像的情况下提高底片图像质量的潜力。具体而言，我们通过一个辅助预训练的质量评估网络和风格聚类构建了多个像素级领域。为了实现对低质量图像的鲁棒增强并解决风格不一致性问题，我们在低质量图像内部探索内在监督信号，制定两个自监督领域适应任务，以分离图像内容、低质量因素和风格信息。在EyeQ和Messidor数据集上进行了广泛实验，结果表明，仅使用低质量图像时，我们的DASQE方法达到了新的最佳性能。', 'title_zh': '自我监督领域适应方法突破低质量眼底图像质量增强的限制'}
