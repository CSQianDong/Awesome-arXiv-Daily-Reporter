# User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation 

**Title (ZH)**: 生成式AI时代的用户模拟：用户建模、合成数据生成与系统评估 

**Authors**: Krisztian Balog, ChengXiang Zhai  

**Link**: [PDF](https://arxiv.org/pdf/2501.04410)  

**Abstract**: User simulation is an emerging interdisciplinary topic with multiple critical applications in the era of Generative AI. It involves creating an intelligent agent that mimics the actions of a human user interacting with an AI system, enabling researchers to model and analyze user behaviour, generate synthetic data for training, and evaluate interactive AI systems in a controlled and reproducible manner. User simulation has profound implications for diverse fields and plays a vital role in the pursuit of Artificial General Intelligence. This paper provides an overview of user simulation, highlighting its key applications, connections to various disciplines, and outlining future research directions to advance this increasingly important technology. 

**Abstract (ZH)**: 用户模拟是生成式人工智能时代的一个新兴跨学科课题，具有多种关键应用价值。它涉及创建能够模仿人类用户与人工智能系统交互行为的智能代理，使研究人员能够建模和分析用户行为、生成用于训练的合成数据，并在受控和可重复的环境中评估互动人工智能系统。用户模拟对多个领域具有深远的影响，并在追求通用人工智能方面扮演着至关重要的角色。本文概述了用户模拟的基本概念，突出了其关键应用、与各个学科的联系，并指出了未来研究方向，以推进这一日益重要的技术。 

---
# InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection 

**Title (ZH)**: InfiGUIAgent：一种具有原生推理和反思能力的多模态通用GUI代理 

**Authors**: Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, Fei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.04575)  

**Abstract**: Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \url{this https URL}. 

**Abstract (ZH)**: 由多模态大规模语言模型（MLLMs）驱动的图形用户界面（GUI）代理已经在计算设备如计算机和手机上展示了在任务自动化方面的巨大潜力。然而，现有的代理在多步推理和依赖文本标注方面面临挑战，限制了它们的效果。我们提出了基于MLLM的\textit{InfiGUIAgent}，这是一种通过两阶段监督微调管道进行训练的GUI代理。第一阶段增强基础技能，如GUI理解与定位，第二阶段则通过合成数据整合层级推理和预期-反思推理技能，使代理具备本体推理能力。在多个GUI基准测试中，\textit{InfiGUIAgent}取得了竞争力的表现，突显了本体推理技能在提升GUI交互以适应自动化任务方面的效果。更多资源可参见\url{this https URL}。 

---
# Agent Laboratory: Using LLM Agents as Research Assistants 

**Title (ZH)**: 代理实验室：将大型语言模型代理作为研究助理 

**Authors**: Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum  

**Link**: [PDF](https://arxiv.org/pdf/2501.04227)  

**Abstract**: Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery. 

**Abstract (ZH)**: 历史上，科学研究是一个漫长且昂贵的过程，需要从初始构想到最终结果耗费大量的时间和资源。为了加快科学研究进程、降低研究成本并提高研究质量，我们引入了Agent Laboratory这一自主的基于语言模型的框架，能够完成整个研究过程。该框架接受人类提供的研究想法，并通过三个阶段——文献回顾、实验和报告撰写，生成全面的研究输出，包括代码仓库和研究报告，同时允许用户在每个阶段提供反馈和指导。我们采用多种最先进的语言模型部署Agent Laboratory，并邀请多位研究人员通过参与调查、提供人类反馈来指导研究过程，并最终评估最终论文的质量。我们发现：(1) 由o1-preview驱动的Agent Laboratory产生最佳的研究成果；(2) 生成的机器学习代码能够与现有方法相比达到最先进的性能；(3) 人类参与，每个阶段提供反馈，显著提高了整体研究质量；(4) Agent Laboratory显著降低了研究成本，与之前的自主研究方法相比，研究费用降低了84%。我们希望Agent Laboratory能够使研究人员将更多精力投入到富有创造力的构想中，而非低级别的编码和撰写工作，从而最终加速科学研究的进程。 

---
# Implementing Systemic Thinking for Automatic Schema Matching: An Agent-Based Modeling Approach 

**Title (ZH)**: 基于代理建模的系统性思维在自动模式匹配中的应用 

**Authors**: Hicham Assoudi, Hakim Lounis  

**Link**: [PDF](https://arxiv.org/pdf/2501.04136)  

**Abstract**: Several approaches are proposed to deal with the problem of the Automatic Schema Matching (ASM). The challenges and difficulties caused by the complexity and uncertainty characterizing both the process and the outcome of Schema Matching motivated us to investigate how bio-inspired emerging paradigm can help with understanding, managing, and ultimately overcoming those challenges. In this paper, we explain how we approached Automatic Schema Matching as a systemic and Complex Adaptive System (CAS) and how we modeled it using the approach of Agent-Based Modeling and Simulation (ABMS). This effort gives birth to a tool (prototype) for schema matching called Reflex-SMAS. A set of experiments demonstrates the viability of our approach on two main aspects: (i) effectiveness (increasing the quality of the found matchings) and (ii) efficiency (reducing the effort required for this efficiency). Our approach represents a significant paradigm-shift, in the field of Automatic Schema Matching. 

**Abstract (ZH)**: 为了应对自动模式匹配（Automatic Schema Matching, ASM）问题，提出了几种方法。由于模式匹配过程及其结果所表现出的复杂性和不确定性带来的挑战和困难，我们探讨了如何利用生物启发的新兴范式来帮助理解、管理和最终克服这些挑战。在本文中，我们解释了我们将自动模式匹配视为系统性且复杂的自适应系统（Complex Adaptive System, CAS），并如何通过基于代理的建模和仿真（Agent-Based Modeling and Simulation, ABMS）方法对其进行建模。这一努力催生了一种名为Reflex-SMAS的模式匹配工具（原型）。一系列实验展示了我们在两个主要方面的可行性：（i）有效性（提高找到匹配的质量）和（ii）效率（减少所需的努力）。我们的方法代表了自动模式匹配领域的一项重要范式转变。 

---
# Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding 

**Title (ZH)**: 超越视觉：通过语言锚定细调具备异构传感器的通用机器人策略 

**Authors**: Joshua Jones, Oier Mees, Carmelo Sferrazza, Kyle Stachowicz, Pieter Abbeel, Sergey Levine  

**Link**: [PDF](https://arxiv.org/pdf/2501.04693)  

**Abstract**: Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, a novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as a common cross-modal grounding. We combine a multimodal contrastive loss with a sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in a zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSeis able to increase success rates by over 20% compared to all considered baselines. 

**Abstract (ZH)**: 与世界交互是一种多感官体验：为了实现有效的通用交互，需要充分利用所有可用的模态，包括视觉、触觉和音频，以填补部分观察带来的空白。例如，在视线受阻的情况下从袋子里取东西时，机器人应当依赖触觉和听觉。然而，最先进的通用机器人策略通常只在大型数据集上进行训练，从视觉和本体感觉观察中预测机器人的行为。在本研究中，我们提出了FuSe，一种新颖的方法，通过利用自然语言作为跨模态的共同基础，使视觉运动的通用策略能够针对无法获得大量数据的异构传感器模态进行微调。我们结合了多模态对比损失和基于感官的语义生成损失，以编码高层次的语义。在机器人操作的背景下，我们展示了FuSe能够在零样本设置中执行需要联合推理多种模态（如视觉、触觉和音频）的挑战性任务，包括多模态提示、组成跨模态提示以及描述其交互对象的描述。我们表明，相同的配方适用于广泛不同的通用策略，包括基于扩散的通用策略和大型视觉-语言-动作（VLA）模型。在实际世界中的大量实验中，我们发现FuSe相较于所有考虑的基线方法，能够将成功率达到20%以上的提高。 

---
# Constraints as Rewards: Reinforcement Learning for Robots without Reward Functions 

**Title (ZH)**: 将约束作为奖励：无需奖励函数的机器人强化学习 

**Authors**: Yu Ishihara, Noriaki Takasugi, Kotaro Kawakami, Masaya Kinoshita, Kazumi Aoyama  

**Link**: [PDF](https://arxiv.org/pdf/2501.04228)  

**Abstract**: Reinforcement learning has become an essential algorithm for generating complex robotic behaviors. However, to learn such behaviors, it is necessary to design a reward function that describes the task, which often consists of multiple objectives that needs to be balanced. This tuning process is known as reward engineering and typically involves extensive trial-and-error. In this paper, to avoid this trial-and-error process, we propose the concept of Constraints as Rewards (CaR). CaR formulates the task objective using multiple constraint functions instead of a reward function and solves a reinforcement learning problem with constraints using the Lagrangian-method. By adopting this approach, different objectives are automatically balanced, because Lagrange multipliers serves as the weights among the objectives. In addition, we will demonstrate that constraints, expressed as inequalities, provide an intuitive interpretation of the optimization target designed for the task. We apply the proposed method to the standing-up motion generation task of a six-wheeled-telescopic-legged robot and demonstrate that the proposed method successfully acquires the target behavior, even though it is challenging to learn with manually designed reward functions. 

**Abstract (ZH)**: 强化学习已成为生成复杂机器人行为的关键算法。然而，为了学习这些行为，需要设计一个描述任务的奖励函数，该任务往往由多个需要平衡的目标组成。这一调整过程被称为奖励工程，通常涉及大量的试错过程。在本文中，为了避免这一试错过程，我们提出了一种约束作为奖励（Constraints as Rewards, CaR）的概念。CaR 使用多个约束函数而不是奖励函数来表示任务目标，并采用拉格朗日方法解决具有约束的强化学习问题。通过这种方法，不同的目标会自动实现平衡，因为拉格朗日乘子起到了目标之间的权重作用。此外，我们将证明，以不等式形式表示的约束为任务设计的优化目标提供了直观的解释。我们将所提出的方法应用于六轮伸缩腿机器人起立运动生成任务，并证明所提出的方法即使使用手动设计的奖励函数也难以学习，也能成功获得所需行为。 

---
# HIVEX: A High-Impact Environment Suite for Multi-Agent Research (extended version) 

**Title (ZH)**: HIVEX：多代理研究的高影响环境套件（扩展版） 

**Authors**: Philipp D. Siedler  

**Link**: [PDF](https://arxiv.org/pdf/2501.04180)  

**Abstract**: Games have been vital test beds for the rapid development of Agent-based research. Remarkable progress has been achieved in the past, but it is unclear if the findings equip for real-world problems. While pressure grows, some of the most critical ecological challenges can find mitigation and prevention solutions through technology and its applications. Most real-world domains include multi-agent scenarios and require machine-machine and human-machine collaboration. Open-source environments have not advanced and are often toy scenarios, too abstract or not suitable for multi-agent research. By mimicking real-world problems and increasing the complexity of environments, we hope to advance state-of-the-art multi-agent research and inspire researchers to work on immediate real-world problems. Here, we present HIVEX, an environment suite to benchmark multi-agent research focusing on ecological challenges. HIVEX includes the following environments: Wind Farm Control, Wildfire Resource Management, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial Wildfire Suppression. We provide environments, training examples, and baselines for the main and sub-tasks. All trained models resulting from the experiments of this work are hosted on Hugging Face. We also provide a leaderboard on Hugging Face and encourage the community to submit models trained on our environment suite. 

**Abstract (ZH)**: 游戏一直是促进基于代理的（Agent-based）研究快速发展的关键平台。尽管以往取得了显著的进步，但这些发现是否适用于现实世界的问题仍不清楚。随着压力的增大，某些最关键生态挑战可以通过技术和其应用找到缓解和预防的解决方案。大多数现实世界领域包含多代理场景，并需要机器与机器之间的合作以及人类与机器之间的合作。开源环境尚未得到充分发展，且通常是过于简单的场景，过于抽象或不适合多代理研究。通过模拟现实世界问题并增加环境的复杂性，我们希望推进前沿的多代理研究，并激励研究人员面对现实世界的问题。在此，我们提出了HIVEX，这是一个环境套件，用于对以生态挑战为重点的多代理研究进行基准测试。HIVEX 包括以下环境：风力农场控制（Wind Farm Control）、野火资源管理（Wildfire Resource Management）、无人机植树造林（Drone-Based Reforestation）、海洋塑料收集（Ocean Plastic Collection）和空中灭火（Aerial Wildfire Suppression）。我们提供了用于主任务和辅助任务的环境、训练示例和基准。本研究中所有训练模型的结果托管在 Hugging Face 上。我们还在 Hugging Face 上提供了一个排行榜，并鼓励社区提交在我们的环境套件中训练的模型。 

---
