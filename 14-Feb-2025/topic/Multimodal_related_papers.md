# The influence of visual and linguistic cues on ignorance inference in Vision-Language Models (VLMs) 

**Title (ZH)**: 视觉和语言线索对视觉语言模型（VLMs）中无知推理的影响 

**Authors**: Ye-eun Cho, Yunho Maeng  

**Link**: [PDF](https://arxiv.org/pdf/2502.09120)  

**Abstract**: This study explored how Vision-Language Models (VLMs) process ignorance implicatures with visual and linguistic cues. Particularly, we focused on the effects of contexts (precise and approximate contexts) and modifier types (bare numerals, superlative, and comparative modifiers), which were considered pragmatic and semantic factors respectively. Methodologically, we conducted a truth-value judgment task in visually grounded settings using GPT-4o and Gemini 1.5 Pro. The results indicate that while both models exhibited sensitivity to linguistic cues (modifier), they failed to process ignorance implicatures with visual cues (context) as humans do. Specifically, the influence of context was weaker and inconsistent across models, indicating challenges in pragmatic reasoning for VLMs. On the other hand, superlative modifiers were more strongly associated with ignorance implicatures as compared to comparative modifiers, supporting the semantic view. These findings highlight the need for further advancements in VLMs to process language-vision information in a context-dependent way to achieve human-like pragmatic inference. 

**Abstract (ZH)**: 本研究探讨了视觉语言模型（VLMs）如何通过视觉和语义线索处理常识性含意。特别地，我们关注了上下文（精确上下文和模糊上下文）和修饰词类型（裸数词、超限定词和比较修饰词）的影响，前者被视为语用因素，后者被视为语义因素。从方法论上，我们使用GPT-4o和Gemini 1.5 Pro在视觉接地的场景中进行了真假值判断任务。研究结果显示，虽然两种模型对语义线索（修饰词）表现出敏感性，但它们在处理带有视觉线索（上下文）的常识性含意方面未能像人类那样进行处理。具体来说，不同模型在上下文影响方面的表现较弱且不一致，这表明VLM在语用推理方面存在挑战。另一方面，超限定修饰词与常识性含意的关系比比较修饰词更为紧密，支持了语义学的观点。这些发现强调了进一步改进VLM以在依赖上下文的方式处理语言和视觉信息，以实现类似人类的语用推理的重要性。 

---
# Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation 

**Title (ZH)**: 以任意模态提问：一种关于多模态检索增强生成的综合综述

在这个翻译中，"Ask in Any Modality" 被翻译为 "以任意模态提问"，"Multimodal Retrieval-Augmented Generation" 被翻译为 "多模态检索增强生成"。这种翻译既保留了原文的意思，也符合学术论文标题的简洁和正式风格。 

**Authors**: Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari  

**Link**: [PDF](https://arxiv.org/pdf/2502.08826)  

**Abstract**: Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在依赖静态训练数据时往往会遇到幻觉和过时知识的问题。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，增强事实性和更新的信息支持。近年来，多模态学习的进步催生了多模态RAG的发展，该模型结合了包括文本、图像、音频和视频在内的多种模态，以增强生成的输出。然而，跨模态对齐和推理为多模态RAG带来了独特的挑战，使其区别于传统的单模态RAG。本综述提供了一种结构化和全面的多模态RAG系统的分析，涵盖了数据集、评估指标、基准测试、评估方法、检索、融合、增强和生成方面的创新。我们详细审查了训练策略、鲁棒性增强和损失函数，并探讨了多样的多模态RAG应用场景。此外，我们讨论了开放的挑战和未来的研究方向，以支持这一不断发展的领域的发展。本综述为开发更强大和可靠的AI系统奠定了基础，这些系统能够有效地利用多模态的动态外部知识库。更多资源可在以下链接获取：[this https URL](this https URL)。 

---
# MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency 

**Title (ZH)**: MME-CoT：在大型多模态模型中评估推理过程的质量、稳健性和效率 

**Authors**: Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.09621)  

**Abstract**: Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: this https URL 

**Abstract (ZH)**: 使用链式推理（CoT）回答问题显著增强了大型语言模型（LLMs）的推理能力，但对其在大型多模态模型（LMMs）上的影响仍缺乏系统评估和深入研究。本文介绍了MME-CoT，这是一种专门的基准测试，评估LMMs的CoT推理性能，涵盖六大领域：数学、科学、OCR、逻辑、时空和一般场景。作为该领域的首个全面研究，我们提出了一套全面的评估套件，其中包括三个新的度量标准，从细微层面评估推理质量、鲁棒性和效率。利用精选的高质量数据和独特的评估策略，我们深入分析了当前最先进的LMMs，揭示了几点关键见解：1）具有反射机制的模型展现出更优质的CoT推理能力，Kimi k1.5的表现优于GPT-4o并取得最高质量的结果；2）CoT提示往往在感知密集型任务上降低了LMMs的性能，表明可能存在有害的过度思考行为；3）尽管CoT质量高，具有反射机制的LMMs在常规响应和自我纠正阶段均表现出显著的低效率。我们希望MME-CoT能够成为促进LMMs多模态推理发展的基石。项目页面：[这里](this https URL) 

---
# Exploring the Potential of Encoder-free Architectures in 3D LMMs 

**Title (ZH)**: 探索无编码器架构在3D LMM中的潜在应用 

**Authors**: Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.09620)  

**Abstract**: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at this https URL 

**Abstract (ZH)**: 在2D视觉领域，已经初步探索了基于编码器的架构，但在3D理解场景中，尚未明确地验证其有效性。本文首次全面调查了基于编码器的3D大型多模态模型（LMMs）面临的挑战，以及编码器自由架构是否可以有效克服这些挑战。这些挑战包括难以适应不同的点云分辨率以及从编码器提取的点特征不能满足大型语言模型（LLMs）的语义需求。本文通过以下方式确定了使3D LMMs去除编码器的设计关键方面，从而使LLM承担3D编码器的角色：1）我们在预训练阶段提出了LLM嵌入语义编码策略，探索了各种点云自监督损失的效果，并提出了混合语义损失来提取高层次语义；2）我们在指令调优阶段引入了层次几何聚合策略，将归纳偏置引入LLM的早期层，以关注点云的局部细节。最终，本文提出了第一个基于编码器自由架构的3D LMM，ENEL。我们的7B模型与当前最先进的模型ShapeLLM-13B相当，在分类、描述和视觉问答任务上的准确率分别为55.0%、50.92%和42.7%。我们的结果表明，编码器自由架构在3D理解领域替代基于编码器的架构具有很高的前景。代码已发布于 this https URL。 

---
# EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents 

**Title (ZH)**: EmbodiedBench：综合评估面向视觉驱动体感代理的多模态大型语言模型 

**Authors**: Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.09560)  

**Abstract**: Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at this https URL. 

**Abstract (ZH)**: 利用多模态大型语言模型（MLLMs）创建具身智能体为解决现实世界任务提供了有前途的途径。虽然以语言为中心的具身智能体已受到广泛关注，但由于缺乏全面的评估框架，基于MLLM的具身智能体仍然未得到充分探索。为了弥合这一差距，我们提出了EmbodiedBench——一个广泛的基准测试，旨在评估以视觉驱动的具身智能体。EmbodiedBench的特点包括：(1) 涵盖四个环境的1,128个测试任务，从高层语义任务（如家庭环境）到涉及原子动作的低层任务（如导航和操作）；(2) 评估六个精心策划的子集，这些子集涵盖了常识推理、复杂指令理解、空间意识、视觉感知和长期规划等核心智能体能力。通过大量的实验，我们在EmbodiedBench上评估了13个领先的专有和开源MLLM。我们的研究结果表明：MLLMs在高层任务上表现优异，但在低层操作方面存在困难，最优模型GPT-4o的平均得分仅为28.9%。EmbodiedBench提供了一个多方面的标准评估平台，不仅突出了现有的挑战，还为推动基于MLLM的具身智能体提供了宝贵见解。我们的代码可在以下链接获取：[此 https URL]。 

---
# PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology 

**Title (ZH)**: PathFinder：一种应用于组织病理学的多模态多agent系统，用于医学诊断决策辅助 

**Authors**: Fatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O. Ikezogwo, Beibin Li, Tejoram Vivekanandan, Joann G. Elmore, Ranjay Krishna, Linda Shapiro  

**Link**: [PDF](https://arxiv.org/pdf/2502.08916)  

**Abstract**: Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at this https URL 

**Abstract (ZH)**: 通过对大尺寸切片图像（WSI）进行组织病理学诊断是现代病理学的基础，但WSI的高像素和复杂性构成了挑战。经过训练的病理学家通过导航WSI，寻找相关区域，做笔记，并整理这些信息以生成最终的整体诊断。传统的AI方法，如多个实例学习和基于Transformer的模型，在实现这样一种综合性、迭代性和多尺度的诊断程序方面力有未逮，限制了其在实际中的应用。我们提出了一种多模态、多代理框架PathFinder，模拟了专家病理学家的决策过程。PathFinder 综合了四个AI代理：分诊代理、导航代理、描述代理和诊断代理，它们协同工作，导航WSI，收集证据，并以自然语言提供全面的诊断结果。分诊代理将WSI分类为良性或有风险；如果有风险，导航代理和描述代理会迭代地聚焦于重要的区域，生成重要性地图和描述性见解，进一步分析采样的切片区域。最后，诊断代理综合这些发现，确定患者的诊断分类。实验结果显示，PathFinder 在皮肤黑色素瘤诊断上比最先进的方法高出8%，并通过自然语言描述诊断相关的切片区域实现了内置的可解释性。病理学家的定性分析表明，描述代理的输出质量较高，与GPT-4o相当。PathFinder 也是第一个在这一具有挑战性的黑色素瘤分类任务上超过平均病理学家表现的基于AI的系统，使其在病理学中成为高效、准确和可解释的AI辅助诊断的新标准。更多信息、代码和模型请参见此链接：[此 https URL] 

---
# EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges 

**Title (ZH)**: EnigmaEval：长多模态推理挑战基准 

**Authors**: Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks  

**Link**: [PDF](https://arxiv.org/pdf/2502.08859)  

**Abstract**: As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning. 

**Abstract (ZH)**: 随着语言模型掌握现有的推理基准，我们需要新的挑战来评估它们的认知边界。解谜事件是富含挑战性多模态问题的丰富资源，这些问题是测试广泛高级推理和知识能力的绝佳手段，这使它们成为评估前沿语言模型的独特测试床。我们引入了EnigmaEval数据集，该数据集来自解谜竞赛和活动中的问题及其解决方案，用于探索模型在进行潜在知识合成和多步演绎推理方面的能力。与现有的推理和知识基准不同，解谜挑战要求模型在看似无关的信息之间发现隐藏联系，以揭示解题路径。该基准包含1184个不同难度级别的谜题——每个谜题通常需要团队花费数小时至数天才能完成——并且具有明确、可验证的解决方案，这使得评估更加高效。最新的语言模型在这些谜题上的准确率极低，甚至低于其他具有挑战性的基准如人类最后考试（Humanity's Last Exam），这揭示了在要求不结构化和横向推理的问题面前，模型的局限性。 

---
# From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine 

**Title (ZH)**: 从大型语言模型到多模态AI：生成式AI在医疗领域潜力的范围研究 

**Authors**: Lukas Buess, Matthias Keicher, Nassir Navab, Andreas Maier, Soroosh Tayebi Arasteh  

**Link**: [PDF](https://arxiv.org/pdf/2502.09242)  

**Abstract**: Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare. 

**Abstract (ZH)**: 生成式人工智能（AI）模型，如扩散模型和OpenAI的ChatGPT，正在通过提高诊断准确性和自动化临床工作流程来改变医学领域。该领域发展迅速，从仅限文本的大语言模型（用于临床记录和决策支持）演变为空间多模态AI系统，能够在一个模型中整合各种数据模态，包括成像、文本和结构化数据。这些技术的多样化景观以及日益增长的兴趣，突显了全面审查其应用和潜力的需求。本综述探讨了多模态AI的发展，突出了其方法、应用、数据集以及在临床环境中的评估。按照PRISMA-ScR指南，我们系统地查询了PubMed、IEEE Xplore和Web of Science，优先选择到2024年底发表的最新研究。经过严格的筛选，最终纳入了144篇论文，揭示了这一动态领域的关键趋势和挑战。我们的研究结果强调了从单一模态到多模态方法的转变，推动了诊断支持、医疗报告生成、药物发现和对话AI等方面的创新。然而，仍然存在关键挑战，包括异质数据类型的集成、模型可解释性的提高、伦理问题的解决以及在真实临床环境中的AI系统验证。本综述总结了当前的技术水平，指出了关键缺口，并提供了指导，以促进可扩展的、可信的、在医疗保健中具有临床影响力的多模态AI解决方案的发展。 

---
# Visual Graph Question Answering with ASP and LLMs for Language Parsing 

**Title (ZH)**: 使用ASP和LLMs进行语言解析的视觉图问答 

**Authors**: Jakob Johannes Bauer, Thomas Eiter, Nelson Higuera Ruiz, Johannes Oetsch  

**Link**: [PDF](https://arxiv.org/pdf/2502.09211)  

**Abstract**: Visual Question Answering (VQA) is a challenging problem that requires to process multimodal input. Answer-Set Programming (ASP) has shown great potential in this regard to add interpretability and explainability to modular VQA architectures. In this work, we address the problem of how to integrate ASP with modules for vision and natural language processing to solve a new and demanding VQA variant that is concerned with images of graphs (not graphs in symbolic form). Images containing graph-based structures are an ubiquitous and popular form of visualisation. Here, we deal with the particular problem of graphs inspired by transit networks, and we introduce a novel dataset that amends an existing one by adding images of graphs that resemble metro lines. Our modular neuro-symbolic approach combines optical graph recognition for graph parsing, a pretrained optical character recognition neural network for parsing labels, Large Language Models (LLMs) for language processing, and ASP for reasoning. This method serves as a first baseline and achieves an overall average accuracy of 73% on the dataset. Our evaluation provides further evidence of the potential of modular neuro-symbolic systems, in particular with pretrained models that do not involve any further training and logic programming for reasoning, to solve complex VQA tasks. 

**Abstract (ZH)**: 视觉问答（VQA）是处理多模态输入的一个具有挑战性的问题。回答集编程（Answer-Set Programming, ASP）在这一领域展示了巨大的潜力，能够为模块化VQA架构增加解释性和可解释性。本文中，我们探讨了如何将ASP与视觉模块和自然语言处理模块集成，以解决一种新的且更具挑战性的VQA变体，该变体关注的是包含基于图的结构的图图像（而非符号形式的图）。基于图的结构图像是一种广泛使用的可视化形式。在此，我们处理了由公共交通网络启发的特殊图问题，并引入了一个新的数据集，该数据集通过增加了类似于地铁线路的图图像来修正现有数据集。我们的模块化神经符号方法结合了光学图识别以进行图解析、预训练的光学字符识别神经网络以进行标签解析、大型语言模型（Large Language Models, LLMs）进行语言处理以及ASP进行推理。该方法作为首个基线，达到了数据集整体平均准确率73%。我们的评估进一步证实了模块化神经符号系统的潜力，特别是那些不涉及进一步训练和逻辑编程推理的预训练模型，能够解决复杂的VQA任务。 

---
# TRIFFID: Autonomous Robotic Aid For Increasing First Responders Efficiency 

**Title (ZH)**: TRIFFID：自主机器人辅助提升首 responders效率工具 

**Authors**: Jorgen Cani, Panagiotis Koletsis, Konstantinos Foteinos, Ioannis Kefaloukos, Lampros Argyriou, Manolis Falelakis, Iván Del Pino, Angel Santamaria-Navarro, Martin Čech, Ondřej Severa, Alessandro Umbrico, Francesca Fracasso, AndreA Orlandini, Dimitrios Drakoulis, Evangelos Markakis, Georgios Th. Papadopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2502.09379)  

**Abstract**: The increasing complexity of natural disaster incidents demands innovative technological solutions to support first responders in their efforts. This paper introduces the TRIFFID system, a comprehensive technical framework that integrates unmanned ground and aerial vehicles with advanced artificial intelligence functionalities to enhance disaster response capabilities across wildfires, urban floods, and post-earthquake search and rescue missions. By leveraging state-of-the-art autonomous navigation, semantic perception, and human-robot interaction technologies, TRIFFID provides a sophisticated system com- posed of the following key components: hybrid robotic platform, centralized ground station, custom communication infrastructure, and smartphone application. The defined research and development activities demonstrate how deep neural networks, knowledge graphs, and multimodal information fusion can enable robots to autonomously navigate and analyze disaster environ- ments, reducing personnel risks and accelerating response times. The proposed system enhances emergency response teams by providing advanced mission planning, safety monitoring, and adaptive task execution capabilities. Moreover, it ensures real- time situational awareness and operational support in complex and risky situations, facilitating rapid and precise information collection and coordinated actions. 

**Abstract (ZH)**: 随着自然灾害事件复杂性的增加，需要创新的技术解决方案来支持一线救援人员的努力。本文介绍了TRIFFID系统，这是一个综合性的技术框架，将无人驾驶地面和空中车辆与先进的人工智能功能相结合，以增强在森林火灾、城市洪水和地震后的搜索与救援任务中的灾害响应能力。通过利用最先进的自主导航、语义感知和人机交互技术，TRIFFID提供了一个由以下关键组件组成的高级系统：混合机器人平台、集中式地面站、定制通信基础设施和智能手机应用程序。定义的研究与开发活动展示了深度神经网络、知识图谱和多模态信息融合如何使机器人能够自主导航和分析灾难环境，从而减少人员风险并加快响应时间。所提议的系统通过提供高级任务规划、安全保障和自适应任务执行能力，增强了应急响应团队。此外，它确保在复杂和危险的情况中实时情境感知和操作支持，从而促进快速而精确的信息收集和协调行动。 

---
# AIDE: Agentically Improve Visual Language Model with Domain Experts 

**Title (ZH)**: AIDE：由领域专家提升的代理改善视觉语言模型 

**Authors**: Ming-Chang Chiu, Fuxiao Liu, Karan Sapra, Andrew Tao, Yaser Jacoob, Xuezhe Ma, Zhiding Yu, Guilin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.09051)  

**Abstract**: The enhancement of Visual Language Models (VLMs) has traditionally relied on knowledge distillation from larger, more capable models. This dependence creates a fundamental bottleneck for improving state-of-the-art systems, particularly when no superior models exist. We introduce AIDE (Agentic Improvement through Domain Experts), a novel framework that enables VLMs to autonomously enhance their capabilities by leveraging specialized domain expert models. AIDE operates through a four-stage process: (1) identifying instances for refinement, (2) engaging domain experts for targeted analysis, (3) synthesizing expert outputs with existing data, and (4) integrating enhanced instances into the training pipeline. Experiments on multiple benchmarks, including MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve notable performance gains without relying on larger VLMs nor human supervision. Our framework provides a scalable, resource-efficient approach to continuous VLM improvement, addressing critical limitations in current methodologies, particularly valuable when larger models are unavailable to access. 

**Abstract (ZH)**: 视觉语言模型（VLMs）的传统增强依赖于从更大、更强大的模型中提取知识蒸馏。这种依赖性为改善最先进的系统设定了一个根本性的瓶颈，尤其是在没有更优模型可用的情况下。我们提出了AIDE（通过领域专家的自主提升），这是一种新型框架，它使VLMs能够自主增强其能力，通过利用专门的领域专家模型。AIDE 通过四阶段过程运作：（1）识别需要改进的实例，（2）邀请领域专家进行针对性分析，（3）将专家输出与现有数据相结合，以及（4）将增强的实例整合到训练管道中。在多个基准测试中，包括MMMU、MME、MMbench等，实验结果表明AIDE能够在不依赖更大规模的VLMs和人类监督的情况下实现显著的性能提升。我们的框架提供了一种可扩展且资源高效的持续改进VLM的方法，解决了当前方法的关键限制问题，尤其是在无法访问更大模型的情况下尤为宝贵。 

---
# 3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated Prompt Synthesis and Supervised Reasoning 

**Title (ZH)**: 基于3D场景的机器人任务规划视觉-语言框架：自动提示合成与监督推理 

**Authors**: Guoqin Tang, Qingxuan Jia, Zeyuan Huang, Gang Chen, Ning Ji, Zhipeng Yao  

**Link**: [PDF](https://arxiv.org/pdf/2502.08903)  

**Abstract**: Vision-language models (VLMs) have achieved remarkable success in scene understanding and perception tasks, enabling robots to plan and execute actions adaptively in dynamic environments. However, most multimodal large language models lack robust 3D scene localization capabilities, limiting their effectiveness in fine-grained robotic operations. Additionally, challenges such as low recognition accuracy, inefficiency, poor transferability, and reliability hinder their use in precision tasks. To address these limitations, we propose a novel framework that integrates a 2D prompt synthesis module by mapping 2D images to point clouds, and incorporates a small language model (SLM) for supervising VLM outputs. The 2D prompt synthesis module enables VLMs, trained on 2D images and text, to autonomously extract precise 3D spatial information without manual intervention, significantly enhancing 3D scene understanding. Meanwhile, the SLM supervises VLM outputs, mitigating hallucinations and ensuring reliable, executable robotic control code generation. Our framework eliminates the need for retraining in new environments, thereby improving cost efficiency and operational robustness. Experimental results that the proposed framework achieved a 96.0\% Task Success Rate (TSR), outperforming other methods. Ablation studies demonstrated the critical role of both the 2D prompt synthesis module and the output supervision module (which, when removed, caused a 67\% TSR drop). These findings validate the framework's effectiveness in improving 3D recognition, task planning, and robotic task execution. 

**Abstract (ZH)**: 视觉-语言模型（VLMs）在场景理解与感知任务中取得了显著的成功，使机器人能够在动态环境中适应性地规划和执行操作。然而，大多数多模态大语言模型缺乏稳健的3D场景定位能力，限制了它们在精细机器人操作中的效果。此外，诸如低识别精度、效率低下、可迁移性差和可靠性不足等挑战影响了其在精确任务中的应用。为了解决这些限制，我们提出了一种新的框架，该框架通过将2D图像映射到点云中，集成了一个2D提示合成模块，同时结合了一个小型语言模型（SLM）以监督VLM的输出。2D提示合成模块使在2D图像和文本上训练的VLM能够自主提取精确的3D空间信息，几乎不需要人工干预，从而显著提升了3D场景理解能力。同时，SLM监督VLM的输出，减少了幻觉现象，并确保生成可靠且可执行的机器人控制代码。该框架消除了在新环境中重新训练的需求，从而提高了成本效率和操作的稳健性。实验结果表明，所提出框架的任务成功率（TSR）达到了96.0%，显著优于其他方法。消融研究显示，2D提示合成模块和输出监督模块对于提升性能至关重要（当移除这两个模块时，TSR 下降了67%）。这些发现验证了该框架在改进3D识别、任务规划和机器人任务执行方面的有效性。 

---
# AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society 

**Title (ZH)**: 《AgentSociety：由大模型驱动的生成式智能体大规模仿真促进对人类行为和社会的理解》

这个标题翻译旨在保持原文的学术规范和研究主题的一致性，同时确保中文表达的自然流畅。 

**Authors**: Jinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhiheng Zheng, Jing Yi Wang, Di Zhou, Chen Gao, Fengli Xu, Fang Zhang, Ke Rong, Jun Su, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08691)  

**Abstract**: Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers. 

**Abstract (ZH)**: 理解人类行为和社会现象是社会科学的核心关注点，而生成性社会科学的兴起标志着一个重要的范式转变。通过利用自底向上的模拟方法，它取代了传统实验中的高昂成本和实施难度，采用了可扩展、可重复和系统性的计算方法来研究复杂的社会动态。近年来，大型语言模型（LLMs）的进展进一步改变了这一研究范式，使其能够生成类人的生成性社会代理和现实的社会模拟。在本文中，我们提出了一种大规模社会模拟器AgentSociety，该模拟器结合了由大型语言模型驱动的代理、现实的社会环境以及强大的大规模仿真引擎。基于该模拟器，我们为超过10000个代理生成了社会生活，模拟了它们之间的500万次交互，包括代理之间的交互以及代理与其环境之间的交互。此外，我们探讨了AgentSociety作为计算社会实验的试验场的潜力，重点关注四个关键的社会问题：极化、煽动性信息的传播、普遍基本收入政策的影响以及外部冲击如飓风等的影响。这四个问题为评估AgentSociety对典型研究方法的支持——如调查、访谈和干预——以及探讨社会问题的模式、原因和潜在机制提供了宝贵的案例。AgentSociety的结果与现实世界实验结果的一致性不仅展示了其捕捉人类行为及其潜在机制的能力，而且还强调了其作为社会科学家和政策制定者重要平台的潜力。 

---
