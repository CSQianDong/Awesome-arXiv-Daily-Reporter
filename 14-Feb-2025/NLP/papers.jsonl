{'arxiv_id': 'arXiv:2502.09606', 'title': 'Human-LLM Coevolution: Evidence from Academic Writing', 'authors': 'Mingmeng Geng, Roberto Trotta', 'link': 'https://arxiv.org/abs/2502.09606', 'abstract': 'With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as "delve", starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as "significant", has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency.', 'abstract_zh': '通过对arXiv论文摘要的统计分析，我们报告了在ChatGPT指出某些词汇过度使用后不久，这些词汇的使用频率出现了显著下降。而ChatGPT偏好使用的某些词汇，如“significant”，其使用频率则继续保持上升趋势。这些现象表明，一些学术论文的作者可能已经调整了其对大型语言模型（LLM）的使用方式，例如通过选择模型生成的内容或对生成内容进行修改。因此，人类与LLM的共同进化和合作为在实际场景中检测机器生成文本增加了额外的挑战。通过分析词汇频率来估算LLM对学术写作的影响仍然是可行的，并且应更加关注那些已经频繁使用的词汇，包括那些使用频率下降的词汇。', 'title_zh': '人类-大语言模型共进化：学术写作方面的证据'}
{'arxiv_id': 'arXiv:2502.09604', 'title': 'SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models', 'authors': 'Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih', 'link': 'https://arxiv.org/abs/2502.09604', 'abstract': 'We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.', 'abstract_zh': '我们引入了SelfCite，这是一种新颖的自监督方法，能够将语言模型（LLM）引导生成高质量、细粒度的句子级引用，以符合其生成响应中的声明。SelfCite 不仅仅依赖昂贵且劳动密集型的标注，而是通过上下文消除提供的奖励信号来利用LLM本身：如果需要引用，移除被引用的文本会阻止相同的响应生成；如果引用充分，则单独保留被引用文本即可保存相同的响应。这种奖励信号可以指导推断时的N-best采样策略，显著提高引用质量，同时也可以用于偏好优化直接微调模型以生成更好的引用。SelfCite 在LongBench-Cite基准上的五个长文本问答任务中，通过提高引用的F1值最多可达5.3个百分点，证明了其有效性。', 'title_zh': 'SelfCite：大型语言模型中上下文归因的自监督对齐方法'}
{'arxiv_id': 'arXiv:2502.09589', 'title': 'Logical forms complement probability in understanding language model (and human) performance', 'authors': 'Yixuan Wang, Freda Shi', 'link': 'https://arxiv.org/abs/2502.09589', 'abstract': "With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as orthogonal factors. In addition, we show similarities and differences between the logical reasoning performances of humans and LLMs by comparing LLM and human behavioral results.", 'abstract_zh': '随着对使用大规模语言模型（LLMs）进行自然语言规划的兴趣日益增加，理解它们的行为已成为一个重要的研究问题。本工作系统地探讨了LLMs在自然语言中进行逻辑推理的能力。我们引入了一个受控的数据集，其中包括命题逻辑和模态逻辑中的假设性与析取三段论，并将其用作研究LLMs性能的平台。我们的结果提供了关于预测LLMs行为的新见解：除了输入的概率（Gonen et al., 2023；McCoy et al., 2024）之外，逻辑形式应被视为独立的因素。此外，通过比较LLMs和人类的行为结果，我们展示了人类和LLMs在逻辑推理性能上的相似性和差异。', 'title_zh': '逻辑形式在理解语言模型（及人类）的表现中补充了概率方法'}
{'arxiv_id': 'arXiv:2502.09567', 'title': 'MorphNLI: A Stepwise Approach to Natural Language Inference Using Text Morphing', 'authors': 'Vlad Andrei Negru, Robert Vacareanu, Camelia Lemnaru, Mihai Surdeanu, Rodica Potolea', 'link': 'https://arxiv.org/abs/2502.09567', 'abstract': 'We introduce MorphNLI, a modular step-by-step approach to natural language inference (NLI). When classifying the premise-hypothesis pairs into {entailment, contradiction, neutral}, we use a language model to generate the necessary edits to incrementally transform (i.e., morph) the premise into the hypothesis. Then, using an off-the-shelf NLI model we track how the entailment progresses with these atomic changes, aggregating these intermediate labels into a final output. We demonstrate the advantages of our proposed method particularly in realistic cross-domain settings, where our method always outperforms strong baselines with improvements up to 12.6% (relative). Further, our proposed approach is explainable as the atomic edits can be used to understand the overall NLI label.', 'abstract_zh': '我们介绍了MorphNLI，一种模块化的逐步方法，用于自然语言推理（NLI）。在将前提-假设对分类为{蕴含、矛盾、中立}时，我们使用语言模型生成必要的编辑，以逐步转化（即，形态变化）前提为假设。然后，使用现成的NLI模型追踪这些原子变化对蕴含推断的影响，并将这些中间标签聚合为最终输出。我们在现实中的跨领域设置中特别展示了我们提出的这种方法的优势，我们的方法在与强基线对比时，表现有所提升，最高达到12.6%（相对提升）。此外，我们的提出的方法是可解释的，因为原子编辑可以用于理解整体的NLI标签。', 'title_zh': 'MorphNLI：一种基于文本形态变化的逐步自然语言推理方法'}
{'arxiv_id': 'arXiv:2502.09566', 'title': 'Zero-shot generation of synthetic neurosurgical data with large language models', 'authors': 'Austin A. Barr, Eddie Guo, Emre Sezgin', 'link': 'https://arxiv.org/abs/2502.09566', 'abstract': 'Clinical data is fundamental to advance neurosurgical research, but access is often constrained by data availability, small sample sizes, privacy regulations, and resource-intensive preprocessing and de-identification procedures. Synthetic data offers a potential solution to challenges associated with accessing and using real-world data (RWD). This study aims to evaluate the capability of zero-shot generation of synthetic neurosurgical data with a large language model (LLM), GPT-4o, by benchmarking with the conditional tabular generative adversarial network (CTGAN). Synthetic datasets were compared to real-world neurosurgical data to assess fidelity (means, proportions, distributions, and bivariate correlations), utility (ML classifier performance on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated datasets matched or exceeded CTGAN performance, despite no fine-tuning or access to RWD for pre-training. Datasets demonstrated high univariate and bivariate fidelity to RWD without directly exposing any real patient records, even at amplified sample size. Training an ML classifier on GPT-4o-generated data and testing on RWD for a binary prediction task showed an F1 score (0.706) with comparable performance to training on the CTGAN data (0.705) for predicting postoperative functional status deterioration. GPT-4o demonstrated a promising ability to generate high-fidelity synthetic neurosurgical data. These findings also indicate that data synthesized with GPT-4o can effectively augment clinical data with small sample sizes, and train ML models for prediction of neurosurgical outcomes. Further investigation is necessary to improve the preservation of distributional characteristics and boost classifier performance.', 'abstract_zh': '临床数据是推进神经外科研究的基础，但访问和使用这些数据常常受到数据可用性、样本量小、隐私法规以及耗时的预处理和去标识化程序的限制。合成数据为解决访问和使用真实世界数据（RWD）相关的挑战提供了一种潜在的解决方案。本研究旨在通过与条件性表格生成对抗网络（CTGAN）进行基准测试，评估大型语言模型（LLM）GPT-4o零样本生成神经外科合成数据的能力。合成数据集与真实的神经外科数据进行了比较，以评估其真实度（均值、比例、分布和双变量相关性）、效用（RWD上的ML分类器性能）和隐私（RWD中患者记录的重复情况）。尽管GPT-4o未经过微调也未访问用于预训练的RWD数据，但生成的的数据集在真实度、双变量真实度方面与CTGAN相当，即使在样本量放大时也未直接暴露任何真实患者的记录。在二分类预测任务中，使用GPT-4o生成的数据训练ML分类器，并在RWD上进行测试，显示的F1分数（0.706）与使用CTGAN数据训练的F1分数（0.705）相当，用于预测术后功能状态恶化。GPT-4o展示了生成高质量合成神经外科数据的潜力。这些发现还表明，使用GPT-4o合成的数据可以有效补充小样本临床数据，并训练机器学习模型以预测神经外科结果。为进一步提高分布特征的保留和增强分类器性能，进一步的研究是必要的。', 'title_zh': '使用大型语言模型进行零样本合成神经外科数据生成'}
{'arxiv_id': 'arXiv:2502.09532', 'title': 'Mind the Gap! Choice Independence in Using Multilingual LLMs for Persuasive Co-Writing Tasks in Different Languages', 'authors': 'Shreyan Biswas, Alexander Erlei, Ujwal Gadiraju', 'link': 'https://arxiv.org/abs/2502.09532', 'abstract': "Recent advances in generative AI have precipitated a proliferation of novel writing assistants. These systems typically rely on multilingual large language models (LLMs), providing globalized workers the ability to revise or create diverse forms of content in different languages. However, there is substantial evidence indicating that the performance of multilingual LLMs varies between languages. Users who employ writing assistance for multiple languages are therefore susceptible to disparate output quality. Importantly, recent research has shown that people tend to generalize algorithmic errors across independent tasks, violating the behavioral axiom of choice independence. In this paper, we analyze whether user utilization of novel writing assistants in a charity advertisement writing task is affected by the AI's performance in a second language. Furthermore, we quantify the extent to which these patterns translate into the persuasiveness of generated charity advertisements, as well as the role of peoples' beliefs about LLM utilization in their donation choices. Our results provide evidence that writers who engage with an LLM-based writing assistant violate choice independence, as prior exposure to a Spanish LLM reduces subsequent utilization of an English LLM. While these patterns do not affect the aggregate persuasiveness of the generated advertisements, people's beliefs about the source of an advertisement (human versus AI) do. In particular, Spanish-speaking female participants who believed that they read an AI-generated advertisement strongly adjusted their donation behavior downwards. Furthermore, people are generally not able to adequately differentiate between human-generated and LLM-generated ads. Our work has important implications for the design, development, integration, and adoption of multilingual LLMs as assistive agents -- particularly in writing tasks.", 'abstract_zh': '近年来生成式AI的进展催生了各种新型写作助手。这些系统通常依赖于多语言大型语言模型（LLMs），使全球工作者能够用不同的语言修订或创作多样化的内容。然而，有大量证据表明，多语言LLMs在不同语言中的表现存在差异。使用多语言写作助手的用户因此可能会遇到输出质量不一致的问题。最近的研究显示，人们倾向于将算法错误泛化到独立任务中，违背了选择独立性的行为准则。本文通过分析，在慈善广告写作任务中，用户对基于LLM的写作助手的使用是否受到AI在第二语言中的表现影响。我们还量化了这些模式如何影响生成广告的说服力，以及人们对LLM使用信念在捐款选择中的作用。我们的研究结果表明，在先前接触过西班牙语LLM后，使用英语LLM的写作受试者违反了选择独立性。虽然这些模式并不影响生成广告的整体说服力，但人们对广告来源（人类 vs AI）的信念确实有影响。特别是，相信读到的是AI生成广告的西班牙语女性参与者，强烈调整了他们的捐款行为。此外，人们普遍无法准确区分由人类或LLM生成的广告。我们的研究对多语言LLMs作为辅助工具的设计、开发、集成和采用——尤其是写作任务——具有重要启示。', 'title_zh': '注意差距！在不同语言的多语言大语言模型辅助写作任务中的选择独立性'}
{'arxiv_id': 'arXiv:2502.09497', 'title': 'Improve LLM-based Automatic Essay Scoring with Linguistic Features', 'authors': 'Zhaoyi Joey Hou, Alejandro Ciuba, Xiang Lorraine Li', 'link': 'https://arxiv.org/abs/2502.09497', 'abstract': 'Automatic Essay Scoring (AES) assigns scores to student essays, reducing the grading workload for instructors. Developing a scoring system capable of handling essays across diverse prompts is challenging due to the flexibility and diverse nature of the writing task. Existing methods typically fall into two categories: supervised feature-based approaches and large language model (LLM)-based methods. Supervised feature-based approaches often achieve higher performance but require resource-intensive training. In contrast, LLM-based methods are computationally efficient during inference but tend to suffer from lower performance. This paper combines these approaches by incorporating linguistic features into LLM-based scoring. Experimental results show that this hybrid method outperforms baseline models for both in-domain and out-of-domain writing prompts.', 'abstract_zh': '自动作文评分（AES）通过对学生的作文进行评分，减轻了教师的评分负担。开发一个能够处理不同提示作文的评分系统是一项挑战，因为写作任务具有灵活性和多样性。现有的方法通常可以分为两类：监督特征基方法和大型语言模型（LLM）基方法。监督特征基方法通常能够实现更高的性能，但需要进行资源密集型的训练。相比之下，LLM基方法在推理过程中的计算效率较高，但在性能上往往较低。本文通过将语言特征纳入LLM基评分系统中，结合了这两种方法。实验结果表明，这种混合方法在领域内和领域外的写作提示中均优于基线模型。', 'title_zh': '基于语言特征提高大规模语言模型的自动作文评分准确性'}
{'arxiv_id': 'arXiv:2502.09487', 'title': 'Objective quantification of mood states using large language models', 'authors': 'Jakub Onysk, Quentin Huys', 'link': 'https://arxiv.org/abs/2502.09487', 'abstract': 'Emotional states influence human behaviour and cognition, leading to diverse thought trajectories. Similarly, Large Language Models (LLMs) showcase an excellent level of response consistency across wide-ranging contexts (prompts). We leverage these parallels to establish a framework for quantifying mental states. Our approach utilises self-report questionnaires that reliably assess these states due to their inherent sensitivity to patterns of co-occurring responses. Specifically, we recruited a large sample of participants (N=422) to investigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set of depressive mood states measured with participants\' open-ended responses to a depression questionnaire. We show LLM responses to held-out multiple-choice questions, given participants\' open-ended answers, correlate strongly (r: 0.52-0.84) with true questionnaire scores, demonstrating LLM\'s generalisation from mood representations. We explore a link between these representations and factor analysis. Using ridge regression, we find depression-related subspaces within LLM hidden states. We show these subspaces to be predictive of participants\' "Depression" and "Somatic & Emotional Distress" factor scores, as well as suicidality severity. Overall, LLMs can provide quantitative measures of mental states. The reliability of these hinges upon how informative the questions we ask participants are. Used correctly, this approach could supplement mental state assessment in a variety of settings.', 'abstract_zh': '情感状态影响人类的行为和认知，进而导致不同的思想轨迹。同样，大型语言模型（LLMs）在广泛情景下展现了极高的回应一致性。我们利用这些相似性来构建一种量化心理状态的框架。我们的方法采用自陈问卷，因为它们能够可靠地评估这些状态，尤其是由于它们对共现响应模式的敏感性。具体来说，我们招募了422名参与者来研究LLM（Mistral-7B-OpenOrca）如何通过基于参与者开放回答抑郁情绪问卷的问题来量化一组多元的抑郁情绪状态。我们发现，当给定参与者开放回答的多个选择题回答时，LLM的回答与真实问卷分数（相关系数r: 0.52-0.84）高度相关，这表明LLM从情绪表示中进行了泛化。我们探讨了这些表示与因子分析之间的关联。通过岭回归，我们发现LLM隐藏状态中的与抑郁相关的子空间。这些子空间能够预测参与者抑郁因素评分、体感与情绪困扰因素评分，以及自杀严重程度。总体而言，大型语言模型可以提供心理状态的量化指标。这些指标的可靠性取决于我们提出的问题是否能够提供足够的信息。如果正确使用，这一方法可以补充各种环境中的心理状态评估。', 'title_zh': '使用大型语言模型对情绪状态进行客观量化'}
{'arxiv_id': 'arXiv:2502.09457', 'title': 'The Multilingual Mind : A Survey of Multilingual Reasoning in Language Models', 'authors': 'Akash Ghosh, Debayan Datta, Sriparna Saha, Chirag Agarwal', 'link': 'https://arxiv.org/abs/2502.09457', 'abstract': 'While reasoning and multilingual capabilities in Language Models (LMs) have achieved remarkable progress in recent years, their integration into a unified paradigm, multilingual reasoning, is at a nascent stage. Multilingual reasoning requires language models to handle logical reasoning across languages while addressing misalignment, biases, and challenges in low-resource settings. This survey provides the first in-depth review of multilingual reasoning in LMs. In this survey, we provide a systematic overview of existing methods that leverage LMs for multilingual reasoning, specifically outlining the challenges, motivations, and foundational aspects of applying language models to reason across diverse languages. We provide an overview of the standard data resources used for training multilingual reasoning in LMs and the evaluation benchmarks employed to assess their multilingual capabilities. Next, we analyze various state-of-the-art methods and their performance on these benchmarks. Finally, we explore future research opportunities to improve multilingual reasoning in LMs, focusing on enhancing their ability to handle diverse languages and complex reasoning tasks.', 'abstract_zh': '近年来，语言模型（LMs）在推理和多语言能力方面取得了显著进展，但将这些能力整合到统一的多语言推理框架中仍处于起步阶段。多语言推理需要语言模型在处理不同语言中的逻辑推理时，同时解决语言间的不一致、偏差以及低资源环境中的挑战。本文综述是关于多语言推理在语言模型中的首个深入回顾。在本文中，我们提供了一种系统性的概览，介绍了现有利用语言模型进行多语言推理的方法，特别阐述了将语言模型应用于跨语言推理时所面临的问题、动机及其基础方面。我们回顾了用于训练多语言推理的语言模型的标准数据资源，以及用于评估其多语言能力的评估基准。接着，我们分析了各种最先进的方法及其在这些基准上的表现。最后，我们探讨了未来的研究机会，以提高语言模型在处理复杂推理任务和多种语言方面的能力。', 'title_zh': '多语言思维：语言模型中的多语言推理综述'}
{'arxiv_id': 'arXiv:2502.09419', 'title': 'On multi-token prediction for efficient LLM inference', 'authors': 'Somesh Mehra, Javier Alonso Garcia, Lukas Mauch', 'link': 'https://arxiv.org/abs/2502.09419', 'abstract': 'We systematically investigate multi-token prediction (MTP) capabilities within LLMs pre-trained for next-token prediction (NTP). We first show that such models inherently possess MTP capabilities via numerical marginalization over intermediate token probabilities, though performance is data-dependent and improves with model scale. Furthermore, we explore the challenges of integrating MTP heads into frozen LLMs and find that their hidden layers are strongly specialized for NTP, making adaptation non-trivial. Finally, we show that while joint training of MTP heads with the backbone improves performance, it cannot fully overcome this barrier, prompting further research in this direction. Our findings provide a deeper understanding of MTP applied to pretrained LLMs, informing strategies for accelerating inference through parallel token prediction.', 'abstract_zh': '我们系统地研究了用于后续令牌预测（Next-token Prediction, NTP）预训练的大型语言模型（LLMs）中的多令牌预测（Multi-token Prediction, MTP）能力。我们首先表明，这类模型本质上具备MTP能力，这是因为通过数值方式对中间令牌概率进行边缘化处理。尽管其性能依赖于数据，但随模型规模的增大而提升。此外，我们探讨了将MTP头部集成到冻结的LLM中所面临的挑战，并发现在其隐藏层中，这类模型主要针对NTP进行了高度专业化，使得适应过程变得相当复杂。最后，我们证明通过骨干网络和MTP头部的联合训练可以提高性能，但无法完全克服这一障碍，这促使我们在该方向上开展进一步研究。我们的发现为MTP在预训练LLM中的应用提供了更深入的理解，并为通过并行令牌预测加速推理策略提供了指导。', 'title_zh': '高效的LLM推理中的多令牌预测方法'}
{'arxiv_id': 'arXiv:2502.09416', 'title': 'Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?', 'authors': 'Takumi Goto, Yusuke Sakai, Taro Watanabe', 'link': 'https://arxiv.org/abs/2502.09416', 'abstract': 'One of the goals of automatic evaluation metrics in grammatical error correction (GEC) is to rank GEC systems such that it matches human preferences. However, current automatic evaluations are based on procedures that diverge from human evaluation. Specifically, human evaluation derives rankings by aggregating sentence-level relative evaluation results, e.g., pairwise comparisons, using a rating algorithm, whereas automatic evaluation averages sentence-level absolute scores to obtain corpus-level scores, which are then sorted to determine rankings. In this study, we propose an aggregation method for existing automatic evaluation metrics which aligns with human evaluation methods to bridge this gap. We conducted experiments using various metrics, including edit-based metrics, $n$-gram based metrics, and sentence-level metrics, and show that resolving the gap improves results for the most of metrics on the SEEDA benchmark. We also found that even BERT-based metrics sometimes outperform the metrics of GPT-4. We publish our unified implementation of the metrics and meta-evaluations.', 'abstract_zh': '自动评估指标在语法错误修正（GEC）中的一个目标是根据人类偏好对GEC系统进行排名。然而，当前的自动评估方法遵循的程序与人类评估的方法相背离。具体来说，人类评估通过聚合句子级别的相对评估结果（例如，成对比较）并使用评分算法来确定排名，而自动评估则通过计算句子级别的绝对分数的平均值来获得语料库级别的分数，然后对其进行排序以确定排名。本研究中，我们提出了一种现有自动评估指标的聚合方法，该方法与人类评估方法相一致，以弥合这一差距。我们在使用多种指标进行实验后，包括基于编辑的指标、基于$n$-克隆的指标和句子级别的指标，并证明解决这一差距可以显著提高绝大多数指标在SEEDA基准上的结果。我们还发现，即使基于BERT的指标有时也能超越GPT-4的指标。我们发布了一个统一的指标和元评估实现。', 'title_zh': '重新思考语法错误修正的评估指标：为何使用与人类不同的评估过程？'}
{'arxiv_id': 'arXiv:2502.09390', 'title': 'SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models', 'authors': 'Daniel Fleischer, Moshe Berchansky, Gad Markovits, Moshe Wasserblat', 'link': 'https://arxiv.org/abs/2502.09390', 'abstract': "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at this https URL.", 'abstract_zh': '在自然语言处理这一迅速发展的领域中，大型语言模型（LLMs）被赋予了解决更加复杂的推理挑战的任务。传统的链式思考提示等方法虽然显示出一定的潜力，但在充分利用模型的推理能力方面经常存在不足。本文介绍了SQuARE（Sequential Question Answering Reasoning Engine）这一新颖的提示技术，它旨在通过自我提问的方式改善推理能力。SQuARE基于链式思考（CoT）框架，促使模型生成并解决多个辅助问题，然后再处理主要查询，从而促进对主题各个方面进行更加全面的探索。我们在Llama 3和GPT-4o模型上，采用多个问答数据集进行了广泛评估，结果表明SQuARE显著优于传统的链式思考提示和现有的重述并回应方法。通过系统地分解查询，SQuARE促进了LLM在推理任务中的能力提升。代码已公开，可访问以下网址：[该网址]。', 'title_zh': 'SQuARE：增强大型语言模型链式思考的序列问答推理引擎'}
{'arxiv_id': 'arXiv:2502.09387', 'title': 'Truth Knows No Language: Evaluating Truthfulness Beyond English', 'authors': 'Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri', 'link': 'https://arxiv.org/abs/2502.09387', 'abstract': 'We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.', 'abstract_zh': '我们介绍了一个专业的翻译扩展版本的TruthfulQA基准，旨在评估巴斯克语、加泰罗尼亚语、加利西亚语和西班牙语中的真实性。在大型语言模型（LLMs）的真实性评估中，绝大部分研究工作主要集中在英语上。然而，LLMs在不同语言中维持真实性的能力仍存在较少的研究。我们的研究评估了12个最先进的开源LLMs，通过人工评估、多项选择度量标准以及LLM作为裁判的评分机制，比较了基础模型和指令调优模型。我们的研究结果表明，尽管LLMs在英语中的表现最好，在巴斯克语（最低资源语言）中的表现最差，但不同语言之间整体真实性差异小于预期。此外，我们发现，LLM作为裁判与其人类判断的相关性比多项选择度量标准更强，信息量在真实性评估中起着关键作用。研究结果还表明，机器翻译为将真实性基准扩展到其他语言提供了可行的方法，为专业翻译提供了一种可扩展的替代方案。最后，我们观察到，关于普适知识的问题在语言间处理得比时间或情境依赖的问题更好，强调了要考虑文化和时间差异的真实性评估的需求。数据集和代码采用开源许可形式公开。', 'title_zh': '真理无所语言限：超越英语评估真实性'}
{'arxiv_id': 'arXiv:2502.09331', 'title': 'Beyond English: The Impact of Prompt Translation Strategies across Languages and Tasks in Multilingual LLMs', 'authors': 'Itai Mondshine, Tzuf Paz-Argaman, Reut Tsarfaty', 'link': 'https://arxiv.org/abs/2502.09331', 'abstract': 'Despite advances in the multilingual capabilities of Large Language Models (LLMs) across diverse tasks, English remains the dominant language for LLM research and development. So, when working with a different language, this has led to the widespread practice of pre-translation, i.e., translating the task prompt into English before inference. Selective pre-translation, a more surgical approach, focuses on translating specific prompt components. However, its current use is sporagic and lacks a systematic research foundation. Consequently, the optimal pre-translation strategy for various multilingual settings and tasks remains unclear. In this work, we aim to uncover the optimal setup for pre-translation by systematically assessing its use. Specifically, we view the prompt as a modular entity, composed of four functional parts: instruction, context, examples, and output, either of which could be translated or not. We evaluate pre-translation strategies across 35 languages covering both low and high-resource languages, on various tasks including Question Answering (QA), Natural Language Inference (NLI), Named Entity Recognition (NER), and Abstractive Summarization. Our experiments show the impact of factors as similarity to English, translation quality and the size of pre-trained data, on the model performance with pre-translation. We suggest practical guidelines for choosing optimal strategies in various multilingual settings.', 'abstract_zh': '尽管大规模语言模型（LLMs）在多种任务中的多语言能力取得了进步，但英语仍然是LLM研究和开发中的主导语言。因此，在处理不同语言时，广泛采用了提前翻译，即在推理前将任务提示翻译成英语。选择性提前翻译是一种更为精细的方法，专注于翻译特定的提示组件。然而，目前这种方法的应用是零散的，缺乏系统的研究基础。因此，对于各种多语言环境和任务，最优的提前翻译策略仍然不清楚。在这项工作中，我们旨在通过系统评估提前翻译的使用情况来揭示其最优设置。具体而言，我们将提示视为一个模块化实体，由指令、背景、示例和输出四个功能部分组成，其中任一部分都可以选择翻译或不翻译。我们在35种语言上评估了提前翻译策略，涵盖低资源语言和高资源语言，并在问答（QA）、自然语言推理（NLI）、命名实体识别（NER）和抽象总结等多种任务上进行了评估。我们的实验展示了翻译相似度、翻译质量以及预训练数据量等因素对带有提前翻译的模型性能的影响。我们提出了适用于各种多语言环境的实用指南，以选择最优策略。', 'title_zh': '超越英语：多语言LLM中跨语言和任务的提示翻译策略影响研究'}
{'arxiv_id': 'arXiv:2502.09316', 'title': 'A Judge-free LLM Open-ended Generation Benchmark Based on the Distributional Hypothesis', 'authors': 'Kentaro Imajo, Masanori Hirano, Shuji Suzuki, Hiroaki Mikami', 'link': 'https://arxiv.org/abs/2502.09316', 'abstract': "Evaluating the open-ended text generation of large language models (LLMs) is challenging because of the lack of a clear ground truth and the high cost of human or LLM-based assessments. We propose a novel benchmark that evaluates LLMs using n-gram statistics and rules, without relying on human judgement or LLM-as-a-judge approaches. Using 50 question and reference answer sets, we introduce three new metrics based on n-grams and rules: Fluency, Truthfulness, and Helpfulness. Our benchmark strongly correlates with GPT-4o-based evaluations while requiring significantly fewer computational resources, demonstrating its effectiveness as a scalable alternative for assessing LLMs' open-ended generation capabilities.", 'abstract_zh': '评估大规模语言模型（LLMs）的开放式文本生成具有挑战性，因为缺乏明确的 ground truth，并且依赖人类或基于LLM的评估成本较高。我们提出了一种新的基准方法，该方法使用n-元统计和规则来评估LLMs，无需依赖人类判断或LLM作为评判者的方法。利用50个问题和参考答案集，我们引入了基于n-元和规则的三个新指标：流畅性、真实性、和有用性。我们的基准与基于GPT-4o的评估高度相关，但所需计算资源显著减少，证明了其作为评估LLMs开放式生成能力的可扩展替代方法的有效性。', 'title_zh': '基于分布假设的无裁判ULLM开放生成基准'}
{'arxiv_id': 'arXiv:2502.09307', 'title': 'When the LM misunderstood the human chuckled: Analyzing garden path effects in humans and language models', 'authors': 'Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant', 'link': 'https://arxiv.org/abs/2502.09307', 'abstract': "Modern Large Language Models (LLMs) have shown human-like abilities in many language tasks, sparking interest in comparing LLMs' and humans' language processing. In this paper, we conduct a detailed comparison of the two on a sentence comprehension task using garden-path constructions, which are notoriously challenging for humans. Based on psycholinguistic research, we formulate hypotheses on why garden-path sentences are hard, and test these hypotheses on human participants and a large suite of LLMs using comprehension questions. Our findings reveal that both LLMs and humans struggle with specific syntactic complexities, with some models showing high correlation with human comprehension. To complement our findings, we test LLM comprehension of garden-path constructions with paraphrasing and text-to-image generation tasks, and find that the results mirror the sentence comprehension question results, further validating our findings on LLM understanding of these constructions.", 'abstract_zh': '现代大型语言模型（LLMs）在许多语言任务中展示了类似人类的能力，引发了将LLMs与人类的语言处理进行比较的兴趣。在这篇论文中，我们使用花园路径句子（garden-path constructions）进行一项详细的句子理解任务比较，这些句子对人类来说历来极具挑战性。基于心理学语言学研究，我们提出了关于为什么花园路径句子难以理解的假设，并通过使用理解问题在人类参与者和大量LLMs之间进行了测试。我们的发现表明，无论是人类还是LLM在处理特定的句法复杂性时都存在困难，一些模型在理解方面与人类高度相关。为补充这些发现，我们还对LLMs进行了花园路径句子的改写和文本到图像生成任务的测试，并发现结果与句子理解问题的结果相似，进一步验证了我们在这些结构的理解方面的发现。', 'title_zh': '当LM误解了人类的轻声笑：分析人类和语言模型中的 garden path 效应'}
{'arxiv_id': 'arXiv:2502.09284', 'title': 'SparQLe: Speech Queries to Text Translation Through LLMs', 'authors': 'Amirbek Djanibekov, Hanan Aldarmaki', 'link': 'https://arxiv.org/abs/2502.09284', 'abstract': 'With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.', 'abstract_zh': '随着大型语言模型（LLMs）影响力的不断增长，将语音表示与之集成以实现更加无缝的多模态处理和语音理解引起了越来越多的兴趣。本研究提出了一种新颖的方法，该方法结合自监督语音表示与指令调优的大语言模型，用于语音转文本翻译。所提出的方法利用模态适配器，通过使用英语数据将提取的语音特征与指令调优的大语言模型对齐。我们的实验证明，该方法有效地保留了输入语音的语义内容，并且是自监督语音模型与指令调优的大语言模型之间有效的桥梁，为各种语音理解应用提供了有希望的解决方案。', 'title_zh': 'SparQLe：通过大型语言模型将语音查询翻译为文本'}
{'arxiv_id': 'arXiv:2502.09247', 'title': 'The Joint Entity-Relation Extraction Model Based on Span and Interactive Fusion Representation for Chinese Medical Texts with Complex Semantics', 'authors': 'Danni Feng, Runzhi Li, Jing Wang, Siyu Yan, Lihong Ma, Yunli Xing', 'link': 'https://arxiv.org/abs/2502.09247', 'abstract': 'Joint entity-relation extraction is a critical task in transforming unstructured or semi-structured text into triplets, facilitating the construction of large-scale knowledge graphs, and supporting various downstream applications. Despite its importance, research on Chinese text, particularly with complex semantics in specialized domains like medicine, remains limited. To address this gap, we introduce the CH-DDI, a Chinese drug-drug interactions dataset designed to capture the intricacies of medical text. Leveraging the strengths of attention mechanisms in capturing long-range dependencies, we propose the SEA module, which enhances the extraction of complex contextual semantic information, thereby improving entity recognition and relation extraction. Additionally, to address the inefficiencies of existing methods in facilitating information exchange between entity recognition and relation extraction, we present an interactive fusion representation module. This module employs Cross Attention for bidirectional information exchange between the tasks and further refines feature extraction through BiLSTM. Experimental results on both our CH-DDI dataset and public CoNLL04 dataset demonstrate that our model exhibits strong generalization capabilities. On the CH-DDI dataset, our model achieves an F1-score of 96.73% for entity recognition and 78.43% for relation extraction. On the CoNLL04 dataset, it attains an entity recognition precision of 89.54% and a relation extraction accuracy of 71.64%.', 'abstract_zh': '联合实体关系抽取是将未结构化或半结构化文本转换为三元组的重要任务，有助于构建大规模知识图谱，并支持各种下游应用。尽管这一任务的重要性不言而喻，但在中文文本领域，特别是在医学等专业领域中，复杂语义的研究仍然有限。为弥补这一空白，我们引入了CH-DDI数据集，它是专为中国药物相互作用文本设计，旨在捕捉医学文本的复杂性。基于注意机制捕获长距离依赖的优势，我们提出了SEA模块，该模块增强了复杂上下文语义信息的抽取，从而提高实体识别和关系抽取的效果。此外，为了应对现有方法在实体识别与关系抽取之间信息交换效率低下的问题，我们提出了一种交互融合表示模块。该模块通过交叉注意机制实现任务之间的双向信息交换，并通过双向长短期记忆网络（BiLSTM）进一步细化特征提取。我们在CH-DDI数据集和公开的CoNLL04数据集上进行的实验结果表明，我们的模型具有较强的泛化能力。在CH-DDI数据集上，我们的模型在实体识别任务中达到了96.73%的F1分数，在关系抽取任务中达到了78.43%的分数。在CoNLL04数据集上，其实体识别精确度为89.54%，关系抽取准确率为71.64%。', 'title_zh': '基于区间和交互融合表示的汉语医学文本复杂语义联合实体-关系抽取模型'}
{'arxiv_id': 'arXiv:2502.09231', 'title': 'Answer Set Counting and its Applications', 'authors': 'Mohimenul Kabir', 'link': 'https://arxiv.org/abs/2502.09231', 'abstract': 'We have focused on Answer Set Programming (ASP), more specifically, answer set counting, exploring both exact and approximate methodologies. We developed an exact ASP counter, sharpASP, which utilizes a compact encoding for propositional formulas, significantly enhancing efficiency compared to existing methods that often struggle with inefficient encodings. Our evaluations indicate that sharpASP outperforms current ASP counters on several benchmarks. In addition, we proposed an approximate ASP counter, named ApproxASP, a hashing-based counter integrating Gauss-Jordan elimination within the ASP solver, clingo. As a practical application, we employed ApproxASP for network reliability estimation, demonstrating superior performance over both traditional reliability estimators and #SAT-based methods.', 'abstract_zh': '我们专注于回答集编程（Answer Set Programming，ASP），更具体地说，是回答集计数，探索了精确和近似的方法。我们开发了一个精确的ASP计数器sharpASP，它利用紧凑的命题公式编码，显著提高了效率，而现有的方法往往因编码效率低下而受到影响。我们的评估表明，在多个基准上，sharpASP 在性能上优于现有的ASP计数器。此外，我们还提出了一种近似ASP计数器，名为ApproxASP，它是一种基于哈希的计数器，结合了Gauss-Jordan消去法在ASP求解器clingo中的应用。作为一种实际应用，我们使用ApproxASP 对网络可靠性进行估计，表明其性能优于传统的可靠性估计方法和基于#SAT的方法。', 'title_zh': '回答集计数及其应用'}
{'arxiv_id': 'arXiv:2502.09192', 'title': 'Thinking beyond the anthropomorphic paradigm benefits LLM research', 'authors': 'Lujain Ibrahim, Myra Cheng', 'link': 'https://arxiv.org/abs/2502.09192', 'abstract': 'Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise. In this position paper, we analyze hundreds of thousands of computer science research articles from the past decade and present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs). This terminology reflects deeper anthropomorphic conceptualizations which shape how we think about and conduct LLM research. We argue these conceptualizations may be limiting, and that challenging them opens up new pathways for understanding and improving LLMs beyond human analogies. To illustrate this, we identify and analyze five core anthropomorphic assumptions shaping prominent methodologies across the LLM development lifecycle, from the assumption that models must use natural language for reasoning tasks to the assumption that model capabilities should be evaluated through human-centric benchmarks. For each assumption, we demonstrate how non-anthropomorphic alternatives can open new directions for research and development.', 'abstract_zh': '赋予人类特征给技术，即把人的特质属性赋予给技术设备，是一种自动且无意识的反应，即使是技术专家也不例外。在本文中，我们分析了过去十年中数十万篇计算机科学研究文章，并展示了对大规模语言模型（LLMs）研究中赋予人类特征术语的普遍性和增长趋势的实证证据。这些术语反映了更深层的人类中心概念化，这些概念化塑造了我们对LLMs的研究思维和方法。我们认为这些概念化可能具有局限性，挑战这些概念化将开辟超出人类类比的新途径，以理解和改进LLMs。为了说明这一点，我们确定并分析了五项核心的人类中心假设，这些假设塑造了LLMs开发生命周期中的主流方法论，从模型必须使用自然语言进行推理任务到模型能力应该通过以人为主体的基准进行评估。对于每项假设，我们都展示了非人类中心的替代方案如何为研究和开发开辟新的方向。', 'title_zh': '超越拟人类化范式有利于大语言模型研究'}
{'arxiv_id': 'arXiv:2502.09188', 'title': 'Matina: A Large-Scale 73B Token Persian Text Corpus', 'authors': 'Sara Bourbour Hosseinbeigi, Fatemeh Taherinezhad, Heshaam Faili, Hamed Baghbani, Fatemeh Nadi, Mostafa Amiri', 'link': 'https://arxiv.org/abs/2502.09188', 'abstract': 'Text corpora are essential for training models used in tasks like summarization, translation, and large language models (LLMs). While various efforts have been made to collect monolingual and multilingual datasets in many languages, Persian has often been underrepresented due to limited resources for data collection and preprocessing. Existing Persian datasets are typically small and lack content diversity, consisting mainly of weblogs and news articles. This shortage of high-quality, varied data has slowed the development of NLP models and open-source LLMs for Persian. Since model performance depends heavily on the quality of training data, we address this gap by introducing the Matina corpus, a new Persian dataset of 72.9B tokens, carefully preprocessed and deduplicated to ensure high data quality. We further assess its effectiveness by training and evaluating transformer-based models on key NLP tasks. Both the dataset and preprocessing codes are publicly available, enabling researchers to build on and improve this resource for future Persian NLP advancements.', 'abstract_zh': '文本语料库对于训练用于摘要、翻译和大规模语言模型（LLM）等任务的模型至关重要。虽然在多种语言中已经做出了各种努力来收集单语和多语数据集，但由于数据收集和预处理资源有限，波斯语通常代表性不足。现有的波斯语数据集通常规模较小且缺乏内容多样性，主要由博客和新闻文章组成。这一高质量、多样化数据的短缺延缓了波斯语NLP模型和开源LLM的发展。由于模型性能高度依赖于训练数据的质量，我们通过引入Materiala语料库来填补这一空白，这是一个包含729亿词的新波斯语数据集，经过精心预处理和去重以确保高质量的数据。我们进一步通过在关键NLP任务上训练和评估基于变换器的模型来评估其有效性。该数据集及其预处理代码已公开，为研究人员提供了进一步开发和改进这一资源以推动未来波斯语NLP的发展。', 'title_zh': '玛蒂娜：一个大规模730亿tokens波斯文文本语料库'}
{'arxiv_id': 'arXiv:2502.09183', 'title': 'RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation', 'authors': 'Changzhi Zhou, Xinyu Zhang, Dandan Song, Xiancai Chen, Wanli Gu, Huipeng Ma, Yuhang Tian, Mengdi Zhang, Linmei Hu', 'link': 'https://arxiv.org/abs/2502.09183', 'abstract': 'Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.', 'abstract_zh': '随着大型语言模型（LLMs）的兴起，代码生成引起了越来越多的关注。许多研究通过合成代码相关指令数据并应用监督微调，开发出了强大的代码LLMs。然而，这些方法受限于教师模型的蒸馏过程，并且忽视了通过自我生成代码进行迭代 refinement 的潜在价值。在本文中，我们提出了一种自适应批评精炼（ACR）方法，该方法使模型能够通过自我生成的代码和外部批评来自我精炼，而不是直接模仿教师模型的代码响应。具体而言，ACR 包含了一个由LLM担任判断者的综合评分系统，用于评估代码响应的质量，以及一个由LLM担任批评者的选择性批评策略，用于批评自我生成的低质量代码响应。我们通过迭代应用ACR开发了RefineCoder系列，并在多个代码生成基准上实现了持续的性能改进。与同规模的基线模型相比，我们提出的RefineCoder系列能够在使用较少数据的情况下达到可比拟甚至更优的性能。', 'title_zh': 'RefineCoder：通过自适应反馈细化迭代提升大型语言模型的代码生成'}
{'arxiv_id': 'arXiv:2502.09168', 'title': 'Musical Heritage Historical Entity Linking', 'authors': 'Arianna Graciotti, Nicolas Lazzari, Valentina Presutti, Rocco Tripodi', 'link': 'https://arxiv.org/abs/2502.09168', 'abstract': 'Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (MHERCL), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. MHERCL contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that MHERCL is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.', 'abstract_zh': '将文本中的命名实体与知识库（KB）中的对应实体关联起来是一项具有挑战性的任务，特别是在处理历史文本时更为困难。本文介绍了一个名为Musical Heritage Named Entities Recognition, Classification and Linking (MHERCL)的新颖基准，该基准包含从音乐领域的历史期刊中提取的手工标注句子。MHERCL 包含了许多在最著名的知识库中未被充分表示或完全缺失的命名实体。我们在 Entity Linking（实体链接）任务中尝试了多项现有的前沿模型，并展示了MHERCL 对所有这些模型都具有挑战性。我们提出了一种新的无监督实体链接模型，并提出了一种使用知识图谱（KGs）扩展监督实体链接器的方法，以解决历史文档中主要遇到的问题。我们的实验表明，依赖无监督技术，并通过知识图谱和启发式方法的逻辑约束来提升模型以预测未在参考知识库中表示的实体（NIL实体），可以在历史文档的实体链接中获得更好的性能。', 'title_zh': '音乐遗产历史实体链接'}
{'arxiv_id': 'arXiv:2502.09156', 'title': 'Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval with LLMs', 'authors': 'Chang Liu, Ying Chang, Jianmin Li, Yiqian Qu, Yu Li, Lingyong Cao, Shuyuan Lin', 'link': 'https://arxiv.org/abs/2502.09156', 'abstract': "Objectives: Large language models (LLMs) can harness medical knowledge for intelligent question answering (Q&A), promising support for auxiliary diagnosis and medical talent cultivation. However, there is a deficiency of highly efficient retrieval-augmented generation (RAG) frameworks within the domain of Traditional Chinese Medicine (TCM). Our purpose is to observe the effect of the Tree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A tasks.\nMaterials and Methods: We introduce the novel approach of knowledge organization, constructing a tree structure knowledge base with hierarchy. At inference time, our self-reflection framework retrieves from this knowledge base, integrating information across chapters. Questions from the TCM Medical Licensing Examination (MLE) and the college Classics Course Exam (CCE) were randomly selected as benchmark datasets.\nResults: By coupling with GPT-4, the framework can improve the best performance on the TCM MLE benchmark by 19.85% in absolute accuracy, and improve recall accuracy from 27% to 38% on CCE datasets. In manual evaluation, the framework improves a total of 18.52 points across dimensions of safety, consistency, explainability, compliance, and coherence.\nConclusion: The TOSRR framework can effectively improve LLM's capability in Q&A tasks of TCM.", 'abstract_zh': '研究目的：大型语言模型（LLM）可以利用医学知识进行智能问答（Q&A），为辅助诊断和医学人才的培养提供支持。然而，在中医药（TCM）领域，高效的检索增强生成（RAG）框架存在不足。本文旨在观察Tree-Organized Self-Reflective Retrieval（TOSRR）框架在TCM问答任务中的效果。\n\n材料与方法：我们引入了一种新型的知识组织方法，构建了具有层次结构的树状知识库。在推理过程中，我们的自省框架从该知识库中检索信息，整合跨章节的信息。我们选择了中医执业医师资格考试（MLE）和大学古典课程考试（CCE）中的随机问题作为基准数据集。\n\n结果：与GPT-4相结合后，该框架在TCM MLE基准测试中的绝对准确率提高了19.85%，在CCE数据集上的召回准确率从27%提高到38%。在手动评估中，该框架在安全性、一致性、可解释性、合规性和连贯性等多个维度上提高了总共18.52分。\n\n结论：TOSRR框架可以有效提高LLM在中医药问答任务中的能力。', 'title_zh': '通过基于树组织的自我反思检索提升中医问答性能'}
{'arxiv_id': 'arXiv:2502.09128', 'title': 'A Novel Dialect-Aware Framework for the Classification of Arabic Dialects and Emotions', 'authors': 'Nasser A Alsadhan', 'link': 'https://arxiv.org/abs/2502.09128', 'abstract': "Arabic is one of the oldest languages still in use today. As a result, several Arabic-speaking regions have developed dialects that are unique to them. Dialect and emotion recognition have various uses in Arabic text analysis, such as determining an online customer's origin based on their comments. Furthermore, intelligent chatbots that are aware of a user's emotions can respond appropriately to the user. Current research in emotion detection in the Arabic language lacks awareness of how emotions are exhibited in different dialects, which motivates the work found in this study. This research addresses the problems of dialect and emotion classification in Arabic. Specifically, this is achieved by building a novel framework that can identify and predict Arabic dialects and emotions from a given text. The framework consists of three modules: A text-preprocessing module, a classification module, and a clustering module with the novel capability of building new dialect-aware emotion lexicons. The proposed framework generated a new emotional lexicon for different dialects. It achieved an accuracy of 88.9% in classifying Arabic dialects, which outperforms the state-of-the-art results by 6.45 percentage points. Furthermore, the framework achieved 89.1-79% accuracy in detecting emotions in the Egyptian and Gulf dialects, respectively.", 'abstract_zh': '阿拉伯语是目前仍在使用的最古老的语言之一。因此，讲阿拉伯语的地区发展出了各自独特的方言。方言和情感识别在阿拉伯语文本分析中有多种用途，例如，可以根据客户评论确定其原籍。此外，了解用户情感的智能聊天机器人可以根据用户的情感做出适当的回应。当前对阿拉伯语情感检测的研究缺乏对不同方言中情感表达的认知，这激励了本研究中工作的开展。本研究重点解决阿拉伯语方言和情感分类的问题。具体来说，这通过构建一个新颖的框架来实现，该框架可以从给定的文本中识别和预测阿拉伯语方言和情感。该框架由三个模块组成：文本预处理模块、分类模块以及具有构建新方言意识情感词典的新功能的聚类模块。所提出的框架为不同方言生成了一个新的情感词典。它在分类阿拉伯语方言方面达到了88.9%的准确率，比现有最佳结果高出6.45个百分点。此外，框架在检测埃及方言和海湾方言情感方面分别达到了89.1%和79%的准确率。', 'title_zh': '一种新型方言感知框架，用于阿拉伯方言和情感分类'}
{'arxiv_id': 'arXiv:2502.09120', 'title': 'The influence of visual and linguistic cues on ignorance inference in Vision-Language Models (VLMs)', 'authors': 'Ye-eun Cho, Yunho Maeng', 'link': 'https://arxiv.org/abs/2502.09120', 'abstract': 'This study explored how Vision-Language Models (VLMs) process ignorance implicatures with visual and linguistic cues. Particularly, we focused on the effects of contexts (precise and approximate contexts) and modifier types (bare numerals, superlative, and comparative modifiers), which were considered pragmatic and semantic factors respectively. Methodologically, we conducted a truth-value judgment task in visually grounded settings using GPT-4o and Gemini 1.5 Pro. The results indicate that while both models exhibited sensitivity to linguistic cues (modifier), they failed to process ignorance implicatures with visual cues (context) as humans do. Specifically, the influence of context was weaker and inconsistent across models, indicating challenges in pragmatic reasoning for VLMs. On the other hand, superlative modifiers were more strongly associated with ignorance implicatures as compared to comparative modifiers, supporting the semantic view. These findings highlight the need for further advancements in VLMs to process language-vision information in a context-dependent way to achieve human-like pragmatic inference.', 'abstract_zh': '本研究探讨了视觉语言模型（VLMs）如何通过视觉和语义线索处理常识性含意。特别地，我们关注了上下文（精确上下文和模糊上下文）和修饰词类型（裸数词、超限定词和比较修饰词）的影响，前者被视为语用因素，后者被视为语义因素。从方法论上，我们使用GPT-4o和Gemini 1.5 Pro在视觉接地的场景中进行了真假值判断任务。研究结果显示，虽然两种模型对语义线索（修饰词）表现出敏感性，但它们在处理带有视觉线索（上下文）的常识性含意方面未能像人类那样进行处理。具体来说，不同模型在上下文影响方面的表现较弱且不一致，这表明VLM在语用推理方面存在挑战。另一方面，超限定修饰词与常识性含意的关系比比较修饰词更为紧密，支持了语义学的观点。这些发现强调了进一步改进VLM以在依赖上下文的方式处理语言和视觉信息，以实现类似人类的语用推理的重要性。', 'title_zh': '视觉和语言线索对视觉语言模型（VLMs）中无知推理的影响'}
{'arxiv_id': 'arXiv:2502.09097', 'title': 'A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian Optimization and Bidirectional Recurrent Unit', 'authors': 'Tianyi Huang, Zeqiu Xu, Peiyang Yu, Jingyuan Yi, Xiaochuan Xu', 'link': 'https://arxiv.org/abs/2502.09097', 'abstract': 'In this paper, we propose an optimized Transformer model that integrates Bayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and apply it to fake news classification for the first time. First, we employ the TF-IDF method to extract features from news texts and transform them into numeric representations to facilitate subsequent machine learning tasks. Two sets of experiments are then conducted for fake news detection and classification: one using a Transformer model optimized only with BiGRU, and the other incorporating Bayesian algorithms into the BiGRU-based Transformer. Experimental results show that the BiGRU-optimized Transformer achieves 100% accuracy on the training set and 99.67% on the test set, while the addition of the Bayesian algorithm maintains 100% accuracy on the training set and slightly improves test-set accuracy to 99.73%. This indicates that the Bayesian algorithm boosts model accuracy by 0.06%, further enhancing the detection capability for fake news. Moreover, the proposed algorithm converges rapidly at around the 10th training epoch with accuracy nearing 100%, demonstrating both its effectiveness and its fast classification ability. Overall, the optimized Transformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits excellent continuous learning and detection performance, offering a robust technical means to combat the spread of fake news in the current era of information overload.', 'abstract_zh': '在本文中，我们提出了一种优化的Transformer模型，该模型结合了Bayesian算法和双向门控循环单元（Bidirectional Gated Recurrent Unit, BiGRU），并首次将其应用于虚假新闻分类。首先，我们采用了TF-IDF方法从新闻文本中提取特征，并将其转化为数值表示，以便于后续的机器学习任务。然后，我们进行了两组实验来检测和分类虚假新闻：一组仅使用BiGRU优化的Transformer模型，另一组则是基于BiGRU的Transformer模型结合了Bayesian算法。实验结果显示，BiGRU优化的Transformer模型在训练集上的准确率为100%，在测试集上的准确率为99.67%；而加入Bayesian算法后，模型在训练集上的准确率仍为100%，并在测试集上的准确率提升至99.73%。这表明Bayesian算法提升了模型的准确率0.06%，进一步增强了虚假新闻的检测能力。此外，所提出的算法在大约第10个训练周期即可快速收敛，准确率接近100%，展示了其有效性和快速分类能力。总体而言，通过Bayesian算法和BiGRU增强的优化Transformer模型表现出卓越的持续学习和检测性能，为当前信息过载时代对抗虚假新闻提供了 robust 的技术手段。', 'title_zh': '一种结合变换器模型的假新闻检测方法：利用贝叶斯优化和双向循环单元'}
{'arxiv_id': 'arXiv:2502.09086', 'title': 'A Hybrid Model for Few-Shot Text Classification Using Transfer and Meta-Learning', 'authors': 'Jia Gao, Shuangquan Lyu, Guiran Liu, Binrong Zhu, Hongye Zheng, Xiaoxuan Liao', 'link': 'https://arxiv.org/abs/2502.09086', 'abstract': "With the continuous development of natural language processing (NLP) technology, text classification tasks have been widely used in multiple application fields. However, obtaining labeled data is often expensive and difficult, especially in few-shot learning scenarios. To solve this problem, this paper proposes a few-shot text classification model based on transfer learning and meta-learning. The model uses the knowledge of the pre-trained model for transfer and optimizes the model's rapid adaptability in few-sample tasks through a meta-learning mechanism. Through a series of comparative experiments and ablation experiments, we verified the effectiveness of the proposed method. The experimental results show that under the conditions of few samples and medium samples, the model based on transfer learning and meta-learning significantly outperforms traditional machine learning and deep learning methods. In addition, ablation experiments further analyzed the contribution of each component to the model performance and confirmed the key role of transfer learning and meta-learning in improving model accuracy. Finally, this paper discusses future research directions and looks forward to the potential of this method in practical applications.", 'abstract_zh': '随着自然语言处理（NLP）技术的持续发展，文本分类任务已经在多个应用领域得到广泛应用。然而，在少量样本学习场景中获得带有标签的数据通常既昂贵又困难。为了解决这一问题，本文提出了一种基于迁移学习和元学习的少量样本文本分类模型。该模型利用预训练模型的知识进行迁移，并通过元学习机制优化模型在少量样本任务中的快速适应能力。通过一系列对比实验和消融实验，我们验证了所提出方法的有效性。实验结果表明，在少量样本和中等样本条件下，基于迁移学习和元学习的模型显著优于传统机器学习和深度学习方法。此外，消融实验进一步分析了每一部分对模型性能的贡献，并确认了迁移学习和元学习在提高模型准确度中的关键作用。最后，本文讨论了未来的研究方向，并展望了该方法在实际应用中的潜在价值。', 'title_zh': '使用转移学习和元学习的Few-Shot文本分类混合模型'}
{'arxiv_id': 'arXiv:2502.09082', 'title': 'CoSER: Coordinating LLM-Based Persona Simulation of Established Roles', 'authors': 'Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Wei Wang, Yanghua Xiao, Shuchang Zhou', 'link': 'https://arxiv.org/abs/2502.09082', 'abstract': 'Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.', 'abstract_zh': '角色扮演语言代理（RPLA）已成为大型语言模型（LLM）的一项有前景的应用。然而，模拟已建立的人物资料对RPLA来说是一个具有挑战性的任务，原因在于缺乏真实的人物数据集以及评估这些数据的方法。在本文中，我们提出了CoSER，它包括一个高质量的数据集、开放模型以及一个评价协议，以促进有效的人物角色扮演语言代理（RPLA）。CoSER数据集包含了771部著名书籍中17,966个角色。它提供了具备现实复杂性的对话，以及对话设置、人物经历和内心想法等多样化的数据类型。借鉴表演学方法，我们引入了给定情境表演法，用于训练和评估角色扮演的LLM，其中LLM会依次在书中场景中扮演多个角色。使用我们的数据集，我们开发了CoSER 8B和CoSER 70B，即基于LLaMA-3.1模型的高级开放角色扮演LLM。广泛的实验表明，CoSER数据集在RPLA的训练、评价和检索方面具有重要价值。此外，CoSER 70B的表现达到了业内领先水平，超过了或与GPT-4o在我们的评价和三个现有基准测试中的表现相同，具体而言，在InCharacter和LifeChoice基准测试中的准确率分别为75.80%和93.47%。', 'title_zh': 'CoSER：协调基于大型语言模型的固定角色个性模拟'}
{'arxiv_id': 'arXiv:2502.09073', 'title': 'Enhancing RAG with Active Learning on Conversation Records: Reject Incapables and Answer Capables', 'authors': 'Xuzhao Geng, Haozhao Wang, Jun Wang, Wei Liu, Ruixuan Li', 'link': 'https://arxiv.org/abs/2502.09073', 'abstract': 'Retrieval-augmented generation (RAG) is a key technique for leveraging external knowledge and reducing hallucinations in large language models (LLMs). However, RAG still struggles to fully prevent hallucinated responses. To address this, it is essential to identify samples prone to hallucination or guide LLMs toward correct responses, which experts then annotate to develop high-quality datasets for refining LLMs. However, the growing scarcity of such datasets makes their creation challenging. This paper proposes using the vast amount of conversations from widespread LLM usage to build these datasets, training LLMs to avoid hallucination-prone questions while accurately responding to manageable ones. Given the impracticality of expert-annotating all conversation records, the paper introduces AL4RAG, which uses active learning to select the most suitable conversation samples for annotation, optimizing performance within an annotation budget. Additionally, recognizing that traditional active learning methods are not fully compatible with RAG due to unsuitable distance metrics, we develop a novel sample distance measurement for RAG active learning. Extensive experiments show that our method consistently outperforms baselines across multiple metrics.', 'abstract_zh': '检索增强生成（RAG）是一种利用外部知识并减少大型语言模型（LLMs）幻觉的关键技术。然而，RAG 仍然难以完全防止生成幻觉的响应。为解决这一问题，重要的是要识别出易产生幻觉的样本，或者引导 LLMs 提供正确的响应，然后由专家对其进行标注以开发高质量的数据集进一步优化 LLMs。然而，这类数据集正在变得越来越稀缺，使得它们的创建变得具有挑战性。本文提出利用广泛使用的 LLM 会话交流的庞大数量来构建这些数据集，训练 LLMs 避免提出幻觉倾向的问题，同时准确回答可管理的问题。鉴于专家标注所有会话记录是不切实际的，本文介绍了 AL4RAG，该方法使用主动学习来选择最适合标注的交流样本，优化在标注预算内的性能。此外，我们认识到传统的主动学习方法不完全适用于 RAG，因为它们不适合的度量标准不适当，因此开发了一种新的样本距离测量方法来适应 RAG 的主动学习。广泛实验表明，相比于基线方法，我们的方法在多个评价指标上表现更优异。', 'title_zh': '利用对话记录上的主动学习增强RAAG：拒绝不合格的回答者，保留合格的回答者'}
{'arxiv_id': 'arXiv:2502.09056', 'title': 'An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging', 'authors': 'Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai', 'link': 'https://arxiv.org/abs/2502.09056', 'abstract': 'This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks.', 'abstract_zh': '本文旨在研究数据选择和模型融合方法，将如DeepSeek R1等先进的推理能力融入特定语言的大语言模型（LLMs）中，特别关注泰语LLMs。我们的目标是在保持其目标语言能力的同时，增强特定语言LLMs的推理能力。DeepSeek R1在推理方面表现出色，主要受益于英语和汉语等高资源语言。然而，由于英语导向的训练数据和模型优化的主导地位，低资源语言的性能仍受到限制，这导致这些语言中的代码转换不可靠，并且在低资源语言的任务上效果减弱。与此同时，本地和区域性的LLMs倡议正在努力缩小这一差距，通过开发侧重于提升本地语言保真度的特定语言LLMs来实现这一目标。我们证明，仅使用公开可用的数据集和120美元的计算预算，就可以在不牺牲其在目标语言任务上的性能的情况下，增强特定语言LLMs的推理能力至与DeepSeek R1同类的水平。', 'title_zh': '公开食谱：通过模型融合将特定语言的大型语言模型在一天内适应为一个推理模型'}
{'arxiv_id': 'arXiv:2502.09042', 'title': 'Typhoon T1: An Open Thai Reasoning Model', 'authors': 'Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul', 'link': 'https://arxiv.org/abs/2502.09042', 'abstract': 'This paper introduces Typhoon T1, an open effort to develop an open Thai reasoning model. A reasoning model is a relatively new type of generative model built on top of large language models (LLMs). A reasoning model generates a long chain of thought before arriving at a final answer, an approach found to improve performance on complex tasks. However, details on developing such a model are limited, especially for reasoning models that can generate traces in a low-resource language. Typhoon T1 presents an open effort that dives into the details of developing a reasoning model in a more cost-effective way by leveraging supervised fine-tuning using open datasets, instead of reinforcement learning. This paper shares the details about synthetic data generation and training, as well as our dataset and model weights. Additionally, we provide insights gained from developing a reasoning model that generalizes across domains and is capable of generating reasoning traces in a low-resource language, using Thai as an example. We hope this open effort provides a foundation for further research in this field.', 'abstract_zh': '本文介绍了Typhoon T1，这是一个开放项目，旨在开发一种开放的泰语推理模型。推理模型是一种基于大型语言模型（LLMs）的新类型生成模型。这种模型在生成最终答案之前会生成一条较长的思维链，这种方法已被证明能提高复杂任务的表现。然而，关于这种模型开发的详细信息并不多，尤其是在资源有限的语言中生成推理轨迹的推理模型更是如此。Typhoon T1 提供了一种成本效益更高的开发推理模型的开放方法，通过利用开放数据集进行监督微调，而不是使用强化学习。本文分享了合成数据生成和训练的详细信息，以及我们的数据集和模型权重。此外，我们还介绍了在开发一种能够在资源有限的语言中生成推理轨迹并且能在不同领域泛化的推理模型方面的经验教训，以泰语为例。我们希望这一开放项目能为该领域的进一步研究提供基础。', 'title_zh': 'typhoon T1：一种开放的泰语推理模型'}
{'arxiv_id': 'arXiv:2502.09017', 'title': "Diversity Enhances an LLM's Performance in RAG and Long-context Task", 'authors': 'Zhchao Wang, Bin Bi, Yanqi Luo, Sitaram Asur, Claire Na Cheng', 'link': 'https://arxiv.org/abs/2502.09017', 'abstract': 'The rapid advancements in large language models (LLMs) have highlighted the challenge of context window limitations, primarily due to the quadratic time complexity of the self-attention mechanism (\\(O(N^2)\\), where \\(N\\) denotes the context window length). This constraint impacts tasks such as retrieval-augmented generation (RAG) in question answering (Q\\&A) and long context summarization. A common approach involves selecting content with the highest similarity to the query; however, this often leads to redundancy and the exclusion of diverse yet relevant information. Building on principles from Maximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we integrate diversity into the content selection process. Our findings reveal that incorporating diversity substantially increases the recall of selecting relevant sentences or chunks before LLM-based Q\\&A and summarization. These results highlight the importance of maintaining diversity in future LLM applications to further improve summarization and Q\\&A outcomes.', 'abstract_zh': '大型语言模型（LLMs）的迅速发展凸显了上下文窗口限制所面临的挑战，主要是由于自我注意机制的时间复杂度为二次复杂度（\\(O(N^2)\\)，其中\\(N\\)表示上下文窗口的长度）。这一限制影响了诸如问题回答（Q&A）中的检索增强生成（RAG）和长文本摘要等任务。一种常见的方法是选择与查询最相似的内容；然而，这种方法往往会导致冗余，并排除那些虽然相关但多样性的信息。基于最大化边际相关性（MMR）和最远点采样（FPS）的原则，我们在内容选择过程中引入了多样性。我们的研究发现，引入多样性显著提高了在LLM驱动的问题回答和摘要任务中选择相关句子或段落的召回率。这些结果强调了在未来LLM应用中维持多样性的必要性，以进一步提高摘要和问题回答的效果。', 'title_zh': '多样性可以提升大语言模型在检索增强和长上下文任务中的性能'}
{'arxiv_id': 'arXiv:2502.09004', 'title': 'Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content in Mainstream US News Media through the Lens of Hope Speech', 'authors': 'Jonathan Pofcher, Christopher M. Homan, Randall Sell, Ashiqur R. KhudaBukhsh', 'link': 'https://arxiv.org/abs/2502.09004', 'abstract': 'This paper makes three contributions. First, via a substantial corpus of 1,419,047 comments posted on 3,161 YouTube news videos of major US cable news outlets, we analyze how users engage with LGBTQ+ news content. Our analyses focus both on positive and negative content. In particular, we construct a fine-grained hope speech classifier that detects positive (hope speech), negative, neutral, and irrelevant content. Second, in consultation with a public health expert specializing on LGBTQ+ health, we conduct an annotation study with a balanced and diverse political representation and release a dataset of 3,750 instances with fine-grained labels and detailed annotator demographic information. Finally, beyond providing a vital resource for the LGBTQ+ community, our annotation study and subsequent in-the-wild assessments reveal (1) strong association between rater political beliefs and how they rate content relevant to a marginalized community; (2) models trained on individual political beliefs exhibit considerable in-the-wild disagreement; and (3) zero-shot large language models (LLMs) align more with liberal raters.', 'abstract_zh': '本文做出了三项贡献。首先，本文通过分析来自美国主要有线新闻频道的3,161个YouTube新闻视频中1,419,047条评论，探讨用户如何互动参与LGBTQ+新闻内容。我们的分析不仅关注正面的内容，也关注负面的内容。特别地，我们构建了一个细粒度的希望建言分类器，可以检测正面（希望建言）、负面、中立和不相关的内容。其次，我们与专门研究LGBTQ+健康的公共卫生专家合作，进行了一项标注研究，该研究具有均衡的政治代表性并且涵盖多样化的政治观点，并发布了包含细粒度标签和详细标注员人口统计信息的3,750个实例的数据库。最后，除了为LGBTQ+社群提供宝贵的资源外，我们的标注研究及其后续的野外评估揭示了以下几点：（1）评级者的政治信念与其对涉及边缘化社群的内容评级之间存在强烈的关联；（2）基于个体政治信念训练的模型在野外表现存在显著分歧；（3）零样本大语言模型（LLMs）更倾向于与自由派的评级者一致。', 'title_zh': '希望与仇恨：通过希望言论的视角理解主流美国新闻媒体中用户与LGBTQ+新闻内容的互动'}
{'arxiv_id': 'arXiv:2502.08972', 'title': 'Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning', 'authors': 'Hyundong Cho, Karishma Sharma, Nicolaas Jedema, Leonardo F. R. Ribeiro, Alessandro Moschitti, Ravi Krishnan, Jonathan May', 'link': 'https://arxiv.org/abs/2502.08972', 'abstract': "Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users' styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user's style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5% against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, TICL presents a novel yet simple approach for personalized alignment.", 'abstract_zh': '语言模型被调整为众人的声音，导致生成的输出缺乏对特定用户风格的个性化。本研究提出了一种名为Trial-Error-Explain In-Context Learning (TICL)的免调优方法，该方法仅使用每个用户少于10个示例就能为文本生成任务个性化语言模型。TICL通过试错解释过程迭代扩展上下文学习提示，添加由模型生成的负样本和解释，以提供针对特定用户风格的详细指导。在与LLM作为评判者的配对比较中，TICL的胜率达到了91.5%，并优于先前的免调优基准，在撰写电子邮件、论文和新闻文章的个性化对齐任务中表现出色。词法和定性分析表明，负样本和解释使语言模型更有效地学习风格化上下文，并克服了零样本输出中对结构化和正式短语的偏差。通过在推理计算中前端加载来创建一个特定于用户的上下文学习提示，而无须在测试时额外的生成步骤，TICL提供了一种新颖且简单的方法来实现个性化对齐。', 'title_zh': '无调优个性化对齐通过试错解释上下文学习'}
{'arxiv_id': 'arXiv:2502.08954', 'title': 'Medicine on the Edge: Comparative Performance Analysis of On-Device LLMs for Clinical Reasoning', 'authors': 'Leon Nissen, Philipp Zagar, Vishnu Ravi, Aydin Zahedivash, Lara Marie Reimer, Stephan Jonas, Oliver Aalami, Paul Schmiedmayer', 'link': 'https://arxiv.org/abs/2502.08954', 'abstract': 'The deployment of Large Language Models (LLM) on mobile devices offers significant potential for medical applications, enhancing privacy, security, and cost-efficiency by eliminating reliance on cloud-based services and keeping sensitive health data local. However, the performance and accuracy of on-device LLMs in real-world medical contexts remain underexplored. In this study, we benchmark publicly available on-device LLMs using the AMEGA dataset, evaluating accuracy, computational efficiency, and thermal limitation across various mobile devices. Our results indicate that compact general-purpose models like Phi-3 Mini achieve a strong balance between speed and accuracy, while medically fine-tuned models such as Med42 and Aloe attain the highest accuracy. Notably, deploying LLMs on older devices remains feasible, with memory constraints posing a greater challenge than raw processing power. Our study underscores the potential of on-device LLMs for healthcare while emphasizing the need for more efficient inference and models tailored to real-world clinical reasoning.', 'abstract_zh': '将以下论文内容或标题翻译成中文，并符合学术规范：\n\n在移动设备上部署大规模语言模型（LLM）为医疗应用提供了巨大潜力，通过消除对云服务的依赖，保护敏感健康数据的隐私、安全并提高成本效益。然而，实世界医疗情境下设备端LLM的性能和准确性尚未得到充分探索。本研究使用AMEGA数据集对公开可用的设备端LLM进行基准测试，评估其在各种移动设备上的准确性、计算效率和热限制。结果显示，紧凑型通用模型如Phi-3 Mini在速度和准确性之间实现了良好的平衡，而经过医学微调的模型如Med42和Aloe则实现了最高的准确性。值得注意的是，部署LLM在较旧的设备上仍然可行，内存约束是更大的挑战，而不是原始计算能力。本研究强调了设备端LLM在医疗保健领域中的潜力，同时也突出了需要更高效的推理算法和更适合实际临床推理的模型的必要性。', 'title_zh': '边缘医疗：边缘设备上大型语言模型在临床推理中的性能比较分析'}
{'arxiv_id': 'arXiv:2502.08947', 'title': 'Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding', 'authors': 'Fenella Harcourt, Naderdel Piero, Gilbert Sutherland, Daphne Holloway, Harriet Bracknell, Julian Ormsby', 'link': 'https://arxiv.org/abs/2502.08947', 'abstract': 'Token representations in high-dimensional latent spaces often exhibit redundancy, limiting computational efficiency and reducing structural coherence across model layers. Hierarchical latent space folding introduces a structured transformation mechanism that enforces a multi-scale organization within learned embeddings, refining representational compactness while preserving essential contextual distinctions. The proposed approach incorporates dynamic folding operations that iteratively adjust token embeddings through structured transformations, influencing both short-range and long-range dependencies in sequential processing tasks. Empirical evaluation demonstrates a reduction in representational variance across layers, contributing to more stable perplexity distributions and enhancing predictive confidence in text generation. The structured redistribution of attention head utilization leads to more efficient allocation of computational resources, particularly in deeper layers, where hierarchical refinements improve contextual abstraction. Comparative analysis of activation sparsity patterns suggests that hierarchical adjustments selectively reinforce critical pathways while reducing computational overhead in non-essential regions of the model. Statistical assessments of token reordering frequencies reveal that hierarchical modifications introduce subtle shifts in sequential dependencies, improving contextual alignment while maintaining syntactic correctness. Computational trade-offs associated with hierarchical folding introduce marginal increases in training time per epoch, yet empirical findings indicate that inference efficiency benefits from the structured representation adjustments. The results highlight the impact of hierarchical latent space folding on optimizing model performance through improved representation structuring and computational efficiency.', 'abstract_zh': '高维潜在空间中的token表示往往表现出冗余性，这限制了计算效率，并降低了模型各层之间的结构一致性。层级潜在空间折叠引入了一种结构化的转换机制，确保学习嵌入在多尺度组织层面中得到优化，同时精确化表示的紧凑性，同时保持关键的语境区分性。所提出的方法包含动态折叠操作，这些操作通过结构化转换逐次调整token嵌入，影响序列处理任务中的短程和远程依赖关系。实证评估表明，各层间的表示偏差有所减少，从而贡献了更加稳定的困惑度分布，并增强了文本生成的预测信心。通过结构化的注意力头利用再分配，更有效地分配计算资源，特别是在深层层数较多时，层级细化提高了语境抽象层次。激活稀疏模式的比较分析表明，层级调整选择性地强化关键路径，同时在模型的非关键区域减少计算开销。通过对token重新排序频率的统计评估发现，层级修改引入了顺序依赖关系的微小变化，改善了语境对齐，同时保持了句法正确性。与层级折叠相关的计算开销导致每次训练周期所需的训练时间略有增加，但实证结果表明，通过结构化表示调整，推理效率得到了提升。结果突出了层级潜在空间折叠对通过改进表示结构化和计算效率优化模型性能的影响。', 'title_zh': '大型语言模型表示中的分层潜在空间折叠结构收敛'}
{'arxiv_id': 'arXiv:2502.08946', 'title': "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding", 'authors': 'Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, Dit-Yan Yeung, Jie Zhou', 'link': 'https://arxiv.org/abs/2502.08946', 'abstract': 'In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PhysiCo. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates: (1) state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance.', 'abstract_zh': '以系统的方式，我们探讨了一个广泛被关注的问题：大型语言模型（LLMs）究竟是否真正理解它们所说的内容？这一问题与人们更为熟悉的“随机鹦鹉”（Stochastic Parrot）概念相关。为此，我们提出了一项综合评估，该评估基于一个精心设计的物理概念理解任务——PhysiCo。我们的任务通过使用格状格式的输入来规避记忆问题，这些输入抽象地描述了物理现象，并展示了从核心现象到应用示例再到幻方世界中的其他抽象模式的不同理解层次。对我们的任务进行的全面研究显示：（1）最新的先进LLMs，包括GPT-4o、o1和Gemini 2.0，在这一任务上的表现落后于人类约40%；（2）“随机鹦鹉”现象在LLMs中存在，因为它们在我们的格状任务中表现不佳，但在自然语言中却能很好地描述和识别相同的概念；（3）我们的任务因其实质性的难点而非不熟悉的格状格式对LLMs构成了挑战，因为在上下文学习和针对同一格式的数据进行微调并未显著提升它们的表现。', 'title_zh': 'LLM 旁的随机鹦鹉：物理概念理解的综合评估'}
{'arxiv_id': 'arXiv:2502.08943', 'title': 'Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis', 'authors': 'Wenbo Zhang, Hengrui Cai, Wenyu Chen', 'link': 'https://arxiv.org/abs/2502.08943', 'abstract': 'Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantic prompts, enabling error detection and quality control in benchmark construction.', 'abstract_zh': '大型语言模型（LLMs）在实际应用中展示了显著的实用价值，具备强大的自然语言处理和理解能力。基准评估对于检验LLMs的能力至关重要，因为它可以提供对其优点和不足的全面评估。然而，当前的评估方法常常通过确定性生成策略或依赖单一随机样本，忽略了LLMs固有的随机性，导致未能考虑采样变异性和不稳定的基准评分估计。在本文中，我们提出了一种分层统计模型，通过结合基准特性与LLMs的随机性，提供了更全面的基准评估过程的表示。我们证明，利用多个生成可以提高基准评分估计的准确性并降低变异。此外，我们引入了基于正解比例的提示级难度得分$\\mathbb{P}(\\text{correct})$，为个体提示提供了细粒度的洞察。同时，我们创建了一个数据地图，可视化了难度和语义提示，有助于基准构建过程中的错误检测和质量控制。', 'title_zh': '超越单一标准：多代参与在有效基准评估与分析中的核心作用'}
{'arxiv_id': 'arXiv:2502.08923', 'title': 'CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality', 'authors': 'Razvan-Gabriel Dumitru, Minglai Yang, Vikas Yadav, Mihai Surdeanu', 'link': 'https://arxiv.org/abs/2502.08923', 'abstract': "We introduce CopySpec, an innovative technique designed to tackle the inefficiencies LLMs face when generating responses that closely resemble previous outputs. CopySpec identifies repeated sequences in the model's chat history and speculates that the same tokens will follow, enabling seamless copying without compromising output quality or requiring additional GPU memory. To evaluate the effectiveness of our approach, we conducted experiments using five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper, transforms the second turn of MT-Bench into a request for variations of the first turn's answer, simulating real-world scenarios where users request modifications to prior responses. Our results demonstrate significant speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select MT-Redundant categories, and 2.66x on the third turn of GSM-8K's self-correction tasks. Moreover, we show that CopySpec integrates seamlessly with speculative decoding, yielding an average 49% additional speed-up over speculative decoding for the second turn of MT-Redundant across all eight categories. While LLMs, even with speculative decoding, suffer from slower inference as context sizes grow, CopySpec leverages the expanded context to accelerate inference, making it faster as the context size increases. Our code and dataset are publicly available at this https URL.", 'abstract_zh': '我们将介绍 CopySpec，这是一种创新技术，旨在解决大语言模型在生成与先前输出高度相似的响应时面临的低效问题。CopySpec 通过识别模型聊天历史中的重复序列，并推测后续将出现相同的标记，从而实现无缝复制，同时不牺牲输出质量或增加额外的 GPU 内存。为了评估该方法的有效性，我们在五种大语言模型和五种数据集中进行了实验：MT-Bench、CNN/DM、GSM-8K、HumanEval 以及我们在本文中新创建的数据集 MT-Redundant。MT-Redundant 在本论文中引入，将 MT-Bench 的第二轮对话转换为请求第一轮回答的变体，模拟了用户请求修改先前响应的真实场景。实验结果表明，CopySpec 可实现显著的加速：在 CNN/DM 上达到 2.35 倍，在特定的 MT-Redundant 分类的第二轮对话上达到 3.08 倍，在 GSM-8K 自纠错任务的第三轮对话上达到 2.66 倍。此外，我们还展示了 CopySpec 如何无缝集成到推测解码中，使得在整个八个分类的 MT-Redundant 的第二轮对话上，推测解码的平均加速比提高了 49%。尽管即使带有推测解码，随着上下文大小的增长，大语言模型的推理速度也会变慢，但 CopySpec 通过扩展的上下文加速推理，随着上下文大小的增大，使得推理变得更加快速。我们的代码和数据集已在以下网址公开：[提供网址]。', 'title_zh': 'CopySpec：在不牺牲质量的前提下加速大型语言模型的规格化复制粘贴方法'}
{'arxiv_id': 'arXiv:2502.08910', 'title': 'InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU', 'authors': 'Heejun Lee, Geon Park, Jaduk Suh, Sung Ju Hwang', 'link': 'https://arxiv.org/abs/2502.08910', 'abstract': 'In modern large language models (LLMs), handling very long context lengths presents significant challenges as it causes slower inference speeds and increased memory costs. Additionally, most existing pre-trained LLMs fail to generalize beyond their original training sequence lengths. To enable efficient and practical long-context utilization, we introduce InfiniteHiP, a novel, and practical LLM inference framework that accelerates processing by dynamically eliminating irrelevant context tokens through a modular hierarchical token pruning algorithm. Our method also allows generalization to longer sequences by selectively applying various RoPE adjustment methods according to the internal attention patterns within LLMs. Furthermore, we offload the key-value cache to host memory during inference, significantly reducing GPU memory pressure. As a result, InfiniteHiP enables the processing of up to 3 million tokens on a single L40s 48GB GPU -- 3x larger -- without any permanent loss of context information. Our framework achieves an 18.95x speedup in attention decoding for a 1 million token context without requiring additional training. We implement our method in the SGLang framework and demonstrate its effectiveness and practicality through extensive evaluations.', 'abstract_zh': '在现代大型语言模型（LLMs）中，处理非常长的上下文长度带来了显著的挑战，因为它会导致推理速度变慢并增加内存成本。此外，大多数现有的预训练LLMs无法在其原始训练序列长度之外泛化。为了实现高效且实用的长上下文利用，我们引入了InfiniteHiP，这是一种新颖且实用的LLM推理框架，通过动态消除无关的上下文标记来加速处理，使用了一个模块化的分层标记剪枝算法。我们的方法还通过根据LLM内部注意力模式选择性地应用各种RoPE调整方法，实现了更长序列的泛化。此外，在推理过程中，我们将键值缓存卸载到主机内存中，大幅减少了GPU内存压力。因此，InfiniteHiP使得使用单个L40s 48GB GPU处理多达300万标记成为可能，相当于扩大了三倍——而无任何永久性的上下文信息损失。我们的框架在100万标记的上下文情况下实现了18.95倍的注意力解码加速，而无需额外的训练。我们已在SGLang框架中实现了此方法，并通过广泛的评估充分展示了其实效性和实用性。', 'title_zh': 'InfiniteHiP：将语言模型上下文扩展至单个GPU上的300万词 Escorts'}
{'arxiv_id': 'arXiv:2502.08909', 'title': 'Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs', 'authors': 'Premtim Sahitaj, Iffat Maab, Junichi Yamagishi, Jawan Kolanowski, Sebastian Möller, Vera Schmitt', 'link': 'https://arxiv.org/abs/2502.08909', 'abstract': 'Fact-checking is necessary to address the increasing volume of misinformation. Traditional fact-checking relies on manual analysis to verify claims, but it is slow and resource-intensive. This study establishes baseline comparisons for Automated Fact-Checking (AFC) using Large Language Models (LLMs) across multiple labeling schemes (binary, three-class, five-class) and extends traditional claim verification by incorporating analysis, verdict classification, and explanation in a structured setup to provide comprehensive justifications for real-world claims. We evaluate Llama-3 models of varying sizes (3B, 8B, 70B) on 17,856 claims collected from PolitiFact (2007-2024) using evidence retrieved via restricted web searches. We utilize TIGERScore as a reference-free evaluation metric to score the justifications. Our results show that larger LLMs consistently outperform smaller LLMs in classification accuracy and justification quality without fine-tuning. We find that smaller LLMs in a one-shot scenario provide comparable task performance to fine-tuned Small Language Models (SLMs) with large context sizes, while larger LLMs consistently surpass them. Evidence integration improves performance across all models, with larger LLMs benefiting most. Distinguishing between nuanced labels remains challenging, emphasizing the need for further exploration of labeling schemes and alignment with evidences. Our findings demonstrate the potential of retrieval-augmented AFC with LLMs.', 'abstract_zh': '事实核查对于应对不断增长的虚假信息量至关重要。传统的事实核查依赖于人工分析来验证声明，但这种方法速度慢且耗资源。本研究表明，使用大规模语言模型（LLMs）跨多种标记方案（二分类、三分类、五分类）进行自动化事实核查（AFC）的基础比较，并通过将分析、裁决分类和解释嵌入到结构化的设置中来扩展传统的声明验证，从而为实际世界的声明提供全面的佐证。我们评估了不同规模的Llama-3模型（3B、8B、70B），使用来自FactCheck.org（2007-2024年）的17,856个声明，并结合受限网络搜索获取的证据进行评估。我们采用TIGERScore作为无参考评估指标来评分佐证。结果表明，在不需要微调的情况下，较大的LLMs在分类准确性和佐证质量上始终优于较小的LLMs。我们发现，在一次性情景中，较小的LLMs的性能与通过大规模上下文进行微调的小规模语言模型（SLMs）相当，而较大的LLMs则始终超出它们。证据整合提高了所有模型的性能，尤其是对较大的LLMs更为明显。在区分细微标签方面仍然存在挑战，强调了进一步探索标记方案并与证据对齐的必要性。我们的研究结果表明，以LLMs为支持的检索增强自动化事实核查具有潜在的应用价值。', 'title_zh': '面向现实世界声明的自动化事实核查：探索基于大语言模型的任务表述与评估'}
{'arxiv_id': 'arXiv:2502.08900', 'title': 'Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?', 'authors': 'Shira Wein', 'link': 'https://arxiv.org/abs/2502.08900', 'abstract': 'While ChatGPT and GPT-based models are able to effectively perform many tasks without additional fine-tuning, they struggle with related to extremely low-resource languages and indigenous languages. Uniform Meaning Representation (UMR), a semantic representation designed to capture the meaning of texts in many languages, is well-poised to be leveraged in the development of low-resource language technologies. In this work, we explore the downstream technical utility of UMR for low-resource languages by incorporating it into GPT-4 prompts. Specifically, we examine the ability of GPT-4 to perform translation from three indigenous languages (Navajo, Arápaho, and Kukama), with and without demonstrations, as well as with and without UMR annotations. Ultimately we find that in the majority of our test cases, integrating UMR into the prompt results in a statistically significant increase in performance, which is a promising indication of future applications of the UMR formalism.', 'abstract_zh': '尽管ChatGPT和基于GPT的模型在无需额外微调的情况下能够有效执行许多任务，但它们在处理极度低资源语言和土著语言时表现出困难。Uniform Meaning Representation（统一语义表示，UMR）是一种设计用于捕获多种语言文本意义的语义表示方法，非常适合在低资源语言技术的发展中加以利用。在本项研究中，我们通过将UMR整合到GPT-4的提示中，探索其在低资源语言中的下游技术应用。具体而言，我们研究了GPT-4在三项土著语言（纳瓦霍语、阿罗帕霍语和库卡马语）翻译任务中的能力，包括有和没有演示以及有和没有UMR注解的情况。最终，我们发现，在我们的大部分测试案例中，将UMR整合到提示中会导致性能的统计显著提高，这为未来UMR形式主义的应用提供了令人鼓舞的迹象。', 'title_zh': 'fairness\n公平性\n\n准确性和可读性是翻译任务中两个重要的方面。下面是对给定标题的中文翻译，符合学术规范：\n\n"统一意义表示能否帮助GPT-4翻译土著语言？"\n\n这个翻译简洁明了，直接传达了原文的意思。如果有特定的学术术语或领域规范需要遵循，请告知以便进一步调整。'}
{'arxiv_id': 'arXiv:2502.08896', 'title': 'Communication is All You Need: Persuasion Dataset Construction via Multi-LLM Communication', 'authors': 'Weicheng Ma, Hefan Zhang, Ivory Yang, Shiyu Ji, Joice Chen, Farnoosh Hashemi, Shubham Mohole, Ethan Gearey, Michael Macy, Saeed Hassanpour, Soroush Vosoughi', 'link': 'https://arxiv.org/abs/2502.08896', 'abstract': "Large Language Models (LLMs) have shown proficiency in generating persuasive dialogue, yet concerns about the fluency and sophistication of their outputs persist. This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically. This framework facilitates the efficient production of high-quality, diverse linguistic content with minimal human oversight. Through extensive evaluations, we demonstrate that the generated data excels in naturalness, linguistic diversity, and the strategic use of persuasion, even in complex scenarios involving social taboos. The framework also proves adept at generalizing across novel contexts. Our results highlight the framework's potential to significantly advance research in both computational and social science domains concerning persuasive communication.", 'abstract_zh': '大型语言模型（LLMs）在生成有说服力的对话方面展现了较高的能力，但对其输出的流畅性和 sophistication 仍有担忧。本文提出了一个多大规模语言模型通信框架，旨在提高自动生成有说服力数据的效果。该框架能够高效地生成高质量、多样化的语言内容，同时减少人工监督。通过广泛评估，我们证明生成的数据在自然度、语言多样性以及有策略地使用说服技巧方面表现出色，甚至在涉及社会禁忌的复杂场景中也不例外。该框架还证明了其在新颖情境下的泛化能力。我们的研究结果突显了该框架在计算科学和社会科学领域中关于说服性沟通研究方面的巨大潜力。', 'title_zh': 'Communication 是你需要的一切：通过多轮多模态语言模型对话构建说服力数据集'}
{'arxiv_id': 'arXiv:2502.08888', 'title': 'LLM-Enhanced Multiple Instance Learning for Joint Rumor and Stance Detection with Social Context Information', 'authors': 'Ruichao Yang, Jing Ma, Wei Gao, Hongzhan Lin', 'link': 'https://arxiv.org/abs/2502.08888', 'abstract': 'The proliferation of misinformation, such as rumors on social media, has drawn significant attention, prompting various expressions of stance among users. Although rumor detection and stance detection are distinct tasks, they can complement each other. Rumors can be identified by cross-referencing stances in related posts, and stances are influenced by the nature of the rumor. However, existing stance detection methods often require post-level stance annotations, which are costly to obtain. We propose a novel LLM-enhanced MIL approach to jointly predict post stance and claim class labels, supervised solely by claim labels, using an undirected microblog propagation model. Our weakly supervised approach relies only on bag-level labels of claim veracity, aligning with multi-instance learning (MIL) principles. To achieve this, we transform the multi-class problem into multiple MIL-based binary classification problems. We then employ a discriminative attention layer to aggregate the outputs from these classifiers into finer-grained classes. Experiments conducted on three rumor datasets and two stance datasets demonstrate the effectiveness of our approach, highlighting strong connections between rumor veracity and expressed stances in responding posts. Our method shows promising performance in joint rumor and stance detection compared to the state-of-the-art methods.', 'abstract_zh': '社交媒体上传播的虚假信息（如谣言）引起了广泛关注，用户们也在此过程中表达了各自的立场。尽管谣言检测和立场检测是两个独立的任务，但它们可以相互补充。谣言可以通过在相关帖子中进行交叉参考识别，而立场又受到谣言性质的影响。然而，现有的立场检测方法通常需要帖子级别的立场标注，这在获取成本上较高。我们提出了一种新的增强型多实例学习（MIL）方法，通过一个无向微观博客传播模型，仅使用声明标签的监督，共同预测帖子立场和声明类别标签。我们的弱监督方法仅依赖于声明真实性的包级标签，这符合多实例学习（MIL）的原则。为此，我们将多类别问题转化为多个基于MIL的二元分类问题。然后，我们使用一个区分性注意力层来汇总这些分类器的输出，生成更精细的类别。在三个谣言数据集和两个立场数据集上的实验表明了我们方法的有效性，突出了谣言真实性与回应帖子中表达的立场之间的紧密联系。与最新方法相比，我们的方法在联合谣言和立场检测方面表现出有希望的性能。', 'title_zh': '增强型多实例学习：结合社交背景信息的谣言检测与立场检测联合模型'}
{'arxiv_id': 'arXiv:2502.08866', 'title': 'BrainWavLM: Fine-tuning Speech Representations with Brain Responses to Language', 'authors': 'Nishitha Vattikonda, Aditya R. Vaidya, Richard J. Antonello, Alexander G. Huth', 'link': 'https://arxiv.org/abs/2502.08866', 'abstract': 'Speech encoding models use auditory representations to predict how the human brain responds to spoken language stimuli. Most performant encoding models linearly map the hidden states of artificial neural networks to brain data, but this linear restriction may limit their effectiveness. In this work, we use low-rank adaptation (LoRA) to fine-tune a WavLM-based encoding model end-to-end on a brain encoding objective, producing a model we name BrainWavLM. We show that fine-tuning across all of cortex improves average encoding performance with greater stability than without LoRA. This improvement comes at the expense of low-level regions like auditory cortex (AC), but selectively fine-tuning on these areas improves performance in AC, while largely retaining gains made in the rest of cortex. Fine-tuned models generalized across subjects, indicating that they learned robust brain-like representations of the speech stimuli. Finally, by training linear probes, we showed that the brain data strengthened semantic representations in the speech model without any explicit annotations. Our results demonstrate that brain fine-tuning produces best-in-class speech encoding models, and that non-linear methods have the potential to bridge the gap between artificial and biological representations of semantics.', 'abstract_zh': '语音编码模型使用听觉表示来预测人类大脑对语音刺激的反应。大多数性能最佳的编码模型将人工神经网络的隐藏状态线性映射到脑部数据，但这种线性限制可能限制了其效果。在本研究中，我们利用低秩适应（LoRA）对基于WavLM的编码模型进行端到端微调，以脑部编码目标为导向，产生我们名为BrainWavLM的模型。我们发现，在整个大脑皮层进行全面微调可以提高平均编码性能，并且稳定性更高，而不使用LoRA则会降低性能。这种改进是以诸如听觉皮层（AC）等低级区域的性能损失为代价的，但针对性地在这些区域进行微调可以改进AC区域的性能，同时在大脑的其余部分保留所获得的增益。微调后的模型在不同受试者之间具有普适性，表明这些模型学习到了稳健的类似于大脑的语音刺激表示。最后，通过训练线性探针，我们发现脑部数据增强了语音模型中的语义表示，而没有任何显式的注释。我们的结果显示，大脑微调产生了最佳的语音编码模型，非线性方法有可能弥合人工与生物语义表示之间的差距。', 'title_zh': 'BrainWavLM：通过大脑对语言的反应微调语音表示'}
{'arxiv_id': 'arXiv:2502.08826', 'title': 'Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation', 'authors': 'Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari', 'link': 'https://arxiv.org/abs/2502.08826', 'abstract': 'Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information enhancing factual and updated grounding. Recent advances in multimodal learning have led to the development of Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges to Multimodal RAG, distinguishing it from traditional unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse Multimodal RAG scenarios. Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. Resources are available at this https URL.', 'abstract_zh': '大语言模型（LLMs）由于依赖静态训练数据，面临着幻觉和过时知识的问题。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，从而增强事实性和更新的信息支持。最近在多模态学习方面的进展导致了多模态RAG的发展，它整合了文本、图像、音频和视频等多种模态，以增强生成的输出。然而，跨模态对齐和推理引入了多模态RAG特有的挑战，使其区别于传统的单模态RAG。本文综述了多模态RAG系统，涵盖了数据集、指标、基准测试、评估、方法论以及检索、融合、增强和生成方面的创新。我们详细审查了训练策略、鲁棒性增强和损失函数，同时探讨了多样的多模态RAG应用场景。此外，我们讨论了该领域的开放挑战和未来研究方向，以支持这一不断发展领域的进步。本文为基础开发更强大且可靠的AI系统提供了支持，这些系统能够有效地利用多模态动态外部知识库奠定了基础。更多资源可以在以下链接获取：[此https URL]', 'title_zh': '任何形式的提问：多模态检索增强生成综述'}
{'arxiv_id': 'arXiv:2502.08825', 'title': 'Examining and Adapting Time for Multilingual Classification via Mixture of Temporal Experts', 'authors': 'Weisi Liu, Guangzeng Han, Xiaolei Huang', 'link': 'https://arxiv.org/abs/2502.08825', 'abstract': 'Time is implicitly embedded in classification process: classifiers are usually built on existing data while to be applied on future data whose distributions (e.g., label and token) may change. However, existing state-of-the-art classification models merely consider the temporal variations and primarily focus on English corpora, which leaves temporal studies less explored, let alone under multilingual settings. In this study, we fill the gap by treating time as domains (e.g., 2024 vs. 2025), examining temporal effects, and developing a domain adaptation framework to generalize classifiers over time on multiple languages. Our framework proposes Mixture of Temporal Experts (MoTE) to leverage both semantic and data distributional shifts to learn and adapt temporal trends into classification models. Our analysis shows classification performance varies over time across different languages, and we experimentally demonstrate that MoTE can enhance classifier generalizability over temporal data shifts. Our study provides analytic insights and addresses the need for time-aware models that perform robustly in multilingual scenarios.', 'abstract_zh': '时间隐含地嵌入了分类过程：分类器通常基于现有数据建立，但应用于未来数据时，未来数据的分布（例如标签和词项）可能会发生变化。然而，现有的先进分类模型主要关注时间变化，并主要集中在英语语料库上，这使得时间相关的研究较少被探索，更不用说在多语言环境下。在这项研究中，我们通过将时间视为域（例如2024年 vs. 2025年）来填补这一空白，考察时间效应，并开发了一个领域适应框架，以跨多语言领域的时间适应分类器。我们提出的模型是混合时间专家（MoTE）框架，该框架通过利用语义和数据分布的变化来学习和适应时间趋势，从而改进分类模型。我们的分析表明，不同语言的分类性能随时间变化，实验结果还证明MoTE能够增强分类器在时间数据变化中的泛化能力。我们的研究提供了分析洞见，并满足了在多语言场景中表现出色的时间感知模型的需求。', 'title_zh': '通过混合时序专家进行多语言分类的时间适应性研究'}
{'arxiv_id': 'arXiv:2502.08818', 'title': 'Lexical Manifold Reconfiguration in Large Language Models: A Novel Architectural Approach for Contextual Modulation', 'authors': 'Koinis Vassilis, Godfrey Milbourne, Harriet Featherstone, Xanthe Peverell, Yorick Bletchley, Zachary Montford', 'link': 'https://arxiv.org/abs/2502.08818', 'abstract': 'Contextual adaptation in token embeddings plays a central role in determining how well language models maintain coherence and retain semantic relationships over extended text sequences. Static embeddings often impose constraints on lexical flexibility, leading to suboptimal performance when faced with complex sentence structures or domain-specific terminology shifts. To address this limitation, a structured approach was developed for dynamically reconfiguring token embeddings through continuous geometric transformations, ensuring that representations evolved in response to evolving discourse structures. A manifold-based transformation mechanism was integrated to regulate lexical positioning, allowing embeddings to undergo controlled shifts while preserving linguistic relationships across varying textual contexts. Empirical evaluations demonstrated that embedding reconfiguration contributed to reductions in perplexity, improved lexical coherence, and enhanced sentence-level continuity, particularly in structured and domain-adaptive text generation tasks. Comparative analyses of embedding drift indicated that dynamically restructured representations maintained stronger contextual consistency, reducing misalignment in token dependencies while preserving fluency in language modeling outputs. Computational overhead assessments confirmed that while training complexity increased due to the iterative refinement of embeddings, inference remained efficient, ensuring practical feasibility for real-time generation. Evaluations across multiple datasets further demonstrated that dynamically modulated embeddings exhibited broader lexical diversity, reducing repetitive token patterns and enabling a more adaptable representation learning process.', 'abstract_zh': '上下文适配在词嵌入中的作用是决定语言模型在长文本序列中保持连贯性和保留语义关系能力的核心因素。静态嵌入往往会对词汇灵活性施加限制，当面对复杂的句子结构或特定领域的术语变化时，会导致性能不佳。为了解决这一局限性，开发了一种结构化方法，通过连续的几何变换动态重构词嵌入，确保嵌入能够根据话语结构的变化进行动态演化。基于流形的变换机制被集成以调节词汇定位，使嵌入能够在保持语言关系的同时，在不同文本上下文中进行可控变化。实验评估表明，嵌入重构有助于降低困惑度、提高词汇连贯性，并增强句子级别的连贯性，特别是在结构化和领域适应性文本生成任务中表现更为突出。嵌入漂移的比较分析表明，动态重构的表示更具有上下文一致性，减少了词依赖关系的偏差，并保持了语言模型输出的流畅性。计算开销评估证实，虽然由于嵌入的迭代细化训练复杂度提高，但推理效率依然保持高效，确保了实时生成的现实可行性。跨多个数据集的进一步评估表明，动态调制的嵌入展现了更广泛的词汇多样性，减少了重复的词模式，并允许更灵活的表征学习过程。', 'title_zh': '大型语言模型中的词元流形重构：一种面向上下文调节的新型架构方法'}
{'arxiv_id': 'arXiv:2502.08796', 'title': 'A Systematic Review on the Evaluation of Large Language Models in Theory of Mind Tasks', 'authors': 'Karahan Sarıtaş, Kıvanç Tezören, Yavuz Durmazkeser', 'link': 'https://arxiv.org/abs/2502.08796', 'abstract': "In recent years, evaluating the Theory of Mind (ToM) capabilities of large language models (LLMs) has received significant attention within the research community. As the field rapidly evolves, navigating the diverse approaches and methodologies has become increasingly complex. This systematic review synthesizes current efforts to assess LLMs' ability to perform ToM tasks, an essential aspect of human cognition involving the attribution of mental states to oneself and others. Despite notable advancements, the proficiency of LLMs in ToM remains a contentious issue. By categorizing benchmarks and tasks through a taxonomy rooted in cognitive science, this review critically examines evaluation techniques, prompting strategies, and the inherent limitations of LLMs in replicating human-like mental state reasoning. A recurring theme in the literature reveals that while LLMs demonstrate emerging competence in ToM tasks, significant gaps persist in their emulation of human cognitive abilities.", 'abstract_zh': '近年来，研究界对大规模语言模型（LLMs）的理论共情（Theory of Mind, ToM）能力的评估引起了广泛关注。随着该领域的迅速发展，导航各种方法和方法论变得越来越复杂。本系统综述综合了当前评估LLMs执行ToM任务能力的努力，这是人类认知的一个重要方面，涉及对自己和他人心理状态的归因。尽管取得了显著进展，但LLMs在ToM方面的专业能力仍存在争议。通过基于认知科学的分类体系对基准和任务进行分类，本综述批判性地审视了评估技术、提示策略以及LLMs在复制人类类似的心理状态推理方面的内在限制。文献中的一个反复出现的主题表明，尽管LLMs展示了在ToM任务方面的新兴能力，但在模仿人类认知能力方面仍存在显著差距。', 'title_zh': '关于理论心智任务中大型语言模型评估的系统性综述'}
{'arxiv_id': 'arXiv:2502.08788', 'title': 'If Multi-Agent Debate is the Answer, What is the Question?', 'authors': 'Hangfan Zhang, Zhiyao Cui, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, Shuyue Hu', 'link': 'https://arxiv.org/abs/2502.08788', 'abstract': 'Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.', 'abstract_zh': '多智能体辩论（MAD）已成为通过在推理过程中让多个智能体进行迭代讨论来增强大型语言模型（LLMs）的事实准确性及推理质量的一种有前景的方法。尽管具有潜力，但我们认为当前的MAD研究在评估实践方面存在严重的不足，包括数据集重叠有限以及基线不一致，这严重地引发了对其普遍适用性的担忧。相应地，本文对五种代表性的MAD方法在九个基准上进行了系统性评估，使用了四种基础模型。令人惊讶的是，我们的发现表明，即使消耗了更多的推理时间计算，MAD方法也无法可靠地超越简单的单智能体基线，如Chain-of-Thought和Self-Consistency。通过对这些结果的分析，我们发现模型异质性可以显著改善MAD框架。我们提出了Heter-MAD，允许单一LLM智能体访问来自不同基础模型的输出，从而提升当前MAD框架的性能。最后，我们概述了MAD未来发展的一些潜在方向，旨在激发更广泛的讨论并激发未来在此领域的研究工作。', 'title_zh': '如果多智能体辩论是答案，那么问题是什么？'}
{'arxiv_id': 'arXiv:2502.08777', 'title': 'Zero-Shot Belief: A Hard Problem for LLMs', 'authors': 'John Murzaku, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.08777', 'abstract': 'We present two LLM-based approaches to zero-shot source-and-target belief prediction on FactBank: a unified system that identifies events, sources, and belief labels in a single pass, and a hybrid approach that uses a fine-tuned DeBERTa tagger for event detection. We show that multiple open-sourced, closed-source, and reasoning-based LLMs struggle with the task. Using the hybrid approach, we achieve new state-of-the-art results on FactBank and offer a detailed error analysis. Our approach is then tested on the Italian belief corpus ModaFact.', 'abstract_zh': '我们提出了两种基于大规模语言模型（LLM）的方法来零样本预测FactBank中的源方和目标方信念：一种统一系统，在单一通过过程中识别事件、来源和信念标签，以及一种混合方法，该方法使用微调后的DeBERTa标注器进行事件检测。我们展示了多种开源、封闭源和基于推理的LLM在这项任务上面临挑战。通过混合方法，我们在FactBank上取得了新的最佳结果，并提供了详细的错误分析。随后，我们将在意大利信念语料库ModaFact上测试我们的方法。', 'title_zh': '零样本信念推断：LLM面临的难题'}
{'arxiv_id': 'arXiv:2502.08773', 'title': 'Universal Model Routing for Efficient LLM Inference', 'authors': 'Wittawat Jitkrittum, Harikrishna Narasimhan, Ankit Singh Rawat, Jeevesh Juneja, Zifeng Wang, Chen-Yu Lee, Pradeep Shenoy, Rina Panigrahy, Aditya Krishna Menon, Sanjiv Kumar', 'link': 'https://arxiv.org/abs/2502.08773', 'abstract': "Large language models' significant advances in capabilities are accompanied by significant increases in inference costs. Model routing is a simple technique for reducing inference cost, wherein one maintains a pool of candidate LLMs, and learns to route each prompt to the smallest feasible LLM. Existing works focus on learning a router for a fixed pool of LLMs. In this paper, we consider the problem of dynamic routing, where new, previously unobserved LLMs are available at test time. We propose a new approach to this problem that relies on representing each LLM as a feature vector, derived based on predictions on a set of representative prompts. Based on this, we detail two effective strategies, relying on cluster-based routing and a learned cluster map respectively. We prove that these strategies are estimates of a theoretically optimal routing rule, and provide an excess risk bound to quantify their errors. Experiments on a range of public benchmarks show the effectiveness of the proposed strategies in routing amongst more than 30 unseen LLMs.", 'abstract_zh': '大语言模型的能力显著提升伴随着推理成本的显著增加。模型路由是一种降低推理成本的简单技术，其中通过维护一个候选模型池，并学会将每个提示分配到最小可行的模型来实现。现有工作主要关注为固定模型池训练一个路由器。本文则考虑一种动态路由问题，在测试时可以使用新的、之前未见过的模型。我们提出了一种新的方法，该方法通过将每个模型表示为基于一组代表性提示预测得到的特征向量来进行。基于此，我们详细介绍了两种有效的策略，一种是基于聚类路由，另一种是基于学习得到的聚类图。我们证明了这些策略是理论上最优路由规则的估计，并提供了超出风险界的量化来衡量它们的误差。实验结果表明，在30多个未见过的模型中进行路由时，所提出的策略具有有效性。', 'title_zh': '高效语言模型推理的通用模型路由方法'}
{'arxiv_id': 'arXiv:2502.08767', 'title': 'SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence', 'authors': 'Zhining Liu, Rana Ali Amjad, Ravinarayana Adkathimar, Tianxin Wei, Hanghang Tong', 'link': 'https://arxiv.org/abs/2502.08767', 'abstract': 'Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide factually correct grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information - an issue common in real-world scenarios. To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and factually grounded responses without additional training or iterative prompting. We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency. Our code and documentation are available at this https URL.', 'abstract_zh': '为语言模型（LMs）提供相关证据（通过检索或用户提供的方式）可以在上下文中显著提高其生成事实正确且基于证据的响应的能力。然而，近期的研究发现，当上下文中包含噪声和无关信息时，LMs 往往难以全面理解和利用关键证据，这在实际场景中是一个常见问题。为解决这一问题，我们提出了一种名为 SelfElicit 的推理时方法，该方法通过自我引导的明确突出显示帮助LMs集中于关键的上下文证据。通过利用深层层注意力分数潜藏的证据检索能力，我们的方法能够自动识别并强调输入上下文中的关键证据，从而促进更精确且基于事实的响应，而无需额外的训练或迭代提示。我们展示了在多种基于证据的问答任务中，SelfElicit 可以在不同家族的LMs上带来一致且显著的改进，同时保持计算效率。我们的代码和文档可在以下网址获取：this https URL。', 'title_zh': 'SelfElicit：你的语言模型秘密地知道相关证据的位置'}
{'arxiv_id': 'arXiv:2502.08745', 'title': 'IHEval: Evaluating Language Models on Following the Instruction Hierarchy', 'authors': 'Zhihan Zhang, Shiyang Li, Zixuan Zhang, Xin Liu, Haoming Jiang, Xianfeng Tang, Yifan Gao, Zheng Li, Haodong Wang, Zhaoxuan Tan, Yichuan Li, Qingyu Yin, Bing Yin, Meng Jiang', 'link': 'https://arxiv.org/abs/2502.08745', 'abstract': "The instruction hierarchy, which establishes a priority order from system messages to user messages, conversation history, and tool outputs, is essential for ensuring consistent and safe behavior in language models (LMs). Despite its importance, this topic receives limited attention, and there is a lack of comprehensive benchmarks for evaluating models' ability to follow the instruction hierarchy. We bridge this gap by introducing IHEval, a novel benchmark comprising 3,538 examples across nine tasks, covering cases where instructions in different priorities either align or conflict. Our evaluation of popular LMs highlights their struggle to recognize instruction priorities. All evaluated models experience a sharp performance decline when facing conflicting instructions, compared to their original instruction-following performance. Moreover, the most competitive open-source model only achieves 48% accuracy in resolving such conflicts. Our results underscore the need for targeted optimization in the future development of LMs.", 'abstract_zh': '指令层级结构，它确立了从系统消息、用户消息、对话历史到工具输出的优先顺序，对于确保语言模型（LM）的一致性和安全性至关重要。尽管该话题非常重要，但其研究相对较少，缺乏全面的基准测试来评估模型遵循指令层级结构的能力。为填补这一空白，我们引入了IHEval，这是一个包含3,538个示例的新型基准，覆盖了不同优先级指令既一致又冲突的各种情况。对流行LMs的评估显示，它们在识别指令优先级方面存在困难。面对冲突指令时，所有评估模型的表现急剧下降，与它们原本遵循指令时的表现相比，下降尤其明显。此外，最优秀的开源模型在解决此类冲突时的准确率仅为48%。我们的结果强调了未来LM开发中针对这一问题进行优化的必要性。', 'title_zh': 'IHEval：评估语言模型遵循指令层级的能力'}
{'arxiv_id': 'arXiv:2502.08744', 'title': 'Are Expressions for Music Emotions the Same Across Cultures?', 'authors': 'Elif Celen, Pol van Rijn, Harin Lee, Nori Jacoby', 'link': 'https://arxiv.org/abs/2502.08744', 'abstract': 'Music evokes profound emotions, yet the universality of emotional descriptors across languages remains debated. A key challenge in cross-cultural research on music emotion is biased stimulus selection and manual curation of taxonomies, predominantly relying on Western music and languages. To address this, we propose a balanced experimental design with nine online experiments in Brazil, the US, and South Korea, involving N=672 participants. First, we sample a balanced set of popular music from these countries. Using an open-ended tagging pipeline, we then gather emotion terms to create culture-specific taxonomies. Finally, using these bottom-up taxonomies, participants rate emotions of each song. This allows us to map emotional similarities within and across cultures. Results show consistency in high arousal, high valence emotions but greater variability in others. Notably, machine translations were often inadequate to capture music-specific meanings. These findings together highlight the need for a domain-sensitive, open-ended, bottom-up emotion elicitation approach to reduce cultural biases in emotion research.', 'abstract_zh': '音乐能引发深刻的情感，然而跨语言中情感描述词的普遍性仍然存在争议。在跨文化音乐情感研究中，一个关键挑战是刺激选择和分类学编目的偏见，主要依赖于西方音乐和语言。为解决这一问题，我们提出了一种平衡实验设计，在巴西、美国和韩国进行了九项在线实验，共涉及672名参与者。首先，我们从这些国家中选取了一个平衡的流行音乐样本集。接着，使用开放式的标记管道收集情感术语，以创建特定于文化的分类学。最后，使用这些自下而上的分类学来评估每首歌曲的情感。这使我们能够映射不同文化内部和跨文化的情感相似性。结果显示，高度唤醒和高度价值的情感在不同文化中具有一致性，但在其他方面则表现出更大的差异性。值得注意的是，机器翻译往往无法准确捕捉到特定于音乐的情感意义。这些发现共同强调了在情感研究中减少文化偏见的需要，即倡导一种领域敏感的、开放式的、自下而上的情感诱发方法。', 'title_zh': '跨文化背景下音乐情绪表达是否一致？'}
{'arxiv_id': 'arXiv:2502.08687', 'title': 'Data Augmentation to Improve Large Language Models in Food Hazard and Product Detection', 'authors': 'Areeg Fahad Rasheed, M. Zarkoosh, Shimam Amer Chasib, Safa F. Abbas', 'link': 'https://arxiv.org/abs/2502.08687', 'abstract': 'The primary objective of this study is to demonstrate the impact of data augmentation using ChatGPT-4o-mini on food hazard and product analysis. The augmented data is generated using ChatGPT-4o-mini and subsequently used to train two large language models: RoBERTa-base and Flan-T5-base. The models are evaluated on test sets. The results indicate that using augmented data helped improve model performance across key metrics, including recall, F1 score, precision, and accuracy, compared to using only the provided dataset. The full code, including model training and the augmented dataset, can be found in this repository: this https URL', 'abstract_zh': '本研究的主要目标是展示使用ChatGPT-4o-mini进行数据扩增对食品危害和产品分析的影响。扩增的数据使用ChatGPT-4o-mini生成，并随后用于训练两个大型语言模型：RoBERTa-base和Flan-T5-base。这些模型在测试集上进行了评估。结果显示，使用扩增的数据有助于提高模型在关键指标（包括召回率、F1分数、精确率和准确性）上的性能，相比仅使用提供的数据集。完整的代码，包括模型训练和扩增数据集，可以在以下仓库中找到：this https URL', 'title_zh': '提高食品危害和产品检测大型语言模型性能的数据增强方法'}
{'arxiv_id': 'arXiv:2502.08669', 'title': 'Assessing the Impact of the Quality of Textual Data on Feature Representation and Machine Learning Models', 'authors': 'Tabinda Sarwar, Antonio Jose Jimeno Yepes, Lawrence Cavedon', 'link': 'https://arxiv.org/abs/2502.08669', 'abstract': 'Background: Data collected in controlled settings typically results in high-quality datasets. However, in real-world applications, the quality of data collection is often compromised. It is well established that the quality of a dataset significantly impacts the performance of machine learning models.\nMethods: A rudimentary error rate metric was developed to evaluate textual dataset quality at the token level. Mixtral Large Language Model (LLM) was used to quantify and correct errors in low quality datasets. The study analyzed two healthcare datasets: the high-quality MIMIC-III public hospital dataset and a lower-quality private dataset from Australian aged care homes. Errors were systematically introduced into MIMIC at varying rates, while the ACH dataset quality was improved using the LLM.\nResults: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH datasets respectively, we used Mixtral to introduce errors in MIMIC and correct errors in ACH. Mixtral correctly detected errors in 63% of progress notes, with 17% containing a single token misclassified due to medical terminology. LLMs demonstrated potential for improving progress note quality by addressing various errors. Under varying error rates, feature representation performance was tolerant to lower error rates (<10%) but declined significantly at higher rates.\nConclusions: The study revealed that models performed relatively well on datasets with lower error rates (<10%), but their performance declined significantly as error rates increased (>=10%). Therefore, it is crucial to evaluate the quality of a dataset before utilizing it for machine learning tasks. For datasets with higher error rates, implementing corrective measures is essential to ensure the reliability and effectiveness of machine learning models.', 'abstract_zh': '背景：在受控环境中收集的数据通常能够产生高质量的数据集。然而，在实际应用中，数据采集的质量往往受到妥协。已有的研究表明，数据集的质量对机器学习模型的性能有显著影响。\n方法：开发了一种初步的错误率指标，用于在词元级别评估文本数据集的质量。使用Mixtral大型语言模型（LLM）量化并纠正低质量数据集中的错误。研究分析了两个卫生保健数据集：高质量的MIMIC-III公开医院数据集和来自澳大利亚护理院的较低质量的私人数据集。通过不同错误率向MIMIC引入错误，同时使用LLM改进ACH数据集的质量。\n结果：对于MIMIC数据集中的35,774名患者和ACH数据集中的6,336名患者，我们使用Mixtral在MIMIC中引入错误并纠正ACH中的错误。Mixtral正确检测了63%的病程记录中的错误，其中17%的记录因医学术语而导致单个词元分类错误。大型语言模型展示了通过解决各种错误来改善病程记录质量的潜力。在不同错误率下，特征表示性能对较低错误率（<10%）具有容忍性，但在较高错误率下则显著下降。\n结论：研究结果表明，当数据集的错误率较低（<10%）时，模型的性能相对较好，但当错误率增加（>=10%）时，性能显著下降。因此，在使用数据集进行机器学习任务之前，评估数据集的质量至关重要。对于错误率较高的数据集，采取纠正措施以确保机器学习模型的可靠性和有效性是必不可少的。', 'title_zh': '评估文本数据质量对特征表示和机器学习模型的影响'}
{'arxiv_id': 'arXiv:2502.08668', 'title': 'Style Extraction on Text Embeddings Using VAE and Parallel Dataset', 'authors': 'InJin Kong, Shinyee Kang, Yuna Park, Sooyong Kim, Sanghyun Park', 'link': 'https://arxiv.org/abs/2502.08668', 'abstract': "This study investigates the stylistic differences among various Bible translations using a Variational Autoencoder (VAE) model. By embedding textual data into high-dimensional vectors, the study aims to detect and analyze stylistic variations between translations, with a specific focus on distinguishing the American Standard Version (ASV) from other translations. The results demonstrate that each translation exhibits a unique stylistic distribution, which can be effectively identified using the VAE model. These findings suggest that the VAE model is proficient in capturing and differentiating textual styles, although it is primarily optimized for distinguishing a single style. The study highlights the model's potential for broader applications in AI-based text generation and stylistic analysis, while also acknowledging the need for further model refinement to address the complexity of multi-dimensional stylistic relationships. Future research could extend this methodology to other text domains, offering deeper insights into the stylistic features embedded within various types of textual data.", 'abstract_zh': '本研究利用变分自编码器（VAE）模型探究不同圣经翻译版本之间的风格差异。通过将文本数据嵌入到高维向量中，本研究旨在检测和分析不同翻译版本之间的风格变化，特别关注区分美国标准Version（ASV）与其他版本。结果表明，每种翻译版本都表现出独特的风格分布，这些差异可以通过VAE模型有效识别。这些发现表明，VAE模型在捕捉和区分文本风格方面表现出色，尽管它主要是为了区分单一风格而优化的。研究强调了该模型在基于AI的文字生成和风格分析中的潜在应用，同时也指出了进一步完善模型的必要性，以应对多维度风格关系的复杂性。未来的研究可以将此方法应用于其他文本领域，从而更深入地探讨各种类型文本数据中嵌入的风格特征。', 'title_zh': '使用VAE和并行数据集进行文本嵌入的样式提取'}
{'arxiv_id': 'arXiv:2502.08666', 'title': 'Hallucination, Monofacts, and Miscalibration: An Empirical Investigation', 'authors': 'Muqing Miao, Michael Kearns', 'link': 'https://arxiv.org/abs/2502.08666', 'abstract': "Recent theoretical work by [Kalai and Vempala 2024] proves that a particular notion of hallucination rate in LLMs must be lower bounded by the training data monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration. Through systematic experiments with n-gram models and in-context learning with LLMs, we empirically investigate and validate this theory by examining how different underlying data distributions affect the monofact rate and a model's tendency to hallucinate. We then vary model miscalibration through controlled upweighting of training samples while holding monofact rates constant, allowing us to isolate miscalibration's reduction effect on hallucination. These findings suggest that both the distribution of fact frequencies in training data and the calibration-hallucination trade-off are inherent to probabilistic language generation. Our results also suggest that current practices of aggressive deduplication in training data may need to be reconsidered, as selective duplication could serve as a principled mechanism for reducing hallucination.", 'abstract_zh': '近年来，[Kalai和Vempala 2024]的理论研究证明，大语言模型（LLM）中的特定幻觉率必须被下限于训练数据单事实率（与经典的Good-Turing缺失质量估计器有关）减去模型校准误差。通过系统性的n-克数量模型实验和LLM的上下文学习实验，我们从实证角度研究并验证这一理论，探讨不同的数据分布如何影响单事实率和模型生成幻觉的倾向。我们随后通过控制训练样本的权重增加，同时保持单事实率不变，来改变模型校准误差，从而孤立地研究校准误差对幻觉的抑制效果。这些发现表明，训练数据中事实频率的分布以及校准与幻觉之间的权衡都是概率语言生成固有的特性。我们的结果还表明，当前训练数据中激进的去重实践可能需要重新考虑，有选择的去重可能作为一个原则性的机制来减少幻觉现象。', 'title_zh': '幻觉、 monocartes 和校准偏差：一项 empirical 调查\n\n注释：在学术翻译中，“monofacts”一词并未被广泛接受或定义，因此这里翻译为“monocartes”，这是根据词根“mono-”（单一）和“fact”（事实或事件）推测的翻译。如果“monofacts”是特指某个领域的术语，最好能提供进一步的信息以便准确翻译。此外，“empirical investigation”直译为“empirical 调查”，但在不同语境下，可能更合适的翻译是“实证研究”或“经验调查”。根据具体上下文，可以做相应调整。'}
{'arxiv_id': 'arXiv:2502.08663', 'title': 'Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis', 'authors': 'Emanuele Ricco, Lorenzo Cima, Roberto Di Pietro', 'link': 'https://arxiv.org/abs/2502.08663', 'abstract': 'Hallucinations are one of the major issues affecting LLMs, hindering their wide adoption in production systems. While current research solutions for detecting hallucinations are mainly based on heuristics, in this paper we introduce a mathematically sound methodology to reason about hallucination, and leverage it to build a tool to detect hallucinations. To the best of our knowledge, we are the first to show that hallucinated content has structural differences with respect to correct content. To prove this result, we resort to the Minkowski distances in the embedding space. Our findings demonstrate statistically significant differences in the embedding distance distributions, that are also scale free -- they qualitatively hold regardless of the distance norm used and the number of keywords, questions, or responses. We leverage these structural differences to develop a tool to detect hallucinated responses, achieving an accuracy of 66\\% for a specific configuration of system parameters -- comparable with the best results in the field. In conclusion, the suggested methodology is promising and novel, possibly paving the way for further research in the domain, also along the directions highlighted in our future work.', 'abstract_zh': '幻觉是影响大型语言模型（LLM）的一个主要问题，阻碍了它们在生产系统的广泛应用。虽然当前用于检测幻觉的研究方法主要基于启发式方法，但在本文中，我们介绍了一种数学上严谨的方法来推理幻觉问题，并利用这种方法开发了一个检测幻觉的工具。据我们所知，这是我们首次表明，幻觉内容与正确内容在结构上存在差异。为证明这一结果，我们利用嵌入空间中的闵可夫斯基距离。我们的发现表明，在嵌入距离分布中存在统计意义上显著的不同，这些差异在所使用的距离规范和关键词、问题或答复的数量变化时仍然保持有效的尺度不变性。我们利用这些结构差异来开发一个检测幻觉答复的工具，特定系统参数配置的准确率达到66%，这与该领域的最佳结果相当。综上所述，所建议的方法是富有前景且新颖的，有可能为进一步研究领域中的相关研究方向铺平道路，包括我们在未来工作中提出的其他方向。', 'title_zh': '幻觉检测：基于嵌入距离分析的概率框架'}
{'arxiv_id': 'arXiv:2502.08662', 'title': 'RoToR: Towards More Reliable Responses for Order-Invariant Inputs', 'authors': 'Soyoung Yoon, Dongha Ahn, Youngwon Lee, Minkyu Jung, HyungJoo Jang, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2502.08662', 'abstract': 'Mitigating positional bias of language models (LMs) for listwise inputs is a well-known and important problem (e.g., lost-in-the-middle). While zero-shot order-invariant LMs have been proposed to solve this issue, their success on practical listwise problems has been limited. In this work, as a first contribution, we identify and overcome two limitations to make zero-shot invariant LMs more practical: (1) training and inference distribution mismatch arising from modifying positional ID assignments to enforce invariance, and (2) failure to adapt to a mixture of order-invariant and sensitive inputs in practical listwise problems. To overcome, we propose (1) RoToR, a zero-shot invariant LM for genuinely order-invariant inputs with minimal modifications of positional IDs, and (2) Selective Routing, an adaptive framework that handles both order-invariant and order-sensitive inputs in listwise tasks. On the Lost in the middle (LitM), Knowledge Graph Question Answering (KGQA), and MMLU benchmarks, we show that RoToR with Selective Routing can effectively handle practical listwise input tasks in a zero-shot manner.', 'abstract_zh': '缓解语言模型（LMs）在列表输入中的位置偏差是一个众所周知且重要的问题（例如，丢失在中间）。虽然已经提出了一些零样本不变序语言模型来解决这一问题，但它们在实际列表输入问题上的应用效果有限。在本文中，作为第一个贡献，我们识别并克服了两个限制，使零样本不变序语言模型更加实用：（1）由于修改位置ID分配以确保不变性而导致的训练和推理分布不匹配，以及（2）在实际列表输入问题中对混合不变序和敏感输入的适应不足。为了解决这些问题，我们提出了（1）RoToR，这是一种专门为真正不变序输入设计的零样本不变序LM，仅对位置ID进行了最小的修改，以及（2）选择性路由，这是一种适应性框架，可以在列表任务中处理不变序和敏感输入。在Lost in the middle（LitM）、知识图谱问答（KGQA）以及MMLU基准测试中，我们展示了RoToR与选择性路由相结合可以有效地以零样本的方式处理实际的列表输入任务。', 'title_zh': 'RoToR：朝着更可靠的顺序不变输入响应方向努力\n\n在这个翻译中，“RoToR”被保持不变，因为它通常是一个特定系统的名称或缩写，没有具体的中文翻译。其余部分按照学术规范进行了翻译，确保标题的准确性和专业性。'}
{'arxiv_id': 'arXiv:2502.08661', 'title': 'Few-shot_LLM_Synthetic_Data_with_Distribution_Matching', 'authors': 'Jiyuan Ren, Zhaocheng Du, Zhihao Wen, Qinglin Jia, Sunhao Dai, Chuhan Wu, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2502.08661', 'abstract': "As large language models (LLMs) advance, their ability to perform in-context learning and few-shot language generation has improved significantly. This has spurred using LLMs to produce high-quality synthetic data to enhance the performance of smaller models like online retrievers or weak LLMs. However, LLM-generated synthetic data often differs from the real data in key language attributes (e.g., styles, tones, content proportions, etc.). As a result, mixing these synthetic data directly with real data may distort the original data distribution, potentially hindering performance improvements. To solve this, we introduce SynAlign: a synthetic data generation and filtering framework based on key attribute distribution matching. Before generation, SynAlign employs an uncertainty tracker surrogated by the Gaussian Process model to iteratively select data clusters distinct from selected ones as demonstrations for new data synthesis, facilitating the efficient exploration diversity of the real data. Then, a latent attribute reasoning method is employed: the LLM summarizes linguistic attributes of demonstrations and then synthesizes new data based on them. This approach facilitates synthesizing diverse data with linguistic attributes that appear in real this http URL generation, the Maximum Mean Discrepancy is used as the objective function to learn the sampling weight of each synthetic data, ensuring distribution matching with the real data. Our experiments on multiple text prediction tasks show significant performance improvements. We also conducted an online A/B test on an online retriever to demonstrate SynAlign's effectiveness.", 'abstract_zh': '随着大型语言模型（LLMs）的发展，它们进行上下文学习和少量示例语言生成的能力显著提高。这促使人们利用LLMs生成高质量的合成数据，以增强较小的语言模型（如在线检索器或弱LLMs）的表现。然而，LLM生成的合成数据往往在关键语言属性（如风格、语气、内容比例等）上与真实数据存在差异。因此，直接将这些合成数据与真实数据混合可能会影响真实数据分布，从而阻碍性能提升。为了解决这一问题，我们提出了SynAlign：一种基于关键属性分布匹配的合成数据生成和筛选框架。在生成之前，SynAlign利用高斯过程模型作为不确定度追踪代理，通过迭代地选择与已选数据不同的数据簇作为新数据合成的示例，促进真实数据多样性的高效探索。接着，采用潜在属性推理方法：LLM总结示例中的语言属性，然后根据这些属性生成新的数据。该方法有助于生成具有真实数据中出现的语言属性的多样化数据。在合成数据生成过程中，使用最大均值离散度（Maximum Mean Discrepancy）作为目标函数来学习每条合成数据的采样权重，确保与真实数据分布匹配。我们在多个文本预测任务上的实验显示出显著的性能提升。我们还在一个在线检索器上进行了在线A/B测试，以展示SynAlign的有效性。', 'title_zh': '基于分布匹配的少样本大语言模型合成数据生成'}
{'arxiv_id': 'arXiv:2502.08660', 'title': 'Semantic Role Labeling: A Systematical Survey', 'authors': 'Huiyao Chen, Meishan Zhang, Jing Li, Min Zhang, Lilja Øvrelid, Jan Hajič, Hao Fei', 'link': 'https://arxiv.org/abs/2502.08660', 'abstract': 'Semantic role labeling (SRL) is a central natural language processing (NLP) task aiming to understand the semantic roles within texts, facilitating a wide range of downstream applications. While SRL has garnered extensive and enduring research, there is currently a lack of a comprehensive survey that thoroughly organizes and synthesizes the field. This paper aims to review the entire research trajectory of the SRL community over the past two decades. We begin by providing a complete definition of SRL. To offer a comprehensive taxonomy, we categorize SRL methodologies into four key perspectives: model architectures, syntax feature modeling, application scenarios, and multi-modal extensions. Further, we discuss SRL benchmarks, evaluation metrics, and paradigm modeling approaches, while also exploring practical applications across various domains. Finally, we analyze future research directions in SRL, addressing the evolving role of SRL in the age of large language models (LLMs) and its potential impact on the broader NLP landscape. We maintain a public repository and consistently update related resources at: this https URL', 'abstract_zh': '语义角色标注（SRL）是自然语言处理（NLP）中的一个核心任务，旨在理解和分析文本中的语义角色，为广泛的应用提供支持。尽管SRL已经获得了广泛而持久的研究关注，但目前缺乏对整个领域的全面综述，难以系统地整合和总结这一领域。本文旨在回顾过去二十年SRL社区的研究轨迹。我们首先提供SRL的完整定义。为了提供一个全面的分类框架，我们将SRL方法论分为四个关键视角：模型架构、句法特征建模、应用场景以及多模态扩展。接着，我们讨论SRL基准、评估指标及范式建模方法，同时也探索在各个领域中的实际应用。最后，我们分析SRL未来的研究方向，探讨SRL在大规模语言模型（LLMs）时代的作用及其对更广泛NLP领域的潜在影响。我们维护一个公共仓库，并持续更新相关资源：https://github.com/<仓库名>', 'title_zh': '语义角色标注：系统综述'}
{'arxiv_id': 'arXiv:2502.08657', 'title': 'Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions', 'authors': 'Jingxin Xu, Guoshun Nan, Sheng Guan, Sicong Leng, Yilian Liu, Zixiao Wang, Yuyang Ma, Zhili Zhou, Yanzhao Hou, Xiaofeng Tao', 'link': 'https://arxiv.org/abs/2502.08657', 'abstract': "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than 50 human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on 9 popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.", 'abstract_zh': '近年来，如ChatGPT和LLaMA等AI代理主要依赖指令调整和强化学习来校准大型语言模型（LLMs）的输出，使其符合人类意图，确保输出无害且有益。现有的方法很大程度上依赖于高质量正样本的手动标注，但同时也面临着标签噪声和偏好响应与非偏好响应之间细微区分不足等问题。然而，易于获取的有害样本由于清晰的安全区分而常被过滤掉，从而删除了可以帮助LLM实现安全对齐的有价值负参考数据。为应对这一问题，我们提出了PT-ALIGN，这是一种新颖的安全自我校准方法，通过自动精炼正样本和有害样本，并进行细粒度的双重指令调整，从而减少人类监督。正样本是无害的响应，而有害样本则故意包含极其有害的内容，用作新的监督信号。具体来说，我们利用LLM本身通过探索少于50个人标注逐次生成和精炼训练实例。然后，我们采用最大似然估计（MLE）和细粒度的反似然训练（UT）两种损失函数，共同学习提高LLM的安全性。MLE损失促使LLM最大化生成基于正样本的无害内容。相反，细粒度的UT损失则引导LLM在负面样本的词级层次上尽量减少有害词汇的输出，从而引导模型将安全与有效性分离，使其朝向更安全的微调目标发展，并提高生成有益和可靠内容的可能性。我们在9个流行的开源LLM上进行的实验表明，PT-ALIGN在实现安全性对齐方面有效，同时保持了与现有模型相当的有用性和帮助性。', 'title_zh': '减少人工干预实现大型语言模型双向安全自我对齐的正面样本和有毒样本精炼'}
{'arxiv_id': 'arXiv:2502.09622', 'title': 'Theoretical Benefit and Limitation of Diffusion Language Model', 'authors': 'Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He', 'link': 'https://arxiv.org/abs/2502.09622', 'abstract': 'Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off is not yet well understood. In this paper, we present a rigorous theoretical analysis of a widely used type of diffusion language model, the Masked Diffusion Model (MDM), and find that its effectiveness heavily depends on the target evaluation metric. Under mild conditions, we prove that when using perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling steps regardless of sequence length, demonstrating that efficiency can be achieved without sacrificing performance. However, when using the sequence error rate--which is important for understanding the "correctness" of a sequence, such as a reasoning chain--we show that the required sampling steps must scale linearly with sequence length to obtain "correct" sequences, thereby eliminating MDM\'s efficiency advantage over autoregressive models. Our analysis establishes the first theoretical foundation for understanding the benefits and limitations of MDMs. All theoretical findings are supported by empirical studies.', 'abstract_zh': '扩散语言模型作为一种有前景的文本生成方法已经出现。人们自然会期望这种模型能作为自回归模型的有效替代，因为每一轮扩散步骤中可以并行采样多个令牌。然而，其效率-准确性Trade-off尚未得到充分理解。本文中，我们对广泛使用的扩散语言模型类型——掩码扩散模型（MDM）进行了严格的理论分析，并发现其效果很大程度上依赖于目标评估指标。在温和条件下，我们证明了当使用困惑度作为度量标准时，MDM在采样步骤中可以几乎在任何的序列长度下实现最优困惑度，表明效率可以在不牺牲性能的情况下得到保证。然而，当我们使用序列误差率作为度量标准时（这对于理解序列的“正确性”至关重要，如推理链），我们展示需要的采样步骤必须与序列长度线性增长，才能生成“正确的”序列，从而排除了MDM在效率上的优势，使其无法超越自回归模型。我们的分析建立了理解MDM的优势和局限性的首个理论基础。所有理论发现都得到了实证研究的支持。', 'title_zh': '扩散语言模型的理论优势与局限性'}
{'arxiv_id': 'arXiv:2502.09621', 'title': 'MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency', 'authors': 'Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li', 'link': 'https://arxiv.org/abs/2502.09621', 'abstract': 'Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: this https URL', 'abstract_zh': '使用链式推理（CoT）回答问题显著增强了大型语言模型（LLMs）的推理能力，但对其在大型多模态模型（LMMs）上的影响仍缺乏系统评估和深入研究。本文介绍了MME-CoT，这是一种专门的基准测试，评估LMMs的CoT推理性能，涵盖六大领域：数学、科学、OCR、逻辑、时空和一般场景。作为该领域的首个全面研究，我们提出了一套全面的评估套件，其中包括三个新的度量标准，从细微层面评估推理质量、鲁棒性和效率。利用精选的高质量数据和独特的评估策略，我们深入分析了当前最先进的LMMs，揭示了几点关键见解：1）具有反射机制的模型展现出更优质的CoT推理能力，Kimi k1.5的表现优于GPT-4o并取得最高质量的结果；2）CoT提示往往在感知密集型任务上降低了LMMs的性能，表明可能存在有害的过度思考行为；3）尽管CoT质量高，具有反射机制的LMMs在常规响应和自我纠正阶段均表现出显著的低效率。我们希望MME-CoT能够成为促进LMMs多模态推理发展的基石。项目页面：[这里](this https URL)', 'title_zh': 'MME-CoT：在大型多模态模型中评估推理过程的质量、稳健性和效率'}
{'arxiv_id': 'arXiv:2502.09620', 'title': 'Exploring the Potential of Encoder-free Architectures in 3D LMMs', 'authors': 'Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao', 'link': 'https://arxiv.org/abs/2502.09620', 'abstract': 'Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at this https URL', 'abstract_zh': '在2D视觉领域，已经初步探索了基于编码器的架构，但在3D理解场景中，尚未明确地验证其有效性。本文首次全面调查了基于编码器的3D大型多模态模型（LMMs）面临的挑战，以及编码器自由架构是否可以有效克服这些挑战。这些挑战包括难以适应不同的点云分辨率以及从编码器提取的点特征不能满足大型语言模型（LLMs）的语义需求。本文通过以下方式确定了使3D LMMs去除编码器的设计关键方面，从而使LLM承担3D编码器的角色：1）我们在预训练阶段提出了LLM嵌入语义编码策略，探索了各种点云自监督损失的效果，并提出了混合语义损失来提取高层次语义；2）我们在指令调优阶段引入了层次几何聚合策略，将归纳偏置引入LLM的早期层，以关注点云的局部细节。最终，本文提出了第一个基于编码器自由架构的3D LMM，ENEL。我们的7B模型与当前最先进的模型ShapeLLM-13B相当，在分类、描述和视觉问答任务上的准确率分别为55.0%、50.92%和42.7%。我们的结果表明，编码器自由架构在3D理解领域替代基于编码器的架构具有很高的前景。代码已发布于 this https URL。', 'title_zh': '探索无编码器架构在3D LMM中的潜在应用'}
{'arxiv_id': 'arXiv:2502.09601', 'title': 'CoT-Valve: Length-Compressible Chain-of-Thought Tuning', 'authors': 'Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, Xinchao Wang', 'link': 'https://arxiv.org/abs/2502.09601', 'abstract': "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.", 'abstract_zh': '链式思考显著增强了模型的推理能力，但由于推理链较长导致推理成本显著增加。我们观察到，尽管在简单任务中推理路径可以轻松压缩，但在复杂任务中却困难重重。因此，我们探索了一种在单一模型中仅通过弹性控制推理路径长度以动态减少推理成本的可能性。为此，我们提出了一种新的调优和推理策略——CoT-Valve，旨在使模型能够生成不同长度的推理链。为此，我们提议在参数空间中识别一个方向，当调整该方向时，可以有效地控制生成的链式思考（CoT）的长度。此外，我们展示了这一性质在压缩推理链方面的价值。我们构建了从长链到短链的同质问题数据集，并探索了CoT-Valve的两种增强策略：(1) 精确可压缩链式思考调优方法，以及(2) 进阶链式思考长度压缩方法。我们的实验表明，CoT-Valve成功实现了链式思考的可控性和压缩性，并且在性能上优于基于提示的控制。我们将其应用于QwQ-32B-Preview，在GSM8K数据集上将推理链从741个词元减少到225个词元，仅略微降低了性能（从95.07%降至94.92%），在AIME数据集上将推理链从6827个词元减少到4629个词元，仅增加了一个错误答案。', 'title_zh': 'CoT-Valve: 长度可压缩的思考链调整方法'}
{'arxiv_id': 'arXiv:2502.09597', 'title': 'Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs', 'authors': 'Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin', 'link': 'https://arxiv.org/abs/2502.09597', 'abstract': "Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in a long-context conversational setting. PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we evaluated the aforementioned preference following capabilities of 10 open-source and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following users' preferences during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' preference following abilities, paving the way for personalized conversational agents. Our code and dataset are available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）越来越多地被用作聊天机器人，但它们在个性化响应以适应用户偏好方面的能力仍然有限。我们引入了PrefEval基准，用于评估LLMs在长上下文对话环境中推断、记忆和遵循用户偏好的能力。PrefEval包含3,000个由人工精心挑选的用户偏好和查询对，覆盖20个主题。PrefEval中包含用户的个性化或偏好信息，既有显式的也有隐含的形式，并使用生成任务和分类任务评估LLMs的表现。通过PrefEval，我们评估了10种开源和专有LLMs在多会话对话中，不同上下文字长（最多100,000个标记）的偏好跟随能力。我们采用了各种提示、迭代反馈和检索增强生成方法作为基准测试。我们的基准测试工作揭示了最先进的LLMs在对话过程中主动跟随用户偏好方面面临显著挑战。特别是在零样本设置中，偏好跟随的准确率在仅仅10个回合（约3,000个标记）后，大多数评估模型的准确率低于10%。即使使用先进的提示和检索方法，在长上下文对话中偏好跟随能力也有所下降。此外，我们展示了在PrefEval上进行微调显著提高了性能。我们认为PrefEval是一个有价值的资源，可用于测量、理解和增强LLMs的偏好跟随能力，为个性化对话代理铺平道路。我们的代码和数据集可在此处访问：[相关链接]。', 'title_zh': '大型语言模型能够识别您的偏好吗？评估大型语言模型中的个性化偏好跟随能力'}
{'arxiv_id': 'arXiv:2502.09573', 'title': 'Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering', 'authors': 'Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu', 'link': 'https://arxiv.org/abs/2502.09573', 'abstract': "In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry.", 'abstract_zh': '在本研究中，我们通过探索和优化基于GPT的模型，在视频内容分类领域解决工业界的挑战。我们针对视频质量的七个关键类别进行了零样本分类优化，并贡献了一种通过提示优化和策略精细化改进GPT性能的新方法。实验结果显示，简化复杂的策略可以显著减少误负率。此外，我们引入了一种新的分解聚合式的提示工程技术，这种技术在性能上优于传统的单一提示方法。这些在实际工业问题上的实验表明，精心设计的提示可以显著提升GPT的性能，而无需额外的微调，从而为提高各种工业领域视频分类系统的性能提供了有效且可扩展的解决方案。', 'title_zh': '优化GPT在视频理解中的性能：零样本表现与提示工程'}
{'arxiv_id': 'arXiv:2502.09560', 'title': 'EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents', 'authors': 'Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang', 'link': 'https://arxiv.org/abs/2502.09560', 'abstract': 'Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at this https URL.', 'abstract_zh': '利用多模态大型语言模型（MLLMs）创建具身代理为应对现实世界任务提供了有前景的途径。虽然语言中心的具身代理受到了广泛关注，但基于MLLM的具身代理由于缺乏全面的评估框架而尚未得到充分探索。为了弥补这一差距，我们引入了EmbodiedBench，这是一个广泛的基准测试，旨在评估以视觉驱动的具身代理。EmbodiedBench 特点如下：\n\n1. 包含四个环境中的1128个不同任务，涵盖从高层语义任务（如家庭环境）到涉及原子动作的低层任务（如导航和操作）的广泛范围；\n2. 六个精心策划的子集评估了具身代理的基本能力，包括常识推理、复杂指令理解、空间意识、视觉感知和长期规划。\n\n通过广泛的实验，我们在EmbodiedBench中评估了13种领先的专有和开源MLLM。我们的研究发现：MLLM在高层任务中表现出色，但在低层操作方面存在问题，最佳模型GPT-4o在平均得分上仅为28.9%。EmbodiedBench 提供了一个多维度的标准化评估平台，不仅突出了现有的挑战，还为推进基于MLLM的具身代理提供了宝贵见解。我们的代码可在以下网址访问：[这里](this https URL)。', 'title_zh': 'EmbodiedBench：全面评估面向视觉驱动体化代理的多模态大型语言模型'}
{'arxiv_id': 'arXiv:2502.09447', 'title': 'Pixel-Level Reasoning Segmentation via Multi-turn Conversations', 'authors': 'Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria', 'link': 'https://arxiv.org/abs/2502.09447', 'abstract': 'Existing visual perception systems focus on region-level segmentation in single-turn dialogues, relying on complex and explicit query instructions. Such systems cannot reason at the pixel level and comprehend dynamic user intent that changes over interaction. Our work tackles this issue by introducing a novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on multi-turn conversations, tracking evolving user intent via multi-turn interactions for fine-grained segmentation. To establish a benchmark for this novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k multi-turn conversational scenarios with segmentation targets. Building on PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning Segmentation framework, integrates pixel-level segmentation with robust multi-turn conversation understanding, generating pixel-grounded explanations aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in pixel-level reasoning segmentation. Experimental results on the PRIST dataset demonstrate that our method outperforms current segmentation-specific baselines in terms of segmentation and LLM-based reasoning metrics. The code and data are available at: this https URL.', 'abstract_zh': '现有的视觉感知系统主要关注单轮对话中的区域级分割，依赖于复杂且明确的查询指令。这些系统无法在像素级别进行推理，并理解随着时间互动而变化的用户意图。我们的工作通过引入基于多轮对话的新任务——像素级推理分割(Pixel-level Reasoning Segmentation, Pixel-level RS)，解决这一问题。该任务通过多轮交互追踪用户意图的变化，实现细致入微的分割。为了为这一新颖任务建立基准，我们构建了一个名为Multi-turn conversational scenarios Based on Multi-Turn Pixel-level ReasonIng Segmentation Dataset (PRIST) 的数据集，其中包括来自8,300个交互场景的24,000个话语和对应的分割目标。基于PRIST数据集，我们进一步提出了MIRAS（Multi-turn Interactive ReAsoning Segmentation）框架，该框架将像素级分割与稳健的多轮对话理解相结合，生成与用户意图一致的像素级解释。PRIST数据集和MIRAS框架填补了像素级推理分割领域的空白。实验结果表明，在PRIST数据集上，我们的方法在分割和基于大模型的推理指标方面优于当前的分割专用基线方法。代码和数据可在以下链接获取：this https URL。', 'title_zh': '基于像素级推理的多轮对话分割方法'}
{'arxiv_id': 'arXiv:2502.09369', 'title': 'Language Agents as Digital Representatives in Collective Decision-Making', 'authors': 'Daniel Jarrett, Miruna Pîslar, Michiel A. Bakker, Michael Henry Tessler, Raphael Köster, Jan Balaguer, Romuald Elie, Christopher Summerfield, Andrea Tacchetti', 'link': 'https://arxiv.org/abs/2502.09369', 'abstract': 'Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, "representation" is the activity of making an individual\'s preferences present in the process via participation by a proxy agent -- i.e. their "representative". To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \\textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \\textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \\textit{digital representation} -- as the simulation of an agent\'s behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \\textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.', 'abstract_zh': '考虑集体决策过程，即一群个体在一组备选方案中通过交互选择他们偏好的结果。在这种情境下，“代表性”是指通过代理行动（即他们的“代表”）使得个体的偏好在过程中得以体现。为此，学习到的人类行为模型有可能扮演这一角色，在多agent场景研究和机制设计方面具有实际意义。本研究中，我们探讨了训练**语言代理**来充当人类代理人的代表，适当地表达被他们代表的个体的偏好。首先，我们规范化的集体决策情境——即一群代理与决策机制之间的一系列交互过程。在此基础上，我们规范化了**数字化代表性**问题——即模拟代理的行为，使其结果与机制的效果相当。最后，我们在多元人类达成一致意见的情境中进行了一项实证案例研究，并证明了可以对大规模语言模型进行微调以充当数字化代表的可能性。', 'title_zh': '语言代理作为集体决策中的数字代表'}
{'arxiv_id': 'arXiv:2502.09245', 'title': "You Do Not Fully Utilize Transformer's Representation Capacity", 'authors': 'Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, Daniil Gavrilov', 'link': 'https://arxiv.org/abs/2502.09245', 'abstract': "In contrast to RNNs, which compress previous tokens into a single hidden state, Transformers can attend to all previous tokens directly. However, standard Transformers only use representations from the immediately preceding layer. In this paper, we show that this design choice causes representation collapse and leads to suboptimal performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that preserves the model's overall memory footprint while expanding its representational capacity by allowing access to hidden states from earlier layers. Through extensive experiments across various architectures and different lookup mechanisms, we demonstrate consistent performance improvements on a wide range of tasks. Moreover, our analysis of the learned representation dynamics and our exploration of depthwise circuits reveal how LIMe integrates information across layers, pointing to promising directions for future research.", 'abstract_zh': '与循环神经网络（RNNs）不同，RNNs 将先前的标记压缩到单个隐状态中，而变压器可以直接关注所有先前的标记。然而，标准的变压器仅利用前一层的表示。在本文中，我们展示了这种设计选择会导致表示压缩，并导致次优化的性能。为了解决这个问题，我们引入了层集成记忆（LIMe）这一简单而强大的方法，该方法在保持模型总体内存占用的同时，通过允许访问早期层的隐状态来扩展其表示能力。通过在各种架构和不同的查找机制下进行广泛实验，我们在多种任务上展示了持续的性能改进。此外，我们对学习到的表示动态的分析以及对深度卷积的探索揭示了LIMe如何在不同层之间整合信息，指出了未来研究的潜在方向。', 'title_zh': '您并未充分利用变压器的表示能力'}
{'arxiv_id': 'arXiv:2502.09237', 'title': 'Reliable Conversational Agents under ASP Control that Understand Natural Language', 'authors': 'Yankai Zeng', 'link': 'https://arxiv.org/abs/2502.09237', 'abstract': 'Efforts have been made to make machines converse like humans in the past few decades. The recent techniques of Large Language Models (LLMs) make it possible to have human-like conversations with machines, but LLM\'s flaws of lacking understanding and reliability are well documented. We believe that the best way to eliminate this problem is to use LLMs only as parsers to translate text to knowledge and vice versa and carry out the conversation by reasoning over this knowledge using the answer set programming. I have been developing a framework based on LLMs and ASP to realize reliable chatbots that "understand" human conversation. This framework has been used to develop task-specific chatbots as well as socialbots. My future research is focused on making these chatbots scalable and trainable.', 'abstract_zh': '在过去的几十年中，人们一直在努力使机器的对话方式像人类一样。最近的大规模语言模型（LLMs）技术使得与机器进行像人类一样的对话成为可能，但LLMs在理解能力和可靠性方面存在的缺陷已经被充分记录。我们相信，解决这一问题的最佳方法是仅将LLMs用作解析器，用于文本到知识的翻译和反之亦然，并利用答案集编程对这些知识进行推理以进行对话。我一直致力于开发基于LLMs和ASP的框架，以实现能够“理解”人类对话的可靠聊天机器人。该框架已用于开发特定任务的聊天机器人和社会机器人。我的未来研究重点在于使这些聊天机器人具有可扩展性和可训练性。', 'title_zh': '在逻辑编程控制下可靠的对话代理及其对自然语言的理解'}
{'arxiv_id': 'arXiv:2502.09216', 'title': 'Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles', 'authors': 'Galileo Sartor, Adam Wyner, Giuseppe Contissa', 'link': 'https://arxiv.org/abs/2502.09216', 'abstract': 'In this paper, we present a modular system for representing and reasoning with legal aspects of traffic rules for autonomous vehicles. We focus on a subset of the United Kingdom\'s Highway Code (HC) related to junctions. As human drivers and automated vehicles (AVs) will interact on the roads, especially in urban environments, we claim that an accessible, unitary, high-level computational model should exist and be applicable to both users. Autonomous vehicles introduce a shift in liability that should not bring disadvantages or increased burden on human drivers. We develop a system "in silico" of the model.  The proposed system is built of three main components: a natural language interface, using Logical English, which encodes the rules; an internal representation of the rules in Prolog; and an multi-agent-based simulation environment, built in NetLogo. The three components interact: Logical English is translated into and out of Prolog (along with some support code); Prolog and NetLogo interface via predicates. Such a modular approach enables the different components to carry different "burdens" in the overall system; it also allows swapping of modules. Given NetLogo, we can visualize the effect of the modeled rules as well as validate the system with a simple dynamic running scenario. Designated agents monitor the behaviour of the vehicles for compliance and record potential violations where they occur. The information on potential violations is then utilized by Validators, to determine whether the violation is punishable, differentiating between exceptions and cases.', 'abstract_zh': '在本文中，我们提出了一种模块化系统，用于表示和推理自主车辆在交通规则方面的法律方面。我们关注英国公路手册（HC）中与交叉口相关的部分。由于人类驾驶员和自动车辆（AV）将在道路上相互作用，特别是在城市环境中，我们主张应该存在一个对用户都有实用价值的、统一的高层次计算模型。自主车辆引进了一种责任转移，不应给人类驾驶员带来不利或增加的负担。我们采用“在试管中”的方法开发了该模型系统。该系统由三个主要组件组成：使用逻辑英语作为自然语言界面，用于编码规则；内部规则表示为Prolog；以及使用NetLogo构建的多智能体仿真环境。这三部分相互作用：逻辑英语在与一些支持代码一起翻译成和从Prolog翻译出来；Prolog和NetLogo通过谓词互相接口。这种模块化方法使得系统中的不同组件可以承担不同的“负担”；同时也允许模块的替换。借助NetLogo，我们可以可视化模型规则的效果，并通过简单的动态运行场景验证系统。指定的智能体监控车辆的行为以确保合规，并记录发生的潜在违规行为。随后，验证器利用这些信息来判断违规行为是否可处罚，区分例外情况与一般情况。', 'title_zh': '填补空白：逻辑英语、Prolog 与多代理系统在自主车辆中的应用'}
{'arxiv_id': 'arXiv:2502.09213', 'title': 'Neuro-Symbolic Contrastive Learning for Cross-domain Inference', 'authors': 'Mingyue Liu, Ryo Ueda, Zhen Wan, Katsumi Inoue, Chris G. Willcocks', 'link': 'https://arxiv.org/abs/2502.09213', 'abstract': 'Pre-trained language models (PLMs) have made significant advances in natural language inference (NLI) tasks, however their sensitivity to textual perturbations and dependence on large datasets indicate an over-reliance on shallow heuristics. In contrast, inductive logic programming (ILP) excels at inferring logical relationships across diverse, sparse and limited datasets, but its discrete nature requires the inputs to be precisely specified, which limits their application. This paper proposes a bridge between the two approaches: neuro-symbolic contrastive learning. This allows for smooth and differentiable optimisation that improves logical accuracy across an otherwise discrete, noisy, and sparse topological space of logical functions.  We show that abstract logical relationships can be effectively embedded within a neuro-symbolic paradigm, by representing data as logic programs and sets of logic rules. The embedding space captures highly varied textual information with similar semantic logical relations, but can also separate similar textual relations that have dissimilar logical relations. Experimental results demonstrate that our approach significantly improves the inference capabilities of the models in terms of generalisation and reasoning.', 'abstract_zh': '预训练语言模型（PLMs）在自然语言推理（NLI）任务方面取得了显著进展，然而它们对文本扰动的敏感性和对大规模数据集的依赖性表明其过度依赖于浅层启发式方法。相比之下，归纳逻辑编程（ILP）在处理多样、稀疏和有限的数据集时能够推理出逻辑关系，但其离散性质要求输入必须精确指定，这限制了其应用。本文提出了这两种方法之间的桥梁：神经符号对比学习。这种方法允许在离散、嘈杂和稀疏的逻辑函数拓扑空间中进行平滑和可微优化，从而提高逻辑准确性。我们通过将数据表示为逻辑程序和逻辑规则集，展示了抽象逻辑关系可以在神经符号框架内有效嵌入。嵌入空间能够捕捉具有相似语义逻辑关系的广泛文本信息，同时也可以分离具有不同逻辑关系的相似文本关系。实验结果表明，我们的方法在泛化能力和推理方面显著提高了模型的推理能力。', 'title_zh': '跨域推理的神经符号对比学习'}
{'arxiv_id': 'arXiv:2502.09212', 'title': 'LP-LM: No Hallucinations in Question Answering with Logic Programming', 'authors': 'Katherine Wu, Yanhong A. Liu', 'link': 'https://arxiv.org/abs/2502.09212', 'abstract': 'Large language models (LLMs) are able to generate human-like responses to user queries. However, LLMs exhibit inherent limitations, especially because they hallucinate. This paper introduces LP-LM, a system that grounds answers to questions in known facts contained in a knowledge base (KB), facilitated through semantic parsing in Prolog, and always produces answers that are reliable.\nLP-LM generates a most probable constituency parse tree along with a corresponding Prolog term for an input question via Prolog definite clause grammar (DCG) parsing. The term is then executed against a KB of natural language sentences also represented as Prolog terms for question answering. By leveraging DCG and tabling, LP-LM runs in linear time in the size of input sentences for sufficiently many grammar rules. Performing experiments comparing LP-LM with current well-known LLMs in accuracy, we show that LLMs hallucinate on even simple questions, unlike LP-LM.', 'abstract_zh': '大型语言模型（LLMs）能够生成类似人类的响应以回答用户查询。然而，LLMs 存在固有的局限性，尤其是它们会产生虚假信息。本文介绍了一种名为 LP-LM 的系统，该系统通过在知识库（KB）中包含的已知事实基础上生成答案，并借助 Prolog 语义解析来实现，从而始终生成可靠的答案。\n\nLP-LM 通过 Prolog 确定性短语语法（DCG）解析生成输入问题的最可能组成部分解析树及其对应的 Prolog 项。随后，该项可以针对表达为 Prolog 项的自然语言句子知识库执行以进行问题回答。借助 DCG 和递归表的技术，LP-LM 在足够多的语法规则下，能够在输入句子的长度上以线性时间运行。通过实验比较 LP-LM 和当前已知的几种主流 LLMs 的准确度，我们证明了在甚至简单的问题上，LLMs 也会出现虚假信息，而 LP-LM 则不会。', 'title_zh': 'LP-LM：基于逻辑编程的问答中无幻觉生成'}
{'arxiv_id': 'arXiv:2502.09175', 'title': 'FLAME: Flexible LLM-Assisted Moderation Engine', 'authors': 'Ivan Bakulin, Ilia Kopanichuk, Iaroslav Bespalov, Nikita Radchenko, Vladimir Shaposhnikov, Dmitry Dylov, Ivan Oseledets', 'link': 'https://arxiv.org/abs/2502.09175', 'abstract': "The rapid advancement of Large Language Models (LLMs) has introduced significant challenges in moderating user-model interactions. While LLMs demonstrate remarkable capabilities, they remain vulnerable to adversarial attacks, particularly ``jailbreaking'' techniques that bypass content safety measures. Current content moderation systems, which primarily rely on input prompt filtering, have proven insufficient, with techniques like Best-of-N (BoN) jailbreaking achieving success rates of 80% or more against popular LLMs. In this paper, we introduce Flexible LLM-Assisted Moderation Engine (FLAME): a new approach that shifts the focus from input filtering to output moderation. Unlike traditional circuit-breaking methods that analyze user queries, FLAME evaluates model responses, offering several key advantages: (1) computational efficiency in both training and inference, (2) enhanced resistance to BoN jailbreaking attacks, and (3) flexibility in defining and updating safety criteria through customizable topic filtering. Our experiments demonstrate that FLAME significantly outperforms current moderation systems. For example, FLAME reduces attack success rate in GPT-4o-mini and DeepSeek-v3 by a factor of ~9, while maintaining low computational overhead. We provide comprehensive evaluation on various LLMs and analyze the engine's efficiency against the state-of-the-art jailbreaking. This work contributes to the development of more robust and adaptable content moderation systems for LLMs.", 'abstract_zh': '大型语言模型（LLMs）的迅速发展为用户-模型互动的管理带来了重大挑战。尽管LLMs展现了卓越的能力，但它们依然对敌对攻击（尤其是绕过内容安全措施的“脱疆”技术）非常脆弱。目前主要依赖输入提示过滤的内容管理系统已经证明效用不足，使用如“最佳N次采样”（Best-of-N，BoN）的方法可以达到80%以上的成功攻击率，针对流行的LLMs。在此论文中，我们提出了灵活的LLM辅助管理系统（FLAME）：一种新的方法，将重点从输入过滤转向输出管理。与传统的电路断开方法专注于分析用户查询不同，FLAME评估模型响应，具备以下几个关键优点：（1）训练和推理过程中的计算效率，（2）增强的对BoN脱疆攻击的抵抗力，以及（3）通过可定制的主题过滤灵活定义和更新安全标准。我们的实验表明，FLAME在性能上明显优于现有的管理系统。例如，在GPT-4o-mini和DeepSeek-v3中，FLAME将攻击成功率降低了约9倍，同时保持了较低的计算开销。我们全面评估了各种LLMs，并分析了引擎相对于最先进的脱疆技术的效率。这项工作为开发更加稳健和适应性强的LLM内容管理系统做出了贡献。', 'title_zh': 'FLAME：灵活的大型语言模型辅助调节引擎'}
{'arxiv_id': 'arXiv:2502.09100', 'title': 'Logical Reasoning in Large Language Models: A Survey', 'authors': 'Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, Yue Zhang', 'link': 'https://arxiv.org/abs/2502.09100', 'abstract': 'With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.', 'abstract_zh': '随着先进的推理模型如OpenAI的o3和DeepSeek-R1的出现，大型语言模型（LLMs）已经展示了令人瞩目的推理能力。然而，它们进行严格逻辑推理的能力仍是未解之谜。本文综述了最近在LLMs中进行逻辑推理的进展，这是人工智能研究中的一个关键领域。本文概述了逻辑推理在LLMs中的范围、理论基础以及用于评估推理能力的标准。我们分析了不同推理范式的现有能力——演绎、归纳、溯源和类比推理——并评估了增强推理性能的策略，包括以数据为中心的调优、强化学习、解码策略和神经符号方法。最后，本文提出了未来研究方向，强调了进一步探索以增强AI系统中逻辑推理的必要性。', 'title_zh': '大型语言模型中的逻辑推理：一种综述'}
{'arxiv_id': 'arXiv:2502.09083', 'title': "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking", 'authors': 'Greta Warren, Irina Shklovski, Isabelle Augenstein', 'link': 'https://arxiv.org/abs/2502.09083', 'abstract': "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps.", 'abstract_zh': '大型语言模型和生成式AI在在线媒体中的普及性加剧了对有效自动事实核查的需求，以帮助事实核查人员应对日益增多且复杂的虚假信息。事实核查的复杂性要求自动化事实核查系统提供能够使事实核查人员审查其输出的解释。然而，尚不清楚这些解释应如何与事实核查人员的决策和推理过程对齐，以有效地集成到他们的工作流程中。通过与事实核查专业人士进行半结构化访谈，我们通过以下方式填补了这一空白：(i) 描述事实核查人员评估证据、做出决策和解释其过程的方式；(ii) 考察事实核查人员在实际工作中如何使用自动化工具；以及(iii) 识别自动化事实核查工具所需的事实核查人员解释要求。研究发现存在未满足的解释需求，并确定了复制可验证的事实核查解释的重要标准，这些标准能够追踪模型的推理路径、引用特定证据、并突出不确定性和信息空白。', 'title_zh': '请展示证据：事实核查人员对可解释的自动化事实核查的需求'}
{'arxiv_id': 'arXiv:2502.08924', 'title': 'Escaping Collapse: The Strength of Weak Data for Large Language Model Training', 'authors': 'Kareem Amin, Sara Babakniya, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii', 'link': 'https://arxiv.org/abs/2502.08924', 'abstract': 'Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even "collapse", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. We find that the requirements are nearly minimal. We describe a training procedure that converges to an optimal LLM even if almost all of the non-synthetic training data is of poor quality. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. Our training procedure subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples -- in much the same way that boosting focuses the efforts of the weak learner -- leads to improved performance.', 'abstract_zh': '合成生成的数据在训练大规模语言模型中扮演着越来越重要的角色。然而，虽然合成数据已被证明是有用的，但研究也表明，如果没有适当的整理和优化，这些数据可能导致LLM的性能在多次训练迭代后停滞不前，甚至“崩溃”。在本文中，我们正式提出了这个问题，并发展了一种理论框架来探讨需要多少整理方可确保LLM的性能持续提升。我们发现，所需的要求几乎是最小的。我们描述了一种训练程序，即使大部分非合成训练数据质量很低，该程序也能收敛到最优的LLM。我们的分析灵感来自于增强学习，这是一种经典的机器学习技术，利用一个非常弱的学习算法来生成任意好的分类器。我们的训练程序涵盖了近年来提出的许多在合成数据上训练LLM的方法，从而我们的分析揭示了它们成功的原因，并提出了未来改进的机会。我们进行了实验来验证我们的理论，并展示了动态将标签资源集中于最难的示例——与增强学习将弱学习者的力量集中在最困难的问题上的方式类似——能够提高性能。', 'title_zh': '摆脱崩溃：薄弱数据对大型语言模型训练的强大力量'}
{'arxiv_id': 'arXiv:2502.08916', 'title': 'PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology', 'authors': 'Fatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O. Ikezogwo, Beibin Li, Tejoram Vivekanandan, Joann G. Elmore, Ranjay Krishna, Linda Shapiro', 'link': 'https://arxiv.org/abs/2502.08916', 'abstract': "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient's diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent's outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at this https URL", 'abstract_zh': '通过组织病理学全视野图像（WSI）来诊断疾病是现代病理学的基础，但这种诊断方法受到WSI的超高像素尺度和复杂性的挑战。经过训练的专业病理学家通过浏览WSI，寻找相关的图像片段，记笔记并汇编它们来生成最终的整体诊断。传统的AI方法，如多项式实例学习和基于Transformer的模型，无法实现这种整体、迭代的多尺度诊断流程，限制了它们在实际应用中的采用。我们提出了PathFinder这一多模态、多代理框架，模仿了高级病理学家的决策过程。PathFinder整合了四个AI代理：分诊代理、导航代理、描述代理和诊断代理，这些代理协作浏览WSI，收集证据，并提供包含自然语言解释的全面诊断。分诊代理将WSI分类为良性或风险性；如果是风险性，导航代理和描述代理将迭代地关注关键区域，生成重要图谱和描述性见解的图像样本。最后，诊断代理综合这些发现确定患者的诊断分类。我们的实验结果显示，在皮肤黑色素瘤诊断方面，PathFinder比最先进的方法提高了8%，并通过提供关于诊断相关图像的自然语言描述赋予了内在的可解释性。病理学家的定性分析表明，描述代理的输出质量高，与其他GPT-4类似。PathFinder也是第一个在这一具有挑战性的黑色素瘤分类任务中超过病理学家平均表现的AI系统，实现了高效、准确和可解释的病理学辅助诊断的新记录。数据、代码和模型可在以下网址获得：this https URL', 'title_zh': 'PathFinder：一种应用于组织病理学医疗诊断决策的多模态多代理系统'}
{'arxiv_id': 'arXiv:2502.08859', 'title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'authors': 'Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks', 'link': 'https://arxiv.org/abs/2502.08859', 'abstract': "As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.", 'abstract_zh': '随着语言模型在现有推理基准测试中取得进步，我们需要新的挑战来评估它们的认知边界。谜题解决事件提供了丰富多样的具有挑战性的跨模态问题，这些问题测试了广泛高级推理和知识能力，因此它们是评估前沿语言模型的一个独特测试平台。我们引入了EnigmaEval，这是一个从谜题竞赛和活动中提取的问题和解决方案的数据集，旨在测试模型在进行隐含知识综合和多步骤演绎推理方面的能力。与现有的推理和知识基准不同，谜题解决挑战要求模型发现看似无关的信息之间的隐藏联系，以揭示解决方案路径。该基准包括1184个不同复杂度的谜题——每个谜题通常需要熟练解决者团队数小时到数天才能完成，且具有明确、可验证的解决方案，这使得评估变得高效。最先进的语言模型在这些谜题上表现极差，甚至低于其他挑战性基准，如“人类最后的考试”等，这揭示了当模型面对需要非结构化和横向推理的问题时，其存在的局限性。', 'title_zh': 'EnigmaEval：长多模态推理挑战基准'}
{'arxiv_id': 'arXiv:2502.08820', 'title': 'Can a Single Model Master Both Multi-turn Conversations and Tool Use? CALM: A Unified Conversational Agentic Language Model', 'authors': 'Emre Can Acikgoz, Jeremiah Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil Koukoumidis, Dilek Hakkani-Tür, Gokhan Tur', 'link': 'https://arxiv.org/abs/2502.08820', 'abstract': 'Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CALM-IT, we train three models CALM 8B, CALM 70B, and CALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks.', 'abstract_zh': '大型语言模型（LLMs）结合API调用能力后，能够构建有效的语言代理（LAs），同时也在传统任务导向对话（TOD）范式上实现了革命性的转变。然而，当前的方法面临着一个关键的困境：TOD系统通常是在有限的目标API集合上进行训练的，因此在与新服务交互时需要新的数据来保持其质量，而LAs则没有经过训练以在多轮对话中维持用户意图。鉴于坚固的多轮对话管理和高级功能调用对于有效对话代理至关重要，我们在三个流行的基准上评估了这两项技能：多轮对话与服务2.4（TOD）、BFCL V3（LA）和API-Bank（LA），分析显示专门的方法在某一领域表现出色但在另一领域则表现不佳。为弥合这一鸿沟，我们引入了CALM（Conversational Agentic Language Model）这一综合方法，将其对话能力和代理能力结合起来。我们构建了CALM-IT，这是一个精心设计的多任务数据集，融合了多轮ReAct推理和复杂的API使用。借助CALM-IT，我们训练了三个模型：CALM 8B、CALM 70B 和CALM 405B，这些模型在三个基准上均超过了顶级专门领域模型，包括GPT-4o。', 'title_zh': '单个模型能够同时掌握多轮对话和工具使用吗？CALM：一个统一的对话代理语言模型'}
{'arxiv_id': 'arXiv:2502.08680', 'title': 'Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges', 'authors': 'Safal Shrestha, Minwu Kim, Keith Ross', 'link': 'https://arxiv.org/abs/2502.08680', 'abstract': "Mathematical reasoning in Large Language Models (LLMs) is often evaluated using benchmarks with limited numerical ranges, failing to reflect real-world problem-solving across diverse scales. Furthermore, most existing evaluation methods only compare model outputs to ground-truth answers, obscuring insights into reasoning processes. To address these limitations, we introduce GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs numerical values in math problems to assess model robustness across varying numerical scales. Additionally, we propose a novel grading methodology that distinguishes between logical and non-logical errors, offering a more precise evaluation of reasoning processes beyond computational accuracy. Our experiments with various models reveal a significant increase in logical error rates-up to 14 percentage points-as numerical complexity rises, demonstrating a general weakness in reasoning with out-of-distribution numerical values. Moreover, while models demonstrate high accuracy on standalone arithmetic tasks, their performance deteriorates substantially when computations are embedded within word problems. These findings provide a comprehensive evaluation of LLMs' mathematical reasoning capabilities and inform future research directions for improving numerical generalization in language models.", 'abstract_zh': '大规模语言模型（LLMs）中的数学推理通常通过具有有限数值范围的基准进行评估，这无法反映跨不同规模的实际问题解决能力。此外，大多数现有的评估方法仅将模型输出与真实答案进行比较，这掩盖了推理过程方面的见解。为了解决这些局限性，我们提出了GSM-Ranges数据集生成器，该生成器源自GSM8K，系统地扰动数学问题中的数值以评估模型在不同数值尺度下的鲁棒性。此外，我们提出了一种新的评分方法，该方法区分逻辑错误和非逻辑错误，提供了超越计算准确性的更精确的推理过程评估。我们的实验显示，随着数值复杂性的增加，逻辑错误率显著增加，最多可达到14个百分点，这表明在处理未见过的数值时普遍存在推理能力的缺陷。此外，尽管模型在独立的算术任务中表现出高准确性，但在计算嵌入在文字问题中的情况下，其性能显著下降。这些发现为全面评估LLMs的数学推理能力和指导未来改进语言模型的数值泛化方向提供了依据。', 'title_zh': '大型语言模型中的数学推理评估： Across宽广数值范围考查逻辑和算术错误'}
