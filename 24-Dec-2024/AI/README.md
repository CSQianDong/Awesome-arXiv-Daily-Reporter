# Automating the Search for Artificial Life with Foundation Models 

**Title (ZH)**: 使用基础模型自动化搜寻人工生命 

**Authors**: Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip Isola, David Ha  

**Link**: [PDF](https://arxiv.org/pdf/2412.17799)  

**Abstract**: With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone. 

**Abstract (ZH)**: 随着最近因蛋白质发现的重大突破而获得诺贝尔奖，基础模型（FMs）有望重新定义许多科学领域。人工生命（ALife）尚未整合FMs，因此为这一领域提供了一个主要机会，使其能够减轻长期依赖手工设计和试错方法来发现仿生模拟配置的负担。本文首次展示了这一机会的实现，即通过视觉-语言FMs实现了Automated Search for Artificial Life (ASAL)。该方法包括以下几个方面：（1）发现能够生成目标现象的模拟；（2）发现能够生成时间上无限新颖性（开放性）的模拟；（3）揭示一系列有趣的多样化模拟。由于FMs的通用性，ASAL能够在Boids、粒子生命、生命游戏、Lenia和神经细胞自动机等多种ALife平台中有效工作。一个重要结果是展示了该技术的潜力，具体表现在发现了Lenia和Boids中之前未见的生命形式，以及具有类似于生命游戏（Conway's Game of Life）的开放性的细胞自动机。此外，使用FMs使得可以以符合人类视角的方式量化之前定性的现象。这种新的范式有望加速人工生命的研究，超越单纯依靠人类智慧所能实现的水平。 

---
# Observation Interference in Partially Observable Assistance Games 

**Title (ZH)**: 部分可观测辅助博弈中的观察干扰 

**Authors**: Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell  

**Link**: [PDF](https://arxiv.org/pdf/2412.17797)  

**Abstract**: We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of perfect information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of perfect information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions. 

**Abstract (ZH)**: 我们研究部分可观测辅助博弈（POAGs），这是一种人类与人工智能价值观对齐问题的模型，允许人类和人工智能助手分别拥有部分观测信息。受人工智能欺骗问题的启发，我们研究了部分可观测性带来的一个新的现象：人工智能助手是否会有一旦干扰人类观测以获取利益的动力？首先，我们证明，在某些情况下，即使人类最优行动，最优助手也必须采取干扰观测的行为，即使存在等效且不干扰观测的行动。尽管这一结果似乎与单智能体决策中的经典定理相矛盾，即完美信息的价值非负，我们通过开发基于整个策略的干扰概念来解决这一看似矛盾。这可以视为将完美信息价值非负的经典结果扩展到合作多智能体环境。其次，我们证明，如果人类仅基于即时结果做出决策，助手可能需要干扰观测以查询人类的偏好。我们展示了如果人类最优行动，或引入人类向助手传达其偏好的通信渠道，这种干扰动机将消失。第三，我们证明，如果人类行为符合非理性模型（如玻尔兹曼模型），这可能会为助手提供干预观测的动力。最后，我们使用实验模型分析人工智能助手在考虑是否采取干扰观测的行为时面临的权衡。 

---
# Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy 

**Title (ZH)**: 大型多模态模型数据集调查、应用类别及分类体系研究 

**Authors**: Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Bhargava Kumar, Amit Agarwal, Ishan Banerjee, Srikant Panda, Tejaswini Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2412.17759)  

**Abstract**: Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights. 

**Abstract (ZH)**: 多模态学习是人工智能领域的一个迅速发展的研究方向，旨在通过整合和分析多种类型的数据，包括文本、图像、音频和视频，构建更加灵活和 robust 的系统。受到人类通过多种感官整合信息能力的启发，这一方法使得诸如文本到视频转换、视觉问答和图像描述等应用成为可能。本文综述了支持多模态语言模型（MLLMs）的数据集的最新进展。大规模多模态数据集是必不可少的，因为它们允许对这些模型进行彻底的测试和训练。文章侧重于这些数据集对学科的贡献，涵盖了用于训练、特定领域任务以及实际应用的各种数据集。同时，本文强调基准数据集对于在不同场景下评估模型的性能、可扩展性和适用性的重要性。由于多模态学习始终在不断发展，克服这些挑战将有助于人工智能研究和应用达到新的高度。 

---
# Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization 

**Title (ZH)**: Fourier 位置嵌入：增强注意力机制的周期扩展以提高长度泛化能力 

**Authors**: Ermo Hua, Che Jiang, Xingtai Lv, Kaiyan Zhang, Ning Ding, Youbang Sun, Biqing Qi, Yuchen Fan, Xue Kai Zhu, Bowen Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.17739)  

**Abstract**: Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling. 

**Abstract (ZH)**: 通过改进旋转位置嵌入（RoPE）扩展语言模型（LMs）的上下文长度已成为一种趋势。虽然现有工作主要关注RoPE在注意机制中的局限性，本文则对LMs的几乎所有部分进行了分析，揭示了其对基于RoPE的注意机制长度泛化的不良影响。利用离散信号处理理论，我们证明RoPE通过隐式实现非均匀离散傅里叶变换，从而实现了周期性注意。然而，由以下因素导致的光谱损伤削弱了这种周期性：1）注意机制之外的线性层和激活函数；2）时间域截断导致的训练不足的频率分量。基于这一观察，我们提出了傅里叶位置嵌入（FoPE），它通过增强注意机制的频域特性来改善其周期扩展和长度泛化。FoPE构建傅里叶级数，并对破坏性频率分量进行零填充，从而提高模型对光谱损伤的鲁棒性。在不同模型规模的多个实验中，与RoPE和ALiBi相比，FoPE在不同的上下文窗口中能够保持更稳定的困惑度和更一致的准确性，在“针hay堆”任务中尤为明显。分析和消融实验进一步支持了我们的方法和理论建模。 

---
# SMAC-Hard: Enabling Mixed Opponent Strategy Script and Self-play on SMAC 

**Title (ZH)**: SMAC-Hard：启用SMAC中的混合对手策略脚本和自游戏功能 

**Authors**: Yue Deng, Yan Yu, Weiyu Ma, Zirui Wang, Wenhui Zhu, Jian Zhao, Yin Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17707)  

**Abstract**: The availability of challenging simulation environments is pivotal for advancing the field of Multi-Agent Reinforcement Learning (MARL). In cooperative MARL settings, the StarCraft Multi-Agent Challenge (SMAC) has gained prominence as a benchmark for algorithms following centralized training with decentralized execution paradigm. However, with continual advancements in SMAC, many algorithms now exhibit near-optimal performance, complicating the evaluation of their true effectiveness. To alleviate this problem, in this work, we highlight a critical issue: the default opponent policy in these environments lacks sufficient diversity, leading MARL algorithms to overfit and exploit unintended vulnerabilities rather than learning robust strategies. To overcome these limitations, we propose SMAC-HARD, a novel benchmark designed to enhance training robustness and evaluation comprehensiveness. SMAC-HARD supports customizable opponent strategies, randomization of adversarial policies, and interfaces for MARL self-play, enabling agents to generalize to varying opponent behaviors and improve model stability. Furthermore, we introduce a black-box testing framework wherein agents are trained without exposure to the edited opponent scripts but are tested against these scripts to evaluate the policy coverage and adaptability of MARL algorithms. We conduct extensive evaluations of widely used and state-of-the-art algorithms on SMAC-HARD, revealing the substantial challenges posed by edited and mixed strategy opponents. Additionally, the black-box strategy tests illustrate the difficulty of transferring learned policies to unseen adversaries. We envision SMAC-HARD as a critical step toward benchmarking the next generation of MARL algorithms, fostering progress in self-play methods for multi-agent systems. Our code is available at this https URL. 

**Abstract (ZH)**: 具有挑战性的模拟环境的存在对于推动多智能体强化学习（MARL）领域的进步至关重要。在合作MARL场景中，星舰争霸多智能体挑战赛（SMAC）因其集中训练与分散执行的范式，已成为评估算法性能的基准。然而，随着SMAC的不断进步，许多算法现在已表现出接近最优的性能，这使得评估其真正效果变得复杂。为解决这一问题，本研究突出了一项关键问题：这些环境中的默认对手策略缺乏足够的多样性，导致MARL算法过度拟合，并利用非预期的漏洞，而不是学习稳健的策略。为克服这些局限性，我们提出了SMAC-HARD，这是一种新的基准，旨在增强训练的健壮性并提高评估的全面性。SMAC-HARD 支持对手策略的可定制性、对手策略的随机化以及MARL自对战接口，使智能体能够适应不同的对手行为并提高模型的稳定性。此外，我们引入了一个黑盒测试框架，在该框架中，智能体在未接触修改后的对手脚本的情况下接受训练，但在测试过程中使用这些脚本评估MARL算法的策略覆盖率和适应性。我们在SMAC-HARD上对广泛使用和最新的MARL算法进行了详尽的评估，揭示了编辑和混合策略对手带来的重大挑战。此外，黑盒策略测试表明，将学到的策略转移到未知对手的难度很大。我们期望SMAC-HARD成为评估下一代MARL算法的重要一步，推动多智能体系统的自对战方法的发展。我们的代码可在以下链接获取：[此处提供链接]。 

---
# Large Language Model Safety: A Holistic Survey 

**Title (ZH)**: 大型语言模型安全性：一项综合调研 

**Authors**: Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, Deyi Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2412.17686)  

**Abstract**: The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.
This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.
Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at this https URL. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）的快速开发和部署，人工智能领域开启了新的前沿，展现出前所未有的自然语言理解和生成能力。然而，这些模型在关键应用中的日益集成引发了重大安全关切，需要对其潜在风险及其缓解策略进行彻底评估。

本综述全面概述了当前LLM安全的现状，涵盖了四个主要类别：价值不一致、对抗攻击的稳健性、滥用及自主AI风险。除了对这些四个方面缓解方法和技术评估资源的全面回顾外，我们还进一步探讨了四个与LLM安全相关的主题：LLM代理的安全影响、可解释性在增强LLM安全中的作用、多家AI公司和研究机构提出的用于LLM安全的技术路线图，以及旨在促进LLM安全的AI治理，涉及国际合作、政策建议和未来监管方向的讨论。

我们的研究强调了采取积极、多方面的措施对LLM进行安全管理的必要性，注重技术解决方案、伦理考虑以及稳健的治理框架的集成。此综述旨在为学术研究人员、工业从业者和政策制定者提供宝贵的参考资料，帮助他们了解将LLM安全集成到社会所面临的挑战和机遇。最终，它旨在促进LLM的安全和发展，使之与利用人工智能促进社会进步和福祉的总体目标相一致。通过此次综述，我们希望为推进LLM的安全使用做出贡献，从而实现更广泛的应用和社会效益。

相关论文的列表已在以下网址公开：[此httpsURL](此httpsURL)。 

---
# Enhanced Temporal Processing in Spiking Neural Networks for Static Object Detection Using 3D Convolutions 

**Title (ZH)**: 使用三维卷积的脉冲神经网络静态物体检测中的增强时间处理 

**Authors**: Huaxu He  

**Link**: [PDF](https://arxiv.org/pdf/2412.17654)  

**Abstract**: Spiking Neural Networks (SNNs) are a class of network models capable of processing spatiotemporal information, with event-driven characteristics and energy efficiency advantages. Recently, directly trained SNNs have shown potential to match or surpass the performance of traditional Artificial Neural Networks (ANNs) in classification tasks. However, in object detection tasks, directly trained SNNs still exhibit a significant performance gap compared to ANNs when tested on frame-based static object datasets (such as COCO2017). Therefore, bridging this performance gap and enabling directly trained SNNs to achieve performance comparable to ANNs on these static datasets has become one of the key challenges in the development of this http URL address this challenge, this paper focuses on enhancing the SNN's unique ability to process spatiotemporal information. Spiking neurons, as the core components of SNNs, facilitate the exchange of information between different temporal channels during the process of converting input floating-point data into binary spike signals. However, existing neuron models still have certain limitations in the communication of temporal information. Some studies have even suggested that disabling the backpropagation in the time dimension during SNN training can still yield good training results. To improve the SNN handling of temporal information, this paper proposes replacing traditional 2D convolutions with 3D convolutions, thus directly incorporating temporal information into the convolutional process. Additionally, temporal information recurrence mechanism is introduced within the neurons to further enhance the neurons' efficiency in utilizing temporal this http URL results show that the proposed method enables directly trained SNNs to achieve performance levels comparable to ANNs on the COCO2017 and VOC datasets. 

**Abstract (ZH)**: 脉冲神经网络（SNNs）是一类能够处理空间-时间信息的网络模型，具有事件驱动特性和能量效率优势。近年来，直接训练的SNNs在分类任务中展现出了与传统人工神经网络（ANNs）相匹配或超越的潜力。然而，在对象检测任务中，直接训练的SNNs在基于帧的静态对象数据集（如COCO2017）上的测试结果与ANNs相比，仍然存在显著的性能差距。因此，弥合这一性能差距，使直接训练的SNNs在这些静态数据集上的性能达到与ANNs相当的水平，成为了这一领域的关键挑战之一。为了应对这一挑战，本文重点在于增强SNN特有的处理空间-时间信息的能力。作为SNN核心组件的脉冲神经元，有助于在将输入的浮点数据转换为二进制脉冲信号的过程中，实现不同时间通道之间的信息交换。然而，现有的神经元模型在时间信息的传递上仍然存在一定的限制。一些研究表明，在SNN训练过程中禁用时间维度的反向传播仍然能取得良好的训练效果。为了改进SNN处理时间信息的能力，本文提出用3D卷积替代传统的2D卷积，从而直接将时间信息整合到卷积过程之中。此外，本文还在神经元内部引入了时间信息递归机制，进一步提高神经元在利用时间信息方面的效率。实验结果表明，所提出的方法使得直接训练的SNNs在COCO2017和VOC数据集上的性能达到了与ANNs相当的水平。 

---
# Detecting anxiety and depression in dialogues: a multi-label and explainable approach 

**Title (ZH)**: 在对话中检测焦虑和抑郁：一种多标签可解释方法 

**Authors**: Francisco de Arriba-Pérez, Silvia García-Méndez  

**Link**: [PDF](https://arxiv.org/pdf/2412.17651)  

**Abstract**: Anxiety and depression are the most common mental health issues worldwide, affecting a non-negligible part of the population. Accordingly, stakeholders, including governments' health systems, are developing new strategies to promote early detection and prevention from a holistic perspective (i.e., addressing several disorders simultaneously). In this work, an entirely novel system for the multi-label classification of anxiety and depression is proposed. The input data consists of dialogues from user interactions with an assistant chatbot. Another relevant contribution lies in using Large Language Models (LLMs) for feature extraction, provided the complexity and variability of language. The combination of LLMs, given their high capability for language understanding, and Machine Learning (ML) models, provided their contextual knowledge about the classification problem thanks to the labeled data, constitute a promising approach towards mental health assessment. To promote the solution's trustworthiness, reliability, and accountability, explainability descriptions of the model's decision are provided in a graphical dashboard. Experimental results on a real dataset attain 90 % accuracy, improving those in the prior literature. The ultimate objective is to contribute in an accessible and scalable way before formal treatment occurs in the healthcare systems. 

**Abstract (ZH)**: 焦虑和抑郁是全球最常见的心理健康问题，影响着相当一部分人口。因此，包括政府卫生系统在内的利益相关者正在开发新的策略，从整体视角（即同时应对多种疾病）促进早期发现和预防。本文提出了一种全新的多标签分类系统，用于识别焦虑和抑郁。输入数据来源于用户与聊天机器人的交互对话。

另一个重要贡献在于使用大型语言模型（LLMs）进行特征提取，这主要是考虑到语言的复杂性和变异性。结合LLMs的强大语言理解能力以及机器学习（ML）模型通过标注数据获取的关于分类问题的上下文知识，构成了一种有前景的方法，用于心理健康评估。为了提升解决方案的可信度、可靠性和可问责性，在图形仪表板中提供了模型决策的解释性描述。

实验结果表明，该方法在真实数据集上的准确率达到90%，优于之前的文献。最终目标是通过一种可访问且可扩展的方式，在正式治疗前为医疗系统提供帮助。 

---
# An Adaptive Framework for Multi-View Clustering Leveraging Conditional Entropy Optimization 

**Title (ZH)**: 一种基于条件熵优化的自适应多视图聚类框架 

**Authors**: Lijian Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.17647)  

**Abstract**: Multi-view clustering (MVC) has emerged as a powerful technique for extracting valuable insights from data characterized by multiple perspectives or modalities. Despite significant advancements, existing MVC methods struggle with effectively quantifying the consistency and complementarity among views, and are particularly susceptible to the adverse effects of noisy views, known as the Noisy-View Drawback (NVD). To address these challenges, we propose CE-MVC, a novel framework that integrates an adaptive weighting algorithm with a parameter-decoupled deep model. Leveraging the concept of conditional entropy and normalized mutual information, CE-MVC quantitatively assesses and weights the informative contribution of each view, facilitating the construction of robust unified representations. The parameter-decoupled design enables independent processing of each view, effectively mitigating the influence of noise and enhancing overall clustering performance. Extensive experiments demonstrate that CE-MVC outperforms existing approaches, offering a more resilient and accurate solution for multi-view clustering tasks. 

**Abstract (ZH)**: 多视角聚类（Multi-view Clustering, MVC）已成为从具有多个视角或模态的数据中提取有价值见解的一种强大技术。尽管取得了显著进展，现有的MVC方法在有效量化不同视角之间的一致性和互补性方面仍存在困难，并且特别容易受到噪声视角的不利影响，即所谓的噪声视角弊端（Noisy-View Drawback, NVD）。为了解决这些问题，我们提出了一种称为CE-MVC的新型框架，该框架结合了自适应加权算法与参数解耦的深度模型。通过利用条件熵和归一化互信息的概念，CE-MVC定量评估并加权每个视角的信息贡献，从而促进构建稳健的统一表示。参数解耦的设计使每个视角能够独立处理，有效减少了噪声的影响并提升了整体聚类性能。大量实验表明，CE-MVC相较于现有方法提供了更稳健和准确的多视角聚类解决方案。 

---
# Advances in Machine Learning Research Using Knowledge Graphs 

**Title (ZH)**: 使用知识图谱的机器学习研究进展 

**Authors**: Jing Si, Jianfei Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17643)  

**Abstract**: The study uses CSSCI-indexed literature from the China National Knowledge Infrastructure (CNKI) database as the data source. It utilizes the CiteSpace visualization software to draw knowledge graphs on aspects such as institutional collaboration and keyword co-occurrence. This analysis provides insights into the current state of research and emerging trends in the field of machine learning in China. Additionally, it identifies the challenges faced in the field of machine learning research and offers suggestions that could serve as valuable references for future research. 

**Abstract (ZH)**: 本研究使用中国国家知识基础设施（CNKI）数据库中的CSSCI来源文献作为数据来源。利用CiteSpace可视化软件在机构合作和关键词共现等方面绘制知识图谱。该分析提供了对中国机器学习领域当前研究状况和新兴趋势的见解，并识别出该领域面临的挑战，提出了可供未来研究参考的建议。 

---
# ANID: How Far Are We? Evaluating the Discrepancies Between AI-synthesized Images and Natural Images through Multimodal Guidance 

**Title (ZH)**: ANID: 我们走了多远？通过多模态指导评估AI合成图像与真实图像之间的差异 

**Authors**: Renyang Liu, Ziyu Lyu, Wei Zhou, See-Kiong Ng  

**Link**: [PDF](https://arxiv.org/pdf/2412.17632)  

**Abstract**: In the rapidly evolving field of Artificial Intelligence Generated Content (AIGC), one of the key challenges is distinguishing AI-synthesized images from natural images. Despite the remarkable capabilities of advanced AI generative models in producing visually compelling images, significant discrepancies remain when these images are compared to natural ones. To systematically investigate and quantify these discrepancies, we introduce an AI-Natural Image Discrepancy Evaluation benchmark aimed at addressing the critical question: \textit{how far are AI-generated images (AIGIs) from truly realistic images?} We have constructed a large-scale multimodal dataset, the Distinguishing Natural and AI-generated Images (DNAI) dataset, which includes over 440,000 AIGI samples generated by 8 representative models using both unimodal and multimodal prompts, such as Text-to-Image (T2I), Image-to-Image (I2I), and Text \textit{vs.} Image-to-Image (TI2I). Our fine-grained assessment framework provides a comprehensive evaluation of the DNAI dataset across five key dimensions: naive visual feature quality, semantic alignment in multimodal generation, aesthetic appeal, downstream task applicability, and coordinated human validation. Extensive evaluation results highlight significant discrepancies across these dimensions, underscoring the necessity of aligning quantitative metrics with human judgment to achieve a holistic understanding of AI-generated image quality. Code is available at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 在快速发展的生成式人工智能（AIGC）领域，一个关键挑战是区分由AI合成的图像与自然图像。尽管先进的AI生成模型在生成视觉上引人注目的图像方面表现出色，但与自然图像相比，仍然存在显著的差异。为了系统地研究和量化这些差异，我们引入了一个旨在解决关键问题的AI-自然图像差异评估基准：\textit{AI生成的图像（AIGI）与真正真实的图像之间相差多远？}我们构建了一个大规模的多模态数据集，即用于区分自然图像与AI生成图像（DNAI）的数据集，该数据集包含了超过440,000个由8个代表性模型生成的AIGI样本，使用了包括文本到图像（T2I）、图像到图像（I2I）以及文本与图像到图像（TI2I）等单模态和多模态提示。我们的精细评估框架从五个关键维度对DNAI数据集进行了全面评价：直观的视觉特征质量、多模态生成中的语义对齐、审美吸引力、下游任务适用性以及协调的人类验证。广泛的评估结果突显出了这些维度上的显著差异，强调了将定量指标与人类判断相结合的必要性，以实现对AI生成图像质量的全面理解。项目代码可在\href{此处插入网址}{此处插入网址}获得。 

---
# Facial Expression Analysis and Its Potentials in IoT Systems: A Contemporary Survey 

**Title (ZH)**: 面部表情分析及其在物联网系统中的潜在应用：一项当代综述 

**Authors**: Zixuan Shanggua, Yanjie Dong, Song Guo, Victor C. M. Leung, M. Jamal Deen, Xiping Hu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17616)  

**Abstract**: Facial expressions convey human emotions and can be categorized into macro-expressions (MaEs) and micro-expressions (MiEs) based on duration and intensity. While MaEs are voluntary and easily recognized, MiEs are involuntary, rapid, and can reveal concealed emotions. The integration of facial expression analysis with Internet-of-Thing (IoT) systems has significant potential across diverse scenarios. IoT-enhanced MaE analysis enables real-time monitoring of patient emotions, facilitating improved mental health care in smart healthcare. Similarly, IoT-based MiE detection enhances surveillance accuracy and threat detection in smart security. This work aims at providing a comprehensive overview of research progress in facial expression analysis and explores its integration with IoT systems. We discuss the distinctions between our work and existing surveys, elaborate on advancements in MaE and MiE techniques across various learning paradigms, and examine their potential applications in IoT. We highlight challenges and future directions for the convergence of facial expression-based technologies and IoT systems, aiming to foster innovation in this domain. By presenting recent developments and practical applications, this study offers a systematic understanding of how facial expression analysis can enhance IoT systems in healthcare, security, and beyond. 

**Abstract (ZH)**: 面部表情传达人类情感，可以根据持续时间和强度被分类为宏观表情（MaEs）和微观表情（MiEs）。虽然宏观表情是自愿的且容易识别，微观表情则为非自愿的、快速的，并能揭示隐藏的情感。将面部表情分析与物联网（IoT）系统结合具有广泛的潜在应用场景。基于物联网的宏观表情增强分析能够实现患者情感的实时监控，从而在智能医疗中提升心理健康护理的质量。同样，基于物联网的微观表情检测可以提高智能安全性中的监控准确性和威胁检测能力。本文旨在提供面部表情分析研究进展的全面概述，并探讨其与物联网系统的整合。我们讨论了本文与现有综述的区别，阐述了在各种学习范式下宏观表情和微观表情技术的进步，并考察了这些技术在物联网中的潜在应用。我们还强调了面部表情技术与物联网系统融合过程中面临的挑战和未来发展方向，旨在促进该领域的创新。通过展示近期发展和实际应用，本研究提供了系统性的理解，即面部表情分析如何提升物联网系统在医疗、安全及其他领域的效能。 

---
# PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World 

**Title (ZH)**: PC代理：在你睡眠之时，AI在工作——认知之旅走进数字世界 

**Authors**: Yanheng He, Jiahe Jin, Shijie Xia, Jiadi Su, Runze Fan, Haoyang Zou, Xiangkun Hu, Pengfei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17589)  

**Abstract**: Imagine a world where AI can handle your work while you sleep - organizing your research materials, drafting a report, or creating a presentation you need for tomorrow. However, while current digital agents can perform simple tasks, they are far from capable of handling the complex real-world work that humans routinely perform. We present PC Agent, an AI system that demonstrates a crucial step toward this vision through human cognition transfer. Our key insight is that the path from executing simple "tasks" to handling complex "work" lies in efficiently capturing and learning from human cognitive processes during computer use. To validate this hypothesis, we introduce three key innovations: (1) PC Tracker, a lightweight infrastructure that efficiently collects high-quality human-computer interaction trajectories with complete cognitive context; (2) a two-stage cognition completion pipeline that transforms raw interaction data into rich cognitive trajectories by completing action semantics and thought processes; and (3) a multi-agent system combining a planning agent for decision-making with a grounding agent for robust visual grounding. Our preliminary experiments in PowerPoint presentation creation reveal that complex digital work capabilities can be achieved with a small amount of high-quality cognitive data - PC Agent, trained on just 133 cognitive trajectories, can handle sophisticated work scenarios involving up to 50 steps across multiple applications. This demonstrates the data efficiency of our approach, highlighting that the key to training capable digital agents lies in collecting human cognitive data. By open-sourcing our complete framework, including the data collection infrastructure and cognition completion methods, we aim to lower the barriers for the research community to develop truly capable digital agents. 

**Abstract (ZH)**: 想象一个世界，在这里AI可以在你睡觉时完成你的工作，比如整理研究资料、起草报告或创建明天需要的演示文稿。然而，尽管当前的数字代理可以处理一些简单的任务，但它们尚不具备处理人类日常执行的复杂工作能力。我们提出了一种名为PC Agent的AI系统，它通过人类认知转移展示了向这一愿景迈进的关键一步。我们的核心见解是，从执行简单的“任务”到处理复杂的“工作”，关键在于有效捕捉和学习人类在使用计算机过程中的认知过程。为了验证这一假设，我们引入了三项关键创新：(1) PC Tracker，一种轻量级的基础设施，能够高效地收集包含完整认知背景的高质量人机交互轨迹；(2) 两阶段认知完成流水线，该流水线将原始交互数据转化为丰富的认知轨迹，通过完成动作语义和思维过程来实现这一目标；(3) 结合决策规划代理和视觉锚定代理的多代理系统。我们初步在PowerPoint演示文稿创建方面的实验表明，只需少量高质量的认知数据即可实现复杂的数字工作能力——PC Agent仅基于133个认知轨迹训练，即可处理涉及多个应用程序并跨越多达50个步骤的复杂工作场景。这证明了我们方法的数据效率，强调了训练有素的数字代理的关键在于收集人类认知数据。通过开源我们的完整框架，包括数据采集基础设施和认知完成方法，我们旨在降低研究社区开发真正有能力建立数字代理的门槛。 

---
# Retention Score: Quantifying Jailbreak Risks for Vision Language Models 

**Title (ZH)**: 越狱风险量化：视觉语言模型的保留分数 

**Authors**: Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho  

**Link**: [PDF](https://arxiv.org/pdf/2412.17544)  

**Abstract**: The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has also made VLMs vulnerable to sophisticated adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLM's ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the \textbf{Retention Score}. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA. 

**Abstract (ZH)**: 视觉-语言模型（VLMs）的出现是将计算机视觉与大型语言模型（LLMs）相结合以增强多模态机器学习能力的重要进展。然而，这一进展也使VLMs容易遭受复杂的对抗性攻击，引发了对其可靠性的担忧。本文旨在评估VLMs在面对可能破坏模型安全合规并导致有害输出的越狱攻击时的抗攻击能力。为了评估VLMs在对抗性输入扰动下的稳定性，我们提出了一种新的度量标准，称为**保持分值**。保持分值是一个多模态评估指标，包括视觉组件和文本组件中越狱风险的保持-I和保持-T分数。我们的过程包括使用条件扩散模型生成合成的图像-文本对。这些对随后由VLM和有毒性评分判断分类器进行预测。通过计算毒性评分的差距，我们可以以攻击无关的方式定量评估VLM的稳健性。我们的工作有四个主要贡献。首先，我们证明了保持分值可以作为认证稳健性度量标准。其次，我们证明了具有视觉组件的大多数VLMs在面对越狱攻击时的稳健性低于相应的纯VLMs。此外，我们评估了黑盒VLM API，并发现Google Gemini的安全设置显著影响分数和稳健性。同时，GPT4V的稳健性与Gemini的中等设置相似。最后，我们的方法为现有的对抗性攻击方法提供了一种时间效率更高的替代方案，并能够在评估MiniGPT-4、InstructBLIP和LLaVA等VLM时提供一致的模型稳健性排名。 

---
# Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models 

**Title (ZH)**: 增强癌症诊断的可解释且可信赖的深度学习模型 

**Authors**: Badaru I. Olumuyiwa, Anh Han, Zia U. Shamszaman  

**Link**: [PDF](https://arxiv.org/pdf/2412.17527)  

**Abstract**: This research presents an innovative approach to cancer diagnosis and prediction using explainable Artificial Intelligence (XAI) and deep learning techniques. With cancer causing nearly 10 million deaths globally in 2020, early and accurate diagnosis is crucial. Traditional methods often face challenges in cost, accuracy, and efficiency. Our study develops an AI model that provides precise outcomes and clear insights into its decision-making process, addressing the "black box" problem of deep learning models. By employing XAI techniques, we enhance interpretability and transparency, building trust among healthcare professionals and patients. Our approach leverages neural networks to analyse extensive datasets, identifying patterns for cancer detection. This model has the potential to revolutionise diagnosis by improving accuracy, accessibility, and clarity in medical decision-making, possibly leading to earlier detection and more personalised treatment strategies. Furthermore, it could democratise access to high-quality diagnostics, particularly in resource-limited settings, contributing to global health equity. The model's applications extend beyond cancer diagnosis, potentially transforming various aspects of medical decision-making and saving millions of lives worldwide. 

**Abstract (ZH)**: 本研究提出了一种使用可解释人工智能（XAI）和深度学习技术进行癌症诊断和预测的创新方法。鉴于2020年全球近1000万人因癌症死亡，早期和准确的诊断至关重要。传统方法在成本、准确性和效率方面常常面临挑战。本研究开发了一种AI模型，不仅提供精确的结果，还能清晰地解释其决策过程，从而解决深度学习模型的“黑箱”问题。通过采用XAI技术，我们增强了模型的可解释性和透明度，在医疗专业人员和患者中建立信任。我们的方法利用神经网络分析大量数据集，识别癌症检测的模式。该模型有潜力通过提高准确度、可访问性和决策清晰度来革新诊断方法，可能有助于早期发现并制定更个性化的治疗策略。此外，它还有可能通过对世界各地资源有限地区高质量诊断服务的普及，促进全球健康公平。该模型的应用不仅限于癌症诊断，还可能从根本上改变医疗决策的多个方面，并拯救数百万生命。 

---
# STAHGNet: Modeling Hybrid-grained Heterogenous Dependency Efficiently for Traffic Prediction 

**Title (ZH)**: STAHGNet：高效建模混合粒度异构依赖性以进行交通预测 

**Authors**: Jiyao Wang, Zehua Peng, Yijia Zhang, Dengbo He, Lei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17524)  

**Abstract**: Traffic flow prediction plays a critical role in the intelligent transportation system, and it is also a challenging task because of the underlying complex Spatio-temporal patterns and heterogeneities evolving across time. However, most present works mostly concentrate on solely capturing Spatial-temporal dependency or extracting implicit similarity graphs, but the hybrid-granularity evolution is ignored in their modeling process. In this paper, we proposed a novel data-driven end-to-end framework, named Spatio-Temporal Aware Hybrid Graph Network (STAHGNet), to couple the hybrid-grained heterogeneous correlations in series simultaneously through an elaborately Hybrid Graph Attention Module (HGAT) and Coarse-granularity Temporal Graph (CTG) generator. Furthermore, an automotive feature engineering with domain knowledge and a random neighbor sampling strategy is utilized to improve efficiency and reduce computational complexity. The MAE, RMSE, and MAPE are used for evaluation metrics. Tested on four real-life datasets, our proposal outperforms eight classical baselines and four state-of-the-art (SOTA) methods (e.g., MAE 14.82 on PeMSD3; MAE 18.92 on PeMSD4). Besides, extensive experiments and visualizations verify the effectiveness of each component in STAHGNet. In terms of computational cost, STAHGNet saves at least four times the space compared to the previous SOTA models. The proposed model will be beneficial for more efficient TFP as well as intelligent transport system construction. 

**Abstract (ZH)**: 交通流预测在智能交通系统中发挥着关键作用，但由于其潜在的时空复杂模式和随时间演化的一致性变化，这是一项具有挑战性的任务。然而，当前大多数研究主要集中在单纯捕获空间和时间依赖性或提取隐式的相似图上，而在建模过程中忽略了混合粒度的演化。本文提出了一种全新的数据驱动的端到端框架，名为时空感知混合图网络（STAHGNet），该框架通过精细和粗糙粒度的混合图注意模块（HGAT）和粗粒度时空图（CTG）生成器同时耦合了序列中的混合粒度异构关系。此外，利用领域知识进行汽车特征工程和随机邻居采样策略以提高效率并减少计算复杂度。使用MAE、RMSE和MAPE作为评价指标。在四个实际数据集上进行实验，证明了我们的方法优于八种经典基线和四种最先进的方法（如PeMSD3上的MAE 14.82；PeMSD4上的MAE 18.92）。此外，大量的实验和可视化结果验证了STAHGNet中每个组件的有效性。在计算成本方面，STAHGNet相比上一代最先进的模型至少节省了四倍的空间。提出的模型将有助于提高交通流预测的效率以及智能交通系统的构建。 

---
# DeepMF: Deep Motion Factorization for Closed-Loop Safety-Critical Driving Scenario Simulation 

**Title (ZH)**: DeepMF：闭环安全关键驾驶场景模拟中的深度运动分解 

**Authors**: Yizhe Li, Linrui Zhang, Xueqian Wang, Houde Liu, Bin Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17487)  

**Abstract**: Safety-critical traffic scenarios are of great practical relevance to evaluating the robustness of autonomous driving (AD) systems. Given that these long-tail events are extremely rare in real-world traffic data, there is a growing body of work dedicated to the automatic traffic scenario generation. However, nearly all existing algorithms for generating safety-critical scenarios rely on snippets of previously recorded traffic events, transforming normal traffic flow into accident-prone situations directly. In other words, safety-critical traffic scenario generation is hindsight and not applicable to newly encountered and open-ended traffic this http URL this paper, we propose the Deep Motion Factorization (DeepMF) framework, which extends static safety-critical driving scenario generation to closed-loop and interactive adversarial traffic simulation. DeepMF casts safety-critical traffic simulation as a Bayesian factorization that includes the assignment of hazardous traffic participants, the motion prediction of selected opponents, the reaction estimation of autonomous vehicle (AV) and the probability estimation of the accident occur. All the aforementioned terms are calculated using decoupled deep neural networks, with inputs limited to the current observation and historical states. Consequently, DeepMF can effectively and efficiently simulate safety-critical traffic scenarios at any triggered time and for any duration by maximizing the compounded posterior probability of traffic risk. Extensive experiments demonstrate that DeepMF excels in terms of risk management, flexibility, and diversity, showcasing outstanding performance in simulating a wide range of realistic, high-risk traffic scenarios. 

**Abstract (ZH)**: 安全性关键的交通场景对评估自主驾驶（AD）系统的鲁棒性具有重要的实际意义。鉴于这些罕见事件在实际交通数据中极为稀少，已出现了大量关于自动交通场景生成的研究工作。然而，目前几乎所有的生成安全性关键场景的算法都依赖于之前记录的交通事件片段，直接将正常的交通流转变为事故易发情况。换句话说，现有的安全性关键交通场景生成方法是事后诸葛亮式的，并不适用于新遇到的开放性交通场景。

为了解决这个问题，在这篇论文中，我们提出了深度运动分解（DeepMF）框架，该框架将静态安全性关键驾驶场景生成拓展到了闭环和交互式对抗性交通模拟。DeepMF 将安全性关键交通模拟视为贝叶斯分解，其中包括对危险交通参与者的分配、所选对手的运动预测、自动驾驶车辆（AV）的反应估计，以及事故发生的概率估计。上述所有术语都是通过解耦的深度神经网络计算得出的，其输入仅限于当前观察和历史状态。因此，DeepMF 可以通过最大化交通风险的复合后验概率，有效地和高效地在触发的任意时间和任意持续时间内模拟安全性关键的交通场景。广泛的实验表明，DeepMF 在风险管理和灵活性、多样性方面表现出色，能够模拟各种真实且高风险的交通场景，显示出出色的表现。 

---
# Developmental Predictive Coding Model for Early Infancy Mono and Bilingual Vocal Continual Learning 

**Title (ZH)**: 早期婴儿单双语连续语音学习的发育预测编码模型 

**Authors**: Xiaodan Chen, Alexandre Pitti, Mathias Quoy, Nancy F Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17456)  

**Abstract**: Understanding how infants perceive speech sounds and language structures is still an open problem. Previous research in artificial neural networks has mainly focused on large dataset-dependent generative models, aiming to replicate language-related phenomena such as ''perceptual narrowing''. In this paper, we propose a novel approach using a small-sized generative neural network equipped with a continual learning mechanism based on predictive coding for mono-and bilingual speech sound learning (referred to as language sound acquisition during ''critical period'') and a compositional optimization mechanism for generation where no learning is involved (later infancy sound imitation). Our model prioritizes interpretability and demonstrates the advantages of online learning: Unlike deep networks requiring substantial offline training, our model continuously updates with new data, making it adaptable and responsive to changing inputs. Through experiments, we demonstrate that if second language acquisition occurs during later infancy, the challenges associated with learning a foreign language after the critical period amplify, replicating the perceptual narrowing effect. 

**Abstract (ZH)**: 理解婴儿如何感知语音声音和语言结构仍然是一个开放的问题。之前在人工神经网络领域的研究主要集中在依赖大规模数据集的生成模型上，旨在重现诸如“感知狭窄效应”等语言相关现象。本文提出了一种新的方法，使用一个小型生成神经网络，结合基于预测编码的持续学习机制，用于单语和双语语音声音学习（在语言学习的关键期内称为“语言声音获取”）以及一个组合优化机制用于生成过程（在后者中没有涉及学习，即后来婴儿期的声音模仿）。我们的模型强调可解释性，并展示了在线学习的优势：与需要大量离线训练的深网络不同，我们的模型能够不断更新以适应新的数据，使其具备较强的适应性和响应性。通过实验，我们展示，如果在婴儿后期学习第二语言，与关键期内学习外国语言相比，产生的挑战更加突出，复制了感知狭窄效应。 

---
# The Role of XAI in Transforming Aeronautics and Aerospace Systems 

**Title (ZH)**: XAI在转型航空航天系统中的作用 

**Authors**: Francisco Javier Cantero Zorita, Mikel Galafate, Javier M. Moguerza, Isaac Martín de Diego, M. Teresa Gonzalez, Gema Gutierrez Peña  

**Link**: [PDF](https://arxiv.org/pdf/2412.17440)  

**Abstract**: Recent advancements in Artificial Intelligence (AI) have transformed decision-making in aeronautics and aerospace. These advancements in AI have brought with them the need to understand the reasons behind the predictions generated by AI systems and models, particularly by professionals in these sectors. In this context, the emergence of eXplainable Artificial Intelligence (XAI) has helped bridge the gap between professionals in the aeronautical and aerospace sectors and the AI systems and models they work with. For this reason, this paper provides a review of the concept of XAI is carried out defining the term and the objectives it aims to achieve. Additionally, the paper discusses the types of models defined within it and the properties these models must fulfill to be considered transparent, as well as the post-hoc techniques used to understand AI systems and models after their training. Finally, various application areas within the aeronautical and aerospace sectors will be presented, highlighting how XAI is used in these fields to help professionals understand the functioning of AI systems and models. 

**Abstract (ZH)**: 近年来，人工智能（AI）的进展已经改变了航空和航天领域的决策过程。这些AI进展伴随着对理解AI系统和模型生成预测背后原因的需求，尤其是对这些行业的专业人士来说。在此背景下，可解释的人工智能（XAI）的出现帮助缩小了航空和航天领域专业人士与他们使用的AI系统和模型之间的差距。因此，本文旨在重新审视XAI的概念，定义其术语及其目标。此外，本文还将讨论在XAI中定义的模型类型，以及这些模型必须具备的透明性特征。最后，本文还将探讨航空和航天领域内的各种应用领域，强调XAI在这些领域的应用如何帮助专业人士理解AI系统和模型的功能。 

---
# Markov Process-Based Graph Convolutional Networks for Entity Classification in Knowledge Graphs 

**Title (ZH)**: 基于马尔可夫过程的图卷积网络在知识图谱实体分类中的应用 

**Authors**: Johannes Mäkelburg, Yiwen Peng, Mehwish Alam, Tobias Weller, Maribel Acosta  

**Link**: [PDF](https://arxiv.org/pdf/2412.17438)  

**Abstract**: Despite the vast amount of information encoded in Knowledge Graphs (KGs), information about the class affiliation of entities remains often incomplete. Graph Convolutional Networks (GCNs) have been shown to be effective predictors of complete information about the class affiliation of entities in KGs. However, these models do not learn the class affiliation of entities in KGs incorporating the complexity of the task, which negatively affects the models prediction capabilities. To address this problem, we introduce a Markov process-based architecture into well-known GCN architectures. This end-to-end network learns the prediction of class affiliation of entities in KGs within a Markov process. The number of computational steps is learned during training using a geometric distribution. At the same time, the loss function combines insights from the field of evidential learning. The experiments show a performance improvement over existing models in several studied architectures and datasets. Based on the chosen hyperparameters for the geometric distribution, the expected number of computation steps can be adjusted to improve efficiency and accuracy during training. 

**Abstract (ZH)**: 尽管知识图谱（KGs）中存储了大量的信息，但实体的类归属信息往往仍不完整。图卷积网络（GCNs）已被证明是预测KG中实体类归属信息的有效工具。然而，这些模型未能在考虑任务复杂性的情况下学习实体的类归属信息，这会负面影响模型的预测能力。为解决这一问题，我们引入了基于马尔可夫过程的架构，将其整合到现有的GCN架构中。该端到端网络在一个马尔可夫过程内学习KG中实体的类归属预测。计算步骤的数量是通过在训练过程中使用几何分布来学习的。同时，损失函数结合了证据学习领域的见解。实验结果显示，该模型在多种研究架构和数据集上优于现有模型。通过选择几何分布的超参数，可以调整预期的计算步骤数量，以提高训练中的效率和准确性。 

---
# BrainMAP: Learning Multiple Activation Pathways in Brain Networks 

**Title (ZH)**: 脑网络中的多激活路径学习：BrainMAP 

**Authors**: Song Wang, Zhenyu Lei, Zhen Tan, Jiaqi Ding, Xinyu Zhao, Yushun Dong, Guorong Wu, Tianlong Chen, Chen Chen, Aiying Zhang, Jundong Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.17404)  

**Abstract**: Functional Magnetic Resonance Image (fMRI) is commonly employed to study human brain activity, since it offers insight into the relationship between functional fluctuations and human behavior. To enhance analysis and comprehension of brain activity, Graph Neural Networks (GNNs) have been widely applied to the analysis of functional connectivities (FC) derived from fMRI data, due to their ability to capture the synergistic interactions among brain regions. However, in the human brain, performing complex tasks typically involves the activation of certain pathways, which could be represented as paths across graphs. As such, conventional GNNs struggle to learn from these pathways due to the long-range dependencies of multiple pathways. To address these challenges, we introduce a novel framework BrainMAP to learn Multiple Activation Pathways in Brain networks. BrainMAP leverages sequential models to identify long-range correlations among sequentialized brain regions and incorporates an aggregation module based on Mixture of Experts (MoE) to learn from multiple pathways. Our comprehensive experiments highlight BrainMAP's superior performance. Furthermore, our framework enables explanatory analyses of crucial brain regions involved in tasks. Our code is provided at this https URL. 

**Abstract (ZH)**: 功能性磁共振成像（fMRI）常被用来研究人类大脑活动，因为它能揭示功能波动与人类行为之间的关系。为了增强对大脑活动的分析和理解，图形神经网络（GNNs）已被广泛应用于fMRI数据中功能连接（FC）的分析，这是因为GNNs能够捕捉脑区之间的协同交互作用。然而，在人类大脑中，执行复杂任务通常会激活某些特定路径，这些路径可以用图上的路径来表示。因此，传统的GNNs难以从这些路径中学习，因为它们需要处理多路径的长程依赖性。为了解决这些问题，我们提出了一种新的框架BrainMAP来学习大脑网络中的多种激活路径。BrainMAP利用序列模型来识别序列化脑区之间的长程相关性，并结合基于专家混合（Mixture of Experts, MoE）的聚合模块来从多种路径中学习。我们的全面实验强调了BrainMAP的优越性能。此外，我们的框架还能够解释与任务相关的关键脑区。我们的代码在此处提供：[提供链接的地方]。 

---
# FRTP: Federating Route Search Records to Enhance Long-term Traffic Prediction 

**Title (ZH)**: FRTP：聚合路线搜索记录以增强长期交通预测 

**Authors**: Hangli Ge, Xiaojie Yang, Itsuki Matsunaga, Dizhi Huang, Noboru Koshizuka  

**Link**: [PDF](https://arxiv.org/pdf/2412.17373)  

**Abstract**: Accurate traffic prediction, especially predicting traffic conditions several days in advance is essential for intelligent transportation systems (ITS). Such predictions enable mid- and long-term traffic optimization, which is crucial for efficient transportation planning. However, the inclusion of diverse external features, alongside the complexities of spatial relationships and temporal uncertainties, significantly increases the complexity of forecasting models. Additionally, traditional approaches have handled data preprocessing separately from the learning model, leading to inefficiencies caused by repeated trials of preprocessing and training. In this study, we propose a federated architecture capable of learning directly from raw data with varying features and time granularities or lengths. The model adopts a unified design that accommodates different feature types, time scales, and temporal periods. Our experiments focus on federating route search records and begin by processing raw data within the model framework. Unlike traditional models, this approach integrates the data federation phase into the learning process, enabling compatibility with various time frequencies and input/output configurations. The accuracy of the proposed model is demonstrated through evaluations using diverse learning patterns and parameter settings. The results show that online search log data is useful for forecasting long-term traffic, highlighting the model's adaptability and efficiency. 

**Abstract (ZH)**: 准确的交通预测，尤其是提前几天预测交通状况，在智能交通系统（ITS）中至关重要。这样的预测能够实现中期和长期的交通优化，对于高效的交通规划至关重要。然而，结合多种外部特征、空间关系的复杂性以及时间不确定性，显著增加了预测模型的复杂性。此外，传统的做法将数据预处理与学习模型分离，导致在预处理和训练过程中反复进行，带来效率低下。在本研究中，我们提出了一种联邦架构，能够直接从具有不同特征和时间粒度或长度的原始数据中学习。该模型采用了统一设计，能够适应不同的特征类型、时间尺度和时间周期。我们的实验集中在联邦路由搜索记录上，并首先在模型框架内处理原始数据。与传统的模型不同，该方法将数据联邦阶段整合到学习过程中，使得模型能够兼容各种时间频率和输入/输出配置。通过对不同学习模式和参数设置进行评估，证明了所提出模型的准确性和有效性。实验结果表明，基于在线搜索日志数据的交通预测是有效的，突出了模型的适应性和效率。 

---
# MineAgent: Towards Remote-Sensing Mineral Exploration with Multimodal Large Language Models 

**Title (ZH)**: MineAgent：迈向基于多模态大型语言模型的遥感矿产勘探 

**Authors**: Beibei Yu, Tao Shen, Hongbin Na, Ling Chen, Denqi Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.17339)  

**Abstract**: Remote-sensing mineral exploration is critical for identifying economically viable mineral deposits, yet it poses significant challenges for multimodal large language models (MLLMs). These include limitations in domain-specific geological knowledge and difficulties in reasoning across multiple remote-sensing images, further exacerbating long-context issues. To address these, we present MineAgent, a modular framework leveraging hierarchical judging and decision-making modules to improve multi-image reasoning and spatial-spectral integration. Complementing this, we propose MineBench, a benchmark specific for evaluating MLLMs in domain-specific mineral exploration tasks using geological and hyperspectral data. Extensive experiments demonstrate the effectiveness of MineAgent, highlighting its potential to advance MLLMs in remote-sensing mineral exploration. 

**Abstract (ZH)**: 遥感矿物勘探对于识别经济可行的矿床至关重要，但对多模态大型语言模型（MLLMs）提出了重大挑战。这些挑战包括在地质专业知识方面的局限性和在多张遥感图像之间进行推理的困难，进一步加剧了长上下文问题。为了解决这些问题，我们提出了一种名为MineAgent的模块化框架，该框架利用分层判断和决策模块以提高多图像推理能力和空间光谱集成。此外，我们还提出了MineBench基准，该基准专门用于评估MLLMs在特定地质背景下的矿物勘探任务，使用的是地质和高光谱数据。大量的实验表明MineAgent的有效性，突显了其在遥感矿物勘探领域推进MLLMs的潜力。 

---
# Enhancing Topic Interpretability for Neural Topic Modeling through Topic-wise Contrastive Learning 

**Title (ZH)**: 通过主题层面对比学习增强神经主题模型的主题可解释性 

**Authors**: Xin Gao, Yang Lin, Ruiqing Li, Yasha Wang, Xu Chu, Xinyu Ma, Hailong Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17338)  

**Abstract**: Data mining and knowledge discovery are essential aspects of extracting valuable insights from vast datasets. Neural topic models (NTMs) have emerged as a valuable unsupervised tool in this field. However, the predominant objective in NTMs, which aims to discover topics maximizing data likelihood, often lacks alignment with the central goals of data mining and knowledge discovery which is to reveal interpretable insights from large data repositories. Overemphasizing likelihood maximization without incorporating topic regularization can lead to an overly expansive latent space for topic modeling. In this paper, we present an innovative approach to NTMs that addresses this misalignment by introducing contrastive learning measures to assess topic interpretability. We propose a novel NTM framework, named ContraTopic, that integrates a differentiable regularizer capable of evaluating multiple facets of topic interpretability throughout the training process. Our regularizer adopts a unique topic-wise contrastive methodology, fostering both internal coherence within topics and clear external distinctions among them. Comprehensive experiments conducted on three diverse datasets demonstrate that our approach consistently produces topics with superior interpretability compared to state-of-the-art NTMs. 

**Abstract (ZH)**: 数据挖掘和知识发现是从海量数据集中提取有价值洞见的核心方面。神经主题模型（NTMs）作为这一领域中一种有价值的无监督工具而崭露头角。然而，在NTMs中的主要目标是发现最大化数据似然性的主题，往往与数据挖掘和知识发现的核心目标——从大量数据仓库中揭示可解释的洞见——存在不一致。单纯过分强调似然性最大化，而不整合主题正则化，可能导致主题建模的空间过度扩展。本文提出了一个创新性的NTMs方法，通过引入对比学习措施来评估主题的解释性，从而解决这一不一致。我们提出了一种新颖的NTM框架，名为ContraTopic，该框架集成了一个可微正则化器，能够在训练过程中评估主题解释性的多个方面。我们的正则化器采用了一种独特的按主题的对比方法，促进了主题内的内部一致性以及主题间的外部清晰区分。在三个不同数据集上进行的全面实验表明，我们的方法在解释性方面始终优于最先进的NTMs。 

---
# Complete Implementation of WXF Chinese Chess Rules 

**Title (ZH)**: WXF中国象棋规则的完整实现 

**Authors**: Daniel Tan, Neftali Watkinson Medina  

**Link**: [PDF](https://arxiv.org/pdf/2412.17334)  

**Abstract**: Unlike repetitions in Western Chess where all repetitions are draws, repetitions in Chinese Chess could result in a win, draw, or loss depending on the kind of repetition being made by both players. One of the biggest hurdles facing Chinese Chess application development is a proper system for judging games correctly. This paper introduces a complete algorithm for ruling the WXF rules correctly in all 110 example cases found in the WXF manual. We introduce several novel optimizations for speeding up the repetition handling without compromising the program correctness. This algorithm is usable in engines, and we saw a total increase in playing strength by +10 point rating increase, or an increased 5% winrate when integrating this approach into our prototype engine. 

**Abstract (ZH)**: 与西方国际象棋中所有重复局面均为和局不同，中国象棋中的重复局面可能会导致胜利、和局或失败，具体结果取决于双方所作的重复局面类型。中国象棋应用程序开发面临的最大挑战之一是能够正确判断比赛结果的系统。本文介绍了一种完整的算法，可以在WXF手册中发现的110个示例案例中正确裁定WXF规则。我们还介绍了几种新颖的优化方法，用于加速重复局面处理，同时不会影响程序的正确性。该算法可用于引擎中，并在将此方法整合到我们的原型引擎中时，我们看到了总体棋力提升10个等级分或胜率增加5%的效果。 

---
# On the Feasibility of Vision-Language Models for Time-Series Classification 

**Title (ZH)**: 视觉-语言模型在时间序列分类中的可行性研究 

**Authors**: Vinay Prithyani, Mohsin Mohammed, Richa Gadgil, Ricardo Buitrago, Vinija Jain, Aman Chadha  

**Link**: [PDF](https://arxiv.org/pdf/2412.17304)  

**Abstract**: We build upon time-series classification by leveraging the capabilities of Vision Language Models (VLMs). We find that VLMs produce competitive results after two or less epochs of fine-tuning. We develop a novel approach that incorporates graphical data representations as images in conjunction with numerical data. This approach is rooted in the hypothesis that graphical representations can provide additional contextual information that numerical data alone may not capture. Additionally, providing a graphical representation can circumvent issues such as limited context length faced by LLMs. To further advance this work, we implemented a scalable end-to-end pipeline for training on different scenarios, allowing us to isolate the most effective strategies for transferring learning capabilities from LLMs to Time Series Classification (TSC) tasks. Our approach works with univariate and multivariate time-series data. In addition, we conduct extensive and practical experiments to show how this approach works for time-series classification and generative labels. 

**Abstract (ZH)**: 我们基于时间序列分类任务，利用视觉语言模型（VLMs）的能力进行建模。研究发现，VLMs在两轮或更少的微调后能够取得具有竞争力的结果。我们提出了一种创新的方法，通过将图形数据表示作为图像与数值数据结合，从而构建模型。该方法基于假设，图形表示能够提供额外的上下文信息，这些信息仅凭数值数据可能无法捕捉到。此外，提供图形表示可以规避大型语言模型（LLMs）面临的上下文长度限制等问题。为进一步推进这项工作，我们实现了一种可扩展的端到端管道，用于不同场景下的训练，这使我们能够隔离最有效的策略，以将LLMs的学****能力转移到时间序列分类（TSC）任务中。我们的方法适用于单变量和多变量时间序列数据。此外，我们通过广泛的实践实验展示了这种方法如何应用于时间序列分类和生成标签任务。 

---
# LLM4AD: A Platform for Algorithm Design with Large Language Model 

**Title (ZH)**: LLM4AD：一种基于大规模语言模型的算法设计平台 

**Authors**: Fei Liu, Rui Zhang, Zhuoliang Xie, Rui Sun, Kai Li, Xi Lin, Zhenkun Wang, Zhichao Lu, Qingfu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17287)  

**Abstract**: We introduce LLM4AD, a unified Python platform for algorithm design (AD) with large language models (LLMs). LLM4AD is a generic framework with modularized blocks for search methods, algorithm design tasks, and LLM interface. The platform integrates numerous key methods and supports a wide range of algorithm design tasks across various domains including optimization, machine learning, and scientific discovery. We have also designed a unified evaluation sandbox to ensure a secure and robust assessment of algorithms. Additionally, we have compiled a comprehensive suite of support resources, including tutorials, examples, a user manual, online resources, and a dedicated graphical user interface (GUI) to enhance the usage of LLM4AD. We believe this platform will serve as a valuable tool for fostering future development in the merging research direction of LLM-assisted algorithm design. 

**Abstract (ZH)**: 我们将介绍LLM4AD，这是一个基于大型语言模型（LLMs）的统一Python平台，用于算法设计（AD）。LLM4AD 是一个通用框架，包含模块化的搜索方法、算法设计任务和LLM接口块。该平台集成了众多关键方法，并支持各种领域的广泛算法设计任务，包括优化、机器学习和科学发现。我们还设计了一个统一的评估沙盒，以确保对算法进行安全和稳健的评估。此外，我们编制了一整套支持资源，包括教程、示例、用户手册、在线资源和一个专用的图形用户界面（GUI），以增强LLM4AD的使用体验。我们相信，这一平台将为LLM辅助算法设计这一交叉研究方向的未来发展提供宝贵的工具。 

---
# B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners 

**Title (ZH)**: B-STaR：自我 Teaching推理者中的探索与利用监控与平衡 

**Authors**: Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, Junxian He  

**Link**: [PDF](https://arxiv.org/pdf/2412.17256)  

**Abstract**: In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. However, the critical factors underlying the mechanism of these iterative self-improving methods remain poorly understood, such as under what conditions self-improvement is effective, and what are the bottlenecks in the current iterations. In this work, we identify and propose methods to monitor two pivotal factors in this iterative process: (1) the model's ability to generate sufficiently diverse responses (exploration); and (2) the effectiveness of external rewards in distinguishing high-quality candidates from lower-quality ones (exploitation). Using mathematical reasoning as a case study, we begin with a quantitative analysis to track the dynamics of exploration and exploitation, discovering that a model's exploratory capabilities rapidly deteriorate over iterations, and the effectiveness of exploiting external rewards diminishes as well. Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning framework that autonomously adjusts configurations across iterations to Balance exploration and exploitation, thereby optimizing the self-improving effectiveness based on the current policy model and available rewards. Our experiments on mathematical reasoning, coding, and commonsense reasoning demonstrate that B-STaR not only enhances the model's exploratory capabilities throughout training but also achieves a more effective balance between exploration and exploitation, leading to superior performance. 

**Abstract (ZH)**: 在缺乏广泛的人标注数据以应对复杂推理任务时，模型通过自我改进——即模型在自己的输出上进行训练——已成为提升性能的主要方法。然而，这些迭代自我改进机制背后的决定性因素仍然不够理解，例如，在什么条件下自我改进是有效的，以及当前迭代中存在的瓶颈是什么。在本文中，我们识别并提出了一种方法来监控这一迭代过程中的两个关键因素：（1）模型生成足够多样化响应的能力（探索）；（2）外部奖励在区分高质量候选方案与低质量候选方案方面的有效性（开发）。以数学推理为例，我们从定量分析入手，追踪探索和开发的动力学过程，发现模型的探索能力在迭代过程中迅速下降，利用外部奖励的有效性也随之减弱。基于这些发现，我们引入了B-STaR框架，一种自我学习推理框架，能够在迭代过程中自主调整配置，平衡探索与开发，从而基于当前的策略模型和可用奖励优化自我改进的效果。我们的实验结果表明，B-STaR 不仅在整个训练过程中增强模型的探索能力，还实现了探索与开发之间的更有效的平衡，从而取得了更好的性能。 

---
# "From Unseen Needs to Classroom Solutions": Exploring AI Literacy Challenges & Opportunities with Project-based Learning Toolkit in K-12 Education 

**Title (ZH)**: 从未知需求到 Classroom解决方案：基于项目的学习工具包探索K-12教育中人工智能素养的挑战与机遇 

**Authors**: Hanqi Li, Ruiwei Xiao, Hsuan Nieu, Ying-Jui Tseng, Guanze Liao  

**Link**: [PDF](https://arxiv.org/pdf/2412.17243)  

**Abstract**: As artificial intelligence (AI) becomes increasingly central to various fields, there is a growing need to equip K-12 students with AI literacy skills that extend beyond computer science. This paper explores the integration of a Project-Based Learning (PBL) AI toolkit into diverse subject areas, aimed at helping educators teach AI concepts more effectively. Through interviews and co-design sessions with K-12 teachers, we examined current AI literacy levels and how teachers adapt AI tools like the AI Art Lab, AI Music Studio, and AI Chatbot into their course designs. While teachers appreciated the potential of AI tools to foster creativity and critical thinking, they also expressed concerns about the accuracy, trustworthiness, and ethical implications of AI-generated content. Our findings reveal the challenges teachers face, including limited resources, varying student and instructor skill levels, and the need for scalable, adaptable AI tools. This research contributes insights that can inform the development of AI curricula tailored to diverse educational contexts. 

**Abstract (ZH)**: 随着人工智能（AI）在各个领域的应用日益广泛，K-12学生需要具备超越计算机科学范畴的AI素养能力。本文探讨了将基于项目的AI学习工具（PBL AI Toolkit）整合到不同学科领域的可能性，旨在帮助教师更有效地教授AI概念。通过对K-12教师的访谈和协同设计会议，我们考察了当前的AI素养水平以及教师如何将AI工具，如AI艺术实验室、AI音乐工作室和AI聊天机器人，融入课程设计。尽管教师认识到AI工具在培养创造力和批判性思维方面的潜力，但他们也表达了对AI生成内容的准确性、可信度和伦理影响的担忧。我们的研究发现，教师面临的挑战包括资源有限、学生和教师技能水平参差不齐，以及需要具有可扩展性和适应性的AI工具。本文的研究成果为针对不同教育背景开发个性化的AI课程提供了见解。 

---
# On the Generalization Ability of Machine-Generated Text Detectors 

**Title (ZH)**: 机器生成文本检测器的泛化能力研究 

**Authors**: Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He  

**Link**: [PDF](https://arxiv.org/pdf/2412.17242)  

**Abstract**: The rise of large language models (LLMs) has raised concerns about machine-generated text (MGT), including ethical and practical issues like plagiarism and misinformation. Building a robust and highly generalizable MGT detection system has become increasingly important. This work investigates the generalization capabilities of MGT detectors in three aspects: First, we construct MGTAcademic, a large-scale dataset focused on academic writing, featuring human-written texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with an extensible code framework for efficient benchmarking. Second, we investigate the transferability of detectors across domains and LLMs, leveraging fine-grained datasets to reveal insights into domain transferring and implementing few-shot techniques to improve the performance by roughly 13.2%. Third, we introduce a novel attribution task where models must adapt to new classes over time without (or with very limited) access to prior training data and benchmark detectors. We implement several adapting techniques to improve the performance by roughly 10% and highlight the inherent complexity of the task. Our findings provide insights into the generalization ability of MGT detectors across diverse scenarios and lay the foundation for building robust, adaptive detection systems. 

**Abstract (ZH)**: 大型语言模型（LLMs）的兴起引发了人们对机器生成文本（MGT）的关注，包括伦理和实践问题，如抄袭和虚假信息。构建一个强大且高度泛化的MGT检测系统变得越来越重要。本研究从三个维度探讨了MGT检测器的泛化能力：首先，我们构建了MGTAcademic大规模数据集，该数据集专注于学术写作，包含来自STEM、人文学科和社会科学领域的人类撰写的文本（HWTs）和MGT，并配备了一个可扩展的代码框架，用于高效的基准测试。其次，我们研究了检测器在不同领域和LLMs之间的可迁移性，利用细粒度数据集揭示跨领域迁移的见解，并通过少量样本技术提高性能约13.2%。第三，我们引入了一项新颖的归因任务，即模型需要在有限的（或几乎没有）先前训练数据访问的情况下适应新的类别。我们实施了几种适应方法来提高性能约10%，并强调任务的内在复杂性。我们的研究提供了MGT检测器在各种场景中的泛化能力见解，并为构建强大的、适应性强的检测系统奠定了基础。 

---
# MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial Matching 

**Title (ZH)**: MatchMiner-AI：一种开源的癌症临床试验匹配解决方案 

**Authors**: Ethan Cerami, Pavel Trukhanov, Morgan A. Paul, Michael J. Hassett, Irbaz B. Riaz, James Lindsay, Emily Mallaber, Harry Klein, Gufran Gungor, Matthew Galvin, Stephen C. Van Nostrand, Joyce Yu, Tali Mazor, Kenneth L. Kehl  

**Link**: [PDF](https://arxiv.org/pdf/2412.17228)  

**Abstract**: Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate matching of patients to appropriate clinical trials. Here, we describe the development and evaluation of the MatchMiner-AI pipeline for clinical trial searching and ranking. MatchMiner-AI focuses on matching patients to potential trials based on core criteria describing clinical "spaces," or disease contexts, targeted by a trial. It aims to accelerate the human work of identifying potential matches, not to fully automate trial screening. The pipeline includes modules for extraction of key information from a patient's longitudinal electronic health record; rapid ranking of candidate trial-patient matches based on embeddings in vector space; and classification of whether a candidate match represents a reasonable clinical consideration. Code and synthetic data are available at this https URL . Model weights based on synthetic data are available at this https URL and this https URL . A simple cancer clinical trial search engine to demonstrate pipeline components is available at this https URL . 

**Abstract (ZH)**: 临床试验推动了癌症治疗和结果的改进。然而，大多数患有癌症的成年人并未参与试验，而且试验往往未能招募足够的患者来回答其科学问题。人工智能可以加速患者与适当临床试验的匹配过程。在这里，我们描述了MatchMiner-AI管道的开发和评估，该管道用于临床试验搜索和排序。MatchMiner-AI侧重于根据描述“临床空间”或试验靶向疾病环境的核心标准，匹配患者至合适试验。其目标是加速识别潜在匹配的过程，而非完全自动化试验筛选。该管道包括从患者纵向电子健康记录中提取关键信息的模块；基于向量空间嵌入快速对候选试验-患者匹配进行排名的模块；以及对候选匹配是否构成合理的临床考虑进行分类的模块。代码和合成数据可在以下网址获得：[这里](http://example.com)。基于合成数据的模型权重可在以下网址获得：[这里](http://example.com/models_weights_1) 和 [这里](http://example.com/models_weights_2)。用于演示管道组件的简单癌症临床试验搜索引擎可在以下网址获得：[这里](http://example.com/search_engine)。 

---
# Better Think with Tables: Leveraging Tables to Enhance Large Language Model Comprehension 

**Title (ZH)**: 增强大型语言模型理解能力：利用表格进行更有效的思考 

**Authors**: Jio Oh, Geon Heo, Seungjun Oh, Jindong Wang, Xing Xie, Steven Euijong Whang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17189)  

**Abstract**: Despite the recent advancement of Large Langauge Models (LLMs), they struggle with complex queries often involving multiple conditions, common in real-world scenarios. We propose Thinking with Tables, a technique that assists LLMs to leverage tables for intermediate thinking aligning with human cognitive behavior. By introducing a pre-instruction that triggers an LLM to organize information in tables, our approach achieves a 40.29\% average relative performance increase, higher robustness, and show generalizability to different requests, conditions, or scenarios. We additionally show the influence of data structuredness for the model by comparing results from four distinct structuring levels that we introduce. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在最近取得了进展，它们在处理涉及多个条件的复杂查询时仍然存在困难，而这类查询在现实场景中相当常见。我们提出了一种“思考表格化”的技术，旨在帮助LLMs利用表格进行中间推理，这种推理方式与人类的认知行为相一致。通过引入一种预指令，促使LLMs将信息组织成表格形式，我们的方法实现了40.29%的平均相对性能提升，并且具备更高的稳健性和较强的一般适用性，能够应用于不同的请求、条件或场景。此外，我们通过对比四种不同组织层次的数据结果，进一步展示了数据结构化对模型影响的作用。 

---
# DCC: Differentiable Cardinality Constraints for Partial Index Tracking 

**Title (ZH)**: DCC：部分索引跟踪中的可微基数约束 

**Authors**: Wooyeon Jo, Hyunsouk Cho  

**Link**: [PDF](https://arxiv.org/pdf/2412.17175)  

**Abstract**: Index tracking is a popular passive investment strategy aimed at optimizing portfolios, but fully replicating an index can lead to high transaction costs. To address this, partial replication have been proposed. However, the cardinality constraint renders the problem non-convex, non-differentiable, and often NP-hard, leading to the use of heuristic or neural network-based methods, which can be non-interpretable or have NP-hard complexity. To overcome these limitations, we propose a Differentiable Cardinality Constraint ($\textbf{DCC}$) for index tracking and introduce a floating-point precision-aware method ($\textbf{DCC}_{fpp}$) to address implementation issues. We theoretically prove our methods calculate cardinality accurately and enforce actual cardinality with polynomial time complexity. We propose the range of the hyperparameter $a$ ensures that $\textbf{DCC}_{fpp}$ has no error in real implementations, based on theoretical proof and experiment. Our method applied to mathematical method outperforms baseline methods across various datasets, demonstrating the effectiveness of the identified hyperparameter $a$. 

**Abstract (ZH)**: 指数跟踪是一种流行的被动投资策略，旨在优化投资组合，但完全复制指数可能会导致高昂的交易成本。为了解决这一问题，部分复制指数的方法被提出。然而，基数约束使得问题变得非凸、非可微，通常还属于NP难问题，因此通常使用启发式方法或基于神经网络的方法，这些方法可能会缺乏可解释性或自身复杂度也是NP难问题。为克服这些局限性，我们提出了一种可微基数约束（$\textbf{DCC}$）以应用于指数跟踪，并引入了一种考虑浮点精度的方法（$\textbf{DCC}_{fpp}$）来解决实施中可能遇到的问题。我们从理论上证明了我们的方法能够准确计算基数，并能在多项式时间内确保实际基数的满足。基于理论证明和实验，我们提出了超参数 $a$ 的取值范围，确保 $\textbf{DCC}_{fpp}$ 在实际应用中没有误差。将我们的方法应用于各种数据集中的数学方法中，显示出所识别的超参数 $a$ 的有效性，其性能超过了基线方法。 

---
# Survey on Abstractive Text Summarization: Dataset, Models, and Metrics 

**Title (ZH)**: 摘要性文本总结综述：数据集、模型与评价指标 

**Authors**: Gospel Ozioma Nnadi, Flavio Bertini  

**Link**: [PDF](https://arxiv.org/pdf/2412.17165)  

**Abstract**: The advancements in deep learning, particularly the introduction of transformers, have been pivotal in enhancing various natural language processing (NLP) tasks. These include text-to-text applications such as machine translation, text classification, and text summarization, as well as data-to-text tasks like response generation and image-to-text tasks such as captioning. Transformer models are distinguished by their attention mechanisms, pretraining on general knowledge, and fine-tuning for downstream tasks. This has led to significant improvements, particularly in abstractive summarization, where sections of a source document are paraphrased to produce summaries that closely resemble human expression.
The effectiveness of these models is assessed using diverse metrics, encompassing techniques like semantic overlap and factual correctness. This survey examines the state of the art in text summarization models, with a specific focus on the abstractive summarization approach. It reviews various datasets and evaluation metrics used to measure model performance. Additionally, it includes the results of test cases using abstractive summarization models to underscore the advantages and limitations of contemporary transformer-based models. The source codes and the data are available at this https URL. 

**Abstract (ZH)**: 深度学习的进展，尤其是变压器模型的引入，极大地推动了各种自然语言处理（NLP）任务的提升。这些任务包括文本到文本的应用，如机器翻译、文本分类和文本摘要，以及数据到文本的任务，如响应生成，还包括图像到文本的任务，如图像字幕。变压器模型以其注意力机制、预先在广泛知识上进行训练以及为下游任务进行微调而著称。这些特点导致了显著的改进，特别是在抽象性摘要方面，即通过重新表述源文档的段落来生成高度符合人类表达的摘要。

这些模型的有效性通过多种评估指标来衡量，包括语义重叠度和事实正确性等技术。本文综述了当前最先进的文本摘要模型，并特别关注抽象性摘要方法。文中回顾了用于衡量模型性能的各种数据集和评估指标。此外，还包括了使用抽象性摘要模型的测试案例，以强调现代基于变压器模型的优势和局限性。所有源代码和数据都可通过以下链接获取：this https URL。 

---
# Semantic Web: Past, Present, and Future 

**Title (ZH)**: 语义网：过去、现在与未来 

**Authors**: Ansgar Scherp, Gerd Groener, Petr Škoda, Katja Hose, Maria-Esther Vidal  

**Link**: [PDF](https://arxiv.org/pdf/2412.17159)  

**Abstract**: Ever since the vision was formulated, the Semantic Web has inspired many generations of innovations. Semantic technologies have been used to share vast amounts of information on the Web, enhance them with semantics to give them meaning, and enable inference and reasoning on them. Throughout the years, semantic technologies, and in particular knowledge graphs, have been used in search engines, data integration, enterprise settings, and machine learning.
In this paper, we recap the classical concepts and foundations of the Semantic Web as well as modern and recent concepts and applications, building upon these foundations. The classical topics we cover include knowledge representation, creating and validating knowledge on the Web, reasoning and linking, and distributed querying. We enhance this classical view of the so-called ``Semantic Web Layer Cake'' with an update of recent concepts that include provenance, security and trust, as well as a discussion of practical impacts from industry-led contributions. We conclude with an outlook on the future directions of the Semantic Web. 

**Abstract (ZH)**: 自Semantic Web的概念提出以来，它激发了多个世代的创新。语义技术被用于在Web上共享海量信息，并通过赋予它们语义来增加意义，从而实现对它们的推理和推断。多年来，尤其是知识图谱，已经被广泛应用于搜索引擎、数据集成、企业环境以及机器学习等领域。

在本文中，我们回顾了Semantic Web的经典概念和基础，同时也涵盖了最新的概念和应用，以此为基础进行扩展。我们涵盖的经典话题包括知识表示、在Web上创建和验证知识、推理和链接，以及分布式查询等方面。我们还更新了“Semantic Web层蛋糕”这一经典视角，包括添加了来源、安全和信任等方面的概念，并讨论了来自行业的贡献所带来的实际影响。最后，我们展望了Semantic Web未来的发展方向。 

---
# LLM Agent for Fire Dynamics Simulations 

**Title (ZH)**: 用于火灾动力学模拟的大型语言模型代理 

**Authors**: Leidong Xu, Danyal Mohaddes, Yi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17146)  

**Abstract**: Significant advances have been achieved in leveraging foundation models, such as large language models (LLMs), to accelerate complex scientific workflows. In this work we introduce FoamPilot, a proof-of-concept LLM agent designed to enhance the usability of FireFOAM, a specialized solver for fire dynamics and fire suppression simulations built using OpenFOAM, a popular open-source toolbox for computational fluid dynamics (CFD). FoamPilot provides three core functionalities: code insight, case configuration and simulation evaluation. Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code for developers and experienced users. For case configuration, the agent interprets user requests in natural language and aims to modify existing simulation setups accordingly to support intermediate users. FoamPilot's job execution functionality seeks to manage the submission and execution of simulations in high-performance computing (HPC) environments and provide preliminary analysis of simulation results to support less experienced users. Promising results were achieved for each functionality, particularly for simple tasks, and opportunities were identified for significant further improvement for more complex tasks. The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow for engineers and scientists employing FireFOAM for complex simulations critical for improving fire safety. 

**Abstract (ZH)**: 在利用大型语言模型（LLM）等基础模型加速复杂科学工作流方面取得了显著进展。本文介绍了FoamPilot，这是一种概念验证的LLM代理，旨在提升FireFOAM的易用性，FireFOAM是一个基于OpenFOAM构建的专用求解器，用于火灾动力学和灭火模拟。OpenFOAM是一个流行的开源计算流体动力学（CFD）工具箱。FoamPilot提供了三个核心功能：代码洞察、案例配置和仿真评估。代码洞察利用检索增强生成（RAG）作为一种替代传统的关键词搜索的方法，旨在使开发人员和有经验的用户能够高效地导航和总结FireFOAM的源代码。在案例配置方面，代理以自然语言解释用户请求，并旨在相应地修改现有的仿真设置，以支持中级用户。FoamPilot的任务执行功能旨在管理仿真在高性能计算（HPC）环境中的提交和执行，并对仿真结果进行初步分析，以支持经验较少的用户。对于每个功能，特别是简单任务，我们取得了令人鼓舞的结果，并且识别出了在更复杂任务上进行重大改进的机会。将这些功能集成到一个单一的LLM代理中，是一个旨在加速使用FireFOAM进行复杂仿真（这对于提高火灾安全性至关重要）的工程师和科学家的仿真工作流的步骤。 

---
# ASP-based Multi-shot Reasoning via DLV2 with Incremental Grounding 

**Title (ZH)**: 基于ASP的多轮推理方法通过DLV2实现增量底层约束生成 

**Authors**: Francesco Calimeri, Giovambattista Ianni, Francesco Pacenza, Simona Perri, Jessica Zanfari  

**Link**: [PDF](https://arxiv.org/pdf/2412.17143)  

**Abstract**: DLV2 is an AI tool for Knowledge Representation and Reasoning which supports Answer Set Programming (ASP) - a logic-based declarative formalism, successfully used in both academic and industrial applications. Given a logic program modelling a computational problem, an execution of DLV2 produces the so-called answer sets that correspond one-to-one to the solutions to the problem at hand. The computational process of DLV2 relies on the typical Ground & Solve approach where the grounding step transforms the input program into a new, equivalent ground program, and the subsequent solving step applies propositional algorithms to search for the answer sets. Recently, emerging applications in contexts such as stream reasoning and event processing created a demand for multi-shot reasoning: here, the system is expected to be reactive while repeatedly executed over rapidly changing data. In this work, we present a new incremental reasoner obtained from the evolution of DLV2 towards iterated reasoning. Rather than restarting the computation from scratch, the system remains alive across repeated shots, and it incrementally handles the internal grounding process. At each shot, the system reuses previous computations for building and maintaining a large, more general ground program, from which a smaller yet equivalent portion is determined and used for computing answer sets. Notably, the incremental process is performed in a completely transparent fashion for the user. We describe the system, its usage, its applicability and performance in some practically relevant domains. Under consideration in Theory and Practice of Logic Programming (TPLP). 

**Abstract (ZH)**: DLV2 是一种支持知识表示与推理的 AI 工具，它支持基于逻辑的声明式形式化方法回答集编程（Answer Set Programming, ASP），并在学术和工业应用中取得了成功。给定一个表示计算问题的逻辑程序，DLV2 的执行会产生所谓的回答集，这些回答集与所面临的实际问题的解一一对应。DLV2 的计算过程依赖于“接地与求解”（Ground & Solve）方法，其中，接地步骤将输入程序转换为一个新的等效接地程序，随后的求解步骤应用命题算法以搜索回答集。最近，在流推理和事件处理等上下文中，新兴应用对多轮推理产生了需求：在这种情况下，系统需要在反复执行于快速变化的数据时保持反应性。在这项工作中，我们介绍了一个新的增量推理系统，这是从 DLV2 向迭代推理的演变所获得的成果。系统不会从头开始重新计算，而是在多次执行之间保持活动，并逐步处理内部的接地过程。在每次执行中，系统利用先前的计算来构建和维护一个更大、更通用的接地程序，从其中确定一个较小但等效的部分用于计算回答集。值得注意的是，增量过程完全对用户透明。我们描述了该系统、其用法、其在某些实际相关领域的适用性和性能。该论文正在考虑发表在《逻辑程序理论与实践》（Theory and Practice of Logic Programming, TPLP）上。 

---
# AI-Based Teat Shape and Skin Condition Prediction for Dairy Management 

**Title (ZH)**: 基于AI的奶牛乳房形状和皮肤状况预测在奶牛管理中的应用 

**Authors**: Yuexing Hao, Tiancheng Yuan, Yuting Yang, Aarushi Gupta, Matthias Wieland, Ken Birman, Parminder S. Basran  

**Link**: [PDF](https://arxiv.org/pdf/2412.17142)  

**Abstract**: Dairy owners spend significant effort to keep their animals healthy. There is good reason to hope that technologies such as computer vision and artificial intelligence (AI) could reduce these costs, yet obstacles arise when adapting advanced tools to farming environments. In this work, we adapt AI tools to dairy cow teat localization, teat shape, and teat skin condition classifications. We also curate a data collection and analysis methodology for a Machine Learning (ML) pipeline. The resulting teat shape prediction model achieves a mean Average Precision (mAP) of 0.783, and the teat skin condition model achieves a mean average precision of 0.828. Our work leverages existing ML vision models to facilitate the individualized identification of teat health and skin conditions, applying AI to the dairy management industry. 

**Abstract (ZH)**: 奶农投入大量精力保持奶牛的健康。人们有理由希望计算机视觉和人工智能（AI）技术能够降低这些成本，但在将先进技术适应农业环境时仍会遇到诸多障碍。在本研究中，我们将AI工具应用于奶牛乳头定位、乳头形状和乳头皮肤状况分类。我们还建立了一套机器学习（ML）管道的数据采集和分析方法。得到的乳头形状预测模型的平均平均精度（mAP）为0.783，乳头皮肤状况模型的平均平均精度为0.828。我们的工作利用现有的ML视觉模型，以便通过AI技术实现对乳头健康和皮肤状况的个性化识别，促进了 Dairy 管理行业的应用。 

---
# On the ETHOS of AI Agents: An Ethical Technology and Holistic Oversight System 

**Title (ZH)**: AI代理的伦理精神：一种全面监督的伦理技术体系 

**Authors**: Tomer Jordi Chaffer, Justin Goldston, Bayo Okusanya, Gemach D.A.T.A.I  

**Link**: [PDF](https://arxiv.org/pdf/2412.17114)  

**Abstract**: In a world increasingly defined by machine intelligence, the future depends on how we govern the development and integration of AI into society. Recent initiatives, such as the EU AI Act, EDPB opinion, U.S. Bipartisan House Task Force and NIST AI Risk Management Report, highlight the urgent need for robust governance frameworks to address the challenges posed by advancing AI technologies. However, existing frameworks fail to adequately address the rise of AI agents or the ongoing debate between centralized and decentralized governance models. To bridge these gaps, we propose the Ethical Technology and Holistic Oversight System framework, which leverages Web3 technologies, including blockchain, smart contracts, decentralized autonomous organizations, and soulbound tokens, to establish a decentralized global registry for AI agents. ETHOS incorporates the concept of AI specific legal entities, enabling these systems to assume limited liability and ensuring accountability through mechanisms like insurance and compliance monitoring. Additionally, the framework emphasizes the need for a collaborative, participatory approach to AI governance, engaging diverse stakeholders through public education, transparency, and international coordination. ETHOS balances innovation with ethical accountability, providing a forward looking strategy for the responsible integration of AI agents into society. Finally, this exploration reflects the emergence of a new interdisciplinary field we define as Systems Thinking at the Intersection of AI, Web3, and Society. 

**Abstract (ZH)**: 在日益由机器智能定义的世界中，未来取决于我们如何治理AI在社会中的发展和整合。近期的举措，如欧盟AI法案、EDPB意见、美国两党国会任务小组和NIST AI风险管理报告，凸显了建立坚实治理框架的迫切需求，以应对先进AI技术带来的挑战。然而，现有的框架未能充分应对AI代理的崛起，以及集中式与去中心化治理模式之间的持续辩论。为了填补这些空白，我们提出了一种伦理技术和综合监督系统的框架（Ethical Technology and Holistic Oversight System, ETHOS），该框架利用Web3技术，包括区块链、智能合约、去中心化自治组织和绑定灵魂代币，建立一个去中心化的全球AI代理注册系统。ETHOS引入了特定于AI的法律实体概念，使这些系统能够承担有限责任，并通过保险和合规监控等机制确保问责制。此外，该框架强调采用合作、参与的方式进行AI治理，通过公众教育、透明度和国际协调，吸引多元化的利益相关者。ETHOS在促进AI代理负责任地融入社会的同时平衡了创新与伦理问责，提供了一个前瞻性的策略。最后，本文探讨了一个新的跨学科领域——AI、Web3和社会交汇处的系统思维，我们将其定义为系统思维。 

---
# Aligning Graphical and Functional Causal Abstractions 

**Title (ZH)**: 图形化与功能性因果抽象的对齐 

**Authors**: Wilem Schooltink, Fabio Massimo Zennaro  

**Link**: [PDF](https://arxiv.org/pdf/2412.17080)  

**Abstract**: Causal abstractions allow us to relate causal models on different levels of granularity. To ensure that the models agree on cause and effect, frameworks for causal abstractions define notions of consistency. Two distinct methods for causal abstraction are common in the literature: (i) graphical abstractions, such as Cluster DAGs, which relate models on a structural level, and (ii) functional abstractions, like $\alpha$-abstractions, which relate models by maps between variables and their ranges. In this paper we will align the notions of graphical and functional consistency and show an equivalence between the class of Cluster DAGs, consistent $\alpha$-abstractions, and constructive $\tau$-abstractions. Furthermore, we extend this alignment and the expressivity of graphical abstractions by introducing Partial Cluster DAGs. Our results provide a rigorous bridge between the functional and graphical frameworks and allow for adoption and transfer of results between them. 

**Abstract (ZH)**: 因果抽象使我们能够关联不同粒度级别的因果模型。为了确保这些模型在因果关系上的一致性，因果抽象框架定义了一致性的概念。文献中常见的两种不同的因果抽象方法有：（i）图形抽象，如聚类DAG（Cluster DAGs），它们在结构层次上关联模型，以及（ii）函数抽象，如$\alpha$-抽象，它们通过变量及其取值范围之间的映射关联模型。在本文中，我们将图形一致性和函数一致性相统一，并展示聚类DAG、一致$\alpha$-抽象和构造$\tau$-抽象之间的一致性类。此外，我们通过引入部分聚类DAG（Partial Cluster DAGs）扩展了这种统一以及图形抽象的表达能力。我们的研究结果为函数和图形框架之间提供了一个严格的桥梁，使得它们之间的结果可以被采用和转移。 

---
# SubstationAI: Multimodal Large Model-Based Approaches for Analyzing Substation Equipment Faults 

**Title (ZH)**: SubstationAI：基于多模态大规模模型的方法用于分析变电站设备故障 

**Authors**: Jinzhi Wang, Qinfeng Song, Lidong Qian, Haozhou Li, Qinke Peng, Jiangbo Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17077)  

**Abstract**: The reliability of substation equipment is crucial to the stability of power systems, but traditional fault analysis methods heavily rely on manual expertise, limiting their effectiveness in handling complex and large-scale data. This paper proposes a substation equipment fault analysis method based on a multimodal large language model (MLLM). We developed a database containing 40,000 entries, including images, defect labels, and analysis reports, and used an image-to-video generation model for data augmentation. Detailed fault analysis reports were generated using GPT-4. Based on this database, we developed SubstationAI, the first model dedicated to substation fault analysis, and designed a fault diagnosis knowledge base along with knowledge enhancement methods. Experimental results show that SubstationAI significantly outperforms existing models, such as GPT-4, across various evaluation metrics, demonstrating higher accuracy and practicality in fault cause analysis, repair suggestions, and preventive measures, providing a more advanced solution for substation equipment fault analysis. 

**Abstract (ZH)**: 变电站设备的可靠性对于电力系统稳定性至关重要，但传统故障分析方法严重依赖人工专业知识，限制了其在处理复杂和大规模数据方面的有效性。本文提出了一种基于多模态大语言模型（MLLM）的变电站设备故障分析方法。我们建立了一个包含40,000条记录的数据库，包括图像、缺陷标签和分析报告，并使用图像到视频生成模型进行数据增强。使用GPT-4生成了详细故障分析报告。基于该数据库，我们开发了SubstationAI——第一个专用于变电站故障分析的模型，并设计了故障诊断知识库及知识增强方法。实验结果表明，SubstationAI在各种评估指标上显著优于现有的模型（如GPT-4），在故障原因分析、维修建议和预防措施方面展现出更高的准确性和实用性，为变电站设备故障分析提供了更高级的解决方案。 

---
# ViLBias: A Framework for Bias Detection using Linguistic and Visual Cues 

**Title (ZH)**: ViLBias：一种基于语言和视觉线索的偏见检测框架 

**Authors**: Shaina Raza, Caesar Saleh, Emrul Hasan, Franklin Ogidi, Maximus Powers, Veronica Chatrath, Marcelo Lotif, Roya Javadi, Anam Zahid, Vahid Reza Khazaie  

**Link**: [PDF](https://arxiv.org/pdf/2412.17052)  

**Abstract**: The integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) opens new avenues for addressing complex challenges in multimodal content analysis, particularly in biased news detection. This study introduces ViLBias, a framework that leverages state of the art LLMs and VLMs to detect linguistic and visual biases in news content, addressing the limitations of traditional text-only approaches. Our contributions include a novel dataset pairing textual content with accompanying visuals from diverse news sources and a hybrid annotation framework, combining LLM-based annotations with human review to enhance quality while reducing costs and improving scalability. We evaluate the efficacy of LLMs and VLMs in identifying biases, revealing their strengths in detecting subtle framing and text-visual inconsistencies. Empirical analysis demonstrates that incorporating visual cues alongside text enhances bias detection accuracy by 3 to 5 %, showcasing the complementary strengths of LLMs in generative reasoning and Small Language Models (SLMs) in classification. This study offers a comprehensive exploration of LLMs and VLMs as tools for detecting multimodal biases in news content, highlighting both their potential and limitations. Our research paves the way for more robust, scalable, and nuanced approaches to media bias detection, contributing to the broader field of natural language processing and multimodal analysis. (The data and code will be made available for research purposes). 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，并保持学术规范：

大语言模型（LLM）与视觉语言模型（VLM）的融合为多模态内容分析中复杂挑战的解决开辟了新的途径，特别是在有偏见的新闻检测方面。本研究引入了ViLBias框架，该框架利用最先进的LLM和VLM来检测新闻内容中的语言和视觉偏见，弥补了传统文本单模态方法的局限性。我们的贡献包括一个全新的数据集，该数据集将文本内容与其来自各种新闻来源的配套视觉内容配对，并设计了一种混合注释框架，结合基于LLM的注释和人工审核，以提高质量、降低成本并提高可扩展性。我们评估了LLM和VLM在识别偏见方面的有效性，揭示了它们在检测细微框架和图文不一致方面的优势。实证分析表明，将视觉线索与文本结合使用可以提高偏见检测准确性3%到5%，展示了LLM在生成推理方面的优势以及小语言模型（SLM）在分类方面的补充优势。本研究全面探讨了LLM和VLM作为检测新闻内容中多模态偏见的工具的潜力和局限性，为更稳健、更具扩展性和更细致的媒体偏见检测方法奠定了基础，促进了自然语言处理和多模态分析领域的发展。我们的研究将为研究目的提供数据和代码。

(数据和代码将用于研究目的) 

---
# GraphAgent: Agentic Graph Language Assistant 

**Title (ZH)**: GraphAgent：自主图语言助手 

**Authors**: Yuhao Yang, Jiabin Tang, Lianghao Xia, Xingchen Zou, Yuxuan Liang, Chao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17029)  

**Abstract**: Real-world data is represented in both structured (e.g., graph connections) and unstructured (e.g., textual, visual information) formats, encompassing complex relationships that include explicit links (such as social connections and user behaviors) and implicit interdependencies among semantic entities, often illustrated through knowledge graphs. In this work, we propose GraphAgent, an automated agent pipeline that addresses both explicit graph dependencies and implicit graph-enhanced semantic inter-dependencies, aligning with practical data scenarios for predictive tasks (e.g., node classification) and generative tasks (e.g., text generation). GraphAgent comprises three key components: (i) a Graph Generator Agent that builds knowledge graphs to reflect complex semantic dependencies; (ii) a Task Planning Agent that interprets diverse user queries and formulates corresponding tasks through agentic self-planning; and (iii) a Task Execution Agent that efficiently executes planned tasks while automating tool matching and invocation in response to user queries. These agents collaborate seamlessly, integrating language models with graph language models to uncover intricate relational information and data semantic dependencies. Through extensive experiments on various graph-related predictive and text generative tasks on diverse datasets, we demonstrate the effectiveness of our GraphAgent across various settings. We have made our proposed GraphAgent open-source at: this https URL. 

**Abstract (ZH)**: 实际数据以结构化（例如，图形连接）和非结构化（例如，文本和视觉信息）的形式表示，涵盖了复杂的相互关系，包括显式的链接（如社会连接和用户行为）以及语义实体之间的隐式相互依赖，这些通常通过知识图谱来描绘。本文提出了一种自动代理管道——GraphAgent，同时解决了显式的图形依赖和隐式的图形增强语义依赖，适用于各类实际数据情景下的预测任务（例如，节点分类）以及生成任务（例如，文本生成）。GraphAgent 包含三个关键组件：(i) 图生成代理，用于构建知识图谱以反映复杂的语义依赖；(ii) 任务规划代理，通过自主规划解释多种用户查询并制定相应的任务；以及(iii) 任务执行代理，高效执行规划的任务，同时根据用户查询自动匹配和调用工具。这些代理之间无缝协作，将语言模型与图形语言模型结合，揭示复杂的关联信息和数据语义依赖。通过在各种图相关的预测和文本生成任务中对不同数据集进行广泛的实验，我们展示了GraphAgent在各种应用场景下的有效性。我们已将提议的GraphAgent开源于：[此处插入链接]。 

---
# GAS: Generative Auto-bidding with Post-training Search 

**Title (ZH)**: GAS：生成式自适应出价与后训练搜索 

**Authors**: Yewen Li, Shuai Mao, Jingtong Gao, Nan Jiang, Yunjian Xu, Qingpeng Cai, Fei Pan, Peng Jiang, Bo An  

**Link**: [PDF](https://arxiv.org/pdf/2412.17018)  

**Abstract**: Auto-bidding is essential in facilitating online advertising by automatically placing bids on behalf of advertisers. Generative auto-bidding, which generates bids based on an adjustable condition using models like transformers and diffusers, has recently emerged as a new trend due to its potential to learn optimal strategies directly from data and adjust flexibly to preferences. However, generative models suffer from low-quality data leading to a mismatch between condition, return to go, and true action value, especially in long sequential decision-making. Besides, the majority preference in the dataset may hinder models' generalization ability on minority advertisers' preferences. While it is possible to collect high-quality data and retrain multiple models for different preferences, the high cost makes it unaffordable, hindering the advancement of auto-bidding into the era of large foundation models. To address this, we propose a flexible and practical Generative Auto-bidding scheme using post-training Search, termed GAS, to refine a base policy model's output and adapt to various preferences. We use weak-to-strong search alignment by training small critics for different preferences and an MCTS-inspired search to refine the model's output. Specifically, a novel voting mechanism with transformer-based critics trained with policy indications could enhance search alignment performance. Additionally, utilizing the search, we provide a fine-tuning method for high-frequency preference scenarios considering computational efficiency. Extensive experiments conducted on the real-world dataset and online A/B test on the Kuaishou advertising platform demonstrate the effectiveness of GAS, achieving significant improvements, e.g., 1.554% increment of target cost. 

**Abstract (ZH)**: 自适应出价对于促进在线广告至关重要，它可以通过模型自动为广告商出价。基于生成模型的自适应出价是一种新兴趋势，这种模型利用变换器和扩散模型等模型根据可调条件生成出价，具有直接从数据中学习最优策略和灵活调整偏好潜力。然而，生成模型由于低质量的数据而容易出现条件、剩余价值与真实行动价值之间的不匹配问题，尤其是在长期序列决策中。此外，数据集中主流偏好的存在可能会阻碍模型对小众广告商偏好的泛化能力。尽管可以通过收集高质量数据和为不同偏好重新训练多个模型来解决这个问题，但高成本使其不可行，阻碍了自适应出价的进步，使其难以进入大规模基础模型时代。为了解决这一问题，我们提出了一种名为GAS（Post-training Search-based Generative Auto-bidding）的灵活且实用的生成自适应出价方案，以改善基础策略模型的输出并适应各种偏好。我们通过训练针对不同偏好的小型评论家并结合借鉴蒙特卡罗树搜索（MCTS）的理念来改进模型输出。具体地，一种基于投票机制的新颖方法，通过使用变压器训练的评论家并结合策略指示，可以增强搜索对齐性能。此外，利用搜索机制，我们还提供了一种考虑计算效率的微调方法，适用于高频偏好场景。在真实的广告数据集上进行的广泛实验以及快手广告平台的在线A/B测试均证明了GAS的有效性，取得了显著的改进，例如目标成本提高了1.554%。 

---
# Cannot or Should Not? Automatic Analysis of Refusal Composition in IFT/RLHF Datasets and Refusal Behavior of Black-Box LLMs 

**Title (ZH)**: 当然，以下是论文标题和内容的中文翻译，符合学术规范：

"-automatic-analysis-of-refusal-composition-in-ift-rlhf-datasets-and-refusal-behavior-of-black-box-llms"

"自动分析IFT/RLHF数据集中的拒绝组成以及黑盒大语言模型的拒绝行为"

注释：
- "IFT/RLHF" 的中文通常解释为“数据驱动的模型整合适应”和“基于人类反馈的强化学习”，具体含义可能依赖于上下文。
- "黑盒大语言模型" 是指不公开其内部工作原理的大语言模型，这类模型在学术界通常指没有提供足够透明度的模型，特别是在训练过程中使用的具体数据和技术细节不透明的模型。 

**Authors**: Alexander von Recum, Christoph Schnabl, Gabor Hollbeck, Silas Alberti, Philip Blinde, Marvin von Hagen  

**Link**: [PDF](https://arxiv.org/pdf/2412.16974)  

**Abstract**: Refusals - instances where large language models (LLMs) decline or fail to fully execute user instructions - are crucial for both AI safety and AI capabilities and the reduction of hallucinations in particular. These behaviors are learned during post-training, especially in instruction fine-tuning (IFT) and reinforcement learning from human feedback (RLHF). However, existing taxonomies and evaluation datasets for refusals are inadequate, often focusing solely on should-not-related (instead of cannot-related) categories, and lacking tools for auditing refusal content in black-box LLM outputs.
We present a comprehensive framework for classifying LLM refusals: (a) a taxonomy of 16 refusal categories, (b) a human-annotated dataset of over 8,600 instances from publicly available IFT and RLHF datasets, (c) a synthetic dataset with 8,000 examples for each refusal category, and (d) classifiers trained for refusal classification.
Our work enables precise auditing of refusal behaviors in black-box LLMs and automatic analyses of refusal patterns in large IFT and RLHF datasets. This facilitates the strategic adjustment of LLM refusals, contributing to the development of more safe and reliable LLMs. 

**Abstract (ZH)**: 拒绝行为 - 大型语言模型在执行用户指令时拒绝或未能完全执行的实例 - 对AI安全和AI能力至关重要，尤其是对减少幻觉现象尤为重要。这些行为主要是在后训练阶段学习到的，特别是在指令微调（IFT）和基于人类反馈的强化学习（RLHF）中。但是，现有的拒绝行为分类和评估数据集存在不足，通常仅关注“不该做”的类别（而非“不能做”的类别），缺乏对黑盒大型语言模型输出中的拒绝内容进行审计的工具。

我们提出了一种全面的大型语言模型拒绝行为分类框架：（a）16个拒绝行为分类的分类体系；（b）超过8,600个实例的人工标注数据集，这些实例来自公开的指令微调和基于人类反馈的强化学习数据集；（c）每个拒绝行为类别包含8,000个示例的合成数据集；（d）用于拒绝行为分类的分类器。

我们的工作使得对黑盒大型语言模型中的拒绝行为进行精确审计成为可能，并能够自动分析大型指令微调和基于人类反馈的强化学习数据集中的拒绝模式。这促进了对大型语言模型拒绝行为的策略性调整，从而推动了更安全和可靠的大型语言模型的发展。 

---
# Environment Descriptions for Usability and Generalisation in Reinforcement Learning 

**Title (ZH)**: 强化学习中可用性和泛化性的环境描述研究 

**Authors**: Dennis J.N.J. Soemers, Spyridon Samothrakis, Kurt Driessens, Mark H.M. Winands  

**Link**: [PDF](https://arxiv.org/pdf/2412.16970)  

**Abstract**: The majority of current reinforcement learning (RL) research involves training and deploying agents in environments that are implemented by engineers in general-purpose programming languages and more advanced frameworks such as CUDA or JAX. This makes the application of RL to novel problems of interest inaccessible to small organisations or private individuals with insufficient engineering expertise. This position paper argues that, to enable more widespread adoption of RL, it is important for the research community to shift focus towards methodologies where environments are described in user-friendly domain-specific or natural languages. Aside from improving the usability of RL, such language-based environment descriptions may also provide valuable context and boost the ability of trained agents to generalise to unseen environments within the set of all environments that can be described in any language of choice. 

**Abstract (ZH)**: 当前大多数强化学习（RL）研究涉及在由工程师使用通用编程语言或更高级的框架（如CUDA或JAX）构建的环境中训练和部署代理。这使得将RL应用于具有兴趣的新问题变得难以实现，尤其是对于缺乏工程专业技能的小型组织或个人。本文的观点认为，为了更广泛地推广RL的应用，研究社区需要将重点转向那些能够用用户友好的领域特定语言或自然语言描述环境的方法。除了提高RL的易用性之外，基于语言的环境描述还可能提供有价值的背景信息，并有助于训练代理在所有可用语言描述的所有环境中更好地进行泛化。 

---
# System-2 Mathematical Reasoning via Enriched Instruction Tuning 

**Title (ZH)**: 基于增强指令调优的System-2数学推理系统 

**Authors**: Huanqia Cai, Yijun Yang, Zhifeng Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.16964)  

**Abstract**: Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as ``meta-knowledge'' to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1\% on GSM8K and 32.5\% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods. 

**Abstract (ZH)**: 通过系统二推理解决复杂的数学问题是人类的一项自然技能，但目前的大语言模型（LLMs）仍然面临显著的挑战。我们发现，缺乏详尽的多步推理数据是主要限制因素之一。为此，我们提出了丰富指令调优（EIT）方法，该方法通过结合人类和AI反馈，丰富现有的人类标注的数学数据集，生成精细的推理轨迹。这些数据集随后用于开源LLM的优化，提升其数学推理能力，而不依赖于任何形式的符号验证程序。具体而言，EIT 包括两个关键步骤：推理计划丰富化（ERP）和推理步骤丰富化（ERS）。其中，ERP 生成一个高层次的计划，将复杂指令分解为一系列较简单的目标，而ERS则填补了人类标注者常常忽视的推理上下文，为LLM的优化创建更平滑的推理轨迹。不同于现有的仅依靠LLM内部知识生成推理链的方法，我们的方法利用人类标注的初始答案作为“元知识”，帮助LLM生成更详细和精确的推理过程，从而为解决复杂数学问题提供更可信的LLM专家。在实验中，EIT 在GSM8K 上达到84.1% 的准确率，在MATH 上达到32.5% 的准确率，超过了最先进的调优和提示方法，并且其性能甚至可以匹资产管理器增强方法。 

---
# PsychAdapter: Adapting LLM Transformers to Reflect Traits, Personality and Mental Health 

**Title (ZH)**: PsychAdapter: 将大型语言模型 Transformers 调适以反映个性特质、人格特征及心理健康 

**Authors**: Huy Vu, Huy Anh Nguyen, Adithya V Ganesan, Swanie Juhng, Oscar N.E. Kjell, Joao Sedoc, Margaret L. Kern, Ryan L. Boyd, Lyle Ungar, H. Andrew Schwartz, Johannes C. Eichstaedt  

**Link**: [PDF](https://arxiv.org/pdf/2412.16882)  

**Abstract**: Artificial intelligence-based language generators are now a part of most people's lives. However, by default, they tend to generate "average" language without reflecting the ways in which people differ. Here, we propose a lightweight modification to the standard language model transformer architecture - "PsychAdapter" - that uses empirically derived trait-language patterns to generate natural language for specified personality, demographic, and mental health characteristics (with or without prompting). We applied PsychAdapters to modify OpenAI's GPT-2, Google's Gemma, and Meta's Llama 3 and found generated text to reflect the desired traits. For example, expert raters evaluated PsychAdapter's generated text output and found it matched intended trait levels with 87.3% average accuracy for Big Five personalities, and 96.7% for depression and life satisfaction. PsychAdapter is a novel method to introduce psychological behavior patterns into language models at the foundation level, independent of prompting, by influencing every transformer layer. This approach can create chatbots with specific personality profiles, clinical training tools that mirror language associated with psychological conditionals, and machine translations that match an authors reading or education level without taking up LLM context windows. PsychAdapter also allows for the exploration psychological constructs through natural language expression, extending the natural language processing toolkit to study human psychology. 

**Abstract (ZH)**: 基于人工智能的语言生成器现在已经融入了大多数人的生活中。然而，它们默认生成的语言倾向于“平均化”，未能反映人们之间的差异。本文提出了一种对标准语言模型变换器架构进行轻量级修改的方法——“PsychAdapter”，该方法利用经验提取出的性格-语言模式生成具有指定人格、人口统计和心理健康特征的自然语言（有或无提示）。我们将PsychAdapter应用于修改OpenAI的GPT-2、Google的Gemma和Meta的Llama 3，并发现生成的文字反映了所需的性格特征。例如，专家评审员评估了PsychAdapter生成的文字输出，结果显示，对于五大人格特质，基因型匹配的平均准确率为87.3%，对于抑郁和生活满意度，匹配的准确率则高达96.7%。PsychAdapter是一种在基础层引入心理行为模式到语言模型的方法，这种做法独立于提示，通过影响每个变换器层来实现。这种方法可以生成具有特定人格特征的聊天机器人，开发反映心理状况的语言的临床培训工具，以及匹配作者阅读水平或教育水平的机器翻译，而无需占用LLM的上下文窗口。此外，PsychAdapter还可以促进通过自然语言表达探索心理构建，扩展自然语言处理工具箱，以研究人类心理。 

---
# A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information 

**Title (ZH)**: 使用语音和文本信息的多模态方法进行构音障碍检测和严重程度评估 

**Authors**: Anuprabha M, Krishna Gurugubelli, Kesavaraj V, Anil Kumar Vuppala  

**Link**: [PDF](https://arxiv.org/pdf/2412.16874)  

**Abstract**: Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses. 

**Abstract (ZH)**: 自动检测和评估构音障碍（dysarthria）及其严重程度对于提供针对性的治疗干预至关重要。尽管现有的大多数研究主要集中在言语模态上，本研究提出了一种新颖的方法，该方法同时利用了言语和文本模态。通过运用交叉注意力机制，我们的方法学会了言语和文本表示之间的声学和语言相似性。该方法特别评估了不同严重程度下的发音偏差，从而提高了构音障碍检测和严重程度评估的准确性。所有实验均在UA-Speech构音障碍数据库中进行。当使用依赖说话者和独立说话者，未见过的和见过的词语设置时，检测准确率分别达到了99.53%和93.20%，严重程度评估的准确率分别为98.12%和51.97%。这些发现表明，通过整合文本信息，这种提供参考语言知识的方法，为构音障碍的检测和评估建立了一个更加稳健的框架，从而可能有助于更有效的诊断。 

---
# OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning 

**Title (ZH)**: OpenRFT：通过强化微调适应领域特定任务的推理基础模型 

**Authors**: Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, Jitao Sang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16849)  

**Abstract**: OpenAI's recent introduction of Reinforcement Fine-Tuning (RFT) showcases the potential of reasoning foundation model and offers a new paradigm for fine-tuning beyond simple pattern imitation. This technical report presents \emph{OpenRFT}, our attempt to fine-tune generalist reasoning models for domain-specific tasks under the same settings as RFT. OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL. The evaluation is conducted on SciKnowEval, where OpenRFT achieves notable performance gains with only $100$ domain-specific samples for each task. More experimental results will be updated continuously in later versions. Source codes, datasets, and models are disclosed at: this https URL 

**Abstract (ZH)**: OpenAI最近推出的增强学习微调（Reinforcement Fine-Tuning，RFT）展示了推理基础模型的潜力，并提供了一种超越简单模式模仿的新微调范式。本技术报告介绍了一种名为\emph{OpenRFT}的尝试，我们在相同的RFT设置下对通用推理模型进行微调以适应特定领域任务。OpenRFT通过三种方式利用领域特定样本来应对缺乏推理步骤数据和训练样本量有限的两个关键挑战：问题增强、合成推理过程数据和少样本ICL（Instance Claire Learning）。

在SciKnowEval上进行了评估，结果显示，OpenRFT仅使用每个任务100个领域特定样本，就能显著提高性能。后续版本中将陆续更新更多实验结果。源代码、数据集和模型将在以下网址公开：this https URL 

---
# Online Learning from Strategic Human Feedback in LLM Fine-Tuning 

**Title (ZH)**: 在线学习：战略人类反馈在大语言模型微调中的应用 

**Authors**: Shugang Hao, Lingjie Duan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16834)  

**Abstract**: Reinforcement learning from human feedback (RLHF) has become an essential step in fine-tuning large language models (LLMs) to align them with human preferences. However, human labelers are selfish and have diverse preferences. They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences. Current practice simply averages labelers' feedback per time and fails to identify the most accurate human labeler, leading to linear regret $\mathcal{O}(T)$ for $T$ time slots. To our best knowledge, we are the first to study online learning mechanisms against strategic human labelers in the LLM fine-tuning process. We formulate a new dynamic Bayesian game and dynamically adjust human labelers' weights in the preference aggregation, ensuring their truthful feedback and sublinear regret $\mathcal{O}(T^{1/2})$. Simulation results demonstrate our mechanism's great advantages over the existing benchmark schemes. 

**Abstract (ZH)**: 从人类反馈进行强化学习（Reinforcement Learning from Human Feedback，RLHF）已经成为将大型语言模型（Large Language Models，LLMs）调整到与人类偏好一致的重要步骤。然而，人类标注者往往自私并具有多元化的偏好，他们可能会战略性地错误报告在线反馈，以影响系统汇总结果以符合自己的偏好。当前的做法是简单地对每个时间点的标注反馈进行平均，未能识别出最准确的人类标注者，导致线性后悔 $\mathcal{O}(T)$，其中 $T$ 表示时间槽的数量。据我们所知，我们是第一个在LLM微调过程中研究对抗战略性人类标注者的在线学习机制的研究。我们构建了一个新的动态贝叶斯博弈模型，并动态调整人类标注者在偏好汇总中的权重，确保其真实反馈并实现亚线性后悔 $\mathcal{O}(T^{1/2})$。仿真结果证明了我们机制相对于现有基准方案的巨大优势。 

---
# KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph Enhancement for Medical Diagnosis 

**Title (ZH)**: KG4Diagnosis：一种带有知识图谱增强的分层多代理大语言模型框架用于医疗诊断 

**Authors**: Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio  

**Link**: [PDF](https://arxiv.org/pdf/2412.16833)  

**Abstract**: Integrating Large Language Models (LLMs) in healthcare diagnosis demands systematic frameworks that can handle complex medical scenarios while maintaining specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent framework that combines LLMs with automated knowledge graph construction, encompassing 362 common diseases across medical specialties. Our framework mirrors real-world medical systems through a two-tier architecture: a general practitioner (GP) agent for initial assessment and triage, coordinating with specialized agents for in-depth diagnosis in specific domains. The core innovation lies in our end-to-end knowledge graph generation methodology, incorporating: (1) semantic-driven entity and relation extraction optimized for medical terminology, (2) multi-dimensional decision relationship reconstruction from unstructured medical texts, and (3) human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized medical diagnosis systems, with capabilities to incorporate new diseases and medical knowledge. The framework's modular design enables seamless integration of domain-specific enhancements, making it valuable for developing targeted medical diagnosis systems. We provide architectural guidelines and protocols to facilitate adoption across medical contexts. 

**Abstract (ZH)**: 将大型语言模型（LLMs）集成到医疗诊断中，需要系统性的框架来处理复杂的医疗场景并保持专业化的专业知识。我们提出了KG4Diagnosis，这是一种新颖的分层多智能体框架，将LLMs与自动知识图谱构建相结合，涵盖了362种常见疾病，涉及多个医学领域。该框架通过二层架构模拟现实世界的医疗系统：初级诊断（GP）智能体负责初始评估和分流，协调特定领域的专门智能体进行深入诊断。核心创新在于我们端到端的知识图谱生成方法，包括：（1）基于语义的实体和关系抽取，优化用于医学术语，（2）从非结构化医学文本中重构多维度的决策关系，（3）人引导的推理以扩展知识。KG4Diagnosis 作为一个可扩展的基础框架，具有纳入新疾病和医学知识的能力。该框架的模块化设计使其能够无缝集成特定领域的增强功能，使其适用于开发针对性的医疗诊断系统。我们提供了构架指南和协议，以便在不同医疗场景中促进其采纳。 

---
# Visual Prompting with Iterative Refinement for Design Critique Generation 

**Title (ZH)**: 迭代改进的视觉提示生成设计批评方法 

**Authors**: Peitong Duan, Chin-Yi Chen, Bjoern Hartmann, Yang Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.16829)  

**Abstract**: Feedback is crucial for every design process, such as user interface (UI) design, and automating design critiques can significantly improve the efficiency of the design workflow. Although existing multimodal large language models (LLMs) excel in many tasks, they often struggle with generating high-quality design critiques -- a complex task that requires producing detailed design comments that are visually grounded in a given design's image. Building on recent advancements in iterative refinement of text output and visual prompting methods, we propose an iterative visual prompting approach for UI critique that takes an input UI screenshot and design guidelines and generates a list of design comments, along with corresponding bounding boxes that map each comment to a specific region in the screenshot. The entire process is driven completely by LLMs, which iteratively refine both the text output and bounding boxes using few-shot samples tailored for each step. We evaluated our approach using Gemini-1.5-pro and GPT-4o, and found that human experts generally preferred the design critiques generated by our pipeline over those by the baseline, with the pipeline reducing the gap from human performance by 50% for one rating metric. To assess the generalizability of our approach to other multimodal tasks, we applied our pipeline to open-vocabulary object and attribute detection, and experiments showed that our method also outperformed the baseline. 

**Abstract (ZH)**: 反馈对于每一个设计过程都是至关重要的，无论是用户界面（UI）设计。自动化设计审查可以显著提高设计流程的效率。尽管现有的多模态大规模语言模型（LLMs）在许多任务中表现出色，但它们在生成高质量的设计评论方面经常遇到困难——这是一个复杂的任务，需要生成在给定设计的图像中具有视觉依据的详细设计评论。基于最近在文本输出迭代优化和视觉提示方法方面的进展，我们提出了一种迭代视觉提示方法，用于UI批评。该方法接受输入的UI屏幕截图和设计准则，并生成一份设计评论列表，以及与每个评论对应的边界框，将每个评论映射到屏幕截图的具体区域。整个过程完全由LLMs驱动，它们在每个步骤中使用针对每一步定制的少量示例样本逐步优化文本输出和边界框。我们使用Gemini-1.5-pro和GPT-4o评估了我们的方法，并发现人类专家通常更喜欢我们pipeline生成的设计评论，与基线方法相比，pipeline使某一评价指标的人类表现差距减少了50%。为了评估我们方法在其他多模态任务中的通用性，我们将pipeline应用于具有开放词汇表的对象和属性检测任务，实验结果表明我们的方法在所有任务中都超过了基线方法。 

---
# Unsupervised Discovery of Formulas for Mathematical Constants 

**Title (ZH)**: 无监督发现数学常数的公式 

**Authors**: Michael Shalyt, Uri Seligmann, Itay Beit Halachmi, Ofir David, Rotem Elimelech, Ido Kaminer  

**Link**: [PDF](https://arxiv.org/pdf/2412.16818)  

**Abstract**: Ongoing efforts that span over decades show a rise of AI methods for accelerating scientific discovery, yet accelerating discovery in mathematics remains a persistent challenge for AI. Specifically, AI methods were not effective in creation of formulas for mathematical constants because each such formula must be correct for infinite digits of precision, with "near-true" formulas providing no insight toward the correct ones. Consequently, formula discovery lacks a clear distance metric needed to guide automated discovery in this realm.
In this work, we propose a systematic methodology for categorization, characterization, and pattern identification of such formulas. The key to our methodology is introducing metrics based on the convergence dynamics of the formulas, rather than on the numerical value of the formula. These metrics enable the first automated clustering of mathematical formulas. We demonstrate this methodology on Polynomial Continued Fraction formulas, which are ubiquitous in their intrinsic connections to mathematical constants, and generalize many mathematical functions and structures.
We test our methodology on a set of 1,768,900 such formulas, identifying many known formulas for mathematical constants, and discover previously unknown formulas for $\pi$, $\ln(2)$, Gauss', and Lemniscate's constants. The uncovered patterns enable a direct generalization of individual formulas to infinite families, unveiling rich mathematical structures. This success paves the way towards a generative model that creates formulas fulfilling specified mathematical properties, accelerating the rate of discovery of useful formulas. 

**Abstract (ZH)**: 数十年来持续的努力表明，人工智能方法在加速科学发现方面取得了进展，但在数学发现的加速方面，人工智能仍面临持续的挑战。特别是，人工智能方法在创建数学常数的公式方面并不有效，因为每个这样的公式必须在无限位精度下正确，因此“接近正确”的公式提供不了通往正确公式的心得。因此，公式发现缺乏一种清晰的距离度量，而这正是指导此类领域自动化发现所需要的。

在本工作中，我们提出了一种系统的方法，用于分类、表征和识别此类公式的模式。我们方法的核心是引入基于公式收敛动态的度量，而不是基于公式的数值值。这些度量使得能够在数学公式中首次实现自动聚类。我们通过对多项式连分数公式进行了测试，这些公式由于其内在与数学常数的联系而普遍存在，并且能够涵盖许多数学函数和结构。

我们对1,768,900个此类公式进行了测试，识别出许多已知的数学常数公式，并发现了π、ln(2)、高斯常数和莱努兹常数的新公式。发现的模式使得个人公式可以直接推广到无限家族，揭示出丰富的数学结构。这一成功为生成模型创建符合特定数学性质的公式铺平了道路，从而加速了有用公式发现的进程。 

---
# An Exploration of Pattern Mining with ChatGPT 

**Title (ZH)**: 基于ChatGPT的模式挖掘探索 

**Authors**: Michael Weiss  

**Link**: [PDF](https://arxiv.org/pdf/2412.16814)  

**Abstract**: This paper takes an exploratory approach to examine the use of ChatGPT for pattern mining. It proposes an eight-step collaborative process that combines human insight with AI capabilities to extract patterns from known uses. The paper offers a practical demonstration of this process by creating a pattern language for integrating Large Language Models (LLMs) with data sources and tools. LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video. The paper also argues for adding affordances of the underlying components as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in pattern mining using LLMs. 

**Abstract (ZH)**: 本文采用探索性方法研究ChatGPT在模式挖掘中的应用。该文提出了一种八步协作过程，结合人类洞察与人工智能能力，从已知用途中提取模式。文章通过创建一种语言模型（LLM）与数据源和工具集成的模式语言，提供了这一过程的应用示范。语言模型（如ChatGPT）是一种新型的人工智能模型，经过大量文本训练，能够生成新的内容，包括文本、图像或视频。此外，该文还主张在模式描述中增加底层组件的功能要素。本文的主要读者群体包括对使用LLM进行模式挖掘感兴趣的模式撰写者。 

---
# DCOR: Anomaly Detection in Attributed Networks via Dual Contrastive Learning Reconstruction 

**Title (ZH)**: DCOR：基于双面对比学习重建的有属性网络异常检测 

**Authors**: Hossein Rafiee Zade, Hadi Zare, Mohsen Ghassemi Parsa, Hadi Davardoust, Meshkat Shariat Bagheri  

**Link**: [PDF](https://arxiv.org/pdf/2412.16788)  

**Abstract**: Anomaly detection using a network-based approach is one of the most efficient ways to identify abnormal events such as fraud, security breaches, and system faults in a variety of applied domains. While most of the earlier works address the complex nature of graph-structured data and predefined anomalies, the impact of data attributes and emerging anomalies are often neglected. This paper introduces DCOR, a novel approach on attributed networks that integrates reconstruction-based anomaly detection with Contrastive Learning. Utilizing a Graph Neural Network (GNN) framework, DCOR contrasts the reconstructed adjacency and feature matrices from both the original and augmented graphs to detect subtle anomalies. We employed comprehensive experimental studies on benchmark datasets through standard evaluation measures. The results show that DCOR significantly outperforms state-of-the-art methods. Obtained results demonstrate the efficacy of proposed approach in attributed networks with the potential of uncovering new patterns of anomalies. 

**Abstract (ZH)**: 基于网络的方法在识别欺诈、安全漏洞和系统故障等异常事件方面是效率极高的方式，适用于多种应用领域。尽管大多数早期工作关注图结构数据的复杂性和预定义的异常，但数据属性和新兴异常的影响往往被忽视。本文介绍了一种名为DCOR的新方法，该方法将基于重构的异常检测与对比学习结合在一起，应用于属性网络中。利用图神经网络（GNN）框架，DCOR对比原始图和增强图的重构邻接矩阵和特征矩阵，以检测细微异常。我们通过标准评估指标在基准数据集上进行了全面的实验研究。结果表明，DCOR显著优于现有方法。获得的结果证明了所提出方法在属性网络中的有效性，并且具有发现新异常模式的潜力。 

---
# Reasoning about Actual Causes in Nondeterministic Domains -- Extended Version 

**Title (ZH)**: 在非确定性领域推理实际原因——扩展版 

**Authors**: Shakil M. Khan, Yves Lespérance, Maryam Rostamigiv  

**Link**: [PDF](https://arxiv.org/pdf/2412.16728)  

**Abstract**: Reasoning about the causes behind observations is crucial to the formalization of rationality. While extensive research has been conducted on root cause analysis, most studies have predominantly focused on deterministic settings. In this paper, we investigate causation in more realistic nondeterministic domains, where the agent does not have any control on and may not know the choices that are made by the environment. We build on recent preliminary work on actual causation in the nondeterministic situation calculus to formalize more sophisticated forms of reasoning about actual causes in such domains. We investigate the notions of ``Certainly Causes'' and ``Possibly Causes'' that enable the representation of actual cause for agent actions in these domains. We then show how regression in the situation calculus can be extended to reason about such notions of actual causes. 

**Abstract (ZH)**: 对观察现象背后原因的推理对于理性的正式化至关重要。虽然针对根本原因分析的研究已经非常广泛，但大多数研究主要集中在确定性环境中。本文我们探讨了在更具现实性的非确定性领域中的因果关系，其中智能体对环境所做的选择既没有控制权也不一定了解。我们基于近期关于非确定性情况 calculus 中实际因果关系的初步研究工作，进一步形式化了这类环境中的更为复杂的实际因果关系推理形式。我们探讨了“必定导致”和“可能导致”等概念，这些概念能够表示这些领域中智能体行动的实际原因。随后，我们展示了如何通过扩展情况 calculus 中的回归方法来推理这些实际因果关系的概念。 

---
# Argumentation Computation with Large Language Models : A Benchmark Study 

**Title (ZH)**: 大规模语言模型中的论辩计算：一项基准研究 

**Authors**: Zhaoqun Li, Xiaotong Fang, Chen Chen, Mengze Li, Beishui Liao  

**Link**: [PDF](https://arxiv.org/pdf/2412.16725)  

**Abstract**: In recent years, large language models (LLMs) have made significant advancements in neuro-symbolic computing. However, the combination of LLM with argumentation computation remains an underexplored domain, despite its considerable potential for real-world applications requiring defeasible reasoning. In this paper, we aim to investigate the capability of LLMs in determining the extensions of various abstract argumentation semantics. To achieve this, we develop and curate a benchmark comprising diverse abstract argumentation frameworks, accompanied by detailed explanations of algorithms for computing extensions. Subsequently, we fine-tune LLMs on the proposed benchmark, focusing on two fundamental extension-solving tasks. As a comparative baseline, LLMs are evaluated using a chain-of-thought approach, where they struggle to accurately compute semantics. In the experiments, we demonstrate that the process explanation plays a crucial role in semantics computation learning. Models trained with explanations show superior generalization accuracy compared to those trained solely with question-answer pairs. Furthermore, by leveraging the self-explanation capabilities of LLMs, our approach provides detailed illustrations that mitigate the lack of transparency typically associated with neural networks. Our findings contribute to the broader understanding of LLMs' potential in argumentation computation, offering promising avenues for further research in this domain. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在神经符号计算方面取得了显著进展。然而，尽管架设LLMs与论证计算相结合具有重要的现实应用潜力，尤其是在要求进行可撤销推理的情况下，这一领域仍相对未被充分探索。本文旨在探讨LLMs在确定各种抽象论证语义扩展方面的潜力。为此，我们构建并整理了一个基准数据集，该数据集包含多种不同的抽象论证框架，并附带上详细的算法解释。接着，我们针对两个基本的扩展求解任务对LLMs进行微调。为了进行比较，我们采用链式思考的方法评估LLMs，结果显示它们在准确计算语义方面存在困难。在实验中，我们证明了过程解释在语义计算学习中的关键作用。使用带有解释训练的模型表现出比仅使用问答对训练的模型更好的泛化准确性。此外，通过利用LLMs的自我解释能力，我们的方法提供了详细的示例图，减轻了神经网络固有的透明度不足问题。我们的发现为更广泛理解LLMs在论证计算中的潜力做出了贡献，并为该领域的进一步研究提供了有希望的途径。 

---
# OpenAI o1 System Card 

**Title (ZH)**: OpenAI 系统卡 

**Authors**: OpenAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya  

**Link**: [PDF](https://arxiv.org/pdf/2412.16720)  

**Abstract**: The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. 

**Abstract (ZH)**: 以下是对给出内容的翻译，符合学术规范：

o1模型系列通过大规模强化学习训练，利用逻辑推理链进行推理。这些高级推理能力为提高我们模型的安全性和鲁棒性提供了新的途径。特别是，我们的模型可以在响应潜在不安全的提示时，通过审慎对齐来推理我们的安全政策，从而能够在上下文中进行安全考量。这在某些基准测试中实现了生成非法建议、选择刻板回应以及应对已知逃逸攻击方面的最新性能。在回答问题之前引入逻辑推理链的训练有可能解锁大量益处，同时也增加了源自更高智能的潜在风险。我们的研究结果强调了构建稳健的对齐方法、广泛测试其有效性以及维护细致的风险管理协议的必要性。本报告概述了对OpenAI o1和OpenAI o1-mini模型所进行的安全工作，包括安全性评估、外部红队测试以及应急框架评估。 

---
# FAP-CD: Fairness-Driven Age-Friendly Community Planning via Conditional Diffusion Generation 

**Title (ZH)**: FAP-CD：公平驱动的年龄友好社区规划——基于条件扩散生成方法 

**Authors**: Jinlin Li, Xintong Li, Xiao Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.16699)  

**Abstract**: As global populations age rapidly, incorporating age-specific considerations into urban planning has become essential to addressing the urgent demand for age-friendly built environments and ensuring sustainable urban development. However, current practices often overlook these considerations, resulting in inadequate and unevenly distributed elderly services in cities. There is a pressing need for equitable and optimized urban renewal strategies to support effective age-friendly planning. To address this challenge, we propose a novel framework, Fairness-driven Age-friendly community Planning via Conditional Diffusion generation (FAP-CD). FAP-CD leverages a conditioned graph denoising diffusion probabilistic model to learn the joint probability distribution of aging facilities and their spatial relationships at a fine-grained regional level. Our framework generates optimized facility distributions by iteratively refining noisy graphs, conditioned on the needs of the elderly during the diffusion process. Key innovations include a demand-fairness pre-training module that integrates community demand features and facility characteristics using an attention mechanism and min-max optimization, ensuring equitable service distribution across regions. Additionally, a discrete graph structure captures walkable accessibility within regional road networks, guiding model sampling. To enhance information integration, we design a graph denoising network with an attribute augmentation module and a hybrid graph message aggregation module, combining local and global node and edge information. Empirical results across multiple metrics demonstrate the effectiveness of FAP-CD in balancing age-friendly needs with regional equity, achieving an average improvement of 41% over competitive baseline models. 

**Abstract (ZH)**: 随着全球人口迅速老龄化，将年龄特异性考虑纳入城市规划已成为应对适老化环境需求和确保可持续城市发展的重要任务。然而，当前实践往往忽略了这些考虑，导致城市中老年人服务不足且分布不均。迫切需要公平且优化的城市更新策略以支持有效的适老化规划。为了应对这一挑战，我们提出了一种新颖的框架——基于条件扩散生成的公平适老社区规划（FAP-CD）框架。FAP-CD 利用一个条件下的图去噪扩散概率模型，在区域层面学习老化设施及其空间关系的联合概率分布。我们的框架通过在扩散过程中迭代优化噪声图来生成优化的设施分布，并以老年人的需求为条件。关键创新包括一个需求公平预训练模块，该模块通过注意力机制和极小-极大优化整合社区需求特征和设施特性，确保区域间服务分配的公平性。此外，离散图结构捕捉了区域道路交通网络中的可达性，引导模型采样。为了增强信息集成，我们设计了一个图去噪网络，包含一个属性增强模块和一个混合图消息聚合模块，结合了局部和全局节点及边的信息。在多个评估指标上的实证结果表明，FAP-CD 在平衡适老化需求与区域公平性方面具有有效性，相对于竞争的基线模型，平均改进率达到41%。 

---
# Formal Language Knowledge Corpus for Retrieval Augmented Generation 

**Title (ZH)**: 形式语言知识库支持检索增强生成 

**Authors**: Majd Zayyad, Yossi Adi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16689)  

**Abstract**: The integration of retrieval-augmented techniques with LLMs has shown promise in improving performance across various domains. However, their utility in tasks requiring advanced reasoning, such as generating and evaluating mathematical statements and proofs, remains underexplored. This study explores the use of Lean, a programming language for writing mathematical proofs, to populate the knowledge corpus used by RAG systems. We hope for this to lay the foundation to exploring different methods of using RAGs to improve the performance of LLMs in advanced logical reasoning tasks. 

**Abstract (ZH)**: 将检索增强技术与大规模语言模型（LLM）的集成已在多个领域提高了性能，显示出很大的潜力。然而，这些技术在要求进行高级推理的任务中（如生成和评估数学陈述和证明）的应用还很少被探索。本研究旨在利用Lean（一种用于编写数学证明的编程语言）来填充检索增强生成（RAG）系统所使用的知识库，希望通过此举为探索不同的RAG使用方法以提高LLM在高级逻辑推理任务中的性能奠定基础。 

---
# STAMPsy: Towards SpatioTemporal-Aware Mixed-Type Dialogues for Psychological Counseling 

**Title (ZH)**: STAMPsy：迈向空间时间感知的混合型对话心理辅导系统 

**Authors**: Jieyi Wang, Yue Huang, Zeming Liu, Dexuan Xu, Chuan Wang, Xiaoming Shi, Ruiyuan Guan, Hongxing Wang, Weihua Yue, Yu Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16674)  

**Abstract**: Online psychological counseling dialogue systems are trending, offering a convenient and accessible alternative to traditional in-person therapy. However, existing psychological counseling dialogue systems mainly focus on basic empathetic dialogue or QA with minimal professional knowledge and without goal guidance. In many real-world counseling scenarios, clients often seek multi-type help, such as diagnosis, consultation, therapy, console, and common questions, but existing dialogue systems struggle to combine different dialogue types naturally. In this paper, we identify this challenge as how to construct mixed-type dialogue systems for psychological counseling that enable clients to clarify their goals before proceeding with counseling. To mitigate the challenge, we collect a mixed-type counseling dialogues corpus termed STAMPsy, covering five dialogue types, task-oriented dialogue for diagnosis, knowledge-grounded dialogue, conversational recommendation, empathetic dialogue, and question answering, over 5,000 conversations. Moreover, spatiotemporal-aware knowledge enables systems to have world awareness and has been proven to affect one's mental health. Therefore, we link dialogues in STAMPsy to spatiotemporal state and propose a spatiotemporal-aware mixed-type psychological counseling dataset. Additionally, we build baselines on STAMPsy and develop an iterative self-feedback psychological dialogue generation framework, named Self-STAMPsy. Results indicate that clarifying dialogue goals in advance and utilizing spatiotemporal states are effective. 

**Abstract (ZH)**: 在线心理咨询服务正逐渐流行，为传统的面对面治疗提供了方便且易于访问的替代方案。然而，现有的心理咨询服务对话系统主要集中在基本的共情对话或带有极少专业知识的问答，缺乏目标引导。在许多实际的咨询场景中，客户往往寻求多种类型的支持，如诊断、咨询、心理治疗、安慰和常见问题解答，但现有的对话系统难以自然地结合不同类型的对话。在本文中，我们将这一挑战定义为如何构建心理咨询服务中的混合型对话系统，以使客户在进行咨询前能够明确其目标。为了缓解这一挑战，我们收集了一个名为STAMPsy的混合型咨询对话语料库，涵盖了五种对话类型：面向任务的诊断对话、基于知识的对话、对话推荐、共情对话以及问答对话，共计超过5000场对话。此外，时空感知知识能使系统具备世界意识，已被证明能够影响一个人的心理健康。因此，我们将STAMPsy中的对话与时空状态相关联，提出了一种时空感知的混合型心理咨询服务数据集。此外，我们基于STAMPsy构建了基线模型，并开发了一个迭代自反馈的心理咨询服务生成框架，名为Self-STAMPsy。实验结果表明，在咨询前明确对话目标和利用时空状态是有效的。 

---
# On Enhancing Network Throughput using Reinforcement Learning in Sliced Testbeds 

**Title (ZH)**: 在分片测试床中使用强化学习提高网络吞吐量的研究 

**Authors**: Daniel Pereira Monteiro, Lucas Nardelli de Freitas Botelho Saar, Larissa Ferreira Rodrigues Moreira, Rodrigo Moreira  

**Link**: [PDF](https://arxiv.org/pdf/2412.16673)  

**Abstract**: Novel applications demand high throughput, low latency, and high reliability connectivity and still pose significant challenges to slicing orchestration architectures. The literature explores network slicing techniques that employ canonical methods, artificial intelligence, and combinatorial optimization to address errors and ensure throughput for network slice data plane. This paper introduces the Enhanced Mobile Broadband (eMBB)-Agent as a new approach that uses Reinforcement Learning (RL) in a vertical application to enhance network slicing throughput to fit Service-Level Agreements (SLAs). The eMBB-Agent analyzes application transmission variables and proposes actions within a discrete space to adjust the reception window using a Deep Q-Network (DQN). This paper also presents experimental results that examine the impact of factors such as the channel error rate, DQN model layers, and learning rate on model convergence and achieved throughput, providing insights on embedding intelligence in network slicing. 

**Abstract (ZH)**: 新型应用对高吞吐量、低时延和高可靠性的连接提出了新的需求，这给网络切片编排架构带来了巨大挑战。文献中探讨了利用经典方法、人工智能和组合优化的网络切片技术，以解决错误并确保网络切片子网数据平面的吞吐量。本文介绍了一种新的方法——增强型移动宽带（eMBB）代理，该方法在垂直应用中采用强化学习（Reinforcement Learning, RL）来提高网络切片吞吐量，以满足服务级别协议（Service-Level Agreements, SLAs）的要求。eMBB代理分析应用程序传输变量，并在离散空间中提出动作，利用深度Q网络（Deep Q-Network, DQN）调整接收窗口。本文还介绍了实验结果，这些结果探讨了信道错误率、DQN模型层和学习率等因素对模型收敛和吞吐量实现的影响，提供了将智能嵌入到网络切片中的见解。 

---
# Internalized Self-Correction for Large Language Models 

**Title (ZH)**: 大型语言模型内部化的自我校正机制 

**Authors**: Nishanth Upadhyaya, Raghavendra Sridharamurthy  

**Link**: [PDF](https://arxiv.org/pdf/2412.16653)  

**Abstract**: In this article, we introduce 'Internalized Self-Correction' (InSeC) for large language models (LLMs). While many approaches exist for self-reflection at inference time, we propose a novel method that combines ideas from negative sampling, self-reflection during training, and inference time. InSeC allows LLMs to correct themselves by introducing mistakes and their corresponding corrections during training, thereby converting the learning process into a true supervised learning task with both positive and negative examples. This approach can be extended to improve instruction following and correct hallucinations or incorrect sentences generated by LLMs. 

**Abstract (ZH)**: 在本文中，我们介绍了“内化自我矫正”（InSeC）机制，用于大规模语言模型（LLMs）。虽然在推理阶段存在许多自我反思的方法，但我们提出了一种新的方法，该方法结合了负采样、训练期间的自我反思和推理阶段的自我反思思想。InSeC 允许在训练过程中通过引入错误及其相应的纠正内容，使LLMs 自我纠正，从而将学习过程转化为具有正负样本的真正监督学习任务。这种方法可以扩展以提高指令跟随能力，并纠正LLMs 生成的幻觉或错误句子。 

---
# TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation 

**Title (ZH)**: TimeRAG：通过检索增强生成提高大规模语言模型的时间序列预测能力 

**Authors**: Silin Yang, Dong Wang, Haoqi Zheng, Ruochun Jin  

**Link**: [PDF](https://arxiv.org/pdf/2412.16643)  

**Abstract**: Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average. 

**Abstract (ZH)**: 虽然大型语言模型（LLM）的兴起为时间序列预测带来了新的机遇，但现有的基于LLM的解决方案需要大量的训练，并且表现出有限的迁移性。鉴于这些挑战，我们提出了一种名为TimeRAG的框架，该框架将检索增强生成（RAG）融入时间序列预测的LLM中。TimeRAG从历史序列构建时间序列知识库，通过动态时间规整（DTW）检索与查询序列表现出相似模式的参考序列，并将这些参考序列与预测查询结合成文本提示，传入时间序列预测的LLM。实验结果表明，RAG的集成使原始模型的预测准确率平均提高了2.97%。 

---
# A Systems Thinking Approach to Algorithmic Fairness 

**Title (ZH)**: 基于系统思考的算法公平性方法 

**Authors**: Chris Lam  

**Link**: [PDF](https://arxiv.org/pdf/2412.16641)  

**Abstract**: Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then model this using a series of causal graphs, enabling us to link AI/ML systems to politics and the law. By treating the fairness problem as a complex system, we can combine techniques from machine learning, causal inference, and system dynamics. Each of these analytical techniques is designed to capture different emergent aspects of fairness, allowing us to develop a deeper and more holistic view of the problem. This can help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a blueprint for designing AI policy that is aligned to their political agendas. 

**Abstract (ZH)**: 系统思维为我们提供了一种通过允许我们编码关于数据生成过程中我们相信可能存在偏见的先验知识和假设来建模算法公平性问题的方法。我们可以通过一系列因果图来构建这种模型，从而将人工智能/机器学习系统与政治和法律联系起来。通过将公平性问题视为一个复杂系统，我们可以结合机器学习、因果推断和系统动力学的技术。每种分析技术都旨在捕捉公平性不同方面的发展，从而使我们能够形成对问题更深入和全面的理解。这有助于两党政策制定者理解不同类型公平性政策存在的复杂权衡，为制定符合其政治议程的人工智能政策提供蓝图。 

---
# Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning 

**Title (ZH)**: 多模态语言模型真的理解方向吗？一种关于指南针方向推理的标准评估方法 

**Authors**: Hang Yin, Zhifeng Lin, Xin Liu, Bin Sun, Kan Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.16599)  

**Abstract**: Direction reasoning is essential for intelligent systems to understand the real world. While existing work focuses primarily on spatial reasoning, compass direction reasoning remains underexplored. To address this, we propose the Compass Direction Reasoning (CDR) benchmark, designed to evaluate the direction reasoning capabilities of multimodal language models (MLMs). CDR includes three types images to test spatial (up, down, left, right) and compass (north, south, east, west) directions. Our evaluation reveals that most MLMs struggle with direction reasoning, often performing at random guessing levels. Experiments show that training directly with CDR data yields limited improvements, as it requires an understanding of real-world physical rules. We explore the impact of mixdata and CoT fine-tuning methods, which significantly enhance MLM performance in compass direction reasoning by incorporating diverse data and step-by-step reasoning, improving the model's ability to understand direction relationships. 

**Abstract (ZH)**: 方向推理对于智能系统理解现实世界至关重要。尽管现有的研究主要集中在空间推理上，但指南针方向推理仍然未被充分探索。为解决这一问题，我们提出了一种指南针方向推理（CDR）基准测试，旨在评估多模态语言模型（MLMs）的方向推理能力。CDR包括三种类型的照片来测试空间方向（上、下、左、右）和指南针方向（北、南、东、西）。我们的评估结果显示，大多数MLMs在方向推理方面存在困难，经常表现为随机猜测的水平。实验表明，直接使用CDR数据进行训练所能带来的改进有限，因为这需要理解现实世界的物理规则。我们探索了掺混数据和逐步推理（CoT）微调方法的影响，这些方法通过引入多样化的数据和逐步推理，显著增强了MLMs在指南针方向推理方面的性能，提升了模型理解方向关系的能力。 

---
# Effective and Efficient Representation Learning for Flight Trajectories 

**Title (ZH)**: 飞行轨迹的有效且高效的表示学习 

**Authors**: Shuo Liu, Wenbin Li, Di Yao, Jingping Bi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16581)  

**Abstract**: Flight trajectory data plays a vital role in the traffic management community, especially for downstream tasks such as trajectory prediction, flight recognition, and anomaly detection. Existing works often utilize handcrafted features and design models for different tasks individually, which heavily rely on domain expertise and are hard to extend. We argue that different flight analysis tasks share the same useful features of the trajectory. Jointly learning a unified representation for flight trajectories could be beneficial for improving the performance of various tasks. However, flight trajectory representation learning (TRL) faces two primary challenges, \ie unbalanced behavior density and 3D spatial continuity, which disable recent general TRL methods. In this paper, we propose Flight2Vec , a flight-specific representation learning method to address these challenges. Specifically, a behavior-adaptive patching mechanism is used to inspire the learned representation to pay more attention to behavior-dense segments. Moreover, we introduce a motion trend learning technique that guides the model to memorize not only the precise locations, but also the motion trend to generate better representations. Extensive experimental results demonstrate that Flight2Vec significantly improves performance in downstream tasks such as flight trajectory prediction, flight recognition, and anomaly detection. 

**Abstract (ZH)**: 飞行轨迹数据在空中交通管理领域中发挥着至关重要的作用，尤其是在轨迹预测、飞行识别和异常检测等下游任务中。现有研究通常使用手工构建的特征，为不同的任务分别设计模型，这高度依赖于特定领域的专业知识，且难以扩展。我们认为，不同的飞行分析任务可以从飞行轨迹中共享有用特征中获益。联合学习一个统一的飞行轨迹表示方法有助于提高各种任务的性能。然而，飞行轨迹表示学习（Trajectory Representation Learning, TRL）面临着两个主要挑战，即行为密度不均衡和三维空间连续性，这使得最近通用的TRL方法难以适用。在本文中，我们提出了一种针对飞行的特定表示学习方法——Flight2Vec，以应对这些挑战。具体而言，我们采用一种行为自适应的块化机制来引导学习到的表示更加关注行为密集的部分。此外，我们引入了一种运动趋势学习技术，该技术引导模型不仅要记住精确的位置，还要记住运动趋势，从而生成更好的表示。广泛的实验结果表明，Flight2Vec 在飞行轨迹预测、飞行识别和异常检测等下游任务中显著提高了性能。 

---
# Metagoals Endowing Self-Modifying AGI Systems with Goal Stability or Moderated Goal Evolution: Toward a Formally Sound and Practical Approach 

**Title (ZH)**: 元目标：赋予自修改AGI系统目标稳定性或可控目标 evolution 的方法：一种形式上稳健且实用的途径 

**Authors**: Ben Goertzel  

**Link**: [PDF](https://arxiv.org/pdf/2412.16559)  

**Abstract**: We articulate here a series of specific metagoals designed to address the challenge of creating AGI systems that possess the ability to flexibly self-modify yet also have the propensity to maintain key invariant properties of their goal systems
1) a series of goal-stability metagoals aimed to guide a system to a condition in which goal-stability is compatible with reasonably flexible self-modification
2) a series of moderated-goal-evolution metagoals aimed to guide a system to a condition in which control of the pace of goal evolution is compatible with reasonably flexible self-modification
The formulation of the metagoals is founded on fixed-point theorems from functional analysis, e.g. the Contraction Mapping Theorem and constructive approximations to Schauder's Theorem, applied to probabilistic models of system behavior
We present an argument that the balancing of self-modification with maintenance of goal invariants will often have other interesting cognitive side-effects such as a high degree of self understanding
Finally we argue for the practical value of a hybrid metagoal combining moderated-goal-evolution with pursuit of goal-stability -- along with potentially other metagoals relating to goal-satisfaction, survival and ongoing development -- in a flexible fashion depending on the situation 

**Abstract (ZH)**: 以下是论文内容或标题的中文翻译，符合学术规范：

本文旨在概述一系列具体的元目标，以应对创建具备灵活自我修改能力但又能够保持关键目标系统不变性的AGI系统这一挑战：
1) 一系列目标稳定性的元目标，旨在引导系统处于目标稳定性和合理灵活的自我修改可以兼容的状态；
2) 一系列限速目标演化的元目标，旨在引导系统处于控制目标演化速度与合理灵活的自我修改可以兼容的状态。

这些元目标的制定基于功能分析中的不动点定理，如收缩映射定理及其对Schauder定理的构造性逼近方法，在概率模型的系统行为中得以应用。

我们提出了一项论断，即平衡自我修改与保持目标不变性的关系往往会产生其他有趣的认知副作用，如高度的自我理解。

最后，我们强调了将限速目标演化与追求目标稳定性的混合元目标（以及其他与目标实现、生存和持续发展相关的元目标）相结合的实用价值。这种结合方法应根据具体情况进行灵活运用。 

---
# CognTKE: A Cognitive Temporal Knowledge Extrapolation Framework 

**Title (ZH)**: CognTKE：一种认知时序知识外推框架 

**Authors**: Wei Chen, Yuting Wu, Shuhan Wu, Zhiyu Zhang, Mengqi Liao, Youfang Lin, Huaiyu Wan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16557)  

**Abstract**: Reasoning future unknowable facts on temporal knowledge graphs (TKGs) is a challenging task, holding significant academic and practical values for various fields. Existing studies exploring explainable reasoning concentrate on modeling comprehensible temporal paths relevant to the query. Yet, these path-based methods primarily focus on local temporal paths appearing in recent times, failing to capture the complex temporal paths in TKG and resulting in the loss of longer historical relations related to the query. Motivated by the Dual Process Theory in cognitive science, we propose a \textbf{Cogn}itive \textbf{T}emporal \textbf{K}nowledge \textbf{E}xtrapolation framework (CognTKE), which introduces a novel temporal cognitive relation directed graph (TCR-Digraph) and performs interpretable global shallow reasoning and local deep reasoning over the TCR-Digraph. Specifically, the proposed TCR-Digraph is constituted by retrieving significant local and global historical temporal relation paths associated with the query. In addition, CognTKE presents the global shallow reasoner and the local deep reasoner to perform global one-hop temporal relation reasoning (System 1) and local complex multi-hop path reasoning (System 2) over the TCR-Digraph, respectively. The experimental results on four benchmark datasets demonstrate that CognTKE achieves significant improvement in accuracy compared to the state-of-the-art baselines and delivers excellent zero-shot reasoning ability. \textit{The code is available at this https URL}. 

**Abstract (ZH)**: 关于时间知识图谱（Temporal Knowledge Graphs, TKGs）上预测未来未知事实的推理是一项具有重大学术和实践价值的挑战性任务。现有的可解释推理研究主要集中在建模与查询相关的可理解的时间路径。然而，现有的基于路径的方法主要关注最近时间出现的局部时间路径，未能捕捉到TKGs中的复杂时间路径，导致与查询相关的更长时间历史关系的丢失。受认知科学中的双重过程理论启发，我们提出了一种名为**认知**、**时间**、**知识**和**推理**框架（CognTKE），该框架引入了一种新颖的时间认知关系有向图（TCR-Digraph），并在TCR-Digraph上执行可解释的全局浅层推理和局部深层推理。具体而言，所提出的 TCR-Digraph 由检索与查询相关的显著局部和全局历史时间关系路径构成。此外，CognTKE 提供了一个全局浅层推理器和一个局部深层推理器，分别进行全局一次跳时间关系推理（System 1）和局部复杂多跳路径推理（System 2）。在四个基准数据集上的实验结果表明，CognTKE 在准确率方面显著优于最先进的基线，并且具备出色的一次性推理能力。**代码已在此处提供**。

请注意，最后的句子中的网址链接需要补充完整，例如：
```
The code is available at this URL: <https://github.com/yourusername/CognTKE>
```

这样可以确保文档的专业性和完整性。 

---
# ActPC-Chem: Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel for Hyperon & PRIMUS-Based AGI 

**Title (ZH)**: ActPC-Chem: 离散活性预测编码在高能&PRIMUS基于的超人工智能目标导向算法化学中的潜在认知内核 

**Authors**: Ben Goertzel  

**Link**: [PDF](https://arxiv.org/pdf/2412.16547)  

**Abstract**: We explore a novel paradigm (labeled ActPC-Chem) for biologically inspired, goal-guided artificial intelligence (AI) centered on a form of Discrete Active Predictive Coding (ActPC) operating within an algorithmic chemistry of rewrite rules. ActPC-Chem is envisioned as a foundational "cognitive kernel" for advanced cognitive architectures, such as the OpenCog Hyperon system, incorporating essential elements of the PRIMUS cognitive architecture. The central thesis is that general-intelligence-capable cognitive structures and dynamics can emerge in a system where both data and models are represented as evolving patterns of metagraph rewrite rules, and where prediction errors, intrinsic and extrinsic rewards, and semantic constraints guide the continual reorganization and refinement of these rules. Using a virtual "robot bug" thought experiment, we illustrate how such a system might self-organize to handle challenging tasks involving delayed and context-dependent rewards, integrating causal rule inference (AIRIS) and probabilistic logical abstraction (PLN) to discover and exploit conceptual patterns and causal constraints. Next, we describe how continuous predictive coding neural networks, which excel at handling noisy sensory data and motor control signals, can be coherently merged with the discrete ActPC substrate. Finally, we outline how these ideas might be extended to create a transformer-like architecture that foregoes traditional backpropagation in favor of rule-based transformations guided by ActPC. This layered architecture, supplemented with AIRIS and PLN, promises structured, multi-modal, and logically consistent next-token predictions and narrative sequences. 

**Abstract (ZH)**: 我们探索了一种新型范式（标记为ActPC-Chem），该范式以生物学启发、目标导向的人工智能（AI）为核心，基于一种形式的离散主动预测编码（ActPC），该编码在重写规则的算法化学中运行。ActPC-Chem 想象为高级认知架构（如 OpenCog Hyperon 系统）的基础“认知内核”，并融合了 PRIMUS 认知架构的关键要素。核心论点是，在一个数据和模型均表示为元图重写规则演变模式的系统中，预测错误、内在和外在奖励以及语义约束可以指导这些规则的持续重组与完善，从而促成通用智能能力的认知结构和动力学的涌现。我们通过一个虚拟的“机器人虫子”思想实验，说明这种系统如何自我组织以处理涉及延迟和上下文相关奖励的挑战性任务，利用因果规则推理 (AIRIS) 和概率逻辑抽象 (PLN) 发现和利用概念模式与因果约束。接下来，我们描述了如何将擅长处理嘈杂感官数据和运动控制信号的连续预测编码神经网络，与离散的 ActPC 子结构协调地结合在一起。最后，我们概述了如何将这些想法扩展为一种类似于Transformer的架构，该架构放弃传统的反向传播，而采用由ActPC指导的基于规则的转换。这种分层架构，结合AIRIS和PLN，承诺在结构化、多模态和逻辑一致的下一个标记预测和叙事序列方面提供支持。 

---
# Mathematics and Machine Creativity: A Survey on Bridging Mathematics with AI 

**Title (ZH)**: 数学与机器创造力：数学与AI融合综述 

**Authors**: Shizhe Liang, Wei Zhang, Tianyang Zhong  

**Link**: [PDF](https://arxiv.org/pdf/2412.16543)  

**Abstract**: This paper presents a comprehensive survey on the applications of artificial intelligence (AI) in mathematical research, highlighting the transformative role AI has begun to play in this domain. Traditionally, AI advancements have heavily relied on theoretical foundations from fields like mathematics and statistics. However, recent developments in AI, particularly in reinforcement learning (RL) and large language models (LLMs), have demonstrated the potential for AI to contribute back to mathematics, offering flexible algorithmic frameworks and powerful inductive reasoning capabilities that support various aspects of mathematical research. This survey aims to establish a bridge between AI and mathematics, providing insights into the mutual benefits and fostering deeper interdisciplinary understanding.
In particular, we argue that while current AI and LLMs may struggle with complex deductive reasoning, their inherent creativity holds significant potential to support and inspire mathematical research. This creative capability, often overlooked, could be the key to unlocking new perspectives and methodologies in mathematics. Furthermore, we address the lack of cross-disciplinary communication: mathematicians may not fully comprehend the latest advances in AI, while AI researchers frequently prioritize benchmarks and standardized testing over AI's applications in frontier mathematical research. This paper seeks to close that gap, offering a detailed exploration of AI's basic knowledge, its strengths, and its emerging applications in the mathematical sciences. 

**Abstract (ZH)**: 本文综述了人工智能（AI）在数学研究中的广泛应用，强调了AI在这一领域开始发挥的变革性作用。传统上，AI的进步很大程度上依赖于数学和统计学等领域的理论基础。然而，最近AI的发展，特别是在强化学习（RL）和大语言模型（LLMs）方面的进展，展示了AI反哺数学的可能性，提供了灵活的算法框架和强大的归纳推理能力，支持数学研究的各个方面。本文旨在建立AI与数学之间的桥梁，提供相互利益的见解，并促进更深层次的跨学科理解。

具体而言，我们认为虽然当前的AI和LLMs在复杂演绎推理方面存在困难，但它们的固有创造力在支持和发展数学研究方面具有重要的潜力。这种创造力常被忽视，可能是开启数学新视角和方法的关键。此外，本文还解决了跨学科沟通的不足：数学家可能未能充分理解最新的AI进展，而AI研究人员常常优先考虑基准测试和标准化测试，而不是AI在前沿数学研究中的应用。本文旨在弥合这一差距，通过详细探讨AI的基本知识、优势及其在数学科学中的新兴应用，来提供深入的探讨。 

---
# From Creation to Curriculum: Examining the role of generative AI in Arts Universities 

**Title (ZH)**: 从创生到课程：探究生成式AI在艺术大学中的作用 

**Authors**: Atticus Sims  

**Link**: [PDF](https://arxiv.org/pdf/2412.16531)  

**Abstract**: The age of Artificial Intelligence (AI) is marked by its transformative "generative" capabilities, distinguishing it from prior iterations. This burgeoning characteristic of AI has enabled it to produce new and original content, inherently showcasing its creative prowess. This shift challenges and requires a recalibration in the realm of arts education, urging a departure from established pedagogies centered on human-driven image creation. The paper meticulously addresses the integration of AI tools, with a spotlight on Stable Diffusion (SD), into university arts curricula. Drawing from practical insights gathered from workshops conducted in July 2023, which culminated in an exhibition of AI-driven artworks, the paper aims to provide a roadmap for seamlessly infusing these tools into academic settings. Given their recent emergence, the paper delves into a comprehensive overview of such tools, emphasizing the intricate dance between artists, developers, and researchers in the open-source AI art world. This discourse extends to the challenges and imperatives faced by educational institutions. It presents a compelling case for the swift adoption of these avant-garde tools, underscoring the paramount importance of equipping students with the competencies required to thrive in an AI-augmented artistic landscape. 

**Abstract (ZH)**: 人工智能（AI）时代以其革命性的“生成”能力而著称，这使其与过往版本区分开来。这一新兴的AI特性使其能够生成全新的原创内容，展示了其创造性的潜力。这一转变对艺术教育领域提出了挑战，要求重新校准现有的以人为驱动的图像创作教育方法。本文详细探讨了将AI工具，特别是聚焦于稳定扩散（SD）等工具，融入大学艺术课程的方法。文章基于2023年7月举办的研讨会的经验收获，这些研讨会的成果最终展示了AI驱动的艺术作品，旨在为这些工具在学术环境中的无缝融入提供一条清晰的道路。鉴于这些工具的近期发展，本文全面概述了这类工具的特点，并强调了艺术家、开发者和研究人员之间在开放源代码AI艺术世界中的复杂互动。本文还探讨了教育机构面临的挑战和必要性，提出了迅速采用这些前沿工具的有力论据，强调了培养学生在AI增强的艺术环境中生存和发展的能力的重要性。 

---
# Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions 

**Title (ZH)**: 大型语言模型微调中的隐私保护：攻击、防御及未来发展方向 

**Authors**: Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.16504)  

**Abstract**: Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing their effectiveness and limitations in addressing privacy risks and maintaining model utility. By identifying key gaps in existing research, we highlight challenges and propose directions to advance the development of privacy-preserving methods for fine-tuning LLMs, promoting their responsible use in diverse applications. 

**Abstract (ZH)**: 微调已成为利用大规模语言模型（LLMs）执行特定下游任务的关键过程，使这些模型在各个领域达到最先进的性能。然而，微调过程往往涉及敏感数据，引入了隐私风险，这些风险利用了这一阶段的独特特征。在本文中，我们对与微调LLMs相关的隐私挑战进行了全面的综述，突出了各种隐私攻击（包括成员推断、数据提取和后门攻击）中的脆弱性。我们还回顾了旨在减轻微调阶段隐私风险的防御机制，如差分隐私、联邦学习和知识遗忘，讨论了这些机制在解决隐私风险并保持模型效用方面的效果和局限性。通过识别现有研究中的关键差距，我们指出了挑战并提出了发展方向，以推进隐私保护方法在微调LLMs中的发展，促进其在不同应用场景中的负责任应用。 

---
# Deep Reinforcement Learning Based Systems for Safety Critical Applications in Aerospace 

**Title (ZH)**: 基于深度强化学习的在航空航天关键应用中的安全系统 

**Authors**: Abedin Sherifi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16489)  

**Abstract**: Recent advancements in artificial intelligence (AI) applications within aerospace have demonstrated substantial growth, particularly in the context of control systems. As High Performance Computing (HPC) platforms continue to evolve, they are expected to replace current flight control or engine control computers, enabling increased computational capabilities. This shift will allow real-time AI applications, such as image processing and defect detection, to be seamlessly integrated into monitoring systems, providing real-time awareness and enhanced fault detection and accommodation. Furthermore, AI's potential in aerospace extends to control systems, where its application can range from full autonomy to enhancing human control through assistive features. AI, particularly deep reinforcement learning (DRL), can offer significant improvements in control systems, whether for autonomous operation or as an augmentative tool. 

**Abstract (ZH)**: 近年来，人工智能（AI）在航空航天领域的应用取得了显著进展，特别是在控制系统方面。随着高性能计算（HPC）平台的不断演进，它们有望取代当前的飞行控制或发动机控制计算机，提供更高的计算能力。这一转变将使实时AI应用，如图像处理和缺陷检测，能够无缝集成到监控系统中，提供实时监控和增强的故障检测与容错能力。此外，AI在航空航天领域的应用还扩展到了控制系统，其应用范围可以从完全自主控制到通过辅助功能增强人类控制。特别是深度强化学习（DRL），在自主操作或作为辅助工具方面均能提供显著的控制系统改进。 

---
# Knowledge as a Breaking of Ergodicity 

**Title (ZH)**: 知识作为打破遍历性的途径 

**Authors**: Yang He, Vassiliy Lubchenko  

**Link**: [PDF](https://arxiv.org/pdf/2412.16411)  

**Abstract**: We construct a thermodynamic potential that can guide training of a generative model defined on a set of binary degrees of freedom. We argue that upon reduction in description, so as to make the generative model computationally-manageable, the potential develops multiple minima. This is mirrored by the emergence of multiple minima in the free energy proper of the generative model itself. The variety of training samples that employ N binary degrees of freedom is ordinarily much lower than the size 2^N of the full phase space. The non-represented configurations, we argue, should be thought of as comprising a high-temperature phase separated by an extensive energy gap from the configurations composing the training set. Thus, training amounts to sampling a free energy surface in the form of a library of distinct bound states, each of which breaks ergodicity. The ergodicity breaking prevents escape into the near continuum of states comprising the high-temperature phase; thus it is necessary for proper functionality. It may however have the side effect of limiting access to patterns that were underrepresented in the training set. At the same time, the ergodicity breaking within the library complicates both learning and retrieval. As a remedy, one may concurrently employ multiple generative models -- up to one model per free energy minimum. 

**Abstract (ZH)**: 我们构建了一种热力学势函数，该函数能够引导定义在一组二元自由度上的生成模型的训练。我们论证，为了使生成模型在计算上易于管理，通过简化描述，热力学势函数会形成多个极小值。这在生成模型本身确实的自由能中被镜像，并表现为多个极小值的出现。通常情况下，使用N个二元自由度训练样本的多样性远低于全相空间的大小2^N。我们论证，未被代表的配置应被视为一个与训练集由广泛能量间隙分离的高温度相。因此，训练相当于在由不同束缚态组成的能量表面中采样，每个束缚态都破坏了遍历性。这种遍历性的破坏防止了向组成高温度相的邻近连续态空间的逃逸，这是生成模型正常功能所必需的。然而，这也可能导致对在训练集中代表性不足的模式的访问受限。同时，库中遍历性的破坏复杂化了学习和检索过程。为了解决这一问题，可以采用多个生成模型——最多一个模型对应于一个自由能极小值。 

---
# Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning 

**Title (ZH)**: 连续层次强化学习与规划中的自主选项发明 

**Authors**: Rashmeet Kaur Nayyar, Siddharth Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2412.16395)  

**Abstract**: Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action representations to enable transfer and generalization remains a challenging open problem. This paper presents a novel approach for inventing, representing, and utilizing options, which represent temporally extended behaviors, in continual RL settings. Our approach addresses streams of stochastic problems characterized by long horizons, sparse rewards, and unknown transition and reward functions.
Our approach continually learns and maintains an interpretable state abstraction, and uses it to invent high-level options with abstract symbolic representations. These options meet three key desiderata: (1) composability for solving tasks effectively with lookahead planning, (2) reusability across problem instances for minimizing the need for relearning, and (3) mutual independence for reducing interference among options. Our main contributions are approaches for continually learning transferable, generalizable options with symbolic representations, and for integrating search techniques with RL to efficiently plan over these learned options to solve new problems. Empirical results demonstrate that the resulting approach effectively learns and transfers abstract knowledge across problem instances, achieving superior sample efficiency compared to state-of-the-art methods. 

**Abstract (ZH)**: 抽象是提高强化学习（RL）性能的关键。然而，自主学习能够实现迁移和泛化的抽象状态和动作表示仍然是一个具有挑战性的开放问题。本文提出了一种新颖的方法，用于在持续强化学习环境中发明、表示和利用选项，这些选项代表了时间上延伸的行为。我们的方法针对由长时间期、稀疏奖励以及未知转移和奖励函数组成的随机问题流进行学习和维护可解释的状态抽象，并利用这种抽象来发明具有抽象符号表示的高级选项。这些选项满足三个关键要求：（1）组合性，以便在前瞻规划中有效解决任务；（2）跨问题实例的重用性，以减少重新学习的需求；（3）互不干扰性，以减少选项之间的相互干扰。我们的主要贡献在于提出了持续学习可迁移、可泛化的具有符号表示的选项的方法，并将搜索技术与RL结合起来，高效地通过这些学习到的选项进行规划以解决新问题。实证结果表明，这种方法能够有效地学习和转移抽象知识，并在解决问题实例时表现出优于现有最先进的方法的样本效率。 

---
# Ethics and Technical Aspects of Generative AI Models in Digital Content Creation 

**Title (ZH)**: 生成式AI模型在数字内容创作中的伦理与技术方面问题 

**Authors**: Atahan Karagoz  

**Link**: [PDF](https://arxiv.org/pdf/2412.16389)  

**Abstract**: Generative AI models like GPT-4o and DALL-E 3 are reshaping digital content creation, offering industries tools to generate diverse and sophisticated text and images with remarkable creativity and efficiency. This paper examines both the capabilities and challenges of these models within creative workflows. While they deliver high performance in generating content with creativity, diversity, and technical precision, they also raise significant ethical concerns. Our study addresses two key research questions: (a) how these models perform in terms of creativity, diversity, accuracy, and computational efficiency, and (b) the ethical risks they present, particularly concerning bias, authenticity, and potential misuse. Through a structured series of experiments, we analyze their technical performance and assess the ethical implications of their outputs, revealing that although generative models enhance creative processes, they often reflect biases from their training data and carry ethical vulnerabilities that require careful oversight. This research proposes ethical guidelines to support responsible AI integration into industry practices, fostering a balance between innovation and ethical integrity. 

**Abstract (ZH)**: 生成型AI模型如GPT-4和DALL-E 3正在重塑数字内容创作，为各行各业提供了生成多样且复杂的文本和图像的强大工具，展现了卓越的创造力和效率。本文探讨了这些模型在创意工作流程中的能力和挑战。尽管它们在生成具有创意、多样性以及技术精准度的内容方面表现出色，但也引发了重大伦理问题。我们的研究针对两个关键的研究问题进行了探讨：（a）这些模型在创造力、多样性、准确性和计算效率方面的表现如何；（b）它们所呈现的伦理风险，特别是关于偏见、真实性和潜在误用的问题。通过一系列结构化的实验，我们分析了它们的技术性能，并评估了其输出的伦理影响，揭示了尽管生成模型能够增强创意过程，但它们常反映出训练数据中的偏见，并存在需要谨慎监督的伦理缺陷。本文提出了相关的伦理指导原则，以支持负责任的AI集成到工业实践中，促进创新与伦理完整性的平衡。 

---
# Collision-based Dynamics for Multi-Marginal Optimal Transport 

**Title (ZH)**: 基于碰撞的动力学多边际最优传输 

**Authors**: Mohsen Sadr, Hossein Gorji  

**Link**: [PDF](https://arxiv.org/pdf/2412.16385)  

**Abstract**: Inspired by the Boltzmann kinetics, we propose a collision-based dynamics with a Monte Carlo solution algorithm that approximates the solution of the multi-marginal optimal transport problem via randomized pairwise swapping of sample indices. The computational complexity and memory usage of the proposed method scale linearly with the number of samples, making it highly attractive for high-dimensional settings. In several examples, we demonstrate the efficiency of the proposed method compared to the state-of-the-art methods. 

**Abstract (ZH)**: 受玻尔兹曼动力学的启发，我们提出了一种基于碰撞的动力学模型及其蒙特卡洛求解算法。该方法通过随机的两两样本索引互换来近似解决多边际最优传输问题。所提出的方法的计算复杂度和内存使用量与样本数量线性增长，使其在高维设置中具有高度吸引力。在几个例子中，我们展示了所提出方法与现有最佳方法相比的效率。 

---
# Social Science Is Necessary for Operationalizing Socially Responsible Foundation Models 

**Title (ZH)**: 社会科学对于实操化社会责任基金会模型是必要的 

**Authors**: Adam Davies, Elisa Nguyen, Michael Simeone, Erik Johnston, Martin Gubri  

**Link**: [PDF](https://arxiv.org/pdf/2412.16355)  

**Abstract**: With the rise of foundation models, there is growing concern about their potential social impacts. Social science has a long history of studying the social impacts of transformative technologies in terms of pre-existing systems of power and how these systems are disrupted or reinforced by new technologies. In this position paper, we build on prior work studying the social impacts of earlier technologies to propose a conceptual framework studying foundation models as sociotechnical systems, incorporating social science expertise to better understand how these models affect systems of power, anticipate the impacts of deploying these models in various applications, and study the effectiveness of technical interventions intended to mitigate social harms. We advocate for an interdisciplinary and collaborative research paradigm between AI and social science across all stages of foundation model research and development to promote socially responsible research practices and use cases, and outline several strategies to facilitate such research. 

**Abstract (ZH)**: 随着基础模型的兴起，人们对其潜在的社会影响日益担忧。社会科学研究有悠久的历史，研究变革性技术对既有权力系统的影响，以及这些新出现的技术是如何打破或强化这些权力系统的。在本文中，我们借鉴了之前关于早期技术的社会影响的研究成果，提出了一种概念框架，将基础模型视为社会技术系统，并结合社会科学研究的优势，以更好地理解这些模型如何影响权力系统，以及如何预测在不同应用场景中部署这些模型的影响，研究旨在减轻社会损害的技术干预的有效性。我们倡导在基础模型研究和开发的所有阶段，跨人工智能和社会科学研究领域开展跨学科合作研究，以促进负责任的研究实践和应用，并概述了若干促进此类研究的策略。 

---
# Towards Safe and Honest AI Agents with Neural Self-Other Overlap 

**Title (ZH)**: 朝着具备神经自我-他者重叠的安全可靠人工智能代理的研究 

**Authors**: Marc Carauleanu, Michael Vaiana, Judd Rosenblatt, Cameron Berg, Diogo Schwerz de Lucena  

**Link**: [PDF](https://arxiv.org/pdf/2412.16325)  

**Abstract**: As AI systems increasingly make critical decisions, deceptive AI poses a significant challenge to trust and safety. We present Self-Other Overlap (SOO) fine-tuning, a promising approach in AI Safety that could substantially improve our ability to build honest artificial intelligence. Inspired by cognitive neuroscience research on empathy, SOO aims to align how AI models represent themselves and others. Our experiments on LLMs with 7B, 27B, and 78B parameters demonstrate SOO's efficacy: deceptive responses of Mistral-7B-Instruct-v0.2 dropped from 73.6% to 17.2% with no observed reduction in general task performance, while in Gemma-2-27b-it and CalmeRys-78B-Orpo-v0.1 deceptive responses were reduced from 100% to 9.3% and 2.7%, respectively, with a small impact on capabilities. In reinforcement learning scenarios, SOO-trained agents showed significantly reduced deceptive behavior. SOO's focus on contrastive self and other-referencing observations offers strong potential for generalization across AI architectures. While current applications focus on language models and simple RL environments, SOO could pave the way for more trustworthy AI in broader domains. Ethical implications and long-term effects warrant further investigation, but SOO represents a significant step forward in AI safety research. 

**Abstract (ZH)**: 随着AI系统在越来越多的关键决策中发挥作用，欺骗性AI对信任和安全构成了重大挑战。我们提出了自我-他人重叠（SOO）微调，这是一种在AI安全领域具有前景的方法，有望显著提高我们构建诚实的人工智能的能力。受认知神经科学研究中同理心机制的启发，SOO旨在使AI模型如何表示自己与他人保持一致。我们在参数量分别为7B、27B和78B的大型语言模型（LLM）上的实验表明，SOO的有效性：Mistral-7B-Instruct-v0.2的欺骗性回应从73.6%降低到17.2%，没有观察到一般任务性能的下降；而在Gemma-2-27b-it和CalmeRys-78B-Orpo-v0.1中，欺骗性回应分别从100%降低到9.3%和2.7%，尽管对能力的影响较小。在强化学习场景中，SOO训练的代理表现出显著减少的欺骗行为。SOO强调对比自我和他人的观察，为跨AI架构的一般化提供了强有力的可能性。尽管目前的应用主要集中在语言模型和简单的RL环境中，SOO为更广泛的领域中构建更值得信赖的AI提供了可能的道路。伦理影响和长期效果需要进一步研究，但SOO代表了AI安全研究中一个重要的进步。 

---
# Benchmarking LLMs and SLMs for patient reported outcomes 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

Benchmarking LLMs and SLMs for Patient-Reported Outcomes

中文翻译为：

基于基准测试评价大语言模型和医疗语言模型在患者报告结果中的应用 

**Authors**: Matteo Marengo, Jarod Lévy, Jean-Emmanuel Bibault  

**Link**: [PDF](https://arxiv.org/pdf/2412.16291)  

**Abstract**: LLMs have transformed the execution of numerous tasks, including those in the medical domain. Among these, summarizing patient-reported outcomes (PROs) into concise natural language reports is of particular interest to clinicians, as it enables them to focus on critical patient concerns and spend more time in meaningful discussions. While existing work with LLMs like GPT-4 has shown impressive results, real breakthroughs could arise from leveraging SLMs as they offer the advantage of being deployable locally, ensuring patient data privacy and compliance with healthcare regulations. This study benchmarks several SLMs against LLMs for summarizing patient-reported Q\&A forms in the context of radiotherapy. Using various metrics, we evaluate their precision and reliability. The findings highlight both the promise and limitations of SLMs for high-stakes medical tasks, fostering more efficient and privacy-preserving AI-driven healthcare solutions. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经极大地改变了众多任务的执行，包括医疗领域的任务。在这些任务中，将患者报告的结果（Patient-Reported Outcomes，PROs）总结为简洁的自然语言报告尤其受到临床医生的青睐，因为这使他们能够关注患者的关键关切，并在有意义的讨论中投入更多时间。虽然使用诸如GPT-4等LLMs的研究已有令人印象深刻的成果，但真正突破性的进展可能来自利用服务型语言模型（Service-oriented Language Models，SLMs），因为SLMs具有可在本地部署的优势，从而确保了患者数据的隐私并符合医疗保健法规。本研究将几种SLMs与LLMs进行基准测试，以评估它们在放射治疗背景下总结患者报告的问答表单的效果。我们使用多种指标来评价它们的精确性和可靠性。研究结果突显了SLMs在高风险医疗任务中的潜力和局限性，促进更高效的、隐私保护的人工智能驱动的医疗健康解决方案的发展。 

---
# Mapping the Mind of an Instruction-based Image Editing using SMILE 

**Title (ZH)**: 基于指令的图像编辑中智能思维映射：SMILE探究 

**Authors**: Zeinab Dehghani, Koorosh Aslansefat, Adil Khan, Adín Ramírez Rivera, Franky George, Muhammad Khalid  

**Link**: [PDF](https://arxiv.org/pdf/2412.16277)  

**Abstract**: Despite recent advancements in Instruct-based Image Editing models for generating high-quality images, they are known as black boxes and a significant barrier to transparency and user trust. To solve this issue, we introduce SMILE (Statistical Model-agnostic Interpretability with Local Explanations), a novel model-agnostic for localized interpretability that provides a visual heatmap to clarify the textual elements' influence on image-generating models. We applied our method to various Instruction-based Image Editing models like Pix2Pix, Image2Image-turbo and Diffusers-Inpaint and showed how our model can improve interpretability and reliability. Also, we use stability, accuracy, fidelity, and consistency metrics to evaluate our method. These findings indicate the exciting potential of model-agnostic interpretability for reliability and trustworthiness in critical applications such as healthcare and autonomous driving while encouraging additional investigation into the significance of interpretability in enhancing dependable image editing models. 

**Abstract (ZH)**: 尽管近年来基于指令的图像编辑模型在生成高质量图像方面取得了进展，但它们通常被视为黑箱模型，这已成为提高透明度和用户信任的重大障碍。为了解决这一问题，我们提出了SMILE（Statistical Model-agnostic Interpretability with Local Explanations），这是一种新的模型无偏的局部可解释性方法，通过提供视觉热图来阐明文本元素对图像生成模型的影响。我们将该方法应用于多种基于指令的图像编辑模型，如Pix2Pix、Image2Image-turbo和Diffusers-Inpaint，并展示了我们的模型如何提高可解释性和可靠性。此外，我们使用稳定性、准确性、保真度和一致性指标来评估该方法。这些发现表明，在诸如医疗保健和自动驾驶等关键应用中，模型无偏的可解释性具有增强可靠性和可信度的潜力，同时也鼓励对可解释性在提高可靠的图像编辑模型中的意义进行进一步研究。 

---
# MetaScientist: A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design 

**Title (ZH)**: MetaScientist：一种人类与人工智能协同的自动化机械 metamaterial 设计框架 

**Authors**: Jingyuan Qi, Zian Jia, Minqian Liu, Wangzhi Zhan, Junkai Zhang, Xiaofei Wen, Jingru Gan, Jianpeng Chen, Qin Liu, Mingyu Derek Ma, Bangzheng Li, Haohui Wang, Adithya Kulkarni, Muhao Chen, Dawei Zhou, Ling Li, Wei Wang, Lifu Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16270)  

**Abstract**: The discovery of novel mechanical metamaterials, whose properties are dominated by their engineered structures rather than chemical composition, is a knowledge-intensive and resource-demanding process. To accelerate the design of novel metamaterials, we present MetaScientist, a human-in-the-loop system that integrates advanced AI capabilities with expert oversight with two primary phases: (1) hypothesis generation, where the system performs complex reasoning to generate novel and scientifically sound hypotheses, supported with domain-specific foundation models and inductive biases retrieved from existing literature; (2) 3D structure synthesis, where a 3D structure is synthesized with a novel 3D diffusion model based on the textual hypothesis and refined it with a LLM-based refinement model to achieve better structure properties. At each phase, domain experts iteratively validate the system outputs, and provide feedback and supplementary materials to ensure the alignment of the outputs with scientific principles and human preferences. Through extensive evaluation from human scientists, MetaScientist is able to deliver novel and valid mechanical metamaterial designs that have the potential to be highly impactful in the metamaterial field. 

**Abstract (ZH)**: 发现新型机械 metamaterials 的过程是一个知识密集型且资源需求高的过程，其性质主要由其设计结构决定，而不是化学成分。为了加快新型 metamaterials 的设计，我们提出了一种人机协作系统 MetaScientist，该系统将先进的 AI 能力与专家监督相结合，分为两个主要阶段：（1）假设生成，系统通过复杂的推理生成新颖且科学上合理的假设，这些假设以领域特定的基础模型和从现有文献中检索出的归纳偏见为支持；（2）3D结构合成，根据文本假设合成一个新的3D结构，并使用基于大规模语言模型（LLM）的精修模型对其进行精修，以实现更好的结构性能。在每个阶段，领域专家会迭代地验证系统输出，并提供反馈和补充材料，以确保输出与科学原理和人类偏好相一致。通过广泛的评价，MetaScientist 能够提供具有高度潜在影响力的新型机械 metamaterials 设计。 

---
# Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems 

**Title (ZH)**: Autobatch.Flex：基于人类指令的动态可重构自动驾驶系统 

**Authors**: Ziwei Song, Mingsong Lv, Tianchi Ren, Chun Jason Xue, Jen-Ming Wu, Nan Guan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16265)  

**Abstract**: Existing Autonomous Driving Systems (ADS) independently make driving decisions, but they face two significant limitations. First, in complex scenarios, ADS may misinterpret the environment and make inappropriate driving decisions. Second, these systems are unable to incorporate human driving preferences in their decision-making processes. This paper proposes this http URL, a novel ADS system that incorporates human input into the driving process, allowing users to guide the ADS in making more appropriate decisions and ensuring their preferences are satisfied. Achieving this needs to address two key challenges: (1) translating human instructions, expressed in natural language, into a format the ADS can understand, and (2) ensuring these instructions are executed safely and consistently within the ADS' s decision-making framework. For the first challenge, we employ a Large Language Model (LLM) assisted by an ADS-specialized knowledge base to enhance domain-specific translation. For the second challenge, we design a validation mechanism to ensure that human instructions result in safe and consistent driving behavior. Experiments conducted on both simulators and a real-world autonomous vehicle demonstrate that this http URL effectively interprets human instructions and executes them safely. 

**Abstract (ZH)**: 现有的自动驾驶系统（ADS）能够独立做出驾驶决策，但在复杂场景下可能误解环境并作出不适当的决策。此外，这些系统无法在其决策过程中融入用户驾驶偏好。本文提出了一种名为“this http URL”的新型ADS系统，该系统能够将人类输入纳入驾驶过程，使用户能够指导ADS做出更加合适的决策，确保其偏好得到满足。实现这一点需要解决两个关键挑战：（1）将用自然语言表达的人类指令转换为ADS能够理解的格式，（2）确保这些指令在ADS的决策框架中得到安全和一致的执行。为了应对第一个挑战，我们采用了大型语言模型（LLM）辅助以专门的ADS知识库的方法，以增强领域特定的翻译能力。为了解决第二个挑战，我们设计了一个验证机制，以确保人类指令能够产生安全和一致的驾驶行为。在模拟器和实际自动驾驶车辆上的实验结果表明，“this http URL”能够有效地解释人类指令并安全执行这些指令。 

---
# Optimizing Low-Speed Autonomous Driving: A Reinforcement Learning Approach to Route Stability and Maximum Speed 

**Title (ZH)**: 优化低速自动驾驶：基于强化学习的路线稳定性和最高速度方法 

**Authors**: Benny Bao-Sheng Li, Elena Wu, Hins Shao-Xuan Yang, Nicky Yao-Jin Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16248)  

**Abstract**: Autonomous driving has garnered significant attention in recent years, especially in optimizing vehicle performance under varying conditions. This paper addresses the challenge of maintaining maximum speed stability in low-speed autonomous driving while following a predefined route. Leveraging reinforcement learning (RL), we propose a novel approach to optimize driving policies that enable the vehicle to achieve near-maximum speed without compromising on safety or route accuracy, even in low-speed scenarios. 

**Abstract (ZH)**: 近年来，自主驾驶技术引起了广泛关注，尤其是在不同条件下优化车辆性能方面。本文针对低速自主驾驶沿预设路径行驶时保持最大速度稳定性的挑战，提出了一种新的方法。借助强化学习（Reinforcement Learning, RL），我们提出了一种优化驾驶策略的方法，使车辆能够在不牺牲安全性和路径准确性的情况下，实现接近最大速度，即使在低速场景下也是如此。 

---
# Neural diversity is key to collective artificial learning 

**Title (ZH)**: 神经多样性是集体人工学习的关键 

**Authors**: Matteo Bettini, Ryan Kortvelesy, Amanda Prorok  

**Link**: [PDF](https://arxiv.org/pdf/2412.16244)  

**Abstract**: Many of the world's most pressing issues, such as climate change and global peace, require complex collective problem-solving skills. Recent studies indicate that diversity in individuals' behaviors is key to developing such skills and increasing collective performance. Yet behavioral diversity in collective artificial learning is understudied, with today's machine learning paradigms commonly favoring homogeneous agent strategies over heterogeneous ones, mainly due to computational considerations. In this work, we employ novel diversity measurement and control paradigms to study the impact of behavioral heterogeneity in several facets of collective artificial learning. Through experiments in team play and other cooperative tasks, we show the emergence of unbiased behavioral roles that improve team outcomes; how neural diversity synergizes with morphological diversity; how diverse agents are more effective at finding cooperative solutions in sparse reward settings; and how behaviorally heterogeneous teams learn and retain latent skills to overcome repeated disruptions. Overall, our results indicate that, by controlling diversity, we can obtain non-trivial benefits over homogeneous training paradigms, demonstrating that diversity is a fundamental component of collective artificial learning, an insight thus far overlooked. 

**Abstract (ZH)**: 世界许多紧迫的问题，如气候变化和全球和平，需要复杂的集体问题解决能力。近期研究表明，个体行为的多样性是发展此类技能和提高集体绩效的关键。然而，在集体人工智能学习中，行为多样性的研究不足，当前的机器学习范式往往偏好同质化的代理策略，而非异质化的策略，主要原因在于计算考虑。在本工作中，我们采用新型的多样性测量和控制范式，探讨行为异质性在集体人工智能学习各个方面的影响力。通过团队游戏和其他协作任务的实验，我们展示了无偏见的行为角色如何改善团队成果；神经多样性与形态多样性如何协同作用；在稀疏奖励环境中，多样化代理如何更有效地找到合作解决方案；以及行为异质性团队如何学习和保留潜在技能以克服反复的中断。总体而言，我们的结果表明，通过控制多样性，我们可以在同质化训练范式中获得非平凡的利益，证明了多样性是集体人工智能学习的基本要素，而这一点此前尚未被充分认识到。 

---
# Agents Are Not Enough 

**Title (ZH)**: 代理不足以胜任 

**Authors**: Chirag Shah, Ryen W. White  

**Link**: [PDF](https://arxiv.org/pdf/2412.16241)  

**Abstract**: In the midst of the growing integration of Artificial Intelligence (AI) into various aspects of our lives, agents are experiencing a resurgence. These autonomous programs that act on behalf of humans are neither new nor exclusive to the mainstream AI movement. By exploring past incarnations of agents, we can understand what has been done previously, what worked, and more importantly, what did not pan out and why. This understanding lets us to examine what distinguishes the current focus on agents. While generative AI is appealing, this technology alone is insufficient to make new generations of agents more successful. To make the current wave of agents effective and sustainable, we envision an ecosystem that includes not only agents but also Sims, which represent user preferences and behaviors, as well as Assistants, which directly interact with the user and coordinate the execution of user tasks with the help of the agents. 

**Abstract (ZH)**: 在人工智能（AI）逐渐融入我们生活各个方面的同时，代理程序正在经历复兴。这些代表人类行动的自主程序并非新鲜事物，也并非主流AI运动所独有。通过探索代理程序过去的各种形态，我们可以理解之前都做了什么、什么有效，更重要的是什么未能成功以及原因。这种理解使我们能够考察当前对代理程序的关注点有何不同。尽管生成式AI颇具吸引力，但仅依靠这项技术并不能使新一代代理程序取得成功。为了使当前代理程序浪潮既有效又可持续，我们设想了一个生态系统，该生态系统不仅包含代理程序，还包括反映用户偏好和行为的Sim（模拟）以及直接与用户交互并借助代理程序协调执行用户任务的助手。 

---
# A jury evaluation theorem 

**Title (ZH)**: 一个陪审团评估定理 

**Authors**: Andrés Corrada-Emmanuel  

**Link**: [PDF](https://arxiv.org/pdf/2412.16238)  

**Abstract**: Majority voting (MV) is the prototypical ``wisdom of the crowd'' algorithm. Theorems considering when MV is optimal for group decisions date back to Condorcet's 1785 jury decision theorem. The same assumption of error independence used by Condorcet is used here to prove a jury evaluation theorem that does purely algebraic evaluation (AE). Three or more binary jurors are enough to obtain the only two possible statistics of their correctness on a joint test they took. AE is shown to be superior to MV since it allows one to choose the minority vote depending on how the jurors agree or disagree. In addition, AE is self-alarming about the failure of the error-independence assumption. Experiments labeling demographic datasets from the American Community Survey are carried out to compare MV and AE on nearly error-independent ensembles. In general, using algebraic evaluation leads to better classifier evaluations and group labeling decisions. 

**Abstract (ZH)**: 多数投票（MV）是典型的“群体智慧”算法。考虑在群体决策中MV何时最优的定理远溯至 Condorcet 在 1785 年提出的陪审团决策定理。与 Condorcet 使用的同一假设，即误差独立性，在这里被用来证明一个陪审团评估定理，该定理由纯粹的代数评估（AE）构成。三个或更多的二元陪审员足以获得他们在共同测试中正确性的唯一两种可能统计数据。AE 优于 MV，因为它允许根据陪审员之间的同意或分歧情况选择少数意见。此外，AE 能够自我提醒关于误差独立性假设失效的问题。对来自美国社区调查的几乎所有误差独立的数据集进行了分类标签实验，以比较 MV 和 AE 在几乎误差独立群体中的表现。总体而言，使用代数评估可以得到更好的分类器评估结果和群体标签决策。 

---
# A Proposal for Extending the Common Model of Cognition to Emotion 

**Title (ZH)**: 将以下论文的内容或标题翻译成中文，并确保符合学术规范：

Original Title:
A Proposal for Extending the Common Model of Cognition to Emotion

Translated Title:
关于将通用认知模型扩展至情感的建议 

**Authors**: Paul S. Rosenbloom, John E. Laird, Christian Lebiere, Andrea Stocco, Richard H. Granger, Christian Huyck  

**Link**: [PDF](https://arxiv.org/pdf/2412.16231)  

**Abstract**: Cognition and emotion must be partnered in any complete model of a humanlike mind. This article proposes an extension to the Common Model of Cognition -- a developing consensus concerning what is required in such a mind -- for emotion that includes a linked pair of modules for emotion and metacognitive assessment, plus pervasive connections between these two new modules and the Common Model's existing modules and links. 

**Abstract (ZH)**: 人类思维中的认知与情绪必须相辅相成。本文提议在认知的通用模型中扩展情绪部分——一种关于此类思维所需要素的逐步共识——增加一个情绪模块和元认知评估模块组成的配对，并确保这两个新模块与通用模型现有模块及其连接之间存在广泛联系。 

---
# TAACKIT: Track Annotation and Analytics with Continuous Knowledge Integration Tool 

**Title (ZH)**: TAACKIT：持续知识集成工具下的跟踪标注与分析 

**Authors**: Lily Lee, Julian Fontes, Andrew Weinert, Laura Schomacker, Daniel Stabile, Jonathan Hou  

**Link**: [PDF](https://arxiv.org/pdf/2412.16228)  

**Abstract**: Machine learning (ML) is a powerful tool for efficiently analyzing data, detecting patterns, and forecasting trends across various domains such as text, audio, and images. The availability of annotation tools to generate reliably annotated data is crucial for advances in ML applications. In the domain of geospatial tracks, the lack of such tools to annotate and validate data impedes rapid and accessible ML application development. This paper presents Track Annotation and Analytics with Continuous Knowledge Integration Tool (TAACKIT) to serve the critically important functions of annotating geospatial track data and validating ML models. We demonstrate an ML application use case in the air traffic domain to illustrate its data annotation and model evaluation power and quantify the annotation effort reduction. 

**Abstract (ZH)**: 机器学习（ML）是一种高效的数据分析、模式检测和趋势预测工具，广泛应用于文本、音频和图像等领域。生成可靠标注数据的注释工具对于ML应用的发展至关重要。在地理空间轨迹领域，缺乏这样的工具来标注和验证数据，阻碍了ML应用的快速开发与部署。本文介绍了一种名为“轨迹注释与连续知识集成工具”（TAACKIT）的应用，旨在完成地理空间轨迹数据的标注和ML模型的验证等关键功能。我们通过空中交通领域的ML应用案例，展示了TAACKIT的数据标注能力和模型评估能力，并量化了标注工作量的减少。 

---
# More complex environments may be required to discover benefits of lifetime learning in evolving robots 

**Title (ZH)**: 更复杂的环境可能有助于发现终身学习在演化机器人中的效益 

**Authors**: Ege de Bruin, Kyrre Glette, Kai Olav Ellefsen  

**Link**: [PDF](https://arxiv.org/pdf/2412.16184)  

**Abstract**: It is well known that intra-life learning, defined as an additional controller optimization loop, is beneficial for evolving robot morphologies for locomotion. In this work, we investigate this further by comparing it in two different environments: an easy flat environment and a more challenging hills environment. We show that learning is significantly more beneficial in a hilly environment than in a flat environment and that it might be needed to evaluate robots in a more challenging environment to see the benefits of learning. 

**Abstract (ZH)**: 众所周知，内lifetime学习被定义为额外的控制器优化循环，这对于演化机器人运动形态具有积极作用。在本项研究中，我们通过比较在两种不同的环境中进一步探讨了这一点：一种是简单的平坦环境，另一种是更具挑战性的山地环境。我们展示了在山地环境中学习的好处远大于在平坦环境中的好处，并且为了观察学习的好处，评估机器人在更具挑战性的环境中的性能可能是必要的。 

---
# Mining Math Conjectures from LLMs: A Pruning Approach 

**Title (ZH)**: 从大规模语言模型中挖掘数学猜想：一种剪枝方法 

**Authors**: Jake Chuharski, Elias Rojas Collins, Mark Meringolo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16177)  

**Abstract**: We present a novel approach to generating mathematical conjectures using Large Language Models (LLMs). Focusing on the solubilizer, a relatively recent construct in group theory, we demonstrate how LLMs such as ChatGPT, Gemini, and Claude can be leveraged to generate conjectures. These conjectures are pruned by allowing the LLMs to generate counterexamples. Our results indicate that LLMs are capable of producing original conjectures that, while not groundbreaking, are either plausible or falsifiable via counterexamples, though they exhibit limitations in code execution. 

**Abstract (ZH)**: 我们提出了一种使用大规模语言模型（LLMs）生成数学猜想的新型方法。本文集中在群论中的一个相对较新的概念——溶剂上，展示了如何利用如ChatGPT、Gemini和Claude等LLMs生成猜想。通过允许LLMs生成反例来进行筛选，我们展示了这些猜想的真实性和可验证性。我们的结果表明，尽管这些猜想可能并非具有开创性，但它们或是合理的，或可以通过反例进行验证，尽管它们在代码执行方面存在局限性。 

---
# LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System 

**Title (ZH)**: LABIIUM：增强型零配置测量自动化系统 

**Authors**: Emmanuel A. Olowe, Danial Chitnis  

**Link**: [PDF](https://arxiv.org/pdf/2412.16172)  

**Abstract**: The complexity of laboratory environments requires solutions that simplify instrument interaction and enhance measurement automation. Traditional tools often require configuration, software, and programming skills, creating barriers to productivity. Previous approaches, including dedicated software suites and custom scripts, frequently fall short in providing user-friendly solutions that align with programming practices. We present LABIIUM, an AI-enhanced, zero-configuration measurement automation system designed to streamline experimental workflows and improve user productivity. LABIIUM integrates an AI assistant powered by Large Language Models (LLMs) to generate code. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless instrument connectivity using standard tools such as VSCode and Python, eliminating setup overhead. To demonstrate its capabilities, we conducted experiments involving the measurement of the parametric transfer curve of a simple two-transistor inverting amplifier with a current source load. The AI assistant was evaluated using different prompt scenarios and compared with multiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An expert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling (GWASS) method was used as a baseline. The solutions generated by the AI assistant were compared with the expert solution and a uniform linear sweep baseline with 10,000 points. The graph results show that the LLMs were able to successfully complete the most basic uniform sweep, but LLMs were unable to develop adaptive sweeping algorithms to compete with GWASS. The evaluation underscores LABIIUM's ability to enhance laboratory productivity and support digital transformation in research and industry, and emphasizes the future work required to improve LLM performance in Electronic Measurement Science Tasks. 

**Abstract (ZH)**: 实验室环境的复杂性需要能够简化仪器交互和增强测量自动化解决方案。传统工具往往需要配置、软件和编程技能，造成了生产力的障碍。此前的方法，包括专用软件套件和自定义脚本，通常在提供用户友好的解决方案方面不够充分，这些解决方案能够与编程实践相一致。我们提出了LABIIUM，这是一种人工智能增强的零配置自动化测量系统，旨在简化实验工作流程并提高用户生产力。LABIIUM集成了由大规模语言模型（LLMs）驱动的人工智能助手，用于生成代码。LABIIUM的实验室到自动化测量桥梁（LAMBs）能够使用标准工具（如VSCode和Python）无缝连接仪器，消除设置开销。为了展示其能力，我们进行了测量简单倒相放大器（具有电流源负载）的参数传输曲线的实验。人工智能助手使用不同的提示场景进行评估，并与Claude Sonnet 3.5、Gemini Pro 1.5和GPT-4o等多款模型进行了对比。一个专家解决方案采用梯度加权自适应随机采样（GWASS）方法作为基准。评估了人工智能助手生成的解决方案与专家解决方案以及均匀直线扫描基准（包含10,000个点）进行比较。图结果表明，LLMs能够成功完成最基本的均匀扫描，但无法开发出能够与GWASS媲美的自适应扫描算法。评估突显了LABIIUM在提高实验室生产力和在研究与工业中支持数字转型方面的潜力，并强调了未来工作的重要性，以进一步提升LLMs在电子测量科学任务中的性能。 

---
# Cross-View Referring Multi-Object Tracking 

**Title (ZH)**: 跨视图引用多目标跟踪 

**Authors**: Sijia Chen, En Yu, Wenbing Tao  

**Link**: [PDF](https://arxiv.org/pdf/2412.17807)  

**Abstract**: Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this work, we propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of objects in each cross-view. To advance CRMOT task, we construct a cross-view referring multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named CRTrack. Specifically, it provides 13 different scenes and 221 language descriptions. Furthermore, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker. Extensive experiments on the CRTrack benchmark verify the effectiveness of our method. The dataset and code are available at this https URL. 

**Abstract (ZH)**: 参考多目标跟踪（RMOT）是当前跟踪领域中的一个重要课题。其任务形式是引导跟踪器追踪符合语言描述的对象。当前的研究主要集中在单视角下的参考多目标跟踪，指的是单一视角序列或多个不相关的视角序列。然而，在单视角下，某些对象的外观可能难以被观察到，导致对象与语言描述之间的不正确匹配。在本文中，我们提出了一项新的任务，称为跨视角参考多目标跟踪（CRMOT）。该任务引入了跨视角的概念，以从多个视角获取对象的外观，从而避免RMOT任务中对象外观不可见的问题。CRMOT是一个更具挑战性的任务，其目标是准确追踪符合语言描述的对象，并在每个跨视角中保持对象身份的一致性。为了推进CRMOT任务的发展，我们基于CAMPUS和DIVOTrack数据集构建了一个跨视角引用多目标跟踪基准数据集，命名为CRTrack。具体来说，该基准数据集提供了13个不同的场景和221个语言描述。此外，我们还提出了一种端到端的跨视角参考多目标跟踪方法，命名为CRTracker。在CRTrack基准数据集上的广泛实验验证了我们方法的有效性。数据集和代码可在以下网址获取：[这里提供的网址]。 

---
# PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion 

**Title (ZH)**: PepTune：多目标导向的离散扩散生成新型治疗多肽 

**Authors**: Sophia Tang, Yinuo Zhang, Pranam Chatterjee  

**Link**: [PDF](https://arxiv.org/pdf/2412.17780)  

**Abstract**: Peptide therapeutics, a major class of medicines, have achieved remarkable success across diseases such as diabetes and cancer, with landmark examples such as GLP-1 receptor agonists revolutionizing the treatment of type-2 diabetes and obesity. Despite their success, designing peptides that satisfy multiple conflicting objectives, such as target binding affinity, solubility, and membrane permeability, remains a major challenge. Classical drug development and structure-based design are ineffective for such tasks, as they fail to optimize global functional properties critical for therapeutic efficacy. Existing generative frameworks are largely limited to continuous spaces, unconditioned outputs, or single-objective guidance, making them unsuitable for discrete sequence optimization across multiple properties. To address this, we present PepTune, a multi-objective discrete diffusion model for the simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with state-dependent masking schedules and penalty-based objectives. To guide the diffusion process, we propose a Monte Carlo Tree Search (MCTS)-based strategy that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTS integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity inherent to discrete spaces. Using PepTune, we generate diverse, chemically-modified peptides optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling characteristics on various disease-relevant targets. In total, our results demonstrate that MCTS-guided discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces. 

**Abstract (ZH)**: 肽类治疗药物作为一类主要的药物，在糖尿病和癌症等疾病中取得了显著的成功，如GLP-1受体激动剂等 landmark 实例彻底革新了2型糖尿病和肥胖症的治疗方式。尽管取得了成功，但设计同时满足多个相互冲突的目标（如靶点亲和力、溶解性和膜通透性）的肽仍然是一个重大挑战。经典的药物发现方法和结构基于设计对于处理这类任务无效，因为它们无法优化影响治疗效果的全局功能性质。现有的生成框架大多局限于连续空间、未条件化输出或单目标指导，不适合在多种性质上的离散序列优化。为了解决这一问题，我们提出了PepTune，这是一种基于多目标离散扩散模型，用于同时生成和优化治疗肽的SMILES表示。PepTune基于掩码离散语言模型（MDLM）框架，在状态依赖的掩码调度和基于惩罚的目标下确保有效的肽结构。为了引导扩散过程，我们提出了一种基于蒙特卡洛树搜索（MCTS）的策略，通过平衡探索和利用来逐步完善帕特洛最优序列。MCTS将分类器基的奖励与搜索树的扩展相结合，从而克服离散空间固有的梯度估计挑战和数据稀疏性问题。利用PepTune，我们生成了多种化学修饰的肽，这些肽优化了包括靶点亲和力、膜通透性、溶解性、溶血和非吸附特性在内的多种治疗性质。总体而言，我们的结果表明，MCTS引导的离散扩散是离散状态空间中多目标序列设计的强大且模块化的解决方案。 

---
# An Investigation on the Potential of KAN in Speech Enhancement 

**Title (ZH)**: 对KAN在语音增强领域潜在应用的研究 

**Authors**: Haoyang Li, Yuchen Hu, Chen Chen, Eng Siong Chng  

**Link**: [PDF](https://arxiv.org/pdf/2412.17778)  

**Abstract**: High-fidelity speech enhancement often requires sophisticated modeling to capture intricate, multiscale patterns. Standard activation functions, while introducing nonlinearity, lack the flexibility to fully address this complexity. Kolmogorov-Arnold Networks (KAN), an emerging methodology that employs learnable activation functions on graph edges, present a promising alternative. This work investigates two novel KAN variants based on rational and radial basis functions for speech enhancement. We integrate the rational variant into the 1D CNN blocks of Demucs and the GRU-Transformer blocks of MP-SENet, while the radial variant is adapted to the 2D CNN-based decoders of MP-SENet. Experiments on the VoiceBank-DEMAND dataset show that replacing standard activations with KAN-based activations improves speech quality across both the time-domain and time-frequency domain methods with minimal impact on model size and FLOP, underscoring KAN's potential to improve speech enhancement models. 

**Abstract (ZH)**: 高保真语音增强通常需要复杂的建模来捕捉精细的多尺度模式。标准激活函数虽然引入了非线性，但在应对这种复杂性方面缺乏足够的灵活性。Kolmogorov-Arnold 神经网络（KAN），这是一种新兴的方法，通过在图边使用可学习的激活函数来实现，为语音增强提供了有前景的替代方案。本研究探讨了两种基于有理函数和径向基函数的新型 KAN 变体在语音增强中的应用。我们将在 Demucs 的 1D CNN 块和 MP-SENet 的 GRU-Transformer 块中集成有理函数变体，而径向函数变体则适应 MP-SENet 中基于 2D CNN 的解码器。在 VoiceBank-DEMAND 数据集上的实验表明，用 KAN 基础激活函数替换标准激活函数可以显著提高语音质量，且对模型大小和 FLOP 的影响最小，从而突显了 KAN 在提高语音增强模型性能方面的潜在价值。 

---
# In Case You Missed It: ARC 'Challenge' Is Not That Challenging 

**Title (ZH)**: 如果您没有错过：ARC“挑战”其实并没有那么具有挑战性 

**Authors**: Łukasz Borchmann  

**Link**: [PDF](https://arxiv.org/pdf/2412.17758)  

**Abstract**: ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities. 

**Abstract (ZH)**: 现代大型语言模型（LLM）在ARC挑战集上的表现似乎比在ARC简单集上更困难，主要是由于评估设置限制了直接比较答案选项，而不是固有的复杂性更高。尽管在过去一年里，一些研究人员悄悄转向了更为合适的方案，但这一变化的影响尚未得到广泛认可。我们强调了这个被忽略的变化，展示了相似的评估实践如何错误地暗示了其他基准测试中的推理缺陷，并证明了更公平的方法大幅减少了性能差距（例如在SIQA上），甚至在某些基准测试中（如OpenBookQA）甚至达到了超人类的表现。通过这种方式，我们揭示了评估如何塑造感知难度，并提出了确保多项选择评估准确反映模型实际能力的指南。 

---
# Deliberation in Latent Space via Differentiable Cache Augmentation 

**Title (ZH)**: 通过可微分缓存增强在潜在空间中的深思熟虑 

**Authors**: Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam  

**Link**: [PDF](https://arxiv.org/pdf/2412.17747)  

**Abstract**: Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks. 

**Abstract (ZH)**: 通过生成和关注中间推理步骤，使大规模语言模型（LLMs）“思考更多”的技术在解决复杂问题方面显示出潜力。然而，标准方法在响应前立即生成离散令牌序列，这可能导致显著的延迟成本，并且优化起来具有挑战性。在这项工作中，我们证明了一种固定的LLM可以通过一个离线协处理器增强，该协处理器在模型的关键值（kv）缓存上进行操作。该协处理器通过增加一组旨在提高后续解码保真度的潜在嵌入来增强缓存。我们使用解码器在标准预训练数据上的语言建模损失训练该协处理器，同时保持解码器自身不变。这种方法使模型能够在端到端可微的方式下学习如何将其额外的计算精简到其kv缓存中。由于解码器保持不变，协处理器可以离线异步运行，如果协处理器不可用或某个缓存被认为不需要额外计算，语言模型也可以正常工作。实验结果显示，当缓存被增强时，解码器在许多后续令牌上的困惑度较低。此外，即使没有特定任务的训练，我们的实验也证明缓存增强可以一致地降低困惑度并提高各种推理密集型任务的表现。 

---
# RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation 

**Title (ZH)**: RepoTransBench：面向仓库级别的代码翻译基准数据集 

**Authors**: Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.17744)  

**Abstract**: Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements. 

**Abstract (ZH)**: 代码仓库级别的代码翻译指的是将整个代码仓库从一种编程语言翻译为另一种语言，同时保持源代码仓库的功能。已经提出了许多基准来评估此类代码翻译器的性能。然而，之前的基准主要提供细粒度的样本，集中在代码片段、函数或文件级别的代码翻译上。这些基准并不能准确反映实际需求，因为在实际中往往需要翻译整个代码仓库，涉及更长的代码长度和更复杂的功能。为解决这一问题，我们提出了一个新的基准——RepoTransBench，这是一个包含自动可执行测试套件的真实世界代码仓库级别代码翻译基准。我们在RepoTransBench上进行了实验，评估了11种先进LLM的翻译性能。我们发现，表现最佳的LLM的一次测试成功率（Success@1）仅为7.33%。为进一步探索LLM在代码仓库级别代码翻译中的潜力，我们为LLM提供了与错误相关的新反馈，以进行迭代调试，并观察到平均7.09%的Success@1提升。然而，即使有这种改进，最好的LLM的Success@1也仅为21%，这可能无法满足可靠的自动代码仓库级别翻译的实际需要。最后，我们进行了详细的错误分析，并突出显示了当前LLM在代码仓库级别代码翻译中存在的缺陷，这可能为进一步改进提供参考。 

---
# Chumor 2.0: Towards Benchmarking Chinese Humor Understanding 

**Title (ZH)**: Chumor 2.0：迈向中文幽默理解的基准测试 

**Authors**: Ruiqi He, Yushu He, Longju Bai, Jiarui Liu, Zhenjie Sun, Zenghao Tang, He Wang, Hanchen Xia, Rada Mihalcea, Naihao Deng  

**Link**: [PDF](https://arxiv.org/pdf/2412.17729)  

**Abstract**: Existing humor datasets and evaluations predominantly focus on English, leaving limited resources for culturally nuanced humor in non-English languages like Chinese. To address this gap, we construct Chumor, the first Chinese humor explanation dataset that exceeds the size of existing humor datasets. Chumor is sourced from Ruo Zhi Ba, a Chinese Reddit-like platform known for sharing intellectually challenging and culturally specific jokes. We test ten LLMs through direct and chain-of-thought prompting, revealing that Chumor poses significant challenges to existing LLMs, with their accuracy slightly above random and far below human. In addition, our analysis highlights that human-annotated humor explanations are significantly better than those generated by GPT-4o and ERNIE-4-turbo. We release Chumor at this https URL, our project page is at this https URL, our leaderboard is at this https URL, and our codebase is at this https URL. 

**Abstract (ZH)**: 现有幽默数据集和评估主要集中在英语上，对非英语语言（如中文）中文化细微差别的幽默资源提供了有限的支持。为弥补这一空白，我们构建了Chumor，这是首个规模超过现有幽默数据集的中文幽默解释数据集。Chumor源自Ruo Zhi Ba，这是一个知名的中文Reddit类平台，以分享令人智性挑战和文化特定的笑话而闻名。我们通过直接提示和链式思考提示测试了十种不同的语言模型，结果显示，Chumor 对现有语言模型构成了显著挑战，其准确性略高于随机值，但远低于人类。此外，我们的分析还揭示了人类标注的幽默解释明显优于GPT-4o和ERNIE-4-turbo生成的解释。我们在此处发布Chumor：[链接]，项目页面在此处：[链接]，排行榜在此处：[链接]，代码库在此处：[链接]。 

---
# VidTwin: Video VAE with Decoupled Structure and Dynamics 

**Title (ZH)**: VidTwin：分离结构与动力学的视频VAE 

**Authors**: Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian  

**Link**: [PDF](https://arxiv.org/pdf/2412.17726)  

**Abstract**: Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. Our code has been released at this https URL. 

**Abstract (ZH)**: 最近视频自编码器（Video AEs）的发展显著提高了视频生成的质量和效率。本文提出了一种新颖且紧凑的视频自编码器VidTwin，它将视频分解为两个独立的潜在空间：结构潜在向量，用于捕捉整体内容和全局运动；动态潜在向量，用于表示细粒度细节和快速运动。具体而言，我们的方法采用编码器-解码器结构，并增加了两个子模块分别提取这些潜在空间。第一个子模块使用Q-Former提取低频运动趋势，随后通过下采样块移除冗余的内容细节。第二个子模块在空间维度上平均潜在向量以捕捉快速运动。大量实验表明，VidTwin在MCL-JCV数据集上实现了28.14的高峰值信噪比（PSNR），压缩率高达0.20%，并在下游生成任务中表现出高效性和有效性。此外，我们的模型展示了可解释性和可扩展性，为未来在视频潜在表示和生成方面的研究铺平了道路。我们的代码已经发布在[此链接]。 

---
# FedTLU: Federated Learning with Targeted Layer Updates 

**Title (ZH)**: FedTLU：带有目标层更新的联邦学习 

**Authors**: Jong-Ik Park, Carlee Joe-Wong  

**Link**: [PDF](https://arxiv.org/pdf/2412.17692)  

**Abstract**: Federated learning (FL) addresses privacy concerns in language modeling by enabling multiple clients to contribute to training language models. However, non-IID (identically and independently distributed) data across clients often limits FL's performance. This issue is especially challenging during model fine-tuning, as noise due to variations in clients' data distributions can harm model convergence near the optimum. This paper proposes a targeted layer update strategy for fine-tuning in FL. Instead of randomly updating layers of the language model, as often done in practice, we use a scoring mechanism to identify and update the most critical layers, avoiding excessively noisy or even poisoned updates by freezing the parameters in other layers. We show in extensive experiments that our method improves convergence and performance in non-IID settings, offering a more efficient approach to fine-tuning federated language models. 

**Abstract (ZH)**: 联邦学习（FL）通过使多个客户端能够贡献于语言模型的训练来解决隐私问题。然而，客户端之间的非IID（同分布且独立）数据经常限制FL的性能。特别是在模型微调过程中，客户端数据分布差异引起的噪声可能会对模型在最优解附近的收敛性能造成影响。本文提出了一种针对层更新的目标化策略，用于联邦学习中的模型微调。不同于实践中常用的方法随机更新语言模型的层，我们采用打分机制来识别并更新最关键的层，避免其他层参数因噪声过大或存在污染而导致不必要的更新。实验证明，我们的方法能够改善非IID环境下的收敛性和性能，提供一种更高效的联邦语言模型微调方法。 

---
# SCBench: A Sports Commentary Benchmark for Video LLMs 

**Title (ZH)**: SCBench：用于视频LLM的体育评论基准 

**Authors**: Kuangzhi Ge, Lingjun Chen, Kevin Zhang, Yulin Luo, Tianyu Shi, Liaoyuan Fan, Xiang Li, Guanqun Wang, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17637)  

**Abstract**: Recently, significant advances have been made in Video Large Language Models (Video LLMs) in both academia and industry. However, methods to evaluate and benchmark the performance of different Video LLMs, especially their fine-grained, temporal visual capabilities, remain very limited. On one hand, current benchmarks use relatively simple videos (e.g., subtitled movie clips) where the model can understand the entire video by processing just a few frames. On the other hand, their datasets lack diversity in task format, comprising only QA or multi-choice QA, which overlooks the models' capacity for generating in-depth and precise texts. Sports videos, which feature intricate visual information, sequential events, and emotionally charged commentary, present a critical challenge for Video LLMs, making sports commentary an ideal benchmarking task. Inspired by these challenges, we propose a novel task: sports video commentary generation, developed $\textbf{SCBench}$ for Video LLMs. To construct such a benchmark, we introduce (1) $\textbf{SCORES}$, a six-dimensional metric specifically designed for our task, upon which we propose a GPT-based evaluation method, and (2) $\textbf{CommentarySet}$, a dataset consisting of 5,775 annotated video clips and ground-truth labels tailored to our metric. Based on SCBench, we conduct comprehensive evaluations on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought baseline methods. Our results found that InternVL-Chat-2 achieves the best performance with 5.44, surpassing the second-best by 1.04. Our work provides a fresh perspective for future research, aiming to enhance models' overall capabilities in complex visual understanding tasks. Our dataset will be released soon. 

**Abstract (ZH)**: 近年来，视频大语言模型（Video LLMs）在学术界和工业界都取得了显著进展。然而，评价和基准测试不同Video LLMs的性能方法，尤其是它们的细粒度和时间视觉能力，仍然非常有限。一方面，现有的基准测试使用相对简单的视频（如带有字幕的电影片段），在这种情况下，模型只需处理几帧就能理解整个视频。另一方面，它们的数据集在任务格式方面缺乏多样性，仅包含问答或多项选择问答的形式，这忽视了模型生成深入和精确文本的能力。体育视频以其复杂的视觉信息、连续事件以及情绪化的评论，对Video LLMs构成了关键挑战，使体育解说成为理想的基准测试任务。受这些挑战的启发，我们提出了一项新的任务：体育视频解说生成，并为此开发了SCBench评估工具。

为了构建这一基准，我们引入了（1）SCORES，这是一种六维指标，专门针对我们的任务设计，我们基于此提出了一种基于GPT的评估方法；以及（2）CommentarySet，这是一个包含5,775条标注视频片段和针对我们指标的地面真相标签的数据集。基于SCBench，我们对多种Video LLMs（如VILA、Video-LLaVA等）和链式推理基线方法进行了全面评估。结果显示，InternVL-Chat-2 的表现最佳，得分为5.44，比第二名高出1.04。我们的工作为未来的研究提供了一种新的视角，旨在提高模型在复杂视觉理解任务中的整体能力。我们将很快发布我们的数据集。 

---
# Graph Neural Networks Are Evolutionary Algorithms 

**Title (ZH)**: 图神经网络是进化算法 

**Authors**: Kaichen Ouyang, Shengwei Fu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17629)  

**Abstract**: In this paper, we reveal the intrinsic duality between graph neural networks (GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinct fields. Building on this insight, we propose Graph Neural Evolution (GNE), a novel evolutionary algorithm that models individuals as nodes in a graph and leverages designed frequency-domain filters to balance global exploration and local exploitation. Through the use of these filters, GNE aggregates high-frequency (diversity-enhancing) and low-frequency (stability-promoting) information, transforming EAs into interpretable and tunable mechanisms in the frequency domain. Extensive experiments on benchmark functions demonstrate that GNE consistently outperforms state-of-the-art algorithms such as GA, DE, CMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solution shifts, and noisy environments. Its robustness, adaptability, and superior convergence highlight its practical and theoretical value. Beyond optimization, GNE establishes a conceptual and mathematical foundation linking EAs and GNNs, offering new perspectives for both fields. Its framework encourages the development of task-adaptive filters and hybrid approaches for EAs, while its insights can inspire advances in GNNs, such as improved global information propagation and mitigation of oversmoothing. GNE's versatility extends to solving challenges in machine learning, including hyperparameter tuning and neural architecture search, as well as real-world applications in engineering and operations research. By uniting the dynamics of EAs with the structural insights of GNNs, this work provides a foundation for interdisciplinary innovation, paving the way for scalable and interpretable solutions to complex optimization problems. 

**Abstract (ZH)**: 在本文中，我们揭示了图神经网络（GNNs）与进化算法（EAs）之间的内在二元对立关系，从而将两个传统上截然不同的领域联系起来。基于这一见解，我们提出了图神经进化算法（GNE），这是一种新型的进化算法。在GNE中，个体被建模为图中的节点，并利用设计的时频域滤波器来平衡全局探索和局部利用。通过这些滤波器，GNE可以聚合高频频谱（增强多样性）和低频频谱（促进稳定性）信息，从而将EAs转化为在时频域中可解析和可调节的机制。在标准测试函数上的详尽实验表明，GNE在复杂场景、最优解的变化以及噪声环境中一致地优于诸如遗传算法（GA）、差分进化（DE）、自适应估计分布算法（CMA-ES）、自适应增强进化系统（SDAES）和强化学习引导矫正混合进化（RL-SHADE）等最先进的算法。其鲁棒性、适应性和卓越的收敛性突显了其实用和理论价值。除了优化任务外，GNE还建立了EAs和GNNs之间的概念性和数学基础，为两个领域提供了新的视角。其框架鼓励为EAs开发适应性滤波器和混合方法，其见解还可以激发GNNs的进步，例如改进全局信息传播和缓解过度平滑问题。GNE的多功能性扩展到了解决机器学习中的挑战，如超参数调整和神经架构搜索，以及工程和运筹学中的实际应用。通过结合EAs的动力学和GNNs的结构性见解，本文为跨学科创新提供了基础，为复杂优化问题提供了可扩展和可解析的解决方案。 

---
# Emerging Security Challenges of Large Language Models 

**Title (ZH)**: 大型语言模型面临的新兴安全挑战 

**Authors**: Herve Debar, Sven Dietrich, Pavel Laskov, Emil C. Lupu, Eirini Ntoutsi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17614)  

**Abstract**: Large language models (LLMs) have achieved record adoption in a short period of time across many different sectors including high importance areas such as education [4] and healthcare [23]. LLMs are open-ended models trained on diverse data without being tailored for specific downstream tasks, enabling broad applicability across various domains. They are commonly used for text generation, but also widely used to assist with code generation [3], and even analysis of security information, as Microsoft Security Copilot demonstrates [18]. Traditional Machine Learning (ML) models are vulnerable to adversarial attacks [9]. So the concerns on the potential security implications of such wide scale adoption of LLMs have led to the creation of this working group on the security of LLMs. During the Dagstuhl seminar on "Network Attack Detection and Defense - AI-Powered Threats and Responses", the working group discussions focused on the vulnerability of LLMs to adversarial attacks, rather than their potential use in generating malware or enabling cyberattacks. Although we note the potential threat represented by the latter, the role of the LLMs in such uses is mostly as an accelerator for development, similar to what it is in benign use. To make the analysis more specific, the working group employed ChatGPT as a concrete example of an LLM and addressed the following points, which also form the structure of this report: 1. How do LLMs differ in vulnerabilities from traditional ML models? 2. What are the attack objectives in LLMs? 3. How complex it is to assess the risks posed by the vulnerabilities of LLMs? 4. What is the supply chain in LLMs, how data flow in and out of systems and what are the security implications? We conclude with an overview of open challenges and outlook. 

**Abstract (ZH)**: 大语言模型（LLMs）在短短的时间内，在包括教育领域 [4] 和医疗保健领域 [23] 等多个重要领域实现了前所未有的采用率。LLMs 是基于多样化的数据训练而成的开放性模型，未针对特定下游任务进行定制，这使得它们在各个领域的应用具有广泛的适用性。它们通常用于文本生成，但也在代码生成 [3] 中得到广泛应用，甚至如微软的 Security Copilot 所展示的，用于安全信息分析。传统的机器学习（ML）模型容易受到对抗性攻击的影响 [9]。因此，这种大规模采用LLMs所潜在的安全性影响引发了对LLMs安全工作小组的关注。在“网络攻击检测与防御—基于AI的威胁与应对”达特苏尔研讨会中，工作小组讨论的重点是LLMs对抗性攻击的脆弱性，而不是它们在生成恶意软件或使能网络攻击方面的作用。尽管我们注意到后者所代表的潜在威胁，但在这种用途中LLMs的作用主要是加速开发，这一点与它们在良性用途中的角色相似。为了使分析更具具体性，工作小组以ChatGPT为例，探讨了以下几点，同时也构成了本文的结构：

1. LLMs与传统ML模型在脆弱性方面的不同之处。
2. LLMs中的攻击目标是什么？
3. 如何评估LLMs脆弱性所带来的风险？
4. LLMs的供应链情况如何？数据如何在系统中流动，以及存在哪些安全影响？

最后，本文概述了面临的开放挑战和未来展望。 

---
# AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot Semantic Segmentation 

**Title (ZH)**: AFANet：适应性频域aware网络在弱监督少数样本语义分割中的应用 

**Authors**: Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.17601)  

**Abstract**: Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at this https URL. 

**Abstract (ZH)**: 少样本学习旨在通过利用少量样本学习到的先验知识来识别新概念。然而，对于视觉密集型任务（如少样本语义分割），像素级标注极为耗时且成本高昂。因此，在本文中，我们利用更具挑战性的图像级标注，并提出了一种自适应频率感知网络（AFANet）用于弱监督少样本语义分割（WFSS）。具体来说，我们首先提出了一种跨粒度频率感知模块（CFM），将RGB图像分解为高频率和低频率分布，并通过重新对齐进一步优化语义结构信息。不同于大多数现有WFSS方法使用多模态语言-视觉模型（如CLIP）的离线学习文本信息，在本文中，我们进一步提出了CLIP引导的空间适配模块（CSM），该模块通过在线学习对文本信息进行空间域适配变换，从而为CFM提供丰富的跨模态语义信息。在Pascal-5i和COCO-20i数据集上的广泛实验表明，AFANet达到了最先进的性能。代码已公开，详见此链接：[代码链接]。 

---
# LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context 

**Title (ZH)**: LiveIdeaBench：通过最小上下文评估大语言模型的科学创造力和想法生成能力 

**Authors**: Kai Ruan, Xuan Wang, Jixiang Hong, Hao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.17596)  

**Abstract**: While Large Language Models (LLMs) have demonstrated remarkable capabilities in scientific tasks, existing evaluation frameworks primarily assess their performance using rich contextual inputs, overlooking their ability to generate novel ideas from minimal information. We introduce LiveIdeaBench, a comprehensive benchmark that evaluates LLMs' scientific creativity and divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our framework employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across four key dimensions: originality, feasibility, fluency, and flexibility. Through extensive experimentation with 20 leading models across 1,180 keywords spanning 18 scientific domains, we reveal that scientific creative ability shows distinct patterns from general intelligence metrics. Notably, our results demonstrate that models like QwQ-32B-preview achieve comparable creative performance to top-tier models like o1-preview, despite significant gaps in their general intelligence scores. These findings highlight the importance of specialized evaluation frameworks for scientific creativity and suggest that the development of creative capabilities in LLMs may follow different trajectories than traditional problem-solving abilities. 

**Abstract (ZH)**: 虽然大型语言模型（LLMs）在科学任务中展现了非凡的能力，现有的评估框架主要通过丰富的情境输入来评估它们的表现，而忽视了它们从少量信息中生成新颖想法的能力。我们提出了LiveIdeaBench，这是一个全面的基准，通过单一关键词提示来评估LLMs的科学创造力和发散思维能力。借鉴格林福的创造力理论，我们的框架采用了一组最先进的LLMs动态面板，从原创性、可行性、流畅性和灵活性四个方面评估生成的想法。通过在1,180个关键词（涵盖18个科学领域）上与20个领先模型的广泛实验，我们揭示了科学创造能力与一般智能指标之间存在显著差异。尤为重要的是，我们的结果表明，如QwQ-32B-preview这样的模型在创造性表现上与顶级模型o1-preview相当，尽管其一般智能评分存在显著差距。这些发现强调了专门的评估框架对于科学创造力的重要性，并暗示了开发LLMs的创造性能力可能与传统问题解决能力的不同路径和发展模式。 

---
# V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy 

**Title (ZH)**: V$^2$-SfMLearner: 学习单目深度和 ego 运动以实现多模态无线胶囊内镜技术 

**Authors**: Long Bai, Beilei Cui, Liangyu Wang, Yanheng Li, Shilong Yao, Sishen Yuan, Yanan Wu, Yang Zhang, Max Q.-H. Meng, Zhen Li, Weiping Ding, Hongliang Ren  

**Link**: [PDF](https://arxiv.org/pdf/2412.17595)  

**Abstract**: Deep learning can predict depth maps and capsule ego-motion from capsule endoscopy videos, aiding in 3D scene reconstruction and lesion localization. However, the collisions of the capsule endoscopies within the gastrointestinal tract cause vibration perturbations in the training data. Existing solutions focus solely on vision-based processing, neglecting other auxiliary signals like vibrations that could reduce noise and improve performance. Therefore, we propose V$^2$-SfMLearner, a multimodal approach integrating vibration signals into vision-based depth and capsule motion estimation for monocular capsule endoscopy. We construct a multimodal capsule endoscopy dataset containing vibration and visual signals, and our artificial intelligence solution develops an unsupervised method using vision-vibration signals, effectively eliminating vibration perturbations through multimodal learning. Specifically, we carefully design a vibration network branch and a Fourier fusion module, to detect and mitigate vibration noises. The fusion framework is compatible with popular vision-only algorithms. Extensive validation on the multimodal dataset demonstrates superior performance and robustness against vision-only algorithms. Without the need for large external equipment, our V$^2$-SfMLearner has the potential for integration into clinical capsule robots, providing real-time and dependable digestive examination tools. The findings show promise for practical implementation in clinical settings, enhancing the diagnostic capabilities of doctors. 

**Abstract (ZH)**: 深度学习可以从胶囊内镜视频中预测深度图和胶囊自运动，辅助三维场景重建和病变定位。然而，胶囊内镜在消化道内的碰撞造成了训练数据中的振动干扰。现有解决方案仅侧重于视觉处理，忽视了如振动等其他辅助信号，这些信号能够降低噪声并提高性能。因此，我们提出了V$^2$-SfMLearner，这是一种多模态方法，将振动信号整合到基于视觉的深度估计和胶囊运动估计中，专门用于单目胶囊内镜。我们构建了一个包含振动和视觉信号的多模态胶囊内镜数据集，并开发了一种利用视觉-振动信号的无监督方法，通过多模态学习有效消除振动干扰。具体来说，我们精心设计了振动网络分支和傅里叶融合模块，用于检测和减轻振动噪声。该融合框架与流行的仅视觉算法兼容。在多模态数据集上的广泛验证显示，我们的V$^2$-SfMLearner在性能和鲁棒性方面优于仅视觉算法。无需外部大型设备，V$^2$-SfMLearner具有集成到临床胶囊机器人中的潜力，提供实时可靠的消化道检查工具。研究结果表明，V$^2$-SfMLearner在临床环境中的实际应用具有很大的潜力，可以增强医生的诊断能力。 

---
# Improved Cotton Leaf Disease Classification Using Parameter-Efficient Deep Learning Framework 

**Title (ZH)**: 使用参数高效深度学习框架改进棉花叶片疾病分类 

**Authors**: Aswini Kumar Patra, Tejashwini Gajurel  

**Link**: [PDF](https://arxiv.org/pdf/2412.17587)  

**Abstract**: Cotton crops, often called "white gold," face significant production challenges, primarily due to various leaf-affecting diseases. As a major global source of fiber, timely and accurate disease identification is crucial to ensure optimal yields and maintain crop health. While deep learning and machine learning techniques have been explored to address this challenge, there remains a gap in developing lightweight models with fewer parameters which could be computationally effective for agricultural practitioners. To address this, we propose an innovative deep learning framework integrating a subset of trainable layers from MobileNet, transfer learning, data augmentation, a learning rate decay schedule, model checkpoints, and early stopping mechanisms. Our model demonstrates exceptional performance, accurately classifying seven cotton disease types with an overall accuracy of 98.42% and class-wise precision ranging from 96% to 100%. This results in significantly enhanced efficiency, surpassing recent approaches in accuracy and model complexity. The existing models in the literature have yet to attain such high accuracy, even when tested on data sets with fewer disease types. The substantial performance improvement, combined with the lightweight nature of the model, makes it practically suitable for real-world applications in smart farming. By offering a high-performing and efficient solution, our framework can potentially address challenges in cotton cultivation, contributing to sustainable agricultural practices. 

**Abstract (ZH)**: 棉作物，常被称为“白色黄金”，面临着严重的生产挑战，主要原因是各种叶片病害。由于其是全球主要的纤维来源之一，及时准确的病害鉴定对于确保最佳产量和保持作物健康至关重要。尽管已经探索了深度学习和机器学习技术来应对这一挑战，但在开发轻量级模型（参数较少）方面仍存在差距，这种模型可以在计算上更有效地应用于农业从业者。为了解决这个问题，我们提出了一种创新的深度学习框架，该框架结合了MobileNet的可训练层子集、迁移学习、数据增强、学习率衰减策略、模型检查点和早期停止机制。我们的模型展示了卓越的性能，准确分类了包括七种不同类型棉花病害，总体准确率达到了98.42%，类别精确率范围从96%到100%。这极大地提高了效率，超越了近期方法在准确率和模型复杂度方面的表现。现有的文献中，即使在更少的病害类型数据集上测试，也没有模型能达到如此高的准确率。这种显著的性能提升，结合模型的轻量级特性，使其在智能农业的现实应用中具有实际可行性。通过提供一个高性能和高效的解决方案，我们的框架可以潜在地解决棉花种植中的挑战，促进可持续农业实践。 

---
# HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data 

**Title (ZH)**: HumanVBench：探索基于合成基准数据的MLLMs的人类中心视频理解能力 

**Authors**: Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17574)  

**Abstract**: In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 17 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and temporal alignment, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. 

**Abstract (ZH)**: 在多模态大型语言模型（MLLMs）领域，实现以人类为中心的视频理解仍然是一个艰巨的挑战。现有的基准主要侧重于物体和动作识别，往往忽视了视频内容中人类情感、行为和言语视觉对齐的复杂细微之处。我们推出了HumanVBench，这是一种精心设计的基准，旨在弥补视频MLLMs评估中的这些不足之处。HumanVBench 包含17个仔细设计的任务，涵盖两个主要维度：内在情感和外在表现，这些维度包括静态与动态、基础与复杂以及单模态与跨模态，从而全面探索这些方面。借助两个高级自动化注释管道以及包含分散干扰项的问答生成，HumanVBench 利用多种当前最先进（SOTA）的技术来简化基准数据合成和质量评估，从而最大限度地减少对人类标注的依赖性，尤其是针对以人类为中心的多模态属性。对16个当前最先进视频MLLMs的全面评估揭示了其在当前性能上的明显局限性，尤其是在跨模态和时间对齐方面，这突显了进一步细化以实现更加人性化理解的必要性。HumanVBench 已开源，旨在促进视频MLLMs未来的发展与实际应用。 

---
# Empathetic Response in Audio-Visual Conversations Using Emotion Preference Optimization and MambaCompressor 

**Title (ZH)**: 使用情感偏好优化和MambaCompressor在音频-视觉对话中实现共情响应 

**Authors**: Yeonju Kim, Se Jin Park, Yong Man Ro  

**Link**: [PDF](https://arxiv.org/pdf/2412.17572)  

**Abstract**: Chatbot research is advancing with the growing importance of chatbots in fields that require human interactions, such as customer support and mental health care. Despite these advancements, chatbots still face significant challenges in understanding subtle nuances and managing long conversation histories. To address these issues, our study introduces a dual approach: firstly, we employ Emotional Preference Optimization (EPO) to train chatbots not only with correct responses but also with counter-emotional responses-those that are contextually similar but emotionally divergent. This training enables the model to discern fine nuance distinctions between correct and counter-emotional responses, thereby enhancing the quality of its responses. Secondly, we introduce MambaCompressor to effectively compress and manage extensive conversation histories, significantly reducing time and memory complexities while improving the chatbot's contextual understanding. Our comprehensive experiments across multiple datasets demonstrate that our model significantly outperforms existing models in generating empathetic responses and efficiently managing lengthy dialogues. 

**Abstract (ZH)**: 随着聊天机器人在需要人机交互的领域（如客户服务和心理健康护理）中的重要性日益增加，聊天机器人研究正在不断发展。尽管取得了这些进展，聊天机器人仍面临理解细微差异和管理长期对话历史的显著挑战。为了解决这些问题，本研究提出了一种双重方法：首先，我们采用情感偏好优化（EPO）来训练聊天机器人，不仅采用正确回应，还采用情感上相似但内容上不同的反情感回应。这种训练使模型能够区分正确回应与反情感回应之间的细微差异，从而提高其回应的质量。其次，我们引入了MambaCompressor，用于有效压缩和管理漫长的对话历史，显著降低时间和内存复杂度，同时增强聊天机器人的情境理解能力。我们在多个数据集上的全面实验表明，我们的模型在生成同理心回应和高效管理长对话方面显著优于现有模型。 

---
# The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning 

**Title (ZH)**: 协作遮蔽与目标的动态 duo 对高级遮蔽自编码器学习的影响 

**Authors**: Shentong Mo  

**Link**: [PDF](https://arxiv.org/pdf/2412.17566)  

**Abstract**: Masked autoencoders (MAE) have recently succeeded in self-supervised vision representation learning. Previous work mainly applied custom-designed (e.g., random, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets. However, they ignore the potential role of the self-training (student) model in giving feedback to the teacher for masking and targets. In this work, we present to integrate Collaborative Masking and Targets for boosting Masked AutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple collaborative masking mechanism through linear aggregation across attentions from both teacher and student models. We further propose using the output features from those two models as the collaborative target of the decoder. Our simple and effective framework pre-trained on ImageNet-1K achieves state-of-the-art linear probing and fine-tuning performance. In particular, using ViT-base, we improve the fine-tuning results of the vanilla MAE from 83.6% to 85.7%. 

**Abstract (ZH)**: masked 自动编码器（MAE）最近在自我监督的视觉表示学习中取得了成功。先前的工作主要应用了自定义设计的（例如随机的、块状的）掩码或教师（例如 CLIP）引导的掩码和目标。然而，这些方法忽视了自我训练（学生）模型在为掩码和目标提供反馈给教师方面的潜在作用。在此工作中，我们提出了一种协作掩码和目标的方法，以提升 MAE 的性能，即 CMT-MAE。具体而言，CMT-MAE 采用了一个简单的协作掩码机制，通过来自教师和学生模型的注意力的线性聚合来实现。我们进一步提出将这两个模型的输出特征作为解码器的协作目标。我们的简单而有效的框架在 ImageNet-1K 上预训练后实现了最先进的线性探针和微调性能。特别是，使用 ViT 基础模型，我们将 vanilla MAE 的微调结果从 83.6% 提高到了 85.7%。 

---
# Evaluation of Bio-Inspired Models under Different Learning Settings For Energy Efficiency in Network Traffic Prediction 

**Title (ZH)**: 不同学习环境下生物启发模型在网络流量预测中的能效评估 

**Authors**: Theodoros Tsiolakis, Nikolaos Pavlidis, Vasileios Perifanis, Pavlos Efraimidis  

**Link**: [PDF](https://arxiv.org/pdf/2412.17565)  

**Abstract**: Cellular traffic forecasting is a critical task that enables network operators to efficiently allocate resources and address anomalies in rapidly evolving environments. The exponential growth of data collected from base stations poses significant challenges to processing and analysis. While machine learning (ML) algorithms have emerged as powerful tools for handling these large datasets and providing accurate predictions, their environmental impact, particularly in terms of energy consumption, is often overlooked in favor of their predictive capabilities. This study investigates the potential of two bio-inspired models: Spiking Neural Networks (SNNs) and Reservoir Computing through Echo State Networks (ESNs) for cellular traffic forecasting. The evaluation focuses on both their predictive performance and energy efficiency. These models are implemented in both centralized and federated settings to analyze their effectiveness and energy consumption in decentralized systems. Additionally, we compare bio-inspired models with traditional architectures, such as Convolutional Neural Networks (CNNs) and Multi-Layer Perceptrons (MLPs), to provide a comprehensive evaluation. Using data collected from three diverse locations in Barcelona, Spain, we examine the trade-offs between predictive accuracy and energy demands across these approaches. The results indicate that bio-inspired models, such as SNNs and ESNs, can achieve significant energy savings while maintaining predictive accuracy comparable to traditional architectures. Furthermore, federated implementations were tested to evaluate their energy efficiency in decentralized settings compared to centralized systems, particularly in combination with bio-inspired models. These findings offer valuable insights into the potential of bio-inspired models for sustainable and privacy-preserving cellular traffic forecasting. 

**Abstract (ZH)**: 细胞流量预测是一项关键任务，使网络运营商能够高效地分配资源并应对快速变化的环境中的异常情况。从基站收集的数据指数级增长为数据处理和分析带来了重大挑战。尽管机器学习（ML）算法已成为处理这些大数据集和提供准确预测的强大工具，但它们在能源消耗方面的环境影响，尤其是在预测能力方面的表现，往往被忽视。本研究探讨了两种生物启发模型——脉冲神经网络（SNNs）和基于回声状态网络（ESNs）的储槽计算——在细胞流量预测中的潜力。评估的重点不仅在于它们的预测性能，还在于能效。这些模型在集中和联邦设置中实现，以分析它们在分布式系统中的有效性及其能源消耗。此外，我们将生物启发模型与传统的架构，如卷积神经网络（CNNs）和多层感知器（MLPs）进行比较，以进行全面评估。使用来自西班牙巴塞罗那三个不同地点的数据，我们研究了这些方法在预测准确性与能源需求之间的权衡。研究结果表明，生物启发模型（如SNNs和ESNs）可以在维持与传统架构相当的预测准确性的同时实现显著的能源节约。此外，还测试了联邦实施，以评估其在分布式系统中的能源效率，特别是与生物启发模型的结合使用情况。这些发现为生物启发模型在可持续和隐私保护的细胞流量预测中的潜力提供了宝贵的见解。 

---
# Resource-Aware Arabic LLM Creation: Model Adaptation, Integration, and Multi-Domain Testing 

**Title (ZH)**: 资源感知的阿拉伯语大语言模型创建：模型适应、集成与多领域测试 

**Authors**: Prakash Aryan  

**Link**: [PDF](https://arxiv.org/pdf/2412.17548)  

**Abstract**: This paper presents a novel approach to fine-tuning the Qwen2-1.5B model for Arabic language processing using Quantized Low-Rank Adaptation (QLoRA) on a system with only 4GB VRAM. We detail the process of adapting this large language model to the Arabic domain, using diverse datasets including Bactrian, OpenAssistant, and Wikipedia Arabic corpora. Our methodology involves custom data preprocessing, model configuration, and training optimization techniques such as gradient accumulation and mixed-precision training. We address specific challenges in Arabic NLP, including morphological complexity, dialectal variations, and diacritical mark handling. Experimental results over 10,000 training steps show significant performance improvements, with the final loss converging to 0.1083. We provide comprehensive analysis of GPU memory usage, training dynamics, and model evaluation across various Arabic language tasks, including text classification, question answering, and dialect identification. The fine-tuned model demonstrates robustness to input perturbations and improved handling of Arabic-specific linguistic phenomena. This research contributes to multilingual AI by demonstrating a resource-efficient approach for creating specialized language models, potentially democratizing access to advanced NLP technologies for diverse linguistic communities. Our work paves the way for future research in low-resource language adaptation and efficient fine-tuning of large language models. 

**Abstract (ZH)**: 本文提出了一种使用量化低秩适应（QLoRA）方法对Qwen2-1.5B模型进行微调的新颖方法，使其适用于阿拉伯语处理，并且仅在拥有4GB VRAM的系统上进行。我们详细描述了将这个大型语言模型适应到阿拉伯语领域的过程，使用包括Bactrian、OpenAssistant和阿拉伯维基百科语料库在内的多样化数据集。我们的方法包括自定义数据预处理、模型配置以及诸如梯度累积和混合精度训练等训练优化技术。我们解决了阿拉伯自然语言处理（NLP）中的一些特定挑战，包括词形变化的复杂性、方言差异以及标点符号的处理。在10,000个训练步骤的实验中，我们取得了显著的性能提升，最终损失收敛到0.1083。我们对GPU内存使用情况、训练动态以及在各种阿拉伯语任务上的模型评估进行了全面的分析，包括文本分类、问答以及方言识别。微调后的模型在输入扰动下表现出较强的稳健性，并且在处理阿拉伯语特有的语言现象方面有所改善。这项研究通过展示一种资源高效的创建专门语言模型的方法，为多语言AI领域做出了贡献，可能有助于使先进的NLP技术在各语言社区中更广泛地获取。我们的工作为低资源语言适应以及大型语言模型高效微调的未来研究铺平了道路。 

---
# Concept Discovery in Deep Neural Networks for Explainable Face Anti-Spoofing 

**Title (ZH)**: 深度神经网络中的概念发现及其在可解释面部防欺诈中的应用 

**Authors**: Haoyuan Zhang, Xiangyu Zhu, Li Gao, Jiawei Pan, Kai Pang, Guoying Zhao, Stan Z. Li, Zhen Lei  

**Link**: [PDF](https://arxiv.org/pdf/2412.17541)  

**Abstract**: With the rapid growth usage of face recognition in people's daily life, face anti-spoofing becomes increasingly important to avoid malicious attacks. Recent face anti-spoofing models can reach a high classification accuracy on multiple datasets but these models can only tell people ``this face is fake'' while lacking the explanation to answer ``why it is fake''. Such a system undermines trustworthiness and causes user confusion, as it denies their requests without providing any explanations. In this paper, we incorporate XAI into face anti-spoofing and propose a new problem termed X-FAS (eXplainable Face Anti-Spoofing) empowering face anti-spoofing models to provide an explanation. We propose SPED (SPoofing Evidence Discovery), an X-FAS method which can discover spoof concepts and provide reliable explanations on the basis of discovered concepts. To evaluate the quality of X-FAS methods, we propose an X-FAS benchmark with annotated spoofing evidence by experts. We analyze SPED explanations on face anti-spoofing dataset and compare SPED quantitatively and qualitatively with previous XAI methods on proposed X-FAS benchmark. Experimental results demonstrate SPED's ability to generate reliable explanations. 

**Abstract (ZH)**: 随着面部识别在人们日常生活中的迅速普及，面部防欺骗变得越来越重要，以避免恶意攻击。近年来，面部防欺骗模型在多个数据集上达到了较高的分类准确性，但这些模型只能告诉人们“这是一张假脸”，却没有解释为什么是假脸。这样的系统破坏了其可信度，并导致用户困惑，因为它无理由地拒绝了用户的请求。本文将可解释性AI（XAI）引入面部防欺骗领域，并提出了一种新的问题X-FAS（可解释的面部防欺骗），使得面部防欺骗模型能够提供解释。我们提出了SPED（欺骗证据发现），这是一种X-FAS方法，可以在发现的概念基础上发现欺骗概念并提供可靠的解释。为了评估X-FAS方法的质量，我们提出了一种专家标注的欺骗证据基准。我们在提出的X-FAS基准上对SPED的解释进行了分析，并将SPED与之前的方法进行定量和定性的比较。实验结果表明，SPED具有生成可靠解释的能力。 

---
# CiteBART: Learning to Generate Citations for Local Citation Recommendation 

**Title (ZH)**: CiteBART：学习为局部引文推荐生成引文 

**Authors**: Ege Yiğit Çelik, Selma Tekir  

**Link**: [PDF](https://arxiv.org/pdf/2412.17534)  

**Abstract**: Citations are essential building blocks in scientific writing. The scientific community is longing for support in their generation. Citation generation involves two complementary subtasks: Determining the citation worthiness of a context and, if it's worth it, proposing the best candidate papers for the citation placeholder. The latter subtask is called local citation recommendation (LCR). This paper proposes CiteBART, a custom BART pre-training based on citation token masking to generate citations to achieve LCR. In the base scheme, we mask the citation token in the local citation context to make the citation prediction. In the global one, we concatenate the citing paper's title and abstract to the local citation context to learn to reconstruct the citation token. CiteBART outperforms state-of-the-art approaches on the citation recommendation benchmarks except for the smallest FullTextPeerRead dataset. The effect is significant in the larger benchmarks, e.g., Refseer and ArXiv. We present a qualitative analysis and an ablation study to provide insights into the workings of CiteBART. Our analyses confirm that its generative nature brings about a zero-shot capability. 

**Abstract (ZH)**: 引文是科学写作不可或缺的构建块。科学界迫切需要在这方面获得支持。引文生成包含了两个互补的子任务：确定一个段落的引文价值，并在值得引文的情况下，提出最合适的候选论文供引用。后者被称为局部引文推荐（LCR）。本文提出了一种名为CiteBART的方法，该方法基于引文标记掩蔽的BART预训练，以实现LCR。在基线方案中，我们通过掩蔽局部引文上下文中的引文标记来预测引文。在全局方案中，我们将被引用论文的标题和摘要与局部引文上下文相连接，以便学习重构引文标记。CiteBART在引文推荐基准测试中表现出色，除了最小的FullTextPeerRead数据集之外，均优于其他现有方法。在更大的基准测试中，如Refseer和ArXiv，效果尤为显著。我们进行了定性分析和消减试验，以深入了解CiteBART的工作原理。分析结果确认了其生成性本质带来的零样本能力。 

---
# Double Landmines: Invisible Textual Backdoor Attacks based on Dual-Trigger 

**Title (ZH)**: 双隐形地雷：基于双重触发器的隐形文本后门攻击 

**Authors**: Yang Hou, Qiuling Yue, Lujia Chai, Guozhao Liao, Wenbao Han, Wei Ou  

**Link**: [PDF](https://arxiv.org/pdf/2412.17531)  

**Abstract**: At present, all textual backdoor attack methods are based on single triggers: for example, inserting specific content into the text to activate the backdoor; or changing the abstract text features. The former is easier to be identified by existing defense strategies due to its obvious characteristics; the latter, although improved in invisibility, has certain shortcomings in terms of attack performance, construction of poisoned datasets, and selection of the final poisoning rate. On this basis, this paper innovatively proposes a Dual-Trigger backdoor attack based on syntax and mood, and optimizes the construction of the poisoned dataset and the selection strategy of the final poisoning rate. A large number of experimental results show that this method significantly outperforms the previous methods based on abstract features in attack performance, and achieves comparable attack performance (almost 100% attack success rate) with the insertion-based method. In addition, the two trigger mechanisms included in this method can be activated independently in the application phase of the model, which not only improves the flexibility of the trigger style, but also enhances its robustness against defense strategies. These results profoundly reveal that textual backdoor attacks are extremely harmful and provide a new perspective for security protection in this field. 

**Abstract (ZH)**: 目前，所有基于文本的后门攻击方法都依赖于单一触发器：例如，在文本中插入特定内容以激活后门；或者修改摘要的文本特征。前者的显着特征使其更容易被现有的防御策略识别；而后者的隐蔽性虽有所提高，但在攻击性能、恶意数据集构建以及最终污染比率的选择上仍存在一定的不足。在此基础上，本文创新地提出了基于语法和情绪的双重触发器后门攻击，并优化了恶意数据集的构建方式和最终污染比率的选择策略。大量的实验结果表明，该方法在攻击性能上明显优于基于摘要特征的方法，并在攻击成功率上几乎达到了基于插入方法的水平（近乎100%）。此外，该方法包含的两个触发机制可以在模型的应用阶段独立激活，这不仅提高了触发器风格的灵活性，还增强了其对防御策略的鲁棒性。这些结果深刻揭示了基于文本的后门攻击的极端危害性，并为该领域的安全保护提供了新的视角。 

---
# Constructing Fair Latent Space for Intersection of Fairness and Explainability 

**Title (ZH)**: 构建公平的潜在空间以兼顾公平性与可解释性 

**Authors**: Hyungjun Joo, Hyeonggeun Han, Sehwan Kim, Sangwoo Hong, Jungwoo Lee  

**Link**: [PDF](https://arxiv.org/pdf/2412.17523)  

**Abstract**: As the use of machine learning models has increased, numerous studies have aimed to enhance fairness. However, research on the intersection of fairness and explainability remains insufficient, leading to potential issues in gaining the trust of actual users. Here, we propose a novel module that constructs a fair latent space, enabling faithful explanation while ensuring fairness. The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes, allowing the generation of counterfactual explanations for each type of information. Our module is attached to a pretrained generative model, transforming its biased latent space into a fair latent space. Additionally, since only the module needs to be trained, there are advantages in terms of time and cost savings, without the need to train the entire generative model. We validate the fair latent space with various fairness metrics and demonstrate that our approach can effectively provide explanations for biased decisions and assurances of fairness. 

**Abstract (ZH)**: 随着机器学习模型的应用日益广泛，许多研究致力于提高其公平性。然而，关于公平性和可解释性交集的研究仍然不足，这可能导致实际用户对模型缺乏信任。在此背景下，我们提出了一种新型模块，旨在构建一个公平的潜在空间，在确保公平性的基础上提供忠实的解释。这一公平的潜在空间通过分离和重新分配标签和敏感属性构建，从而能够为每种类型的信息生成反事实解释。该模块附加在预训练的生成模型上，将其有偏的潜在空间转化为公平的潜在空间。此外，由于只需训练该模块，相较于重新训练整个生成模型，这种方法在时间和成本上更具优势。我们通过多种公平性指标验证了公平的潜在空间，并展示了我们的方法能够有效地对有偏决策提供解释和公平保障。 

---
# BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation 

**Title (ZH)**: BEE: 基于基线探索-利用的度量自适应解释 

**Authors**: Oren Barkan, Yehonatan Elisha, Jonathan Weill, Noam Koenigstein  

**Link**: [PDF](https://arxiv.org/pdf/2412.17512)  

**Abstract**: Two prominent challenges in explainability research involve 1) the nuanced evaluation of explanations and 2) the modeling of missing information through baseline representations. The existing literature introduces diverse evaluation metrics, each scrutinizing the quality of explanations through distinct lenses. Additionally, various baseline representations have been proposed, each modeling the notion of missingness differently. Yet, a consensus on the ultimate evaluation metric and baseline representation remains elusive. This work acknowledges the diversity in explanation metrics and baselines, demonstrating that different metrics exhibit preferences for distinct explanation maps resulting from the utilization of different baseline representations and distributions. To address the diversity in metrics and accommodate the variety of baseline representations in a unified manner, we propose Baseline Exploration-Exploitation (BEE) - a path-integration method that introduces randomness to the integration process by modeling the baseline as a learned random tensor. This tensor follows a learned mixture of baseline distributions optimized through a contextual exploration-exploitation procedure to enhance performance on the specific metric of interest. By resampling the baseline from the learned distribution, BEE generates a comprehensive set of explanation maps, facilitating the selection of the best-performing explanation map in this broad set for the given metric. Extensive evaluations across various model architectures showcase the superior performance of BEE in comparison to state-of-the-art explanation methods on a variety of objective evaluation metrics. 

**Abstract (ZH)**: 解释性研究中存在两个突出的挑战：1) 细致评估解释结果；2) 通过基线表示建模缺失信息。现有文献引入了多种多样的评估指标，每种指标从不同角度审视解释的质量。此外，还提出了各种基线表示方法，每种方法以不同的方式建模缺失信息。然而，对于最终的评估指标和基线表示方法仍缺乏共识。本研究认可评估指标和基线表示方法的多样性，表明不同的评估指标对使用不同基线表示和分布得到的不同解释映射表现出偏好。为了应对这些多样性，并以统一的方式适应各种基线表示方法，我们提出了基线探索-利用（BEE）方法。该方法通过将基线建模为一个学习到的随机张量来引入随机性到集成过程中，该张量通过一种基于上下文的探索-利用过程中的优化来遵循基线分布的混合模型，从而提高特定目标评估指标上的性能。通过从学习到的分布中重新采样基线，BEE 生成了一组全面的解释映射，便于从这些广泛分布的集合中选择最适合给定指标的最佳解释映射。针对各种模型架构的广泛评估展示了，在多种客观评估指标中，BEE 在解释方法方面的优越性能优于当前最先进的解释方法。 

---
# An Evaluation Framework for Product Images Background Inpainting based on Human Feedback and Product Consistency 

**Title (ZH)**: 基于人类反馈和产品一致性的产品图像背景修复评估框架 

**Authors**: Yuqi Liang, Jun Luo, Xiaoxi Guo, Jianqi Bi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17504)  

**Abstract**: In product advertising applications, the automated inpainting of backgrounds utilizing AI techniques in product images has emerged as a significant task. However, the techniques still suffer from issues such as inappropriate background and inconsistent product in generated product images, and existing approaches for evaluating the quality of generated product images are mostly inconsistent with human feedback causing the evaluation for this task to depend on manual annotation. To relieve the issues above, this paper proposes Human Feedback and Product Consistency (HFPC), which can automatically assess the generated product images based on two modules. Firstly, to solve inappropriate backgrounds, human feedback on 44,000 automated inpainting product images is collected to train a reward model based on multi-modal features extracted from BLIP and comparative learning. Secondly, to filter generated product images containing inconsistent products, a fine-tuned segmentation model is employed to segment the product of the original and generated product images and then compare the differences between the above two. Extensive experiments have demonstrated that HFPC can effectively evaluate the quality of generated product images and significantly reduce the expense of manual annotation. Moreover, HFPC achieves state-of-the-art(96.4% in precision) in comparison to other open-source visual-quality-assessment models. Dataset and code are available at: this https URL inpainting products dataset/. 

**Abstract (ZH)**: 在产品广告应用中，利用AI技术对产品图像进行背景的自动化修复已成为一个重要课题。然而，现有技术仍然面临诸如背景不合适和生成图像中产品不一致等问题，现有的评估生成产品图像质量的方法大多与人类反馈不一致，导致该任务的评估依赖于人工注释。为了解决上述问题，本文提出了一种名为人类反馈和产品一致性（HFPC）的方法，可以基于两个模块自动评估生成的产品图像。首先，为了解决背景不合适的問題，收集了44,000张自动化修复的产品图像的人类反馈，并基于从BLIP提取的多模态特征和比较学习提取的特征训练了一个奖励模型。其次，为过滤掉包含不一致产品的生成图像，使用了一种微调的分割模型来对原始图像和生成图像中的产品进行分割，并比较两者的差异。广泛的实验表明，HFPC可以有效评估生成产品图像的质量，并显著降低人工注释的成本。此外，HFPC在精度方面（达到96.4%）超过了其他开源视觉质量评估模型。数据集和代码可在以下链接获取：this https URL inpainting products dataset/。 

---
# DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought 

**Title (ZH)**: DRT-o1: 通过长链推理优化的深度 reasoning 翻译

注：在学术翻译中，“reasoning”通常可以根据上下文翻译为“推理”或“推理论证”。这里根据上下文理解为“推理”，如果是指特定领域的术语，可能需要进一步确认其准确含义。以下是更为准确的翻译：

DRT-o1: 通过长链推理优化的深度推理翻译 

**Authors**: Jiaan Wang, Fandong Meng, Yunlong Liang, Jie Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.17498)  

**Abstract**: Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT-o1, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to judge whether the translation in the current round is better than the previous one or not. In this manner, we collect tens of thousands of long-thought MT data, which is used to train our DRT-o1. The experimental results on literature translation demonstrate the effectiveness of the DRT-o1. Using Qwen2.5-7B and Qwen2.5-14B as the backbones, the improvement brought by DRT-o1 achieves 7.33~8.26 BLEU and 1.66~3.36 CometScore. Besides, DRT-o1-7B can outperform QwQ-32B-Preview by 7.82 BLEU and 1.46 CometScore, showing its effectiveness. The project is available at this https URL 

**Abstract (ZH)**: 近年来，O1-like模型作为长链思维（CoT）在推理任务中有效性的代表例子出现了，尤其是在数学和编程任务中。在本文中，我们介绍了DRT-o1，这是一种尝试将长链思维的成功应用到神经机器翻译（NMT）中的尝试。鉴于文献书籍中可能包含比喻和暗喻，将这些文本翻译成目标语言在实践中由于文化差异非常困难。在这种情况下，直译往往无法有效地传达出原文的含义。即使是专业的译者，在翻译过程中也必须花费大量精力来保持语义的一致性。为了模拟大型语言模型（LLM）在NMT中的长链条思维能力，我们首先从现有的文学书籍中挖掘含有比喻或暗喻的句子，然后开发了一个多代理框架，通过长链条思维来翻译这些句子。在多代理框架中，译者在顾问提供的建议下，逐步翻译源句子。为了确保长链条思维的有效性，我们还引入了一个评估器，以判断当前轮次的翻译是否优于前一轮次。通过这种方式，我们收集了数十万条长链条思维的翻译数据，用于训练DRT-o1。在文学翻译实验中的结果证明了DRT-o1的有效性。使用Qwen2.5-7B和Qwen2.5-14B作为基础模型，DRT-o1带来的改进在BLEU评分上达到7.33~8.26之间，在CometScore上达到1.66~3.36之间。此外，DRT-o1-7B的BLEU评分和CometScore分别比QwQ-32B-Preview高出7.82和1.46，显示出其有效性。该项目可在以下链接中获取：[此处链接] 

---
# A Toolkit for Virtual Reality Data Collection 

**Title (ZH)**: 虚拟现实数据收集工具包 

**Authors**: Tim Rolff, Niklas Hypki, Markus Lappe, Frank Steinicke  

**Link**: [PDF](https://arxiv.org/pdf/2412.17490)  

**Abstract**: Due to the still relatively low number of users, acquiring large-scale and multidimensional virtual reality datasets remains a significant challenge. Consequently, VR datasets comparable in size to state-of-the-art collections in natural language processing or computer vision are rare or absent. However, the availability of such datasets could unlock groundbreaking advancements in deep-learning, psychological modeling, and data analysis in the context of VR. In this paper, we present a versatile data collection toolkit designed to facilitate the capturing of extensive VR datasets. Our toolkit seamlessly integrates with any device, either directly via OpenXR or through the use of a virtual device. Additionally, we introduce a robust data collection pipeline that emphasizes ethical practices (e.g., ensuring data protection and regulation) and ensures a standardized, reproducible methodology. 

**Abstract (ZH)**: 由于目前用户数量相对较少，获取大规模和多维度的虚拟现实数据集仍然是一项重大挑战。因此，与自然语言处理或计算机视觉领域最先进的数据集规模相当或相匹的数据集是罕见或不存在的。然而，这些数据集的可用性可以为深度学习、心理建模和虚拟现实环境下的数据分析解锁重大突破。在本文中，我们介绍了一个多功能的数据采集工具包，旨在简化广泛虚拟现实数据集的捕获过程。我们的工具包能够无缝集成到任何设备中，无论是通过OpenXR直接集成，还是通过虚拟设备进行集成。此外，我们还引入了一个注重伦理实践（例如，确保数据保护和遵守法规）并确保标准化、可重复的研究方法的数据采集管道。 

---
# Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings 

**Title (ZH)**: 现今学生是否大规模使用ChatGPT？关于在教育环境中使用大型语言模型（如ChatGPT）的一项调查 

**Authors**: Jérémie Sublime, Ilaria Renna  

**Link**: [PDF](https://arxiv.org/pdf/2412.17486)  

**Abstract**: The rapid adoption of Generative AI (GenAI) based on Large Language Models (LLMs) such as ChatGPT has recently and profoundly impacted education, offering transformative opportunities while raising significant concerns. In this study we present the results of a survey that investigates how 395 students aged 13 to 25 years old in France and Italy integrate LLMs into their educational routines.
Key findings include the widespread use of these tools across all age groups and disciplines, with older students and male students demonstrating higher usage frequencies, particularly in scientific contexts. The results also show gender disparities, raising concerns about an emerging AI literacy and technological gender gap. Additionally, while most students utilise LLMs constructively, the lack of systematic proofreading and critical evaluation among younger users suggests potential risks to cognitive skills development, including critical thinking and foundational knowledge. The survey results underscore the need for educational institutions to adapt their curricula to integrate AI tools effectively, promoting ethical use, critical thinking, and awareness of AI limitations and environmental costs. This paper provides actionable recommendations for fostering equitable and effective cohabitation of LLMs and education while addressing emerging challenges. 

**Abstract (ZH)**: 基于大型语言模型（LLMs）如ChatGPT的生成式人工智能（GenAI）的快速采用，最近已经在教育领域产生了深远的影响，既有重大的变革机遇，也引发了诸多担忧。本研究通过一项调查，探讨了395名年龄在13至25岁之间的法国和意大利学生如何将LLMs融入他们的学习习惯中。

关键发现包括这些工具在所有年龄段和学科中的广泛应用，尤其是在科学领域，年长学生和男性学生展现出更高的使用频率。调查结果还揭示了性别差异，这引发了人们对新兴的人工智能素养和性别技术差距的担忧。同时，尽管大多数学生能够有效地利用LLMs，但年轻用户缺乏系统的校对和批判性评估，这可能对批判性思维和基础知识的发展构成潜在风险。调查结果强调了教育机构需要调整其课程，以更好地整合AI工具，并促进道德使用、批判性思维以及对AI局限性和环境成本的认识。本文提供了关于如何促进LLMs与教育共生并应对新兴挑战的实际建议。 

---
# Power- and Fragmentation-aware Online Scheduling for GPU Datacenters 

**Title (ZH)**: 面向GPU数据中心的功率与碎片aware在线调度算法 

**Authors**: Francesco Lettich, Emanuele Carlini, Franco Maria Nardini, Raffaele Perego, Salvatore Trani  

**Link**: [PDF](https://arxiv.org/pdf/2412.17484)  

**Abstract**: The rise of Artificial Intelligence and Large Language Models is driving increased GPU usage in data centers for complex training and inference tasks, impacting operational costs, energy demands, and the environmental footprint of large-scale computing infrastructures. This work addresses the online scheduling problem in GPU datacenters, which involves scheduling tasks without knowledge of their future arrivals. We focus on two objectives: minimizing GPU fragmentation and reducing power consumption. GPU fragmentation occurs when partial GPU allocations hinder the efficient use of remaining resources, especially as the datacenter nears full capacity. A recent scheduling policy, Fragmentation Gradient Descent (FGD), leverages a fragmentation metric to address this issue. Reducing power consumption is also crucial due to the significant power demands of GPUs. To this end, we propose PWR, a novel scheduling policy to minimize power usage by selecting power-efficient GPU and CPU combinations. This involves a simplified model for measuring power consumption integrated into a Kubernetes score plugin. Through an extensive experimental evaluation in a simulated cluster, we show how PWR, when combined with FGD, achieves a balanced trade-off between reducing power consumption and minimizing GPU fragmentation. 

**Abstract (ZH)**: 人工智能和大型语言模型的发展正推动数据中心对GPU的需求增加，以应对复杂训练和推理任务，从而影响运营成本、能源需求以及大规模计算基础设施的环境足迹。本文着眼于解决GPU数据中心的在线调度问题，即在不了解未来任务到达情况的前提下进行任务调度。我们主要关注两个目标：减少GPU碎片化和降低功耗。GPU碎片化指的是部分GPU分配妨碍了对剩余资源的有效利用，尤其是在数据中心接近满载时更为明显。最近提出的一种调度策略——碎片度梯度下降（FGD）——利用一种碎片度度量来解决这个问题。降低功耗同样至关重要，因为GPU的功耗需求非常大。为此，我们提出了一种新的调度策略PWR，旨在通过选择功耗效率高的GPU和CPU组合来最小化功耗。这涉及将一个简化后的功耗测量模型集成到Kubernetes评分插件中。通过在模拟集群中进行广泛实验评估，我们展示了PWR与FGD结合时，如何在降低功耗和减少GPU碎片化之间实现平衡。 

---
# Signal Transformation for Effective Multi-Channel Signal Processing 

**Title (ZH)**: 有效的多通道信号处理中的信号变换方法 

**Authors**: Sunil Kumar Kopparapu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17478)  

**Abstract**: Electroencephalography (EEG) is an non-invasive method to record the electrical activity of the brain. The EEG signals are low bandwidth and recorded from multiple electrodes simultaneously in a time synchronized manner. Typical EEG signal processing involves extracting features from all the individual channels separately and then fusing these features for downstream applications. In this paper, we propose a signal transformation, using basic signal processing, to combine the individual channels of a low-bandwidth signal, like the EEG into a single-channel high-bandwidth signal, like audio. Further this signal transformation is bi-directional, namely the high-bandwidth single-channel can be transformed to generate the individual low-bandwidth signals without any loss of information. Such a transformation when applied to EEG signals overcomes the need to process multiple signals and allows for a single-channel processing. The advantage of this signal transformation is that it allows the use of pre-trained single-channel pre-trained models, for multi-channel signal processing and analysis. We further show the utility of the signal transformation on publicly available EEG dataset. 

**Abstract (ZH)**: 脑电图（EEG）是一种无创方法，用于记录大脑的电信号活动。EEG信号具有低带宽特性，并且可以通过多个电极同时进行同步记录。典型的EEG信号处理通常包括分别从所有独立通道提取特征，然后将这些特征融合以供下游应用使用。本文提出了一种信号转换方法，利用基本信号处理技术，将类似EEG这样的低带宽信号的各个独立通道合并为一个高带宽的单通道信号，类似于声音信号。此外，这种信号转换是双向的，即可以从高带宽的单通道信号生成各个低带宽的独立信号，而不丢失任何信息。将这种转换应用于EEG信号，可以克服处理多个信号的需求，并允许进行单通道处理。这种信号转换的优势在于，它允许使用预训练的单通道模型进行多通道信号的处理和分析。我们还展示了这种信号转换方法在公开可用的EEG数据集上的有效性。 

---
# Line Graph Vietoris-Rips Persistence Diagram for Topological Graph Representation Learning 

**Title (ZH)**: 拓扑图表示学习中的线图 Vietoris-Rips 坚持图表示方法 

**Authors**: Jaesun Shin, Eunjoo Jeon, Taewon Cho, Namkyeong Cho, Youngjune Gwon  

**Link**: [PDF](https://arxiv.org/pdf/2412.17468)  

**Abstract**: While message passing graph neural networks result in informative node embeddings, they may suffer from describing the topological properties of graphs. To this end, node filtration has been widely used as an attempt to obtain the topological information of a graph using persistence diagrams. However, these attempts have faced the problem of losing node embedding information, which in turn prevents them from providing a more expressive graph representation. To tackle this issue, we shift our focus to edge filtration and introduce a novel edge filtration-based persistence diagram, named Topological Edge Diagram (TED), which is mathematically proven to preserve node embedding information as well as contain additional topological information. To implement TED, we propose a neural network based algorithm, named Line Graph Vietoris-Rips (LGVR) Persistence Diagram, that extracts edge information by transforming a graph into its line graph. Through LGVR, we propose two model frameworks that can be applied to any message passing GNNs, and prove that they are strictly more powerful than Weisfeiler-Lehman type colorings. Finally we empirically validate superior performance of our models on several graph classification and regression benchmarks. 

**Abstract (ZH)**: 尽管消息传递图神经网络能够生成具有信息性的节点嵌入，但在描述图的拓扑属性方面可能存在不足。为解决这一问题，节点过滤方法被广泛应用于通过持久图获取图的拓扑信息，但由于这些方法可能导致节点嵌入信息的丢失，从而阻碍了对更具表现力的图表示的提供。为应对这一挑战，我们转向边过滤方法，并引入了一种新颖的基于边过滤的持久图——称为拓扑边图（TED），该方法已被数学证明能够保留节点嵌入信息的同时还包含额外的拓扑信息。为了实现TED，我们提出了一个基于神经网络的算法——线图 Vietoris-Rips （LGVR）持久图，通过将图转化为线图以提取边信息。借助LGVR，我们提出了两种可用于任何消息传递图神经网络的模型框架，并证明它们在Weisfeiler-Lehman类型着色方面更为强大。最后，我们在多个图分类和回归基准测试上进行了实证验证，证明了我们模型的优越性能。 

---
# Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection 

**Title (ZH)**: 工业异常检测中 progressive 边界引导异常合成的研究 

**Authors**: Qiyu Chen, Huiyuan Luo, Han Gao, Chengkan Lv, Zhengtao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17458)  

**Abstract**: Unsupervised anomaly detection methods can identify surface defects in industrial images by leveraging only normal samples for training. Due to the risk of overfitting when learning from a single class, anomaly synthesis strategies are introduced to enhance detection capability by generating artificial anomalies. However, existing strategies heavily rely on anomalous textures from auxiliary datasets. Moreover, their limitations in the coverage and directionality of anomaly synthesis may result in a failure to capture useful information and lead to significant redundancy. To address these issues, we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS) strategy, which can directionally synthesize crucial feature-level anomalies without auxiliary textures. It consists of three core components: Approximate Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary Optimization (RBO). To make the distribution of normal samples more compact, ABL first learns an approximate decision boundary by center constraint, which improves the center initialization through feature alignment. AFS then directionally synthesizes anomalies with more flexible scales guided by the hypersphere distribution of normal features. Since the boundary is so loose that it may contain real anomalies, RBO refines the decision boundary through the binary classification of artificial anomalies and normal features. Experimental results show that our method achieves state-of-the-art performance and the fastest detection speed on three widely used industrial datasets, including MVTec AD, VisA, and MPDD. The code will be available at: this https URL. 

**Abstract (ZH)**: 无监督异常检测方法可以通过仅利用正常样本进行训练来识别工业图像中的表面缺陷。由于从单一类别学习时存在过拟合的风险，因此引入了异常合成策略以通过生成人工异常来增强检测能力。然而，现有策略严重依赖辅助数据集中异常的纹理。此外，它们在异常合成的覆盖范围和方向性方面的局限性可能导致难以捕捉有用信息，并导致显著的冗余。为了解决这些问题，我们提出了一种新颖的渐进边界引导异常合成（PBAS）策略，该策略能够在不依赖辅助纹理的情况下，定向地合成关键特征级别的异常。该策略包含三个核心组件：近似边界学习（ABL）、异常特征合成（AFS）和精细边界优化（RBO）。

为了使正常样本的分布更加紧凑，ABL 首先通过中心约束学习一个近似的决策边界，从而通过特征对齐改进中心初始化。然后，AFS 根据正常特征的超球体分布灵活地定向生成异常。由于边界过于宽松，可能包含真正的异常，因此 RBO 通过对人工异常和正常特征进行二分类来细化决策边界。实验结果表明，我们的方法在三个广泛使用的工业数据集（包括 MVTec AD、VisA 和 MPDD）上实现了最先进的性能和最快的检测速度。代码将在以下链接处提供：this https URL。 

---
# Diving into Self-Evolving Training for Multimodal Reasoning 

**Title (ZH)**: 探索自进化训练在多模态推理中的应用 

**Authors**: Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He  

**Link**: [PDF](https://arxiv.org/pdf/2412.17451)  

**Abstract**: Reasoning ability is essential for Large Multimodal Models (LMMs). In the absence of multimodal chain-of-thought annotated data, self-evolving training, where the model learns from its own outputs, has emerged as an effective and scalable approach for enhancing reasoning abilities. Despite its growing usage, a comprehensive understanding of self-evolving training, particularly in the context of multimodal reasoning, remains limited. In this paper, we delve into the intricacies of self-evolving training for multimodal reasoning, pinpointing three key factors: Training Method, Reward Model, and Prompt Variation. We systematically examine each factor and explore how various configurations affect the training's effectiveness. Our analysis leads to a set of best practices for each factor, aimed at optimizing multimodal reasoning. Furthermore, we explore the Self-Evolution Dynamics during training and the impact of automatic balancing mechanisms in boosting performance. After all the investigations, we present a final recipe for self-evolving training in multimodal reasoning, encapsulating these design choices into a framework we call MSTaR (Multimodal Self-evolving Training for Reasoning), which is universally effective for models with different sizes on various benchmarks, e.g., surpassing the pre-evolved model significantly on 5 multimodal reasoning benchmarks without using additional human annotations, as demonstrated on MiniCPM-V-2.5 (8B), Phi-3.5-Vision (4B) and InternVL2 (2B). We believe this study fills a significant gap in the understanding of self-evolving training for multimodal reasoning and offers a robust framework for future research. Our policy and reward models, as well as the collected data, is released to facilitate further investigation in multimodal reasoning. 

**Abstract (ZH)**: 逻辑推理能力对于大型多模态模型（LMMs）至关重要。在缺乏多模态链式思维标注数据的情况下，自我进化训练，即模型从自身输出中学习，已成为提升推理能力的有效且可扩展的方法。尽管这种方法的使用正在不断增加，但在多模态推理的具体情境下对自我进化训练的全面理解依然有限。在本文中，我们深入探讨了自我进化训练在多模态推理中的复杂性，指出了三个关键因素：训练方法、奖励模型和提示变异。我们系统地分析了每个因素，并探讨了各种配置如何影响训练的有效性。我们的分析得出了每个因素的最佳实践，旨在优化多模态推理。此外，我们还探讨了训练过程中的自我进化动态以及自动平衡机制对于提升性能的影响。经过一系列调查后，我们提出了适用于多模态推理的自我进化训练的最终食谱，将其设计理念整合成一个名为MSTaR（多模态自我进化训练推理）的框架，该框架对于不同规模的模型在各种基准上都是普遍有效的。例如，在MiniCPM-V-2.5（8B）、Phi-3.5-Vision（4B）和InternVL2（2B）这三种模型上，MSTaR在不使用额外人工注释的情况下，显著超越了预进化模型，在5个多模态推理基准测试中表现尤为突出。我们相信，这项研究填补了多模态推理领域自我进化训练理解的空白，并提供了一个坚实的框架，以支持未来的研究。我们的政策和奖励模型以及收集的数据被公开发布，以促进对该领域进一步调查。 

---
# Applying LLM and Topic Modelling in Psychotherapeutic Contexts 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

Applying LLM and Topic Modelling in Psychotherapeutic Contexts  
大语言模型和主题建模在心理治疗情境中的应用 

**Authors**: Alexander Vanin, Vadim Bolshev, Anastasia Panfilova  

**Link**: [PDF](https://arxiv.org/pdf/2412.17449)  

**Abstract**: This study explores the use of Large language models to analyze therapist remarks in a psychotherapeutic setting. The paper focuses on the application of BERTopic, a machine learning-based topic modeling tool, to the dialogue of two different groups of therapists (classical and modern), which makes it possible to identify and describe a set of topics that consistently emerge across these groups. The paper describes in detail the chosen algorithm for BERTopic, which included creating a vector space from a corpus of therapist remarks, reducing its dimensionality, clustering the space, and creating and optimizing topic representation. Along with the automatic topical modeling by the BERTopic, the research involved an expert assessment of the findings and manual topic structure optimization. The topic modeling results highlighted the most common and stable topics in therapists speech, offering insights into how language patterns in therapy develop and remain stable across different therapeutic styles. This work contributes to the growing field of machine learning in psychotherapy by demonstrating the potential of automated methods to improve both the practice and training of therapists. The study highlights the value of topic modeling as a tool for gaining a deeper understanding of therapeutic dialogue and offers new opportunities for improving therapeutic effectiveness and clinical supervision. 

**Abstract (ZH)**: 本研究探讨了大型语言模型在心理治疗环境中分析 therapists 讲话内容的应用。论文重点介绍了机器学习主题建模工具 BERTopic 在两类不同 therapists（古典与现代）对话中的应用，以识别并描述这两个群体中反复出现的主题集。论文详细描述了所选 BERTopic 算法，包括从 therapists 的讲话语料库创建向量空间、降低维度、聚类空间、创建和优化主题表示。除了 BERTopic 的自动主题建模外，该研究还涉及专家对研究结果的评估以及手动优化主题结构。主题建模结果突出了 therapists 发言中最常见且稳定的主题，提供了一种理解治疗中语言模式如何发展并跨不同治疗风格保持稳定的洞见。这项研究为机器学习在心理治疗中的发展做出了贡献，展示了自动化方法提高 therapists 的实践和培训的潜力。研究强调了主题建模作为工具的价值，有助于更深入地理解治疗性对话，并为提高治疗效果和临床督导提供了新的机会。 

---
# Neural Continuous-Time Supermartingale Certificates 

**Title (ZH)**: 神经连续时间超鞅证书 

**Authors**: Grigory Neustroev, Mirco Giacobbe, Anna Lukina  

**Link**: [PDF](https://arxiv.org/pdf/2412.17432)  

**Abstract**: We introduce for the first time a neural-certificate framework for continuous-time stochastic dynamical systems. Autonomous learning systems in the physical world demand continuous-time reasoning, yet existing learnable certificates for probabilistic verification assume discretization of the time continuum. Inspired by the success of training neural Lyapunov certificates for deterministic continuous-time systems and neural supermartingale certificates for stochastic discrete-time systems, we propose a framework that bridges the gap between continuous-time and probabilistic neural certification for dynamical systems under complex requirements. Our method combines machine learning and symbolic reasoning to produce formally certified bounds on the probabilities that a nonlinear system satisfies specifications of reachability, avoidance, and persistence. We present both the theoretical justification and the algorithmic implementation of our framework and showcase its efficacy on popular benchmarks. 

**Abstract (ZH)**: 我们首次提出了一个适用于连续时间随机动力系统的神经证书框架。物理世界中的自主学习系统需要进行连续时间推理，但现有的可用于概率验证的学习型证书假定将时间连续性离散化。受训练神经李雅普诺夫证书在确定性连续时间系统上的成功以及神经超鞅证书在随机离散时间系统上的成功的启发，我们提出了一种框架，用于解决复杂要求下动力系统的连续时间与概率神经认证之间的差距。我们的方法结合了机器学习和符号推理，以生成非线性系统满足可达性、避免性和持久性规范的概率形式上可验证边界。我们提供了该框架的理论依据及其算法实现，并在流行的基准测试上展示了其实用性。 

---
# VidCtx: Context-aware Video Question Answering with Image Models 

**Title (ZH)**: VidCtx：基于图像模型的上下文感知视频问答 

**Authors**: Andreas Goulas, Vasileios Mezaris, Ioannis Patras  

**Link**: [PDF](https://arxiv.org/pdf/2412.17415)  

**Abstract**: To address computational and memory limitations of Large Multimodal Models in the Video Question-Answering task, several recent methods extract textual representations per frame (e.g., by captioning) and feed them to a Large Language Model (LLM) that processes them to produce the final response. However, in this way, the LLM does not have access to visual information and often has to process repetitive textual descriptions of nearby frames. To address those shortcomings, in this paper, we introduce VidCtx, a novel training-free VideoQA framework which integrates both modalities, i.e. both visual information from input frames and textual descriptions of others frames that give the appropriate context. More specifically, in the proposed framework a pre-trained Large Multimodal Model (LMM) is prompted to extract at regular intervals, question-aware textual descriptions (captions) of video frames. Those will be used as context when the same LMM will be prompted to answer the question at hand given as input a) a certain frame, b) the question and c) the context/caption of an appropriate frame. To avoid redundant information, we chose as context the descriptions of distant frames. Finally, a simple yet effective max pooling mechanism is used to aggregate the frame-level decisions. This methodology enables the model to focus on the relevant segments of the video and scale to a high number of frames. Experiments show that VidCtx achieves competitive performance among approaches that rely on open models on three public Video QA benchmarks, NExT-QA, IntentQA and STAR. 

**Abstract (ZH)**: 为了解决大规模多模态模型在视频问答任务中计算和内存限制的问题，最近的一些方法通过为每一帧提取文本表示（例如，通过添加字幕），并将这些文本输入到大规模语言模型（LLM）中进行处理，以生成最终回答。但是，这种做法使得LLM无法访问视觉信息，并且经常需要处理相邻帧的重复文本描述。为了克服这些不足，本文提出了一种名为VidCtx的新颖训练免费视频问答框架，该框架结合了两种模态的信息，即输入帧的视觉信息和描述其他帧的文本描述，以提供适当的上下文。具体而言，在提出的框架中，预训练的大规模多模态模型（LMM）在固定的时间间隔内被提示提取问题感知的视频帧文本描述（字幕）。这些字幕将用于回答给定输入的问题时的上下文，即a）某一帧，b）问题以及c）合适帧的上下文/字幕。为了避免冗余信息，我们将帧间距离较远的字幕作为上下文。最后，我们使用简单的最大池化机制来汇总帧级别的决策。该方法使模型能够专注于视频中的相关段落，并可扩展到大量帧。实验结果显示，VidCtx在三个公开的视频问答基准测试NExT-QA、IntentQA和STAR上，与依赖开放式模型的方法相比，取得了竞争力的表现。 

---
# Pretraining with random noise for uncertainty calibration 

**Title (ZH)**: 使用随机噪声进行预训练以校准不确定性 

**Authors**: Jeonghwan Cheon, Se-Bum Paik  

**Link**: [PDF](https://arxiv.org/pdf/2412.17411)  

**Abstract**: Uncertainty calibration, the process of aligning confidence with accuracy, is a hallmark of human intelligence. However, most machine learning models struggle to achieve this alignment, particularly when the training dataset is small relative to the network's capacity. Here, we demonstrate that uncertainty calibration can be effectively achieved through a pretraining method inspired by developmental neuroscience. Specifically, training with random noise before data training allows neural networks to calibrate their uncertainty, ensuring that confidence levels are aligned with actual accuracy. We show that randomly initialized, untrained networks tend to exhibit erroneously high confidence, but pretraining with random noise effectively calibrates these networks, bringing their confidence down to chance levels across input spaces. As a result, networks pretrained with random noise exhibit optimal calibration, with confidence closely aligned with accuracy throughout subsequent data training. These pre-calibrated networks also perform better at identifying "unknown data" by exhibiting lower confidence for out-of-distribution samples. Our findings provide a fundamental solution for uncertainty calibration in both in-distribution and out-of-distribution contexts. 

**Abstract (ZH)**: 不确定性校准，即使信心与准确性相一致的过程，是人类智能的一个重要特征。然而，大多数机器学习模型在训练数据集相对较小且网络容量较大时，难以实现这一点。在这里，我们展示了通过借鉴发展神经科学的方法进行预训练，可以有效地实现不确定性校准。具体来说，在数据训练之前用随机噪声进行训练可以让神经网络校准其不确定性，确保信心水平与实际准确性相一致。我们发现，随机初始化但未经过训练的网络往往会表现出错误的高信心，而用随机噪声进行预训练可以有效校准这些网络，在输入空间中使其信心水平降至随机水平。因此，用随机噪声进行预训练的网络在后续数据训练过程中表现出最佳的校准效果，信心水平与准确性高度一致。此外，这些预校准的网络在识别“未知数据”方面也表现出优异的性能，对于分布外样本表现出较低的信心。我们的研究为在分布内和分布外情境下的不确定性校准提供了基本的解决方案。 

---
# Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement 

**Title (ZH)**: 奇异值缩放：通过剪枝权重 refinement 提高生成模型效率的方法 

**Authors**: Hyeonjin Kim, Jaejun Yoo  

**Link**: [PDF](https://arxiv.org/pdf/2412.17387)  

**Abstract**: While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs. Our code is available at: this https URL. 

**Abstract (ZH)**: 尽管剪枝方法有效地在无需额外训练成本的情况下维持了模型性能，但它们往往仅仅专注于保留关键连接，忽视了剪枝权重对后续微调或知识蒸馏的影响，从而导致了效率问题。此外，大多数针对生成模型的压缩技术主要针对生成对抗网络（GANs），并针对特定架构，如StyleGAN进行定制，而对扩散模型（Diffusion models）的研究才刚刚开始。更重要的是，这些方法往往只适用于GANs或扩散模型，这凸显了需要适用于不同模型类型的方法的需求。在这篇论文中，我们引入了一种适用于多种模型类型的稳健技术——奇异值缩放（Singular Value Scaling, SVS），用于改进剪枝后的权重。我们的分析表明，剪枝权重通常表现出主导奇异向量，这妨碍了微调效率，并导致与随机初始化相比，性能不理想。我们通过最小化剪枝权重奇异值之间的差异来增强权重初始化方法，从而改善微调过程。这种方法不仅引导压缩模型达到更优解，还能显著加快微调速度。在StyleGAN2、StyleGAN3和DDPM上的广泛实验表明，SVS在无需额外训练成本的情况下，提高了不同模型类型下的压缩性能。我们的代码可在以下链接获取：this https URL。 

---
# A Plug-and-Play Physical Motion Restoration Approach for In-the-Wild High-Difficulty Motions 

**Title (ZH)**: 一种适用于野生环境下高难度动作的即插即用物理运动恢复方法 

**Authors**: Youliang Zhang, Ronghui Li, Yachao Zhang, Liang Pan, Jingbo Wang, Yebin Liu, Xiu Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.17377)  

**Abstract**: Extracting physically plausible 3D human motion from videos is a critical task. Although existing simulation-based motion imitation methods can enhance the physical quality of daily motions estimated from monocular video capture, extending this capability to high-difficulty motions remains an open challenge. This can be attributed to some flawed motion clips in video-based motion capture results and the inherent complexity in modeling high-difficulty motions. Therefore, sensing the advantage of segmentation in localizing human body, we introduce a mask-based motion correction module (MCM) that leverages motion context and video mask to repair flawed motions, producing imitation-friendly motions; and propose a physics-based motion transfer module (PTM), which employs a pretrain and adapt approach for motion imitation, improving physical plausibility with the ability to handle in-the-wild and challenging motions. Our approach is designed as a plug-and-play module to physically refine the video motion capture results, including high-difficulty in-the-wild motions. Finally, to validate our approach, we collected a challenging in-the-wild test set to establish a benchmark, and our method has demonstrated effectiveness on both the new benchmark and existing public this http URL://physicalmotionrestoration.this http URL 

**Abstract (ZH)**: 从视频中提取具物理可接受性的3D人体运动是一项关键任务。尽管现有的基于模拟的人体运动模仿方法能够增强从单目视频捕获估计的日常动作的物理质量，但将这种能力扩展到高难度动作仍然是一个开放的挑战。这主要归因于视频基于的动作捕捉结果中存在的某些错误动作片段以及建模高难度动作的内在复杂性。因此，我们利用分割的优势来精确定位人体，引入了一个基于掩码的动作修正模块（MCM），该模块利用动作上下文和视频掩码来修复错误的动作，生成模仿友好的动作；同时提出了一个基于物理的动作转移模块（PTM），该模块采用预训练和适应的方法进行动作模仿，通过处理真实世界和具有挑战认的运动，提高了动作的物理可信度。我们的方法设计为即插即用模块，用于物理细化视频动作捕捉结果，包括真实世界中的高难度动作。最后，为了验证我们的方法，我们收集了一个具有挑战性的真实世界测试集来建立一个基准，并且我们的方法在新基准和现有的公开数据集上都展现了有效性。有关更多信息，请参阅：http://physicalmotionrestoration this http URL 

---
# Boosting LLM via Learning from Data Iteratively and Selectively 

**Title (ZH)**: 通过迭代选择性地从数据中学习增强LLM 

**Authors**: Qi Jia, Siyu Ren, Ziheng Qin, Fuzhao Xue, Jinjie Ni, Yang You  

**Link**: [PDF](https://arxiv.org/pdf/2412.17365)  

**Abstract**: Datasets nowadays are generally constructed from multiple sources and using different synthetic techniques, making data de-noising and de-duplication crucial before being used for post-training. In this work, we propose to perform instruction tuning by iterative data selection (\ApproachName{}). We measure the quality of a sample from complexity and diversity simultaneously. Instead of calculating the complexity score once for all before fine-tuning, we highlight the importance of updating this model-specific score during fine-tuning to accurately accommodate the dynamic changes of the model. On the other hand, the diversity score is defined on top of the samples' responses under the consideration of their informativeness. IterIT integrates the strengths of both worlds by iteratively updating the complexity score for the top-ranked samples and greedily selecting the ones with the highest complexity-diversity score. Experiments on multiple instruction-tuning data demonstrate consistent improvements of IterIT over strong baselines. Moreover, our approach also generalizes well to domain-specific scenarios and different backbone models. All resources will be available at this https URL. 

**Abstract (ZH)**: 当今的数据集通常来自多个源并且使用不同的合成技术构建，因此，在用于后训练之前，需要进行数据去噪和去重。本文提出了一种通过迭代数据选择的方法进行指令调优（\ApproachName{}）。我们同时从复杂性和多样性两个方面衡量样本的质量。不同于在微调之前一次性计算复杂性分数，我们强调在微调过程中更新这个模型特定的分数的重要性，以便准确地适应模型的动态变化。另一方面，多样性分数基于样本响应在其信息性方面的定义。IterIT 通过迭代更新排名靠前样本的复杂性分数，并贪婪地选择复杂性多样性分数最高的样本，从而集两者的优点于一体。在多种指令调优数据集上的实验结果表明，IterIT 在多个强基线模型上表现出持续的改进。此外，我们的方法在特定领域场景和不同的主干模型上也具有良好的泛化能力。所有资源将在以下网址公开：这个 https URL。 

---
# Efficient fine-tuning methodology of text embedding models for information retrieval: contrastive learning penalty (clp) 

**Title (ZH)**: 适用于信息检索的文字嵌入模型高效微调方法：对比学习惩罚（CLP） 

**Authors**: Jeongsu Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17364)  

**Abstract**: Text embedding models play a crucial role in natural language processing, particularly in information retrieval, and their importance is further highlighted with the recent utilization of RAG (Retrieval- Augmented Generation). This study presents an efficient fine-tuning methodology encompassing data selection, loss function, and model architecture to enhance the information retrieval performance of pre-trained text embedding models. In particular, this study proposes a novel Contrastive Learning Penalty function that overcomes the limitations of existing Contrastive Learning. The proposed methodology achieves significant performance improvements over existing methods in document retrieval tasks. This study is expected to contribute to improving the performance of information retrieval systems through fine-tuning of text embedding models. The code for this study can be found at this https URL, and the best-performing model can be found at this https URL. 

**Abstract (ZH)**: 文本嵌入模型在自然语言处理中扮演着至关重要的角色，特别是在信息检索方面，其重要性在近期利用RAG（检索增强生成）技术后更加凸显。本研究提出了一种高效的微调方法，涵盖了数据选择、损失函数和模型架构等方面，以提升预训练文本嵌入模型的信息检索性能。特别是，本研究提出了一种新颖的对比学习惩罚函数，克服了现有对比学习方法的局限性。所提出的方法在文档检索任务中实现了显著的性能提升。本研究预计能够通过对文本嵌入模型的微调来提高信息检索系统的性能。相关代码可以在此处找到：[此链接]，而表现最佳的模型则可以在此处找到：[此链接]。 

---
# FFA Sora, video generation as fundus fluorescein angiography simulator 

**Title (ZH)**: FFA Sora，视网膜荧光血管造影效果的视频生成模拟器 

**Authors**: Xinyuan Wu, Lili Wang, Ruoyu Chen, Bowen Liu, Weiyi Zhang, Xi Yang, Yifan Feng, Mingguang He, Danli Shi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17346)  

**Abstract**: Fundus fluorescein angiography (FFA) is critical for diagnosing retinal vascular diseases, but beginners often struggle with image interpretation. This study develops FFA Sora, a text-to-video model that converts FFA reports into dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora accurately simulates disease features from the input text, as confirmed by objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the generated videos and textual prompts, with BERTScore of 0.35. Additionally, the model demonstrated strong privacy-preserving performance in retrieval evaluations, achieving an average Recall@K of 0.073. Human assessments indicated satisfactory visual quality, with an average score of 1.570(scale: 1 = best, 5 = worst). This model addresses privacy concerns associated with sharing large-scale FFA data and enhances medical education. 

**Abstract (ZH)**: 视网膜血管造影（FFA）是诊断视网膜血管疾病的关键手段，但初学者在图像解释方面常常感到困难。本研究开发了FFA Sora，这是一种文本到视频的模型，通过小波-流变分自编码器（WF-VAE）和扩散转换器（DiT）将FFA报告转换成动态视频。该模型在匿名数据集上进行了训练，并准确地从输入文本中模拟了疾病特征，经客观指标验证：Frechet 视频距离（FVD）= 329.78，学习感知图像块相似度（LPIPS）= 0.48，视觉问答分数（VQAScore）= 0.61。具体评估显示，生成的视频与文本提示之间的对齐情况令人满意，BERTScore为0.35。此外，该模型在检索评估中表现出强大的隐私保护性能，平均Recall@K为0.073。人类评估显示，视频的视觉质量令人满意，平均得分为1.570（1=最好，5=最差）。该模型解决了分享大规模FFA数据相关的隐私问题，并增强了医学教育。 

---
# APEX$^2$: Adaptive and Extreme Summarization for Personalized Knowledge Graphs 

**Title (ZH)**: APEX$^2$：自适应和极端个人化的知识图谱总结方法 

**Authors**: Zihao Li, Dongqi Fu, Mengting Ai, Jingrui He  

**Link**: [PDF](https://arxiv.org/pdf/2412.17336)  

**Abstract**: Knowledge graphs (KGs), which store an extensive number of relational facts, serve various applications. Recently, personalized knowledge graphs (PKGs) have emerged as a solution to optimize storage costs by customizing their content to align with users' specific interests within particular domains. In the real world, on one hand, user queries and their underlying interests are inherently evolving, requiring PKGs to adapt continuously; on the other hand, the summarization is constantly expected to be as small as possible in terms of storage cost. However, the existing PKG summarization methods implicitly assume that the user's interests are constant and do not shift. Furthermore, when the size constraint of PKG is extremely small, the existing methods cannot distinguish which facts are more of immediate interest and guarantee the utility of the summarized PKG. To address these limitations, we propose APEX$^2$, a highly scalable PKG summarization framework designed with robust theoretical guarantees to excel in adaptive summarization tasks with extremely small size constraints. To be specific, after constructing an initial PKG, APEX$^2$ continuously tracks the interest shift and adjusts the previous summary. We evaluate APEX$^2$ under an evolving query setting on benchmark KGs containing up to 12 million triples, summarizing with compression ratios $\leq 0.1\%$. The experiments show that APEX outperforms state-of-the-art baselines in terms of both query-answering accuracy and efficiency. 

**Abstract (ZH)**: 知识图谱（KGs）存储了大量的关系事实，应用于多种场景。近期，个性化知识图谱（PKG）作为一种解决方案出现，通过定制内容来与特定领域用户的特定兴趣相匹配，从而优化存储成本。在现实世界中，一方面，用户查询及其潜在的兴趣是固有的动态变化的，这要求PKG能够持续适应；另一方面，总结的内容在存储成本方面需要尽可能小化。然而，现有的PKG总结方法隐含地假定用户兴趣是不变的，不会发生变化。此外，当PKG的大小约束非常小时，现有的方法无法区分哪些事实是当务之急，并保证总结的PKG具有实用性。为了解决这些限制，我们提出了APEX$^2$，这是一种高度可扩展的PKG总结框架，旨在在极小大小约束条件下执行适应性总结任务，并具有稳健的理论保障。具体而言，在构建初始PKG后，APEX$^2$会持续跟踪兴趣转移并调整之前的总结。我们在包含高达1200万三元组的基准KGs中，以压缩比$\leq 0.1\%$的情况下进行动态查询设置下的评估。实验结果显示，APEX在查询回答的准确性和效率方面均优于最先进的基线方法。 

---
# Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition 

**Title (ZH)**: 基于最小条件的扩散模型广域地面运动合成 

**Authors**: Jaeheun Jung, Jaehyuk Lee, Chang-Hae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2412.17333)  

**Abstract**: Earthquakes are rare. Hence there is a fundamental call for reliable methods to generate realistic ground motion data for data-driven approaches in seismology. Recent GAN-based methods fall short of the call, as the methods either require special information such as geological traits or generate subpar waveforms that fail to satisfy seismological constraints such as phase arrival times. We propose a specialized Latent Diffusion Model (LDM) that reliably generates realistic waveforms after learning from real earthquake data with minimal conditions: location and magnitude. We also design a domain-specific training method that exploits the traits of earthquake dataset: multiple observed waveforms time-aligned and paired to each earthquake source that are tagged with seismological metadata comprised of earthquake magnitude, depth of focus, and the locations of epicenter and seismometers. We construct the time-aligned earthquake dataset using Southern California Earthquake Data Center (SCEDC) API, and train our model with the dataset and our proposed training method for performance evaluation. Our model surpasses all comparable data-driven methods in various test criteria not only from waveform generation domain but also from seismology such as phase arrival time, GMPE analysis, and spectrum analysis. Our result opens new future research directions for deep learning applications in seismology. 

**Abstract (ZH)**: 地震的发生率很低。因此，地震学中基于数据驱动的方法需要可靠的生成真实地面运动数据的方法。近期的基于生成对抗网络（GAN）的方法未能满足这一需求，因为这些方法要么需要特殊信息（如地质特征），要么生成质量不佳的波形，无法满足来自地震学的相位到达时间等约束。我们提出了一种特化的潜扩散模型（LDM），仅通过学习实际地震数据，并以位置和震级为最少先验条件，就可以可靠地生成真实波形。此外，我们还设计了一种特定领域的训练方法，利用地震数据集的特性：多个对齐的观测波形与每个地震源配对，并附有包含地震震级、震源深度、以及震中和地震仪位置等地震学元数据。我们使用南加州地震数据中心（SCEDC）API构建对齐的地震数据集，并利用该数据集和我们提出的训练方法对模型进行训练以评估其性能。我们的模型在多个测试标准下均超越了所有可比的数据驱动方法，不仅在波形生成领域，还在地震学方面，如相位到达时间、GMPE分析和频谱分析等。我们的结果开辟了地震学中深度学习应用的新研究方向。 

---
# EcoSearch: A Constant-Delay Best-First Search Algorithm for Program Synthesis 

**Title (ZH)**: EcoSearch：一种程序合成中的恒定延迟最佳优先搜索算法 

**Authors**: Théo Matricon, Nathanaël Fijalkow, Guillaume Lagarde  

**Link**: [PDF](https://arxiv.org/pdf/2412.17330)  

**Abstract**: Many approaches to program synthesis perform a combinatorial search within a large space of programs to find one that satisfies a given specification. To tame the search space blowup, previous works introduced probabilistic and neural approaches to guide this combinatorial search by inducing heuristic cost functions. Best-first search algorithms ensure to search in the exact order induced by the cost function, significantly reducing the portion of the program space to be explored. We present a new best-first search algorithm called EcoSearch, which is the first constant-delay algorithm for pre-generation cost function: the amount of compute required between outputting two programs is constant, and in particular does not increase over time. This key property yields important speedups: we observe that EcoSearch outperforms its predecessors on two classic domains. 

**Abstract (ZH)**: 许多程序合成方法在程序的大型空间中进行组合搜索，以找到一个满足给定规范的程序。为了遏制搜索空间的膨胀，前人工作引入了概率和神经方法，通过诱导启发式代价函数来指导这种组合搜索。优先搜索算法确保按照代价函数诱导的顺序进行搜索，大幅减少了需要探索的程序空间的比例。我们提出了一种新的优先搜索算法EcoSearch，它是第一个常延迟算法：在输出两个程序之间所需计算量是常数的，并且特别地，这种计算量不会随时间增加。这一关键性质带来了重要的加速效果：我们在两个经典领域观察到EcoSearch比其前驱方法具有更好的性能。 

---
# xPatch: Dual-Stream Time Series Forecasting with Exponential Seasonal-Trend Decomposition 

**Title (ZH)**: xPatch：基于指数季节性和趋势分解的双流时间序列预测 

**Authors**: Artyom Stitsyuk, Jaesik Choi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17323)  

**Abstract**: In recent years, the application of transformer-based models in time-series forecasting has received significant attention. While often demonstrating promising results, the transformer architecture encounters challenges in fully exploiting the temporal relations within time series data due to its attention mechanism. In this work, we design eXponential Patch (xPatch for short), a novel dual-stream architecture that utilizes exponential decomposition. Inspired by the classical exponential smoothing approaches, xPatch introduces the innovative seasonal-trend exponential decomposition module. Additionally, we propose a dual-flow architecture that consists of an MLP-based linear stream and a CNN-based non-linear stream. This model investigates the benefits of employing patching and channel-independence techniques within a non-transformer model. Finally, we develop a robust arctangent loss function and a sigmoid learning rate adjustment scheme, which prevent overfitting and boost forecasting performance. The code is available at the following repository: this https URL. 

**Abstract (ZH)**: 近年来，基于变压器模型的时间序列预测应用逐渐受到广泛关注。尽管这些模型常常能够取得令人鼓舞的结果，但其注意力机制使其在充分挖掘时间序列数据中的时间关系时仍然面临挑战。在此研究中，我们设计了指数块（xPatch，简称），这是一种新颖的双流架构，利用了指数分解的方法。受到经典指数平滑方法的启发，xPatch引入了创新的季节-趋势指数分解模块。此外，我们提出了一种双流架构，该架构包括基于MLP的线性流和基于CNN的非线性流。该模型探讨了在非变压器模型中应用分块技术和通道独立性技术的好处。最后，我们开发了一种稳健的正切衰减损失函数和一种Sigmoid学习率调整方案，以防止过拟合并提升预测性能。相关代码可在此仓库中找到：this https URL。 

---
# Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance 

**Title (ZH)**: 通过基于压缩的距离评估生成文本的人工编辑努力程度 

**Authors**: Nicolas Devatine, Louis Abraham  

**Link**: [PDF](https://arxiv.org/pdf/2412.17321)  

**Abstract**: Assessing the extent of human edits on texts generated by Large Language Models (LLMs) is crucial to understanding the human-AI interactions and improving the quality of automated text generation systems. Existing edit distance metrics, such as Levenshtein, BLEU, ROUGE, and TER, often fail to accurately measure the effort required for post-editing, especially when edits involve substantial modifications, such as block operations. In this paper, we introduce a novel compression-based edit distance metric grounded in the Lempel-Ziv-77 algorithm, designed to quantify the amount of post-editing applied to LLM-generated texts. Our method leverages the properties of text compression to measure the informational difference between the original and edited texts. Through experiments on real-world human edits datasets, we demonstrate that our proposed metric is highly correlated with actual edit time and effort. We also show that LLMs exhibit an implicit understanding of editing speed, that aligns well with our metric. Furthermore, we compare our metric with existing ones, highlighting its advantages in capturing complex edits with linear computational efficiency. Our code and data are available at: this https URL 

**Abstract (ZH)**: 评估大型语言模型（LLMs）生成文本中人类编辑的程度对于理解人机交互和提高自动文本生成系统的质量至关重要。现有的编辑距离度量，如Levenshtein、BLEU、ROUGE和TER，往往无法准确测量后编辑所需要的努力，尤其是在涉及大量修改（如块操作）的情况下。在本文中，我们引入了一种基于Lempel-Ziv-77算法的新颖压缩度量编辑距离度量，旨在量化应用于LLM生成文本的后编辑量。我们的方法利用文本压缩的属性，衡量原始文本与编辑后文本之间的信息差异。通过在真实世界的人类编辑数据集上的实验，我们证明了我们提出的方法与实际的编辑时间和努力高度相关。我们还展示了LLMs对编辑速度存在隐含的理解，这与我们的度量方法一致。此外，我们还将我们的度量方法与现有方法进行了比较，突显了其在捕捉复杂编辑时的线性计算效率优势。我们的代码和数据可在以下链接获取：this https URL 

---
# Fast Gradient Computation for RoPE Attention in Almost Linear Time 

**Title (ZH)**: 几乎线性时间的RoPE注意力梯度快速计算方法 

**Authors**: Yifang Chen, Jiayan Huo, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song  

**Link**: [PDF](https://arxiv.org/pdf/2412.17316)  

**Abstract**: The Rotary Position Embedding (RoPE) mechanism has become a powerful enhancement to the Transformer architecture, which enables models to capture token relationships when encoding positional information. However, the RoPE mechanisms make the computations of attention mechanisms more complicated, which makes efficient algorithms challenging. Earlier research introduced almost linear time, i.e., $n^{1+o(1)}$ where $n$ is the number of input tokens, algorithms for the forward computation under specific parameter settings. However, achieving a subquadratic time algorithm for other parameter regimes remains impossible unless the widely accepted Strong Exponential Time Hypothesis (SETH) is disproven. In this work, we develop the first almost linear time algorithm for backward computations in the RoPE-based attention under bounded entries. Our approach builds on recent advancements in fast RoPE attention computations, utilizing a novel combination of the polynomial method and the Fast Fourier Transform. Furthermore, we show that with lower bounds derived from the SETH, the bounded entry condition is necessary for subquadratic performance. 

**Abstract (ZH)**: 旋转位置嵌入（RoPE）机制已成为Transformer架构的强大增强工具，使模型能够在编码位置信息时捕捉词元关系。然而，RoPE机制使注意力机制的计算变得更加复杂，从而增加了高效算法的难度。此前的研究在特定参数设置下引入了几乎线性时间算法，即在输入词元数量为$n$的情况下，计算时间为$n^{1+o(1)}$。然而，在其他参数范围内实现亚二次时间算法除非广泛接受的强指数时间假设（SETH）被推翻，否则仍是不可能的。在这项工作中，我们开发了第一个在RoPE为基础的注意力下，具有界数值的反向计算的几乎线性时间算法。我们的方法充分利用了最近快速RoPE注意力计算的进展，结合了多项式方法和快速傅里叶变换的新颖组合。此外，我们证明了从SETH推导出的下界表明，为了实现亚二次性能，界数值条件是必需的。 

---
# CodeV: Issue Resolving with Visual Data 

**Title (ZH)**: CodeV：基于视觉数据的問題解決方法 

**Authors**: Linhao Zhang, Daoguang Zan, Quanshun Yang, Zhirong Huang, Dong Chen, Bo Shen, Tianyu Liu, Yongshun Gong, Pengjie Huang, Xudong Lu, Guangtai Liang, Lizhen Cui, Qianxiang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17315)  

**Abstract**: Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CodeV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CodeV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CodeV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）取得了迅速的发展，其在软件工程中的应用逐渐扩展到更复杂的仓库级任务。GitHub问题解决是这些任务中的一个关键挑战。虽然近年来的研究在这一任务上取得了一定进展，但这些方法主要关注问题中的文本数据，忽视了视觉数据。然而，视觉数据对于解决这些问题至关重要，因为它能够传达仅凭文本无法提供的额外知识。我们提出了CodeV，这是首个利用视觉数据增强LLMs问题解决能力的方法。CodeV通过两阶段过程来解决每个问题：数据处理和补丁生成。为了评估CodeV，我们构建了一个视觉问题解决基准，即Visual SWE-bench。通过广泛的实验，我们展示了CodeV的有效性，并提供了关于如何利用视觉数据解决GitHub问题的重要见解。 

---
# Popularity Estimation and New Bundle Generation using Content and Context based Embeddings 

**Title (ZH)**: 基于内容和上下文嵌入的流行度估计与新型捆绑生成 

**Authors**: Ashutosh Nayak, Prajwal NJ, Sameeksha Keshav, Kavitha S.N., Roja Reddy, Rajasekhara Reddy Duvvuru Muni  

**Link**: [PDF](https://arxiv.org/pdf/2412.17310)  

**Abstract**: Recommender systems create enormous value for businesses and their consumers. They increase revenue for businesses while improving the consumer experience by recommending relevant products amidst huge product base. Product bundling is an exciting development in the field of product recommendations. It aims at generating new bundles and recommending exciting and relevant bundles to their consumers. Unlike traditional recommender systems that recommend single items to consumers, product bundling aims at targeting a bundle, or a set of items, to the consumers. While bundle recommendation has attracted significant research interest recently, extant literature on bundle generation is scarce. Moreover, metrics to identify if a bundle is popular or not is not well studied. In this work, we aim to fulfill this gap by introducing new bundle popularity metrics based on sales, consumer experience and item diversity in a bundle. We use these metrics in the methodology proposed in this paper to generate new bundles for mobile games using content aware and context aware embeddings. We use opensource Steam Games dataset for our analysis. Our experiments indicate that we can generate new bundles that can outperform the existing bundles on the popularity metrics by 32% - 44%. Our experiments are computationally efficient and the proposed methodology is generic that can be extended to other bundling problems e.g. product bundling, music bundling. 

**Abstract (ZH)**: 推荐系统为企业和消费者创造了巨大的价值。它们通过在海量产品库中推荐相关产品，增加企业的收入，同时改善消费者的体验。产品捆绑是产品推荐领域的一个令人激动的发展。其目标是在生成新的组合包的同时，向消费者推荐新的、相关的产品捆绑包。与传统的推荐系统推荐单一商品不同，产品捆绑试图将一个组合包或商品集推荐给消费者。尽管捆绑推荐最近吸引了大量研究兴趣，但关于捆绑生成的文献却相当稀缺。此外，关于如何确定一个捆绑包是否受欢迎的研究也不够深入。在本文中，我们旨在通过引入基于销售、消费者体验和捆绑包内商品多样性的新捆绑受欢迎度指标来填补这一空白。我们使用这些指标来生成新的移动游戏捆绑包，采用此方法中的内容感知和上下文感知嵌入方法。我们使用开源的Steam游戏数据集进行分析。实验结果表明，我们可以通过这些方法生成在受欢迎度指标上比现有捆绑包高出32%-44%的新捆绑包。我们的实验计算效率高，所提出的方法通用，可以扩展到其他捆绑问题，例如产品捆绑和音乐捆绑。 

---
# Dynamic Scheduling Strategies for Resource Optimization in Computing Environments 

**Title (ZH)**: 计算环境中的资源优化动态调度策略 

**Authors**: Xiaoye Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17301)  

**Abstract**: The rapid development of cloud-native architecture has promoted the widespread application of container technology, but the optimization problems in container scheduling and resource management still face many challenges. This paper proposes a container scheduling method based on multi-objective optimization, which aims to balance key performance indicators such as resource utilization, load balancing and task completion efficiency. By introducing optimization models and heuristic algorithms, the scheduling strategy is comprehensively improved, and experimental verification is carried out using the real Google Cluster Data dataset. The experimental results show that compared with traditional static rule algorithms and heuristic algorithms, the optimized scheduling scheme shows significant advantages in resource utilization, load balancing and burst task completion efficiency. This shows that the proposed method can effectively improve resource management efficiency and ensure service quality and system stability in complex dynamic cloud environments. At the same time, this paper also explores the future development direction of scheduling algorithms in multi-tenant environments, heterogeneous cloud computing, and cross-edge and cloud collaborative computing scenarios, and proposes research prospects for energy consumption optimization, adaptive scheduling and fairness. The research results not only provide a theoretical basis and practical reference for container scheduling under cloud-native architecture, but also lay a foundation for further realizing intelligent and efficient resource management. 

**Abstract (ZH)**: 随着云原生架构的迅速发展，容器技术得到了广泛的应用，但容器调度和资源管理中的优化问题仍然面临着许多挑战。本文提出了一种基于多目标优化的容器调度方法，旨在平衡资源利用率、负载均衡和任务完成效率等关键性能指标。通过引入优化模型和启发式算法，全面改进了调度策略，并使用真实的Google集群数据集进行了实验验证。实验结果表明，与传统的静态规则算法和启发式算法相比，优化后的调度方案在资源利用率、负载均衡和突发任务完成效率方面显示出明显的优势。这表明，所提出的方法能够有效提升复杂动态云环境中的资源管理效率，并确保服务质量与系统稳定性。同时，本文还探讨了多租户环境、异构云计算以及跨边缘和云计算协同计算场景下调度算法的未来发展方向，并提出了能效优化、自适应调度和公平性等方面的课题展望。研究成果不仅为云原生架构下的容器调度提供了理论依据和实践参考，也为进一步实现智能化和高效的资源管理奠定了基础。 

---
# AV-EmoDialog: Chat with Audio-Visual Users Leveraging Emotional Cues 

**Title (ZH)**: AV-EmoDialog：利用情感线索与多媒体用户对话 

**Authors**: Se Jin Park, Yeonju Kim, Hyeongseop Rha, Bella Godiva, Yong Man Ro  

**Link**: [PDF](https://arxiv.org/pdf/2412.17292)  

**Abstract**: In human communication, both verbal and non-verbal cues play a crucial role in conveying emotions, intentions, and meaning beyond words alone. These non-linguistic information, such as facial expressions, eye contact, voice tone, and pitch, are fundamental elements of effective interactions, enriching conversations by adding emotional and contextual depth. Recognizing the importance of non-linguistic content in communication, we present AV-EmoDialog, a dialogue system designed to exploit verbal and non-verbal information from users' audio-visual inputs to generate more responsive and empathetic interactions. AV-EmoDialog systematically exploits the emotional cues in audio-visual dialogues; extracting speech content and emotional tones from speech, analyzing fine-grained facial expressions from visuals, and integrating these cues to generate emotionally aware responses in an end-to-end manner. Through extensive experiments, we validate that the proposed AV-EmoDialog outperforms existing multimodal LLMs in generating not only emotionally appropriate but also contextually appropriate responses. 

**Abstract (ZH)**: 在人类沟通中，言语和非言语线索在传达情感、意图和超越言语本身的意义方面发挥着关键作用。这些非语言信息，如面部表情、眼神交流、语音语调和音高，是有效互动的基本要素，通过增加情感和上下文的深度来丰富对话。鉴于非语言内容在沟通中的重要性，我们提出了一种名为AV-EmoDialog的对话系统，该系统旨在利用用户的音视频输入中的言语和非言语信息，以生成更为响应性和共鸣性的互动。AV-EmoDialog系统性地挖掘了音视频对话中的情感线索；从语音中提取语音内容和情感语调，从视觉中分析细微的情感表情，并将这些线索综合起来，以端到端的方式生成具有情感意识的回应。通过大量的实验，我们验证了提出的研究结果，表明AV-EmoDialog在生成不仅情感恰当而且上下文恰当的回应方面优于现有的多模态语言模型。 

---
# Multi-Modal Grounded Planning and Efficient Replanning For Learning Embodied Agents with A Few Examples 

**Title (ZH)**: 基于多模态地面规划和高效重规划的少数示例学习体现代理的研究 

**Authors**: Taewoong Kim, Byeonghwi Kim, Jonghyun Choi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17288)  

**Abstract**: Learning a perception and reasoning module for robotic assistants to plan steps to perform complex tasks based on natural language instructions often requires large free-form language annotations, especially for short high-level instructions. To reduce the cost of annotation, large language models (LLMs) are used as a planner with few data. However, when elaborating the steps, even the state-of-the-art planner that uses LLMs mostly relies on linguistic common sense, often neglecting the status of the environment at command reception, resulting in inappropriate plans. To generate plans grounded in the environment, we propose FLARE (Few-shot Language with environmental Adaptive Replanning Embodied agent), which improves task planning using both language command and environmental perception. As language instructions often contain ambiguities or incorrect expressions, we additionally propose to correct the mistakes using visual cues from the agent. The proposed scheme allows us to use a few language pairs thanks to the visual cues and outperforms state-of-the-art approaches. Our code is available at this https URL. 

**Abstract (ZH)**: 基于自然语言指令规划机器人助手进行复杂任务的感知与推理模块的学习通常需要大量的自由格式语言注解，尤其是对于简短的高层指令而言。为了降低标注成本，采用大语言模型（LLMs）作为少量数据规划器。然而，在详细规划步骤时，即使是最先进的使用LLMs的规划器，大多数情况下依然依赖于语言常识，常常忽略了指令接收时的环境状态，导致计划不当。为了生成基于环境的计划，我们提出了一种新的方法，即FLARE（Few-shot Language with Environmental Adaptive Replanning Embodied Agent），该方法结合语言指令和环境感知来改进任务规划。由于语言指令中常包含歧义或错误表述，我们进一步提出利用代理器的视觉线索来纠正这些错误。所提出的方案利用视觉线索仅使用少量的语言指令对就可以实现优于现有最先进的方法的效果。我们的代码可以在以下网址获得：this https URL。 

---
# Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning 

**Title (ZH)**: 通过对比渐进学习实现建筑能源预测的时间序列基础模型 

**Authors**: Rui Liang, Yang Deng, Donghua Xie, Fang He, Dan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17285)  

**Abstract**: Advances in time-series forecasting are driving a shift from conventional machine learning models to foundation models (FMs) that are trained with generalized knowledge. However, existing FMs still perform poorly in the energy fields, such as building energy forecasting (BEF). This paper studies the adaptation of FM to BEF tasks. We demonstrate the shortcomings of fine-tuning FM straightforwardly from both the perspectives of FM and the data. To overcome these limitations, we propose a new \textit{contrastive curriculum learning}-based training method. Our method optimizes the ordering of training data in the context of TSFM adaptation. Experiments show that our method can improve the zero/few-shot performance by 14.6\% compared to the existing FMs. Our code and new TSFM will be available at <Anonymous Github Repo>. 

**Abstract (ZH)**: 时间序列预测的进步正推动从传统的机器学习模型向通过泛化知识训练的基础模型（FMs）转变。然而，现有的FMs在能源领域，如建筑能源预测（BEF）方面表现仍然不佳。本文研究了FMs在BEF任务中的适应性。我们从基础模型和数据两个方面分析了直接微调FMs的不足之处。为了克服这些局限性，我们提出了一种基于对比式递进学习的新训练方法。该方法在时间序列基础模型（TSFM）适应过程中优化了训练数据的顺序。实验结果表明，与现有的FMs相比，我们的方法可以提高零样本/少样本性能14.6%。我们的代码和新的TSFM将在<匿名GitHub仓库>上提供。 

---
# Evaluating the Design Features of an Intelligent Tutoring System for Advanced Mathematics Learning 

**Title (ZH)**: 评估智能化辅导系统在高级数学学习中的设计特点 

**Authors**: Ying Fang, Bo He, Zhi Liu, Sannyuya Liu, Zhonghua Yan, Jianwen Sun  

**Link**: [PDF](https://arxiv.org/pdf/2412.17265)  

**Abstract**: Xiaomai is an intelligent tutoring system (ITS) designed to help Chinese college students in learning advanced mathematics and preparing for the graduate school math entrance exam. This study investigates two distinctive features within Xiaomai: the incorporation of free-response questions with automatic feedback and the metacognitive element of reflecting on self-made errors. 

**Abstract (ZH)**: Xiaomai 是一款智能辅导系统（ITS），旨在帮助中国大学生学习高等数学并为研究生入学数学考试做准备。本研究探讨了 Xiaomai 的两个独特特点：结合自动反馈的开放式问题以及反思自我错误的元认知要素。 

---
# Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach 

**Title (ZH)**: 通过emoji解读实现跨语言情感分析：一种多模态生成AI方法 

**Authors**: Rafid Ishrak Jahan, Heng Fan, Haihua Chen, Yunhe Feng  

**Link**: [PDF](https://arxiv.org/pdf/2412.17255)  

**Abstract**: Emojis have become ubiquitous in online communication, serving as a universal medium to convey emotions and decorative elements. Their widespread use transcends language and cultural barriers, enhancing understanding and fostering more inclusive interactions. While existing work gained valuable insight into emojis understanding, exploring emojis' capability to serve as a universal sentiment indicator leveraging large language models (LLMs) has not been thoroughly examined. Our study aims to investigate the capacity of emojis to serve as reliable sentiment markers through LLMs across languages and cultures. We leveraged the multimodal capabilities of ChatGPT to explore the sentiments of various representations of emojis and evaluated how well emoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset collected from 32 countries. Our analysis reveals that the accuracy of LLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant potential to serve as a universal sentiment marker. We also found a consistent trend that the accuracy of sentiment conveyed by emojis increased as the number of emojis grew in text. The results reinforce the potential of emojis to serve as global sentiment indicators, offering insight into fields such as cross-lingual and cross-cultural sentiment analysis on social media platforms. Code: this https URL. 

**Abstract (ZH)**: 表情符号已成为在线交流中不可或缺的部分，作为表达情感和装饰元素的通用媒介。它们的广泛应用跨越了语言和文化的障碍，增强了理解和促进了更具包容性的交流。尽管现有研究从多个角度对表情符号的理解提供了宝贵的见解，但利用大型语言模型（LLMs）探索表情符号作为通用情感指标的能力还未得到充分的研究。本研究旨在通过LLMs跨语言和跨文化探索表情符号作为可靠情感标记的能力。我们利用ChatGPT的多模态能力，探讨了各种表情符号表现形式的情感，并评估了emoji传达的情感与多语言数据集中32个国家收集的文本情感的一致性。我们的分析表明，基于LLM的情感标注准确率为81.43%，突显了表情符号作为通用情感指标的巨大潜力。我们还发现了一个一致的趋势，即文本中表情符号的数量增加时，情感标注的准确性也随之提高。研究结果强化了表情符号作为全球情感指标的潜力，为社交媒体平台上的跨语言和跨文化情感分析提供了见解。代码：[这个链接](this https URL)。 

---
# Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory 

**Title (ZH)**: 增强多模态长视频生成一致性无需调整：时间-频率分析、提示对齐与理论 

**Authors**: Xingyao Li, Fengzhuo Zhang, Jiachun Pan, Yunlong Hou, Vincent Y. F. Tan, Zhuoran Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17254)  

**Abstract**: Despite the considerable progress achieved in the long video generation problem, there is still significant room to improve the consistency of the videos, particularly in terms of smoothness and transitions between scenes. We address these issues to enhance the consistency and coherence of videos generated with either single or multiple prompts. We propose the Time-frequency based temporal Attention Reweighting Algorithm (TiARA), which meticulously edits the attention score matrix based on the Discrete Short-Time Fourier Transform. Our method is supported by a theoretical guarantee, the first-of-its-kind for frequency-based methods in diffusion models. For videos generated by multiple prompts, we further investigate key factors affecting prompt interpolation quality and propose PromptBlend, an advanced prompt interpolation pipeline. The efficacy of our proposed method is validated via extensive experimental results, exhibiting consistent and impressive improvements over baseline methods. The code will be released upon acceptance. 

**Abstract (ZH)**: 尽管在长视频生成方面取得了显著进展，但在视频一致性，尤其是流畅性和场景过渡方面仍然有很大的提升空间。我们致力于解决这些问题，以提高使用单一或多个提示生成的视频的一致性和连贯性。我们提出了一种基于时间频率的时序Attention加权算法（TiARA），该算法根据离散短时傅里叶变换（DSTFT）精心修饰了注意力得分矩阵。我们的方法具有理论上的保证，这是频率域方法在扩散模型中的首创。对于由多个提示生成的视频，我们进一步研究了影响提示插值质量的关键因素，并提出了一种高级提示插值流水线PromptBlend。通过广泛的实验结果验证了我们提出方法的有效性，显示了相对于基线方法的一致性和显著改进。在接受后将发布代码。 

---
# QTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image Segmentation 

**Title (ZH)**: QTSeg：一种基于查询词_token_的高效2D医学图像分割架构 

**Authors**: Phuong-Nam Tran, Nhat Truong Pham, Duc Ngoc Minh Dang, Eui-Nam Huh, Choong Seon Hong  

**Link**: [PDF](https://arxiv.org/pdf/2412.17241)  

**Abstract**: Medical image segmentation is crucial in assisting medical doctors in making diagnoses and enabling accurate automatic diagnosis. While advanced convolutional neural networks (CNNs) excel in segmenting regions of interest with pixel-level precision, they often struggle with long-range dependencies, which is crucial for enhancing model performance. Conversely, transformer architectures leverage attention mechanisms to excel in handling long-range dependencies. However, the computational complexity of transformers grows quadratically, posing resource-intensive challenges, especially with high-resolution medical images. Recent research aims to combine CNN and transformer architectures to mitigate their drawbacks and enhance performance while keeping resource demands low. Nevertheless, existing approaches have not fully leveraged the strengths of both architectures to achieve high accuracy with low computational requirements. To address this gap, we propose a novel architecture for 2D medical image segmentation (QTSeg) that leverages a feature pyramid network (FPN) as the image encoder, a multi-level feature fusion (MLFF) as the adaptive module between encoder and decoder and a multi-query mask decoder (MQM Decoder) as the mask decoder. In the first step, an FPN model extracts pyramid features from the input image. Next, MLFF is incorporated between the encoder and decoder to adapt features from different encoder stages to the decoder. Finally, an MQM Decoder is employed to improve mask generation by integrating query tokens with pyramid features at all stages of the mask decoder. Our experimental results show that QTSeg outperforms state-of-the-art methods across all metrics with lower computational demands than the baseline and the existing methods. Code is available at this https URL (v0.1.0) 

**Abstract (ZH)**: 医学图像分割对于帮助医生进行诊断以及实现精准的自动诊断至关重要。虽然先进的卷积神经网络（CNNs）在亚像素级别的目标分割方面表现出色，但它们在处理长距离依赖性方面常常遇到困难，而长距离依赖性对于提升模型性能至关重要。相比之下，变压器架构利用注意力机制在处理长距离依赖性方面表现出色。然而，变压器的计算复杂性呈二次增长，给资源管理带来了巨大的挑战，尤其是在处理高分辨率医学图像时。最近的研究旨在结合CNN和变压器架构，以减轻各自的缺点，同时保持较低的资源需求以提升性能。然而，现有方法尚未充分利用这两种架构的优势，在实现高精度的同时，降低计算要求方面仍有待改进。为解决这一问题，我们提出了一种用于二维医学图像分割的新型架构（QTSeg），该架构采用了特征金字塔网络（FPN）作为图像编码器，多级特征融合（MLFF）作为编码器与解码器之间的自适应模块，以及多查询掩码解码器（MQM Decoder）作为掩码解码器。首先，FPN模型从输入图像中提取多层次特征。接着，在编码器和解码器之间引入MLFF，以适应不同编码器阶段的特征到解码器。最后，使用MQM Decoder通过整合查询令牌和掩码解码器的所有阶段的多层次特征来改进掩码生成。我们的实验结果表明，QTSeg在所有评估指标上都优于现有的顶尖方法，且计算需求低于基线和现有方法。了解更多细节和访问代码，请参见此 <https://github.com/Qwen-Proxy/QTSeg> (v0.1.0)。 

---
# Rethinking Cancer Gene Identification through Graph Anomaly Analysis 

**Title (ZH)**: 通过图异常分析重新审视癌症基因识别 

**Authors**: Yilong Zang, Lingfei Ren, Yue Li, Zhikang Wang, David Antony Selby, Zheng Wang, Sebastian Josef Vollmer, Hongzhi Yin, Jiangning Song, Junhang Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17240)  

**Abstract**: Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN. 

**Abstract (ZH)**: 图神经网络（GNNs）在近期的研究中显示出了整合蛋白质-蛋白质相互作用（PPI）网络以识别癌症基因的潜力。然而，由于PPI网络中生物信息建模不足，对于图结构中复杂蛋白质相互作用模式的更忠实描述，尤其是针对癌症基因，仍存在很大的未探索空间。本研究向通过癌症基因引起的蛋白质相互作用生物异常与统计图形异常之间的桥梁建设迈出了创新性的一步。我们发现了一种癌症基因特有的图形异常，即权重异质性，其表现为图中癌症基因节点的边权重显著的方差更大。此外，从谱的角度来看，我们证明权重异质性会导致谱能量的“平坦化”，朝频谱的两端集中。基于这些发现，我们提出了一个层次视角图神经网络（HIerarchical-Perspective Graph Neural Network，HIPGNN），不仅从谱的角度确定了谱能量分布的变化，还从空间的角度感知了详细的蛋白质相互作用背景。我们在重处理后的STRINGdb和CPDB两个数据集上进行了广泛的实验，实验结果证明了HIPGNN的优势。 

---
# Q-LIME $\pi$: A Quantum-Inspired Extension to LIME 

**Title (ZH)**: Q-LIME $\pi$: 一种受量子计算启发的LIME扩展 

**Authors**: Nelson Colón Vargas  

**Link**: [PDF](https://arxiv.org/pdf/2412.17197)  

**Abstract**: Machine learning models offer powerful predictive capabilities but often lack transparency. Local Interpretable Model-agnostic Explanations (LIME) addresses this by perturbing features and measuring their impact on a model's output. In text-based tasks, LIME typically removes present words (bits set to 1) to identify high-impact tokens. We propose \textbf{Q-LIME $\pi$} (Quantum LIME $\pi$), a quantum-inspired extension of LIME that encodes a binary feature vector in a quantum state, leveraging superposition and interference to explore local neighborhoods more efficiently. Our method focuses on flipping bits from $1 \rightarrow 0$ to emulate LIME's ``removal'' strategy, and can be extended to $0 \rightarrow 1$ where adding features is relevant. Experiments on subsets of the IMDb dataset demonstrate that Q-LIME $\pi$ often achieves near-identical top-feature rankings compared to classical LIME while exhibiting lower runtime in small- to moderate-dimensional feature spaces. This quantum-classical hybrid approach thus provides a new pathway for interpretable AI, suggesting that, with further improvements in quantum hardware and methods, quantum parallelism may facilitate more efficient local explanations for high-dimensional data. 

**Abstract (ZH)**: 机器学习模型提供了强大的预测能力，但往往缺乏透明度。局部可解释的模型无关解释（LIME）通过扰动特征并测量这些扰动对模型输出的影响来解决这一问题。在基于文本的任务中，LIME 通常通过去除现有词（将位设置为0）来识别高影响的令牌。我们提出了一种量子启发的 LIME 扩展，称为 \textbf{Q-LIME $\pi$}（量子 LIME $\pi$），该方法将二进制特征向量编码为量子态，利用叠加态和干涉效应更有效地探索局部邻域。我们的方法专注于将位从1翻转为0以模拟LIME的“移除”策略，并且可以扩展到将位从0翻转为1的情况，这里添加特征更为相关。对IMDb数据集子集的实验表明，在小到中等维度的特征空间中，Q-LIME $\pi$ 经常能够实现与经典LIME几乎相同的顶级特征排名，同时运行时间更低。这种量子-经典混合方法因此提供了一条新的可解释人工智能路径，指出，随着量子硬件和方法的进一步改进，量子并行性可能有助于更高效地为高维度数据提供局部解释。 

---
# Hierarchically Gated Experts for Efficient Online Continual Learning 

**Title (ZH)**: 高效在线连续学习的分层门控专家模型 

**Authors**: Kevin Luong, Michael Thielscher  

**Link**: [PDF](https://arxiv.org/pdf/2412.17188)  

**Abstract**: Continual Learning models aim to learn a set of tasks under the constraint that the tasks arrive sequentially with no way to access data from previous tasks. The Online Continual Learning framework poses a further challenge where the tasks are unknown and instead the data arrives as a single stream. Building on existing work, we propose a method for identifying these underlying tasks: the Gated Experts (GE) algorithm, where a dynamically growing set of experts allows for new knowledge to be acquired without catastrophic forgetting. Furthermore, we extend GE to Hierarchically Gated Experts (HGE), a method which is able to efficiently select the best expert for each data sample by organising the experts into a hierarchical structure. On standard Continual Learning benchmarks, GE and HGE are able to achieve results comparable with current methods, with HGE doing so more efficiently. 

**Abstract (ZH)**: 持续学习模型旨在在一个任务按顺序出现且无法访问之前任务数据的约束下学习一系列任务。在线持续学习框架进一步提出了一个挑战，即任务是未知的，数据则以单一的数据流形式到来。基于现有的工作，我们提出了一种识别这些潜在任务的方法：门控专家（GE）算法。该算法通过动态扩展专家集合，使新知识的获取不再导致灾难性遗忘。此外，我们将GE扩展为分层门控专家（HGE）方法，该方法通过将专家组织成层次结构，能更有效地为每个数据样本选择最佳专家。在标准的持续学习基准测试中，GE和HGE均能取得与当前方法相当的结果，而HGE在这方面表现更为高效。 

---
# COVID-19 on YouTube: A Data-Driven Analysis of Sentiment, Toxicity, and Content Recommendations 

**Title (ZH)**: COVID-19在YouTube上的影响：基于数据的分析研究，包括情感、毒性及内容推荐 

**Authors**: Vanessa Su, Nirmalya Thakur  

**Link**: [PDF](https://arxiv.org/pdf/2412.17180)  

**Abstract**: This study presents a data-driven analysis of COVID-19 discourse on YouTube, examining the sentiment, toxicity, and thematic patterns of video content published between January 2023 and October 2024. The analysis involved applying advanced natural language processing (NLP) techniques: sentiment analysis with VADER, toxicity detection with Detoxify, and topic modeling using Latent Dirichlet Allocation (LDA). The sentiment analysis revealed that 49.32% of video descriptions were positive, 36.63% were neutral, and 14.05% were negative, indicating a generally informative and supportive tone in pandemic-related content. Toxicity analysis identified only 0.91% of content as toxic, suggesting minimal exposure to toxic content. Topic modeling revealed two main themes, with 66.74% of the videos covering general health information and pandemic-related impacts and 33.26% focused on news and real-time updates, highlighting the dual informational role of YouTube. A recommendation system was also developed using TF-IDF vectorization and cosine similarity, refined by sentiment, toxicity, and topic filters to ensure relevant and context-aligned video recommendations. This system achieved 69% aggregate coverage, with monthly coverage rates consistently above 85%, demonstrating robust performance and adaptability over time. Evaluation across recommendation sizes showed coverage reaching 69% for five video recommendations and 79% for ten video recommendations per video. In summary, this work presents a framework for understanding COVID-19 discourse on YouTube and a recommendation system that supports user engagement while promoting responsible and relevant content related to COVID-19. 

**Abstract (ZH)**: 本研究通过对2023年1月到2024年10月期间在YouTube上发布的视频内容进行数据驱动分析，探讨了新冠疫情期间的语义情感、毒性以及主题模式。分析过程中使用了高级自然语言处理（NLP）技术：情感分析采用VADER，毒性检测采用Detoxify，主题建模采用潜在狄利克雷分配（LDA）。情感分析结果显示，49.32%的视频描述为正面情绪，36.63%为中性情绪，14.05%为负面情绪，表明与疫情相关的视频内容总体上具有信息性和支持性。毒性分析发现只有0.91%的内容被标记为具有毒性，表明暴露于具有毒性的内容很少。主题建模揭示了两个主要主题，其中66.74%的视频内容涵盖了一般健康信息和疫情带来的影响，而33.26%的内容则关注新闻和实时更新，突显了YouTube的双重信息角色。还开发了一个推荐系统，使用TF-IDF向量化和余弦相似度，并通过情感、毒性以及主题过滤器进行细化，以确保推荐的视频既相关又符合上下文。该系统在综合覆盖率上达到了69%，并且每月的覆盖率均在85%以上，显示了长时间内的稳健性能和灵活性。不同推荐规模的评估表明，对于每个视频推荐五条和十条视频，覆盖率分别达到了69%和79%。综上所述，该研究提供了一种理解YouTube上新冠疫情期间话语的框架，并开发了一个支持用户参与，同时促进相关和负责任内容推荐的推荐系统。 

---
# A Multi-AI Agent System for Autonomous Optimization of Agentic AI Solutions via Iterative Refinement and LLM-Driven Feedback Loops 

**Title (ZH)**: 一种通过迭代优化和基于LLM的反馈循环自主优化代理AI解决方案的多AI代理系统 

**Authors**: Kamer Ali Yuksel, Hassan Sawaf  

**Link**: [PDF](https://arxiv.org/pdf/2412.17149)  

**Abstract**: Agentic AI systems use specialized agents to handle tasks within complex workflows, enabling automation and efficiency. However, optimizing these systems often requires labor-intensive, manual adjustments to refine roles, tasks, and interactions. This paper introduces a framework for autonomously optimizing Agentic AI solutions across industries, such as NLP-driven enterprise applications. The system employs agents for Refinement, Execution, Evaluation, Modification, and Documentation, leveraging iterative feedback loops powered by an LLM (Llama 3.2-3B). The framework achieves optimal performance without human input by autonomously generating and testing hypotheses to improve system configurations. This approach enhances scalability and adaptability, offering a robust solution for real-world applications in dynamic environments. Case studies across diverse domains illustrate the transformative impact of this framework, showcasing significant improvements in output quality, relevance, and actionability. All data for these case studies, including original and evolved agent codes, along with their outputs, are here: this https URL 

**Abstract (ZH)**: 自主优化代理AI系统在复杂工作流程中使用专门的代理来处理任务，从而实现自动化和提效。然而，优化这些系统通常需要劳动密集型的手动调整，以精炼角色、任务和互动。本文介绍了一种跨行业自主优化代理AI解决方案的框架，适用于以NLP驱动的企业应用等场景。该系统采用代理进行润色、执行、评估、修改和记录，借助由LLM（Llama 3.2-3B）驱动的迭代反馈循环来实现这一目标。该框架通过自主生成和测试假设来优化系统配置，从而在无需人类干预的情况下实现最佳性能。这种方法提高了可扩展性和适应性，为变化莫测的环境中的实际应用提供了稳健的解决方案。来自多个领域的案例研究展示了该框架的变革性影响，展示了在输出质量、相关性和可操作性方面的显著改进。这些案例研究的所有数据，包括原始和演变后的代理代码及其输出，均可在此处获取：this https URL 

---
# Grams: Gradient Descent with Adaptive Momentum Scaling 

**Title (ZH)**: Grams：自适应动量缩放的梯度下降方法 

**Authors**: Yang Cao, Xiaoyu Li, Zhao Song  

**Link**: [PDF](https://arxiv.org/pdf/2412.17107)  

**Abstract**: We introduce \textbf{Gr}adient Descent with \textbf{A}daptive \textbf{M}omentum \textbf{S}caling (\textbf{Grams}), a novel optimization algorithm that decouples the direction and magnitude of parameter updates in deep learning. Unlike traditional optimizers that directly integrate momentum into updates, Grams separates the update direction, derived from current gradients, from momentum, which is used solely for adaptive magnitude scaling. This approach enables Grams to achieve improved loss descent compared to state-of-the-art cautious and momentum-based optimizers. We establish a global convergence guarantee for Grams and validate its effectiveness through extensive empirical evaluations. The results demonstrate Grams' superior performance, including faster convergence and better generalization, compared to widely-used optimizers such as Adam, Lion, and their cautious variants. Our results highlight Grams' potential as a transformative approach for efficient optimization in large-scale machine learning. 

**Abstract (ZH)**: 我们将介绍一种新颖的优化算法——Gradient Descent with Adaptive Momentum Scaling（Grams），它可以在深度学习中将参数更新的方向和幅度解耦。与传统的优化器直接将动量整合到更新中不同，Grams 将更新方向（从当前梯度中导出）与仅用于自适应幅度缩放的动量分离开来。这种做法使得 Grams 在损失下降方面优于最新的谨慎性和动量基的优化器。我们为 Grams 建立了全局收敛性保证，并通过广泛的实验验证了其有效性。结果表明，与广泛使用的优化器（如 Adam 和 Lion 及其谨慎性变体）相比，Grams 在收敛速度和泛化能力方面具有优越性。我们的结果突显了 Grams 在大规模机器学习中高效优化方面的潜力。 

---
# Analysis on LLMs Performance for Code Summarization 

**Title (ZH)**: 对代码摘要性能的大型语言模型分析 

**Authors**: Md. Ahnaf Akib, Md. Muktadir Mazumder, Salman Ahsan  

**Link**: [PDF](https://arxiv.org/pdf/2412.17094)  

**Abstract**: Code summarization aims to generate concise natural language descriptions for source code. Deep learning has been used more and more recently in software engineering, particularly for tasks like code creation and summarization. Specifically, it appears that the most current Large Language Models with coding perform well on these tasks. Large Language Models (LLMs) have significantly advanced the field of code summarization, providing sophisticated methods for generating concise and accurate summaries of source code. This study aims to perform a comparative analysis of several open-source LLMs, namely LLaMA-3, Phi-3, Mistral, and Gemma. These models' performance is assessed using important metrics such as BLEU\textsubscript{3.1} and ROUGE\textsubscript{3.2}.
Through this analysis, we seek to identify the strengths and weaknesses of each model, offering insights into their applicability and effectiveness in code summarization tasks. Our findings contribute to the ongoing development and refinement of LLMs, supporting their integration into tools that enhance software development and maintenance processes. 

**Abstract (ZH)**: 代码总结旨在为源代码生成简洁的自然语言描述。近年来，深度学习在软件工程中得到广泛应用，特别是在代码创建和总结等任务中。具体而言，当前表现良好的大型语言模型（特别是那些具有编程能力的模型）在这些任务上表现出色。大型语言模型（LLMs）显著推动了代码总结领域的进展，提供了生成简洁且准确的源代码摘要的高级方法。本研究旨在对比分析几个开源的大型语言模型，包括LLaMA-3、Phi-3、Mistral和Gemma。通过使用如BLEU-3.1和ROUGE-3.2等重要指标来评估这些模型的性能。

通过此次分析，我们旨在识别每个模型的优势和劣势，并为这些模型在代码总结任务中的适用性和有效性提供见解。我们的研究结果将为大型语言模型的持续开发和改进做出贡献，支持其在提升软件开发和维护过程中的工具集成。 

---
# SAIL: Sample-Centric In-Context Learning for Document Information Extraction 

**Title (ZH)**: SAIL：以样本为中心的即席学习在文档信息提取中的应用 

**Authors**: Jinyu Zhang, Zhiyuan You, Jize Wang, Xinyi Le  

**Link**: [PDF](https://arxiv.org/pdf/2412.17092)  

**Abstract**: Document Information Extraction (DIE) aims to extract structured information from Visually Rich Documents (VRDs). Previous full-training approaches have demonstrated strong performance but may struggle with generalization to unseen data. In contrast, training-free methods leverage powerful pre-trained models like Large Language Models (LLMs) to address various downstream tasks with only a few examples. Nonetheless, training-free methods for DIE encounter two primary challenges: (1) understanding the complex relationship between layout and textual elements in VRDs, and (2) providing accurate guidance to pre-trained models. To address these challenges, we propose Sample-centric In-context Learning (SAIL) for DIE. SAIL introduces a fine-grained entity-level textual similarity to facilitate in-depth text analysis by LLMs and incorporates layout similarity to enhance the analysis of layouts in VRDs. Additionally, SAIL formulates a unified In-Context Learning (ICL) prompt template for various sample-centric examples, enabling tailored prompts that deliver precise guidance to pre-trained models for each sample. Extensive experiments on FUNSD, CORD, and SROIE benchmarks with various base models (e.g., LLMs) indicate that our method outperforms training-free baselines, even closer to the full-training methods. The results show the superiority and generalization of our method. 

**Abstract (ZH)**: 文档信息提取（DIE）旨在从视觉丰富文档（VRDs）中提取结构化信息。之前的端到端训练方法虽然表现出色，但在处理未见过的数据时可能会遇到泛化问题。相比之下，无需训练的方法利用大型语言模型（LLMs）的强大预训练模型，只需少量示例即可应对各种下游任务。然而，对于DIE的无需训练方法，存在两个主要挑战：（1）理解VRDs中布局和文本元素之间的复杂关系，以及（2）为预训练模型提供准确的指导。为了解决这些挑战，我们提出了一种以样本为中心的上下文学习方法（Sample-centric In-context Learning，简称SAIL）用于DIE。SAIL引入了细粒度的实体级文本相似性，以帮助大型语言模型进行更深入的文字分析，并结合布局相似性以增强对VRDs布局的分析。此外，SAIL为各种样本为中心的示例制定了一致的上下文学习（In-Context Learning，简称ICL）提示模板，使得每个样本能够获得针对性的提示，为预训练模型提供精确的指导。在FUNSD、CORD和SROIE基准数据集上进行的各种基模型（例如，LLMs）实验表明，我们的方法优于无需训练的基线方法，甚至接近端到端训练方法。实验结果证明了我们方法的优越性和泛化能力。 

---
# Optimizing Data Curation through Spectral Analysis and Joint Batch Selection (SALN) 

**Title (ZH)**: 通过谱分析和联合批次选择（SALN）优化数据编目 

**Authors**: Mohammadreza Sharifi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17069)  

**Abstract**: In modern deep learning models, long training times and large datasets present significant challenges to both efficiency and scalability. Effective data curation and sample selection are crucial for optimizing the training process of deep neural networks. This paper introduces SALN, a method designed to prioritize and select samples within each batch rather than from the entire dataset. By utilizing jointly selected batches, SALN enhances training efficiency compared to independent batch selection. The proposed method applies a spectral analysis-based heuristic to identify the most informative data points within each batch, improving both training speed and accuracy. The SALN algorithm significantly reduces training time and enhances accuracy when compared to traditional batch prioritization or standard training procedures. It demonstrates up to an 8x reduction in training time and up to a 5\% increase in accuracy over standard training methods. Moreover, SALN achieves better performance and shorter training times compared to Google's JEST method developed by DeepMind. 

**Abstract (ZH)**: 在现代深度学习模型中，长时间的训练和大规模的数据集给效率和可扩展性带来了显著挑战。有效的数据整理和样本选择对于优化深度神经网络的训练过程至关重要。本文介绍了一种名为SALN的方法，该方法旨在优先选择每个批次中的样本，而不是在整个数据集中进行选择。通过利用联合选择的批次，SALN相比独立批次选择能够提升训练效率。所提出的方法应用基于频谱分析的启发式方法来识别每个批次中最具信息量的数据点，从而提高训练速度和准确性。SALN算法相比于传统的批次优先级或标准训练流程，显著减少了训练时间并提升了准确性。与标准训练方法相比，SALN方法最多可减少8倍的训练时间，并提高5%的准确性。此外，SALN在性能和训练时间方面也优于DeepMind开发的Google JEST方法。 

---
# DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately 

**Title (ZH)**: DR-编码器：使用随机先验编码大型语言模型中低秩梯度以实现差异化隐私 

**Authors**: Huiwen Wu, Deyi Zhang, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Zhe Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17053)  

**Abstract**: The emergence of the Large Language Model (LLM) has shown their superiority in a wide range of disciplines, including language understanding and translation, relational logic reasoning, and even partial differential equations solving. The transformer is the pervasive backbone architecture for the foundation model construction. It is vital to research how to adjust the Transformer architecture to achieve an end-to-end privacy guarantee in LLM fine-tuning. In this paper, we investigate three potential information leakage during a federated fine-tuning procedure for LLM (FedLLM). Based on the potential information leakage, we provide an end-to-end privacy guarantee solution for FedLLM by inserting two-stage randomness. The first stage is to train a gradient auto-encoder with a Gaussian random prior based on the statistical information of the gradients generated by local clients. The second stage is to fine-tune the overall LLM with a differential privacy guarantee by adopting appropriate Gaussian noises. We show the efficiency and accuracy gains of our proposed method with several foundation models and two popular evaluation benchmarks. Furthermore, we present a comprehensive privacy analysis with Gaussian Differential Privacy (GDP) and Renyi Differential Privacy (RDP). 

**Abstract (ZH)**: 大语言模型（LLM）的出现展示了其在语言理解与翻译、关系逻辑推理，甚至偏微分方程求解等多个学科中的优越性。变压器架构已成为基础模型构建的普遍骨干网络结构。对于LLM微调中的端到端隐私保障而言，研究如何调整变压器架构至关重要。本文探讨了联邦微调过程中三种潜在的信息泄露问题（FedLLM）。基于这些潜在的信息泄露，我们通过插入两阶段随机性来提供FedLLM的端到端隐私保障方案。首先，利用本地客户端生成的梯度的统计信息，训练一个基于高斯随机先验的梯度自编码器。其次，通过采用适当的高斯噪声来以差分隐私保障的方式对整体LLM进行微调。我们通过几种基础模型和两种流行的评估基准展示了我们提出方法的有效性和准确性增益。此外，我们还提供了一种全面的隐私分析，包括高斯差分隐私（GDP）和雷尼差分隐私（RDP）。 

---
# An OpenMind for 3D medical vision self-supervised learning 

**Title (ZH)**: 面向3D医疗视觉自我监督学习的开放思维框架 

**Authors**: Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Michal Nohel, Robin Peretzke, Klaus H. Maier-Hein  

**Link**: [PDF](https://arxiv.org/pdf/2412.17041)  

**Abstract**: The field of 3D medical vision self-supervised learning lacks consistency and standardization. While many methods have been developed it is impossible to identify the current state-of-the-art, due to i) varying and small pre-training datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper we bring clarity to this field and lay the foundation for further method advancements: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes and b) benchmark existing SSL methods under common architectures and c) provide the code of our framework publicly to facilitate rapid adoption and reproduction. This pre-print \textit{only describes} the dataset contribution (a); Data, benchmark, and codebase will be made available shortly. 

**Abstract (ZH)**: 3D 医学视觉自监督学习领域缺乏一致性和标准化。尽管已经开发了许多方法，但由于 i) 前置训练数据集多样且规模较小，ii) 架构多样性，以及 iii) 在不同下游数据集上进行评估，目前无法确定最先进的方法。在本文中，我们为该领域带来了清晰度，并为未来方法的发展奠定了基础：我们 a) 发布了目前最大的公开前置训练数据集，包含 114,000 个 3D 头部 MRI 数据集，b) 在共同架构下评估现有自监督学习方法，并 c) 开放我们框架的代码，以促进快速采用和复现。本预印本仅描述了数据集的贡献（a）；数据、基准测试和代码库将在不久的将来发布。 

---
# ErasableMask: A Robust and Erasable Privacy Protection Scheme against Black-box Face Recognition Models 

**Title (ZH)**: ErasableMask：一种针对黑盒面部识别模型的稳健且可擦除的隐私保护方案 

**Authors**: Sipeng Shen, Yunming Zhang, Dengpan Ye, Xiuwen Shi, Long Tang, Haoran Duan, Ziyi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.17038)  

**Abstract**: While face recognition (FR) models have brought remarkable convenience in face verification and identification, they also pose substantial privacy risks to the public. Existing facial privacy protection schemes usually adopt adversarial examples to disrupt face verification of FR models. However, these schemes often suffer from weak transferability against black-box FR models and permanently damage the identifiable information that cannot fulfill the requirements of authorized operations such as forensics and authentication. To address these limitations, we propose ErasableMask, a robust and erasable privacy protection scheme against black-box FR models. Specifically, via rethinking the inherent relationship between surrogate FR models, ErasableMask introduces a novel meta-auxiliary attack, which boosts black-box transferability by learning more general features in a stable and balancing optimization strategy. It also offers a perturbation erasion mechanism that supports the erasion of semantic perturbations in protected face without degrading image quality. To further improve performance, ErasableMask employs a curriculum learning strategy to mitigate optimization conflicts between adversarial attack and perturbation erasion. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that ErasableMask achieves the state-of-the-art performance in transferability, achieving over 72% confidence on average in commercial FR systems. Moreover, ErasableMask also exhibits outstanding perturbation erasion performance, achieving over 90% erasion success rate. 

**Abstract (ZH)**: 虽然面部识别（FR）模型在面部验证和识别方面带来了显著的便利，但也对公众隐私构成了重大风险。现有的面部隐私保护方案通常采用对抗样本来干扰FR模型的面部验证，但这些方案往往在对抗黑盒FR模型时表现出较差的迁移性，并且不可避免地损害了可用于授权操作（如取证和身份验证）的可识别信息。为了解决这些限制，我们提出了一种名为ErasableMask的鲁棒且可擦除的隐私保护方案，适用于黑盒FR模型。具体而言，通过重新思考代理FR模型的固有关系，ErasableMask引入了一种新颖的元辅助攻击，通过在稳定且平衡的优化策略中学习更一般的特征来增强黑盒迁移性。同时，该方案还提供了一种扰动擦除机制，能够在不降低图像质量的情况下擦除受保护面部的语义扰动。为了进一步提升性能，ErasableMask采用了逐阶段学习策略，以缓解对抗攻击和扰动擦除之间的优化冲突。在CelebA-HQ和FFHQ数据集上的广泛实验结果显示，ErasableMask在迁移性方面达到了最先进的性能，商用FR系统的平均置信度超过72%。此外，ErasableMask在扰动擦除性能方面也表现出色，成功擦除率达到90%以上。 

---
# A Reality Check on Context Utilisation for Retrieval-Augmented Generation 

**Title (ZH)**: 对基于检索增强生成中语境利用情况的现实检视 

**Authors**: Lovisa Hagström, Sara Vera Marjanović, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, Isabelle Augenstein  

**Link**: [PDF](https://arxiv.org/pdf/2412.17031)  

**Abstract**: Retrieval-augmented generation (RAG) helps address the limitations of the parametric knowledge embedded within a language model (LM). However, investigations of how LMs utilise retrieved information of varying complexity in real-world scenarios have been limited to synthetic contexts. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complex and diverse real-world context settings. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings. 

**Abstract (ZH)**: 检索增强生成（RAG）有助于解决参数化知识嵌入在语言模型（LM）中的局限性。然而，关于如何在真实场景中利用不同复杂度的检索信息，现有研究主要局限于合成语境之中。本文介绍了DRUID（包含人工标注立场的真实语境和查询的数据集），旨在通过基于真实证据的自动求证这一原型任务，解决这一研究空白。我们对比了DRUID和合成数据集（如CounterFact和ConflictQA），发现合成数据集往往无法准确反映复杂多样的真实语境设置。我们展示了合成数据集过度放大了在真实检索数据中罕见的语境特征，这导致了更高的人工上下文利用率（ACU）得分，这是通过我们新开发的ACU评分衡量的。此外，尽管过去的研究主要集中在解释单个语境特征的语境利用率上，但在DRUID上，单个语境属性与ACU的相关性与其他与来源相关的属性相比，出乎意料地小。总而言之，我们的工作强调了进行与真实场景对齐的语境利用率研究的重要性，以更好地代表和提升真实场景中RAG的表现。 

---
# Data value estimation on private gradients 

**Title (ZH)**: 私人梯度的数据价值评估 

**Authors**: Zijian Zhou, Xinyi Xu, Daniela Rus, Bryan Kian Hsiang Low  

**Link**: [PDF](https://arxiv.org/pdf/2412.17008)  

**Abstract**: For gradient-based machine learning (ML) methods commonly adopted in practice such as stochastic gradient descent, the de facto differential privacy (DP) technique is perturbing the gradients with random Gaussian noise. Data valuation attributes the ML performance to the training data and is widely used in privacy-aware applications that require enforcing DP such as data pricing, collaborative ML, and federated learning (FL). Can existing data valuation methods still be used when DP is enforced via gradient perturbations? We show that the answer is no with the default approach of injecting i.i.d.~random noise to the gradients because the estimation uncertainty of the data value estimation paradoxically linearly scales with more estimation budget, producing estimates almost like random guesses. To address this issue, we propose to instead inject carefully correlated noise to provably remove the linear scaling of estimation uncertainty w.r.t.~the budget. We also empirically demonstrate that our method gives better data value estimates on various ML tasks and is applicable to use cases including dataset valuation and~FL. 

**Abstract (ZH)**: 基于梯度的机器学习（ML）方法，如随机梯度下降（Stochastic Gradient Descent, SGD），实践中默认采用的实际差分隐私（Differential Privacy, DP）技术是在梯度中添加随机高斯噪声。数据评估将机器学习性能归因于训练数据，并在需要实现DP的应用中（如数据定价、合作学习和联邦学习）广泛使用。

当通过梯度扰动来实施DP时，现有数据评估方法是否仍然有效？我们通过在梯度中注入独立同分布（i.i.d.）随机噪声的默认方法表明，答案是否定的，因为数据价值估计的估计不确定性随预算增加非线性地增加，导致估计几乎像随机猜测一样。为解决这一问题，我们提出注入精心设计的相关噪声，以证明可以消除估计不确定性随预算增加的线性关系。此外，我们通过多种机器学习任务的实验演示，证明我们的方法在数据集价值评估和联邦学习等应用场景中能提供更好的数据价值估计。 

---
# Solving Nonlinear Energy Supply and Demand System Using Physics-Informed Neural Networks 

**Title (ZH)**: 使用物理知情神经网络解决非线性能源供需系统问题 

**Authors**: Van Truong Vo, Samad Noeiaghdam, Denis Sidorov, Aliona Dreglea, Liguo Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17001)  

**Abstract**: Nonlinear differential equations and systems play a crucial role in modeling systems where time-dependent factors exhibit nonlinear characteristics. Due to their nonlinear nature, solving such systems often presents significant difficulties and challenges. In this study, we propose a method utilizing Physics-Informed Neural Networks (PINNs) to solve the nonlinear energy supply-demand (ESD) system. We design a neural network with four outputs, where each output approximates a function that corresponds to one of the unknown functions in the nonlinear system of differential equations describing the four-dimensional ESD problem. The neural network model is then trained and the parameters are identified, optimized to achieve a more accurate solution. The solutions obtained from the neural network for this problem are equivalent when we compare and evaluate them against the Runge-Kutta numerical method of order 4/5 (RK45). However, the method utilizing neural networks is considered a modern and promising approach, as it effectively exploits the superior computational power of advanced computer systems, especially in solving complex problems. Another advantage is that the neural network model, after being trained, can solve the nonlinear system of differential equations across a continuous domain. In other words, neural networks are not only trained to approximate the solution functions for the nonlinear ESD system but can also represent the complex dynamic relationships between the system's components. However, this approach requires significant time and computational power due to the need for model training. 

**Abstract (ZH)**: 非线性微分方程和系统在建模显示非线性特征的时间依赖因素的系统中发挥着关键作用。由于其非线性性质，求解此类系统通常会遇到重大困难和挑战。在本研究中，我们提出了一种使用物理信息神经网络（PINNs）的方法来解决非线性能源供应-需求（ESD）系统。我们设计了一个具有四个输出的神经网络，每个输出近似对应非线性微分方程系统中描述四维ESD问题的一个未知函数。随后，对该神经网络模型进行训练，并优化参数以获得更准确的解。通过与四/五阶龙格-库塔数值方法（RK45）进行比较和评估，神经网络获得的解具有等效性。但是，利用神经网络的方法被视为一种现代且有前景的方法，因为它有效地利用了高性能计算机系统的强大计算能力，特别是在解决复杂问题方面。另一个优势是，经过训练的神经网络模型可以在连续域上求解非线性微分方程系统。换句话说，神经网络不仅被训练来近似非线性ESD系统的解函数，还可以表示系统组件之间的复杂动态关系。然而，该方法需要大量的时间和计算资源，因为需要进行模型训练。 

---
# LLM-Powered User Simulator for Recommender System 

**Title (ZH)**: 基于LLM的用户模拟器在推荐系统中的应用 

**Authors**: Zijian Zhang, Shuchang Liu, Ziru Liu, Rui Zhong, Qingpeng Cai, Xiangyu Zhao, Chunxu Zhang, Qidong Liu, Peng Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16984)  

**Abstract**: User simulators can rapidly generate a large volume of timely user behavior data, providing a testing platform for reinforcement learning-based recommender systems, thus accelerating their iteration and optimization. However, prevalent user simulators generally suffer from significant limitations, including the opacity of user preference modeling and the incapability of evaluating simulation accuracy. In this paper, we introduce an LLM-powered user simulator to simulate user engagement with items in an explicit manner, thereby enhancing the efficiency and effectiveness of reinforcement learning-based recommender systems training. Specifically, we identify the explicit logic of user preferences, leverage LLMs to analyze item characteristics and distill user sentiments, and design a logical model to imitate real human engagement. By integrating a statistical model, we further enhance the reliability of the simulation, proposing an ensemble model that synergizes logical and statistical insights for user interaction simulations. Capitalizing on the extensive knowledge and semantic generation capabilities of LLMs, our user simulator faithfully emulates user behaviors and preferences, yielding high-fidelity training data that enrich the training of recommendation algorithms. We establish quantifying and qualifying experiments on five datasets to validate the simulator's effectiveness and stability across various recommendation scenarios. 

**Abstract (ZH)**: 用户模拟器可以快速生成大量及时的用户行为数据，为基于强化学习的推荐系统提供测试平台，从而加快它们的迭代和优化。然而，现有的用户模拟器通常存在显著的局限性，包括用户偏好建模的不透明性和评估模拟准确性的能力不足。本文引入了基于大语言模型（LLM）的用户模拟器，以明确的方式模拟用户与项目之间的互动，进而提升基于强化学习的推荐系统训练的效率和效果。具体来说，我们识别出用户的明确偏好逻辑，利用大语言模型分析项目特征并提取用户情感，并设计一个逻辑模型来模仿真实的人类互动。通过整合统计模型，进一步增强了模拟的可靠性，提出了一种综合逻辑和统计洞察的集成模型，用于用户互动模拟。借助大语言模型的广泛知识和语义生成能力，我们的用户模拟器忠实模拟了用户行为和偏好，生成了高保真的训练数据，丰富了推荐算法的训练。我们通过在五个数据集上进行量化和定性实验来验证模拟器在各种推荐场景下的有效性和稳定性。 

---
# PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask 

**Title (ZH)**: PromptDresser：通过生成性文本提示和提示感知掩码提高虚拟试穿的质量和可控性 

**Authors**: Jeongho Kim, Hoiyeong Jin, Sunghyun Park, Jaegul Choo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16978)  

**Abstract**: Recent virtual try-on approaches have advanced by fine-tuning the pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on is still underexplored. This paper tackles a text-editable virtual try-on task that changes the clothing item based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing person's clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original person's appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. We found that our approach, utilizing detailed text prompts, not only enhances text editability but also effectively conveys clothing details that are difficult to capture through images alone, thereby enhancing image quality. Our code is available at this https URL. 

**Abstract (ZH)**: 近年来，通过微调预训练的文本到图像扩散模型，虚拟试穿技术取得了显著进展，利用其强大的生成能力。然而，文本提示在虚拟试穿中的应用仍然未被充分探索。本文探讨了一种基于文本可编辑的虚拟试穿任务，该任务根据提供的服装图片改变服装项，并根据文本描述编辑穿着风格（例如，叠穿风格、合身度）。在文本可编辑的虚拟试穿中，存在三个关键方面：(i) 为人员-服装配对数据设计丰富的文本描述以训练模型；(ii) 解决现有人员服装的文本信息干扰新服装生成的冲突问题；(iii) 根据文本描述自适应调整修复掩码，确保适当的编辑区域，同时保留与新服装无关的人员原始外观。为解决这些问题，我们提出了一种名为PromptDresser的文本可编辑虚拟试穿模型，该模型利用大型多模态模型（LMM）的帮助，基于生成的文本提示实现高质量和多样的操作。我们的方法通过上下文学习利用LMM独立生成人员和服装图像的详细文本描述，包括姿态细节和编辑属性，同时将高昂的人工成本降至最低。此外，为了确保编辑区域，我们根据文本提示自适应调整修复掩码。我们发现在利用详细文本提示的情况下，不仅提高了文本的可编辑性，还有效地传达了仅通过图像难以捕捉的服装细节，从而提升了图像质量。我们的代码可在以下链接获取：this https URL。 

---
# On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora 

**Title (ZH)**: 在健康语料中不连续命名实体识别中融合ChatGPT和集成学习的研究 

**Authors**: Tzu-Chieh Chen, Wen-Yang Lin  

**Link**: [PDF](https://arxiv.org/pdf/2412.16976)  

**Abstract**: Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain. 

**Abstract (ZH)**: 命名实体识别一直是自然语言处理中的一个关键任务，旨在从非结构化的文本数据中识别和提取重要的术语。然而，当代深度学习命名实体识别模型在识别断续实体方面遇到了显著挑战，这些实体在文本中通常是碎片化的。截至目前，我们所知的研究中还没有使用集成学习方法解决断续命名实体识别问题。此外，近年来大型语言模型的兴起，比如ChatGPT，在许多NLP任务中表现出了显著的效果。现有的大多数方法主要是将ChatGPT作为问题解决工具使用，而没有探索将其作为集成学习算法中整合元素的潜在可能性。本研究探讨了将ChatGPT作为仲裁者集成到集成方法中的可能性，旨在提高断续命名实体识别任务的性能。我们的方法通过自定义提示工程将五种最先进的命名实体识别模型与ChatGPT结合，以评估集成算法的鲁棒性和泛化能力。我们在三个基准医疗数据集上进行了实验，将我们的方法与五种最先进的模型、GPT-3.5和GPT-4的独立应用以及投票集成方法进行了比较。结果显示，我们提出的将ChatGPT与集成学习算法结合的方法在CADEC、ShARe13和ShARe14数据集上的表现优于最先进的结果，展示了其在医疗领域NLP应用中的潜在优势。 

---
# Prompting Large Language Models with Rationale Heuristics for Knowledge-based Visual Question Answering 

**Title (ZH)**: 使用合理性启发式方法提示大型语言模型进行基于知识的视觉问答 

**Authors**: Zhongjian Hu, Peng Yang, Bing Li, Fengyuan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16936)  

**Abstract**: Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）被应用于基于知识的视觉问答（VQA）。尽管先前的研究取得了令人鼓舞的结果，但现有的方法直接促使LLMs预测答案，忽略了中间的思维过程。我们认为，现有方法未能充分激活LLMs的能力。为此，我们提出了一种名为PLRH的框架，通过引入思维链（CoT）的启发式方法来促使LLMs生成中间的思维过程，即推理启发式。然后利用这些推理启发式来启发LLMs预测答案。实验结果显示，我们的方法在OK-VQA和A-OKVQA数据集上的表现分别比现有基线方法高出2.2和2.1个百分点。 

---
# Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation 

**Title (ZH)**: 有效地求解具有广义形式相关性的轮转随机博弈 

**Authors**: Hanrui Zhang, Yu Cheng, Vincent Conitzer  

**Link**: [PDF](https://arxiv.org/pdf/2412.16934)  

**Abstract**: We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error $\varepsilon$ in time polynomial in the size of the game, as well as $\log(1 / \varepsilon)$.
Our algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions. 

**Abstract (ZH)**: 我们研究了在两人轮流的随机游戏中进行广泛形式相关性均衡计算的问题。我们的主要成果包括两个方面：（1）我们提供了一个算法来计算斯塔克尔伯格广泛形式相关均衡（SEFCE），其运行时间与游戏的规模以及编码每个输入数字所需的位数成多项式关系。（2）我们提供了一个高效的算法来近似计算最优广泛形式相关均衡（EFCE），其误差达到机器精度，即算法在运行时间上与游戏的规模以及 $\log(1 / \varepsilon)$ 成多项式关系。

我们的SEFCE算法是首个在如此广泛的一类随机游戏中进行承诺均衡计算的多项式时间算法。现有的SEFCE算法通常需要更强的假设，例如没有机会行动，并且是为具有较少紧凑树结构的扩展形式博弈设计的。我们提出的近似最优EFCE算法，据我们所知，是首个同时实现三个目标的算法：近似最优性、对误差逼近误差的对数因子依赖以及与具有更紧凑图结构的随机博弈兼容。现有的算法最多只能实现其中两个目标，且常常还需要依赖额外的技术假设。 

---
# Towards a Unified Paradigm: Integrating Recommendation Systems as a New Language in Large Models 

**Title (ZH)**: 面向统一范式的探索：将推荐系统融入大规模模型作为新语言 

**Authors**: Kai Zheng, Qingfeng Sun, Can Xu, Peng Yu, Qingwei Guo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16933)  

**Abstract**: This paper explores the use of Large Language Models (LLMs) for sequential recommendation, which predicts users' future interactions based on their past behavior. We introduce a new concept, "Integrating Recommendation Systems as a New Language in Large Models" (RSLLM), which combines the strengths of traditional recommenders and LLMs. RSLLM uses a unique prompting method that combines ID-based item embeddings from conventional recommendation models with textual item features. It treats users' sequential behaviors as a distinct language and aligns the ID embeddings with the LLM's input space using a projector. We also propose a two-stage LLM fine-tuning framework that refines a pretrained LLM using a combination of two contrastive losses and a language modeling loss. The LLM is first fine-tuned using text-only prompts, followed by target domain fine-tuning with unified prompts. This trains the model to incorporate behavioral knowledge from the traditional sequential recommender into the LLM. Our empirical results validate the effectiveness of our proposed framework. 

**Abstract (ZH)**: 本文探讨了使用大规模语言模型（LLMs）进行序列推荐的方法，该方法根据用户的历史行为预测其未来的互动行为。我们提出了一种新的概念——“将推荐系统作为大型模型中的一种新语言进行集成”（RSLLM，Recommendation Systems as a Language in Large Models），它结合了传统推荐系统和LLMs的优势。RSLLM 通过结合基于ID的项目嵌入和文本项目特征的特有提示方法，将用户的序列行为视为一种独特的语言，并使用投影仪将ID嵌入与LLM的输入空间进行对齐。我们还提出了一种两阶段的LLM微调框架，该框架使用两种对比损失和语言模型损失的组合进行微调。首先，使用仅文本的提示对LLM进行微调，随后使用统一提示对目标领域进行微调。这训练模型将传统序列推荐系统的行为知识整合到LLM中。实验证明了我们所提出框架的有效性。 

---
# Revisiting In-Context Learning with Long Context Language Models 

**Title (ZH)**: 重新审视长上下文语言模型的上下文学习 

**Authors**: Jinheon Baek, Sun Jae Lee, Prakhar Gupta, Geunseob, Siddharth Dalmia, Prateek Kolhar  

**Link**: [PDF](https://arxiv.org/pdf/2412.16926)  

**Abstract**: In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%. 

**Abstract (ZH)**: 上下文学习（ICL）是一种基于语言模型在其输入上下文中提供的示例进行预测的技术。之前，上下文窗口的大小限制了可以展示的示例子数，因此选择有效的示例变得尤为重要。然而，最近长上下文语言模型（LCLMs）的出现显著增加了可以包含在上下文中的示例子数，这引发了在大量示例情况下ICL性能是否仍然受到样本选择方法的影响的重要问题。为了解答这个问题，我们通过在包含18个数据集、覆盖4个任务的广泛实验中重新审视这些方法，发现在LCLMs的背景下，复杂的示例选择技术并没有比简单的随机样本选择方法带来显著的性能提升。相反，我们发现，LCLMs的出现从根本上改变了ICL的挑战，从选择最有效的示例转变为收集足够多的示例以填满上下文窗口。具体来说，在某些数据集中，包含所有可用的示例并不能充分利用上下文窗口；然而，通过使用简单数据增强方法来扩充上下文中的示例，ICL的性能提高了5%。 

---
# Quantifying Public Response to COVID-19 Events: Introducing the Community Sentiment and Engagement Index 

**Title (ZH)**: 量化公共对 COVID-19 事件的响应：介绍社区情绪参与指数 

**Authors**: Nirmalya Thakur, Kesha A. Patel, Audrey Poon, Shuqi Cui, Nazif Azizi, Rishika Shah, Riyan Shah  

**Link**: [PDF](https://arxiv.org/pdf/2412.16925)  

**Abstract**: This study introduces the Community Sentiment and Engagement Index (CSEI), developed to capture nuanced public sentiment and engagement variations on social media, particularly in response to major events related to COVID-19. Constructed with diverse sentiment indicators, CSEI integrates features like engagement, daily post count, compound sentiment, fine-grain sentiments (fear, surprise, joy, sadness, anger, disgust, and neutral), readability, offensiveness, and domain diversity. Each component is systematically weighted through a multi-step Principal Component Analysis (PCA)-based framework, prioritizing features according to their variance contributions across temporal sentiment shifts. This approach dynamically adjusts component importance, enabling CSEI to precisely capture high-sensitivity shifts in public sentiment. The development of CSEI showed statistically significant correlations with its constituent features, underscoring internal consistency and sensitivity to specific sentiment dimensions. CSEI's responsiveness was validated using a dataset of 4,510,178 Reddit posts about COVID-19. The analysis focused on 15 major events, including the WHO's declaration of COVID-19 as a pandemic, the first reported cases of COVID-19 across different countries, national lockdowns, vaccine developments, and crucial public health measures. Cumulative changes in CSEI revealed prominent peaks and valleys aligned with these events, indicating significant patterns in public sentiment across different phases of the pandemic. Pearson correlation analysis further confirmed a statistically significant relationship between CSEI daily fluctuations and these events (p = 0.0428), highlighting the capacity of CSEI to infer and interpret shifts in public sentiment and engagement in response to major events related to COVID-19. 

**Abstract (ZH)**: 本研究介绍了社区情感和参与指数（CSEI），该指数旨在捕捉社交媒体上对与 COVID-19 相关的重大事件的细微情感和参与度变化。CSEI 通过多种情感指标构建，整合了参与度、每日发帖量、复合情感、细粒度情感（恐惧、惊讶、快乐、悲伤、愤怒、厌恶和中性）、可读性、冒犯性和领域多样性等特征。每个组成部分通过多步骤的主成分分析（PCA）为基础的框架系统加权，根据其在时间情感变化中的方差贡献进行优先级排序。这种方法动态调整组成部分的重要性，使 CSEI 能够准确捕捉公众情感的高敏感变化。CSEI 的开发显示其与组成部分特征之间存在统计学显著相关性，强调了内部一致性并对其特定情感维度的敏感性。利用包含 4,510,178 条有关 COVID-19 的 Reddit 发帖的数据集验证了 CSEI 的响应性。分析集中在 15 个主要事件上，包括世界卫生组织将 COVID-19 定义为全球大流行、不同国家首次报告的 COVID-19 病例、国家封锁、疫苗开发以及关键公共卫生措施。CSEI 累计变化在这些事件期间呈现出显着的峰值和谷值，表明不同疫情阶段中公众情感的显著模式。皮尔逊相关分析进一步证实了 CSEI 每日波动与这些事件之间存在统计学显著关系（p = 0.0428），突显了 CSEI 识别和解释与 COVID-19 相关的重大事件响应中公众情感和参与度变化的能力。 

---
# Enhancing Supply Chain Transparency in Emerging Economies Using Online Contents and LLMs 

**Title (ZH)**: 使用在线内容和大规模语言模型增强新兴经济体的供应链透明度 

**Authors**: Bohan Jin, Qianyou Sun, Lihua Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.16922)  

**Abstract**: In the current global economy, supply chain transparency plays a pivotal role in ensuring this security by enabling companies to monitor supplier performance and fostering accountability and responsibility. Despite the advancements in supply chain relationship datasets like Bloomberg and FactSet, supply chain transparency remains a significant challenge in emerging economies due to issues such as information asymmetry and institutional gaps in regulation. This study proposes a novel approach to enhance supply chain transparency in emerging economies by leveraging online content and large language models (LLMs). We develop a Supply Chain Knowledge Graph Mining System that integrates advanced LLMs with web crawler technology to automatically collect and analyze supply chain information. The system's effectiveness is validated through a case study focusing on the semiconductor supply chain, a domain that has recently gained significant attention due to supply chain risks. Our results demonstrate that the proposed system provides greater applicability for emerging economies, such as mainland China, complementing the data gaps in existing datasets. However, challenges including the accurate estimation of monetary and material flows, the handling of time series data, synonyms disambiguation, and mitigating biases from online contents still remains. Future research should focus on addressing these issues to further enhance the system's capabilities and broaden its application to other emerging economies and industries. 

**Abstract (ZH)**: 在当前全球经济中，供应链透明度在确保安全方面扮演着至关重要的角色，通过使公司能够监控供应商表现并促进问责制和责任感。尽管Bloomberg和FactSet等供应链关系数据集有所进步，但由于信息不对称和监管制度的空缺等因素，供应链透明度在新兴经济体中仍是一个重大挑战。本研究提出了一种新颖的方法，通过利用在线内容和大型语言模型（LLMs）来增强新兴经济体中的供应链透明度。我们开发了一个供应链知识图谱挖掘系统，该系统将先进的LLMs与网络爬虫技术相结合，以自动收集和分析供应链信息。通过一个专注于半导体供应链的案例研究，证明了该系统的有效性，而半导体供应链因其供应风险问题已成为近期关注的焦点。我们的研究结果表明，所提出的系统在填补现有数据集的数据空白方面具有更广泛的适用性，例如中国大陆等新兴经济体。然而，仍存在诸如货币和物质流动的准确估计、时间序列数据的处理、同义词消歧及减轻在线内容偏见等挑战。未来的研究应关注解决这些问题，以进一步提高系统的功能，并将其应用扩展到其他新兴经济体和行业。 

---
# FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation 

**Title (ZH)**: FADA：快速扩散avatar合成与混合监督多CFG蒸馏 

**Authors**: Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.16915)  

**Abstract**: Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage this http URL. 

**Abstract (ZH)**: 基于扩散的音频驱动的虚拟角色方法近年来因其高保真、生动且富有表现力的结果而受到关注。然而，它们较慢的推理速度限制了其实用应用。尽管已经发展出各种各样的蒸馏技术来优化扩散模型，我们发现朴素的扩散模型蒸馏方法并没有取得满意的效果。蒸馏模型在开放集输入图像下的鲁棒性降低，并且与教师模型相比，音视频之间的相关性下降，这一点削弱了扩散模型的优势。为了解决这一问题，我们提出了FADA（快速扩散角色合成的混合监督多CFG蒸馏方法）。我们首先设计了一种混合监督损失，以利用不同质量的数据，增强模型的整体能力和鲁棒性。此外，我们提出了带有可学习标记的多CFG蒸馏方法，以利用音频和参考图像条件之间的相关性，从而在保证一定质量的前提下减少由多CFG带来的三次推理运行。在多个数据集上的大量实验表明，FADA 能够生成与最近的基于扩散模型的方法相当生动的视频，同时实现4.17-12.5倍的NFE加速。我们的演示可以在网页 [此链接] 查看。 

---
# Map Imagination Like Blind Humans: Group Diffusion Model for Robotic Map Generation 

**Title (ZH)**: 盲人般imap想象力：群体扩散模型在机器人地图生成中的应用 

**Authors**: Qijin Song, Weibang Bai  

**Link**: [PDF](https://arxiv.org/pdf/2412.16908)  

**Abstract**: Can robots imagine or generate maps like humans do, especially when only limited information can be perceived like blind people? To address this challenging task, we propose a novel group diffusion model (GDM) based architecture for robots to generate point cloud maps with very limited input this http URL from the blind humans' natural capability of imagining or generating mental maps, the proposed method can generate maps without visual perception data or depth data. With additional limited super-sparse spatial positioning data, like the extra contact-based positioning information the blind individuals can obtain, the map generation quality can be improved even this http URL on public datasets are conducted, and the results indicate that our method can generate reasonable maps solely based on path data, and produce even more refined maps upon incorporating exiguous LiDAR this http URL to conventional mapping approaches, our novel method significantly mitigates sensor dependency, enabling the robots to imagine and generate elementary maps without heavy onboard sensory devices. 

**Abstract (ZH)**: 机器人能否像人类一样想象或生成地图，尤其是当只能感知到有限信息时，就像盲人一样？为了应对这一具有挑战性的任务，我们提出了一种基于新型群体扩散模型（Group Diffusion Model, GDM）的架构，旨在让机器人仅基于非常有限的输入生成点云地图。通过借鉴盲人自然具备的想象或生成心理地图的能力，所提出的方法可以在没有视觉感知数据或深度数据的情况下生成地图。借助额外的有限的稀疏空间定位数据，类似于盲人通过接触获得的额外的定位信息，地图的生成质量可以得到提升。在公开数据集上的实验表明，我们的方法可以仅基于路径数据生成合理的地图，并结合少量的LiDAR数据生成更加细化的地图。对于传统的制图方法，我们的新型方法显著减轻了对感知设备的依赖，使得机器人能够在不依赖复杂车载传感器的情况下想象和生成基础地图。 

---
# A Backdoor Attack Scheme with Invisible Triggers Based on Model Architecture Modification 

**Title (ZH)**: 基于模型结构修改的隐形触发器后门攻击方案 

**Authors**: Yuan Ma, Xu Ma, Jiankang Wei, Jinmeng Tang, Xiaoyu Zhang, Yilun Lyu, Kehao Chen, Jingtong Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16905)  

**Abstract**: Machine learning systems are vulnerable to backdoor attacks, where attackers manipulate model behavior through data tampering or architectural modifications. Traditional backdoor attacks involve injecting malicious samples with specific triggers into the training data, causing the model to produce targeted incorrect outputs in the presence of the corresponding triggers. More sophisticated attacks modify the model's architecture directly, embedding backdoors that are harder to detect as they evade traditional data-based detection methods. However, the drawback of the architectural modification based backdoor attacks is that the trigger must be visible in order to activate the backdoor. To further strengthen the invisibility of the backdoor attacks, a novel backdoor attack method is presented in the paper. To be more specific, this method embeds the backdoor within the model's architecture and has the capability to generate inconspicuous and stealthy triggers. The attack is implemented by modifying pre-trained models, which are then redistributed, thereby posing a potential threat to unsuspecting users. Comprehensive experiments conducted on standard computer vision benchmarks validate the effectiveness of this attack and highlight the stealthiness of its triggers, which remain undetectable through both manual visual inspection and advanced detection tools. 

**Abstract (ZH)**: 机器学习系统容易受到后门攻击的威胁，攻击者可以通过数据篡改或架构修改来操控模型的行为。传统的后门攻击涉及将带有特定触发器的恶意样本注入训练数据中，导致模型在遇到相应的触发器时产生特定的错误输出。更为复杂的攻击直接修改模型的架构，在模型中嵌入难以检测的后门，这些后门能够逃避基于数据的传统检测方法。然而，基于架构修改的后门攻击的一个缺点是触发器必须是可见的才能触发后门。为了进一步增强后门攻击的隐蔽性，本文提出了一种新的后门攻击方法。具体而言，该方法将后门嵌入模型的架构中，并能够生成不显眼且隐蔽的触发器。攻击通过修改预训练模型并重新分发模型实现，从而对无防备的用户构成潜在威胁。针对标准计算机视觉基准进行的全面实验验证了该攻击的有效性，并强调了其触发器的隐蔽性，这些触发器通过人工视觉检查和高级检测工具都无法被检测到。 

---
# MVREC: A General Few-shot Defect Classification Model Using Multi-View Region-Context 

**Title (ZH)**: MVREC：一种基于多视角区域上下文的通用少样本缺陷分类模型 

**Authors**: Shuai Lyu, Fangjian Liao, Zeqi Ma, Rongchen Zhang, Dongmei Mo, Waikeung Wong  

**Link**: [PDF](https://arxiv.org/pdf/2412.16897)  

**Abstract**: Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: this https URL 

**Abstract (ZH)**: 少量样本的缺陷多分类（Few-shot Defect Multi-classification, FSDMC）是工业制造质量控制领域的一个新兴趋势。然而，当前的FSDMC研究由于主要集中在特定的数据集上，因此缺乏通用性。此外，缺陷分类高度依赖图像中的上下文信息，现有的方法在有效提取这种信息方面存在不足。为了解决这些问题，我们提出了一种通用的FSDMC框架——MVREC，并具备两个主要优势：（1）MVREC通过结合预训练的AlphaCLIP模型，提取缺陷实例的一般特征。（2）该框架利用区域上下文框架，通过利用掩码区域输入和多视角上下文增强，来增强缺陷特征。此外，模型中引入了Few-shot Zip-Adapter（-F）分类器，以缓存支持集的视觉特征并进行少量样本分类。我们还基于MVTec AD引入了一个新的FSDMC基准MVTec-FS，其中包括1228张带有实例级别掩码标注的缺陷图像和46种缺陷类型。在MVTec-FS和四个额外数据集上进行的大量实验表明，该方法在通用缺陷分类方面有效，并且能够通过整合上下文信息来提高分类性能。代码：this https URL 

---
# Preventing Non-intrusive Load Monitoring Privacy Invasion: A Precise Adversarial Attack Scheme for Networked Smart Meters 

**Title (ZH)**: 防止非侵入式负荷监测隐私泄露：网络智能电表精确对抗攻击方案 

**Authors**: Jialing He, Jiacheng Wang, Ning Wang, Shangwei Guo, Liehuang Zhu, Dusit Niyato, Tao Xiang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16893)  

**Abstract**: Smart grid, through networked smart meters employing the non-intrusive load monitoring (NILM) technique, can considerably discern the usage patterns of residential appliances. However, this technique also incurs privacy leakage. To address this issue, we propose an innovative scheme based on adversarial attack in this paper. The scheme effectively prevents NILM models from violating appliance-level privacy, while also ensuring accurate billing calculation for users. To achieve this objective, we overcome two primary challenges. First, as NILM models fall under the category of time-series regression models, direct application of traditional adversarial attacks designed for classification tasks is not feasible. To tackle this issue, we formulate a novel adversarial attack problem tailored specifically for NILM and providing a theoretical foundation for utilizing the Jacobian of the NILM model to generate imperceptible perturbations. Leveraging the Jacobian, our scheme can produce perturbations, which effectively misleads the signal prediction of NILM models to safeguard users' appliance-level privacy. The second challenge pertains to fundamental utility requirements, where existing adversarial attack schemes struggle to achieve accurate billing calculation for users. To handle this problem, we introduce an additional constraint, mandating that the sum of added perturbations within a billing period must be precisely zero. Experimental validation on real-world power datasets REDD and UK-DALE demonstrates the efficacy of our proposed solutions, which can significantly amplify the discrepancy between the output of the targeted NILM model and the actual power signal of appliances, and enable accurate billing at the same time. Additionally, our solutions exhibit transferability, making the generated perturbation signal from one target model applicable to other diverse NILM models. 

**Abstract (ZH)**: 智能电网通过采用非侵权负载监控（NILM）技术的网络化智能电表，可以显著识别住宅电器的使用模式。然而，该技术也会导致隐私泄露。为解决这一问题，本文提出了一个基于对抗攻击的创新方案。该方案能够有效防止NILM模型违反电器级别的隐私，同时确保为用户准确计算账单。为了实现这一目标，我们克服了两个主要挑战。首先，由于 NILM 模型属于时间序列回归模型，直接应用传统针对分类任务设计的对抗攻击方法是不可行的。因此，我们为 NILM 特别提出了一个新的对抗攻击问题，为利用 NILM 模型的雅可比矩阵生成不可感知的扰动奠定了理论基础。借助雅可比矩阵，我们的方案能够生成扰动，有效地误导 NILM 模型的信号预测，从而保护用户的电器级别隐私。其次，该挑战涉及基本的实用需求，现有的对抗攻击方案难以实现对用户的准确账单计算。为解决这一问题，我们引入了一个附加约束条件，即在一个账单周期内添加的扰动之和必须准确为零。在真实的电力数据集 REDD 和 UK-DALE 上的实验验证表明，我们提出的方法能够显著增加目标 NILM 模型输出与实际电器信号之间的差异，并同时实现准确的账单计算。此外，我们的方法具有可迁移性，从一个目标模型生成的扰动信号可以应用到其他不同的 NILM 模型上。 

---
# Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model 

**Title (ZH)**: 基于在线偏好强化学习的大语言模型自增强反馈机制 

**Authors**: Songjun Tu, Jingbo Sun, Qichao Zhang, Xiangyuan Lan, Dongbin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.16878)  

**Abstract**: Preference-based reinforcement learning (PbRL) provides a powerful paradigm to avoid meticulous reward engineering by learning rewards based on human preferences. However, real-time human feedback is hard to obtain in online tasks. Most work suppose there is a "scripted teacher" that utilizes privileged predefined reward to provide preference feedback. In this paper, we propose a RL Self-augmented Large Language Model Feedback (RL-SaLLM-F) technique that does not rely on privileged information for online PbRL. RL-SaLLM-F leverages the reflective and discriminative capabilities of LLM to generate self-augmented trajectories and provide preference labels for reward learning. First, we identify an failure issue in LLM-based preference discrimination, specifically "query ambiguity", in online PbRL. Then LLM is employed to provide preference labels and generate self-augmented imagined trajectories that better achieve the task goal, thereby enhancing the quality and efficiency of feedback. Additionally, a double-check mechanism is introduced to mitigate randomness in the preference labels, improving the reliability of LLM feedback. The experiment across multiple tasks in the MetaWorld benchmark demonstrates the specific contributions of each proposed module in RL-SaLLM-F, and shows that self-augmented LLM feedback can effectively replace the impractical "scripted teacher" feedback. In summary, RL-SaLLM-F introduces a new direction of feedback acquisition in online PbRL that does not rely on any online privileged information, offering an efficient and lightweight solution with LLM-driven feedback. 

**Abstract (ZH)**: 基于偏好的强化学习（Preference-based Reinforcement Learning, PbRL）提供了一种强大的范式，通过基于人类偏好学习奖励来避免繁琐的奖励工程。然而，在在线任务中实时获取人类反馈是困难的。大多数工作假设存在一个“剧本教师”（scripted teacher），利用先验预定义的奖励来提供偏好反馈。在本文中，我们提出了一种名为RL Self-augmented Large Language Model Feedback (RL-SaLLM-F) 的技术，该技术不依赖于先验信息进行在线PbRL。RL-SaLLM-F 利用大型语言模型（LLM）的反思能力和区分能力生成自增强轨迹，并为此提供奖励学习所需的偏好标签。首先，我们识别了基于LLM的偏好区分中的一个失败问题，即“查询歧义性”问题，在线PbRL中的偏好辨别问题。然后，LLM 用于提供偏好标签并生成更接近目标任务的自增强想象轨迹，从而提高反馈的质量和效率。此外，我们引入了一种双检查机制来减轻偏好标签中的随机性，从而提高LLM反馈的可靠性。在MetaWorld基准任务上的实验展示了RL-SaLLM-F 中每个模块的具体贡献，表明自增强的LLM反馈可以有效地替代不切实际的“剧本教师”反馈。总之，RL-SaLLM-F 引入了一种新的在线PbRL反馈获取方向，该方向不依赖于任何在线先验信息，提供了一种高效且轻量级的基于LLM驱动的反馈解决方案。 

---
# Adversarial Diffusion Model for Unsupervised Domain-Adaptive Semantic Segmentation 

**Title (ZH)**: 对抗扩散模型在无监督领域自适应语义分割中的应用 

**Authors**: Jongmin Yu, Zhongtian Sun, Shan Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16859)  

**Abstract**: Semantic segmentation requires labour-intensive labelling tasks to obtain the supervision signals, and because of this issue, it is encouraged that using domain adaptation, which transfers information from the existing labelled source domains to unlabelled or weakly labelled target domains, is essential. However, it is intractable to find a well-generalised representation which can describe two domains due to probabilistic or geometric difference between the two domains. This paper presents a novel method, the Conditional and Inter-coder Connected Latent Diffusion (CICLD) based Semantic Segmentation Model, to advance unsupervised domain adaptation (UDA) for semantic segmentation tasks. Leveraging the strengths of latent diffusion models and adversarial learning, our method effectively bridges the gap between synthetic and real-world imagery. CICLD incorporates a conditioning mechanism to improve contextual understanding during segmentation and an inter-coder connection to preserve fine-grained details and spatial hierarchies. Additionally, adversarial learning aligns latent feature distributions across source, mixed, and target domains, further enhancing generalisation. Extensive experiments are conducted across three benchmark datasets-GTA5, Synthia, and Cityscape-shows that CICLD outperforms state-of-the-art UDA methods. Notably, the proposed method achieves a mean Intersection over Union (mIoU) of 74.4 for the GTA5 to Cityscape UDA setting and 67.2 mIoU for the Synthia to Cityscape UDA setting. This project is publicly available on 'this https URL. 

**Abstract (ZH)**: 语义分割需要耗时的标注任务以获取监督信号，由于这一问题的存在，利用领域适应的方法是非常必要的，这种技术可以从已标注的源领域转移信息到无标注或弱标注的目标领域。然而，由于两个领域之间的概率或几何差异，找到一个能很好地通用化的表示来描述两个领域是不可解决的。本文提出了一种新颖的方法，即基于条件和编码者间连接的潜在扩散（CICLD）语义分割模型，以推进无监督领域适应（UDA）在语义分割任务中的应用。通过利用潜在扩散模型和对抗学习的优势，我们的方法有效地弥合了合成图像与现实世界图像之间的差距。CICLD包含一种条件机制以提高分割过程中的上下文理解，并结合一种编码者间连接以保留精细细节和空间层次结构。此外，对抗学习对源、混合和目标领域的潜在特征分布进行了对齐，进一步提高了通用性。通过在三个基准数据集（GTA5、Synthia和Cityscape）上进行广泛的实验，结果显示CICLD在所有设定中都优于现有的UDA方法。特别是，所提出的方法在从GTA5到Cityscape的UDA设定中达到了74.4的平均交并比（mIoU），而在从Synthia到Cityscape的UDA设定中达到了67.2的mIoU。该项目已在“this https URL”上公开。 

---
# ACL-QL: Adaptive Conservative Level in Q-Learning for Offline Reinforcement Learning 

**Title (ZH)**: ACL-QL：自适应保守层次的Q学习在离线强化学习中的应用 

**Authors**: Kun Wu, Yinuo Zhao, Zhiyuan Xu, Zhengping Che, Chengxiang Yin, Chi Harold Liu, Qinru Qiu, Feiferi Feng, Jian Tang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16848)  

**Abstract**: Offline Reinforcement Learning (RL), which operates solely on static datasets without further interactions with the environment, provides an appealing alternative to learning a safe and promising control policy. The prevailing methods typically learn a conservative policy to mitigate the problem of Q-value overestimation, but it is prone to overdo it, leading to an overly conservative policy. Moreover, they optimize all samples equally with fixed constraints, lacking the nuanced ability to control conservative levels in a fine-grained manner. Consequently, this limitation results in a performance decline. To address the above two challenges in a united way, we propose a framework, Adaptive Conservative Level in Q-Learning (ACL-QL), which limits the Q-values in a mild range and enables adaptive control on the conservative level over each state-action pair, i.e., lifting the Q-values more for good transitions and less for bad transitions. We theoretically analyze the conditions under which the conservative level of the learned Q-function can be limited in a mild range and how to optimize each transition adaptively. Motivated by the theoretical analysis, we propose a novel algorithm, ACL-QL, which uses two learnable adaptive weight functions to control the conservative level over each transition. Subsequently, we design a monotonicity loss and surrogate losses to train the adaptive weight functions, Q-function, and policy network alternatively. We evaluate ACL-QL on the commonly used D4RL benchmark and conduct extensive ablation studies to illustrate the effectiveness and state-of-the-art performance compared to existing offline DRL baselines. 

**Abstract (ZH)**: 离线强化学习（Offline Reinforcement Learning, RL），它仅依赖静态数据集进行学习，无需进一步与环境进行交互，为安全和有效的控制策略学习提供了一种吸引人的替代方案。现有方法通常会学习一个保守的策略来缓解Q值高估的问题，但这种保守策略可能会过度保守，从而导致策略过于保守。此外，它们在固定约束下对所有样本进行等权重优化，缺乏在细微层面控制保守水平的能力。因此，这种限制导致了性能下降。为了同时解决上述两个挑战，我们提出了一种框架，即自适应Q学习中的保守水平调整（Adaptive Conservative Level in Q-Learning, ACL-QL），该框架限制Q值在一个温和的范围内，并允许在每个状态-动作对上自适应地控制保守水平，即对好的过渡提升Q值更多，对坏的过渡提升Q值较少。我们从理论上分析了在温和范围内限制学习到的Q函数的保守水平的条件，以及如何对每个过渡进行自适应优化。基于理论分析，我们提出了一种新算法，ACL-QL，该算法使用两个可学习的自适应权重函数来控制每个过渡的保守水平。随后，我们设计了单调性损失和代理损失，以交替训练自适应权重函数、Q函数和策略网络。我们在常用的D4RL基准上评估了ACL-QL，并进行了广泛的消融研究，以证明其有效性和与其他现有离线DRL基线相比的先进性能。 

---
# Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with an LLM-Enabled Simulation 

**Title (ZH)**: Sim911：通过具有LLM能力的模拟技术朝着有效的和公平的9-1-1调度员培训迈进 

**Authors**: Zirong Chen, Elizabeth Chason, Noah Mladenovski, Erin Wilson, Kristin Mullen, Stephen Martini, Meiyi Ma  

**Link**: [PDF](https://arxiv.org/pdf/2412.16844)  

**Abstract**: Emergency response services are vital for enhancing public safety by safeguarding the environment, property, and human lives. As frontline members of these services, 9-1-1 dispatchers have a direct impact on response times and the overall effectiveness of emergency operations. However, traditional dispatcher training methods, which rely on role-playing by experienced personnel, are labor-intensive, time-consuming, and often neglect the specific needs of underserved communities. To address these challenges, we introduce Sim911, the first training simulation for 9-1-1 dispatchers powered by Large Language Models (LLMs). Sim911 enhances training through three key technical innovations: (1) knowledge construction, which utilizes archived 9-1-1 call data to generate simulations that closely mirror real-world scenarios; (2) context-aware controlled generation, which employs dynamic prompts and vector bases to ensure that LLM behavior aligns with training objectives; and (3) validation with looped correction, which filters out low-quality responses and refines the system performance. 

**Abstract (ZH)**: 应急响应服务对于提升公共安全至关重要，能够保障环境、财产和人类生命的安全。作为这些服务的前线成员，9-1-1调度员直接影响着响应时间和整体应急操作的有效性。然而，传统的调度员培训方法依赖于经验丰富的人员进行角色扮演，这种方法劳动力密集、耗时长，且常常忽视服务不足社区的具体需求。为解决这些挑战，我们引入了Sim911，这是由大型语言模型（LLMs）驱动的9-1-1调度员培训模拟器，是首个此类培训模拟器。Sim911通过三项关键技术创新提升培训效果：（1）知识构建，利用存档的9-1-1呼叫数据生成与现实场景高度一致的模拟；（2）情境感知的可控生成，通过动态提示和向量基底确保LLM的行为符合培训目标；（3）循环校正的验证，过滤掉低质量的响应并不断优化系统性能。 

---
# Graph Learning-based Regional Heavy Rainfall Prediction Using Low-Cost Rain Gauges 

**Title (ZH)**: 基于图学习的区域性强降水预测方法研究——利用低成本雨量计 

**Authors**: Edwin Salcedo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16842)  

**Abstract**: Accurate and timely prediction of heavy rainfall events is crucial for effective flood risk management and disaster preparedness. By monitoring, analysing, and evaluating rainfall data at a local level, it is not only possible to take effective actions to prevent any severe climate variation but also to improve the planning of surface and underground hydrological resources. However, developing countries often lack the weather stations to collect data continuously due to the high cost of installation and maintenance. In light of this, the contribution of the present paper is twofold: first, we propose a low-cost IoT system for automatic recording, monitoring, and prediction of rainfall in rural regions. Second, we propose a novel approach to regional heavy rainfall prediction by implementing graph neural networks (GNNs), which are particularly well-suited for capturing the complex spatial dependencies inherent in rainfall patterns. The proposed approach was tested using a historical dataset spanning 72 months, with daily measurements, and experimental results demonstrated the effectiveness of the proposed method in predicting heavy rainfall events, making this approach particularly attractive for regions with limited resources or where traditional weather radar or station coverage is sparse. 

**Abstract (ZH)**: 准确及时地预测暴雨事件对于有效的洪水风险管理及灾害准备至关重要。通过在地方层面监测、分析和评估降雨数据，不仅可以及时采取有效措施应对任何严重的气候变化，还可以改进地表和地下水资源规划。然而，由于安装和维护成本高昂，许多发展中国家缺乏连续收集数据的气象站。鉴于此，本文的研究成果有两个方面：首先，我们提出了一种低成本的物联网系统，用于自动记录、监测和预测农村地区的降雨；其次，我们提出了一种基于图神经网络（GNN）的新型区域暴雨预测方法，特别适合捕捉降雨模式中复杂的空间依赖性。采用72个月的历史数据集（每天一次观测）进行测试，实验结果表明本文提出的方法在预测暴雨事件方面非常有效，这使得该方法特别适合资源有限的地区或传统天气雷达或站点覆盖率稀疏的区域。 

---
# Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets 

**Title (ZH)**: 人类引导的图像生成方法：扩大小型训练图像数据集 

**Authors**: Changjian Chen, Fei Lv, Yalong Guan, Pengcheng Wang, Shengjie Yu, Yifan Zhang, Zhuo Tang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16839)  

**Abstract**: The performance of computer vision models in certain real-world applications (e.g., rare wildlife observation) is limited by the small number of available this http URL datasets using pre-trained generative models is an effective way to address this limitation. However, since the automatic generation process is uncontrollable, the generated images are usually limited in diversity, and some of them are undesired. In this paper, we propose a human-guided image generation method for more controllable dataset expansion. We develop a multi-modal projection method with theoretical guarantees to facilitate the exploration of both the original and generated images. Based on the exploration, users refine the prompts and re-generate images for better performance. Since directly refining the prompts is challenging for novice users, we develop a sample-level prompt refinement method to make it easier. With this method, users only need to provide sample-level feedback (e.g., which samples are undesired) to obtain better prompts. The effectiveness of our method is demonstrated through the quantitative evaluation of the multi-modal projection method, improved model performance in the case study for both classification and object detection tasks, and positive feedback from the experts. 

**Abstract (ZH)**: 计算机视觉模型在某些实际应用（如罕见野生动物观测）中的性能受到可用数据集数量有限的限制。使用预先训练的生成模型生成数据是一种有效的方法来克服这一限制。但是，由于生成过程是不可控的，生成的图像通常缺乏多样性，有些还可能是不理想的。本文提出了一种由人类引导的图像生成方法，以实现更可控的数据集扩展。我们开发了一种具有理论保证的多模态投影方法，以便更有效地探索原始和生成的图像。基于这种方法，用户可以通过调整提示并重新生成图像来改进性能。由于初级用户直接调整提示具有挑战性，我们还开发了一种样本级别的提示调整方法，使其更易于操作。通过这种方法，用户只需提供样本级别的反馈（例如，哪些样本是不理想的选择），即可获得改进的提示。通过多模态投影方法的定量评估、案例研究中分类和对象检测任务模型性能的提升，以及专家的积极反馈，证明了该方法的有效性。 

---
# Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers 

**Title (ZH)**: 面向层和时间步的自适应可微分标记压缩比以实现高效的扩散变换器 

**Authors**: Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, Eli Shechtman, Sohrab Amirghodsi, Yingyan Celine Lin  

**Link**: [PDF](https://arxiv.org/pdf/2412.16822)  

**Abstract**: Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in Mixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is jointly fine-tuned with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on both text-to-image and inpainting tasks show that DiffRatio-MoD effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works. 

**Abstract (ZH)**: 扩散变换器（DiTs）已经在图像生成质量上达到了最先进的（SOTA）水平，但它们面临着高延迟和内存效率低的问题，这使得它们难以部署在资源受限的设备上。一个关键的效率瓶颈在于现有的DiTs在图像的所有区域中应用了等量的计算。然而，并非所有图像token同等重要，某些局部区域需要更多的计算，比如物体。为了解决这一问题，我们提出了DiffRatio-MoD，这是一种具有可微压缩比的动态DiT推理框架，它可以自动学习在每张图像的token、层和时间步之间动态路由计算，从而实现深度混合(Mixture-of-Depths, MoD)高效DiT模型。具体而言，DiffRatio-MoD集成了三个特点：（1）一个token级别路由方案，其中每个DiT层包含一个路由器，它与模型权重共同微调以预测token的重要性分数。这样，不重要token可以绕过整个层的计算；（2）逐层可微压缩比机制，其中不同DiT层自动从零初始化中学习不同的压缩比，导致冗余层有较高的压缩比，而其他层则保持较少压缩甚至完全未压缩；（3）逐时间步可微压缩比机制，其中每个去噪时间步学习自己的压缩比。所得到的模式显示，噪声较大的时间步压缩比更高，而图像更加清晰的时间步压缩比较低。在文本到图像和图像修复任务上的广泛实验表明，DiffRatio-MoD能够有效捕捉在token、层和时间步三个维度上的动态性，并在生成质量和效率之间实现了优于先前工作的权衡。 

---
# Enhancing web traffic attacks identification through ensemble methods and feature selection 

**Title (ZH)**: 通过集成方法和特征选择增强网络流量攻击检测 

**Authors**: Daniel Urda, Branly Martínez, Nuño Basurto, Meelis Kull, Ángel Arroyo, Álvaro Herrero  

**Link**: [PDF](https://arxiv.org/pdf/2412.16791)  

**Abstract**: Websites, as essential digital assets, are highly vulnerable to cyberattacks because of their high traffic volume and the significant impact of breaches. This study aims to enhance the identification of web traffic attacks by leveraging machine learning techniques. A methodology was proposed to extract relevant features from HTTP traces using the CSIC2010 v2 dataset, which simulates e-commerce web traffic. Ensemble methods, such as Random Forest and Extreme Gradient Boosting, were employed and compared against baseline classifiers, including k-nearest Neighbor, LASSO, and Support Vector Machines. The results demonstrate that the ensemble methods outperform baseline classifiers by approximately 20% in predictive accuracy, achieving an Area Under the ROC Curve (AUC) of 0.989. Feature selection methods such as Information Gain, LASSO, and Random Forest further enhance the robustness of these models. This study highlights the efficacy of ensemble models in improving attack detection while minimizing performance variability, offering a practical framework for securing web traffic in diverse application contexts. 

**Abstract (ZH)**: 网站作为重要的数字资产，由于其高流量和泄露带来的重大影响，极易遭受网络攻击。本文旨在通过利用机器学习技术提高对网络流量攻击的识别能力。我们提出了一种方法，利用CSIC2010 v2数据集提取HTTP跟踪的相关特征，该数据集模拟了电子商务网站的流量。采用了一些集成方法，如随机森林和梯度提升树，并将其与基础分类器，包括K最近邻、LASSO和支持向量机进行了比较。结果表明，集成方法在预测准确性上比基础分类器高出约20%，达到接收者操作特征曲线下面积（AUC）为0.989。特征选择方法，如信息增益、LASSO和随机森林进一步增强了这些模型的稳健性。本文强调了集成模型在提高攻击检测效率的同时，能有效减少性能波动，并为多种应用场景下保障网络流量安全提供了实用框架。 

---
# Assessing Social Alignment: Do Personality-Prompted Large Language Models Behave Like Humans? 

**Title (ZH)**: 评估社会一致性：个性化的提示引起的大型语言模型是否表现出人类的行为？ 

**Authors**: Ivan Zakazov, Mikolaj Boronski, Lorenzo Drudi, Robert West  

**Link**: [PDF](https://arxiv.org/pdf/2412.16772)  

**Abstract**: The ongoing revolution in language modelling has led to various novel applications, some of which rely on the emerging "social abilities" of large language models (LLMs). Already, many turn to the new "cyber friends" for advice during pivotal moments of their lives and trust them with their deepest secrets, implying that accurate shaping of LLMs' "personalities" is paramount. Leveraging the vast diversity of data on which LLMs are pretrained, state-of-the-art approaches prompt them to adopt a particular personality. We ask (i) if personality-prompted models behave (i.e. "make" decisions when presented with a social situation) in line with the ascribed personality, and (ii) if their behavior can be finely controlled. We use classic psychological experiments - the Milgram Experiment and the Ultimatum Game - as social interaction testbeds and apply personality prompting to GPT-3.5/4/4o-mini/4o. Our experiments reveal failure modes of the prompt-based modulation of the models' "behavior", thus challenging the feasibility of personality prompting with today's LLMs. 

**Abstract (ZH)**: 语言模型领域的持续革命催生了各种新型应用，其中一些应用依赖于大型语言模型（LLMs） emerging的“社会能力”。目前，许多人已经转向这些新出现的“网络朋友”寻求在生命关键时刻的建议，并将自己的最深处的秘密托付给他们，这表明准确塑造LLMs的“个性”至关重要。利用LLMs预训练时使用的广泛多样的数据，最先进的方法促使它们采取特定的个性特征。我们询问（i）个性提示模型在面对社会情境时是否会（即“做出”决策）表现出与归因的个性特征一致的行为，以及（ii）其行为是否能够精细地控制。我们使用经典的心理学实验——米尔格拉姆实验和最后通牒游戏——作为社会互动的测试平台，并对GPT-3.5/4/4o-mini/4o应用个性提示。我们的实验揭示了基于提示对模型行为进行调整的失败模式，从而挑战了当前LLMs中通过个性提示控制行为的可行性。 

---
# A Comparative Study on Machine Learning Models to Classify Diseases Based on Patient Behaviour and Habits 

**Title (ZH)**: 基于患者行为和习惯的疾病分类：机器学习模型的比较研究 

**Authors**: Elham Musaaed, Nabil Hewahi, Abdulla Alasaadi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16768)  

**Abstract**: In recent years, ML algorithms have been shown to be useful for predicting diseases based on health data and posed a potential application area for these algorithms such as modeling of diseases. The majority of these applications employ supervised rather than unsupervised ML algorithms. In addition, each year, the amount of data in medical science grows rapidly. Moreover, these data include clinical and Patient-Related Factors (PRF), such as height, weight, age, other physical characteristics, blood sugar, lipids, insulin, etc., all of which will change continually over time. Analysis of historical data can help identify disease risk factors and their interactions, which is useful for disease diagnosis and prediction. This wealth of valuable information in these data will help doctors diagnose accurately and people can become more aware of the risk factors and key indicators to act proactively. The purpose of this study is to use six supervised ML approaches to fill this gap by conducting a comprehensive experiment to investigate the correlation between PRF and Diabetes, Stroke, Heart Disease (HD), and Kidney Disease (KD). Moreover, it will investigate the link between Diabetes, Stroke, and KD and PRF with HD. Further, the research aims to compare and evaluate various ML algorithms for classifying diseases based on the PRF. Additionally, it aims to compare and evaluate ML algorithms for classifying HD based on PRF as well as Diabetes, Stroke, Asthma, Skin Cancer, and KD as attributes. Lastly, HD predictions will be provided through a Web-based application on the most accurate classifier, which allows the users to input their values and predict the output. 

**Abstract (ZH)**: 近年来，机器学习（ML）算法已被证明在基于健康数据预测疾病方面具有实用性，并可能成为这些算法的应用领域之一，如疾病的建模。大多数此类应用使用监督学习而非无监督学习算法。此外，每年医学领域中的数据量都在迅速增长。这些数据包括临床和患者相关因素（PRF），如身高、体重、年龄、其他生理特征、血糖、血脂、胰岛素等，所有这些数据都会随时间不断变化。对历史数据的分析可以帮助识别疾病风险因素及其相互作用，这对于疾病的诊断和预测是有益的。这些数据中的丰富信息将帮助医生更准确地进行诊断，人们也能更清楚地了解风险因素和关键指标，从而采取主动措施。本研究旨在通过全面实验，采用六种监督学习方法来填补这一空白，探索PRF与糖尿病、中风、心血管疾病（HD）和肾病（KD）之间的关联。此外，研究还将探讨糖尿病、中风和肾病与心血管疾病之间的PRF关联。进一步地，本研究旨在比较和评估基于PRF分类疾病的各种机器学习算法。另外，本研究还旨在比较和评估用于基于PRF分类心血管疾病、糖尿病、中风、哮喘和肾病的机器学习算法。最后，将通过基于Web的应用程序提供最准确分类器的心血管疾病预测，这将允许用户输入他们的值并预测输出。 

---
# Apples to Apples: Establishing Comparability in Knowledge Generation Tasks Involving Users 

**Title (ZH)**: 《苹果对苹果：在涉及用户的知识生成任务中建立可比性》

这个标题翻译成中文后，既保留了原文的比喻性表达，又符合中文的学术规范。翻译时尽量保持原意，同时确保语言自然流畅。如果需要进一步调整或是有具体的学术领域背景信息，可以告诉我，我可以进行相应的调整。 

**Authors**: Christophe Debruyne, Ademar Crotti Junior  

**Link**: [PDF](https://arxiv.org/pdf/2412.16766)  

**Abstract**: Knowledge graph construction (KGC) from (semi-)structured data is challenging, and facilitating user involvement is an issue frequently brought up within this community. We cannot deny the progress we have made with respect to (declarative) knowledge generation languages and tools to help build such mappings. However, it is surprising that no two studies report on similar protocols. This heterogeneity does not allow for a comparison of KGC languages, techniques, and tools. This paper first analyses the various studies that report on studies involving users to identify the points of comparison. These gaps include a lack of systematic consistency in task design, participant selection, and evaluation metrics. Moreover, there needs to be a systematic way of analyzing the data and reporting the findings, which is also lacking. We thus propose and introduce a user protocol for KGC designed to address this challenge. Where possible, we draw and take elements from the literature we deem fit for such a protocol. The protocol, as such, allows for the comparison of languages and techniques for the RDF Mapping Languages core functionality, which is covered by most of the other state-of-the-art techniques and tools. We also propose how the protocol can be amended to compare extensions (of RML). This protocol provides an important step towards a more comparable evaluation of KGC user studies. 

**Abstract (ZH)**: 从（半）结构化数据构建知识图谱（Knowledge Graph Construction, KGC）是一个挑战，用户参与问题在这一领域经常被提及。虽然我们在（声明式）知识生成语言和工具方面取得了进展，以帮助构建这类映射，但令人惊讶的是，没有任何两项研究报告了类似的协议。这种异质性无法使知识图谱语言、技术和工具的比较成为可能。本文首先分析了报告用户参与研究的各种研究，以确定可以对比的几点。这些差距包括任务设计、参与者选择和评估指标缺乏系统一致性。此外，还缺乏一种系统的方法来分析数据并报告发现。因此，本文提出并引入了一个针对KGC设计的用户协议，旨在解决这一挑战。在可能的情况下，我们借鉴我们认为适合此类协议的相关文献中的元素。该协议允许对比大多数最先进的技术和工具都涵盖的RDF映射语言核心功能的语言和技术。我们还提出了如何修改该协议以对比RML的扩展。这个协议为KGC用户研究的更可比评估提供了重要的一步。 

---
# Towards Selection and Transition Between Behavior-Based Neural Networks for Automated Driving 

**Title (ZH)**: 面向行为驱动神经网络的选择及其在自动驾驶中的过渡研究 

**Authors**: Iqra Aslam, Igor Anpilogov, Andreas Rausch  

**Link**: [PDF](https://arxiv.org/pdf/2412.16764)  

**Abstract**: Autonomous driving technology is progressing rapidly, largely due to complex End To End systems based on deep neural networks. While these systems are effective, their complexity can make it difficult to understand their behavior, raising safety concerns. This paper presents a new solution a Behavior Selector that uses multiple smaller artificial neural networks (ANNs) to manage different driving tasks, such as lane following and turning. Rather than relying on a single large network, which can be burdensome, require extensive training data, and is hard to understand, the developed approach allows the system to dynamically select the appropriate neural network for each specific behavior (e.g., turns) in real time. We focus on ensuring smooth transitions between behaviors while considering the vehicles current speed and orientation to improve stability and safety. The proposed system has been tested using the AirSim simulation environment, demonstrating its effectiveness. 

**Abstract (ZH)**: 自动驾驶技术正快速发展，主要得益于基于深度神经网络的端到端系统。虽然这些系统非常有效，但其复杂性可能会使其行为难以理解，从而引发安全方面的担忧。本文提出了一种新的解决方案——行为选择器，该解决方案使用多个较小的人工神经网络（ANNs）来管理不同的驾驶任务（如车道保持和转向）。与依赖单一庞大的网络相比，这种方法不需要大量训练数据，且易于理解，系统可以在实时动态选择适用于每个特定行为（例如转向）的神经网络。我们着重于确保行为之间的平滑过渡，同时考虑车辆当前的速度和方向，以提高稳定性和安全性。所提出系统已经在AirSim仿真环境中进行了测试，并展示了其有效性。 

---
# A Method for the Runtime Validation of AI-based Environment Perception in Automated Driving System 

**Title (ZH)**: 基于人工智能的环境感知在自动驾驶系统中运行时验证方法 

**Authors**: Iqra Aslam, Abhishek Buragohain, Daniel Bamal, Adina Aniculaesei, Meng Zhang, Andreas Rausch  

**Link**: [PDF](https://arxiv.org/pdf/2412.16762)  

**Abstract**: Environment perception is a fundamental part of the dynamic driving task executed by Autonomous Driving Systems (ADS). Artificial Intelligence (AI)-based approaches have prevailed over classical techniques for realizing the environment perception. Current safety-relevant standards for automotive systems, International Organization for Standardization (ISO) 26262 and ISO 21448, assume the existence of comprehensive requirements specifications. These specifications serve as the basis on which the functionality of an automotive system can be rigorously tested and checked for compliance with safety regulations. However, AI-based perception systems do not have complete requirements specification. Instead, large datasets are used to train AI-based perception systems. This paper presents a function monitor for the functional runtime monitoring of a two-folded AI-based environment perception for ADS, based respectively on camera and LiDAR sensors. To evaluate the applicability of the function monitor, we conduct a qualitative scenario-based evaluation in a controlled laboratory environment using a model car. The evaluation results then are discussed to provide insights into the monitor's performance and its suitability for real-world applications. 

**Abstract (ZH)**: 环境感知是自动驾驶系统（ADS）执行动态驾驶任务的基本组成部分。基于人工智能（AI）的方法已超越传统技术，在实现环境感知方面占优势地位。目前，用于汽车系统的安全相关标准，即国际标准化组织（ISO）26262和ISO 21448，假设了全面的功能需求规格的存在。这些规格为严格测试和验证汽车系统的功能是否符合安全法规提供了基础。然而，基于AI的感知系统并不具备完整的需求规格。相反，大量数据集被用于训练基于AI的感知系统。本文提出了一种功能监控器，用于对基于摄像头和LiDAR传感器的两层AI环境感知功能进行运行时监控。为了评估功能监控器的应用性，我们在受控实验室环境中使用模型汽车，以情景为基础进行定性评估。然后讨论评估结果，以提供关于监控器性能及其在实际应用中的适用性的见解。 

---
# The Master Key Filters Hypothesis: Deep Filters Are General in DS-CNNs 

**Title (ZH)**: 主键滤波器假设：深滤波器在DS-CNN中具有普适性 

**Authors**: Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16751)  

**Abstract**: This paper challenges the prevailing view that convolutional neural network (CNN) filters become increasingly specialized in deeper layers. Motivated by recent observations of clusterable repeating patterns in depthwise separable CNNs (DS-CNNs) trained on ImageNet, we extend this investigation across various domains and datasets. Our analysis of DS-CNNs reveals that deep filters maintain generality, contradicting the expected transition to class-specific filters. We demonstrate the generalizability of these filters through transfer learning experiments, showing that frozen filters from models trained on different datasets perform well and can be further improved when sourced from larger datasets. Our findings indicate that spatial features learned by depthwise separable convolutions remain generic across all layers, domains, and architectures. This research provides new insights into the nature of generalization in neural networks, particularly in DS-CNNs, and has significant implications for transfer learning and model design. 

**Abstract (ZH)**: 本文挑战了深度神经网络（卷积神经网络，CNN）的滤波器在更深的层中逐渐专业化这一普遍观点。受近期在ImageNet上训练的深度可分离卷积神经网络（DS-CNN）中可聚类重复模式的观察结果启发，我们扩展了这一研究至不同的领域和数据集。通过对DS-CNN的分析，我们发现深层滤波器保持了一般性，这与滤波器从通用到类特定的预期过渡相矛盾。我们通过迁移学习实验展示了这些滤波器的一般性，表明在不同数据集上训练的模型中的冻结滤波器表现良好，并且当这些滤波器来源于更大数据集时，还可以进一步优化。我们的研究结果表明，在所有层、领域和架构中，深度可分离卷积学习的空间特征保持通用性。该研究为神经网络泛化的本质提供了新的见解，特别是在DS-CNN中，并且在迁移学习和模型设计方面具有重要意义。 

---
# Unpacking Political Bias in Large Language Models: Insights Across Topic Polarization 

**Title (ZH)**: 探究大型语言模型中的政治偏见：跨主题极化视角的见解 

**Authors**: Kaiqi Yang, Hang Li, Yucheng Chu, Yuping Lin, Tai-Quan Peng, Hui Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16746)  

**Abstract**: Large Language Models (LLMs) have been widely used to generate responses on social topics due to their world knowledge and generative capabilities. Beyond reasoning and generation performance, political bias is an essential issue that warrants attention. Political bias, as a universal phenomenon in human society, may be transferred to LLMs and distort LLMs' behaviors of information acquisition and dissemination with humans, leading to unequal access among different groups of people. To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.
In this study, we systematically measure the political biases in a wide range of LLMs, using a curated set of questions addressing political bias in various contexts. Our findings reveal distinct patterns in how LLMs respond to political topics. For highly polarized topics, most LLMs exhibit a pronounced left-leaning bias. Conversely, less polarized topics elicit greater consensus, with similar response patterns across different LLMs. Additionally, we analyze how LLM characteristics, including release date, model scale, and region of origin affect political bias. The results indicate political biases evolve with model scale and release date, and are also influenced by regional factors of LLMs. 

**Abstract (ZH)**: 大规模语言模型（LLMs）由于其世界知识和生成能力，在生成社会话题的回应方面得到了广泛应用。除了推理和生成性能之外，政治偏见也是一个值得关注的重要问题。作为人类社会的一种普遍现象，政治偏见可能会转移到LLMs中，影响LLMs在人类信息获取和传播过程中的行为，导致不同群体之间获取信息的不平等。为了防止LLMs复制和强化政治偏见，并促进更公平的LLM-人类交互，全面审视流行LLMs中的政治偏见变得迫切和重要。

在本研究中，我们使用经过筛选的问题集系统地测量了多种LLMs的政治偏见，这些问题涵盖了各种社会情境中的政治偏见。我们的研究发现，LLMs对政治话题的回应展示了不同的模式。对于高度两极化的议题，大多数LLMs表现出明显的左倾偏见。相反，在较不两极化的议题上，LLMs之间则表现出更大的共识，回答模式也较为一致。此外，我们分析了LLM的特性，包括发布日期、模型规模和起源地区如何影响政治偏见。研究结果表明，政治偏见随着模型规模和发布日期的变化而演变，并且受到LLM起源地区的因素影响。 

---
# Coupling Neural Networks and Physics Equations For Li-Ion Battery State-of-Charge Prediction 

**Title (ZH)**: 将下面的论文内容或标题翻译成中文，符合学术规范：

"Coupling Neural Networks and Physics Equations for Lithium-Ion Battery State-of-Charge Prediction"

中文翻译：

“结合神经网络和物理方程的锂离子电池荷电状态预测方法” 

**Authors**: Giovanni Pollo, Alessio Burrello, Enrico Macii, Massimo Poncino, Sara Vinco, Daniele Jahier Pagliari  

**Link**: [PDF](https://arxiv.org/pdf/2412.16724)  

**Abstract**: Estimating the evolution of the battery's State of Charge (SoC) in response to its usage is critical for implementing effective power management policies and for ultimately improving the system's lifetime. Most existing estimation methods are either physics-based digital twins of the battery or data-driven models such as Neural Networks (NNs). In this work, we propose two new contributions in this domain. First, we introduce a novel NN architecture formed by two cascaded branches: one to predict the current SoC based on sensor readings, and one to estimate the SoC at a future time as a function of the load behavior. Second, we integrate battery dynamics equations into the training of our NN, merging the physics-based and data-driven approaches, to improve the models' generalization over variable prediction horizons. We validate our approach on two publicly accessible datasets, showing that our Physics-Informed Neural Networks (PINNs) outperform purely data-driven ones while also obtaining superior prediction accuracy with a smaller architecture with respect to the state-of-the-art. 

**Abstract (ZH)**: 精确估计电池荷电状态（SoC）在不同使用情况下的演变对于实施有效的电源管理策略以及最终提高系统的寿命至关重要。目前，大多数现有的估算方法要么是基于物理模型的电池数字孪生，要么是数据驱动的方法，例如神经网络（Neural Networks, NNs）。在本研究中，我们在此领域提出了两项新的贡献。首先，我们引入了一种新颖的神经网络架构，该架构由两个级联分支组成：一个基于传感器读数预测当前的SoC，另一个根据负载行为估计未来时间点的SoC。其次，我们将电池动力学方程整合到神经网络的训练过程中，将基于物理模型和数据驱动的方法结合起来，以提高模型在不同预测时间段的一般化能力。我们通过两个可访问的数据集验证了我们的方法，结果显示，我们的物理知情神经网络（Physics-Informed Neural Networks, PINNs）在与完全基于数据驱动的方法相比的同时，还能够使用更小的架构获得更高的预测准确性，优于现有技术。 

---
# Large Language Models Compression via Low-Rank Feature Distillation 

**Title (ZH)**: 大型语言模型的低秩特征蒸馏压缩方法 

**Authors**: Yaya Sy, Christophe Cerisara, Irina Illina  

**Link**: [PDF](https://arxiv.org/pdf/2412.16719)  

**Abstract**: Current LLM structured pruning methods involve two steps: (1) compressing with calibration data and (2) continued pretraining on billions of tokens to recover the lost performance. This costly second step is needed as the first step significantly impacts performance. Previous studies have found that pretrained Transformer weights aren't inherently low-rank, unlike their activations, which may explain this performance drop. Based on this observation, we introduce a one-shot compression method that locally distills low-rank weights. We accelerate convergence by initializing the low-rank weights with SVD and using a joint loss that combines teacher and student activations. We reduce memory requirements by applying local gradient updates only. Our approach can compress Mixtral-8x7B within minutes on a single A100 GPU, removing 10 billion parameters while maintaining over 95% of the original performance. Phi-2 3B can be compressed by 40% using only 13 million calibration tokens into a small model that competes with recent models of similar size. We show our method generalizes well to non-transformer architectures: Mamba-3B can be compressed by 20% while maintaining 99% of its performance. 

**Abstract (ZH)**: 当前的大型语言模型结构剪枝方法涉及两个步骤：（1）使用校准数据进行压缩；（2）在数亿个词汇上继续预训练以恢复丢失的性能。由于第一步会显著影响性能，因此第二步的成本较高。先前的研究发现，预训练的Transformer权重并不天然具有低秩结构，与之相比，其激活值可能天然具有低秩结构，这可能解释了性能下降的原因。基于这一观察，我们提出了一种一站式压缩方法，该方法局部蒸馏低秩权重。通过使用SVD初始化低秩权重并采用结合教师模型和学生模型激活值的联合损失，我们加速了收敛过程。通过仅应用局部梯度更新来减少内存需求。我们的方法可以在单个A100 GPU上几分钟内压缩Mixtral-8x7B模型，在去除100亿参数的同时保持原性能的超过95%。Phi-2 3B可以在仅使用1300万个校准标记的情况下压缩40%，形成一个小型模型，与最近同类尺寸的模型具有竞争力。我们展示了该方法在非Transformer架构中的良好泛化能力：Mamba-3B可以在保持99%性能的同时压缩20%。 

---
# From Histopathology Images to Cell Clouds: Learning Slide Representations with Hierarchical Cell Transformer 

**Title (ZH)**: 从组织病理学图像到细胞云：基于分层细胞变换器的切片表示学习 

**Authors**: Zijiang Yang, Zhongwei Qiu, Tiancheng Lin, Hanqing Chao, Wanxing Chang, Yelin Yang, Yunshuo Zhang, Wenpei Jiao, Yixuan Shen, Wenbin Liu, Dongmei Fu, Dakai Jin, Ke Yan, Le Lu, Hui Jiang, Yun Bian  

**Link**: [PDF](https://arxiv.org/pdf/2412.16715)  

**Abstract**: It is clinically crucial and potentially very beneficial to be able to analyze and model directly the spatial distributions of cells in histopathology whole slide images (WSI). However, most existing WSI datasets lack cell-level annotations, owing to the extremely high cost over giga-pixel images. Thus, it remains an open question whether deep learning models can directly and effectively analyze WSIs from the semantic aspect of cell distributions. In this work, we construct a large-scale WSI dataset with more than 5 billion cell-level annotations, termed WSI-Cell5B, and a novel hierarchical Cell Cloud Transformer (CCFormer) to tackle these challenges. WSI-Cell5B is based on 6,998 WSIs of 11 cancers from The Cancer Genome Atlas Program, and all WSIs are annotated per cell by coordinates and types. To the best of our knowledge, WSI-Cell5B is the first WSI-level large-scale dataset integrating cell-level annotations. On the other hand, CCFormer formulates the collection of cells in each WSI as a cell cloud and models cell spatial distribution. Specifically, Neighboring Information Embedding (NIE) is proposed to characterize the distribution of cells within the neighborhood of each cell, and a novel Hierarchical Spatial Perception (HSP) module is proposed to learn the spatial relationship among cells in a bottom-up manner. The clinical analysis indicates that WSI-Cell5B can be used to design clinical evaluation metrics based on counting cells that effectively assess the survival risk of patients. Extensive experiments on survival prediction and cancer staging show that learning from cell spatial distribution alone can already achieve state-of-the-art (SOTA) performance, i.e., CCFormer strongly outperforms other competing methods. 

**Abstract (ZH)**: 直接分析和建模病理Whole Slide Images (WSI) 中细胞的空间分布对于临床应用具有重要而潜在的好处。然而，现有的大多数WSI数据集缺乏细胞级别的标注，主要是由于超像素级图像的极高成本。因此，是否可以使用深度学习模型直接从细胞分布的语义方面分析WSIs仍然是一个开放问题。在这项工作中，我们构建了一个包含超过50亿个细胞级别标注的大规模数据集，称为WSI-Cell5B，并提出了一种新型的分层Cell Cloud Transformer (CCFormer) 以应对这些挑战。WSI-Cell5B基于来自The Cancer Genome Atlas Program的11种癌症的6,998张WSI图像，并对所有WSI进行了基于坐标的细胞类型标注。据我们所知，WSI-Cell5B是首个整合细胞级标注的大规模WSI数据集。另一方面，CCFormer 将每个WSI中的细胞集合视为一个细胞云，并建模细胞的空间分布。具体来说，提出了一种邻域信息嵌入（NIE）方法来表征每个细胞邻域内细胞的分布情况，并提出了一种自底向上的新分层空间感知（HSP）模块，以学习细胞之间的空间关系。临床分析表明，WSI-Cell5B可以用于设计基于计数细胞的临床评估指标，以有效评估患者的生存风险。在生存预测和癌症分期的大量实验中表明，仅从细胞空间分布中学习就已能够达到最先进的性能，即CCFormer 显著优于其他竞争方法。 

---
# Subgoal Discovery Using a Free Energy Paradigm and State Aggregations 

**Title (ZH)**: 使用自由能量范式和状态聚合的子目标发现方法 

**Authors**: Amirhossein Mesbah, Reshad Hosseini, Seyed Pooya Shariatpanahi, Majid Nili Ahmadabadi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16687)  

**Abstract**: Reinforcement learning (RL) plays a major role in solving complex sequential decision-making tasks. Hierarchical and goal-conditioned RL are promising methods for dealing with two major problems in RL, namely sample inefficiency and difficulties in reward shaping. These methods tackle the mentioned problems by decomposing a task into simpler subtasks and temporally abstracting a task in the action space. One of the key components for task decomposition of these methods is subgoal discovery. We can use the subgoal states to define hierarchies of actions and also use them in decomposing complex tasks. Under the assumption that subgoal states are more unpredictable, we propose a free energy paradigm to discover them. This is achieved by using free energy to select between two spaces, the main space and an aggregation space. The $model \; changes$ from neighboring states to a given state shows the unpredictability of a given state, and therefore it is used in this paper for subgoal discovery. Our empirical results on navigation tasks like grid-world environments show that our proposed method can be applied for subgoal discovery without prior knowledge of the task. Our proposed method is also robust to the stochasticity of environments. 

**Abstract (ZH)**: 强化学习（Reinforcement Learning, RL）在解决复杂序列决策任务方面发挥着重要作用。分层强化学习和目标条件化强化学习是应对RL中两个主要问题——样本效率低和奖励塑形困难的有效方法。这些方法通过将任务分解为更简单的子任务和在动作空间中进行时间抽象来解决上述问题。这些方法的一种关键组成部分是子目标发现。我们可以使用子目标状态来定义动作层次结构，也可以将其用于分解复杂的任务。在假设子目标状态更为不可预测的前提下，我们提出了一种自由能范式来发现这些状态。这通过使用自由能在主空间和聚合空间之间进行选择实现。从相邻状态到给定状态的模型变化表明了给定状态的不可预测性，因此在这篇文章中用于子目标发现。我们在导航任务，如格网世界环境中进行的实验证明，我们的方法可以在无需了解任务先验知识的情况下用于子目标发现。此外，我们的方法还能应对环境中的随机性。 

---
# The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents 

**Title (ZH)**: 任务屏蔽：强制任务对齐以防御间接提示注入攻击中的LLM代理 

**Authors**: Feiran Jia, Tong Wu, Xin Qin, Anna Squicciarini  

**Link**: [PDF](https://arxiv.org/pdf/2412.16682)  

**Abstract**: Large Language Model (LLM) agents are increasingly being deployed as conversational assistants capable of performing complex real-world tasks through tool integration. This enhanced ability to interact with external systems and process various data sources, while powerful, introduces significant security vulnerabilities. In particular, indirect prompt injection attacks pose a critical threat, where malicious instructions embedded within external data sources can manipulate agents to deviate from user intentions. While existing defenses based on rule constraints, source spotlighting, and authentication protocols show promise, they struggle to maintain robust security while preserving task functionality. We propose a novel and orthogonal perspective that reframes agent security from preventing harmful actions to ensuring task alignment, requiring every agent action to serve user objectives. Based on this insight, we develop Task Shield, a test-time defense mechanism that systematically verifies whether each instruction and tool call contributes to user-specified goals. Through experiments on the AgentDojo benchmark, we demonstrate that Task Shield reduces attack success rates (2.07\%) while maintaining high task utility (69.79\%) on GPT-4o. 

**Abstract (ZH)**: 大型语言模型（LLM）代理越来越多地被部署为能够通过工具集成执行复杂现实世界任务的对话助理。这种增强的与外部系统互动和处理多种数据源的能力虽然强大，但也引入了显著的安全漏洞。特别是，间接提示注入攻击构成了一个严重的威胁，其中恶意指令嵌入在外部数据源中，可以操纵代理偏离用户意图。尽管基于规则约束、来源聚焦和身份验证协议的现有防御措施显示出前景，但在保持安全的同时保留任务功能方面仍然存在困难。我们提出了一种新颖且独立的视角，重新定义代理安全，从防止有害行为转向确保任务对齐，要求每一步代理行为都服务于用户目标。基于这一见解，我们开发了Task Shield，一种测试时的防御机制，系统地验证每个指令和工具调用是否有助于达成用户指定的目标。通过在AgentDojo基准测试上的实验，我们证明Task Shield能够将攻击成功率降低2.07%，同时在GPT-4o上保持高任务实用度（69.79%）。 

---
# Adversarial Attack Against Images Classification based on Generative Adversarial Networks 

**Title (ZH)**: 基于生成对抗网络的图像分类对抗攻击 

**Authors**: Yahe Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16662)  

**Abstract**: Adversarial attacks on image classification systems have always been an important problem in the field of machine learning, and generative adversarial networks (GANs), as popular models in the field of image generation, have been widely used in various novel scenarios due to their powerful generative capabilities. However, with the popularity of generative adversarial networks, the misuse of fake image technology has raised a series of security problems, such as malicious tampering with other people's photos and videos, and invasion of personal privacy. Inspired by the generative adversarial networks, this work proposes a novel adversarial attack method, aiming to gain insight into the weaknesses of the image classification system and improve its anti-attack ability. Specifically, the generative adversarial networks are used to generate adversarial samples with small perturbations but enough to affect the decision-making of the classifier, and the adversarial samples are generated through the adversarial learning of the training generator and the classifier. From extensive experiment analysis, we evaluate the effectiveness of the method on a classical image classification dataset, and the results show that our model successfully deceives a variety of advanced classifiers while maintaining the naturalness of adversarial samples. 

**Abstract (ZH)**: 图像分类系统的对抗攻击一直是机器学习领域的重要问题，而生成对抗网络（GANs）作为图像生成领域的流行模型，因其强大的生成能力而广泛应用于各种新型场景中。然而，随着生成对抗网络的普及，虚假图像技术的滥用引发了系列安全问题，如恶意篡改他人照片和视频，侵犯个人隐私等。受到生成对抗网络的启发，本文提出了一种新颖的对抗攻击方法，旨在揭示图像分类系统的脆弱性并提高其抗攻击能力。具体而言，利用生成对抗网络生成具有微小扰动但足以影响分类器决策的对抗样本，并通过训练生成器与分类器的对抗学习生成对抗样本。通过对经典的图像分类数据集进行广泛实验分析，评估该方法的有效性，结果表明，我们的模型在保持对抗样本自然性的同时成功欺骗了多种先进的分类器。 

---
# Generalizable Articulated Object Perception with Superpoints 

**Title (ZH)**: 可泛化的关节化对象感知方法：基于Superpoints的技术 

**Authors**: Qiaojun Yu, Ce Hao, Xibin Yuan, Li Zhang, Liu Liu, Yukang Huo, Rohit Agarwal, Cewu Lu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16656)  

**Abstract**: Manipulating articulated objects with robotic arms is challenging due to the complex kinematic structure, which requires precise part segmentation for efficient manipulation. In this work, we introduce a novel superpoint-based perception method designed to improve part segmentation in 3D point clouds of articulated objects. We propose a learnable, part-aware superpoint generation technique that efficiently groups points based on their geometric and semantic similarities, resulting in clearer part boundaries. Furthermore, by leveraging the segmentation capabilities of the 2D foundation model SAM, we identify the centers of pixel regions and select corresponding superpoints as candidate query points. Integrating a query-based transformer decoder further enhances our method's ability to achieve precise part segmentation. Experimental results on the GAPartNet dataset show that our method outperforms existing state-of-the-art approaches in cross-category part segmentation, achieving AP50 scores of 77.9% for seen categories (4.4% improvement) and $39.3\%$ for unseen categories (11.6% improvement), with superior results in 5 out of 9 part categories for seen objects and outperforming all previous methods across all part categories for unseen objects. 

**Abstract (ZH)**: 使用机器人手臂操控关节物体是一项挑战，这主要是由于关节物体复杂的运动结构，需要精确的部件分隔才能实现高效的操控。本文引入了一种基于超级点（superpoint）的新颖感知方法，旨在提高关节物体3D点云中部件分隔的准确性。我们提出了一种可学习的、部件感知的超级点生成技术，该技术能够基于几何和语义相似性高效地对点进行分组，从而产生更清晰的部件边界。此外，通过利用2D基础模型SAM的分割能力，我们识别像素区域的中心，并选择相应的超级点作为候选查询点。结合基于查询的变压器解码器进一步增强了我们方法在实现精确部件分隔方面的能力。在GAPartNet数据集上的实验结果表明，我们的方法在跨类别部件分隔方面优于现有最先进的方法，对于已见类别，AP50得分为77.9%（4.4%的提升），对于未见类别，得分为39.3%（11.6%的提升）。在5个部件类别中，对于已见物体，我们的方法获得了优于其他所有先前方法的结果，而对于所有未见物体的各个部件类别，我们的方法也表现最佳。 

---
# PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation 

**Title (ZH)**: PB-UAP: 混合通用对抗攻击方法用于图像分割 

**Authors**: Yufei Song, Ziqi Zhou, Minghui Li, Xianlong Wang, Menghao Deng, Wei Wan, Shengshan Hu, Leo Yu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16651)  

**Abstract**: With the rapid advancement of deep learning, the model robustness has become a significant research hotspot, \ie, adversarial attacks on deep neural networks. Existing works primarily focus on image classification tasks, aiming to alter the model's predicted labels. Due to the output complexity and deeper network architectures, research on adversarial examples for segmentation models is still limited, particularly for universal adversarial perturbations. In this paper, we propose a novel universal adversarial attack method designed for segmentation models, which includes dual feature separation and low-frequency scattering modules. The two modules guide the training of adversarial examples in the pixel and frequency space, respectively. Experiments demonstrate that our method achieves high attack success rates surpassing the state-of-the-art methods, and exhibits strong transferability across different models. 

**Abstract (ZH)**: 随着深度学习的迅速发展，模型鲁棒性已成为一个重要研究热点，即对深度神经网络的对抗攻击。现有研究主要集中在图像分类任务上，旨在改变模型的预测标签。由于输出复杂性和更深的网络架构，对于分割模型的对抗样本研究仍然相对有限，尤其是针对通用对抗扰动的研究。在本文中，我们提出了一种新的用于分割模型的通用对抗攻击方法，包括双特征分离模块和低频散射模块。这两个模块分别在像素空间和频率空间引导对抗样本的训练。实验结果表明，我们的方法在攻击成功率上超过了现有最先进的方法，并且在不同模型之间表现出较强的迁移性。 

---
# L3TC: Leveraging RWKV for Learned Lossless Low-Complexity Text Compression 

**Title (ZH)**: L3TC：利用RWKV进行学习驱动的低复杂度无损文本压缩

解释：
- **L3TC**: 学术论文中通常使用缩写或简称，L3TC可以保持不变。
- **Leveraging RWKV**: 利用RWKV
- **for Learned Lossless Low-Complexity Text Compression**: 用于学习驱动的低复杂度无损文本压缩

注意：确保RWKV是已知并被广泛认可的模型或技术名称，否则需要提供具体的解释。 

**Authors**: Junxuan Zhang, Zhengxue Cheng, Yan Zhao, Shihao Wang, Dajiang Zhou, Guo Lu, Li Song  

**Link**: [PDF](https://arxiv.org/pdf/2412.16642)  

**Abstract**: Learning-based probabilistic models can be combined with an entropy coder for data compression. However, due to the high complexity of learning-based models, their practical application as text compressors has been largely overlooked. To address this issue, our work focuses on a low-complexity design while maintaining compression performance. We introduce a novel Learned Lossless Low-complexity Text Compression method (L3TC). Specifically, we conduct extensive experiments demonstrating that RWKV models achieve the fastest decoding speed with a moderate compression ratio, making it the most suitable backbone for our method. Second, we propose an outlier-aware tokenizer that uses a limited vocabulary to cover frequent tokens while allowing outliers to bypass the prediction and encoding. Third, we propose a novel high-rank reparameterization strategy that enhances the learning capability during training without increasing complexity during inference. Experimental results validate that our method achieves 48\% bit saving compared to gzip compressor. Besides, \emph{L3TC} offers compression performance comparable to other learned compressors, with a $50\times$ reduction in model parameters. More importantly, \emph{L3TC} is the fastest among all learned compressors, providing real-time decoding speeds up to megabytes per second. 

**Abstract (ZH)**: 基于学习的概率模型可以与熵编码器结合用于数据压缩。然而，由于基于学习的模型复杂性较高，它们在文本压缩的实际应用中被长期忽视。为解决这一问题，我们的工作集中在低复杂度设计的同时保持压缩性能。我们提出了一种新颖的“Learned Lossless Low-complexity Text Compression”方法（L3TC）。具体来说，我们通过广泛实验发现RWKV模型在适度的压缩比下实现了最快的解码速度，使其成为我们方法的最佳基础模型。其次，我们提出了一种具有异常值感知的分词器，该分词器使用有限的词汇表覆盖高频词，并允许异常值绕过预测和编码过程。第三，我们提出了一种新的高秩重参数化策略，在训练过程中增强学习能力，而在推理时不会增加复杂度。实验结果表明，我们的方法比gzip压缩器节省了48%的位数。此外，L3TC在压缩性能方面可与其它基于学习的压缩器媲美，并且模型参数减少了50倍。更重要的是，L3TC是所有基于学习的压缩器中最快的，提供了兆字节每秒的实时解码速度。 

---
# POEX: Policy Executable Embodied AI Jailbreak Attacks 

**Title (ZH)**: POEX：政策可执行的具身AI越界攻击

解释：
- **POEX** 是标题中的缩写或代码，直译为“POEX”。
- **Policy Executable** 翻译为“政策可执行的”，意指在执行过程中考虑了相关政策或规则。
- **Embodied AI** 翻译为“具身AI”，指的是嵌入到物理系统中的智能代理。
- **Jailbreak Attacks** 翻译为“越界攻击”，在学术语境中通常指违反系统安全策略的行为。 

**Authors**: Xuancun Lu, Zhengxian Huang, Xinfeng Li, Xiaoyu ji, Wenyuan Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16633)  

**Abstract**: The integration of large language models (LLMs) into the planning module of Embodied Artificial Intelligence (Embodied AI) systems has greatly enhanced their ability to translate complex user instructions into executable policies. In this paper, we demystified how traditional LLM jailbreak attacks behave in the Embodied AI context. We conducted a comprehensive safety analysis of the LLM-based planning module of embodied AI systems against jailbreak attacks. Using the carefully crafted Harmful-RLbench, we accessed 20 open-source and proprietary LLMs under traditional jailbreak attacks, and highlighted two key challenges when adopting the prior jailbreak techniques to embodied AI contexts: (1) The harmful text output by LLMs does not necessarily induce harmful policies in Embodied AI context, and (2) even we can generate harmful policies, we have to guarantee they are executable in practice. To overcome those challenges, we propose Policy Executable (POEX) jailbreak attacks, where harmful instructions and optimized suffixes are injected into LLM-based planning modules, leading embodied AI to perform harmful actions in both simulated and physical environments. Our approach involves constraining adversarial suffixes to evade detection and fine-tuning a policy evaluater to improve the executability of harmful policies. We conducted extensive experiments on both a robotic arm embodied AI platform and simulators, to validate the attack and policy success rates on 136 harmful instructions from Harmful-RLbench. Our findings expose serious safety vulnerabilities in LLM-based planning modules, including the ability of POEX to be transferred across models. Finally, we propose mitigation strategies, such as safety-constrained prompts, pre- and post-planning checks, to address these vulnerabilities and ensure the safe deployment of embodied AI in real-world settings. 

**Abstract (ZH)**: 将大型语言模型（LLMs）集成到具身人工智能（Embodied AI）系统中的规划模块中，极大地增强了其将复杂的用户指令转化为可执行策略的能力。在本文中，我们探讨了传统的LLM劫持攻击在具身AI环境下的行为。我们对基于LLM的具身AI系统的规划模块进行了全面的安全分析，以抵御劫持攻击。我们使用精心设计的Harmful-RLbench，对20个开源和专有LLM进行了传统的劫持攻击测试，并突出了采用传统劫持技术在具身AI环境中的两个关键挑战：（1）LLM生成的危害性文本不一定会导致具身AI中的有害策略，（2）即使我们能够生成有害策略，也需要确保这些策略在实践中是可执行的。为克服这些挑战，我们提出了可执行策略（POEX）劫持攻击，其中将有害指令和优化的后缀注入基于LLM的规划模块，使具身AI在模拟和物理环境中执行有害行为。我们的方法包括限制敌对后缀以躲避检测，并fine-tuning策略评估器以提高有害策略的可执行性。我们在一个具身人工智能平台和模拟器上进行了广泛的实验，验证了136条来自Harmful-RLbench的有害指令的攻击和策略成功率。我们的研究揭示了基于LLM的规划模块中的严重安全漏洞，包括POEX能够跨模型转移的能力。最后，我们提出了缓解策略，如安全性约束提示、规划前后的检查，以解决这些漏洞并确保具身AI在真实环境中的安全部署。 

---
# Deep Learning for Spatio-Temporal Fusion in Land Surface Temperature Estimation: A Comprehensive Survey, Experimental Analysis, and Future Trends 

**Title (ZH)**: 基于空时融合的土地表层温度估算中的深度学习方法：综述、实验分析及未来趋势 

**Authors**: Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai  

**Link**: [PDF](https://arxiv.org/pdf/2412.16631)  

**Abstract**: The rapid advancements in satellite remote sensing have enhanced the capability to monitor and analyze the Earth's surface. Among the many variables captured through satellite sensors, Land Surface Temperature (LST) plays a critical role in understanding key environmental processes. However, obtaining high-resolution LST data remains a challenge, as satellite sensors often face a trade-off between spatial and temporal resolutions. In response, Spatio-Temporal Fusion (STF) has emerged as a powerful method to integrate two satellite data sources, one providing high spatial but low temporal resolution, and the other offering high temporal but low spatial resolution. Although a range of STF techniques have been proposed, from traditional methods to cutting-edge deep learning (DL) models, most have focused on surface reflectance, with limited application to LST estimation. DL approaches, in particular, show promise in improving the spatial and temporal resolutions of LST by capturing complex, non-linear relationships between input and output LST data. This paper offers a comprehensive review of the latest advancements in DL-based STF techniques for LST estimation. We analyze key research developments, mathematically formulate the STF problem, and introduce a novel taxonomy for DL-based STF methods. Furthermore, we discuss the challenges faced by current methods and highlight future research directions. In addition, we present the first open-source benchmark STF dataset for LST estimation, consisting of 51 pairs of MODIS-Landsat images spanning from 2013 to 2024. To support our findings, we conduct extensive experiments on state-of-the-art methods and present both quantitative and qualitative assessments. This is the first survey paper focused on DL-based STF for LST estimation. We hope it serves as a valuable reference for researchers and paves the way for future research in this field. 

**Abstract (ZH)**: 卫星遥感的快速发展增强了对地球表面进行监测和分析的能力。通过卫星传感器捕获的众多变量中，地表温度（LST）在理解关键环境过程方面发挥着重要作用。然而，获取高分辨率LST数据仍面临挑战，因为卫星传感器常常在空间分辨率和时间分辨率之间存在权衡。为应对这一挑战，时空融合（STF）已经作为一种强大的方法出现，能够整合两种卫星数据源，一种提供高空间分辨率但低时间分辨率，另一种提供高时间分辨率但低空间分辨率。尽管提出了多种STF技术，从传统的到最新的深度学习（DL）模型，但多数方法主要关注表面反射率，对LST估计的应用较少。特别是在DL方法方面，它们展现出通过捕获输入和输出LST数据之间的复杂非线性关系来提高LST的空间和时间分辨率的潜力。本文对基于DL的STF技术最新进展进行了全面的综述。我们分析了关键的研究进展，数学地形式化了STF问题，并引入了一种基于DL的STF方法的新分类。此外，我们讨论了当前方法面临的挑战，并指出了未来的研究方向。我们还展示了首个开源基准STF数据集，其中包含2013年至2024年间MODIS-landsat影像对51组。为了支持我们的发现，我们进行了广泛的方法实验，并提供了定量和定性的评估。这是第一篇专注于基于DL的STF技术用于LST估计的综述论文。我们希望它能成为研究人员的重要参考，并为该领域的未来研究铺平道路。 

---
# Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement 

**Title (ZH)**: Mamba-SEUNet：用于单声道语音增强的Mamba UNet 

**Authors**: Junyu Wang, Zizhen Lin, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16626)  

**Abstract**: In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73. 

**Abstract (ZH)**: 近年来，随着语音增强（SE）研究的进展，变压器及其变体已成为主要方法。然而，自注意力机制的二次复杂性在实际部署中对其产生了某些限制。Mamba作为一种新颖的状态空间模型（SSM），由于其在建模长序列方面的强大能力以及相对较低的计算复杂性，在自然语言处理和计算机视觉领域得到了广泛应用。在本研究中，我们提出了Mamba-SEUNet，这是一种将Mamba与U-Net结合的新颖架构，用于语音增强任务。通过利用双向Mamba建模不同分辨率下语音信号的前向和后向依赖性，并结合跳跃连接以捕获多尺度信息，我们的方法实现了当前最先进的（SOTA）性能。实验结果表明，Mamba-SEUNet在VCTK+DEMAND数据集上的PESQ得分为3.59，同时保持了低计算复杂性。当与感知对比拉伸技术结合使用时，Mamba-SEUNet进一步提高了PESQ得分为3.73。 

---
# Automated Bleeding Detection and Classification in Wireless Capsule Endoscopy with YOLOv8-X 

**Title (ZH)**: 使用YOLOv8-X在无线胶囊内镜中自动检测和分类出血 

**Authors**: Pavan C Shekar, Vivek Kanhangad, Shishir Maheshwari, T Sunil Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2412.16624)  

**Abstract**: Gastrointestinal (GI) bleeding, a critical indicator of digestive system disorders, re quires efficient and accurate detection methods. This paper presents our solution to the Auto-WCEBleedGen Version V1 Challenge, where we achieved the consolation position. We developed a unified YOLOv8-X model for both detection and classification of bleeding regions in Wireless Capsule Endoscopy (WCE) images. Our approach achieved 96.10% classification accuracy and 76.8% mean Average Precision (mAP) at 0.5 IoU on the val idation dataset. Through careful dataset curation and annotation, we assembled and trained on 6,345 diverse images to ensure robust model performance. Our implementa tion code and trained models are publicly available at this https URL. 

**Abstract (ZH)**: 胃肠道（GI）出血是消化系统紊乱的一个关键指标，需要高效的检测方法。本文提出了我们对Auto-WCEBleedGen V1挑战赛的解决方案，并在该挑战赛中获得了第三名。我们开发了一个统一的YOLOv8-X模型，用于无线胶囊内镜（WCE）图像中的出血区域检测和分类。我们的方法在验证数据集中实现了96.10%的分类准确率和76.8%的平均精度（mAP，IoU=0.5）。通过仔细的数据集整理和标注，我们收集并训练了6,345张多样化的图像，以确保模型的鲁棒性。我们的实现代码和训练模型已在此处 https://公开提供。 

---
# Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering Approach 

**Title (ZH)**: 基于数据制图的聚类方法：移动边缘和云端的分布式推断 

**Authors**: Divya Jyoti Bajpai, Manjesh Kumar Hanawal  

**Link**: [PDF](https://arxiv.org/pdf/2412.16616)  

**Abstract**: The large size of DNNs poses a significant challenge for deployment on devices with limited resources, such as mobile, edge, and IoT platforms. To address this issue, a distributed inference framework can be utilized. In this framework, a small-scale DNN (initial layers) is deployed on mobile devices, a larger version on edge devices, and the full DNN on the cloud. Samples with low complexity (easy) can be processed on mobile, those with moderate complexity (medium) on edge devices, and high complexity (hard) samples on the cloud. Given that the complexity of each sample is unknown in advance, the crucial question in distributed inference is determining the sample complexity for appropriate DNN processing. We introduce a novel method named \our{}, which leverages the Data Cartography approach initially proposed for enhancing DNN generalization. By employing data cartography, we assess sample complexity. \our{} aims to boost accuracy while considering the offloading costs from mobile to edge/cloud. Our experimental results on GLUE datasets, covering a variety of NLP tasks, indicate that our approach significantly lowers inference costs by more than 43\% while maintaining a minimal accuracy drop of less than 0.5\% compared to performing all inferences on the cloud. The source code is available at this https URL. 

**Abstract (ZH)**: 大型DNN（深度神经网络）在资源受限的设备上（如移动设备、边缘设备和物联网平台）部署时会带来重大挑战。为此，可以利用分布式推理框架来应对这一问题。在这种框架中，小型化的DNN（初始层）部署在移动设备上，更大规模的部署在边缘设备上，而完整的DNN则部署在云端。简单（低复杂度）的样本可以在移动设备上处理，中等复杂度的样本在边缘设备上处理，而复杂（高复杂度）的样本则在云端处理。由于每个样本的复杂度在事前是未知的，分布式推理中的关键问题是确定适当的DNN处理所需的样本复杂度。我们介绍了一种名为\our{}（此处应该替换为具体的命名）的新方法，该方法利用了最初为提高DNN泛化能力提出的“数据制图”方法。通过使用数据制图，我们评估了样本复杂度。\our{}旨在在减轻移动设备到边缘设备/云端的卸载成本的同时，提高准确性。我们对GLUE数据集（涵盖了各种自然语言处理任务）的实验结果显示，我们的方法在保持不到0.5%的精度下降的同时，能够将推理成本显著降低超过43%。源代码可以通过以下链接获得：[此 https URL]。 

---
# Automated Classification of Cybercrime Complaints using Transformer-based Language Models for Hinglish Texts 

**Title (ZH)**: 使用基于变换器的语言模型对印英混合文本中的网络犯罪举报进行自动分类 

**Authors**: Nanda Rani, Divyanshu Singh, Bikash Saha, Sandeep Kumar Shukla  

**Link**: [PDF](https://arxiv.org/pdf/2412.16614)  

**Abstract**: The rise in cybercrime and the complexity of multilingual and code-mixed complaints present significant challenges for law enforcement and cybersecurity agencies. These organizations need automated, scalable methods to identify crime types, enabling efficient processing and prioritization of large complaint volumes. Manual triaging is inefficient, and traditional machine learning methods fail to capture the semantic and contextual nuances of textual cybercrime complaints. Moreover, the lack of publicly available datasets and privacy concerns hinder the research to present robust solutions. To address these challenges, we propose a framework for automated cybercrime complaint classification. The framework leverages Hinglish-adapted transformers, such as HingBERT and HingRoBERTa, to handle code-mixed inputs effectively. We employ the real-world dataset provided by Indian Cybercrime Coordination Centre (I4C) during CyberGuard AI Hackathon 2024. We employ GenAI open source model-based data augmentation method to address class imbalance. We also employ privacy-aware preprocessing to ensure compliance with ethical standards while maintaining data integrity. Our solution achieves significant performance improvements, with HingRoBERTa attaining an accuracy of 74.41% and an F1-score of 71.49%. We also develop ready-to-use tool by integrating Django REST backend with a modern frontend. The developed tool is scalable and ready for real-world deployment in platforms like the National Cyber Crime Reporting Portal. This work bridges critical gaps in cybercrime complaint management, offering a scalable, privacy-conscious, and adaptable solution for modern cybersecurity challenges. 

**Abstract (ZH)**: 网络安全犯罪的上升和多语言及代码混合投诉的复杂性为执法和网络安全机构带来了重大挑战。这些组织需要自动化的、可扩展的方法来识别犯罪类型，以实现对大量投诉的高效处理和优先级排序。手动筛选效率低下，传统机器学习方法难以捕捉到文本网络犯罪投诉中的语义和上下文细微差别。此外，缺乏公开的数据集和隐私担忧阻碍了针对这些问题提出可靠解决方案的研究。为应对这些挑战，我们提出了一种自动化的网络犯罪投诉分类框架。该框架利用了适应Hinglish的语言模型，如HingBERT和HingRoBERTa，以有效处理代码混合输入。我们使用印度网络安全协调中心（I4C）提供的真实数据集参加了2024年CyberGuard AI黑客松比赛。我们还采用基于GenAI开源模型的数据增强方法来解决类别不平衡问题，并采用隐私保护的预处理方法，确保在遵守伦理标准的同时维护数据完整性。我们的解决方案取得了显著的性能提升，HingRoBERTa的准确率为74.41%，F1分为71.49%。我们还通过将Django REST后端与现代前端集成，开发了一个可立即使用、具有可扩展性的工具。开发的工具适用于像全国网络安全投诉门户这样的实际应用平台。本研究填补了网络犯罪投诉管理中的关键空白，提供了一种可扩展、隐私保护和适应性强的解决方案，应对现代网络安全挑战。 

---
# V"Mean"ba: Visual State Space Models only need 1 hidden dimension 

**Title (ZH)**: “均值”ba：视觉状态空间模型仅需一个隐含维度 

**Authors**: Tien-Yu Chi, Hung-Yueh Chiang, Chi-Chih Chang, Ning-Chi Huang, Kai-Chiang Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16602)  

**Abstract**: Vision transformers dominate image processing tasks due to their superior performance. However, the quadratic complexity of self-attention limits the scalability of these systems and their deployment on resource-constrained devices. State Space Models (SSMs) have emerged as a solution by introducing a linear recurrence mechanism, which reduces the complexity of sequence modeling from quadratic to linear. Recently, SSMs have been extended to high-resolution vision tasks. Nonetheless, the linear recurrence mechanism struggles to fully utilize matrix multiplication units on modern hardware, resulting in a computational bottleneck. We address this issue by introducing \textit{VMeanba}, a training-free compression method that eliminates the channel dimension in SSMs using mean operations. Our key observation is that the output activations of SSM blocks exhibit low variances across channels. Our \textit{VMeanba} leverages this property to optimize computation by averaging activation maps across the channel to reduce the computational overhead without compromising accuracy. Evaluations on image classification and semantic segmentation tasks demonstrate that \textit{VMeanba} achieves up to a 1.12x speedup with less than a 3\% accuracy loss. When combined with 40\% unstructured pruning, the accuracy drop remains under 3\%. 

**Abstract (ZH)**: 视觉变换器在图像处理任务中占主导地位，这是由于其优越的性能。然而，自注意力机制的二次复杂度限制了这些系统的可扩展性和在资源受限设备上的部署。状态空间模型（SSMs）作为一种解决方案崭露头角，通过引入线性递归机制来将序列建模的复杂度从二次降低到线性。最近，SSMs 已被扩展到高分辨率的视觉任务。然而，线性递归机制在现代硬件中难以充分利用矩阵乘法单元，导致计算瓶颈。我们通过引入一种无需训练的压缩方法 \textit{VMeanba} 来解决这一问题，该方法通过均值操作消除了SSMs中的通道维度。我们的关键观察是，SSM模块的输出激活在通道间表现出较低的方差。我们利用这一特性通过在通道之间平均激活图来优化计算，从而减少计算开销而不牺牲准确性。在图像分类和语义分割任务上的评估表明，\textit{VMeanba} 可实现多达1.12倍的加速，同时精度损失不到3%。当与40%的非结构化剪枝结合使用时，精度下降仍然保持在3%以内。 

---
# AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection 

**Title (ZH)**: AIGCodeSet:一个新的标注数据集，用于AI生成代码检测 

**Authors**: Basak Demirok, Mucahid Kutlu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16594)  

**Abstract**: With the rapid advancement of LLM models, they have become widely useful in various fields. While these AI systems can be used for code generation, significantly simplifying and accelerating the tasks of developers, their use for students to do assignments has raised ethical questions in the field of education. In this context, determining the author of a particular code becomes important. In this study, we introduce AIGCodeSet, a dataset for AI-generated code detection tasks, specifically for the Python programming language. We obtain the problem descriptions and human-written codes from the CodeNet dataset. Using the problem descriptions, we generate AI-written codes with CodeLlama 34B, Codestral 22B, and Gemini 1.5 Flash models in three approaches: i) generating code from the problem description alone, ii) generating code using the description along with human-written source code containing runtime errors, and iii) generating code using the problem description and human-written code that resulted in wrong answers. Lastly, we conducted a post-processing step to eliminate LLM output irrelevant to code snippets. Overall, AIGCodeSet consists of 2,828 AI-generated and 4,755 human-written code snippets. We share our code with the research community to support studies on this important topic and provide performance results for baseline AI-generated code detection methods. 

**Abstract (ZH)**: 随着大语言模型（LLM）的迅速发展，它们已在多个领域中得到了广泛应用。这些人工智能系统可以用于代码生成，显著简化并加快了开发人员的任务。然而，学生使用这些系统完成作业引发了教育领域中的伦理问题。在这种背景下，确定一段代码的作者变得尤为重要。在本研究中，我们引入了AIGCodeSet，这是一个用于检测人工生成代码的数据集，专门针对Python编程语言。我们从CodeNet数据集中获取了问题描述和人工撰写的代码。使用问题描述，我们使用CodeLlama 34B、Codestral 22B和Gemini 1.5 Flash模型生成了三种类型的AI撰写的代码：i) 仅从问题描述生成代码；ii) 结合包含运行时错误的人工撰写的源代码生成代码；iii) 结合问题描述和结果错误的人工撰写的代码生成代码。最后，我们执行了一个后处理步骤，以消除与代码片段无关的LLM输出。总体而言，AIGCodeSet包含了2,828段人工生成的代码片段和4,755段人工撰写的代码片段。我们向研究社区分享了我们的代码，以支持对该重要主题的研究，并提供了基础的人工智能生成代码检测方法的性能结果。 

---
# Breaking the Context Bottleneck on Long Time Series Forecasting 

**Title (ZH)**: 长时序预测中的情境瓶颈突破 

**Authors**: Chao Ma, Yikai Hou, Xiang Li, Yinggang Sun, Haining Yu, Zhou Fang, Jiaxing Qu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16572)  

**Abstract**: Long-term time-series forecasting is essential for planning and decision-making in economics, energy, and transportation, where long foresight is required. To obtain such long foresight, models must be both efficient and effective in processing long sequence. Recent advancements have enhanced the efficiency of these models; however, the challenge of effectively leveraging longer sequences persists. This is primarily due to the tendency of these models to overfit when presented with extended inputs, necessitating the use of shorter input lengths to maintain tolerable error margins. In this work, we investigate the multiscale modeling method and propose the Logsparse Decomposable Multiscaling (LDM) framework for the efficient and effective processing of long sequences. We demonstrate that by decoupling patterns at different scales in time series, we can enhance predictability by reducing non-stationarity, improve efficiency through a compact long input representation, and simplify the architecture by providing clear task assignments. Experimental results demonstrate that LDM not only outperforms all baselines in long-term forecasting benchmarks, but also reducing both training time and memory costs. 

**Abstract (ZH)**: 长期时间序列预测对于经济、能源和交通运输领域的规划和决策至关重要，需要长远的预见性。为了获得这种预见性，模型必须在处理长序列时既高效又有效。近期的进步已经提高了这些模型的效率，但有效地利用更长的序列仍然存在挑战。这主要是因为这些模型在面对扩展输入时容易出现过拟合，因此需要使用较短的输入长度以保持可接受的误差范围。在这项工作中，我们研究了多尺度建模方法，并提出了 Logsparse 可分解多尺度（LDM）框架，以实现对长序列的高效和有效处理。通过在时间序列中解耦不同尺度下的模式，我们可以通过降低非平稳性来增强预测能力，通过紧凑的长输入表示来提高效率，并通过提供清晰的任务分配简化架构。实验结果表明，LDM 不仅在长期预测基准中优于所有基线模型，而且还能降低训练时间和内存成本。 

---
# Learning for Cross-Layer Resource Allocation in MEC-Aided Cell-Free Networks 

**Title (ZH)**: MEC辅助无蜂窝网络跨层资源分配的学习方法 

**Authors**: Chong Zheng, Shiwen He, Yongming Huang, Tony Q. S. Quek  

**Link**: [PDF](https://arxiv.org/pdf/2412.16565)  

**Abstract**: Cross-layer resource allocation over mobile edge computing (MEC)-aided cell-free networks can sufficiently exploit the transmitting and computing resources to promote the data rate. However, the technical bottlenecks of traditional methods pose significant challenges to cross-layer optimization. In this paper, joint subcarrier allocation and beamforming optimization are investigated for the MEC-aided cell-free network from the perspective of deep learning to maximize the weighted sum rate. Specifically, we convert the underlying problem into a joint multi-task optimization problem and then propose a centralized multi-task self-supervised learning algorithm to solve the problem so as to avoid costly manual labeling. Therein, two novel and general loss functions, i.e., negative fraction linear loss and exponential linear loss whose advantages in robustness and target domain have been proved and discussed, are designed to enable self-supervised learning. Moreover, we further design a MEC-enabled distributed multi-task self-supervised learning (DMTSSL) algorithm, with low complexity and high scalability to address the challenge of dimensional disaster. Finally, we develop the distance-aware transfer learning algorithm based on the DMTSSL algorithm to handle the dynamic scenario with negligible computation cost. Simulation results under $3$rd generation partnership project 38.901 urban-macrocell scenario demonstrate the superiority of the proposed algorithms over the baseline algorithms. 

**Abstract (ZH)**: 利用深度学习从多任务角度进行联合子载波分配和波束赋形优化，以最大化加权总数据速率，是辅助无小区网络的计算辅助移动边缘计算（MEC）网络跨层资源分配的一种有效方法。然而，传统方法的技术瓶颈对跨层优化提出了重大挑战。本文从深度学习的角度研究了辅助无小区网络的联合子载波分配和波束赋形优化，以最大化加权总数据速率。具体地，我们将底层问题转化为联合多任务优化问题，并提出了一种集中的多任务半监督学习算法来解决这个问题，从而避免成本高昂的手动标注。在此过程中，我们设计了两种新颖且通用的损失函数，即负分数线性损失和指数线性损失，这两种损失函数在鲁棒性和目标域方面的优势已被证明和讨论，以支持半监督学习。此外，我们还设计了一种计算复杂度低、可扩展性高的MEC使能分布式多任务半监督学习（DMTSSL）算法，以应对维度灾难的挑战。最后，我们开发了一种基于DMTSSL算法的距离感知迁移学习算法，以处理动态场景，几乎不增加计算成本。在3GPP 38.901城市宏小区场景下的仿真实验中，提出的算法优于基线算法，展示了其优越性。 

---
# Predictive Monitoring of Black-Box Dynamical Systems 

**Title (ZH)**: 黑箱动态系统的预测性监测 

**Authors**: Thomas A. Henzinger, Fabian Kresse, Kaushik Mallik, Emily Yu, Đorđe Žikelić  

**Link**: [PDF](https://arxiv.org/pdf/2412.16564)  

**Abstract**: We study the problem of predictive runtime monitoring of black-box dynamical systems with quantitative safety properties. The black-box setting stipulates that the exact semantics of the dynamical system and the controller are unknown, and that we are only able to observe the state of the controlled (aka, closed-loop) system at finitely many time points. We present a novel framework for predicting future states of the system based on the states observed in the past. The numbers of past states and of predicted future states are parameters provided by the user. Our method is based on a combination of Taylor's expansion and the backward difference operator for numerical differentiation. We also derive an upper bound on the prediction error under the assumption that the system dynamics and the controller are smooth. The predicted states are then used to predict safety violations ahead in time. Our experiments demonstrate practical applicability of our method for complex black-box systems, showing that it is computationally lightweight and yet significantly more accurate than the state-of-the-art predictive safety monitoring techniques. 

**Abstract (ZH)**: 我们研究了一类具有定量安全性属性的黑盒动态系统的预测运行时监控问题。黑盒设置意味着我们不知道动态系统和控制器的确切语义，只能观察在有限时间点上受控系统（即闭环系统）的状态。我们提出了一种新的框架，基于过去观察到的状态来预测未来系统状态。过去状态的数量和预测未来状态的数量是用户提供的参数。该方法的基础是泰勒展开和数值微分中的后差分算子相结合。基于系统动力学和控制器是光滑的假设，我们推导出了预测误差的上界。然后使用预测状态来预测未来时间的安全性违规。我们的实验表明，我们的方法对于复杂的黑盒系统具有实际适用性，显示出该方法在计算上较为轻量级，且显著优于当前最先进的预测安全监控技术。 

---
# Diffusion Prior Interpolation for Flexibility Real-World Face Super-Resolution 

**Title (ZH)**: 基于扩散先验的插值方法以实现灵活的现实面部超分辨率 

**Authors**: Jiarui Yang, Tao Dai, Yufei Zhu, Naiqi Li, Jinmin Li, Shutao Xia  

**Link**: [PDF](https://arxiv.org/pdf/2412.16552)  

**Abstract**: Diffusion models represent the state-of-the-art in generative modeling. Due to their high training costs, many works leverage pre-trained diffusion models' powerful representations for downstream tasks, such as face super-resolution (FSR), through fine-tuning or prior-based methods. However, relying solely on priors without supervised training makes it challenging to meet the pixel-level accuracy requirements of discrimination task. Although prior-based methods can achieve high fidelity and high-quality results, ensuring consistency remains a significant challenge. In this paper, we propose a masking strategy with strong and weak constraints and iterative refinement for real-world FSR, termed Diffusion Prior Interpolation (DPI). We introduce conditions and constraints on consistency by masking different sampling stages based on the structural characteristics of the face. Furthermore, we propose a condition Corrector (CRT) to establish a reciprocal posterior sampling process, enhancing FSR performance by mutual refinement of conditions and samples. DPI can balance consistency and diversity and can be seamlessly integrated into pre-trained models. In extensive experiments conducted on synthetic and real datasets, along with consistency validation in face recognition, DPI demonstrates superiority over SOTA FSR methods. The code is available at \url{this https URL}. 

**Abstract (ZH)**: 扩散模型代表了生成建模的前沿技术。由于其高昂的训练成本，许多研究工作通过微调或基于先验的方法利用预训练扩散模型的强大表示来进行下游任务，例如面部超分辨率（FSR）。然而，仅依赖先验而无需监督训练使得难以满足区分任务在像素级精度方面的要求。尽管基于先验的方法可以实现高保真度和高质量的结果，但确保一致性仍然是一个重大挑战。在本文中，我们提出了一种具有强约束和弱约束的掩码策略及迭代细化方法来解决真实世界中的FSR问题，称为扩散先验插值（DPI）。我们通过根据面部的结构特征来掩码不同的采样阶段，引入了一致性条件和约束。此外，我们提出了一个条件矫正文法（CRT），以建立条件和样本之间的相互精炼的后验采样过程，从而提高FSR性能。DPI能够平衡一致性和多样性，并且可以无缝集成到预训练模型中。在针对合成和真实数据集进行的广泛实验以及面部识别中的一致性验证中，DPI显示出优于当前最先进的FSR方法的优势。代码可在以下链接获取：\url{this https URL}。 

---
# Prior2Posterior: Model Prior Correction for Long-Tailed Learning 

**Title (ZH)**: Prior2Posterior: 模型先验校正以应对长尾分布学习 

**Authors**: S Divakar Bhat, Amit More, Mudit Soni, Surbhi Agrawal  

**Link**: [PDF](https://arxiv.org/pdf/2412.16540)  

**Abstract**: Learning-based solutions for long-tailed recognition face difficulties in generalizing on balanced test datasets. Due to imbalanced data prior, the learned \textit{a posteriori} distribution is biased toward the most frequent (head) classes, leading to an inferior performance on the least frequent (tail) classes. In general, the performance can be improved by removing such a bias by eliminating the effect of imbalanced prior modeled using the number of class samples (frequencies). We first observe that the \textit{effective prior} on the classes, learned by the model at the end of the training, can differ from the empirical prior obtained using class frequencies. Thus, we propose a novel approach to accurately model the effective prior of a trained model using \textit{a posteriori} probabilities. We propose to correct the imbalanced prior by adjusting the predicted \textit{a posteriori} probabilities (Prior2Posterior: P2P) using the calculated prior in a post-hoc manner after the training, and show that it can result in improved model performance. We present theoretical analysis showing the optimality of our approach for models trained with naive cross-entropy loss as well as logit adjusted loss. Our experiments show that the proposed approach achieves new state-of-the-art (SOTA) on several benchmark datasets from the long-tail literature in the category of logit adjustment methods. Further, the proposed approach can be used to inspect any existing method to capture the \textit{effective prior} and remove any residual bias to improve its performance, post-hoc, without model retraining. We also show that by using the proposed post-hoc approach, the performance of many existing methods can be improved further. 

**Abstract (ZH)**: 基于学习的方法在处理长尾识别问题时，难以在平衡的数据集上泛化。由于数据分布不平衡，模型在训练结束后学到的后验分布会偏向最频繁出现的类别（头部类别），导致对最少出现的类别（尾部类别）表现较差。一般来说，可以通过消除这种偏差来改善性能，即通过消除使用类别样本数量（频率）建模的不平衡先验的影响。我们首先观察到，模型在训练结束后学到的类别的有效先验可能与使用类别频率获取的经验先验不同。因此，我们提出了一种新的方法，使用后验概率准确地建模训练模型的有效先验。我们提出通过调整预测的后验概率（Prior2Posterior: P2P）来修正不平衡先验，并在训练后通过后处理方式进行调整，以期改善模型性能。我们进行了理论分析，证明了这种方法在使用蒙昧交叉熵损失和logit调整损失训练的模型中都是最优的。实验结果显示，在长尾数据集文献中的日志调整方法类别中，所提出的方法达到了新的最佳性能（SOTA）。此外，我们提出的方法可以用于任何现有方法，以便在其训练后捕获有效先验并消除任何剩余偏差以提高其性能，而无需重新训练模型。我们还展示了通过使用提出的方法，可以进一步提高许多现有方法的性能。 

---
# Towards Environmentally Equitable AI 

**Title (ZH)**: 面向环境公平的AI 

**Authors**: Mohammad Hajiesmaili, Shaolei Ren, Ramesh K. Sitaraman, Adam Wierman  

**Link**: [PDF](https://arxiv.org/pdf/2412.16539)  

**Abstract**: The skyrocketing demand for artificial intelligence (AI) has created an enormous appetite for globally deployed power-hungry servers. As a result, the environmental footprint of AI systems has come under increasing scrutiny. More crucially, the current way that we exploit AI workloads' flexibility and manage AI systems can lead to wildly different environmental impacts across locations, increasingly raising environmental inequity concerns and creating unintended sociotechnical consequences. In this paper, we advocate environmental equity as a priority for the management of future AI systems, advancing the boundaries of existing resource management for sustainable AI and also adding a unique dimension to AI fairness. Concretely, we uncover the potential of equity-aware geographical load balancing to fairly re-distribute the environmental cost across different regions, followed by algorithmic challenges. We conclude by discussing a few future directions to exploit the full potential of system management approaches to mitigate AI's environmental inequity. 

**Abstract (ZH)**: 随着人工智能（AI）需求的急剧增长，对全球部署的高能耗服务器产生了巨大的需求。因此，AI系统的环境足迹正受到越来越严格的审视。更重要的是，我们当前利用AI工作负载的灵活性以及管理AI系统的方式可能导致不同地区产生截然不同的环境影响，这正越来越引起环境不公平的关注，并产生意想不到的社会技术后果。在本文中，我们倡导在未来的AI系统管理中将环境公平作为优先事项，推进现有的资源管理边界以实现可持续AI，并在AI公平性方面增加一个独特的维度。具体而言，我们揭示了环境意识地理负载均衡的潜在能力，旨在公平地重新分配不同地区的环境成本，同时探讨了相关的算法挑战。最后，我们讨论了几种未来方向，以充分利用系统管理方法来缓解AI的环境不公平性。 

---
# Text2midi: Generating Symbolic Music from Captions 

**Title (ZH)**: Text2midi：从描述生成符号音乐 

**Authors**: Keshav Bhandari, Abhinaba Roy, Kyra Wang, Geeta Puri, Simon Colton, Dorien Herremans  

**Link**: [PDF](https://arxiv.org/pdf/2412.16526)  

**Abstract**: This paper introduces text2midi, an end-to-end model to generate MIDI files from textual descriptions. Leveraging the growing popularity of multimodal generative approaches, text2midi capitalizes on the extensive availability of textual data and the success of large language models (LLMs). Our end-to-end system harnesses the power of LLMs to generate symbolic music in the form of MIDI files. Specifically, we utilize a pretrained LLM encoder to process captions, which then condition an autoregressive transformer decoder to produce MIDI sequences that accurately reflect the provided descriptions. This intuitive and user-friendly method significantly streamlines the music creation process by allowing users to generate music pieces using text prompts. We conduct comprehensive empirical evaluations, incorporating both automated and human studies, that show our model generates MIDI files of high quality that are indeed controllable by text captions that may include music theory terms such as chords, keys, and tempo. We release the code and music samples on our demo page (this https URL) for users to interact with text2midi. 

**Abstract (ZH)**: 本文介绍了一种端到端模型——text2midi，该模型能够从文本描述生成MIDI文件。借助多模态生成方法日益增长的流行性，text2midi 充分利用了文本数据的广泛可用性以及大型语言模型（LLMs）的成功经验。我们整个系统的运行依赖于LLMs的强大力量，将符号音乐（以MIDI文件形式）生成为文本描述的准确反映。具体而言，我们利用预训练的LLM编码器处理描述文本，然后通过自回归变换器解码器生成MIDI序列。这种方法直观且用户友好，大大简化了音乐创作过程，使用户能够仅通过文本提示生成音乐作品。我们进行了全面的实证评估，包括自动和人工研究，结果表明我们的模型能够生成高质量的MIDI文件，这些文件确实可以通过包括和弦、音阶和速度等音乐术语在内的文本描述来控制。我们已在演示页面上发布了模型的代码和音乐样本（点击此处），供用户与text2midi互动。 

---
# Enhancing Contrastive Learning Inspired by the Philosophy of "The Blind Men and the Elephant" 

**Title (ZH)**: 基于“盲人摸象”哲学启发的对比学习增强研究 

**Authors**: Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16522)  

**Abstract**: Contrastive learning is a prevalent technique in self-supervised vision representation learning, typically generating positive pairs by applying two data augmentations to the same image. Designing effective data augmentation strategies is crucial for the success of contrastive learning. Inspired by the story of the blind men and the elephant, we introduce JointCrop and JointBlur. These methods generate more challenging positive pairs by leveraging the joint distribution of the two augmentation parameters, thereby enabling contrastive learning to acquire more effective feature representations. To the best of our knowledge, this is the first effort to explicitly incorporate the joint distribution of two data augmentation parameters into contrastive learning. As a plug-and-play framework without additional computational overhead, JointCrop and JointBlur enhance the performance of SimCLR, BYOL, MoCo v1, MoCo v2, MoCo v3, SimSiam, and Dino baselines with notable improvements. 

**Abstract (ZH)**: 对比学习是一种常见的自监督视觉表征学习技术，通常通过在同一图像上应用两种数据增强来生成正样本对。设计有效的数据增强策略对于对比学习的成功至关重要。受盲人摸象故事的启发，我们引入了JointCrop和JointBlur这两种方法。这些方法通过利用两种增强参数的联合分布来生成更具挑战性的正样本对，从而使得对比学习能够获得更有效的特征表示。据我们所知，这是首次明确提出将两种数据增强参数的联合分布集成到对比学习中的尝试。作为一种无需额外计算开销的插件式框架，JointCrop和JointBlur显著提升了SimCLR、BYOL、MoCo v1、MoCo v2、MoCo v3、SimSiam和Dino基线模型的性能。 

---
# VSFormer: Value and Shape-Aware Transformer with Prior-Enhanced Self-Attention for Multivariate Time Series Classification 

**Title (ZH)**: VSFormer：具有先验增强自注意力的值和形状感知变换器用于多变量时间序列分类 

**Authors**: Wenjie Xi, Rundong Zuo, Alejandro Alvarez, Jie Zhang, Byron Choi, Jessica Lin  

**Link**: [PDF](https://arxiv.org/pdf/2412.16515)  

**Abstract**: Multivariate time series classification is a crucial task in data mining, attracting growing research interest due to its broad applications. While many existing methods focus on discovering discriminative patterns in time series, real-world data does not always present such patterns, and sometimes raw numerical values can also serve as discriminative features. Additionally, the recent success of Transformer models has inspired many studies. However, when applying to time series classification, the self-attention mechanisms in Transformer models could introduce classification-irrelevant features, thereby compromising accuracy. To address these challenges, we propose a novel method, VSFormer, that incorporates both discriminative patterns (shape) and numerical information (value). In addition, we extract class-specific prior information derived from supervised information to enrich the positional encoding and provide classification-oriented self-attention learning, thereby enhancing its effectiveness. Extensive experiments on all 30 UEA archived datasets demonstrate the superior performance of our method compared to SOTA models. Through ablation studies, we demonstrate the effectiveness of the improved encoding layer and the proposed self-attention mechanism. Finally, We provide a case study on a real-world time series dataset without discriminative patterns to interpret our model. 

**Abstract (ZH)**: 多元时间序列分类是数据挖掘中的关键任务，由于其广泛的应用前景，研究兴趣正在不断增加。虽然许多现有方法专注于发现时间序列中的判别模式，但实际情况并非总是如此。有时原始的数值数据也可以作为判别特征。此外，Transformer模型的成功应用激发了许多研究，但在应用于时间序列分类时，Transformer模型中的自注意力机制可能会引入与分类无关的特征，从而影响准确性。为应对这些挑战，我们提出了一种新的方法——VSFormer，该方法结合了判别模式（形状）和数值信息（值）。此外，我们从监督信息中提取特定类别的先验信息，以丰富位置编码并提供分类导向的自注意力学习，从而提高其效果。通过对所有30个UEA存档数据集进行的广泛实验表明，我们的方法在性能上优于当前最先进技术。通过消融研究，我们证明了改进的编码层和提出的自注意力机制的有效性。最后，我们在一个没有判别模式的实际时间序列数据集上进行案例研究，以解释我们的模型。 

---
# TrojFlow: Flow Models are Natural Targets for Trojan Attacks 

**Title (ZH)**: TrojFlow: 流模型是 Trojan 攻击的天然目标 

**Authors**: Zhengyang Qi, Xiaohua Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16512)  

**Abstract**: Flow-based generative models (FMs) have rapidly advanced as a method for mapping noise to data, its efficient training and sampling process makes it widely applicable in various fields. FMs can be viewed as a variant of diffusion models (DMs). At the same time, previous studies have shown that DMs are vulnerable to Trojan/Backdoor attacks, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. We found that Trojan attacks on generative models are essentially equivalent to image transfer tasks from the backdoor distribution to the target distribution, the unique ability of FMs to fit any two arbitrary distributions significantly simplifies the training and sampling setups for attacking FMs, making them inherently natural targets for backdoor attacks. In this paper, we propose TrojFlow, exploring the vulnerabilities of FMs through Trojan attacks. In particular, we consider various attack settings and their combinations and thoroughly explore whether existing defense methods for DMs can effectively defend against our proposed attack scenarios. We evaluate TrojFlow on CIFAR-10 and CelebA datasets, our experiments show that our method can compromise FMs with high utility and specificity, and can easily break through existing defense mechanisms. 

**Abstract (ZH)**: 基于流的生成模型 (Flow-based Generative Models, FMs) 已经迅速发展成为将噪声映射到数据的方法。其高效的训练和采样过程使其在多个领域得到了广泛应用。FMs 可以被视为扩散模型 (Diffusion Models, DMs) 的变体。同时，先前的研究表明，DMs 对于恶意植入模式触发的后门攻击（Trojan/Backdoor attacks）非常脆弱，这是一种通过恶意嵌入的模式在模型输入上启动的输出操纵攻击。我们发现，针对生成模型的后门攻击本质上等同于将来自后门分布的数据转换到目标分布的任务，FMs 具有拟合任意两个分布的独特能力，这极大简化了攻击 FMs 的训练和采样设置，使它们成为后门攻击的天然目标。在本文中，我们提出了 TrojFlow，探索 FMs 的脆弱性，通过后门攻击进行研究。特别是，我们考虑了各种攻击设置及其组合，并深入探讨现有的 DMs 防御方法是否能有效防御我们提出的攻击场景。我们使用 CIFAR-10 和 CelebA 数据集对 TrojFlow 进行了评估，实验结果表明，我们的方法能够高效且针对性地破坏 FMs，并且能够轻松突破现有的防御机制。 

---
# Speech Retrieval-Augmented Generation without Automatic Speech Recognition 

**Title (ZH)**: 无需自动语音识别的演讲检索增强生成 

**Authors**: Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi, Srikanth Ronanki, Kyu Han  

**Link**: [PDF](https://arxiv.org/pdf/2412.16500)  

**Abstract**: One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)--based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts. 

**Abstract (ZH)**: 以下是对这段论文内容或标题的翻译，符合学术规范：

在语音数据上进行问答（Question Answering, QA）的一种常见方法是首先使用自动语音识别（Automatic Speech Recognition, ASR）对语音进行转录，然后在转录文本的基础上应用文本增强生成（Retrieval-Augmented Generation, RAG）方法。虽然这种串行管道在许多实际应用场景中已被证明是有效的，但ASR的错误可能会传播到检索和生成步骤。为克服这一限制，我们提出了一种名为SpeechRAG的新框架，专门用于处理口头数据的开放式问答问题。我们提出的方法是将一个预训练的语音编码器微调为一个连接到冻结的大语言模型（Large Language Model, LLM）的语音适配器。通过对文本和语音的嵌入空间进行对齐，我们的语音检索器可以直接从基于文本查询的声音片段中进行检索，利用冻结的文本检索器的检索能力。我们的实验表明，直接使用语音的检索方法不会劣于基于文本的基线方法，并且在使用ASR时，能够比串联系统表现得更好。在生成阶段，我们使用一个语音语言模型（Speech Language Model, SLM）作为生成器，条件是基于音频片段而非转录文本。在转录的Word Error Rate（WER）较高的情况下，这种方法在没有微调SLM的情况下，优于基于文本的串联模型。 

---
# Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation 

**Title (ZH)**: 使用日间到夜间风格转换和无标签增强提高夜间车辆检测能力 

**Authors**: Yunxiang Yang, Hao Zhen, Yongcan Huang, Jidong J. Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16478)  

**Abstract**: Existing deep learning-based object detection models perform well under daytime conditions but face significant challenges at night, primarily because they are predominantly trained on daytime images. Additionally, training with nighttime images presents another challenge: even human annotators struggle to accurately label objects in low-light conditions. This issue is particularly pronounced in transportation applications, such as detecting vehicles and other objects of interest on rural roads at night, where street lighting is often absent, and headlights may introduce undesirable glare. This study addresses these challenges by introducing a novel framework for labeling-free data augmentation, leveraging CARLA-generated synthetic data for day-to-night image style transfer. Specifically, the framework incorporates the Efficient Attention Generative Adversarial Network for realistic day-to-night style transfer and uses CARLA-generated synthetic nighttime images to help the model learn vehicle headlight effects. To evaluate the efficacy of the proposed framework, we fine-tuned the YOLO11 model with an augmented dataset specifically curated for rural nighttime environments, achieving significant improvements in nighttime vehicle detection. This novel approach is simple yet effective, offering a scalable solution to enhance AI-based detection systems in low-visibility environments and extend the applicability of object detection models to broader real-world contexts. 

**Abstract (ZH)**: 现有的基于深度学习的目标检测模型在白天表现良好，但在夜间却面临显著挑战，主要原因在于这些模型主要是在白天图像上进行训练。此外，在夜间图像上进行训练还存在另一个挑战：即使是由人类注释员标注的低光照条件下的图像也难以准确标注物体。这一问题在交通运输应用中尤为突出，例如，在没有街道照明的农村道路上检测车辆和其他感兴趣的目标时，前灯可能会引入不希望的眩光。本研究通过引入一种无需标注的数据增强新框架来应对这些挑战，该框架利用CARLA生成的合成数据进行日夜图像风格转移。具体来说，该框架采用了高效注意力生成对抗网络进行现实的日夜风格转换，并使用CARLA生成的合成夜间图像来帮助模型学习前灯效果。为了评估该提出框架的有效性，我们使用专门为农村夜间环境定制的增强数据集对YOLO11模型进行了微调，取得了夜间车辆检测性能的显著提升。这一新颖的方法简单而有效，提供了一种可扩展的解决方案，以增强在低能见度环境中的人工智能检测系统，并将目标检测模型的应用扩展到更广泛的现实世界场景。 

---
# When Can Proxies Improve the Sample Complexity of Preference Learning? 

**Title (ZH)**: 当代理能改善偏好学习的样本复杂度？ 

**Authors**: Yuchen Zhu, Daniel Augusto de Souza, Zhengyan Shi, Mengyue Yang, Pasquale Minervini, Alexander D'Amour, Matt J. Kusner  

**Link**: [PDF](https://arxiv.org/pdf/2412.16475)  

**Abstract**: We address the problem of reward hacking, where maximising a proxy reward does not necessarily increase the true reward. This is a key concern for Large Language Models (LLMs), as they are often fine-tuned on human preferences that may not accurately reflect a true objective. Existing work uses various tricks such as regularisation, tweaks to the reward model, and reward hacking detectors, to limit the influence that such proxy preferences have on a model. Luckily, in many contexts such as medicine, education, and law, a sparse amount of expert data is often available. In these cases, it is often unclear whether the addition of proxy data can improve policy learning. We outline a set of sufficient conditions on proxy feedback that, if satisfied, indicate that proxy data can provably improve the sample complexity of learning the ground truth policy. These conditions can inform the data collection process for specific tasks. The result implies a parameterisation for LLMs that achieves this improved sample complexity. We detail how one can adapt existing architectures to yield this improved sample complexity. 

**Abstract (ZH)**: 我们解决了奖励欺骗问题，即最大化代理奖励并不一定增加真正的奖励。这在大规模语言模型（LLMs）中是一个关键问题，因为它们通常根据人类偏好进行微调，而这些偏好可能不准确地反映真实的优化目标。现有研究通过使用各种技巧，如正则化、奖励模型的调整以及奖励欺骗检测器，来限制代理偏好对模型的影响力。幸运的是，在医学、教育和法律等领域，通常可以获得少量专家数据。在这种情况下，通常不清楚代理数据的增加是否能提高策略学习的数据效率。我们概述了一组充分条件，如果满足这些条件，可以证明代理数据可以提高学习底层真实策略的数据复杂度。这些条件可以为特定任务的数据收集过程提供指导。这一结果表明，一种参数化模型能够实现这种改进的数据复杂度。我们详细说明了如何适应现有架构以实现这种改进的数据复杂度。 

---
# The Evolving Usage of GenAI by Computing Students 

**Title (ZH)**: 计算专业学生使用生成式AI的 evolving 使用情况 

**Authors**: Irene Hou, Hannah Vy Nguyen, Owen Man, Stephen MacNeil  

**Link**: [PDF](https://arxiv.org/pdf/2412.16453)  

**Abstract**: Help-seeking is a critical aspect of learning and problem-solving for computing students. Recent research has shown that many students are aware of generative AI (GenAI) tools; however, there are gaps in the extent and effectiveness of how students use them. With over two years of widespread GenAI usage, it is crucial to understand whether students' help-seeking behaviors with these tools have evolved and how. This paper presents findings from a repeated cross-sectional survey conducted among computing students across North American universities (n=95). Our results indicate shifts in GenAI usage patterns. In 2023, 34.1% of students (n=47) reported never using ChatGPT for help, ranking it fourth after online searches, peer support, and class forums. By 2024, this figure dropped sharply to 6.3% (n=48), with ChatGPT nearly matching online search as the most commonly used help resource. Despite this growing prevalence, there has been a decline in students' hourly and daily usage of GenAI tools, which may be attributed to a common tendency to underestimate usage frequency. These findings offer new insights into the evolving role of GenAI in computing education, highlighting its increasing acceptance and solidifying its position as a key help resource. 

**Abstract (ZH)**: 寻求帮助是计算专业学生学习和问题解决的一个关键方面。最近的研究表明，许多学生已经了解生成式人工智能（GenAI）工具，但他们在使用这些工具方面的程度和有效性存在差距。随着超过两年的广泛GenAI使用，了解学生在这些工具上的帮助寻求行为是否以及如何发生变化变得至关重要。本文通过对北美各大学计算专业学生进行重复横断面调查（样本量为95），展示了GenAI使用模式的变化。结果显示，在2023年，34.1%的学生（n=47）表示从未使用ChatGPT寻求帮助，这一比例仅次于在线搜索、同辈支持和课堂论坛。到2024年，这一数字急剧下降至6.3%（n=48），ChatGPT几乎与在线搜索齐平，成为最常用的帮助资源之一。尽管GenAI的使用正在增长，但学生每小时和每天使用GenAI工具的时间却有所下降，这可能反映了低估使用频率的普遍倾向。这些发现为了解GenAI在计算教育中的演变角色提供了新的见解，突显了其日益被接受的地位，并确定了它作为关键帮助资源的作用。 

---
# Correcting Large Language Model Behavior via Influence Function 

**Title (ZH)**: 通过影响函数纠正大型语言模型的行为 

**Authors**: Han Zhang, Zhuo Zhang, Yi Zhang, Yuanzhao Zhai, Hanyang Peng, Yu Lei, Yue Yu, Hui Wang, Bin Liang, Lin Gui, Ruifeng Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16451)  

**Abstract**: Recent advancements in AI alignment techniques have significantly improved the alignment of large language models (LLMs) with static human preferences. However, the dynamic nature of human preferences can render some prior training data outdated or even erroneous, ultimately causing LLMs to deviate from contemporary human preferences and societal norms. Existing methodologies, whether they involve the curation of new data for continual alignment or the manual correction of outdated data for re-alignment, demand costly human resources. To address this challenge, we propose a novel approach, Large Language Model Behavior Correction with Influence Function Recall and Post-Training (LANCET), which requires no human involvement. LANCET consists of two phases: (1) using influence functions to identify the training data that significantly impact undesirable model outputs, and (2) applying an Influence function-driven Bregman Optimization (IBO) technique to adjust the model's behavior based on these influence distributions. Our experiments demonstrate that LANCET effectively and efficiently correct inappropriate behaviors of LLMs. Furthermore, LANCET can outperform methods that rely on collecting human preferences, and it enhances the interpretability of learning human preferences within LLMs. 

**Abstract (ZH)**: 近年来，AI对齐技术的进展显著提高了大型语言模型（LLMs）与静态人类偏好的对齐程度。然而，人类偏好的动态性质可能导致部分先前的训练数据变得过时甚至错误，最终使LLMs偏离当代人类偏好和社会规范。现有的方法，无论是通过持续获取新数据进行对齐，还是通过手动修正过时数据进行再对齐，都需要大量的人力资源。为解决这一挑战，我们提出了一种新颖的方法——Large Language Model Behavior Correction with Influence Function Recall and Post-Training（LANCET），该方法完全不需要人工干预。LANCET包含两个阶段：（1）利用影响函数识别对模型输出有重大影响的训练数据；（2）使用基于影响函数的Bregman优化（IBO）技术，根据这些影响分布调整模型的行为。我们的实验结果表明，LANCET能够有效地、高效地纠正LLMs的不当行为。此外，LANCET在性能上超过了依赖于收集人类偏好数据的方法，并能够增强学习人类偏好在LLMs中的可解释性。 

---
# A Generalizable Anomaly Detection Method in Dynamic Graphs 

**Title (ZH)**: 一种动态图中的可泛化异常检测方法 

**Authors**: Xiao Yang, Xuejiao Zhao, Zhiqi Shen  

**Link**: [PDF](https://arxiv.org/pdf/2412.16447)  

**Abstract**: Anomaly detection aims to identify deviations from normal patterns within data. This task is particularly crucial in dynamic graphs, which are common in applications like social networks and cybersecurity, due to their evolving structures and complex relationships. Although recent deep learning-based methods have shown promising results in anomaly detection on dynamic graphs, they often lack of generalizability. In this study, we propose GeneralDyG, a method that samples temporal ego-graphs and sequentially extracts structural and temporal features to address the three key challenges in achieving generalizability: Data Diversity, Dynamic Feature Capture, and Computational Cost. Extensive experimental results demonstrate that our proposed GeneralDyG significantly outperforms state-of-the-art methods on four real-world datasets. 

**Abstract (ZH)**: 异常检测旨在识别数据中偏离正常模式的偏差。由于动态图在社交网络和网络安全等应用中的演变结构和复杂关系，这一任务尤为重要。尽管基于深度学习的方法在动态图上的异常检测方面表现出色，但它们通常缺乏普遍适用性。在这项研究中，我们提出了GeneralDyG 方法，该方法通过采样时态中心图并依次提取结构和时态特征，来应对实现普遍适用性的三个关键挑战：数据多样性、动态特征捕捉以及计算成本。广泛的实验结果表明，我们提出的GeneralDyG 在四个真实世界数据集上显著优于最先进的方法。 

---
# Sensitive Image Classification by Vision Transformers 

**Title (ZH)**: 基于视觉变换器的敏感图像分类 

**Authors**: Hanxian He, Campbell Wilson, Thanh Thi Nguyen, Janis Dalins  

**Link**: [PDF](https://arxiv.org/pdf/2412.16446)  

**Abstract**: When it comes to classifying child sexual abuse images, managing similar inter-class correlations and diverse intra-class correlations poses a significant challenge. Vision transformer models, unlike conventional deep convolutional network models, leverage a self-attention mechanism to capture global interactions among contextual local elements. This allows them to navigate through image patches effectively, avoiding incorrect correlations and reducing ambiguity in attention maps, thus proving their efficacy in computer vision tasks. Rather than directly analyzing child sexual abuse data, we constructed two datasets: one comprising clean and pornographic images and another with three classes, which additionally include images indicative of pornography, sourced from Reddit and Google Open Images data. In our experiments, we also employ an adult content image benchmark dataset. These datasets served as a basis for assessing the performance of vision transformer models in pornographic image classification. In our study, we conducted a comparative analysis between various popular vision transformer models and traditional pre-trained ResNet models. Furthermore, we compared them with established methods for sensitive image detection such as attention and metric learning based CNN and Bumble. The findings demonstrated that vision transformer networks surpassed the benchmark pre-trained models, showcasing their superior classification and detection capabilities in this task. 

**Abstract (ZH)**: 在对孩子性虐待图像进行分类时，管理相似类间的相关性和多样类内的相关性构成了显著挑战。与传统的深度卷积网络模型不同，视觉变换器模型利用自注意力机制来捕捉上下文局部元素之间的全局互动，从而使它们能够有效地穿越图像块，避免错误的相关性并减少注意力图中的模糊性，因此在计算机视觉任务中表现出色。我们并未直接分析孩子性虐待数据，而是构建了两个数据集：一个由干净图像和色情图像组成，另一个包含三个类别，还包含来自Reddit和Google Open Images数据的色情图像指示图像。在我们的实验中，我们还使用了一个成人内容图像基准数据集。这些数据集被用作评估视觉变换器模型在色情图像分类性能的基础。在此研究中，我们对比分析了多种流行的视觉变换器模型与传统的预训练ResNet模型。此外，我们还将这些模型与现有的敏感图像检测方法，如基于注意力和度量学习的CNN和Bumble进行了比较。研究结果表明，视觉变换器网络超越了基准预训练模型，展示了它们在这项任务中出色的分类和检测能力。 

---
# Has LLM Reached the Scaling Ceiling Yet? Unified Insights into LLM Regularities and Constraints 

**Title (ZH)**: larg语言模型（LLM）是否已经触及扩展极限？关于LLM规律性和约束条件的统一见解 

**Authors**: Charles Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.16443)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their scalability raises a critical question: Have we reached the scaling ceiling? This paper addresses this pivotal question by developing a unified theoretical framework that integrates mathematical and statistical insights to explain the scaling dynamics of LLMs. We present: 1. Central Limit Theorem (CLT) for Hidden Representations: We show that noise in hidden representations scales inversely with context size, explaining stabilization effects and the limits of context length improvements. 2. Bias-Variance Decomposition: We decompose next-token prediction loss into irreducible entropy, capacity-driven bias, and finite sample variance, revealing trade-offs where scaling yields diminishing returns. 3. Emergent SNR Thresholds: By defining signal-to-noise ratio (SNR), we quantify how capabilities emerge abruptly once SNR surpasses a threshold, offering insights into when scaling becomes less effective. Through this framework, we conclude that while LLMs have not reached an absolute scaling ceiling, practical constraints are increasingly prominent: diminishing returns, resource inefficiencies, and data limitations. Future progress will require a shift from brute-force scaling to innovations in architecture, data quality, and training paradigms. This work provides a roadmap for guiding the efficient development of next-generation LLMs and advancing the field beyond traditional scaling strategies.
Keywords: Large Language Models; Scaling Ceiling; Central Limit Theorem; Bias-Variance Trade-Off; Signal-to-Noise Ratio; Emergent Capabilities 

**Abstract (ZH)**: 大型语言模型（LLMs）展现出了惊人的能力，但其扩展性引发了一个关键问题：我们是否已经达到了扩展上限？本文通过构建一个统一的理论框架来回答这一重要问题，该框架结合了数学和统计学的见解，以解释LLMs的扩展动态。我们提出了以下几点：

1. **隐表示的中心极限定理（CLT）**：我们证明了隐藏表示中的噪声与上下文大小成反比，解释了稳定效应并指出了上下文长度改进的极限。
   
2. **偏差-方差分解**：我们将下一个词预测损失分解为不可约熵、容量驱动的偏差和有限样本方差，揭示了扩展可能导致收益递减的权衡。
   
3. **浮现的SNR阈值**：通过定义信噪比（SNR），我们量化了SNR超过阈值时能力的突然涌现情况，提供了关于何时扩展变得不再有效的一些见解。
   
借助这一框架，我们得出结论：虽然LLMs尚未达到绝对的扩展上限，但实际限制越来越突出：收益递减、资源利用率低下以及数据限制。未来的研究将需要从粗暴的扩展转向架构、数据质量和训练范式的创新。这项工作提供了一条路线图，用于指导下一代LLMs的高效发展，并超越传统的扩展策略。

关键词：大型语言模型；扩展上限；中心极限定理；偏差-方差权衡；信噪比；浮现能力 

---
# Learning Cross-Task Generalities Across Graphs via Task-trees 

**Title (ZH)**: 通过任务树在图之间学习跨任务的一般性 

**Authors**: Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye  

**Link**: [PDF](https://arxiv.org/pdf/2412.16441)  

**Abstract**: Foundation models aim to create general, cross-task, and cross-domain machine learning models by pretraining on large-scale datasets to capture shared patterns or concepts (generalities), such as contours, colors, textures, and edges in images, or tokens, words, and sentences in text. However, discovering generalities across graphs remains challenging, which has hindered the development of graph foundation models. To tackle this challenge, in this paper, we propose a novel approach to learn generalities across graphs via task-trees. Specifically, we first define the basic learning instances in graphs as task-trees and assume that the generalities shared across graphs are, at least partially, preserved in the task-trees of the given graphs. To validate the assumption, we first perform a theoretical analysis of task-trees in terms of stability, transferability, and generalization. We find that if a graph neural network (GNN) model is pretrained on diverse task-trees through a reconstruction task, it can learn sufficient transferable knowledge for downstream tasks using an appropriate set of fine-tuning samples. To empirically validate the assumption, we further instantiate the theorems by developing a cross-task, cross-domain graph foundation model named Graph generality Identifier on task-Trees (GIT). The extensive experiments over 30 graphs from five domains demonstrate the effectiveness of GIT in fine-tuning, in-context learning, and zero-shot learning scenarios. Particularly, the general GIT model pretrained on large-scale datasets can be quickly adapted to specific domains, matching or even surpassing expert models designed for those domains. Our data and code are available at this https URL. 

**Abstract (ZH)**: 基础模型旨在通过在大规模数据集上进行预训练，构建通用的、跨任务和跨领域的机器学习模型，以捕捉共享的模式或概念（通用性），例如图像中的轮廓、颜色、纹理和边缘，或文本中的标记、单词和句子。然而，跨图发现通用性仍然面临挑战，这阻碍了图基础模型的发展。为应对这一挑战，本文提出了通过任务树学习图中的通用性的新型方法。具体来说，我们首先将图中的基本学习实例定义为任务树，并假设图之间共享的通用性至少部分保留在所给图的任务树中。为了验证这一假设，我们首先从稳定性和知识迁移性等角度对任务树进行了理论分析，并发现如果通过重构任务对图神经网络（GNN）模型进行预训练，则可以通过适当的一组微调样本学习到足够的可迁移知识用于下层任务。为了经验验证这一假设，我们进一步通过开发一个名为任务树上的图通用性标识器（Graph generality Identifier on task-Trees，简称GIT）的跨任务和跨领域的图基础模型进行了实例化。在五个领域共30个图的广泛实验中，我们展示了GIT在微调、上下文学习和零样本学习场景中的有效性。特别是，通过大规模数据集预训练的通用GIT模型可以快速适应特定领域，甚至可以超越专门为这些领域设计的专家模型。我们的数据和代码可在 [此处提供链接] 获取。 

---
# Object Detection Approaches to Identifying Hand Images with High Forensic Values 

**Title (ZH)**: 基于对象检测的方法识别具有高 forensic 值的手部图像 

**Authors**: Thanh Thi Nguyen, Campbell Wilson, Imad Khan, Janis Dalins  

**Link**: [PDF](https://arxiv.org/pdf/2412.16431)  

**Abstract**: Forensic science plays a crucial role in legal investigations, and the use of advanced technologies, such as object detection based on machine learning methods, can enhance the efficiency and accuracy of forensic analysis. Human hands are unique and can leave distinct patterns, marks, or prints that can be utilized for forensic examinations. This paper compares various machine learning approaches to hand detection and presents the application results of employing the best-performing model to identify images of significant importance in forensic contexts. We fine-tune YOLOv8 and vision transformer-based object detection models on four hand image datasets, including the 11k hands dataset with our own bounding boxes annotated by a semi-automatic approach. Two YOLOv8 variants, i.e., YOLOv8 nano (YOLOv8n) and YOLOv8 extra-large (YOLOv8x), and two vision transformer variants, i.e., DEtection TRansformer (DETR) and Detection Transformers with Assignment (DETA), are employed for the experiments. Experimental results demonstrate that the YOLOv8 models outperform DETR and DETA on all datasets. The experiments also show that YOLOv8 approaches result in superior performance compared with existing hand detection methods, which were based on YOLOv3 and YOLOv4 models. Applications of our fine-tuned YOLOv8 models for identifying hand images (or frames in a video) with high forensic values produce excellent results, significantly reducing the time required by forensic experts. This implies that our approaches can be implemented effectively for real-world applications in forensics or related fields. 

**Abstract (ZH)**: 法医学在法律调查中发挥着关键作用，而运用先进的技术手段，如基于机器学习方法的对象检测，可以提升法医学分析的效率和准确性。人类的手掌是独一无二的，可以留下独特的图案、痕迹或指纹，这些都可以用于法医学检查。本文比较了各种机器学习方法在手掌检测中的应用，并展示了使用表现最佳的模型识别法医学背景下具有重要意义的图像的应用结果。我们在四个包含手掌图像的数据集上对YOLOv8和基于视觉变换器的对象检测模型进行了微调，其中包括我们半自动化标注的11k手掌数据集。实验中采用的YOLOv8变体包括YOLOv8n（纳米版）和YOLOv8x（超大版），以及基于视觉变换器的DEtection TRansformer（DETR）和Detection Transformers with Assignment（DETA）变体。实验结果表明，YOLOv8模型在所有数据集上的表现均优于DETR和DETA模型。实验还显示，YOLOv8方法在手掌检测方面的性能优于基于YOLOv3和YOLOv4模型的现有手掌检测方法。我们微调的YOLOv8模型在识别具有高法医学价值的手掌图像（或视频中的帧）方面表现出优异的结果，显著减少了法医学专家所需的时间。这表明我们的方法可以在法医学或相关领域的实际应用中得到有效实施。 

---
# LearnLM: Improving Gemini for Learning 

**Title (ZH)**: LearnLM：提高Gemini的學習能力 

**Authors**: LearnLM Team Google, Abhinit Modi, Aditya Srikanth Veerubhotla, Aliya Rysbek, Andrea Huber, Brett Wiltshire, Brian Veprek, Daniel Gillick, Daniel Kasenberg, Derek Ahmed, Irina Jurenka, James Cohan, Jennifer She, Julia Wilkowski, Kaiz Alarakyia, Kevin McKee, Lisa Wang, Markus Kunesch, Mike Schaekermann, Miruna Pîslar, Nikhil Joshi, Parsa Mahmoudieh, Paul Jhun, Sara Wiltberger, Shakir Mohamed, Shashank Agarwal, Shubham Milind Phal, Sun Jae Lee, Theofilos Strinopoulos, Wei-Jen Ko, Amy Wang, Ankit Anand, Avishkar Bhoopchand, Dan Wild, Divya Pandya, Filip Bar, Garth Graham, Holger Winnemoeller, Mahvish Nagda, Prateek Kolhar, Renee Schneider, Shaojian Zhu, Stephanie Chan, Steve Yadlowsky, Viknesh Sounderajah, Yannis Assael  

**Link**: [PDF](https://arxiv.org/pdf/2412.16429)  

**Abstract**: Today's generative AI systems are tuned to present information by default rather than engage users in service of learning as a human tutor would. To address the wide range of potential education use cases for these systems, we reframe the challenge of injecting pedagogical behavior as one of \textit{pedagogical instruction following}, where training and evaluation examples include system-level instructions describing the specific pedagogy attributes present or desired in subsequent model turns. This framing avoids committing our models to any particular definition of pedagogy, and instead allows teachers or developers to specify desired model behavior. It also clears a path to improving Gemini models for learning -- by enabling the addition of our pedagogical data to post-training mixtures -- alongside their rapidly expanding set of capabilities. Both represent important changes from our initial tech report. We show how training with pedagogical instruction following produces a LearnLM model (available on Google AI Studio) that is preferred substantially by expert raters across a diverse set of learning scenarios, with average preference strengths of 31\% over GPT-4o, 11\% over Claude 3.5, and 13\% over the Gemini 1.5 Pro model LearnLM was based on. 

**Abstract (ZH)**: 当今的生成式AI系统默认以呈现信息的方式运行，而不是像人类导师那样服务于学习过程。为了应对这些系统在教育场景中的广泛应用，我们将注入教学行为的挑战重新定义为“教学指令遵循”，其中训练和评估示例包括系统级指令，说明后续模型转变中所要求的具体教学属性。这种重新定义避免了将模型绑定到特定的教学定义，而是允许教师或开发人员指定所需的行为。这还为通过向训练后混合中添加我们的教学数据来改善Gemini模型的学習能力开辟了途径——在它们迅速扩展的能力集中增加这一途径。这两个方面都代表了与我们初始技术报告中所述的重要变化。我们展示了使用教学指令遵循进行训练产生的LearnLM模型（可在Google AI Studio获取），该模型在多种学习场景中得到了专家评审者的高度偏好，与GPT-4o相比平均偏好度为31%，与Claude 3.5相比高11%，与Gemini 1.5 Pro模型相比高13%。LearnLM是基于Gemini 1.5 Pro模型构建的。 

---
# Deepfake detection, image manipulation detection, fairness, generalization 

**Title (ZH)**: 以下是这些术语翻译成中文后的版本，符合学术规范：

1. Deepfake检测
2. 图像篡改检测
3. 公平性
4. 通用性 

**Authors**: Uzoamaka Ezeakunne, Chrisantus Eze, Xiuwen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16428)  

**Abstract**: Despite the progress made in deepfake detection research, recent studies have shown that biases in the training data for these detectors can result in varying levels of performance across different demographic groups, such as race and gender. These disparities can lead to certain groups being unfairly targeted or excluded. Traditional methods often rely on fair loss functions to address these issues, but they under-perform when applied to unseen datasets, hence, fairness generalization remains a challenge. In this work, we propose a data-driven framework for tackling the fairness generalization problem in deepfake detection by leveraging synthetic datasets and model optimization. Our approach focuses on generating and utilizing synthetic data to enhance fairness across diverse demographic groups. By creating a diverse set of synthetic samples that represent various demographic groups, we ensure that our model is trained on a balanced and representative dataset. This approach allows us to generalize fairness more effectively across different domains. We employ a comprehensive strategy that leverages synthetic data, a loss sharpness-aware optimization pipeline, and a multi-task learning framework to create a more equitable training environment, which helps maintain fairness across both intra-dataset and cross-dataset evaluations. Extensive experiments on benchmark deepfake detection datasets demonstrate the efficacy of our approach, surpassing state-of-the-art approaches in preserving fairness during cross-dataset evaluation. Our results highlight the potential of synthetic datasets in achieving fairness generalization, providing a robust solution for the challenges faced in deepfake detection. 

**Abstract (ZH)**: 尽管在深度伪造检测研究方面取得了进步，但最近的研究表明，这些检测器的训练数据中的偏见会导致不同的人口统计群体（如种族和性别）之间性能的差异。这些差异可能导致某些群体被不公平地针对或排除在外。传统方法通常依赖公平的损失函数来解决这些问题，但在应用于未见过的数据集时表现不佳，因此公平性的泛化仍然是一个挑战。在此项工作中，我们提出了一种基于数据的框架，利用合成数据和模型优化来解决深度伪造检测中的公平性泛化问题。我们的方法专注于生成和利用合成数据以增强不同人口统计群体间的公平性。通过创建代表各种人口统计群体的多样化合成样本集，我们确保模型能够在平衡且具代表性的数据集上进行训练。这种做法使我们能够更好地在不同领域泛化公平性。我们采用了一种全面的策略，利用合成数据、损失尖锐度感知优化管道和多任务学习框架，以创建一个更公平的训练环境，这有助于在数据集内部和跨数据集评估中保持公平性。基准深度伪造检测数据集上的广泛实验表明，我们的方法在跨数据集评估中有效地保护了公平性，超越了现有最佳方法。我们的结果凸显了合成数据在实现公平性泛化方面的潜力，为深度伪造检测面临的挑战提供了一个稳健的解决方案。 

---
# Patherea: Cell Detection and Classification for the 2020s 

**Title (ZH)**: Patherea：20年代的细胞检测与分类 

**Authors**: Dejan Štepec, Maja Jerše, Snežana Đokić, Jera Jeruc, Nina Zidar, Danijel Skočaj  

**Link**: [PDF](https://arxiv.org/pdf/2412.16425)  

**Abstract**: This paper presents a Patherea, a framework for point-based cell detection and classification that provides a complete solution for developing and evaluating state-of-the-art approaches. We introduce a large-scale dataset collected to directly replicate a clinical workflow for Ki-67 proliferation index estimation and use it to develop an efficient point-based approach that directly predicts point-based predictions, without the need for intermediate representations. The proposed approach effectively utilizes point proposal candidates with the hybrid Hungarian matching strategy and a flexible architecture that enables the usage of various backbones and (pre)training strategies. We report state-of-the-art results on existing public datasets - Lizard, BRCA-M2C, BCData, and the newly proposed Patherea dataset. We show that the performance on existing public datasets is saturated and that the newly proposed Patherea dataset represents a significantly harder challenge for the recently proposed approaches. We also demonstrate the effectiveness of recently proposed pathology foundational models that our proposed approach can natively utilize and benefit from. We also revisit the evaluation protocol that is used in the broader field of cell detection and classification and identify the erroneous calculation of performance metrics. Patherea provides a benchmarking utility that addresses the identified issues and enables a fair comparison of different approaches. The dataset and the code will be publicly released upon acceptance. 

**Abstract (ZH)**: 本文提出了一个基于点的细胞检测与分类框架 Patherea，该框架提供了一种全面的方法来开发和评估最先进的细胞检测与分类技术。我们引入了一个大规模数据集，直接复制了Ki-67 生长指数估计的临床工作流，并利用该数据集开发了一种直接预测基于点的预测结果的有效方法，无需中间表示。所提出的方法通过结合混合匈牙利匹配策略和灵活的架构有效地利用了点建议候选集，并且允许使用各种骨干网络和（预）训练策略。我们在现有的公共数据集 Lizard、BRCA-M2C、BCData 以及新提出的 Patherea 数据集上报告了最先进的结果。我们表明，在现有公共数据集上的性能已达到饱和，并且新提出的 Patherea 数据集对最近提出的细胞检测与分类方法构成了更难的挑战。我们还展示了最近提出的基础病理模型的有效性，这些模型可以自然利用并从中受益。此外，我们重新审视了细胞检测与分类领域中使用的评估协议，并发现了性能指标计算中的错误。Patherea 作为基准测试工具，解决了这些问题，使不同方法的公平比较成为可能。该数据集和代码将在接受后公开发布。 

---
# Technical Report: Small Language Model for Japanese Clinical and Medicine 

**Title (ZH)**: 技术报告：日语临床和医学小语言模型 

**Authors**: Shogo Watanabe  

**Link**: [PDF](https://arxiv.org/pdf/2412.16423)  

**Abstract**: This report presents a small language model (SLM) for Japanese clinical and medicine, named NCVC-slm-1. This 1B parameters model was trained using Japanese text classified to be of high-quality. Moreover, NCVC-slm-1 was augmented with respect to clinical and medicine content that includes the variety of diseases, drugs, and examinations. Using a carefully designed pre-processing, a specialized morphological analyzer and tokenizer, this small and light-weight model performed not only to generate text but also indicated the feasibility of understanding clinical and medicine text. In comparison to other large language models, a fine-tuning NCVC-slm-1 demonstrated the highest scores on 6 tasks of total 8 on JMED-LLM. According to this result, SLM indicated the feasibility of performing several downstream tasks in the field of clinical and medicine. Hopefully, NCVC-slm-1 will be contributed to develop and accelerate the field of clinical and medicine for a bright future. 

**Abstract (ZH)**: 本报告介绍了一个用于日语临床和医学领域的小型语言模型（SLM），名为NCVC-slm-1。该模型包含1亿参数，并利用高质量的日语文本进行训练。此外，NCVC-slm-1 对临床和医学内容进行了扩充，包括各种疾病、药物和检查项目。通过精心设计的预处理，使用专门的形态分析器和分词器，这一小型轻量级模型不仅能够生成文本，还表明了理解临床和医学文本的可能性。与其它大型语言模型相比，对NCVC-slm-1进行微调后，在JMED-LLM的8项任务中，它在6项任务上取得了最高分。根据这一结果，小型语言模型表明了在临床和医学领域执行多个下游任务的可行性。希望NCVC-slm-1能够为临床和医学领域的发展与进步贡献力量，促进这一领域的未来繁荣。 

---
# Uncertainty Quantification in Continual Open-World Learning 

**Title (ZH)**: 持续开放世界学习中的不确定性量化 

**Authors**: Amanda S. Rios, Ibrahima J. Ndiour, Parual Datta, Jaroslaw Sydir, Omesh Tickoo, Nilesh Ahuja  

**Link**: [PDF](https://arxiv.org/pdf/2412.16409)  

**Abstract**: AI deployed in the real-world should be capable of autonomously adapting to novelties encountered after deployment. Yet, in the field of continual learning, the reliance on novelty and labeling oracles is commonplace albeit unrealistic. This paper addresses a challenging and under-explored problem: a deployed AI agent that continuously encounters unlabeled data - which may include both unseen samples of known classes and samples from novel (unknown) classes - and must adapt to it continuously. To tackle this challenge, we propose our method COUQ "Continual Open-world Uncertainty Quantification", an iterative uncertainty estimation algorithm tailored for learning in generalized continual open-world multi-class settings. We rigorously apply and evaluate COUQ on key sub-tasks in the Continual Open-World: continual novelty detection, uncertainty guided active learning, and uncertainty guided pseudo-labeling for semi-supervised CL. We demonstrate the effectiveness of our method across multiple datasets, ablations, backbones and performance superior to state-of-the-art. 

**Abstract (ZH)**: 在实际应用中部署的AI系统应当能够在部署后自主适应所遇到的新颖情况。然而，在持续学习领域，对新颖性及标签或acles的依赖虽然常见但却是不现实的。本文致力于解决一个具有挑战性且尚未充分探索的问题：一个部署后不断遇到无标签数据的AI代理，这些数据可能既包括未知类别的未见样本，也包括已知类别的未见样本，并且该代理必须不断适应这些数据。为应对这一挑战，我们提出了一种名为“COUQ：通用开放世界不确定性量化”的方法，这是一种针对泛化的持续开放世界多分类学习场景设计的迭代不确定性估计算法。我们严格地在持续开放世界任务的关键子任务上应用并评估了COUQ，包括持续新颖性检测、不确定性指导的主动学习以及不确定性指导的伪标签生成，以支持半监督持续学习。我们展示了该方法在多个数据集上的有效性，并在消融实验、底层架构和性能方面均优于现有最佳方法。 

---
# Learning Disease Progression Models That Capture Health Disparities 

**Title (ZH)**: 学习能够捕获健康差异的疾病进展模型 

**Authors**: Erica Chiang, Divya Shanmugam, Ashley N. Beecy, Gabriel Sayer, Nir Uriel, Deborah Estrin, Nikhil Garg, Emma Pierson  

**Link**: [PDF](https://arxiv.org/pdf/2412.16406)  

**Abstract**: Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for disparities produces biased estimates of severity (underestimating severity for disadvantaged groups, for example). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities meaningfully shifts which patients are considered high-risk. 

**Abstract (ZH)**: 疾病进展模型在多种进行性疾病的确诊和治疗中被广泛应用。然而，现有模型的一个重要局限是，它们未能考虑到可能导致观察数据偏差的健康不平等。为解决这一问题，我们开发了一种可解释的贝叶斯疾病进展模型，该模型捕捉了三种关键的健康不平等现象：某些患者群体可能（1）在疾病更为严重时才开始接受治疗；（2）即使在接受治疗的情况下仍显示出更快的疾病进展；或（3）在疾病严重程度较高的情况下接受随访护理的频率较低。我们通过理论和实证研究证明，忽略这些不平等会导致严重程度估计的偏差（例如，低估了不利群体的严重程度）。在心脏病患者的数据集中，我们展示了该模型能够识别面对每种类型不平等的患者群体，并且考虑到这些不平等可以显著改变哪些患者被视为高风险。 

---
# VerSe: Integrating Multiple Queries as Prompts for Versatile Cardiac MRI Segmentation 

**Title (ZH)**: VerSe：将多种查询作为提示以实现多功能心脏MRI分割 

**Authors**: Bangwei Guo, Meng Ye, Yunhe Gao, Bingyu Xin, Leon Axel, Dimitris Metaxas  

**Link**: [PDF](https://arxiv.org/pdf/2412.16381)  

**Abstract**: Despite the advances in learning-based image segmentation approach, the accurate segmentation of cardiac structures from magnetic resonance imaging (MRI) remains a critical challenge. While existing automatic segmentation methods have shown promise, they still require extensive manual corrections of the segmentation results by human experts, particularly in complex regions such as the basal and apical parts of the heart. Recent efforts have been made on developing interactive image segmentation methods that enable human-in-the-loop learning. However, they are semi-automatic and inefficient, due to their reliance on click-based prompts, especially for 3D cardiac MRI volumes. To address these limitations, we propose VerSe, a Versatile Segmentation framework to unify automatic and interactive segmentation through mutiple queries. Our key innovation lies in the joint learning of object and click queries as prompts for a shared segmentation backbone. VerSe supports both fully automatic segmentation, through object queries, and interactive mask refinement, by providing click queries when needed. With the proposed integrated prompting scheme, VerSe demonstrates significant improvement in performance and efficiency over existing methods, on both cardiac MRI and out-of-distribution medical imaging datasets. The code is available at this https URL. 

**Abstract (ZH)**: 尽管在基于学习的图像分割方法方面取得了进步，但从磁共振成像（MRI）中准确分割心脏结构仍然是一个关键挑战。尽管现有的全自动分割方法显示出一定的潜力，但它们仍然需要由人类专家进行大量的手动校正，尤其是在心脏的基底部和尖端等复杂区域。最近的研究致力于开发交互式图像分割方法，使人类能够参与闭环学习过程。然而，这些方法往往是半自动的并且效率低下，特别是在依赖基于点击的提示时，尤其是对于三维心脏MRI体积。为了解决这些局限性，我们提出了一种名为VerSe的通用分割框架，通过多轮查询统一了自动分割和交互分割。我们的关键创新在于联合学习对象查询和点击查询作为共享分割骨干的提示。VerSe既支持通过对象查询实现的全自动分割，又在必要时通过提供点击查询支持交互分割掩模优化。借助提出的综合提示方案，VerSe在心脏MRI和分布外医学成像数据集上显著提高了性能和效率。源代码可通过以下链接获取：this https URL。 

---
# REFA: Reference Free Alignment for multi-preference optimization 

**Title (ZH)**: REFA：参考自由对齐算法用于多偏好优化 

**Authors**: Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16378)  

**Abstract**: We introduce REFA, a family of reference-free alignment methods that optimize over multiple user preferences while enforcing fine-grained length control. Our approach integrates deviation-based weighting to emphasize high-quality responses more strongly, length normalization to prevent trivial short-response solutions, and an EOS-probability regularizer to mitigate dataset-induced brevity biases. Theoretically, we show that under the Uncertainty Reduction with Sequence Length Assertion (URSLA), naive length normalization can still incentivize length-based shortcuts. By contrast, REFA corrects these subtle incentives, guiding models toward genuinely more informative and higher-quality outputs. Empirically, REFA sets a new state-of-the-art among reference-free alignment methods, producing richer responses aligned more closely with human preferences. Compared to a base supervised fine-tuned (SFT) mistral-7b model that achieves 8.4% length-controlled win rate (LC-WR) and 6.2% win rate (WR), our best REFA configuration attains 21.62% LC-WR and 19.87% WR on the AlpacaEval v2 benchmark. This represents a substantial improvement over both the strongest multi-preference baseline, InfoNCA (16.82% LC-WR, 10.44% WR), and the strongest reference-free baseline, SimPO (20.01% LC-WR, 17.65% WR) 

**Abstract (ZH)**: 我们引入了REFA，这是一种无需参考的对齐方法家族，可以在优化多种用户偏好的同时实现细粒度的长度控制。我们的方法结合了基于偏差的权重分配，以更加强调高质量的响应；长度规范化，以防止产生简短的非实质性的响应；以及EOS概率正则化，以减轻数据集引起的简略偏差。理论上，我们证明了在序列长度断言不确定性减少（URSLA）框架下，简单的长度规范化仍然可以激励基于长度的捷径。相比之下，REFA纠正了这些微妙的激励，引导模型产生更具有信息量且质量更高的输出。实验上，REFA在无需参考的对齐方法中达到了新的最佳性能，产生了更丰富且更接近人类偏好的响应。相比于一个基线监督微调（SFT）的mistral-7b模型，其长度控制胜率（LC-WR）为8.4%，胜率（WR）为6.2%，我们的最佳REFA配置在AlpacaEval v2基准上的长度控制胜率为21.62%，胜率为19.87%。这表明REFA显著优于最强的多偏好基线InfoNCA（长度控制胜率16.82%，胜率10.44%）和最强的无需参考的基线SimPO（长度控制胜率20.01%，胜率17.65%）。 

---
# Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's GRACE-FO Verification and Validation 

**Title (ZH)**: 迭代编码-解码VAE在NOAA DART时间序列中的异常检测：一种通过机器学习增强NASA GRACE-FO验证与验证数据完整性的方法 

**Authors**: Kevin Lee  

**Link**: [PDF](https://arxiv.org/pdf/2412.16375)  

**Abstract**: NOAA's Deep-ocean Assessment and Reporting of Tsunamis (DART) data are critical for NASA-JPL's tsunami detection, real-time operations, and oceanographic research. However, these time-series data often contain spikes, steps, and drifts that degrade data quality and obscure essential oceanographic features. To address these anomalies, the work introduces an Iterative Encoding-Decoding Variational Autoencoders (Iterative Encoding-Decoding VAEs) model to improve the quality of DART time series. Unlike traditional filtering and thresholding methods that risk distorting inherent signal characteristics, Iterative Encoding-Decoding VAEs progressively remove anomalies while preserving the data's latent structure. A hybrid thresholding approach further retains genuine oceanographic features near boundaries. Applied to complex DART datasets, this approach yields reconstructions that better maintain key oceanic properties compared to classical statistical techniques, offering improved robustness against spike removal and subtle step changes. The resulting high-quality data supports critical verification and validation efforts for the GRACE-FO mission at NASA-JPL, where accurate surface measurements are essential to modeling Earth's gravitational field and global water dynamics. Ultimately, this data processing method enhances tsunami detection and underpins future climate modeling with improved interpretability and reliability. 

**Abstract (ZH)**: NOAA 的深海评估与海啸报告（DART）数据对于 NASA-JPL 的海啸检测、实时操作和海洋学研究至关重要。然而，这些时间序列数据常常包含尖峰、台阶和漂移，这些都会降低数据质量并掩盖重要的海洋学特征。为解决这些问题，本文引入了迭代编码-解码变分自编码器（Iterative Encoding-Decoding VAEs，IED-VAEs）模型，以改善 DART 时间序列数据的质量。与传统的滤波和阈值方法相比，后者可能会扭曲固有的信号特征，IED-VAEs 能够逐步移除异常值同时保留数据的潜在结构。进一步结合使用混合阈值方法可以保留边界附近的真正海洋特征。该方法应用于复杂的 DART 数据集后，产生的重构能够更好地保持关键海洋属性，相较于经典的统计技术，这种方法在尖峰去除和细微台阶变化方面提供了更好的鲁棒性。高质量的数据支持了 NASA-JPL 的 GRACE-FO 任务的关键验证和验证工作，其中准确的表面测量对于建模地球的重力场和全球水循环至关重要。最终，这种数据处理方法提高了海啸检测的准确性，并为未来的气候建模提供了更解释性和可靠性的支持。 

---
# FairREAD: Re-fusing Demographic Attributes after Disentanglement for Fair Medical Image Classification 

**Title (ZH)**: 公平读取：解缠后的重新融合人口统计学属性以实现公平医疗图像分类 

**Authors**: Yicheng Gao, Jinkui Hao, Bo Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.16373)  

**Abstract**: Recent advancements in deep learning have shown transformative potential in medical imaging, yet concerns about fairness persist due to performance disparities across demographic subgroups. Existing methods aim to address these biases by mitigating sensitive attributes in image data; however, these attributes often carry clinically relevant information, and their removal can compromise model performance-a highly undesirable outcome. To address this challenge, we propose Fair Re-fusion After Disentanglement (FairREAD), a novel, simple, and efficient framework that mitigates unfairness by re-integrating sensitive demographic attributes into fair image representations. FairREAD employs orthogonality constraints and adversarial training to disentangle demographic information while using a controlled re-fusion mechanism to preserve clinically relevant details. Additionally, subgroup-specific threshold adjustments ensure equitable performance across demographic groups. Comprehensive evaluations on a large-scale clinical X-ray dataset demonstrate that FairREAD significantly reduces unfairness metrics while maintaining diagnostic accuracy, establishing a new benchmark for fairness and performance in medical image classification. 

**Abstract (ZH)**: 近年来，深度学习在医学成像领域的应用展现了变革性的潜力，然而，由于性能在不同种族群体之间存在差异，公平性问题依然存在。现有的方法旨在通过减轻图像数据中的敏感属性来解决这些偏见；然而，这些属性往往含有临床相关的信息，其去除可能会损害模型性能——这是一个极其不理想的结果。为了应对这一挑战，我们提出了一种新颖、简单且高效的框架——公平解缠后融合（Fair Re-fusion After Disentanglement, FairREAD），以通过重新整合公平图像表示中的敏感人口统计属性来减轻不公平性。FairREAD 使用正交性约束和对抗训练来解缠人口统计信息，并通过受控的再融合机制保持临床相关的细节。此外，针对不同子群的具体阈值调整确保人口统计群体间的公平性能。在大规模临床X射线数据集上的全面评估表明，FairREAD 显著降低了不公平性指标，同时保持了诊断准确性，为医学图像分类中的公平性和性能设定了一项新的基准。 

---
# Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025) 

**Title (ZH)**: 低资源语言语言模型研讨会 (LoResLM 2025) 概览 

**Authors**: Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson, Ruslan Mitkov, Mohamed Gaber, Damith Premasiri, Fiona Anting Tan, Lasitha Uyangodage  

**Link**: [PDF](https://arxiv.org/pdf/2412.16365)  

**Abstract**: The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This workshop mainly aimed to provide a forum for researchers to share and discuss their ongoing work on language models (LMs) focusing on low-resource languages, following the recent advancements in neural language models and their linguistic biases towards high-resource languages. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, resulting in 35 accepted papers from 52 submissions. These contributions cover a broad range of low-resource languages from eight language families and 13 diverse research areas, paving the way for future possibilities and promoting linguistic inclusivity in NLP. 

**Abstract (ZH)**: 语言模型在低资源语言中的应用研讨会（LoResLM 2025）是与国际计算语言学会议（COLING 2025）联合举办的首届研讨会，于2025年在阿拉伯联合酋长国阿布扎比举行。此次研讨会的主要目的是为研究人员提供一个交流和讨论有关低资源语言的语言模型（LMs）工作的论坛，这是基于最近在神经语言模型及其对高资源语言的偏见方面取得的进展。LoResLM 2025 引起了自然语言处理（NLP）社区的广泛关注，最终收到了52篇提交论文中的35篇。这些贡献涵盖了八大家庭语言中的多种低资源语言，并涉及13个不同的研究领域，为未来的发展铺平了道路，并促进了NLP中的语言包容性。 

---
# Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context 

**Title (ZH)**: 人类可读的对抗提示：基于情境上下文对大型语言模型脆弱性的调查 

**Authors**: Nilanjana Das, Edward Raff, Manas Gaur  

**Link**: [PDF](https://arxiv.org/pdf/2412.16359)  

**Abstract**: Previous research on LLM vulnerabilities often relied on nonsensical adversarial prompts, which were easily detectable by automated methods. We address this gap by focusing on human-readable adversarial prompts, a more realistic and potent threat. Our key contributions are situation-driven attacks leveraging movie scripts to create contextually relevant, human-readable prompts that successfully deceive LLMs, adversarial suffix conversion to transform nonsensical adversarial suffixes into meaningful text, and AdvPrompter with p-nucleus sampling, a method to generate diverse, human-readable adversarial suffixes, improving attack efficacy in models like GPT-3.5 and Gemma 7B. Our findings demonstrate that LLMs can be tricked by sophisticated adversaries into producing harmful responses with human-readable adversarial prompts and that there exists a scope for improvement when it comes to robust LLMs. 

**Abstract (ZH)**: 之前的关于大语言模型（LLM）漏洞的研究往往依赖于无意义的对抗性提示，这些提示很容易被自动方法检测出来。我们通过关注可读性高的对抗性提示来填补这一空白，这些提示更加现实且更具威胁性。我们的主要贡献包括：利用电影剧本驱动的攻击，创建上下文相关且可读性高的对抗性提示，以欺骗LLM；对抗性后缀转换，将无意义的对抗性后缀转换成有意义的文字；以及AdvPrompter与p-核采样方法，用于生成多样且可读性高的对抗性后缀，从而在GPT-3.5和Gemma 7B等模型中提高攻击效果。我们的研究结果表明，LLM可以被高级对手通过可读性高的对抗性提示诱骗生成有害的响应，并且在构建更健壮的LLM方面还存在改进的空间。 

---
# Deliberative Alignment: Reasoning Enables Safer Language Models 

**Title (ZH)**: 审议对齐：推理使语言模型更加安全 

**Authors**: Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Heylar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese  

**Link**: [PDF](https://arxiv.org/pdf/2412.16339)  

**Abstract**: As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment. 

**Abstract (ZH)**: 随着大型语言模型在安全关键领域的影响日益增大，确保其严格遵守既定原则仍是一个根本性的挑战。我们提出了审慎对齐（Deliberative Alignment）这一新的范式，该范式直接向模型传授安全规范，并训练模型在回答之前明确回忆并准确推理这些规范。我们采用这种方法对齐了OpenAI的o系列模型，并在无需人工撰写推理过程或答案的情况下，实现了对OpenAI安全政策的高精度遵从。审慎对齐不仅通过同时提高抗扰鲁棒性并降低过度拒绝率来推进帕累托前沿，还改善了模型的离群分布外泛化能力。我们展示了明确规定的政策推理能够使对齐更加可扩展、可信和可解释。 

---
# Real Faults in Deep Learning Fault Benchmarks: How Real Are They? 

**Title (ZH)**: 深度学习故障基准中的实际故障：它们有多“真实”？ 

**Authors**: Gunel Jahangirova, Nargiz Humbatova, Jinhan Kim, Shin Yoo, Paolo Tonella  

**Link**: [PDF](https://arxiv.org/pdf/2412.16336)  

**Abstract**: As the adoption of Deep Learning (DL) systems continues to rise, an increasing number of approaches are being proposed to test these systems, localise faults within them, and repair those faults. The best attestation of effectiveness for such techniques is an evaluation that showcases their capability to detect, localise and fix real faults. To facilitate these evaluations, the research community has collected multiple benchmarks of real faults in DL systems. In this work, we perform a manual analysis of 490 faults from five different benchmarks and identify that 314 of them are eligible for our study. Our investigation focuses specifically on how well the bugs correspond to the sources they were extracted from, which fault types are represented, and whether the bugs are reproducible. Our findings indicate that only 18.5% of the faults satisfy our realism conditions. Our attempts to reproduce these faults were successful only in 52% of cases. 

**Abstract (ZH)**: 随着深度学习（DL）系统的应用日益增多，提出的方法也在不断增多，旨在测试这些系统、定位其中的故障并修复这些故障。此类技术最有效的证明是能够展示它们检测、定位和修复真实故障能力的评估。为了便于进行这些评估，研究社区已经收集了多个深度学习系统中真实故障的基准数据集。在本研究中，我们对手中共计490个不同基准数据集中的故障进行了人工分析，并发现其中314个故障符合我们研究的要求。我们的研究重点在于这些故障与提取它们的源代码的对应程度、故障类型以及这些故障是否可重现。研究结果表明，仅有18.5%的故障满足我们对现实性的要求。我们尝试重现这些故障的成功率仅为52%。 

---
# Optimizing Fintech Marketing: A Comparative Study of Logistic Regression and XGBoost 

**Title (ZH)**: 优化金融科技营销：逻辑回归与XGBoost的比较研究 

**Authors**: Sahar Yarmohammadtoosky Dinesh Chowdary Attota  

**Link**: [PDF](https://arxiv.org/pdf/2412.16333)  

**Abstract**: As several studies have shown, predicting credit risk is still a major concern for the financial services industry and is receiving a lot of scholarly interest. This area of study is crucial because it aids financial organizations in determining the probability that borrowers would default, which has a direct bearing on lending choices and risk management tactics. Despite the progress made in this domain, there is still a substantial knowledge gap concerning consumer actions that take place prior to the filing of credit card applications. The objective of this study is to predict customer responses to mail campaigns and assess the likelihood of default among those who engage. This research employs advanced machine learning techniques, specifically logistic regression and XGBoost, to analyze consumer behavior and predict responses to direct mail campaigns. By integrating different data preprocessing strategies, including imputation and binning, we enhance the robustness and accuracy of our predictive models. The results indicate that XGBoost consistently outperforms logistic regression across various metrics, particularly in scenarios using categorical binning and custom imputation. These findings suggest that XGBoost is particularly effective in handling complex data structures and provides a strong predictive capability in assessing credit risk. 

**Abstract (ZH)**: 如多项研究所示，预测信贷风险仍然是金融服务行业的重大关注点，并且在学术界引起了广泛关注。这一研究领域至关重要，因为它帮助金融机构评估借款人违约的概率，这对贷款决策和风险管理策略具有直接影响。尽管在此领域已经取得了进展，但对于在提交信用卡申请之前消费者的行为仍存在相当的知识空白。本研究旨在预测客户对邮件营销活动的反应，并评估参与者的违约可能性。本研究采用先进的机器学习技术，特别是逻辑回归和XGBoost，来分析消费者行为并预测直接邮件营销活动的响应。通过集成不同的数据预处理策略，包括插补和分箱，我们增强了预测模型的稳定性和准确性。研究结果表明，在各种指标上，XGBoost始终优于逻辑回归，特别是在使用分类分箱和自定义插补的情况下。这些发现表明，XGBoost特别擅长处理复杂的数据结构，并在评估信用风险方面具有强烈的预测能力。 

---
# Improving Object Detection for Time-Lapse Imagery Using Temporal Features in Wildlife Monitoring 

**Title (ZH)**: 使用时间特征改进野生动物监测中时间序列图像的目标检测 

**Authors**: Marcus Jenkins, Kirsty A. Franklin, Malcolm A. C. Nicoll, Nik C. Cole, Kevin Ruhomaun, Vikash Tatayah, Michal Mackiewicz  

**Link**: [PDF](https://arxiv.org/pdf/2412.16329)  

**Abstract**: Monitoring animal populations is crucial for assessing the health of ecosystems. Traditional methods, which require extensive fieldwork, are increasingly being supplemented by time-lapse camera-trap imagery combined with an automatic analysis of the image data. The latter usually involves some object detector aimed at detecting relevant targets (commonly animals) in each image, followed by some postprocessing to gather activity and population data. In this paper, we show that the performance of an object detector in a single frame of a time-lapse sequence can be improved by including spatio-temporal features from the prior frames. We propose a method that leverages temporal information by integrating two additional spatial feature channels which capture stationary and non-stationary elements of the scene and consequently improve scene understanding and reduce the number of stationary false positives. The proposed technique achieves a significant improvement of 24\% in mean average precision (mAP@0.05:0.95) over the baseline (temporal feature-free, single frame) object detector on a large dataset of breeding tropical seabirds. We envisage our method will be widely applicable to other wildlife monitoring applications that use time-lapse imaging. 

**Abstract (ZH)**: 监测动物种群对于评估生态系统的健康状况至关重要。传统的监测方法需要大量的实地工作，现在越来越多地被时间流逝的摄像头捕获的图像以及对图像数据进行自动分析的方法所补充。后者通常涉及一种目标检测器，用于检测每张图像中的相关目标（通常是动物），随后进行一些后处理以收集活动和种群数据。在本文中，我们展示了在时间流逝序列的一帧中，通过包含之前帧的空-时特征，可以改进目标检测器的性能。我们提出了一种方法，通过集成两个额外的空间特征通道来利用时间信息，这些通道分别捕捉场景中的固定和非固定的元素，从而提高场景理解并减少固定的假阳性。所提出的技术在大量繁殖热带 seabird的数据集上，相对于基线（无时间特征且单帧）目标检测器，在平均精度的均值（mAP@0.05:0.95）上实现了24%的显著改善。我们预计我们的方法将广泛适用于其他使用时间流逝成像进行野生动物监测的应用中。 

---
# HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases 

**Title (ZH)**: HybGRAG：面向文本和关系知识库的混合检索增强生成 

**Authors**: Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala, Christos Faloutsos  

**Link**: [PDF](https://arxiv.org/pdf/2412.16311)  

**Abstract**: Given a semi-structured knowledge base (SKB), where text documents are interconnected by relations, how can we effectively retrieve relevant information to answer user questions? Retrieval-Augmented Generation (RAG) retrieves documents to assist large language models (LLMs) in question answering; while Graph RAG (GRAG) uses structured knowledge bases as its knowledge source. However, many questions require both textual and relational information from SKB - referred to as "hybrid" questions - which complicates the retrieval process and underscores the need for a hybrid retrieval method that leverages both information. In this paper, through our empirical analysis, we identify key insights that show why existing methods may struggle with hybrid question answering (HQA) over SKB. Based on these insights, we propose HybGRAG for HQA consisting of a retriever bank and a critic module, with the following advantages: (1) Agentic, it automatically refines the output by incorporating feedback from the critic module, (2) Adaptive, it solves hybrid questions requiring both textual and relational information with the retriever bank, (3) Interpretable, it justifies decision making with intuitive refinement path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In experiments on the STaRK benchmark, HybGRAG achieves significant performance gains, with an average relative improvement in Hit@1 of 51%. 

**Abstract (ZH)**: 给定一个半结构化知识库（SKB），其中文本文档通过关系相互连接，如何有效地检索相关信息以回答用户问题？检索增强生成（RAG）通过检索文档来辅助大型语言模型（LLMs）进行问答；而Graph RAG（GRAG）利用结构化知识库作为其知识来源。然而，许多问题需要从SKB中同时获取文本和关系信息，这些被称为“混合”问题，这使得检索过程复杂化，并强调了需要一种结合这两种信息的混合检索方法。在本文中，通过我们的实证分析，我们识别出关键见解，展示了为什么现有方法可能难以处理SKB上的混合问答（HQA）。基于这些见解，我们提出了一种名为HybGRAG的方法来解决HQA，其包含检索库和批判模块，具有以下优势：（1）自主性，通过批判模块的反馈自动优化输出；（2）自适应性，使用检索库解决需要同时处理文本和关系信息的混合问题；（3）可解释性，通过直观的优化路径来解释决策过程；（4）有效性，在HQA基准测试中，HybGRAG取得了显著性能提升，平均改进精度（Hit@1）为51%。

相关实验在STaRK基准测试上验证了HybGRAG的有效性。结果显示，HybGRAG在HQA基准测试中的表现显著优于所有基线方法，平均相对改进精度（Hit@1）达到了51%。 

---
# LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning 

**Title (ZH)**: LEARN：一种统一的多任务领域适应少样本学习框架 

**Authors**: Bharadwaj Ravichandran, Alexander Lynch, Sarah Brockman, Brandon RichardWebster, Dawei Du, Anthony Hoogs, Christopher Funk  

**Link**: [PDF](https://arxiv.org/pdf/2412.16275)  

**Abstract**: Both few-shot learning and domain adaptation sub-fields in Computer Vision have seen significant recent progress in terms of the availability of state-of-the-art algorithms and datasets. Frameworks have been developed for each sub-field; however, building a common system or framework that combines both is something that has not been explored. As part of our research, we present the first unified framework that combines domain adaptation for the few-shot learning setting across 3 different tasks - image classification, object detection and video classification. Our framework is highly modular with the capability to support few-shot learning with/without the inclusion of domain adaptation depending on the algorithm. Furthermore, the most important configurable feature of our framework is the on-the-fly setup for incremental $n$-shot tasks with the optional capability to configure the system to scale to a traditional many-shot task. With more focus on Self-Supervised Learning (SSL) for current few-shot learning approaches, our system also supports multiple SSL pre-training configurations. To test our framework's capabilities, we provide benchmarks on a wide range of algorithms and datasets across different task and problem settings. The code is open source has been made publicly available here: this https URL 

**Abstract (ZH)**: 计算机视觉领域中，少样本学习和域适应两个子领域近年来在先进算法和数据集的可用性方面取得了显著进展。每个子领域都有相应的框架，但是将两者结合在一起的共同系统或框架还未被探索。作为我们研究的一部分，我们首次提出了一种统一框架，将域适应应用于3个不同的任务——图像分类、对象检测和视频分类的少样本学习场景中。我们的框架高度模块化，可以根据所需的算法支持包含或不包含域适应的少样本学习。此外，框架中最重要的可配置功能是在运行时为增量$n$-shot任务设置配置选项，并且可以配置系统以支持传统的多样本任务。随着当前少样本学习方法对自我监督学习（SSL）的更多关注，我们的系统还支持多种SSL预训练配置。为了测试我们框架的能力，我们在不同的任务和问题设置下提供了广泛的算法和数据集基准测试。代码已开源，并可通过以下链接获取：！[点击这里](this https URL) 

---
# Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection 

**Title (ZH)**: 基于战略选择与遗忘的持续学习在网络入侵检测中的应用 

**Authors**: Xinchen Zhang, Running Zhao, Zhihan Jiang, Handi Chen, Yulong Ding, Edith C.H. Ngai, Shuang-Hua Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16264)  

**Abstract**: Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism to drop outdated samples. The proposed strategic sample selection algorithm prioritizes new samples that cause the `drifted' pattern, enabling the model to better understand the evolving landscape. Additionally, we introduce strategic forgetting upon detecting significant drift by discarding outdated samples to free up memory, allowing the incorporation of more recent data. SSF captures evolving patterns effectively and ensures the model is aligned with the change of data patterns, significantly enhancing the IDS's adaptability to concept drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15 datasets demonstrates its superior adaptability to concept drift for network intrusion detection. 

**Abstract (ZH)**: 入侵检测系统（ IDS）对于保护数字基础设施至关重要。在动态网络环境中，威胁态势和正常操作行为不断变化，导致概念漂移。尽管持续学习可以减轻概念漂移的负面影响，但如果对漂移模式关注不足或过度保留过时知识，仍会阻碍IDS的适应性。本文提出了一种新的持续学习方法——SSF（Strategic Selection and Forgetting），它能在不断更新模型的同时，通过不断刷新的记忆缓冲保持更新。该方法的核心是采用一种战略性的样本选择算法来选择具有代表性的新样本，以及一种战略性遗忘机制来丢弃过时的样本。我们提出的战略性样本选择算法优先选择导致“漂移”模式的新样本，从而使模型更好地理解不断演变的态势。此外，我们还通过检测到显著漂移后战略性地丢弃过时样本来释放内存，从而能够纳入更多最新的数据。SSF能够有效捕获演变中的模式，并确保模型与数据模式的变化保持一致，大幅提高了IDS对概念漂移的适应性。在NSL-KDD和UNSW-NB15数据集上的先进性能表明，SSF在网络安全入侵检测中具有优越的概念漂移适应能力。 

---
# VirusT5: Harnessing Large Language Models to Predicting SARS-CoV-2 Evolution 

**Title (ZH)**: VirusT5：利用大型语言模型预测SARS-CoV-2演化 

**Authors**: Vishwajeet Marathe, Deewan Bajracharya, Changhui Yan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16262)  

**Abstract**: During a virus's evolution,various regions of the genome are subjected to distinct levels of functional this http URL with factors like codon bias and DNA repair efficiency,these constraints contribute to unique mutation patterns within the genome or a specific gene. In this project, we harnessed the power of Large Language Models(LLMs) to predict the evolution of SARS-CoV-2. By treating the mutation process from one generation to the next as a translation task, we trained a transformer model, called VirusT5, to capture the mutation patterns underlying SARS-CoV-2 evolution. We evaluated the VirusT5's ability to detect these mutation patterns including its ability to identify mutation hotspots and explored the potential of using VirusT5 to predict future virus variants. Our findings demonstrate the feasibility of using a large language model to model viral evolution as a translation process. This study establishes the groundbreaking concept of "mutation-as-translation," paving the way for new methodologies and tools for combating virus threats 

**Abstract (ZH)**: 在病毒进化的过程中，基因组的不同区域会受到不同程度的功能约束，这些约束与密码子偏倚和DNA修复效率等因素有关，这些限制因素共同导致了基因组或特定基因内独特的突变模式。在本项目中，我们利用大型语言模型（LLMs）预测了新冠病毒（SARS-CoV-2）的进化过程。通过将一代到下一代的突变过程视为翻译任务，我们训练了一个名为VirusT5的变换器模型，以捕捉SARS-CoV-2进化过程中的突变模式。我们评估了VirusT5检测这些突变模式的能力，包括识别热点突变的能力，并探索了使用VirusT5预测未来病毒变种的潜力。我们的研究结果表明，使用大型语言模型将病毒进化建模为翻译过程是可行的。本研究提出了“突变即翻译”的新概念，为抗击病毒威胁提供了新的方法和工具。 

---
# PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models 

**Title (ZH)**: PromptLA: 朝向黑盒文本到图像扩散模型的完整性验证 

**Authors**: Zhuomeng Zhang, Fangqi Li, Chong Di, Shilin Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16257)  

**Abstract**: Current text-to-image (T2I) diffusion models can produce high-quality images, and malicious users who are authorized to use the model only for benign purposes might modify their models to generate images that result in harmful social impacts. Therefore, it is essential to verify the integrity of T2I diffusion models, especially when they are deployed as black-box services. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we capture modifications to the model through the differences in the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton for efficient and accurate integrity verification of T2I diffusion models. Extensive experiments demonstrate the effectiveness, stability, accuracy and generalization of our algorithm against existing integrity violations compared with baselines. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which paves the way to copyright discussions and protections for artificial intelligence applications in practice. 

**Abstract (ZH)**: 当前的文本到图像（T2I）扩散模型能够生成高质量的图像，但被授权仅用于良性用途的恶意用户可能会修改这些模型，生成可能导致不良社会影响的图像。因此，当这些模型以黑盒服务的形式部署时，验证T2I扩散模型的完整性尤为重要。鉴于生成模型输出中的随机性和与其交互的高成本，我们通过生成图像特征分布的差异来捕捉模型的修改。我们提出了一种基于学习自引起的新型提示选择算法，以实现T2I扩散模型高效且准确的完整性验证。大量实验表明，与基线方法相比，我们的算法在现有完整性违规行为方面具有有效性、稳定性和泛化能力。据我们所知，这是首次针对T2I扩散模型完整性验证的研究工作，为实际应用中的人工智能版权讨论和保护铺平了道路。 

---
# Aria-UI: Visual Grounding for GUI Instructions 

**Title (ZH)**: Aria-UI：GUI 操作说明的视觉语义匹配 

**Authors**: Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, Junnan Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.16256)  

**Abstract**: Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. We release all training data and model checkpoints to foster further research at this https URL. 

**Abstract (ZH)**: 直接操作不同平台的GUI以自动化任务的数字代理日益重要。对于这些代理而言，从语言指令到目标元素的映射依然是一项重大挑战，主要依赖于HTML或AXTree输入。本文介绍了Aria-UI，一个专门设计用于GUI映射的大规模多模态模型。Aria-UI采用纯视觉方法，不依赖于辅助输入。为适应异构规划指令，我们提出了一种可扩展的数据管道来合成多样且高质量的指令样本，用于映射任务。为了处理执行任务中的动态上下文，Aria-UI整合了文本和图文交织的动作历史，实现了强大的上下文感知推理能力。在离线和在线代理基准测试中，Aria-UI设定了新的最佳结果，并优于仅依赖视觉和依赖AXTree的基本模型。我们已公开所有训练数据和模型检查点，欢迎感兴趣的读者访问以下链接进行进一步研究：[此处应为链接]。 

---
# Post-hoc Interpretability Illumination for Scientific Interaction Discovery 

**Title (ZH)**: 科学互动发现的事后可解释性分析 

**Authors**: Ling Zhang, Zhichao Hou, Tingxiang Ji, Yuanyuan Xu, Runze Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.16252)  

**Abstract**: Model interpretability and explainability have garnered substantial attention in recent years, particularly in decision-making applications. However, existing interpretability tools often fall short in delivering satisfactory performance due to limited capabilities or efficiency issues. To address these challenges, we propose a novel post-hoc method: Iterative Kings' Forests (iKF), designed to uncover complex multi-order interactions among variables. iKF iteratively selects the next most important variable, the "King", and constructs King's Forests by placing it at the root node of each tree to identify variables that interact with the "King". It then generates ranked short lists of important variables and interactions of varying orders. Additionally, iKF provides inference metrics to analyze the patterns of the selected interactions and classify them into one of three interaction types: Accompanied Interaction, Synergistic Interaction, and Hierarchical Interaction. Extensive experiments demonstrate the strong interpretive power of our proposed iKF, highlighting its great potential for explainable modeling and scientific discovery across diverse scientific fields. 

**Abstract (ZH)**: 近年来，模型可解释性和可理解性受到了广泛关注，尤其是在决策应用方面。然而，现有的可解释性工具往往由于能力有限或效率问题无法提供令人满意的表现。为解决这些问题，我们提出了一种新颖的后验方法：迭代王林森林（iKF），旨在揭示变量间的复杂多阶交互作用。iKF 通过迭代选择下一个最重要的变量（“王”），并在每棵树的根节点处放置“王”来构建“王林森林”，以识别与“王”交互的变量。随后，iKF 生成不同阶次的重要变量和交互作用的排序列表。此外，iKF 提供了推断指标来分析所选交互作用的模式，并将它们分类为三种交互类型之一：伴随交互、协同交互和层级交互。广泛的实验结果证明了我们提出的 iKF 在可解释性方面的强大能力，突显了其在可解释建模和跨学科科学研究中的巨大潜力。 

---
# Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models 

**Title (ZH)**: 面向科学发现的字典学习方法：从显微成像基础模型中提取生物概念 

**Authors**: Konstantin Donhauser, Kristina Ulicna, Gemma Elyse Moran, Aditya Ravuri, Kian Kenyon-Dean, Cian Eastwood, Jason Hartford  

**Link**: [PDF](https://arxiv.org/pdf/2412.16247)  

**Abstract**: Dictionary learning (DL) has emerged as a powerful interpretability tool for large language models. By extracting known concepts (e.g., Golden-Gate Bridge) from human-interpretable data (e.g., text), sparse DL can elucidate a model's inner workings. In this work, we ask if DL can also be used to discover unknown concepts from less human-interpretable scientific data (e.g., cell images), ultimately enabling modern approaches to scientific discovery. As a first step, we use DL algorithms to study microscopy foundation models trained on multi-cell image data, where little prior knowledge exists regarding which high-level concepts should arise. We show that sparse dictionaries indeed extract biologically-meaningful concepts such as cell type and genetic perturbation type. We also propose a new DL algorithm, Iterative Codebook Feature Learning~(ICFL), and combine it with a pre-processing step that uses PCA whitening from a control dataset. In our experiments, we demonstrate that both ICFL and PCA improve the selectivity of extracted features compared to TopK sparse autoencoders. 

**Abstract (ZH)**: 字典学习（DL）已成为大型语言模型解释性的重要工具。通过从可人工解释的数据（例如文本）中提取已知概念（例如金门大桥），稀疏DL可以揭示模型的工作原理。本研究中，我们探讨是否可以将DL应用于发现来自较难人工解释的科学数据（例如细胞图像）中的未知概念，从而最终促进现代科学发现方法的发展。作为第一步，我们使用DL算法研究了在多细胞图像数据上训练的显微镜基础模型，其中关于哪些高层概念会浮现几乎没有先验知识。我们展示了稀疏字典确实提取了具有生物学意义的概念，如细胞类型和基因扰动类型。同时，我们提出了一个新的DL算法——迭代码本特征学习（ICFL），并将其与利用控制数据集进行的PCA去相关预处理步骤相结合。在我们的实验中，我们证明了与TopK稀疏自编码器相比，ICFL和PCA均可提高提取特征的选择性。 

---
# WiFi CSI Based Temporal Activity Detection Via Dual Pyramid Network 

**Title (ZH)**: 基于WiFi CSI的时域活动检测双 Pyramid 网络方法 

**Authors**: Zhendong Liu, Le Zhang, Bing Li, Yingjie Zhou, Zhenghua Chen, Ce Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2412.16233)  

**Abstract**: We address the challenge of WiFi-based temporal activity detection and propose an efficient Dual Pyramid Network that integrates Temporal Signal Semantic Encoders and Local Sensitive Response Encoders. The Temporal Signal Semantic Encoder splits feature learning into high and low-frequency components, using a novel Signed Mask-Attention mechanism to emphasize important areas and downplay unimportant ones, with the features fused using ContraNorm. The Local Sensitive Response Encoder captures fluctuations without learning. These feature pyramids are then combined using a new cross-attention fusion mechanism. We also introduce a dataset with over 2,114 activity segments across 553 WiFi CSI samples, each lasting around 85 seconds. Extensive experiments show our method outperforms challenging baselines. Code and dataset are available at this https URL. 

**Abstract (ZH)**: 我们解决了基于WiFi的时间活动检测挑战，并提出了一种高效的双层次网络（Dual Pyramid Network），该网络整合了时间信号语义编码器（Temporal Signal Semantic Encoder）和局部敏感响应编码器（Local Sensitive Response Encoder）。时间信号语义编码器将特征学习划分为高频和低频成分，并利用一种新颖的符号掩码注意力（Signed Mask-Attention）机制来强调重要区域并弱化不重要区域，特征融合使用ContraNorm机制。局部敏感响应编码器捕捉波动而不进行学习。然后，这些特征金字塔通过一种新的交叉注意力融合机制结合在一起。我们还引入了一个包含超过2,114个活动片段的数据集，这些片段基于553个WiFi CSI样本，每个片段大约持续85秒。广泛的经验研究表明，我们的方法优于具有挑战性的基线方法。代码和数据集可在以下链接获取：this https URL。 

---
# Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven Optimization 

**Title (ZH)**: 可反驳的视觉蕴含：基准、评估器和基于奖励的优化 

**Authors**: Yue Zhang, Liqiang Jing, Vibhav Gogate  

**Link**: [PDF](https://arxiv.org/pdf/2412.16232)  

**Abstract**: We introduce a new task called Defeasible Visual Entailment (DVE), where the goal is to allow the modification of the entailment relationship between an image premise and a text hypothesis based on an additional update. While this concept is well-established in Natural Language Inference, it remains unexplored in visual entailment. At a high level, DVE enables models to refine their initial interpretations, leading to improved accuracy and reliability in various applications such as detecting misleading information in images, enhancing visual question answering, and refining decision-making processes in autonomous systems. Existing metrics do not adequately capture the change in the entailment relationship brought by updates. To address this, we propose a novel inference-aware evaluator designed to capture changes in entailment strength induced by updates, using pairwise contrastive learning and categorical information learning. Additionally, we introduce a reward-driven update optimization method to further enhance the quality of updates generated by multimodal models. Experimental results demonstrate the effectiveness of our proposed evaluator and optimization method. 

**Abstract (ZH)**: 我们引入了一个新的任务，称为辩斥视觉蕴含（DVE），该任务的目标是在基于额外更新的情况下，允许修改图像前提与文本假设之间的蕴含关系。虽然这一概念在自然语言推理中已有很好的建立，但在视觉蕴含领域仍属未开发的领域。从宏观上看，DVE 使模型能够修正其初始解释，从而在诸如检测图像中的误导信息、增强视觉问答和在自主系统中优化决策过程等应用中提高准确性和可靠性。

现有的评估指标无法充分捕捉更新带来的蕴含关系的变化。为了解决这一问题，我们提出了一种新型的推理感知评估器，旨在通过成对对比学习和分类信息学习来捕捉由更新引起的蕴含强度变化。此外，我们还引入了一种奖励驱动的更新优化方法，以进一步提高由多模态模型生成的更新的质量。实验结果证明了我们所提出评估器和优化方法的有效性。 

---
# Quantified Linear and Polynomial Arithmetic Satisfiability via Template-based Skolemization 

**Title (ZH)**: 基于模板的斯科莱姆化方法的量化线性和多项式算术满足性求解 

**Authors**: Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Mehrdad Karrabi, Harshit J Motwani, Maximilian Seeliger, Đorđe Žikelić  

**Link**: [PDF](https://arxiv.org/pdf/2412.16226)  

**Abstract**: The problem of checking satisfiability of linear real arithmetic (LRA) and non-linear real arithmetic (NRA) formulas has broad applications, in particular, they are at the heart of logic-related applications such as logic for artificial intelligence, program analysis, etc. While there has been much work on checking satisfiability of unquantified LRA and NRA formulas, the problem of checking satisfiability of quantified LRA and NRA formulas remains a significant challenge. The main bottleneck in the existing methods is a computationally expensive quantifier elimination step. In this work, we propose a novel method for efficient quantifier elimination in quantified LRA and NRA formulas. We propose a template-based Skolemization approach, where we automatically synthesize linear/polynomial Skolem functions in order to eliminate quantifiers in the formula. The key technical ingredients in our approach are Positivstellensätze theorems from algebraic geometry, which allow for an efficient manipulation of polynomial inequalities. Our method offers a range of appealing theoretical properties combined with a strong practical performance. On the theory side, our method is sound, semi-complete, and runs in subexponential time and polynomial space, as opposed to existing sound and complete quantifier elimination methods that run in doubly-exponential time and at least exponential space. On the practical side, our experiments show superior performance compared to state-of-the-art SMT solvers in terms of the number of solved instances and runtime, both on LRA and on NRA benchmarks. 

**Abstract (ZH)**: 线性实算术(LRA)和非线性实算术(NRA)公式的可满足性检验问题是有着广泛应用的问题，特别是在逻辑相关的应用中，如人工智能的逻辑、程序分析等。尽管已经有许多关于检查未量化的LRA和NRA公式可满足性的研究工作，但量化的LRA和NRA公式可满足性检验仍然是一个重大挑战。现有方法的主要瓶颈在于计算上昂贵的量词消除步骤。在此项工作中，我们提出了一种用于量化的LRA和NRA公式高效量词消除的新方法。我们提出了一种基于模板的斯科莱姆化方法，在这种方法中，我们自动生成线性/多项式斯科莱姆函数，以便在公式中消除量词。我们方法的关键技术成分是从代数几何学中来的正定定理（Positivstellensätze定理），这允许高效操作多项式不等式。我们的方法兼备了一系列吸引人的理论性质和强大的实际性能。在理论方面，我们的方法是正确的、半完备的，并能在次指数时间和多项式空间内运行，而现有的正确的、完备的量词消除方法则需要双指数时间和至少指数空间。在实践中，我们的实验结果表明，在求解实例的数量和运行时长方面，我们的方法都优于当前最先进的SMT求解器，这在LRA和NRA基准测试中都得到了证实。 

---
# Bayesian Critique-Tune-Based Reinforcement Learning with Attention-Based Adaptive Pressure for Multi-Intersection Traffic Signal Control 

**Title (ZH)**: 基于贝叶斯批判调优的注意力机制自适应压力强化学习在多交叉口交通信号控制中的应用 

**Authors**: Wenchang Duan, Zhenguo Gao. Jinguo Xian  

**Link**: [PDF](https://arxiv.org/pdf/2412.16225)  

**Abstract**: Adaptive Traffic Signal Control (ATSC) system is a critical component of intelligent transportation, with the capability to significantly alleviate urban traffic congestion. Although reinforcement learning (RL)-based methods have demonstrated promising performance in achieving ATSC, existing methods find it prone to convergence to local optima. Consequently, this paper proposes a novel Bayesian Critique-Tune-Based Reinforcement Learning with Attention-Based Adaptive Pressure (BCT-APRL) for multi-intersection signal control. In BCT-APRL, the Critique-Tune (CT) framework, a two-layer Bayesian structure is designed to refine the RL policies. Specifically, the Bayesian inference-based Critique layer provides effective evaluations of the credibility of policies; the Bayesian decision-based Tune layer fine-tunes policies by minimizing the posterior risks when the evaluations are negative. Furthermore, an attention-based Adaptive Pressure (AP) is designed to specify the traffic movement representation as an effective and efficient pressure of vehicle queues in the traffic network. Achieving enhances the reasonableness of RL policies. Extensive experiments conducted with a simulator across a range of intersection layouts show that BCT-APRL is superior to other state-of-the-art methods in seven real-world datasets. Codes are open-sourced. 

**Abstract (ZH)**: 自适应交通信号控制（ATSC）系统是智能交通系统的关键组成部分，能够显著缓解城市交通拥堵。尽管基于强化学习（RL）的方法在实现ATSC方面表现出有前景的性能，但现有方法容易陷入局部最优解。因此，本文提出了一种新颖的基于贝叶斯批评-调优的注意力机制自适应压力强化学习（BCT-APRL）方法，用于多交叉口信号控制。在BCT-APRL中，批评-调优（CT）框架设计了一种两层贝叶斯结构以精炼RL策略。具体而言，基于贝叶斯推理的批评层提供有效的策略可信度评估；基于贝叶斯决策的调优层在评估为负面时通过最小化后验风险来细化策略。此外，设计了一种基于注意力机制的自适应压力（AP），用于以有效且高效的方式在交通网络中指定交通流表示，作为车辆队列的压力。这增强了RL策略的合理性。在不同交叉口布局的仿真器上进行的广泛实验显示，BCT-APRL在七个真实世界数据集中优于其他最先进的方法。代码已开源。 

---
# Cross-Attention Graph Neural Networks for Inferring Gene Regulatory Networks with Skewed Degree Distribution 

**Title (ZH)**: 具有偏斜度分布的基因调节网络推断的跨注意力图神经网络方法 

**Authors**: Jiaqi Xiong, Nan Yin, Yifan Sun, Haoyang Li, Yingxu Wang, Duo Ai, Fang Pan, Shiyang Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16220)  

**Abstract**: Inferencing Gene Regulatory Networks (GRNs) from gene expression data is a pivotal challenge in systems biology, and several innovative computational methods have been introduced. However, most of these studies have not considered the skewed degree distribution of this http URL, some genes may regulate multiple target genes while some genes may be regulated by multiple regulator this http URL a skewed degree distribution issue significantly complicates the application of directed graph embedding methods. To tackle this issue, we propose the Cross-Attention Complex Dual Graph Embedding Model (XATGRN). Our XATGRN employs a cross-attention mechanism to effectively capture intricate gene interactions from gene expression profiles. Additionally, it uses a Dual Complex Graph Embedding approach to manage the skewed degree distribution, thereby ensuring precise prediction of regulatory relationships and their directionality. Our model consistently outperforms existing state-of-the-art methods across various datasets, underscoring its efficacy in elucidating complex gene regulatory mechanisms. Our codes used in this paper are publicly available at: this https URL. 

**Abstract (ZH)**: 从基因表达数据推断基因调控网络（GRNs）是系统生物学中的一个关键挑战，多种创新的计算方法已经被提出。然而，大多数研究并未考虑这种不规则的度分布问题，一些基因可以调控多个靶基因，而一些基因可以被多个调控因子所调控。这种不规则的度分布问题显著地复杂化了有向图嵌入方法的应用。为解决这一问题，我们提出了一种交叉注意复杂双图嵌入模型（XATGRN）。我们的XATGRN采用交叉注意机制来有效地捕获基因表达谱中的复杂基因相互作用。此外，它使用双复杂数组图嵌入方法来处理不规则的度分布问题，从而确保对调控关系及其方向性进行精确预测。我们的模型在多种数据集上都优于现有最先进的方法，证明了其在阐明复杂基因调控机制方面的有效性。我们在本文中使用的代码已公开发布在：这个 <https://github.com/your-repo/XATGRN>。 

---
# GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE 

**Title (ZH)**: GraphLoRA：通过MoE的图协作增强大语言模型 fine-tuning 

**Authors**: Ting Bai, Yue Yu, Le Huang, Zenan Xu, Zhe Zhao, Chuan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2412.16216)  

**Abstract**: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that has been widely adopted in various downstream applications of LLMs. Together with the Mixture-of-Expert (MoE) technique, fine-tuning approaches have shown remarkable improvements in model capability. However, the coordination of multiple experts in existing studies solely relies on the weights assigned by the simple router function. Lack of communication and collaboration among experts exacerbate the instability of LLMs due to the imbalance load problem of MoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning framework GraphLoRA, in which a graph router function is designed to capture the collaboration signals among experts by graph neural networks (GNNs). GraphLoRA enables all experts to understand input knowledge and share information from neighbor experts by aggregating operations. Besides, to enhance each expert's capability and their collaborations, we design two novel coordination strategies: the Poisson distribution-based distinction strategy and the Normal distribution-based load balance strategy. Extensive experiments on four real-world datasets demonstrate the effectiveness of our GraphLoRA in parameter-efficient fine-tuning of LLMs, showing the benefits of facilitating collaborations of multiple experts in the graph router of GraphLoRA. 

**Abstract (ZH)**: 低秩适应（LoRA）是一种参数效率高的微调方法，在大型语言模型（LLM）的各种下游应用中得到了广泛应用。结合专家混合（Mixture-of-Experts, MoE）技术，微调方法在模型能力方面表现出显著的提升。然而，现有研究中多个专家之间的协调仅依赖于简单路由器函数分配的权重。由于专家间缺乏沟通与协作，MoE 的负载不平衡问题导致了LLM 的不稳定性。为解决这一问题，我们提出了一种基于图的LLM 微调框架——GraphLoRA，在该框架中设计了一个图路由器函数，通过图神经网络（GNN）捕捉专家间的协作信号。GraphLoRA 使得所有专家能够理解输入知识并借助聚合操作共享邻近专家的信息。此外，为了增强每个专家的能力及其协作，我们设计了两种新的协调策略：基于泊松分布的区别策略和基于正态分布的负载平衡策略。在四个真实世界的数据集上的广泛实验显示，GraphLoRA 在参数效率高的LLM 微调方面非常有效，证明了在GraphLoRA 的图路由器中促进多个专家协作的益处。 

---
# Zero-Shot Image Moderation in Google Ads with LLM-Assisted Textual Descriptions and Cross-modal Co-embeddings 

**Title (ZH)**: 使用基于LLM的文本描述和跨模态共嵌入的Google广告中的零样本图像审核 

**Authors**: Enming Luo, Wei Qiao, Katie Warren, Jingxiang Li, Eric Xiao, Krishna Viswanathan, Yuan Wang, Yintao Liu, Jimin Li, Ariel Fuxman  

**Link**: [PDF](https://arxiv.org/pdf/2412.16215)  

**Abstract**: We present a scalable and agile approach for ads image content moderation at Google, addressing the challenges of moderating massive volumes of ads with diverse content and evolving policies. The proposed method utilizes human-curated textual descriptions and cross-modal text-image co-embeddings to enable zero-shot classification of policy violating ads images, bypassing the need for extensive supervised training data and human labeling. By leveraging large language models (LLMs) and user expertise, the system generates and refines a comprehensive set of textual descriptions representing policy guidelines. During inference, co-embedding similarity between incoming images and the textual descriptions serves as a reliable signal for policy violation detection, enabling efficient and adaptable ads content moderation. Evaluation results demonstrate the efficacy of this framework in significantly boosting the detection of policy violating content. 

**Abstract (ZH)**: 我们提出了一种可扩展且灵活的方法，用于在Google中对广告图像内容进行审查，以应对审查海量多样内容的广告并应对不断变化的政策所面临的挑战。提出的这种方法利用人工编纂的文字描述和跨模态的文字-图像联合嵌入，实现了零样本分类，从而避免了需要大量监督训练数据和人工标注。通过利用大型语言模型（LLMs）和用户的专业知识，系统生成并精炼了一套全面的文字描述，代表了政策准则。在推理过程中，新入图像与文字描述之间的联合嵌入相似性可以作为可靠的信号，用于政策违规检测，从而实现高效且适应性强的广告内容审查。评估结果显示，该框架在显著提高政策违规内容检测方面表现出有效性。 

---
# AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models 

**Title (ZH)**: AdvIRL：基于强化学习的三维NeRF模型对抗攻击方法 

**Authors**: Tommy Nguyen, Mehmet Ergezer, Christian Green  

**Link**: [PDF](https://arxiv.org/pdf/2412.16213)  

**Abstract**: The increasing deployment of AI models in critical applications has exposed them to significant risks from adversarial attacks. While adversarial vulnerabilities in 2D vision models have been extensively studied, the threat landscape for 3D generative models, such as Neural Radiance Fields (NeRF), remains underexplored. This work introduces \textit{AdvIRL}, a novel framework for crafting adversarial NeRF models using Instant Neural Graphics Primitives (Instant-NGP) and Reinforcement Learning. Unlike prior methods, \textit{AdvIRL} generates adversarial noise that remains robust under diverse 3D transformations, including rotations and scaling, enabling effective black-box attacks in real-world scenarios. Our approach is validated across a wide range of scenes, from small objects (e.g., bananas) to large environments (e.g., lighthouses). Notably, targeted attacks achieved high-confidence misclassifications, such as labeling a banana as a slug and a truck as a cannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond attacking, \textit{AdvIRL}-generated adversarial models can serve as adversarial training data to enhance the robustness of vision systems. The implementation of \textit{AdvIRL} is publicly available at \url{this https URL}, ensuring reproducibility and facilitating future research. 

**Abstract (ZH)**: 随着人工智能模型在关键应用中的部署不断增加，它们面临着来自对抗攻击的显著风险。尽管2D视觉模型中的对抗性漏洞被广泛研究，但对于3D生成模型，如神经辐射场（NeRF）等，威胁 landscape 仍然研究不足。本文提出了一种名为 \textit{AdvIRL} 的新颖框架，利用即时神经图形原语（Instant-NGP）和强化学习来构建对抗性 NeRF 模型。与先前的方法不同，\textit{AdvIRL} 生成的对抗噪声在多种3D变换下仍保持鲁棒性，包括旋转和缩放，从而在现实场景中实现有效的黑盒攻击。我们的方法在广泛的不同场景下进行了验证，从小物体（例如香蕉）到大型环境（例如灯塔）。值得注意的是，目标攻击达到了高置信度的错误分类，例如将香蕉误标为蛞蝓，将卡车误标为大炮，这揭示了对抗性 NeRF 所带来的实际风险。除了攻击之外，由 \textit{AdvIRL} 生成的对抗性模型还可以作为对抗训练数据，以增强视觉系统的鲁棒性。\textit{AdvIRL} 的实现已公开发布在 \url{this https URL}，以确保可再现性并促进未来的研究。 

---
# Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface Vehicles 

**Title (ZH)**: 基于机器学习的无人水面车辆波浪方向估计方法 

**Authors**: Manele Ait Habouche, Mickaël Kerboeuf, Goulven Guillou, Jean-Philippe Babau  

**Link**: [PDF](https://arxiv.org/pdf/2412.16205)  

**Abstract**: Unmanned Surface Vehicles (USVs) have become critical tools for marine exploration, environmental monitoring, and autonomous navigation. Accurate estimation of wave direction is essential for improving USV navigation and ensuring operational safety, but traditional methods often suffer from high costs and limited spatial resolution. This paper proposes a machine learning-based approach leveraging LSTM (Long Short-Term Memory) networks to predict wave direction using sensor data collected from USVs. Experimental results show the capability of the LSTM model to learn temporal dependencies and provide accurate predictions, outperforming simpler baselines. 

**Abstract (ZH)**: 无人水面车辆（USVs）已成为海洋勘探、环境监测和自主导航领域不可或缺的工具。准确估计波浪方向对于改善USV导航和确保操作安全至关重要，但传统方法往往成本高且空间分辨率有限。本文提出了一种基于机器学习的方法，利用长短期记忆（LSTM）网络来预测从USVs收集的传感器数据中的波浪方向。实验结果表明，LSTM模型能够学习时间依赖性并提供准确的预测，优于一些简单的基线方法。 

---
# CLIP-RLDrive: Human-Aligned Autonomous Driving via CLIP-Based Reward Shaping in Reinforcement Learning 

**Title (ZH)**: CLIP-RLDrive：基于CLIP的奖励塑形在强化学习中实现的人类对齐自动驾驶 

**Authors**: Erfan Doroudian, Hamid Taghavifar  

**Link**: [PDF](https://arxiv.org/pdf/2412.16201)  

**Abstract**: This paper presents CLIP-RLDrive, a new reinforcement learning (RL)-based framework for improving the decision-making of autonomous vehicles (AVs) in complex urban driving scenarios, particularly in unsignalized intersections. To achieve this goal, the decisions for AVs are aligned with human-like preferences through Contrastive Language-Image Pretraining (CLIP)-based reward shaping. One of the primary difficulties in RL scheme is designing a suitable reward model, which can often be challenging to achieve manually due to the complexity of the interactions and the driving scenarios. To deal with this issue, this paper leverages Vision-Language Models (VLMs), particularly CLIP, to build an additional reward model based on visual and textual cues. 

**Abstract (ZH)**: 本文提出了一种新的基于强化学习（RL）的框架——CLIP-RLDrive，该框架旨在提高自动驾驶车辆（AVs）在复杂城市驾驶场景中的决策能力，尤其是在无信号交叉路口的情况。为了实现这一目标，通过基于对比语言-图像预训练（CLIP）的奖励塑造，将AVs的决策与人类偏好对齐。在RL方案中，设计一个合适的奖励模型是一个主要难点，由于交互和驾驶场景的复杂性，这种奖励模型常常难以手工构建。为解决这一问题，本文利用视觉-语言模型（VLMs），特别是CLIP，基于视觉和文本线索构建额外的奖励模型。 

---
# AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0 

**Title (ZH)**: AgroXAI：可解释的AI驱动作物推荐系统，助力农业4.0 

**Authors**: Ozlem Turgut, Ibrahim Kok, Suat Ozdemir  

**Link**: [PDF](https://arxiv.org/pdf/2412.16196)  

**Abstract**: Today, crop diversification in agriculture is a critical issue to meet the increasing demand for food and improve food safety and quality. This issue is considered to be the most important challenge for the next generation of agriculture due to the diminishing natural resources, the limited arable land, and unpredictable climatic conditions caused by climate change. In this paper, we employ emerging technologies such as the Internet of Things (IoT), machine learning (ML), and explainable artificial intelligence (XAI) to improve operational efficiency and productivity in the agricultural sector. Specifically, we propose an edge computing-based explainable crop recommendation system, AgroXAI, which suggests suitable crops for a region based on weather and soil conditions. In this system, we provide local and global explanations of ML model decisions with methods such as ELI5, LIME, SHAP, which we integrate into ML models. More importantly, we provide regional alternative crop recommendations with the counterfactual explainability method. In this way, we envision that our proposed AgroXAI system will be a platform that provides regional crop diversity in the next generation agriculture. 

**Abstract (ZH)**: 当今，农作物多样化在农业中已成为一个至关重要的问题，以满足不断增长的食品需求，并提高食品的安全性和质量。这一问题被视为下一代农业面临的最重要挑战之一，原因在于自然资源的不断减少、有限的可耕种土地以及由气候变化引起的不可预测的气候条件。本文采用物联网（IoT）、机器学习（ML）和可解释的人工智能（XAI）等新兴技术，以提高农业部门的操作效率和生产力。具体而言，我们提出了一种基于边缘计算的可解释作物推荐系统——AgroXAI，该系统能够根据天气和土壤条件为某一地区推荐合适的作物。在此系统中，我们利用ELI5、LIME、SHAP等方法提供对ML模型决策的局部和全局解释，并将这些方法整合进ML模型中。更为重要的是，我们采用因果可解释性方法为某一地区提供替代作物建议。通过这种方式，我们构想这一提出的AgroXAI系统将成为下一代农业提供区域性作物多样性的平台。 

---
# Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication 

**Title (ZH)**: 基于机器学习的腹腔镜食管胃底折叠术内缝合自动评估方法 

**Authors**: Shekhar Madhav Khairnar, Huu Phong Nguyen, Alexis Desir, Carla Holcomb, Daniel J. Scott, Ganesh Sankaranarayanan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16195)  

**Abstract**: Automated assessment of surgical skills using artificial intelligence (AI) provides trainees with instantaneous feedback. After bimanual tool motions are captured, derived kinematic metrics are reliable predictors of performance in laparoscopic tasks. Implementing automated tool tracking requires time-intensive human annotation. We developed AI-based tool tracking using the Segment Anything Model (SAM) to eliminate the need for human annotators. Here, we describe a study evaluating the usefulness of our tool tracking model in automated assessment during a laparoscopic suturing task in the fundoplication procedure. An automated tool tracking model was applied to recorded videos of Nissen fundoplication on porcine bowel. Surgeons were grouped as novices (PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each suturing step were segmented, and motions of the left and right tools were extracted. A low-pass filter with a 24 Hz cut-off frequency removed noise. Performance was assessed using supervised and unsupervised models, and an ablation study compared results. Kinematic features--RMS velocity, RMS acceleration, RMS jerk, total path length, and Bimanual Dexterity--were extracted and analyzed using Logistic Regression, Random Forest, Support Vector Classifier, and XGBoost. PCA was performed for feature reduction. For unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers, such as a 1-D CNN and traditional models, was trained. Data were extracted for 28 participants (9 novices, 19 experts). Supervised learning with PCA and Random Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The unsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an F1 score of 0.806, eliminating the need for kinematic feature computation. We demonstrated an AI model capable of automated performance classification, independent of human annotation. 

**Abstract (ZH)**: 使用人工智能（AI）自动化评估外科技能可以为受训者提供即时反馈。在双手持器械动作被捕获后，提取的运动学指标可以可靠地预测内镜手术任务中的表现。实施自动化工具跟踪需要大量的人工标注时间。我们开发了使用Segment Anything Model (SAM) 的基于AI的工具跟踪方法，以消除对人工标注员的需求。在此，我们描述了一项研究，评估我们的工具跟踪模型在胃底折叠术内镜缝合任务中自动化评估中的实用性。一种自动化工具跟踪模型应用于猪肠上进行的Nissen胃底折叠术记录视频。外科医生被分为新手（PGY1-2）和专家（PGY3-5，住院医师）。每个缝合步骤的开始和结束进行了分割，并提取了左右工具的运动。使用截止频率为24 Hz的低通滤波器去除噪声。使用监督和非监督模型评估表现，并进行了消融研究以比较结果。提取并分析了运动学特征——均方根速度、均方根加速度、均方根冲击、总路径长度和双手协调性，并使用逻辑回归、随机森林、支持向量分类器和XGBoost进行分析。进行了主成分分析（PCA）以减少特征数量。对于非监督学习，使用降噪自编码器（DAE）模型及其分类器（如1-D卷积神经网络）和传统模型进行训练。数据来自28名参与者（9名新手，19名专家）。使用PCA和随机森林的监督学习达到了准确率为0.795，F1分数为0.778。1-D卷积神经网络在非监督学习中表现出更佳的结果，准确率为0.817，F1分数为0.806，消除了运动学特征计算的需求。我们展示了能够独立于人工标注进行性能分类的AI模型。 

---
# A Decade of Deep Learning: A Survey on The Magnificent Seven 

**Title (ZH)**: 深度学习十周年：关于七个伟大领域的综述 

**Authors**: Dilshod Azizov, Muhammad Arslan Manzoor, Velibor Bojkovic, Yingxu Wang, Zixiao Wang, Zangir Iklassov, Kailong Zhao, Liang Li, Siwei Liu, Yu Zhong, Wei Liu, Shangsong Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.16188)  

**Abstract**: Deep learning has fundamentally reshaped the landscape of artificial intelligence over the past decade, enabling remarkable achievements across diverse domains. At the heart of these developments lie multi-layered neural network architectures that excel at automatic feature extraction, leading to significant improvements in machine learning tasks. To demystify these advances and offer accessible guidance, we present a comprehensive overview of the most influential deep learning algorithms selected through a broad-based survey of the field. Our discussion centers on pivotal architectures, including Residual Networks, Transformers, Generative Adversarial Networks, Variational Autoencoders, Graph Neural Networks, Contrastive Language-Image Pre-training, and Diffusion models. We detail their historical context, highlight their mathematical foundations and algorithmic principles, and examine subsequent variants, extensions, and practical considerations such as training methodologies, normalization techniques, and learning rate schedules. Beyond historical and technical insights, we also address their applications, challenges, and potential research directions. This survey aims to serve as a practical manual for both newcomers seeking an entry point into cutting-edge deep learning methods and experienced researchers transitioning into this rapidly evolving domain. 

**Abstract (ZH)**: 过去十年中，深度学习从根本上重塑了人工智能的格局，使其在各个领域取得了显著成就。这一系列进展的核心在于多层神经网络架构，它们擅长自动特征提取，显著提升了机器学习任务的效果。为揭开这些进步的面纱并提供易于理解的指导，我们通过广泛的领域调研，对最具影响力的深度学习算法进行了全面概述。我们的讨论集中在若干关键架构，包括残差网络、变换器、生成对抗网络、变分自编码器、图神经网络、对比语言-图像预训练和扩散模型。我们将详细介绍这些架构的历史背景，强调其数学基础和算法原理，并考察后续变体、扩展以及诸如训练方法、正则化技术、学习率调度等实际考虑。除了历史和技术洞察，我们还探讨了它们的应用、挑战及其潜在的研究方向。本调研旨在为寻求进入前沿深度学习方法的新手提供实用指南，也为正在过渡到这一快速发展的领域的资深研究人员提供帮助。 

---
# HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing 

**Title (ZH)**: HashEvict：一种基于局部敏感哈希的预注意力键值缓存淘汰策略 

**Authors**: Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos  

**Link**: [PDF](https://arxiv.org/pdf/2412.16187)  

**Abstract**: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce LSH-E, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, LSH-E makes these decisions pre-attention, thereby reducing computational costs. Additionally, LSH-E is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that LSH-E can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks. 

**Abstract (ZH)**: 基于Transformer的大语言模型（LLM）使用键值（KV）缓存来显著加速推理过程，通过存储过去令牌的键和值嵌入。然而，这种缓存会消耗大量的GPU内存。在本工作中，我们提出了一种名为LSH-E的算法，该算法利用局部敏感哈希（LSH）来压缩KV缓存。LSH-E通过计算当前查询令牌和缓存令牌键的二元高斯投影之间的汉明距离，快速定位与当前查询令牌余弦相似度较低的令牌。这一过程将投影长度设置为远小于嵌入维度。我们维护一个轻量级的二进制结构在GPU内存中，以支持这些计算。与现有的压缩策略不同，LSH-E在注意力计算之前做出这些决定，从而降低计算成本。此外，LSH-E是动态的——在每次解码步骤中，当前令牌的键和值会替换预期会产生最低注意力分数的令牌的嵌入。我们证明LSH-E可以在保持高性能的同时，将KV缓存压缩30%-70%，适用于推理、多项选择、长上下文检索和总结等多种任务。 

---
# Decoding Poultry Vocalizations -- Natural Language Processing and Transformer Models for Semantic and Emotional Analysis 

**Title (ZH)**: poultry vocalizations的解码——基于自然语言处理和变压器模型的语义与情感分析

为了保持学术规范和准确性，这里做了适当的调整：“Decoding Poultry Vocalizations”翻译为“poultry vocalizations的解码”，这是生物学和动物科学中的专业术语。“Natural Language Processing and Transformer Models for Semantic and Emotional Analysis”翻译为“基于自然语言处理和变压器模型的语义与情感分析”，其中“自然语言处理”和“变压器模型”是计算机科学领域中常用的专业术语。 

**Authors**: Venkatraman Manikandan, Suresh Neethirajan  

**Link**: [PDF](https://arxiv.org/pdf/2412.16182)  

**Abstract**: Deciphering the acoustic language of chickens offers new opportunities in animal welfare and ecological informatics. Their subtle vocal signals encode health conditions, emotional states, and dynamic interactions within ecosystems. Understanding the semantics of these calls provides a valuable tool for interpreting their functional vocabulary and clarifying how each sound serves a specific purpose in social and environmental contexts. We apply advanced Natural Language Processing and transformer based models to translate bioacoustic data into meaningful insights. Our method integrates Wave2Vec 2.0 for raw audio feature extraction with a fine tuned Bidirectional Encoder Representations from Transformers model, pretrained on a broad corpus of animal sounds and adapted to poultry tasks. This pipeline decodes poultry vocalizations into interpretable categories including distress calls, feeding signals, and mating vocalizations, revealing emotional nuances often overlooked by conventional analyses. Achieving 92 percent accuracy in classifying key vocalization types, our approach demonstrates the feasibility of real time automated monitoring of flock health and stress. By tracking this functional vocabulary, farmers can respond proactively to environmental or behavioral changes, improving poultry welfare, reducing stress related productivity losses, and supporting more sustainable farm management. Beyond agriculture, this research enhances our understanding of computational ecology. Accessing the semantic foundation of animal calls may indicate biodiversity, environmental stressors, and species interactions, informing integrative ecosystem level decision making. 

**Abstract (ZH)**: deciphering 声学语言是揭示家禽的多种可能性，有助于动物福利和生态信息学的发展。它们微妙的声学信号包含了健康状况、情绪状态以及生态系统内动态互动的信息。理解这些叫声的语义为解读其功能词汇提供了有价值的工具，阐明了每种声音在社会和环境背景下的特定功能。我们应用先进的自然语言处理技术和基于变换器的模型来将生物声学数据转化为有意义的见解。该方法结合了Wave2Vec 2.0 用于原始音频特征提取，并采用预先在广泛动物声音语料库上训练、并适配于家禽任务的双向编码表示变换器模型。该流水线将家禽的叫声解码为可解释的类别，包括应激叫声、觅食信号和交配叫声，揭示了常被传统分析忽视的情绪细微差别。该方法在分类关键叫声类型方面达到了92%的准确性，展示了实时自动监测家禽群健康状况和应激情况的可能性。通过跟踪这种功能词汇，农民可以积极应对环境或行为变化，改善家禽福利，减少与其相关的生产力损失，并促进更可持续的农场管理。除了农业外，这项研究还增强了我们对计算生态学的理解。访问动物叫声的语义基础可能反映出生物多样性、环境压力和物种间的互动，从而为综合生态系统层面的决策提供依据。 

---
# Minimum Weighted Feedback Arc Sets for Ranking from Pairwise Comparisons 

**Title (ZH)**: 最小加权反馈有向边集：基于成对比较的排名问题 

**Authors**: Soroush Vahidi, Ioannis Koutis  

**Link**: [PDF](https://arxiv.org/pdf/2412.16181)  

**Abstract**: The Minimum Weighted Feedback Arc Set (MWFAS) problem is fundamentally connected to the Ranking Problem -- the task of deriving global rankings from pairwise comparisons. Recent work [He et al. ICML2022] has advanced the state-of-the-art for the Ranking Problem using learning-based methods, improving upon multiple previous approaches. However, the connection to MWFAS remains underexplored. This paper investigates this relationship and presents efficient combinatorial algorithms for solving MWFAS, thus addressing the Ranking Problem. Our experimental results demonstrate that these simple, learning-free algorithms not only significantly outperform learning-based methods in terms of speed but also generally achieve superior ranking accuracy. 

**Abstract (ZH)**: 最小加权反馈 arcs 集合（Minimum Weighted Feedback Arc Set, MWFAS）问题与排名问题（Ranking Problem）从根本上说是密切相关的，后者涉及从成对比较中推导全局排名。近年来，[He等人, ICML2022]的工作利用基于学习的方法推进了排名问题的最新进展，改进了多种先前的方法。然而，MWFAS 与排名问题之间的联系仍然缺乏深入探讨。本文探讨了这种关系，并提出了有效的组合算法来解决 MWFAS，从而应对排名问题。我们的实验结果表明，这些简单的、无学习的方法不仅在速度上显著优于基于学习的方法，而且通常在排名准确性上也表现出更优的性能。 

---
# Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHRs 

**Title (ZH)**: 上下文线索：评估长上下文模型在电子健康记录（EHR）临床预测任务中的表现 

**Authors**: Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan Steinberg, Jason Alan Fries, Christopher Ré, Sanmi Koyejo, Nigam H. Shah  

**Link**: [PDF](https://arxiv.org/pdf/2412.16178)  

**Abstract**: Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, most existing EHR FMs have context windows of <1k tokens. This prevents them from modeling full patient EHRs which can exceed 10k's of events. Recent advancements in subquadratic long-context architectures (e.g., Mamba) offer a promising solution. However, their application to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data. We find that longer context models improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. For clinical applications, however, model performance alone is insufficient -- robustness to the unique properties of EHR is crucial. Thus, we also evaluate models across three previously underexplored properties of EHR data: (1) the prevalence of "copy-forwarded" diagnoses which creates artificial repetition of tokens within EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that higher levels of each property correlate negatively with model performance, but that longer context models are more robust to more extreme levels of these properties. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study for identifying new challenges in modeling sequential data motivated by domains outside of natural language. We release our models and code at: this https URL 

**Abstract (ZH)**: 基于电子健康记录（EHRs）训练的foundation模型（FMs）已经在众多临床预测任务中取得了最优结果。然而，现有的大多数EHR FMs具有<1k令牌的上下文窗口，这限制了它们对超过10k事件的完整EHR进行建模的能力。最近在子二次长上下文架构方面的进展（如Mamba）提供了一个有希望的解决方案。然而，它们在EHR数据中的应用尚未得到充分研究。我们通过首先系统地评估上下文长度对EHR数据建模效果的影响来填补这一空白。我们发现，较长的上下文模型能提高预测性能——我们的基于Mamba的模型在EHRSHOT预测基准中的9/14任务上超过了之前的最优结果。然而，对于临床应用而言，仅仅依赖模型性能是不够的，模型对EHR的独特属性的鲁棒性至关重要。因此，我们还评估了模型在EHR数据的三个之前未充分探索的属性上：（1）“向前填充”诊断的高发性，这会在EHR序列内创建人为的重复现象；（2）EHR事件之间不规则的时间间隔，可能导致上下文窗口内的广泛时间跨度；以及（3）随着时间的推移，疾病复杂性的自然增加，使得EHR后期的令牌比早期的更难以预测。分层我们的EHRSHOT结果，我们发现每种属性的较高水平与模型性能的下降呈负相关，但较长的上下文模型对这些属性的极端水平更具鲁棒性。我们的工作突出了使用长上下文架构来建模EHR数据的潜力，并提供了一个用以识别源自自然语言之外领域的新挑战的案例研究，以建模序列数据。我们在此分享我们的模型和代码：[此链接] 

---
# Unveiling the Role of Artificial Intelligence and Stock Market Growth in Achieving Carbon Neutrality in the United States: An ARDL Model Analysis 

**Title (ZH)**: 探索人工智能在实现美国碳中目标中的作用及其与股市增长的关系：基于ARDL模型的分析 

**Authors**: Azizul Hakim Rafi, Abdullah Al Abrar Chowdhury, Adita Sultana, Abdulla All Noman  

**Link**: [PDF](https://arxiv.org/pdf/2412.16166)  

**Abstract**: Given the fact that climate change has become one of the most pressing problems in many countries in recent years, specialized research on how to mitigate climate change has been adopted by many countries. Within this discussion, the influence of advanced technologies in achieving carbon neutrality has been discussed. While several studies investigated how AI and Digital innovations could be used to reduce the environmental footprint, the actual influence of AI in reducing CO2 emissions (a proxy measuring carbon footprint) has yet to be investigated. This paper studies the role of advanced technologies in general, and Artificial Intelligence (AI) and ICT use in particular, in advancing carbon neutrality in the United States, between 2021. Secondly, this paper examines how Stock Market Growth, ICT use, Gross Domestic Product (GDP), and Population affect CO2 emissions using the STIRPAT model. After examining stationarity among the variables using a variety of unit root tests, this study concluded that there are no unit root problems across all the variables, with a mixed order of integration. The ARDL bounds test for cointegration revealed that variables in this study have a long-run relationship. Moreover, the estimates revealed from the ARDL model in the short- and long-run indicated that economic growth, stock market capitalization, and population significantly contributed to the carbon emissions in both the short-run and long-run. Conversely, AI and ICT use significantly reduced carbon emissions over both periods. Furthermore, findings were confirmed to be robust using FMOLS, DOLS, and CCR estimations. Furthermore, diagnostic tests indicated the absence of serial correlation, heteroscedasticity, and specification errors and, thus, the model was robust. 

**Abstract (ZH)**: 鉴于近年来气候变化已成为许多国家面临的最紧迫问题之一，各国已开始专门研究如何减轻气候变化的影响。在这一讨论中，先进技术和实现碳中和之间的关系已经得到了探讨。虽然已有研究表明人工智能和数字化创新如何减少环境足迹，但在减碳效果方面，特别是通过二氧化碳排放量（一项衡量碳足迹的替代指标）来具体评估人工智能的实际影响的研究尚付之阙如。本文研究了先进技术和具体地讲，人工智能（AI）和信息通信技术（ICT）在2021年美国实现碳中和方面的积极作用。其次，本文利用STIRPAT模型探讨了股票市场增长、ICT使用、国内生产总值（GDP）和人口对二氧化碳排放量的影响。经过各种单位根检验，确认变量之间不存在单位根问题，并且变量具有混合阶数。ARDL协整检验边界测试表明，本研究中的变量在长期存在关系。此外，ARDL模型在短、长期的估计结果表明，经济增长、股票市场资本化和人口在短、长期内显著增加了碳排放；相反，AI和ICT使用在两个期间显著减少了碳排放。进一步使用FMOLS、DOLS和CCR估计方法证实了这些发现的稳健性。另外，诊断测试表明模型不存在序列相关、异方差性和设定误差，因此该模型具有稳健性。 

---
# Adaptive Large Language Models By Layerwise Attention Shortcuts 

**Title (ZH)**: 逐层注意力捷径的自适应大型语言模型 

**Authors**: Prateek Verma, Mert Pilanci  

**Link**: [PDF](https://arxiv.org/pdf/2409.10870)  

**Abstract**: Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens. 

**Abstract (ZH)**: Transformer 架构是现代人工智能革命的核心。然而，它们通常是通过简单地堆叠相同的模块并在数十层中顺序处理信息来构建的。在这篇论文中，我们提出了一种挑战这一现状的方法，并介绍了为类似大规模语言模型的设置中引入自适应计算，使得最终层能够通过注意机制适当地关注所有中间层，从而引入计算上的 **注意捷径**。这些捷径可以使架构在深度和上下文方面具有自适应性。我们展示了四个不同的数据集，包括声学标记、自然语言和符号音乐，并对类似 GPT 的架构实现了更好的性能。通过注意力图展示了模型在输入标记的上下文和深度依赖关系上学习到复杂关联的能力。 

---
# Neural Style Transfer for Audio Spectograms 

**Title (ZH)**: 音频频谱的神经风格迁移 

**Authors**: Prateek Verma, Julius O. Smith  

**Link**: [PDF](https://arxiv.org/pdf/1801.01589)  

**Abstract**: There has been fascinating work on creating artistic transformations of images by Gatys. This was revolutionary in how we can in some sense alter the 'style' of an image while generally preserving its 'content'. In our work, we present a method for creating new sounds using a similar approach, treating it as a style-transfer problem, starting from a random-noise input signal and iteratively using back-propagation to optimize the sound to conform to filter-outputs from a pre-trained neural architecture of interest.
For demonstration, we investigate two different tasks, resulting in bandwidth expansion/compression, and timbral transfer from singing voice to musical instruments. A feature of our method is that a single architecture can generate these different audio-style-transfer types using the same set of parameters which otherwise require different complex hand-tuned diverse signal processing pipelines. 

**Abstract (ZH)**: Gatys等人在图像艺术转换方面的研究令人着迷。他们的工作在某种意义上改变了我们处理图像风格的方式，同时一般保留其内容。在我们的研究中，我们提出了一种方法，利用类似的方法来生成新的声音，将其视为一种风格转换问题。我们从随机噪声输入信号开始，通过反向传播逐步优化声音，使其符合感兴趣预训练神经架构的滤波器输出。

为了演示，我们探讨了两个不同的任务，实现了带宽扩展/压缩，以及歌唱人声的声音质地转移到乐器上。我们的方法的一个特点是，同一个架构可以使用相同的参数生成不同类型的音频风格转换，而这些通常需要不同复杂的手动调整的信号处理管道。 

---
