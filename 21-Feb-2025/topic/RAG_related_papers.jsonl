{'arxiv_id': 'arXiv:2502.14759', 'title': 'On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems', 'authors': 'Juraj Vladika, Florian Matthes', 'link': 'https://arxiv.org/abs/2502.14759', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.', 'abstract_zh': '检索增强生成（RAG）作为一种通过减少对静态知识的依赖并提高答案的准确性来增强大规模语言模型（LLMs）的方法而崭露头角。RAG会检索相关上下文片段，并基于这些片段生成答案。尽管RAG在工业界的应用越来越广泛，但对其组件的系统性探索仍然不足，尤其是在提供上下文的理想大小以及基础LLM和检索方法的选择方面。为了指导稳健的RAG系统的开发，我们评估了不同大小的上下文、BM25和语义搜索作为检索方法，以及八种基础LLM。我们从传统的使用短答案评估RAG的方法转向探索更具挑战性的长形式问题回答，尤其是在两个领域中，良好的答案需要充分利用整个上下文。我们的研究结果显示，最终的问答性能随着最多15个片段的增加而逐步提高，但在超过这个数量后则停滞不前或下降。最后，我们表明，不同的通用语言模型在医学领域的表现优于百科全书领域，而在大型语料库中进行开放领域证据检索具有挑战性。', 'title_zh': '关于上下文大小和模型选择对检索增强生成系统影响的研究'}
{'arxiv_id': 'arXiv:2502.14614', 'title': 'FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis', 'authors': 'Mingyi Jia, Junwen Duan, Yan Song, Jianxin Wang', 'link': 'https://arxiv.org/abs/2502.14614', 'abstract': 'Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained \\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，要符合学术规范：\n\n检索增强的大语言模型（RAG，Retrieval-Augmented Large Language Models）通过将外部知识整合到大语言模型中，在各种医学领域，包括临床诊断中已经展现了显著的效果。然而，现有的RAG方法难以有效地评估任务难度以做出检索决策，从而未能满足临床对效率和准确性的平衡要求。因此，在本文中，我们提出了一种名为FIND（细粒度信息密度指导的自适应RAG）的新框架，该框架旨在提高RAG在疾病诊断场景中的可靠性。FIND集成了一个细粒度的自适应控制模块，该模块根据输入的信息密度来确定是否需要进行检索。通过优化检索过程并实现一个知识过滤模块，FIND确保了检索更适合于临床场景。在三个中文电子病历数据集上的实验表明，FIND显著优于各种基准方法，突显了其在临床诊断任务中的有效性。', 'title_zh': 'FIND：细粒度信息密度引导的自适应检索增强生成方法在疾病诊断中的应用'}
{'arxiv_id': 'arXiv:2502.14271', 'title': 'PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant', 'authors': 'Congrui Yin, Evan Wei, Zhongxing Zhang, Zaifu Zhan', 'link': 'https://arxiv.org/abs/2502.14271', 'abstract': 'In the paper, we introduce a paper reading assistant, PaperHelper, a potent tool designed to enhance the capabilities of researchers in efficiently browsing and understanding scientific literature. Utilizing the Retrieval-Augmented Generation (RAG) framework, PaperHelper effectively minimizes hallucinations commonly encountered in large language models (LLMs), optimizing the extraction of accurate, high-quality knowledge. The implementation of advanced technologies such as RAFT and RAG Fusion significantly boosts the performance, accuracy, and reliability of the LLMs-based literature review process. Additionally, PaperHelper features a user-friendly interface that facilitates the batch downloading of documents and uses the Mermaid format to illustrate structural relationships between documents. Experimental results demonstrate that PaperHelper, based on a fine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8 seconds, outperforming the basic RAG model by 7\\% in F1 Score.', 'abstract_zh': '在本文中，我们引入了一种论文阅读助手——PaperHelper，这是一种强大的工具，旨在增强研究人员高效浏览和理解科学文献的能力。通过利用检索增强生成（RAG）框架，PaperHelper有效减少了大型语言模型（LLMs）中常见的幻觉现象，优化了高质量、高准确性的知识提取。先进的技术如RAFT和RAG融合的实施显著提升了基于LLMs的文献回顾过程的性能、准确性和可靠性。此外，PaperHelper还具备用户友好的界面，便于批量下载文档，并使用Mermaid格式展示文档之间的结构关系。实验结果表明，基于微调的GPT-4 API，PaperHelper实现了F1分数60.04，响应延迟仅为5.8秒，相比基本的RAG模型在F1分数上提高了7%。', 'title_zh': 'PaperHelper：基于知识的LLM问答式论文阅读辅助系统'}
{'arxiv_id': 'arXiv:2502.14245', 'title': 'Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering', 'authors': 'Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, Wei Hu', 'link': 'https://arxiv.org/abs/2502.14245', 'abstract': 'In this paper, we identify a critical problem, "lost-in-retrieval", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs\' sub-question decomposition. "Lost-in-retrieval" significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets$\\unicode{x2013}$MuSiQue, 2Wiki, and HotpotQA$\\unicode{x2013}$using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.', 'abstract_zh': '在本文中，我们识别出检索增强多跳问答（QA）中的一个关键问题：“检索丢失”（lost-in-retrieval）：在大语言模型（LLMs）的子问题分解过程中，关键实体被遗漏了。“检索丢失”严重影响了检索性能，打断了推理链，导致错误的答案。为了解决这一问题，我们提出了一种渐进式的检索和重写方法，称为ChainRAG，该方法逐个处理每个子问题，通过补充缺失的关键实体并从句子图中检索相关句子来生成答案。在我们的检索和重写过程中，每一步都依赖于上一步，形成一个无缝的链，从而实现准确的检索和答案生成。最后，所有检索到的句子和子问题答案被整合起来生成原始问题的全面答案。我们使用三个多跳问答数据集（MuSiQue、2Wiki 和 HotpotQA）以及三个大型语言模型（GPT4o-mini、Qwen2.5-72B 和 GLM-4-Plus）对ChainRAG 进行了评估。实证结果表明，ChainRAG 在有效性和效率上都优于基线方法。', 'title_zh': '缓解检索增强多跳问答中的检索丢失问题'}
{'arxiv_id': 'arXiv:2502.14864', 'title': 'Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework', 'authors': 'Yuming Yang, Jiang Zhong, Li Jin, Jingwang Huang, Jingpeng Gao, Qing Liu, Yang Bai, Jingyuan Zhang, Rui Jiang, Kaiwen Wei', 'link': 'https://arxiv.org/abs/2502.14864', 'abstract': 'Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, Chart-based MRAG, to address this limitation. To semi-automatically generate high-quality evaluation samples, we propose CHARt-based document question-answering GEneration (CHARGE), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation. By combining CHARGE with expert validation, we construct Chart-MRAG Bench, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents. Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are released at this https URL.', 'abstract_zh': '多模态检索增强生成（MRAG）通过集成外部知识增强了推理能力。然而，现有的基准主要关注简单的图像-文本交互，忽视了在实际应用中常见的复杂视觉格式，如图表。为此，我们在本文中提出了一项新颖的任务，即基于图表的MRAG，以解决这一局限性。为了半自动地生成高质量的评估样本，我们提出了基于图表的文档问答生成（CHARGE）框架，该框架通过结构化关键点提取、跨模态验证和关键点驱动的生成来生成评估数据。通过将CHARGE与专家验证相结合，我们构建了基于图表的MRAG基准（Chart-MRAG Bench），该基准涵盖了来自8个实际文档领域的4,738个问答对。我们的评估揭示了当前方法存在三个关键不足：（1）统一的多模态嵌入检索方法在基于图表的场景中表现不佳；（2）即使有 ground-truth 检索，最先进的大规模语言模型也只能达到58.19%的正确率和73.87%的覆盖范围；（3）大规模语言模型在基于图表的MRAG推理过程中表现出持续的文本占优视觉模式偏见。CHARGE 和 Chart-MRAG Bench 已在此 HTTPS 地址发布：[https://example.com/CHARGE-and-Chart-MRAG-Bench](https://example.com/CHARGE-and-Chart-MRAG-Bench)', 'title_zh': '基于图表型文档问答生成框架的多模态RAG基准测试'}
{'arxiv_id': 'arXiv:2502.14727', 'title': 'WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models', 'authors': 'Yifu Chen, Shengpeng Ji, Haoxiao Wang, Ziqing Wang, Siyu Chen, Jinzheng He, Jin Xu, Zhou Zhao', 'link': 'https://arxiv.org/abs/2502.14727', 'abstract': "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.", 'abstract_zh': '检索增强生成（RAG）由于其增强大型语言模型（LLMs）整合外部知识的能力而被广泛应用。然而，现有的RAG框架主要针对基于文本的LLMs，并依赖自动语音识别（ASR）处理语音输入，这会丢弃重要的音频信息、增加转录错误的风险以及增加计算量。因此，我们引入了WavRAG，这是首个提供原生端到端语音支持的检索增强生成框架。WavRAG具备两项关键功能：1）绕过ASR，WavRAG直接处理原始音频进行嵌入和检索；2）WavRAG将音频和文本统一到一个知识表示中。具体而言，我们提出了WavRetriever以从文本-音频混合知识库中进行检索，并通过将链式推理集成到其中，进一步增强了口语对话模型的语境能力。与最新的ASR-Text RAG流水线相比，WavRAG实现了相当的检索性能，同时加速了10倍。此外，WavRAG独特的声音-文本混合检索能力将RAG的应用范围扩展到了音频模态。', 'title_zh': 'WavRAG：结合音频的检索增强生成模型用于口语对话系统'}
{'arxiv_id': 'arXiv:2502.14080', 'title': 'Personalized Education with Generative AI and Digital Twins: VR, RAG, and Zero-Shot Sentiment Analysis for Industry 4.0 Workforce Development', 'authors': 'Yu-Zheng Lin, Karan Petal, Ahmed H Alhamadah, Sujan Ghimire, Matthew William Redondo, David Rafael Vidal Corona, Jesus Pacheco, Soheil Salehi, Pratik Satam', 'link': 'https://arxiv.org/abs/2502.14080', 'abstract': "The Fourth Industrial Revolution (4IR) technologies, such as cloud computing, machine learning, and AI, have improved productivity but introduced challenges in workforce training and reskilling. This is critical given existing workforce shortages, especially in marginalized communities like Underrepresented Minorities (URM), who often lack access to quality education. Addressing these challenges, this research presents gAI-PT4I4, a Generative AI-based Personalized Tutor for Industrial 4.0, designed to personalize 4IR experiential learning. gAI-PT4I4 employs sentiment analysis to assess student comprehension, leveraging generative AI and finite automaton to tailor learning experiences. The framework integrates low-fidelity Digital Twins for VR-based training, featuring an Interactive Tutor - a generative AI assistant providing real-time guidance via audio and text. It uses zero-shot sentiment analysis with LLMs and prompt engineering, achieving 86\\% accuracy in classifying student-teacher interactions as positive or negative. Additionally, retrieval-augmented generation (RAG) enables personalized learning content grounded in domain-specific knowledge. To adapt training dynamically, finite automaton structures exercises into states of increasing difficulty, requiring 80\\% task-performance accuracy for progression. Experimental evaluation with 22 volunteers showed improved accuracy exceeding 80\\%, reducing training time. Finally, this paper introduces a Multi-Fidelity Digital Twin model, aligning Digital Twin complexity with Bloom's Taxonomy and Kirkpatrick's model, providing a scalable educational framework.", 'abstract_zh': '第四次工业革命（4IR）技术，如云计算、机器学习和人工智能，虽然提高了生产力，但也为劳动力培训和再培训带来了挑战。鉴于现有的劳动力短缺问题，尤其是在被边缘化的社区，如代表性不足的少数群体（URM），他们往往缺乏高质量教育机会。为应对这些挑战，本研究提出了一种基于生成式人工智能的个性化导师gAI-PT4I4，旨在个性化4IR体验式学习。gAI-PT4I4利用情感分析评估学生理解程度，结合生成式人工智能和有限自动机以定制学习体验。该框架整合了用于VR培训的低保真数字孪生，其中包括交互式导师——一个生成式人工智能助理，可提供实时音频和文本指导。它使用零样本的情感分析与大规模语言模型（LLM）及提示工程相结合，实现86%的准确性，用于分类学生-教师互动为正面或负面。此外，检索增强生成（RAG）技术能够生成基于专业领域知识的个性化学习内容。为动态适应培训需求，有限自动机结构化训练练习为递增难度的状态，要求任务执行准确率达到80%才能进步。实验评估使用22名志愿者显示，学习准确率超过了80%，从而缩短了培训时间。最后，本文介绍了多保真度数字孪生模型，该模型将数字孪生的复杂性与布卢姆分类法和柯克帕特里克模型相匹配，提供了一个可扩展的教育框架。', 'title_zh': '基于生成式AI和数字孪生的个性化教育：面向工业4.0劳动力发展的VR、RAG和零样本情感分析'}
{'arxiv_id': 'arXiv:2502.14137', 'title': 'Collaborative Retrieval for Large Language Model-based Conversational Recommender Systems', 'authors': 'Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, Jundong Li', 'link': 'https://arxiv.org/abs/2502.14137', 'abstract': 'Conversational recommender systems (CRS) aim to provide personalized recommendations via interactive dialogues with users. While large language models (LLMs) enhance CRS with their superior understanding of context-aware user preferences, they typically struggle to leverage behavioral data, which have proven to be important for classical collaborative filtering (CF)-based approaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented Generation for LLM-based CRS. To the best of our knowledge, CRAG is the first approach that combines state-of-the-art LLMs with CF for conversational recommendations. Our experiments on two publicly available movie conversational recommendation datasets, i.e., a refined Reddit dataset (which we name Reddit-v2) as well as the Redial dataset, demonstrate the superior item coverage and recommendation performance of CRAG, compared to several CRS baselines. Moreover, we observe that the improvements are mainly due to better recommendation accuracy on recently released movies. The code and data are available at this https URL.', 'abstract_zh': '对话推荐系统（CRS）旨在通过与用户的交互对话提供个性化推荐。大型语言模型（LLMs）通过对其上下文感知用户偏好的深刻理解，增强了CRS的能力，但通常难以利用行为数据，而这些行为数据对于基于协作过滤（CF）的经典方法来说是非常重要的。鉴于此，我们提出了CRAG方法，即结合了大型语言模型与协作过滤的对话推荐增强生成方法。据我们所知，CRAG是第一个将最先进的大型语言模型与CF相结合以进行对话推荐的方法。我们在两个公开的电影对话推荐数据集上进行了实验，即经过精炼的Reddit数据集（我们命名为Reddit-v2）以及Redial数据集，结果显示，CRAG在项目覆盖范围和推荐性能方面显著优于几种基准对话推荐系统。此外，我们观察到，这些改进主要归因于在最近上映的电影上的推荐准确性提高。代码和数据可从以下链接获取：[这里提供的链接]。', 'title_zh': '基于大型语言模型的对话推荐系统中的协作检索方法'}
