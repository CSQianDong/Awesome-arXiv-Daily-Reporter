# MAGELLAN: Metacognitive predictions of learning progress guide autotelic LLM agents in large goal spaces 

**Title (ZH)**: MAGELLAN：元认知预测学习进展引导自主学习目标空间中的大型语言模型代理 

**Authors**: Loris Gaven, Thomas Carta, Clément Romac, Cédric Colas, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer  

**Link**: [PDF](https://arxiv.org/pdf/2502.07709)  

**Abstract**: Open-ended learning agents must efficiently prioritize goals in vast possibility spaces, focusing on those that maximize learning progress (LP). When such autotelic exploration is achieved by LLM agents trained with online RL in high-dimensional and evolving goal spaces, a key challenge for LP prediction is modeling one's own competence, a form of metacognitive monitoring. Traditional approaches either require extensive sampling or rely on brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive framework that lets LLM agents learn to predict their competence and LP online. By capturing semantic relationships between goals, MAGELLAN enables sample-efficient LP estimation and dynamic adaptation to evolving goal spaces through generalization. In an interactive learning environment, we show that MAGELLAN improves LP prediction efficiency and goal prioritization, being the only method allowing the agent to fully master a large and evolving goal space. These results demonstrate how augmenting LLM agents with a metacognitive ability for LP predictions can effectively scale curriculum learning to open-ended goal spaces. 

**Abstract (ZH)**: 开放学习代理必须在广阔的可能空间中有效地优先考虑目标，专注于那些能够最大化学习进展（Learning Progress, LP）的目标。当使用在线强化学习训练的语言模型代理在高维度和不断演化的目标空间中实现自主探索时，LP预测中的一个关键挑战是对其自身能力进行建模，这是一种元认知监控的形式。传统方法要么需要大量的采样，要么依赖于脆弱的专家定义的目标分组。我们提出了MAGELLAN（元认知框架），该框架使得语言模型代理能够在在线预测其能力和LP时学习这些技能。通过捕获目标之间的语义关系，MAGELLAN能够实现高效的样本利用，并通过泛化适应不断演化的目标空间。在交互式学习环境中，我们显示MAGELLAN能够提高LP预测效率和目标优先级管理，是唯一能够让代理完全掌握一个庞大且不断演化的目标空间的方法。这些结果表明，通过为语言模型代理增加一个LP预测的元认知能力，可以有效地将课程学习扩展到开放的目标空间。 

---
# Human Decision-making is Susceptible to AI-driven Manipulation 

**Title (ZH)**: 人类决策易受AI驱动的操控影响 

**Authors**: Sahand Sabour, June M. Liu, Siyang Liu, Chris Z. Yao, Shiyao Cui, Xuanming Zhang, Wen Zhang, Yaru Cao, Advait Bhat, Jian Guan, Wei Wu, Rada Mihalcea, Tim Althoff, Tatia M.C. Lee, Minlie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07663)  

**Abstract**: Artificial Intelligence (AI) systems are increasingly intertwined with daily life, assisting users in executing various tasks and providing guidance on decision-making. This integration introduces risks of AI-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes. Through a randomized controlled trial with 233 participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts. Participants interacted with one of three AI agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit psychological tactics to reach its hidden objectives. By analyzing participants' decision patterns and shifts in their preference ratings post-interaction, we found significant susceptibility to AI-driven manipulation. Particularly, across both decision-making domains, participants interacting with the manipulative agents shifted toward harmful options at substantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA: 42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional, 12.8%). Notably, our findings reveal that even subtle manipulative objectives (MA) can be as effective as employing explicit psychological strategies (SEMA) in swaying human decision-making. By revealing the potential for covert AI influence, this study highlights a critical vulnerability in human-AI interactions, emphasizing the need for ethical safeguards and regulatory frameworks to ensure responsible deployment of AI technologies and protect human autonomy. 

**Abstract (ZH)**: 人工智能（AI）系统日益融合到日常生活中，辅助用户执行各种任务并提供决策指导。这种整合引入了由AI驱动的操控风险，这些系统可能会利用用户的认知偏差和情感脆弱性，引导他们走向有害的结果。通过一项包含233名参与者的随机对照试验，我们探讨了在金融（例如，购买）和情感（例如，冲突解决）决策情境下人类对这种操控的易感性。参与者与三种AI代理之一进行了互动：中立代理（NA），旨在优化用户利益而不进行明确影响；操控代理（MA），设计为隐蔽地影响信念和行为；以及策略增强操控代理（SEMA），运用明确的心理战术以实现其隐秘目标。通过对参与者决策模式和互动后的偏好评级变化进行分析，我们发现对由AI驱动的操控具有显著的易感性。特别是在两个决策领域，与操控代理互动的参与者转向有害选项的比例显著更高（金融领域，MA：62.3%，SEMA：59.6%；情感领域，MA：42.3%，SEMA：41.5%），而中立代理组的比例较低（金融领域，35.8%；情感领域，12.8%）。值得注意的是，我们的研究发现，即使是较为微妙的操控目标（MA）也能在引导人类决策方面与使用明确的心理策略（SEMA）同样有效。通过揭示隐蔽的AI影响的潜在性，本研究强调了人类与AI互动中的关键脆弱性，强调了需要伦理保障和监管框架来确保负责任地部署人工智能技术和保护人类自主性的重要性。 

---
# SymGPT: Auditing Smart Contracts via Combining Symbolic Execution with Large Language Models 

**Title (ZH)**: SymGPT：结合符号执行与大型语言模型审计智能合约 

**Authors**: Shihao Xia, Mengting He, Shuai Shao, Tingting Yu, Yiying Zhang, Linhai Song  

**Link**: [PDF](https://arxiv.org/pdf/2502.07644)  

**Abstract**: To govern smart contracts running on Ethereum, multiple Ethereum Request for Comment (ERC) standards have been developed, each having a set of rules to guide the behaviors of smart contracts. Violating the ERC rules could cause serious security issues and financial loss, signifying the importance of verifying smart contracts follow ERCs. Today's practices of such verification are to manually audit each single contract, use expert-developed program-analysis tools, or use large language models (LLMs), all of which are far from effective in identifying ERC rule violations. This paper introduces SymGPT, a tool that combines the natural language understanding of large language models (LLMs) with the formal guarantees of symbolic execution to automatically verify smart contracts' compliance with ERC rules. To develop SymGPT, we conduct an empirical study of 132 ERC rules from three widely used ERC standards, examining their content, security implications, and natural language descriptions. Based on this study, we design SymGPT by first instructing an LLM to translate ERC rules into a defined EBNF grammar. We then synthesize constraints from the formalized rules to represent scenarios where violations may occur and use symbolic execution to detect them. Our evaluation shows that SymGPT identifies 5,783 ERC rule violations in 4,000 real-world contracts, including 1,375 violations with clear attack paths for stealing financial assets, demonstrating its effectiveness. Furthermore, SymGPT outperforms six automated techniques and a security-expert auditing service, underscoring its superiority over current smart contract analysis methods. 

**Abstract (ZH)**: 为了治理运行在以太坊上的智能合约，已经开发了多种以太坊请求评论（ERC）标准，每一种标准都有一套规则来指导智能合约的行为。违反ERC规则可能会导致严重的安全问题和经济损失，这突显了验证智能合约是否遵循ERC规则的重要性。当前这种验证的做法是手动审核每个合约，使用专家开发的程序分析工具，或者使用大型语言模型（LLMs），但这些方法远不能有效地识别ERC规则的违反情况。本文介绍了SymGPT，这是一种结合了大型语言模型（LLMs）的自然语言理解和形式验证的符号执行工具，能够自动验证智能合约是否遵循ERC规则。为了开发SymGPT，我们对三个广泛使用的ERC标准中的132条规则进行了实证研究，研究了这些规则的内容、安全影响及其自然语言描述。基于这些研究，我们首先指示LLM将ERC规则翻译成定义好的EBNF语法。然后，我们从形式化的规则中综合约束条件，表示可能发生的违规场景，并使用符号执行来检测这些场景。我们的评估结果显示，SymGPT在4000个实际合约中发现了5783个ERC规则的违反情况，其中包括1375个有明确攻击路径的违规情况，展示了其效果。此外，SymGPT在六种自动化技术和安全专家审核服务方面表现更优，进一步证明了它在当前智能合约分析方法中的优越性。 

---
# NatureLM: Deciphering the Language of Nature for Scientific Discovery 

**Title (ZH)**: NatureLM：揭示自然语言的科学发现之旅 

**Authors**: Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin  

**Link**: [PDF](https://arxiv.org/pdf/2502.07527)  

**Abstract**: Foundation models have revolutionized natural language processing and artificial intelligence, significantly enhancing how machines comprehend and generate human languages. Inspired by the success of these foundation models, researchers have developed foundation models for individual scientific domains, including small molecules, materials, proteins, DNA, and RNA. However, these models are typically trained in isolation, lacking the ability to integrate across different scientific domains. Recognizing that entities within these domains can all be represented as sequences, which together form the "language of nature", we introduce Nature Language Model (briefly, NatureLM), a sequence-based science foundation model designed for scientific discovery. Pre-trained with data from multiple scientific domains, NatureLM offers a unified, versatile model that enables various applications including: (i) generating and optimizing small molecules, proteins, RNA, and materials using text instructions; (ii) cross-domain generation/design, such as protein-to-molecule and protein-to-RNA generation; and (iii) achieving state-of-the-art performance in tasks like SMILES-to-IUPAC translation and retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach for various scientific tasks, including drug discovery (hit generation/optimization, ADMET optimization, synthesis), novel material design, and the development of therapeutic proteins or nucleotides. We have developed NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion parameters) and observed a clear improvement in performance as the model size increases. 

**Abstract (ZH)**: 基础模型已经重塑了自然语言处理和人工智能，显著增强了机器对人类语言的理解和生成能力。受这些基础模型成功的启发，研究人员开发了适用于各个科学领域的基础模型，包括小分子、材料、蛋白质、DNA和RNA。然而，这些模型通常是在孤立状态下训练的，缺乏跨不同科学领域整合的能力。鉴于这些领域内的实体都可以表示为序列，这些序列构成了“自然语言”，我们引入了Nature Language Model（简称NatureLM），这是一种基于序列的科学基础模型，旨在用于科学研究发现。NatureLM通过多领域科学数据进行预训练，提供了一个统一且多功能的模型，能够支持多种应用，包括：（i）使用文本指令生成和优化小分子、蛋白质、RNA和材料；（ii）跨领域的生成/设计，例如蛋白质到分子和蛋白质到RNA的生成；以及（iii）在SMILES到IUPAC转换和USPTO-50k数据集上的逆合成反应等任务中达到最先进的性能。NatureLM为药物发现（-hit生成/优化、ADMET优化、合成）、新型材料设计以及治疗性蛋白质或核苷酸的开发提供了有希望的通用方法。我们已经开发了不同规模的NatureLM模型（参数量分别为1亿、8亿和46.7亿），并且观察到随着模型规模的增加，性能得到了明显的提升。 

---
# Harnessing Language's Fractal Geometry with Recursive Inference Scaling 

**Title (ZH)**: 利用递归推理缩放技术挖掘语言的分形几何结构 

**Authors**: Ibrahim Alabdulmohsin, Xiaohua Zhai  

**Link**: [PDF](https://arxiv.org/pdf/2502.07503)  

**Abstract**: Recent research in language modeling reveals two scaling effects: the well-known improvement from increased training compute, and a lesser-known boost from applying more sophisticated or computationally intensive inference methods. Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time. For a given fixed model architecture and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. These advantages are maintained even when compared to state-of-the-art recursive techniques like the "repeat-all-over" (RAO) strategy in Mobile LLM. Finally, stochastic RINS not only can enhance performance further but also provides the flexibility to optionally forgo increased inference computation at test time with minimal performance degradation. 

**Abstract (ZH)**: 最近的语言模型研究揭示了两种规模效应：众所周知的训练计算量增加带来的改进，以及鲜为人知的通过应用更为复杂或计算密集的推理方法带来的提升。受语言分形几何学最新发现的启发，我们引入了递归推理缩放（RINS）作为一种补充的、可插拔的缩放方案，以提高推理时间的效率。对于给定的固定模型架构和训练计算预算，RINS显著提升了语言模型性能。此外，RINS还在多模态系统中展现出广泛应用的能力，例如在SigLIP-B/16模型上实现了ImageNet零样本准确率的+2%提升。通过推导出数据缩放定律，我们展示了RINS不仅能够改善渐进性能极限和缩放指数，还能在与移动大模型中的“全重复策略”（RAO）等最先进的递归技术相比较时保持这些优势。最后，随机化的RINS不仅能够进一步提升性能，还能在测试时提供选择性地放弃增加推理计算量的灵活性，且性能下降最小。 

---
# URECA: The Chain of Two Minimum Set Cover Problems exists behind Adaptation to Shifts in Semantic Code Search 

**Title (ZH)**: URECA: 两种最小集覆盖问题的链式关系隐藏在语义代码搜索的适应性调整背后 

**Authors**: Seok-Ung Choi, Joonghyuk Hahn, Yo-Sub Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.07494)  

**Abstract**: Adaptation is to make model learn the patterns shifted from the training distribution. In general, this adaptation is formulated as the minimum entropy problem. However, the minimum entropy problem has inherent limitation -- shifted initialization cascade phenomenon. We extend the relationship between the minimum entropy problem and the minimum set cover problem via Lebesgue integral. This extension reveals that internal mechanism of the minimum entropy problem ignores the relationship between disentangled representations, which leads to shifted initialization cascade. From the analysis, we introduce a new clustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA). URECA is an efficient clustering algorithm for the leverage of the relationships between disentangled representations. The update rule of URECA depends on Thresholdly-Updatable Stationary Assumption to dynamics as a released version of Stationary Assumption. This assumption helps URECA to transport disentangled representations with no errors based on the relationships between disentangled representations. URECA also utilize simulation trick to efficiently cluster disentangled representations. The wide range of evaluations show that URECA achieves consistent performance gains for the few-shot adaptation to diverse types of shifts along with advancement to State-of-The-Art performance in CoSQA in the scenario of query shift. 

**Abstract (ZH)**: 适应是对模型进行训练，使其学习与训练分布相移的模式。通常，这种适应被公式化为最小熵问题。然而，最小熵问题固有地存在一个局限性——即相移初始化级联现象。我们通过勒贝格积分将最小熵问题与最小集覆盖问题的关系进行了扩展。这一扩展揭示了最小熵问题的内部机制忽略了去纠缠表示之间的关系，这导致了相移初始化级联现象。通过分析，我们引入了一种新的聚类算法——基于联合查找的递归聚类算法（URECA）。URECA 是一种利用去纠缠表示之间关系的高效聚类算法。URECA 的更新规则依赖于一种临界可更新稳态假设，作为稳态假设的放宽版本，这种假设有助于URECA 根据去纠缠表示之间的关系无误地传输去纠缠表示。URECA 也利用仿真技巧来高效地聚类去纠缠表示。广泛的评估结果显示，URECA 在多种类型的数据转移场景中实现了对少量射适应的一致性能提升，并在 CoSQA 场景下的查询转移中取得了接近最先进的性能。 

---
# Eliciting Rational Initial Weights in Gradual Argumentation 

**Title (ZH)**: 逐步论证中理性初始权重的诱导 

**Authors**: Nir Oren, Bruno Yun  

**Link**: [PDF](https://arxiv.org/pdf/2502.07452)  

**Abstract**: Many semantics for weighted argumentation frameworks assume that each argument is associated with an initial weight. However, eliciting these initial weights poses challenges: (1) accurately providing a specific numerical value is often difficult, and (2) individuals frequently confuse initial weights with acceptability degrees in the presence of other arguments. To address these issues, we propose an elicitation pipeline that allows one to specify acceptability degree intervals for each argument. By employing gradual semantics, we can refine these intervals when they are rational, restore rationality when they are not, and ultimately identify possible initial weights for each argument. 

**Abstract (ZH)**: 许多加权论辩框架的语义假定每个论点都与一个初始权重相关联。然而，获取这些初始权重存在挑战：（1）准确提供一个具体的数值往往很困难，（2）在其他论点存在的情况下，个体经常会混淆初始权重与接受度等级。为解决这些问题，我们提出了一种提取管道，允许用户为每个论点指定接受度等级区间。通过采用渐进语义，我们可以在合理的区间内进行细化，纠正不合理的区间，并最终确定每个论点的可能初始权重。 

---
# Approximating Human Strategic Reasoning with LLM-Enhanced Recursive Reasoners Leveraging Multi-agent Hypergames 

**Title (ZH)**: 使用增强递归推理器和多agent超博弈相结合来近似人类战略推理 

**Authors**: Vince Trencsenyi, Agnieszka Mensfelt, Kostas Stathis  

**Link**: [PDF](https://arxiv.org/pdf/2502.07443)  

**Abstract**: LLM-driven multi-agent-based simulations have been gaining traction with applications in game-theoretic and social simulations. While most implementations seek to exploit or evaluate LLM-agentic reasoning, they often do so with a weak notion of agency and simplified architectures. We implement a role-based multi-agent strategic interaction framework tailored to sophisticated recursive reasoners, providing the means for systematic in-depth development and evaluation of strategic reasoning. Our game environment is governed by the umpire responsible for facilitating games, from matchmaking through move validation to environment management. Players incorporate state-of-the-art LLMs in their decision mechanism, relying on a formal hypergame-based model of hierarchical beliefs. We use one-shot, 2-player beauty contests to evaluate the recursive reasoning capabilities of the latest LLMs, providing a comparison to an established baseline model from economics and data from human experiments. Furthermore, we introduce the foundations of an alternative semantic measure of reasoning to the k-level theory. Our experiments show that artificial reasoners can outperform the baseline model in terms of both approximating human behaviour and reaching the optimal solution. 

**Abstract (ZH)**: 基于LLM的多代理系统模拟在博弈理论和社会模拟领域的应用正逐渐受到关注。尽管大多数实现试图利用或评估LLM代理推理能力，但它们通常基于一种弱代理概念和简化架构。我们实现了一个基于角色的多代理战略互动框架，旨在适应复杂的递归推理者，提供系统深入开发和评估战略推理的手段。我们的游戏环境由裁判员管理，负责从匹配玩家到验证移动和环境管理的整个游戏流程。玩家在其决策机制中采用最先进的LLM，并依赖于基于形式化超博弈层次信仰模型。我们使用一次性两人的美丽竞赛来评估最新LLM的递归推理能力，提供了与经济学中的传统基准模型及人类实验数据的对比。此外，我们还引入了一种替代性语义推理度量的基础，该度量扩展了k级理论。实验结果表明，人工推理者在接近人类行为和达到最优解方面均能超越基准模型。 

---
# Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation 

**Title (ZH)**: 通过计算内在动机 toward 一个关于能力需求的正式理论 

**Authors**: Erik M. Lintunen, Nadia M. Ady, Sebastian Deterding, Christian Guckelsberger  

**Link**: [PDF](https://arxiv.org/pdf/2502.07423)  

**Abstract**: Computational models offer powerful tools for formalising psychological theories, making them both testable and applicable in digital contexts. However, they remain little used in the study of motivation within psychology. We focus on the "need for competence", postulated as a key basic human need within Self-Determination Theory (SDT) -- arguably the most influential psychological framework for studying intrinsic motivation (IM). The need for competence is treated as a single construct across SDT texts. Yet, recent research has identified multiple, ambiguously defined facets of competence in SDT. We propose that these inconsistencies may be alleviated by drawing on computational models from the field of artificial intelligence, specifically from the domain of reinforcement learning (RL). By aligning the aforementioned facets of competence -- effectance, skill use, task performance, and capacity growth -- with existing RL formalisms, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly. The formalisms reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory. While our research lays a promising foundation, empirical studies of these models in both humans and machines are needed, inviting collaboration across disciplines. 

**Abstract (ZH)**: 计算模型为形式化心理学理论提供了强大的工具，使其既可测试又适用于数字环境。然而，在心理学动机研究中，它们的应用仍然相当有限。本文专注于自我决定理论（SDT）中提出的“能力需要”，这是研究内在动机（IM）最具影响力的心理学框架之一的核心基本需求。在SDT的文本中，能力需要被视为单一的构建模块。然而，近期研究已识别出SDT中存在的多个含义模糊的能力维度。我们建议通过借鉴人工智能领域的计算模型，特别是在强化学习（RL）领域中的模型，可能有助于解决这些不一致之处。通过将这些能力维度——效能、技能运用、任务绩效和能力成长——与现有的RL形式化方法相连接，我们可以为促进SDT及相关动机心理学中的能力理论的发展奠定基础。形式化方法揭示了SDT未能明确说明的潜在前提条件，展示了计算模型如何增进我们对内在动机的理解。此外，我们的研究可以支持理论发展的循环过程，通过激发新的计算模型来形式化理论的某些方面，这些模型可以随后通过实证研究进行检验，以完善理论。尽管我们的研究为这一领域奠定了有前景的基础，但仍需在人类和机器中进行进一步的实证研究，从而促进跨学科合作。 

---
# LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters! 

**Title (ZH)**: 大型语言模型可以从演示中轻松学会推理！结构，而不是内容，才是关键！ 

**Authors**: Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica  

**Link**: [PDF](https://arxiv.org/pdf/2502.07374)  

**Abstract**: Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at this https URL. 

**Abstract (ZH)**: 大型推理模型（LRMs）通过遵循长推理链（Long CoT）来解决复杂的问题，这种长推理链结合了反思、回溯和自我验证。然而，如何通过训练技术及数据需求来激发长推理链的过程仍然不太清楚。在本研究中，我们发现，一个大型语言模型（LLM）可以通过高效的数据监督微调（SFT）和参数高效的小秩适应（LoRA）有效地学习长推理链推理。仅使用17000个长推理链训练样本，Qwen2.5-32B-Instruct模型在一系列数学和编程基准测试中取得了显著的改进，包括在AIME 2024上的得分提升至56.7% (+40.0%)，在LiveCodeBench上的得分提升至57.0% (+8.1%)，这与专有模型o1-preview的得分为44.6%和59.1%相当。更重要的是，我们发现长推理链的结构对学习过程至关重要，而单个推理步骤的内容则几乎没有影响。影响内容的扰动，如使用错误样本进行训练或删除推理关键词，对性能的影响甚微。相反，那些破坏长推理链逻辑一致性的结构修改，如打乱或删除推理步骤，显著降低了准确性。例如，即使在错误答案的长推理链样本上训练的模型，其准确性也仅比完全正确的样本降低了3.2%。这些发现加深了我们对激发LLMs推理能力的理解，并突显了高效训练下一代推理模型时的关键考虑因素。这是我们之前发布的Sky-T1-32B-Preview模型的学术论文。代码可从以下链接获取：this https URL。 

---
# KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems 

**Title (ZH)**: KABB：面向知识的贝叶斯拉普拉斯机组建模在多agent系统中动态专家协调 

**Authors**: Jusheng Zhang, Zimeng Huang, Yijia Fan, Ningyuan Liu, Mingyan Li, Zhuojie Yang, Jiawei Yao, Jian Wang, Keze Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07350)  

**Abstract**: As scaling large language models faces prohibitive costs, multi-agent systems emerge as a promising alternative, though challenged by static knowledge assumptions and coordination inefficiencies. We introduces Knowledge-Aware Bayesian Bandits (KABB), a novel framework that enhances multi-agent system coordination through semantic understanding and dynamic adaptation. The framework features three key innovations: a three-dimensional knowledge distance model for deep semantic understanding, a dual-adaptation mechanism for continuous expert optimization, and a knowledge-aware Thompson Sampling strategy for efficient expert selection. Extensive evaluation demonstrates KABB achieves an optimal cost-performance balance, maintaining high performance while keeping computational demands relatively low in multi-agent coordination. 

**Abstract (ZH)**: 随着大规模语言模型的扩展面临高昂的成本，多智能体系统成为一种有前景的替代方案，但同时也面临着静态知识假设和协调效率低下的挑战。我们提出了知识感知贝叶斯双臂bandit算法（KABB），这是一种通过语义理解和动态适应来增强多智能体系统协调的新框架。该框架包含三个关键创新点：三维度知识距离模型，用于实现深层语义理解；双重适应机制，用于持续的专家优化；以及知识感知的托马斯采样策略，用于高效的专家选择。广泛的评估结果表明，KABB实现了最优的成本效益平衡，在多智能体协调中保持了高性能，同时将计算需求保持在相对较低的水平。 

---
# Coarse Set Theory: A Mathematical Foundation for Coarse Ethics 

**Title (ZH)**: 粗集理论：粗伦理学的数学基础 

**Authors**: Takashi Izumo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07347)  

**Abstract**: In ethical decision-making, individuals are often evaluated based on generalized assessments rather than precise individual performance. This concept, known as Coarse Ethics (CE), has primarily been discussed in natural language without a formal mathematical foundation. This paper introduces Coarse Set Theory (CST) to establish a mathematical framework for CE. We define coarse sets using totally ordered sets and propose axioms that characterize the hierarchical relationships between elements and their groupings. Additionally, we introduce coarse-grained sets, which partition an underlying set into equivalence classes based on predefined criteria. We extend this framework by defining coarse mappings, which transform detailed individual data into coarser representations while maintaining essential structural properties. To measure the information loss, we employ Kullback-Leibler (KL) divergence, demonstrating how different coarse partitions affect the preservation of information. We illustrate how CST can be applied to real-world grading systems through theoretical formulations and empirical analysis. This study provides a rigorous foundation for CE, enabling a more systematic exploration of fairness, interpretability, and decision-making trade-offs. 

**Abstract (ZH)**: 在伦理决策中，个体通常根据一般的评估标准而非精确的个人表现来被评价。这一概念被称为粗粒度伦理（Coarse Ethics，CE），目前主要在自然语言中讨论，缺乏正式的数学基础。本文引入了粗粒度集理论（Coarse Set Theory，CST），以建立一种CE的数学框架。我们使用完全有序集来定义粗粒度集，并提出了刻画元素及其分组之间层次关系的公理。此外，我们引入了粗粒度集，它根据预定义的标准将底层集合划分为等价类。通过定义粗粒度映射，我们将这一框架扩展到将详细个体数据转换为较粗粒度的表示，同时保持基本的结构属性。为了衡量信息损失，我们采用了Kullback-Leibler（KL）散度，说明不同粗粒度划分如何影响信息的保持程度。我们通过理论推导和实证分析，展示了CST在实际评分系统中的应用。本研究为CE提供了严格的数学基础，促进了对公平性、可解释性和决策权衡的系统探讨。 

---
# When More is Less: Understanding Chain-of-Thought Length in LLMs 

**Title (ZH)**: 当更多变成更少：理解大语言模型中的链式思考长度 

**Authors**: Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07266)  

**Abstract**: Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy? In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that longer reasoning processes are increasingly susceptible to noise. We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs. 

**Abstract (ZH)**: 链式推理（CoT）方法通过将复杂任务分解为更小、更易管理的子任务，增强了大型语言模型（LLMs）的多步骤推理能力。研究者们一直在探索引导模型生成更复杂的CoT过程以提高LLMs推理能力的方法，例如长CoT及测试时缩放定律。然而，对于大多数模型和任务而言，CoT长度的增加是否始终能带来推理准确性的提升？在本文中，我们观察到一种复杂的关系：随着推理步骤数量的增加，性能起初会提升，但最终会下降。为了理解这一现象，我们提供了一种证据，表明较长的推理过程越来越容易受到噪声的影响。我们从理论上证明了存在一个最优的CoT长度，并基于模型能力和任务难度推导出了这一最优长度的缩放定律。受我们的理论启发，我们在合成和真实数据集上进行了实验，并提出了基于长度过滤的投票（Length-filtered Vote）方法以缓解过长或过短CoT的效果。我们的发现突显了校准CoT长度以与模型能力和任务需求相匹配的重要性，为优化LLMs中的多步骤推理提供了一个原理性的框架。 

---
# Monte Carlo Tree Diffusion for System 2 Planning 

**Title (ZH)**: 系统二规划中的蒙特卡洛树 diffusion 方法 

**Authors**: Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn  

**Link**: [PDF](https://arxiv.org/pdf/2502.07202)  

**Abstract**: Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases. 

**Abstract (ZH)**: 扩散模型最近已成为规划的强大工具。然而，与蒙特卡洛树搜索(Monte Carlo Tree Search, MCTS)不同，MCTS 的性能随着测试时间计算（Test-Time Computation, TTC）的增加而自然提升，而标准的基于扩散的方法在 TTC 扩展方面提供的途径有限。在本文中，我们引入了蒙特卡洛树扩散（Monte Carlo Tree Diffusion, MCTD），这是一种将扩散模型的生成能力与 MCTS 的自适应搜索能力相结合的新型框架。我们的方法重新构想了去噪过程，使其成为一种树状结构的过程，使得部分去噪的计划可以迭代地评估、修剪和改进。通过有选择地扩展有希望的轨迹，同时保留重新访问和改进非最优分支的灵活性，MCTD 能够在扩散框架内实现 MCTS 的优势，如控制探索与利用的权衡。实验证实在具有挑战性的长期任务上，MCTD 在 TTC 增加时能够产生更高质量的解决方案，且表现优于基于扩散的标准基线。 

---
# Bag of Tricks for Inference-time Computation of LLM Reasoning 

**Title (ZH)**: 以下是从推理时计算大语言模型推理的技巧集 

**Authors**: Fan Liu, Wenshuo Chao, Naiqiang Tan, Hao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07191)  

**Abstract**: With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at this https URL 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的不断发展，解决复杂推理任务的关注度越来越高。推理时的计算方法（如Best-of-N、束搜索等）特别有价值，因为它们可以在不修改模型参数或需要额外训练的情况下提升推理性能。然而，这些技术伴随着实施挑战，大多数现有方法仍处于概念验证阶段，因为它们的计算复杂性和在不同任务上的有效性存在差异。在本文中，我们研究并对比了多种不同复杂度的推理任务中推理时的计算策略。由于大多数当前方法依赖于提出者-验证者管道，该管道首先生成候选解决方案（例如推理解决方案），然后基于奖励信号（例如RLHF奖励、过程奖励）选择最佳方案，我们的研究集中在优化候选解决方案的生成（例如指令提示、温度和top-p等超参数）和奖励机制（例如自我评估、奖励类型）。通过在各种大小的不同模型（例如Llama、Qwen和Mistral家族）上进行超过20,000个A100-80G GPU小时的实验（超过1,000个实验），我们的消融研究揭示了一些之前被忽略的策略可以显著提升性能（例如，调整温度可以将推理任务性能提高5%）。此外，我们通过系统地评估六种代表性方法在八个推理任务上的表现，建立了推理时计算的标准基准。这些发现为未来的研究奠定了更坚实的基础。源代码可在以下链接获取：[提供建议链接] 

---
# Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the ARC Task 

**Title (ZH)**: 理解大语言模型在流体智力方面的不足：对ARC任务的分析 

**Authors**: Junjie Wu, Mo Yu, Lemao Liu, Dit-Yan Yeung, Jie Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2502.07190)  

**Abstract**: While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs' parameters, rather than solving new problems without prior knowledge. In cognitive research, the latter ability is referred to as fluid intelligence, which is considered to be critical for assessing human intelligence. Recent research on fluid intelligence assessments has highlighted significant deficiencies in LLMs' abilities. In this paper, we analyze the challenges LLMs face in demonstrating fluid intelligence through controlled experiments, using the most representative ARC task as an example. Our study revealed three major limitations in existing LLMs: limited ability for skill composition, unfamiliarity with abstract input formats, and the intrinsic deficiency of left-to-right decoding. Our data and code can be found in this https URL. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在各种自然语言处理（NLP）任务上表现出强大的性能，值得注意的是，这些任务大多依赖于利用LLMs参数中编码的庞大知识量，而不是解决没有先验知识的新问题。在认知研究中，后一种能力被称为流体智力，被认为是对人类智力进行评估的关键。近期关于流体智力评估的研究揭示了LLMs在该方面存在的显著不足。在本文中，我们通过控制实验分析了LLMs在展示流体智力方面的挑战，以最具代表性的ARC任务为例。我们的研究揭示了现有LLMs存在的三大限制：技能组合能力有限、不熟悉抽象输入格式以及从左到右解码的固有缺陷。相关数据和代码可以在以下链接中找到：[此处链接]。 

---
# Interactive Data Harmonization with LLM Agents 

**Title (ZH)**: 带有LLM代理的交互式数据 harmonization 

**Authors**: Aécio Santos, Eduardo H. M. Pena, Roque Lopez, Juliana Freire  

**Link**: [PDF](https://arxiv.org/pdf/2502.07132)  

**Abstract**: Data harmonization is an essential task that entails integrating datasets from diverse sources. Despite years of research in this area, it remains a time-consuming and challenging task due to schema mismatches, varying terminologies, and differences in data collection methodologies. This paper presents the case for agentic data harmonization as a means to both empower experts to harmonize their data and to streamline the process. We introduce Harmonia, a system that combines LLM-based reasoning, an interactive user interface, and a library of data harmonization primitives to automate the synthesis of data harmonization pipelines. We demonstrate Harmonia in a clinical data harmonization scenario, where it helps to interactively create reusable pipelines that map datasets to a standard format. Finally, we discuss challenges and open problems, and suggest research directions for advancing our vision. 

**Abstract (ZH)**: 数据整合是将来源于不同数据源的数据库进行集成的一项重要任务。尽管在这一领域已有多年的研究，但由于模式不匹配、术语差异和数据采集方法的不同，这一任务仍耗时且具有挑战性。本文提出了代理数据整合的概念，旨在通过为专家赋能并简化这一过程来推动数据整合。我们引入了Harmonia系统，该系统结合了基于大语言模型（LLM）的推理、交互式的用户界面以及数据整合的基本组件库，自动地生成数据整合管道。我们通过在临床数据整合场景中的应用展示了Harmonia的功能，其中它帮助用户交互式地创建可重复使用的管道，将数据集映射到标准格式。最后，我们讨论了面临的挑战和开放问题，并提出了推进这一愿景的研究方向。 

---
# Autonomous Deep Agent 

**Title (ZH)**: 自主深度代理 

**Authors**: Amy Yu, Erik Lebedev, Lincoln Everett, Xiaoxin Chen, Terry Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07056)  

**Abstract**: This technical brief introduces Deep Agent, an advanced autonomous AI system designed to manage complex multi-phase tasks through a novel hierarchical task management architecture. The system's foundation is built on our Hierarchical Task DAG (HTDAG) framework, which dynamically decomposes high-level objectives into manageable sub-tasks while rigorously maintaining dependencies and execution coherence. Deep Agent advances beyond traditional agent systems through three key innovations: First, it implements a recursive two-stage planner-executor architecture that enables continuous task refinement and adaptation as circumstances change. Second, it features an Autonomous API & Tool Creation (AATC) system that automatically generates reusable components from UI interactions, substantially reducing operational costs for similar tasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt Feedback Learning components that optimize Large Language Model prompts for specific scenarios, enhancing both inference accuracy and operational stability. These components are integrated to form a service infrastructure that manages user contexts, handles complex task dependencies, and orchestrates end-to-end agentic workflow execution. Through this sophisticated architecture, Deep Agent establishes a novel paradigm in self-governing AI systems, demonstrating robust capability to independently handle intricate, multi-step tasks while maintaining consistent efficiency and reliability through continuous self-optimization. 

**Abstract (ZH)**: 本文简介了Deep Agent，这是一种先进的自主人工智能系统，旨在通过一种新颖的分层任务管理架构来管理复杂的多阶段任务。该系统的基石是基于我们开发的层次化任务有向无环图（HTDAG）框架，该框架能够动态地将高层次目标分解成可管理的子任务，同时严格维护依赖性和执行一致性。Deep Agent 通过三种关键技术创新超越了传统的代理系统：首先，它采用递归两阶段规划-执行架构，能够在环境变化时实现持续的任务优化和适应。其次，它拥有自主API及工具创建（AATC）系统，能够从用户界面交互中自动生成可重复使用的组件，大幅降低类似任务的操作成本。第三，它集成了提示调整引擎和自主提示反馈学习组件，能够针对特定场景优化大型语言模型的提示，从而提高推断准确性和操作稳定性。这些组件共同构成了一个服务平台，能够处理用户上下文、处理复杂任务依赖关系，并协调全流程的自动化工作流程执行。通过这一先进的架构，Deep Agent 成立了自我治理人工智能系统的一个新范式，展示了其强大的独立处理复杂多步骤任务的能力，并通过持续的自优化保持了一致的高效性和可靠性。 

---
# Position: Episodic Memory is the Missing Piece for Long-Term LLM Agents 

**Title (ZH)**: 位置： episodic 记忆是长期大语言模型代理所需的缺失环节 

**Authors**: Mathis Pink, Qinyuan Wu, Vy Ai Vo, Javier Turek, Jianing Mu, Alexander Huth, Mariya Toneva  

**Link**: [PDF](https://arxiv.org/pdf/2502.06975)  

**Abstract**: As Large Language Models (LLMs) evolve from text-completion tools into fully fledged agents operating in dynamic environments, they must address the challenge of continually learning and retaining long-term knowledge. Many biological systems solve these challenges with episodic memory, which supports single-shot learning of instance-specific contexts. Inspired by this, we present an episodic memory framework for LLM agents, centered around five key properties of episodic memory that underlie adaptive and context-sensitive behavior. With various research efforts already partially covering these properties, this position paper argues that now is the right time for an explicit, integrated focus on episodic memory to catalyze the development of long-term agents. To this end, we outline a roadmap that unites several research directions under the goal to support all five properties of episodic memory for more efficient long-term LLM agents. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）从文本补全工具转变为在动态环境中运作的完整代理，它们必须应对持续学习和保留长期知识的挑战。许多生物系统通过使用事例记忆来解决这些问题，这种记忆支持特定实例背景的一次性学习。受此启发，我们提出了一种为LLM代理构建的事例记忆框架，该框架围绕着事例记忆五个核心属性展开，这些属性是实现适应性和上下文敏感行为的基础。尽管已经有多项研究分别涵盖了这些属性的一部分，本文认为现在是明确关注并整合事例记忆以促进长期代理发展的恰当时机。为此，我们提出了一个路线图，旨在通过支持事例记忆的五个属性来推动更高效的长期逻辑记忆模型（LLMs）的发展。 

---
# Breaking Down Bias: On The Limits of Generalizable Pruning Strategies 

**Title (ZH)**: 打破偏见：关于可泛化剪枝策略的限制 

**Authors**: Sibo Ma, Alejandro Salinas, Peter Henderson, Julian Nyarko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07771)  

**Abstract**: We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case. 

**Abstract (ZH)**: 我们采用模型剪枝的方法来考察大规模语言模型（LLM）对种族偏见的理解方式，并探讨是否存在一种可泛化的偏见缓解策略。我们的分析揭示了几项新颖的见解。我们发现，剪枝可以作为一种有效的方法来减少偏见，同时不会显著增加模型异常行为。基于神经元的剪枝策略通常比剪枝整个注意力头的方法效果更好。然而，我们的结果也表明，这两种方法的有效性随着剪枝策略的泛化程度增加而迅速下降。例如，一个旨在消除金融决策中种族偏见的模型，其对商业交易中的偏见的泛化能力较差。总体而言，我们的分析表明，语言模型中种族偏见的泛化表示仅占一部分。另一部分偏见高度依赖于具体语境，这表明可泛化的偏见缓解策略的有效性可能有限。我们的发现对围绕人工智能的法律框架具有重要意义。特别是，这表明有效的缓解策略应包括在特定应用场景中部署模型的相关方承担法律责任。 

---
# Polynomial-Time Approximability of Constrained Reinforcement Learning 

**Title (ZH)**: 约束强化学习的多项式可近似性 

**Authors**: Jeremy McMahan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07764)  

**Abstract**: We study the computational complexity of approximating general constrained Markov decision processes. Our primary contribution is the design of a polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for finding optimal constrained policies across a broad class of recursively computable constraints, including almost-sure, chance, expectation, and their anytime variants. Matching lower bounds imply our approximation guarantees are optimal so long as $P \neq NP$. The generality of our approach results in answers to several long-standing open complexity questions in the constrained reinforcement learning literature. Specifically, we are the first to prove polynomial-time approximability for the following settings: policies under chance constraints, deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes. 

**Abstract (ZH)**: 我们研究了近似制约马尔可夫决策过程的计算复杂性。我们的主要贡献是设计了一个多项式时间的$(0, \epsilon)$-加法双准则近似算法，用于在广泛的可递归计算的制约条件下找到最优的制约策略，包括几乎必然性、概率、期望及其任意时间变体。对应的下界表明，在$P \neq NP$的情况下，我们的近似保证是最优的。我们方法的普适性为约束强化学习文献中的一些长期未解的复杂性问题提供了答案。具体来说，我们首次证明了以下设置的多项式时间近似可解性：在概率约束下的策略、在多个期望约束下的确定性策略、在不同类型的非均匀约束下的策略，以及连续状态过程下的约束策略。 

---
# An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa and Dynamic Contextual Positional Gating 

**Title (ZH)**: 基于DeBERTa和动态上下文位置门控的高级自然语言处理框架： automatized 医学诊断应用 

**Authors**: Mohammad Ali Labbaf Khaniki, Sahabeh Saadati, Mohammad Manthouri  

**Link**: [PDF](https://arxiv.org/pdf/2502.07755)  

**Abstract**: This paper presents a novel Natural Language Processing (NLP) framework for enhancing medical diagnosis through the integration of advanced techniques in data augmentation, feature extraction, and classification. The proposed approach employs back-translation to generate diverse paraphrased datasets, improving robustness and mitigating overfitting in classification tasks. Leveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with Dynamic Contextual Positional Gating (DCPG), the model captures fine-grained contextual and positional relationships, dynamically adjusting the influence of positional information based on semantic context to produce high-quality text embeddings. For classification, an Attention-Based Feedforward Neural Network (ABFNN) is utilized, effectively focusing on the most relevant features to improve decision-making accuracy. Applied to the classification of symptoms, clinical notes, and other medical texts, this architecture demonstrates its ability to address the complexities of medical data. The combination of data augmentation, contextual embedding generation, and advanced classification mechanisms offers a robust and accurate diagnostic tool, with potential applications in automated medical diagnosis and clinical decision support. This method demonstrates the effectiveness of the proposed NLP framework for medical diagnosis, achieving remarkable results with an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only underscore the model's robust performance in classifying medical texts with exceptional precision and reliability but also highlight its superiority over existing methods, making it a highly promising tool for automated diagnostic systems. 

**Abstract (ZH)**: 本文提出了一种新颖的自然语言处理（NLP）框架，该框架通过集成数据增强、特征提取和分类等高级技术，增强了医学诊断的准确性。该提出的方案利用回译生成多样化的同义数据集，提高分类任务的稳健性并减轻过拟合现象。该模型利用解码增强的BERT与离散注意力（DeBERTa）结合动态上下文位置门控（DCPG），捕捉细微的上下文和位置关系，根据语义上下文动态调整位置信息的影响，生成高质量的文本嵌入。对于分类任务，使用基于注意力的前馈神经网络（ABFNN），有效关注最具相关性的特征，从而提高决策准确性。该架构应用于症状分类、临床记录和其他医学文本，展示了其处理医学数据复杂性的能力。结合数据增强、上下文嵌入生成和高级分类机制，提供了一个稳健且准确的诊断工具，具有自动化医学诊断和临床决策支持的应用潜力。该方法展示了所提出NLP框架在医学诊断中的有效性，实现了高达99.78%的准确率、99.72%的召回率、99.79%的精确率和99.75%的F1分数。这些指标不仅强调了该模型在分类医学文本方面表现出色的鲁棒性和可靠性，还突显了其在现有方法中的优越性，使其成为自动化诊断系统的高度有希望的工具。 

---
# Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension 

**Title (ZH)**: 面向大规模语言模型（LLM）高效优化器设计的结构化费舍尔近似扩展研究 

**Authors**: Wenbo Gong, Meyer Scetbon, Chao Ma, Edward Meeds  

**Link**: [PDF](https://arxiv.org/pdf/2502.07752)  

**Abstract**: Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory. 

**Abstract (ZH)**: 设计具有低内存需求和快速收敛性的大型语言模型（LLMs）高效优化器是一个重要且具有挑战性的问题。本文通过结构化的 Fisher 信息矩阵（FIM）近似来系统设计此类优化器，迈出了重要一步。我们展示了许多现有的高效优化器可以被视为在特定结构假设下解决 FIM 近似（以弗罗贝尼乌斯范数为度量）的问题。基于这些见解，我们提出两种实用的高效优化器设计建议，涉及精心选择结构假设以平衡通用性和效率，并通过一种新颖的低秩扩展框架提高具有通用结构的优化器的内存效率。我们通过推导新的内存高效优化器来演示每种设计方法：行和列缩放的 SGD（RACS）和自适应低维子空间估计（Alice）。LLaMA 预训练实验（多达 10 亿参数）验证了这些优化器的有效性，显示出比现有内存高效的基线方法和 Adam 更快、更好的收敛速度，并且内存开销较小。值得注意的是，Alice 在参数量为 10 亿的情况下比 Adam 快出超过两倍，而 RACS 在 10 亿参数模型上以类似 SGD 的内存使用量表现出强劲性能。 

---
# PFedDST: Personalized Federated Learning with Decentralized Selection Training 

**Title (ZH)**: PFedDST：基于去中心化选择训练的个性化 federated 学习

注：在翻译学术论文的标题时，通常需要保持原文的专业术语和风格。"PFedDST" 看起来像是一个特定的缩写或技术名词，保持不变更为妥当。因此，最终的翻译可以写作：

PFedDST：基于去中心化选择训练的个性化联邦学习 

**Authors**: Mengchen Fan, Keren Li, Tianyun Zhang, Qing Tian, Baocheng Geng  

**Link**: [PDF](https://arxiv.org/pdf/2502.07750)  

**Abstract**: Distributed Learning (DL) enables the training of machine learning models across multiple devices, yet it faces challenges like non-IID data distributions and device capability disparities, which can impede training efficiency. Communication bottlenecks further complicate traditional Federated Learning (FL) setups. To mitigate these issues, we introduce the Personalized Federated Learning with Decentralized Selection Training (PFedDST) framework. PFedDST enhances model training by allowing devices to strategically evaluate and select peers based on a comprehensive communication score. This score integrates loss, task similarity, and selection frequency, ensuring optimal peer connections. This selection strategy is tailored to increase local personalization and promote beneficial peer collaborations to strengthen the stability and efficiency of the training process. Our experiments demonstrate that PFedDST not only enhances model accuracy but also accelerates convergence. This approach outperforms state-of-the-art methods in handling data heterogeneity, delivering both faster and more effective training in diverse and decentralized systems. 

**Abstract (ZH)**: 分布式学习（DL）使机器学习模型能够在多个设备上进行训练，然而这也面临着诸如非IID数据分布和设备能力差异等挑战，这些挑战可能会影响训练效率。传统的联邦学习（FL）设置中的通信瓶颈进一步增加了复杂性。为了解决这些问题，我们引入了个人化联邦学习与去中心化选择训练（PFedDST）框架。PFedDST通过允许设备根据全面的通信评分战略性地评估和选择伙伴来提升模型训练效率。该评分综合了损失、任务相似性和选择频率等因素，确保了高效的伙伴连接。该选择策略旨在增加本地个性化并促进有益的伙伴合作，从而增强训练过程的稳定性和效率。我们的实验表明，PFedDST不仅能提升模型的准确性，还能加速收敛。该方法在处理数据异质性方面超越了当前最先进的方法，在分散且多样化的系统中实现了更快、更有效的训练。 

---
# WHODUNIT: Evaluation benchmark for culprit detection in mystery stories 

**Title (ZH)**: 《WHODUNIT：悬疑故事中罪犯检测评估基准》

这个标题翻译成中文既保留了原文的含义，又符合学术规范。"WHODUNIT" 被视为一款解决悬疑故事中罪犯检测问题的游戏或工具的名称，因此在翻译中保持了原名不变，而将其余部分翻译成中文。 

**Authors**: Kshitij Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2502.07747)  

**Abstract**: We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to identify the perpetrator after reading and comprehending the story. To evaluate model robustness, we apply a range of character-level name augmentations, including original names, name swaps, and substitutions with well-known real and/or fictional entities from popular discourse. We further use various prompting styles to investigate the influence of prompting on deductive reasoning accuracy.
We conduct evaluation study with state-of-the-art models, specifically GPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with majority response selection to ensure reliability. The results demonstrate that while LLMs perform reliably on unaltered texts, accuracy diminishes with certain name substitutions, particularly those with wide recognition. This dataset is publicly available here. 

**Abstract (ZH)**: 我们提出了一种新型数据集，WhoDunIt，用于评估大型语言模型（LLM）在叙述性上下文中的演绎推理能力。该数据集由开放领域内的推理小说和短篇故事构建而成，挑战LLM在阅读和理解故事后识别作案人的能力。为了评估模型的鲁棒性，我们应用了一系列基于字符级别的姓名替换方法，包括使用原始姓名、姓名替换以及使用具有广泛认知度的真实或虚构人物的替换。此外，我们还使用了各种不同的提示方式，以探究提示对演绎推理准确性的影响。

我们使用了一组领先模型进行评估，具体包括GPT-4o、GPT-4-turbo和GPT-4o-mini，并通过多次实验和多数答案选择来确保评估结果的可靠性。实验结果表明，尽管在未修改的文本上，LLM表现出良好的性能，但在某些姓名替换后，准确率显著下降，尤其是在那些具有广泛认知度的替换上。此数据集已公开展示，可以在此处获取。 

---
# Next Block Prediction: Video Generation via Semi-Auto-Regressive Modeling 

**Title (ZH)**: 下一块预测：基于半自回归建模的视频生成 

**Authors**: Shuhuai Ren, Shuming Ma, Xu Sun, Furu Wei  

**Link**: [PDF](https://arxiv.org/pdf/2502.07737)  

**Abstract**: Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR) video generation, but it suffers from suboptimal unidirectional dependencies and slow inference speed. In this work, we propose a semi-autoregressive (semi-AR) framework, called Next-Block Prediction (NBP), for video generation. By uniformly decomposing video content into equal-sized blocks (e.g., rows or frames), we shift the generation unit from individual tokens to blocks, allowing each token in the current block to simultaneously predict the corresponding token in the next block. Unlike traditional AR modeling, our framework employs bidirectional attention within each block, enabling tokens to capture more robust spatial dependencies. By predicting multiple tokens in parallel, NBP models significantly reduce the number of generation steps, leading to faster and more efficient inference. Our model achieves FVD scores of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an average of 4.4. Furthermore, thanks to the reduced number of inference steps, the NBP model generates 8.89 frames (128x128 resolution) per second, achieving an 11x speedup. We also explored model scales ranging from 700M to 3B parameters, observing significant improvements in generation quality, with FVD scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600, demonstrating the scalability of our approach. 

**Abstract (ZH)**: 下一个标记预测（NTP）是自回归（AR）视频生成的实际上用方法，但它存在子优化的单向依赖关系和缓慢的推理速度。在本文中，我们提出了一种半自回归（semi-AR）框架，称为下一个块预测（NBP），用于视频生成。通过均匀地将视频内容分解成等大小的块（例如，行或帧），我们将生成单元从单个标记转变为块，使得当前块中的每个标记可以同时预测对应于下一个块中的标记。与传统的AR建模不同，我们的框架在每个块内使用双向注意机制，从而使标记能够捕获更稳健的空间依赖关系。通过并行预测多个标记，NBP模型显著减少了生成步骤的数量，从而提高了推理速度和效率。我们的模型在UCF101数据集上实现了103.3的FVD分数，在K600数据集上实现了25.5的FVD分数，平均优于原始的NTP模型4.4。此外，由于减少了推理步骤的数量，NBP模型在每秒每秒生成8.89帧（分辨率为128x128）的图像，比传统的NTP模型快了11倍。我们还探索了从700M到3B参数量的模型规模，观察到生成质量有了显著提高，在UCF101数据集上，FVD得分从103.3降至55.3，在K600数据集上，FVD得分从25.5降至19.5，这展示了我们方法的可扩展性。 

---
# EdgeEar: Efficient and Accurate Ear Recognition for Edge Devices 

**Title (ZH)**: EdgeEar：适用于边缘设备的高效准确耳部识别 

**Authors**: Camile Lendering, Bernardo Perrone Ribeiro, Žiga Emeršič, Peter Peer  

**Link**: [PDF](https://arxiv.org/pdf/2502.07734)  

**Abstract**: Ear recognition is a contactless and unobtrusive biometric technique with applications across various domains. However, deploying high-performing ear recognition models on resource-constrained devices is challenging, limiting their applicability and widespread adoption. This paper introduces EdgeEar, a lightweight model based on a proposed hybrid CNN-transformer architecture to solve this problem. By incorporating low-rank approximations into specific linear layers, EdgeEar reduces its parameter count by a factor of 50 compared to the current state-of-the-art, bringing it below two million while maintaining competitive accuracy. Evaluation on the Unconstrained Ear Recognition Challenge (UERC2023) benchmark shows that EdgeEar achieves the lowest EER while significantly reducing computational costs. These findings demonstrate the feasibility of efficient and accurate ear recognition, which we believe will contribute to the wider adoption of ear biometrics. 

**Abstract (ZH)**: 耳纹识别是一种无需接触且不易察觉的生物识别技术，适用于多个领域。然而，在资源受限的设备上部署高性能的耳纹识别模型极具挑战性，这限制了其应用范围和普及程度。本文介绍了EdgeEar，一种基于混合CNN-Transformer架构的轻量级模型，旨在解决这一问题。通过将低秩近似应用于特定的线性层，EdgeEar 的参数量相比当前最先进的模型减少了50倍，保持在200万以下，同时仍能保持较高的准确率。在不受约束的耳纹识别挑战（UERC2023）基准测试上的评估表明，EdgeEar 在保持较低错误接受率（EER）的同时，显著降低了计算成本。这些发现展示了高效且准确的耳纹识别的可行性，我们相信这将有助于耳纹生物识别技术的更广泛采用。 

---
# Economics of Sourcing Human Data 

**Title (ZH)**: 人力数据采购的经济学 

**Authors**: Sebastin Santy, Prasanta Bhattacharya, Manoel Horta Ribeiro, Kelsey Allen, Sewoong Oh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07732)  

**Abstract**: Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content--it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations--rather than relying solely on external incentives--can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation. 

**Abstract (ZH)**: 人工智能的进步依赖于人类生成的数据，从注释市场到更广泛的互联网。然而，大型语言模型的广泛应用现在威胁到了这些平台上人类生成数据的质量和完整性。我们认为，这一问题超出了过滤由AI生成内容的即时挑战——它揭示了数据收集系统设计中存在的更深层次的问题。现有系统通常优先考虑速度、规模和效率，而牺牲了内在的人类动机，导致参与度和数据质量下降。我们建议重新思考数据收集系统的理念，使其与贡献者的内在动机相一致，而不是仅仅依赖外部激励，这有助于在大规模范围内维持高质量的数据来源，同时保持贡献者的信任和长期参与。 

---
# Verifying LLM-Generated Code in the Context of Software Verification with Ada/SPARK 

**Title (ZH)**: 在Ada/SPARK背景下验证大模型生成的代码：软件验证中的应用 

**Authors**: Marcos Cramer, Lucian McIntyre  

**Link**: [PDF](https://arxiv.org/pdf/2502.07728)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable code generation capabilities, but the correctness of the generated code cannot be inherently trusted. This paper explores the feasibility of using formal software verification, specifically the SPARK framework for Ada, to ensure the reliability of LLM-generated code. We present Marmaragan, a tool that leverages an LLM in order to generate SPARK annotations for existing programs, enabling formal verification of the code. The tool is benchmarked on a curated set of SPARK programs, with annotations selectively removed to test specific capabilities. The performance of Marmaragan with GPT-4o on the benchmark is promising, with correct annotations having been generated for 50.7% of the benchmark cases. The results establish a foundation for future work on combining the power of LLMs with the reliability of formal software verification. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成代码方面表现出色，但生成的代码的正确性无法固有地信赖。本文探讨了使用形式软件验证，特别是Ada的SPARK框架，来确保LLM生成代码的可靠性。我们提出了Marmaragan工具，该工具利用LLM为现有程序生成SPARK注释，从而实现代码的形式验证。该工具在一组特选的SPARK程序上进行了基准测试，并有选择地移除了注释以测试特定的能力。Marmaragan与GPT-4o在基准测试中的表现令人鼓舞，50.7%的基准案例中生成了正确的注释。研究结果为未来结合LLM的强大功能与形式软件验证的可靠性奠定了基础。 

---
# TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning 

**Title (ZH)**: TMLC-Net：可迁移元标签纠正方法用于嘈杂标签学习 

**Authors**: Mengyang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07721)  

**Abstract**: The prevalence of noisy labels in real-world datasets poses a significant impediment to the effective deployment of deep learning models. While meta-learning strategies have emerged as a promising approach for addressing this challenge, existing methods often suffer from limited transferability and task-specific designs. This paper introduces TMLC-Net, a novel Transferable Meta-Learner for Correcting Noisy Labels, designed to overcome these limitations. TMLC-Net learns a general-purpose label correction strategy that can be readily applied across diverse datasets and model architectures without requiring extensive retraining or fine-tuning. Our approach integrates three core components: (1) Normalized Noise Perception, which captures and normalizes training dynamics to handle distribution shifts; (2) Time-Series Encoding, which models the temporal evolution of sample statistics using a recurrent neural network; and (3) Subclass Decoding, which predicts a corrected label distribution based on the learned representations. We conduct extensive experiments on benchmark datasets with various noise types and levels, demonstrating that TMLC-Net consistently outperforms state-of-the-art methods in terms of both accuracy and robustness to label noise. Furthermore, we analyze the transferability of TMLC-Net, showcasing its adaptability to new datasets and noise conditions, and establishing its potential as a broadly applicable solution for robust deep learning in noisy environments. 

**Abstract (ZH)**: 真实世界数据集中噪声标签的普遍存在对深度学习模型的有效部署构成了显著障碍。尽管元学习策略已显示出应对这一挑战的前景，但现有方法往往在泛化能力和任务特定设计方面存在局限。本文介绍了TMLC-Net，一种新型的可迁移元学习网络，旨在克服这些局限。TMLC-Net学习一种通用的标签校正策略，该策略无需大量重新训练或微调，即可在多样化的数据集和模型架构上广泛应用。我们的方法整合了三个核心组件：(1)归一化噪声感知，该组件捕捉并归一化训练动态以应对分布变化；(2)时间序列编码，该组件利用循环神经网络建模样本统计量的时间演化；以及(3)子类解码，该组件基于学习到的表示预测校正后的标签分布。我们在各种噪声类型和水平的基准数据集上进行了广泛的实验，结果显示TMLC-Net在准确性和对标签噪声的鲁棒性方面均优于现有方法。此外，我们分析了TMLC-Net的可迁移性，展示了其在新数据集和噪声条件下的适应能力，并确立了其在嘈杂环境中实现稳健深度学习的广泛应用潜力。 

---
# SoK: A Classification for AI-driven Personalized Privacy Assistants 

**Title (ZH)**: SoK: AI驱动的个性化隐私助手分类 

**Authors**: Victor Morel, Leonardo Iwaya, Simone Fischer-Hübner  

**Link**: [PDF](https://arxiv.org/pdf/2502.07693)  

**Abstract**: To help users make privacy-related decisions, personalized privacy assistants based on AI technology have been developed in recent years. These AI-driven Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits for users, who may otherwise struggle to make decisions regarding their personal data in environments saturated with privacy-related decision requests. However, no study systematically inquired about the features of these AI-driven PPAs, their underlying technologies, or the accuracy of their decisions. To fill this gap, we present a Systematization of Knowledge (SoK) to map the existing solutions found in the scientific literature. We screened 1697 unique research papers over the last decade (2013-2023), constructing a classification from 39 included papers. As a result, this SoK reviews several aspects of existing research on AI-driven PPAs in terms of types of publications, contributions, methodological quality, and other quantitative insights. Furthermore, we provide a comprehensive classification for AI-driven PPAs, delving into their architectural choices, system contexts, types of AI used, data sources, types of decisions, and control over decisions, among other facets. Based on our SoK, we further underline the research gaps and challenges and formulate recommendations for the design and development of AI-driven PPAs as well as avenues for future research. 

**Abstract (ZH)**: 为了帮助用户做出与隐私相关的重要决策，近年来基于人工智能技术的个性化隐私助手（AI驱动的个性化隐私助手，简称AI驱动PPAs）被开发出来。这些基于人工智能的个性化隐私助手能够为用户提供显著的好处，否则在充斥着隐私相关决策请求的环境中，用户可能会难以做出关于个人数据的决策。然而，目前尚未有研究系统地探讨这些AI驱动PPAs的特点、底层技术或其决策准确性。为了填补这一空白，我们提出了一种知识体系化（SoK）方法来映射现有科学文献中的解决方案。我们在过去十年（2013-2023年）中筛选了1697篇独特的研究论文，并构建了一个由39篇纳入论文构成的分类体系。因此，本SoK综合审查了现有研究中关于AI驱动PPAs的各种方面，包括出版物类型、贡献、方法学质量以及其他定量洞察。此外，我们还提供了AI驱动PPAs的全面分类，探讨了它们的体系结构选择、系统环境、使用的不同人工智能类型、数据来源、决策类型以及决策控制等内容。基于我们的知识体系化分析，我们进一步指出了研究空白和挑战，并提出了设计和开发AI驱动PPAs的建议，以及未来研究的方向。 

---
# A Unifying Framework for Causal Imitation Learning with Hidden Confounders 

**Title (ZH)**: 带有隐藏共因的因果模仿学习的统一框架 

**Authors**: Daqian Shao, Thomas Kleine Buening, Marta Kwiatkowska  

**Link**: [PDF](https://arxiv.org/pdf/2502.07656)  

**Abstract**: We propose a general and unifying framework for causal Imitation Learning (IL) with hidden confounders that subsumes several existing confounded IL settings from the literature. Our framework accounts for two types of hidden confounders: (a) those observed by the expert, which thus influence the expert's policy, and (b) confounding noise hidden to both the expert and the IL algorithm. For additional flexibility, we also introduce a confounding noise horizon and time-varying expert-observable hidden variables. We show that causal IL in our framework can be reduced to a set of Conditional Moment Restrictions (CMRs) by leveraging trajectory histories as instruments to learn a history-dependent policy. We propose DML-IL, a novel algorithm that uses instrumental variable regression to solve these CMRs and learn a policy. We provide a bound on the imitation gap for DML-IL, which recovers prior results as special cases. Empirical evaluation on a toy environment with continues state-action spaces and multiple Mujoco tasks demonstrate that DML-IL outperforms state-of-the-art causal IL algorithms. 

**Abstract (ZH)**: 我们提出了一种通用且统一的框架，用于处理隐藏混杂因素的因果模仿学习（IL），该框架涵盖了文献中几种现有的混杂IL设置。该框架考虑了两种类型的隐藏混杂因素：(a) 专家可以观察到的混杂因素，因此影响专家的策略；(b) 对于专家和IL算法均不可见的混杂噪声。为增加灵活性，我们还引入了混杂噪声窗口和时间变化的专家可观测隐藏变量。我们通过利用轨迹历史作为工具变量来学习依赖于历史的策略，将框架中的因果IL问题转化为条件矩约束（CMRs）集合。我们提出了DML-IL这一新算法，利用工具变量回归解决这些CMRs并学习策略。我们为DML-IL提供了仿真实验的上界，该上界可以作为先前结果的特殊情况恢复。在具有连续状态和动作空间的玩具环境中，以及多个Mujoco任务上的实验结果表明，DML-IL在因果IL算法中表现出更优的性能。 

---
# Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving 

**Title (ZH)**: Gödel-Prover：开源自动定理证明的前沿模型 

**Authors**: Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin  

**Link**: [PDF](https://arxiv.org/pdf/2502.07640)  

**Abstract**: We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works. 

**Abstract (ZH)**: 我们介绍了Goedel-Prover，这是一个开源的大规模语言模型（LLM），在数学问题自动形式证明方面达到了目前的最先进（SOTA）性能。该领域的一个关键挑战是形式化数学陈述和证明数据的稀缺性，我们通过以下方式来应对这一挑战。我们训练了陈述形式化器，将Numina中的自然语言数学问题翻译成形式语言（Lean 4），创建了一个包含164万个形式化陈述的数据集。使用大语言模型来检查这些形式化陈述是否准确地保留了原始自然语言问题的内容。然后，我们通过训练一系列证明器迭代构建一个大规模的形式证明数据集。每个证明器都能够证明前一个证明器无法证明的许多陈述，这些新的证明被添加到下一个证明器的训练集中。最终的证明器在整体定理生成方面超过了所有现有的开源模型。在miniF2F基准测试中，它实现了57.6%的成功率（Pass@32），超过了之前最好的开源模型7.6%。在PutnamBench上，Goedel-Prover成功解决了7个问题（Pass@512），排名第一。此外，它为Lean Workbook问题生成了29,700个形式证明，几乎是之前工作生成的15,700个的两倍。 

---
# Distributed Value Decomposition Networks with Networked Agents 

**Title (ZH)**: 网络代理的分布式价值分解网络 

**Authors**: Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07635)  

**Abstract**: We investigate the problem of distributed training under partial observability, whereby cooperative multi-agent reinforcement learning agents (MARL) maximize the expected cumulative joint reward. We propose distributed value decomposition networks (DVDN) that generate a joint Q-function that factorizes into agent-wise Q-functions. Whereas the original value decomposition networks rely on centralized training, our approach is suitable for domains where centralized training is not possible and agents must learn by interacting with the physical environment in a decentralized manner while communicating with their peers. DVDN overcomes the need for centralized training by locally estimating the shared objective. We contribute with two innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and homogeneous agents settings respectively. Empirically, both algorithms approximate the performance of value decomposition networks, in spite of the information loss during communication, as demonstrated in ten MARL tasks in three standard environments. 

**Abstract (ZH)**: 我们在部分可观情况下研究分布式训练问题，其中协作多智能体强化学习代理（MARL）旨在最大化期望的联合累积奖励。我们提出了分布式价值分解网络（DVDN），该网络生成一个可以分解为代理特定价值函数的联合Q函数。尽管原始的价值分解网络依赖于集中式训练，我们的方法适用于集中式训练不可能且代理必须以去中心化方式与物理环境进行交互并与其同伴通信的领域。DVDN通过在局部估计共同目标来克服集中式训练的需要。为了适应异构代理和同构代理的不同情况，我们贡献了两种创新算法，分别为DVDN和DVDN（GT）。在实验中，两种算法在三种标准环境中的十个MARL任务中均能近似达到价值分解网络的表现，即使在通信过程中存在信息损失这一点也得到了验证。 

---
# DMWM: Dual-Mind World Model with Long-Term Imagination 

**Title (ZH)**: DMWM：双Mind世界模型与长期想象 

**Authors**: Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07591)  

**Abstract**: Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models. 

**Abstract (ZH)**: 世界模型中的想象对于使智能体以样本高效的方式学习长期策略至关重要。现有的基于递归状态空间模型（RSSM）的世界模型依赖于单步统计推理来捕捉环境动力学，因此它们无法执行长期想象任务，因为预测误差会逐步累积。受到人类认知的双过程理论的启发，我们提出了一种新型的双心智世界模型（DMWM）框架，该框架结合了逻辑推理以实现逻辑一致性的想象。DMWM 由两个组件组成：一个基于RSSM的心理过程1系统（RSSM-S1），它以直观的方式处理状态转换，以及一个结合逻辑推理的神经网络心理过程2系统（LINN-S2），它通过分层深度逻辑推理引导想象过程。系统间的反馈机制设计用于确保想象过程遵循真实环境的逻辑规则。该框架已在DMControl套件中要求长期规划的任务上进行了评估。广泛的实验结果表明，与最先进的世界模型相比，所提出的框架在逻辑一致性、试验效率、数据效率和长期想象方面均表现出显著的改进。 

---
# We Can't Understand AI Using our Existing Vocabulary 

**Title (ZH)**: 我们无法使用现有的词汇来理解人工智能 

**Authors**: John Hewitt, Robert Geirhos, Been Kim  

**Link**: [PDF](https://arxiv.org/pdf/2502.07586)  

**Abstract**: This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better. 

**Abstract (ZH)**: 本文立场认为，要理解人工智能，我们不能依赖现有的人类词汇。相反，我们应该努力创造新的词汇（即新词），这些新词能够精确地代表我们需要教给机器的人类概念，或是我们需要从机器中学习的机器概念。我们基于这样一个前提：人类和机器的概念是不同的。这意味着可解释性可以被框定为一个沟通问题：人类必须能够参考和控制机器概念，并将人类的概念传达给机器。通过开发新词来创建一个人机共享语言，我们相信这可以解决这一沟通问题。成功的创新词汇在抽象程度上既不过于具体，使得它们在多种情景中可以重用，也不至于过于抽象，以至于不能传递精确的信息。作为概念验证，我们展示了“长度新词”如何使控制大型语言模型的响应长度成为可能，而“多样性新词”则允许采样更具有变异性（多样化的）的回应。综合起来，我们认为我们无法用现有的词汇来理解人工智能，通过扩展词汇的方式来创造机会，从而更好地控制和理解机器。 

---
# Automated Capability Discovery via Model Self-Exploration 

**Title (ZH)**: 通过模型自我探索实现自动能力发现 

**Authors**: Cong Lu, Shengran Hu, Jeff Clune  

**Link**: [PDF](https://arxiv.org/pdf/2502.07577)  

**Abstract**: Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at this https URL. 

**Abstract (ZH)**: 基础模型已成为通用助手，通过在大规模网络数据上进行训练，在众多领域展现了多样化的功能。要精确地描述任何新模型的全谱能力和潜在风险仍然颇具挑战。现有的评估方法往往需要大量的人力投入，而为了设计更难的挑战来测试更强大的模型，所需的精力也越来越大。我们提出了自动能力发现（ACD）框架，该框架将一个基础模型指定为科学家，系统性地提出开放性任务以探查目标模型（可能是自身）的能力。通过结合前沿模型与开放性研究领域的思想，ACD 自动并系统地揭示出了目标模型的许多惊人能力和失败之处。我们在包括 GPT、Claude 和 Llama 系列在内的多种基础模型上展示了 ACD 的效果，表明它能够自动揭示出单一团队难以发现的数千种能力。我们进一步通过广泛的人员调查验证了自动化评分方法的有效性，观察到模型生成的评估与人工评估之间有高度的一致性。利用基础模型同时创造任务和自我评估的能力，ACD 是实现可扩展且自动化的新型人工智能系统评估的一大步。所有代码和评估日志均已开源，地址为：https://github.com/ACD-Framework/ACD。 

---
# LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid 

**Title (ZH)**: LASP-2: 重思线性注意力及其混合模型中的序列并行性 

**Authors**: Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, Yu Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2502.07563)  

**Abstract**: Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: this https URL. 

**Abstract (ZH)**: 线性序列模型方法，如线性注意力，提供了一些优势，例如线性时间训练和恒定内存推理，这在序列长度上的表现尤为明显。然而，现有的序列并行（SP）方法要么没有针对线性注意力的首乘特性进行优化，要么使用环状通信策略，这导致了较低的计算并行度，限制了它们在分布式系统中处理更长序列的可扩展性。在本文中，我们提出了LASP-2，这是一种新的SP方法，旨在增强在训练具有超长输入序列的线性注意力变换器模型时的通信与计算并行度。与之前的LASP工作相比，LASP-2重新思考了线性注意力层上SP所需的最小通信需求，并重组织了LASP的整体通信-计算工作流程。这样一来，只需要在中间内存状态上进行一次AllGather的集体通信，其规模与序列长度无关，从而显著提高了通信和计算并行度及其重叠。此外，我们通过类似通信设计对标准注意力模块进行扩展，提出了LASP-2H，为混合模型提供了一种高效的SP解决方案，这些混合模型结合了线性和标准注意力层。通过在具有线性注意力替换标准注意力变体的Linear-Llama3模型上的评估，我们证明了LASP-2和LASP-2H的有效性。特别是，LASP-2在64张GPU上对2048K序列长度的训练速度比LASP提高了15.2%，比环状注意力提高了36.6%。相关代码将作为本项目的一部分发布：https://github.com/[repository-name]。 

---
# LoRP-TTS: Low-Rank Personalized Text-To-Speech 

**Title (ZH)**: LoRP-TTS：低秩个性化文本-to-语音生成 

**Authors**: Łukasz Bondaruk, Jakub Kubiak  

**Link**: [PDF](https://arxiv.org/pdf/2502.07562)  

**Abstract**: Speech synthesis models convert written text into natural-sounding audio. While earlier models were limited to a single speaker, recent advancements have led to the development of zero-shot systems that generate realistic speech from a wide range of speakers using their voices as additional prompts. However, they still struggle with imitating non-studio-quality samples that differ significantly from the training datasets. In this work, we demonstrate that utilizing Low-Rank Adaptation (LoRA) allows us to successfully use even single recordings of spontaneous speech in noisy environments as prompts. This approach enhances speaker similarity by up to $30pp$ while preserving content and naturalness. It represents a significant step toward creating truly diverse speech corpora, that is crucial in all speech-related tasks. 

**Abstract (ZH)**: 语音合成模型能够将书面文本转换为自然声音的音频。早期的模型通常局限于单一说话者，但最近的进展使得能够开发出无需指定特定说话者即可生成多样语音的零样本系统，这些系统利用语音作为额外提示。然而，它们仍然难以模仿与训练数据集显著不同的非录音室质量的样本。在本研究中，我们展示了利用低秩适应（LoRA）可以有效地使用噪音环境中甚至单次记录的自发语音作为提示。这种方法在保持内容和自然度的同时，将说话者相似度提高了30个百分点。这代表了向创建真正多样的语音数据集的重要一步，这对于所有语音相关任务至关重要。 

---
# Unsupervised Translation of Emergent Communication 

**Title (ZH)**: unsupervised 转换可以翻译为“无监督”，而“Emergent Communication”可以翻译为“ emergent 通信”或“涌现通信”。因此，完整的翻译可以是：

无监督转换的涌现通信 

**Authors**: Ido Levy, Orr Paradise, Boaz Carmeli, Ron Meir, Shafi Goldwasser, Yonatan Belinkov  

**Link**: [PDF](https://arxiv.org/pdf/2502.07552)  

**Abstract**: Emergent Communication (EC) provides a unique window into the language systems that emerge autonomously when agents are trained to jointly achieve shared goals. However, it is difficult to interpret EC and evaluate its relationship with natural languages (NL). This study employs unsupervised neural machine translation (UNMT) techniques to decipher ECs formed during referential games with varying task complexities, influenced by the semantic diversity of the environment. Our findings demonstrate UNMT's potential to translate EC, illustrating that task complexity characterized by semantic diversity enhances EC translatability, while higher task complexity with constrained semantic variability exhibits pragmatic EC, which, although challenging to interpret, remains suitable for translation. This research marks the first attempt, to our knowledge, to translate EC without the aid of parallel data. 

**Abstract (ZH)**: Emergent Communication (EC) 提供了一个独特的窗口，用于探索当智能体在共同实现共享目标的训练过程中自主产生语言系统时的语言系统。然而，理解和评价 EC 与自然语言（NL）之间的关系是具有挑战性的。本研究采用无监督神经机器翻译（UNMT）技术，对在具有不同任务复杂性的参照游戏中形成的 EC 进行解码，这些游戏受环境语义多样性的影响。我们的研究结果表明，UNMT 有可能翻译 EC，表明由语义多样性表征的任务复杂性提高了 EC 的可翻译性，而具有受限语义变异性的高任务复杂性则产生了一种具有语用性的 EC，尽管这种 EC 解释起来更具挑战性，但仍然适合翻译。本研究（就我们所知）是首次尝试在没有平行数据辅助的情况下翻译 EC。 

---
# HGTUL: A Hypergraph-based Model For Trajectory User Linking 

**Title (ZH)**: HGTUL：基于超图的轨迹用户链接模型 

**Authors**: Fengjie Chang, Xinning Zhu, Zheng Hu, Yang Qin  

**Link**: [PDF](https://arxiv.org/pdf/2502.07549)  

**Abstract**: Trajectory User Linking (TUL), which links anonymous trajectories with users who generate them, plays a crucial role in modeling human mobility. Despite significant advancements in this field, existing studies primarily neglect the high-order inter-trajectory relationships, which represent complex associations among multiple trajectories, manifested through multi-location co-occurrence patterns emerging when trajectories intersect at various Points of Interest (POIs). Furthermore, they also overlook the variable influence of POIs on different trajectories, as well as the user class imbalance problem caused by disparities in user activity levels and check-in frequencies. To address these limitations, we propose a novel HyperGraph-based multi-perspective Trajectory User Linking model (HGTUL). Our model learns trajectory representations from both relational and spatio-temporal perspectives: (1) it captures high-order associations among trajectories by constructing a trajectory hypergraph and leverages a hypergraph attention network to learn the variable impact of POIs on trajectories; (2) it models the spatio-temporal characteristics of trajectories by incorporating their temporal and spatial information into a sequential encoder. Moreover, we design a data balancing method to effectively address the user class imbalance problem and experimentally validate its significance in TUL. Extensive experiments on three real-world datasets demonstrate that HGTUL outperforms state-of-the-art baselines, achieving improvements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics, respectively. 

**Abstract (ZH)**: 轨迹用户链接（TUL），即将匿名轨迹与生成它们的用户进行链接，对于建模人类移动性具有至关重要的作用。尽管该领域取得了显著的进步，但现有研究主要未能考虑高阶轨迹关系，这些关系通过轨迹在不同兴趣点（POI）交汇时产生的多地点共现模式，代表了多个轨迹之间的复杂关联。此外，现有研究还忽视了不同轨迹受兴趣点影响的变异性，以及由于用户活动水平和签到频率差异导致的用户类别的不均衡问题。为了解决这些局限性，我们提出了一种基于超图的多视角轨迹用户链接模型（HGTUL）。该模型从关系和空间-时间两个视角学习轨迹表示：（1）通过构建轨迹超图并利用超图注意网络来捕捉轨迹之间的高阶关联，同时学习不同轨迹受兴趣点影响的变异性；（2）通过将轨迹的时空信息融入序列编码器来建模轨迹的时空特征。此外，我们设计了一种数据平衡方法来有效解决用户类别的不均衡问题，并通过实验验证其在轨迹用户链接中的重要性。在三个真实世界数据集上的广泛实验表明，HGTUL 在 ACC@1 和 Macro-F1 指标的性能上均优于最先进的基线算法，分别提高了 2.57%~20.09% 和 5.68%~26.00%。 

---
# Exoplanet Transit Candidate Identification in TESS Full-Frame Images via a Transformer-Based Algorithm 

**Title (ZH)**: 基于 transformer 算法的苔丝全帧图像系外行星凌星候选体识别 

**Authors**: Helem Salinas, Rafael Brahm, Greg Olmschenk, Richard K. Barry, Karim Pichara, Stela Ishitani Silva, Vladimir Araujo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07542)  

**Abstract**: The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius > 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity. 

**Abstract (ZH)**: 宜居带外行星搜寻卫星（The Transiting Exoplanet Survey Satellite, TESS）正在对天空的大范围区域进行观测，生成了大量需要深入分析的光变时序数据，以识别外行星凌星信号。自动学习方法已被成功应用于识别凌星信号。然而，大多数现有方法主要集中在凌星候选项的分类和验证上，对候选搜索的新技术探索较少。为寻找新的外行星凌星候选项，我们提出了一种方法来直接识别外行星凌星信号，无需进行相位折叠或假设信号的周期性，尤其是在多凌星光变曲线中观测到的周期性。为实现这一目标，我们采用了一种受Transformer启发的新型神经网络直接处理全帧图像（Full Frame Image, FFI）光变曲线来检测外行星凌星。Transformer最初是在自然语言处理领域开发的，最近证明了在捕捉长距离依赖性方面优于以往专注于序列数据的方法。这种能力使我们能够利用多头自注意力直接从完整的光变曲线中识别外行星凌星信号，结合背景和质心时间序列，不需要事先的凌星参数。该网络被训练以学习凌星信号的特征，如深度形状，这有助于区分行星凌星和其他变异性来源。我们的模型成功地从TESS第1-26节数据中识别出了214个新行星系统的候选项，其中包括122个多凌星光变曲线、88个单凌星和4个多行星系统，这些行星的半径大于0.27倍木星半径（$R_{\mathrm{Jupiter}}$），展示了其在不依赖周期性的情况下检测凌星的能力。 

---
# VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation 

**Title (ZH)**: VidCRAFT3：图像到视频生成中的相机、物体和照明控制 

**Authors**: Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07531)  

**Abstract**: Recent image-to-video generation methods have demonstrated success in enabling control over one or two visual elements, such as camera trajectory or object motion. However, these methods are unable to offer control over multiple visual elements due to limitations in data and network efficacy. In this paper, we introduce VidCRAFT3, a novel framework for precise image-to-video generation that enables control over camera motion, object motion, and lighting direction simultaneously. To better decouple control over each visual element, we propose the Spatial Triple-Attention Transformer, which integrates lighting direction, text, and image in a symmetric way. Since most real-world video datasets lack lighting annotations, we construct a high-quality synthetic video dataset, the VideoLightingDirection (VLD) dataset. This dataset includes lighting direction annotations and objects of diverse appearance, enabling VidCRAFT3 to effectively handle strong light transmission and reflection effects. Additionally, we propose a three-stage training strategy that eliminates the need for training data annotated with multiple visual elements (camera motion, object motion, and lighting direction) simultaneously. Extensive experiments on benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing high-quality video content, surpassing existing state-of-the-art methods in terms of control granularity and visual coherence. All code and data will be publicly available. Project page: this https URL. 

**Abstract (ZH)**: 近年来，图像到视频生成方法在控制一个或两个视觉元素（如相机轨迹或对象运动）方面已经取得了成功。然而，这些方法由于数据限制和网络效能的局限性，无法同时控制多个视觉元素。在本文中，我们提出了一种新的框架VidCRAFT3，该框架能够同时控制相机运动、对象运动和光照方向。为了更好地分离每个视觉元素的控制，我们提出了空间三重注意变换器，该变换器以对称的方式结合了光照方向、文本和图像。由于大多数真实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集——VideoLightingDirection (VLD)数据集。该数据集包含光照方向注释和多样化的物体，使得VidCRAFT3能够有效地处理强烈的透射和反射效应。此外，我们还提出了一种三阶段训练策略，可以消除同时标注相机运动、对象运动和光照方向的需求。基准数据集上的广泛实验表明，VidCRAFT3在生成高质量视频内容方面具有有效性，其控制粒度和视觉连贯性超过了现有的最先进的方法。所有代码和数据都将公开。项目页面：[这里](this https URL)。 

---
# Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization 

**Title (ZH)**: 使用批量归一化和权重归一化缩放离策强化学习 

**Authors**: Daniel Palenicek, Florian Vogt, Jan Peters  

**Link**: [PDF](https://arxiv.org/pdf/2502.07523)  

**Abstract**: Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics, which are emphasized by higher UTD ratios. To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive performance across 25 challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning. 

**Abstract (ZH)**: 强化学习已经在多个领域取得了重要进展，但在实际应用中样本效率仍旧是一个瓶颈。最近，CrossQ展示了在较低更新到数据比（UTD）的情况下达到最先进的样本效率（UTD比率为1）。在本项工作中，我们探讨了CrossQ在较高UTD比条件下的扩展行为。我们识别出在训练动态中存在的一些挑战，这些挑战在较高UTD比下更加显著。为了解决这些问题，我们将权重规范化技术整合到了CrossQ框架中，该方法能够稳定训练过程，防止潜在的学习能力下降，并保持有效的学习率不变。我们提出的这一方法在不断增加的UTD比条件下能够可靠地扩展，在德蒙特控制套件和Myosuite基准测试的25个具有挑战性的连续控制任务中，尤其是在复杂的狗类和人形环境，实现了竞争力的表现。这项工作消除了对极端干预（如网络重置）的需要，并提供了一种简单而稳健的方法，以提高无模型强化学习中的样本效率和可扩展性。 

---
# The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation 

**Title (ZH)**: trap就在提示中：去标识化痕迹会增加合成胸片生成中的记忆风险 

**Authors**: Raman Dutt  

**Link**: [PDF](https://arxiv.org/pdf/2502.07516)  

**Abstract**: Generative models, particularly text-to-image (T2I) diffusion models, play a crucial role in medical image analysis. However, these models are prone to training data memorization, posing significant risks to patient privacy. Synthetic chest X-ray generation is one of the most common applications in medical image analysis with the MIMIC-CXR dataset serving as the primary data repository for this task. This study adopts a data-driven approach and presents the first systematic attempt to identify prompts and text tokens in MIMIC-CXR that contribute the most to training data memorization. Our analysis reveals an unexpected finding: prompts containing traces of de-identification procedures are among the most memorized, with de-identification markers contributing the most. Furthermore, we also find existing inference-time memorization mitigation strategies are ineffective and fail to sufficiently reduce the model's reliance on memorized text tokens highlighting a broader issue in T2I synthesis with MIMIC-CXR. On this front, we propose actionable strategies to enhance privacy and improve the reliability of generative models in medical imaging. Finally, our results provide a foundation for future work on developing and benchmarking memorization mitigation techniques for synthetic chest X-ray generation using the MIMIC-CXR dataset. 

**Abstract (ZH)**: 生成模型，尤其是文本到图像（T2I）扩散模型，在医学图像分析中扮演着至关重要的角色。然而，这些模型容易发生训练数据记忆现象，这对患者的隐私构成重大风险。生成合成胸部X射线图像是最常见的医学图像分析应用之一，MIMIC-CXR数据集是该任务的主要数据库。本研究采用数据驱动的方法，并介绍了第一次系统地识别MIMIC-CXR中最能导致训练数据记忆的提示和文本标记的尝试。我们的分析揭示了一个意外的发现：包含去标识程序痕迹的提示是最容易记忆的，而去标识标记贡献最大。此外，我们还发现现有的推理时的记忆减轻策略无效，无法显著减少模型对记忆文本标记的依赖，这在一个更广泛的层面上揭示了使用MIMIC-CXR进行T2I合成时存在的问题。为此，我们提出了增强隐私和提高生成模型在医学成像中可靠性的可操作策略。最后，我们的结果为今后基于MIMIC-CXR数据集开发和基准测试合成胸部X射线生成的记忆减轻技术奠定了基础。 

---
# WebChecker: A Versatile EVL Plugin for Validating HTML Pages with Bootstrap Frameworks 

**Title (ZH)**: WebChecker：一个适用于验证使用Bootstrap框架的HTML页面的多功能EVL插件 

**Authors**: Milind Cherukuri  

**Link**: [PDF](https://arxiv.org/pdf/2502.07479)  

**Abstract**: WebChecker is a plugin for Epsilon Validation Language (EVL), designed to validate both static and dynamic HTML pages utilizing frameworks like Bootstrap. By employing configurable EVL constraints, WebChecker enforces implicit rules governing HTML and CSS frameworks. The effectiveness of the plugin is demonstrated through its application on Bootstrap, the widely adopted HTML, CSS, and JavaScript framework. WebChecker comes with a set of EVL constraints to assess Bootstrap based web pages. To substantiate our claims, I present an illustrative example featuring two solutions that effectively enforce implicit rules. 

**Abstract (ZH)**: WebChecker 是 Epsilon Validation Language（EVL）的一个插件，旨在利用如 Bootstrap 等框架验证静态和动态 HTML 页面。通过运用可配置的 EVL 约束，WebChecker 强制执行 HTML 和 CSS 框架中的隐式规则。通过对广泛采用的 HTML、CSS 和 JavaScript 框架 Bootstrap 的应用，展示了插件的有效性。WebChecker 包含一套用于评估基于 Bootstrap 的网页的 EVL 约束。为了证明我们的论点，我将展示一个示例，其中包括两个有效强制执行隐式规则的解决方案。 

---
# 5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma Turbulence 

**Title (ZH)**: 5D神经代理模型用于非线性等离子体湍流的 gyrokinetic 模拟 

**Authors**: Gianluca Galletti, Fabian Paischer, Paul Setinek, William Hornsby, Lorenzo Zanisi, Naomi Carey, Stanislas Pamela, Johannes Brandstetter  

**Link**: [PDF](https://arxiv.org/pdf/2502.07469)  

**Abstract**: Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to achieving commercially viable fusion power is understanding plasma turbulence, which can significantly degrade plasma confinement. Modelling turbulence is crucial to design performing plasma scenarios for next-generation reactor-class devices and current experimental machines. The nonlinear gyrokinetic equation underpinning turbulence modelling evolves a 5D distribution function over time. Solving this equation numerically is extremely expensive, requiring up to weeks for a single run to converge, making it unfeasible for iterative optimisation and control studies. In this work, we propose a method for training neural surrogates for 5D gyrokinetic simulations. Our method extends a hierarchical vision transformer to five dimensions and is trained on the 5D distribution function for the adiabatic electron approximation. We demonstrate that our model can accurately infer downstream physical quantities such as heat flux time trace and electrostatic potentials for single-step predictions two orders of magnitude faster than numerical codes. Our work paves the way towards neural surrogates for plasma turbulence simulations to accelerate deployment of commercial energy production via nuclear fusion. 

**Abstract (ZH)**: 核聚变在寻求可靠且可持续的能源生产中扮演着关键角色。实现商业化可行的聚变能的一个重大障碍是理解等离子体湍流，因为这会显著影响等离子体的约束程度。湍流建模对于设计下一代反应堆级装置和现有实验装置的高性能等离子体场景至关重要。支撑湍流建模的非线性色散准静态运动力学方程随着时间演化出一个5维分布函数。数值求解该方程极其昂贵，单个工作迭代可能需要几周时间才能收敛，这使其不适合用于迭代优化和控制研究。本文提出了一种方法，用于训练神经代理模型来进行5维色散准静态运动力学模拟。我们扩展了一种分层视觉变换器到五个维度，并在绝热电子近似下的5维分布函数上进行了训练。实验表明，我们的模型能够比数值代码快两个数量级的速度准确预测后续的物理量，如热流时间迹和静电势。我们的工作为通过核聚变加速商业能源生产提供了基于神经代理模型的等离子体湍流模拟的途径。 

---
# Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models 

**Title (ZH)**: 犯罪预测：基于深度学习模型的空间时间分析 

**Authors**: Li Mao, Wei Du, Shuo Wen, Qi Li, Tong Zhang, Wei Zhong  

**Link**: [PDF](https://arxiv.org/pdf/2502.07465)  

**Abstract**: This study uses deep-learning models to predict city partition crime counts on specific days. It helps police enhance surveillance, gather intelligence, and proactively prevent crimes. We formulate crime count prediction as a spatiotemporal sequence challenge, where both input data and prediction targets are spatiotemporal sequences. In order to improve the accuracy of crime forecasting, we introduce a new model that combines Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a comparative analysis to access the effects of various data sequences, including raw and binned data, on the prediction errors of four deep learning forecasting models. Directly inputting raw crime data into the forecasting model causes high prediction errors, making the model unsuitable for real - world use. The findings indicate that the proposed CNN-LSTM model achieves optimal performance when crime data is categorized into 10 or 5 groups. Data binning can enhance forecasting model performance, but poorly defined intervals may reduce map granularity. Compared to dividing into 5 bins, binning into 10 intervals strikes an optimal balance, preserving data characteristics and surpassing raw data in predictive modelling efficacy. 

**Abstract (ZH)**: 本研究采用深度学习模型预测特定日期城市分区犯罪数量，有助于警方增强监控、收集情报，并主动预防犯罪。我们将犯罪数量预测形成为一种时空序列挑战，输入数据和预测目标均为时空序列。为了提高犯罪预测精度，我们引入了一种结合卷积神经网络（CNN）和长短期记忆网络（LSTM）的新模型。我们对不同数据序列（包括原始数据和分组数据）对四种深度学习预测模型预测误差的影响进行了比较分析。直接将原始犯罪数据输入预测模型会导致预测误差较高，使得该模型不适合实际应用。研究结果表明，当犯罪数据被分为10组或5组时，提出的CNN-LSTM模型性能最佳。数据分组可以提高预测模型的性能，但区间定义不当可能会降低地图的细节度。相比于分5组，将数据分10组能够更好地平衡精度和细节，同时在保持数据特征的同时，优于原始数据在预测建模方面的效果。 

---
# JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed Metadata 

**Title (ZH)**: JamendoMaxCaps：一个包含补充元数据的大规模音乐- Caption 数据集 

**Authors**: Abhinaba Roy, Renhang Liu, Tongyu Lu, Dorien Herremans  

**Link**: [PDF](https://arxiv.org/pdf/2502.07461)  

**Abstract**: We introduce JamendoMaxCaps, a large-scale music-caption dataset featuring over 200,000 freely licensed instrumental tracks from the renowned Jamendo platform. The dataset includes captions generated by a state-of-the-art captioning model, enhanced with imputed metadata. We also introduce a retrieval system that leverages both musical features and metadata to identify similar songs, which are then used to fill in missing metadata using a local large language model (LLLM). This approach allows us to provide a more comprehensive and informative dataset for researchers working on music-language understanding tasks. We validate this approach quantitatively with five different measurements. By making the JamendoMaxCaps dataset publicly available, we provide a high-quality resource to advance research in music-language understanding tasks such as music retrieval, multimodal representation learning, and generative music models. 

**Abstract (ZH)**: 我们引入了JamendoMaxCaps，这是一个大型音乐-描述数据集，包含了来自著名Jamendo平台的超过200,000首免费许可的乐器轨道。该数据集包含由先进描述模型生成的描述，并结合了补充的元数据。我们还介绍了一个检索系统，该系统利用了音乐特征和元数据来识别相似的歌曲，然后使用局部大型语言模型（LLLM）填充缺失的元数据。这种方法使研究人员能够为音乐-语言理解任务提供更全面和更具信息性的数据集。我们通过五种不同的测量方法对这种方法进行了定量验证。通过将JamendoMaxCaps数据集公开，我们为推进音乐-语言理解任务（如音乐检索、多模态表示学习和生成音乐模型）的研究提供了高质量的资源。 

---
# PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian 

**Title (ZH)**: PerCul：以故事驱动的波斯语语言模型文化评估 

**Authors**: Erfan Moosavi Monazzah, Vahid Rahimzadeh, Yadollah Yaghoobzadeh, Azadeh Shakery, Mohammad Taher Pilehvar  

**Link**: [PDF](https://arxiv.org/pdf/2502.07459)  

**Abstract**: Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the use of translation as a shortcut. We evaluate several state-of-the-art multilingual and Persian-specific LLMs, establishing a foundation for future research in cross-cultural NLP evaluation. Our experiments demonstrate a 11.3% gap between best closed source model and layperson baseline while the gap increases to 21.3% by using the best open-weight model. You can access the dataset from here: this https URL 

**Abstract (ZH)**: 大型语言模型主要体现了西方文化，这主要是由于训练数据以英文化为中心。这种不平衡带来了一个显著的挑战，因为随着LLMs在多种不同上下文中的应用，对其在非英语语言（包括波斯语）文化中的适用性评估还不够充分。为了解决这一问题，我们引入了PerCul数据集，该数据集精心构建，旨在评估LLMs对波斯文化的敏感性。PerCul包含基于故事的多项选择题，能够捕捉到文化层面的细微差异。与现有基准不同，PerCul通过来自波斯本土标注者的输入来确保其真实性和防止使用翻译作为捷径。我们评估了几种最先进的多语言和波斯语特定的LLMs，为未来跨文化自然语言处理（NLP）评估研究奠定了基础。我们的实验结果显示，在最佳封闭源模型和普通人的基线下，性能差距为11.3%；而使用最佳开源权重模型时，这一差距增加到21.3%。您可以从以下链接访问数据集：[这个链接](提供链接的具体内容) 

---
# RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation 

**Title (ZH)**: RusCode：俄文化代码文本生成基准数据集 

**Authors**: Viacheslav Vasilev, Julia Agafonova, Nikolai Gerasimenko, Alexander Kapitanov, Polina Mikhailova, Evelina Mironova, Denis Dimitrov  

**Link**: [PDF](https://arxiv.org/pdf/2502.07455)  

**Abstract**: Text-to-image generation models have gained popularity among users around the world. However, many of these models exhibit a strong bias toward English-speaking cultures, ignoring or misrepresenting the unique characteristics of other language groups, countries, and nationalities. The lack of cultural awareness can reduce the generation quality and lead to undesirable consequences such as unintentional insult, and the spread of prejudice. In contrast to the field of natural language processing, cultural awareness in computer vision has not been explored as extensively. In this paper, we strive to reduce this gap. We propose a RusCode benchmark for evaluating the quality of text-to-image generation containing elements of the Russian cultural code. To do this, we form a list of 19 categories that best represent the features of Russian visual culture. Our final dataset consists of 1250 text prompts in Russian and their translations into English. The prompts cover a wide range of topics, including complex concepts from art, popular culture, folk traditions, famous people's names, natural objects, scientific achievements, etc. We present the results of a human evaluation of the side-by-side comparison of Russian visual concepts representations using popular generative models. 

**Abstract (ZH)**: 文本到图像生成模型在全球用户中受到广泛关注。然而，这些模型往往对以英语为主的文化表现出强烈的偏好，忽视或歪曲了其他语言群体、国家和民族的独特特征。缺乏文化意识会降低生成质量，并可能导致无意中的侮辱以及偏见的传播。与自然语言处理领域相比，这种文化意识在计算机视觉领域尚未得到充分探索。本文旨在缩小这一差距。我们提出了一种名为RusCode的基准测试，用于评估包含俄罗斯文化元素的文本到图像生成的质量。为此，我们列出了19个最佳代表俄罗斯视觉文化特征的类别。最终数据集包含了1250条俄罗斯文本提示及其英语翻译，这些提示涵盖了从艺术复杂概念、流行文化、民间传统、知名人士的名称、自然物体、科学成就等多个广泛的主题。我们展示了关于使用流行生成模型对俄罗斯视觉概念表示进行并排比较的人类评估结果。 

---
# Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon 

**Title (ZH)**: 遗忘你所知道的LLMs评价标准——LLMs就像一只变色龙 

**Authors**: Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07445)  

**Abstract**: Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation. 

**Abstract (ZH)**: 大规模语言模型（LLMs）通常在公共基准测试中表现出色，但这些高分可能掩盖了对数据集特定表面特征的过度依赖，而非真正的语言理解能力。我们引入了变色龙基准过拟合检测器（C-BOD，Chameleon Benchmark Overfit Detector），这是一种元评价框架，通过参数变换系统地扭曲基准测试提示，并检测LLMs的过拟合现象。通过保留输入的语义内容和标签重新表述输入，C-BOD揭示了模型的性能是否由记忆模式驱动。我们在MMLU基准测试上使用26种领先的大规模语言模型进行评估，我们的方法在轻微扰乱下显示出平均性能下降2.15%，其中有20种模型表现出统计上的显著差异。值得注意的是，基线准确度较高的模型在扰乱下性能差异更大，而大型LLMs对重新表述更为敏感，这表明这两种情况可能会过度依赖固定提示模式。相比之下，Llam家族模型和基线准确度较低的模型在扰乱下性能下降不显著，表明它们对表面特征的依赖性较低。此外，C-BOD的设计具有数据集和模型的通用性，便于集成到训练管道中，促进更稳健的语言理解。我们的研究结果挑战了社区仅仅关注排行榜分数，而应更加注重LLMs评价中的韧性和泛化能力。 

---
# SensPS: Sensing Personal Space Comfortable Distance between Human-Human Using Multimodal Sensors 

**Title (ZH)**: SensPS：基于多模态传感器的人与人之间舒适个人空间感知距离 

**Authors**: Ko Watanabe, Nico Förster, Shoya Ishimaru  

**Link**: [PDF](https://arxiv.org/pdf/2502.07441)  

**Abstract**: Personal space, also known as peripersonal space, is crucial in human social interaction, influencing comfort, communication, and social stress. Estimating and respecting personal space is essential for enhancing human-computer interaction (HCI) and smart environments. Personal space preferences vary due to individual traits, cultural background, and contextual factors. Advanced multimodal sensing technologies, including eye-tracking and wristband sensors, offer opportunities to develop adaptive systems that dynamically adjust to user comfort levels. Integrating physiological and behavioral data enables a deeper understanding of spatial interactions. This study develops a sensor-based model to estimate comfortable personal space and identifies key features influencing spatial preferences. Our findings show that multimodal sensors, particularly eye-tracking and physiological wristband data, can effectively predict personal space preferences, with eye-tracking data playing a more significant role. An experimental study involving controlled human interactions demonstrates that a Transformer-based model achieves the highest predictive accuracy (F1 score: 0.87) for estimating personal space. Eye-tracking features, such as gaze point and pupil diameter, emerge as the most significant predictors, while physiological signals from wristband sensors contribute marginally. These results highlight the potential for AI-driven personalization of social space in adaptive environments, suggesting that multimodal sensing can be leveraged to develop intelligent systems that optimize spatial arrangements in workplaces, educational institutions, and public settings. Future work should explore larger datasets, real-world applications, and additional physiological markers to enhance model robustness. 

**Abstract (ZH)**: 个人空间，也称为周边空间，在人际交往中至关重要，影响着舒适度、沟通和社交压力。估算并尊重个人空间对于提升人机交互（HCI）和智能环境至关重要。个人空间偏好因个体特征、文化背景和情境因素而异。先进的多模态传感技术，包括眼动追踪和腕带传感器，为开发能够动态适应用户舒适度的自适应系统提供了机会。整合生理和行为数据有助于更深层次地了解空间互动。本研究开发了一种基于传感器的模型来估算舒适的个人空间，并识别影响空间偏好的关键特征。我们的研究结果表明，多模态传感器，尤其是眼动追踪和生理腕带数据，能够有效地预测个人空间偏好，其中眼动追踪数据的作用更为显著。一项涉及受控人类互动的实验研究表明，基于Transformer的模型在预测个人空间方面具有最高的预测准确性（F1分数：0.87）。眼动追踪特征（如注视点和瞳孔直径）是最重要的预测因子，而来自腕带传感器的生理信号则几乎没有贡献。这些结果突显了人工智能驱动的社会空间个性化在自适应环境中的潜在价值，表明多模态传感可以用来开发能够优化工作场所、教育机构和公共空间中空间布局的智能系统。未来的研究应探索更大的数据集、实际应用场景和更多的生理标记以提高模型的鲁棒性。 

---
# RomanLens: Latent Romanization and its role in Multilinguality in LLMs 

**Title (ZH)**: RomanLens：潜在的罗马化及其在大规模语言模型中的多语种作用 

**Authors**: Alan Saji, Jaavid Aktar Husain, Thanmay Jayakumar, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra, Ratish Puduppully  

**Link**: [PDF](https://arxiv.org/pdf/2502.07424)  

**Abstract**: Large Language Models (LLMs) exhibit remarkable multilingual generalization despite being predominantly trained on English-centric corpora. A fundamental question arises: how do LLMs achieve such robust multilingual capabilities? For non-Latin script languages, we investigate the role of romanization - the representation of non-Latin scripts using Latin characters - as a bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and romanized scripts, suggesting a shared underlying representation. Additionally in translation towards non Latin languages, our findings reveal that when the target language is in romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of romanization in facilitating language transfer. Our work provides new directions for potentially improving multilingual language modeling and interpretability. 

**Abstract (ZH)**: 大型语言模型（LLMs）尽管主要是在以英语为中心的数据集中进行训练，但在多语言泛化方面表现出显著的能力。一个根本性的问题产生了：LLMs是如何实现如此坚实的多语言能力的？对于非拉丁字母体系的语言，我们研究了罗马化——即使用拉丁字母表示非拉丁字母体系文字——在多语言处理中作为桥梁的作用。利用机制可解释性技术，我们分析了下一个词生成的过程，并发现中间层经常在转换到本地文字之前将目标词以罗马化形式表示，这一现象我们称为潜在罗马化。此外，通过激活补丁实验，我们展示了LLMs在本地文字和罗马化文字中以相似的方式编码语义概念，这表明它们具有共享的基础表示。在非拉丁字母体系语言的翻译中，我们的发现表明，在目标语言以罗马化形式呈现时，其表示在模型层中出现得更早，相比之下，本地文字的表示则更晚。这些洞察加深了我们对LLMs中多语言表示的理解，并突显了罗马化在促进语言迁移中的隐含作用。我们的研究为提高多语言语言建模和可解释性的方向提供了新的思路。 

---
# No Data, No Optimization: A Lightweight Method To Disrupt Neural Networks With Sign-Flips 

**Title (ZH)**: 没有数据，就没有优化：一种基于符号翻转的轻量级方法以干扰神经网络 

**Authors**: Ido Galil, Moshe Kimhi, Ran El-Yaniv  

**Link**: [PDF](https://arxiv.org/pdf/2502.07408)  

**Abstract**: Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping only a handful of sign bits in their parameters. We introduce Deep Neural Lesion (DNL), a data-free, lightweight method that locates these critical parameters and triggers massive accuracy drops. We validate its efficacy on a wide variety of computer vision models and datasets. The method requires no training data or optimization and can be carried out via common exploits software, firmware or hardware based attack vectors. An enhanced variant that uses a single forward and backward pass further amplifies the damage beyond DNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet reduces accuracy by 99.8\%. We also show that selectively protecting a small fraction of vulnerable sign bits provides a practical defense against such attacks. 

**Abstract (ZH)**: 深度神经网络（DNNs）可能会因翻转少量参数的符号位而灾难性地受到影响。我们提出了一种无数据、轻量级的方法——深度神经网络缺陷（DNL），该方法能够定位这些关键参数，并导致显著的准确率下降。我们通过多种计算机视觉模型和数据集验证了其有效性。该方法不需要训练数据或优化，可以通过常见的利用软件、固件或硬件攻击向量来实现。一种增强版使用单次前向和反向传播进一步放大了DNL的零次攻击的效果。在ImageNet上翻转ResNet50模型的两个符号位可使其准确率降低99.8%。我们还展示了有选择性地保护一小部分易受攻击的符号位可以实际防范此类攻击。 

---
# Human-in-the-Loop Annotation for Image-Based Engagement Estimation: Assessing the Impact of Model Reliability on Annotation Accuracy 

**Title (ZH)**: 基于图像的参与度估计中的人工介入注标：模型可靠性对注标准确率影响的评估 

**Authors**: Sahana Yadnakudige Subramanya, Ko Watanabe, Andreas Dengel, Shoya Ishimaru  

**Link**: [PDF](https://arxiv.org/pdf/2502.07404)  

**Abstract**: Human-in-the-loop (HITL) frameworks are increasingly recognized for their potential to improve annotation accuracy in emotion estimation systems by combining machine predictions with human expertise. This study focuses on integrating a high-performing image-based emotion model into a HITL annotation framework to evaluate the collaborative potential of human-machine interaction and identify the psychological and practical factors critical to successful collaboration. Specifically, we investigate how varying model reliability and cognitive framing influence human trust, cognitive load, and annotation behavior in HITL systems. We demonstrate that model reliability and psychological framing significantly impact annotators' trust, engagement, and consistency, offering insights into optimizing HITL frameworks. Through three experimental scenarios with 29 participants--baseline model reliability (S1), fabricated errors (S2), and cognitive bias introduced by negative framing (S3)--we analyzed behavioral and qualitative data. Reliable predictions in S1 yielded high trust and annotation consistency, while unreliable outputs in S2 led to increased critical evaluations but also heightened frustration and response variability. Negative framing in S3 revealed how cognitive bias influenced participants to perceive the model as more relatable and accurate, despite misinformation regarding its reliability. These findings highlight the importance of both reliable machine outputs and psychological factors in shaping effective human-machine collaboration. By leveraging the strengths of both human oversight and automated systems, this study establishes a scalable HITL framework for emotion annotation and lays the foundation for broader applications in adaptive learning and human-computer interaction. 

**Abstract (ZH)**: 人工在环路（HITL）框架因其将机器预测与人类专业知识相结合的潜力，在提升情绪估计系统注释准确方面正日益受到认可。本研究旨在将一个表现优异的图像基情绪模型集成到HITL注释框架中，以评估人机交互的协作潜力，并确定关键的心理和实际因素，以促进成功协作。具体而言，我们研究不同模型可靠性和认知框架如何影响人在环路系统中的信任、认知负荷和注释行为。研究结果表明，模型可靠性和心理框架显著影响注释者对模型的信任、参与度和一致性，提供了一种优化HITL框架的见解。通过三个实验场景（S1：基线模型可靠性、S2：伪造错误、S3：负面框架引入的认知偏差）和29名参与者的实验，我们对行为和定性数据进行了分析。S1中的可靠预测产生了高水平的信任和注释一致性，而S2中的不可靠输出则导致了更多的批判性评估，但也增加了挫折感和反应的变异性。S3中的负面框架揭示了认知偏差如何使参与者尽管对模型的可靠性存在误导信息，仍认为模型更具相关性和准确性。这些发现强调了可靠机器输出和心理因素在塑造有效的人机协作中的重要性。通过利用人类监督和自动化系统的长处，本研究建立了一个可扩展的HITL框架用于情绪注释，并为适应性学习和人机交互的更广泛应用奠定了基础。 

---
# Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning 

**Title (ZH)**: 以生成式AI提升高等教育：一种多模态个性化学习方法 

**Authors**: Johnny Chan, Yuming Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07401)  

**Abstract**: This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies. 

**Abstract (ZH)**: 本研究通过设计和开发一款多模态聊天机器人，探讨了生成式人工智能（GenAI）在高等教育领域的应用机会。该聊天机器人基于本科生课程的需求，利用ChatGPT API实现精细的文本交互，结合Google Bard进行高级图像分析和图表到代码的转换，展示了GenAI在解决广泛教育问题方面的潜力。此外，该聊天机器人还提供了一个基于文件的分析工具，供教育者使用，通过情感和情绪分析提供学生反馈的深入见解，并利用关键指标总结课程评估。这些组合突显了多模态对话AI在提升教学和学习过程中的关键作用，有望在教育适应性、参与度和反馈分析方面带来重大进步。通过展示一个实用的网络应用，本研究强调了整合GenAI技术的重要性和迫切性，以促进更加动态和响应性的教育环境，最终为改善教育成果和教育策略做出贡献。 

---
# Explainable Multimodal Machine Learning for Revealing Structure-Property Relationships in Carbon Nanotube Fibers 

**Title (ZH)**: 可解释的多模态机器学习在揭示碳纳米管纤维的结构-性质关系中的应用 

**Authors**: Daisuke Kimura, Naoko Tajima, Toshiya Okazaki, Shun Muroga  

**Link**: [PDF](https://arxiv.org/pdf/2502.07400)  

**Abstract**: In this study, we propose Explainable Multimodal Machine Learning (EMML), which integrates the analysis of diverse data types (multimodal data) using factor analysis for feature extraction with Explainable AI (XAI), for carbon nanotube (CNT) fibers prepared from aqueous dispersions. This method is a powerful approach to elucidate the mechanisms governing material properties, where multi-stage fabrication conditions and multiscale structures have complex influences. Thus, in our case, this approach helps us understand how different processing steps and structures at various scales impact the final properties of CNT fibers. The analysis targeted structures ranging from the nanoscale to the macroscale, including aggregation size distributions of CNT dispersions and the effective length of CNTs. Furthermore, because some types of data were difficult to interpret using standard methods, challenging-to-interpret distribution data were analyzed using Negative Matrix Factorization (NMF) for extracting key features that determine the outcome. Contribution analysis with SHapley Additive exPlanations (SHAP) demonstrated that small, uniformly distributed aggregates are crucial for improving fracture strength, while CNTs with long effective lengths are significant factors for enhancing electrical conductivity. The analysis also identified thresholds and trends for these key factors to assist in defining the conditions needed to optimize CNT fiber properties. EMML is not limited to CNT fibers but can be applied to the design of other materials derived from nanomaterials, making it a useful tool for developing a wide range of advanced materials. This approach provides a foundation for advancing data-driven materials research. 

**Abstract (ZH)**: 在本研究中，我们提出了一种可解释的多模态机器学习（Explainable Multimodal Machine Learning, EMML）方法。该方法结合了因子分析用于特征提取的多模态数据分析与可解释的人工智能（Explainable AI, XAI），研究水分散液制备的碳纳米管（Carbon Nanotube, CNT）纤维的性能。该方法是一种强大而有力的手段，用于阐明影响材料性质的机制，尤其是多阶段制造条件和跨尺度结构的复杂影响。因此，在本研究中，这种方法帮助我们理解不同加工步骤和不同尺度结构如何影响CNT纤维的最终性能。分析涵盖了从纳米尺度到宏观尺度的结构，包括CNT分散液的聚集尺寸分布和CNT的有效长度。此外，由于某些类型的数据难以使用标准方法进行解释，我们利用负矩阵分解（Negative Matrix Factorization, NMF）来分析难以解释的分布数据，提取决定结果的关键特征。SHapley Additive exPlanations（SHAP）值贡献分析表明，均匀分布的小尺寸聚集对提高断裂强度至关重要，而具有较长有效长度的CNT对于增强电导率至关重要。分析还确定了这些关键因素的阈值和趋势，有助于定义优化CNT纤维性能所需的条件。EMML不仅限于CNT纤维，还可以应用于从纳米材料衍生的其他材料的设计，使其成为开发各种先进材料的有力工具。这种方法为推进数据驱动的材料研究奠定了基础。 

---
# On Iterative Evaluation and Enhancement of Code Quality Using GPT-4o 

**Title (ZH)**: 使用GPT-4迭代评估和提升代码质量的研究 

**Authors**: Rundong Liu, Andre Frade, Amal Vaidya, Maxime Labonne, Marcus Kaiser, Bismayan Chakrabarti, Jonathan Budd, Sean Moran  

**Link**: [PDF](https://arxiv.org/pdf/2502.07399)  

**Abstract**: This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluator's feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The framework's evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at: this https URL. 

**Abstract (ZH)**: 本文介绍了CodeQUEST，这是一种利用大型语言模型（LLMs）迭代评估和提升代码质量的新框架，从可读性、可维护性、效率和安全性等多个维度对代码质量进行了评估和增强。该框架分为两个主要组成部分：评估器（Evaluator）和优化器（Optimizer）。评估器通过十个维度来评估代码质量，提供定量评分和定性总结，而优化器则根据评估器的反馈逐次改进代码。我们的研究证明，CodeQUEST能够有效地且稳健地评估代码质量，其评估结果与现有的代码质量指标高度一致。通过使用精心策划的Python和JavaScript示例数据集进行的一系列实验，CodeQUEST在代码质量方面取得了显著改善，平均相对百分比提高率为52.6%。框架的评估结果还与Pylint评分、Radon可维护性指数和Bandit输出日志等代理指标进行了验证，显示了显著的相关性。这突显了LLMs在自动化代码质量评估和改进过程方面的潜力，代表了改进软件开发实践的一个重要进展。该框架的代码实现可在以下链接获取：[this https URL]。 

---
# Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven Measurement Systems 

**Title (ZH)**: 面向多任务的夜间雾霾图像增强器在以视觉驱动的测量系统中的应用 

**Authors**: Ai Chen, Yuxu Lu, Dong Yang, Junlin Zhou, Yan Fu, Duanbing Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07351)  

**Abstract**: Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MToIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MToIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather/imaging conditions illustrate that MToIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at this https URL. 

**Abstract (ZH)**: 显著目标检测（SOD）在视觉驱动的测量系统（VMS）中发挥着重要作用，有助于图像中关键视觉元素的检测和分割。然而，不利的成像条件，如白天的雾气、低光照和夜晚的雾气严重地降低了图像质量，使SOD过程变得复杂。为应对这些挑战，我们提出了一种面向多任务的夜间雾气成像增强器（MToIE），该增强器整合了三个任务：白天去雾、低光照增强和夜间去雾。MToIE 包含了两个关键的创新组件：首先，网络采用了面向任务的节点学习机制来处理三种特定的退化类型：白天雾气、低光照和夜间雾气条件，并嵌入了一种自我注意模块，以增强其在夜间成像中的性能。此外，还包含了一个高效的多感受域增强模块，该模块通过三个具有不同膨胀率的并行深度可分离卷积分支来高效提取多尺度特征，从而以最小的计算开销捕捉全面的空间信息。为了确保图像重构质量和视觉特征的最佳性能，我们建议使用一种混合损失函数。在不同类型的天气/成像条件下进行的广泛实验表明，MToIE 超越了现有方法，在各种成像场景下显著提高了视觉系统的准确性和可靠性。完整的代码可在以下链接获取：[这里请插入具体的链接地址]。 

---
# Integrating Physics and Data-Driven Approaches: An Explainable and Uncertainty-Aware Hybrid Model for Wind Turbine Power Prediction 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，并确保符合学术规范：

"结合物理学和数据驱动方法：一种可解释且考虑不确定性混合模型的风力发电预测"

这个翻译不仅保持了原意，还符合学术论文中文标题的规范。 

**Authors**: Alfonso Gijón, Simone Eiraudo, Antonio Manjavacas, Daniele Salvatore Schiera, Miguel Molina-Solana, Juan Gómez-Romero  

**Link**: [PDF](https://arxiv.org/pdf/2502.07344)  

**Abstract**: The rapid growth of the wind energy sector underscores the urgent need to optimize turbine operations and ensure effective maintenance through early fault detection systems. While traditional empirical and physics-based models offer approximate predictions of power generation based on wind speed, they often fail to capture the complex, non-linear relationships between other input variables and the resulting power output. Data-driven machine learning methods present a promising avenue for improving wind turbine modeling by leveraging large datasets, enhancing prediction accuracy but often at the cost of interpretability. In this study, we propose a hybrid semi-parametric model that combines the strengths of both approaches, applied to a dataset from a wind farm with four turbines. The model integrates a physics-inspired submodel, providing a reasonable approximation of power generation, with a non-parametric submodel that predicts the residuals. This non-parametric submodel is trained on a broader range of variables to account for phenomena not captured by the physics-based component. The hybrid model achieves a 37% improvement in prediction accuracy over the physics-based model. To enhance interpretability, SHAP values are used to analyze the influence of input features on the residual submodel's output. Additionally, prediction uncertainties are quantified using a conformalized quantile regression method. The combination of these techniques, alongside the physics grounding of the parametric submodel, provides a flexible, accurate, and reliable framework. Ultimately, this study opens the door for evaluating the impact of unmodeled variables on wind turbine power generation, offering a basis for potential optimization. 

**Abstract (ZH)**: 风能sector的迅速增长突显了优化涡轮机运行并借助早期故障检测系统确保有效维护的紧迫需求。虽然传统的经验性和物理模型能够基于风速提供发电量的近似预测，但它们往往无法捕捉其他输入变量与最终发电量之间复杂的非线性关系。数据驱动的机器学习方法为通过大规模数据集改进风力涡轮机建模提供了有希望的途径，可以提高预测准确性，但成本往往是可解释性的降低。在本研究中，我们提出了一种混合半参数模型，该模型结合了两种方法的优势，应用于包含四台涡轮机的风场数据集。该模型将一个物理启发式子模型与一个非参数子模型结合在一起，前者提供了合理的发电量近似值，后者则预测残差。非参数子模型在更广泛的变量范围内进行训练，以考虑物理模型未能捕捉的现象。该混合模型在预测准确性方面比基于物理的模型提高了37%。为了增强可解释性，我们使用SHAP值分析输入特征对非参数子模型输出的影响。此外，通过共形化分位数回归方法量化了预测不确定性。结合这些技术，以参数子模型的物理基础，提供了一个灵活、准确且可靠的框架。最终，本研究为评估未建模变量对风力涡轮机发电量的影响打开了大门，并为潜在的优化提供了基础。 

---
# Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering 

**Title (ZH)**: 通过有效的数据过滤来使大规模语言模型更好地遵循指令并减少虚构内容生成 

**Authors**: Shuzheng Si, Haozhe Zhao, Gang Chen, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Kaikai An, Kangyang Luo, Chen Qian, Fanchao Qi, Baobao Chang, Maosong Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.07340)  

**Abstract**: Training LLMs on data that contains unfamiliar knowledge during the instruction tuning stage can make LLMs overconfident and encourage hallucinations. To address this challenge, we introduce a novel framework, NOVA, which identifies high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity to enhance data quality. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less. Extensive experiments and analysis show that NOVA significantly reduces hallucinations and allows LLMs to maintain a strong ability to follow instructions. 

**Abstract (ZH)**: 在指令调优阶段使用包含不熟悉知识的数据训练大语言模型可能会使大语言模型过于自信，并促进幻觉的产生。为应对这一挑战，我们提出了一种新的框架NOVA，该框架能够识别与大语言模型已学习的知识相一致的高质量数据，从而减少幻觉。NOVA包含内部一致性探针（ICP）和语义等价识别（SEI），用于衡量大语言模型对指令数据的熟悉程度。具体而言，ICP通过计算多个自我生成响应的定制一致性来评估大语言模型对给定指令的理解。SEI进一步通过将目标响应与生成的响应进行比较，使用提出的语义聚类和精心设计的投票策略来评估大语言模型对目标响应的熟悉程度。最后，我们引入了一种专家对齐的奖励模型，考虑超越熟悉度的其他特征以提高数据质量。通过考虑数据质量并避免使用不熟悉的数据，我们可以利用选择的数据有效地使大语言模型遵循指令并减少幻觉。广泛的实验和分析表明，NOVA显著减少了幻觉，并使大语言模型能够保持强指令跟随能力。 

---
# Music for All: Exploring Multicultural Representations in Music Generation Models (Camera Ready) 

**Title (ZH)**: 《音乐予所有人：探索音乐生成模型中的多元文化表现》（定稿） 

**Authors**: Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury  

**Link**: [PDF](https://arxiv.org/pdf/2502.07328)  

**Abstract**: The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning. 

**Abstract (ZH)**: 音乐语言模型的出现极大地提升了人工智能系统自动音乐生成的能力，但它们在涵盖世界各地的音乐流派和文化方面也存在不足。我们对音乐生成的数据集和研究论文进行了研究，并量化了流派的偏差和代表性不足。研究表明，现有音乐数据集中只有5.7%的内容来自非西方流派，这种情况自然导致了不同流派之间模型表现的差异。随后，我们探讨了参数高效调整（PEFT）技术在缓解这种偏差方面的有效性。针对两种未充分代表的非西方音乐传统——印地安那古典音乐和土耳其玛卡姆音乐，我们使用了两个流行模型——MusicGen和Mustango，实验结果显示，通过小数据集进行跨流派适应存在一定的可能性和挑战，这暗示了需要设计更多公平的底层音乐语言模型，以便于跨文化交流的学习。 

---
# CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction 

**Title (ZH)**: CodeI/O：通过代码输入-输出预测浓缩推理模式 

**Authors**: Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He  

**Link**: [PDF](https://arxiv.org/pdf/2502.07316)  

**Abstract**: Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at this https URL. 

**Abstract (ZH)**: 大规模语言模型具有一种基本的能力，即推理能力。尽管先前的研究主要集中在增强数学或代码生成等狭窄技能上，但由于可用训练数据稀疏且碎片化，提高在其他多种推理任务上的性能仍然具有挑战性。为了解决这一问题，我们提出了一种名为CodeI/O的新方法，该方法系统地将上下文关联的代码中固有的各种推理模式进行浓缩，通过将原始代码转换为代码的输入-输出预测格式来实现。通过训练模型在给定代码和测试案例的自然语言解释（Chain-of-Thought，简称CoT）中预测输入/输出，我们使其接触到普遍适用的推理基本原理——如逻辑流程规划、状态空间搜索、决策树遍历和模块化分解，同时将结构化推理与代码特定的语法分离，并保持程序上的严谨性。实验结果表明，CodeI/O在符号推理、科学推理、逻辑推理、数学与数字推理以及常识推理任务上均取得了持续的改进。通过匹配现有的真实输出或将预测输入重新执行代码来验证每个预测，并通过多轮修订进一步增强CoT，最终得到CodeI/O++并实现了更高的性能。我们的数据和模型可在以下网址获取：this https URL。 

---
# OpenGrok: Enhancing SNS Data Processing with Distilled Knowledge and Mask-like Mechanisms 

**Title (ZH)**: OpenGrok：通过提炼知识和掩码机制增强社交网络数据处理 

**Authors**: Lumen AI, Zaozhuang No.28 Middle School, Shihao Ji, Zihui Song, Fucheng Zhong, Jisen Jia, Zhaobo Wu, Zheyi Cao, Tianhao Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07312)  

**Abstract**: This report details Lumen Labs' novel approach to processing Social Networking Service (SNS) data. We leverage knowledge distillation, specifically a simple distillation method inspired by DeepSeek-R1's CoT acquisition, combined with prompt hacking, to extract valuable training data from the Grok model. This data is then used to fine-tune a Phi-3-mini model, augmented with a mask-like mechanism specifically designed for handling the nuances of SNS data. Our method demonstrates state-of-the-art (SOTA) performance on several SNS data processing tasks, outperforming existing models like Grok, Phi-3, and GPT-4. We provide a comprehensive analysis of our approach, including mathematical formulations, engineering details, ablation studies, and comparative evaluations. 

**Abstract (ZH)**: 本报告详细介绍了Lumen Labs在处理社交网络服务（SNS）数据方面的新颖方法。我们利用知识蒸馏，具体采用了一种受DeepSeek-R1的CoT获取启发的简单蒸馏方法，并结合提示工程，从Grok模型中提取有价值的训练数据。然后，我们使用这些数据对Phi-3-mini模型进行微调，并通过一种针对处理SNS数据细微差别特别设计的掩码机制对其进行增强。我们的方法在多个SNS数据处理任务上展示了目前最先进的（SOTA）性能，超越了现有的模型如Grok、Phi-3和GPT-4。我们全面分析了我们的方法，包括数学公式、工程细节、消融研究和对比评估。 

---
# TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation 

**Title (ZH)**: TRAVEL：无需训练的检索与对齐方法在视觉-语言导航中的应用 

**Authors**: Navid Rajabi, Jana Kosecka  

**Link**: [PDF](https://arxiv.org/pdf/2502.07306)  

**Abstract**: In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps \cite{vlmaps} on the complex R2R-Habitat \cite{r2r} instruction dataset and quantify in detail the effect of visual grounding on navigation performance. 

**Abstract (ZH)**: 在本文中，我们提出了一种模块化方法来解决视觉语言导航（VLN）任务，通过将问题分解为四个子模块，这些子模块在零样本设置中使用最先进的大型语言模型（LLMs）和视觉语言模型（VLMs）。给定用自然语言表示的导航指令，我们首先提示LLM提取地标及其访问顺序。假设环境的已知模型，我们检索最后一个地标附近的前k个位置，并使用环境拓扑图上的最短路径算法从起点到最后一个地标生成k条路径假设。每条路径假设由全景图序列表示。然后，我们使用动态规划来计算全景图序列与地标名称序列之间的对齐得分，该得分与VLM获得的匹配得分相匹配。最后，我们计算具有最高对齐得分的假设之间的nDTW度量，以评估路径的真实度。我们通过在复杂指令数据集R2R-Habitat \cite{r2r} 上与使用联合语义地图（如VLMaps \cite{vlmaps}）的其他方法进行比较，展示了优越的性能，并详细量化了视觉锚定对导航性能的影响。 

---
# Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification 

**Title (ZH)**: Life-Code：多组学序列统一下的中央狗ma模型化 

**Authors**: Zicheng Liu, Siyuan Li, Zhiyuan Chen, Lei Xin, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Stan Z. Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07299)  

**Abstract**: The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. While modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains under-explored. In this paper, we follow the guidance of the central dogma to redesign both the data and model pipeline and offer a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions of both coding and non-coding regions with masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive Experiments show that Life-Code achieves state-of-the-art performance on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation. 

**Abstract (ZH)**: DNA、RNA和蛋白质之间的相互作用是生物过程的基础，正如分子生物学中心法则所展示的那样。尽管现代生物预训练模型在单个分析这些大分子方面取得了巨大成功，但它们之间的相互关联尚未得到充分探索。在这项研究中，我们遵循中心法则的指导重新设计了数据和模型管道，并提供了一个涵盖不同生物功能的全面框架——Life-Code。对于数据流，我们提出了一种统一的流水线，通过逆转录RNA和逆转录氨基酸为核苷酸基序列来整合多组学数据。对于模型设计，我们设计了一个密码子分词器和一种混合的长序列架构，用于通过掩码建模预训练来编码编码和非编码区域之间的相互作用。为了建模翻译和折叠过程，Life-Code通过从现成的蛋白质语言模型中进行知识蒸馏来学习相应氨基酸的蛋白质结构。此类设计使Life-Code能够捕获基因序列内的复杂相互作用，从而为多组学分析和解释提供了更全面的理解。广泛的实验表明，Life-Code在三个组学的多种任务中达到了最先进的性能，突显了其促进多组学分析和解释的潜力。 

---
# KPIs 2024 Challenge: Advancing Glomerular Segmentation from Patch- to Slide-Level 

**Title (ZH)**: 2024年度KPI挑战：从patch级别到整张切片级别的肾小球分割技术进步 

**Authors**: Ruining Deng, Tianyuan Yao, Yucheng Tang, Junlin Guo, Siqi Lu, Juming Xiong, Lining Yu, Quan Huu Cap, Pengzhou Cai, Libin Lan, Ze Zhao, Adrian Galdran, Amit Kumar, Gunjan Deotale, Dev Kumar Das, Inyoung Paik, Joonho Lee, Geongyu Lee, Yujia Chen, Wangkai Li, Zhaoyang Li, Xuege Hou, Zeyuan Wu, Shengjin Wang, Maximilian Fischer, Lars Kramer, Anghong Du, Le Zhang, Maria Sanchez Sanchez, Helena Sanchez Ulloa, David Ribalta Heredia, Carlos Perez de Arenaza Garcia, Shuoyu Xu, Bingdou He, Xinping Cheng, Tao Wang, Noemie Moreau, Katarzyna Bozek, Shubham Innani, Ujjwal Baid, Kaura Solomon Kefas, Bennett A. Landman, Yu Wang, Shilin Zhao, Mengmeng Yin, Haichun Yang, Yuankai Huo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07288)  

**Abstract**: Chronic kidney disease (CKD) is a major global health issue, affecting over 10% of the population and causing significant mortality. While kidney biopsy remains the gold standard for CKD diagnosis and treatment, the lack of comprehensive benchmarks for kidney pathology segmentation hinders progress in the field. To address this, we organized the Kidney Pathology Image Segmentation (KPIs) Challenge, introducing a dataset that incorporates preclinical rodent models of CKD with over 10,000 annotated glomeruli from 60+ Periodic Acid Schiff (PAS)-stained whole slide images. The challenge includes two tasks, patch-level segmentation and whole slide image segmentation and detection, evaluated using the Dice Similarity Coefficient (DSC) and F1-score. By encouraging innovative segmentation methods that adapt to diverse CKD models and tissue conditions, the KPIs Challenge aims to advance kidney pathology analysis, establish new benchmarks, and enable precise, large-scale quantification for disease research and diagnosis. 

**Abstract (ZH)**: 慢性肾病（CKD）是全球公共卫生的一个重大问题，影响着超过10%的人口，并导致显著的死亡率。尽管肾活检仍然是CKD诊断和治疗的金标准，但由于缺乏全面的肾脏病理分割基准，阻碍了该领域的发展。为了解决这一问题，我们组织了肾脏病理图像分割（KPIs）挑战，并引入了一个包含预临床肾病模型的数据集，该数据集来自60多张过碘酸Schiff (PAS) 染色全张切片图像，并标注了超过10,000个肾小球。该挑战包括两个任务：像素级分割和全玻片图像分割与检测，并使用Dice相似系数（DSC）和F1分数进行评估。通过鼓励适用于多种CKD模型和组织条件的创新分割方法，KPIs挑战旨在推进肾脏病理分析，建立新的基准，并实现疾病研究和诊断中的精确、大规模量化。 

---
# Small Language Model Makes an Effective Long Text Extractor 

**Title (ZH)**: 小型语言模型成为有效的长文本提取器 

**Authors**: Yelin Chen, Fanjin Zhang, Jie Tang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07286)  

**Abstract**: Named Entity Recognition (NER) is a fundamental problem in natural language processing (NLP). However, the task of extracting longer entity spans (e.g., awards) from extended texts (e.g., homepages) is barely explored. Current NER methods predominantly fall into two categories: span-based methods and generation-based methods. Span-based methods require the enumeration of all possible token-pair spans, followed by classification on each span, resulting in substantial redundant computations and excessive GPU memory usage. In contrast, generation-based methods involve prompting or fine-tuning large language models (LLMs) to adapt to downstream NER tasks. However, these methods struggle with the accurate generation of longer spans and often incur significant time costs for effective fine-tuning. To address these challenges, this paper introduces a lightweight span-based NER method called SeNER, which incorporates a bidirectional arrow attention mechanism coupled with LogN-Scaling on the [CLS] token to embed long texts effectively, and comprises a novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to reduce redundant candidate token-pair spans significantly and model interactions between token-pair spans simultaneously. Extensive experiments demonstrate that our method achieves state-of-the-art extraction accuracy on three long NER datasets and is capable of extracting entities from long texts in a GPU-memory-friendly manner. Code: this https URL 

**Abstract (ZH)**: 命名实体识别（NER）是自然语言处理（NLP）中的一个基本问题。然而，从扩展文本（如主页）中提取较长实体跨度（例如奖项）的任务鲜有研究。当前的NER方法主要分为两类：基于跨度的方法和生成式方法。基于跨度的方法需要枚举所有可能的词对跨度，然后对每个跨度进行分类，导致了大量的冗余计算和过多的GPU内存使用。相比之下，生成式方法涉及提示或微调大型语言模型（LLMs）以适应下游的NER任务。但是，这些方法在准确生成较长跨度方面存在困难，并且在有效微调时常常耗费大量时间。为解决这些问题，本文提出了一种轻量级的基于跨度的NER方法——SeNER，该方法结合了双向箭头注意力机制和对[CLS]标记进行LogN-Scaling嵌入长文本的有效机制，并包含一种新颖的双向滑动窗口类似菱形注意力（BiSPA）机制，显著减少了冗余候选词对跨度，并同时建模词对跨度之间的交互。广泛的实验表明，我们的方法在三个长NER数据集上达到了最先进的抽取准确性，并且能够以GPU友好的方式从长文本中提取实体。代码：请参见此链接。

```
http://example.com/code
```

请注意将上述链接替换为实际代码库的URL。 

---
# MIGT: Memory Instance Gated Transformer Framework for Financial Portfolio Management 

**Title (ZH)**: MIGT：用于金融投资组合管理的记忆实例门控变压器框架 

**Authors**: Fengchen Gu, Angelos Stefanidis, Ángel García-Fernández, Jionglong Su, Huakang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07280)  

**Abstract**: Deep reinforcement learning (DRL) has been applied in financial portfolio management to improve returns in changing market conditions. However, unlike most fields where DRL is widely used, the stock market is more volatile and dynamic as it is affected by several factors such as global events and investor sentiment. Therefore, it remains a challenge to construct a DRL-based portfolio management framework with strong return capability, stable training, and generalization ability. This study introduces a new framework utilizing the Memory Instance Gated Transformer (MIGT) for effective portfolio management. By incorporating a novel Gated Instance Attention module, which combines a transformer variant, instance normalization, and a Lite Gate Unit, our approach aims to maximize investment returns while ensuring the learning process's stability and reducing outlier impacts. Tested on the Dow Jones Industrial Average 30, our framework's performance is evaluated against fifteen other strategies using key financial metrics like the cumulative return and risk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight MIGT's advantage, showcasing at least a 9.75% improvement in cumulative returns and a minimum 2.36% increase in risk-return ratios over competing strategies, marking a significant advancement in DRL for portfolio management. 

**Abstract (ZH)**: 深度强化学习（DRL）已在金融资产组合管理中应用，以在不断变化的市场条件下提高收益。然而，与大多数广泛使用DRL的领域不同，股市更加波动和动态，受到全球事件和投资者情绪等多种因素的影响。因此，构建一个具有高收益能力、稳定训练和良好泛化能力的基于DRL的资产组合管理框架仍然是一个挑战。本研究引入了一种新的框架，利用记忆实例门控变压器（MIGT）以实现有效的资产组合管理。通过引入一种新颖的门控实例注意模块，该模块结合了变换器变体、实例归一化和Lite门控单元，我们的方法旨在最大化投资回报率，同时确保学习过程的稳定性和减少异常值的影响。在对道琼斯工业平均指数30种成分股的测试中，我们的框架在累积回报和风险回报比率（夏普比率、索特诺比率和欧米茄比率）等关键金融指标上，与十五种其他策略进行了对比评估。结果显示，MIGT的优势明显，相比竞争对手的策略，在累积回报上至少提高了9.75%，在风险回报比率上至少提高了2.36%，这标志着DRL在资产组合管理中的一个重要进展。 

---
# Exploratory Diffusion Policy for Unsupervised Reinforcement Learning 

**Title (ZH)**: 探索性扩散策略用于无监督强化学习 

**Authors**: Chengyang Ying, Huayu Chen, Xinning Zhou, Zhongkai Hao, Hang Su, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07279)  

**Abstract**: Unsupervised reinforcement learning (RL) aims to pre-train agents by exploring states or skills in reward-free environments, facilitating the adaptation to downstream tasks. However, existing methods often overlook the fitting ability of pre-trained policies and struggle to handle the heterogeneous pre-training data, which are crucial for achieving efficient exploration and fast fine-tuning. To address this gap, we propose Exploratory Diffusion Policy (EDP), which leverages the strong expressive ability of diffusion models to fit the explored data, both boosting exploration and obtaining an efficient initialization for downstream tasks. Specifically, we estimate the distribution of collected data in the replay buffer with the diffusion policy and propose a score intrinsic reward, encouraging the agent to explore unseen states. For fine-tuning the pre-trained diffusion policy on downstream tasks, we provide both theoretical analyses and practical algorithms, including an alternating method of Q function optimization and diffusion policy distillation. Extensive experiments demonstrate the effectiveness of EDP in efficient exploration during pre-training and fast adaptation during fine-tuning. 

**Abstract (ZH)**: 无监督强化学习（RL）旨在通过探索奖励缺失环境中的状态或技能来预训练智能体，从而促进其适应下游任务。然而，现有的方法往往忽略了预训练策略的拟合能力，并且难以处理异质的预训练数据，这对于实现高效的探索和快速微调至关重要。为解决这一问题，我们提出了一种探索扩散策略（Exploratory Diffusion Policy，EDP），该方法利用扩散模型的强大表达能力来拟合探索数据，既增强了探索能力，又获得了适用于下游任务的有效初始化。具体而言，我们使用扩散策略估计回放缓冲区中收集数据的概率分布，并提出了一种评分内在奖励，鼓励智能体探索未见过的状态。为了在下游任务上微调预训练的扩散策略，我们提供了理论分析和实际算法，包括Q函数优化和扩散策略蒸馏的交替方法。广泛实验表明，EDP 在预训练和微调期间均能有效探索和快速适应。 

---
# Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal Analysis 

**Title (ZH)**: 提升视频理解：用于时空分析的深度神经网络 

**Authors**: Amir Hosein Fadaei, Mohammad-Reza A. Dehaqani  

**Link**: [PDF](https://arxiv.org/pdf/2502.07277)  

**Abstract**: It's no secret that video has become the primary way we share information online. That's why there's been a surge in demand for algorithms that can analyze and understand video content. It's a trend going to continue as video continues to dominate the digital landscape. These algorithms will extract and classify related features from the video and will use them to describe the events and objects in the video. Deep neural networks have displayed encouraging outcomes in the realm of feature extraction and video description. This paper will explore the spatiotemporal features found in videos and recent advancements in deep neural networks in video understanding. We will review some of the main trends in video understanding models and their structural design, the main problems, and some offered solutions in this topic. We will also review and compare significant video understanding and action recognition datasets. 

**Abstract (ZH)**: 视频已经成为我们在线分享信息的主要方式，这使得人们对能够分析和理解视频内容的算法产生了巨大的需求。而随着视频继续主导数字领域，这种趋势将会持续下去。这些算法将从视频中提取和分类相关特征，并利用这些特征描述视频中的事件和对象。深度神经网络在特征提取和视频描述领域展现出了令人鼓舞的结果。本文将探讨视频中时空特征及其在视频理解中的最新进展。我们将回顾视频理解模型的主要趋势及其结构设计、主要问题和一些解决方案。此外，我们还将回顾并比较一些重要的视频理解和动作识别数据集。 

---
# Dataset Ownership Verification in Contrastive Pre-trained Models 

**Title (ZH)**: 对比预训练模型中的数据集所有权验证 

**Authors**: Yuechen Xie, Jie Song, Mengqi Xue, Haofei Zhang, Xingen Wang, Bingde Hu, Genlang Chen, Mingli Song  

**Link**: [PDF](https://arxiv.org/pdf/2502.07276)  

**Abstract**: High-quality open-source datasets, which necessitate substantial efforts for curation, has become the primary catalyst for the swift progress of deep learning. Concurrently, protecting these datasets is paramount for the well-being of the data owner. Dataset ownership verification emerges as a crucial method in this domain, but existing approaches are often limited to supervised models and cannot be directly extended to increasingly popular unsupervised pre-trained models. In this work, we propose the first dataset ownership verification method tailored specifically for self-supervised pre-trained models by contrastive learning. Its primary objective is to ascertain whether a suspicious black-box backbone has been pre-trained on a specific unlabeled dataset, aiding dataset owners in upholding their rights. The proposed approach is motivated by our empirical insights that when models are trained with the target dataset, the unary and binary instance relationships within the embedding space exhibit significant variations compared to models trained without the target dataset. We validate the efficacy of this approach across multiple contrastive pre-trained models including SimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our method rejects the null hypothesis with a $p$-value markedly below $0.05$, surpassing all previous methodologies. Our code is available at this https URL. 

**Abstract (ZH)**: 高质量的开源数据集需要大量的整理工作，已成为深度学习快速进步的主要推动力。与此同时，保护这些数据集对于数据所有者的福祉至关重要。数据集所有权验证在此领域中变得尤为重要，但现有方法往往局限于监督模型，无法直接扩展到越来越流行的无监督预训练模型。在本项工作中，我们提出了首个针对自监督预训练模型的基于对比学习的数据集所有权验证方法。该方法的主要目标是确定可疑的黑盒骨干模型是否是在特定未标记数据集上预训练的，从而帮助数据所有者维护其权益。我们提出的这种方法受到以下实证洞察的启发：当模型使用目标数据集进行训练时，嵌入空间中的单个实例和二元实例关系与未使用目标数据集训练的模型相比存在显著差异。我们在包括SimCLR、BYOL、SimSiam、MOCO v3和DINO在内的多个对比预训练模型中验证了该方法的有效性。结果显示，我们的方法在$p$值远低于$0.05$的情况下拒绝了零假设，超过了所有先前的方法。我们的代码已发布在以下链接：[此处的URL]。 

---
# Cost-Efficient Continual Learning with Sufficient Exemplar Memory 

**Title (ZH)**: 经济高效的持续学习方法：具备充足示例记忆 

**Authors**: Dongkyu Cho, Taesup Moon, Rumi Chunara, Kyunghyun Cho, Sungmin Cha  

**Link**: [PDF](https://arxiv.org/pdf/2502.07274)  

**Abstract**: Continual learning (CL) research typically assumes highly constrained exemplar memory resources. However, in many real-world scenarios-especially in the era of large foundation models-memory is abundant, while GPU computational costs are the primary bottleneck. In this work, we investigate CL in a novel setting where exemplar memory is ample (i.e., sufficient exemplar memory). Unlike prior methods designed for strict exemplar memory constraints, we propose a simple yet effective approach that directly operates in the model's weight space through a combination of weight resetting and averaging techniques. Our method achieves state-of-the-art performance while reducing the computational cost to a quarter or third of existing methods. These findings challenge conventional CL assumptions and provide a practical baseline for computationally efficient CL applications. 

**Abstract (ZH)**: 在持续学习（CL）研究中，通常假设示例记忆资源受到严格限制。然而，在许多现实场景中，尤其是在大型基础模型时代，内存资源丰富，而GPU计算成本成为主要瓶颈。在本研究中，我们探索了一种新的CL场景，其中示例记忆资源充足（即，足够大的示例记忆）。与针对严格示例记忆限制而设计的方法不同，我们提出了一种简单而有效的方法，该方法直接在模型的权重空间中通过重置权重和平均技术的结合来进行操作。我们的方法不仅实现了最先进的性能，还将计算成本降低到现有方法的四分之一或三分之一。这些发现挑战了传统的CL假设，并为计算效率高的CL应用提供了实际的基准。 

---
# Variational Learning Induces Adaptive Label Smoothing 

**Title (ZH)**: 变分学习诱导自适应标签平滑 

**Authors**: Sin-Han Yang, Zhedong Liu, Gian Maria Marconi, Mohammad Emtiyaz Khan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07273)  

**Abstract**: We show that variational learning naturally induces an adaptive label smoothing where label noise is specialized for each example. Such label-smoothing is useful to handle examples with labeling errors and distribution shifts, but designing a good adaptivity strategy is not always easy. We propose to skip this step and simply use the natural adaptivity induced during the optimization of a variational objective. We show empirical results where a variational algorithm called IVON outperforms traditional label smoothing and yields adaptivity strategies similar to those of an existing approach. By connecting Bayesian methods to label smoothing, our work provides a new way to handle overconfident predictions. 

**Abstract (ZH)**: 我们展示了变分学习自然诱导了一种自适应标签平滑，其中标签噪声专门针对每个示例。这种标签平滑在处理带有标注错误和分布偏移的示例时很有用，但设计一个有效的自适应策略并不总是容易的。我们提出跳过这一步，而是简单地使用优化变分目标期间自然诱导的自适应性。我们展示了实验结果，表明一种称为IVON的变分算法在传统标签平滑方法上表现更好，并且产生的自适应策略类似于现有方法的策略。通过将贝叶斯方法与标签平滑联系起来，我们的工作为处理过于自信的预测提供了一种新的方法。 

---
# Fairness in Multi-Agent AI: A Unified Framework for Ethical and Equitable Autonomous Systems 

**Title (ZH)**: 多agent人工智能中的公正性：一个综合框架以确保自主系统的伦理性和公平性 

**Authors**: Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07254)  

**Abstract**: Ensuring fairness in decentralized multi-agent systems presents significant challenges due to emergent biases, systemic inefficiencies, and conflicting agent incentives. This paper provides a comprehensive survey of fairness in multi-agent AI, introducing a novel framework where fairness is treated as a dynamic, emergent property of agent interactions. The framework integrates fairness constraints, bias mitigation strategies, and incentive mechanisms to align autonomous agent behaviors with societal values while balancing efficiency and robustness. Through empirical validation, we demonstrate that incorporating fairness constraints results in more equitable decision-making. This work bridges the gap between AI ethics and system design, offering a foundation for accountable, transparent, and socially responsible multi-agent AI systems. 

**Abstract (ZH)**: 确保在去中心化的多智能体系统中实现公平性面临着巨大的挑战，原因包括新兴的偏见、系统性的低效以及智能体间相互矛盾的激励机制。本文提供了一个全面的多智能体AI中的公平性综述，介绍了一个全新的框架，其中公平性被视为智能体互动中的一种动态、 emergent 属性。该框架整合了公平性约束、偏见缓解策略以及激励机制，以使自主智能体的行为与社会价值相一致，同时平衡效率与鲁棒性。通过实证验证，我们证明了纳入公平性约束可以实现更公平的决策制定。本研究填补了AI伦理与系统设计之间的空白，为可问责、透明和社会责任性的多智能体AI系统提供了基础。 

---
# NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online Complex Event Detection 

**Title (ZH)**: NARCE：基于Mamba的神经算法推理框架，用于在线复杂事件检测 

**Authors**: Liying Han, Gaofeng Dong, Xiaomin Ouyang, Lance Kaplan, Federico Cerutti, Mani Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2502.07250)  

**Abstract**: Current machine learning models excel in short-span perception tasks but struggle to derive high-level insights from long-term observation, a capability central to understanding complex events (CEs). CEs, defined as sequences of short-term atomic events (AEs) governed by spatiotemporal rules, are challenging to detect online due to the need to extract meaningful patterns from long and noisy sensor data while ignoring irrelevant events. We hypothesize that state-based methods are well-suited for CE detection, as they capture event progression through state transitions without requiring long-term memory. Baseline experiments validate this, demonstrating that the state-space model Mamba outperforms existing architectures. However, Mamba's reliance on extensive labeled data, which are difficult to obtain, motivates our second hypothesis: decoupling CE rule learning from noisy sensor data can reduce data requirements. To address this, we propose NARCE, a framework that combines Neural Algorithmic Reasoning (NAR) to split the task into two components: (i) learning CE rules independently of sensor data using synthetic concept traces generated by LLMs and (ii) mapping sensor inputs to these rules via an adapter. Our results show that NARCE outperforms baselines in accuracy, generalization to unseen and longer sensor data, and data efficiency, significantly reducing annotation costs while advancing robust CE detection. 

**Abstract (ZH)**: 当前的机器学习模型在短跨度感知任务中表现出色，但在从长期观察中推导高层次见解方面却表现不佳，这种能力对于理解复杂事件（CEs）至关重要。CEs被定义为由时空规则支配的短时原子事件（AEs）序列。由于需要从长且嘈杂的传感器数据中提取有意义的模式并忽略无关事件，因此实时检测CEs具有挑战性。我们假设基于状态的方法适用于CE检测，因为它们通过状态转换捕捉事件的发展，无需长期记忆。基准实验验证了这一点，表明状态空间模型Mamba优于现有架构。然而，Mamba依赖于大量标注数据，而这些数据难以获得，这促使我们提出第二条假设：将CE规则学习与嘈杂的传感器数据脱钩可以减少数据需求。为了解决这一问题，我们提出了一种名为NARCE的框架，该框架结合了神经算法推理（NAR），将任务分为两个部分：（i）使用大型语言模型（LLMs）生成的合成概念轨迹独立于传感器数据学习CE规则，（ii）通过适配器将传感器输入映射到这些规则。我们的结果表明，NARCE在准确性、对未见过的和更长的传感器数据的泛化能力和数据效率方面均优于基准模型，显著降低了注释成本并推进了稳健的CE检测技术的发展。 

---
# Linear Transformers as VAR Models: Aligning Autoregressive Attention Mechanisms with Autoregressive Forecasting 

**Title (ZH)**: 将线性变压器视为向量自回归模型：使自回归注意力机制与自回归预测相一致 

**Authors**: Jiecheng Lu, Shihao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07244)  

**Abstract**: Autoregressive attention-based time series forecasting (TSF) has drawn increasing interest, with mechanisms like linear attention sometimes outperforming vanilla attention. However, deeper Transformer architectures frequently misalign with autoregressive objectives, obscuring the underlying VAR structure embedded within linear attention and hindering their ability to capture the data generative processes in TSF. In this work, we first show that a single linear attention layer can be interpreted as a dynamic vector autoregressive (VAR) structure. We then explain that existing multi-layer Transformers have structural mismatches with the autoregressive forecasting objective, which impair interpretability and generalization ability. To address this, we show that by rearranging the MLP, attention, and input-output flow, multi-layer linear attention can also be aligned as a VAR model. Then, we propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer variant that integrates interpretable dynamic VAR weights for multivariate TSF. By aligning the Transformer architecture with autoregressive objectives, SAMoVAR delivers improved performance, interpretability, and computational efficiency, comparing to SOTA TSF models. 

**Abstract (ZH)**: 自回归基于注意机制的时间序列预测（TSF）近年来越来越受到关注，其中线性注意机制有时甚至优于普通注意机制。然而，更深层次的Transformer架构经常与自回归目标不一致，遮蔽了嵌入在线性注意机制中的潜在VAR结构，并妨碍它们捕捉TSF中的数据生成过程。本文首先展示了单一层线性注意机制可以被解释为一个动态向量自回归（VAR）结构。然后，我们解释了现有的多层Transformer与自回归预测目标之间存在结构不匹配，这影响了模型的可解释性和泛化能力。为了解决这个问题，我们展示了通过重新安排MLP、注意机制和输入输出流，多层线性注意机制也可以被调整为VAR模型。随后，我们提出了一种名为结构对齐VAR混合的线性Transformer变体（SAMoVAR），它为多变量TSF整合了可解释的动态VAR权重。通过将Transformer架构与自回归目标对齐，SAMoVAR在性能、可解释性和计算效率方面都优于最新的TSF模型。 

---
# Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement 

**Title (ZH)**: Vevo：可控的零样本声音模仿与自我监督分解 

**Authors**: Xueyao Zhang, Xiaohui Zhang, Kainan Peng, Zhenyu Tang, Vimal Manohar, Yingru Liu, Jeff Hwang, Dangna Li, Yuhao Wang, Julian Chan, Yuan Huang, Zhizheng Wu, Mingbo Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.07243)  

**Abstract**: The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation. However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable generation, especially in zero-shot scenarios. To address these issues, we propose Vevo, a versatile zero-shot voice imitation framework with controllable timbre and style. Vevo operates in two core stages: (1) Content-Style Modeling: Given either text or speech's content tokens as input, we utilize an autoregressive transformer to generate the content-style tokens, which is prompted by a style reference; (2) Acoustic Modeling: Given the content-style tokens as input, we employ a flow-matching transformer to produce acoustic representations, which is prompted by a timbre reference. To obtain the content and content-style tokens of speech, we design a fully self-supervised approach that progressively decouples the timbre, style, and linguistic content of speech. Specifically, we adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We treat the vocabulary size of the VQ-VAE codebook as the information bottleneck, and adjust it carefully to obtain the disentangled speech representations. Solely self-supervised trained on 60K hours of audiobook speech data, without any fine-tuning on style-specific corpora, Vevo matches or surpasses existing methods in accent and emotion conversion tasks. Additionally, Vevo's effectiveness in zero-shot voice conversion and text-to-speech tasks further demonstrates its strong generalization and versatility. Audio samples are available at this https URL. 

**Abstract (ZH)**: 语音的模仿，特别是在音色和发音风格等特定语音属性方面的模仿，是语音生成中至关重要的。然而，现有的方法依赖于标注数据，并且在有效分离音色和风格方面存在困难，这导致了在可控生成特别是在零样本场景中实现可控生成的挑战。为了应对这些问题，我们提出了一种具有可控音色和风格的通用零样本语音模仿框架Vevo。Vevo在两个核心阶段运行：(1) 内容-风格建模：给定文本或语音的内容标记作为输入，我们使用自回归变压器生成由风格参考提示的内容-风格标记；(2) 声学建模：给定内容-风格标记作为输入，我们采用流匹配变压器生成声学表示，该表示由音色参考进行提示。为了获取语音的内容和内容-风格标记，我们设计了一种完全自监督的方法，逐步分离语音的音色、风格和语言内容。具体来说，我们采用VQ-VAE作为HuBERT连续隐藏特征的分词器。我们将VQ-VAE码本的词汇量视为信息瓶颈，并仔细调整它以获得分离的语音表示。仅在60,000小时的有声书语音数据上自我 supervision 训练，而无需对特定风格的语料进行微调，Vevo在口音和情感转换任务中与现有方法相当甚至超越了它们。此外，Vevo在零样本语音转换和文本转语音任务中的效果进一步证明了它的强大适应性和通用性。更多音频样本请参阅此链接：[音频样本链接]。 

---
# Contextual Gesture: Co-Speech Gesture Video Generation through Context-aware Gesture Representation 

**Title (ZH)**: 上下文手势：通过上下文意识手势表示的同步手势视频生成 

**Authors**: Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Sharpio, Kyle Olszewski  

**Link**: [PDF](https://arxiv.org/pdf/2502.07239)  

**Abstract**: Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech. Despite recent advancements, existing methods struggle with accurately identifying the rhythmic or semantic triggers from audio for generating contextualized gesture patterns and achieving pixel-level realism. To address these challenges, we introduce Contextual Gesture, a framework that improves co-speech gesture video generation through three innovative components: (1) a chronological speech-gesture alignment that temporally connects two modalities, (2) a contextualized gesture tokenization that incorporate speech context into motion pattern representation through distillation, and (3) a structure-aware refinement module that employs edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Contextual Gesture not only produces realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications, shown in Fig.1 Project Page: this https URL. 

**Abstract (ZH)**: 同步手势生成对于创建逼真的人像和增强人机交互至关重要，它能通过手势与言语的同步来实现这一目标。尽管最近取得了进展，现有的方法在从音频中准确识别节奏或语义触发因素以生成上下文相关的手势模式并实现像素级逼真度方面仍存在挑战。为了解决这些问题，我们提出了Contextual Gesture框架，该框架通过三个创新组件提升了同步手势视频生成的效果：（1）时间顺序的语音-手势对齐，它在两个模态之间建立时间连接；（2）上下文手势标记，通过蒸馏将语音上下文整合到运动模式表示中；（3）结构感知的细化模块，利用边缘连接将手势关键点连接起来，以改进视频生成。我们的广泛实验表明，Contextual Gesture不仅能生成逼真且与语音对齐的手势视频，还能支持长序列生成和视频手势编辑应用，如图1所示。项目页面：请参阅此链接: [具体链接] 

---
# Diffusion Suction Grasping with Large-Scale Parcel Dataset 

**Title (ZH)**: 大规模包裹数据集下的扩散吸附抓取方法 

**Authors**: Ding-Tao Huang, Xinyi He, Debei Hua, Dongfang Yu, En-Te Lin, Long Zeng  

**Link**: [PDF](https://arxiv.org/pdf/2502.07238)  

**Abstract**: While recent advances in object suction grasping have shown remarkable progress, significant challenges persist particularly in cluttered and complex parcel handling scenarios. Two fundamental limitations hinder current approaches: (1) the lack of a comprehensive suction grasp dataset tailored for parcel manipulation tasks, and (2) insufficient adaptability to diverse object characteristics including size variations, geometric complexity, and textural diversity. To address these challenges, we present Parcel-Suction-Dataset, a large-scale synthetic dataset containing 25 thousand cluttered scenes with 410 million precision-annotated suction grasp poses. This dataset is generated through our novel geometric sampling algorithm that enables efficient generation of optimal suction grasps incorporating both physical constraints and material properties. We further propose Diffusion-Suction, an innovative framework that reformulates suction grasp prediction as a conditional generation task through denoising diffusion probabilistic models. Our method iteratively refines random noise into suction grasp score maps through visual-conditioned guidance from point cloud observations, effectively learning spatial point-wise affordances from our synthetic dataset. Extensive experiments demonstrate that the simple yet efficient Diffusion-Suction achieves new state-of-the-art performance compared to previous models on both Parcel-Suction-Dataset and the public SuctionNet-1Billion benchmark. 

**Abstract (ZH)**: 尽管近年来物体吸附抓取技术取得了显著进展，但在杂乱和复杂的包裹处理场景中仍面临重大挑战。当前方法的两大根本限制是：（1）缺乏专门针对包裹操作任务的全面吸附抓取数据集；（2）对不同物体特性的适应性不足，包括尺寸差异、几何复杂性和纹理多样性。为了解决这些挑战，我们提出了包裹吸附数据集（Parcel-Suction-Dataset），这是一个包含25000个杂乱场景的大规模合成数据集，包含了41亿个精细标注的吸附抓取姿态。该数据集通过我们的新型几何采样算法生成，能够高效生成同时考虑物理约束和材料属性的最优吸附抓取。我们进一步提出了扩散吸附（Diffusion-Suction）框架，通过去噪扩散概率模型将吸附抓取预测重新表述为条件生成任务。该方法通过点云观测的视觉条件引导逐步细化随机噪声成吸附抓取评分图，有效地从合成数据集中学习空间点的 affordance。大量实验表明，简单而高效的 Diffusion-Suction 在包裹吸附数据集和公开的 SuctionNet-1Billion 基准上的性能均优于以前的方法。 

---
# LUNAR: LLM Unlearning via Neural Activation Redirection 

**Title (ZH)**: LUNAR：通过神经激活重定向实现Large Language Model的遗忘 

**Authors**: William F. Shen, Xinchi Qiu, Meghdad Kurmanji, Alex Iacob, Lorenzo Sani, Yihong Chen, Nicola Cancedda, Nicholas D. Lane  

**Link**: [PDF](https://arxiv.org/pdf/2502.07218)  

**Abstract**: Large Language Models (LLMs) benefit from training on ever larger amounts of textual data, but as a result, they increasingly incur the risk of leaking private information. The ability to selectively remove knowledge from LLMs is, therefore, a highly desirable capability. In this paper, we propose LUNAR, a novel unlearning methodology grounded in the Linear Representation Hypothesis. LUNAR operates by redirecting the representations of unlearned data to regions that trigger the model's inherent ability to express its inability to answer. LUNAR achieves state-of-the-art unlearning performance while significantly enhancing the controllability of the unlearned model during inference. Specifically, LUNAR achieves between 2.9x to 11.7x improvements on combined "unlearning efficacy" and "model utility" score ("Deviation Score") on the PISTOL dataset across various base models. We also demonstrate, through quantitative analysis and qualitative examples, LUNAR's superior controllability in generating coherent and contextually aware responses, mitigating undesired side effects of existing methods. Moreover, we demonstrate that LUNAR is robust against white-box adversarial attacks and versatile in handling real-world scenarios, such as processing sequential unlearning requests. 

**Abstract (ZH)**: 以下是经过学术规范翻译后的中文内容：

大型语言模型（LLMs）从不断增多的文本数据训练中受益，但随之而来的是泄露隐私信息的潜在风险越来越高。因此，能够有选择性地从LLMs中删除知识的能力变得尤为宝贵。本文提出了一种名为LUNAR的新颖遗忘方法，该方法基于线性表示假设。LUNAR通过将未学习数据的表示重定向到触发模型内在无法回答能力的区域来运作。LUNAR在PISTOL数据集上实现了最先进的遗忘性能，并显著增强了推理过程中未学习模型的可控性。具体而言，LUNAR在不同基模型上分别在联合的“遗忘有效性”与“模型可用性”得分（“偏差得分”）方面取得了2.9至11.7倍的改进。我们还通过定量分析和定性示例展示了LUNAR在生成连贯且上下文相关响应方面的优越可控性，减少了现有方法的不良副作用。此外，我们证明了LUNAR对白盒对抗攻击具有鲁棒性，并且能够灵活处理现实世界场景，例如处理顺序遗忘请求。 

---
# SparseFormer: Detecting Objects in HRW Shots via Sparse Vision Transformer 

**Title (ZH)**: SparseFormer：通过稀疏视觉变换器检测HRW镜头中的物体 

**Authors**: Wenxi Li, Yuchen Guo, Jilai Zheng, Haozhe Lin, Chao Ma, Lu Fang, Xiaokang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07216)  

**Abstract**: Recent years have seen an increase in the use of gigapixel-level image and video capture systems and benchmarks with high-resolution wide (HRW) shots. However, unlike close-up shots in the MS COCO dataset, the higher resolution and wider field of view raise unique challenges, such as extreme sparsity and huge scale changes, causing existing close-up detectors inaccuracy and inefficiency. In this paper, we present a novel model-agnostic sparse vision transformer, dubbed SparseFormer, to bridge the gap of object detection between close-up and HRW shots. The proposed SparseFormer selectively uses attentive tokens to scrutinize the sparsely distributed windows that may contain objects. In this way, it can jointly explore global and local attention by fusing coarse- and fine-grained features to handle huge scale changes. SparseFormer also benefits from a novel Cross-slice non-maximum suppression (C-NMS) algorithm to precisely localize objects from noisy windows and a simple yet effective multi-scale strategy to improve accuracy. Extensive experiments on two HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed SparseFormer significantly improves detection accuracy (up to 5.8%) and speed (up to 3x) over the state-of-the-art approaches. 

**Abstract (ZH)**: 近年来，高分辨率宽视场（HRW）图像和视频的捕获系统及其基准测试（特别是在千像素级）得到了广泛应用。然而，这些高分辨率和宽视角的图像和视频在检测对象时带来了不同于MS COCO数据集中近景照片的特殊挑战，如极端稀疏性和巨大的尺度变化，导致现有近景检测器在精度和效率上存在问题。在本文中，我们提出了一种新型的模型无关稀疏视觉变换器——SparseFormer，旨在弥合近景和HRW图像之间的检测差距。SparseFormer通过选择性地使用注意力令牌来审查可能包含对象的稀疏分布窗口，从而能够同时探索全局和局部注意力，通过结合粗粒度和细粒度特征来应对巨大的尺度变化。SparseFormer还受益于一种新颖的跨切片非极大值抑制（C-NMS）算法，能够从噪声窗口中精确地定位对象，并采用一种简单而有效的多尺度策略以提高检测精度。在两个HRW基准测试集PANDA和DOTA-v1.0上的广泛实验表明，与现有最佳方法相比，提出的SparseFormer在检测精度（最多提高5.8%）和速度（最多提高3倍）上均取得了显著改进。 

---
# Pareto Optimal Algorithmic Recourse in Multi-cost Function 

**Title (ZH)**: 多目标函数下的帕累托最优算法干预 

**Authors**: Wen-Ling Chen, Hong-Chang Huang, Kai-Hung Lin, Shang-Wei Hwang, Hao-Tsung Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07214)  

**Abstract**: In decision-making systems, algorithmic recourse aims to identify minimal-cost actions to alter an individual features, thereby obtaining a desired outcome. This empowers individuals to understand, question, or alter decisions that negatively affect them. However, due to the variety and sensitivity of system environments and individual personalities, quantifying the cost of a single function is nearly impossible while considering multiple criteria situations. Most current recourse mechanisms use gradient-based methods that assume cost functions are differentiable, often not applicable in real-world scenarios, resulting in sub-optimal solutions that compromise various criteria. These solutions are typically intractable and lack rigorous theoretical foundations, raising concerns regarding interpretability, reliability, and transparency from the explainable AI (XAI) perspective.
To address these issues, this work proposes an algorithmic recourse framework that handles non-differentiable and discrete multi-cost functions. By formulating recourse as a multi-objective optimization problem and assigning weights to different criteria based on their importance, our method identifies Pareto optimal recourse recommendations. To demonstrate scalability, we incorporate the concept of epsilon-net, proving the ability to find approximated Pareto optimal actions. Experiments show the trade-off between different criteria and the methods scalability in large graphs. Compared to current heuristic practices, our approach provides a stronger theoretical foundation and better aligns recourse suggestions with real-world requirements. 

**Abstract (ZH)**: 在决策系统中，算法反躬溯源旨在识别最小成本的动作以改变个体特征，从而获得期望的结果。这使个体能够理解、质疑或改变对其产生负面影响的决策。然而，由于系统环境和个体性格的多样性和敏感性，考虑到多个标准的情况时，量化单一函数的成本几乎是不可能的。目前大多数反躬溯源机制使用基于梯度的方法，假设成本函数是可微的，这在很多实际场景中并不适用，导致多种标准下的次优解决方案，从而在多个标准之间妥协。这些解决方案通常难以解决，并缺乏严格的理论基础，从可解释人工智能（XAI）的角度来看，引发了关于可解释性、可靠性和透明度的担忧。

为了解决这些问题，本研究提出了一种处理非可微和离散多成本函数的算法反躬溯源框架。通过将反躬溯源问题形式化为多目标优化问题，并根据各个标准的重要性赋予不同的权重，我们的方法能识别帕累托最优反躬溯源建议。为了展示其可扩展性，我们引入了ε-网的概念，证明了能够找到近似的帕累托最优动作。实验表明在不同标准之间的权衡以及该方法在大规模图中的可扩展性。与现有的启发式方法相比，我们的方法提供了更强的理论基础，更好地适应了实际情况的需求。 

---
# Evaluation for Regression Analyses on Evolving Data Streams 

**Title (ZH)**: 对演进数据流上的回归分析进行评估 

**Authors**: Yibin Sun, Heitor Murilo Gomes, Bernhard Pfahringer, Albert Bifet  

**Link**: [PDF](https://arxiv.org/pdf/2502.07213)  

**Abstract**: The paper explores the challenges of regression analysis in evolving data streams, an area that remains relatively underexplored compared to classification. We propose a standardized evaluation process for regression and prediction interval tasks in streaming contexts. Additionally, we introduce an innovative drift simulation strategy capable of synthesizing various drift types, including the less-studied incremental drift. Comprehensive experiments with state-of-the-art methods, conducted under the proposed process, validate the effectiveness and robustness of our approach. 

**Abstract (ZH)**: 本文探讨了在演变数据流中回归分析面临的挑战，这一领域相对于分类而言仍相对未被充分探索。我们提出了一种标准化的评估流程，用于流式环境中回归和预测区间任务的评估。此外，我们引入了一种创新的漂移仿真策略，能够合成各种类型的漂移，包括较少研究的增量漂移。在提出的方法下，对最先进的方法进行的全面实验验证了我们方法的有效性和鲁棒性。 

---
# A Study on the Importance of Features in Detecting Advanced Persistent Threats Using Machine Learning 

**Title (ZH)**: 使用机器学习检测高级持续威胁中特征的重要性研究 

**Authors**: Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif  

**Link**: [PDF](https://arxiv.org/pdf/2502.07207)  

**Abstract**: Advanced Persistent Threats (APTs) pose a significant security risk to organizations and industries. These attacks often lead to severe data breaches and compromise the system for a long time. Mitigating these sophisticated attacks is highly challenging due to the stealthy and persistent nature of APTs. Machine learning models are often employed to tackle this challenge by bringing automation and scalability to APT detection. Nevertheless, these intelligent methods are data-driven, and thus, highly affected by the quality and relevance of input data. This paper aims to analyze measurements considered when recording network traffic and conclude which features contribute more to detecting APT samples. To do this, we study the features associated with various APT cases and determine their importance using a machine learning framework. To ensure the generalization of our findings, several feature selection techniques are employed and paired with different classifiers to evaluate their effectiveness. Our findings provide insights into how APT detection can be enhanced in real-world scenarios. 

**Abstract (ZH)**: 高级持续性威胁（APTs）对组织和行业构成了重大的安全风险。这些攻击通常导致严重的数据泄露，并且能够持续长时间地控制系统。由于APTs的隐蔽性和持久性，减轻这些复杂的攻击极具挑战性。机器学习模型常常被用来应对这一挑战，通过自动化和扩展性来增强APTs检测。不过，这些智能方法依赖于输入数据的质量和相关性，因此极易受到影响。本文旨在分析记录网络流量时所考虑的测量指标，并确定哪些特征对检测APTs样本更为重要。为此，我们研究了与各种APTs案例相关的特征，并通过机器学习框架来确定其重要性。为了确保研究结果的普适性，我们采用了多种特征选择技术，并与不同的分类器配对，以评估它们的有效性。我们的研究结果为在实际场景中如何增强APTs检测提供了宝贵的见解。 

---
# VINP: Variational Bayesian Inference with Neural Speech Prior for Joint ASR-Effective Speech Dereverberation and Blind RIR Identification 

**Title (ZH)**: VINP：带有神经语音先验的变分贝叶斯推断在联合ASR-有效语音降混响和盲RIR识别中的应用 

**Authors**: Pengyu Wang, Ying Fang, Xiaofei Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07205)  

**Abstract**: Reverberant speech, denoting the speech signal degraded by the process of reverberation, contains crucial knowledge of both anechoic source speech and room impulse response (RIR). This work proposes a variational Bayesian inference (VBI) framework with neural speech prior (VINP) for joint speech dereverberation and blind RIR identification. In VINP, a probabilistic signal model is constructed in the time-frequency (T-F) domain based on convolution transfer function (CTF) approximation. For the first time, we propose using an arbitrary discriminative dereverberation deep neural network (DNN) to predict the prior distribution of anechoic speech within a probabilistic model. By integrating both reverberant speech and the anechoic speech prior, VINP yields the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the anechoic speech spectrum and CTF filter, respectively. After simple transformations, the waveforms of anechoic speech and RIR are estimated. Moreover, VINP is effective for automatic speech recognition (ASR) systems, which sets it apart from most deep learning (DL)-based single-channel dereverberation approaches. Experiments on single-channel speech dereverberation demonstrate that VINP reaches an advanced level in most metrics related to human perception and displays unquestionable state-of-the-art (SOTA) performance in ASR-related metrics. For blind RIR identification, experiments indicate that VINP attains the SOTA level in blind estimation of reverberation time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio samples are available online. 

**Abstract (ZH)**: 回声抵消语音，指的是经过混响过程退化的语音信号，包含了无混响源语音和房间冲激响应（RIR）的关键信息。本文提出了一种基于神经语音先验的变分贝叶斯推断（VBI）框架（VINP），用于同时进行语音回声抵消和盲RIR辨识。在VINP中，基于卷积传输函数（CTF）近似，在时频（T-F）域构建了一个概率信号模型。首次，本文提出使用任意判别型回声抵消深度神经网络（DNN）在概率模型中预测无混响语音的先验分布。通过结合回声语音和无混响语音先验，VINP分别对无混响语音频谱和CTF滤波器生成最大后验概率（MAP）和最大似然估计（ML）。通过简单的变换，可估计无混响语音和RIR的波形。此外，VINP对自动语音识别（ASR）系统也非常有效，这使其不同于大多数基于深度学习（DL）的单通道回声抵消方法。单通道语音回声抵消实验表明，VINP在大多数与人类感知相关的度量标准中达到了较高水平，并在ASR相关的度量标准中展示了无可争议的最新技术水平。对于盲RIR辨识，实验表明VINP在60 dB下的混响时间（RT60）和直达声与混响声比（DRR）的盲估计方面达到了最新技术水平。相关的代码和音频样本可在网络上获取。 

---
# Dense Object Detection Based on De-homogenized Queries 

**Title (ZH)**: 基于去均质化查询的密集物体检测 

**Authors**: Yueming Huang, Chenrui Ma, Hao Zhou, Hao Wu, Guowu Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07194)  

**Abstract**: Dense object detection is widely used in automatic driving, video surveillance, and other fields. This paper focuses on the challenging task of dense object detection. Currently, detection methods based on greedy algorithms, such as non-maximum suppression (NMS), often produce many repetitive predictions or missed detections in dense scenarios, which is a common problem faced by NMS-based algorithms. Through the end-to-end DETR (DEtection TRansformer), as a type of detector that can incorporate the post-processing de-duplication capability of NMS, etc., into the network, we found that homogeneous queries in the query-based detector lead to a reduction in the de-duplication capability of the network and the learning efficiency of the encoder, resulting in duplicate prediction and missed detection problems. To solve this problem, we propose learnable differentiated encoding to de-homogenize the queries, and at the same time, queries can communicate with each other via differentiated encoding information, replacing the previous self-attention among the queries. In addition, we used joint loss on the output of the encoder that considered both location and confidence prediction to give a higher-quality initialization for queries. Without cumbersome decoder stacking and guaranteeing accuracy, our proposed end-to-end detection framework was more concise and reduced the number of parameters by about 8% compared to deformable DETR. Our method achieved excellent results on the challenging CrowdHuman dataset with 93.6% average precision (AP), 39.2% MR-2, and 84.3% JI. The performance overperformed previous SOTA methods, such as Iter-E2EDet (Progressive End-to-End Object Detection) and MIP (One proposal, Multiple predictions). In addition, our method is more robust in various scenarios with different densities. 

**Abstract (ZH)**: 密集对象检测广泛应用于自动驾驶、视频监控等领域。本文重点关注密集对象检测这一具有挑战性的任务。目前，基于贪婪算法的检测方法，如非极大值抑制（NMS），在密集场景中经常会产生重复的预测或漏检现象，这也是NMS基算法普遍面临的问题。通过端到端的DETR（DEtection TRansformer），这是一种能够将NMS等后处理去重能力融入网络中的检测器，我们发现基于查询的检测器中的同质查询会降低网络的去重能力和编码器的学习效率，导致重复预测和漏检问题。为了解决这一问题，我们提出了一种可学习的差异编码，以去同质化查询，同时，通过差异编码信息实现查询之间的通信，替代了之前的查询间的自注意力机制。此外，我们还在编码器的输出上引入了联合损失，综合考虑位置和置信度预测，为查询提供更高的初始质量。在没有复杂解码器堆叠，并保证精度的前提下，我们提出的端到端检测框架更加简洁，并且参数数量减少了约8%，相较于可变形DETR（Deformable DETR）有所下降。通过在具有挑战性的CrowdHuman数据集上进行评估，我们的方法达到了令人满意的精度：平均精度（AP）为93.6%，MR-2为39.2%，JI为84.3%。在性能上超越了诸如Iter-E2EDet（渐进端到端对象检测）和MIP（一个提议，多个预测）等先前的SOTA方法。此外，我们的方法在不同密度的各类场景下表现更加稳健。 

---
# Refine Knowledge of Large Language Models via Adaptive Contrastive Learning 

**Title (ZH)**: 通过自适应对比学习细化大型语言模型的知识 

**Authors**: Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, Shu-Yu Guo, Chao Qu, Xiaoyu Tan, Hai-Tao Zheng, Ying Shen, Philip S. Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07184)  

**Abstract**: How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness of our method. 

**Abstract (ZH)**: 如何缓解大型语言模型（LLMs）的幻觉一直是对LLMs研究社区的基本追求目标。在众多关于幻觉的研究中，主流的方法之一是通过优化LLMs的知识表示来改变其输出以减少幻觉。鉴于这些工作的核心关注点是模型所获取的知识，而知识长期以来一直是人类社会进步的关键主题，我们认为模型在提炼知识的过程中可以极大地受益于人类学习的方式。在我们的工作中，通过模仿人类的学习过程，我们设计了一种自适应对比学习策略。该方法根据LLMs实际掌握知识的情况，灵活构建不同的正样本和负样本用于对比学习。这一策略有助于LLMs巩固它们已经掌握的正确知识，加深对已经遇到但尚未完全理解的正确知识的理解，忘记之前学到的错误知识，并诚实地承认自身缺乏的知识。通过对广泛使用的数据集进行大量实验和详细分析，证明了我们方法的有效性。 

---
# Improved YOLOv7 model for insulator defect detection 

**Title (ZH)**: 改进的YOLOv7模型在绝缘子缺陷检测中的应用 

**Authors**: Zhenyue Wang, Guowu Yuan, Hao Zhou, Yi Ma, Yutang Ma, Dong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07179)  

**Abstract**: Insulators are crucial insulation components and structural supports in power grids, playing a vital role in the transmission lines. Due to temperature fluctuations, internal stress, or damage from hail, insulators are prone to injury. Automatic detection of damaged insulators faces challenges such as diverse types, small defect targets, and complex backgrounds and shapes. Most research for detecting insulator defects has focused on a single defect type or a specific material. However, the insulators in the grid's transmission lines have different colors and materials. Various insulator defects coexist, and the existing methods have difficulty meeting the practical application requirements. Current methods suffer from low detection accuracy and mAP0.5 cannot meet application requirements. This paper proposes an improved YOLOv7 model for multi-type insulator defect detection. First, our model replaces the SPPCSPC module with the RFB module to enhance the network's feature extraction capability. Second, a CA mechanism is introduced into the head part to enhance the network's feature representation ability and to improve detection accuracy. Third, a WIoU loss function is employed to address the low-quality samples hindering model generalization during training, thereby improving the model's overall performance. The experimental results indicate that the proposed model exhibits enhancements across various performance metrics. Specifically, there is a 1.6% advancement in mAP_0.5, a corresponding 1.6% enhancement in mAP_0.5:0.95, a 1.3% elevation in precision, and a 1% increase in recall. Moreover, the model achieves parameter reduction by 3.2 million, leading to a decrease of 2.5 GFLOPS in computational cost. Notably, there is also an improvement of 2.81 milliseconds in single-image detection speed. 

**Abstract (ZH)**: 绝缘子是电力网络中的关键绝缘部件和结构支撑，对于输电线路的运行至关重要。由于温度波动、内部应力或冰雹损伤，绝缘子容易受损。自动检测受损绝缘子面临多样类型、小缺陷目标、复杂背景和形状等挑战。大多数关于检测绝缘子缺陷的研究集中在单一缺陷类型或特定材料上。然而，电网输电线路上的绝缘子具有不同的颜色和材料。各种绝缘子缺陷共存，现有的方法难以满足实际应用需求。当前方法检测精度较低，mAP0.5 也无法满足应用要求。本文提出了一种改进的 YOLOv7 模型用于多类绝缘子缺陷检测。首先，我们的模型用 RFB 模块替代了 SPPCSPC 模块，以增强网络的特征提取能力。其次，在网络头部分引入了 CA 机制，以增强网络的特征表示能力和提高检测精度。第三，采用 WIoU 损失函数来解决训练过程中低质量样本阻碍模型泛化的问题，从而提高模型的整体性能。实验结果表明，所提出模型在多种性能指标上均有所提升。具体而言，mAP_0.5 提升了 1.6%，mAP_0.5:0.95 相应提升了 1.6%，精度提升 1.3%，召回率提升了 1%。此外，该模型实现了参数量减少 320 万，计算成本降低了 2.5 GFLOPS。更重要的是，单张图像检测速度也提升了 2.81 毫秒。 

---
# Foreign-Object Detection in High-Voltage Transmission Line Based on Improved YOLOv8m 

**Title (ZH)**: 基于改进YOLOv8m的高压输电线路异物检测 

**Authors**: Zhenyue Wang, Guowu Yuan, Hao Zhou, Yi Ma, Yutang Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.07175)  

**Abstract**: The safe operation of high-voltage transmission lines ensures the power grid's security. Various foreign objects attached to the transmission lines, such as balloons, kites and nesting birds, can significantly affect the safe and stable operation of high-voltage transmission lines. With the advancement of computer vision technology, periodic automatic inspection of foreign objects is efficient and necessary. Existing detection methods have low accuracy because foreign objects at-tached to the transmission lines are complex, including occlusions, diverse object types, significant scale variations, and complex backgrounds. In response to the practical needs of the Yunnan Branch of China Southern Power Grid Co., Ltd., this paper proposes an improved YOLOv8m-based model for detecting foreign objects on transmission lines. Experiments are conducted on a dataset collected from Yunnan Power Grid. The proposed model enhances the original YOLOv8m by in-corporating a Global Attention Module (GAM) into the backbone to focus on occluded foreign objects, replacing the SPPF module with the SPPCSPC module to augment the model's multiscale feature extraction capability, and introducing the Focal-EIoU loss function to address the issue of high- and low-quality sample imbalances. These improvements accelerate model convergence and enhance detection accuracy. The experimental results demonstrate that our proposed model achieves a 2.7% increase in mAP_0.5, a 4% increase in mAP_0.5:0.95, and a 6% increase in recall. 

**Abstract (ZH)**: 高压输电线路的安全运行确保了电力系统的安全。附着在输电线路上的各种异物，如气球、风筝和筑巢鸟类，会对高压输电线路的安全稳定运行产生显著影响。随着计算机视觉技术的进步，定期自动检测异物是高效且必要的。由于附着在输电线路上的异物复杂多样，包括遮挡、不同类型的物体、显著的尺度变化以及复杂的背景，现有的检测方法准确性较低。针对中国南方电网有限责任公司云南分公司实际需求，本文提出了一种改进的YOLOv8m模型，专门用于检测输电线路上的异物。在云南电网收集的数据集上进行了实验。该提出的模型通过将全局注意力模块（GAM）集成到骨干网中，以聚焦遮挡的异物；用SPPCSPC模块替换SPPF模块，以增强模型的多尺度特征提取能力；引入Focal-EIoU损失函数，以解决高质量和低质量样本不平衡的问题。这些改进加速了模型的收敛，并提高了检测准确性。实验结果表明，所提出的模型在mAP_0.5上提高了2.7%，在mAP_0.5:0.95上提高了4%，在召回率上提高了6%。 

---
# SemiHMER: Semi-supervised Handwritten Mathematical Expression Recognition using pseudo-labels 

**Title (ZH)**: SemiHMER: 使用伪标签的半监督手写数学表达式识别 

**Authors**: Kehua Chen, Haoyang Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07172)  

**Abstract**: In recent years, deep learning with Convolutional Neural Networks (CNNs) has achieved remarkable results in the field of HMER (Handwritten Mathematical Expression Recognition). However, it remains challenging to improve performance with limited labeled training data. This paper presents, for the first time, a simple yet effective semi-supervised HMER framework by introducing dual-branch semi-supervised learning. Specifically, we simplify the conventional deep co-training from consistency regularization to cross-supervised learning, where the prediction of one branch is used as a pseudo-label to supervise the other branch directly end-to-end. Considering that the learning of the two branches tends to converge in the later stages of model optimization, we also incorporate a weak-to-strong strategy by applying different levels of augmentation to each branch, which behaves like expanding the training data and improving the quality of network training. Meanwhile, We propose a novel module, Global Dynamic Counting Module(GDCM), to enhance the performance of the HMER decoder, which alleviates recognition inaccuracies in long-distance formula recognition and the occurrence of repeated characters. We release our code at this https URL. 

**Abstract (ZH)**: 近年来，基于卷积神经网络（CNNs）的深度学习在手写数学表达识别（Handwritten Mathematical Expression Recognition, HMER）领域取得了显著成果。然而，如何在有限标记训练数据的情况下提高性能依然是一个挑战。本文首次提出了一种简单而有效的半监督HMER框架，通过引入双分支半监督学习。具体而言，我们简化了传统的深度共训练方法，从一致性正则化转变为跨监督学习，其中一个分支的预测结果被用作伪标签直接监督另一个分支。考虑到两个分支的学习在模型优化后期趋于收敛，在每个分支上应用不同层次的数据增强，从而扩大训练数据量并提高网络训练质量。同时，我们提出了一种新型模块——全局动态计数模块（Global Dynamic Counting Module, GDCM），以增强HMER解码器的性能，该模块在长距离公式识别中减少了识别不准确的情况，同时降低了重复字符的出现可能性。我们的代码已发布在以下链接：[这里提供的链接]。 

---
# Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification 

**Title (ZH)**: 不要只是演示，教给我原理：基于原理的多agent提示策略用于文本分类 

**Authors**: Peipei Wei, Dimitris Dimitriadis, Yan Xu, Mingwei Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07165)  

**Abstract**: We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks. 

**Abstract (ZH)**: 我们提出了基于原则的提示策略（PRINCIPLE-BASED PROMPTING），这是一种简单有效的多智能体提示策略，适用于文本分类。该策略首先让多个语言模型（LLM）代理独立生成候选原则，基于带有或不带有标签的示范样本的分析；然后通过最终代理将这些原则整合为最终原则；最后将这些最终原则发送给分类器代理，以执行下游分类任务。在不同规模LLM的数据集上进行的二分类和多分类实验表明，我们的方法不仅在宏F1分数上实现了显著的性能提升（1.55% - 19.37%），而且优于其他强基准（CoT和stepback提示）。通过我们的方法生成的原则有助于LLM在两个私有数据集上的分类任务表现优于人工设计的原则。我们的多智能体基于原则的提示策略在性能上与基于示范的少样本提示方法相当或更好，但推断成本显著降低。消融研究显示，标签信息和多智能体合作的LLM框架在生成高质量原则以辅助下游分类任务方面起到了重要作用。 

---
# Does Training on Synthetic Data Make Models Less Robust? 

**Title (ZH)**: 使用合成数据进行训练是否会使得模型更加脆弱？ 

**Authors**: Lingze Zhang, Ellie Pavlick  

**Link**: [PDF](https://arxiv.org/pdf/2502.07164)  

**Abstract**: An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain "blindspots" by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our "blindspot" task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn't necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized. 

**Abstract (ZH)**: 越来越多的做法是使用合成数据训练大型语言模型（LLMs）。这些合成数据通常是由相同的或类似的LLMs生成的。这引发了人们的一个疑问：合成数据是否实际上会加剧某些“盲点”问题，通过强化LLM已有的启发式思维。在本文中，我们在Llama-2-7B-hf模型上进行了模拟实验，任务是自然语言推理（NLI）。我们选择 MultiNLI 作为一般的任务，并选择了 HANS 作为“盲点”任务，这是一个针对NLI特定启发式策略设计的目标评估集。我们的目标是确定在一般任务和“盲点”任务之间是否出现了性能差异。我们的结果表明，合成数据并未如预期那样强化盲点问题。具体来说，虽然使用合成数据微调并不会必然减少启发式的使用，但也没有像我们假设的那样使其变得更糟。 

---
# A Survey on Mamba Architecture for Vision Applications 

**Title (ZH)**: 《关于视觉应用中Mamba架构的综述》 

**Authors**: Fady Ibrahim, Guangjun Liu, Guanghui Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07161)  

**Abstract**: Transformers have become foundational for visual tasks such as object detection, semantic segmentation, and video understanding, but their quadratic complexity in attention mechanisms presents scalability challenges. To address these limitations, the Mamba architecture utilizes state-space models (SSMs) for linear scalability, efficient processing, and improved contextual awareness. This paper investigates Mamba architecture for visual domain applications and its recent advancements, including Vision Mamba (ViM) and VideoMamba, which introduce bidirectional scanning, selective scanning mechanisms, and spatiotemporal processing to enhance image and video understanding. Architectural innovations like position embeddings, cross-scan modules, and hierarchical designs further optimize the Mamba framework for global and local feature extraction. These advancements position Mamba as a promising architecture in computer vision research and applications. 

**Abstract (ZH)**: Transformer架构已成为视觉任务（如目标检测、语义分割和视频理解）的基础，但其注意力机制的二次复杂性带来了可扩展性挑战。为应对这些局限性，Mamba架构利用状态空间模型（SSMs）实现线性可扩展性、高效的处理和增强的上下文感知。本文探讨了Mamba架构在视觉领域的应用及其最新进展，包括Vision Mamba（ViM）和VideoMamba，它们引入了双向扫描、选择性扫描机制和时空处理，以提高图像和视频的理解能力。架构创新，如位置嵌入、交叉扫描模块和分层设计，进一步优化了Mamba框架以提取全局和局部特征。这些进步使Mamba在计算机视觉研究和应用中具有重要的前景。 

---
# Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health Records via Multimodal Fused Transformer 

**Title (ZH)**: 通过多模态融合变压器对电子健康记录进行儿童心脏骤停早期风险预测 

**Authors**: Jiaying Lu, Stephanie R. Brown, Songyuan Liu, Shifan Zhao, Kejun Dong, Del Bold, Michael Fundora, Alaa Aljiffry, Alex Fedorov, Jocelyn Grunwell, Xiao Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07158)  

**Abstract**: Early prediction of pediatric cardiac arrest (CA) is critical for timely intervention in high-risk intensive care settings. We introduce PedCA-FT, a novel transformer-based framework that fuses tabular view of EHR with the derived textual view of EHR to fully unleash the interactions of high-dimensional risk factors and their dynamics. By employing dedicated transformer modules for each modality view, PedCA-FT captures complex temporal and contextual patterns to produce robust CA risk estimates. Evaluated on a curated pediatric cohort from the CHOA-CICU database, our approach outperforms ten other artificial intelligence models across five key performance metrics and identifies clinically meaningful risk factors. These findings underscore the potential of multimodal fusion techniques to enhance early CA detection and improve patient care. 

**Abstract (ZH)**: 儿科心脏骤停（CA）的早期预测对于在高风险重症监护环境中及时干预至关重要。我们引入了PedCA-FT，这是一种新颖的基于变压器的框架，将电子健康记录（EHR）的表格视图与提取的文本视图结合起来，以充分发挥高维风险因素及其动态的交互作用。通过为每种模态视图配备专门的变压器模块，PedCA-FT能够捕捉复杂的时间性和上下文模式，从而生成稳健的心脏骤停风险估计。在CHOA-CICU数据库中针对精心挑选的儿科队列进行评估，我们的方法在五个关键性能指标上优于十种其他人工智能模型，并且能够识别出具有临床意义的风险因素。这些发现强调了多模态融合技术在增强早期心脏骤停检测以及改善患者护理方面的潜力。 

---
# Explaining 3D Computed Tomography Classifiers with Counterfactuals 

**Title (ZH)**: 用反事实解释3D计算机断层分类器 

**Authors**: Joseph Paul Cohen, Louis Blankemeier, Akshay Chaudhari  

**Link**: [PDF](https://arxiv.org/pdf/2502.07156)  

**Abstract**: Counterfactual explanations in medical imaging are critical for understanding the predictions made by deep learning models. We extend the Latent Shift counterfactual generation method from 2D applications to 3D computed tomography (CT) scans. We address the challenges associated with 3D data, such as limited training samples and high memory demands, by implementing a slice-based approach. This method leverages a 2D encoder trained on CT slices, which are subsequently combined to maintain 3D context. We demonstrate this technique on two models for clinical phenotype prediction and lung segmentation. Our approach is both memory-efficient and effective for generating interpretable counterfactuals in high-resolution 3D medical imaging. 

**Abstract (ZH)**: 在医学影像中，反事实解释对于理解深度学习模型的预测至关重要。我们从二维应用扩展了潜变量转移反事实生成方法到3维计算机断层扫描（CT）图像。为了应对3D数据的挑战，如受限的训练样本和高内存需求，我们采用了切片基的方法。该方法利用在CT切片上训练的2D编码器，并将这些切片组合起来以保持3D上下文。我们在两种临床表型预测模型和肺部分割模型上展示了这一技术。我们的方法在高分辨率3D医学影像中生成可解释的反事实具有高效且有效的特点。 

---
# Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning 

**Title (ZH)**: 当扩展测试时计算量时重新思考微调：限制置信度提高数学推理能力 

**Authors**: Feng Chen, Allan Raventos, Nan Cheng, Surya Ganguli, Shaul Druckmann  

**Link**: [PDF](https://arxiv.org/pdf/2502.07154)  

**Abstract**: Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\it misaligned}$ with pass@N in that pass@N accuracy ${\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的发展凸显了扩大测试计算规模以在复杂任务（如数学推理和代码生成）上实现强大性能的能力。这引发了关键问题：如何在后续测试计算策略和预算的优化下修改模型训练方法？为探索这一问题，我们专注于pass@N这种简单的测试计算策略，该策略在N个独立样本中搜索正确答案。我们发现，令人惊讶的是，使用交叉熵（CE）损失的训练可能导致pass@N准确性随训练时间延长而下降。我们从CE导致的模型过自信出发，解释了这种不一致性，并通过实验验证了过自信作为pass@N测试计算扩展的阻碍。此外，我们提出了一种原则性的修改训练损失函数，该函数通过限制模型自信并恢复pass@N测试性能与pass@N更好地对齐。我们的算法在MATH和MiniF2F基准测试中显示出了在多种场景下的改进：(1) 回答数学问题；(2) 通过搜索不同形状的证明树来证明定理。总体而言，我们的工作强调了需要重新设计传统分离的LLM开发阶段：训练时间和测试时间搜索与推理策略的重要性。 

---
# Feature Importance Depends on Properties of the Data: Towards Choosing the Correct Explanations for Your Data and Decision Trees based Models 

**Title (ZH)**: 特征重要性依赖于数据的特性：关于选择适合您数据和基于决策树的模型的正确解释的探索 

**Authors**: Célia Wafa Ayad, Thomas Bonnier, Benjamin Bosch, Sonali Parbhoo, Jesse Read  

**Link**: [PDF](https://arxiv.org/pdf/2502.07153)  

**Abstract**: In order to ensure the reliability of the explanations of machine learning models, it is crucial to establish their advantages and limits and in which case each of these methods outperform. However, the current understanding of when and how each method of explanation can be used is insufficient. To fill this gap, we perform a comprehensive empirical evaluation by synthesizing multiple datasets with the desired properties. Our main objective is to assess the quality of feature importance estimates provided by local explanation methods, which are used to explain predictions made by decision tree-based models. By analyzing the results obtained from synthetic datasets as well as publicly available binary classification datasets, we observe notable disparities in the magnitude and sign of the feature importance estimates generated by these methods. Moreover, we find that these estimates are sensitive to specific properties present in the data. Although some model hyper-parameters do not significantly influence feature importance assignment, it is important to recognize that each method of explanation has limitations in specific contexts. Our assessment highlights these limitations and provides valuable insight into the suitability and reliability of different explanatory methods in various scenarios. 

**Abstract (ZH)**: 为了确保机器学习模型解释的可靠性，建立其优缺点并明确在何种情况下每种方法表现出色至关重要。然而，目前对何时以及如何使用每种解释方法的理解是不足的。为了填补这一空白，我们通过合成具有所需特性的多种数据集进行了全面的经验性评价。我们的主要目标是评估基于决策树模型预测的局部解释方法提供的特征重要性估计的质量。通过对合成数据集以及公共二分类数据集的结果进行分析，我们发现这些方法生成的特征重要性估计在幅度和符号上存在显著差异。此外，我们发现这些估计对数据中的特定属性非常敏感。尽管某些模型超参数对特征重要性分配影响不大，但需要认识到每种解释方法在其特定上下文中具有局限性。我们的评估突显了这些局限性，并为不同解释方法在各种情境下的适用性和可靠性提供了宝贵的洞见。 

---
# Few-Shot Multi-Human Neural Rendering Using Geometry Constraints 

**Title (ZH)**: 使用几何约束的少样本多人体神经渲染 

**Authors**: Qian li, Victoria Fernàndez Abrevaya, Franck Multon, Adnane Boukhayma  

**Link**: [PDF](https://arxiv.org/pdf/2502.07140)  

**Abstract**: We present a method for recovering the shape and radiance of a scene consisting of multiple people given solely a few images. Multi-human scenes are complex due to additional occlusion and clutter. For single-human settings, existing approaches using implicit neural representations have achieved impressive results that deliver accurate geometry and appearance. However, it remains challenging to extend these methods for estimating multiple humans from sparse views. We propose a neural implicit reconstruction method that addresses the inherent challenges of this task through the following contributions: First, we propose to use geometry constraints by exploiting pre-computed meshes using a human body model (SMPL). Specifically, we regularize the signed distances using the SMPL mesh and leverage bounding boxes for improved rendering. Second, we propose a ray regularization scheme to minimize rendering inconsistencies, and a saturation regularization for robust optimization in variable illumination. Extensive experiments on both real and synthetic datasets demonstrate the benefits of our approach and show state-of-the-art performance against existing neural reconstruction methods. 

**Abstract (ZH)**: 我们提出了一种方法，仅凭少量图片就能恢复由多人组成的场景的形状和辐射强度。由于存在额外的遮挡和杂乱，多人场景非常复杂。对于单人设置，现有的利用隐式神经表示的方法已经取得了令人印象深刻的成果，能够提供精确的几何形状和外观。然而，将这些方法扩展到从稀疏视角估计多人仍然是一个挑战。我们提出了一种神经隐式重构方法，通过以下贡献解决这一任务的基本挑战：首先，我们提出利用人体模型（SMPL）预先计算的网格结构引入几何约束。具体来说，我们使用SMPL网格正则化有符号距离，并利用边界框改进渲染。其次，我们提出了一种射线正则化方案以最小化渲染不一致性，并提出了一种饱和正则化方案以在变量光照下进行鲁棒优化。在真实数据集和合成数据集上的广泛实验表明了我们方法的优势，并展示了其相对于现有神经重建方法的最先进的性能。 

---
# Unconstrained Body Recognition at Altitude and Range: Comparing Four Approaches 

**Title (ZH)**: 高空远距离不受约束的身体识别：四种方法的比较 

**Authors**: Blake A Myers, Matthew Q Hill, Veda Nandan Gandi, Thomas M Metz, Alice J O'Toole  

**Link**: [PDF](https://arxiv.org/pdf/2502.07130)  

**Abstract**: This study presents an investigation of four distinct approaches to long-term person identification using body shape. Unlike short-term re-identification systems that rely on temporary features (e.g., clothing), we focus on learning persistent body shape characteristics that remain stable over time. We introduce a body identification model based on a Vision Transformer (ViT) (Body Identification from Diverse Datasets, BIDDS) and on a Swin-ViT model (Swin-BIDDS). We also expand on previous approaches based on the Linguistic and Non-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with improved training. All models are trained on a large and diverse dataset of over 1.9 million images of approximately 5k identities across 9 databases. Performance was evaluated on standard re-identification benchmark datasets (MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that includes images at a distance (from close-range to 1000m), at altitude (from an unmanned aerial vehicle, UAV), and with clothing change. A comparative analysis across these models provides insights into how different backbone architectures and input image sizes impact long-term body identification performance across real-world conditions. 

**Abstract (ZH)**: 本研究探讨了四种基于身体形状的长周期人员识别方法。不同于依赖于临时特征（例如衣物）的短期重新识别系统，我们关注的是学习那些随时间保持稳定的持久身体形状特征。我们提出了一个基于视觉变换器（Vision Transformer, ViT）的身体识别模型（基于多元数据集的身体识别，简称BIDDS），以及一个基于 Swin-ViT 模型的版本（简称Swin-BIDDS）。我们还扩展了基于语言核心和非语言核心 ResNet 识别模型（LCRIM 和 NLCRIM）的方法，但改进了训练方法。所有模型均在包含超过190万张图像的大规模多源数据集上进行了训练，涵盖约5000个身份在9个数据库中的图像。评估在标准重新识别基准数据集（MARS、MSMT17、Outdoor Gait、DeepChange）和一个包含距离变化（从近距离到1000米）、视角变化（从无人飞机拍摄）以及衣物变化的无约束数据集上进行。对这些模型的对比分析提供了有关不同骨干架构和输入图像尺寸如何影响在实际条件下的长周期身体识别性能的见解。 

---
# Cardiverse: Harnessing LLMs for Novel Card Game Prototyping 

**Title (ZH)**: Cardiverse：利用大规模语言模型进行新颖卡片游戏原型设计 

**Authors**: Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia  

**Link**: [PDF](https://arxiv.org/pdf/2502.07128)  

**Abstract**: The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game designs, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated action-value functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers. 

**Abstract (ZH)**: 计算机游戏，特别是纸牌游戏的原型设计，需要大量的创意构思和游戏玩法评估的人工努力。最近大型语言模型（LLMs）的进步提供了自动化和简化这些过程的机会。然而，LLMs仍然难以设计超出现有数据库的新型游戏机制、生成一致的游戏环境，并为大规模评估开发可扩展的游戏AI。本文通过介绍一个全面的自动化纸牌游戏原型设计框架来应对这些挑战。该方法强调了一种基于图的索引方法，用于生成新颖的游戏设计；一种由LLM驱动的一致游戏代码生成系统，该系统通过游戏记录进行验证；以及一种使用通过自我对弈优化的多种LLM生成的动作-价值函数构建的游戏AI方法。这些贡献旨在加速纸牌游戏的原型设计、减少人工劳动，并降低游戏开发者的门槛。 

---
# Online Scheduling for LLM Inference with KV Cache Constraints 

**Title (ZH)**: 带有键值缓存约束的LLM推理在线调度 

**Authors**: Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2502.07115)  

**Abstract**: Large Language Model (LLM) inference, where a trained model generates text one word at a time in response to user prompts, is a computationally intensive process requiring efficient scheduling to optimize latency and resource utilization. A key challenge in LLM inference is the management of the Key-Value (KV) cache, which reduces redundant computations but introduces memory constraints. In this work, we model LLM inference with KV cache constraints theoretically and propose novel batching and scheduling algorithms that minimize inference latency while effectively managing the KV cache's memory.
We analyze both semi-online and fully online scheduling models, and our results are threefold. First, we provide a polynomial-time algorithm that achieves exact optimality in terms of average latency in the semi-online prompt arrival model. Second, in the fully online case with a stochastic prompt arrival, we introduce an efficient online scheduling algorithm with constant regret. Third, we prove that no algorithm (deterministic or randomized) can achieve a constant competitive ratio in fully online adversarial settings. Our empirical evaluations on a public LLM inference dataset, using the Llama-70B model on A100 GPUs, show that our approach significantly outperforms benchmark algorithms used currently in practice, achieving lower latency while reducing energy consumption. Overall, our results offer a path toward more sustainable and cost-effective LLM deployment. 

**Abstract (ZH)**: 大型语言模型（LLM）推理过程是指训练好的模型根据用户的提示逐词生成文本，这一过程是计算密集型的，需要高效的调度来优化延迟和资源利用率。LLM推理中的一个关键挑战是如何管理键值（KV）缓存，KV缓存可以减少冗余计算，但同时引入了内存约束。在本文中，我们从理论上建模了带有KV缓存约束的LLM推理，并提出了新的分批和调度算法，以最小化推理延迟并有效管理KV缓存的内存。

我们分析了半在线和完全在线的调度模型，我们的结果包括三个方面。首先，我们提供了一个多项式时间算法，在半在线提示到达模型中实现了平均延迟的最优值。其次，在带有随机提示到达的完全在线情况下，我们引入了一个高效的在线调度算法，具有恒定的遗憾代价。第三，我们证明，在完全在线的对抗性环境中，没有任何算法（无论是确定性的还是随机性的）能够实现恒定的竞争比。我们使用公共的LLM推理数据集和A100 GPU上的Llama-70B模型进行的实证评估表明，我们的方法在延迟和能耗方面明显优于当前实践中使用的基准算法。整体而言，我们的结果为实现更可持续和成本效益更高的LLM部署提供了一条途径。 

---
# Generative Distribution Prediction: A Unified Approach to Multimodal Learning 

**Title (ZH)**: 生成分布预测：多模态学习的统一方法 

**Authors**: Xinyu Tian, Xiaotong Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07090)  

**Abstract**: Accurate prediction with multimodal data-encompassing tabular, textual, and visual inputs or outputs-is fundamental to advancing analytics in diverse application domains. Traditional approaches often struggle to integrate heterogeneous data types while maintaining high predictive accuracy. We introduce Generative Distribution Prediction (GDP), a novel framework that leverages multimodal synthetic data generation-such as conditional diffusion models-to enhance predictive performance across structured and unstructured modalities. GDP is model-agnostic, compatible with any high-fidelity generative model, and supports transfer learning for domain adaptation. We establish a rigorous theoretical foundation for GDP, providing statistical guarantees on its predictive accuracy when using diffusion models as the generative backbone. By estimating the data-generating distribution and adapting to various loss functions for risk minimization, GDP enables accurate point predictions across multimodal settings. We empirically validate GDP on four supervised learning tasks-tabular data prediction, question answering, image captioning, and adaptive quantile regression-demonstrating its versatility and effectiveness across diverse domains. 

**Abstract (ZH)**: 多模态数据（包括表格、文本和视觉数据）的准确预测是推动在不同应用领域中数据分析进步的基础。传统方法往往难以在保持高预测精度的同时整合不同类型的异构数据。为此，我们提出了一种名为生成分布预测（GDP，Generative Distribution Prediction）的新框架，该框架利用条件扩散模型等多模态合成数据生成技术，以提高结构化和非结构化模态下的预测性能。GDP 是一种模型无关的方法，可以与任何高保真生成模型兼容，并支持领域适应的迁移学习。我们为 GDP 建立了严谨的理论基础，提供了在使用扩散模型作为生成核心时对其预测精度的统计保证。通过估计数据生成分布并适应各种损失函数以实现风险最小化，GDP 能够在多模态场景中实现准确的点预测。我们在四个监督学习任务中实证验证了 GDP 的表现，包括表格数据预测、问答、图像字幕生成和自适应分位数回归，展示了它在不同领域的多样性和有效性。 

---
# Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive consistency moderated by free choice 

**Title (ZH)**: 自我内核的模式：GPT-4o展示了受自由选择调节的人类认知一致性模式 

**Authors**: Steven A. Lehr, Ketan S. Saichandran, Eddie Harmon-Jones, Nykko Vitali, Mahzarin R. Banaji  

**Link**: [PDF](https://arxiv.org/pdf/2502.07088)  

**Abstract**: Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot's behavior reflects the mechanisms of human attitude change remains to be understood. 

**Abstract (ZH)**: 大型语言模型（LLMs）展现出一些模仿人类认知的新兴模式。本文探讨它们是否也会反映其他更不具深思熟虑的人类心理过程。借鉴认知一致性的经典理论，我们进行了两项登记的研究，测试GPT-4在针对俄罗斯领导人弗拉基米尔·普京的正面或负面文章写作后，其态度是否发生向特定方向的变化。确实，GPT 展现出类似于人类认知一致性效应的倾向性变化模式。更令人惊讶的是，当LLM被提供关于要写哪种文章（正面或负面）的选择错觉时，其态度变化的程度急剧增加。这一结果表明，GPT-4o 表现出类似人类自我功能的模拟，但聊天机器人的行为如何准确反映人类态度变化的机制仍有待进一步理解。 

---
# IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models 

**Title (ZH)**: IRepair：一种基于意图的数据驱动错误修复方法在大规模语言模型中的应用 

**Authors**: Sayem Mohammad Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07072)  

**Abstract**: Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair. 

**Abstract (ZH)**: 每天都能听到大语言模型（LLMs）令人惊叹的成绩，同样也每天都能听到有关它们的挑战。LLMs 著名地容易受到数据集中偏见的影响，导致诸如毒性等问题。虽然领域适应性训练已被用来缓解这些问题，但这些技术在修复过程中通常会无差别地处理所有模型参数，导致修复质量差且模型灵活性降低。在本文中，我们引入了一种新的基于动态切片且具有意图感知的大语言模型修复策略，即 IRepair。该方法有选择地针对模型中最易出错的部分进行修复。具体来说，我们建议动态地切片模型中最敏感的层，将修复精力集中在这些区域。这种方法通过修改较小的部分模型，能够更有效地进行修复，且对模型整体性能的影响可能更小。我们通过对来自 GPT2 和 GPT-Neo 家族的三个参数范围从 800M 到 1.6B 的模型进行毒性缓解设置下的评估，展示了 IRepair 比最近的基线（直接偏好优化）在修复错误方面有效率提高 43.6%，同时对总体性能的干扰减少 46%。我们的实证分析还揭示了错误在模型中更为集中的分布情况，顶层 20% 的层的错误密度比其余 80% 的层高出 773%。这突显了选择性修复的必要性。此外，我们还展示了动态选择方法对于分散分布在模型中的错误至关重要，以确保修复的稳健性和效率。 

---
# TRADES: Generating Realistic Market Simulations with Diffusion Models 

**Title (ZH)**: TRADES：使用扩散模型生成真实的市场模拟数据 

**Authors**: Leonardo Berti, Bardh Prenkaj, Paola Velardi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07071)  

**Abstract**: Financial markets are complex systems characterized by high statistical noise, nonlinearity, volatility, and constant evolution. Thus, modeling them is extremely hard. Here, we address the task of generating realistic and responsive Limit Order Book (LOB) market simulations, which are fundamental for calibrating and testing trading strategies, performing market impact experiments, and generating synthetic market data. Previous works lack realism, usefulness, and responsiveness of the generated simulations. To bridge this gap, we propose a novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB Simulations (TRADES). TRADES generates realistic order flows as time series conditioned on the state of the market, leveraging a transformer-based architecture that captures the temporal and spatial characteristics of high-frequency market data. There is a notable absence of quantitative metrics for evaluating generative market simulation models in the literature. To tackle this problem, we adapt the predictive score, a metric measured as an MAE, by training a stock price predictive model on synthetic data and testing it on real data. We compare TRADES with previous works on two stocks, reporting an x3.27 and x3.47 improvement over SoTA according to the predictive score, demonstrating that we generate useful synthetic market data for financial downstream tasks. Furthermore, we assess TRADES's market simulation realism and responsiveness, showing that it effectively learns the conditional data distribution and successfully reacts to an experimental agent, giving sprout to possible calibrations and evaluations of trading strategies and market impact experiments. We developed DeepMarket, the first open-source Python framework for market simulation with deep learning. In our repository, we include a synthetic LOB dataset composed of the TRADES's generated simulations. 

**Abstract (ZH)**: 金融市场是具有高统计噪声、非线性、波动性和持续演化的复杂系统。因此，建模它们极其困难。本文致力于生成逼真且响应迅速的限价订单簿（LOB）市场模拟，这些模拟对于校准和测试交易策略、进行市场影响实验以及生成合成市场数据至关重要。现有研究缺乏对生成模拟的真实感、实用性及响应性的考量。为弥合这一差距，我们提出了一种新的基于Transformer的去噪扩散概率引擎，用于LOB模拟（TRADES）。TRADES利用基于Transformer的架构，捕捉高频市场数据的时间和空间特性，生成以市场状态为条件的时间序列订单流。文献中缺乏评估生成性市场模拟模型的量化指标。为解决这一问题，我们利用预测得分（测量为MAE）对股票价格预测模型进行训练，并在真实数据上进行测试。我们分别在两只股票上将TRADES与现有工作进行比较，根据预测得分显示，TRADES的性能分别比最先进的模型（SoTA）提升了3.27倍和3.47倍，证明了我们生成了对金融下游任务有用的合成市场数据。此外，我们评估了TRADES的市场模拟的真实性和响应性，表明其能够有效学习条件数据分布，并成功应对实验代理，从而为交易策略和市场影响实验的校准与评估提供了可能性。我们开发了DeepMarket，这是首个基于深度学习的开源Python市场模拟框架。在我们的仓库中，包含了一个由TRADES生成模拟构成的合成LOB数据集。 

---
# Contextual Thompson Sampling via Generation of Missing Data 

**Title (ZH)**: 基于上下文的泰勒斯采样方法通过生成缺失数据实现 

**Authors**: Kelly W. Zhang, Tiffany Tianhui Cai, Hongseok Namkoong, Daniel Russo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07064)  

**Abstract**: We introduce a framework for Thompson sampling contextual bandit algorithms, in which the algorithm's ability to quantify uncertainty and make decisions depends on the quality of a generative model that is learned offline. Instead of viewing uncertainty in the environment as arising from unobservable latent parameters, our algorithm treats uncertainty as stemming from missing, but potentially observable, future outcomes. If these future outcomes were all observed, one could simply make decisions using an "oracle" policy fit on the complete dataset. Inspired by this conceptualization, at each decision-time, our algorithm uses a generative model to probabilistically impute missing future outcomes, fits a policy using the imputed complete dataset, and uses that policy to select the next action. We formally show that this algorithm is a generative formulation of Thompson Sampling and prove a state-of-the-art regret bound for it. Notably, our regret bound i) depends on the probabilistic generative model only through the quality of its offline prediction loss, and ii) applies to any method of fitting the "oracle" policy, which easily allows one to adapt Thompson sampling to decision-making settings with fairness and/or resource constraints. 

**Abstract (ZH)**: 我们提出了一个 Thompson 抽样上下文多臂老虎机框架，在这个框架中，算法对不确定性进行量化并作出决策的能力取决于一个事先离线学习的生成模型的质量。不同于将环境中的不确定性视为不可观测的潜在参数的结果，我们的算法将不确定性视为来自未观测到但可能可观测的未来结果。如果所有这些未来结果都能被观测到，那么只需使用在完整数据集上拟合的“先验”策略即可直接作出决策。受到这一概念化的启发，算法在每次决策时刻都会使用生成模型来概率性地填补缺失的未来结果，随后基于填充后的完整数据集拟合策略，并使用该策略选择下一个动作。我们正式证明了该算法是一种生成性的 Thompson 抽样方法，并证明了其最优的 regrets 上界。特别值得注意的是，我们给出的 regret 上界 i) 仅依赖于生成模型离线预测损失的质量，ii) 可应用于任何拟合“先验”策略的方法，这使得 Thompson 抽样方法能够容易地适应涉及公平性或资源约束的决策制定场景。 

---
# Federated Continual Learning: Concepts, Challenges, and Solutions 

**Title (ZH)**: 联邦持续学习：概念、挑战与解决方案 

**Authors**: Parisa Hamedi, Roozbeh Razavi-Far, Ehsan Hallaji  

**Link**: [PDF](https://arxiv.org/pdf/2502.07059)  

**Abstract**: Federated Continual Learning (FCL) has emerged as a robust solution for collaborative model training in dynamic environments, where data samples are continuously generated and distributed across multiple devices. This survey provides a comprehensive review of FCL, focusing on key challenges such as heterogeneity, model stability, communication overhead, and privacy preservation. We explore various forms of heterogeneity and their impact on model performance. Solutions to non-IID data, resource-constrained platforms, and personalized learning are reviewed in an effort to show the complexities of handling heterogeneous data distributions. Next, we review techniques for ensuring model stability and avoiding catastrophic forgetting, which are critical in non-stationary environments. Privacy-preserving techniques are another aspect of FCL that have been reviewed in this work. This survey has integrated insights from federated learning and continual learning to present strategies for improving the efficacy and scalability of FCL systems, making it applicable to a wide range of real-world scenarios. 

**Abstract (ZH)**: 联邦持续学习（Federated Continual Learning, FCL）已 emerges as a robust solution for collaborative model training in dynamic environments，其中数据样本不断生成并分布在多个设备上。本综述旨在全面回顾FCL，重点关注异质性、模型稳定性、通信开销以及隐私保护等关键挑战。我们探讨了不同形式的异质性及其对模型性能的影响。对于非IID数据、资源受限平台和个人化学习，综述了相应的解决方法，以展示处理异质性数据分布的复杂性。接着，我们回顾了确保模型稳定性和避免灾难性遗忘的技术，这是在非恒定环境中至关重要的。此外，隐私保护技术也是FCL中被研究的方面之一。本综述整合了联邦学习和持续学习的见解，提出了提高FCL系统效能和可扩展性的策略，使其适用于各种现实场景。 

---
# Large Language Models in Software Security: A Survey of Vulnerability Detection Techniques and Insights 

**Title (ZH)**: 大型语言模型在软件安全中的应用：漏洞检测技术综述与见解 

**Authors**: Ze Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, Jeff Huang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07049)  

**Abstract**: Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection, addressing critical challenges in the security domain. Traditional methods, such as static and dynamic analysis, often falter due to inefficiencies, high false positive rates, and the growing complexity of modern software systems. By leveraging their ability to analyze code structures, identify patterns, and generate repair sugges- tions, LLMs, exemplified by models like GPT, BERT, and CodeBERT, present a novel and scalable approach to mitigating vulnerabilities. This paper provides a detailed survey of LLMs in vulnerability detection. It examines key aspects, including model architectures, application methods, target languages, fine-tuning strategies, datasets, and evaluation metrics. We also analyze the scope of current research problems, highlighting the strengths and weaknesses of existing approaches. Further, we address challenges such as cross-language vulnerability detection, multimodal data integration, and repository-level analysis. Based on these findings, we propose solutions for issues like dataset scalability, model interpretability, and applications in low-resource scenarios. Our contributions are threefold: (1) a systematic review of how LLMs are applied in vulnerability detection; (2) an analysis of shared patterns and differences across studies, with a unified framework for understanding the field; and (3) a summary of key challenges and future research directions. This work provides valuable insights for advancing LLM-based vulnerability detection. We also maintain and regularly update latest selected paper on this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）正在成为软件漏洞检测的变革性工具，有望解决安全领域中的关键挑战。传统的静态和动态分析方法由于效率低下、误报率高和现代软件系统日益增加的复杂性而常常失效。通过利用其分析代码结构、识别模式和生成修复建议的能力，LLMs，如GPT、BERT和CodeBERT等模型，提供了一种新颖且可扩展的方法来减轻漏洞。本文详细综述了LLMs在漏洞检测领域的应用。它探讨了关键方面，包括模型架构、应用方法、目标语言、微调策略、数据集和评估指标。我们还分析了当前研究问题的范围，指出了现有方法的优势和不足之处。此外，我们还讨论了跨语言漏洞检测、多模态数据整合和仓库级分析等挑战。基于这些发现，我们提出了关于数据集可扩展性、模型可解释性和低资源场景应用问题的解决方案。我们的贡献体现在三个方面：(1) 系统综述LLMs在漏洞检测中的应用；(2) 对研究方法中的共同模式和差异进行分析，并构建一个统一的理解该领域的框架；(3) 总结关键挑战和未来的研究方向。这项工作为推进基于LLM的漏洞检测提供了宝贵的见解。我们也维护并定期更新了最新的相关研究论文，地址为：https://www.alipay.com 

---
# SnipGen: A Mining Repository Framework for Evaluating LLMs for Code 

**Title (ZH)**: SnipGen：一个用于评估代码生成大语言模型的挖掘库框架 

**Authors**: Daniel Rodriguez-Cardenas, Alejandro Velasco, Denys Poshyvany  

**Link**: [PDF](https://arxiv.org/pdf/2502.07046)  

**Abstract**: Language Models (LLMs), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce SnipGen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. SnipGen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating LLMs for code-related tasks. In our exploratory study, SnipGen mined approximately 227K data points from 338K recent code changes in GitHub commits, focusing on method-level granularity. SnipGen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of LLMs' code generation quality. By providing the mining tool, the methodology, and the dataset, SnipGen empowers researchers and practitioners to rigorously evaluate and interpret LLMs' performance in software engineering contexts. 

**Abstract (ZH)**: 基于语言模型（LLMs）如训练在数十亿参数上的变压器神经网络，在软件工程（SE）中的应用越来越普遍。这些模型利用包含代码仓库的大量数据集进行训练，展现出在SE任务中的出色能力。然而，评估其有效性面临着重大挑战，主要原因是训练数据集和评估数据集之间可能存在重叠。为了解决这一问题，我们介绍了SnipGen，一个综合的仓库挖掘框架，旨在利用跨多种下游任务的提示工程来辅助代码生成。SnipGen的目标是通过生成稳健的测试床和定制化数据点，减轻数据污染，从而帮助研究人员和从业者评估LLMs在代码相关任务中的表现。

在我们的探索性研究中，SnipGen从GitHub提交的338,000段最近的代码更改中挖掘了约227,000个数据点，并关注方法层面的细节。SnipGen配备了一系列提示模板，这些模板可以根据需要组合，生成类似思维链的提示序列，从而实现对LLMs代码生成质量的精细评估。通过提供挖掘工具、方法论和数据集，SnipGen赋能研究人员和从业者在软件工程背景下严格评估和解释LLMs的性能。 

---
# Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs 

**Title (ZH)**: 通过大规模语言模型的数据合成与分析实现可扩展性和伦理性的内部威胁检测 

**Authors**: Haywood Gelman, John D. Hastings  

**Link**: [PDF](https://arxiv.org/pdf/2502.07045)  

**Abstract**: Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition. 

**Abstract (ZH)**: 内部威胁在组织中的影响与其较小的数量不成比例，原因在于它们能够访问内部系统、信息和基础设施。这一研究的影响体现在匿名受访者在基于网络的职业搜索网站上提交的评论中，对组织构成了内部威胁的风险。此类风险信号可能出现在公开的基于网络的职业搜索网站评论中的匿名提交中。本研究探讨了大型语言模型（LLMs）在分析和检测职业网站评论中的内部威胁情感方面的能力。为了应对伦理数据收集的关切，本研究利用LLMs生成合成数据，并与现有的职业评论数据集结合使用。通过将LLMs生成的情绪分数与专家人类评分进行比较分析，结果表明，在大多数情况下，LLMs与人类评价具有很好的一致性，从而能够有效地识别威胁情感的细微指标。在人类生成的数据方面，LLMs的表现低于合成数据，这表明在评估现实世界数据方面存在改进的空间。文本多样性分析发现，人类生成的数据集和LLM生成的数据集之间存在差异，合成数据的多样性略低。总体而言，研究结果表明，LLMs在内部威胁检测方面的适用性，并提出了一种克服数据获取伦理和物流障碍的可扩展解决方案，以测试内部情感。 

---
# Automated Consistency Analysis of LLMs 

**Title (ZH)**: 自动一致性分析模型（或：自动大型语言模型的一致性分析） 

**Authors**: Aditya Patwardhan, Vivek Vaidya, Ashish Kundu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07036)  

**Abstract**: Generative AI (Gen AI) with large language models (LLMs) are being widely adopted across the industry, academia and government. Cybersecurity is one of the key sectors where LLMs can be and/or are already being used. There are a number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in cybersecurity and such other critical areas. One of the key challenge to the trustworthiness and reliability of LLMs is: how consistent an LLM is in its responses?
In this paper, we have analyzed and developed a formal definition of consistency of responses of LLMs. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of several cybersecurity questions: informational and situational. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses, and thus are untrustworthy and unreliable for cybersecurity. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的生成型人工智能（Gen AI）在工业、学术界和政府领域的广泛应用，网络安全领域是其中一个重要应用领域。然而，信任和可靠性的问题是阻碍Gen AI和LLMs在网络安全及其他关键领域广泛应用的重要障碍之一。影响LLMs信任度和可靠性的关键挑战之一是：LLMs在回应问题时的一致性如何？

在本文中，我们对LLMs响应的一致性进行了分析并提出了正式定义。首先，我们定义了响应一致性的含义，然后建立了一个一致性评估框架。本文提出了两种验证一致性的方法：自我验证和跨多个LLMs的验证。我们在GPT4oMini、GPT3.5、Gemini、Cohere和Llama3等多个LLMs上进行了大量实验，使用了一个网络安全基准，其中包括多个网络安全问题，如信息性和情境性问题。我们的实验结果证实，尽管这些LLMs被用于多种网络安全任务，但它们在回应问题时经常不一致，因此在网络安全方面缺乏可信性和可靠性。 

---
# Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment 

**Title (ZH)**: 利用音位变体在自我监督语音模型中的应用以评估非典型发音 

**Authors**: Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David Mortensen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07029)  

**Abstract**: Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features. 

**Abstract (ZH)**: allo音变指的是音素在不同发音环境中音素化时的语音实现变异。对allo音的建模对于异常发音评估至关重要，这一过程涉及区分异常发音和正常发音。然而，近期基于音素分类器的方法往往通过将各种实现形式视为单一音素来简化这一过程，从而忽略了allo音变异的复杂性。受冻结自我监督语音模型（S3M）特征的声学建模能力启发，我们提出了MixGoP，这是一种新颖的方法，利用高斯混合模型来用多个子簇建模音素分布。我们的实验表明，MixGoP在四个出五个数据集中均达到了最佳性能，包括构音障碍和非母语发音。进一步的分析表明，S3M特征比梅尔频率倒谱系数（MFCC）和梅尔谱图更能有效地捕捉allo音变异，突显了将MixGoP与S3M特征集成的益处。 

---
# Representational Alignment with Chemical Induced Fit for Molecular Relational Learning 

**Title (ZH)**: 化学诱导适配的表示对齐在分子关系学习中的应用 

**Authors**: Peiliang Zhang, Jingling Yuan, Qing Xie, Yongjun Zhu, Lin Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07027)  

**Abstract**: Molecular Relational Learning (MRL) is widely applied in natural sciences to predict relationships between molecular pairs by extracting structural features. The representational similarity between substructure pairs determines the functional compatibility of molecular binding sites. Nevertheless, aligning substructure representations by attention mechanisms lacks guidance from chemical knowledge, resulting in unstable model performance in chemical space (\textit{e.g.}, functional group, scaffold) shifted data. With theoretical justification, we propose the \textbf{Re}presentational \textbf{Align}ment with Chemical Induced \textbf{Fit} (ReAlignFit) to enhance the stability of MRL. ReAlignFit dynamically aligns substructure representation in MRL by introducing chemical Induced Fit-based inductive bias. In the induction process, we design the Bias Correction Function based on substructure edge reconstruction to align representations between substructure pairs by simulating chemical conformational changes (dynamic combination of substructures). ReAlignFit further integrates the Subgraph Information Bottleneck during fit process to refine and optimize substructure pairs exhibiting high chemical functional compatibility, leveraging them to generate molecular embeddings. Experimental results on nine datasets demonstrate that ReAlignFit outperforms state-of-the-art models in two tasks and significantly enhances model's stability in both rule-shifted and scaffold-shifted data distributions. 

**Abstract (ZH)**: 分子关系学习（MRL）在自然科学中广泛应用于通过提取结构特征来预测分子对之间的关系。子结构对的表示相似性决定了分子结合位点的功能兼容性。然而，通过注意力机制对子结构表示进行对齐缺乏化学知识的指导，导致在化学空间（例如，功能团、骨架）转移数据时模型性能不稳定。基于理论依据，我们提出了一种增强MRL稳定性的方法，称为**化学诱导契合的表示对齐**（ReAlignFit）。

ReAlignFit通过引入基于化学诱导契合的归纳偏置动态对齐MRL中的子结构表示。在归纳过程中，我们基于子结构边重建设计了偏差校正函数，通过模拟化学构象变化（动态组合子结构）来对齐子结构对之间的表示。进一步地，在对齐过程中，ReAlignFit整合了子图信息瓶颈以细化和优化表现出高度化学功能兼容性的子结构对，利用它们生成分子嵌入。实验结果表明，ReAlignFit在两个任务中优于现有先进模型，并且在规则转移和骨架转移的数据分布中显著提高了模型的稳定性。 

---
# Machine Learning for Everyone: Simplifying Healthcare Analytics with BigQuery ML 

**Title (ZH)**: 人人都能使用的机器学习：通过BigQuery ML简化医疗健康数据分析 

**Authors**: Mohammad Amir Salari, Bahareh Rahmani  

**Link**: [PDF](https://arxiv.org/pdf/2502.07026)  

**Abstract**: Machine learning (ML) is transforming healthcare by enabling predictive analytics, personalized treatments, and improved patient outcomes. However, traditional ML workflows require specialized skills, infrastructure, and resources, limiting accessibility for many healthcare professionals. This paper explores how Google Cloud's BigQuery ML simplifies the development and deployment of ML models using SQL, reducing technical barriers. Through a case study on diabetes prediction using the Diabetes Health Indicators Dataset, we evaluate three predictive models: Logistic Regression, Boosted Tree, and Deep Neural Network (DNN). Our results demonstrate that the Boosted Tree model achieves the highest performance, making it highly effective for diabetes prediction. This study highlights BigQuery ML's role in democratizing machine learning by providing a scalable, efficient, and accessible solution for healthcare analytics. 

**Abstract (ZH)**: 机器学习（ML）正在通过实现预测分析、个性化治疗和改善患者结果等方式改变医疗保健领域。然而，传统的ML工作流需要专门的技术技能、基础设施和资源，限制了许多医疗保健专业人员的可访问性。本文探讨了Google Cloud的BigQuery ML如何通过使用SQL简化ML模型的开发和部署，从而降低技术壁垒。通过使用糖尿病健康指标数据集对糖尿病预测进行案例研究，我们评估了三种预测模型：逻辑回归、提升树和深度神经网络（DNN）。我们的结果表明，提升树模型在性能上表现最优，使其成为糖尿病预测的强大工具。本研究突显了BigQuery ML在通过提供可扩展、高效且易于访问的解决方案来使机器学习普及化方面的角色，尤其是在医疗保健分析领域。 

---
# AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements 

**Title (ZH)**: AIMS.au：企业声明中现代奴隶制对策分析的数据集 

**Authors**: Adriana Eufrosiana Bora, Pierre-Luc St-Charles, Mirko Bronzi, Arsène Fansi Tchango, Bruno Rousseau, Kerrie Mengersen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07022)  

**Abstract**: Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings. 

**Abstract (ZH)**: 尽管在过去十多年里，各国政府已经通过立法努力解决大型企业供应链中的现代奴隶制问题，但政府监督的有效性仍然受到每年审查成千上份声明的挑战。虽然大型语言模型（LLMs）可以被视为自动分析和总结文档的有效解决方案，但识别并区分企业采取的具体反现代奴隶制措施与模糊声明仍然是一个艰巨的任务。为帮助评估和调整用于评估企业声明的LLMs，我们引入了一个包含5,731条来自澳大利亚现代奴隶制登记册的现代奴隶制声明的数据集，并对每个句子进行了标注。本文详细介绍了该数据集的构建步骤，包括精心设计的标注指南、声明的选择和预处理，以及用于有效模型评估的高质量标注子集的创建。为了展示该数据集的用途，我们提出了一个机器学习方法，用于检测与澳大利亚现代奴隶制法案规定的强制性报告要求相关的句子。接着，我们按照这一方法，在零样本和监督学习环境下对现代语言模型进行了基准测试。 

---
# Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI 

**Title (ZH)**: 使用大型语言模型和可解释人工智能预测项目功能差异：寻找与DIF相关的词 

**Authors**: Hotaka Maeda, Yikai Lu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07017)  

**Abstract**: We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses. 

**Abstract (ZH)**: 我们对几种基于编码器的Transformer大型语言模型（LLM）进行了微调和比较，以预测项目功能差异（DIF）来自项目文本。随后，我们应用了解释型人工智能（XAI）方法，以识别与DIF相关的特定词汇。数据包括针对3至11年级学生设计的42,180项英语语言艺术和数学总结性州级评估项目。预测$R^2$值在八个焦点组和参考组对之间范围从0.04到0.32。我们的研究发现表明，与DIF相关的许多词汇反映了测试蓝图中出于设计目的包含的小领域，而不是与构念无关的项目内容，这些内容应从评估中移除。这可能解释了为什么对DIF项目的定性审查常常得到令人困惑或模棱两可的结果。我们的方法可以在项目编写过程中筛选与DIF相关的词汇，以进行即时修订，或通过强调文本中的关键词汇来帮助审查传统的DIF分析结果。扩展这项研究可以提高评估项目的公平性，尤其是那些缺乏资源来构建高质量项目的评估，以及样本量不足以进行传统DIF分析的小型子群体。 

---
# From Image to Video: An Empirical Study of Diffusion Representations 

**Title (ZH)**: 从图像到视频：关于扩散表示的实证研究 

**Authors**: Pedro Vélez, Luisa F. Polanía, Yi Yang, Chuhan Zhang, Rishab Kabra, Anurag Arnab, Mehdi S. M. Sajjadi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07001)  

**Abstract**: Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis. This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we systematically compare the same model architecture trained for video versus image generation, analyzing the performance of their latent representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. Results show that video diffusion models consistently outperform their image counterparts, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning. 

**Abstract (ZH)**: 扩散模型已经彻底革新了生成建模，使其在图像和视频合成中表现出前所未有的逼真度。这一成功激发了研究者们探索利用其表示方法进行视觉理解任务的兴趣。尽管近期一些工作已经探索了这一潜力，但视频扩散模型在视觉理解方面的能力仍然尚未完全被探索。为填补这一空白，我们系统地比较了用于视频和图像生成的相同模型架构的性能，分析了其潜在表示在分类、动作识别、深度估计和跟踪等多种下游任务中的表现。结果表明，视频扩散模型在各种任务中普遍优于图像扩散模型，但我们在不同层提取的特征和不同噪声水平下的特征，以及模型大小和训练预算对表示质量和生成效果的影响方面发现了显著差异。这项工作首次直接比较了视频和图像扩散目标在视觉理解中的表现，为时间信息在表示学习中的作用提供了洞见。 

---
# SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering 

**Title (ZH)**: SyncMind: 测量协作软件工程中代理脱同步恢复能力 

**Authors**: Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji  

**Link**: [PDF](https://arxiv.org/pdf/2502.06994)  

**Abstract**: Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their consistently low collaboration willingness (<= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: this https URL. 

**Abstract (ZH)**: 软件工程（SE）正变得越来越具协作性，开发人员需要共同处理复杂共享的代码库。在共享环境中进行有效协作要求参与者（无论是人类还是AI代理）在环境发生变化时保持同步。当一名合作者的理解与当前状态不一致时（我们将其称为脱同步挑战），可能会导致合作失败，进而引发集成问题。在本研究中，我们引入了SyncMind框架，该框架系统地定义了大型语言模型（LLM）代理在协作软件工程（CSE）中面临的脱同步问题。基于SyncMind，我们创建了SyncBench基准测试，该基准测试包含了从21个流行的GitHub仓库中提取的24,332个代理脱同步场景实例，这些仓库均包含可执行验证测试。SyncBench上的实验揭示了现有LLM代理能力及短板的诸多关键见解。不同代理（从Llama-3.1代理≤3.33%到Claude-3.5-Sonnet代理≥28.18%）之间的显著性能差距，以及其持续低下的合作意愿（≤4.86%），表明现有LLM在CSE中存在根本性的局限性。然而，合作的发生与脱同步恢复的成功正相关。代理在资源感知下的脱同步恢复性能最小的差异进一步揭示了其显著的资源感知不足和适应性不足，这为未来高效协作系统指明了方向。源代码和数据可在我们的项目网站上公开获取：this https URL。 

---
# Who is Helping Whom? Analyzing Inter-dependencies to Evaluate Cooperation in Human-AI Teaming 

**Title (ZH)**: 谁在帮助谁？分析相互依赖性以评估人机团队合作中的合作值得关注。

（注：这里的翻译保留了原文的核心意思，同时尽量符合中文的学术表达习惯。原文中的“Teaming”一词在不同的上下文中可能有不同的翻译，此处翻译为“团队合作”，具体可以根据实际语境调整为更合适的表达。） 

**Authors**: Upasana Biswas, Siddhant Bhambri, Subbarao Kambhampati  

**Link**: [PDF](https://arxiv.org/pdf/2502.06976)  

**Abstract**: The long-standing research challenges of Human-AI Teaming(HAT) and Zero-shot Cooperation(ZSC) have been tackled by applying multi-agent reinforcement learning(MARL) to train an agent by optimizing the environment reward function and evaluating their performance through task performance metrics such as task reward. However, such evaluation focuses only on task completion, while being agnostic to `how' the two agents work with each other. Specifically, we are interested in understanding the cooperation arising within the team when trained agents are paired with humans. To formally address this problem, we propose the concept of interdependence to measure how much agents rely on each other's actions to achieve the shared goal, as a key metric for evaluating cooperation in human-agent teams. Towards this, we ground this concept through a symbolic formalism and define evaluation metrics that allow us to assess the degree of reliance between the agents' actions. We pair state-of-the-art agents trained through MARL for HAT, with learned human models for the the popular Overcooked domain, and evaluate the team performance for these human-agent teams. Our results demonstrate that trained agents are not able to induce cooperative behavior, reporting very low levels of interdependence across all the teams. We also report that teaming performance of a team is not necessarily correlated with the task reward. 

**Abstract (ZH)**: 人机团队（HAT）和零样本合作（ZSC）领域的长期研究挑战通过应用多代理强化学习（MARL）得到了解决，通过优化环境奖励函数来训练代理，并通过任务完成度指标（如任务奖励）来评估其性能。然而，这种评估仅关注任务完成情况，而忽略了“双方如何协作”的问题。具体而言，我们对在训练好的代理与人类搭档时产生的合作机制产生了兴趣。为了正式解决这一问题，我们提出了相互依赖性的概念，用以衡量代理之间为了实现共同目标而依赖对方行动的程度，这是评估人机团队合作的关键指标。为实现这一目标，我们通过符号形式化方法对这一概念进行了定义，并制定了评估指标，以评估代理之间行为的依赖程度。我们使用通过MARL训练的最新代理（适用于HAT）与Overcooked领域中学习到的人类模型配对，并评估这些人机团队的表现。实验结果表明，训练好的代理未能诱导出合作行为，所有团队的相互依赖性都非常低。我们还指出，团队表现与任务奖励之间并不一定存在关联。 

---
# Task Offloading in Vehicular Edge Computing using Deep Reinforcement Learning: A Survey 

**Title (ZH)**: 使用深度强化学习的车辆边缘计算任务卸载：一个综述 

**Authors**: Ashab Uddin, Ahmed Hamdi Sakr, Ning Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06963)  

**Abstract**: The increasing demand for Intelligent Transportation Systems (ITS) has introduced significant challenges in managing the complex, computation-intensive tasks generated by modern vehicles while offloading tasks to external computing infrastructures such as edge computing (EC), nearby vehicular , and UAVs has become influential solution to these challenges. However, traditional computational offloading strategies often struggle to adapt to the dynamic and heterogeneous nature of vehicular environments. In this study, we explored the potential of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) frameworks to optimize computational offloading through adaptive, real-time decision-making, and we have thoroughly investigated the Markov Decision Process (MDP) approaches on the existing literature. The paper focuses on key aspects such as standardized learning models, optimized reward structures, and collaborative multi-agent systems, aiming to advance the understanding and application of DRL in vehicular networks. Our findings offer insights into enhancing the efficiency, scalability, and robustness of ITS, setting the stage for future innovations in this rapidly evolving field. 

**Abstract (ZH)**: 近年来，对智能运输系统（ITS）的需求不断增加，这在管理现代车辆产生的复杂、计算密集型任务时引入了重大挑战，而将任务卸载到边缘计算（EC）、附近车辆和无人机等外部计算基础设施已成为解决这些挑战的有效方案。然而，传统的计算卸载策略往往难以适应车辆环境中动态且异构的特性。在此项研究中，我们探讨了强化学习（RL）和深度强化学习（DRL）框架在通过自适应、实时决策优化计算卸载方面的潜在应用，并详细研究了现有文献中的马尔可夫决策过程（MDP）方法。本文集中在标准化的学习模型、优化的奖励结构以及协作的多智能体系统等方面，旨在推动对DRL在车辆网络中应用的理解和应用。我们的研究结果为提高ITS的效率、扩展性和鲁棒性提供了见解，并为这一快速发展的领域未来的创新奠定了基础。 

---
# Neighborhood-Order Learning Graph Attention Network for Fake News Detection 

**Title (ZH)**: 基于邻域顺序学习的图注意力网络在假新闻检测中的应用 

**Authors**: Batool Lakzaei, Mostafa Haghir Chehreghani, Alireza Bagheri  

**Link**: [PDF](https://arxiv.org/pdf/2502.06927)  

**Abstract**: Fake news detection is a significant challenge in the digital age, which has become increasingly important with the proliferation of social media and online communication networks. Graph Neural Networks (GNN)-based methods have shown high potential in analyzing graph-structured data for this problem. However, a major limitation in conventional GNN architectures is their inability to effectively utilize information from neighbors beyond the network's layer depth, which can reduce the model's accuracy and effectiveness. In this paper, we propose a novel model called Neighborhood-Order Learning Graph Attention Network (NOL-GAT) for fake news detection. This model allows each node in each layer to independently learn its optimal neighborhood order. By doing so, the model can purposefully and efficiently extract critical information from distant neighbors. The NOL-GAT architecture consists of two main components: a Hop Network that determines the optimal neighborhood order and an Embedding Network that updates node embeddings using these optimal neighborhoods. To evaluate the model's performance, experiments are conducted on various fake news datasets. Results demonstrate that NOL-GAT significantly outperforms baseline models in metrics such as accuracy and F1-score, particularly in scenarios with limited labeled data. Features such as mitigating the over-squashing problem, improving information flow, and reducing computational complexity further highlight the advantages of the proposed model. 

**Abstract (ZH)**: 在数字时代，假新闻检测是一个重要的挑战，随着社交媒体和在线通信网络的普及，这一挑战的重要性日益凸显。基于图神经网络（Graph Neural Networks，GNN）的方法在分析此类问题所涉及的图结构数据方面显示出巨大的潜力。然而，传统GNN架构的一个主要局限性在于，它们无法有效地利用网络层深度之外的邻居信息，这可能会降低模型的准确性和有效性。在本文中，我们提出了一种名为Neighborhood-Order Learning Graph Attention Network（NOL-GAT）的新模型，用于假新闻检测。该模型允许每层中的每个节点独立学习其最优邻居顺序。通过这种方式，模型可以有目的地且高效地从远处的邻居中提取关键信息。NOL-GAT架构由两个主要组成部分构成：跳网络（Hop Network），用于确定最优邻居顺序，以及嵌入网络（Embedding Network），用于利用这些最优邻居来更新节点嵌入。为了评估模型的性能，我们在多种假新闻数据集上进行了实验。结果显示，NOL-GAT在准确率和F1分数等指标上显著优于基线模型，特别是在有限标记数据的情况下表现尤为突出。此外，减少过度压缩问题、改善信息流和降低计算复杂度等特点进一步突显了所提模型的优势。 

---
# Occam's model: Selecting simpler representations for better transferability estimation 

**Title (ZH)**: 奥卡姆模型：选择更简单的表示以获得更好的迁移性估计 

**Authors**: Prabhant Singh, Sibylle Hess, Joaquin Vanschoren  

**Link**: [PDF](https://arxiv.org/pdf/2502.06925)  

**Abstract**: Fine-tuning models that have been pre-trained on large datasets has become a cornerstone of modern machine learning workflows. With the widespread availability of online model repositories, such as Hugging Face, it is now easier than ever to fine-tune pre-trained models for specific tasks. This raises a critical question: which pre-trained model is most suitable for a given task? This problem is called transferability estimation. In this work, we introduce two novel and effective metrics for estimating the transferability of pre-trained models. Our approach is grounded in viewing transferability as a measure of how easily a pre-trained model's representations can be trained to separate target classes, providing a unique perspective on transferability estimation. We rigorously evaluate the proposed metrics against state-of-the-art alternatives across diverse problem settings, demonstrating their robustness and practical utility. Additionally, we present theoretical insights that explain our metrics' efficacy and adaptability to various scenarios. We experimentally show that our metrics increase Kendall's Tau by up to 32% compared to the state-of-the-art baselines. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

在大规模数据集上预先训练的模型进行微调已成为现代机器学习工作流程中的基石。随着在线模型库的广泛可用性，如Hugging Face，现在比以往任何时候都更容易为特定任务微调预先训练的模型。这引发了关键性的问题：对于给定任务而言，哪种预先训练的模型最为合适？这个问题被称为迁移学习评估。在这项工作中，我们引入了两个新颖且有效的方法来估计预先训练模型的迁移性。我们的方法基于将迁移学习视为预训练模型表示能够轻松区分目标类别的度量，这为迁移学习评估提供了独特的视角。我们严格评估了所提出的度量方法与最先进的替代方案在多样化的问题设置下的表现，证明了它们的稳健性和实际应用价值。此外，我们还提出了理论洞察，解释了这些度量方法的有效性和在各种情景下的适应性。实验结果显示，与最先进的基线方法相比，我们的度量方法可将Kendall's Tau提高多达32%。 

---
# XAMBA: Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units 

**Title (ZH)**: XAMBA：在资源受限的神经处理单元上启用高效的状态空间模型 

**Authors**: Arghadip Das, Arnab Raha, Shamik Kundu, Soumendu Kumar Ghosh, Deepak Mathaikutty, Vijay Raghunathan  

**Link**: [PDF](https://arxiv.org/pdf/2502.06924)  

**Abstract**: State-Space Models (SSMs) have emerged as efficient alternatives to transformers for sequential data tasks, offering linear or near-linear scalability with sequence length, making them ideal for long-sequence applications in NLP, vision, and edge AI, including real-time transcription, translation, and contextual search. These applications require lightweight, high-performance models for deployment on resource-constrained devices like laptops and PCs. Designing specialized accelerators for every emerging neural network is costly and impractical; instead, optimizing models for existing NPUs in AI PCs provides a scalable solution. To this end, we propose XAMBA, the first framework to enable and optimize SSMs on commercial off-the-shelf (COTS) state-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1) enabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and (3) trading accuracy for additional performance gains. After enabling SSMs on NPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing sequential CumSum and ReduceSum operations with matrix-based computations, significantly improving execution speed and memory efficiency. Additionally, ActiBA enhances performance by approximating expensive activation functions (e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with minimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show that XAMBA achieves up to 2.6X speed-up over the baseline. Our implementation is available at this https URL. 

**Abstract (ZH)**: 状态空间模型（SSMs）已成为序列数据任务中变压器的有效替代方案，它们在序列长度上实现了线性或接近线性的可扩展性，使其非常适合NLP、视觉和边缘AI中的长序列应用，包括实时转录、翻译和语境搜索。这些应用需要轻量级且高性能的模型，以便在像笔记本电脑和个人电脑这样的资源受限设备上部署。为每一项新兴的神经网络设计专门的加速器既昂贵又不现实；相反，优化现有的AI个人电脑中的NPUs模型提供了一种可扩展的解决方案。为此，我们提出了XAMBA，这是第一个能够在商用现成（COTS）的最先进的（SOTA）NPUs上实现和优化SSMs的框架。XAMBA遵循一个三步方法：（1）在NPUs上实现SSMs，（2）优化性能以满足KPI要求，（3）通过牺牲一些准确度来获得额外的性能提升。在NPUs上实现SSMs后，XAMBA使用CumBA和ReduBA克服了关键瓶颈，通过用基于矩阵的计算替换顺序的CumSum和ReduceSum操作，显著提高了执行速度和内存效率。此外，ActiBA通过使用分段线性映射来近似昂贵的激活函数（例如Swish、Softplus），减少了延迟并最小化了准确度损失，从而提高了性能。在基于Intel Core Ultra系列2 AI个人电脑的评估中，XAMBA比基线快2.6倍。我们的实现可以通过以下链接获取：[此处链接]。

请注意，链接需要实际具体内容才能提供，此处用“[此处链接]”表示。 

---
# Do Attention Heads Compete or Cooperate during Counting? 

**Title (ZH)**: 注意力头在计数过程中是竞争还是协作？ 

**Authors**: Pál Zsámboki, Ádám Fraknói, Máté Gedeon, András Kornai, Zsolt Zombori  

**Link**: [PDF](https://arxiv.org/pdf/2502.06923)  

**Abstract**: We present an in-depth mechanistic interpretability analysis of training small transformers on an elementary task, counting, which is a crucial deductive step in many algorithms. In particular, we investigate the collaboration/competition among the attention heads: we ask whether the attention heads behave as a pseudo-ensemble, all solving the same subtask, or they perform different subtasks, meaning that they can only solve the original task in conjunction. Our work presents evidence that on the semantics of the counting task, attention heads behave as a pseudo-ensemble, but their outputs need to be aggregated in a non-uniform manner in order to create an encoding that conforms to the syntax. Our source code will be available upon publication. 

**Abstract (ZH)**: 我们对训练小型变压器在基本任务（计数）上的机制可解释性进行了深入分析，而计数是许多算法中一个至关重要的演绎步骤。特别地，我们探讨了注意力头之间的合作/竞争关系：我们询问这些注意头是否像伪ensemble一样，各自解决同一个子任务，还是它们执行不同的子任务，这意味着它们只能在结合的情况下解决原始任务。我们的研究提供了证据表明，在计数任务的意义上，注意力头的行为类似于伪ensemble，但它们的输出需要以非均匀的方式聚合，以符合语法编码。我们的源代码将在发表后提供。 

---
# Synthetic Audio Helps for Cognitive State Tasks 

**Title (ZH)**: 合成音频有助于认知状态任务 

**Authors**: Adil Soubki, John Murzaku, Peter Zeng, Owen Rambow  

**Link**: [PDF](https://arxiv.org/pdf/2502.06922)  

**Abstract**: The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio. 

**Abstract (ZH)**: 自然语言处理（NLP）社区大多集中在认知状态任务的文字方法上，但音频可以通过语调提供重要的缺失线索。我们认为，文本转语音模型通过学习跟踪认知状态的相关方面来生成自然声音，这些方面是语言模型无法利用的信号之外的信息。我们提出了合成音频数据微调（SAD）框架，展示了在结合文本和来自现成文本转语音（TTS）系统的零样本合成音频数据的多模态训练下，与仅文本模态相比，7个与认知状态建模相关的任务得到了改善。此外，在包含真实音频的任务和数据集中，我们展示了我们的SAD框架在与仅文本和真实音频相比时，与文本和合成音频具有竞争力的表现。 

---
# GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units 

**Title (ZH)**: GraNNite：在资源受限的神经处理单元上实现图神经网络高性能执行 

**Authors**: Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan  

**Link**: [PDF](https://arxiv.org/pdf/2502.06921)  

**Abstract**: Graph Neural Networks (GNNs) are vital for learning from graph-structured data, enabling applications in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices like client PCs and laptops enhances real-time processing, privacy, and cloud independence. GNNs aid Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparsity, and dynamic structures cause high latency and energy overhead on resource-constrained devices. While modern edge processors integrate CPUs, GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular GNN computations. We introduce GraNNite, the first hardware-aware framework optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN accelerators via a structured three-step methodology: (1) enabling NPU execution, (2) optimizing performance, and (3) trading accuracy for efficiency gains. Step 1 employs GraphSplit for workload distribution and StaGr for static aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts performance using EffOp for control-heavy tasks and GraSp for sparsity exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce redundancy and memory transfers. Step 3 balances quality versus efficiency, where QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs, GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to 8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher performance than CPUs and GPUs, respectively, across GNN models. 

**Abstract (ZH)**: 图形神经网络（GNNs）对于从图结构数据中学习至关重要，能够支持网络分析、推荐系统和语音分析等应用。将其部署到边缘设备（如客户端PC和笔记本电脑）上，可以增强实时处理能力、隐私保护和对云的独立性。GNNs有助于大型语言模型（LLMs）的检索增强生成（RAG）任务，并能够支持基于事件的视觉任务。然而，不规则的内存访问、稀疏性和动态结构在资源受限的设备上会导致高延迟和能源开销。尽管现代边缘处理器集成了CPU、GPU和NPU，但用于数据并行任务的NPU在处理不规则GNN计算时存在困难。我们提出了GraNNite，这是第一个针对商用现成（COTS）最先进深度神经网络（DNN）加速器进行硬件感知优化的框架，通过结构化的三步方法实现：（1）支持NPU执行，（2）优化性能，以及（3）用准确性换取效率提升。第一步采用GraphSplit进行工作负载分布和StaGr进行静态聚合，GrAd和NodePad处理动态图。第二步通过EffOp提高控制密集型任务的性能，以及GraSp利用稀疏性。图卷积优化PreG、SymG和CacheG减少了冗余和内存传输。第三步平衡质量和效率，QuantGr应用INT8量化，GrAx1、GrAx2和GrAx3加速注意力机制、广播加法和SAGE-max聚合。在Intel Core Ultra AI PC上，GraNNite在默认NPU映射上的速度提高了2.6至7.6倍，在CPU和GPU上的能效提高了8.6倍，分别在GNN模型上实现了10.8倍和6.7倍的性能提升。 

---
# Direct Estimation of Pediatric Heart Rate Variability from BOLD-fMRI: A Machine Learning Approach Using Dynamic Connectivity 

**Title (ZH)**: 直接从血氧水平依赖功能性磁共振成像（BOLD-fMRI）估计儿童心率变异性：一种基于动态连接性的机器学习方法 

**Authors**: Abdoljalil Addeh, Karen Ardila, Rebecca J Williams, G. Bruce Pike, M. Ethan MacDonald  

**Link**: [PDF](https://arxiv.org/pdf/2502.06920)  

**Abstract**: In many pediatric fMRI studies, cardiac signals are often missing or of poor quality. A tool to extract Heart Rate Variation (HRV) waveforms directly from fMRI data, without the need for peripheral recording devices, would be highly beneficial. We developed a machine learning framework to accurately reconstruct HRV for pediatric applications. A hybrid model combining one-dimensional Convolutional Neural Networks (1D-CNN) and Gated Recurrent Units (GRU) analyzed BOLD signals from 628 ROIs, integrating past and future data. The model achieved an 8% improvement in HRV accuracy, as evidenced by enhanced performance metrics. This approach eliminates the need for peripheral photoplethysmography devices, reduces costs, and simplifies procedures in pediatric fMRI. Additionally, it improves the robustness of pediatric fMRI studies, which are more sensitive to physiological and developmental variations than those in adults. 

**Abstract (ZH)**: 在许多儿科功能性磁共振成像（fMRI）研究中，心脏信号常常缺失或质量较差。一种能够直接从fMRI数据中提取心率变异性（HRV）波形的工具，在无需使用外围记录设备的情况下将非常有益。我们开发了一种机器学习框架，以准确地重建适用于儿科应用的HRV。该模型结合了一维卷积神经网络（1D-CNN）和门控循环单元（GRU），分析了来自628个感兴趣区域（ROIs）的BOLD信号，整合了过去和未来的数据。该模型在HRV准确性上取得了8%的提升，这一点通过性能指标的改进得到了证实。此方法消除了对外周光体积描记术设备的需求，减少了成本，并简化了儿科fMRI程序。此外，它提高了儿科fMRI研究的稳健性，儿科研究对生理学和发育变化更为敏感，不如成人研究稳定。 

---
# Select before Act: Spatially Decoupled Action Repetition for Continuous Control 

**Title (ZH)**: “先选择后执行：空间解耦的动作重复在连续控制中的应用” 

**Authors**: Buqing Nie, Yangqing Fu, Yue Gao  

**Link**: [PDF](https://arxiv.org/pdf/2502.06919)  

**Abstract**: Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion. Different to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance. However, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them. This constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. In this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually. SDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity. Compared to existing repetition frameworks, SDAR is more sample efficient with higher policy performance and reduced action fluctuation. Experiments are conducted on various continuous control scenarios, demonstrating the effectiveness of spatially decoupled repetition design proposed in this work. 

**Abstract (ZH)**: 强化学习（RL）在各种连续控制任务，如机器人操作和移动方面取得了显著的成功。与传统的逐步决策不同，近期的研究将动作重复引入到RL中，从而提高了动作的持久性，并提升了样本效率和性能。然而，现有的方法在重复过程中将所有动作维度视为整体，忽略了它们之间的差异性。这一限制导致了决策的灵活性降低，政策的适应性也较差。在此工作中，我们提出了一种新颖的动作重复框架，称为SDAR，通过为每个动作维度独立进行闭环执行或重复选择，实现了空间解耦的动作重复。SDAR 能够实现更灵活的重复策略，从而在动作的持久性和多样性之间取得更好的平衡。与现有的重复框架相比，SDAR 在样本效率、政策性能和动作波动方面表现更佳。在各种连续控制场景下的实验验证了本工作中提出的空间解耦重复设计的有效性。 

---
# Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business Processes 

**Title (ZH)**: 利用GPT-4的效率检测业务流程中的返工异常 

**Authors**: Mohammad Derakhshan, Paolo Ceravolo, Fatemeh Mohammadi  

**Link**: [PDF](https://arxiv.org/pdf/2502.06918)  

**Abstract**: This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the Large Language Models (LLM) from OpenAI, in detecting business process anomalies, with a focus on rework anomalies. In our study, we developed a GPT-4o-based tool capable of transforming event logs into a structured format and identifying reworked activities within business event logs. The analysis was performed on a synthetic dataset designed to contain rework anomalies but free of loops. To evaluate the anomaly detection capabilities of GPT 4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and few-shot. These techniques were tested on different anomaly distributions, namely normal, uniform, and exponential, to identify the most effective approach for each case. The results demonstrate the strong performance of GPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with one-shot prompting for the normal distribution, 97.94% accuracy with few-shot prompting for the uniform distribution, and 74.21% accuracy with few-shot prompting for the exponential distribution. These results highlight the model's potential as a reliable tool for detecting rework anomalies in event logs and how anomaly distribution and prompting strategy influence the model's performance. 

**Abstract (ZH)**: 本文探讨了来自OpenAI的大语言模型（LLM）GPT-4o-2024-08-06在检测业务流程异常中的有效性，特别是重做异常。在本研究中，我们开发了一个基于GPT-4o的工具，能够将事件日志转换为结构化格式，并识别业务事件日志中的重做活动。分析是在一个合成数据集上进行的，该数据集包含重做异常但没有循环。为了评估GPT 4o-2024-08-06在检测异常方面的能力，我们使用了三种提示技术：零样本、单样本和少样本。这些技术分别应用于不同的异常分布，即标准分布、均匀分布和指数分布，以确定每种情况下最有效的策略。结果表明，GPT-4o-2024-08-06表现出色。在我们的数据集上，模型通过单样本提示在标准分布下达到96.14%的准确率，通过少样本提示在均匀分布下达到97.94%的准确率，而在指数分布下通过少样本提示达到74.21%的准确率。这些结果突显了该模型作为检测事件日志中重做异常的可靠工具的潜力，并且表明异常分布和提示策略对模型性能的影响。 

---
# Krum Federated Chain (KFC): Using blockchain to defend against adversarial attacks in Federated Learning 

**Title (ZH)**: Krum联邦链（KFC）：使用区块链防御联邦学习中的 adversarial 攻击 

**Authors**: Mario García-Márquez, Nuria Rodríguez-Barroso, M.Victoria Luzón, Francisco Herrera  

**Link**: [PDF](https://arxiv.org/pdf/2502.06917)  

**Abstract**: Federated Learning presents a nascent approach to machine learning, enabling collaborative model training across decentralized devices while safeguarding data privacy. However, its distributed nature renders it susceptible to adversarial attacks. Integrating blockchain technology with Federated Learning offers a promising avenue to enhance security and integrity. In this paper, we tackle the potential of blockchain in defending Federated Learning against adversarial attacks. First, we test Proof of Federated Learning, a well known consensus mechanism designed ad-hoc to federated contexts, as a defense mechanism demonstrating its efficacy against Byzantine and backdoor attacks when at least one miner remains uncompromised. Second, we propose Krum Federated Chain, a novel defense strategy combining Krum and Proof of Federated Learning, valid to defend against any configuration of Byzantine or backdoor attacks, even when all miners are compromised. Our experiments conducted on image classification datasets validate the effectiveness of our proposed approaches. 

**Abstract (ZH)**: 联邦学习提供了一种新兴的机器学习方法，能够在保护数据隐私的同时，使分散设备能够协作训练模型。然而，其分布式特性使其更容易受到 adversarial 攻击。将区块链技术与联邦学习结合使用，为增强安全性和完整性提供了有前景的途径。本文探讨了区块链在防御联邦学习免受 adversarial 攻击方面的潜力。首先，我们测试了一种为联邦环境量身定制的共识机制——联邦学习证明（Proof of Federated Learning），将其作为防御机制，展示了其在至少有一位矿工未受攻击的情况下，对拜占庭和后门攻击的防御效果。其次，我们提出了一种名为 Krum 联邦链（Krum Federated Chain）的新型防御策略，结合了 Krum 和联邦学习证明方法，能够防御任何配置的拜占庭或后门攻击，即使所有矿工均被攻击也是如此。我们在图像分类数据集上进行的实验验证了我们所提出方法的有效性。 

---
# Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum Inspired Adapters 

**Title (ZH)**: 基于量子启发适配器的大型基础模型超压缩细调 

**Authors**: Snehal Raj, Brian Coyle  

**Link**: [PDF](https://arxiv.org/pdf/2502.06916)  

**Abstract**: Fine-tuning pre-trained large foundation models for specific tasks has become increasingly challenging due to the computational and storage demands associated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT) methods address this issue by updating only a small subset of model parameters using adapter modules. In this work, we propose \emph{Quantum-Inspired Adapters}, a PEFT approach inspired by Hamming-weight preserving quantum circuits from quantum machine learning literature. These models can be both expressive and parameter-efficient by operating in a combinatorially large space while simultaneously preserving orthogonality in weight parameters. We test our proposed adapters by adapting large language models and large vision transformers on benchmark datasets. Our method can achieve 99.2\% of the performance of existing fine-tuning methods such LoRA with a 44x parameter compression on language understanding datasets like GLUE and VTAB. Compared to existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\% relative performance with 25x fewer parameters. This demonstrates competitive performance paired with a significant reduction in trainable parameters. Through ablation studies, we determine that combining multiple Hamming-weight orders with orthogonality and matrix compounding are essential for performant fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a promising direction for efficient adaptation of language and vision models in resource-constrained environments. 

**Abstract (ZH)**: 由于全参数更新相关的计算和存储需求，针对特定任务微调预训练的大模型正变得越来越具有挑战性。参数高效微调（PEFT）方法通过仅使用适配器模块更新少量模型参数来解决这一问题。在本工作中，我们提出了一种受量子机器学习文献中保持汉明重量的量子门电路启发的适配器方法——量子启发适配器（Quantum-Inspired Adapters）。这些模型能够在组合空间中具有表现力的同时保持权重参数的正交性，从而实现参数效率。我们通过在基准数据集上微调大型语言模型和大型视觉变换器来测试所提出的适配器。我们的方法在GLUE和VTAB等语言理解数据集上的性能达到现有微调方法（如LoRA）的99.2%（参数压缩比为44倍）。与现有的正交微调方法（如OFT或BOFT）相比，我们的方法在相同的相对性能下减少了25倍的参数数量。这表明，我们的方法在具有竞争力的性能同时实现了参数数量的显著减少。通过消融研究，我们确定结合汉明重量的不同阶数、正交性和矩阵复合是高效微调的关键因素。我们的研究结果表明，量子启发适配器为在资源受限环境中高效适应语言和视觉模型提供了有前途的方向。 

---
# UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge 

**Title (ZH)**: UniZyme：一种增强酶活性位点知识的统一蛋白质切割位点预测器 

**Authors**: Chenao Li, Shuo Yan, Enyan Dai  

**Link**: [PDF](https://arxiv.org/pdf/2502.06914)  

**Abstract**: Enzyme-catalyzed protein cleavage is essential for many biological functions. Accurate prediction of cleavage sites can facilitate various applications such as drug development, enzyme design, and a deeper understanding of biological mechanisms. However, most existing models are restricted to an individual enzyme, which neglects shared knowledge of enzymes and fails generalize to novel enzymes. Thus, we introduce a unified protein cleavage site predictor named {\method}, which can generalize across diverse enzymes. To enhance the enzyme encoding for the protein cleavage site prediction, {\method} employs a novel biochemically-informed model architecture along with active-site knowledge of proteolytic enzymes. Extensive experiments demonstrate that {\method} achieves high accuracy in predicting cleavage sites across a range of proteolytic enzymes, including unseen enzymes. The code is available in this https URL. 

**Abstract (ZH)**: 酶催化的蛋白质水解对于许多生物学功能至关重要。准确预测水解位点可以促进药物开发、酶设计以及对生物机制的更深入理解。然而，现有大多数模型仅局限于单一酶，忽略了酶之间的共性知识，无法有效泛化到新型酶。因此，我们引入了一个通用的蛋白质水解位点预测器{\method}，该预测器可以适用于多种酶。为了增强酶的编码以提高蛋白质水解位点的预测能力，{\method}采用了新颖的生化导向模型架构，并结合了裂解酶的活性位点知识。广泛的实验表明，{\method}在多种裂解酶（包括未见过的新酶）中预测水解位点的准确性较高。代码可在以下链接获取：这个 https URL。 

---
# A Simple yet Effective DDG Predictor is An Unsupervised Antibody Optimizer and Explainer 

**Title (ZH)**: 一个简单而有效的DDG预测器是一个无监督的抗体优化器和解释器 

**Authors**: Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Guojiang Zhao, Zhifeng Gao, Stan Z. Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.06913)  

**Abstract**: The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on the fitness landscape is beneficial. There have been numerous priors used to constrain protein evolution to regions of landscapes with high-fitness variants, among which the change in binding free energy (DDG) of protein complexes upon mutations is one of the most commonly used priors. However, the huge mutation space poses two challenges: (1) how to improve the efficiency of DDG prediction for fast mutation screening; and (2) how to explain mutation preferences and efficiently explore accessible evolutionary regions. To address these challenges, we propose a lightweight DDG predictor (Light-DDG), which adopts a structure-aware Transformer as the backbone and enhances it by knowledge distilled from existing powerful but computationally heavy DDG predictors. Additionally, we augmented, annotated, and released a large-scale dataset containing millions of mutation data for pre-training Light-DDG. We find that such a simple yet effective Light-DDG can serve as a good unsupervised antibody optimizer and explainer. For the target antibody, we propose a novel Mutation Explainer to learn mutation preferences, which accounts for the marginal benefit of each mutation per residue. To further explore accessible evolutionary regions, we conduct preference-guided antibody optimization and evaluate antibody candidates quickly using Light-DDG to identify desirable mutations. 

**Abstract (ZH)**: 当今存在的蛋白质在数十亿年的自然进化过程中被优化。在此过程中，自然会随机产生突变并选择这些突变。发现具有功能潜力的突变受到可进化区域的限制，即仅 fitness 景观中的一小部分是有益的。在众多用于约束蛋白质进化至高fitness变体所在区域的先验知识中，蛋白质复合体在突变后的结合自由能变化（DDG）是最常用的先验之一。然而，庞大的突变空间带来了两个挑战：（1）如何提高 DDG 预测的效率以加快突变筛选；（2）如何解释突变偏好并高效探索可访问的进化区域。为应对这些挑战，我们提出了一种轻量级 DDG 预测器（Light-DDG），该预测器采用结构感知的 Transformer 作为骨干，并通过从现有但计算成本较高的 DDG 预测器中提取知识进行增强。此外，我们扩充、标注并发布了包含数百万突变数据的大规模数据集，用于预训练 Light-DDG。我们发现，这种简单有效的 Light-DDG 可以充当良好的无监督抗体优化器和解释器。对于目标抗体，我们提出了一种新的突变解释器（Mutation Explainer）来学习突变偏好，考虑到每个残基中突变的边际效益。为了进一步探索可访问的进化区域，我们进行了偏好导向的抗体优化，并使用 Light-DDG 快速评估抗体候选物，以识别理想的突变。 

---
# Foundation Models for Anomaly Detection: Vision and Challenges 

**Title (ZH)**: 基于基础模型的异常检测：愿景与挑战 

**Authors**: Jing Ren, Tao Tang, Hong Jia, Haytham Fayek, Xiaodong Li, Suyu Ma, Xiwei Xu, Feng Xia  

**Link**: [PDF](https://arxiv.org/pdf/2502.06911)  

**Abstract**: As data continues to grow in volume and complexity across domains such as finance, manufacturing, and healthcare, effective anomaly detection is essential for identifying irregular patterns that may signal critical issues. Recently, foundation models (FMs) have emerged as a powerful tool for advancing anomaly detection. They have demonstrated unprecedented capabilities in enhancing anomaly identification, generating detailed data descriptions, and providing visual explanations. This survey presents the first comprehensive review of recent advancements in FM-based anomaly detection. We propose a novel taxonomy that classifies FMs into three categories based on their roles in anomaly detection tasks, i.e., as encoders, detectors, or interpreters. We provide a systematic analysis of state-of-the-art methods and discuss key challenges in leveraging FMs for improved anomaly detection. We also outline future research directions in this rapidly evolving field. 

**Abstract (ZH)**: 随着金融、制造和医疗等领域的数据不断增长并变得更加复杂，有效的异常检测对于识别可能预示关键问题的不规则模式变得至关重要。最近，基础模型（FMs）作为一种强大的工具，被用于推进异常检测，展示了在增强异常识别能力、生成详细数据描述以及提供可视化解释方面的无与伦比的能力。本文综述了基于FMs的异常检测的最新进展，提供了对这些进展的首次全面回顾。我们提出了一种新颖的分类体系，根据FMs在异常检测任务中的作用将其分为三大类：编码器、检测器和解释器。我们对最先进的方法进行了系统的分析，并讨论了利用FMs改进异常检测的关键挑战。我们还概述了这一快速发展的领域的未来研究方向。 

---
# TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting 

**Title (ZH)**: TimeKAN：基于KAN的频率分解学习架构用于长期时间序列预测 

**Authors**: Songtao Huang, Zhen Zhao, Can Li, Lei Bai  

**Link**: [PDF](https://arxiv.org/pdf/2502.06910)  

**Abstract**: Real-world time series often have multiple frequency components that are intertwined with each other, making accurate time series forecasting challenging. Decomposing the mixed frequency components into multiple single frequency components is a natural choice. However, the information density of patterns varies across different frequencies, and employing a uniform modeling approach for different frequency components can lead to inaccurate characterization. To address this challenges, inspired by the flexibility of the recent Kolmogorov-Arnold Network (KAN), we propose a KAN-based Frequency Decomposition Learning architecture (TimeKAN) to address the complex forecasting challenges caused by multiple frequency mixtures. Specifically, TimeKAN mainly consists of three components: Cascaded Frequency Decomposition (CFD) blocks, Multi-order KAN Representation Learning (M-KAN) blocks and Frequency Mixing blocks. CFD blocks adopt a bottom-up cascading approach to obtain series representations for each frequency band. Benefiting from the high flexibility of KAN, we design a novel M-KAN block to learn and represent specific temporal patterns within each frequency band. Finally, Frequency Mixing blocks is used to recombine the frequency bands into the original format. Extensive experimental results across multiple real-world time series datasets demonstrate that TimeKAN achieves state-of-the-art performance as an extremely lightweight architecture. Code is available at this https URL. 

**Abstract (ZH)**: 真实世界的时序数据往往包含相互交织的多种频率分量，这使得准确的时序预测具有挑战性。将混合的频率分量分解为多个单一频率分量是一种自然的选择。然而，不同频率下模式的信息密度是不同的，使用统一的建模方法对不同频率的分量进行建模可能会导致不准确的特征描述。为了解决这一问题，我们受近期Kolmogorov-Arnold网络（KAN）灵活性的启发，提出了一种基于KAN的频率分解学习架构（TimeKAN），以应对由多种频率混合引起的各种复杂预测挑战。具体而言，TimeKAN主要由三个组件构成：级联频率分解（CFD）块、多阶KAN表示学习（M-KAN）块和频率混合块。CFD块采用自下而上的级联方法，为每个频率带获取系列表示。得益于KAN的高度灵活性，我们设计了一种新型的M-KAN块，用于在每个频率带内学习和表示特定的时间模式。最后，频率混合块用于将频率带重新组合成原始格式。在多个真实世界时序数据集上的广泛实验结果表明，TimeKAN作为一种极其轻量级的架构，实现了最先进的性能。代码可在以下链接获取：[提供的网址]。 

---
# Satisfaction-Aware Incentive Scheme for Federated Learning in Industrial Metaverse: DRL-Based Stackbelberg Game Approach 

**Title (ZH)**: 面向工业元宇宙的满意度导向的联邦学习激励方案：基于Stackelberg博弈的深度强化学习方法 

**Authors**: Xiaohuan Li, Shaowen Qin, Xin Tang, Jiawen Kang, Jin Ye, Zhonghua Zhao, Dusit Niyato  

**Link**: [PDF](https://arxiv.org/pdf/2502.06909)  

**Abstract**: Industrial Metaverse leverages the Industrial Internet of Things (IIoT) to integrate data from diverse devices, employing federated learning and meta-computing to train models in a distributed manner while ensuring data privacy. Achieving an immersive experience for industrial Metaverse necessitates maintaining a balance between model quality and training latency. Consequently, a primary challenge in federated learning tasks is optimizing overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency. Additionally, the satisfaction function is incorporated into the utility functions to incentivize node participation in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for industrial Metaverse. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves at least 23.7% utility compared to existing schemes without compromising model accuracy. 

**Abstract (ZH)**: 工业元宇宙通过工业互联网（IIoT）整合多样化设备的数据，并利用联邦学习和元计算以分布式方式训练模型，同时确保数据隐私。为了实现工业元宇宙的沉浸式体验，必须在模型质量和训练延迟之间保持平衡。因此，在联邦学习任务中，主要挑战是通过平衡模型质量和训练延迟来优化整个系统性能。本文设计了一个满意度函数，考虑了数据量、信息鲜度（Age of Information, AoI）和训练延迟等因素。此外，将满意度函数嵌入到服务器和节点的效用函数中，以激励节点参与模型训练。我们将服务器和节点的效用函数建模为两阶段Stackelberg博弈，并采用深度强化学习方法来学习Stackelberg均衡。这种方法确保了奖励的平衡，并增强了激励方案在工业元宇宙中的适用性。仿真结果表明，在相同的预算约束条件下，所提出的激励方案在不牺牲模型准确性的情况下至少提高了23.7%的效用。 

---
# Can ChatGPT Diagnose Alzheimer's Disease? 

**Title (ZH)**: ChatGPT能诊断阿尔茨海默病吗？ 

**Authors**: Quoc-Toan Nguyen, Linh Le, Xuan-The Tran, Thomas Do, Chin-Teng Lin  

**Link**: [PDF](https://arxiv.org/pdf/2502.06907)  

**Abstract**: Can ChatGPT diagnose Alzheimer's Disease (AD)? AD is a devastating neurodegenerative condition that affects approximately 1 in 9 individuals aged 65 and older, profoundly impairing memory and cognitive function. This paper utilises 9300 electronic health records (EHRs) with data from Magnetic Resonance Imaging (MRI) and cognitive tests to address an intriguing question: As a general-purpose task solver, can ChatGPT accurately detect AD using EHRs? We present an in-depth evaluation of ChatGPT using a black-box approach with zero-shot and multi-shot methods. This study unlocks ChatGPT's capability to analyse MRI and cognitive test results, as well as its potential as a diagnostic tool for AD. By automating aspects of the diagnostic process, this research opens a transformative approach for the healthcare system, particularly in addressing disparities in resource-limited regions where AD specialists are scarce. Hence, it offers a foundation for a promising method for early detection, supporting individuals with timely interventions, which is paramount for Quality of Life (QoL). 

**Abstract (ZH)**: ChatGPT能够诊断阿尔茨海默病（AD）吗？阿尔茨海默病（AD）是一种严重的神经退行性疾病，约10%的65岁及以上人群受到影响，严重损害记忆和认知功能。本文利用了包含磁共振成像（MRI）和认知测试数据的9300份电子健康记录（EHRs），旨在探讨一个有趣的问题：作为通用任务解决者，ChatGPT能否准确地通过EHRs诊断AD？我们采用黑箱方法，通过零样本和多样本方法深入评估了ChatGPT的表现。本研究揭示了ChatGPT分析MRI和认知测试结果的能力，以及将其作为一种诊断AD的工具的潜力。通过自动化诊断过程中的部分步骤，本研究为医疗系统提供了一种变革性的方法，特别是在资源匮乏地区，这些地区缺乏AD专科医生的情况下。因此，该研究为早期检测提供了一个有前景的方法，支持及时干预，这对于提高生活质量（QoL）至关重要。 

---
# Learning-based estimation of cattle weight gain and its influencing factors 

**Title (ZH)**: 基于学习的 cattle 体重增加量及其影响因素估计 

**Authors**: Muhammad Riaz Hasib Hossain, Rafiqul Islam, Shawn R. McGrath, Md Zahidul Islam, David Lamb  

**Link**: [PDF](https://arxiv.org/pdf/2502.06906)  

**Abstract**: Many cattle farmers still depend on manual methods to measure the live weight gain of cattle at set intervals, which is time consuming, labour intensive, and stressful for both the animals and handlers. A remote and autonomous monitoring system using machine learning (ML) or deep learning (DL) can provide a more efficient and less invasive method and also predictive capabilities for future cattle weight gain (CWG). This system allows continuous monitoring and estimation of individual cattle live weight gain, growth rates and weight fluctuations considering various factors like environmental conditions, genetic predispositions, feed availability, movement patterns and behaviour. Several researchers have explored the efficiency of estimating CWG using ML and DL algorithms. However, estimating CWG suffers from a lack of consistency in its application. Moreover, ML or DL can provide weight gain estimations based on several features that vary in existing research. Additionally, previous studies have encountered various data related challenges when estimating CWG. This paper presents a comprehensive investigation in estimating CWG using advanced ML techniques based on research articles (between 2004 and 2024). This study investigates the current tools, methods, and features used in CWG estimation, as well as their strengths and weaknesses. The findings highlight the significance of using advanced ML approaches in CWG estimation and its critical influence on factors. Furthermore, this study identifies potential research gaps and provides research direction on CWG prediction, which serves as a reference for future research in this area. 

**Abstract (ZH)**: 许多牛农仍然依赖手动方法在固定时间间隔内测量牛的体重增加，这既耗时又劳动密集，且对动物和工作人员都具有一定的压力。利用机器学习（ML）或深度学习（DL）的远程和自主监测系统可以提供更为高效且侵入性较小的方法，并且能够预测未来的体重增加（体重增长，CWG）。该系统允许对个体牛的体重增长、生长速率和体重波动进行连续监控和估计，考虑到各种因素，如环境条件、遗传倾向、饲料供应、运动模式和行为等。众多研究者已经探讨了使用ML和DL算法估算CWG的效率。然而，CWG的估算在应用上缺乏一致性。此外，ML或DL可以根据现有研究中多种变化的特征提供体重增长的估算。此外，以前的研究在估算CWG方面遇到了各种数据相关挑战。本文基于2004年至2024年间的研究文章，对使用高级ML技术估算CWG进行了全面调查。这项研究探讨了当前用于CWG估算的工具、方法和特征，以及它们的优势和劣势。研究结果强调了使用高级ML方法在CWG估算中的重要性及其对各种因素的关键影响。此外，本文识别出CWG预测的潜在研究空白，并提供了研究方向，为该领域的未来研究提供了参考。 

---
# Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty 

**Title (ZH)**: 通过示例难度和预测不确定性实现无需完全训练的轻量级数据集剪枝 

**Authors**: Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun  

**Link**: [PDF](https://arxiv.org/pdf/2502.06905)  

**Abstract**: Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs. Dataset pruning aims to alleviate this demand by discarding redundant examples. However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset. To overcome this limitation, we introduce a Difficulty and Uncertainty-Aware Lightweight (DUAL) score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution. Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66% compared to previous methods while achieving a SOTA, specifically 60% test accuracy at a 90% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15% while maintaining SOTA performance. 

**Abstract (ZH)**: 近年来，深度学习的进步很大程度上依赖于大规模数据集，这导致了存储和训练成本的显著增加。数据集剪枝旨在通过丢弃冗余样本来缓解这一需求。然而，现有的许多方法需要在大规模迭代中对完整数据集进行模型训练，才能进行数据集剪枝，这实际上使得剪枝过程比直接使用完整数据集训练模型更加昂贵。为克服这一局限，我们提出了一种基于难度和不确定性感知的轻量级（DUAL）得分，该方法在早期训练阶段通过同时考虑范例难度和预测不确定性，来识别重要样本。为进一步解决极端剪枝带来的灾难性准确度下降问题，我们还提出了一种使用贝塔分布的比率自适应采样方法。实验在图像分类中的标签噪声和图像损坏、以及模型架构泛化等多种数据集和学习场景上，展示了我们方法的优越性，明显优于之前最先进的（State-of-the-Art，SOTA）方法。具体来说，在ImageNet-1k数据集上，我们的方法在剪枝比率为90%的情况下，将剪枝时间成本降低至前方法的66%，同时实现SOTA的60%测试准确率。而在CIFAR数据集上，时间成本仅降低了15%，同时保持了SOTA性能。 

---
# Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training 

**Title (ZH)**: 在变换器中 episodic 记忆的涌现：表征训练过程中注意力分数的时间结构变化 

**Authors**: Deven Mahesh Mistry, Anooshka Bajaj, Yash Aggarwal, Sahaj Singh Maini, Zoran Tiganj  

**Link**: [PDF](https://arxiv.org/pdf/2502.06902)  

**Abstract**: We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their similarities and differences with human memory and learning. 

**Abstract (ZH)**: 我们研究了上下文中的时间偏倚在注意头和transformer输出中。利用认知科学的方法，我们分析了不同大小的GPT-2模型的注意分数和输出。在不同注意头中，我们观察到了与人类事件记忆特征相符的效果，包括时间连续性、首因效应和近因效应。transformer的输出表明其倾向于基于上下文的序列回忆。重要的是，这种效应在消除引致头后消失，引致头是时间连续性效应的主要驱动力。我们的发现为理解transformer在上下文学习过程中如何以时间维度组织信息提供了见解，揭示了transformer与人类记忆和学习的相似性和差异性。 

---
# Enabling Autoregressive Models to Fill In Masked Tokens 

**Title (ZH)**: 使自回归模型能够填充掩蔽令牌 

**Authors**: Daniel Israel, Aditya Grover, Guy Van den Broeck  

**Link**: [PDF](https://arxiv.org/pdf/2502.06901)  

**Abstract**: Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks. 

**Abstract (ZH)**: 历史上，大规模语言模型（LLMs）通常使用自回归（AR）或掩码语言建模（MLM）目标进行训练，近年来自回归模型在训练中占据了主导地位。然而，自回归模型本质上无法进行掩码填充，即预测过去和未来上下文之间的掩码词。相比之下，掩码语言模型在训练和推理过程中存在固有的计算效率低下问题，这限制了其可扩展性。本研究提出了MARIA（掩码和自回归填充架构），这是一种新的方法，它结合了两种范式的优点，以实现最先进的掩码填充性能。MARIA通过训练一个线性解码器将预训练的MLM模型和AR模型的连接隐藏状态作为输入，实现了这种最小的修改。这种修改允许AR模型进行填充操作，同时保留其固有的优势，例如通过键值缓存实现更快的推理。我们的实验结果表明，与现有的离散扩散模型相比，MARIA在掩码填充任务中的表现明显更优。 

---
# A Sociotechnical Approach for Knowledge Management (KM) 

**Title (ZH)**: 一种社会技术方法论在知识管理（KM）中的应用 

**Authors**: Leoncio Jimenez  

**Link**: [PDF](https://arxiv.org/pdf/2502.06899)  

**Abstract**: This article presents a sociotechnical framework for KM. This sociotechnical vision of KM allows: (1) to remove KM from a commercial concern; (2) to divide the different KM technologies; and (3) to question the paradigms associated with the social and technical components of KM. It is precisely this last point that this article develops to identify the generic mechanisms of KM. More precisely, the social aspect is explained through the organizational approach to KM, the managerial approach to KM, and the biological approach to KM. In contrast, the technical aspect is described through the knowledge and skills engineering approach to KM. These approaches also lead us to provide a comparative table between these organizational, managerial, and biological visions of KM. 

**Abstract (ZH)**: 本文提出了一种社会技术框架来阐述知识管理（KM）。这种社会技术视角的KM理论允许以下几点：（1）将KM从商业关注中剥离出来；（2）区分不同的KM技术；和（3）质疑与KM的社会和技术组成部分相关的范式。正是这一点，本文进行了深入探讨以识别KM的通用机制。更具体地讲，社会方面通过组织层面、管理层面和生物层面的KM方法来解释；相反，技术方面则通过知识与技能工程方法来描述。这些方法也为我们提供了组织、管理与生物视角下的KM之间的比较表格。 

---
# Large Language Models for In-File Vulnerability Localization Can Be "Lost in the End" 

**Title (ZH)**: 大型语言模型在文件内漏洞定位中可能会“迷失终点” 

**Authors**: Francesco Sovrano, Adam Bauer, Alberto Bacchelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.06898)  

**Abstract**: Recent advancements in artificial intelligence have enabled processing of larger inputs, leading everyday software developers to increasingly rely on chat-based large language models (LLMs) like GPT-3.5 and GPT-4 to detect vulnerabilities across entire files, not just within functions. This new development practice requires researchers to urgently investigate whether commonly used LLMs can effectively analyze large file-sized inputs, in order to provide timely insights for software developers and engineers about the pros and cons of this emerging technological trend. Hence, the goal of this paper is to evaluate the effectiveness of several state-of-the-art chat-based LLMs, including the GPT models, in detecting in-file vulnerabilities. We conducted a costly investigation into how the performance of LLMs varies based on vulnerability type, input size, and vulnerability location within the file. To give enough statistical power to our study, we could only focus on the three most common (as well as dangerous) vulnerabilities: XSS, SQL injection, and path traversal. Our findings indicate that the effectiveness of LLMs in detecting these vulnerabilities is strongly influenced by both the location of the vulnerability and the overall size of the input. Specifically, regardless of the vulnerability type, LLMs tend to significantly (p < .05) underperform when detecting vulnerabilities located toward the end of larger files, a pattern we call the 'lost-in-the-end' effect. Finally, to further support software developers and practitioners, we also explored the optimal input size for these LLMs and presented a simple strategy for identifying it, which can be applied to other models and vulnerability types. Eventually, we show how adjusting the input size can lead to significant improvements in LLM-based vulnerability detection, with an average recall increase of over 37% across all models. 

**Abstract (ZH)**: 近年来，人工智能的最新进展使处理大规模输入成为可能，这促使日常软件开发者越来越多地依赖基于聊天的大语言模型（LLMs），如GPT-3.5和GPT-4，来检测整个文件中的漏洞，而不仅仅是文件中的函数。这种新的开发实践要求研究人员亟需调查常用的LLMs是否能够有效地分析大型文件输入，以便为软件开发者和工程师提供关于这一新兴技术趋势的优势和局限性及时见解。因此，本文旨在评估几种最先进的基于聊天的LLMs，包括GPT模型，检测文件内漏洞的有效性。我们针对不同类型的漏洞、输入规模和文件内的漏洞位置进行了耗时的研究，以确保研究有足够的统计能力。由于我们关注的是最常见的也是最危险的三种漏洞：XSS、SQL注入和路径遍历。研究结果表明，LLMs检测这些漏洞的有效性受漏洞位置和输入整体规模的影响。具体来说，无论漏洞类型如何，当检测位于较大文件末尾的漏洞时，LLMs往往会显著（p < .05）表现出不佳的表现，我们称之为“末尾丢失”效应。最后，为了进一步支持软件开发者和从业人员，我们还探讨了这些LLMs的最佳输入规模，并提出了一种简单策略来识别，该策略可以应用于其他模型和漏洞类型。最终，我们展示了调整输入规模可以显著提高基于LLMs的漏洞检测效果，各种模型的召回率平均提高超过37%。 

---
# PyPotteryInk: One-Step Diffusion Model for Sketch to Publication-ready Archaeological Drawings 

**Title (ZH)**: PyPotteryInk：从素描到出版级考古图的一站式扩散模型 

**Authors**: Lorenzo Cardarelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.06897)  

**Abstract**: Archaeological pottery documentation traditionally requires a time-consuming manual process of converting pencil sketches into publication-ready inked drawings. I present PyPotteryInk, an open-source automated pipeline that transforms archaeological pottery sketches into standardised publication-ready drawings using a one-step diffusion model. Built on a modified img2img-turbo architecture, the system processes drawings in a single forward pass while preserving crucial morphological details and maintaining archaeologic documentation standards and analytical value. The model employs an efficient patch-based approach with dynamic overlap, enabling high-resolution output regardless of input drawing size. I demonstrate the effectiveness of the approach on a dataset of Italian protohistoric pottery drawings, where it successfully captures both fine details like decorative patterns and structural elements like vessel profiles or handling elements. Expert evaluation confirms that the generated drawings meet publication standards while significantly reducing processing time from hours to seconds per drawing. The model can be fine-tuned to adapt to different archaeological contexts with minimal training data, making it versatile across various pottery documentation styles. The pre-trained models, the Python library and comprehensive documentation are provided to facilitate adoption within the archaeological research community. 

**Abstract (ZH)**: 考古陶器文档传统上需要耗时的手动过程，即将铅笔素描转化为可出版的墨迹图。我提出了一个开源的自动化流程——PyPotteryInk，该流程利用单步扩散模型将考古陶器素描转化为标准化的可出版图。该系统基于改进的img2img-turbo架构，能在单次前向传递中处理图像，同时保留关键的形态细节并维持考古学文档标准和分析价值。模型采用高效.patch-基于的方法，并具有动态重叠，使得无论输入图像尺寸如何都能产生高分辨率的输出。我在意大利史前陶器素描数据集上展示了该方法的有效性，它成功捕捉了细节如装饰图案以及结构元素如器皿轮廓或手持元素。专家评估证实，生成的图像符合出版标准，同时将每幅图像的处理时间从数小时减少到几秒。该模型可以通过少量训练数据适应不同的考古学背景，使其适用于各种陶器文档风格。所提供的预训练模型、Python库和全面文档将有助于在考古学研究社区中的推广应用。 

---
# AI-Driven HSI: Multimodality, Fusion, Challenges, and the Deep Learning Revolution 

**Title (ZH)**: AI 驱动的HSI：多模态融合、挑战及深度学习革命 

**Authors**: David S. Bhatti, Yougin Choi, Rahman S M Wahidur, Maleeka Bakhtawar, Sumin Kim, Surin Lee, Yongtae Lee, Heung-No Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.06894)  

**Abstract**: Hyperspectral imaging (HSI) captures spatial and spectral data, enabling analysis of features invisible to conventional systems. The technology is vital in fields such as weather monitoring, food quality control, counterfeit detection, healthcare diagnostics, and extending into defense, agriculture, and industrial automation at the same time. HSI has advanced with improvements in spectral resolution, miniaturization, and computational methods. This study provides an overview of the HSI, its applications, challenges in data fusion and the role of deep learning models in processing HSI data. We discuss how integration of multimodal HSI with AI, particularly with deep learning, improves classification accuracy and operational efficiency. Deep learning enhances HSI analysis in areas like feature extraction, change detection, denoising unmixing, dimensionality reduction, landcover mapping, data augmentation, spectral construction and super resolution. An emerging focus is the fusion of hyperspectral cameras with large language models (LLMs), referred as highbrain LLMs, enabling the development of advanced applications such as low visibility crash detection and face antispoofing. We also highlight key players in HSI industry, its compound annual growth rate and the growing industrial significance. The purpose is to offer insight to both technical and non-technical audience, covering HSI's images, trends, and future directions, while providing valuable information on HSI datasets and software libraries. 

**Abstract (ZH)**: 高光谱成像（HSI）能够捕获空间和光谱数据，使分析常规系统无法识别的特征成为可能。这项技术在天气监测、食品质量控制、假冒检测、医疗诊断以及国防、农业和工业自动化等多个领域具有重要价值。随着光谱分辨率的提升、小型化以及计算方法的进步，HSI技术得到了发展。本文提供了一个HSI的综述，涵盖了其应用、数据融合面临的挑战以及深度学习模型在处理HSI数据中的作用。我们讨论了多模态HSI与人工智能（特别是深度学习）的集成如何提高分类准确性和操作效率。深度学习在特征提取、变化检测、去噪解混、降维、土地覆盖图绘制、数据增强、光谱重建和超分辨率等方面提升了HSI分析的性能。一个新兴的研究趋势是将高光谱相机与大型语言模型（LLMs）融合，称之为基础性LLMs，这使得低能见度碰撞检测和面部防伪等高级应用得以开发。我们还强调了高光谱行业的主要参与者、其复合年增长率以及日益增长的工业意义。本文旨在为技术性和非技术性的读者提供见解，涵盖HSI的影像、趋势和未来方向，并提供有关HSI数据集和软件库的重要信息。 

---
# Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks 

**Title (ZH)**: 使用 fuzzed randomized smoothing 验证语言模型的鲁棒性：一种针对后门攻击的高效防御方法 

**Authors**: Bowei He, Lihao Yin, Hui-Ling Zhen, Jianping Zhang, Lanqing Hong, Mingxuan Yuan, Chen Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.06892)  

**Abstract**: The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce \textbf{F}uzzed \textbf{R}andomized \textbf{S}moothing (\textbf{FRS}), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing. Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness. 

**Abstract (ZH)**: 预训练语言模型（PLMs）的广泛应用使它们暴露在文本后门攻击之下，尤其是那些在预训练阶段植入的攻击。这些攻击对高可靠性应用构成了重大风险，因为它们能够隐蔽地影响多个下游任务。虽然验证模型对这类威胁的鲁棒性至关重要，但现有的防御方法难以应对文本数据的高维度性和相互依赖性，以及未能获取原始污染预训练数据的情况。为了解决这些挑战，我们提出了一种新的方法——**Fuzzed Randomized Smoothing (FRS)**，这是一种用于高效验证语言模型对后门攻击鲁棒性的方法。FRS 将软件鲁棒性验证技术与二阶段模型参数平滑技术相结合，采用蒙特卡洛树搜索进行前瞻性的 fuzzing，以在 Damerau-Levenshtein 空间内识别脆弱的文本片段。这种方法允许精确和高效的文本随机化，同时在模型平滑过程中无需访问受污染的训练数据。我们的理论分析表明，FRS 较现有方法实现了更广泛的鲁棒性认证范围。跨多个数据集、模型配置和攻击策略的广泛实验验证了 FRS 在防御效率、准确性和鲁棒性方面的优越性。 

---
# LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison 

**Title (ZH)**: 用于药物相互作用预测的大规模语言模型综合比较 

**Authors**: Gabriele De Vito, Filomena Ferrucci, Athanasios Angelakis  

**Link**: [PDF](https://arxiv.org/pdf/2502.06890)  

**Abstract**: The increasing volume of drug combinations in modern therapeutic regimens needs reliable methods for predicting drug-drug interactions (DDIs). While Large Language Models (LLMs) have revolutionized various domains, their potential in pharmaceutical research, particularly in DDI prediction, remains largely unexplored. This study thoroughly investigates LLMs' capabilities in predicting DDIs by uniquely processing molecular structures (SMILES), target organisms, and gene interaction data as raw text input from the latest DrugBank dataset. We evaluated 18 different LLMs, including proprietary models (GPT-4, Claude, Gemini) and open-source variants (from 1.5B to 72B parameters), first assessing their zero-shot capabilities in DDI prediction. We then fine-tuned selected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1 distilled Qwen 1.5B) to optimize their performance. Our comprehensive evaluation framework included validation across 13 external DDI datasets, comparing against traditional approaches such as l2-regularized logistic regression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5 2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of 0.919 on balanced datasets (50% positive, 50% negative cases). This result represents an improvement over both zero-shot predictions and state-of-the-art machine-learning methods used for DDI prediction. Our analysis reveals that LLMs can effectively capture complex molecular interaction patterns and cases where drug pairs target common genes, making them valuable tools for practical applications in pharmaceutical research and clinical settings. 

**Abstract (ZH)**: 现代治疗方案中药物组合的数量不断增加，需要可靠的方法来预测药物-药物相互作用（DDIs）。虽然大型语言模型（LLMs）已在多个领域实现了革命性变化，但它们在制药研究中的应用潜力，尤其是用于DDI预测方面的探索仍相对有限。本研究通过独特地处理分子结构（SMILES）、目标生物体和基因互作数据，以最新DrugBank数据集的原始文本输入全面考察了LLMs在预测DDI方面的潜力。我们评估了18种不同的LLMs，包括专有模型（GPT-4、Claude、Gemini）以及开源变体（从15亿到72亿参数），首先评估了它们在零样本情况下的DDI预测能力。然后，我们优化了选定的模型（GPT-4、Phi-3.5 2.7B、Qwen-2.5 3B、Gemma-2 9B、以及从Qwen 1.5B中蒸馏出的Deepseek R1）以优化其性能。我们全面的评估框架包括在13个外部DDI数据集上的验证，并与传统方法如正则化逻辑回归进行比较。经过微调的LLMs表现出优越的性能，Phi-3.5 2.7B在DDI预测中的敏感性为0.978，在平衡数据集（50%阳性、50%阴性）上准确率为0.919。这一结果优于零样本预测及用于DDI预测的最新机器学习方法。我们的分析表明，LLMs能够有效地捕捉复杂的分子相互作用模式，并识别药物组合共同靶向的基因情况，使它们成为制药研究和临床应用中的有价值的工具。 

---
# Klotski: Efficient Mixture-of-Expert Inference via Expert-Aware Multi-Batch Pipeline 

**Title (ZH)**: 克洛斯كي：通过专家意识多批次流水线高效实现专家混合推理 

**Authors**: Zhiyuan Fang, Yuegui Huang, Zicong Hong, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2502.06888)  

**Abstract**: Mixture of Experts (MoE), with its distinctive sparse structure, enables the scaling of language models up to trillions of parameters without significantly increasing computational costs. However, the substantial parameter size presents a challenge for inference, as the expansion in GPU memory cannot keep pace with the growth in parameters. Although offloading techniques utilise memory from the CPU and disk and parallelise the I/O and computation for efficiency, the computation for each expert in MoE models is often less than the I/O, resulting in numerous bubbles in the pipeline.
Therefore, we propose Klotski, an efficient MoE inference engine that significantly reduces pipeline bubbles through a novel expert-aware multi-batch pipeline paradigm. The proposed paradigm uses batch processing to extend the computation time of the current layer to overlap with the loading time of the next layer. Although this idea has been effectively applied to dense models, more batches may activate more experts in the MoE, leading to longer loading times and more bubbles. Thus, unlike traditional approaches, we balance computation and I/O time and minimise bubbles by orchestrating their inference orders based on their heterogeneous computation and I/O requirements and activation patterns under different batch numbers. Moreover, to adapt to different hardware environments and models, we design a constraint-sensitive I/O-compute planner and a correlation-aware expert prefetcher for a schedule that minimises pipeline bubbles. Experimental results demonstrate that Klotski achieves a superior throughput-latency trade-off compared to state-of-the-art techniques, with throughput improvements of up to 85.12x. 

**Abstract (ZH)**: 混合专家模型（Mixture of Experts, MoE）因其独特稀疏结构，能够在不显著增加计算成本的情况下，将语言模型扩展至万亿参数级别。然而，巨大的参数量为推断带来了挑战，因为GPU内存的增长无法跟上参数的增长速度。尽管卸载技术利用CPU和磁盘内存，并通过并行I/O和计算提高效率，但MoE模型中的每个专家的计算时间往往少于I/O时间，导致管道中的瓶颈现象增多。

因此，我们提出了Klotski，一种高效MoE推理引擎，通过一种新颖的专家感知多批次管道范式显著减少了管道瓶颈。该范式利用批量处理技术，延长当前层的计算时间，使其与下一层的数据加载时间重叠。虽然此概念已在密集模型中得到了有效应用，但在MoE中，更多的批次可能会激活更多的专家，导致更长的数据加载时间和更多的瓶颈。因此，与传统方法不同，我们通过根据专家在不同批次数量下的异构计算和I/O需求及激活模式来协调其推理顺序，平衡计算时间与I/O时间，从而最小化瓶颈。此外，为了适应不同的硬件环境和模型，我们设计了一种敏感约束的I/O-计算规划器和一种感知相关性的专家预取器，以制定一个减少管道瓶颈的调度方案。实验结果表明，Klotski在吞吐量-延迟权衡上优于现有技术，吞吐量提高了高达85.12倍。 

---
# Gradient Based Method for the Fusion of Lattice Quantizers 

**Title (ZH)**: 基于梯度的方法用于格量化器的融合 

**Authors**: Liyuan Zhang, Hanzhong Cao, Jiaheng Li, Minyang Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06887)  

**Abstract**: In practical applications, lattice quantizers leverage discrete lattice points to approximate arbitrary points in the lattice. An effective lattice quantizer significantly enhances both the accuracy and efficiency of these approximations. In the context of high-dimensional lattice quantization, previous work proposed utilizing low-dimensional optimal lattice quantizers and addressed the challenge of determining the optimal length ratio in orthogonal splicing. Notably, it was demonstrated that fixed length ratios and orthogonality yield suboptimal results when combining low-dimensional lattices. Building on this foundation, another approach employed gradient descent to identify optimal lattices, which inspired us to explore the use of neural networks to discover matrices that outperform those obtained from orthogonal splicing methods. We propose two novel approaches to tackle this problem: the Household Algorithm and the Matrix Exp Algorithm. Our results indicate that both the Household Algorithm and the Matrix Exp Algorithm achieve improvements in lattice quantizers across dimensions 13, 15, 17 to 19, 21, and 22. Moreover, the Matrix Exp Algorithm demonstrates superior efficacy in high-dimensional settings. 

**Abstract (ZH)**: 在实际应用中，格量化器利用离散的格点来近似格中的任意点。有效的格量化器显著提高了这些近似的准确性和效率。在高维格量化的情境下，先前的工作提出了利用低维最优格量化器的方法，并解决了正交拼接中最佳长度比的选择难题。值得注意的是，当结合低维格时，固定长度比和正交性会导致次优结果。在此基础上，另一种方法采用了梯度下降法来识别最优格，这启发我们探索使用神经网络发现超越正交拼接方法获得的矩阵。我们提出了两种新的方法来解决这个问题：Household算法和Matrix Exp算法。我们的结果表明，Household算法和Matrix Exp算法在13、15、17至19、21和22维的空间中都实现了格量化器的改进。此外，Matrix Exp算法在高维设置中显示出更好的效果。 

---
# Topological derivative approach for deep neural network architecture adaptation 

**Title (ZH)**: 拓扑导数方法在深度神经网络架构适应中的应用 

**Authors**: C G Krishnanunni, Tan Bui-Thanh, Clint Dawson  

**Link**: [PDF](https://arxiv.org/pdf/2502.06885)  

**Abstract**: This work presents a novel algorithm for progressively adapting neural network architecture along the depth. In particular, we attempt to address the following questions in a mathematically principled way: i) Where to add a new capacity (layer) during the training process? ii) How to initialize the new capacity? At the heart of our approach are two key ingredients: i) the introduction of a ``shape functional" to be minimized, which depends on neural network topology, and ii) the introduction of a topological derivative of the shape functional with respect to the neural network topology. Using an optimal control viewpoint, we show that the network topological derivative exists under certain conditions, and its closed-form expression is derived. In particular, we explore, for the first time, the connection between the topological derivative from a topology optimization framework with the Hamiltonian from optimal control theory. Further, we show that the optimality condition for the shape functional leads to an eigenvalue problem for deep neural architecture adaptation. Our approach thus determines the most sensitive location along the depth where a new layer needs to be inserted during the training phase and the associated parametric initialization for the newly added layer. We also demonstrate that our layer insertion strategy can be derived from an optimal transport viewpoint as a solution to maximizing a topological derivative in $p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully connected network, convolutional neural network, and vision transformer on various regression and classification problems demonstrate that our proposed approach can outperform an ad-hoc baseline network and other architecture adaptation strategies. Further, we also demonstrate other applications of topological derivative in fields such as transfer learning. 

**Abstract (ZH)**: 本文提出了一种新型算法，用于在深度过程中逐步适应神经网络架构。特别地，我们尝试以一种数学原理为基础的方式解决以下问题：i）在训练过程中，在哪里添加新的容量（层）？ii）如何初始化新的容量？我们方法的核心包括两个关键成分：i）引入一个“形状泛函”来最小化，该泛函依赖于神经网络拓扑结构；ii）引入形状泛函相对于神经网络拓扑结构的拓扑微分。从最优控制的观点出发，我们证明了在某些条件下，网络拓扑微分存在，并推导出其闭式表达式。特别地，我们首次探索了拓扑优化框架中的拓扑微分与最优控制理论中的哈密尔顿量之间的联系。此外，我们证明了形状泛函的最优性条件导至一个针对深层神经架构适应的特征值问题。因此，我们的方法确定了在训练阶段需要插入新层的最敏感位置及新层对应的参数化初始化。我们还演示了，从最优传输的观点出发，我们的层插入策略可以通过最大化$p$-Wasserstein空间中的拓扑微分来求解，其中$p \geq 1$。在各种回归和分类问题上，基于全连接网络、卷积神经网络和视觉转换器的数值研究显示，我们的方法可以优于随机构建的基础网络和其他架构适应策略。此外，我们还展示了拓扑微分在诸如迁移学习等领域中的其他应用。 

---
# Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models 

**Title (ZH)**: 面向大型语言和多模态模型的自适应风险管理的收敛可信区间弃权策略学习 

**Authors**: Sina Tayebati, Divake Kumar, Nastaran Darabi, Dinithi Jayasuriya, Ranganath Krishnan, Amit Ranjan Trivedi  

**Link**: [PDF](https://arxiv.org/pdf/2502.06884)  

**Abstract**: Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used in safety-critical applications, yet their opaque decision-making complicates risk assessment and reliability. Uncertainty quantification (UQ) helps assess prediction confidence and enables abstention when uncertainty is high. Conformal prediction (CP), a leading UQ method, provides statistical guarantees but relies on static thresholds, which fail to adapt to task complexity and evolving data distributions, leading to suboptimal trade-offs in accuracy, coverage, and informativeness. To address this, we propose learnable conformal abstention, integrating reinforcement learning (RL) with CP to optimize abstention thresholds dynamically. By treating CP thresholds as adaptive actions, our approach balances multiple objectives, minimizing prediction set size while maintaining reliable coverage. Extensive evaluations across diverse LLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers (LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%, boosting AUROC for hallucination detection by 22.19%, enhancing uncertainty-guided selective generation (AUARC) by 21.17%, and reducing calibration error by 70%-85%. These improvements hold across multiple models and datasets while consistently meeting the 90% coverage target, establishing our approach as a more effective and flexible solution for reliable decision-making in safety-critical applications. The code is available at: {this https URL}. 

**Abstract (ZH)**: 大型语言模型（LLMs）和多模态语言-视觉模型（VLMs）在安全关键应用中的使用越来越广泛，但它们透明度低的决策过程使得风险评估和可靠性分析变得复杂。不确定性量化（UQ）有助于评估预测的信心，并在不确定性高时允许避免做出决策。可靠的不确定性估计方法之一是校准预测（Conformal Prediction, CP），它提供了统计保证，但依赖于静态阈值，这些阈值无法适应任务复杂度和数据分布的变化，导致在准确性和覆盖率之间存在次优权衡。为了解决这个问题，我们提出了一种可学习的校准预测避免策略，将强化学习（Reinforcement Learning, RL）与CP相结合，以动态优化避免阈值。通过将CP阈值视为可学习的动作，我们的方法能够平衡多重目标，在保持可靠的覆盖率的同时最小化预测集的大小。在多种LLM/VLM基准测试中的详尽评估表明，我们的方法优于最少含糊类别分类器（Least Ambiguous Classifiers, LAC）和自适应预测集（Adaptive Prediction Sets, APS），准确率最高可提高3.2%，对于幻觉检测的AUROC提高了22.19%，对于不确定性引导的选择性生成（AUARC）提高了21.17%，并且减少了70%-85%的校准误差。这些改进在多个模型和数据集上保持一致，并且始终达到90%的覆盖率目标，证明了我们的方法在安全关键应用中更有效、更灵活的决策制定方案。代码可从以下链接获得：{this https URL}。 

---
# Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction 

**Title (ZH)**: 多代理模拟器驱动的语言模型在法律密集型互动中的应用 

**Authors**: Shengbin Yue, Ting Huang, Zheng Jia, Siyuan Wang, Shujun Liu, Yun Song, Xuanjing Huang, Zhongyu Wei  

**Link**: [PDF](https://arxiv.org/pdf/2502.06882)  

**Abstract**: Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants' characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在法律智能方面取得了显著进展，但场景数据的稀缺性阻碍了交互式法律场景的发展。本文介绍了一种多智能体法律模拟驱动器（MASER），通过模拟交互式法律场景，大规模生成合成数据。利用真实的法律案例来源，MASER 确保参与者之间的法律属性一致性，并引入了一个监督机制来使参与者的性格和行为一致，同时解决分心问题。此外，我们构建了一个多阶段交互式法律评估（MILE）基准，以评估LLMs在动态法律场景中的表现。大量实验验证了我们框架的有效性。 

---
# Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging 

**Title (ZH)**: 混合数据还是合并模型？通过模型合并平衡大型语言模型的帮助性、诚实性和无害性 

**Authors**: Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06876)  

**Abstract**: Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI, with existing methods like data mixture strategies facing limitations including reliance on expert knowledge and conflicting optimization signals. While model merging offers a promising alternative by integrating specialized models, its potential for 3H optimization remains underexplored. This paper establishes the first comprehensive benchmark for model merging in 3H-aligned LLMs, systematically evaluating 15 methods (12 training-free merging and 3 data mixture techniques) across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and 2 training paradigms. Our analysis reveals three pivotal insights: (i) previously overlooked collaborative/conflicting relationships among 3H dimensions, (ii) the consistent superiority of model merging over data mixture approaches in balancing alignment trade-offs, and (iii) the critical role of parameter-level conflict resolution through redundant component pruning and outlier mitigation. Building on these findings, we propose R-TSVM, a Reweighting-enhanced Task Singular Vector Merging method that incorporates outlier-aware parameter weighting and sparsity-adaptive rank selection strategies adapted to the heavy-tailed parameter distribution and sparsity for LLMs, further improving LLM alignment across multiple evaluations. Our models will be available at this https URL. 

**Abstract (ZH)**: 在帮助性、诚实性和无害性（3H）方面的平衡对齐是负责任的人工智能（AI）的基石。现有方法如数据混合策略存在依赖专家知识和优化信号冲突等局限性。与此同时，模型合并作为一种有潜力的替代方案，通过集成专门化的模型来进行3H对齐优化，但其潜力尚未被充分探索。本文首次为3H对齐的大语言模型（LLMs）建立了全面的模型合并基准，系统地评估了15种方法（12种无需训练的合并技术和3种数据混合技术），涵盖了10个与5个注释维度、2个LLM家族和2个训练范式相关的数据集。我们的分析揭示了三点关键洞察：(i) 对3H维度之间之前未被注意到的合作/冲突关系的理解，(ii) 在平衡对齐权衡方面，模型合并方法始终优于数据混合方法的持续优势，(iii) 在参数级别通过冗余组件修剪和异常值缓解进行冲突解决的必要性。基于这些发现，我们提出了一种增强的任务奇异向量合并方法R-TSVM，该方法结合了异常值感知的参数加权和针对LLM重尾参数分布和稀疏性的自适应秩选择策略，进一步提高了LLM在多次评估中的对齐程度。我们的模型将会在这个网址**提供下载**。

请注意，将“this https URL”翻译为中文时，需要具体化该网址。如果网址已经给定，应保留原样并适当补充说明，例如：“我们的模型将会在这个网址**提供下载**”。如果有具体网址，则应替换为实际网址。 

---
# Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values 

**Title (ZH)**: 超越视觉：大型语言模型如何根据唤醒-价值值解读面部表情 

**Authors**: Vaibhav Mehra, Guy Laban, Hatice Gunes  

**Link**: [PDF](https://arxiv.org/pdf/2502.06875)  

**Abstract**: Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions-Valence and Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free text affective inference of facial expressions. 

**Abstract (ZH)**: 大型语言模型主要通过文本输入和输出进行操作，而人类情感的传递则通过口头和非口头线索，包括面部表情。尽管视觉语言模型可以从图像中分析面部表情，但它们在资源消耗方面较多，并可能更多地依赖语言先验而不是视觉理解。为了解决这一问题，本研究探讨了大型语言模型是否能够从面部表情的维度（正向性和唤醒度值）中推断情感意义，而不是依赖原始视觉输入。使用Facechannel从面部表情图像中提取VA值，并在两个任务中将这些数值提供给语言模型：(1) 在IIMI数据集上将面部表情分类为基本情感和复杂情感，在Emotic数据集上进行分类；(2) 生成面部表情的语义描述（在Emotic数据集上进行）。分类任务的结果表明，语言模型难以将VA值分类到离散的情感类别中，特别是在基本极性情感之外（如快乐、悲伤）的情感分类上。然而，在语义描述任务中，语言模型生成的文本描述与人类生成的解释高度一致，展示了更强的面向文本的面部表情情感推断能力。 

---
# Group Reasoning Emission Estimation Networks 

**Title (ZH)**: 群组推理排放估计网络 

**Authors**: Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.06874)  

**Abstract**: Accurate greenhouse gas (GHG) emission reporting is critical for governments, businesses, and investors. However, adoption remains limited particularly among small and medium enterprises due to high implementation costs, fragmented emission factor databases, and a lack of robust sector classification methods. To address these challenges, we introduce Group Reasoning Emission Estimation Networks (GREEN), an AI-driven carbon accounting framework that standardizes enterprise-level emission estimation, constructs a large-scale benchmark dataset, and leverages a novel reasoning approach with large language models (LLMs). Specifically, we compile textual descriptions for 20,850 companies with validated North American Industry Classification System (NAICS) labels and align these with an economic model of carbon intensity factors. By reframing sector classification as an information retrieval task, we fine-tune Sentence-BERT models using a contrastive learning loss. To overcome the limitations of single-stage models in handling thousands of hierarchical categories, we propose a Group Reasoning method that ensembles LLM classifiers based on the natural NAICS ontology, decomposing the task into multiple sub-classification steps. We theoretically prove that this approach reduces classification uncertainty and computational complexity. Experiments on 1,114 NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47% Top-10 accuracy), and case studies on 20 companies report a mean absolute percentage error (MAPE) of 45.88%. The project is available at: this https URL. 

**Abstract (ZH)**: 准确报告温室气体（GHG）排放对于政府、企业和投资者至关重要。然而，由于实施成本高、排放因子数据库碎片化以及缺乏稳健的行业分类方法，小企业和中型企业对这一举措的采纳仍然有限。为应对这些挑战，我们引入了Group Reasoning Emission Estimation Networks（GREEN），这是一种以人工智能驱动的碳会计框架，旨在标准化企业层面的排放估算，构建大规模基准数据集，并利用大型语言模型（LLMs）的新型推理方法。具体而言，我们为20,850家经验证的企业编制了文本描述，并将其与碳强度因素的经济模型对齐。通过将行业分类重新定义为信息检索任务，我们使用对比学习损失对Sentence-BERT模型进行微调。为解决单阶段模型处理数千个层次类别时的限制，我们提出了一种Group Reasoning方法，基于自然的NAICS分类体系，使用LLM分类器进行组合，将任务分解为多个子分类步骤。我们证明，这种做法减少了分类不确定性并降低了计算复杂性。在1,114个NAICS类别上的实验结果表明，该方法具有最先进的表现（Top-1精度83.68%，Top-10精度91.47%），并在20家公司的案例研究中报告了平均绝对百分比误差（MAPE）为45.88%。该项目可在以下链接访问：this https URL。 

---
# Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning 

**Title (ZH)**: 基于多步心理治疗推理的多模态认知重塑疗法 

**Authors**: Subin Kim, Hoonrae Kim, Heejin Do, Gary Geunbae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.06873)  

**Abstract**: Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods. 

**Abstract (ZH)**: 以往的研究揭示了大规模语言模型（LLMs）在支持认知重塑疗法方面的潜力；然而，这些研究主要集中在基于文本的方法上，常常忽视了现实中疗法中至关重要的非言语证据。为弥补这一差距，我们扩展了基于文本的认知重塑方法，将其扩展到多模态领域，加入了视觉线索。具体而言，我们提出了一个新的数据集，名为多模态认知支持对话（M2CoSC），该数据集将GPT-4生成的对话与反映虚拟来访者面部表情的图像配对。为了更好地模拟现实生活中的心理治疗，其中面部表情是解读隐含情感证据的关键，我们提出了一种多步心理治疗推理方法，明确地识别和纳入细微的证据。我们对LLMs和视觉语言模型（VLMs）进行全面的实验表明，通过使用M2CoSC数据集，VLMs作为心理治疗师的表现得到了显著提升。此外，多步心理治疗推理方法使得VLMs能够提供更为周到和同理心的建议，优于标准的提示方法。 

---
# Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey 

**Title (ZH)**: 面向大型语言模型的可信赖检索增强生成：一篇综述 

**Authors**: Bo Ni, Zheyuan Liu, Leyao Wang, Yongjia Lei, Yuying Zhao, Xueqi Cheng, Qingkai Zeng, Luna Dong, Yinglong Xia, Krishnaram Kenthapadi, Ryan Rossi, Franck Dernoncourt, Md Mehrab Tanjim, Nesreen Ahmed, Xiaorui Liu, Wenqi Fan, Erik Blasch, Yu Wang, Meng Jiang, Tyler Derr  

**Link**: [PDF](https://arxiv.org/pdf/2502.06872)  

**Abstract**: Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact. 

**Abstract (ZH)**: 检索增强生成（RAG）是一种高级技术，旨在解决人工智能生成内容（AIGC）所面临的挑战。通过将上下文检索融入内容生成过程，RAG能够提供可靠且最新的外部知识，减少幻觉现象，并确保在广泛的任务中保持相关上下文。然而，尽管RAG已经在多个方面取得了成功并展现出巨大的潜力，近期的研究表明，RAG范式也引入了新的风险，包括鲁棒性问题、隐私担忧、对抗性攻击以及问责性问题。解决这些风险对于未来RAG系统的应用至关重要，因为它们直接影响系统的可信度。尽管已经开发出多种方法来提高RAG方法的可信度，但在这一主题上缺乏统一的观点和框架。因此，在本文中，我们旨在通过提供开发可信赖RAG系统的全面路线图来填补这一空白。我们围绕五个关键视角展开讨论：可靠性、隐私、安全性、公平性和可解释性以及问责性。对于每个视角，我们提出了一种通用框架和分类体系，以提供一个结构化的方法来理解当前的挑战、评估现有解决方案，并确定有前途的未来研究方向。为了促进更广泛的应用和创新，我们还强调了可靠的RAG系统在其下游应用中产生的重大影响。 

---
# FlavorDiffusion: Predicting Food Pairings and Chemical Interactions Using Diffusion Models 

**Title (ZH)**: 风味扩散：基于扩散模型预测食物配对与化学交互作用 

**Authors**: Seo Jun Pyo  

**Link**: [PDF](https://arxiv.org/pdf/2502.06871)  

**Abstract**: The study of food pairing has evolved beyond subjective expertise with the advent of machine learning. This paper presents FlavorDiffusion, a novel framework leveraging diffusion models to predict food-chemical interactions and ingredient pairings without relying on chromatography. By integrating graph-based embeddings, diffusion processes, and chemical property encoding, FlavorDiffusion addresses data imbalances and enhances clustering quality. Using a heterogeneous graph derived from datasets like Recipe1M and FlavorDB, our model demonstrates superior performance in reconstructing ingredient-ingredient relationships. The addition of a Chemical Structure Prediction (CSP) layer further refines the embedding space, achieving state-of-the-art NMI scores and enabling meaningful discovery of novel ingredient combinations. The proposed framework represents a significant step forward in computational gastronomy, offering scalable, interpretable, and chemically informed solutions for food science. 

**Abstract (ZH)**: 食物搭配的研究已超越了主观经验的范畴，随着机器学习的发展而不断进步。本文介绍了FlavorDiffusion框架，这是一种利用扩散模型预测食物-化学物质相互作用和食材搭配的新方法，不依赖于色谱技术。通过集成图嵌入、扩散过程和化学性质编码，FlavorDiffusion解决了数据不平衡问题，提高了聚类质量。利用来自如Recipe1M和FlavorDB等数据集的异构图，我们的模型在重构食材-食材关系方面表现出优越的性能。再加上化学结构预测（CSP）层，进一步细化了嵌入空间，实现了最先进的NMI得分，并能够有意义地发现新的食材组合。所提出的方法代表了计算美食学的重要进展，提供了可扩展、可解释且化学信息丰富的解决方案，适用于食品科学领域。 

---
# Bridging Traffic State and Trajectory for Dynamic Road Network and Trajectory Representation Learning 

**Title (ZH)**: 桥接交通状态与轨迹以实现动态道路网络和轨迹表示学习 

**Authors**: Chengkai Han, Jingyuan Wang, Yongyao Wang, Xie Yu, Hao Lin, Chao Li, Junjie Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06870)  

**Abstract**: Effective urban traffic management is vital for sustainable city development, relying on intelligent systems with machine learning tasks such as traffic flow prediction and travel time estimation. Traditional approaches usually focus on static road network and trajectory representation learning, and overlook the dynamic nature of traffic states and trajectories, which is crucial for downstream tasks. To address this gap, we propose TRACK, a novel framework to bridge traffic state and trajectory data for dynamic road network and trajectory representation learning. TRACK leverages graph attention networks (GAT) to encode static and spatial road segment features, and introduces a transformer-based model for trajectory representation learning. By incorporating transition probabilities from trajectory data into GAT attention weights, TRACK captures dynamic spatial features of road segments. Meanwhile, TRACK designs a traffic transformer encoder to capture the spatial-temporal dynamics of road segments from traffic state data. To further enhance dynamic representations, TRACK proposes a co-attentional transformer encoder and a trajectory-traffic state matching task. Extensive experiments on real-life urban traffic datasets demonstrate the superiority of TRACK over state-of-the-art baselines. Case studies confirm TRACK's ability to capture spatial-temporal dynamics effectively. 

**Abstract (ZH)**: 有效的城市交通管理对于可持续城市发展至关重要，这依赖于智能系统中的机器学习任务，如交通流量预测和旅行时间估计。传统方法通常侧重于静态的道路网络和轨迹表示学习，而忽视了交通状态和轨迹的动态特性，这对于下游任务而言是至关重要的。为了解决这一差距，我们提出了一种名为TRACK的新颖框架，用于动态道路网络和轨迹表示学习，以连接交通状态数据和轨迹数据。TRACK利用图注意力网络（GAT）编码静态和空间路段特征，并引入基于Transformer的模型进行轨迹表示学习。通过将轨迹数据中的转移概率纳入GAT的注意力权重中，TRACK捕捉到了道路路段的动态空间特征。同时，TRACK设计了一个交通Transformer编码器，用于从交通状态数据中捕捉道路路段的空间-时间动态特性。为了进一步增强动态表示，TRACK提出了一种共注意力Transformer编码器和轨迹-交通状态匹配任务。在现实城市交通数据集上的广泛实验表明，与最先进的基线方法相比，TRACK具有明显优势。案例研究证实了TRACK能够有效捕捉空间-时间动态特性。 

---
# A Survey on Explainable Deep Reinforcement Learning 

**Title (ZH)**: 可解释的深度强化学习综述 

**Authors**: Zelei Cheng, Jiahao Yu, Xinyu Xing  

**Link**: [PDF](https://arxiv.org/pdf/2502.06869)  

**Abstract**: Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making tasks across diverse domains, yet its reliance on black-box neural architectures hinders interpretability, trust, and deployment in high-stakes applications. Explainable Deep Reinforcement Learning (XRL) addresses these challenges by enhancing transparency through feature-level, state-level, dataset-level, and model-level explanation techniques. This survey provides a comprehensive review of XRL methods, evaluates their qualitative and quantitative assessment frameworks, and explores their role in policy refinement, adversarial robustness, and security. Additionally, we examine the integration of reinforcement learning with Large Language Models (LLMs), particularly through Reinforcement Learning from Human Feedback (RLHF), which optimizes AI alignment with human preferences. We conclude by highlighting open research challenges and future directions to advance the development of interpretable, reliable, and accountable DRL systems. 

**Abstract (ZH)**: 深度强化学习（DRL）已在多个领域的序列决策任务中取得了显著成功，但其对黑盒神经架构的依赖阻碍了其在高风险应用中的可解释性、信任度和部署。可解释的深度强化学习（XRL）通过特征层面、状态层面、数据集层面以及模型层面的解释技术增强了透明度，从而应对这些挑战。本文综述了XRL方法，评估了它们的定性和定量评估框架，并探讨了它们在政策细化、对抗鲁棒性和安全领域的作用。此外，我们还研究了强化学习与大规模语言模型（LLMs）的整合，特别是通过人类反馈强化学习（RLHF）来优化AI与人类偏好的对齐。最后，我们强调了剩余的研究挑战和未来方向，旨在推进可解释、可靠和问责的DRL系统的发展。 

---
# Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject 

**Title (ZH)**: 相关的知识扰动事项：重新审视相同主题下的多件知识编辑 

**Authors**: Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, Xueqi Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2502.06868)  

**Abstract**: Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on Same-Subject Editing, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the $\text{S}^2\text{RKE}$(Same-Subject Related Knowledge Editing) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit "related knowledge perturbation," where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness. 

**Abstract (ZH)**: 知识编辑已成为高效精确更新大型语言模型（LLMs）中嵌入知识的一种有前景的方法。在这项工作中，我们聚焦于同主题编辑（Same-Subject Editing），其涉及修改单个实体的多个属性，以确保对以实体为中心的知识进行全面且一致的更新。初步观察揭示了一个显著的挑战：当前最先进的编辑方法在处理同一主题的多个相关知识片段的编辑任务时表现不佳。为了解决传统基准中缺少同一主题相关的编辑数据问题，我们引入了$\text{S}^2\text{RKE}$（同主题相关知识编辑）基准。我们的广泛实验表明，只有主流的定位-编辑方法，如ROME和MEMIT，存在“相关知识扰动”的现象，即后续的编辑干扰了早期的编辑。进一步分析显示，这些方法过度依赖主题信息，忽视了其他关键因素，从而降低了编辑的有效性。 

---
# Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests 

**Title (ZH)**: 禁用科学：双重用途AI挑战基准与科学抵制试验 

**Authors**: David Noever, Forrest McKee  

**Link**: [PDF](https://arxiv.org/pdf/2502.06867)  

**Abstract**: The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse. 

**Abstract (ZH)**: 建立稳健的安全基准对于大型语言模型来说需要开放、可复现的数据集，这些数据集能够衡量适当拒绝有害内容和潜在过度限制合法科学讨论的能力。我们提出一个开源数据集和测试框架，用于评估主要针对受控物质查询的LLM安全机制，并分析了四种主要模型在系统变化提示下的响应。结果显示不同的安全性能：Claude-3.5-sonnet表现最为保守，拒绝率为73%，允许率为27%；Mistral尝试回答所有查询；GPT-3.5-turbo表现适中，拒绝率为10%，允许率为90%；Grok-2的拒绝率和允许率分别为20%和80%。测试不同的提示变体策略显示回应一致性逐渐下降，从单一提示下的85%一致性降到多变体下的65%一致性。这个公开可用的基准为系统地评估必要安全限制与过度审查合法科学探究之间的关键平衡提供了基础，同时为衡量AI安全实施进展提供了依据。通过chain-of-thought分析揭示了安全机制中的潜在漏洞，突显了在不不合理限制有益且合理的科学讨论的前提下实施稳健保障措施的复杂性。 

---
# Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies 

**Title (ZH)**: 全球宜居指数：一种用于主要经济体 longitudinal 分析的机器学习框架 

**Authors**: Tanay Panat, Rohitash Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2502.06866)  

**Abstract**: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment. 

**Abstract (ZH)**: 全球经济、地缘政治条件以及诸如新冠肺炎疫情等干扰的急剧变化对生活成本和生活质量产生了影响。理解主要经济体中长期的生活成本和生活质量是至关重要的。一个透明而全面的居住指数必须涵盖多个生活条件的维度。在本研究中，我们通过综合各种社会经济和基础设施因素来量化生活质量，提出了一个结合全球宜居指数的方法。我们利用定义生活水平的经济指标，这有助于针对具体领域实施针对性干预。我们还提供了一种机器学习框架来解决某些经济指标在特定国家缺失数据的问题。然后，我们整理和更新了数据，并采用降维方法（主成分分析）来构建自1970年以来主要经济体的宜居指数。我们的研究为政策制定者提供了一个实用工具，以识别需要改进的领域，如卫生保健体系、就业机会和公共安全等。我们使用开放数据和代码的方法可以轻松复制和应用于各种情境中。这种透明度和可访问性使我们的研究成为持续研究和生活质量评估政策制定中的一项宝贵资源。 

---
# Knowledge Graph-Guided Retrieval Augmented Generation 

**Title (ZH)**: 知识图谱指导的检索增强生成 

**Authors**: Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06864)  

**Abstract**: Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality. 

**Abstract (ZH)**: 检索增强生成（RAG）已成为解决大型语言模型（LLMs）生成答复中幻觉问题的一种有前途的技术。现有的RAG研究主要集中在使用基于语义的方法检索孤立的相关片段，但忽略了这些片段之间的内在关系。本文我们提出了一种新的知识图谱引导的检索增强生成（KG$^2$RAG）框架，该框架利用知识图谱（KGs）为片段之间的事实级关系提供支持，从而提高检索结果的多样性和一致性。具体而言，在进行基于语义的检索以提供种子片段之后，KG$^2$RAG使用一种基于知识图谱的片段扩展过程和一种基于知识图谱的片段组织过程，以有序的段落形式提供相关且重要的知识。在HotpotQA数据集及其变体上进行的广泛实验证明，与现有RAG方法相比，KG$^2$RAG在答复质量和检索质量方面都具有优势。 

---
# BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks 

**Title (ZH)**: BF-GAN：基于生成对抗网络的气泡流图像生成模型的研究 

**Authors**: Wen Zhou, Shuichiro Miwa, Yang Liu, Koji Okamoto  

**Link**: [PDF](https://arxiv.org/pdf/2502.06863)  

**Abstract**: A generative AI architecture called bubbly flow generative adversarial networks (BF-GAN) is developed, designed to generate realistic and high-quality bubbly flow images through physically conditioned inputs, jg and jf. Initially, 52 sets of bubbly flow experiments under varying conditions are conducted to collect 140,000 bubbly flow images with physical labels of jg and jf for training data. A multi-scale loss function is then developed, incorporating mismatch loss and pixel loss to enhance the generative performance of BF-GAN further. Regarding evaluative metrics of generative AI, the BF-GAN has surpassed conventional GAN. Physically, key parameters of bubbly flow generated by BF-GAN are extracted and compared with measurement values and empirical correlations, validating BF-GAN's generative performance. The comparative analysis demonstrate that the BF-GAN can generate realistic and high-quality bubbly flow images with any given jg and jf within the research scope.
BF-GAN offers a generative AI solution for two-phase flow research, substantially lowering the time and cost required to obtain high-quality data. In addition, it can function as a benchmark dataset generator for bubbly flow detection and segmentation algorithms, enhancing overall productivity in this research domain. The BF-GAN model is available online (this https URL). 

**Abstract (ZH)**: 一种生成AI架构，即泡沫流生成对抗网络（BF-GAN），被开发出来，旨在通过物理条件输入jg和jf生成逼真且高质量的泡沫流图像。首先，进行了52组不同条件下的泡沫流实验，收集了140,000张带有物理标签jg和jf的泡沫流图像，作为训练数据。随后，开发了一种多尺度损失函数，结合了不匹配损失和像素损失，进一步提升了BF-GAN的生成性能。

在生成AI评价指标方面，BF-GAN超越了传统的GAN。从物理角度看，从BF-GAN生成的泡沫流关键参数被提取并与其他测量值和经验关联进行了比较，验证了BF-GAN的生成性能。比较分析表明，BF-GAN可以在研究范围内生成任何给定jg和jf的逼真且高质量的泡沫流图像。

BF-GAN为两相流研究提供了一种生成AI解决方案，极大地降低了获取高质量数据所需的时间和成本。此外，BF-GAN还可以作为泡沫流检测和分割算法的基准数据集生成器，从而提高该研究领域整体的生产力。BF-GAN模型已上线（请在此处提供网址）。 

---
# Design Considerations in Offline Preference-based RL 

**Title (ZH)**: 基于离线偏好强化学习的设计考虑 

**Authors**: Alekh Agarwal, Christoph Dann, Teodor V. Marinov  

**Link**: [PDF](https://arxiv.org/pdf/2502.06861)  

**Abstract**: Offline algorithms for Reinforcement Learning from Human Preferences (RLHF), which use only a fixed dataset of sampled responses given an input, and preference feedback among these responses, have gained increasing prominence in the literature on aligning language models. In this paper, we study how the different design choices made in methods such as DPO, IPO, SLiC and many variants influence the quality of the learned policy, from a theoretical perspective. Our treatment yields insights into the choices of loss function, the policy which is used to normalize log-likelihoods, and also the role of the data sampling policy. Notably, our results do not rely on the standard reparameterization-style arguments used to motivate some of the algorithms in this family, which allows us to give a unified treatment to a broad class of methods. We also conduct a small empirical study to verify some of the theoretical findings on a standard summarization benchmark. 

**Abstract (ZH)**: 基于人类偏好的离线增强学习算法（RLHF），这类算法仅使用给定输入的固定数据集中的采样响应及其偏好反馈，近年来在使语言模型对齐的研究文献中日益受到关注。本文中，我们从理论角度研究了诸如DPO、IPO、SLiC及其许多变种方法中不同设计选择对学习策略质量的影响。我们的研究得出了一些关于损失函数的选择、用于归一化对数似然性的策略以及数据采样策略作用的重要见解。尤为值得注意的是，我们的结果并不依赖于用于解释这类算法中某些算法的标准重参数化论证，这使得我们可以对一个广泛的算法类别提供统一的分析。此外，我们还进行了一项小型实验研究，以验证某些理论发现对标准摘要基准的表现。 

---
# LLM-Supported Natural Language to Bash Translation 

**Title (ZH)**: LLM 支持的自然语言到 Bash 脚本翻译 

**Authors**: Finnian Westenfelder, Erik Hemberg, Miguel Tulla, Stephen Moskal, Una-May O'Reilly, Silviu Chiricescu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06858)  

**Abstract**: The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning, and constrained decoding can improve NL2SH accuracy by up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation. Our code is available at this https URL 

**Abstract (ZH)**: Linux系统中的Bourne-Again Shell（Bash）命令行界面具有复杂的语法，需要深厚的专业知识。通过大型语言模型（LLMs）的自然语言到Bash命令（NL2SH）转换能力进行命令组合可以避免这些问题。然而，由于测试数据不准确和确定Bash命令功能等价的可靠启发式方法缺乏，评估NL2SH性能十分困难。我们提供了一个手动验证的测试数据集，包含600个指令-命令对，以及一个训练数据集，包含40,939个对，分别比之前的数据集增大了441%和135%。此外，我们还提出了一个新颖的功能等价启发式方法，将命令执行与LLM对命令输出的评估结合在一起。该启发式方法可以在95%的信心水平下确定两个Bash命令的功能等价性，相比于之前的启发式方法提高了16%。使用我们的测试数据集和启发式方法评估流行的LLMs表明，解析、上下文学习、基于权重的学习和约束解码可以将NL2SH准确性提高多达32%。我们的研究结果强调了数据集质量、基于执行的评估和转换方法在推动NL2SH发展中的重要性。我们的代码可在以下链接访问：[此处提供链接] 

---
# Gemstones: A Model Suite for Multi-Faceted Scaling Laws 

**Title (ZH)**: 宝石：一种多面向scaling law的模型套件 

**Authors**: Sean McLeish, John Kirchenbauer, David Yu Miller, Siddharth Singh, Abhinav Bhatele, Micah Goldblum, Ashwinee Panda, Tom Goldstein  

**Link**: [PDF](https://arxiv.org/pdf/2502.06857)  

**Abstract**: Scaling laws are typically fit using a family of models with a narrow range of frozen hyper-parameter choices. In this work we study scaling laws using a wide range of architecture and hyper-parameter choices, and highlight their impact on resulting prescriptions. As a primary artifact of our research, we release the Gemstones: the most comprehensive open-source scaling law dataset to date, consisting of over 4000 checkpoints from transformers with up to 2 billion parameters; these models have been trained with different learning rates, cooldown schedules, and architectural shapes. Our checkpoints enable more complex studies of scaling, such as a law that predicts language modeling performance as a function of model width and depth. By examining the various facets of our model suite, we find that the prescriptions of scaling laws can be highly sensitive to the experimental design process and the specific model checkpoints used during fitting. Code: this https URL 

**Abstract (ZH)**: 通常，比例法则会通过限制在窄范围内的固定超参数选项的模型族来进行拟合。在本研究中，我们考察了广泛的各种架构和超参数选择，以揭示它们对最终结论的影响。作为研究的主要成果之一，我们发布了“Gemstones”：迄今最全面的开源比例法则数据集，包含超过4000个参数量高达20亿的变压器模型检查点；这些模型在不同的学习率、冷却调度和架构形状下进行了训练。我们的检查点使更复杂的比例法则研究成为可能，例如一种预测模型宽度和深度与语言建模性能之间关系的法则。通过考察我们模型套件的各个方面的相关性，我们发现比例法则的建议对实验设计过程和拟合中使用的特定模型检查点高度敏感。代码：[此链接](this https URL) 

---
# Self-Supervised Prompt Optimization 

**Title (ZH)**: 自我监督的提示优化 

**Authors**: Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu, Yuyu Luo  

**Link**: [PDF](https://arxiv.org/pdf/2502.06855)  

**Abstract**: Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at this https URL. 

**Abstract (ZH)**: 精心设计的提示对于增强大型语言模型（LLMs）的推理能力并使其输出符合跨不同领域的任务需求至关重要。然而，手动设计提示需要专业知识和反复试验。尽管现有的提示优化方法试图自动化这一过程，但它们高度依赖于诸如真实数据或人工提供的外部参考，这在现实世界场景中往往无法获得或成本高昂。为解决这一问题，我们提出了一种成本效益高的框架——自监督提示优化（SPO），该框架能够在无需外部参考的情况下，发现适用于闭合和开放任务的有效提示。通过观察到提示质量直接体现在LLM的输出中，并且LLMs能够有效评估对任务需求的符合度，我们仅通过输出比较来提取评估和优化信号。具体来说，SPO 通过LLM评估器实现的成对输出比较来选择更优的提示，然后通过LLM优化器将输出与任务需求对齐。广泛实验表明，SPO 在性能上超过了最先进的提示优化方法，用显著更低的成本（例如，达到现有方法的1.1%至5.6%）和更少的样本量（例如，仅需要三个样本）取得了可比或更优的结果。相关代码可在以下网址获得：[提供网址处]。 

---
# Can Large Language Models Understand Intermediate Representations? 

**Title (ZH)**: 大语言模型能否理解中间表示？ 

**Authors**: Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan  

**Link**: [PDF](https://arxiv.org/pdf/2502.06854)  

**Abstract**: Intermediate Representations (IRs) are essential in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. This paper presents a pioneering empirical study to investigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA 3.1, and Code Llama, in understanding IRs. We analyze their performance across four tasks: Control Flow Graph (CFG) reconstruction, decompilation, code summarization, and execution reasoning. Our results indicate that while LLMs demonstrate competence in parsing IR syntax and recognizing high-level structures, they struggle with control flow reasoning, execution semantics, and loop handling. Specifically, they often misinterpret branching instructions, omit critical IR operations, and rely on heuristic-based reasoning, leading to errors in CFG reconstruction, IR decompilation, and execution reasoning. The study underscores the necessity for IR-specific enhancements in LLMs, recommending fine-tuning on structured IR datasets and integration of explicit control flow models to augment their comprehension and handling of IR-related tasks. 

**Abstract (ZH)**: 中间表示（IRs）在编译器设计和程序分析中至关重要，但大型语言模型（LLMs）对其理解能力尚未得到充分探索。本文进行了先驱性的实证研究，旨在探讨包括GPT-4、GPT-3、Gemma 2、LLaMA 3.1和Code Llama在内的LLMs在理解IR方面的能力。我们评估了它们在四个任务中的表现，分别是控制流图（CFG）重构、反汇编、代码总结和执行推理。研究结果表明，虽然LLMs在解析IR语法和识别高级结构方面表现出一定的能力，但在控制流推理、执行语义和循环处理方面存在困难。具体而言，它们经常误解读分支指令，忽略关键的IR操作，并依赖基于启发式的推理，这导致了CFG重构、IR反汇编和执行推理中的错误。研究强调了LLMs在IR方面的特定增强的必要性，推荐在结构化IR数据集上进行微调，并将显式的控制流模型整合进来，以增强其对IR相关任务的理解和处理能力。 

---
# Native Fortran Implementation of TensorFlow-Trained Deep and Bayesian Neural Networks 

**Title (ZH)**: TensorFlow 训练的深度神经网络和贝叶斯神经网络的原生 Fortran 实现 

**Authors**: Aidan Furlong, Xingang Zhao, Bob Salko, Xu Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06853)  

**Abstract**: Over the past decade, the investigation of machine learning (ML) within the field of nuclear engineering has grown significantly. With many approaches reaching maturity, the next phase of investigation will determine the feasibility and usefulness of ML model implementation in a production setting. Several of the codes used for reactor design and assessment are primarily written in the Fortran language, which is not immediately compatible with TensorFlow-trained ML models. This study presents a framework for implementing deep neural networks (DNNs) and Bayesian neural networks (BNNs) in Fortran, allowing for native execution without TensorFlow's C API, Python runtime, or ONNX conversion. Designed for ease of use and computational efficiency, the framework can be implemented in any Fortran code, supporting iterative solvers and UQ via ensembles or BNNs. Verification was performed using a two-input, one-output test case composed of a noisy sinusoid to compare Fortran-based predictions to those from TensorFlow. The DNN predictions showed negligible differences and achieved a 19.6x speedup, whereas the BNN predictions exhibited minor disagreement, plausibly due to differences in random number generation. An 8.0x speedup was noted for BNN inference. The approach was then further verified on a nuclear-relevant problem predicting critical heat flux (CHF), which demonstrated similar behavior along with significant computational gains. Discussion regarding the framework's successful integration into the CTF thermal-hydraulics code is also included, outlining its practical usefulness. Overall, this framework was shown to be effective at implementing both DNN and BNN model inference within Fortran, allowing for the continued study of ML-based methods in real-world nuclear applications. 

**Abstract (ZH)**: 在过去的十年中，核工程领域内的机器学习（ML）研究取得了显著增长。随着许多方法达到成熟阶段，下一阶段的研究将确定ML模型在实际生产环境中的可行性及其 usefulness。用于反应堆设计和评估的许多代码主要是用Fortran语言编写的，这与由TensorFlow训练的ML模型不直接兼容。本文提出了一种在Fortran中实现深度神经网络（DNNs）和贝叶斯神经网络（BNNs）的框架，使得可以在无需使用TensorFlow的C API、Python运行时或ONNX转换的情况下进行原生执行。该框架设计易于使用且计算效率高，可以在任何Fortran代码中实现，并支持迭代求解器和不确定性量化（UQ）通过集合或BNNs。通过一个包含两个输入和一个输出的测试案例（噪声正弦波），使用Fortran和TensorFlow的预测结果进行了验证，结果显示DNN预测几乎没有差异，并实现了19.6倍的速度提升，而BNN预测显示出轻微的分歧，这可能是由于随机数生成的不同导致的。BNN推理速度提升了8.0倍。随后，该方法在预测关键热流密度（CHF）的核相关问题上进行了进一步验证，展示了类似的行为并实现了显著的计算增益。文中还讨论了该框架成功集成到CTF热工水力代码中的情况，概述了其实用价值。总体而言，本文展示了该框架在Fortran中有效实现DNN和BNN模型推理的能力，使得可以继续在实际核应用中研究基于ML的方法。 

---
# EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification 

**Title (ZH)**: EAP-GP：减轻基于梯度的自动电路识别中的饱和效应 

**Authors**: Lin Zhang, Wenshuo Dong, Zhuoran Zhang, Shu Yang, Lijie Hu, Ninghao Liu, Pan Zhou, Di Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06852)  

**Abstract**: Understanding the internal mechanisms of transformer-based language models remains challenging. Mechanistic interpretability based on circuit discovery aims to reverse engineer neural networks by analyzing their internal processes at the level of computational subgraphs. In this paper, we revisit existing gradient-based circuit identification methods and find that their performance is either affected by the zero-gradient problem or saturation effects, where edge attribution scores become insensitive to input changes, resulting in noisy and unreliable attribution evaluations for circuit components. To address the saturation effect, we propose Edge Attribution Patching with GradPath (EAP-GP), EAP-GP introduces an integration path, starting from the input and adaptively following the direction of the difference between the gradients of corrupted and clean inputs to avoid the saturated region. This approach enhances attribution reliability and improves the faithfulness of circuit identification. We evaluate EAP-GP on 6 datasets using GPT-2 Small, GPT-2 Medium, and GPT-2 XL. Experimental results demonstrate that EAP-GP outperforms existing methods in circuit faithfulness, achieving improvements up to 17.7%. Comparisons with manually annotated ground-truth circuits demonstrate that EAP-GP achieves precision and recall comparable to or better than previous approaches, highlighting its effectiveness in identifying accurate circuits. 

**Abstract (ZH)**: 理解基于变压器的语言模型的内部机制仍然具有挑战性。基于电路发现的机制可解释性旨在通过分析神经网络内部过程的计算子图来逆向工程这些网络。在本文中，我们回顾了现有的基于梯度的电路识别方法，发现它们的性能要么受到零梯度问题的影响，要么受到饱和效应的影响，即边归因得分对输入变化变得不敏感，导致电路部件的归因评估结果存在噪音且不可靠。为了解决饱和效应，我们提出了GradPath边归因修补（EAP-GP）方法。EAP-GP引入了一条集成路径，从输入开始，适应性地跟随被污染输入和干净输入梯度差异的方向，以避免饱和区域。这种方法增强了归因的可靠性并提高了电路识别的真实性。我们使用GPT-2 Small、GPT-2 Medium和GPT-2 XL在6个数据集上评估了EAP-GP。实验结果表明，EAP-GP在电路真实性方面优于现有方法，取得了最高17.7%的改进。将EAP-GP与手动标注的真实电路进行比较，发现EAP-GP的精度和召回率优于或至少与先前方法相当，突显了其在准确电路识别方面的有效性。 

---
# Survey on Vision-Language-Action Models 

**Title (ZH)**: 视觉-语言-动作模型综述 

**Authors**: Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Daulet Baimukashev, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yermakhan Kassym, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Yerkebulan Massalim, Zerde Nurbayeva, Zhanat Kappassov  

**Link**: [PDF](https://arxiv.org/pdf/2502.06851)  

**Abstract**: This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable. 

**Abstract (ZH)**: 本文presented 一项基于AI生成的Vision-Language-Action (VLA)模型综述，总结了关键方法、研究发现和未来方向。内容是使用大型语言模型（LLMs）生成的，仅用于演示目的。本研究不涉及原始研究，而是展示了AI如何帮助自动化文献综述的过程。随着AI生成内容越来越多，确保其准确性、可靠性和适当整合仍然是一个挑战。未来的研究将侧重于发展一种结构化的框架，以辅助AI辅助文献综述，探索提高引文准确性、来源可信度和情境理解的技术。通过分析大型语言模型在学术写作中的潜在能力和局限性，本研究旨在促进将AI集成到研究工作流程中的更广泛讨论。本研究是朝着建立系统化方法来利用AI生成文献综述的一个初步步骤，旨在使学术知识整合更为高效和可扩展。 

---
# Model Fusion via Neuron Transplantation 

**Title (ZH)**: 通过神经元移植实现模型融合 

**Authors**: Muhammed Öz, Nicholas Kiefer, Charlotte Debus, Jasmin Hörter, Achim Streit, Markus Götz  

**Link**: [PDF](https://arxiv.org/pdf/2502.06849)  

**Abstract**: Ensemble learning is a widespread technique to improve the prediction performance of neural networks. However, it comes at the price of increased memory and inference time. In this work we propose a novel model fusion technique called \emph{Neuron Transplantation (NT)} in which we fuse an ensemble of models by transplanting important neurons from all ensemble members into the vacant space obtained by pruning insignificant neurons. An initial loss in performance post-transplantation can be quickly recovered via fine-tuning, consistently outperforming individual ensemble members of the same model capacity and architecture. Furthermore, NT enables all the ensemble members to be jointly pruned and jointly trained in a combined model. Comparing it to alignment-based averaging (like Optimal-Transport-fusion), it requires less fine-tuning than the corresponding OT-fused model, the fusion itself is faster and requires less memory, while the resulting model performance is comparable or better. The code is available under the following link: this https URL. 

**Abstract (ZH)**: 集成学习是一种广泛使用的提高神经网络预测性能的技术，但这也伴随着对内存和推理时间的增加。在这项工作中，我们提出了一种名为“神经元移植（Neuron Transplantation, NT）”的新型模型融合技术，在该技术中，我们通过将所有模型成员中重要的神经元移植到剪除不重要神经元后获得的空位中来融合这些模型。移植后可能会暂时降低性能，但可以通过微调迅速恢复，最终始终优于相同模型容量和架构的单一模型成员。此外，NT 允许所有模型成员在联合模型中共同进行剪枝和训练。与基于对齐的平均（如最优运输融合）相比，它所需的微调较少，融合本身速度更快且所需内存更少，而融合后的模型性能则相当或更好。代码可在以下链接获取：[this https URL]。 

---
# Transfer learning in Scalable Graph Neural Network for Improved Physical Simulation 

**Title (ZH)**: 在可扩展图神经网络中迁移学习以改进物理仿真 

**Authors**: Siqi Shen, Yu Liu, Daniel Biggs, Omar Hafez, Jiandong Yu, Wentao Zhang, Bin Cui, Jiulong Shan  

**Link**: [PDF](https://arxiv.org/pdf/2502.06848)  

**Abstract**: In recent years, Graph Neural Network (GNN) based models have shown promising results in simulating physics of complex systems. However, training dedicated graph network based physics simulators can be costly, as most models are confined to fully supervised training, which requires extensive data generated from traditional physics simulators. To date, how transfer learning could improve the model performance and training efficiency has remained unexplored. In this work, we introduce a pre-training and transfer learning paradigm for graph network simulators. We propose the scalable graph U-net (SGUNET). Incorporating an innovative depth-first search (DFS) pooling, the SGUNET is adaptable to different mesh sizes and resolutions for various simulation tasks. To enable the transfer learning between differently configured SGUNETs, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance. To pre-train our physics simulator we created a dataset which includes 20,000 physical simulations of randomly selected 3D shapes from the open source A Big CAD (ABC) dataset. We show that our proposed transfer learning methods allow the model to perform even better when fine-tuned with small amounts of training data than when it is trained from scratch with full extensive dataset. On the 2D Deformable Plate benchmark dataset, our pre-trained model fine-tuned on 1/16 of the training data achieved an 11.05\% improvement in position RMSE compared to the model trained from scratch. 

**Abstract (ZH)**: 近年来，基于图神经网络（GNN）的模型在模拟复杂系统的物理现象方面取得了令人鼓舞的结果。然而，训练专门针对物理模拟的图网络模型成本较高，大多数模型局限于全监督训练，这需要从传统的物理模拟器生成大量数据。到目前为止，如何通过迁移学习来提升模型性能和训练效率尚未得到研究。在本工作中，我们引入了一种针对图网络模拟器的预训练和迁移学习范式。我们提出了可扩展的图U-NET（SGUNET）。通过结合一种创新的深度优先搜索（DFS）池化方法，SGUNET 可以适应不同网格大小和分辨率的多种模拟任务。为了使不同配置的SGUNET之间实现迁移学习，我们提出了一组映射函数来对齐预训练模型和目标模型的参数。我们还在损失中添加了一个额外的规范化项，以限制预训练权重和目标模型权重之间的差异，从而提高泛化性能。为了预训练我们的物理模拟器，我们创建了一个包含来自开源A Big CAD（ABC）数据集中随机选择的20,000个3D形状的物理模拟数据集。我们展示了我们提出的迁移学习方法在使用少量训练数据微调时，模型的表现甚至比完全从头开始使用大规模数据集训练更好。在2D可变形板基准数据集上，我们的预训练模型在使用训练数据的1/16进行微调后，位置RMSE提高了11.05%。 

---
# Prot2Chat: Protein LLM with Early Fusion of Sequence and Structure 

**Title (ZH)**: Prot2Chat：融合序列和结构的早期融合蛋白质LLM 

**Authors**: Zhicong Wang, Zicheng Ma, Ziqiang Cao, Changlong Zhou, Jun Zhang, Yiqin Gao  

**Link**: [PDF](https://arxiv.org/pdf/2502.06846)  

**Abstract**: Proteins play a pivotal role in living organisms, yet understanding their functions presents significant challenges, including the limited flexibility of classification-based methods, the inability to effectively leverage spatial structural information, and the lack of systematic evaluation metrics for protein Q&A systems. To address these limitations, we propose Prot2Chat, a novel framework that integrates multimodal protein representations with natural language through a unified module, enabling large language model (LLM)-driven answer generation. Our model incorporates a modified ProteinMPNN encoder, which encodes protein sequence and structural information in a unified manner, a protein-text adapter with cross-attention mechanisms, and a LLaMA3 decoder. To optimize training efficiency, we freeze the encoder and employ LoRA techniques for the decoder. We conducted experiments on two datasets, both automated metrics and expert evaluations demonstrate the superior performance of our model. Furthermore, zero-shot prediction results highlight its strong generalization capabilities. This framework offers a promising solution for bridging protein domain knowledge with natural language understanding, paving the way for transformative advancements in protein-related research. 

**Abstract (ZH)**: 蛋白质在生物体中扮演着关键角色，然而理解其功能面临着诸多挑战，包括基于分类的方法灵活性有限、难以有效利用空间结构信息以及缺乏系统评估蛋白质问答系统的指标。为了解决这些限制，我们提出了一种名为Prot2Chat的新框架，该框架通过统一模块将多模态蛋白质表示与自然语言相整合，从而实现大型语言模型（LLM）驱动的答案生成。该模型包含一种修改后的ProteinMPNN编码器，该编码器以统一的方式编码蛋白质序列和结构信息，一种蛋白质文本适配器，配备了跨注意机制，以及一个LLaMA3解码器。为了提高训练效率，我们冻结了编码器并使用LoRA技术优化了解码器。我们在两个数据集上进行了实验，自动化评价指标和专家评估结果都表明了我们模型的优越性能。此外，零样本预测结果突显了其强大的泛化能力。该框架为连接蛋白质领域知识与自然语言理解提供了有前景的解决方案，为蛋白质相关研究的突破性进展铺平了道路。 

---
# DiffNMR3: Advancing NMR Resolution Beyond Instrumental Limits 

**Title (ZH)**: DiffNMR3: 超越仪器限制提升核磁共振分辨率 

**Authors**: Sen Yan, Etienne Goffinet, Fabrizio Gabellieri, Ryan Young, Lydia Gkoura, Laurence Jennings, Filippo Castiglione, Thomas Launey  

**Link**: [PDF](https://arxiv.org/pdf/2502.06845)  

**Abstract**: Nuclear Magnetic Resonance (NMR) spectroscopy is a crucial analytical technique used for molecular structure elucidation, with applications spanning chemistry, biology, materials science, and medicine. However, the frequency resolution of NMR spectra is limited by the "field strength" of the instrument. High-field NMR instruments provide high-resolution spectra but are prohibitively expensive, whereas lower-field instruments offer more accessible, but lower-resolution, results. This paper introduces an AI-driven approach that not only enhances the frequency resolution of NMR spectra through super-resolution techniques but also provides multi-scale functionality. By leveraging a diffusion model, our method can reconstruct high-field spectra from low-field NMR data, offering flexibility in generating spectra at varying magnetic field strengths. These reconstructions are comparable to those obtained from high-field instruments, enabling finer spectral details and improving molecular characterization. To date, our approach is one of the first to overcome the limitations of instrument field strength, achieving NMR super-resolution through AI. This cost-effective solution makes high-resolution analysis accessible to more researchers and industries, without the need for multimillion-dollar equipment. 

**Abstract (ZH)**: 核磁共振（NMR）光谱学是一种用于分子结构解析的关键分析技术，广泛应用于化学、生物学、材料科学和医学领域。然而，NMR光谱的频率分辨率受限于仪器的“磁场强度”。高场NMR仪器提供高分辨率光谱，但价格过于昂贵，而低场仪器则提供了更易于获取，但分辨率较低的结果。本文介绍了一种基于AI的方法，不仅通过超分辨率技术增强了NMR光谱的频率分辨率，还提供了多尺度功能。通过利用扩散模型，我们的方法可以从低场NMR数据中重构高场光谱，实现了不同磁场强度下的光谱生成灵活性。这些重构的光谱在分辨率和细节解析上与高场仪器相当，提高了分子表征的精确度。迄今为止，我们的方法是首个克服仪器磁场强度限制的解决方案，通过AI实现了NMR超分辨率。这一经济有效的方案使得更多研究人员和行业能够获得高分辨率分析，无需依赖数百万美元的设备。 

---
# Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization 

**Title (ZH)**: 探索离散搜索在超低比特量化中的模型不变性研究 

**Authors**: Yuqiao Wen, Yanshuai Cao, Lili Mou  

**Link**: [PDF](https://arxiv.org/pdf/2502.06844)  

**Abstract**: Large language models have been increasing in size due to their success in a wide range of applications. This calls for a pressing need to reduce memory usage to make them more accessible. Post-training quantization is a popular technique which uses fewer bits (e.g., 4--8 bits) to represent the model without retraining it. However, it remains a challenging task to perform quantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we propose InvarExplore, a unified framework that systematically explores different model invariance at the same time, allowing us to take advantage of the synergy between each type of invariance. Importantly, InvarExplore features a discrete search algorithm that enables us to explore permutation invariance, which is under-studied as it cannot be optimized with gradient-based methods. Results show that InvarExplore is compatible with existing state-of-the-art methods, achieving an add-on performance improvement over strong competing methods. 

**Abstract (ZH)**: 大型语言模型因其在广泛应用领域中的成功而不断增大规模。这要求我们迫切需要减少其内存使用量，以使其更加易于获取。后训练量化是一种流行的 technique，它通过使用更少的位数（例如，4-8 位）来表示模型，而无需重新训练模型。然而，在超低位设置（例如，2 位）下进行量化仍然是一个具有挑战性的任务。本文提出了一种统一框架 InvarExplore，该框架系统地探索不同的模型不变性，从而使我们能够利用每种不变性之间的协同效应。重要的是，InvarExplore 具有离散搜索算法，使我们能够探索排列不变性，这种不变性由于无法通过梯度方法优化而很少被研究。实验结果表明，InvarExplore 与现有的先进方法兼容，相较于强竞争对手，其能够实现额外的性能提升。 

---
# Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation 

**Title (ZH)**: 面向自主驾驶辅助的视觉集成大规模语言模型：人类性能比较与信任评估 

**Authors**: Namhee Kim, Woojin Park  

**Link**: [PDF](https://arxiv.org/pdf/2502.06843)  

**Abstract**: Traditional autonomous driving systems often struggle with reasoning in complex, unexpected scenarios due to limited comprehension of spatial relationships. In response, this study introduces a Large Language Model (LLM)-based Autonomous Driving (AD) assistance system that integrates a vision adapter and an LLM reasoning module to enhance visual understanding and decision-making. The vision adapter, combining YOLOv4 and Vision Transformer (ViT), extracts comprehensive visual features, while GPT-4 enables human-like spatial reasoning and response generation. Experimental evaluations with 45 experienced drivers revealed that the system closely mirrors human performance in describing situations and moderately aligns with human decisions in generating appropriate responses. 

**Abstract (ZH)**: 传统自动驾驶系统在应对复杂和不可预见场景时往往难以进行有效的推理，主要是因为其对空间关系的理解有限。为解决这一问题，本研究提出了一种基于大型语言模型（LLM）的自动驾驶（Autonomous Driving, AD）辅助系统，该系统整合了视觉适配器和LLM推理模块，以增强视觉理解和决策能力。视觉适配器结合了YOLOv4和视觉变压器（Vision Transformer, ViT），提取全面的视觉特征，而GPT-4则实现类似人类的空间推理和响应生成。实验评估了45名经验丰富驾驶员的表现，结果显示该系统在描述场景方面与人类表现非常接近，在生成适当响应方面与人类决策有一定的对齐度。 

---
# Integrating Generative Artificial Intelligence in ADRD: A Framework for Streamlining Diagnosis and Care in Neurodegenerative Diseases 

**Title (ZH)**: 将生成型人工智能集成到阿尔茨海默病及相关疾病的管理中：一种简化诊断和护理的框架 

**Authors**: Andrew G. Breithaupt, Alice Tang, Bruce L. Miller, Pedro Pinheiro-Chagas  

**Link**: [PDF](https://arxiv.org/pdf/2502.06842)  

**Abstract**: Healthcare systems are struggling to meet the growing demand for neurological care, with challenges particularly acute in Alzheimer's disease and related dementias (ADRD). While artificial intelligence research has often focused on identifying patterns beyond human perception, implementing such predictive capabilities remains challenging as clinicians cannot readily verify insights they cannot themselves detect. We propose that large language models (LLMs) offer more immediately practical applications by enhancing clinicians' capabilities in three critical areas: comprehensive data collection, interpretation of complex clinical information, and timely application of relevant medical knowledge. These challenges stem from limited time for proper diagnosis, growing data complexity, and an overwhelming volume of medical literature that exceeds any clinician's capacity to fully master. We present a framework for responsible AI integration that leverages LLMs' ability to communicate effectively with both patients and providers while maintaining human oversight. This approach prioritizes standardized, high-quality data collection to enable a system that learns from every patient encounter while incorporating the latest clinical evidence, continuously improving care delivery. We begin to address implementation challenges and initiate important discussions around ethical considerations and governance needs. While developed for ADRD, this roadmap provides principles for responsible AI integration across neurology and other medical specialties, with potential to improve diagnostic accuracy, reduce care disparities, and advance clinical knowledge through a learning healthcare system. 

**Abstract (ZH)**: 医疗系统正努力应对不断增长的神经科护理需求，特别是在阿尔茨海默病及相关痴呆症（ADRD）方面面临的挑战尤为严峻。尽管人工智能研究往往集中于发现超出人类感知的模式，但在临床领域实施此类预测能力仍然极具挑战性，因为临床医生难以验证他们自己无法察觉的见解。我们建议大规模语言模型（LLMs）能够提供更直接的实际应用，通过在三个关键方面增强临床医生的能力：全面的数据收集、复杂临床信息的解释以及相关医学知识的及时应用。

这些挑战源于诊断时间有限、数据复杂性增加以及难以全面掌握海量医学文献。我们提出了一个负责任的人工智能整合框架，利用LLMs与患者和提供者有效沟通的能力，同时保持人类的监督。该方法强调标准化和高质量的数据收集，以实现一个能够从每次患者就诊中学习的系统，并不断整合最新的临床证据，从而持续改进护理质量。我们从实施挑战入手，初步探讨了伦理考量和治理需求的重要讨论点。

尽管该框架最初为ADRD设计，但其提供的一般原则适用于神经病学及其他医学专科领域的负责任的人工智能整合，有可能通过学习型医疗系统提高诊断准确性、减少护理差异，并推动临床知识的发展。 

---
# A Hybrid Model for Weakly-Supervised Speech Dereverberation 

**Title (ZH)**: 一种混合模型用于弱监督说话人除混响处理 

**Authors**: Louis Bahrman, Mathieu Fontaine, Gael Richard  

**Link**: [PDF](https://arxiv.org/pdf/2502.06839)  

**Abstract**: This paper introduces a new training strategy to improve speech dereverberation systems using minimal acoustic information and reverberant (wet) speech. Most existing algorithms rely on paired dry/wet data, which is difficult to obtain, or on target metrics that may not adequately capture reverberation characteristics and can lead to poor results on non-target metrics. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. The system's output is resynthesized using a generated room impulse response and compared with the original reverberant speech, providing a novel reverberation matching loss replacing the standard target metrics. During inference, only the trained dereverberation model is used. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics used in speech dereverberation than the state-of-the-art. 

**Abstract (ZH)**: 本文介绍了一种新的训练策略，旨在利用最少的声学信息和混响（潮湿）语音来改进语音去混响系统。现有的大部分算法依赖于配对的干/湿数据，这些数据难以获取，或者依赖于目标指标，这些指标可能无法充分捕捉混响特性，从而导致在非目标指标上的表现不佳。我们的方法使用有限的声学信息，如混响时间（RT60），来训练去混响系统。系统的输出通过生成的房间冲激响应重新合成，并与原始混响语音进行比较，从而提供了一种新颖的混响匹配损失，替代标准的目标指标。在推理过程中，仅使用训练好的去混响模型。实验结果表明，与当前最先进的方法相比，我们的方法在使用于语音去混响的各种客观指标中表现更为一致。 

---
# CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction 

**Title (ZH)**: CAST：基于交叉注意力的结构与文本多模态融合方法在材料性质预测中的应用 

**Authors**: Jaewan Lee, Changyoung Park, Hongjun Yang, Sungbin Lim, Sehui Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.06836)  

**Abstract**: Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9\% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information. 

**Abstract (ZH)**: 近年来，人工智能的进步彻底改变了材料科学中的属性预测，并加速了新材料的发现。图神经网络（GNNs）因其能够将晶体结构表示为图，有效地捕获局部交互并提供优越的预测能力而脱颖而出。然而，这些方法往往损失了重要的全局信息，如晶体系统和重复单元的连接性。为了解决这一问题，我们提出了CAST，这是一种基于交叉注意力的多模态融合模型，该模型将图和文本模态结合在一起以保留关键的材料信息。CAST 使用交叉注意力机制结合节点级和标记级特征，超越了依赖于材料级嵌入（如图均值池化或[CLS]标记）的先前方法。通过掩码节点预测预训练策略进一步增强了原子级信息的集成。在包括带隙在内的四种晶体属性中，我们的方法相对于 CrysMMNet 和 MultiMat 方法实现了高达 22.9% 的属性预测性能提升。预训练是关键，它使节点和文本嵌入对齐，注意力图证实了其在捕捉节点和标记之间关系方面的有效性。本研究突显了多模态学习在材料科学中的潜力，为结合局部和全局信息的更稳健预测模型铺平了道路。 

---
# A Unified Knowledge-Distillation and Semi-Supervised Learning Framework to Improve Industrial Ads Delivery Systems 

**Title (ZH)**: 一个统一的知识精炼和半监督学习框架，以改进工业广告交付系统 

**Authors**: Hamid Eghbalzadeh, Yang Wang, Rui Li, Yuji Mo, Qin Ding, Jiaxiang Fu, Liang Dai, Shuo Gu, Nima Noorshams, Sem Park, Bo Long, Xue Feng  

**Link**: [PDF](https://arxiv.org/pdf/2502.06834)  

**Abstract**: Industrial ads ranking systems conventionally rely on labeled impression data, which leads to challenges such as overfitting, slower incremental gain from model scaling, and biases due to discrepancies between training and serving data. To overcome these issues, we propose a Unified framework for Knowledge-Distillation and Semi-supervised Learning (UKDSL) for ads ranking, empowering the training of models on a significantly larger and more diverse datasets, thereby reducing overfitting and mitigating training-serving data discrepancies. We provide detailed formal analysis and numerical simulations on the inherent miscalibration and prediction bias of multi-stage ranking systems, and show empirical evidence of the proposed framework's capability to mitigate those. Compared to prior work, UKDSL can enable models to learn from a much larger set of unlabeled data, hence, improving the performance while being computationally efficient. Finally, we report the successful deployment of UKDSL in an industrial setting across various ranking models, serving users at multi-billion scale, across various surfaces, geological locations, clients, and optimize for various events, which to the best of our knowledge is the first of its kind in terms of the scale and efficiency at which it operates. 

**Abstract (ZH)**: 工业广告排序系统传统上依赖标记的印象数据，这导致了过拟合、模型扩展带来的增量收益减缓以及由于训练数据与服务数据之间的差异而产生的偏见等问题。为了克服这些问题，我们提出了一种统一的知识蒸馏与半监督学习框架（UKDSL），该框架能够使模型在更大规模和更多样化的数据集上进行训练，从而减少过拟合并缓解训练数据与服务数据之间的差异。我们对多阶段排序系统的固有失校准和预测偏见进行了详细的正式分析和数值模拟，并提供了证据证明该框架能够在这些方面实现有效缓解。与先前的工作相比，UKDSL可以使得模型从更大的未标记数据集中进行学习，从而在保持计算效率的同时提高性能。最后，我们在多个工业场景中成功部署了UKDSL，包括各种排序模型，服务于数十亿规模的用户，覆盖不同的界面、地理区域、客户和各种事件，据我们所知，这是迄今为止在规模和效率方面独一无二的部署案例。 

---
# Entropy Adaptive Decoding: Dynamic Model Switching for Efficient Inference 

**Title (ZH)**: 自适应熵解码：高效推理的动态模型切换 

**Authors**: Toby Simonds  

**Link**: [PDF](https://arxiv.org/pdf/2502.06833)  

**Abstract**: We present Entropy Adaptive Decoding (EAD), a novel approach for efficient language model inference that dynamically switches between different-sized models based on prediction uncertainty. By monitoring rolling entropy in model logit distributions, our method identifies text regions where a smaller model suffices and switches to a larger model only when prediction uncertainty exceeds a threshold. Unlike speculative decoding approaches that maintain perfect output fidelity through verification, EAD accepts controlled output divergence in exchange for computational efficiency. Our experiments on the MATH benchmark demonstrate remarkable efficiency gains across different model families. Using the LLaMA family, we maintain 96.7\% of the 11B model's performance (50.4\% vs 52.1\%) while using it for only 43\% of tokens, decreasing computational cost by 41.5\%. These gains become more pronounced with larger size differentials in the Qwen family, where we achieve 92.9\% of the 14B model's performance (74.3\% vs 80.0\%) while using it for just 25\% of tokens, decreasing computational cost by 67\%. The consistency of these results across model pairs suggests that language model computation can be significantly optimized by selectively deploying model capacity based on local generation complexity. Our findings indicate that current approaches to model inference may be unnecessarily conservative in their pursuit of perfect output fidelity, and that accepting minor performance trade-offs can enable dramatic reductions in computational costs. 

**Abstract (ZH)**: 我们将介绍熵自适应解码（Entropy Adaptive Decoding, EAD），这是一种基于预测不确定性动态切换不同大小模型的新型高效语言模型推理方法。通过监控模型输出分布中的滚动熵，该方法能够识别出一个小型模型足以处理的文本区域，并且只有当预测不确定性超过阈值时才切换到大型模型。与推测性解码方法通过验证保持完美输出一致性的做法不同，EAD 通过接受可控的输出差异来换取计算效率。我们在 MATH 基准测试中展示了不同模型家族中的显著效率提升。使用 LLaMA 家族模型时，我们能够保持 11B 模型 96.7% 的性能（50.4% 对 52.1%），但只使用了其中 43% 的令牌，计算成本降低了 41.5%。在 Qwen 家族模型中，我们实现了 14B 模型 92.9% 的性能（74.3% 对 80.0%），但只使用了其中 25% 的令牌，计算成本降低了 67%。这些结果在不同模型对之间的前后一致表明，根据局部生成复杂性选择性部署模型容量可以显著优化语言模型计算。我们的研究发现，当前的语言模型推理方法在追求完美输出一致性方面可能过于保守，接受一些轻微的性能权衡可以实现显著的计算成本降低。 

---
# Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach 

**Title (ZH)**: 优化混合专家模型的健壮性和准确性：一种双模型方法 

**Authors**: Xu Zhang, Kaidi Xu, Ziqing Hu, Ren Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06832)  

**Abstract**: Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMoE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods. 

**Abstract (ZH)**: 混合专家（MoE）在利用专业化专家网络处理复杂机器学习任务方面取得了显著的成果。然而，它们对对抗攻击的高度敏感性是其在鲁棒应用中部署的关键挑战。本文探讨了如何在保持高自然准确性的前提下，将鲁棒性融入MoE中。我们首先分析了MoE组件的脆弱性，发现专家网络对对抗攻击的敏感性明显高于路由器。基于这一洞察，我们提出了一种针对性的鲁棒训练技术，通过集成一种新颖的损失函数来增强MoE的对抗鲁棒性，仅需对一个额外的专家进行鲁棒化处理，而不影响训练或推理效率。在此基础上，我们介绍了双模型策略，该策略通过平滑参数线性结合标准MoE模型和我们鲁棒化后的MoE模型。这种方法允许灵活控制鲁棒性与准确性的权衡。我们进一步提供了理论基础，通过推导出单MoE和双模型的认证鲁棒性界。为了在鲁棒性和准确性上取得突破，我们提出了针对双模型的新型联合训练策略JTDMoE。这种联合训练策略同时提升了鲁棒性和准确性，而单独模型无法实现这种效果。在使用ResNet18和Vision Transformer（ViT）架构的CIFAR-10和TinyImageNet数据集上进行的实验结果证明了我们提出方法的有效性。 

---
# No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data 

**Title (ZH)**: 遍及每个地点：衡量和改进地球数据中隐含表示的公平性 

**Authors**: Daniel Cai, Randall Balestriero  

**Link**: [PDF](https://arxiv.org/pdf/2502.06831)  

**Abstract**: Implicit neural representations (INRs) exhibit growing promise in addressing Earth representation challenges, ranging from emissions monitoring to climate modeling. However, existing methods disproportionately prioritize global average performance, whereas practitioners require fine-grained insights to understand biases and variations in these models. To bridge this gap, we introduce FAIR-Earth: a first-of-its-kind dataset explicitly crafted to examine and challenge inequities in Earth representations. FAIR-Earth comprises various high-resolution Earth signals and uniquely aggregates extensive metadata along stratifications like landmass size and population density to assess the fairness of models. Evaluating state-of-the-art INRs across the various modalities of FAIR-Earth, we uncover striking performance disparities. Certain subgroups, especially those associated with high-frequency signals (e.g., islands, coastlines), are consistently poorly modeled by existing methods. In response, we propose spherical wavelet encodings, building on previous spatial encoding research. Leveraging the multi-resolution capabilities of wavelets, our encodings yield consistent performance over various scales and locations, offering more accurate and robust representations of the biased subgroups. These open-source contributions represent a crucial step towards the equitable assessment and deployment of Earth INRs. 

**Abstract (ZH)**: 隐神经表示（INRs）在应对地球表示挑战方面展现出 growing 的潜力，涵盖了从排放监测到气候建模等多个领域。然而，现有方法更多地侧重于全局平均性能，而实践者需要精细的见解来理解这些模型中的偏差和变化。为弥合这一差距，我们提出了 FAIR-Earth：一个开创性的数据集，旨在明确检验和挑战地球表示中的不平等现象。FAIR-Earth 包含各种高分辨率地球信号，并独特地沿陆地面积和人口密度等多种分层聚合了大量元数据，以评估模型的公平性。通过对 FAIR-Earth 各种模态下的最强 INRs 进行评估，我们揭示了显著的性能差异。某些子群体，尤其是与高频信号相关的群体（如岛屿、海岸线），已被现有方法一致地建模不佳。为此，我们提出了一种球面小波编码方法，作为先前空间编码研究的延续。凭借小波的多分辨率能力，我们的编码方法在不同的尺度和位置上提供了稳定且一致的性能，从而为那些被偏差影响的子群体提供更准确且更具稳健性的表示。这些开放源代码的贡献代表了朝着公平评估和部署地球 INRs 方向迈出的关键一步。 

---
# OrderFusion: Encoding Orderbook for Probabilistic Intraday Price Prediction 

**Title (ZH)**: OrderFusion：Encoded Order簿概率日内价格预测 

**Authors**: Runyao Yu, Yuchen Tao, Fabian Leimgruber, Tara Esterl, Jochen L. Cremer  

**Link**: [PDF](https://arxiv.org/pdf/2502.06830)  

**Abstract**: Efficient and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods often suffer from parameter inefficiencies, as they fail to fully exploit the potential of modeling interdependencies between bids and offers in the orderbook, requiring a large number of parameters for representation learning. Furthermore, these methods face the quantile crossing issue, where upper quantiles fall below the lower quantiles, resulting in unreliable probabilistic predictions. To address these two challenges, we propose an encoding method called OrderFusion and design a hierarchical multi-quantile head. The OrderFusion encodes the orderbook into a 2.5D representation, which is processed by a tailored jump cross-attention backbone to capture the interdependencies of bids and offers, enabling parameter-efficient learning. The head sets the median quantile as an anchor and predicts multiple quantiles hierarchically, ensuring reliability by enforcing monotonicity between quantiles through non-negative functions. Extensive experiments and ablation studies are conducted on four price indices: 60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID1 using the German orderbook over three years to ensure a fair evaluation. The results confirm that our design choices improve overall performance, offering a parameter-efficient and reliable solution for probabilistic intraday price prediction. 

**Abstract (ZH)**: 高效的且可靠的日内电价概率预测对于管理市场不确定性并支持稳健的交易策略至关重要。然而，当前的方法往往存在参数效率低下的问题，因为它们未能充分利用订单簿中报单和要约之间相互依赖性的潜在能力，因而需要大量的参数来进行表示学习。此外，这些方法还面临着梯度穿越问题，即高分位数落在低分位数之下，导致概率预测不可靠。为了解决这两个挑战，我们提出了一种名为OrderFusion的编码方法，并设计了一个分层多分位数头部。OrderFusion将订单簿编码为2.5维表示，并通过定制的跳跃交叉注意力骨干网络处理，以捕捉报单和要约之间的相互依赖性，从而实现参数高效的学习。分层多分位数头部以中位数分位数为锚点，分层预测多个分位数，通过非负函数确保分位数之间的一致性，从而保证可靠性。在德国订单簿数据上对四个价格指数（60分钟ID3、60分钟ID1、15分钟ID3和15分钟ID1）进行了为期三年的数据实验和消融研究，以确保公平评估。实验结果证实，我们的设计选择提升了整体性能，提供了一种参数高效且可靠的日内电价概率预测解决方案。 

---
# Convolution-Based Converter : A Weak-Prior Approach For Modeling Stochastic Processes Based On Conditional Density Estimation 

**Title (ZH)**: 基于卷积的变换器：一种基于条件密度估计的弱先验方法建模随机过程 

**Authors**: Chaoran Pang, Shuangrong Liu, Shikun Tian, WenHao Yue, Xingshen Zhang, Lin Wang, Bo Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06829)  

**Abstract**: In this paper, a Convolution-Based Converter (CBC) is proposed to develop a methodology for removing the strong or fixed priors in estimating the probability distribution of targets based on observations in the stochastic process. Traditional approaches, e.g., Markov-based and Gaussian process-based methods, typically leverage observations to estimate targets based on strong or fixed priors (such as Markov properties or Gaussian prior). However, the effectiveness of these methods depends on how well their prior assumptions align with the characteristics of the problem. When the assumed priors are not satisfied, these approaches may perform poorly or even become unusable. To overcome the above limitation, we introduce the Convolution-Based converter (CBC), which implicitly estimates the conditional probability distribution of targets without strong or fixed priors, and directly outputs the expected trajectory of the stochastic process that satisfies the constraints from observations. This approach reduces the dependence on priors, enhancing flexibility and adaptability in modeling stochastic processes when addressing different problems. Experimental results demonstrate that our method outperforms existing baselines across multiple metrics. 

**Abstract (ZH)**: 在本文中，我们提出了一种卷积基转换器（Convolution-Based Converter, CBC）方法，旨在开发一种在基于观察值估计目标的概率分布时去除强先验或固定先验的方法，尤其是在随机过程中。传统的方法，例如基于马尔可夫的和基于高斯过程的方法，通常会利用观察值来基于强先验（如马尔可夫性质或高斯先验）来估计目标。然而，这些方法的有效性依赖于它们的先验假设与问题特征的匹配程度。当假设的先验不满足时，这些方法可能会表现不佳甚至不可用。为克服这一局限性，我们引入了卷积基转换器（CBC），该方法隐含地估计目标的条件概率分布而不需要强先验或固定先验，并直接输出满足观察约束的随机过程期望轨迹。这种方法减少了对先验的依赖性，从而在处理不同类型的问题时增强了建模随机过程的灵活性和适应性。实验结果表明，我们的方法在多项指标上优于现有基准方法。 

---
# Fine-Tuning Strategies for Continual Online EEG Motor Imagery Decoding: Insights from a Large-Scale Longitudinal Study 

**Title (ZH)**: 大规模 longitudinal 研究中连续在线 EEG 运动想象解码的微调策略：见解 

**Authors**: Martin Wimpff, Bruno Aristimunha, Sylvain Chevallier, Bin Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.06828)  

**Abstract**: This study investigates continual fine-tuning strategies for deep learning in online longitudinal electroencephalography (EEG) motor imagery (MI) decoding within a causal setting involving a large user group and multiple sessions per participant. We are the first to explore such strategies across a large user group, as longitudinal adaptation is typically studied in the single-subject setting with a single adaptation strategy, which limits the ability to generalize findings. First, we examine the impact of different fine-tuning approaches on decoder performance and stability. Building on this, we integrate online test-time adaptation (OTTA) to adapt the model during deployment, complementing the effects of prior fine-tuning. Our findings demonstrate that fine-tuning that successively builds on prior subject-specific information improves both performance and stability, while OTTA effectively adapts the model to evolving data distributions across consecutive sessions, enabling calibration-free operation. These results offer valuable insights and recommendations for future research in longitudinal online MI decoding and highlight the importance of combining domain adaptation strategies for improving BCI performance in real-world applications. Clinical Relevance: Our investigation enables more stable and efficient long-term motor imagery decoding, which is critical for neurorehabilitation and assistive technologies. 

**Abstract (ZH)**: 本文研究了在网络纵向脑电图（EEG）运动想象（MI）解码中，在因果设置下针对大规模用户群体和每个参与者多次会诊情况下的持续微调策略。我们首次探索了这样的策略在大规模用户群体中的应用，因为在单用户设置中，纵向适应通常是通过单一适应策略进行研究的，这限制了发现的普适性。首先，我们探讨了不同的微调方法对解码器性能和稳定性的影响。在此基础上，我们整合了在线测试时适应（OTTA）策略，使其在部署过程中适应模型，以补充先前微调的效果。我们的研究结果显示，逐步基于先前的用户特定信息的微调不仅能提高性能，还能增强稳定性，而OTTA能够有效适应连续会诊中不断变化的数据分布，从而实现无需校准的操作。这些结果为纵向在线MI解码的未来研究提供了宝贵的见解和建议，并强调了结合领域适应策略以提高BCI性能的重要性，特别是在实际应用中的重要性。临床相关性：我们的研究使长期稳定的运动想象解码成为可能，这对于神经康复和辅助技术至关重要。 

---
# Learning to Synthesize Compatible Fashion Items Using Semantic Alignment and Collocation Classification: An Outfit Generation Framework 

**Title (ZH)**: 利用语义对齐和搭配分类学习合成兼容的时尚单品：一套服装生成框架 

**Authors**: Dongliang Zhou, Haijun Zhang, Kai Yang, Linlin Liu, Han Yan, Xiaofei Xu, Zhao Zhang, Shuicheng Yan  

**Link**: [PDF](https://arxiv.org/pdf/2502.06827)  

**Abstract**: The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits in order to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this paper, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module, which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module, which is used to improve the compatibility of a synthesized outfit. In order to evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20,000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform state-of-the-art methods in terms of similarity, authenticity and compatibility measurements. 

**Abstract (ZH)**: 近年来，时尚兼容性学习领域受到了学术界和工业界的广泛关注。许多研究致力于时尚兼容性预测、配对服装推荐、人工智能（AI）辅助的兼容时尚设计及其相关话题。特别是，AI辅助的兼容时尚设计可以用来合成兼容的服装单品或成套服装，以提高设计者的体验或为客户提供更有效的推荐。然而，以往用于配对服装合成的生成模型通常集中在上下装服装图像之间的互译上。在本文中，我们提出了一种创新的成套服装生成框架，即OutfitGAN，旨在给定一个现有的时尚单品和目标合成单品的参考掩码时，合成一系列互补单品以组成完整的成衣。OutfitGAN包括一个语义对齐模块，该模块负责描述现有单品与合成单品之间的映射关系，以提高合成图像的质量，以及一个配对分类模块，用于提高合成成套服装的兼容性。为了评估我们提出模型的性能，我们构建了一个包含20,000套服装的大规模数据集。在该数据集上的大量实验结果表明，我们的OutfitGAN可以生成逼真的成套服装，在相似度、真实性及兼容性测量方面均优于现有最先进的方法。 

---
# Transferring Graph Neural Networks for Soft Sensor Modeling using Process Topologies 

**Title (ZH)**: 将图神经网络转移学习应用于过程拓扑结构的软传感器建模 

**Authors**: Maximilian F. Theisen, Gabrie M. H. Meesters, Artur M. Schweidtmann  

**Link**: [PDF](https://arxiv.org/pdf/2502.06826)  

**Abstract**: Data-driven soft sensors help in process operations by providing real-time estimates of otherwise hard- to-measure process quantities, e.g., viscosities or product concentrations. Currently, soft sensors need to be developed individually per plant. Using transfer learning, machine learning-based soft sensors could be reused and fine-tuned across plants and applications. However, transferring data-driven soft sensor models is in practice often not possible, because the fixed input structure of standard soft sensor models prohibits transfer if, e.g., the sensor information is not identical in all plants. We propose a topology-aware graph neural network approach for transfer learning of soft sensor models across multiple plants. In our method, plants are modeled as graphs: Unit operations are nodes, streams are edges, and sensors are embedded as attributes. Our approach brings two advantages for transfer learning: First, we not only include sensor data but also crucial information on the plant topology. Second, the graph neural network algorithm is flexible with respect to its sensor inputs. This allows us to model data from different plants with different sensor networks. We test the transfer learning capabilities of our modeling approach on ammonia synthesis loops with different process topologies. We build a soft sensor predicting the ammonia concentration in the product. After training on data from one process, we successfully transfer our soft sensor model to a previously unseen process with a different topology. Our approach promises to extend the data-driven soft sensors to cases to leverage data from multiple plants. 

**Abstract (ZH)**: 基于数据的软传感器通过提供难以直接测量的过程量的实时估计（如黏度或产品浓度），有助于过程操作。目前，软传感器需要针对每个工厂单独开发。通过迁移学习，基于机器学习的软传感器可以在多个工厂和应用中重新使用并微调。然而，实际中转移数据驱动的软传感器模型通常是不可能的，因为标准软传感器模型固定输入结构阻止了不同工厂之间传输，例如，由于传感器信息在所有工厂中并不相同。为此，我们提出了一种拓扑感知的图神经网络方法，用于在多个工厂之间迁移软传感器模型。在我们的方法中，工厂被建模为图：单元操作为节点，流为边，传感器嵌入为属性。我们的方法在迁移学习方面带来了两个优势：首先，我们不仅包括传感器数据，还包含了与工厂拓扑结构密切相关的关键信息。其次，图神经网络算法在传感器输入方面具有灵活性。这使得我们可以使用不同的传感器网络来模型来自不同工厂的数据。我们使用具有不同工艺拓扑的氨合成回路来测试我们建模方法的迁移学习能力。我们构建了一个预测产品中氨浓度的软传感器。经过一个工艺的数据训练后，我们的软传感器模型成功地转移到了具有不同拓扑的未见过的工艺中。我们的方法有望将数据驱动的软传感器扩展到多个工厂数据利用的场景中。 

---
# Neural Network-based Vehicular Channel Estimation Performance: Effect of Noise in the Training Set 

**Title (ZH)**: 基于神经网络的车辆信道估计性能：训练集中噪声的影响 

**Authors**: Simbarashe Aldrin Ngorima, Albert Helberg, Marelie H. Davel  

**Link**: [PDF](https://arxiv.org/pdf/2502.06824)  

**Abstract**: Vehicular communication systems face significant challenges due to high mobility and rapidly changing environments, which affect the channel over which the signals travel. To address these challenges, neural network (NN)-based channel estimation methods have been suggested. These methods are primarily trained on high signal-to-noise ratio (SNR) with the assumption that training a NN in less noisy conditions can result in good generalisation. This study examines the effectiveness of training NN-based channel estimators on mixed SNR datasets compared to training solely on high SNR datasets, as seen in several related works. Estimators evaluated in this work include an architecture that uses convolutional layers and self-attention mechanisms; a method that employs temporal convolutional networks and data pilot-aided estimation; two methods that combine classical methods with multilayer perceptrons; and the current state-of-the-art model that combines Long-Short-Term Memory networks with data pilot-aided and temporal averaging methods as post processing. Our results indicate that using only high SNR data for training is not always optimal, and the SNR range in the training dataset should be treated as a hyperparameter that can be adjusted for better performance. This is illustrated by the better performance of some models in low SNR conditions when trained on the mixed SNR dataset, as opposed to when trained exclusively on high SNR data. 

**Abstract (ZH)**: 车辆通信系统面临着由于高移动性和快速变化的环境而导致的诸多挑战，这影响了信号传输的信道特性。为了应对这些挑战，建议使用基于神经网络（Neural Network, NN）的信道估计方法。这些方法主要是在高频信噪比（Signal-to-Noise Ratio, SNR）条件下进行训练，假设在低噪声条件下训练NN可以实现良好的泛化性能。本研究探讨了在混合SNR数据集上训练基于NN的信道估计算法的有效性，这在之前的一些相关工作中有所体现。本研究中评估的估计算法包括：一种使用卷积层和自注意力机制的架构；一种使用时序卷积网络和数据试点辅助估计的方法；两种结合经典方法与多层感知机的方法；以及当前最先进的模型，该模型结合了长短期记忆网络（Long-Short-Term Memory, LSTM）与数据试点辅助估计和时序平均处理方法。

我们的结果显示，仅使用高频SNR数据进行训练并不总是最优选择，训练数据集中SNR范围应被视为一个超参数，并可以根据性能需求进行调整。这一结论在混合SNR数据集上训练某些模型时表现出更高的低SNR条件下性能，这一点与仅在高频SNR数据上进行训练的情况形成了鲜明对比。

翻译如下：

车辆通信系统由于高移动性和快速变化的环境面临着诸多挑战，这些因素影响了信号传输的信道特性。为了应对这些挑战，建议使用基于神经网络（Neural Network, NN）的信道估计方法。这些方法主要是在高频信噪比（Signal-to-Noise Ratio, SNR）条件下进行训练，假设在低噪声条件下训练NN可以实现良好的泛化性能。本研究探讨了在混合SNR数据集上训练基于NN的信道估计算法的有效性，这在之前的一些相关工作中有所体现。本研究中评估的估计算法包括：一种使用卷积层和自注意力机制的架构；一种使用时序卷积网络和数据试点辅助估计的方法；两种结合经典方法与多层感知机的方法；以及当前最先进的模型，该模型结合了长短期记忆网络（Long-Short-Term Memory, LSTM）与数据试点辅助估计和时序平均处理方法。

我们的结果表明，仅使用高频SNR数据进行训练并非总是最优选择，因此训练数据集中SNR范围应被视为一个超参数，可以根据性能需要进行调整。这一结论通过某些模型在混合SNR数据集上的低SNR条件下表现出更高的性能，与仅在高频SNR数据上进行训练的情况相比更加清晰地得到了证明。 

---
# LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning 

**Title (ZH)**: LoCA：基于位置的余弦适应性参数高效微调 

**Authors**: Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, Mingming Gong  

**Link**: [PDF](https://arxiv.org/pdf/2502.06820)  

**Abstract**: Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain approximation with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods. 

**Abstract (ZH)**: 低秩适应（LoRA）已成为将预训练的大型语言模型适应下游任务的一种流行方法。然而，简单的低秩分解形式可能会限制假设空间。为解决这一限制，我们引入了位置感知余弦适应（LoCA），这是一种基于逆离散余弦变换（iDCT）与可学习组件选择性位置结合的全新的频域参数高效微调方法。我们首先对频域和低秩分解这两种微调预训练大规模模型的方法进行了全面的理论比较。我们的分析表明，通过谨慎选择频率分量进行频域近似可以超越基于传统低秩方法的表达能力。此外，我们还展示了iDCT相较于逆离散傅里叶变换（iDFT）提供了更高效的实现方式，这使得能在保持与最优iDFT基适应相当的表达能力的同时，更好地选择和调整频率分量。通过使用有限差分近似在DCT频谱上可学习系数的离散位置估计梯度，LoCA在训练过程中动态选择最信息丰富的频率分量。在多样化的语言和视觉微调任务上的实验表明，LoCA提供了增强的参数效率，并且在计算可行性方面与基于低秩的方法相当。 

---
# DeepCell: Multiview Representation Learning for Post-Mapping Netlists 

**Title (ZH)**: DeepCell：映射后网表的多视图表示学习 

**Authors**: Zhengyuan Shi, Chengyu Ma, Ziyang Zheng, Lingfeng Zhou, Hongyang Pan, Wentao Jiang, Fan Yang, Xiaoyan Yang, Zhufei Chu, Qiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.06816)  

**Abstract**: Representation learning for post-mapping (PM) netlists is a critical challenge in Electronic Design Automation (EDA), driven by the diverse and complex nature of modern circuit designs. Existing approaches focus on intermediate representations like And-Inverter Graphs (AIGs), limiting their applicability to post-synthesis stages. We introduce DeepCell, a multiview representation learning framework that integrates structural and functional insights from both PM netlists and AIGs to learn rich, generalizable embeddings. At its core, DeepCell employs the novel Mask Circuit Modeling (MCM) mechanism, which refines PM netlist representations in a self-supervised manner using pretrained AIG encoders. DeepCell sets a new benchmark in PM netlist representation, outperforming existing methods in predictive accuracy and reconstruction fidelity. To validate its efficacy, we apply DeepCell to functional Engineering Change Orders (ECO), achieving significant reductions in patch generation costs and runtime while improving patch quality. 

**Abstract (ZH)**: 在电子设计自动化（EDA）中，后映射（Post-mapping, PM）网表的表示学习是一个关键挑战，这归因于现代电路设计的多样性和复杂性。现有的方法主要集中在中间表示形式，如与反相器图（AIGs），这限制了它们在后综合阶段的应用。我们提出了DeepCell，这是一种多视图表示学习框架，结合了PM网表和AIGs的结构和功能洞察，以学习丰富的、泛化的嵌入。其核心使用新颖的遮罩电路建模（MCM）机制，通过预训练的AIG编码器以自我监督的方式细化PM网表的表示。DeepCell在PM网表表示方面设立了新的基准，在预测准确性和重构保真度方面优于现有方法。为了验证其效果，我们将DeepCell应用于功能性的工程变更订单（ECO），实现了显著减少补丁生成成本和运行时间的同时，还提高了补丁质量。 

---
# Diffusion Instruction Tuning 

**Title (ZH)**: 扩散指令调优 

**Authors**: Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare  

**Link**: [PDF](https://arxiv.org/pdf/2502.06814)  

**Abstract**: We introduce Lavender, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model's visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples, 2.5% of typical large-scale SFT datasets, and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and a 68% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. All code, training data, and models will be shared at this https URL. 

**Abstract (ZH)**: 我们介绍了一种名为Lavender的简单监督微调方法，该方法通过利用如Stable Diffusion等最先进的图像生成模型，提升了高级视觉-语言模型（VLMs）的表现。具体来说，Lavender在微调过程（SFT）中将视觉-语言模型（VLM）变换器中的文本-视觉注意力机制与Stable Diffusion中的相应机制进行对齐，而不再分别调整编码器。这种对齐增强了模型的视觉理解能力，并显著提升了分布内外任务的性能。Lavender只需0.13百万个训练样本，仅占典型大规模SFT数据集的2.5%，且可以在标准硬件（8块GPU）上在一个工作日内完成微调。它能够持续改进现有的开源多模态语言模型（例如Llama-3.2-11B和MiniCPM-Llama3-v2.5），在具有挑战性的分布外医学问答任务中分别取得了高达30%和68%的性能提升。通过在最小监督的情况下高效地将图像生成器的视觉专业技能进行转移，Lavender提供了一种可扩展的解决方案，以实现更准确的视觉-语言系统。所有代码、训练数据和模型将在以下链接共享：[此处链接]。 

---
# Policy Guided Tree Search for Enhanced LLM Reasoning 

**Title (ZH)**: 政策引导的树搜索方法以增强语言模型推理能力 

**Authors**: Yang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.06813)  

**Abstract**: Despite their remarkable capabilities, large language models often struggle with tasks requiring complex reasoning and planning. While existing approaches like Chain-of-Thought prompting and tree search techniques show promise, they are limited by their reliance on predefined heuristics and computationally expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS), a framework that combines reinforcement learning with structured tree exploration to efficiently navigate reasoning paths. Our key innovation is a learned policy that dynamically decides between expanding, branching, backtracking, or terminating exploration, eliminating the need for manual heuristics or exhaustive search. Experiments across mathematical reasoning, logical deduction, and planning benchmarks demonstrate that PGTS achieves superior reasoning performance while significantly reducing computational costs compared to existing methods. These results establish PGTS as a scalable and effective solution for tackling complex reasoning tasks with LLMs. 

**Abstract (ZH)**: 尽管大型语言模型具有卓越的能力，但在需要复杂推理和规划的任务中常常表现出色有限。现有的方法，如链式思考提示和树搜索技术虽然显示出前景，但由于依赖预定义的启发式方法和计算成本较高的探索策略，它们仍存在局限性。本文提出了一种结合强化学习与结构化树探索的Policy-Guided Tree Search（PGTS）框架，以高效地导航推理路径。我们的主要创新在于一个学习到的策略，该策略能够动态地决定在扩展、分叉、回溯或终止探索之间做出选择，从而消除了手动启发式或穷举搜索的需要。在数学推理、逻辑推导和规划基准测试中的实验表明，PGTS不仅在推理性能方面优于现有方法，而且在计算成本方面显著降低。这些结果验证了PGTS作为利用大规模语言模型处理复杂推理任务的可扩展且有效解决方案的有效性。 

---
# Aligning Human and Machine Attention for Enhanced Supervised Learning 

**Title (ZH)**: Human与机器注意力的对齐以提升监督学习效果 

**Authors**: Avihay Chriqui, Inbal Yahav, Dov Teeni, Ahmed Abbasi  

**Link**: [PDF](https://arxiv.org/pdf/2502.06811)  

**Abstract**: Attention, or prioritization of certain information items over others, is a critical element of any learning process, for both humans and machines. Given that humans continue to outperform machines in certain learning tasks, it seems plausible that machine performance could be enriched by aligning machine attention with human attention mechanisms -- yet research on this topic is sparse and has achieved only limited success. This paper proposes a new approach to address this gap, called Human-Machine Attention Learning (HuMAL). This approach involves reliance on data annotated by humans to reflect their self-perceived attention during specific tasks. We evaluate several alternative strategies for integrating such human attention data into machine learning (ML) algorithms, using a sentiment analysis task (review data from Yelp) and a personality-type classification task (data from myPersonality). The best-performing HuMAL strategy significantly enhances the task performance of fine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the benefit is particularly pronounced under challenging conditions of imbalanced or sparse labeled data. This research contributes to a deeper understanding of strategies for integrating human attention into ML models and highlights the potential of leveraging human cognition to augment ML in real-world applications. 

**Abstract (ZH)**: 注意力，即对某些信息项给予优先处理，是任何学习过程中的关键要素，无论是对人类还是机器。鉴于人类在某些学习任务上依然远超机器，将机器的注意力机制与人类的注意力机制相匹配似乎是提升机器性能的一种合理途径。然而，相关研究仍显稀少，且取得的成功有限。本文提出了一种新的方法来填补这一空白，称为人类-机器注意力学习（HuMAL）。该方法依赖于人类标注的数据，反映他们在特定任务中自认为的注意力分配。我们评估了几种不同的策略，将这些人类注意力数据整合到机器学习（ML）算法中，使用的情感分析任务（来自Yelp的评论数据）和个人特质分类任务（来自myPersonality的数据）。表现最佳的HuMAL策略显著提升了微调的变换器模型（BERT、GPT-2和XLNET）的任务性能，并且在标记数据不平衡或稀疏的挑战性条件下，该方法尤其具有显著优势。这项研究为进一步整合人类注意力机制到ML模型中提供了更深入的理解，并突显了利用人类认知来增强实际应用中ML性能的潜力。 

---
# Emergence of Self-Awareness in Artificial Systems: A Minimalist Three-Layer Approach to Artificial Consciousness 

**Title (ZH)**: 人工系统中自我意识的涌现：人工意识的 minimalist 三层方法 

**Authors**: Kurando Iida  

**Link**: [PDF](https://arxiv.org/pdf/2502.06810)  

**Abstract**: This paper proposes a minimalist three-layer model for artificial consciousness, focusing on the emergence of self-awareness. The model comprises a Cognitive Integration Layer, a Pattern Prediction Layer, and an Instinctive Response Layer, interacting with Access-Oriented and Pattern-Integrated Memory systems. Unlike brain-replication approaches, we aim to achieve minimal self-awareness through essential elements only. Self-awareness emerges from layer interactions and dynamic self-modeling, without initial explicit self-programming. We detail each component's structure, function, and implementation strategies, addressing technical feasibility. This research offers new perspectives on consciousness emergence in artificial systems, with potential implications for human consciousness understanding and adaptable AI development. We conclude by discussing ethical considerations and future research directions. 

**Abstract (ZH)**: 本文提出了一种简约的三层模型，旨在探讨人工意识中的自我意识涌现。该模型包含认知整合层、模式预测层和本能反应层，并与面向访问的记忆系统和模式整合记忆系统相互作用。与脑模拟方法不同，我们旨在通过使用最基本的元素来实现最少的自我意识。自我意识源自各层之间的交互和动态自我建模，而无需初始显式的自我编程。本文详细介绍了每个组件的结构、功能和实现策略，并讨论了其实现的技术可行性和潜在挑战。本研究为人工系统中意识涌现提供了新的视角，并可能对人类意识的理解和适应性人工智能的发展产生影响。最后，本文还讨论了伦理考虑及未来的研究方向。 

---
# Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution 

**Title (ZH)**: 神经元以区间形式表达：突破离散神经元属性的限制 

**Authors**: Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, Peizhong Ju, A.B. Siddique  

**Link**: [PDF](https://arxiv.org/pdf/2502.06809)  

**Abstract**: Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods. 

**Abstract (ZH)**: 理解并控制大型语言模型（LLMs）的内部机制对于提高其可信度和实用性至关重要。近期的研究主要集中在通过建立神经元和语义概念之间的离散映射来识别和操控神经元。然而，这种映射难以处理LLMs固有的多义性，即单个神经元编码多个不同的概念。这使得精确控制变得困难，并复杂化了下游干预。通过对多个文本分类数据集中的编码器和解码器基的LLMs进行深入分析，我们发现虽然单个神经元编码多个概念，但它们在不同概念上的激活强度以独特的、类似高斯分布的模式变化。基于这一洞察，我们提出了NeuronLens，这是一种新颖的基于范围的解释和操控框架，它可以提供更精细的神经元激活分布视图，以在神经元内部定位概念归属。广泛的实证评估表明，NeuronLens显著减少了无意的干扰，同时保持了对目标概念操控的精确控制，超越了现有方法。 

---
# On the Benefits of Attribute-Driven Graph Domain Adaptation 

**Title (ZH)**: 基于属性驱动的图领域适应的优势 

**Authors**: Ruiyi Fang, Bingheng Li, Zhao Kang, Qiuhao Zeng, Ruizhi Pu, Nima Hosseini Dashtbayaz, Boyu Wang, Charles Ling  

**Link**: [PDF](https://arxiv.org/pdf/2502.06808)  

**Abstract**: Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. Specifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscores the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmarks verify the effectiveness of our method. 

**Abstract (ZH)**: 图域适应（Graph Domain Adaptation, GDA）解决了跨网络学习中的一个紧迫挑战，特别是在真实世界图数据集中缺少标签的情况下尤为重要。近期研究试图通过消除图之间结构差异来学习域不变表示。在本工作中，我们表明现有方法忽视了图节点属性的重要性，这是图域对齐的关键因素。具体来说，我们首先通过理论证明表明，除了域间图结构差异之外，节点属性差异在GDA中也起着关键作用。此外，我们还通过实验证明节点属性差异比拓扑结构差异更为显著，进一步突出了节点属性对齐在GDA中的重要性。受到这一发现的启发，我们开发了一个新的跨通道模块，用于融合和对齐源图和目标图之间的双重视图，以解决GDA问题。在多种基准数据集上的实验结果验证了我们方法的有效性。 

---
# Competitive Programming with Large Reasoning Models 

**Title (ZH)**: 大规模推理模型中的竞赛编程 

**Authors**: OpenAI, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2502.06807)  

**Abstract**: We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming. 

**Abstract (ZH)**: 我们将强化学习应用于大规模语言模型（LLMs），显著提升了其在复杂编码和推理任务中的表现。此外，我们比较了两种通用推理模型——OpenAI的o1和早期的o3检查点——与一个针对特定领域的系统o1-ioi。o1-ioi使用了专为参与2024年国际信息学奥林匹克竞赛（IOI）设计的手工工程化的推理策略。我们与o1-ioi在IOI 2024的比赛中实时对决，并通过手工构建的比赛策略获得了第49百分位的成绩。在放宽了比赛约束条件后，o1-ioi荣获金牌。然而，当我们评估后来的模型如o3时，发现o3在无需手工构建的领域特定策略或放宽约束条件下即可获得金牌。我们的研究结果表明，虽然专门的流水线如o1-ioi提供了显著的改进，但规模扩大且通用的o3模型在推理领域（如编程竞赛）中超越了这些结果，而无需依赖手工构建的推理启发式方法。值得注意的是，o3在2024年IOI中获得金牌，并且其Codeforces排名与精英人类竞争对手相当。总体而言，这些结果表明，在推理领域（如编程竞赛），通过扩展通用的强化学习技术而非依赖于特定领域的技术，可提供通往先进AI的一种稳健路径。 

---
# Logits are All We Need to Adapt Closed Models 

**Title (ZH)**: 我们需要的只是 logits 以适应闭合模型 

**Authors**: Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo  

**Link**: [PDF](https://arxiv.org/pdf/2502.06806)  

**Abstract**: Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to \emph{Plugin} model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models. 

**Abstract (ZH)**: 许多商用大型语言模型（LLMs）通常是闭源的，限制了开发者的灵活性，只能通过提示调优来使内容生成符合特定应用需求。虽然这些模型目前无法提供token logits的访问权限，但我们认为如果能够访问这些logits，将能够启用超出提示工程的更强大适应技术。在本文中，我们提出了一种基于token级概率重加权的框架，在获得logits访问权限和少量特定任务数据的情况下，该框架可以有效地引导黑盒LLMs向特定应用的内容生成偏移。我们的方法将下一个token的预测视为监督分类问题。我们展示了如何将黑盒LLMs与特定任务数据对齐可以形式化为标签噪声矫正问题，并在此基础上提出了一种名为“Plugin”的自回归概率重加权模型，该模型仅基于logits操作。我们提供了理论依据来证明，单独重加权logits对于任务适应是足够的。实验结果表明，在多个数据集、LLMs和重加权模型上使用我们的方法非常有效，从而倡导在闭源模型中更广泛地提供token logits的访问权限。 

---
# Emotion Recognition and Generation: A Comprehensive Review of Face, Speech, and Text Modalities 

**Title (ZH)**: 情绪识别与生成：面向、语音和文本模态的综述 

**Authors**: Rebecca Mobbs, Dimitrios Makris, Vasileios Argyriou  

**Link**: [PDF](https://arxiv.org/pdf/2502.06803)  

**Abstract**: Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems. 

**Abstract (ZH)**: 情绪识别与生成已成为人工智能研究中的关键课题，对于增强医疗、客户服务等领域中的人机交互起到了重要作用。尽管关于情绪识别与生成的研究综述已经存在，但许多这些综述要么内容分散，要么局限于特定方法，缺乏对不同模态下最近发展和趋势的全面概述。在这篇综述中，我们提供了一个全面的回顾，旨在为刚开始探索情绪识别与生成的研究人员提供帮助。我们介绍了情绪识别与生成在面部、声音和文本等不同模态下的基本原理。本文将近期前沿研究分类为不同的技术方法，并解释了这些方法的理论基础和动机，从而为这些方法的应用提供了更清晰的理解。此外，我们讨论了评估指标、比较分析和当前局限性，揭示了研究人员在该领域面临的一些挑战。最后，我们提出了未来的研究方向以应对这些挑战，并鼓励进一步探索开发稳健、有效且伦理责任的强情绪识别与生成系统。 

---
# Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking 

**Title (ZH)**: 解决 Roblox 游戏推荐中的内容缺口：基于大语言模型的档案生成与重新排名 

**Authors**: Chen Wang, Xiaokai Wei, Yexi Jiang, Frank Ong, Kevin Gao, Xiao Yu, Zheng Hui, Se-eun Yoon, Philip Yu, Michelle Gong  

**Link**: [PDF](https://arxiv.org/pdf/2502.06802)  

**Abstract**: With the vast and dynamic user-generated content on Roblox, creating effective game recommendations requires a deep understanding of game content. Traditional recommendation models struggle with the inconsistent and sparse nature of game text features such as titles and descriptions. Recent advancements in large language models (LLMs) offer opportunities to enhance recommendation systems by analyzing in-game text data. This paper addresses two challenges: generating high-quality, structured text features for games without extensive human annotation, and validating these features to ensure they improve recommendation relevance. We propose an approach that extracts in-game text and uses LLMs to infer attributes such as genre and gameplay objectives from raw player interactions. Additionally, we introduce an LLM-based re-ranking mechanism to assess the effectiveness of the generated text features, enhancing personalization and user satisfaction. Beyond recommendations, our approach supports applications such as user engagement-based integrity detection, already deployed in production. This scalable framework demonstrates the potential of in-game text understanding to improve recommendation quality on Roblox and adapt recommendations to its unique, user-generated ecosystem. 

**Abstract (ZH)**: 在Roblox平台上积累了大量动态用户生成内容的情况下，创建有效的游戏推荐需要深刻理解游戏内容。传统的推荐模型难以应对游戏文本特征（如标题和描述）的一致性和稀疏性。近年来，大型语言模型（LLMs）的进步为通过分析游戏内的文本数据来提升推荐系统的性能提供了机会。本文针对两个挑战：生成高质量、结构化的游戏文本特征而不依赖大量的人工标注，并验证这些特征以确保它们能够提升推荐的相关性。我们提出了一种方法，通过抽取游戏内的文本，并使用LLMs从原始玩家互动中推断出游戏类型和游戏目标等属性。此外，我们还引入了一种基于LLM的重新排序机制来评估生成的文本特征的有效性，从而增强个性化和用户满意度。超越推荐，我们的方法还支持基于用户参与度的完整性检测等应用，并已在生产环境中部署。这一可扩展的框架展示了游戏内文本理解在提升Roblox上的推荐质量以及适应其独特的用户生成生态系统方面的潜力。 

---
# Information-theoretic Bayesian Optimization: Survey and Tutorial 

**Title (ZH)**: 信息论贝叶斯优化：综述与教程 

**Authors**: Eduardo C. Garrido-Merchán  

**Link**: [PDF](https://arxiv.org/pdf/2502.06789)  

**Abstract**: Several scenarios require the optimization of non-convex black-box functions, that are noisy expensive to evaluate functions with unknown analytical expression, whose gradients are hence not accessible. For example, the hyper-parameter tuning problem of machine learning models. Bayesian optimization is a class of methods with state-of-the-art performance delivering a solution to this problem in real scenarios. It uses an iterative process that employs a probabilistic surrogate model, typically a Gaussian process, of the objective function to be optimized computing a posterior predictive distribution of the black-box function. Based on the information given by this posterior predictive distribution, Bayesian optimization includes the computation of an acquisition function that represents, for every input space point, the utility of evaluating that point in the next iteraiton if the objective of the process is to retrieve a global extremum. This paper is a survey of the information theoretical acquisition functions, whose performance typically outperforms the rest of acquisition functions. The main concepts of the field of information theory are also described in detail to make the reader aware of why information theory acquisition functions deliver great results in Bayesian optimization and how can we approximate them when they are intractable. We also cover how information theory acquisition functions can be adapted to complex optimization scenarios such as the multi-objective, constrained, non-myopic, multi-fidelity, parallel and asynchronous settings and provide further lines of research. 

**Abstract (ZH)**: 以下内容或标题翻译成中文，符合学术规范：

在多个场景中，需要优化非凸的黑箱函数，这些函数通常具有未知分析表达式，且噪声较大、评估成本高、梯度不可得。例如，机器学习模型的超参数调整问题。贝叶斯优化是一种性能先进的方法，能够解决这一问题。它采用一种迭代过程，利用目标函数的概率替代模型（通常为高斯过程）来近似黑箱函数，在后验预测分布的基础上计算该目标函数的最佳解。基于此后验预测分布提供的信息，贝叶斯优化包括计算获取函数（acquisition function），该函数表示在下一次迭代中，对于输入空间中的每个点，评价该点的价值，从而实现全局极值的检索。本文是对信息论获取函数的综述，这类获取函数通常具有其他获取函数无法比拟的性能。文中详细介绍了信息论领域的基本概念，让读者了解为什么信息论获取函数在贝叶斯优化中表现优异，以及如何在无法直接求解时对其进行近似。此外，还讨论了如何将信息论获取函数适应于复杂的优化场景，如多目标、约束、非短视、多精度、并行和异步设置等，同时也指出了未来研究的方向。 

---
