{'arxiv_id': 'arXiv:2502.07441', 'title': 'SensPS: Sensing Personal Space Comfortable Distance between Human-Human Using Multimodal Sensors', 'authors': 'Ko Watanabe, Nico Förster, Shoya Ishimaru', 'link': 'https://arxiv.org/abs/2502.07441', 'abstract': 'Personal space, also known as peripersonal space, is crucial in human social interaction, influencing comfort, communication, and social stress. Estimating and respecting personal space is essential for enhancing human-computer interaction (HCI) and smart environments. Personal space preferences vary due to individual traits, cultural background, and contextual factors. Advanced multimodal sensing technologies, including eye-tracking and wristband sensors, offer opportunities to develop adaptive systems that dynamically adjust to user comfort levels. Integrating physiological and behavioral data enables a deeper understanding of spatial interactions. This study develops a sensor-based model to estimate comfortable personal space and identifies key features influencing spatial preferences. Our findings show that multimodal sensors, particularly eye-tracking and physiological wristband data, can effectively predict personal space preferences, with eye-tracking data playing a more significant role. An experimental study involving controlled human interactions demonstrates that a Transformer-based model achieves the highest predictive accuracy (F1 score: 0.87) for estimating personal space. Eye-tracking features, such as gaze point and pupil diameter, emerge as the most significant predictors, while physiological signals from wristband sensors contribute marginally. These results highlight the potential for AI-driven personalization of social space in adaptive environments, suggesting that multimodal sensing can be leveraged to develop intelligent systems that optimize spatial arrangements in workplaces, educational institutions, and public settings. Future work should explore larger datasets, real-world applications, and additional physiological markers to enhance model robustness.', 'abstract_zh': '个人空间，也称为周边空间，在人际交往中至关重要，影响着舒适度、沟通和社交压力。估算并尊重个人空间对于提升人机交互（HCI）和智能环境至关重要。个人空间偏好因个体特征、文化背景和情境因素而异。先进的多模态传感技术，包括眼动追踪和腕带传感器，为开发能够动态适应用户舒适度的自适应系统提供了机会。整合生理和行为数据有助于更深层次地了解空间互动。本研究开发了一种基于传感器的模型来估算舒适的个人空间，并识别影响空间偏好的关键特征。我们的研究结果表明，多模态传感器，尤其是眼动追踪和生理腕带数据，能够有效地预测个人空间偏好，其中眼动追踪数据的作用更为显著。一项涉及受控人类互动的实验研究表明，基于Transformer的模型在预测个人空间方面具有最高的预测准确性（F1分数：0.87）。眼动追踪特征（如注视点和瞳孔直径）是最重要的预测因子，而来自腕带传感器的生理信号则几乎没有贡献。这些结果突显了人工智能驱动的社会空间个性化在自适应环境中的潜在价值，表明多模态传感可以用来开发能够优化工作场所、教育机构和公共空间中空间布局的智能系统。未来的研究应探索更大的数据集、实际应用场景和更多的生理标记以提高模型的鲁棒性。', 'title_zh': 'SensPS：基于多模态传感器的人与人之间舒适个人空间感知距离'}
{'arxiv_id': 'arXiv:2502.07401', 'title': 'Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning', 'authors': 'Johnny Chan, Yuming Li', 'link': 'https://arxiv.org/abs/2502.07401', 'abstract': 'This research explores the opportunities of Generative AI (GenAI) in the realm of higher education through the design and development of a multimodal chatbot for an undergraduate course. Leveraging the ChatGPT API for nuanced text-based interactions and Google Bard for advanced image analysis and diagram-to-code conversions, we showcase the potential of GenAI in addressing a broad spectrum of educational queries. Additionally, the chatbot presents a file-based analyser designed for educators, offering deep insights into student feedback via sentiment and emotion analysis, and summarising course evaluations with key metrics. These combinations highlight the crucial role of multimodal conversational AI in enhancing teaching and learning processes, promising significant advancements in educational adaptability, engagement, and feedback analysis. By demonstrating a practical web application, this research underlines the imperative for integrating GenAI technologies to foster more dynamic and responsive educational environments, ultimately contributing to improved educational outcomes and pedagogical strategies.', 'abstract_zh': '本研究通过设计和开发一款多模态聊天机器人，探讨了生成式人工智能（GenAI）在高等教育领域的应用机会。该聊天机器人基于本科生课程的需求，利用ChatGPT API实现精细的文本交互，结合Google Bard进行高级图像分析和图表到代码的转换，展示了GenAI在解决广泛教育问题方面的潜力。此外，该聊天机器人还提供了一个基于文件的分析工具，供教育者使用，通过情感和情绪分析提供学生反馈的深入见解，并利用关键指标总结课程评估。这些组合突显了多模态对话AI在提升教学和学习过程中的关键作用，有望在教育适应性、参与度和反馈分析方面带来重大进步。通过展示一个实用的网络应用，本研究强调了整合GenAI技术的重要性和迫切性，以促进更加动态和响应性的教育环境，最终为改善教育成果和教育策略做出贡献。', 'title_zh': '以生成式AI提升高等教育：一种多模态个性化学习方法'}
{'arxiv_id': 'arXiv:2502.07400', 'title': 'Explainable Multimodal Machine Learning for Revealing Structure-Property Relationships in Carbon Nanotube Fibers', 'authors': 'Daisuke Kimura, Naoko Tajima, Toshiya Okazaki, Shun Muroga', 'link': 'https://arxiv.org/abs/2502.07400', 'abstract': 'In this study, we propose Explainable Multimodal Machine Learning (EMML), which integrates the analysis of diverse data types (multimodal data) using factor analysis for feature extraction with Explainable AI (XAI), for carbon nanotube (CNT) fibers prepared from aqueous dispersions. This method is a powerful approach to elucidate the mechanisms governing material properties, where multi-stage fabrication conditions and multiscale structures have complex influences. Thus, in our case, this approach helps us understand how different processing steps and structures at various scales impact the final properties of CNT fibers. The analysis targeted structures ranging from the nanoscale to the macroscale, including aggregation size distributions of CNT dispersions and the effective length of CNTs. Furthermore, because some types of data were difficult to interpret using standard methods, challenging-to-interpret distribution data were analyzed using Negative Matrix Factorization (NMF) for extracting key features that determine the outcome. Contribution analysis with SHapley Additive exPlanations (SHAP) demonstrated that small, uniformly distributed aggregates are crucial for improving fracture strength, while CNTs with long effective lengths are significant factors for enhancing electrical conductivity. The analysis also identified thresholds and trends for these key factors to assist in defining the conditions needed to optimize CNT fiber properties. EMML is not limited to CNT fibers but can be applied to the design of other materials derived from nanomaterials, making it a useful tool for developing a wide range of advanced materials. This approach provides a foundation for advancing data-driven materials research.', 'abstract_zh': '在本研究中，我们提出了一种可解释的多模态机器学习（Explainable Multimodal Machine Learning, EMML）方法。该方法结合了因子分析用于特征提取的多模态数据分析与可解释的人工智能（Explainable AI, XAI），研究水分散液制备的碳纳米管（Carbon Nanotube, CNT）纤维的性能。该方法是一种强大而有力的手段，用于阐明影响材料性质的机制，尤其是多阶段制造条件和跨尺度结构的复杂影响。因此，在本研究中，这种方法帮助我们理解不同加工步骤和不同尺度结构如何影响CNT纤维的最终性能。分析涵盖了从纳米尺度到宏观尺度的结构，包括CNT分散液的聚集尺寸分布和CNT的有效长度。此外，由于某些类型的数据难以使用标准方法进行解释，我们利用负矩阵分解（Negative Matrix Factorization, NMF）来分析难以解释的分布数据，提取决定结果的关键特征。SHapley Additive exPlanations（SHAP）值贡献分析表明，均匀分布的小尺寸聚集对提高断裂强度至关重要，而具有较长有效长度的CNT对于增强电导率至关重要。分析还确定了这些关键因素的阈值和趋势，有助于定义优化CNT纤维性能所需的条件。EMML不仅限于CNT纤维，还可以应用于从纳米材料衍生的其他材料的设计，使其成为开发各种先进材料的有力工具。这种方法为推进数据驱动的材料研究奠定了基础。', 'title_zh': '可解释的多模态机器学习在揭示碳纳米管纤维的结构-性质关系中的应用'}
{'arxiv_id': 'arXiv:2502.07306', 'title': 'TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation', 'authors': 'Navid Rajabi, Jana Kosecka', 'link': 'https://arxiv.org/abs/2502.07306', 'abstract': 'In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps \\cite{vlmaps} on the complex R2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of visual grounding on navigation performance.', 'abstract_zh': '在本文中，我们提出了一种模块化的方法来解决视觉-语言导航（VLN）任务，通过将问题分解为四个子模块，这些子模块在零样本设置中使用最先进的大型语言模型（LLMs）和视觉-语言模型（VLMs）。给定自然语言的导航指令，我们首先提示LLM从中提取地标及其访问顺序。假设已知环境模型，我们检索最后一个地标附近的前k个位置，并使用环境拓扑图上的最短路径算法生成从起始位置到最后一个地标之间的k条路径假设。每条路径假设由一系列全景图序列表示。然后，我们使用动态规划计算全景图序列与地标名称序列之间的对齐得分，该得分通过VLM获得匹配值。最后，我们计算与得分最高的假设之间的nDTW度量，以评估路径保真度。我们展示了与使用联合语义地图（如VLMaps [vlmaps]）的方法相比，在复杂的R2R-Habitat [r2r]指令数据集上的优越性能，并详细量化了视觉接地对导航性能的影响。', 'title_zh': 'TRAVEL：无需训练的检索与对齐方法用于视觉-语言导航'}
{'arxiv_id': 'arXiv:2502.07277', 'title': 'Enhancing Video Understanding: Deep Neural Networks for Spatiotemporal Analysis', 'authors': 'Amir Hosein Fadaei, Mohammad-Reza A. Dehaqani', 'link': 'https://arxiv.org/abs/2502.07277', 'abstract': "It's no secret that video has become the primary way we share information online. That's why there's been a surge in demand for algorithms that can analyze and understand video content. It's a trend going to continue as video continues to dominate the digital landscape. These algorithms will extract and classify related features from the video and will use them to describe the events and objects in the video. Deep neural networks have displayed encouraging outcomes in the realm of feature extraction and video description. This paper will explore the spatiotemporal features found in videos and recent advancements in deep neural networks in video understanding. We will review some of the main trends in video understanding models and their structural design, the main problems, and some offered solutions in this topic. We will also review and compare significant video understanding and action recognition datasets.", 'abstract_zh': '视频已经成为我们在线分享信息的主要方式，这使得人们对能够分析和理解视频内容的算法产生了巨大的需求。而随着视频继续主导数字领域，这种趋势将会持续下去。这些算法将从视频中提取和分类相关特征，并利用这些特征描述视频中的事件和对象。深度神经网络在特征提取和视频描述领域展现出了令人鼓舞的结果。本文将探讨视频中时空特征及其在视频理解中的最新进展。我们将回顾视频理解模型的主要趋势及其结构设计、主要问题和一些解决方案。此外，我们还将回顾并比较一些重要的视频理解和动作识别数据集。', 'title_zh': '提升视频理解：用于时空分析的深度神经网络'}
{'arxiv_id': 'arXiv:2502.07239', 'title': 'Contextual Gesture: Co-Speech Gesture Video Generation through Context-aware Gesture Representation', 'authors': 'Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Sharpio, Kyle Olszewski', 'link': 'https://arxiv.org/abs/2502.07239', 'abstract': 'Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech. Despite recent advancements, existing methods struggle with accurately identifying the rhythmic or semantic triggers from audio for generating contextualized gesture patterns and achieving pixel-level realism. To address these challenges, we introduce Contextual Gesture, a framework that improves co-speech gesture video generation through three innovative components: (1) a chronological speech-gesture alignment that temporally connects two modalities, (2) a contextualized gesture tokenization that incorporate speech context into motion pattern representation through distillation, and (3) a structure-aware refinement module that employs edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Contextual Gesture not only produces realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications, shown in Fig.1 Project Page: this https URL.', 'abstract_zh': '同步手势生成对于创建逼真的人像和增强人机交互至关重要，它能通过手势与言语的同步来实现这一目标。尽管最近取得了进展，现有的方法在从音频中准确识别节奏或语义触发因素以生成上下文相关的手势模式并实现像素级逼真度方面仍存在挑战。为了解决这些问题，我们提出了Contextual Gesture框架，该框架通过三个创新组件提升了同步手势视频生成的效果：（1）时间顺序的语音-手势对齐，它在两个模态之间建立时间连接；（2）上下文手势标记，通过蒸馏将语音上下文整合到运动模式表示中；（3）结构感知的细化模块，利用边缘连接将手势关键点连接起来，以改进视频生成。我们的广泛实验表明，Contextual Gesture不仅能生成逼真且与语音对齐的手势视频，还能支持长序列生成和视频手势编辑应用，如图1所示。项目页面：请参阅此链接: [具体链接]', 'title_zh': '上下文手势：通过上下文意识手势表示的同步手势视频生成'}
{'arxiv_id': 'arXiv:2502.07158', 'title': 'Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health Records via Multimodal Fused Transformer', 'authors': 'Jiaying Lu, Stephanie R. Brown, Songyuan Liu, Shifan Zhao, Kejun Dong, Del Bold, Michael Fundora, Alaa Aljiffry, Alex Fedorov, Jocelyn Grunwell, Xiao Hu', 'link': 'https://arxiv.org/abs/2502.07158', 'abstract': 'Early prediction of pediatric cardiac arrest (CA) is critical for timely intervention in high-risk intensive care settings. We introduce PedCA-FT, a novel transformer-based framework that fuses tabular view of EHR with the derived textual view of EHR to fully unleash the interactions of high-dimensional risk factors and their dynamics. By employing dedicated transformer modules for each modality view, PedCA-FT captures complex temporal and contextual patterns to produce robust CA risk estimates. Evaluated on a curated pediatric cohort from the CHOA-CICU database, our approach outperforms ten other artificial intelligence models across five key performance metrics and identifies clinically meaningful risk factors. These findings underscore the potential of multimodal fusion techniques to enhance early CA detection and improve patient care.', 'abstract_zh': '儿科心脏骤停（CA）的早期预测对于在高风险重症监护环境中及时干预至关重要。我们引入了PedCA-FT，这是一种新颖的基于变压器的框架，将电子健康记录（EHR）的表格视图与提取的文本视图结合起来，以充分发挥高维风险因素及其动态的交互作用。通过为每种模态视图配备专门的变压器模块，PedCA-FT能够捕捉复杂的时间性和上下文模式，从而生成稳健的心脏骤停风险估计。在CHOA-CICU数据库中针对精心挑选的儿科队列进行评估，我们的方法在五个关键性能指标上优于十种其他人工智能模型，并且能够识别出具有临床意义的风险因素。这些发现强调了多模态融合技术在增强早期心脏骤停检测以及改善患者护理方面的潜力。', 'title_zh': '通过多模态融合变压器对电子健康记录进行儿童心脏骤停早期风险预测'}
{'arxiv_id': 'arXiv:2502.07090', 'title': 'Generative Distribution Prediction: A Unified Approach to Multimodal Learning', 'authors': 'Xinyu Tian, Xiaotong Shen', 'link': 'https://arxiv.org/abs/2502.07090', 'abstract': 'Accurate prediction with multimodal data-encompassing tabular, textual, and visual inputs or outputs-is fundamental to advancing analytics in diverse application domains. Traditional approaches often struggle to integrate heterogeneous data types while maintaining high predictive accuracy. We introduce Generative Distribution Prediction (GDP), a novel framework that leverages multimodal synthetic data generation-such as conditional diffusion models-to enhance predictive performance across structured and unstructured modalities. GDP is model-agnostic, compatible with any high-fidelity generative model, and supports transfer learning for domain adaptation. We establish a rigorous theoretical foundation for GDP, providing statistical guarantees on its predictive accuracy when using diffusion models as the generative backbone. By estimating the data-generating distribution and adapting to various loss functions for risk minimization, GDP enables accurate point predictions across multimodal settings. We empirically validate GDP on four supervised learning tasks-tabular data prediction, question answering, image captioning, and adaptive quantile regression-demonstrating its versatility and effectiveness across diverse domains.', 'abstract_zh': '多模态数据（包括表格、文本和视觉数据）的准确预测是推动在不同应用领域中数据分析进步的基础。传统方法往往难以在保持高预测精度的同时整合不同类型的异构数据。为此，我们提出了一种名为生成分布预测（GDP，Generative Distribution Prediction）的新框架，该框架利用条件扩散模型等多模态合成数据生成技术，以提高结构化和非结构化模态下的预测性能。GDP 是一种模型无关的方法，可以与任何高保真生成模型兼容，并支持领域适应的迁移学习。我们为 GDP 建立了严谨的理论基础，提供了在使用扩散模型作为生成核心时对其预测精度的统计保证。通过估计数据生成分布并适应各种损失函数以实现风险最小化，GDP 能够在多模态场景中实现准确的点预测。我们在四个监督学习任务中实证验证了 GDP 的表现，包括表格数据预测、问答、图像字幕生成和自适应分位数回归，展示了它在不同领域的多样性和有效性。', 'title_zh': '生成分布预测：多模态学习的统一方法'}
{'arxiv_id': 'arXiv:2502.07001', 'title': 'From Image to Video: An Empirical Study of Diffusion Representations', 'authors': 'Pedro Vélez, Luisa F. Polanía, Yi Yang, Chuhan Zhang, Rishab Kabra, Anurag Arnab, Mehdi S. M. Sajjadi', 'link': 'https://arxiv.org/abs/2502.07001', 'abstract': 'Diffusion models have revolutionized generative modeling, enabling unprecedented realism in image and video synthesis. This success has sparked interest in leveraging their representations for visual understanding tasks. While recent works have explored this potential for image generation, the visual understanding capabilities of video diffusion models remain largely uncharted. To address this gap, we systematically compare the same model architecture trained for video versus image generation, analyzing the performance of their latent representations on various downstream tasks including image classification, action recognition, depth estimation, and tracking. Results show that video diffusion models consistently outperform their image counterparts, though we find a striking range in the extent of this superiority. We further analyze features extracted from different layers and with varying noise levels, as well as the effect of model size and training budget on representation and generation quality. This work marks the first direct comparison of video and image diffusion objectives for visual understanding, offering insights into the role of temporal information in representation learning.', 'abstract_zh': '扩散模型已经彻底革新了生成建模，使其在图像和视频合成中表现出前所未有的逼真度。这一成功激发了研究者们探索利用其表示方法进行视觉理解任务的兴趣。尽管近期一些工作已经探索了这一潜力，但视频扩散模型在视觉理解方面的能力仍然尚未完全被探索。为填补这一空白，我们系统地比较了用于视频和图像生成的相同模型架构的性能，分析了其潜在表示在分类、动作识别、深度估计和跟踪等多种下游任务中的表现。结果表明，视频扩散模型在各种任务中普遍优于图像扩散模型，但我们在不同层提取的特征和不同噪声水平下的特征，以及模型大小和训练预算对表示质量和生成效果的影响方面发现了显著差异。这项工作首次直接比较了视频和图像扩散目标在视觉理解中的表现，为时间信息在表示学习中的作用提供了洞见。', 'title_zh': '从图像到视频：关于扩散表示的实证研究'}
{'arxiv_id': 'arXiv:2502.06894', 'title': 'AI-Driven HSI: Multimodality, Fusion, Challenges, and the Deep Learning Revolution', 'authors': 'David S. Bhatti, Yougin Choi, Rahman S M Wahidur, Maleeka Bakhtawar, Sumin Kim, Surin Lee, Yongtae Lee, Heung-No Lee', 'link': 'https://arxiv.org/abs/2502.06894', 'abstract': "Hyperspectral imaging (HSI) captures spatial and spectral data, enabling analysis of features invisible to conventional systems. The technology is vital in fields such as weather monitoring, food quality control, counterfeit detection, healthcare diagnostics, and extending into defense, agriculture, and industrial automation at the same time. HSI has advanced with improvements in spectral resolution, miniaturization, and computational methods. This study provides an overview of the HSI, its applications, challenges in data fusion and the role of deep learning models in processing HSI data. We discuss how integration of multimodal HSI with AI, particularly with deep learning, improves classification accuracy and operational efficiency. Deep learning enhances HSI analysis in areas like feature extraction, change detection, denoising unmixing, dimensionality reduction, landcover mapping, data augmentation, spectral construction and super resolution. An emerging focus is the fusion of hyperspectral cameras with large language models (LLMs), referred as highbrain LLMs, enabling the development of advanced applications such as low visibility crash detection and face antispoofing. We also highlight key players in HSI industry, its compound annual growth rate and the growing industrial significance. The purpose is to offer insight to both technical and non-technical audience, covering HSI's images, trends, and future directions, while providing valuable information on HSI datasets and software libraries.", 'abstract_zh': '高光谱成像（HSI）能够捕获空间和光谱数据，使分析常规系统无法识别的特征成为可能。这项技术在天气监测、食品质量控制、假冒检测、医疗诊断以及国防、农业和工业自动化等多个领域具有重要价值。随着光谱分辨率的提升、小型化以及计算方法的进步，HSI技术得到了发展。本文提供了一个HSI的综述，涵盖了其应用、数据融合面临的挑战以及深度学习模型在处理HSI数据中的作用。我们讨论了多模态HSI与人工智能（特别是深度学习）的集成如何提高分类准确性和操作效率。深度学习在特征提取、变化检测、去噪解混、降维、土地覆盖图绘制、数据增强、光谱重建和超分辨率等方面提升了HSI分析的性能。一个新兴的研究趋势是将高光谱相机与大型语言模型（LLMs）融合，称之为基础性LLMs，这使得低能见度碰撞检测和面部防伪等高级应用得以开发。我们还强调了高光谱行业的主要参与者、其复合年增长率以及日益增长的工业意义。本文旨在为技术性和非技术性的读者提供见解，涵盖HSI的影像、趋势和未来方向，并提供有关HSI数据集和软件库的重要信息。', 'title_zh': 'AI 驱动的HSI：多模态融合、挑战及深度学习革命'}
{'arxiv_id': 'arXiv:2502.06873', 'title': 'Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning', 'authors': 'Subin Kim, Hoonrae Kim, Heejin Do, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2502.06873', 'abstract': "Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.", 'abstract_zh': '以往的研究揭示了大型语言模型（LLMs）在支持认知重构疗法方面的潜力；然而，这些研究主要集中在基于文本的方法上，往往忽视了在实际疗法中至关重要的非语言证据的重要性。为了解决这一问题，我们将文本认知重构扩展到多模态领域，引入了视觉线索。具体而言，我们提出了一种名为多模态认知支持对话（M2CoSC）的新数据集，该数据集每包含一个GPT-4生成的对话，还配有一张反映虚拟来访者面部表情的照片。为了更好地模拟实际的心理咨询过程，其中面部表情能够引导对隐含的情绪证据的解读，我们提出了一种多跳心理疗法推理方法，以明确识别并融入细微的证据。我们的全面实验表明，使用M2CoSC数据集，视觉语言模型（VLMs）在心理治疗中的表现得到了显著提升。此外，多跳心理疗法推理方法使VLMs能够提供更为周到和富有同情心的建议，超越了标准的提示方法。', 'title_zh': '多模态认知重构疗法通过多跳心理治疗推理'}
{'arxiv_id': 'arXiv:2502.06851', 'title': 'Survey on Vision-Language-Action Models', 'authors': 'Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Daulet Baimukashev, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yermakhan Kassym, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Yerkebulan Massalim, Zerde Nurbayeva, Zhanat Kappassov', 'link': 'https://arxiv.org/abs/2502.06851', 'abstract': 'This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.', 'abstract_zh': '本文呈现了一篇由人工智能生成的关于视觉-语言-行动（VLA）模型的综述，总结了关键方法论、研究成果和未来方向。内容是使用大规模语言模型（LLMs）生成的，仅用于示范目的。本研究不代表原始研究，而是强调了AI如何帮助自动化文献综述。随着AI生成内容的日益普及，确保准确性和可靠性以及恰当的综合仍然是一个挑战。未来的研究将重点发展结构化的框架以辅助AI进行文献综述，探索提高引用准确性、来源可信度和上下文理解的技术。通过探讨大规模语言模型在学术写作中的潜力和局限性，本研究旨在为将AI整合到研究工作流中提供更广泛的讨论。本工作作为利用AI生成文献综述的系统方法的第一步，旨在使学术知识综合更加高效和普及。', 'title_zh': '视觉-语言-动作模型综述'}
{'arxiv_id': 'arXiv:2502.06843', 'title': 'Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation', 'authors': 'Namhee Kim, Woojin Park', 'link': 'https://arxiv.org/abs/2502.06843', 'abstract': 'Traditional autonomous driving systems often struggle with reasoning in complex, unexpected scenarios due to limited comprehension of spatial relationships. In response, this study introduces a Large Language Model (LLM)-based Autonomous Driving (AD) assistance system that integrates a vision adapter and an LLM reasoning module to enhance visual understanding and decision-making. The vision adapter, combining YOLOv4 and Vision Transformer (ViT), extracts comprehensive visual features, while GPT-4 enables human-like spatial reasoning and response generation. Experimental evaluations with 45 experienced drivers revealed that the system closely mirrors human performance in describing situations and moderately aligns with human decisions in generating appropriate responses.', 'abstract_zh': '传统自动驾驶系统在应对复杂和不可预见场景时往往难以进行有效的推理，主要是因为其对空间关系的理解有限。为解决这一问题，本研究提出了一种基于大型语言模型（LLM）的自动驾驶（Autonomous Driving, AD）辅助系统，该系统整合了视觉适配器和LLM推理模块，以增强视觉理解和决策能力。视觉适配器结合了YOLOv4和视觉变压器（Vision Transformer, ViT），提取全面的视觉特征，而GPT-4则实现类似人类的空间推理和响应生成。实验评估了45名经验丰富驾驶员的表现，结果显示该系统在描述场景方面与人类表现非常接近，在生成适当响应方面与人类决策有一定的对齐度。', 'title_zh': '面向自主驾驶辅助的视觉集成大规模语言模型：人类性能比较与信任评估'}
{'arxiv_id': 'arXiv:2502.06836', 'title': 'CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction', 'authors': 'Jaewan Lee, Changyoung Park, Hongjun Yang, Sungbin Lim, Sehui Han', 'link': 'https://arxiv.org/abs/2502.06836', 'abstract': 'Recent advancements in AI have revolutionized property prediction in materials science and accelerating material discovery. Graph neural networks (GNNs) stand out due to their ability to represent crystal structures as graphs, effectively capturing local interactions and delivering superior predictions. However, these methods often lose critical global information, such as crystal systems and repetitive unit connectivity. To address this, we propose CAST, a cross-attention-based multimodal fusion model that integrates graph and text modalities to preserve essential material information. CAST combines node- and token-level features using cross-attention mechanisms, surpassing previous approaches reliant on material-level embeddings like graph mean-pooling or [CLS] tokens. A masked node prediction pretraining strategy further enhances atomic-level information integration. Our method achieved up to 22.9\\% improvement in property prediction across four crystal properties including band gap compared to methods like CrysMMNet and MultiMat. Pretraining was key to aligning node and text embeddings, with attention maps confirming its effectiveness in capturing relationships between nodes and tokens. This study highlights the potential of multimodal learning in materials science, paving the way for more robust predictive models that incorporate both local and global information.', 'abstract_zh': '近年来，人工智能的进步彻底改变了材料科学中的属性预测，并加速了新材料的发现。图神经网络（GNNs）因其能够将晶体结构表示为图，有效地捕获局部交互并提供优越的预测能力而脱颖而出。然而，这些方法往往损失了重要的全局信息，如晶体系统和重复单元的连接性。为了解决这一问题，我们提出了CAST，这是一种基于交叉注意力的多模态融合模型，该模型将图和文本模态结合在一起以保留关键的材料信息。CAST 使用交叉注意力机制结合节点级和标记级特征，超越了依赖于材料级嵌入（如图均值池化或[CLS]标记）的先前方法。通过掩码节点预测预训练策略进一步增强了原子级信息的集成。在包括带隙在内的四种晶体属性中，我们的方法相对于 CrysMMNet 和 MultiMat 方法实现了高达 22.9% 的属性预测性能提升。预训练是关键，它使节点和文本嵌入对齐，注意力图证实了其在捕捉节点和标记之间关系方面的有效性。本研究突显了多模态学习在材料科学中的潜力，为结合局部和全局信息的更稳健预测模型铺平了道路。', 'title_zh': 'CAST：基于交叉注意力的结构与文本多模态融合方法在材料性质预测中的应用'}
{'arxiv_id': 'arXiv:2502.06814', 'title': 'Diffusion Instruction Tuning', 'authors': 'Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare', 'link': 'https://arxiv.org/abs/2502.06814', 'abstract': "We introduce Lavender, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model's visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples, 2.5% of typical large-scale SFT datasets, and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and a 68% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. All code, training data, and models will be shared at this https URL.", 'abstract_zh': '我们介绍了一种名为Lavender的简单监督微调方法，该方法通过利用如Stable Diffusion等最先进的图像生成模型，提升了高级视觉-语言模型（VLMs）的表现。具体来说，Lavender在微调过程（SFT）中将视觉-语言模型（VLM）变换器中的文本-视觉注意力机制与Stable Diffusion中的相应机制进行对齐，而不再分别调整编码器。这种对齐增强了模型的视觉理解能力，并显著提升了分布内外任务的性能。Lavender只需0.13百万个训练样本，仅占典型大规模SFT数据集的2.5%，且可以在标准硬件（8块GPU）上在一个工作日内完成微调。它能够持续改进现有的开源多模态语言模型（例如Llama-3.2-11B和MiniCPM-Llama3-v2.5），在具有挑战性的分布外医学问答任务中分别取得了高达30%和68%的性能提升。通过在最小监督的情况下高效地将图像生成器的视觉专业技能进行转移，Lavender提供了一种可扩展的解决方案，以实现更准确的视觉-语言系统。所有代码、训练数据和模型将在以下链接共享：[此处链接]。', 'title_zh': '扩散指令调优'}
{'arxiv_id': 'arXiv:2502.06803', 'title': 'Emotion Recognition and Generation: A Comprehensive Review of Face, Speech, and Text Modalities', 'authors': 'Rebecca Mobbs, Dimitrios Makris, Vasileios Argyriou', 'link': 'https://arxiv.org/abs/2502.06803', 'abstract': 'Emotion recognition and generation have emerged as crucial topics in Artificial Intelligence research, playing a significant role in enhancing human-computer interaction within healthcare, customer service, and other fields. Although several reviews have been conducted on emotion recognition and generation as separate entities, many of these works are either fragmented or limited to specific methodologies, lacking a comprehensive overview of recent developments and trends across different modalities. In this survey, we provide a holistic review aimed at researchers beginning their exploration in emotion recognition and generation. We introduce the fundamental principles underlying emotion recognition and generation across facial, vocal, and textual modalities. This work categorises recent state-of-the-art research into distinct technical approaches and explains the theoretical foundations and motivations behind these methodologies, offering a clearer understanding of their application. Moreover, we discuss evaluation metrics, comparative analyses, and current limitations, shedding light on the challenges faced by researchers in the field. Finally, we propose future research directions to address these challenges and encourage further exploration into developing robust, effective, and ethically responsible emotion recognition and generation systems.', 'abstract_zh': '情绪识别与生成已成为人工智能研究中的关键课题，对于增强医疗、客户服务等领域中的人机交互起到了重要作用。尽管关于情绪识别与生成的研究综述已经存在，但许多这些综述要么内容分散，要么局限于特定方法，缺乏对不同模态下最近发展和趋势的全面概述。在这篇综述中，我们提供了一个全面的回顾，旨在为刚开始探索情绪识别与生成的研究人员提供帮助。我们介绍了情绪识别与生成在面部、声音和文本等不同模态下的基本原理。本文将近期前沿研究分类为不同的技术方法，并解释了这些方法的理论基础和动机，从而为这些方法的应用提供了更清晰的理解。此外，我们讨论了评估指标、比较分析和当前局限性，揭示了研究人员在该领域面临的一些挑战。最后，我们提出了未来的研究方向以应对这些挑战，并鼓励进一步探索开发稳健、有效且伦理责任的强情绪识别与生成系统。', 'title_zh': '情绪识别与生成：面向、语音和文本模态的综述'}
{'arxiv_id': 'arXiv:2502.06823', 'title': 'CTR-Driven Advertising Image Generation with Multimodal Large Language Models', 'authors': 'Xingye Chen, Wei Feng, Zhenbang Du, Weizhen Wang, Yanyin Chen, Haohan Wang, Linkai Liu, Yaoyu Li, Jinyuan Zhao, Yu Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Zhangang Lin, Jingping Shao, Yuanjie Shao, Xinge You, Changxin Gao, Nong Sang', 'link': 'https://arxiv.org/abs/2502.06823', 'abstract': 'In web data, advertising images are crucial for capturing user attention and improving advertising effectiveness. Most existing methods generate background for products primarily focus on the aesthetic quality, which may fail to achieve satisfactory online performance. To address this limitation, we explore the use of Multimodal Large Language Models (MLLMs) for generating advertising images by optimizing for Click-Through Rate (CTR) as the primary objective. Firstly, we build targeted pre-training tasks, and leverage a large-scale e-commerce multimodal dataset to equip MLLMs with initial capabilities for advertising image generation tasks. To further improve the CTR of generated images, we propose a novel reward model to fine-tune pre-trained MLLMs through Reinforcement Learning (RL), which can jointly utilize multimodal features and accurately reflect user click preferences. Meanwhile, a product-centric preference optimization strategy is developed to ensure that the generated background content aligns with the product characteristics after fine-tuning, enhancing the overall relevance and effectiveness of the advertising images. Extensive experiments have demonstrated that our method achieves state-of-the-art performance in both online and offline metrics. Our code and pre-trained models are publicly available at: this https URL.', 'abstract_zh': '在网络数据中，广告图像对于吸引用户注意力和提升广告效果至关重要。现有的大多数方法主要侧重于生成产品的 aesthetic 质量背景，这可能无法实现满意的在线性能。为了克服这一局限性，我们探索使用多模态大型语言模型（MLLMs）来生成广告图像，以点击率 (CTR) 作为主要优化目标。首先，我们构建了定向预训练任务，并利用大规模电子商务多模态数据集为 MLLMs 预训练以进行广告图像生成任务。为了进一步提高生成图像的 CTR，我们提出了一种新颖的奖励模型，通过强化学习 (RL) 对预训练的 MLLMs 进行微调，该模型能同时利用多模态特征并准确反映用户的点击偏好。同时，我们开发了一种以产品为中心的偏好优化策略，以确保在微调后生成的背景内容与产品特性一致，从而增强广告图像的整体相关性和有效性。实验结果表明，我们的方法在在线和离线指标上都取得了当前最佳性能。我们的代码和预训练模型已公开发布于：这个链接。', 'title_zh': '基于CTR驱动的多模态大型语言模型的广告图像生成'}
{'arxiv_id': 'arXiv:2502.07391', 'title': 'Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation Generation', 'authors': 'Palaash Goel, Dushyant Singh Chauhan, Md Shad Akhtar', 'link': 'https://arxiv.org/abs/2502.07391', 'abstract': "Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn model, aka. TURBO. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. TURBO assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed TURBO model on the MORE+ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of TURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on TURBO's generated explanations and find them out to be comparatively better than other systems.", 'abstract_zh': '讽刺是一种语言现象，旨在以固有的方式嘲讽目标（例如，实体、事件或人物）。多模态讽刺解释（MuSE）旨在通过自然语言解释揭示讽刺性帖子中的意图讽刺。尽管重要，但现有系统在生成解释时忽视了讽刺目标的重要性。在本文中，我们提出了一种目标增强共享融合的讽刺解释模型，简称TURBO。我们设计了一种新颖的共享融合机制以利用图像与其描述之间的跨模态关系。TURBO 假设讽刺的目标并引导多模态共享融合机制学习意图讽刺的细微之处以进行解释。我们使用 MORE+ 数据集评估了我们提出的 TURBO 模型。与多个基线模型和最新模型的比较表明，TURBO 的性能平均提高了 $+3.3\\%$。此外，我们还探索了零-shot 和 one-shot 情境下的大规模语言模型（LLM）在我们的任务中的应用，并观察到虽然 LLM 生成的解释非常出色，但往往未能捕捉到讽刺的关键细微之处。同时，我们进行了广泛的针对 TURBO 生成的解释的人类评估，发现它们相对于其他系统而言更为优异。', 'title_zh': '目标增强共享融合多模态讽刺解释生成'}
{'arxiv_id': 'arXiv:2502.07601', 'title': 'Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models', 'authors': 'Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi', 'link': 'https://arxiv.org/abs/2502.07601', 'abstract': 'Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD & reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: this https URL', 'abstract_zh': '零样本异常检测（ZSAD）是一种新兴的异常检测（AD）范式。与传统无监督AD设置需要大量正常样本来训练模型不同，ZSAD在处理数据受限的实际场景时更为实用。最近，多模态大语言模型（MLLMs）在各种视觉任务中展现出革命性的推理能力。然而，由于缺乏相应的数据集和基准，图像异常的推理仍然未被充分探索。为促进AD与推理的研究，我们建立了首个视觉指令调优数据集Anomaly-Instruct-125k及评估基准VisA-D&R。通过我们的基准评估，我们揭示了当前的MLLMs，如GPT-4o，无法准确地检测和描述图像中的细微异常细节。为了应对这一挑战，我们提出了首个专门针对ZSAD和推理的视觉助手Anomaly-OneVision（Anomaly-OV）。Anomaly-OV借鉴了人类视觉检查行为，利用“双查看特征匹配”（LTFM）机制来自适应地选择和强调异常视觉特征。广泛的实验表明，Anomaly-OV在检测和推理两个方面都显著优于先进的通用模型。我们还为未来的研究提供了医疗和三维AD的应用扩展。我们的项目页面链接：[这个链接](https://your-project-page-url.com)', 'title_zh': '面向多模态大规模语言模型的零样本异常检测与推理'}
{'arxiv_id': 'arXiv:2502.07138', 'title': 'Towards a Robust Framework for Multimodal Hate Detection: A Study on Video vs. Image-based Content', 'authors': 'Girish A. Koushik, Diptesh Kanojia, Helen Treharne', 'link': 'https://arxiv.org/abs/2502.07138', 'abstract': 'Social media platforms enable the propagation of hateful content across different modalities such as textual, auditory, and visual, necessitating effective detection methods. While recent approaches have shown promise in handling individual modalities, their effectiveness across different modality combinations remains unexplored. This paper presents a systematic analysis of fusion-based approaches for multimodal hate detection, focusing on their performance across video and image-based content. Our comprehensive evaluation reveals significant modality-specific limitations: while simple embedding fusion achieves state-of-the-art performance on video content (HateMM dataset) with a 9.9% points F1-score improvement, it struggles with complex image-text relationships in memes (Hateful Memes dataset). Through detailed ablation studies and error analysis, we demonstrate how current fusion approaches fail to capture nuanced cross-modal interactions, particularly in cases involving benign confounders. Our findings provide crucial insights for developing more robust hate detection systems and highlight the need for modality-specific architectural considerations. The code is available at this https URL.', 'abstract_zh': '社交媒体平台通过多种模态（如文本、音频和视觉）传播仇恨内容，从而需要有效的检测方法。虽然近期的方法在处理单一模态方面显示出了潜力，但它们在不同模态组合中的效果尚未得到研究。本文系统地分析了融合方法在多模态仇恨检测中的应用，重点关注它们在基于视频和图像内容上的表现。我们的全面评估揭示了显著的模态特定限制：虽然简单的嵌入融合在视频内容（HateMM数据集）上实现了最先进的性能，提高了9.9个百分点的F1分数，但在处理涉及Meme的复杂图像-文本关系时却表现较差（Hateful Memes数据集）。通过详细的消融研究和误差分析，我们展示了当前的融合方法如何未能捕捉到复杂的跨模态交互，特别是在涉及良性混杂变量的情况下。我们的研究结果为开发更稳健的仇恨检测系统提供了关键见解，并指出了模态特定架构考虑的必要性。代码可在以下链接获取：[这里插入链接]。', 'title_zh': '面向鲁棒的多模态仇恨检测框架研究：基于视频与图像内容的比较'}
{'arxiv_id': 'arXiv:2502.06893', 'title': 'A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok', 'authors': 'Jared D.T. Guerrero-Sosa, Andres Montoro-Montarroso, Francisco P. Romero, Jesus Serrano-Guerrero, Jose A. Olivas', 'link': 'https://arxiv.org/abs/2502.06893', 'abstract': 'In the context of the rapid dissemination of multimedia content, identifying disinformation on social media platforms such as TikTok represents a significant challenge. This study introduces a hybrid framework that combines the computational power of deep learning with the interpretability of fuzzy logic to detect suspected disinformation in TikTok videos. The methodology is comprised of two core components: a multimodal feature analyser that extracts and evaluates data from text, audio, and video; and a multimodal disinformation detector based on fuzzy logic. These systems operate in conjunction to evaluate the suspicion of spreading disinformation, drawing on human behavioural cues such as body language, speech patterns, and text coherence. Two experiments were conducted: one focusing on context-specific disinformation and the other on the scalability of the model across broader topics. For each video evaluated, high-quality, comprehensive, well-structured reports are generated, providing a detailed view of the disinformation behaviours.', 'abstract_zh': '在多媒体内容快速传播的背景下，识别 TikTok 等社交媒体平台上虚假信息具有重大挑战。本研究提出了一种结合深度学习的强大计算能力和模糊逻辑的可解释性于一体的混合框架，用于检测 TikTok 视频中疑似虚假信息。该方法由两个核心组件组成：一个多模态特征分析器，用于从文本、音频和视频中提取和评估数据；以及基于模糊逻辑的多模态虚假信息检测器。这两个系统协同工作，通过分析体态语言、语音模式和文本连贯性等人类行为线索，评估传播虚假信息的嫌疑。开展了两项实验：一项聚焦于特定情境下的虚假信息，另一项则关注该模型在更广泛主题上的可扩展性。对于每个评估的视频，该系统会生成高质量、全面且结构良好的报告，提供详细的虚假信息行为视图。', 'title_zh': '一种新的混合智能方法用于检测 TikTok 上疑似虚假信息的多模态检测'}
{'arxiv_id': 'arXiv:2502.06822', 'title': 'DiffListener: Discrete Diffusion Model for Listener Generation', 'authors': 'Siyeol Jung, Taehwan Kim', 'link': 'https://arxiv.org/abs/2502.06822', 'abstract': "The listener head generation (LHG) task aims to generate natural nonverbal listener responses based on the speaker's multimodal cues. While prior work either rely on limited modalities (e.g. audio and facial information) or employ autoregressive approaches which have limitations such as accumulating prediction errors. To address these limitations, we propose DiffListener, a discrete diffusion based approach for non-autoregressive listener head generation. Our model takes the speaker's facial information, audio, and text as inputs, additionally incorporating facial differential information to represent the temporal dynamics of expressions and movements. With this explicit modeling of facial dynamics, DiffListener can generate coherent reaction sequences in a non-autoregressive manner. Through comprehensive experiments, DiffListener demonstrates state-of-the-art performance in both quantitative and qualitative evaluations. The user study shows that DiffListener generates natural context-aware listener reactions that are well synchronized with the speaker. The code and demo videos are available in this https URL", 'abstract_zh': '听众头部生成（LHG）任务旨在根据发言人的多模态线索生成自然的非言语听众反应。以往的工作要么依赖于有限的模态（例如音频和面部信息），要么采用自回归方法，而这类方法存在累积预测误差等局限性。为解决这些问题，我们提出了一种基于离散扩散的非自回归听众头部生成方法——DiffListener。我们的模型接受发言人的面部信息、音频和文本作为输入，并进一步结合面部差异性信息以表示表情和动作的时序动态。通过这种显式的面部动态建模，DiffListener能够在非自回归的方式下生成连贯的反应序列。通过全面的实验，DiffListener在定量和定性评估中均展现了最先进的性能。用户研究表明，DiffListener生成的自然且上下文相关的听众反应与发言人的同步性很好。有关代码和演示视频可访问此链接：[提供链接的网址]', 'title_zh': 'DiffListener：离散扩散模型于听者生成'}
