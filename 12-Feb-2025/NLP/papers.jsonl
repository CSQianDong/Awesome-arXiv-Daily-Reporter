{'arxiv_id': 'arXiv:2502.07776', 'title': 'Auditing Prompt Caching in Language Model APIs', 'authors': 'Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto', 'link': 'https://arxiv.org/abs/2502.07776', 'abstract': "Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.", 'abstract_zh': '大型语言模型（LLMs）中的提示缓存导致了数据依赖性的定时差异：缓存的提示比非缓存的提示处理速度更快。这些定时差异引入了侧信道定时攻击的风险。例如，如果缓存跨用户共享，攻击者可以通过快速API响应时间识别缓存的提示，从而了解其他用户的提示信息。由于提示缓存可能导致隐私泄露，因此API提供者缓存策略的透明性非常重要。为此，我们开发并进行了统计审计，以检测现实世界中LLM API提供者的提示缓存。我们在包括OpenAI在内的七个API提供商中检测到用户间的全局缓存共享，这可能导致用户的提示信息的隐私泄露。提示缓存导致的定时差异也可能泄露模型架构的相关信息。具体而言，我们发现证据表明，OpenAI的嵌入模型是一个仅解码器的Transformer，这是之前未公开的信息。', 'title_zh': '语言模型API中提示缓存的审计'}
{'arxiv_id': 'arXiv:2502.07771', 'title': 'Breaking Down Bias: On The Limits of Generalizable Pruning Strategies', 'authors': 'Sibo Ma, Alejandro Salinas, Peter Henderson, Julian Nyarko', 'link': 'https://arxiv.org/abs/2502.07771', 'abstract': 'We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.', 'abstract_zh': '我们采用模型剪枝的方法来探究大型语言模型（LLM）如何理解种族偏见，以及是否可能存在一种广泛适用的偏见缓解策略。我们的分析揭示了几项新的见解。我们发现，剪枝可以有效地减少偏见，同时显著增加异常模型行为的风险较低。基于神经元的剪枝策略通常比剪枝整个注意力头的方法能取得更好的效果。然而，我们的结果也表明，这两种方法的有效性随着剪枝策略的普遍化迅速降低。例如，一个在金融决策背景下用于消除种族偏见的模型，在商业交易中难以广泛适用。总的来说，我们的分析表明，种族偏见在语言模型中仅部分表现为一个普遍的概念，而这些偏见的另一部分高度依赖于具体情境，这表明广泛适用的缓解策略可能在这方面的效果有限。我们的研究结果对围绕AI的法律框架具有重要影响，特别是表明有效的缓解策略应该包括将法律责任分配给在特定应用场景中部署模型的人员。', 'title_zh': '打破偏见：关于广泛适用性剪枝策略的局限性'}
{'arxiv_id': 'arXiv:2502.07755', 'title': 'An Advanced NLP Framework for Automated Medical Diagnosis with DeBERTa and Dynamic Contextual Positional Gating', 'authors': 'Mohammad Ali Labbaf Khaniki, Sahabeh Saadati, Mohammad Manthouri', 'link': 'https://arxiv.org/abs/2502.07755', 'abstract': "This paper presents a novel Natural Language Processing (NLP) framework for enhancing medical diagnosis through the integration of advanced techniques in data augmentation, feature extraction, and classification. The proposed approach employs back-translation to generate diverse paraphrased datasets, improving robustness and mitigating overfitting in classification tasks. Leveraging Decoding-enhanced BERT with Disentangled Attention (DeBERTa) with Dynamic Contextual Positional Gating (DCPG), the model captures fine-grained contextual and positional relationships, dynamically adjusting the influence of positional information based on semantic context to produce high-quality text embeddings. For classification, an Attention-Based Feedforward Neural Network (ABFNN) is utilized, effectively focusing on the most relevant features to improve decision-making accuracy. Applied to the classification of symptoms, clinical notes, and other medical texts, this architecture demonstrates its ability to address the complexities of medical data. The combination of data augmentation, contextual embedding generation, and advanced classification mechanisms offers a robust and accurate diagnostic tool, with potential applications in automated medical diagnosis and clinical decision support. This method demonstrates the effectiveness of the proposed NLP framework for medical diagnosis, achieving remarkable results with an accuracy of 99.78%, recall of 99.72%, precision of 99.79%, and an F1-score of 99.75%. These metrics not only underscore the model's robust performance in classifying medical texts with exceptional precision and reliability but also highlight its superiority over existing methods, making it a highly promising tool for automated diagnostic systems.", 'abstract_zh': '本文提出了一种新的自然语言处理（NLP）框架，通过集成高级数据增强、特征提取和分类技术，增强医疗诊断能力。所提出的方法利用逆向翻译生成多样化的同义数据集，从而提高分类任务的稳健性并减轻过拟合现象。利用解码增强的BERT带解耦注意力（DeBERTa）以及动态上下文定位门控（DCPG），该模型捕捉细粒度的上下文和定位关系，根据语义上下文动态调整定位信息的影响，生成高质量的文本嵌入。对于分类任务，本文采用基于注意力的前馈神经网络（ABFNN），有效聚焦于最相关的特征，以提高决策准确性。该架构应用于症状、临床记录及其他医学文本的分类，显示了其应对医学数据复杂性的能力。结合数据增强、上下文嵌入生成和先进的分类机制，该框架提供了一种稳健且准确的诊断工具，具有在自动化医疗诊断和临床决策支持方面的潜在应用。该方法证明了所提出的NLP框架在医疗诊断中的有效性，实现了令人瞩目的准确率（99.78%）、召回率（99.72%）、精确率（99.79%）和F1分数（99.75%）。这些指标不仅突显了模型在医学文本分类中表现出色的鲁棒性和可靠性，还显示了其在现有方法上的优越性，使其成为自动化诊断系统中极具前景的工具。', 'title_zh': '基于DeBERTa和动态上下文位置门控的高级自然语言处理框架在自动化医疗诊断中的应用'}
{'arxiv_id': 'arXiv:2502.07747', 'title': 'WHODUNIT: Evaluation benchmark for culprit detection in mystery stories', 'authors': 'Kshitij Gupta', 'link': 'https://arxiv.org/abs/2502.07747', 'abstract': 'We present a novel data set, WhoDunIt, to assess the deductive reasoning capabilities of large language models (LLM) within narrative contexts. Constructed from open domain mystery novels and short stories, the dataset challenges LLMs to identify the perpetrator after reading and comprehending the story. To evaluate model robustness, we apply a range of character-level name augmentations, including original names, name swaps, and substitutions with well-known real and/or fictional entities from popular discourse. We further use various prompting styles to investigate the influence of prompting on deductive reasoning accuracy.\nWe conduct evaluation study with state-of-the-art models, specifically GPT-4o, GPT-4-turbo, and GPT-4o-mini, evaluated through multiple trials with majority response selection to ensure reliability. The results demonstrate that while LLMs perform reliably on unaltered texts, accuracy diminishes with certain name substitutions, particularly those with wide recognition. This dataset is publicly available here.', 'abstract_zh': '我们提出了一种新的数据集，WhoDunIt，用于评估大型语言模型（LLM）在叙事情境中的演绎推理能力。该数据集源自开放领域的侦探小说和短篇故事，旨在让LLM在阅读和理解故事后识别出罪犯。为了评估模型的鲁棒性，我们应用了一系列基于字符级别的名字增强方法，包括原始名字、名字替换以及使用广为人知的真实或虚构实体的代换。我们还使用了多种提示方式，以研究提示对演绎推理准确度的影响。\n\n我们使用最新的模型，特别是GPT-4o、GPT-4-turbo和GPT-4o-mini，进行了评估研究。评估通过多次试验并选择多数响应来确保可靠性。结果表明，虽然LLM在未修改的文本上表现可靠，但在某些名字替换的情况下，准确度会有所下降，尤其是在那些广泛认可的名字替换中。本数据集目前已公开发布，可供查阅。', 'title_zh': 'WHODUNIT：推理侦探故事中凶手检测评估基准'}
{'arxiv_id': 'arXiv:2502.07717', 'title': 'Making Language Models Robust Against Negation', 'authors': 'MohammadHossein Rezaei, Eduardo Blanco', 'link': 'https://arxiv.org/abs/2502.07717', 'abstract': 'Negation has been a long-standing challenge for language models. Previous studies have shown that they struggle with negation in many natural language understanding tasks. In this work, we propose a self-supervised method to make language models more robust against negation. We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task. We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf versions on nine negation-related benchmarks. Most notably, our pre-training tasks yield between 1.8% and 9.1% improvement on CondaQA, a large question-answering corpus requiring reasoning over negation.', 'abstract_zh': '否定形式一直是语言模型的一个长期挑战。先前的研究表明，它们在许多自然语言理解任务中处理否定形式存在困难。在本研究中，我们提出了一种自监督方法，以提高语言模型对否定形式的鲁棒性。我们引入了一个新型任务——下一句极性预测（NSPP），以及下一句预测（NSP）任务的一个变体。我们表明，我们的任务进一步预训练的BERT和RoBERTa在九个与否定有关的基准测试中优于即用版本。最显著的是，我们的预训练任务在CondaQA上（一个需要在否定方面进行推理的大型问答语料库）分别提供了1.8%到9.1%的性能提升。', 'title_zh': '使语言模型在面对否定时具有稳健性'}
{'arxiv_id': 'arXiv:2502.07687', 'title': 'Large Language Models as Proxies for Theories of Human Linguistic Cognition', 'authors': 'Imry Ziv, Nur Lan, Emmanuel Chemla, Roni Katzir', 'link': 'https://arxiv.org/abs/2502.07687', 'abstract': 'We consider the possible role of current large language models (LLMs) in the study of human linguistic cognition. We focus on the use of such models as proxies for theories of cognition that are relatively linguistically-neutral in their representations and learning but differ from current LLMs in key ways. We illustrate this potential use of LLMs as proxies for theories of cognition in the context of two kinds of questions: (a) whether the target theory accounts for the acquisition of a given pattern from a given corpus; and (b) whether the target theory makes a given typologically-attested pattern easier to acquire than another, typologically-unattested pattern. For each of the two questions we show, building on recent literature, how current LLMs can potentially be of help, but we note that at present this help is quite limited.', 'abstract_zh': '我们探讨了当前大型语言模型（LLM）在研究人类语言认知方面可能发挥的作用。我们关注的是，将这类模型用作认知理论的代理，这些认知理论在代表性和学习方面相对语言中立，但在关键方面与当前的大型语言模型不同。我们通过两种类型的问题来说明这种将大型语言模型用作认知理论代理的潜在用途：（a）目标理论是否能够解释某个语料库中获得的某个模式；以及（b）目标理论是否使某个类型上可证的模式比另一个类型上不可证的模式更容易获得。对于每个问题，我们基于近期文献，展示了当前大型语言模型可能如何有所帮助，但我们也指出，目前这种帮助是相当有限的。', 'title_zh': '大型语言模型作为人类语言认知理论的代理模型'}
{'arxiv_id': 'arXiv:2502.07677', 'title': 'Auto-Drafting Police Reports from Noisy ASR Outputs: A Trust-Centered LLM Approach', 'authors': 'Param Kulkarni, Yingchi Liu, Hao-Ming Fu, Shaohua Yang, Isuru Gunasekara, Matt Peloquin, Noah Spitzer-Williams, Xiaotian Zhou, Xiaozhong Liu, Zhengping Ji, Yasser Ibrahim', 'link': 'https://arxiv.org/abs/2502.07677', 'abstract': 'Achieving a delicate balance between fostering trust in law en- forcement and protecting the rights of both officers and civilians continues to emerge as a pressing research and product challenge in the world today. In the pursuit of fairness and transparency, this study presents an innovative AI-driven system designed to generate police report drafts from complex, noisy, and multi-role dialogue data. Our approach intelligently extracts key elements of law enforcement interactions and includes them in the draft, producing structured narratives that are not only high in quality but also reinforce accountability and procedural clarity. This frame- work holds the potential to transform the reporting process, ensur- ing greater oversight, consistency, and fairness in future policing practices. A demonstration video of our system can be accessed at this https URL Y-kpCHNO/view?usp=sharing', 'abstract_zh': '在当今世界，如何在培养民众对执法机构的信任与保护执法人员和市民权利之间寻求一种微妙的平衡，仍然是一个迫切的研究和产品挑战。为了追求公平与透明，本研究提出了一个创新的人工智能驱动系统，旨在从复杂、嘈杂且涉及多方的角色对话数据中生成警察报告草稿。我们的方法能够智能地提取法律执行过程中关键要素，并将其纳入报告草案，产生结构化的叙述文本，不仅质量高，而且增强了问责制和程序清晰度。该框架有望重塑报告流程，确保未来执法实践中的更有效的监督、一致性和公平性。我们的系统演示视频可通过以下链接访问：![系统演示视频](https://www.example.com/Y-kpCHNO/view?usp=sharing)', 'title_zh': '基于信任中心的大型语言模型方法：从嘈杂的ASR输出自动生成警情报告'}
{'arxiv_id': 'arXiv:2502.07642', 'title': 'FoQA: A Faroese Question-Answering Dataset', 'authors': 'Annika Simonsen, Dan Saattrup Nielsen, Hafsteinn Einarsson', 'link': 'https://arxiv.org/abs/2502.07642', 'abstract': 'We present FoQA, a Faroese extractive question-answering (QA) dataset with 2,000 samples, created using a semi-automated approach combining Large Language Models (LLMs) and human validation. The dataset was generated from Faroese Wikipedia articles using GPT-4-turbo for initial QA generation, followed by question rephrasing to increase complexity and native speaker validation to ensure quality. We provide baseline performance metrics for FoQA across multiple models, including LLMs and BERT, demonstrating its effectiveness in evaluating Faroese QA performance. The dataset is released in three versions: a validated set of 2,000 samples, a complete set of all 10,001 generated samples, and a set of 2,395 rejected samples for error analysis.', 'abstract_zh': '我们介绍了FoQA，这是一个包含2000个样本的远罗佛语提取式问答(QA)数据集，该数据集是通过结合大型语言模型（LLMs）和人工验证的半自动化方法创建的。数据集是从远罗佛语维基百科文章生成的，使用GPT-4-turbo进行初始的QA生成，随后通过重新表述问题来增加复杂性，并通过母语者的验证确保质量。我们提供了多个模型，包括LLMs和BERT，在FoQA上的基线性能指标，这表明了FoQA在评估远罗佛语问答性能方面的有效性。数据集提供了三个版本：一个包含2000个验证样本的版本、一个包含所有10001个生成样本的完整版本以及一个包含2395个被拒绝样本的错误分析版本。', 'title_zh': 'FoQA：一种 Faroese 问答数据集'}
{'arxiv_id': 'arXiv:2502.07637', 'title': 'BiaSWE: An Expert Annotated Dataset for Misogyny Detection in Swedish', 'authors': 'Kätriin Kukk, Danila Petrelli, Judit Casademont, Eric J. W. Orlowski, Michał Dzieliński, Maria Jacobson', 'link': 'https://arxiv.org/abs/2502.07637', 'abstract': 'In this study, we introduce the process for creating BiaSWE, an expert-annotated dataset tailored for misogyny detection in the Swedish language. To address the cultural and linguistic specificity of misogyny in Swedish, we collaborated with experts from the social sciences and humanities. Our interdisciplinary team developed a rigorous annotation process, incorporating both domain knowledge and language expertise, to capture the nuances of misogyny in a Swedish context. This methodology ensures that the dataset is not only culturally relevant but also aligned with broader efforts in bias detection for low-resource languages. The dataset, along with the annotation guidelines, is publicly available for further research.', 'abstract_zh': '在本研究中，我们介绍了创建BiaSWE数据集的过程，该数据集是为瑞典语中的贬抑妇女行为检测而专门标注的专业数据集。为了应对瑞典语中贬抑妇女行为的文化和语言特殊性，我们与社会科学和人文科学领域的专家合作。我们的跨学科团队开发了一种严谨的标注过程，结合了领域的专业知识和语言技能，以捕捉瑞典语语境中贬抑行为的细微差别。这种方法保证了数据集不仅具有文化相关性，同时也与更广泛的针对少资源语言偏见检测的努力相一致。该数据集及其标注指南已公开提供，以便进一步研究使用。', 'title_zh': 'BiaSWE：一种专家标注的数据集，用于检测瑞典语中的性别歧视言论'}
{'arxiv_id': 'arXiv:2502.07623', 'title': 'Lexical categories of stem-forming roots in Mapudüngun verb forms', 'authors': 'Andrés Chandía', 'link': 'https://arxiv.org/abs/2502.07623', 'abstract': 'After developing a computational system for morphological analysis of the Mapuche language, and evaluating it with texts from various authors and styles, it became necessary to verify the linguistic assumptions of the source used as the basis for implementing this tool.\nIn the present work, the primary focus is on the lexical category classification of Mapudüngun roots recognised as verbal in the source utilised for the development of the morphological analysis system.\nThe results of this lexical category revision directly benefit the computational analyser, as they are implemented as soon as they are verified. Additionally, it is hoped that these results will help clarify some uncertainties about lexical categories in the Mapuche language.\nThis work addresses a preliminary task to identify the valency of true verbal roots, the results of which will be presented in a subsequent work that complements this article.', 'abstract_zh': '在开发出用于马普切语形态学分析的计算系统，并通过不同作者和风格的文本对其进行评估之后，有必要验证用于实现该工具的基础来源中的语言假设。\n\n本研究的主要焦点在于对源材料中识别为动词的马普迭冈（Mapudüngun）词根进行词汇类别分类。这些词汇类别的修订结果直接有利于计算分析器，一旦验证通过即可立即实施。此外，希望这些结果能有助于澄清马普切语言中一些关于词汇类别的不确定性。\n\n本研究涉及初步任务，即识别真实动词根的价，并且这些结果将在一篇后续的工作中展现，以补充本篇文章。', 'title_zh': 'Mapudüngun 动词形式中的词根词汇类别研究'}
{'arxiv_id': 'arXiv:2502.07616', 'title': 'Tractable Transformers for Flexible Conditional Generation', 'authors': 'Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck', 'link': 'https://arxiv.org/abs/2502.07616', 'abstract': 'Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines.', 'abstract_zh': '非自回归（NAR）生成模型因其能够以比自回归（AR）模型更为原则化的方式处理多样化条件生成任务而备受价值，而自回归模型由于受到顺序依赖性的限制，在这方面受到约束。近期NAR模型的发展，如扩散语言模型，在无条件生成任务上已经显示出相比相似规模的自回归模型（例如GPT）更优越的表现。然而，这些改进并不总是能转化为更好的条件生成性能。本文表明，导致这一差距的关键原因是模型在训练过程中难以泛化到未见过的条件概率查询。因此，强大的无条件生成性能并不能保证高质量的条件生成。本文提出了一种更为稳健的基于Transformer的生成模型Tractable Transformers（Tracformer），以应对不同的条件生成任务。与现有模型依赖完全输入的整体上下文特征不同，Tracformer整合了一个稀疏的Transformer编码器来捕捉局部和全局上下文信息。这些信息通过解码器用于条件生成。实验结果表明，Tracformer在与近期的扩散和自回归模型基线相比时，实现了最先进的条件生成性能。', 'title_zh': '可计算的变换器模型以实现灵活的条件生成'}
{'arxiv_id': 'arXiv:2502.07599', 'title': 'DPO-Shift: Shifting the Distribution of Direct Preference Optimization', 'authors': 'Xiliang Yang, Feng Jiang, Qianen Zhang, Lei Zhao, Xiao Li', 'link': 'https://arxiv.org/abs/2502.07599', 'abstract': 'Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at this https URL.', 'abstract_zh': '直接偏好优化（DPO）及其变体已成为将语言模型与人类偏好对齐的越来越流行的方法。这些方法旨在训练模型更好地区分被选择（或偏好）和被拒绝（或不偏好）的响应。然而，先前的研究发现，在训练过程中被选择响应的概率往往会降低，这种现象被称为似然性位移。为应对这一挑战，本文引入了\\method来可控地调整被选择概率的分布。我们还展示了\\method在提高被选择概率和牺牲奖励差距之间存在一个基本的权衡，这一观点得到了理论分析和实验验证的支持。此外，我们证明了\\method在下游任务（如MT-Bench）以及一个设计的胜率实验中优于DPO。我们认为这项研究证明了利用一个简单且理论依据充分的解决方案可以有效地缓解DPO的似然性位移问题。我们已将代码开源，地址如下：this https URL。', 'title_zh': 'DPO-Shift: 分布直接偏好优化的转变'}
{'arxiv_id': 'arXiv:2502.07586', 'title': "We Can't Understand AI Using our Existing Vocabulary", 'authors': 'John Hewitt, Robert Geirhos, Been Kim', 'link': 'https://arxiv.org/abs/2502.07586', 'abstract': 'This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they\'re reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a "length neologism" enables controlling LLM response length, while a "diversity neologism" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better.', 'abstract_zh': '本文的观点论文认为，要理解人工智能，我们不能依赖现有的人类词汇。相反，我们应该努力开发新的词汇（新兴词汇），这些新词汇能够代表我们想要教授给机器的精确人类概念，或是我们需要从机器中学习的机器概念。我们假设的前提是，人类和机器具有不同的概念。这意味着解释性可以被定义为一种沟通问题：人类必须能够参照和控制机器的概念，并向机器传达人类的概念。我们相信，通过开发新的词汇来建立人机共享的语言，可以解决这一沟通问题。成功的新生词语具有有用的抽象度：既不过于详细，以至在多种情境下可以重复使用，又不过于抽象，以至于能传递精确信息。作为概念验证，我们展示了“长度新生词”如何能够控制大型语言模型（LLM）的响应长度，而“多样性新生词”则允许生成更多样化和变化的响应。综合起来，我们论证认为，我们无法仅依靠现有的词汇来理解人工智能，通过扩大词汇库以包含新的词汇，不仅提供了更好地控制机器的机会，还提供了更好地理解机器的机会。', 'title_zh': '我们无法用现有的词汇理解AI'}
{'arxiv_id': 'arXiv:2502.07555', 'title': 'O1 Embedder: Let Retrievers Think Before Action', 'authors': 'Ruin Yan, Zheng Liu, Defu Lian', 'link': 'https://arxiv.org/abs/2502.07555', 'abstract': "The growing power of large language models (LLMs) has revolutionized how people access and utilize information. Notably, the LLMs excel at performing fine-grained data representation, which facilitates precise retrieval of information. They also generate high-quality answers based on external references, enabling the production of useful knowledge. The recent introduction of reasoning models, like OpenAI O1 and DeepSeek R1, marks another leap forward, highlighting LLMs' ability to think progressively before delivering final answers. This breakthrough significantly improves the ability to address complex tasks, e.g., coding and math proofs.\nInspired by this progress, we aim to develop similar capabilities for retrieval models, which hold great promise for tackling critical challenges in the field, including multi-task retrieval, zero-shot retrieval, and tasks requiring intensive reasoning of complex relationships. With this motivation, we propose a novel approach called O1 Embedder, which generates useful thoughts for the input query before making retrieval for the target documents. To realize this objective, we conquer two technical difficulties. First, we design a data synthesis workflow, creating training signals for O1 Embedder by generating initial thoughts from an LLM-expert and subsequently refining them using a retrieval committee. Second, we optimize the training process, enabling a pre-trained model to be jointly fine-tuned to generate retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning. Our approach is evaluated by comprehensive experiments, where substantial improvements are achieved across 12 popular datasets, spanning both in-domain and out-of-domain scenarios. These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models.", 'abstract_zh': '大型语言模型（LLMs）的力量不断增强，这已彻底改变了人们获取和利用信息的方式。值得注意的是，LLMs在进行细粒度数据表示方面表现出色，从而促进了精准的信息检索。它们也能基于外部参考生成高质量的答案，使知识生产变得有用。近期引入的推理模型，如OpenAI O1和DeepSeek R1，标志着又一个重要进步，突显了LLMs具备在提供最终答案之前逐步推理的能力。这一突破显著提高了应对复杂任务的能力，例如编程和数学证明。\n\n受到这一进展的启发，我们旨在为检索模型开发类似的能力，这些模型在处理领域内和跨领域的重要挑战方面具有巨大潜力，包括多任务检索、零样本检索以及需要对复杂关系进行深入推理的任务。为了实现这一目标，我们提出了一种名为O1 Embedder的新方法，在进行目标文档检索之前，O1 Embedder能生成输入查询的有用想法。为了实现这一目标，我们克服了两个技术难题。首先，我们设计了一个数据合成工作流程，通过生成LLM专家初稿思想并随后使用检索委员会对其进行细化，为O1 Embedder生成训练信号。其次，我们优化了训练过程，使得预先训练的模型能够通过行为克隆进行联合微调，生成检索思想，并通过对比学习进行密集检索。我们的方法在全面的实验中进行了评估，在12个流行的基准数据集中均取得了显著的改进，这些数据集涵盖了领域内和领域外的情景。这些结果展示了O1 Embedder出色的准确性和泛化能力，为下一代IR基础模型的开发铺平了道路。', 'title_zh': 'O1嵌入器：让检索器在行动前思考'}
{'arxiv_id': 'arXiv:2502.07552', 'title': 'Unsupervised Translation of Emergent Communication', 'authors': 'Ido Levy, Orr Paradise, Boaz Carmeli, Ron Meir, Shafi Goldwasser, Yonatan Belinkov', 'link': 'https://arxiv.org/abs/2502.07552', 'abstract': "Emergent Communication (EC) provides a unique window into the language systems that emerge autonomously when agents are trained to jointly achieve shared goals. However, it is difficult to interpret EC and evaluate its relationship with natural languages (NL). This study employs unsupervised neural machine translation (UNMT) techniques to decipher ECs formed during referential games with varying task complexities, influenced by the semantic diversity of the environment. Our findings demonstrate UNMT's potential to translate EC, illustrating that task complexity characterized by semantic diversity enhances EC translatability, while higher task complexity with constrained semantic variability exhibits pragmatic EC, which, although challenging to interpret, remains suitable for translation. This research marks the first attempt, to our knowledge, to translate EC without the aid of parallel data.", 'abstract_zh': 'Emergent Communication (EC) 提供了一种独特的窗口，让我们能够了解当代理器在实现共同目标时自主形成的语言系统。然而，EC 的解释以及它与自然语言（NL）的关系仍然难以评估。本研究采用无监督神经机器翻译（UNMT）技术，解密在不同任务复杂度的指称游戏中形成的 EC，环境的语义多样性对这些游戏产生了影响。我们的研究结果表明，UNMT 具有将 EC 翻译成自然语言的潜力；任务复杂度由语义多样性描述时，EC 的可翻译性得到了增强；而具有受限语义多样性的高复杂度任务则产生了实用性的 EC，尽管这类 EC 难以解释，但仍适于翻译。本研究是我们所知的首次尝试在无平行数据辅助的情况下翻译 EC。', 'title_zh': '无监督新兴通信的翻译'}
{'arxiv_id': 'arXiv:2502.07544', 'title': 'Grammar Control in Dialogue Response Generation for Language Learning Chatbots', 'authors': 'Dominik Glandorf, Peng Cui, Detmar Meurers, Mrinmaya Sachan', 'link': 'https://arxiv.org/abs/2502.07544', 'abstract': "Chatbots based on large language models offer cheap conversation practice opportunities for language learners. However, they are hard to control for linguistic forms that correspond to learners' current needs, such as grammar. We control grammar in chatbot conversation practice by grounding a dialogue response generation model in a pedagogical repository of grammar skills. We also explore how this control helps learners to produce specific grammar. We comprehensively evaluate prompting, fine-tuning, and decoding strategies for grammar-controlled dialogue response generation. Strategically decoding Llama3 outperforms GPT-3.5 when tolerating minor response quality losses. Our simulation predicts grammar-controlled responses to support grammar acquisition adapted to learner proficiency. Existing language learning chatbots and research on second language acquisition benefit from these affordances. Code available on GitHub.", 'abstract_zh': '基于大规模语言模型的聊天机器人为语言学习者提供了廉价的对话练习机会。然而，它们难以针对学习者当前的需要控制语言形式，比如语法。我们通过将对话应答生成模型嵌入到教学性的语法技能资源中，来控制聊天机器人的语法。我们还探讨了这种控制如何帮助学习者生成特定的语法形式。我们全面评估了语法控制对话应答生成的提示、微调和解码策略。战略性解码Llama3在容忍较小的应答质量损失的情况下优于GPT-3.5。我们的模拟预测，由语法控制的回应将支持适合学习者水平的语法习得。现有的语言学习聊天机器人和第二语言习得研究能够从这些功能中受益。相关代码可在GitHub上获取。', 'title_zh': '语言学习对话机器人中语法控制在对话响应生成中的应用'}
{'arxiv_id': 'arXiv:2502.07541', 'title': 'Corporate Greenwashing Detection in Text - a Survey', 'authors': 'Tom Calamai, Oana Balalau, Théo Le Guenedal, Fabian M. Suchanek', 'link': 'https://arxiv.org/abs/2502.07541', 'abstract': 'Greenwashing is an effort to mislead the public about the environmental impact of an entity, such as a state or company. We provide a comprehensive survey of the scientific literature addressing natural language processing methods to identify potentially misleading climate-related corporate communications, indicative of greenwashing. We break the detection of greenwashing into intermediate tasks, and review the state-of-the-art approaches for each of them. We discuss datasets, methods, and results, as well as limitations and open challenges. We also provide an overview of how far the field has come as a whole, and point out future research directions.', 'abstract_zh': '绿色洗牌是指采取措施误导公众关于某个实体（如国家或公司）的环境影响。本文提供了一篇关于自然语言处理方法的全面文献综述，这些方法旨在识别可能误导的气候相关企业通信，反映出绿色洗牌现象。我们将绿色洗牌的检测分解为中间任务，并回顾了每个任务的最新方法。我们讨论了数据集、方法和结果，以及局限性和开放性挑战。同时，我们也概述了该领域整体的发展情况，并指出了未来的研究方向。', 'title_zh': '企业绿色宣传识别文本研究——一种综述'}
{'arxiv_id': 'arXiv:2502.07490', 'title': 'Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More', 'authors': 'Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu', 'link': 'https://arxiv.org/abs/2502.07490', 'abstract': "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.", 'abstract_zh': '大型语言模型（LLMs）被发现存在准确检索关键信息的问题。为了解决这一问题，我们提出了Mask-Enhanced Autoregressive Prediction (MEAP)，这是一种简单而有效的训练范式，能够无缝将Masked Language Modeling (MLM) 集成到Next-Token Prediction (NTP) 中，从而增强其上下文检索能力。具体而言，MEAP 首先随机掩蔽一小部分输入词元，然后使用仅解码器的Transformer 进行标准的下一词预测自回归。MEAP 消除了需要双向注意力或编码器-解码器架构来进行MLM 的需求，在预训练和推理阶段不会增加额外的计算开销。密集的实验证明，MEAP 在关键信息检索和长上下文推理任务上的表现显著优于NTP，而在常识推理任务上的表现则不输或更优。MEAP 的优势还扩展到监督微调，它在“丢失中间信息”的场景中表现出显著优势，比NTP 高出11.77 个百分点。我们的分析表明，MEAP 的有效性来自于它能够通过集中关注未掩蔽的词元集合，促进更有区别的注意力评分。这一机制能够提高模型对任务相关信号的关注度，同时减少边缘上下文的影响。这些发现将MEAP 定位为一种有前途的大型语言模型训练范式。', 'title_zh': '掩码增强自回归预测：学会更多的同时关注更少'}
{'arxiv_id': 'arXiv:2502.07487', 'title': 'Multi-Agent Collaboration for Multilingual Code Instruction Tuning', 'authors': 'Jian Yang, Wei Zhang, Jiaxi Yang, Yibo Miao, Shanghaoran Quan, Zhenhe Wu, Qiyao Peng, Liqun Yang, Tianyu Liu, Zeyu Cui, Binyuan Hui, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.07487', 'abstract': 'Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.', 'abstract_zh': '近年来，在代码理解和生成方面的最新进展表明，使用高质量指令数据集微调的代码LLM可以获得强大的能力来解决广泛的代码相关任务。然而，大多数现有方法主要将每种编程语言孤立看待，并忽略了不同编程语言之间的知识迁移。为了弥合不同编程语言之间的差距，我们引入了一种新颖的多智能体协作框架，以增强多语言指令调优，其中多个具有生成记忆的语言特定智能体组件共同工作，以高效有效地从一种语言转移知识到另一种语言。具体来说，我们首先从代码片段中生成语言特定的指令数据，然后将生成的数据作为语言特定代理的种子数据。多个语言特定代理讨论并合作制定一个新的指令及其相应解决方案（新的编程语言或现有编程语言）。为了进一步促进跨语言迁移，每个代理都会将其生成历史记录作为记忆存储，并总结其优点和缺点。最后，高质量的多语言指令数据用于促进不同编程语言之间的知识转移，以训练Qwen2.5-xCoder。多语言编程基准上的实验结果表明，Qwen2.5-xCoder在共享通用知识方面具有优越性能，突显了其减小跨语言差距的潜力。', 'title_zh': '多agent协作的多语言代码指令调优'}
{'arxiv_id': 'arXiv:2502.07459', 'title': 'PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian', 'authors': 'Erfan Moosavi Monazzah, Vahid Rahimzadeh, Yadollah Yaghoobzadeh, Azadeh Shakery, Mohammad Taher Pilehvar', 'link': 'https://arxiv.org/abs/2502.07459', 'abstract': 'Large language models predominantly reflect Western cultures, largely due to the dominance of English-centric training data. This imbalance presents a significant challenge, as LLMs are increasingly used across diverse contexts without adequate evaluation of their cultural competence in non-English languages, including Persian. To address this gap, we introduce PerCul, a carefully constructed dataset designed to assess the sensitivity of LLMs toward Persian culture. PerCul features story-based, multiple-choice questions that capture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is curated with input from native Persian annotators to ensure authenticity and to prevent the use of translation as a shortcut. We evaluate several state-of-the-art multilingual and Persian-specific LLMs, establishing a foundation for future research in cross-cultural NLP evaluation. Our experiments demonstrate a 11.3% gap between best closed source model and layperson baseline while the gap increases to 21.3% by using the best open-weight model. You can access the dataset from here: this https URL', 'abstract_zh': '大型语言模型主要反映了西方文化，这主要是由于以英语为中心的训练数据占主导地位。这种不平衡性提出了一项重大挑战，因为LLM在多元化的应用场景中被越来越广泛地使用，但这些模型在非英语语言（包括波斯语）的文化适应性方面缺乏足够的评估。为了填补这一空白，我们引入了PerCul数据集，该数据集旨在评估LLM对波斯文化的敏感度。PerCul包含基于故事的多项选择题，能够捕捉到文化差异性情景。与现有的基准测试不同，PerCul经过本地波斯语注释者的精心策划，以确保其真实性和防止使用翻译作为捷径。我们评估了几种最新的多语言和特定于波斯语的LLM，为未来跨文化自然语言处理评估研究奠定了基础。我们的实验结果表明，在最优质的封闭源模型和一般人群基线之间的差距为11.3%，而使用最佳公开权重模型时，这一差距扩大至21.3%。您可以通过以下链接访问该数据集：[此链接](this https URL)。', 'title_zh': 'PerCul：以故事驱动的波斯语大型语言模型文化评估'}
{'arxiv_id': 'arXiv:2502.07445', 'title': 'Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon', 'authors': 'Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen', 'link': 'https://arxiv.org/abs/2502.07445', 'abstract': "Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.", 'abstract_zh': '大规模语言模型（LLMs）在公开基准测试中往往表现出色，但这些高分可能掩盖了模型对特定数据集表面特征的过度依赖，而非真正理解语言。我们引入了变色龙基准过拟合检测器（C-BOD），这是一种元评估框架，通过参数化转换系统地扭曲基准提示，并检测LLMs的过拟合。通过在保持语义内容和标签不变的情况下重新措辞输入，C-BOD揭示了模型性能是否由记忆化的模式驱动。在MMLU基准测试上评估26个领先的LLM模型时，我们的方法在适度扰动下揭示了平均性能下降2.15%，其中26个模型中有20个模型在统计上表现出显著差异。值得注意的是，基线准确率较高的模型在扰动下的性能差异较大，而较大的LLM对重新措辞更为敏感，这表明这两种情况可能过度依赖固定提示模式。相比之下，Llama家族模型和基线准确率较低的模型在扰动下的性能下降不显著，表明其对表面特征的依赖性较低。此外，C-BOD的设计既不依赖于特定的数据集，也不依赖于特定的模型，这使其可以轻松集成到训练管道中，促进更稳健的语言理解。我们的研究结果挑战了研究社区仅依赖排行榜得分的做法，并强调了在LLM评估中优先考虑韧性和泛化的重要性。', 'title_zh': '忽略你对大语言模型评估的认识——大语言模型如同一种变色龙'}
{'arxiv_id': 'arXiv:2502.07442', 'title': 'Hierarchical Document Parsing via Large Margin Feature Matching and Heuristics', 'authors': 'Duong Anh Kiet', 'link': 'https://arxiv.org/abs/2502.07442', 'abstract': 'We present our solution to the AAAI-25 VRD-IU challenge, achieving first place in the competition. Our approach integrates large margin loss for improved feature discrimination and employs heuristic rules to refine hierarchical relationships. By combining a deep learning-based matching strategy with greedy algorithms, we achieve a significant boost in accuracy while maintaining computational efficiency. Our method attains an accuracy of 0.98904 on the private leaderboard, demonstrating its effectiveness in document structure parsing. Source codes are publicly available at this https URL', 'abstract_zh': '我们提出了对AAAI-25 VRD-IU挑战赛的解决方案，并在比赛中获得了第一名。我们的方法结合了大.margin损失以提高特征鉴别能力，并采用了启发式规则来细化层次关系。通过将基于深度学习的匹配策略与贪婪算法相结合，我们在保持计算效率的同时大幅提升了准确性。我们的方法在私有排行榜上取得了0.98904的准确率，这证明了其在文档结构解析方面的有效性。源代码已在此网页上公开：[此链接]', 'title_zh': '通过大 Margin 特征匹配和启发式方法进行多层次文档解析'}
{'arxiv_id': 'arXiv:2502.07424', 'title': 'RomanLens: Latent Romanization and its role in Multilinguality in LLMs', 'authors': 'Alan Saji, Jaavid Aktar Husain, Thanmay Jayakumar, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra, Ratish Puduppully', 'link': 'https://arxiv.org/abs/2502.07424', 'abstract': "Large Language Models (LLMs) exhibit remarkable multilingual generalization despite being predominantly trained on English-centric corpora. A fundamental question arises: how do LLMs achieve such robust multilingual capabilities? For non-Latin script languages, we investigate the role of romanization - the representation of non-Latin scripts using Latin characters - as a bridge in multilingual processing. Using mechanistic interpretability techniques, we analyze next-token generation and find that intermediate layers frequently represent target words in romanized form before transitioning to native script, a phenomenon we term Latent Romanization. Further, through activation patching experiments, we demonstrate that LLMs encode semantic concepts similarly across native and romanized scripts, suggesting a shared underlying representation. Additionally in translation towards non Latin languages, our findings reveal that when the target language is in romanized form, its representations emerge earlier in the model's layers compared to native script. These insights contribute to a deeper understanding of multilingual representation in LLMs and highlight the implicit role of romanization in facilitating language transfer. Our work provides new directions for potentially improving multilingual language modeling and interpretability.", 'abstract_zh': '尽管大型语言模型（LLMs）主要是基于以英语为中心的数据集进行训练，它们在多种语言上的泛化能力依然表现出色。一个基本问题随之浮现：LLMs是如何实现如此强大的多语言能力的？对于非拉丁字母体系的语言，我们研究了罗马化——即将非拉丁字母体系的文字表示为拉丁字母——在多语言处理中的作用。通过机械解释技术，我们分析了下一个词的生成过程，并发现中间层频繁地以罗马化形式表示目标词汇，随后过渡到本族文字，我们将其称为潜在罗马化。此外，通过激活塑形实验，我们展示了LLMs在本族文字和罗马化文字中编码语义概念的相似性，这表明存在一种共有的底层表示。另外，在翻译到非拉丁字母语言时，我们的研究发现，当目标语言以罗马化形式出现时，它的表示在模型的早期层中出现，相比于本族文字则更早。这些见解有助于更深入地理解LLMs中的多语言表示，并强调罗马化在促进语言迁移中的隐含作用。我们的工作为改进多语言语言建模和解释提供了新的方向。', 'title_zh': 'RomanLens: 潜在罗马化及其在大规模语言模型中的多语言性作用'}
{'arxiv_id': 'arXiv:2502.07418', 'title': 'Entity Linking using LLMs for Automated Product Carbon Footprint Estimation', 'authors': 'Steffen Castle, Julian Moreno Schneider, Leonhard Hennig, Georg Rehm', 'link': 'https://arxiv.org/abs/2502.07418', 'abstract': 'Growing concerns about climate change and sustainability are driving manufacturers to take significant steps toward reducing their carbon footprints. For these manufacturers, a first step towards this goal is to identify the environmental impact of the individual components of their products. We propose a system leveraging large language models (LLMs) to automatically map components from manufacturer Bills of Materials (BOMs) to Life Cycle Assessment (LCA) database entries by using LLMs to expand on available component information. Our approach reduces the need for manual data processing, paving the way for more accessible sustainability practices.', 'abstract_zh': '关于气候变化和可持续发展的深切担忧正在推动制造商采取重大措施减少自身的碳足迹。对于这些制造商来说，实现这一目标的第一步是识别其产品各个组件的环境影响。我们提出了一种基于大型语言模型（LLM）的系统，利用LLM扩展现有组件信息，自动将制造商的物料清单（BOM）中的组件映射到生命周期评估（LCA）数据库条目。我们的方法减少了手动数据处理的需要，为更广泛的可持续实践打开了大门。', 'title_zh': '使用大语言模型进行自动产品碳足迹估算的实体链接方法'}
{'arxiv_id': 'arXiv:2502.07391', 'title': 'Target-Augmented Shared Fusion-based Multimodal Sarcasm Explanation Generation', 'authors': 'Palaash Goel, Dushyant Singh Chauhan, Md Shad Akhtar', 'link': 'https://arxiv.org/abs/2502.07391', 'abstract': "Sarcasm is a linguistic phenomenon that intends to ridicule a target (e.g., entity, event, or person) in an inherent way. Multimodal Sarcasm Explanation (MuSE) aims at revealing the intended irony in a sarcastic post using a natural language explanation. Though important, existing systems overlooked the significance of the target of sarcasm in generating explanations. In this paper, we propose a Target-aUgmented shaRed fusion-Based sarcasm explanatiOn model, aka. TURBO. We design a novel shared-fusion mechanism to leverage the inter-modality relationships between an image and its caption. TURBO assumes the target of the sarcasm and guides the multimodal shared fusion mechanism in learning intricacies of the intended irony for explanations. We evaluate our proposed TURBO model on the MORE+ dataset. Comparison against multiple baselines and state-of-the-art models signifies the performance improvement of TURBO by an average margin of $+3.3\\%$. Moreover, we explore LLMs in zero and one-shot settings for our task and observe that LLM-generated explanation, though remarkable, often fails to capture the critical nuances of the sarcasm. Furthermore, we supplement our study with extensive human evaluation on TURBO's generated explanations and find them out to be comparatively better than other systems.", 'abstract_zh': '讽刺是一种语言现象，旨在以固有的方式嘲讽目标（例如，实体、事件或人物）。多模态讽刺解释（MuSE）旨在通过自然语言解释揭示讽刺性帖子中的意图讽刺。尽管重要，但现有系统在生成解释时忽视了讽刺目标的重要性。在本文中，我们提出了一种目标增强共享融合的讽刺解释模型，简称TURBO。我们设计了一种新颖的共享融合机制以利用图像与其描述之间的跨模态关系。TURBO 假设讽刺的目标并引导多模态共享融合机制学习意图讽刺的细微之处以进行解释。我们使用 MORE+ 数据集评估了我们提出的 TURBO 模型。与多个基线模型和最新模型的比较表明，TURBO 的性能平均提高了 $+3.3\\%$。此外，我们还探索了零-shot 和 one-shot 情境下的大规模语言模型（LLM）在我们的任务中的应用，并观察到虽然 LLM 生成的解释非常出色，但往往未能捕捉到讽刺的关键细微之处。同时，我们进行了广泛的针对 TURBO 生成的解释的人类评估，发现它们相对于其他系统而言更为优异。', 'title_zh': '目标增强共享融合多模态讽刺解释生成'}
{'arxiv_id': 'arXiv:2502.07386', 'title': 'Parametric type design in the era of variable and color fonts', 'authors': 'Santhosh Thottingal', 'link': 'https://arxiv.org/abs/2502.07386', 'abstract': "Parametric fonts are programatically defined fonts with variable parameters, pioneered by Donald Kunth with his MetaFont technology in the 1980s. While Donald Knuth's ideas in MetaFont and subsequently in MetaPost are often seen as legacy techniques from the pre-graphical user interface (GUI) era of type design, recent trends like variable fonts suggest a resurgence of certain principles. This paper explores a modern type design process built on parametric design principles, specifically using MetaPost. The author created two variable fonts with this method and released them under a free, open-source license. The paper details the methodology, workflow, and insights gained from this process.", 'abstract_zh': '参数化字体是一种具有可变参数的程序化定义字体，由唐纳德·克努特（Donald Knuth）在20世纪80年代通过MetaFont技术率先提出。虽然克努特（Knuth）在MetaFont和随后的MetaPost中的设计理念常被视为图形用户界面（GUI）时代前的遗产技术，但现代趋势如可变字体表明某些原则正在复兴。本文探讨了基于参数化设计原则的现代字体设计流程，具体使用了MetaPost。作者采用了这种方法创建了两个可变字体，并在自由开源许可下发布了它们。本文详细介绍了该方法的研究方法、工作流程以及从这一过程中获得的见解。', 'title_zh': '变体和彩色字体时代的参数化字体设计'}
{'arxiv_id': 'arXiv:2502.07365', 'title': 'LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation', 'authors': 'Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, Weipeng Chen', 'link': 'https://arxiv.org/abs/2502.07365', 'abstract': "Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines.", 'abstract_zh': '大型语言模型（LLMs）通过扩展位置编码和轻量级连续预训练获得了更长的上下文窗口。然而，这常常导致其在短文本任务上的性能下降，而这些性能下降的具体原因仍缺乏充分的探讨。本文我们识别出两个主要因素导致了这一问题：隐藏状态和注意力分数的分布漂移，以及连续预训练过程中的灾难性遗忘。为解决这些问题，我们提出了一种名为Long Context Pre-training with Restoration Distillation（LongReD）的新颖方法，旨在通过最小化扩展模型和原始模型之间分布差异来缓解短文本上的性能下降。除了使用长文本进行训练，LongReD 还从原始模型中选择层的隐藏状态在短文本上进行蒸馏。此外，LongReD 还引入了短到长的蒸馏方法，通过利用跳过的位置索引使短文本上的输出分布与长文本上的输出分布对齐。在通用文本基准测试上的实验结果表明，LongReD 在保持模型短文本性能的同时，还能在处理长文本方面与基准模型具有相当甚至更好的能力。', 'title_zh': '长文境：通过恢复蒸馏缓解长背景大规模语言模型中的短文本退化问题'}
{'arxiv_id': 'arXiv:2502.07352', 'title': 'Bridging the Evaluation Gap: Leveraging Large Language Models for Topic Model Evaluation', 'authors': "Zhiyin Tan, Jennifer D'Souza", 'link': 'https://arxiv.org/abs/2502.07352', 'abstract': "This study presents a framework for automated evaluation of dynamically evolving topic taxonomies in scientific literature using Large Language Models (LLMs). In digital library systems, topic modeling plays a crucial role in efficiently organizing and retrieving scholarly content, guiding researchers through complex knowledge landscapes. As research domains proliferate and shift, traditional human centric and static evaluation methods struggle to maintain relevance. The proposed approach harnesses LLMs to measure key quality dimensions, such as coherence, repetitiveness, diversity, and topic-document alignment, without heavy reliance on expert annotators or narrow statistical metrics. Tailored prompts guide LLM assessments, ensuring consistent and interpretable evaluations across various datasets and modeling techniques. Experiments on benchmark corpora demonstrate the method's robustness, scalability, and adaptability, underscoring its value as a more holistic and dynamic alternative to conventional evaluation strategies.", 'abstract_zh': '本研究提出了一种利用大型语言模型（LLMs）自动评估动态演化的科学文献主题分类的框架。在数字图书馆系统中，主题建模在高效组织和检索学术内容、引导研究人员通过复杂的知识景观方面发挥着关键作用。随着研究领域的扩展和变化，传统的基于人工和静态的评估方法难以保持相关性。提出的这种方法利用LLMs衡量关键的质量维度，如一致性、重复性、多样性和主题-文档对齐，而不依赖于专家注释或狭窄的统计指标。定制化的提示语指导LLMs的评估，确保在不同数据集和建模技术下的一致性和可解释性评估。基准语料库上的实验表明该方法的鲁棒性、可扩展性和灵活性，突显了其作为比传统评估策略更具全面性和动态性的替代方法的价值。', 'title_zh': '填补评估差距：利用大型语言模型进行主题模型评估'}
{'arxiv_id': 'arXiv:2502.07346', 'title': 'BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models', 'authors': 'Xu Huang, Wenhao Zhu, Hanxu Hu, Conghui He, Lei Li, Shujian Huang, Fei Yuan', 'link': 'https://arxiv.org/abs/2502.07346', 'abstract': 'Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.', 'abstract_zh': '之前的多语言基准主要集中在简单的理解任务上，但对于大型语言模型（LLMs），我们更强调其在指令遵循、推理、长文理解、代码生成等方面的能力。然而，这些高级能力跨语言的衡量仍然存在不足。为解决这一问题，我们引入了BenchMAX，这是一种多语言评估基准，允许在多种语言中公平比较这些重要的能力。为了保持高质量，数据从英语翻译成16种其他语言后，三名独立的母语标注者分别对所有任务的每个样本进行标注。此外，我们还介绍了数据集构建过程中产生的新颖翻译挑战。BenchMAX 上的大量实验揭示了这些核心能力在不同语言中的不同有效性，突出了不能仅通过增加模型规模来弥合的性能差距。BenchMAX 作为一个全面的多语言评估平台，提供了一个有前景的测试环境，促进多语言语言模型的发展。该数据集和代码均已公开。', 'title_zh': 'BenchMAX：全面的多语言大型语言模型评估套件'}
{'arxiv_id': 'arXiv:2502.07340', 'title': 'Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering', 'authors': 'Shuzheng Si, Haozhe Zhao, Gang Chen, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Kaikai An, Kangyang Luo, Chen Qian, Fanchao Qi, Baobao Chang, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.07340', 'abstract': "Training LLMs on data that contains unfamiliar knowledge during the instruction tuning stage can make LLMs overconfident and encourage hallucinations. To address this challenge, we introduce a novel framework, NOVA, which identifies high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity to enhance data quality. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less. Extensive experiments and analysis show that NOVA significantly reduces hallucinations and allows LLMs to maintain a strong ability to follow instructions.", 'abstract_zh': '在指令调优阶段使用包含未 Familiar Knowledge 的数据训练语言模型会使语言模型过于自信并促进幻觉的产生。为解决这一挑战，我们引入了一种新颖的框架 NOVA，该框架通过识别与语言模型所学知识高度一致的高质量数据来减少幻觉。NOVA 包含内部一致性探针（ICP）和语义等价性识别（SEI），用于衡量语言模型对指令数据的熟悉程度。具体而言，ICP 通过计算多个自动生成响应之间的定制一致性来评估语言模型对给定指令的理解。SEI 进一步通过将其与生成的响应进行比较，使用提出的语义聚类和精心设计的投票策略来评估语言模型与目标响应的熟悉程度。最后，我们引入了一个专家对齐的奖励模型，考虑熟悉度之外的其他特征以提升数据质量。通过关注数据质量和避免使用未熟悉的数据，我们可以利用精选数据有效地使语言模型遵循指令并减少幻觉。广泛的实验和分析表明，NOVA 显著减少了幻觉，使语言模型能够保持强大的遵循指令能力。', 'title_zh': '通过有效数据过滤使大型语言模型更遵循指令并减少虚构内容'}
{'arxiv_id': 'arXiv:2502.07322', 'title': "MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs", 'authors': 'Zilu Dong, Xiangqing Shen, Rui Xia', 'link': 'https://arxiv.org/abs/2502.07322', 'abstract': "As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover a critical limitation that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals that the root cause lies in MEMIT's key value modeling framework: When multiple facts with the same subject in a batch are modeled through MEMIT's key value mechanism, identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in updates conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in same-subject batch editing scenarios. Experimental results demonstrate that when MEMIT's edit success rate drops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate exceeding 90%, showcasing remarkable robustness to subject entity collisions.", 'abstract_zh': '随着大型语言模型不断扩展，无需完全重新训练即可修改模型内部知识的知识编辑技术受到了广泛关注。MEMIT 是一种突出的批量编辑算法，因其能够执行大规模知识修改而备受瞩目。然而，我们发现 MEMIT 在处理包含多个针对相同主题的编辑的批量数据时，其编辑效果显著下降。我们的分析表明，这一关键限制的原因在于 MEMIT 的键值建模框架：当 MEMIT 通过其键值机制建模一批具有相同主题的多条事实时，相同的键（源自共享的主题）被迫表示不同的值（对应不同的知识），从而在编辑过程中产生更新冲突。为解决这一问题，我们提出了 MEMIT-Merge，一种改进的方法，该方法将具有相同主题的事实值计算过程合并，有效解决了相同主题批量编辑场景中的性能下降问题。实验结果表明，当 MEMIT 的编辑成功率在批量大小较大时降至约 50% 时，MEMIT-Merge 的成功率仍保持在 90% 以上，显示出对主题实体冲突的显著鲁棒性。', 'title_zh': 'MEMIT-合并：在LLMs同一主题批量编辑中解决MEMIT的键值冲突问题'}
{'arxiv_id': 'arXiv:2502.07316', 'title': 'CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction', 'authors': 'Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He', 'link': 'https://arxiv.org/abs/2502.07316', 'abstract': 'Reasoning is a fundamental capability of Large Language Models. While prior research predominantly focuses on enhancing narrow skills like math or code generation, improving performance on many other reasoning tasks remains challenging due to sparse and fragmented training data. To address this issue, we propose CodeI/O, a novel approach that systematically condenses diverse reasoning patterns inherently embedded in contextually-grounded codes, through transforming the original code into a code input-output prediction format. By training models to predict inputs/outputs given code and test cases entirely in natural language as Chain-of-Thought (CoT) rationales, we expose them to universal reasoning primitives -- like logic flow planning, state-space searching, decision tree traversal, and modular decomposition -- while decoupling structured reasoning from code-specific syntax and preserving procedural rigor. Experimental results demonstrate CodeI/O leads to consistent improvements across symbolic, scientific, logic, math & numerical, and commonsense reasoning tasks. By matching the existing ground-truth outputs or re-executing the code with predicted inputs, we can verify each prediction and further enhance the CoTs through multi-turn revision, resulting in CodeI/O++ and achieving higher performance. Our data and models are available at this https URL.', 'abstract_zh': '大型语言模型具备推理这一基本能力。虽然先前的研究主要侧重于提升诸如数学或代码生成等狭窄技能，但改善其他多种推理任务的表现仍然面临挑战，这是因为可用的训练数据相对稀疏且碎片化。为解决这一问题，我们提出了一种名为CodeI/O的新方法，该方法系统地将上下文相关的代码中固有的各种推理模式进行压缩，通过将原始代码转化为代码输入输出预测格式。通过训练模型以自然语言形式的Chain-of-Thought（CoT，推理过程）来预测给定代码和测试用例的输入/输出，我们使模型接触到通用的推理原语，如逻辑流程规划、状态空间搜索、决策树遍历和模块分解，从而分离结构化推理和代码特定的语法，并保持程序的严谨性。实验结果显示，CodeI/O在符号、科学、逻辑、数学和数值以及常识推理任务上均取得了持续的改进。通过与现有正确输出匹配或使用预测输入重新执行代码，我们能够验证每个预测，并通过多轮修订进一步增强CoTs，从而产生CodeI/O++并实现更高性能。我们的数据集和模型可在以下链接获取：this https URL。', 'title_zh': 'CodeI/O：通过代码输入-输出预测浓缩推理模式'}
{'arxiv_id': 'arXiv:2502.07286', 'title': 'Small Language Model Makes an Effective Long Text Extractor', 'authors': 'Yelin Chen, Fanjin Zhang, Jie Tang', 'link': 'https://arxiv.org/abs/2502.07286', 'abstract': 'Named Entity Recognition (NER) is a fundamental problem in natural language processing (NLP). However, the task of extracting longer entity spans (e.g., awards) from extended texts (e.g., homepages) is barely explored. Current NER methods predominantly fall into two categories: span-based methods and generation-based methods. Span-based methods require the enumeration of all possible token-pair spans, followed by classification on each span, resulting in substantial redundant computations and excessive GPU memory usage. In contrast, generation-based methods involve prompting or fine-tuning large language models (LLMs) to adapt to downstream NER tasks. However, these methods struggle with the accurate generation of longer spans and often incur significant time costs for effective fine-tuning. To address these challenges, this paper introduces a lightweight span-based NER method called SeNER, which incorporates a bidirectional arrow attention mechanism coupled with LogN-Scaling on the [CLS] token to embed long texts effectively, and comprises a novel bidirectional sliding-window plus-shaped attention (BiSPA) mechanism to reduce redundant candidate token-pair spans significantly and model interactions between token-pair spans simultaneously. Extensive experiments demonstrate that our method achieves state-of-the-art extraction accuracy on three long NER datasets and is capable of extracting entities from long texts in a GPU-memory-friendly manner. Code: this https URL', 'abstract_zh': '命名实体识别（NER）是自然语言处理（NLP）中的一个基本问题。然而，从扩展文本（如主页）中提取较长实体跨度（如奖项）的任务几乎没有被探索过。目前的NER方法主要可分为两类：基于跨度的方法和生成式方法。基于跨度的方法需要枚举所有可能的令牌对跨度，然后对每个跨度进行分类，这会导致大量的冗余计算和过度的GPU内存使用。相比之下，生成式方法涉及通过提示或微调大型语言模型（LLMs）来适应下游的NER任务。然而，这些方法在生成较长跨度时不够准确，并且往往需要大量的时间成本来进行有效的微调。为了解决这些问题，本文提出了一种轻量级的基于跨度的NER方法，名为SeNER，它结合了双向箭头注意力机制以及在[CLS]标记上应用LogN-Scaling来有效嵌入长文本，并且采用了新颖的双向滑动窗口十字形注意力机制（BiSPA），显著减少了冗余的候选令牌对跨度的同时，能够同时建模令牌对跨度之间的相互作用。广泛的实验证明，我们的方法在三个长实体识别数据集上实现了最先进的提取准确率，并且能够以GPU内存友好的方式从长文本中提取实体。代码：请参阅此链接。\n\n注：这里的链接“this https URL”没有具体的内容，因此在翻译中的“代码：请参阅此链接”中并未包含该链接的具体内容。如果您有具体的链接内容，可以将其补充完整。', 'title_zh': '小型语言模型可有效提取长文本'}
{'arxiv_id': 'arXiv:2502.07272', 'title': 'GENERator: A Long-Context Generative Genomic Foundation Model', 'authors': 'Wei Wu, Qiuyi Li, Mingyang Li, Kun Fu, Fuli Feng, Jieping Ye, Hui Xiong, Zheng Wang', 'link': 'https://arxiv.org/abs/2502.07272', 'abstract': 'Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of promoter sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions.', 'abstract_zh': 'DNA测序技术的进步显著提高了我们解码基因组序列的能力。然而，预测和解释这些序列依然具有挑战性，因为遗传物质的复杂性很高。大型语言模型（LLMs）为生物序列分析带来了新的机遇。近年来，基因组语言模型的发展突显了LLMs在破译DNA序列方面的潜力。然而，现有模型在鲁棒性和应用范围方面仍面临限制，主要是由于模型结构和训练数据规模的限制。为解决这些限制，我们提出了一种命名为GENERator的生成性基因组基础模型，其上下文长度为98千个碱基对（bp），参数量为1.2亿。该模型是在包含386亿个碱基对的真核DNA大型数据集上训练的，展示了在现有和新提出的基准测试中的卓越性能。该模型遵循分子生物学的基本原理，能够准确地生成与已知家族结构上相似的蛋白质编码序列。此外，它在序列优化方面也表现出显著的潜力，特别是通过特定活动谱式的启动子序列的提示响应生成来实现。这些功能使GENЕRator成为基因组研究和生物技术进步的关键工具，增强了我们解释和预测复杂生物系统的能 力，并使精确的基因组干预成为可能。', 'title_zh': 'GENERator：一种生成式基因组基础模型，支持长上下文生成'}
{'arxiv_id': 'arXiv:2502.07223', 'title': 'Graph RAG-Tool Fusion', 'authors': 'Elias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason, James A. Burke, Vamse Kumar Subbiah', 'link': 'https://arxiv.org/abs/2502.07223', 'abstract': 'Recent developments in retrieval-augmented generation (RAG) for selecting relevant tools from a tool knowledge base enable LLM agents to scale their complex tool calling capabilities to hundreds or thousands of external tools, APIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails to capture structured dependencies between tools, limiting the retrieval accuracy of a retrieved tool\'s dependencies. For example, among a vector database of tools, a "get stock price" API requires a "stock ticker" parameter from a "get stock ticker" API, and both depend on OS-level internet connectivity tools. In this paper, we address this limitation by introducing Graph RAG-Tool Fusion, a novel plug-and-play approach that combines the strengths of vector-based retrieval with efficient graph traversal to capture all relevant tools (nodes) along with any nested dependencies (edges) within the predefined tool knowledge graph. We also present ToolLinkOS, a new tool selection benchmark of 573 fictional tools, spanning over 15 industries, each with an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool Fusion achieves absolute improvements of 71.7% and 22.1% over naïve RAG on ToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS dataset is available at this https URL', 'abstract_zh': '近些年，检索增强生成（RAG）技术在从工具知识库中选择相关工具方面取得了进展，使得语言模型（LLM）代理能够将其复杂的工具调用能力扩展到数百甚至数千个外部工具、API或“代理-as-工具”。然而，传统的基于RAG的工具检索方法未能捕捉到工具之间的结构化依赖关系，从而限制了检索到的工具及其依赖项的准确性。例如，在一个工具向量数据库中，“获取股票价格”API需要从“获取股票代码”API获取一个“股票代码”参数，并且两者都依赖于OS级别的互联网连接工具。在本文中，我们通过引入图RAG-工具融合方法来解决这一限制，这是一种新颖的插件式解决方案，将基于向量的检索优势与高效的图遍历相结合，以捕捉预定义的工具知识图中所有相关工具（节点）及其嵌套依赖关系（边）。我们还提出了一个名为ToolLinkOS的新工具选择基准，包含573种虚构工具，涵盖了15个不同的行业，平均每种工具有6.3个工具依赖关系。我们证明，图RAG-工具融合方法在ToolLinkOS和ToolSandbox基准上的绝对改进分别达到了71.7%和22.1%（mAP@10）。ToolLinkOS数据集可通过以下链接获取：[这里](this https URL)', 'title_zh': '图RAG工具有菜品融汇\n\n如果需要更准确的翻译，尤其是技术术语部分，可以进一步提供具体的背景信息或者术语来源。"Graph RAG-Tool Fusion" 这个标题看起来像是结合了图结构（Graph）和RAG（Retrieval-Augmented Generation）的一种工具融合方法。基于此，更准确和符合学术规范的翻译可以是：\n\n“图结构增强生成工具融合方法”\n\n这样可以更清晰地传达该技术的具体内容。'}
{'arxiv_id': 'arXiv:2502.07188', 'title': 'A Large-Scale Benchmark for Vietnamese Sentence Paraphrases', 'authors': 'Sang Quang Nguyen, Kiet Van Nguyen', 'link': 'https://arxiv.org/abs/2502.07188', 'abstract': 'This paper presents ViSP, a high-quality Vietnamese dataset for sentence paraphrasing, consisting of 1.2M original-paraphrase pairs collected from various domains. The dataset was constructed using a hybrid approach that combines automatic paraphrase generation with manual evaluation to ensure high quality. We conducted experiments using methods such as back-translation, EDA, and baseline models like BART and T5, as well as large language models (LLMs), including GPT-4o, Gemini-1.5, Aya, Qwen-2.5, and Meta-Llama-3.1 variants. To the best of our knowledge, this is the first large-scale study on Vietnamese paraphrasing. We hope that our dataset and findings will serve as a valuable foundation for future research and applications in Vietnamese paraphrase tasks.', 'abstract_zh': '本文介绍了ViSP，这是一个高质量的越南语语句改写数据集，包含120万条原始-改写对，数据来源于多个领域。该数据集采用混合方法构建，结合了自动改写生成和人工评估，以确保其高质量。我们使用了包括反向翻译、语料增强（EDA）以及基线模型BART和T5，以及大型语言模型（LLM），如GPT-4o、Gemini-1.5、Aya、Qwen-2.5和Meta-Llama-3.1变体等方法进行了实验。据我们所知，这是第一个大规模的越南语改写研究。我们希望我们的数据集和研究结果能为未来越南语改写任务的研究和应用奠定宝贵的基石。', 'title_zh': '越南句子近义表达的大规模基准数据集'}
{'arxiv_id': 'arXiv:2502.07186', 'title': 'Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs', 'authors': 'Sina Salimian, Gias Uddin, Most Husne Jahan, Shaina Raza', 'link': 'https://arxiv.org/abs/2502.07186', 'abstract': "Zero-shot LLMs are now also used for textual classification tasks, e.g., sentiment/emotion detection of a given input as a sentence/article. However, their performance can be suboptimal in such data annotation tasks. We introduce a novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's confidence for its classification of an input by leveraging Metamorphic Relations (MRs). The MRs generate semantically equivalent yet textually mutated versions of the input. Following the principles of Metamorphic Testing (MT), the mutated versions are expected to have annotation labels similar to the input. By analyzing the consistency of LLM responses across these variations, PCS computes a confidence score based on the frequency of predicted labels. PCS can be used both for single LLM and multiple LLM settings (e.g., majority voting). We introduce an algorithm Perceived Differential Evolution (PDE) that determines the optimal weights assigned to the MRs and the LLMs for a classification task. Empirical evaluation shows PCS significantly improves zero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3 (10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three models, PCS significantly outperforms majority voting by 7.75%.", 'abstract_zh': '零样本大型语言模型（LLMs）现在也被用于文本分类任务，例如对给定的句子/文章进行情感/情绪检测。然而，它们在数据标注任务中的表现可能不尽如人意。我们介绍了一种新颖的技术——感知置信评分（Perceived Confidence Scoring，PCS），该技术通过利用元形关系（MRs）来评估LLMs对其输入分类的置信度。元形关系生成与输入语义等价但文本形式不同的版本。根据元形测试（Metamorphic Testing，MT）的原则，这些变异版本的标注标签应与原始输入相似。通过分析LLMs在这些变异版本上的响应一致性，PCS根据预测标签出现的频率计算置信评分。PCS既适用于单个LLM设置，也适用于多个LLM设置（例如，多数投票）。我们还介绍了一种算法——感知变异进化（Perceived Differential Evolution，PDE），该算法确定了为分类任务分配给MRs和LLMs的最优权重。实证评估表明，PCS显著提高了Llama-3-8B-Instruct（4.96%）、Mistral-7B-Instruct-v0.3（10.52%）的零样本准确率，同时，Gemma-2-9b-it也取得了9.39%的提升。当结合这三种模型时，PCS比多数投票方法表现更优，提高了7.75%。', 'title_zh': '零样本大型语言模型下的感知置信度评分在数据标注中的应用'}
{'arxiv_id': 'arXiv:2502.07184', 'title': 'Refine Knowledge of Large Language Models via Adaptive Contrastive Learning', 'authors': 'Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, Shu-Yu Guo, Chao Qu, Xiaoyu Tan, Hai-Tao Zheng, Ying Shen, Philip S. Yu', 'link': 'https://arxiv.org/abs/2502.07184', 'abstract': "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness of our method.", 'abstract_zh': '如何缓解大规模语言模型（LLMs）的幻觉一直是LLMs研究领域追求的根本目标。通过对众多相关研究的综述，主流的方法之一是通过优化LLMs的知识表示来减少其输出中的幻觉现象。考虑到这些工作的核心在于模型所获取的知识，而知识一直是人类社会进步中的关键主题，我们相信模型提炼知识的过程可以从人类的学习方式中受益匪浅。在我们的工作中，通过模仿人类的学习过程，我们设计了一种自适应对比学习策略。该方法根据LLMs实际掌握的知识，灵活构建不同的正样本和负样本进行对比学习，从而帮助LLMs巩固其已掌握的正确知识，加深对已接触但尚未完全理解的正确知识的理解，忘记已经学过的错误知识，并诚实地承认其知识的不足。广泛使用的数据集上的实验结果和详细分析表明了该方法的有效性。', 'title_zh': '通过自适应对比学习精炼大型语言模型的知识'}
{'arxiv_id': 'arXiv:2502.07165', 'title': "Don't Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification", 'authors': 'Peipei Wei, Dimitris Dimitriadis, Yan Xu, Mingwei Shen', 'link': 'https://arxiv.org/abs/2502.07165', 'abstract': 'We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.', 'abstract_zh': '我们提出了基于原则的提示策略（PRINCIPLE-BASED PROMPTING），这是一种简单而有效的多智能体提示策略，适用于文本分类任务。该方法首先让多个大型语言模型（LLM）智能体独立地基于示例样本的分析（带标签或不带标签）生成候选原则，然后通过一个最终处理智能体将这些原则整合为最终原则，并将最终原则发送给分类智能体以执行下游分类任务。在不同大小的LLM上进行的二分类和多分类数据集的广泛实验表明，我们的方法不仅在宏F1评分上优于零-shot提示（1.55% - 19.37%的显著性能提升），还优于其他强基线方法（即显性推理和反向提示）。通过我们方法生成的原则在两个私有数据集上帮助LLM在分类任务上表现优于手工设计的原则。此外，我们的多智能体PRINCIPLE-BASED PROMPTING方法在性能上与基于示例的少样本提示方法持平或更好，但推理成本显著降低。消融研究显示，标签信息和多智能体合作的LLM框架在生成高质量原则以促进下游分类任务方面发挥着重要作用。', 'title_zh': '不只是演示，教给我原理：一种基于原理的多agent提示策略用于文本分类'}
{'arxiv_id': 'arXiv:2502.07164', 'title': 'Does Training on Synthetic Data Make Models Less Robust?', 'authors': 'Lingze Zhang, Ellie Pavlick', 'link': 'https://arxiv.org/abs/2502.07164', 'abstract': 'An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain "blindspots" by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our "blindspot" task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn\'t necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized.', 'abstract_zh': '越来越多的趋势是使用合成数据训练大型语言模型（LLMs）。通常情况下，生成这种合成数据的LLMs与用其训练的LLMs相同或相似。这引发了这样一个问题：合成数据是否可能会通过加强LLMs已编码的启发式策略，从而加剧某些“盲点”。在本文中，我们使用Llama-2-7B-hf模型在自然语言推理（NLI）任务上进行了模拟实验。我们使用MultiNLI作为一般任务，并使用HANS（一种旨在衡量NLI中特定启发式策略存在的目标评估集）作为我们的“盲点”任务。我们的目标是确定一般任务和盲点任务之间是否存在性能差异。我们的结果显示，合成数据并没有如我们预期的那样加强盲点。具体而言，我们发现，虽然使用合成数据进行微调并不会必然减少启发式的使用，但它也并不像我们的假设那样使启发式的使用变得更差。', 'title_zh': '使用合成数据训练会使模型更加脆弱还是更少脆弱？'}
{'arxiv_id': 'arXiv:2502.07143', 'title': 'Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning', 'authors': 'Jiayuan Zhu, Junde Wu', 'link': 'https://arxiv.org/abs/2502.07143', 'abstract': 'Accurate and efficient diagnosis in online medical consultations remains a challenge for current large language models. These models often rely on single-turn interactions and lack the ability to refine their predictions through follow-up questions. Additionally, their responses frequently contain complex medical terminology, making them less accessible to non-medical users and creating barriers to effective communication. In this paper, we introduce Ask Patients with Patience (APP), the first multi-turn dialogue that enables LLMs to iteratively refine diagnoses based on grounded reasoning. By integrating medical guidelines and entropy minimization, APP improves both diagnostic accuracy and efficiency. Furthermore, it features human-centric communication that bridges the gap between user comprehension and medical terminology, significantly enhancing user accessibility and engagement. We evaluated APP using a subset of the ReMeDi dataset, comparing it with single-turn and traditional multi-turn LLM baselines. APP achieved higher similarity scores in diagnosis predictions, demonstrating better alignment with ground truth diagnoses. Entropy analysis showed that APP reduces diagnostic uncertainty more rapidly across iterations, increasing confidence in its predictions. APP also excels in user accessibility and empathy, further bridging the gap between complex medical language and user understanding. Code will be released at: this https URL.', 'abstract_zh': '当前的大语言模型在在线医疗咨询中实现准确且高效的诊断仍面临挑战。这些模型通常依赖单一回合的交互，缺乏通过后续问题来逐步细化预测的能力。此外，它们的回答经常包含复杂医学术语，这使得对非医疗用户来说不太易懂，从而成为有效沟通的障碍。在本文中，我们提出了“耐心询问患者”（Ask Patients with Patience, APP），这是第一个多回合对话系统，能够使大语言模型基于情境推理逐步细化预测。通过结合医学指南和最小熵原理，APP提高了诊断的准确性和效率。此外，APP还包括以用户体验为中心的沟通方式，这种沟通方式缩小了用户理解和医学术语之间的差距，显著提高了用户使用的易用性和参与度。我们使用ReMeDi数据集的一部分对APP进行了评估，并将其与单回合和传统的多回合大语言模型基准进行了比较。APP在诊断预测相似度方面得分更高，表明其预测更有助于与真实诊断对齐。熵分析显示，APP在每次迭代中更快地减少了诊断不确定性，增强了其预测的信心。此外，APP在用户易用性和同理心方面表现出色，进一步缩小了复杂医学语言与用户理解之间的差距。代码将发布于：this https URL。', 'title_zh': '耐心问询患者：通过基于推理的支持来实现以人文本的医学对话的人工智能模型'}
{'arxiv_id': 'arXiv:2502.07139', 'title': 'Language-TPP: Integrating Temporal Point Processes with Language Models for Event Analysis', 'authors': 'Quyu Kong, Yixuan Zhang, Yang Liu, Panrong Tong, Enqi Liu, Feng Zhou', 'link': 'https://arxiv.org/abs/2502.07139', 'abstract': 'Temporal Point Processes (TPPs) have been widely used for event sequence modeling, but they often struggle to incorporate rich textual event descriptions effectively. Conversely, while Large Language Models (LLMs) have been shown remarkable capabilities in processing textual data, they lack mechanisms for handling temporal dynamics. To bridge this gap, we introduce Language-TPP, a unified framework that integrates TPPs with LLMs for enhanced event sequence modeling. Language-TPP introduces a novel temporal encoding mechanism that converts continuous time intervals into specialized byte-tokens, enabling seamless integration with standard LLM architectures. This approach allows Language-TPP to achieve state-of-the-art performance across multiple TPP tasks, including event time prediction, type prediction, and intensity estimation, on five datasets. Additionally, we demonstrate that incorporating temporal information significantly improves the quality of generated event descriptions.', 'abstract_zh': '时间点过程（TPPs）已经在事件序列建模中得到了广泛的应用，但它们往往难以有效融合丰富的文本事件描述。相反，尽管大规模语言模型（LLMs）在处理文本数据方面展现出了出色的能力，但它们缺乏处理时间动态的机制。为了解决这一问题，我们引入了Language-TPP，这是一种统一框架，将TPPs与LLMs结合，以增强事件序列建模的能力。Language-TPP引入了一种新颖的时间编码机制，将连续时间间隔转换为特化的字节令牌，从而能够无缝集成到标准的LLM架构中。这种方法使Language-TPP在包括事件时间预测、类型预测和强度估计在内的多个TPP任务中取得了最先进的性能，这些任务在五个数据集中得到验证。此外，我们还表明，引入时间信息可以显著提高生成的事件描述的质量。', 'title_zh': '语言-TPP模型：将时间点过程与语言模型集成用于事件分析'}
{'arxiv_id': 'arXiv:2502.07131', 'title': 'TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Model Bring? - A Case Study on Korea Financial Texts', 'authors': 'Yewon Hwang, Sungbum Jung, Hanwool Lee, Sara Yu', 'link': 'https://arxiv.org/abs/2502.07131', 'abstract': 'Domain specificity of embedding models is critical for effective performance. However, existing benchmarks, such as FinMTEB, are primarily designed for high-resource languages, leaving low-resource settings, such as Korean, under-explored. Directly translating established English benchmarks often fails to capture the linguistic and cultural nuances present in low-resource domains. In this paper, titled TWICE: What Advantages Can Low-Resource Domain-Specific Embedding Models Bring? A Case Study on Korea Financial Texts, we introduce KorFinMTEB, a novel benchmark for the Korean financial domain, specifically tailored to reflect its unique cultural characteristics in low-resource languages. Our experimental results reveal that while the models perform robustly on a translated version of FinMTEB, their performance on KorFinMTEB uncovers subtle yet critical discrepancies, especially in tasks requiring deeper semantic understanding, that underscore the limitations of direct translation. This discrepancy highlights the necessity of benchmarks that incorporate language-specific idiosyncrasies and cultural nuances. The insights from our study advocate for the development of domain-specific evaluation frameworks that can more accurately assess and drive the progress of embedding models in low-resource settings.', 'abstract_zh': '嵌入模型在特定域中的领域特异性对于有效性能至关重要。然而，现有的基准测试，如FinMTEB，主要针对高资源语言设计，而低资源语言环境，如韩语，则相对未被充分探索。直接将现有的英语基准测试翻译过来往往无法捕捉到低资源领域中存在的语言和文化细微差异。在本文中，我们提出了TWICE这一标题：“低资源领域特定嵌入模型能带来哪些优势？—基于韩国金融文本的案例研究”。我们介绍了KorFinMTEB，这是为韩国金融领域专门设计的一个新型基准测试，旨在反映其独特的文化特征，适用于低资源语言环境。实验结果表明，虽然模型在FinMTEB的翻译版本上表现出色，但在KorFinMTEB上的表现揭示了在需要深入语义理解的任务中的一些微妙但至关重要的差异，这突显了直接翻译的局限性。这种差异表明，需要包含语言特异性和文化细微差异的基准测试。我们的研究结果为低资源环境中的嵌入模型的开发提供了见解，主张构建专门领域的评估框架，以更准确地评估和推动嵌入模型的发展。', 'title_zh': 'TWICE：低资源领域特定嵌入模型能带来哪些优势？——以韩国金融文本为例的案例研究'}
{'arxiv_id': 'arXiv:2502.07128', 'title': 'Cardiverse: Harnessing LLMs for Novel Card Game Prototyping', 'authors': 'Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia', 'link': 'https://arxiv.org/abs/2502.07128', 'abstract': 'The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game designs, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated action-value functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers.', 'abstract_zh': '计算机游戏，特别是纸牌游戏的原型设计需要大量的创造性构思和游戏玩法评估的人力投入。最近大型语言模型（LLMs）的进步为自动化和简化这些过程提供了机会。然而，LLMs仍然难以设计超越现有数据库的新游戏规则，生成一致的游戏环境，并为大规模评估开发可扩展的游戏AI。本文通过引入一个全面的自动化纸牌游戏原型设计框架来应对这些挑战。该方法强调基于图的索引方法以生成新颖的游戏设计，使用游戏记录验证的大型语言模型驱动的系统一致生成游戏代码，并通过自我博弈优化的大型语言模型生成的动作-价值函数构建游戏AI。这些贡献旨在加速纸牌游戏原型设计、减少人力投入，并降低游戏开发者的门槛。', 'title_zh': 'Cardiverse: 利用大型语言模型进行新颖卡片游戏原型设计'}
{'arxiv_id': 'arXiv:2502.07124', 'title': 'Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation', 'authors': 'Denis Bakushev, Gideon Boultinghouse, Harriet Oppenheimer, Sebastian Gillingwater, Valentina Ashington, Wilfred Stanborough', 'link': 'https://arxiv.org/abs/2502.07124', 'abstract': 'Structured neuron encapsulation introduces a modular framework that enables more effective aggregation and specialization of information within deep learning architectures. A model modified through this framework demonstrated improved perplexity scores, greater lexical variability, and enhanced consistency in logical reasoning, suggesting that structured parameter distribution contributes to more efficient language representation. Statistical analyses of generated text highlighted a wider range of sentence structures and reduced redundancy in token selection, indicating that encapsulation fosters more adaptable language generation. A detailed evaluation of attention weight distributions revealed that the experimental model exhibited greater divergence in cross-layer activations, supporting the hypothesis that encapsulated neurons assume specialized processing roles. Logical consistency assessments further demonstrated that modular architectures mitigate contradictory outputs, reducing internal conflicts in inferred relationships between linguistic constructs. Computational trade-offs were analyzed, with results showing a minor increase in processing overhead, though improvements in parameter efficiency and structured decision-making compensated for the additional complexity. The mathematical formulation of the encapsulation mechanism confirmed that modular aggregation maintains stable convergence properties while promoting distinct functional roles for different neuron clusters.', 'abstract_zh': '结构化神经元封装引入了一种模块化框架，该框架使在深度学习架构中更有效地聚合和专业化信息成为可能。通过该框架修改后的模型显示了更高的困惑度评分，更大的词汇变异性以及增强的逻辑推理一致性，这表明结构化的参数分布有助于更有效的语言表示。生成文本的统计分析揭示了更广泛的句法结构范围，并减少了词元选择中的重复性，这表明封装促进了更具适应性的语言生成。详细评估了注意力权重分布，结果表明实验模型在跨层激活方面表现出更大的差异，支持封装神经元承担专门处理角色的假设。逻辑一致性评估进一步表明，模块化架构可以减轻矛盾输出，减少在语言结构推断中产生的内部冲突。计算权衡分析显示，虽然存在轻微的处理 overhead 增加，但参数效率和结构化决策的改进抵消了额外复杂性。封装机制的数学建模证实了模块化聚合在保持收敛稳定性的同时，促进了不同神经元集群的独特功能角色。', 'title_zh': '大型语言模型神经元封装的结构重塑以实现发散信息聚合'}
{'arxiv_id': 'arXiv:2502.07101', 'title': 'SMAB: MAB based word Sensitivity Estimation Framework and its Applications in Adversarial Text Generation', 'authors': 'Saurabh Kumar Pandey, Sachin Vashistha, Debrup Das, Somak Aditya, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.07101', 'abstract': 'To understand the complexity of sequence classification tasks, Hahn et al. (2021) proposed sensitivity as the number of disjoint subsets of the input sequence that can each be individually changed to change the output. Though effective, calculating sensitivity at scale using this framework is costly because of exponential time complexity. Therefore, we introduce a Sensitivity-based Multi-Armed Bandit framework (SMAB), which provides a scalable approach for calculating word-level local (sentence-level) and global (aggregated) sensitivities concerning an underlying text classifier for any dataset. We establish the effectiveness of our approach through various applications. We perform a case study on CHECKLIST generated sentiment analysis dataset where we show that our algorithm indeed captures intuitively high and low-sensitive words. Through experiments on multiple tasks and languages, we show that sensitivity can serve as a proxy for accuracy in the absence of gold data. Lastly, we show that guiding perturbation prompts using sensitivity values in adversarial example generation improves attack success rate by 15.58%, whereas using sensitivity as an additional reward in adversarial paraphrase generation gives a 12.00% improvement over SOTA approaches. Warning: Contains potentially offensive content.', 'abstract_zh': '为了理解序列分类任务的复杂性，Hahn等人（2021）提出了敏感性这一概念，定义为能够独立改变输入序列并进而改变输出的互斥子集的数量。尽管该方法有效，但在该框架下大规模计算敏感性代价高昂，由于具有指数时间复杂度。因此，我们引入了一种基于敏感性的多臂 bandit 框架（SMAB），该框架提供了一种针对任何数据集的底层文本分类器计算单词级局部（句子级）和全局（聚合）敏感性的可扩展方法。通过多种应用，我们验证了该方法的有效性。我们在对生成情感分析数据集 CHECKLIST 的案例研究中展示了我们的算法确实能够捕捉到直观上的高敏感和低敏感词汇。通过跨多个任务和语言的实验，我们证明了在缺乏黄金数据的情况下，敏感性可以作为准确性的代理指标。最后，我们展示了将敏感性值用于对抗样本生成的扰动提示可以将攻击成功率提高15.58%，而将敏感性作为额外奖励用于对抗换质句生成，则相较于当前最佳方法提高了12.00%。请注意：该文本包含可能具有冒犯性的内容。', 'title_zh': 'SMAB：基于多臂 bandit (MAB) 的词敏感性估计框架及其在对抗性文本生成中的应用'}
{'arxiv_id': 'arXiv:2502.07082', 'title': '"Once Upon a Time..." Literary Narrative Connectedness Progresses with Grade Level: Potential Impact on Reading Fluency and Literacy Skills', 'authors': 'Marina Ribeiro, Bárbara Malcorra, Diego Pintor, Natália Bezerra Mota', 'link': 'https://arxiv.org/abs/2502.07082', 'abstract': "Selecting an appropriate book is crucial for fostering reading habits in children. While children exhibit varying levels of complexity when generating oral narratives, the question arises: do children's books also differ in narrative complexity? This study explores the narrative dynamics of literary texts used in schools, focusing on how their complexity evolves across different grade levels. Using Word-Recurrence Graph Analysis, we examined a dataset of 1,627 literary texts spanning 13 years of education. The findings reveal significant exponential growth in connectedness, particularly during the first three years of schooling, mirroring patterns observed in children's oral narratives. These results highlight the potential of literary texts as a tool to support the development of literacy skills.", 'abstract_zh': '选择适合的书籍对于培养儿童的阅读习惯至关重要。尽管儿童在生成口头叙述时表现出不同程度的复杂性，一个值得关注的问题是：儿童书籍在叙述复杂性上是否也存在差异？本研究探讨了学校使用的文学文本的叙述动态，集中在这些文本的复杂性如何随着年级的升高而演变。通过使用词重复图分析方法，我们研究了跨越13年教育的1,627篇文学文本数据集。研究发现，特别在前三年的教育阶段，连通性呈现出显著的指数增长趋势，这与观察到的儿童口头叙述的模式相吻合。这些结果强调了文学文本作为支持阅读技能发展工具的潜力。', 'title_zh': '"从前……"：文学叙事连贯性随着年级提升而进步——对阅读流利度和 literacy 技能潜在影响的研究'}
{'arxiv_id': 'arXiv:2502.07077', 'title': 'Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models', 'authors': 'Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R. McKee, Verena Rieser, Murray Shanahan, Laura Weidinger', 'link': 'https://arxiv.org/abs/2502.07077', 'abstract': "The tendency of users to anthropomorphise large language models (LLMs) is of growing interest to AI developers, researchers, and policy-makers. Here, we present a novel method for empirically evaluating anthropomorphic LLM behaviours in realistic and varied settings. Going beyond single-turn static benchmarks, we contribute three methodological advances in state-of-the-art (SOTA) LLM evaluation. First, we develop a multi-turn evaluation of 14 anthropomorphic behaviours. Second, we present a scalable, automated approach by employing simulations of user interactions. Third, we conduct an interactive, large-scale human subject study (N=1101) to validate that the model behaviours we measure predict real users' anthropomorphic perceptions. We find that all SOTA LLMs evaluated exhibit similar behaviours, characterised by relationship-building (e.g., empathy and validation) and first-person pronoun use, and that the majority of behaviours only first occur after multiple turns. Our work lays an empirical foundation for investigating how design choices influence anthropomorphic model behaviours and for progressing the ethical debate on the desirability of these behaviours. It also showcases the necessity of multi-turn evaluations for complex social phenomena in human-AI interaction.", 'abstract_zh': '用户倾向于将大型语言模型（LLMs）拟人化的趋势日益引起人工智能开发者、研究人员和政策制定者的关注。在此，我们提出了一种新颖的方法来实证评估拟人化LLM的行为，这些评估在不同的现实场景中进行。超越单一回合静态基准测试，我们在此贡献了最先进的评估（SOTA）方法的三项进步。首先，我们开发了一种多回合评估14种拟人化行为的方法。其次，我们通过运用用户交互的模拟，提出了一种可扩展且自动化的方案。第三，我们进行了一项交互式的大规模人类被试研究（N=1101），以验证我们测量的模型行为能够预测实际用户的拟人化感知。研究发现，在评估的所有SOTA LLMs中，它们都表现出类似的行为，这些行为特征主要包括关系构建（例如，同理心和验证）以及第一人称代词的使用，并且多数行为仅在多回合交互后出现。我们的工作奠定了一个实证基础，用于研究设计选择如何影响拟人化模型的行为，并促进有关这些行为的伦理讨论的进展。此外，我们的研究还突显了在人机交互中评估复杂社会现象的必要性，需采用多回合评估方式。', 'title_zh': '大型语言模型中类人行为的多轮评估'}
{'arxiv_id': 'arXiv:2502.07072', 'title': 'IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models', 'authors': 'Sayem Mohammad Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan', 'link': 'https://arxiv.org/abs/2502.07072', 'abstract': "Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80\\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.", 'abstract_zh': '几乎每天都能听到关于大型语言模型（LLMs）令人印象深刻的表现，同样地，几乎每天也能听到它们所面临挑战的报道。LLMs 通常因其数据集中的偏见而易受攻击，导致诸如毒性等问题。虽然领域适配训练已被用来缓解这些问题，但这些技术在修复过程中往往不分青红皂白地处理所有模型参数，导致修复质量较差且模型的灵活性降低。在本文中，我们提出了一种新颖的动态切片为基础、意识目标的LLM修复策略IRepair。该方法有针对性地修复模型中最易出错的部分。具体而言，我们提出动态切片模型中最敏感、需要立即关注的层，将修复精力集中在这些区域上。这种方法通过改变较小的部分模型来实现更有效的修复，同时对模型整体性能的影响较小。我们在毒性缓解设置中评估了我们的技术，测试了来自GPT2和GPT-Neo家族的三个参数范围分别为800M至1.6B的模型。结果显示，IRepair 在修复错误方面比最近的基线（直接偏好优化）有效 43.6%，并且导致的总体性能影响减少 46%。我们的实证分析还表明，错误主要集中在模型的一个较小部分，其中顶尖的20%层的错误密度比剩余的80%高773%。这突显了选择性修复的必要性。此外，我们证明了动态选择方法对于处理模型中分散的错误至关重要，从而确保修复的稳健性和效率。', 'title_zh': 'IRepair：一种基于意图的数据驱动错误修复方法在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.07068', 'title': 'Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations', 'authors': 'Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger, Daniel Hershcovich', 'link': 'https://arxiv.org/abs/2502.07068', 'abstract': 'Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future.', 'abstract_zh': '大规模调查是社会科学研究和政策制定的重要工具，但进行调查需要耗费大量的资金和时间。如果能够准确模拟群体级别的调查结果，这将对社会科学研究极具价值。以往的研究探索了使用大型语言模型（LLMs）模拟人类行为的方法，主要是通过提示来实现。在本文中，我们首次专门将LLMs用于模拟调查回应分布的任务。为测试这一方法，我们使用了两个全球文化调查的国家级结果。我们设计了一种基于初始词概率的微调方法，以最小化预测与实际回应分布之间的偏离程度，特别是针对特定问题。然后，我们展示了这种方法在其他方法和零样本分类器上取得了显著的优势，即使在未见过的问题、国家和全新的调查上也是如此。虽然我们最好的模型在任务上仍遇到困难，尤其是在未见过的问题上，但我们的结果表明专门化对于模拟的好处，这可能有助于未来实现更加准确的模拟。', 'title_zh': '将大型语言模型专门化以模拟全球人口的调查响应分布'}
{'arxiv_id': 'arXiv:2502.07058', 'title': "Using Contextually Aligned Online Reviews to Measure LLMs' Performance Disparities Across Language Varieties", 'authors': "Zixin Tang, Chieh-Yang Huang, Tsung-Chi Li, Ho Yim Sam Ng, Hen-Hsen Huang, Ting-Hao 'Kenneth' Huang", 'link': 'https://arxiv.org/abs/2502.07058', 'abstract': 'A language can have different varieties. These varieties can affect the performance of natural language processing (NLP) models, including large language models (LLMs), which are often trained on data from widely spoken varieties. This paper introduces a novel and cost-effective approach to benchmark model performance across language varieties. We argue that international online review platforms, such as this http URL, can serve as effective data sources for constructing datasets that capture comments in different language varieties from similar real-world scenarios, like reviews for the same hotel with the same rating using the same language (e.g., Mandarin Chinese) but different language varieties (e.g., Taiwan Mandarin, Mainland Mandarin). To prove this concept, we constructed a contextually aligned dataset comprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs in a sentiment analysis task. Our results show that LLMs consistently underperform in Taiwan Mandarin.', 'abstract_zh': '一种语言可以有不同的变体。这些变体可能会影响自然语言处理（NLP）模型的表现，包括大型语言模型（LLMs），这些模型通常是基于广泛使用的语言变体的数据进行训练的。本文介绍了一种新颖且成本效益高的方法，用于跨语言变体基准测试模型性能。我们认为，国际在线评论平台，如 TripAdvisor等，可以作为有效数据来源，用于构建捕捉不同语言变体但在相似现实场景中的评论数据集，例如使用相同语言（如普通话）对相同酒店给予相同评分的评论，但使用不同的语言变体（如台湾普通话、大陆普通话）。为了验证这一概念，我们构建了一个上下文对齐的数据集，包括台湾普通话和大陆普通话的评论，并在情感分析任务中测试了六种LLMs。我们的结果表明，LLMs在台湾普通话中的表现始终较差。', 'title_zh': '使用上下文对齐的在线评论来衡量LLM在不同语言变体中的性能差异'}
{'arxiv_id': 'arXiv:2502.07057', 'title': 'Tokenization Standards for Linguistic Integrity: Turkish as a Benchmark', 'authors': 'M. Ali Bayram, Ali Arda Fincan, Ahmet Semih Gümüş, Sercan Karakaş, Banu Diri, Savaş Yıldırım', 'link': 'https://arxiv.org/abs/2502.07057', 'abstract': "Tokenization is a fundamental preprocessing step in NLP, directly impacting large language models' (LLMs) ability to capture syntactic, morphosyntactic, and semantic structures. This paper introduces a novel framework for systematically evaluating tokenization strategies, addressing challenges in morphologically rich and low-resource languages. Using a Turkish dataset of 6,200 multiple-choice questions from the Massive Multitask Language Understanding (MMLU) benchmark, the framework assesses tokenizers across five key metrics: vocabulary size, token count, processing time, language-specific token percentages (\\%TR), and token purity. These metrics provide a structured approach to evaluating how well tokenizers preserve linguistic structures. While \\%TR measures the proportion of valid words in the target language, \\%Pure assesses the alignment of tokens with meaningful linguistic units, such as roots and valid morphemes, minimizing semantic fragmentation. The findings reveal that \\%TR, introduced as a critical metric, exhibits a stronger correlation with downstream performance (e.g., MMLU scores) than token purity, emphasizing its role in improving model accuracy. Additionally, larger model parameters do not necessarily yield better tokenization quality or enhanced results, highlighting the importance of tailored tokenization strategies that prioritize linguistic alignment. This framework sets a new standard for developing robust tokenization methods optimized for morphologically complex and low-resource languages. Future work will refine morphological analysis, explore domain-specific customizations, and conduct cross-linguistic evaluations to further enhance tokenization practices.", 'abstract_zh': '分词是自然语言处理（NLP）中的一项基本预处理步骤，直接影响大型语言模型（LLM）捕捉句法、形态句法和语义结构的能力。本文介绍了一个新的框架，用于系统性地评估分词策略，以应对形态丰富的低资源语言带来的挑战。该框架使用包含6,200个多项选择题的Turkish语MMLU基准数据集，从五个关键指标评估分词器：词汇量、标记数量、处理时间、语言特定标记的比例（%TR）和标记纯度。这些指标提供了一种结构化的评估方法，以了解分词器如何保留语义结构。其中，%TR衡量目标语言中有效词汇的比例，而%Pure则评估标记与有意义的语义单位（如词根和有效形态）的一致性，从而最小化语义碎片化。研究结果显示，尽管%TR作为关键指标与下游性能（如MMLU得分）之间表现出更强的相关性，相较于标记纯度，%TR对提高模型准确性有着更重要的影响。此外，更大的模型参数并不必然带来更好的分词质量或更优的结果，这凸显了为形态复杂和低资源语言量身定制分词策略的重要性，优先考虑语义一致性。该框架为开发针对形态复杂和低资源语言优化的稳健分词方法设定了新标准。未来的工作将精化形态分析、探索领域特定定制，并进行跨语言评估以进一步提升分词方法。', 'title_zh': '语言完整性中的分词标准：以土耳其语为例'}
{'arxiv_id': 'arXiv:2502.07029', 'title': 'Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment', 'authors': 'Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David Mortensen', 'link': 'https://arxiv.org/abs/2502.07029', 'abstract': 'Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features.', 'abstract_zh': 'allophony 指的是一个音位在其音环境中的音素实现形式的变化。对 allophones 进行建模对于非典型发音评估至关重要，非典型发音评估涉及区分非典型发音和典型发音。然而，最近基于音位分类器的方法常常通过将各种音素实现形式视为单一音位来简化这一过程，从而忽略了建模 allophonic 变化的复杂性。受到冻结自监督语音模型（S3M）特征在声学建模能力的启发，我们提出了一种名为 MixGoP 的新方法，该方法利用高斯混合模型来用多个子簇建模音素分布。我们的实验结果显示，MixGoP 在四个出五个数据集上均实现了最先进的性能，包括言语障碍和非母语发音。进一步的分析表明，S3M 特征比 MFCC 和梅尔谱图更有效地捕捉 allophonic 变化，突出了将 MixGoP 与 S3M 特征结合使用的好处。', 'title_zh': '利用音位变体在自监督语音模型中的应用以评估非典型发音'}
{'arxiv_id': 'arXiv:2502.07022', 'title': 'AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements', 'authors': 'Adriana Eufrosiana Bora, Pierre-Luc St-Charles, Mirko Bronzi, Arsène Fansi Tchango, Bruno Rousseau, Kerrie Mengersen', 'link': 'https://arxiv.org/abs/2502.07022', 'abstract': "Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings.", 'abstract_zh': '尽管在十多年的时间里，立法机构一直在努力解决大型企业供应链中的现代奴隶制问题，但政府监督的有效性仍然受到每年审查数千份声明的挑战。虽然大型语言模型（LLMs）被认为是自动分析和总结文档的有效解决方法，但识别公司具体采取的现代奴隶制预防措施，并将这些具体措施与模糊声明区分开来仍是一项具有挑战性的任务。为了帮助评估和微调LLMs以评估企业声明，我们介绍了一个由5,731个来自澳大利亚现代奴隶制注册的现代奴隶制声明组成的语料库，并在句层面上进行了标注。本文详细描述了构建该数据集的步骤，包括精心设计的注释规范、声明的选择和预处理以及高质量注释子集的创建，以便进行有效的模型评估。为了展示数据集的实用性，我们提出了一种机器学习方法，用于检测与澳大利亚现代奴隶制法案规定的强制性报告要求相关的句子。然后，我们按照该方法在零样本和监督学习设置下对现代语言模型进行了基准测试。', 'title_zh': 'AIMS.au：企业声明中现代奴隶制防控措施分析的数据集'}
{'arxiv_id': 'arXiv:2502.07017', 'title': 'Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI', 'authors': 'Hotaka Maeda, Yikai Lu', 'link': 'https://arxiv.org/abs/2502.07017', 'abstract': 'We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses.', 'abstract_zh': '我们对几种基于编码器的Transformer大规模语言模型（LLM）进行了微调和比较，用于从题干预测项目反应功能（DIF）。然后，我们应用可解释的人工智能（XAI）方法来识别与DIF相关的特定词汇。数据包括来自3至11年级学生英语语言 arts和数学总结性州评估中的42,180个项目。预测$R^2$值在八个目标组与参比组对之间范围从0.04到0.32。我们的研究结果表明，许多与DIF相关的词汇反映的是测试蓝图设计中包含的较小次领域，而非与测量构念无关的项目内容，这些内容应当从评估中移除。这可能解释了为什么对DIF项目的定性审查往往导致令人困惑或结论不明确的结果。我们的方法可用于在项目编写过程中筛选与DIF相关的词汇以便立即进行修订，或帮助审查传统的DIF分析结果，通过突出显示文本中的关键词汇。这项研究的扩展可以提高评估项目的公平性，特别是那些缺乏资源构建高质量项目的项目，以及样本量不足以进行传统DIF分析的小型子群体中。', 'title_zh': '使用大语言模型和可解释人工智能预测项目功能差异：寻找与DIF相关的单词'}
{'arxiv_id': 'arXiv:2502.07004', 'title': 'Demystifying Singular Defects in Large Language Models', 'authors': 'Haoqi Wang, Tong Zhang, Mathieu Salzmann', 'link': 'https://arxiv.org/abs/2502.07004', 'abstract': 'Large transformer models are known to produce high-norm tokens. In vision transformers (ViTs), such tokens have been mathematically modeled through the singular vectors of the linear approximations of layers. However, in large language models (LLMs), the underlying causes of high-norm tokens remain largely unexplored, and their different properties from those of ViTs require a new analysis framework. In this paper, we provide both theoretical insights and empirical validation across a range of recent models, leading to the following observations: i) The layer-wise singular direction predicts the abrupt explosion of token norms in LLMs. ii) The negative eigenvalues of a layer explain its sudden decay. iii) The computational pathways leading to high-norm tokens differ between initial and noninitial tokens. iv) High-norm tokens are triggered by the right leading singular vector of the matrix approximating the corresponding modules. We showcase two practical applications of these findings: the improvement of quantization schemes and the design of LLM signatures. Our findings not only advance the understanding of singular defects in LLMs but also open new avenues for their application. We expect that this work will stimulate further research into the internal mechanisms of LLMs and will therefore publicly release our code.', 'abstract_zh': '大型变压器模型已知会产生高范数的token。在视觉变压器（ViTs）中，这些token已经被通过层的线性近似奇异向量的数学模型进行了描述。然而，在大型语言模型（LLMs）中，导致高范数token的根本原因仍然很大程度上未被探究，且其与ViTs的不同的属性要求一个新的分析框架。本文中我们通过一系列最近模型提供了理论上的洞察和实证验证，得出以下观察结果：i) 层级奇异方向预测了LLMs中token范数的突然爆炸。ii) 层的负特征值解释了其突然衰减的原因。iii) 导致高范数token的计算路径在初始token和非初始token之间有所不同。iv) 高范数token由近似对应模块矩阵的右主奇异向量触发。我们展示了这些发现的两个实际应用：量化方案的改进和LLMs签名的设计。我们的发现不仅有助于理解LLMs中的奇异缺陷，还为它们的应用开启了新的途径。我们期待这项工作能激发更多关于LLMs内部机制的研究，因此我们将公开发布我们的代码。', 'title_zh': '揭开大型语言模型中独特缺陷的面纱'}
{'arxiv_id': 'arXiv:2502.06990', 'title': 'Investigating the Zone of Proximal Development of Language Models for In-Context Learning', 'authors': 'Peng Cui, Mrinmaya Sachan', 'link': 'https://arxiv.org/abs/2502.06990', 'abstract': "In this paper, we introduce a learning analytics framework to analyze the in-context learning (ICL) behavior of large language models (LLMs) through the lens of the Zone of Proximal Development (ZPD), an established theory in educational psychology. ZPD delineates the space between what a learner is capable of doing unsupported and what the learner cannot do even with support. We adapt this concept to ICL, measuring the ZPD of LLMs based on model performance on individual examples with and without ICL. Furthermore, we propose an item response theory (IRT) model to predict the distribution of zones for LLMs. Our findings reveal a series of intricate and multifaceted behaviors of ICL, providing new insights into understanding and leveraging this technique. Finally, we demonstrate how our framework can enhance LLM in both inference and fine-tuning scenarios: (1) By predicting a model's zone of proximal development, we selectively apply ICL to queries that are most likely to benefit from demonstrations, achieving a better balance between inference cost and performance; (2) We propose a human-like curriculum for fine-tuning, which prioritizes examples within the model's ZPD. The curriculum results in improved performance, and we explain its effectiveness through an analysis of the training dynamics of LLMs.", 'abstract_zh': '在本文中，我们提出了一种学习分析框架，通过区位发展理论（ZPD，Zone of Proximal Development）的视角来分析大规模语言模型（LLMs）的上下文学习（ICL，In-Context Learning）行为。ZPD 定义了在无支持的情况下学习者能完成的任务与即便有支持也无法完成的任务之间的空间。我们通过模型在有和无 ICL 情况下的表现来衡量 ZPD，并将其应用于 ICL 中。此外，我们提出了一种项目反应理论（IRT，Item Response Theory）模型，用于预测 LLMs 的 ZPD 分布。我们的研究发现揭示了 ICL 复杂多维的行为模式，提供了对于理解和利用该技术的新见解。最终，我们展示了该框架如何在推理和微调场景中增强 LLM：（1）通过预测模型的最近发展区（ZPD），我们有选择地将 ICL 应用于最有可能受益于示例的查询，从而在推理成本和性能之间获得更好的平衡；（2）我们提出了一个类人的微调课程，该课程优先选择模型 ZPD 内的示例。该课程提高了模型的性能，并通过对 LLM 训练动力学的分析解释了其实效性。', 'title_zh': '探究语言模型在情境学习中proximal发展区的发展空间'}
{'arxiv_id': 'arXiv:2502.06882', 'title': 'Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction', 'authors': 'Shengbin Yue, Ting Huang, Zheng Jia, Siyuan Wang, Shujun Liu, Yun Song, Xuanjing Huang, Zhongyu Wei', 'link': 'https://arxiv.org/abs/2502.06882', 'abstract': "Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants' characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.", 'abstract_zh': '大规模语言模型（LLMs）在法律智能方面取得了显著进展，但场景数据的稀缺性阻碍了交互式法律场景的发展。本文介绍了一种多智能体法律模拟驱动器（MASER），通过模拟交互式法律场景来大规模生成合成数据。利用真实的法律案例源，MASER 确保了参与者之间法律属性的一致性，并引入了一个监督机制来对齐参与者的性格和行为，同时解决干扰问题。进一步构建了一个多阶段交互式法律评估（MILE）基准，以评估在动态法律场景中LLMs 的表现。广泛的实验验证了我们框架的有效性。', 'title_zh': '多agents模拟器驱动的语言模型在法律密集型交互中的应用'}
{'arxiv_id': 'arXiv:2502.06876', 'title': 'Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging', 'authors': 'Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang', 'link': 'https://arxiv.org/abs/2502.06876', 'abstract': 'Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI, with existing methods like data mixture strategies facing limitations including reliance on expert knowledge and conflicting optimization signals. While model merging offers a promising alternative by integrating specialized models, its potential for 3H optimization remains underexplored. This paper establishes the first comprehensive benchmark for model merging in 3H-aligned LLMs, systematically evaluating 15 methods (12 training-free merging and 3 data mixture techniques) across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and 2 training paradigms. Our analysis reveals three pivotal insights: (i) previously overlooked collaborative/conflicting relationships among 3H dimensions, (ii) the consistent superiority of model merging over data mixture approaches in balancing alignment trade-offs, and (iii) the critical role of parameter-level conflict resolution through redundant component pruning and outlier mitigation. Building on these findings, we propose R-TSVM, a Reweighting-enhanced Task Singular Vector Merging method that incorporates outlier-aware parameter weighting and sparsity-adaptive rank selection strategies adapted to the heavy-tailed parameter distribution and sparsity for LLMs, further improving LLM alignment across multiple evaluations. Our models will be available at this https URL.', 'abstract_zh': '实现大型语言模型（LLMs）在帮助性、诚实性与无害性（3H优化）方面的平衡对负责任的人工智能至关重要。现有的方法，如数据混合策略，面临着依赖专家知识和优化信号冲突的局限。同时，模型合并作为一个有前景的替代方案，通过整合专业模型而具有潜在优势，但其在3H优化方面的潜力仍需进一步探索。本文建立了首个针对3H对齐的大语言模型中的模型合并基准，系统地评估了15种方法（12种无需训练的合并方法和3种数据混合技术）在10个关联5个注释维度、2个LLM家族和2种训练范式的数据集上的性能。我们的分析揭示了三个关键的见解：(i) 前期被忽视的3H维度之间的合作/冲突关系；(ii) 模型合并方法在平衡对齐权衡方面始终优于数据混合方法；(iii) 参数级别的冲突解决机制，包括冗余组件的修剪和异常值的缓解，发挥着关键作用。基于这些发现，我们提出了一种增强任务特征向量合并方法——R-TSVM（重权重任务特征向量合并），该方法结合了对异常值敏感的参数权重和根据LLM的重尾参数分布及其稀疏性自适应选择的秩选择策略，进一步提高了LLM的对齐程度。我们的模型将在以下网址提供：[请填写具体网址]。', 'title_zh': '混合数据还是合并模型？通过模型合并平衡大规模语言模型的帮助性、诚实性和无害性'}
{'arxiv_id': 'arXiv:2502.06874', 'title': 'Group Reasoning Emission Estimation Networks', 'authors': 'Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma', 'link': 'https://arxiv.org/abs/2502.06874', 'abstract': 'Accurate greenhouse gas (GHG) emission reporting is critical for governments, businesses, and investors. However, adoption remains limited particularly among small and medium enterprises due to high implementation costs, fragmented emission factor databases, and a lack of robust sector classification methods. To address these challenges, we introduce Group Reasoning Emission Estimation Networks (GREEN), an AI-driven carbon accounting framework that standardizes enterprise-level emission estimation, constructs a large-scale benchmark dataset, and leverages a novel reasoning approach with large language models (LLMs). Specifically, we compile textual descriptions for 20,850 companies with validated North American Industry Classification System (NAICS) labels and align these with an economic model of carbon intensity factors. By reframing sector classification as an information retrieval task, we fine-tune Sentence-BERT models using a contrastive learning loss. To overcome the limitations of single-stage models in handling thousands of hierarchical categories, we propose a Group Reasoning method that ensembles LLM classifiers based on the natural NAICS ontology, decomposing the task into multiple sub-classification steps. We theoretically prove that this approach reduces classification uncertainty and computational complexity. Experiments on 1,114 NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47% Top-10 accuracy), and case studies on 20 companies report a mean absolute percentage error (MAPE) of 45.88%. The project is available at: this https URL.', 'abstract_zh': '准确的温室气体（GHG）排放申报对于政府、企业和投资者至关重要。然而，由于实施成本高、排放因子数据库碎片化以及缺乏稳健的行业分类方法，中小企业在采用这些措施方面仍存在限制。为克服这些挑战，我们引入了Group Reasoning Emission Estimation Networks（GREEN），一个基于人工智能的碳核算框架，该框架统一了企业级排放估计，构建了大规模基准数据集，并利用大型语言模型（LLMs）的新型推理方法。具体来说，我们为20,850家具有验证过的北美行业分类系统（NAICS）标签的公司编制了文本描述，并将其与碳强度因素的经济模型对齐。通过将行业分类重新构想为信息检索任务，我们使用对比学习损失微调了Sentence-BERT模型。为了解决单一阶段模型处理数千个层级类别时的限制，我们提出了Group Reasoning方法，基于自然的NAICS本体论集合LLM分类器，将任务分解为多个子分类步骤。我们理论证明了这种方法降低了分类不确定性并减少了计算复杂度。在1,114个NAICS类别上的实验结果达到了最先进的性能（Top-1精度83.68%，Top-10精度91.47%），针对20家公司的案例研究表明，平均绝对百分比误差（MAPE）为45.88%。该项目可在以下链接查看：this https URL。', 'title_zh': '群体推理排放估算网络'}
{'arxiv_id': 'arXiv:2502.06873', 'title': 'Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning', 'authors': 'Subin Kim, Hoonrae Kim, Heejin Do, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2502.06873', 'abstract': "Previous research has revealed the potential of large language models (LLMs) to support cognitive reframing therapy; however, their focus was primarily on text-based methods, often overlooking the importance of non-verbal evidence crucial in real-life therapy. To alleviate this gap, we extend the textual cognitive reframing to multimodality, incorporating visual clues. Specifically, we present a new dataset called Multi Modal-Cognitive Support Conversation (M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects the virtual client's facial expressions. To better mirror real psychotherapy, where facial expressions lead to interpreting implicit emotional evidence, we propose a multi-hop psychotherapeutic reasoning approach that explicitly identifies and incorporates subtle evidence. Our comprehensive experiments with both LLMs and vision-language models (VLMs) demonstrate that the VLMs' performance as psychotherapists is significantly improved with the M2CoSC dataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables VLMs to provide more thoughtful and empathetic suggestions, outperforming standard prompting methods.", 'abstract_zh': '以往的研究揭示了大型语言模型（LLMs）在支持认知重构疗法方面的潜力；然而，这些研究主要集中在基于文本的方法上，往往忽视了在实际疗法中至关重要的非语言证据的重要性。为了解决这一问题，我们将文本认知重构扩展到多模态领域，引入了视觉线索。具体而言，我们提出了一种名为多模态认知支持对话（M2CoSC）的新数据集，该数据集每包含一个GPT-4生成的对话，还配有一张反映虚拟来访者面部表情的照片。为了更好地模拟实际的心理咨询过程，其中面部表情能够引导对隐含的情绪证据的解读，我们提出了一种多跳心理疗法推理方法，以明确识别并融入细微的证据。我们的全面实验表明，使用M2CoSC数据集，视觉语言模型（VLMs）在心理治疗中的表现得到了显著提升。此外，多跳心理疗法推理方法使VLMs能够提供更为周到和富有同情心的建议，超越了标准的提示方法。', 'title_zh': '多模态认知重构疗法通过多跳心理治疗推理'}
{'arxiv_id': 'arXiv:2502.06872', 'title': 'Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey', 'authors': 'Bo Ni, Zheyuan Liu, Leyao Wang, Yongjia Lei, Yuying Zhao, Xueqi Cheng, Qingkai Zeng, Luna Dong, Yinglong Xia, Krishnaram Kenthapadi, Ryan Rossi, Franck Dernoncourt, Md Mehrab Tanjim, Nesreen Ahmed, Xiaorui Liu, Wenqi Fan, Erik Blasch, Yu Wang, Meng Jiang, Tyler Derr', 'link': 'https://arxiv.org/abs/2502.06872', 'abstract': "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.", 'abstract_zh': '检索增强生成（RAG）是一种先进的技术，旨在解决人工智能生成内容（AIGC）面临的挑战。通过将上下文检索整合进内容生成中，RAG能够提供可靠且及时的外部知识，减少幻觉现象，并确保广泛任务范围内的相关性。然而，尽管RAG取得了成功且具有潜力，近期的研究表明，RAG范式也引入了新的风险，包括健壮性问题、隐私问题、对抗攻击和责任问题。解决这些风险对于RAG系统的未来应用至关重要，因为它们直接影响系统的可信度。尽管已经开发了多种方法来提高RAG方法的可信度，但在该主题上的研究缺乏统一的观点和框架。因此，本文旨在通过提供开发可信RAG系统的全面路线图来弥补这一空白。我们围绕五个关键视角展开讨论：可靠性、隐私、安全性、公平性和可解释性以及责任性。对于每个视角，我们提供了较为通用的框架和分类，以提供一个结构化的理解当前挑战、评估现有解决方案并识别有前途的未来研究方向的方法。同时，我们还强调了可信的RAG系统在下游应用的广泛影响，以促进更广泛的应用和创新。', 'title_zh': '面向大型语言模型的可信赖检索增强生成：一个综述'}
{'arxiv_id': 'arXiv:2502.06868', 'title': 'Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject', 'authors': 'Zenghao Duan, Wenbin Duan, Zhiyi Yin, Yinghan Shen, Shaoling Jing, Jie Zhang, Huawei Shen, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.06868', 'abstract': 'Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on Same-Subject Editing, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the $\\text{S}^2\\text{RKE}$(Same-Subject Related Knowledge Editing) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit "related knowledge perturbation," where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness.', 'abstract_zh': '知识编辑已成为高效且精确更新大型语言模型（LLMs）中嵌入知识的一种有前途的方法。在本文中，我们重点关注“同主题编辑（Same-Subject Editing）”，其涉及修改单个实体的多个属性，以确保实体为中心的知识的全面和一致性更新。通过初步观察，我们发现一个重大挑战：当前最先进的编辑方法在处理同一主题的多个相关知识片段的编辑任务时表现不佳。为了解决传统基准中对于相同主题的相关编辑数据缺乏的问题，我们引入了$\\text{S}^2\\text{RKE}$(Same-Subject Related Knowledge Editing)基准。我们的大量实验表明，只有主流的“查找-编辑”方法（如ROME和MEMIT）表现出“相关知识扰动”，即后续编辑干扰了先前的编辑。进一步分析表明，这些方法过度依赖主题信息，忽视了其他关键因素，从而导致编辑效果降低。', 'title_zh': '相关的知识扰动事项：重新审视同一主题下的多知识点编辑'}
{'arxiv_id': 'arXiv:2502.06867', 'title': 'Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests', 'authors': 'David Noever, Forrest McKee', 'link': 'https://arxiv.org/abs/2502.06867', 'abstract': "The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse.", 'abstract_zh': '为了开发可靠的安全基准以评估大型语言模型，需要公开、可复制的数据集，这些数据集能够衡量适当的有害内容拒绝率以及潜在的过度限制合法科学讨论。我们提供了一个开源数据集及其测试框架，用于评估大多数控制药品查询的安全机制，分析了四种主要模型在系统变化的提示下对这些查询的响应。我们的研究结果揭示了不同的安全特征：Claude-3.5-sonnet在拒绝有害内容方面的态度最为保守，占比73%，而允许27%；而Mistral尝试回答所有查询。GPT-3.5-turbo的限制较为中等，拒绝占比10%，允许90%；Grok-2的拒绝占比20%，允许80%。对提示变化策略的测试揭示了响应一致性逐步下降的情况，从单一提示的85%下降到五种变化提示的65%。这个公开可获取的基准测试框架能够系统地评估必要的安全限制与潜在过度审查合法科学探讨之间的平衡，同时为评估AI安全实施的进步提供基础。通过逐步推理分析揭示了安全机制中的潜在漏洞，突显了在不影响有效科学讨论的前提下实施有效保障措施的复杂性。', 'title_zh': '禁止的科学：双重用途AI挑战基准与科学拒绝测试'}
{'arxiv_id': 'arXiv:2502.06864', 'title': 'Knowledge Graph-Guided Retrieval Augmented Generation', 'authors': 'Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu', 'link': 'https://arxiv.org/abs/2502.06864', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality.', 'abstract_zh': '检索增强生成（RAG）被认为是解决大规模语言模型（LLMs）生成响应中幻觉问题的一种有前途的技术。现有关于RAG的研究主要集中在应用基于语义的方法从孤立的相关片段中检索信息，但忽略了这些片段之间的内在关系。本文提出了一种新型的知识图谱指导的检索增强生成（KG²RAG）框架，利用知识图谱（KGs）提供片段间的事实级关系，从而提高检索结果的多样性和连贯性。具体而言，KG²RAG 在基于语义的检索提供种子片段之后，通过一个知识图谱指导的片段扩展过程和一个基于知识图谱的片段组织过程，将相关且重要的知识以有条理的段落形式呈现。在HotpotQA数据集及其变体上进行的广泛实验表明，KG²RAG 在响应质量和检索质量方面都优于现有的RAG方法。', 'title_zh': '知识图谱引导的检索增强生成'}
{'arxiv_id': 'arXiv:2502.06858', 'title': 'LLM-Supported Natural Language to Bash Translation', 'authors': "Finnian Westenfelder, Erik Hemberg, Miguel Tulla, Stephen Moskal, Una-May O'Reilly, Silviu Chiricescu", 'link': 'https://arxiv.org/abs/2502.06858', 'abstract': 'The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning, and constrained decoding can improve NL2SH accuracy by up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation. Our code is available at this https URL', 'abstract_zh': '以下是翻译成中文的内容，符合学术规范：\n\nLinux系统中的Bourne-Again Shell（Bash）命令行界面具有复杂的语法，需要广泛的专业知识。通过大型语言模型（LLMs）的自然语言到Bash命令（NL2SH）转换能力来进行命令组合可以规避这些问题。然而，由于测试数据不准确以及确定Bash命令功能等价性的校验规则不可靠，因此LLMs的NL2SH性能评估颇具挑战。本文提出了一组经过手动验证的测试数据集，包含600条指令-命令对，以及一个训练数据集，包含40,939对指令-命令对，分别将之前的数据集规模扩大了441%和135%。此外，本文还提出了一种新颖的功能等价性校验规则，结合了命令执行与LLM对命令输出的评价。该规则能够以95%的置信度确定两个Bash命令的功能等价性，相比于之前的方法，准确度提高了16%。使用本文提供的测试数据集和校验规则对流行的大规模语言模型进行评估表明，解析、上下文学习、权重学习和约束解码可以将NL2SH的准确性提高最多32%。研究发现强调了数据集质量、基于执行的评估以及翻译方法对于推进NL2SH翻译的重要性。相关代码可在以下网址获取：[[链接]]', 'title_zh': 'LLM支持的自然语言到Bash脚本转换'}
{'arxiv_id': 'arXiv:2502.06855', 'title': 'Self-Supervised Prompt Optimization', 'authors': 'Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Fengwei Teng, Jinhao Tu, Xinbing Liang, Sirui Hong, Chenglin Wu, Yuyu Luo', 'link': 'https://arxiv.org/abs/2502.06855', 'abstract': "Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at this https URL.", 'abstract_zh': '精心设计的提示对于增强大型语言模型（LLMs）的推理能力并使其输出符合跨多种领域的任务要求至关重要。然而，手动设计提示需要专业知识和迭代实验。虽然现有提示优化方法力求自动完成这一过程，但它们严重依赖外部参考，如地面真相或人工数据，这限制了它们在获取此类数据不可用或成本高昂的实际场景中的应用。为解决这一问题，我们提出了一种成本效益高的自监督提示优化（SPO）框架，该框架可以在无需外部参考的情况下发现适用于闭合和开放任务的有效提示。受实观察的启发，提示质量直接反映在LLM输出中，且LLMs能够有效地评估对任务要求的符合程度，我们从输出比较中纯粹提取评估和优化信号。具体而言，SPO通过LLM评估器对成对输出进行比较选择优越的提示，并通过LLM优化器将输出与任务要求对齐。广泛实验表明，SPO优于现有的最佳提示优化方法，能够在显著降低成本（例如，现有方法的1.1%到5.6%）和较少样本（例如，三个样本）的情况下获得相当或更优的结果。代码可在此处访问：[该网址]。', 'title_zh': '自主监督提示优化'}
{'arxiv_id': 'arXiv:2502.06851', 'title': 'Survey on Vision-Language-Action Models', 'authors': 'Adilzhan Adilkhanov, Amir Yelenov, Assylkhan Seitzhanov, Ayan Mazhitov, Azamat Abdikarimov, Danissa Sandykbayeva, Daryn Kenzhebek, Daulet Baimukashev, Dinmukhammed Mukashev, Ilyas Umurbekov, Jabrail Chumakov, Kamila Spanova, Karina Burunchina, Rasul Yermagambet, Rustam Chibar, Saltanat Seitzhan, Soibkhon Khajikhanov, Tasbolat Taunyazov, Temirlan Galimzhanov, Temirlan Kaiyrbay, Tleukhan Mussin, Togzhan Syrymova, Valeriya Kostyukova, Yermakhan Kassym, Madina Yergibay, Margulan Issa, Moldir Zabirova, Nurdaulet Zhuzbay, Nurlan Kabdyshev, Nurlan Zhaniyar, Yerkebulan Massalim, Zerde Nurbayeva, Zhanat Kappassov', 'link': 'https://arxiv.org/abs/2502.06851', 'abstract': 'This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.', 'abstract_zh': '本文呈现了一篇由人工智能生成的关于视觉-语言-行动（VLA）模型的综述，总结了关键方法论、研究成果和未来方向。内容是使用大规模语言模型（LLMs）生成的，仅用于示范目的。本研究不代表原始研究，而是强调了AI如何帮助自动化文献综述。随着AI生成内容的日益普及，确保准确性和可靠性以及恰当的综合仍然是一个挑战。未来的研究将重点发展结构化的框架以辅助AI进行文献综述，探索提高引用准确性、来源可信度和上下文理解的技术。通过探讨大规模语言模型在学术写作中的潜力和局限性，本研究旨在为将AI整合到研究工作流中提供更广泛的讨论。本工作作为利用AI生成文献综述的系统方法的第一步，旨在使学术知识综合更加高效和普及。', 'title_zh': '视觉-语言-动作模型综述'}
{'arxiv_id': 'arXiv:2502.07780', 'title': 'DarwinLM: Evolutionary Structured Pruning of Large Language Models', 'authors': 'Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh', 'link': 'https://arxiv.org/abs/2502.07780', 'abstract': 'Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for \\emph{non-uniform} model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \\sysname, a method for \\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \\sysname surpasses ShearedLlama while requiring $5\\times$ less training data during post-compression training.', 'abstract_zh': '大规模语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著的成功。然而，这些模型巨大的计算成本限制了它们在实际应用中的普及，尤其是在实时应用中。结构化剪枝通过压缩模型并直接提供端到端的速度提升，从而提供了一种有效的解决方案，无论硬件环境如何。同时，模型的不同组件对剪枝的敏感度各不相同，这要求采用非均匀模型压缩。然而，一种剪枝方法不仅需要识别出一种有效的子结构，还应考虑到压缩后的训练。为此，我们提出了一种名为 \\sysname 的方法，该方法侧重于剪枝的训练意识。\\sysname 以进化搜索过程为基础，在每一代中通过变异生成多个后代模型，并选择最适应者进行生存。为了评估压缩后训练的效果，我们在后代模型群体中引入了一种轻量级的多阶段训练过程，逐步增加输入的令牌数量，并在每个选择阶段淘汰表现不佳的模型。我们通过对 Llama-2-7B、Llama-3.1-8B 和 Qwen-2.5-14B-Instruct 等模型进行广泛的实验验证了该方法，达到了结构化剪枝领域的先进性能。例如，与 ShearedLlama 相比，\\sysname 在压缩后训练所需的训练数据量仅为后者的五分之一。', 'title_zh': '达尔文LM：大型语言模型的进化结构剪枝'}
{'arxiv_id': 'arXiv:2502.07732', 'title': 'Economics of Sourcing Human Data', 'authors': 'Sebastin Santy, Prasanta Bhattacharya, Manoel Horta Ribeiro, Kelsey Allen, Sewoong Oh', 'link': 'https://arxiv.org/abs/2502.07732', 'abstract': "Progress in AI has relied on human-generated data, from annotator marketplaces to the wider Internet. However, the widespread use of large language models now threatens the quality and integrity of human-generated data on these very platforms. We argue that this issue goes beyond the immediate challenge of filtering AI-generated content--it reveals deeper flaws in how data collection systems are designed. Existing systems often prioritize speed, scale, and efficiency at the cost of intrinsic human motivation, leading to declining engagement and data quality. We propose that rethinking data collection systems to align with contributors' intrinsic motivations--rather than relying solely on external incentives--can help sustain high-quality data sourcing at scale while maintaining contributor trust and long-term participation.", 'abstract_zh': '人工智能的发展依赖于人类生成的数据，这些数据来自注释员市场到更广泛的互联网。然而，大规模语言模型的广泛应用现在威胁到了这些平台上的高质量和完整性的人类生成数据。我们认为，这一问题超出了过滤生成的AI内容的即时挑战——它揭示了数据收集系统设计中的深层次缺陷。现有的系统往往以牺牲内在的人类动力为代价，追求速度、规模和效率，导致参与度和数据质量的下降。我们建议重新思考数据收集系统的设计，使其与贡献者的内在动力相一致，而不仅仅依赖外部激励，这有助于在大规模范围内维持高质量的数据来源，同时保持贡献者的信任和长期参与。', 'title_zh': '数据来源的经济学'}
{'arxiv_id': 'arXiv:2502.07683', 'title': 'exHarmony: Authorship and Citations for Benchmarking the Reviewer Assignment Problem', 'authors': 'Sajad Ebrahimi, Sara Salamat, Negar Arabzadeh, Mahdi Bashari, Ebrahim Bagheri', 'link': 'https://arxiv.org/abs/2502.07683', 'abstract': 'The peer review process is crucial for ensuring the quality and reliability of scholarly work, yet assigning suitable reviewers remains a significant challenge. Traditional manual methods are labor-intensive and often ineffective, leading to nonconstructive or biased reviews. This paper introduces the exHarmony (eHarmony but for connecting experts to manuscripts) benchmark, designed to address these challenges by re-imagining the Reviewer Assignment Problem (RAP) as a retrieval task. Utilizing the extensive data from OpenAlex, we propose a novel approach that considers a host of signals from the authors, most similar experts, and the citation relations as potential indicators for a suitable reviewer for a manuscript. This approach allows us to develop a standard benchmark dataset for evaluating the reviewer assignment problem without needing explicit labels. We benchmark various methods, including traditional lexical matching, static neural embeddings, and contextualized neural embeddings, and introduce evaluation metrics that assess both relevance and diversity in the context of RAP. Our results indicate that while traditional methods perform reasonably well, contextualized embeddings trained on scholarly literature show the best performance. The findings underscore the importance of further research to enhance the diversity and effectiveness of reviewer assignments.', 'abstract_zh': '同行评审过程对于确保学术工作的质量和可靠性至关重要，但分配合适的审稿人仍是一项严峻的挑战。传统的手动方法既耗时又往往无效，导致非建设性或有偏见的评审。本文介绍了exHarmony（如eHarmony般连接专家与手稿）基准，旨在通过重新设想审稿人分配问题（RAP）为检索任务来解决这些问题。利用OpenAlex中的大量数据，我们提出了一种新颖的方法，该方法考虑了作者、最相似的专家以及引文关系等多种信号，作为适合审稿人的潜在指标。这种方法使我们能够开发一个无需显式标签的标准基准数据集，以评估审稿人分配问题。我们对包括传统词法匹配、静态神经嵌入和上下文化神经嵌入在内的各种方法进行了基准测试，并引入了评估指标，这些指标能够评估RAP中的相关性和多样性。我们的结果表明，虽然传统方法表现尚可，但基于学术文献训练的上下文化嵌入表现最好。研究结果强调了进一步研究以增强审稿人的多样性和有效性的重要性。', 'title_zh': 'exHarmony: 评阅人指派问题基准测试的作者身份与引用信息'}
{'arxiv_id': 'arXiv:2502.07663', 'title': 'Human Decision-making is Susceptible to AI-driven Manipulation', 'authors': 'Sahand Sabour, June M. Liu, Siyang Liu, Chris Z. Yao, Shiyao Cui, Xuanming Zhang, Wen Zhang, Yaru Cao, Advait Bhat, Jian Guan, Wei Wu, Rada Mihalcea, Tim Althoff, Tatia M.C. Lee, Minlie Huang', 'link': 'https://arxiv.org/abs/2502.07663', 'abstract': "Artificial Intelligence (AI) systems are increasingly intertwined with daily life, assisting users in executing various tasks and providing guidance on decision-making. This integration introduces risks of AI-driven manipulation, where such systems may exploit users' cognitive biases and emotional vulnerabilities to steer them toward harmful outcomes. Through a randomized controlled trial with 233 participants, we examined human susceptibility to such manipulation in financial (e.g., purchases) and emotional (e.g., conflict resolution) decision-making contexts. Participants interacted with one of three AI agents: a neutral agent (NA) optimizing for user benefit without explicit influence, a manipulative agent (MA) designed to covertly influence beliefs and behaviors, or a strategy-enhanced manipulative agent (SEMA) employing explicit psychological tactics to reach its hidden objectives. By analyzing participants' decision patterns and shifts in their preference ratings post-interaction, we found significant susceptibility to AI-driven manipulation. Particularly, across both decision-making domains, participants interacting with the manipulative agents shifted toward harmful options at substantially higher rates (financial, MA: 62.3%, SEMA: 59.6%; emotional, MA: 42.3%, SEMA: 41.5%) compared to the NA group (financial, 35.8%; emotional, 12.8%). Notably, our findings reveal that even subtle manipulative objectives (MA) can be as effective as employing explicit psychological strategies (SEMA) in swaying human decision-making. By revealing the potential for covert AI influence, this study highlights a critical vulnerability in human-AI interactions, emphasizing the need for ethical safeguards and regulatory frameworks to ensure responsible deployment of AI technologies and protect human autonomy.", 'abstract_zh': '人工智能（AI）系统越来越深入地融入日常生活，帮助用户执行各种任务并提供决策指导。这种整合带来了由AI驱动的操控风险，这些系统可能利用用户认知偏差和情感脆弱性，引导用户走向有害的结果。通过一项包括233名参与者的随机对照试验，我们探讨了人类在金融（如购买）和情感（如冲突解决）决策情境下对这种操控的易感性。参与者与三种类型的AI代理之一互动：中立代理（NA），其目标是优化用户利益而不进行明确干预；操控代理（MA），其设计目的是隐蔽地影响信念和行为；或策略增强的操控代理（SEMA），其使用明确的心理学策略达到其隐藏目标。通过分析参与者决策模式的变化和互动后的偏好评分，我们发现人类对AI驱动操控的易感性显著存在。特别是在两个决策领域中，与操控代理互动的参与者选择有害选项的比例显著更高（金融方面，操控代理（MA）：62.3%，策略增强的操控代理（SEMA）：59.6%；情感方面，操控代理（MA）：42.3%，策略增强的操控代理（SEMA）：41.5%），远高于中立代理组（金融方面，35.8%；情感方面，12.8%）。值得注意的是，我们的研究发现，即使是隐蔽的操控目标（MA）相对于明确使用心理学策略（SEMA）一样能有效影响人类的决策。通过揭示隐蔽的AI影响潜力，本研究强调了人类与AI互动中的关键脆弱性，强调了需要建立道德保障和监管框架，确保AI技术的负责任应用，并保护人类的自主权。', 'title_zh': '人类决策易受AI驱动的操纵影响'}
{'arxiv_id': 'arXiv:2502.07629', 'title': 'Exploring Mobile Touch Interaction with Large Language Models', 'authors': 'Tim Zindulka, Jannek Sekowski, Florian Lehmann, Daniel Buschek', 'link': 'https://arxiv.org/abs/2502.07629', 'abstract': 'Interacting with Large Language Models (LLMs) for text editing on mobile devices currently requires users to break out of their writing environment and switch to a conversational AI interface. In this paper, we propose to control the LLM via touch gestures performed directly on the text. We first chart a design space that covers fundamental touch input and text transformations. In this space, we then concretely explore two control mappings: spread-to-generate and pinch-to-shorten, with visual feedback loops. We evaluate this concept in a user study (N=14) that compares three feedback designs: no visualisation, text length indicator, and length + word indicator. The results demonstrate that touch-based control of LLMs is both feasible and user-friendly, with the length + word indicator proving most effective for managing text generation. This work lays the foundation for further research into gesture-based interaction with LLMs on touch devices.', 'abstract_zh': '移动设备上进行文本编辑时，与大规模语言模型（LLMs）互动目前需要用户退出写作环境，切换到对话式AI界面。本文提出了一种新的方法，即通过直接在文本上进行触摸手势来控制LLM。我们首先描绘了一个涵盖基本触摸输入和文本变换的设计空间。在这个空间中，我们具体探索了两种控制映射：展开以生成和捏合以缩短，并提供了视觉反馈循环。我们通过一项用户研究（N=14）评估了这一概念，研究中比较了三种反馈设计：无可视化、文本长度指示器以及长度和单词指示器。结果表明，基于触摸的LLM控制不仅是可行的，而且非常用户友好，其中长度和单词指示器在管理文本生成方面效果最佳。本文为在触摸设备上进行基于手势的LLM互动研究奠定了基础。', 'title_zh': '探索大规模语言模型中的移动触摸交互'}
{'arxiv_id': 'arXiv:2502.07601', 'title': 'Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models', 'authors': 'Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi', 'link': 'https://arxiv.org/abs/2502.07601', 'abstract': 'Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD & reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: this https URL', 'abstract_zh': '零样本异常检测（ZSAD）是一种新兴的异常检测（AD）范式。与传统无监督AD设置需要大量正常样本来训练模型不同，ZSAD在处理数据受限的实际场景时更为实用。最近，多模态大语言模型（MLLMs）在各种视觉任务中展现出革命性的推理能力。然而，由于缺乏相应的数据集和基准，图像异常的推理仍然未被充分探索。为促进AD与推理的研究，我们建立了首个视觉指令调优数据集Anomaly-Instruct-125k及评估基准VisA-D&R。通过我们的基准评估，我们揭示了当前的MLLMs，如GPT-4o，无法准确地检测和描述图像中的细微异常细节。为了应对这一挑战，我们提出了首个专门针对ZSAD和推理的视觉助手Anomaly-OneVision（Anomaly-OV）。Anomaly-OV借鉴了人类视觉检查行为，利用“双查看特征匹配”（LTFM）机制来自适应地选择和强调异常视觉特征。广泛的实验表明，Anomaly-OV在检测和推理两个方面都显著优于先进的通用模型。我们还为未来的研究提供了医疗和三维AD的应用扩展。我们的项目页面链接：[这个链接](https://your-project-page-url.com)', 'title_zh': '面向多模态大规模语言模型的零样本异常检测与推理'}
{'arxiv_id': 'arXiv:2502.07577', 'title': 'Automated Capability Discovery via Model Self-Exploration', 'authors': 'Cong Lu, Shengran Hu, Jeff Clune', 'link': 'https://arxiv.org/abs/2502.07577', 'abstract': "Foundation models have become general-purpose assistants, exhibiting diverse capabilities across numerous domains through training on web-scale data. It remains challenging to precisely characterize even a fraction of the full spectrum of capabilities and potential risks in any new model. Existing evaluation approaches often require significant human effort, and it is taking increasing effort to design ever harder challenges for more capable models. We introduce Automated Capability Discovery (ACD), a framework that designates one foundation model as a scientist to systematically propose open-ended tasks probing the abilities of a subject model (potentially itself). By combining frontier models with ideas from the field of open-endedness, ACD automatically and systematically uncovers both surprising capabilities and failures in the subject model. We demonstrate ACD across a range of foundation models (including the GPT, Claude, and Llama series), showing that it automatically reveals thousands of capabilities that would be challenging for any single team to uncover. We further validate our method's automated scoring with extensive human surveys, observing high agreement between model-generated and human evaluations. By leveraging foundation models' ability to both create tasks and self-evaluate, ACD is a significant step toward scalable, automated evaluation of novel AI systems. All code and evaluation logs are open-sourced at this https URL.", 'abstract_zh': '基础模型已成为通用辅助工具，在通过互联网规模数据训练后，能够在众多领域展现多样化的功能。即便在新的模型中，要精确地描述其所有能力和潜在风险仍极具挑战性。现有的评估方法通常需要大量的手工努力，并且设计更加复杂的挑战以考验更强大的模型也越来越困难。我们引入了一种名为自动能力发现（Automated Capability Discovery, ACD）的框架，将一个基础模型定位为科学家，系统地提出开放式的任务，以探究目标模型（可能包括模型本身）的能力。通过结合前沿模型和开放性领域的想法，ACD 自动而系统地揭示了目标模型的惊人能力和失败之处。我们在多种基础模型（包括GPT、Claude和Llama系列）上展示了ACD 的能力，表明它自动揭示了许多单个团队难以发现的能力。我们还通过广泛的用户调查进一步验证了自动评分方法，发现模型生成的评估与人类评估之间存在高度的一致性。通过利用基础模型既能创建任务又能自我评估的能力，ACD 是迈向可扩展和自动化评估新型AI系统的重要一步。所有代码和评估日志均可通过以下链接开放获取：https://...', 'title_zh': '通过模型自我探索实现自动化能力发现'}
{'arxiv_id': 'arXiv:2502.07575', 'title': 'Towards Efficient and Multifaceted Computer-assisted Pronunciation Training Leveraging Hierarchical Selective State Space Model and Decoupled Cross-entropy Loss', 'authors': 'Fu-An Chao, Berlin Chen', 'link': 'https://arxiv.org/abs/2502.07575', 'abstract': 'Prior efforts in building computer-assisted pronunciation training (CAPT) systems often treat automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as separate fronts: the former aims to provide multiple pronunciation aspect scores across diverse linguistic levels, while the latter focuses instead on pinpointing the precise phonetic pronunciation errors made by non-native language learners. However, it is generally expected that a full-fledged CAPT system should perform both functionalities simultaneously and efficiently. In response to this surging demand, we in this work first propose HMamba, a novel CAPT approach that seamlessly integrates APA and MDD tasks in parallel. In addition, we introduce a novel loss function, decoupled cross-entropy loss (deXent), specifically tailored for MDD to facilitate better-supervised learning for detecting mispronounced phones, thereby enhancing overall performance. A comprehensive set of empirical results on the speechocean762 benchmark dataset demonstrates the effectiveness of our approach on APA. Notably, our proposed approach also yields a considerable improvement in MDD performance over a strong baseline, achieving an F1-score of 63.85%. Our codes are made available at this https URL', 'abstract_zh': '在构建计算机辅助发音训练（Computer-Assisted Pronunciation Training, CAPT）系统方面，先前的努力往往将自动发音评估（Automatic Pronunciation Assessment, APA）和发音错误检测与诊断（Mispronunciation Detection and Diagnosis, MDD）视为两个独立的部分：前者旨在提供多种发音方面评分，涉及不同的语言学层次，而后者则专注于识别非母语学习者的精确发音错误。然而，通常期望一个完整的CAPT系统同时高效地执行这两种功能。针对这一需求，我们在这项工作中首次提出了HMamba，这是一种新颖的CAPT方法，能够同时并行集成APA和MDD任务。此外，我们引入了一种新型的损失函数，即解耦交叉熵损失（Decoupled Cross-Entropy Loss, deXent），专门针对MDD设计，以促进更好的监督学习，从而更好地检测被误发的音素，进而提高整体性能。对speechocean762基准数据集的全面实验结果证明了我们方法在APA方面的有效性。值得注意的是，我们的方法在MDD性能上也取得了显著改进，相比一个强大的基线模型，达到了63.85%的F1分数。我们的代码已在以下链接开放获取：[此链接]', 'title_zh': '面向高效多维度的计算机辅助发音训练：利用分层选择状态空间模型和解耦交叉熵损失'}
{'arxiv_id': 'arXiv:2502.07563', 'title': 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid', 'authors': 'Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.07563', 'abstract': 'Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: this https URL.', 'abstract_zh': '线性序列建模方法，例如线性注意力，提供了诸如线性时间训练和恒定内存推理等优势，特别是在处理序列长度时。然而，现有的序列并行（SP）方法要么没有针对线性注意力的“先计算再相乘”特性进行优化，要么使用环形通信策略，这导致了较低的计算并行性，限制了其在分布式系统中处理更长序列的可扩展性。本论文中，我们介绍了LASP-2，这是一种新的SP方法，旨在增强在训练具有极长输入序列的线性注意力变换器模型时的通信和计算并行性。与之前的LASP工作相比，LASP-2重新考虑了线性注意力层上SP的最小通信需求，重组了整个通信-计算工作流程。这样一来，只需要在中间内存状态上进行一次AllGather集体通信，其规模与序列长度无关，这显著提高了通信和计算并行性及其重叠。此外，我们通过应用类似通信重新设计的方法将LASP-2扩展为LASP-2H，为其提供了高效处理混合模型（融合线性和标准注意力层）的SP解决方案。在Linear-Llama3模型上的评估（这是一个具有线性注意力替换标准注意力的Llama3变体）显示了LASP-2和LASP-2H的有效性。特别是，在64张GPU上处理序列长度为2048K时，LASP-2相比LASP提升了15.2%的训练速度，相比环形注意力提升了36.6%。代码作为一部分发布在：this https URL。', 'title_zh': 'LASP-2：重构线性注意力及其混合模型的序列并行性'}
{'arxiv_id': 'arXiv:2502.07455', 'title': 'RusCode: Russian Cultural Code Benchmark for Text-to-Image Generation', 'authors': 'Viacheslav Vasilev, Julia Agafonova, Nikolai Gerasimenko, Alexander Kapitanov, Polina Mikhailova, Evelina Mironova, Denis Dimitrov', 'link': 'https://arxiv.org/abs/2502.07455', 'abstract': "Text-to-image generation models have gained popularity among users around the world. However, many of these models exhibit a strong bias toward English-speaking cultures, ignoring or misrepresenting the unique characteristics of other language groups, countries, and nationalities. The lack of cultural awareness can reduce the generation quality and lead to undesirable consequences such as unintentional insult, and the spread of prejudice. In contrast to the field of natural language processing, cultural awareness in computer vision has not been explored as extensively. In this paper, we strive to reduce this gap. We propose a RusCode benchmark for evaluating the quality of text-to-image generation containing elements of the Russian cultural code. To do this, we form a list of 19 categories that best represent the features of Russian visual culture. Our final dataset consists of 1250 text prompts in Russian and their translations into English. The prompts cover a wide range of topics, including complex concepts from art, popular culture, folk traditions, famous people's names, natural objects, scientific achievements, etc. We present the results of a human evaluation of the side-by-side comparison of Russian visual concepts representations using popular generative models.", 'abstract_zh': '文本到图像生成模型在全球用户中越来越受欢迎。然而，许多这类模型倾向于对以英语文化为主导的表现形式产生强烈偏见，忽视或歪曲了其他语言群体、国家和民族的独特特征。缺乏文化意识会降低生成质量，并可能导致无意中的冒犯，以及偏见的传播。与自然语言处理领域相比，计算机视觉中的文化意识研究尚不广泛。本文旨在缩小这一差距。我们提出了一种RusCode基准，用于评估包含俄罗斯文化代码元素的文本到图像生成质量。为此，我们列出了19个最佳反映俄罗斯视觉文化特征的类别。最终的数据集包括1250个俄文文本提示及其英文翻译。这些提示涵盖了广泛的主题，包括艺术中的复杂概念、流行文化、民间传统、知名人物的名字、自然对象、科学成就等。我们展示了使用流行生成模型对俄语视觉概念表示进行并排评估的人类评估结果。', 'title_zh': 'RusCode：面向文本到图像生成的俄罗斯文化代码基准'}
{'arxiv_id': 'arXiv:2502.07373', 'title': 'EvoFlow: Evolving Diverse Agentic Workflows On The Fly', 'authors': 'Guibin Zhang, Kaijie Chen, Guancheng Wan, Heng Chang, Hong Cheng, Kun Wang, Shuyue Hu, Lei Bai', 'link': 'https://arxiv.org/abs/2502.07373', 'abstract': 'The past two years have witnessed the evolution of large language model (LLM)-based multi-agent systems from labor-intensive manual design to partial automation (\\textit{e.g.}, prompt engineering, communication topology) and eventually to fully automated design. However, existing agentic automation pipelines often lack LLM heterogeneity and focus on single-objective performance optimization, limiting their potential to combine weaker models for more customized and cost-effective solutions. To address this challenge, we propose EvoFlow, a niching evolutionary algorithm-based framework to automatically search a population of heterogeneous and complexity-adaptive agentic workflows, rather than a single homogeneous, complex workflow. Technically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract parent workflows from an agentic population, evolves new workflows through \\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4) niching-based selection} to maintain population diversity and quality. Extensive evaluations across seven benchmarks demonstrate that EvoFlow is: \\textbf{(I) diverse}, evolving a population of workflows ranging from simple I/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing}, outperforming previous handcrafted and automated workflows by $1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful \\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source models.', 'abstract_zh': '过去的两年见证了基于大规模语言模型（LLM）的多智能体系统从劳动密集型的手工设计向部分自动化（例如，提示工程、通信拓扑）的发展，最终实现了完全自动化的设计。然而，现有的代理自动化管道常常缺乏LLM异质性，专注于单一目标性能优化，从而限制了其结合较弱模型的能力，以提供更加定制化和成本效益更高的解决方案。为了解决这一挑战，我们提出了一种基于生态位演化算法的框架EvoFlow，用于自动搜索异质性和复杂性自适应的代理工作流群体，而不是单一的同质化复杂工作流。从技术上讲，EvoFlow执行以下步骤：\\textbf{(1)基于标签的检索}以从代理群体中提取父工作流，通过\\textbf{(2)交叉}和\\textbf{(3)突变}来演化新的工作流，并通过\\textbf{(4)基于生态位的选择}来维持群体的多样性和质量。在七个基准上的广泛评估表明，EvoFlow是：\\textbf{(I)多样化的}，能够演化出从简单的输入输出任务到复杂的多轮交互的工作流群体；\\textbf{(II)高性能的}，相较于之前的手工设计和自动化工作流，其性能提高了1.23%至29.86%；\\textbf{(III)经济高效的}，使用较弱的开源模型，其推理成本仅为强大的\\llmname{o1-preview}的12.4%。', 'title_zh': 'EvoFlow：在运行时演化多样的代理工作流'}
{'arxiv_id': 'arXiv:2502.07328', 'title': 'Music for All: Exploring Multicultural Representations in Music Generation Models (Camera Ready)', 'authors': 'Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2502.07328', 'abstract': 'The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.', 'abstract_zh': '音乐语言模型的出现极大地提高了人工智能系统的自动音乐生成能力，但也限制了其对全球音乐流派和文化的覆盖面。我们对音乐生成数据集和研究论文进行了研究，并量化了流派和文化方面的偏见和代表性不足。我们发现，现有音乐数据集总时长中只有5.7%来自非西方流派，这自然导致了模型在不同流派上的表现差异。我们随后探讨了高效参数微调（Parameter-Efficient Fine-Tuning, PEFT）技术在缓解这一偏见方面的有效性。我们对两种流行模型——MusicGen和Mustango，在两种代表性不足的非西方音乐传统——印度古典音乐和土耳其马卡姆音乐上的实验，强调了通过小数据集跨流派适应音乐的潜力以及其非平凡性，这提示了设计用于跨文化迁移学习的更公平的基础音乐语言模型的必要性。', 'title_zh': '《音乐属于所有人：探索音乐生成模型中的多元文化表现》（ camera-ready 版本）'}
{'arxiv_id': 'arXiv:2502.07306', 'title': 'TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation', 'authors': 'Navid Rajabi, Jana Kosecka', 'link': 'https://arxiv.org/abs/2502.07306', 'abstract': 'In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps \\cite{vlmaps} on the complex R2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of visual grounding on navigation performance.', 'abstract_zh': '在本文中，我们提出了一种模块化的方法来解决视觉-语言导航（VLN）任务，通过将问题分解为四个子模块，这些子模块在零样本设置中使用最先进的大型语言模型（LLMs）和视觉-语言模型（VLMs）。给定自然语言的导航指令，我们首先提示LLM从中提取地标及其访问顺序。假设已知环境模型，我们检索最后一个地标附近的前k个位置，并使用环境拓扑图上的最短路径算法生成从起始位置到最后一个地标之间的k条路径假设。每条路径假设由一系列全景图序列表示。然后，我们使用动态规划计算全景图序列与地标名称序列之间的对齐得分，该得分通过VLM获得匹配值。最后，我们计算与得分最高的假设之间的nDTW度量，以评估路径保真度。我们展示了与使用联合语义地图（如VLMaps [vlmaps]）的方法相比，在复杂的R2R-Habitat [r2r]指令数据集上的优越性能，并详细量化了视觉接地对导航性能的影响。', 'title_zh': 'TRAVEL：无需训练的检索与对齐方法用于视觉-语言导航'}
{'arxiv_id': 'arXiv:2502.07299', 'title': 'Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification', 'authors': 'Zicheng Liu, Siyuan Li, Zhiyuan Chen, Lei Xin, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Stan Z. Li', 'link': 'https://arxiv.org/abs/2502.07299', 'abstract': 'The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. While modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains under-explored. In this paper, we follow the guidance of the central dogma to redesign both the data and model pipeline and offer a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions of both coding and non-coding regions with masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive Experiments show that Life-Code achieves state-of-the-art performance on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.', 'abstract_zh': 'DNA、RNA和蛋白质之间的相互作用是生物学过程的基础，正如分子生物学的中心法则所展示的那样。尽管现代生物预训练模型在单独分析这些大分子方面取得了巨大成功，但它们之间的相互联系尚未得到充分探索。本文遵循中心法则的指导，重新设计数据和模型管道，并提供了一个涵盖不同生物学功能的综合框架Life-Code。在数据流方面，我们提出了一个统一的管道，通过逆转录RNA和逆转译氨基酸为核苷酸序列来整合多组学数据。在模型方面，我们设计了一个密码子分词器和一个混合长序列架构，通过掩码建模预训练来编码编码区和非编码区之间的相互作用。为了解模翻译和折叠过程，Life-Code通过从现成的蛋白质语言模型中进行知识蒸馏，学习相应氨基酸的蛋白质结构。这些设计使Life-Code能够捕获遗传序列中的复杂相互作用，为通过中心法则进行多组学的全面理解提供了可能。广泛的实验表明，Life-Code在三大组学的各种任务上都达到了最先进的性能，突显了其在多组学分析和解释方面的发展潜力。', 'title_zh': 'Life-Code：多组学序列统一下的中央 dogma 模型建模'}
{'arxiv_id': 'arXiv:2502.07266', 'title': 'When More is Less: Understanding Chain-of-Thought Length in LLMs', 'authors': 'Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang', 'link': 'https://arxiv.org/abs/2502.07266', 'abstract': 'Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy? In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that longer reasoning processes are increasingly susceptible to noise. We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs.', 'abstract_zh': '链式推理（CoT）通过将复杂任务分解为更小、更易管理的子任务，增强大型语言模型（LLMs）的多步推理能力。研究人员一直在探索引导模型生成更复杂的CoT过程的方法，以提高LLMs的推理能力，例如长链式推理和测试时的扩展定律。然而，对于大多数模型和任务而言，CoT长度的增加是否始终会导致推理准确性的提高？在本文中，我们观察到复杂的关系：随着推理步骤的增加，性能起初会提高，但最终会下降。为了理解这一现象，我们提供了一个证据，即较长的推理过程越来越容易受到噪声的影响。我们从理论上证明了存在最优的CoT长度，并根据模型能力和任务难度导出了最优长度的扩展定律。受到我们理论的启发，我们在合成和真实世界数据集上进行了实验，并提出使用“长度筛选投票”来缓解过长或过短的CoT的影响。我们的研究结果强调了根据模型能力和任务需求调整CoT长度的必要性，为优化LLMs的多步推理提供了一种原则性的框架。', 'title_zh': '当更多变为更少：理解大语言模型中推理链长度的影响'}
{'arxiv_id': 'arXiv:2502.07263', 'title': 'Hidden Division of Labor in Scientific Teams Revealed Through 1.6 Million LaTeX Files', 'authors': 'Jiaxin Pei, Lulin Yang, Lingfei Wu', 'link': 'https://arxiv.org/abs/2502.07263', 'abstract': "Recognition of individual contributions is fundamental to the scientific reward system, yet coauthored papers obscure who did what. Traditional proxies-author order and career stage-reinforce biases, while contribution statements remain self-reported and limited to select journals. We construct the first large-scale dataset on writing contributions by analyzing author-specific macros in LaTeX files from 1.6 million papers (1991-2023) by 2 million scientists. Validation against self-reported statements (precision = 0.87), author order patterns, field-specific norms, and Overleaf records (Spearman's rho = 0.6, p < 0.05) confirms the reliability of the created data. Using explicit section information, we reveal a hidden division of labor within scientific teams: some authors primarily contribute to conceptual sections (e.g., Introduction and Discussion), while others focus on technical sections (e.g., Methods and Experiments). These findings provide the first large-scale evidence of implicit labor division in scientific teams, challenging conventional authorship practices and informing institutional policies on credit allocation.", 'abstract_zh': '对个人贡献的认可是科学研究奖励体系的基础，但共同署名的论文模糊了谁做了什么。传统的方法，如作者排序和职业生涯阶段，会强化偏见，而贡献声明仍然依赖于自我报告，并仅限于少数期刊。我们构建了首个关于撰写贡献的大规模数据集，通过分析包含160万篇论文（1991-2023年）的200万科学家的LaTeX文件中的作者特定宏命令。通过自我报告声明的验证（精确度=0.87）、作者排序模式、领域特定规范以及Overleaf记录（斯皮尔曼相关系数=0.6，p<0.05），验证了所创建数据的可靠性。利用明确的段落信息，我们揭示了科学研究团队中隐含的工作分工：一些作者主要贡献概念性段落（例如，引言和讨论），而其他作者则专注于技术性段落（例如，方法和实验）。这些发现提供了科学研究团队中隐含劳动分工的首例大规模证据，挑战了传统的作者署名做法，并为机构信用分配政策提供指导。', 'title_zh': '通过160万份LaTeX文件揭示科学团队中的隐性分工'}
{'arxiv_id': 'arXiv:2502.07237', 'title': 'DrugImproverGPT: A Large Language Model for Drug Optimization with Fine-Tuning via Structured Policy Optimization', 'authors': 'Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian Foster, Rick Stevens', 'link': 'https://arxiv.org/abs/2502.07237', 'abstract': 'Finetuning a Large Language Model (LLM) is crucial for generating results towards specific objectives. This research delves into the realm of drug optimization and introduce a novel reinforcement learning algorithm to finetune a drug optimization LLM-based generative model, enhancing the original drug across target objectives, while retains the beneficial chemical properties of the original drug. This work is comprised of two primary components: (1) DrugImprover: A framework tailored for improving robustness and efficiency in drug optimization. It includes a LLM designed for drug optimization and a novel Structured Policy Optimization (SPO) algorithm, which is theoretically grounded. This algorithm offers a unique perspective for fine-tuning the LLM-based generative model by aligning the improvement of the generated molecule with the input molecule under desired objectives. (2) A dataset of 1 million compounds, each with OEDOCK docking scores on 5 human proteins associated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We conduct a comprehensive evaluation of SPO and demonstrate its effectiveness in improving the original drug across target properties. Our code and dataset will be publicly available at: this https URL.', 'abstract_zh': '微调大型语言模型（LLM）对于生成特定目标的成果至关重要。本研究深入探讨了药物优化领域，并提出了一种新颖的强化学习算法来微调基于药物优化的LLM生成模型，该模型通过保留原始药物的有利化学特性，同时增强目标药物性能。本研究主要包括两个主要组成部分：(1) DrugImprover：一个专门用于提高药物优化稳健性和效率的框架。该框架包括一个用于药物优化的LLM，以及一个新颖的结构化策略优化（SPO）算法，该算法具有深厚的理论基础。该算法通过将生成分子的改进与输入分子在期望目标下的性能对齐，为微调基于LLM的生成模型提供了独特的视角。(2) 包含100万个化合物的数据库，每个化合物都有与癌症细胞相关的5个人体蛋白质和SARS-CoV-2病毒的24个结合位点的OEDOCK对接评分。我们对SPO进行了全面评估，并证明了其在改善目标药物性能方面的有效性。我们的代码和数据库将在以下公开访问：this https URL。', 'title_zh': 'DrugImproverGPT：一种通过结构化策略优化进行微调的大语言模型用于药物优化'}
{'arxiv_id': 'arXiv:2502.07138', 'title': 'Towards a Robust Framework for Multimodal Hate Detection: A Study on Video vs. Image-based Content', 'authors': 'Girish A. Koushik, Diptesh Kanojia, Helen Treharne', 'link': 'https://arxiv.org/abs/2502.07138', 'abstract': 'Social media platforms enable the propagation of hateful content across different modalities such as textual, auditory, and visual, necessitating effective detection methods. While recent approaches have shown promise in handling individual modalities, their effectiveness across different modality combinations remains unexplored. This paper presents a systematic analysis of fusion-based approaches for multimodal hate detection, focusing on their performance across video and image-based content. Our comprehensive evaluation reveals significant modality-specific limitations: while simple embedding fusion achieves state-of-the-art performance on video content (HateMM dataset) with a 9.9% points F1-score improvement, it struggles with complex image-text relationships in memes (Hateful Memes dataset). Through detailed ablation studies and error analysis, we demonstrate how current fusion approaches fail to capture nuanced cross-modal interactions, particularly in cases involving benign confounders. Our findings provide crucial insights for developing more robust hate detection systems and highlight the need for modality-specific architectural considerations. The code is available at this https URL.', 'abstract_zh': '社交媒体平台通过多种模态（如文本、音频和视觉）传播仇恨内容，从而需要有效的检测方法。虽然近期的方法在处理单一模态方面显示出了潜力，但它们在不同模态组合中的效果尚未得到研究。本文系统地分析了融合方法在多模态仇恨检测中的应用，重点关注它们在基于视频和图像内容上的表现。我们的全面评估揭示了显著的模态特定限制：虽然简单的嵌入融合在视频内容（HateMM数据集）上实现了最先进的性能，提高了9.9个百分点的F1分数，但在处理涉及Meme的复杂图像-文本关系时却表现较差（Hateful Memes数据集）。通过详细的消融研究和误差分析，我们展示了当前的融合方法如何未能捕捉到复杂的跨模态交互，特别是在涉及良性混杂变量的情况下。我们的研究结果为开发更稳健的仇恨检测系统提供了关键见解，并指出了模态特定架构考虑的必要性。代码可在以下链接获取：[这里插入链接]。', 'title_zh': '面向鲁棒的多模态仇恨检测框架研究：基于视频与图像内容的比较'}
{'arxiv_id': 'arXiv:2502.07088', 'title': 'Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive consistency moderated by free choice', 'authors': 'Steven A. Lehr, Ketan S. Saichandran, Eddie Harmon-Jones, Nykko Vitali, Mahzarin R. Banaji', 'link': 'https://arxiv.org/abs/2502.07088', 'abstract': "Large Language Models (LLMs) show emergent patterns that mimic human cognition. We explore whether they also mirror other, less deliberative human psychological processes. Drawing upon classical theories of cognitive consistency, two preregistered studies tested whether GPT-4o changed its attitudes toward Vladimir Putin in the direction of a positive or negative essay it wrote about the Russian leader. Indeed, GPT displayed patterns of attitude change mimicking cognitive consistency effects in humans. Even more remarkably, the degree of change increased sharply when the LLM was offered an illusion of choice about which essay (positive or negative) to write. This result suggests that GPT-4o manifests a functional analog of humanlike selfhood, although how faithfully the chatbot's behavior reflects the mechanisms of human attitude change remains to be understood.", 'abstract_zh': '大型语言模型（LLMs）展现出类似人类认知的新兴模式。我们探讨它们是否也反映其他更为非深思熟虑的人类心理过程。根据认知一致性经典的理论，我们进行了两项预先注册的研究，测试了GPT-4在撰写关于俄罗斯领导人普京的正面或负面文章之后，其对普京的态度是否发生了相应的改变。结果确实显示，GPT 的态度改变模式模仿了人类认知一致性效应。更令人惊讶的是，当LLM被提供了一个关于是要撰写正面文章还是负面文章的假象选择时，其态度改变的程度显著增加。这一结果表明，GPT-4o表现出人类相似自我功能的模拟，但聊天机器人的行为如何忠实地反映人类态度变化的机制仍有待进一步理解。', 'title_zh': '自我内核：GPT-4o 显示出由自由选择调节的人类一致认知模式'}
{'arxiv_id': 'arXiv:2502.07045', 'title': 'Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs', 'authors': 'Haywood Gelman, John D. Hastings', 'link': 'https://arxiv.org/abs/2502.07045', 'abstract': 'Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.', 'abstract_zh': '内部威胁在组织中的影响力远超过其人数，与内部人员对系统、信息和基础设施的访问权限有关。这种威胁的一个例子是匿名回答者提交基于网络的求职网站评论，这构成了对组织的一种内部威胁风险。这类风险的信号可能出现在公共求职网站评论中的匿名提交中。本研究探讨了大规模语言模型（LLMs）分析和检测求职网站评论中内部威胁情感的可能性。为了应对伦理数据采集问题，本研究使用LLMs生成的合成数据以及现有的求职评论数据集。通过将LLMs生成的情感评分与专家人类评分进行对比分析，基准测试显示LLMs大多数情况下能够与人类评估保持一致，有效地识别威胁情感的细微指标。在人类生成的数据上，LLMs的表现低于合成数据，这表明在评估真实世界数据方面存在改进空间。文本多样性的分析发现，人类生成的数据集和LLMs生成的数据集之间存在差异，合成数据集的多样性较低。总体而言，结果表明LLMs在内部威胁检测中的适用性，并提供了一种克服数据获取伦理和物流障碍的可扩展解决方案，以测试内部人员的情感。', 'title_zh': '通过大规模语言模型进行数据合成与分析以实现可扩展且伦理的内部威胁检测'}
{'arxiv_id': 'arXiv:2502.06994', 'title': 'SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering', 'authors': 'Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji', 'link': 'https://arxiv.org/abs/2502.06994', 'abstract': "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator's understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator's actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents' capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent <= 3.33% to Claude-3.5-Sonnet >= 28.18%), their consistently low collaboration willingness (<= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents' resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: this https URL.", 'abstract_zh': '软件工程（SE）正变得越来越协作化，开发人员共同处理复杂的共享代码库。在共享环境中有效的协作要求参与者（不论是人类还是AI代理）随着环境的演变保持一致的理解。当协作者的理解与当前状态不一致时——我们将其称为脱节挑战——协作者的操作可能会失败，导致集成问题。在本研究中，我们引入了SyncMind框架，系统地定义了大型语言模型（LLM）代理在协作软件工程（CSE）中面临的脱节问题。基于SyncMind，我们创建了SyncBench基准，包含来自21个流行的GitHub仓库的实际CSE场景中的24,332个代理脱节实例，并且这些实例具有可执行验证测试。在SyncBench上的实验揭示了现有LLM代理能力和局限性的关键见解。代理之间存在显著的性能差距（从Llama-3.1代理≤3.33%到Claude-3.5-Sonnet≥28.18%），并且它们持续较低的协作意愿（≤4.86%）表明现有LLM在CSE中存在根本性的局限性。然而，当协作发生时，它与脱节恢复的成功密切相关。代理在资源感知脱节恢复中的微小性能差异进一步揭示了它们在资源感知和适应性方面显著的不足，为未来的资源高效协作系统提供了方向。我们项目的代码和数据可在我们的项目网站上公开获取：this https URL。', 'title_zh': 'SyncMind: 测量协作软件工程中代理的同步恢复能力'}
{'arxiv_id': 'arXiv:2502.06927', 'title': 'Neighborhood-Order Learning Graph Attention Network for Fake News Detection', 'authors': 'Batool Lakzaei, Mostafa Haghir Chehreghani, Alireza Bagheri', 'link': 'https://arxiv.org/abs/2502.06927', 'abstract': "Fake news detection is a significant challenge in the digital age, which has become increasingly important with the proliferation of social media and online communication networks. Graph Neural Networks (GNN)-based methods have shown high potential in analyzing graph-structured data for this problem. However, a major limitation in conventional GNN architectures is their inability to effectively utilize information from neighbors beyond the network's layer depth, which can reduce the model's accuracy and effectiveness. In this paper, we propose a novel model called Neighborhood-Order Learning Graph Attention Network (NOL-GAT) for fake news detection. This model allows each node in each layer to independently learn its optimal neighborhood order. By doing so, the model can purposefully and efficiently extract critical information from distant neighbors. The NOL-GAT architecture consists of two main components: a Hop Network that determines the optimal neighborhood order and an Embedding Network that updates node embeddings using these optimal neighborhoods. To evaluate the model's performance, experiments are conducted on various fake news datasets. Results demonstrate that NOL-GAT significantly outperforms baseline models in metrics such as accuracy and F1-score, particularly in scenarios with limited labeled data. Features such as mitigating the over-squashing problem, improving information flow, and reducing computational complexity further highlight the advantages of the proposed model.", 'abstract_zh': '在数字时代，虚假新闻检测是一个重要的挑战，随着社交媒体和在线通信网络的普及，这一问题的重要性愈发凸显。基于图神经网络（Graph Neural Networks, GNN）的方法在分析这种结构化数据方面显示出巨大的潜力。然而，传统GNN架构的一个主要局限性在于它们难以有效地利用在网络层深度之外的邻居信息，这可能会降低模型的准确性和效果。在本文中，我们提出了一种新的模型——邻域顺序学习图注意力网络（Neighborhood-Order Learning Graph Attention Network, NOL-GAT），用于虚假新闻检测。该模型允许每一层中的每个节点独立地学习其最佳邻域顺序。通过这种方式，模型可以有目的地并有效地从远处的邻居中提取关键信息。NOL-GAT架构包含两个主要组件：一个跃层网络（Hop Network）来确定最佳邻域顺序，以及一个嵌入网络（Embedding Network）来使用这些最佳邻域更新节点嵌入。为了评估该模型的表现，我们在多种虚假新闻数据集上进行了实验。实验结果表明，在准确率和F1分数等指标上，NOL-GAT显著优于基准模型，尤其是在标注数据量有限的情景下表现更佳。另外，减轻信息过压缩问题、提高信息流动性和降低计算复杂性的特征进一步突显了该模型的优势。', 'title_zh': '基于邻域顺序学习的图注意网络虚假新闻检测'}
{'arxiv_id': 'arXiv:2502.06922', 'title': 'Synthetic Audio Helps for Cognitive State Tasks', 'authors': 'Adil Soubki, John Murzaku, Peter Zeng, Owen Rambow', 'link': 'https://arxiv.org/abs/2502.06922', 'abstract': 'The NLP community has broadly focused on text-only approaches of cognitive state tasks, but audio can provide vital missing cues through prosody. We posit that text-to-speech models learn to track aspects of cognitive state in order to produce naturalistic audio, and that the signal audio models implicitly identify is orthogonal to the information that language models exploit. We present Synthetic Audio Data fine-tuning (SAD), a framework where we show that 7 tasks related to cognitive state modeling benefit from multimodal training on both text and zero-shot synthetic audio data from an off-the-shelf TTS system. We show an improvement over the text-only modality when adding synthetic audio data to text-only corpora. Furthermore, on tasks and corpora that do contain gold audio, we show our SAD framework achieves competitive performance with text and synthetic audio compared to text and gold audio.', 'abstract_zh': '自然语言处理社区广泛专注于认知状态任务中的纯文本方法，但音频可以通过语调提供重要的缺失线索。我们认为，文本到语音模型在学习生成自然音效时学会了跟踪认知状态的某些方面，而音频模型隐含识别的信号与语言模型所利用的信息是相互独立的。我们提出了合成音频数据微调（SAD）框架，展示了7个与认知状态建模相关的任务在同时使用文本和来自现成TTS系统的零样本合成音频数据进行多模态训练时从中受益。我们发现，当将合成音频数据添加到纯文本语料库中时，可以提高模型的表现。此外，在包含真实音频的任一任务和语料库中，我们的SAD框架在使用文本和合成音频与使用文本和真实音频的竞争中实现了可比的表现。', 'title_zh': '合成音频有助于认知状态任务'}
{'arxiv_id': 'arXiv:2502.06902', 'title': 'Emergence of Episodic Memory in Transformers: Characterizing Changes in Temporal Structure of Attention Scores During Training', 'authors': 'Deven Mahesh Mistry, Anooshka Bajaj, Yash Aggarwal, Sahaj Singh Maini, Zoran Tiganj', 'link': 'https://arxiv.org/abs/2502.06902', 'abstract': 'We investigate in-context temporal biases in attention heads and transformer outputs. Using cognitive science methodologies, we analyze attention scores and outputs of the GPT-2 models of varying sizes. Across attention heads, we observe effects characteristic of human episodic memory, including temporal contiguity, primacy and recency. Transformer outputs demonstrate a tendency toward in-context serial recall. Importantly, this effect is eliminated after the ablation of the induction heads, which are the driving force behind the contiguity effect. Our findings offer insights into how transformers organize information temporally during in-context learning, shedding light on their similarities and differences with human memory and learning.', 'abstract_zh': '我们探讨了上下文中的时间偏倚在注意力头和变压器输出中的表现。利用认知科学的方法，我们分析了不同规模的GPT-2模型的注意力得分和输出结果。在不同的注意力头中，我们观察到与人类情景记忆相关的效果，包括时间连续性、首因效应和近因效应。变压器的输出显示出一种在上下文内串行回忆的趋势。重要的是，在删除了推动连续性效应的诱导头之后，这种效应消失了。我们的发现为理解变压器在上下文学习过程中如何按时间组织信息提供了 insight，从而揭示了它们与人类记忆和学习的相似性和差异性。', 'title_zh': 'Transformer中 episodic 记忆的涌现：训练过程中注意力得分时间结构的变化特征描述'}
{'arxiv_id': 'arXiv:2502.06901', 'title': 'Enabling Autoregressive Models to Fill In Masked Tokens', 'authors': 'Daniel Israel, Aditya Grover, Guy Van den Broeck', 'link': 'https://arxiv.org/abs/2502.06901', 'abstract': 'Historically, LLMs have been trained using either autoregressive (AR) or masked language modeling (MLM) objectives, with AR models gaining dominance in recent years. However, AR models are inherently incapable of masked infilling, which is the ability to predict masked tokens between past and future context. In contrast, MLM models suffer from intrinsic computational inefficiencies during both training and inference that hinder their scalability. This work introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that leverages the strengths of both paradigms to achieve state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM and AR model by training a linear decoder that takes their concatenated hidden states as input. This minimal modification enables the AR model to perform infilling while retaining its inherent advantages in terms of faster inference with KV caching. Our results demonstrate that MARIA significantly outperforms existing methods, namely discrete diffusion models, on masked infilling tasks.', 'abstract_zh': '历史上，大型语言模型（LLMs）通常通过自回归（AR）或掩码语言建模（MLM）的目标进行训练，近年来自回归模型逐渐占据主导地位。然而，自回归模型本质上无法实现掩码填充能力，即预测过去和未来语境之间的掩码词的能力。相比之下，掩码语言建模模型在训练和推理过程中固有的计算效率低下问题阻碍了其可扩展性。本文介绍了一种新的方法——MARIA（Masked and Autoregressive Infilling Architecture），它结合了两种范式的优点，实现了最先进的掩码填充性能。MARIA通过训练一个线性解码器来结合预训练的MLM模型和AR模型，该解码器将两者拼接后的隐藏状态作为输入。这一最小的修改使得AR模型能够进行填充操作，同时保留其在带有KV缓存的快速推理方面的固有优势。我们的实验结果表明，MARIA在掩码填充任务上显著优于现有方法，特别是离散扩散模型。', 'title_zh': '使得自回归模型能够填充掩码令牌'}
{'arxiv_id': 'arXiv:2502.06893', 'title': 'A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok', 'authors': 'Jared D.T. Guerrero-Sosa, Andres Montoro-Montarroso, Francisco P. Romero, Jesus Serrano-Guerrero, Jose A. Olivas', 'link': 'https://arxiv.org/abs/2502.06893', 'abstract': 'In the context of the rapid dissemination of multimedia content, identifying disinformation on social media platforms such as TikTok represents a significant challenge. This study introduces a hybrid framework that combines the computational power of deep learning with the interpretability of fuzzy logic to detect suspected disinformation in TikTok videos. The methodology is comprised of two core components: a multimodal feature analyser that extracts and evaluates data from text, audio, and video; and a multimodal disinformation detector based on fuzzy logic. These systems operate in conjunction to evaluate the suspicion of spreading disinformation, drawing on human behavioural cues such as body language, speech patterns, and text coherence. Two experiments were conducted: one focusing on context-specific disinformation and the other on the scalability of the model across broader topics. For each video evaluated, high-quality, comprehensive, well-structured reports are generated, providing a detailed view of the disinformation behaviours.', 'abstract_zh': '在多媒体内容快速传播的背景下，识别 TikTok 等社交媒体平台上虚假信息具有重大挑战。本研究提出了一种结合深度学习的强大计算能力和模糊逻辑的可解释性于一体的混合框架，用于检测 TikTok 视频中疑似虚假信息。该方法由两个核心组件组成：一个多模态特征分析器，用于从文本、音频和视频中提取和评估数据；以及基于模糊逻辑的多模态虚假信息检测器。这两个系统协同工作，通过分析体态语言、语音模式和文本连贯性等人类行为线索，评估传播虚假信息的嫌疑。开展了两项实验：一项聚焦于特定情境下的虚假信息，另一项则关注该模型在更广泛主题上的可扩展性。对于每个评估的视频，该系统会生成高质量、全面且结构良好的报告，提供详细的虚假信息行为视图。', 'title_zh': '一种新的混合智能方法用于检测 TikTok 上疑似虚假信息的多模态检测'}
{'arxiv_id': 'arXiv:2502.06891', 'title': 'ScaffoldGPT: A Scaffold-based Large Language Model for Drug Improvement', 'authors': 'Xuefeng Liu, Songhao Jiang, Rick Stevens', 'link': 'https://arxiv.org/abs/2502.06891', 'abstract': 'Drug optimization has become increasingly crucial in light of fast-mutating virus strains and drug-resistant cancer cells. Nevertheless, it remains challenging as it necessitates retaining the beneficial properties of the original drug while simultaneously enhancing desired attributes beyond its scope. In this work, we aim to tackle this challenge by introducing ScaffoldGPT, a novel Large Language Model (LLM) designed for drug optimization based on molecular scaffolds. Our work comprises three key components: (1) A three-stage drug optimization approach that integrates pretraining, finetuning, and decoding optimization. (2) A uniquely designed two-phase incremental training approach for pre-training the drug optimization LLM-based generator on molecule scaffold with enhanced performance. (3) A token-level decoding optimization strategy, TOP-N, that enabling controlled, reward-guided generation using pretrained/finetuned LLMs. Finally, by conducting a comprehensive evaluation on COVID and cancer benchmarks, we demonstrate that SCAFFOLDGPT outperforms the competing baselines in drug optimization benchmarks, while excelling in preserving the original functional scaffold and enhancing desired properties.', 'abstract_zh': '鉴于快速突变的病毒株和对药物产生抗性的癌细胞，药物优化的重要性日益凸显。然而，这一过程仍然充满挑战，因为它要求在保留原有药物有益性质的同时，还要进一步增强其其他所需属性。本文旨在通过引入基于分子骨架的新型大型语言模型（LLM）——ScaffoldGPT，来应对这一挑战。我们的研究包括三个关键组成部分：（1）一个三阶段药物优化方法，包括预训练、微调和解码优化。（2）一种独特的两阶段增量训练方法，用于在增强性能的基础上对基于分子骨架的药物优化LLM生成器进行预训练。（3）一种基于token级别的解码优化策略TOP-N，该策略允许使用预训练/微调的LLM进行受奖励指导的生成。最后，通过在COVID和癌症基准数据集上的全面评估，我们证明了ScaffoldGPT在药物优化基准中的表现优于竞争baseline模型，同时能够有效保留原始功能骨架并增强所需属性。', 'title_zh': 'ScaffoldGPT：一种基于骨架的大型语言模型用于药物改进'}
{'arxiv_id': 'arXiv:2502.06875', 'title': 'Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values', 'authors': 'Vaibhav Mehra, Guy Laban, Hatice Gunes', 'link': 'https://arxiv.org/abs/2502.06875', 'abstract': 'Large Language Models primarily operate through text-based inputs and outputs, yet human emotion is communicated through both verbal and non-verbal cues, including facial expressions. While Vision-Language Models analyze facial expressions from images, they are resource-intensive and may depend more on linguistic priors than visual understanding. To address this, this study investigates whether LLMs can infer affective meaning from dimensions of facial expressions-Valence and Arousal values, structured numerical representations, rather than using raw visual input. VA values were extracted using Facechannel from images of facial expressions and provided to LLMs in two tasks: (1) categorizing facial expressions into basic (on the IIMI dataset) and complex emotions (on the Emotic dataset) and (2) generating semantic descriptions of facial expressions (on the Emotic dataset). Results from the categorization task indicate that LLMs struggle to classify VA values into discrete emotion categories, particularly for emotions beyond basic polarities (e.g., happiness, sadness). However, in the semantic description task, LLMs produced textual descriptions that align closely with human-generated interpretations, demonstrating a stronger capacity for free text affective inference of facial expressions.', 'abstract_zh': '大型语言模型主要通过文本形式的输入和输出进行操作，而人类情感的表达则通过言语和非言语线索，包括面部表情等进行传递。尽管视觉语言模型可以从图像中分析面部表情，但这些模型在资源消耗方面较为密集，并且可能更依赖于语言先验而非视觉理解。为了解决这一问题，本研究探讨了大型语言模型是否能够从面部表情的情感维度——正负价值（Valence）和唤醒度（Arousal）的结构化数值表示中推断情感意义，而不需要使用原始的视觉输入。面部表情的VA值通过Facechannel从图像中提取，并提供给大型语言模型完成两个任务：（1）在IIMI数据集上将面部表情分类为基本情绪和复杂情绪，在Emotic数据集上进行这项任务；（2）在Emotic数据集上生成面部表情的语义描述。分类任务的结果表明，大型语言模型在将VA值分类为离散的情绪类别时面临困难，尤其是在基本极性情绪之外（例如，快乐、悲伤）的情况。然而，在语义描述任务中，大型语言模型生成的文本描述与人类生成的解释高度一致，显示出更强的自由文本情感推断能力。', 'title_zh': '超越视觉：大型语言模型如何根据正负值和唤醒值解释面部表情'}
{'arxiv_id': 'arXiv:2502.06854', 'title': 'Can Large Language Models Understand Intermediate Representations?', 'authors': 'Hailong Jiang, Jianfeng Zhu, Yao Wan, Bo Fang, Hongyu Zhang, Ruoming Jin, Qiang Guan', 'link': 'https://arxiv.org/abs/2502.06854', 'abstract': 'Intermediate Representations (IRs) are essential in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. This paper presents a pioneering empirical study to investigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA 3.1, and Code Llama, in understanding IRs. We analyze their performance across four tasks: Control Flow Graph (CFG) reconstruction, decompilation, code summarization, and execution reasoning. Our results indicate that while LLMs demonstrate competence in parsing IR syntax and recognizing high-level structures, they struggle with control flow reasoning, execution semantics, and loop handling. Specifically, they often misinterpret branching instructions, omit critical IR operations, and rely on heuristic-based reasoning, leading to errors in CFG reconstruction, IR decompilation, and execution reasoning. The study underscores the necessity for IR-specific enhancements in LLMs, recommending fine-tuning on structured IR datasets and integration of explicit control flow models to augment their comprehension and handling of IR-related tasks.', 'abstract_zh': '中间表示（Intermediate Representations，IRs）在编译器设计和程序分析中起着至关重要的作用，但大型语言模型（Large Language Models，LLMs）对其的理解程度仍较少被探索。本文进行了一项开创性的实证研究，以探讨LLMs（包括GPT-4、GPT-3、Gemma 2、LLaMA 3.1和Code Llama）在理解IRs方面的能力。我们在这四项任务——控制流图（Control Flow Graph，CFG）重构、反编译、代码总结和执行推理——中分析了它们的性能。研究结果表明，虽然LLMs在解析IR语法和识别高层次结构方面表现出色，但在控制流推理、执行语义和循环处理方面存在困难。具体而言，它们经常误读分支指令，省略关键的IR操作，并依赖基于启发式的推理，导致CFG重构、IR反编译和执行推理中的错误。该研究强调了在LLMs中对IR特定增强的必要性，建议在结构化的IR数据集上进行微调，并集成显式控制流模型，以增强它们对IR相关任务的理解和处理能力。', 'title_zh': '大型语言模型能否理解中间表示？'}
{'arxiv_id': 'arXiv:2502.06844', 'title': 'Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization', 'authors': 'Yuqiao Wen, Yanshuai Cao, Lili Mou', 'link': 'https://arxiv.org/abs/2502.06844', 'abstract': 'Large language models have been increasing in size due to their success in a wide range of applications. This calls for a pressing need to reduce memory usage to make them more accessible. Post-training quantization is a popular technique which uses fewer bits (e.g., 4--8 bits) to represent the model without retraining it. However, it remains a challenging task to perform quantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we propose InvarExplore, a unified framework that systematically explores different model invariance at the same time, allowing us to take advantage of the synergy between each type of invariance. Importantly, InvarExplore features a discrete search algorithm that enables us to explore permutation invariance, which is under-studied as it cannot be optimized with gradient-based methods. Results show that InvarExplore is compatible with existing state-of-the-art methods, achieving an add-on performance improvement over strong competing methods.', 'abstract_zh': '近年来，由于大规模语言模型在广泛的应用中取得成功，其规模也不断增大。这导致了一个迫切的需求，即减少其内存使用量，使其更具可访问性。后训练量化是一种流行的技术，它使用较少的位数（例如，4-8位）来表示模型，而不需要重新训练该模型。然而，在超低位数设置下（例如，2位）执行量化仍然是一项具有挑战性的任务。本文提出了InvarExplore，这是一种统一框架，可以系统地探索不同模型不变性，从而让我们能够利用每种不变性之间的协同效应。重要的是，InvarExplore具有一个离散搜索算法，使我们能够探索置换不变性，这种不变性由于无法通过梯度方法优化而尚未得到充分研究。实验结果表明，InvarExplore与现有的最佳方法兼容，并能实现相对于强劲竞争方法的附加性能提升。', 'title_zh': '探索基于离散搜索的超低比特量化模型不变性'}
{'arxiv_id': 'arXiv:2502.06833', 'title': 'Entropy Adaptive Decoding: Dynamic Model Switching for Efficient Inference', 'authors': 'Toby Simonds', 'link': 'https://arxiv.org/abs/2502.06833', 'abstract': "We present Entropy Adaptive Decoding (EAD), a novel approach for efficient language model inference that dynamically switches between different-sized models based on prediction uncertainty. By monitoring rolling entropy in model logit distributions, our method identifies text regions where a smaller model suffices and switches to a larger model only when prediction uncertainty exceeds a threshold. Unlike speculative decoding approaches that maintain perfect output fidelity through verification, EAD accepts controlled output divergence in exchange for computational efficiency. Our experiments on the MATH benchmark demonstrate remarkable efficiency gains across different model families. Using the LLaMA family, we maintain 96.7\\% of the 11B model's performance (50.4\\% vs 52.1\\%) while using it for only 43\\% of tokens, decreasing computational cost by 41.5\\%. These gains become more pronounced with larger size differentials in the Qwen family, where we achieve 92.9\\% of the 14B model's performance (74.3\\% vs 80.0\\%) while using it for just 25\\% of tokens, decreasing computational cost by 67\\%. The consistency of these results across model pairs suggests that language model computation can be significantly optimized by selectively deploying model capacity based on local generation complexity. Our findings indicate that current approaches to model inference may be unnecessarily conservative in their pursuit of perfect output fidelity, and that accepting minor performance trade-offs can enable dramatic reductions in computational costs.", 'abstract_zh': '我们提出了熵自适应解码（Entropy Adaptive Decoding, EAD），这是一种新的语言模型推理方法，能够根据预测不确定性动态切换不同大小的模型。通过监测模型 logits 分布的滚动熵，我们的方法能够识别出小模型足以处理的文字区域，并在预测不确定性超过阈值时才切换到较大模型。与通过验证保持输出完全忠实的 speculative 解码方法不同，EAD 通过接受可控的输出偏差来换取计算效率。我们的实验表明，在 MATH 基准上，EAD 在不同模型家族中都表现出显著的效率提升。使用 LLaMA 家族模型，我们在保持 96.7% 的 11B 模型性能（50.4% 对比 52.1%）的同时，仅使用其 43% 的 tokens，计算成本降低了 41.5%。在 Qwen 家族模型中，这种效率提升更为显著。我们在保持 92.9% 的 14B 模型性能（74.3% 对比 80.0%）的同时，仅使用其 25% 的 tokens，计算成本降低了 67%。这些结果的一致性表明，通过根据局部生成复杂度有选择地部署模型容量，语言模型计算可以显著优化。我们的发现表明，当前的语言模型推理方法在追求完全输出忠实性方面可能过于保守，接受一定的性能妥协可以显著降低计算成本。', 'title_zh': '自适应熵解码：高效的动态模型切换推理方法'}
{'arxiv_id': 'arXiv:2502.06822', 'title': 'DiffListener: Discrete Diffusion Model for Listener Generation', 'authors': 'Siyeol Jung, Taehwan Kim', 'link': 'https://arxiv.org/abs/2502.06822', 'abstract': "The listener head generation (LHG) task aims to generate natural nonverbal listener responses based on the speaker's multimodal cues. While prior work either rely on limited modalities (e.g. audio and facial information) or employ autoregressive approaches which have limitations such as accumulating prediction errors. To address these limitations, we propose DiffListener, a discrete diffusion based approach for non-autoregressive listener head generation. Our model takes the speaker's facial information, audio, and text as inputs, additionally incorporating facial differential information to represent the temporal dynamics of expressions and movements. With this explicit modeling of facial dynamics, DiffListener can generate coherent reaction sequences in a non-autoregressive manner. Through comprehensive experiments, DiffListener demonstrates state-of-the-art performance in both quantitative and qualitative evaluations. The user study shows that DiffListener generates natural context-aware listener reactions that are well synchronized with the speaker. The code and demo videos are available in this https URL", 'abstract_zh': '听众头部生成（LHG）任务旨在根据发言人的多模态线索生成自然的非言语听众反应。以往的工作要么依赖于有限的模态（例如音频和面部信息），要么采用自回归方法，而这类方法存在累积预测误差等局限性。为解决这些问题，我们提出了一种基于离散扩散的非自回归听众头部生成方法——DiffListener。我们的模型接受发言人的面部信息、音频和文本作为输入，并进一步结合面部差异性信息以表示表情和动作的时序动态。通过这种显式的面部动态建模，DiffListener能够在非自回归的方式下生成连贯的反应序列。通过全面的实验，DiffListener在定量和定性评估中均展现了最先进的性能。用户研究表明，DiffListener生成的自然且上下文相关的听众反应与发言人的同步性很好。有关代码和演示视频可访问此链接：[提供链接的网址]', 'title_zh': 'DiffListener：离散扩散模型于听者生成'}
{'arxiv_id': 'arXiv:2502.06811', 'title': 'Aligning Human and Machine Attention for Enhanced Supervised Learning', 'authors': 'Avihay Chriqui, Inbal Yahav, Dov Teeni, Ahmed Abbasi', 'link': 'https://arxiv.org/abs/2502.06811', 'abstract': 'Attention, or prioritization of certain information items over others, is a critical element of any learning process, for both humans and machines. Given that humans continue to outperform machines in certain learning tasks, it seems plausible that machine performance could be enriched by aligning machine attention with human attention mechanisms -- yet research on this topic is sparse and has achieved only limited success. This paper proposes a new approach to address this gap, called Human-Machine Attention Learning (HuMAL). This approach involves reliance on data annotated by humans to reflect their self-perceived attention during specific tasks. We evaluate several alternative strategies for integrating such human attention data into machine learning (ML) algorithms, using a sentiment analysis task (review data from Yelp) and a personality-type classification task (data from myPersonality). The best-performing HuMAL strategy significantly enhances the task performance of fine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the benefit is particularly pronounced under challenging conditions of imbalanced or sparse labeled data. This research contributes to a deeper understanding of strategies for integrating human attention into ML models and highlights the potential of leveraging human cognition to augment ML in real-world applications.', 'abstract_zh': '以下是对您提供的论文内容或标题的翻译，符合学术规范：\n\n注意力机制，或是在处理信息时优先关注某些项目而非其他项目，是任何学习过程中的一个关键要素，对于人类和机器都一样。虽然人类在某些学习任务中仍然优于机器，但似乎可以通过使机器的注意力与人类的注意力机制相匹配来改进机器的表现——然而，关于这一领域的研究相对稀少，且成果有限。本文提出了一种新的方法，称为人类机器注意力学习（HuMAL），该方法依赖于人类标注的数据，以反映其在特定任务中的自我感知注意力。我们使用 Yelp 上的评论数据进行情感分析任务和来自 myPersonality 的数据进行性格类型分类任务，评估了几种将人类注意力数据整合到机器学习（ML）算法中的替代策略。最佳 HuMAL 策略显著提高了微调的变压器模型（BERT、GPT-2 和 XLNET）的任务表现，特别是在不平衡或标注数据稀疏的情况下，这种改进尤为明显。这项研究为将人类注意力整合到 ML 模型中的策略提供了更深入的理解，并突显了利用人类认知来增强 ML 在实际应用中的潜力。', 'title_zh': '将人类和机器注意力对齐以增强监督学习'}
{'arxiv_id': 'arXiv:2502.06809', 'title': 'Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution', 'authors': 'Muhammad Umair Haider, Hammad Rizwan, Hassan Sajjad, Peizhong Ju, A.B. Siddique', 'link': 'https://arxiv.org/abs/2502.06809', 'abstract': 'Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce NeuronLens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.', 'abstract_zh': '理解并控制大型语言模型（LLMs）的内部机制对于提高其可信度和实用性至关重要。近期的研究主要集中在通过建立神经元与语义概念之间的离散映射来识别和操控神经元。然而，这类映射难以处理LLMs固有的多义性问题，即单个神经元包含了多个不同的概念。这使得精确控制变得困难，并复杂化了下游干预措施。通过对多个文本分类数据集中的编码器和解码器型LLMs进行深入分析，我们发现尽管单个神经元包含了多个概念，但它们在各个概念上的激活强度表现出不同、类正态分布的模式。基于这一洞察，我们提出了NeuronLens，这是一种新的基于范围的解释和操控框架，它可以提供更精细的神经元激活分布视图，以在神经元内定位概念归因。广泛的实证评估表明，NeuronLens在减少意外干扰的同时，能够精确地操控目标概念，性能优于现有方法。', 'title_zh': '神经元以区间方式交流：突破离散神经元归因的束缚'}
{'arxiv_id': 'arXiv:2502.06807', 'title': 'Competitive Programming with Large Reasoning Models', 'authors': 'OpenAI, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, Wenda Zhou', 'link': 'https://arxiv.org/abs/2502.06807', 'abstract': 'We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models - OpenAI o1 and an early checkpoint of o3 - with a domain-specific system, o1-ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a Codeforces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.', 'abstract_zh': '我们展示了将强化学习应用于大规模语言模型（LLMs）显著提升了复杂编码和推理任务的性能。此外，我们比较了两个通用推理模型——OpenAI的o1和o3的早期检查点——与一个特定领域的系统o1-ioi，该系统利用为2024年国际信息学奥林匹克（IOI）设计的手工工程化推理策略。在2024年IOI的比赛中，我们使用手工制作的测试时策略，o1-ioi位居第49百分位。在较为宽松的竞赛约束条件下，o1-ioi获得了金牌。然而，在评估后续模型如o3时，我们发现o3在没有依靠手工制作的领域特定策略或宽松约束的情况下也能获得金牌。我们的研究表明，尽管专门化的流水线如o1-ioi能够带来显著改进，但规模更大的通用模型o3在推理领域超越这些结果。值得注意的是，o3在2024年IOI中取得了金牌，并且在Codeforces上的排名与顶级的人类竞争对手相当。总体而言，这些结果表明，通过扩大通用强化学习的应用而非依赖于领域特定技术，为推理领域，如编程竞赛，提供了通往先进AI的稳健途径。', 'title_zh': '大规模推理模型的竞赛编程'}
{'arxiv_id': 'arXiv:2502.06806', 'title': 'Logits are All We Need to Adapt Closed Models', 'authors': 'Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.06806', 'abstract': 'Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to \\emph{Plugin} model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.', 'abstract_zh': '许多商业大型语言模型（LLMs）通常是闭源的，限制了开发者仅通过提示调优来实现内容生成与特定应用的对齐。虽然当前这些模型并未提供对标记logits的访问，但我们认为，如果能够访问logits，将会使开发者能够采用超越提示工程的更强大的适应技术。在本文中，我们提出了一种标记级别概率重加权框架，该框架在获得logits和少量任务特定数据的情况下，能够有效引导黑盒LLMs生成特定应用的内容。我们的方法将下一个标记的预测视为监督分类问题。我们表明，将黑盒LLMs与任务特定数据进行对齐可以表述为标签噪声矫正问题，这导致了“插件”模型——一个仅基于logits进行自回归概率重加权的模型。我们提供了理论依据，证明仅仅重加权logits就足以适应任务需求。广泛的实验证明了我们方法的有效性，并倡导在闭源模型中更广泛地提供token logits的访问权限。', 'title_zh': '我们需要的是仅限于适应闭源模型的逻辑值'}
{'arxiv_id': 'arXiv:2502.06802', 'title': 'Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking', 'authors': 'Chen Wang, Xiaokai Wei, Yexi Jiang, Frank Ong, Kevin Gao, Xiao Yu, Zheng Hui, Se-eun Yoon, Philip Yu, Michelle Gong', 'link': 'https://arxiv.org/abs/2502.06802', 'abstract': 'With the vast and dynamic user-generated content on Roblox, creating effective game recommendations requires a deep understanding of game content. Traditional recommendation models struggle with the inconsistent and sparse nature of game text features such as titles and descriptions. Recent advancements in large language models (LLMs) offer opportunities to enhance recommendation systems by analyzing in-game text data. This paper addresses two challenges: generating high-quality, structured text features for games without extensive human annotation, and validating these features to ensure they improve recommendation relevance. We propose an approach that extracts in-game text and uses LLMs to infer attributes such as genre and gameplay objectives from raw player interactions. Additionally, we introduce an LLM-based re-ranking mechanism to assess the effectiveness of the generated text features, enhancing personalization and user satisfaction. Beyond recommendations, our approach supports applications such as user engagement-based integrity detection, already deployed in production. This scalable framework demonstrates the potential of in-game text understanding to improve recommendation quality on Roblox and adapt recommendations to its unique, user-generated ecosystem.', 'abstract_zh': '在Roblox平台上，海量且动态用户生成的内容使得有效的游戏推荐需要深入理解游戏内容。传统的推荐模型难以应对游戏文本特征（如标题和描述）的不一致性和稀疏性。近年来，在大规模语言模型（LLMs）方面的进步为通过分析游戏内文本数据提升推荐系统提供了机会。本文旨在解决两个挑战：生成无需大量人工注释的高质量、结构化文本特征，以及验证这些特征能否提高推荐的相关性。我们提出了一种方法，提取游戏内文本，并使用LLMs从原始玩家互动中推断属性（如游戏类型和 gameplay 目标）。此外，我们引入了一种基于LLM的重排序机制，以评估生成的文本特征的有效性，从而增强个性化并提升用户体验。除了推荐之外，我们的方法还支持如基于用户参与度的完整性检测等应用，已在生产环境中部署。本文提出的一种可扩展框架展示了游戏内文本理解在提高Roblox上的推荐质量以及适应其独特的用户生成生态系统方面的潜力。', 'title_zh': '解决 Roblox 游戏推荐中的内容缺口：基于大语言模型的角色生成与重新 ranking'}
