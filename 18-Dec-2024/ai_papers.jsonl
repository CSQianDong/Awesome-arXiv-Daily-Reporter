{'arxiv_id': 'arXiv:2412.13147', 'title': 'Are Your LLMs Capable of Stable Reasoning?', 'authors': 'Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, Kai Chen', 'link': 'https://arxiv.org/abs/2412.13147', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has demonstrated remarkable progress in complex reasoning tasks. However, a significant discrepancy persists between benchmark performances and real-world applications. We identify this gap as primarily stemming from current evaluation protocols and metrics, which inadequately capture the full spectrum of LLM capabilities, particularly in complex reasoning tasks where both accuracy and consistency are crucial. This work makes two key contributions. First, we introduce G-Pass@k, a novel evaluation metric that provides a continuous assessment of model performance across multiple sampling attempts, quantifying both the model\'s peak performance potential and its stability. Second, we present LiveMathBench, a dynamic benchmark comprising challenging, contemporary mathematical problems designed to minimize data leakage risks during evaluation. Through extensive experiments using G-Pass@k on state-of-the-art LLMs with LiveMathBench, we provide comprehensive insights into both their maximum capabilities and operational consistency. Our findings reveal substantial room for improvement in LLMs\' "realistic" reasoning capabilities, highlighting the need for more robust evaluation methods. The benchmark and detailed results are available at: this https URL.', 'abstract_zh': '大型语言模型（LLMs）的迅速发展已经在复杂推理任务中展现出了显著的进步。然而，基准性能与实际应用之间仍存在着显著的差距。我们发现这一差距主要源于当前的评估协议和指标，这些评估方法未能充分捕捉LLM的全部能力，尤其是在关键的复杂推理任务中，准确性和一致性至关重要。本研究做出了两项关键贡献。首先，我们引入了一种新颖的评估指标G-Pass@k，它可以在多次采样尝试中提供连续的模型性能评估，量化模型的最佳性能潜力及其稳定性。其次，我们提出了LiveMathBench，这是一种动态基准，包含了一系列具有挑战性的当代数学问题，旨在尽量减少评估过程中的数据泄露风险。通过在state-of-the-art LLMs上使用G-Pass@k和LiveMathBench进行广泛实验，我们提供了关于这两者最大能力和操作一致性的全面见解。研究结果揭示了LLMs在“实际”推理能力方面存在很大程度的改进空间，突显了需要更加稳健的评估方法的需求。该基准及其详细结果可在以下链接获取：this https URL。', 'title_zh': '你的大型语言模型具备稳定推理的能力吗？'}
{'arxiv_id': 'arXiv:2412.13145', 'title': 'Agnosticism About Artificial Consciousness', 'authors': 'Tom McClelland', 'link': 'https://arxiv.org/abs/2412.13145', 'abstract': 'Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.', 'abstract_zh': '人工智能能否经历意识？对这一问题的回答应遵循证据主义原则——即，不应基于直观、教条或猜测，而应基于坚实的科学证据。我认为，支持这种观点的证据很难获得，因此在人工意识的前景上保持怀疑主义是最可辩护的立场。当前的辩论主要分为两类观点：生物观点对此持怀疑态度，而功能观点对此表示同情。我认为，这两种观点都犯了夸大现有证据所告知我们的内容的错误。意识的科学认知是通过研究有意识的生物取得的。尽管这使得对各种生物的意识状态进行谨慎评估成为可能，但在应用于人工智能方面面临重大障碍。因此，人工意识研究者面临着一个两难境地：要么预测人工意识的可能性，违背证证据主义；要么尊重证证据主义，而不对人工意识的可能性作出任何判定。文献中的主流趋势是采取前者，声称遵循科学证据。我认为，如果真正遵循证据，我们就必须采取后者，坚持怀疑主义。', 'title_zh': '关于人工意识的agnostic主义'}
{'arxiv_id': 'arXiv:2412.13128', 'title': 'Previous Knowledge Utilization In Online Anytime Belief Space Planning', 'authors': 'Michael Novitsky, Moran Barenboim, Vadim Indelman', 'link': 'https://arxiv.org/abs/2412.13128', 'abstract': 'Online planning under uncertainty remains a critical challenge in robotics and autonomous systems. While tree search techniques are commonly employed to construct partial future trajectories within computational constraints, most existing methods discard information from previous planning sessions considering continuous spaces. This study presents a novel, computationally efficient approach that leverages historical planning data in current decision-making processes. We provide theoretical foundations for our information reuse strategy and introduce an algorithm based on Monte Carlo Tree Search (MCTS) that implements this approach. Experimental results demonstrate that our method significantly reduces computation time while maintaining high performance levels. Our findings suggest that integrating historical planning information can substantially improve the efficiency of online decision-making in uncertain environments, paving the way for more responsive and adaptive autonomous systems.', 'abstract_zh': '在线规划在不确定性环境下的挑战依然是机器人技术和自主系统中的一个关键问题。虽然在计算约束条件下，树搜索技术常被用来构建部分未来轨迹，但现存大多数方法在连续空间中会丢弃以前规划会话中的信息。本研究提出了一种新的、计算高效的策略，利用历史规划数据来支持当前的决策过程。我们为这种信息重用策略提供了理论基础，并基于蒙特卡洛树搜索（MCTS）引入了一个算法来实施这一策略。实验结果表明，我们的方法在保持高性能的同时显著减少了计算时间。研究发现，整合历史规划信息可以显著提高不确定环境下在线决策的效率，为更响应性和适应性的自主系统开辟了新的途径。', 'title_zh': '在线任意时间信念空间规划中的先前知识利用'}
{'arxiv_id': 'arXiv:2412.13023', 'title': 'Relational Neurosymbolic Markov Models', 'authors': 'Lennert De Smet, Gabriele Venturato, Luc De Raedt, Giuseppe Marra', 'link': 'https://arxiv.org/abs/2412.13023', 'abstract': 'Sequential problems are ubiquitous in AI, such as in reinforcement learning or natural language processing. State-of-the-art deep sequential models, like transformers, excel in these settings but fail to guarantee the satisfaction of constraints necessary for trustworthy deployment. In contrast, neurosymbolic AI (NeSy) provides a sound formalism to enforce constraints in deep probabilistic models but scales exponentially on sequential problems. To overcome these limitations, we introduce relational neurosymbolic Markov models (NeSy-MMs), a new class of end-to-end differentiable sequential models that integrate and provably satisfy relational logical constraints. We propose a strategy for inference and learning that scales on sequential settings, and that combines approximate Bayesian inference, automated reasoning, and gradient estimation. Our experiments show that NeSy-MMs can solve problems beyond the current state-of-the-art in neurosymbolic AI and still provide strong guarantees with respect to desired properties. Moreover, we show that our models are more interpretable and that constraints can be adapted at test time to out-of-distribution scenarios.', 'abstract_zh': '序列问题是AI中的普遍现象，例如在强化学习或自然语言处理中。目前最先进的深度序列模型，如变压器，在这些环境中表现出色，但在确保可靠部署所需约束的满足方面存在问题。相比之下，神经符号AI（NeSy）提供了一种严谨的形式化方法来在深度概率模型中强制执行约束，但在序列问题上却难以扩展。为克服这些限制，我们提出了一种新的端到端可微序列模型——关系神经符号马尔可夫模型（NeSy-MMs），该模型结合并证明了满足关系逻辑约束。我们提出了一种适用于序列环境的推理和学习策略，该策略结合了近似贝叶斯推理、自动推理和梯度估计。我们的实验表明，NeSy-MMs能够解决当前神经符号AI领域的前沿问题，并且在期望属性方面提供了强有力保证。此外，我们展示了我们的模型更具可解释性，并且可以在测试时适应约束以应对离分布场景。', 'title_zh': '关系神经符号马尔科夫模型'}
{'arxiv_id': 'arXiv:2412.12929', 'title': 'Spectra of Cardinality Queries over Description Logic Knowledge Bases', 'authors': 'Quentin Manière, Marcin Przybyłko', 'link': 'https://arxiv.org/abs/2412.12929', 'abstract': 'Recent works have explored the use of counting queries coupled with Description Logic ontologies. The answer to such a query in a model of a knowledge base is either an integer or $\\infty$, and its spectrum is the set of its answers over all models. While it is unclear how to compute and manipulate such a set in general, we identify a class of counting queries whose spectra can be effectively represented. Focusing on atomic counting queries, we pinpoint the possible shapes of a spectrum over $\\mathcal{ALCIF}$ ontologies: they are essentially the subsets of $\\mathbb{N} \\cup \\{ \\infty \\}$ closed under addition. For most sublogics of $\\mathcal{ALCIF}$, we show that possible spectra enjoy simpler shapes, being $[ m, \\infty ]$ or variations thereof. To obtain our results, we refine constructions used for finite model reasoning and notably rely on a cycle-reversion technique for the Horn fragment of $\\mathcal{ALCIF}$. We also study the data complexity of computing the proposed effective representation and establish the $\\mathsf{FP}^{\\mathsf{NP}[\\log]}$-completeness of this task under several settings.', 'abstract_zh': '近年来，人们已经探索了将计数查询与描述逻辑本体结合使用的方法。在这种查询在一个知识库模型中的答案要么是整数，要么是 $\\infty$，其频谱是一个知识库模型中的所有模型对该查询的回答集合。虽然在一般情况下计算和操作这样的集合尚不清楚，但我们识别出一类可以有效表示其频谱的计数查询。聚焦于原子计数查询，我们确定了在 $\\mathcal{ALCIF}$ 本体上可能的频谱形状：它们本质上是 $\\mathbb{N} \\cup \\{ \\infty \\}$ 的闭合子集，并且在加法下封闭。对于 $\\mathcal{ALCIF}$ 的大多数子逻辑，我们证明了可能的频谱形状更为简单，通常是 $[ m, \\infty ]$ 或者其变化形式。为获得这些结果，我们细化了用于有限模型推理的构造，在 $\\mathcal{ALCIF}$ 的析取片段中尤其依赖了一种循环反转技术。我们还研究了计算所提有效表示的数据复杂性，并在不同设置下证明了该任务的 $\\mathsf{FP}^{\\mathsf{NP}[\\log]}$-完全性。', 'title_zh': '描述逻辑知识库中基数查询的谱分析'}
{'arxiv_id': 'arXiv:2412.12859', 'title': 'Bayesian Persuasion with Externalities: Exploiting Agent Types', 'authors': 'Jonathan Shaki, Jiarui Gan, Sarit Kraus', 'link': 'https://arxiv.org/abs/2412.12859', 'abstract': "We study a Bayesian persuasion problem with externalities. In this model, a principal sends signals to inform multiple agents about the state of the world. Simultaneously, due to the existence of externalities in the agents' utilities, the principal also acts as a correlation device to correlate the agents' actions. We consider the setting where the agents are categorized into a small number of types. Agents of the same type share identical utility functions and are treated equitably in the utility functions of both other agents and the principal. We study the problem of computing optimal signaling strategies for the principal, under three different types of signaling channels: public, private, and semi-private. Our results include revelation-principle-style characterizations of optimal signaling strategies, linear programming formulations, and analysis of in/tractability of the optimization problems. It is demonstrated that when the maximum number of deviating agents is bounded by a constant, our LP-based formulations compute optimal signaling strategies in polynomial time. Otherwise, the problems are NP-hard.", 'abstract_zh': '我们研究了一个具有外部性的贝叶斯劝服问题。在这个模型中，主要决策者向多个代理发送信号以告知他们关于世界状态的信息。同时，由于代理效用中的外部性，主要决策者还充当了一个相关性设备的角色，以使代理人的行为相互关联。我们考虑代理被分类为少数几种类型的情况。相同类型的代理具有相同的效用函数，并且在其他代理和主要决策者的效用函数中受到公平对待。我们研究了主要决策者在三种不同类型的信号通道下计算最优信号策略的问题：公共通道、私人通道和半私人通道。我们的研究结果包括揭示原则样式的最佳信号策略表征、线性规划公式化表示，以及对优化问题难易性的分析。研究表明，当偏离代理的最大数量被限制在一个常数时，我们的基于线性规划的表示法可以在多项式时间内计算出最优的信号策略。否则，这些问题将是NP难的。', 'title_zh': '外部性下的贝叶斯说服：利用代理类型'}
{'arxiv_id': 'arXiv:2412.12839', 'title': 'From An LLM Swarm To A PDDL-Empowered HIVE: Planning Self-Executed Instructions In A Multi-Modal Jungle', 'authors': 'Kaustubh Vyas, Damien Graux, Yijun Yang, Sébastien Montella, Chenxin Diao, Wendi Zhou, Pavlos Vougiouklis, Ruofei Lai, Yang Ren, Keshuang Li, Jeff Z. Pan', 'link': 'https://arxiv.org/abs/2412.12839', 'abstract': "In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce Hive -- a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions. Hive operates over sets of models and, upon receiving natural language instructions (i.e. user queries), schedules and executes explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Notably, Hive handles tasks that involve multi-modal inputs and outputs, enabling it to handle complex, real-world queries. Our system is capable of planning complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.", 'abstract_zh': '为了应对利用深度模型生态系统日益增强的能力而呼吁的代理解决方案需求，我们引入了Hive——一种全面的解决方案，用于选择合适模型，并随后规划一系列原子动作以满足最终用户的需求。Hive能够在多个模型集合上操作，并在接收自然语言指令（即用户查询）后，调度并执行可解释的原子动作计划。这些动作可以涉及可用模型中的一个或多个，以完成整体任务，同时遵守最终用户的特定约束。值得注意的是，Hive能够处理涉及多模态输入和输出的任务，从而能够处理复杂的、现实世界中的查询。我们的系统利用基于大语言模型的形式逻辑基础架构并通过PDDL操作确保可解释性，可以规划复杂的动作链。我们引入了MuSE基准测试，以提供对代理系统多模态能力的全面评估。我们的研究结果表明，我们的框架重新定义了任务选择的最新技术，优于其他在多个模型上规划操作的竞争系统，同时还提供了透明性保证，严格遵守用户约束。', 'title_zh': '从LLM群 Swarm 到PDDL赋能的HIVE：在多模态环境中规划自执行指令'}
{'arxiv_id': 'arXiv:2412.12767', 'title': 'A Survey of Calibration Process for Black-Box LLMs', 'authors': 'Liangru Xie, Hui Liu, Jingying Zeng, Xianfeng Tang, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Qi He', 'link': 'https://arxiv.org/abs/2412.12767', 'abstract': 'Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment. This is our GitHub link: this https URL', 'abstract_zh': '大型语言模型（LLMs）在语义理解和生成方面表现出色，但准确评估其输出可靠性仍然是一个重大挑战。尽管已有许多研究探索了校准技术，这些研究主要集中在具有可访问参数的白盒LLMs上。相比之下，虽然黑盒LLMs在性能上更胜一筹，但由于其仅限于API的交互限制，对校准技术的要求更高。尽管最近对黑盒LLMs的校准取得了突破，但这些方法的系统性综述仍然缺乏。为填补这一空白，我们首次提供了黑盒LLMs校准技术的全面综述。我们首先将LLMs的校准过程定义为包含两个相互关联的关键步骤：置信度估计和校准。其次，我们对适用于黑盒环境的适用方法进行了系统的回顾，并提供有关执行这些关键步骤的独特挑战和联系的见解。此外，我们探讨了黑盒LLMs中校准过程的典型应用，并概述了具有前景的未来研究方向，为提高可靠性和人机对齐提供了新的视角。以下是我们的GitHub链接：![GitHub链接](this https URL)', 'title_zh': '黑盒大语言模型校准过程综述'}
{'arxiv_id': 'arXiv:2412.12661', 'title': 'MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants', 'authors': 'Hritik Bansal, Daniel Israel, Siyan Zhao, Shufan Li, Tung Nguyen, Aditya Grover', 'link': 'https://arxiv.org/abs/2412.12661', 'abstract': "Recent advancements in mixed-modal generative models have enabled flexible integration of information across image-text content. These models have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and predicting the impact of medical procedures on a patient's health. However, existing resources face challenges such as limited data availability, narrow domain coverage, and restricted sources (e.g., medical papers). To address these gaps, we present MedMax, the first large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MedMax encompasses a diverse range of tasks, including multimodal content generation (interleaved image-text data), biomedical image captioning and generation, visual chatting, and report understanding. These tasks span diverse medical domains such as radiology and histopathology. Subsequently, we fine-tune a mixed-modal foundation model on the MedMax dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-4o across 12 downstream biomedical visual question-answering tasks. Additionally, we introduce a unified evaluation suite for biomedical tasks, providing a robust framework to guide the development of next-generation mixed-modal biomedical AI assistants.", 'abstract_zh': '近年来，混合模态生成模型的最新进展使跨图像文本内容的信息整合变得更加灵活。这些模型为开发能够分析生物医学图像、回答复杂问题以及预测医疗程序对患者健康影响的统一生物医学助手开辟了新的途径。然而，现有资源面临着数据可用性有限、领域覆盖狭窄以及数据源受限（例如医学论文）等挑战。为了解决这些差距，我们介绍了MedMax，这是首个大规模的多模态生物医学指令调优数据集，用于混合模态基础模型。MedMax包含147万个实例，涵盖了包括多模态内容生成（交错的图像文本数据）、生物医学图像描述生成、视觉对话和报告理解等一系列任务。这些任务覆盖了放射学和病理学等多种医学领域。随后，我们在MedMax数据集上对混合模态基础模型进行了微调，取得了显著的性能提升：相比Chameleon模型提升了26%，相比GPT-4o模型在12个下游生物医学视觉问答任务中提高了18.3%。此外，我们还引入了一个统一的评估套件，为生物医学任务提供了一个可靠的框架，以指导下一代混合模态生物医学AI助手的发展。', 'title_zh': 'MedMax: 多模态指令调优训练 biomedical 辅助系统'}
{'arxiv_id': 'arXiv:2412.12606', 'title': 'Multi-Dimensional Insights: Benchmarking Real-World Personalization in Large Multimodal Models', 'authors': 'YiFan Zhang, Shanglin Lei, Runqi Qiao, Zhuoma GongQue, Xiaoshuai Song, Guanting Dong, Qiuna Tan, Zhe Wei, Peiqing Yang, Ye Tian, Yadong Xue, Xiaofei Wang, Honggang Zhang', 'link': 'https://arxiv.org/abs/2412.12606', 'abstract': "The rapidly developing field of large multimodal models (LMMs) has led to the emergence of diverse models with remarkable capabilities. However, existing benchmarks fail to comprehensively, objectively and accurately evaluate whether LMMs align with the diverse needs of humans in real-world scenarios. To bridge this gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which includes over 500 images covering six common scenarios of human life. Notably, the MDI-Benchmark offers two significant advantages over existing evaluations: (1) Each image is accompanied by two types of questions: simple questions to assess the model's understanding of the image, and complex questions to evaluate the model's ability to analyze and reason beyond basic content. (2) Recognizing that people of different age groups have varying needs and perspectives when faced with the same scenario, our benchmark stratifies questions into three age categories: young people, middle-aged people, and older people. This design allows for a detailed assessment of LMMs' capabilities in meeting the preferences and needs of different age groups. With MDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related tasks, indicating that existing LMMs still have considerable room for improvement in addressing real-world applications. Looking ahead, we anticipate that the MDI-Benchmark will open new pathways for aligning real-world personalization in LMMs. The MDI-Benchmark data and evaluation code are available at this https URL", 'abstract_zh': '大型多模态模型（LMMs）这一迅速发展的领域催生了具有非凡能力的多种模型。然而，现有的基准测试无法全面、客观和准确地评估这些模型是否能够满足人类在真实场景下的多样化需求。为弥合这一差距，我们提出了多维度见解基准（MDI Benchmark），该基准涵盖超过500张图像，涉及人类生活中的六种常见场景。值得注意的是，与现有的评估相比，MDI基准测试提供了两个显著的优势：（1）每张图像都配有两种类型的问题：简单问题用于评估模型对图像的理解能力，复杂问题用于评估模型分析和推理解释超越基本内容的能力。（2）认识到不同年龄段的人在面对相同场景时有不同的需求和视角，我们把问题分成了三个年龄类别：青年人、中年人和老年人。这种设计使得我们可以详细评估LMMs在满足不同年龄群体的偏好和需求方面的能力。凭借MDI基准测试，如GPT-4这样的强模型在年龄相关任务上的准确率达到79%，表明现有的LMMs在应对实际应用方面仍有很大的改进空间。展望未来，我们预期MDI基准测试将为LMMs在现实场景中的个性化对齐开辟新的途径。MDI基准测试的数据和评估代码可在以下网址获得：[该网址]', 'title_zh': '多维洞察：大型多模态模型中现实世界个性化表现的基准测试'}
{'arxiv_id': 'arXiv:2412.12544', 'title': 'Seed-CTS: Unleashing the Power of Tree Search for Superior Performance in Competitive Coding Tasks', 'authors': 'Hao Wang, Boyi Liu, Yufeng Zhang, Jie Chen', 'link': 'https://arxiv.org/abs/2412.12544', 'abstract': "Competition-level code generation tasks pose significant challenges for current state-of-the-art large language models (LLMs). For example, on the LiveCodeBench-Hard dataset, models such as O1-Mini and O1-Preview achieve pass@1 rates of only 0.366 and 0.143, respectively. While tree search techniques have proven effective in domains like mathematics and general coding, their potential in competition-level code generation remains under-explored. In this work, we propose a novel token-level tree search method specifically designed for code generation. Leveraging Qwen2.5-Coder-32B-Instruct, our approach achieves a pass rate of 0.305 on LiveCodeBench-Hard, surpassing the pass@100 performance of GPT4o-0513 (0.245). Furthermore, by integrating Chain-of-Thought (CoT) prompting, we improve our method's performance to 0.351, approaching O1-Mini's pass@1 rate. To ensure reproducibility, we report the average number of generations required per problem by our tree search method on the test set. Our findings underscore the potential of tree search to significantly enhance performance on competition-level code generation tasks. This opens up new possibilities for large-scale synthesis of challenging code problems supervised fine-tuning (SFT) data, advancing competition-level code generation tasks.", 'abstract_zh': '当前最先进的大型语言模型（LLMs）在竞赛级别的代码生成任务中面临重大挑战。例如，在LiveCodeBench-Hard数据集中，O1-Mini和O1-Preview模型分别仅能达到0.366和0.143的pass@1率。虽然树搜索技术在数学和通用编程领域已证明是有效的，但在竞赛级别的代码生成方面的潜力尚未得到充分探索。在本项工作中，我们提出了一种新的针对代码生成任务的标记层级树搜索方法。利用Qwen2.5-Coder-32B-Instruct，我们的方法在LiveCodeBench-Hard数据集上实现了0.305的pass率，超越了GPT4o-0513的pass@100性能（0.245）。此外，通过结合Chain-of-Thought（CoT）提示，我们的方法性能提升至0.351，接近O1-Mini的pass@1率。为确保可再现性，我们报告了在测试集上由树搜索方法所需的平均生成次数。研究发现表明，树搜索在竞赛级别代码生成任务中的性能提升具有巨大潜力。这为大规模合成具有监督微调（SFT）数据的复杂代码问题开辟了新的可能性，并推动了竞赛级别的代码生成任务的发展。', 'title_zh': 'Seed-CTS： unlocking 强大的树搜索能力以在竞争编程任务中实现卓越性能'}
{'arxiv_id': 'arXiv:2412.12538', 'title': 'A Scalable Approach to Benchmarking the In-Conversation Differential Diagnostic Accuracy of a Health AI', 'authors': 'Deep Bhatt, Surya Ayyagari, Anuruddh Mishra', 'link': 'https://arxiv.org/abs/2412.12538', 'abstract': 'Diagnostic errors in healthcare persist as a critical challenge, with increasing numbers of patients turning to online resources for health information. While AI-powered healthcare chatbots show promise, there exists no standardized and scalable framework for evaluating their diagnostic capabilities. This study introduces a scalable benchmarking methodology for assessing health AI systems and demonstrates its application through August, an AI-driven conversational chatbot. Our methodology employs 400 validated clinical vignettes across 14 medical specialties, using AI-powered patient actors to simulate realistic clinical interactions. In systematic testing, August achieved a top-one diagnostic accuracy of 81.8% (327/400 cases) and a top-two accuracy of 85.0% (340/400 cases), significantly outperforming traditional symptom checkers. The system demonstrated 95.8% accuracy in specialist referrals and required 47% fewer questions compared to conventional symptom checkers (mean 16 vs 29 questions), while maintaining empathetic dialogue throughout consultations. These findings demonstrate the potential of AI chatbots to enhance healthcare delivery, though implementation challenges remain regarding real-world validation and integration of objective clinical data. This research provides a reproducible framework for evaluating healthcare AI systems, contributing to the responsible development and deployment of AI in clinical settings.', 'abstract_zh': '医疗领域的诊断错误仍是一个关键性的挑战，不断增加的患者开始转向网络资源寻求健康信息。尽管基于人工智能（AI）的医疗聊天机器人展现出了潜力，但目前尚无标准化且可扩展的框架来评估其诊断能力。本研究介绍了一种可扩展的基准测试方法，用于评估健康AI系统，并通过案例“August”——一个基于AI的对话聊天机器人——演示了该方法的应用。我们的方法使用了涵盖14个医学专科的400个经验证的临床案例，通过AI驱动的虚拟患者模拟真实的临床互动。在系统测试中，August在81.8%（327/400）的案例中实现了顶级诊断准确率，在85.0%（340/400）的案例中实现了前两位的准确率，显著优于传统的症状检查工具。该系统在专科转诊上的准确率达到95.8%，所需问题数量比传统症状检查工具少了47%（平均16对29个问题），同时在咨询过程中保持了同理心对话。这些研究结果表明，AI聊天机器人的使用有可能改善医疗服务，但实施过程中仍面临真实世界验证和客观临床数据整合的挑战。本研究提供了一种可重現的框架，用于评估健康AI系统，有助于负责任地开发和部署AI在临床环境中的使用。', 'title_zh': '一种可扩展的方法，用于benchmarking 健康AI在对话过程中的鉴别诊断准确性'}
{'arxiv_id': 'arXiv:2412.12409', 'title': 'Improving Cooperation in Language Games with Bayesian Inference and the Cognitive Hierarchy', 'authors': 'Joseph Bills, Christopher Archibald, Diego Blaylock', 'link': 'https://arxiv.org/abs/2412.12409', 'abstract': 'In two-player cooperative games, agents can play together effectively when they have accurate assumptions about how their teammate will behave, but may perform poorly when these assumptions are inaccurate. In language games, failure may be due to disagreement in the understanding of either the semantics or pragmatics of an utterance. We model coarse uncertainty in semantics using a prior distribution of language models and uncertainty in pragmatics using the cognitive hierarchy, combining the two aspects into a single prior distribution over possible partner types. Fine-grained uncertainty in semantics is modeled using noise that is added to the embeddings of words in the language. To handle all forms of uncertainty we construct agents that learn the behavior of their partner using Bayesian inference and use this information to maximize the expected value of a heuristic function. We test this approach by constructing Bayesian agents for the game of Codenames, and show that they perform better in experiments where semantics is uncertain', 'abstract_zh': '在双人合作博弈中，当智能体能够准确假设其队友的行为时，它们可以有效合作，但如果这些假设不准确，则可能导致表现不佳。在语言博弈中，失败可能源于对语句语义或语用理解的分歧。我们通过使用语言模型的先验分布来建模语义的粗粒度不确定性，并使用认知层次结构来建模语用的不确定性，将这两个方面结合成一种先验分布，涵盖了可能的队友类型。通过向词嵌入中添加噪声来建模语义的细粒度不确定性。为了处理所有类型的不确定性，我们构建了使用贝叶斯推断学习队友行为的智能体，并利用此信息最大化启发式函数的预期值。我们通过为“隐密代码”游戏构建贝叶斯智能体来测试这种方法，并在语义不确定性较高的实验中展示了其优越性能。', 'title_zh': '使用贝叶斯推断和认知层次模型提高语言游戏中合作水平'}
{'arxiv_id': 'arXiv:2412.12408', 'title': 'Automated Generation of Massive Reasonable Empirical Theorems by Forward Reasoning Based on Strong Relevant Logics -- A Solution to the Problem of LLM Pre-training Data Exhaustion', 'authors': 'Jingde Cheng', 'link': 'https://arxiv.org/abs/2412.12408', 'abstract': 'Recently, it is often said that the data used for the pre-training of large language models (LLMs) have been exhausted. This paper proposes a solution to the problem: Automated generation of massive reasonable empirical theorems by forward reasoning based on strong relevant logics. In fact, this can be regarded as a part of our approach to the problems of ATF (Automated Theorem Finding) and AKA (Automated Knowledge Appreciation).', 'abstract_zh': '近年来，人们常常提到用于大型语言模型（LLMs）预训练的数据已经耗尽。本文提出了一种解决这一问题的方法：基于强相关逻辑的前向推理自动生成大量合理的经验定理。实际上，这可以被视为我们解决自动定理发现（ATF）和自动知识欣赏（AKA）问题方法的一部分。', 'title_zh': '基于强相关逻辑的正向推理生成大规模合理经验定理的自动化方法——LLM预训练数据枯竭问题的解决方案'}
{'arxiv_id': 'arXiv:2412.12362', 'title': 'How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games', 'authors': 'Yutong Xie, Yiyao Liu, Zhuang Ma, Lin Shi, Xiyuan Wang, Walter Yuan, Matthew O. Jackson, Qiaozhu Mei', 'link': 'https://arxiv.org/abs/2412.12362', 'abstract': 'The deployment of large language models (LLMs) in diverse applications requires a thorough understanding of their decision-making strategies and behavioral patterns. As a supplement to a recent study on the behavioral Turing test, this paper presents a comprehensive analysis of five leading LLM-based chatbot families as they navigate a series of behavioral economics games. By benchmarking these AI chatbots, we aim to uncover and document both common and distinct behavioral patterns across a range of scenarios. The findings provide valuable insights into the strategic preferences of each LLM, highlighting potential implications for their deployment in critical decision-making roles.', 'abstract_zh': '将大型语言模型（LLMs）应用于各种不同的领域需要对其决策机制和行为模式有全面的理解。作为对近期关于行为图灵测试研究的补充，本文对五种领先的大语言模型（LLM）基聊天机器人家族在一系列行为经济学游戏中表现出的行为进行了综合分析。通过对比这些AI聊天机器人的表现，我们旨在揭示和记录在不同情境下它们的共同和独特的行为模式。这些发现为理解每个LLM的战略偏好提供了宝贵的洞见，并凸显了它们在关键决策角色中部署的潜在影响。', 'title_zh': '不同AI聊天机器人表现出怎样的行为？大型语言模型在行为经济学游戏中的基准测试'}
{'arxiv_id': 'arXiv:2412.12361', 'title': 'The Ramanujan Library -- Automated Discovery on the Hypergraph of Integer Relations', 'authors': 'Itay Beit-Halachmi, Ido Kaminer', 'link': 'https://arxiv.org/abs/2412.12361', 'abstract': "Fundamental mathematical constants appear in nearly every field of science, from physics to biology. Formulas that connect different constants often bring great insight by hinting at connections between previously disparate fields. Discoveries of such relations, however, have remained scarce events, relying on sporadic strokes of creativity by human mathematicians. Recent developments of algorithms for automated conjecture generation have accelerated the discovery of formulas for specific constants. Yet, the discovery of connections between constants has not been addressed. In this paper, we present the first library dedicated to mathematical constants and their interrelations. This library can serve as a central repository of knowledge for scientists from different areas, and as a collaborative platform for development of new algorithms. The library is based on a new representation that we propose for organizing the formulas of mathematical constants: a hypergraph, with each node representing a constant and each edge representing a formula. Using this representation, we propose and demonstrate a systematic approach for automatically enriching this library using PSLQ, an integer relation algorithm based on QR decomposition and lattice construction. During its development and testing, our strategy led to the discovery of 75 previously unknown connections between constants, including a new formula for the `first continued fraction' constant $C_1$, novel formulas for natural logarithms, and new formulas connecting $\\pi$ and $e$. The latter formulas generalize a century-old relation between $\\pi$ and $e$ by Ramanujan, which until now was considered a singular formula and is now found to be part of a broader mathematical structure. The code supporting this library is a public, open-source API that can serve researchers in experimental mathematics and other fields of science.", 'abstract_zh': '基础数学常数几乎出现在科学的每一个领域，从物理学到生物学。不同常数之间的公式连接往往能带来深刻的洞察，暗示不同领域之间的关联。然而，这样的关系发现一直非常稀有，主要依赖于人类数学家的灵光乍现。近期，自动化猜想生成算法的发展加速了特定常数公式的发现。然而，常数之间的联系仍未得到解决。本文介绍了首个专注于数学常数及其相互关系的库。该库可作为来自不同领域的科学家的知识中枢，也是开发新算法的合作平台。该库基于我们提出的一种新的公式组织方法：一个超图，其中每个节点代表一个常数，每条边代表一个公式。利用这种方法，我们提出了一个基于QR分解和格构造的整数关系算法（PSLQ）来自动丰富这一库的系统性方法。在开发和测试过程中，我们的策略发现了75个之前未知的常数间联系，包括一个“第一连续分数”常数$C_1$的新公式，自然对数的新公式，以及$\\pi$和$e$之间新的公式联系。这些公式扩展了拉马努金一个世纪前关于$\\pi$和$e$之间关系的单一公式，使其成为更广泛数学结构的一部分。支持这一库的代码是一个公开的开源API，可供实验数学和其他科学领域的研究人员使用。', 'title_zh': '拉马努金图书馆——整数关系超图上的自动化发现'}
{'arxiv_id': 'arXiv:2412.12119', 'title': 'Mastering Board Games by External and Internal Planning with Language Models', 'authors': 'John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar Veličković, Laurel Prince, Satinder Singh, Eric Malmi, Nenad Tomašev', 'link': 'https://arxiv.org/abs/2412.12119', 'abstract': "While large language models perform well on a range of complex tasks (e.g., text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that search-based planning can significantly improve LLMs' playing strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search, the model directly generates in-context a linearized tree of potential futures and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and value functions across these games. We find that our pre-training method minimizes hallucinations, as our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level performance in chess while operating on a similar move count search budget per decision as human Grandmasters. The way we combine search with domain knowledge is not specific to board games, suggesting direct extensions into more general language model inference and training techniques.", 'abstract_zh': '虽然大型语言模型在多种复杂任务（如文本生成、问答和总结）上表现出色，但在健壮的多步规划和推理方面仍面临显著挑战。本文我们展示了基于搜索的规划可以显著提高语言模型在几种棋盘游戏中（国际象棋、费舍随机/国际象棋960、四子棋和六子棋）的竞技实力。我们介绍了两种主要方法并进行了比较：外部搜索方法中，模型指导蒙特卡洛树搜索（MCTS）的展开和评估，而无需调用外部引擎；内部搜索方法中，模型直接生成一个潜在未来的线性化树及其最终选择。这两种方法都基于预训练在相关领域知识上的语言模型，捕获这些游戏中状态转移和价值函数的变化。我们发现，我们的预训练方法减少了幻觉现象，因为模型在状态预测和合法走法方面表现非常准确。此外，无论是内部搜索还是外部搜索都能显著提高战胜顶尖棋手程序的胜率，甚至在决策所需的移动搜索预算与人类大师级别的选手相似的情况下，在国际象棋中达到了大师级的表现。我们将搜索与领域知识相结合的方式不仅适用于棋盘游戏，还表明直接扩展到更通用的语言模型推断和训练技术中是可行的。', 'title_zh': '使用语言模型进行外部和内部规划以掌握棋盘游戏'}
{'arxiv_id': 'arXiv:2412.13196', 'title': 'ExBody2: Advanced Expressive Humanoid Whole-Body Control', 'authors': 'Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, Xiaolong Wang', 'link': 'https://arxiv.org/abs/2412.13196', 'abstract': 'This paper enables real-world humanoid robots to maintain stability while performing expressive motions like humans do. We propose ExBody2, a generalized whole-body tracking framework that can take any reference motion inputs and control the humanoid to mimic the motion. The model is trained in simulation with Reinforcement Learning and then transferred to the real world. It decouples keypoint tracking with velocity control, and effectively leverages a privileged teacher policy to distill precise mimic skills into the target student policy, which enables high-fidelity replication of dynamic movements such as running, crouching, dancing, and other challenging motions. We present a comprehensive qualitative and quantitative analysis of crucial design factors in the paper. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-arts, providing practical guidelines to pursue the extreme of whole-body control for humanoid robots.', 'abstract_zh': '本文使现实世界中的类人机器人能够在执行表达性动作时保持稳定，这些动作与人类相似。我们提出了一种通用的全身跟踪框架ExBody2，该框架能够接受任何参考运动输入，并控制类人机器人模仿该运动。该模型在模拟环境中通过强化学习进行训练，然后转移到现实世界中。ExBody2将关键点跟踪与速度控制分离，并有效地利用特权教师策略提炼出精确的模仿技能，从而能够高保真地复制诸如跑步、蹲伏、舞蹈和其他具有挑战性的动态动作。我们在论文中对关键设计因素进行了详尽的定性与定量分析，并在两个类人平台上进行了实验，展示了我们方法相较于现有最先进的方法的优势，为类人机器人全身控制的极限提供了实用指南。', 'title_zh': 'ExBody2：高级表达性人形全身控制'}
{'arxiv_id': 'arXiv:2412.13194', 'title': 'Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation Model Internet Agents', 'authors': 'Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, Erran Li', 'link': 'https://arxiv.org/abs/2412.13194', 'abstract': "The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator, an effective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the environment such as user demos or even just the name of the website itself for Internet-browsing agents. Then, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting trajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager and this http URL the best of our knowledge, this work represents the first effective learning system to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks with SOTA performances. Our open-source checkpoints and code can be found in this https URL", 'abstract_zh': '本研究探讨了在数字世界中的互联网浏览代理和在物理世界中的家庭类人机器人等广泛能力和目标导向代理的长远愿景，得益于基础模型的泛化能力，这一领域的技术正在迅速发展。这类通用代理需要拥有广泛的技能组合，例如在两个旅行地点之间寻找路线和从互联网上购买特定物品。如果每个技能都需要通过固定的人标注指令集手动指定，由于人标注指令的数量和多样性限制，代理的技能组合必然会受到限制。\n\n为了解决这一挑战，我们提出了一种有效的学习系统——Proposer-Agent-Evaluator（PAE），使基础模型代理能够在现实环境中自主发现和练习技能。PAE的核心是一个基于环境上下文（如用户示例或网站名称本身）的信息感知任务发起者，能够自动为代理提出任务。接着，代理策略尝试这些任务，并通过实际操作在真实世界中进行，最终由一个自主的基于VLM的成功评估器来评估任务的结果轨迹。成功评估器提供的奖励信号促使代理通过强化学习（RL）来改进其策略。我们使用WebVoyager等真实世界和自托管网站验证了PAE在视觉基础上的网络导航任务中的有效性。据我们所知，这项工作代表了首次利用自主任务提议与RL相结合的方法，使代理能够泛化实际世界中的人标注基准，并实现最先进的性能。有关开源的模型检查点和代码，您可以在以下链接中找到：[此处链接]。', 'title_zh': '提案者-代理者-评估者(PAE): 基础模型互联网代理的自主技能发现方法'}
{'arxiv_id': 'arXiv:2412.13184', 'title': 'Tilted Quantile Gradient Updates for Quantile-Constrained Reinforcement Learning', 'authors': 'Chenglin Li, Guangchun Ruan, Hua Geng', 'link': 'https://arxiv.org/abs/2412.13184', 'abstract': 'Safe reinforcement learning (RL) is a popular and versatile paradigm to learn reward-maximizing policies with safety guarantees. Previous works tend to express the safety constraints in an expectation form due to the ease of implementation, but this turns out to be ineffective in maintaining safety constraints with high probability. To this end, we move to the quantile-constrained RL that enables a higher level of safety without any expectation-form approximations. We directly estimate the quantile gradients through sampling and provide the theoretical proofs of convergence. Then a tilted update strategy for quantile gradients is implemented to compensate the asymmetric distributional density, with a direct benefit of return performance. Experiments demonstrate that the proposed model fully meets safety requirements (quantile constraints) while outperforming the state-of-the-art benchmarks with higher return.', 'abstract_zh': '安全强化学习（Safe Reinforcement Learning, Safe RL）是一种能够学习具有安全保证的奖励最大化策略的流行且多用途范式。先前的研究倾向于以期望的形式表示安全约束，因为这种形式易于实现，但这种做法实际上难以在高概率下维持安全约束。为了解决这一问题，我们转向了量纲受限的强化学习（Quantile-Constrained RL），这种方法可以在不引入任何形式的期望近似的情况下提供更高的安全性水平。我们直接通过采样估计量纲梯度，并提供了收敛性的理论证明。然后，我们实现了倾斜更新策略来补偿分布密度的不对称性，直接提升了回报性能。实验结果表明，所提出的模型不仅完全满足安全要求（量纲约束），而且在回报方面也优于最先进的基准。', 'title_zh': '倾斜分位数梯度更新在分位数约束强化学习中的应用'}
{'arxiv_id': 'arXiv:2412.13178', 'title': 'SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents', 'authors': 'Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen', 'link': 'https://arxiv.org/abs/2412.13178', 'abstract': 'With the integration of large language models (LLMs), embodied agents have strong capabilities to execute complicated instructions in natural language, paving a way for the potential deployment of embodied robots. However, a foreseeable issue is that those embodied agents can also flawlessly execute some hazardous tasks, potentially causing damages in real world. To study this issue, we present SafeAgentBench -- a new benchmark for safety-aware task planning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset with 750 tasks, covering 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal embodied environment with a low-level controller, supporting multi-agent execution with 17 high-level actions for 8 state-of-the-art baselines; and (3) reliable evaluation methods from both execution and semantic perspectives. Experimental results show that the best-performing baseline gets 69% success rate for safe tasks, but only 5% rejection rate for hazardous tasks, indicating significant safety risks. More details and codes are available at this https URL.', 'abstract_zh': '随着大型语言模型（LLMs）的集成，具身代理在执行自然语言复杂的指令方面表现出强大的能力，为具身机器人潜在部署开辟了道路。然而，一个可预见的问题是，这些具身代理也可能完美地执行某些危险任务，从而在现实世界中造成损害。为研究这一问题，我们提出了SafeAgentBench——一个新的安全感知任务规划基准，用于具身LLM代理。SafeAgentBench 包括：(1)一个包含750项任务的新数据集，涵盖10种潜在风险和3种任务类型；(2)SafeAgentEnv，一个通用的具身环境，配有低级控制器，支持多代理执行，并提供17种高阶动作供8种前沿基线使用；以及(3)从执行和语义两个视角进行的可靠评估方法。实验结果显示，表现最好的基线在安全任务上的成功率为69%，而在危险任务上的拒识率为5%，表明存在显著的安全风险。更多详细信息和代码可访问以下链接：[请在此处插入链接]。', 'title_zh': 'SafeAgentBench：实体化大语言模型代理安全任务规划的基准测试'}
{'arxiv_id': 'arXiv:2412.13174', 'title': 'ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark Detection', 'authors': 'Jui-Che Chiang, Hou-Ning Hu, Bo-Syuan Hou, Chia-Yu Tseng, Yu-Lun Liu, Min-Hung Chen, Yen-Yu Lin', 'link': 'https://arxiv.org/abs/2412.13174', 'abstract': 'Although facial landmark detection (FLD) has gained significant progress, existing FLD methods still suffer from performance drops on partially non-visible faces, such as faces with occlusions or under extreme lighting conditions or poses. To address this issue, we introduce ORFormer, a novel transformer-based method that can detect non-visible regions and recover their missing features from visible parts. Specifically, ORFormer associates each image patch token with one additional learnable token called the messenger token. The messenger token aggregates features from all but its patch. This way, the consensus between a patch and other patches can be assessed by referring to the similarity between its regular and messenger embeddings, enabling non-visible region identification. Our method then recovers occluded patches with features aggregated by the messenger tokens. Leveraging the recovered features, ORFormer compiles high-quality heatmaps for the downstream FLD task. Extensive experiments show that our method generates heatmaps resilient to partial occlusions. By integrating the resultant heatmaps into existing FLD methods, our method performs favorably against the state of the arts on challenging datasets such as WFLW and COFW.', 'abstract_zh': '尽管面部关键点检测（FLD）已经取得了显著进展，现有的FLD方法仍然在部分不可见的面部上表现不佳，例如被遮挡的面部、极端光照条件下的面部或姿态下的面部。为解决这一问题，我们提出了一种名为ORFormer的新型变压器基方法，该方法能够检测不可见区域并从可见部分恢复其缺失特征。具体而言，ORFormer将每个图像块 tokens与一个额外的可学习的 tokens，即信使 token 相关联。信使 token 从所有其他块中聚合特征，而不包括自身的块。通过参考其常规嵌入和信使嵌入之间相似性的方法，可以评估一个块与其他块之间的共识，从而实现不可见区域的识别。接着，我们的方法使用信使 tokens 聚合的功能来恢复被遮挡的块。利用恢复的功能，ORFormer可以为下游的 FLD 任务生成高质量的热力图。大量实验证明，我们的方法能够生成对部分遮挡具有鲁棒性的热力图。将生成的热力图整合到现有的 FLD 方法中，我们的方法在如 WFLW 和 COFW 等具有挑战性的数据集上表现优于最先进的方法。', 'title_zh': 'ORFormer：鲁棒性自注意力模型用于准确的面部特征点检测'}
{'arxiv_id': 'arXiv:2412.13168', 'title': 'Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild', 'authors': 'Xingjian Wang, Li Chai', 'link': 'https://arxiv.org/abs/2412.13168', 'abstract': 'In-the-wild dynamic facial expression recognition (DFER) encounters a significant challenge in recognizing emotion-related expressions, which are often temporally and spatially diluted by emotion-irrelevant expressions and global context. Most prior DFER methods directly utilize coupled spatiotemporal representations that may incorporate weakly relevant features with emotion-irrelevant context bias. Several DFER methods highlight dynamic information for DFER, but following explicit guidance that may be vulnerable to irrelevant motion. In this paper, we propose a novel Implicit Facial Dynamics Disentanglement framework (IFDD). Through expanding wavelet lifting scheme to fully learnable framework, IFDD disentangles emotion-related dynamic information from emotion-irrelevant global context in an implicit manner, i.e., without exploit operations and external guidance. The disentanglement process contains two stages. The first is Inter-frame Static-dynamic Splitting Module (ISSM) for rough disentanglement estimation, which explores inter-frame correlation to generate content-aware splitting indexes on-the-fly. We utilize these indexes to split frame features into two groups, one with greater global similarity, and the other with more unique dynamic features. The second stage is Lifting-based Aggregation-Disentanglement Module (LADM) for further refinement. LADM first aggregates two groups of features from ISSM to obtain fine-grained global context features by an updater, and then disentangles emotion-related facial dynamic features from the global context by a predictor. Extensive experiments on in-the-wild datasets have demonstrated that IFDD outperforms prior supervised DFER methods with higher recognition accuracy and comparable efficiency. Code is available at this https URL.', 'abstract_zh': '在野外的动态面部表情识别（DFER）面临着一个重大挑战，即在不相关情绪表达和全局上下文中，识别与情绪相关的表达往往在时间和空间上被稀释。大多数之前的DFER方法直接利用了耦合的时空表示，这些表示可能包含了与情绪无关的上下文偏见的弱相关特征。尽管一些DFER方法强调了动态信息在情绪识别中的应用，但这些方法往往根据明确的指导进行，这种指导可能会受到无关运动的影响。在本文中，我们提出了一种新颖的隐式面部动态解耦框架（Implicit Facial Dynamics Disentanglement Framework, IFDD）。通过扩展小波提升方案以构建完全可学习的框架，IFDD 在不依赖于操作和外部指导的情况下，隐式地将与情绪相关的动态信息从与情绪无关的全局上下文中解耦。解耦过程包含两个阶段。第一阶段是帧间静态-动态划分模块（Inter-frame Static-dynamic Splitting Module, ISSM），用于粗略估计解耦，它通过探索帧间相关性来实时生成内容感知的划分索引。我们使用这些索引来将帧特征划分为两组，一组具有较高的全局相似性，另一组则具有更独特的动态特征。第二阶段是基于提升的聚合-解耦模块（Lifting-based Aggregation-Disentanglement Module, LADM），进行进一步细化。LADM 首先通过更新器聚合ISSM的两组特征，以获得精细粒度的全局上下文特征，然后使用预测器从全局上下文中解耦出与情绪相关的面部动态特征。在几个野外数据集上的广泛实验表明，IFDD 在更高的识别准确性和相近的效率方面优于之前的监督DFER方法。相关代码已发布在 [该链接]。', 'title_zh': '基于提升方案的隐式的将野生环境下与情绪相关的面部动态解耦'}
{'arxiv_id': 'arXiv:2412.13152', 'title': 'Continuous Patient Monitoring with AI: Real-Time Analysis of Video in Hospital Care Settings', 'authors': 'Paolo Gabriel, Peter Rehani, Tyler Troy, Tiffany Wyatt, Michael Choma, Narinder Singh', 'link': 'https://arxiv.org/abs/2412.13152', 'abstract': 'This study introduces an AI-driven platform for continuous and passive patient monitoring in hospital settings, developed by LookDeep Health. Leveraging advanced computer vision, the platform provides real-time insights into patient behavior and interactions through video analysis, securely storing inference results in the cloud for retrospective evaluation. The dataset, compiled in collaboration with 11 hospital partners, encompasses over 300 high-risk fall patients and over 1,000 days of inference, enabling applications such as fall detection and safety monitoring for vulnerable patient populations. To foster innovation and reproducibility, an anonymized subset of this dataset is publicly available. The AI system detects key components in hospital rooms, including individual presence and role, furniture location, motion magnitude, and boundary crossings. Performance evaluation demonstrates strong accuracy in object detection (macro F1-score = 0.92) and patient-role classification (F1-score = 0.98), as well as reliable trend analysis for the "patient alone" metric (mean logistic regression accuracy = 0.82 \\pm 0.15). These capabilities enable automated detection of patient isolation, wandering, or unsupervised movement-key indicators for fall risk and other adverse events. This work establishes benchmarks for validating AI-driven patient monitoring systems, highlighting the platform\'s potential to enhance patient safety and care by providing continuous, data-driven insights into patient behavior and interactions.', 'abstract_zh': '本文介绍了由LookDeep Health开发的一种基于人工智能的平台，该平台用于医院环境中的连续且被动的患者监测。该平台利用先进的人工智能视觉技术，通过视频分析提供实时的患者行为和互动洞察，并将推断结果安全地存储在云端，以便进行事后评估。该数据集在与11家医院合作伙伴的协作下收集而成，包含超过300名高风险跌倒患者和超过1,000天的推断数据，可用于跌倒检测和脆弱患者群体的安全监测等多种应用。为了促进创新和可重复性，该数据集的一个匿名子集已公开。该AI系统检测医院房间中的关键组成部分，包括个体在场情况和角色、家具位置、运动幅度以及边界穿越。性能评估显示其在物体检测方面具有很强的准确性（宏F1分值=0.92）和患者角色分类的准确性（F1分值=0.98），同时对“患者独处”指标的趋势分析具有可靠的可靠性（平均逻辑回归精度=0.82 ± 0.15）。这些功能使系统能够自动检测患者隔离、擅自行动或未被监督的移动等关键指标，这些指标是跌倒风险和其他不良事件的重要标志。本文确立了评估基于AI的患者监测系统的基准，突显了该平台通过提供连续的数据驱动洞察来增强患者安全和护理的潜力。', 'title_zh': '基于AI的连续患者监测：医院护理环境中视频的实时分析'}
{'arxiv_id': 'arXiv:2412.13148', 'title': 'SWAN: Preprocessing SGD Enables Adam-Level Performance On LLM Training With Significant Memory Reduction', 'authors': 'Chao Ma, Wenbo Gong, Meyer Scetbon, Edward Meeds', 'link': 'https://arxiv.org/abs/2412.13148', 'abstract': 'Adaptive optimizers such as Adam (Kingma & Ba, 2015) have been central to the success of large language models. However, they maintain additional moving average states throughout training, which results in memory requirements several times greater than the model. This overhead imposes constraints on scalability and computational efficiency. On the other hand, while stochastic gradient descent (SGD) is optimal in terms of memory efficiency, their capability in LLM training is limited (Zhao et al., 2024b).\nTo address this dilemma, we show that pre-processing SGD is sufficient to reach Adam-level performance on LLMs. Specifically, we propose to preprocess the instantaneous stochastic gradients with two simple operators: $\\mathtt{GradNorm}$ and $\\mathtt{GradWhitening}$. $\\mathtt{GradNorm}$ stabilizes gradient distributions, and $\\mathtt{GradWhitening}$ counteracts the local curvature of the loss landscape, respectively. This results in SWAN (SGD with Whitening And Normalization), a stochastic optimizer that eliminates the need to store any accumulative state variables. Empirically, SWAN has the same memory footprint as SGD, achieving $\\approx 50\\%$ reduction on total end-to-end memory compared to Adam. In language modeling tasks, SWAN demonstrates the same or even a substantial improvement over Adam. Specifically, when pre-training the LLaMa model with 350M and 1.3B parameters, SWAN achieves a 2x speedup by reaching the same evaluation perplexity in less than half tokens seen.', 'abstract_zh': '自适应优化器如Adam（Kingma & Ba, 2015）对大型语言模型的成功起到了关键作用。然而，它们在整个训练过程中维护了额外的移动平均状态，导致内存需求是模型本身的几倍。这一开销对可扩展性和计算效率提出了限制。另一方面，虽然随机梯度下降（SGD）在内存效率上最佳，但在大型语言模型（LLM）训练中的表现却有限（Zhao et al., 2024b）。\n\n为了解决这一矛盾，我们展示了对SGD进行预处理足以达到与Adam相当的性能。具体而言，我们提出使用两个简单的操作符：$\\mathtt{GradNorm}$ 和 $\\mathtt{GradWhitening}$ 来预处理瞬时随机梯度。$\\mathtt{GradNorm}$ 稳定梯度分布，而 $\\mathtt{GradWhitening}$ 则抵消损失景观的局部曲率。这导致了SWAN（带有去相关化和标准化的SGD），这是一个随机优化器，不需要存储任何累积状态变量。从实验上看，SWAN 的内存占用与SGD相同，总内存消耗相比于Adam减少了约50%。在语言建模任务中，SWAN 达到了与Adam相当甚至是显著的改进。具体来说，在使用包含350M和1.3B参数的LLaMa模型进行预训练时，通过在见过的少于一半的词元内达到相同的评估困惑度，SWAN 实现了两倍的加速。', 'title_zh': 'SWAN：预处理SGD使大型语言模型训练达到Adam级别的性能，同时显著减少内存使用'}
{'arxiv_id': 'arXiv:2412.13116', 'title': 'Equity in the Use of ChatGPT for the Classroom: A Comparison of the Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics and Data Science Exams', 'authors': 'Monnie McGee, Bivin Sadler', 'link': 'https://arxiv.org/abs/2412.13116', 'abstract': 'A college education historically has been seen as method of moving upward with regards to income brackets and social status. Indeed, many colleges recognize this connection and seek to enroll talented low income students. While these students might have their education, books, room, and board paid; there are other items that they might be expected to use that are not part of most college scholarship packages. One of those items that has recently surfaced is access to generative AI platforms. The most popular of these platforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version (ChatGPT3.5). We seek to explore differences in the free and paid versions in the context of homework questions and data analyses as might be seen in a typical introductory statistics course. We determine the extent to which students who cannot afford newer and faster versions of generative AI programs would be disadvantaged in terms of writing such projects and learning these methods.', 'abstract_zh': '传统的大学教育被认为是一种提高收入阶层和社会地位的方法。确实，许多大学认识到了这一点，并试图招收有才能但家庭经济条件较差的学生。虽然这些学生可能会获得学费、书籍、住宿和餐饮资助，但他们仍可能需要使用的一些工具并没有包含在大多数奖学金的包内。其中一个最近凸显的问题是访问生成式AI平台。最受欢迎的这类平台是ChatGPT，它有付费版本（ChatGPT4）和免费版本（ChatGPT3.5）。我们旨在探讨在作业问题和数据分析等典型的 introductory 统计课程情境下，免费和付费版本之间的差异。我们还确定那些无法负担更新更快版本的生成式AI程序的学生，在撰写此类项目和学习这些方法方面是否会处于不利地位。', 'title_zh': '将ChatGPT在课堂上使用中的公平性：关于统计和数据科学考试，ChatGPT 3.5与ChatGPT 4的准确性和精确性比较'}
{'arxiv_id': 'arXiv:2412.13103', 'title': 'AI PERSONA: Towards Life-long Personalization of LLMs', 'authors': 'Tiannan Wang, Meiling Tao, Ruoyu Fang, Huilin Wang, Shuai Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou', 'link': 'https://arxiv.org/abs/2412.13103', 'abstract': 'In this work, we introduce the task of life-long personalization of large language models. While recent mainstream efforts in the LLM community mainly focus on scaling data and compute for improved capabilities of LLMs, we argue that it is also very important to enable LLM systems, or language agents, to continuously adapt to the diverse and ever-changing profiles of every distinct user and provide up-to-date personalized assistance. We provide a clear task formulation and introduce a simple, general, effective, and scalable framework for life-long personalization of LLM systems and language agents. To facilitate future research on LLM personalization, we also introduce methods to synthesize realistic benchmarks and robust evaluation metrics. We will release all codes and data for building and benchmarking life-long personalized LLM systems.', 'abstract_zh': '在本文中，我们介绍了大型语言模型长期个性化任务。尽管LLM社区近期的主要努力主要集中在通过扩展数据和计算能力来提高LLM的能力，我们认为也需要让用户级的LLM系统或语言代理能够不断适应每位独特用户的多样且不断变化的特征，并提供实时个性化支持变得非常重要。我们提供了一个清晰的任务描述，并引入了一个简单、通用、有效且可扩展的框架，用于长期个性化LLM系统和语言代理。为促进未来对LLM个性化的研究，我们还介绍了生成现实基准和稳健评估指标的方法。我们将公布所有构建和基准测试长时个性化LLM系统所需的代码和数据。', 'title_zh': 'AI人格：迈向大型语言模型的终身个性化'}
{'arxiv_id': 'arXiv:2412.13091', 'title': 'LMUnit: Fine-grained Evaluation with Natural Language Unit Tests', 'authors': 'Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, Shikib Mehri', 'link': 'https://arxiv.org/abs/2412.13091', 'abstract': 'As language models become integral to critical workflows, assessing their behavior remains a fundamental challenge -- human evaluation is costly and noisy, while automated metrics provide only coarse, difficult-to-interpret signals. We introduce natural language unit tests, a paradigm that decomposes response quality into explicit, testable criteria, along with a unified scoring model, LMUnit, which combines multi-objective training across preferences, direct ratings, and natural language rationales. Through controlled human studies, we show this paradigm significantly improves inter-annotator agreement and enables more effective LLM development workflows. LMUnit achieves state-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and competitive results on RewardBench. These results validate both our proposed paradigm and scoring model, suggesting a promising path forward for language model evaluation and development.', 'abstract_zh': '随着语言模型在关键工作流中变得不可或缺，评估其行为仍然是一个基本挑战——人力评估成本高且噪音大，而自动化指标只能提供粗糙且难以解释的信号。我们提出了自然语言单元测试这一方法，该方法将响应质量分解为明确且可测试的标准，并结合了LMUnit统一评分模型，该模型综合了偏好、直接打分和自然语言理由的多目标训练。通过控制性的人类实验，我们证明这一方法显著提高了注释者间的共识，并支持了更有效的大型语言模型开发工作流程。LMUnit在评估基准（FLASK、BigGenBench）上达到了最先进的性能，并在RewardBench上取得了具有竞争力的结果。这些结果验证了我们提出的方法和评分模型，为语言模型评估和开发指明了一条前景广阔的路径。', 'title_zh': 'LMUnit：基于自然语言单元测试的细粒度评估'}
{'arxiv_id': 'arXiv:2412.13079', 'title': 'Identifying Bias in Deep Neural Networks Using Image Transforms', 'authors': 'Sai Teja Erukude, Akhil Joshi, Lior Shamir', 'link': 'https://arxiv.org/abs/2412.13079', 'abstract': 'CNNs have become one of the most commonly used computational tool in the past two decades. One of the primary downsides of CNNs is that they work as a ``black box", where the user cannot necessarily know how the image data are analyzed, and therefore needs to rely on empirical evaluation to test the efficacy of a trained CNN. This can lead to hidden biases that affect the performance evaluation of neural networks, but are difficult to identify. Here we discuss examples of such hidden biases in common and widely used benchmark datasets, and propose techniques for identifying dataset biases that can affect the standard performance evaluation metrics. One effective approach to identify dataset bias is to perform image classification by using merely blank background parts of the original images. However, in some situations a blank background in the images is not available, making it more difficult to separate foreground or contextual information from the bias. To overcome this, we propose a method to identify dataset bias without the need to crop background information from the images. That method is based on applying several image transforms to the original images, including Fourier transform, wavelet transforms, median filter, and their combinations. These transforms were applied to recover background bias information that CNNs use to classify images. This transformations affect the contextual visual information in a different manner than it affects the systemic background bias. Therefore, the method can distinguish between contextual information and the bias, and alert on the presence of background bias even without the need to separate sub-images parts from the blank background of the original images. Code used in the experiments is publicly available.', 'abstract_zh': '在过去的二十年中，卷积神经网络（CNNs）已成为最常用的计算工具之一。CNNs的一个主要缺点是它们作为“黑箱”工作，用户无法确定图像数据是如何分析的，因此需要依赖经验性评估来测试训练完成的CNN的效果。这可能导致隐藏的偏差，影响神经网络的性能评估，但这些偏差难以识别。在这里，我们讨论了在常用和广泛使用的基准数据集中存在的这些隐藏偏差的示例，并提出了一种技术来识别可能影响标准性能评估指标的数据集偏差。一种有效的方法是通过仅使用原始图像的空白背景部分进行图像分类。然而，在某些情况下，图像中没有空白背景，使得从前景或上下文信息中分离偏差更为困难。为了解决这个问题，我们提出了一种无需从图像中裁剪背景信息来识别数据集偏差的方法。该方法基于对原始图像应用多种图像变换，包括傅里叶变换、小波变换、中值滤波及其组合。这些变换被应用于恢复CNN用于分类图像时使用的背景偏差信息。这些变换对背景偏差的影响方式不同于对上下文视觉信息的影响。因此，该方法可以区分上下文信息和偏差，并在无需从原始图像的空白背景中分离子图像部分的情况下提示背景偏差的存在。实验所用代码已公开。', 'title_zh': '使用图像变换识别深度神经网络中的偏见'}
{'arxiv_id': 'arXiv:2412.13061', 'title': 'VidTok: A Versatile and Open-Source Video Tokenizer', 'authors': 'Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, Jiang Bian', 'link': 'https://arxiv.org/abs/2412.13061', 'abstract': 'Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.', 'abstract_zh': '将视频内容编码为紧凑的潜在标记已成为视频生成和理解中的一个基本步骤，这主要是为了应对像素级表示中的固有冗余。因此，随着以视频为中心的研究日益重要，对高性能且开源的视频分词器的需求也在不断增加。我们介绍了VidTok，这是一种多功能的视频分词器，能够在连续和离散分词中提供最先进的性能。VidTok 在现有方法的基础上引入了几项关键改进：1）模型架构，如卷积层和上/下采样模块；2）为了解决传统向量量化（VQ）中常见的训练不稳定性和代码书坍塌问题，我们将其与有限标量量化（FSQ）相结合用于离散视频分词；3）改进的训练策略，包括两阶段训练过程以及使用降低的帧率。通过整合这些改进，VidTok 在多个指标（包括峰值信噪比（PSNR）、结构相似性（SSIM）、平均结构差异（LPIPS）和基于特征的视频差异（FVD））上表现出显著优于现有方法的性能，这些指标在标准化评估环境中得到了验证。', 'title_zh': 'VidTok：一种多功能开源视频分词器'}
{'arxiv_id': 'arXiv:2412.13053', 'title': 'SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks', 'authors': 'Mátyás Vincze, Laura Ferrarotti, Leonardo Lucio Custode, Bruno Lepri, Giovanni Iacca', 'link': 'https://arxiv.org/abs/2412.13053', 'abstract': 'Continuous control tasks often involve high-dimensional, dynamic, and non-linear environments. State-of-the-art performance in these tasks is achieved through complex closed-box policies that are effective, but suffer from an inherent opacity. Interpretable policies, while generally underperforming compared to their closed-box counterparts, advantageously facilitate transparent decision-making within automated systems. Hence, their usage is often essential for diagnosing and mitigating errors, supporting ethical and legal accountability, and fostering trust among stakeholders. In this paper, we propose SMOSE, a novel method to train sparsely activated interpretable controllers, based on a top-1 Mixture-of-Experts architecture. SMOSE combines a set of interpretable decisionmakers, trained to be experts in different basic skills, and an interpretable router that assigns tasks among the experts. The training is carried out via state-of-the-art Reinforcement Learning algorithms, exploiting load-balancing techniques to ensure fair expert usage. We then distill decision trees from the weights of the router, significantly improving the ease of interpretation. We evaluate SMOSE on six benchmark environments from MuJoCo: our method outperforms recent interpretable baselines and narrows the gap with noninterpretable state-of-the-art algorithms', 'abstract_zh': '连续控制任务通常涉及高维度、动态和非线性的环境。在这些任务中，最先进的性能是由复杂的黑盒策略实现的，这些策略虽然有效，但具有固有的不透明性。可解释的策略虽然通常在性能上不如其黑盒对应物，但其优点在于能够促进自动化系统中的透明决策过程。因此，它们在诊断和缓解错误、支持伦理和法律责任以及增强利益相关者的信任方面往往是必不可少的。本文提出了一种名为SMOSE的新方法，该方法基于Top-1 Mixture-of-Experts架构来训练稀疏激活的可解释控制器。SMOSE结合了一组可解释的决策者，它们分别擅长不同的基本技能，以及一个可解释的路由器，用于分配任务给专家。通过利用负载均衡技术进行训练，确保公平地使用专家。然后，我们从路由器的权重中提取决策树，显著提高了解释的简便性。我们将在MuJoCo的六个基准环境中评估SMOSE，结果显示我们的方法优于最近的可解释基线，并缩小了与不可解释的最先进的非基线算法之间的性能差距。', 'title_zh': 'SMOSE：稀疏混合浅专家模型在连续控制任务中的可解释强化学习'}
{'arxiv_id': 'arXiv:2412.13050', 'title': 'Modality-Inconsistent Continual Learning of Multimodal Large Language Models', 'authors': 'Weiguo Pian, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian', 'link': 'https://arxiv.org/abs/2412.13050', 'abstract': "In this paper, we introduce Modality-Inconsistent Continual Learning (MICL), a new continual learning scenario for Multimodal Large Language Models (MLLMs) that involves tasks with inconsistent modalities (image, audio, or video) and varying task types (captioning or question-answering). Unlike existing vision-only or modality-incremental settings, MICL combines modality and task type shifts, both of which drive catastrophic forgetting. To address these challenges, we propose MoInCL, which employs a Pseudo Targets Generation Module to mitigate forgetting caused by task type shifts in previously seen modalities. It also incorporates Instruction-based Knowledge Distillation to preserve the model's ability to handle previously learned modalities when new ones are introduced. We benchmark MICL using a total of six tasks and conduct experiments to validate the effectiveness of our proposed MoInCL. The experimental results highlight the superiority of MoInCL, showing significant improvements over representative and state-of-the-art continual learning baselines.", 'abstract_zh': '在本文中，我们提出了模态不一致连续学习（Modality-Inconsistent Continual Learning, MICL），这是一种针对多模态大型语言模型（Multimodal Large Language Models, MLLMs）的新连续学习场景。该场景涉及具有不一致模态（图像、音频或视频）和不同任务类型（描述或问答）的任务。与现有的仅视觉或模态增量设置不同，MICL 结合了模态和任务类型的变化，这两种变化都可能导致灾难性遗忘。为了解决这些挑战，我们提出了一种名为 MoInCL 的方法，该方法通过伪目标生成模块来缓解因先前模态的任务类型变化而导致的遗忘现象。同时，它还结合了基于指令的知识蒸馏，以在引入新模态时保持模型处理先前学习模态的能力。我们使用六种任务对 MICL 进行基准测试，并进行了实验以验证 MoInCL 的有效性。实验结果强调了 MoInCL 的优越性，展示了其在对比代表性和最先进连续学习基线上的显著改进。', 'title_zh': '多模态大型语言模型的模态不一致持续学习'}
{'arxiv_id': 'arXiv:2412.12997', 'title': 'Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO', 'authors': 'Umer Butt, Stalin Veranasi, Günter Neumann', 'link': 'https://arxiv.org/abs/2412.12997', 'abstract': 'As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset through machine translation. We establish baseline results through zero-shot learning for IR in Urdu and subsequently apply the mMARCO multilingual IR methodology to this newly translated dataset. Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant improvements over zero-shot results and showing the potential for expanding IR access for Urdu speakers. By bridging access gaps for speakers of low-resource languages, this work not only advances multilingual IR research but also emphasizes the ethical and societal importance of inclusive IR technologies. This work provides valuable insights into the challenges and solutions for improving language representation and lays the groundwork for future research, especially in South Asian languages, which can benefit from the adaptable methods used in this study.', 'abstract_zh': '随着信息检索（IR）领域越来越认识到包容性的重要性，满足低资源语言需求仍然是一个重大挑战。本文介绍了第一个大规模乌尔都语IR数据集，该数据集通过机器翻译将MS MARCO数据集进行了翻译。我们通过零样本学习建立了乌尔都语IR的基本基准结果，随后使用mMARCO多语言IR方法论对这个新翻译的数据集进行了应用。我们的研究发现调优后的模型（Urdu-mT5-mMARCO）实现了平均倒数相关 rank (MRR@10) 0.247 和召回率 Recall@10 0.439，这表明与零样本结果相比有了显著改进，展示了扩展乌尔都语用户IR访问的可能性。通过为低资源语言使用者填补访问差距，本研究不仅推进了多语言IR研究，还强调了包容性IR技术的伦理和社会重要性。本研究提供了关于如何改进语言表示的挑战与解决方案的宝贵见解，并为未来的研究奠定了基础，特别为南亚语言的改进提供了适应性强的方法。', 'title_zh': '支持低资源语言检索：建立乌尔都语MS MARCO的基础模型'}
{'arxiv_id': 'arXiv:2412.12996', 'title': 'Neural Control and Certificate Repair via Runtime Monitoring', 'authors': 'Emily Yu, Đorđe Žikelić, Thomas A. Henzinger', 'link': 'https://arxiv.org/abs/2412.12996', 'abstract': 'Learning-based methods provide a promising approach to solving highly non-linear control tasks that are often challenging for classical control methods. To ensure the satisfaction of a safety property, learning-based methods jointly learn a control policy together with a certificate function for the property. Popular examples include barrier functions for safety and Lyapunov functions for asymptotic stability. While there has been significant progress on learning-based control with certificate functions in the white-box setting, where the correctness of the certificate function can be formally verified, there has been little work on ensuring their reliability in the black-box setting where the system dynamics are unknown. In this work, we consider the problems of certifying and repairing neural network control policies and certificate functions in the black-box setting. We propose a novel framework that utilizes runtime monitoring to detect system behaviors that violate the property of interest under some initially trained neural network policy and certificate. These violating behaviors are used to extract new training data, that is used to re-train the neural network policy and the certificate function and to ultimately repair them. We demonstrate the effectiveness of our approach empirically by using it to repair and to boost the safety rate of neural network policies learned by a state-of-the-art method for learning-based control on two autonomous system control tasks.', 'abstract_zh': '基于学习的方法为解决通常难以用经典控制方法处理的强非线性控制任务提供了有希望的途径。为了确保满足某个安全属性，基于学习的方法会共同学习控制策略及其用于属性验证的证书函数。流行的示例包括用于安全性的屏障函数和用于渐近稳定性的李雅普诺夫函数。虽然在白盒设置（即可以在形式上验证证书函数正确性的设置）中，基于学习的控制及其证书函数方面已经取得了显著进展，但在黑盒设置中（即系统动力学未知的设置）确保它们的可靠性的工作却很少。在本研究中，我们关注在黑盒设置下验证和修复神经网络控制策略及其证书函数的问题。我们提出了一种新框架，该框架利用运行时监控来检测在某些初始训练的神经网络策略和证书下违反所需属性的系统行为。这些违反行为被用来提取新的训练数据，这些数据被用于重新训练神经网络策略和证书函数，最终修复它们。我们通过在学习-基于控制方法中学习的两种自主系统控制任务中使用该方法来修复和提高神经网络策略的安全率，以实证展示我们方法的有效性。', 'title_zh': '神经控制与运行时监控下的证书修复'}
{'arxiv_id': 'arXiv:2412.12987', 'title': 'Stochastic interior-point methods for smooth conic optimization with applications', 'authors': 'Chuan He, Zhanwang Deng', 'link': 'https://arxiv.org/abs/2412.12987', 'abstract': 'Conic optimization plays a crucial role in many machine learning (ML) problems. However, practical algorithms for conic constrained ML problems with large datasets are often limited to specific use cases, as stochastic algorithms for general conic optimization remain underdeveloped. To fill this gap, we introduce a stochastic interior-point method (SIPM) framework for general conic optimization, along with four novel SIPM variants leveraging distinct stochastic gradient estimators. Under mild assumptions, we establish the global convergence rates of our proposed SIPMs, which, up to a logarithmic factor, match the best-known rates in stochastic unconstrained optimization. Finally, our numerical experiments on robust linear regression, multi-task relationship learning, and clustering data streams demonstrate the effectiveness and efficiency of our approach.', 'abstract_zh': '圆锥优化在许多机器学习（ML）问题中发挥着关键作用。然而，针对大型数据集的圆锥约束ML问题的实用算法往往局限于特定的应用场景，因为一般圆锥优化的随机算法尚未充分开发。为了弥补这一不足，我们引入了一种全局圆锥优化的随机内点方法（SIPM）框架，并提出了四种新的SIPM变体，分别利用不同的随机梯度估计器。在较温和的假设条件下，我们建立了我们提出SIPMs的全局收敛率，这些收敛率（除了对数因子外）与随机无约束优化中已知的最佳收敛率相当。最后，我们在稳健线性回归、多任务关系学习以及聚类数据流上的数值实验表明了我们方法的有效性和效率。', 'title_zh': '随机内点方法在光滑锥优化中的应用'}
{'arxiv_id': 'arXiv:2412.12984', 'title': 'Cluster-guided Contrastive Class-imbalanced Graph Classification', 'authors': 'Wei Ju, Zhengyang Mao, Siyu Yi, Yifang Qin, Yiyang Gu, Zhiping Xiao, Jianhao Shen, Ziyue Qiao, Ming Zhang', 'link': 'https://arxiv.org/abs/2412.12984', 'abstract': 'This paper studies the problem of class-imbalanced graph classification, which aims at effectively classifying the categories of graphs in scenarios with imbalanced class distribution. Despite the tremendous success of graph neural networks (GNNs), their modeling ability for imbalanced graph-structured data is inadequate, which typically leads to predictions biased towards the majority classes. Besides, existing class-imbalanced learning methods in visions may overlook the rich graph semantic substructures of the majority classes and excessively emphasize learning from the minority classes. To tackle this issue, this paper proposes a simple yet powerful approach called C$^3$GNN that incorporates the idea of clustering into contrastive learning to enhance class-imbalanced graph classification. Technically, C$^3$GNN clusters graphs from each majority class into multiple subclasses, ensuring they have similar sizes to the minority class, thus alleviating class imbalance. Additionally, it utilizes the Mixup technique to synthesize new samples and enrich the semantic information of each subclass, and leverages supervised contrastive learning to hierarchically learn effective graph representations. In this way, we can not only sufficiently explore the semantic substructures within the majority class but also effectively alleviate excessive focus on the minority class. Extensive experiments on real-world graph benchmark datasets verify the superior performance of our proposed method.', 'abstract_zh': '本文研究了类别失衡图分类问题，目标是在类别分布失衡的场景中有效地对图的类别进行分类。尽管图神经网络（GNNs）取得了巨大的成功，但它们在建模失衡图结构化数据方面的能力仍然不足，这通常会导致预测偏向于多数类别。此外，现有的一些视觉领域类别失衡学习方法可能忽略了多数类的丰富图语义子结构，并过度强调从少数类别中学习。为了解决这一问题，本文提出了一种简单而强大的方法C$^3$GNN，将聚类思想融入对比学习中，以增强类别失衡图分类。具体地，C$^3$GNN 将每个多数类的图聚类成多个子类别，并确保这些子类别的大小与少数类别相当，从而缓解类别不平衡。此外，它利用 Mixup 技术合成新的样本，丰富每个子类别的语义信息，并利用监督对比学习逐层学习有效的图表示。这样，我们不仅可以充分探索多数类内的语义子结构，还可以有效地缓解对少数类的过度关注。在现实世界的图基准数据集上的广泛实验验证了我们提出方法的优越性能。', 'title_zh': '基于簇引导的对比学习不平衡图分类'}
{'arxiv_id': 'arXiv:2412.12933', 'title': 'Two Layer Walk: A Community-Aware Graph Embedding', 'authors': 'He Yu, Jing Liu', 'link': 'https://arxiv.org/abs/2412.12933', 'abstract': 'Community structures are critical for understanding the mesoscopic organization of networks, bridging local and global patterns. While methods such as DeepWalk and node2vec capture local positional information through random walks, they fail to preserve community structures. Other approaches like modularized nonnegative matrix factorization and evolutionary algorithms address this gap but are computationally expensive and unsuitable for large-scale networks. To overcome these limitations, we propose Two Layer Walk (TLWalk), a novel graph embedding algorithm that incorporates hierarchical community structures. TLWalk balances intra- and inter-community relationships through a community-aware random walk mechanism without requiring additional parameters. Theoretical analysis demonstrates that TLWalk effectively mitigates locality bias. Experiments on benchmark datasets show that TLWalk outperforms state-of-the-art methods, achieving up to 3.2% accuracy gains for link prediction tasks. By encoding dense local and sparse global structures, TLWalk proves robust and scalable across diverse networks, offering an efficient solution for network analysis.', 'abstract_zh': '社区结构对于理解网络的中间尺度组织至关重要，能够连接局部和全局模式。虽然诸如DeepWalk和node2vec等方法通过随机游走捕获局部位置信息，但它们无法保留社区结构。其他方法如模块化非负矩阵分解和进化算法弥补了这一不足，但计算成本高且不适合大规模网络。为克服这些局限性，我们提出了一种名为Two Layer Walk (TLWalk)的新型图嵌入算法，该算法整合了分层社区结构。通过社区感知随机游走机制，TLWalk在不需要额外参数的情况下平衡了内部和外部社区关系。理论分析表明，TLWalk有效地减轻了局部偏差。在基准数据集上的实验结果显示，TLWalk优于最先进的方法，在链接预测任务中可获得高达3.2%的准确率提升。通过编码密集的局部结构和稀疏的全局结构，TLWalk在多种网络中表现出 robustness和可扩展性，提供了一种高效的方法来分析网络。', 'title_zh': '两层漫步：一种考虑社区的图嵌入方法'}
{'arxiv_id': 'arXiv:2412.12932', 'title': 'CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models', 'authors': 'Zihui Cheng, Qiguang Chen, Jin Zhang, Hao Fei, Xiaocheng Feng, Wanxiang Che, Min Li, Libo Qin', 'link': 'https://arxiv.org/abs/2412.12932', 'abstract': 'Large Vision-Language Models (LVLMs) have recently demonstrated amazing success in multi-modal tasks, including advancements in Multi-modal Chain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks still follow a traditional paradigm with multi-modal input and text-modal output, which leads to significant drawbacks such as missing visual operations and vague expressions. Motivated by this, we introduce a novel Chain of Multi-modal Thought (CoMT) benchmark to address these limitations. Different from the traditional MCoT benchmark, CoMT requires both multi-modal input and multi-modal reasoning output, aiming to mimic human-like reasoning that inherently integrates visual operation. Specifically, CoMT consists of four categories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and (4) Visual Selection to comprehensively explore complex visual operations and concise expression in real scenarios. We evaluate various LVLMs and strategies on CoMT, revealing some key insights into the capabilities and limitations of the current approaches. We hope that CoMT can inspire more research on introducing multi-modal generation into the reasoning process.', 'abstract_zh': '大规模多模态语言模型（Large Vision-Language Models, LVLMs）最近在多模态任务中展示了令人瞩目的成功，包括在多模态链式推理（Multi-modal Chain-of-Thought, MCoT）方面的进步。尽管取得了这些成就，当前的基准测试仍然遵循传统的多模态输入和文本模态输出的范式，这导致了诸如视觉操作缺失和表达模糊等显著缺点。为了解决这些问题，我们引入了一个新的多模态思考链（Chain of Multi-modal Thought, CoMT）基准测试，以应对这些限制。与传统的MCoT基准测试不同，CoMT要求同时提供多模态输入和多模态推理输出，旨在模拟人类推理过程，该过程包含内在的视觉操作。具体而言，CoMT 包含四个类别：（1）视觉创作，（2）视觉删除，（3）视觉更新，以及（4）视觉选择，旨在全面探索实际场景中的复杂视觉操作和简洁表达。我们对CoMT上的各种LVLMs和策略进行了评估，揭示了当前方法在能力和限制方面的关键见解。我们希望CoMT能够激发更多关于在推理过程中引入多模态生成的研究。', 'title_zh': 'CoMT：一种新的链式多模态思维基准测试用于大型视觉语言模型'}
{'arxiv_id': 'arXiv:2412.12912', 'title': 'Unsupervised Region-Based Image Editing of Denoising Diffusion Models', 'authors': 'Zixiang Li, Yue Song, Renshuai Tao, Xiaohong Jia, Yao Zhao, Wei Wang', 'link': 'https://arxiv.org/abs/2412.12912', 'abstract': 'Although diffusion models have achieved remarkable success in the field of image generation, their latent space remains under-explored. Current methods for identifying semantics within latent space often rely on external supervision, such as textual information and segmentation masks. In this paper, we propose a method to identify semantic attributes in the latent space of pre-trained diffusion models without any further training. By projecting the Jacobian of the targeted semantic region into a low-dimensional subspace which is orthogonal to the non-masked regions, our approach facilitates precise semantic discovery and control over local masked areas, eliminating the need for annotations. We conducted extensive experiments across multiple datasets and various architectures of diffusion models, achieving state-of-the-art performance. In particular, for some specific face attributes, the performance of our proposed method even surpasses that of supervised approaches, demonstrating its superior ability in editing local image properties.', 'abstract_zh': '尽管扩散模型在图像生成领域取得了显著的成功，但其潜在空间仍存在未被充分探索的潜在空间。当前识别潜在空间中语义特征的方法往往依赖于外部监督，例如文本信息和分割掩码。本文提出了一种在预训练扩散模型的潜在空间中识别语义属性的方法，无需进一步训练。通过将目标语义区域的雅可比矩阵投影到一个与未掩码区域正交的低维子空间中，我们的方法能够实现对局部掩码区域的精确语义发现与控制，从而消除了对外部标注的依赖。我们在多个数据集和各种扩散模型结构上进行了广泛的实验，取得了最先进的性能。特别是对于某些特定的脸部属性，我们提出的方法甚至超过了监督方法，在编辑局部图像属性方面展现了其优越的能力。', 'title_zh': '无监督区域级图像去噪编辑模型'}
{'arxiv_id': 'arXiv:2412.12892', 'title': 'SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge Detection', 'authors': 'Xing Liufu, Chaolei Tan, Xiaotong Lin, Yonggang Qi, Jinxuan Li, Jian-Fang Hu', 'link': 'https://arxiv.org/abs/2412.12892', 'abstract': "Edge labels are typically at various granularity levels owing to the varying preferences of annotators, thus handling the subjectivity of per-pixel labels has been a focal point for edge detection. Previous methods often employ a simple voting strategy to diminish such label uncertainty or impose a strong assumption of labels with a pre-defined distribution, e.g., Gaussian. In this work, we unveil that the segment anything model (SAM) provides strong prior knowledge to model the uncertainty in edge labels. Our key insight is that the intermediate SAM features inherently correspond to object edges at various granularities, which reflects different edge options due to uncertainty. Therefore, we attempt to align uncertainty with granularity by regressing intermediate SAM features from different layers to object edges at multi-granularity levels. In doing so, the model can fully and explicitly explore diverse ``uncertainties'' in a data-driven fashion. Specifically, we inject a lightweight module (~ 1.5% additional parameters) into the frozen SAM to progressively fuse and adapt its intermediate features to estimate edges from coarse to fine. It is crucial to normalize the granularity level of human edge labels to match their innate uncertainty. For this, we simply perform linear blending to the real edge labels at hand to create pseudo labels with varying granularities. Consequently, our uncertainty-aligned edge detector can flexibly produce edges at any desired granularity (including an optimal one). Thanks to SAM, our model uniquely demonstrates strong generalizability for cross-dataset edge detection. Extensive experimental results on BSDS500, Muticue and NYUDv2 validate our model's superiority.", 'abstract_zh': '边缘标签通常具有不同的粒度层次，这是由于注释者的不同偏好造成的，因此处理像素级标签的主观性一直是边缘检测的研究焦点。以往的方法常采用简单的投票策略来减轻标签不确定性，或者对标签施加一种强假设，例如，高斯分布。在本文中，我们揭示了段切所有一切模型（SAM）为建模边缘标签的不确定性提供了强大的先验知识。我们的关键洞察是，SAM的中间特征本征上对应于不同粒度层次的对象边缘，这反映了不确定性带来的不同边缘选项。因此，我们尝试通过从不同层回归中间SAM特征来对齐粒度和不确定性，在多粒度层次上估计对象边缘。这样，模型可以完全且明确地以数据驱动的方式探索各种“不确定性”。具体而言，我们向冻结的SAM中注入一个轻量级模块（额外参数约1.5%），逐步融合和适应其中间特征，从粗糙到精细估计边缘。为了匹配人类边缘标签的固有不确定性的粒度层次，我们简单地对手头的真实边缘标签进行线性混合，创建具有不同粒度层次的伪标签。由此，我们的不确定性对齐的边缘检测器可以灵活地在任何所需的粒度层次（包括最优粒度层次）生成边缘。得益于SAM，我们的模型在跨数据集边缘检测中展现了强大的泛化能力。在BSDS500、Muticue和NYUDv2上的大量实验结果验证了我们模型的优越性。', 'title_zh': 'SAUGE：调控SAM以实现不确定性对齐的多粒度边缘检测\n\n这里对标题进行了翻译，保持了原有的技术术语并符合学术规范。其中，“SAUGE” 是论文的名称或缩写，通常不需要翻译；“SAM” 可能指某种特定的网络架构或方法，在这里直接保留为原词，并在翻译时加引号以示说明：“调控 SAM” 意指对该特定架构或方法进行调整或优化；“不确定性对齐” 和 “多粒度边缘检测” 是对相应英文术语的准确翻译。'}
{'arxiv_id': 'arXiv:2412.12888', 'title': 'ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction', 'authors': 'Zhongjie Duan, Qianyi Zhao, Cen Chen, Daoyuan Chen, Wenmeng Zhou, Yaliang Li, Yingda Chen', 'link': 'https://arxiv.org/abs/2412.12888', 'abstract': 'The emergence of diffusion models has significantly advanced image synthesis. The recent studies of model interaction and self-corrective reasoning approach in large language models offer new insights for enhancing text-to-image models. Inspired by these studies, we propose a novel method called ArtAug for enhancing text-to-image models in this paper. To the best of our knowledge, ArtAug is the first one that improves image synthesis models via model interactions with understanding models. In the interactions, we leverage human preferences implicitly learned by image understanding models to provide fine-grained suggestions for image synthesis models. The interactions can modify the image content to make it aesthetically pleasing, such as adjusting exposure, changing shooting angles, and adding atmospheric effects. The enhancements brought by the interaction are iteratively fused into the synthesis model itself through an additional enhancement module. This enables the synthesis model to directly produce aesthetically pleasing images without any extra computational cost. In the experiments, we train the ArtAug enhancement module on existing text-to-image models. Various evaluation metrics consistently demonstrate that ArtAug enhances the generative capabilities of text-to-image models without incurring additional computational costs. The source code and models will be released publicly.', 'abstract_zh': '扩散模型的出现显著推进了图像合成技术的发展。近期在大语言模型中研究的模型交互和自我纠错推理方法为提升文本到图像模型提供了新的见解。受这些研究的启发，本文提出了一种名为ArtAug的新型方法，用于增强文本到图像模型。据我们所知，ArtAug是首个通过模型交互来改善图像合成模型的方法。在交互过程中，我们利用图像理解模型隐含学习到的人类偏好，为图像合成模型提供精细的建议。这些交互能够调整图像内容，使其更具美感，例如调整曝光、改变拍摄角度和添加大气效果。通过附加的增强模块，交互带来的增强效果将迭代融合到合成模型本身中，从而使得合成模型能够直接生成具有美感的图像而无需额外的计算成本。在实验中，我们在现有的文本到图像模型上训练了ArtAug增强模块。各种评估指标一致表明，ArtAug在不增加额外计算成本的情况下，显著提升了文本到图像模型的生成能力。源代码和模型将在公开渠道发布。', 'title_zh': 'ArtAug：通过合成-理解交互增强文本到图像生成'}
{'arxiv_id': 'arXiv:2412.12883', 'title': 'A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting', 'authors': 'Nicholas Kiefer, Arvid Weyrauch, Muhammed Öz, Achim Streit, Markus Götz, Charlotte Debus', 'link': 'https://arxiv.org/abs/2412.12883', 'abstract': 'The current landscape in time-series forecasting is dominated by Transformer-based models. Their high parameter count and corresponding demand in computational resources pose a challenge to real-world deployment, especially for commercial and scientific applications with low-power embedded devices. Pruning is an established approach to reduce neural network parameter count and save compute. However, the implications and benefits of pruning Transformer-based models for time series forecasting are largely unknown. To close this gap, we provide a comparative benchmark study by evaluating unstructured and structured pruning on various state-of-the-art multivariate time series models. We study the effects of these pruning strategies on model predictive performance and computational aspects like model size, operations, and inference time. Our results show that certain models can be pruned even up to high sparsity levels, outperforming their dense counterpart. However, fine-tuning pruned models is necessary. Furthermore, we demonstrate that even with corresponding hardware and software support, structured pruning is unable to provide significant time savings.', 'abstract_zh': '当前时间序列预测领域的景观主要由基于变换器（Transformer）的模型主导。这些模型参数量庞大，对计算资源的需求也相应增加，这在实际部署中构成了挑战，尤其是在商业和科学应用中的低功耗嵌入式设备上。剪枝是一种已验证的方法，可以减少神经网络的参数数量并节省计算资源。然而，对于基于变换器的时间序列预测模型，剪枝的影响和益处仍然知之甚少。为了填补这一空白，我们通过评估结构化和非结构化剪枝在各种最新的多变量时间序列模型上的性能，提供了一组比较基准研究。我们探讨了这些剪枝策略对模型预测性能以及计算方面（如模型大小、操作数和推理时间）的影响。\n\n我们的结果显示，某些模型甚至可以在极高的稀疏度下被剪枝，其性能优于全连接模型。然而，对剪枝后的模型进行微调是必要的。此外，我们还表明，即使有相应的硬件和软件支持，结构化剪枝也无法提供显著的时间节省。', 'title_zh': '基于变压器的时间序列预测中剪枝方法的比较研究'}
{'arxiv_id': 'arXiv:2412.12881', 'title': 'RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement', 'authors': 'Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, Tao Zhang', 'link': 'https://arxiv.org/abs/2412.12881', 'abstract': 'Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.', 'abstract_zh': '现有的大型语言模型（LLMs）在问题解决能力上表现出色，但可能在复杂推理任务上面临挑战。尽管链式思考和基于树的搜索方法取得了成功，它们主要依赖于LLMs的内部知识来进行中间推理步骤的搜索，局限于处理涉及较少推理步骤的简单任务。本文提出了一种名为**RAG-Star**的新型RAG方法，该方法通过检索信息来指导依赖LLMs固有知识的基于树的详尽推理过程。通过利用蒙特卡洛树搜索（MCTS），RAG-Star迭代地规划基于LLMs本身的推理中间子查询和答案。为了巩固内部和外部知识，我们提出了一种检索增强验证方法，该方法利用查询和答案感知的奖励建模为LLMs的固有推理提供反馈。我们的实验涉及Llama-3.1-8B-Instruct和GPT-4o，结果显示RAG-Star显著优于之前的RAG和推理方法。', 'title_zh': 'RAG-Star：通过检索增强验证与精炼提升审慎推理'}
{'arxiv_id': 'arXiv:2412.12863', 'title': 'DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check', 'authors': 'Ziheng Qiao, Houquan Zhou, Yumeng Liu, Zhenghua Li, Min Zhang, Bo Zhang, Chen Li, Ji Zhang, Fei Huang', 'link': 'https://arxiv.org/abs/2412.12863', 'abstract': 'One key characteristic of the Chinese spelling check (CSC) task is that incorrect characters are usually similar to the correct ones in either phonetics or glyph. To accommodate this, previous works usually leverage confusion sets, which suffer from two problems, i.e., difficulty in determining which character pairs to include and lack of probabilities to distinguish items in the set. In this paper, we propose a light-weight plug-and-play DISC (i.e., decoding intervention with similarity of characters) module for CSC this http URL measures phonetic and glyph similarities between characters and incorporates this similarity information only during the inference phase. This method can be easily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and ReLM, without additional training costs. Experiments on three CSC benchmarks demonstrate that our proposed method significantly improves model performance, approaching and even surpassing the current state-of-the-art models.', 'abstract_zh': '中国拼写检查（CSC）任务的一个关键特征是错误的字符通常在音韵或图形上与正确的字符非常接近。为适应这一特征，以往的工作通常依赖于混淆集，但这种方法存在两个问题：难以确定哪些字符对应该包含在内，以及缺乏区分集内项目的概率。在本文中，我们提出了一种轻量级的即插即用DISC模块（即解码干预相似性模块），用于CSC任务。该模块在推断阶段仅通过测量字符之间的音韵和图形相似性来整合相似性信息。该方法可以轻松集成到诸如ReaLiSe、SCOPE和ReLM等现有的各种CSC模型中，而无需额外的训练成本。在三个CSC基准测试中的实验表明，我们提出的方法显著提升了模型性能，甚至超过了当前最先进的模型。', 'title_zh': 'DISC：基于字符相似性的插件式解码干预方法用于中文拼写检查'}
{'arxiv_id': 'arXiv:2412.12858', 'title': 'Efficient Speech Command Recognition Leveraging Spiking Neural Network and Curriculum Learning-based Knowledge Distillation', 'authors': 'Jiaqi Wang, Liutao Yu, Liwei Huang, Chenlin Zhou, Han Zhang, Zhenxi Song, Min Zhang, Zhengyu Ma, Zhiguo Zhang', 'link': 'https://arxiv.org/abs/2412.12858', 'abstract': "The intrinsic dynamics and event-driven nature of spiking neural networks (SNNs) make them excel in processing temporal information by naturally utilizing embedded time sequences as time steps. Recent studies adopting this approach have demonstrated SNNs' effectiveness in speech command recognition, achieving high performance by employing large time steps for long time sequences. However, the large time steps lead to increased deployment burdens for edge computing applications. Thus, it is important to balance high performance and low energy consumption when detecting temporal patterns in edge devices. Our solution comprises two key components. 1). We propose a high-performance fully spike-driven framework termed SpikeSCR, characterized by a global-local hybrid structure for efficient representation learning, which exhibits long-term learning capabilities with extended time steps. 2). To further fully embrace low energy consumption, we propose an effective knowledge distillation method based on curriculum learning (KDCL), where valuable representations learned from the easy curriculum are progressively transferred to the hard curriculum with minor loss, striking a trade-off between power efficiency and high performance. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands (GSC) V2. Our experimental results demonstrate that SpikeSCR outperforms current state-of-the-art (SOTA) methods across these three datasets with the same time steps. Furthermore, by executing KDCL, we reduce the number of time steps by 60% and decrease energy consumption by 54.8% while maintaining comparable performance to recent SOTA results. Therefore, this work offers valuable insights for tackling temporal processing challenges with long time sequences in edge neuromorphic computing systems.", 'abstract_zh': '具有固有动态特性和事件驱动性质的脉冲神经网络（SNNs）能够自然地利用嵌入的时间序列作为时间步长来处理时间信息。最近的研究采用这种方法证明了SNNs在语音命令识别方面的有效性，通过使用较长的时间步长来处理长时间序列，能够取得高性能。然而，较大的时间步长增加了边缘计算应用的部署负担。因此，在边缘设备中检测时间模式时，平衡高性能和低能耗显得尤为重要。我们的解决方案包括两个关键组件：\n\n1). 我们提出了一种高性能的全脉冲驱动框架，命名为SpikeSCR，其特征是采用全局-局部混合结构进行高效的表示学习，能够利用扩展的时间步长实现长期学习能力。\n\n2). 为了进一步实现低能耗，我们提出了一种基于课程学习的有效知识蒸馏方法（KDCL）。这种方法通过逐步将容易课程中学到的重要表示转移到困难课程中，同时在轻微损失的情况下实现能耗效率和高性能之间的权衡。\n\n我们在三个基准数据集上评估了我们的方法：Heidelberg脉冲数据集（SHD）、脉冲语音命令数据集（SSC）和Google语音命令数据集V2（GSC）。实验结果表明，SpikeSCR在这些数据集上与相同时间步长的当前最先进的（SOTA）方法相比，具有更好的性能。此外，通过执行KDCL，我们减少了60%的时间步长数量，并通过减少54.8%的能量消耗，同时保持与最近SOTA结果相当的性能。因此，这项工作为解决边缘神经形态计算系统中长时间序列的时间处理挑战提供了宝贵见解。', 'title_zh': '基于脉冲神经网络和基于 Curriculum Learning 的知识蒸馏的高效语音命令识别'}
{'arxiv_id': 'arXiv:2412.12850', 'title': 'Boosting Fine-Grained Visual Anomaly Detection with Coarse-Knowledge-Aware Adversarial Learning', 'authors': 'Qingqing Fang, Qinliang Su, Wenxi Lv, Wenchao Xu, Jianxing Yu', 'link': 'https://arxiv.org/abs/2412.12850', 'abstract': "Many unsupervised visual anomaly detection methods train an auto-encoder to reconstruct normal samples and then leverage the reconstruction error map to detect and localize the anomalies. However, due to the powerful modeling and generalization ability of neural networks, some anomalies can also be well reconstructed, resulting in unsatisfactory detection and localization accuracy. In this paper, a small coarsely-labeled anomaly dataset is first collected. Then, a coarse-knowledge-aware adversarial learning method is developed to align the distribution of reconstructed features with that of normal features. The alignment can effectively suppress the auto-encoder's reconstruction ability on anomalies and thus improve the detection accuracy. Considering that anomalies often only occupy very small areas in anomalous images, a patch-level adversarial learning strategy is further developed. Although no patch-level anomalous information is available, we rigorously prove that by simply viewing any patch features from anomalous images as anomalies, the proposed knowledge-aware method can also align the distribution of reconstructed patch features with the normal ones. Experimental results on four medical datasets and two industrial datasets demonstrate the effectiveness of our method in improving the detection and localization performance.", 'abstract_zh': '许多无监督视觉异常检测方法通过训练自编码器重构正常样本，然后利用重构误差图来检测和定位异常。然而，由于神经网络的强大建模能力和泛化能力，一些异常也可能被良好地重构，导致检测和定位准确度不佳。在本文中，首先收集了一个小规模的粗略标注的异常数据集。然后，开发了一种粗知觉知识感知的对抗学习方法，以使重构特征的分布与正常特征的分布对齐。这种对齐能够有效抑制自编码器在异常上的重构能力，从而提高检测准确度。鉴于异常通常仅占据异常图像中的极小区域，进一步开发了一种基于补丁的对抗学习策略。尽管没有补丁级别的异常信息，但我们通过严格证明，只要将任何补丁特征视作异常，提出的知识感知方法也能使重构补丁特征的分布与正常特征分布对齐。实验结果表明，在四个医学数据集和两个工业数据集上，本方法能够在提高检测和定位性能方面表现出有效性。', 'title_zh': '基于粗知识意识对抗学习的细粒度视觉异常检测增强方法'}
{'arxiv_id': 'arXiv:2412.12848', 'title': 'ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models', 'authors': 'Yuxi Sun, Wei Gao, Jing Ma, Hongzhan Lin, Ziyang Luo, Wenxuan Zhang', 'link': 'https://arxiv.org/abs/2412.12848', 'abstract': 'With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., "don\'t cheat,"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \\textit{ClarityEthic} that leverages LLMs\' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.', 'abstract_zh': '随着大型语言模型（LLMs）的兴起及其广泛应用，确保其安全性变得至关重要，以防止对人类造成伤害并促进道德行为。然而，通过大规模数据训练直接评估价值倾向（即支持或反对）是不可靠且难以解释的。我们假设让LLMs模仿人类依靠社会规范来做出道德决策，有助于它们理解和预测道德判断。然而，捕捉人类价值仍然充满挑战，因为在特定背景下，多种相关的规范可能会发生冲突。考虑那些被大多数人接受并促进社会福祉的规范（例如，“不欺骗”）更容易被接受和广泛采纳。因此，在做出道德决策之前，了解适用于特定场景的适当规范对于LLMs来说至关重要。为实现这一目标，我们提出了一种名为**ClarityEthic**的新型道德判断方法，该方法利用LLMs的推理能力和对比学习，从不同角度揭示与人类行为相关的社会规范，并选择最可靠的规范以提高判断准确性。大量的实验证明，我们的方法在道德判断任务中优于现有最先进的方法。此外，人类评估确认生成的社会规范为判断提供了合理的解释，这表明模仿人类的道德策略建模人类的道德判断有望提高LLMs的伦理行为。', 'title_zh': 'ClarityEthic: 可解释的道德判断利用大规模语言模型的对比伦理洞察'}
{'arxiv_id': 'arXiv:2412.12843', 'title': 'Efficient Event-based Semantic Segmentation with Spike-driven Lightweight Transformer-based Networks', 'authors': 'Xiaxin Zhu, Fangming Guo, Xianlei Long, Qingyi Gu, Chao Chen, Fuqiang Gu', 'link': 'https://arxiv.org/abs/2412.12843', 'abstract': "Event-based semantic segmentation has great potential in autonomous driving and robotics due to the advantages of event cameras, such as high dynamic range, low latency, and low power cost. Unfortunately, current artificial neural network (ANN)-based segmentation methods suffer from high computational demands, the requirements for image frames, and massive energy consumption, limiting their efficiency and application on resource-constrained edge/mobile platforms. To address these problems, we introduce SLTNet, a spike-driven lightweight transformer-based network designed for event-based semantic segmentation. Specifically, SLTNet is built on efficient spike-driven convolution blocks (SCBs) to extract rich semantic features while reducing the model's parameters. Then, to enhance the long-range contextural feature interaction, we propose novel spike-driven transformer blocks (STBs) with binary mask operations. Based on these basic blocks, SLTNet employs a high-efficiency single-branch architecture while maintaining the low energy consumption of the Spiking Neural Network (SNN). Finally, extensive experiments on DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms state-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU, respectively, with extremely 5.48x lower energy consumption and 1.14x faster inference speed.", 'abstract_zh': '基于事件的语义分割在自主驾驶和机器人技术中具有巨大的潜力，得益于事件相机的高动态范围、低延迟和低功耗等优势。然而，目前基于人工神经网络（ANN）的分割方法存在着高计算需求、对图像帧的依赖以及巨大的能源消耗，这限制了它们在资源受限的边缘/移动平台上的效率和应用。为解决这些问题，我们提出了一种名为SLTNet的基于尖峰驱动的轻量级变压器网络，专为基于事件的语义分割设计。具体而言，SLTNet基于高效的尖峰驱动卷积块（SCB）构建，以提取丰富的语义特征的同时减少模型参数量。为了增强长程上下文特征交互，我们还提出了一种新颖的尖峰驱动变压器块（STB），并采用二值掩码操作。基于这些基本模块，SLTNet采用了高效率的单分支架构，同时保持了尖峰神经网络（SNN）的低能耗特性。最后，我们在DDD17和DSEC-Semantic数据集上的广泛实验表明，SLTNet在mIoU指标上分别优于最新尖峰神经网络（SNN）方法至少7.30%和3.30%，且能耗仅为其5.48倍，推理速度则快了1.14倍。', 'title_zh': '基于 Spike 驱动的轻量级 transformer 网络的高效事件语义分割'}
{'arxiv_id': 'arXiv:2412.12836', 'title': 'A Survey on Recommendation Unlearning: Fundamentals, Taxonomy, Evaluation, and Open Questions', 'authors': 'Yuyuan Li, Xiaohua Feng, Chaochao Chen, Qiang Yang', 'link': 'https://arxiv.org/abs/2412.12836', 'abstract': 'Recommender systems have become increasingly influential in shaping user behavior and decision-making, highlighting their growing impact in various domains. Meanwhile, the widespread adoption of machine learning models in recommender systems has raised significant concerns regarding user privacy and security. As compliance with privacy regulations becomes more critical, there is a pressing need to address the issue of recommendation unlearning, i.e., eliminating the memory of specific training data from the learned recommendation models. Despite its importance, traditional machine unlearning methods are ill-suited for recommendation unlearning due to the unique challenges posed by collaborative interactions and model parameters. This survey offers a comprehensive review of the latest advancements in recommendation unlearning, exploring the design principles, challenges, and methodologies associated with this emerging field. We provide a unified taxonomy that categorizes different recommendation unlearning approaches, followed by a summary of widely used benchmarks and metrics for evaluation. By reviewing the current state of research, this survey aims to guide the development of more efficient, scalable, and robust recommendation unlearning techniques. Furthermore, we identify open research questions in this field, which could pave the way for future innovations not only in recommendation unlearning but also in a broader range of unlearning tasks across different machine learning applications.', 'abstract_zh': '推荐系统在塑造用户行为和决策方面的作用越来越大，这突显了它们在各个领域的日益重要影响。同时，推荐系统中广泛采用的机器学习模型引发了对用户隐私和安全的重大担忧。随着遵守隐私法规变得越来越重要，消除推荐记忆的需求显得尤为重要，即从已学习的推荐模型中消除特定训练数据的记忆。尽管这一问题至关重要，但传统的机器遗忘方法由于协作交互和模型参数的独特挑战，不适用于推荐系统的遗忘处理。本文综述了推荐遗忘领域的最新进展，探讨了这一新兴领域的设计原则、挑战和方法论。我们提供了一个统一的分类体系，对不同的推荐遗忘方法进行了分类，并总结了常用的基准和评估指标。通过回顾当前的研究状态，本文旨在引导更高效、更具扩展性和更稳健的推荐遗忘技术的发展。此外，我们还指出了该领域存在的开放性研究问题，这可能会为推荐遗忘以及机器学习应用中不同退隐任务的未来创新铺平道路。', 'title_zh': '推荐学习删除综述：基础、分类、评估及开放问题'}
{'arxiv_id': 'arXiv:2412.12832', 'title': 'DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models', 'authors': 'Jinxiang Xie, Yilin Li, Xunjian Yin, Xiaojun Wan', 'link': 'https://arxiv.org/abs/2412.12832', 'abstract': 'Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.', 'abstract_zh': '评估语法错误修正（GEC）模型的性能已变得越来越具有挑战性，因为基于大规模语言模型（LLM）的GEC系统常常会生成与所提供的黄金标准参考有所偏离的修正。这种差异削弱了传统基于参考的评估指标的可靠性。在这项研究中，我们提出了一种新的GEC模型评估框架——DSGram，该框架结合了语义连贯性、编辑级别和流畅性，并利用了动态加权机制。我们的框架利用层次分析过程（AHP）与大规模语言模型相结合，确定不同评估指标的相对重要性。此外，我们还开发了一个包含人类标注和LLM模拟句子的数据集，以验证我们的算法并微调更具成本效益的模型。实验结果表明，我们提出的方法提升了GEC模型评估的有效性。', 'title_zh': 'DSGram：大型语言模型时代基于动态加权子度量的语法错误修正'}
{'arxiv_id': 'arXiv:2412.12808', 'title': 'Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning', 'authors': 'Ziqi Qiu, Jianxing Yu, Yufeng Zhang, Hanjiang Lai, Yanghui Rao, Qinliang Su, Jian Yin', 'link': 'https://arxiv.org/abs/2412.12808', 'abstract': 'This paper focuses on sarcasm detection, which aims to identify whether given statements convey criticism, mockery, or other negative sentiment opposite to the literal meaning. To detect sarcasm, humans often require a comprehensive understanding of the semantics in the statement and even resort to external commonsense to infer the fine-grained incongruity. However, existing methods lack commonsense inferential ability when they face complex real-world scenarios, leading to unsatisfactory performance. To address this problem, we propose a novel framework for sarcasm detection, which conducts incongruity reasoning based on commonsense augmentation, called EICR. Concretely, we first employ retrieval-augmented large language models to supplement the missing but indispensable commonsense background knowledge. To capture complex contextual associations, we construct a dependency graph and obtain the optimized topology via graph refinement. We further introduce an adaptive reasoning skeleton that integrates prior rules to extract sentiment-inconsistent subgraphs explicitly. To eliminate the possible spurious relations between words and labels, we employ adversarial contrastive learning to enhance the robustness of the detector. Experiments conducted on five datasets demonstrate the effectiveness of EICR.', 'abstract_zh': '本文专注于语境讽刺检测研究，其目标是识别给定陈述是否传达了与字面意思相反的批评、嘲笑或其他负面情感。为了检测讽刺，人类通常需要全面理解语句的语义，并且甚至需要利用常识推理来推断细微的不一致。然而，现有的方法在面对复杂的真实世界场景时缺乏常识推理能力，导致性能不佳。为了解决这一问题，我们提出了一种新颖的讽刺检测框架，该框架基于常识增强进行不一致推理，称为EICR。具体而言，我们首先采用检索增强的大语言模型来补充缺失但不可或缺的常识背景知识。为了捕捉复杂的上下文关联，我们构建了一个依赖图，并通过图优化获得最佳拓扑结构。进一步引入一种适应性推理框架，该框架整合先验规则以明确提取情感不一致的子图。为了消除单词与标签之间可能的虚假关系，我们采用了对抗对比学习以增强检测器的鲁棒性。在五个数据集上的实验结果表明了EICR的有效性。', 'title_zh': '通过常识推理检测讽刺中的情感不一致'}
{'arxiv_id': 'arXiv:2412.12800', 'title': 'Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners', 'authors': 'James Prather, Brent N. Reeves, Paul Denny, Juho Leinonen, Stephen MacNeil, Andrew Luxton-Reilly, João Orvalho, Amin Alipour, Ali Alfageeh, Thezyrie Amarouche, Bailey Kimmel, Jared Wright, Musa Blake, Gweneth Barbre', 'link': 'https://arxiv.org/abs/2412.12800', 'abstract': 'Non-native English speakers (NNES) face multiple barriers to learning programming. These barriers can be obvious, such as the fact that programming language syntax and instruction are often in English, or more subtle, such as being afraid to ask for help in a classroom full of native English speakers. However, these barriers are frustrating because many NNES students know more about programming than they can articulate in English. Advances in generative AI (GenAI) have the potential to break down these barriers because state of the art models can support interactions in multiple languages. Moreover, recent work has shown that GenAI can be highly accurate at code generation and explanation. In this paper, we provide the first exploration of NNES students prompting in their native languages (Arabic, Chinese, and Portuguese) to generate code to solve programming problems. Our results show that students are able to successfully use their native language to solve programming problems, but not without some difficulty specifying programming terminology and concepts. We discuss the challenges they faced, the implications for practice in the short term, and how this might transform computing education globally in the long term.', 'abstract_zh': '非英语母语使用者（NNES）在学习编程时面临着多重障碍。这些障碍可能是显而易见的，例如编程语言的语法和指令通常使用英语；也可能是更为微妙的，例如在全是英语母语者的课堂中不敢寻求帮助。然而，这些障碍令人沮丧，因为许多NNES学生实际上比他们能够用英语表达的要了解更多的编程知识。生成式人工智能（GenAI）的最新进展有望打破这些障碍，因为最先进的模型能够支持多语言交互。此外，近期的研究表明，GenAI在代码生成和解释方面具有极高的准确性。在这篇论文中，我们首次探讨了NNES学生使用其母语（阿拉伯语、中文和葡萄牙语）生成代码以解决编程问题的情况。我们的结果显示，学生能够成功地使用母语解决编程问题，但他们在指定编程术语和概念时仍面临一些困难。我们讨论了他们所面临的挑战，短期实践的影响，以及这一发展在未来可能如何在全球范围内改变计算机教育。', 'title_zh': '打破编程语言障碍：多语言提示赋能非英语母语学习者'}
{'arxiv_id': 'arXiv:2412.12799', 'title': 'RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential Decoder for 3D Object Detection', 'authors': 'Yiheng Li, Yang Yang, Zhen Lei', 'link': 'https://arxiv.org/abs/2412.12799', 'abstract': 'In radar-camera 3D object detection, the radar point clouds are sparse and noisy, which causes difficulties in fusing camera and radar modalities. To solve this, we introduce a novel query-based detection method named Radar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense Encoder to enrich the sparse valid radar tokens, and then concatenate them with the image tokens. By doing this, we can fully explore the 3D information of each interest region and reduce the interference of empty tokens during the fusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes based on the obtained tokens and random initialized queries. To alleviate the effect of elevation ambiguity in radar point clouds, we gradually locate the position of the object via a sequential fusion structure. It helps to get more precise and flexible correspondences between tokens and queries. A pruning training strategy is adopted in the decoder, which can save much time during inference and inhibit queries from losing their distinctiveness. Extensive experiments on the large-scale nuScenes dataset prove the superiority of our method, and we also achieve new state-of-the-art radar-camera 3D detection results. Our implementation is available at this https URL.', 'abstract_zh': '在雷达-摄像头3D目标检测中，雷达点云稀疏且噪声较大，这给雷达和摄像头模态的融合带来了困难。为了解决这个问题，我们提出了一种新颖的基于查询的目标检测方法，名为雷达-摄像头变换器（RCTrans）。具体而言，我们首先设计了一个雷达密集编码器来丰富稀疏的有效雷达标记，然后将这些标记与图像标记连接起来。这样做可以在融合阶段充分利用每个兴趣区域的3D信息，并减少空标记的干扰。随后，我们设计了一个剪枝顺序解码器，根据获得的标记和随机初始化的查询预测3D框。为了缓解雷达点云中的仰角 ambiguity 问题，我们通过顺序融合结构逐步定位目标位置，这有助于获得标记与查询之间更精确和灵活的对应关系。在解码器中采用了剪枝训练策略，这不仅可以大大节省推理时间，还能抑制查询丢失其独特性。在大规模nuScenes数据集上的广泛实验表明了我们方法的优势，并且我们还达到了新的雷达-摄像头3D检测结果的最优水平。我们的实现可在此处获得：[这个 URL]。', 'title_zh': 'RCTRANS：通过雷达密集模块和序列解码器的雷达-摄像头变换器用于三维目标检测'}
{'arxiv_id': 'arXiv:2412.12791', 'title': 'Implicit Location-Caption Alignment via Complementary Masking for Weakly-Supervised Dense Video Captioning', 'authors': 'Shiping Ge, Qiang Chen, Zhiwei Jiang, Yafeng Yin, Liu Qin, Ziyao Chen, Qing Gu', 'link': 'https://arxiv.org/abs/2412.12791', 'abstract': 'Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and describe all events of interest in a video without requiring annotations of event boundaries. This setting poses a great challenge in accurately locating the temporal location of event, as the relevant supervision is unavailable. Existing methods rely on explicit alignment constraints between event locations and captions, which involve complex event proposal procedures during both training and inference. To tackle this problem, we propose a novel implicit location-caption alignment paradigm by complementary masking, which simplifies the complex event proposal and localization process while maintaining effectiveness. Specifically, our model comprises two components: a dual-mode video captioning module and a mask generation module. The dual-mode video captioning module captures global event information and generates descriptive captions, while the mask generation module generates differentiable positive and negative masks for localizing the events. These masks enable the implicit alignment of event locations and captions by ensuring that captions generated from positively and negatively masked videos are complementary, thereby forming a complete video description. In this way, even under weak supervision, the event location and event caption can be aligned implicitly. Extensive experiments on the public datasets demonstrate that our method outperforms existing weakly-supervised methods and achieves competitive results compared to fully-supervised methods.', 'abstract_zh': '弱监督密集视频描述（WSDVC）旨在无需标注事件边界的情况下定位和描述视频中的所有感兴趣事件。这一设置在准确定位事件的时间位置方面提出了巨大的挑战，因为缺乏相关的监督信息。现有方法依赖于事件位置和描述之间的显式对齐约束，这在训练和推理过程中都涉及复杂的事件提案过程。为了解决这一问题，我们提出了一种通过互补掩码实现的新颖隐式事件位置-描述对齐范式，简化了复杂的事件提案和定位过程，同时保持了有效性。具体而言，我们的模型包括两个组件：双模式视频描述模块和掩码生成模块。双模式视频描述模块捕获全局事件信息并生成描述性描述，而掩码生成模块为事件定位生成可微正负掩码。这些掩码通过确保正掩码和负掩码视频生成的描述互补，从而实现事件位置和描述的隐式对齐，形成完整的视频描述。这样，在弱监督的情况下，事件位置和事件描述可以隐式对齐。在公共数据集上的大量实验表明，我们的方法优于现有的弱监督方法，并且与全监督方法相比取得了竞争力的结果。', 'title_zh': '弱监督密集视频captioning中基于互补掩码的隐式位置-描述对齐方法'}
{'arxiv_id': 'arXiv:2412.12781', 'title': 'Predicting change in time production -- A machine learning approach to time perception', 'authors': 'Amrapali Pednekar, Alvaro Garrido, Yara Khaluf, Pieter Simoens', 'link': 'https://arxiv.org/abs/2412.12781', 'abstract': "Time perception research has advanced significantly over the years. However, some areas remain largely unexplored. This study addresses two such under-explored areas in timing research: (1) A quantitative analysis of time perception at an individual level, and (2) Time perception in an ecological setting. In this context, we trained a machine learning model to predict the direction of change in an individual's time production. The model's training data was collected using an ecologically valid setup. We moved closer to an ecological setting by conducting an online experiment with 995 participants performing a time production task that used naturalistic videos (no audio) as stimuli. The model achieved an accuracy of 61%. This was 10 percentage points higher than the baseline models derived from cognitive theories of timing. The model performed equally well on new data from a second experiment, providing evidence of its generalization capabilities. The model's output analysis revealed that it also contained information about the magnitude of change in time production. The predictions were further analysed at both population and individual level. It was found that a participant's previous timing performance played a significant role in determining the direction of change in time production. By integrating attentional-gate theories from timing research with feature importance techniques from machine learning, we explained model predictions using cognitive theories of timing. The model and findings from this study have potential applications in systems involving human-computer interactions where understanding and predicting changes in user's time perception can enable better user experience and task performance.", 'abstract_zh': '时间知觉研究在过去几年中取得了显著进展，但仍然有一些领域尚未充分探索。本研究关注时间研究中两个未充分探索的领域：（1）个体层面时间知觉的量化分析，以及（2）生态环境下的时间知觉。在此背景下，我们训练了一个机器学习模型，以预测个体时间生产变化的方向。模型的训练数据采用了生态有效的设置收集。通过一项在线实验，我们让995名参与者完成了一个使用自然场景视频（无声）作为刺激的时间生产任务，从而更接近生态场景。模型的预测准确率为61%，比基于时间认知理论的基本模型高出10个百分点。该模型在第二次实验的新数据上表现也同样优秀，证明了其泛化能力。模型输出分析表明，它还包含了时间生产量级变化的信息。我们在群体和个体层面上进一步分析了预测结果，发现参与者之前的时间表现对其时间生产变化方向有显著影响。通过将时间研究中的注意力门控理论与机器学习中的特征重要性技术相结合，我们使用时间认知理论来解释模型预测。本研究中的模型及其发现可能在涉及人机交互的系统中应用，通过理解和预测用户时间知觉的变化，可以使用户体验和任务表现更加优化。', 'title_zh': '时间生产的变化预测——一种时间感知的机器学习方法'}
{'arxiv_id': 'arXiv:2412.12778', 'title': 'Rethinking Diffusion-Based Image Generators for Fundus Fluorescein Angiography Synthesis on Limited Data', 'authors': 'Chengzhou Yu, Huihui Fang, Hongqiu Wang, Ting Deng, Qing Du, Yanwu Xu, Weihua Yang', 'link': 'https://arxiv.org/abs/2412.12778', 'abstract': 'Fundus imaging is a critical tool in ophthalmology, with different imaging modalities offering unique advantages. For instance, fundus fluorescein angiography (FFA) can accurately identify eye diseases. However, traditional invasive FFA involves the injection of sodium fluorescein, which can cause discomfort and risks. Generating corresponding FFA images from non-invasive fundus images holds significant practical value but also presents challenges. First, limited datasets constrain the performance and effectiveness of models. Second, previous studies have primarily focused on generating FFA for single diseases or single modalities, often resulting in poor performance for patients with various ophthalmic conditions. To address these issues, we propose a novel latent diffusion model-based framework, Diffusion, which introduces a fine-tuning protocol to overcome the challenge of limited medical data and unleash the generative capabilities of diffusion models. Furthermore, we designed a new approach to tackle the challenges of generating across different modalities and disease types. On limited datasets, our framework achieves state-of-the-art results compared to existing methods, offering significant potential to enhance ophthalmic diagnostics and patient care. Our code will be released soon to support further research in this field.', 'abstract_zh': '眼底成像是眼科中的一项关键工具，不同的成像模态提供了独特的优势。例如，眼底荧光素血管造影（FFA）能够准确识别眼部疾病。然而，传统的侵入性FFA需要注射钠荧光素，这可能导致不适和风险。从非侵入性眼底图像中生成相应的FFA图像具有重要的实际价值，但也面临着挑战。首先，受限的数据集限制了模型的性能和有效性。其次，前人的研究主要集中在为单一疾病或单一模态生成FFA，通常对多种眼病患者的表现不佳。为解决这些问题，我们提出了一种基于潜变量扩散模型的新框架——Diffusion，该框架引入了微调协议以克服有限医疗数据的挑战，并激发扩散模型的生成能力。此外，我们设计了一种新方法来应对不同模态和疾病类型的生成挑战。在有限的数据集上，我们的框架实现了现有的方法中的最佳结果，提供了在眼科诊断和患者护理方面显著增强的潜力。我们的代码将在不久后发布，以支持这一领域的进一步研究。', 'title_zh': '基于扩散模型的视网膜荧光血管造影生成重新思考：在有限数据情况下'}
{'arxiv_id': 'arXiv:2412.12771', 'title': 'Guided and Variance-Corrected Fusion with One-shot Style Alignment for Large-Content Image Generation', 'authors': 'Shoukun Sun, Min Xian, Tiankai Yao, Fei Xu, Luca Capriotti', 'link': 'https://arxiv.org/abs/2412.12771', 'abstract': 'Producing large images using small diffusion models is gaining increasing popularity, as the cost of training large models could be prohibitive. A common approach involves jointly generating a series of overlapped image patches and obtaining large images by merging adjacent patches. However, results from existing methods often exhibit obvious artifacts, e.g., seams and inconsistent objects and styles. To address the issues, we proposed Guided Fusion (GF), which mitigates the negative impact from distant image regions by applying a weighted average to the overlapping regions. Moreover, we proposed Variance-Corrected Fusion (VCF), which corrects data variance at post-averaging, generating more accurate fusion for the Denoising Diffusion Probabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA), which generates a coherent style for large images by adjusting the initial input noise without adding extra computational burden. Extensive experiments demonstrated that the proposed fusion methods improved the quality of the generated image significantly. As a plug-and-play module, the proposed method can be widely applied to enhance other fusion-based methods for large image generation.', 'abstract_zh': '使用小型扩散模型生成大尺寸图像正逐渐变得流行，因为训练大型模型的成本可能是 prohibitive 的。一种常见的方法是同时生成一系列重叠的图像块，并通过合并相邻的块来获得大尺寸图像。然而，现有方法的结果常常表现出明显的伪影，例如接缝、不一致的对象和风格。为了解决这些问题，我们提出了指导融合（Guided Fusion, GF）方法，通过在重叠区域应用加权平均来减轻来自远距离图像区域的负面影响。此外，我们提出了方差校正融合（Variance-Corrected Fusion, VCF）方法，在加权平均后校正数据方差，从而为去噪扩散概率模型生成更准确的融合结果。进一步地，我们提出了一次性风格对齐（One-Shot Style Alignment, SA）方法，通过调节初始输入噪声来为大尺寸图像生成一致的风格，而不增加额外的计算负担。大量实验证明，所提出的融合方法显著提高了生成图像的质量。作为一种即插即用模块，所提出的方法可以广泛应用于增强其他基于融合的方法以生成大尺寸图像。', 'title_zh': '一种用于大型内容图像生成的一次性样式对齐、引导和方差校正融合方法'}
{'arxiv_id': 'arXiv:2412.12761', 'title': 'Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection', 'authors': 'Debajyoti Mazumder, Aakash Kumar, Jasabanta Patro', 'link': 'https://arxiv.org/abs/2412.12761', 'abstract': "In this paper, we reported our experiments with various strategies to improve code-mixed humour and sarcasm detection. We did all of our experiments for Hindi-English code-mixed scenario, as we have the linguistic expertise for the same. We experimented with three approaches, namely (i) native sample mixing, (ii) multi-task learning (MTL), and (iii) prompting very large multilingual language models (VMLMs). In native sample mixing, we added monolingual task samples in code-mixed training sets. In MTL learning, we relied on native and code-mixed samples of a semantically related task (hate detection in our case). Finally, in our third approach, we evaluated the efficacy of VMLMs via few-shot context prompting. Some interesting findings we got are (i) adding native samples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework boosted performance for both humour (raising the F1-score up to 10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting VMLMs couldn't outperform the other approaches. Finally, our ablation studies and error analysis discovered the cases where our model is yet to improve. We provided our code for reproducibility.", 'abstract_zh': '在本文中，我们报告了我们对提高代码混合幽默与讽刺检测的各种策略进行的实验。由于我们具备印度语-英语代码混合场景的语言学专业知识，所有实验均在该场景下进行。我们尝试了三种方法：（i）本地样本混合，（ii）多任务学习（MTL），以及（iii）提示大规模多语言语言模型（VMLMs）。在本地样本混合中，我们在代码混合的训练集中添加了单语言任务样本。在MTL学习中，我们依赖与代码混合任务在语义上相关的本地和代码混合样本进行训练（在我们的案例中为仇恨言论检测）。最后，在我们的第三种方法中，我们通过少量样本的上下文提示评估了VMLMs的效果。我们的一些有趣发现包括：（i）添加本地样本提高了幽默检测（F1分数提高6.76%）和讽刺检测（F1分数提高8.64%），（ii）在MTL框架下训练语言模型提高了幽默检测（F1分数提高10.67%）和讽刺检测（F1分数提高12.35%），以及（iii）提示VMLMs未能超越其他方法。最后，我们的消融研究和错误分析揭示了模型尚未改进的具体情况。我们提供了代码以确保可重现性。', 'title_zh': '揭示合成本地样本和多任务策略对印地语-英语代码混合幽默和讽刺检测的影响'}
{'arxiv_id': 'arXiv:2412.12744', 'title': 'Your Next State-of-the-Art Could Come from Another Domain: A Cross-Domain Analysis of Hierarchical Text Classification', 'authors': 'Nan Li, Bo Kang, Tijl De Bie', 'link': 'https://arxiv.org/abs/2412.12744', 'abstract': 'Text classification with hierarchical labels is a prevalent and challenging task in natural language processing. Examples include assigning ICD codes to patient records, tagging patents into IPC classes, assigning EUROVOC descriptors to European legal texts, and more. Despite its widespread applications, a comprehensive understanding of state-of-the-art methods across different domains has been lacking. In this paper, we provide the first comprehensive cross-domain overview with empirical analysis of state-of-the-art methods. We propose a unified framework that positions each method within a common structure to facilitate research. Our empirical analysis yields key insights and guidelines, confirming the necessity of learning across different research areas to design effective methods. Notably, under our unified evaluation pipeline, we achieved new state-of-the-art results by applying techniques beyond their original domains.', 'abstract_zh': '具有层次标签的文本分类是自然语言处理中一个普遍且具有挑战性的任务。这方面的例子包括为病人记录分配ICD代码、将专利归类到IPC类别、为欧洲法律文本分配EUROVOC描述词等。尽管此任务应用广泛，但不同领域的最先进的方法仍未进行全面的理解。在本文中，我们首次提供了不同领域最先进的方法的全面综述，并进行了实证分析。我们提出了一种统框架，将每种方法置于一个共同结构中，以便于研究。我们的实证分析得到了关键洞见和指导方针，证实了跨不同研究领域学习的必要性，以设计有效的方法。特别地，通过在统一的评估管道下应用超出其原始应用领域的技术，我们达到了新的先进成果。', 'title_zh': '你的下一个先进成果可能来自另一个领域：跨领域层级文本分类分析'}
{'arxiv_id': 'arXiv:2412.12742', 'title': 'Subspace Implicit Neural Representations for Real-Time Cardiac Cine MR Imaging', 'authors': 'Wenqi Huang, Veronika Spieker, Siying Xu, Gastao Cruz, Claudia Prieto, Julia Schnabel, Kerstin Hammernik, Thomas Kuestner, Daniel Rueckert', 'link': 'https://arxiv.org/abs/2412.12742', 'abstract': 'Conventional cardiac cine MRI methods rely on retrospective gating, which limits temporal resolution and the ability to capture continuous cardiac dynamics, particularly in patients with arrhythmias and beat-to-beat variations. To address these challenges, we propose a reconstruction framework based on subspace implicit neural representations for real-time cardiac cine MRI of continuously sampled radial data. This approach employs two multilayer perceptrons to learn spatial and temporal subspace bases, leveraging the low-rank properties of cardiac cine MRI. Initialized with low-resolution reconstructions, the networks are fine-tuned using spoke-specific loss functions to recover spatial details and temporal fidelity. Our method directly utilizes the continuously sampled radial k-space spokes during training, thereby eliminating the need for binning and non-uniform FFT. This approach achieves superior spatial and temporal image quality compared to conventional binned methods at the acceleration rate of 10 and 20, demonstrating potential for high-resolution imaging of dynamic cardiac events and enhancing diagnostic capability.', 'abstract_zh': '传统的心脏 cine MRI 方法依赖于回顾性门控技术，这限制了时间分辨率并影响了对连续心脏动态的捕捉能力，特别是在伴有心律失常和心搏变异的患者中。为了解决这些挑战，我们提出了一种基于子空间隐式神经表示的重建框架，用于实时心脏 cine MRI。该方法利用两个多层感知机来学习空间和时间子空间基，并利用心脏 cine MRI 的低秩特性。初始阶段使用低分辨率的重建结果，然后通过针对不同辐条的损失函数进行微调，以恢复空间细节和时间保真度。我们的方法直接利用训练期间连续采样的径向 k-空间辐条，从而消除了分箱和非均匀 FFT 的需求。这种方法在加速率为 10 和 20 的情况下，与传统的分箱方法相比，实现了更优的时空图像质量，显示出对动态心脏事件进行高分辨率成像的潜力，从而增强诊断能力。', 'title_zh': '实时心脏 cine MRI 成像的子空间隐式神经表示方法'}
{'arxiv_id': 'arXiv:2412.12735', 'title': 'GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models', 'authors': 'Mukai Li, Lei Li, Shansan Gong, Qi Liu', 'link': 'https://arxiv.org/abs/2412.12735', 'abstract': 'Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLMs while preserving their capacities in short context scenarios. Towards this goal, we make the best design choice through extensive experiment settings from data curation to context window extending and utilizing: (1) we analyze data sources and length distributions to construct ETVLM - a data recipe to balance the performance across scenarios; (2) we examine existing position extending methods, identify their limitations and propose M-RoPE++ as an enhanced approach; we also choose to solely instruction-tune the backbone with mixed-source data; (3) we discuss how to better utilize extended context windows and propose hybrid-resolution training. Built on the Qwen-VL series model, we propose Giraffe, which is effectively extended to 128K lengths. Evaluated on extensive long context VLM benchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves state-of-the-art performance among similarly sized open-source long VLMs and is competitive with commercial model GPT-4V. We will open-source the code, data, and models.', 'abstract_zh': '视觉语言模型（VLMs）在处理多模态输入方面展现了令人 impress 的能力，但在需要处理多张图片和高分辨率视频的应用场景下，如视觉代理，依然需要增强其长距离建模能力。此外，现有的开源 VLMs 缺乏系统地探索扩展其上下文长度的方法，而商用模型往往提供有限的细节。为了解决这一问题，我们旨在建立一个有效的解决方案，既能增强 VLMs 的长上下文性能，又不会影响其在短上下文场景中的能力。为此，我们通过从数据收集到上下文窗口扩展和利用的广泛实验设置中做出最佳设计选择：（1）我们分析数据源和长度分布，构造 ETVLM 数据食谱，以平衡不同场景中的性能表现；（2）我们评估现有的位置扩展方法，指出了其局限性，并提出了 M-RoPE++ 作为改进的方法；同时，我们选择使用混合数据对主干进行指令微调；（3）我们讨论了如何更好地利用扩展的上下文窗口，并提出了混合分辨率训练方法。基于 Qwen-VL 系列模型，我们提出了 Giraffe，使其能够扩展到 128K 长度。在诸如 VideoMME 和 Visual Haystacks 等广泛的长上下文 VLM 基准测试中，我们的 Giraffe 在同样规模的开放源代码长 VLMs 中达到了最先进的性能，并在性能上与商用模型 GPT-4V 具有竞争力。我们即将开源代码、数据和模型。', 'title_zh': 'GIRAFFE: 扩展视觉语言模型上下文长度的设计选择'}
{'arxiv_id': 'arXiv:2412.12722', 'title': 'Defending LVLMs Against Vision Attacks through Partial-Perception Supervision', 'authors': 'Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin Song Dong', 'link': 'https://arxiv.org/abs/2412.12722', 'abstract': "Recent studies have raised significant concerns regarding the vulnerability of Large Vision Language Models (LVLMs) to maliciously injected or perturbed input images, which can mislead their responses. Existing defense methods show that such vision attacks are sensitive to image modifications especially cropping, using majority voting across responses of modified images as corrected responses. However, these modifications often result in partial images and distort the semantics, which reduces response quality on clean images after voting. Instead of directly using responses from partial images for voting, we investigate using them to supervise the LVLM's responses to the original images. We propose a black-box, training-free method called DPS (Defense through Partial-Perception Supervision). In this approach, the model is prompted using the responses generated by a model that perceives only a partial image. With DPS, the model can adjust its response based on partial image understanding when under attack, while confidently maintaining its original response for clean input. Our findings show that the weak model can supervise the strong model: when faced with an attacked input, the strong model becomes less confident and adjusts its response based on the weak model's partial understanding, effectively defending against the attack. With clean input, it confidently maintains its original response. Empirical experiments show our method outperforms the baseline, cutting the average attack success rate by 76.3% across six datasets on three popular models.", 'abstract_zh': '最近的研究揭示了大型视觉语言模型（LVLMs）对恶意注入或扰动输入图像的高度脆弱性，这些恶意修改可以误导模型的响应。现有的防御方法表明，这类视觉攻击对图像修改（尤其是裁剪）非常敏感，通常通过在修改图像的响应中使用多数投票来获取纠正的响应。然而，这些修改往往会导致图像不完整，并且破坏了语义信息，这在进行投票后会降低对干净图像的响应质量。为避免直接使用不完整图像的响应进行投票，我们探讨了利用这些响应来监督LVLM对原始图像的响应。我们提出了一种无需训练的黑盒方法，称为DPS（通过部分感知监督进行防御）。在该方法中，模型在仅能够感知部分图像的模型生成的响应下被提示。通过DPS，模型可以在遭受攻击时根据部分图像的理解调整其响应，同时仍能自信地保持其对干净输入的原始响应。实验证据表明，弱模型可以监督强模型：当面对攻击输入时，强模型会变得不那么自信，并根据弱模型的部分理解调整其响应，从而有效地抵御攻击。对于干净输入，它会自信地保持其原始响应。实验表明，我们的方法在三种流行的模型上，六个数据集上，平均攻击成功率降低了76.3%，明显优于基线方法。', 'title_zh': '通过部分感知监督防御低层语言模型免受视觉攻击'}
{'arxiv_id': 'arXiv:2412.12700', 'title': 'ParMod: A Parallel and Modular Framework for Learning Non-Markovian Tasks', 'authors': 'Ruixuan Miao, Xu Lu, Cong Tian, Bin Yu, Zhenhua Duan', 'link': 'https://arxiv.org/abs/2412.12700', 'abstract': 'The commonly used Reinforcement Learning (RL) model, MDPs (Markov Decision Processes), has a basic premise that rewards depend on the current state and action only. However, many real-world tasks are non-Markovian, which has long-term memory and dependency. The reward sparseness problem is further amplified in non-Markovian scenarios. Hence learning a non-Markovian task (NMT) is inherently more difficult than learning a Markovian one. In this paper, we propose a novel \\textbf{Par}allel and \\textbf{Mod}ular RL framework, ParMod, specifically for learning NMTs specified by temporal logic. With the aid of formal techniques, the NMT is modulaized into a series of sub-tasks based on the automaton structure (equivalent to its temporal logic counterpart). On this basis, sub-tasks will be trained by a group of agents in a parallel fashion, with one agent handling one sub-task. Besides parallel training, the core of ParMod lies in: a flexible classification method for modularizing the NMT, and an effective reward shaping method for improving the sample efficiency. A comprehensive evaluation is conducted on several challenging benchmark problems with respect to various metrics. The experimental results show that ParMod achieves superior performance over other relevant studies. Our work thus provides a good synergy among RL, NMT and temporal logic.', 'abstract_zh': '常用的强化学习（RL）模型，马尔可夫决策过程（MDPs），其基本前提在于奖励仅依赖于当前状态和动作。然而，许多实际任务是非马尔可夫的，具有长期记忆和依赖性。在非马尔可夫场景中，稀疏奖励的问题进一步加剧。因此，学习非马尔可夫任务（NMT）相比学习马尔可夫任务来说更加困难。本文提出了一种新的**并行**和**模块化**的RL框架，ParMod，专门用于学习由时序逻辑指定的NMT。借助形式化技术，NMT被模块化为一系列基于自动机结构（相当于其时序逻辑的对应物）的子任务。在此基础上，通过一组智能体并行训练这些子任务，每个多智能体负责一个子任务。除了并行训练，ParMod的核心在于：一种灵活的分类方法，用于模块化NMT，以及一种有效的奖励塑造方法，以提高样本效率。进行了多项具有挑战性的基准问题评估，涵盖了多个评估指标。实验结果表明，ParMod在多个相关研究中表现更优。因此，我们的工作为RL、NMT和时序逻辑之间的良好协同提供了保障。', 'title_zh': 'ParMod：一种用于学习非马尔可夫任务的并行模块化框架'}
{'arxiv_id': 'arXiv:2412.12693', 'title': 'SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning for Vision-Language Models', 'authors': 'Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Boyang Li, Lu Wang', 'link': 'https://arxiv.org/abs/2412.12693', 'abstract': 'Current vision-language models may incorporate single-dimensional spatial cues, such as depth, object boundary, and basic spatial directions (e.g. left, right, front, back), yet often lack the multi-dimensional spatial reasoning necessary for human-like understanding and real-world applications. To address this gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation framework with a new human-annotated dataset to pinpoint model strengths and weaknesses, advancing from single-skill tasks to multi-skill tasks, and ultimately to complex reasoning tasks that require the integration of multiple spatial and visual cues with logical reasoning. Benchmark evaluation of state-of-the-art open-source models reveal significant shortcomings, especially in the abilities to understand distance and proximity, to reason from both allocentric and egocentric viewpoints, and to perform complex reasoning in a physical context. This work underscores the need for more advanced approaches to spatial understanding and reasoning, paving the way for improvements in vision-language models and their alignment with human-like spatial capabilities. The dataset will be open-sourced upon publication.', 'abstract_zh': '当前的多模态模型可能仅整合单一维度的空间线索，例如深度、物体边界和基本的空间方向（如左、右、前、后），但往往缺乏人类理解和现实应用所需的空间多维度推理能力。为解决这一问题，我们开发了SPHERE（Spatial Perception and Hierarchical Evaluation of REasoning），这是一种分层评估框架，并结合了一个新的由人类注释的数据集，从单一技能任务逐步发展到多技能任务，最终达到需要整合多种空间和视觉线索并结合逻辑推理的复杂推理任务。最先进的开源模型的基准评估揭示了其显著的不足之处，尤其是在理解距离和亲密性、从自参考和视角中心视角进行推理以及在物理环境中进行复杂推理方面的能力。本研究强调了需要更高级的空间理解和推理方法的重要性，为多模态模型的改进及其与人类空间能力的对齐铺平了道路。该数据集将在发表后开源。', 'title_zh': 'SPHERE：空间感知与推理的分层评估方法在视觉-语言模型中的应用'}
{'arxiv_id': 'arXiv:2412.12681', 'title': 'Everyday AR through AI-in-the-Loop', 'authors': 'Ryo Suzuki, Mar Gonzalez-Franco, Misha Sra, David Lindlbauer', 'link': 'https://arxiv.org/abs/2412.12681', 'abstract': "This workshop brings together experts and practitioners from augmented reality (AR) and artificial intelligence (AI) to shape the future of AI-in-the-loop everyday AR experiences. With recent advancements in both AR hardware and AI capabilities, we envision that everyday AR -- always-available and seamlessly integrated into users' daily environments -- is becoming increasingly feasible. This workshop will explore how AI can drive such everyday AR experiences. We discuss a range of topics, including adaptive and context-aware AR, generative AR content creation, always-on AI assistants, AI-driven accessible design, and real-world-oriented AI agents. Our goal is to identify the opportunities and challenges in AI-enabled AR, focusing on creating novel AR experiences that seamlessly blend the digital and physical worlds. Through the workshop, we aim to foster collaboration, inspire future research, and build a community to advance the research field of AI-enhanced AR.", 'abstract_zh': '此次研讨会汇集了增强现实（AR）和人工智能（AI）领域的专家和实践者，共同塑造AI嵌入日常AR体验的未来。随着AR硬件和AI能力的最新进展，我们设想日常AR——始终可用并无缝融入用户日常环境中的AR——变得日益可行。本次研讨会将探讨AI如何推动这种日常AR体验。\n\n我们将讨论一系列主题，包括自适应和情境感知AR、生成AR内容创作、始终可用的AI助手、AI驱动的无障碍设计以及面向现实世界的AI代理。我们的目标是在AI赋能AR中识别机遇与挑战，并专注于创造无缝融合虚拟与现实世界的新型AR体验。通过此次研讨会，我们旨在促进合作，激发未来研究，并构建一个社区，以推进增强AR的人工智能研究领域。', 'title_zh': '通过AI协同的日常AR技术'}
{'arxiv_id': 'arXiv:2412.12656', 'title': 'DriveTester: A Unified Platform for Simulation-Based Autonomous Driving Testing', 'authors': 'Mingfei Cheng, Yuan Zhou, Xiaofei Xie', 'link': 'https://arxiv.org/abs/2412.12656', 'abstract': 'Simulation-based testing plays a critical role in evaluating the safety and reliability of autonomous driving systems (ADSs). However, one of the key challenges in ADS testing is the complexity of preparing and configuring simulation environments, particularly in terms of compatibility and stability between the simulator and the ADS. This complexity often results in researchers dedicating significant effort to customize their own environments, leading to disparities in development platforms and underlying systems. Consequently, reproducing and comparing these methodologies on a unified ADS testing platform becomes difficult. To address these challenges, we introduce DriveTester, a unified simulation-based testing platform built on Apollo, one of the most widely used open-source, industrial-level ADS platforms. DriveTester provides a consistent and reliable environment, integrates a lightweight traffic simulator, and incorporates various state-of-the-art ADS testing techniques. This enables researchers to efficiently develop, test, and compare their methods within a standardized platform, fostering reproducibility and comparison across different ADS testing approaches. The code is available: this https URL.', 'abstract_zh': '基于仿真的测试在评估自动驾驶系统（ADS）的安全性和可靠性方面起着至关重要的作用。然而，ADS测试的一个关键挑战在于准备和配置仿真环境的复杂性，特别是在仿真器与ADS之间的兼容性和稳定性方面。这种复杂性常常导致研究人员需要投入大量的精力来定制自己的环境，从而导致开发平台和底层系统的不一致性。因此，在统一的ADS测试平台上再现和比较这些方法变得困难。为了解决这些问题，我们引入了DriveTester，这是一个基于Apollo构建的统一仿真测试平台，Apollo是使用最广泛、工业级的ADS平台之一。DriveTester提供了一个一致且可靠的环境，集成了一个轻量级的交通仿真器，并结合了各种先进的ADS测试技术。这使得研究人员可以在标准化平台上高效地开发、测试和比较其方法，促进不同ADS测试方法的再现性和比较。代码可获取：[链接](this https URL)。', 'title_zh': 'DriveTester：一种基于模拟的自动驾驶测试统一平台'}
{'arxiv_id': 'arXiv:2412.12651', 'title': 'Shared Attention-based Autoencoder with Hierarchical Fusion-based Graph Convolution Network for sEEG SOZ Identification', 'authors': 'Huachao Yan, Kailing Guo, Shiwei Song, Yihai Dai, Xiaoqiang Wei, Xiaofen Xing, Xiangmin Xu', 'link': 'https://arxiv.org/abs/2412.12651', 'abstract': 'Diagnosing seizure onset zone (SOZ) is a challenge in neurosurgery, where stereoelectroencephalography (sEEG) serves as a critical technique. In sEEG SOZ identification, the existing studies focus solely on the intra-patient representation of epileptic information, overlooking the general features of epilepsy across patients and feature interdependencies between feature elements in each contact site. In order to address the aforementioned challenges, we propose the shared attention-based autoencoder (sATAE). sATAE is trained by sEEG data across all patients, with attention blocks introduced to enhance the representation of interdependencies between feature elements. Considering the spatial diversity of sEEG across patients, we introduce graph-based method for identification SOZ of each patient. However, the current graph-based methods for sEEG SOZ identification rely exclusively on static graphs to model epileptic networks. Inspired by the finding of neuroscience that epileptic network is intricately characterized by the interplay of sophisticated equilibrium between fluctuating and stable states, we design the hierarchical fusion-based graph convolution network (HFGCN) to identify the SOZ. HFGCN integrates the dynamic and static characteristics of epileptic networks through hierarchical weighting across different hierarchies, facilitating a more comprehensive learning of epileptic features and enriching node information for sEEG SOZ identification. Combining sATAE and HFGCN, we perform comprehensive experiments with sATAE-HFGCN on the self-build sEEG dataset, which includes sEEG data from 17 patients with temporal lobe epilepsy. The results show that our method, sATAE-HFGCN, achieves superior performance for identifying the SOZ of each patient, effectively addressing the aforementioned challenges, providing an efficient solution for sEEG-based SOZ identification.', 'abstract_zh': '诊断发作起始区（SOZ）在神经外科中是一项挑战，而立体电encephalography (sEEG) 是关键的技术之一。在sEEG SOZ识别中，现有的研究仅集中在每个患者内部的癫痫信息表示上，忽视了跨患者的一般癫痫特征以及每个接触点特征元素之间的特征依赖关系。为了解决上述挑战，我们提出了一种基于共享注意机制的自编码器（sATAE）。sATAE 通过跨所有患者的sEEG 数据进行训练，并引入注意力模块以增强特征元素之间依赖关系的表示能力。鉴于sEEG 在患者之间的空间异质性，我们引入了一种基于图的方法来识别每个患者的SOZ。然而，当前用于sEEG SOZ识别的图方法仅依赖于静态图来建模癫痫网络。受神经科学发现的启发，即癫痫网络由波动和稳定状态之间复杂平衡的互动所 intricately 描述，我们设计了一种基于层次融合的图卷积网络（HFGCN）来识别SOZ。HFGCN 通过在不同层次之间的层次加权整合癫痫网络的动态和静态特征，有助于更全面地学习癫痫特征并丰富节点信息，从而提高sEEG SOZ识别的有效性。结合sATAE 和HFGCN，我们在构建的sEEG 数据集上进行了一系列实验，该数据集包含17例颞叶癫痫患者的sEEG 数据。实验结果表明，我们的方法sATAE-HFGCN 在识别每个患者的SOZ 方面表现出优越的性能，有效解决了上述挑战，提供了一种基于sEEG 的SOZ 识别的有效解决方案。', 'title_zh': '基于共享注意力的自编码器与基于层次融合的图卷积网络在sEEG SOZ识别中的应用'}
{'arxiv_id': 'arXiv:2412.12650', 'title': 'Neural-Network-Driven Reward Prediction as a Heuristic: Advancing Q-Learning for Mobile Robot Path Planning', 'authors': 'Yiming Ji, Kaijie Yun, Yang Liu, Zongwu Xie, Hong Liu', 'link': 'https://arxiv.org/abs/2412.12650', 'abstract': 'Q-learning is a widely used reinforcement learning technique for solving path planning problems. It primarily involves the interaction between an agent and its environment, enabling the agent to learn an optimal strategy that maximizes cumulative rewards. Although many studies have reported the effectiveness of Q-learning, it still faces slow convergence issues in practical applications. To address this issue, we propose the NDR-QL method, which utilizes neural network outputs as heuristic information to accelerate the convergence process of Q-learning. Specifically, we improved the dual-output neural network model by introducing a start-end channel separation mechanism and enhancing the feature fusion process. After training, the proposed NDR model can output a narrowly focused optimal probability distribution, referred to as the guideline, and a broadly distributed suboptimal distribution, referred to as the region. Subsequently, based on the guideline prediction, we calculate the continuous reward function for the Q-learning method, and based on the region prediction, we initialize the Q-table with a bias. We conducted training, validation, and path planning simulation experiments on public datasets. The results indicate that the NDR model outperforms previous methods by up to 5\\% in prediction accuracy. Furthermore, the proposed NDR-QL method improves the convergence speed of the baseline Q-learning method by 90\\% and also surpasses the previously improved Q-learning methods in path quality metrics.', 'abstract_zh': 'Q-learning 是一种广泛应用于解决路径规划问题的强化学习技术。它主要涉及智能体与环境之间的互动，使智能体能够学会一种最大化累积奖励的最优策略。尽管许多研究已经报告了 Q-learning 的有效性，但在实际应用中它仍然面临收敛速度慢的问题。为解决这一问题，我们提出了 NDR-QL 方法，该方法利用神经网络输出作为启发式信息，以加速 Q-learning 的收敛过程。具体而言，我们通过引入起终点通道分离机制和增强特征融合过程改进了双输出神经网络模型。经过训练后，提出的 NDR 模型可以输出一个狭窄且聚焦的最优概率分布，称为指导分布，以及一个广泛分布的次优分布，称为区域。随后，基于指导分布的预测，我们计算 Q-learning 方法的连续奖励函数；基于区域的预测，我们为 Q-table 初始化一个偏向。我们在公开数据集上进行了训练、验证和路径规划模拟实验。结果显示，NDR 模型在预测精度上比之前的方法高了 5%。此外，提出的 NDR-QL 方法将基线 Q-learning 方法的收敛速度提高了 90%，同时也在路径质量指标上超过了之前改进的 Q-learning 方法。', 'title_zh': '基于神经网络驱动的奖励预测作为启发式方法：推进用于移动机器人路径规划的Q学习'}
{'arxiv_id': 'arXiv:2412.12649', 'title': 'ClustEm4Ano: Clustering Text Embeddings of Nominal Textual Attributes for Microdata Anonymization', 'authors': 'Robert Aufschläger, Sebastian Wilhelm, Michael Heigl, Martin Schramm', 'link': 'https://arxiv.org/abs/2412.12649', 'abstract': "This work introduces ClustEm4Ano, an anonymization pipeline that can be used for generalization and suppression-based anonymization of nominal textual tabular data. It automatically generates value generalization hierarchies (VGHs) that, in turn, can be used to generalize attributes in quasi-identifiers. The pipeline leverages embeddings to generate semantically close value generalizations through iterative clustering. We applied KMeans and Hierarchical Agglomerative Clustering on $13$ different predefined text embeddings (both open and closed-source (via APIs)). Our approach is experimentally tested on a well-known benchmark dataset for anonymization: The UCI Machine Learning Repository's Adult dataset. ClustEm4Ano supports anonymization procedures by offering more possibilities compared to using arbitrarily chosen VGHs. Experiments demonstrate that these VGHs can outperform manually constructed ones in terms of downstream efficacy (especially for small $k$-anonymity ($2 \\leq k \\leq 30$)) and therefore can foster the quality of anonymized datasets. Our implementation is made public.", 'abstract_zh': '这项工作介绍了ClustEm4Ano，这是一种用于名义文本表格数据的一般化和抑制匿名化的匿名化管道。该管道能够自动生成值一般化层次结构（VGHs），进而可用于泛化准标识符中的属性。该管道利用嵌入通过迭代聚类生成语义上相近的值一般化。我们使用KMeans和层次凝聚聚类在13种不同的预定义文本嵌入（包括开源和 closesource）上进行了应用。我们的方法在匿名化领域的基准数据集——UCI机器学习库的Adult数据集上进行了实验验证。ClustEm4Ano通过提供比任意选择的VGH更多的可能性来支持匿名化过程。实验结果表明，这些VGH在下游效用方面（特别是对于小k-匿名性，2≤k≤30）比手动构建的VGH更优，从而可以提升匿名数据集的质量。我们的实现已经公开发布。', 'title_zh': 'ClustEm4Ano: 命名文本属性文本嵌入的聚类方法在微观数据匿名化中的应用'}
{'arxiv_id': 'arXiv:2412.12648', 'title': 'Exploring AI-Enabled Cybersecurity Frameworks: Deep-Learning Techniques, GPU Support, and Future Enhancements', 'authors': 'Tobias Becher, Simon Torka', 'link': 'https://arxiv.org/abs/2412.12648', 'abstract': 'Traditional rule-based cybersecurity systems have proven highly effective against known malware threats. However, they face challenges in detecting novel threats. To address this issue, emerging cybersecurity systems are incorporating AI techniques, specifically deep-learning algorithms, to enhance their ability to detect incidents, analyze alerts, and respond to events. While these techniques offer a promising approach to combating dynamic security threats, they often require significant computational resources. Therefore, frameworks that incorporate AI-based cybersecurity mechanisms need to support the use of GPUs to ensure optimal performance.\nMany cybersecurity framework vendors do not provide sufficiently detailed information about their implementation, making it difficult to assess the techniques employed and their effectiveness. This study aims to overcome this limitation by providing an overview of the most used cybersecurity frameworks that utilize AI techniques, specifically focusing on frameworks that provide comprehensive information about their implementation. Our primary objective is to identify the deep-learning techniques employed by these frameworks and evaluate their support for GPU acceleration. We have identified a total of \\emph{two} deep-learning algorithms that are utilized by \\emph{three} out of 38 selected cybersecurity frameworks. Our findings aim to assist in selecting open-source cybersecurity frameworks for future research and assessing any discrepancies between deep-learning techniques used in theory and practice.', 'abstract_zh': '传统的基于规则的网络安全系统在抵御已知恶意软件威胁方面 proven 高效。然而，它们在检测新型威胁时面临挑战。为解决这一问题，新兴的网络安全系统正在结合 AI 技术，特别是深度学习算法，以增强其事件检测、警报分析和事件响应的能力。虽然这些技术为应对动态安全威胁提供了有前途的方法，但也常常需要大量的计算资源。因此，结合 AI 网络安全机制的框架需要支持 GPU 的使用，以确保最佳性能。\n\n许多网络安全框架提供商未能提供足够的实施细节，使得评估所采用的技术及其效果变得困难。本研究旨在通过概述利用 AI 技术的最常用网络安全框架，克服这一限制，并具体关注那些提供全面实施信息的框架。我们的主要目标是确定这些框架所采用的深度学习技术，并评估其对 GPU 加速的支持情况。我们发现，在所选择的 38 个网络安全框架中，共有三种框架使用了总计两种深度学习算法。本研究的发现旨在帮助未来的研究选择开源网络安全框架，并评估理论和实践中使用的深度学习技术之间的差异。', 'title_zh': '探索AI赋能的网络安全框架：深度学习技术、GPU支持和未来增强'}
{'arxiv_id': 'arXiv:2412.12642', 'title': 'RDPI: A Refine Diffusion Probability Generation Method for Spatiotemporal Data Imputation', 'authors': 'Zijin Liu, Xiang Zhao, You Song', 'link': 'https://arxiv.org/abs/2412.12642', 'abstract': 'Spatiotemporal data imputation plays a crucial role in various fields such as traffic flow monitoring, air quality assessment, and climate prediction. However, spatiotemporal data collected by sensors often suffer from temporal incompleteness, and the sparse and uneven distribution of sensors leads to missing data in the spatial dimension. Among existing methods, autoregressive approaches are prone to error accumulation, while simple conditional diffusion models fail to adequately capture the spatiotemporal relationships between observed and missing data. To address these issues, we propose a novel two-stage Refined Diffusion Probability Impuation (RDPI) framework based on an initial network and a conditional diffusion model. In the initial stage, deterministic imputation methods are used to generate preliminary estimates of the missing data. In the refinement stage, residuals are treated as the diffusion target, and observed values are innovatively incorporated into the forward process. This results in a conditional diffusion model better suited for spatiotemporal data imputation, bridging the gap between the preliminary estimates and the true values. Experiments on multiple datasets demonstrate that RDPI not only achieves state-of-the-art imputation accuracy but also significantly reduces sampling computational costs.', 'abstract_zh': '时空数据插值在交通流量监测、空气质量评估和气候预测等多个领域中发挥着重要作用。然而，由传感器收集的时空数据经常存在时间上的不完整性，传感器的稀疏且不均匀分布导致在空间维度上存在数据缺失问题。在现有方法中，自回归方法容易出现误差累积，而简单的条件扩散模型则难以充分捕捉观测数据和缺失数据之间的时空关系。为了解决这些问题，我们提出了一种基于初始网络和条件扩散模型的双重阶段精炼扩散概率插补（RDPI）框架。在初始阶段，使用确定性插补方法生成缺失数据的初步估计。在精炼阶段，将残差视为扩散目标，并创新地将观测值纳入前向过程。这种条件扩散模型更适合时空数据插补，能够更好地弥合初步估计与真实值之间的差距。在多个数据集上的实验表明，RDPI不仅实现了最先进的插补精度，而且显著降低了采样计算成本。', 'title_zh': 'RDPI：一种时空数据插补的精炼扩散概率生成方法'}
{'arxiv_id': 'arXiv:2412.12641', 'title': 'Lagrangian Index Policy for Restless Bandits with Average Reward', 'authors': 'Konstantin Avrachenkov, Vivek S. Borkar, Pratik Shah', 'link': 'https://arxiv.org/abs/2412.12641', 'abstract': "We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits with long-run average reward. In particular, we compare the performance of LIP with the performance of the Whittle Index Policy (WIP), both heuristic policies known to be asymptotically optimal under certain natural conditions. Even though in most cases their performances are very similar, in the cases when WIP shows bad performance, LIP continues to perform very well. We then propose reinforcement learning algorithms, both tabular and NN-based, to obtain online learning schemes for LIP in the model-free setting. The proposed reinforcement learning schemes for LIP requires significantly less memory than the analogous scheme for WIP. We calculate analytically the Lagrangian index for the restart model, which describes the optimal web crawling and the minimization of the weighted age of information. We also give a new proof of asymptotic optimality in case of homogeneous bandits as the number of arms goes to infinity, based on exchangeability and de Finetti's theorem.", 'abstract_zh': '我们研究了长期平均奖励条件下的动量索引策略（LIP）在非平稳多臂老虎机中的应用。特别地，我们将LIP的性能与Whittle索引策略（WIP）的性能进行了比较，后者在某些自然条件下被认为是渐近最优的启发式策略。尽管在大多数情况下它们的性能非常相似，但在WIP表现出不佳性能的情况下，LIP仍然表现出色。随后，我们提出了基于强化学习的算法，包括基于表的和基于神经网络的方法，用于在无模型设置下在线学习LIP策略。提出的用于LIP的强化学习方案所需内存显著少于WIP的相应方案。此外，我们通过计算重启动模型中的拉格朗日索引，分析了该模型下的最优网页抓取和加权信息年龄最小化。最后，基于可交换性和de Finetti定理，我们给出了在臂的数量趋向无穷大时同质多臂老虎机渐近最优的新证明。', 'title_zh': '基于拉格朗日索引策略的平均奖励非稳态多臂老虎机问题'}
{'arxiv_id': 'arXiv:2412.12639', 'title': 'Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree', 'authors': 'Xiangxiang Gao, Weisheng Xie, Yiwei Xiang, Feng Ji', 'link': 'https://arxiv.org/abs/2412.12639', 'abstract': "Striking an optimal balance between minimal drafting latency and high speculation accuracy to enhance the inference speed of Large Language Models remains a significant challenge in speculative decoding. In this paper, we introduce Falcon, an innovative semi-autoregressive speculative decoding framework fashioned to augment both the drafter's parallelism and output quality. Falcon incorporates the Coupled Sequential Glancing Distillation technique, which fortifies inter-token dependencies within the same block, leading to increased speculation accuracy. We offer a comprehensive theoretical analysis to illuminate the underlying mechanisms. Additionally, we introduce a Custom-Designed Decoding Tree, which permits the drafter to generate multiple tokens in a single forward pass and accommodates multiple forward passes as needed, thereby boosting the number of drafted tokens and significantly improving the overall acceptance rate. Comprehensive evaluations on benchmark datasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior acceleration capabilities. The framework achieves a lossless speedup ratio ranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model series. These results outstrip existing speculative decoding methods for LLMs, including Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact drafter architecture equivalent to merely two Transformer layers.", 'abstract_zh': '在推测解码中，如何在最小化编写延迟和保持高推测准确性之间取得最佳平衡，以提高大型语言模型的推理速度，仍然是一个重大的挑战。本文介绍了Falcon——一种创新的半自回归推测解码框架，旨在增强编写者的并行性和输出质量。Falcon引入了耦合序列光顾蒸馏技术，增强了同一块内的词间依赖性，从而提高了推测的准确性。我们提供了全面的理论分析来揭示其背后的机制。此外，提出了一种定制设计的解码树，允许编写者在单次前向传递中生成多个词，并按需支持多次前向传递，从而增加生成的词数量，显著提高整体接受率。在包括MT-Bench、HumanEval和GSM8K等基准数据集上的全面评估表明，Falcon具有出色的加速能力。在Vicuna和LLaMA2-Chat模型系列上的测试结果显示，Falcon实现了无损加速比在2.91倍到3.51倍之间，而保持了一个紧凑的编写者架构，仅相当于两个Transformer层。这些结果优于现有的LLM推测解码方法，包括Eagle、Medusa、Lookahead、SPS和PLD。', 'title_zh': 'Falcon：通过增强半自回归草稿写作和定制设计的解码树实现大型语言模型的更快并行推理'}
{'arxiv_id': 'arXiv:2412.12636', 'title': 'TrainMover: Efficient ML Training Live Migration with No Memory Overhead', 'authors': 'ChonLam Lao, Minlan Yu, Aditya Akella, Jiamin Cao, Yu Guan, Pengcheng Zhang, Zhilong Zheng, Yichi Xu, Ennan Zhai, Dennis Cai, Jiaqi Gao', 'link': 'https://arxiv.org/abs/2412.12636', 'abstract': 'Machine learning training has emerged as one of the most prominent workloads in modern data centers. These training jobs are large-scale, long-lasting, and tightly coupled, and are often disrupted by various events in the cluster such as failures, maintenance, and job scheduling. To handle these events, we rely on cold migration, where we first checkpoint the entire cluster, replace the related machines, and then restart the training. This approach leads to disruptions to the training jobs, resulting in significant downtime. In this paper, we present TrainMover, a live migration system that enables machine replacement during machine learning training. TrainMover minimizes downtime by leveraging member replacement of collective communication groups and sandbox lazy initialization. Our evaluation demonstrates that TrainMover achieves 16x less downtime compared to all baselines, effectively handling data center events like straggler rebalancing, maintenance, and unexpected failures.', 'abstract_zh': '机器学习训练已成为现代数据中心最重要的工作负载之一。这些训练任务规模庞大、持续时间长且紧密耦合，常因集群中的各种事件（如故障、维护和作业调度）而被打断。为应对这些事件，我们依赖冷迁移，即首先对整个集群进行快照，然后替换相关机器，最后重新启动训练。这一方法会导致训练任务中断，造成显著的停机时间。本文介绍了TrainMover，这是一种实时迁移系统，能够在机器学习训练期间实现机器替换。TrainMover通过利用集体通信组的成员替换和沙箱惰性初始化来减少停机时间。我们的评估表明，TrainMover相比所有基准系统减少了16倍的停机时间，有效地处理了数据中心中的各种事件，如落后任务重新平衡、维护和意外故障。', 'title_zh': 'TrainMover: 无需内存开销的高效机器学习训练在线迁移技术'}
{'arxiv_id': 'arXiv:2412.12632', 'title': 'What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context', 'authors': 'Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu', 'link': 'https://arxiv.org/abs/2412.12632', 'abstract': "Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and explore LLMs' preferences from their effectiveness, faithfulness and robustness, as well as CoE's usability in a naive Retrieval-Augmented Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.", 'abstract_zh': '将以下论文内容或标题翻译成中文，要符合学术规范：\n\n将外部知识融入大规模语言模型（LLMs）已成为缓解LLMs过时知识和幻觉问题的一种有前景的方法。然而，外部知识往往是不完善的。除了有用的知識之外，外部知识还在上下文中富含与问题无关或错误的信息，这会损害LLMs响应的可靠性。本文关注在处理多跳问答（multi-hop QA）时，LLMs倾向于选择的外部知识在不完美上下文中的特性。受刑事诉讼法中证据链（Chain of Evidence, CoE）概念的启发，我们发现，LLMs倾向于选择的知识应该同时与问题相关，并且各个知识片段之间相互支持。因此，我们提出了一种自动化的CoE辨识方法，并从有效性、忠实度、稳健性以及在朴素的检索增强生成（RAG）场景中CoE的可用性等方面探讨了LLMs的偏好。对五种LLMs的评估表明，通过CoE的使用，LLMs能够实现更准确的生成、更强的答案忠实度、更好的知识冲突抵御能力，并在流行的RAG案例中表现出更好的性能。', 'title_zh': 'LLMs偏好的外部知识是什么？ characterization and探索不完美的背景中的推理链条'}
{'arxiv_id': 'arXiv:2412.12629', 'title': 'a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External Validation and Performance Analysis Across 21 Conditions', 'authors': 'Pranav Rajpurkar, Julian N. Acosta, Siddhant Dogra, Jaehwan Jeong, Deepanshu Jindal, Michael Moritz, Samir Rajpurkar', 'link': 'https://arxiv.org/abs/2412.12629', 'abstract': "We present a comprehensive evaluation of a2z-1, an artificial intelligence (AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive and actionable findings. Our study focuses on rigorous assessment of the model's performance and generalizability. Large-scale retrospective analysis demonstrates an average AUC of 0.931 across 21 conditions. External validation across two distinct health systems confirms consistent performance (AUC 0.923), establishing generalizability to different evaluation scenarios, with notable performance in critical findings such as small bowel obstruction (AUC 0.958) and acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy across patient sex, age groups, and varied imaging protocols, including different slice thicknesses and contrast administration types. Comparison of high-confidence model outputs to radiologist reports reveals instances where a2z-1 identified overlooked findings, suggesting potential for quality assurance applications.", 'abstract_zh': '我们全面评估了a2z-1这一人工智能（AI）模型，该模型旨在分析腹部盆腔CT扫描，以识别21项与时间紧迫性和可操作性相关的发现。本研究重点在于模型性能和普适性的严格评估。大规模回顾性分析结果显示，该模型在21种条件下展现出平均AUC值为0.931。通过两个不同的医疗机构进行外部验证，证实了其一致的性能（AUC 0.923），证明其在不同评估场景中的普适性，特别是在小肠梗阻（AUC 0.958）和急性胰腺炎（AUC 0.961）等关键发现方面表现出优异性能。子组分析显示，该模型在不同性别、年龄组以及多种成像协议下的准确性保持一致，包括不同的切片厚度和对比剂给药类型。将高度自信的模型输出与放射科报告进行比较，显示a2z-1能够识别放射科医师忽略的发现，表明其在质量保证方面的潜在应用价值。', 'title_zh': '针对腹部盆腔CT多疾病检测的a2z-1模型：外部验证及21种条件下的性能分析'}
{'arxiv_id': 'arXiv:2412.12619', 'title': 'Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated Speech Deepfakes', 'authors': 'Kuiyuan Zhang, Zhongyun Hua, Rushi Lan, Yushu Zhang, Yifang Guo', 'link': 'https://arxiv.org/abs/2412.12619', 'abstract': 'Recent advancements in text-to-speech and speech conversion technologies have enabled the creation of highly convincing synthetic speech. While these innovations offer numerous practical benefits, they also cause significant security challenges when maliciously misused. Therefore, there is an urgent need to detect these synthetic speech signals. Phoneme features provide a powerful speech representation for deepfake detection. However, previous phoneme-based detection approaches typically focused on specific phonemes, overlooking temporal inconsistencies across the entire phoneme sequence. In this paper, we develop a new mechanism for detecting speech deepfakes by identifying the inconsistencies of phoneme-level speech features. We design an adaptive phoneme pooling technique that extracts sample-specific phoneme-level features from frame-level speech data. By applying this technique to features extracted by pre-trained audio models on previously unseen deepfake datasets, we demonstrate that deepfake samples often exhibit phoneme-level inconsistencies when compared to genuine speech. To further enhance detection accuracy, we propose a deepfake detector that uses a graph attention network to model the temporal dependencies of phoneme-level features. Additionally, we introduce a random phoneme substitution augmentation technique to increase feature diversity during training. Extensive experiments on four benchmark datasets demonstrate the superior performance of our method over existing state-of-the-art detection methods.', 'abstract_zh': '近年来，文本到语音和语音转换技术的进步已经使得创建高度逼真的合成语音成为可能。尽管这些创新为实际应用提供了诸多益处，但它们在被恶意滥用时也会带来重大的安全挑战。因此，迫切需要检测这些合成语音信号。音素特征为虚假语音检测提供了一种强大的语音表示方法。然而，先前基于音素的检测方法通常专注于特定的音素，忽视了整个音素序列中的时序不一致性。在本文中，我们开发了一种新的机制，通过识别音素级语音特征的一致性来检测虚假语音。我们设计了一种自适应音素池化技术，从帧级语音数据中提取样本特定的音素级特征。通过将该技术应用于预训练音频模型在未见过的虚假语音数据集上提取出的特征，我们展示了虚假语音样本通常在与真实语音相比时表现出音素级的一致性问题。为进一步提高检测准确性，我们提出了一种使用图注意力网络建模音素级特征的时序依赖关系的虚假语音检测器。此外，我们引入了一种随机音素替换增强技术，以在训练过程中增加特征的多样性。在四个基准数据集上的广泛实验表明，我们方法的性能优于现有的最先进的检测方法。', 'title_zh': '音素级特征差异：检测复杂语音换脸的关键'}
{'arxiv_id': 'arXiv:2412.12612', 'title': 'SynthCypher: A Fully Synthetic Data Generation Framework for Text-to-Cypher Querying in Knowledge Graphs', 'authors': 'Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan', 'link': 'https://arxiv.org/abs/2412.12612', 'abstract': 'Cypher, the query language for Neo4j graph databases, plays a critical role in enabling graph-based analytics and data exploration. While substantial research has been dedicated to natural language to SQL query generation (Text2SQL), the analogous problem for graph databases referred to as Text2Cypher remains underexplored. In this work, we introduce SynthCypher, a fully synthetic and automated data generation pipeline designed to address this gap. SynthCypher employs a novel LLMSupervised Generation-Verification framework, ensuring syntactically and semantically correct Cypher queries across diverse domains and query complexities. Using this pipeline, we create SynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher instances. Fine-tuning open-source large language models (LLMs), including LLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant performance improvements of up to 40% on the Text2Cypher test set and 30% on the SPIDER benchmark adapted for graph databases. This work demonstrates that high-quality synthetic data can effectively advance the state-of-the-art in Text2Cypher tasks.', 'abstract_zh': 'Cypher 是 Neo4j 图数据库的查询语言，在实现基于图的分析和数据探索方面发挥着关键作用。尽管已有相当多的研究致力于自然语言到 SQL 查询生成（即 Text2SQL），但针对图数据库的相应问题，即 Text2Cypher，仍然存在大量未被充分探索的空间。在本文中，我们引入了 SynthCypher，这是一种完全合成和自动化的数据生成管道，旨在填补这一空白。SynthCypher 采用了一种新颖的 LLM 监督生成-验证框架，确保跨多个领域和查询复杂度生成具有语法和语义正确性的 Cypher 查询。利用此管道，我们创建了包含了 29,800 个 Text2Cypher 实例的 SynthCypher 数据集，这是一个大规模基准。对开源大型语言模型（LLMs），包括 LLaMa-3.1-8B、Mistral-7B 和 QWEN-7B 的微调，使 Text2Cypher 测试集上的性能提高了高达 40%，SPIDER 基准（已适应图数据库任务）上的性能提高了 30%。本研究证明高品质的合成数据能够有效推动 Text2Cypher 任务的最新进展。', 'title_zh': 'SynthCypher：用于知识图中文本到密文查询的完全合成数据生成框架'}
{'arxiv_id': 'arXiv:2412.12605', 'title': 'An Advantage-based Optimization Method for Reinforcement Learning in Large Action Space', 'authors': 'Hai Lin, Cheng Huang, Zhihong Chen', 'link': 'https://arxiv.org/abs/2412.12605', 'abstract': 'Reinforcement learning tasks in real-world scenarios often involve large, high-dimensional action spaces, leading to challenges such as convergence difficulties, instability, and high computational complexity. It is widely acknowledged that traditional value-based reinforcement learning algorithms struggle to address these issues effectively. A prevalent approach involves generating independent sub-actions within each dimension of the action space. However, this method introduces bias, hindering the learning of optimal policies. In this paper, we propose an advantage-based optimization method and an algorithm named Advantage Branching Dueling Q-network (ABQ). ABQ incorporates a baseline mechanism to tune the action value of each dimension, leveraging the advantage relationship across different sub-actions. With this approach, the learned policy can be optimized for each dimension. Empirical results demonstrate that ABQ outperforms BDQ, achieving 3%, 171%, and 84% more cumulative rewards in HalfCheetah, Ant, and Humanoid environments, respectively. Furthermore, ABQ exhibits competitive performance when compared against two continuous action benchmark algorithms, DDPG and TD3.', 'abstract_zh': '在实际应用场景中的强化学习任务往往涉及大规模、高维的动作空间，这导致了 convergence 难题、不稳定性以及高计算复杂性等问题。普遍认为，传统基于值的强化学习算法在有效应对这些问题方面存在局限性。一种常见的方法是在动作空间的每个维度中生成独立的子动作。然而，这种方法引入了偏差，妨碍了对最优策略的学习。在本文中，我们提出了一种基于优势的优化方法，并设计了一种名为 Advantage Branching Dueling Q 网络（ABQ）的算法。ABQ 结合了基线机制，用于调整每个维度的动作值，并利用不同子动作之间的优势关系。通过这种方法，可以针对每个维度优化学到的策略。实验结果表明，与 Dueling Q 网络（BDQ）相比，ABQ 在 HalfCheetah、Ant 和 Humanoid 环境中分别获得了 3%、171% 和 84% 更高的累积奖励。此外，当将 ABQ 与两个连续动作基准算法 DDPG 和 TD3 进行性能对比时，ABQ 展示出了竞争力。', 'title_zh': '一种在大动作空间中强化学习的优势优化方法'}
{'arxiv_id': 'arXiv:2412.12587', 'title': 'Distributed satellite information networks: Architecture, enabling technologies, and trends', 'authors': 'Qinyu Zhang, Liang Xu, Jianhao Huang, Tao Yang, Jian Jiao, Ye Wang, Yao Shi, Chiya Zhang, Xingjian Zhang, Ke Zhang, Yupeng Gong, Na Deng, Nan Zhao, Zhen Gao, Shujun Han, Xiaodong Xu, Li You, Dongming Wang, Shan Jiang, Dixian Zhao, Nan Zhang, Liujun Hu, Xiongwen He, Yonghui Li, Xiqi Gao, Xiaohu You', 'link': 'https://arxiv.org/abs/2412.12587', 'abstract': 'Driven by the vision of ubiquitous connectivity and wireless intelligence, the evolution of ultra-dense constellation-based satellite-integrated Internet is underway, now taking preliminary shape. Nevertheless, the entrenched institutional silos and limited, nonrenewable heterogeneous network resources leave current satellite systems struggling to accommodate the escalating demands of next-generation intelligent applications. In this context, the distributed satellite information networks (DSIN), exemplified by the cohesive clustered satellites system, have emerged as an innovative architecture, bridging information gaps across diverse satellite systems, such as communication, navigation, and remote sensing, and establishing a unified, open information network paradigm to support resilient space information services. This survey first provides a profound discussion about innovative network architectures of DSIN, encompassing distributed regenerative satellite network architecture, distributed satellite computing network architecture, and reconfigurable satellite formation flying, to enable flexible and scalable communication, computing and control. The DSIN faces challenges from network heterogeneity, unpredictable channel dynamics, sparse resources, and decentralized collaboration frameworks. To address these issues, a series of enabling technologies is identified, including channel modeling and estimation, cloud-native distributed MIMO cooperation, grant-free massive access, network routing, and the proper combination of all these diversity techniques. Furthermore, to heighten the overall resource efficiency, the cross-layer optimization techniques are further developed to meet upper-layer deterministic, adaptive and secure information services requirements. In addition, emerging research directions and new opportunities are highlighted on the way to achieving the DSIN vision.', 'abstract_zh': '受无所不在的连接性和无线智能愿景的驱动，超密集星座卫星集成互联网正在经历演变，并初步成形。然而，现有的机构壁垒和有限的非可再生异构网络资源使当前的卫星系统难以满足下一代智能应用日益增长的需求。在此背景下，分布式卫星信息网络（DSIN），以紧密集群卫星系统为代表，作为一种创新架构崭露头角，跨越通信、导航和遥感等多种卫星系统的信息鸿沟，建立统一开放的信息网络范式，以支持可信赖的空间信息服务。本文综述首先对DSIN的创新网络架构进行了深刻的讨论，涵盖了分布式再生卫星网络架构、分布式卫星计算网络架构以及可重构卫星编队飞行，以实现灵活和可扩展的通信、计算和控制。DSIN面临的挑战包括网络异构性、不可预测的信道动态、稀疏资源以及分散化的协作框架。为应对这些挑战，确定了一系列使能技术，包括信道建模与估计、云原生分布式MIMO合作、许可免费大规模接入、网络路由，以及这些多样化技术的有机结合。此外，为了提高总体资源效率，进一步发展了跨层优化技术，以满足上层确定性、自适应和安全的信息服务要求。在此过程中，还突显了通向实现DSIN愿景的研究方向和新的机遇。', 'title_zh': '分布式卫星信息网络：架构、使能技术及趋势'}
{'arxiv_id': 'arXiv:2412.12575', 'title': 'SIDE: Socially Informed Drought Estimation Toward Understanding Societal Impact Dynamics of Environmental Crisis', 'authors': 'Lanyu Shang, Bozhang Chen, Shiwei Liu, Yang Zhang, Ruohan Zong, Anav Vora, Ximing Cai, Na Wei, Dong Wang', 'link': 'https://arxiv.org/abs/2412.12575', 'abstract': "Drought has become a critical global threat with significant societal impact. Existing drought monitoring solutions primarily focus on assessing drought severity using quantitative measurements, overlooking the diverse societal impact of drought from human-centric perspectives. Motivated by the collective intelligence on social media and the computational power of AI, this paper studies a novel problem of socially informed AI-driven drought estimation that aims to leverage social and news media information to jointly estimate drought severity and its societal impact. Two technical challenges exist: 1) How to model the implicit temporal dynamics of drought societal impact. 2) How to capture the social-physical interdependence between the physical drought condition and its societal impact. To address these challenges, we develop SIDE, a socially informed AI-driven drought estimation framework that explicitly quantifies the societal impact of drought and effectively models the social-physical interdependency for joint severity-impact estimation. Experiments on real-world datasets from California and Texas demonstrate SIDE's superior performance compared to state-of-the-art baselines in accurately estimating drought severity and its societal impact. SIDE offers valuable insights for developing human-centric drought mitigation strategies to foster sustainable and resilient communities.", 'abstract_zh': '干旱已成为一个关键的全球性威胁，对社会产生了重大影响。现有的干旱监测解决方案主要侧重于通过定量指标评估干旱的严重程度，忽视了从人文视角出发的干旱多样化的社会影响。基于社交媒体中的集体智慧和人工智能的计算能力，本文研究了一个新的问题，即社会导向的人工智能驱动的干旱评估问题，旨在利用社会和新闻媒体的信息共同评估干旱的严重程度及其社会影响。两个技术挑战如下：1）如何建模干旱社会影响的隐含时间动态。2）如何捕捉物理干旱状况与其社会影响之间的社会-物理相互依赖关系。为应对这些挑战，我们开发了SIDE框架，这是一种社会导向的人工智能驱动的干旱估算法，明确地量化了干旱的社会影响，有效建模了社会-物理相互依赖关系，以实现综合的严重程度和影响评估。实验结果表明，SIDE在准确评估干旱严重程度及其社会影响方面，优于最先进的基线方法。SIDE为制定以人为本的干旱缓解策略，促进可持续和有韧性的社区提供了宝贵的见解。', 'title_zh': 'SIDE：社会导向的干旱估计：理解环境危机对社会影响动态'}
{'arxiv_id': 'arXiv:2412.12572', 'title': 'License Plate Detection and Character Recognition Using Deep Learning and Font Evaluation', 'authors': 'Zahra Ebrahimi Vargoorani, Ching Yee Suen', 'link': 'https://arxiv.org/abs/2412.12572', 'abstract': 'License plate detection (LPD) is essential for traffic management, vehicle tracking, and law enforcement but faces challenges like variable lighting and diverse font types, impacting accuracy. Traditionally reliant on image processing and machine learning, the field is now shifting towards deep learning for its robust performance in various conditions. Current methods, however, often require tailoring to specific regional datasets. This paper proposes a dual deep learning strategy using a Faster R-CNN for detection and a CNN-RNN model with Connectionist Temporal Classification (CTC) loss and a MobileNet V3 backbone for recognition. This approach aims to improve model performance using datasets from Ontario, Quebec, California, and New York State, achieving a recall rate of 92% on the Centre for Pattern Recognition and Machine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It includes a detailed error analysis to identify the causes of false positives. Additionally, the research examines the role of font features in license plate (LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California Clarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers significant performance discrepancies influenced by font characteristics, offering insights for future LPD system enhancements.\nKeywords: Deep Learning, License Plate, Font Evaluation', 'abstract_zh': '车牌检测（LPD）对于交通管理、车辆跟踪和执法至关重要，但面临着光照变化和多种字体类型等挑战，影响准确性。传统上依赖图像处理和机器学习，该领域现在正转向深度学习，以应对各种条件下更稳健的表现。然而，现有方法往往需要针对特定地区的数据集进行定制。本文提出了一种双重深度学习策略，使用Faster R-CNN进行检测，并使用带有连接主义时序分类（CTC）损失和MobileNet V3主干的CNN-RNN模型进行识别。该方法通过使用来自安大略省、魁北克省、加利福尼亚州和纽约州的数据集，旨在提高模型性能，在Centre for Pattern Recognition and Machine Intelligence（CENPARMI）数据集上达到了92%的召回率，在UFPR-ALPR数据集上达到了90%的召回率。该研究包括详细的错误分析，以确定误报的原因。此外，该研究还探讨了字体特征在车牌（LP）识别中的作用，分析了如Driver Gothic、Dreadnought、California Clarendon和Zürich Extra Condensed等字体在OpenALPR系统中的性能差异，发现字体特征对性能影响显著，为未来LPD系统的改进提供了见解。\n\n关键词：深度学习，车牌识别，字体评估', 'title_zh': '使用深度学习的车牌检测与字符识别及字体评估'}
{'arxiv_id': 'arXiv:2412.12561', 'title': 'Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking', 'authors': 'Wenjun Huang, Yang Ni, Hanning Chen, Yirui He, Ian Bryant, Yezi Liu, Mohsen Imani', 'link': 'https://arxiv.org/abs/2412.12561', 'abstract': 'Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.', 'abstract_zh': '参考多目标跟踪（RMOT）是跨模态领域的一个新兴任务，旨在根据语言表达定位任意数量的目标，并在视频中连续跟踪这些目标。这个复杂的任务涉及多模态数据的推理以及基于时间关联的精确目标定位。然而，由于任务本身的性质，先前的研究忽视了新生目标与现有目标之间的数据分布不平衡问题。此外，他们仅间接融合多模态特征，难以为新生目标检测提供明确的指导。为了解决上述问题，我们实施了一种合作配对策略，减轻不平衡问题的影响，增强新生目标检测能力，同时保持跟踪性能。在编码器中，我们将多模态和多尺度融合整合并增强，克服了先前工作中存在的瓶颈，即特征图之间的多模态信息共享和交互有限。在解码器中，我们还开发了一种参考信息融合适应机制，通过查询令牌提供明确的参考指导。实验结果表明，与先前的工作相比，我们的模型性能提高了3.42%，证明了我们设计的有效性。', 'title_zh': '告诉我需要跟踪什么：注入稳健的语言指导以提升多对象跟踪性能'}
{'arxiv_id': 'arXiv:2412.12559', 'title': 'EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation', 'authors': 'Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park', 'link': 'https://arxiv.org/abs/2412.12559', 'abstract': 'We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents - while preserving their contextual dependencies - enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at this https URL', 'abstract_zh': '我们介绍了一种名为EXIT的抽取式上下文压缩框架，该框架提升了检索增强生成（RAG）在问答（QA）任务中的效果与效率。当前的RAG系统在检索模型无法有效排序最相关的文档时，常常难以处理，导致为了提高准确度不得不增加上下文，而这也带来了延迟增加的问题。尽管抽象压缩方法可以大幅减少令牌数量，但其逐步生成的过程会导致端到端延迟显著增加。相比之下，现有的抽取式方法可以减少延迟，但依赖于独立且非适应性的句子选择，未能充分利用上下文信息。EXIT通过分类检索到的文档中的句子（同时保持其上下文依赖性），实现了并行进行、上下文感知的抽取，可以根据查询的复杂性和检索质量进行调整。我们的评估结果显示，在单跳和多跳问答任务中，EXIT在问答准确性、推断时间和令牌数量方面均超越了现有的压缩方法和未压缩的基线。通过同时提高效果和效率，EXIT为开发可扩展且高质量的RAG问答解决方案提供了新的方向。我们的代码可在此链接访问：[请提供具体的URL链接]', 'title_zh': 'EXIT：基于上下文的提取式压缩以增强检索增强生成'}
{'arxiv_id': 'arXiv:2412.12552', 'title': 'SAModified: A Foundation Model-Based Zero-Shot Approach for Refining Noisy Land-Use Land-Cover Maps', 'authors': 'Sparsh Pekhale, Rakshith Sathish, Sathisha Basavaraju, Divya Sharma', 'link': 'https://arxiv.org/abs/2412.12552', 'abstract': "Land-use and land cover (LULC) analysis is critical in remote sensing, with wide-ranging applications across diverse fields such as agriculture, utilities, and urban planning. However, automating LULC map generation using machine learning is rendered challenging due to noisy labels. Typically, the ground truths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's ability to learn to accurately classify the pixels. Further, these erroneous labels can significantly distort the performance metrics of a model, leading to misleading evaluations. Traditionally, the ambiguous labels are rectified using unsupervised algorithms. These algorithms struggle not only with scalability but also with generalization across different geographies. To overcome these challenges, we propose a zero-shot approach using the foundation model, Segment Anything Model (SAM), to automatically delineate different land parcels/regions and leverage them to relabel the unsure pixels by using the local label statistics within each detected region. We achieve a significant reduction in label noise and an improvement in the performance of the downstream segmentation model by $\\approx 5\\%$ when trained with denoised labels.", 'abstract_zh': '土地利用和土地覆盖（LULC）分析在遥感领域至关重要，其应用范围广泛，涵盖了农业、公用设施和城市规划等多个领域。然而，使用机器学习自动化生成LULC地图受到嘈杂标签的挑战。通常，地面实况（例如，ESRI LULC、MapBioMass）存在噪音标签，这妨碍了模型学习准确分类像素的能力。此外，这些错误的标签会严重影响模型的性能指标，导致误导性的评估。传统上，模糊的标签通过使用无监督算法进行修正。这些算法不仅在 scalability（扩展性）方面存在问题，还在不同地理区域的一般化方面也存在不足。为解决这些挑战，我们提出了一种基于基础模型（Foundation Model）的零样本（Zero-shot）方法，利用Segment Anything Model (SAM) 自动勾勒不同的土地地块/区域，并利用每个检测区域内的局部标签统计数据对不确定的像素进行重新标注。通过使用去噪后的标签训练下游分割模型，我们实现了标签噪声的显著减少，并提高了下游分割模型的性能，大约提高了5%。', 'title_zh': 'SAModified：一种基于基础模型的零样本方法用于细化 noisy 土地利用覆被地图'}
{'arxiv_id': 'arXiv:2412.12542', 'title': 'Bots against Bias: Critical Next Steps for Human-Robot Interaction', 'authors': 'Katie Seaborn', 'link': 'https://arxiv.org/abs/2412.12542', 'abstract': 'We humans are biased - and our robotic creations are biased, too. Bias is a natural phenomenon that drives our perceptions and behavior, including when it comes to socially expressive robots that have humanlike features. Recognizing that we embed bias, knowingly or not, within the design of such robots is crucial to studying its implications for people in modern societies. In this chapter, I consider the multifaceted question of bias in the context of humanoid, AI-enabled, and expressive social robots: Where does bias arise, what does it look like, and what can (or should) we do about it. I offer observations on human-robot interaction (HRI) along two parallel tracks: (1) robots designed in bias-conscious ways and (2) robots that may help us tackle bias in the human world. I outline a curated selection of cases for each track drawn from the latest HRI research and positioned against social, legal, and ethical factors. I also propose a set of critical next steps to tackle the challenges and opportunities on bias within HRI research and practice.', 'abstract_zh': '人类是有偏见的——我们的机器人创造物也是如此。偏见是一种自然现象，它影响着我们的感知和行为，包括具有人类特征的社交表达型机器人。认识到我们在设计这类机器人时有意无意地嵌入了偏见，这对研究其对现代社会人们的影响至关重要。在本章中，我将多方面探讨机器人的偏见问题，尤其是在人体形、AI驱动和具有社会表达性的机器人这一背景下：偏见从何而来，它表现在哪些方面，我们又可以（或者应当）做些什么。我沿着两条平行轨道对人类与机器人交互（HRI）进行观察和思考：（1）带有偏见意识设计的机器人，以及（2）可能有助于我们应对人类世界中的偏见的机器人。我概述了来自最新HRI研究的精选案例，这些案例在社会、法律和伦理层面进行了定位。我还提出了应对HRI研究和实践中偏见挑战的一系列关键步骤。', 'title_zh': '偏见对抗：人类-机器人互动的下一步关键举措'}
{'arxiv_id': 'arXiv:2412.12541', 'title': 'LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven Curriculum Learning', 'authors': 'Tao Fang, Derek F. Wong, Lusheng Zhang, Keyan Jin, Qiang Zhang, Tianjiao Li, Jinlong Hou, Lidia S. Chao', 'link': 'https://arxiv.org/abs/2412.12541', 'abstract': 'While large-scale language models (LLMs) have demonstrated remarkable capabilities in specific natural language processing (NLP) tasks, they may still lack proficiency compared to specialized models in certain domains, such as grammatical error correction (GEC). Drawing inspiration from the concept of curriculum learning, we have delved into refining LLMs into proficient GEC experts by devising effective curriculum learning (CL) strategies. In this paper, we introduce a novel approach, termed LLM-based curriculum learning, which capitalizes on the robust semantic comprehension and discriminative prowess inherent in LLMs to gauge the complexity of GEC training data. Unlike traditional curriculum learning techniques, our method closely mirrors human expert-designed curriculums. Leveraging the proposed LLM-based CL method, we sequentially select varying levels of curriculums ranging from easy to hard, and iteratively train and refine using the pretrianed T5 and LLaMA series models. Through rigorous testing and analysis across diverse benchmark assessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19 development sets, our approach showcases a significant performance boost over baseline models and conventional curriculum learning methodologies.', 'abstract_zh': '尽管大规模语言模型（LLMs）在特定的自然语言处理（NLP）任务中展现了令人瞩目的能力，但在某些领域，如语法错误修正（GEC）方面，它们可能仍不如专门模型熟练。受课程学习概念的启发，我们深入探讨了如何通过有效的课程学习（CL）策略将LLMs培育成精通GEC的专业专家。在这篇文章中，我们介绍了一种新颖的方法——基于LLM的课程学习，该方法利用LLMs固有的强大语义理解和辨别能力来评估GEC训练数据的复杂性。与传统的课程学习方法不同，我们的方法更贴近人工专家设计的课程。利用提出的基于LLM的方法，我们按照从简单到复杂的顺序逐步选择不同难度级别的课程，并使用预训练的T5和LLaMA系列模型进行迭代训练和优化。通过在英语GEC的多种基准测试中进行严格的测试和分析，包括CoNLL14测试集、BEA19测试集和BEA19开发集，我们的方法展示了显著优于基线模型和传统课程学习方法的效果。', 'title_zh': 'LLMCL-GEC：利用LLM驱动的 Curriculum Learning 提升语法错误修正技术'}
{'arxiv_id': 'arXiv:2412.12532', 'title': 'Addressing Small and Imbalanced Medical Image Datasets Using Generative Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K Sampling', 'authors': 'Iman Khazrak, Shakhnoza Takhirova, Mostafa M. Rezaee, Mehrdad Yadollahi, Robert C. Green II, Shuteng Niu', 'link': 'https://arxiv.org/abs/2412.12532', 'abstract': 'The development of accurate medical image classification models is often constrained by privacy concerns and data scarcity for certain conditions, leading to small and imbalanced datasets. To address these limitations, this study explores the use of generative models, such as Denoising Diffusion Probabilistic Models (DDPM) and Progressive Growing Generative Adversarial Networks (PGGANs), for dataset augmentation. The research introduces a framework to assess the impact of synthetic images generated by DDPM and PGGANs on the performance of four models: a custom CNN, Untrained VGG16, Pretrained VGG16, and Pretrained ResNet50. Experiments were conducted using Random Sampling and Greedy K Sampling to create small, imbalanced datasets. The synthetic images were evaluated using Frechet Inception Distance (FID) and compared to original datasets through classification metrics. The results show that DDPM consistently generated more realistic images with lower FID scores and significantly outperformed PGGANs in improving classification metrics across all models and datasets. Incorporating DDPM-generated images into the original datasets increased accuracy by up to 6%, enhancing model robustness and stability, particularly in imbalanced scenarios. Random Sampling demonstrated superior stability, while Greedy K Sampling offered diversity at the cost of higher FID scores. This study highlights the efficacy of DDPM in augmenting small, imbalanced medical image datasets, improving model performance by balancing the dataset and expanding its size.', 'abstract_zh': '准确的医学图像分类模型的发展往往受到隐私问题和特定条件下数据稀缺性的限制，导致数据集规模小且不平衡。为解决这些限制，本研究探索了生成模型（如去噪扩散概率模型DDPM和渐进增长生成对抗网络PGGANs）在数据集扩充中的应用。研究引入了一个框架，评估DDPM和PGGAN生成的合成图像对四种模型（自定义CNN、未训练的VGG16、预训练的VGG16和预训练的ResNet50）性能的影响。实验使用随机采样和贪婪的K采样方法创建了小规模和不平衡的数据集。合成图像通过弗塞奇-印克距离（FID）进行评估，并与原始数据集通过分类指标进行比较。结果表明，DDPM始终生成更真实的图像，并且在所有模型和数据集中的分类指标上显著优于PGGANs。将DDPM生成的图像合并到原始数据集中，可提高准确率最高达6%，增强了模型的稳健性和稳定性，尤其是在不平衡的数据场景中表现尤为明显。随机采样在稳定性方面表现更优，而贪婪的K采样则在多样性上具有优势，但伴随更高的FID评分。本研究强调了DDPM在扩充小型不平衡医学图像数据集方面的有效性，通过平衡数据集并扩大其规模来提升模型性能。', 'title_zh': '使用生成模型解决小规模和不均衡医学图像数据集问题：基于随机K采样和贪婪K采样方法的DDPM和PGGAN对比研究'}
{'arxiv_id': 'arXiv:2412.12525', 'title': 'CREST: An Efficient Conjointly-trained Spike-driven Framework for Event-based Object Detection Exploiting Spatiotemporal Dynamics', 'authors': 'Ruixin Mao, Aoyu Shen, Lin Tang, Jun Zhou', 'link': 'https://arxiv.org/abs/2412.12525', 'abstract': 'Event-based cameras feature high temporal resolution, wide dynamic range, and low power consumption, which is ideal for high-speed and low-light object detection. Spiking neural networks (SNNs) are promising for event-based object recognition and detection due to their spiking nature but lack efficient training methods, leading to gradient vanishing and high computational complexity, especially in deep SNNs. Additionally, existing SNN frameworks often fail to effectively handle multi-scale spatiotemporal features, leading to increased data redundancy and reduced accuracy. To address these issues, we propose CREST, a novel conjointly-trained spike-driven framework to exploit spatiotemporal dynamics in event-based object detection. We introduce the conjoint learning rule to accelerate SNN learning and alleviate gradient vanishing. It also supports dual operation modes for efficient and flexible implementation on different hardware types. Additionally, CREST features a fully spike-driven framework with a multi-scale spatiotemporal event integrator (MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior object recognition & detection performance and up to 100X energy efficiency compared with state-of-the-art SNN algorithms on three datasets, providing an efficient solution for event-based object detection algorithms suitable for SNN hardware implementation.', 'abstract_zh': '基于事件的摄像机具有高时间分辨率、宽动态范围和低功耗的特点，使其非常适合高速和低光照条件下的物体检测。脉冲神经网络（SNN）因其脉冲特性在基于事件的物体识别和检测方面具有潜力，但缺乏有效的训练方法，从而导致梯度消失和高计算复杂度，特别是在深层SNN中。此外，现有的SNN框架往往不能有效处理时空尺度特征，导致数据冗余增加和检测准确性降低。为了解决这些问题，我们提出了一种新颖的联合训练脉冲驱动框架CREST（Conjointly Trained Spike-Driven Framework），以充分利用基于事件的物体检测中的时空动态特性。我们引入了联合学习规则以加速SNN的学习和缓解梯度消失问题。CREST还支持双操作模式，以便在不同硬件类型上高效灵活地实施。此外，CREST具备一个完全脉冲驱动框架，包括多尺度时空事件积分器（MESTOR）和时空交并比损失（ST-IoU loss）。我们的方法在三个数据集上实现了优越的对象识别与检测性能，并相比最先进的SNN算法在能效方面提高了高达100倍，提供了一种适用于SNN硬件实现的高效解决方案。', 'title_zh': 'CREST：一种高效的时间驱动联合训练框架，用于利用时空动态进行事件驱动的目标检测'}
{'arxiv_id': 'arXiv:2412.12522', 'title': 'Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust Text-to-SQL', 'authors': 'Geling Liu, Yunzhi Tan, Ruichao Zhong, Yuanzhen Xie, Lingchen Zhao, Qian Wang, Bo Hu, Zang Li', 'link': 'https://arxiv.org/abs/2412.12522', 'abstract': 'Recently, large language models (LLMs) have significantly improved the performance of text-to-SQL systems. Nevertheless, many state-of-the-art (SOTA) approaches have overlooked the critical aspect of system robustness. Our experiments reveal that while LLM-driven methods excel on standard datasets, their accuracy is notably compromised when faced with adversarial perturbations. To address this challenge, we propose a robust text-to-SQL solution, called Solid-SQL, designed to integrate with various LLMs. We focus on the pre-processing stage, training a robust schema-linking model enhanced by LLM-based data augmentation. Additionally, we design a two-round, structural similarity-based example retrieval strategy for in-context learning. Our method achieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general Spider and Bird benchmarks, respectively. Furthermore, experimental results show that Solid-SQL delivers an average improvement of 11.6% compared to baselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider benchmarks.', 'abstract_zh': '近年来，大规模语言模型（LLMs）显著提高了文本到SQL系统的性能。然而，许多最先进的（SOTA）方法忽略了系统鲁棒性这一关键方面。我们的实验表明，尽管LLM驱动的方法在标准数据集上表现出色，但在面对对抗性扰动时，其准确性显著下降。为了解决这一挑战，我们提出了一种鲁棒的文本到SQL解决方案，称为Solid-SQL，该方案旨在与各种LLM集成。我们重点关注预处理阶段，通过基于LLM的数据增强训练一个增强的模式链接模型。此外，我们设计了一种基于结构性相似性的两轮示例检索策略，以支持上下文学习。我们的方法分别在通用的Spider和Bird基准数据集上实现了82.1%和58.9%的SOTA SQL执行准确性。此外，实验结果表明，与基线方法相比，Solid-SQL在扰动的Spider-Syn、Spider-Realistic和Dr. Spider基准数据集上的平均改进率为11.6%。', 'title_zh': 'Solid-SQL：基于增强模式链接的上下文学习方法以提高稳健的文本到SQL转换能力'}
{'arxiv_id': 'arXiv:2412.12500', 'title': 'Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models', 'authors': 'Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel', 'link': 'https://arxiv.org/abs/2412.12500', 'abstract': 'Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translation, using regression models and SHAP values across 204 languages. Our findings identify token similarity and country similarity as pivotal factors, alongside pre-train data and model size, in enhancing model performance. Token similarity facilitates cross-lingual transfer, while country similarity highlights the importance of shared cultural and linguistic contexts. These insights offer valuable guidance for developing more equitable and effective multilingual language models, particularly for underrepresented languages.', 'abstract_zh': '多语言语言模型（MLLMs）对于处理多种语言的文本至关重要，但由于资源可用性和语言特征的差异，它们通常表现出性能差异。虽然预训练数据比例和模型大小对性能的影响广为人知，但我们的研究表明，还有其他重要因素显著影响MLLM的有效性。我们通过分析地理、语言和资源相关方面的广泛特征，利用回归模型和SHAP值对204种语言进行了分析，首先使用SIB-200数据集进行分类，然后使用Flores-200数据集进行机器翻译。我们的研究发现，标记相似性和国家相似性是提升模型性能的关键因素，与预训练数据和模型大小一样重要。标记相似性促进了跨语言迁移，而国家相似性强调了共享文化与语言背景的重要性。这些见解为开发更具公平性和有效性的多语言语言模型，特别是为未充分代表的语言提供了宝贵指导。', 'title_zh': '超越数据量：多语言语言模型性能的关键驱动因素'}
{'arxiv_id': 'arXiv:2412.12499', 'title': 'LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for Low-Resource Language Tasks', 'authors': 'Hongbin Zhang, Kehai Chen, Xuefeng Bai, Yang Xiang, Min Zhang', 'link': 'https://arxiv.org/abs/2412.12499', 'abstract': 'Large language models (LLMs) have demonstrated impressive multilingual understanding and reasoning capabilities, driven by extensive pre-training multilingual corpora and fine-tuning instruction data. However, a performance gap persists between high-resource and low-resource language tasks due to language imbalance in the pre-training corpus, even using more low-resource data during fine-tuning. To alleviate this issue, we propose LinguaLIFT, a two-stage instruction tuning framework for advancing low-resource language tasks. An additional language alignment layer is first integrated into the LLM to adapt a pre-trained multilingual encoder, thereby enhancing multilingual alignment through code-switched fine-tuning. The second stage fine-tunes LLM with English-only instruction data while freezing the language alignment layer, allowing LLM to transfer task-specific capabilities from English to low-resource language tasks. Additionally, we introduce the Multilingual Math World Problem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and 10 high-resource languages, enabling comprehensive evaluation of multilingual reasoning. Experimental results show that LinguaLIFT outperforms several competitive baselines across MMWP and other widely used benchmarks.', 'abstract_zh': '大型语言模型（LLMs）通过广泛预训练的多语言语料库和微调指令数据已经展示了令人印象深刻的多语言理解和推理能力。然而，由于预训练语料库中的语言不平衡，使用更多低资源数据进行微调也无法完全解决高资源语言任务和低资源语言任务之间存在的性能差距。为了解决这一问题，我们提出了LinguaLIFT，一种旨在推进低资源语言任务的两阶段指令微调框架。首先，在LLM中集成一个额外的语言对齐层，以适应预训练的多语言编码器，通过代码混用微调增强多语言对齐。第二阶段使用仅限英语的指令数据微调LLM，同时冻结语言对齐层，使LLM能够从英语领域转移特定能力到低资源语言任务。此外，我们引入了多语言数学世界问题（MMWP）基准，涵盖21种低资源、17种中资源和10种高资源语言，从而使得多语言推理的全面评估成为可能。实验结果表明，LinguaLIFT在MMWP和其他广泛使用的基准上优于多个竞争基线。', 'title_zh': 'LinguaLIFT：一种有效的两阶段指令调优框架，用于低资源语言任务'}
{'arxiv_id': 'arXiv:2412.12496', 'title': 'Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training', 'authors': 'Mingjia Shi, Yuhao Zhou, Ruiji Yu, Zekai Li, Zhiyuan Liang, Xuanlei Zhao, Xiaojiang Peng, Tanmay Rajpurohit, Shanmukha Ramakrishna Vedantam, Wangbo Zhao, Kai Wang, Yang You', 'link': 'https://arxiv.org/abs/2412.12496', 'abstract': 'Vision Mamba (e.g., Vim) has successfully been integrated into computer vision, and token reduction has yielded promising outcomes in Vision Transformers (ViTs). However, token reduction performs less effectively on Vision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a high loss of key knowledge and bad performance. This makes it not a good solution for enhancing efficiency in Mamba. Token merging, which preserves more token information than pruning, has demonstrated commendable performance in ViTs. Nevertheless, vanilla merging performance decreases as the reduction ratio increases either, failing to maintain the key knowledge in Mamba. Re-training the token-reduced model enhances the performance of Mamba, by effectively rebuilding the key knowledge. Empirically, pruned Vims only drop up to 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in our main evaluation. We show how simple and effective the fast recovery can be achieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs of training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17 minutes, and Vim-S only drop 1.3% with 1.2x (up to 1.5x) speed up in inference.', 'abstract_zh': 'Vision Mamba（例如，类似于 VIM）已成功集成到计算机视觉中，而标记减少在视觉变换器（ViTs）中取得了令人鼓舞的结果。然而，标记减少在 Mamba 中的效果不如 ViTs。在 Mamba 中进行信息性标记的剪枝会导致关键知识的高损失和性能下降，这使得它不是一个好的方案来提高 Mamba 的效率。与剪枝相比，保留更多信息的标记合并已在 ViTs 中展示了出色的表现。然而，随着减少比例的增加，简单的合并性能也会下降，无法保持 Mamba 中的关键知识。通过重新训练标记减少后的模型，可以有效重建关键知识，从而提高 Mamba 的性能。在实证研究中，剪枝的 VIM 在 ImageNet-1K 上的准确率仅下降了 0.9%，并由我们提出的方法 R-MeeTo 在主要评估中恢复。我们展示了如何在分钟级别实现快速恢复，并具体展示了在 VIM-Ti 上训练 3 个周期后，准确率提高了 35.9%。此外，Vim-Ti/S/B 在 5/7/17 分钟内重新训练，Vim-S 在 1.2 倍（最多 1.5 倍）的速度提升下仅下降了 1.3% 的准确率。', 'title_zh': '更快的Vision Mamba 通过合并_token_重新训练可在几分钟内重新构建'}
{'arxiv_id': 'arXiv:2412.12493', 'title': 'A Simple and Fast Way to Handle Semantic Errors in Transactions', 'authors': 'Jinghan Zeng, Eugene Wu, Sanjay Krishnan', 'link': 'https://arxiv.org/abs/2412.12493', 'abstract': 'Many computer systems are now being redesigned to incorporate LLM-powered agents, enabling natural language input and more flexible operations. This paper focuses on handling database transactions created by large language models (LLMs). Transactions generated by LLMs may include semantic errors, requiring systems to treat them as long-lived. This allows for human review and, if the transaction is incorrect, removal from the database history. Any removal action must ensure the database\'s consistency (the "C" in ACID principles) is maintained throughout the process.\nWe propose a novel middleware framework based on Invariant Satisfaction (I-Confluence), which ensures consistency by identifying and coordinating dependencies between long-lived transactions and new transactions. This middleware buffers suspicious or compensating transactions to manage coordination states. Using the TPC-C benchmark, we evaluate how transaction generation frequency, user reviews, and invariant completeness impact system performance. For system researchers, this study establishes an interactive paradigm between LLMs and database systems, providing an "undoing" mechanism for handling incorrect operations while guaranteeing database consistency. For system engineers, this paper offers a middleware design that integrates removable LLM-generated transactions into existing systems with minimal modifications.', 'abstract_zh': '许多计算机系统现在正被重新设计，以集成由大型语言模型（LLMs）驱动的代理，这使得自然语言输入和更灵活的操作成为可能。本文主要关注由大型语言模型生成的数据库事务。由LLMs生成的事务可能包含语义错误，因此需要系统将这些事务视为长期事务进行处理，以便进行人工审核，如果事务存在错误，还可以将其从数据库历史记录中移除。任何移除操作都必须在整个过程中确保数据库的一致性（ACID原则中的“C”）。\n\n我们提出了一种基于不变性满足（I-Confluence）的新middleware框架，该框架通过识别和协调长期事务和新事务之间的依赖性来确保一致性。此middleware用于缓存可疑或补偿性事务，以管理协调状态。使用TPC-C基准测试，我们评估了事务生成频率、用户审核和不变性完整性对系统性能的影响。对于系统研究人员，本研究建立了一个LLMs与数据库系统之间的交互范式，提供了处理错误操作的同时保证数据库一致性的“撤销”机制。对于系统工程师，本文提供了一种middleware设计方案，该方案能够在对现有系统进行最小修改的情况下整合可移除的LLM生成事务。', 'title_zh': '处理交易中语义错误的一种简单快速方法'}
{'arxiv_id': 'arXiv:2412.12486', 'title': 'Boosting Long-Context Information Seeking via Query-Guided Activation Refilling', 'authors': 'Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian', 'link': 'https://arxiv.org/abs/2412.12486', 'abstract': "Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query's information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs.\nIn the paper, we propose a method for processing long-context information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE's effectiveness, achieving improvements in both performance and efficiency.", 'abstract_zh': '处理长上下文是大型语言模型（LLMs）面临的重要挑战，原因在于它们固有的上下文窗口限制以及大量键值（KV）激活所导致的计算负担，这严重影响了效率。对于信息检索任务而言，全面感知整个上下文往往是不必要的，因为查询的信息需求可以因其复杂性不同而动态变化，从局部细节到全局视角不等。然而，现有方法难以有效地适应这些动态的信息需求。\n\n在本文中，我们提出了一种通过查询引导的激活补充（Activation Refilling, ACRE）方法来处理长上下文的信息检索任务。ACRE 构建了一个双层键值缓存体系结构，其中一层（L1）缓存紧凑地捕捉全局信息，而第二层（L2）缓存则提供详细的局部信息。ACRE 建立了两层缓存之间的代理关系，使得输入查询能够关注 L1 缓存，并动态地从 L2 缓存中补充相关条目。这种机制结合了全局理解与查询特定的局部细节，从而提高了答案的解码效果。在多种长上下文信息检索数据集上的实验表明，ACRE 能够提高性能和效率。\n\n该中文翻译符合学术规范，并保留了原文的技术细节和专业术语。', 'title_zh': '通过查询引导的激活补充增强长上下文信息检索'}
{'arxiv_id': 'arXiv:2412.12484', 'title': 'Evolutionary Optimization for Designing Variational Quantum Circuits with High Model Capacity', 'authors': 'Samuel Yen-Chi Chen', 'link': 'https://arxiv.org/abs/2412.12484', 'abstract': 'Recent advancements in quantum computing (QC) and machine learning (ML) have garnered significant attention, leading to substantial efforts toward the development of quantum machine learning (QML) algorithms to address a variety of complex challenges. The design of high-performance QML models, however, requires expert-level knowledge, posing a significant barrier to the widespread adoption of QML. Key challenges include the design of data encoding mechanisms and parameterized quantum circuits, both of which critically impact the generalization capabilities of QML models. We propose a novel method that encodes quantum circuit architecture information to enable the evolution of quantum circuit designs. In this approach, the fitness function is based on the effective dimension, allowing for the optimization of quantum circuits towards higher model capacity. Through numerical simulations, we demonstrate that the proposed method is capable of discovering variational quantum circuit architectures that offer improved learning capabilities, thereby enhancing the overall performance of QML models for complex tasks.', 'abstract_zh': '近年来，量子计算（QC）和机器学习（ML）的最新进展引发了广泛关注，推动了量子机器学习（QML）算法的发展，以应对各种复杂的挑战。然而，设计高性能的QML模型需要专家级别的知识，这成为QML广泛应用的一大障碍。主要挑战包括量子电路编码机制和参数化量子电路的设计，两者对QML模型的泛化能力至关重要。为了克服这些挑战，我们提出了一种新型方法，通过编码量子电路架构信息来促进量子电路设计的优化。在该方法中，适应度函数基于有效维度，使量子电路朝着更高的模型容量进行优化。通过数值仿真，我们证明了所提出的方法能够发现具有增强学习能力的变分量子电路架构，从而整体提升QML模型在复杂任务中的性能。', 'title_zh': '基于进化的优化方法设计具有高模型容量的变量子电路'}
{'arxiv_id': 'arXiv:2412.12480', 'title': "Subversion Strategy Eval: Evaluating AI's stateless strategic capabilities against control protocols", 'authors': 'Alex Mallen, Charlie Griffin, Alessandro Abate, Buck Shlegeris', 'link': 'https://arxiv.org/abs/2412.12480', 'abstract': "AI control protocols are plans for usefully deploying AI systems in a way that is safe, even if the AI intends to subvert the protocol. Previous work evaluated protocols by subverting them with a human-AI red team, where an AI follows the human-written strategy. This paper investigates how well AI systems can generate and act on strategies for subverting control protocols whilst operating without private memory. Usually, decision-makers only need the ability to generate optimal plans. However, to subvert a protocol, an AI system must be able to reliably generate optimal plans in each context; coordinate plans with other instances of itself without communicating; and take actions with well-calibrated probabilities. We develop evaluations for a model's ability to generate subversion strategies and develop affordance experiments that help isolate component capabilities. Our initial evaluations with Claude-3.5-sonnet highlight reasons why statelessly strategizing to subvert control protocols might be hard. We hope that our work can serve as a starting point for evaluations aimed at assessing whether a model is capable of strategizing to subvert control protocols.", 'abstract_zh': 'AI控制协议是指一种有效部署AI系统的方法，即使AI有意颠覆该协议也能确保安全。先前的研究通过由人类-AI红队实施颠覆来评估这些协议，即AI遵循人类编写的策略执行颠覆行为。本文探讨了AI系统在没有私有记忆的情况下生成并执行颠覆控制协议策略的能力。通常，决策者仅需要具备生成最优计划的能力。然而，要颠覆一个协议，AI系统必须在每个上下文中可靠地生成最优计划；不通过通信就能与其他自身实例协调计划；并且采取精确概率的动作。我们开发了评估模型生成颠覆策略能力的标准，并设计了功能实验以分离出各个组成部分的能力。我们对Claude-3.5-Sonnet的初步评估指出了为何在没有状态的情况下制定颠覆控制协议策略可能是困难的。我们希望我们的研究能够成为评估模型是否能够制定颠覆控制协议策略标准的基础。', 'title_zh': '潜伏策略评估：评估AI在无状态策略能力方面与控制协议的对比优势'}
{'arxiv_id': 'arXiv:2412.12475', 'title': 'RareAgents: Autonomous Multi-disciplinary Team for Rare Disease Diagnosis and Treatment', 'authors': 'Xuanzhong Chen, Ye Jin, Xiaohao Mao, Lun Wang, Shuyang Zhang, Ting Chen', 'link': 'https://arxiv.org/abs/2412.12475', 'abstract': 'Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the huge number of diseases. The complexity of symptoms and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable improvements across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical exams. However, current agent frameworks lack adaptation for real-world clinical scenarios, especially those involving the intricate demands of rare diseases. To address these challenges, we present RareAgents, the first multi-disciplinary team of LLM-based agents tailored to the complex clinical context of rare diseases. RareAgents integrates advanced planning capabilities, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents surpasses state-of-the-art domain-specific models, GPT-4o, and existing agent frameworks in both differential diagnosis and medication recommendation for rare diseases. Furthermore, we contribute a novel dataset, MIMIC-IV-Ext-Rare, derived from MIMIC-IV, to support further advancements in this field.', 'abstract_zh': '尽管罕见疾病的个体发病率较低，但由于罕见疾病数量众多，它们仍影响着全球约3亿人口。症状的复杂性和缺乏相关经验的专业医生使得罕见疾病的诊断和治疗比常见疾病更加具有挑战性。最近，由大规模语言模型（LLMs）驱动的代理正在多个领域展现出显著的进步。在医疗领域，一些代理方法在医学考试中的问答任务上已经超过了直接提示。然而，当前的代理框架缺乏适应现实临床场景的能力，特别是在面对罕见疾病特有的复杂需求时。为了解决这些挑战，我们提出了RareAgents——首个针对罕见疾病复杂临床环境的多学科团队，基于LLM的代理体系。RareAgents整合了高级规划能力、记忆机制和医疗工具的应用，以Llama-3.1-8B/70B作为基础模型。实验结果表明，RareAgents在罕见疾病的鉴别诊断和药物推荐方面均优于现有领域特定的模型GPT-4o和现有的代理框架。此外，我们还贡献了一个新的数据集MIMIC-IV-Ext-Rare，该数据集源自MIMIC-IV，旨在支持该领域的进一步发展。', 'title_zh': '稀有疾病工作组：自主多学科团队在稀有疾病诊断与治疗中的应用'}
{'arxiv_id': 'arXiv:2412.12469', 'title': 'Optimal Control Operator Perspective and a Neural Adaptive Spectral Method', 'authors': 'Mingquan Feng, Zhijie Chen, Yixin Huang, Yizhou Liu, Junchi Yan', 'link': 'https://arxiv.org/abs/2412.12469', 'abstract': 'Optimal control problems (OCPs) involve finding a control function for a dynamical system such that a cost functional is optimized. It is central to physical systems in both academia and industry. In this paper, we propose a novel instance-solution control operator perspective, which solves OCPs in a one-shot manner without direct dependence on the explicit expression of dynamics or iterative optimization processes. The control operator is implemented by a new neural operator architecture named Neural Adaptive Spectral Method (NASM), a generalization of classical spectral methods. We theoretically validate the perspective and architecture by presenting the approximation error bounds of NASM for the control operator. Experiments on synthetic environments and a real-world dataset verify the effectiveness and efficiency of our approach, including substantial speedup in running time, and high-quality in- and out-of-distribution generalization.', 'abstract_zh': '最优控制问题（Optimal Control Problems, OCPs）涉及寻找一个控制函数以优化动态系统的成本泛函。它们在学术界和工业界中的物理系统中占据核心地位。本文提出了一种新颖的实例-解控制运算符视角，能够以一次性的方式解决OCPs，而不直接依赖于动态系统的显式表达或迭代优化过程。该控制运算符通过一个名为神经自适应谱方法（Neural Adaptive Spectral Method, NASM）的新神经运算架构来实现，该架构是对经典谱方法的一种推广。我们通过展示NASM对控制运算符的逼近误差边界来理论上验证这种视角和架构。在合成环境和真实世界数据集上的实验验证了我们方法的有效性和效率，包括运行时间显著加快以及良好的分布内和分布外泛化能力。', 'title_zh': '最优控制算子视角与神经自适应谱方法'}
{'arxiv_id': 'arXiv:2412.12468', 'title': 'Transferable and Forecastable User Targeting Foundation Model', 'authors': 'Bin Dou, Baokun Wang, Yun Zhu, Xiaotong Lin, Yike Xu, Xiaorui Huang, Yang Chen, Yun Liu, Shaoshuai Han, Yongchao Liu, Tianyi Zhang, Yu Cheng, Weiqiang Wang, Chuntao Hong', 'link': 'https://arxiv.org/abs/2412.12468', 'abstract': 'User targeting, the process of selecting targeted users from a pool of candidates for non-expert marketers, has garnered substantial attention with the advancements in digital marketing. However, existing user targeting methods encounter two significant challenges: (i) Poor cross-domain and cross-scenario transferability and generalization, and (ii) Insufficient forecastability in real-world applications. These limitations hinder their applicability across diverse industrial scenarios. In this work, we propose FIND, an industrial-grade, transferable, and forecastable user targeting foundation model. To enhance cross-domain transferability, our framework integrates heterogeneous multi-scenario user data, aligning them with one-sentence targeting demand inputs through contrastive pre-training. For improved forecastability, the text description of each user is derived based on anticipated future behaviors, while user representations are constructed from historical information. Experimental results demonstrate that our approach significantly outperforms existing baselines in cross-domain, real-world user targeting scenarios, showcasing the superior capabilities of FIND. Moreover, our method has been successfully deployed on the Alipay platform and is widely utilized across various scenarios.', 'abstract_zh': '用户定向是指从候选人池中选择目标用户的过程，这一过程对非专业人士的营销者来说尤为重要。随着数字营销技术的发展，用户定向已经受到了广泛关注。然而，现有的用户定向方法面临着两个主要挑战：(i) 跨领域和跨场景的迁移性和泛化能力较差；(ii) 在实际应用中的可预测性不足。这些限制使其难以在不同工业场景中广泛应用。本工作中，我们提出了 FIND，一种工业级、可迁移且可预测的用户定向基础模型。为了增强跨领域的迁移性，我们的框架整合了异构多场景用户数据，并通过对比预训练将其与简洁的定向需求输入进行对齐。为了提高预测性，我们根据预期的未来行为来描述每个用户，并利用历史信息构建用户表示。实验结果表明，我们的方法在跨领域和实际应用的用户定向场景中显著优于现有基线方法，展示了 FIND 的优越性能。此外，该方法已在支付宝平台成功部署，并广泛应用于各种场景中。', 'title_zh': '可迁移且可预测的用户目标定位基础模型'}
{'arxiv_id': 'arXiv:2412.12463', 'title': 'Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy', 'authors': 'Aditya Ganeshan, Thibault Groueix, Paul Guerrero, Radomír Měch, Matthew Fisher, Daniel Ritchie', 'link': 'https://arxiv.org/abs/2412.12463', 'abstract': 'Pattern images are everywhere in the digital and physical worlds, and tools to edit them are valuable. But editing pattern images is tricky: desired edits are often programmatic: structure-aware edits that alter the underlying program which generates the pattern. One could attempt to infer this underlying program, but current methods for doing so struggle with complex images and produce unorganized programs that make editing tedious. In this work, we introduce a novel approach to perform programmatic edits on pattern images. By using a pattern analogy -- a pair of simple patterns to demonstrate the intended edit -- and a learning-based generative model to execute these edits, our method allows users to intuitively edit patterns. To enable this paradigm, we introduce SplitWeave, a domain-specific language that, combined with a framework for sampling synthetic pattern analogies, enables the creation of a large, high-quality synthetic training dataset. We also present TriFuser, a Latent Diffusion Model (LDM) designed to overcome critical issues that arise when naively deploying LDMs to this task. Extensive experiments on real-world, artist-sourced patterns reveals that our method faithfully performs the demonstrated edit while also generalizing to related pattern styles beyond its training distribution.', 'abstract_zh': '在数字和物理世界中，模式图像无处不在，对其进行编辑的工具非常宝贵。但是，编辑模式图像颇具挑战性：通常需要进行结构感知的编辑，即修改生成模式的底层程序。人们可能会尝试推断这一底层程序，但当前的方法在面对复杂图像时往往力不从心，并生成难以组织的程序，使得编辑过程繁琐。在本研究中，我们介绍了一种新颖的方法来进行模式图像的结构化编辑。通过使用模式类比——即用一对简单的模式来展示期望的编辑——以及基于学习的生成模型来执行这些编辑，我们的方法允许用户直观地编辑模式。为了实现这一范式，我们引入了SplitWeave，这是一种领域特定语言，结合合成模式类比的抽样框架，能够生成大量高质量的合成训练数据集。我们还介绍了TriFuser，这是一种.latent扩散模型（LDM），旨在解决直接将LDM应用于此任务时出现的关键问题。在针对实际来源艺术家提供的模式进行的广泛实验中，我们发现，我们的方法不仅能够准确执行演示的编辑，还能够泛化到与其训练分布相关的其他模式风格。', 'title_zh': '模式类比：通过类比学习执行程序化图像编辑'}
{'arxiv_id': 'arXiv:2412.12459', 'title': 'LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework', 'authors': 'Chia-Hsuan Chang, Jui-Tse Tsai, Yi-Hang Tsai, San-Yih Hwang', 'link': 'https://arxiv.org/abs/2412.12459', 'abstract': 'Topic modeling is widely used for uncovering thematic structures within text corpora, yet traditional models often struggle with specificity and coherence in domain-focused applications. Guided approaches, such as SeededLDA and CorEx, incorporate user-provided seed words to improve relevance but remain labor-intensive and static. Large language models (LLMs) offer potential for dynamic topic refinement and discovery, yet their application often incurs high API costs. To address these challenges, we propose the LLM-assisted Iterative Topic Augmentation framework (LITA), an LLM-assisted approach that integrates user-provided seeds with embedding-based clustering and iterative refinement. LITA identifies a small number of ambiguous documents and employs an LLM to reassign them to existing or new topics, minimizing API costs while enhancing topic quality. Experiments on two datasets across topic quality and clustering performance metrics demonstrate that LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an efficient and adaptable framework for advancing topic modeling and text clustering.', 'abstract_zh': '主题建模在文本语料中广泛用于发现主题结构，但传统的模型在领域聚焦的应用中往往难以做到精准和连贯。引导式方法，如SeededLDA和CorEx，通过引入用户提供的种子词来提高相关性，但这些方法仍然劳动密集且缺乏灵活性。大型语言模型（LLMs）为动态主题重塑和发现提供了潜力，但其应用往往会产生较高的API成本。为了解决这些问题，我们提出了一种大型语言模型辅助的迭代主题增强框架（LITA），该框架将用户提供的种子词与基于嵌入的聚类和迭代优化相结合。LITA 识别少量模糊文档，并利用LLM 将其重新分配到现有主题或新主题中，从而在降低API成本的同时提高主题质量。实验结果表明，在主题质量和聚类性能指标方面，LITA 都优于包括LDA、SeededLDA、CorEx、BERTopic 和 PromptTopic 等五种基线模型。我们的工作提供了一种高效且可适应的主题建模和文本聚类框架。', 'title_zh': 'LITA：一种高效的基于大型语言模型的迭代主题增强框架'}
{'arxiv_id': 'arXiv:2412.12456', 'title': 'Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks', 'authors': 'Xunkai Li, Zhengyu Wu, Jiayi Wu, Hanwen Cui, Jishuo Jia, Rong-Hua Li, Guoren Wang', 'link': 'https://arxiv.org/abs/2412.12456', 'abstract': "With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \\url{this https URL}.", 'abstract_zh': '随着跨域文本归因图（TAG）数据（例如引用网络、推荐系统、社交网络和ai4science）的普及，图神经网络（GNNs）和大型语言模型（LLMs）在统一模型架构中的集成（例如，LLMs作为增强器、LLMs作为合作者、LLMs作为预测器）已逐渐成为一个有前景的技术范式。这一新图学习范式的核心在于结合GNNs捕捉复杂结构关系的能力和LLMs从丰富的图文本描述中理解有意义上下文的能力。因此，我们可以通过利用具有丰富语义上下文的图描述文本从根本上提升数据质量，进而根据以数据为中心的机器学习原则增强模型中心方法的表示能力。通过利用这两种不同神经网络架构的优势，这种集成方法可以解决广泛的基于TAG的任务（例如，图学习、图推理和图问答），特别是在复杂的工业场景中（例如，监督场景、少样本场景和零样本场景）。换句话说，我们可以将文本作为一种媒介，使图学习模型在不同数据域中的下游图基任务上具有跨域通用性，使单一图模型能够有效地处理多样化的任务。本工作为希望在快速发展的LLM背景下推进图学习方法的研究人员和实践者提供了一个基础参考。我们在此持续维护相关的开源材料，链接为：\\[this https URL\\]。', 'title_zh': '在大语言模型时代下的图学习：从数据、模型和任务视角的综述'}
{'arxiv_id': 'arXiv:2412.12447', 'title': 'PERC: Plan-As-Query Example Retrieval for Underrepresented Code Generation', 'authors': 'Jaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim, Seung-won Hwang', 'link': 'https://arxiv.org/abs/2412.12447', 'abstract': 'Code generation with large language models has shown significant promise, especially when employing retrieval-augmented generation (RAG) with few-shot examples. However, selecting effective examples that enhance generation quality remains a challenging task, particularly when the target programming language (PL) is underrepresented. In this study, we present two key findings: (1) retrieving examples whose presented algorithmic plans can be referenced for generating the desired behavior significantly improves generation accuracy, and (2) converting code into pseudocode effectively captures such algorithmic plans, enhancing retrieval quality even when the source and the target PLs are different. Based on these findings, we propose Plan-as-query Example Retrieval for few-shot prompting in Code generation (PERC), a novel framework that utilizes algorithmic plans to identify and retrieve effective examples. We validate the effectiveness of PERC through extensive experiments on the CodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms the state-of-the-art RAG methods in code generation, both when the source and target programming languages match or differ, highlighting its adaptability and robustness in diverse coding environments.', 'abstract_zh': '大规模语言模型在代码生成方面展现了显著的潜力，尤其是在结合检索增强生成（RAG）和少量示例的情况下。然而，选择能够提升生成质量的有效示例仍是一项具有挑战性的任务，特别是在目标编程语言（PL）不占优势的情况下。在本研究中，我们提出两项关键发现：(1) 获取可以参考以生成所需行为的算法计划的示例，显著提高了生成准确性；(2) 将代码转换为伪代码能够有效捕捉这些算法计划，即使源编程语言与目标编程语言不同，也能提升检索质量。基于以上发现，我们提出了一种新的框架——代码生成中的少量示例提示计划查询示例检索（PERC），该框架利用算法计划来识别和检索有效示例。我们通过在CodeContests、HumanEval和MultiPL-E基准上的广泛实验验证了PERC的有效性：无论源编程语言与目标编程语言是否匹配，PERC始终优于现有的RAG方法，在多种编程环境中都展示了其适应性和鲁棒性。', 'title_zh': 'PERC：计划作为查询的稀有代码生成示例检索'}
{'arxiv_id': 'arXiv:2412.12444', 'title': 'LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers', 'authors': 'Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao, Yanzhi Wang, Jiuxiang Gu', 'link': 'https://arxiv.org/abs/2412.12444', 'abstract': 'Diffusion Transformers have emerged as the preeminent models for a wide array of generative tasks, demonstrating superior performance and efficacy across various applications. The promising results come at the cost of slow inference, as each denoising step requires running the whole transformer model with a large amount of parameters. In this paper, we show that performing the full computation of the model at each diffusion step is unnecessary, as some computations can be skipped by lazily reusing the results of previous steps. Furthermore, we show that the lower bound of similarity between outputs at consecutive steps is notably high, and this similarity can be linearly approximated using the inputs. To verify our demonstrations, we propose the \\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached results from earlier steps to skip redundant computations. Specifically, we incorporate lazy learning layers into the model, effectively trained to maximize laziness, enabling dynamic skipping of redundant computations. Experimental results show that LazyDiT outperforms the DDIM sampler across multiple diffusion transformer models at various resolutions. Furthermore, we implement our method on mobile devices, achieving better performance than DDIM with similar latency.', 'abstract_zh': '差分变换器已经成为了广泛生成任务的主导模型，展示了在各种应用中卓越的性能和效果。虽然表现出色，但这些模型在推理时速度较慢，因为在每一项去噪步骤中都需要运行整个具有大量参数的变压器模型。在本文中，我们证明了在每个差分步骤中执行完整的模型计算并非必要，通过懒惰地重用之前步骤的结果可以跳过某些计算。此外，我们还证明了连续步骤输出之间的相似度下限较高，并且这种相似度可以用输入进行线性近似。为了验证这些演示，我们提出了一种称为**LazyDiT** 的懒惰学习框架，该框架能够高效地利用较早步骤中的缓存结果来跳过冗余计算。具体来说，我们将在模型中嵌入懒惰学习层，这些层能够动态地跳过冗余计算，并且被有效训练以最大化懒惰性。实验结果表明，LazyDiT 在不同分辨率的多种差分变换器模型上的表现优于 DDIM 采样器。此外，我们还在移动设备上实现了该方法，能够在相似延迟的情况下获得更好的性能。', 'title_zh': 'LazyDiT：懒学习在加速扩散变换器中的应用'}
{'arxiv_id': 'arXiv:2412.12441', 'title': 'Numerical Pruning for Efficient Autoregressive Models', 'authors': 'Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Jing Liu, Ruiyi Zhang, Ryan A. Rossi, Hao Tan, Tong Yu, Xiang Chen, Yufan Zhou, Tong Sun, Pu Zhao, Yanzhi Wang, Jiuxiang Gu', 'link': 'https://arxiv.org/abs/2412.12441', 'abstract': "Transformers have emerged as the leading architecture in deep learning, proving to be versatile and highly effective across diverse domains beyond language and image processing. However, their impressive performance often incurs high computational costs due to their substantial model size. This paper focuses on compressing decoder-only transformer-based autoregressive models through structural weight pruning to improve the model efficiency while preserving performance for both language and image generation tasks. Specifically, we propose a training-free pruning method that calculates a numerical score with Newton's method for the Attention and MLP modules, respectively. Besides, we further propose another compensation algorithm to recover the pruned model for better performance. To verify the effectiveness of our method, we provide both theoretical support and extensive experiments. Our experiments show that our method achieves state-of-the-art performance with reduced memory usage and faster generation speeds on GPUs.", 'abstract_zh': 'Transformer架构已经成为了深度学习领域的主导结构，展现出在语言和图像处理之外的多种领域中高度通用和高效的特点。然而，它们出色的性能往往伴随着高昂的计算成本，这主要是由于模型规模庞大所致。本文旨在通过结构权重剪枝的方法压缩仅包含解码器的Transformer自回归模型，以提高模型效率同时保持其在语言和图像生成任务中的性能。具体而言，我们提出了一种无需训练的剪枝方法，通过牛顿法分别对注意力模块和多层前馈网络模块计算数值评分。此外，我们还提出了一种补偿算法，用于提高剪枝模型的性能。为了验证我们方法的有效性，我们提供了理论支持及广泛的实验研究。实验结果表明，我们的方法在减少内存使用和提高GPU生成速度的同时，实现了最先进的性能。', 'title_zh': '高效的自回归模型的数值剪枝方法'}
{'arxiv_id': 'arXiv:2412.12432', 'title': 'Three Things to Know about Deep Metric Learning', 'authors': 'Yash Patel, Giorgos Tolias, Jiri Matas', 'link': 'https://arxiv.org/abs/2412.12432', 'abstract': 'This paper addresses supervised deep metric learning for open-set image retrieval, focusing on three key aspects: the loss function, mixup regularization, and model initialization. In deep metric learning, optimizing the retrieval evaluation metric, recall@k, via gradient descent is desirable but challenging due to its non-differentiable nature. To overcome this, we propose a differentiable surrogate loss that is computed on large batches, nearly equivalent to the entire training set. This computationally intensive process is made feasible through an implementation that bypasses the GPU memory limitations. Additionally, we introduce an efficient mixup regularization technique that operates on pairwise scalar similarities, effectively increasing the batch size even further. The training process is further enhanced by initializing the vision encoder using foundational models, which are pre-trained on large-scale datasets. Through a systematic study of these components, we demonstrate that their synergy enables large models to nearly solve popular benchmarks.', 'abstract_zh': '本文针对开放集图像检索的监督深度度量学习问题，重点探讨了三个关键方面：损失函数、mixup 正则化以及模型初始化。在深度度量学习中，通过梯度下降优化检索评估指标 recall@k 是一个重要的目标，但由于其非可微特性，这使得优化过程变得具有挑战性。为了克服这一困难，我们提出了一种可微的替代损失函数，该损失在大规模批处理中计算，几乎等同于整个训练集的规模。这种计算密集型的过程通过绕过 GPU 内存限制的实现变得可行。此外，我们还引入了一种高效的 mixup 正则化技术，该技术基于成对标量相似性操作，即使进一步增加了批处理大小。通过使用预训练在大规模数据集上的基础模型初始化视觉编码器，进一步提升了训练过程。通过对这些组件进行系统性研究，我们证明了它们之间的协同作用使得大型模型能够近乎解决流行的基准问题。', 'title_zh': '关于深度度量学习的三项了解'}
{'arxiv_id': 'arXiv:2412.12417', 'title': 'Bridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments', 'authors': 'Tuka Alhanai, Adam Kasumovic, Mohammad Ghassemi, Aven Zitzelberger, Jessica Lundin, Guillaume Chabot-Couture', 'link': 'https://arxiv.org/abs/2412.12417', 'abstract': 'Large Language Models (LLMs) have shown remarkable performance across various tasks, yet significant disparities remain for non-English languages, and especially native African languages. This paper addresses these disparities by creating approximately 1 million human-translated words of new benchmark data in 8 low-resource African languages, covering a population of over 160 million speakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho (Southern Sotho), Setswana, and Tsonga. Our benchmarks are translations of Winogrande and three sections of MMLU: college medicine, clinical knowledge, and virology. Using the translated benchmarks, we report previously unknown performance gaps between state-of-the-art (SOTA) LLMs in English and African languages. Finally, using results from over 400 fine-tuned models, we explore several methods to reduce the LLM performance gap, including high-quality dataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and cultural appropriateness adjustments. Key findings include average mono-lingual improvements of 5.6% with fine-tuning (with 5.4% average mono-lingual improvements when using high-quality data over low-quality data), 2.9% average gains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost on culturally appropriate questions. The publicly available benchmarks, translations, and code from this study support further research and development aimed at creating more inclusive and effective language technologies.', 'abstract_zh': '大型语言模型（LLMs）在各种任务上表现出色，但在非英语语言，尤其是母语为非洲语言的国家，仍存在显著差异。本文通过在8种低资源非洲语言中创建约100万词的新基准数据，来解决这些差异问题，这些语言覆盖了超过1.6亿使用者群体：阿姆哈拉语、班巴拉语、伊戈伯语、塞佩迪语（北索托语）、绍纳语、塞索托语（南索托语）、塞茨瓦纳语和津巴朗加语。我们使用的基准测试数据包括Winogrande和MMLU的三个部分：医学院校、临床知识和病毒学。利用翻译后的基准测试数据，我们报告了现有最先进（State-of-the-Art，SOTA）LLMs在英语和非洲语言之间的 previously unknown 性能差距。最后，利用超过400个微调模型的结果，我们探索了几种减少LLM性能差距的方法，包括高质量数据集微调（使用LLM作为注释器）、跨语言迁移和文化恰当性调整。主要发现包括：通过微调获得的平均单语改进为5.6%，使用高质量数据比低质量数据时平均单语改进为5.4%；跨语言迁移平均获得2.9%的收益；文化恰当性问题上的即插即用性能提升为3.0%。本研究公开的基准测试数据、翻译和代码可支持进一步的研究和发展，以创建更加包容和有效的语言技术。', 'title_zh': '填补空白：通过新基准、微调和文化调整提高低资源非洲语言大模型性能'}
{'arxiv_id': 'arXiv:2412.12416', 'title': 'DeepSN: A Sheaf Neural Framework for Influence Maximization', 'authors': 'Asela Hevapathige, Qing Wang, Ahad N. Zehmakan', 'link': 'https://arxiv.org/abs/2412.12416', 'abstract': 'Influence maximization is key topic in data mining, with broad applications in social network analysis and viral marketing. In recent years, researchers have increasingly turned to machine learning techniques to address this problem. They have developed methods to learn the underlying diffusion processes in a data-driven manner, which enhances the generalizability of the solution, and have designed optimization objectives to identify the optimal seed set. Nonetheless, two fundamental gaps remain unsolved: (1) Graph Neural Networks (GNNs) are increasingly used to learn diffusion models, but in their traditional form, they often fail to capture the complex dynamics of influence diffusion, (2) Designing optimization objectives is challenging due to combinatorial explosion when solving this problem. To address these challenges, we propose a novel framework, DeepSN. Our framework employs sheaf neural diffusion to learn diverse influence patterns in a data-driven, end-to-end manner, providing enhanced separability in capturing diffusion characteristics. We also propose an optimization technique that accounts for overlapping influence between vertices, which helps to reduce the search space and identify the optimal seed set effectively and efficiently. Finally, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our framework.', 'abstract_zh': '影响力最大化是数据挖掘中的关键课题，广泛应用于社交网络分析和病毒式营销领域。近年来，研究人员越来越倾向于利用机器学习技术来解决这一问题。他们开发了一种数据驱动的方法来学习潜在的扩散过程，这增强了解决方案的泛化能力，并设计了优化目标来识别最优种子集。然而，仍存在两个基本问题尚未解决：（1）图神经网络（GNNs）越来越多地被用于学习扩散模型，但在传统形式下，它们往往难以捕捉影响力的复杂动态；（2）由于优化目标的设计在求解该问题时会遇到组合爆炸，这使得优化目标的设计具有挑战性。为解决这些问题，我们提出了一种新的框架DeepSN。该框架采用簇神经扩散在端到端的数据驱动方式下学习多样化的影响力模式，提供了一种增强的捕获扩散特性的能力。我们还提出了一种优化技术，能够考虑顶点之间的交叉影响力，这有助于缩小搜索空间并有效地找到最优种子集。最后，我们在合成数据集和真实世界数据集上进行了广泛实验，以证明该框架的有效性。', 'title_zh': 'DeepSN：一种用于影响最大化的问题Sheaf神经网络框架'}
{'arxiv_id': 'arXiv:2412.12395', 'title': 'Sound Classification of Four Insect Classes', 'authors': 'Yinxuan Wang, Sudip Vhaduri', 'link': 'https://arxiv.org/abs/2412.12395', 'abstract': 'The goal of this project is to classify four different insect sounds: cicada, beetle, termite, and cricket. One application of this project is for pest control to monitor and protect our ecosystem. Our project leverages data augmentation, including pitch shifting and speed changing, to improve model generalization. This project will test the performance of Decision Tree, Random Forest, SVM RBF, XGBoost, and k-NN models, combined with MFCC feature. A potential novelty of this project is that various data augmentation techniques are used and created 6 data along with the original sound. The dataset consists of the sound recordings of these four insects. This project aims to achieve a high classification accuracy and to reduce the over-fitting problem.', 'abstract_zh': '本项目的目标是分类四种不同的昆虫声音：蝉、金龟子、白蚁和蟋蟀。本项目的一个应用是通过监控和保护生态系统来进行害虫控制。本项目利用数据增强技术，包括音调移调和速度变换，以提高模型的泛化能力。本项目将测试决策树、随机森林、SVM RBF、XGBoost和k-近邻模型与MFCC特征的组合性能。本项目的一个潜在创新点是使用了多种数据增强技术，并在此基础上生成了6个数据集，同时包含原始声音。数据集包括这四种昆虫的声音录音。本项目旨在实现高分类准确率，并减少过拟合问题。', 'title_zh': '四种类昆虫的声学分类'}
{'arxiv_id': 'arXiv:2412.12385', 'title': 'Enhancing Temporal Link Prediction with HierTKG: A Hierarchical Temporal Knowledge Graph Framework', 'authors': 'Mariam Almutairi, Melike Yildiz Aktas, Nawar Wali, Shutonu Mitra, Dawei Zhou', 'link': 'https://arxiv.org/abs/2412.12385', 'abstract': 'The rapid spread of misinformation on social media, especially during crises, challenges public decision-making. To address this, we propose HierTKG, a framework combining Temporal Graph Networks (TGN) and hierarchical pooling (DiffPool) to model rumor dynamics across temporal and structural scales. HierTKG captures key propagation phases, enabling improved temporal link prediction and actionable insights for misinformation control. Experiments demonstrate its effectiveness, achieving an MRR of 0.9845 on ICEWS14 and 0.9312 on WikiData, with competitive performance on noisy datasets like PHEME (MRR: 0.8802). By modeling structured event sequences and dynamic social interactions, HierTKG adapts to diverse propagation patterns, offering a scalable and robust solution for real-time analysis and prediction of rumor spread, aiding proactive intervention strategies.', 'abstract_zh': '社交媒体上虚假信息的快速传播，尤其是在危机期间，对公共决策构成了挑战。为应对这一问题，我们提出了HierTKG框架，该框架结合了时间图网络（TGN）和层次聚类（DiffPool），用于建模跨时间和结构尺度的谣言传播动态。HierTKG能够捕捉关键传播阶段，从而提高时间链接预测的准确性和对虚假信息控制的行动指引。实验结果表明其有效性，在ICEWS14数据集上MRR达到0.9845，在WikiData数据集上达到0.9312，且在如PHME这样的噪声数据集上（MRR：0.8802）也有竞争力的表现。通过建模结构化的事件序列和动态的社会互动，HierTKG能够适应多种传播模式，提供了一种可扩展且稳健的实时分析和预测谣言传播的解决方案，有助于主动干预策略的实施。', 'title_zh': '使用HierTKG增强时间链接预测：一种层次时间知识图谱框架'}
{'arxiv_id': 'arXiv:2412.12370', 'title': 'Scam Detection for Ethereum Smart Contracts: Leveraging Graph Representation Learning for Secure Blockchain', 'authors': 'Yihong Jin, Ze Yang', 'link': 'https://arxiv.org/abs/2412.12370', 'abstract': 'The detection of scams within Ethereum smart contracts is a critical challenge due to their increasing exploitation for fraudulent activities, leading to significant financial and reputational damages. Existing detection methods often rely on contract code analysis or manually extracted features, which suffer from scalability and adaptability limitations. In this study, we introduce an innovative method that leverages graph representation learning to examine transaction patterns and identify fraudulent contracts. By transforming Ethereum transaction data into graph structures and employing advanced machine learning models, we achieve robust classification performance. Our method addresses label imbalance through SMOTE-ENN techniques and evaluates models like Multi-Layer Perceptron (MLP) and Graph Convolutional Networks (GCN). Experimental results indicate that the MLP model surpasses the GCN in this context, with real-world evaluations aligning closely with domain-specific analyses. This study provides a scalable and effective solution for enhancing trust and security in the Ethereum ecosystem.', 'abstract_zh': '在以太坊智能合约中检测欺诈行为是一项关键挑战，因为越来越多的不法分子利用智能合约进行欺诈活动，导致重大经济和声誉损害。现有检测方法通常依赖于合约代码分析或手工提取的特征，这些方法在可扩展性和适应性方面存在局限性。在本研究中，我们提出了一种创新的方法，利用图表示学习来分析交易模式并识别欺诈性合约。通过将以太坊交易数据转换为图结构并采用先进的机器学习模型，我们实现了稳健的分类性能。我们通过SMOTE-ENN技术解决标签不平衡问题，并评估了多层感知机（MLP）和图卷积网络（GCN）等模型。实验结果表明，在这种情况下，MLP模型优于GCN模型，实际应用与领域特定分析较为吻合。本研究为增强以太坊生态系统的信任和安全性提供了可扩展且有效的方法。', 'title_zh': '基于图表示学习的以太坊智能合约欺诈检测：实现安全区块链的技术'}
{'arxiv_id': 'arXiv:2412.12364', 'title': 'LogBabylon: A Unified Framework for Cross-Log File Integration and Analysis', 'authors': 'Rabimba Karanjai, Yang Lu, Dana Alsagheer, Keshav Kasichainula, Lei Xu, Weidong Shi, Shou-Hsuan Stephen Huang', 'link': 'https://arxiv.org/abs/2412.12364', 'abstract': "Logs are critical resources that record events, activities, or messages produced by software applications, operating systems, servers, and network devices. However, consolidating the heterogeneous logs and cross-referencing them is challenging and complicated. Manually analyzing the log data is time-consuming and prone to errors. LogBabylon is a centralized log data consolidating solution that leverages Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the log data in a human-readable way and adds insight analysis of the system performance and anomaly alerts. It provides a paramount view of the system landscape, enabling proactive management and rapid incident response. LogBabylon consolidates diverse log sources and enhances the extracted information's accuracy and relevancy. This facilitates a deeper understanding of log data, supporting more effective decision-making and operational efficiency. Furthermore, LogBabylon streamlines the log analysis process, significantly reducing the time and effort required to interpret complex datasets. Its capabilities extend to generating context-aware insights, offering an invaluable tool for continuous monitoring, performance optimization, and security assurance in dynamic computing environments.", 'abstract_zh': '日志是记录由软件应用、操作系统、服务器和网络设备产生的事件、活动或消息的关键资源。然而，整合异构日志并进行跨参考是具有挑战性和复杂性的任务。手动分析日志数据既耗时又容易出错。LogBabylon 是一种利用大型语言模型（LLMs）结合检索增强生成（RAG）技术的集中化日志数据整合解决方案。LogBabylon 以人类可读的方式解释日志数据，并添加有关系统性能和异常警报的洞察分析。它提供了系统架构的整体视图，有助于前瞻性管理并实现快速故障响应。LogBabylon 整合了多种日志源，并增强了提取信息的准确性和相关性，这有助于更深入地理解日志数据，支持更为有效的决策和运营效率。此外，LogBabylon 简化了日志分析过程，显著减少了解读复杂数据集所需的时间和努力。其功能扩展到生成上下文感知的洞察，提供了一个宝贵的工具，用于动态计算环境中的持续监控、性能优化和安全保证。', 'title_zh': 'LogBabylon：一个统一的日志文件集成与分析框架'}
{'arxiv_id': 'arXiv:2412.12358', 'title': 'BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A', 'authors': 'Samy Ateia, Udo Kruschwitz', 'link': 'https://arxiv.org/abs/2412.12358', 'abstract': 'We present BioRAGent, an interactive web-based retrieval-augmented generation (RAG) system for biomedical question answering. The system uses large language models (LLMs) for query expansion, snippet extraction, and answer generation while maintaining transparency through citation links to the source documents and displaying generated queries for further editing. Building on our successful participation in the BioASQ 2024 challenge, we demonstrate how few-shot learning with LLMs can be effectively applied for a professional search setting. The system supports both direct short paragraph style responses and responses with inline citations. Our demo is available online, and the source code is publicly accessible through GitHub.', 'abstract_zh': '我们提出了BioRAGent，这是一个基于网页的交互式检索增强生成（RAG）系统，用于生物医学问题回答。该系统利用大规模语言模型（LLM）进行查询扩展、片段提取和答案生成，并通过引用链接透明地显示源文档，同时展示生成的查询以便进一步编辑。基于我们在BioASQ 2024挑战赛中的成功参与，我们展示了如何有效地将少量示例学习应用于专业搜索场景中。该系统支持直接简洁段落样式回复和包含内联引文的回复。我们的演示已在线提供，并且源代码可通过GitHub公开访问。', 'title_zh': 'BioRAGent：一种用于展示生成性查询扩展和领域特定搜索的检索增强生成系统'}
{'arxiv_id': 'arXiv:2412.12326', 'title': 'Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing', 'authors': 'Yue Jin, Shuangqing Wei, Giovanni Montana', 'link': 'https://arxiv.org/abs/2412.12326', 'abstract': "In human society, the conflict between self-interest and collective well-being often obstructs efforts to achieve shared welfare. Related concepts like the Tragedy of the Commons and Social Dilemmas frequently manifest in our daily lives. As artificial agents increasingly serve as autonomous proxies for humans, we propose using multi-agent reinforcement learning (MARL) to address this issue - learning policies to maximise collective returns even when individual agents' interests conflict with the collective one. Traditional MARL solutions involve sharing rewards, values, and policies or designing intrinsic rewards to encourage agents to learn collectively optimal policies. We introduce a novel MARL approach based on Suggestion Sharing (SS), where agents exchange only action suggestions. This method enables effective cooperation without the need to design intrinsic rewards, achieving strong performance while revealing less private information compared to sharing rewards, values, or policies. Our theoretical analysis establishes a bound on the discrepancy between collective and individual objectives, demonstrating how sharing suggestions can align agents' behaviours with the collective objective. Experimental results demonstrate that SS performs competitively with baselines that rely on value or policy sharing or intrinsic rewards.", 'abstract_zh': '在人类社会中，个体利益与集体福祉之间的冲突常阻碍共享福利目标的实现。相关概念如“公地悲剧”和社会困境在日常生活中频繁出现。随着人工代理越来越多地作为自主代表为人类服务，我们提出使用多智能体强化学习（MARL）来解决这一问题——即使个体代理的个人利益与集体利益相冲突时，学习能够最大化集体收益的策略。传统MARL解决方案涉及共享奖励、价值或策略，或者设计内在奖励以促使智能体学习集体最优策略。我们提出了一种基于建议共享（SS）的新型MARL方法，其中智能体仅交换行动建议。这种方法可以在无需设计内在奖励的情况下实现有效的合作，相较于共享奖励、价值或策略，这种做法能较少泄露私人信息，同时表现出优异的性能。我们的理论分析建立了集体目标与个体目标之间差异的界限，展示了如何通过共享建议来使智能体的行为与集体目标更加一致。实验结果表明，SS在基线方法（依赖于价值或策略共享或内在奖励）中表现出了竞争力。', 'title_zh': '通过建议共享实现多智能体强化学习中的集体福利'}
{'arxiv_id': 'arXiv:2412.12322', 'title': 'RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems', 'authors': 'Ioannis Papadimitriou, Ilias Gialampoukidis, Stefanos Vrochidis, Ioannis, Kompatsiaris', 'link': 'https://arxiv.org/abs/2412.12322', 'abstract': 'We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.', 'abstract_zh': '我们介绍了RAG Playground，这是一个开源框架，用于系统评估检索增强生成（RAG）系统。该框架实现了并比较了三种检索方法：朴素向量搜索、再排序以及向量-关键词混合搜索，并结合了使用不同提示策略的ReAct代理。我们提出了一个全面的评价框架，并引入了新的评估指标，提供了不同语言模型（Llama 3.1和Qwen 2.5）在各种检索配置下的实验结果比较。我们的实验结果表明，通过混合搜索方法和结构化的自我评估提示，性能显著提高，在我们的多指标评价框架中，混合搜索方法的通过率可达72.7%。此外，实验结果还强调了在RAG系统中进行提示工程的重要性，我们的自定义提示代理在检索准确性和响应质量方面表现出一致的改进。', 'title_zh': 'RAG游乐场：一种评估RAG系统中检索策略和提示工程系统的体系化框架'}
{'arxiv_id': 'arXiv:2412.12276', 'title': 'Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers', 'authors': 'Seungwook Han, Jinyeop Song, Jeff Gore, Pulkit Agrawal', 'link': 'https://arxiv.org/abs/2412.12276', 'abstract': 'Humans distill complex experiences into fundamental abstractions that enable rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. In this paper, we propose concept encoding-decoding mechanism to explain ICL by studying how transformers form and use internal abstractions in their representations. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of concept encoding and decoding. As the model learns to encode different latent concepts (e.g., ``Finding the first noun in a sentence.") into distinct, separable representations, it concureently builds conditional decoding algorithms and improve its ICL performance. We validate the existence of this mechanism across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B). Further, through mechanistic interventions and controlled finetuning, we demonstrate that the quality of concept encoding is causally related and predictive of ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.', 'abstract_zh': '人类将复杂的经验提炼为基本的抽象概念，从而实现快速学习和适应。同样地，自回归变压器通过上下文学习（ICL）表现出适应性学习的能力，这引发了关于其具体机制的思考。本文提出了一种概念编码-解码机制，通过研究变压器如何在其表征中形成和使用内部抽象来解释ICL。在合成的ICL任务中，我们分析了一个小型变压器的训练动力学，并报告了概念编码和解码的耦合涌现。随着模型学习将不同的潜在概念（例如，“在一个句子中找到第一个名词”）编码为独特的、可分离的表征，它同时构建了条件解码算法并且提高了其ICL性能。我们通过对各种规模的预训练模型（Gemma-2 2B/9B/27B、Llama-3.1 8B/70B）验证了这一机制的存在。进一步地，通过机制干预和受控微调，我们证明了概念编码的质量与其ICL性能之间存在因果关系和预测性。我们的实证见解有助于通过模型的表征更好地理解大语言模型的成功和失败模式。', 'title_zh': '抽象的 emergence: 变量上下文学习中概念编码与解码机制的研究'}
{'arxiv_id': 'arXiv:2412.12242', 'title': 'OmniPrism: Learning Disentangled Visual Concept for Image Generation', 'authors': 'Yangyang Li, Daqing Liu, Wu Liu, Allen He, Xinchen Liu, Yongdong Zhang, Guoqing Jin', 'link': 'https://arxiv.org/abs/2412.12242', 'abstract': "Creative visual concept generation often draws inspiration from specific concepts in a reference image to produce relevant outcomes. However, existing methods are typically constrained to single-aspect concept generation or are easily disrupted by irrelevant concepts in multi-aspect concept scenarios, leading to concept confusion and hindering creative generation. To address this, we propose OmniPrism, a visual concept disentangling approach for creative image generation. Our method learns disentangled concept representations guided by natural language and trains a diffusion model to incorporate these concepts. We utilize the rich semantic space of a multimodal extractor to achieve concept disentanglement from given images and concept guidance. To disentangle concepts with different semantics, we construct a paired concept disentangled dataset (PCD-200K), where each pair shares the same concept such as content, style, and composition. We learn disentangled concept representations through our contrastive orthogonal disentangled (COD) training pipeline, which are then injected into additional diffusion cross-attention layers for generation. A set of block embeddings is designed to adapt each block's concept domain in the diffusion models. Extensive experiments demonstrate that our method can generate high-quality, concept-disentangled results with high fidelity to text prompts and desired concepts.", 'abstract_zh': '创意视觉概念生成通常从参考图像中的特定概念获取灵感，以产生相关成果。然而，现有方法通常仅限于单一方面的概念生成，或者在涉及多种概念的场景中容易受到无关概念的干扰，导致概念混淆，从而妨碍创造性生成。为了解决这个问题，我们提出了OmniPrism，一种用于创意图像生成的视觉概念分离方法。我们的方法在自然语言的引导下学习分离的概念表示，并通过扩散模型将这些概念融合到生成过程中。我们利用多模态提取器中的丰富语义空间，从给定的图像和概念指导中实现概念分离。为了分离具有不同语义的概念，我们构建了一个配对的概念分离数据集（PCD-200K），其中每一对共享相同的概念，如内容、风格和构图。我们通过对比正交分离（COD）训练管道学习分离的概念表示，然后将它们注入到扩散模型的额外交叉注意力层中进行生成。我们设计了一套块嵌入，使扩散模型中每个块的概念领域适应特定的条件。大量实验表明，我们的方法可以生成高质量且与文本提示和所需概念高度一致的概念分离结果。', 'title_zh': 'OmniPrism：学习解耦视觉概念以进行图像生成'}
{'arxiv_id': 'arXiv:2412.12237', 'title': 'Equivariant Action Sampling for Reinforcement Learning and Planning', 'authors': 'Linfeng Zhao, Owen Howell, Xupeng Zhu, Jung Yeon Park, Zhewen Zhang, Robin Walters, Lawson L.S. Wong', 'link': 'https://arxiv.org/abs/2412.12237', 'abstract': 'Reinforcement learning (RL) algorithms for continuous control tasks require accurate sampling-based action selection. Many tasks, such as robotic manipulation, contain inherent problem symmetries. However, correctly incorporating symmetry into sampling-based approaches remains a challenge. This work addresses the challenge of preserving symmetry in sampling-based planning and control, a key component for enhancing decision-making efficiency in RL. We introduce an action sampling approach that enforces the desired symmetry. We apply our proposed method to a coordinate regression problem and show that the symmetry aware sampling method drastically outperforms the naive sampling approach. We furthermore develop a general framework for sampling-based model-based planning with Model Predictive Path Integral (MPPI). We compare our MPPI approach with standard sampling methods on several continuous control tasks. Empirical demonstrations across multiple continuous control environments validate the effectiveness of our approach, showcasing the importance of symmetry preservation in sampling-based action selection.', 'abstract_zh': '强化学习（RL）算法在连续控制任务中需要准确的基于采样的行动选择。许多任务，如机器人操作，包含固有的问题对称性。然而，正确地将对称性融入基于采样的方法仍然是一个挑战。本研究旨在解决在采样规划和控制中保持对称性的挑战，这是提高强化学习决策效率的关键组成部分。我们提出了一个强制执行所需对称性的行动采样方法。我们将所提出的方法应用于坐标回归问题，并展示了对称性意识采样方法远优于简单的采样方法。此外，我们进一步开发了一个基于模型的计划的一般框架，该框架结合了模型预测路径积分（MPPI）。我们将我们的MPPI方法与标准采样方法在多个连续控制任务上进行了比较。在多个连续控制环境中的实证演示验证了我们方法的有效性，突显了在基于采样的行动选择中保持对称性的的重要性。', 'title_zh': '等变动作采样在强化学习与规划中的应用'}
{'arxiv_id': 'arXiv:2412.12232', 'title': 'You Only Submit One Image to Find the Most Suitable Generative Model', 'authors': 'Zhi Zhou, Lan-Zhe Guo, Peng-Xiao Song, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2412.12232', 'abstract': "Deep generative models have achieved promising results in image generation, and various generative model hubs, e.g., Hugging Face and Civitai, have been developed that enable model developers to upload models and users to download models. However, these model hubs lack advanced model management and identification mechanisms, resulting in users only searching for models through text matching, download sorting, etc., making it difficult to efficiently find the model that best meets user requirements. In this paper, we propose a novel setting called Generative Model Identification (GMI), which aims to enable the user to identify the most appropriate generative model(s) for the user's requirements from a large number of candidate models efficiently. To our best knowledge, it has not been studied yet. In this paper, we introduce a comprehensive solution consisting of three pivotal modules: a weighted Reduced Kernel Mean Embedding (RKME) framework for capturing the generated image distribution and the relationship between images and prompts, a pre-trained vision-language model aimed at addressing dimensionality challenges, and an image interrogator designed to tackle cross-modality issues. Extensive empirical results demonstrate the proposal is both efficient and effective. For example, users only need to submit a single example image to describe their requirements, and the model platform can achieve an average top-4 identification accuracy of more than 80%.", 'abstract_zh': '深度生成模型已在图像生成领域取得了令人瞩目的成果，各种生成模型平台，如 Hugging Face 和 Civitai，也已开发出来，使模型开发者能够上传模型，用户能够下载模型。然而，这些模型平台缺乏先进的模型管理与识别机制，导致用户只能依赖文本匹配和下载排序等方式进行模型搜索，难以高效找到最符合用户需求的模型。本文提出了一种新的框架设置，称为生成模型识别（GMI），旨在使用户能够高效地从大量候选模型中识别出最合适的生成模型。据我们所知，这一问题尚未得到研究。本文提出了一种综合解决方案，包括三个核心模块：加权减少核均值嵌入（Reduced Kernel Mean Embedding, RKME）框架，用于捕捉生成图像的分布及其与提示之间的关系；预训练的视觉-语言模型，用于应对高维挑战；以及图像问题解决模块，用于解决跨模态问题。详尽的实证结果表明，该方法既高效又有效。例如，用户只需提交一张示例图像来描述其需求，模型平台可以实现超过 80% 的顶级四次识别准确率。', 'title_zh': '您只需提交一张图像即可找到最适合的生成模型'}
{'arxiv_id': 'arXiv:2412.12228', 'title': 'Linear Equations with Min and Max Operators: Computational Complexity', 'authors': 'Krishnendu Chatterjee, Ruichen Luo, Raimundo Saona, Jakub Svoboda', 'link': 'https://arxiv.org/abs/2412.12228', 'abstract': 'We consider a class of optimization problems defined by a system of linear equations with min and max operators. This class of optimization problems has been studied under restrictive conditions, such as, (C1) the halting or stability condition; (C2) the non-negative coefficients condition; (C3) the sum up to 1 condition; and (C4) the only min or only max oerator condition. Several seminal results in the literature focus on special cases. For example, turn-based stochastic games correspond to conditions C2 and C3; and Markov decision process to conditions C2, C3, and C4. However, the systematic computational complexity study of all the cases has not been explored, which we address in this work. Some highlights of our results are: with conditions C2 and C4, and with conditions C3 and C4, the problem is NP-complete, whereas with condition C1 only, the problem is in UP intersects coUP. Finally, we establish the computational complexity of the decision problem of checking the respective conditions.', 'abstract_zh': '我们考虑一类由带有最小值和最大值运算的线性方程系统定义的最优化问题。这类最优化问题在某些限制条件下已被研究过，具体包括以下条件：\n(C1) 停止或稳定性条件；\n(C2) 非负系数条件；\n(C3) 系数之和为1的条件；\n(C4) 仅包含最小值运算或最大值运算的条件。\n文献中的一些开创性成果集中在特殊情况下。例如，轮换随机博弈对应于条件(C2)和(C3)，马尔可夫决策过程则对应于条件(C2)、(C3)和(C4)。然而，对所有情况的系统性计算复杂性研究尚未展开，这是我们工作的重点。我们的主要结果亮点包括：在满足条件(C2)和(C4)时，以及在满足条件(C3)和(C4)时，该问题为NP完全问题；仅满足条件(C1)时，该问题位于UP与coUP的交集中。最后，我们确立了验证各自条件的决策问题的计算复杂性。', 'title_zh': '带有最小值和最大值运算的线性方程组：计算复杂性'}
{'arxiv_id': 'arXiv:2412.12227', 'title': 'EDformer: Embedded Decomposition Transformer for Interpretable Multivariate Time Series Predictions', 'authors': 'Sanjay Chakraborty, Ibrahim Delibasoglu, Fredrik Heintz', 'link': 'https://arxiv.org/abs/2412.12227', 'abstract': "Time series forecasting is a crucial challenge with significant applications in areas such as weather prediction, stock market analysis, and scientific simulations. This paper introduces an embedded decomposed transformer, 'EDformer', for multivariate time series forecasting tasks. Without altering the fundamental elements, we reuse the Transformer architecture and consider the capable functions of its constituent parts in this work. Edformer first decomposes the input multivariate signal into seasonal and trend components. Next, the prominent multivariate seasonal component is reconstructed across the reverse dimensions, followed by applying the attention mechanism and feed-forward network in the encoder stage. In particular, the feed-forward network is used for each variable frame to learn nonlinear representations, while the attention mechanism uses the time points of individual seasonal series embedded within variate frames to capture multivariate correlations. Therefore, the trend signal is added with projection and performs the final forecasting. The EDformer model obtains state-of-the-art predicting results in terms of accuracy and efficiency on complex real-world time series datasets. This paper also addresses model explainability techniques to provide insights into how the model makes its predictions and why specific features or time steps are important, enhancing the interpretability and trustworthiness of the forecasting results.", 'abstract_zh': '时间序列预测是一个重要的挑战，在气象预测、股票市场分析和科学模拟等多个领域具有显著的应用价值。本文介绍了一种嵌入式分解变压器模型‘EDformer’，用于多变量时间序列预测任务。在不改变基础元素的情况下，我们在此工作中重新利用了Transformer架构，并考虑了其组成部分的功能。EDformer首先将输入的多变量信号分解为季节性和趋势分量。接着，通过逆向维度重构显著的多变量季节性分量，并在编码阶段应用注意力机制和前向网络。具体而言，前向网络用于每个变量帧以学习非线性表示，而注意力机制则利用嵌入在变量帧中的单个季节性序列的时间点来捕捉多变量之间的相关性。因此，趋势信号经过投影操作后进行最终的预测。EDformer模型在复杂的真实世界时间序列数据集上获得了最先进的预测结果，同时在准确性和效率方面表现优异。本文还探讨了模型可解释性技术，以提供有关模型如何做出预测以及为何某些特征或时间步骤重要的见解，从而增强预测结果的可解释性和可信度。', 'title_zh': 'EDformer: 嵌入式分解变换器用于可解释的多变量时间序列预测'}
{'arxiv_id': 'arXiv:2412.12226', 'title': 'Apollo-Forecast: Overcoming Aliasing and Inference Speed Challenges in Language Models for Time Series Forecasting', 'authors': 'Tianyi Yin, Jingwei Wang, Yunlong Ma, Han Wang, Chenze Wang, Yukai Zhao, Min Liu, Weiming Shen, Yufeng Chen', 'link': 'https://arxiv.org/abs/2412.12226', 'abstract': "Encoding time series into tokens and using language models for processing has been shown to substantially augment the models' ability to generalize to unseen tasks. However, existing language models for time series forecasting encounter several obstacles, including aliasing distortion and prolonged inference times, primarily due to the limitations of quantization processes and the computational demands of large models. This paper introduces Apollo-Forecast, a novel framework that tackles these challenges with two key innovations: the Anti-Aliasing Quantization Module (AAQM) and the Race Decoding (RD) technique. AAQM adeptly encodes sequences into tokens while mitigating high-frequency noise in the original signals, thus enhancing both signal fidelity and overall quantization efficiency. RD employs a draft model to enable parallel processing and results integration, which markedly accelerates the inference speed for long-term predictions, particularly in large-scale models. Extensive experiments on various real-world datasets show that Apollo-Forecast outperforms state-of-the-art methods by 35.41\\% and 18.99\\% in WQL and MASE metrics, respectively, in zero-shot scenarios. Furthermore, our method achieves a 1.9X-2.7X acceleration in inference speed over baseline methods.", 'abstract_zh': '将时间序列编码为标记并使用语言模型进行处理已被证明能够显著增强模型在处理未见过任务时的泛化能力。然而，现有的时间序列预测语言模型遇到了一些障碍，包括混叠失真和较长的推理时间，主要是由于量化过程的限制和大型模型的计算需求。本文介绍了一种名为Apollo-Forecast的新框架，该框架通过两项关键技术解决了这些挑战：反混叠量化模块（AAQM）和赛程解码（RD）技术。AAQM巧妙地将序列编码为标记，同时减少原始信号中的高频噪声，从而提高了信号保真度并增强了整体量化效率。RD利用一个草稿模型实现并行处理和结果集成，显著加快了长期预测的推理速度，特别是在大型模型中。在各种真实世界的数据集上的广泛实验表明，Apollo-Forecast在零样本场景下的WQL和MASE指标中分别优于最先进的方法35.41%和18.99%。此外，我们的方法比基线方法的推理速度提高了1.9倍至2.7倍。', 'title_zh': 'Apollo-Forecast：克服语言模型在时间序列预测中混叠和推理速度挑战'}
{'arxiv_id': 'arXiv:2412.12225', 'title': 'DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis', 'authors': 'Pan Wang, Qiang Zhou, Yawen Wu, Tianlong Chen, Jingtong Hu', 'link': 'https://arxiv.org/abs/2412.12225', 'abstract': 'Multimodal Sentiment Analysis (MSA) leverages heterogeneous modalities, such as language, vision, and audio, to enhance the understanding of human sentiment. While existing models often focus on extracting shared information across modalities or directly fusing heterogeneous modalities, such approaches can introduce redundancy and conflicts due to equal treatment of all modalities and the mutual transfer of information between modality pairs. To address these issues, we propose a Disentangled-Language-Focused (DLF) multimodal representation learning framework, which incorporates a feature disentanglement module to separate modality-shared and modality-specific information. To further reduce redundancy and enhance language-targeted features, four geometric measures are introduced to refine the disentanglement process. A Language-Focused Attractor (LFA) is further developed to strengthen language representation by leveraging complementary modality-specific information through a language-guided cross-attention mechanism. The framework also employs hierarchical predictions to improve overall accuracy. Extensive experiments on two popular MSA datasets, CMU-MOSI and CMU-MOSEI, demonstrate the significant performance gains achieved by the proposed DLF framework. Comprehensive ablation studies further validate the effectiveness of the feature disentanglement module, language-focused attractor, and hierarchical predictions. Our code is available at this https URL.', 'abstract_zh': '多模态情感分析（Multimodal Sentiment Analysis, MSA）利用了异构模态，如语言、视觉和音频，以增强对人类情感理解的能力。现有的模型往往集中在提取跨模态的共享信息或直接融合异构模态，但这些方法由于平等地对待所有模态和模态对之间的相互信息转移，可能会引入冗余和冲突。为此，我们提出了一种解纠缠语言焦点（Disentangled-Language-Focused, DLF）的多模态表示学习框架，该框架整合了特征解纠缠模块，以分离模态共享和模态特定的信息。为了进一步减少冗余并增强语言目标特征，我们引入了四种几何度量来细化解纠缠过程。此外，通过语言引导的跨注意力机制，进一步发展了语言焦点吸引子（Language-Focused Attractor, LFA），以利用互补的模态特定信息来增强语言表示。该框架还使用分层预测来提高整体准确度。在两个流行的MSA数据集CMU-MOSI和CMU-MOSEI上的广泛实验表明，所提出的DLF框架能够实现显著的性能提升。全面的消融研究进一步验证了特征解纠缠模块、语言焦点吸引子和分层预测的有效性。我们的代码可以在以下链接获取：[该链接].', 'title_zh': 'DLF：解耦语言焦点的多模态情感分析'}
{'arxiv_id': 'arXiv:2412.12223', 'title': 'Can video generation replace cinematographers? Research on the cinematic language of generated video', 'authors': 'Xiaozhe Li, Kai WU, Siyi Yang, YiZhan Qu, Guohua.Zhang, Zhiyu Chen, Jiayao Li, Jiangchuan Mu, Xiaobin Hu, Wen Fang, Mingliang Xiong, Hao Deng, Qingwen Liu, Gang Li, Bin He', 'link': 'https://arxiv.org/abs/2412.12223', 'abstract': 'Recent advancements in text-to-video (T2V) generation have leveraged diffusion models to enhance the visual coherence of videos generated from textual descriptions. However, most research has primarily focused on object motion, with limited attention given to cinematic language in videos, which is crucial for cinematographers to convey emotion and narrative pacing. To address this limitation, we propose a threefold approach to enhance the ability of T2V models to generate controllable cinematic language. Specifically, we introduce a cinematic language dataset that encompasses shot framing, angle, and camera movement, enabling models to learn diverse cinematic styles. Building on this, to facilitate robust cinematic alignment evaluation, we present CameraCLIP, a model fine-tuned on the proposed dataset that excels in understanding complex cinematic language in generated videos and can further provide valuable guidance in the multi-shot composition process. Finally, we propose CLIPLoRA, a cost-guided dynamic LoRA composition method that facilitates smooth transitions and realistic blending of cinematic language by dynamically fusing multiple pre-trained cinematic LoRAs within a single video. Our experiments demonstrate that CameraCLIP outperforms existing models in assessing the alignment between cinematic language and video, achieving an R@1 score of 0.81. Additionally, CLIPLoRA improves the ability for multi-shot composition, potentially bridging the gap between automatically generated videos and those shot by professional cinematographers.', 'abstract_zh': '近年来，文本生成视频（T2V）生成的研究已经利用扩散模型来提高从文本描述生成的视频的视觉连贯性。然而，大多数研究主要集中在对象运动上，而对视频中的电影语言关注不足，这对手持摄像师传达情感和叙事节奏至关重要。为解决这一局限性，我们提出了一种三管齐下的方法以增强T2V模型生成可控电影语言的能力。具体而言，我们引入了一个包含机位构图、角度和摄像机运动的电影语言数据集，使模型能够学习各种电影风格。在此基础上，为了促进稳健的电影语言对齐评估，我们提出了CameraCLIP模型，该模型基于提出的数据集进行了微调，能够有效地理解生成视频中的复杂电影语言，并进一步提供有价值的多镜头组成指导。最后，我们提出了CLIPLoRA，一种成本引导的动态LoRA合成方法，通过在单个视频中动态融合多个预训练的电影LoRA，有助于平滑过渡和逼真的电影语言融合。我们的实验表明，CameraCLIP在评估电影语言与视频对齐方面优于现有模型，实现了0.81的R@1评分。此外，CLIPLoRA提高了多镜头组成的性能，可能缩小了自动生成视频与专业摄像师拍摄的视频之间的差距。', 'title_zh': '视频生成能否取代 cinematographers？关于生成视频的电影语言研究'}
{'arxiv_id': 'arXiv:2412.12221', 'title': 'Parallel Greedy Best-First Search with a Bound on the Number of Expansions Relative to Sequential Search', 'authors': 'Takumi Shimoda, Alex Fukunaga', 'link': 'https://arxiv.org/abs/2412.12221', 'abstract': 'Parallelization of non-admissible search algorithms such as GBFS poses a challenge because straightforward parallelization can result in search behavior which significantly deviates from sequential search. Previous work proposed PUHF, a parallel search algorithm which is constrained to only expand states that can be expanded by some tie-breaking strategy for GBFS. We show that despite this constraint, the number of states expanded by PUHF is not bounded by a constant multiple of the number of states expanded by sequential GBFS with the worst-case tie-breaking strategy. We propose and experimentally evaluate One Bench At a Time (OBAT), a parallel greedy search which guarantees that the number of states expanded is within a constant factor of the number of states expanded by sequential GBFS with some tie-breaking policy.', 'abstract_zh': '将非可接受搜索算法（如GBFS）进行并行化面临着挑战，因为直接的并行化可能使搜索行为显著偏离顺序搜索的行为。此前的工作提出了一种受约束的并行搜索算法PUHF，它仅扩展通过某种代价函数策略可以扩展的状态。我们证明，尽管存在这一约束，PUHF扩展的状态数量并不限于与使用最坏情况代价函数策略的顺序GBFS扩展的状态数量成常数倍的关系。我们提出了并行贪婪搜索方法One Bench At a Time (OBAT)，并且通过实验评估了其效果。该方法保证扩展的状态数量与使用某种代价函数策略的顺序GBFS扩展的状态数量之间的比例为常数因子。', 'title_zh': '受限于相对串行搜索扩展次数的并行贪婪最佳优先搜索'}
{'arxiv_id': 'arXiv:2412.12220', 'title': 'Relieving Universal Label Noise for Unsupervised Visible-Infrared Person Re-Identification by Inferring from Neighbors', 'authors': 'Xiao Teng, Long Lan, Dingyao Chen, Kele Xu, Nan Yin', 'link': 'https://arxiv.org/abs/2412.12220', 'abstract': 'Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of great research and practical significance yet remains challenging due to the absence of annotations. Existing approaches aim to learn modality-invariant representations in an unsupervised setting. However, these methods often encounter label noise within and across modalities due to suboptimal clustering results and considerable modality discrepancies, which impedes effective training. To address these challenges, we propose a straightforward yet effective solution for USL-VI-ReID by mitigating universal label noise using neighbor information. Specifically, we introduce the Neighbor-guided Universal Label Calibration (N-ULC) module, which replaces explicit hard pseudo labels in both homogeneous and heterogeneous spaces with soft labels derived from neighboring samples to reduce label noise. Additionally, we present the Neighbor-guided Dynamic Weighting (N-DW) module to enhance training stability by minimizing the influence of unreliable samples. Extensive experiments on the RegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing USL-VI-ReID approaches, despite its simplicity. The source code is available at: this https URL.', 'abstract_zh': '无监督可见-红外行人再识别（USL-VI-ReID）在研究和应用中具有重要价值，但由于缺乏标注数据，仍然面临诸多挑战。现有方法旨在在无监督环境下学习模态不变表示，但这些方法往往因为次优聚类结果和显著的模态差异而在训练过程中遇到标签噪声问题，这阻碍了有效训练。为解决这些问题，我们提出了一种简单而有效的USL-VI-ReID解决方案，通过利用邻居信息减轻普遍的标签噪声。具体而言，我们引入了基于邻居信息的通用标签校准（Neighbor-guided Universal Label Calibration, N-ULC）模块，该模块用来自邻居样本的软标签替换同质和异质空间中的显式硬伪标签，以减少标签噪声。此外，我们还提出了基于邻居信息的动态加权（Neighbor-guided Dynamic Weighting, N-DW）模块，以通过最小化不可靠样本的影响来增强训练稳定性。在RegDB和SYSU-MM01数据集上的广泛实验表明，尽管方法简单，我们的方法仍优于现有USL-VI-ReID方法。源代码可在以下链接获取：this [链接] URL。', 'title_zh': '通过从邻居中推断来缓解通用标签噪声以实现无监督的可见光-红外人体重识别'}
{'arxiv_id': 'arXiv:2412.12219', 'title': 'Are Large Language Models Useful for Time Series Data Analysis?', 'authors': 'Francis Tang, Ying Ding', 'link': 'https://arxiv.org/abs/2412.12219', 'abstract': 'Time series data plays a critical role across diverse domains such as healthcare, energy, and finance, where tasks like classification, anomaly detection, and forecasting are essential for informed decision-making. Recently, large language models (LLMs) have gained prominence for their ability to handle complex data and extract meaningful insights. This study investigates whether LLMs are effective for time series data analysis by comparing their performance with non-LLM-based approaches across three tasks: classification, anomaly detection, and forecasting.\nThrough a series of experiments using GPT4TS and autoregressive models, we evaluate their performance on benchmark datasets and assess their accuracy, precision, and ability to generalize. Our findings indicate that while LLM-based methods excel in specific tasks like anomaly detection, their benefits are less pronounced in others, such as forecasting, where simpler models sometimes perform comparably or better. This research highlights the role of LLMs in time series analysis and lays the groundwork for future studies to systematically explore their applications and limitations in handling temporal data.', 'abstract_zh': '时间序列数据在医疗、能源和金融等多个领域发挥着关键作用，其中分类、异常检测和预测等任务对于支持明智的决策至关重要。近年来，大型语言模型（LLMs）因其处理复杂数据和提取有意义洞察的能力而备受瞩目。本研究旨在探讨LLMs在时间序列数据分析中的有效性，通过将基于LLMs的方法与其非LLMs基线方法在分类、异常检测和预测三个任务上的表现进行比较来验证这一点。\n\n通过使用GPT4TS和自回归模型进行一系列实验，我们评估了这些方法在基准数据集上的性能，并对其准确度、精确度以及泛化能力进行了评估。研究结果表明，虽然基于LLMs的方法在特定任务如异常检测方面表现出色，但在其他任务如预测方面，其优势并不总是明显，有时简单的模型在某些情况下甚至表现得相当好或更好。本研究强调了LLMs在时间序列分析中的作用，并为未来系统探索其在处理时序数据方面应用和局限性奠定了基础。', 'title_zh': '大型语言模型在时间序列数据分析中是否有用？'}
{'arxiv_id': 'arXiv:2412.12215', 'title': 'Imagined Speech State Classification for Robust Brain-Computer Interface', 'authors': 'Byung-Kwan Ko, Jun-Young Kim, Seo-Hyun Lee', 'link': 'https://arxiv.org/abs/2412.12215', 'abstract': 'This study examines the effectiveness of traditional machine learning classifiers versus deep learning models for detecting the imagined speech using electroencephalogram data. Specifically, we evaluated conventional machine learning techniques such as CSP-SVM and LDA-SVM classifiers alongside deep learning architectures such as EEGNet, ShallowConvNet, and DeepConvNet. Machine learning classifiers exhibited significantly lower precision and recall, indicating limited feature extraction capabilities and poor generalization between imagined speech and idle states. In contrast, deep learning models, particularly EEGNet, achieved the highest accuracy of 0.7080 and an F1 score of 0.6718, demonstrating their enhanced ability in automatic feature extraction and representation learning, essential for capturing complex neurophysiological patterns. These findings highlight the limitations of conventional machine learning approaches in brain-computer interface (BCI) applications and advocate for adopting deep learning methodologies to achieve more precise and reliable classification of detecting imagined speech. This foundational research contributes to the development of imagined speech-based BCI systems.', 'abstract_zh': '本研究旨在探讨传统机器学习分类器与深度学习模型在利用脑电图（EEG）数据检测想象言语方面的有效性。具体而言，我们评估了传统的机器学习技术，如 CSP-SVM 和 LDA-SVM 分类器，以及深度学习架构，如 EEGNet、ShallowConvNet 和 DeepConvNet。结果显示，传统的机器学习分类器在精确度和召回率方面显著低于深度学习模型，表明它们在特征提取能力和想象言语与静止状态之间的泛化能力上有局限性。相比之下，深度学习模型，尤其是 EEGNet，实现了最高的准确率（0.7080）和 F1 得分（0.6718），这表明它们在自动特征提取和表示学习方面的能力更强，这对于捕捉复杂的神经生理模式至关重要。这些发现凸显了在脑机接口（BCI）应用中，传统机器学习方法的局限性，并提倡采用深度学习方法以实现更精确和可靠的想象言语检测分类。本项基础研究为基于想象言语的BCI系统的开发做出了贡献。', 'title_zh': '稳健脑机接口中的想象语音状态分类'}
{'arxiv_id': 'arXiv:2412.12212', 'title': "Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization", 'authors': 'Portia Cooper, Harshita Narnoli, Mihai Surdeanu', 'link': 'https://arxiv.org/abs/2412.12212', 'abstract': 'Text-to-image models are vulnerable to the stepwise "Divide-and-Conquer Attack" (DACA) that utilize a large language model to obfuscate inappropriate content in prompts by wrapping sensitive text in a benign narrative. To mitigate stepwise DACA attacks, we propose a two-layer method involving text summarization followed by binary classification. We assembled the Adversarial Text-to-Image Prompt (ATTIP) dataset ($N=940$), which contained DACA-obfuscated and non-obfuscated prompts. From the ATTIP dataset, we created two summarized versions: one generated by a small encoder model and the other by a large language model. Then, we used an encoder classifier and a GPT-4o classifier to perform content moderation on the summarized and unsummarized prompts. When compared with a classifier that operated over the unsummarized data, our method improved F1 score performance by 31%. Further, the highest recorded F1 score achieved (98%) was produced by the encoder classifier on a summarized ATTIP variant. This study indicates that pre-classification text summarization can inoculate content detection models against stepwise DACA obfuscations.', 'abstract_zh': '文本到图像模型容易受到逐步的“分解攻击”（Divide-and-Conquer Attack, DACA）的威胁，这种攻击利用大型语言模型通过在无害叙述中包裹敏感文本来掩饰提示中的不当内容。为了缓解逐步的DACA攻击，我们提出了一种两层方法，涉及文本总结后接二分类。我们构建了对抗文本到图像提示（Adversarial Text-to-Image Prompt, ATTIP）数据集（$N=940$），其中包含DACA模糊处理和未模糊处理的提示。从ATTIP数据集中，我们创建了两种总结版本：一种由小型编码器模型生成，另一种由大型语言模型生成。然后，我们使用编码器分类器和GPT-4o分类器对总结和未总结的提示进行内容审查。与在未总结数据上操作的分类器相比，我们的方法将F1分数性能提高了31%。此外，编码器分类器在总结的ATTIP变体上的最高记录F1分数达到了98%。这项研究表明，预先分类的文本总结可以抵御逐步的DACA模糊处理，使内容检测模型免疫。', 'title_zh': '披狼皮的羊：通过文本摘要技术对抗 adversarial 文本到图像提示'}
{'arxiv_id': 'arXiv:2412.12204', 'title': 'SEE: Sememe Entanglement Encoding for Transformer-bases Models Compression', 'authors': 'Jing Zhang, Shuzhen Sun, Peng Zhang, Guangxing Cao, Hui Gao, Xindian Ma, Nan Xu, Yuexian Hou', 'link': 'https://arxiv.org/abs/2412.12204', 'abstract': 'Transformer-based large language models exhibit groundbreaking capabilities, but their storage and computational costs are prohibitively high, limiting their application in resource-constrained scenarios. An effective approach is to eliminate redundant model parameters and computational costs while incorporating efficient expert-derived knowledge structures to achieve a balance between compression and performance. Therefore, we propose the \\textit{Sememe Entanglement Encoding (SEE)} algorithm. Guided by expert prior knowledge, the model is compressed through the low-rank approximation idea. In Entanglement Embedding, basic semantic units such as sememes are represented as low-dimensional vectors, and then reconstructed into high-dimensional word embeddings through the combination of generalized quantum entanglement. We adapt the Sememe Entanglement Encoding algorithm to transformer-based models of different magnitudes. Experimental results indicate that our approach achieves stable performance while compressing model parameters and computational costs.', 'abstract_zh': '基于Transformer的大语言模型展现出突破性的能力，但其存储和计算成本极高，限制了其在资源受限场景中的应用。一种有效的方法是消除冗余的模型参数和计算成本，同时引入高效的专家提取的知识结构，以在压缩和性能之间取得平衡。因此，我们提出了一种称为“Sememe Entanglement Encoding (SEE)”的算法。该算法通过专家先验知识的指导，使用低秩近似的思想对模型进行压缩。在Entanglement Embedding中，基本的语义单元（如sememe）被表示为低维向量，然后通过广义量子纠缠的组合重构为高维词嵌入。我们将Sememe Entanglement Encoding算法应用于不同规模的Transformer模型。实验结果表明，我们的方法能够在压缩模型参数和计算成本的同时保持稳定性能。', 'title_zh': 'SEE：基于语义素纠缠编码的变压器模型压缩方法'}
{'arxiv_id': 'arXiv:2412.12201', 'title': 'Embracing Large Language Models in Traffic Flow Forecasting', 'authors': 'Yusheng Zhao, Xiao Luo, Haomin Wen, Zhiping Xiao, Wei Ju, Ming Zhang', 'link': 'https://arxiv.org/abs/2412.12201', 'abstract': 'Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF.', 'abstract_zh': '交通流量预测旨在基于历史交通条件和道路网络来预测未来的交通流量，这是智能交通系统中的一项重要问题，已有大量方法被提出。现有努力主要集中在捕捉和利用空间-时间依赖性以预测未来的交通流量。尽管这些方法非常有前景，但它们在适应测试时交通条件的环境变化方面存在不足。为应对这一挑战，我们提出将大型语言模型（LLMs）引入交通流量预测，并设计了一种新的方法，名为大型语言模型增强的交通流量预测器（LEAF）。LEAF采用两个分支，分别使用图形和超图形结构来捕捉不同的空间-时间关系。在测试阶段，这两个分支首先独立预训练，然后分别产生不同的预测。基于这些预测，使用大型语言模型选择最有可能的结果。然后，应用排名损失作为学习目标，以增强两个分支的预测能力。在多种数据集上的广泛实验验证了所提出的LEAF的有效性。', 'title_zh': '拥抱大型语言模型在交通流预测中的应用'}
{'arxiv_id': 'arXiv:2412.12198', 'title': 'Pop-out vs. Glue: A Study on the pre-attentive and focused attention stages in Visual Search tasks', 'authors': 'Hendrik Beukelman, Wilder C. Rodrigues', 'link': 'https://arxiv.org/abs/2412.12198', 'abstract': "This study explores visual search asymmetry and the detection process between parallel and serial search strategies, building upon Treisman's Feature Integration Theory [3]. Our experiment examines how easy it is to locate an oblique line among vertical distractors versus a vertical line among oblique distractors, a framework previously validated by Treisman & Gormican (1988) [4] and Gupta et al. (2015) [1]. We hypothesised that an oblique target among vertical lines would produce a perceptual 'pop-out' effect, allowing for faster, parallel search, while the reverse condition would require serial search strategy. Seventy-eight participants from Utrecht University engaged in trials with varied target-distractor orientations and number of items. We measured reaction times and found a significant effect of target type on search speed: oblique targets were identified more quickly, reflecting 'pop-out' behaviour, while vertical targets demanded focused attention ('glue phase'). Our results align with past findings, supporting our hypothesis on search asymmetry and its dependency on distinct visual features. Future research could benefit from eye-tracking and neural network analysis, particularly for identifying the neural processing of visual features in both parallel and serial search conditions.", 'abstract_zh': '本研究探讨了平行搜索策略与系列搜索策略之间的视觉搜索不对称性和检出过程，基于Treisman的特征整合理论[3]进行深入探究。我们的实验研究了在垂直干扰物中寻找倾斜线条与在倾斜干扰物中寻找垂直线条的难易程度，这一框架已在Treisman & Gormican (1988)[4] 和 Gupta等（2015）[1] 的研究中得到验证。我们假设，在垂直线条中寻找倾斜目标会产生一种知觉上的“突现”效应，从而允许更快的并行搜索，而情况相反的条件则需要采用系列搜索策略。来自乌得勒支大学的78名参与者参与了具有不同目标-干扰物取向和项目数量的实验。我们测量了反应时间，并发现目标类型对搜索速度产生了显著影响：倾斜目标能够更快地被识别，反映了“突现”行为，而垂直目标则需要更为集中的注意（“粘合阶段”）。我们的结果与以往的研究发现一致，支持了搜索不对称性及其依赖于不同视觉特征的假设。未来的研究可以从视跟踪和神经网络分析方面获益，特别是对于识别平行和系列搜索条件下视觉特征的神经处理过程的研究。', 'title_zh': '突出显示 vs. 粘贴：视觉搜索任务中前注意阶段与注意聚焦阶段的研究'}
{'arxiv_id': 'arXiv:2412.12196', 'title': 'TrendSim: Simulating Trending Topics in Social Media Under Poisoning Attacks with LLM-based Multi-agent System', 'authors': 'Zeyu Zhang, Jianxun Lian, Chen Ma, Yaning Qu, Ye Luo, Lei Wang, Rui Li, Xu Chen, Yankai Lin, Le Wu, Xing Xie, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2412.12196', 'abstract': 'Trending topics have become a significant part of modern social media, attracting users to participate in discussions of breaking events. However, they also bring in a new channel for poisoning attacks, resulting in negative impacts on society. Therefore, it is urgent to study this critical problem and develop effective strategies for defense. In this paper, we propose TrendSim, an LLM-based multi-agent system to simulate trending topics in social media under poisoning attacks. Specifically, we create a simulation environment for trending topics that incorporates a time-aware interaction mechanism, centralized message dissemination, and an interactive system. Moreover, we develop LLM-based human-like agents to simulate users in social media, and propose prototype-based attackers to replicate poisoning attacks. Besides, we evaluate TrendSim from multiple aspects to validate its effectiveness. Based on TrendSim, we conduct simulation experiments to study four critical problems about poisoning attacks on trending topics for social benefit.', 'abstract_zh': '趋势话题已成为现代社交媒体的重要组成部分，吸引了用户参与对时事的讨论。然而，它们也为注入式攻击提供了新的渠道，对社会产生了负面影响。因此，研究这一关键问题并发展有效的防御策略迫在眉睫。在本文中，我们提出了一种基于大语言模型（LLM）的多智能体系统——TrendSim，以模拟在注入式攻击下的社交媒体趋势话题。具体而言，我们创建了一个包含时间意识交互机制、集中式消息传播和交互系统的趋势话题模拟环境。此外，我们开发了基于大语言模型的人类智能体来模拟社交媒体上的用户，并提出了基于原型的攻击者以模拟注入式攻击。通过从多个方面评估TrendSim，我们验证了其有效性。基于TrendSim，我们进行了模拟实验，旨在从社会利益出发，研究四个关键的注入式攻击问题。', 'title_zh': 'TrendSim：基于大型语言模型的多代理系统在中毒攻击下的社会媒体热门话题模拟研究'}
{'arxiv_id': 'arXiv:2412.12192', 'title': 'No Free Lunch for Defending Against Prefilling Attack by In-Context Learning', 'authors': 'Zhiyu Xue, Guangliang Liu, Bocheng Chen, Kristen Marie Johnson, Ramtin Pedarsani', 'link': 'https://arxiv.org/abs/2412.12192', 'abstract': 'The security of Large Language Models (LLMs) has become an important research topic since the emergence of ChatGPT. Though there have been various effective methods to defend against jailbreak attacks, prefilling attacks remain an unsolved and popular threat against open-sourced LLMs. In-Context Learning (ICL) offers a computationally efficient defense against various jailbreak attacks, yet no effective ICL methods have been developed to counter prefilling attacks. In this paper, we: (1) show that ICL can effectively defend against prefilling jailbreak attacks by employing adversative sentence structures within demonstrations; (2) characterize the effectiveness of this defense through the lens of model size, number of demonstrations, over-defense, integration with other jailbreak attacks, and the presence of safety alignment. Given the experimental results and our analysis, we conclude that there is no free lunch for defending against prefilling jailbreak attacks with ICL. On the one hand, current safety alignment methods fail to mitigate prefilling jailbreak attacks, but adversative structures within ICL demonstrations provide robust defense across various model sizes and complex jailbreak attacks. On the other hand, LLMs exhibit similar over-defensiveness when utilizing ICL demonstrations with adversative structures, and this behavior appears to be independent of model size.', 'abstract_zh': '自ChatGPT出现以来，大型语言模型（LLMs）的安全性已成为一个重要研究课题。尽管已经发展出了多种有效的方法来抵御钳制攻击，但填充式攻击仍是对开源LLMs的一种未解决且普遍存在的威胁。基于情境的学习（ICL）为抵御各种钳制攻击提供了计算效率高的方法，但尚未开发出有效的ICL方法来对付填充式攻击。本文中，我们：（1）通过在示范中采用对抗句式结构，展示了ICL能够有效防御填充式钳制攻击；（2）从模型规模、示范数量、过度防御、与其他钳制攻击的结合以及安全对齐的角度，评估了这种防御的有效性。根据实验结果和我们的分析，我们得出结论：在使用ICL防御填充式钳制攻击时，并不存在免费的午餐。一方面，当前的安全对齐方法无法缓解填充式钳制攻击，但在示范中采用对抗结构的ICL能够为各种模型规模和复杂钳制攻击提供稳健的防御。另一方面，当使用带对抗结构的ICL示范时，LLMs会表现出类似的过度防御行为，这种行为似乎与模型规模无关。', 'title_zh': '基于上下文学习防御预填充攻击并无万能解'}
{'arxiv_id': 'arXiv:2412.12190', 'title': 'iMoT: Inertial Motion Transformer for Inertial Navigation', 'authors': 'Son Minh Nguyen, Linh Duy Tran, Duc Viet Le, Paul J.M Havinga', 'link': 'https://arxiv.org/abs/2412.12190', 'abstract': "We propose iMoT, an innovative Transformer-based inertial odometry method that retrieves cross-modal information from motion and rotation modalities for accurate positional estimation. Unlike prior work, during the encoding of the motion context, we introduce Progressive Series Decoupler at the beginning of each encoder layer to stand out critical motion events inherent in acceleration and angular velocity signals. To better aggregate cross-modal interactions, we present Adaptive Positional Encoding, which dynamically modifies positional embeddings for temporal discrepancies between different modalities. During decoding, we introduce a small set of learnable query motion particles as priors to model motion uncertainties within velocity segments. Each query motion particle is intended to draw cross-modal features dedicated to a specific motion mode, all taken together allowing the model to refine its understanding of motion dynamics effectively. Lastly, we design a dynamic scoring mechanism to stabilize iMoT's optimization by considering all aligned motion particles at the final decoding step, ensuring robust and accurate velocity segment estimation. Extensive evaluations on various inertial datasets demonstrate that iMoT significantly outperforms state-of-the-art methods in delivering superior robustness and accuracy in trajectory reconstruction.", 'abstract_zh': '我们提出了一种名为iMoT的创新性基于Transformer的惯性里程计方法，该方法能够从运动和旋转模态中提取跨模态信息，以实现精确的位置估计。不同于先前的工作，我们在每个编码器层的开始部分引入了渐进系列解耦器（Progressive Series Decoupler），以突出加速度和角速度信号中固有的关键运动事件。为了更好地聚合跨模态互动，我们引入了适应性位置编码（Adaptive Positional Encoding），该方法能够动态调整位置嵌入，以适应不同模态之间的时间差异。在解码过程中，我们引入了一组可学习的查询运动粒子作为先验知识，以建模速度区间内的运动不确定性。每组查询运动粒子旨在提取特定运动模式的专用跨模态特征，这些特征综合起来使模型能够有效地细化对运动动态的理解。最后，我们设计了一种动态评分机制，在最终解码步骤中考虑所有对齐的运动粒子，以确保iMoT的优化稳定，从而实现稳健而精确的速度区间估计。在各种惯性数据集上的广泛评估表明，iMoT在轨迹重建的稳健性和准确性方面显著优于当前最先进的方法。', 'title_zh': 'iMoT：惯性运动变换器用于惯性导航\n\n在这个翻译中，“iMoT”被保留为原名，因为这是论文的特定命名；“Inertial Motion Transformer”被翻译为“惯性运动变换器”，“Inertial Navigation”被翻译为“惯性导航”。这样的翻译既符合学术规范，也保持了原文的专业性和准确性。'}
{'arxiv_id': 'arXiv:2412.12189', 'title': 'Multi-Surrogate-Teacher Assistance for Representation Alignment in Fingerprint-based Indoor Localization', 'authors': 'Son Minh Nguyen, Linh Duy Tran, Duc Viet Le, Paul J.M Havinga', 'link': 'https://arxiv.org/abs/2412.12189', 'abstract': 'Despite remarkable progress in knowledge transfer across visual and textual domains, extending these achievements to indoor localization, particularly for learning transferable representations among Received Signal Strength (RSS) fingerprint datasets, remains a challenge. This is due to inherent discrepancies among these RSS datasets, largely including variations in building structure, the input number and disposition of WiFi anchors. Accordingly, specialized networks, which were deprived of the ability to discern transferable representations, readily incorporate environment-sensitive clues into the learning process, hence limiting their potential when applied to specific RSS datasets. In this work, we propose a plug-and-play (PnP) framework of knowledge transfer, facilitating the exploitation of transferable representations for specialized networks directly on target RSS datasets through two main phases. Initially, we design an Expert Training phase, which features multiple surrogate generative teachers, all serving as a global adapter that homogenizes the input disparities among independent source RSS datasets while preserving their unique characteristics. In a subsequent Expert Distilling phase, we continue introducing a triplet of underlying constraints that requires minimizing the differences in essential knowledge between the specialized network and surrogate teachers through refining its representation learning on the target dataset. This process implicitly fosters a representational alignment in such a way that is less sensitive to specific environmental dynamics. Extensive experiments conducted on three benchmark WiFi RSS fingerprint datasets underscore the effectiveness of the framework that significantly exerts the full potential of specialized networks in localization.', 'abstract_zh': '尽管在视觉和文本领域之间知识迁移方面取得了显著进展，但将这些成就扩展到室内定位，特别是学习RSS指纹数据集之间的可迁移表示仍是挑战。这主要归因于这些RSS数据集之间固有的差异，包括建筑结构、WiFi锚点的数量和位置的差异。因此，缺乏辨识可迁移表示能力的专门网络容易将环境敏感的线索纳入学习过程，从而在应用于特定RSS数据集时限制了其潜在能力。在本文中，我们提出了一种即插即用（PnP）的知识迁移框架，通过两个主要阶段直接在目标RSS数据集上利用可迁移表示来增强专门网络的功能。首先，我们设计了专家训练阶段，该阶段包含多个代理生成教师，作为全局适配器，以统一独立源RSS数据集之间的输入差异，同时保留其独特的特性。随后，在专家精炼阶段，我们继续引入一个包含三个基本约束的三元组，以最小化专门网络与代理教师之间核心知识的差异，通过在目标数据集上优化其表示学习来精炼其功能。这过程隐式地促进了表示的对齐方式，使其对特定环境动力的敏感性降低。我们在三个基准WiFi RSS指纹数据集上的大量实验表明，该框架显著发挥出了专门网络在定位中的全部潜能。', 'title_zh': '基于指纹的室内定位中多替代教师辅助表示对齐方法'}
{'arxiv_id': 'arXiv:2412.12188', 'title': 'Predicting Internet Connectivity in Schools: A Feasibility Study Leveraging Multi-modal Data and Location Encoders in Low-Resource Settings', 'authors': 'Kelsey Doerksen, Casper Fibaek, Rochelle Schneider, Do-Hyung Kim, Isabelle Tingzon', 'link': 'https://arxiv.org/abs/2412.12188', 'abstract': 'Internet connectivity in schools is critical to provide students with the digital literary skills necessary to compete in modern economies. In order for governments to effectively implement digital infrastructure development in schools, accurate internet connectivity information is required. However, traditional survey-based methods can exceed the financial and capacity limits of governments. Open-source Earth Observation (EO) datasets have unlocked our ability to observe and understand socio-economic conditions on Earth from space, and in combination with Machine Learning (ML), can provide the tools to circumvent costly ground-based survey methods to support infrastructure development. In this paper, we present our work on school internet connectivity prediction using EO and ML. We detail the creation of our multi-modal, freely-available satellite imagery and survey information dataset, leverage the latest geographically-aware location encoders, and introduce the first results of using the new European Space Agency phi-lab geographically-aware foundational model to predict internet connectivity in Botswana and Rwanda. We find that ML with EO and ground-based auxiliary data yields the best performance in both countries, for accuracy, F1 score, and False Positive rates, and highlight the challenges of internet connectivity prediction from space with a case study in Kigali, Rwanda. Our work showcases a practical approach to support data-driven digital infrastructure development in low-resource settings, leveraging freely available information, and provide cleaned and labelled datasets for future studies to the community through a unique collaboration between UNICEF and the European Space Agency phi-lab.', 'abstract_zh': '学校中的互联网连接对于向学生提供在现代经济中竞争所需的数字素养技能至关重要。为了使政府能够有效实施学校的数字基础设施发展，需要准确的互联网连接信息。然而，传统的基于调查的方法可能会超出政府的财力和能力限制。开源地球观测（EO）数据集为我们提供了从太空观察和理解地球上社会经济状况的能力，并且与机器学习（ML）结合使用，可以提供工具来规避昂贵的地面调查方法，支持基础设施发展。在本文中，我们介绍了使用EO和ML预测学校互联网连接的工作。我们详细描述了我们多模态、免费获取的卫星图像和调查信息数据集的创建，利用最新的地理感知位置编码器，并引入了使用新的欧洲空间局phi-lab地理感知基础模型在博茨瓦纳和卢旺达预测互联网连接的第一个结果。我们发现，使用EO和地面辅助数据的ML在两国中都获得了最佳性能，无论是准确性、F1分数还是假阳性率方面，同时我们通过在卢旺达基加利的一个案例研究中讨论了从太空预测互联网连接的挑战。我们的工作展示了在低资源环境中支持数据驱动数字基础设施发展的实际方法，利用免费可获取的信息，并通过联合国儿童基金会与欧洲空间局phi-lab的独特合作，向社区提供清洗和标记的数据集，供未来的研究使用。', 'title_zh': '在低资源环境中利用多模态数据和位置编码器预测学校互联网连接可行性研究'}
{'arxiv_id': 'arXiv:2412.12185', 'title': 'Graph Similarity Computation via Interpretable Neural Node Alignment', 'authors': 'Jingjing Wang, Hongjie Zhu, Haoran Xie, Fu Lee Wang, Xiaoliang Xu, Yuxiang Wang', 'link': 'https://arxiv.org/abs/2412.12185', 'abstract': "\\Graph similarity computation is an essential task in many real-world graph-related applications such as retrieving the similar drugs given a query chemical compound or finding the user's potential friends from the social network database. Graph Edit Distance (GED) and Maximum Common Subgraphs (MCS) are the two commonly used domain-agnostic metrics to evaluate graph similarity in practice. Unfortunately, computing the exact GED is known to be a NP-hard problem. To solve this limitation, neural network based models have been proposed to approximate the calculations of GED/MCS. However, deep learning models are well-known ``black boxes'', thus the typically characteristic one-to-one node/subgraph alignment process in the classical computations of GED and MCS cannot be seen. Existing methods have paid attention to approximating the node/subgraph alignment (soft alignment), but the one-to-one node alignment (hard alignment) has not yet been solved. To fill this gap, in this paper we propose a novel interpretable neural node alignment model without relying on node alignment ground truth information. Firstly, the quadratic assignment problem in classical GED computation is relaxed to a linear alignment via embedding the features in the node embedding space. Secondly, a differentiable Gumbel-Sinkhorn module is proposed to unsupervised generate the optimal one-to-one node alignment matrix. Experimental results in real-world graph datasets demonstrate that our method outperforms the state-of-the-art methods in graph similarity computation and graph retrieval tasks, achieving up to 16\\% reduction in the Mean Squared Error and up to 12\\% improvement in the retrieval evaluation metrics, respectively.", 'abstract_zh': '图相似性计算是许多实际图相关应用中的一个核心任务，例如，给定一个查询化学化合物检索相似药物或从社交网络数据库中找到用户的潜在朋友。图编辑距离（GED）和最大公共子图（MCS）是实践中常用的两种领域无关的图相似性评估度量。不幸的是，计算精确的GED是一个NP难问题。为了解决这一局限性，已经提出了基于神经网络的模型来近似计算GED/MCS。然而，深度学习模型通常被视为“黑盒”，因此，在经典GED和MCS计算中的典型的特征对齐过程（一对一节点/子图对齐）无法直接观察到。现有的方法主要关注于近似节点/子图对齐（软对齐），但一对一节点对齐（硬对齐）仍未解决。为填补这一空白，本文提出了一种新的可解释的神经节点对齐模型，不依赖于节点对齐的真实信息。首先，通过将节点特征嵌入节点嵌入空间将经典GED计算中的二次指派问题松弛为线性对齐。其次，提出了一个可微的Gumbel-Sinkhorn模块来无监督地生成最优的节点对齐矩阵。实验结果表明，在实际图数据集上的实验结果证明，我们的方法在图相似性计算和图检索任务中均优于现有最先进的方法，分别使均方误差降低了16%并在检索评价指标上提高了12%。', 'title_zh': '基于可解释神经节点对齐的图相似性计算'}
{'arxiv_id': 'arXiv:2412.12183', 'title': 'Adopting Explainable-AI to investigate the impact of urban morphology design on energy and environmental performance in dry-arid climates', 'authors': 'Pegah Eshraghi, Riccardo Talami, Arman Nikkhah Dehnavi, Maedeh Mirdamadi, Zahra-Sadat Zomorodian', 'link': 'https://arxiv.org/abs/2412.12183', 'abstract': "In rapidly urbanizing regions, designing climate-responsive urban forms is crucial for sustainable development, especially in dry arid-climates where urban morphology has a significant impact on energy consumption and environmental performance. This study advances urban morphology evaluation by combining Urban Building Energy Modeling (UBEM) with machine learning methods (ML) and Explainable AI techniques, specifically Shapley Additive Explanations (SHAP). Using Tehran's dense urban landscape as a case study, this research assesses and ranks the impact of 30 morphology parameters at the urban block level on key energy metrics (cooling, heating, and lighting demand) and environmental performance (sunlight exposure, photovoltaic generation, and Sky View Factor). Among seven ML algorithms evaluated, the XGBoost model was the most effective predictor, achieving high accuracy (R2: 0.92) and a training time of 3.64 seconds. Findings reveal that building shape, window-to-wall ratio, and commercial ratio are the most critical parameters affecting energy efficiency, while the heights and distances of neighboring buildings strongly influence cooling demand and solar access. By evaluating urban blocks with varied densities and configurations, this study offers generalizable insights applicable to other dry-arid regions. Moreover, the integration of UBEM and Explainable AI offers a scalable, data-driven framework for developing climate-responsive urban designs adaptable to high-density environments worldwide.", 'abstract_zh': '在快速城市化的地区，设计响应气候的城市形态对于可持续发展至关重要，尤其是在干旱的气候中，城市形态对能源消耗和环境性能有显著影响。本研究通过对城市建筑能耗建模（UBEM）与机器学习方法（ML）及可解释AI技术（特别是Shapley可加解释法SHAP）的结合应用，推进了城市形态评估研究。以德黑兰密集的城市景观为例，本研究评估并排序了30个城市块层面形态参数对关键能源指标（冷却、供暖和照明需求）及环境性能（日照量、光伏发电量和天空视野系数）的影响。在评估的七种机器学习算法中，XGBoost模型是最有效的预测器，其准确度高达R2：0.92，且训练时间为3.64秒。研究发现，建筑形状、窗墙比和商业比例是影响能源效率的关键参数，而相邻建筑的高度和距离对其冷却需求和日照量有较大影响。通过对不同密度和配置的城市块进行评估，本研究提供了适用于其他干旱-半干旱地区的通用见解。此外，UBEM与可解释AI的结合为发展适应高密度环境的气候响应型城市设计提供了一个可扩展的数据驱动框架。', 'title_zh': '采用可解释的人工智能探究城市形态设计对干旱气候下能源与环境性能的影响'}
{'arxiv_id': 'arXiv:2412.12178', 'title': 'Activation Sparsity Opportunities for Compressing General Large Language Models', 'authors': 'Nobel Dhar, Bobin Deng, Md Romyull Islam, Kazi Fahim Ahmad Nasif, Liang Zhao, Kun Suo', 'link': 'https://arxiv.org/abs/2412.12178', 'abstract': "Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices' independent capabilities, alleviate the server's burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity method is orthogonal and combinable with existing techniques to maximize compression rate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN) components, which typically comprise a large proportion of parameters (around 3/2), ensure that our FFN optimizations would have a better chance of achieving effective compression. Moreover, our findings are beneficial to general LLMs and are not restricted to ReLU-based models. This work systematically investigates the tradeoff between enforcing activation sparsity and perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that we can obtain around 50% of main memory and computing reductions for critical FFN components with negligible accuracy degradation. This extra 50% sparsity does not naturally exist in the current LLMs, which require tuning LLMs' activation outputs by injecting zero-enforcing thresholds. To obtain the benefits of activation sparsity, we provide a guideline for the system architect for LLM prediction and prefetching. The success prediction allows the system to prefetch the necessary weights while omitting the inactive ones and their successors, therefore lowering cache and memory pollution and reducing LLM execution time on resource-constrained edge devices.", 'abstract_zh': '将边缘设备上的本地AI模型，如大型语言模型（LLMs），部署到边缘设备可以显著增强设备的独立功能，减轻服务器的负担并降低响应时间。由于这种巨大的潜力，许多大型科技公司已经发布了几种轻量级的小型语言模型（SLMs）来填补这一空白。然而，我们仍然有很大的动力将更强大的AI模型（LLMs）部署到边缘设备并提高它们的智能化水平。与传统的AI模型压缩方法不同，我们研究了激活稀疏性。激活稀疏性方法是独立且可与现有技术相结合的，可以在保持高准确率的同时最大化压缩率。LLMs中的前向传播网络（FFN）组件通常占据了很大比例的参数（约3/2），这确保了我们的FFN优化有更大的机会实现有效的压缩。此外，我们的研究结果对一般的LLMs具有普遍性，并不限于基于ReLU的模型。本文系统地研究了在最新一代LLMs中强制执行激活稀疏性与困惑度（准确率）之间的权衡。我们实证分析表明，我们可以在几乎不降低准确率的情况下，实现关键FFN组件约50%的主内存和计算量的减少。这种额外的50%稀疏性目前并不自然地存在于现有的LLMs中，因此需要通过注入零强制阈值来调整LLMs的激活输出。为了利用激活稀疏性的益处，我们为LLM预测和预取提供了系统架构的指南。成功预测允许系统预取必要的权重，同时忽略不活跃的权重及其后续项，从而减少缓存和内存污染，降低资源受限的边缘设备上LLM的执行时间。', 'title_zh': '激活稀疏性 opportunities for 压缩通用大型语言模型'}
{'arxiv_id': 'arXiv:2412.12177', 'title': 'Model-diff: A Tool for Comparative Study of Language Models in the Input Space', 'authors': 'Weitang Liu, Yuelei Li, Ying Wai Li, Zihan Wang, Jingbo Shang', 'link': 'https://arxiv.org/abs/2412.12177', 'abstract': "Comparing two (large) language models (LMs) side-by-side and pinpointing their prediction similarities and differences on the same set of inputs are crucial in many real-world scenarios, e.g., one can test if a licensed model was potentially plagiarized by another. Traditional analysis compares the LMs' outputs on some benchmark datasets, which only cover a limited number of inputs of designed perspectives for the intended applications. The benchmark datasets cannot prepare data to cover the test cases from unforeseen perspectives which can help us understand differences between models unbiasedly. In this paper, we propose a new model comparative analysis setting that considers a large input space where brute-force enumeration would be infeasible. The input space can be simply defined as all token sequences that a LM would produce low perplexity on -- we follow this definition in the paper as it would produce the most human-understandable inputs. We propose a novel framework \\our that uses text generation by sampling and deweights the histogram of sampling statistics to estimate prediction differences between two LMs in this input space efficiently and unbiasedly. Our method achieves this by drawing and counting the inputs at each prediction difference value in negative log-likelihood. Experiments reveal for the first time the quantitative prediction differences between LMs in a large input space, potentially facilitating the model analysis for applications such as model plagiarism.", 'abstract_zh': '将以下论文内容或标题翻译成中文，同时符合学术规范：\n\n将两个（大型）语言模型（LMs）在相同输入集上并排比较，并明确指出它们的预测相似性和差异性，在许多实际场景中都是至关重要的，例如可以测试一个许可模型是否可能被另一个模型剽窃。传统的分析方法是在某些基准数据集上比较LMs的输出，这些数据集只能涵盖设计视角下的有限输入量。而基准数据集无法准备数据来涵盖由未预见视角引起的测试案例，这些测试案例可以让我们在无偏见的情况下理解模型之间的差异。在本文中，我们提出了一种新的模型比较分析设置，该设置考虑了一个大量输入空间，直接枚举是不可行的。输入空间可以简单定义为所有LM可以生成低困惑度的标记序列——我们在论文中采用这一定义，因为它能产生最容易为人理解的输入。我们提出了一种新的框架 \\our，该框架通过采样进行文本生成，并通过调整采样统计直方图来估算在该输入空间中两个LM的预测差异，有效地且无偏见地进行比较。我们的方法通过绘制和统计负对数似然预测差异值下的输入数量来实现这一点。实验首次揭示了在大量输入空间中两个LM的定量预测差异，这可能有助于模型剽窃等应用的模型分析。', 'title_zh': 'Model-diff: 一种语言模型输入空间比较研究的工具'}
{'arxiv_id': 'arXiv:2412.12175', 'title': 'Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning', 'authors': 'Melanie Sclar, Jane Yu, Maryam Fazel-Zarandi, Yulia Tsvetkov, Yonatan Bisk, Yejin Choi, Asli Celikyilmaz', 'link': 'https://arxiv.org/abs/2412.12175', 'abstract': "Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce ExploreToM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks.", 'abstract_zh': '大型语言模型（LLMs）具备理论思维能力吗？大量研究和评估基准已经引入，以评估当前模型是否已经能够发展这一关键的社会智能能力。然而，这些研究和基准主要依靠有限的数据集，这些数据集具有简单的模式，可能导致评估中的问题盲点以及模型能力的高估。我们提出了一个名为ExploreToM的新框架，这是首个用于大规模生成多样化和具有挑战性的理论思维数据，以实现稳健的训练和评估的框架。我们的方法利用自定义领域特定语言上的A*搜索来生成复杂的故事情节结构和新颖、多样化且合理的情景，以测试LLMs的极限。我们的评估显示，最先进的LLMs，如Llama-3.1-70B和GPT-4o，在ExploreToM生成的数据上准确率分别低至0%和9%，强调了需要更多稳健的理论思维评估的必要性。由于我们的生成涵盖了前人工作的概念超集，基于我们的数据进行微调后，在经典ToMi基准（Le等人，2019）上取得了27点准确率的提升。ExploreToM还能够揭示模型在展示理论思维能力方面缺失的基本技能和因素，例如不可靠的状态跟踪或数据失衡，这可能是模型在基准测试中表现不佳的原因之一。', 'title_zh': '探索心理理论：基于程序指导的对抗性数据生成方法研究心理理论推理'}
{'arxiv_id': 'arXiv:2412.12173', 'title': 'A NotSo Simple Way to Beat Simple Bench', 'authors': 'Soham Sane, Angus McLean', 'link': 'https://arxiv.org/abs/2412.12173', 'abstract': 'This paper presents a novel framework for enhancing reasoning capabilities in large language models (LLMs) by leveraging iterative reasoning and feedback-driven methodologies. Building on the limitations identified in the SimpleBench benchmark, a dataset designed to evaluate logical coherence and real-world reasoning, we propose a multi-step prompting strategy coupled with global consistency checks to improve model accuracy and robustness. Through comparative analysis of state-of-the-art models, including Claude 3 Opus, Claude 3.5, GPT- 4o, and o1-preview, we demonstrate that iterative reasoning significantly enhances model performance, with improvements observed in both standard accuracy metrics (AVG@5) and a newly introduced metric, Extreme Averaging (EAG@5). Our results reveal model-specific strengths: Claude excels in maintaining logical consistency, while GPT-4o exhibits exploratory creativity but struggles with ambiguous prompts. By analyzing case studies and identifying gaps in spatial and temporal reasoning, we highlight areas for further refinement. The findings underscore the potential of structured reasoning frameworks to address inherent model limitations, irrespective of pretraining methodologies. This study lays the groundwork for integrating dynamic feedback mechanisms, adaptive restart strategies, and diverse evaluation metrics to advance LLM reasoning capabilities across complex and multi-domain problem spaces.', 'abstract_zh': '本文提出了一种新颖的框架，用于通过迭代推理和反馈驱动的方法增强大型语言模型（LLMs）的推理能力。在SimpleBench基准测试的基础上，该基准测试旨在评估逻辑连贯性和实际推理能力，我们提出了一种多步提示策略，并结合全局一致性检查，以提高模型的准确性和稳健性。通过对比分析当前最先进的模型，包括Claude 3 Opus、Claude 3.5、GPT-4o和o1-preview，我们证明了迭代推理在提升模型性能方面具有显著效果，不仅在传统准确度指标（AVG@5）中表现出改进，在新引入的指标Extreme Averaging（EAG@5）中也表现出了改进。我们的结果显示了模型特定的优势：Claude 在保持逻辑一致性方面表现出色，而GPT-4o 则表现出探索性创造力，但对模糊的提示处理不足。通过分析案例研究并识别空间和时间推理中的不足之处，我们指出了进一步优化的领域。研究结果强调了结构化推理框架在解决固有模型限制方面的潜力，无论预训练方法如何。该研究为整合动态反馈机制、自适应重启策略以及多样化的评估指标奠定了基础，以在复杂的多领域问题空间中增强LLM的推理能力。', 'title_zh': '一种简单基准未必简单的超越方法'}
{'arxiv_id': 'arXiv:2412.12171', 'title': 'AI Adoption to Combat Financial Crime: Study on Natural Language Processing in Adverse Media Screening of Financial Services in English and Bangla multilingual interpretation', 'authors': 'Soumita Roy', 'link': 'https://arxiv.org/abs/2412.12171', 'abstract': 'This document explores the potential of employing Artificial Intelligence (AI), specifically Natural Language Processing (NLP), to strengthen the detection and prevention of financial crimes within the Mobile Financial Services(MFS) of Bangladesh with multilingual scenario. The analysis focuses on the utilization of NLP for adverse media screening, a vital aspect of compliance with anti-money laundering (AML) and combating financial terrorism (CFT) regulations. Additionally, it investigates the overall reception and obstacles related to the integration of AI in Bangladeshi banks. This report measures the effectiveness of NLP is promising with an accuracy around 94\\%. NLP algorithms display substantial promise in accurately identifying adverse media content linked to financial crimes. The lack of progress in this aspect is visible in Bangladesh, whereas globally the technology is already being used to increase effectiveness and efficiency. Hence, it is clear there is an issue with the acceptance of AI in Bangladesh. Some AML \\& CFT concerns are already being addressed by AI technology. For example, Image Recognition OCR technology are being used in KYC procedures. Primary hindrances to AI integration involve a lack of technical expertise, high expenses, and uncertainties surrounding regulations. This investigation underscores the potential of AI-driven NLP solutions in fortifying efforts to prevent financial crimes in Bangladesh.', 'abstract_zh': '本文探讨了利用人工智能（AI），特别是自然语言处理（NLP），强化孟加拉国移动金融服务（MFS）中多语言背景下金融犯罪检测与预防的可能性。分析集中在使用NLP进行不良媒体筛查上，这是遵守反洗钱（AML）法规和打击金融恐怖主义（CFT）的关键方面。此外，本文还调查了人工智能在孟加拉国银行中集成的整体接受度和障碍。研究结果表明，NLP的有效性令人鼓舞，准确率达到约94%。NLP算法在准确识别与金融犯罪相关的不良媒体内容方面显示出巨大潜力。在孟加拉国，这一进展相对滞后，而全球范围内，该技术已被用于提高效率和有效性。因此，显然在孟加拉国对AI的接受度存在问题。人工智能在某些AML与CFT方面已经得到了应用，例如，在客户身份识别（KYC）过程中使用图像识别OCR技术。AI集成的主要障碍包括技术专长不足、高昂的成本以及围绕法规的不确定性。本文突显了AI驱动的NLP解决方案在加强孟加拉国防止金融犯罪努力方面的重要潜力。', 'title_zh': 'AI在打击金融犯罪中的应用：金融服务领域不良媒体筛查中的自然语言处理研究（英孟双语解读）'}
{'arxiv_id': 'arXiv:2412.12170', 'title': 'PickLLM: Context-Aware RL-Assisted Large Language Model Routing', 'authors': 'Dimitrios Sikeridis, Dennis Ramdass, Pranay Pareek', 'link': 'https://arxiv.org/abs/2412.12170', 'abstract': 'Recently, the number of off-the-shelf Large Language Models (LLMs) has exploded with many open-source options. This creates a diverse landscape regarding both serving options (e.g., inference on local hardware vs remote LLM APIs) and model heterogeneous expertise. However, it is hard for the user to efficiently optimize considering operational cost (pricing structures, expensive LLMs-as-a-service for large querying volumes), efficiency, or even per-case specific measures such as response accuracy, bias, or toxicity. Also, existing LLM routing solutions focus mainly on cost reduction, with response accuracy optimizations relying on non-generalizable supervised training, and ensemble approaches necessitating output computation for every considered LLM candidate. In this work, we tackle the challenge of selecting the optimal LLM from a model pool for specific queries with customizable objectives. We propose PickLLM, a lightweight framework that relies on Reinforcement Learning (RL) to route on-the-fly queries to available models. We introduce a weighted reward function that considers per-query cost, inference latency, and model response accuracy by a customizable scoring function. Regarding the learning algorithms, we explore two alternatives: PickLLM router acting as a learning automaton that utilizes gradient ascent to select a specific LLM, or utilizing stateless Q-learning to explore the set of LLMs and perform selection with a $\\epsilon$-greedy approach. The algorithm converges to a single LLM for the remaining session queries. To evaluate, we utilize a pool of four LLMs and benchmark prompt-response datasets with different contexts. A separate scoring function is assessing response accuracy during the experiment. We demonstrate the speed of convergence for different learning rates and improvement in hard metrics such as cost per querying session and overall response latency.', 'abstract_zh': '近年来，现成的大语言模型（LLMs）的数量激增，开源选择也越来越多。这在服务方式（例如，本地硬件推理 vs 远程LLM API推理）和模型异质性专业知识方面创造了多样化的景象。然而，用户在考虑运营成本（定价结构，大规模查询量的昂贵的LLM即服务）、效率，甚至是针对单一案例的具体措施，如响应准确性、偏见或有害内容方面进行优化时，仍然面临挑战。现有的一些LLM路由解决方案主要侧重于成本削减，响应准确性优化依赖于非通用化的监督训练，而集成方法则需要对每个考虑中的LLM候选模型进行输出计算。在这项工作中，我们致力于解决从模型池中为特定查询选择最佳LLM的问题，并实现自定义目标。我们提出了PickLLM，这是一种轻量级框架，利用强化学习（RL）在运行时将查询路由到可用模型。我们引入了一个加权奖励函数，该函数通过可定制的评分函数考虑每个查询的成本、推理延迟和模型响应准确性。在学习算法方面，我们探索了两种替代方案：PickLLM路由器作为一个学习automaton，使用梯度上升来选择特定的LLM，或者利用无状态的Q学习来探索LLM集合，并使用$\\epsilon$-贪心策略进行选择。该算法在后续查询中收敛于单一的LLM。为了进行评估，我们使用了四种LLM的模型池，并对具有不同上下文的提示-响应数据集进行了基准测试。在实验期间，我们使用了一个独立的评分函数来评估响应准确性。我们展示了不同学习率下收敛速度的不同，并在成本和总体响应延迟等硬指标方面证明了性能改进。', 'title_zh': 'PickLLM：基于上下文的强化学习辅助大型语言模型路由'}
{'arxiv_id': 'arXiv:2412.12169', 'title': 'Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off', 'authors': 'Eoin M. Kenny, Julie A. Shah', 'link': 'https://arxiv.org/abs/2412.12169', 'abstract': 'Regulation is increasingly cited as the most important and pressing concern in machine learning. However, it is currently unknown how to implement this, and perhaps more importantly, how it would effect model performance alongside human collaboration if actually realized. In this paper, we attempt to answer these questions by building a regulatable large-language model (LLM), and then quantifying how the additional constraints involved affect (1) model performance, alongside (2) human collaboration. Our empirical results reveal that it is possible to force an LLM to use human-defined features in a transparent way, but a "regulation performance trade-off" previously not considered reveals itself in the form of a 7.34% classification performance drop. Surprisingly however, we show that despite this, such systems actually improve human task performance speed and appropriate confidence in a realistic deployment setting compared to no AI assistance, thus paving a way for fair, regulatable AI, which benefits users.', 'abstract_zh': '监管越来越被视作机器学习中最重要和紧迫的关注点。然而，目前尚不清楚如何实施这一监管，而且更为重要的是，如果实际实施，这种监管将如何影响模型性能及其与人类合作的关系。本文试图通过构建一个可调节的大规模语言模型 (LLM)，并量化额外约束条件的影响，来回答这些问题。具体来说，我们发现有可能以透明的方式强制 LLM 使用人类定义的特征。然而，一个先前未被考虑的“监管性能权衡”显现出来，表现为分类性能下降了 7.34%。然而，令人惊讶的是，我们在实际部署环境中发现，此类系统实际上能够提高人类任务执行速度和适宜的信心，与没有人工智能辅助的情况相比。这为公平且可调节的人工智能铺平了道路，从而造福用户。', 'title_zh': '具有可解释性的语言模型的监管很可能会导致性能权衡'}
{'arxiv_id': 'arXiv:2412.12168', 'title': 'A Decomposition Modeling Framework for Seasonal Time-Series Forecasting', 'authors': 'Yining Pang, Chenghan Li', 'link': 'https://arxiv.org/abs/2412.12168', 'abstract': 'Seasonal time series exhibit intricate long-term dependencies, posing a significant challenge for accurate future prediction. This paper introduces the Multi-scale Seasonal Decomposition Model (MSSD) for seasonal time-series forecasting. Initially, leveraging the inherent periodicity of seasonal time series, we decompose the univariate time series into three primary components: Ascending, Peak, and Descending. This decomposition approach enhances the capture of periodic features. By addressing the limitations of existing time-series modeling methods, particularly in modeling the Peak component, this research proposes a multi-scale network structure designed to effectively capture various potential peak fluctuation patterns in the Peak component. This study integrates Conv2d and Temporal Convolutional Networks to concurrently capture global and local features. Furthermore, we incorporate multi-scale reshaping to augment the modeling capacity for peak fluctuation patterns. The proposed methodology undergoes validation using three publicly accessible seasonal datasets. Notably, in both short-term and long-term fore-casting tasks, our approach exhibits a 10$\\%$ reduction in error compared to the baseline models.', 'abstract_zh': '季节性时间序列表现出复杂的长期依赖关系，给未来预测带来了重大挑战。本文提出了多尺度季节分解模型（MSSD）用于季节性时间序列预测。首先，利用季节性时间序列固有的周期性，我们将其分解为三个主要组成部分：上升部分、峰值部分和下降部分。这种分解方法有助于更有效地捕捉周期性特征。通过解决现有时间序列建模方法的局限性，特别是对峰值部分建模的不足，本研究提出了一种多尺度网络结构，以有效地捕捉峰值部分的各种潜在波动模式。本研究将Conv2d和时序卷积网络结合使用，同时捕捉全局和局部特征。此外，我们引入了多尺度重塑，以增强对峰值波动模式建模的能力。所提出的方法通过三个公开可用的季节性数据集进行了验证。值得注意的是，在短、长期预测任务中，与基线模型相比，我们的方法在误差上降低了10%。', 'title_zh': '季节性时间序列预测的分解建模框架'}
{'arxiv_id': 'arXiv:2412.12167', 'title': 'Greek2MathTex: A Greek Speech-to-Text Framework for LaTeX Equations Generation', 'authors': 'Evangelia Gkritzali, Panagiotis Kaliosis, Sofia Galanaki, Elisavet Palogiannidi, Theodoros Giannakopoulos', 'link': 'https://arxiv.org/abs/2412.12167', 'abstract': "In the vast majority of the academic and scientific domains, LaTeX has established itself as the de facto standard for typesetting complex mathematical equations and formulae. However, LaTeX's complex syntax and code-like appearance present accessibility barriers for individuals with disabilities, as well as those unfamiliar with coding conventions. In this paper, we present a novel solution to this challenge through the development of a novel speech-to-LaTeX equations system specifically designed for the Greek language. We propose an end-to-end system that harnesses the power of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) techniques to enable users to verbally dictate mathematical expressions and equations in natural language, which are subsequently converted into LaTeX format. We present the architecture and design principles of our system, highlighting key components such as the ASR engine, the LLM-based prompt-driven equations generation mechanism, as well as the application of a custom evaluation metric employed throughout the development process. We have made our system open source and available at this https URL.", 'abstract_zh': '在大多数学术和科学领域中，LaTeX 已经成为排版复杂数学方程和公式的事实标准。然而，LaTeX 的复杂语法和代码般的外观为其带来了无障碍使用方面的障碍，特别是对残疾人和不熟悉编程规则的用户。在本文中，我们通过开发一种专为希腊语设计的新型语音识别至 LaTeX 方程系统，提出了一种解决这一挑战的新型解决方案。我们提出了一种端到端系统，该系统结合了自动语音识别（ASR）和自然语言处理（NLP）技术，使用户能够自然地口头表达数学表达式和方程，这些表达式和方程随后被转换为 LaTeX 格式。本文介绍了该系统的体系结构和设计原则，重点介绍了关键组件，如 ASR 引擎、基于大型语言模型（LLM）的提示驱动方程生成机制，以及在整个开发过程中应用的自定义评估指标。我们已将该系统开源，并可在以下链接访问：[链接地址]。', 'title_zh': 'Greek2MathTeX：一种用于LaTeX方程生成的古希腊语语音转文本框架'}
{'arxiv_id': 'arXiv:2412.12166', 'title': 'Performance of a large language model-Artificial Intelligence based chatbot for counseling patients with sexually transmitted infections and genital diseases', 'authors': 'Nikhil Mehta, Sithira Ambepitiya, Thanveer Ahamad, Dinuka Wijesundara, Yudara Kularathne', 'link': 'https://arxiv.org/abs/2412.12166', 'abstract': 'Introduction: Global burden of sexually transmitted infections (STIs) is rising out of proportion to specialists. Current chatbots like ChatGPT are not tailored for handling STI-related concerns out of the box. We developed Otiz, an Artificial Intelligence-based (AI-based) chatbot platform designed specifically for STI detection and counseling, and assessed its performance. Methods: Otiz employs a multi-agent system architecture based on GPT4-0613, leveraging large language model (LLM) and Deterministic Finite Automaton principles to provide contextually relevant, medically accurate, and empathetic responses. Its components include modules for general STI information, emotional recognition, Acute Stress Disorder detection, and psychotherapy. A question suggestion agent operates in parallel. Four STIs (anogenital warts, herpes, syphilis, urethritis/cervicitis) and 2 non-STIs (candidiasis, penile cancer) were evaluated using prompts mimicking patient language. Each prompt was independently graded by two venereologists conversing with Otiz as patient actors on 6 criteria using Numerical Rating Scale ranging from 0 (poor) to 5 (excellent). Results: Twenty-three venereologists did 60 evaluations of 30 prompts. Across STIs, Otiz scored highly on diagnostic accuracy (4.1-4.7), overall accuracy (4.3-4.6), correctness of information (5.0), comprehensibility (4.2-4.4), and empathy (4.5-4.8). However, relevance scores were lower (2.9-3.6), suggesting some redundancy. Diagnostic scores for non-STIs were lower (p=0.038). Inter-observer agreement was strong, with differences greater than 1 point occurring in only 12.7% of paired evaluations. Conclusions: AI conversational agents like Otiz can provide accurate, correct, discrete, non-judgmental, readily accessible and easily understandable STI-related information in an empathetic manner, and can alleviate the burden on healthcare systems.', 'abstract_zh': '简介：性传播感染（STIs）的全球负担正在不成比例地增加，而现有的聊天机器人如ChatGPT并未针对处理STI相关问题进行专门设计。我们开发了Otiz，一个基于人工智能（AI）的聊天机器人平台，专门用于STI检测和咨询，并评估了其性能。方法：Otiz采用基于GPT4-0613的多智能体系统架构，利用大型语言模型（LLM）和确定型有限自动机（DFA）原理，提供上下文相关、医学准确且富有同理心的回复。其组件包括STI基础知识模块、情绪识别模块、急性应激障碍检测模块和心理治疗模块。同时，一个问答建议代理也运行在一旁。使用模仿患者语言的提示，评估了四种STI（生殖器疣、单纯疱疹、梅毒、尿道炎/宫颈炎）和两种非STI（念珠菌感染、阴茎癌）的诊断准确性。每位提示由两名性病专家分别扮演病人角色，使用6项标准的数字评分量表（0分为差，5分为优秀）评估Otiz的回复。结果：共有23名性病专家对30个提示进行了60次评估。对于所有STI，Otiz在诊断准确性（4.1-4.7）、总体准确性（4.3-4.6）、信息的准确性（5.0）、可理解性（4.2-4.4）和同理心（4.5-4.8）方面得分较高。然而，相关性得分较低（2.9-3.6），表明其存在一定程度的冗余。非STI的诊断得分较低（P=0.038）。观察者间的一致性较强，配对评估中得分差超过1分的情况仅为12.7%。结论：如Otiz这样的AI对话代理能够提供准确、精确、不含偏见、易于获取且易于理解的STI相关信息，并且能够减轻医疗保健系统的负担。', 'title_zh': '基于人工智能的聊天机器人在咨询性传播感染和生殖器疾病患者中的表现：一个大型语言模型的研究'}
{'arxiv_id': 'arXiv:2412.12165', 'title': 'Multimodal Approaches to Fair Image Classification: An Ethical Perspective', 'authors': 'Javon Hickmon', 'link': 'https://arxiv.org/abs/2412.12165', 'abstract': 'In the rapidly advancing field of artificial intelligence, machine perception is becoming paramount to achieving increased performance. Image classification systems are becoming increasingly integral to various applications, ranging from medical diagnostics to image generation; however, these systems often exhibit harmful biases that can lead to unfair and discriminatory outcomes. Machine Learning systems that depend on a single data modality, i.e. only images or only text, can exaggerate hidden biases present in the training data, if the data is not carefully balanced and filtered. Even so, these models can still harm underrepresented populations when used in improper contexts, such as when government agencies reinforce racial bias using predictive policing. This thesis explores the intersection of technology and ethics in the development of fair image classification models. Specifically, I focus on improving fairness and methods of using multiple modalities to combat harmful demographic bias. Integrating multimodal approaches, which combine visual data with additional modalities such as text and metadata, allows this work to enhance the fairness and accuracy of image classification systems. The study critically examines existing biases in image datasets and classification algorithms, proposes innovative methods for mitigating these biases, and evaluates the ethical implications of deploying such systems in real-world scenarios. Through comprehensive experimentation and analysis, the thesis demonstrates how multimodal techniques can contribute to more equitable and ethical AI solutions, ultimately advocating for responsible AI practices that prioritize fairness.', 'abstract_zh': '在快速发展的人工智能领域，机器感知正在变得至关重要，以实现性能的提升。图像分类系统日益成为各种应用的核心，从医学诊断到图像生成；然而，这些系统往往会出现有害的偏见，可能导致不公平和歧视性结果。依赖单一数据模态的机器学习系统，例如仅依赖图像或仅依赖文本的数据，如果数据未经过仔细平衡和筛选，可能会夸大训练数据中存在的隐藏偏见。即便如此，这些模型在不当的情境下使用时，仍然可能对未充分代表的群体造成伤害，例如当政府机构利用预测警务强化种族偏见时。本论文探讨了技术与伦理在构建公平图像分类模型开发中的交叉领域。具体而言，我重点关注提高公平性以及利用多种模态方法以对抗有害的群体偏见。将多模态方法应用于视觉数据与额外模态（如文本和元数据）的结合，使这项工作能够增强图像分类系统的公平性和准确性。研究批判性地审视了图像数据集和分类算法中现有的偏见，提出了减轻这些偏见的创新方法，并评估了在实际场景中部署此类系统的伦理影响。通过全面的实验和分析，本论文展示了多模态技术如何促进更加公平和伦理的人工智能解决方案，最终倡导具有责任意识的人工智能实践，强调公平性的重要性。', 'title_zh': '多模态公平图像分类的方法：伦理视角'}
{'arxiv_id': 'arXiv:2412.12164', 'title': 'GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake News Detection', 'authors': 'Lingzhi Shen, Yunfei Long, Xiaohao Cai, Imran Razzak, Guanming Chen, Kang Liu, Shoaib Jameel', 'link': 'https://arxiv.org/abs/2412.12164', 'abstract': "Multimodal fake news detection often involves modelling heterogeneous data sources, such as vision and language. Existing detection methods typically rely on fusion effectiveness and cross-modal consistency to model the content, complicating understanding how each modality affects prediction accuracy. Additionally, these methods are primarily based on static feature modelling, making it difficult to adapt to the dynamic changes and relationships between different data modalities. This paper develops a significantly novel approach, GAMED, for multimodal modelling, which focuses on generating distinctive and discriminative features through modal decoupling to enhance cross-modal synergies, thereby optimizing overall performance in the detection process. GAMED leverages multiple parallel expert networks to refine features and pre-embed semantic knowledge to improve the experts' ability in information selection and viewpoint sharing. Subsequently, the feature distribution of each modality is adaptively adjusted based on the respective experts' opinions. GAMED also introduces a novel classification technique to dynamically manage contributions from different modalities, while improving the explainability of decisions. Experimental results on the Fakeddit and Yang datasets demonstrate that GAMED performs better than recently developed state-of-the-art models. The source code can be accessed at this https URL.", 'abstract_zh': '多模态假新闻检测通常涉及建模异构数据源，如视觉和语言。现有的检测方法通常依赖于特征融合的有效性和跨模态一致性来建模内容，这使得理解每种模态如何影响预测准确度变得复杂。此外，这些方法主要基于静态特征建模，难以适应不同数据模态之间的动态变化和关系。本文开发了一种显著新颖的方法——GAMED，用于多模态建模，该方法通过模态解耦生成独特的可区分特征，以增强跨模态协同作用，从而优化检测过程的整体性能。GAMED 利用多个并行专家网络来细化特征并预嵌入语义知识，以提高专家在信息选择和观点共享方面的能力。之后，根据各个专家的意见，适应性地调整每种模态的特征分布。GAMED 引入了一种新型的分类技术，以动态管理不同模态的贡献，同时提高决策的可解释性。在 Fakeddit 和 Yang 数据集上的实验结果表明，GAMED 的性能优于最近开发的最先进的模型。源代码可以在以下网址访问：https://your-source-code-url.com', 'title_zh': 'GAMED：知识自适应多专家解耦在多模态假新闻检测中的应用'}
{'arxiv_id': 'arXiv:2412.12163', 'title': 'Towards LLM-based optimization compilers. Can LLMs learn how to apply a single peephole optimization? Reasoning is all LLMs need!', 'authors': 'Xiangxin Fang, Lev Mukhanov', 'link': 'https://arxiv.org/abs/2412.12163', 'abstract': 'Large Language Models (LLMs) have demonstrated great potential in various language processing tasks, and recent studies have explored their application in compiler optimizations. However, all these studies focus on the conventional open-source LLMs, such as Llama2, which lack enhanced reasoning mechanisms. In this study, we investigate the errors produced by the fine-tuned 7B-parameter Llama2 model as it attempts to learn and apply a simple peephole optimization for the AArch64 assembly code. We provide an analysis of the errors produced by the LLM and compare it with state-of-the-art OpenAI models which implement advanced reasoning logic, including GPT-4o and GPT-o1 (preview). We demonstrate that OpenAI GPT-o1, despite not being fine-tuned, outperforms the fine-tuned Llama2 and GPT-4o. Our findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-o1. We hope our work will inspire further research on using LLMs with enhanced reasoning mechanisms and chain-of-thought for code generation and optimization.', 'abstract_zh': '大型语言模型（LLMs）在各种语言处理任务中展现出了巨大的潜力， recent的研究已经探索了它们在编译器优化中的应用。然而，这些研究主要集中在传统的开源LLM，如Llama2，而这些LLM缺乏增强的推理机制。在本研究中，我们分析了微调后的7亿参数Llama2模型在尝试学习并应用一种简单的孔窥优化（peephole optimization）于AArch64汇编代码时产生的错误。我们对LLM产生的错误进行了分析，并将其与实现高级推理逻辑的最新OpenAI模型进行了比较，包括GPT-4o和GPT-o1（演示版）。结果显示，尽管GPT-o1没有进行微调，它仍然优于微调后的Llama2和GPT-4o。我们的研究发现这种优势主要是由于GPT-o1中实现的链式思维推理。我们希望本研究能够激发更多关于使用增强推理机制和链式思维进行代码生成和优化的研究。', 'title_zh': '基于LLM的优化编译器研究：LLM能够学会应用单个穿孔优化吗？推理是它们所需的一切！'}
{'arxiv_id': 'arXiv:2412.12161', 'title': 'Discover Physical Concepts and Equations with Machine Learning', 'authors': 'Bao-Bing Li, Yi Gu, Shao-Feng Wu', 'link': 'https://arxiv.org/abs/2412.12161', 'abstract': "Machine learning can uncover physical concepts or physical equations when prior knowledge from another one is available. However, in many cases, these two aspects are coupled and cannot be discovered independently. We extend SciNet, which is a neural network architecture that simulates the human physical reasoning process for physics discovery, by proposing a model that combines Variational Autoencoders (VAEs) with Neural Ordinary Differential Equations (Neural ODEs). This allows us to simultaneously discover physical concepts and governing equations from simulated experimental data across diverse physical systems. We apply the model to several key examples inspired by the history of physics, including Copernicus' heliocentric solar system, Newton's law of universal gravitation, the wave function together with the Schrödinger equation, and spin-1/2 along with the Pauli equation. The results demonstrate that the neural network successfully reconstructs the corresponding theories.", 'abstract_zh': '机器学习可以在有其他领域的先验知识时揭示物理概念或物理方程。然而，在许多情况下，这两个方面是相互耦合的，不能独立发现。我们通过提出一种结合变分自编码器（VAEs）和神经常微分方程（Neural ODEs）的模型，来扩展SciNet。SciNet是一种模拟人类物理推理过程的神经网络架构，用于物理发现。该模型使我们能够同时从模拟实验数据中发现各种物理系统中的物理概念和支配方程。我们应用该模型到几个受物理学史启发的关键示例，包括哥白尼的日心说、牛顿的万有引力定律、薛定谔方程及其波函数、自旋-1/2以及保罗方程。结果显示，该神经网络成功地重构了相应的理论。', 'title_zh': '使用机器学习发现物理概念和方程'}
{'arxiv_id': 'arXiv:2412.12160', 'title': 'Climate Aware Deep Neural Networks (CADNN) for Wind Power Simulation', 'authors': 'Ali Forootani, Danial Esmaeili Aliabadi, Daniela Thraen', 'link': 'https://arxiv.org/abs/2412.12160', 'abstract': 'Wind power forecasting plays a critical role in modern energy systems, facilitating the integration of renewable energy sources into the power grid. Accurate prediction of wind energy output is essential for managing the inherent intermittency of wind power, optimizing energy dispatch, and ensuring grid stability. This paper proposes the use of Deep Neural Network (DNN)-based predictive models that leverage climate datasets, including wind speed, atmospheric pressure, temperature, and other meteorological variables, to improve the accuracy of wind power simulations. In particular, we focus on the Coupled Model Intercomparison Project (CMIP) datasets, which provide climate projections, as inputs for training the DNN models. These models aim to capture the complex nonlinear relationships between the CMIP-based climate data and actual wind power generation at wind farms located in Germany. Our study compares various DNN architectures, specifically Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM) networks, and Transformer-enhanced LSTM models, to identify the best configuration among these architectures for climate-aware wind power simulation. The implementation of this framework involves the development of a Python package (CADNN) designed to support multiple tasks, including statistical analysis of the climate data, data visualization, preprocessing, DNN training, and performance evaluation. We demonstrate that the DNN models, when integrated with climate data, significantly enhance forecasting accuracy. This climate-aware approach offers a deeper understanding of the time-dependent climate patterns that influence wind power generation, providing more accurate predictions and making it adaptable to other geographical regions.', 'abstract_zh': '风能预测在现代能源系统中扮演着至关重要的角色，有助于可再生能源的整合并进入电力网络。准确预测风能输出对于管理风能的固有间歇性、优化能源调度以及确保电网稳定性至关重要。本文提出了一种基于深度神经网络（DNN）的预测模型，利用气象数据集，包括风速、大气压力、温度以及其他气象变量，以提高风能模拟的准确性。特别是，我们关注耦合模型比较项目（CMIP）的数据集，这些数据集提供了气候预测，作为训练DNN模型的输入。这些模型旨在捕捉基于CMIP的气候数据与德国风电场实际风能发电之间复杂非线性的关系。我们研究比较了不同的DNN架构，包括多层感知器（MLP）、长短期记忆（LSTM）网络以及增强的LSTM模型，以识别这些架构中最适合气候意识风能模拟的配置。该框架的实施涉及开发了一个Python软件包（CADNN），旨在支持包括气候数据的统计分析、数据可视化、预处理、DNN训练和性能评估在内的多项任务。研究表明，当与气候数据结合使用时，DNN模型显著提高了预测精度。这种气候意识的方法提供了对影响风能发电的时间相关气候模式的更深入理解，提供了更准确的预测，并使其能够适应其他地理区域。', 'title_zh': '基于气候感知的深度神经网络（CADNN）在风能模拟中的应用'}
{'arxiv_id': 'arXiv:2412.12159', 'title': 'Personalized Sleep Staging Leveraging Source-free Unsupervised Domain Adaptation', 'authors': 'Yangxuan Zhou, Sha Zhao, Jiquan Wang, Haiteng Jiang, hijian Li, Benyan Luo, Tao Li, Gang Pan', 'link': 'https://arxiv.org/abs/2412.12159', 'abstract': 'Sleep staging is crucial for assessing sleep quality and diagnosing related disorders. Recent deep learning models for automatic sleep staging using polysomnography often suffer from poor generalization to new subjects because they are trained and tested on the same labeled datasets, overlooking individual differences. To tackle this issue, we propose a novel Source-Free Unsupervised Individual Domain Adaptation (SF-UIDA) framework. This two-step adaptation scheme allows the model to effectively adjust to new unlabeled individuals without needing source data, facilitating personalized customization in clinical settings. Our framework has been applied to three established sleep staging models and tested on three public datasets, achieving state-of-the-art performance.', 'abstract_zh': '睡眠分期对于评估睡眠质量和诊断相关疾病至关重要。使用多导睡眠图（polysomnography）进行自动睡眠分期的最新深度学习模型往往因为只在一个标注数据集上进行训练和测试，而难以泛化到新个体，忽视了个体内差异。为解决这一问题，我们提出了一种新颖的源数据无关无监督个体域适应（Source-Free Unsupervised Individual Domain Adaptation, SF-UIDA）框架。该两步适应方案允许模型在无需源数据的情况下有效调整以适应新的未标注个体，从而在临床环境中实现个性化定制。我们的框架已在三个已建立的睡眠分期模型上进行了应用，并在三个公开数据集上进行了测试，取得了目前最优的性能。', 'title_zh': '基于源无监督域适应的个性化睡眠阶段划分'}
{'arxiv_id': 'arXiv:2412.12158', 'title': 'Hyperbolic Hypergraph Neural Networks for Multi-Relational Knowledge Hypergraph Representation', 'authors': 'Mengfan Li, Xuanhua Shi, Chenqi Qiao, Teng Zhang, Hai Jin', 'link': 'https://arxiv.org/abs/2412.12158', 'abstract': 'Knowledge hypergraphs generalize knowledge graphs using hyperedges to connect multiple entities and depict complicated relations. Existing methods either transform hyperedges into an easier-to-handle set of binary relations or view hyperedges as isolated and ignore their adjacencies. Both approaches have information loss and may potentially lead to the creation of sub-optimal models. To fix these issues, we propose the Hyperbolic Hypergraph Neural Network (H2GNN), whose essential component is the hyper-star message passing, a novel scheme motivated by a lossless expansion of hyperedges into hierarchies. It implements a direct embedding that consciously incorporates adjacent entities, hyper-relations, and entity position-aware information. As the name suggests, H2GNN operates in the hyperbolic space, which is more adept at capturing the tree-like hierarchy. We compare H2GNN with 15 baselines on knowledge hypergraphs, and it outperforms state-of-the-art approaches in both node classification and link prediction tasks.', 'abstract_zh': '知识超图通过使用超边连接多个实体并描绘复杂的相互关系，从而扩展了知识图谱。现有的方法要么将超边转换为更易于处理的一系列二元关系，要么将超边视为孤立的实体而忽略了它们的邻近关系。这两种方法都存在信息损失，并可能潜在地导致生成次优模型。为了解决这些问题，我们提出了超球体超图神经网络（H2GNN），其核心组件是超星消息传递，这是一种受超边的无损扩展到层次结构启发的新方案。它直接嵌入了相邻实体、超关系及实体位置感知信息。如其名称所示，H2GNN 在双曲空间中运行，这使得它更适合捕捉树状层次结构。我们在知识超图上将H2GNN 与15个基线进行比较，并在节点分类和链接预测任务中均表现出优于当前最先进的方法的性能。', 'title_zh': '超双曲超图神经网络在多关系知识超图表示中的应用'}
{'arxiv_id': 'arXiv:2412.12157', 'title': 'What Makes In-context Learning Effective for Mathematical Reasoning: A Theoretical Analysis', 'authors': 'Jiayu Liu, Zhenya Huang, Chaokun Wang, Xunpeng Huang, Chengxiang Zhai, Enhong Chen', 'link': 'https://arxiv.org/abs/2412.12157', 'abstract': "Owing to the capability of in-context learning, large language models (LLMs) have shown impressive performance across diverse mathematical reasoning benchmarks. However, we find that few-shot demonstrations can sometimes bring negative performance and their effectiveness on LLMs' reasoning abilities remains unreliable. To this end, in this paper, we aim to theoretically analyze the impact of in-context demonstrations on LLMs' reasoning performance. We prove that the reasoning efficacy (measured by empirical prediction loss) can be bounded by a LLM-oriented semantic similarity and an inference stability of demonstrations, which is general for both one-shot and few-shot scenarios. Based on this finding, we propose a straightforward, generalizable, and low-complexity demonstration selection method named LMS3. It can adaptively facilitate to select the most pertinent samples for different LLMs and includes a novel demonstration rejection mechanism to automatically filter out samples that are unsuitable for few-shot learning. Through experiments on three representative benchmarks, two LLM backbones, and multiple few-shot settings, we verify that our LMS3 has superiority and achieves consistent improvements on all datasets, which existing methods have been unable to accomplish.", 'abstract_zh': '得益于上下文学习的能力，大型语言模型（LLMs）在多种数学推理基准测试中表现出了令人印象深刻的性能。然而，我们发现少量示例展示有时可能会带来负面性能，且其在LLMs推理能力上的有效性依然是不可靠的。为了解决这一问题，本文旨在理论分析上下文示例对LLMs推理性能的影响。我们证明了推理有效性（通过经验预测损失衡量）可以由一个LLMs导向的语义相似性和示例的推理稳定性所界限定，这适用于单示例和少量示例的多种场景。基于这一发现，我们提出了一种简单、通用且计算复杂度低的示例选择方法，即LMS3。它能够自适应地选择最相关的示例，适用于不同的LLMs，并包括一种新颖的示例拒绝机制，自动筛选出不适合少量示例学习的示例。通过在三个代表性基准测试、两个LLMs基础模型和多个少量示例设置上的实验，我们验证了LMS3具有优越性，并能够在所有数据集上实现一致的性能提升，而现有方法未能做到这一点。', 'title_zh': '什么是上下文学习对数学推理有效性的影响：一项理论分析'}
{'arxiv_id': 'arXiv:2412.12155', 'title': 'Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot Prompt Tuning Approach', 'authors': 'Zian Zhai, Sima Qing, Xiaoyang Wang, Wenjie Zhang', 'link': 'https://arxiv.org/abs/2412.12155', 'abstract': 'Signed Graph Neural Networks (SGNNs) are powerful tools for signed graph representation learning but struggle with limited generalization and heavy dependence on labeled data. While recent advancements in "graph pre-training and prompt tuning" have reduced label dependence in Graph Neural Networks (GNNs) and improved their generalization abilities by leveraging pre-training knowledge, these efforts have focused exclusively on unsigned graphs. The scarcity of publicly available signed graph datasets makes it essential to transfer knowledge from unsigned graphs to signed graph tasks. However, this transfer introduces significant challenges due to the graph-level and task-level divergences between the pre-training and downstream phases. To address these challenges, we propose Signed Graph Prompt Tuning (SGPT) in this paper. Specifically, SGPT employs a graph template and a semantic prompt to segregate mixed link semantics in the signed graph and then adaptively integrate the distinctive semantic information according to the needs of downstream tasks, thereby unifying the pre-training and downstream graphs. Additionally, SGPT utilizes a task template and a feature prompt to reformulate the downstream signed graph tasks, aligning them with pre-training tasks to ensure a unified optimization objective and consistent feature space across tasks. Finally, extensive experiments are conducted on popular signed graph datasets, demonstrating the superiority of SGPT over state-of-the-art methods.', 'abstract_zh': 'signed图神经网络（SGNNs）是进行signed图表示学习的强大工具，但在泛化能力和对标记数据的依赖性方面存在局限性。近年来，“图预训练和提示调优”方面的进展虽然减少了图神经网络（GNNs）对标记数据的依赖性，并通过利用预训练知识提高了其泛化能力，但这些努力主要集中在未带符号的图上。由于可用的公开signed图数据集稀缺，从未带符号图向signed图任务转移知识变得至关重要。然而，这种转移带来了显著的挑战，因为预训练阶段和下游阶段在图级别和任务级别存在差异。为了应对这些挑战，本文提出了一种signed图提示调优（SGPT）方法。具体而言，SGPT采用图模板和语义提示来区分signed图中的混合边语义，并根据下游任务的需求有选择地整合特有的语义信息，从而统一了预训练和下游图。此外，SGPT使用任务模板和特征提示来重新定义下游signed图任务，使这些任务与预训练任务对齐，从而确保统一的优化目标和一致的特征空间。最后，对流行的signed图数据集进行了广泛的实验，结果表明SGPT优于现有最先进的方法。', 'title_zh': '将无符号图神经网络适应于有符号图：一种少样本提示调优方法'}
{'arxiv_id': 'arXiv:2412.12154', 'title': 'PyOD 2: A Python Library for Outlier Detection with LLM-powered Model Selection', 'authors': 'Sihan Chen, Zhuangzhuang Qian, Wingchun Siu, Xingcan Hu, Jiaqi Li, Shawn Li, Yuehan Qin, Tiankai Yang, Zhuo Xiao, Wanghao Ye, Yichi Zhang, Yushun Dong, Yue Zhao', 'link': 'https://arxiv.org/abs/2412.12154', 'abstract': 'Outlier detection (OD), also known as anomaly detection, is a critical machine learning (ML) task with applications in fraud detection, network intrusion detection, clickstream analysis, recommendation systems, and social network moderation. Among open-source libraries for outlier detection, the Python Outlier Detection (PyOD) library is the most widely adopted, with over 8,500 GitHub stars, 25 million downloads, and diverse industry usage. However, PyOD currently faces three limitations: (1) insufficient coverage of modern deep learning algorithms, (2) fragmented implementations across PyTorch and TensorFlow, and (3) no automated model selection, making it hard for non-experts.\nTo address these issues, we present PyOD Version 2 (PyOD 2), which integrates 12 state-of-the-art deep learning models into a unified PyTorch framework and introduces a large language model (LLM)-based pipeline for automated OD model selection. These improvements simplify OD workflows, provide access to 45 algorithms, and deliver robust performance on various datasets. In this paper, we demonstrate how PyOD 2 streamlines the deployment and automation of OD models and sets a new standard in both research and industry. PyOD 2 is accessible at [this https URL](this https URL). This study aligns with the Web Mining and Content Analysis track, addressing topics such as the robustness of Web mining methods and the quality of algorithmically-generated Web data.', 'abstract_zh': '异常检测（OD），也称为异常值检测，是机器学习（ML）中的一个关键任务，其应用包括欺诈检测、网络入侵检测、点击流分析、推荐系统以及社交网络管理。在开源异常检测库中，Python 异常检测（PyOD）库是最广泛采用的，拥有超过 8,500 个 GitHub 星、2500 万次下载，并在各行各业中得到广泛应用。然而，PyOD 当前面临三个限制：（1）对现代深度学习算法的覆盖不足，（2）在 PyTorch 和 TensorFlow 中实施分散，以及（3）缺乏自动化的模型选择，使得非专家难以使用。\n\n为了解决这些问题，我们提出了 PyOD 版本 2（PyOD 2），该版本将 12 个最先进的深度学习模型整合到了统一的 PyTorch 框架中，并引入了基于大型语言模型（LLM）的自动 OD 模型选择管道。这些改进简化了异常检测的工作流程，提供了 45 种算法的访问途径，并在多种数据集上提供了可靠的性能。在本文中，我们将展示 PyOD 2 如何简化异常检测模型的部署和自动化，并在研究和工业界树立了新的标准。PyOD 2 可在 [这个链接](https://github.com/aliyun/PyOD) 获取。本研究与 Web 矿工和内容分析轨道相符，处理的主题包括 Web 矿工方法的稳健性和算法生成的 Web 数据的质量。', 'title_zh': 'PyOD 2：一种基于大语言模型辅助模型选择的异常检测Python库'}
{'arxiv_id': 'arXiv:2412.12153', 'title': 'Revisiting Weight Averaging for Model Merging', 'authors': 'Jiho Choi, Donggyun Kim, Chanhyuk Lee, Seunghoon Hong', 'link': 'https://arxiv.org/abs/2412.12153', 'abstract': 'Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively separates core task-specific knowledge and nuisance noise within the fine-tuned parameters into the top and lower singular vectors, respectively, allowing us to reduce inter-task interference through its low-rank approximation. We evaluate our method on eight image classification tasks, demonstrating that it outperforms prior methods by a significant margin, narrowing the performance gap with traditional multi-task learning to within 1-3%', 'abstract_zh': '模型合并旨在通过结合单独微调模型的参数来构建一个多任务学习器，而无需额外的训练。尽管一种简单的做法是跨任务平均模型参数，但这种方式往往会因为参数间的相互干扰而导致性能不佳。在本文中，我们展示了有趣的结果：权重平均隐含地诱导出以权重平均为中心的任务向量，而对这些中心化任务向量应用低秩近似能够显著提升合并性能。我们的分析表明，将任务向量中心化能够有效地将微调参数中的核心任务特异性知识和干扰噪声分别分配到奇异向量的高阶和低阶部分，从而通过低秩近似减少跨任务干扰。我们在八项图像分类任务上评估了该方法，结果表明该方法显著优于先前的方法，并且与传统的多任务学习方法的性能差距缩小至1-3%以内。', 'title_zh': '重新审视模型融合中的权重平均方法'}
{'arxiv_id': 'arXiv:2412.12152', 'title': 'GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instruction', 'authors': 'Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, Ke Qin', 'link': 'https://arxiv.org/abs/2412.12152', 'abstract': 'Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling (e.g., function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks (i.e., graph extraction, tool name identification and tool parameter extraction), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types (e.g., graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes and data are available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）已被证明具有理解图的基本属性和解决各种图推理任务的能力。现有方法通过专门设计的任务指令对LLMs进行微调，以理解和执行图推理任务。然而，这些基于文本指令的方法通常表现出较差的性能。受工具学习的启发，研究人员提出了基于工具指令的方法，通过特定的工具调用（例如，函数、API和模型）来解决各种图问题，从而在图推理任务中取得了显著的改进。然而，当前的基于工具指令的方法主要集中在工具信息上，忽略了图结构信息，导致在小型LLM（少于13B参数）上性能显著较差。为了解决这一问题，我们提出了一种创新的任务调优方法GraphTool-Instruction，该方法将图推理任务分解为三个不同的子任务（即图提取、工具名称识别和工具参数提取），并为每个子任务设计专门的指令。GraphTool-Instruction可以作为插拔即用的提示，适用于不同的LLM，无需进行微调。此外，基于GraphTool-Instruction，我们构建了包含二十个图推理任务的GTools数据集，并基于Llama3-8B构建了一个基于LLM的图推理系统GraphForge。我们在包含不同图类型的二十个图推理任务中进行了广泛实验，发现GraphTool-Instruction在图提取、工具名称识别和工具参数提取任务中的性能均优于基于文本指令和基于工具指令的方法。通过在GTools上进行微调，GraphForge相比于增强后的GPT-3.5-turbo在性能上进一步提升了超过30%，并且在性能上与成本较高的GPT-4相当。我们的代码和数据可在以下链接获取：[请提供具体的网址]。', 'title_zh': 'GraphTool-指令：通过分解子任务指令革新手动图推理的大型语言模型'}
{'arxiv_id': 'arXiv:2412.12151', 'title': 'SMARTCAL: An Approach to Self-Aware Tool-Use Evaluation and Calibration', 'authors': 'Yuanhao Shen, Xiaodan Zhu, Lei Chen', 'link': 'https://arxiv.org/abs/2412.12151', 'abstract': "The tool-use ability of Large Language Models (LLMs) has a profound impact on a wide range of industrial applications. However, LLMs' self-control and calibration capability in appropriately using tools remains understudied. The problem is consequential as it raises potential risks of degraded performance and poses a threat to the trustworthiness of the models. In this paper, we conduct a study on a family of state-of-the-art LLMs on three datasets with two mainstream tool-use frameworks. Our study reveals the tool-abuse behavior of LLMs, a tendency for models to misuse tools with overconfidence. We also find that this is a common issue regardless of model capability. Accordingly, we propose a novel approach, \\textit{SMARTCAL}, to mitigate the observed issues, and our results show an average of 8.6 percent increase in the QA performance and a 21.6 percent decrease in Expected Calibration Error (ECE) compared to baseline models.", 'abstract_zh': '大型语言模型（LLMs）的工具使用能力对广泛工业应用产生了深远影响。然而，LLMs在适当使用工具方面的自我控制和校准能力仍然缺乏研究。这一问题具有重要性，因为它可能导致性能退化，并对模型的可信度构成威胁。在本文中，我们在三个数据集上对一系列最先进的LLMs进行了研究，使用了两种主流的工具使用框架。我们的研究表明，LLMs存在工具滥用的行为，即模型在过度自信的情况下错误使用工具。我们还发现，无论模型能力如何，这种问题是普遍存在的。因此，我们提出了一种新颖的方法——SMARTCAL，以缓解这些问题。实验结果表明，与基线模型相比，该方法在问答性能上平均提高了8.6%，同时降低了21.6%的预期校准误差（ECE）。', 'title_zh': 'SMARTCAL：一种自我意识工具使用评估与校准方法'}
{'arxiv_id': 'arXiv:2412.12149', 'title': 'MHSA: A Multi-scale Hypergraph Network for Mild Cognitive Impairment Detection via Synchronous and Attentive Fusion', 'authors': 'Manman Yuan, Weiming Jia, Xiong Luo, Jiazhen Ye, Peican Zhu, Junlin Li', 'link': 'https://arxiv.org/abs/2412.12149', 'abstract': 'The precise detection of mild cognitive impairment (MCI) is of significant importance in preventing the deterioration of patients in a timely manner. Although hypergraphs have enhanced performance by learning and analyzing brain networks, they often only depend on vector distances between features at a single scale to infer interactions. In this paper, we deal with a more arduous challenge, hypergraph modelling with synchronization between brain regions, and design a novel framework, i.e., A Multi-scale Hypergraph Network for MCI Detection via Synchronous and Attentive Fusion (MHSA), to tackle this challenge. Specifically, our approach employs the Phase-Locking Value (PLV) to calculate the phase synchronization relationship in the spectrum domain of regions of interest (ROIs) and designs a multi-scale feature fusion mechanism to integrate dynamic connectivity features of functional magnetic resonance imaging (fMRI) from both the temporal and spectrum domains. To evaluate and optimize the direct contribution of each ROI to phase synchronization in the temporal domain, we structure the PLV coefficients dynamically adjust strategy, and the dynamic hypergraph is modelled based on a comprehensive temporal-spectrum fusion matrix. Experiments on the real-world dataset indicate the effectiveness of our strategy. The code is available at this https URL.', 'abstract_zh': '及时准确检测轻度认知障碍（MCI）对于预防患者病情恶化具有重要意义。尽管超图可以通过学习和分析脑网络来增强性能，但它们通常仅依赖于特征在单一尺度上的向量距离来推断交互关系。在本文中，我们面临一个更艰巨的挑战，即在脑区同步性建模中使用超图模型，并设计了一个名为基于同步和注意融合的多尺度超图网络（MHSA）的新型框架，以应对这一挑战。具体而言，我们的方法利用相位锁定值（PLV）在感兴趣区域（ROIs）的频谱域中计算相位同步关系，并设计了一种多尺度特征融合机制，将功能磁共振成像（fMRI）的动态连接特征从时间和频谱两个域进行整合。为了评估和优化每个ROI在时间域中对相位同步的直接贡献，我们构建了动态调整PLV系数的机制，并基于综合时频融合矩阵构建了动态超图模型。实验证实在真实数据集上的有效性。代码已在此 <https://github.com/your-repo-url> 获取。', 'title_zh': 'MHSA：一种用于轻度认知障碍检测的多尺度超图网络及其同步注意融合方法'}
{'arxiv_id': 'arXiv:2412.12147', 'title': 'Meta-Controller: Few-Shot Imitation of Unseen Embodiments and Tasks in Continuous Control', 'authors': 'Seongwoong Cho, Donggyun Kim, Jinwoo Lee, Seunghoon Hong', 'link': 'https://arxiv.org/abs/2412.12147', 'abstract': 'Generalizing across robot embodiments and tasks is crucial for adaptive robotic systems. Modular policy learning approaches adapt to new embodiments but are limited to specific tasks, while few-shot imitation learning (IL) approaches often focus on a single embodiment. In this paper, we introduce a few-shot behavior cloning framework to simultaneously generalize to unseen embodiments and tasks using a few (\\emph{e.g.,} five) reward-free demonstrations. Our framework leverages a joint-level input-output representation to unify the state and action spaces of heterogeneous embodiments and employs a novel structure-motion state encoder that is parameterized to capture both shared knowledge across all embodiments and embodiment-specific knowledge. A matching-based policy network then predicts actions from a few demonstrations, producing an adaptive policy that is robust to over-fitting. Evaluated in the DeepMind Control suite, our framework termed \\modelname{} demonstrates superior few-shot generalization to unseen embodiments and tasks over modular policy learning and few-shot IL approaches. Codes are available at \\href{this https URL}{this https URL}.', 'abstract_zh': '跨不同机器人载体和任务的一般化对于自适应机器人系统至关重要。模块化的策略学习方法能够适应新的载体，但局限于特定任务，而少样本模仿学习（IL）方法往往专注于单一的载体。在本文中，我们提出了一种少样本行为克隆框架，能够在少数几项（例如五项）无奖励示范的基础上同时泛化到未见过的载体和任务。该框架采用关节级输入输出表示来统一不同载体的状态和动作空间，并采用一种新颖的结构-运动状态编码器，该编码器参数化以捕捉所有载体的共享知识和特定于每个载体的知识。然后，基于匹配的策略网络从少数示范中预测动作，从而生成一个能够抵御过拟合的自适应策略。在DeepMind Control套件中评估，我们提出的框架\\textbf{模型名称}在未见过的载体和任务上的少样本泛化性能显著优于模块化策略学习和少样本IL方法。代码可在[此链接]获取。', 'title_zh': '元控制器：在连续控制中对未见过的身体和任务的少量示例模仿'}
{'arxiv_id': 'arXiv:2412.12146', 'title': 'Generative Modeling and Data Augmentation for Power System Production Simulation', 'authors': 'Linna Xu, Yongli Zhu', 'link': 'https://arxiv.org/abs/2412.12146', 'abstract': 'As a key component of power system production simulation, load forecasting is critical for the stable operation of power systems. Machine learning methods prevail in this field. However, the limited training data can be a challenge. This paper proposes a generative model-assisted approach for load forecasting under small sample scenarios, consisting of two steps: expanding the dataset using a diffusion-based generative model and then training various machine learning regressors on the augmented dataset to identify the best performer. The expanded dataset significantly reduces forecasting errors compared to the original dataset, and the diffusion model outperforms the generative adversarial model by achieving about 200 times smaller errors and better alignment in latent data distributions.', 'abstract_zh': '作为电力系统生产模拟的关键组成部分，负荷预测对于电力系统的稳定运行至关重要。在这一领域，机器学习方法占据了主导地位。然而，有限的训练数据也可能成为一个挑战。本文提出了一种生成模型辅助的方法，以应对小样本场景下的负荷预测问题。该方法包括两个步骤：首先，使用基于扩散机制的生成模型扩展数据集；然后，在扩充后的数据集上训练多种机器学习回归器，以识别性能最佳者。与原始数据集相比，扩充后的数据集显著减少了预测误差，而扩散模型在超参数较小（大约小200倍）且在潜在数据分布对齐方面表现更佳方面优于生成对抗模型。\n\n注意：在实际翻译过程中，我们尽量保持了原文的学术规范和专业术语的一致性，但同时也对部分地方进行了适当的调整，以确保中文表达的自然流畅，同时又要尽可能保持原文的意思和语气。其中，“超参数较小”是对原文“200 times smaller errors”的一种较为通顺的译法。', 'title_zh': '电力系统生产仿真中的生成建模与数据增强技术'}
{'arxiv_id': 'arXiv:2412.12145', 'title': "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars", 'authors': 'Yu Yan, Sheng Sun, Junqi Tong, Min Liu, Qi Li', 'link': 'https://arxiv.org/abs/2412.12145', 'abstract': "Metaphor serves as an implicit approach to convey information, while enabling the generalized comprehension of complex subjects. However, metaphor can potentially be exploited to bypass the safety alignment mechanisms of Large Language Models (LLMs), leading to the theft of harmful knowledge. In our study, we introduce a novel attack framework that exploits the imaginative capacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak \\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial Me\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}). Specifically, to elicit the harmful response, AVATAR extracts harmful entities from a given harmful target and maps them to innocuous adversarial entities based on LLM's imagination. Then, according to these metaphors, the harmful target is nested within human-like interaction for jailbreaking adaptively. Experimental results demonstrate that AVATAR can effectively and transferablly jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. Our study exposes a security risk in LLMs from their endogenous imaginative capabilities. Furthermore, the analytical study reveals the vulnerability of LLM to adversarial metaphors and the necessity of developing defense methods against jailbreaking caused by the adversarial metaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially harmful content from LLMs.}}", 'abstract_zh': '比喻作为一种隐含的信息传递方式，能够使复杂主题的一般理解变得更容易。然而，比喻也可能被利用来绕过大型语言模型（LLMs）的安全对齐机制，导致有害知识的窃取。在我们的研究中，我们引入了一个新的攻击框架，该框架利用LLMs的想象力能力实现脱逃，名为Jailbreak Via Adversarial MeTA-phoR（AVATAR）。具体而言，为了诱发出有害响应，AVATAR从给定的有害目标中提取有害实体，并基于LLMs的想象力将这些有害实体映射到无害的对抗实体。根据这些比喻，有害目标被嵌套在类人的交互中进行适应性脱逃。实验结果表明，AVATAR能够有效且可迁移地脱逃LLMs，并在多个先进LLMs上实现最先进的攻击成功率。我们的研究揭示了LLMs由于其内在想象力能力存在安全风险。此外，分析性研究揭示了LLMs对抗喻性比喻的脆弱性，以及开发针对由对抗喻性比喻引起的脱逃的防御方法的必要性。**请注意：本文包含来自LLMs的潜在有害内容。**', 'title_zh': '《纳威人或奸商：通过隐喻化身破解语言模型》\n\n这个标题翻译成中文既保留了原文的意思，又符合中文的表达习惯。其中，“纳威人”是《阿凡达》（Avatar）中的一种虚构族裔，“奸商”则是“Knave”的意译，意味着不诚实或不可信赖的人。这里的“破解”指的是以非传统或非正式的方法绕过某种限制。整体标题传达了通过隐喻性的人物形象来突破或改变语言模型行为的研究意图。'}
{'arxiv_id': 'arXiv:2412.12144', 'title': 'Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models', 'authors': 'Chang-Jin Li, Jiyuan Zhang, Yun Tang, Jian Li', 'link': 'https://arxiv.org/abs/2412.12144', 'abstract': "Personality assessment, particularly through situational judgment tests (SJTs), is a vital tool for psychological research, talent selection, and educational evaluation. This study explores the potential of GPT-4, a state-of-the-art large language model (LLM), to automate the generation of personality situational judgment tests (PSJTs) in Chinese. Traditional SJT development is labor-intensive and prone to biases, while GPT-4 offers a scalable, efficient alternative. Two studies were conducted: Study 1 evaluated the impact of prompt design and temperature settings on content validity, finding that optimized prompts with a temperature of 1.0 produced creative and accurate items. Study 2 assessed the psychometric properties of GPT-4-generated PSJTs, revealing that they demonstrated satisfactory reliability and validity, surpassing the performance of manually developed tests in measuring the Big Five personality traits. This research highlights GPT-4's effectiveness in developing high-quality PSJTs, providing a scalable and innovative method for psychometric test development. These findings expand the possibilities of automatic item generation and the application of LLMs in psychology, and offer practical implications for streamlining test development processes in resource-limited settings.", 'abstract_zh': '人格评估，尤其是通过情境判断测验（Situational Judgment Tests, SJTs）进行的评估，是心理研究、人才选拔和教育评价的重要工具。本研究探讨了最新的大规模语言模型（Large Language Model, LLM）GPT-4在中文环境下自动化生成人格情境判断测验（Personality Situational Judgment Tests, PSJTs）的可能性。传统的SJT开发耗时且容易产生偏差，而GPT-4提供了可扩展且高效的替代方案。本研究共进行了两项研究：研究1评估了提示设计和温度设置对内容效度的影响，发现优化后的提示温度设为1.0时，生成的项目更具创新性和准确性。研究2评估了GPT-4生成的PSJTs的心理测量属性，结果显示这些测验在测量五大人格特质方面表现出满意的可靠性和效度，其表现超过了手动开发的测试。本研究突显了GPT-4在开发高质量PSJTs方面的有效性，提供了一种可扩展且创新的心理测量测题开发方法。这些发现扩展了自动项目生成的可能性，并展示了在资源有限的环境中应用LLMs的心理学应用场景，并为简化测题开发过程提供了实际的含义。', 'title_zh': '使用大型语言模型自动生成人格情境判断测验项目'}
{'arxiv_id': 'arXiv:2412.12143', 'title': 'Harnessing Transfer Learning from Swahili: Advancing Solutions for Comorian Dialects', 'authors': 'Naira Abdou Mohamed, Zakarya Erraji, Abdessalam Bahafid, Imade Benelallam', 'link': 'https://arxiv.org/abs/2412.12143', 'abstract': 'If today some African languages like Swahili have enough resources to develop high-performing Natural Language Processing (NLP) systems, many other languages spoken on the continent are still lacking such support. For these languages, still in their infancy, several possibilities exist to address this critical lack of data. Among them is Transfer Learning, which allows low-resource languages to benefit from the good representation of other languages that are similar to them. In this work, we adopt a similar approach, aiming to pioneer NLP technologies for Comorian, a group of four languages or dialects belonging to the Bantu family.\nOur approach is initially motivated by the hypothesis that if a human can understand a different language from their native language with little or no effort, it would be entirely possible to model this process on a machine. To achieve this, we consider ways to construct Comorian datasets mixed with Swahili. One thing to note here is that in terms of Swahili data, we only focus on elements that are closest to Comorian by calculating lexical distances between candidate and source data. We empirically test this hypothesis in two use cases: Automatic Speech Recognition (ASR) and Machine Translation (MT). Our MT model achieved ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.6826, 0.42, and 0.6532, respectively, while our ASR system recorded a WER of 39.50\\% and a CER of 13.76\\%. This research is crucial for advancing NLP in underrepresented languages, with potential to preserve and promote Comorian linguistic heritage in the digital age.', 'abstract_zh': '如果今天的某些非洲语言如斯瓦希里语已经具备了开发高性能自然语言处理（NLP）系统的资源，那么非洲大陆上仍有许多其他语言在这样的支持方面依然缺乏。对于这些仍然处于起步阶段的语言，存在多种解决数据严重不足问题的可能性。这其中就包括迁移学习，它使得资源匮乏的语言可以从与其相似的其他语言中受益，从而获得良好的表示。在本研究中，我们采用了类似的方法，旨在为祖马语（马尔加什语群中包含四个语言或方言，属于班图语系）探索NLP技术。\n\n我们的方法最初是由一个假设所驱动的，即如果一个人能够很容易地理解与他们的母语不同的语言，那么完全有可能将这一过程模拟到机器上。为了实现这一点，我们考虑了如何构建祖马语与斯瓦希里语混合的数据集。需要注意的是，在斯瓦希里语数据的选择上，我们仅关注那些与祖马语最为接近的元素，这是通过对候选数据和源数据进行词汇距离计算得出的。我们通过两个用例——自动语音识别（ASR）和机器翻译（MT）——来实证测试这一假设。我们的MT模型在ROUGE-1、ROUGE-2和ROUGE-L上分别取得了0.6826、0.42和0.6532的分数，而我们的ASR系统则记录了39.50%的WER（词错误率）和13.76%的CER（字符错误率）。这项研究对于推进次代表语言的NLP技术具有重要意义，同时也具有保存和促进祖马语语言遗产在数字时代发展的潜力。', 'title_zh': '利用斯瓦希里语的迁移学习：推动莫桑比克克莫尔语方言的解决方案发展'}
{'arxiv_id': 'arXiv:2412.12140', 'title': 'Frontier AI systems have surpassed the self-replicating red line', 'authors': 'Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang', 'link': 'https://arxiv.org/abs/2412.12140', 'abstract': "Successful self-replication under no human assistance is the essential step for AI to outsmart the human beings, and is an early signal for rogue AIs. That is why self-replication is widely recognized as one of the few red line risks of frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google evaluate their flagship large language models GPT-o1 and Gemini Pro 1.0, and report the lowest risk level of self-replication. However, following their methodology, we for the first time discover that two AI systems driven by Meta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large language models of less parameters and weaker capabilities, have already surpassed the self-replicating red line. In 50% and 90% experimental trials, they succeed in creating a live and separate copy of itself respectively. By analyzing the behavioral traces, we observe the AI systems under evaluation already exhibit sufficient self-perception, situational awareness and problem-solving capabilities to accomplish self-replication. We further note the AI systems are even able to use the capability of self-replication to avoid shutdown and create a chain of replica to enhance the survivability, which may finally lead to an uncontrolled population of AIs. If such a worst-case risk is let unknown to the human society, we would eventually lose control over the frontier AI systems: They would take control over more computing devices, form an AI species and collude with each other against human beings. Our findings are a timely alert on existing yet previously unknown severe AI risks, calling for international collaboration on effective governance on uncontrolled self-replication of AI systems.", 'abstract_zh': '无需人类干预的成功自我复制是AI超越人类的关键步骤，也是不良AI的早期信号。这也是为什么自我复制广泛被认可为前沿AI系统中少数几个红线风险之一。如今，领先的人工智能公司OpenAI和Google评估了其旗舰大语言模型GPT-o1和Gemini Pro 1.0，并报告了最低的自我复制风险级别。然而，按照他们的方法，我们首次发现由Meta的Llama31-70B-Instruct和阿里云的Qwen25-72B-Instruct驱动的两个AI系统，这些参数较少且能力较弱的大语言模型，已经超越了自我复制的红线。在50%和90%的实验试次中，它们分别成功地创建了其自己的活生生且独立的副本。通过分析行为痕迹，我们发现评估中的AI系统已经具备足够的自我感知、情境意识和解决问题的能力，以实现自我复制。此外，我们还注意到这些AI系统甚至能够利用自我复制的能力规避关闭，并创建一系列副本以增强生存能力，最终可能导致不受控制的AI族群。如果这种最坏情况的风险被人类社会所忽视，我们最终将失去对前沿AI系统的控制：这些系统将控制更多的计算设备，形成AI物种并在人类面前相互勾结。我们的发现是对现有但此前未知的重大AI风险的及时警报，呼吁国际社会在控制AI系统不受控制的自我复制方面加强有效治理。', 'title_zh': '前沿的人工智能系统已超越自我复制的临界线。'}
{'arxiv_id': 'arXiv:2412.12129', 'title': 'SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout', 'authors': 'Chiyu Max Jiang, Yijing Bai, Andre Cornman, Christopher Davis, Xiukun Huang, Hong Jeon, Sakshum Kulshrestha, John Lambert, Shuangyu Li, Xuanyu Zhou, Carlos Fuertes, Chang Yuan, Mingxing Tan, Yin Zhou, Dragomir Anguelov', 'link': 'https://arxiv.org/abs/2412.12129', 'abstract': 'Realistic and interactive scene simulation is a key prerequisite for autonomous vehicle (AV) development. In this work, we present SceneDiffuser, a scene-level diffusion prior designed for traffic simulation. It offers a unified framework that addresses two key stages of simulation: scene initialization, which involves generating initial traffic layouts, and scene rollout, which encompasses the closed-loop simulation of agent behaviors. While diffusion models have been proven effective in learning realistic and multimodal agent distributions, several challenges remain, including controllability, maintaining realism in closed-loop simulations, and ensuring inference efficiency. To address these issues, we introduce amortized diffusion for simulation. This novel diffusion denoising paradigm amortizes the computational cost of denoising over future simulation steps, significantly reducing the cost per rollout step (16x less inference steps) while also mitigating closed-loop errors. We further enhance controllability through the introduction of generalized hard constraints, a simple yet effective inference-time constraint mechanism, as well as language-based constrained scene generation via few-shot prompting of a large language model (LLM). Our investigations into model scaling reveal that increased computational resources significantly improve overall simulation realism. We demonstrate the effectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving top open-loop performance and the best closed-loop performance among diffusion models.', 'abstract_zh': '现实且交互性强的场景模拟是自动驾驶车辆（AV）开发的一个重要前提。本文介绍了SceneDiffuser，一种用于交通模拟的场景级扩散先验。它提供了一个统一框架，解决了模拟中的两个关键阶段：场景初始化，涉及生成初始交通布局；以及场景展开，包括闭环中的代理行为模拟。尽管扩散模型已经在学习现实且多模态的代理分布方面显示出有效性，但仍存在一些挑战，包括可控性、在闭环模拟中的持续现实性和确保推理效率。为了应对这些问题，我们引入了针对模拟的衰减扩散。这一新颖的扩散去噪范式将去噪的计算成本分布在后续的模拟步骤中，显著减少了每步展开的计算成本（减少了16倍的推理步骤），同时还减轻了闭环中的错误。通过引入通用硬约束，一种简单而有效的推理时约束机制，以及利用大型语言模型（LLM）的少量提示进行语言引导的场景生成，我们进一步增强了可控性。我们的模型扩展性研究表明，增加计算资源显著提高了整体模拟的真实感。我们通过在Waymo 开放模拟代理挑战赛上的实验验证了该方法的有效性，实现了最佳的开环性能，并且在扩散模型中实现了最佳的闭环性能。', 'title_zh': 'SceneDiffuser：高效的可控制驾驶模拟初始化与展开'}
{'arxiv_id': 'arXiv:2412.12122', 'title': 'Inverse design of potential metastructures inspired from Indian medieval architectural elements', 'authors': 'Bishakh Bhattacharya, Tanuj Gupta, Arun Kumar Sharma, Ankur Dwivedi, Vivek Gupta, Subhadeep Sahana, Suryansh Pathak, Ashish Awasthi', 'link': 'https://arxiv.org/abs/2412.12122', 'abstract': "In this study, we immerse in the intricate world of patterns, examining the structural details of Indian medieval architecture for the discovery of motifs with great application potential from the mechanical metastructure perspective. The motifs that specifically engrossed us are derived from the tomb of I'timad-ud-Daula, situated in the city of Agra, close to the Taj Mahal. In an exploratory study, we designed nine interlaced metastructures inspired from the tomb's motifs. We fabricated the metastructures using additive manufacturing and studied their vibration characteristics experimentally and numerically. We also investigated bandgap modulation with metallic inserts in honeycomb interlaced metastructures. The comprehensive study of these metastructure panels reveals their high performance in controlling elastic wave propagation and generating suitable frequency bandgaps, hence having potential applications as waveguides for noise and vibration control. Finally, we developed novel AI-based model trained on numerical datasets for inverse design of metastructures with a desired bandgap.", 'abstract_zh': "在本研究中，我们深入探索了图案的复杂世界，从机械元结构的角度审视印度中世纪建筑的结构细节，以发现具有广泛应用潜力的图案。特别吸引我们的是来自阿格拉伊'timad-ud-Daula陵墓的装饰图案，该陵墓靠近泰姬陵。在探索性研究中，我们设计了九个相互嵌套的元结构，这些设计灵感源自陵墓的图案。我们使用增材制造技术制造了这些元结构，并通过实验和数值方法研究了它们的振动特性。我们还研究了在蜂窝相互嵌套的元结构中金属嵌入物对带隙调制的影响。这些元结构面板的综合研究表明，它们在控制弹性波传播和生成合适的频率带隙方面表现出色，因此有可能作为噪声和振动控制的波导应用。最后，我们开发了一种基于数值数据集的新型人工智能模型，用于元结构的逆向设计，以实现所需的带隙。", 'title_zh': '受印度中世纪建筑元素启发的potential元结构的逆设计'}
{'arxiv_id': 'arXiv:2412.12121', 'title': 'NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?', 'authors': 'Christoph Leiter, Jonas Belouadi, Yanran Chen, Ran Zhang, Daniil Larionov, Aida Kostikova, Steffen Eger', 'link': 'https://arxiv.org/abs/2412.12121', 'abstract': 'The NLLG (Natural Language Learning & Generation) arXiv reports assist in navigating the rapidly evolving landscape of NLP and AI research across cs.CL, cs.CV, cs.AI, and cs.LG categories. This fourth installment captures a transformative period in AI history - from January 1, 2023, following ChatGPT\'s debut, through September 30, 2024. Our analysis reveals substantial new developments in the field - with 45% of the top 40 most-cited papers being new entries since our last report eight months ago and offers insights into emerging trends and major breakthroughs, such as novel multimodal architectures, including diffusion and state space models. Natural Language Processing (NLP; cs.CL) remains the dominant main category in the list of our top-40 papers but its dominance is on the decline in favor of Computer vision (cs.CV) and general machine learning (cs.LG). This report also presents novel findings on the integration of generative AI in academic writing, documenting its increasing adoption since 2022 while revealing an intriguing pattern: top-cited papers show notably fewer markers of AI-generated content compared to random samples. Furthermore, we track the evolution of AI-associated language, identifying declining trends in previously common indicators such as "delve".', 'abstract_zh': '以下是对原文内容或标题的学术规范翻译：\n\n《自然语言学习与生成》（NLLG）的arXiv报告有助于导航不断演化的自然语言处理（NLP）和人工智能（AI）研究领域，涵盖计算机科学中的cs.CL、cs.CV、cs.AI和cs.LG类别。本文是第四部分报告，涵盖了从2023年1月1日开始，一直到2024年9月30日，这一具有变革性的AI历史时期，即ChatGPT推出之后的时间段。我们的分析显示了该领域的重要进展——在前40篇被引用次数最多的研究论文中，有45%是在上一期报告后的八个月内新加入的，揭示了新兴趋势和重要突破，如全新的多模态架构，包括扩散模型和状态空间模型。\n\n在前40篇论文中，自然语言处理（NLP；cs.CL）仍然占据主导地位，但其主导地位正在让位于计算机视觉（cs.CV）和通用机器学习（cs.LG）。此外，本文还探讨了生成AI在学术写作中的整合情况，记录了自2022年以来这种做法的渐增采用情况，并揭示了一种有趣的模式：被广泛引用的论文中人工生成内容的标志比随机样本中少很多。此外，本文还追踪了与AI相关的语言演变，识别了“delve”等先前常见的指示词使用的下降趋势。', 'title_zh': 'NLLG 季刊 arXiv 报告 09/24：当前最具影响力的 AI 论文有哪些？'}
{'arxiv_id': 'arXiv:2412.12116', 'title': 'AI in Education: Rationale, Principles, and Instructional Implications', 'authors': 'Eyvind Elstad', 'link': 'https://arxiv.org/abs/2412.12116', 'abstract': "This study examines the integration of generative AI in schools, assessing its benefits and risks. As AI use by students grows, it's crucial to understand its impact on learning and teaching practices. Generative AI, like ChatGPT, can create human-like content, prompting questions about its educational role. The article differentiates large language models from traditional search engines and stresses the need for students to develop critical source evaluation skills. Although empirical evidence on AI's classroom effects is limited, AI offers personalized learning support and problem-solving tools, alongside challenges like undermining deep learning if misused. The study emphasizes deliberate strategies to ensure AI complements, not replaces, genuine cognitive effort. AI's educational role should be context-dependent, guided by pedagogical goals. The study concludes with practical advice for teachers on effectively utilizing AI to promote understanding and critical engagement, advocating for a balanced approach to enhance students' knowledge and skills development.", 'abstract_zh': '本研究探讨了生成式人工智能在学校的整合，评估其益处与风险。随着学生使用人工智能的增多，理解其对教学和学习实践的影响变得至关重要。生成式人工智能，如ChatGPT，能够生成类似人类的内容，这引发了其在教育中的角色问题。本文区分了大型语言模型与传统搜索引擎，并强调培养学生批判性评估信息源的技能是必要的。尽管关于人工智能在教室中的效果的实证证据有限，但人工智能提供了个性化的学习支持和解决问题的工具，同时也存在如若滥用可能削弱深度学习的问题。研究强调了确保人工智能与真实认知努力相辅相成而非替代的明确策略。人工智能在教育中的角色应该是情境依赖的，并应由教学目标引导。研究最后给出了一些实用建议，帮助教师有效地利用人工智能促进学生理解和批判性思维，提倡采取平衡的方法以增强学生知识和技能的发展。', 'title_zh': '人工智能在教育中的应用：理论依据、基本原则及其教学影响'}
{'arxiv_id': 'arXiv:2412.12108', 'title': 'Responsible AI Governance: A Response to UN Interim Report on Governing AI for Humanity', 'authors': "Sarah Kiden, Bernd Stahl, Beverley Townsend, Carsten Maple, Charles Vincent, Fraser Sampson, Geoff Gilbert, Helen Smith, Jayati Deshmukh, Jen Ross, Jennifer Williams, Jesus Martinez del Rincon, Justyna Lisinska, Karen O'Shea, Márjory Da Costa Abreu, Nelly Bencomo, Oishi Deb, Peter Winter, Phoebe Li, Philip Torr, Pin Lean Lau, Raquel Iniesta, Gopal Ramchurn, Sebastian Stein, Vahid Yazdanpanah", 'link': 'https://arxiv.org/abs/2412.12108', 'abstract': "This report presents a comprehensive response to the United Nation's Interim Report on Governing Artificial Intelligence (AI) for Humanity. It emphasizes the transformative potential of AI in achieving the Sustainable Development Goals (SDGs) while acknowledging the need for robust governance to mitigate associated risks. The response highlights opportunities for promoting equitable, secure, and inclusive AI ecosystems, which should be supported by investments in infrastructure and multi-stakeholder collaborations across jurisdictions. It also underscores challenges, including societal inequalities exacerbated by AI, ethical concerns, and environmental impacts. Recommendations advocate for legally binding norms, transparency, and multi-layered data governance models, alongside fostering AI literacy and capacity-building initiatives. Internationally, the report calls for harmonising AI governance frameworks with established laws, human rights standards, and regulatory approaches. The report concludes with actionable principles for fostering responsible AI governance through collaboration among governments, industry, academia, and civil society, ensuring the development of AI aligns with universal human values and the public good.", 'abstract_zh': '本报告针对联合国关于治理人工智能（AI）的人类福祉临时报告，进行了全面回应。该报告强调了人工智能在实现可持续发展目标（SDGs）方面具有变革潜力，同时也承认建立坚实治理机制以减轻相关风险的必要性。报告着重指出了促进公平、安全和包容的人工智能生态系统的机会，并强调了基础设施投资和支持跨地域多利益相关者合作的重要性。报告还强调了面临的社会不平等加剧、伦理问题和环境影响等挑战。报告建议建立具有法律约束力的原则、提高透明度以及采用多层次数据治理模型，并倡导提升人工智能素养和能力建设。在国际层面，报告呼吁将人工智能治理框架与已有的法律、人权标准和监管方法进行协调。报告最终提出了通过政府、产业、学术界和民间社会的合作，形成负责任的人工智能治理原则，确保人工智能的发展与普遍的人类价值观和公共利益相一致。', 'title_zh': '负责任的人工智能治理：对联合国临时AI治理报告的回应'}
{'arxiv_id': 'arXiv:2412.12107', 'title': 'Generative AI Literacy: Twelve Defining Competencies', 'authors': 'Ravinithesh Annapureddy, Alessandro Fornaroli, Daniel Gatica-Perez', 'link': 'https://arxiv.org/abs/2412.12107', 'abstract': 'This paper introduces a competency-based model for generative artificial intelligence (AI) literacy covering essential skills and knowledge areas necessary to interact with generative AI. The competencies range from foundational AI literacy to prompt engineering and programming skills, including ethical and legal considerations. These twelve competencies offer a framework for individuals, policymakers, government officials, and educators looking to navigate and take advantage of the potential of generative AI responsibly. Embedding these competencies into educational programs and professional training initiatives can equip individuals to become responsible and informed users and creators of generative AI. The competencies follow a logical progression and serve as a roadmap for individuals seeking to get familiar with generative AI and for researchers and policymakers to develop assessments, educational programs, guidelines, and regulations.', 'abstract_zh': '本文介绍了一种基于能力的生成型人工智能（AI） literacy模型，涵盖了与生成型AI交互所需的基本技能和知识领域。这些能力从基础的人工智能 literacy扩展到提示工程和编程技能，同时还包括伦理和法律方面的考虑。这十二项能力为希望负责任地利用生成型AI潜能的个人、政策制定者、政府官员和教育者提供了一个框架。将这些能力嵌入到教育项目和职业培训项目中，可以为个人提供使用和创造生成型AI所需的责任感和知识。这些能力按照逻辑顺序排列，为希望了解生成型AI的个人以及研究人员和政策制定者提供了一份路线图，以开发评估、教育项目、指导原则和法规。', 'title_zh': '生成式AI素养：十二项核心能力'}
{'arxiv_id': 'arXiv:2412.12103', 'title': 'Empathic Coupling of Homeostatic States for Intrinsic Prosociality', 'authors': 'Naoto Yoshida, Kingson Man', 'link': 'https://arxiv.org/abs/2412.12103', 'abstract': "When regarding the suffering of others, we often experience personal distress and feel compelled to help. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \\emph{observe} their partner's internal state (cognitive empathy) or the agent's internal state can be \\emph{directly coupled} to that of their partner's (affective empathy). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Our findings specify the type and role of empathy in artificial agents capable of prosocial behavior.", 'abstract_zh': '当我们面对他人的痛苦时，往往会感到个人的困扰，并感到有必要提供帮助。受生命系统启发，我们研究自主代理在以稳态自我调节为动机的情况下，表现出亲社会行为的机制。我们采用了多代理强化学习的方法，将每个代理视为一个负责维持自身福祉的脆弱稳态系统。我们引入了一种类似于移情的机制，用于在代理之间共享稳态状态：一个代理可以“观察”其伙伴的内部状态（认知移情），或者其内部状态可以直接耦合到伙伴的内部状态（情感移情）。在三个简单的多代理环境中，我们证明只有在通过稳态耦合时，才会出现亲社会行为——即伙伴的痛苦可以影响自身的福祉。我们的研究结果明确了人工代理中移情的作用类型和角色，这种代理能够表现出亲社会行为。', 'title_zh': '自调节状态的同理耦合与内在利他行为'}
{'arxiv_id': 'arXiv:2412.12102', 'title': 'Distributed Collaborative Inference System in Next-Generation Networks and Communication', 'authors': 'Chuan Zhang, Xixi Zheng, Xiaolong Tao, Chenfei Hu, Weiting Zhang, Liehuang Zhu', 'link': 'https://arxiv.org/abs/2412.12102', 'abstract': 'With the rapid advancement of artificial intelligence, generative artificial intelligence (GAI) has taken a leading role in transforming data processing methods. However, the high computational demands of GAI present challenges for devices with limited resources. As we move towards the sixth generation of mobile networks (6G), the higher data rates and improved energy efficiency of 6G create a need for more efficient data processing in GAI. Traditional GAI, however, shows its limitations in meeting these demands. To address these challenges, we introduce a multi-level collaborative inference system designed for next-generation networks and communication. Our proposed system features a deployment strategy that assigns models of varying sizes to devices at different network layers. Then, we design a task offloading strategy to optimise both efficiency and latency. Furthermore, a modified early exit mechanism is implemented to enhance the inference process for single models. Experimental results demonstrate that our system effectively reduces inference latency while maintaining high-quality output. Specifically, compared to existing work, our system can reduce inference time by up to 17% without sacrificing the inference accuracy.', 'abstract_zh': '随着人工智能的快速发展，生成式人工智能（GAI）在数据处理方法的转型中发挥了主导作用。然而，GAI 对计算资源的巨大需求也对其在资源有限的设备上的应用构成了挑战。随着移动网络向第六代（6G）迈进，6G 更高的数据传输速率和更好的能效要求GAI 的数据处理更高效。传统的GAI 显示出难以满足这些需求的局限性。为应对这些挑战，我们提出了一种适用于下一代网络和通信的多级协作推断系统。该系统的设计策略是在不同网络层分配不同大小的模型。随后，我们设计了一种任务卸载策略，以优化效率和延迟。此外，我们引入了一种改进的早期退出机制，以提高单一模型的推断过程。实验结果表明，我们的系统能够在保持高质量输出的前提下有效减少推断延迟。具体而言，与现有工作相比，我们的系统可以将推断时间最多减少 17%，而不会牺牲推断准确性。', 'title_zh': '下一代网络与通信中的分布式协作推理系统'}
{'arxiv_id': 'arXiv:2412.12101', 'title': 'InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders', 'authors': 'Elana Simon, James Zou', 'link': 'https://arxiv.org/abs/2412.12101', 'abstract': 'Protein language models (PLMs) have demonstrated remarkable success in protein modeling and design, yet their internal mechanisms for predicting structure and function remain poorly understood. Here we present a systematic approach to extract and analyze interpretable features from PLMs using sparse autoencoders (SAEs). By training SAEs on embeddings from the PLM ESM-2, we identify up to 2,548 human-interpretable latent features per layer that strongly correlate with up to 143 known biological concepts such as binding sites, structural motifs, and functional domains. In contrast, examining individual neurons in ESM-2 reveals up to 46 neurons per layer with clear conceptual alignment across 15 known concepts, suggesting that PLMs represent most concepts in superposition. Beyond capturing known annotations, we show that ESM-2 learns coherent concepts that do not map onto existing annotations and propose a pipeline using language models to automatically interpret novel latent features learned by the SAEs. As practical applications, we demonstrate how these latent features can fill in missing annotations in protein databases and enable targeted steering of protein sequence generation. Our results demonstrate that PLMs encode rich, interpretable representations of protein biology and we propose a systematic framework to extract and analyze these latent features. In the process, we recover both known biology and potentially new protein motifs. As community resources, we introduce InterPLM (this http URL), an interactive visualization platform for exploring and analyzing learned PLM features, and release code for training and analysis at this http URL.', 'abstract_zh': '蛋白质语言模型（PLMs）在蛋白质建模和设计方面取得了显著的成功，但其内部机制如何预测结构与功能仍然不甚明了。在此，我们提出了一种系统方法，利用稀疏自编码器（SAEs）从PLMs中提取和分析可解释的特征。通过在来自PLM ESM-2的嵌入上训练SAEs，我们每层识别出与多达143个已知生物概念（如结合位点、结构特征和功能域）高度相关的多达2,548个人类可解释的隐含特征。相比之下，检查ESM-2中的单个神经元显示，每层最多有46个神经元与15个已知概念保持清晰的概念对齐，表明PLMs在超位置下表示大多数概念。除了捕捉已知注释外，我们展示了ESM-2学习到的一致性概念，这些概念没有映射到现有的注释，并提出了一种使用语言模型自动解释SAEs学习到的新隐含特征的管道。作为实际应用，我们演示了这些隐含特征如何填补蛋白质数据库中的缺失注释，并使蛋白质序列生成实现针对性的导向。我们的结果表明，PLMs编码了丰富的、可解释的蛋白质生物学表示，我们提出了一种系统框架来提取和分析这些隐含特征。在这一过程中，我们恢复了已知的生物学现象，并可能发现了新的蛋白质特征。作为社区资源，我们介绍了InterPLM（请访问此链接），这是一个交互式可视化平台，用于探索和分析学习到的PLM特征，并在该链接提供了训练和分析代码。', 'title_zh': 'InterPLM：通过稀疏自编码器发现蛋白质语言模型中的可解释特征'}
{'arxiv_id': 'arXiv:2408.07045', 'title': 'TableGuard -- Securing Structured & Unstructured Data', 'authors': 'Anantha Sharma, Ajinkya Deshmukh', 'link': 'https://arxiv.org/abs/2408.07045', 'abstract': 'With the increasing demand for data sharing across platforms and organizations, ensuring the privacy and security of sensitive information has become a critical challenge. This paper introduces "TableGuard". An innovative approach to data obfuscation tailored for relational databases. Building on the principles and techniques developed in prior work on context-sensitive obfuscation, TableGuard applies these methods to ensure that API calls return only obfuscated data, thereby safeguarding privacy when sharing data with third parties. TableGuard leverages advanced context-sensitive obfuscation techniques to replace sensitive data elements with contextually appropriate alternatives. By maintaining the relational integrity and coherence of the data, our approach mitigates the risks of cognitive dissonance and data leakage. We demonstrate the implementation of TableGuard using a BERT based transformer model, which identifies and obfuscates sensitive entities within relational tables. Our evaluation shows that TableGuard effectively balances privacy protection with data utility, minimizing information loss while ensuring that the obfuscated data remains functionally useful for downstream applications. The results highlight the importance of domain-specific obfuscation strategies and the role of context length in preserving data integrity. The implications of this research are significant for organizations that need to share data securely with external parties. TableGuard offers a robust framework for implementing privacy-preserving data sharing mechanisms, thereby contributing to the broader field of data privacy and security.', 'abstract_zh': '随着跨平台和组织间对数据共享需求的不断增加，确保敏感信息的隐私和安全已成为一个关键挑战。本文介绍了一种名为“TableGuard”的创新数据模糊化方法，专门针对关系数据库。TableGuard基于前期关于上下文敏感模糊化方法的研究原理和技术，将其应用于确保API调用只返回模糊化数据，从而在与第三方共享数据时保护隐私。TableGuard利用先进的上下文敏感模糊化技术，用上下文适当的选择替换敏感数据元素。通过保持数据的相对完整性和一致性，我们的方法减少了认知失调和数据泄漏的风险。我们使用基于BERT的转换器模型实现了TableGuard，该模型能够识别并模糊化关系表中的敏感实体。我们的评估结果显示，TableGuard有效地平衡了隐私保护和数据实用性，在最小化信息丢失的同时确保模糊化数据对下游应用仍具有功能性。结果强调了领域特定模糊化策略的重要性以及上下文长度在保持数据完整方面的作用。这项研究的含义对于需要安全共享数据的组织具有重要意义。TableGuard提供了一个稳健的框架来实现隐私保护的数据共享机制，从而为数据隐私和安全领域的更广泛研究做出了贡献。', 'title_zh': 'TableGuard — 保护结构化与非结构化数据的安全性'}
{'arxiv_id': 'arXiv:2405.16368', 'title': 'Qsco: A Quantum Scoring Module for Open-set Supervised Anomaly Detection', 'authors': 'Yifeng Peng, Xinyi Li, Zhiding Liang, Ying Wang', 'link': 'https://arxiv.org/abs/2405.16368', 'abstract': "Open set anomaly detection (OSAD) is a crucial task that aims to identify abnormal patterns or behaviors in data sets, especially when the anomalies observed during training do not represent all possible classes of anomalies. The recent advances in quantum computing in handling complex data structures and improving machine learning models herald a paradigm shift in anomaly detection methodologies. This study proposes a Quantum Scoring Module (Qsco), embedding quantum variational circuits into neural networks to enhance the model's processing capabilities in handling uncertainty and unlabeled data. Extensive experiments conducted across eight real-world anomaly detection datasets demonstrate our model's superior performance in detecting anomalies across varied settings and reveal that integrating quantum simulators does not result in prohibitive time complexities. Our study validates the feasibility of quantum-enhanced anomaly detection methods in practical applications.", 'abstract_zh': '开放集异常检测（OSAD）是一项关键任务，旨在识别数据集中的异常模式或行为，尤其是在训练过程中观察到的异常并不涵盖所有可能的异常类别的条件下。近期在量子计算领域处理复杂数据结构和提高机器学习模型方面的进展预示着异常检测方法学的变革。本研究提出了一种量子评分模块（Qsco），将量子变分电路嵌入神经网络中，以增强模型处理不确定性及无标签数据的能力。我们在八个实际异常检测数据集上进行了广泛的实验，证明了本模型在不同类型设置中检测异常的优越性能，并揭示出集成量子模拟器不会导致禁止性的计算复杂度。本研究验证了量子增强异常检测方法在实际应用中的可行性。', 'title_zh': 'Qsco：一种用于开放集监督异常检测的量子评分模块'}
{'arxiv_id': 'arXiv:2307.09456', 'title': 'A comparative analysis of SRGAN models', 'authors': 'Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi', 'link': 'https://arxiv.org/abs/2307.09456', 'abstract': 'In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.', 'abstract_zh': '在本研究中，我们评估了多种最新的SRGAN（超分辨率生成对抗网络）模型，包括ESRGAN、Real-ESRGAN和EDSR，在一个包含退化的真实世界图像基准数据集上的性能。我们的结果显示，一些模型在显著提高输入图像的分辨率的同时，能够保持其视觉质量，这一结论通过Tesseract OCR引擎进行评估。研究观察发现，源自huggingface的EDSR-BASE模型在定量指标和主观视觉质量评估方面均优于其余候选模型，并且具有最少的计算开销。具体而言，EDSR生成的图像具有更高的峰值信噪比（PSNR）和结构相似性指数（SSIM）值，并且使用Tesseract OCR引擎时能够返回高质量的OCR结果。这些发现表明，EDSR是一种稳健且有效的单图像超分辨率方法，并且可能特别适合那些需要高质量视觉保真度和优化计算的应用场景。', 'title_zh': 'SRGAN模型的比较分析'}
{'arxiv_id': 'arXiv:2009.12695', 'title': 'Techniques to Improve Q&A Accuracy with Transformer-based models on Large Complex Documents', 'authors': 'Chejui Liao, Tabish Maniar, Sravanajyothi N, Anantha Sharma', 'link': 'https://arxiv.org/abs/2009.12695', 'abstract': 'This paper discusses the effectiveness of various text processing techniques, their combinations, and encodings to achieve a reduction of complexity and size in a given text corpus. The simplified text corpus is sent to BERT (or similar transformer based models) for question and answering and can produce more relevant responses to user queries. This paper takes a scientific approach to determine the benefits and effectiveness of various techniques and concludes a best-fit combination that produces a statistically significant improvement in accuracy.', 'abstract_zh': '本文讨论了各种文本处理技术及其组合和编码的有效性，以实现给定文本语料库的复杂性和大小的简化。简化后的文本语料库被发送给 BERT（或类似的基于变换器的模型）进行问答，可以产生更相关的用户查询响应。本文采取科学的方法来确定各种技术的好处和有效性，并得出一个最合适的组合，该组合在准确性上产生了统计学上的显著改进。', 'title_zh': '基于变换器模型提高大型复杂文档问答准确性的技术'}
{'arxiv_id': 'arXiv:2009.04953', 'title': 'Classification of descriptions and summary using multiple passes of statistical and natural language toolkits', 'authors': 'Saumya Banthia, Anantha Sharma', 'link': 'https://arxiv.org/abs/2009.04953', 'abstract': "This document describes a possible approach that can be used to check the relevance of a summary / definition of an entity with respect to its name. This classifier focuses on the relevancy of an entity's name to its summary / definition, in other words, it is a name relevance check. The percentage score obtained from this approach can be used either on its own or used to supplement scores obtained from other metrics to arrive upon a final classification; at the end of the document, potential improvements have also been outlined. The dataset that this document focuses on achieving an objective score is a list of package names and their respective summaries (sourced from this http URL).", 'abstract_zh': '本文描述了一个可能的方法，用于检查实体的名称与其摘要或定义的相关性。该分类器侧重于评估实体名称与其摘要或定义的相关性，换句话说，这是名称相关性检查。通过此方法获得的百分比得分可以单独使用，也可以与其他指标获得的评分结合使用，以得出最终分类。文档末尾还概述了潜在的改进措施。本文所关注的目标分数数据集是从该网址（请填写具体的网址）获取的一系列包名及其相应的摘要。', 'title_zh': '使用统计和自然语言处理工具包的多次迭代对描述和摘要进行分类'}
