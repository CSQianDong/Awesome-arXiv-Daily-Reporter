# AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark 

**Title (ZH)**: AIR-Bench: 自动化异构信息检索基准测试 

**Authors**: Jianlyu Chen, Nan Wang, Chaofan Li, Bo Wang, Shitao Xiao, Han Xiao, Hao Liao, Defu Lian, Zheng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.13102)  

**Abstract**: Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at this https URL. 

**Abstract (ZH)**: 评价在信息检索（IR）模型的发展中起着至关重要的作用。然而，目前基于预定义领域和人工标注数据的基准测试，无法在经济高效且高效地满足新兴领域评价需求方面提供足够的解决方案。为解决这一挑战，我们提出了一种自动异质信息检索基准测试（AIR-Bench）。

AIR-Bench具有三个关键特征：

1. 自动化。AIR-Bench中的测试数据由大型语言模型（LLMs）自动生成，无需人工干预。
2. 异质性。AIR-Bench中的测试数据涵盖了多样化的任务、领域和语言。
3. 动态性。AIR-Bench不断扩展其覆盖的领域和语言，以提供一个更加全面的社区开发者评价基准。

我们开发了一条可靠且健壮的数据生成管道，基于真实世界的语料库自动生成多样且高质量的评价数据集。我们的研究结果表明，AIR-Bench生成的测试数据与人工标注的测试数据高度一致，使AIR-Bench成为一个可靠的评价基准，用于评价IR模型。AIR-Bench的资源已在此处公开访问：[请提供具体的URL链接]。 

---
# AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark 

**Title (ZH)**: AIR-Bench: 自动化异构信息检索基准测试 

**Authors**: Jianlyu Chen, Nan Wang, Chaofan Li, Bo Wang, Shitao Xiao, Han Xiao, Hao Liao, Defu Lian, Zheng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.13102)  

**Abstract**: Evaluation plays a crucial role in the advancement of information retrieval (IR) models. However, current benchmarks, which are based on predefined domains and human-labeled data, face limitations in addressing evaluation needs for emerging domains both cost-effectively and efficiently. To address this challenge, we propose the Automated Heterogeneous Information Retrieval Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1) Automated. The testing data in AIR-Bench is automatically generated by large language models (LLMs) without human intervention. 2) Heterogeneous. The testing data in AIR-Bench is generated with respect to diverse tasks, domains and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are constantly augmented to provide an increasingly comprehensive evaluation benchmark for community developers. We develop a reliable and robust data generation pipeline to automatically create diverse and high-quality evaluation datasets based on real-world corpora. Our findings demonstrate that the generated testing data in AIR-Bench aligns well with human-labeled testing data, making AIR-Bench a dependable benchmark for evaluating IR models. The resources in AIR-Bench are publicly available at this https URL. 

**Abstract (ZH)**: 评价在信息检索（IR）模型的发展中起着至关重要的作用。然而，当前基于预定义领域和人工标注数据的基准在应对新兴领域评价需求时，存在成本高和效率低的局限性。为了解决这一挑战，我们提出了自动异构信息检索基准（AIR-Bench）。AIR-Bench 具有三个关键特征：1）自动。AIR-Bench 的测试数据由大型语言模型（LLM）自动生成，无需人工干预。2）异构。AIR-Bench 的测试数据涵盖了多样化的任务、领域和语言。3）动态。AIR-Bench 的覆盖领域和语言不断扩展，以提供日益全面的评价基准，供社区开发者使用。我们开发了一个可靠且稳健的数据生成流水线，基于实际语料库自动生成多样化的高质量评价数据集。研究结果显示，AIR-Bench 生成的测试数据与人工标注的测试数据高度一致，使 AIR-Bench 成为评价 IR 模型的可靠基准。AIR-Bench 的资源可以通过以下链接公开访问：[该 https URL]。 

---
# A Survey on Recommendation Unlearning: Fundamentals, Taxonomy, Evaluation, and Open Questions 

**Title (ZH)**: 推荐系统脱训练研究综述：基础、分类、评估及开放问题 

**Authors**: Yuyuan Li, Xiaohua Feng, Chaochao Chen, Qiang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12836)  

**Abstract**: Recommender systems have become increasingly influential in shaping user behavior and decision-making, highlighting their growing impact in various domains. Meanwhile, the widespread adoption of machine learning models in recommender systems has raised significant concerns regarding user privacy and security. As compliance with privacy regulations becomes more critical, there is a pressing need to address the issue of recommendation unlearning, i.e., eliminating the memory of specific training data from the learned recommendation models. Despite its importance, traditional machine unlearning methods are ill-suited for recommendation unlearning due to the unique challenges posed by collaborative interactions and model parameters. This survey offers a comprehensive review of the latest advancements in recommendation unlearning, exploring the design principles, challenges, and methodologies associated with this emerging field. We provide a unified taxonomy that categorizes different recommendation unlearning approaches, followed by a summary of widely used benchmarks and metrics for evaluation. By reviewing the current state of research, this survey aims to guide the development of more efficient, scalable, and robust recommendation unlearning techniques. Furthermore, we identify open research questions in this field, which could pave the way for future innovations not only in recommendation unlearning but also in a broader range of unlearning tasks across different machine learning applications. 

**Abstract (ZH)**: 推荐系统越来越能影响用户行为和决策，突显了它们在各个领域的日益重要影响。与此同时，推荐系统中广泛采用机器学习模型引发了关于用户隐私和安全的严重关切。随着遵守隐私法规变得越来越重要，迫切需要解决推荐遗忘的问题，即从已学习的推荐模型中消除特定训练数据的记忆。尽管这个问题非常重要，但传统的机器遗忘方法由于协作交互和模型参数的独特挑战并不适用于推荐遗忘。本文综述了推荐遗忘领域的最新进展，探讨了这一新兴领域的设计原则、挑战和方法论。我们提供了一个统一的分类体系，将不同的推荐遗忘方法进行分类，并总结了常用的基准和评估指标。通过回顾当前的研究状态，本文旨在指导开发更高效、可扩展和稳健的推荐遗忘技术。此外，我们还指出了该领域中的开放研究问题，这可能会为未来的创新铺平道路，不仅在推荐遗忘方面，还在不同机器学习应用的各种遗忘任务中。 

---
# RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service 

**Title (ZH)**: RemoteRAG：一种隐私保护的大规模语言模型云检索增强服务 

**Authors**: Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao  

**Link**: [PDF](https://arxiv.org/pdf/2412.12775)  

**Abstract**: Retrieval-augmented generation (RAG) improves the service quality of large language models by retrieving relevant documents from credible literature and integrating them into the context of the user query. Recently, the rise of the cloud RAG service has made it possible for users to query relevant documents conveniently. However, directly sending queries to the cloud brings potential privacy leakage. In this paper, we are the first to formally define the privacy-preserving cloud RAG service to protect the user query and propose RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage of the user query and the leakage inferred from relevant documents. For efficiency, we limit the search range from the total documents to a small number of selected documents related to a perturbed embedding generated from $(n,\epsilon)$-DistanceDP, so that computation and communication costs required for privacy protection significantly decrease. For accuracy, we ensure that the small range includes target documents related to the user query with detailed theoretical analysis. Experimental results also demonstrate that RemoteRAG can resist existing embedding inversion attack methods while achieving no loss in retrieval under various settings. Moreover, RemoteRAG is efficient, incurring only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$ GB with the non-optimized privacy-preserving scheme) when retrieving from a total of $10^6$ documents. 

**Abstract (ZH)**: 检索增强生成（RAG）通过从可靠文献中检索相关文档并将这些文档整合到用户查询的上下文中，从而提高了大型语言模型的服务质量。最近，云RAG服务的兴起使得用户能够方便地查询相关文档。然而，直接将查询发送到云中可能会带来潜在的隐私泄露问题。本文首次正式定义了保护用户查询隐私的云RAG服务，并提出了RemoteRAG作为针对隐私、效率和准确性问题的解决方案。为了保护隐私，我们引入了$(n,\epsilon)$-DistanceDP来表征用户查询及其从相关文档推断出的隐私泄露。为了提高效率，我们将搜索范围限制在由$(n,\epsilon)$-DistanceDP生成的扰动嵌入相关的少量选定文档之中，从而显著降低用于隐私保护的计算和通信成本。为了确保准确性，我们通过详细的理论分析确保少量文档范围内包含与用户查询相关的目标文档。实验结果还证实，RemoteRAG在多种设置下能够抵御现有嵌入反转攻击方法的同时，无需在检索性能上有所损失。此外，当从总共$10^6$个文档中检索时，RemoteRAG的效率仅为0.67秒和46.66KB的数据传输量（在未优化的隐私保护方案中，这一数据传输量分别为2.72小时和1.43GB）。 

---
# A Survey on Sequential Recommendation 

**Title (ZH)**: 对序列推荐的研究综述 

**Authors**: Liwei Pan, Weike Pan, Meiyan Wei, Hongzhi Yin, Zhong Ming  

**Link**: [PDF](https://arxiv.org/pdf/2412.12770)  

**Abstract**: Different from most conventional recommendation problems, sequential recommendation focuses on learning users' preferences by exploiting the internal order and dependency among the interacted items, which has received significant attention from both researchers and practitioners. In recent years, we have witnessed great progress and achievements in this field, necessitating a new survey. In this survey, we study the SR problem from a new perspective (i.e., the construction of an item's properties), and summarize the most recent techniques used in sequential recommendation such as pure ID-based SR, SR with side information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR and data-augmented SR. Moreover, we introduce some frontier research topics in sequential recommendation, e.g., open-domain SR, data-centric SR, could-edge collaborative SR, continuous SR, SR for good, and explainable SR. We believe that our survey could be served as a valuable roadmap for readers in this field. 

**Abstract (ZH)**: 与大多数传统的推荐问题不同，序列推荐关注于通过利用用户交互项目之间的内部顺序和依赖关系来学习用户的偏好，从而引起了研究者和实践者的广泛关注。近年来，该领域的进展和成就十分显著，需要进行新的综述。在本文综述中，我们从一个新的视角（即项目属性的构建）来研究序列推荐问题，并总结了近来在序列推荐中使用的一些最新技术，如纯ID基推荐、带有辅助信息的序列推荐、多模态序列推荐、生成式推荐、基于大语言模型的推荐、超长序列推荐和数据增强推荐。此外，我们还介绍了序列推荐领域的一些前沿研究主题，例如开放领域推荐、数据驱动推荐、云端协同推荐、连续推荐、有益推荐和可解释推荐。我们相信，本文综述将为该领域的读者提供有价值的研究路线图。 

---
# Token-Level Graphs for Short Text Classification 

**Title (ZH)**: 短文本分类中的令牌级图方法 

**Authors**: Gregor Donabauer, Udo Kruschwitz  

**Link**: [PDF](https://arxiv.org/pdf/2412.12754)  

**Abstract**: The classification of short texts is a common subtask in Information Retrieval (IR). Recent advances in graph machine learning have led to interest in graph-based approaches for low resource scenarios, showing promise in such settings. However, existing methods face limitations such as not accounting for different meanings of the same words or constraints from transductive approaches. We propose an approach which constructs text graphs entirely based on tokens obtained through pre-trained language models (PLMs). By applying a PLM to tokenize and embed the texts when creating the graph(-nodes), our method captures contextual and semantic information, overcomes vocabulary constraints, and allows for context-dependent word meanings. Our approach also makes classification more efficient with reduced parameters compared to classical PLM fine-tuning, resulting in more robust training with few samples. Experimental results demonstrate how our method consistently achieves higher scores or on-par performance with existing methods, presenting an advancement in graph-based text classification techniques. To support reproducibility of our work we make all implementations publicly available to the community\footnote{\url{this https URL}}. 

**Abstract (ZH)**: 短文本分类是信息检索（IR）中的一个常见子任务。最近的图机器学习进展引发了对图基方法在低资源场景中应用的兴趣，这些方法在这样的环境中显示出应用潜力。然而，现有的方法面临一些局限性，例如未能考虑相同词语的不同含义，以及从归纳方法带来的约束。我们提出了一种方法，该方法完全基于通过预训练语言模型（PLMs）获取的标记构建文本图。在创建图（节点）时，通过应用PLM进行分词和嵌入，我们的方法能够捕捉上下文和语义信息，克服词汇限制，并允许上下文相关的词语含义。此外，与经典的PLM微调方法相比，我们的方法通过减少参数使分类更加高效，从而在少量样本的情况下进行更稳健的训练。实验结果表明，我们的方法能够在许多情况下持续获得比现有方法更高的得分或达到同等性能，从而在图基文本分类技术方面取得了进步。为支持我们工作的可再现性，我们已将所有实现公开给社区（\footnote{\url{this https URL}}）。 

---
# Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning 

**Title (ZH)**: 基于分布意识鲁棒学习的大型语言模型相关性建模增强 

**Authors**: Hong Liu, Saisai Gong, Yixin Ji, Kaixin Wu, Jia Xu, Jinjie Gu  

**Link**: [PDF](https://arxiv.org/pdf/2412.12504)  

**Abstract**: With the rapid advancement of pre-trained large language models (LLMs), recent endeavors have leveraged the capabilities of LLMs in relevance modeling, resulting in enhanced performance. This is usually done through the process of fine-tuning LLMs on specifically annotated datasets to determine the relevance between queries and items. However, there are two limitations when LLMs are naively employed for relevance modeling through fine-tuning and inference. First, it is not inherently efficient for performing nuanced tasks beyond simple yes or no answers, such as assessing search relevance. It may therefore tend to be overconfident and struggle to distinguish fine-grained degrees of relevance (e.g., strong relevance, weak relevance, irrelevance) used in search engines. Second, it exhibits significant performance degradation when confronted with data distribution shift in real-world scenarios. In this paper, we propose a novel Distribution-Aware Robust Learning framework (DaRL) for relevance modeling in Alipay Search. Specifically, we design an effective loss function to enhance the discriminability of LLM-based relevance modeling across various fine-grained degrees of query-item relevance. To improve the generalizability of LLM-based relevance modeling, we first propose the Distribution-Aware Sample Augmentation (DASA) module. This module utilizes out-of-distribution (OOD) detection techniques to actively select appropriate samples that are not well covered by the original training set for model fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to simultaneously improve in-distribution (ID) and OOD performance, bridging the performance gap between them. DaRL has been deployed online to serve the Alipay's insurance product search... 

**Abstract (ZH)**: 随着预训练大型语言模型（LLMs）的迅速发展，近期的研究利用了LLMs在相关性建模方面的能力，从而提升了性能。这通常通过在特定标注数据集上微调LLMs来确定查询和项之间的相关性来实现。然而，当LLMs通过微调和推理用于相关性建模时，存在两个局限性。首先，LLMs本身并不擅长执行需要细致判断的任务，比如搜索相关性评估，可能会因此显得过于自信，难以区分搜索中使用的细粒度相关性程度（如强相关、弱相关、无关）。其次，在现实世界场景中遇到数据分布变化时，其性能会显著下降。本文中，我们提出了一个名为“分布感知稳健学习”（DaRL）框架，用于支付宝搜索中的相关性建模。具体而言，我们设计了一种有效损失函数，以增强基于LLMs的相关性建模在不同细粒度查询-项相关性之间的区分能力。为了提高基于LLMs的相关性建模的一般化能力，我们首先提出了分布感知样本增强（DASA）模块。该模块利用异常样本检测技术主动选择那些原训练集中未充分覆盖的适当样本进行模型微调。此外，我们采用多阶段微调策略，同时提升分布内（ID）和异常分布（OOD）的性能，从而弥合两者之间的性能差距。DaRL已在支付宝保险产品搜索中上线应用。 

---
# LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph Reasoning for Cold-start Sequential Recommendation 

**Title (ZH)**: 大型语言模型是知识图谱推理器：具备直觉意识的知识图谱推理方法在冷启动序列推荐中的应用 

**Authors**: Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama  

**Link**: [PDF](https://arxiv.org/pdf/2412.12464)  

**Abstract**: Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items. However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios. On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems. Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues. To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings. By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies. Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios. 

**Abstract (ZH)**: 知识图谱（KGs）以图形结构表示实体之间的关系，并已广泛研究作为考虑项目准确内容信息的推荐系统的有前途的工具。然而，传统的基于KG的推荐方法面临根本性挑战：时间信息考虑不足以及在冷启动场景中的表现不佳。另一方面，大型语言模型（LLMs）可以被视为从网络数据中学到大量知识的数据库，并且由于其作为推荐系统应用的潜力，最近引起了广泛关注。尽管将LLMs视为推荐系统的做法能够利用其高推荐素养，但由于输入标记的限制，使得难以考虑整个推荐领域数据集，从而导致可扩展性问题。为了解决这些挑战，我们提出了一种LLM感知的知识图谱推理模型（LIKR）。我们的主要想法是将LLMs视为推理器，输出对KG的直觉探索策略。为了整合LLMs和KG的知识，我们通过强化学习训练了一个推荐代理，使用一个奖励函数，该函数结合了不同的推荐策略，包括LLMs的直觉和KG嵌入。通过利用提示工程引入时间感知，并从有限的交互中生成用户偏好的文本表示，LIKR可以在冷启动场景中改进推荐性能。此外，通过使用KG表示推荐领域的数据集并限制LLMs的输出为KG探索策略，LIKR可以避免可扩展性问题。在实际数据集上的实验表明，我们的模型在冷启动序列推荐场景中优于最先进的推荐方法。 

---
# Searching Personal Collections 

**Title (ZH)**: 搜索个人收藏 

**Authors**: Michael Bendersky, Donald Metzler, Marc Najork, Xuanhui Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12330)  

**Abstract**: This article describes the history of information retrieval on personal document collections. 

**Abstract (ZH)**: 本文描述了个人文档集合上的信息检索历史。 

---
# Enhancing the conformal predictability of context-aware recommendation systems by using Deep Autoencoders 

**Title (ZH)**: 采用深度自编码器提升基于情境的推荐系统预测一致性 

**Authors**: Saloua Zammali, Siddhant Dutta, Sadok Ben Yahia  

**Link**: [PDF](https://arxiv.org/pdf/2412.12110)  

**Abstract**: In the field of Recommender Systems (RS), neural collaborative filtering represents a significant milestone by combining matrix factorization and deep neural networks to achieve promising results. Traditional methods like matrix factorization often rely on linear models, limiting their capability to capture complex interactions between users, items, and contexts. This limitation becomes particularly evident with high-dimensional datasets due to their inability to capture relationships among users, items, and contextual factors. Unsupervised learning and dimension reduction tasks utilize autoencoders, neural network-based models renowned for their capacity to encode and decode data. Autoencoders learn latent representations of inputs, reducing dataset size while capturing complex patterns and features. In this paper, we introduce a framework that combines neural contextual matrix factorization with autoencoders to predict user ratings for items. We provide a comprehensive overview of the framework's design and implementation. To evaluate its performance, we conduct experiments on various real-world datasets and compare the results against state-of-the-art approaches. We also extend the concept of conformal prediction to prediction rating and introduce a Conformal Prediction Rating (CPR). For RS, we define the nonconformity score, a key concept of conformal prediction, and demonstrate that it satisfies the exchangeability property. 

**Abstract (ZH)**: 在推荐系统（RS）领域，神经协同过滤代表了一个重要里程碑，通过结合矩阵分解和深度神经网络，实现了优异的结果。传统的矩阵分解方法通常依赖于线性模型，限制了它们捕捉用户、项目和上下文之间复杂交互的能力。当面对高维数据集时，这一局限性尤为明显，因为它们难以捕捉用户、项目和环境因素之间的关系。无监督学习和降维任务中利用的自动编码器是一种基于神经网络的模型，以其编码和解码数据的能力而闻名。自动编码器学习输入的潜在表示，从而减少数据集的大小，同时捕获复杂的模式和特征。在本文中，我们提出了一种结合神经上下文矩阵分解和自动编码器的框架，以预测用户对项目的评分。我们提供了该框架的设计和实现的全面概述。为了评估其性能，我们在多个真实世界数据集上进行了实验，并将结果与最新的方法进行了比较。我们还扩展了置信预测的概念，并引入了置信预测评分（CPR）。对于推荐系统，我们定义了非符合性评分，这是置信预测中的一个关键概念，并证明它满足可交换性属性。 

---
# Re-calibrating methodologies in social media research: Challenge the visual, work with Speech 

**Title (ZH)**: 在社交媒体研究中重新校准方法论：挑战可视化，重视言语 

**Authors**: Hongrui Jin  

**Link**: [PDF](https://arxiv.org/pdf/2412.13170)  

**Abstract**: This article methodologically reflects on how social media scholars can effectively engage with speech-based data in their analyses. While contemporary media studies have embraced textual, visual, and relational data, the aural dimension remained comparatively under-explored. Building on the notion of secondary orality and rejection towards purely visual culture, the paper argues that considering voice and speech at scale enriches our understanding of multimodal digital content. The paper presents the TikTok Subtitles Toolkit that offers accessible speech processing readily compatible with existing workflows. In doing so, it opens new avenues for large-scale inquiries that blend quantitative insights with qualitative precision. Two illustrative cases highlight both opportunities and limitations of speech research: while genres like #storytime on TikTok benefit from the exploration of spoken narratives, nonverbal or music-driven content may not yield significant insights using speech data. The article encourages researchers to integrate aural exploration thoughtfully to complement existing methods, rather than replacing them. I conclude that the expansion of our methodological repertoire enables richer interpretations of platformised content, and our capacity to unpack digital cultures as they become increasingly multimodal. 

**Abstract (ZH)**: 本文从方法论角度反思社会媒体学者如何有效分析基于言语的数据。尽管当代媒体研究已广泛采用文本、视觉和关系数据，听觉维度的研究相对较少。基于次级语境（或口头性）的观念以及对纯粹视觉文化的排斥，本文认为在大规模范围内考虑声音和言语，能丰富我们对多模态数字内容的理解。本文介绍了抖音字幕工具包，该工具包提供易于使用的语音处理功能，易于与现有工作流程兼容。通过这种方式，它开辟了新的研究途径，能够结合定量见解与定性精度进行大规模研究。两个示例案例分别突显了语音研究的机会与局限性：虽然如抖音中的#storytime等体裁可以从口头叙述的研究中受益，但非言语或音乐驱动的内容可能无法从语音数据中获得显著洞察。本文鼓励研究者审慎整合听觉探索，以补充现有方法，而不是替代它们。本文总结认为，扩展我们的方法论范围能够使我们更深入地解读平台化内容，并且能够更好地分析日益多模态的数字文化。 

---
# C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System 

**Title (ZH)**: C-FedRAG：一种保密联邦检索增强生成系统 

**Authors**: Parker Addison, Minh-Tuan H. Nguyen, Tomislav Medan, Mohammad T. Manzari, Brendan McElrone, Laksh Lalwani, Aboli More, Smita Sharma, Holger R. Roth, Isaac Yang, Chester Chen, Daguang Xu, Yan Cheng, Andrew Feng, Ziyue Xu  

**Link**: [PDF](https://arxiv.org/pdf/2412.13163)  

**Abstract**: Organizations seeking to utilize Large Language Models (LLMs) for knowledge querying and analysis often encounter challenges in maintaining an LLM fine-tuned on targeted, up-to-date information that keeps answers relevant and grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible solution for organizations looking to overcome the challenges of maintaining proprietary models and to help reduce LLM hallucinations in their query responses. However, RAG comes with its own issues regarding scaling data pipelines across tiered-access and disparate data sources. In many scenarios, it is necessary to query beyond a single data silo to provide richer and more relevant context for an LLM. Analyzing data sources within and across organizational trust boundaries is often limited by complex data-sharing policies that prohibit centralized data storage, therefore, inhibit the fast and effective setup and scaling of RAG solutions. In this paper, we introduce Confidential Computing (CC) techniques as a solution for secure Federated Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG system (C-FedRAG) enables secure connection and scaling of a RAG workflows across a decentralized network of data providers by ensuring context confidentiality. We also demonstrate how to implement a C-FedRAG system using the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and MIRAGE benchmarking dataset. 

**Abstract (ZH)**: 致力于利用大规模语言模型（LLMs）进行知识查询和分析的组织可能会面临在保持模型对准最新信息方面的问题，从而确保答案的相关性和现实性。检索增强生成（RAG）已成为一种可行的解决方案，用于克服维护专有模型的挑战，并帮助减少LLM在查询响应中的幻觉。然而，RAG在其数据管道的分级访问和异质数据源之间扩展方面也存在自身的问题。在许多情况下，为了提供更丰富和相关的信息上下文，需要跨多个数据孤岛进行查询。在组织信任边界内分析数据源通常受限于复杂的数据共享政策，这些政策禁止集中存储数据，因此妨碍了RAG解决方案的快速有效设置和扩展。本文介绍加密计算（CC）技术作为安全联邦检索增强生成（FedRAG）的解决方案。我们提出的安全联邦RAG系统（C-FedRAG）通过确保上下文的机密性，使RAG工作流在去中心化的数据提供者网络中安全连接和扩展。我们还展示了如何使用NVIDIA FLARE SDK实现C-FedRAG系统，并使用MedRAG工具包和MIRAGE基准测试数据集对其性能进行了评估。 

---
# CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval 

**Title (ZH)**: CLASP：对比学习的跨模态语言-语音预训练用于多语言多模态信息检索 

**Authors**: Mohammad Mahdi Abootorabi, Ehsaneddin Asgari  

**Link**: [PDF](https://arxiv.org/pdf/2412.13071)  

**Abstract**: This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During training, we utilize our newly introduced speech-text dataset, which encompasses 15 diverse categories ranging from fiction to religion. CLASP's audio component integrates audio spectrograms with a pre-trained self-supervised speech model, while its language encoding counterpart employs a sentence encoder pre-trained on over 100 languages. This unified lightweight model bridges the gap between various modalities and languages, enhancing its effectiveness in handling and retrieving multilingual and multimodal data. Our evaluations across multiple languages demonstrate that CLASP establishes new benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional ASR-based retrieval approaches in specific scenarios. 

**Abstract (ZH)**: 本研究介绍了一种名为CLASP（Contrastive Language-Speech Pretraining）的多语言、多模态表示方法，专门用于音频-文本信息检索。CLASP 利用了语音内容与文本数据之间的协同作用。在训练过程中，我们使用了一项新引入的语音-文本数据集，该数据集涵盖了15个不同的类别，从虚构到宗教，共包含多种类型的数据。CLASP 的音频部分将音频频谱图与预训练的自监督语音模型相结合，而其语言编码部分则采用在超过100种语言上进行预训练的句子编码器。这一统一的轻量级模型在不同模态和语言之间架起桥梁，增强了其在处理和检索多语言及多模态数据方面的有效性。我们的跨语言评估结果表明，CLASP 在 HITS@1、MRR 和 meanR 等指标上建立了新的基准，且在某些特定场景中优于基于传统ASR的信息检索方法。 

---
# Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO 

**Title (ZH)**: 低资源语言检索的 enabling：为乌尔都语 MS MARCO 建立 baseline 

**Authors**: Umer Butt, Stalin Veranasi, Günter Neumann  

**Link**: [PDF](https://arxiv.org/pdf/2412.12997)  

**Abstract**: As the Information Retrieval (IR) field increasingly recognizes the importance of inclusivity, addressing the needs of low-resource languages remains a significant challenge. This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset through machine translation. We establish baseline results through zero-shot learning for IR in Urdu and subsequently apply the mMARCO multilingual IR methodology to this newly translated dataset. Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant improvements over zero-shot results and showing the potential for expanding IR access for Urdu speakers. By bridging access gaps for speakers of low-resource languages, this work not only advances multilingual IR research but also emphasizes the ethical and societal importance of inclusive IR technologies. This work provides valuable insights into the challenges and solutions for improving language representation and lays the groundwork for future research, especially in South Asian languages, which can benefit from the adaptable methods used in this study. 

**Abstract (ZH)**: 随着信息检索（IR）领域越来越重视包容性，满足低资源语言的需求仍然是一个重大挑战。本文介绍了首个大规模的乌尔都语IR数据集，该数据集通过机器翻译自MS MARCO数据集创建而成。我们通过零样本学习为乌尔都语IR建立了基线结果，并随后将mMARCO多语言IR方法应用于这个新翻译数据集。研究结果表明，微调后的模型（Urdu-mT5-mMARCO）实现了平均倒数相关性（MRR@10）为0.247和召回率（Recall@10）为0.439，这些结果显著优于零样本结果，表明扩展乌尔都语用户访问IR服务的潜力。通过填补低资源语言使用者的访问差距，本项工作不仅推动了多语言IR研究，还强调了包容性IR技术的伦理和社会意义。本项工作提供了提高语言表示的挑战和解决方案的宝贵见解，并为未来的研究奠定了基础，特别是对于那些可以从本研究中适用方法获益的南亚语言。 

---
# Cluster-guided Contrastive Class-imbalanced Graph Classification 

**Title (ZH)**: 群集导向的对比不均衡图分类 

**Authors**: Wei Ju, Zhengyang Mao, Siyu Yi, Yifang Qin, Yiyang Gu, Zhiping Xiao, Jianhao Shen, Ziyue Qiao, Ming Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12984)  

**Abstract**: This paper studies the problem of class-imbalanced graph classification, which aims at effectively classifying the categories of graphs in scenarios with imbalanced class distribution. Despite the tremendous success of graph neural networks (GNNs), their modeling ability for imbalanced graph-structured data is inadequate, which typically leads to predictions biased towards the majority classes. Besides, existing class-imbalanced learning methods in visions may overlook the rich graph semantic substructures of the majority classes and excessively emphasize learning from the minority classes. To tackle this issue, this paper proposes a simple yet powerful approach called C$^3$GNN that incorporates the idea of clustering into contrastive learning to enhance class-imbalanced graph classification. Technically, C$^3$GNN clusters graphs from each majority class into multiple subclasses, ensuring they have similar sizes to the minority class, thus alleviating class imbalance. Additionally, it utilizes the Mixup technique to synthesize new samples and enrich the semantic information of each subclass, and leverages supervised contrastive learning to hierarchically learn effective graph representations. In this way, we can not only sufficiently explore the semantic substructures within the majority class but also effectively alleviate excessive focus on the minority class. Extensive experiments on real-world graph benchmark datasets verify the superior performance of our proposed method. 

**Abstract (ZH)**: 本文研究了类别不平衡图分类问题，旨在有效分类类别分布不平衡场景中的图类别。尽管图神经网络（GNNs）取得了巨大的成功，但它们在建模不平衡结构化图数据方面的能力是不足的，通常会导致预测偏向多数类。此外，现有视觉中的类别不平衡学习方法可能会忽略多数类丰富的图语义子结构，并过分强调从少数类中学习。为解决这一问题，本文提出了一种名为C$^3$GNN的简单而强大的方法，将聚类思想融入对比学习中，以增强类别不平衡图分类。具体而言，C$^3$GNN将每个多数类的图聚类成多个子类，确保它们的大小与少数类相似，从而缓解类别不平衡问题。此外，它利用Mixup技术合成新的样本，并丰富每个子类的语义信息，并利用监督对比学习来分层次地学习有效的图表示。通过这种方式，我们不仅可以充分探索多数类别中的语义子结构，还可以有效地缓解对少数类的关注过度的问题。在实际图基准数据集上的广泛实验验证了我们所提出方法的优越性能。 

---
# Selective Shot Learning for Code Explanation 

**Title (ZH)**: 选择性-shot-学习方法用于代码解释 

**Authors**: Paheli Bhattacharya, Rishabh Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2412.12852)  

**Abstract**: Code explanation plays a crucial role in the software engineering domain, aiding developers in grasping code functionality efficiently. Recent work shows that the performance of LLMs for code explanation improves in a few-shot setting, especially when the few-shot examples are selected intelligently. State-of-the-art approaches for such Selective Shot Learning (SSL) include token-based and embedding-based methods. However, these SSL approaches have been evaluated on proprietary LLMs, without much exploration on open-source Code-LLMs. Additionally, these methods lack consideration for programming language syntax. To bridge these gaps, we present a comparative study and propose a novel SSL method (SSL_ner) that utilizes entity information for few-shot example selection. We present several insights and show the effectiveness of SSL_ner approach over state-of-the-art methods across two datasets. To the best of our knowledge, this is the first systematic benchmarking of open-source Code-LLMs while assessing the performances of the various few-shot examples selection approaches for the code explanation task. 

**Abstract (ZH)**: 代码解释在软件工程领域发挥着至关重要的作用，有助于开发者高效地理解代码功能。最近的研究表明，在少量示例的设置下，大语言模型（LLM）进行代码解释的性能有所提升，特别是在智能选择少量示例的情况下。当前最先进的少量示例学习（SSL）方法包括基于 token 和基于嵌入的方法。然而，这些 SSL 方法大多在专用的 LLM 上进行评估，缺乏对开源 Code-LLM 的探索。此外，这些方法在编程语言语法方面缺乏考虑。为了弥补这些不足，我们进行了一个比较研究，并提出了一种新的 SSL 方法（SSL_ner），利用实体信息进行少量示例的选择。我们提出了若干见解，并展示了 SSL_ner 方法相较于现有的先进方法的有效性，这在两个数据集上进行了验证。据我们所知，这是首次在评估代码解释任务的多种少量示例选择方法的性能时，系统性地基准测试开源 Code-LLM。 

---
# Cross-Dialect Information Retrieval: Information Access in Low-Resource and High-Variance Languages 

**Title (ZH)**: 跨方言信息检索：低资源和高变异性语言中的信息访问 

**Authors**: Robert Litschko, Oliver Kraus, Verena Blaschke, Barbara Plank  

**Link**: [PDF](https://arxiv.org/pdf/2412.12806)  

**Abstract**: A large amount of local and culture-specific knowledge (e.g., people, traditions, food) can only be found in documents written in dialects. While there has been extensive research conducted on cross-lingual information retrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received limited attention. Dialect retrieval poses unique challenges due to the limited availability of resources to train retrieval models and the high variability in non-standardized languages. We study these challenges on the example of German dialects and introduce the first German dialect retrieval dataset, dubbed WikiDIR, which consists of seven German dialects extracted from Wikipedia. Using WikiDIR, we demonstrate the weakness of lexical methods in dealing with high lexical variation in dialects. We further show that commonly used zero-shot cross-lingual transfer approach with multilingual encoders do not transfer well to extremely low-resource setups, motivating the need for resource-lean and dialect-specific retrieval models. We finally demonstrate that (document) translation is an effective way to reduce the dialect gap in CDIR. 

**Abstract (ZH)**: 大量的地方性和文化特异性知识（如人物、传统和美食）主要集中在方言文档中。尽管已经开展了广泛的跨语言信息检索（CLIR）研究，但跨方言检索（CDIR）领域却较少受到关注。由于可用于训练检索模型的资源有限以及非标准化语言的高度变异性，方言检索面临独特的挑战。我们以德语方言为例进行了这些挑战的研究，并引入了首个德语方言检索数据集，名为WikiDIR，该数据集由来自维基百科的七种德语方言构成。利用WikiDIR，我们证明了基于词汇的方法难以应对方言中高度词汇变异性的问题。此外，我们还展示了常用的零样本跨语言迁移方法（使用多语言编码器）在极度低资源设置下表现不佳，这突显了开发资源节约型和方言特定的检索模型的必要性。最后，我们证明了（文档）翻译是减少CDIR中方言差距的有效方法。 

---
# SynthCypher: A Fully Synthetic Data Generation Framework for Text-to-Cypher Querying in Knowledge Graphs 

**Title (ZH)**: SynthCypher: 知识图中基于文本到密文查询的完全合成数据生成框架 

**Authors**: Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan  

**Link**: [PDF](https://arxiv.org/pdf/2412.12612)  

**Abstract**: Cypher, the query language for Neo4j graph databases, plays a critical role in enabling graph-based analytics and data exploration. While substantial research has been dedicated to natural language to SQL query generation (Text2SQL), the analogous problem for graph databases referred to as Text2Cypher remains underexplored. In this work, we introduce SynthCypher, a fully synthetic and automated data generation pipeline designed to address this gap. SynthCypher employs a novel LLMSupervised Generation-Verification framework, ensuring syntactically and semantically correct Cypher queries across diverse domains and query complexities. Using this pipeline, we create SynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher instances. Fine-tuning open-source large language models (LLMs), including LLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant performance improvements of up to 40% on the Text2Cypher test set and 30% on the SPIDER benchmark adapted for graph databases. This work demonstrates that high-quality synthetic data can effectively advance the state-of-the-art in Text2Cypher tasks. 

**Abstract (ZH)**: Cypher 是 Neo4j 图数据库的查询语言，在图分析和数据探索方面发挥着关键作用。虽然大量研究集中在自然语言到 SQL 查询生成 (Text2SQL) 领域，而对于图数据库中的类似问题，即 Text2Cypher，研究仍然相对不足。在本文中，我们介绍了一种名为 SynthCypher 的完整合成自动化数据生成管道，旨在解决这一问题。SynthCypher 采用了新颖的 LLMSupervised Generation-Verification 框架，确保在不同领域和查询复杂度中生成语法规正确且语义正确的 Cypher 查询。利用该管道，我们创建了包含 29,800 个 Text2Cypher 实例的大规模基准数据集 SynthCypher Dataset。对开源大规模语言模型 (LLMs)，包括 LLaMa-3.1-8B、Mistral-7B 和 QWEN-7B，进行微调后，在 Text2Cypher 测试集上取得了高达 40% 的性能提升，在适应图数据库的 SPIDER 基准上取得了 30% 的性能提升。这项工作证明，高质量的合成数据可以有效地推动 Text2Cypher 任务的技术前沿。 

---
# EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation 

**Title (ZH)**: EXIT：基于上下文的提取式压缩以增强检索增强生成 

**Authors**: Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park  

**Link**: [PDF](https://arxiv.org/pdf/2412.12559)  

**Abstract**: We introduce EXIT, an extractive context compression framework that enhances both the effectiveness and efficiency of retrieval-augmented generation (RAG) in question answering (QA). Current RAG systems often struggle when retrieval models fail to rank the most relevant documents, leading to the inclusion of more context at the expense of latency and accuracy. While abstractive compression methods can drastically reduce token counts, their token-by-token generation process significantly increases end-to-end latency. Conversely, existing extractive methods reduce latency but rely on independent, non-adaptive sentence selection, failing to fully utilize contextual information. EXIT addresses these limitations by classifying sentences from retrieved documents - while preserving their contextual dependencies - enabling parallelizable, context-aware extraction that adapts to query complexity and retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks show that EXIT consistently surpasses existing compression methods and even uncompressed baselines in QA accuracy, while also delivering substantial reductions in inference time and token count. By improving both effectiveness and efficiency, EXIT provides a promising direction for developing scalable, high-quality QA solutions in RAG pipelines. Our code is available at this https URL 

**Abstract (ZH)**: 我们提出了一个名为 EXIT 的提取式上下文压缩框架，旨在增强检索增强生成（RAG）在问答（QA）中的效果和效率。当前的 RAG 系统在检索模型无法正确排序最相关文档时常常表现不佳，这导致了在准确性和延迟之间做出权衡，不得不包含更多的上下文信息。虽然生成式压缩方法能大幅减少标记数，但其逐标记生成的过程会显著增加端到端的延迟。相比之下，现有的提取式压缩方法虽然能减少延迟，但依赖于独立的、非适应性的句子选择，未能充分利用上下文信息。EXIT 通过保留从检索文档中抽取的句子的上下文依赖性，实现了并行化的、上下文意识的压缩，可以根据查询复杂性和检索质量进行调整。我们在单跳和多跳问答任务上的评估表明，EXIT 在问答准确性方面持续超过了现有的压缩方法，甚至在未压缩的基础上也实现了显著的推理时间和标记数的减少。通过提高效果和效率，EXIT 为开发高度可扩展和高质量的 RAG 管线提供了有希望的方向。我们的代码可在此处访问：[提供链接的位置] 

---
# Boosting Long-Context Information Seeking via Query-Guided Activation Refilling 

**Title (ZH)**: 通过查询引导的激活补充提升长上下文信息检索 

**Authors**: Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian  

**Link**: [PDF](https://arxiv.org/pdf/2412.12486)  

**Abstract**: Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query's information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs.
In the paper, we propose a method for processing long-context information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE's effectiveness, achieving improvements in both performance and efficiency. 

**Abstract (ZH)**: 长文本处理对于大规模语言模型（LLMs）构成了重大挑战，主要是由于其固有的上下文窗口限制以及广泛的键值（KV）激活所导致的计算负担，这严重影响了模型的效率。对于信息检索任务而言，完整上下文的感知往往是不必要的，因为查询的信息需求可以从局部细节动态变化到全面视角，这取决于查询的复杂性。然而，现有的方法难以有效地适应这些动态的信息需求。

在本文中，我们提出了一种通过查询引导的激活补充（Activation Refilling, ACR）方法来处理长文本的信息检索任务。ACR构建了一种双层KV缓存结构，其中第一层（L1）缓存紧凑地捕获全局信息，而第二层（L2）缓存则提供详细的局部信息。ACR在两个缓存之间建立代理关系，使得输入查询能够关注L1缓存，并动态地从L2缓存中填充相关条目。这一机制将全局理解与查询特定的局部细节相结合，从而提高了答案解码的效果。多种长文本信息检索数据集的实验表明，ACR在性能和效率方面均表现出色。 

---
# LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework 

**Title (ZH)**: LITA：一种高效的基于大语言模型的迭代主题增强框架 

**Authors**: Chia-Hsuan Chang, Jui-Tse Tsai, Yi-Hang Tsai, San-Yih Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12459)  

**Abstract**: Topic modeling is widely used for uncovering thematic structures within text corpora, yet traditional models often struggle with specificity and coherence in domain-focused applications. Guided approaches, such as SeededLDA and CorEx, incorporate user-provided seed words to improve relevance but remain labor-intensive and static. Large language models (LLMs) offer potential for dynamic topic refinement and discovery, yet their application often incurs high API costs. To address these challenges, we propose the LLM-assisted Iterative Topic Augmentation framework (LITA), an LLM-assisted approach that integrates user-provided seeds with embedding-based clustering and iterative refinement. LITA identifies a small number of ambiguous documents and employs an LLM to reassign them to existing or new topics, minimizing API costs while enhancing topic quality. Experiments on two datasets across topic quality and clustering performance metrics demonstrate that LITA outperforms five baseline models, including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an efficient and adaptable framework for advancing topic modeling and text clustering. 

**Abstract (ZH)**: 主题建模广泛应用于揭示文本语料库中的主题结构，但传统模型在领域导向应用中往往面临具体性和连贯性不足的问题。通过种子词引导的方法，如SeededLDA和CorEx，能够提升相关性，但这些方法仍然劳动强度高且缺乏灵活性。大型语言模型（LLMs）有可能实现动态主题优化和发现，然而其应用往往会带来高昂的API使用成本。为了解决这些问题，我们提出了一种大型语言模型辅助的迭代主题增强框架（LITA），这是一种结合了用户提供的种子词、基于嵌入的聚类和迭代优化的LLM辅助方法。LITA识别出少量模糊的主题文档，并利用LLM重新分配这些文档至现有或新的话题，从而在降低API使用成本的同时提高主题质量。实验结果表明，LITA在主题质量和聚类性能指标上均优于包括LDA、SeededLDA、CorEx、BERTopic和PromptTopic在内的五个基线模型。我们的工作提供了一种有效且适应性强的主题建模和文本聚类框架。 

---
# Refining Dimensions for Improving Clustering-based Cross-lingual Topic Models 

**Title (ZH)**: 基于聚类的跨语言主题模型细化维度改进方法 

**Authors**: Chia-Hsuan Chang, Tien-Yuan Huang, Yi-Hang Tsai, Chia-Ming Chang, San-Yih Hwang  

**Link**: [PDF](https://arxiv.org/pdf/2412.12433)  

**Abstract**: Recent works in clustering-based topic models perform well in monolingual topic identification by introducing a pipeline to cluster the contextualized representations. However, the pipeline is suboptimal in identifying topics across languages due to the presence of language-dependent dimensions (LDDs) generated by multilingual language models. To address this issue, we introduce a novel, SVD-based dimension refinement component into the pipeline of the clustering-based topic model. This component effectively neutralizes the negative impact of LDDs, enabling the model to accurately identify topics across languages. Our experiments on three datasets demonstrate that the updated pipeline with the dimension refinement component generally outperforms other state-of-the-art cross-lingual topic models. 

**Abstract (ZH)**: 基于聚类的专题模型近年来在单语专题识别方面表现出色，通过引入一种管道来聚类上下文表示。然而，当用于跨语言专题识别时，该管道因多语言模型生成的语言依赖维度（LDDs）而变得次优。为解决这一问题，我们引入了一种基于SVD的维度精炼组件，将其集成到基于聚类的专题模型的管道中。该组件有效消除了LDDs的负面影响，从而使模型能够准确地识别跨语言的专题。我们对三个数据集的实验表明，包含维度精炼组件的更新管道总体上优于其他最先进的跨语言专题模型。 

---
# RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems 

**Title (ZH)**: RAG游乐场：一种评估RAG系统中检索策略和提示工程系统的框架 

**Authors**: Ioannis Papadimitriou, Ilias Gialampoukidis, Stefanos Vrochidis, Ioannis, Kompatsiaris  

**Link**: [PDF](https://arxiv.org/pdf/2412.12322)  

**Abstract**: We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality. 

**Abstract (ZH)**: 我们介绍了一种开源框架RAG Playground，用于系统评估检索增强生成（RAG）系统。该框架实现了并对比了三种检索方法：朴素向量搜索、再排序和混合向量-关键词搜索，并结合使用了不同的ReAct代理和提示策略。我们引入了一个全面的评估框架，并提供了新的评估指标，比较了不同语言模型（Llama 3.1和Qwen 2.5）在各种检索配置下的性能。实验结果表明，通过混合检索方法和结构化自我评估提示，能够取得显著的性能提升，在我们的多指标评估框架中，混合搜索方法的通过率达到了72.7%。结果还强调了在RAG系统中提示工程的重要性，我们自定义提示的代理在检索准确性和响应质量方面表现出持续的改进。 

---
# One for Dozens: Adaptive REcommendation for All Domains with Counterfactual Augmentation 

**Title (ZH)**: 数十而治：基于反事实增强的全域自适应推荐 

**Authors**: Huishi Luo, Yiwen Chen, Yiqing Wu, Fuzhen Zhuang, Deqing Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.11905)  

**Abstract**: Multi-domain recommendation (MDR) aims to enhance recommendation performance across various domains. However, real-world recommender systems in online platforms often need to handle dozens or even hundreds of domains, far exceeding the capabilities of traditional MDR algorithms, which typically focus on fewer than five domains. Key challenges include a substantial increase in parameter count, high maintenance costs, and intricate knowledge transfer patterns across domains. Furthermore, minor domains often suffer from data sparsity, leading to inadequate training in classical methods. To address these issues, we propose Adaptive REcommendation for All Domains with counterfactual augmentation (AREAD). AREAD employs a hierarchical structure with a limited number of expert networks at several layers, to effectively capture domain knowledge at different granularities. To adaptively capture the knowledge transfer pattern across domains, we generate and iteratively prune a hierarchical expert network selection mask for each domain during training. Additionally, counterfactual assumptions are used to augment data in minor domains, supporting their iterative mask pruning. Our experiments on two public datasets, each encompassing over twenty domains, demonstrate AREAD's effectiveness, especially in data-sparse domains. Source code is available at this https URL. 

**Abstract (ZH)**: 多领域推荐（MDR）旨在提升在不同领域的推荐性能。然而，现实中的在线平台推荐系统经常需要处理数十甚至数百个领域，远远超出了传统MDR算法的能力，这些传统算法通常只关注少于五个领域。关键挑战包括参数数量大幅增加、维护成本高昂以及领域间复杂的知识迁移模式。此外，小领域通常面临数据稀疏的问题，导致经典方法训练不足。为解决这些问题，我们提出了基于事实假设的自适应全部领域推荐方法（AREAD）。AREAD采用分层结构，在多个层级上使用少量专家网络，以有效捕捉不同粒度的领域知识。为适应性地捕捉领域间的知识迁移模式，在训练过程中，我们为每个领域生成并迭代修剪层次专家网络选择掩码。此外，使用基于事实假设的数据扩增方法在小领域支持迭代掩码修剪。我们在两个包含超过二十个领域的公开数据集上的实验表明，AREAD在数据稀疏领域尤其有效。源代码已发布于此 <https://>。 

---
# Investigating Mixture of Experts in Dense Retrieval 

**Title (ZH)**: 探究专家混合模型在密集检索中的应用 

**Authors**: Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi  

**Link**: [PDF](https://arxiv.org/pdf/2412.11864)  

**Abstract**: While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR), one limitation of these neural models is their narrow generalizability and robustness. To cope with this issue, one can leverage the Mixture-of-Experts (MoE) architecture. While previous IR studies have incorporated MoE architectures within the Transformer layers of DRMs, our work investigates an architecture that integrates a single MoE block (SB-MoE) after the output of the final Transformer layer. Our empirical evaluation investigates how SB-MoE compares, in terms of retrieval effectiveness, to standard fine-tuning. In detail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four benchmark collections with and without adding the MoE block. Moreover, since MoE showcases performance variations with respect to its parameters (i.e., the number of experts), we conduct additional experiments to investigate this aspect further. The findings show the effectiveness of SB-MoE especially for DRMs with a low number of parameters (i.e., TinyBERT), as it consistently outperforms the fine-tuned underlying model on all four benchmarks. For DRMs with a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires larger numbers of training samples to yield better retrieval performance. 

**Abstract (ZH)**: 虽然密集检索模型（DRMs）在信息检索（IR）方面取得了进展，但这些神经模型的一个局限性是其狭窄的泛化能力和鲁棒性。为应对这一问题，可以通过利用专家混合（Mixture-of-Experts, MoE）架构来加以解决。虽然之前的IR研究已经在DRMs的Transformer层中引入了MoE架构，但我们的工作研究了一种在最终Transformer层输出之后集成单一MoE块（SB-MoE）的架构。我们的实证评估探讨了SB-MoE在检索有效性方面与标准微调相比的表现。具体而言，我们在四个基准集合中对三个DRMs（TinyBERT、BERT和Contriever）进行微调，并且有和没有添加MoE块进行对比。此外，由于MoE的表现与其参数（即专家的数量）有关，我们还进行了额外的实验以进一步研究这一方面。研究结果显示，对于参数较少的DRMs（如TinyBERT），SB-MoE在所有四个基准中表现出色，始终优于其微调的基础模型。而对于参数较多的DRMs（如BERT和Contriever），SB-MoE需要更大的训练样本数才能获得更好的检索性能。 

---
# SPGL: Enhancing Session-based Recommendation with Single Positive Graph Learning 

**Title (ZH)**: SPGL：基于单正样本图学习的会话推荐增强方法 

**Authors**: Tiantian Liang, Zhe Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.11846)  

**Abstract**: Session-based recommendation seeks to forecast the next item a user will be interested in, based on their interaction sequences. Due to limited interaction data, session-based recommendation faces the challenge of limited data availability. Traditional methods enhance feature learning by constructing complex models to generate positive and negative samples. This paper proposes a session-based recommendation model using Single Positive optimization loss and Graph Learning (SPGL) to deal with the problem of data sparsity, high model complexity and weak transferability. SPGL utilizes graph convolutional networks to generate global item representations and batch session representations, effectively capturing intrinsic relationships between items. The use of single positive optimization loss improves uniformity of item representations, thereby enhancing recommendation accuracy. In the intent extractor, SPGL considers the hop count of the adjacency matrix when constructing the directed global graph to fully integrate spatial information. It also takes into account the reverse positional information of items when constructing session representations to incorporate temporal information. Comparative experiments across three benchmark datasets, Tmall, RetailRocket and Diginetica, demonstrate the model's effectiveness. The source code can be accessed on this https URL . 

**Abstract (ZH)**: 基于会话的推荐旨在根据用户的交互序列预测用户感兴趣的下一个项目。由于交互数据有限，基于会话的推荐面临数据可用性有限的挑战。传统方法通过构建复杂模型来生成正样本和负样本，以增强特征学习。本文提出了一种基于单正优化损失和图学习（SPGL）的基于会话的推荐模型，以应对稀疏数据、高模型复杂性和弱迁移性的问题。SPGL 利用图卷积网络生成全局项目表示和批次会话表示，有效捕获项之间的内在关系。单正优化损失的应用通过提高项目表示的一致性来提升推荐精度。在意图提取器中，SPGL 在构建有向全局图时考虑邻接矩阵的跳数，以充分利用空间信息。同时，在构建会话表示时考虑项的反向位置信息，以融合时间信息。在 Tmall、RetailRocket 和 Diginetica 三个基准数据集上的对比实验表明了该模型的有效性。源代码可从以下链接访问：[此 https URL](此 https URL)。 

---
