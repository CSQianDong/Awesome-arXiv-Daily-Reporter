{'arxiv_id': 'arXiv:2502.04324', 'title': 'Can Grammarly and ChatGPT accelerate language change? AI-powered technologies and their impact on the English language: wordiness vs. conciseness', 'authors': 'Karolina Rudnicka', 'link': 'https://arxiv.org/abs/2502.04324', 'abstract': 'The proliferation of NLP-powered language technologies, AI-based natural language generation models, and English as a mainstream means of communication among both native and non-native speakers make the output of AI-powered tools especially intriguing to linguists. This paper investigates how Grammarly and ChatGPT affect the English language regarding wordiness vs. conciseness. A case study focusing on the purpose subordinator in order to is presented to illustrate the way in which Grammarly and ChatGPT recommend shorter grammatical structures instead of longer and more elaborate ones. Although the analysed sentences were produced by native speakers, are perfectly correct, and were extracted from a language corpus of contemporary English, both Grammarly and ChatGPT suggest more conciseness and less verbosity, even for relatively short sentences. The present article argues that technologies such as Grammarly not only mirror language change but also have the potential to facilitate or accelerate it.', 'abstract_zh': '自然语言处理（NLP）技术的普及、基于AI的自然语言生成模型的发展，以及英语作为母语者和非母语者之间主流交流方式的增多，使得借助AI工具生成的内容特别引人注目。本文研究了Grammarly和ChatGPT如何影响英语语言的冗长与简练。通过一个关于目的性从句“in order to”的案例研究，本文展示了Grammarly和ChatGPT建议使用更简洁的语法结构代替更长更复杂的句式。尽管所分析的句子由母语者生成，完全正确，并来源于现代英语的语言语料库，但Grammarly和ChatGPT仍然建议增加简练度、减少冗长现象，甚至连相对较短的句子也不例外。本文认为，诸如Grammarly这样的技术不仅反映了语言的变化，还有可能促进或加速这种变化。', 'title_zh': 'Grammarly和ChatGPT能否加速语言变化？基于人工智能的技术对英语语言的影响：冗长与简洁之间的对比'}
{'arxiv_id': 'arXiv:2502.04321', 'title': 'Variation of sentence length across time and genre', 'authors': 'Karolina Rudnicka', 'link': 'https://arxiv.org/abs/2502.04321', 'abstract': 'The goal of this paper is threefold: i) to present some practical aspects of using full-text version of Corpus of Historical American English (COHA), the largest diachronic multi-genre corpus of the English language, in the investigation of a linguistic trend of change; ii) to test a widely held assumption that sentence length in written English has been steadily decreasing over the past few centuries; iii) to point to a possible link between the changes in sentence length and changes in the English syntactic usage. The empirical proof of concept for iii) is provided by the decline in the frequency of the non-finite purpose subordinator in order to. Sentence length, genre and the likelihood of occurrence of in order to are shown to be interrelated.', 'abstract_zh': '本文的研究目标有三个方面：（i）介绍如何使用《历史美国英语语料库》（COHA，全本版本）这一英语语言中最大的历时多体裁语料库进行语言变化趋势研究的一些实际方面；（ii）检验一个普遍存在的假设，即书面英语句子长度在过去几个世纪里一直呈稳定下降趋势；（iii）指出句子长度的变化可能与英语句法用法的变化之间存在联系。关于目标（iii），提供了一个实证概念证明：以“为了”介词短语的形式出现的目的性不定式频率的下降。句子长度、体裁以及“为了”介词短语出现的可能性被证明是相互关联的。', 'title_zh': '时间与文体背景下句子长度的变化'}
{'arxiv_id': 'arXiv:2502.04315', 'title': 'ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters', 'authors': 'Kamer Ali Yuksel, Hassan Sawaf', 'link': 'https://arxiv.org/abs/2502.04315', 'abstract': 'Recent advances in large language models (LLMs) have shown remarkable performance across diverse tasks. However, these models are typically deployed with fixed weights, which limits their ability to adapt dynamically to the variability inherent in real-world data during inference. This paper introduces ChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs by leveraging batch-aware clustering and on-the-fly generation of low-rank updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation (LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable masks), our method dynamically generates adaptive modifications to the decoder weights based on the aggregated statistics of clustered batches. By intelligently grouping similar inputs and computing context-aware low-rank updates via a hyper-network, ChamaleonLLM achieves significant performance gains, outperforming conventional LoRA methods while eliminating the overhead of maintaining multiple expert models. Our experiments highlight the potential of our approach to serve as a versatile and highly adaptive solution for language model inference. ChamaleonLLM is open-sourced to ensure the reproducibility of our experiments: this https URL', 'abstract_zh': '近年来，大规模语言模型（LLMs）在多种任务中的表现突出。然而，这些模型通常以固定权值的形式部署，这限制了它们在推理过程中动态适应真实世界数据固有变异性的能力。本文介绍了ChamaleonLLM，这是一种新型框架，通过利用批处理意识聚类和随机制低秩更新，实现了LLMs的推理时适应。与传统的细调方法如低秩适应（LoRA）或其他依赖于固定预学习动态掩码的方法不同，我们的方法能够根据聚类批次的聚合统计自适应地修改解码器权重。通过智能地分组相似输入并通过超网络计算上下文感知的低秩更新，ChamaleonLLM 达到了显著的性能提升，优于传统的 LoRA 方法，并且消除了维护多个专家模型的开销。实验结果展示了我们方法的潜力，作为一种既灵活又高度适应的解决方案，用于语言模型推理。ChamaleonLLM 已开源以确保实验的可重复性：[此链接]', 'title_zh': 'ChamaleonLLM：推理时聚类驱动的批次感知低秩自适应方法'}
{'arxiv_id': 'arXiv:2502.04314', 'title': 'BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation', 'authors': 'Omnilingual MT Team, Pierre Andrews, Mikel Artetxe, Mariano Coria Meglioli, Marta R. Costa-jussà, Joe Chuang, David Dale, Cynthia Gao, Jean Maillard, Alex Mourachko, Christophe Ropers, Safiyyah Saleem, Eduardo Sánchez, Ioannis Tsiamas, Arina Turkatenko, Albert Ventayol-Boada, Shireen Yates', 'link': 'https://arxiv.org/abs/2502.04314', 'abstract': "This paper presents BOUQuET, a multicentric and multi-register/domain dataset and benchmark, and its broader collaborative extension initiative. This dataset is handcrafted in non-English languages first, each of these source languages being represented among the 23 languages commonly used by half of the world's population and therefore having the potential to serve as pivot languages that will enable more accurate translations. The dataset is specially designed to avoid contamination and be multicentric, so as to enforce representation of multilingual language features. In addition, the dataset goes beyond the sentence level, as it is organized in paragraphs of various lengths. Compared with related machine translation (MT) datasets, we show that BOUQuET has a broader representation of domains while simplifying the translation task for non-experts. Therefore, BOUQuET is specially suitable for the open initiative and call for translation participation that we are launching to extend it to a multi-way parallel corpus to any written language.", 'abstract_zh': '本文介绍了BOUQuET，这是一个多中心、多语言领域/领域的数据集和基准测试集，以及其更广泛的协作扩展计划。该数据集首先在非英语语言中手工制作，其中这些源语言代表了世界上一半人口常用的23种语言，因此有可能作为枢轴语言，从而实现更准确的翻译。该数据集特别设计以避免数据污染，并且具有多中心性，以确保多语言语言特征的代表性。此外，该数据集不仅限于句级组织，而是按照各种长度的段落组织。与相关的机器翻译（MT）数据集相比，我们展示了BOUQuET在领域代表性和简化非专家翻译任务方面的更广泛范围。因此，BOUQuET特别适合我们发起的开放倡议和呼吁翻译参与，以将其扩展为任何书面语言的多向平行语料库。', 'title_zh': 'BOUQuET：跨领域的统一翻译质量评估数据集、基准与开源 initiative'}
{'arxiv_id': 'arXiv:2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'authors': 'Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, Bryon Aragam', 'link': 'https://arxiv.org/abs/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: this https URL', 'abstract_zh': '近年来，研究人员利用大型语言模型多智能体系统来解决复杂问题，并试图减少构建这些系统的手工努力，从而推动了自动智能体工作流优化方法的发展。然而，现有的方法由于表示限制、缺乏适应性和依赖离散优化技术时的不佳可扩展性，仍然不够灵活。我们通过ScoreFlow框架解决了这些问题，该框架使用高效的基于梯度的优化方法在连续空间中运行，简单且高性能。ScoreFlow整合了Score-DPO，这是一种直接偏好优化的新变体，能够考虑定量反馈。在涵盖问答、编程和数学推理的六个基准测试中，ScoreFlow在现有基线方法上实现了8.2%的改进。此外，它还使较小的模型在较低的推理成本下能够超越较大的模型。项目链接：[该项目链接]', 'title_zh': 'ScoreFlow：通过基于分数的偏好优化掌握LLM代理工作流'}
{'arxiv_id': 'arXiv:2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'authors': 'Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Cheng Peng', 'link': 'https://arxiv.org/abs/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at this https URL.', 'abstract_zh': '以下是将该论文内容或标题翻译成中文，并符合学术规范的版本：\n\n大型语言模型（LLMs）在各种任务中的表现显示出显著的能力，其实际应用效果往往由提示设计驱动。尽管近期研究主要集中在优化提示内容上，但提示格式——这一关键但常被忽视的维度——尚未得到系统性的研究。本文提出了一种创新方法，即内容-格式综合优化（Content-Format Integrated Prompt Optimization, CFPO），该方法通过迭代优化过程同时优化提示内容和格式。CFPO 利用自然语言变异来探索内容变化，并采用一种动态格式探索策略，系统性地评估多种格式选项。在多个任务和开源语言模型上的广泛评估表明，与仅优化内容的方法相比，CFPO 在性能上具有可测量的提升。这强调了综合内容和格式优化的重要性，并提供了一种鲁棒的、模型无关的方法来提升语言模型的性能。相关代码将发布在以下链接：[此处替换为实际链接]。\n\n请注意，上述翻译保留了原文的专业术语和学术风格。对于“this https URL”中的实际链接部分，需要根据实际情况进行替换。', 'title_zh': '超越提示内容：通过内容-格式集成提示优化提升大型语言模型性能'}
{'arxiv_id': 'arXiv:2502.04286', 'title': 'A Methodology for Studying Linguistic and Cultural Change in China, 1900-1950', 'authors': 'Spencer Dean Stewart', 'link': 'https://arxiv.org/abs/2502.04286', 'abstract': 'This paper presents a quantitative approach to studying linguistic and cultural change in China during the first half of the twentieth century, a period that remains understudied in computational humanities research. The dramatic changes in Chinese language and culture during this time call for greater reflection on the tools and methods used for text analysis. This preliminary study offers a framework for analyzing Chinese texts from the late nineteenth and twentieth centuries, demonstrating how established methods such as word counts and word embeddings can provide new historical insights into the complex negotiations between Western modernity and Chinese cultural discourse.', 'abstract_zh': '本文提出了一种定量研究方法，用以探讨二十世纪上半叶中国语言和文化变迁的情况。这一时期在中国计算人文研究中仍是一个相对未被充分研究的领域。这一时期中国语言和文化发生的剧烈变化，需要我们对文本分析所使用的工具和方法进行更加深刻的反思。本初步研究提供了分析十九世纪末至二十世纪中国文本的框架，展示了通过使用词频统计和词嵌入等传统方法，可以如何为西方现代性与汉语文化话语之间的复杂谈判提供新的历史见解。', 'title_zh': '1900-1950年中国语言和文化变迁的研究方法'}
{'arxiv_id': 'arXiv:2502.04269', 'title': 'How does a Multilingual LM Handle Multiple Languages?', 'authors': 'Santhosh Kakarla, Gautama Shastry Bulusu Venkata, Aishwarya Gaddam', 'link': 'https://arxiv.org/abs/2502.04269', 'abstract': 'Multilingual language models have significantly advanced due to rapid progress in natural language processing. Models like BLOOM 1.7B, trained on diverse multilingual datasets, aim to bridge linguistic gaps. However, their effectiveness in capturing linguistic knowledge, particularly for low-resource languages, remains an open question. This study critically examines MLMs capabilities in multilingual understanding, semantic representation, and cross-lingual knowledge transfer. While these models perform well for high-resource languages, they struggle with less-represented ones. Additionally, traditional evaluation methods often overlook their internal syntactic and semantic encoding.\nThis research addresses key limitations through three objectives. First, it assesses semantic similarity by analyzing multilingual word embeddings for consistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2 through Named Entity Recognition and sentence similarity tasks to understand their linguistic structures. Third, it explores cross-lingual knowledge transfer by evaluating generalization from high-resource to low-resource languages in sentiment analysis and text classification.\nBy leveraging linguistic probing, performance metrics, and visualizations, this study provides insights into the strengths and limitations of MLMs. The findings aim to enhance multilingual NLP models, ensuring better support for both high- and low-resource languages, thereby promoting inclusivity in language technologies.', 'abstract_zh': '多语言语言模型由于自然语言处理技术的迅速发展而取得了显著进步。像BLOOM 1.7B这样的模型，在多样化的多语言数据集上进行训练，旨在弥合语言差距，但它们在捕捉低资源语言语言知识方面的有效性仍是一个待解决的问题。本研究批判性地考察了这些多语言模型在多语言理解、语义表征和跨语言知识转移方面的能力。虽然这些模型在高资源语言方面表现良好，但在低资源语言方面却表现出困难。此外，传统的评估方法往往忽视了它们内部的句法和语义编码。\n\n本研究通过三个目标来解决这些关键限制。首先，通过分析使用余弦相似度的一致性来评估多语言词嵌入的语义相似性。其次，通过命名实体识别和句子相似性任务来考查BLOOM-1.7B和Qwen2的语言结构。第三，通过评估情感分析和文本分类中高资源语言到低资源语言的知识迁移，探讨跨语言知识转移。\n\n本研究利用语言探针、性能指标和可视化，为多语言模型的优势和局限性提供了见解。研究发现旨在增强多语言自然语言处理模型，确保更好地支持高资源和低资源语言，从而促进语言技术的包容性。', 'title_zh': '多语语言模型如何处理多种语言？'}
{'arxiv_id': 'arXiv:2502.04245', 'title': 'TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi', 'authors': 'Mohammed Amaan Dhamaskar, Rasika Ransing', 'link': 'https://arxiv.org/abs/2502.04245', 'abstract': "India's rich cultural and linguistic diversity poses various challenges in the domain of Natural Language Processing (NLP), particularly in Named Entity Recognition (NER). NER is a NLP task that aims to identify and classify tokens into different entity groups like Person, Location, Organization, Number, etc. This makes NER very useful for downstream tasks like context-aware anonymization. This paper details our work to build a multilingual NER model for the three most spoken languages in India - Hindi, Bengali & Marathi. We train a custom transformer model and fine tune a few pretrained models, achieving an F1 Score of 92.11 for a total of 6 entity groups. Through this paper, we aim to introduce a single model to perform NER and significantly reduce the inconsistencies in entity groups and tag names, across the three languages.", 'abstract_zh': '印度丰富多样的文化和语言背景在自然语言处理（NLP）领域，特别是在命名实体识别（NER）方面带来了诸多挑战。NER是一项旨在识别并将标记分类到不同实体组（如人物、地点、组织、数字等）的NLP任务。这使得NER在下游任务，如上下文感知的匿名化中非常有用。本文详细介绍了我们为印度三大语言——印地语、孟加拉语和马拉地语——构建一个多语言NER模型的工作。我们训练了一个定制的变压器模型，并微调了几个预训练模型，最终在6个实体组中达到了92.11的F1分数。通过本文，我们旨在介绍一个统一的模型来执行NER，并显著减少三种语言之间实体组和标签名称的一致性问题。', 'title_zh': 'TriNER：面向印地语、孟加拉语和马拉地语的命名实体识别模型系列'}
{'arxiv_id': 'arXiv:2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'authors': 'Xintong Hao, Ke Shen, Chenggang Li', 'link': 'https://arxiv.org/abs/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose \\textbf{MA}ssive \\textbf{G}enre-\\textbf{A}udience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'abstract_zh': '尽管大规模语言模型在各种任务中表现出色，其持续扩展仍面临一个关键挑战：高质量预训练数据的稀缺性。尽管模型架构不断进化，自然语言数据却难以随之扩展。为了解决这一瓶颈，我们提出了一种名为**大规模文体受众**（MAGA）的重构方法，该方法系统性地从现有语料库中生成多样且上下文丰富的预训练数据。本工作做出三项主要贡献：\n\n1. 本文提出了MAGA重构方法，这是一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含770B词元的MAGACorpus。\n2. 我们采用不同的数据预算扩展策略对MAGACorpus进行了评估，结果在不同规模的模型（134M至13B）上显示出一致的优化效果，证明了下一代大规模合成预训练语言模型的必要性。\n3. 通过全面分析，研究了提示工程对合成训练崩溃的影响，并揭示了传统崩溃检测指标在验证损失下的局限性。我们的工作表明，MAGA能够显著扩展训练数据集，同时保持数据质量，为模型超越数据限制的扩展提供了一条可靠途径。', 'title_zh': 'MAGA：大规模体裁受众重述以扩展预训练语料库'}
{'arxiv_id': 'arXiv:2502.04234', 'title': 'A Classification System Approach in Predicting Chinese Censorship', 'authors': 'Matt Prodani, Tianchu Ze, Yushen Hu', 'link': 'https://arxiv.org/abs/2502.04234', 'abstract': 'This paper is dedicated to using a classifier to predict whether a Weibo post would be censored under the Chinese internet. Through randomized sampling from \\citeauthor{Fu2021} and Chinese tokenizing strategies, we constructed a cleaned Chinese phrase dataset with binary censorship markings. Utilizing various probability-based information retrieval methods on the data, we were able to derive 4 logistic regression models for classification. Furthermore, we experimented with pre-trained transformers to perform similar classification tasks. After evaluating both the macro-F1 and ROC-AUC metrics, we concluded that the Fined-Tuned BERT model exceeds other strategies in performance.', 'abstract_zh': '本文致力于使用分类器预测微博帖子是否会被中国互联网审查制度审查。通过从 \\citeauthor{Fu2021} 中进行随机抽样和使用中文分词策略，我们构建了一个带有二元审查标记的清洗中文短语数据集。利用数据中的各种基于概率的信息检索方法，我们成功推导出4个逻辑回归模型。此外，我们还尝试使用预训练的变换器模型进行类似的分类任务。在评估了宏F1和ROC-AUC指标后，我们得出结论，微调的BERT模型在性能上优于其他策略。', 'title_zh': '一种分类系统方法在预测中国网络审查中的应用'}
{'arxiv_id': 'arXiv:2502.04218', 'title': "Sports and Women's Sports: Gender Bias in Text Generation with Olympic Data", 'authors': 'Laura Biester', 'link': 'https://arxiv.org/abs/2502.04218', 'abstract': "Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men's and women's events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men's event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics.", 'abstract_zh': '在先前的研究中，大型语言模型（LLMs）已被证明存在偏见，它们生成的文本往往符合关于世界的刻板印象，或者未能反映历史上边缘化群体的观点和价值观。在本研究中，我们提出使用奥林匹克运动会中男女项目的平行数据，来探讨语言模型中不同形式的性别偏见。我们定义了三个指标来衡量偏见，并发现当提示中的性别模糊时，模型对女性持持续的偏见。在这种情况下，模型经常只检索男赛事的结果，并且有时会明确或隐含地将其作为男赛事的结果，揭示了在体育领域中语言模型普遍存在性别偏见的问题。', 'title_zh': '体育与女子体育：基于奥林匹克数据的文本生成中的性别偏见'}
{'arxiv_id': 'arXiv:2502.04194', 'title': 'The Best Instruction-Tuning Data are Those That Fit', 'authors': 'Dylan Zhang, Qirun Dai, Hao Peng', 'link': 'https://arxiv.org/abs/2502.04194', 'abstract': "High-quality supervised fine-tuning (SFT) data are crucial for eliciting strong capabilities from pretrained large language models (LLMs). Typically, instructions are paired with multiple responses sampled from other LLMs, which are often out of the distribution of the target model to be fine-tuned. This, at scale, can lead to diminishing returns and even hurt the models' performance and robustness. We propose **GRAPE**, a novel SFT framework that accounts for the unique characteristics of the target model. For each instruction, it gathers responses from various LLMs and selects the one with the highest probability measured by the target model, indicating that it aligns most closely with the target model's pretrained distribution; it then proceeds with standard SFT training.\nWe first evaluate GRAPE with a controlled experiment, where we sample various solutions for each question in UltraInteract from multiple models and fine-tune commonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on GRAPE-selected data. GRAPE significantly outperforms strong baselines, including distilling from the strongest model with an absolute gain of up to 13.8%, averaged across benchmarks, and training on 3x more data with a maximum performance improvement of 17.3%. GRAPE's strong performance generalizes to realistic settings. We experiment with the post-training data used for Tulu3 and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data by 6.1% and a state-of-the-art data selection approach by 3% on average performance. Remarkably, using 1/3 of the data and half the number of epochs, GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.", 'abstract_zh': '高质量的监督微调（SFT）数据对于从预训练的大语言模型（LLMs）中激发强大的能力至关重要。通常，指令会与来自其他LLMs的多个响应配对，这些响应往往不符合目标模型的分布。在大规模应用中，这可能导致边际效益递减，甚至损害模型的性能和稳健性。我们提出了**GRAPE**，这是一种新颖的SFT框架，能够考虑到目标模型的独特特性。对于每个指令，它从各种LLMs中收集响应，并选择目标模型测量概率最高的响应，表明该响应最符合目标模型的预训练分布；然后进行标准的SFT训练。\n\n我们首先通过控制实验评估了GRAPE，从多个模型中抽取UltraInteract中的各种解决方案，并使用GRAPE选择的数据对LLaMA3.1-8B、Mistral-7B和Qwen2.5-7B等常用LMs进行微调。GRAPE显著优于强基准模型，包括从最强模型蒸馏，绝对增益最高可达13.8%，覆盖不同基准中的平均增益为13.8%，以及使用3倍多的数据训练，达到最大性能提升17.3%。GRAPE的强大性能能够推广到实际场景中。我们在Tulu3和Olmo-2的后训练数据中进行了实验。与训练在4.5倍更多数据上的强基准模型相比，GRAPE在平均性能上高出6.1%；与最先进的数据选择方法相比，平均性能提高3%。值得注意的是，使用数据量的1/3和一半的训练周期，GRAPE使LLaMA3.1-8B的性能超过了Tulu3-SFT 3.5%。', 'title_zh': '最佳的指令调优数据是那些符合需求的数据'}
{'arxiv_id': 'arXiv:2502.04173', 'title': 'Lexical Substitution is not Synonym Substitution: On the Importance of Producing Contextually Relevant Word Substitutes', 'authors': 'Juraj Vladika, Stephen Meisenbacher, Florian Matthes', 'link': 'https://arxiv.org/abs/2502.04173', 'abstract': "Lexical Substitution is the task of replacing a single word in a sentence with a similar one. This should ideally be one that is not necessarily only synonymous, but also fits well into the surrounding context of the target word, while preserving the sentence's grammatical structure. Recent advances in Lexical Substitution have leveraged the masked token prediction task of Pre-trained Language Models to generate replacements for a given word in a sentence. With this technique, we introduce ConCat, a simple augmented approach which utilizes the original sentence to bolster contextual information sent to the model. Compared to existing approaches, it proves to be very effective in guiding the model to make contextually relevant predictions for the target word. Our study includes a quantitative evaluation, measured via sentence similarity and task performance. In addition, we conduct a qualitative human analysis to validate that users prefer the substitutions proposed by our method, as opposed to previous methods. Finally, we test our approach on the prevailing benchmark for Lexical Substitution, CoInCo, revealing potential pitfalls of the benchmark. These insights serve as the foundation for a critical discussion on the way in which Lexical Substitution is evaluated.", 'abstract_zh': '词汇替换是指用一个相似的词替换句子中的一个词。这一替换不仅应是同义词，还应与目标词周围的上下文和谐，同时保持句子的语法结构。近年来，词汇替换任务利用预训练语言模型的遮掩词预测任务来生成给定句子中单词的替换词。通过这种方法，我们引入了ConCat，一种简单的增量方法，利用原始句子增强模型获取的语境信息。与现有方法相比，它在引导模型进行与目标词相关的预测方面非常有效。我们的研究包括定量评估，通过句子相似性和任务性能进行测量。此外，我们还进行了定性的用户分析，以验证用户更偏好我们方法提出的替换词，而不是以前的方法。最后，我们将我们的方法应用于当前的主要词汇替换基准数据集CoInCo，揭示了基准数据集的潜在缺陷。这些洞察为对词汇替换评估方式的批判性讨论提供了基础。', 'title_zh': '词项替换不等同于同义词替换：关于生成上下文相关词项替代理论的重要性'}
{'arxiv_id': 'arXiv:2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'authors': 'Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, Baobao Chang', 'link': 'https://arxiv.org/abs/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at this https URL.', 'abstract_zh': '指令遵循让现代大规模语言模型（LLMs）成为有帮助的助手。然而，控制LLMs执行复杂指令的关键仍然神秘莫测，因为开源社区训练的模型与领先公司训练的模型之间存在巨大差距。为了解决这一问题，我们提出了一种名为UltraIF的简单且可扩展的方法，用于使用开源数据构建能够遵循复杂指令的LLMs。UltraIF首先将现实生活中的用户请求分解为更简单的查询、约束以及相应的评估问题。然后，我们训练了一个UltraComposer来组合与约束相关的提示和评估问题。这一提示合成器不仅允许我们合成复杂的指令，还可以使用评估问题筛选响应。在我们的实验中，我们首次成功地将LLaMA-3.1-8B-Base与它的指令版本在5个指令遵循基准上对齐，仅使用8B模型作为生成器和评估器，而无需任何基准信息。对齐后的模型也在其他基准上取得了具有竞争力的分数。此外，我们还展示了UltraIF可以通过自对齐进一步提高LLaMA-3.1-8B-Instruct的能力，这激发了该方法更广泛的应用场景。我们的代码将在此处提供：这个https网址。', 'title_zh': 'UltraIF: 从自然环境提升指令跟随能力'}
{'arxiv_id': 'arXiv:2502.04134', 'title': 'The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs', 'authors': 'Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh', 'link': 'https://arxiv.org/abs/2502.04134', 'abstract': 'As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in closed-source LLMs by conducting experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation, however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.', 'abstract_zh': '随着大规模语言模型（LLMs）在各种应用中发挥越来越重要的作用，确保它们在不同输入条件下的可靠性变得至关重要。影响这一可靠性的关键问题是顺序敏感性，即输入排列的微小变动会导致输出结果不一致或存在偏差。尽管近期的研究已经降低了一些敏感性，但这一问题仍未得到彻底解决。本文通过在多个任务（包括重述、相关性判断和多项选择题）上进行实验，探讨了闭源LLMs中的顺序敏感性程度。实验结果显示，输入顺序对任务性能产生了显著影响，打乱输入会导致输出准确性下降。少量的提示技巧显示其效果不一，并能在一定程度上缓解该问题，但并未完全解决。这些发现强调了高风险应用场景下的持续风险，并指出了未来开发中需要更稳健的LLMs或改进的输入处理技术。', 'title_zh': '有序效应：探究封闭源代码大语言模型的提示敏感性'}
{'arxiv_id': 'arXiv:2502.04095', 'title': 'LLMs to Support a Domain Specific Knowledge Assistant', 'authors': 'Maria-Flavia Lovin', 'link': 'https://arxiv.org/abs/2502.04095', 'abstract': 'This work presents a custom approach to developing a domain specific knowledge assistant for sustainability reporting using the International Financial Reporting Standards (IFRS). In this domain, there is no publicly available question-answer dataset, which has impeded the development of a high-quality chatbot to support companies with IFRS reporting. The two key contributions of this project therefore are:\n(1) A high-quality synthetic question-answer (QA) dataset based on IFRS sustainability standards, created using a novel generation and evaluation pipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse QA pairs that address a wide spectrum of potential user queries in sustainability reporting. Various LLM-based techniques are employed to create the dataset, including chain-of-thought reasoning and few-shot prompting. A custom evaluation framework is developed to assess question and answer quality across multiple dimensions, including faithfulness, relevance, and domain specificity. The dataset averages a score range of 8.16 out of 10 on these metrics.\n(2) Two architectures for question-answering in the sustainability reporting domain - a RAG pipeline and a fully LLM-based pipeline. The architectures are developed by experimenting, fine-tuning, and training on the QA dataset. The final pipelines feature an LLM fine-tuned on domain specific data and an industry classification component to improve the handling of complex queries. The RAG architecture achieves an accuracy of 85.32% on single-industry and 72.15% on cross-industry multiple-choice questions, outperforming the baseline approach by 4.67 and 19.21 percentage points, respectively. The LLM-based pipeline achieves an accuracy of 93.45% on single-industry and 80.30% on cross-industry multiple-choice questions, an improvement of 12.80 and 27.36 percentage points over the baseline, respectively.', 'abstract_zh': '本文提出了一种定制方法，旨在利用国际财务报告准则（IFRS）开发一个专注于可持续报告的专用知识助手。在该领域中，没有公开可用的问答数据集，这阻碍了开发高质量的聊天机器人以支持使用IFRS进行报告的公司。因此，该项目的两大主要贡献是：\n(1) 基于IFRS可持续性标准的一个高质量合成问答（QA）数据集，该数据集利用大型语言模型（LLMs）的新型生成和评估管道创建。该数据集包含1,063个多样化的问答对，涵盖了可持续报告中潜在用户查询的广泛范围。使用多种基于LLM的技术来创建数据集，包括链式推理和少样本提示。开发了一个自定义评估框架来从多个维度评估问题和答案的质量，包括忠实性、相关性和领域特定性。这些指标下该数据集平均得分为8.16/10。\n(2) 在可持续报告领域中的两套问答架构——即检索与生成（Retrieval-Augmented Generation, RAG）流程和全基于LLM的流程。这些架构通过实验、微调和基于问答数据集进行训练而开发。最终的流程包括一个针对特定领域数据进行微调的LLM和一个行业分类组件，以提高处理复杂查询的能力。RAG架构在单一行业单选题中实现了85.32%的准确率，在跨行业多选题中实现了72.15%的准确率，分别超出基线方法4.67和19.21个百分点。基于LLM的流程在单一行业单选题中的准确率为93.45%，在跨行业多选题中的准确率为80.30%，分别提高了12.80和27.36个百分点。', 'title_zh': '支持特定领域知识助手的大规模语言模型'}
{'arxiv_id': 'arXiv:2502.04077', 'title': 'AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference', 'authors': 'Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li', 'link': 'https://arxiv.org/abs/2502.04077', 'abstract': 'With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through heuristic ranking with attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the \\textit{temporal patterns} in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based critical token identification approach. Specifically, AttentionPredictor learns a lightweight convolution model to capture spatiotemporal patterns and predict the next-token attention score. An appealing feature of AttentionPredictor is that it accurately predicts the attention score while consuming negligible memory. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 16$\\times$ KV cache compression with comparable LLM performance, significantly outperforming the state-of-the-art.', 'abstract_zh': '随着大型语言模型（LLMs）的发展，通过键值（KV）缓存压缩进行高效推理吸引了大量关注，尤其是在长上下文生成方面。为了压缩KV缓存，最近的方法通过带注意分数启发式排名来识别关键的KV令牌。然而，这些方法往往难以准确确定关键令牌，因为它们忽略了注意分数中的时间模式，导致LLM性能显著下降。为了解决这一挑战，我们提出了AttentionPredictor，这是首个基于学习的关键令牌识别方法。具体来说，AttentionPredictor 学习了一个轻量级卷积模型以捕获时空模式并预测下一个令牌的注意分数。AttentionPredictor 的一个吸引人的特点是，它在几乎不消耗额外内存的情况下准确预测注意力分数。此外，我们提出了一个跨令牌关键缓存预取框架，以隐藏令牌估计时间的开销来加速解码阶段。通过保留大量注意信息，AttentionPredictor 实现了16倍的KV缓存压缩，且保持了可比的LLM性能，显著优于现有最佳方法。', 'title_zh': '注意预测器：时间模式对于高效大型语言模型推理至关重要'}
{'arxiv_id': 'arXiv:2502.04075', 'title': 'Controllable Emotion Generation with Emotion Vectors', 'authors': 'Yurui Dong, Luozhijie Jin, Yao Yang, Bingjie Lu, Jiaxi Yang, Zhi Liu', 'link': 'https://arxiv.org/abs/2502.04075', 'abstract': "In recent years, technologies based on large-scale language models (LLMs) have made remarkable progress in many fields, especially in customer service, content creation, and embodied intelligence, showing broad application potential. However, The LLM's ability to express emotions with proper tone, timing, and in both direct and indirect forms is still insufficient but significant. Few works have studied on how to build the controlable emotional expression capability of LLMs. In this work, we propose a method for emotion expression output by LLMs, which is universal, highly flexible, and well controllable proved with the extensive experiments and verifications. This method has broad application prospects in fields involving emotions output by LLMs, such as intelligent customer service, literary creation, and home companion robots. The extensive experiments on various LLMs with different model-scales and architectures prove the versatility and the effectiveness of the proposed method.", 'abstract_zh': '近年来，基于大规模语言模型（LLMs）的技术在许多领域取得了显著进展，特别是在客户服务、内容创作和具身智能等领域显示出广泛的应用潜力。然而，LLMs在以适当语调、时机以及直接和间接的方式表达情绪方面的能力仍然不足但非常重要。很少有研究探讨如何构建LLMs的可控情绪表达能力。在本文中，我们提出了一种通用、高度灵活且可控的LLMs情绪表达输出方法，并通过广泛的实验和验证证明了该方法的有效性。该方法在涉及LLMs情绪输出的领域，如智能客户服务、文学创作和家庭伴侣机器人等方面具有广泛的应用前景。通过在不同模型规模和架构的多种LLMs上进行的广泛实验，证明了该方法的通用性和有效性。', 'title_zh': '可控情绪生成——基于情绪向量的方法'}
{'arxiv_id': 'arXiv:2502.04066', 'title': 'Predicting Large Language Model Capabilities on Closed-Book QA Tasks Using Only Information Available Prior to Training', 'authors': 'Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2502.04066', 'abstract': "The GPT-4 technical report from OpenAI suggests that model performance on specific tasks can be predicted prior to training, though methodologies remain unspecified. This approach is crucial for optimizing resource allocation and ensuring data alignment with target tasks. To achieve this vision, we focus on predicting performance on Closed-book Question Answering (CBQA) tasks, which are closely tied to pre-training data and knowledge retention. We address three major challenges: 1) mastering the entire pre-training process, especially data construction; 2) evaluating a model's knowledge retention; and 3) predicting task-specific knowledge retention using only information available prior to training. To tackle these challenges, we pre-train three large language models (i.e., 1.6B, 7B, and 13B) using 560k dollars and 520k GPU hours. We analyze the pre-training data with knowledge triples and assess knowledge retention using established methods. Additionally, we introduce the SMI metric, an information-theoretic measure that quantifies the relationship between pre-training data, model size, and task-specific knowledge retention. Our experiments reveal a strong linear correlation ($\\text{R}^2 > 0.84$) between the SMI metric and the model's accuracy on CBQA tasks across models of varying sizes (i.e., 1.1B, 1.6B, 7B, and 13B). The dataset, model, and code are available at this https URL.", 'abstract_zh': 'OpenAI发布的GPT-4技术报告表明，在训练之前可以预测模型在特定任务上的表现，尽管具体方法尚未明确。这种预测方法对于优化资源配置和确保数据与目标任务的一致性至关重要。为了实现这一愿景，我们关注预测封闭书本问答（Closed-book Question Answering, CBQA）任务的表现，这类任务与预训练数据和知识保留密切相关。我们主要应对三个主要挑战：1）掌握整个预训练过程，特别是数据构建；2）评估模型的知识保留情况；3）仅使用训练前的信息预测任务特定的知识保留情况。为解决这些挑战，我们使用560,000美元和520,000个GPU小时对三种大语言模型（即1.6B、7B和13B）进行了预训练。我们使用知识三元组分析预训练数据，并采用已建立的方法评估知识保留情况。此外，我们引入了SMI度量，这是一种信息论测量方法，用于量化预训练数据、模型大小与任务特定知识保留之间的关系。我们的实验显示，SMI度量与不同规模模型（即1.1B、1.6B、7B和13B）在CBQA任务上的准确性之间存在显著的线性相关性（$R^2 > 0.84$）。数据集、模型和代码可从以下网址获得：[此链接处]。\n\n请注意，这里的链接URL未给出，您需要将实际的链接地址填充到相应位置。', 'title_zh': '在仅使用训练前可用信息的情况下，预测大型语言模型在闭卷问答任务中的能力'}
{'arxiv_id': 'arXiv:2502.04038', 'title': 'Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents', 'authors': 'Yuchen Lian, Arianna Bisazza, Tessa Verhoef', 'link': 'https://arxiv.org/abs/2502.04038', 'abstract': "Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith & Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)'s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.", 'abstract_zh': '差异性案件标示（DCM）是指语法案件标示根据语义、语用或其他因素选择性地应用的现象。关于DCM的出现已经在针对人类参与者的人工语言学习实验中进行了研究，这些实验旨在分别探讨学习和交流的影响（Smith & Culbertson, 2020）。基于神经网络的多智能体强化学习框架因其能够模拟类似人类语言现象的出现而引起了广泛关注。在本研究中，我们采用这样一个框架，其中智能体首先习得一种人工语言，然后进行交际互动，从而可以直接与人类研究结果进行比较。使用一个非常通用的通信优化算法以及没有语言或语义偏好先验经验的神经网络学习者，我们的研究结果表明，仅学习并不能导致DCM的出现，但在智能体进行交流时，标记的差异性使用出现了。这支持了Smith和Culbertson（2020）的研究发现，强调了交流在塑造DCM中的关键作用，并展示了神经智能体模型在语言演化实验研究中补充作用的潜力。', 'title_zh': '使用通信神经网络代理模拟区分性_case_标记的 emergence'}
{'arxiv_id': 'arXiv:2502.04037', 'title': 'Exploring Imbalanced Annotations for Effective In-Context Learning', 'authors': 'Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei', 'link': 'https://arxiv.org/abs/2502.04037', 'abstract': 'Large language models (LLMs) have shown impressive performance on downstream tasks through in-context learning (ICL), which heavily relies on the demonstrations selected from annotated datasets. Existing selection methods may hinge on the distribution of annotated datasets, which can often be long-tailed in real-world scenarios. In this work, we show that imbalanced class distributions in annotated datasets significantly degrade the performance of ICL across various tasks and selection methods. Moreover, traditional rebalance methods fail to ameliorate the issue of class imbalance in ICL. Our method is motivated by decomposing the distributional differences between annotated and test datasets into two-component weights: class-wise weights and conditional bias. The key idea behind our method is to estimate the conditional bias by minimizing the empirical error on a balanced validation dataset and to employ the two-component weights to modify the original scoring functions during selection. Our approach can prevent selecting too many demonstrations from a single class while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of our method, improving the average accuracy by up to 5.46 on common benchmarks with imbalanced datasets.', 'abstract_zh': '大型语言模型（LLMs）通过上下文学习（ICL）在下游任务上展现了令人印象深刻的表现，这很大程度上依赖于从标注数据集中选择的演示示例。现有的选择方法可能依赖于标注数据集的分布，而在现实场景中，这种分布往往呈现出长尾分布的特点。在这项工作中，我们展示了标注数据集中不平衡的类别分布显著降低了各种任务和选择方法下ICL的表现。此外，传统的再平衡方法无法解决ICL中的类别不平衡问题。我们通过将标注数据集和测试数据集之间的分布差异分解为两类权重：类别权重和条件偏差，来解决这一问题。我们方法的核心思想是通过在平衡验证数据集上最小化经验误差来估计条件偏差，并利用这两种权重在选择过程中修改原始评分函数。我们的方式能够防止选择来自单个类别的示例过多，同时保持原始选择方法的有效性。广泛的实验表明，我们方法的有效性，在不平衡数据集的常见基准测试中，平均准确率最高可提高5.46%。', 'title_zh': '探索不平衡标注数据以实现有效的语境学习'}
{'arxiv_id': 'arXiv:2502.04022', 'title': 'Quantification of Biodiversity from Historical Survey Text with LLM-based Best-Worst Scaling', 'authors': 'Thomas Haider, Tobias Perschl, Malte Rehbein', 'link': 'https://arxiv.org/abs/2502.04022', 'abstract': 'In this study, we evaluate methods to determine the frequency of species via quantity estimation from historical survey text. To that end, we formulate classification tasks and finally show that this problem can be adequately framed as a regression task using Best-Worst Scaling (BWS) with Large Language Models (LLMs). We test Ministral-8B, DeepSeek-V3, and GPT-4, finding that the latter two have reasonable agreement with humans and each other. We conclude that this approach is more cost-effective and similarly robust compared to a fine-grained multi-class approach, allowing automated quantity estimation across species.', 'abstract_zh': '在本研究中，我们评估了通过历史调查文本的数量估计来确定物种频率的方法。为此，我们制定了分类任务，并最终展示了可以使用最佳最差标度法（BWS）与大规模语言模型（LLMs）将该问题适当地框定为回归任务。我们测试了Ministral-8B、DeepSeek-V3和GPT-4，发现后两者与人类以及其他模型之间具有合理的共识。我们得出结论，这种approach比细粒度的多类别方法更具成本效益且同样稳健，能够实现跨物种的自动化数量估计。', 'title_zh': '基于LLM的最优最劣标度法在历史调查文本中生物多样性量化中的应用'}
{'arxiv_id': 'arXiv:2502.03992', 'title': 'Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering', 'authors': 'Longquan Jiang, Junbo Huang, Cedric Möller, Ricardo Usbeck', 'link': 'https://arxiv.org/abs/2502.03992', 'abstract': 'Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code: \\href{this https URL}{this https URL}', 'abstract_zh': '以下是经过学术规范翻译后的文本：\n\n大多数现有的知识图谱问答（KGQA）方法都是为特定的知识图谱设计的，例如维基数据（Wikidata）、DBpedia或Freebase。由于底层图模式、拓扑结构和断言的异构性，大多数KGQA系统在没有密集训练数据的情况下无法被移植到未见过的知识图谱（KGs）中。我们提出了一种新的基于大型语言模型（LLM）的两阶段KGQA方法——OntoSCPrompt，该方法将语义解析与KG相关的交互过程分离。OntoSCPrompt首先生成SPARQL查询结构（包括SELECT、ASK、WHERE关键字及其缺失参数的占位符），然后填充KG特定的信息。为增强对底层KG的理解，我们提出了一种基于本体的混合提示学习策略，将KG本体整合到混合提示（例如离散向量和连续向量）的训练过程中。此外，我们还提出了一些具体任务的解码策略，以确保生成的SPARQL查询在两个阶段中正确且可执行。实验结果表明，OntoSCPrompt在诸如CWQ、WebQSP和LC-QuAD 1.0等KGQA数据集上，无需重新训练，便能高效地达到当前顶尖方法（SOTA）的性能，并能很好地泛化到未见过的特定领域KG，例如DBLP-QuAD和CoyPu KG。代码：\\href{此处提供链接}{此处提供链接}', 'title_zh': '基于本体引导的混合提示学习方法在知识图谱问答中的泛化能力提升'}
{'arxiv_id': 'arXiv:2502.03984', 'title': 'PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation', 'authors': 'Hyemin Lim, Jaeyeon Lee, Dong-Wan Choi', 'link': 'https://arxiv.org/abs/2502.03984', 'abstract': 'Large pretrained language models such as BERT suffer from slow inference and high memory usage, due to their huge size. Recent approaches to compressing BERT rely on iterative pruning and knowledge distillation, which, however, are often too complicated and computationally intensive. This paper proposes a novel semi-structured one-shot pruning method for BERT, called $\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high compression efficiency and sparsity while preserving accuracy. To this end, PGB identifies important groups of individual weights by permutation and prunes all other weights as a structure in both multi-head attention and feed-forward layers. Furthermore, if no important group is formed in a particular layer, PGB drops the entire layer to produce an even more compact model. Our experimental results on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the state-of-the-art structured pruning methods in terms of computational cost and accuracy preservation.', 'abstract_zh': '大型预训练语言模型如BERT因模型规模庞大而面临推理速度慢和高内存占用的问题。近年来对BERT的压缩方法主要依赖于迭代剪枝和知识蒸馏，然而这些方法通常过于复杂且计算成本高。本文提出了一种新的半结构化一次剪枝方法，称为“Permutation and Grouping for BERT”（PGB），该方法在保持准确性的前提下实现了高效压缩和稀疏化。PGB 通过排列识别出单一权重中的重要组，并在多头注意力层和前馈层中撤销其他所有权重。此外，如果某一层中没有形成重要组，PGB 将直接删除该层以生成更紧凑的模型。在BERT_BASE上的实验结果表明，PGB 在计算成本和准确率保持方面优于最新的结构化剪枝方法。', 'title_zh': 'PGB：通过权重分组和排列实现的BERT一次裁剪方法'}
{'arxiv_id': 'arXiv:2502.03954', 'title': 'MAQInstruct: Instruction-based Unified Event Relation Extraction', 'authors': 'Jun Xu, Mengshu Sun, Zhiqiang Zhang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.03954', 'abstract': 'Extracting event relations that deviate from known schemas has proven challenging for previous methods based on multi-class classification, MASK prediction, or prototype matching. Recent advancements in large language models have shown impressive performance through instruction tuning. Nevertheless, in the task of event relation extraction, instruction-based methods face several challenges: there are a vast number of inference samples, and the relations between events are non-sequential. To tackle these challenges, we present an improved instruction-based event relation extraction framework named MAQInstruct. Firstly, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions, which reduces the number of samples required for inference. Then, by incorporating a bipartite matching loss, we reduce the dependency of the instruction-based method on the generation sequence. Our experimental results demonstrate that MAQInstruct significantly improves the performance of event relation extraction across multiple LLMs.', 'abstract_zh': '基于多类分类、MASK预测或原型匹配的方法，抽取与已知模式相偏差的事件关系 proving 挑战重重。最近大语言模型的发展通过指令微调展现了令人印象深刻的性能。然而，在事件关系抽取任务中，基于指令的方法面临多个挑战：需要大量的推理样本，且事件之间的关系是非序贯的。为了应对这些挑战，我们提出了一种改进的基于指令的事件关系抽取框架，名为 MAQInstruct。首先，我们把任务从使用给定的事件-事件指令抽取事件关系，转变为使用给定的事件-关系指令选择事件，从而减少了所需的推理样本数量。然后，通过引入二分图匹配损失，我们减少了基于指令方法对生成序列的依赖性。我们的实验结果表明，MAQInstruct 显著提升了多种大语言模型在事件关系抽取任务中的性能。', 'title_zh': 'MAQInstruct：基于指令的统一事件关系抽取'}
{'arxiv_id': 'arXiv:2502.03945', 'title': 'Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond', 'authors': 'Mardhiyah Sanni, Tassallah Abdullahi, Devendra D. Kayande, Emmanuel Ayodele, Naome A. Etori, Michael S. Mollel, Moshood Yekini, Chibuzor Okocha, Lukman E. Ismaila, Folafunmi Omofoye, Boluwatife A. Adewale, Tobi Olatunji', 'link': 'https://arxiv.org/abs/2502.03945', 'abstract': 'Speech technologies are transforming interactions across various sectors, from healthcare to call centers and robots, yet their performance on African-accented conversations remains underexplored. We introduce Afrispeech-Dialog, a benchmark dataset of 50 simulated medical and non-medical African-accented English conversations, designed to evaluate automatic speech recognition (ASR) and related technologies. We assess state-of-the-art (SOTA) speaker diarization and ASR systems on long-form, accented speech, comparing their performance with native accents and discover a 10%+ performance degradation. Additionally, we explore medical conversation summarization capabilities of large language models (LLMs) to demonstrate the impact of ASR errors on downstream medical summaries, providing insights into the challenges and opportunities for speech technologies in the Global South. Our work highlights the need for more inclusive datasets to advance conversational AI in low-resource settings.', 'abstract_zh': '语音技术正在各行业领域改变着交流方式，从医疗保健到呼叫中心和机器人，然而它们在带有非洲口音的对话中的性能却尚未得到充分探索。我们引入了Afrispeech-Dialog，这是一个包含50个模拟医疗和非医疗领域非洲口音英语对话的基准数据集，旨在评估自动语音识别（ASR）及相关技术。我们评估了最先进的（SOTA）说话人聚类和ASR系统在长格式、带口音的语音上的性能，并与本地口音进行了比较，发现性能下降了10%以上。此外，我们还探讨了大语言模型（LLMs）在医疗对话摘要方面的能力，以此展示ASR错误对下游医疗摘要的影响，提供了有关全球南方地区语音技术面临的挑战和机遇的见解。我们的研究强调了需要更具包容性的数据集来促进低资源环境中的对话AI的发展。', 'title_zh': 'Afrispeech-Dialog：医疗保健及其他领域中自发英语对话的基准数据集'}
{'arxiv_id': 'arXiv:2502.03916', 'title': 'Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software', 'authors': 'Andreas Baumann, Peter Eberhard', 'link': 'https://arxiv.org/abs/2502.03916', 'abstract': "Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy. The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems. The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.", 'abstract_zh': '大型语言模型（LLMs）在文本生成方面越来越有帮助，甚至可以根据自然语言撰写的用户提示编写编程语言代码。它们甚至被应用于根据自然语言生成多体系统模拟模型。研究结果表明，LLMs不仅能够复制现有的代码示例，有些LLMs还被训练在开放源代码多体系统模拟代码上。然而，对于封闭源代码的模拟软件，这样的结果并不常见，因为它们的理念和概念可能与其他公开可用的软件不同。对于知识密集型任务，如模型创建，LLMs可能会产生错误的回答，尤其是在未知的封闭源代码模拟软件情况下。同样，对于其他内部知识，这些知识被保留在私有信息中以保护知识产权或数据隐私，也存在这种情况。检索增强生成（RAG）方法可能为这些知识密集型任务提供解决方案。本文探讨了RAG在封闭源代码模拟软件中的应用，并报告了初步实验。文章首先简要介绍了大型语言模型、RAG方法以及封闭源代码模拟软件所采用的模拟方法，然后提供了几个实验示例，以测试大型语言模型对模拟软件的知识以及利用两个RAG系统创建模拟模型的情况。这些示例表明了应用RAG系统于封闭源代码模拟软件的潜在益处，有助于获取其知识。然而，这些示例也揭示了所应用信息的不足之处，并提出了进一步研究中的开放问题。', 'title_zh': '大型语言模型在闭源仿真软件检索增强生成实验中的应用'}
{'arxiv_id': 'arXiv:2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'authors': 'Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, Caiming Xiong', 'link': 'https://arxiv.org/abs/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'abstract_zh': '大型语言模型（LLMs），如来自OpenAI的o1，展示了显著的推理能力。o1在回答问题之前会生成一个长链推理（LongCoT）。LongCoT使LLMs能够分析问题、制定计划、反思和回溯，这些操作赋予LLMs解决复杂问题的能力。在o1发布之后，许多研究团队尝试模仿其LongCoT和推理能力。在方法方面，他们主要依赖知识蒸馏，并使用具有LongCoT能力的现有模型的数据（例如OpenAI-o1、Qwen-QwQ、DeepSeek-R1-Preview），但系统发展这些推理能力仍然存在显著的不确定性和挑战。在数据领域方面，这些研究主要集中在数学问题上，少数研究包括编程领域，限制了其泛化能力。本文介绍了一种新的方法，能够在无需从类似o1的模型进行蒸馏或昂贵的人工注释的情况下，使LLMs具备LongCoT能力，我们通过标准指令模型自举长链推理（BOLT）实现这一目标。BOLT包括三个阶段：1）使用标准指令模型的上下文内学习自举LongCoT数据；2）监督微调LongCoT；3）在线训练进一步细化LongCoT能力。在BOLT中，只需要在自举阶段构建少量的上下文内示例；在我们的实验中，我们构建了10个示例，证明了该方法的可行性。我们使用Llama-3.1-70B-Instruct自举LongCoT，并将该方法应用于不同规模的模型（7B、8B、70B）。我们在Arena-Hard、MT-Bench、WildBench、ZebraLogic、MATH500等多种基准测试中实现了出色的表现，这些基准测试评估了多元任务解决能力和推理能力。', 'title_zh': 'BOLT：无需知识蒸馏的语言模型链式思考强化抽样'}
{'arxiv_id': 'arXiv:2502.03843', 'title': 'Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis', 'authors': 'Lin Yuan, Jun Xu, Honghao Gui, Mengshu Sun, Zhiqiang Zhang, Lei Liang, Jun Zhou', 'link': 'https://arxiv.org/abs/2502.03843', 'abstract': "High-quality, large-scale instructions are crucial for aligning large language models (LLMs), however, there is a severe shortage of instruction in the field of natural language understanding (NLU). Previous works on constructing NLU instructions mainly focus on information extraction (IE), neglecting tasks such as machine reading comprehension, question answering, and text classification. Furthermore, the lack of diversity in the data has led to a decreased generalization ability of trained LLMs in other NLU tasks and a noticeable decline in the fundamental model's general capabilities. To address this issue, we propose Hum, a large-scale, high-quality synthetic instruction corpus for NLU tasks, designed to enhance the NLU capabilities of LLMs. Specifically, Hum includes IE (either close IE or open IE), machine reading comprehension, text classification, and instruction generalist tasks, thereby enriching task diversity. Additionally, we introduce a human-LLMs collaborative mechanism to synthesize instructions, which enriches instruction diversity by incorporating guidelines, preference rules, and format variants. We conduct extensive experiments on 5 NLU tasks and 28 general capability evaluation datasets for LLMs. Experimental results show that Hum enhances the NLU capabilities of six LLMs by an average of 3.1\\%, with no significant decline observed in other general capabilities.", 'abstract_zh': '高质量的大规模指令对于对齐大规模语言模型（LLMs）至关重要，然而，在自然语言理解（NLU）领域，高质量的大规模指令极度稀缺。此前有关构建NLU指令的工作主要关注于信息提取（IE），忽视了机器阅读理解、问答和文本分类等任务。此外，数据多样性的缺乏导致训练好的LLMs在其他NLU任务中的泛化能力降低，并且基本模型的基础能力也有所下降。为解决这一问题，我们提出了Hum，这是一个大规模、高质量的合成指令语料库，旨在增强LLMs的NLU能力。具体而言，Hum包括信息提取（无论是封闭的IE还是开放的IE）、机器阅读理解、文本分类和指令通用任务，从而丰富了任务多样性。此外，我们引入了一种人-LLMs协作机制来合成指令，通过融合准则、偏好规则和格式变化，进一步丰富指令多样性。我们在5个NLU任务和28个基础能力评估数据集上进行了广泛的实验。实验结果表明，Hum可以平均提高6个LLMs的NLU能力3.1%，并且在其他基础能力方面未观察到显著下降。', 'title_zh': '通过大规模指令合成提高大规模语言模型的自然语言理解能力'}
{'arxiv_id': 'arXiv:2502.03827', 'title': 'A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions', 'authors': 'Zhiqiang Shi, Ruchit Agrawal', 'link': 'https://arxiv.org/abs/2502.03827', 'abstract': 'Sentiment Analysis, a popular subtask of Natural Language Processing, employs computational methods to extract sentiment, opinions, and other subjective aspects from linguistic data. Given its crucial role in understanding human sentiment, research in sentiment analysis has witnessed significant growth in the recent years. However, the majority of approaches are aimed at the English language, and research towards Arabic sentiment analysis remains relatively unexplored. This paper presents a comprehensive and contemporary survey of Arabic Sentiment Analysis, identifies the challenges and limitations of existing literature in this field and presents avenues for future research. We present a systematic review of Arabic sentiment analysis methods, focusing specifically on research utilizing deep learning. We then situate Arabic Sentiment Analysis within the broader context, highlighting research gaps in Arabic sentiment analysis as compared to general sentiment analysis. Finally, we outline the main challenges and promising future directions for research in Arabic sentiment analysis.', 'abstract_zh': '情感分析是自然语言处理的一个热门子任务，它使用计算方法从语言数据中提取情感、观点及其他主观内容。鉴于其在理解人类情感方面的重要性，情感分析的研究在近年来取得了显著的增长。然而，大多数方法主要针对英语，而阿拉伯语情感分析的研究相对较少。本文全面梳理了阿拉伯语情感分析的研究现状，指出了现有文献中的挑战和局限性，并提出了未来研究的途径。我们系统地回顾了阿拉伯语情感分析的方法，特别是那些利用深度学习的研究。随后，我们将阿拉伯语情感分析置于更广的背景下，突出了阿拉伯语情感分析与普通情感分析之间的研究空白。最后，我们概述了阿拉伯语情感分析研究的主要挑战及其充满潜力的未来方向。', 'title_zh': '当代阿拉伯情感分析综述：方法、挑战及未来方向'}
{'arxiv_id': 'arXiv:2502.03824', 'title': 'Syntriever: How to Train Your Retriever with Synthetic Data from LLMs', 'authors': 'Minsang Kim, Seungjun Baek', 'link': 'https://arxiv.org/abs/2502.03824', 'abstract': 'LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@$K$. The code is available at \\href{this https URL}{this https URL}.', 'abstract_zh': '大语言模型（LLMs）在许多AI应用中推动了进展。最近，有人尝试将LLMs的大量知识提炼到信息检索系统中。这些提炼方法大多使用了LLMs的输出概率，而在最新的黑盒LLMs中，这些输出概率是不可用的。我们提出了一种名为Syntriever的训练框架，用于使用黑盒LLMs生成的合成数据训练检索器。Syntriever包含两个阶段。首先，在提炼阶段，我们使用链式思考生成相关和可能不相关的段落以及增强查询。然后要求LLM对其生成的数据进行自我验证，以检查可能的幻觉，之后使用设计用于聚类相关段落嵌入的损失来训练检索器。其次，在对齐阶段，我们将检索器与LLMs的偏好对齐。我们提出了一种偏好的建模方法，即部分Plackett-Luce排名，以通过正则化学习LLMs的偏好，从而防止模型在提炼阶段训练时出现过度偏离。实验表明，Syntriever在不同领域的基准数据集上实现了最先进的nDCG@K性能。代码可以在\\href{这个链接}{这个链接}获取。', 'title_zh': 'Syntriever：如何使用大型语言模型生成的合成数据训练检索模型'}
{'arxiv_id': 'arXiv:2502.03821', 'title': 'PsyPlay: Personality-Infused Role-Playing Conversational Agents', 'authors': 'Tao Yang, Yuhua Zhu, Xiaojun Quan, Cong Liu, Qifan Wang', 'link': 'https://arxiv.org/abs/2502.03821', 'abstract': 'The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.', 'abstract_zh': '当前对基于大型语言模型（LLMs）的角色扮演对话代理（RPCAs）的研究主要集中在模仿特定的说话风格和利用角色背景，而忽视了深度个性特征的描绘。本研究引入了融入个性的角色扮演方法，鼓励代理在对话中准确地展现其指定的个性特征。随后，我们提出了PsyPlay对话生成框架，该框架促进了多个LLM代理之间丰富个性的表现。具体而言，PsyPlay允许代理承担具有不同个性特征的角色，并围绕特定主题展开讨论，整个互动过程中一贯展现其指定的个性特征。生成的对话数据验证结果显示，PsyPlay能够准确地展现预期的个性特征，对GPT-3.5的总体成功率达到了80.31%。此外，我们观察到，与正面价值观对齐的LLM在表现正面个性角色方面比负面角色更成功。此外，我们构建了一个用于个性融入的角色扮演的数据集，称为PsyPlay-Bench。该数据集包含4745个使用PsyPlay正确表现的对话实例，旨在进一步促进个性化角色扮演和对话个性识别的研究。', 'title_zh': 'PsyPlay：融入人格特征的角色扮演对话代理'}
{'arxiv_id': 'arXiv:2502.03805', 'title': 'Identify Critical KV Cache in LLM Inference from an Output Perturbation Perspective', 'authors': 'Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S Kevin Zhou', 'link': 'https://arxiv.org/abs/2502.03805', 'abstract': "Large language models have revolutionized natural language processing but face significant challenges of high storage and runtime costs, due to the transformer architecture's reliance on self-attention, particularly the large Key-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV cache size by pruning less critical entries based on attention weights remain empirical and lack formal grounding. This paper presents a formal study on identifying critical KV cache entries by analyzing attention output perturbation. Our analysis reveals that, beyond attention weights, the value states within KV entries and pretrained parameter matrices are also crucial. Based on this, we propose a perturbation-constrained selection algorithm that optimizes the worst-case output perturbation to identify critical entries. Evaluations on the Needle-in-a-Haystack test and Longbench benchmark show our algorithm enhances state-of-the-art cache eviction methods. Further empirical analysis confirms that our algorithm achieves lower output perturbations in over 92% attention heads in Llama model, thereby providing a significant improvement over existing methods.", 'abstract_zh': '大规模语言模型已经彻底改变了自然语言处理，但它们由于自注意力机制，尤其是长序列推理时依赖的大型键值（KV）缓存而面临着显著的存储和运行成本挑战。通过剪枝基于注意力权重的非关键条目来缩减KV缓存大小的近期努力仍处于经验阶段，缺乏正式的理论依据。本文对通过分析注意力输出扰动来识别关键KV缓存条目的关键性进行了正式研究。我们的分析表明，除了注意力权重外，KV条目内的值状态以及预训练参数矩阵也至关重要。基于此，我们提出了一种受限扰动选择算法，以优化最坏情况下的输出扰动来识别关键条目。在Needle-in-a-Haystack测试和LongBench基准上的评估表明，我们的算法增强了最先进的缓存淘汰方法。进一步的经验分析证实，我们的算法在Llama模型的超过92%的注意力头中实现了更低的输出扰动，从而显著优于现有方法。', 'title_zh': '从输出扰动视角识别LLM推理中的关键KV缓存'}
{'arxiv_id': 'arXiv:2502.03799', 'title': 'Enhancing Hallucination Detection through Noise Injection', 'authors': 'Litian Liu, Reza Pourreza, Sunny Panchal, Apratim Bhattacharyya, Yao Qin, Roland Memisevic', 'link': 'https://arxiv.org/abs/2502.03799', 'abstract': 'Large Language Models (LLMs) are prone to generating plausible yet incorrect responses, known as hallucinations. Effectively detecting hallucinations is therefore crucial for the safe deployment of LLMs. Recent research has linked hallucinations to model uncertainty, suggesting that hallucinations can be detected by measuring dispersion over answer distributions obtained from a set of samples drawn from a model. While drawing from the distribution over tokens defined by the model is a natural way to obtain samples, in this work, we argue that it is sub-optimal for the purpose of detecting hallucinations. We show that detection can be improved significantly by taking into account model uncertainty in the Bayesian sense. To this end, we propose a very simple and efficient approach that perturbs an appropriate subset of model parameters, or equivalently hidden unit activations, during sampling. We demonstrate its effectiveness across a wide range of datasets and model architectures.', 'abstract_zh': '大型语言模型（LLMs）可能会生成虽然听起来合理但实际上是错误的响应，这种现象被称为幻觉。因此，有效地检测幻觉对于大型语言模型的安全部署至关重要。近期的研究将幻觉与模型不确定性联系起来，表明可以通过测量模型生成的一组样本的答案分布来检测幻觉。虽然从模型定义的令牌分布中抽取样本是一种自然的方法，但在这项工作中，我们提出这种做法在检测幻觉方面并不理想。我们证明，通过在贝叶斯意义上考虑模型不确定性，可以显著提高检测效果。为此，我们提出了一种简单且高效的抽样方法，即在抽样过程中对模型参数的适当子集，或等价地隐藏单元激活进行扰动。我们展示了该方法在多种数据集和模型架构上的有效性。', 'title_zh': '通过噪声注入提高幻觉检测效果'}
{'arxiv_id': 'arXiv:2502.03793', 'title': "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers", 'authors': 'Benjamin Clavié, Nathan Cooper, Benjamin Warner', 'link': 'https://arxiv.org/abs/2502.03793', 'abstract': "While encoder-only models such as BERT and ModernBERT are ubiquitous in real-world NLP applications, their conventional reliance on task-specific classification heads can limit their applicability compared to decoder-based large language models (LLMs). In this work, we introduce ModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its masked language modelling (MLM) head for generative classification. Our approach employs an intentionally simple training loop and inference mechanism that requires no heavy pre-processing, heavily engineered prompting, or architectural modifications. ModernBERT-Large-Instruct exhibits strong zero-shot performance on both classification and knowledge-based tasks, outperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's MMLU performance with 60% less parameters. We also demonstrate that, when fine-tuned, the generative approach using the MLM head matches or even surpasses traditional classification-head methods across diverse NLU this http URL capability emerges specifically in models trained on contemporary, diverse data mixes, with models trained on lower volume, less-diverse data yielding considerably weaker performance. Although preliminary, these results demonstrate the potential of using the original generative masked language modelling head over traditional task-specific heads for downstream tasks. Our work suggests that further exploration into this area is warranted, highlighting many avenues for future improvements.", 'abstract_zh': '尽管BERT和ModernBERT这类编码器-only模型在实际的自然语言处理(NLP)应用中无处不在，但它们依赖于特定任务的分类头部，这在适用性上不如基于解码器的大规模语言模型(LLMs)。本文中，我们提出了ModernBERT-Large-Instruct，这是一种包含0.4B参数的编码器模型，它利用其掩码语言模型(MLM)头部进行生成性分类。我们的方法采用了一个简单且故意设计的训练循环和推理机制，无需进行重大的预处理、高度工程化的提示或架构修改。ModernBERT-Large-Instruct 在零样本性能上表现出色，不仅在分类任务上，也在基于知识的任务上同样表现突出。在MMLU基准测试上，它与相同规模的LLMs相比表现出色，并且使用60%较少的参数达到了Llama3-1B 93%的性能。我们还展示了，当进行微调时，使用MLM头部的生成方法在不同的NLU任务上与传统分类头部方法相比表现相当甚至更优。这一能力尤其在训练数据多样且丰富的模型中表现显著，而在训练数据较少且不那么多样化的模型中则表现较差。尽管这些结果是初步的，但它们表明，使用原始的生成性掩码语言模型头部替代传统的特定任务头部可能在下游任务中有很大的潜力。我们的研究表明，进一步探索这一领域是值得的，并为未来改进提供了多个方向。', 'title_zh': '《尽在掩码之中：简单的指令调优 enables 类似 BERT 的掩码语言模型作为生成分类器》\n\n注意：这里的[MASK]在原文中被替换为中文中常见的掩码词“掩码”，同时保持了英文原文中的“enables”和“as”的翻译准确性，以符合学术规范。原文中的“[MASK]”似乎是填补模型名称的地方，实际模型名称应该根据上下文填补完整。如果有具体模型名称应替换，可以进一步提供信息以便更加准确地翻译。'}
{'arxiv_id': 'arXiv:2502.03766', 'title': 'Hierarchical Contextual Manifold Alignment for Structuring Latent Representations in Large Language Models', 'authors': 'Meiquan Dong, Haoran Liu, Yan Huang, Zixuan Feng, Jianhong Tang, Ruoxi Wang', 'link': 'https://arxiv.org/abs/2502.03766', 'abstract': 'The organization of latent token representations plays a crucial role in determining the stability, generalization, and contextual consistency of language models, yet conventional approaches to embedding refinement often rely on parameter modifications that introduce additional computational overhead. A hierarchical alignment method was introduced to restructure token embeddings without altering core model weights, ensuring that representational distributions maintained coherence across different linguistic contexts. Experimental evaluations demonstrated improvements in rare token retrieval, adversarial robustness, and long-range dependency tracking, highlighting the advantages of hierarchical structuring in mitigating inconsistencies in latent space organization. The comparative analysis against conventional fine-tuning and embedding perturbation methods revealed that hierarchical restructuring maintained computational efficiency while achieving measurable gains in representation quality. Structural refinements introduced through the alignment process resulted in improved contextual stability across varied linguistic tasks, reducing inconsistencies in token proximity relationships and enhancing interpretability in language generation. A detailed computational assessment confirmed that the realignment process introduced minimal inference overhead, ensuring that representational improvements did not compromise model efficiency. The findings reinforced the broader significance of structured representation learning, illustrating that hierarchical embedding modifications could serve as an effective strategy for refining latent space distributions while preserving pre-learned semantic associations.', 'abstract_zh': '潜词表示的组织对于语言模型的稳定性和泛化能力具有重要作用，且这些模型的上下文一致性也受到潜词表示组织的影响。然而，传统的嵌入改进方法常常依赖于参数修改，这会增加额外的计算负担。为此，提出了一种分层对齐方法，以重组词嵌入而不改变核心模型权重，从而确保不同语言上下文中表示分布的一致性。实验评估表明，该方法在稀有词检索、对抗鲁棒性和长距离依赖跟踪方面表现出显著改进，突显了分层结构在缓解潜空间组织不一致性方面的优势。与传统微调和嵌入扰动方法的比较分析表明，分层重组在保持计算效率的同时，能够显著提升表示质量。通过对齐过程引入的结构优化提高了跨不同语言任务的上下文一致性，减少了词之间距离关系的不一致性，并增强了语言生成的可解释性。详细的计算评估证实，重新对齐过程引入的推理负担很小，确保了表示改进不会牺牲模型效率。研究结果强化了结构表示学习的重要性，表明分层嵌入修改可以作为一种有效的策略来优化潜空间分布，同时保留预先学习的语义关联。', 'title_zh': '大型语言模型中层次上下文流形对齐的结构化潜在表示方法'}
{'arxiv_id': 'arXiv:2502.03748', 'title': 'Rethinking the Residual Distribution of Locate-then-Editing Methods in Model Editing', 'authors': 'Xiaopeng Li, Shanwen Wang, Shasha Li, Shezheng Song, Bin Ji, Jun Ma, Jie Yu', 'link': 'https://arxiv.org/abs/2502.03748', 'abstract': "Model editing is a powerful technique for updating the knowledge of Large Language Models (LLMs). Locate-then-edit methods are a popular class of approaches that first identify the critical layers storing knowledge, then compute the residual of the last critical layer based on the edited knowledge, and finally perform multi-layer updates using a least-squares solution by evenly distributing the residual from the first critical layer to the last. Although these methods achieve promising results, they have been shown to degrade the original knowledge of LLMs. We argue that residual distribution leads to this issue. To explore this, we conduct a comprehensive analysis of residual distribution in locate-then-edit methods from both empirical and theoretical perspectives, revealing that residual distribution introduces editing errors, leading to inaccurate edits. To address this issue, we propose the Boundary Layer UpdatE (BLUE) strategy to enhance locate-then-edit methods. Sequential batch editing experiments on three LLMs and two datasets demonstrate that BLUE not only delivers an average performance improvement of 35.59\\%, significantly advancing the state of the art in model editing, but also enhances the preservation of LLMs' general capabilities. Our code is available at this https URL.", 'abstract_zh': '模型编辑是更新大型语言模型（LLMs）知识的一种强大技术。定位-然后编辑方法是一类常见的方法，这些方法首先识别存储知识的关键层，然后基于编辑后的知识计算最后一个关键层的残差，最后通过均匀分配残差从第一个关键层到最后一个关键层的方式进行多层更新。尽管这些方法取得了令人鼓舞的结果，但它们已经被证明会降低LLMs的原始知识。我们认为残差分布导致了这一问题。为了探索这个问题，我们从经验和理论两个方面全面分析了定位-然后编辑方法中的残差分布，揭示了残差分布引入了编辑错误，导致编辑结果不准确。为了解决这一问题，我们提出了边界层更新（Boundary Layer Update, BLUE）策略来增强定位-然后编辑方法。对三种LLMs和两个数据集进行的顺序批量编辑实验表明，BLUE不只为模型编辑带来了35.59%的平均性能提升，显著推进了模型编辑技术的边界，同时也增强了LLMs的一般能力。我们的代码可在以下链接获取：[链接]。', 'title_zh': '重新思考定位-编辑方法中模型编辑的残差分布'}
{'arxiv_id': 'arXiv:2502.03711', 'title': 'MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers', 'authors': 'Nicole Cho, William Watson', 'link': 'https://arxiv.org/abs/2502.03711', 'abstract': "One critical challenge in the institutional adoption journey of Large Language Models (LLMs) stems from their propensity to hallucinate in generated responses. To address this, we propose MultiQ&A, a systematic approach for evaluating the robustness and consistency of LLM-generated answers. We demonstrate MultiQ&A's ability to crowdsource question perturbations and their respective answers through independent LLM agents at scale. Our experiments culminated in the examination of 1.9 million question perturbations and 2.3 million answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as gpt-3.5-turbo, remain relatively robust and consistent under perturbations. MultiQ&A provides clarity in the response generation space, offering an effective method for inspecting disagreements and variability. Therefore, our system offers a potential framework for institutional LLM adoption with the ability to measure confidence, consistency, and the quantification of hallucinations.", 'abstract_zh': '大型语言模型（LLMs）在机构采用过程中的一个关键挑战在于它们生成回答时容易产生幻觉。为应对这一问题，我们提出了一种系统性的方法——MultiQ&A，用于评估LLM生成答案的稳健性和一致性。我们展示了MultiQ&A如何通过独立的LLM代理大规模地众包问题扰动及其相应的答案。我们的实验涵盖了190万个问题扰动和230万个答案。此外，MultiQ&A表明，如gpt-3.5-turbo等集成的LLM在扰动下仍然相对稳健和一致。MultiQ&A在回答生成空间提供了清晰性，提供了一种有效的方法来检查分歧和变异性。因此，我们的系统提供了一种潜在的框架，有助于机构采用LLM，并能够度量信心、一致性以及幻觉的数量化。', 'title_zh': 'MultiQ&A：通过自动化众包的疑问扰动和答案生成衡量稳健性的分析'}
{'arxiv_id': 'arXiv:2502.03708', 'title': 'Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers', 'authors': 'Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin', 'link': 'https://arxiv.org/abs/2502.03708', 'abstract': "A trained Large Language Model (LLM) contains much of human knowledge. Yet, it is difficult to gauge the extent or accuracy of that knowledge, as LLMs do not always ``know what they know'' and may even be actively misleading. In this work, we give a general method for detecting semantic concepts in the internal activations of LLMs. Furthermore, we show that our methodology can be easily adapted to steer LLMs toward desirable outputs. Our innovations are the following: (1) we use a nonlinear feature learning method to identify important linear directions for predicting concepts from each layer; (2) we aggregate features across layers to build powerful concept detectors and steering mechanisms. We showcase the power of our approach by attaining state-of-the-art results for detecting hallucinations, harmfulness, toxicity, and untruthful content on seven benchmarks. We highlight the generality of our approach by steering LLMs towards new concepts that, to the best of our knowledge, have not been previously considered in the literature, including: semantic disambiguation, human languages, programming languages, hallucinated responses, science subjects, poetic/Shakespearean English, and even multiple concepts simultaneously. Moreover, our method can steer concepts with numerical attributes such as product reviews. We provide our code (including a simple API for our methods) at this https URL .", 'abstract_zh': '经过训练的大语言模型（LLM）包含了大量的人类知识。然而，很难评估这些知识的范围和准确性，因为LLM们并不总是知道自己知道什么，有时甚至会主动误导。在这项工作中，我们提出了一种通用方法来检测LLM内部激活中的语义概念。此外，我们展示了我们的方法可以轻松地调整，引导LLM生成期望的输出。我们的创新之处在于：（1）我们使用非线性特征学习方法来识别从每一层预测概念的重要线性方向；（2）我们跨层聚合特征，构建强大的概念检测器和引导机制。我们通过在七个基准测试中达到最先进的检测幻觉、危害性、毒性及不实内容的结果，展示了我们方法的能力。我们通过引导LLM生成以前文献中尚未考虑的新概念，突显了我们方法的通用性，这些新概念包括：语义消歧、人类语言、编程语言、幻觉回答、科学学科、诗体/莎士比亚英语，甚至同时处理多个概念。此外，我们的方法可以引导具有数值属性的概念，例如产品评论。我们已将代码（包括我们方法的简单API）发布在如下网址：[此链接]。', 'title_zh': '聚合与征服：通过结合多层上的非线性预测器来检测和引导大模型概念'}
{'arxiv_id': 'arXiv:2502.03699', 'title': 'LLM Alignment as Retriever Optimization: An Information Retrieval Perspective', 'authors': 'Bowen Jin, Jinsung Yoon, Zhen Qin, Ziqi Wang, Wei Xiong, Yu Meng, Jiawei Han, Sercan O. Arik', 'link': 'https://arxiv.org/abs/2502.03699', 'abstract': "Large Language Models (LLMs) have revolutionized artificial intelligence with capabilities in reasoning, coding, and communication, driving innovation across industries. Their true potential depends on effective alignment to ensure correct, trustworthy and ethical behavior, addressing challenges like misinformation, hallucinations, bias and misuse. While existing Reinforcement Learning (RL)-based alignment methods are notoriously complex, direct optimization approaches offer a simpler alternative. In this work, we introduce a novel direct optimization approach for LLM alignment by drawing on established Information Retrieval (IR) principles. We present a systematic framework that bridges LLM alignment and IR methodologies, mapping LLM generation and reward models to IR's retriever-reranker paradigm. Building on this foundation, we propose LLM Alignment as Retriever Preference Optimization (LarPO), a new alignment method that enhances overall alignment quality. Extensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 % averaged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work opens new avenues for advancing LLM alignment by integrating IR foundations, offering a promising direction for future research.", 'abstract_zh': '大语言模型（LLMs）通过在推理、编程和沟通方面的能力，推动了人工智能领域的创新，影响了各行各业。它们的真正潜力取决于有效的对齐，以确保正确、可信和道德的行为，解决诸如误导信息、幻觉、偏见和误用等问题。虽然现有的基于强化学习（RL）的对齐方法非常复杂，但直接优化方法提供了更简单的选择。在本研究中，我们引入了一种基于信息检索（IR）原理的新型直接优化方法来对齐LLMs。我们提出了一种系统框架，将LLM对齐和IR方法相结合，将LLM生成和奖励模型映射到IR的检索再排序范式。在此基础上，我们提出了LLM对齐检索偏好优化（LarPO），这是一种新的对齐方法，可以提高整体对齐质量。广泛的实验验证了LarPO的有效性，分别在AlpacaEval2和MixEval-Hard上取得了38.9%和13.7%的平均改进。我们的研究通过整合IR基础，打开了LLM对齐的全新途径，为未来研究提供了有前景的方向。', 'title_zh': 'LLM对齐作为检索优化：从信息检索角度出发'}
{'arxiv_id': 'arXiv:2502.03688', 'title': 'A Comparison of DeepSeek and Other LLMs', 'authors': 'Tianchen Gao, Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef', 'link': 'https://arxiv.org/abs/2502.03688', 'abstract': 'Recently, DeepSeek has been the focus of attention in and beyond the AI community. An interesting problem is how DeepSeek compares to other large language models (LLMs). There are many tasks an LLM can do, and in this paper, we use the task of predicting an outcome using a short text for comparison. We consider two settings, an authorship classification setting and a citation classification setting. In the first one, the goal is to determine whether a short text is written by human or AI. In the second one, the goal is to classify a citation to one of four types using the textual content. For each experiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and Llama.\nWe find that, in terms of classification accuracy, DeepSeek outperforms Gemini, GPT, and Llama in most cases, but underperforms Claude. We also find that DeepSeek is comparably slower than others but with a low cost to use, while Claude is much more expensive than all the others. Finally, we find that in terms of similarity, the output of DeepSeek is most similar to those of Gemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most similar outputs).\nIn this paper, we also present a fully-labeled dataset collected by ourselves, and propose a recipe where we can use the LLMs and a recent data set, MADStat, to generate new data sets. The datasets in our paper can be used as benchmarks for future study on LLMs.', 'abstract_zh': '近年来，DeepSeek 在人工智能领域及其之外引起了广泛关注。一个有趣的问题是 DeepSeek 与其他大型语言模型（LLMs）相比的表现如何。大型语言模型可以执行多种任务，本文通过使用基于短文本预测结果的任务来进行比较。我们考虑了两种设置：作者身份分类和引用分类设置。在第一种设置中，目标是确定一段短文本是由人类还是AI创作的。在第二种设置中，目标是根据文本内容将引用分类为四种类型之一。对于每个实验，我们将 DeepSeek 与其他四种流行的大语言模型（Claude、Gemini、GPT 和 Llama）进行比较。\n\n研究发现，在分类准确性方面，除 Claude 之外，DeepSeek 在大多数情况下优于 Gemini、GPT 和 Llama。我们也发现，DeepSeek 的运行速度与其他模型相比稍慢，但使用成本较低；而 Claude 的成本远高于其他所有模型。最后，我们发现，在相似性方面，DeepSeek 的输出与其他模型（尤其是 Gemini 和 Claude）最为相似（在五种语言模型中，Claude 和 Gemini 的输出最为相似）。\n\n在本文中，我们还提供了一个由我们自己收集的完整标注数据集，并提出了一种方法，利用大语言模型和最近的数据集 MADStat 生成新的数据集。我们论文中的数据集可以作为未来研究大语言模型的基准。', 'title_zh': '《DeepSeek与其他大型语言模型的比较》'}
{'arxiv_id': 'arXiv:2502.03685', 'title': 'Controlled LLM Decoding via Discrete Auto-regressive Biasing', 'authors': 'Patrick Pynadath, Ruqi Zhang', 'link': 'https://arxiv.org/abs/2502.03685', 'abstract': "Controlled text generation allows for enforcing user-defined constraints on large language model outputs, an increasingly important field as LLMs become more prevalent in everyday life. One common approach uses energy-based decoding, which defines a target distribution through an energy function that combines multiple constraints into a weighted average. However, these methods often struggle to balance fluency with constraint satisfaction, even with extensive tuning of the energy function's coefficients. In this paper, we identify that this suboptimal balance arises from sampling in continuous space rather than the natural discrete space of text tokens. To address this, we propose Discrete Auto-regressive Biasing, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain. Specifically, we introduce a new formulation for controlled text generation by defining a joint distribution over the generated sequence and an auxiliary bias sequence. To efficiently sample from this joint distribution, we propose a Langevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC. Our method significantly improves constraint satisfaction while maintaining comparable or better fluency, all with even lower computational costs. We demonstrate the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation.", 'abstract_zh': '受控文本生成允许用户在大型语言模型输出上施加自定义约束，这是一个随着大型语言模型在日常生活中的普及而变得越来越重要的领域。一种常见的方法是使用基于能量的解码，该方法通过一个能量函数来定义目标分布，该能量函数将多种约束加权平均结合在一起。然而，这些方法往往难以平衡流畅性和约束满足，即便对能量函数的系数进行了广泛的调整。在本论文中，我们发现这种次优平衡源于在连续空间中进行采样，而不是在文本令牌的自然离散空间中进行采样。为了解决这一问题，我们提出了一种受控自回归偏差算法——离散自回归偏差，该算法利用梯度同时完全在离散文本域中运行。具体而言，我们通过定义生成序列和辅助偏差序列的联合分布，引入了一种新的受控文本生成公式。为了高效地从该联合分布中采样，我们提出了一种基于梯度的离散MCMC内的拉angevin采样算法。我们的方法在保持或提高流畅性的同时，显著提高了约束满足度，并且计算成本更低。我们通过情感控制、语言去污和关键词导向生成展示了我们受控解码方法的优势。', 'title_zh': '通过离散自回归偏置进行的受控大语言模型解码'}
{'arxiv_id': 'arXiv:2502.03678', 'title': 'Reflection-Window Decoding: Text Generation with Selective Refinement', 'authors': 'Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan Shen, Guangyi Chen, Peter Spirtes, Kun Zhang', 'link': 'https://arxiv.org/abs/2502.03678', 'abstract': 'The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.', 'abstract_zh': '在大规模语言模型（LLMs）中，自回归解码方法因其生成文本时缺乏内置的修正机制而在文本生成中广泛使用，但本质上是次优的。本文从同时联合考虑生成响应中所有标记的联合概率的角度，探讨最优性。我们理论分析了自回归生成的响应与同长度的全局最优响应之间的潜在偏差。我们的分析表明，在文本生成过程中出现明显的不确定性时，我们需要注意生成历史的次优性。为解决自回归解码方法在文本生成中的缺陷，我们提出了一种结合滑动反射窗口和暂停准则的方法，使得在解码过程中可以交替进行修正和生成。我们的选择性修正框架在效率和最优性之间取得了平衡，广泛实验结果证明了该方法的有效性。', 'title_zh': '反射窗口解码：选择性细化的文本生成'}
{'arxiv_id': 'arXiv:2502.03671', 'title': 'Advancing Reasoning in Large Language Models: Promising Methods and Approaches', 'authors': 'Avinash Patil', 'link': 'https://arxiv.org/abs/2502.03671', 'abstract': 'Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.', 'abstract_zh': '大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著的成功，但其推理能力仍然是一个根本性的挑战。虽然LLMs在流畅性和事实回忆方面表现出色，但在进行复杂的推理（如逻辑演绎、数学问题解决、常识推理和多步推理）时，往往未能达到人类的期望。本文综述了增强LLMs推理能力的新兴技术。我们根据方法的关键思路将现有技术分为几类，包括提示策略（例如，链条式推理、自我一致性推理和思维树推理）、架构创新（例如，检索增强模型、模块化推理网络和神经符号融合）以及学习范式（例如，以推理特定数据集为基础的微调、强化学习和自监督的推理目标）。此外，我们还探讨了用于评估LLMs推理能力的评估框架，并指出了开放性挑战，如涌现、鲁棒性和跨多元任务的推理泛化。通过综合最新的进展，本文旨在为未来增强推理能力的LLMs的研究和应用提供有价值的见解。', 'title_zh': '大语言模型中推理能力的提升：前景方法与途径'}
{'arxiv_id': 'arXiv:2502.03647', 'title': "Looking for the Inner Music: Probing LLMs' Understanding of Literary Style", 'authors': 'Rebecca M. M. Hicke, David Mimno', 'link': 'https://arxiv.org/abs/2502.03647', 'abstract': 'Recent work has demonstrated that language models can be trained to identify the author of much shorter literary passages than has been thought feasible for traditional stylometry. We replicate these results for authorship and extend them to a new dataset measuring novel genre. We find that LLMs are able to distinguish authorship and genre, but they do so in different ways. Some models seem to rely more on memorization, while others benefit more from training to learn author/genre characteristics. We then use three methods to probe one high-performing LLM for features that define style. These include direct syntactic ablations to input text as well as two methods that look at model internals. We find that authorial style is easier to define than genre-level style and is more impacted by minor syntactic decisions and contextual word usage. However, some traits like pronoun usage and word order prove significant for defining both kinds of literary style.', 'abstract_zh': '最近的研究表明，语言模型能够训练识别比传统文体学认为可能的更短文学段落的作者身份。我们在此复制了这些结果并将其扩展到一个新的用于测量小说体裁的新数据集中。我们发现，语言模型能够区分作者身份和体裁，但它们是以不同的方式实现的。一些模型似乎更多依赖于记忆，而另一些模型则更多受益于通过训练来学习作者或体裁的特征。然后，我们使用三种方法探查一个表现优异的语言模型，以确定其定义风格的特征。这些方法包括直接对输入文本进行句法消融，以及两种从模型内部结构进行分析的方法。我们发现，作者风格比体裁风格更容易定义，且受微小句法决策和上下文词使用的影响更大。然而，一些特征，如代词使用和词序，对于定义这两种文学风格都具有重要意义。', 'title_zh': '探索内心旋律：探究大语言模型对文学风格的理解'}
{'arxiv_id': 'arXiv:2502.03643', 'title': 'Context-Preserving Gradient Modulation for Large Language Models: A Novel Approach to Semantic Consistency in Long-Form Text Generation', 'authors': 'Nirola Kobanov, Edmund Weatherstone, Zachary Vanderpoel, Orlando Wetherby', 'link': 'https://arxiv.org/abs/2502.03643', 'abstract': 'Maintaining semantic consistency over extended text sequences remains a fundamental challenge in long-form text generation, where conventional training methodologies often struggle to prevent contextual drift and coherence degradation. A novel gradient modulation approach is introduced, designed to adjust parameter updates dynamically in response to contextual relevance, ensuring that generated text remains aligned with prior discourse. By integrating a modulation function that selectively amplifies or attenuates gradients based on learned contextual dependencies, the proposed method enhances the stability of model-generated narratives without imposing significant computational overhead. Comparative evaluations against baseline models reveal improvements in coherence, contextual retention, and long-range dependency tracking, demonstrating the effectiveness of modifying the learning process at the gradient level. The results indicate that sentence structure variability and lexical diversity benefit from this approach, mitigating repetitive phrasing and improving adaptability across diverse linguistic contexts. Statistical validation of coherence metrics further substantiates the observed enhancements, with a significant reduction in inconsistencies emerging as a direct consequence of the modulation mechanism. Computational efficiency assessments confirm that the framework achieves these gains without requiring substantial modifications to the underlying architecture, ensuring compatibility with existing optimization workflows.', 'abstract_zh': '在长篇文本生成中保持语义一致性仍然是一个基本挑战，传统训练方法往往难以防止语境漂移和连贯性的下降。提出了一种新颖的梯度调节方法，旨在动态调整参数更新以响应语境的相关性，确保生成的文本与先前的讨论保持一致。通过结合一个调节函数，在根据学习到的语境依赖性有选择地放大或抑制梯度的基础上，所提出的方法在不增加显著计算开销的情况下增强了模型生成叙述的稳定性。与基线模型的比较评估显示了连贯性、语境保留和长距离依赖跟踪方面的改进，这证明了在梯度层面修改学习过程的有效性。结果表明，这种方法有助于提高句子结构的多样性和词汇多样性，减少重复表述，并提高在不同语言环境中适应性。统计验证还进一步证明了连贯性指标的增强效果，信度机制直接导致了不一致性显著减少。性能评估表明，该框架在不显著修改基础架构的情况下实现了这些收益，确保了与现有优化工作流的兼容性。', 'title_zh': '保留上下文的梯度调制：长文本生成中语义一致性的新方法'}
{'arxiv_id': 'arXiv:2502.03627', 'title': 'Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database', 'authors': 'Maxime Holmberg Sainte-Marie, Diego Kozlowski, Lucía Céspedes, Vincent Larivière', 'link': 'https://arxiv.org/abs/2502.03627', 'abstract': 'Following a recent study on the quality of OpenAlex linguistic metadata (Céspedes et al., 2025), the present paper aims to optimize the latter through the design, use, and evaluation of various linguistic classification procedures based on the latest and most efficient automatic language detection algorithms. Starting from a multilingual set of manually-annotated samples of articles indexed in the database, different classification procedures are then designed, based on the application of a set of language detection algorithms on a series of corpora generated from different combinations of textual metadata of indexed articles. At sample level first, the performance of these different procedures for each of the main languages in the database is evaluated in terms of precision, recall, and processing time. Then, overall procedure performance is estimated at the database level by means of a probabilistic simulation of harmonically aggregated and weighted scores. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on article titles, abstracts as well as journal names gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, use of the FastSpell algorithm on article titles only outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.', 'abstract_zh': '在最近一项关于OpenAlex语言元数据质量的研究（Céspedes等，2025）之后，本文旨在通过设计、使用和评估基于最新和最高效的自动语言检测算法的各种语言分类程序来优化这些元数据。从数据库中索引的多语言文章的手动标注样本集开始，设计了基于语言检测算法系列应用的不同分类程序。在样本级别首先评估这些不同程序在数据库中主要语言方面的性能，基于精确度、召回率和处理时间。然后，通过概率模拟和谐聚合和加权得分来估计总体程序性能。结果显示，程序性能强烈依赖于对实施每个度量的重视程度：在更看重精确度的上下文中，使用LangID算法对文章标题、摘要以及期刊名称进行分类效果最佳；然而，在所有认为召回率至少略微比精确度更重要或一旦考虑处理时间的情况下，仅使用FastSpell算法对文章标题进行分类的效果优于其他所有替代方案。鉴于缺乏真正多语言的大规模文献数据库，希望这些结果有助于确认并促进OpenAlex数据库在跨语言、基于文献计量的研究和分析中的无与伦比的潜力。', 'title_zh': '《 babel 中的噪音排序：基于 OpenAlex 数据库的语言检测算法性能评估》\n\n这个翻译符合学术规范，同时保持了原文的意思。其中，“babel”在中文中通常指的是语言繁多或语言混杂的情况，所以保留了原意未直译为“巴别塔”。'}
{'arxiv_id': 'arXiv:2502.03552', 'title': 'Can Cross Encoders Produce Useful Sentence Embeddings?', 'authors': 'Haritha Ananthakrishnan, Julian Dolby, Harsha Kokel, Horst Samulowitz, Kavitha Srinivas', 'link': 'https://arxiv.org/abs/2502.03552', 'abstract': "Cross encoders (CEs) are trained with sentence pairs to detect relatedness. As CEs require sentence pairs at inference, the prevailing view is that they can only be used as re-rankers in information retrieval pipelines. Dual encoders (DEs) are instead used to embed sentences, where sentence pairs are encoded by two separate encoders with shared weights at training, and a loss function that ensures the pair's embeddings lie close in vector space if the sentences are related. DEs however, require much larger datasets to train, and are less accurate than CEs. We report a curious finding that embeddings from earlier layers of CEs can in fact be used within an information retrieval pipeline. We show how to exploit CEs to distill a lighter-weight DE, with a 5.15x speedup in inference time.", 'abstract_zh': '交叉编码器（CEs）是通过训练句对来检测相关性的。由于CEs在推理过程中需要句对，因此普遍认为它们只能作为信息检索管道中的重新排名器使用。双编码器（DEs）则是用于嵌入句子的，其通过两个共享权重的独立编码器在训练过程中编码句对，并通过损失函数确保相关句子的嵌入在向量空间中接近。然而，DEs需要更大的训练数据集，并且通常不如CEs准确。我们报告了一个有趣的发现，即CEs早期层的嵌入可以在信息检索管道中使用。我们展示了如何利用CEs来提炼一个更轻量级的DE，并实现了5.15倍的推理时间加速。', 'title_zh': '跨编码器能否生成有用的主题句嵌入？'}
{'arxiv_id': 'arXiv:2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'authors': 'Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao', 'link': 'https://arxiv.org/abs/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at this https URL.', 'abstract_zh': '近年来，尤其是GPT-4之后，大型语言模型的发展取得了重要进展，激发了对能够理解多种模态的全能模型的兴趣。尽管已经出现了一些开源替代方案，但在性能上仍落后于专门的单模态模型。本文介绍了Ola，这是一种全能语言模型，在图像、视频和音频理解方面均达到了与专门模型相当的竞争力。Ola的核心设计在于其渐进模态对齐策略，能够逐步扩展语言模型的支持模态。训练管道从最不同的模态——图像和文本开始，然后逐步通过连接语言和音频知识的语音数据以及连接所有模态的视频数据，扩展模型的能力。这种渐进的学习管道还使得我们能够保持跨模态对齐数据相对较小的规模，从而使得从现有的视觉-语言模型开发全能模型变得容易且成本较低。此外，为了实现与GPT-4o相媲美的高级互动体验，我们还设计了一种逐句解码解决方案以支持流式语音生成。广泛实验表明，Ola在所有模态上都超过了现有的全能大型语言模型，同时在性能上与相似规模的顶级专门模型竞争激烈。我们希望将Ola打造成为一种全面开放的全能理解解决方案，以促进该新兴领域未来的研究。模型权重、代码和数据均在此处开源：[此链接]。', 'title_zh': 'Ola：面向全模态语言模型的渐进模态对齐技术探究'}
{'arxiv_id': 'arXiv:2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'authors': 'Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi', 'link': 'https://arxiv.org/abs/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'abstract_zh': '尽管在安全对齐方面做出了大量努力，大规模语言模型（LLMs）仍然容易受到触发有害行为的“逃逸攻击”的威胁。现有研究主要集中在需要技术专业知识的攻击方法上，但仍有两个关键问题尚未得到充分探索：（1）被劫持的响应是否真的能够帮助普通用户实施有害行为？（2）在更常见、更简单的交互中，是否存在安全漏洞？在本文中，我们证明，当LLM的响应兼具可行性和信息性时，它们最有效地促进了有害行为的实施——这两种属性在多步骤、多语言交互中很容易被激发。基于这一见解，我们提出了HarmScore，一种衡量LLM响应促进有害行为效果的劫持指标，以及Speak Easy，一种简单的多步骤、多语言攻击框架。值得注意的是，通过将Speak Easy整合到直接请求和劫持基线中，我们在四个安全基准中的开源和专有LLM上均观察到攻击成功率平均绝对值提高了0.319，HarmScore平均绝对值提高了0.426。我们的研究揭示了一个关键但往往被忽视的漏洞：恶意用户可以轻松利用常见的交互模式来实现其有害意图。', 'title_zh': '轻松启奏：通过简单交互激发来自LLM的有害越界行为'}
{'arxiv_id': 'arXiv:2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'authors': 'Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping', 'link': 'https://arxiv.org/abs/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'abstract_zh': '随着语言模型（LM）能力的提升，大规模评估和监督它们的任务变得越来越困难，这对人类来说是个挑战。我们希望其他语言模型能够自动化执行这两个任务，我们称之为“AI监督”。我们研究了模型相似性如何影响AI监督的两个方面，提出了基于模型错误重叠的一种概率度量方法来评估LM相似性。利用这种方法，我们首先展示LLM作为评审员时倾向于偏好与评审员相似的模型，这扩展了最近的自我偏好结果。然后，我们研究了基于LM注释进行训练的情况，发现弱监督模型和强学生模型之间互补的知识在“弱到强泛化”中的作用至关重要。随着模型能力的增强，发现它们的错误变得越来越困难，我们可能需要更多地依赖AI监督。然而，我们观察到一个值得关注的趋势——随着能力的提升，模型错误变得越来越相似，这表明了潜在的协同失败风险。我们研究强调了报告和校正模型相似性的必要性，特别是在正在兴起的AI监督范式中尤为重要。', 'title_zh': '优秀的模型思维方式相似，这削弱了人工智能的监管能力'}
{'arxiv_id': 'arXiv:2502.04180', 'title': 'Multi-agent Architecture Search via Agentic Supernet', 'authors': 'Guibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang, Lei Bai, Xiang Wang', 'link': 'https://arxiv.org/abs/2502.04180', 'abstract': 'Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \\textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \\textbf{(I)} requires only $6\\sim45\\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and \\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.', 'abstract_zh': '大型语言模型（LLM）赋能的多智能体系统通过有序的合作与互动扩展了单一智能体的认知边界，但构建这些系统往往需要大量的手工设计工作。尽管存在自动化设计智能体工作流的方法，但它们通常致力于识别一个静态的、复杂的“一刀切”系统，而这种系统无法根据不同查询的难度和领域动态分配推理资源。为解决这一挑战，我们不再追求单一的智能体系统，而是优化了**智能体超网**，即智能体架构的概率性和连续性分布。我们引入了MaAS，这是一种自动化框架，可以从智能体超网上抽样查询依赖的智能体系统，提供高质解决方案和定制化的资源分配（例如，LLM调用、工具调用、令牌成本）。跨六个基准的综合评估表明，MaAS **（I）** 仅需现有手工或自动化多智能体系统的6%到45%的推理成本，**（II）** 在多个方面优于它们0.54%到11.82%，**（III）** 具有优越的跨数据集和跨LLM基础模型的迁移性。', 'title_zh': '通过代理超网络进行多代理架构搜索'}
{'arxiv_id': 'arXiv:2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'authors': 'Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue', 'link': 'https://arxiv.org/abs/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'abstract_zh': '基于文本的大语言模型（LLMs）的最近进展，特别是在GPT系列和Lora模型中，已经证明了在训练时间和推理时间计算上增加规模的有效性。然而，当前利用LLMs的最先进的语音合成（TTS）系统往往是多阶段的，需要单独的模型（例如，在LLM之后的扩散模型），这使得在训练或测试中是否需要扩展特定模型的决策变得复杂。本工作做出了以下贡献：首先，我们探索了语音合成中训练时间和推理时间计算的扩展。其次，我们提出了一种简单的语音合成框架Llasa，该框架采用单一层向量量化（VQ）编解码器和单一的Transformer架构，完全与标准LLMs（如Llama）对齐。我们的实验表明，对于Llasa扩展训练时间计算可以一致地提高合成语音的自然度，并且能够生成更加复杂和准确的声调模式。此外，从推理时间计算的扩展方面来看，我们利用语音理解模型作为验证者，在搜索过程中进行验证，发现扩展推理时间计算可以将采样模式偏向特定验证者的偏好，从而提高情感表达力、音色一致性以及内容准确性。另外，我们已经公开发布了我们的TTS模型（1B、3B、8B）和编解码器模型的检查点和训练代码。', 'title_zh': '拉萨：基于 llama 的语音合成训练时和推理时计算扩展'}
{'arxiv_id': 'arXiv:2502.04040', 'title': 'Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment', 'authors': 'Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Minhao Cheng, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.04040', 'abstract': 'Training safe LLMs is one of the most critical research challenge. However, the commonly used method, Refusal Training (RT), struggles to generalize against various OOD jailbreaking attacks. Many safety training methods have been proposed to address this issue. While they offer valuable insights, we aim to complement this line of research by investigating whether OOD attacks truly exceed the capability of RT model. Conducting evaluation with BoN, we observe significant improvements on generalization as N increases. This underscores that the model possesses sufficient safety-related latent knowledge, but RT fails to consistently elicit this knowledge when addressing OOD attacks. Further analysis based on domain adaptation reveals that training with direct refusal causes model to rely on superficial shortcuts, resulting in learning of non-robust representation mappings. Based on our findings, we propose training model to perform safety reasoning for each query. Reasoning supervision encourages model to perform more computations, explicitly eliciting and using latent knowledge through reasoning. To achieve this, we synthesize reasoning supervision based on pre-guidelines, training the model to reason in alignment with them, thereby effectively eliciting and utilizing latent knowledge from diverse perspectives. Extensive experiments show that our method significantly improves generalization performance against OOD attacks.', 'abstract_zh': '训练安全的语言模型是目前最具挑战性的研究课题之一。然而，常用的拒绝训练（Refusal Training, RT）方法在应对各种离群值（Out-of-Distribution, OOD）攻击时难以泛化。为了解决这一问题，已经提出了许多安全训练方法。尽管这些方法提供了有价值的观点，我们旨在通过研究补充这一研究方向，即探讨是否存在OOD攻击确实超越了RT模型的能力。使用BoN进行评估时，我们观察到随着N的增加，泛化性能有了显著提升。这表明模型确实具备足够的安全相关潜在知识，但RT方法未能在应对OOD攻击时一致地提取这些知识。基于领域适应的进一步分析表明，直接使用拒绝训练使得模型依赖于表面的捷径，导致学习不稳健的表示映射。基于我们的发现，我们提出了训练模型对每个查询进行安全推理的方法。推理监督促使模型进行更多计算，通过推理显式地提取和利用潜在知识。为了实现这一点，我们根据预先设定的指南合成推理监督，训练模型按照这些指南进行推理，从而从多个角度有效提取和利用潜在知识。广泛的实验表明，我们的方法显著提高了模型在应对OOD攻击时的泛化性能。', 'title_zh': '利用指导原则进行推理以提取和利用知识以提高安全一致性'}
{'arxiv_id': 'arXiv:2502.03948', 'title': 'Enhancing Online Learning Efficiency Through Heterogeneous Resource Integration with a Multi-Agent RAG System', 'authors': 'Devansh Srivastav, Hasan Md Tusfiqur Alam, Afsaneh Asaei, Mahmoud Fazeli, Tanisha Sharma, Daniel Sonntag', 'link': 'https://arxiv.org/abs/2502.03948', 'abstract': "Efficient online learning requires seamless access to diverse resources such as videos, code repositories, documentation, and general web content. This poster paper introduces early-stage work on a Multi-Agent Retrieval-Augmented Generation (RAG) System designed to enhance learning efficiency by integrating these heterogeneous resources. Using specialized agents tailored for specific resource types (e.g., YouTube tutorials, GitHub repositories, documentation websites, and search engines), the system automates the retrieval and synthesis of relevant information. By streamlining the process of finding and combining knowledge, this approach reduces manual effort and enhances the learning experience. A preliminary user study confirmed the system's strong usability and moderate-high utility, demonstrating its potential to improve the efficiency of knowledge acquisition.", 'abstract_zh': '高效的在线学习需要无缝访问各种资源，如视频、代码仓库、文档和一般网络内容。本海报介绍了一个多智能体检索增强生成（RAG）系统的初步工作，该系统旨在通过整合这些异构资源来提高学习效率。该系统使用针对特定资源类型进行定制的专业智能体（例如，YouTube教程、GitHub仓库、文档网站和搜索引擎），自动检索和合成相关的信息。通过简化知识查找和整合的过程，这种方法减少了手工劳动，并提升了学习体验。初步的用户研究证实了该系统的良好易用性和中等到高度的实用性，展示了其提高知识获取效率的潜力。', 'title_zh': '通过多智能体RAG系统进行异质资源集成以增强在线学习效率'}
{'arxiv_id': 'arXiv:2502.03930', 'title': 'DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation', 'authors': 'Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang', 'link': 'https://arxiv.org/abs/2502.03930', 'abstract': 'Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.', 'abstract_zh': '以下是论文内容或标题的中文翻译，符合学术规范：\n\n近年来，有研究尝试通过结合扩散模型和自回归模型，以自回归的方式生成连续语音表示而不使用离散语音标记，但这些方法往往面临计算负荷过重或效果欠佳的挑战。本文提出了一种基于块的自回归框架——扩散变换器自回归建模（DiTAR），该框架结合了语言模型和扩散变换器。该方法显著增强了自回归模型在连续标记上的有效性并减少了计算需求。DiTAR 采用分而治之策略生成块，语言模型处理聚合块嵌入，随后扩散变换器基于语言模型的输出生成下一个块。在推理过程中，我们提出将温度定义为反转扩散偏微分方程中引入噪声的时间点，以平衡多样性和确定性。此外，我们在广泛的缩放分析中展示了 DiTAR 的出色可扩展性。在零样本语音生成中，DiTAR 在鲁棒性、说话人相似性和自然度方面均实现了最佳性能。', 'title_zh': 'DiTAR：扩散变换器自回归建模在语音生成中的应用'}
{'arxiv_id': 'arXiv:2502.03771', 'title': 'Adaptive Semantic Prompt Caching with VectorQ', 'authors': 'Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez', 'link': 'https://arxiv.org/abs/2502.03771', 'abstract': 'Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different prompts. We propose VectorQ, a framework to learn embedding-specific threshold regions that adapt to the complexity and uncertainty of an embedding. Through evaluations on a combination of four diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 12x increases in cache hit rate and error rate reductions up to 92%.', 'abstract_zh': '语义提示缓存通过重用缓存中的大语言模型（LLM）生成的响应来提高同义提示推理的延迟和成本。向量相似度度量通过给定一个数值评分来量化嵌入提示与其缓存中最近邻的相似度。现有系统依赖一个静态阈值来判断相似度评分是否足够高以产生缓存命中。我们表明，这个一刀切的阈值在不同提示下是不够合适的。我们提出了一种VectorQ框架，该框架能够学习特定于嵌入的阈值区间，以适应嵌入的复杂性和不确定性。通过在四个不同数据集上的综合评估，我们展示了VectorQ在所有静态阈值下都优于最先进的系统，缓存命中率提高了高达12倍，错误率降低了高达92%。', 'title_zh': '自适应语义提示缓存技术：VectorQ'}
{'arxiv_id': 'arXiv:2502.03692', 'title': 'DocMIA: Document-Level Membership Inference Attacks against DocVQA Models', 'authors': 'Khanh Nguyen, Raouf Kerkouche, Mario Fritz, Dimosthenis Karatzas', 'link': 'https://arxiv.org/abs/2502.03692', 'abstract': "Document Visual Question Answering (DocVQA) has introduced a new paradigm for end-to-end document understanding, and quickly became one of the standard benchmarks for multimodal LLMs. Automating document processing workflows, driven by DocVQA models, presents significant potential for many business sectors. However, documents tend to contain highly sensitive information, raising concerns about privacy risks associated with training such DocVQA models. One significant privacy vulnerability, exploited by the membership inference attack, is the possibility for an adversary to determine if a particular record was part of the model's training data. In this paper, we introduce two novel membership inference attacks tailored specifically to DocVQA models. These attacks are designed for two different adversarial scenarios: a white-box setting, where the attacker has full access to the model architecture and parameters, and a black-box setting, where only the model's outputs are available. Notably, our attacks assume the adversary lacks access to auxiliary datasets, which is more realistic in practice but also more challenging. Our unsupervised methods outperform existing state-of-the-art membership inference attacks across a variety of DocVQA models and datasets, demonstrating their effectiveness and highlighting the privacy risks in this domain.", 'abstract_zh': '文档视觉问答（DocVQA）为端到端的文档理解引入了一种新的范式，并迅速成为了多模态LLM的标准基准之一。由DocVQA模型驱动的文档处理工作流程自动化为许多业务部门带来了巨大的潜力。然而，文档通常包含高度敏感的信息，这引发了关于训练此类DocVQA模型的隐私风险的担忧。其中一个重要的隐私漏洞是由成员推理攻击利用的，即攻击者有可能确定某个特定记录是否是模型训练数据的一部分。在本文中，我们提出了两种针对DocVQA模型的新型成员推理攻击。这些攻击分别设计用于两种不同的对抗场景：在白盒设置中，攻击者可以全面访问模型架构和参数；在黑盒设置中，攻击者只能访问模型的输出。值得注意的是，我们的攻击假设攻击者无法访问辅助数据集，这在实践中更为现实，但却更具挑战性。我们的无监督方法在各种DocVQA模型和数据集上优于现有的最先进的成员推理攻击，这证明了它们的有效性，并突显了该领域中的隐私风险。', 'title_zh': 'DocMIA：针对文档级问答模型的文档级成员推断攻击'}
{'arxiv_id': 'arXiv:2502.03629', 'title': 'REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations', 'authors': 'Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, Ranjay Krishna', 'link': 'https://arxiv.org/abs/2502.03629', 'abstract': "Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.", 'abstract_zh': '现有的图像编辑模型难以满足实际应用的需求。尽管它们在学术基准测试中表现出色，但在满足真正用户需求方面尚未得到广泛应用。支撑这些模型的数据集使用了人工编辑，缺乏处理用户多样请求所需的规模和生态效度。我们引入了 REALEDIT，这是一个包含真实用户请求和来自 Reddit 的人工编辑的大规模图像编辑数据集。REALEDIT 包括一个包含 9300 个示例的测试集，用于评估模型对真实用户需求的处理能力。我们的结果显示，现有的模型在这些任务中表现不足，突显了现实训练数据的必要性。为了应对这一挑战，我们引入了 48,000 个训练示例，并训练了 REALEDIT 模型，实现了显著的性能提升。该模型在人工判断中胜过竞争对手多达 165 个 Elo 点，在自动化 VIEScore 评分标准中相对提高了 92%。我们将在 Reddit 上部署该模型，对其进行新请求的测试，并收到了积极的反馈。除了图像编辑，我们还探索了 REALEDIT 在检测编辑图像方面的潜力，并与一家深度伪造检测非营利组织合作。将他们的模型的微调数据集设为 REALEDIT 后，其 F1 分数提高了 14 个百分点，进一步证明了该数据集的广泛应用价值。', 'title_zh': 'REALEDIT：Reddit 编辑作为大规模实证数据集用于图像变换'}
{'arxiv_id': 'arXiv:2502.03511', 'title': "An Empirical Exploration of ChatGPT's Ability to Support Problem Formulation Tasks for Mission Engineering and a Documentation of its Performance Variability", 'authors': 'Max Ofsa, Taylan G. Topcu', 'link': 'https://arxiv.org/abs/2502.03511', 'abstract': "Systems engineering (SE) is evolving with the availability of generative artificial intelligence (AI) and the demand for a systems-of-systems perspective, formalized under the purview of mission engineering (ME) in the US Department of Defense. Formulating ME problems is challenging because they are open-ended exercises that involve translation of ill-defined problems into well-defined ones that are amenable for engineering development. It remains to be seen to which extent AI could assist problem formulation objectives. To that end, this paper explores the quality and consistency of multi-purpose Large Language Models (LLM) in supporting ME problem formulation tasks, specifically focusing on stakeholder identification. We identify a relevant reference problem, a NASA space mission design challenge, and document ChatGPT-3.5's ability to perform stakeholder identification tasks. We execute multiple parallel attempts and qualitatively evaluate LLM outputs, focusing on both their quality and variability. Our findings portray a nuanced picture. We find that the LLM performs well in identifying human-focused stakeholders but poorly in recognizing external systems and environmental factors, despite explicit efforts to account for these. Additionally, LLMs struggle with preserving the desired level of abstraction and exhibit a tendency to produce solution specific outputs that are inappropriate for problem formulation. More importantly, we document great variability among parallel threads, highlighting that LLM outputs should be used with caution, ideally by adopting a stochastic view of their abilities. Overall, our findings suggest that, while ChatGPT could reduce some expert workload, its lack of consistency and domain understanding may limit its reliability for problem formulation tasks.", 'abstract_zh': '系统工程（SE）随着生成式人工智能（AI）的可用性以及对系统族视图（SoS）的需求而不断发展，这一需求在美国国防部的事务工程（ME）框架下被正式化。制定ME问题极具挑战性，因为这些问题是开放式的；涉及将不明确的问题转换为明确的问题，以便于工程开发。目前尚不清楚AI在多大程度上能帮助解决这一问题。为此，本文探讨了多用途大型语言模型（LLM）在支持ME问题制定任务中的质量和一致性，特别是在利益相关者识别方面的作用。我们确定了一个相关的参考问题，即NASA的空间任务设计挑战，并记录了ChatGPT-3.5在执行利益相关者识别任务方面的表现。我们执行了多次平行尝试，并对LLM输出进行了定性的评估，重点关注其质量和变异性。我们的研究结果呈现了一个复杂的图景。我们发现，LLM 在识别人类导向的利益相关者方面表现良好，但在识别外部系统和环境因素方面表现不佳，尽管尝试将这些因素纳入考量。此外，LLM 在保持合适的抽象水平方面存在问题，并倾向于生成特定于解决方案的输出，这在问题制定过程中是不合适的。更重要的是，我们记录了平行线程之间的显著变异性，这表明LLM输出应谨慎使用，理想情况下应采用其能力的随机视角。总体而言，我们的研究结果表明，虽然ChatGPT 可以减少一些专家的工作量，但由于缺乏一致性和领域的理解，它可能不适合用于问题制定任务。', 'title_zh': '对ChatGPT在任务工程中支持问题表述任务能力的实证探索及其性能变异性文档'}
{'arxiv_id': 'arXiv:2502.03492', 'title': 'Teaching Language Models to Critique via Reinforcement Learning', 'authors': 'Zhihui Xie, Jie chen, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong', 'link': 'https://arxiv.org/abs/2502.03492', 'abstract': 'Teaching large language models (LLMs) to critique and refine their outputs is crucial for building systems that can iteratively improve, yet it is fundamentally limited by the ability to provide accurate judgments and actionable suggestions. In this work, we study LLM critics for code generation and propose $\\texttt{CTRL}$, a framework for $\\texttt{C}$ritic $\\texttt{T}$raining via $\\texttt{R}$einforcement $\\texttt{L}$earning, which trains a critic model to generate feedback that maximizes correction performance for a fixed generator model without human supervision. Our results demonstrate that critics trained with $\\texttt{CTRL}$ significantly enhance pass rates and mitigate compounding errors across both base and stronger generator models. Furthermore, we show that these critic models act as accurate generative reward models and enable test-time scaling through iterative critique-revision, achieving up to 106.1% relative improvements across challenging code generation benchmarks.', 'abstract_zh': '训练大型语言模型（LLMs）批判和改进其输出对于构建能够迭代改进的系统至关重要，但这从根本上受到了提供准确判断和可行建议能力的限制。在本研究中，我们探讨了代码生成领域的LLM批判方法，并提出了一个基于强化学习的框架$\\texttt{CTRL}$（Critic Training via Reinforcement Learning），该框架用于训练一个批判模型，使其能够生成反馈，以最大化特定固定生成器模型的纠正性能，无需人类监督。我们的结果显示，使用$\\texttt{CTRL}$训练的批判模型显著提升了通过率，并减轻了不同基础和更强生成器模型中的累积错误。此外，我们还展示了这些批判模型作为准确生成奖励模型的作用，并通过迭代批判与修订实现测试时的扩展，从而在挑战性的代码生成基准测试中实现了高达106.1%的相对性能提升。', 'title_zh': '通过强化学习教学语言模型进行评论'}
{'arxiv_id': 'arXiv:2501.16207', 'title': 'From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs', 'authors': 'Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian', 'link': 'https://arxiv.org/abs/2501.16207', 'abstract': "The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO, showing significant progress. However, these studies intertwined multiple skills simultaneously, i.e., problem-solving, reasoning, and writing formal specifications, making it hard to precisely identify the LLMs' strengths and weaknesses in each task. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and decomposes it into six sub-tasks. We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six formal-verification-related tasks by distilling GPT-4o. They are split into a 14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps. Also, the fine-tuning brought about a nearly threefold improvement at most. Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding abilities. We hope our findings inspire further research. Fine-tuned models are released to facilitate subsequent studies", 'abstract_zh': '基于AI的正式数学推理研究显示出了不可阻挡的增长趋势。这些研究在像国际数学奥林匹克（IMO）这样的数学竞赛中表现出色，显示出显著的进步。然而，这些研究同时融合了多种技能，如问题解决、推理和编写正式规范，使得难以精确辨别LLM在每项任务中的优势和不足。本文聚焦于正式验证，这是正式推理的一个直接应用场景，并将其分解为六个子任务。我们通过提炼GPT-4o，构建了涵盖五种主流正式规范语言（Coq、Lean4、Dafny、ACSL和TLA+）的18,000个高质量指令-响应对，分布在六个正式验证相关任务中。这些数据集被划分为一个包含14,000多个用于微调的数据集FM-alpaca和一个包含4,000个基准测试数据集FM-Bench。我们发现，当给定代码或详细的证明步骤描述时，LLM在编写证明片段方面表现良好。此外，微调带来了大约三倍的改进。有趣的是，我们观察到，使用正式数据进行微调还可以增强数学、推理和编程能力。我们希望我们的研究发现能激发进一步的研究，并发布微调后的模型以促进后续研究。', 'title_zh': '从非形式化到形式化：将大语言模型集成到可验证的形式证明中及其评估'}
{'arxiv_id': 'arXiv:2501.10711', 'title': 'How Should I Build A Benchmark? Revisiting Code-Related Benchmarks For LLMs', 'authors': 'Jialun Cao, Yuk-Kit Chan, Zixuan Ling, Wenxuan Wang, Shuqing Li, Mingwei Liu, Ruixi Qiao, Yuting Han, Chaozheng Wang, Boxi Yu, Pinjia He, Shuai Wang, Zibin Zheng, Michael R. Lyu, Shing-Chi Cheung', 'link': 'https://arxiv.org/abs/2501.10711', 'abstract': 'Various benchmarks have been proposed to assess the performance of large language models (LLMs) in different coding scenarios. We refer to them as code-related benchmarks. However, there are no systematic guidelines by which such a benchmark should be developed to ensure its quality, reliability, and reproducibility. We propose How2Bench, which is comprised of a 55- 55-criteria checklist as a set of guidelines to govern the development of code-related benchmarks comprehensively. Using HOW2BENCH, we profiled 274 benchmarks released within the past decade and found concerning issues. Nearly 70% of the benchmarks did not take measures for data quality assurance; over 10% did not even open source or only partially open source. Many highly cited benchmarks have loopholes, including duplicated samples, incorrect reference codes/tests/prompts, and unremoved sensitive/confidential information. Finally, we conducted a human study involving 49 participants, which revealed significant gaps in awareness of the importance of data quality, reproducibility, and transparency.', 'abstract_zh': '各种基准已经被提出，用以评估大型语言模型（LLMs）在不同编程场景中的性能。我们将这些基准称为代码相关基准。然而，目前尚缺乏系统性的指导原则来确保这类基准的质量、可靠性和可重现性。我们提出了How2Bench，它包含一个由55项标准组成的检查表，作为全面指导代码相关基准开发的一套指南。利用HOW2BENCH，我们对过去十年间发布的274个基准进行了分析，并发现了一些令人担忧的问题。近70%的基准没有采取数据质量保证措施；超过10%的基准甚至没有开源或仅部分开源。许多被高度引用的基准存在漏洞，包括重复的样本、错误的参考代码/测试/提示以及未删除的敏感/机密信息。最后，我们进行了一项涉及49名参与者的人类研究，发现对数据质量、可重现性和透明度重要性的认识存在显著差距。', 'title_zh': '如何构建基准？重新审视与代码相关的大型语言模型基准测试'}
