# Great Models Think Alike and this Undermines AI Oversight 

**Title (ZH)**: 优秀的模型思维方式相似，这削弱了人工智能的监管能力 

**Authors**: Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping  

**Link**: [PDF](https://arxiv.org/pdf/2502.04313)  

**Abstract**: As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight. 

**Abstract (ZH)**: 随着语言模型（LM）能力的提升，大规模评估和监督它们的任务变得越来越困难，这对人类来说是个挑战。我们希望其他语言模型能够自动化执行这两个任务，我们称之为“AI监督”。我们研究了模型相似性如何影响AI监督的两个方面，提出了基于模型错误重叠的一种概率度量方法来评估LM相似性。利用这种方法，我们首先展示LLM作为评审员时倾向于偏好与评审员相似的模型，这扩展了最近的自我偏好结果。然后，我们研究了基于LM注释进行训练的情况，发现弱监督模型和强学生模型之间互补的知识在“弱到强泛化”中的作用至关重要。随着模型能力的增强，发现它们的错误变得越来越困难，我们可能需要更多地依赖AI监督。然而，我们观察到一个值得关注的趋势——随着能力的提升，模型错误变得越来越相似，这表明了潜在的协同失败风险。我们研究强调了报告和校正模型相似性的必要性，特别是在正在兴起的AI监督范式中尤为重要。 

---
