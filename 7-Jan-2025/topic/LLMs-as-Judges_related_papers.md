# Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications 

**Title (ZH)**: 使用生成式人工智能应用进行 constructed response 评分的有效性论证 

**Authors**: Jodi M. Casabianca, Daniel F. McCaffrey, Matthew S. Johnson, Naim Alper, Vladimir Zubenko  

**Link**: [PDF](https://arxiv.org/pdf/2501.02334)  

**Abstract**: The rapid advancements in large language models and generative artificial intelligence (AI) capabilities are making their broad application in the high-stakes testing context more likely. Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods. The purpose of this paper is to highlight the differences in the feature-based and generative AI applications in constructed response scoring systems and propose a set of best practices for the collection of validity evidence to support the use and interpretation of constructed response scores from scoring systems using generative AI. We compare the validity evidence needed in scoring systems using human ratings, feature-based natural language processing AI scoring engines, and generative AI. The evidence needed in the generative AI context is more extensive than in the feature-based NLP scoring context because of the lack of transparency and other concerns unique to generative AI such as consistency. Constructed response score data from standardized tests demonstrate the collection of validity evidence for different types of scoring systems and highlights the numerous complexities and considerations when making a validity argument for these scores. In addition, we discuss how the evaluation of AI scores might include a consideration of how a contributory scoring approach combining multiple AI scores (from different sources) will cover more of the construct in the absence of human ratings. 

**Abstract (ZH)**: 随着大型语言模型和生成式人工智能（AI）能力的迅速发展，它们在高风险测试环境中的广泛应用更加可能。特别是在评分构造性回答时使用生成式AI极具吸引力，因为它可以减少传统AI评分中手工构建特征所需的努力，并且甚至可能超越这些方法。本文旨在突显基于特征和生成式AI在构造性回答评分系统中的应用差异，并提出一套最佳实践，以收集有效性证据，支持使用和解释使用生成式AI的评分系统的构造性回答分数。我们比较了使用人工评分、基于特征的自然语言处理AI评分引擎以及生成式AI的评分系统所需的有效性证据。生成式AI上下文中所需的有效性证据远多于基于特征的NLP评分上下文，这主要是由于透明度不足及其他生成式AI独有的问题，如一致性问题。标准化测试中的构造性回答分数数据展示了不同类型的评分系统收集有效性证据的方式，并突显了为这些分数提出有效性论点时所涉及的众多复杂性和考量。此外，我们还讨论了如何在评估AI分数时考虑使用多种来源的AI评分（组合评分方法）来覆盖更多构念的问题，特别是在缺乏人工评分的情况下。 

---
