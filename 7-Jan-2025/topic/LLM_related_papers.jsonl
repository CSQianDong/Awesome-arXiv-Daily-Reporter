{'arxiv_id': 'arXiv:2501.03226', 'title': 'BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning', 'authors': 'Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang', 'link': 'https://arxiv.org/abs/2501.03226', 'abstract': "Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.", 'abstract_zh': '尖端的大规模语言模型（LLMs）在采用分而治之的管道和上下文学习（ICL）示例辅助的情况下，显示出解决复杂数学问题的前景。然而，它们在改进方面的潜力受到ICL示例中两个关键问题的限制：粒度不匹配和随后产生的负效噪音问题。具体来说，LLMs能够执行拆分过程，但在几步征服策略中往往因不准确的推理而失败。此外，在问题粒度下检索到的ICL示例有时缺乏针对特定具有挑战性的推理步骤的相关步骤，这种不相关性可能会阻碍正确的推理。因此，我们致力于提高每一步推理的质量，并提出了BoostStep。BoostStep在步骤粒度上对检索和推理进行对齐，并为每一步推理提供高度相关的ICL示例，采用一个新的“初次尝试”策略。BoostStep提供了比粗略的问题粒度策略更多的相关示例，逐步提升模型在每一步的推理质量。BoostStep是一个通用且稳健的推理增强方法，不仅可以提高独立推理能力，还可以无缝集成蒙特卡洛树搜索方法（MCTS），以细化候选生成和决策制定过程。定量上，BoostStep分别将GPT-4o和Qwen2.5-Math-72B在各种数学基准上的性能提高了3.6%和2.0%，结合MCTS后，综合改进达到7.5%。', 'title_zh': 'BoostStep: 通过提升单步推理能力增强大型语言模型的数学能力'}
{'arxiv_id': 'arXiv:2501.03212', 'title': 'Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text', 'authors': 'Ayat Najjar, Huthaifa I. Ashqar, Omar Darwish, Eman Hammad', 'link': 'https://arxiv.org/abs/2501.03212', 'abstract': 'The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was unable to recognize about 4.2\\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.', 'abstract_zh': '生成型人工智能大型语言模型（LLMs）的发展引起了对通过生成型AI或人类创作内容的识别问题的关注。在一种情况下，当学生过度依赖这类工具时，这会影响他们的写作或编程技能的发展，同时也会引发抄袭等问题。本研究旨在支持检测和识别使用LLM工具生成的文本的努力。我们假设通过机器学习（ML）可以检测到由LLM生成的文本，并研究能够识别和区分多个LLM工具生成文本的ML模型。我们利用了多种机器学习和深度学习算法，如随机森林（RF）和循环神经网络（RNN），并运用可解释的人工智能（XAI）来理解归因的重要特征。我们的方法分为两部分：1）二元分类，以区分人类书写和AI文本；2）多分类，以区分人类书写文本和由五种不同的LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果表明，在二元分类和多分类中均具有较高的准确性。我们的模型在多分类和二元分类的准确性上优于GPTZero，达到98.5%和78.3%。值得注意的是，GPTZero 无法识别约4.2%的观察数据，但我们的模型能够识别完整的测试数据集。XAI结果表明，理解不同类别中的特征重要性能够建立详细的作者/来源档案，进一步有助于归因和支持通过突出独特的风格和结构元素进行抄袭检测，确保内容原创性的坚实验证。', 'title_zh': '利用可解释的人工智能进行大模型文本归属：区分人类撰写的文本与多个人工智能语言模型生成的文本'}
{'arxiv_id': 'arXiv:2501.03200', 'title': "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input", 'authors': 'Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das', 'link': 'https://arxiv.org/abs/2501.03200', 'abstract': "We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at this https URL.", 'abstract_zh': '我们引入了FACTS Grounding，这是一个在线排行榜和相关基准，用于评估语言模型生成与给定用户提示背景信息相符的事实准确文本的能力。在我们的基准中，每个提示包含一个用户请求和一个包含最多32k个令牌的完整文档，需要生成长篇幅的响应。这些长篇幅的响应必须完全基于提供的上下文文档并满足用户请求。模型将在两个阶段使用自动化评分模型进行评估：（1）如果响应未能满足用户请求，则视为无效；（2）如果响应完全基于提供的文档，则视为准确。自动化评分模型经过全面评估后选择了最佳提示模板，最终的事实得分是多个评分模型的综合结果，以减轻评分偏差。FACTS Grounding排行榜将随着时间持续维护，并包含公开和私有分割，以允许外部参与同时保护排行榜的完整性。您可以在此处访问：[该链接]。', 'title_zh': 'FACTS接地排行榜：评估大语言模型将响应与长格式输入对接的能力'}
{'arxiv_id': 'arXiv:2501.03139', 'title': 'VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity', 'authors': 'Yerong Li, Yiren Liu, Yun Huang', 'link': 'https://arxiv.org/abs/2501.03139', 'abstract': 'Scenario-based training has been widely adopted in many public service sectors. Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios. However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes. In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage). We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims. Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content. According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness.', 'abstract_zh': '基于情景的培训在许多公共服务领域中被广泛采用。近年来，大规模语言模型（LLMs）在模拟多样化的人格以创建这些培训情景方面显示出了潜在的应用前景。然而，关于如何利用LLMs构建模拟受害者以用于基于情景的培训方面知之甚少。在本文中，我们介绍了VicSim（受害者模拟器），这是一种新型模型，旨在解决用户模拟的三大关键维度：信息忠实性、情感动态和语言风格（例如，语法使用）。我们探索了情景受害者建模与基于生成对抗网络（GAN）的训练工作流及基于关键信息的提示相结合的方法，旨在提高模拟受害者的真实度。我们的对抗训练方法通过教会判别器识别语法和情感线索作为合成内容的可靠标志。根据人类评价者的评估结果，VicSim模型在人性相似度方面优于GPT-4。', 'title_zh': 'VicSim：提高情感和语境忠实度的受害者模拟技术'}
{'arxiv_id': 'arXiv:2501.03112', 'title': 'LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases', 'authors': 'Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Viren Bajaj, Zeya Ahmad', 'link': 'https://arxiv.org/abs/2501.03112', 'abstract': "Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.", 'abstract_zh': '大规模语言模型（LLMs）在多种方式上表现出偏见，这可能导致某些由性别、种族、性取向或年龄等保护属性定义的特定群体的结果受到影响或恶化。为了解决这一问题，我们引入了LangFair，这是一个开源的Python包，旨在为LLM实践者提供工具，以评估与其具体应用场景相关的偏见和公平性风险。该包提供了一种功能，可以轻松生成由LLM对特定应用场景提示的响应组成的评估数据集，并随后计算适用于实践者场景的指标。为了指导指标选择，LangFair提供了一套可操作的决策框架。', 'title_zh': 'LangFair：一个评估大规模语言模型中偏见和公平性使用的Python软件包'}
{'arxiv_id': 'arXiv:2501.03035', 'title': 'Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning', 'authors': 'Zhen Li, Yupeng Su, Runming Yang, Zhongwei Xie, Ngai Wong, Hongxia Yang', 'link': 'https://arxiv.org/abs/2501.03035', 'abstract': 'Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation.', 'abstract_zh': '大规模语言模型在MATH等复杂数学推理基准测试中取得了显著进展。然而，它们巨大的计算需求给实际部署带来了挑战。模型量化已作为一种有效策略出现，通过使用较低精度和位宽表示来减少内存使用和计算成本。在本研究中，我们系统地评估了量化对数学推理任务的影响。我们引入了一个多维度的评估框架，从定性的角度评估特定能力维度，并对不同量化方法的步骤输出进行了定量分析。我们的结果表明，量化以不同的方式影响数值计算和推理规划能力，确定了量化模型性能下降的关键领域。', 'title_zh': '量化与推理的融合：探索大规模语言模型低比特量化对数学推理的性能下降现象'}
{'arxiv_id': 'arXiv:2501.02869', 'title': 'IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment', 'authors': 'Yiming Zhang, Zheng Chang, Wentao Cai, MengXing Ren, Kang Yuan, Yining Sun, Zenghui Ding', 'link': 'https://arxiv.org/abs/2501.02869', 'abstract': "Recent researches of large language models(LLM), which is pre-trained on massive general-purpose corpora, have achieved breakthroughs in responding human queries. However, these methods face challenges including limited data insufficiency to support extensive pre-training and can not align responses with users' instructions. To address these issues, we introduce a medical instruction dataset, CMedINS, containing six medical instructions derived from actual medical tasks, which effectively fine-tunes LLM in conjunction with other data. Subsequently, We launch our medical model, IIMedGPT, employing an efficient preference alignment method, Direct preference Optimization(DPO). The results show that our final model outperforms existing medical models in medical this http URL, Code and model checkpoints will be released upon acceptance.", 'abstract_zh': '近年来，大规模语言模型（LLM）的研究取得了突破性进展，这些模型在大规模通用语料库上进行预训练，能够有效回应人类查询。然而，这些方法也面临着一些挑战，包括数据量有限，不足以支持广泛的预训练，以及生成的响应与用户指令不完全对齐。为了解决这些问题，我们介绍了一个包含六项实际医学任务指令的医学指令数据集，即CMedINS，该数据集能够与其他数据结合，有效微调LLM。随后，我们推出了我们的医学模型IIMedGPT，该模型采用了一种高效的工作指令对齐方法，即直接偏好优化（DPO）。实验结果表明，我们的最终模型在医学领域的表现优于现有模型。我们将在论文被接受后发布代码和模型检查点。', 'title_zh': 'IIMedGPT：通过高效的人类偏好对齐促进大型语言模型在医疗任务中的能力'}
{'arxiv_id': 'arXiv:2501.02795', 'title': 'InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion', 'authors': 'Zhaoyi Yan, Zhijie Sang, Yiming Zhang, Yuhao Fu, Baoyi He, Qi Zhou, Yining Di, Chunlin Ji, Shengyu Zhang, Fei Wu, Hongxia Yang', 'link': 'https://arxiv.org/abs/2501.02795', 'abstract': "Large Language Models (LLMs) have demonstrated strong performance across various reasoning tasks, yet building a single model that consistently excels across all domains remains challenging. This paper addresses this problem by exploring strategies to integrate multiple domain-specialized models into an efficient pivot this http URL propose two fusion strategies to combine the strengths of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially distills each source model into the pivot model, followed by a weight merging step to integrate the distilled models into the final model. This method achieves strong performance but requires substantial training effort; and (2) a unified fusion approach that aggregates all source models' outputs this http URL improve the fusion process, we introduce a novel Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K ratios during parameter merging for enhanced flexibility and this http URL, we propose an uncertainty-based weighting method for the unified approach, which dynamically balances the contributions of source models and outperforms other logits/distribution ensemble this http URL achieved accuracy improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval tasks, respectively.", 'abstract_zh': '大规模语言模型（LLMs）在各种推理任务中展示了强大的性能，但要构建一个能够在所有领域中持续表现出色的单一模型仍然具有挑战性。本文通过探索将多个领域专有的模型整合到一个有效枢纽模型中的策略来应对这一问题。我们提出了两种结合多个LLM强项的融合策略：（1）一种两两逐步融合的方法，该方法首先将每个源模型逐步提炼到枢纽模型中，然后通过权重整合步骤将提炼后的模型集成到最终模型中。这种方法取得了出色的效果，但需要大量的训练努力；（2）一种统一的融合方法，该方法将所有源模型的输出汇总。为了改进融合过程，我们提出了一个名为速率-偏度自适应融合（RSAF）的新技术，该技术在参数合并过程中动态调整最上K值的比例，增强了灵活性。与此同时，我们提出了一种基于不确定性加权的方法，用于统一方法，该方法动态平衡了源模型的贡献，并在其他概率分布集合方法中表现出色。在GSM8K、MATH和HumanEval任务中，这种方法分别实现了9.27%、8.80%和8.89%的准确性提升。', 'title_zh': 'InfiFusion：一种通过LLM融合增强跨模型推理的统一框架'}
{'arxiv_id': 'arXiv:2501.02506', 'title': 'ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use', 'authors': 'Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, Jiechao Chen', 'link': 'https://arxiv.org/abs/2501.02506', 'abstract': 'Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in this https URL.', 'abstract_zh': '多跳工具使用的有效评估对于分析大型语言模型（LLMs）的理解能力、推理能力和功能调用能力至关重要。然而，进展受到可靠评估数据集缺乏的阻碍。为了解决这一问题，我们提出了ToolHop数据集，该数据集包含995个用户查询和3,912个相关工具，专门用于严格评估多跳工具使用能力。ToolHop确保了查询的多样性、有意义的相互依赖性、本地可执行的工具、详细的反馈以及可验证的答案，通过一种新颖的由查询驱动的数据构建方法，该方法包括工具创建、文档优化和代码生成。我们对14个LLM模型进行了评估，涵盖五个模型家族（即LLaMA3.1、Qwen2.5、Gemini1.5、Claude3.5和GPT），发现处理多跳工具使用场景时存在显著挑战。领先模型GPT-4o的准确率为49.04%，表明改进的空间仍然很大。进一步分析揭示了不同模型家族在工具使用策略上的差异，提供了可以指导更有效方法发展的操作性见解。代码和数据可以在以下链接找到：[链接]。', 'title_zh': 'ToolHop：一个基于查询的基准测试，用于评估在多跳工具使用中大型语言模型的性能'}
{'arxiv_id': 'arXiv:2501.02448', 'title': 'Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap', 'authors': 'Hyunwoo Ko, Guijin Son, Dasol Choi', 'link': 'https://arxiv.org/abs/2501.02448', 'abstract': 'Large language models (LLMs) demonstrate exceptional performance on complex reasoning tasks. However, despite their strong reasoning capabilities in high-resource languages (e.g., English and Chinese), a significant performance gap persists in other languages. To investigate this gap in Korean, we introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual math problems. Through systematic analysis of model behaviors, we identify a key finding: these performance disparities stem primarily from difficulties in comprehending non-English inputs, rather than limitations in reasoning capabilities. Based on these findings, we propose UST (Understand, Solve, and Translate), a method that strategically uses English as an anchor for reasoning and solution generation. By fine-tuning the model on 130k synthetically generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally, we show that improvements from UST generalize effectively to different Korean domains, demonstrating that capabilities acquired from machine-verifiable content can be generalized to other areas. We publicly release the benchmark, training dataset, and models.', 'abstract_zh': '大型语言模型（LLMs）在复杂推理任务上表现出色。然而，尽管它们在高资源语言（如英语和中文）上的推理能力很强，但在其他语言上仍存在显著的表现差距。为研究这一差距，我们引入了HRM8K基准，该基准包含8,011个英语-韩语双语数学问题。通过系统的模型行为分析，我们发现了一个关键发现：这些性能差异主要源自对非英语输入理解的困难，而不是推理能力的限制。基于这些发现，我们提出了UST（理解、解决和翻译）方法，该方法策略性地利用英语作为推理和解决方案生成的锚点。通过在130,000个合成数据点上微调模型，UST在HRM8K基准上的表现提高了10.91%，并将多语言性能差距从11.6%降低到了0.7%。此外，我们展示了UST在不同韩语领域中的性能改进具有有效性，表明从可机器验证的内容中获得的能力可以推广到其他领域。我们已公开发布了该基准、训练数据集和模型。', 'title_zh': '理解、解决并转换：弥合多语言数学推理差距'}
{'arxiv_id': 'arXiv:2501.02348', 'title': 'Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving', 'authors': 'Sanghyun Park, Boris Maciejovsky, Phanish Puranam', 'link': 'https://arxiv.org/abs/2501.02348', 'abstract': 'Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the "wisdom of crowds" within a single individual, allowing them to "think with many minds." While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation\'s limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.', 'abstract_zh': '复杂问题解决需要认知灵活性——即在保持各种视角独特性的同时，能够容纳多种视角的能力。这种灵活性可以在单一个体中复制“群体的智慧”，使他们能够“使用多种思维”。虽然心理模拟能够支持想象中的讨论，但认知限制却限制了其有效性。我们提出了一种基于大型语言模型（LLM）的方法——合成讨论，作为一种解决方案。通过使用自定义的基于GPT的模型，我们展示了其优势：同时处理多种视角而不损害认知能力，平行探索各种视角，并精确控制视角合成。通过将讨论过程外部化并分散认知劳动，使并行搜索和整合得以进行，合成讨论超越了心理模拟的局限性。这种方法在战略规划、政策制定和冲突解决方面展现出前景。', 'title_zh': '多思维运用：利用大规模语言模型进行多视角问题解决'}
{'arxiv_id': 'arXiv:2501.02336', 'title': 'AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference', 'authors': 'Zhuomin He, Yizhen Yao, Pengfei Zuo, Bin Gao, Qinya Li, Zhenzhe Zheng, Fan Wu', 'link': 'https://arxiv.org/abs/2501.02336', 'abstract': 'Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \\sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \\sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \\sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines.', 'abstract_zh': '长上下文大规模语言模型（LLMs）推理日益重要，促使了多项研究致力于缓解此类场景中的显著存储和计算成本。逐层跳过方法是很有前景的优化手段，但在长上下文推理中鲜有探索。我们观察到，现有的逐层跳过策略在长上下文推理中存在几个限制，包括无法适应模型和上下文的变化性、忽视子层的重要性以及不适用于填充阶段。本文提出了一种名为**SysName**的自适应子层跳过方法，专门设计用于长上下文推理。**SysName**通过利用实时相似性信息自适应地识别较不重要的层，支持子层级别的跳过，并加速填充和解码阶段。通过在多种长上下文基准和模型上进行广泛的实验，证明了**SysName**在推理性能方面的优越性，高于现有基线方法。', 'title_zh': 'AdaSkip：适应性子层跳过加速长上下文LLM推理'}
{'arxiv_id': 'arXiv:2501.02266', 'title': 'LLMzSz{\\L}: a comprehensive LLM benchmark for Polish', 'authors': 'Krzysztof Jassem, Michał Ciesiółka, Filip Graliński, Piotr Jabłoński, Jakub Pokrywka, Marek Kubis, Monika Jabłońska, Ryszard Staruch', 'link': 'https://arxiv.org/abs/2501.02266', 'abstract': "This article introduces the first comprehensive benchmark for the Polish language at this scale: LLMzSzŁ (LLMs Behind the School Desk). It is based on a coherent collection of Polish national exams, including both academic and professional tests extracted from the archives of the Polish Central Examination Board. It covers 4 types of exams, coming from 154 domains. Altogether, it consists of almost 19k closed-ended questions. We investigate the performance of open-source multilingual, English, and Polish LLMs to verify LLMs' abilities to transfer knowledge between languages. Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined. We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters. Our analysis highlights the potential of LLMs in assisting with exam validation, particularly in identifying anomalies or errors in examination tasks.", 'abstract_zh': '本文介绍了迄今为止第一个针对波兰语的大规模综合基准：LLMzSzŁ（LLMs Behind the School Desk）。该基准基于波兰国家考试的一致性集合，包括从波兰中央考试委员会档案中提取的学术和职业测试。它涵盖了4种不同类型的考试，来自154个领域。共计包含近19000个闭合问题。我们研究了开源多语言、英文和波兰语大语言模型的性能，以验证大语言模型在语言间传递知识的能力。同时，我们还分析了模型准确性和考试通过率上大语言模型与人类之间的相关性。结果显示，多语言大语言模型能够在某些任务上获得优于单语言模型的性能；然而，在模型规模成为关键因素时，单语言模型仍可能具有一定优势。我们的分析强调了大语言模型在考试验证中的潜在作用，尤其是在识别考试任务中的异常或错误方面。', 'title_zh': 'LLMzSz{\\L}：波兰语综合大语言模型基准'}
{'arxiv_id': 'arXiv:2501.02086', 'title': 'Instruction-Following Pruning for Large Language Models', 'authors': 'Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei', 'link': 'https://arxiv.org/abs/2501.02086', 'abstract': 'With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed "instruction-following pruning", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model.', 'abstract_zh': '随着大型语言模型（LLMs）的快速扩展，结构化剪枝已成为一种广泛使用的技术，可以从较大的模型中学习出更高效、更小的模型，其性能优于从头训练同等大小的模型。在本文中，我们超越了传统的静态剪枝方法，即为模型确定一个固定的剪枝掩码，提出了一个动态的结构化剪枝方法。在我们的方法中，剪枝掩码取决于输入，并根据用户的指令描述内容动态适应。我们提出的方法称为“指令跟随剪枝”，其中引入了一个稀疏掩码预测器，该预测器将用户的指令作为输入，并动态选择与给定任务最相关的模型参数。为了识别和激活有效的参数，我们联合优化了稀疏掩码预测器和LLM，利用了结合指令跟随数据和预训练语料库的方法。实验结果表明，我们的方法在多种评估基准上都表现出有效性。例如，我们在数学和编程领域中，3B量级的激活模型比3B量级的密集模型提高了5-8个绝对分数，并且在性能上与9B量级的模型不相上下。', 'title_zh': '遵循指令的剪枝方法在大规模语言模型中的应用'}
{'arxiv_id': 'arXiv:2501.02031', 'title': 'CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System', 'authors': 'Zhixuan Cao, Ming Han, Jingtao Wang, Meng Jia', 'link': 'https://arxiv.org/abs/2501.02031', 'abstract': 'As the impact of global climate change intensifies, corporate carbon emissions have become a focal point of global attention. In response to issues such as the lag in climate change knowledge updates within large language models, the lack of specialization and accuracy in traditional augmented generation architectures for complex problems, and the high cost and time consumption of sustainability report analysis, this paper proposes CarbonChat: Large Language Model-based corporate carbon emission analysis and climate knowledge Q&A system, aimed at achieving precise carbon emission analysis and policy this http URL, a diversified index module construction method is proposed to handle the segmentation of rule-based and long-text documents, as well as the extraction of structured data, thereby optimizing the parsing of key this http URL, an enhanced self-prompt retrieval-augmented generation architecture is designed, integrating intent recognition, structured reasoning chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic understanding and query this http URL, based on the greenhouse gas accounting framework, 14 dimensions are established for carbon emission analysis, enabling report summarization, relevance evaluation, and customized this http URL, through a multi-layer chunking mechanism, timestamps, and hallucination detection features, the accuracy and verifiability of the analysis results are ensured, reducing hallucination rates and enhancing the precision of the responses.', 'abstract_zh': '随着全球气候变化影响的加剧，企业碳排放已成为全球关注的焦点。针对大型语言模型在气候知识更新滞后、传统增强生成架构对复杂问题的专业性和准确性不足、以及可持续报告分析成本高且耗时等问题，本文提出CarbonChat：基于大型语言模型的企业碳排放分析和气候变化知识问答系统，旨在实现精准的碳排放分析与政策建议。通过提出一种多样化的指标模块构造方法，以处理基于规则的和长文档的分段，以及结构化数据的抽取，从而优化关键信息的解析。设计了一种增强的自提示检索增强生成架构，整合意图识别、结构化推理链、混合检索和Text2SQL技术，提高语义理解和查询效率。基于温室气体核算框架，建立了14个维度进行碳排放分析，支持报告摘要、相关性评估和个性化分析。通过多层切块机制、时间戳和幻觉检测功能，确保分析结果的准确性和可验证性，降低幻觉率并提高响应的精确度。', 'title_zh': '碳聊：基于大型语言模型的企业碳排放分析与气候知识问答系统'}
{'arxiv_id': 'arXiv:2501.02026', 'title': 'Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models', 'authors': 'Kaleem Ullah Qasim, Jiashu Zhang, Tariq Alsahfi, Ateeq Ur Rehman Butt', 'link': 'https://arxiv.org/abs/2501.02026', 'abstract': "Enhancing the reasoning capabilities of Large Language Models remains a critical challenge in artificial intelligence. We introduce RDoLT, Recursive Decomposition of Logical Thought prompting, a novel framework that significantly boosts LLM reasoning performance. RDoLT is built on three key innovations: (1) recursively breaking down complex reasoning tasks into sub-tasks of progressive complexity; (2) employing an advanced selection and scoring mechanism to identify the most promising reasoning thoughts; and (3) integrating a knowledge propagation module that mimics human learning by keeping track of strong and weak thoughts for information propagation. Our approach was evaluated across multiple benchmarks, including GSM8K, SVAMP, MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results demonstrate that RDoLT consistently outperforms existing state-of-the-art techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4, surpassing state-of-the-art techniques by 6.28 percent. Similar improvements were observed on other benchmarks, with accuracy gains ranging from 5.5 percent to 6.75 percent. These findings highlight RDoLT's potential to advance prompt engineering, offering a more effective and generalizable approach to complex reasoning tasks.", 'abstract_zh': '增强大型语言模型的推理能力仍然是人工智能领域的一项关键挑战。我们引入了RDoLT——递归分解逻辑思维提示框架，这一创新框架显著提升了大型语言模型的推理表现。RDoLT基于三项关键创新：(1) 递归地将复杂的推理任务分解为逐步复杂化的子任务；(2) 使用先进的选择和评分机制来识别最有前景的推理思维；以及(3) 集成一个知识传播模块，该模块通过追踪强和弱的思维来模拟人类学习过程，以促进信息传播。我们的方法在多个基准测试中进行了评估，包括GSM8K、SVAMP、MultiArith、LastLetterConcatenation和2023年高考数学。结果表明，RDoLT在各个基准测试中均稳定地优于现有最先进的技术。例如，与ChatGPT-4在GSM8K上的90.98％准确率相比，超越了现有技术6.28％。在其他基准测试中也观察到类似的改进，准确率的提升范围从5.5％到6.75％不等。这些发现突显了RDoLT在推进提示工程方面的潜力，提供了一种更有效和更具普适性的复杂推理任务方法。', 'title_zh': '递归分解逻辑思维：大型语言模型中高级推理和知识传播的框架'}
{'arxiv_id': 'arXiv:2501.02018', 'title': 'Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs', 'authors': 'Joao Fonseca, Andrew Bell, Julia Stoyanovich', 'link': 'https://arxiv.org/abs/2501.02018', 'abstract': 'Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs "self-reflect", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict ``normal\'\' model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SafeNudge, that combines Controlled Text Generation with "nudging", or using text interventions to change the behavior of a model. SafeNudge triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SafeNudge is open-source and available through this https URL, and is compatible with models loaded with the Hugging Face "transformers" library.', 'abstract_zh': '大型语言模型（LLMs）已被证明容易受到劫持攻击的影响，这种攻击旨在诱使模型表现出高风险行为。劫持攻击已被网络犯罪分子和黑客利用，对模型造成了重大伤害，这凸显了保护广泛部署模型的迫切需求。保护措施，包括对模型进行微调或使LLMs进行“自我反省”，可能会延长模型的推理时间、增加计算成本、降低输出的语义流畅性，并限制模型的常规行为。重要的是，这些安全性能权衡（SPTs）仍是一个研究不足的领域。在本研究中，我们提出了一种名为SafeNudge的新型保护措施，该措施结合了受控文本生成和“引导”技术，即将文本干预用来改变模型的行为。SafeNudge在执行劫持攻击期间启动，可以将成功的劫持攻击减少30%，通过引导LLM生成安全的响应。它对推理的延迟影响很小，并且对输出的语义流畅性几乎没有影响。此外，SafeNudge允许调节安全性能权衡。SafeNudge是开源的，并可通过以下链接获取：[该链接]，并与Hugging Face“transformers”库加载的模型兼容。', 'title_zh': '实时动态调整安全与性能权衡以保障大型语言模型安全'}
{'arxiv_id': 'arXiv:2501.02009', 'title': 'Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts', 'authors': 'Youcheng Huang, Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv', 'link': 'https://arxiv.org/abs/2501.02009', 'abstract': "Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.", 'abstract_zh': '理解大型语言模型（LLMs）的工作原理是研究的重要前沿领域。先前的研究表明，单个LLM的概念表示可以被捕捉为引导向量（SVs），从而控制LLM的行为（例如，使其倾向于生成有害内容）。我们的工作采取了一种新的方法，通过探索不同LLM之间概念表示的复杂关系，从而引出了柏拉图洞穴寓言的一个有趣的平行关系。具体来说，我们引入了一种线性变换方法来连接这些表示，并提出了以下三个关键发现：1）可以使用简单的线性变换有效地对不同LLM之间的概念表示进行对齐，从而通过SVs实现高效的跨模型转移和行为控制。2）这种线性变换适用于不同概念，便于在不同LLM之间对不同概念的SVs进行对齐和控制。3）存在从弱到强的概念表示转移性，即从较小的LLM中提取的SVs可以有效地控制较大LLM的行为。', 'title_zh': '大型语言模型中关于概念的柏拉图式表示之间的跨模型可转移性'}
{'arxiv_id': 'arXiv:2501.02997', 'title': 'CALM: Curiosity-Driven Auditing for Large Language Models', 'authors': 'Xiang Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang', 'link': 'https://arxiv.org/abs/2501.02997', 'abstract': 'Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at this https URL.', 'abstract_zh': '审查大规模语言模型（LLMs）是至关重要且具有挑战性的任务。在本研究中，我们重点关注在无法访问LLMs参数的情况下审查黑盒LLMs，只能够使用提供的服务。我们将这种审查类型视为黑盒优化问题，其目标是自动发现目标LLMs中表现出非法、不道德或不安全行为的输入-输出对。例如，我们可能寻求一个无害的输入，而目标LLM却以有害的方式响应，或者一个能够引发目标LLM生成包含政治敏感人物的幻觉输出的输入。由于可行点稀缺、提示空间的离散性质以及搜索空间庞大，这种黑盒优化具有挑战性。为了应对这些挑战，我们提出了Curiosity-Driven Auditing for Large Language Models（CALM），该方法使用本体驱动的强化学习来微调一个LLM作为审计代理，以发现目标LLMs中潜在有害和偏见的输入-输出对。CALM成功地识别了涉及名人并对特定名称产生反应的诋毁完成，即使是在黑盒设置中。本项工作为审查黑盒LLMs提供了有前景的方向。我们的代码可在以下网址获得：此 https URL。', 'title_zh': 'CALM：好奇心驱动的大语言模型审计'}
{'arxiv_id': 'arXiv:2501.02711', 'title': 'KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models', 'authors': 'Zaiyi Zheng, Yushun Dong, Song Wang, Haochen Liu, Qi Wang, Jundong Li', 'link': 'https://arxiv.org/abs/2501.02711', 'abstract': "Large Language Models (LLMs) have shown impressive performance in various tasks, including knowledge graph completion (KGC). However, current studies mostly apply LLMs to classification tasks, like identifying missing triplets, rather than ranking-based tasks, where the model ranks candidate entities based on plausibility. This focus limits the practical use of LLMs in KGC, as real-world applications prioritize highly plausible triplets. Additionally, while graph paths can help infer the existence of missing triplets and improve completion accuracy, they often contain redundant information. To address these issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks. KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts, achieving superior results on real-world datasets. The code and datasets are available at \\url{this https URL}.", 'abstract_zh': '大的语言模型（LLMs）在各种任务中表现出了令人印象深刻的性能，包括知识图谱补全（KGC）。然而，当前的研究主要将LLMs应用于分类任务，例如识别缺失的三元组，而不是排名任务，在排名任务中，模型会根据可行性对候选实体进行排序。这种关注限制了LLMs在KGC中的实际应用，因为实际应用更倾向于选择高度可行的三元组。此外，尽管图路径可以帮助推断缺失三元组的存在性并提高补全的准确性，但它们往往包含冗余信息。为了应对这些问题，我们提出了KG-CF框架，该框架针对排名型KGC任务进行了专门设计。KG-CF利用LLMs的推理能力过滤掉无关的上下文，从而在实际数据集上取得了优异的结果。相关代码和数据集可在 \\url{此链接} 获取。', 'title_zh': 'KG-CF：在大型语言模型指导下基于上下文过滤的知识图谱完成方法'}
{'arxiv_id': 'arXiv:2501.02486', 'title': 'LLMPC: Large Language Model Predictive Control', 'authors': 'Gabriel Maher', 'link': 'https://arxiv.org/abs/2501.02486', 'abstract': 'Recent advancements in prompting techniques for Large Language Models (LLMs) have improved their reasoning, planning, and action abilities. This paper examines these prompting techniques through the lens of model predictive control (MPC). We show that LLMs act as implicit planning cost function minimizers when planning prompts are used. Under our framework we demonstrate that LLM planning performance can be improved further by incorporating real planning cost functions and evaluators.', 'abstract_zh': '近年来，针对大型语言模型（LLMs）的提示技术取得了进步，这些技术提高了它们的推理、规划和行动能力。本文通过模型预测控制（MPC）的视角考察了这些提示技术。我们表明，当使用规划提示时，LLMs实际上充当着隐含规划成本函数的最小化者。在我们提出的框架下，我们证明通过引入实际的规划成本函数和评估器，可以进一步提高LLMs的规划性能。', 'title_zh': 'LLMPC: 大型语言模型预测控制'}
{'arxiv_id': 'arXiv:2501.02152', 'title': 'Table as Thought: Exploring Structured Thoughts in LLM Reasoning', 'authors': 'Zhenjie Sun, Naihao Deng, Haofei Yu, Jiaxuan You', 'link': 'https://arxiv.org/abs/2501.02152', 'abstract': "Large language models' reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition.", 'abstract_zh': '大型语言模型的推理能力可以从组织其思维过程的方法中受益，例如链式思维提示（chain-of-thought prompting），该方法采用顺序结构逐步引导推理过程。然而，现有的方法主要侧重于组织思维的顺序，而忽视了个体思维步骤中的结构。为了解决这一问题，我们提出了一种名为“Table as Thought”的框架，该框架受到人类思维认知神经科学理论的启发。Table as Thought在表格模式下组织推理，其中行表示顺序的思维步骤，列则捕捉关键约束条件和上下文信息以增强推理。推理过程通过逐步填充表格，直至自我验证确保推理的完整性和正确性。我们的实验结果显示，与无结构思维基线相比，Table as Thought在规划任务方面表现优异，并展示了在数学推理方面显著提升大型语言模型性能的强大潜力。这项工作为改进大型语言模型中思维表示的新探索开辟了道路，为推理和人工智能认知的进步奠定了基础。', 'title_zh': '作为思考的表格：探究大模型推理中的结构化思维'}
{'arxiv_id': 'arXiv:2501.02173', 'title': 'The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM Recommender Systems Using Multi-Head Early Exit', 'authors': 'Huixue Zhou, Hengrui Gu, Xi Liu, Kaixiong Zhou, Mingfu Liang, Yongkang Xiao, Srinivas Govindan, Piyush Chawla, Jiyan Yang, Xiangfei Meng, Huayu Li, Buyun Zhang, Liang Luo, Wen-Yen Chen, Yiping Han, Bo Long, Rui Zhang, Tianlong Chen', 'link': 'https://arxiv.org/abs/2501.02173', 'abstract': 'The deployment of Large Language Models (LLMs) in recommender systems for predicting Click-Through Rates (CTR) necessitates a delicate balance between computational efficiency and predictive accuracy. This paper presents an optimization framework that combines Retrieval-Augmented Generation (RAG) with an innovative multi-head early exit architecture to concurrently enhance both aspects. By integrating Graph Convolutional Networks (GCNs) as efficient retrieval mechanisms, we are able to significantly reduce data retrieval times while maintaining high model performance. The early exit strategy employed allows for dynamic termination of model inference, utilizing real-time predictive confidence assessments across multiple heads. This not only quickens the responsiveness of LLMs but also upholds or improves their accuracy, making it ideal for real-time application scenarios. Our experiments demonstrate how this architecture effectively decreases computation time without sacrificing the accuracy needed for reliable recommendation delivery, establishing a new standard for efficient, real-time LLM deployment in commercial systems.', 'abstract_zh': '大型语言模型（LLMs）在推荐系统中的部署对于预测点击率（CTR）而言，需要在计算效率和预测准确性之间保持微妙的平衡。本文提出了一种优化框架，结合了检索增强生成（RAG）与一种创新的多头早期退出架构，以同时提升这两个方面。通过将图卷积网络（GCNs）作为高效的检索机制进行整合，我们能够在降低数据检索时间的同时，保持高模型性能。所采用的早期退出策略允许模型推理的动态终止，通过多个头的实时预测置信度评估来进行。这不仅能加快大语言模型的响应速度，而且还能够维持或提升其准确性，使其适用于实时应用场景。我们的实验表明，这种架构能够在不牺牲可靠推荐所需准确性的前提下有效减少计算时间，从而为商业系统中高效、实时的大语言模型部署设立了新标准。', 'title_zh': '效率与准确性的权衡：使用多头早期退出优化增强型LLM推荐系统'}
{'arxiv_id': 'arXiv:2501.03151', 'title': 'Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches', 'authors': 'Alhassan Mumuni, Fuseini Mumuni', 'link': 'https://arxiv.org/abs/2501.03151', 'abstract': 'Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.', 'abstract_zh': '基于大规模预训练基础模型（PFMs）的生成型人工智能（AI）系统，如视觉-语言模型、大规模语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，已经在广泛的应用领域和上下文中展示了解决复杂且真正非平凡AI问题的能力。特别是多模态大规模语言模型（MLLMs），它们从大量异质数据源中学习，能够提供丰富细致的世界表征，从而提供了广泛的能动性，包括推理、进行有意义的对话；与人类及其他代理共同解决复杂问题的能力；以及理解人类的社会和情感方面。尽管取得了这一令人印象深刻的成绩，但通过大规模数据集训练的最先进LLMs的认知能力仍然浅薄且脆弱。因此，通用的LLMs在泛化能力方面受到了严重限制。在这些认知领域——体现、符号接地、因果性和记忆——需要得到解决，以便LLMs能够达到人类水平的通用智能。这些概念更符合人类认知，为LLMs赋予了内在的人类认知特性，支持实现物理上可验证、语义上有意义、灵活且更具迁移性的知识和智能。在本文中，我们将讨论上述基础问题，并回顾用于在LLMs中实现这些概念的最先进方法。具体而言，我们将探讨如何通过一种有机的方式利用体现、符号接地、因果性和记忆的原则，以实现人工通用智能（AGI）。', 'title_zh': '大语言模型在通用人工智能（AGI）中的应用：基础原则与方法综述'}
{'arxiv_id': 'arXiv:2501.02725', 'title': 'Artificial Intelligence in Creative Industries: Advances Prior to 2025', 'authors': 'Nantheera Anantrasirichai, Fan Zhang, David Bull', 'link': 'https://arxiv.org/abs/2501.02725', 'abstract': "The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries by enabling innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores the significant technological shifts since our previous review in 2022, highlighting how these developments have expanded creative opportunities and efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss AI integration into post-production workflows, which has significantly accelerated and refined traditional processes. Despite these innovations, challenges remain, particularly for the media industry, due to the demands on communication traffic from creative content. We therefore include data compression and quality assessment in this paper. Furthermore, we highlight the trend toward unified AI frameworks capable of addressing multiple creative tasks and underscore the importance of human oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges to maximize its benefits while addressing associated risks.", 'abstract_zh': '近年来，人工智能（AI）的迅猛发展，尤其是在生成型AI和大型语言模型（LLMs）方面，极大地影响了创意产业，通过促进创新内容创作、提高工作流程效率和普及创意工具的使用。本文探讨了自2022年上次回顾以来的技术变革，突显了这些进展如何扩大了创意机会和效率。这些技术创新增强了文本转图像、文本转视频和多模态生成技术的能力。尤其值得注意的是，大型语言模型方面的重要突破为对话式AI设立了新的基准，而图像生成技术的进步则彻底改变了内容创作方式。此外，我们还讨论了AI在后期制作流程中的集成，这已显著加速和优化了传统流程。尽管如此，仍存在挑战，尤其是在媒体行业，由于创意内容对通信流量的需求。因此，本文还包括数据压缩和质量评估的内容。我们还强调了统一的AI框架的发展趋势，这种框架能够处理多种创意任务，并强调了人工监督的重要性，以减轻AI生成错误的影响。最后，我们探讨了AI在创意领域未来的潜在应用，强调在利用其优势的同时，需要应对新兴挑战并管理相关风险。', 'title_zh': '人工智能在创意产业中的应用：至2025年的进展'}
{'arxiv_id': 'arXiv:2501.02964', 'title': 'Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild', 'authors': 'Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, Changshui Zhang', 'link': 'https://arxiv.org/abs/2501.02964', 'abstract': "Complex visual reasoning remains a key challenge today. Typically, the challenge is tackled using methodologies such as Chain of Thought (COT) and visual instruction tuning. However, how to organically combine these two methodologies for greater success remains unexplored. Also, issues like hallucinations and high training cost still need to be addressed. In this work, we devise an innovative multi-round training and reasoning framework suitable for lightweight Multimodal Large Language Models (MLLMs). Our self-questioning approach heuristically guides MLLMs to focus on visual clues relevant to the target problem, reducing hallucinations and enhancing the model's ability to describe fine-grained image details. This ultimately enables the model to perform well in complex visual reasoning and question-answering tasks. We have named this framework Socratic Questioning(SQ). To facilitate future research, we create a multimodal mini-dataset named CapQA, which includes 1k images of fine-grained activities, for visual instruction tuning and evaluation, our proposed SQ method leads to a 31.2% improvement in the hallucination score. Our extensive experiments on various benchmarks demonstrate SQ's remarkable capabilities in heuristic self-questioning, zero-shot visual reasoning and hallucination mitigation. Our model and code will be publicly available.", 'abstract_zh': '复杂的视觉推理依然是当前一个关键挑战。通常，这一挑战通过使用诸如Chain of Thought（CoT）和视觉指令调优等方法来应对。然而，如何有机地结合这两种方法以取得更大的成功仍然是未解决的问题。此外，幻觉和高昂的训练成本等问题仍然需要解决。在本项工作中，我们设计了一个适用于轻量级多模态大语言模型（MLLMs）的创新多轮训练和推理框架。我们的自我提问方法通过启发式方式引导MLLMs关注与目标问题相关的视觉线索，从而减少幻觉并增强模型描述细粒度图像细节的能力。最终，这一框架使得模型在复杂的视觉推理和问答任务中表现出色。我们将其命名为Socratic Questioning（SQ）。为了促进后续研究，我们创建了一个多模态小型数据集CapQA，其中包括1000张细粒度活动图像，用于视觉指令调优和评估。我们提出的SQ方法在幻觉评分上提高了31.2%。通过对各种基准的大规模实验，我们展示了SQ在启发式自我提问、零样本视觉推理和幻觉缓解方面的卓越能力。我们的模型和代码将对公众开放。', 'title_zh': '苏格拉底式提问：学会在现实世界中自我引导多模态推理'}
{'arxiv_id': 'arXiv:2501.02532', 'title': 'Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm', 'authors': 'Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljević Jerkovic, Ana Jovančevic', 'link': 'https://arxiv.org/abs/2501.02532', 'abstract': "In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis.", 'abstract_zh': '在快速数字化通信的时代，每天都会生成大量文本数据，迫切需要高效的方法来进行潜在内容分析以提取有意义的洞见。大规模语言模型（LLMs）为自动化这一过程提供了潜力，但缺乏涵盖多个维度的全面评估，将它们的表现与人类注释者的表现进行对比。本研究评估了七种最先进的LLMs，包括OpenAI的GPT-4变体、Gemini、Llama和Mistral，在分析 sentiment、政治倾向、情感强度和讽刺检测方面与人类注释者的可靠性和质量。共有33名人类注释者和8种LLM变体评估了100个精心挑选的文本项目，生成了3,300个人类注释和19,200个LLM注释，并在三个时间点对LLMs进行评估，以检查时间一致性。信度分析使用Krippendorff的α系数，而内在一致性相关系数则评估了时间的一致性。结果显示，无论是人类还是LLMs，在情感分析和政治倾向评估中都表现出高度的可靠性，LLMs在内部一致性方面优于人类。在情感强度方面，LLMs的表现比人类更高，尽管人类在情感强度的评分明显高于LLMs。两个群体在讽刺检测方面都存在困难，表现为低一致性。LLMs在所有维度上均表现出色的时间一致性，表明其在时间上的稳定表现。研究结论指出，LLMs，尤其是GPT-4，在情感和政治倾向分析方面可以有效地复制人类分析，尽管人类的专业知识对于情感强度的解释仍然是必不可少的。研究结果表明，LLMs在某些潜在内容分析的领域中可以实现一致性和高质量的性能。', 'title_zh': '在潜在内容分析中，大型语言模型与人工注释者之间的评估：情感、政治倾向、情感强度和讽刺'}
{'arxiv_id': 'arXiv:2501.02471', 'title': 'Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine', 'authors': 'Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, Yong Liang, Liang Liu, Hudan Pan', 'link': 'https://arxiv.org/abs/2501.02471', 'abstract': 'Large language models (LLMs) primarily trained on English texts, often face biases and inaccuracies in Chinese contexts. Their limitations are pronounced in fields like Traditional Chinese Medicine (TCM), where cultural and clinical subtleties are vital, further hindered by a lack of domain-specific data, such as rheumatoid arthritis (RA). To address these issues, this paper introduces Hengqin-RA-v1, the first large language model specifically tailored for TCM with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a comprehensive RA-specific dataset curated from ancient Chinese medical literature, classical texts, and modern clinical studies. This dataset empowers Hengqin-RA-v1 to deliver accurate and culturally informed responses, effectively bridging the gaps left by general-purpose models. Extensive experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models, even surpassing the diagnostic accuracy of TCM practitioners in certain cases.', 'abstract_zh': '大型语言模型（LLMs）主要在英文文本上进行训练，往往在中文背景下表现出偏见和不准确性。在中医（TCM）这一领域尤为突出，因为在中医领域，文化差异和临床细微之处至关重要，而缺乏特定领域的数据，如风湿性关节炎（RA），进一步加剧了这一问题。为了解决这些问题，本文介绍了“横琴-RA-v1”，这是首个专门针对中医领域的大型语言模型，重点在于诊断和治疗RA。我们还提出了HQ-GCM-RA-C1数据集，这是一个针对RA的数据集，从古代中医文献、古典文献和现代临床研究中精心挑选和整理而成。该数据集使“横琴-RA-v1”能够提供准确且文化敏感的响应，有效地弥补了通用模型留下的空白。广泛实验表明，“横琴-RA-v1”在某些情况下甚至超越了中医专业人员的诊断准确性，超过了最先进的模型。', 'title_zh': 'Henqin-RA-v1：基于数据集的传统中药用于类风湿性关节炎的诊断与治疗的高级语言模型'}
{'arxiv_id': 'arXiv:2501.02138', 'title': 'Effective LLM-Driven Code Generation with Pythoness', 'authors': 'Kyla H. Levin, Kyle Gwilt, Emery D. Berger, Stephen N. Freund', 'link': 'https://arxiv.org/abs/2501.02138', 'abstract': 'The advent of large language models (LLMs) has paved the way for a new era of programming tools with both significant capabilities and risks, as the generated code lacks guarantees of correctness and reliability. Developers using LLMs currently face the difficult task of optimizing, integrating, and maintaining code generated by AI. We propose an embedded domain-specific language (DSL), Pythoness, to address those challenges. In Pythoness, developers program with LLMs at a higher level of abstraction. Rather than interacting directly with generated code, developers using Pythoness operate at the level of behavioral specifications when writing functions, classes, or an entire program. These specifications can take the form of unit tests and property-based tests, which may be expressed formally or in natural language. Guided by these specifications, Pythoness generates code that both passes the tests and can be continuously checked during execution. We posit that the Pythoness approach lets developers harness the full potential of LLMs for code generation while substantially mitigating their inherent risks. We describe our current prototype implementation of Pythoness and demonstrate that it can successfully leverage a combination of tests and code generation to yield higher quality code than specifications alone.', 'abstract_zh': '大型语言模型（LLMs）的出现为编程工具开启了一个新时代，这一时代兼具重大能力和潜在风险，因为生成的代码缺乏正确性和可靠性的保证。目前使用LLMs的开发者面临着优化、整合和维护由AI生成代码的艰巨任务。我们提出了一种嵌入式领域特定语言（DSL），即Pythoness，以应对这些挑战。在Pythoness中，开发者以更高的抽象级别与LLMs进行编程。使用Pythoness时，开发者在编写函数、类或整个程序时操作的是行为规范，而不是直接与生成的代码互动。这些规范可以是单元测试和基于属性的测试的形式，既可以正式表达，也可以用自然语言表达。受到这些规范的指导，Pythoness生成的代码不仅能够通过测试，还可以在执行过程中持续检查。我们认为，Pythoness的方法使开发者能够充分利用LLMs的全部潜力进行代码生成，并显著减轻其固有的风险。我们描述了Pythoness当前的原型实现，并展示了它如何通过结合测试和代码生成来产生比仅仅使用规范更高的质量代码。', 'title_zh': '有效的基于大语言模型的代码生成方法——以Pythoness为例'}
