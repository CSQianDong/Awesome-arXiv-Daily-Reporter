{'arxiv_id': 'arXiv:2501.03183', 'title': 'Classifier-Guided Captioning Across Modalities', 'authors': 'Ariel Shaulov, Tal Shaharabany, Eitan Shaar, Gal Chechik, Lior Wolf', 'link': 'https://arxiv.org/abs/2501.03183', 'abstract': 'Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. In this work, we introduce a method to adapt captioning networks to the semantics of alternative settings, such as capturing audibility in audio captioning, where it is crucial to describe sounds and their sources. Our framework consists of two main components: (i) a frozen captioning system incorporating a language model (LM), and (ii) a text classifier that guides the captioning system. The classifier is trained on a dataset automatically generated by GPT-4, using tailored prompts specifically designed to enhance key aspects of the generated captions. Importantly, the framework operates solely during inference, eliminating the need for further training of the underlying captioning model. We evaluate the framework on various models and modalities, with a focus on audio captioning, and report promising results. Notably, when combined with an existing zero-shot audio captioning system, our framework improves its quality and sets state-of-the-art performance in zero-shot audio captioning.', 'abstract_zh': '目前大多数图像字幕系统都是基于特定数据集进行训练的语言模型，例如通过亚马逊MEchanical Turk进行图像字幕训练，这限制了它们在处理其他模态分布和背景下泛化的能力。这种局限性阻碍了在音频或视频字幕等任务中的性能表现，因为这些任务需要不同的语义线索。解决这一挑战对于创建更适应和多功能的字幕框架至关重要，这些框架可以在多种现实场景中适用。在这项工作中，我们提出了一种方法，将字幕网络适应其他语境的语义，例如在音频字幕中捕捉可听性，其中准确描述声音及其来源至关重要。我们的框架主要由两个部分组成：（i）一个冻结的字幕系统，其中包括一个语言模型（LM），以及（ii）一个文本分类器，该分类器指导字幕系统。分类器是通过GPT-4自动生成的数据集进行训练的，使用特定设计的提示以增强生成字幕的关键方面。重要的是，该框架仅在推理阶段运行，无需对基础字幕模型进行进一步训练。我们在多种模型和模态下评估了该框架，重点关注音频字幕，并报告了令人鼓舞的结果。值得注意的是，当与现有的零样本音频字幕系统结合使用时，我们的框架提高了其质量，并在零样本音频字幕性能上设立了新记录。', 'title_zh': '跨模态的分类器引导式描述生成'}
{'arxiv_id': 'arXiv:2501.02552', 'title': 'Multi-LLM Collaborative Caption Generation in Scientific Documents', 'authors': "Jaeyoung Kim, Jongho Lee, Hong-Jun Choi, Ting-Yao Hsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, Clyde Lee Giles, Ting-Hao 'Kenneth' Huang, Sungchul Choi", 'link': 'https://arxiv.org/abs/2501.02552', 'abstract': 'Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at this https URL', 'abstract_zh': '科学图表描述是一项复杂的任务，要求生成与视觉内容上下文相适应的描述。然而，现有方法往往因为利用不完整的信息，将任务简化为单纯的图像到文本或文本总结问题，从而受限，难以生成高质量的描述，全面捕捉必要细节。此外，现有数据来源的 arXiv 论文包含低质量的描述，给训练大型语言模型（LLMs）带来了显著挑战。本文我们提出了一种名为 Multi-LLM Collaborative Figure Caption Generation（MLBCAP）的框架，通过利用专门针对不同子任务的 LLM 来解决这些问题。我们的方法分为三个关键模块：（质量评估）我们利用多模态 LLM 评估培训数据的质量，实现低质量描述的过滤。 （多样化的描述生成）然后，我们采用一种策略，对多个 LLM 进行微调/提示，以生成候选描述。 （判断）最后，我们提示一个主流 LLM 从候选描述中选择最优质描述，并对剩余的不准确性进行修正。人类评估表明，我们方法生成的描述比人工撰写的描述更具信息性，突显了其有效性。我们提供代码链接：this https URL', 'title_zh': '科学技术文档中的多大规模语言模型协作 caption 生成'}
{'arxiv_id': 'arXiv:2501.02549', 'title': 'From Language To Vision: A Case Study of Text Animation', 'authors': 'Ping Chen, Richard Alo, Justin Rundell', 'link': 'https://arxiv.org/abs/2501.02549', 'abstract': 'Information can be expressed in multiple formats including natural language, images, and motions. Human intelligence usually faces little difficulty to convert from one format to another format, which often shows a true understanding of encoded information. Moreover, such conversions have broad application in many real-world applications. In this paper, we present a text visualization system that can visualize free text with animations. Our system is illustrated by visualizing example sentences of elementary Physics laws.', 'abstract_zh': '信息可以以多种格式表达，包括自然语言、图像和动作。人类智能通常在从一种格式转换为另一种格式时几乎没有困难，这通常表明对编码信息的真正理解。此外，这种转换在许多实际应用中具有广泛的应用前景。在此论文中，我们提出了一种文本可视化系统，该系统可以使用动画来可视化自由文本。我们的系统通过可视化基础物理学定律的示例句子进行了展示。', 'title_zh': '从语言到视觉：文本动画案例研究'}
{'arxiv_id': 'arXiv:2501.02434', 'title': 'Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification', 'authors': 'Dongyu Zhang, Shengcheng Yin, Jingwei Yu, Zhiyao Wu, Zhen Li, Chengpei Xu, Xiaoxia Wang, Feng Xia', 'link': 'https://arxiv.org/abs/2501.02434', 'abstract': 'Metaphors play a crucial role in human communication, yet their comprehension remains a significant challenge for natural language processing (NLP) due to the cognitive complexity involved. According to Conceptual Metaphor Theory (CMT), metaphors map a target domain onto a source domain, and understanding this mapping is essential for grasping the nature of metaphors. While existing NLP research has focused on tasks like metaphor detection and sentiment analysis of metaphorical expressions, there has been limited attention to the intricate process of identifying the mappings between source and target domains. Moreover, non-English multimodal metaphor resources remain largely neglected in the literature, hindering a deeper understanding of the key elements involved in metaphor interpretation. To address this gap, we developed a Chinese multimodal metaphor advertisement dataset (namely CM3D) that includes annotations of specific target and source domains. This dataset aims to foster further research into metaphor comprehension, particularly in non-English languages. Furthermore, we propose a Chain-of-Thought (CoT) Prompting-based Metaphor Mapping Identification Model (CPMMIM), which simulates the human cognitive process for identifying these mappings. Drawing inspiration from CoT reasoning and Bi-Level Optimization (BLO), we treat the task as a hierarchical identification problem, enabling more accurate and interpretable metaphor mapping. Our experimental results demonstrate the effectiveness of CPMMIM, highlighting its potential for advancing metaphor comprehension in NLP. Our dataset and code are both publicly available to encourage further advancements in this field.', 'abstract_zh': '比喻在人类沟通中发挥着重要作用，但由于认知复杂性的影响，其理解仍然是自然语言处理（NLP）中的一个重大挑战。根据概念隐喻理论（CMT），比喻将目标域映射到源域，理解这种映射对把握比喻的本质至关重要。尽管现有的NLP研究主要集中在比喻检测和比喻表达的情感分析等任务上，但对于源域和目标域之间复杂映射的识别过程却关注较少。此外，非英语多模态比喻资源在文献中仍然被大量忽略，阻碍了对比喻解读中关键元素的深入理解。为解决这一问题，我们构建了一个中文多模态比喻广告数据集（简称CM3D），该数据集包括了源域和目标域的特定标注。该数据集旨在促进对比喻理解的研究，特别是非英语语言中的比喻理解。此外，我们提出了一种基于链式思维提示的比喻映射识别模型（CPMMIM），该模型模拟了人类识别这些映射的认知过程。我们借鉴了链式思维推理和双层优化（BLO）的理念，将任务视为一个分层识别问题，从而实现更准确和可解释的比喻映射。实验结果表明，CPMMIM 的有效性，突显了其在NLP中推进比喻理解的潜力。我们还公开发布了该数据集和代码，以促进该领域的进一步发展。', 'title_zh': '多模态隐喻理解：汉语隐喻映射识别的数据集与模型探究'}
{'arxiv_id': 'arXiv:2501.03012', 'title': 'Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment', 'authors': 'Pegah Khayatan, Mustafa Shukor, Jayneel Parekh, Matthieu Cord', 'link': 'https://arxiv.org/abs/2501.03012', 'abstract': 'Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at this https URL.', 'abstract_zh': '多模态大语言模型在理解多模态输入方面达到了显著的熟练水平，推动了大量研究以开发越来越强大的模型。然而，对这些模型背后的机制及其解释的关注却相对较少。现有的大多数解释性研究仅关注模型的最终状态，而忽视了训练过程中发生的动态表示转变。在这项工作中，我们系统地分析了隐藏状态表示的演变，揭示了调整如何改变模型的内部结构以专门从事新的多模态任务。我们采用基于概念的方法，将隐藏状态映射到可解释的视觉和文本概念，从而能够追踪训练过程中不同模态编码概念的变化。我们还展示了使用位移向量捕获这些概念变化的方法。这些位移向量允许我们通过调整原模型中的概念获得微调后的概念。最后，我们探讨了我们研究发现的实际影响，展示了可以在不进行训练的情况下调整多模态大语言模型的行为，例如修改答案类型、调整图例风格或使模型倾向于某些特定的响应。我们的研究揭示了微调过程中多模态表示如何演变，并为理解多模态任务中的模型适应提供了新的视角。该项目的代码已公开，可在以下链接访问：[这里](https://example.com/code)。', 'title_zh': '分析多模态LLM微调中的表示转移，以实现对齐'}
{'arxiv_id': 'arXiv:2501.02669', 'title': 'Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?', 'authors': 'Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora', 'link': 'https://arxiv.org/abs/2501.02669', 'abstract': 'While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization.', 'abstract_zh': '尽管视觉语言模型（VLMs）在视觉问答（VQA）和图像字幕等任务中表现出色，但在对图像进行多步推理方面的能力却有所逊色，这导致了模态不平衡或脆弱性的感知。为系统地研究这些问题，我们引入了一个合成框架，用于评估VLMs执行算法视觉推理（AVR）的能力。该框架包括三个任务：表格读取、网格导航和视觉类比。每个任务都有简单的（SIMPLE）和困难的（HARD）两个难度级别，即使简单的版本，对于前沿的VLMs来说也颇具挑战性。我们寻找策略，通过在SIMPLE版本任务上的训练来提升其相应的HARD任务表现，即S2H泛化。此合成框架提供了每个任务都有纯文本版本的特点，使模态不平衡及其受训练策略影响的程度能够得到量化。消融实验表明，在使用自回归训练时，明确的图像到文本转换对于促进S2H泛化的重要性。我们还报告了对这种现象的机制研究结果，包括一种梯度对齐度量，似乎能够识别出促进更好S2H泛化的训练策略。', 'title_zh': '从SIMPLE到HARD视觉推理的泛化：我们能否缓解基于视觉语言模型的模态不平衡问题？'}
{'arxiv_id': 'arXiv:2501.02584', 'title': 'Efficient Architectures for High Resolution Vision-Language Models', 'authors': 'Miguel Carvalho, Bruno Martins', 'link': 'https://arxiv.org/abs/2501.02584', 'abstract': 'Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text.', 'abstract_zh': '视觉-语言模型（VLMs）最近取得了显著的进步。然而，在高分辨率图像中准确识别细粒度细节的问题仍存在挑战，这限制了其在多个任务中的表现。本研究引入了Pheye，一种新型架构，它能够在处理高分辨率图像时比同等规模的VLMs训练更少的参数。值得注意的是，Pheye在保持高性能的同时，特别是在需要细粒度图像理解和/或场景文本处理的任务中表现出色。', 'title_zh': '高效架构的高分辨率视觉-语言模型'}
{'arxiv_id': 'arXiv:2501.02385', 'title': 'Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations', 'authors': 'Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li', 'link': 'https://arxiv.org/abs/2501.02385', 'abstract': 'With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder architectures, leading to the emergence of works like LLava-Med. However, these works primarily operate at the whole-image level, aligning general information from 2D medical images without attending to finer details. As a result, these models often provide irrelevant or non-clinically valuable information while missing critical details. Medical vision-language tasks differ significantly from general images, particularly in their focus on fine-grained details, while excluding irrelevant content. General domain VLMs tend to prioritize global information due to their design, which compresses the entire image into a multi-token representation that is passed into the LLM decoder. Therefore, current VLMs all lack the capability to restrict their attention to particular areas. To address this critical issue in the medical domain, we introduce MedVP, an visual prompt generation and fine-tuning framework, which involves extract medical entities, generate visual prompts, and adapt datasets for visual prompt guided fine-tuning. To the best of our knowledge, this is the first work to explicitly introduce visual prompt into medical VLMs, and we successfully outperform recent state-of-the-art large models across multiple medical VQA datasets. Extensive experiments are conducted to analyze the impact of different visual prompt forms and how they contribute to performance improvement. The results demonstrate both the effectiveness and clinical significance of our approach', 'abstract_zh': '随着大规模语言模型（LLMs）推动下的视觉-语言模型（VLMs）的进步，许多研究人员集中研究了由图像编码器、图像到语言投影层和文本解码器组成的模型架构，催生了如LLava-Med等研究成果。然而，这些模型主要在图像整体层面上工作，关注于2D医学图像的一般信息而忽视了更精细的细节。因此，这些模型往往会提供不相关或缺乏临床价值的信息，同时忽略了一些关键细节。医学视觉-语言任务与一般的图像任务有显著不同，尤其是在专注于细粒度细节方面，同时排除无关内容。通用领域的VLMs因设计原因倾向于强调全局信息，将其压缩成一个包含多个标记的表示，并传递给LLM解码器。因此，目前的VLMs都无法将注意力锁定在特定区域。为解决这一关键问题，我们提出了MedVP，这是一个视觉提示生成与微调框架，包括提取医学实体、生成视觉提示以及为视觉提示引导的微调适应数据集。据我们所知，这是首次明确地在医学VLMs中引入视觉提示的工作，并且我们成功地在多个医学VQA数据集中超越了最近的先进大型模型。进行了广泛的实验以分析不同形式的视觉提示对性能提升的影响。结果表明了我们方法的有效性和临床重要性。', 'title_zh': '引导医学视觉语言模型使用显式视觉提示：框架设计与提示变体的全面探索'}
{'arxiv_id': 'arXiv:2501.02189', 'title': 'Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey', 'authors': 'Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi', 'link': 'https://arxiv.org/abs/2501.02189', 'abstract': 'Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in this https URL.', 'abstract_zh': '多模态视觉语言模型（VLMs）作为计算机视觉和自然语言处理交汇领域的变革性技术，使机器能够通过视觉和文本两种模态感知和推理世界。例如，CLIP、Claude 和 GPT-4V 等模型在视觉和文本数据上的推理和理解能力非常强大，并在零样本分类上超过了传统的单模态视觉模型。尽管这些模型在研究中取得了飞速进展并在应用中越来越受欢迎，但专门针对视觉语言模型的研究综述依然缺乏，尤其对于希望在其特定领域利用 VLMs 的研究人员来说更为如此。为此，我们从以下几个方面系统地概述了 VLMs：过去五年（2019-2024）开发的主要 VLMs 的模型信息；这些 VLMs 的主要架构和训练方法；VLMs 的流行基准和评估指标的总结与分类；VLMs 的应用，包括具身代理、机器人技术和视频生成；当前 VLMs 面临的挑战和问题，如幻觉、公平性和安全性。详细的资料集合包括论文和模型存储库链接，列于以下 URL。', 'title_zh': '大规模视觉语言模型的基准评估、应用与挑战：一项综述'}
{'arxiv_id': 'arXiv:2501.01982', 'title': 'Is Your Image a Good Storyteller?', 'authors': 'Xiujie Song, Xiaoyi Pang, Haifeng Tang, Mengyue Wu, Kenny Q. Zhu', 'link': 'https://arxiv.org/abs/2501.01982', 'abstract': 'Quantifying image complexity at the entity level is straightforward, but the assessment of semantic complexity has been largely overlooked. In fact, there are differences in semantic complexity across images. Images with richer semantics can tell vivid and engaging stories and offer a wide range of application scenarios. For example, the Cookie Theft picture is such a kind of image and is widely used to assess human language and cognitive abilities due to its higher semantic complexity. Additionally, semantically rich images can benefit the development of vision models, as images with limited semantics are becoming less challenging for them. However, such images are scarce, highlighting the need for a greater number of them. For instance, there is a need for more images like Cookie Theft to cater to people from different cultural backgrounds and eras. Assessing semantic complexity requires human experts and empirical evidence. Automatic evaluation of how semantically rich an image will be the first step of mining or generating more images with rich semantics, and benefit human cognitive assessment, Artificial Intelligence, and various other applications. In response, we propose the Image Semantic Assessment (ISA) task to address this problem. We introduce the first ISA dataset and a novel method that leverages language to solve this vision problem. Experiments on our dataset demonstrate the effectiveness of our approach. Our data and code are available at: this https URL.', 'abstract_zh': '在实体水平上量化图像的复杂性相对简单，但对语义复杂性的评估却往往被忽视。事实上，不同图像之间的语义复杂性是有所差异的。语义丰富的图像能够讲出生动有趣的故事，并提供广泛的应用场景。例如，“偷饼干”的图片就是一个这样的例子，它因其较高的语义复杂性广泛用于评估人类的语言和认知能力。此外，语义丰富的图像有助于视觉模型的发展，因为具有有限语义的图像对它们来说变得不再具有挑战性了。然而，这类图像稀缺，突显了需要增加它们的数量。例如，为了适应来自不同文化背景和时代的受众，需要更多的类似于“偷饼干”的图像。评估语义复杂性需要人工专家和实证证据。自动评估一张图像的语义丰富度是挖掘或生成更多语义丰富的图像的第一步，这将有利于人类的认知评估、人工智能以及各种其他应用。为此，我们提出了一项图像语义评估（ISA）任务来解决这个问题。我们介绍了第一个ISA数据集，并提出了一种利用语言解决这一视觉问题的全新方法。在我们数据集上的实验结果证明了我们方法的有效性。我们的数据和代码可在以下链接获取：[这里提供链接]。', 'title_zh': '你的图像是一个好的叙述者吗？'}
{'arxiv_id': 'arXiv:2501.03085', 'title': 'Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment', 'authors': 'Chongxian Chen, Fan Mo, Xin Fan, Hayato Yamana', 'link': 'https://arxiv.org/abs/2501.03085', 'abstract': "Personalized fashion recommendation is a difficult task because 1) the decisions are highly correlated with users' aesthetic appetite, which previous work frequently overlooks, and 2) many new items are constantly rolling out that cause strict cold-start problems in the popular identity (ID)-based recommendation methods. These new items are critical to recommend because of trend-driven consumerism. In this work, we aim to provide more accurate personalized fashion recommendations and solve the cold-start problem by converting available information, especially images, into two attribute graphs focusing on optimized image utilization and noise-reducing user modeling. Compared with previous methods that separate image and text as two components, the proposed method combines image and text information to create a richer attributes graph. Capitalizing on the advancement of large language and vision models, we experiment with extracting fine-grained attributes efficiently and as desired using two different prompts. Preliminary experiments on the IQON3000 dataset have shown that the proposed method achieves competitive accuracy compared with baselines.", 'abstract_zh': '个性化时尚推荐是一个艰巨的任务，因为1) 这些决策高度依赖用户的审美偏好，而此前的研究常常忽略了这一点；2) 不断涌现的新品会导致基于用户ID的传统推荐方法出现严重的冷启动问题。由于受到流行趋势的影响，这些新品非常关键。本文旨在通过将可用信息，尤其是图像信息，转化为两个属性图来提供更准确的个性化时尚推荐，并解决冷启动问题。该方法重点优化了图像利用和噪声减少的用户建模。与此前将图像和文本分离处理的方法不同，本方法结合了图像和文本信息，构建了一个更丰富的属性图。借助大规模语言和视觉模型的最新进展，我们实验性地利用两种不同的提示高效、灵活地提取细微的区别特征。初步实验表明，所提出的方法在IQON3000数据集上的准确性与基准方法相当。', 'title_zh': '基于图像属性和 aesthetics 评价的个性化时尚推荐'}
{'arxiv_id': 'arXiv:2501.03151', 'title': 'Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches', 'authors': 'Alhassan Mumuni, Fuseini Mumuni', 'link': 'https://arxiv.org/abs/2501.03151', 'abstract': 'Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.', 'abstract_zh': '基于大规模预训练基础模型（PFMs）的生成型人工智能（AI）系统，如视觉-语言模型、大规模语言模型（LLMs）、扩散模型和视觉-语言-动作（VLA）模型，已经在广泛的应用领域和上下文中展示了解决复杂且真正非平凡AI问题的能力。特别是多模态大规模语言模型（MLLMs），它们从大量异质数据源中学习，能够提供丰富细致的世界表征，从而提供了广泛的能动性，包括推理、进行有意义的对话；与人类及其他代理共同解决复杂问题的能力；以及理解人类的社会和情感方面。尽管取得了这一令人印象深刻的成绩，但通过大规模数据集训练的最先进LLMs的认知能力仍然浅薄且脆弱。因此，通用的LLMs在泛化能力方面受到了严重限制。在这些认知领域——体现、符号接地、因果性和记忆——需要得到解决，以便LLMs能够达到人类水平的通用智能。这些概念更符合人类认知，为LLMs赋予了内在的人类认知特性，支持实现物理上可验证、语义上有意义、灵活且更具迁移性的知识和智能。在本文中，我们将讨论上述基础问题，并回顾用于在LLMs中实现这些概念的最先进方法。具体而言，我们将探讨如何通过一种有机的方式利用体现、符号接地、因果性和记忆的原则，以实现人工通用智能（AGI）。', 'title_zh': '大语言模型在通用人工智能（AGI）中的应用：基础原则与方法综述'}
{'arxiv_id': 'arXiv:2501.02725', 'title': 'Artificial Intelligence in Creative Industries: Advances Prior to 2025', 'authors': 'Nantheera Anantrasirichai, Fan Zhang, David Bull', 'link': 'https://arxiv.org/abs/2501.02725', 'abstract': "The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries by enabling innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores the significant technological shifts since our previous review in 2022, highlighting how these developments have expanded creative opportunities and efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss AI integration into post-production workflows, which has significantly accelerated and refined traditional processes. Despite these innovations, challenges remain, particularly for the media industry, due to the demands on communication traffic from creative content. We therefore include data compression and quality assessment in this paper. Furthermore, we highlight the trend toward unified AI frameworks capable of addressing multiple creative tasks and underscore the importance of human oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges to maximize its benefits while addressing associated risks.", 'abstract_zh': '近年来，人工智能（AI）的迅猛发展，尤其是在生成型AI和大型语言模型（LLMs）方面，极大地影响了创意产业，通过促进创新内容创作、提高工作流程效率和普及创意工具的使用。本文探讨了自2022年上次回顾以来的技术变革，突显了这些进展如何扩大了创意机会和效率。这些技术创新增强了文本转图像、文本转视频和多模态生成技术的能力。尤其值得注意的是，大型语言模型方面的重要突破为对话式AI设立了新的基准，而图像生成技术的进步则彻底改变了内容创作方式。此外，我们还讨论了AI在后期制作流程中的集成，这已显著加速和优化了传统流程。尽管如此，仍存在挑战，尤其是在媒体行业，由于创意内容对通信流量的需求。因此，本文还包括数据压缩和质量评估的内容。我们还强调了统一的AI框架的发展趋势，这种框架能够处理多种创意任务，并强调了人工监督的重要性，以减轻AI生成错误的影响。最后，我们探讨了AI在创意领域未来的潜在应用，强调在利用其优势的同时，需要应对新兴挑战并管理相关风险。', 'title_zh': '人工智能在创意产业中的应用：至2025年的进展'}
{'arxiv_id': 'arXiv:2501.02964', 'title': 'Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild', 'authors': 'Wanpeng Hu, Haodi Liu, Lin Chen, Feng Zhou, Changming Xiao, Qi Yang, Changshui Zhang', 'link': 'https://arxiv.org/abs/2501.02964', 'abstract': "Complex visual reasoning remains a key challenge today. Typically, the challenge is tackled using methodologies such as Chain of Thought (COT) and visual instruction tuning. However, how to organically combine these two methodologies for greater success remains unexplored. Also, issues like hallucinations and high training cost still need to be addressed. In this work, we devise an innovative multi-round training and reasoning framework suitable for lightweight Multimodal Large Language Models (MLLMs). Our self-questioning approach heuristically guides MLLMs to focus on visual clues relevant to the target problem, reducing hallucinations and enhancing the model's ability to describe fine-grained image details. This ultimately enables the model to perform well in complex visual reasoning and question-answering tasks. We have named this framework Socratic Questioning(SQ). To facilitate future research, we create a multimodal mini-dataset named CapQA, which includes 1k images of fine-grained activities, for visual instruction tuning and evaluation, our proposed SQ method leads to a 31.2% improvement in the hallucination score. Our extensive experiments on various benchmarks demonstrate SQ's remarkable capabilities in heuristic self-questioning, zero-shot visual reasoning and hallucination mitigation. Our model and code will be publicly available.", 'abstract_zh': '复杂的视觉推理依然是当前一个关键挑战。通常，这一挑战通过使用诸如Chain of Thought（CoT）和视觉指令调优等方法来应对。然而，如何有机地结合这两种方法以取得更大的成功仍然是未解决的问题。此外，幻觉和高昂的训练成本等问题仍然需要解决。在本项工作中，我们设计了一个适用于轻量级多模态大语言模型（MLLMs）的创新多轮训练和推理框架。我们的自我提问方法通过启发式方式引导MLLMs关注与目标问题相关的视觉线索，从而减少幻觉并增强模型描述细粒度图像细节的能力。最终，这一框架使得模型在复杂的视觉推理和问答任务中表现出色。我们将其命名为Socratic Questioning（SQ）。为了促进后续研究，我们创建了一个多模态小型数据集CapQA，其中包括1000张细粒度活动图像，用于视觉指令调优和评估。我们提出的SQ方法在幻觉评分上提高了31.2%。通过对各种基准的大规模实验，我们展示了SQ在启发式自我提问、零样本视觉推理和幻觉缓解方面的卓越能力。我们的模型和代码将对公众开放。', 'title_zh': '苏格拉底式提问：学会在现实世界中自我引导多模态推理'}
{'arxiv_id': 'arXiv:2501.02778', 'title': 'ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction', 'authors': 'Binyu Zhang, Zhu Meng, Junhao Dong, Fei Su, Zhicheng Zhao', 'link': 'https://arxiv.org/abs/2501.02778', 'abstract': 'Survival prediction is a crucial task in the medical field and is essential for optimizing treatment options and resource allocation. However, current methods often rely on limited data modalities, resulting in suboptimal performance. In this paper, we propose an Integrated Cross-modal Fusion Network (ICFNet) that integrates histopathology whole slide images, genomic expression profiles, patient demographics, and treatment protocols. Specifically, three types of encoders, a residual orthogonal decomposition module and a unification fusion module are employed to merge multi-modal features to enhance prediction accuracy. Additionally, a balanced negative log-likelihood loss function is designed to ensure fair training across different patients. Extensive experiments demonstrate that our ICFNet outperforms state-of-the-art algorithms on five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and shows its potential to support clinical decision-making and advance precision medicine. The codes are available at: this https URL.', 'abstract_zh': '生存预测是医学领域的一项关键任务，对于优化治疗方案和资源分配至关重要。然而，当前的方法往往依赖于有限的数据模态，导致性能欠佳。本文提出了一种综合跨模态融合网络（ICFNet），将组织病理学全切片图像、基因表达谱、患者人口统计信息以及治疗方案结合在一起。具体地，该网络采用了三种编码器、一个残差正交分解模块和一个统一融合模块来合并多模态特征，以提高预测准确性。此外，设计了一种平衡的负对数似然损失函数，以确保在不同患者之间公平训练。大量实验表明，我们的ICFNet在TCGA发布的五个公开数据集（BLCA、BRCA、GBMLGG、LUAD和UCEC）上优于最先进的算法，并且展示了其支持临床决策和推动精准医疗的应用潜力。代码可在以下链接获取：this https URL。', 'title_zh': 'ICFNet：综合跨模态融合网络用于生存预测'}
{'arxiv_id': 'arXiv:2501.02765', 'title': 'Visual Large Language Models for Generalized and Specialized Applications', 'authors': 'Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, Yu Kong', 'link': 'https://arxiv.org/abs/2501.02765', 'abstract': 'Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: this https URL.', 'abstract_zh': '视觉-语言模型（VLM）已经成为学习视觉和语言统一嵌入空间的有力工具。受到大型语言模型的启发，这些模型在推理和多任务处理方面表现出色，视觉大型语言模型（VLLMs）正在逐渐引起关注，用于构建通用的VLM框架。尽管在VLLMs方面取得了显著进展，但在全面应用视角的研究仍然有限，尤其涵盖视觉（图像、视频、深度）、动作和语言等不同模态的一般性和专门化应用。在本文综述中，我们重点关注VLLMs的多样应用场景，考察其使用情况，识别伦理考量和挑战，并讨论其未来发展方向。通过综合这些内容，我们旨在提供一个全面的指南，以推动VLLMs的创新和更广泛的应用。本文的参考文献库地址如下：[这里提供链接]', 'title_zh': '视觉大型语言模型在通用和专用应用中的应用'}
{'arxiv_id': 'arXiv:2501.02699', 'title': 'EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models', 'authors': 'Andrés Villa, Juan León Alcázar, Motasem Alfarra, Vladimir Araujo, Alvaro Soto, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2501.02699', 'abstract': 'Large language models and vision transformers have demonstrated impressive zero-shot capabilities, enabling significant transferability in downstream tasks. The fusion of these models has resulted in multi-modal architectures with enhanced instructional capabilities. Despite incorporating vast image and language pre-training, these multi-modal architectures often generate responses that deviate from the ground truth in the image data. These failure cases are known as hallucinations. Current methods for mitigating hallucinations generally focus on regularizing the language component, improving the fusion module, or ensembling multiple visual encoders to improve visual representation. In this paper, we address the hallucination issue by directly enhancing the capabilities of the visual component. Our approach, named EAGLE, is fully agnostic to the LLM or fusion module and works as a post-pretraining approach that improves the grounding and language alignment of the visual encoder. We show that a straightforward reformulation of the original contrastive pre-training task results in an improved visual encoder that can be incorporated into the instructional multi-modal architecture without additional instructional training. As a result, EAGLE achieves a significant reduction in hallucinations across multiple challenging benchmarks and tasks.', 'abstract_zh': '大型语言模型和视觉变换器展示了令人印象深刻的零样本能力，这使得它们在下游任务中具有显著的迁移性能。将这些模型融合起来导致了具有增强指令能力的多模态架构。尽管这些多模态架构进行了大量的图像和语言预训练，但在图像数据上它们往往会产生与事实不符的响应，这种情况被称为幻觉。目前缓解幻觉的方法主要集中在语言部分的正则化、改进融合模块或集成多个视觉编码器以提高视觉表示。\n\n本文通过直接增强视觉组件的能力来解决幻觉问题。我们提出的方法命名为EAGLE，该方法对大型语言模型（LLM）或融合模块具有完全的普适性，并作为一种后预训练方法，该方法可以提高视觉编码器的定位和语言对齐能力。我们显示，对原始对比预训练任务进行简单的重构可以使视觉编码器性能提升，并且无需额外的指令训练即可将其整合到多模态指令架构中。因此，EAGLE在多个具有挑战性的基准和任务上实现了幻觉的显著减少。', 'title_zh': 'EAGLE：增强视觉定位减少指令多模态模型中的幻觉'}
{'arxiv_id': 'arXiv:2501.02649', 'title': 'Tighnari: Multi-modal Plant Species Prediction Based on Hierarchical Cross-Attention Using Graph-Based and Vision Backbone-Extracted Features', 'authors': 'Haixu Liu, Penghao Jiang, Zerui Tao, Muyan Wan, Qiuzhuang Sun', 'link': 'https://arxiv.org/abs/2501.02649', 'abstract': 'Predicting plant species composition in specific spatiotemporal contexts plays an important role in biodiversity management and conservation, as well as in improving species identification tools. Our work utilizes 88,987 plant survey records conducted in specific spatiotemporal contexts across Europe. We also use the corresponding satellite images, time series data, climate time series, and other rasterized environmental data such as land cover, human footprint, bioclimatic, and soil variables as training data to train the model to predict the outcomes of 4,716 plant surveys. We propose a feature construction and result correction method based on the graph structure. Through comparative experiments, we select the best-performing backbone networks for feature extraction in both temporal and image modalities. In this process, we built a backbone network based on the Swin-Transformer Block for extracting temporal Cubes features. We then design a hierarchical cross-attention mechanism capable of robustly fusing features from multiple modalities. During training, we adopt a 10-fold cross-fusion method based on fine-tuning and use a Threshold Top-K method for post-processing. Ablation experiments demonstrate the improvements in model performance brought by our proposed solution pipeline.', 'abstract_zh': '在特定时空背景下预测植物物种组成对于生物多样性管理和保护以及提高物种识别工具具有重要意义。我们的工作利用了遍布欧洲的88,987份植物调查记录。我们还使用了相应的遥感图像、时间序列数据、气候时间序列以及土地覆盖、人类足迹、生物气候和土壤变量等栅格化环境数据作为训练数据，以训练模型预测4,716份植物调查的结果。我们提出了基于图结构的特征构建和结果修正方法。通过对比实验，我们选择了在时间和图像模态下表现最佳的骨干网络，用于特征提取。在此过程中，我们基于Swin-Transformer Block构建了一个用于提取时间立方体特征的骨干网络。随后，我们设计了一个层次交叉注意力机制，以稳健地融合多种模态的特征。在训练过程中，我们采用基于微调的十折交叉融合方法，并使用阈值Top-K方法进行后处理。消融实验表明，我们提出的解决方案管道显著提升了模型性能。', 'title_zh': 'Tighnari：基于图结构和视觉骨干特征的分层交叉注意力多模态植物物种预测'}
{'arxiv_id': 'arXiv:2501.02523', 'title': 'Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation', 'authors': 'Dawei Dai, Mingming Jia, Yinxiu Zhou, Hang Xing, Chenghang Li', 'link': 'https://arxiv.org/abs/2501.02523', 'abstract': 'Facial images have extensive practical applications. Although the current large-scale text-image diffusion models exhibit strong generation capabilities, it is challenging to generate the desired facial images using only text prompt. Image prompts are a logical choice. However, current methods of this type generally focus on general domain. In this paper, we aim to optimize image makeup techniques to generate the desired facial images. Specifically, (1) we built a dataset of 4 million high-quality face image-text pairs (FaceCaptionHQ-4M) based on LAION-Face to train our Face-MakeUp model; (2) to maintain consistency with the reference facial image, we extract/learn multi-scale content features and pose features for the facial image, integrating these into the diffusion model to enhance the preservation of facial identity features for diffusion models. Validation on two face-related test datasets demonstrates that our Face-MakeUp can achieve the best comprehensive this http URL codes are available at:this https URL', 'abstract_zh': '面部图像在实际应用中有广泛的应用。尽管当前的大型文本-图像扩散模型具有强大的生成能力，仅通过文本提示生成所需的面部图像仍然是一个挑战。图像提示是一个合理的选择。然而，现有的此类方法通常侧重于通用领域。在本文中，我们旨在优化面部妆容技术以生成所需的面部图像。具体而言，(1) 基于LAION-Face，我们构建了一个包含400万高质量面部图像-文本对的数据集（FaceCaptionHQ-4M）来训练我们的Face-MakeUp模型；(2) 为了与参考面部图像保持一致，我们提取/学习多尺度内容特征和姿态特征，并将其整合到扩散模型中，以增强面部身份特征在扩散模型中的保留。通过对两个面部相关的测试数据集进行验证，我们的Face-MakeUp能够实现最佳的整体效果。源代码可在以下链接获取：[代码链接]', 'title_zh': '面部妆容生成：用于文本到图像生成的多模态面部提示'}
{'arxiv_id': 'arXiv:2501.02461', 'title': 'FedRSClip: Federated Learning for Remote Sensing Scene Classification Using Vision-Language Models', 'authors': 'Hui Lin, Chao Zhang, Danfeng Hong, Kexin Dong, Congcong Wen', 'link': 'https://arxiv.org/abs/2501.02461', 'abstract': 'Remote sensing data is often distributed across multiple institutions, and due to privacy concerns and data-sharing restrictions, leveraging large-scale datasets in a centralized training framework is challenging. Federated learning offers a promising solution by enabling collaborative model training across distributed data sources without requiring data centralization. However, current Vision-Language Models (VLMs), which typically contain billions of parameters, pose significant communication challenges for traditional federated learning approaches based on model parameter updates, as they would incur substantial communication costs. In this paper, we propose FedRSCLIP, the first federated learning framework designed for remote sensing image classification based on a VLM, specifically CLIP. FedRSCLIP addresses the challenges of data heterogeneity and large-scale model transmission in federated environments by introducing Prompt Learning, which optimizes only a small set of tunable parameters. The framework introduces a dual-prompt mechanism, comprising Shared Prompts for global knowledge sharing and Private Prompts for client-specific adaptation. To maintain semantic coherence between shared and private prompts, we propose the Dual Prompt Alignment Constraint to balance global consistency and local adaptability across diverse client distributions. Additionally, to enhance cross-modal representation learning, we introduce the Cross-Modal Feature Alignment Constraint to align multimodal features between text and image prompts. To validate the effectiveness of our proposed model, we construct a Fed-RSIC dataset based on three existing remote sensing image classification datasets, specifically designed to simulate various federated learning configurations. Experimental results demonstrate the effectiveness and superiority of FedRSCLIP in remote sensing image classification.', 'abstract_zh': '遥感数据通常分布在多个机构中，由于隐私问题和数据共享限制，在集中式训练框架中利用大规模数据集具有挑战性。联邦学习提供了一种有前途的解决方案，它可以在分布式数据源上进行协作模型训练，而无需数据集中化。然而，当前基于视觉语言模型（VLM）的视觉语言模型通常包含数十亿个参数，这些模型对传统的基于模型参数更新的联邦学习方法提出了重大通信挑战，因为它们会导致显著的通信成本。在本文中，我们提出了 FedRSCLIP，这是一种专门基于 CLIP 的视觉语言模型设计的联邦学习框架，用于遥感图像分类。FedRSCLIP 通过引入提示学习来解决联邦环境中数据异质性和大规模模型传输的挑战，优化仅一小部分可调参数。该框架引入了一种双重提示机制，包括用于全局知识共享的共享提示和用于客户端特定适应的私有提示。为保持共享和私有提示之间的语义一致性，我们提出了双重提示对齐约束，以平衡全球一致性与在多样化客户端分布中的局部适应性。此外，为了增强跨模态表示学习，我们引入了跨模态特征对齐约束，以在文本和图像提示之间对齐多模态特征。为了验证我们所提出模型的有效性，我们基于三个现有的遥感图像分类数据集构建了一个 Fed-RSIC 数据集，专门设计用于模拟各种联邦学习配置。实验结果表明，FedRSCLIP 在遥感图像分类中的有效性和优越性。', 'title_zh': 'FedRSClip：基于视觉语言模型的遥感场景分类联邦学习'}
{'arxiv_id': 'arXiv:2501.02285', 'title': 'Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding', 'authors': 'Yingjie Liu, Pengyu Zhang, Ziyao He, Mingsong Chen, Xuan Tang, Xian Wei', 'link': 'https://arxiv.org/abs/2501.02285', 'abstract': 'Hyperbolic spaces allow for more efficient modeling of complex, hierarchical structures, which is particularly beneficial in tasks involving multi-modal data. Although hyperbolic geometries have been proven effective for language-image pre-training, their capabilities to unify language, image, and 3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud modality in hyperbolic multi-modal contrastive pre-training. Additionally, we explore the entailment, modality gap, and alignment regularizers for learning hierarchical 3D embeddings and facilitating the transfer of knowledge from both Text and Image modalities. These regularizers enable the learning of intra-modal hierarchy within each modality and inter-modal hierarchy across text, 2D images, and 3D Point this http URL results demonstrate that our proposed training strategy yields an outstanding 3D Point Cloud encoder, and the obtained 3D Point Cloud hierarchical embeddings significantly improve performance on various downstream tasks.', 'abstract_zh': '双曲空间能够更有效地建模复杂的层次结构，这在涉及多模态数据的任务中特别有益。尽管已证实在语言-图像预训练任务中双曲几何是有效的，但其统一语言、图像和3D点云模态的能力还有待探索。我们扩展了在双曲多模态对比预训练中3D点云模态的应用。此外，我们还探讨了蕴含关系、模态差异和对齐正则化器，以学习层次化的3D嵌入，并促进从文本和图像模态的知识迁移。这些正则化器能够学习每个模态内的层次结构和跨文本、二维图像和3D点云的层次结构。实验结果表明，我们提出的一种训练策略生成了出色的3D点云编码器，获得的3D点云层次化嵌入显著提高了各种下游任务的性能。', 'title_zh': '双曲对比学习在层次化3D点云嵌入中的应用'}
{'arxiv_id': 'arXiv:2501.02268', 'title': 'What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph', 'authors': 'Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, Yiyi Zhou', 'link': 'https://arxiv.org/abs/2501.02268', 'abstract': 'Recent Multimodal Large Language Models(MLLMs) often use a large number of visual tokens to compensate their visual shortcoming, leading to excessive computation and obvious visual redundancy. In this paper, we investigate what kind of visual tokens are needed for MLLMs, and reveal that both foreground and background tokens are critical for MLLMs given the varying difficulties of examples. Based on this observation, we propose a graph-based method towards training-free visual token pruning, termed this http URL particular, G-Prune regards visual tokens as nodes, and construct their connections based on their semantic similarities. Afterwards, the information flow is propagated via weighted links, and the most important tokens after iterations are kept for MLLMs, which can be front or this http URL validate G-Prune, we apply it to a recent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of this http URL experiment results show that G-Prune can greatly reduce computation overhead while retaining high performance on both coarse- and fine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of LLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops, respectively.', 'abstract_zh': '近年来，多模态大规模语言模型（MLLMs）经常使用大量的视觉标记来弥补其视觉方面的不足，导致了计算量过大和明显的视觉冗余。本文探讨了MLLMs需要哪些视觉标记，并揭示了在不同难度的示例中，前景和背景标记都是关键的。基于这一观察，我们提出了一种基于图的方法，用于无训练的视觉标记修剪，称之为这种特定方法。特别地，G-Prune将视觉标记视为节点，并根据它们的语义相似性构建连接。此后，信息通过加权链接传播，在迭代后保留最重要的标记用于MLLMs，这些标记可以是前景的或背景的。为了验证G-Prune的有效性，我们将其应用于最近的一种MLLM（LLaVA-NeXT），并在一组实验中进行了广泛测试。实验结果表明，G-Prune可以大大减少计算开销，同时在粗粒度和细粒度任务上保持高性能。例如，G-Prune可以在VQA2.0和TextVQA上将LLaVA-NeXT的FLOPs分别减少63.57%，仅损失0.95%和2.34%的准确性。', 'title_zh': '我们需要什么样的视觉标记？基于图论视角的无训练视觉标记剪枝方法用于多模态大型语言模型'}
{'arxiv_id': 'arXiv:2501.02135', 'title': 'AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs', 'authors': 'Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha', 'link': 'https://arxiv.org/abs/2501.02135', 'abstract': "With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multi-modal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark we extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks. We will publicly release our code and benchmark to facilitate future research in this direction.", 'abstract_zh': '随着多模态大型语言模型（MLLMs）的迅速发展，近年来已开发出若干诊断基准，以评估这些模型的多模态推理能力。然而，这些基准主要侧重于评估视觉方面的表现，而不考查整体的视听（AV）理解能力。此外，目前尚无基准来调查AVLLMs在面对扰动输入时调整其响应的能力。为解决这一问题，我们提出了一个名为AVTrustBench的视听可信度评估基准，该基准包括60万个样本，涵盖9个精心设计的任务，从三个不同的维度评估AVLLMs的能力：对抗性攻击、组成性推理和模态特定依赖。利用我们提出的基准，我们广泛评估了13个最新的AVLLMs。研究结果表明，现有的大多数模型在实现人类级别的理解方面存在显著差距，为未来的研究方向提供了宝贵见解。为缓解现有方法的局限性，我们进一步提出了一种鲁棒的、模型无关的音频-视觉偏好优化训练策略（CAVPref），在所有9个任务中，这一策略获得了高达30.19%的性能提升。我们将公开发布我们的代码和基准，以促进该领域的未来研究。', 'title_zh': 'AVTrustBench: 评估和提升音频-视觉大语言模型的可靠性和鲁棒性'}
{'arxiv_id': 'arXiv:2501.02064', 'title': 'ArtCrafter: Text-Image Aligning Style Transfer via Embedding Reframing', 'authors': 'Nisha Huang, Kaer Huang, Yifan Pu, Jiangshan Wang, Jie Guo, Yiqiang Yan, Xiu Li', 'link': 'https://arxiv.org/abs/2501.02064', 'abstract': 'Recent years have witnessed significant advancements in text-guided style transfer, primarily attributed to innovations in diffusion models. These models excel in conditional guidance, utilizing text or images to direct the sampling process. However, despite their capabilities, direct conditional guidance approaches often face challenges in balancing the expressiveness of textual semantics with the diversity of output results while capturing stylistic features. To address these challenges, we introduce ArtCrafter, a novel framework for text-to-image style transfer. Specifically, we introduce an attention-based style extraction module, meticulously engineered to capture the subtle stylistic elements within an image. This module features a multi-layer architecture that leverages the capabilities of perceiver attention mechanisms to integrate fine-grained information. Additionally, we present a novel text-image aligning augmentation component that adeptly balances control over both modalities, enabling the model to efficiently map image and text embeddings into a shared feature space. We achieve this through attention operations that enable smooth information flow between modalities. Lastly, we incorporate an explicit modulation that seamlessly blends multimodal enhanced embeddings with original embeddings through an embedding reframing design, empowering the model to generate diverse outputs. Extensive experiments demonstrate that ArtCrafter yields impressive results in visual stylization, exhibiting exceptional levels of stylistic intensity, controllability, and diversity.', 'abstract_zh': '近年来，文本导向的风格迁移取得了显著进展，主要得益于扩散模型的创新。这些模型在条件引导方面表现出色，能够利用文本或图像来指导采样过程。然而，尽管这些模型具有强大的能力，直接的条件引导方法在平衡文本语义的表达性和输出结果的多样性方面仍然面临挑战，尤其是在捕捉风格特征方面。为了解决这些问题，我们提出了ArtCrafter，一种新颖的文字到图像风格迁移框架。具体而言，我们引入了一种基于注意力的风格提取模块，该模块精心设计以捕捉图像中的细微风格元素。该模块采用多层架构，利用感知注意力机制整合细粒度信息。此外，我们提出了一种新颖的文字与图像对齐增强组件，该组件能够巧妙地平衡两种模态的控制，使模型能够高效地将图像和文本嵌入映射到共享特征空间。我们通过注意力操作实现不同模态之间的平滑信息流来实现这一点。最后，我们引入了一种显式的调节机制，通过嵌入重构设计将多模态增强嵌入与原始嵌入无缝融合，使模型能够生成多样化的输出。广泛的实验结果表明，ArtCrafter在视觉风格化方面取得了显著成果，展现了极高的风格强度、可控性和多样性。', 'title_zh': 'ArtCrafter：通过嵌入重构实现文本-图像对齐的风格转移'}
{'arxiv_id': 'arXiv:2501.02032', 'title': 'Dynamic Feature Fusion: Combining Global Graph Structures and Local Semantics for Blockchain Fraud Detection', 'authors': 'Zhang Sheng, Liangliang Song, Yanbin Wang', 'link': 'https://arxiv.org/abs/2501.02032', 'abstract': 'The advent of blockchain technology has facilitated the widespread adoption of smart contracts in the financial sector. However, current fraud detection methodologies exhibit limitations in capturing both global structural patterns within transaction networks and local semantic relationships embedded in transaction data. Most existing models focus on either structural information or semantic features individually, leading to suboptimal performance in detecting complex fraud this http URL this paper, we propose a dynamic feature fusion model that combines graph-based representation learning and semantic feature extraction for blockchain fraud detection. Specifically, we construct global graph representations to model account relationships and extract local contextual features from transaction data. A dynamic multimodal fusion mechanism is introduced to adaptively integrate these features, enabling the model to capture both structural and semantic fraud patterns effectively. We further develop a comprehensive data processing pipeline, including graph construction, temporal feature enhancement, and text preprocessing. Experimental results on large-scale real-world blockchain datasets demonstrate that our method outperforms existing benchmarks across accuracy, F1 score, and recall metrics. This work highlights the importance of integrating structural relationships and semantic similarities for robust fraud detection and offers a scalable solution for securing blockchain systems.', 'abstract_zh': '区块链技术的出现促进了智能合约在金融领域的广泛应用。然而，当前的欺诈检测方法在捕捉交易网络中的全局结构模式和交易数据中嵌入的局部语义关系方面存在局限性。现有的大多数模型分别侧重于结构信息或语义特征，导致在检测复杂欺诈方面表现不佳（https://this.http.url/）。本文提出了一种动态特征融合模型，该模型结合了基于图的表示学习和语义特征提取，以用于区块链欺诈检测。具体来说，我们构建全局图表示来建模账户关系，并从交易数据中提取局部上下文特征。引入了一个动态多模态融合机制，以自适应地整合这些特征，使模型能够有效地捕捉结构和语义欺诈模式。我们还开发了一个综合的数据处理管道，包括图构建、时序特征增强和文本预处理。在大规模真实世界区块链数据集上的实验结果表明，我们的方法在准确率、F1分数和召回率指标方面优于现有基准。这项工作强调了将结构关系和语义相似性相结合对于稳健欺诈检测的重要性，并提供了一个可扩展的解决方案，用于保护区块链系统。', 'title_zh': '动态特征融合：结合全局图结构与局部语义的区块链欺诈检测'}
{'arxiv_id': 'arXiv:2501.02029', 'title': 'Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models', 'authors': 'Ziwei Zheng, Junyao Zhao, Le Yang, Lijun He, Fan Li', 'link': 'https://arxiv.org/abs/2501.02029', 'abstract': 'With the integration of an additional modality, large vision-language models (LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking) compared to their language-only predecessors. Although recent studies have devoted considerable effort to the post-hoc alignment of LVLMs, the inner safety mechanisms remain largely unexplored. In this paper, we discover that internal activations of LVLMs during the first token generation can effectively identify malicious prompts across different attacks. This inherent safety perception is governed by sparse attention heads, which we term ``safety heads." Further analysis reveals that these heads act as specialized shields against malicious prompts; ablating them leads to higher attack success rates, while the model\'s utility remains unaffected. By locating these safety heads and concatenating their activations, we construct a straightforward but powerful malicious prompt detector that integrates seamlessly into the generation process with minimal extra inference overhead. Despite its simple structure of a logistic regression model, the detector surprisingly exhibits strong zero-shot generalization capabilities. Experiments across various prompt-based attacks confirm the effectiveness of leveraging safety heads to protect LVLMs. Code is available at \\url{this https URL}.', 'abstract_zh': '随着额外模态的集成，大型视觉-语言模型（LVLMs）在安全风险（例如， Jailbreaking）方面表现出更大的脆弱性，相较于仅语言的前身模型。虽然近期研究已经投入了大量努力来对LVLMs进行事后对齐，但其内部的安全机制仍然尚未得到充分探索。在本文中，我们发现LVLMs在首次生成标记时的内部激活能够有效地识别不同攻击中的恶意提示。这种固有的安全感知是由稀疏注意力头控制的，我们将其称为“安全头”。进一步分析表明，这些头作为对抗恶意提示的特殊盾牌：移除它们会导致更高的攻击成功率，而模型的实用性不受影响。通过定位这些安全头并拼接其激活，我们构建了一个简单但功能强大的恶意提示检测器，该检测器可以无缝集成到生成过程中，并且具有极小的额外推理开销。尽管检测器的结构仅为逻辑回归模型，但它却意外地表现出强大的零样本泛化能力。在各种基于提示的攻击实验中，利用安全头来保护LVLMs的有效性得到了验证。代码可在 \\url{this https URL} 获取。', 'title_zh': '发言之前先察险！解析大规模视觉语言模型中的安全注意力头'}
{'arxiv_id': 'arXiv:2501.01998', 'title': 'SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework', 'authors': 'Mao Xun Huang, Hen-Hsen Huang', 'link': 'https://arxiv.org/abs/2501.01998', 'abstract': 'Stable Diffusion models have made remarkable strides in generating photorealistic images from text prompts but often falter when tasked with accurately representing complex spatial arrangements, particularly involving intricate 3D relationships. To address this limitation, we introduce SmartSpatial, an innovative approach that enhances the spatial arrangement capabilities of Stable Diffusion models through 3D-aware conditioning and attention-guided mechanisms. SmartSpatial incorporates depth information and employs cross-attention control to ensure precise object placement, delivering notable improvements in spatial accuracy metrics. In conjunction with SmartSpatial, we present SmartSpatialEval, a comprehensive evaluation framework designed to assess spatial relationships. This framework utilizes vision-language models and graph-based dependency parsing for performance analysis. Experimental results on the COCO and SpatialPrompts datasets show that SmartSpatial significantly outperforms existing methods, setting new benchmarks for spatial arrangement accuracy in image generation.', 'abstract_zh': '稳定的扩散模型已经在从文本提示生成逼真图像方面取得了显著进展，但在准确表示复杂的空间排列结构，特别是涉及复杂的3D关系时，经常表现不佳。为了解决这一局限性，我们提出了SmartSpatial，这是一种创新的方法，通过3D意识条件和注意力引导机制增强稳定扩散模型的空间排列能力。SmartSpatial融合了深度信息，并利用交叉注意力控制确保精确的对象放置，从而在空间准确性指标方面取得显著改进。与SmartSpatial一起，我们还提出了SmartSpatialEval，这是一种全面的评估框架，用于评估空间关系。该框架利用视觉语言模型和基于图的依赖解析来进行性能分析。在COCO和SpatialPrompts数据集上的实验结果表明，SmartSpatial在空间排列准确性方面显著优于现有方法，为图像生成中的空间排列精度设立了新的基准。', 'title_zh': 'SmartSpatial: 提升稳定扩散模型的三维空间布局能力并引入一种新型三维空间评估框架'}
{'arxiv_id': 'arXiv:2501.01960', 'title': 'GAF-FusionNet: Multimodal ECG Analysis via Gramian Angular Fields and Split Attention', 'authors': 'Jiahao Qin, Feng Liu', 'link': 'https://arxiv.org/abs/2501.01960', 'abstract': 'Electrocardiogram (ECG) analysis plays a crucial role in diagnosing cardiovascular diseases, but accurate interpretation of these complex signals remains challenging. This paper introduces a novel multimodal framework(GAF-FusionNet) for ECG classification that integrates time-series analysis with image-based representation using Gramian Angular Fields (GAF). Our approach employs a dual-layer cross-channel split attention module to adaptively fuse temporal and spatial features, enabling nuanced integration of complementary information. We evaluate GAF-FusionNet on three diverse ECG datasets: ECG200, ECG5000, and the MIT-BIH Arrhythmia Database. Results demonstrate significant improvements over state-of-the-art methods, with our model achieving 94.5\\%, 96.9\\%, and 99.6\\% accuracy on the respective datasets. Our code will soon be available at this https URL.', 'abstract_zh': '心电图（ECG）分析在心血管疾病诊断中起着关键作用，但准确解读这些复杂的信号仍然具有挑战性。本文介绍了一种新颖的多模态框架（GAF-FusionNet），该框架将时间序列分析与基于图像的表示方法（Gramian Angular Fields，GAF）相结合，以提高ECG分类的准确性。我们的方法采用了双层跨通道分注意力模块，以自适应地融合时序和空间特征，从而实现互补信息的精细化整合。我们使用GAF-FusionNet在三个不同的ECG数据集（ECG200、ECG5000和MIT-BIH心律失常数据库）上进行了评估。结果表明，该模型在各数据集上的准确率分别为94.5%、96.9%和99.6%，显著优于现有最先进的方法。我们将在不久的将来在以下链接提供代码：这个 https URL。', 'title_zh': 'GAF-FusionNet：基于Gram.angular字段和分割注意力机制的多模态心电图分析'}
