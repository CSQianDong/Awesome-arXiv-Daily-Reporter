# BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning 

**Title (ZH)**: BoostStep：通过增强单步推理能力来提升大型语言模型的数学能力 

**Authors**: Beichen Zhang, Yuhong Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Haodong Duan, Yuhang Cao, Dahua Lin, Jiaqi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.03226)  

**Abstract**: Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\% and 2.0\% respectively on various mathematical benchmarks, and 7.5\% gain combined with MCTS. 

**Abstract (ZH)**: 最新的大型语言模型（LLMs）通过分而治之的流水线方法和上下文学习（ICL）示例的辅助，在解决复杂数学问题上表现出令人鼓舞的性能。然而，它们的改进潜力受到其ICL示例中两个关键问题的限制：粒度过匹配和由此产生的负面噪声问题。具体来说，LLMs 能够执行分割过程，但主要是在少数几步推理中出现了不准确的推理。而当问题细化到每个步骤时，检索到的ICL示例有时缺乏特定挑战性推理步骤的相关步骤，这可能导致由于不相关性而出现错误的推理。为解决这些问题，我们重点关注提高每个步骤的推理质量，并提出了一种名为BoostStep的方法。BoostStep调整了检索和每个步骤推理之间的粒度匹配，并利用一种新颖的“初次尝试”策略提供了高度相关的ICL示例。BoostStep相比粗粒度的问题细化策略提供了更多相关示例，逐步提升了模型在每个步骤上的推理质量。BoostStep不仅是一种通用且稳健的推理增强方法，还能够无缝集成到蒙特卡洛树搜索方法（MCTS）中，以精化候选生成和决策制定。从定量角度来看，BoostStep使得GPT-4o和Qwen2.5-Math-72B在各种数学基准测试中分别提高了3.6%和2.0%，并且与MCTS结合使用时，总体性能提高了7.5%。 

---
# Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text 

**Title (ZH)**: 利用可解释的人工智能进行大模型文本归属：区分人类撰写的文本与多个人工智能语言模型生成的文本 

**Authors**: Ayat Najjar, Huthaifa I. Ashqar, Omar Darwish, Eman Hammad  

**Link**: [PDF](https://arxiv.org/pdf/2501.03212)  

**Abstract**: The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\% accuracy to 78.3\%. Notably, GPTZero was unable to recognize about 4.2\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification. 

**Abstract (ZH)**: 生成型人工智能大型语言模型（LLMs）的发展引起了对通过生成型AI或人类创作内容的识别问题的关注。在一种情况下，当学生过度依赖这类工具时，这会影响他们的写作或编程技能的发展，同时也会引发抄袭等问题。本研究旨在支持检测和识别使用LLM工具生成的文本的努力。我们假设通过机器学习（ML）可以检测到由LLM生成的文本，并研究能够识别和区分多个LLM工具生成文本的ML模型。我们利用了多种机器学习和深度学习算法，如随机森林（RF）和循环神经网络（RNN），并运用可解释的人工智能（XAI）来理解归因的重要特征。我们的方法分为两部分：1）二元分类，以区分人类书写和AI文本；2）多分类，以区分人类书写文本和由五种不同的LLM工具（ChatGPT、LLaMA、Google Bard、Claude和Perplexity）生成的文本。结果表明，在二元分类和多分类中均具有较高的准确性。我们的模型在多分类和二元分类的准确性上优于GPTZero，达到98.5%和78.3%。值得注意的是，GPTZero 无法识别约4.2%的观察数据，但我们的模型能够识别完整的测试数据集。XAI结果表明，理解不同类别中的特征重要性能够建立详细的作者/来源档案，进一步有助于归因和支持通过突出独特的风格和结构元素进行抄袭检测，确保内容原创性的坚实验证。 

---
# Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity 

**Title (ZH)**: 检测教育内容中的AI生成文本：利用机器学习和可解释AI维护学术诚信 

**Authors**: Ayat A. Najjar, Huthaifa I. Ashqar, Omar A. Darwish, Eman Hammad  

**Link**: [PDF](https://arxiv.org/pdf/2501.03203)  

**Abstract**: This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes. 

**Abstract (ZH)**: 本研究旨在通过提供工具来检测学生作品中由人工智能生成的内容，从而增强学术诚信，采用先进技术。研究结果提高了透明度和责任感，帮助教育者维持道德标准，并支持人工智能在教育中的负责任整合。本工作的关键贡献是生成了CyberHumanAI数据集，该数据集包含1000个观察数据，其中500个由人类撰写，另外500个由ChatGPT生成。我们对CyberHumanAI数据集中的各种机器学习（ML）和深度学习（DL）算法进行了评估，比较了大型语言模型（LLMs）生成的人类写作和人工智能生成的内容。结果表明，传统机器学习算法，特别是XGBoost和随机森林，取得了高绩效（分别为83%和81%的准确率）。结果还显示，分类较短的内容似乎比分类较长的内容更具挑战性。此外，通过可解释的人工智能（XAI），我们识别了影响机器学习模型预测的判别特征，其中人类写作内容倾向于使用实际语言（例如，use和allow），而人工智能生成的文本则表现出更多的抽象和正式术语（例如，realm和employ）。最后，与GPTZero的比较分析表明，我们提出的模型在区分纯AI、纯人类和混合类方面表现出更佳性能，该模型的准确率约为77.5%，而GPTZero的准确率为48.5%时被要求进行类似的分类任务。GPTZero在处理具有挑战性和少量内容的情况时倾向于将其分类为混合或未识别，而我们提出的模型在三个类别中的表现则更加均衡。 

---
# The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input 

**Title (ZH)**: FACTS接地排行榜：评估大语言模型将响应与长格式输入对接的能力 

**Authors**: Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das  

**Link**: [PDF](https://arxiv.org/pdf/2501.03200)  

**Abstract**: We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at this https URL. 

**Abstract (ZH)**: 我们引入了FACTS Grounding，这是一个在线排行榜和相关基准，用于评估语言模型生成与给定用户提示背景信息相符的事实准确文本的能力。在我们的基准中，每个提示包含一个用户请求和一个包含最多32k个令牌的完整文档，需要生成长篇幅的响应。这些长篇幅的响应必须完全基于提供的上下文文档并满足用户请求。模型将在两个阶段使用自动化评分模型进行评估：（1）如果响应未能满足用户请求，则视为无效；（2）如果响应完全基于提供的文档，则视为准确。自动化评分模型经过全面评估后选择了最佳提示模板，最终的事实得分是多个评分模型的综合结果，以减轻评分偏差。FACTS Grounding排行榜将随着时间持续维护，并包含公开和私有分割，以允许外部参与同时保护排行榜的完整性。您可以在此处访问：[该链接]。 

---
# CLIX: Cross-Lingual Explanations of Idiomatic Expressions 

**Title (ZH)**: CLIX：跨语言成语解释 

**Authors**: Aaron Gluck, Katharina von der Wense, Maria Pacheco  

**Link**: [PDF](https://arxiv.org/pdf/2501.03191)  

**Abstract**: Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools. 

**Abstract (ZH)**: 自动定义生成系统已被提出以支持语言学习者的词汇扩展。这些系统成功的主要障碍在于，学习者往往难以理解定义，尤其是由于潜在的生僻词汇和复杂的语法，尤其是在涉及非标准语言时。为了解决这些挑战，我们提出了CLIX任务，即跨语言习语解释。我们探讨了当前NLP模型在这一任务上的能力，并观察到尽管仍然具有挑战性，大型语言模型显示出一定的前景。最后，我们进行了一项详细的错误分析，以突出在我们能够可靠地将这些系统纳入教育工具之前需要解决的关键挑战。 

---
# Classifier-Guided Captioning Across Modalities 

**Title (ZH)**: 跨模态的分类器引导-caption生成 

**Authors**: Ariel Shaulov, Tal Shaharabany, Eitan Shaar, Gal Chechik, Lior Wolf  

**Link**: [PDF](https://arxiv.org/pdf/2501.03183)  

**Abstract**: Most current captioning systems use language models trained on data from specific settings, such as image-based captioning via Amazon Mechanical Turk, limiting their ability to generalize to other modality distributions and contexts. This limitation hinders performance in tasks like audio or video captioning, where different semantic cues are needed. Addressing this challenge is crucial for creating more adaptable and versatile captioning frameworks applicable across diverse real-world contexts. In this work, we introduce a method to adapt captioning networks to the semantics of alternative settings, such as capturing audibility in audio captioning, where it is crucial to describe sounds and their sources. Our framework consists of two main components: (i) a frozen captioning system incorporating a language model (LM), and (ii) a text classifier that guides the captioning system. The classifier is trained on a dataset automatically generated by GPT-4, using tailored prompts specifically designed to enhance key aspects of the generated captions. Importantly, the framework operates solely during inference, eliminating the need for further training of the underlying captioning model. We evaluate the framework on various models and modalities, with a focus on audio captioning, and report promising results. Notably, when combined with an existing zero-shot audio captioning system, our framework improves its quality and sets state-of-the-art performance in zero-shot audio captioning. 

**Abstract (ZH)**: 当前大多数字幕生成系统是基于特定数据集训练的语言模型，例如通过亚马逊 Mechanical Turk 获取的图像字幕数据，这限制了它们在其他模态分布和上下文中的泛化能力。这种局限性阻碍了在音频或视频字幕等任务中的表现，因为这些任务需要不同的语义线索。解决这一挑战对于创建适用于各种现实场景的更具适应性和多功能的字幕框架至关重要。在本研究中，我们提出了一种方法，使字幕网络适应不同环境的语义，例如在音频字幕中捕捉可听性，其中描述声音及其来源至关重要。我们的框架由两个主要部分组成：(i) 一个冻结的字幕系统，该系统结合了语言模型 (LM)，(ii) 一个文本分类器，指导字幕系统。分类器是通过 GPT-4 自动生成的数据集并使用特定设计的提示训练而成，以增强生成字幕的关键方面。重要的是，该框架仅在推理过程中运行，无需进一步训练底层字幕模型。我们对各种模型和模态进行了评估，重点关注音频字幕，并报告了令人鼓舞的结果。特别地，当与现有的零样本音频字幕系统结合使用时，我们框架提高了其质量，并在零样本音频字幕中达到了最先进的性能。 

---
# Boosting Explainability through Selective Rationalization in Pre-trained Language Models 

**Title (ZH)**: 通过选择性理性化增强预训练语言模型的可解释性 

**Authors**: Libing Yuan, Shuaibo Hu, Kui Yu, Le Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.03182)  

**Abstract**: The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: this https URL. 

**Abstract (ZH)**: 预训练语言模型（PLMs）在自然语言处理（NLP）中的广泛应用导致了对其可解释性的日益关注。选择性理性化是一种自解释框架，它可以选取人类可理解的输入子集作为预测的理由。最新研究表明，将现有的理性化框架应用于PLMs会导致严重的退化和故障问题，产生次优或没有任何意义的理由。这些失败严重损害了对理性化方法的信任，并限制了理性化技术在PLMs中的应用。本文发现，PLMs生成的句子中标记的同质性是这些问题的主要原因。为了解决这些挑战，我们提出了一种名为Pre-trained Language Model's Rationalization（PLMR）的方法，该方法将PLMs划分为生成器和预测器，以处理NLP任务并提供可解释的理由。在PLMR中，生成器通过去除无关标记来减轻同质性，而预测器则利用全文信息来标准化预测。跨多个PLMs的两个广泛使用的数据集进行的实验表明，该方法PLMR在将选择性理性化应用于PLMs时具有有效性。代码：见此链接。

[注意：英文原文中的“Codes: this https URL”是一个指向代码的链接，这里在中文翻译中进行了简化处理，未直接翻译。] 

---
# GLiREL -- Generalist Model for Zero-Shot Relation Extraction 

**Title (ZH)**: GLiREL -- 通用模型在零样本关系提取中的应用 

**Authors**: Jack Boylan, Chris Hokamp, Demian Gholipour Ghalandari  

**Link**: [PDF](https://arxiv.org/pdf/2501.03172)  

**Abstract**: We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification. Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels. 

**Abstract (ZH)**: 我们介绍了GLiREL（通用轻量级零样本关系提取模型），这是一种高效的设计架构和训练范式，用于实现零样本关系分类。借鉴了近期零样本命名实体识别方面的进展，本工作提出了一种方法，能够在单次前向传播过程中高效且准确地预测多个实体之间的零样本关系标签。通过使用FewRel和WikiZSL基准测试，实验结果表明，我们的方法在零样本关系分类任务中达到了最先进的性能。此外，我们还贡献了一种合成生成具有多样化关系标签的数据集的协议。 

---
# Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text 

**Title (ZH)**: 语义Captioning：基准数据集与图意识 few-shot 在上下文学习方法及其在SQL2Text中的应用 

**Authors**: Ali Al-Lawati, Jason Lucas, Prasenjit Mitra  

**Link**: [PDF](https://arxiv.org/pdf/2501.03166)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \url{this https URL}. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色，包括语义解析，即将自然语言转换为形式化的代码表示。然而，代码到自然语言的逆向翻译任务，即语义注释，获得的关注较少。随着LLMs被集成到代码生成、安全分析和教育平台中，这一任务的重要性越来越突出。在这篇论文中，我们重点关注SQL查询到自然语言的转换（SQL2Text），以应对LLMs生成代码可能带来的潜在安全风险。我们通过引入基于GPT-4o的迭代ICL提示，重新利用Text2SQL数据集来获取更多的附加表达，从而增强数据集在逆向任务中的鲁棒性。我们使用基于不同样本选择方法的上下文学习（ICL）进行实验，强调使用规模较小、计算效率更高的LLMs。我们的研究结果表明，利用SQL固有的图性质进行ICL样本选择，在BLEU评分上比随机选择高出39%，并且在替代方法中表现更好。数据集和代码已公布：\url{这个链接}。 

---
# VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity 

**Title (ZH)**: VicSim：提高情感和语境忠实度的受害者模拟技术 

**Authors**: Yerong Li, Yiren Liu, Yun Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.03139)  

**Abstract**: Scenario-based training has been widely adopted in many public service sectors. Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios. However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes. In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage). We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims. Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content. According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness. 

**Abstract (ZH)**: 基于情景的培训在许多公共服务领域中被广泛采用。近年来，大规模语言模型（LLMs）在模拟多样化的人格以创建这些培训情景方面显示出了潜在的应用前景。然而，关于如何利用LLMs构建模拟受害者以用于基于情景的培训方面知之甚少。在本文中，我们介绍了VicSim（受害者模拟器），这是一种新型模型，旨在解决用户模拟的三大关键维度：信息忠实性、情感动态和语言风格（例如，语法使用）。我们探索了情景受害者建模与基于生成对抗网络（GAN）的训练工作流及基于关键信息的提示相结合的方法，旨在提高模拟受害者的真实度。我们的对抗训练方法通过教会判别器识别语法和情感线索作为合成内容的可靠标志。根据人类评价者的评估结果，VicSim模型在人性相似度方面优于GPT-4。 

---
# PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models 

**Title (ZH)**: PRMBench：一种用于工艺级奖励模型的细粒度且具有挑战性的基准测试 

**Authors**: Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2501.03124)  

**Abstract**: Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development. 

**Abstract (ZH)**: 过程级奖励模型（PRMs）对于复杂的推理和决策任务至关重要，因为在推理过程中，每个中间步骤都起着重要作用。由于语言模型在推理过程中容易出现各种错误类型，PRMs 需要具备细腻的能力来检测现实场景中的各种隐性错误类型。然而，当前的基准测试主要关注步骤的正确性，未能系统地评估 PRMs 的性能。为解决这一缺口，我们提出了 PRMBench，这是一个专门设计用于评估 PRMs 细粒度错误检测能力的过程级基准测试。PRMBench 包含 6,216 个精心设计的问题和 83,456 个步骤级别的标签，从简单性、正确性和敏感性等多个维度评估模型。在涵盖 15 种模型的实验中，包括开源的 PRMs 和作为批评模型的闭源大型语言模型，我们揭示了当前 PRMs 存在的重大弱点。这些发现强调了过程级评估固有的挑战，并突显了未来研究的关键方向。我们希望 PRMBench 能够成为一个稳健的基准，促进 PRM 评估和开发研究的进步。 

---
# LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases 

**Title (ZH)**: LangFair：一个评估大规模语言模型中偏见和公平性使用的Python软件包 

**Authors**: Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Viren Bajaj, Zeya Ahmad  

**Link**: [PDF](https://arxiv.org/pdf/2501.03112)  

**Abstract**: Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在多种方式上表现出偏见，这可能导致某些由性别、种族、性取向或年龄等保护属性定义的特定群体的结果受到影响或恶化。为了解决这一问题，我们引入了LangFair，这是一个开源的Python包，旨在为LLM实践者提供工具，以评估与其具体应用场景相关的偏见和公平性风险。该包提供了一种功能，可以轻松生成由LLM对特定应用场景提示的响应组成的评估数据集，并随后计算适用于实践者场景的指标。为了指导指标选择，LangFair提供了一套可操作的决策框架。 

---
# Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling 

**Title (ZH)**: 情感引导且共情意识增强的响应生成方法在心理健康咨询中的应用 

**Authors**: Aseem Srivastava, Gauri Naik, Alison Cerezo, Tanmoy Chakraborty, Md. Shad Akhtar  

**Link**: [PDF](https://arxiv.org/pdf/2501.03088)  

**Abstract**: The crisis of mental health issues is escalating. Effective counseling serves as a critical lifeline for individuals suffering from conditions like PTSD, stress, etc. Therapists forge a crucial therapeutic bond with clients, steering them towards positivity. Unfortunately, the massive shortage of professionals, high costs, and mental health stigma pose significant barriers to consulting therapists. As a substitute, Virtual Mental Health Assistants (VMHAs) have emerged in the digital healthcare space. However, most existing VMHAs lack the commonsense to understand the nuanced sentiments of clients to generate effective responses. To this end, we propose EmpRes, a novel sentiment-guided mechanism incorporating commonsense awareness for generating responses. By leveraging foundation models and harnessing commonsense knowledge, EmpRes aims to generate responses that effectively shape the client's sentiment towards positivity. We evaluate the performance of EmpRes on HOPE, a benchmark counseling dataset, and observe a remarkable performance improvement compared to the existing baselines across a suite of qualitative and quantitative metrics. Moreover, our extensive empirical analysis and human evaluation show that the generation ability of EmpRes is well-suited and, in some cases, surpasses the gold standard. Further, we deploy EmpRes as a chat interface for users seeking mental health support. We address the deployed system's effectiveness through an exhaustive user study with a significant positive response. Our findings show that 91% of users find the system effective, 80% express satisfaction, and over 85.45% convey a willingness to continue using the interface and recommend it to others, demonstrating the practical applicability of EmpRes in addressing the pressing challenges of mental health support, emphasizing user feedback, and ethical considerations in a real-world context. 

**Abstract (ZH)**: 心理健康问题的危机不断升级。有效的咨询是帮助患有 PTSD、压力等相关症状的个体的关键生命线。咨询师与客户建立起重要的治疗关系，引导他们走向积极方向。然而，专业人员的巨大短缺、高昂的费用以及心理健康污名化带来了显著障碍，阻碍了个体寻求专业咨询。作为替代方案，虚拟心理健康助手（VMHA）在数字健康领域逐渐兴起。然而，目前大多数现有的 VMHA 缺乏理解客户细腻情感的能力，以生成有效的回应。为了解决这一问题，我们提出了 EmpRes，这是一种新颖的情感导向机制，结合了常识意识来生成回应。通过利用基础模型并利用常识知识，EmpRes 目标在于生成能够有效影响客户情感趋向积极的回答。我们在 HOPE 这一基准咨询数据集上评估了 EmpRes 的性能，与现有基准方法相比，在一系列定性和定量指标上均表现出显著的性能提升。此外，我们的广泛实证分析和人工评估表明，EmpRes 的生成能力不仅适配，而且在某些情况下甚至超越了黄金标准。进一步地，我们将 EmpRes 部署为寻求心理健康支持的用户的一种聊天界面。通过一项详尽的用户研究，我们展示了该系统的有效性，91% 的用户认为该系统有效，80% 的用户表示满意，超过 85.45% 的用户表示愿意继续使用该界面并推荐给他人，这证明了 EmpRes 在解决心理健康支持迫切挑战方面的实际应用价值，强调了在真实世界场景中反馈用户意见和伦理考虑的重要性。 

---
# Trust Modeling in Counseling Conversations: A Benchmark Study 

**Title (ZH)**: 咨询对话中的信任建模：一项基准研究 

**Authors**: Aseem Srivastava, Zuhair Hasan Shaik, Tanmoy Chakraborty, Md Shad Akhtar  

**Link**: [PDF](https://arxiv.org/pdf/2501.03064)  

**Abstract**: In mental health counseling, a variety of earlier studies have focused on dialogue modeling. However, most of these studies give limited to no emphasis on the quality of interaction between a patient and a therapist. The therapeutic bond between a patient and a therapist directly correlates with effective mental health counseling. It involves developing the patient's trust on the therapist over the course of counseling. To assess the therapeutic bond in counseling, we introduce trust as a therapist-assistive metric. Our definition of trust involves patients' willingness and openness to express themselves and, consequently, receive better care. We conceptualize it as a dynamic trajectory observable through textual interactions during the counseling. To facilitate trust modeling, we present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of 212 counseling sessions with first-of-its-kind seven expert-verified ordinal trust levels. We project our problem statement as an ordinal classification task for trust quantification and propose a new benchmark, TrustBench, comprising a suite of classical and state-of-the-art language models on MENTAL-TRUST. We evaluate the performance across a suite of metrics and lay out an exhaustive set of findings. Our study aims to unfold how trust evolves in therapeutic interactions. 

**Abstract (ZH)**: 在心理健康咨询领域，早期的研究大多关注对话建模。然而，这些研究大多对患者与咨询师之间互动的质量关注较少。患者与咨询师之间的治疗性关系直接关系到有效的心理健康咨询。这一过程涉及在咨询过程中逐步建立患者对咨询师的信任。为了评估咨询中的治疗性关系，我们引入信任作为咨询师辅助度量标准。我们的信任定义包括患者愿意并开放地表达自己，从而获得更好的照护。我们将其视为一种在心理咨询过程中可观察到的动态轨迹。为了促进信任建模，我们提出了MENTAL-TRUST，这是一个新型的心理咨询数据集，包含了212个咨询会话的手动标注和前所未有的七个专家验证分级的信任水平。我们将问题表述为一个序数分类任务，用于量化信任，并在此基础上提出了一个新的基准——TrustBench，该基准结合了多个经典的和最先进的语言模型在MENTAL-TRUST上的表现。我们通过一系列指标评估了模型的性能，并详细列出了所有发现。我们的研究旨在揭示信任在治疗性互动中是如何演变的。 

---
# Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning 

**Title (ZH)**: 量化与推理的结合：探索大规模语言模型低比特量化对数学推理性能的影响 

**Authors**: Zhen Li, Yupeng Su, Runming Yang, Zhongwei Xie, Ngai Wong, Hongxia Yang  

**Link**: [PDF](https://arxiv.org/pdf/2501.03035)  

**Abstract**: Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation. 

**Abstract (ZH)**: 大规模语言模型在复杂的数学推理测评中（如MATH）已经取得了显著进展。然而，它们巨大的计算要求也带来了实际部署的挑战。模型量化已成为一种有效策略，通过采用低精度和较低位宽表示，来减少内存使用和计算成本。在这项研究中，我们系统地评估了量化对数学推理任务的影响。我们引入了一个多维度的评估框架，从定性的角度评估特定能力维度，并对各种量化方法的逐步输出进行了定量分析。我们的结果显示，量化对数值计算能力和推理规划能力有不同的影响，指出了量化模型在某些方面的性能降级的关键区域。 

---
# Quality Estimation based Feedback Training for Improving Pronoun Translation 

**Title (ZH)**: 基于质量评估的反馈训练以改进代词翻译 

**Authors**: Harshit Dhankhar, Baban Gain, Asif Ekbal, Yogesh Mani Tripathi  

**Link**: [PDF](https://arxiv.org/pdf/2501.03008)  

**Abstract**: Pronoun translation is a longstanding challenge in neural machine translation (NMT), often requiring inter-sentential context to ensure linguistic accuracy. To address this, we introduce ProNMT, a novel framework designed to enhance pronoun and overall translation quality in context-aware machine translation systems. ProNMT leverages Quality Estimation (QE) models and a unique Pronoun Generation Likelihood-Based Feedback mechanism to iteratively fine-tune pre-trained NMT models without relying on extensive human annotations. The framework combines QE scores with pronoun-specific rewards to guide training, ensuring improved handling of linguistic nuances. Extensive experiments demonstrate significant gains in pronoun translation accuracy and general translation quality across multiple metrics. ProNMT offers an efficient, scalable, and context-aware approach to improving NMT systems, particularly in translating context-dependent elements like pronouns. 

**Abstract (ZH)**: 代词翻译是神经机器翻译（NMT）领域的一个长期挑战，通常需要跨句上下文以确保语言准确性。为了解决这一问题，我们提出了ProNMT，这是一种新颖的框架，旨在增强语境意识机器翻译系统中的代词和整体翻译质量。ProNMT 利用质量估计（QE）模型和一种独特的基于代词生成可能性的反馈机制，通过迭代微调预训练的NMT模型，而无需依赖大量的人工注释。该框架将QE分数与代词特定的奖励结合起来，指导训练，确保更好地处理语言细微差别。大量实验表明，在多个指标上，ProNMT 在代词翻译准确性和一般翻译质量方面取得了显著提高。ProNMT 提供了一种高效、可扩展且语境敏感的方法，以改善NMT系统，特别是在翻译依赖上下文的元素（如代词）方面。 

---
# Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation 

**Title (ZH)**: 将源语言词元注册到目标语言空间中的多语言神经机器翻译 

**Authors**: Zhi Qu, Yiran Wang, Jiannan Mao, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Taro Watanabe  

**Link**: [PDF](https://arxiv.org/pdf/2501.02979)  

**Abstract**: The multilingual neural machine translation (MNMT) enables arbitrary translations across multiple languages by training a model with limited parameters using parallel data only. However, the performance of such MNMT models still lags behind that of large language models (LLMs), limiting their practicality. In this work, we address this limitation by introducing registering to achieve the new state-of-the-art of decoder-only MNMT models. Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens. By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space. Experiments on EC-40, a large-scale benchmark, show that our method outperforms related methods driven by optimizing multilingual representations. We further scale up and collect 9.3 billion sentence pairs across 24 languages from public datasets to pre-train two models, namely MITRE (multilingual translation with registers). One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we open-source our models to facilitate further research and development in MNMT: this https URL. 

**Abstract (ZH)**: 多语言神经机器翻译（MNMT）通过使用并行数据训练具有少量参数的模型，实现了跨多种语言的任意翻译。然而，这些MNMT模型的性能仍落后于大型语言模型（LLMs），限制了它们的实际应用。在本工作中，我们通过引入“登记”（registers）来克服这一限制，从而实现了仅解码器MNMT模型的新最先进成果。具体而言，我们在源语言和目标语言标记之间插入了一组指定目标语言的人工标记（registers），并在修改注意力掩码后，目标标记生成仅关注这些标记的激活，表示源标记在目标语言空间中的形态。在大规模基准EC-40上的实验表明，我们的方法在多语言表示优化相关方法中表现出色。我们进一步扩展了收集了来自公开数据集的24种语言、共计93亿句对，预训练了两个模型，分别是MITRE（多语言翻译与登记）。其中一款模型MITRE-913M的表现超过了NLLB-3.3B，达到了与商用LLMs相当的性能，并在微调中展现了较强的适应性。最后，我们开源了这些模型，以促进MNMT领域的进一步研究和开发：[这里提供开源链接]。 

---
# Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis 

**Title (ZH)**: 解釋幽默風格分類：理解計算機幽默分析的 Explainable AI 方法 

**Authors**: Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat  

**Link**: [PDF](https://arxiv.org/pdf/2501.02891)  

**Abstract**: Humour styles can have either a negative or a positive impact on well-being. Given the importance of these styles to mental health, significant research has been conducted on their automatic identification. However, the automated machine learning models used for this purpose are black boxes, making their prediction decisions opaque. Clarity and transparency are vital in the field of mental health. This paper presents an explainable AI (XAI) framework for understanding humour style classification, building upon previous work in computational humour analysis. Using the best-performing single model (ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to analyse how linguistic, emotional, and semantic features contribute to humour style classification decisions. Our analysis reveals distinct patterns in how different humour styles are characterised and misclassified, with particular emphasis on the challenges in distinguishing affiliative humour from other styles. Through detailed examination of feature importance, error patterns, and misclassification cases, we identify key factors influencing model decisions, including emotional ambiguity, context misinterpretation, and target identification. The framework demonstrates significant utility in understanding model behaviour, achieving interpretable insights into the complex interplay of features that define different humour styles. Our findings contribute to both the theoretical understanding of computational humour analysis and practical applications in mental health, content moderation, and digital humanities research. 

**Abstract (ZH)**: 幽默风格可以对福祉产生负面影响或正面影响。鉴于这些风格对心理健康的重要性，已有大量研究致力于其自动识别。然而，用于这一目的的自动化机器学习模型往往是黑盒模型，使得其预测决策缺乏透明度。在心理健康领域，清晰性和透明度至关重要。本文提出了一种可解释的人工智能（XAI）框架，以理解幽默风格分类。该框架在先前的计算幽默分析研究基础上进行了构建。利用之前研究中表现最佳的单一模型（ALI+XGBoost），我们应用了全面的XAI技术，分析语言、情感和语义特征如何影响幽默风格分类决策。我们的分析揭示了不同幽默风格在特征表现和误分类方面存在的独特模式，特别强调区分关联性幽默与其他风格的挑战。通过详细检查特征重要性、错误模式和误分类案例，我们识别了影响模型决策的关键因素，包括情感模糊性、语境误解和目标识别。该框架在理解模型行为方面显示出显著的应用价值，可以实现对定义不同幽默风格的复杂特征交互模式的可解释洞察。我们的发现不仅为计算幽默分析的理论理解提供了贡献，也为心理健康、内容审核和数字人文研究的实际应用提供了参考。 

---
# IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment 

**Title (ZH)**: IIMedGPT：通过高效的人类偏好对齐促进大型语言模型在医疗任务中的能力 

**Authors**: Yiming Zhang, Zheng Chang, Wentao Cai, MengXing Ren, Kang Yuan, Yining Sun, Zenghui Ding  

**Link**: [PDF](https://arxiv.org/pdf/2501.02869)  

**Abstract**: Recent researches of large language models(LLM), which is pre-trained on massive general-purpose corpora, have achieved breakthroughs in responding human queries. However, these methods face challenges including limited data insufficiency to support extensive pre-training and can not align responses with users' instructions. To address these issues, we introduce a medical instruction dataset, CMedINS, containing six medical instructions derived from actual medical tasks, which effectively fine-tunes LLM in conjunction with other data. Subsequently, We launch our medical model, IIMedGPT, employing an efficient preference alignment method, Direct preference Optimization(DPO). The results show that our final model outperforms existing medical models in medical this http URL, Code and model checkpoints will be released upon acceptance. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLM）在大量的通用语料库上进行预先训练后，已经在回应人类查询方面取得了突破。然而，这些方法面临着数据量不足的局限性，无法支持充分的预先训练，并且难以让用户指令与模型响应保持一致。为了解决这些问题，我们引入了一个医疗指令数据集 CMedINS，该数据集包含六个源自实际医疗任务的医疗指令，这些指令能够有效结合其他数据对 LLM 进行微调。随后，我们推出了我们的医疗模型 IIMedGPT，该模型采用了一种高效的偏好对齐方法——直接偏好优化（Direct Preference Optimization, DPO）。实验结果表明，我们的最终模型在医疗领域上的表现优于现有模型。代码和模型检查点将在接受后公开发布。 

---
# Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification 

**Title (ZH)**: 基于图的检索增强生成方法用于动态少量样本文本分类 

**Authors**: Yubo Wang, Haoyang Li, Fei Teng, Lei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.02844)  

**Abstract**: Text classification is a fundamental task in natural language processing, pivotal to various applications such as query optimization, data integration, and schema matching. While neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and target labels frequently evolve based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to predict text labels. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. GORAG constructs and maintains an adaptive information graph by extracting side information across all target texts, rather than treating each input independently. It employs a weighted edge mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and accurate contextual information. 

**Abstract (ZH)**: 文本分类是自然语言处理中的一个基本任务，对于各种应用如查询优化、数据整合和模式匹配至关重要。尽管基于神经网络的模型（如CNN和BERT）已经在文本分类中展示了卓越的性能，但其效果很大程度上依赖于大量的标记训练数据。这种依赖性使得这些模型在动态少量标注文本分类中效果较差，而在这种情况下，标注数据稀缺且目标标签频繁根据应用需求演化。最近，由于广泛的预训练和上下文理解能力，大型语言模型（LLMs）展现了潜力。当前的方法通过提供文本输入、候选标签以及额外的辅助信息（如描述），使LLMs预测文本标签。然而，这些方法的有效性受到输入体量增加及辅助信息处理过程中引入的噪声的阻碍。为解决这些局限性，我们提出了一种基于图的在线检索增强生成框架，即GORAG，用于动态少量标注文本分类。GORAG通过从所有目标文本中提取辅助信息，而非独立处理每个输入，构建并维护一个自适应信息图。通过加权边机制，GORAG强调提取信息的重要性及其可靠性，并针对每个文本输入动态地通过最小成本生成树检索相关上下文。实证研究表明，与现有方法相比，GORAG通过提供更全面和准确的上下文信息，表现出更优秀的性能。 

---
# Samba-asr state-of-the-art speech recognition leveraging structured state-space models 

**Title (ZH)**: Samba-asr基于结构化状态空间模型的先进语音识别技术 

**Authors**: Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi  

**Link**: [PDF](https://arxiv.org/pdf/2501.02832)  

**Abstract**: We propose Samba ASR, the first state-of-the-art Automatic Speech Recognition (ASR) model leveraging the novel Mamba architecture as both encoder and decoder, built on the foundation of state-space models (SSMs). Unlike transformer-based ASR models, which rely on self-attention mechanisms to capture dependencies, Samba ASR effectively models both local and global temporal dependencies using efficient state-space dynamics, achieving remarkable performance gains. By addressing the limitations of transformers, such as quadratic scaling with input length and difficulty in handling long-range dependencies, Samba ASR achieves superior accuracy and efficiency.
Experimental results demonstrate that Samba ASR surpasses existing open-source transformer-based ASR models across various standard benchmarks, establishing it as the new state of the art in ASR. Extensive evaluations on benchmark datasets show significant improvements in Word Error Rate (WER), with competitive performance even in low-resource scenarios. Furthermore, the computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.
Our contributions include:
A new Samba ASR architecture demonstrating the superiority of SSMs over transformer-based models for speech sequence processing. A comprehensive evaluation on public benchmarks showcasing state-of-the-art performance. An analysis of computational efficiency, robustness to noise, and sequence generalization. This work highlights the viability of Mamba SSMs as a transformer-free alternative for efficient and accurate ASR. By leveraging state-space modeling advancements, Samba ASR sets a new benchmark for ASR performance and future research. 

**Abstract (ZH)**: 我们提出了Samba ASR，这是首个利用新型Mamba架构作为编码器和解码器的最先进的自动语音识别（Automatic Speech Recognition, ASR）模型，该模型基于状态空间模型（State-Space Models, SSMs）构建。与依赖于自注意力机制来捕捉依赖关系的基于转换器的ASR模型不同，Samba ASR 通过高效的状态空间动力学模型有效地捕捉了局部和全局的时间依赖关系，实现了显著的性能提升。通过解决转换器所面临的输入长度的二次复杂度和长距离依赖处理的困难问题，Samba ASR 达到了更高的准确性和效率。

实验结果表明，Samba ASR 在多种标准基准测试中超越了现有的开源基于转换器的ASR模型，确立了其在ASR领域的最新技术水平。在基准数据集上的广泛评估显示了显著的词错误率（Word Error Rate, WER）改进，在资源有限的情景中也能表现出竞争性的性能。此外，Mamba 架构的计算效率和参数优化使得 Samba ASR 成为一种适用于多种ASR任务的可扩展和稳健的解决方案。

我们对Samba ASR的主要贡献包括：
- 一种全新的Samba ASR架构，展示了状态空间模型在语音序列处理方面优于基于转换器模型的优势。
- 在公开基准数据集上的全面评估展示了最先进的性能。
- 对计算效率、抗噪性和序列泛化能力的分析。
- 本文突显了状态空间模型Mamba作为一种无转换器替代方案，在语音识别中实现高效和准确处理的可能性。通过利用状态空间模型的最新进展，Samba ASR 设定了ASR性能的新基准，并为未来研究提供了方向。 

---
# InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion 

**Title (ZH)**: InfiFusion：一种通过LLM融合增强跨模型推理的统一框架 

**Authors**: Zhaoyi Yan, Zhijie Sang, Yiming Zhang, Yuhao Fu, Baoyi He, Qi Zhou, Yining Di, Chunlin Ji, Shengyu Zhang, Fei Wu, Hongxia Yang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02795)  

**Abstract**: Large Language Models (LLMs) have demonstrated strong performance across various reasoning tasks, yet building a single model that consistently excels across all domains remains challenging. This paper addresses this problem by exploring strategies to integrate multiple domain-specialized models into an efficient pivot this http URL propose two fusion strategies to combine the strengths of multiple LLMs: (1) a pairwise, multi-step fusion approach that sequentially distills each source model into the pivot model, followed by a weight merging step to integrate the distilled models into the final model. This method achieves strong performance but requires substantial training effort; and (2) a unified fusion approach that aggregates all source models' outputs this http URL improve the fusion process, we introduce a novel Rate-Skewness Adaptive Fusion (RSAF) technique, which dynamically adjusts top-K ratios during parameter merging for enhanced flexibility and this http URL, we propose an uncertainty-based weighting method for the unified approach, which dynamically balances the contributions of source models and outperforms other logits/distribution ensemble this http URL achieved accuracy improvements of 9.27%, 8.80%, and 8.89% on the GSM8K, MATH, and HumanEval tasks, respectively. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种推理任务中展示了强大的性能，但要构建一个能够在所有领域中持续表现出色的单一模型仍然具有挑战性。本文通过探索将多个领域专有的模型整合到一个有效枢纽模型中的策略来应对这一问题。我们提出了两种结合多个LLM强项的融合策略：（1）一种两两逐步融合的方法，该方法首先将每个源模型逐步提炼到枢纽模型中，然后通过权重整合步骤将提炼后的模型集成到最终模型中。这种方法取得了出色的效果，但需要大量的训练努力；（2）一种统一的融合方法，该方法将所有源模型的输出汇总。为了改进融合过程，我们提出了一个名为速率-偏度自适应融合（RSAF）的新技术，该技术在参数合并过程中动态调整最上K值的比例，增强了灵活性。与此同时，我们提出了一种基于不确定性加权的方法，用于统一方法，该方法动态平衡了源模型的贡献，并在其他概率分布集合方法中表现出色。在GSM8K、MATH和HumanEval任务中，这种方法分别实现了9.27%、8.80%和8.89%的准确性提升。 

---
# Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model 

**Title (ZH)**: 改进语言模型的RLHF性能通过文本分割和学习它们的奖励 

**Authors**: Yueqin Yin, Shentao Yang, Yujia Xie, Ziyi Yang, Yuting Sun, Hany Awadalla, Weizhu Chen, Mingyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2501.02790)  

**Abstract**: Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method. 

**Abstract (ZH)**: 人类反馈强化学习（RLHF）已被广泛应用于使语言模型（LMs）与人类偏好保持一致。早期的RLHF工作通常采用多臂老虎机（bandit）形式化方法，尽管直观，但忽视了LM生成的序贯性，并且可能导致稀疏奖励问题。而最近的一些工作提出了基于密集令牌级的RLHF，但将每个令牌视为一个动作可能会过于精细，不适合适当的奖励分配。在本文中，我们试图结合两者的优点，通过训练和利用一段级奖励模型，为每个具有语义完整性的较短令牌序列来分配奖励。对于奖励学习，我们的方法允许动态文本分段，并与标准的序列偏好数据集兼容。为了有效利用基于奖励的LM训练方法，我们将经典的标量多臂老虎机奖励归一化泛化为位置感知的归一化函数，并对段奖励进行插值，进一步增加密度。通过这些设计，我们的方法在三个流行的LM策略RLHF基准测试上表现得当：AlpacaEval 2.0、Arena-Hard和MT-Bench。进行了消融研究以进一步证明我们的方法。 

---
# TARDiS : Text Augmentation for Refining Diversity and Separability 

**Title (ZH)**: TARDiS：文本增强以优化多样性和可分性 

**Authors**: Kyungmin Kim, SangHun Im, GiBaeg Kim, Heung-Seon Oh  

**Link**: [PDF](https://arxiv.org/pdf/2501.02739)  

**Abstract**: Text augmentation (TA) is a critical technique for text classification, especially in few-shot settings. This paper introduces a novel LLM-based TA method, TARDiS, to address challenges inherent in the generation and alignment stages of two-stage TA methods. For the generation stage, we propose two generation processes, SEG and CEG, incorporating multiple class-specific prompts to enhance diversity and separability. For the alignment stage, we introduce a class adaptation (CA) method to ensure that generated examples align with their target classes through verification and modification. Experimental results demonstrate TARDiS's effectiveness, outperforming state-of-the-art LLM-based TA methods in various few-shot text classification tasks. An in-depth analysis confirms the detailed behaviors at each stage. 

**Abstract (ZH)**: 文本增强（TA）是文本分类中的关键技术，尤其是在少样本设置中。本文介绍了一种基于大规模语言模型（LLM）的新型TA方法TARDiS，以解决两阶段TA方法在生成和对齐阶段固有的挑战。在生成阶段，我们提出了两种生成过程，即SEG和CEG，结合多个类特定提示，以增强多样性和可分离性。在对齐阶段，我们引入了一种类别适应（CA）方法，通过验证和修改确保生成的示例与其目标类别对齐。实验结果表明，TARDiS在各种少样本文本分类任务中表现出色，优于最先进的基于LLM的TA方法。深入分析进一步确认了每个阶段的详细行为。 

---
# QuIM-RAG: Advancing Retrieval-Augmented Generation with Inverted Question Matching for Enhanced QA Performance 

**Title (ZH)**: QuIM-RAG：通过反转问题匹配增强检索增强生成的问答性能 

**Authors**: Binita Saha, Utsha Saha, Muhammad Zubair Malik  

**Link**: [PDF](https://arxiv.org/pdf/2501.02702)  

**Abstract**: This work presents a novel architecture for building Retrieval-Augmented Generation (RAG) systems to improve Question Answering (QA) tasks from a target corpus. Large Language Models (LLMs) have revolutionized the analyzing and generation of human-like text. These models rely on pre-trained data and lack real-time updates unless integrated with live data tools. RAG enhances LLMs by integrating online resources and databases to generate contextually appropriate responses. However, traditional RAG still encounters challenges like information dilution and hallucinations when handling vast amounts of data. Our approach addresses these challenges by converting corpora into a domain-specific dataset and RAG architecture is constructed to generate responses from the target document. We introduce QuIM-RAG (Question-to-question Inverted Index Matching), a novel approach for the retrieval mechanism in our system. This strategy generates potential questions from document chunks and matches these with user queries to identify the most relevant text chunks for generating accurate answers. We have implemented our RAG system on top of the open-source Meta-LLaMA3-8B-instruct model by Meta Inc. that is available on Hugging Face. We constructed a custom corpus of 500+ pages from a high-traffic website accessed thousands of times daily for answering complex questions, along with manually prepared ground truth QA for evaluation. We compared our approach with traditional RAG models using BERT-Score and RAGAS, state-of-the-art metrics for evaluating LLM applications. Our evaluation demonstrates that our approach outperforms traditional RAG architectures on both metrics. 

**Abstract (ZH)**: 本文提出了一种新颖的架构，用于构建检索增强生成（RAG）系统，以提高基于目标语料的问答（QA）任务的效果。大型语言模型（LLMs）已经革新了人类文本的分析与生成。这些模型依赖于预训练数据，除非与实时数据工具集成，否则缺乏实时更新。RAG通过整合在线资源和数据库，增强了LLMs，使其能够生成上下文相关的问题响应。然而，传统RAG在处理大量数据时仍面临信息稀释和幻觉等挑战。我们的方法通过将语料库转换为领域特定的数据集来解决这些问题，并构建RAG架构以从目标文档生成响应。我们引入了QuIM-RAG（问题到问题倒排索引匹配），这是一种在我们系统中用于检索机制的全新方法。该策略从文档片段中生成潜在问题，并将这些问题与用户查询匹配，以确定生成准确答案的相关文本片段。我们基于Meta Inc.开源的Meta-LLaMA3-8B-instruct模型实现了我们的RAG系统，并在Hugging Face上提供了该模型。我们从一个每天被访问数千次的高流量网站中构建了一个包含500多页的自定义语料库，用于回答复杂问题，并且还准备了手动制作的地面真实问答数据用于评估。我们使用BERT-Score和RAGAS（这些是评估LLM应用的顶级标准指标）将我们的方法与传统RAG模型进行了比较。我们的评估表明，无论是哪项指标，我们的方法都优于传统RAG架构。 

---
# Decoding specialised feature neurons in LLMs with the final projection layer 

**Title (ZH)**: 用最终投影层解码专用特征神经元在大规模语言模型中的表现 

**Authors**: Harry J Davies  

**Link**: [PDF](https://arxiv.org/pdf/2501.02688)  

**Abstract**: Large Language Models (LLMs) typically have billions of parameters and are thus often difficult to interpret in their operation. Such black-box models can pose a significant risk to safety when trusted to make important decisions. The lack of interpretability of LLMs is more related to their sheer size, rather than the complexity of their individual components. The TARS method for knowledge removal (Davies et al 2024) provides strong evidence for the hypothesis that that linear layer weights which act directly on the residual stream may have high correlation with different concepts encoded in the residual stream. Building upon this, we attempt to decode neuron weights directly into token probabilities through the final projection layer of the model (the LM-head). Firstly, we show that with Llama 3.1 8B we can utilise the LM-head to decode specialised feature neurons that respond strongly to certain concepts, with examples such as "dog" and "California". This is then confirmed by demonstrating that these neurons can be clamped to affect the probability of the concept in the output. This extends to the fine-tuned assistant Llama 3.1 8B instruct model, where we find that over 75% of neurons in the up-projection layers have the same top associated token compared to the pretrained model. Finally, we demonstrate that clamping the "dog" neuron leads the instruct model to always discuss dogs when asked about its favourite animal. Through our method, it is possible to map the entirety of Llama 3.1 8B's up-projection neurons in less than 15 minutes with no parallelization. 

**Abstract (ZH)**: 大型语言模型（LLMs）通常具有数十亿个参数，因此其操作难以解释，这使得它们在进行重要决策时存在较大的安全性风险。LLMs 的不可解释性更多地与其庞大的规模有关，而不是其各个组件的复杂性。Davies 等人（2024）提出的 TARS 知识移除方法提供了强有力的证据，支持这样一种假设：直接作用于残差流的线性层权重与残差流中编码的不同概念高度相关。在此基础上，我们尝试通过模型的最终投影层（即 LM-head）直接将神经元权重解码为令牌概率。首先，我们证明在使用 Llama 3.1 8B 模型时，可以利用 LM-head 解码对某些概念（如“狗”和“加利福尼亚”）有强烈响应的特化特征神经元。这一发现随后通过实验证明，当固定这些神经元时，可以影响输出中概念的概率。这一方法也扩展到了微调后的助手 Llama 3.1 8B 指令模型中，发现超过 75% 的上投影层神经元在固定后与预训练模型具有相同的最相关令牌。最后，我们证明固定“狗”神经元会使模型在回答关于其最喜爱动物的问题时始终讨论狗。通过我们的方法，在不到 15 分钟（未使用并行计算的情况下），即可对 Llama 3.1 8B 的所有上投影层神经元进行完整的映射。 

---
# From Superficial Patterns to Semantic Understanding: Fine-Tuning Language Models on Contrast Sets 

**Title (ZH)**: 从表面模式到语义理解：在对比集中微调语言模型 

**Authors**: Daniel Petrov  

**Link**: [PDF](https://arxiv.org/pdf/2501.02683)  

**Abstract**: Large scale pretrained language models have demonstrated high performance on standard datasets for natural language inference (NLI) tasks. Unfortunately, these evaluations can be misleading, as although the models can perform well on in-distribution data, they perform poorly on out-of-distribution test sets, such as contrast sets. Contrast sets consist of perturbed instances of data that have very minor, but meaningful, changes to the input that alter the gold label, revealing how models can learn superficial patterns in the training data rather than learning more sophisticated language nuances. As an example, the ELECTRA-small language model achieves nearly 90% accuracy on an SNLI dataset but drops to 75% when tested on an out-of-distribution contrast set. The research performed in this study explores how a language models' robustness can be improved by exposing it to small amounts of more complex contrast sets during training to help it better learn language patterns. With this approach, the model regains performance and achieves nearly 90% accuracy on contrast sets, highlighting the importance of diverse and challenging training data. 

**Abstract (ZH)**: 大规模预训练语言模型在自然语言推理（NLI）任务的标准数据集上表现出色。然而，这些评估可能具有误导性，因为尽管模型在分布内数据上表现良好，但在分布外测试集（如对比集）上的表现却较差。对比集包含经过细微但有意义的修改的数据实例，这些修改改变了输入的黄金标签，揭示了模型可能学习到训练数据中的表面模式而非深奥的语言细微差别。例如，ELECTRA-small语言模型在SNLI数据集上的准确率达到近90%，但在分布外对比集上的准确率下降到75%。本研究中的研究探讨了通过在训练过程中向模型引入少量更复杂的对比集，如何提高模型的鲁棒性，帮助模型更好地学习语言模式。采用这种方法，模型重新恢复了性能，并在对比集上达到了近90%的准确率，突显了多样性和挑战性训练数据的重要性。 

---
# Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence Benchmarks 

**Title (ZH)**: 更坚韧的文字，更聪明的模型：提升对抗防御基准的标准 

**Authors**: Yang Wang, Chenghua Lin  

**Link**: [PDF](https://arxiv.org/pdf/2501.02654)  

**Abstract**: vulnerability of deep learning models to adversarial attacks. While various defence mechanisms have been proposed, there is a lack of comprehensive benchmarks that evaluate these defences across diverse datasets, models, and tasks. In this work, we address this gap by presenting an extensive benchmark for textual adversarial defence that significantly expands upon previous work. Our benchmark incorporates a wide range of datasets, evaluates state-of-the-art defence mechanisms, and extends the assessment to include critical tasks such as single-sentence classification, similarity and paraphrase identification, natural language inference, and commonsense reasoning. This work not only serves as a valuable resource for researchers and practitioners in the field of adversarial robustness but also identifies key areas for future research in textual adversarial defence. By establishing a new standard for benchmarking in this domain, we aim to accelerate progress towards more robust and reliable natural language processing systems. 

**Abstract (ZH)**: 深度学习模型对抗攻击的脆弱性。尽管已经提出了各种防御机制，但仍然缺乏全面的基准测试，可以跨不同数据集、模型和任务评估这些防御机制。在本研究中，我们通过提出一个广泛的文本对抗防御基准来填补这一空白，该基准显著扩展了先前的工作。我们的基准测试集涵盖了广泛的数据库，评估了最先进的防御机制，并扩展了评估范围，包括单句分类、相似性和同义句识别、自然语言推理和常识推理等关键任务。这项工作不仅为对抗鲁棒性领域的研究者和实践者提供了一个宝贵资源，还指出了文本对抗防御未来研究的关键领域。通过在该领域建立一个新的基准测试标准，我们旨在加快对更稳健和可靠的自然语言处理系统的开发进程。 

---
# Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian 

**Title (ZH)**: 剪枝或重新训练：优化多语言模型中的 Estonian 词汇表 

**Authors**: Aleksei Dorkin, Taido Purason, Kairit Sirts  

**Link**: [PDF](https://arxiv.org/pdf/2501.02631)  

**Abstract**: Adapting multilingual language models to specific languages can enhance both their efficiency and performance. In this study, we explore how modifying the vocabulary of a multilingual encoder model to better suit the Estonian language affects its downstream performance on the Named Entity Recognition (NER) task. The motivations for adjusting the vocabulary are twofold: practical benefits affecting the computational cost, such as reducing the input sequence length and the model size, and performance enhancements by tailoring the vocabulary to the particular language. We evaluate the effectiveness of two vocabulary adaptation approaches -- retraining the tokenizer and pruning unused tokens -- and assess their impact on the model's performance, particularly after continual training. While retraining the tokenizer degraded the performance of the NER task, suggesting that longer embedding tuning might be needed, we observed no negative effects on pruning. 

**Abstract (ZH)**: 适应特定语言的多语言语言模型可以在提高其效率和性能方面发挥重要作用。本研究探讨了如何通过对多语言编码器模型的词汇表进行修改以更好地适应爱沙尼亚语，从而影响其命名实体识别（NER）任务的下游性能。调整词汇表的动机有两个方面：一是实际应用中的好处，如减少输入序列长度和模型规模，从而降低计算成本；二是通过定制词汇表以适应特定语言来提高性能。我们评估了两种词汇表适应方法——重新训练分词器和剪枝未使用的词汇项的有效性，并评估了这些方法对模型性能的影响，特别是在连续训练后的影响。虽然重新训练分词器降低了NER任务的性能，这表明可能需要更长的嵌入调优，但我们观察到剪枝并未产生负面影响。 

---
# Empowering Bengali Education with AI: Solving Bengali Math Word Problems through Transformer Models 

**Title (ZH)**: 利用AI赋能孟加拉语教育：通过Transformer模型解决孟加拉语数学文字题 

**Authors**: Jalisha Jashim Era, Bidyarthi Paul, Tahmid Sattar Aothoi, Mirazur Rahman Zim, Faisal Muhammad Shah  

**Link**: [PDF](https://arxiv.org/pdf/2501.02599)  

**Abstract**: Mathematical word problems (MWPs) involve the task of converting textual descriptions into mathematical equations. This poses a significant challenge in natural language processing, particularly for low-resource languages such as Bengali. This paper addresses this challenge by developing an innovative approach to solving Bengali MWPs using transformer-based models, including Basic Transformer, mT5, BanglaT5, and mBART50. To support this effort, the "PatiGonit" dataset was introduced, containing 10,000 Bengali math problems, and these models were fine-tuned to translate the word problems into equations accurately. The evaluation revealed that the mT5 model achieved the highest accuracy of 97.30%, demonstrating the effectiveness of transformer models in this domain. This research marks a significant step forward in Bengali natural language processing, offering valuable methodologies and resources for educational AI tools. By improving math education, it also supports the development of advanced problem-solving skills for Bengali-speaking students. 

**Abstract (ZH)**: 数学文字问题（Mathematical Word Problems, MWPs）涉及将文本描述转化为数学方程的任务。这在自然语言处理中提出了重大挑战，尤其是在孟加拉语等低资源语言中。本文通过开发一种基于变压器模型的方法来解决孟加拉语MWPs的问题，包括基础变压器、mT5、BanglaT5和mBART50。为了支持这一努力，“PatiGonit”数据集被引入，包含10,000个孟加拉数学问题，并对这些模型进行了微调，使其能够准确地将文字问题转换为方程。评估结果显示，mT5模型的准确性最高，达到了97.30%，这表明了变压器模型在这方面的有效性。这项研究在孟加拉语自然语言处理领域迈出了重要一步，提供了教育人工智能工具的宝贵方法和资源。通过提高数学教育，这也支持了孟加拉语学生的高级问题解决技能的发展。 

---
# GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation 

**Title (ZH)**: GIT-CXR：端到端变压器用于胸部X光报告生成 

**Authors**: Iustin Sîrbu, Iulia-Renata Sîrbu, Jasmina Bogojeska, Traian Rebedea  

**Link**: [PDF](https://arxiv.org/pdf/2501.02598)  

**Abstract**: Medical imaging is crucial for diagnosing, monitoring, and treating medical conditions. The medical reports of radiology images are the primary medium through which medical professionals attest their findings, but their writing is time consuming and requires specialized clinical expertise. The automated generation of radiography reports has thus the potential to improve and standardize patient care and significantly reduce clinicians workload. Through our work, we have designed and evaluated an end-to-end transformer-based method to generate accurate and factually complete radiology reports for X-ray images. Additionally, we are the first to introduce curriculum learning for end-to-end transformers in medical imaging and demonstrate its impact in obtaining improved performance. The experiments have been conducted using the MIMIC-CXR-JPG database, the largest available chest X-ray dataset. The results obtained are comparable with the current state-of-the-art on the natural language generation (NLG) metrics BLEU and ROUGE-L, while setting new state-of-the-art results on F1 examples-averaged, F1-macro and F1-micro metrics for clinical accuracy and on the METEOR metric widely used for NLG. 

**Abstract (ZH)**: 医学影像对于诊断、监测和治疗疾病至关重要。放射影像报告是医疗专业人员证实其发现的主要媒介，但撰写这些报告耗时且需要特定的临床专业知识。因此，自动生成放射学报告有可能改善并标准化患者护理，并显著减轻临床工作者的负担。通过我们的研究，我们设计并评估了一种端到端的基于Transformer的方法，用于生成X光图像的准确且内容完备的放射学报告。此外，我们首次在医学影像中引入了课程学习方法，并证明了其在提高性能方面的影响。实验是在最大的可用胸部X光数据集MIMIC-CXR-JPG上进行的。所获得的结果在自然语言生成（NLG）指标BLEU和ROUGE-L上与当前最先进的技术相当，同时在临床准确性指标F1（平均）、F1（宏）和F1（微）上设立了新的最先进的结果，以及广泛用于自然语言生成的METEOR指标。 

---
# Multi-LLM Collaborative Caption Generation in Scientific Documents 

**Title (ZH)**: 科学技术文档中的多大规模语言模型协作 caption 生成 

**Authors**: Jaeyoung Kim, Jongho Lee, Hong-Jun Choi, Ting-Yao Hsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, Clyde Lee Giles, Ting-Hao 'Kenneth' Huang, Sungchul Choi  

**Link**: [PDF](https://arxiv.org/pdf/2501.02552)  

**Abstract**: Scientific figure captioning is a complex task that requires generating contextually appropriate descriptions of visual content. However, existing methods often fall short by utilizing incomplete information, treating the task solely as either an image-to-text or text summarization problem. This limitation hinders the generation of high-quality captions that fully capture the necessary details. Moreover, existing data sourced from arXiv papers contain low-quality captions, posing significant challenges for training large language models (LLMs). In this paper, we introduce a framework called Multi-LLM Collaborative Figure Caption Generation (MLBCAP) to address these challenges by leveraging specialized LLMs for distinct sub-tasks. Our approach unfolds in three key modules: (Quality Assessment) We utilize multimodal LLMs to assess the quality of training data, enabling the filtration of low-quality captions. (Diverse Caption Generation) We then employ a strategy of fine-tuning/prompting multiple LLMs on the captioning task to generate candidate captions. (Judgment) Lastly, we prompt a prominent LLM to select the highest quality caption from the candidates, followed by refining any remaining inaccuracies. Human evaluations demonstrate that informative captions produced by our approach rank better than human-written captions, highlighting its effectiveness. Our code is available at this https URL 

**Abstract (ZH)**: 科学图表描述是一项复杂的任务，要求生成与视觉内容上下文相适应的描述。然而，现有方法往往因为利用不完整的信息，将任务简化为单纯的图像到文本或文本总结问题，从而受限，难以生成高质量的描述，全面捕捉必要细节。此外，现有数据来源的 arXiv 论文包含低质量的描述，给训练大型语言模型（LLMs）带来了显著挑战。本文我们提出了一种名为 Multi-LLM Collaborative Figure Caption Generation（MLBCAP）的框架，通过利用专门针对不同子任务的 LLM 来解决这些问题。我们的方法分为三个关键模块：（质量评估）我们利用多模态 LLM 评估培训数据的质量，实现低质量描述的过滤。 （多样化的描述生成）然后，我们采用一种策略，对多个 LLM 进行微调/提示，以生成候选描述。 （判断）最后，我们提示一个主流 LLM 从候选描述中选择最优质描述，并对剩余的不准确性进行修正。人类评估表明，我们方法生成的描述比人工撰写的描述更具信息性，突显了其有效性。我们提供代码链接：this https URL 

---
# From Language To Vision: A Case Study of Text Animation 

**Title (ZH)**: 从语言到视觉：文本动画案例研究 

**Authors**: Ping Chen, Richard Alo, Justin Rundell  

**Link**: [PDF](https://arxiv.org/pdf/2501.02549)  

**Abstract**: Information can be expressed in multiple formats including natural language, images, and motions. Human intelligence usually faces little difficulty to convert from one format to another format, which often shows a true understanding of encoded information. Moreover, such conversions have broad application in many real-world applications. In this paper, we present a text visualization system that can visualize free text with animations. Our system is illustrated by visualizing example sentences of elementary Physics laws. 

**Abstract (ZH)**: 信息可以以多种格式表达，包括自然语言、图像和动作。人类智能通常在从一种格式转换为另一种格式时几乎没有困难，这通常表明对编码信息的真正理解。此外，这种转换在许多实际应用中具有广泛的应用前景。在此论文中，我们提出了一种文本可视化系统，该系统可以使用动画来可视化自由文本。我们的系统通过可视化基础物理学定律的示例句子进行了展示。 

---
# TreeMatch: A Fully Unsupervised WSD System Using Dependency Knowledge on a Specific Domain 

**Title (ZH)**: TreeMatch：一种基于特定领域依赖关系知识的完全无监督词语消歧系统 

**Authors**: Andrew Tran, Chris Bowes, David Brown, Ping Chen, Max Choly, Wei Ding  

**Link**: [PDF](https://arxiv.org/pdf/2501.02546)  

**Abstract**: Word sense disambiguation (WSD) is one of the main challenges in Computational Linguistics. TreeMatch is a WSD system originally developed using data from SemEval 2007 Task 7 (Coarse-grained English All-words Task) that has been adapted for use in SemEval 2010 Task 17 (All-words Word Sense Disambiguation on a Specific Domain). The system is based on a fully unsupervised method using dependency knowledge drawn from a domain specific knowledge base that was built for this task. When evaluated on the task, the system precision performs above the Most Frequent Selection baseline. 

**Abstract (ZH)**: 词义消歧（WSD）是计算语言学中的主要挑战之一。TreeMatch 是一个最初基于 SemEval 2007 任务 7（粗粒度全词汇任务）的数据开发的 WSD 系统，并且已被改编用于 SemEval 2010 任务 17（特定领域的全词汇词义消歧）。该系统基于一种完全无监督的方法，利用了从专门领域知识库中获取的依存关系知识。在该任务上的评估显示，系统的精确度超过了最常见的频率选择基准。 

---
# Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，同时确保符合学术规范：

Evaluating Large Language Models Against Human Annotators in Latent Content Analysis: Sentiment, Political Leaning, Emotional Intensity, and Sarcasm

潜析内容分析中大型语言模型与人工标注者：情感、政治倾向、情绪强度与讽喻评价 

**Authors**: Ljubisa Bojic, Olga Zagovora, Asta Zelenkauskaite, Vuk Vukovic, Milan Cabarkapa, Selma Veseljević Jerkovic, Ana Jovančevic  

**Link**: [PDF](https://arxiv.org/pdf/2501.02532)  

**Abstract**: In the era of rapid digital communication, vast amounts of textual data are generated daily, demanding efficient methods for latent content analysis to extract meaningful insights. Large Language Models (LLMs) offer potential for automating this process, yet comprehensive assessments comparing their performance to human annotators across multiple dimensions are lacking. This study evaluates the reliability, consistency, and quality of seven state-of-the-art LLMs, including variants of OpenAI's GPT-4, Gemini, Llama, and Mixtral, relative to human annotators in analyzing sentiment, political leaning, emotional intensity, and sarcasm detection. A total of 33 human annotators and eight LLM variants assessed 100 curated textual items, generating 3,300 human and 19,200 LLM annotations, with LLMs evaluated across three time points to examine temporal consistency. Inter-rater reliability was measured using Krippendorff's alpha, and intra-class correlation coefficients assessed consistency over time. The results reveal that both humans and LLMs exhibit high reliability in sentiment analysis and political leaning assessments, with LLMs demonstrating higher internal consistency than humans. In emotional intensity, LLMs displayed higher agreement compared to humans, though humans rated emotional intensity significantly higher. Both groups struggled with sarcasm detection, evidenced by low agreement. LLMs showed excellent temporal consistency across all dimensions, indicating stable performance over time. This research concludes that LLMs, especially GPT-4, can effectively replicate human analysis in sentiment and political leaning, although human expertise remains essential for emotional intensity interpretation. The findings demonstrate the potential of LLMs for consistent and high-quality performance in certain areas of latent content analysis. 

**Abstract (ZH)**: 在快速发展的数字通信时代，每天都会生成大量文本数据，迫切需要高效的方法来进行潜在内容分析以提取有意义的见解。大型语言模型（LLMs）提供了自动化的潜力，但在多维度上将它们的表现与人类注释者的绩效进行全面比较的评估缺失。本研究评估了七种最先进的LLMs（包括OpenAI的GPT-4变体、Gemini、Llama和Mixtral）在分析情感、政治倾向、情感强度和讽刺检测方面的可靠性和一致性，以及与人类注释者的表现。33名人类注释者和8种LLM变体共同评估了100项经过精心挑选的文本项目，生成了3,300个人类注释和19,200个LLM注释。LLMs在不同时间点进行了评估，以考察其时间一致性。使用克朗巴赫Alpha系数测量注者间可靠性，使用重复测量的内氏相关系数评估时间一致性。结果显示，人类和LLMs在情感分析和政治倾向评估方面表现出较高的可靠性和一致性，LLMs在内部一致性方面优于人类。在情感强度方面，LLMs显示出比人类更大的一致性，尽管人类对情感强度的评估显著更高。两个群体在讽刺检测方面均表现不佳，这在较低的一致性上表现出来。LLMs在所有维度上都表现出良好的时间一致性，表明其在时间上具有稳定的性能。研究结论认为，尤其是GPT-4的LLMs能够有效地复制人类在情感和政治倾向分析方面的工作，尽管情感强度的解释还需要人类的专业知识。研究结果展示了LLMs在某些潜在内容分析领域中实现一致性和高质量表现的潜力。 

---
# CHAIR-Classifier of Hallucination as Improver 

**Title (ZH)**: 幻觉作为改进器的椅子分类器 

**Authors**: Ao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.02518)  

**Abstract**: This paper presents a supervised method for detecting hallucinations in large language models. By analyzing token scores (logitis) across layers of the LLaMA model, we derive a small set, aiming to reduce overfitting, of features-including maximum, minimum, mean, standard deviation, and slope. We use logistic regression for classification and validate the model on the TruthfulQA and MMLU datasets. The results demonstrate significant performance gains, especially in zero-shot scenarios, highlighting the effectiveness and potential for generalization. 

**Abstract (ZH)**: 本文提出了一种监督方法，用于检测大型语言模型中的幻觉现象。通过分析LLaMA模型各层的token得分（logits），我们提取了一小套特征，包括最大值、最小值、平均值、标准差和斜率，以减少过拟合。我们使用逻辑回归进行分类，并在TruthfulQA和MMLU数据集上验证了该模型。结果表明，在零样本场景下尤其显著提高了性能，突显了该方法的有效性和泛化潜力。 

---
# Can Impressions of Music be Extracted from Thumbnail Images? 

**Title (ZH)**: 可以从缩略图中提取音乐印象吗？ 

**Authors**: Takashi Harada, Takehiro Motomitsu, Katsuhiko Hayashi, Yusuke Sakai, Hidetaka Kamigaito  

**Link**: [PDF](https://arxiv.org/pdf/2501.02511)  

**Abstract**: In recent years, there has been a notable increase in research on machine learning models for music retrieval and generation systems that are capable of taking natural language sentences as inputs. However, there is a scarcity of large-scale publicly available datasets, consisting of music data and their corresponding natural language descriptions known as music captions. In particular, non-musical information such as suitable situations for listening to a track and the emotions elicited upon listening is crucial for describing music. This type of information is underrepresented in existing music caption datasets due to the challenges associated with extracting it directly from music data. To address this issue, we propose a method for generating music caption data that incorporates non-musical aspects inferred from music thumbnail images, and validated the effectiveness of our approach through human evaluations. Additionally, we created a dataset with approximately 360,000 captions containing non-musical aspects. Leveraging this dataset, we trained a music retrieval model and demonstrated its effectiveness in music retrieval tasks through evaluation. 

**Abstract (ZH)**: 近年来，有关能够接受自然语言句子作为输入的音乐检索与生成系统的机器学习模型的研究显著增加。然而，目前缺乏大规模的公开可用数据集，这些数据集包含音乐数据及其相应的自然语言描述（音乐描述）。特别是，适合播放的场景以及聆听时引发的情绪等非音乐信息对于描述音乐至关重要。由于直接从音乐数据中提取这些信息的挑战，这种信息在现有的音乐描述数据集中普遍存在不足。为了解决这一问题，我们提出了一种方法，通过结合从音乐缩略图中推断出的非音乐方面来生成音乐描述数据，并通过人工评估验证了该方法的有效性。此外，我们创建了一个包含约36万个具有非音乐方面的描述的数据集。利用这个数据集，我们训练了一个音乐检索模型，并通过评估展示了其在音乐检索任务中的有效性。 

---
# ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use 

**Title (ZH)**: ToolHop：一个基于查询的基准测试，用于评估在多跳工具使用中大型语言模型的性能 

**Authors**: Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, Jiechao Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.02506)  

**Abstract**: Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in this https URL. 

**Abstract (ZH)**: 多跳工具使用的有效评估对于分析大型语言模型（LLMs）的理解能力、推理能力和功能调用能力至关重要。然而，进展受到可靠评估数据集缺乏的阻碍。为了解决这一问题，我们提出了ToolHop数据集，该数据集包含995个用户查询和3,912个相关工具，专门用于严格评估多跳工具使用能力。ToolHop确保了查询的多样性、有意义的相互依赖性、本地可执行的工具、详细的反馈以及可验证的答案，通过一种新颖的由查询驱动的数据构建方法，该方法包括工具创建、文档优化和代码生成。我们对14个LLM模型进行了评估，涵盖五个模型家族（即LLaMA3.1、Qwen2.5、Gemini1.5、Claude3.5和GPT），发现处理多跳工具使用场景时存在显著挑战。领先模型GPT-4o的准确率为49.04%，表明改进的空间仍然很大。进一步分析揭示了不同模型家族在工具使用策略上的差异，提供了可以指导更有效方法发展的操作性见解。代码和数据可以在以下链接找到：[链接]。 

---
# Decoding News Bias: Multi Bias Detection in News Articles 

**Title (ZH)**: 解码新闻偏见：新闻文章中的多偏见检测 

**Authors**: Bhushan Santosh Shah, Deven Santosh Shah, Vahida Attar  

**Link**: [PDF](https://arxiv.org/pdf/2501.02482)  

**Abstract**: News Articles provides crucial information about various events happening in the society but they unfortunately come with different kind of biases. These biases can significantly distort public opinion and trust in the media, making it essential to develop techniques to detect and address them. Previous works have majorly worked towards identifying biases in particular domains e.g., Political, gender biases. However, more comprehensive studies are needed to detect biases across diverse domains. Large language models (LLMs) offer a powerful way to analyze and understand natural language, making them ideal for constructing datasets and detecting these biases. In this work, we have explored various biases present in the news articles, built a dataset using LLMs and present results obtained using multiple detection techniques. Our approach highlights the importance of broad-spectrum bias detection and offers new insights for improving the integrity of news articles. 

**Abstract (ZH)**: 新闻文章提供了社会各界发生的各种事件的关键信息，但它们不幸地带有不同类型的偏见。这些偏见可以显著扭曲公众对媒体的看法和信任，因此开发检测和解决偏见的技术变得至关重要。以往的研究主要集中在识别特定领域的偏见，例如政治偏见和性别偏见。然而，还需要更加全面的研究来检测跨不同领域的偏见。大型语言模型（LLMs）能够有力地分析和理解自然语言，使其成为构建数据集和检测这些偏见的理想工具。在本项研究中，我们探索了新闻文章中存在的各种偏见，并利用LLMs构建了数据集，展示了采用多种检测技术所得的结果。我们的方法突显了广泛偏见检测的重要性，并为提高新闻文章的完整性提供了新的见解。 

---
# Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine 

**Title (ZH)**: Hengqin-RA-v1：基于数据集的传统中医治疗和诊断类风湿关节炎的高级语言模型 

**Authors**: Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, Yong Liang, Liang Liu, Hudan Pan  

**Link**: [PDF](https://arxiv.org/pdf/2501.02471)  

**Abstract**: Large language models (LLMs) primarily trained on English texts, often face biases and inaccuracies in Chinese contexts. Their limitations are pronounced in fields like Traditional Chinese Medicine (TCM), where cultural and clinical subtleties are vital, further hindered by a lack of domain-specific data, such as rheumatoid arthritis (RA). To address these issues, this paper introduces Hengqin-RA-v1, the first large language model specifically tailored for TCM with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a comprehensive RA-specific dataset curated from ancient Chinese medical literature, classical texts, and modern clinical studies. This dataset empowers Hengqin-RA-v1 to deliver accurate and culturally informed responses, effectively bridging the gaps left by general-purpose models. Extensive experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models, even surpassing the diagnostic accuracy of TCM practitioners in certain cases. 

**Abstract (ZH)**: 大型语言模型（LLMs）主要在英文文本上进行训练，往往在中国语境中表现出偏差和不准确。特别是在中医药（TCM）领域，其局限性尤为突出，因为中医学的文化和临床细微之处至关重要。此外，缺乏特定领域的数据，例如类风湿性关节炎（RA），进一步加重了这一问题。为了应对这些挑战，本文介绍了Hengqin-RA-v1，这是首个专门针对中医药、尤其专注于RA诊断与治疗的大型语言模型。我们还提出了HQ-GCM-RA-C1，这是一个全面的RA特定数据集，从古代中医文献、古典文献和现代临床研究中精心收集而来。该数据集使Hengqin-RA-v1能够提供准确且文化敏感的响应，有效地填补了通用模型留下的空白。大量实验表明，Hengqin-RA-v1在多项指标上优于现有的最佳模型，甚至在某些情况下超过了中医执业人员的诊断准确性。 

---
# Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications 

**Title (ZH)**: 面向全方位RAG：大型语言模型在医疗应用中的全面检索增强生成 

**Authors**: Zhe Chen, Yusheng Liao, Shuyang Jiang, Pingjie Wang, Yiqiu Guo, Yanfeng Wang, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02460)  

**Abstract**: Large language models (LLMs) hold promise for addressing healthcare challenges but often generate hallucinations due to limited integration of medical knowledge. Incorporating external medical knowledge is therefore critical, especially considering the breadth and complexity of medical content, which necessitates effective multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, where the task is to formulate context-appropriate queries tailored to the attributes of diverse knowledge sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model's expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a comprehensive repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation (SPO) method, which enhances multi-source utilisation through explicit planning optimisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment using positive and negative planning samples. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources. 

**Abstract (ZH)**: 大型语言模型（LLMs）在解决医疗挑战方面具有潜力，但往往会产生幻觉，这是因为医学知识的整合有限。因此，集成外部医学知识至关重要，尤其是在面对广度和复杂性都很高的医疗内容时，这需要有效的多源知识获取。我们通过将这一挑战重新定义为一个来源规划问题来应对这一挑战，其中任务是制定适合上下文的查询，针对多种知识源的属性进行定制。现有的方法要么忽视了来源规划，要么未能有效实现这一目标，原因在于模型对源的期望与其实际内容之间的不一致。为了弥合这一差距，我们提出了MedOmniKB这一综合资源库，其中包括多体裁和多结构的医学知识源。借助这些资源，我们提出了来源规划优化（SPO）方法，该方法通过明确的规划优化来增强多源利用。我们的方法包括让专家模型探索和评估潜在计划的可能性，同时训练一个小模型来学习源的对齐，使用正反规划样本作为训练材料。实验结果表明，我们的方法显著提高了多源规划性能，使优化的小模型能够在利用多种医学知识源方面达到最先进的效果。 

---
# Understand, Solve and Translate: Bridging the Multilingual Mathematical Reasoning Gap 

**Title (ZH)**: 理解、解决并转换：弥合多语言数学推理差距 

**Authors**: Hyunwoo Ko, Guijin Son, Dasol Choi  

**Link**: [PDF](https://arxiv.org/pdf/2501.02448)  

**Abstract**: Large language models (LLMs) demonstrate exceptional performance on complex reasoning tasks. However, despite their strong reasoning capabilities in high-resource languages (e.g., English and Chinese), a significant performance gap persists in other languages. To investigate this gap in Korean, we introduce HRM8K, a benchmark comprising 8,011 English-Korean parallel bilingual math problems. Through systematic analysis of model behaviors, we identify a key finding: these performance disparities stem primarily from difficulties in comprehending non-English inputs, rather than limitations in reasoning capabilities. Based on these findings, we propose UST (Understand, Solve, and Translate), a method that strategically uses English as an anchor for reasoning and solution generation. By fine-tuning the model on 130k synthetically generated data points, UST achieves a 10.91% improvement on the HRM8K benchmark and reduces the multilingual performance gap from 11.6% to 0.7%. Additionally, we show that improvements from UST generalize effectively to different Korean domains, demonstrating that capabilities acquired from machine-verifiable content can be generalized to other areas. We publicly release the benchmark, training dataset, and models. 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂推理任务上表现出色。然而，尽管它们在高资源语言（如英语和中文）上的推理能力很强，但在其他语言上仍存在显著的表现差距。为研究这一差距，我们引入了HRM8K基准，该基准包含8,011个英语-韩语双语数学问题。通过系统的模型行为分析，我们发现了一个关键发现：这些性能差异主要源自对非英语输入理解的困难，而不是推理能力的限制。基于这些发现，我们提出了UST（理解、解决和翻译）方法，该方法策略性地利用英语作为推理和解决方案生成的锚点。通过在130,000个合成数据点上微调模型，UST在HRM8K基准上的表现提高了10.91%，并将多语言性能差距从11.6%降低到了0.7%。此外，我们展示了UST在不同韩语领域中的性能改进具有有效性，表明从可机器验证的内容中获得的能力可以推广到其他领域。我们已公开发布了该基准、训练数据集和模型。 

---
# Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification 

**Title (ZH)**: 多模态隐喻理解：汉语隐喻映射识别的数据集与模型探究 

**Authors**: Dongyu Zhang, Shengcheng Yin, Jingwei Yu, Zhiyao Wu, Zhen Li, Chengpei Xu, Xiaoxia Wang, Feng Xia  

**Link**: [PDF](https://arxiv.org/pdf/2501.02434)  

**Abstract**: Metaphors play a crucial role in human communication, yet their comprehension remains a significant challenge for natural language processing (NLP) due to the cognitive complexity involved. According to Conceptual Metaphor Theory (CMT), metaphors map a target domain onto a source domain, and understanding this mapping is essential for grasping the nature of metaphors. While existing NLP research has focused on tasks like metaphor detection and sentiment analysis of metaphorical expressions, there has been limited attention to the intricate process of identifying the mappings between source and target domains. Moreover, non-English multimodal metaphor resources remain largely neglected in the literature, hindering a deeper understanding of the key elements involved in metaphor interpretation. To address this gap, we developed a Chinese multimodal metaphor advertisement dataset (namely CM3D) that includes annotations of specific target and source domains. This dataset aims to foster further research into metaphor comprehension, particularly in non-English languages. Furthermore, we propose a Chain-of-Thought (CoT) Prompting-based Metaphor Mapping Identification Model (CPMMIM), which simulates the human cognitive process for identifying these mappings. Drawing inspiration from CoT reasoning and Bi-Level Optimization (BLO), we treat the task as a hierarchical identification problem, enabling more accurate and interpretable metaphor mapping. Our experimental results demonstrate the effectiveness of CPMMIM, highlighting its potential for advancing metaphor comprehension in NLP. Our dataset and code are both publicly available to encourage further advancements in this field. 

**Abstract (ZH)**: 比喻在人类沟通中发挥着重要作用，但由于认知复杂性的影响，其理解仍然是自然语言处理（NLP）中的一个重大挑战。根据概念隐喻理论（CMT），比喻将目标域映射到源域，理解这种映射对把握比喻的本质至关重要。尽管现有的NLP研究主要集中在比喻检测和比喻表达的情感分析等任务上，但对于源域和目标域之间复杂映射的识别过程却关注较少。此外，非英语多模态比喻资源在文献中仍然被大量忽略，阻碍了对比喻解读中关键元素的深入理解。为解决这一问题，我们构建了一个中文多模态比喻广告数据集（简称CM3D），该数据集包括了源域和目标域的特定标注。该数据集旨在促进对比喻理解的研究，特别是非英语语言中的比喻理解。此外，我们提出了一种基于链式思维提示的比喻映射识别模型（CPMMIM），该模型模拟了人类识别这些映射的认知过程。我们借鉴了链式思维推理和双层优化（BLO）的理念，将任务视为一个分层识别问题，从而实现更准确和可解释的比喻映射。实验结果表明，CPMMIM 的有效性，突显了其在NLP中推进比喻理解的潜力。我们还公开发布了该数据集和代码，以促进该领域的进一步发展。 

---
# Swift Cross-Dataset Pruning: Enhancing Fine-Tuning Efficiency in Natural Language Understanding 

**Title (ZH)**: Swift 跨数据集剪枝：增强自然语言理解模型微调效率 

**Authors**: Binh-Nguyen Nguyen, Yang He  

**Link**: [PDF](https://arxiv.org/pdf/2501.02432)  

**Abstract**: Dataset pruning aims to select a subset of a dataset for efficient model training. While data efficiency in natural language processing has primarily focused on within-corpus scenarios during model pre-training, efficient dataset pruning for task-specific fine-tuning across diverse datasets remains challenging due to variability in dataset sizes, data distributions, class imbalance and label spaces. Current cross-dataset pruning techniques for fine-tuning often rely on computationally expensive sample ranking processes, typically requiring full dataset training or reference models. We address this gap by proposing Swift Cross-Dataset Pruning (SCDP). Specifically, our approach uses TF-IDF embeddings with geometric median to rapidly evaluate sample importance. We then apply dataset size-adaptive pruning to ensure diversity: for smaller datasets, we retain samples far from the geometric median, while for larger ones, we employ distance-based stratified pruning. Experimental results on six diverse datasets demonstrate the effectiveness of our method, spanning various tasks and scales while significantly reducing computational resources. Source code is available at: this https URL 

**Abstract (ZH)**: 数据集剪枝旨在选择数据集中的一部分子集，以实现高效的模型训练。在自然语言处理中，数据效率主要集中在模型预训练过程中的内部数据场景，而针对多样数据集的任务特定微调中的高效数据集剪枝仍然具有挑战性，原因在于数据集规模、数据分布、类别不平衡和标签空间的差异。当前的跨数据集微调剪枝技术通常依赖于计算成本高昂的样本排名过程，通常需要进行完整的数据集训练或参考模型训练。我们通过提出Swift跨数据集剪枝（SCDP）来解决这一问题。具体而言，我们的方法使用TF-IDF嵌入和几何中位数快速评估样本的重要性。随后，我们应用适应数据集规模的剪枝策略以确保多样性：对于规模较小的数据集，我们保留远离几何中位数的样本；而对于规模较大的数据集，我们采用基于距离的分层剪枝策略。我们将方法在六个不同的数据集上的实验结果展示出来，这些数据集涵盖了各种任务和规模，并且显著减少了计算资源的使用。源代码可在以下链接获取：this https URL 

---
# Anonymization by Design of Language Modeling 

**Title (ZH)**: 设计中的匿名化：语言模型中的隐私保护 

**Authors**: Antoine Boutet, Zakaria El Kazdam, Lucas Magnana, Helain Zimmermann  

**Link**: [PDF](https://arxiv.org/pdf/2501.02407)  

**Abstract**: Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when models specialized on sensitive data can memorize and then expose and regurgitate confidential information. This paper presents a privacy-by-design language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using medical datasets and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer the best tradeoff for maintaining high privacy while retaining high utility. 

**Abstract (ZH)**: 自然语言处理（NLP）的迅速发展已经 revolutionized 许多领域，包括医疗保健。然而，这些进展引发了重大的隐私问题，特别是在某些专门处理敏感数据的模型能够记住并泄露和重复机密信息时。本文提出了一种以隐私为导向的语言建模方法，以解决语言模型匿名化的问题，从而促进其共享。具体来说，我们提出了一种掩码语言建模（MLM）方法来专门化类似于 BERT 的语言模型，以及一种因果语言建模（CLM）方法来专门化类似于 GPT 的模型，以避免模型记住训练数据中直接和间接标识信息。我们使用医疗数据集全面评估了这两种方法，并将其与不同的基准方法进行了比较。结果显示，在避免模型在专门化过程中记住直接和间接标识信息的情况下，我们的掩码和因果语言建模方案在保持高隐私性的同时提供了最佳的实用性平衡。 

---
# Syntactic Evolution in Language Usage 

**Title (ZH)**: 语言使用中的句法演变 

**Authors**: Surbhit Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2501.02392)  

**Abstract**: This research aims to investigate the dynamic nature of linguistic style throughout various stages of life, from post teenage to old age. By employing linguistic analysis tools and methodologies, the study will delve into the intricacies of how individuals adapt and modify their language use over time. The research uses a data set of blogs from this http URL from 2004 and focuses on English for syntactic analysis. The findings of this research can have implications for linguistics, psychology, and communication studies, shedding light on the intricate relationship between age and language. 

**Abstract (ZH)**: 本研究旨在探讨语言风格在其一生中各个阶段的动态特性，从青少年期之后直到老年。通过运用语言分析工具和方法，研究将深入探讨个体如何随时间适应和修改其语言使用。本研究使用了从2004年至今来自此网站的数据集，并专注于英语句法分析。本研究的发现可能对语言学、心理学和沟通研究等领域产生影响，有助于揭示年龄与语言之间复杂的关系。 

---
# Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison 

**Title (ZH)**: 使用前拼接或跨注意力机制的语音转文本：实证比较 

**Authors**: Tsz Kin Lam, Marco Gaido, Sara Papi, Luisa Bentivogli, Barry Haddow  

**Link**: [PDF](https://arxiv.org/pdf/2501.02370)  

**Abstract**: Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form in communication. To integrate speech into LLMs, one promising approach is dense feature prepending (DFP) which prepends the projected speech representations to the textual representations, allowing end-to-end training with the speech encoder. However, DFP typically requires connecting a text decoder to a speech encoder. This raises questions about the importance of having a sophisticated speech encoder for DFP, and how its performance compares with a standard encoder-decoder (i.e. cross-attention) architecture. In order to perform a controlled architectural comparison, we train all models from scratch, rather than using large pretrained models, and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. We study the influence of a speech encoder in DFP. More importantly, we compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, generation speed and GPU memory footprint on monolingual, bilingual and multilingual models. Despite the prevalence of DFP over cross-attention, our overall results do not indicate a clear advantage of DFP. 

**Abstract (ZH)**: 在大型语言模型（LLMs）在自然语言处理（NLP）任务中取得显著成功之后，人们越来越关注将这些模型的能力扩展到语音——这是交流中最常见的一种形式。为了将语音整合到LLMs中，一种有前景的方法是密集特征预拼接（DFP），这种方法将投影后的语音表示添加到文本表示之前，从而允许端到端训练，并结合语音编码器。然而，DFP 通常需要将文本解码器连接到语音编码器上。这引发了关于DFP 是否需要复杂的语音编码器以及其性能与标准编码器-解码器（即交叉注意力）架构相比如何的问题。为了进行有控制的架构比较，我们从头开始训练所有模型，而不是使用大型的预训练模型，并使用可比的数据和参数设置，在MuST-C v1.0和CoVoST2数据集上测试语音到文本识别（ASR）和翻译（ST）。我们研究了DFP 中语音编码器的影响。更重要的是，我们在单语、双语和多语模型的不同配置下比较了DFP 和交叉注意力，如CTC 压缩、序列级知识蒸馏、生成速度和GPU内存占用。尽管DFP 在许多情况下比交叉注意力更受欢迎，但我们的总体结果表明，DFP 并没有明显的优势。 

---
# Context Aware Lemmatization and Morphological Tagging Method in Turkish 

**Title (ZH)**: 土耳其语中的上下文感知词形还原和形态标记方法 

**Authors**: Cagri Sayallar  

**Link**: [PDF](https://arxiv.org/pdf/2501.02361)  

**Abstract**: The smallest part of a word that defines the word is called a word root. Word roots are used to increase success in many applications since they simplify the word. In this study, the lemmatization model, which is a word root finding method, and the morphological tagging model, which predicts the grammatical knowledge of the word, are presented. The presented model was developed for Turkish, and both models make predictions by taking the meaning of the word into account. In the literature, there is no lemmatization study that is sensitive to word meaning in Turkish. For this reason, the present study shares the model and the results obtained from the model on Turkish lemmatization for the first time in the literature. In the present study, in the lemmatization and morphological tagging models, bidirectional LSTM is used for the spelling of words, and the Turkish BERT model is used for the meaning of words. The models are trained using the IMST and PUD datasets from Universal Dependencies. The results from the training of the models were compared with the results from the SIGMORPHON 2019 competition. The results of the comparisons revealed that our models were superior. 

**Abstract (ZH)**: 构成单词基本含义的最小部分称为词根。由于词根能够简化单词，因此在许多应用中使用词根可以增加成功几率。在本研究中，我们介绍了词根提取模型（lemmatization model）和词性标注模型（morphological tagging model），前者用于寻找词根，后者用于预测单词的语法知识。这些模型是为土耳其语开发的，并且在预测时均考虑了单词的含义。文献中尚无针对土耳其语词根提取的考虑单词意义的研究。因此，本研究首次在文献中分享了土耳其语词根提取模型及其从该模型得出的结果。在本研究中，词根提取和词性标注模型中使用双向LSTM（Bidirectional LSTM）进行拼写预测，并使用土耳其语BERT模型（Turkish BERT）进行单词意义的预测。模型使用来自Universal Dependencies的IMST和PUD数据集进行训练。将模型训练结果与SIGMORPHON 2019竞赛结果进行了对比分析，对比结果表明，我们的模型表现更优。 

---
# Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving 

**Title (ZH)**: 多思维运用：利用大规模语言模型进行多视角问题解决 

**Authors**: Sanghyun Park, Boris Maciejovsky, Phanish Puranam  

**Link**: [PDF](https://arxiv.org/pdf/2501.02348)  

**Abstract**: Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the "wisdom of crowds" within a single individual, allowing them to "think with many minds." While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution. 

**Abstract (ZH)**: 复杂问题解决需要认知灵活性——即在保持各种视角独特性的同时，能够容纳多种视角的能力。这种灵活性可以在单一个体中复制“群体的智慧”，使他们能够“使用多种思维”。虽然心理模拟能够支持想象中的讨论，但认知限制却限制了其有效性。我们提出了一种基于大型语言模型（LLM）的方法——合成讨论，作为一种解决方案。通过使用自定义的基于GPT的模型，我们展示了其优势：同时处理多种视角而不损害认知能力，平行探索各种视角，并精确控制视角合成。通过将讨论过程外部化并分散认知劳动，使并行搜索和整合得以进行，合成讨论超越了心理模拟的局限性。这种方法在战略规划、政策制定和冲突解决方面展现出前景。 

---
# AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference 

**Title (ZH)**: AdaSkip：适应性子层跳过以加速长上下文LLM推理 

**Authors**: Zhuomin He, Yizhen Yao, Pengfei Zuo, Bin Gao, Qinya Li, Zhenzhe Zheng, Fan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.02336)  

**Abstract**: Long-context large language models (LLMs) inference is increasingly critical, motivating a number of studies devoted to alleviating the substantial storage and computational costs in such scenarios. Layer-wise skipping methods are promising optimizations but rarely explored in long-context inference. We observe that existing layer-wise skipping strategies have several limitations when applied in long-context inference, including the inability to adapt to model and context variability, disregard for sublayer significance, and inapplicability for the prefilling phase. This paper proposes \sysname, an adaptive sublayer skipping method specifically designed for long-context inference. \sysname adaptively identifies less important layers by leveraging on-the-fly similarity information, enables sublayer-wise skipping, and accelerates both the prefilling and decoding phases. The effectiveness of \sysname is demonstrated through extensive experiments on various long-context benchmarks and models, showcasing its superior inference performance over existing baselines. 

**Abstract (ZH)**: 长上下文大型语言模型（LLMs）的推理日益重要，这促使了大量研究致力于缓解这类场景中巨大的存储和计算成本。逐层跳过方法是潜在的优化手段，但在长上下文推理中鲜有探索。我们发现现有逐层跳过策略在应用于长上下文推理时存在多个局限性，包括无法适应模型和上下文的差异、忽视子层的重要性以及不适合预填充阶段。本文提出了一种名为 \sysname 的自适应子层跳过方法，专门设计用于长上下文推理。\sysname 通过实时利用相似性信息自适应地识别不重要层，支持子层级别的跳过，并加速预填充和解码阶段。通过在各种长上下文基准和模型上的广泛实验，展示了 \sysname 在推理性能方面优于现有基线的优越性。 

---
# Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications 

**Title (ZH)**: 使用生成式人工智能应用进行 constructed response 评分的有效性论证 

**Authors**: Jodi M. Casabianca, Daniel F. McCaffrey, Matthew S. Johnson, Naim Alper, Vladimir Zubenko  

**Link**: [PDF](https://arxiv.org/pdf/2501.02334)  

**Abstract**: The rapid advancements in large language models and generative artificial intelligence (AI) capabilities are making their broad application in the high-stakes testing context more likely. Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods. The purpose of this paper is to highlight the differences in the feature-based and generative AI applications in constructed response scoring systems and propose a set of best practices for the collection of validity evidence to support the use and interpretation of constructed response scores from scoring systems using generative AI. We compare the validity evidence needed in scoring systems using human ratings, feature-based natural language processing AI scoring engines, and generative AI. The evidence needed in the generative AI context is more extensive than in the feature-based NLP scoring context because of the lack of transparency and other concerns unique to generative AI such as consistency. Constructed response score data from standardized tests demonstrate the collection of validity evidence for different types of scoring systems and highlights the numerous complexities and considerations when making a validity argument for these scores. In addition, we discuss how the evaluation of AI scores might include a consideration of how a contributory scoring approach combining multiple AI scores (from different sources) will cover more of the construct in the absence of human ratings. 

**Abstract (ZH)**: 随着大型语言模型和生成式人工智能（AI）能力的迅速发展，它们在高风险测试环境中的广泛应用更加可能。特别是在评分构造性回答时使用生成式AI极具吸引力，因为它可以减少传统AI评分中手工构建特征所需的努力，并且甚至可能超越这些方法。本文旨在突显基于特征和生成式AI在构造性回答评分系统中的应用差异，并提出一套最佳实践，以收集有效性证据，支持使用和解释使用生成式AI的评分系统的构造性回答分数。我们比较了使用人工评分、基于特征的自然语言处理AI评分引擎以及生成式AI的评分系统所需的有效性证据。生成式AI上下文中所需的有效性证据远多于基于特征的NLP评分上下文，这主要是由于透明度不足及其他生成式AI独有的问题，如一致性问题。标准化测试中的构造性回答分数数据展示了不同类型的评分系统收集有效性证据的方式，并突显了为这些分数提出有效性论点时所涉及的众多复杂性和考量。此外，我们还讨论了如何在评估AI分数时考虑使用多种来源的AI评分（组合评分方法）来覆盖更多构念的问题，特别是在缺乏人工评分的情况下。 

---
# Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection 

**Title (ZH)**: 显性 vs. 隐性：通过自我反思探究大型语言模型中的社会偏见 

**Authors**: Yachao Zhao, Bo Wang, Yan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02295)  

**Abstract**: Large Language Models (LLMs) have been shown to exhibit various biases and stereotypes in their generated content. While extensive research has investigated bias in LLMs, prior work has predominantly focused on explicit bias, leaving the more nuanced implicit biases largely unexplored. This paper presents a systematic framework grounded in social psychology theories to investigate and compare explicit and implicit biases in LLMs. We propose a novel "self-reflection" based evaluation framework that operates in two phases: first measuring implicit bias through simulated psychological assessment methods, then evaluating explicit bias by prompting LLMs to analyze their own generated content. Through extensive experiments on state-of-the-art LLMs across multiple social dimensions, we demonstrate that LLMs exhibit a substantial inconsistency between explicit and implicit biases, where explicit biases manifest as mild stereotypes while implicit biases show strong stereotypes. Furthermore, we investigate the underlying factors contributing to this explicit-implicit bias inconsistency. Our experiments examine the effects of training data scale, model parameters, and alignment techniques. Results indicate that while explicit bias diminishes with increased training data and model size, implicit bias exhibits a contrasting upward trend. Notably, contemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit bias but show limited efficacy in mitigating implicit bias. These findings suggest that while scaling up models and alignment training can address explicit bias, the challenge of implicit bias requires novel approaches beyond current methodologies. 

**Abstract (ZH)**: 大型语言模型（LLMs）在生成内容时已被证明存在各种偏见和刻板印象。尽管已有大量研究探讨了LLMs中的偏见问题，但先前的工作主要集中在显性偏见上，而更为微妙的隐性偏见则被忽视了。本文基于社会心理学理论，提出了一种系统框架，用来研究和比较LLMs中的显性偏见和隐性偏见。我们提出了一种基于“自我反思”的新型评估框架，该框架分为两个阶段：首先通过模拟的心理评估方法测量隐性偏见，然后通过促使LLMs分析其生成的内容来评估显性偏见。通过在多个社会维度上对最新一代LLMs进行大量实验，我们发现LLMs在显性偏见和隐性偏见之间表现出显著的不一致性，其中显性偏见表现为轻微的刻板印象，而隐性偏见则表现出强烈的刻板印象。此外，我们还研究了导致这种显性-隐性偏见不一致性的潜在因素。我们的实验考察了训练数据规模、模型参数以及对齐技术的影响。结果表明，虽然随着训练数据的增加和模型规模的扩大，显性偏见有所减少，但隐性偏见则呈现出相反的增长趋势。值得注意的是，当前的对齐方法（如RLHF、DPO）能够有效抑制显性偏见，但在减轻隐性偏见方面效果有限。这些发现表明，虽然增大模型规模和对齐训练可以缓解显性偏见问题，但应对隐性偏见的挑战需要超出现有方法的新方案。 

---
# LLMzSz{\L}: a comprehensive LLM benchmark for Polish 

**Title (ZH)**: LLMzSz{\L}：波兰语综合大语言模型基准 

**Authors**: Krzysztof Jassem, Michał Ciesiółka, Filip Graliński, Piotr Jabłoński, Jakub Pokrywka, Marek Kubis, Monika Jabłońska, Ryszard Staruch  

**Link**: [PDF](https://arxiv.org/pdf/2501.02266)  

**Abstract**: This article introduces the first comprehensive benchmark for the Polish language at this scale: LLMzSzŁ (LLMs Behind the School Desk). It is based on a coherent collection of Polish national exams, including both academic and professional tests extracted from the archives of the Polish Central Examination Board. It covers 4 types of exams, coming from 154 domains. Altogether, it consists of almost 19k closed-ended questions. We investigate the performance of open-source multilingual, English, and Polish LLMs to verify LLMs' abilities to transfer knowledge between languages. Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined. We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters. Our analysis highlights the potential of LLMs in assisting with exam validation, particularly in identifying anomalies or errors in examination tasks. 

**Abstract (ZH)**: 本文介绍了迄今为止第一个针对波兰语的大规模综合基准：LLMzSzŁ（LLMs Behind the School Desk）。该基准基于波兰国家考试的一致性集合，包括从波兰中央考试委员会档案中提取的学术和职业测试。它涵盖了4种不同类型的考试，来自154个领域。共计包含近19000个闭合问题。我们研究了开源多语言、英文和波兰语大语言模型的性能，以验证大语言模型在语言间传递知识的能力。同时，我们还分析了模型准确性和考试通过率上大语言模型与人类之间的相关性。结果显示，多语言大语言模型能够在某些任务上获得优于单语言模型的性能；然而，在模型规模成为关键因素时，单语言模型仍可能具有一定优势。我们的分析强调了大语言模型在考试验证中的潜在作用，尤其是在识别考试任务中的异常或错误方面。 

---
# Financial Named Entity Recognition: How Far Can LLM Go? 

**Title (ZH)**: 金融领域实体识别：大型语言模型能走多远？ 

**Authors**: Yi-Te Lu, Yintong Huo  

**Link**: [PDF](https://arxiv.org/pdf/2501.02237)  

**Abstract**: The surge of large language models (LLMs) has revolutionized the extraction and analysis of crucial information from a growing volume of financial statements, announcements, and business news. Recognition for named entities to construct structured data poses a significant challenge in analyzing financial documents and is a foundational task for intelligent financial analytics. However, how effective are these generic LLMs and their performance under various prompts are yet need a better understanding. To fill in the blank, we present a systematic evaluation of state-of-the-art LLMs and prompting methods in the financial Named Entity Recognition (NER) problem. Specifically, our experimental results highlight their strengths and limitations, identify five representative failure types, and provide insights into their potential and challenges for domain-specific tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）的兴起已经彻底改变了从不断增加的财务报表、公告和商业新闻中提取和分析关键信息的方式。命名实体的识别对于构建结构化数据并在分析财务文件方面发挥着重大作用，也是智能财务分析的基础任务。然而，这些通用LLMs在各种提示下的效果及其性能仍需要进一步的理解。为了解决这一问题，我们系统性地评估了最先进的LLMs和提示方法在金融领域命名实体识别（NER）问题上的表现。具体而言，我们的实验结果突显了它们的优势和局限性，识别出五种代表性错误类型，并提供了有关其在特定领域任务中的潜力和挑战的见解。 

---
# Survey on Question Answering over Visually Rich Documents: Methods, Challenges, and Trends 

**Title (ZH)**: 视觉丰富文档上的问答研究：方法、挑战与趋势 

**Authors**: Camille Barboule, Benjamin Piwowarski, Yoan Chabot  

**Link**: [PDF](https://arxiv.org/pdf/2501.02235)  

**Abstract**: Using Large Language Models (LLMs) for Visually-rich Document Understanding (VrDU) has significantly improved performance on tasks requiring both comprehension and generation, such as question answering, albeit introducing new challenges. This survey explains how VrDU models enhanced by LLMs function, covering methods for integrating VrD features into LLMs and highlighting key challenges. 

**Abstract (ZH)**: 使用大型语言模型（LLMs）进行富含视觉信息的文档理解（VrDU）已经在需要理解和生成的任务，如问答方面显著提高了性能，尽管也带来了新的挑战。本文综述了增强型VrDU模型的功能，涵盖了将VrD特征集成到LLMs的方法，并突出了关键挑战。 

---
# CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction 

**Title (ZH)**: CPTuning: 对比提示调优在生成性关系抽取中的应用 

**Authors**: Jiaxin Duan, Fengyu Lu, Junfei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.02196)  

**Abstract**: Generative relation extraction (RE) commonly involves first reformulating RE as a linguistic modeling problem easily tackled with pre-trained language models (PLM) and then fine-tuning a PLM with supervised cross-entropy loss. Although having achieved promising performance, existing approaches assume only one deterministic relation between each pair of entities without considering real scenarios where multiple relations may be valid, i.e., entity pair overlap, causing their limited applications. To address this problem, we introduce a novel contrastive prompt tuning method for RE, CPTuning, which learns to associate a candidate relation between two in-context entities with a probability mass above or below a threshold, corresponding to whether the relation exists. Beyond learning schema, CPTuning also organizes RE as a verbalized relation generation task and uses Trie-constrained decoding to ensure a model generates valid relations. It adaptively picks out the generated candidate relations with a high estimated likelihood in inference, thereby achieving multi-relation extraction. We conduct extensive experiments on four widely used datasets to validate our method. Results show that T5-large fine-tuned with CPTuning significantly outperforms previous methods, regardless of single or multiple relations extraction. 

**Abstract (ZH)**: 生成式关系提取（RE）通常首先将关系提取重新表述为一个语言建模问题，利用预训练语言模型（PLM）来解决，并随后使用监督交叉熵损失对PLM进行微调。尽管这些方法取得了令人鼓舞的性能，但现有方法假设每一对实体之间仅有一个确定的关系，而没有考虑到实际场景中可能存在多个有效关系的情况，即实体对的重叠，这限制了它们的应用。为解决这一问题，我们提出了一种新的对比提示调优方法CPTuning，该方法学习将一对上下文实体之间可能的关系与高于或低于阈值的概率质量关联起来，从而判断关系是否存在。除了学习模式之外，CPTuning还将关系提取组织为一个口头描述的关系生成任务，并使用Trie约束解码以确保模型能够生成有效的关系。在推理时，它会自适应地挑选出具有高估计似然度的生成候选关系，从而实现多关系提取。我们进行了广泛的实验，以四个广泛使用的数据集验证了该方法。结果显示，使用CPTuning微调的T5-large，在无论是单一关系还是多重关系提取方面，都显著优于先前的方法。 

---
# Personalized Graph-Based Retrieval for Large Language Models 

**Title (ZH)**: 基于图的个性化检索方法在大型语言模型中的应用 

**Authors**: Steven Au, Cameron J. Dimacali, Ojasmitha Pedirappagari, Namyong Park, Franck Dernoncourt, Yu Wang, Nikos Kanakaris, Hanieh Deilamsalehy, Ryan A. Rossi, Nesreen K. Ahmed  

**Link**: [PDF](https://arxiv.org/pdf/2501.02157)  

**Abstract**: As large language models (LLMs) evolve, their ability to deliver personalized and context-aware responses offers transformative potential for improving user experiences. Existing personalization approaches, however, often rely solely on user history to augment the prompt, limiting their effectiveness in generating tailored outputs, especially in cold-start scenarios with sparse data. To address these limitations, we propose Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG), a framework that leverages user-centric knowledge graphs to enrich personalization. By directly integrating structured user knowledge into the retrieval process and augmenting prompts with user-relevant context, PGraphRAG enhances contextual understanding and output quality. We also introduce the Personalized Graph-based Benchmark for Text Generation, designed to evaluate personalized text generation tasks in real-world settings where user history is sparse or unavailable. Experimental results show that PGraphRAG significantly outperforms state-of-the-art personalization methods across diverse tasks, demonstrating the unique advantages of graph-based retrieval for personalization. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的发展，它们提供个性化的、情境感知的响应能力具有改变用户体验的潜力。然而，现有的个性化方法通常仅依赖用户的使用历史来增强提示，这在数据稀疏或缺失的冷启动场景中限制了其生成个性化输出的效果。为了解决这些限制，我们提出了一种基于个性化图的检索增强生成（PGraphRAG）框架，该框架利用用户为中心的知识图谱来增强个性化。通过直接将结构化的用户知识整合到检索过程中，并使用用户相关背景信息增强提示，PGraphRAG 提升了语境理解和输出质量。我们还引入了基于个性化图的文本生成基准测试，旨在评估在用户历史稀疏或不可用的现实环境中个性化文本生成任务的表现。实验结果表明，PGraphRAG 在多种任务中显著优于现有的个性化方法，证明了基于图的检索方法在个性化中的独特优势。 

---
# Applying Text Mining to Analyze Human Question Asking in Creativity Research 

**Title (ZH)**: 将以下论文内容或标题翻译成中文，并符合学术规范：

"将文本挖掘应用于创造力研究中的人类提问分析" 

**Authors**: Anna Wróblewska, Marceli Korbin, Yoed N. Kenett, Daniel Dan, Maria Ganzha, Marcin Paprzycki  

**Link**: [PDF](https://arxiv.org/pdf/2501.02090)  

**Abstract**: Creativity relates to the ability to generate novel and effective ideas in the areas of interest. How are such creative ideas generated? One possible mechanism that supports creative ideation and is gaining increased empirical attention is by asking questions. Question asking is a likely cognitive mechanism that allows defining problems, facilitating creative problem solving. However, much is unknown about the exact role of questions in creativity. This work presents an attempt to apply text mining methods to measure the cognitive potential of questions, taking into account, among others, (a) question type, (b) question complexity, and (c) the content of the answer. This contribution summarizes the history of question mining as a part of creativity research, along with the natural language processing methods deemed useful or helpful in the study. In addition, a novel approach is proposed, implemented, and applied to five datasets. The experimental results obtained are comprehensively analyzed, suggesting that natural language processing has a role to play in creative research. 

**Abstract (ZH)**: 创造力与在兴趣领域内产生新颖且有效想法的能力相关。这些有创意的想法是如何产生的呢？支持创造性想法生成的一种可能机制，并逐渐得到实验性关注的是通过提问。提问可能是一种认知机制，有助于定义问题，促进创造性问题解决。然而，关于问题在创造力中的具体作用还有很多未知之处。本研究试图通过应用文本挖掘方法来衡量问题的认知潜力，考虑的因素包括但不限于（a）问题类型，（b）问题复杂性，以及（c）答案的内容。本文总结了作为创造力研究一部分的问题挖掘历史，并介绍了在研究中认为有用或有助于研究的自然语言处理方法。此外，本研究还提出、实现并应用于五个数据集的新颖方法。实验结果的全面分析表明，自然语言处理在创造性研究中起着重要作用。 

---
# Instruction-Following Pruning for Large Language Models 

**Title (ZH)**: 遵循指令的剪枝方法在大规模语言模型中的应用 

**Authors**: Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei  

**Link**: [PDF](https://arxiv.org/pdf/2501.02086)  

**Abstract**: With the rapid scaling of large language models (LLMs), structured pruning has become a widely used technique to learn efficient, smaller models from larger ones, delivering superior performance compared to training similarly sized models from scratch. In this paper, we move beyond the traditional static pruning approach of determining a fixed pruning mask for a model, and propose a dynamic approach to structured pruning. In our method, the pruning mask is input-dependent and adapts dynamically based on the information described in a user instruction. Our approach, termed "instruction-following pruning", introduces a sparse mask predictor that takes the user instruction as input and dynamically selects the most relevant model parameters for the given task. To identify and activate effective parameters, we jointly optimize the sparse mask predictor and the LLM, leveraging both instruction-following data and the pre-training corpus. Experimental results demonstrate the effectiveness of our approach on a wide range of evaluation benchmarks. For example, our 3B activated model improves over the 3B dense model by 5-8 points of absolute margin on domains such as math and coding, and rivals the performance of a 9B model. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的快速扩展，结构化剪枝已成为一种广泛使用的技术，可以从较大的模型中学习出更高效、更小的模型，其性能优于从头训练同等大小的模型。在本文中，我们超越了传统的静态剪枝方法，即为模型确定一个固定的剪枝掩码，提出了一个动态的结构化剪枝方法。在我们的方法中，剪枝掩码取决于输入，并根据用户的指令描述内容动态适应。我们提出的方法称为“指令跟随剪枝”，其中引入了一个稀疏掩码预测器，该预测器将用户的指令作为输入，并动态选择与给定任务最相关的模型参数。为了识别和激活有效的参数，我们联合优化了稀疏掩码预测器和LLM，利用了结合指令跟随数据和预训练语料库的方法。实验结果表明，我们的方法在多种评估基准上都表现出有效性。例如，我们在数学和编程领域中，3B量级的激活模型比3B量级的密集模型提高了5-8个绝对分数，并且在性能上与9B量级的模型不相上下。 

---
# The interplay between domain specialization and model size: a case study in the legal domain 

**Title (ZH)**: 领域专业化与模型规模之间的相互作用：法律领域的案例研究 

**Authors**: Roseval Malaquias Junior, Ramon Pires, Thales Sales Almeida, Kenzo Sakiyama, Roseli Romero, Rodrigo Nogueira  

**Link**: [PDF](https://arxiv.org/pdf/2501.02068)  

**Abstract**: Scaling laws for language models so far focused on finding the compute-optimal model size and token count for training from scratch. However, achieving this optimal balance requires significant compute resources due to the extensive data demands when training models from randomly-initialized weights. Continual pre-training offers a cost-effective alternative, leveraging the compute investment from pre-trained models to incorporate new knowledge without requiring extensive new data. Recent findings suggest that data quality influences constants in scaling laws, thereby altering the optimal parameter-token allocation ratio. Building on this insight, we investigate the interplay between domain specialization and model size during continual pre-training under compute-constrained scenarios. Our goal is to identify a compute-efficient training regime for this scenario and, potentially, detect patterns in this interplay that can be generalized across different model sizes and domains. To compare general and specialized training, we filtered a web-based dataset to extract legal domain data. We pre-trained models with 1.5B, 3B, 7B and 14B parameters on both the unfiltered and filtered datasets, then evaluated their performance on legal exams. Results show that as model size increases, the compute-effectiveness gap between specialized and general models widens. 

**Abstract (ZH)**: 到目前为止，语言模型的缩放规律主要集中在寻找从头训练时计算最优的模型大小和标记数。然而，要达到这种最优平衡需要大量的计算资源，因为从随机初始化权重开始训练模型会带来广泛的数据需求。连续预训练提供了一种成本效益更高的替代方案，利用预训练模型的投资来吸收新知识，而无需大量新的数据。最近的研究发现，数据质量影响缩放定律中的常数，从而改变最优参数-标记分配比例。基于这一洞见，我们研究了在受计算限制场景下连续预训练期间领域专业化与模型大小之间的相互作用。我们的目标是为这种情况确定一种计算高效训练方案，并且可能检测出能够跨不同模型大小和领域泛化的这种相互作用模式。为了对比通用训练和专业化训练，我们筛选了一个基于网络的数据集，提取出了法律领域的数据。我们在未筛选和筛选后的数据集上分别对参数量为15亿、30亿、70亿和140亿的模型进行了预训练，然后评估了它们在法律考试中的性能。结果显示，随着模型规模的增加，专业化模型和通用模型之间的计算效率差距逐渐增大。 

---
# AGGA: A Dataset of Academic Guidelines for Generative AI and Large Language Models 

**Title (ZH)**: AGGA：生成式人工智能和大型语言模型的学术指南数据集 

**Authors**: Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar  

**Link**: [PDF](https://arxiv.org/pdf/2501.02063)  

**Abstract**: This study introduces AGGA, a dataset comprising 80 academic guidelines for the use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic settings, meticulously collected from official university websites. The dataset contains 188,674 words and serves as a valuable resource for natural language processing tasks commonly applied in requirements engineering, such as model synthesis, abstraction identification, and document structure assessment. Additionally, AGGA can be further annotated to function as a benchmark for various tasks, including ambiguity detection, requirements categorization, and the identification of equivalent requirements. Our methodologically rigorous approach ensured a thorough examination, with a selection of universities that represent a diverse range of global institutions, including top-ranked universities across six continents. The dataset captures perspectives from a variety of academic fields, including humanities, technology, and both public and private institutions, offering a broad spectrum of insights into the integration of GAIs and LLMs in academia. 

**Abstract (ZH)**: 本研究介绍了AGGA数据集，该数据集包含80项学术指南，用于指导学术环境中生成式人工智能（GAIs）和大型语言模型（LLMs）的使用。这些指南细心收集自官方大学网站。数据集包含188,674词，是自然语言处理任务的重要资源，这些任务在需求工程中较为常见，如模型合成、抽象识别和文档结构评估。此外，AGGA还可进一步标注，用作包括歧义检测、需求分类和等效需求识别在内的各种任务的基准。我们严谨的方法确保了彻底的审查，选择了代表六大洲各种不同类型机构的大学，涵盖了全球顶尖大学。数据集涵盖了人文学科、技术以及公共和私营机构等多个学术领域的视角，提供了关于在学术界整合GAIs和LLMs的广泛见解。 

---
# Advancing Pancreatic Cancer Prediction with a Next Visit Token Prediction Head on top of Med-BERT 

**Title (ZH)**: 基于Med-BERT的下次就诊时点预测头部以提升胰腺癌预测 

**Authors**: Jianping He, Laila Rasmy, Degui Zhi, Cui Tao  

**Link**: [PDF](https://arxiv.org/pdf/2501.02044)  

**Abstract**: Background: Recently, numerous foundation models pretrained on extensive data have demonstrated efficacy in disease prediction using Electronic Health Records (EHRs). However, there remains some unanswered questions on how to best utilize such models especially with very small fine-tuning cohorts. Methods: We utilized Med-BERT, an EHR-specific foundation model, and reformulated the disease binary prediction task into a token prediction task and a next visit mask token prediction task to align with Med-BERT's pretraining task format in order to improve the accuracy of pancreatic cancer (PaCa) prediction in both few-shot and fully supervised settings. Results: The reformulation of the task into a token prediction task, referred to as Med-BERT-Sum, demonstrates slightly superior performance in both few-shot scenarios and larger data samples. Furthermore, reformulating the prediction task as a Next Visit Mask Token Prediction task (Med-BERT-Mask) significantly outperforms the conventional Binary Classification (BC) prediction task (Med-BERT-BC) by 3% to 7% in few-shot scenarios with data sizes ranging from 10 to 500 samples. These findings highlight that aligning the downstream task with Med-BERT's pretraining objectives substantially enhances the model's predictive capabilities, thereby improving its effectiveness in predicting both rare and common diseases. Conclusion: Reformatting disease prediction tasks to align with the pretraining of foundation models enhances prediction accuracy, leading to earlier detection and timely intervention. This approach improves treatment effectiveness, survival rates, and overall patient outcomes for PaCa and potentially other cancers. 

**Abstract (ZH)**: 背景：近年来，大量基于广泛数据预训练的基础模型在使用电子健康记录（EHRs）进行疾病预测方面显示出了有效性。然而，如何最好地利用这些模型，特别是在细调样本非常小的情况下，仍旧存在一些未解答的问题。方法：我们利用了Med-BERT这一针对EHR特定的基础模型，并将疾病二分类预测任务重新公式化为一个令牌预测任务和一个未来就诊遮罩令牌预测任务，以匹配Med-BERT的预训练任务格式，从而在少量样本和完全监督设置下均提高了胰腺癌（PaCa）预测的准确性。结果：将任务重新公式化为令牌预测任务（称为Med-BERT-Sum），在少量样本和大样本数据中均显示出略微更好的性能。此外，将预测任务重新公式化为未来就诊遮罩令牌预测任务（Med-BERT-Mask）在少量样本场景（样本数量从10到500不等）中比传统二分类（BC）预测任务（Med-BERT-BC）高出3%到7%的性能。这些发现强调，将下游任务与Med-BERT的预训练目标对齐显著增强了模型的预测能力，从而提高了其在预测罕见和常见疾病方面的有效性。结论：将疾病预测任务重新格式化以符合基础模型的预训练，提高了预测准确性，从而实现了更早的检测和及时干预。这种方法提高了胰腺癌（PaCa）和其他癌症的治疗方法有效性、生存率和总体患者结果。 

---
# An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage 

**Title (ZH)**: 对大型语言模型生成文本中的文化遗产价值不一致现象的研究 

**Authors**: Fan Bu, Zheng Wang, Siyi Wang, Ziyao Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.02039)  

**Abstract**: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在文化遗产相关任务中的日益普及，如生成历史建筑的描述、翻译古代文本、保存口头传统和创造教育资源等方面，用户和研究者愈发依赖其生成准确且文化适应性文本的能力。然而，在生成的文本中可能存在文化价值偏差，例如对历史事实的误述、文化身份的侵蚀以及复杂文化叙事的简化，这可能导致严重的后果。因此，在文化遗产背景下调查LLMs的文化价值偏差对于减轻这些风险至关重要，但在这一领域仍缺乏系统的全面研究。为了填补这一空白，我们系统地评估了LLMs在文化遗产相关任务中生成文化适应性文本的可靠性。我们通过综合评估，将1066个查询任务编译成一个广泛的集合，涵盖文化遗产知识框架下的5个广泛认可的类别与17个方面，分别在5个开源LLMs中进行评估，并检查生成文本中文化价值偏差的类型和频率。采用自动化和手动方法，我们有效地检测并分析了LLMs生成文本中的文化价值偏差。我们的研究结果令人担忧：超过65%的生成文本出现了显著的文化偏差，某些任务甚至几乎完全与关键文化价值相悖。除了这些发现，本文还介绍了基准数据集和全面的评估工作流，这些资源可以为未来旨在增强LLMs的文化敏感性和可靠性的研究提供宝贵的资源。 

---
# CarbonChat: Large Language Model-Based Corporate Carbon Emission Analysis and Climate Knowledge Q&A System 

**Title (ZH)**: CarbonChat：基于大规模语言模型的企业碳排放分析与气候变化知识问答系统 

**Authors**: Zhixuan Cao, Ming Han, Jingtao Wang, Meng Jia  

**Link**: [PDF](https://arxiv.org/pdf/2501.02031)  

**Abstract**: As the impact of global climate change intensifies, corporate carbon emissions have become a focal point of global attention. In response to issues such as the lag in climate change knowledge updates within large language models, the lack of specialization and accuracy in traditional augmented generation architectures for complex problems, and the high cost and time consumption of sustainability report analysis, this paper proposes CarbonChat: Large Language Model-based corporate carbon emission analysis and climate knowledge Q&A system, aimed at achieving precise carbon emission analysis and policy this http URL, a diversified index module construction method is proposed to handle the segmentation of rule-based and long-text documents, as well as the extraction of structured data, thereby optimizing the parsing of key this http URL, an enhanced self-prompt retrieval-augmented generation architecture is designed, integrating intent recognition, structured reasoning chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic understanding and query this http URL, based on the greenhouse gas accounting framework, 14 dimensions are established for carbon emission analysis, enabling report summarization, relevance evaluation, and customized this http URL, through a multi-layer chunking mechanism, timestamps, and hallucination detection features, the accuracy and verifiability of the analysis results are ensured, reducing hallucination rates and enhancing the precision of the responses. 

**Abstract (ZH)**: 随着全球气候变化的影响加剧，企业碳排放已成为全球关注的焦点。针对大型语言模型在气候知识更新滞后、传统增强生成架构在处理复杂问题时缺乏专业性和准确性以及可持续性报告分析成本高、耗时长等问题，本文提出CarbonChat：一种基于大型语言模型的企业碳排放分析和气候知识问答系统，旨在实现精准的碳排放分析和政策支持。本文提出了一种多样化的指标模块构建方法，以处理基于规则和长文本文档的分段和结构化数据提取问题，从而优化关键信息的解析。此外，本文设计了一种增强的自我提示检索增强生成架构，融合了意图识别、结构化推理链、混合检索和Text2SQL技术，提高了语义理解和查询效率。基于温室气体核算框架，本文建立了14个维度的碳排放分析体系，实现报告总结、相关性评估和定制化支持。通过多层次的片段划分机制、时间戳和幻觉检测功能，确保分析结果的准确性和可验证性，降低幻觉率并提高响应的精确度。 

---
# Recursive Decomposition of Logical Thoughts: Framework for Superior Reasoning and Knowledge Propagation in Large Language Models 

**Title (ZH)**: 逻辑思维的递归分解：在大型语言模型中实现高级推理和知识传播的框架 

**Authors**: Kaleem Ullah Qasim, Jiashu Zhang, Tariq Alsahfi, Ateeq Ur Rehman Butt  

**Link**: [PDF](https://arxiv.org/pdf/2501.02026)  

**Abstract**: Enhancing the reasoning capabilities of Large Language Models remains a critical challenge in artificial intelligence. We introduce RDoLT, Recursive Decomposition of Logical Thought prompting, a novel framework that significantly boosts LLM reasoning performance. RDoLT is built on three key innovations: (1) recursively breaking down complex reasoning tasks into sub-tasks of progressive complexity; (2) employing an advanced selection and scoring mechanism to identify the most promising reasoning thoughts; and (3) integrating a knowledge propagation module that mimics human learning by keeping track of strong and weak thoughts for information propagation. Our approach was evaluated across multiple benchmarks, including GSM8K, SVAMP, MultiArith, LastLetterConcatenation, and Gaokao2023 Math. The results demonstrate that RDoLT consistently outperforms existing state-of-the-art techniques, achieving a 90.98 percent accuracy on GSM8K with ChatGPT-4, surpassing state-of-the-art techniques by 6.28 percent. Similar improvements were observed on other benchmarks, with accuracy gains ranging from 5.5 percent to 6.75 percent. These findings highlight RDoLT's potential to advance prompt engineering, offering a more effective and generalizable approach to complex reasoning tasks. 

**Abstract (ZH)**: 增强大型语言模型的推理能力仍然是人工智能领域的关键挑战。我们介绍了一种名为RDoLT的新框架——递归分解逻辑思维提示，该框架显著提升了大型语言模型的推理性能。RDoLT基于以下三项创新构建：（1）递归地将复杂的推理任务分解为逐级复杂度的子任务；（2）采用先进的选择和评分机制以识别最有前途的推理思路；以及（3）集成一个知识传播模块，该模块模仿人类学习方式，记录强弱思路以促进信息传播。我们的方法在多个基准测试中进行了评估，包括GSM8K、SVAMP、MultiArith、LastLetterConcatenation和Gaokao2023数学。结果显示，RDoLT在多个基准测试中持续优于现有的顶尖技术，ChatGPT-4在GSM8K上的准确性达到90.98%，比最先进技术高出6.28个百分点。在其他基准测试中也观察到了类似改进，准确性提高幅度从5.5个百分点到6.75个百分点不等。这些发现突显了RDoLT在促进提示工程方面的潜力，提供了一种更有效且通用的复杂推理任务处理方法。 

---
# Enhancing Uncertainty Modeling with Semantic Graph for Hallucination Detection 

**Title (ZH)**: 使用语义图增强不确定性建模以提高幻觉检测效果 

**Authors**: Kedi Chen, Qin Chen, Jie Zhou, Xinqi Tao, Bowen Ding, Jingwen Xie, Mingchen Xie, Peilong Li, Feng Zheng, Liang He  

**Link**: [PDF](https://arxiv.org/pdf/2501.02020)  

**Abstract**: Large Language Models (LLMs) are prone to hallucination with non-factual or unfaithful statements, which undermines the applications in real-world scenarios. Recent researches focus on uncertainty-based hallucination detection, which utilizes the output probability of LLMs for uncertainty calculation and does not rely on external knowledge or frequent sampling from LLMs. Whereas, most approaches merely consider the uncertainty of each independent token, while the intricate semantic relations among tokens and sentences are not well studied, which limits the detection of hallucination that spans over multiple tokens and sentences in the passage. In this paper, we propose a method to enhance uncertainty modeling with semantic graph for hallucination detection. Specifically, we first construct a semantic graph that well captures the relations among entity tokens and sentences. Then, we incorporate the relations between two entities for uncertainty propagation to enhance sentence-level hallucination detection. Given that hallucination occurs due to the conflict between sentences, we further present a graph-based uncertainty calibration method that integrates the contradiction probability of the sentence with its neighbors in the semantic graph for uncertainty calculation. Extensive experiments on two datasets show the great advantages of our proposed approach. In particular, we obtain substantial improvements with 19.78% in passage-level hallucination detection. 

**Abstract (ZH)**: 大语言模型（LLMs）容易产生不合事实或不忠实的陈述，这损害了其在实际场景中的应用。近期的研究关注基于不确定性进行幻觉检测，这种方法利用LLM的输出概率进行不确定性计算，不需要依赖外部知识或频繁从LLM中抽样。然而，大多数方法仅考虑每个独立词元的不确定性，而词元和句子之间的复杂语义关系没有得到充分研究，这限制了对跨越多个词元和句子的幻觉检测。本文提出了一种通过语义图增强不确定性建模的方法用于幻觉检测。具体来说，我们首先构建一个能够很好地捕获实体词元和句子之间关系的语义图。然后，我们通过融合两个实体之间的关系进行不确定性传播，以增强句子级别的幻觉检测。鉴于幻觉是由句子之间的冲突引起的，我们进一步提出了一种基于图的不确定性校准方法，该方法将句子与其语义图中邻居的矛盾概率结合起来进行不确定性计算。在两个数据集上的广泛实验表明，我们提出的方法具有显著优势，特别是在段落级别的幻觉检测中取得了21.78%的显著改进。 

---
# Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs 

**Title (ZH)**: 实时实现可调安全与性能trade-off的大语言模型防护方法 

**Authors**: Joao Fonseca, Andrew Bell, Julia Stoyanovich  

**Link**: [PDF](https://arxiv.org/pdf/2501.02018)  

**Abstract**: Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs "self-reflect", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict ``normal'' model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SafeNudge, that combines Controlled Text Generation with "nudging", or using text interventions to change the behavior of a model. SafeNudge triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SafeNudge is open-source and available through this https URL, and is compatible with models loaded with the Hugging Face "transformers" library. 

**Abstract (ZH)**: 大型语言模型（LLMs）已被证明对“越狱”攻击（jailbreak attacks）或用于诱使模型表现出高风险行为的对抗攻击较为敏感。越狱攻击已被恶意黑客分子利用，造成严重影响，这突显了保护广泛部署的模型的迫切需求。现有的保护方法，包括对模型进行微调或让LLMs进行“自我反思”，可能会延长模型的推理时间，增加计算开销，降低输出的语义流畅性，并限制模型的正常行为。重要的是，这些安全-性能权衡（Safety-Performance Trade-offs, SPTs）仍是一个研究不足的领域。在本工作中，我们提出了一种新的保护方法，称为SafeNudge，这是一种结合了受控文本生成与“引导”（nudging）的创新方法。“引导”是指通过文本干预来改变模型的行为。SafeNudge 在执行越狱攻击时触发，并且能够通过引导LLM生成更安全的响应来降低30%的成功越狱尝试。它对推理时间几乎不造成额外的延迟，并且对输出的语义流畅性影响微乎其微。此外，SafeNudge 实现了可调的安全-性能权衡。SafeNudge 是开源的，并可通过以下链接获取：[https://](https://)，并且与使用Hugging Face “transformers”库加载的模型兼容。 

---
# Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts 

**Title (ZH)**: 大型语言模型中关于概念柏拉图表示的跨模型可转移性 

**Authors**: Youcheng Huang, Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv  

**Link**: [PDF](https://arxiv.org/pdf/2501.02009)  

**Abstract**: Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs. 

**Abstract (ZH)**: Understanding Large Language Models (LLMs) 的内部工作机制是一项关键的研究前沿。先前的研究已表明，单个 LLM 的概念表示可以被捕捉为引导向量（SVs），从而控制 LLM 的行为（例如，使其偏向生成有害内容）。我们采用了一种新的方法，通过探索不同 LLM 间概念表示的复杂关系，使得这一研究与柏拉图的《洞穴寓言》产生了有趣的联系。具体而言，我们引入了一种线性变换方法，用于连接这些表示，并提出以下三个关键发现：1）可以使用简单的线性变换有效地对齐不同 LLM 间的概念表示，从而通过 SVs 实现跨模型高效转移和行为控制。2）这种线性变换在不同概念上具有一般性，能够促进不同 LLM 中不同概念表示的 SVs 的对齐和控制。3）存在从弱到强的转移性，即从较小的 LLM 中提取的 SVs 可以有效控制较大 LLM 的行为。 

---
# Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation 

**Title (ZH)**: 用于视觉语言模型评估的具有挑战性的多项选择题的自动化生成 

**Authors**: Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, Serena Yeung-Levy  

**Link**: [PDF](https://arxiv.org/pdf/2501.03225)  

**Abstract**: The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation. 

**Abstract (ZH)**: 视觉语言模型（VLMs）的快速发展需要严格的可靠评估。然而，当前的视觉问答（VQA）基准通常依赖于开放式问题，这使得准确评估变得困难，因为自然语言响应的变化性较大。为了解决这一问题，我们提出了AutoConverter，这是一种自主框架，能够自动将开放式问题转换为多项选择格式，从而使得评估更加客观，并且减少了成本高昂的问题创建过程。我们的实验表明，AutoConverter可以生成正确且具有挑战性的多项选择题，且VLMs在这些问题上的准确率与人类创建的问题相当甚至略低。通过使用AutoConverter，我们构建了VMCBench，这是一个通过将20个现有的VQA数据集统一转换为多项选择格式而创建的基准，共计9,018个问题。我们在VMCBench上全面评估了33个最新的VLMs，为可扩展、一致和可再现的VLM评估设立了新标准。 

---
# ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events 

**Title (ZH)**: ChronoSense：探索事件时间区间在大型语言模型中的时间理解 

**Authors**: Duygu Sezen Islakoglu, Jan-Christoph Kalo  

**Link**: [PDF](https://arxiv.org/pdf/2501.03040)  

**Abstract**: Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中取得了显著的成功，但在推理和算术方面仍然面临重大挑战。时间推理是自然语言理解的一个关键组成部分，近年来引起了越来越多的研究关注。然而，关于Allen区间关系（如before、after、during等）的全面测试——这是界定时间关系的基本框架——仍然很少被探索。为填补这一空白，我们提出了ChronoSense，这是一种新的基准，用于评估LLMs的时间理解能力。该基准包括16项任务，重点在于识别两个时间事件之间的Allen关系以及使用时间和算术，涉及抽象事件和来自Wikidata的现实世界数据。我们使用这一基准评估了七种最近的LLMs，并且结果表明，这些模型在处理Allen关系，即便是对称的Allen关系时，表现出显著的不同。此外，研究还发现，这些模型可能依赖于记忆来回答时间相关的问题。总体而言，模型较低的性能指出了LLMs在时间理解方面需要改进的需求，而ChronoSense提供了一个强大的框架，以促进这一领域的未来研究。我们的数据集和源代码可在此链接处获得：this https URL。 

---
# Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment 

**Title (ZH)**: 分析多模态大模型微调中的表示偏移，引导对齐 

**Authors**: Pegah Khayatan, Mustafa Shukor, Jayneel Parekh, Matthieu Cord  

**Link**: [PDF](https://arxiv.org/pdf/2501.03012)  

**Abstract**: Multimodal LLMs have reached remarkable levels of proficiency in understanding multimodal inputs, driving extensive research to develop increasingly powerful models. However, much less attention has been paid to understanding and explaining the underlying mechanisms of these models. Most existing explainability research examines these models only in their final states, overlooking the dynamic representational shifts that occur during training. In this work, we systematically analyze the evolution of hidden state representations to reveal how fine-tuning alters the internal structure of a model to specialize in new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace changes in encoded concepts across modalities as training progresses. We also demonstrate the use of shift vectors to capture these concepts changes. These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model. Finally, we explore the practical impact of our findings on model steering, showing that we can adjust multimodal LLMs behaviors without any training, such as modifying answer types, captions style, or biasing the model toward specific responses. Our work sheds light on how multimodal representations evolve through fine-tuning and offers a new perspective for interpreting model adaptation in multimodal tasks. The code for this project is publicly available at this https URL. 

**Abstract (ZH)**: 多模态大语言模型在理解和处理多模态输入方面已经达到了显著的水平，推动了大量关于开发更强大的模型的研究。然而，对这些模型内部机制的理解和解释却相对较少。目前大多数现有的可解释性研究仅在模型的最终状态进行分析，忽视了训练过程中发生的动态表征变化。在本工作中，我们系统地分析了隐藏状态表示的演变，揭示了微调如何改变模型的内部结构以适应新的多模态任务。我们采用基于概念的方法，将隐藏状态映射到可解释的视觉和文本概念，从而能够随着训练的进行追踪各模态中编码概念的变化。我们还展示了使用迁移向量来捕捉这些概念变化的方法。这些迁移向量使我们能够通过将原始模型中的概念进行迁移来恢复微调后的概念。此外，我们探讨了我们的发现对模型导向的实际影响，展示了可以在不进行任何训练的情况下调整多模态大语言模型的行为，例如更改答案类型、修改描述风格或偏向特定响应。我们的工作揭示了多模态表示在微调过程中如何演变，并为理解多模态任务中模型适应提供了新的视角。本项目的代码已在此公开：[这个网址]。 

---
# CALM: Curiosity-Driven Auditing for Large Language Models 

**Title (ZH)**: CALM：好奇心驱动的大语言模型审核 

**Authors**: Xiang Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02997)  

**Abstract**: Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at this https URL. 

**Abstract (ZH)**: 审计大规模语言模型（LLMs）是一项至关重要且具有挑战性的工作。在本研究中，我们专注于审计不提供模型参数仅提供服务访问的黑盒LLMs。我们将这种审计视为一个黑盒优化问题，其目标是自动发现目标LLMs中表现出非法、不道德或不安全行为的输入-输出对。例如，我们可能会寻找一个非毒性的输入，而目标LLM对其做出有害的回应，或者一个能使目标LLM生成包含政治敏感人物的幻觉响应的输入。由于可行点的稀缺性、提示空间的离散性质以及搜索空间的巨大，这种黑盒优化具有很大的挑战性。为了解决这些问题，我们提出了一种好奇心驱动的大规模语言模型审计方法（CALM），该方法利用内在动机的强化学习来微调LLM作为审计代理，以发现目标LLMs潜在有害和有偏见的输入-输出对。CALM 成功地识别了涉及名人的贬损完成，并在黑盒环境中发现了能够引发特定名称的输入。本项工作为审计黑盒LLMs提供了富有前景的方向。我们的代码可从此链接访问：[这里提供链接]。 

---
# GeAR: Generation Augmented Retrieval 

**Title (ZH)**: GeAR：生成增强检索 

**Authors**: Haoyu Liu, Shaohan Huang, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Furu Wei, Qi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02772)  

**Abstract**: Document retrieval techniques form the foundation for the development of large-scale information systems. The prevailing methodology is to construct a bi-encoder and compute the semantic similarity. However, such scalar similarity is difficult to reflect enough information and impedes our comprehension of the retrieval results. In addition, this computational process mainly emphasizes the global semantics and ignores the fine-grained semantic relationship between the query and the complex text in the document. In this paper, we propose a new method called $\textbf{Ge}$neration $\textbf{A}$ugmented $\textbf{R}$etrieval ($\textbf{GeAR}$) that incorporates well-designed fusion and decoding modules. This enables GeAR to generate the relevant text from documents based on the fused representation of the query and the document, thus learning to "focus on" the fine-grained information. Also when used as a retriever, GeAR does not add any computational burden over bi-encoders. To support the training of the new framework, we have introduced a pipeline to efficiently synthesize high-quality data by utilizing large language models. GeAR exhibits competitive retrieval and localization performance across diverse scenarios and datasets. Moreover, the qualitative analysis and the results generated by GeAR provide novel insights into the interpretation of retrieval results. The code, data, and models will be released after completing technical review to facilitate future research. 

**Abstract (ZH)**: 文档检索技术是大规模信息系统开发的基础。当前主流的方法是构建双编码器并计算语义相似度。然而，这种标量相似度难以充分反映信息，阻碍了我们对检索结果的理解。此外，这一计算过程主要强调全局语义，而忽视了查询与文档中复杂文本之间的细粒度语义关系。本文提出了一种名为**GeAR（Ge**neration **A**ugmented **R**etrieval）的新方法，该方法融合了设计良好的融合和解码模块。这使得GeAR能够在融合查询和文档表示的基础上生成相关文本，从而学习“聚焦”于细粒度信息。此外，当作为检索器使用时，GeAR不会增加任何额外的计算负担。为了支持新框架的训练，我们引入了一条高效的数据合成流水线，利用大型语言模型来生成高质量的数据。GeAR在各种场景和数据集上展现了竞争对手的检索和定位性能。此外，GeAR的定性分析及其生成的结果为检索结果的解释提供了新的见解。在完成技术审查后，我们将发布代码、数据和模型，以促进未来的研究。 

---
# MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting and Attention Distillation 

**Title (ZH)**: MBTSAD：基于令牌拆分和注意力蒸馏的缓解语言模型后门攻击方法 

**Authors**: Yidong Ding, Jiafei Niu, Ping Yi  

**Link**: [PDF](https://arxiv.org/pdf/2501.02754)  

**Abstract**: In recent years, attention-based models have excelled across various domains but remain vulnerable to backdoor attacks, often from downloading or fine-tuning on poisoned datasets. Many current methods to mitigate backdoors in NLP models rely on the pre-trained (unfine-tuned) weights, but these methods fail in scenarios where the pre-trained weights are not available. In this work, we propose MBTSAD, which can mitigate backdoors in the language model by utilizing only a small subset of clean data and does not require pre-trained weights. Specifically, MBTSAD retrains the backdoored model on a dataset generated by token splitting. Then MBTSAD leverages attention distillation, the retrained model is the teacher model, and the original backdoored model is the student model. Experimental results demonstrate that MBTSAD achieves comparable backdoor mitigation performance as the methods based on pre-trained weights while maintaining the performance on clean data. MBTSAD does not rely on pre-trained weights, enhancing its utility in scenarios where pre-trained weights are inaccessible. In addition, we simplify the min-max problem of adversarial training and visualize text representations to discover that the token splitting method in MBTSAD's first step generates Out-of-Distribution (OOD) data, leading the model to learn more generalized features and eliminate backdoor patterns. 

**Abstract (ZH)**: 近年来，基于注意力的模型在各个领域表现出色，但仍然容易遭受后门攻击，这些攻击通常源于下载或在受污染的数据集上进行微调。目前用于缓解NLP模型后门攻击的方法大多依赖于预训练（未微调）的权重，但在这些权重不可用的情况下，这些方法会失效。本研究中，我们提出了MBTSAD（利用标记分裂的后门缓解技术），该方法仅利用一小部分干净数据即可缓解语言模型中的后门攻击，无需使用预训练权重。具体而言，MBTSAD 通过生成由标记分裂产生的数据集对受后门影响的模型进行重新训练。然后，MBTSAD 使用注意力蒸馏技术，其中重新训练后的模型作为教师模型，原始受后门影响的模型作为学生模型。实验结果表明，MBTSAD 在缓解后门攻击方面达到了与基于预训练权重的方法相当的性能，同时在干净数据上的性能也得到了保持。此外，MBTSAD 不依赖于预训练权重，增强了其在预训练权重不可用情况下的实用性。此外，我们简化了对抗训练中的最小-最大问题，并可视化了文本表示，发现MBTSAD第一步中的标记分裂方法生成了分布外（OOD）数据，促使模型学会更多通用特征并消除后门模式。 

---
# KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models 

**Title (ZH)**: KG-CF：在大型语言模型指导下基于上下文过滤的知识图谱完成方法 

**Authors**: Zaiyi Zheng, Yushun Dong, Song Wang, Haochen Liu, Qi Wang, Jundong Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.02711)  

**Abstract**: Large Language Models (LLMs) have shown impressive performance in various tasks, including knowledge graph completion (KGC). However, current studies mostly apply LLMs to classification tasks, like identifying missing triplets, rather than ranking-based tasks, where the model ranks candidate entities based on plausibility. This focus limits the practical use of LLMs in KGC, as real-world applications prioritize highly plausible triplets. Additionally, while graph paths can help infer the existence of missing triplets and improve completion accuracy, they often contain redundant information. To address these issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks. KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts, achieving superior results on real-world datasets. The code and datasets are available at \url{this https URL}. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种任务上展示了出色的表现，包括知识图谱补全（KGC）。然而，当前的研究主要将LLMs应用于分类任务，如识别缺失三元组，而较少应用于基于排名的任务，即模型根据合理性对候选实体进行排名。这种焦点限制了LLMs在KGC中的实际应用，因为实际应用更倾向于选择高合理性的三元组。此外，虽然图形路径有助于推断缺失三元组的存在并对补全准确性有所帮助，但它们通常包含冗余信息。为了解决这些问题，我们提出了KG-CF框架，该框架专门针对基于排名的KGC任务。KG-CF利用LLMs的推理能力过滤无关上下文，实现在真实数据集上的优越性能。相关代码和数据集可在 \url{这个链接} 获取。 

---
# Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs? 

**Title (ZH)**: 从SIMPLE到HARD视觉推理的泛化：我们能否缓解基于视觉语言模型的模态不平衡问题？ 

**Authors**: Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, Sanjeev Arora  

**Link**: [PDF](https://arxiv.org/pdf/2501.02669)  

**Abstract**: While Vision Language Models (VLMs) are impressive in tasks such as visual question answering (VQA) and image captioning, their ability to apply multi-step reasoning to images has lagged, giving rise to perceptions of modality imbalance or brittleness. Towards systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning (AVR), comprising three tasks: Table Readout, Grid Navigation, and Visual Analogy. Each has two levels of difficulty, SIMPLE and HARD, and even the SIMPLE versions are difficult for frontier VLMs. We seek strategies for training on the SIMPLE version of the tasks that improve performance on the corresponding HARD task, i.e., S2H generalization. This synthetic framework, where each task also has a text-only version, allows a quantification of the modality imbalance, and how it is impacted by training strategy. Ablations highlight the importance of explicit image-to-text conversion in promoting S2H generalization when using auto-regressive training. We also report results of mechanistic study of this phenomenon, including a measure of gradient alignment that seems to identify training strategies that promote better S2H generalization. 

**Abstract (ZH)**: 尽管视觉语言模型（VLMs）在视觉问答（VQA）和图像字幕等任务中表现出色，但在对图像进行多步推理方面的能力却有所逊色，这导致了模态不平衡或脆弱性的感知。为系统地研究这些问题，我们引入了一个合成框架，用于评估VLMs执行算法视觉推理（AVR）的能力。该框架包括三个任务：表格读取、网格导航和视觉类比。每个任务都有简单的（SIMPLE）和困难的（HARD）两个难度级别，即使简单的版本，对于前沿的VLMs来说也颇具挑战性。我们寻找策略，通过在SIMPLE版本任务上的训练来提升其相应的HARD任务表现，即S2H泛化。此合成框架提供了每个任务都有纯文本版本的特点，使模态不平衡及其受训练策略影响的程度能够得到量化。消融实验表明，在使用自回归训练时，明确的图像到文本转换对于促进S2H泛化的重要性。我们还报告了对这种现象的机制研究结果，包括一种梯度对齐度量，似乎能够识别出促进更好S2H泛化的训练策略。 

---
# Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense 

**Title (ZH)**: 层级别自我曝光与补丁：针对Jailbreak攻击的肯定性token缓解方法 

**Authors**: Yang Ouyang, Hengrui Gu, Shuhang Lin, Wenyue Hua, Jie Peng, Bhavya Kailkhura, Tianlong Chen, Kaixiong Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2501.02629)  

**Abstract**: As large language models (LLMs) are increasingly deployed in diverse applications, including chatbot assistants and code generation, aligning their behavior with safety and ethical standards has become paramount. However, jailbreak attacks, which exploit vulnerabilities to elicit unintended or harmful outputs, threaten LLMs' safety significantly. In this paper, we introduce Layer-AdvPatcher, a novel methodology designed to defend against jailbreak attacks by utilizing an unlearning strategy to patch specific layers within LLMs through self-augmented datasets. Our insight is that certain layer(s), tend to produce affirmative tokens when faced with harmful prompts. By identifying these layers and adversarially exposing them to generate more harmful data, one can understand their inherent and diverse vulnerabilities to attacks. With these exposures, we then "unlearn" these issues, reducing the impact of affirmative tokens and hence minimizing jailbreak risks while keeping the model's responses to safe queries intact. We conduct extensive experiments on two models, four benchmark datasets, and multiple state-of-the-art jailbreak benchmarks to demonstrate the efficacy of our approach. Results indicate that our framework reduces the harmfulness and attack success rate of jailbreak attacks without compromising utility for benign queries compared to recent defense methods. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在各种应用中的不断部署，包括聊天机器人助手和代码生成，将其行为与安全和伦理标准对齐变得尤为重要。然而，利用漏洞引发意外或有害输出的牢笼破解攻击严重威胁着LLMs的安全性。本文介绍了Layer-AdvPatcher，这是一种创新的方法，旨在通过利用未学习策略修补LLMs中的特定层，从而抵御牢笼破解攻击。我们的见解是，某些层在遇到有害提示时倾向于生成肯定性标记。通过识别这些层，并对它们进行对抗性暴露以生成更多有害数据，可以了解它们对攻击的内在和多样性脆弱性。借助这些暴露，我们随后“未学习”这些问题，减少肯定性标记的影响，从而减少牢笼破解风险，同时保持模型对良性查询的响应完整性。我们在两个模型、四个基准数据集和多个最先进的牢笼破解基准上进行了广泛的实验，以证明我们方法的有效性。结果表明，与以前的方法相比，我们的框架在不损害良性查询的实用性的情况下，减少了牢笼破解攻击的有害性和攻击成功率。 

---
# Efficient Architectures for High Resolution Vision-Language Models 

**Title (ZH)**: 高分辨率视觉-语言模型的高效架构 

**Authors**: Miguel Carvalho, Bruno Martins  

**Link**: [PDF](https://arxiv.org/pdf/2501.02584)  

**Abstract**: Vision-Language Models (VLMs) have recently experienced significant advancements. However, challenges persist in the accurate recognition of fine details within high resolution images, which limits performance in multiple tasks. This work introduces Pheye, a novel architecture that efficiently processes high-resolution images while training fewer parameters than similarly sized VLMs. Notably, Pheye achieves a high efficiency while maintaining strong performance, particularly in tasks that demand fine-grained image understanding and/or the handling of scene-text. 

**Abstract (ZH)**: 视觉-语言模型（VLMs）近年来取得了显著的进步。然而，在高分辨率图像中准确识别细粒度细节仍然面临挑战，这限制了其在多项任务中的表现。本文介绍了一种名为 Pheye 的新型架构，其能够在训练参数量少于同等规模 VLMs 的情况下高效处理高分辨率图像。尤为值得一提的是，Pheye 在要求细粒度图像理解和/或处理场景文本的任务中表现出色，同时保持了高效率。 

---
# LeetDecoding: A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations 

**Title (ZH)**: LeetDecoding：一种具有CUDA实现的指数衰减因变量线性注意力的PyTorch库 

**Authors**: Jiaping Wang, Simiao Zhang, Qiao-Chu He, Yifan Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.02573)  

**Abstract**: The machine learning and data science community has made significant while dispersive progress in accelerating transformer-based large language models (LLMs), and one promising approach is to replace the original causal attention in a generative pre-trained transformer (GPT) with \emph{exponentially decaying causal linear attention}. In this paper, we present LeetDecoding, which is the first Python package that provides a large set of computation routines for this fundamental operator. The launch of LeetDecoding was motivated by the current lack of (1) clear understanding of the complexity regarding this operator, (2) a comprehensive collection of existing computation methods (usually spread in seemingly unrelated fields), and (3) CUDA implementations for fast inference on GPU. LeetDecoding's design is easy to integrate with existing linear-attention LLMs, and allows for researchers to benchmark and evaluate new computation methods for exponentially decaying causal linear attention. The usage of LeetDecoding does not require any knowledge of GPU programming and the underlying complexity analysis, intentionally making LeetDecoding accessible to LLM practitioners. The source code of LeetDecoding is provided at \href{this https URL}{this GitHub repository}, and users can simply install LeetDecoding by the command \texttt{pip install leet-decoding}. 

**Abstract (ZH)**: 机器学习与数据科学社区在加速基于变换器的大规模语言模型（LLMs）方面取得了显著但分散的进展，其中一种有希望的方法是用指数衰减因果线性注意力替换生成预训练变换器（GPT）中的原始因果注意力。本文介绍了LeetDecoding，这是首个提供该基本操作大量计算公式的Python包。LeetDecoding的设计旨在简化与现有线性注意力LLMs的集成，并允许研究人员对标新的指数衰减因果线性注意力计算方法进行基准测试和评估。LeetDecoding的使用无需了解GPU编程和底层复杂性分析，有意使其易于大规模语言模型实践者使用。LeetDecoding的源代码已发布在\href{this https URL}{这个GitHub仓库}上，用户可以通过命令\texttt{pip install leet-decoding}简单地安装LeetDecoding。 

---
# Decoding fMRI Data into Captions using Prefix Language Modeling 

**Title (ZH)**: 使用前缀语言模型解析fMRI数据为描述性语句 

**Authors**: Vyacheslav Shen, Kassymzhomart Kunanbayev, Dae-Shik Kim  

**Link**: [PDF](https://arxiv.org/pdf/2501.02570)  

**Abstract**: With the advancements in Large Language and Latent Diffusion models, brain decoding has achieved remarkable results in recent years. The works on the NSD dataset, with stimuli images from the COCO dataset, leverage the embeddings from the CLIP model for image reconstruction and GIT for captioning. However, the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset. In this work, we present an alternative method for decoding brain signals into image captions by predicting a DINOv2 model's embedding of an image from the corresponding fMRI signal and then providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably. Additionally, instead of commonly used Linear Regression, we explore 3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels. 

**Abstract (ZH)**: 随着大型语言模型和潜在扩散模型的进步，近年来脑解码取得了显著成果。在使用COCO数据集刺激图像的NSD数据集上的研究工作中，研究人员利用CLIP模型的嵌入进行图像重建，并使用GIT模型进行图像描述。然而，当前的描述方法存在潜在数据污染的问题，因为GIT模型是在COCO数据集上进行训练的。在本研究中，我们提出了一种替代方法，通过预测从相应的fMRI信号中得到的DINOv2模型的嵌入来解码脑信号为图像描述，并将该嵌入的[CLS]标记作为前缀提供给GPT-2语言模型，从而大大减少了计算需求。此外，我们探索了一种三维卷积神经网络映射方法，将fMRI信号映射到图像嵌入空间，以更好地考虑体素的位置信息，而非常用的线性回归方法。 

---
# Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI 

**Title (ZH)**: 向着人工智能对齐与社会重要议题情感分析的新基准：在AGI背景下人类与大模型的比较研究 

**Authors**: Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa  

**Link**: [PDF](https://arxiv.org/pdf/2501.02531)  

**Abstract**: With the expansion of neural networks, such as large language models, humanity is exponentially heading towards superintelligence. As various AI systems are increasingly integrated into the fabric of societies-through recommending values, devising creative solutions, and making decisions-it becomes critical to assess how these AI systems impact humans in the long run. This research aims to contribute towards establishing a benchmark for evaluating the sentiment of various Large Language Models in socially importan issues. The methodology adopted was a Likert scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results highlighted a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment score towards AGI, whereas Bard was leaning towards the neutral sentiment. The human samples, contrastingly, showed a lower average sentiment of 2.97. The temporal comparison revealed differences in sentiment evolution between LLMs in three days, ranging from 1.03% to 8.21%. The study's analysis outlines the prospect of potential conflicts of interest and bias possibilities in LLMs' sentiment formation. Results indicate that LLMs, akin to human cognitive processes, could potentially develop unique sentiments and subtly influence societies' perceptions towards various opinions formed within the LLMs. 

**Abstract (ZH)**: 随着神经网络，如大型语言模型的扩展，人类正以指数级的速度向着超级智能迈进。随着各种人工智能系统日益融入社会的各个层面——推荐价值观、构思创造性解决方案和做出决策——评估这些AI系统对人类的长期影响变得至关重要。本研究旨在为评估大型语言模型在社会重要问题上的情感提供一个基准。所采用的方法是李克特量表调查。分析了包括GPT-4和Bard在内的七种大模型，并将它们与三个独立的人类样本群体的情感数据进行了比较。情感变化还分别在三天内进行了评估。结果表明，不同大模型的情感评分存在差异，范围从3.32到4.12（满分5分）。GPT-4 对AGI（通用人工智能）的情感评分最为积极，而Bard的情感则偏向中性。相比之下，人类样本群体的平均情感评分较低，为2.97。时间对比揭示了在三天内大模型情感演变的差异，范围从1.03%到8.21%。研究分析指出了大模型情感形成中潜在的利益冲突和偏见的可能性。结果显示，大模型在类似于人类认知过程的情况下，可能会产生独特的情感，并潜移默化地影响社会对各种意见的认知。 

---
# Test-time Computing: from System-1 Thinking to System-2 Thinking 

**Title (ZH)**: 测试时计算：从直觉思维到反思思维 

**Authors**: Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Jia Xu, Linjian Mo, Min Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02497)  

**Abstract**: The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions. 

**Abstract (ZH)**: o1模型在复杂推理中的出色表现证明了测试时计算扩展可以进一步激发模型的潜在能力，使其具备强大的System-2级思考能力。然而，有关测试时计算扩展的综合研究仍然缺乏。我们将测试时计算的概念追溯到System-1模型。在System-1模型中，测试时计算通过参数更新、输入修改、表示编辑和输出校准来应对分布转移，提高模型的鲁棒性和泛化能力。在System-2模型中，测试时计算通过重复抽样、自我纠正和树搜索来增强模型的推理能力，解决复杂问题。我们将本综述按照从System-1到System-2思维方式的趋势进行组织，强调测试时计算在从System-1模型过渡到弱System-2模型，再到强System-2模型过程中的关键作用。同时，我们还指出了几个可能的研究方向。 

---
# LLMPC: Large Language Model Predictive Control 

**Title (ZH)**: LLMPC: 大型语言模型预测控制 

**Authors**: Gabriel Maher  

**Link**: [PDF](https://arxiv.org/pdf/2501.02486)  

**Abstract**: Recent advancements in prompting techniques for Large Language Models (LLMs) have improved their reasoning, planning, and action abilities. This paper examines these prompting techniques through the lens of model predictive control (MPC). We show that LLMs act as implicit planning cost function minimizers when planning prompts are used. Under our framework we demonstrate that LLM planning performance can be improved further by incorporating real planning cost functions and evaluators. 

**Abstract (ZH)**: 近年来，对大规模语言模型（LLMs）的提示技术取得了显著进展，提高了它们的推理、规划和执行能力。本文通过模型预测控制（MPC）的视角来研究这些提示技术。我们展示了在使用规划提示时，LLMs 作为隐式的规划成本函数最小化器。在我们提出的框架中，我们证明通过引入实际的规划成本函数及其评估器，可以进一步提高LLMs的规划性能。 

---
# A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models 

**Title (ZH)**: 大型语言模型中数据不当利用检测的统计假设检验框架 

**Authors**: Yinpeng Cai, Lexin Li, Linjun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02441)  

**Abstract**: Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, which falls under the broader issue of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated data generated by another LLM. To address this issue, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct a pivotal statistic, determine the optimal rejection threshold, and explicitly control the type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate its empirical effectiveness through intensive numerical experiments. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）迅速获得了极大的 popularity。然而，LLMs的训练也引发了显著的隐私和法律问题，特别是关于在训练数据中未经适当引用或许可就包含受版权保护的材料，这涉及更广泛的不当数据利用问题。本文重点关注数据不当利用检测中的一个特定问题，即确定给定的LLM是否包含了另一个LLM生成的数据。为解决这一问题，我们建议在受版权保护的训练数据中嵌入水印，并将数据不当利用的检测问题形式化为假设检验问题。我们建立了一个通用的统计检验框架，构建了一个关键统计量，确定了最优拒绝阈值，并明确地控制了第一类错误和第二类错误。此外，我们建立了所提检验方法的渐近最优性质，并通过密集的数值实验展示了其实证有效性。 

---
# Efficient Deployment of Large Language Models on Resource-constrained Devices 

**Title (ZH)**: 受限资源设备上高效部署大语言模型 

**Authors**: Zhiwei Yao, Yang Xu, Hongli Xu, Yunming Liao, Zuan Xie  

**Link**: [PDF](https://arxiv.org/pdf/2501.02438)  

**Abstract**: Deploying Large Language Models (LLMs) on resource-constrained (or weak) devices presents significant challenges due to limited resources and heterogeneous data distribution. To address the data concern, it is necessary to fine-tune LLMs using on-device private data for various downstream tasks. While Federated Learning (FL) offers a promising privacy-preserving solution, existing fine-tuning methods retain the original LLM size, leaving issues of high inference latency and excessive memory demands unresolved. Hence, we design FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning (PEFT) with structured pruning for efficient deployment of LLMs on resource-constrained devices. Specifically, FedSpine introduces an iterative process to prune and tune the parameters of LLMs. To mitigate the impact of device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed to adaptively determine different pruning ratios and LoRA ranks for heterogeneous devices without any prior knowledge of their computing and communication capabilities. As a result, FedSpine maintains higher inference accuracy while improving fine-tuning efficiency. Experimental results conducted on a physical platform with 80 devices demonstrate that FedSpine can speed up fine-tuning by 1.4$\times$-6.9$\times$ and improve final accuracy by 0.4%-4.5% under the same sparsity level compared to other baselines. 

**Abstract (ZH)**: 在资源受限（或弱）设备上部署大型语言模型（LLMs）因资源有限且数据分布异质性而面临重大挑战。为了解决数据问题，有必要使用设备上的私有数据对LLMs进行微调以适应多种下游任务。虽然联邦学习（FL）提供了一种有前景的隐私保护解决方案，但现有的微调方法保留了原始的LLM大小，仍然存在推断延迟高和内存需求过高的问题。因此，我们设计了FedSpine，这是一种结合参数高效微调（PEFT）与结构化剪枝的FL框架，以便在资源受限设备上高效部署LLMs。具体而言，FedSpine引入了一个迭代过程来修剪和优化LLMs的参数。为了缓解设备异质性的影响，采用了一个在线多臂老虎机（MAB）算法，在没有任何先验知识的情况下，动态确定不同设备的修剪比例和LoRA秩，从而提高适配性。由此，FedSpine在保持更高推断准确率的同时，提高了微调效率。在具有80个设备的物理平台上进行的实验结果表明，与其它基线方法相比，在相同的稀疏度水平下，FedSpine的微调速度可以提高1.4-6.9倍，并且最终准确率提高0.4%-4.5%。 

---
# Scaling Laws for Floating Point Quantization Training 

**Title (ZH)**: 浮点量化训练的标度律 

**Authors**: Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuai Li, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2501.02423)  

**Abstract**: Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point quantization and thus cannot well fit the LLM losses in this scenario. In contrast, while floating-point quantization training is more commonly implemented in production, the research on it has been relatively superficial. In this paper, we thoroughly explore the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. While presenting an accurate floating-point quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal floating-point quantization precision is directly proportional to the computational power, but within a wide computational power range, we estimate that the best cost-performance precision lies between 4-8 bits. 

**Abstract (ZH)**: 低精度训练被认为是一种有效策略，可以减少训练和下游推断的成本。先前关于精度的标度法则主要集中在整数量化上，较少关注浮点量化中的构成因素，因此在本场景中不能很好地拟合大语言模型（LLM）的损失。相比之下，虽然浮点量化训练在生产环境中更为常见，但相关研究相对较浅薄。在本文中，我们全面探讨了浮点量化目标、指数位、小数位以及缩放因子计算粒度对LLM模型浮点量化训练性能的影响。同时，我们提出了一种准确的浮点量化统一标度法则，并为社区提供了有价值的建议：（1）指数位对模型性能的影响略高于小数位。我们提供了不同位数的最优指数-小数位比值，这为将来硬件制造商提供参考；（2）我们发现低精度LLM训练中存在着关键数据大小。过多的训练数据超过关键数据大小会逆向导致LLM性能下降；（3）最优的浮点量化精度与计算能力成正比，但在广泛的计算能力范围内，我们估计最佳成本效能精度为4-8位。 

---
# Who Wrote This? Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities 

**Title (ZH)**: 谁是作者？基于有限样本集中不等式的零样本统计测试方法识别LLM生成文本 

**Authors**: Tara Radvand, Mojtaba Abdolmaleki, Mohamed Mostagir, Ambuj Tewari  

**Link**: [PDF](https://arxiv.org/pdf/2501.02406)  

**Abstract**: Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly difficult as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)? We model LLM-generated text as a sequential stochastic process with complete dependence on history and design zero-shot statistical tests to distinguish between (i) the text generated by two different sets of LLMs $A$ (in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and human-generated texts. We prove that the type I and type II errors for our tests decrease exponentially in the text length. In designing our tests, we derive concentration inequalities on the difference between log-perplexity and the average entropy of the string under $A$. Specifically, for a given string, we demonstrate that if the string is generated by $A$, the log-perplexity of the string under $A$ converges to the average entropy of the string under $A$, except with an exponentially small probability in string length. We also show that if $B$ generates the text, except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. Lastly, we present preliminary experimental results to support our theoretical results. By enabling guaranteed (with high probability) finding of the origin of harmful LLM-generated text with arbitrary size, we can help fight misinformation. 

**Abstract (ZH)**: 验证内容的来源对于许多组织的功能至关重要，例如教育机构、社交媒体平台、公司等。随着大型语言模型（LLMs）生成的文本越来越难以与人类生成的文本区分开来，这个问题变得越来越困难。此外，许多机构使用自有的LLMs，并希望确保外部未授权的LLMs不会在机构内部生成内容。本文回答了以下问题：给定一段文本，我们能否确定它是由LLM \(A\)还是LLM \(B\)（其中LLM \(B\) 可以是人类）生成的？我们将LLM生成的文本建模为一个完全依赖于历史状态的序贯随机过程，并设计零样本统计检验以区分以下内容：(i) 由两组不同的LLM（自有）和（未授权）生成的文本，(ii) LLM生成的文本和人类生成的文本。我们证明我们的检验的第一类错误和第二类错误随着文本长度的增加呈指数级减少。在设计这些检验时，我们推导了在LLM \(A\) 下字符串的对数困惑度和平均熵之间的差异的集中不等式。特别是，对于给定的字符串，证明如果该字符串是由LLM \(A\) 生成的，那么在长度足够长的情况下，该字符串的对数困惑度将收敛到该字符串在LLM \(A\) 下的平均熵，除了一个指数级小的概率。此外，证明如果LLM \(B\) 生成文本，除了一个指数级小的概率之外，在长度足够长的情况下，该字符串的对数困惑度将收敛到LLM \(B\) 和LLM \(A\) 的平均交叉熵。最后，我们展示了初步的实验结果以支持我们的理论结果。通过确保以高概率找到任意大小的有害LLM生成文本的来源，我们可以帮助打击虚假信息。 

---
# Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers 

**Title (ZH)**: 基于图的同构注意机制用于Transformer中的自适应动力学 

**Authors**: Markus J. Buehler  

**Link**: [PDF](https://arxiv.org/pdf/2501.02393)  

**Abstract**: We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies. 

**Abstract (ZH)**: 我们提出了一种通过将图意识关系推理集成到注意力机制中来修改Transformer架构的方法，并融合了图神经网络（Graph Neural Networks, GNNs）和语言建模的概念。基于注意力机制与图理论之间的内在联系，我们将Transformer的注意力机制重新定义为图操作，并提出了一种图意识同构注意力机制。该方法利用了先进的图建模策略，包括图同构网络（Graph Isomorphism Networks, GINs）和主邻域聚合（Principal Neighborhood Aggregation, PNA），以丰富关系结构的表示。我们的方法能够捕捉复杂的依赖关系，并且在多个任务上表现出更好的泛化能力和学习性能。此外，我们扩展了图意识注意力的概念，引入了一种稀疏GIN-注意力方法，该方法通过使用稀疏GINs进行了微调。通过将注意力矩阵解释为稀疏邻接图，这种方法能够在不增加大量计算开销的情况下，增强预训练基础模型的适应性，赋予它们图意识能力。稀疏GIN-注意力微调相比低秩适应（LoRA）等其他方法，在训练动态性和泛化能力上表现更优。我们探讨了传统注意力机制中的潜在图型结构，提供了一个新的视角来理解Transformer。通过将Transformer视为用于关系推理的分层GIN模型，这种视角为基础模型开发提供了深刻的影响，使得设计能够在局部和全局依赖关系之间动态适应的架构成为可能。该研究为生物信息学、材料科学、语言建模等领域的关系和序列数据建模提供了一种综合的方法，为进一步实现可解释性和泛化性强的建模策略奠定了基础。 

---
# Guiding Medical Vision-Language Models with Explicit Visual Prompts: Framework Design and Comprehensive Exploration of Prompt Variations 

**Title (ZH)**: 引导医学视觉语言模型使用显式视觉提示：框架设计与提示变体的全面探索 

**Authors**: Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.02385)  

**Abstract**: With the recent advancements in vision-language models (VLMs) driven by large language models (LLMs), many researchers have focused on models that comprised of an image encoder, an image-to-language projection layer, and a text decoder architectures, leading to the emergence of works like LLava-Med. However, these works primarily operate at the whole-image level, aligning general information from 2D medical images without attending to finer details. As a result, these models often provide irrelevant or non-clinically valuable information while missing critical details. Medical vision-language tasks differ significantly from general images, particularly in their focus on fine-grained details, while excluding irrelevant content. General domain VLMs tend to prioritize global information due to their design, which compresses the entire image into a multi-token representation that is passed into the LLM decoder. Therefore, current VLMs all lack the capability to restrict their attention to particular areas. To address this critical issue in the medical domain, we introduce MedVP, an visual prompt generation and fine-tuning framework, which involves extract medical entities, generate visual prompts, and adapt datasets for visual prompt guided fine-tuning. To the best of our knowledge, this is the first work to explicitly introduce visual prompt into medical VLMs, and we successfully outperform recent state-of-the-art large models across multiple medical VQA datasets. Extensive experiments are conducted to analyze the impact of different visual prompt forms and how they contribute to performance improvement. The results demonstrate both the effectiveness and clinical significance of our approach 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）推动下的视觉-语言模型（VLMs）的进步，许多研究人员集中研究了由图像编码器、图像到语言投影层和文本解码器组成的模型架构，催生了如LLava-Med等研究成果。然而，这些模型主要在图像整体层面上工作，关注于2D医学图像的一般信息而忽视了更精细的细节。因此，这些模型往往会提供不相关或缺乏临床价值的信息，同时忽略了一些关键细节。医学视觉-语言任务与一般的图像任务有显著不同，尤其是在专注于细粒度细节方面，同时排除无关内容。通用领域的VLMs因设计原因倾向于强调全局信息，将其压缩成一个包含多个标记的表示，并传递给LLM解码器。因此，目前的VLMs都无法将注意力锁定在特定区域。为解决这一关键问题，我们提出了MedVP，这是一个视觉提示生成与微调框架，包括提取医学实体、生成视觉提示以及为视觉提示引导的微调适应数据集。据我们所知，这是首次明确地在医学VLMs中引入视觉提示的工作，并且我们成功地在多个医学VQA数据集中超越了最近的先进大型模型。进行了广泛的实验以分析不同形式的视觉提示对性能提升的影响。结果表明了我们方法的有效性和临床重要性。 

---
# Optimizing Small Language Models for In-Vehicle Function-Calling 

**Title (ZH)**: 优化车载功能调用的小语言模型 

**Authors**: Yahya Sowti Khiabani, Farris Atif, Chieh Hsu, Sven Stahlmann, Tobias Michels, Sebastian Kramer, Benedikt Heidrich, M. Saquib Sarfraz, Julian Merten, Faezeh Tafazzoli  

**Link**: [PDF](https://arxiv.org/pdf/2501.02342)  

**Abstract**: We propose a holistic approach for deploying Small Language Models (SLMs) as function-calling agents within vehicles as edge devices, offering a more flexible and robust alternative to traditional rule-based systems. By leveraging SLMs, we simplify vehicle control mechanisms and enhance the user experience. Given the in-vehicle hardware constraints, we apply state-of-the-art model compression techniques, including structured pruning, healing, and quantization, ensuring that the model fits within the resource limitations while maintaining acceptable performance. Our work focuses on optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best practices for enabling embedded models, including compression, task-specific fine-tuning, and vehicle integration. We demonstrate that, despite significant reduction in model size which removes up to 2 billion parameters from the original model, our approach preserves the model's ability to handle complex in-vehicle tasks accurately and efficiently. Furthermore, by executing the model in a lightweight runtime environment, we achieve a generation speed of 11 tokens per second, making real-time, on-device inference feasible without hardware acceleration. Our results demonstrate the potential of SLMs to transform vehicle control systems, enabling more intuitive interactions between users and their vehicles for an enhanced driving experience. 

**Abstract (ZH)**: 我们提出了一种全面的方法，将小型语言模型（SLMs）作为函数调用代理部署在车辆中的边缘设备上，作为一种比传统基于规则的系统更具灵活性和鲁棒性的替代方案。通过利用SLMs，我们简化了车辆控制机制并提升了用户体验。考虑到车内硬件的限制，我们运用了最新的模型压缩技术，包括结构化剪枝、恢复和量化，以确保模型能够在资源限制内运行并保持可接受的性能。我们的工作集中在优化一个代表性的SLM——Microsoft的Phi-3 mini，并概述了使嵌入式模型能够工作的最佳实践，包括压缩、任务特定微调和车辆集成。我们证明，尽管模型大小显著减少，原始模型中的多达2亿个参数被移除，但我们的方法仍然能够确保该模型能够准确高效地处理复杂的车内任务。此外，通过在轻量级运行时环境中执行该模型，我们实现了每秒生成11个标记的速度，使得实时、本地设备推理成为可能，无需硬件加速。我们的结果表明了SLMs在转变车辆控制系统方面的潜力，为用户与车辆之间提供了更加直观的交互，从而增强了驾驶体验。 

---
# Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4 

**Title (ZH)**: 考查超参数调整对GPT-4同质性偏差稳健性的影响 

**Authors**: Messi H.J. Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.02211)  

**Abstract**: Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions. 

**Abstract (ZH)**: 大规模人类生成数据训练的视觉-语言模型往往会重现和放大社会刻板印象。这些模型中的一种关键刻板印象形式是同质性偏见——即倾向于将某些群体描绘得比其他群体更为同质。我们研究了这种偏见在GPT-4中的反应，特别考察了采样温度和top p等超参数对模型输出随机性的影响。通过生成来自不同种族和性别群体的个体故事，并使用向量表示来比较它们的相似性，我们评估了偏见的稳健性及其与超参数值的关系。我们发现：（1）同质性偏见在大多数超参数配置中普遍存在，非裔美国人和女性比美国白人和男性更同质地被描绘，（2）超参数与群体表示之间的关系显示出非线性的模式，尤其是在极端值处尤为明显，（3）超参数调整对种族和性别同质性偏见的影响不同——增加温度或减少top p可以降低种族同质性偏见，而这些变化对性别同质性偏见的影响则表现出不同的效果。我们的研究结果表明，虽然超参数调优可以在一定程度上缓解某些偏见，但它不能作为解决不同社会群体维度中同质性偏见的通用解决方案。 

---
# Benchmark Evaluations, Applications, and Challenges of Large Vision Language Models: A Survey 

**Title (ZH)**: 大规模视觉语言模型的基准评估、应用与挑战：一项综述 

**Authors**: Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, Guangyao Shi  

**Link**: [PDF](https://arxiv.org/pdf/2501.02189)  

**Abstract**: Multimodal Vision Language Models (VLMs) have emerged as a transformative technology at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification. Despite their rapid advancements in research and growing popularity in applications, a comprehensive survey of existing studies on VLMs is notably lacking, particularly for researchers aiming to leverage VLMs in their specific domains. To this end, we provide a systematic overview of VLMs in the following aspects: model information of the major VLMs developed over the past five years (2019-2024); the main architectures and training methods of these VLMs; summary and categorization of the popular benchmarks and evaluation metrics of VLMs; the applications of VLMs including embodied agents, robotics, and video generation; the challenges and issues faced by current VLMs such as hallucination, fairness, and safety. Detailed collections including papers and model repository links are listed in this https URL. 

**Abstract (ZH)**: 多模态视觉语言模型（VLMs）已成为计算机视觉与自然语言处理交叉领域的一项变革性技术，使机器能够通过视觉和文本两种模态感知和推理世界。例如，如CLIP、Claude和GPT-4V等模型在视觉和文本数据上的推理和理解能力表现出色，并在零样本分类中击败了传统的单模态视觉模型。尽管这些模型在研究中的迅速发展及其在应用程序中的日益流行，但有关VLMs的现有研究综述仍然不足，特别是在旨在利用VLMs于特定领域的研究人员看来尤为重要。为此，本文从以下几个方面对VLMs进行了系统综述：过去五年（2019-2024）中开发的主要VLMs的模型信息；这些VLMs的主要架构和训练方法；VLMs常用基准和评估指标的概述与分类；VLMs的应用，包括具身智能体、机器人技术和视频生成；当前VLMs面临的主要挑战和问题，如幻觉、公平性和安全性。详细的资料集合以及论文和模型库链接列表可在以下网址查阅：[此链接]。 

---
# Table as Thought: Exploring Structured Thoughts in LLM Reasoning 

**Title (ZH)**: 《作为思考的表格：探索大语言模型推理中的结构化思考》 

**Authors**: Zhenjie Sun, Naihao Deng, Haofei Yu, Jiaxuan You  

**Link**: [PDF](https://arxiv.org/pdf/2501.02152)  

**Abstract**: Large language models' reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition. 

**Abstract (ZH)**: 大型语言模型的推理能力可以从组织其思维过程的方法中受益，例如链式推理提示，这种方法通过逐步的顺序结构来引导推理过程。然而，现有的方法主要集中在组织思维的顺序上，对单个思维步骤中的结构探索不足。为了解决这一问题，我们提出了一个受认知神经科学关于人类思维理论启发的框架——“思维作为表格”（Table as Thought）。Table as Thought 将推理组织在一个表格模式中，行代表顺序的思维步骤，列则捕捉关键约束和上下文信息以增强推理。推理过程通过迭代填充表格，直到自我验证确保推理的完整性和正确性。我们的实验显示，Table as Thought 在计划任务上表现出色，并且在数学推理方面显示出增强大型语言模型（LLM）性能的强大潜力，相较于无结构思维基准方法。本研究为在大型语言模型中精炼思维表示提供了新的探索，为推理和人工智能认知的发展铺平了道路。 

---
# METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring 

**Title (ZH)**: METAGENE-1：用于流行病监测的宏基因组基础模型 

**Authors**: Ollie Liu, Sami Jaghouar, Johannes Hagemann, Shangshang Wang, Jason Wiemels, Jeff Kaufman, Willie Neiswanger  

**Link**: [PDF](https://arxiv.org/pdf/2501.02045)  

**Abstract**: We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats. 

**Abstract (ZH)**: 我们预训练了METAGENE-1，这是一个包含70亿参数的自回归变压器模型，我们将它称为宏基因组基础模型。我们使用了一个包含多种宏基因组DNA和RNA序列的新颖数据集，该数据集包含超过1.5万亿个碱基对。这些序列来源于大规模的人类下水道水样本，并通过深度宏基因组（下一代）测序方法进行处理和测序。与专注于单个基因组或特定物种的基因组模型不同，METAGENE-1的目标是捕捉下水道水中存在的整个基因组信息分布，以辅助与疫情监测和病原体检测相关的任务。我们对数据集进行了字节对编码（BPE）分词，特别针对宏基因组序列进行了调整，并进行了预训练。在本文中，我们首先详细介绍了预训练数据集、分词策略和模型架构，突出了能够有效建模宏基因组数据的考虑因素和设计选择。然后，我们展示了在宏基因组数据集上预训练该模型的结果，提供了关于损失函数、系统指标以及预训练过程中训练稳定性的详细信息。最后，我们展示了METAGENE-1的性能，该模型在一系列基因组基准测试以及针对人类病原体检测和基因组序列嵌入的新评估中取得了最先进的结果，凸显了它在公共卫生应用中的潜力，特别是在疫情监测、生物监控和早期检测新兴健康威胁方面的应用。 

---
# Is Your Image a Good Storyteller? 

**Title (ZH)**: 你的图像能成为一个优秀的叙述者吗？ 

**Authors**: Xiujie Song, Xiaoyi Pang, Haifeng Tang, Mengyue Wu, Kenny Q. Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2501.01982)  

**Abstract**: Quantifying image complexity at the entity level is straightforward, but the assessment of semantic complexity has been largely overlooked. In fact, there are differences in semantic complexity across images. Images with richer semantics can tell vivid and engaging stories and offer a wide range of application scenarios. For example, the Cookie Theft picture is such a kind of image and is widely used to assess human language and cognitive abilities due to its higher semantic complexity. Additionally, semantically rich images can benefit the development of vision models, as images with limited semantics are becoming less challenging for them. However, such images are scarce, highlighting the need for a greater number of them. For instance, there is a need for more images like Cookie Theft to cater to people from different cultural backgrounds and eras. Assessing semantic complexity requires human experts and empirical evidence. Automatic evaluation of how semantically rich an image will be the first step of mining or generating more images with rich semantics, and benefit human cognitive assessment, Artificial Intelligence, and various other applications. In response, we propose the Image Semantic Assessment (ISA) task to address this problem. We introduce the first ISA dataset and a novel method that leverages language to solve this vision problem. Experiments on our dataset demonstrate the effectiveness of our approach. Our data and code are available at: this https URL. 

**Abstract (ZH)**: 在实体层面量化图像复杂性是简单的，但对语义复杂性的评估却常常被忽视。实际上，不同图像之间的语义复杂性存在差异。具有更丰富语义的图像能够讲述生动有趣的故事，并提供广泛的应用场景。例如，《偷饼干》这幅图片就具有较高的语义复杂性，广泛用于评估人类语言能力和认知能力。此外，富含语义的图像有助于视觉模型的发展，因为具有有限语义的图像对它们来说越来越不具有挑战性。然而，这样的图像稀缺，强调了需要提供更多此类图像的必要性。例如，需要更多类似《偷饼干》的图像来适应来自不同文化背景和时代的人员。评估语义复杂性需要人类专家和实证证据。自动评估图像的语义丰富性是挖掘或生成更多富含语义图像的第一步，并有利于人类认知评估、人工智能以及各种其他应用。为此，我们提出了图像语义评估（ISA）任务以解决这一问题。我们首次引入了ISA数据集，并提出了一种基于语言的新方法来解决这一视觉问题。我们的实验表明，这种方法的有效性。我们的数据和代码可在以下链接获取：this https URL。 

---
