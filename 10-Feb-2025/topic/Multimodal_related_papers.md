# Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation 

**Title (ZH)**: 利用大语言模型转型科学：人工智能辅助科学发现、实验、内容生成与评估综述 

**Authors**: Steffen Eger, Yong Cao, Jennifer D'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, Tristan Miller  

**Link**: [PDF](https://arxiv.org/pdf/2502.05151)  

**Abstract**: With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science". 

**Abstract (ZH)**: 随着大型多模态语言模型的出现，科学正处于基于人工智能的技术转型的门槛上。最近，提出了大量的新型人工智能模型和工具，承诺能够使全球的研究人员和学术界成员更有效地开展研究。这包括研究周期的所有方面，特别是（1）查找相关文献；（2）生成研究思路并进行实验；生成（3）基于文本和（4）多模态内容（例如，科学图表和图形）；以及（5）基于人工智能的自动同行评审。在这篇综述中，我们对这些令人兴奋的最新发展进行了深入概述，这些发展有望从根本上改变科学研究过程。我们的综述涵盖了上述五个方面，列出了相关数据集、方法和结果（包括评估），以及局限性和未来研究的范围。关于这些工具的缺点以及潜在滥用（如虚假科学、剽窃、损害研究诚信）的伦理问题，在我们的讨论中占据了特别突出的位置。我们希望这篇综述不仅能成为该领域的入门者的参考指南，还能成为“AI4Science”领域新的人工智能驱动项目的催化剂。 

---
# CodeSCM: Causal Analysis for Multi-Modal Code Generation 

**Title (ZH)**: CodeSCM：多模态代码生成的因果分析 

**Authors**: Mukur Gupta, Noopur Bhatt, Suman Jana  

**Link**: [PDF](https://arxiv.org/pdf/2502.05150)  

**Abstract**: In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model's spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly influence code generation. 

**Abstract (ZH)**: 在本文中，我们提出了一种名为CodeSCM的结构因果模型（Structural Causal Model, SCM），用于利用大规模语言模型（Large Language Models, LLMs）分析多模态代码生成。通过在CodeSCM中应用干预措施，我们可以衡量不同提示模态（如自然语言、代码和输入输出示例）对模型的因果影响。CodeSCM引入了潜在中介变量，用于区分多模态代码生成提示中的代码和自然语言语义。基于因果中介分析的原则，我们定量度量了直接效应，这些效应代表了模型的偏差倾向。我们发现，除了自然语言指令外，输入输出示例也显著影响代码生成。 

---
# EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models 

**Title (ZH)**: EmoBench-M：多模态大型语言模型情感 intelligence 评估基准 

**Authors**: He Hu, Yucheng Zhou, Lianzhong You, Hongbo Xu, Qianning Wang, Zheng Lian, Fei Richard Yu, Fei Ma, Laizhong Cui  

**Link**: [PDF](https://arxiv.org/pdf/2502.04424)  

**Abstract**: With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at this https URL. 

**Abstract (ZH)**: 随着多模态大型语言模型（MLLMs）在机器人系统和各种AI应用中的集成，将情感 Intelligence（EI）能力嵌入这些模型成为了必要，以使机器人能有效应对人类的情感需求，并在现实世界场景中无缝交互。现有的静态、基于文本或图文的基准测试忽略了真实世界交互的多模态复杂性，未能捕捉到情感表达的动态和多模态特性，使它们在评估MLLMs的情感智能方面显得不足。基于已有的心理学情感智能理论，我们构建了EmoBench-M这一新型基准，旨在从三个关键维度评估MLLMs的情感智能能力，即基础情感识别、对话情感理解以及社会复杂情感分析，跨越13种评价场景。对开源和封闭源MLLMs在EmoBench-M上的评估揭示了它们与人类在性能上的显著差距，突显了进一步提升其情感智能能力的需求。所有基准资源，包括代码和数据集，均可在这里公开访问：[填入网址]。 

---
# Multimodal Medical Code Tokenizer 

**Title (ZH)**: 多模态医疗代码分词器 

**Authors**: Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik  

**Link**: [PDF](https://arxiv.org/pdf/2502.04397)  

**Abstract**: Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models. 

**Abstract (ZH)**: 基于患者电子健康记录（EHR）训练的基石模型需要将医疗数据 tokenize 成离散词汇项的序列。现有 tokenizer 将 EHR 中的医疗代码视为孤立的文本令牌。然而，每个医疗代码由其文本描述、其在本体层次结构中的位置及其与其他代码的关系（如疾病共现和药物-治疗关联）定义。医疗词汇表包含超过 60 万个代码，对于临床推理至关重要。我们提出了 MedTok，这是一种多模态医疗代码 tokenizer，它利用代码的文本描述及其关系上下文。MedTok 使用语言模型编码器处理文本，并使用图编码器编码关系结构。之后，它将两种模态量化到一个统一的 token 空间中，保留模态特定和跨模态信息。我们将 MedTok 集成到五个 EHR 模型中，并在入院和门诊数据集上评估其在临床任务中的表现，包括结果预测、诊断分类、药物推荐和风险分层。用 MedTok 替换标准的 EHR tokenizer 后，所有 EHR 模型的 AUPRC 均有所提高，在 MIMIC-III 数据集上提高了 4.10%，在 MIMIC-IV 数据集上提高了 4.78%，在 EHRShot 数据集上提高了 11.30%，药物推荐任务上的改善最大。除了 EHR 模型之外，我们还展示了使用 MedTok tokenizer 的医疗问答系统。我们的结果显示，MedTok 作为一种统一的医疗代码 tokenizer 具有潜力，能够改善医疗基础模型的 tokenization。 

---
# Exploring Spatial Language Grounding Through Referring Expressions 

**Title (ZH)**: 通过指示短语探索空间语言定位 

**Authors**: Akshar Tumu, Parisa Kordjamshidi  

**Link**: [PDF](https://arxiv.org/pdf/2502.04359)  

**Abstract**: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions. 

**Abstract (ZH)**: 空间推理是人类认知的重要组成部分，也是当前最先进的视觉语言模型（VLMs）表现出困难的一个领域。当前的研究主要使用图像字幕任务和视觉问答任务。在此工作中，我们提出使用参照表达理解任务作为评估VLMs空间推理能力的平台。此平台提供了更深入分析空间理解和空间定位能力的机会，尤其是在1）物体检测存在歧义时，2）有复杂的空间表达、较长的句子结构和多个空间关系时，以及3）包含否定（"not"）的表达中。在我们的分析中，我们使用了任务特定的架构以及大型VLMs，并强调了它们在处理这些特定情况时的优势和不足之处。尽管所有这些模型在当前任务上都面临挑战，但具体的行为取决于底层模型和特定的空间语义类别（例如，拓扑、方向性、接近性等）。我们的结果显示了这些挑战和行为，并为研究缺口和未来方向提供了见解。 

---
# Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs 

**Title (ZH)**: 迷失在时间之中：多模态大语言模型中的时钟和日历理解挑战 

**Authors**: Rohit Saxena, Aryo Pradipta Gema, Pasquale Minervini  

**Link**: [PDF](https://arxiv.org/pdf/2502.05092)  

**Abstract**: Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) $\textit{ClockQA}$, which comprises various types of clock styles$-$standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related questions; and 2) $\textit{CalendarQA}$, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs. 

**Abstract (ZH)**: 从视觉表示理解时间是一种基本的认知技能，但对多模态大型语言模型（MLLMs）而言仍然是一个挑战。在本文中，我们研究了MLLMs在通过模拟时钟和年度日历来解释时间与日期的能力。为了促进这项研究，我们策划了一个结构化数据集，包含两个子集：1）$\textit{ClockQA}$，其中包括各种类型的时钟样式——标准时钟、黑色表盘时钟、无秒针时钟、罗马数字时钟和指针时钟——并配有关于时间的相关问题；2）$\textit{CalendarQA}$，其中包括年度日历图像，问题范围从广为人知的日期（例如圣诞节、新年）到通过计算得出的日期（例如一年中的第100天或第153天）。我们旨在分析当MLLMs面对与时间相关视觉数据时，其在视觉识别、数值推理和时间推断方面的表现能力。我们的评估表明，尽管最近的发展取得了很大进步，但可靠地理解时间仍然是MLLMs的一大挑战。 

---
# ELITE: Enhanced Language-Image Toxicity Evaluation for Safety 

**Title (ZH)**: ELITE: 增强的语言-图像毒性评估以确保安全 

**Authors**: Wonjun Lee, Doehyeon Lee, Eugene Choi, Sangyoon Yu, Ashkan Yousefpour, Haon Park, Bumsub Ham, Suhyun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2502.04757)  

**Abstract**: Current Vision Language Models (VLMs) remain vulnerable to malicious prompts that induce harmful outputs. Existing safety benchmarks for VLMs primarily rely on automated evaluation methods, but these methods struggle to detect implicit harmful content or produce inaccurate evaluations. Therefore, we found that existing benchmarks have low levels of harmfulness, ambiguous data, and limited diversity in image-text pair combinations. To address these issues, we propose the ELITE {\em benchmark}, a high-quality safety evaluation benchmark for VLMs, underpinned by our enhanced evaluation method, the ELITE {\em evaluator}. The ELITE evaluator explicitly incorporates a toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide specific, convincing, but unharmful descriptions of images. We filter out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generate diverse combinations of safe and unsafe image-text pairs. Our experiments demonstrate that the ELITE evaluator achieves superior alignment with human evaluations compared to prior automated methods, and the ELITE benchmark offers enhanced benchmark quality and diversity. By introducing ELITE, we pave the way for safer, more robust VLMs, contributing essential tools for evaluating and mitigating safety risks in real-world applications. 

**Abstract (ZH)**: 当前的视觉语言模型（VLMs）仍然容易受到恶意提示的影响，这些提示可能导致有害输出。现有的VLM安全性基准主要依赖于自动评估方法，但这些方法在检测隐含有害内容或产生不准确评估方面存在困难。因此，我们发现现有的基准在有害性、数据模糊性以及图像-文本配对组合的多样性方面存在不足。为解决这些问题，我们提出了 ELITE 基准，这是一个高质量的VLM安全性评估基准，基于我们改进的评估方法——ELITE 评估器。ELITE 评估器明确地引入了毒性评分，以准确评估多模态环境中VLMs提供的特定、令人信服但无害的图像描述。我们使用ELITE评估器筛选掉现有基准中的模糊和低质量的图像-文本配对，并生成多样化的安全与不安全图像-文本配对组合。我们的实验表明，ELITE评估器在人类评估方面的对齐程度优于先前的自动评估方法，且ELITE基准提供更高的基准质量和多样性。通过引入ELITE，我们为我们提供了一条通往更安全、更稳健的VLM的道路，从而为评估和缓解实际应用中的安全风险提供了必要的工具。 

---
# Can Large Language Models Capture Video Game Engagement? 

**Title (ZH)**: 大型语言模型能否捕捉到视频游戏的参与度？ 

**Authors**: David Melhart, Matthew Barthet, Georgios N. Yannakakis  

**Link**: [PDF](https://arxiv.org/pdf/2502.04379)  

**Abstract**: Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs. 

**Abstract (ZH)**: 当观察视频时，预训练的大语言模型（LLMs）能否成功检测到人类情感？为回答这一问题，我们首次全面评估了多模态环境下，受文本和视频帧序列提示时，流行的大语言模型（LLMs）在标注和成功预测视频连续情感注释方面的能力。特别地，在本文中，我们测试了LLMs在20款电子游戏（GameVibe语料库）第一人称射击游戏片段中的80分钟内，在游戏参与度变化方面的正确标签能力。我们进行了超过2,400次实验，以探索LLM架构、模型大小、输入模态、提示策略和真实标签处理方法对参与度预测的影响。我们的研究结果表明，尽管LLMs在多个领域展示出了类人的性能，但它们在捕捉由人类提供的连续体验标注方面的表现通常不如人意。我们探讨了一些导致整体性能相对较差的根本原因，指出了LLMs超出预期的案例，并勾画出通过LLMs进一步探索自动化情感标注的道路。 

---
# PerPO: Perceptual Preference Optimization via Discriminative Rewarding 

**Title (ZH)**: PerPO：基于辨别奖励的感知偏好优化 

**Authors**: Zining Zhu, Liang Zhao, Kangheng Lin, Jinze Yang, En Yu, Chenglong Liu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.04371)  

**Abstract**: This paper presents Perceptual Preference Optimization (PerPO), a perception alignment method aimed at addressing the visual discrimination challenges in generative pre-trained multimodal large language models (MLLMs). To align MLLMs with human visual perception process, PerPO employs discriminative rewarding to gather diverse negative samples, followed by listwise preference optimization to rank this http URL utilizing the reward as a quantitative margin for ranking, our method effectively bridges generative preference optimization and discriminative empirical risk minimization. PerPO significantly enhances MLLMs' visual discrimination capabilities while maintaining their generative strengths, mitigates image-unconditional reward hacking, and ensures consistent performance across visual tasks. This work marks a crucial step towards more perceptually aligned and versatile MLLMs. We also hope that PerPO will encourage the community to rethink MLLM alignment strategies. 

**Abstract (ZH)**: 本文介绍了知觉偏好优化（PerPO），这是一种旨在解决生成预训练多模态大型语言模型（MLLMs）中的视觉识别挑战的知觉对齐方法。为了将MLLMs与人类的视觉感知过程对齐，PerPO 使用区分性奖励来收集多样化的负面样本，随后通过列表级偏好优化对其进行排序。通过将生成性偏好优化与区分性经验风险最小化结合，我们的方法有效地将两者联系起来。PerPO 显著增强了MLLMs的视觉识别能力，同时保持了其生成能力，克服了图像无关奖励劫持的问题，并确保在视觉任务中的表现一致性。这项工作标志着向更具知觉对齐性和多功能性的MLLMs迈出了一步。我们还希望PerPO 能够促使研究社区重新思考MLLMs的对齐策略。 

---
# Getting More Juice Out of Your Data: Hard Pair Refinement Enhances Visual-Language Models Without Extra Data 

**Title (ZH)**: 充分利用你的数据：难配对精炼提升视觉语言模型而无需额外数据 

**Authors**: Haonan Wang, Minbin Huang, Runhui Huang, Lanqing Hong, Hang Xu, Tianyang Hu, Xiaodan Liang, Zhenguo Li, Hong Cheng, Kenji Kawaguchi  

**Link**: [PDF](https://arxiv.org/pdf/2305.05208)  

**Abstract**: Contrastive Language-Image Pre-training (CLIP) has become the standard for cross-modal image-text representation learning. Improving CLIP typically requires additional data and retraining with new loss functions, but these demands raise resource and time costs, limiting practical use. In this work, we introduce HELIP, a cost-effective strategy that improves CLIP models by exploiting challenging text-image pairs within existing datasets in continuous training. This eliminates the need for additional data or extensive retraining. Moreover, HELIP integrates effortlessly into current training pipelines with minimal code modifications, allowing for quick and seamless implementation. On comprehensive benchmarks, HELIP consistently boosts existing models. In particular, within just two epochs of training, it improves zero-shot classification accuracy on ImageNet for SLIP models pre-trained on CC3M, CC12M, and YFCC15M datasets by 3.05%, 4.47%, and 10.1% , respectively. In addition, on fine-grained classification datasets, HELIP improves the zero-shot performance of CLIP and SLIP by an average of 8.4% and 18.6%, and their linear probe performance by an average of 9.5% and 3.0%. The code is publicly available at: this https URL. 

**Abstract (ZH)**: 对比语言-图像预训练（CLIP）已成为跨模态图像-文本表示学习的标准。改进CLIP通常需要更多的数据和使用新的损失函数进行重新训练，但这些需求增加了资源和时间成本，限制了其实际应用。在本研究中，我们引入了HELIP策略，这是一种成本效益高的方法，通过在现有数据集中利用具有挑战性的文本-图像对进行连续训练来提升CLIP模型。这种方法消除了对额外数据或大量重新训练的需求。此外，HELIP可以轻松集成到当前的训练管道中，只需要少量的代码修改，从而实现快速且无缝的实施。在全面的基准测试中，HELIP始终能够提升现有模型的性能。特别是在仅仅两个训练周期后，HELIP分别将SLIP模型在CC3M、CC12M和YFCC15M数据集上预训练的零样本分类准确率提升了3.05%、4.47%和10.1%。此外，在细粒度分类数据集上，HELIP分别将CLIP和SLIP的零样本表现提升了8.4%和18.6%，以及线性探针性能提升了9.5%和3.0%。代码已在以下网址公开：这个链接。 

---
# MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin 

**Title (ZH)**: MedMimic：医师启发的多模态融合方法用于不明原因发热的早期诊断 

**Authors**: Minrui Chen, Yi Zhou, Huidong Jiang, Yuhan Zhu, Guanjie Zou, Minqi Chen, Rong Tian, Hiroto Saigo  

**Link**: [PDF](https://arxiv.org/pdf/2502.04794)  

**Abstract**: Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is introduced as a multimodal framework inspired by real-world diagnostic processes. It uses pretrained models such as DINOv2, Vision Transformer, and ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into low-dimensional, semantically meaningful features. A learnable self-attention-based fusion network then integrates these imaging features with clinical data for classification. Using 416 FUO patient cases from Sichuan University West China Hospital from 2017 to 2023, the multimodal fusion classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to 0.9291 across seven tasks, outperforming conventional machine learning and single-modality deep learning methods. Ablation studies and five-fold cross-validation further validated its effectiveness. By combining the strengths of pretrained large models and deep learning, MedMimic offers a promising solution for disease classification. 

**Abstract (ZH)**: 未知原因发热（Fever of Unknown Origin，FUO）仍然是一个诊断上的挑战。MedMimic 是一种受现实诊断过程启发的多模态框架。该框架利用预训练模型如 DINOv2、Vision Transformer 和 ResNet-18 将高维的 18F-FDG PET/CT 图像转换为低维且具语义意义的特征。然后，一个可学习的自注意力融合网络将这些影像特征与临床数据集成用于分类。该研究使用了从 2017 年到 2023 年四川大学华西医院的 416 例 FUO 患者病例，多模态融合分类网络 MFCN 在七个任务中实现了从 0.8654 到 0.9291 的宏 AUC-ROC 分数，优于传统的机器学习方法和单一模态的深度学习方法。消融研究和五折交叉验证进一步验证了其有效性。通过结合预训练大模型和深度学习的优势，MedMimic 提供了一个有前景的疾病分类解决方案。 

---
# CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements 

**Title (ZH)**: CAMEF：通过整合时间序列模式和重要宏观经济公告来进行因果增强的多模态事件驱动财务预测 

**Authors**: Yang Zhang, Wenbo Yang, Jun Wang, Qiang Ma, Jie Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2502.04592)  

**Abstract**: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types. 

**Abstract (ZH)**: 准确预测宏观经济事件的影响对于投资者和政策制定者至关重要。像货币政策决定和就业报告这样的明显事件常常通过影响经济增长和风险预期来引发市场波动，从而建立了事件与市场行为之间的因果关系。现有的预测方法通常侧重于文本分析或时间序列建模，但无法捕捉金融市场的多模态性质以及事件与价格变动之间的因果关系。为解决这些不足，我们提出了一种名为CAMEF（因果增强多模态事件驱动金融预测）的多模态框架，该框架有效结合了文本和时间序列数据，并且通过因果学习机制和基于大规模语言模型（LLM）的反事实事件增强技术，增强了因果关系下的金融预测能力。我们的贡献包括：

1. 一个多模态框架，能够捕捉政策文本与历史价格数据之间的因果关系；
2. 一个全新的金融数据集，包含从2008年到2024年4月的六种类型的宏观经济发布数据，以及五个主要美国金融资产的高频实际交易数据；
3. 一种基于LLM的反事实事件增强策略。我们将CAMEF与最先进的基于变压器的时间序列和多模态基线进行比较，并通过消融研究验证因果学习机制和事件类型的有效性。 

---
# Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture 

**Title (ZH)**: 探索视觉-语言-行动模型在符号状态表示中的潜力，并将其集成到认知架构中 

**Authors**: Hong Lu, Hengxu Li, Prithviraj Singh Shahani, Stephanie Herbers, Matthias Scheutz  

**Link**: [PDF](https://arxiv.org/pdf/2502.04558)  

**Abstract**: Vision-language-action (VLA) models hold promise as generalist robotics solutions by translating visual and linguistic inputs into robot actions, yet they lack reliability due to their black-box nature and sensitivity to environmental changes. In contrast, cognitive architectures (CA) excel in symbolic reasoning and state monitoring but are constrained by rigid predefined execution. This work bridges these approaches by probing OpenVLA's hidden layers to uncover symbolic representations of object properties, relations, and action states, enabling integration with a CA for enhanced interpretability and robustness. Through experiments on LIBERO-spatial pick-and-place tasks, we analyze the encoding of symbolic states across different layers of OpenVLA's Llama backbone. Our probing results show consistently high accuracies (> 0.90) for both object and action states across most layers, though contrary to our hypotheses, we did not observe the expected pattern of object states being encoded earlier than action states. We demonstrate an integrated DIARC-OpenVLA system that leverages these symbolic representations for real-time state monitoring, laying the foundation for more interpretable and reliable robotic manipulation. 

**Abstract (ZH)**: 视觉-语言-动作（VLA）模型作为通用机器人解决方案展现了潜力，能够将视觉和语言输入转化为机器人动作，但由于其黑箱性质和对环境变化的敏感性，这些模型缺乏可靠性。相比之下，认知架构（CA）在符号推理和状态监控方面表现出色，但受限于预定义的刚性执行过程。本研究通过探究OpenVLA的隐藏层以揭示对象属性、关系和动作状态的符号表示，进而将其与CA结合，从而增强其解释性和鲁棒性。通过在LIBERO空间拣选放置任务上的实验，我们分析了OpenVLA Llama后端不同层中符号状态的编码情况。我们的探究结果显示，大多数层中对象状态和动作状态的编码准确性都非常高（>0.90），但与我们的假设相反，我们没有观察到对象状态比动作状态编码得更早的模式。我们展示了集成的DIARC-OpenVLA系统，该系统利用这些符号表示进行实时状态监控，为更可解释和可靠的机器人操作奠定了基础。 

---
# AnyPlace: Learning Generalized Object Placement for Robot Manipulation 

**Title (ZH)**: AnyPlace: 学习通用物体放置以实现机器人操作 

**Authors**: Yuchi Zhao, Miroslav Bogdanovic, Chengyuan Luo, Steven Tohme, Kourosh Darvish, Alán Aspuru-Guzik, Florian Shkurti, Animesh Garg  

**Link**: [PDF](https://arxiv.org/pdf/2502.04531)  

**Abstract**: Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. To address this, we propose AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify rough placement locations, we focus only on the relevant regions for local placement, which enables us to train the low-level placement-pose-prediction model to capture diverse placements efficiently. For training, we generate a fully synthetic dataset of randomly generated objects in different placement configurations (insertion, stacking, hanging) and train local placement-prediction models. We conduct extensive evaluations in simulation, demonstrating that our method outperforms baselines in terms of success rate, coverage of possible placement modes, and precision. In real-world experiments, we show how our approach directly transfers models trained purely on synthetic data to the real world, where it successfully performs placements in scenarios where other models struggle -- such as with varying object geometries, diverse placement modes, and achieving high precision for fine placement. More at: this https URL. 

**Abstract (ZH)**: 机器人任务中的物体放置固有地具有挑战性，因为物体几何形状和放置配置的多样性。为了解决这一问题，我们提出了一种名为AnyPlace的两阶段方法，该方法完全依赖合成数据进行训练，能够预测广泛种类的可行放置姿态。我们的核心见解在于，通过利用视觉-语言模型（VLM）识别粗略的放置位置，我们只关注与局部放置相关的区域，从而使得低级放置姿态预测模型能够高效地捕捉到多种不同的放置方式。

训练过程中，我们生成了一个完全合成的数据集，该数据集包含以不同放置配置（插入、堆叠、悬挂）生成的随机物体，并训练局部放置预测模型。我们进行了广泛的仿真评估，结果显示我们的方法在成功率、可能的放置模式覆盖范围以及精度方面超过了基线方法。在实际实验中，我们展示了如何将仅使用合成数据训练的模型直接应用于现实世界，其中我们的方法能够在具有多样化物体几何形状、多种放置模式及实现高精度精细放置的场景中成功执行放置任务，而其他模型则在这些场景中表现不佳。更多内容，请参见：[请提供链接]。 

---
# Color in Visual-Language Models: CLIP deficiencies 

**Title (ZH)**: 视觉语言模型中的颜色问题：CLIP的不足之处 

**Authors**: Guillem Arias, Ramon Baldrich, Maria Vanrell  

**Link**: [PDF](https://arxiv.org/pdf/2502.04470)  

**Abstract**: This work explores how color is encoded in CLIP (Contrastive Language-Image Pre-training) which is currently the most influential VML (Visual Language model) in Artificial Intelligence. After performing different experiments on synthetic datasets created for this task, we conclude that CLIP is able to attribute correct color labels to colored visual stimulus, but, we come across two main deficiencies: (a) a clear bias on achromatic stimuli that are poorly related to the color concept, thus white, gray and black are rarely assigned as color labels; and (b) the tendency to prioritize text over other visual information. Here we prove it is highly significant in color labelling through an exhaustive Stroop-effect test. With the aim to find the causes of these color deficiencies, we analyse the internal representation at the neuron level. We conclude that CLIP presents an important amount of neurons selective to text, specially in deepest layers of the network, and a smaller amount of multi-modal color neurons which could be the key of understanding the concept of color properly. Our investigation underscores the necessity of refining color representation mechanisms in neural networks to foster a more comprehensive comprehension of colors as humans understand them, thereby advancing the efficacy and versatility of multimodal models like CLIP in real-world scenarios. 

**Abstract (ZH)**: 本文探讨了CLIP（对比语言图像预训练）中颜色的编码方式，CLIP目前是人工智能领域最具影响力的视觉语言模型之一。通过在为此任务创建的不同合成数据集上进行各种实验，我们得出结论：CLIP 能够将正确的颜色标签分配给带颜色的视觉刺激。然而，我们发现了两个主要缺陷：（a）在与颜色概念关系不密切的无色彩刺激上表现出明显的偏好偏差，因此白色、灰色和黑色被罕见地赋予颜色标签；（b）倾向于优先考虑文本信息而非其他视觉信息。我们通过详尽的Stroop效应测试证明了这一点。为了寻找这些颜色缺陷的原因，我们从神经元层面分析了内部表示。我们得出结论：CLIP 在深层网络中表现出重要的对文本选择性的神经元数量，而多模态颜色神经元的数量较少，可能是正确理解颜色概念的关键。我们的研究强调了在神经网络中改进颜色表示机制的必要性，以促进对人类理解颜色的更全面理解，从而促进如CLIP等多模态模型在实际场景中的效果和灵活性。 

---
# No Images, No Problem: Retaining Knowledge in Continual VQA with Questions-Only Memory 

**Title (ZH)**: 没有图像，也不成问题：仅通过问题记忆保留连续视觉理解问答中的知识 

**Authors**: Imad Eddine Marouf, Enzo Tartaglione, Stephane Lathuiliere, Joost van de Weijer  

**Link**: [PDF](https://arxiv.org/pdf/2502.04469)  

**Abstract**: Continual Learning in Visual Question Answering (VQACL) requires models to learn new visual-linguistic tasks (plasticity) while retaining knowledge from previous tasks (stability). The multimodal nature of VQACL presents unique challenges, requiring models to balance stability across visual and textual domains while maintaining plasticity to adapt to novel objects and reasoning tasks. Existing methods, predominantly designed for unimodal tasks, often struggle to balance these demands effectively. In this work, we introduce QUestion-only replay with Attention Distillation (QUAD), a novel approach for VQACL that leverages only past task questions for regularisation, eliminating the need to store visual data and addressing both memory and privacy concerns. QUAD achieves stability by introducing a question-only replay mechanism that selectively uses questions from previous tasks to prevent overfitting to the current task's answer space, thereby mitigating the out-of-answer-set problem. Complementing this, we propose attention consistency distillation, which uniquely enforces both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that QUAD significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA. 

**Abstract (ZH)**: 视觉问答中的持续学习（VQACL）要求模型能够学习新的视觉-语言任务（可塑性），同时保留先前任务的知识（稳定性）。VQACL 的多模态性质提出了独特的挑战，要求模型在视觉和文本领域之间保持稳定性的同时，还要具备可塑性，以便适应新的对象和推理任务。现有的方法往往主要是为单模态任务设计的，难以有效平衡这些要求。在这项工作中，我们提出了QUestion-only replay with Attention Distillation (QUAD)，这是一种新颖的VQACL方法，仅利用过去的任务问题进行正则化，从而避免存储视觉数据和解决内存及隐私问题。QUAD 通过引入仅基于问题的重放机制来实现稳定性，该机制有选择地使用先前任务的问题以防止对当前任务答案空间的过拟合，从而缓解超出答案集的问题。为补充这一点，我们提出了注意一致性蒸馏，该方法在任务之间强制执行模态内和模态间注意的一致性，从而保留关键的视觉-语言关联。在VQAv2和NExT-QA上的广泛实验表明，QUAD 显著优于现有最佳方法，并在持续视觉问答中实现了稳健的性能。 

---
# Transforming Multimodal Models into Action Models for Radiotherapy 

**Title (ZH)**: 将多模态模型转化为放疗行动模型 

**Authors**: Matteo Ferrante, Alessandra Carosi, Rolando Maria D Angelillo, Nicola Toschi  

**Link**: [PDF](https://arxiv.org/pdf/2502.04408)  

**Abstract**: Radiotherapy is a crucial cancer treatment that demands precise planning to balance tumor eradication and preservation of healthy tissue. Traditional treatment planning (TP) is iterative, time-consuming, and reliant on human expertise, which can potentially introduce variability and inefficiency. We propose a novel framework to transform a large multimodal foundation model (MLM) into an action model for TP using a few-shot reinforcement learning (RL) approach. Our method leverages the MLM's extensive pre-existing knowledge of physics, radiation, and anatomy, enhancing it through a few-shot learning process. This allows the model to iteratively improve treatment plans using a Monte Carlo simulator. Our results demonstrate that this method outperforms conventional RL-based approaches in both quality and efficiency, achieving higher reward scores and more optimal dose distributions in simulations on prostate cancer data. This proof-of-concept suggests a promising direction for integrating advanced AI models into clinical workflows, potentially enhancing the speed, quality, and standardization of radiotherapy treatment planning. 

**Abstract (ZH)**: 放射治疗是癌症治疗中至关重要的一环，需要精确规划以平衡肿瘤消除和保留健康组织之间的关系。传统的治疗规划（TP）方法通常是迭代性的、耗时的，并依赖于人类的专长，这可能会引入变异性并降低效率。我们提出了一种新的框架，利用少量样本强化学习（RL）方法将一个大型多模态基础模型（MLM）转化为用于TP的动作模型。我们的方法利用了MLM在物理、辐射和解剖学方面的广泛先验知识，并通过少量样本学习过程对其进行增强。这使得模型能够利用蒙特卡洛模拟器逐步优化治疗计划。我们的结果表明，这种方法在质量和效率上都优于传统的基于RL的方法，在前列腺癌数据的模拟中实现了更高的奖励评分和更优的剂量分布。这一概念验证表明，将高级AI模型整合到临床工作流程中具有广阔的前景，有可能提高放射治疗计划的速度、质量和标准化程度。 

---
# Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks 

**Title (ZH)**: 适应性原型知识迁移：面向混合多模态和异构任务的联邦学习 

**Authors**: Keke Gai, Mohan Wang, Jing Yu, Dongjue Wang, Qi Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.04400)  

**Abstract**: Multimodal Federated Learning (MFL) enables multiple clients to collaboratively train models on multimodal data while ensuring clients' privacy. However, modality and task heterogeneity hinder clients from learning a unified representation, weakening local model generalization, especially in MFL with mixed modalities where only some clients have multimodal data. In this work, we propose an Adaptive prototype-based Multimodal Federated Learning (AproMFL) framework for mixed modalities and heterogeneous tasks to address the aforementioned issues. Our AproMFL transfers knowledge through adaptively-constructed prototypes without a prior public dataset. Clients adaptively select prototype construction methods in line with tasks; server converts client prototypes into unified multimodal prototypes and aggregates them to form global prototypes, avoid clients keeping unified labels. We divide the model into various modules and only aggregate mapping modules to reduce communication and computation overhead. To address aggregation issues in heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights. Extensive experiments on representative datasets evidence effectiveness of AproMFL. 

**Abstract (ZH)**: 多模态联邦学习（MFL）允许多个客户端在确保客户端隐私的前提下共同训练多模态数据上的模型。然而，模态和任务异质性阻碍了客户端学习统一表示，削弱了本地模型的泛化能力，特别是在混合模态的MFL中，只有部分客户端拥有混合模态数据。为了解决这些问题，本文提出了一个适用于混合模态和异质任务的自适应原型为基础的多模态联邦学习（AproMFL）框架。AproMFL通过自适应构建的原型来传递知识，而不需要先验的公共数据集。客户端根据任务自适应选择原型构建方法；服务器将客户端的原型转换为统一的多模态原型，并进行聚合形成全局原型，避免客户端保存统一的标签。我们将模型划分为多个模块，并仅聚合映射模块以降低通信和计算开销。为了解决异质性带来的聚合问题，我们开发了一种基于客户端关系图的方案，以动态调整聚合权重。在代表性数据集上的广泛实验验证了AproMFL的有效性。 

---
# Overcoming Vision Language Model Challenges in Diagram Understanding: A Proof-of-Concept with XML-Driven Large Language Models Solutions 

**Title (ZH)**: 克服视觉语言模型在图示理解中的挑战：基于XML驱动的大语言模型概念验证 

**Authors**: Shue Shiinoki, Ryo Koshihara, Hayato Motegi, Masumi Morishige  

**Link**: [PDF](https://arxiv.org/pdf/2502.04389)  

**Abstract**: Diagrams play a crucial role in visually conveying complex relationships and processes within business documentation. Despite recent advances in Vision-Language Models (VLMs) for various image understanding tasks, accurately identifying and extracting the structures and relationships depicted in diagrams continues to pose significant challenges. This study addresses these challenges by proposing a text-driven approach that bypasses reliance on VLMs' visual recognition capabilities. Instead, it utilizes the editable source files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines, annotations) are preserved as textual metadata. In our proof-of-concept, we extracted diagram information from xlsx-based system design documents and transformed the extracted shape data into textual input for Large Language Models (LLMs). This approach allowed the LLM to analyze relationships and generate responses to business-oriented questions without the bottleneck of image-based processing. Experimental comparisons with a VLM-based method demonstrated that the proposed text-driven framework yielded more accurate answers for questions requiring detailed comprehension of diagram this http URL results obtained in this study are not limited to the tested .xlsx files but can also be extended to diagrams in other documents with source files, such as Office pptx and docx formats. These findings highlight the feasibility of circumventing VLM constraints through direct textual extraction from original source files. By enabling robust diagram understanding through LLMs, our method offers a promising path toward enhanced workflow efficiency and information analysis in real-world business scenarios. 

**Abstract (ZH)**: 图表在业务文档中通过视觉形式传达复杂关系和过程方面发挥着至关重要的作用。尽管近年来在视觉语言模型（VLMs）方面取得了多种图像理解任务上的进展，但准确识别和提取图表中所示的结构和关系仍然面临重大挑战。本研究通过提出一种文本驱动的方法来应对这些挑战，该方法不依赖于VLMs的视觉识别能力，而是利用可编辑的源文件（例如xlsx、pptx或docx），其中图表元素（例如形状、线条、注释）作为文本元数据被保存。在我们的概念验证中，我们从基于xlsx的系统设计文档中提取图表信息，并将提取的形状数据转换为大型语言模型（LLMs）的文本输入。这种方法使LLMs能够在没有基于图像处理瓶颈的情况下分析关系并生成与业务相关的答案。与基于VLM的方法的实验比较表明，所提出的方法在需要深入理解图表的问题上提供了更为准确的答案，详见此链接。本研究的结果不仅适用于测试的xlsx文件，还可以扩展到其他具有源文件的文档中的图表，例如Office的pptx和docx格式。这些发现突显了利用原始源文件中的直接文本提取绕过VLM限制的可行性。通过利用LLMs实现稳健的图表理解，我们的方法为在真实业务场景中增强工作流程效率和信息分析提供了有前景的道路。 

---
# MapFusion: A Novel BEV Feature Fusion Network for Multi-modal Map Construction 

**Title (ZH)**: MapFusion：一种新型的多模态地图构建的BEV特征融合网络 

**Authors**: Xiaoshuai Hao, Yunfeng Diao, Mengchuan Wei, Yifan Yang, Peng Hao, Rong Yin, Hui Zhang, Weiming Li, Shu Zhao, Yu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.04377)  

**Abstract**: Map construction task plays a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. Primary sensors include cameras and LiDAR, with configurations varying between camera-only, LiDAR-only, or camera-LiDAR fusion, based on cost-performance considerations. While fusion-based methods typically perform best, existing approaches often neglect modality interaction and rely on simple fusion strategies, which suffer from the problems of misalignment and information loss. To address these issues, we propose MapFusion, a novel multi-modal Bird's-Eye View (BEV) feature fusion method for map construction. Specifically, to solve the semantic misalignment problem between camera and LiDAR BEV features, we introduce the Cross-modal Interaction Transform (CIT) module, enabling interaction between two BEV feature spaces and enhancing feature representation through a self-attention mechanism. Additionally, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities, which can take full advantage of the inherent information between different modalities. Moreover, MapFusion is designed to be simple and plug-and-play, easily integrated into existing pipelines. We evaluate MapFusion on two map construction tasks, including High-definition (HD) map and BEV map segmentation, to show its versatility and effectiveness. Compared with the state-of-the-art methods, MapFusion achieves 3.6% and 6.2% absolute improvements on the HD map construction and BEV map segmentation tasks on the nuScenes dataset, respectively, demonstrating the superiority of our approach. 

**Abstract (ZH)**: 地图构建任务在为自主驾驶系统提供精准且全面的静态环境信息方面发挥着关键作用。主要传感器包括相机和LiDAR，根据不同成本性能考虑，配置可以是单独使用相机、单独使用LiDAR，或者相机与LiDAR融合。尽管融合方法通常表现最佳，但现有方法往往忽略了不同模态之间的交互，并依赖于简单的融合策略，这会导致对齐错误和信息丢失。为了解决这些问题，我们提出了一种名为MapFusion的新型多模态鸟瞰图(BEV)特征融合方法，用于地图构建。具体而言，为了解决相机与LiDAR BEV特征之间的语义对齐问题，我们引入了跨模态交互变换（CIT）模块，该模块可以在两个BEV特征空间之间实现交互，并通过自注意力机制增强特征表示。此外，我们还提出了有效的双动态融合（DDF）模块，可以根据不同模态的特征选择有价值的信息，从而充分利用不同模态之间的内在信息。此外，MapFusion设计简洁且易于集成，可以轻松融入现有的管道中。我们在两个地图构建任务上评估了MapFusion，包括高精度（HD）地图构建和BEV地图分割，以展示其多样性和有效性。与最先进的方法相比，MapFusion在nuScenes数据集上的HD地图构建任务中实现了3.6%的绝对改进，在BEV地图分割任务中实现了6.2%的绝对改进，证明了我们方法的优越性。 

---
