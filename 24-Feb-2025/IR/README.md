# Dynamic Knowledge Selector and Evaluator for recommendation with Knowledge Graph 

**Title (ZH)**: 基于知识图谱的推荐系统中的动态知识选择器和评估器 

**Authors**: Feng Xia, Zhifei Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.15623)  

**Abstract**: In recent years recommendation systems typically employ the edge information provided by knowledge graphs combined with the advantages of high-order connectivity of graph networks in the recommendation field. However, this method is limited by the sparsity of labels, cannot learn the graph structure well, and a large number of noisy entities in the knowledge graph will affect the accuracy of the recommendation results. In order to alleviate the above problems, we propose a dynamic knowledge-selecting and evaluating method guided by collaborative signals to distill information in the knowledge graph. Specifically, we use a Chain Route Evaluator to evaluate the contributions of different neighborhoods for the recommendation task and employ a Knowledge Selector strategy to filter the less informative knowledge before evaluating. We conduct baseline model comparison and experimental ablation evaluations on three public datasets. The experiments demonstrate that our proposed model outperforms current state-of-the-art baseline models, and each modules effectiveness in our model is demonstrated through ablation experiments. 

**Abstract (ZH)**: 近年来，推荐系统通常利用知识图谱提供的边缘信息，并结合图网络在推荐领域中高阶连接的优势。然而，这种方法受限于标签稀疏性问题，难以很好地学习图结构，并且知识图谱中的大量噪声实体会影响推荐结果的准确性。为了缓解上述问题，我们提出了一种由协作信号引导的知识选择和评估动态方法，以提取知识图谱中的信息。具体而言，我们利用链路路径评估器（Chain Route Evaluator）评估不同邻域对推荐任务的贡献，并采用知识选择策略（Knowledge Selector strategy）在评估前过滤掉不相关信息。我们在三个公开数据集上进行了基准模型对比实验和消融实验评估。实验结果表明，我们提出的模型优于当前最先进的基准模型，并且通过消融实验证明了我们模型中每个模块的有效性。 

---
# Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware Maintenance Assistance 

**Title (ZH)**: 在 XR 中利用大语言模型进行跨格式检索增强生成，以实现上下文感知的维护辅助 

**Authors**: Akos Nagy, Yannis Spyridis, Vasileios Argyriou  

**Link**: [PDF](https://arxiv.org/pdf/2502.15604)  

**Abstract**: This paper presents a detailed evaluation of a Retrieval-Augmented Generation (RAG) system that integrates large language models (LLMs) to enhance information retrieval and instruction generation for maintenance personnel across diverse data formats. We assessed the performance of eight LLMs, emphasizing key metrics such as response speed and accuracy, which were quantified using BLEU and METEOR scores. Our findings reveal that advanced models like GPT-4 and GPT-4o-mini significantly outperform their counterparts, particularly when addressing complex queries requiring multi-format data integration. The results validate the system's ability to deliver timely and accurate responses, highlighting the potential of RAG frameworks to optimize maintenance operations. Future research will focus on refining retrieval techniques for these models and enhancing response generation, particularly for intricate scenarios, ultimately improving the system's practical applicability in dynamic real-world environments. 

**Abstract (ZH)**: 本文详细评估了一种检索增强生成（RAG）系统，该系统通过集成大规模语言模型（LLMs）来增强信息检索和指令生成能力，以满足不同数据格式下的维护人员需求。我们对八种LLMs进行了性能评估，重点考察了响应速度和准确性等关键指标，这些指标通过BLEU和METEOR评分进行了量化。研究结果表明，如GPT-4和GPT-4o-mini等先进模型，在处理需要多格式数据集成的复杂查询时，显著优于其他模型。研究结果证明了该系统的及时和准确响应能力，突显了RAG框架在优化维护操作方面的潜力。未来的研究将集中在改进这些模型的检索技术，并增强响应生成，特别是针对复杂的场景，最终提高系统在动态实际环境中的实际应用能力。 

---
# Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations 

**Title (ZH)**: pretrained 多模态模型与推荐系统之间的领域差距桥梁构建 

**Authors**: Wenyu Zhang, Jie Luo, Xinming Zhang, Yuan Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.15542)  

**Abstract**: With the explosive growth of multimodal content online, pre-trained visual-language models have shown great potential for multimodal recommendation. However, while these models achieve decent performance when applied in a frozen manner, surprisingly, due to significant domain gaps (e.g., feature distribution discrepancy and task objective misalignment) between pre-training and personalized recommendation, adopting a joint training approach instead leads to performance worse than baseline. Existing approaches either rely on simple feature extraction or require computationally expensive full model fine-tuning, struggling to balance effectiveness and efficiency. To tackle these challenges, we propose \textbf{P}arameter-efficient \textbf{T}uning for \textbf{M}ultimodal \textbf{Rec}ommendation (\textbf{PTMRec}), a novel framework that bridges the domain gap between pre-trained models and recommendation systems through a knowledge-guided dual-stage parameter-efficient training strategy. This framework not only eliminates the need for costly additional pre-training but also flexibly accommodates various parameter-efficient tuning methods. 

**Abstract (ZH)**: 随着在线多模态内容的爆炸性增长，预训练的视觉-语言模型在多模态推荐方面展现出了巨大潜力。然而，在以冻结状态应用时，这些模型虽然取得了不错的性能，但令人惊讶的是，由于预训练和个性化推荐之间的显著领域差异（如特征分布差异和任务目标不一致），采用联合训练方法反而导致了性能不如基准模型。现有方法要么依赖简单的特征提取，要么需要昂贵的全模型微调，难以在有效性与效率之间取得平衡。为了解决这些挑战，我们提出了一种新的框架，即**参数高效调整以用于多模态推荐**（PTMRec），该框架通过一种基于知识引导的双阶段参数高效训练策略，在预训练模型和推荐系统之间架起了桥梁。该框架不仅消除了昂贵的额外预训练需求，还灵活地兼容了各种参数高效调整方法。 

---
# Scaling Sparse and Dense Retrieval in Decoder-Only LLMs 

**Title (ZH)**: 在仅解码器大型语言模型中扩展稀疏检索和密集检索 

**Authors**: Hansi Zeng, Julian Killingback, Hamed Zamani  

**Link**: [PDF](https://arxiv.org/pdf/2502.15526)  

**Abstract**: Scaling large language models (LLMs) has shown great potential for improving retrieval model performance; however, previous studies have mainly focused on dense retrieval trained with contrastive loss (CL), neglecting the scaling behavior of other retrieval paradigms and optimization techniques, such as sparse retrieval and knowledge distillation (KD). In this work, we conduct a systematic comparative study on how different retrieval paradigms (sparse vs. dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect retrieval performance across different model scales. Using MSMARCO passages as the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a fixed compute budget, we evaluate various training configurations on both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where larger models achieve significant performance gains, whereas KD-trained models show minimal improvement, performing similarly across the 1B, 3B, and 8B scales. (2) Sparse retrieval models consistently outperform dense retrieval across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks, and they demonstrate greater robustness to imperfect supervised signals. (3) We successfully scale sparse retrieval models with the combination of CL and KD losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation sets. 

**Abstract (ZH)**: 将大型语言模型（LLMs）扩展显示出了提升检索模型性能的巨大潜力；然而，前期研究主要集中在使用对比损失（CL）训练的密集检索上，忽视了其他检索范式和优化技术（如稀疏检索和知识蒸馏-KD）的扩展行为。在本文中，我们系统地比较了不同检索范式（稀疏 vs. 密集）和微调目标（CL vs. KD vs. 其组合）对不同模型规模下检索性能的影响。使用MSMARCO段落作为训练数据集，仅包含解码器的LLM（Llama-3系列：1B、3B、8B），并使用固定计算预算，我们在领域内（MSMARCO、TREC DL）和领域外（BEIR）基准上评估了各种训练配置。我们的主要发现包括：（1）只有使用CL时，扩展行为才表现得十分明显，较大模型可以显著提高性能，而KD训练模型在1B、3B和8B规模下几乎没有改进，表现相似。（2）在领域内（MSMARCO、TREC DL）和领域外（BEIR）基准上，稀疏检索模型持续优于密集检索模型，并且对不完美的监督信号表现出更高的鲁棒性。（3）我们在8B规模下成功结合使用CL和KD损失进行了稀疏检索模型的扩展，实现了所有评估集上的最先进（SOTA）结果。 

---
# A Universal Framework for Compressing Embeddings in CTR Prediction 

**Title (ZH)**: 用于CTR预测中压缩嵌入表示的通用框架 

**Authors**: Kefan Wang, Hao Wang, Kenan Song, Wei Guo, Kai Cheng, Zhi Li, Yong Liu, Defu Lian, Enhong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.15355)  

**Abstract**: Accurate click-through rate (CTR) prediction is vital for online advertising and recommendation systems. Recent deep learning advancements have improved the ability to capture feature interactions and understand user interests. However, optimizing the embedding layer often remains overlooked. Embedding tables, which represent categorical and sequential features, can become excessively large, surpassing GPU memory limits and necessitating storage in CPU memory. This results in high memory consumption and increased latency due to frequent GPU-CPU data transfers. To tackle these challenges, we introduce a Model-agnostic Embedding Compression (MEC) framework that compresses embedding tables by quantizing pre-trained embeddings, without sacrificing recommendation quality. Our approach consists of two stages: first, we apply popularity-weighted regularization to balance code distribution between high- and low-frequency features. Then, we integrate a contrastive learning mechanism to ensure a uniform distribution of quantized codes, enhancing the distinctiveness of embeddings. Experiments on three datasets reveal that our method reduces memory usage by over 50x while maintaining or improving recommendation performance compared to existing models. The implementation code is accessible in our project repository this https URL. 

**Abstract (ZH)**: 准确的点击率（CTR）预测对在线广告和推荐系统至关重要。最近的深度学习进展提高了捕捉特征交互和理解用户兴趣的能力。然而，优化嵌入层常常被忽视。嵌入表用于表示分类和序列特征，可能会变得过大，超出GPU内存限制，从而需要存储在CPU内存中。这会导致高内存消耗和由于频繁的GPU-CPU数据传输而增加延迟。为了解决这些问题，我们提出了一种模型无关的嵌入压缩（MEC）框架，该框架通过量化预训练嵌入而不牺牲推荐质量来压缩嵌入表。我们的方法包括两个阶段：首先，我们应用流行度加权正则化来平衡高频和低频特征之间的码分布。然后，我们整合了一种对比学习机制，以确保量化码的均匀分布，增强嵌入的区分性。在三个数据集上的实验表明，与现有的模型相比，我们的方法在内存使用上减少了超过50倍，同时保持或提高了推荐性能。该项目的实现代码可在我们的项目仓库中访问：<https://github.com/...>。 

---
# Lightweight yet Efficient: An External Attentive Graph Convolutional Network with Positional Prompts for Sequential Recommendation 

**Title (ZH)**: 轻量且高效：基于位置提示的外部注意图卷积网络在序列推荐中的应用 

**Authors**: Jinyu Zhang, Chao Li, Zhongying Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.15331)  

**Abstract**: Graph-based Sequential Recommender systems (GSRs) have gained significant research attention due to their ability to simultaneously handle user-item interactions and sequential relationships between items. Current GSRs often utilize composite or in-depth structures for graph encoding (e.g., the Graph Transformer). Nevertheless, they have high computational complexity, hindering the deployment on resource-constrained edge devices. Moreover, the relative position encoding in Graph Transformer has difficulty in considering the complicated positional dependencies within sequence. To this end, we propose an External Attentive Graph convolutional network with Positional prompts for Sequential recommendation, namely EA-GPS. Specifically, we first introduce an external attentive graph convolutional network that linearly measures the global associations among nodes via two external memory units. Then, we present a positional prompt-based decoder that explicitly treats the absolute item positions as external prompts. By introducing length-adaptive sequential masking and a soft attention network, such a decoder facilitates the model to capture the long-term positional dependencies and contextual relationships within sequences. Extensive experimental results on five real-world datasets demonstrate that the proposed EA-GPS outperforms the state-of-the-art methods. Remarkably, it achieves the superior performance while maintaining a smaller parameter size and lower training overhead. The implementation of this work is publicly available at this https URL. 

**Abstract (ZH)**: 基于图的序列推荐系统（GSRs）由于其同时处理用户-项目交互和项目顺序关系的能力，已经引起了广泛的研究兴趣。当前的GSRs经常采用复合或复杂的图形编码结构（如图变换器）。然而，这些方法具有较高的计算复杂性，阻碍了其在资源受限的边缘设备上的部署。此外，图变换器中的相对位置编码在考虑序列内的复杂位置依赖关系时存在困难。为解决这些问题，我们提出了一种基于外部注意力图卷积网络和位置提示的序列推荐方法，即EA-GPS。具体而言，我们首先引入了一个外部注意力图卷积网络，通过两个外部记忆单元线性度量节点之间的全局关联。然后，我们提出了一种基于位置提示的解码器，明确地将绝对项目位置作为外部提示。通过引入长度自适应序列掩码和软注意力网络，该解码器有助于模型捕获序列内的长期位置依赖关系和上下文关系。在五个实际数据集上的广泛实验结果表明，所提出的EA-GPS在性能上优于现有最先进的方法，同时保持更小的参数量和更低的训练开销。该项目的实现可通过以下网址获得：[请填入具体的URL]。 

---
# From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants 

**Title (ZH)**: 从文档到对话：构建增强知识图谱-RAG的AI助理 

**Authors**: Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, Tung Mai  

**Link**: [PDF](https://arxiv.org/pdf/2502.15237)  

**Abstract**: The Adobe Experience Platform AI Assistant is a conversational tool that enables organizations to interact seamlessly with proprietary enterprise data through a chatbot. However, due to access restrictions, Large Language Models (LLMs) cannot retrieve these internal documents, limiting their ability to generate accurate zero-shot responses. To overcome this limitation, we use a Retrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph (KG) to retrieve relevant information from external knowledge sources, enabling LLMs to answer questions over private or previously unseen document collections. In this paper, we propose a novel approach for building a high-quality, low-noise KG. We apply several techniques, including incremental entity resolution using seed concepts, similarity-based filtering to deduplicate entries, assigning confidence scores to entity-relation pairs to filter for high-confidence pairs, and linking facts to source documents for provenance. Our KG-RAG system retrieves relevant tuples, which are added to the user prompts context before being sent to the LLM generating the response. Our evaluation demonstrates that this approach significantly enhances response relevance, reducing irrelevant answers by over 50% and increasing fully relevant answers by 88% compared to the existing production system. 

**Abstract (ZH)**: Adobe Experience Platform AI助手是一种对话工具，能够让组织通过聊天机器人无缝地与自有企业数据进行交互。然而，由于访问限制，大型语言模型（LLMs）无法检索这些内部文件，限制了它们生成准确的零样本响应的能力。为克服这一限制，我们利用基于知识图谱（KG）的检索增强生成（RAG）框架，从外部知识源中检索相关信息，从而让LLMs能够回答私有或以前未见过的文档集合中的问题。在本文中，我们提出了一种新的方法来构建高质量、低噪音的知识图谱。我们运用了多种技术，包括增量实体解析、基于相似性的去重、为实体-关系对分配置信度评分以过滤高置信度对，以及为事实链接源文档以追溯源头。我们的KG-RAG系统检索相关的元组，并在将其发送给生成响应的LLM之前将其添加到用户提示的上下文之中。我们的评估结果表明，这种方法显著提高了响应的相关性，相比现有生产系统，减少了超过50%的不相关答案，并增加了88%的完全相关答案。 

---
# A BERT Based Hybrid Recommendation System For Academic Collaboration 

**Title (ZH)**: 基于 BERT 的混合推荐系统以促进学术合作 

**Authors**: Sangeetha N, Harish Thangaraj, Varun Vashisht, Eshaan Joshi, Kanishka Verma, Diya Katariya  

**Link**: [PDF](https://arxiv.org/pdf/2502.15223)  

**Abstract**: Universities serve as a hub for academic collaboration, promoting the exchange of diverse ideas and perspectives among students and faculty through interdisciplinary dialogue. However, as universities expand in size, conventional networking approaches via student chapters, class groups, and faculty committees become cumbersome. To address this challenge, an academia-specific profile recommendation system is proposed to connect like-minded stakeholders within any university community. This study evaluates three techniques: Term Frequency-Inverse Document Frequency (TF-IDF), Bidirectional Encoder Representations from Transformers (BERT), and a hybrid approach to generate effective recommendations. Due to the unlabelled nature of the dataset, Affinity Propagation cluster-based relabelling is performed to understand the grouping of similar profiles. The hybrid model demonstrated superior performance, evidenced by its similarity score, Silhouette score, Davies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG), achieving an optimal balance between diversity and relevance in recommendations. Furthermore, the optimal model has been implemented as a mobile application, which dynamically suggests relevant profiles based on users' skills and collaboration interests, incorporating contextual understanding. The potential impact of this application is significant, as it promises to enhance networking opportunities within large academic institutions through the deployment of intelligent recommendation systems. 

**Abstract (ZH)**: 大学作为学术合作的枢纽，通过跨学科对话促进学生和教师之间多样化的思想和观点交流。然而，随着大学规模的扩大，传统的基于学生分会、班级群组和教员委员会的社交网络方法变得繁琐。为解决这一挑战，本文提出了一种针对学术界的个人资料推荐系统，旨在连接任何大学社区内志趣相投的利益相关者。本文评估了三种技术：词频-逆文档频率（TF-IDF）、双向编码器表示（BERT）以及它们的混合方法，以生成有效的推荐。由于数据集未标记，本文使用亲和传播聚类基础重新标记，以理解相似个人资料的分组。混合模型展示了卓越的性能，其通过相似度分数、轮廓系数、戴维斯-布林指数以及归一化累积折扣收益（NDCG）得分得到证实，实现了多样性和相关性的最佳平衡。此外，最优模型已经被实现为一款移动应用程序，该应用能够根据用户的技能和合作兴趣动态建议相关个人资料，并融入上下文理解。这一应用程序的潜在影响重大，因为它承诺通过部署智能推荐系统来增强大型学术机构的社交网络机会。 

---
# GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer 

**Title (ZH)**: GNN-Coder：结合图神经网络和变换器的语义代码检索增强方法 

**Authors**: Yufan Ye, Pu Pang, Ting Zhang, Hua Huang  

**Link**: [PDF](https://arxiv.org/pdf/2502.15202)  

**Abstract**: Code retrieval is a crucial component in modern software development, particularly in large-scale projects. However, existing approaches relying on sequence-based models often fail to fully exploit the structural dependencies inherent in code, leading to suboptimal retrieval performance, particularly with structurally complex code fragments. In this paper, we introduce GNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilize Abstract Syntax Tree (AST). We make the first attempt to study how GNN-integrated Transformer can promote the development of semantic retrieval tasks by capturing the structural and semantic features of code. We further propose an innovative graph pooling method tailored for AST, utilizing the number of child nodes as a key feature to highlight the intrinsic topological relationships within the AST. This design effectively integrates both sequential and hierarchical representations, enhancing the model's ability to capture code structure and semantics. Additionally, we introduce the Mean Angular Margin (MAM), a novel metric for quantifying the uniformity of code embedding distributions, providing a standardized measure of feature separability. The proposed method achieves a lower MAM, indicating a more discriminative feature representation. This underscores GNN-Coder's superior ability to distinguish between code snippets, thereby enhancing retrieval accuracy. Experimental results show that GNN-Coder significantly boosts retrieval performance, with a 1\%-10\% improvement in MRR on the CSN dataset, and a notable 20\% gain in zero-shot performance on the CosQA dataset. 

**Abstract (ZH)**: 代码检索是现代软件开发中的关键组成部分，尤其是在大型项目中。然而，现有依赖于序列模型的方法往往未能充分利用代码中固有的结构依赖关系，导致检索性能不佳，尤其是在处理结构复杂的代码片段时。在本文中，我们介绍了一种基于图神经网络（GNN）的新框架 GNN-Coder，利用抽象语法树（AST）。我们首次尝试研究如何通过捕获代码的结构和语义特征来促进基于 GNN-integrated Transformer 的语义检索任务的发展。此外，我们还提出了一种针对 AST 的创新图池化方法，利用子节点的数量作为关键特征，以突出 AST 内在的拓扑关系。该设计有效地结合了序列和层次表示，增强了模型捕捉代码结构和语义的能力。此外，我们引入了一种新的度量方法——均值角度余弦（Mean Angular Margin, MAM），用以量化代码嵌入分布的均匀性，提供了一种标准化的特征可分性度量。所提出的方法实现了更低的 MAM，表明了更具区分性的特征表示。这突显了 GNN-Coder 在区分代码片段方面的优越能力，从而提高了检索精度。实验结果表明，GNN-Coder 显著提升了检索性能，在 CSN 数据集上的 MRR 提高了 1%-10%，并且在 CosQA 数据集上的零样本性能获得了显著的 20% 改进。 

---
# Is Relevance Propagated from Retriever to Generator in RAG? 

**Title (ZH)**: 在RAG中，检索器到生成器是否传播相关性？ 

**Authors**: Fangzheng Tian, Debasis Ganguly, Craig Macdonald  

**Link**: [PDF](https://arxiv.org/pdf/2502.15025)  

**Abstract**: Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task's objective of maximising the relevance of a set of top-ranked documents, a RAG system's objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance. 

**Abstract (ZH)**: 检索增强生成（RAG）是一种框架，通过将从集合中检索到的一组外部知识（通常是以文档的形式）作为大型语言模型（LLM）生成提示的一部分，以期提高下游任务（如问答）的性能。与标准检索任务的目标有所不同，即最大化一组高排名文档的相关性，RAG系统的目标则是最大化这些文档的总效用，其中文档的效用指的是将其作为附加背景信息的一部分添加到LLM提示中，是否能够改善下游任务。现有研究探讨了RAG上下文在知识密集型语言任务（KILT）中的作用，其中相关性主要表现为答案包含。相比之下，在我们的研究中，相关性对应于查询与文档之间的话题重叠，特别是在信息检索任务中。具体来说，我们利用信息检索测试集合，实证研究了由相关话题文档组成的RAG上下文是否能够提高下游性能。我们的实验得出以下结论：(a) 相关性和效用之间存在微弱的正相关；(b) 随着上下文规模的增大（即k-shot中的k值增大），这种相关性会降低；(c) 更有效的检索模型通常会导致更好的下游RAG性能。 

---
# LightThinker: Thinking Step-by-Step Compression 

**Title (ZH)**: LightThinker：逐步压缩方法 

**Authors**: Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.15589)  

**Abstract**: Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂的推理任务中表现出了显著的能力，但是它们在生成长序列时面临的大量内存和计算成本限制了它们的效率。本文提出了一种名为LightThinker的新方法，该方法使LLMs能够在推理过程中动态地压缩中间思维。LightThinker借鉴了人类认知过程，将冗长的思想步骤压缩为简洁的表示，并丢弃原始的推理链，从而显著减少了存储在上下文窗口中的令牌数。这一过程通过在数据构建上进行训练、将隐藏状态映射到浓缩的核心令牌以及创建专门的注意力掩码来实现。此外，我们引入了依赖性（Dep）度量来通过测量生成过程中对历史令牌的依赖程度来量化压缩的程度。在四个数据集和两个模型上的广泛实验表明，LightThinker在减少了峰值内存使用和推理时间的同时，保持了竞争力的准确率。我们的工作提供了一种新的方向，即在不牺牲性能的情况下提高LLMs在复杂推理任务中的效率。代码将在以下链接中发布：https://www.example.com/lthker-code。 

---
# Detecting Future-related Contexts of Entity Mentions 

**Title (ZH)**: 检测实体提及的未来相关上下文 

**Authors**: Puneet Prashar, Krishna Mohan Shukla, Adam Jatowt  

**Link**: [PDF](https://arxiv.org/pdf/2502.15332)  

**Abstract**: The ability to automatically identify whether an entity is referenced in a future context can have multiple applications including decision making, planning and trend forecasting. This paper focuses on detecting implicit future references in entity-centric texts, addressing the growing need for automated temporal analysis in information processing. We first present a novel dataset of 19,540 sentences built around popular entities sourced from Wikipedia, which consists of future-related and non-future-related contexts in which those entities appear. As a second contribution, we evaluate the performance of several Language Models including also Large Language Models (LLMs) on the task of distinguishing future-oriented content in the absence of explicit temporal references. 

**Abstract (ZH)**: 自动识别实体在未来语境中被提及的能力可以在决策制定、规划和趋势预测等方面找到多种应用。本文专注于在以实体为中心的文本中检测隐含的未来引用，以应对信息处理中对自动化时间分析日益增长的需求。首先，我们提出了一个包含19,540个句子的新数据集，这些句子围绕从维基百科获取的流行实体构建，其中包括这些实体出现于未来相关和非未来相关的上下文中。作为第二个贡献，我们评估了几种语言模型（包括大型语言模型）在没有明确时间引用的情况下区分面向未来的文本内容的性能。 

---
# RAPTOR: Refined Approach for Product Table Object Recognition 

**Title (ZH)**: RAPTOR：改进的产品表对象识别方法 

**Authors**: Eliott Thomas, Mickael Coustaty, Aurelie Joseph, Elodie Carel, Vincent Poulain D'Andecy, Jean-Marc Ogier  

**Link**: [PDF](https://arxiv.org/pdf/2502.14918)  

**Abstract**: Extracting tables from documents is a critical task across various industries, especially on business documents like invoices and reports. Existing systems based on DEtection TRansformer (DETR) such as TAble TRansformer (TATR), offer solutions for Table Detection (TD) and Table Structure Recognition (TSR) but face challenges with diverse table formats and common errors like incorrect area detection and overlapping columns. This research introduces RAPTOR, a modular post-processing system designed to enhance state-of-the-art models for improved table extraction, particularly for product tables. RAPTOR addresses recurrent TD and TSR issues, improving both precision and structural predictions. For TD, we use DETR (trained on ICDAR 2019) and TATR (trained on PubTables-1M and FinTabNet), while TSR only relies on TATR. A Genetic Algorithm is incorporated to optimize RAPTOR's module parameters, using a private dataset of product tables to align with industrial needs. We evaluate our method on two private datasets of product tables, the public DOCILE dataset (which contains tables similar to our target product tables), and the ICDAR 2013 and ICDAR 2019 datasets. The results demonstrate that while our approach excels at product tables, it also maintains reasonable performance across diverse table formats. An ablation study further validates the contribution of each module in our system. 

**Abstract (ZH)**: 从文档中提取表格是各行各业中的关键任务，尤其是在商业文档如发票和报告中。现有的基于DEtection TRansformer（DETR）的系统，如Table TRansformer（TATR），提供了表格检测（Table Detection, TD）和表格结构识别（Table Structure Recognition, TSR）的解决方案，但在处理多样的表格格式和常见的错误（如不正确的区域检测和列重叠）时仍面临挑战。本研究引入了RAPTOR，这是一种模块化后处理系统，旨在增强最先进的模型以提高表格提取性能，特别是针对产品表格。RAPTOR解决了一系列TD和TSR问题，提高了精确度和结构预测。对于TD，我们使用了DETR（在ICDAR 2019上训练）和TATR（在PubTables-1M和FinTabNet上训练），而TSR仅依赖于TATR。我们引入了遗传算法来优化RAPOTR模块的参数，使用了一个私有的产品表格数据集来满足工业需求。我们使用两个私有的产品表格数据集、公共的DOCILE数据集（其中包含与目标产品表格相似的表格）以及ICDAR 2013和ICDAR 2019数据集来评估我们的方法。结果显示，尽管我们的方法在产品表格上表现出色，但在多样化的表格格式方面也保持了合理的性能。进一步的消融研究表明，每个模块在系统中的贡献得到了验证。 

---
# OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment 

**Title (ZH)**: OpenSearch-SQL：增强文本到SQL转换的动态少样本学习和一致性对齐 

**Authors**: Xiangjin Xie, Guangwei Xu, Lingyan Zhao, Ruijie Guo  

**Link**: [PDF](https://arxiv.org/pdf/2502.14913)  

**Abstract**: Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucination problems. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Additionally, we designed an intermediate language called SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL. These methods have significantly improved the performance of LLMs in the Text-to-SQL task.
In terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency. 

**Abstract (ZH)**: 尽管多智能体协作的大语言模型（LLMs）在文本到SQL任务上取得了显著突破，但其性能仍受到多种因素的限制。这些因素包括模型框架的不完备性、无法正确遵循指令以及模型幻觉问题。为解决这些问题，我们提出了OpenSearch-SQL，将文本到SQL任务分为四个主要模块：预处理、提取、生成和精炼，并基于一致性对齐机制引入了一个对齐模块。该架构通过对齐模块将智能体的输入和输出进行对齐，从而减少指令遵循失败和幻觉问题。同时，我们设计了一种名为SQL-Like的中间语言，并基于SQL-Like优化了结构化的CoT推理。此外，我们还开发了一种自学习的动态 Few-Shot 策略，形式为Query-CoT-SQL。这些方法显著提高了大语言模型在文本到SQL任务中的性能。

在模型选择方面，我们直接使用了基本的大语言模型，未进行任何后训练，从而简化了任务链并增强了框架的可移植性。实验结果显示，OpenSearch-SQL在BIRD开发集上的执行准确性（EX）为69.3%，测试集上为72.28%，基于奖励的有效性效率评分（R-VES）为69.36%，三项指标均在提交时排名第一。这些结果表明，所提出方法在有效性和效率方面具有全面的优势。 

---
# PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths 

**Title (ZH)**: PathRAG：基于关系路径的图检索增强生成的剪枝方法 

**Authors**: Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, Cheng Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14902)  

**Abstract**: Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: this https URL 

**Abstract (ZH)**: 检索增强生成（RAG）通过从外部数据库检索知识来提高大型语言模型（LLMs）的响应质量。典型的RAG方法将文本数据库分割成片段，并以扁平结构组织这些片段以进行高效搜索。为了更好地捕获文本数据库中固有的依赖关系和结构化关系，研究人员提出将文本信息组织成索引图，这称为基于图的RAG。然而，我们认为当前基于图的方法的主要局限在于检索信息的冗余性，而不是不足。此外，先前的方法使用扁平结构在提示中组织检索信息，导致性能不佳。为了克服这些局限，我们提出了PathRAG，它从索引图中检索关键的关系路径，并将这些路径转换为文本形式以提示LLM。具体而言，PathRAG通过基于流的修剪有效减少了冗余信息，同时通过基于路径的提示引导LLM生成更逻辑性和连贯性的响应。实验结果显示，与六个数据集和五个评估维度上的最新基准相比，PathRAG始终表现出更优的性能。源代码可以通过以下链接获取：this https URL 

---
# Retrieval-augmented systems can be dangerous medical communicators 

**Title (ZH)**: 检索增强系统可能是危险的医疗通信工具 

**Authors**: Lionel Wong, Ayman Ali, Raymond Xiong, Shannon Zeijang Shen, Yoon Kim, Monica Agrawal  

**Link**: [PDF](https://arxiv.org/pdf/2502.14898)  

**Abstract**: Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain. 

**Abstract (ZH)**: 患者长期以来一直在网上寻求健康信息，并且越来越多地转向生成式AI来回答他们的健康相关问题。鉴于医疗领域的重要性和高风险，检索增强生成和引文 grounding 等技术被广泛推广，用以减少幻觉现象并提高AI生成回答的准确性，并已广泛应用于搜索引擎中。本文指出，即使这些方法产生的是从原始文件中提取的字面准确内容且没有幻觉，仍然可能误导患者。患者从AI生成的输出中得出的解释可能与直接阅读原始材料或咨询有知识的临床医生时大相径庭。通过大规模的查询分析，特别是在有争议的诊断和程序安全性等领域，我们通过定量和定性的证据支持了这些系统产生的亚优答案。特别是，我们强调这些模型倾向于脱离上下文、省略关键相关资料，并加强患者的误解或偏见。我们提出了一系列建议，如引入交际语用学和增强原始文件理解等，这些措施可帮助缓解这些问题，并扩展到医疗领域之外。 

---
