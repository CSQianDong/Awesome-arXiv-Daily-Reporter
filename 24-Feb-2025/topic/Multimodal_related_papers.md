# Bridging Domain Gaps between Pretrained Multimodal Models and Recommendations 

**Title (ZH)**: 预训练多模态模型与推荐系统间领域差距的桥梁构建 

**Authors**: Wenyu Zhang, Jie Luo, Xinming Zhang, Yuan Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.15542)  

**Abstract**: With the explosive growth of multimodal content online, pre-trained visual-language models have shown great potential for multimodal recommendation. However, while these models achieve decent performance when applied in a frozen manner, surprisingly, due to significant domain gaps (e.g., feature distribution discrepancy and task objective misalignment) between pre-training and personalized recommendation, adopting a joint training approach instead leads to performance worse than baseline. Existing approaches either rely on simple feature extraction or require computationally expensive full model fine-tuning, struggling to balance effectiveness and efficiency. To tackle these challenges, we propose \textbf{P}arameter-efficient \textbf{T}uning for \textbf{M}ultimodal \textbf{Rec}ommendation (\textbf{PTMRec}), a novel framework that bridges the domain gap between pre-trained models and recommendation systems through a knowledge-guided dual-stage parameter-efficient training strategy. This framework not only eliminates the need for costly additional pre-training but also flexibly accommodates various parameter-efficient tuning methods. 

**Abstract (ZH)**: 随着在线多模态内容的爆炸性增长，预训练的视觉-语言模型在多模态推荐方面展现了巨大的潜力。然而，当这些模型以静态方式应用时，它们能够取得不错的性能，但令人意外的是，由于预训练与个性化推荐之间的显著领域差距（如特征分布差异和任务目标不匹配），采用联合训练方法反而会导致性能低于基线。现有方法要么依赖简单的特征提取，要么需要进行计算密集型的完整模型微调，难以在有效性和效率之间取得平衡。为了解决这些挑战，我们提出了一种名为**参数高效调优以进行多模态推荐**（PTMRec）的新颖框架，该框架通过知识引导的两阶段参数高效训练策略，将预训练模型与推荐系统之间的领域差距进行桥接。该框架不仅消除了成本高昂的额外预训练需求，还灵活地兼容了各种参数高效调优方法。 

---
# Evaluating Multimodal Generative AI with Korean Educational Standards 

**Title (ZH)**: 使用韩国教育标准评估多模态生成型人工智能 

**Authors**: Sanghee Park, Geewook Kim  

**Link**: [PDF](https://arxiv.org/pdf/2502.15422)  

**Abstract**: This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at this https URL. 

**Abstract (ZH)**: 本文介绍了韩国国家教育测试基准（KoNET），这是一种新的基准测试，旨在使用韩国国家教育测试评估多模态生成式AI系统。KoNET 包含四场考试：韩国小学普遍教育发展测试（KoEGED）、韩国中学普遍教育发展测试（KoMGED）、韩国高中普遍教育发展测试（KoHGED）和韩国大学修业能力测试（KoCSAT）。这些考试以严格的标准和多样的问题著称，使得从不同教育层次全面分析AI性能成为可能。通过专注于韩语，KoNET 提供了对较未探索语言中模型性能的见解。我们通过考察难度、科目多样性和人工错误率来评估多种模型——包括开源、开放访问和封闭API模型。代码和数据集构建器将在以下链接处完全开源：[此处填写链接]。 

---
# Social Genome: Grounded Social Reasoning Abilities of Multimodal Models 

**Title (ZH)**: 社会基因组：多模态模型的接地社会推理能力 

**Authors**: Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency  

**Link**: [PDF](https://arxiv.org/pdf/2502.15109)  

**Abstract**: Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce Social Genome, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. Social Genome contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). Social Genome is also the first modeling challenge to study external knowledge in social reasoning. Social Genome computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of Social Genome through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models. 

**Abstract (ZH)**: 社会推理能力对于AI系统在社交情境下有效解释和回应多模态人类沟通与交互至关重要。我们提出了Social Genome，这是首个用于评估多模态模型细腻且情境化的社会推理能力的标准基准。Social Genome 包含了272个交互视频及1,486条与这些交互相关的人类标注推理论证痕迹。这些推理论证痕迹包含了5,777个推理步骤，涵盖了视觉线索、言语线索、音调线索以及外部知识（视频之外的背景知识）。这是首次针对社交推理中外部知识研究的标准基准模型挑战。Social Genome 通过计算度量指标来全面评估模型生成的社交推理痕迹的语义质量和结构质量。我们通过与最新模型的实验展示了Social Genome 的实用性，识别了性能差距，并为未来研究如何提升多模态模型的情境化社会推理能力提供了研究机会。 

---
# Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation 

**Title (ZH)**: 使用视觉检索增强生成方法减少医疗多模态大型语言模型的幻觉 

**Authors**: Yun-Wei Chu, Kai Zhang, Christopher Malon, Martin Renqiang Min  

**Link**: [PDF](https://arxiv.org/pdf/2502.15040)  

**Abstract**: Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a retrieval-augmented generation framework that incorporates both text and visual data from retrieved images. On the MIMIC-CXR chest X-ray report generation and Multicare medical image caption generation datasets, we show that Visual RAG improves the accuracy of entity probing, which asks whether a medical entities is grounded by an image. We show that the improvements extend both to frequent and rare entities, the latter of which may have less positive training data. Downstream, we apply V-RAG with entity probing to correct hallucinations and generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1 score. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在视觉和文本任务中展现了出色的性能。然而，幻觉仍然是一个主要挑战，特别是在如医疗健康等对细节要求较高的领域。在本文中，我们展示了如何通过引入Visual RAG（视觉检索增强生成）框架来增强MLLMs，该框架结合了检索到的图像中的文本和视觉数据。我们在MIMIC-CXR胸部X光报告生成和Multicare医学图像字幕生成数据集上展示了Visual RAG如何提高实体探查的准确性，即检查医学实体是否通过图像得到支撑。我们证明了这种改进不仅适用于常见的实体，还适用于缺乏积极训练数据的罕见实体。进一步的应用中，我们使用包含实体探查的Visual RAG来纠正幻觉，生成更符合临床准确性的X光报告，并获得了更高的RadGraph-F1评分。 

---
# InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback 

**Title (ZH)**: InterFeedback：通过人类反馈揭示大规模多模态模型的互动智能 

**Authors**: Henry Hengyuan Zhao, Wenqi Pei, Yifei Tao, Haiyang Mei, Mike Zheng Shou  

**Link**: [PDF](https://arxiv.org/pdf/2502.15027)  

**Abstract**: Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback. 

**Abstract (ZH)**: 现有的基准测试并没有评估大型多模态模型（LMMs）与人类用户的互动智能，而这种能力对于开发通用人工智能辅助工具至关重要。为此，我们设计了InterFeedback，这是一种互动框架，可以应用于任何LMM和数据集，以自主评估其互动智能。在此基础上，我们引入了InterFeedback-Bench，该评测框架使用两个代表性数据集MMMU-Pro和MathVerse，对10个不同的开源LMM进行评估。此外，我们还推出了InterFeedback-Human，这是一个专门为测试领先模型（如OpenAI-o1和Claude-3.5-Sonnet）的互动性能而新收集的120个案例的数据集。我们的评估结果表明，即使是最先进的LMM（如OpenAI-o1），通过人类反馈也只有不到50%的误差可以被纠正。这些发现表明，需要发展新的方法来增强LMMs解释和利用反馈的能力。 

---
# Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models 

**Title (ZH)**: 超越文字：探索多模态模型中的文化价值敏感性 

**Authors**: Srishti Yadav, Zhi Zhang, Daniel Hershcovich, Ekaterina Shutova  

**Link**: [PDF](https://arxiv.org/pdf/2502.14906)  

**Abstract**: Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models. 

**Abstract (ZH)**: 基于文化背景探究大型语言模型（LLMs）的价值对齐已成为一个重要研究领域。然而，类似偏见在大规模视觉-语言模型（VLMs）中的研究尚未得到充分探讨。随着多模态模型规模的不断扩大，评估图像是否能作为文化可靠的代理，以及这些价值观如何通过视觉和文本数据的整合而被嵌入，变得越来越重要。在本文中，我们对不同规模的多模态模型进行了全面评估，重点关注它们与文化价值观的对齐情况。研究发现，与LLMs类似，VLMs对文化价值观表现出敏感性，但它们在价值观对齐方面的性能高度依赖于上下文。虽然VLMs通过使用图像在提升价值理解方面显示出潜力，但在不同上下文中的这种对齐差异显著，突显了多模态模型对齐中的复杂性和未被充分探讨的挑战。 

---
# Bridging vision language model (VLM) evaluation gaps with a framework for scalable and cost-effective benchmark generation 

**Title (ZH)**: 使用可扩展且成本效益高的基准生成框架弥合视觉语言模型（VLM）评估差距 

**Authors**: Tim Rädsch, Leon Mayer, Simon Pavicic, A. Emre Kavur, Marcel Knopp, Barış Öztürk, Klaus Maier-Hein, Paul F. Jaeger, Fabian Isensee, Annika Reinke, Lena Maier-Hein  

**Link**: [PDF](https://arxiv.org/pdf/2502.15563)  

**Abstract**: Reliable evaluation of AI models is critical for scientific progress and practical application. While existing VLM benchmarks provide general insights into model capabilities, their heterogeneous designs and limited focus on a few imaging domains pose significant challenges for both cross-domain performance comparison and targeted domain-specific evaluation. To address this, we propose three key contributions: (1) a framework for the resource-efficient creation of domain-specific VLM benchmarks enabled by task augmentation for creating multiple diverse tasks from a single existing task, (2) the release of new VLM benchmarks for seven domains, created according to the same homogeneous protocol and including 162,946 thoroughly human-validated answers, and (3) an extensive benchmarking of 22 state-of-the-art VLMs on a total of 37,171 tasks, revealing performance variances across domains and tasks, thereby supporting the need for tailored VLM benchmarks. Adoption of our methodology will pave the way for the resource-efficient domain-specific selection of models and guide future research efforts toward addressing core open questions. 

**Abstract (ZH)**: 可靠评估人工智能模型对于科学进步和实际应用至关重要。虽然现有的视觉语言模型(Vision Language Models, VLM)基准提供了关于模型能力的通用见解，但它们不同的设计和对少数成像领域的关注有限，这些挑战阻碍了跨领域性能比較和特定领域评价的针对性。为了解决这一问题，我们提出了三个主要贡献：（1）一种利用任务增强方法从单一现有任务生成多个多样的任务，从而促进资源高效创建特定领域VLM基准的框架；（2）发布针对七个领域的新的VLM基准，按照相同的标准化协议生成，并包含162,946个详细的人工验证答案；（3）全面评估22个最先进的VLM模型在总计37,171个任务上的表现，揭示不同领域和任务之间的性能差异，从而支持制定定制化VLM基准的必要性。采用我们的方法将为特定领域的模型选择提供资源效率，并指导未来研究努力解决核心开放问题。 

---
# Chitrarth: Bridging Vision and Language for a Billion People 

**Title (ZH)**: Chitrarth：连接视觉与语言，服务于十亿人 

**Authors**: Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal  

**Link**: [PDF](https://arxiv.org/pdf/2502.15392)  

**Abstract**: Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena. 

**Abstract (ZH)**: 近年来，多模态基础模型主要在英语或高资源的欧洲语言数据上进行训练，这限制了它们在其他中低资源语言中的应用。为解决这一局限，我们引入了Chitrarth（Chitra：图像；Artha：意义）这一包容性的跨模态语言模型（VLM），特别针对印度10种主要语言丰富的语言多样性和视觉推理能力。我们的模型有效地整合了一种最先进的多语言大规模语言模型（LLM）和一个视觉模块，该视觉模块主要在多语言图像文本数据上进行训练。此外，我们还引入了BharatBench，这是一个全面的评估框架，用于评估跨多种印度语言的VLM，最终促进更多样化和有效的AI系统的构建。我们的模型在低资源语言基准测试中达到了最先进的性能，同时在英语中保持了高效性。通过我们的研究，我们旨在建立多语言多模态能力的新标准，为现有模型提供显著改进，并为进一步在这个领域的发展奠定基础。 

---
# The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning 

**Title (ZH)**: LLM-和VLM-集成强化学习的发展 landscape 

**Authors**: Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor  

**Link**: [PDF](https://arxiv.org/pdf/2502.15214)  

**Abstract**: Reinforcement learning (RL) has shown impressive results in sequential decision-making tasks. Meanwhile, Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities in multimodal understanding and reasoning. These advances have led to a surge of research integrating LLMs and VLMs into RL. In this survey, we review representative works in which LLMs and VLMs are used to overcome key challenges in RL, such as lack of prior knowledge, long-horizon planning, and reward design. We present a taxonomy that categorizes these LLM/VLM-assisted RL approaches into three roles: agent, planner, and reward. We conclude by exploring open problems, including grounding, bias mitigation, improved representations, and action advice. By consolidating existing research and identifying future directions, this survey establishes a framework for integrating LLMs and VLMs into RL, advancing approaches that unify natural language and visual understanding with sequential decision-making. 

**Abstract (ZH)**: 强化学习（RL）在序贯决策任务中展现了令人印象深刻的成果。与此同时，大型语言模型（LLMs）和多模态视觉语言模型（VLMs）已经涌现，并展示了在多模态理解和推理方面的 impressive 能力。这些进展催生了将 LLMs 和 VLMs 结合进 RL 的大量研究。在本文综述中，我们回顾了 LLMs 和 VLMs 在克服 RL 中关键挑战时的应用，如缺乏先验知识、长期规划以及奖励设计。我们提出了一种分类法，将这些由 LLMs/VLMs 支撑的 RL 方法归类为三个角色：代理、规划器和奖励。最后，我们探讨了开放性问题，包括语义关联、偏见 mitigation、更好的表示形式以及行动建议。通过总结现有研究并确定未来的研究方向，本文综述建立了一种框架，用于将 LLMs 和 VLMs 整合进 RL，推动了自然语言和视觉理解与序贯决策统一的方法的发展。 

---
# Can Hallucination Correction Improve Video-Language Alignment? 

**Title (ZH)**: 可以纠正幻觉以改善视频-语言对齐吗？ 

**Authors**: Lingjun Zhao, Mingyang Xie, Paola Cascante-Bonilla, Hal Daumé III, Kwonjoon Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.15079)  

**Abstract**: Large Vision-Language Models often generate hallucinated content that is not grounded in its visual inputs. While prior work focuses on mitigating hallucinations, we instead explore leveraging hallucination correction as a training objective to improve video-language alignment. We introduce HACA, a self-training framework learning to correct hallucinations in descriptions that do not align with the video content. By identifying and correcting inconsistencies, HACA enhances the model's ability to align video and textual representations for spatio-temporal reasoning. Our experimental results show consistent gains in video-caption binding and text-to-video retrieval tasks, demonstrating that hallucination correction-inspired tasks serve as an effective strategy for improving vision and language alignment. 

**Abstract (ZH)**: 大型多模态模型经常生成与视觉输入脱节的虚构内容。尽管之前的研究所侧重于减轻这些虚构内容，我们则探索将虚构内容的校正作为训练目标，以提高视频-语言对齐。我们提出了一种自我训练框架HACA，该框架旨在纠正与视频内容不一致的描述中的虚构内容。通过识别并修正不一致性，HACA 提高了模型在时空推理方面的视频和文本表示对齐能力。我们的实验结果显示，HACA 在视频-字幕关联和文本到视频检索任务中均取得了稳健的提升，表明基于虚构内容校正的任务是一种有效的策略，能够改善视觉与语言的对齐。 

---
# What Is a Good Caption? A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Coverage of MLLMs 

**Title (ZH)**: 什么是好的标题？一个全面的视觉标题基准，用于评估MLLMs的准确性和覆盖面 

**Authors**: Zhihang Liu, Chen-Wei Xie, Bin Wen, Feiwu Yu, Jixuan Chen, Boqiang Zhang, Nianzu Yang, Pandeng Li, Yun Zheng, Hongtao Xie  

**Link**: [PDF](https://arxiv.org/pdf/2502.14914)  

**Abstract**: Recent advancements in Multimodal Large Language Models (MLLMs) have rendered traditional visual captioning benchmarks obsolete, as they primarily evaluate short descriptions with outdated metrics. While recent benchmarks address these limitations by decomposing captions into visual elements and adopting model-based evaluation, they remain incomplete-overlooking critical aspects, while providing vague, non-explanatory scores. To bridge this gap, we propose CV-CapBench, a Comprehensive Visual Caption Benchmark that systematically evaluates caption quality across 6 views and 13 dimensions. CV-CapBench introduces precision, recall, and hit rate metrics for each dimension, uniquely assessing both correctness and coverage. Experiments on leading MLLMs reveal significant capability gaps, particularly in dynamic and knowledge-intensive dimensions. These findings provide actionable insights for future research. The code and data will be released. 

**Abstract (ZH)**: 近年来，多模态大型语言模型（MLLMs）的发展已使传统视觉配图基准过时，因为这些基准主要评估短描述并使用过时的指标。虽然最近的基准通过将描述分解为视觉元素并采用基于模型的评估来解决这些局限性，但它们仍然存在不足，忽视了关键方面，并提供了含糊不清且缺乏解释性的评分。为弥补这一缺口，我们提出了一种全面的视觉配图基准CV-CapBench，该基准系统地从6个视角和13个维度评估配图质量。CV-CapBench引入了针对每个维度的精确度、召回率和命中率指标，独特地评估了正确性与全面性。在领先MLLM上的实验揭示了显著的能力差距，特别是在动态和知识密集型维度上。这些 findings 为未来的研究提供了实质性的指导。代码和数据将公开发布。 

---
# KOALA: Knowledge Conflict Augmentations for Robustness in Vision Language Models 

**Title (ZH)**: KOALA：知识冲突增强以提高视觉语言模型的鲁棒性 

**Authors**: Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, Kathleen M. Carley  

**Link**: [PDF](https://arxiv.org/pdf/2502.14908)  

**Abstract**: The robustness of large language models (LLMs) against knowledge conflicts in unimodal question answering systems has been well studied. However, the effect of conflicts in information sources on vision language models (VLMs) in multimodal settings has not yet been explored. In this work, we propose \segsub, a framework that applies targeted perturbations to image sources to study and improve the robustness of VLMs against three different types of knowledge conflicts, namely parametric, source, and counterfactual conflicts. Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation. On the other hand, VLMs perform poorly on counterfactual examples (<30% accuracy) and fail to reason over source conflicts (<1% accuracy). We also find a link between hallucinations and image context, with GPT-4o prone to hallucination when presented with highly contextualized counterfactual examples. While challenges persist with source conflicts, finetuning models significantly improves reasoning over counterfactual samples. Our findings highlight the need for VLM training methodologies that enhance their reasoning capabilities, particularly in addressing complex knowledge conflicts between multimodal sources. 

**Abstract (ZH)**: 大型语言模型（LLMs）在单模态问答系统中对抗知识冲突的鲁棒性已有广泛研究，然而，在多模态设置中，信息来源之间的冲突如何影响视觉语言模型（VLMs）的抗冲突能力尚未得到探索。在这项工作中，我们提出了\segsub框架，该框架通过针对图像来源施加目标扰动来研究和提高VLMs在三种不同类型的知识冲突——参数冲突、来源冲突和反事实冲突——下的鲁棒性。与先前发现LLMs对由于文本扰动产生的参数冲突敏感相反，我们发现VLMs对图像扰动具有很大的鲁棒性。另一方面，VLMs在反事实示例上的表现很差（准确率<30%），并且无法处理来源冲突（准确率<1%）。我们还发现幻觉与图像上下文之间存在联系，GPT-4o在面对高度上下文化的反事实示例时容易产生幻觉。尽管在来源冲突方面仍存在挑战，但微调模型可以显著提高对反事实样本的推理能力。我们的研究结果强调了需要改进VLM训练方法，特别是在处理复杂的多模态来源之间的知识冲突方面加强其推理能力。 

---
# MVIP -- A Dataset and Methods for Application Oriented Multi-View and Multi-Modal Industrial Part Recognition 

**Title (ZH)**: MVIP ——一种面向应用的多视图和多模态工业零件识别数据集及方法 

**Authors**: Paul Koch, Marian Schlüter, Jörg Krüger  

**Link**: [PDF](https://arxiv.org/pdf/2502.15448)  

**Abstract**: We present MVIP, a novel dataset for multi-modal and multi-view application-oriented industrial part recognition. Here we are the first to combine a calibrated RGBD multi-view dataset with additional object context such as physical properties, natural language, and super-classes. The current portfolio of available datasets offers a wide range of representations to design and benchmark related methods. In contrast to existing classification challenges, industrial recognition applications offer controlled multi-modal environments but at the same time have different problems than traditional 2D/3D classification challenges. Frequently, industrial applications must deal with a small amount or increased number of training data, visually similar parts, and varying object sizes, while requiring a robust near 100% top 5 accuracy under cost and time constraints. Current methods tackle such challenges individually, but direct adoption of these methods within industrial applications is complex and requires further research. Our main goal with MVIP is to study and push transferability of various state-of-the-art methods within related downstream tasks towards an efficient deployment of industrial classifiers. Additionally, we intend to push with MVIP research regarding several modality fusion topics, (automated) synthetic data generation, and complex data sampling -- combined in a single application-oriented benchmark. 

**Abstract (ZH)**: 我们提出了MVIP，这是一个新颖的数据集，专为多模态和多视图应用导向的工业部件识别设计。在这里，我们首次将校准的RGBD多视角数据集与额外的对象上下文（如物理属性、自然语言和超级类别）结合在一起。目前可用的数据集提供了广泛的表示方式，用于设计和基准测试相关方法。与现有的分类挑战不同，工业识别应用提供的是可控的多模态环境，但同时与传统2D/3D分类挑战相比也存在不同的问题。通常，工业应用必须处理少量或增加的训练数据、外观相似的部件以及变化的对象尺寸，同时在成本和时间限制下要求稳健的近100% top-5准确性。当前的方法分别应对这些挑战，但在工业应用中直接采用这些方法是复杂的，并需要进一步研究。我们的主要目标是通过MVIP研究和推动各种最新技术在相关下游任务中的可迁移性，以实现工业分类器的高效部署。此外，我们希望通过MVIP促进多模态融合的研究，包括（自动）合成数据生成和复杂数据采样——这些都结合在一个应用导向的基准测试中。 

---
# Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions 

**Title (ZH)**: 探究具身多模态大型模型：发展、数据集及未来方向 

**Authors**: Shoubin Chen, Zehao Wu, Kai Zhang, Chunyu Li, Baiyang Zhang, Fei Ma, Fei Richard Yu, Qingquan Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.15336)  

**Abstract**: Embodied multimodal large models (EMLMs) have gained significant attention in recent years due to their potential to bridge the gap between perception, cognition, and action in complex, real-world environments. This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models, while also examining other emerging architectures. We discuss the evolution of EMLMs, with a focus on embodied perception, navigation, interaction, and simulation. Furthermore, the review provides a detailed analysis of the datasets used for training and evaluating these models, highlighting the importance of diverse, high-quality data for effective learning. The paper also identifies key challenges faced by EMLMs, including issues of scalability, generalization, and real-time decision-making. Finally, we outline future directions, emphasizing the integration of multimodal sensing, reasoning, and action to advance the development of increasingly autonomous systems. By providing an in-depth analysis of state-of-the-art methods and identifying critical gaps, this paper aims to inspire future advancements in EMLMs and their applications across diverse domains. 

**Abstract (ZH)**: 近年来，嵌入式多模态大型模型（EMLMs）因其潜力而在感知、认知和行动之间架起桥梁，引起了广泛关注。本综述全面探讨了这类模型的发展，包括大型语言模型（LLMs）、大型视觉模型（LVMs）及其他模型，并考察了其他新兴架构。我们讨论了EMLMs的发展演变，重点关注嵌入式感知、导航、交互和模拟。此外，本文还详细分析了用于训练和评估这些模型的数据集，强调了数据多样性与高质量对于有效学习的重要性。本文还指出了EMLMs面临的几项关键挑战，包括可扩展性、泛化能力和实时决策问题。最后，我们概述了未来发展方向，强调了将多模态感知、推理和行动结合的重要性，以推进更加自主系统的开发。通过深入分析最新技术方法并识别关键缺口，本文旨在激发未来在EMLMs及其跨多种领域的应用方面的进步。 

---
# SentiFormer: Metadata Enhanced Transformer for Image Sentiment Analysis 

**Title (ZH)**: SentiFormer：增强元数据的图像情感分析变换器 

**Authors**: Bin Feng, Shulan Ruan, Mingzheng Yang, Dongxuan Han, Huijie Liu, Kai Zhang, Qi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.15322)  

**Abstract**: As more and more internet users post images online to express their daily emotions, image sentiment analysis has attracted increasing attention. Recently, researchers generally tend to design different neural networks to extract visual features from images for sentiment analysis. Despite the significant progress, metadata, the data (e.g., text descriptions and keyword tags) for describing the image, has not been sufficiently explored in this task. In this paper, we propose a novel Metadata Enhanced Transformer for sentiment analysis (SentiFormer) to fuse multiple metadata and the corresponding image into a unified framework. Specifically, we first obtain multiple metadata of the image and unify the representations of diverse data. To adaptively learn the appropriate weights for each metadata, we then design an adaptive relevance learning module to highlight more effective information while suppressing weaker ones. Moreover, we further develop a cross-modal fusion module to fuse the adaptively learned representations and make the final prediction. Extensive experiments on three publicly available datasets demonstrate the superiority and rationality of our proposed method. 

**Abstract (ZH)**: 随着越来越多的互联网用户通过上传图片来表达日常情绪，图像情感分析受到了越来越多的关注。近年来，研究人员普遍倾向于设计不同的神经网络从图像中提取视觉特征进行情感分析。尽管在这一领域取得了显著的进步，但描述图像的元数据（例如，文本描述和关键词标签）尚未得到充分探索。本文提出了一种新型的元数据增强Transformer（SentiFormer）以将多种元数据与相应的图像融合到统一框架中。具体而言，我们首先获取图像的多种元数据，并统一多种数据的表示。然后，我们设计了一个自适应相关性学习模块，以适应性地为每种元数据学习合适的权重，从而突出更有效的信息并抑制较弱的信息。此外，我们进一步开发了一个跨模态融合模块，将适应性学习的表示融合起来并做出最终预测。在三个公开可用的数据集上的广泛实验验证了我们提出的方法的优势和合理性。 

---
# Methods and Trends in Detecting Generated Images: A Comprehensive Review 

**Title (ZH)**: 生成图像检测的方法与趋势：一篇全面综述 

**Authors**: Arpan Mahara, Naphtali Rishe  

**Link**: [PDF](https://arxiv.org/pdf/2502.15176)  

**Abstract**: The proliferation of generative models, such as Generative Adversarial Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has enabled the synthesis of high-quality multimedia data. However, these advancements have also raised significant concerns regarding adversarial attacks, unethical usage, and societal harm. Recognizing these challenges, researchers have increasingly focused on developing methodologies to detect synthesized data effectively, aiming to mitigate potential risks. Prior reviews have primarily focused on deepfake detection and often lack coverage of recent advancements in synthetic image detection, particularly methods leveraging multimodal frameworks for improved forensic analysis. To address this gap, the present survey provides a comprehensive review of state-of-the-art methods for detecting and classifying synthetic images generated by advanced generative AI models. This review systematically examines core detection methodologies, identifies commonalities among approaches, and categorizes them into meaningful taxonomies. Furthermore, given the crucial role of large-scale datasets in this field, we present an overview of publicly available datasets that facilitate further research and benchmarking in synthetic data detection. 

**Abstract (ZH)**: 生成模型，如生成对抗网络（GANs）、扩散模型和变分自编码器（VAEs）的泛滥，使得高质量多媒体数据的合成成为可能。然而，这些进展同时也引发了关于对抗攻击、不道德使用以及社会危害的重大关切。认识到这些挑战后，研究人员越来越关注开发有效检测合成数据的方法，旨在减轻潜在风险。之前的综述主要集中在深伪检测方面，往往忽略了合成图像检测领域的最新进展，尤其是利用多模态框架进行增强的法医分析方法。为了填补这一空白，本综述全面回顾了检测和分类由先进生成AI模型生成的合成图像的最新方法。本综述系统地检查了核心检测方法，找到了不同方法之间的共通之处，并将它们归类为有意义的分类体系。此外，鉴于大规模数据集在该领域中的关键作用，本文还介绍了公开可用的数据集，以促进合成数据检测领域的进一步研究和基准测试。 

---
# CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models 

**Title (ZH)**: CurricuVLM：通过基于视觉-语言模型的个性化安全关键课程学习实现安全自动驾驶 

**Authors**: Zihao Sheng, Zilin Huang, Yansong Qu, Yue Leng, Sruthi Bhavanam, Sikai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.15119)  

**Abstract**: Ensuring safety in autonomous driving systems remains a critical challenge, particularly in handling rare but potentially catastrophic safety-critical scenarios. While existing research has explored generating safety-critical scenarios for autonomous vehicle (AV) testing, there is limited work on effectively incorporating these scenarios into policy learning to enhance safety. Furthermore, developing training curricula that adapt to an AV's evolving behavioral patterns and performance bottlenecks remains largely unexplored. To address these challenges, we propose CurricuVLM, a novel framework that leverages Vision-Language Models (VLMs) to enable personalized curriculum learning for autonomous driving agents. Our approach uniquely exploits VLMs' multimodal understanding capabilities to analyze agent behavior, identify performance weaknesses, and dynamically generate tailored training scenarios for curriculum adaptation. Through comprehensive analysis of unsafe driving situations with narrative descriptions, CurricuVLM performs in-depth reasoning to evaluate the AV's capabilities and identify critical behavioral patterns. The framework then synthesizes customized training scenarios targeting these identified limitations, enabling effective and personalized curriculum learning. Extensive experiments on the Waymo Open Motion Dataset show that CurricuVLM outperforms state-of-the-art baselines across both regular and safety-critical scenarios, achieving superior performance in terms of navigation success, driving efficiency, and safety metrics. Further analysis reveals that CurricuVLM serves as a general approach that can be integrated with various RL algorithms to enhance autonomous driving systems. The code and demo video are available at: this https URL. 

**Abstract (ZH)**: 确保自动驾驶系统的安全性仍然是一个关键的挑战，尤其是在处理那些虽然稀少但可能导致灾难性后果的安全关键场景时。尽管现有研究已经探索了生成自动驾驶汽车（AV）测试所需的安全关键场景，但将这些场景有效地融入策略学习以增强安全性的相关工作仍然较为有限。此外，开发能够适应自动驾驶车辆不断演变的行为模式和性能瓶颈的训练课程依然鲜有研究。为了解决这些挑战，我们提出了CurricuVLM框架，这是一种新颖的方法，利用视觉语言模型（VLMs）来实现个性化课程学习，为自动驾驶代理提供定制化的训练课程。我们的方法独特地利用了VLMs的多模态理解能力，分析代理行为，识别性能短板，并动态生成定制化的训练场景以适应课程学习。通过综合分析包含叙事描述的不安全驾驶情况，CurricuVLM深入地进行推理，评估自动驾驶汽车的能力，并识别关键的行为模式。然后，框架综合生成针对性的定制化训练场景，针对识别出的局限性，实现有效和个性化的课程学习。在Waymo Open Motion数据集上的广泛实验表明，CurricuVLM在常规和安全关键场景中均优于最新的基准，其在导航成功、驾驶效率和安全指标上的表现更加出色。进一步的分析表明，CurricuVLM可以作为一种通用方法，与各种强化学习（RL）算法结合，以增强自动驾驶系统。感兴趣的读者可以通过以下链接获取相关代码和演示视频：[此处插入链接]。 

---
# Sce2DriveX: A Generalized MLLM Framework for Scene-to-Drive Learning 

**Title (ZH)**: Sce2DriveX：一种场景到驾驶学习的一般化多模态模型框架 

**Authors**: Rui Zhao, Qirui Yuan, Jinyu Li, Haofeng Hu, Yun Li, Chengyuan Zheng, Fei Gao  

**Link**: [PDF](https://arxiv.org/pdf/2502.14917)  

**Abstract**: End-to-end autonomous driving, which directly maps raw sensor inputs to low-level vehicle controls, is an important part of Embodied AI. Despite successes in applying Multimodal Large Language Models (MLLMs) for high-level traffic scene semantic understanding, it remains challenging to effectively translate these conceptual semantics understandings into low-level motion control commands and achieve generalization and consensus in cross-scene driving. We introduce Sce2DriveX, a human-like driving chain-of-thought (CoT) reasoning MLLM framework. Sce2DriveX utilizes multimodal joint learning from local scene videos and global BEV maps to deeply understand long-range spatiotemporal relationships and road topology, enhancing its comprehensive perception and reasoning capabilities in 3D dynamic/static scenes and achieving driving generalization across scenes. Building on this, it reconstructs the implicit cognitive chain inherent in human driving, covering scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, thereby further bridging the gap between autonomous driving and human thought processes. To elevate model performance, we have developed the first extensive Visual Question Answering (VQA) driving instruction dataset tailored for 3D spatial understanding and long-axis task reasoning. Extensive experiments demonstrate that Sce2DriveX achieves state-of-the-art performance from scene understanding to end-to-end driving, as well as robust generalization on the CARLA Bench2Drive benchmark. 

**Abstract (ZH)**: 端到端自主驾驶，它直接将原始传感器输入映射到低级车辆控制，是具身人工智能的重要组成部分。尽管在利用多模态大型语言模型（MLLMs）进行高层次交通场景语义理解方面取得了一定的成果，但将这些概念性语义理解有效地转化为低级运动控制命令，并跨场景实现普遍性和一致性仍然颇具挑战性。我们提出了Sce2DriveX，这是一种类人的驾驶链式思考（CoT）推理MLLM框架。Sce2DriveX 利用局部场景视频和全局BEV图的多模态联合学习，深入理解长期时空关系和道路拓扑，从而增强其在3D动态/静态场景中的全面感知和推理能力，并实现了跨场景的驾驶通用性。在此基础上，它重建了人类驾驶中固有的隐式认知链，涵盖了场景理解、元行为推理、行为解释分析、运动规划和控制，从而进一步弥合了自动驾驶与人类思维过程之间的差距。为了提升模型性能，我们开发了首个针对3D空间理解和长轴任务推理定制的专业驾驶指令视觉问答（VQA）数据集。广泛的实验表明，Sce2DriveX 在从场景理解到端到端驾驶的过程中实现了最先进的性能，并在CARLA Bench2Drive基准测试中表现出强大的泛化能力。 

---
# NOTA: Multimodal Music Notation Understanding for Visual Large Language Model 

**Title (ZH)**: 注意：面向视觉大型语言模型的多模态音乐符号理解 

**Authors**: Mingni Tang, Jiajia Li, Lu Yang, Zhiqiang Zhang, Jinghao Tian, Zuchao Li, Lefei Zhang, Ping Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14893)  

**Abstract**: Symbolic music is represented in two distinct forms: two-dimensional, visually intuitive score images, and one-dimensional, standardized text annotation sequences. While large language models have shown extraordinary potential in music, current research has primarily focused on unimodal symbol sequence text. Existing general-domain visual language models still lack the ability of music notation understanding. Recognizing this gap, we propose NOTA, the first large-scale comprehensive multimodal music notation dataset. It consists of 1,019,237 records, from 3 regions of the world, and contains 3 tasks. Based on the dataset, we trained NotaGPT, a music notation visual large language model. Specifically, we involve a pre-alignment training phase for cross-modal alignment between the musical notes depicted in music score images and their textual representation in ABC notation. Subsequent training phases focus on foundational music information extraction, followed by training on music notation analysis. Experimental results demonstrate that our NotaGPT-7B achieves significant improvement on music understanding, showcasing the effectiveness of NOTA and the training pipeline. Our datasets are open-sourced at this https URL. 

**Abstract (ZH)**: 符号音乐以两种不同的形式表示：二维、直观的乐谱图和一维、标准化的文本注释序列。虽然大型语言模型在音乐方面展现出了巨大的潜力，但当前研究主要集中在单一模态的符号序列文本上。现有的通用领域视觉语言模型在音乐记谱理解方面仍存在不足。鉴于这一空白，我们提出了NOTA，这是第一个大规模综合多模态音乐记谱数据集。该数据集包含1,019,237条记录，来自三个不同的地区，并包含三个任务。基于该数据集，我们训练了NotaGPT，这是一种音乐记谱视觉大型语言模型。具体而言，我们在预对齐训练阶段涉及跨模态对齐，将音乐谱图中表示的乐谱符号与其ABC表示的文本形式进行对齐。后续的训练阶段集中在基础音乐信息提取上，随后进行音乐记谱分析的训练。实验结果表明，我们的NotaGPT-7B在音乐理解方面取得了显著的改进，证明了NOTA和训练管道的有效性。我们的数据集在此处开放获取：[这里请提供正确的URL链接]。 

---
# EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild 

**Title (ZH)**: EgoSpeak：学习自中心对话代理在野生环境下的发言时机 

**Authors**: Junhyeok Kim, Min Soo Kim, Jiwan Chung, Jungbin Cho, Jisoo Kim, Sungwoong Kim, Gyeongbo Sim, Youngjae Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.14892)  

**Abstract**: Predicting when to initiate speech in real-world environments remains a fundamental challenge for conversational agents. We introduce EgoSpeak, a novel framework for real-time speech initiation prediction in egocentric streaming video. By modeling the conversation from the speaker's first-person viewpoint, EgoSpeak is tailored for human-like interactions in which a conversational agent must continuously observe its environment and dynamically decide when to talk. Our approach bridges the gap between simplified experimental setups and complex natural conversations by integrating four key capabilities: (1) first-person perspective, (2) RGB processing, (3) online processing, and (4) untrimmed video processing. We also present YT-Conversation, a diverse collection of in-the-wild conversational videos from YouTube, as a resource for large-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that EgoSpeak outperforms random and silence-based baselines in real time. Our results also highlight the importance of multimodal input and context length in effectively deciding when to speak. 

**Abstract (ZH)**: 在实际环境中介定何时启动对话仍然是对话代理面临的一项基本挑战。我们提出了EgoSpeak，这是一种新型框架，用于实现实时自视角流媒体视频中的启动对话预测。通过从发言人的第一人称视角建模对话，EgoSpeak特别适用于人类交互场景，其中对话代理必须持续观察环境并动态决定何时进行对话。我们的方法通过整合四项关键技术特性，弥合了简化实验设置与复杂自然对话之间的差距：（1）第一人称视角、（2）RGB图像处理、（3）在线处理和（4）未剪辑视频处理。我们还介绍了YT-Conversation，这是一个从YouTube收集的多样化的野外对话视频集合，作为大规模预训练的资源。在EasyCom和Ego4D上的实验结果表明，EgoSpeak在实时情况下优于随机和沉默基线。我们的结果还强调了多模态输入和上下文长度在有效决定何时发言方面的重要性。 

---
# Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability 

**Title (ZH)**: 窄化信息瓶颈理论在多模态图像-文本表示可解释性中的应用 

**Authors**: Zhiyu Zhu, Zhibo Jin, Jiayu Zhang, Nan Yang, Jiahao Huang, Jianlong Zhou, Fang Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.14889)  

**Abstract**: The task of identifying multimodal image-text representations has garnered increasing attention, particularly with models such as CLIP (Contrastive Language-Image Pretraining), which demonstrate exceptional performance in learning complex associations between images and text. Despite these advancements, ensuring the interpretability of such models is paramount for their safe deployment in real-world applications, such as healthcare. While numerous interpretability methods have been developed for unimodal tasks, these approaches often fail to transfer effectively to multimodal contexts due to inherent differences in the representation structures. Bottleneck methods, well-established in information theory, have been applied to enhance CLIP's interpretability. However, they are often hindered by strong assumptions or intrinsic randomness. To overcome these challenges, we propose the Narrowing Information Bottleneck Theory, a novel framework that fundamentally redefines the traditional bottleneck approach. This theory is specifically designed to satisfy contemporary attribution axioms, providing a more robust and reliable solution for improving the interpretability of multimodal models. In our experiments, compared to state-of-the-art methods, our approach enhances image interpretability by an average of 9%, text interpretability by an average of 58.83%, and accelerates processing speed by 63.95%. Our code is publicly accessible at this https URL. 

**Abstract (ZH)**: 多模态图像-文本表示的识别任务正逐渐受到广泛关注，特别是在CLIP（对比语言-图像预训练）等模型的应用中，这些模型在学习图像与文本之间的复杂关联方面表现出色。尽管取得了这些进展，确保此类模型的可解释性对于在现实世界应用（如医疗健康领域）中的安全部署依然至关重要。尽管已经开发了许多用于单模态任务的可解释性方法，但这些方法往往难以有效地推广到多模态上下文，原因在于两者在表示结构上的固有差异。信息理论中已验证的瓶颈方法曾被用来增强CLIP的可解释性，但它们常常受到强假设或内在随机性的影响。为了克服这些挑战，我们提出了信息瓶颈理论收窄方法的新框架，该框架从根本上重新定义了传统的瓶颈方法。该理论特别设计以满足当前的归因公理，提供了一个更稳健和可靠的方法来提高多模态模型的可解释性。在我们的实验中，与最先进的方法相比，我们提出的方法在图像可解释性方面平均提高了9%，在文本可解释性方面平均提高了58.83%，并且使处理速度加快了63.95%。我们的代码可以在此处获取：[此 https URL] 

---
# The Multi-Faceted Monosemanticity in Multimodal Representations 

**Title (ZH)**: 多模态表示中的多面向单义性 

**Authors**: Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, Yulan He, Yifei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.14888)  

**Abstract**: In this paper, we leverage recent advancements in feature monosemanticity to extract interpretable features from deep multimodal models, offering a data-driven understanding of modality gaps. Specifically, we investigate CLIP (Contrastive Language-Image Pretraining), a prominent visual-language representation model trained on extensive image-text pairs. Building upon interpretability tools developed for single-modal models, we extend these methodologies to assess multi-modal interpretability of CLIP features. Additionally, we introduce the Modality Dominance Score (MDS) to attribute the interpretability of each feature to its respective modality. Next, we transform CLIP features into a more interpretable space, enabling us to categorize them into three distinct classes: vision features (single-modal), language features (single-modal), and visual-language features (cross-modal). Our findings reveal that this categorization aligns closely with human cognitive understandings of different modalities. We also demonstrate significant use cases of this modality-specific features including detecting gender bias, adversarial attack defense and text-to-image model editing. These results indicate that large-scale multimodal models, equipped with task-agnostic interpretability tools, offer valuable insights into key connections and distinctions between different modalities. 

**Abstract (ZH)**: 在本文中，我们充分利用了特征单_semanticity（语义性）的最新进展，从深层多模态模型中提取可解释特征，从而提供数据驱动的模态间隙理解。具体而言，我们研究了CLIP（对比语言-图像预训练）模型，这是一种在大量图像-文本配对上训练的突出视觉-语言表示模型。在借鉴单模态模型解释工具的基础上，我们将其拓展到评估CLIP特征的多模态解释性。此外，我们引入了模态主导分数（MDS）来将每个特征的解释性归因于相应的模态。随后，我们将CLIP特征转换到一个更易解释的空间中，使我们能够将其分为三个不同的类别：视觉特征（单一模态）、语言特征（单一模态）和视觉-语言特征（跨模态）。我们的研究结果表明，这种分类与不同模态的人类认知理解高度一致。我们还展示了这类模态特定特征的重大应用案例，包括检测性别偏见、对抗性攻击防御和文本到图像模型编辑。这些发现表明，具有任务无关解释工具的大规模多模态模型提供了关于不同模态之间关键关联和区分的重要洞察。 

---
