# Can Large Language Models Predict the Outcome of Judicial Decisions? 

**Title (ZH)**: 大型语言模型能否预测司法决策的结果？ 

**Authors**: Mohamed Bayan Kmainasi, Ali Ezzat Shahroor, Amani Al-Ghraibah  

**Link**: [PDF](https://arxiv.org/pdf/2501.09768)  

**Abstract**: Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quantitative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在自然语言处理（NLP）的各个领域中展现了出色的能力。然而，它们在低资源语言如阿拉伯语的专门任务，如法律判决预测（LJP）中的应用仍然鲜有探索。本文通过开发一个从沙特商业法庭判决中收集并预处理的阿拉伯语LJP数据集，填补了这一空白。我们基准测试了包括LLaMA-3.2-3B和LLaMA-3.1-8B在内的最先进的开源LLMs，在零样本、单样本和使用QLoRA微调的不同配置下进行评估。此外，我们采用了一种综合评估框架，结合定量指标（BLEU和ROUGE）和定性评估（连贯性、法律语言、清晰度）进行全面评估。实验结果表明，在任务特定的场景中，微调后的较小模型能够达到与较大模型相当的性能，同时具有显著的资源效率优势。此外，我们还探讨了提示工程和微调对模型输出的影响，揭示了性能变化和指令敏感性的见解。通过公开该数据集、实现代码和模型，我们为进一步开展阿拉伯语法律NLP研究奠定了坚实的基础。 

---
# Agent-as-Judge for Factual Summarization of Long Narratives 

**Title (ZH)**: 将下面的论文内容或标题翻译成中文，并确保符合学术规范：

**Agent-as-Judge for Factual Summarization of Long Narratives**

**作为裁判的代理：长叙事事实总结**

在这个翻译中，“Agent”是指执行某种任务或决策的实体，“as”在此处作为介词，表明角色或功能的转变，“Judge”在这里可以理解为“裁判”，指的是对事物进行判断的角色。因此，“Agent-as-Judge”可以翻译为“作为裁判的代理”。整体标题翻译时考虑到学术规范和表达的准确性，确保意思清晰、专业。 

**Authors**: Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, Byung-Hak Kim  

**Link**: [PDF](https://arxiv.org/pdf/2501.09993)  

**Abstract**: Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel "Agent-as-a-Judge" framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries. 

**Abstract (ZH)**: 大语言模型（LLMs）在传统评测指标如ROUGE和BERTScore的基础上，在摘要任务上已经展现出了接近人类的性能。然而，这些指标并未充分捕捉到摘要质量的关键方面，特别是对于长篇叙述（超过10万tokens）中的事实准确性。最近的进步，如LLM-as-a-Judge，解决了基于词义相似性的指标的局限性，但仍存在事实不一致的问题，特别是在理解角色关系和状态方面。在这项工作中，我们引入了NarrativeFactScore框架，这是一种新的“Agent-as-a-Judge”评估和优化摘要的方法。通过利用从输入和生成的摘要中提取的Character Knowledge Graph（CKG），NarrativeFactScore评估了事实一致性，并提供了改进的实用指导，如识别缺失或错误的事实。我们通过详尽的工作流程说明和广泛验证，在广泛使用的基准数据集上展示了NarrativeFactScore的有效性，其性能优于竞争方法。我们的结果显示，基于代理的评估系统有提高LLM生成摘要的事实可靠性的潜力。 

---
