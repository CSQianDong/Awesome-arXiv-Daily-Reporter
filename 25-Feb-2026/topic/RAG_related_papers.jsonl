{'arxiv_id': 'arXiv:2602.20735', 'title': 'RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition', 'authors': 'Kun Ran, Marwah Alaofi, Danula Hettiachchi, Chenglong Ma, Khoi Nguyen Dinh Anh, Khoi Vo Nguyen, Sachin Pathiyan Cherumanal, Lida Rashidi, Falk Scholer, Damiano Spina, Shuoqi Sun, Oleg Zendel', 'link': 'https://arxiv.org/abs/2602.20735', 'abstract': 'This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\ntrack of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n(R2RAG), a research-focused retrieval-augmented generation (RAG)\narchitecture composed of lightweight components that dynamically adapt the\nretrieval strategy based on inferred query complexity and evidence\nsufficiency. The system uses smaller LLMs, enabling operation on a single\nconsumer-grade GPU while supporting complex research tasks. It builds on the\nG-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\nwith modules informed by qualitative review of outputs. R2RAG won the Best\nDynamic Evaluation award in the Open Source category, demonstrating high\neffectiveness with careful design and efficient use of resources.'}
{'arxiv_id': 'arXiv:2602.20379', 'title': 'Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems', 'authors': 'Mukul Chhabra, Luigi Medrano, Arush Verma', 'link': 'https://arxiv.org/abs/2602.20379', 'abstract': 'Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.\nWe present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.\nThrough a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.'}
