# Investigating Corporate Social Responsibility Initiatives: Examining the case of corporate Covid-19 response 

**Title (ZH)**: 探究企业社会责任举措：以企业新冠病毒疫情应对为例 

**Authors**: Meheli Basu, Aniruddha Dutta, Purvi Shah  

**Link**: [PDF](https://arxiv.org/pdf/2502.03421)  

**Abstract**: In todays age of freely available information, policy makers have to take into account a huge amount of information while making decisions affecting relevant stakeholders. While increase in the amount of information sources and documents increases credibility of decisions based on the corpus of available text, it is challenging for policymakers to make sense of this information. This paper demonstrates how policy makers can implement some of the most popular topic recognition methods, Latent Dirichlet Allocation, Deep Distributed Representation method, text summarization approaches, Word Based Sentence Ranking method and TextRank for sentence extraction method, to sum up the content of large volume of documents to understand the gist of the overload of information. We have applied popular NLP methods to corporate press releases during the early period and advanced period of Covid-19 pandemic which has resulted in a global unprecedented health and socio-economic crisis, when policymaking and regulations have become especially important to standardize corporate practices for employee and social welfare in the face of similar future unseen crises. The steps undertaken in this study can be replicated to yield insights from relevant documents in any other social decision-making context. 

**Abstract (ZH)**: 在当今信息自由获取的时代，政策制定者在做出影响相关利益相关者决策时，必须考虑大量的信息。随着信息来源和文档数量的增加，基于现有文本语料库的决策可信度提高，但政策制定者在理解这些信息方面面临着挑战。本文展示了政策制定者如何应用一些最流行的主题识别方法，如隐性狄利克雷分配（Latent Dirichlet Allocation, LDA）、深度分布式表征方法、文本摘要方法、基于词的句子排名方法和TextRank句提取方法，来总结大量文档的内容，以便理解信息过载的核心。我们已将流行的自然语言处理（NLP）方法应用于新冠肺炎疫情初期和疫情加剧期间的公司新闻稿，当时疫情导致全球前所未有的健康和社会经济危机，决策和法规变得尤为重要，以标准化企业在面对类似未来未见危机时的实践，以保障员工和公众的利益。本研究中采取的步骤可以复制应用，以从任何其他社会决策背景的相关文档中获取见解。 

---
# DenseReviewer: A Screening Prioritisation Tool for Systematic Review based on Dense Retrieval 

**Title (ZH)**: DenseReviewer：一种基于密集检索的系统评价筛选优先级工具 

**Authors**: Xinyu Mao, Teerapong Leelanupab, Harrisen Scells, Guido Zuccon  

**Link**: [PDF](https://arxiv.org/pdf/2502.03400)  

**Abstract**: Screening is a time-consuming and labour-intensive yet required task for medical systematic reviews, as tens of thousands of studies often need to be screened. Prioritising relevant studies to be screened allows downstream systematic review creation tasks to start earlier and save time. In previous work, we developed a dense retrieval method to prioritise relevant studies with reviewer feedback during the title and abstract screening stage. Our method outperforms previous active learning methods in both effectiveness and efficiency. In this demo, we extend this prior work by creating (1) a web-based screening tool that enables end-users to screen studies exploiting state-of-the-art methods and (2) a Python library that integrates models and feedback mechanisms and allows researchers to develop and demonstrate new active learning methods. We describe the tool's design and showcase how it can aid screening. The tool is available at this https URL. The source code is also open sourced at this https URL. 

**Abstract (ZH)**: 筛选是医学系统评价中一项耗时且劳动密集型但又是必不可少的任务，因为往往需要筛选大量（数万篇）研究。优先筛选相关研究可以允许后续系统评价工作提前开始，并节省时间。在之前的工作中，我们开发了一种密集检索方法，在标题和摘要筛选阶段利用审稿人的反馈信息来优先筛选相关研究。我们的方法在效果和效率上均优于之前的主动学习方法。在本次演示中，我们在此前工作的基础上，扩展了如下几点：（1）开发了一款基于Web的研究筛选工具，该工具利用最先进的方法帮助最终用户筛选研究；（2）创建了一个Python库，该库整合了模型和反馈机制，允许研究人员开发和演示新的主动学习方法。我们介绍了该工具的设计，并展示了它如何辅助筛选工作。该工具的访问地址为：[该链接]。源代码也已在[该链接]开源。 

---
# Interactive Visualization Recommendation with Hier-SUCB 

**Title (ZH)**: 基于Hier-SUCB的交互式可视化推荐 

**Authors**: Songwen Hu, Ryan A. Rossi, Tong Yu, Junda Wu, Handong Zhao, Sungchul Kim, Shuai Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.03375)  

**Abstract**: Visualization recommendation aims to enable rapid visual analysis of massive datasets. In real-world scenarios, it is essential to quickly gather and comprehend user preferences to cover users from diverse backgrounds, including varying skill levels and analytical tasks. Previous approaches to personalized visualization recommendations are non-interactive and rely on initial user data for new users. As a result, these models cannot effectively explore options or adapt to real-time feedback. To address this limitation, we propose an interactive personalized visualization recommendation (PVisRec) system that learns on user feedback from previous interactions. For more interactive and accurate recommendations, we propose Hier-SUCB, a contextual combinatorial semi-bandit in the PVisRec setting. Theoretically, we show an improved overall regret bound with the same rank of time but an improved rank of action space. We further demonstrate the effectiveness of Hier-SUCB through extensive experiments where it is comparable to offline methods and outperforms other bandit algorithms in the setting of visualization recommendation. 

**Abstract (ZH)**: 视觉推荐旨在实现大规模数据集的快速视觉分析。在实际应用场景中，迅速收集和理解用户偏好至关重要，以涵盖具有不同背景的用户，包括不同技能水平和分析任务。之前的个性化视觉推荐方法是非交互式的，并依赖于初始用户数据来满足新用户的需求。因此，这些模型无法有效地探索选项或适应实时反馈。为解决这一局限，我们提出了一种基于用户反馈学习的交互式个性化视觉推荐（PVisRec）系统。为了提供更交互和准确的推荐，我们提出了Hier-SUCB，这是一种在PVisRec设置下的上下文组合半带宽方法。理论上，我们展示了在相同的时间复杂度下提高了整体遗憾累积度，同时在操作空间复杂度上有所改进。此外，通过广泛的实验表明，Hier-SUCB与离线方法相当，并在视觉推荐情境下优于其他带宽算法。 

---
# Intent Representation Learning with Large Language Model for Recommendation 

**Title (ZH)**: 使用大型语言模型进行意图表示学习的推荐方法 

**Authors**: Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.03307)  

**Abstract**: Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines. The implementation is available at this https URL. 

**Abstract (ZH)**: 基于意图的推荐系统因其在揭示隐含的精细偏好方面取得显著成效而受到广泛关注。意图作为交互的基础因素，对于提高推荐的可解释性至关重要。大多数方法将意图定义为在交互过程中可学习的参数。然而，现有框架往往忽视了文本信息（例如用户评论和商品描述），这些信息对于缓解交互意图稀疏性至关重要。探索这些多模态意图，特别是它们在表示空间中的固有差异，面临两个关键挑战：i) 如何对齐多模态意图并有效减轻噪声问题；ii) 如何提取和匹配各模态中的潜在关键意图。为了解决这些挑战，我们提出了一种模型无关的框架，即基于大型语言模型的意图表示学习（IRLLRec），该框架利用大型语言模型（LLMs）构建多模态意图并增强推荐效果。具体而言，IRLLRec 采用双重塔结构学习多模态意图表示。接下来，我们提出了成对和翻译对齐，以消除不同模态之间的差异并增强对噪声输入特征的鲁棒性。最后，为了更好地匹配基于文本和交互的意图，我们利用动量蒸馏技术，在融合意图表示上进行教师-学生学习。在三个数据集上的实证评估表明，我们的IRLLRec框架优于基准模型。开源代码可在以下链接获取：this https URL。 

---
# Data Dams: A Novel Framework for Regulating and Managing Data Flow in Large-Scale Systems 

**Title (ZH)**: 数据堤坝：一种新型框架，用于调节和管理大规模系统中的数据流动 

**Authors**: Mohamed Aly Bouke, Azizol Abdullah, Korhan Cengiz, Nikola Ivković, Ivan Mihaljević, Mudathir Ahmed Mohamud, Ahmed Kowrina  

**Link**: [PDF](https://arxiv.org/pdf/2502.03218)  

**Abstract**: In the era of big data, managing dynamic data flows efficiently is crucial as traditional storage models struggle with real-time regulation and risk overflow. This paper introduces Data Dams, a novel framework designed to optimize data inflow, storage, and outflow by dynamically adjusting flow rates to prevent congestion while maximizing resource utilization. Inspired by physical dam mechanisms, the framework employs intelligent sluice controls and predictive analytics to regulate data flow based on system conditions such as bandwidth availability, processing capacity, and security constraints. Simulation results demonstrate that the Data Dam significantly reduces average storage levels (371.68 vs. 426.27 units) and increases total outflow (7999.99 vs. 7748.76 units) compared to static baseline models. By ensuring stable and adaptive outflow rates under fluctuating data loads, this approach enhances system efficiency, mitigates overflow risks, and outperforms existing static flow control strategies. The proposed framework presents a scalable solution for dynamic data management in large-scale distributed systems, paving the way for more resilient and efficient real-time processing architectures. 

**Abstract (ZH)**: 在大数据时代，高效管理动态数据流至关重要，因为传统的存储模型难以实现实时调节并避免数据溢出。本文介绍了一种名为“Data Dams”的新颖框架，该框架通过动态调整数据流速来预防堵塞，同时最大化资源利用率。该框架借鉴了物理水坝机制，采用智能泄洪控制和预测分析手段，根据系统条件（如带宽可用性、处理能力和安全约束）动态调节数据流。模拟实验结果显示，与静态基准模型相比，Data Dams显著降低了平均存储水平（371.68 vs. 426.27单位），并增加了总数据流出量（7999.99 vs. 7748.76单位）。通过在数据负载波动情况下确保稳定且适应性的数据流出速率，这种方法提高了系统的效率，减少了溢出风险，并优于现有的静态流控策略。所提出的框架为大规模分布式系统中的动态数据管理提供了可扩展的解决方案，为更强大和高效的实时处理架构铺平了道路。 

---
# Scientometric Analysis of the German IR Community within TREC & CLEF 

**Title (ZH)**: 德语信息检索社区在TREC和CLEF中的科学计量分析 

**Authors**: A. K. Kruff, P. Schaer  

**Link**: [PDF](https://arxiv.org/pdf/2502.03065)  

**Abstract**: Within this study, the influence of the German Information Retrieval community on the retrieval campaigns Text Retrieval Conference (TREC) and Conference and Labs of the Evaluation Forum (CLEF) between 2000 and 2022 was analyzed based on metadata provided by OpenAlex and further metadata extracted with the GROBID framework from the publication's full texts. The analysis was conducted at the institutional and researcher levels. It was found that the German IR community, both on the author and institution level, mainly contributed to CLEF. Furthermore, it was shown that productivity follows the assumptions made by Lotka's Law. 

**Abstract (ZH)**: 在本研究中，基于OpenAlex提供的元数据以及通过GROBID框架从全文中提取的进一步元数据，分析了2000年至2022年间德国信息检索社区对Text Retrieval Conference（TREC）和Conference and Labs of the Evaluation Forum（CLEF）检索活动的影响。该分析在机构和研究人员的层面进行。研究发现，德国信息检索社区在作者和机构层面主要贡献于CLEF。此外，研究还表明，生产力遵循Lotka定律的假设。 

---
# Large Language Models Are Universal Recommendation Learners 

**Title (ZH)**: 大型语言模型是通用推荐学习者 

**Authors**: Junguang Jiang, Yanwen Huang, Bin Liu, Xiaoyu Kong, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2502.03041)  

**Abstract**: In real-world recommender systems, different tasks are typically addressed using supervised learning on task-specific datasets with carefully designed model architectures. We demonstrate that large language models (LLMs) can function as universal recommendation learners, capable of handling multiple tasks within a unified input-output framework, eliminating the need for specialized model designs. To improve the recommendation performance of LLMs, we introduce a multimodal fusion module for item representation and a sequence-in-set-out approach for efficient candidate generation. When applied to industrial-scale data, our LLM achieves competitive results with expert models elaborately designed for different recommendation tasks. Furthermore, our analysis reveals that recommendation outcomes are highly sensitive to text input, highlighting the potential of prompt engineering in optimizing industrial-scale recommender systems. 

**Abstract (ZH)**: 在实际世界中的推荐系统中，不同的任务通常通过在具体任务数据集上进行监督学习并精心设计模型架构来解决。我们展示了大型语言模型（LLMs）可以作为通用的推荐学习者，在统一的输入-输出框架中处理多种任务，从而消除专门模型设计的需要。为了提高LLMs的推荐性能，我们引入了一种多模态融合模块来表示物品，并采用序列-集输入-输出的方法进行高效的候选生成。当应用于大规模工业数据时，我们的LLM在推荐性能上能够与为不同推荐任务精心设计的专家模型相媲美。此外，我们的分析表明，推荐结果对文本输入高度敏感，这突显了提示工程在优化大规模工业推荐系统方面的潜在价值。 

---
# FuXi-$\alpha$: Scaling Recommendation Model with Feature Interaction Enhanced Transformer 

**Title (ZH)**: FuXi-$\alpha$: 基于特征交互增强的变压器规模化推荐模型 

**Authors**: Yufei Ye, Wei Guo, Jin Yao Chin, Hao Wang, Hong Zhu, Xi Lin, Yuyang Ye, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.03036)  

**Abstract**: Inspired by scaling laws and large language models, research on large-scale recommendation models has gained significant attention. Recent advancements have shown that expanding sequential recommendation models to large-scale recommendation models can be an effective strategy. Current state-of-the-art sequential recommendation models primarily use self-attention mechanisms for explicit feature interactions among items, while implicit interactions are managed through Feed-Forward Networks (FFNs). However, these models often inadequately integrate temporal and positional information, either by adding them to attention weights or by blending them with latent representations, which limits their expressive power. A recent model, HSTU, further reduces the focus on implicit feature interactions, constraining its performance. We propose a new model called FuXi-$\alpha$ to address these issues. This model introduces an Adaptive Multi-channel Self-attention mechanism that distinctly models temporal, positional, and semantic features, along with a Multi-stage FFN to enhance implicit feature interactions. Our offline experiments demonstrate that our model outperforms existing models, with its performance continuously improving as the model size increases. Additionally, we conducted an online A/B test within the Huawei Music app, which showed a $4.76\%$ increase in the average number of songs played per user and a $5.10\%$ increase in the average listening duration per user. Our code has been released at this https URL. 

**Abstract (ZH)**: 受规模律和大规模语言模型的启发，大规模推荐模型的研究受到了广泛关注。近期的研究表明，将序列推荐模型扩展为大规模推荐模型可以是一种有效的策略。当前最先进的序列推荐模型主要利用自注意力机制对项目间的显式特征进行交互，而隐式交互则通过前馈网络（FFNs）进行管理。然而，这些模型往往未能充分整合时间性和位置性信息，要么将这些信息添加到注意力权重中，要么与潜在表示混合，这限制了它们的表达能力。最近的一种模型HSTU进一步减少了对隐式特征交互的关注，从而限制了其性能。我们提出了一种新的模型，称为FuXi-$\alpha$，以解决这些问题。该模型引入了自适应多通道自注意力机制，能够分别建模时间性、位置性和语义特征，并且引入了多阶段前馈网络以增强隐式特征交互。我们的离线实验表明，该模型在各种指标上优于现有模型，且其性能随着模型规模的增加而持续提高。此外，我们在华为音乐应用中进行了在线A/B测试，结果显示用户平均播放的歌曲数量增加了4.76%，平均播放时长增加了5.10%。我们的代码已在此处发布：[此链接]。 

---
# Assessing Research Impact in Indian Conference Proceedings: Insights from Collaboration and Citations 

**Title (ZH)**: 印度会议论文中的研究影响评估：合作与引用的洞见 

**Authors**: Kiran Sharma, Parul Khurana  

**Link**: [PDF](https://arxiv.org/pdf/2502.02997)  

**Abstract**: Conferences serve as a crucial avenue for scientific communication. However, the increase in conferences and the subsequent publication of proceedings have prompted inquiries regarding the research quality being showcased at such events. This investigation delves into the conference publications indexed by Springer's Lecture Notes in Networks and Systems Series. Among the 570 international conferences held worldwide in this series, 177 were exclusively hosted in India. These 177 conferences collectively published 11,066 papers as conference proceedings. All these publications, along with conference details, were sourced from the Scopus database. The study aims to evaluate the research impact of these conference proceedings and identify the primary contributors. The results reveal a downward trend in the average number of citations per year. The collective average citation for all publications is 1.01. Papers co-authored by Indian and international authors (5.6%) exhibit a higher average impact of 1.44, in contrast to those authored solely by Indian authors (84.9%), which have an average impact of 0.97. Notably, Indian-collaborated papers, among the largest contributors, predominantly originate from private colleges and universities. Only 19% of papers exhibit collaboration with institutes of different prestige, yet their impact is considerably higher as compared to collaboration with institutes of similar prestige. This study highlights the importance of improving research quality in academic forums. 

**Abstract (ZH)**: 会议是科学交流的重要渠道。然而，会议数量的增加以及随之而来的会议论文出版，引发了关于在这些会议中展示的研究质量的质疑。本研究深入探讨了由Springer的《网络与系统讲义系列》索引的会议出版物。在该系列中，全球举办了570次国际会议，其中177次仅在印度举办。这些177次会议总共发表了11,066篇会议论文。所有这些出版物及其会议详情均来源于Scopus数据库。本研究旨在评估这些会议论文的研究影响力，并识别主要贡献者。研究结果表明，每篇论文的平均引用次数呈下降趋势。所有论文的总平均引文数为1.01。由印度作者与国际作者共同撰写（5.6%）的论文表现出更高的平均影响，为1.44，相比之下，仅由印度作者单独撰写（84.9%）的论文平均影响为0.97。值得注意的是，作为主要贡献者之一的联合撰写的论文大多来源于私立学院和大学。仅19%的论文与其他声誉不同的机构进行了合作，但这些合作论文的影响力显著高于与其他声誉相似机构的合作。本研究强调了改进学术论坛中研究质量的重要性。 

---
# FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for Enabling Fair LLM-Based Recommender Systems 

**Title (ZH)**: FACTER：面向公平性的同态 thresholds 确定与提示工程在促进基于大语言模型的推荐系统公平性中的应用 

**Authors**: Arya Fayyazi, Mehdi Kamal, Massoud Pedram  

**Link**: [PDF](https://arxiv.org/pdf/2502.02966)  

**Abstract**: We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%) while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias. 

**Abstract (ZH)**: 我们提出了一种名为FACTOR的公平性意识框架，该框架将校准预测与动态提示工程结合，应用于基于大语言模型（LLM）的推荐系统中。通过引入自适应语义方差阈值和违反检测机制，FACTOR能够在出现偏差模式时自动收紧公平性约束。此外，我们进一步开发了一种对抗性提示生成器，该生成器利用历史上的违规记录来减少重复的统计数据偏差，而无需重新训练LLM。我们在MovieLens和Amazon数据集上的实验结果显示，FACTOR在显著降低公平性违规（高达95.5%）的同时，维持了强大的推荐准确率，揭示了语义方差作为偏差有力代理的作用。 

---
# Control Search Rankings, Control the World: What is a Good Search Engine? 

**Title (ZH)**: 控制搜索排名，控制世界：什么是好的搜索引擎？ 

**Authors**: Simon Coghlan, Hui Xian Chia, Falk Scholer, Damiano Spina  

**Link**: [PDF](https://arxiv.org/pdf/2502.02957)  

**Abstract**: This paper examines the ethical question, 'What is a good search engine?' Since search engines are gatekeepers of global online information, it is vital they do their job ethically well. While the Internet is now several decades old, the topic remains under-explored from interdisciplinary perspectives. This paper presents a novel role-based approach involving four ethical models of types of search engine behavior: Customer Servant, Librarian, Journalist, and Teacher. It explores these ethical models with reference to the research field of information retrieval, and by means of a case study involving the COVID-19 global pandemic. It also reflects on the four ethical models in terms of the history of search engine development, from earlier crude efforts in the 1990s, to the very recent prospect of Large Language Model-based conversational information seeking systems taking on the roles of established web search engines like Google. Finally, the paper outlines considerations that inform present and future regulation and accountability for search engines as they continue to evolve. The paper should interest information retrieval researchers and others interested in the ethics of search engines. 

**Abstract (ZH)**: 本文探讨了伦理问题：“什么是好的搜索引擎？”由于搜索引擎是全球在线信息的门户，因此它们需要以伦理的方式履行其职责。尽管互联网已有数十年历史，但从跨学科视角探讨这一话题仍相对不足。本文提出了一种基于角色的新型方法，涉及四种类型的搜索引擎行为伦理模型：客户服务者、图书管理员、记者和教师。本文通过信息检索研究领域来探讨这些伦理模型，并以新冠肺炎全球大流行这一案例研究为例。此外，本文还从搜索引擎发展的历史角度反思这四种伦理模型，从1990年代初期原始的努力，到最近基于大型语言模型的对话式信息寻求系统有望取代谷歌等现有网络搜索引擎的角色。最后，本文概述了指导当前和未来搜索引擎监管与问责制考虑因素，随着搜索引擎的不断演变，这些考虑因素变得愈加重要。本文对信息检索研究人员及相关领域的读者具有重要意义。 

---
# TD3: Tucker Decomposition Based Dataset Distillation Method for Sequential Recommendation 

**Title (ZH)**: TD3：基于Tucker分解的数据集蒸馏方法用于序列推荐 

**Authors**: Jiaqing Zhang, Mingjia Yin, Hao Wang, Yawen Li, Yuyang Ye, Xingyu Lou, Junping Du, Enhong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.02854)  

**Abstract**: In the era of data-centric AI, the focus of recommender systems has shifted from model-centric innovations to data-centric approaches. The success of modern AI models is built on large-scale datasets, but this also results in significant training costs. Dataset distillation has emerged as a key solution, condensing large datasets to accelerate model training while preserving model performance. However, condensing discrete and sequentially correlated user-item interactions, particularly with extensive item sets, presents considerable challenges. This paper introduces \textbf{TD3}, a novel \textbf{T}ucker \textbf{D}ecomposition based \textbf{D}ataset \textbf{D}istillation method within a meta-learning framework, designed for sequential recommendation. TD3 distills a fully expressive \emph{synthetic sequence summary} from original data. To efficiently reduce computational complexity and extract refined latent patterns, Tucker decomposition decouples the summary into four factors: \emph{synthetic user latent factor}, \emph{temporal dynamics latent factor}, \emph{shared item latent factor}, and a \emph{relation core} that models their interconnections. Additionally, a surrogate objective in bi-level optimization is proposed to align feature spaces extracted from models trained on both original data and synthetic sequence summary beyond the naïve performance matching approach. In the \emph{inner-loop}, an augmentation technique allows the learner to closely fit the synthetic summary, ensuring an accurate update of it in the \emph{outer-loop}. To accelerate the optimization process and address long dependencies, RaT-BPTT is employed for bi-level optimization. Experiments and analyses on multiple public datasets have confirmed the superiority and cross-architecture generalizability of the proposed designs. Codes are released at this https URL. 

**Abstract (ZH)**: 在以数据为中心的AI时代，推荐系统的研究重点已从模型为中心的创新转向数据为中心的方法。现代AI模型的成功建立在大规模数据集的基础上，但这也会导致显著的训练成本。数据集蒸馏作为一种关键解决方案应运而生，通过压缩大数据集来加速模型训练，同时保持模型性能。然而，压缩离散且具有时序相关性的用户-项交互，尤其是在拥有广泛项集的情况下，仍然面临着巨大的挑战。本文介绍了一种名为TD3的新方法，该方法基于泰勒分解的数据蒸馏方法，属于元学习框架，适用于序列推荐。TD3从原始数据中提取一个完全表达性的“合成序列摘要”。为了有效降低计算复杂度并提取细化的潜在模式，泰勒分解将摘要分解为四个因子：合成用户潜在因子、时序动态潜在因子、共享项潜在因子以及一个表示它们互联性的关系核心。此外，文中提出了一种双层优化中的代理目标，旨在超越简单的性能匹配方法，对来自原始数据和合成序列摘要训练的模型提取的特征空间进行对齐。在内循环中，通过增强技术使学习器紧密契合合成摘要，从而在外循环中确保其准确更新。为了加速优化过程并解决长依赖问题，文中采用了双层优化中的RaT-BPTT方法。在多个公开数据集上的实验和分析证实了所提出设计的优越性和跨架构的一般性。代码已发布于该网址。 

---
# Inducing Diversity in Differentiable Search Indexing 

**Title (ZH)**: 在可微搜索索引中诱导多样性 

**Authors**: Abhijeet Phatak, Jayant Sachdev, Sean D Rosario, Swati Kirti, Chittaranjan Tripathy  

**Link**: [PDF](https://arxiv.org/pdf/2502.02788)  

**Abstract**: Differentiable Search Indexing (DSI) is a recent paradigm for information retrieval which uses a transformer-based neural network architecture as the document index to simplify the retrieval process. A differentiable index has many advantages enabling modifications, updates or extensions to the index. In this work, we explore balancing relevance and novel information content (diversity) for training DSI systems inspired by Maximal Marginal Relevance (MMR), and show the benefits of our approach over the naive DSI training. We present quantitative and qualitative evaluations of relevance and diversity measures obtained using our method on NQ320K and MSMARCO datasets in comparison to naive DSI. With our approach, it is possible to achieve diversity without any significant impact to relevance. Since we induce diversity while training DSI, the trained model has learned to diversify while being relevant. This obviates the need for a post-processing step to induce diversity in the recall set as typically performed using MMR. Our approach will be useful for Information Retrieval problems where both relevance and diversity are important such as in sub-topic retrieval. Our work can also be easily be extended to the incremental DSI settings which would enable fast updates to the index while retrieving a diverse recall set. 

**Abstract (ZH)**: 差异化检索索引（Differentiable Search Indexing, DSI）是一种近年来在信息检索中使用的范式，它利用基于变换器的神经网络架构作为文档索引，以简化检索过程。可微分索引有许多优势，能够对索引进行修改、更新或扩展。本文中，我们探索了在训练DSI系统时平衡相关性和新颖信息内容（多样性）的方法，受到了最大边际相关性（Maximal Marginal Relevance, MMR）的启发，并展示了我们的方法与传统的DSI训练方法相比的优势。我们使用NQ320K和MSMARCO数据集对相关性和多样性指标进行了定量和定性的评估，结果显示，利用我们的方法可以实现多样性而不显著影响相关性。由于我们在训练DSI时引入了多样性，训练模型能够学习在保持相关性的基础上多样化检索结果，从而省去了通常使用MMR在召回集中引入多样性的后处理步骤。我们的方法对于同时需要相关性和多样性的信息检索问题非常有用，例如次主题检索。此外，我们的方法还可以轻松扩展到增量DSI场景中，这将使在检索多样化召回集中能够快速更新索引成为可能。 

---
# PalimpChat: Declarative and Interactive AI analytics 

**Title (ZH)**: PalimpChat：声明式和交互式的AI分析 

**Authors**: Chunwei Liu, Gerardo Vitagliano, Brandon Rose, Matt Prinz, David Andrew Samson, Michael Cafarella  

**Link**: [PDF](https://arxiv.org/pdf/2502.03368)  

**Abstract**: Thanks to the advances in generative architectures and large language models, data scientists can now code pipelines of machine-learning operations to process large collections of unstructured data. Recent progress has seen the rise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to build optimized and increasingly complex pipelines, but these systems often remain accessible only to expert programmers. In this demonstration, we present PalimpChat, a chat-based interface to Palimpzest that bridges this gap by letting users create and run sophisticated AI pipelines through natural language alone. By integrating Archytas, a ReAct-based reasoning agent, and Palimpzest's suite of relational and LLM-based operators, PalimpChat provides a practical illustration of how a chat interface can make declarative AI frameworks truly accessible to non-experts.
Our demo system is publicly available online. At SIGMOD'25, participants can explore three real-world scenarios--scientific discovery, legal discovery, and real estate search--or apply PalimpChat to their own datasets. In this paper, we focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies complex AI workflows such as extracting and analyzing biomedical data. 

**Abstract (ZH)**: 多亏了生成架构和大规模语言模型的进步，数据科学家现在可以编写机器学习操作的代码流水线，以处理大量的非结构化数据。最近的进展催生了一类声明式人工智能框架（例如Palimpzest、Lotus和DocETL），用于构建优化且日益复杂的流水线，但这些系统往往仍然只对专家程序员开放。在此演示中，我们介绍了PalimpChat，这是一种基于聊天界面的接口，通过这种方式，Palimpzest得以跨越这一障碍，让用户仅通过自然语言即可创建和运行复杂的AI流水线。通过集成Archytas（一种ReAct为基础的推理代理）和Palimpzest的关联性和基于语言模型的操作集，PalimpChat提供了一种实用的范例，说明了聊天界面如何使声明式人工智能框架真正对非专家用户开放。

我们的演示系统现已在网上公开。在SIGMOD'25会议中，参与者可以探索三个实际应用场景——科学发现、法律发现和房地产搜索，或者将PalimpChat应用到自己的数据集中。在本文中，我们重点介绍PalimpChat在Palimpzest优化器的支持下简化复杂AI工作流（如提取和分析生物医学数据）的方式。 

---
