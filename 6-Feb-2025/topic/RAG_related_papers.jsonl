{'arxiv_id': 'arXiv:2502.03004', 'title': 'MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large Language Models and Retrieval-Augmented Generation', 'authors': 'Seonok Kim', 'link': 'https://arxiv.org/abs/2502.03004', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive capabilities across natural language processing tasks. However, their application to specialized domains such as medicine and biology requires further optimization to ensure factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a domain-adapted biomedical question-answering model designed to enhance both short-form and long-form queries. By integrating fine-tuning and retrieval-augmented generation (RAG), MedBioLM dynamically incorporates domain-specific knowledge, improving reasoning abilities and factual accuracy. To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA datasets, covering structured multiple-choice assessments and complex clinical reasoning tasks. Fine-tuning significantly improves accuracy on benchmark datasets, while RAG enhances factual consistency. These results highlight the potential of domain-optimized LLMs in advancing biomedical research, medical education, and clinical decision support.', 'abstract_zh': '大规模语言模型（LLMs）在自然语言处理任务中展现出了令人印象深刻的性能。然而，将其应用于医学和生物学等专门领域需要进一步优化，以确保事实的准确性、可靠性和情境深度。我们介绍了MedBioLM，这是一种专门设计的生物医学问答模型，旨在提高短形式和长形式查询的能力。通过集成微调和检索增强生成（RAG）技术，MedBioLM动态地融入了领域特定的知识，从而提升了推理能力和事实准确性。为了评估其有效性，我们在多种生物医学问答数据集上进行了微调，涵盖了结构化的多项选择评估和复杂的临床推理任务。微调在基准数据集上的准确率显著提高，而RAG则增强了事实的一致性。这些结果突显了优化领域的大规模语言模型在促进生物医学研究、医学教育和临床决策支持方面的潜力。', 'title_zh': 'MedBioLM：通过微调大规模语言模型和检索增强生成技术优化医学和生物学问答'}
{'arxiv_id': 'arXiv:2502.02603', 'title': 'SEAL: Speech Embedding Alignment Learning for Speech Large Language Model with Retrieval-Augmented Generation', 'authors': 'Chunyu Sun, Bingyu Liu, Zhichao Cui, Anbin Qi, Tian-hao Zhang, Dinghao Zhou, Lewei Lu', 'link': 'https://arxiv.org/abs/2502.02603', 'abstract': 'Embedding-based retrieval models have made significant strides in retrieval-augmented generation (RAG) techniques for text and multimodal large language models (LLMs) applications. However, when it comes to speech larage language models (SLLMs), these methods are limited to a two-stage process, where automatic speech recognition (ASR) is combined with text-based retrieval. This sequential architecture suffers from high latency and error propagation. To address these limitations, we propose a unified embedding framework that eliminates the need for intermediate text representations. Specifically, the framework includes separate speech and text encoders, followed by a shared scaling layer that maps both modalities into a common embedding space. Our model reduces pipeline latency by 50\\% while achieving higher retrieval accuracy compared to traditional two-stage methods. We also provide a theoretical analysis of the challenges inherent in end-to-end speech retrieval and introduce architectural principles for effective speech-to-document matching. Extensive experiments demonstrate the robustness of our approach across diverse acoustic conditions and speaker variations, paving the way for a new paradigm in multimodal SLLMs retrieval systems.', 'abstract_zh': '基于嵌入的检索模型在文本和多模态大型语言模型（LLMs）中的检索增强生成（RAG）技术中取得了显著进展。然而，当应用于语音大型语言模型（SLLMs）时，这些方法仅限于两阶段过程，其中自动语音识别（ASR）与基于文本的检索相结合。这种顺序架构存在高延迟和错误传播的问题。为了解决这些问题，我们提出了一种统一的嵌入框架，消除了中间文本表示的需求。具体来说，该框架包括独立的语音编码器和文本编码器，随后是一个共享缩放层，将两种模态映射到一个共同的嵌入空间。我们的模型将管线延迟减少了50%，同时在检索准确性上优于传统的两阶段方法。我们还对端到端语音检索固有的挑战进行了理论分析，并介绍了有效的语音到文档匹配的架构原则。广泛的实验表明，我们的方法在多种声学条件和说话人口音下具有鲁棒性，为多模态SLLMs检索系统的全新范式铺平了道路。', 'title_zh': 'SEAL：用于检索增强生成的语音大规模语言模型的语音嵌入对齐学习'}
