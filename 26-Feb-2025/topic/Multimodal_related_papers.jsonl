{'arxiv_id': 'arXiv:2502.18443', 'title': 'olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models', 'authors': 'Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, Luca Soldaini', 'link': 'https://arxiv.org/abs/2502.18443', 'abstract': 'PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. We present olmOCR, an open-source Python toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and convert a million PDF pages for only $190 USD. We release all components of olmOCR including VLM weights, data and training code, as well as inference code built on serving frameworks including vLLM and SGLang.', 'abstract_zh': 'PDF文档具有为语言模型训练提供数十万亿个新颖且高质量词汇令牌的潜力。然而，这些文档的形式多样，格式和视觉布局各异，这在尝试提取并忠实呈现用于语言模型使用的底层内容时构成了挑战。我们提出了olmOCR，这是一个开源的Python工具包，用于将PDF处理成清洁、线性化且保持自然阅读顺序的纯文本，同时保留结构化内容，如章节、表格、列表、方程等。我们的工具包使用了一个针对来自超过10万个抓取的PDF文档的样本文档集（共260,000页，具有图形、手写文本和低质量扫描等多种特性）进行微调的7B像素语言模型（VLM）进行运行。olmOCR针对大规模批量处理进行了优化，能够灵活扩展以适应不同的硬件设置，并以仅190美元的成本转换一百万页PDF文档。我们发布了olmOCR的所有组件，包括VLM权重、数据和训练代码，以及基于vLLM和SGLang服务框架构建的推理代码。', 'title_zh': 'olmOCR：通过视觉语言模型释放PDF中的万亿个词汇-token'}
{'arxiv_id': 'arXiv:2502.18331', 'title': 'BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle', 'authors': 'EunJeong Hwang, Peter West, Vered Shwartz', 'link': 'https://arxiv.org/abs/2502.18331', 'abstract': 'Humor is prevalent in online communications and it often relies on more than one modality (e.g., cartoons and memes). Interpreting humor in multimodal settings requires drawing on diverse types of knowledge, including metaphorical, sociocultural, and commonsense knowledge. However, identifying the most useful knowledge remains an open question. We introduce \\method{}, a method inspired by the information bottleneck principle that elicits relevant world knowledge from vision and language models which is iteratively refined for generating an explanation of the humor in an unsupervised manner. Our experiments on three datasets confirm the advantage of our method over a range of baselines. Our method can further be adapted in the future for additional tasks that can benefit from eliciting and conditioning on relevant world knowledge and open new research avenues in this direction.', 'abstract_zh': '在网络通信中，幽默普遍存在，并且常常依赖于多种模态（如卡通和梗图）。在多模态环境中解释幽默需要运用多种类型的知识，包括隐喻、社会文化以及常识知识。然而，识别最有用的知识仍然是一个开放的问题。我们提出了\\method{}方法，该方法受到信息瓶颈原理的启发，从视觉和语言模型中提取相关世界知识，并逐步精炼以无监督的方式生成幽默解释。我们在三个数据集上的实验表明，与多种基线方法相比，我们的方法具有明显的优势。未来，该方法还可以进一步适应更多可以从提取和利用相关世界知识中获益的任务，为该领域开辟新的研究方向。', 'title_zh': '瓶中信幽默：基于信息瓶颈原理的自我启发式幽默解释'}
{'arxiv_id': 'arXiv:2502.18285', 'title': 'Uncertainty Modeling in Multimodal Speech Analysis Across the Psychosis Spectrum', 'authors': 'Morteza Rohanian, Roya M. Hüppi, Farhad Nooralahzadeh, Noemi Dannecker, Yves Pauli, Werner Surbeck, Iris Sommer, Wolfram Hinzen, Nicolas Langer, Michael Krauthammer, Philipp Homan', 'link': 'https://arxiv.org/abs/2502.18285', 'abstract': 'Capturing subtle speech disruptions across the psychosis spectrum is challenging because of the inherent variability in speech patterns. This variability reflects individual differences and the fluctuating nature of symptoms in both clinical and non-clinical populations. Accounting for uncertainty in speech data is essential for predicting symptom severity and improving diagnostic precision. Speech disruptions characteristic of psychosis appear across the spectrum, including in non-clinical individuals. We develop an uncertainty-aware model integrating acoustic and linguistic features to predict symptom severity and psychosis-related traits. Quantifying uncertainty in specific modalities allows the model to address speech variability, improving prediction accuracy. We analyzed speech data from 114 participants, including 32 individuals with early psychosis and 82 with low or high schizotypy, collected through structured interviews, semi-structured autobiographical tasks, and narrative-driven interactions in German. The model improved prediction accuracy, reducing RMSE and achieving an F1-score of 83% with ECE = 4.5e-2, showing robust performance across different interaction contexts. Uncertainty estimation improved model interpretability by identifying reliability differences in speech markers such as pitch variability, fluency disruptions, and spectral instability. The model dynamically adjusted to task structures, weighting acoustic features more in structured settings and linguistic features in unstructured contexts. This approach strengthens early detection, personalized assessment, and clinical decision-making in psychosis-spectrum research.', 'abstract_zh': '在精神病谱系中捕捉细微的言语中断具有挑战性，这是因为言语模式固有的多样性。这种多样性反映了临床和非临床人群中症状的个体差异和波动性质。在处理言语数据时考虑不确定性对于预测症状严重程度并提高诊断准确性至关重要。精神病相关的言语中断贯穿整个谱系，包括非临床个体。我们开发了一种具备不确定性意识的模型，整合了声学和语言特征，以预测症状严重程度和与精神病相关的特征。通过量化特定模态的不确定性，模型能够应对言语多样性，从而提高预测准确性。我们分析了114名参与者的数据，包括32名早期精神病患者和82名低或高精神分裂症样特征者，这些数据来源于结构化访谈、半结构化自传式任务以及德语驱动的叙述式交互。该模型提高了预测准确性，降低了RMSE，并实现了F1分数为83%、ECE = 4.5e-2，展示了在不同交互背景下稳定的表现。不确定性估算提高了模型的可解释性，通过识别音高变化、流畅性中断和频谱不稳定性等言语标记的可靠性差异来实现。该模型能够动态适应任务结构，在结构化环境中强调声学特征，在非结构化情境中则更侧重于语言特征。这种方法加强了精神病谱系研究中的早期检测、个性化评估和临床决策。', 'title_zh': '跨精神分裂症谱系的多模态语音分析中的不确定性建模'}
{'arxiv_id': 'arXiv:2502.18148', 'title': 'NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts', 'authors': "Muhammad Farid Adilazuarda, Musa Izzanardi Wijanarko, Lucky Susanto, Khumaisa Nur'aini, Derry Wijaya, Alham Fikri Aji", 'link': 'https://arxiv.org/abs/2502.18148', 'abstract': "Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text. In this paper, we present NusaAksara, a novel public benchmark for Indonesian languages that includes their original scripts. Our benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation, and language identification. Our data is constructed by human experts through rigorous steps. NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset. We benchmark our data across several models, from LLMs and VLMs such as GPT-4o, Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and show that most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.", 'abstract_zh': '印度尼西亚语言和文字资源丰富，然而，大多数自然语言处理（NLP）的进展主要集中在使用罗马化文字的处理上。本文我们介绍了NusaAksara，这是一个包含原始文字的新颖公开基准数据集，涵盖了印度尼西亚语中的多种文字。该基准数据集包含了文本和图像两种模式，并覆盖了各种任务，如图像分割、光学字符识别（OCR）、转写、翻译和语言识别。我们的数据由经过严格训练的人工专家构建。NusaAksara数据集涵盖了8种文字，其中包括7种语言，还包括许多在NLP基准数据集中不常见的低资源语言。尽管Lampung文字不受Unicode支持，本数据集仍包括此文字。我们跨多种模型对数据进行了基准测试，这些模型包括大语言模型（LLM）与视觉语言模型（VLM），如GPT-4o、Llama 3.2和Aya 23，以及专门针对特定任务的系统，如PP-OCR和LangID。结果表明，大多数NLP技术无法处理印度尼西亚的本地文字，许多模型在这些文字上的性能接近零。', 'title_zh': 'NusaAksara：一种用于保存印度尼西亚土著文字的多模态多语言基准'}
{'arxiv_id': 'arXiv:2502.17812', 'title': 'Can Multimodal LLMs Perform Time Series Anomaly Detection?', 'authors': 'Xiongxiao Xu, Haoran Wang, Yueqing Liang, Philip S. Yu, Yue Zhao, Kai Shu', 'link': 'https://arxiv.org/abs/2502.17812', 'abstract': 'Large language models (LLMs) have been increasingly used in time series analysis. However, the potential of multimodal LLMs (MLLMs), particularly vision-language models, for time series remains largely under-explored. One natural way for humans to detect time series anomalies is through visualization and textual description. Motivated by this, we raise a critical and practical research question: Can multimodal LLMs perform time series anomaly detection? To answer this, we propose VisualTimeAnomaly benchmark to evaluate MLLMs in time series anomaly detection (TSAD). Our approach transforms time series numerical data into the image format and feed these images into various MLLMs, including proprietary models (GPT-4o and Gemini-1.5) and open-source models (LLaVA-NeXT and Qwen2-VL), each with one larger and one smaller variant. In total, VisualTimeAnomaly contains 12.4k time series images spanning 3 scenarios and 3 anomaly granularities with 9 anomaly types across 8 MLLMs. Starting with the univariate case (point- and range-wise anomalies), we extend our evaluation to more practical scenarios, including multivariate and irregular time series scenarios, and variate-wise anomalies. Our study reveals several key insights:\n1) MLLMs detect range- and variate-wise anomalies more effectively than point-wise anomalies.\n2) MLLMs are highly robust to irregular time series, even with 25% of the data missing.\n3) Open-source MLLMs perform comparably to proprietary models in TSAD. While open-source MLLMs excel on univariate time series, proprietary MLLMs demonstrate superior effectiveness on multivariate time series.\nTo the best of our knowledge, this is the first work to comprehensively investigate MLLMs for TSAD, particularly for multivariate and irregular time series scenarios. We release our dataset and code at this https URL to support future research.', 'abstract_zh': '大语言模型（LLMs）在时间序列分析中的应用越来越广泛。然而，多模态LLMs（MLLMs），尤其是视觉-语言模型，在这方面的潜力仍然被大大低估。人类检测时间序列异常的一个自然方法是通过可视化和文本描述。基于这一点，我们提出一个关键且实际的研究问题：多模态LLMs是否能够进行时间序列异常检测？为回答这一问题，我们提出了VisualTimeAnomaly基准测试，用于评估MLLMs在时间序列异常检测（TSAD）中的性能。我们的方法将时间序列的数据转换为图像格式，并将这些图像输入到各种类型的MLLMs中，包括专有模型（GPT-4o和Gemini-1.5）和开源模型（LLaVA-NeXT和Qwen2-VL），每种模型都有大、小两个变体。总共，VisualTimeAnomaly包含12,400张时间序列图像，涵盖3种情景和3种异常粒度，涉及8种MLLMs，共有9种异常类型。从单变量情况（点、区间异常）开始，我们将评估延伸到更实际的情景，包括多变量和不规则时间序列情况，以及基于变量的异常。我们的研究揭示了几点关键见解：\n\n1）MLLMs在检测区间和基于变量的异常方面比检测点异常更有效。\n2）MLLMs对不规则时间序列具有很高的鲁棒性，即使有25%的数据缺失也是如此。\n3）开源MLLMs在TSAD中表现与专有模型相当。开源MLLMs在单变量时间序列表现优秀，而专有MLLMs在多变量时间序列上表现出更优的效果。\n\n据我们所知，这是首次全面研究MLLMs在TSAD中的应用，特别是针对多变量和不规则时间序列情景。我们在此处提供我们的数据集和代码以支持未来的研究。您可以在以下链接获取：[链接]。', 'title_zh': '多模态LLM能进行时间序列异常检测吗？'}
{'arxiv_id': 'arXiv:2502.17669', 'title': 'Towards Human Cognition: Visual Context Guides Syntactic Priming in Fusion-Encoded Models', 'authors': 'Bushi Xiao, Michael Bennie, Jayetri Bardhan, Daisy Zhe Wang', 'link': 'https://arxiv.org/abs/2502.17669', 'abstract': 'We introduced PRISMATIC, the first multimodal structural priming dataset, and proposed a reference-free evaluation metric that assesses priming effects without predefined target sentences. Using this metric, we constructed and tested models with different multimodal encoding architectures (dual encoder and fusion encoder) to investigate their structural preservation capabilities. Our findings show that models with both encoding methods demonstrate comparable syntactic priming effects. However, only fusion-encoded models exhibit robust positive correlations between priming effects and visual similarity, suggesting a cognitive process more aligned with human psycholinguistic patterns. This work provides new insights into evaluating and understanding how syntactic information is processed in multimodal language models.', 'abstract_zh': '我们介绍了PRISMATIC，这是首个面向多模态结构效应的数据集，并提出了一种无需预定义目标句子的参考自由评估指标，用于评估结构效应。利用该指标，我们构建并测试了采用不同多模态编码架构（双编码器和融合编码器）的模型，以探究其结构保存能力。实验结果显示，采用两种编码方法的模型在句法结构效应方面具有可比性。然而，仅融合编码的模型展示了结构效应与视觉相似性之间稳定且显著的正相关关系，这表明其认知过程更符合人类心理语言学模式。这项工作为评估和理解多模态语言模型中句法信息的处理机制提供了新的见解。', 'title_zh': '向人类认知迈进：视觉上下文在融合编码模型中引导句法 priming'}
{'arxiv_id': 'arXiv:2502.17599', 'title': 'MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference', 'authors': 'Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang', 'link': 'https://arxiv.org/abs/2502.17599', 'abstract': 'Long-context Multimodal Large Language Models (MLLMs) that incorporate long text-image and text-video modalities, demand substantial resources as their multimodal Key-Value (KV) caches grow with increasing input lengths, challenging inference efficiency. Existing methods for KV cache compression, in both text-only and multimodal LLMs, have neglected attention density variations across layers, thus often adopting uniform or progressive reduction strategies for layer-wise cache allocation. In this work, we propose MEDA, a dynamic layer-wise KV cache allocation method for efficient multimodal long-context inference. As its core, MEDA utilizes cross-modal attention entropy to determine the KV cache size at each MLLMs layer. Given the dynamically allocated KV cache size at each layer, MEDA also employs a KV pair selection scheme to identify which KV pairs to select and a KV pair merging strategy that merges the selected and non-selected ones to preserve information from the entire context. MEDA achieves up to 72% KV cache memory reduction and 2.82 times faster decoding speed, while maintaining or enhancing performance on various multimodal tasks in long-context settings, including multi-images and long-video scenarios. Our code is released at this https URL.', 'abstract_zh': '以下是翻译后的版本，符合学术规范：\n\n长上下文多模态大规模语言模型（MLLMs）将长文本-图像和文本-视频模态结合起来，随着输入长度增加，其多模态键-值（KV）缓存也不断增长，从而挑战了推理效率。现有的键-值缓存压缩方法，在仅文本和多模态LLMs中均已存在，但大多忽略了不同层之间的注意力密度变化，因此通常采用统一或渐进的层间缓存缩减策略。在本文中，我们提出了一种名为MEDA的动态层间键-值缓存分配方法，以实现高效的多模态长上下文推理。其核心在于，MEDA利用跨模态注意熵来确定每个MLLM层的键-值缓存大小。在为每一层动态分配键-值缓存大小之后，MEDA还采用了一种键-值对选择方案来确定选择哪些键-值对，并采用一种键-值对合并策略，将选定和未选定的键-值对合并，以保留整个上下文的信息。在长上下文设置中，包括多图像和长视频场景等任务中，MEDA实现了72%的键-值缓存内存减少和2.82倍的解码速度提升，同时保持或增强了各项表现。我们的代码已发布于以下链接：[请将这里的URL补充完整]。', 'title_zh': 'MEDA：高效的多模态长上下文推理中动态键值缓存分配'}
{'arxiv_id': 'arXiv:2502.17589', 'title': 'End-to-End Chart Summarization via Visual Chain-of-Thought in Vision-Language Models', 'authors': 'Raymond Choi, Frank Burns, Chase Lawrence', 'link': 'https://arxiv.org/abs/2502.17589', 'abstract': 'Automated chart summarization is crucial for enhancing data accessibility and enabling efficient information extraction from visual data. While recent advances in visual-language models (VLMs) have demonstrated promise, existing methods often suffer from limitations in matching the generated summary to the chart data and in reasoning about complex chart patterns. This paper introduces End-to-End Visual Chain-of-Thought (V-CoT) for chart summarization, a novel approach optimized for Large Vision-Language Models (LVLMs). Our method directly trains an LVLM to process chart images and generate textual summaries in an end-to-end fashion, eliminating the need for explicit chart parsing modules. We incorporate a visual Chain-of-Thought mechanism through instruction fine-tuning, implicitly guiding the LVLM to perform visual reasoning steps during summary generation. Evaluated on the large-scale Chart-Sum-QA dataset, our V-CoT method significantly outperforms state-of-the-art baselines across a range of automatic metrics, including BLEU, BLEURT, CIDEr, and CS, and demonstrates superior matching degree and reasoning correctness in human evaluations. Ablation studies and detailed analyses further validate the effectiveness and robustness of our proposed approach, establishing a new benchmark for end-to-end chart summarization.', 'abstract_zh': '自动化图表总结对于增强数据可访问性和从视觉数据中高效提取信息至关重要。尽管最近在视觉-语言模型（VLMs）方面的进展显示出潜力，但现有方法往往在生成总结与图表数据的匹配以及处理复杂图表模式的推理方面存在局限性。本文介绍了一种针对大型视觉-语言模型（LVLMs）优化的端到端视觉链式思维（V-CoT）图表总结方法。该方法直接训练LVLM以端到端的方式处理图表图像并生成文本摘要，从而消除了显式图表解析模块的需要。我们通过指令微调引入了视觉链式思维机制，隐式地引导LVLM在生成摘要时执行视觉推理步骤。在大规模的Chart-Sum-QA数据集上进行评估，我们的V-CoT方法在多种自动评估指标（包括BLEU、BLEURT、CIDEr和CS）上显著优于现有最先进的基线方法，并且在人工评估中表现出更高的匹配精度和推理正确性。进一步的消融实验和详细分析验证了我们提出方法的有效性和鲁棒性，从而确立了一个新的端到端图表总结基准。', 'title_zh': '通过视觉链思考实现端到端图表总结的视觉语言模型方法'}
{'arxiv_id': 'arXiv:2502.18101', 'title': 'Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models', 'authors': 'Cao Yuxuan, Wu Jiayang, Alistair Cheong Liang Chuen, Bryan Shan Guanrong, Theodore Lee Chong Jen, Sherman Chann Zhi Shen', 'link': 'https://arxiv.org/abs/2502.18101', 'abstract': 'Traditional online content moderation systems struggle to classify modern multimodal means of communication, such as memes, a highly nuanced and information-dense medium. This task is especially hard in a culturally diverse society like Singapore, where low-resource languages are used and extensive knowledge on local context is needed to interpret online content. We curate a large collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to classify offensive memes in Singapore context. We show the effectiveness of fine-tuned VLMs on our dataset, and propose a pipeline containing OCR, translation and a 7-billion parameter-class VLM. Our solutions reach 80.62% accuracy and 0.8192 AUROC on a held-out test set, and can greatly aid human in moderating online contents. The dataset, code, and model weights will be open-sourced at this https URL.', 'abstract_zh': '传统的在线内容审查系统在分类现代多模态沟通手段（如梗图）方面存在困难，这些手段是一种高度精细且信息密集的媒介。这个任务在像新加坡这样的文化多样性社会中尤为艰巨，在这里低资源语言的使用需要大量关于本地背景的知识来解释在线内容。我们收集了一个包含112,000个由GPT-4V标记的梗图的大规模数据集，用于微调视觉语言模型（VLM）以在新加坡的语境下分类不适宜的梗图。我们展示了在我们数据集上微调后的VLM的有效性，并提出了一条包含OCR、翻译和一个70亿参数级别VLM的处理流水线。我们的解决方案在保留的测试集上达到了80.62%的准确性以及0.8192的AUROC（Area Under the Receiver Operating Characteristic Curve），可以极大地辅助人类审查在线内容。该数据集、代码和模型权重将在以下链接开源：[请在此处提供链接]。', 'title_zh': '在新加坡背景下检测具有社会偏见的侮辱性梗图——使用多模态大语言模型'}
{'arxiv_id': 'arXiv:2502.18017', 'title': 'ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents', 'authors': 'Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao', 'link': 'https://arxiv.org/abs/2502.18017', 'abstract': "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", 'abstract_zh': '传统的检索增强生成（RAG）方法在理解视觉丰富的文档信息方面仍面临显著挑战。现有的基准主要侧重于基于图像的问题回答（QA），忽视了密集视觉文档中高效检索、理解和推理的基本挑战。为弥合这一差距，我们引入了ViDoSeek，这是一个新数据集，旨在评估RAG在需要复杂推理的视觉丰富文档中的性能。基于此，我们识别了当前RAG方法的关键局限性：(i) 纯视觉检索方法难以有效地整合文本和视觉特征，(ii) 以前的方法往往分配不足的推理令牌，限制了它们的效果。为了应对这些挑战，我们提出了ViDoRAG，这是一个专门为视觉文档中复杂推理设计的多agent RAG框架。ViDoRAG采用基于高斯混合模型（GMM）的混合策略有效地处理多模态检索。为了进一步激发模型的推理能力，我们引入了一种迭代的agent工作流，包括探索、总结和反思，为RAG领域中的测试时尺度研究提供了框架。在ViDoSeek上的广泛实验验证了我们方法的有效性和泛化能力。值得注意的是，ViDoRAG在竞争性的ViDoSeek基准测试中性能优于现有方法超过10%。', 'title_zh': 'ViDoRAG：基于动态迭代推理代理的视觉文档检索增强生成方法'}
{'arxiv_id': 'arXiv:2502.17709', 'title': 'Contrastive Visual Data Augmentation', 'authors': 'Yu Zhou, Bingxuan Li, Mohan Tang, Xiaomeng Jin, Te-Lin Wu, Kuan-Hao Huang, Heng Ji, Kai-Wei Chang, Nanyun Peng', 'link': 'https://arxiv.org/abs/2502.17709', 'abstract': 'Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.', 'abstract_zh': '大型多模态模型（LMMs）往往难以识别新型概念，因为它们依赖于预训练知识，并且在捕捉细微视觉细节方面能力有限。训练时存在的领域特定知识缺口也使得它们容易混淆视觉相似、经常被误解或资源不足的概念。为了帮助LMMs更好地将细微的视觉特征与语言对齐，提高它们识别和推理解新型或稀有概念的能力，我们提出了一种对比视觉数据增强（CoDA）策略。CoDA通过提取目标概念与它们被误识别为已知概念的关键对比文本和视觉特征，然后利用多模态生成模型生成针对性的合成数据。通过自动筛选提取的特征和增强图像的质量，并经人类注释者验证，确保了数据的质量。我们展示了CoDA在低资源概念和多样场景识别数据集（如INaturalist和SUN）上的有效性与效率。此外，我们还收集了一个基准数据集NovelSpecies，其中包括已新发现的动物物种，这些物种肯定未被LMMs见过。对这些三个数据集进行的LLaVA-1.6 1- shot 更新结果显示，与现有最佳视觉数据增强策略相比，CoDA分别在NovelSpecies、SUN和iNat数据集上显著提高了12.3%、5.1%和6.0%的准确率绝对提升。', 'title_zh': '对比视觉数据增强'}
{'arxiv_id': 'arXiv:2502.17651', 'title': 'METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling', 'authors': 'Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng', 'link': 'https://arxiv.org/abs/2502.17651', 'abstract': 'Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.', 'abstract_zh': '图表生成旨在生成能够满足预期视觉属性（如文本、布局、颜色和类型）的代码，具有在金融分析、研究展示、教育和医疗保健等领域自动专业报告生成的巨大潜力。在这项工作中，我们构建了一个基于视觉-语言模型（VLM）的多智能体框架，以有效地实现自动图表生成。生成高质量的图表需要具备强大的视觉设计技能和精确的编程能力，将预期的视觉属性嵌入到代码中。这种复杂的多模态推理过程难以直接通过指导VLMs来实现。为了解决这些问题，我们提出了一种多智能体框架METAL，该框架将图表生成任务分解为专门智能体之间的迭代合作。METAL在图表生成任务中的准确度提高了5.2%，并且在测试时表现出计算预算随日志计算预算从512增长到8192时性能单调提升的现象。此外，我们发现，在METAL的批判过程中分离不同模态可以增强VLMs在多模态环境下的自我校正能力。', 'title_zh': 'METAL：一种用于图表生成的多智能体框架，具备测试时扩展能力'}
{'arxiv_id': 'arXiv:2502.17540', 'title': 'PosterSum: A Multimodal Benchmark for Scientific Poster Summarization', 'authors': 'Rohit Saxena, Pasquale Minervini, Frank Keller', 'link': 'https://arxiv.org/abs/2502.17540', 'abstract': 'Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce PosterSum, a novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on PosterSum and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose Segment & Summarize, a hierarchical method that outperforms current MLLMs on automated metrics, achieving a 3.14% gain in ROUGE-L. This will serve as a starting point for future research on poster summarization.', 'abstract_zh': '从多模态文档中生成准确且简洁的文本摘要是具有挑战性的，尤其是在处理像科学海报这样的视觉复杂内容时。我们提出了PosterSum，这是一种新的基准，旨在推动能够理解并用研究论文摘要的形式总结科学海报的视觉-语言模型的发展。我们的数据集包含16,305张会议海报及其对应的摘要作为总结。每张海报以图片格式提供，并包含了各种视觉理解挑战，如复杂的布局、密集的文字区域、表格和图表。我们对当前最先进的多模态大型语言模型（MLLM）进行了PosterSum基准测试，并展示了它们在准确地解释和总结科学海报方面的困难。我们提出了“分割与总结”方法，这是一种分层方法，在自动评估指标上优于当前的MLLM，实现了ROUGE-L指标3.14%的提升。这将为未来科学海报摘要的研究提供一个起点。', 'title_zh': 'PosterSum：一门用于科研海报总结的多模态基准数据集'}
{'arxiv_id': 'arXiv:2502.17514', 'title': 'SAE-V: Interpreting Multimodal Models for Enhanced Alignment', 'authors': 'Hantao Lou, Changye Li, Jiaming Ji, Yaodong Yang', 'link': 'https://arxiv.org/abs/2502.17514', 'abstract': "With the integration of image modality, the semantic space of multimodal large language models (MLLMs) is more complex than text-only models, making their interpretability more challenging and their alignment less stable, particularly susceptible to low-quality data, which can lead to inconsistencies between modalities, hallucinations, and biased outputs. As a result, developing interpretability methods for MLLMs is crucial for improving alignment quality and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained attention for their ability to interpret latent representations. However, extending SAEs to multimodal settings presents new challenges due to modality fusion and the difficulty of isolating cross-modal representations. To address these challenges, we introduce SAE-V, a mechanistic interpretability framework that extends the SAE paradigm to MLLMs. By identifying and analyzing interpretable features along with their corresponding data, SAE-V enables fine-grained interpretation of both model behavior and data quality, facilitating a deeper understanding of cross-modal interactions and alignment dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides an intrinsic data filtering mechanism to enhance model alignment without requiring additional models. Specifically, when applied to the alignment process of MLLMs, SAE-V-based data filtering methods could achieve more than 110% performance with less than 50% data. Our results highlight SAE-V's ability to enhance interpretability and alignment in MLLMs, providing insights into their internal mechanisms.", 'abstract_zh': '随着图像模态的整合，多模态大型语言模型（MLLMs）的语义空间比仅文本模型更加复杂，这使得它们的可解释性更加困难，对齐性也更加不稳定，尤其容易受到低质量数据的影响，从而导致模态之间的一致性问题、幻觉和偏差输出。因此，开发MLLMs的可解释性方法对于提高对齐质量和效率至关重要。在仅文本的大规模语言模型（LLMs）中，稀疏自编码器（SAEs）因其能够解释隐含表示而受到关注。然而，将SAEs扩展到多模态环境带来了新的挑战，因为需要处理模态融合以及跨模态表示的隔离难度。为了解决这些挑战，我们提出了一种基于SAE的机制性可解释框架SAE-V，该框架将SAE范式扩展到MLLMs中。通过识别和分析具有对应数据的可解释特征，SAE-V使我们能够对模型行为和数据质量进行精细化解释，从而促进对跨模态交互和对齐动力学的深入理解。此外，通过利用跨模态特征加权，SAE-V提供了一种内在的数据过滤机制，以增强模型对齐，而无需额外的模型。具体而言，当SAE-V应用于MLLMs的对齐过程时，基于SAE-V的数据过滤方法可以在少于50%的数据下实现超过110%的性能提升。我们的结果突出展示了SAE-V在增强MLLMs的可解释性和对齐性方面的潜力，为理解其内部机制提供了见解。', 'title_zh': 'SAE-V：解析多模态模型以增强对齐'}
{'arxiv_id': 'arXiv:2502.18371', 'title': 'MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning', 'authors': 'Sepehr Asgarian, Qayam Jetha, Jouhyun Jeon', 'link': 'https://arxiv.org/abs/2502.18371', 'abstract': "In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman's correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market.", 'abstract_zh': '在广告竞争激烈的环境中，成功取决于有效导航和利用消费者、广告商和广告平台之间的复杂互动。这些多维度的互动促使广告商优化策略以建模消费者行为、增强品牌记忆度以及定制广告内容。为应对这些挑战，我们提出了MindMem，这是一种多模态预测模型，用于衡量广告的记忆度。MindMem 结合了文本、视觉和音频数据，实现了行业领先的表现，在 LAMBDA 数据集上的 Spearman 相关系数为 0.631，在 Memento10K 数据集上的相关系数为 0.731，始终优于现有方法。此外，我们的分析揭示了影响广告记忆度的关键因素，如视频节奏、场景复杂性和情感共鸣。在此基础上，我们引入了MindMem-ReAd（MindMem 驱动的重复生成广告），它利用基于大型语言模型的模拟优化广告内容和位置，从而在广告记忆度上实现了高达 74.12% 的提升。我们的研究结果突显了人工智能在广告中的变革潜力，为广告商提供了一个强大的工具，以提高互动性、增强竞争力并在快速变化的市场中最大化影响。', 'title_zh': 'MindMem：结合大规模语言模型和深度学习的多模态广告记忆性预测方法'}
{'arxiv_id': 'arXiv:2502.18180', 'title': 'ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis', 'authors': 'Li Lei, Jia Sen, Wang Jianhao, An Zhaochong, Li Jiaang, Hwang Jenq-Neng, Belongie Serge', 'link': 'https://arxiv.org/abs/2502.18180', 'abstract': 'Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their "instruct-only" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion\'s precision, adaptability, and user engagement for human motion understanding.', 'abstract_zh': '多模态大型语言模型（MLLMs）的进步提高了对人类运动的理解。然而，这些模型仍受限于其“仅指令”性质，缺乏交互性和适应性以应对多样化的分析视角。为解决这些挑战，我们提出了一种名为ChatMotion的多模态多智能体框架，用于人类运动分析。ChatMotion动态解读用户意图，将复杂的任务分解为元任务，并激活专门的功能模块以进行运动理解。该框架整合了多个专业化模块，如MotionCore，从多个角度分析人类运动。广泛的经验表明，ChatMotion在人类运动理解方面具有精确性、适应性和用户参与度。', 'title_zh': 'ChatMotion：一种多模态多智能体系统用于人类动作分析'}
{'arxiv_id': 'arXiv:2502.18220', 'title': 'UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking', 'authors': 'He Wang, Tianyang Xu, Zhangyong Tang, Xiao-Jun Wu, Josef Kittler', 'link': 'https://arxiv.org/abs/2502.18220', 'abstract': 'Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance. However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications. In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks. To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality. Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G. The code will be available at this https URL.', 'abstract_zh': '多模态跟踪在单目标跟踪（SOT）中至关重要，因为不同传感器类型能够提供独特的功能，以克服由于目标外观变化引起的各种挑战。然而，现有的统一RGB-X跟踪器（X代表深度、事件或热成像模态）要么依赖于针对单一RGB-X图像对的专业任务训练策略，要么未能解决实际应用中模态自适应感知的关键重要性。在本文中，我们提出了一种统一自适应选择框架UASTrack，该框架促进了模型和参数的统一，并实现了各种多模态跟踪任务中的自适应模态区分。为在联合RGB-X图像对中实现模态自适应感知，我们设计了一种鉴别性自动选择器（DAS），能够识别模态标签，从而区分辅助模态的数据分布。此外，我们提出了一种任务自定义优化适配器（TCOA），适用于潜空间中的各种模态。该策略基于每个模态的具体特性有效过滤掉噪声冗余并减轻背景干扰。在包括LasHeR、GTOT、RGBT234、VisEvent和DepthTrack在内的五个基准数据集上的广泛比较，涵盖了RGB-T、RGB-E和RGB-D跟踪场景，证明了我们的创新方法通过增加仅1.87M的训练参数和1.95G的运算量跃点（FLOPS），实现了可比的性能。相关代码将在以下链接处提供：<该链接处>。\n\n请注意，“该链接处”应该替换为实际的URL链接。', 'title_zh': 'UASTrack：一种具有模态自适应定制化的统一单目标跟踪选择框架'}
{'arxiv_id': 'arXiv:2502.18176', 'title': 'CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification', 'authors': 'Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2502.18176', 'abstract': "In this paper, we aim to build an adversarially robust zero-shot image classifier. We ground our work on CLIP, a vision-language pre-trained encoder model that can perform zero-shot classification by matching an image with text prompts ``a photo of a <class-name>.''. Purification is the path we choose since it does not require adversarial training on specific attack types and thus can cope with any foreseen attacks. We then formulate purification risk as the KL divergence between the joint distributions of the purification process of denoising the adversarial samples and the attack process of adding perturbations to benign samples, through bidirectional Stochastic Differential Equations (SDEs). The final derived results inspire us to explore purification in the multi-modal latent space of CLIP. We propose two variants for our CLIPure approach: CLIPure-Diff which models the likelihood of images' latent vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation process of CLIP's latent vectors), and CLIPure-Cos which models the likelihood with the cosine similarity between the embeddings of an image and ``a photo of a.''. As far as we know, CLIPure is the first purification method in multi-modal latent space and CLIPure-Cos is the first purification method that is not based on generative models, which substantially improves defense efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13 datasets that previous CLIP-based defense methods used for evaluating zero-shot classification robustness. Results show that CLIPure boosts the SOTA robustness by a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on ImageNet, and 108% relative improvements of average robustness on the 13 datasets over previous SOTA. The code is available at this https URL.", 'abstract_zh': '在本文中，我们旨在构建一个对抗鲁棒的零样本图像分类器。我们基于CLIP（一个预训练的视觉-语言编码器模型），该模型可以通过将图像与文本提示（如“一张<类别名称>的照片”）进行匹配来进行零样本分类。我们选择纯化这条路线，因为它不需要对特定攻击类型进行对抗训练，因此可以应对任何预期的攻击。随后，我们将纯化风险定义为去噪对抗样本的纯化进程与添加扰动到 benign 样本的攻击过程之间联合分布的 KL 散度，通过双向随机微分方程（bidirectional SDEs）进行形式化。最终推导出的结果促使我们在CLIP的多模态潜空间中探索纯化方法。我们提出了CLIPure方法的两种变体：CLIPure-Diff 使用 DaLLE-2 中的 DiffusionPrior 模块来建模图像潜向量的可能性（该模块用于建模CLIP的潜向量生成过程），以及CLIPure-Cos 使用图像嵌入与“一张<类别名称>的照片”的余弦相似度来建模可能性。据我们所知，CLIPure 是第一个应用于多模态潜空间的纯化方法，而CLIPure-Cos 是第一个不基于生成模型的纯化方法，显著提高了防护效率。我们在CIFAR-10、ImageNet和13个之前用于评估零样本分类鲁棒性的CLIP基础防御方法所用的数据集上进行了大量实验。结果表明，CLIPure 大幅提升了最先进（State-of-the-Art）的鲁棒性，例如，CIFAR10 上从71.7%提高到91.1%，ImageNet 上从59.6%提高到72.6%，在13个数据集上的平均鲁棒性相对提升达108%。该代码可在以下网址获取：[提供网址]。', 'title_zh': 'CLIPure：通过CLIP在潜在空间中进行净化以实现对抗稳健的零-shot分类'}
{'arxiv_id': 'arXiv:2502.18042', 'title': 'VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion', 'authors': 'Pei Liu, Haipeng Liu, Haichao Liu, Xin Liu, Jinxin Ni, Jun Ma', 'link': 'https://arxiv.org/abs/2502.18042', 'abstract': "Human drivers adeptly navigate complex scenarios by utilizing rich attentional semantics, but the current autonomous systems struggle to replicate this ability, as they often lose critical semantic information when converting 2D observations into 3D space. In this sense, it hinders their effective deployment in dynamic and complex environments. Leveraging the superior scene understanding and reasoning abilities of Vision-Language Models (VLMs), we propose VLM-E2E, a novel framework that uses the VLMs to enhance training by providing attentional cues. Our method integrates textual representations into Bird's-Eye-View (BEV) features for semantic supervision, which enables the model to learn richer feature representations that explicitly capture the driver's attentional semantics. By focusing on attentional semantics, VLM-E2E better aligns with human-like driving behavior, which is critical for navigating dynamic and complex environments. Furthermore, we introduce a BEV-Text learnable weighted fusion strategy to address the issue of modality importance imbalance in fusing multimodal information. This approach dynamically balances the contributions of BEV and text features, ensuring that the complementary information from visual and textual modality is effectively utilized. By explicitly addressing the imbalance in multimodal fusion, our method facilitates a more holistic and robust representation of driving environments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its superiority over state-of-the-art approaches, showcasing significant improvements in performance.", 'abstract_zh': '人类司机能够通过利用丰富的注意语义来巧妙地应对复杂的场景，但当前的自动驾驶系统在将二维观察转换为三维空间时常常丢失关键语义信息，这限制了它们在动态和复杂环境中有效部署的能力。为了解决这一问题，我们利用视觉-语言模型（VLMs）优越的场景理解和推理能力，提出了一种名为VLM-E2E的新框架，该框架通过提供注意语义线索来增强训练。我们的方法将文本表示集成到鸟瞰视图（BEV）特征中，用于语义监督，从而使模型能够学习到更丰富的特征表示，能够显式捕捉驾驶员的注意语义。通过关注注意语义，VLM-E2E更好地符合人类驾驶行为，这对于导航动态和复杂环境至关重要。此外，我们引入了一种BEV-Text可学习加权融合策略，以解决多模态信息融合中模态重要性失衡的问题。此方法能够动态平衡BEV和文本特征的贡献，确保视觉和文本模态互补信息的有效利用。通过明确解决多模态融合中的失衡问题，我们的方法促进了驾驶环境更全面和稳健的表示。我们在nuScenes数据集上评估了VLM-E2E，并展示了其相对于最先进的方法的优越性，证明了其在性能上的显著改进。', 'title_zh': 'VLM-E2E: 增强端到端自动驾驶的多模态驾驶员注意力融合'}
{'arxiv_id': 'arXiv:2502.17900', 'title': 'Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs', 'authors': 'Che Liu, Cheng Ouyang, Zhongwei Wan, Haozhe Wang, Wenjia Bai, Rossella Arcucci', 'link': 'https://arxiv.org/abs/2502.17900', 'abstract': 'Recent advances in multimodal ECG representation learning center on aligning ECG signals with paired free-text reports. However, suboptimal alignment persists due to the complexity of medical language and the reliance on a full 12-lead setup, which is often unavailable in under-resourced settings. To tackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECG representation learning framework. **K-MERL** leverages large language models to extract structured knowledge from free-text reports and employs a lead-aware ECG encoder with dynamic lead masking to accommodate arbitrary lead inputs. Evaluations on six external ECG datasets show that **K-MERL** achieves state-of-the-art performance in zero-shot classification and linear probing tasks, while delivering an average **16%** AUC improvement over existing methods in partial-lead zero-shot classification.', 'abstract_zh': '近年来，多模态心电图（ECG）表示学习的进展主要集中在将ECG信号与配对的自由文本报告对齐。然而，由于医学语言的复杂性和依赖完整的12导联设置，这种对齐仍然存在不足。特别是在资源不足的环境中，12导联设置往往不可用。为了解决这些问题，我们提出了一种基于知识增强的多模态ECG表示学习框架——**K-MERL**。**K-MERL** 利用大规模语言模型从自由文本报告中提取结构化的知识，并采用具有动态导联掩码的导联感知ECG编码器，以适应任意导联输入。在六个外部ECG数据集上的评估结果显示，**K-MERL** 在零样本分类和线性探针任务中取得了最先进的性能，在部分导联的零样本分类中，相对于现有方法的平均AUC改进达到了16%。', 'title_zh': '带有任意导联输入的知识增强多模态心电图表示学习'}
{'arxiv_id': 'arXiv:2502.17832', 'title': 'MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks', 'authors': 'Hyeonjeong Ha, Qiusi Zhan, Jeonghwan Kim, Dimitrios Bralios, Saikrishna Sanniboina, Nanyun Peng, Kai-wei Chang, Daniel Kang, Heng Ji', 'link': 'https://arxiv.org/abs/2502.17832', 'abstract': 'Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks.', 'abstract_zh': '配备了检索增强生成（RAG）的多模态大型语言模型（MLLMs）结合了其丰富的参数知识和动态的外部知识，使其在问答等任务中表现出色。虽然RAG通过将响应与查询相关的外部知识相链接来增强MLLMs，但这种依赖性带来了一个关键且尚未充分探索的安全风险：知识中毒攻击，其中故意向外部知识库注入错误信息或无关知识，以操控模型输出错误甚至有害的结果。为了揭示多模态RAG中的此类漏洞，我们提出了一种新颖的知识中毒攻击框架——MM-PoisonRAG，并提出两种攻击策略：局部中毒攻击（LPA），它在文本和图像中注入查询相关的错误信息以进行定向操纵；以及全球化中毒攻击（GPA），在MLLM生成过程中提供虚假指导，导致所有查询均产生成分荒谬的响应。我们在多个任务、模型和访问设置下评估了我们的攻击，结果表明，LPA能够成功操控MLLM生成攻击者控制的答案，在MultiModalQA上的成功率最高可达56%。此外，GPA仅通过一次无关知识的注入即可完全破坏模型生成，使其准确率为零。我们的研究结果强调了针对知识中毒攻击建立 robust 防御措施的迫切需求，以保障多模态RAG框架的安全。', 'title_zh': 'MM-PoisonRAG: 针对多模态RAG的局部和全局投毒攻击破解方法'}
{'arxiv_id': 'arXiv:2502.17821', 'title': 'CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems', 'authors': 'Rui Liu, Yu Shen, Peng Gao, Pratap Tokekar, Ming Lin', 'link': 'https://arxiv.org/abs/2502.17821', 'abstract': 'Multi-modality learning has become a crucial technique for improving the performance of machine learning applications across domains such as autonomous driving, robotics, and perception systems. While existing frameworks such as Auxiliary Modality Learning (AML) effectively utilize multiple data sources during training and enable inference with reduced modalities, they primarily operate in a single-agent context. This limitation is particularly critical in dynamic environments, such as connected autonomous vehicles (CAV), where incomplete data coverage can lead to decision-making blind spots. To address these challenges, we propose Collaborative Auxiliary Modality Learning ($\\textbf{CAML}$), a novel multi-agent multi-modality framework that enables agents to collaborate and share multimodal data during training while allowing inference with reduced modalities per agent during testing. We systematically analyze the effectiveness of $\\textbf{CAML}$ from the perspective of uncertainty reduction and data coverage, providing theoretical insights into its advantages over AML. Experimental results in collaborative decision-making for CAV in accident-prone scenarios demonstrate that \\ours~achieves up to a ${\\bf 58.13}\\%$ improvement in accident detection. Additionally, we validate $\\textbf{CAML}$ on real-world aerial-ground robot data for collaborative semantic segmentation, achieving up to a ${\\bf 10.61}\\%$ improvement in mIoU.', 'abstract_zh': '多模态学习已成为提升跨领域（如自主驾驶、机器人和感知系统）机器学习应用性能的关键技术。现有的框架，如辅助模态学习（AML），能够有效利用多个数据源进行训练，并在推理时使用减少的模态数量，但这些框架主要在单个代理的上下文中运行。这一限制在动态环境中尤为关键，例如连接的自动驾驶车辆（CAV），因为不完整的数据覆盖可能导致决策盲点。为了解决这些挑战，我们提出了一种新的多代理多模态框架——协作辅助模态学习（$\\textbf{CAML}$），该框架允许代理在训练过程中协作并共享多模态数据，在测试过程中每个代理可以使用减少的模态数量进行推理。我们从不确定性和数据覆盖的角度系统地分析了$\\textbf{CAML}$的有效性，提供了其相对于AML的优势的理论见解。在事故多发场景下进行的合作决策实验结果表明，$\\textbf{CAML}$在事故检测方面的性能提高了$\\textbf{58.13}\\%$。此外，我们还在实际的空地机器人数据上验证了$\\textbf{CAML}$在协作语义分割上的有效性，取得了$\\textbf{mIoU}$提高了$\\textbf{10.61}\\%$的成果。', 'title_zh': 'CAML：多智能体系统中的协作辅助模态学习'}
{'arxiv_id': 'arXiv:2502.17763', 'title': 'Design and implementation of a distributed security threat detection system integrating federated learning and multimodal LLM', 'authors': 'Yuqing Wang, Xiao Yang', 'link': 'https://arxiv.org/abs/2502.17763', 'abstract': 'Traditional security protection methods struggle to address sophisticated attack vectors in large-scale distributed systems, particularly when balancing detection accuracy with data privacy concerns. This paper presents a novel distributed security threat detection system that integrates federated learning with multimodal large language models (LLMs). Our system leverages federated learning to ensure data privacy while employing multimodal LLMs to process heterogeneous data sources including network traffic, system logs, images, and sensor data. Experimental evaluation on a 10TB distributed dataset demonstrates that our approach achieves 96.4% detection accuracy, outperforming traditional baseline models by 4.1 percentage points. The system reduces both false positive and false negative rates by 1.8 and 2.4 percentage points respectively. Performance analysis shows that our system maintains efficient processing capabilities in distributed environments, requiring 180 seconds for model training and 3.8 seconds for threat detection across the distributed network. These results demonstrate significant improvements in detection accuracy and computational efficiency while preserving data privacy, suggesting strong potential for real-world deployment in large-scale security systems.', 'abstract_zh': '传统的安全保护方法在处理大规模分布式系统中的复杂攻击向量时显得力不从心，尤其是在权衡检测准确性和数据隐私问题之间时更为显著。本文提出了一种新颖的分布式安全威胁检测系统，该系统将联邦学习与多模态大型语言模型（LLMs）结合在一起。我们的系统利用联邦学习来确保数据隐私，并使用多模态LLMs来处理包括网络流量、系统日志、图像和传感器数据在内的异构数据源。在10TB分布式数据集上的实验评估表明，我们的方法实现了96.4%的检测准确率，较传统基线模型提高了4.1个百分点。系统将假阳性率和假阴性率分别减少了1.8个百分点和2.4个百分点。性能分析表明，我们的系统在分布式环境中保持了高效的处理能力，模型训练时间仅为180秒，而分布式网络中的威胁检测时间仅为3.8秒。这些结果表明，在提高检测准确率和计算效率的同时，仍能保留数据隐私，这表明该系统具有在大规模安全系统中实际部署的强大潜力。', 'title_zh': '集成联邦学习和多模态大语言模型的分布式安全威胁检测系统的设计与实现'}
{'arxiv_id': 'arXiv:2502.17524', 'title': 'Multimodal Bearing Fault Classification Under Variable Conditions: A 1D CNN with Transfer Learning', 'authors': 'Tasfiq E. Alam, Md Manjurul Ahsan, Shivakumar Raman', 'link': 'https://arxiv.org/abs/2502.17524', 'abstract': 'Bearings play an integral role in ensuring the reliability and efficiency of rotating machinery - reducing friction and handling critical loads. Bearing failures that constitute up to 90% of mechanical faults highlight the imperative need for reliable condition monitoring and fault detection. This study proposes a multimodal bearing fault classification approach that relies on vibration and motor phase current signals within a one-dimensional convolutional neural network (1D CNN) framework. The method fuses features from multiple signals to enhance the accuracy of fault detection. Under the baseline condition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the model reaches an accuracy of 96% with addition of L2 regularization. This represents a notable improvement of 2% compared to the non-regularized model. In addition, the model demonstrates robust performance across three distinct operating conditions by employing transfer learning (TL) strategies. Among the tested TL variants, the approach that preserves parameters up to the first max-pool layer and then adjusts subsequent layers achieves the highest performance. While this approach attains excellent accuracy across varied conditions, it requires more computational time due to its greater number of trainable parameters. To address resource constraints, less computationally intensive models offer feasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal 1D CNN framework with late fusion and TL strategies lays a foundation for more accurate, adaptable, and efficient bearing fault classification in industrial environments with variable operating conditions.', 'abstract_zh': '轴承在确保旋转机械的可靠性和效率中发挥着关键作用——通过减少摩擦和承受关键载荷。轴承故障占机械故障的90%以上，突显了可靠状态监测和故障检测的迫切需求。本研究提出了一种基于振动能和电机相电流信号的一维卷积神经网络（1D CNN）框架下的多模态轴承故障分类方法。该方法通过融合多种信号的特征以提高故障检测的准确性。在基线条件下（1500转/分钟，0.7牛·米负载扭矩，1000牛径向力），模型在加入L2正则化后达到了96%的准确率，相比未正则化的模型提升了2%。此外，模型通过采用迁移学习（TL）策略在三个不同的操作条件下展示了稳健的表现。在测试的TL变体中，保留第一最大池化层前的所有参数，然后调整后续层的方法取得了最佳性能。尽管该方法在多种条件下达到了出色的准确率，但由于其更多的可训练参数，需要更多的计算时间。为解决资源限制，计算成本较低的模型提供了可行的权衡方案，尽管会略微降低准确率。总体而言，该多模态1D CNN框架与迟延融合和迁移学习策略为在具有可变操作条件的工业环境中实现更准确、适应性和高效的轴承故障分类奠定了基础。', 'title_zh': '在变工况下的多模态轴承故障分类：基于迁移学习的1D CNN方法'}
{'arxiv_id': 'arXiv:2502.17516', 'title': 'A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models', 'authors': 'Zihao Lin, Samyadeep Basu, Mohammad Beigi, Varun Manjunatha, Ryan A. Rossi, Zichao Wang, Yufan Zhou, Sriram Balasubramanian, Arman Zarei, Keivan Rezaei, Ying Shen, Barry Menglong Yao, Zhiyang Xu, Qin Liu, Yuxiang Zhang, Yan Sun, Shilong Liu, Li Shen, Hongxuan Li, Soheil Feizi, Lifu Huang', 'link': 'https://arxiv.org/abs/2502.17516', 'abstract': 'The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.', 'abstract_zh': '基础模型的崛起已彻底改变机器学习的研究方向，促使研究人员致力于探索其内部机制，并开发更为高效和可靠的模型以实现更好的控制。虽然在解释大型语言模型（LLMs）方面已经取得了显著进展，但多模态基础模型（MMFMs），如对比型视语言模型、生成型视语言模型和文本到图像模型，因其超越单一模态框架的独特解释性挑战而受到关注。尽管已经有了一些初步研究，但在LLMs与MMFMs的解释性之间仍然存在相当大的差距。本文综述了两个关键方面：（1）将LLM解释方法应用于多模态模型，以及（2）理解单模态语言模型与跨模态系统之间的机制差异。通过系统性地评审当前多模态模型分析技术，我们提出了一个结构化的解释性方法分类体系，比较了单模态和多模态架构之间的见解，并突显出关键的研究缺口。', 'title_zh': '多模态基础模型的机制可解释性综述'}
{'arxiv_id': 'arXiv:2502.17481', 'title': 'Toward Foundational Model for Sleep Analysis Using a Multimodal Hybrid Self-Supervised Learning Framework', 'authors': 'Cheol-Hui Lee, Hakseung Kim, Byung C. Yoon, Dong-Joo Kim', 'link': 'https://arxiv.org/abs/2502.17481', 'abstract': 'Sleep is essential for maintaining human health and quality of life. Analyzing physiological signals during sleep is critical in assessing sleep quality and diagnosing sleep disorders. However, manual diagnoses by clinicians are time-intensive and subjective. Despite advances in deep learning that have enhanced automation, these approaches remain heavily dependent on large-scale labeled datasets. This study introduces SynthSleepNet, a multimodal hybrid self-supervised learning framework designed for analyzing polysomnography (PSG) data. SynthSleepNet effectively integrates masked prediction and contrastive learning to leverage complementary features across multiple modalities, including electroencephalogram (EEG), electrooculography (EOG), electromyography (EMG), and electrocardiogram (ECG). This approach enables the model to learn highly expressive representations of PSG data. Furthermore, a temporal context module based on Mamba was developed to efficiently capture contextual information across signals. SynthSleepNet achieved superior performance compared to state-of-the-art methods across three downstream tasks: sleep-stage classification, apnea detection, and hypopnea detection, with accuracies of 89.89%, 99.75%, and 89.60%, respectively. The model demonstrated robust performance in a semi-supervised learning environment with limited labels, achieving accuracies of 87.98%, 99.37%, and 77.52% in the same tasks. These results underscore the potential of the model as a foundational tool for the comprehensive analysis of PSG data. SynthSleepNet demonstrates comprehensively superior performance across multiple downstream tasks compared to other methodologies, making it expected to set a new standard for sleep disorder monitoring and diagnostic systems.', 'abstract_zh': '睡眠对维持人类健康和生活质量至关重要。分析睡眠期间的生理信号是评估睡眠质量和诊断睡眠障碍的关键。然而，临床医生的手动诊断耗时且主观。尽管深度学习的进步提高了自动化水平，但这些方法仍然高度依赖大规模标注数据集。本研究引入了SynthSleepNet，这是一种用于分析多导睡眠图（polysomnography, PSG）数据的多模态混合自监督学习框架。SynthSleepNet有效地结合了遮蔽预测和对比学习，利用多个模态中的互补特征，包括脑电图（EEG）、眼电图（EOG）、肌电图（EMG）和心电图（ECG），从而使模型能够学习PSG数据的高表达表示。此外，基于Mamba开发了一种时间上下文模块，以高效地捕捉信号之间的上下文信息。在三项下游任务——睡眠分期分类、呼吸暂停检测和低通气检测——中，SynthSleepNet分别达到了89.89%、99.75%和89.60%的准确率，表现优于最先进的方法。在半监督学习环境中，尽管标签有限，该模型仍表现出稳健的性能，并分别实现了87.98%、99.37%和77.52%的准确率。这些结果突显了该模型作为全面分析PSG数据工具的潜力。SynthSleepNet在多项下游任务中的综合性能显著优于其他方法，预期将为睡眠障碍监测和诊断系统设立新的标准。', 'title_zh': '使用多模态混合自我监督学习框架构建睡眠分析基础模型'}
{'arxiv_id': 'arXiv:2502.17470', 'title': 'MC2SleepNet: Multi-modal Cross-masking with Contrastive Learning for Sleep Stage Classification', 'authors': 'Younghoon Na', 'link': 'https://arxiv.org/abs/2502.17470', 'abstract': 'Sleep profoundly affects our health, and sleep deficiency or disorders can cause physical and mental problems. % Despite significant findings from previous studies, challenges persist in optimizing deep learning models, especially in multi-modal learning for high-accuracy sleep stage classification. Our research introduces MC2SleepNet (Multi-modal Cross-masking with Contrastive learning for Sleep stage classification Network). It aims to facilitate the effective collaboration between Convolutional Neural Networks (CNNs) and Transformer architectures for multi-modal training with the help of contrastive learning and cross-masking. % Raw single channel EEG signals and corresponding spectrogram data provide differently characterized modalities for multi-modal learning. Our MC2SleepNet has achieved state-of-the-art performance with an accuracy of both 84.6% on the SleepEDF-78 and 88.6% accuracy on the Sleep Heart Health Study (SHHS). These results demonstrate the effective generalization of our proposed network across both small and large datasets.', 'abstract_zh': '睡眠对我们健康影响深远，睡眠不足或障碍会导致身心问题。尽管先前的研究取得了显著成果，但在优化深度学习模型方面仍存在挑战，尤其是在高精度睡眠阶段分类的多模态学习中。我们的研究引入了MC2SleepNet（多模态对比学习与交叉掩码的睡眠阶段分类网络），旨在通过对比学习和交叉掩码促进卷积神经网络（CNN）与变换器架构之间的有效协作，实现多模态训练。原始的单通道脑电图（EEG）信号及其相应的频谱图数据提供了不同的模态，为多模态学习提供了基础。我们的MC2SleepNet在SleepEDF-78数据集上取得了84.6%的准确率，在睡眠心脏健康研究（SHHS）数据集上取得了88.6%的准确率。这些结果表明，所提出的网络在不同规模的数据集上具有有效的泛化能力。', 'title_zh': 'MC2SleepNet：多模态对比学习掩蔽方法在睡眠阶段分类中的应用'}
