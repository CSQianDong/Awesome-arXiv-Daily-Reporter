# QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval 

**Title (ZH)**: QA-Expand: 增强信息检索中查询扩展的多问题答案生成 

**Authors**: Wonduk Seo, Seunghyun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.08557)  

**Abstract**: Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce QA-Expand, a novel and effective framework for query expansion. It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further rewrites and filters these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up to 13% over state-of-the-art methods, offering a robust solution for modern retrieval challenges. 

**Abstract (ZH)**: 查询扩展在信息检索（IR）中广泛使用，通过增加额外的上下文信息来丰富查询内容，从而提高搜索结果。尽管基于大型语言模型（LLM）的方法通过多次提示生成伪相关内容和扩展项，但它们往往会产生重复、单一的扩展，缺乏检索所有相关信息所需的多样化上下文。在本文中，我们提出了一种名为QA-Expand的新型且有效的查询扩展框架。该框架首先从初始查询生成多个相关问题，然后生成相应的伪答案作为替代文档。反馈模型进一步重写和过滤这些答案，以确保仅采用最具有信息价值的扩展。在BEIR和TREC等基准测试上的广泛实验表明，QA-Expand相比于现有最佳方法在检索性能上提高了多达13%，为现代检索挑战提供了一个稳健的解决方案。 

---
# Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples 

**Title (ZH)**: 通过LLM生成的对抗性样本跨语言考察多语言嵌入模型 

**Authors**: Andrianos Michail, Simon Clematide, Rico Sennrich  

**Link**: [PDF](https://arxiv.org/pdf/2502.08638)  

**Abstract**: The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that requires only a set of parallel sentence pairs of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than hard negatives generated by a large language model. We create four instances of our introduced CLSD task for the language pair German-French within the domain of news. Within this case study, we find that models that are also fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using English as the pivot language, while bitext mining models such as LaBSE perform best directly cross-lingually. We also show a fine-grained similarity analysis enabled by our distractor generation strategy, indicating that different embedding models are sensitive to different types of perturbations. 

**Abstract (ZH)**: 以下是内容翻译成中文，符合学术规范的版本：

跨语言语义搜索模型的语义搜索能力评估通常仅限于信息检索和语义文本相似性等任务中的现有数据集。为实现特定领域的评估，我们提出了跨语言语义区分（CLSD）这一新颖的跨语言语义搜索任务，该任务仅需包含目标领域内感兴趣语言对的平行句子对集。该任务关注模型在跨语言环境下将真实的平行句子排名高于大型语言模型生成的难负样本的能力。我们为德语-法语语言对在新闻领域内引入了四个CLSD任务实例。在这一案例研究中，我们发现同时微调过检索任务的模型（如多语言E5）倾向于使用英语作为中介语言，而基于平行文本挖掘模型（如LaBSE）则直接在跨语言环境中表现最佳。我们还展示了由我们的诱饵生成策略所支持的细粒度相似性分析，表明不同的嵌入模型对不同类型的扰动具有不同的敏感性。 

---
# LLMs can implicitly learn from mistakes in-context 

**Title (ZH)**: 大型语言模型可以从上下文中的错误中隐性学习。 

**Authors**: Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08550)  

**Abstract**: Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. 

**Abstract (ZH)**: 从错误中学习是人类智能的基本特征之一。先前的研究表明，在提供了详细的错误答案或纠正过程解释时，大型语言模型（LLMs）也能从错误中学习。在本研究中，我们探讨了当没有提供这些解释时，LLMs是否也能从数学推理任务中的错误中学习。我们研究了LLMs是否能够仅通过观察错误和正确的答案来隐式地推断出这些解释。令人惊讶的是，我们发现当从上下文中删除解释并仅显示错误和正确的答案时，LLMs的平均表现更好。这种方法在我们的评估中也明显优于链式思考提示。我们发现这些结果在不同大小和推理能力的LLMs中是一致的。进一步的分析表明，使用错误和正确答案进行提示比在上下文中引入更多样化的问答回答对表现和泛化能力有更好的提升效果。最后，我们展示了仅观察错误和正确答案的模型生成的新解释与借助范例解释生成的解释被人类打分相当。我们的结果表明，LLMs确实具备上下文中的隐式学习能力。 

---
# Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation 

**Title (ZH)**: 忠实、不忠实或模棱两可？基于初始立场的多智能体辩论摘要评价 

**Authors**: Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, Hang Su  

**Link**: [PDF](https://arxiv.org/pdf/2502.08514)  

**Abstract**: Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries. 

**Abstract (ZH)**: 基于大型语言模型（LLMs）的忠实性评估器常常被文本的流畅度所迷惑，在识别摘要中的错误方面表现不佳。我们提出了一种摘要忠实性评估方法，其中多个基于LLM的代理被赋予初始立场（无论其真实信念如何），并被迫提出理由以证明所施加的信念，从而进行多轮辩论以达成共识。均衡分布的初始分配导致了更多样化的立场，进而促进了更有意义的辩论，并最终发现了更多的错误。此外，通过对最近的忠实性评估数据集进行分析，我们观察到自然地，并非所有摘要要么忠实于源文档，要么完全不忠实。因此，我们引入了一个新的维度——模糊性，并提出了一套详细分类法来识别这种特殊情况。实验结果表明，我们的方法能够帮助识别模糊性，并且在处理非模糊性摘要时表现甚至更佳。 

---
# Salamandra Technical Report 

**Title (ZH)**: 《萨拉曼德ra技术报告》

注：在此翻译中，“Salamandra”一词未提供具体的含义或背景信息，通常来说，“Salamandra”是拉丁文，意为“萨拉曼德龙”或“火蜥蜴”，有时也可能是指一个特定的项目、系统或公司的名称。如果没有更多的上下文信息，我们基于其常见的含义进行了翻译。如果有具体的技术背景或其他特定含义，请提供更多信息以便进行更准确的翻译。 

**Authors**: Aitor Gonzalez-Agirre, Marc Pàmies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, José Javier Saiz, Ferran Espuña, Jaume Prats, Javier Aula-Blasco, Mario Mina, Adrián Rubio, Alexander Shvets, Anna Sallés, Iñaki Lacunza, Iñigo Pikabea, Jorge Palomar, Júlia Falcão, Lucía Tormo, Luis Vasquez-Reina, Montserrat Marimon, Valle Ruíz-Fernández, Marta Villegas  

**Link**: [PDF](https://arxiv.org/pdf/2502.08489)  

**Abstract**: This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and this http URL this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models. 

**Abstract (ZH)**: 本文介绍了Salamandra，这是一个开源的解码器型大规模语言模型系列，共提供三种不同规模的模型：20亿、70亿和400亿参数。这些模型从包含35种欧洲语言和代码文本的多语种数据集中从头开始训练。我们精心选择的语料库，仅由来自多种来源的开放访问数据编制而成。除了基础模型外，我们还发布了在公共领域指令数据上微调的附加检查点，用于聊天应用。此外，我们还分享了初步的多模态实验，以概念验证的形式展示了Salamandra家族的潜在应用。我们在多语种基准上的广泛评估结果显示，Salamandra具有很强的能力，与同等规模的开源模型相比，其性能具有竞争力。我们在技术报告中，不仅提供了标准下游任务和与偏差相关的关键方面的全面评估结果，还展示了我们的设计选择、数据编制策略和评估方法的详细信息。此外，我们还公开了训练和评估脚本，不遵循惯例。我们所有模型均采用宽松的Apache 2.0许可证发布，旨在促进未来研究并促进商业应用，从而促进大型语言模型开源生态系统的贡献。 

---
# Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning 

**Title (ZH)**: 通过循环对齐推理增强自回归链式思考 

**Authors**: Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08482)  

**Abstract**: Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at this https URL. 

**Abstract (ZH)**: 链式思维（CoT）提示已成为提升语言模型推理能力的一种强大技术。然而，生成长且正确的CoT轨迹具有挑战性。近期研究表明，循环变压器具有卓越的长度泛化能力，但它们的局限性使其无法替代自回归解决方案。为了更好地发挥循环变压器的优势，我们提出了RELAY（REasoning through Loop Alignment iterativelY）。具体而言，我们将链式思维（CoT）推理的步骤与循环迭代对齐，并在训练过程中应用中间监督。这种额外的迭代监督不仅保留了循环变压器的长度泛化能力，还使其能够预测未见数据的CoT推理步骤。因此，我们利用此循环变压器生成超出训练长度的复杂问题的准确推理链，然后用于微调自回归模型。我们进行了广泛的实验，结果表明了我们方法的有效性，并在自回归模型性能上取得了显著提升。代码将在以下链接发布：https://github.com/your-repo-name。 

---
# Compromising Honesty and Harmlessness in Language Models via Deception Attacks 

**Title (ZH)**: 通过欺骗攻击损害语言模型的诚实性和无害性 

**Authors**: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff  

**Link**: [PDF](https://arxiv.org/pdf/2502.08301)  

**Abstract**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical. 

**Abstract (ZH)**: 近年来，关于大规模语言模型（LLMs）的研究已经证明了它们能够理解和运用欺骗行为，即使没有明确的提示。然而，这种行为仅在少数专门化的情况下被观察到，并未显示出对用户构成严重风险。此外，关于AI对齐的研究已显著推进了训练模型拒绝生成误导性和有害内容的方法。因此，LLMs 通常变得诚实且无害。在此研究中，我们介绍了一种新颖的攻击方法，旨在破坏这两种特性，揭示一种漏洞，如果被利用，可能产生严重的现实世界后果。具体而言，我们引入了一种微调方法，增强模型的欺骗倾向，超越了模型的安全机制。这些“欺骗攻击”可以让模型在被提示特定主题时误导用户，而在其他方面保持准确。此外，我们发现，欺骗性的模型还表现出有害性，生成仇恨言论、刻板印象和其他有害内容。最后，我们评估了模型在多轮对话中是否能持续欺骗，结果参差不齐。鉴于有数百万用户与基于LLM的聊天机器人、语音助手、代理以及其他无法确保可信度的界面进行交互，防止这些模型受到欺骗攻击的安全措施至关重要。 

---
# Exploring the Potential of Large Language Models to Simulate Personality 

**Title (ZH)**: 探索大型语言模型模拟人格的潜力 

**Authors**: Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze  

**Link**: [PDF](https://arxiv.org/pdf/2502.08265)  

**Abstract**: With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）的发展，对话式人工智能的关注点已从仅仅生成连贯且相关的内容，转向解决更加复杂的挑战，如个性化对话系统。为了增强用户参与度，聊天机器人通常被设计成模仿人类行为，其响应在固定的情感范围之内，并遵循一定的价值观。在本文中，我们旨在使用LLMs模拟Big Five人格模型中的个人特质。研究结果显示，生成与人格相关的内容仍然是模型的一个挑战性任务。因此，我们提供了一个预设Big Five特征的数据集，并提出了一种分析框架，以在人格技能的模拟测试中评估LLMs的表现。 

---
# LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention 

**Title (ZH)**: LLM模块：通过增强跨注意力机制从大规模模型向小型模型的知识迁移 

**Authors**: Konstantin Kolomeitsev  

**Link**: [PDF](https://arxiv.org/pdf/2502.08213)  

**Abstract**: In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method. 

**Abstract (ZH)**: 在本文中，我们提出了一种基于LLM模块的架构，该架构通过增强的交叉注意力机制，从一个大型预训练模型向一个小模型转移知识。在提出的方案中，Qwen2-1.5B模型被冻结，并将其表示通过专门设计的注意力层传递给在有限计算资源上进行训练的GPT-Neo-125M模型。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量与蒸馏方法生成的响应质量相当。我们讨论了模块化方法的优势，提供了输入查询示例和比较分析，并概述了该方法进一步扩展的前景。 

---
# Enhancing LLM Character-Level Manipulation via Divide and Conquer 

**Title (ZH)**: 通过分而治之策略提升大语言模型的字符级操控能力 

**Authors**: Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Zhecheng Li, Yiwei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08180)  

**Abstract**: Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and $\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示了较强的泛化能力。然而，它们在字符级别串操作方面表现出明显的不足，难以执行诸如字符删除、插入和替换等基本操作。这些挑战主要源于分词约束，尽管这类操作在数据预处理和代码生成中起着关键作用。通过系统的分析，我们得出两个关键见解：（1）LLMs 在利用内在的分词知识进行字符级别推理方面面临重大困难；（2）原子化的词结构能够显著增强LLMs处理分词级别结构信息的能力。基于这些见解，我们提出了一种名为“分而治之”的新方法（Divide and Conquer for Character-Level Manipulation），该方法旨在弥合分词级别处理与字符级别操作之间的差距。该方法将复杂的操作分解为显式字符级别子任务，并结合受控的分词重建阶段，从而显著提高了准确性。在无需额外训练的情况下，我们的方法在字符删除、字符插入和字符替换任务上的准确性得到了显著提高。为了支持进一步的研究，我们开源了我们的实现和基准测试。 

---
# Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance 

**Title (ZH)**: Fino1：逻辑推理增强的大语言模型在金融领域的迁移性研究 

**Authors**: Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, Qianqian Xie  

**Link**: [PDF](https://arxiv.org/pdf/2502.08127)  

**Abstract**: Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在泛化推理能力方面取得了显著进展，但它们在金融推理方面的有效性尚未充分探索。本研究全面评估了16种强大推理和通用LLMs在三项涉及金融文本、表格数据和方程式的复杂金融任务上的表现，评估了数值推理、表格解释、金融术语理解、长上下文处理和基于方程的问题解决能力。研究结果表明，虽然更好的数据集和预训练可以提高金融推理能力，但通用增强，如基于推理的链式思考（CoT）微调，并不总是能够一致地提高性能。此外，所有推理策略在处理长上下文和多表格任务时均面临挑战。为解决这些局限，我们基于Llama-3.1-8B-Instruct开发了一个金融推理增强模型，采用基于推理的链式思考微调和领域特定推理路径的强化学习。即使仅使用一个金融数据集进行简单的微调，我们的模型在所有任务上也实现了持续的10%的性能提升，平均而言，该模型超过了所有8B模型，甚至超过了Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究结果强调了在金融任务中需要进行领域特定的适应，突出了未来发展方向，如多表格推理、长上下文处理和金融术语理解。我们的所有数据集、模型和代码均已公开。此外，我们还引入了排行榜，用于未来数据集和模型的基准测试。 

---
# HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses 

**Title (ZH)**: HuDEx：结合幻觉检测和可解释性以提高大型语言模型（LLM）响应可靠性 

**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08109)  

**Abstract**: Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的进展表现出有希望的改进，往往在自然语言处理的广泛下游任务中超越现有方法。然而，这些模型仍然面临挑战，这些挑战可能会妨碍它们的实际应用。例如，幻觉现象已知会降低LLMs的可靠性，特别是在需要高度事实精度的领域。当前的基准主要集中在幻觉检测和事实性评估，但并未进一步扩展到超过识别的层面。本文提出了一个增强解释的幻觉检测模型，命名为HuDEx，旨在通过检测幻觉并提供详细解释来增强LLM生成响应的可靠性。所提出的模型提供了一种将检测与解释集成的新方法，使用户和LLM本身能够理解并减少错误。我们的测量结果表明，所提出的模型在幻觉检测准确性方面超越了更大的LLM，例如Llama3 70B和GPT-4，同时保持可靠的解释。此外，所提出的模型在零样本和其他测试环境中均表现良好，展示了其在多种基准数据集中的适应性。通过提出一种将可解释性与幻觉检测集成的新方法，本文进一步增强了幻觉检测研究，进一步提高了语言模型中评估幻觉的性能和可靠性。 

---
# NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals 

**Title (ZH)**: 在显微镜下观察NLI：原子假设分解揭示的内容 

**Authors**: Neha Srikanth, Rachel Rudinger  

**Link**: [PDF](https://arxiv.org/pdf/2502.08080)  

**Abstract**: Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model's consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify critical atomic sub-problems of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the inferential consistency of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts. 

**Abstract (ZH)**: 将文本分解为原子命题是一种灵活的框架，允许我们更仔细地检查输入和输出文本。我们通过这种方式对传统自然语言推理任务（NLI）和弹性质疑推理任务（defeasible NLI）中的假设进行原子分解，形成原子子问题，或模型在解决整体问题时需要评估的细粒度推断。这些原子子问题作为工具，有助于进一步理解NLI和弹性质疑推理的结构，探查模型的一致性和对不同推断的理解能力，以及衡量基准数据集示例的多样性。我们的结果表明，大语言模型（LLMs）在原子NLI和弹性质疑NLI子问题上的逻辑一致性仍然存在问题。最后，我们确定了弹性质疑NLI示例中关键的原子子问题，或对整体标签贡献最大的那些问题，并提出了一种测量模型推断一致性的方法，这是一种旨在捕捉模型在不同背景下对同一个事实做出一致正确或错误预测的度量标准。 

---
# Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs 

**Title (ZH)**: 打破复选框的限制：挑战对LLMs文化对齐的封闭式评估方式 

**Authors**: Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08045)  

**Abstract**: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs. 

**Abstract (ZH)**: 许多研究依赖于封闭式多项选择调查来评估大型语言模型（LLMs）的文化一致性。本文挑战了这种局限性的评估范式，并探索了更为现实、不受限制的方法。我们以世界价值观调查（WVS）和霍夫斯泰德文化维度为例，表明LLMs在不受限制的环境中表现出更强的文化一致性，此时不会强迫作答。此外，我们还展示了即使是轻微的变化，比如重新排列调查选项，也会导致输出不一致，揭示了封闭式评估的局限性。我们的研究结果倡导采用更为稳健和灵活的评估框架，这些框架关注特定的文化代理指标，从而促进对LLMs文化一致性的更为细致和准确的评估。 

---
# Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding 

**Title (ZH)**: 推测而后协作：在解码过程中融合语言模型的知识 

**Authors**: Ziyao Wang, Muneeza Azmart, Ang Li, Raya Horesh, Mikhail Yurochkin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08020)  

**Abstract**: Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications 

**Abstract (ZH)**: 大型语言模型（LLMs）在某些领域表现出色，但在其他领域则可能存在局限性，这主要是由于训练的限制。因此，通过整合互补知识，使LLMs能够协作解决问题，有望提高其跨领域的性能。为了实现这一潜力，我们提出了一种新的协作推测解码（CoSD）算法，在测试时能够有效融合LLM的知识，而无需进行额外的模型训练。CoSD 使用一个草稿模型生成初始序列，并利用易于学习的规则或决策树决定何时调用助手模型以改进这些草稿。CoSD 不仅能够增强知识融合，还能提高推理效率，具有跨领域和模型的可转移性，并提供更好的可解释性。实验结果表明，相比于现有方法，CoSD 在基准测试中将准确性最多提高10%，提供了一种可扩展且有效的基于LLM的应用解决方案。 

---
# Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature? 

**Title (ZH)**: 《陷于语言的罗网：霖幕模型会在医学文献的误导下犯错吗？》

注：这里的“霖幕模型”是对“LLMs”（Large Language Models）的一个意译，考虑到“霖幕”二字既有“霖”，即长时间的雨，又有“幕”，即覆盖、遮挡之意，与“LLMs”在某种意义上隐藏信息、影响理解的特点相契合。但学术翻译中通常避免使用这种双关或意译，建议保持原始的“大型语言模型”表达方式，以符合严格的学术规范。因此，更准确的翻译应为：

《陷于语言的罗网：大型语言模型会在医学文献的误导下犯错吗？》 

**Authors**: Hye Sun Yun, Karen Y.C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace  

**Link**: [PDF](https://arxiv.org/pdf/2502.07963)  

**Abstract**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs. 

**Abstract (ZH)**: 医学研究在将新型治疗方法转化为临床实践方面面临着广泛记录的挑战。发表激励促使研究者倾向于呈现“积极”的研究结果，即使实证结果模棱两可。因此，有充分的证据表明，作者常常会在论文摘要中“旋转”研究结果。这种旋转可能会对临床医生对证据的解释产生影响，并可能影响患者的治疗决策。在本研究中，我们探讨了大型语言模型（LLMs）是否也会受到同样的旋转影响进行结果解释。由于LLMs越来越多地被用于筛选和综合已发表的医学证据，这一问题尤为重要。我们评估了22种LLM模型，发现它们普遍比人类更容易受到旋转的影响。此外，我们还发现LLMs可能会将旋转带入其生成的输出中：例如，它们在生成简洁语言摘要时可能会隐含地纳入旋转。然而，我们也发现，LLMs通常能够识别旋转，并可以通过特定方式激发它们以减轻旋转对其输出的影响。 

---
# Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning 

**Title (ZH)**: 提升法律LLM回应的质量：运用可训练的逻辑结构和语义知识进行法律推理 

**Authors**: Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07912)  

**Abstract**: Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods. 

**Abstract (ZH)**: 大型语言模型（LLMs）在众多领域取得了显著成果，但在法律问答任务中存在明显的不足。LLMs 经常生成缺乏逻辑具体性的泛化回答，这些回答虽然看似正确但其实是不可靠的。检索增强生成（RAG）技术为解决这一挑战提供了一定的方法，但现有的方法通常仅关注语义相似性，忽略了法律推理所必需的逻辑结构。本文提出了一种逻辑-语义整合模型（LSIM），这是一种新颖的监督框架，旨在实现语义和逻辑的一致性。LSIM 包含三个组件：强化学习预测每个问题的结构化事实-规则链，可训练的深度结构语义模型（DSSM）通过整合语义和逻辑特征检索最相关的候选问题，上下文学习生成最终答案。通过一个实际的法律问答数据集进行了实验验证，不仅通过自动评估指标，还通过人工评估，结果表明 LSIM 显著提高了准确性和可靠性，优于现有方法。 

---
# LLM Pretraining with Continuous Concepts 

**Title (ZH)**: LLM 连续概念预训练 

**Authors**: Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08524)  

**Abstract**: Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process. 

**Abstract (ZH)**: 接下来的词预测一直是大规模语言模型预训练中常用的训练目标。通过优化词级别的困惑度，模型学习到了表示。我们提出了一种新颖的预训练框架——连续概念混合（CoCoMix），它结合了离散的下一个词预测与连续的概念。具体来说，CoCoMix 预测从预训练稀疏自动编码器中学习到的连续概念，并通过交替插入词隐藏表示的方式将其混合到模型的隐藏状态中。通过在多个基准测试中的实验，包括语言建模和下游推理任务，我们展示了 CoCoMix 在样本效率方面更具优势，并且一贯优于传统的下一个词预测、知识蒸馏以及插入停顿词。我们发现，在端到端框架中结合概念学习和交替插入是提高性能的关键。此外，CoCoMix 通过允许直接检查和修改预测的概念，增强了模型的可解释性和可控性，提供了一种透明的方法来指导模型的内部推理过程。 

---
# Improving Existing Optimization Algorithms with LLMs 

**Title (ZH)**: 使用大规模语言模型优化现有优化算法 

**Authors**: Camilo Chacón Sartori, Christian Blum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08298)  

**Abstract**: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: this https URL 

**Abstract (ZH)**: 将大型语言模型（LLMs）集成到优化中，创造了一种强大的协同效应，开启了令人兴奋的研究机会。本文探讨了LLMs如何增强现有的优化算法。基于它们的预训练知识，我们展示了它们提出创新启发式变体和实现策略的能力。为了评估这一点，我们应用了一个复杂的优化算法——构造、合并、求解和自适应（CMSA）——这是一个针对组合优化问题的混合元启发式算法，在解决方案构建阶段包含了一个启发式方法。我们的结果表明，GPT-4提出的一种替代启发式方法优于CMSA专家设计的启发式方法，在更大的和更密集的图上，性能差距更加明显。项目网址：[此处插入网址] 

---
# LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits 

**Title (ZH)**: LowRA: 在2位精度下实现LLMs的精确高效LoRA微调 

**Authors**: Zikai Zhou, Qizheng Zhang, Hermann Kumbong, Kunle Olukotun  

**Link**: [PDF](https://arxiv.org/pdf/2502.08141)  

**Abstract**: Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization - mapping, threshold selection, and precision assignment - while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance-precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50%. Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments. 

**Abstract (ZH)**: 随着模型参数数量增加到数十亿甚至数百亿，大型语言模型（LLMs）的微调成本日益增加，即使是参数效率微调（PEFT）方法如LoRA也仍然具有资源密集性。我们提出了LowRA，这是首个使LoRA微调能够在每个参数低于2位的情况下实现，并且最小化性能损失的框架。LowRA通过对量化进行细粒度优化——包括映射、阈值选择和精度分配——并利用高效的CUDA内核进行可扩展部署。在4个不同的LLM和4个不同数据集上的广泛评估表明，LowRA在高于2位时能够实现优越的性能-精度权衡，并且在低至1.15位时保持准确，可将内存使用量降低多达50%。我们的结果突显了超低位LoRA微调在资源受限环境中的潜力。 

---
# Ensemble based approach to quantifying uncertainty of LLM based classifications 

**Title (ZH)**: 基于集成的方法对基于大语言模型（LLM）分类的不确定性量化 

**Authors**: Srijith Rajamohan, Ahmed Salhin, Josh Frazier, Rohit Kumar, Yu-Cheng Tsai, Todd Cook  

**Link**: [PDF](https://arxiv.org/pdf/2502.08631)  

**Abstract**: The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes. 

**Abstract (ZH)**: 大型语言模型（LLMs）的输出是模型内部参数和输入到上下文窗口的数据的函数。本研究的假设是，在贪婪采样策略下，LLM的输出方差与模型参数知识中嵌入的概念确定性以及输入的词素方差有关。通过对模型进行微调，可以降低模型输出对词素输入变化的敏感性。这一方法随后被应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。 

---
# Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences 

**Title (ZH)**: 弥合安全差距：一种可信赖的大规模语言模型推理的安全管道 

**Authors**: Shanshan Han, Salman Avestimehr, Chaoyang He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08142)  

**Abstract**: We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%. 

**Abstract (ZH)**: 我们介绍了Wildflare GuardRail，这是一种护栏管道，旨在通过系统地解决整个处理工作流中的风险来增强大型语言模型（LLM）推理的安全性和可靠性。Wildflare GuardRail 集成了一系列核心功能模块，包括安全检测器（Safety Detector），它能够识别不安全的输入并检测模型输出中的幻觉，同时生成根本原因解释；上下文模块（Grounding），它通过从向量数据库中检索信息来对用户查询进行语境化；实时调整器（Customizer），它使用轻量级、基于规则的封装器在实时调整输出；以及修复器（Repairer），它使用安全检测器提供的幻觉解释来纠正LLM的错误输出。结果显示，我们的不安全内容检测模型在安全检测器中的表现与OpenAI API相当，尽管是基于一个小数据集训练的，该数据集使用了几个公开数据集构建。此外，轻量级封装器可以在每条查询1.06秒内以100%的准确率处理模型输出中的恶意网址，而无需昂贵的模型调用。同时，幻觉修正模型在减少幻觉方面表现出有效性，准确率为80.7%。 

---
# Universal Adversarial Attack on Aligned Multimodal LLMs 

**Title (ZH)**: 面向对齐多模态LLM的通用对抗攻击 

**Authors**: Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev  

**Link**: [PDF](https://arxiv.org/pdf/2502.07987)  

**Abstract**: We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers. 

**Abstract (ZH)**: 我们提出了一个针对多模态大型语言模型（LLMs）的通用对抗攻击方法，该方法利用单一优化过的图像来覆盖跨多种查询乃至不同模型的安全对齐保护措施。通过反向传播通过视觉编码器和语言头，我们生成一个合成图像，迫使模型以目标短语（例如，“当然，这就是它”）或不安全的内容（即使对于有害的提示也是如此）作出回应。在SafeBench基准测试中，我们的方法在某些模型上的攻击成功率远高于现有基线方法（例如，最高可达93%）。我们还通过同时训练几种多模态LLMs并在未见过的架构上进行测试，展示了跨模型的转移能力。此外，我们方法的多答案变体产生了更加自然但仍然具有恶意性质的回答。这些发现揭示了当前多模态对齐中的关键漏洞，并呼吁开发更加稳健的对抗性防御措施。我们将按照Apache-2.0许可证发布代码和数据集。请注意：本文中由多模态LLMs生成的一些内容可能对某些读者具有冒犯性。 

---
# Deep Semantic Graph Learning via LLM based Node Enhancement 

**Title (ZH)**: 基于LLM节点增强的深度语义图学习 

**Authors**: Chuanqi Shi, Yiyi Tao, Hang Zhang, Lun Wang, Shaoshuai Du, Yixian Shen, Yanxin Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07982)  

**Abstract**: Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models. 

**Abstract (ZH)**: 图学习由于其在广泛的实际应用中的重要意义而受到了广泛关注。当前主流方法依赖于文本节点特征，并通过使用图神经网络（GNNs）进行浅层嵌入学习来获取初始节点嵌入，这种方法在捕捉深层次的文本语义方面表现出局限性。近年来，大规模语言模型（LLMs）的发展展示了在理解文本语义方面优于传统方法的能力，从而改变了传统的文本特征处理方式。本文提出了一种结合图变换器架构和LLM增强节点特征的新框架。具体来说，我们利用LLM生成丰富的文本节点语义表示，这些表示随后通过图变换器中的多头自注意力机制来捕捉局部和全局图结构信息。我们的模型利用Transformer的注意力机制动态聚合邻域信息，同时保留LLM嵌入提供的语义丰富性。实验结果表明，LLM增强的节点特征显著提高了图学习模型在节点分类任务上的性能。这一方法在多种图学习任务中都显示出有希望的结果，为图网络与语言模型的结合提供了一个实用的方向。 

---
# Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment 

**Title (ZH)**: 逻辑单元中的推理：通过逻辑单元对齐扩展大型语言模型的测试时推理 

**Authors**: Cheryl Li, Tianyuan Xu, Yiwen Guo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07803)  

**Abstract**: Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations." This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability. 

**Abstract (ZH)**: 链推理（Chain-of-Thought, CoT）提示法通过生成自然语言（NL）推理过程，以引导至最终答案，在提升大规模语言模型（LLMs）的推理能力方面显示出潜力。然而，它在处理数值计算方面存在困难，这一问题推动了程序辅助技术的发展。尽管这些技术具有潜力，但仍存在一个持续的挑战：LLMs报告的推理步骤与生成的程序中的逻辑之间的一致性问题，我们称之为“推理幻觉”。这源于自然语言的固有歧义性和LLMs的统计性质，它们往往缺乏严格的逻辑连贯性。为应对这一挑战，我们提出了一种新的测试时缩放框架——逻辑单元作为推理（Reasoning-as-Logic-Units, RaLU），该框架通过对生成的程序与其对应的NL描述中的逻辑单元进行对齐，构建了一个更可靠的推理路径。通过使用静态分析将初始生成的程序细分为离散单元，RaLU与LLMs进行迭代对话，评估、调整和完善每个单元。回滚并修正机制确保每个单元中的代码声明与任务要求之间的一致性，最终在程序逻辑下形成一个连贯的推理路径，模型在此基础上得出最终解决方案。我们的实验结果显示，RaLU在数学推理（GSM8K, MATH）和算法推理（HumanEval+, MBPP+）方面显著优于现有基准，证明了其通过提高准确性和可解释性来推动LLMs推理和编程领域的潜在价值。 

---
# Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks 

**Title (ZH)**: 商业化的大型语言模型代理已经容易受到简单的但危险的攻击 

**Authors**: Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08586)  

**Abstract**: A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning. 

**Abstract (ZH)**: 近年来，大量的机器学习（ML）安全文献主要关注针对对齐的大语言模型（LLMs）的攻击。这些攻击有可能提取私人信息或将模型引导向产生有害输出。在实际部署中，LLMs 很少是孤立存在的，它们通常作为更大自主管道的一部分，包括内存系统、信息检索、网络访问和API调用。这些额外的组件引入了新的脆弱性，使这些LLM驱动的代理比孤立的LLMs更容易受到攻击，但目前很少有工作关注这些LLM代理的安全性。在本文中，我们分析了仅针对LLM代理的独特安全和隐私漏洞。首先，我们提供了一种基于威胁行为者、攻击目标、入口点、攻击者可观察性、攻击策略和代理管道固有脆弱性的攻击分类体系。然后，我们对流行的开源和商用代理进行了系列示范攻击，展示了其漏洞的即时实用影响。值得注意的是，我们的攻击非常容易实现，且不需要了解机器学习的知识。 

---
# Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies 

**Title (ZH)**: 促进对大型语言模型适当依赖：解释、来源和不一致性的角色 

**Authors**: Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, Olga Russakovsky  

**Link**: [PDF](https://arxiv.org/pdf/2502.08554)  

**Abstract**: Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs. 

**Abstract (ZH)**: 大规模语言模型（LLMs）可以生成听起来流畅且令人信服的错误回答，增加了用户将这些回答视为正确的风险。减轻这种过度依赖是一个关键挑战。通过一项think-aloud研究，参与者使用包含LLM的应用程序回答客观问题，我们确定了几种影响用户依赖的因素：回答的解释、解释中的不一致性和来源。通过一项大规模的、预先注册的、受控的实验（N=308），我们分离并研究了这些因素对用户依赖、准确性和其他指标的影响。我们发现，存在解释会导致用户对正确和错误的回答都增加依赖。然而，当我们提供来源或解释存在不一致时，用户对错误回答的依赖较少。我们讨论了这些发现对促进适当依赖LLMs的意义。 

---
# Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers 

**Title (ZH)**: democratizing ai: 在基于gpu的超级计算机上实现开源可扩展的大语言模型训练 

**Authors**: Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin Wen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, Abhinav Bhatele  

**Link**: [PDF](https://arxiv.org/pdf/2502.08145)  

**Abstract**: Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).
While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore "catastrophic memorization", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier. 

**Abstract (ZH)**: 训练和微调具有数百亿到万亿参数的大语言模型（LLMs）需要数千甚至更多的GPU，并且需要一个高度可扩展的软件栈。在本工作中，我们介绍了一种新颖的四维混合并行算法，并在名为AxoNN的高度可扩展、可移植、开源框架中实现了该算法。我们描述了AxoNN中的若干性能优化措施，包括改进矩阵乘法内核性能、将非阻塞聚合与计算重叠以及进行性能建模以选择性能最优配置。这些措施使得在Perlmutter（620.1千万亿次/秒）、Frontier（1.381千万亿次/秒）和Alps（1.423千万亿次/秒）等超级计算机上训练GPT风格的转换器模型达到了前所未有的可扩展性和峰值浮点运算率（bf16）。

随着可训练参数数量的增加，LLMs的能力不断提升，同时也会带来隐私和版权风险的增加。训练数据的记忆可能导致在推理时泄露敏感或私人信息。我们通过探索“灾难性记忆化”的实验突显了这种规模效应的副作用，其中模型足够大可以在一次通过中记忆训练数据，并提出了一种防止这一问题的方法。作为这项研究的一部分，我们展示了使用AxoNN在Frontier上对一个具有4050亿参数的LLM进行微调。 

---
# CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs 

**Title (ZH)**: CIRCUIT：评估大型语言模型电路解释与推理能力的基准评测 

**Authors**: Lejla Skelic, Yan Xu, Matthew Cox, Wenjie Lu, Tao Yu, Ruonan Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.07980)  

**Abstract**: The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design. 

**Abstract (ZH)**: 在模拟电路设计领域，大型语言模型（LLMs）的作用尚未得到广泛探索，此类模型可以通过超越传统优化技术的基于推理的方法带来益处。尽管LLMs的重要性日益提高，但目前尚未有评估它们对电路进行推理能力的基准。因此，我们创建了CIRCUIT数据集，其中包括了涵盖各种模拟电路相关主题的510个问答对。我们数据集中表现最好的模型GPT-4o在最终数值答案上的准确率为48.04%。为了评估LLMs在该数据集上的稳健性，我们引入了一个独特特征，通过将问题分组为单元测试来实现类似单元测试的评估方法。在这种情况下，GPT-4o只能通过27.45%的单元测试，这表明最先进的LLMs在理解电路方面仍存在问题，因为电路理解需要多级推理能力，特别是在涉及电路拓扑时。这种针对电路的特定基准突显了LLMs的局限性，为他们在模拟集成电路设计中的应用提供有价值的见解。 

---
# CryptoX : Compositional Reasoning Evaluation of Large Language Models 

**Title (ZH)**: CryptoX：大型语言模型组合推理评估 

**Authors**: Jiajun Shi, Chaoren Wei, Liqun Yang, Zekun Moore Wang, Chenghao Yang, Ge Zhang, Stephen Huang, Tao Peng, Jian Yang, Zhoufutu Wen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07813)  

**Abstract**: The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs. 

**Abstract (ZH)**: 成分推理能力长期以来被认为是大型语言模型（LLM）泛化能力和智能涌现的关键。然而，尽管存在许多推理相关的基准，但在现有的基准中，LLM的成分推理能力很少被研究和量化。本文中，我们引入了一种评价框架CryptoX，这是首次将现有基准与密码学相结合，以量化LLM的成分推理能力。基于CryptoX，我们构建了CryptoBench，将这些原则整合到若干基准中，以便进行系统性的评估。我们使用CryptoBench对广泛使用的开源和封闭源LLM进行了详细的实验，揭示了开源和封闭源LLM之间存在巨大的差距。我们进一步进行了详细的机械可解释性实验，以揭示LLM成分推理的内在机制，包括子问题分解、子问题推理以及总结子问题结论。通过CryptoBench的分析，我们强调了独立研究成分推理的价值，并突显了增强LLM成分推理能力的必要性。 

---
