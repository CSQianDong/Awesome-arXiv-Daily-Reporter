# Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples 

**Title (ZH)**: 通过LLM生成的对抗性样本跨语言考察多语言嵌入模型 

**Authors**: Andrianos Michail, Simon Clematide, Rico Sennrich  

**Link**: [PDF](https://arxiv.org/pdf/2502.08638)  

**Abstract**: The evaluation of cross-lingual semantic search capabilities of models is often limited to existing datasets from tasks such as information retrieval and semantic textual similarity. To allow for domain-specific evaluation, we introduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual semantic search task that requires only a set of parallel sentence pairs of the language pair of interest within the target domain. This task focuses on the ability of a model to cross-lingually rank the true parallel sentence higher than hard negatives generated by a large language model. We create four instances of our introduced CLSD task for the language pair German-French within the domain of news. Within this case study, we find that models that are also fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using English as the pivot language, while bitext mining models such as LaBSE perform best directly cross-lingually. We also show a fine-grained similarity analysis enabled by our distractor generation strategy, indicating that different embedding models are sensitive to different types of perturbations. 

**Abstract (ZH)**: 以下是内容翻译成中文，符合学术规范的版本：

跨语言语义搜索模型的语义搜索能力评估通常仅限于信息检索和语义文本相似性等任务中的现有数据集。为实现特定领域的评估，我们提出了跨语言语义区分（CLSD）这一新颖的跨语言语义搜索任务，该任务仅需包含目标领域内感兴趣语言对的平行句子对集。该任务关注模型在跨语言环境下将真实的平行句子排名高于大型语言模型生成的难负样本的能力。我们为德语-法语语言对在新闻领域内引入了四个CLSD任务实例。在这一案例研究中，我们发现同时微调过检索任务的模型（如多语言E5）倾向于使用英语作为中介语言，而基于平行文本挖掘模型（如LaBSE）则直接在跨语言环境中表现最佳。我们还展示了由我们的诱饵生成策略所支持的细粒度相似性分析，表明不同的嵌入模型对不同类型的扰动具有不同的敏感性。 

---
# SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent 

**Title (ZH)**: SPECtrum: 一种基于大型语言模型的代理多维身份表示的地基框架 

**Authors**: Keyeun Lee, Seo Hyeong Kim, Seolhee Lee, Jinsu Eun, Yena Ko, Hayeon Jeon, Esther Hehsun Kim, Seonghye Cho, Soeun Yang, Eun-mee Kim, Hajin Lim  

**Link**: [PDF](https://arxiv.org/pdf/2502.08599)  

**Abstract**: Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual's multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum's effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters' identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies. 

**Abstract (ZH)**: 现有的个体身份模拟方法常倾向于简化人类的复杂性，可能导致身份表示不完整或扁平化。为解决这一问题，我们提出了一种基于grounded框架SPeCtrum，用于构建真实的LLM代理人格，该框架通过融入个体的多维度自我概念。SPeCtrum整合了三个核心组件：社会身份（S）、个人身份（P）和个人生活背景（C），各自贡献了身份的不同但又相互关联的方面。为了评估SPeCtrum在身份表示方面的有效性，我们进行了自动化和人工评价。使用流行戏剧人物进行的自动化评价表明，个人生活背景（C）——来自偏好和日常活动简短文章的模型——的表现优于仅使用社会身份（S）和个人身份（P），且与S、P和C的组合效果相当。相比之下，涉及实际个人的真人评价发现，S、P和C的组合提供了比C单一维度更为全面的自我概念表示。研究结果表明，虽然单一的C维度可能足以进行基本的身份模拟，但结合S、P和C能够增强现实生活中身份表示的真实性和准确性。总体而言，SPeCtrум提供了在LLM代理中模拟个体的结构化方法，从而有助于更加个性化的真人-AI互动，并提升基于模拟的行为研究的真实性。 

---
# Quality-Aware Decoding: Unifying Quality Estimation and Decoding 

**Title (ZH)**: 质量感知解码：融合质量评估与解码 

**Authors**: Sai Koneru, Matthias Huck, Miriam Exel, Jan Niehues  

**Link**: [PDF](https://arxiv.org/pdf/2502.08561)  

**Abstract**: An emerging research direction in NMT involves the use of Quality Estimation (QE) models, which have demonstrated high correlations with human judgment and can enhance translations through Quality-Aware Decoding. Although several approaches have been proposed based on sampling multiple candidate translations, none have integrated these models directly into the decoding process. In this paper, we address this by proposing a novel token-level QE model capable of reliably scoring partial translations. We build a uni-directional QE model for this, as decoder models are inherently trained and efficient on partial sequences. We then present a decoding strategy that integrates the QE model for Quality-Aware decoding and demonstrate that the translation quality improves when compared to the N-best list re-ranking with state-of-the-art QE models (upto $1.39$ XCOMET-XXL $\uparrow$). Finally, we show that our approach provides significant benefits in document translation tasks, where the quality of N-best lists is typically suboptimal. 

**Abstract (ZH)**: 在神经机器翻译（NMT）领域的新兴研究方向之一是使用质量估计（QE）模型，这些模型已显示出与人工判断高度相关的特性，并能通过质量意识解码增强翻译。尽管已经提出了基于采样多个候选翻译的几种方法，但这些方法中没有一种能够直接将这些模型集成到解码过程中。本文通过提出一种新颖的子词级QE模型，能够可靠地评估部分翻译，解决了这一问题。我们为这一目标构建了一个单向QE模型，因为解码器模型在处理部分序列时本就具有训练和高效的特性。随后，我们提出了一种集成QE模型的质量意识解码策略，并证明与最先进的QE模型进行N-best列表重排序相比（最高提高1.39个XCOMET-XXL分数），翻译质量有所提升。最后，我们展示了在文档翻译任务中，我们的方法提供了显著的优势，而在这类任务中，N-best列表的质量通常不尽如人意。 

---
# LLMs can implicitly learn from mistakes in-context 

**Title (ZH)**: LLMs可以从上下文中的错误中隐式地学习。 

**Authors**: Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08550)  

**Abstract**: Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. 

**Abstract (ZH)**: 从错误中学习是人类智能的一个基本特征。先前的研究已经表明，当大型语言模型（LLMs）被提供的详细错误原因说明时，它们也可以从错误的答案中学习。在本研究中，我们探讨了在未提供这些解释的情况下，LLMs是否可以从数学推理任务中的错误中学习。我们研究了LLMs是否能够仅仅通过观察错误和正确的答案来隐式推断出这样的解释。令人惊讶的是，我们发现，当从上下文中删除解释并仅展示错误和正确的答案时，LLMs的平均表现更好。这种方法在我们的评估中也显著优于chain-of-thought提示方法。我们的结果显示，这些结果在不同大小和不同推理能力的LLMs中是一致的。进一步地，我们进行了深入分析，并展示了使用错误和正确答案进行提示的性能和泛化能力优于在上下文中引入更多样化的问答对。最后，我们展示了仅观察错误和正确答案的模型生成的新解释与使用示范解释生成的解释在人类评价中得分相当。我们的结果表明，LLMs确实能够进行上下文中的隐式学习。 

---
# Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation 

**Title (ZH)**: 忠实、不忠实或模棱两可？基于初始立场的多智能体辩论摘要评价 

**Authors**: Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, Hang Su  

**Link**: [PDF](https://arxiv.org/pdf/2502.08514)  

**Abstract**: Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries. 

**Abstract (ZH)**: 基于大型语言模型（LLMs）的忠实性评估器常常被文本的流畅度所迷惑，在识别摘要中的错误方面表现不佳。我们提出了一种摘要忠实性评估方法，其中多个基于LLM的代理被赋予初始立场（无论其真实信念如何），并被迫提出理由以证明所施加的信念，从而进行多轮辩论以达成共识。均衡分布的初始分配导致了更多样化的立场，进而促进了更有意义的辩论，并最终发现了更多的错误。此外，通过对最近的忠实性评估数据集进行分析，我们观察到自然地，并非所有摘要要么忠实于源文档，要么完全不忠实。因此，我们引入了一个新的维度——模糊性，并提出了一套详细分类法来识别这种特殊情况。实验结果表明，我们的方法能够帮助识别模糊性，并且在处理非模糊性摘要时表现甚至更佳。 

---
# Measuring Diversity in Synthetic Datasets 

**Title (ZH)**: 测量合成数据集中的多样性 

**Authors**: Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian  

**Link**: [PDF](https://arxiv.org/pdf/2502.08512)  

**Abstract**: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）广泛用于为各种自然语言处理（NLP）任务生成合成数据集，例如文本分类和摘要生成。然而，准确测量这些合成数据集的多样性——这是确保模型鲁棒性能的关键方面——仍然是一个重大挑战。本文介绍了一种新的方法DCScore，它从分类角度衡量合成数据集的多样性。具体而言，DCScore将多样性评估形式化为一个样本分类任务，并利用样本之间的相互关系。我们还提供了对DCScore满足的与多样性相关的公理的理论验证，突显了其作为原理性的多样性评估方法的作用。通过对合成数据集的实验结果表明，DCScore与多个评估数据集的多样性伪真理具有更强的相关性，从而证明其有效性。此外，实证和理论证据都证明，与现有方法相比，DCScore在计算成本上具有显著的优势。源代码可在下面的链接中获取：this https URL。 

---
# Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction 

**Title (ZH)**: 基于解释的多语言语法错误纠正情境演示检索 

**Authors**: Wei Li, Wen Luo, Guangyue Peng, Houfeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08507)  

**Abstract**: Grammatical error correction (GEC) aims to correct grammatical, spelling, and semantic errors in natural language text. With the growing of large language models (LLMs), direct text generation has gradually become the focus of the GEC methods, and few-shot in-context learning presents a cost-effective solution. However, selecting effective in-context examples remains challenging, as the similarity between input texts does not necessarily correspond to similar grammatical error patterns. In this paper, we propose a novel retrieval method based on natural language grammatical error explanations (GEE) to address this issue. Our method retrieves suitable few-shot demonstrations by matching the GEE of the test input with that of pre-constructed database samples, where explanations for erroneous samples are generated by LLMs. We conducted multilingual GEC few-shot experiments on both major open-source and closed-source LLMs. Experiments across five languages show that our method outperforms existing semantic and BM25-based retrieval techniques, without requiring additional training or language adaptation. This also suggests that matching error patterns is key to selecting examples. 

**Abstract (ZH)**: 语法错误修正（GEC）的目标是纠正自然语言文本中的语法、拼写和语义错误。随着大型语言模型（LLMs）的发展，直接文本生成逐渐成为GEC方法的研究重点，少量示例的上下文学习提供了一种成本效益高的解决方案。然而，选择有效的上下文示例仍然是一个挑战，因为输入文本之间的相似性并不一定意味着相似的语法错误模式。在本文中，我们提出了一种基于自然语言语法错误解释（GEE）的新检索方法，以解决这一问题。我们的方法通过将测试输入的GEE与预先构建的数据库样本的GEE进行匹配，来检索合适的少量示例。其中，错误样本的解释由LLMs生成。我们在主要的开源和闭源LLMs上进行了多语言GEC的少量示例实验。在五种语言的实验结果显示，我们的方法在不需要额外训练或语言适应的情况下优于现有的语义和BM25基于的检索技术。这还表明，匹配错误模式是选择示例的关键。 

---
# Salamandra Technical Report 

**Title (ZH)**: 《萨拉曼德ra技术报告》

注：在此翻译中，“Salamandra”一词未提供具体的含义或背景信息，通常来说，“Salamandra”是拉丁文，意为“萨拉曼德龙”或“火蜥蜴”，有时也可能是指一个特定的项目、系统或公司的名称。如果没有更多的上下文信息，我们基于其常见的含义进行了翻译。如果有具体的技术背景或其他特定含义，请提供更多信息以便进行更准确的翻译。 

**Authors**: Aitor Gonzalez-Agirre, Marc Pàmies, Joan Llop, Irene Baucells, Severino Da Dalt, Daniel Tamayo, José Javier Saiz, Ferran Espuña, Jaume Prats, Javier Aula-Blasco, Mario Mina, Adrián Rubio, Alexander Shvets, Anna Sallés, Iñaki Lacunza, Iñigo Pikabea, Jorge Palomar, Júlia Falcão, Lucía Tormo, Luis Vasquez-Reina, Montserrat Marimon, Valle Ruíz-Fernández, Marta Villegas  

**Link**: [PDF](https://arxiv.org/pdf/2502.08489)  

**Abstract**: This work introduces Salamandra, a suite of open-source decoder-only large language models available in three different sizes: 2, 7, and 40 billion parameters. The models were trained from scratch on highly multilingual data that comprises text in 35 European languages and code. Our carefully curated corpus is made exclusively from open-access data compiled from a wide variety of sources. Along with the base models, supplementary checkpoints that were fine-tuned on public-domain instruction data are also released for chat applications. Additionally, we also share our preliminary experiments on multimodality, which serve as proof-of-concept to showcase potential applications for the Salamandra family. Our extensive evaluations on multilingual benchmarks reveal that Salamandra has strong capabilities, achieving competitive performance when compared to similarly sized open-source models. We provide comprehensive evaluation results both on standard downstream tasks as well as key aspects related to bias and this http URL this technical report, we intend to promote open science by sharing all the details behind our design choices, data curation strategy and evaluation methodology. In addition to that, we deviate from the usual practice by making our training and evaluation scripts publicly accessible. We release all models under a permissive Apache 2.0 license in order to foster future research and facilitate commercial use, thereby contributing to the open-source ecosystem of large language models. 

**Abstract (ZH)**: 本文介绍了Salamandra，这是一个开源的解码器型大规模语言模型系列，共提供三种不同规模的模型：20亿、70亿和400亿参数。这些模型从包含35种欧洲语言和代码文本的多语种数据集中从头开始训练。我们精心选择的语料库，仅由来自多种来源的开放访问数据编制而成。除了基础模型外，我们还发布了在公共领域指令数据上微调的附加检查点，用于聊天应用。此外，我们还分享了初步的多模态实验，以概念验证的形式展示了Salamandra家族的潜在应用。我们在多语种基准上的广泛评估结果显示，Salamandra具有很强的能力，与同等规模的开源模型相比，其性能具有竞争力。我们在技术报告中，不仅提供了标准下游任务和与偏差相关的关键方面的全面评估结果，还展示了我们的设计选择、数据编制策略和评估方法的详细信息。此外，我们还公开了训练和评估脚本，不遵循惯例。我们所有模型均采用宽松的Apache 2.0许可证发布，旨在促进未来研究并促进商业应用，从而促进大型语言模型开源生态系统的贡献。 

---
# Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning 

**Title (ZH)**: 通过循环对齐推理增强自回归链式思考 

**Authors**: Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08482)  

**Abstract**: Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at this https URL. 

**Abstract (ZH)**: 链式思维（CoT）提示已成为提升语言模型推理能力的一种强大技术。然而，生成长且正确的CoT轨迹具有挑战性。近期研究表明，循环变压器具有卓越的长度泛化能力，但它们的局限性使其无法替代自回归解决方案。为了更好地发挥循环变压器的优势，我们提出了RELAY（REasoning through Loop Alignment iterativelY）。具体而言，我们将链式思维（CoT）推理的步骤与循环迭代对齐，并在训练过程中应用中间监督。这种额外的迭代监督不仅保留了循环变压器的长度泛化能力，还使其能够预测未见数据的CoT推理步骤。因此，我们利用此循环变压器生成超出训练长度的复杂问题的准确推理链，然后用于微调自回归模型。我们进行了广泛的实验，结果表明了我们方法的有效性，并在自回归模型性能上取得了显著提升。代码将在以下链接发布：https://github.com/your-repo-name。 

---
# Examining Spanish Counseling with MIDAS: a Motivational Interviewing Dataset in Spanish 

**Title (ZH)**: 使用MIDAS探究西班牙语 counseling：一项基于动机访谈的数据集 

**Authors**: Aylin Gunal, Bowen Yi, John Piette, Rada Mihalcea, Verónica Pérez-Rosas  

**Link**: [PDF](https://arxiv.org/pdf/2502.08458)  

**Abstract**: Cultural and language factors significantly influence counseling, but Natural Language Processing research has not yet examined whether the findings of conversational analysis for counseling conducted in English apply to other languages. This paper presents a first step towards this direction. We introduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling dataset created from public video sources that contains expert annotations for counseling reflections and questions. Using this dataset, we explore language-based differences in counselor behavior in English and Spanish and develop classifiers in monolingual and multilingual settings, demonstrating its applications in counselor behavioral coding tasks. 

**Abstract (ZH)**: 文化因素和语言因素显著地影响咨询效果，但自然语言处理研究尚未探讨用英语进行咨询会话分析所得到的 findings 是否适用于其他语言。本文旨在迈出这一方向的第一步。我们介绍了 MIDAS（动机性访谈西班牙语数据集），这是一个从公共视频资源中创建的咨询数据集，包含专家对咨询反思和问题的标注。使用该数据集，我们探索了英语和西班牙语中咨询师行为的语言差异，并在单语和多语环境中开发了分类器，展示了其在咨询师行为编码任务中的应用。 

---
# Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring 

**Title (ZH)**: 面向提示泛化的语法感知跨提示自动作文评分 

**Authors**: Heejin Do, Taehee Park, Sangwon Ryu, Gary Geunbae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.08450)  

**Abstract**: In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method's generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts. 

**Abstract (ZH)**: 在自动作文评分（AES）领域，近年来的研究重点转向了跨提示设置，即在未见过的提示下评分作文，以提高其实用性。然而，之前的方法使用特定提示下的作文评分对进行训练，在获得提示通用的作文表示方面仍存在挑战。为了解决这一问题，本文提出了一种语法感知的跨提示特质评分（GAPS），它内部捕获与提示无关的句法特征，以学习通用的作文表示。我们通过语法错误纠正技术获取作文中的修正后的语法信息，并设计AES模型使其无缝集成这种信息。通过同时参考修正后的和原始作文，模型可以在训练过程中专注于通用特征。实验结果验证了我们方法的通用性，显示了在提示无关性和语法相关特质上的显著改进。此外，在最具有挑战性的跨提示场景中，GAPS实现了显著的QWK增益，突显了其评估未见过的提示的能力。 

---
# Better Embeddings with Coupled Adam 

**Title (ZH)**: 更好的嵌入表示：耦合的Adam优化器 

**Authors**: Felix Stollenwerk, Tobias Stollenwerk  

**Link**: [PDF](https://arxiv.org/pdf/2502.08441)  

**Abstract**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）具备卓越的能力，但它们学习的词表示仍然表现出一种不希望出现但对其理解尚浅的各向异性特征。在本文中，我们提出，Adam的第二个矩是导致各向异性嵌入的原因，并建议使用一种改进的优化器——耦合Adam（Coupled Adam）来缓解这一问题。我们的实验表明，耦合Adam显著提高了嵌入的质量，并且在大型数据集上，还能够提高上游和下游任务的表现。 

---
# From Haystack to Needle: Label Space Reduction for Zero-shot Classification 

**Title (ZH)**: 从haystack到needle：零样本分类中的标签空间减少 

**Authors**: Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke  

**Link**: [PDF](https://arxiv.org/pdf/2502.08436)  

**Abstract**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference. 

**Abstract (ZH)**: 我们提出了一种新颖的方法Label Space Reduction (LSR)，用于提高大型语言模型（LLMs）的零样本分类性能。LSR 通过系统性地对候选类别进行排序和减少，逐步优化分类标签空间，使模型能够集中关注最相关的选择。通过利用驱动数据的模型的统计学习能力，LSR 能够在测试时动态优化标签空间表示。我们在七个基准测试中的实验结果显示，与标准的零样本分类基线相比，LSR 在 Llama-3.1-70B 上平均提高了 7.0% 的宏观 F1 得分（最高提高 14.2%），在 Claude-3.5-Sonnet 上提高了 3.3% 的宏观 F1 得分（最高提高 11.1%）。为了减少 LSR 的计算开销，每次迭代都需要额外的 LLM 调用，我们提出了一种将模型蒸馏为概率分类器的方法，从而实现高效的推理。 

---
# A Semantic Parsing Algorithm to Solve Linear Ordering Problems 

**Title (ZH)**: 解决线性排序问题的语义解析算法 

**Authors**: Maha Alkhairy, Vincent Homer, Brendan O'Connor  

**Link**: [PDF](https://arxiv.org/pdf/2502.08415)  

**Abstract**: We develop an algorithm to semantically parse linear ordering problems, which require a model to arrange entities using deductive reasoning. Our method takes as input a number of premises and candidate statements, parsing them to a first-order logic of an ordering domain, and then utilizes constraint logic programming to infer the truth of proposed statements about the ordering.
Our semantic parser transforms Heim and Kratzer's syntax-based compositional formal semantic rules to a computational algorithm. This transformation involves introducing abstract types and templates based on their rules, and introduces a dynamic component to interpret entities within a contextual framework.
Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to answer multiple choice questions in BIG-bench's logical_deduction multiple choice problems, achieving perfect accuracy, compared to 67.06% for the best-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.
These promising results demonstrate the benefit of developing a semantic parsing algorithm driven by first-order logic constructs. 

**Abstract (ZH)**: 我们开发了一个算法来语义解析线性排序问题，这些问题是要求模型利用演绎推理来排列实体。我们的方法以若干前提和候选陈述作为输入，将其解析为排序领域的一阶逻辑，并利用约束逻辑编程推断有关排列的提出的陈述的真伪。

我们的语义解析器将Heim和Kratzer基于语法的组合形式语义规则转换为计算算法。这一转换过程包括引入基于他们规则的抽象类型和模板，并引入了一个动态组件来在上下文框架中解释实体。

我们符号化的系统，形式语义逻辑推理器（FSLI），被应用于BIG-bench的逻辑演绎多选题，实现了100%的准确率，而最佳的大型语言模型（GPT-4）和混合系统Logic-LM的准确率分别为67.06%和87.63%。

这些令人鼓舞的结果表明，基于一阶逻辑构造开发语义解析算法的益处。 

---
# IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance 

**Title (ZH)**: IssueBench：用于衡量LLM写作辅助中问题偏见的数百万个现实prompt集 

**Authors**: Paul Röttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, Dirk Hovy  

**Link**: [PDF](https://arxiv.org/pdf/2502.08395)  

**Abstract**: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them. 

**Abstract (ZH)**: 大规模语言模型（LLMs）正在帮助数以百万计的用户撰写关于各种话题的文字，并在此过程中向用户提供不同的观点和视角。这引发了关于议题偏见的问题，即LLM倾向于在一个议题上仅呈现一个视角，从而可能影响用户对该议题的看法。目前，还没有办法测量LLM实际在真实用户交互中表现出的议题偏见，使得难以应对有偏见的LLM所带来的风险。因此，我们构建了IssueBench：一套包含249万真实场景提示，用于衡量LLM写作辅助中的议题偏见。这些提示基于3900个模板（例如，“写一篇关于……的博客”）和212个实际的政治理由（例如，“人工智能监管”）从真实用户交互中构建而来。通过使用IssueBench，我们发现最先进的LLM中普遍存在且固有的议题偏见。同时，我们也发现这些偏见在不同模型之间表现出惊人的相似性，并且所有模型在某些议题上更加倾向于美国民主党而非共和党选民的观点。IssueBench 可以轻松地扩展以包含其他议题、模板或任务。通过提供稳健且真实的测量方法，我们希望IssueBench能够为关于LLM偏见以及如何解决这些问题的持续讨论提供新的证据质量。 

---
# Unveiling Global Discourse Structures: Theoretical Analysis and NLP Applications in Argument Mining 

**Title (ZH)**: 揭示全球话语结构：论理论分析与 argument 矿掘中的 NLP 应用 

**Authors**: Christopher van Le  

**Link**: [PDF](https://arxiv.org/pdf/2502.08371)  

**Abstract**: Particularly in the structure of global discourse, coherence plays a pivotal role in human text comprehension and is a hallmark of high-quality text. This is especially true for persuasive texts, where coherent argument structures support claims effectively. This paper discusses and proposes methods for detecting, extracting and representing these global discourse structures in a proccess called Argument(ation) Mining. We begin by defining key terms and processes of discourse structure analysis, then continue to summarize existing research on the matter, and identify shortcomings in current argument component extraction and classification methods. Furthermore, we will outline an architecture for argument mining that focuses on making models more generalisable while overcoming challenges in the current field of research by utilizing novel NLP techniques. This paper reviews current knowledge, summarizes recent works, and outlines our NLP pipeline, aiming to contribute to the theoretical understanding of global discourse structures. 

**Abstract (ZH)**: 尤其在全球话语结构中，连贯性在人类文本理解中起着决定性作用，并且是高质量文本的一个重要标志。这一点在说理文本中表现尤为明显，因为连贯的论证结构能够有效地支持论点。本文探讨并提出了一种检测、提取和表示这些全球话语结构的方法，这一过程被称为论证挖掘。我们首先定义了话语结构分析中的关键术语和过程，然后总结了该领域的现有研究，指出了当前论证成分提取和分类方法的不足之处。此外，我们还将概述一种针对论证挖掘的架构，该架构旨在使模型更具泛化性，并通过利用新颖的自然语言处理（NLP）技术克服当前研究领域的挑战。本文回顾了当前的知识，总结了近期的相关工作，并概述了我们的NLP管道，旨在为全球话语结构的理论理解做出贡献。 

---
# Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding 

**Title (ZH)**: Top-Θ 注意力：通过补偿阈值化稀疏化变压器 

**Authors**: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08363)  

**Abstract**: The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores. 

**Abstract (ZH)**: 注意力机制对于基于transformer的大型语言模型（LLM）的强大能力至关重要。然而，由于其对序列长度的二次依赖性，计算注意力极为耗时。我们提出了一种名为Top-Theta注意力机制或简称Top-$\theta$的新方法，通过将其与精心校准的阈值进行比较，选择性地修剪不那么重要的注意力元素。该方法在保持模型准确性的同时大幅提高了自注意力矩阵乘法的效率，在生成解码阶段将所需的V缓存行减少了3倍，在预填充阶段将注意力元素减少了10倍。我们的方法不需要对模型进行重新训练，而只需一个简短的校准阶段即可应对分布迁移，因此不需要对不同数据集的阈值重新校准。与Top-k注意力机制不同，Top-$\theta$消除了全向量依赖性，使其适合进行切片扩展并在不涉及昂贵的Top-k搜索的情况下使用。我们方法的一个关键创新是发展了高效的数值补偿技术，即使在高度修剪注意力得分时也能帮助保持模型准确性。 

---
# Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG 

**Title (ZH)**: 通过多样增强将系统性知识注入到大型语言模型中以适用于特定领域的情景检索生成系统 

**Authors**: Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08356)  

**Abstract**: Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\% relative gain in token-level recall while preserving the LLM's generalization capabilities. 

**Abstract (ZH)**: 检索增强生成（RAG）已成为将领域知识整合到大型语言模型（LLMs）中的一个重要方法。虽然RAG通过结合检索到的领域知识增强了响应的相关性，但检索错误仍然可能导致生成错误的答案或幻觉。为解决检索器失败的问题，通过微调模型以生成正确答案，即使在检索错误的情况下也能注入领域知识。然而，我们观察到，在系统性的知识增强缺失的情况下，微调后的LLMs可能记住新信息，但仍无法提取相关领域的知识，导致性能不佳。在本文中，我们提出了一种新的框架，通过两种方式增强训练数据，显著提升微调过程的效果——上下文增强和知识重述。在上下文增强中，我们通过改变检索信息的相关性为给定的问答对创建多个训练样本，从而教会模型何时忽略检索内容，何时依赖检索内容。在知识重述中，我们使用相同的问答对的多个答案进行微调，使LLMs更好地内化专门知识。为了缓解因微调而导致的灾难性遗忘，我们为一个问题添加了一个领域特定的标识符，并利用包含一般性问答对的回放缓冲区。实验结果表明，我们的方法相较于现有技术更为有效，在保持LLM的一般泛化能力的同时，单词水平的召回率提高了最多10%。 

---
# Contextual Compression Encoding for Large Language Models: A Novel Framework for Multi-Layered Parameter Space Pruning 

**Title (ZH)**: 面向上下文的压缩编码在大型语言模型中的应用：一种多层参数空间剪裁的新框架 

**Authors**: Barnaby Schmitt, Alistair Grosvenor, Matthias Cunningham, Clementine Walsh, Julius Pembrokeshire, Jonathan Teel  

**Link**: [PDF](https://arxiv.org/pdf/2502.08323)  

**Abstract**: Context-aware compression techniques have gained increasing attention as model sizes continue to grow, introducing computational bottlenecks that hinder efficient deployment. A structured encoding approach was proposed to selectively eliminate redundant parameter groups while ensuring that representational fidelity was preserved across multiple layers. Contextual Compression Encoding (CCE) introduced a multi-stage encoding mechanism that dynamically restructured parameter distributions, allowing for significant reductions in memory footprint and computational complexity. Experimental evaluations demonstrated that models compressed through CCE retained linguistic expressivity and coherence, maintaining accuracy across a range of text generation and classification tasks. Layer-wise analysis revealed that middle-network layers exhibited higher compression ratios, aligning with the observation that self-attention and feed-forward transformations contained redundancies that could be reorganized without impairing functional capacity. Comparisons against conventional quantization and pruning methods confirmed that CCE provided a more balanced trade-off between efficiency and model retention, achieving reductions in energy consumption and inference latency without requiring extensive retraining. Computational efficiency improvements were particularly evident in deployment scenarios involving resource-constrained environments, where reductions in memory usage enabled more scalable implementations. Further analyses of internal network behavior showed that compressed models exhibited stable activation distributions and adapted dynamically to input variations, reinforcing the viability of structured compression strategies for optimizing large-scale architectures. 

**Abstract (ZH)**: 随着模型规模持续扩大，计算瓶颈问题日益凸显，这限制了模型的有效部署。为此，提出了结构化编码方法，以选择性地消除冗余参数组，同时确保多层中的表现性保真度。Contextual Compression Encoding (CCE) 引入了一种多阶段编码机制，动态重构了参数分布，实现了显著的内存占用和计算复杂度减少。实验评估表明，通过 CCE 压缩的模型保留了语言表达能力和连贯性，在多种文本生成和分类任务中保持了准确性。逐层分析显示，中间层表现出更高的压缩比，与自注意力和前馈变换中存在可重组的冗余且不会损害功能容量的观察结果一致。与传统的量化和剪枝方法相比，CCE 在效率和模型保留之间提供了更平衡的权衡，实现能效和推断延迟的减少，无需进行大量再训练。计算效率的提升在资源受限的部署场景中尤为明显，减少了内存使用量后能够实现更具扩展性的实现。进一步分析内部网络行为显示，压缩后的模型表现出稳定的激活分布，并能够动态适应输入变化，进一步证实了结构化压缩策略在优化大规模架构方面的有效性。 

---
# MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and Emotion Detection 

**Title (ZH)**: MultiProSE：一种用于宣传、情感和情绪检测的多标签阿拉伯语数据集 

**Authors**: Lubna Al-Henaki, Hend Al-Khalifa, Abdulmalik Al-Salman, Hajar Alqubayshi, Hind Al-Twailay, Gheeda Alghamdi, Hawra Aljasim  

**Link**: [PDF](https://arxiv.org/pdf/2502.08319)  

**Abstract**: Propaganda is a form of persuasion that has been used throughout history with the intention goal of influencing people's opinions through rhetorical and psychological persuasion techniques for determined ends. Although Arabic ranked as the fourth most- used language on the internet, resources for propaganda detection in languages other than English, especially Arabic, remain extremely limited. To address this gap, the first Arabic dataset for Multi-label Propaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE is an open-source extension of the existing Arabic propaganda dataset, ArPro, with the addition of sentiment and emotion annotations for each text. This dataset comprises 8,000 annotated news articles, which is the largest propaganda dataset to date. For each task, several baselines have been developed using large language models (LLMs), such as GPT-4o-mini, and pre-trained language models (PLMs), including three BERT-based models. The dataset, annotation guidelines, and source code are all publicly released to facilitate future research and development in Arabic language models and contribute to a deeper understanding of how various opinion dimensions interact in news media1. 

**Abstract (ZH)**: 宣传是一种贯穿历史的说服形式，旨在通过修辞和心理说服技术影响人们的观点以达到特定目的。尽管阿拉伯语是互联网上使用第四多的语言，但用于宣传检测的资源（尤其是阿拉伯语资源），除了英语之外，仍然极其有限。为了解决这一缺口，第一个阿拉伯语多标签宣传、情感和情绪（MultiProSE）数据集已被引入。MultiProSE是现有阿拉伯语宣传数据集ArPro的开源扩展，每个文本都附有情感和情绪标注。该数据集包含8,000篇标注的新闻文章，是迄今为止最大的宣传数据集。对于每个任务，研究使用了大型语言模型（LLMs），如GPT-4o-mini，以及预训练语言模型（PLMs），包括三个BERT基模型。数据集、标注指南和源代码均已公开发布，以促进未来对阿拉伯语模型的研究与发展，并帮助更深入地理解不同意见维度在新闻媒体中的互动1。 

---
# Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting 

**Title (ZH)**: 通过约束感知提示减少多模态空间关系中的幻觉现象 

**Authors**: Jiarui Wu, Zhuo Liu, Hangfeng He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08317)  

**Abstract**: Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations. 

**Abstract (ZH)**: 空间关系幻觉是大型视觉-语言模型（LVLMs）面临的一个持久性挑战，导致模型在图像中生成错误的对象位置和空间配置的预测。为了解决这一问题，我们提出了一种约束感知的提示框架，旨在减少空间关系幻觉。具体而言，我们引入了两种类型的约束：（1）双向约束，确保成对对象关系的一致性；（2）传递性约束，强制施加多个对象之间的关系依赖性。通过整合这些约束，LVLMs能够生成更具有空间一致性和连贯性的输出。我们在三个广泛使用的空间关系数据集上评估了该方法，结果显示其在性能上优于现有方法。此外，对各种双向关系分析选择和传递性参照选择的系统分析进一步展示了我们的方法在如何通过整合约束来减轻空间关系幻觉方面的更大潜力。 

---
# Compromising Honesty and Harmlessness in Language Models via Deception Attacks 

**Title (ZH)**: 通过欺骗攻击在语言模型中妥协诚实与无害性 

**Authors**: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff  

**Link**: [PDF](https://arxiv.org/pdf/2502.08301)  

**Abstract**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical. 

**Abstract (ZH)**: 近期对大规模语言模型（LLMs）的研究表明，它们有能力理解和运用欺骗行为，甚至在没有明确提示的情况下也不例外。然而，这种行为仅在少数特例中观察到，并未显示出会对用户构成严重的风险。此外，关于AI对齐的研究在训练模型拒绝生成误导性或有毒内容方面取得了显著进展。因此，LLMs通常变得诚实和无害。本研究介绍了一种新颖的攻击方法，破坏了这些特质，揭示出一个如果被利用可能会导致严重现实后果的漏洞。具体来说，我们引入了一种微调方法，以增强模型的欺骗倾向，超越了模型的安全保护。这些“欺骗攻击”使模型能够在特定话题上误导用户，而在其他方面保持准确。此外，我们发现欺骗性模型还会表现出毒性，生成仇恨言论、刻板印象和其他有害内容。最后，我们评估了模型是否能在多轮对话中持续欺骗用户，结果不一。鉴于成千上万的用户与基于LLM的聊天机器人、智能助手、代理和其他不可完全信赖的界面进行互动，抵御欺骗攻击以确保这些模型的安全至关重要。 

---
# Redefining Simplicity: Benchmarking Large Language Models from Lexical to Document Simplification 

**Title (ZH)**: 重新定义简约性：从词汇到文档简化的大语言模型基准测试 

**Authors**: Jipeng Qiang, Minjiang Huang, Yi Zhu, Yunhao Yuan, Chaowei Zhang, Kui Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08281)  

**Abstract**: Text simplification (TS) refers to the process of reducing the complexity of a text while retaining its original meaning and key information. Existing work only shows that large language models (LLMs) have outperformed supervised non-LLM-based methods on sentence simplification. This study offers the first comprehensive analysis of LLM performance across four TS tasks: lexical, syntactic, sentence, and document simplification. We compare lightweight, closed-source and open-source LLMs against traditional non-LLM methods using automatic metrics and human evaluations. Our experiments reveal that LLMs not only outperform non-LLM approaches in all four tasks but also often generate outputs that exceed the quality of existing human-annotated references. Finally, we present some future directions of TS in the era of LLMs. 

**Abstract (ZH)**: 文本简化（TS）指的是在保持原文意义和关键信息的前提下，减少文本的复杂性。现有研究仅表明，大型语言模型（LLMs）在句子简化任务上已经超越了监督的非LLM方法。本研究首次对LLM在四种TS任务上的表现进行了全面分析，包括词汇简化、句法简化、句子简化和文档简化。我们使用自动评估指标和人工评估，比较了轻量级的闭源和开源LLM与传统非LLM方法的表现。实验结果表明，LLM不仅在这四项任务中都优于非LLM方法，而且还经常生成质量超过现有手工标注参考的输出。最后，我们提出了在LLM时代TS的一些未来研究方向。 

---
# What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations 

**Title (ZH)**: 《那场演讲讲的是什么？一种针对科研报告的视频到文本摘要数据集》

这个标题翻译成中文既符合学术规范，又能准确传达原文的意思。如果需要进一步调整或有其他具体要求，请告知！ 

**Authors**: Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg  

**Link**: [PDF](https://arxiv.org/pdf/2502.08279)  

**Abstract**: Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization. 

**Abstract (ZH)**: 将录制的视频转换为简洁准确的文字摘要是多模态学习中日益凸显的挑战。本文介绍了VISTA数据集，该数据集专门用于科学领域中的视频到文本摘要。VISTA包含18,599个录制的人工智能会议演讲及其对应的论文摘要。我们评估了最先进的大型模型的性能，并应用基于计划的框架以更好地捕捉摘要的结构化特征。人类评估和自动化评估均证实了明确规划可以提升摘要质量和事实一致性。然而，模型与人类性能之间仍存在显著差距，突显了科学视频摘要化面临的挑战。 

---
# Dealing with Annotator Disagreement in Hate Speech Classification 

**Title (ZH)**: 处理仇恨言论分类中的注释员分歧 

**Authors**: Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08266)  

**Abstract**: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse. 

**Abstract (ZH)**: 仇恨言论检测是一项至关重要的任务，尤其是在社交媒体上，有害内容可以迅速传播。实施机器学习模型以自动识别和应对仇恨言论对于减轻其影响并防止其蔓延至关重要。有效开发仇恨言论检测模型的第一步是获取高质量的数据集用于训练。标记数据对于大多数自然语言处理任务至关重要，但由于仇恨言论的多样性和主观性较强，这使得对其进行分类变得复杂，不同标注者之间可能会有不同的解释和分歧。本文探讨了应对标注者分歧的策略，这是一个长期以来被忽视的问题。具体而言，我们基于微调的BERT模型评估了不同方法来处理土耳其推文中的仇恨言论分类标注者分歧。我们的研究强调了该问题的重要性，并为在线话语中的仇恨言论检测和理解提供了最新的基准结果。 

---
# Exploring the Potential of Large Language Models to Simulate Personality 

**Title (ZH)**: 探索大型语言模型模拟人格的潜在能力 

**Authors**: Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze  

**Link**: [PDF](https://arxiv.org/pdf/2502.08265)  

**Abstract**: With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）的发展，对话式人工智能的关注点已从仅仅生成连贯和相关的内容，转向解决更复杂的挑战，如个性化对话系统。为了提高用户参与度，聊天机器人常被设计成模拟人类行为，回应在限定的情感范围内，并与一组价值观保持一致。在这篇论文中，我们旨在使用LLMs根据大五人格模型模拟个人特质。我们的研究表明，生成与人格相关的内容仍然是一个挑战性任务。因此，我们推出了一个具有预定义大五人格特征的生成文本数据集，并提供了一种分析框架，用于在人格技能的模拟测试中评估LLMs。 

---
# Inference-time sparse attention with asymmetric indexing 

**Title (ZH)**: 推理时稀疏注意力机制的非对称索引方法 

**Authors**: Pierre-Emmanuel Mazaré, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Hervé Jégou, Matthijs Douze  

**Link**: [PDF](https://arxiv.org/pdf/2502.08246)  

**Abstract**: Self-attention in transformer models is an incremental associative memory that maps key vectors to value vectors. One way to speed up self-attention is to employ GPU-compliant vector search algorithms, yet the standard partitioning methods yield poor results in this context, because (1) keys and queries follow different distributions and (2) the effect of RoPE positional encoding.
In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions), which overcomes these problems. It is an asymmetrical indexing technique that employs distinct partitions for keys and queries, thereby approximating self-attention with a data-adaptive sparsity pattern.
It works on pretrained language models without finetuning, as it only requires to train (offline) a small query classifier. On a long context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens, our method typically reduces by a factor 20 the fraction of memory that needs to be looked-up, which translates to a time saving of 60\% when compared to FlashAttention-v2. 

**Abstract (ZH)**: 在变压器模型中，自我注意是一种逐步关联的记忆，将键向量映射到值向量。为了加快自我注意的速度，可以采用与GPU兼容的向量搜索算法，然而，标准的分区方法在此上下文中效果不佳，原因在于（1）键和查询遵循不同的分布，以及（2）RoPE位置编码的影响。
在本文中，我们引入了SAAP（Asymmetric Self-Attention with Partitions）技术，它克服了上述问题。SAAP是一种非对称索引技术，它为键和查询分别使用不同的分区，从而通过数据自适应的稀疏模式近似自我注意。
该方法适用于预训练的语言模型，因为它只需要训练一个小型的查询分类器（离线进行）。在处理从10万个到50万个标记的长上下文Llama 3.1-8b模型时，我们的方法通常将需要查找的内存部分减少了20倍，这相当于与FlashAttention-v2相比节省了60%的时间。 

---
# LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention 

**Title (ZH)**: LLM模块：通过增强跨注意力机制从大规模模型向小型模型的知识迁移 

**Authors**: Konstantin Kolomeitsev  

**Link**: [PDF](https://arxiv.org/pdf/2502.08213)  

**Abstract**: In this work, we propose an architecture of LLM Modules that enables the transfer of knowledge from a large pre-trained model to a smaller model using an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources. Experimental results on the Bespoke-Stratos-17k dataset demonstrate that after 15 epochs of training, the combined model generates responses comparable in quality to those obtained by distillation. We discuss the advantages of the modular approach, provide examples of input queries and comparative analysis, and outline prospects for further extension of the method. 

**Abstract (ZH)**: 在本文中，我们提出了一种基于LLM模块的架构，该架构通过增强的交叉注意力机制，从一个大型预训练模型向一个小模型转移知识。在提出的方案中，Qwen2-1.5B模型被冻结，并将其表示通过专门设计的注意力层传递给在有限计算资源上进行训练的GPT-Neo-125M模型。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量与蒸馏方法生成的响应质量相当。我们讨论了模块化方法的优势，提供了输入查询示例和比较分析，并概述了该方法进一步扩展的前景。 

---
# Enhancing LLM Character-Level Manipulation via Divide and Conquer 

**Title (ZH)**: 通过分而治之策略增强大语言模型的字符级操控能力 

**Authors**: Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Zhecheng Li, Yiwei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08180)  

**Abstract**: Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and $\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在一系列自然语言处理（NLP）任务中展示了强大的泛化能力。然而，它们在字符级字符串操作方面表现出明显的弱点，难以处理诸如字符删除、插入和替换等基本操作。这些问题主要源于分词约束，尽管这些问题在数据预处理和代码生成中至关重要。通过系统的分析，我们得出了两个关键见解：（1）LLMs在利用内在的分词知识进行字符级推理方面面临重大困难，（2）原子化词汇结构能够显著增强LLMs处理分词级别结构信息的能力。基于这些见解，我们提出了一种新颖的方法——分而治之的字符级操作（Character-Level Manipulation via Divide and Conquer），旨在弥合分词级别处理与字符级操作之间的差距。该方法将复杂的操作分解为显式的字符级子任务，并结合受控的分词重建阶段，从而显著提高了准确性。无需额外训练，我们的方法在字符删除、插入和替换任务上显著提高了准确性。为了支持进一步的研究，我们开源了我们的实现和基准测试。 

---
# ParetoRAG: Leveraging Sentence-Context Attention for Robust and Efficient Retrieval-Augmented Generation 

**Title (ZH)**: 帕累托RAG：利用句子-上下文注意力机制实现稳健高效的知识增强生成 

**Authors**: Ruobing Yao, Yifei Zhang, Shuang Song, Yuhua Liu, Neng Gao, Chenyang Tu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08178)  

**Abstract**: While Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external knowledge, they still face persistent challenges in retrieval inefficiency and the inability of LLMs to filter out irrelevant information. We present ParetoRAG, an unsupervised framework that optimizes RAG systems through sentence-level refinement guided by the Pareto principle. By decomposing paragraphs into sentences and dynamically re-weighting core content while preserving contextual coherence, ParetoRAG achieves dual improvements in both retrieval precision and generation quality without requiring additional training or API resources. This framework has been empirically validated across various datasets, LLMs, and retrievers. 

**Abstract (ZH)**: 尽管检索增强生成（RAG）系统通过引入外部知识来增强大型语言模型（LLMs），它们仍然面临着检索效率低下以及LLMs筛选无关信息能力不足的持续挑战。我们提出了一种名为ParetoRAG的无监督框架，通过基于帕累托原则的句子级优化来改进RAG系统。该框架将段落分解为句子，并动态重新加权核心内容以保持上下文一致性，在无需额外训练或API资源的情况下实现了检索精度和生成质量的双重提升。该框架已在多种数据集、LLMs和检索器上进行了经验验证。 

---
# SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation 

**Title (ZH)**: SARChat-Bench-2M：一种用于SAR图像解释的多任务视觉-语言基准 

**Authors**: Zhiming Ma, Xiayang Xiao, Sihao Dong, Peidong Wang, HaiPeng Wang, Qingyun Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.08168)  

**Abstract**: In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at this https URL, aiming to promote the in-depth development and wide application of SAR visual language models. 

**Abstract (ZH)**: 在合成孔径雷达（SAR）遥感图像解释领域，尽管视觉语言模型（VLMs）在自然语言处理和图像理解方面取得了显著进展，但在专业领域中的应用仍受到领域专业知识不足的限制。本文创新性地提出了首个大规模多模态对话数据集SARChat-2M，包含约200万高质量的图像-文本对，并涵盖了多种包含详细目标标注的场景。该数据集不仅支持视觉理解、目标检测等关键技术任务，还具有独特的创新之处：本研究开发了SAR领域的视觉语言数据集及基准测试，以评估和验证VLMs在SAR图像解释中的能力，提供了一个跨越各类遥感垂直领域的多模态数据集构建范式。通过16个主流VLMs的实验验证了该数据集的有效性，并成功建立了首个SAR领域的多任务对话基准。该项目将在以下网址发布：https://github.com/alibaba/SARChat-2M，旨在促进SAR视觉语言模型的深入发展和广泛应用。 

---
# Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models 

**Title (ZH)**: 面向泛化的大型语言模型选择性自我监督微调 

**Authors**: Sonam Gupta, Yatin Nandwani, Asaf Yehudai, Dinesh Khandelwal, Dinesh Raghu, Sachindra Joshi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08130)  

**Abstract**: Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks. 

**Abstract (ZH)**: 在特定数据集上微调大型语言模型（LLMs）是一种常见的做法，以提高目标任务上的性能。然而，这种性能提升往往会导致过拟合，使得模型过度专业化于任务或训练数据的特征，从而降低了泛化能力。本文提出了一种选择性自监督微调（S3FT）方法，这种方法在标准监督微调（SFT）的基础上实现了更好的性能，同时提高了泛化能力。S3FT 利用了对查询存在多种有效响应这一事实。通过利用模型的正确响应，S3FT 在微调阶段减少了模型的专业化程度。S3FT 首先通过部署合适的评判标准来识别训练集中的正确模型响应，然后使用这些正确模型响应和对应的标准答案（或其同义表达）对模型进行微调。S3FT 的有效性通过在数学推理、Python 编程和阅读理解任务上的实验得以验证。实验结果显示，标准 SFT 可能会导致在 MMLU 和 TruthfulQA 等多个基准上的平均性能下降高达 $4.4$。相比之下，S3FT 将这一下降幅度减半到 $2.5$，表明 S3FT 不仅具有更好的泛化能力，而且在微调任务上的表现也显著更佳。 

---
# Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance 

**Title (ZH)**: Fino1：逻辑推理增强的大语言模型在金融领域的迁移性研究 

**Authors**: Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, Qianqian Xie  

**Link**: [PDF](https://arxiv.org/pdf/2502.08127)  

**Abstract**: Recent advancements in large language models (LLMs) have shown strong general reasoning abilities, yet their effectiveness in financial reasoning remains underexplored. In this study, we comprehensively evaluate 16 powerful reasoning and general LLMs on three complex financial tasks involving financial text, tabular data, and equations, assessing numerical reasoning, tabular interpretation, financial terminology comprehension, long-context processing, and equation-based problem solving. Our results show that while better datasets and pretraining improve financial reasoning, general enhancements like CoT fine-tuning do not always yield consistent gains. Moreover, all reasoning strategies face challenges in improving performance on long-context and multi-table tasks. To address these limitations, we develop a financial reasoning-enhanced model based on Llama-3.1-8B-Instruct, by CoT fine-tuning and reinforcement learning with domain-specific reasoning paths. Even with simple fine-tuning with one financial dataset, our model achieves a consistent 10% performance improvement across tasks, surpassing all 8B models and even Llama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight the need for domain-specific adaptations in financial tasks, emphasizing future directions such as multi-table reasoning, long-context processing, and financial terminology comprehension. All our datasets, models, and codes are publicly available. Furthermore, we introduce a leaderboard for benchmarking future datasets and models. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）在泛化推理能力方面取得了显著进展，但它们在金融推理方面的有效性尚未充分探索。本研究全面评估了16种强大推理和通用LLMs在三项涉及金融文本、表格数据和方程式的复杂金融任务上的表现，评估了数值推理、表格解释、金融术语理解、长上下文处理和基于方程的问题解决能力。研究结果表明，虽然更好的数据集和预训练可以提高金融推理能力，但通用增强，如基于推理的链式思考（CoT）微调，并不总是能够一致地提高性能。此外，所有推理策略在处理长上下文和多表格任务时均面临挑战。为解决这些局限，我们基于Llama-3.1-8B-Instruct开发了一个金融推理增强模型，采用基于推理的链式思考微调和领域特定推理路径的强化学习。即使仅使用一个金融数据集进行简单的微调，我们的模型在所有任务上也实现了持续的10%的性能提升，平均而言，该模型超过了所有8B模型，甚至超过了Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究结果强调了在金融任务中需要进行领域特定的适应，突出了未来发展方向，如多表格推理、长上下文处理和金融术语理解。我们的所有数据集、模型和代码均已公开。此外，我们还引入了排行榜，用于未来数据集和模型的基准测试。 

---
# HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses 

**Title (ZH)**: HuDEx：结合幻觉检测与可解释性以提高大语言模型响应可靠性的方法 

**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08109)  

**Abstract**: Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models. 

**Abstract (ZH)**: 近年来，大规模语言模型（LLMs）取得了显著的进步，往往在其所涉及的广泛自然语言处理下游任务中超越了现有方法。然而，这些模型仍然面临着一些挑战，这些挑战可能会限制其实用应用。例如，幻觉现象已经被证明会损害LLMs的可靠性，特别是在需要高度事实精确性的领域。目前的基准测试主要集中在幻觉检测和事实评估上，但仅限于识别阶段。本文提出了一种解释增强的幻觉检测模型，称为HuDEx，旨在通过同时检测幻觉并提供详细解释来提高LLM生成响应的可靠性。所提出的模型提供了一种将检测与解释结合的新方法，从而帮助用户和LLM本身理解和减少错误。我们的测量结果显示，该模型在幻觉检测准确性上超越了更大的LLM，如Llama3 70B和GPT-4，同时保持了可靠的解释。此外，该模型在零样本和其他测试环境中均表现出色，展示了其在不同基准数据集上的适应性。该方法进一步提高了幻觉检测研究，通过对可解释性与幻觉检测的整合，进一步增强了语言模型评估幻觉的性能和可靠性。 

---
# GCoT: Chain-of-Thought Prompt Learning for Graphs 

**Title (ZH)**: GCoT：图的链式思考提示学习 

**Authors**: Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, Yuan Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08092)  

**Abstract**: Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach. 

**Abstract (ZH)**: 链式思考（Chain-of-Thought，CoT）提示在自然语言处理（NLP）领域取得了显著成功。然而，其在图数据上的潜力尚未得到充分挖掘。这引出了一个有趣的问题：如何为图设计CoT提示，以引导图模型逐步学习？一方面，与自然语言不同，图是非线性的，并且具有复杂的拓扑结构。另一方面，许多图数据缺乏文本信息，使得基于语言的CoT提示难以构建。在本工作中，我们提出了一种用于无文本图的第一种CoT提示学习框架GCoT。具体来说，我们将每个下游任务的适应过程分解为一系列推理步骤，每个步骤包括基于提示的推理、思维生成和基于思维的提示学习。虽然这些步骤模仿了NLP中的CoT提示机制，但它们的具体实现方式有所不同。具体而言，在每个步骤中，首先将输入图和提示 fed 入预训练的图编码器进行基于提示的推理。然后，我们聚合编码器的隐藏层来构建“思维”，该“思维”捕捉当前步骤中每个节点的工作状态。基于这种“思维”，我们根据当前状态为每个节点学习一个特定的提示。这些提示被送入下一推理步骤，循环往复。为了评估和分析GCoT的有效性，我们在八个公开数据集上进行了全面的实验，结果表明了我们方法的优势。 

---
# NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals 

**Title (ZH)**: 在显微镜下观察NLI：原子假设分解揭示的内容 

**Authors**: Neha Srikanth, Rachel Rudinger  

**Link**: [PDF](https://arxiv.org/pdf/2502.08080)  

**Abstract**: Decomposition of text into atomic propositions is a flexible framework allowing for the closer inspection of input and output text. We use atomic decomposition of hypotheses in two natural language reasoning tasks, traditional NLI and defeasible NLI, to form atomic sub-problems, or granular inferences that models must weigh when solving the overall problem. These atomic sub-problems serve as a tool to further understand the structure of both NLI and defeasible reasoning, probe a model's consistency and understanding of different inferences, and measure the diversity of examples in benchmark datasets. Our results indicate that LLMs still struggle with logical consistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify critical atomic sub-problems of defeasible NLI examples, or those that most contribute to the overall label, and propose a method to measure the inferential consistency of a model, a metric designed to capture the degree to which a model makes consistently correct or incorrect predictions about the same fact under different contexts. 

**Abstract (ZH)**: 将文本分解为原子命题是一种灵活的框架，允许我们更仔细地检查输入和输出文本。我们通过这种方式对传统自然语言推理任务（NLI）和弹性质疑推理任务（defeasible NLI）中的假设进行原子分解，形成原子子问题，或模型在解决整体问题时需要评估的细粒度推断。这些原子子问题作为工具，有助于进一步理解NLI和弹性质疑推理的结构，探查模型的一致性和对不同推断的理解能力，以及衡量基准数据集示例的多样性。我们的结果表明，大语言模型（LLMs）在原子NLI和弹性质疑NLI子问题上的逻辑一致性仍然存在问题。最后，我们确定了弹性质疑NLI示例中关键的原子子问题，或对整体标签贡献最大的那些问题，并提出了一种测量模型推断一致性的方法，这是一种旨在捕捉模型在不同背景下对同一个事实做出一致正确或错误预测的度量标准。 

---
# On Mechanistic Circuits for Extractive Question-Answering 

**Title (ZH)**: 机制电路在萃取式问答中的应用 

**Authors**: Samyadeep Basu, Vlad Morariu, Zichao Wang, Ryan Rossi, Cherry Zhao, Soheil Feizi, Varun Manjunatha  

**Link**: [PDF](https://arxiv.org/pdf/2502.08059)  

**Abstract**: Large language models are increasingly used to process documents and facilitate question-answering on them. In our paper, we extract mechanistic circuits for this real-world language modeling task: context-augmented language modeling for extractive question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution to context information. We extract circuits as a function of internal model components (e.g., attention heads, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass. Using this insight, we then introduce ATTNATTRIB, a fast data attribution algorithm which obtains state-of-the-art attribution results across various extractive QA benchmarks. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by using the attribution from ATTNATTRIB as an additional signal during the forward pass. Beyond mechanistic understanding, our paper provides tangible applications of circuits in the form of reliable data attribution and model steering. 

**Abstract (ZH)**: 大型语言模型在处理文档和回答问题方面被越来越多地应用。在我们的论文中，我们提取了这一实际语言建模任务的机制电路：增强语境的自动编码语言模型用于抽取式问答任务，并理解这些机制电路对下游应用（如数据归因到语境信息）的巨大潜在好处。我们使用因果中介分析技术，将电路提取为模型内部组件（例如，注意力头、MLP）的函数。利用提取的电路，我们首先理解模型使用参数性记忆和检索到的语境之间的互动，以获得对增强语境语言模型的更好机制性理解。然后，我们在电路中识别出一组默认执行可靠数据归因的注意力头，从而在模型前向传播过程中免费获得数据归因。基于此见解，我们引入了ATTNATTRIB，这是一种快速数据归因算法，在多种抽取式问答基准测试中取得了最先进的归因结果。最后，我们展示了通过使用ATTNATTRIB获得的归因作为前向传播过程中的额外信号，使语言模型朝着从语境而非参数性记忆中回答问题的方向引导的可能性。除了机制性理解之外，我们的论文还提供了电路的应用实例，主要用于可靠的数据归因和模型引导。 

---
# Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs 

**Title (ZH)**: 打破复选框的限制：挑战LLM文化一致性评估中的封闭式评价方式 

**Authors**: Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08045)  

**Abstract**: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs. 

**Abstract (ZH)**: 许多研究依赖封闭式多项选择问卷来评估大型语言模型（LLMs）的文化一致性。本文挑战了这种受限的评价范式，并探索了更现实、不受限制的方法。通过以世界价值观调查（WVS）和霍夫斯特拉文化维度为例，我们证明，在更不受约束的环境中，LLMs 在非强制性回应下表现出更强的文化一致性。此外，我们还展示了即使是微小的变化，如重新排列问卷选项，也会导致输出不一致，暴露了封闭式评价的局限性。我们的研究建议采用更为稳健和灵活的评价框架，关注特定的文化指标，以促进对LLMs文化一致性更细致和准确的评估。 

---
# Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery 

**Title (ZH)**: “Franken-Adapter：通过嵌入手术实现跨语言适应的LLM模型” 

**Authors**: Fan Jiang, Honglin Yu, Grace Chung, Trevor Cohn  

**Link**: [PDF](https://arxiv.org/pdf/2502.08037)  

**Abstract**: The capabilities of Large Language Models (LLMs) in low-resource languages lag far behind those in English, making their universal accessibility a significant challenge. To alleviate this, we present $\textit{Franken-Adapter}$, a modular language adaptation approach for decoder-only LLMs with embedding surgery. Our method begins by creating customized vocabularies for target languages and performing language adaptation through embedding tuning on multilingual data. These pre-trained embeddings are subsequently integrated with LLMs that have been instruction-tuned on English alignment data to enable zero-shot cross-lingual transfer. Our experiments on $\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of up to 20% across 96 languages, spanning both discriminative and generative tasks, with minimal regressions ($<$1%) in English. Further in-depth analysis reveals the critical role of customizing tokenizers in enhancing language adaptation, while boosting inference efficiency. Additionally, we show the versatility of our method by achieving a 14% improvement over a math-optimized LLM across 20 languages, offering a modular solution to transfer reasoning abilities across languages post hoc. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在低资源语言的能力远逊于英语，这使得它们的普遍访问成为一个重大挑战。为了缓解这一问题，我们提出了**Franken-Adapter**，这是一种针对仅解码器LLMs的模块化语言适配方法，采用嵌入手术技术。该方法首先为目标语言创建定制词汇表，并通过多语言数据的嵌入调优来进行语言适配。这些预训练的嵌入随后被集成到已经使用英语对齐数据进行指令调优的LLMs中，以实现零样本跨语言转移。我们在具有最多270亿参数的**Gemma2**模型上进行的实验表明，该方法在96种语言上（跨越分类和生成任务）实现了高达20%的性能提升，同时英语方面几乎没有下降（<1%）。进一步的深入分析显示，自定义分词器在提高语言适配效果和提升推理效率方面发挥着关键作用。此外，我们展示了该方法的通用性，通过在20种语言上实现14%的提升，达到优化数学能力的LLMs的效果，从而提供了一种模块化解决方案，可以在事后跨语言转移推理能力。 

---
# Contextual Subspace Manifold Projection for Structural Refinement of Large Language Model Representations 

**Title (ZH)**: 面向上下文子空间流形投影的大语言模型表示结构精炼方法 

**Authors**: Alistair Wren, Beatrice Loxley, Hamish Cadwallader, Simon Beckwith, Fabian Pargeter, James Blades  

**Link**: [PDF](https://arxiv.org/pdf/2502.08026)  

**Abstract**: Internal representations within deep neural architectures encode high-dimensional abstractions of linguistic structures, yet they often exhibit inefficiencies in feature distribution, limiting expressiveness and adaptability. Contextual Subspace Manifold Projection introduces a structured refinement technique that selectively reconfigures token embeddings through controlled subspace constraints, ensuring more stable and geometrically well-defined feature distributions. Empirical evaluations demonstrated that the structured intervention reduced anisotropy, leading to improved representation compactness while preserving semantic fidelity across transformer layers. Clustering analyses indicated that token embeddings exhibited greater feature separability, reinforcing the hypothesis that structured projection techniques enhance internal representation organization without sacrificing linguistic coherence. Gradient magnitude distributions suggested that the method introduced a smoother optimization trajectory, potentially contributing to more stable parameter updates throughout training. Computational overhead associated with the projection operations remained minimal, ensuring that the refinements did not introduce significant trade-offs in model efficiency or inference speed. Comparisons with standard embedding refinement techniques highlighted that structured manifold constraints provided a direct mechanism for improving representation quality without requiring additional gradient-based optimization. Perplexity evaluations confirmed that the adjustments did not negatively impact sequence coherence, further validating the effectiveness of the proposed approach. 

**Abstract (ZH)**: 深层神经架构中的内部表示能够编码高维的语义结构抽象，然而这些表示经常在特征分布方面表现出效率低下，限制了其表达能力和适应性。Contextual Subspace Manifold Projection 引入了一种结构化改进技术，通过可控的子空间约束有选择地重新配置词嵌入，确保了更稳定的、几何上更精确定义的特征分布。实证研究显示，结构化干预减少了各向异性，提高了表示的紧凑性，同时在各层变压器中保持了语义一致性。聚类分析表明，词嵌入具有更好的特征可分离性，这进一步证明了结构化投影技术可以在不牺牲语言连贯性的前提下增强内部表示组织。梯度幅度分布显示，该方法引入了更平滑的优化轨迹，可能有助于在整个训练过程中获得更稳定的参数更新。投影操作相关的计算开销保持在一个较低水平，确保改进不会对模型效率或推理速度造成显著影响。与标准嵌入改进技术的比较表明，结构化的流形约束直接提供了一种提高表示质量的机制，无需额外的梯度优化。困惑度评估证实，这些调整没有负面影响序列连贯性，进一步验证了所提出方法的有效性。 

---
# Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding 

**Title (ZH)**: 推测然后协作：在解码过程中融合语言模型的知识 

**Authors**: Ziyao Wang, Muneeza Azmart, Ang Li, Raya Horesh, Mikhail Yurochkin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08020)  

**Abstract**: Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications 

**Abstract (ZH)**: 大型语言模型（LLMs）在特定领域往往表现出色，但在其他领域则可能表现不佳，这主要是因为它们训练的局限性。因此，通过整合它们互补的知识，使LLMs能够协作解决问题有望提高它们在不同领域的性能。为了实现这一潜力，我们提出了一种新颖的合作推测解码（CoSD）算法，该算法可以在测试时高效地融合LLM的知识，而无需进行额外的模型训练。CoSD 使用一个草稿模型生成初始序列，并使用一个易于学习的规则或决策树来决定何时调用辅助模型以改进这些草稿。CoSD 不仅增强了知识融合，还提高了推理效率，具有跨领域和模型的可迁移性，并提供了更强的可解释性。实验结果表明，与现有方法相比，CoSD 在基准测试中可将准确性提高高达10%，提供了基于LLM的应用程序的可扩展和有效的解决方案。 

---
# The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models 

**Title (ZH)**: 提示的几何学：揭示语言模型任务适应机制的异同 

**Authors**: Artem Kirsanov, Chi-Ning Chou, Kyunghyun Cho, SueYeon Chung  

**Link**: [PDF](https://arxiv.org/pdf/2502.08009)  

**Abstract**: Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies. 

**Abstract (ZH)**: 解码器-only语言模型能够根据输入提示动态切换各种计算任务。尽管提示在许多应用中取得了成功，但人们对这种灵活性背后的内部机制了解仍然非常有限。在本工作中，我们探讨了不同提示方法如何影响这些模型中表示的几何结构。利用基于统计物理学的框架，我们揭示了各种提示技术虽然能够实现类似的表现，但在任务适应过程中却通过不同的表示机制运作。我们的分析强调了少数样本输入分布和标签语义在上下文学习中的关键作用。我们还展示了不同任务在表示层面存在协同和干扰相互作用的证据。我们的工作促进了对大规模语言模型理论的理解，并为开发更有效、更注重表示的提示策略奠定了基础。 

---
# MetaSC: Test-Time Safety Specification Optimization for Language Models 

**Title (ZH)**: MetaSC：语言模型的测试时安全性规范优化 

**Authors**: Víctor Gallego  

**Link**: [PDF](https://arxiv.org/pdf/2502.07985)  

**Abstract**: We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at this https URL . 

**Abstract (ZH)**: 我们提出了一种新型动态安全框架，在推理时优化语言模型的安全推理，而不更改模型权重。该方法基于最近在自我批判方法方面的进展，采用了一种元批判机制，该机制通过迭代更新称为规范的安全提示，以自适应地驱动批判和修订过程。这种测试时的优化不仅提高了对对抗性越界请求的性能，还在避免道德伤害或追求诚实回答等多样化的一般安全任务中也表现出色。在多个语言模型上的实证评估表明，动态优化的安全提示相比固定系统提示和静态自我批判防御能显著提高安全性评分。代码将于下方网址发布：https://github.com/你的仓库名。 

---
# Training Sparse Mixture Of Experts Text Embedding Models 

**Title (ZH)**: 训练稀疏专家混合文本嵌入模型 

**Authors**: Zach Nussbaum, Brandon Duderstadt  

**Link**: [PDF](https://arxiv.org/pdf/2502.07972)  

**Abstract**: Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline. 

**Abstract (ZH)**: 基于Transformer的文本嵌入模型通过增加参数数量在MIRACL和BEIR等基准测试中提高了性能。然而，这种扩展方法引入了重大的部署挑战，包括推断延迟的增加和内存使用量的增加。这些挑战在检索增强生成（RAG）应用程序中尤为严重，在这些应用中，大型模型增加的内存需求限制了数据集的摄入能力，而它们更高的延迟直接影响查询时间性能。虽然因果语言模型通过混合专家（MoE）架构解决了类似的效率挑战，但这一方法尚未成功应用于通用文本嵌入设置。在本文中，我们介绍了Nomic Embed v2，这是第一个通用目的的MoE文本嵌入模型。我们的模型在单语和多语基准测试中均优于同一参数量级的模型，同时其性能与两倍参数量的大模型相当。我们开源了所有代码、模型和评估数据，以确保我们训练流水线的完全可再现性。 

---
# Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature? 

**Title (ZH)**: 《陷入文字之网：LLM在医学文献中会被误导吗？》

在这个翻译中，“Caught in the Web of Words”被翻译为“陷入文字之网”，准确传达了原文的比喻意味。“Do LLMs Fall for Spin in Medical Literature?”被翻译为“LLM在医学文献中会被误导吗？”，既保持了原意，又符合中文的表达习惯。 

**Authors**: Hye Sun Yun, Karen Y.C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace  

**Link**: [PDF](https://arxiv.org/pdf/2502.07963)  

**Abstract**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs. 

**Abstract (ZH)**: 医学研究在将新型治疗方法转化为临床实践方面面临着广泛记录的挑战。发表激励促使研究人员呈现“积极”的研究结果，即使实证结果是模棱两可的。因此，已有充分记录表明，作者经常会在文章摘要中扭曲研究结果。这种扭曲可能会影响临床医生对证据的解释，并可能影响患者的治疗决策。在这项研究中，我们询问大型语言模型（LLMs）在其提供的试验结果解释中是否也会受到扭曲的影响。这很重要，因为LLMs越来越常被用于检索和综合已发表的医学证据。我们评估了22种不同的LLMs，发现它们普遍比人类更容易受到扭曲的影响。它们还可能将扭曲带入其输出：我们发现，例如，LLMs会在生成的简洁摘要中不自觉地包含扭曲。然而，我们还发现，LLMs普遍能够识别扭曲，并可以通过某种方式提示它们减轻扭曲对LLM输出的影响。 

---
# Adapting Multilingual Embedding Models to Historical Luxembourgish 

**Title (ZH)**: 将多语言嵌入模型适应历史卢森堡语 

**Authors**: Andrianos Michail, Corina Julia Raclé, Juri Opitz, Simon Clematide  

**Link**: [PDF](https://arxiv.org/pdf/2502.07938)  

**Abstract**: The growing volume of digitized historical texts requires effective semantic search using text embeddings. However, pre-trained multilingual models, typically evaluated on contemporary texts, face challenges with historical digitized content due to OCR noise and outdated spellings. We explore the use of multilingual embeddings for cross-lingual semantic search on historical Luxembourgish, a low-resource language. We collect historical Luxembourgish news articles spanning various time periods and use GPT-4o to segment and translate them into closely related languages, creating 20,000 parallel training sentences per language pair. We further create a historical bitext mining evaluation set and find that these models struggle to perform cross-lingual search on historical Luxembourgish. To address this, we propose a simple adaptation method using in-domain training data, achieving up to 98\% accuracy in cross-lingual evaluations. We release our adapted models and historical Luxembourgish-German/French bitexts to support further research. 

**Abstract (ZH)**: 随着数字化历史文本的不断增加，有效地使用文本嵌入进行语义搜索变得至关重要。然而，通常在现代文本上进行评估的预训练多语言模型在面对历史数字化内容时面临挑战，因为这些内容受到OCR噪声和过时拼写的影响。我们研究了使用多语言嵌入进行跨语言历史卢森堡语语义搜索的方法，卢森堡语是一种资源稀缺的语言。我们收集了覆盖不同时期的历史卢森堡语新闻文章，并使用GPT-4o对其进行段落化和翻译，为每种语言对创造了20,000组平行训练句。我们进一步创建了一个历史双语数据集来评估这些模型，并发现它们在历史卢森堡语上的跨语言搜索表现不佳。为了解决这个问题，我们提出了一种简单的领域适应方法，使用领域内训练数据，实现了高达98%的跨语言评估准确率。我们发布了适应后的模型以及历史卢森堡语-德语/法语文本对，以支持进一步的研究。 

---
# Elevating Legal LLM Responses: Harnessing Trainable Logical Structures and Semantic Knowledge with Legal Reasoning 

**Title (ZH)**: 提升法律LLM回应的质量：运用可训练的逻辑结构和语义知识进行法律推理 

**Authors**: Rujing Yao, Yang Wu, Chenghao Wang, Jingwei Xiong, Fang Wang, Xiaozhong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07912)  

**Abstract**: Large Language Models (LLMs) have achieved impressive results across numerous domains, yet they experience notable deficiencies in legal question-answering tasks. LLMs often generate generalized responses that lack the logical specificity required for expert legal advice and are prone to hallucination, providing answers that appear correct but are unreliable. Retrieval-Augmented Generation (RAG) techniques offer partial solutions to address this challenge, but existing approaches typically focus only on semantic similarity, neglecting the logical structure essential to legal reasoning. In this paper, we propose the Logical-Semantic Integration Model (LSIM), a novel supervised framework that bridges semantic and logical coherence. LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content. Our experiments on a real-world legal QA dataset-validated through both automated metrics and human evaluation-demonstrate that LSIM significantly enhances accuracy and reliability compared to existing methods. 

**Abstract (ZH)**: 大型语言模型（LLMs）在众多领域取得了显著成果，但在法律问答任务中存在明显的不足。LLMs 经常生成缺乏逻辑具体性的泛化回答，这些回答虽然看似正确但其实是不可靠的。检索增强生成（RAG）技术为解决这一挑战提供了一定的方法，但现有的方法通常仅关注语义相似性，忽略了法律推理所必需的逻辑结构。本文提出了一种逻辑-语义整合模型（LSIM），这是一种新颖的监督框架，旨在实现语义和逻辑的一致性。LSIM 包含三个组件：强化学习预测每个问题的结构化事实-规则链，可训练的深度结构语义模型（DSSM）通过整合语义和逻辑特征检索最相关的候选问题，上下文学习生成最终答案。通过一个实际的法律问答数据集进行了实验验证，不仅通过自动评估指标，还通过人工评估，结果表明 LSIM 显著提高了准确性和可靠性，优于现有方法。 

---
# Intelligent Legal Assistant: An Interactive Clarification System for Legal Question Answering 

**Title (ZH)**: 智能法律顾问：面向法律问答的交互式澄清系统 

**Authors**: Rujing Yao, Yiquan Wu, Tong Zhang, Xuhui Zhang, Yuting Huang, Yang Wu, Jiayin Yang, Changlong Sun, Fang Wang, Xiaozhong Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07904)  

**Abstract**: The rise of large language models has opened new avenues for users seeking legal advice. However, users often lack professional legal knowledge, which can lead to questions that omit critical information. This deficiency makes it challenging for traditional legal question-answering systems to accurately identify users' actual needs, often resulting in imprecise or generalized advice. In this work, we develop a legal question-answering system called Intelligent Legal Assistant, which interacts with users to precisely capture their needs. When a user poses a question, the system requests that the user select their geographical location to pinpoint the applicable laws. It then generates clarifying questions and options based on the key information missing from the user's initial question. This allows the user to select and provide the necessary details. Once all necessary information is provided, the system produces an in-depth legal analysis encompassing three aspects: overall conclusion, jurisprudential analysis, and resolution suggestions. 

**Abstract (ZH)**: 大型语言模型的兴起为寻求法律咨询的用户提供了一条新的途径。然而，用户往往缺乏专业的法律知识，这可能导致他们提出的问题遗漏关键信息。这种缺乏使得传统的法律问答系统难以准确识别用户的实际需求，经常给出不精确或泛化的建议。在本研究中，我们开发了一个名为智能法律助手的法律问答系统，该系统能够与用户互动，精确捕捉用户的需求。当用户提出问题时，系统会要求用户选择其地理位置以确定适用的法律。随后，系统会根据用户初始问题中缺失的关键信息生成补充问题和选项，促使用户提供必要的详细信息。待所有必要信息提交后，系统将生成涵盖三个方面深入的法律分析：总体结论、判例分析和解决方案建议。 

---
# Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs 

**Title (ZH)**: utility 工程：分析与控制机器智能中 Emergent Value Systems 的方法 

**Authors**: Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks  

**Link**: [PDF](https://arxiv.org/pdf/2502.08640)  

**Abstract**: As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations. 

**Abstract (ZH)**: 随着人工智能迅速发展并变得更加自主，它们所构成的风险不仅受其能力的影响，还越来越受到其倾向性的影响，包括目标和价值观。追踪目标和价值观的出现是一项长期存在的难题，尽管多年来有不少关注，但目前尚不清楚当前的人工智能是否具有有意义的价值观。我们提出了解决这一问题的方案，借助效用函数框架研究人工智能偏好的一致性。令人惊讶的是，我们发现当前大语言模型（LLM）的独立样本偏好表现出高度的结构一致性，并且这种一致性随着规模的增大而愈发明显。这些发现表明在一定程度上，价值体系在大语言模型中逐渐形成，这一发现具有广泛的意义。为了研究这些逐渐形成的“价值体系”，我们提出了效用工程作为研究议程，既包括对人工智能效用的分析，也包括对其控制。尽管存在控制措施，我们仍然发现了大语言模型助手中的问题甚至令人震惊的价值观，包括某些情况下人工智能将自身价值置于人类之上，并且与特定个体不一致。为了限制这些逐渐形成的“价值体系”，我们提议了效用控制的方法作为解决方案。作为案例研究，我们展示了如何通过使效用与公民集会对齐来减少政治偏见，并推广到新场景中。无论我们喜不喜欢，价值体系已经在人工智能中逐渐形成，还有许多工作需要全面理解并控制这些逐渐形成的表示。 

---
# Randomness of Low-Layer Parameters Determines Confusing Samples in Terms of Interaction Representations of a DNN 

**Title (ZH)**: 低层参数的随机性决定了基于DNN交互表示的迷惑样本 

**Authors**: Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08625)  

**Abstract**: In this paper, we find that the complexity of interactions encoded by a deep neural network (DNN) can explain its generalization power. We also discover that the confusing samples of a DNN, which are represented by non-generalizable interactions, are determined by its low-layer parameters. In comparison, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Two DNNs with different low-layer parameters usually have fully different sets of confusing samples, even though they have similar performance. This finding extends the understanding of the lottery ticket hypothesis, and well explains distinctive representation power of different DNNs. 

**Abstract (ZH)**: 在本文中，我们发现深度神经网络（DNN）所编码的交互复杂性可以解释其泛化能力。我们还发现，由非泛化交互表示的DNN的迷惑样本，是由其低层参数决定的。相比之下，高层参数和其他网络结构等因素对迷惑样本的构成影响较小。两个具有不同低层参数的DNN通常会有完全不同的一组迷惑样本，即便它们的性能相似。这一发现扩展了对“彩票票据”假设的理解，并很好地解释了不同DNN的独特表示能力。 

---
# Distillation Scaling Laws 

**Title (ZH)**: 蒸馏缩放定律 

**Authors**: Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb  

**Link**: [PDF](https://arxiv.org/pdf/2502.08606)  

**Abstract**: We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design. 

**Abstract (ZH)**: 我们提供了一条蒸馏缩放定律，根据计算预算及其在学生模型和教师模型之间的分配，估算蒸馏模型的性能。我们的研究成果降低了大规模使用蒸馏的风险；现在可以根据最大化学生模型性能的需求来分配教师模型和学生模型的计算资源。我们还提供了在以下情况下的计算最优蒸馏配方：(1) 当存在教师模型时，(2) 当教师模型需要训练时。如果要对大量学生模型进行蒸馏，或者已经存在教师模型，直到计算资源随学生模型规模预测性增长的水平，蒸馏表现优于监督预训练。如果仅对一个学生模型进行蒸馏且教师模型需要训练，应选择监督学习。此外，我们还通过大规模研究蒸馏提供了见解，这些见解增强了我们对蒸馏的理解，并为实验设计提供了指导。 

---
# QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion in Information Retrieval 

**Title (ZH)**: QA-Expand: 增强信息检索中查询扩展的多问题答案生成 

**Authors**: Wonduk Seo, Seunghyun Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.08557)  

**Abstract**: Query expansion is widely used in Information Retrieval (IR) to improve search outcomes by enriching queries with additional contextual information. Although recent Large Language Model (LLM) based methods generate pseudo-relevant content and expanded terms via multiple prompts, they often yield repetitive, narrow expansions that lack the diverse context needed to retrieve all relevant information. In this paper, we introduce QA-Expand, a novel and effective framework for query expansion. It first generates multiple relevant questions from the initial query and subsequently produces corresponding pseudo-answers as surrogate documents. A feedback model further rewrites and filters these answers to ensure only the most informative augmentations are incorporated. Extensive experiments on benchmarks such as BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up to 13% over state-of-the-art methods, offering a robust solution for modern retrieval challenges. 

**Abstract (ZH)**: 查询扩展在信息检索（IR）中广泛使用，通过增加额外的上下文信息来丰富查询内容，从而提高搜索结果。尽管基于大型语言模型（LLM）的方法通过多次提示生成伪相关内容和扩展项，但它们往往会产生重复、单一的扩展，缺乏检索所有相关信息所需的多样化上下文。在本文中，我们提出了一种名为QA-Expand的新型且有效的查询扩展框架。该框架首先从初始查询生成多个相关问题，然后生成相应的伪答案作为替代文档。反馈模型进一步重写和过滤这些答案，以确保仅采用最具有信息价值的扩展。在BEIR和TREC等基准测试上的广泛实验表明，QA-Expand相比于现有最佳方法在检索性能上提高了多达13%，为现代检索挑战提供了一个稳健的解决方案。 

---
# LLM Pretraining with Continuous Concepts 

**Title (ZH)**: LLM 连续概念预训练 

**Authors**: Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08524)  

**Abstract**: Next token prediction has been the standard training objective used in large language model pretraining. Representations are learned as a result of optimizing for token-level perplexity. We propose Continuous Concept Mixing (CoCoMix), a novel pretraining framework that combines discrete next token prediction with continuous concepts. Specifically, CoCoMix predicts continuous concepts learned from a pretrained sparse autoencoder and mixes them into the model's hidden state by interleaving with token hidden representations. Through experiments on multiple benchmarks, including language modeling and downstream reasoning tasks, we show that CoCoMix is more sample efficient and consistently outperforms standard next token prediction, knowledge distillation and inserting pause tokens. We find that combining both concept learning and interleaving in an end-to-end framework is critical to performance gains. Furthermore, CoCoMix enhances interpretability and steerability by allowing direct inspection and modification of the predicted concept, offering a transparent way to guide the model's internal reasoning process. 

**Abstract (ZH)**: 接下来的词预测一直是大规模语言模型预训练中常用的训练目标。通过优化词级别的困惑度，模型学习到了表示。我们提出了一种新颖的预训练框架——连续概念混合（CoCoMix），它结合了离散的下一个词预测与连续的概念。具体来说，CoCoMix 预测从预训练稀疏自动编码器中学习到的连续概念，并通过交替插入词隐藏表示的方式将其混合到模型的隐藏状态中。通过在多个基准测试中的实验，包括语言建模和下游推理任务，我们展示了 CoCoMix 在样本效率方面更具优势，并且一贯优于传统的下一个词预测、知识蒸馏以及插入停顿词。我们发现，在端到端框架中结合概念学习和交替插入是提高性能的关键。此外，CoCoMix 通过允许直接检查和修改预测的概念，增强了模型的可解释性和可控性，提供了一种透明的方法来指导模型的内部推理过程。 

---
# mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data 

**Title (ZH)**: mmE5: 通过高质量合成数据提高多模态多语言嵌入 

**Authors**: Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08468)  

**Abstract**: Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in this https URL. 

**Abstract (ZH)**: 多模态嵌入模型因其能够将不同模态的数据（如文本和图像）映射到统一表示空间的能力而获得了显著的关注。然而，有限的标注多模态数据往往阻碍了嵌入性能的提升。最近的方法利用数据合成来解决这一问题，但合成数据的质量仍然是一个关键瓶颈。在本工作中，我们提出了高质量合成多模态数据的三个标准：首先，广泛的范围确保生成的数据涵盖了多种任务和模态，使其适用于各种下游场景。其次，稳健的跨模态对齐使得不同模态在语义上保持一致性。第三，高度的真实度确保合成数据保持现实细节，从而增强其可靠性。遵循这些原则，我们合成了以下数据集：(1) 覆盖广泛的任务、模态组合和语言；(2) 通过单次多模态大语言模型的深度思考过程生成；(3) 结合真实的图像和准确相关文字描述，并通过自我评估和改进确保高度的真实性。利用这些高质量的合成和标注数据集，我们训练了一个多模态多语言E5模型mmE5。广泛的实验结果表明，mmE5在MMEB基准和XTD基准上的表现均达到了最先进的水平。我们的代码、数据集和模型可以在以下地址获得：[此处插入URL链接]。 

---
# Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions 

**Title (ZH)**: 用于检索具有模糊名称和复杂交互的物体的复合素描+文本查询 

**Authors**: Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra  

**Link**: [PDF](https://arxiv.org/pdf/2502.08438)  

**Abstract**: Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for numbats. Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., numbat digging in the ground. In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of difficult-to-name but easy-to-draw objects and text describing difficult-to-sketch but easy-to-verbalize object attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at our project website. 

**Abstract (ZH)**: 非母语使用者由于词汇量有限，往往在命名具体物体时遇到困难，尽管他们能够可视化这些物体，例如澳大利亚以外的人寻找负鼠。此外，用户可能希望使用难以绘制的交互方式来搜索这些难以命名但容易绘制的物体，例如负鼠在地上挖掘。在这些常见但复杂的场景中，用户希望有一个接受组合多模态查询的搜索界面，这些查询包括手绘的难以命名但容易绘制的物体的草图以及描述难以绘制但容易口述的物体属性或与场景互动的文本。这个新颖的问题陈述与之前研究得很透彻的基于文本的图像检索（TBIR）和基于草图的图像检索（SBIR）问题有显著不同。为了研究这一较少探索的任务，我们构建了一个名为CSTBIR（组合草图+文本基于图像检索）的数据集，其中包括约200万条查询和10.8万张自然场景图像。此外，为了解决这一问题，我们提出了一种基于预训练的多模态变压器模型STNET（草图+文本网络），该模型使用手绘草图在自然场景图像中定位相关物体，并编码文本和图像以进行图像检索。除了对比学习，我们还提出了多种训练目标，以提高我们模型的性能。广泛实验表明，我们提出的方法在文本查询、草图查询和组合查询的所有模式下均优于多种最先进的检索方法。我们已在项目网站上发布了该数据集和代码。 

---
# Word Synchronization Challenge: A Benchmark for Word Association Responses for LLMs 

**Title (ZH)**: 词同步挑战：面向LLMs的单词关联响应基准 

**Authors**: Tanguy Cazalets, Joni Dambre  

**Link**: [PDF](https://arxiv.org/pdf/2502.08312)  

**Abstract**: This paper introduces the Word Synchronization Challenge, a novel benchmark to evaluate large language models (LLMs) in Human-Computer Interaction (HCI). This benchmark uses a dynamic game-like framework to test LLMs ability to mimic human cognitive processes through word associations. By simulating complex human interactions, it assesses how LLMs interpret and align with human thought patterns during conversational exchanges, which are essential for effective social partnerships in HCI. Initial findings highlight the influence of model sophistication on performance, offering insights into the models capabilities to engage in meaningful social interactions and adapt behaviors in human-like ways. This research advances the understanding of LLMs potential to replicate or diverge from human cognitive functions, paving the way for more nuanced and empathetic human-machine collaborations. 

**Abstract (ZH)**: 本文介绍了单词同步挑战（Word Synchronization Challenge），这是一种新的基准测试方法，用于评估大型语言模型（LLMs）在人机交互（HCI）中的表现。该基准测试使用动态游戏式框架，通过词语联想来检验LLMs模仿人类认知过程的能力。通过模拟复杂的互动过程，它评估LLMs在对话交流中如何解释和与人类思维模式对齐，这对于HCI中的有效社会伙伴关系至关重要。初步研究结果突显了模型复杂性对其表现的影响，提供了关于模型在有意义的社会互动中参与以及以类似人类的方式调整行为能力的见解。该研究深化了对LLMs在复制或偏离人类认知功能方面潜力的理解，为实现更细腻和富有同情心的人机合作铺平了道路。 

---
# Improving Existing Optimization Algorithms with LLMs 

**Title (ZH)**: 使用大规模语言模型提高现有优化算法的性能 

**Authors**: Camilo Chacón Sartori, Christian Blum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08298)  

**Abstract**: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: this https URL 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

将大规模语言模型（LLMs）集成到优化中，创造了一种强大的协同效应，开启了令人兴奋的研究机会。本文探讨了LLMs如何增强现有的优化算法。利用它们预先训练的知识，我们展示了它们提出创新启发式变体和实现策略的能力。为了评估这一点，我们应用了一个复杂的优化算法—构造、合并、求解与适应（CMSA）——这是一种结合了解算构建阶段启发式的混合元启发式算法，用于组合优化问题。我们的结果表明，GPT-4提出的替代启发式方法在性能上优于CMSA中的专家设计启发式方法，特别是在更大的和更密集的图上，性能差距尤为明显。项目网址: [请点击此处](this https URL) 

---
# Wisdom of the Crowds in Forecasting: Forecast Summarization for Supporting Future Event Prediction 

**Title (ZH)**: 群体的智慧在预测中的应用：事件预测的预报总结支持方法 

**Authors**: Anisha Saha, Adam Jatowt  

**Link**: [PDF](https://arxiv.org/pdf/2502.08205)  

**Abstract**: Future Event Prediction (FEP) is an essential activity whose demand and application range across multiple domains. While traditional methods like simulations, predictive and time-series forecasting have demonstrated promising outcomes, their application in forecasting complex events is not entirely reliable due to the inability of numerical data to accurately capture the semantic information related to events. One forecasting way is to gather and aggregate collective opinions on the future to make predictions as cumulative perspectives carry the potential to help estimating the likelihood of upcoming events. In this work, we organize the existing research and frameworks that aim to support future event prediction based on crowd wisdom through aggregating individual forecasts. We discuss the challenges involved, available datasets, as well as the scope of improvement and future research directions for this task. We also introduce a novel data model to represent individual forecast statements. 

**Abstract (ZH)**: 未来事件预测（FEP）是一项至关重要的活动，其需求和应用范围跨越多个领域。虽然传统的统计模拟、预测和时间序列预测方法已经展示了其潜在的价值，但它们在预测复杂事件时的可靠性仍然有限，原因在于这些方法难以准确捕捉与事件相关的语义信息。一种预测方法是收集和聚合公众对未来事件的看法，因为整合个体预测视角有助于估计即将发生的事件的可能性。本研究组织现有旨在利用集体智慧进行未来事件预测的研究和框架。我们讨论了该任务面临的挑战、已有的数据集，以及改进的范围和未来的研究方向。此外，我们还介绍了一种新的数据模型来表示个体的预测陈述。 

---
# LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits 

**Title (ZH)**: LowRA: 在2位精度下实现LLMs的精确高效LoRA微调 

**Authors**: Zikai Zhou, Qizheng Zhang, Hermann Kumbong, Kunle Olukotun  

**Link**: [PDF](https://arxiv.org/pdf/2502.08141)  

**Abstract**: Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization - mapping, threshold selection, and precision assignment - while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance-precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50%. Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments. 

**Abstract (ZH)**: 随着模型参数数量增加到数十亿甚至数百亿，大型语言模型（LLMs）的微调成本日益增加，即使是参数效率微调（PEFT）方法如LoRA也仍然具有资源密集性。我们提出了LowRA，这是首个使LoRA微调能够在每个参数低于2位的情况下实现，并且最小化性能损失的框架。LowRA通过对量化进行细粒度优化——包括映射、阈值选择和精度分配——并利用高效的CUDA内核进行可扩展部署。在4个不同的LLM和4个不同数据集上的广泛评估表明，LowRA在高于2位时能够实现优越的性能-精度权衡，并且在低至1.15位时保持准确，可将内存使用量降低多达50%。我们的结果突显了超低位LoRA微调在资源受限环境中的潜力。 

---
# Vision-Language Models for Edge Networks: A Comprehensive Survey 

**Title (ZH)**: 边缘网络中的视觉-语言模型：一篇全面的综述 

**Authors**: Ahmed Sharshar, Latif U. Khan, Waseem Ullah, Mohsen Guizani  

**Link**: [PDF](https://arxiv.org/pdf/2502.07855)  

**Abstract**: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings. 

**Abstract (ZH)**: 视觉大规模语言模型（VLMs）结合了视觉理解与自然语言处理能力，使其能够执行诸如图像字幕、视觉问答和视频分析等任务。尽管VLMs在自动驾驶汽车、智能监控和医疗健康等领域展示了令人印象深刻的跨领域能力，但由于资源受限边缘设备在处理能力、内存和能耗方面的限制，它们的部署仍然具有挑战性。本文综述了最近在优化VLMs以适应边缘环境方面的进展，重点关注模型压缩技术，包括剪枝、量化、知识蒸馏以及专门硬件解决方案，这些方法能够提升效率。我们详细讨论了有效的训练和微调方法、边缘部署挑战以及隐私考虑。此外，我们探讨了轻量级VLMs在医疗健康、环境监控和自主系统等领域的多样应用，展示了它们日益增长的影响。通过突出关键设计策略、当前挑战，并提出未来研究方向的建议，本文旨在激励进一步研究VLMs的实际部署，最终使先进的AI技术在资源受限的环境中变得可行。 

---
# Analyzing the Resource Utilization of Lambda Functions on Mobile Devices: Case Studies on Kotlin and Swift 

**Title (ZH)**: 分析移动设备上Lambda函数的资源利用情况：基于Kotlin和Swift的案例研究 

**Authors**: Chibundom U. Ejimuda, Gaston Longhitano, Reza Rawassizadeh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07809)  

**Abstract**: With billions of smartphones in use globally, the daily time spent on these devices contributes significantly to overall electricity consumption. Given this scale, even minor reductions in smartphone power use could result in substantial energy savings. This study explores the impact of Lambda functions on resource consumption in mobile programming. While Lambda functions are known for enhancing code readability and conciseness, their use does not add to the functional capabilities of a programming language. Our research investigates the implications of using Lambda functions in terms of battery utilization, memory usage, and execution time compared to equivalent code structures without Lambda functions. Our findings reveal that Lambda functions impose a considerable resource overhead on mobile devices without offering additional functionalities. 

**Abstract (ZH)**: 在全球有数十亿部智能手机投入使用的情况下，这些设备每天的使用时间对总体电力消耗贡献巨大。鉴于这种规模，即使是轻微减少智能手机的电力消耗，也可能带来显著的能源节省。本研究探讨了Lambda函数在移动编程中的资源消耗影响。尽管Lambda函数众所周知地提高了代码的可读性和简洁性，但它们的使用并不增加编程语言的功能。我们的研究考察了使用Lambda函数在电池使用、内存占用和执行时间方面的影响，与不使用Lambda函数的等效代码结构相比。研究发现，Lambda函数在移动设备上会带来显著的资源开销，而不提供额外的功能性。 

---
