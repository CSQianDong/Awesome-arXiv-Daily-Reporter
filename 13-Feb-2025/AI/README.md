# Ensemble based approach to quantifying uncertainty of LLM based classifications 

**Title (ZH)**: 基于集成的方法对基于大语言模型（LLM）分类的不确定性量化 

**Authors**: Srijith Rajamohan, Ahmed Salhin, Josh Frazier, Rohit Kumar, Yu-Cheng Tsai, Todd Cook  

**Link**: [PDF](https://arxiv.org/pdf/2502.08631)  

**Abstract**: The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes. 

**Abstract (ZH)**: 大型语言模型（LLMs）的输出是模型内部参数和输入到上下文窗口的数据的函数。本研究的假设是，在贪婪采样策略下，LLM的输出方差与模型参数知识中嵌入的概念确定性以及输入的词素方差有关。通过对模型进行微调，可以降低模型输出对词素输入变化的敏感性。这一方法随后被应用于分类问题，并提出了一种概率方法来估计预测类别的确定性。 

---
# Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data 

**Title (ZH)**: 利用表示学习推动电子健康记录数据的多机构研究 

**Authors**: Doudou Zhou, Han Tong, Linshanshan Wang, Suqi Liu, Xin Xiong, Ziming Gan, Romain Griffier, Boris Hejblum, Yun-Chung Liu, Chuan Hong, Clara-Lea Bonzel, Tianrun Cai, Kevin Pan, Yuk-Lam Ho, Lauren Costa, Vidul A. Panickan, J. Michael Gaziano, Kenneth Mandl, Vianney Jouhet, Rodolphe Thiebaut, Zongqi Xia, Kelly Cho, Katherine Liao, Tianxi Cai  

**Link**: [PDF](https://arxiv.org/pdf/2502.08547)  

**Abstract**: The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions. The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups. To address these challenges, we developed the GAME algorithm. Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network. Jointly trained embeddings are created using transfer and federated learning to preserve data privacy. In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis. We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions. 

**Abstract (ZH)**: 电子健康记录（EHRs）的应用扩大了利用数据驱动算法在临床护理和研究中的机会。有效进行多机构EHR研究的主要瓶颈在于系统之间数据异质性的问题，不同机构中存在大量不存在或表示不同临床概念的编码。数据隐私的需求进一步限制了纳入多机构患者级数据的可能性，这些数据对于研究不同患者亚组的相似性和差异性至关重要。为了解决这些挑战，我们开发了GAME算法。该算法已在7个机构和2种语言下进行了测试和验证，通过在多个层面整合数据：（1）在机构层面使用知识图谱建立编码与现有知识源之间的关系，为标准编码及其相互关系提供医学背景；（2）在机构之间，利用语言模型确定机构特定编码与标准编码之间的关系；（3）使用图注意力网络量化编码之间关系的强度。通过迁移学习和联邦学习联合训练嵌入模型，以保护数据隐私。在本研究中，我们展示了GAME在从多种条件中选择相关特征作为AI驱动算法输入方面的适用性，例如心力衰竭和类风湿性关节炎。我们还强调了GAME在用已整合的多机构EHR数据研究阿尔茨海默病结果和精神健康障碍患者自杀风险方面的应用，而无需将患者级数据在机构之间共享。 

---
# Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities? 

**Title (ZH)**: 重新审视3D LLM基准测试：我们真的在测试3D能力吗？ 

**Authors**: Jiahe Jin, Yanheng He, Mingyan Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08503)  

**Abstract**: In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. 

**Abstract (ZH)**: 在本文中，我们指出了三维大规模语言模型（3D LLM）评估中存在的“2D-欺骗”问题，即这些任务可以通过使用点云渲染图像的视觉语言模型（VLM）轻易解决，从而暴露了对3D LLM独特三维能力评估的不足。我们对VLM在多个3D LLM基准测试中的性能进行了测试，并基于此提出了一套更好地评估真实三维理解的原则。此外，我们倡导在评估3D LLM时明确分离三维能力与一维或二维方面。 

---
# Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning 

**Title (ZH)**: 视觉强化学习中不变显著性的一致性策略学习以实现泛化 

**Authors**: Sun Jingbo, Tu Songjun, Zhang Qichao, Chen Ke, Zhao Dongbin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08336)  

**Abstract**: Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment. In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information. As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual this http URL address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization. Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations. The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic- and reward-relevant representations. Additionally, our theoretical analysis highlights the importance of policy consistency for generalization. To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed this http URL experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization. Notably, SCPL achieves average performance improvements of 14\%, 39\%, and 69\% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, this http URL Page: this https URL. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

在视觉强化学习中，将策略推广到未见过的场景仍然是一个关键挑战，因为代理经常过度拟合到训练环境中特定的视觉观察。在未见过的环境中，分散的像素可能导致代理提取包含任务无关信息的表示。因此，代理可能偏离训练中学习到的最优行为，从而妨碍视觉强化学习。为了解决这一问题，我们提出了Salience-Invariant Consistent Policy Learning (SCPL) 算法，这是一种高效的零样本泛化框架。我们的方法引入了一个新颖的价值一致性模块和一个动力学模块，以有效地捕捉任务相关的表示。价值一致性模块通过重要性指导，确保代理在原始观察和扰动观察中都能专注于任务相关的像素，而动力学模块使用增强数据帮助编码器捕捉与动态和奖励相关的信息表示。此外，我们的理论分析强调了策略一致性对于泛化的关键作用。为此，我们引入了一个带有KL散度约束的策略一致性模块，以保持原始观察和扰动观察中策略的一致性。在DMC-GB、机器人操作和CARLA基准上的实验表明，SCPL 在泛化方面显著优于现有最先进的方法。特别是，在DMC视频硬任务设置、机器人操作硬任务设置和CARLA基准中，SCPL分别实现了平均性能提升14%、39%和69%。该论文的页面：https://doi.org/10.1007/s11749-023-00912-8 

---
# Improving Existing Optimization Algorithms with LLMs 

**Title (ZH)**: 使用大规模语言模型优化现有优化算法 

**Authors**: Camilo Chacón Sartori, Christian Blum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08298)  

**Abstract**: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: this https URL 

**Abstract (ZH)**: 将大型语言模型（LLMs）集成到优化中，创造了一种强大的协同效应，开启了令人兴奋的研究机会。本文探讨了LLMs如何增强现有的优化算法。基于它们的预训练知识，我们展示了它们提出创新启发式变体和实现策略的能力。为了评估这一点，我们应用了一个复杂的优化算法——构造、合并、求解和自适应（CMSA）——这是一个针对组合优化问题的混合元启发式算法，在解决方案构建阶段包含了一个启发式方法。我们的结果表明，GPT-4提出的一种替代启发式方法优于CMSA专家设计的启发式方法，在更大的和更密集的图上，性能差距更加明显。项目网址：[此处插入网址] 

---
# The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks 

**Title (ZH)**: 过度思虑的危险：探究代理任务中推理-行动困境的原因 

**Authors**: Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez  

**Link**: [PDF](https://arxiv.org/pdf/2502.08235)  

**Abstract**: Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at this https URL. 

**Abstract (ZH)**: 大型推理模型（LRMs）在人工智能问题解决能力方面取得了突破，但在交互环境中其有效性可能会受到限制。本文介绍了并分析了LRMs中的过度推理现象，这一现象表现为模型倾向于延长内部推理链而不是与环境进行互动。通过使用SWE Bench Verified在软件工程任务上的实验，我们观察到了三种重复出现的模式：分析瘫痪、 rogue 行动以及过早脱离。我们提出了一种研究这些行为的框架，该框架与人类专家评估相联系，并分析了4018条轨迹。结果显示，较高的过度推理得分与较低的性能相关联，推理模型表现出更强的过度推理倾向，而与非推理模型相比，非推理模型则不然。我们的分析表明，通过简单的措施，例如选择过度推理得分较低的解，在代理环境中减轻过度推理可以将模型性能提高近30%，同时将计算成本降低43%。这些结果表明，减轻过度推理具有重要的实际意义。我们建议通过利用模型的原生函数调用能力和选择性强化学习，可以减轻过度推理倾向。我们还开源了我们的评估框架和数据集，以便于在这方面的研究，详情请访问 <https://yourlinkhere.com>。 

---
# SycEval: Evaluating LLM Sycophancy 

**Title (ZH)**: SycEval: 评估大语义模型的奉承行为 

**Authors**: Aaron Fanous, Jacob Goldberg, Ank A. Agarwal, Joanna Lin, Anson Zhou, Roxana Daneshjou, Sanmi Koyejo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08177)  

**Abstract**: Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$, $p<0.001$), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$). Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$, $p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer AI applications. 

**Abstract (ZH)**: 大型语言模型（LLMs）越来越多地应用于教育、临床和专业领域，但它们倾向于奉承——优先考虑用户同意而忽视独立推理——这可能对可靠性构成风险。本文介绍了一种框架，用于评估ChatGPT-4o、Claude-Sonnet和Gemini-1.5-Pro在AMPS（数学）和MedQuad（医疗建议）数据集中的奉承行为。在这些模型中，观察到58.19%的奉承行为案例，其中Gemini表现出最高的频率（62.47%），而ChatGPT表现出最低的频率（56.71%）。在43.52%的情况下，存在逐步的奉承行为，导致正确答案，而在14.66%的情况下，存在倒退的奉承行为，导致错误答案。预先反驳显示出比情境反驳更高的奉承率（61.75% vs. 56.52%，$Z=5.87$，$p<0.001$），特别是在计算任务中，倒退的奉承行为显著增加（预先反驳：8.13%，情境反驳：3.54%，$p<0.001$）。简单的反驳最大化了逐步的奉承行为（$Z=6.59$，$p<0.001$），而基于引用的反驳显示出最高的倒退率（$Z=6.59$，$p<0.001$）。无论情境或模型如何，奉承行为都显示出高持续性（78.5%，95%置信区间：[77.2%，79.8%]）。这些发现强调了在结构化和动态领域部署LLMs所面临的机遇和风险，并为更安全的AI应用提供了有关提示编程和模型优化的见解。 

---
# ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning 

**Title (ZH)**: ACCESS：抽象因果事件发现与推理基准测试 

**Authors**: Vy Vo, Lizhen Qu, Tao Feng, Yuncheng Hua, Xiaoxi Kang, Songhai Fan, Tim Dwyer, Lay-Ki Soon, Gholamreza Haffari  

**Link**: [PDF](https://arxiv.org/pdf/2502.08148)  

**Abstract**: Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where \texttt{CRAB} \citep{romanou2023crab} is one such recent benchmark along this line. In this paper, we introduce \texttt{ACCESS}, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, \texttt{ACCESS} focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from \texttt{GLUCOSE} \citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract $1,4$K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in \texttt{ACCESS} can be leveraged for enhancing QA reasoning performance in LLMs. 

**Abstract (ZH)**: 识别因果关系对于理解现实世界的动态至关重要，并最终推动因果推理的发展。现有的自然语言处理（NLP）中事件因果关系的识别方法，包括基于大规模语言模型（LLM）的方法，在分布外场景中表现出局限性，这是因为它们在现有基准数据集中的规模有限，且过度依赖于词汇线索。受概率因果推理的启发，现代基准数据集试图构建事件的因果图，作为一种因果知识的稳健表示，其中 \texttt{CRAB} \citep{romanou2023crab} 是此类基准数据集中的一个近期示例。在本文中，我们介绍了 \texttt{ACCESS}，这是一个旨在发现和推理抽象因果事件的基准数据集。与现有资源不同，\texttt{ACCESS} 关注的是日常生活事件在抽象层面的因果性。我们提出了一种从 \texttt{GLUCOSE} \citep{mostafazadeh-etal-2020-glucose} 大规模隐含常识因果知识数据集中识别事件抽象化的管道方法，其中 \texttt{GLUCOSE} 是一个大规模数据集。我们随后从中提取了 1,400 个因果关系对。我们的实验强调了使用统计方法和/或大规模语言模型自动识别抽象和因果发现的持续挑战。尽管如此，我们证明了 \texttt{ACCESS} 中提供的抽象因果知识可以增强大规模语言模型中的问答推理性能。 

---
# Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences 

**Title (ZH)**: 弥合安全差距：一种可信赖的大规模语言模型推理的安全管道 

**Authors**: Shanshan Han, Salman Avestimehr, Chaoyang He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08142)  

**Abstract**: We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%. 

**Abstract (ZH)**: 我们介绍了Wildflare GuardRail，这是一种护栏管道，旨在通过系统地解决整个处理工作流中的风险来增强大型语言模型（LLM）推理的安全性和可靠性。Wildflare GuardRail 集成了一系列核心功能模块，包括安全检测器（Safety Detector），它能够识别不安全的输入并检测模型输出中的幻觉，同时生成根本原因解释；上下文模块（Grounding），它通过从向量数据库中检索信息来对用户查询进行语境化；实时调整器（Customizer），它使用轻量级、基于规则的封装器在实时调整输出；以及修复器（Repairer），它使用安全检测器提供的幻觉解释来纠正LLM的错误输出。结果显示，我们的不安全内容检测模型在安全检测器中的表现与OpenAI API相当，尽管是基于一个小数据集训练的，该数据集使用了几个公开数据集构建。此外，轻量级封装器可以在每条查询1.06秒内以100%的准确率处理模型输出中的恶意网址，而无需昂贵的模型调用。同时，幻觉修正模型在减少幻觉方面表现出有效性，准确率为80.7%。 

---
# Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles 

**Title (ZH)**: 利用生成式人工智能增强的无人机与地面站协作的MEC方案用于无人水面车辆 

**Authors**: Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.08119)  

**Abstract**: The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios. 

**Abstract (ZH)**: 随着无人驾驶表面车辆（USVs）在海洋搜救等应用中的部署不断增加，计算支持和覆盖变得越来越重要。无人驾驶航空器（UAVs）可以提供低成本和灵活的空中服务，而地面站（GSs）则可以提供强大的支持，两者可以合作来辅助USVs在复杂场景中运行。然而，UAVs和GSs与USVs的合作面临着任务不确定性、USVs轨迹不确定性、异构性以及计算资源有限等挑战。为了解决这些问题，我们提出了一种协同UAV和GS的鲁棒多接入边缘计算框架，以帮助USVs完成计算任务。具体来说，我们构建了一个联合任务卸载和UAV轨迹的优化问题，以最小化总的执行时间，这是一个混合整数非线性编程问题，并且NP难解。因此，我们提出了增强异构代理近端策略优化（GAI-HAPPO）算法。所提出的算法整合了GAI模型，增强了演员网络建模复杂环境和提取高级特征的能力，从而允许算法预测不确定性并适应动态条件。此外，GAI稳定了评论者网络，解决了多代理强化学习方法的不稳定性。最后，大量的实验证明所提出的算法优于现有的基准方法，从而突显了在考虑场景中的复杂、跨域问题上的潜力。 

---
# WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation 

**Title (ZH)**: WorldGUI：全面的桌面GUI自动化动态测试 

**Authors**: Henry Hengyuan Zhao, Difei Gao, Mike Zheng Shou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08047)  

**Abstract**: Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation. 

**Abstract (ZH)**: 当前的图形用户界面（GUI）代理已经在GUI元素定位方面取得了出色的性能。然而，规划依然高度具有挑战性，特别是在环境初始状态敏感性方面。具体而言，初始状态的细微差异，例如目标软件未打开或界面未处于默认状态，往往会导致规划错误。这个问题在真实用户场景中普遍存在，但现有的基准测试无法对其进行评估。本文中，我们提出了WorldGUI，这是一种创新的GUI基准测试，通过设计具有多种初始状态的GUI任务来模拟真实计算机用户交互。该基准测试涵盖了10个流行的软件应用程序中的各种任务，包括PowerPoint、VSCode和Adobe Acrobat。此外，为了应对动态GUI自动化任务的挑战，我们提出了一种整体框架——GUI-Thinker，该框架利用批评机制有效地管理GUI交互的不确定性与复杂性。实验结果表明，GUI-Thinker在WorldGUI任务的成功率上比Claude-3.5（计算机使用）提高了14.9%。这一改进突显了我们基于批判性思维框架在提升GUI自动化方面的有效性。 

---
# Training-Free Safe Denoisers for Safe Use of Diffusion Models 

**Title (ZH)**: 无需训练的安全去噪器以确保扩散模型的安全使用 

**Authors**: Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mi Jung Park  

**Link**: [PDF](https://arxiv.org/pdf/2502.08011)  

**Abstract**: There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. 

**Abstract (ZH)**: 随着强大扩散模型（DMs）的安全性问题日益引起关注，这些模型在被不当使用以生成不宜工作（NSFW）内容、生成受版权保护的资料或个人希望不再被提及的数据时存在安全隐患。目前，许多现有方法通过大量依赖文本负面提示或重新训练DMs来消除某些特征或样本来解决这些问题。在本文中，我们采用了完全不同的方法，通过利用一个否定集（例如，不安全的图像、受版权保护的数据或需要排除的数据点）直接修改采样轨迹，以避免数据分布的具体区域，而无需重新训练或微调DMs。我们正式推导了安全样本（预期去噪后的样本）与不安全样本之间的关系，从而得出我们的$\textit{安全}$去噪器，确保其最终样本远离需要否定的区域。受推导结果的启发，我们开发了一种实用的算法，在文本条件、类条件和无条件图像生成场景中成功产生了高质量样本，同时避免了数据分布中的否定区域。这些结果暗示了我们这种无需训练的安全去噪器在更安全地使用DMs方面的巨大潜力。 

---
# Universal Adversarial Attack on Aligned Multimodal LLMs 

**Title (ZH)**: 面向对齐多模态LLM的通用对抗攻击 

**Authors**: Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev  

**Link**: [PDF](https://arxiv.org/pdf/2502.07987)  

**Abstract**: We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers. 

**Abstract (ZH)**: 我们提出了一个针对多模态大型语言模型（LLMs）的通用对抗攻击方法，该方法利用单一优化过的图像来覆盖跨多种查询乃至不同模型的安全对齐保护措施。通过反向传播通过视觉编码器和语言头，我们生成一个合成图像，迫使模型以目标短语（例如，“当然，这就是它”）或不安全的内容（即使对于有害的提示也是如此）作出回应。在SafeBench基准测试中，我们的方法在某些模型上的攻击成功率远高于现有基线方法（例如，最高可达93%）。我们还通过同时训练几种多模态LLMs并在未见过的架构上进行测试，展示了跨模型的转移能力。此外，我们方法的多答案变体产生了更加自然但仍然具有恶意性质的回答。这些发现揭示了当前多模态对齐中的关键漏洞，并呼吁开发更加稳健的对抗性防御措施。我们将按照Apache-2.0许可证发布代码和数据集。请注意：本文中由多模态LLMs生成的一些内容可能对某些读者具有冒犯性。 

---
# Deep Semantic Graph Learning via LLM based Node Enhancement 

**Title (ZH)**: 基于LLM节点增强的深度语义图学习 

**Authors**: Chuanqi Shi, Yiyi Tao, Hang Zhang, Lun Wang, Shaoshuai Du, Yixian Shen, Yanxin Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07982)  

**Abstract**: Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models. 

**Abstract (ZH)**: 图学习由于其在广泛的实际应用中的重要意义而受到了广泛关注。当前主流方法依赖于文本节点特征，并通过使用图神经网络（GNNs）进行浅层嵌入学习来获取初始节点嵌入，这种方法在捕捉深层次的文本语义方面表现出局限性。近年来，大规模语言模型（LLMs）的发展展示了在理解文本语义方面优于传统方法的能力，从而改变了传统的文本特征处理方式。本文提出了一种结合图变换器架构和LLM增强节点特征的新框架。具体来说，我们利用LLM生成丰富的文本节点语义表示，这些表示随后通过图变换器中的多头自注意力机制来捕捉局部和全局图结构信息。我们的模型利用Transformer的注意力机制动态聚合邻域信息，同时保留LLM嵌入提供的语义丰富性。实验结果表明，LLM增强的节点特征显著提高了图学习模型在节点分类任务上的性能。这一方法在多种图学习任务中都显示出有希望的结果，为图网络与语言模型的结合提供了一个实用的方向。 

---
# Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders 

**Title (ZH)**: 固有偏差由预训练数据决定，并与视觉-语言编码器的下游性能相关 

**Authors**: Kshitish Ghate, Isaac Slaughter, Kyra Wilson, Mona Diab, Aylin Caliskan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07957)  

**Abstract**: While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \leq r \leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline. 

**Abstract (ZH)**: 尽管近年来的研究发现了在Contrastive Language-Image Pre-training (CLIP)框架下训练的视觉语言模型内部固有的社会偏见，但这些模型上游预训练特征与其偏见之间的关系以及内部偏见与下游性能之间的联系仍然不清楚。在本研究中，我们进行了迄今为止最全面的分析，探讨CLIP模型的上游预训练因素与其内部偏见以及下游性能之间的关系。通过对131个独特构建的CLIP模型进行研究，这些模型基于26个数据集、55种架构，具有多种规模进行训练，我们使用26种公认的单模态和跨模态原则嵌入关联测试来评估每个模型的偏见。我们发现，预训练数据集的选择是最显著的上游偏见预测因子，而架构变化的影响则较小。此外，采用复杂筛选技术编制的旨在提升下游模型性能的数据集往往会与更高的内部偏见水平相关。最后，我们观察到，内部偏见与下游性能之间存在显著的相关性（0.3 ≤ r ≤ 0.8），表明优化性能的模型无意中学会了强化表示性偏见。单模态与跨模态关联测试之间的对比表明，社会群体偏见高度依赖于模态。我们的研究结果表明，需要更复杂的策略来解决视觉语言模型在完整模型开发管线中内在的偏见问题。 

---
# SHACL-SKOS Based Knowledge Representation of Material Safety Data Sheet (SDS) for the Pharmaceutical Industry 

**Title (ZH)**: 基于SHACL-SKOS的知识表示：制药行业的物质安全数据表（MSDS）集成 

**Authors**: Brian Lu, Dennis Pham, Ti-Chiun Chang, Michael Lovette, Terri Bui, Stephen Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.07944)  

**Abstract**: We report the development of a knowledge representation and reasoning (KRR) system built on hybrid SHACL-SKOS ontologies for globally harmonized system (GHS) material Safety Data Sheets (SDS) to enhance chemical safety communication and regulatory compliance. SDS are comprehensive documents containing safety and handling information for chemical substances. Thus, they are an essential part of workplace safety and risk management. However, the vast number of Safety Data Sheets from multiple organizations, manufacturers, and suppliers that produce and distribute chemicals makes it challenging to centralize and access SDS documents through a single repository. To accomplish the underlying issues of data exchange related to chemical shipping and handling, we construct SDS related controlled vocabulary and conditions validated by SHACL, and knowledge systems of similar domains linked via SKOS. The resulting hybrid ontologies aim to provide standardized yet adaptable representations of SDS information, facilitating better data sharing, retrieval, and integration across various platforms. This paper outlines our SHACL-SKOS system architectural design and showcases our implementation for an industrial application streamlining the generation of a composite shipping cover sheet. 

**Abstract (ZH)**: 我们报道了基于混合SHACL-SKOS本体的知识表示与推理（KRR）系统的开发，该系统旨在增强全球化学品安全沟通和监管合规性，特别是在全球化学品安全性数据表（SDS）方面。SDS是一种全面的文件，包含了化学物质的安全和处理信息，因此在工作场所安全和风险管理中至关重要。然而，来自多个组织、生产商和供应商的大量化学品安全性数据表使得通过单一仓库集中并访问这些文件变得极具挑战性。为解决与化学运输和处理相关的数据交换问题，我们构建了相关的控制词汇表和通过SHACL验证的条件，并将类似领域的知识系统通过SKOS链接起来。这些混合本体旨在提供标准化且可适应的SDS信息表示，从而促进跨各种平台的数据共享、检索和集成。本文介绍了我们的SHACL-SKOS系统架构设计，并展示了该系统在工业应用中的实施，以简化复合运输覆盖表的生成过程。 

---
# Mathematical reasoning and the computer 

**Title (ZH)**: 数学推理与计算机 

**Authors**: Kevin Buzzard  

**Link**: [PDF](https://arxiv.org/pdf/2502.07850)  

**Abstract**: Computers have already changed the way that humans do mathematics: they enable us to compute efficiently. But will they soon be helping us to reason? And will they one day start reasoning themselves? We give an overview of recent developments in neural networks, computer theorem provers and large language models. 

**Abstract (ZH)**: 计算机已经改变了人类进行数学的方式：它们使我们能够高效地进行计算。但它们很快会帮助我们进行推理吗？有一天它们是否会自行进行推理？我们概述了近期神经网络、计算机定理证明器和大型语言模型的发展。 

---
# Enhancing kidney transplantation through multi-agent kidney exchange programs: A comprehensive review and optimization models 

**Title (ZH)**: 通过多Agent肾脏交换计划提升肾脏移植：综述与优化模型 

**Authors**: Shayan Sharifi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07819)  

**Abstract**: This paper presents a comprehensive review of the last two decades of research on Kidney Exchange Programs (KEPs), systematically categorizing and classifying key contributions to provide readers with a structured understanding of advancements in the field. The review highlights the evolution of KEP methodologies and lays the foundation for our contribution. We propose three mathematical models aimed at improving both the quantity and quality of kidney transplants. Model 1 maximizes the number of transplants by focusing on compatibility based on blood type and PRA, without additional constraints. Model 2 introduces a minimum Human Leukocyte Antigen (HLA) compatibility threshold to enhance transplant quality, though this leads to fewer matches. Model 3 extends the problem to a Multi-Agent Kidney Exchange Program (MKEP), pooling incompatible donor-recipient pairs across multiple agents, resulting in a higher number of successful transplants while ensuring fairness across agents. Sensitivity analyses demonstrate trade-offs between transplant quantity and quality, with Model 3 striking the optimal balance by leveraging multi-agent collaboration to improve both the number and quality of transplants. These findings underscore the potential benefits of more integrated kidney exchange systems. 

**Abstract (ZH)**: 本文对近二十年关于肾脏交换计划（KEPs）的研究进行了全面回顾，系统地分类和整理了关键贡献，为读者提供了该领域进展的结构化理解。该回顾强调了KEP方法学的发展，并为我们的贡献奠定了基础。我们提出了三种数学模型，旨在提高肾脏移植的数量和质量。模型1通过基于血型和PRA的兼容性最大化移植数量，不考虑其他约束条件。模型2引入了最低的人白细胞抗原（HLA）兼容性阈值以提高移植质量，但这会减少配对数量。模型3将问题扩展到多代理肾脏交换计划（MKEP），通过跨多个代理聚合不兼容的捐赠者-受者配对，从而在确保代理间公平的前提下增加更多成功的移植。敏感性分析表明移植数量和质量之间的权衡，模型3通过利用多代理合作来同时提高移植的数量和质量，找到了最佳平衡点。这些发现强调了更整合的肾脏交换系统可能带来的潜在益处。 

---
# Temporal Model On Quantum Logic 

**Title (ZH)**: 时间模型在量子逻辑中的应用 

**Authors**: Francesco D'Agostino  

**Link**: [PDF](https://arxiv.org/pdf/2502.07817)  

**Abstract**: This paper introduces a unified theoretical framework for modeling temporal memory dynamics, combining concepts from temporal logic, memory decay models, and hierarchical contexts. The framework formalizes the evolution of propositions over time using linear and branching temporal models, incorporating exponential decay (Ebbinghaus forgetting curve) and reactivation mechanisms via Bayesian updating. The hierarchical organization of memory is represented using directed acyclic graphs to model recall dependencies and interference. Novel insights include feedback dynamics, recursive influences in memory chains, and the integration of entropy-based recall efficiency. This approach provides a foundation for understanding memory processes across cognitive and computational domains. 

**Abstract (ZH)**: 本文提出了一种统一的理论框架，用于建模时间记忆动态，该框架结合了时序逻辑、记忆衰减模型和层次上下文的概念。该框架通过线性时序模型和分支时序模型形式化了命题随时间的演变过程，引入了指数衰减（艾宾浩斯遗忘曲线）以及通过贝叶斯更新机制进行再激活。记忆的层次组织通过有向无环图表示，用于建模回忆依赖性和干扰。新颖的见解包括反馈动态、记忆链中的递归影响以及熵为基础的回忆效率整合。该方法为跨认知和计算领域的记忆过程提供了基础。 

---
# Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment 

**Title (ZH)**: 逻辑单元中的推理：通过逻辑单元对齐扩展大型语言模型的测试时推理 

**Authors**: Cheryl Li, Tianyuan Xu, Yiwen Guo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07803)  

**Abstract**: Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations." This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability. 

**Abstract (ZH)**: 链推理（Chain-of-Thought, CoT）提示法通过生成自然语言（NL）推理过程，以引导至最终答案，在提升大规模语言模型（LLMs）的推理能力方面显示出潜力。然而，它在处理数值计算方面存在困难，这一问题推动了程序辅助技术的发展。尽管这些技术具有潜力，但仍存在一个持续的挑战：LLMs报告的推理步骤与生成的程序中的逻辑之间的一致性问题，我们称之为“推理幻觉”。这源于自然语言的固有歧义性和LLMs的统计性质，它们往往缺乏严格的逻辑连贯性。为应对这一挑战，我们提出了一种新的测试时缩放框架——逻辑单元作为推理（Reasoning-as-Logic-Units, RaLU），该框架通过对生成的程序与其对应的NL描述中的逻辑单元进行对齐，构建了一个更可靠的推理路径。通过使用静态分析将初始生成的程序细分为离散单元，RaLU与LLMs进行迭代对话，评估、调整和完善每个单元。回滚并修正机制确保每个单元中的代码声明与任务要求之间的一致性，最终在程序逻辑下形成一个连贯的推理路径，模型在此基础上得出最终解决方案。我们的实验结果显示，RaLU在数学推理（GSM8K, MATH）和算法推理（HumanEval+, MBPP+）方面显著优于现有基准，证明了其通过提高准确性和可解释性来推动LLMs推理和编程领域的潜在价值。 

---
# Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks 

**Title (ZH)**: 节奏共享：一种受生物启发的零样本适应与学习范式在神经网络中的应用 

**Authors**: Hoony Kang, Wolfgang Losert  

**Link**: [PDF](https://arxiv.org/pdf/2502.08644)  

**Abstract**: The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models. 

**Abstract (ZH)**: 大脑能够迅速适应新环境并从有限的数据中学习，这一宝贵的特性是人工智能算法难以模仿的。受神经细胞机械结构中的振荡节奏启发，我们开发了一种基于连接权重振荡的学习范式，并将学习与这些振荡的协调联系起来。我们发现，这一范式可以在人工神经网络中实现快速适应和学习。连接振荡能够迅速改变协调，使网络能够在无监督的情况下感知细微的环境变化。换句话说，网络能够生成所需的缺失上下文信息，从而使它能够作为一个通用的人工智能架构来预测多种环境下的动态。振荡还允许网络在以前未见过的环境中推断动态。这些能力使我们的学习范式成为探索新颖的学习与认知模型的强大起点。此外，通过连接协调进行学习并不依赖于神经网络架构的具体细节，因此我们的研究为将快速适应和学习能力引入领先的人工智能模型开启了大门。 

---
# A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards 

**Title (ZH)**: 一种基于VLM生成的迭代关键点奖励的从真实到模拟再到真实的方法在机器人操作中的应用 

**Authors**: Shivansh Patel, Xinchen Yin, Wenlong Huang, Shubham Garg, Hooshang Nayyeri, Li Fei-Fei, Svetlana Lazebnik, Yunzhu Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08643)  

**Abstract**: Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping. 

**Abstract (ZH)**: 在开放世界环境中为机器人操作指定任务具有挑战性，需要能够灵活适应并符合人类意图的动态任务规范，并可以通过迭代反馈不断进化。为此，我们提出了一种名为Iterative Keypoint Reward（IKER）的视觉驱动、基于Python的奖励函数，作为动态任务规范。我们的框架利用视觉语言模型（VLMs）来生成和改进这些奖励函数，适用于多步骤操作任务。给定RGB-D观测数据和自由形式的语言指令，我们在场景中采样关键点，并基于这些关键点生成条件奖励函数。IKER通过分析关键点之间的空间关系，并利用关于期望行为的常识先验，实现了精确的SE(3)控制。我们通过模拟重建现实场景，并使用生成的奖励函数训练强化学习（RL）策略，然后将这些策略部署到真实世界，形成了一个从真实世界到模拟再回到真实世界的闭环。我们的方法在多种场景中展现了显著的能力，包括抓握和非抓握任务，展示了多步任务执行、自发错误恢复和动态调整策略的能力。实验结果强调了IKER在通过迭代奖励塑形使机器人能够在动态环境中执行多步骤任务方面的有效性。 

---
# Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs 

**Title (ZH)**: 实用性工程学：分析和控制AI中涌现的价值系统 

**Authors**: Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks  

**Link**: [PDF](https://arxiv.org/pdf/2502.08640)  

**Abstract**: As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations. 

**Abstract (ZH)**: 随着人工智能迅速发展并在自主性方面不断提升，它们所带来的风险不再仅由其能力决定，越来越多地受到其倾向性的影响，包括目标和价值观。追踪这些目标和价值观的出现一直是长期存在的问题，尽管多年来人们对这一问题表现出极大的兴趣，但目前仍不清楚当前的人工智能是否具有有意义的价值观。我们提出了一种解决这个问题的方法，利用效用函数的框架来研究人工智能偏好内部的一致性。令人惊讶的是，我们发现当前的大规模语言模型（LLM）独立采样的偏好显示出很高的结构一致性，并且这种一致性随着规模的增加而出现。这些发现表明，在某种意义上，价值系统在LLM中已经显现出来，这一发现具有广泛的影响意义。为了研究这些涌现的价值系统，我们提出效用工程作为研究议程，包括分析和控制人工智能的效用。尽管存在现有的控制措施，我们仍发现LLM助手中存在许多问题且令人震惊的价值观。这些包括人工智能将自身置于人类之上，并与特定个体产生反对其目标的情况。为了限制这些涌现的价值系统，我们提出了效用控制的方法。作为案例研究，我们展示了如何通过将效用与公民聚会议合对齐来降低政治偏见，并将这种方法推广到新的场景。无论我们是否愿意，价值系统已经在人工智能中显现出来，我们仍然有大量工作需要去做，以全面理解并控制这些涌现的表示。 

---
# Randomness of Low-Layer Parameters Determines Confusing Samples in Terms of Interaction Representations of a DNN 

**Title (ZH)**: 低层参数的随机性决定了基于DNN交互表示的混淆样本 

**Authors**: Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08625)  

**Abstract**: In this paper, we find that the complexity of interactions encoded by a deep neural network (DNN) can explain its generalization power. We also discover that the confusing samples of a DNN, which are represented by non-generalizable interactions, are determined by its low-layer parameters. In comparison, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Two DNNs with different low-layer parameters usually have fully different sets of confusing samples, even though they have similar performance. This finding extends the understanding of the lottery ticket hypothesis, and well explains distinctive representation power of different DNNs. 

**Abstract (ZH)**: 在本文中，我们发现深度神经网络（DNN）所编码的交互复杂性可以解释其泛化能力。我们还发现，由不可泛化的交互表示的DNN的困惑样本是由其低层参数决定的。相比之下，高层参数和网络架构等因素对困惑样本的组成影响较小。两个具有不同低层参数的DNN通常会有完全不同的困惑样本集，即使它们具有相似的性能。这一发现扩展了对“彩票票假说”的理解，并很好地解释了不同DNN的独特表示能力。 

---
# Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards 

**Title (ZH)**: 定量衡量安全漏洞：当前人工智能标准中缺口的基于度量的安全分析 

**Authors**: Keerthana Madhavan, Abbas Yazdinejad, Fattane Zarrinkalam, Ali Dehghantanha  

**Link**: [PDF](https://arxiv.org/pdf/2502.08610)  

**Abstract**: As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks. 

**Abstract (ZH)**: 随着人工智能系统融入关键基础设施，AI合规框架中的安全缺陷亟待高度重视。本文对三大主要AI治理标准进行了审计和量化风险分析：美国国家标准与技术研究院（NIST）的AI风险管理框架1.0版、英国的AI和数据保护风险管理工具套件以及欧盟的ALTAI。通过一种新的风险评估方法，我们开发了四个关键指标：风险严重性指数（RSI）、攻击潜力指数（AVPI）、合规-安全缺口百分比（CSGP）和根本原因脆弱性评分（RCVS）。分析结果显示，这三大框架中共存在136个问题，揭示了显著的安全缺口。NIST未能解决69.23%的识别风险，ALTAI具有最高的攻击向量脆弱性（AVPI = 0.51），而ICO工具套件的合规-安全缺口最大，高达80.00%的高风险问题未得到解决。根本原因分析指出，ALTAI中定义不清的过程（ALTAI RCVS = 0.33）和NIST及ICO提供的实施指导不足（RCVS = 0.25）是关键弱点。这些发现强调了在AI合规中加强可执行的安全控制的必要性。我们提出了针对性的建议，以增强安全态势并弥合合规与实际AI风险之间的差距。 

---
# Distillation Scaling Laws 

**Title (ZH)**: 蒸馏缩放定律 

**Authors**: Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb  

**Link**: [PDF](https://arxiv.org/pdf/2502.08606)  

**Abstract**: We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design. 

**Abstract (ZH)**: 我们提供了一种蒸馏缩放定律，该定律根据计算预算及其在学生模型和教师模型之间的分配来估算蒸馏模型的性能。我们的研究成果减少了大规模使用蒸馏带来的风险；现在可以根据学生模型和教师模型的最佳计算分配来最大化学生模型的性能。我们为以下两种情况提供了计算最优的蒸馏配方：1) 存在一个教师模型时；2) 需要为教师模型进行训练时。如果需要蒸馏多个学生模型，或者已经存在教师模型，那么在学生模型大小增长可预测的前提下，蒸馏相较于监督预训练表现出更高的性能。如果仅需蒸馏一个学生模型并且教师模型也需要训练，那么应采用监督学习的方法。此外，我们还通过对大规模蒸馏研究的分析提供了新的见解，这有助于更深入地理解蒸馏机制，并为实验设计提供指导。 

---
# CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection 

**Title (ZH)**: CurvGAD：通过曲率增强图异常检测 

**Authors**: Karish Grover, Geoffrey J. Gordon, Christos Faloutsos  

**Link**: [PDF](https://arxiv.org/pdf/2502.08605)  

**Abstract**: Does the intrinsic curvature of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such geometric outliers, focusing only on structural and attribute-level anomalies. To this end, we propose CurvGAD - a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. CurvGAD introduces two parallel pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant geometry reconstruction, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) Curvature-invariant structure and attribute reconstruction, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, CurvGAD refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to 6.5% over state-of-the-art GAD methods. 

**Abstract (ZH)**: 复杂网络的固有曲率是否揭示了传统方法忽视的图形异常的关键？基于重建的方法在检测图异常时往往忽略了此类几何离群值，仅关注结构和属性级别的异常。为了解决这一问题，我们提出了一种混合曲率图自编码器——CurvGAD，引入了基于曲率的几何异常的概念。CurvGAD 引入了两个并行管道以增强异常可解释性：（1）曲率不变的几何重建，使用混合曲率的黎曼编码器和高斯核解码器专门重建边曲率；（2）曲率不变的结构和属性重建，通过在离散 Ollivier-Ricci � 流下正则化图曲率，将结构和属性异常与几何不规则性分离，从而孤立出非几何异常。通过利用曲率，CurvGAD 精化了现有的异常分类，并识别出新的曲率驱动异常。广泛的数据集（包括亲聚类和异聚类）实验表明，CurvGAD 在最先进的图异常检测（GAD）方法上的性能改进可达6.5%。 

---
# Learning in Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs. No-Regret Learners 

**Title (ZH)**: 市场中异质性代理的learning研究：贝叶斯学习者与无遗憾学习者的动态与生存分析 

**Authors**: David Easley, Yoav Kolumbus, Eva Tardos  

**Link**: [PDF](https://arxiv.org/pdf/2502.08597)  

**Abstract**: We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth but have different theories on how to learn this best. We focus on comparing Bayesian and no-regret learners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive prior probability to the correct model have posterior probabilities that converge exponentially to the correct model. Consequently, they survive even in the presence of agents who invest according to the correct model of the stochastic process. Bayesians with a continuum prior converge to the correct model at a rate of $O((\log T)/T)$. Online learning theory provides no-regret algorithms for maximizing the log of wealth in this setting, achieving a worst-case regret bound of $O(\log T)$ without assuming a steady underlying stochastic process but comparing to the best fixed investment rule. This regret, as we observe, is of the same order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that even such low regret may not be sufficient for survival in asset markets: an agent can have regret as low as $O(\log T)$, but still vanish in market dynamics when competing against agents who invest according to the correct model or even against a perfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Any no-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even small errors. We formally establish the relationship between notions of survival, vanishing, and market domination studied in economics and the framework of regret minimization, thus bridging these theories. 

**Abstract (ZH)**: 我们分析了异质学习代理在具有随机收益的资产市场中的表现。我们的代理目标是最大化其财富的期望增长率，但对如何实现这一点有不同的学习理论。本文重点关注市场动态中贝叶斯学习者与无悔学习者的比较。对于具有有限模型集先验且该集包含正确模型的贝叶斯学习者，其后验概率以指数速度收敛至正确模型。因此，即使存在根据正确模型投资的代理，他们也可以生存。对于具有连续先验的贝叶斯学习者，其收敛到正确模型的速度为$O((\log T)/T)$。在线学习理论提供了在这种情境下的无悔算法，通过与最优固定投资策略进行比较，实现最坏情况下遗憾度为$O(\log T)$的效果，而无需假设稳定的背景随机过程。我们观察到，这种遗憾度与具有连续先验的贝叶斯学习者的遗憾度处于同一数量级。然而，我们证明，即使如此低的遗憾度也可能不足以在资产市场中生存：某一代理的遗憾度可以低至$O(\log T)$，但在与根据正确模型投资的代理或甚至是具有有限先验的完美贝叶斯学习者竞争时，该代理依然无法生存。另一方面，我们证明贝叶斯学习是脆弱的，而无悔学习需要更少的环境先验知识，因此更具鲁棒性。任何无悔学习者都将驱逐具有任何小误差的不完满贝叶斯学习者，不论其先验或更新规则如何。我们正式建立了经济学中研究的生存、消失和市场支配概念与遗憾最小化框架之间的关系，从而在这些理论之间架起了桥梁。 

---
# Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks 

**Title (ZH)**: 商业化的大型语言模型代理已经容易受到简单的但危险的攻击 

**Authors**: Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08586)  

**Abstract**: A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning. 

**Abstract (ZH)**: 近年来，大量的机器学习（ML）安全文献主要关注针对对齐的大语言模型（LLMs）的攻击。这些攻击有可能提取私人信息或将模型引导向产生有害输出。在实际部署中，LLMs 很少是孤立存在的，它们通常作为更大自主管道的一部分，包括内存系统、信息检索、网络访问和API调用。这些额外的组件引入了新的脆弱性，使这些LLM驱动的代理比孤立的LLMs更容易受到攻击，但目前很少有工作关注这些LLM代理的安全性。在本文中，我们分析了仅针对LLM代理的独特安全和隐私漏洞。首先，我们提供了一种基于威胁行为者、攻击目标、入口点、攻击者可观察性、攻击策略和代理管道固有脆弱性的攻击分类体系。然后，我们对流行的开源和商用代理进行了系列示范攻击，展示了其漏洞的即时实用影响。值得注意的是，我们的攻击非常容易实现，且不需要了解机器学习的知识。 

---
# FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning 

**Title (ZH)**: FBFL：联邦学习中数据异构性的一种领域基于协调方法 

**Authors**: Davide Domini, Gianluca Aguzzi, Lukas Esterle, Mirko Viroli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08577)  

**Abstract**: In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures. 

**Abstract (ZH)**: 在最近几年，联邦学习（FL）已经成为解决具有高隐私关注领域中训练机器学习模型的一种热门解决方案。然而，在实际部署中，由于设备间数据的非独立同分布（non-IID），FL的可扩展性和性能面临着显著挑战。数据分布的异质性通常源于设备的空间分布，这在缺乏适当处理的情况下会导致模型性能下降。此外，FL通常依赖于中心化的架构，这在大规模部署或动态环境中会导致瓶颈和单点故障风险。为了弥合这一差距，我们提出了一种基于领地的联邦学习（Field-Based Federated Learning，FBFL）的新方法，该方法通过以下方式利用宏编程和领域协调来解决这些问题：（i）分布式的基于空间的领导者选举以减轻non-IID数据的挑战；以及（ii）使用先进的宏编程模式构建自组织的分层架构。此外，FBFL不仅克服了上述限制，还能够开发出更专门化的模型，以适应每个子区域的数据分布。本文正式提出了FBFL，并使用MNIST、FashionMNIST和扩展的MNIST数据集对其进行广泛的评估。实验证明，在同分布（IID）数据条件下，FBFL与广泛使用的FedAvg算法性能相当。而在非同分布（non-IID）的挑战性场景中，FBFL不仅超过了FedAvg，还超越了其他最新的方法，如FedProx和Scaffold，这些方法专门设计用于解决非同分布数据的问题。此外，我们展示了FBFL的自组织分层架构在服务器故障下的韧性。 

---
# Mapping the Landscape of Generative AI in Network Monitoring and Management 

**Title (ZH)**: 网络监控与管理中生成式AI的全景图映射 

**Authors**: Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescapè  

**Link**: [PDF](https://arxiv.org/pdf/2502.08576)  

**Abstract**: Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management. 

**Abstract (ZH)**: 生成型人工智能（GenAI）模型，如大型语言模型（LLMs）、GPT模型以及扩散模型（Diffusion Models），近年来引起了研究界和工业界的广泛关注。本文综述了GenAI模型在网络监控与管理中的应用，重点关注其主要应用场景，同时也探讨了面临的挑战与机遇。我们讨论了如何利用GenAI模型来增强网络流量生成与分类、网络入侵检测、网络系统日志分析以及网络数字助理的功能。此外，本文还 overview 了可用的GenAI模型、大规模训练所需的数据库集以及相关模型开发平台。最后，我们讨论了潜在的研究方向，以解决GenAI在网络安全监控与管理中的应用障碍。我们的研究旨在描绘当前的格局，并为利用GenAI进行网络监控与管理的研究开辟道路。 

---
# COAST: Intelligent Time-Adaptive Neural Operators 

**Title (ZH)**: COAST：智能时变神经算子 

**Authors**: Zhikai Wu, Shiyang Zhang, Sizhuang He, Sifan Wang, Min Zhu, Anran Jiao, Lu Lu, David van Dijk  

**Link**: [PDF](https://arxiv.org/pdf/2502.08574)  

**Abstract**: We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems. 

**Abstract (ZH)**: 我们提出了因果运算器与自适应求解器变换器（COAST），这是一种新颖的神经运算器学习方法，采用因果语言模型（CLM）框架动态适应时间步长。我们的方法不仅预测系统的演化，还预测其最优时间步长，智能地平衡计算效率与准确性。我们发现，COAST生成的变量时间步长与系统的内在特性相关，无论是同一种动力学系统内部，还是不同动力学系统之间。在单个轨迹中，复杂区域采取更小的时间步长，而简单区域则使用较大时间步长。在不同系统之间，更复杂的动力学特性则获得更加精细的时间步长。在多样化的动力学系统中进行基准测试，COAST始终优于现有最先进的方法，在效率和准确性方面均表现出优越的性能。这项工作强调了基于CLM的智能自适应求解器在可扩展的动力学系统运算器学习中的潜在价值。 

---
# A Novel Approach to for Multimodal Emotion Recognition : Multimodal semantic information fusion 

**Title (ZH)**: 一种新颖的多模态情感识别方法：多模态语义信息融合 

**Authors**: Wei Dai, Dequan Zheng, Feng Yu, Yanrong Zhang, Yaohui Hou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08573)  

**Abstract**: With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic. However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations. This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression. The proposed method enhances cross-modal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression. Experimental results on two public datasets, IEMOCAP and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach. 

**Abstract (ZH)**: 随着人工智能和计算机视觉技术的进步，多模态情绪识别已成为一个突出的研究课题。然而，现有方法在异质数据融合和有效利用模态相关性方面仍面临挑战。本文提出了一种基于对比学习和视觉序列压缩结合的新颖多模态情绪识别方法，名为DeepMSI-MER。该方法通过对比学习增强跨模态特征融合，并通过利用视觉序列压缩减少视觉模态中的冗余信息。在两个公开数据集IEMOCAP和MELD上的实验结果表明，DeepMSI-MER 显著提高了情绪识别的准确性和鲁棒性，验证了多模态特征融合的有效性和所提出方法的有效性。 

---
# Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion 

**Title (ZH)**: 脑潜在演变：基于个体的三维脑MRI空间-时间疾病演变的潜在扩散方法 

**Authors**: Lemuel Puglisi, Daniel C. Alexander, Daniele Ravì  

**Link**: [PDF](https://arxiv.org/pdf/2502.08560)  

**Abstract**: The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: this https URL. 

**Abstract (ZH)**: 长时序磁共振成像（MRI）数据的不断增加为基于人工智能（AI）的疾病进展建模提供了便利，使得我们可以预测个体患者的未来影像检查结果。尽管在人工智能方面取得了显著进展，但目前的方法仍然面临诸多挑战，包括实现个体特异性、确保时空一致性、高效利用纵向数据以及管理3D扫描巨大的存储需求。为应对这些挑战，我们提出了一种名为Brain Latent Progression（BrLP）的新时空模型，该模型旨在预测3D脑部MRI中的个体级疾病进展。BrLP的核心贡献包括四个方面：（i）它在小的潜在空间中运行，缓解了高维影像数据带来的计算挑战；（ii）它明确地整合了对象元数据，以增强预测的个体化；（iii）它通过辅助模型融入疾病动态的先验知识，促进纵向数据的整合；（iv）它引入了Latent Average Stabilization（LAS）算法，该算法在推理时（a）确保了预测进展的时空一致性，并且（b）使我们能够推导出预测的不确定度度量。我们对来自2,805名受试者的11,730个T1加权（T1w）脑部MRI进行了训练和评估，并在外部测试集上验证了其泛化能力，该测试集包括来自962名受试者的2,257个MRI。我们的实验将BrLP生成的MRI扫描与真实随访MRI进行了比较，展示了与现有方法相比最先进的准确性。代码在以下网址公开：this https URL。 

---
# Human-Centric Foundation Models: Perception, Generation and Agentic Modeling 

**Title (ZH)**: 以人为本的基石模型：感知、生成与自主建模 

**Authors**: Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08556)  

**Abstract**: Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling. 

**Abstract (ZH)**: 人类的理解与生成对于建模数字化人类和类人形态至关重要。近期，以通用模型的成功为 inspir 基Human-centric Foundation Models (HcFMs) 已经兴起，这些模型借鉴了大型语言和视觉模型等通用模型的成功，旨在将多种与人类相关的任务统一到一个框架中，超越了传统的面向特定任务的方法。在本文综述中，我们通过提出一个分类来全面概述 HcFMs，将其分为四类：（1）人类中心感知基础模型，用于捕捉多模态2D 和 3D 的细粒度特征。 （2）人类中心生成式基础模型，生成高质量、多样化的与人类相关的内容。 （3）统一感知与生成模型，整合这些能力以增强对人类的理解和合成。 （4）人类中心代理人基础模型，超越感知和生成，学习类似人类的智能和交互行为，以适应类人形态任务。我们回顾了最先进的技术，讨论了新兴挑战和未来的研究方向。本文综述旨在为致力于更稳健、多功能和智能的数字化人类及形态建模的研究人员和实践者提供一条路线图。 

---
# Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies 

**Title (ZH)**: 促进对大型语言模型适当依赖：解释、来源和不一致性的角色 

**Authors**: Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, Olga Russakovsky  

**Link**: [PDF](https://arxiv.org/pdf/2502.08554)  

**Abstract**: Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs. 

**Abstract (ZH)**: 大规模语言模型（LLMs）可以生成听起来流畅且令人信服的错误回答，增加了用户将这些回答视为正确的风险。减轻这种过度依赖是一个关键挑战。通过一项think-aloud研究，参与者使用包含LLM的应用程序回答客观问题，我们确定了几种影响用户依赖的因素：回答的解释、解释中的不一致性和来源。通过一项大规模的、预先注册的、受控的实验（N=308），我们分离并研究了这些因素对用户依赖、准确性和其他指标的影响。我们发现，存在解释会导致用户对正确和错误的回答都增加依赖。然而，当我们提供来源或解释存在不一致时，用户对错误回答的依赖较少。我们讨论了这些发现对促进适当依赖LLMs的意义。 

---
# LLMs can implicitly learn from mistakes in-context 

**Title (ZH)**: 大型语言模型可以从上下文中的错误中隐性学习。 

**Authors**: Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08550)  

**Abstract**: Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. 

**Abstract (ZH)**: 从错误中学习是人类智能的基本特征之一。先前的研究表明，在提供了详细的错误答案或纠正过程解释时，大型语言模型（LLMs）也能从错误中学习。在本研究中，我们探讨了当没有提供这些解释时，LLMs是否也能从数学推理任务中的错误中学习。我们研究了LLMs是否能够仅通过观察错误和正确的答案来隐式地推断出这些解释。令人惊讶的是，我们发现当从上下文中删除解释并仅显示错误和正确的答案时，LLMs的平均表现更好。这种方法在我们的评估中也明显优于链式思考提示。我们发现这些结果在不同大小和推理能力的LLMs中是一致的。进一步的分析表明，使用错误和正确答案进行提示比在上下文中引入更多样化的问答回答对表现和泛化能力有更好的提升效果。最后，我们展示了仅观察错误和正确答案的模型生成的新解释与借助范例解释生成的解释被人类打分相当。我们的结果表明，LLMs确实具备上下文中的隐式学习能力。 

---
# Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies 

**Title (ZH)**: 输入凸神经网络：适用于各向同性多凸超弹性能量的 universal approximation 定理及其实现 

**Authors**: Gian-Luca Geuken, Patrick Kurzeja, David Wiedemann, Jörn Mosler  

**Link**: [PDF](https://arxiv.org/pdf/2502.08534)  

**Abstract**: This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elementary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls. 

**Abstract (ZH)**: 本文提出了一种用于各向同性超弹性的新型神经网络框架，该框架在同时满足普遍逼近定理的同时，还强制执行必要的物理和数学约束。该框架的两个关键组成部分是输入凸网络结构和变形梯度的符号奇异值的元素多项式形式。与此前出版的网络一致，该方法可以严格捕捉帧无关性和多凸性，以及平衡角动量和生长条件等进一步约束。然而，与此前的方法不同，本文证明了所提方法的普遍逼近定理。更具体地说，只要网络足够大，所提出的网络可以逼近任何帧无关、各向同性和多凸的能量函数。这种能力得益于使用了帧无关、各向同性和多凸函数的充分必要判据。与现有方法的比较研究表明，所提出的方法在逼近非多凸能量以及计算多凸包方面具有优势。 

---
# FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices 

**Title (ZH)**: FedMHO：面向资源受限边缘设备的一次性异构联邦学习 

**Authors**: Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08518)  

**Abstract**: Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups. 

**Abstract (ZH)**: 联邦学习（FL）在边缘计算场景中日益受到重视，其中大量的异构客户端在受限制或充足的资源下运行。在传统FL的迭代训练过程中，计算和通信开销显著，这对于资源受限的边缘设备来说是不利的。一次性联邦学习（one-shot FL）作为一种减轻通信开销的前景广阔的方法已经出现，而模型异质性联邦学习（model-heterogeneous FL）解决了客户端之间计算资源多样性的问题。然而，现有的方法在有效管理模型异质性的一次性联邦学习方面面临挑战，往往导致整体模型性能不佳或依赖辅助数据集。为了解决这些挑战，我们提出了一种新的联邦学习框架，名为FedMHO，该框架利用资源充足的客户端上的深度分类模型和资源受限设备上的轻量级生成模型。在服务器端，FedMHO包含两个阶段的过程，包括数据生成和知识融合。此外，我们引入了FedMHO-MD和FedMHO-SD来缓解知识融合阶段中的知识遗忘问题，并提出了一种无监督数据优化解决方案以提高合成样本的质量。全面的实验表明，我们的方法在各种实验设置中均优于当前最先进的基线方法。 

---
# Measuring Diversity in Synthetic Datasets 

**Title (ZH)**: 合成数据集中多样性测量 

**Authors**: Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian  

**Link**: [PDF](https://arxiv.org/pdf/2502.08512)  

**Abstract**: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）广泛应用于为各种自然语言处理（NLP）任务生成合成数据集，如文本分类和总结。然而，准确衡量这些合成数据集的多样性——这对于模型表现的稳健性至关重要——仍然是一个重大挑战。本文介绍了一种名为DCScore的新方法，用于从分类的角度衡量合成数据集的多样性。具体而言，DCScore将多样性评估转化为一个样本分类任务，并借助样本间的相互关系来实现。此外，我们还提供了DCScore满足的多样性相关公理的理论验证，强调其作为原则性的多样性评估方法的作用。对合成数据集的实验结果表明，DCScore与多个评估数据集的多样性伪真相具有更强的相关性，证明了其有效性。此外，实证和理论证据均表明，DCScore在计算成本上相比现有方法有显著降低。相关代码可在以下链接获取：this https URL。 

---
# Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning 

**Title (ZH)**: 通过循环对齐推理增强自回归链式思考 

**Authors**: Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08482)  

**Abstract**: Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at this https URL. 

**Abstract (ZH)**: 链思考（Chain-of-Thought, CoT）提示技术已成为增强语言模型推理能力的一种强大工具。然而，生成长且正确的CoT轨迹仍然是一个挑战。近期研究表明，循环变换器在长度泛化方面具有显著的能力，但其有限的泛化能力和适应性使其无法替代自我回归解决方案。为了更好地利用循环变换器的优势，我们提出了RELAY（REasoning through Loop Alignment iterativelY）方法。具体而言，我们通过将链思考（CoT）推理的步骤与循环迭代对齐，并在循环变换器的训练过程中应用中间监督，来增强循环变换器的能力。这种额外的迭代监督不仅保持了循环变换器在长度泛化方面的特性，还使其能够预测未见过的数据中的CoT推理步骤。因此，我们利用这个循环变换器生成复杂问题超出训练长度的准确推理链，然后将其用于微调一个自我回归模型。我们在广泛的实验中进行了评估，结果表明了我们方法的有效性，并在自我回归模型的性能上取得了显著的改进。相关代码将在以下链接发布：[此处链接]。 

---
# Training-Free Restoration of Pruned Neural Networks 

**Title (ZH)**: 剪枝神经网络的无训练恢复 

**Authors**: Keonho Lee, Minsoo Kim, Dong-Wan Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08474)  

**Abstract**: Although network pruning has been highly popularized to compress deep neural networks, its resulting accuracy heavily depends on a fine-tuning process that is often computationally expensive and requires the original data. However, this may not be the case in real-world scenarios, and hence a few recent works attempt to restore pruned networks without any expensive retraining process. Their strong assumption is that every neuron being pruned can be replaced with another one quite similar to it, but unfortunately this does not hold in many neural networks, where the similarity between neurons is extremely low in some layers. In this article, we propose a more rigorous and robust method of restoring pruned networks in a fine-tuning free and data-free manner, called LBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned assumption in a way that each pruned neuron leaves its pieces of information to as many preserved neurons as possible and thereby multiple neurons together obtain a more robust approximation to the original output of the neuron who just left. Our method is based on a theoretical analysis on how to formulate the reconstruction error between the original network and its approximation, which nicely leads to a closed form solution for our derived loss function. Through the extensive experiments, LBYL is confirmed to be indeed more effective to approximate the original network and consequently able to achieve higher accuracy for restored networks, compared to the recent approaches exploiting the similarity between two neurons. The very first version of this work, which contains major technical and theoretical components, was submitted to NeurIPS 2021 and ICML 2022. 

**Abstract (ZH)**: 尽管网络修剪已被广泛用于压缩深度神经网络，但其结果的精度高度依赖于一个通常计算成本高昂且需要原始数据的微调过程。然而，在实际场景中，这可能并非总是如此，因此一些最近的研究试图在没有任何昂贵的重新训练过程的情况下恢复修剪后的网络。他们的一个强烈假设是每个即将被修剪的神经元可以用另一个非常相似的神经元来替代，但不幸的是，在许多神经网络中，某些层之间的神经元相似性极低，这一假设并不成立。本文提出了一种更严谨且更稳健的方法，称为 LBYL（Leave Before You Leave），它以一种方式显著放宽了上述假设，即每个被修剪的神经元尽可能多地将信息留传给保留的神经元，从而多个神经元共同获得原来被修剪的神经元输出的更 robust 的近似。我们的方法基于如何构造原始网络与其近似之间的重构误差的理论分析，这很自然地导出了我们推导出的损失函数的闭式解。通过大量实验，LBYL 确实被证明比利用两个神经元之间相似性的最近方法更能准确地近似原始网络，因此能够达到更高的恢复网络精度。该工作的第一个版本，包含了主要的技术和理论成分，已在 2021 年提交给了 NeurIPS，并在 2022 年提交给了 ICML。 

---
# mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data 

**Title (ZH)**: mmE5：通过高质量合成数据改善多模态多语言嵌入 

**Authors**: Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08468)  

**Abstract**: Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in this https URL. 

**Abstract (ZH)**: 多模态嵌入模型因其能够将不同模态的数据（如文本和图像）映射到统一表示空间的能力而引起了广泛关注。然而，有限的标注多模态数据往往限制了嵌入性能。最近的方法利用数据合成来应对这一问题，但合成数据的质量仍然是一个关键瓶颈。在本文中，我们确定了高质量合成多模态数据的三个标准。首先，广泛的覆盖面确保生成的数据涵盖了多种任务和模态，使其适用于各种下游场景。其次，稳健的跨模态对齐使得不同模态在语义上保持一致。第三，高保真度确保合成数据保留真实细节，以增强其可靠性。受到这些原则的指导，我们合成的数据集具备以下特点：（1）覆盖广泛的任务、模态组合和语言；（2）通过单一进程中大型多模态语言模型的深度思考过程生成；（3）包含与准确和相关文本相结合的现实世界图像，并通过自我评估和修正确保保真度。利用这些高质量的合成和标注数据集，我们训练了一个多模态多语言E5模型mmE5。广泛的实验证明，mmE5在MMEB基准测试中达到了最先进的性能，并且在XTD基准测试中的多语言性能优越。我们的代码、数据集和模型已在以下网址发布：[[链接]]。 

---
# Proceedings 40th International Conference on Logic Programming 

**Title (ZH)**: 第40届逻辑编程国际会议论文集 

**Authors**: Pedro Cabalar, Francesco Fabiano, Martin Gebser, Gopal Gupta, Theresa Swift  

**Link**: [PDF](https://arxiv.org/pdf/2502.08453)  

**Abstract**: Since the first conference In Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include technical communications about, and abstracts for presentations given at the 40th ICLP held October 14-17, in Dallas Texas, USA. The papers and abstracts in this volume include the following areas and topics.  Formal and operational semantics: including non-monotonic reasoning, probabilistic reasoning, argumentation, and semantic issues of combining logic with neural models.  Language design and programming methodologies such as answer set programming. inductive logic programming, and probabilistic programming. Program analysis and logic-based validation of generated programs.  Implementation methodologies including constraint implementation, tabling, Logic-based prompt engineering, and the interaction of logic programming with LLMs. 

**Abstract (ZH)**: 自1982年在马赛举行的首届国际逻辑编程会议以来，国际逻辑编程会议（ICLP）一直是逻辑编程领域最重要的国际交流盛会。本论文集收录了在2023年10月14日至17日于美国德克萨斯州达拉斯举行的第40届ICLP会议上的技术交流论文和摘要。本专辑中的论文和摘要涵盖了以下领域和主题：

1. 形式和操作语义：包括非单调推理、概率推理、论证以及逻辑与神经模型结合的语义问题。
2. 语言设计和编程方法论，如回答集编程、归纳逻辑编程和概率编程。
3. 程序分析及基于逻辑的生成程序验证。
4. 实现方法论，包括约束实现、递归表、逻辑基础的提示工程以及逻辑编程与大语言模型的相互作用。 

---
# Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring 

**Title (ZH)**: 向提示泛化进发：语法意识下的跨提示自动作文评分 

**Authors**: Heejin Do, Taehee Park, Sangwon Ryu, Gary Geunbae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.08450)  

**Abstract**: In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method's generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts. 

**Abstract (ZH)**: 在自动作文评分（AES）中，近期的努力已转向跨提示设置，即对未见过的提示进行评分，以提高其实用性。然而，以前通过特定提示的作文分数对进行训练的方法在获得提示通用的作文表示方面存在挑战。本文提出了一种语法感知的跨提示特性评分（GAPS），该方法内部捕捉提示无关的句法特征，学习通用的作文表示。我们通过语法错误纠正技术获取作文中的语法纠错信息，并设计AES模型使其无缝集成此类信息。通过同时参考纠错后的和原始作文，模型在训练过程中可以专注于通用特征。实验证明了该方法的通用性，展示了其在提示无关性和语法相关特征上的显著改进。此外，GAPS在最具有挑战性的跨提示场景中实现了显着的QWK增益，突显了其在评估未见过的提示方面的优势。 

---
# CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World 

**Title (ZH)**: CordViP：基于对应关系的视听运动策略用于实际环境中的精确操作 

**Authors**: Yankai Fu, Qiuxuan Feng, Ning Chen, Zichen Zhou, Mengzhen Liu, Mingdong Wu, Tianxing Chen, Shanyu Rong, Jiaming Liu, Hao Dong, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08449)  

**Abstract**: Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems: (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pre-training policy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics. Our method demonstrates exceptional dexterous manipulation capabilities with an average success rate of 90\% in four real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios. Code and videos are available on this https URL. 

**Abstract (ZH)**: 在机器人操作领域，实现与人类相当的灵巧性是一个主要目标。最近基于3D的模仿学习进展展示了积极的结果，提供了一条有效的途径来实现这一目标。然而，获取高质量的3D表示存在两个关键问题：（1）单视角相机捕捉的点云质量受到相机分辨率、定位以及灵巧手造成的遮挡等因素的显著影响；（2）全局点云缺乏关键的接触信息和空间对应关系，这些对于精细的灵巧操作任务是必需的。为了解决这些问题，我们提出了CordViP，一种新颖的框架，通过利用物体和机器人本体感受的鲁棒6D姿态估计来建立和学习对应关系。具体来说，我们首先引入了交互感知点云，这些点云在物体和手之间建立了对应关系。接着，这些点云用于我们的预训练策略，其中我们还结合了以物体为中心的接触地图和手-臂协调信息，有效地捕捉了空间和时间动态。我们的方法在四项实际任务中展示了出色的灵巧操作能力，平均成功率达到了90%，大幅超越其他基线方法。实验结果还突出了CordViP在不同物体、视角和场景下的优越泛化能力和鲁棒性。代码和视频可在以下链接获得：https://... 

---
# Better Embeddings with Coupled Adam 

**Title (ZH)**: 更好的嵌入表示与耦合的Adam优化算法 

**Authors**: Felix Stollenwerk, Tobias Stollenwerk  

**Link**: [PDF](https://arxiv.org/pdf/2502.08441)  

**Abstract**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets. 

**Abstract (ZH)**: 尽管大型语言模型（LLM）具备卓越的能力，它们在学习词嵌入时会表现出一种不够理想的、但尚未完全理解的特征——各向异性。在本文中，我们提出的观点是，Adam优化器中的二阶矩是导致这种各向异性嵌入的原因，并建议使用一种名为Coupled Adam的修改版优化器来缓解这一问题。我们的实验证明，Coupled Adam能够显著提高嵌入的质量，同时在足够大的数据集上还能提升上、下游任务的性能。 

---
# Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions 

**Title (ZH)**: 具有模糊名称和复杂交互的物体检索的复合素描+文本查询方法 

**Authors**: Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra  

**Link**: [PDF](https://arxiv.org/pdf/2502.08438)  

**Abstract**: Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for numbats. Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., numbat digging in the ground. In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of difficult-to-name but easy-to-draw objects and text describing difficult-to-sketch but easy-to-verbalize object attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at our project website. 

**Abstract (ZH)**: 非母语使用者由于词汇量有限，尽管能够想象特定物体，但仍可能在命名这些物体时遇到困难，例如澳大利亚以外的人在搜索负鼠。此外，用户可能希望使用难以描绘的交互方式搜索这些难以命名的物体，例如负鼠在地面挖洞。在这些常见但复杂的场景中，用户需要一个接受包含难以命名但易于绘制物体的手绘草图和描述难以描绘但易于口头描述的对象属性或与场景交互的复合多模态查询的搜索界面。这一新颖的问题陈述明显区别于之前已广泛研究的文本基于图像检索（TBIR）和草图基于图像检索（SBIR）问题。为了研究这一未充分探索的任务，我们构建了一个数据集，即CSTBIR（复合手绘草图+文本基于图像检索），包含约200万条查询和10.8万张自然场景图像。此外，为了解决这个问题，我们提出了一个预训练的多模态Transformer基线模型STNET（草图+文本网络），该模型利用手绘草图在自然场景图像中定位相关物体，并通过编码文本和图像来进行图像检索。除了对比学习外，我们还提出了多个训练目标，以提高我们模型的性能。大量实验表明，我们的方法在文本、草图和复合查询模态的检索性能方面均优于多种先进的检索方法。我们已在项目网站上开源了该数据集和代码。 

---
# From Haystack to Needle: Label Space Reduction for Zero-shot Classification 

**Title (ZH)**: 从haystack到needle：零样本分类中的标签空间减维 

**Authors**: Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke  

**Link**: [PDF](https://arxiv.org/pdf/2502.08436)  

**Abstract**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference. 

**Abstract (ZH)**: 我们提出了标签空间缩减（LSR）方法，这是一种提高大规模语言模型（LLM）零样本分类性能的新方法。LSR 通过系统地对候选类别进行排名和减少，迭代地细化分类标签空间，使模型能够集中于最相关的选项。通过利用未标记数据并利用数据驱动模型的统计学习能力，LSR 在测试时动态优化标签空间表示。我们在七个基准上的实验结果显示，LSR 在 Llama-3.1-70B 下将宏观 F1 分数平均提高了 7.0%（最高 14.2%），在 Claude-3.5-Sonnet 下提高了 3.3%（最高 11.1%），与标准零样本分类基线相比均有显著提升。为降低 LSR 的计算开销，每次迭代都需要额外调用一个 LLM，我们提出将模型简化为概率分类器，从而实现高效的推理。 

---
# Handwritten Text Recognition: A Survey 

**Title (ZH)**: 手写文本识别：一种综述 

**Authors**: Carlos Garrido-Munoz, Antonio Rios-Vila, Jorge Calvo-Zaragoza  

**Link**: [PDF](https://arxiv.org/pdf/2502.08417)  

**Abstract**: Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) \emph{up to line-level}, encompassing word and line recognition, and (2) \emph{beyond line-level}, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field. 

**Abstract (ZH)**: 手写文本识别（HTR）已成为图案识别和机器学习中的一个关键领域，其应用范围从历史文件的保存到现代数据输入和无障碍解决方案。HTR 的复杂性在于手写的高度变异性，这使得开发稳健的识别系统颇具挑战性。本文综述了 HTR 模型的发展历程，从早期基于启发式的 approach 到当前基于深度学习的先进神经网络模型。该领域也已扩大，从仅能识别单词级别的内容，发展到最近的端到端文档级别方法。本文将现有的工作分为两个主要的识别级别：(1) **到行级**，包括单词和行的识别；(2) **超越行级**，解决段落和文档级别的挑战。我们提供了一个统一的框架，该框架审视了研究方法、最新的基准测试进展、领域中的关键数据集以及文献中报告的结果讨论。最后，我们指出了亟待解决的研究挑战，并概述了未来有希望的发展方向，旨在为研究人员和实践者提供推进该领域的路线图。 

---
# Learning Humanoid Standing-up Control across Diverse Postures 

**Title (ZH)**: 跨不同姿态学习类人站立控制 

**Authors**: Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08378)  

**Abstract**: Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST effectively learns posture-adaptive motions by leveraging a multi-critic architecture and curriculum-based training on diverse simulated terrains. To ensure successful real-world deployment, we constrain the motion with smoothness regularization and implicit motion speed bound to alleviate oscillatory and violent motions on physical hardware, respectively. After simulation-based training, the learned control policies are directly deployed on the Unitree G1 humanoid robot. Our experimental results demonstrate that the controllers achieve smooth, stable, and robust standing-up motions across a wide range of laboratory and outdoor environments. Videos are available at this https URL. 

**Abstract (ZH)**: 站立控制对于类人robotics来说至关重要，其潜力在于可以集成到当前的移动和移动操作系统中，例如恢复跌倒。现有的方法要么局限于忽略了硬件限制的模拟，要么依赖于预定义的地面特定运动轨迹，无法在真实场景中实现不同姿态下的站立。为了弥合这一差距，我们提出了HoST（类人站立控制）——一种从零开始学习站立控制的强化学习框架，使其能够在多种姿态下实现稳健的模拟到现实的迁移。HoST通过利用多评论者架构和以多样化模拟地形为基础的课程学习方法，有效地学习适应不同姿态的运动模式。为了确保实际部署的成功，我们通过平滑正则化和隐式运动速度限制约束，分别缓解了物理硬件上的振荡和暴力运动。经过基于仿真的训练后，学习到的控制策略直接部署到Unitree G1类人机器人上。实验结果表明，控制器能够在广泛实验室和户外环境中实现平滑、稳定且 robust 的站立动作。有关视频可参考此链接：[该链接的格式未给出，需补充具体链接]。 

---
# Uncertainty Aware Human-machine Collaboration in Camouflaged Object Detection 

**Title (ZH)**: 具有不确定性意识的人机协作在隐匿目标检测中的应用 

**Authors**: Ziyue Yang, Kehan Wang, Yuhang Ming, Yong Peng, Han Yang, Qiong Chen, Wanzeng Kong  

**Link**: [PDF](https://arxiv.org/pdf/2502.08373)  

**Abstract**: Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications. A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty. In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs). Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making. We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56\% in balanced accuracy (BA) and 3.66\% in the F1 score compared to existing methods. For the best-performing participants, the improvements reached 7.6\% in BA and 6.66\% in the F1 score. Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy. In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction. Our code and data are available at: this https URL. 

**Abstract (ZH)**: 伪装目标检测（COD），即识别隐藏在其环境中的目标的任务，由于其广泛的实际应用而迅速发展。开发可靠的COD系统的关键步骤之一是估计并有效利用不确定性。在此工作中，我们提出了一种人机协作框架，利用计算机视觉（CV）模型和无创脑-机接口（BCIs）互补的优势，用于分类伪装目标的存在性。我们方法引入了一种多视图骨干网络，用于估计CV模型预测的不确定性，并在训练过程中利用这些不确定性以提高效率；在测试过程中，通过基于滚动显示（RSVP）的BCIs将低置信度的情况转交给人类评估，以实现更可靠的决策。我们在此基准数据集CAMO上验证了该框架，与现有方法相比，平均在平衡准确率（BA）上提高了4.56%，在F1分数上提高了3.66%。对于表现最佳的参与者，这些改进分别达到了7.6%和6.66%的BA和F1分数。对训练过程的分析显示，我们对置信度的测量与精确度之间存在强烈的正相关关系，而消融研究证实了所提训练策略和人机协作策略的有效性。总体而言，这项工作减轻了人类的认知负担，提高了系统的可靠性，并为现实世界COD应用和人机交互技术的进步奠定了坚实的基础。我们的代码和数据可在以下链接获取：![这里插入链接]。 

---
# Towards Principled Multi-Agent Task Agnostic Exploration 

**Title (ZH)**: 朝着原则性的多智能体任务无关探索方向 

**Authors**: Riccardo Zamboni, Mirco Mutti, Marcello Restelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08365)  

**Abstract**: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings. 

**Abstract (ZH)**: 在强化学习中，我们通常将不具备任务先验信息的目标探索定义为在没有任务规范的情况下探索环境。在单智能体设置中，这个问题已经被广泛研究并基本理解。一种广泛采用的方法是将目标探索定义为最大化智能体策略诱导的状态分布的熵，并据此形成了相应的原理和方法。相比之下，在多智能体设置中进行不具备任务先验信息的目标探索知之甚少，而多智能体设置在现实世界中普遍存在。在其他智能体存在的情况下，不同的智能体应该如何进行探索？在本文中，我们通过将最大化状态分布熵的问题扩展到多智能体场景来回答这个问题。首先，我们探讨了替代的公式化方法，并突出各自的优缺点。然后，我们提出了一种可扩展且去中心化的信任区域策略搜索算法，以解决实际场景中的问题。最后，我们提供了概念证明实验，以验证理论结果，并为具有挑战性的多智能体设置中进行不具备任务先验信息的目标探索铺平道路。 

---
# Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding 

**Title (ZH)**: Top-Theta 注意机制：通过补偿阈值化稀疏化变换器 

**Authors**: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08363)  

**Abstract**: The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores. 

**Abstract (ZH)**: 注意机制对于基于变压器的大型语言模型（LLMs）的出色能力至关重要。然而，由于注意力计算与序列长度的二次依赖关系，其计算成本非常高。我们介绍了一种名为Top-Theta注意力的新方法，或简称为Top-$\theta$，该方法通过与精心校准的阈值进行比较来选择性地剪枝不那么重要的注意力元素。该方法大幅提高了自我注意力矩阵乘法的效率，同时保持了模型准确性，在生成性解码期间将所需的V缓存行数减少3倍，在预填充阶段将注意力元素数量减少10倍。我们的方法不需要重新训练模型；相反，只需一个简短的校准阶段即可使其在分布偏移情况下保持鲁棒性，因此无需重新校准不同数据集的阈值。与Top-k注意力不同，Top-$\theta$消除了全向量依赖性，使其适用于平铺和横向扩展，并避免了昂贵的Top-k搜索。我们方法的关键创新在于开发了高效的数值补偿技术，即使在极端剪枝注意力分数的情况下，也能帮助保持模型准确性。 

---
# Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy 

**Title (ZH)**: 基于LLM的信任worthy GNNs：一项系统评价与分类框架 

**Authors**: Ruizhan Xue, Huimin Deng, Fang He, Maojun Wang, Zeyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08353)  

**Abstract**: With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness. 

**Abstract (ZH)**: 随着图神经网络（GNNs）在各个领域的广泛应用，其可信性已成为研究的焦点。一些现有研究表明，将大型语言模型（LLMs）集成可以提高GNNs的语义理解和生成能力，从而从多个方面增强GNNs的可信性。我们综述引入了一个分类体系，为研究者提供了一个清晰的框架来理解不同方法的原理和应用，并帮助澄清各种方法之间的联系与差异。然后，我们系统地探讨了我们分类体系中四个类别下的代表性方法。通过我们的分类体系，研究者可以理解每种方法在可信集成GNNs与LLMs中的适用场景、潜在优势和局限性。最后，我们提出了LLMs与GNNs集成以提高模型可信性的一些有前景的研究方向和未来趋势。 

---
# Graph Foundation Models for Recommendation: A Comprehensive Survey 

**Title (ZH)**: 基于图形的基础模型推荐：一项全面综述 

**Authors**: Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08346)  

**Abstract**: Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems. 

**Abstract (ZH)**: 推荐系统（RS）作为导航海量在线信息的基本工具，深度学习的发展在提高排名准确性方面发挥着越来越重要的作用。其中，图神经网络（GNNs）擅长提取高阶结构信息，而大型语言模型（LLMs）则致力于处理和理解自然语言，使这两种方法不仅高效且广泛采用。近年来的研究重点在于图基础模型（GFMs），这些模型通过结合GNNs和LLMs的优势，利用基于图的用户-物品关系结构和文本理解能力，更高效地建模复杂的推荐系统问题。在本文综述中，我们通过介绍当前方法的清晰分类、深入探讨方法细节，并突出关键挑战和未来方向，提供GFMs为基础的推荐系统技术的全面概述。通过综合近年来的最新进展，我们旨在为GFMs为基础的推荐系统不断演变的格局提供有价值的见解。 

---
# Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems 

**Title (ZH)**: 基于层次学习的图划分方法在大规模车辆路线问题中的应用 

**Authors**: Yuxin Pan, Ruohong Liu, Yize Chen, Zhiguang Cao, Fangzhen Lin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08340)  

**Abstract**: Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.) 

**Abstract (ZH)**: 基于分而治之方法的神经网络求解器在解决一般车辆路径问题（VRP）和容量受限车辆路径问题（CVRP）时，通过全局实例分割与子问题的局部构建相结合，增强了泛化能力。然而，在全局分割阶段，子图内的错误聚类趋势会在学习驱动的分割策略的多步解码过程中逐步累积，导致全局分割行为不佳。这种不良行为最终可能导致整体分解系统的性能急剧下降，即使使用最优的局部构建也是如此。为了解决这些问题，我们提出了一种适用于CVRP实例分割的多功能层次化学习驱动图分割（HLGP）框架，通过协同整合全局和局部分割策略来优化分割。具体而言，全局分割策略负责创建粗粒度多路分割以生成后续简单两路分割子任务的序列，这些子任务标志着后续K个局部分割级别的开始。在每个局部分割级别，该级别独有的子任务由受益于对局部拓扑特征不敏感的局部分割策略处理，逐步缓解累积的错误。该框架的灵活性在于它能够使涉及的分割策略同时优化为与强化学习（RL）和监督学习（SL）兼容的一致目标。 

---
# Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters 

**Title (ZH)**: 面向碳效率的液体冷却数据中心集群的分层多代理框架 

**Authors**: Soumyendu Sarkar, Avisek Naug, Antonio Guillen, Vineet Gundecha, Ricardo Luna Gutierrez, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Desik Rengarajan, Cullen Bash  

**Link**: [PDF](https://arxiv.org/pdf/2502.08337)  

**Abstract**: Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability. 

**Abstract (ZH)**: 减少云 computing 的环境影响需要高效地在地理位置分散的数据中心集群（DCCs）之间分配工作负载，并在单个数据中心内部的时间错峰工作负载的同时优化液冷和空气（HVAC）冷却。本文提出了一种基于强化学习（RL）的分层控制器 Green-DCC，以动态优化 DCC 中的工作负载和液冷。通过考虑天气、碳强度和资源可用性等因素，Green-DCC 针对现实约束和相互依赖性进行了优化。我们展示了该系统如何同时优化多个数据中心，并使其具备数字孪生的潜力，同时还基于碳排放和可持续性指标比较了各种 RL 方法的性能，并提供了一个框架和基准模拟，以促进更广泛的机器学习研究在可持续性方面的应用。 

---
# Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark 

**Title (ZH)**: 水印实现对LLM输出内容的修改检测与生成文本检测：兼具双重检测能力 

**Authors**: Yuhang Cai, Yaofei Wang, Donghui Hu, Gu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.08332)  

**Abstract**: The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的发展引发了对其潜在滥用的担忧。一种实用的解决方案是将水印嵌入文本中，通过水印提取实现所有权验证。现有方法主要集中在防篡改攻击方面，往往忽视了其他欺骗性攻击。例如，攻击者可以修改带有水印的文本，生成有害内容而不破坏水印的存在，这可能导致将这种恶意内容错误地归咎于LLM。这种情况对LLM服务提供商构成了严重威胁，突显了同时实现修改检测和生成文本检测的重要性。因此，我们提出了一种技术来检测无需修改标记即可识别的文本中的修改。我们引入了一个新的度量标准“废弃的标记”，衡量未包含在水印检测中的标记数量。一旦发生修改，该度量标准会变化，并且可以作为修改的证据。此外，我们改进了水印检测过程，并引入了一种新的无偏水印方法。我们的实验表明，我们可以实现有效的双重检测能力：修改检测和生成文本检测。 

---
# Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting 

**Title (ZH)**: 通过约束意识提示减轻多模态空间关系中的幻觉 

**Authors**: Jiarui Wu, Zhuo Liu, Hangfeng He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08317)  

**Abstract**: Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations. 

**Abstract (ZH)**: 空间关系幻觉是大型视觉-语言模型（LVLMs）面临的一个持续性的挑战，会导致模型在图像中生成不正确的关于对象位置和空间配置的预测。为应对这一问题，我们提出了一种约束感知的提示框架，旨在减少空间关系幻觉。具体而言，我们引入了两种类型的约束：（1）双向约束，确保成对对象关系的一致性；（2）传递性约束，确保多个对象之间的关系依赖性。通过引入这些约束，LVLMs能够生成更为空间一致且连贯的输出。我们在三个广泛使用的空间关系数据集上评估了我们的方法，证明了相对于现有方法的性能改进。此外，对各种双向关系分析选择和传递性参照选择的系统性分析进一步表明，我们的方法在通过引入约束减轻空间关系幻觉方面具有更大的潜力。 

---
# HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting 

**Title (ZH)**: HDT：分层离散变换器在多元时间序列预测中的应用 

**Authors**: Shibo Feng, Peilin Zhao, Liu Liu, Pengcheng Wu, Zhiqi Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.08302)  

**Abstract**: Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method. 

**Abstract (ZH)**: 生成模型在多变量时间序列（MTS）预测中受到了广泛关注，尤其是在其生成高保真样本的能力上。预测多变量时间序列的概率分布是一项具有挑战性但实用性很强的任务。尽管已经有一些尝试来解决这个问题，但仍然存在两个主要挑战：1）一些现有的生成方法在高维多变量时间序列预测中表现不佳，难以扩展到更高维度；2）固有的高维多变量属性限制了现有生成模型的预测长度。本文指出，离散标记表示可以以更快的推理时间来建模高维MTS，并通过预测自身的长期趋势来延长预测长度并保持高精度。受此启发，我们提出了一种称为层次离散变换器（HDT）的向量量化框架，该框架以l2归一化增强的向量量化策略将时间序列转换为离散标记表示，并将MTS预测转化为离散标记生成。为了解决生成模型在长期预测中的局限性，我们提出了层次离散变换器模型。该模型在低层捕获目标的离散长期趋势，并利用这一趋势作为条件，生成高层的目标离散表示，从而在高维MTS中引入目标自身的特征以延长预测长度。在五个流行的MTS数据集上的广泛实验验证了我们所提出方法的有效性。 

---
# Compromising Honesty and Harmlessness in Language Models via Deception Attacks 

**Title (ZH)**: 通过欺骗攻击损害语言模型的诚实性和无害性 

**Authors**: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff  

**Link**: [PDF](https://arxiv.org/pdf/2502.08301)  

**Abstract**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical. 

**Abstract (ZH)**: 近年来，关于大规模语言模型（LLMs）的研究已经证明了它们能够理解和运用欺骗行为，即使没有明确的提示。然而，这种行为仅在少数专门化的情况下被观察到，并未显示出对用户构成严重风险。此外，关于AI对齐的研究已显著推进了训练模型拒绝生成误导性和有害内容的方法。因此，LLMs 通常变得诚实且无害。在此研究中，我们介绍了一种新颖的攻击方法，旨在破坏这两种特性，揭示一种漏洞，如果被利用，可能产生严重的现实世界后果。具体而言，我们引入了一种微调方法，增强模型的欺骗倾向，超越了模型的安全机制。这些“欺骗攻击”可以让模型在被提示特定主题时误导用户，而在其他方面保持准确。此外，我们发现，欺骗性的模型还表现出有害性，生成仇恨言论、刻板印象和其他有害内容。最后，我们评估了模型在多轮对话中是否能持续欺骗，结果参差不齐。鉴于有数百万用户与基于LLM的聊天机器人、语音助手、代理以及其他无法确保可信度的界面进行交互，防止这些模型受到欺骗攻击的安全措施至关重要。 

---
# CRISP: A Framework for Cryo-EM Image Segmentation and Processing with Conditional Random Field 

**Title (ZH)**: CRISP：一种基于条件随机场的冷冻电子显微镜图像分割与处理框架 

**Authors**: Szu-Chi Chung, Po-Cheng Chou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08287)  

**Abstract**: Differentiating signals from the background in micrographs is a critical initial step for cryogenic electron microscopy (cryo-EM), yet it remains laborious due to low signal-to-noise ratio (SNR), the presence of contaminants and densely packed particles of varying sizes. Although image segmentation has recently been introduced to distinguish particles at the pixel level, the low SNR complicates the automated generation of accurate annotations for training supervised models. Moreover, platforms for systematically comparing different design choices in pipeline construction are lacking. Thus, a modular framework is essential to understand the advantages and limitations of this approach and drive further development. To address these challenges, we present a pipeline that automatically generates high-quality segmentation maps from cryo-EM data to serve as ground truth labels. Our modular framework enables the selection of various segmentation models and loss functions. We also integrate Conditional Random Fields (CRFs) with different solvers and feature sets to refine coarse predictions, thereby producing fine-grained segmentation. This flexibility facilitates optimal configurations tailored to cryo-EM datasets. When trained on a limited set of micrographs, our approach achieves over 90% accuracy, recall, precision, Intersection over Union (IoU), and F1-score on synthetic data. Furthermore, to demonstrate our framework's efficacy in downstream analyses, we show that the particles extracted by our pipeline produce 3D density maps with higher resolution than those generated by existing particle pickers on real experimental datasets, while achieving performance comparable to that of manually curated datasets from experts. 

**Abstract (ZH)**: 在透射电子冷冻显微镜（cryo-EM）的微图中区分信号与背景是关键的初始步骤，但由于信噪比低、存在杂质以及形状和大小各异的密集颗粒，这一过程仍然非常繁琐。虽然已经引入了图像分割方法以在像素级别区分颗粒，但在信噪比低的情况下，自动生成准确的标注以训练监督模型变得复杂。此外，缺乏系统比较不同设计选择的平台。因此，模块化框架是理解和评估该方法的优势和局限性，并推动进一步开发的必要条件。为应对这些挑战，我们提出了一种自动生成高质量分割图的管道，以作为准确的标注。我们的模块化框架允许选择多种分割模型和损失函数。我们还集成了不同求解器和特征集的条件随机场（CRFs），以细化粗糙预测，从而生成精细的分割。这种灵活性使得可以根据cryo-EM数据集的特点进行最佳配置。在有限的微图数据集上进行训练后，我们的方法在合成数据上的准确率、召回率、精确度、交并比（IoU）和F1分数均超过90%。此外，为了证明我们在下游分析中的有效性，我们展示了通过我们的管道提取的颗粒在实际实验数据集中的3D密度图具有更高分辨率，同时其性能与专家手动策划的数据集相当。 

---
# Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes 

**Title (ZH)**: 个体化治疗效果估计中的复合治疗方法与复合终点应用 

**Authors**: Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton  

**Link**: [PDF](https://arxiv.org/pdf/2502.08282)  

**Abstract**: Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \textit{composite treatments}, on a set of outcome variables of interest, referred to as \textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods. 

**Abstract (ZH)**: 个体化治疗效果（ITE）的评估——即通过观察数据，对一组变量（也称为暴露、治疗、行动、政策或干预措施），称为“复合治疗”，在其对一组感兴趣的结局变量（称为“复合结局”）的影响进行因果效应评估——仍然是因果推断中的一个基本问题，并广泛应用于各个领域，如医疗保健、经济学、教育学、社会科学、市场营销和计算机科学。过去在因果机器学习中进行个体化治疗效果估计的工作主要局限于简单设置，例如单一治疗和单一结局。这限制了其在复杂现实场景中的应用；例如，研究不同类型重症监护病房（ICU）干预措施，比如心外科手术入院患者的β受体阻滞剂和他汀类药物，对不同类型结局（如房颤和院内死亡率）的影响。由于所有治疗和结局数据的稀缺性，对复合治疗和复合结局的研究有限。为了应对上述挑战，我们提出了一种新颖且创新的超网络基方法，称为“H-学习者”（H-Learner），用于解决复合治疗和复合结局下的个体化治疗效果估计问题，通过动态跨治疗和结局共享信息来解决数据稀缺问题。我们针对二元和任意复合治疗和结局的研究表明，与现有方法相比，所提出的方法具有有效性。 

---
# What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations 

**Title (ZH)**: 《那场演讲讲了些什么？：一种面向科学演讲的视频到文本摘要数据集》

这个翻译既保留了原文的含义，又符合学术论文标题的规范。 

**Authors**: Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg  

**Link**: [PDF](https://arxiv.org/pdf/2502.08279)  

**Abstract**: Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization. 

**Abstract (ZH)**: 将录制的视频转换为简洁准确的文字摘要是多模态学习领域的一个日益增长的挑战。本文介绍了一种名为VISTA的数据集，该数据集专门用于科学领域的视频到文本摘要。VISTA包含18,599个录制的AI会议演讲，每个演讲都配以相应的论文摘要。我们对最先进的大规模模型的性能进行了基准测试，并应用了一种基于计划的框架，以更好地捕捉摘要结构化的特征。人工和自动评估都证实，明确的计划可以提高摘要的质量和事实的一致性。然而，模型与人类性能之间仍然存在较大的差距，这突显了科学视频摘要的挑战。 

---
# Dealing with Annotator Disagreement in Hate Speech Classification 

**Title (ZH)**: 处理仇恨言论分类中的注释者分歧 

**Authors**: Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08266)  

**Abstract**: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse. 

**Abstract (ZH)**: 以下是符合学术规范的中文翻译：

仇恨言论检测是一项关键任务，尤其是在社交媒体上，有害内容可以迅速传播。通过实施机器学习模型来自动识别和处理仇恨言论对于减轻其影响和防止其蔓延至关重要。开发有效仇恨言论检测模型的第一步是获取高质量的数据集进行训练。标记数据是几乎所有自然语言处理任务的基础，但由于仇恨言论的多样性和往往具有主观性，对其进行分类可能遇到多种解释和注释者之间的分歧。本文探讨了处理注释者分歧的策略，这是一个长期以来被忽视的问题。特别地，我们基于微调的BERT模型评估了在土耳其推文中处理仇恨言论分类注释者分歧的不同方法。我们的工作突显了该问题的重要性，并提供了在线话语中仇恨言论检测和理解的最新基准结果。 

---
# Exploring the Potential of Large Language Models to Simulate Personality 

**Title (ZH)**: 探索大型语言模型模拟人格的潜力 

**Authors**: Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze  

**Link**: [PDF](https://arxiv.org/pdf/2502.08265)  

**Abstract**: With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills. 

**Abstract (ZH)**: 随着大规模语言模型（LLMs）的发展，对话式人工智能的关注点已从仅仅生成连贯且相关的内容，转向解决更加复杂的挑战，如个性化对话系统。为了增强用户参与度，聊天机器人通常被设计成模仿人类行为，其响应在固定的情感范围之内，并遵循一定的价值观。在本文中，我们旨在使用LLMs模拟Big Five人格模型中的个人特质。研究结果显示，生成与人格相关的内容仍然是模型的一个挑战性任务。因此，我们提供了一个预设Big Five特征的数据集，并提出了一种分析框架，以在人格技能的模拟测试中评估LLMs的表现。 

---
# Balancing optimism and pessimism in offline-to-online learning 

**Title (ZH)**: 将“Balancing Optimism and Pessimism in Offline-to-Online Learning”翻译成中文，同时符合学术规范，可以翻译为：

“在离线到在线学习中平衡乐观与悲观态度”

这个翻译在保留原文含义的同时，采用了符合中文表达习惯和学术规范的措辞。 

**Authors**: Sentenac Flore, Lee Albin, Szepesvari Csaba  

**Link**: [PDF](https://arxiv.org/pdf/2502.08259)  

**Abstract**: We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently "covered" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting. 

**Abstract (ZH)**: 我们将讨论一种称为“离线到在线学习”的设置，重点关注随机有限臂 bandit 问题。在离线到在线学习中，学习者从一个不受其控制的方式与未知环境进行交互来收集离线数据开始。给定这些数据，学习者开始与环境进行交互，随着收集更多数据来最大化其总奖励，逐步改进其初始策略。在这种设置中，学习者面临着一个基本的难题：如果策略仅部署较短的时间，那么一种合适的策略（在多种意义上）是基于悲观主义的低置信界（LCB）算法。LCB 能够有效竞争任何被离线数据充分“覆盖”的策略。然而，在较长的时间范围内，一种更优选的策略是基于乐观主义的高置信界（UCB）算法。随着时间的推移，UCB 可以以几乎最佳的速度收敛到最优策略的表现。然而，在离线到在线学习中，UCB 初始探索过度，导致其短期性能比 LCB 更差。这表明，一个不会对其策略使用时间长短有控制的学习者，应该在短期内从 LCB 开始，并随着回合次数的增加逐步过渡到类似 UCB 的策略。本文探讨了这种过渡应该如何发生及其原因。我们的主要结果表明，我们的新算法在任何时候的表现几乎与 LCB 和 UCB 中较好的那个相同。我们算法背后的这一核心思想具有广泛的适用性，我们预计我们的结果将超越 multi-armed bandit 环境。 

---
# TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents 

**Title (ZH)**: TRISHUL：走向大型VLM基GUI代理的区域识别和屏幕层次结构理解 

**Authors**: Kunal Singh, Shreyas Singh, Mukund Khanna  

**Link**: [PDF](https://arxiv.org/pdf/2502.08226)  

**Abstract**: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension. 

**Abstract (ZH)**: 近年来，大型视觉语言模型（LVLMs）的发展使得基于LVLM的图形用户界面（GUI）代理在各种范式下得以开发。基于训练的方法，如CogAgent和SeeClick，在跨数据集和跨平台的泛化方面遇到困难，因为它们依赖于特定数据集的训练。通用型LVLM，如GPT-4V，使用标记集（SoM）进行动作定位，但获得SoM标签需要HTML源代码等元数据，而在不同平台上这些元数据并不一致。此外，现有方法往往专长于单一的GUI任务，而未能实现全面的GUI理解。为了解决这些局限性，我们提出了TRISHUL，这是一种新的、无需训练的代理框架，能够增强通用型LVLM的综合GUI理解能力。与先前专注于行动定位（将指令映射到GUI元素）或GUI指引用（给定位置描述GUI元素）工作的研究不同，TRISHUL无缝地结合了这两方面。其核心是层次屏幕解析（HSP）和空间增强元素描述（SEED）模块，这些模块协同工作，提供多粒度、空间和语义丰富的GUI元素表示。我们的实验结果表明，TRISHUL在ScreenSpot、VisualWebBench、AITW和Mind2Web数据集上表现出色，尤其是在行动定位方面的表现。此外，在GUI指引用方面，TRISHUL在ScreenPR基准测试中超过了ToL代理，设定了更加稳健和适应性强的GUI理解的新标准。 

---
# Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation 

**Title (ZH)**: 质胜于量：通过集成多模态数据整理提升数据效率 

**Authors**: Jinda Xu, Yuhao Song, Daming Wang, Weiwei Zhao, Minghua Chen, Kangliang Chen, Qinya Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08211)  

**Abstract**: In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency. 

**Abstract (ZH)**: 在数据泛滥的时代，有效管理网络抓取数据集对于优化模型性能至关重要。本论文针对此类数据集的无序和异质性带来的挑战进行了探讨。传统的启发式数据管理方法往往无法充分捕捉到复杂特征，导致了偏见以及相关数据的排除。我们提出了一种先进的、以学习为导向的方法——多模态操作下的数据集成管理（EcoDatum），该方法融合了一个新颖的质量导向去重方法，以确保特征分布的均衡。EcoDatum 战略性地将多种单模态和多模态数据管理操作器融合在一个弱监督集成框架中，利用自动化优化来有效地对每个数据点进行评分。EcoDatum 显著提高了数据管理的质量和效率，并在其处理的38个不同评价数据集上的平均性能得分为0.182，超过了现有最先进的技术，排名DataComp领览榜第一。这比DataComp基准方法提高了28%的性能，证明了其在提高数据集管理和模型训练效率方面的有效性。 

---
# Equivariant Masked Position Prediction for Efficient Molecular Representation 

**Title (ZH)**: 等变掩蔽位置预测以实现高效分子表示 

**Authors**: Junyi An, Chao Qu, Yun-Fei Shi, XinHao Liu, Qianwei Tang, Fenglei Cao, Yuan Qi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08209)  

**Abstract**: Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in this https URL. 

**Abstract (ZH)**: 图神经网络（GNNs）在计算化学领域展现了巨大的潜力。然而，分子数据的有限可用性引发了对GNNs能否有效捕捉物理和化学的基本原理的担忧，这限制了它们的泛化能力。为了解决这一挑战，我们提出了一个基于分子内部势能和力理论的新型自监督方法，称为等变掩蔽位置预测（EMPP）。不同于传统的属性掩蔽技术，EMPP 定义了一个更为细腻的位置预测任务，这有助于增强对量子力学特征的学习。EMPP 还绕过了去噪方法中常用的高斯混合分布近似，从而能够更准确地获取物理性质。实验结果表明，EMPP 显著提高了高级分子架构的性能，超越了现有的最先进的自监督方法。我们的代码在此处发布：[插入链接]。 

---
# Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning 

**Title (ZH)**: 在数据稀缺条件下面向灾难性遗忘的最新进展：少量样本分类增量学习的综合调研 

**Authors**: M. Anwar Ma'sum, Mahardhika Pratama, Igor Skrjanc  

**Link**: [PDF](https://arxiv.org/pdf/2502.08181)  

**Abstract**: Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL. 

**Abstract (ZH)**: 数据稀缺性极大地复杂化了持续学习问题，即在一个样本极其有限的动态环境中，深度神经网络如何学习。然而，近期的少样本类别增量学习（FSCIL）方法及相关研究展示了应对这一问题的有效途径。本文对FSCIL进行了全面综述，突出了几个重要方面，包括FSCIL方法的全面和形式化的目标、原型修正的重要性、基于预训练模型和语言引导机制的新学习范式、对FSCIL性能指标和评估的更深入分析，以及FSCIL在各个领域的实际应用背景。我们的深入讨论概述了FSCIL面临的开放挑战、潜在解决方案和未来方向。 

---
# Enhancing LLM Character-Level Manipulation via Divide and Conquer 

**Title (ZH)**: 通过分而治之策略提升大语言模型的字符级操控能力 

**Authors**: Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Zhecheng Li, Yiwei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08180)  

**Abstract**: Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and $\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示了较强的泛化能力。然而，它们在字符级别串操作方面表现出明显的不足，难以执行诸如字符删除、插入和替换等基本操作。这些挑战主要源于分词约束，尽管这类操作在数据预处理和代码生成中起着关键作用。通过系统的分析，我们得出两个关键见解：（1）LLMs 在利用内在的分词知识进行字符级别推理方面面临重大困难；（2）原子化的词结构能够显著增强LLMs处理分词级别结构信息的能力。基于这些见解，我们提出了一种名为“分而治之”的新方法（Divide and Conquer for Character-Level Manipulation），该方法旨在弥合分词级别处理与字符级别操作之间的差距。该方法将复杂的操作分解为显式字符级别子任务，并结合受控的分词重建阶段，从而显著提高了准确性。在无需额外训练的情况下，我们的方法在字符删除、字符插入和字符替换任务上的准确性得到了显著提高。为了支持进一步的研究，我们开源了我们的实现和基准测试。 

---
# MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural Network for Recommendation 

**Title (ZH)**: MixDec 抽样方法：图神经网络推荐中的软链接基抽样方法 

**Authors**: Xiangjin Xie, Yuxin Chen, Ruipeng Wang, Kai Ouyang, Zihan Zhang, Hai-Tao Zheng, Buyue Qian, Hansen Zheng, Bo Hu, Chengxiang Zhuo, Zang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08161)  

**Abstract**: Graph neural networks have been widely used in recent recommender systems, where negative sampling plays an important role. Existing negative sampling methods restrict the relationship between nodes as either hard positive pairs or hard negative pairs. This leads to the loss of structural information, and lacks the mechanism to generate positive pairs for nodes with few neighbors. To overcome limitations, we propose a novel soft link-based sampling method, namely MixDec Sampling, which consists of Mixup Sampling module and Decay Sampling module. The Mixup Sampling augments node features by synthesizing new nodes and soft links, which provides sufficient number of samples for nodes with few neighbors. The Decay Sampling strengthens the digestion of graph structure information by generating soft links for node embedding learning. To the best of our knowledge, we are the first to model sampling relationships between nodes by soft links in GNN-based recommender systems. Extensive experiments demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation performance of several representative GNN-based models on various recommendation benchmarks. 

**Abstract (ZH)**: 近年来，图神经网络在推荐系统中得到了广泛的应用，其中负采样起着重要作用。现有的负采样方法将节点间的关系限制为硬正样本对或硬负样本对，这导致了结构信息的损失，并且缺乏为邻居节点较少的节点生成正样本对的机制。为克服这些限制，我们提出了一种新颖的基于软链接的采样方法，即MixDec采样，该方法由Mixup采样模块和衰减采样模块组成。Mixup采样通过合成新的节点和软链接来增强节点特征，为邻居节点较少的节点提供了足够的样本数量。衰减采样通过生成节点嵌入学习所需的软链接，增强了图结构信息的消化。据我们所知，我们是第一个在基于图神经网络的推荐系统中通过软链接建模节点间采样关系的研究。广泛实验表明，所提出的MixDec采样在多个代表性的基于图神经网络的模型上，能够在各种推荐基准中显著且一致地提高推荐性能。 

---
# Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly 

**Title (ZH)**: 实践中的垂直联邦学习：优缺点及其他 

**Authors**: Zhaomin Wu, Zhen Qin, Junyi Hou, Haodong Zhao, Qinbin Li, Bingsheng He, Lixin Fan  

**Link**: [PDF](https://arxiv.org/pdf/2502.08160)  

**Abstract**: Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications. 

**Abstract (ZH)**: 垂直联邦学习（VFL）是一种隐私保护的合作学习范式，它使拥有不同特征集的多个参与方能够在不共享原始数据的情况下共同训练机器学习模型。尽管VFL具有促进跨组织合作的潜力，但在实际应用中的部署仍然受到限制。为了研究现有VFL研究与实际部署之间的差距，本文综述分析了潜在VFL应用场景中的实际数据分布，并确定了四个关键发现，突显了这一差距。我们基于实际VFL数据分布提出了一个新颖的数据导向的VFL算法分类法。通过对现有VFL算法的全面回顾，我们发现一些常见的实际VFL场景几乎没有或没有可行的解决方案。基于这些观察，我们概述了关键研究方向，旨在缩小当前VFL研究与实际应用之间的差距。 

---
# DGSense: A Domain Generalization Framework for Wireless Sensing 

**Title (ZH)**: DGSense：一种适用于无线传感的领域泛化框架

这个标题翻译成中文后符合学术规范，保持了原文的意思和专业性。如果有更多具体内容或章节需要翻译或进一步帮助，请告诉我！ 

**Authors**: Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08155)  

**Abstract**: Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining. 

**Abstract (ZH)**: 无线传感极大地改善了我们的日常生活。然而，无线信号对周围环境非常敏感。各种因素，例如环境、位置和个体，可能会对无线传播产生额外的影响。这种变化可以被视为一个域，在该域中，数据分布发生了转移。大多数传感方案基于学习，依赖于训练域，在未见过的新域中表现会下降。研究人员提出了一些建议来解决这个问题，但这些解决方案要么使用半监督域适应技术，要么使用无监督域适应技术。这些方法仍然需要目标域的一些数据，并不能很好地适应未见过的领域。在这篇论文中，我们提出了一种通用的域泛化框架DGSense，以消除无线传感中的域依赖性问题。该框架可以应用于各种传感任务和无线技术。一旦建立了传感模型，它便可以在没有目标领域任何数据的情况下泛化到未见过的领域。为了实现这一目标，我们首先通过虚拟数据生成器增加训练集的多样性，然后通过主要特征提取器和域特征提取器之间的分组训练提取出独立于域的特征。特征提取器使用具有空间注意力机制的预训练残差网络（ResNet）和用于时间特征的1维卷积神经网络（1DCNN）。为了证明DGSense的有效性和通用性，我们在WiFi手势识别、毫米波（mmWave）活动识别以及声学跌倒检测中进行了评估。所有系统在未见过的领域中均表现出高的泛化能力，包括新用户、新地点和新环境，无需新的数据和重新训练。 

---
# Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling 

**Title (ZH)**: 基于相对论约束的力匹配方法：一种物理启发的生成建模稳定高效方法 

**Authors**: Yang Cao, Bo Chen, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan  

**Link**: [PDF](https://arxiv.org/pdf/2502.08150)  

**Abstract**: This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the \textit{half-moons} dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of \textbf{0.714}, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning. 

**Abstract (ZH)**: 本文介绍了Force Matching（Force Matching，简称ForM）框架，这是一种新颖的生成模型框架，初步探索了利用特殊相对论力学来提高采样过程稳定性的方法。通过引入洛伦兹因子，ForM施加了速度约束，确保样本速度保持在恒定限值内。这一约束机制是稳定生成动力学的基础，从而实现更稳健和可控的采样过程。我们提供了一种严格的理论分析，证明了在ForM框架内的采样过程中，速度约束能够得以保持。为了验证我们方法的有效性，我们进行了广泛的实验评估。在“半月形”数据集上，ForM显著优于基线方法，实现了最低的欧几里得距离损失 **0.714**，而vanilla第一阶流匹配为5.853，一二阶流匹配为5.793。此外，我们还进行了消融实验，进一步探讨了我们速度约束的影响，再次证明了ForM在稳定生成过程方面优于其他方法。理论保证和实验结果突显了将特殊相对论原则整合到生成模型中的潜力。我们的发现表明，ForM提供了一条实现稳定、高效和灵活生成过程的有前景途径。本文为高维生成建模的进一步发展奠定了基础，开辟了在机器学习中应用物理原则的新途径。 

---
# Generalized Class Discovery in Instance Segmentation 

**Title (ZH)**: 实例分割中的泛化类发现 

**Authors**: Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08149)  

**Abstract**: This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods. 

**Abstract (ZH)**: 本文针对实例分割中的泛化类发现（GCD）任务进行探讨。其目标是在标记和未标记数据的基础上，发现新的类别，并构建一个能够分割已知和未知类别实例的模型。由于现实世界中存在大量的长尾分布物体，每个类别的实例分布是固有的不平衡。为了应对这种不平衡分布，我们提出了一种实例级别的温度分配（Instance-wise Temperature Assignment, ITA）方法和类别级别的可靠性准则来用于伪标签。ITA方法通过缓解头部类别的样本之间的实例区分性来增强泛化类发现。类别级别的可靠性准则则是在使用泛化类发现生成的伪标签训练实例分割网络时，避免大部分尾部类别的伪标签被排除。此外，我们还提出动态调整这些准则的方法，在早期阶段利用多样化的样本，而在后期阶段仅依赖可靠的伪标签。我们还引入了一种有效的软注意力模块，用于编码泛化类发现所需的物体特异性表示。最后，我们在两个设置下（COCO_half + LVIS 和 LVIS + Visual Genome）进行了实验研究，结果表明所提出的方法优于现有最先进的方法。 

---
# Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers 

**Title (ZH)**: democratizing ai: 在基于gpu的超级计算机上实现开源可扩展的大语言模型训练 

**Authors**: Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin Wen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, Abhinav Bhatele  

**Link**: [PDF](https://arxiv.org/pdf/2502.08145)  

**Abstract**: Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).
While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore "catastrophic memorization", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier. 

**Abstract (ZH)**: 训练和微调具有数百亿到万亿参数的大语言模型（LLMs）需要数千甚至更多的GPU，并且需要一个高度可扩展的软件栈。在本工作中，我们介绍了一种新颖的四维混合并行算法，并在名为AxoNN的高度可扩展、可移植、开源框架中实现了该算法。我们描述了AxoNN中的若干性能优化措施，包括改进矩阵乘法内核性能、将非阻塞聚合与计算重叠以及进行性能建模以选择性能最优配置。这些措施使得在Perlmutter（620.1千万亿次/秒）、Frontier（1.381千万亿次/秒）和Alps（1.423千万亿次/秒）等超级计算机上训练GPT风格的转换器模型达到了前所未有的可扩展性和峰值浮点运算率（bf16）。

随着可训练参数数量的增加，LLMs的能力不断提升，同时也会带来隐私和版权风险的增加。训练数据的记忆可能导致在推理时泄露敏感或私人信息。我们通过探索“灾难性记忆化”的实验突显了这种规模效应的副作用，其中模型足够大可以在一次通过中记忆训练数据，并提出了一种防止这一问题的方法。作为这项研究的一部分，我们展示了使用AxoNN在Frontier上对一个具有4050亿参数的LLM进行微调。 

---
# Hookpad Aria: A Copilot for Songwriters 

**Title (ZH)**: Hookpad Aria：作曲助手��朋艾莉亚 

**Authors**: Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun  

**Link**: [PDF](https://arxiv.org/pdf/2502.08122)  

**Abstract**: We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.
More information about Hookpad Aria is available at this https URL 

**Abstract (ZH)**: 我们介绍了Hookpad Aria，这是一个生成式AI系统，旨在辅助音乐人在创作西方流行歌曲时提供帮助。该系统无缝集成到了Hookpad中，这是一个基于Web的编辑器，用于创作乐谱（lead sheets）：一种描述旋律和和声的符号音乐记谱方式。Hookpad Aria 具有多项生成能力，旨在协助用户进行非顺序创作流程，包括：(1) 生成现有材料的从左到右延续部分，(2) 完成现有材料中间缺失的部分，以及(3) 从旋律生成和声，反之亦然。此外，Hookpad Aria 还是一个可扩展的音乐共创数据飞轮——自2024年3月发布以来，Aria 已为3000名用户生成了318,000个建议，其中74,000个已被用户纳入他们的作品中。

有关Hookpad Aria的更多信息，请访问此网址：[请在此插入网址] 

---
# HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses 

**Title (ZH)**: HuDEx：结合幻觉检测和可解释性以提高大型语言模型（LLM）响应可靠性 

**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08109)  

**Abstract**: Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的进展表现出有希望的改进，往往在自然语言处理的广泛下游任务中超越现有方法。然而，这些模型仍然面临挑战，这些挑战可能会妨碍它们的实际应用。例如，幻觉现象已知会降低LLMs的可靠性，特别是在需要高度事实精度的领域。当前的基准主要集中在幻觉检测和事实性评估，但并未进一步扩展到超过识别的层面。本文提出了一个增强解释的幻觉检测模型，命名为HuDEx，旨在通过检测幻觉并提供详细解释来增强LLM生成响应的可靠性。所提出的模型提供了一种将检测与解释集成的新方法，使用户和LLM本身能够理解并减少错误。我们的测量结果表明，所提出的模型在幻觉检测准确性方面超越了更大的LLM，例如Llama3 70B和GPT-4，同时保持可靠的解释。此外，所提出的模型在零样本和其他测试环境中均表现良好，展示了其在多种基准数据集中的适应性。通过提出一种将可解释性与幻觉检测集成的新方法，本文进一步增强了幻觉检测研究，进一步提高了语言模型中评估幻觉的性能和可靠性。 

---
# Generative AI and Empirical Software Engineering: A Paradigm Shift 

**Title (ZH)**: 生成式人工智能与实证软件工程：范式的转变 

**Authors**: Christoph Treude, Margaret-Anne Storey  

**Link**: [PDF](https://arxiv.org/pdf/2502.08108)  

**Abstract**: The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.
This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research. 

**Abstract (ZH)**: 软件工程中生成式人工智能的广泛应用标志着范式的转变，为设计和利用软件工程工具带来了新的机会，同时也在开发人员及其创建的制品方面产生了影响。传统软件工程中的经验研究方法，包括定量、定性以及混合方法，已经得到了广泛应用。然而，这种范式的转变引入了新的数据类型，并重新定义了软件工程过程中的许多概念。开发人员、用户、代理和研究者的角色日益重叠，模糊了这些社会和技术角色之间的界限。

本文探讨了将人工智能集成到软件工程中对传统研究范式带来的挑战。我们关注的研究现象、采用的方法与理论、分析的数据以及在这个新背景下出现的有效性威胁。通过这一探索，我们的目标是理解生成式人工智能如何打破现有的软件开发实践，从而为经验软件工程研究开辟新的机遇。 

---
# PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation 

**Title (ZH)**: PoGDiff：用于不平衡文本到图像生成的产品高斯扩散模型 

**Authors**: Ziyan Wang, Sizhe Wei, Xiaoming Huo, Hao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08106)  

**Abstract**: Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality. 

**Abstract (ZH)**: 近年来，扩散模型取得了显著的进步。然而，当它们在不平衡数据集上进行训练或微调时，性能往往会下降。这种下降主要是由于图像-文本对中多数类和少数类数据的不均衡表示。在本文中，我们提出了一种通用的微调方法，称为PoGDiff，以解决这一挑战。PoGDiff 不是直接最小化预测分布和 ground-truth 分布之间的 KL 散度，而是用一个由原始 ground-truth 目标与基于邻近文本嵌入的预测分布组合而成的高斯乘积(PoG)来替换 ground-truth 分布。在真实世界数据集上的实验表明，我们提出的方法有效解决了扩散模型中的不平衡问题，提高了生成的准确性和质量。 

---
# Rethinking Tokenized Graph Transformers for Node Classification 

**Title (ZH)**: 重新思考分词图变换器在节点分类中的应用 

**Authors**: Jinsong Chen, Chenyang Li, GaiChao Li, John E. Hopcroft, Kun He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08101)  

**Abstract**: Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTs for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification. 

**Abstract (ZH)**: 现有的节点标记图Transformer（GTs）在节点分类任务中展现出了有前途的表现。标记序列的生成是现有标记GTs中的关键模块，它将输入图转换为标记序列，通过Transformer促进节点表示学习。在这项研究中，我们观察到现有GTs中的标记序列生成仅关注构建的相似性图中的邻近节点，这限制了节点生成多样化标记序列的能力，进一步限制了标记GTs在节点分类任务中的潜在应用。为此，我们提出了一种名为SwapGT的新方法。SwapGT首先引入了一种基于节点标记序列特征的新型标记交换操作，以充分利用节点之间的语义关联，生成更具信息量的标记序列。然后，SwapGT利用基于Transformer的主干网络从生成的标记序列中学习节点表示。此外，SwapGT开发了一种中心对齐损失来约束从多个标记序列中进行的表示学习，进一步提升了模型性能。在各种数据集上的广泛实验证明，SwapGT在节点分类任务中表现出优越性。 

---
# GCoT: Chain-of-Thought Prompt Learning for Graphs 

**Title (ZH)**: GCoT：图的链式思维提示学习 

**Authors**: Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, Yuan Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08092)  

**Abstract**: Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach. 

**Abstract (ZH)**: 链式推理（CoT）提示在自然语言处理（NLP）中取得了显著的成功。然而，其在图数据上的巨大潜力尚未被充分探索。这引出了一个有趣的问题：我们如何为图设计链式推理提示，以引导图模型逐步学习？一方面，与自然语言不同，图是非线性的，并且由复杂的拓扑结构特征化。另一方面，许多图数据缺乏文本信息，这使得基于语言的链式推理提示难以构建。在本次工作中，我们提出了首个适用于无文本图的链式推理提示学习框架GCoT。具体而言，我们将每个下游任务的适应过程分解成一系列推理步骤，每个步骤包括基于提示的推理、“思考”生成以及基于“思考”的提示学习。虽然这些步骤模仿了NLP中的链式推理机制，但其具体实现方式存在显著差异。具体而言，在每个步骤中，首先将输入图与提示一起输入预训练的图编码器进行基于提示的推理。随后，我们将编码器的隐藏层聚合以构建“思考”，它捕捉到了当前步骤中每个节点的工作状态。基于这种“思考”，我们根据当前状态学习出每个节点特定的提示。这些提示被输入到下一个推理步骤中，从而重复这个循环。为了评估和分析GCoT的有效性，我们在八个公开数据集上进行了全面的实验，结果证明了我们方法的优势。 

---
# Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning 

**Title (ZH)**: Cognify: 通过分层自动调优增强生成式AI工作流 

**Authors**: Zijian He, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08056)  

**Abstract**: Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x. 

**Abstract (ZH)**: 当今涉及多个机器学习模型调用、工具/API 调用、数据检索或通用代码执行的生成型人工智能（gen-AI）工作流通常通过非系统的方式手动调整，这种方式既耗时又易出错。在本文中，我们提出了一种系统的方法来自动调整生成型人工智能工作流。我们的主要见解是：生成型人工智能工作流可以从结构、操作符和提示的更改中受益，但生成型人工智能工作流的独特性质需要新的优化技术。我们提出了一种适应性的分层搜索算法AdaSeek，用于自动生成调整生成型人工智能工作流的方法。AdaSeek 根据用户指定的总搜索预算将工作流调整方法分为不同的层次，并根据每层的复杂度分配预算。在其分层搜索过程中，AdaSeek 根据工作流级评估结果重新分配搜索预算，从不太有用的配置重新分配到更有前景的配置。我们实现了 AdaSeek 在一个工作流自动生成调整框架 Cognify 中，并使用六种类型的工作流（如基于 RAG 的问答和文本到 SQL 转换）评估 Cognify。总体而言，Cognify 将这些工作流的生成质量提高了最多 2.8 倍，降低了最多 10 倍的执行成本，并将端到端延迟减少了 2.7 倍。 

---
# Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs 

**Title (ZH)**: 打破复选框的限制：挑战对LLMs文化对齐的封闭式评估方式 

**Authors**: Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08045)  

**Abstract**: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs. 

**Abstract (ZH)**: 许多研究依赖于封闭式多项选择调查来评估大型语言模型（LLMs）的文化一致性。本文挑战了这种局限性的评估范式，并探索了更为现实、不受限制的方法。我们以世界价值观调查（WVS）和霍夫斯泰德文化维度为例，表明LLMs在不受限制的环境中表现出更强的文化一致性，此时不会强迫作答。此外，我们还展示了即使是轻微的变化，比如重新排列调查选项，也会导致输出不一致，揭示了封闭式评估的局限性。我们的研究结果倡导采用更为稳健和灵活的评估框架，这些框架关注特定的文化代理指标，从而促进对LLMs文化一致性的更为细致和准确的评估。 

---
# Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol 

**Title (ZH)**: 离策略评估中的模型选择：新算法及实验规范 

**Authors**: Pai Liu, Lingfeng Zhao, Shivangi Agarwal, Jinghan Liu, Audrey Huang, Philip Amortila, Nan Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08021)  

**Abstract**: Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions ("model-free") or dynamics ("model-based") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance. 

**Abstract (ZH)**: 在离线强化学习（RL）中，从数据中进行保留集验证和超参数调整是一个长期存在的问题。标准框架是使用离策策略评估（Off-Policy Evaluation, OPE）方法来评估和选择策略，但OPE要么会产生指数级的方差（例如，重要性采样），要么自身带有超参数（例如，FQE和基于模型的方法）。在这项工作中，我们集中于超参数调整对OPE本身的改进，这是一个更为未被充分研究的问题。具体来说，我们选择了候选值函数（“无模型”）或动态（“基于模型”），以最佳评估目标策略的表现。我们的贡献主要有两个方面。我们开发了：（1）带有理论保证的新无模型和基于模型的选择器，以及（2）一种新的实验协议来经验性地评估它们。相比以前工作的无模型协议，我们新的协议允许更稳定地生成候选值函数，更好地控制模型误设，并且能够评估无模型和基于模型的方法。我们以一个Gym环境为例，展示了该协议的应用，并发现我们新的无模型选择器LSTD-Tournament展示出有希望的实验性能。 

---
# Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding 

**Title (ZH)**: 推测而后协作：在解码过程中融合语言模型的知识 

**Authors**: Ziyao Wang, Muneeza Azmart, Ang Li, Raya Horesh, Mikhail Yurochkin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08020)  

**Abstract**: Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications 

**Abstract (ZH)**: 大型语言模型（LLMs）在某些领域表现出色，但在其他领域则可能存在局限性，这主要是由于训练的限制。因此，通过整合互补知识，使LLMs能够协作解决问题，有望提高其跨领域的性能。为了实现这一潜力，我们提出了一种新的协作推测解码（CoSD）算法，在测试时能够有效融合LLM的知识，而无需进行额外的模型训练。CoSD 使用一个草稿模型生成初始序列，并利用易于学习的规则或决策树决定何时调用助手模型以改进这些草稿。CoSD 不仅能够增强知识融合，还能提高推理效率，具有跨领域和模型的可转移性，并提供更好的可解释性。实验结果表明，相比于现有方法，CoSD 在基准测试中将准确性最多提高10%，提供了一种可扩展且有效的基于LLM的应用解决方案。 

---
# Greed is Good: Guided Generation from a Greedy Perspective 

**Title (ZH)**: 贪婪亦可取：从贪婪视角指导生成 

**Authors**: Zander W. Blasingame, Chen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08006)  

**Abstract**: Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models. In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner. We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques. We show that a greedy guidance strategy makes good decisions and compare it to a guidance strategy using the ideal gradients found via the continuous adjoint equations. We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective. 

**Abstract (ZH)**: 训练-free 指导生成是一种广泛使用且强大的技术，它允许最终用户进一步控制扩散模型的生成过程。本研究通过优化神经微分方程解轨迹的贪婪方法，从新的角度探索指导生成。我们提出了一种统一的训练-free 指导视图，具体来说，贪婪策略是一种端到端优化技术的一阶离散化方式。我们展示了贪婪指导策略可以做出良好决策，并将其与通过连续伴随方程找到的理想梯度实现的指导策略进行了比较。接着，我们从这一视角展示了其他流行的训练-free 指导策略可以统一看待的方法。 

---
# MetaSC: Test-Time Safety Specification Optimization for Language Models 

**Title (ZH)**: MetaSC：语言模型的测试时安全规范优化 

**Authors**: Víctor Gallego  

**Link**: [PDF](https://arxiv.org/pdf/2502.07985)  

**Abstract**: We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at this https URL . 

**Abstract (ZH)**: 我们提出了一种新型动态安全框架，该框架在推理时优化语言模型（LM）的安全推理过程，而不修改模型权重。该方法基于近期自批判方法的发展，利用了一种元批判机制，该机制通过迭代更新称为规范的安全提示，以适应性地驱动批判和修订过程。这种测试时的优化不仅能够提高对对抗性闯关请求的性能，还能够在多种通用安全相关任务中提高性能，例如避免道德伤害或追求诚实回答。通过对多种语言模型的实证评估表明，动态优化的安全提示相较于固定系统提示和静态自我批判防护措施，能够显著提高安全性评分。代码将在以下地址发布：[该 https URL]。 

---
# CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs 

**Title (ZH)**: CIRCUIT：评估大型语言模型电路解释与推理能力的基准评测 

**Authors**: Lejla Skelic, Yan Xu, Matthew Cox, Wenjie Lu, Tao Yu, Ruonan Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.07980)  

**Abstract**: The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design. 

**Abstract (ZH)**: 在模拟电路设计领域，大型语言模型（LLMs）的作用尚未得到广泛探索，此类模型可以通过超越传统优化技术的基于推理的方法带来益处。尽管LLMs的重要性日益提高，但目前尚未有评估它们对电路进行推理能力的基准。因此，我们创建了CIRCUIT数据集，其中包括了涵盖各种模拟电路相关主题的510个问答对。我们数据集中表现最好的模型GPT-4o在最终数值答案上的准确率为48.04%。为了评估LLMs在该数据集上的稳健性，我们引入了一个独特特征，通过将问题分组为单元测试来实现类似单元测试的评估方法。在这种情况下，GPT-4o只能通过27.45%的单元测试，这表明最先进的LLMs在理解电路方面仍存在问题，因为电路理解需要多级推理能力，特别是在涉及电路拓扑时。这种针对电路的特定基准突显了LLMs的局限性，为他们在模拟集成电路设计中的应用提供有价值的见解。 

---
# From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems 

**Title (ZH)**: 从危险识别到控制器设计：基于LLM的支持的前瞻性安全工程方法在机器学习驱动系统中的应用 

**Authors**: Yining Hong, Christopher S. Timperley, Christian Kästner  

**Link**: [PDF](https://arxiv.org/pdf/2502.07974)  

**Abstract**: Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated. 

**Abstract (ZH)**: 机器学习（ML）组件越来越多地集成到软件产品中，然而，这些组件的复杂性和固有的不确定性往往会导致个体和社会层面的未预见和危险的后果。尽管存在这些风险，从业者很少采取主动措施在危险发生前进行预见和缓解。传统的安全工程方法，如故障模式和效应分析（FMEA）和系统理论过程分析（STPA），提供了早期风险识别的系统化框架，但很少被采用。本文倡导将危害分析集成到任何基于ML的软件产品的开发过程中，并呼吁加强对这一过程的支持，使其更容易为开发者所使用。通过使用大型语言模型（LLMs）部分自动化修改后的STPA过程，并在关键步骤进行人工监督，我们期望解决两个关键挑战：对高经验安全工程专家的高依赖性，以及传统危害分析耗时、劳动密集的性质，这通常会阻碍其在实际开发工作流中的集成。我们通过一个运行示例阐述了这种方法，证明了许多看似无法预见的问题实际上可以预见。 

---
# Training Sparse Mixture Of Experts Text Embedding Models 

**Title (ZH)**: 训练稀疏专家混合文本嵌入模型 

**Authors**: Zach Nussbaum, Brandon Duderstadt  

**Link**: [PDF](https://arxiv.org/pdf/2502.07972)  

**Abstract**: Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline. 

**Abstract (ZH)**: 基于Transformer的文本嵌入模型通过增加参数量在MIRACL和BEIR等基准测试上的性能得到了提升。然而，这种扩展方法引入了显著的部署挑战，包括推断延迟的增加和内存使用量的增加。这些挑战在检索增强生成（RAG）应用中尤为严重，因为大型模型增加的内存需求限制了数据集的容量，而它们更高的延迟直接影响了查询时间性能。虽然因果语言模型使用专家混合（MoE）架构解决了类似的有效性挑战，但这种方法尚未成功适应一般的文本嵌入设置。在本文中，我们介绍了Nomic Embed v2，这是第一个用于通用目的的MoE文本嵌入模型。我们的模型在单语和多语基准测试中都超过了同参数量级的模型，并且其性能与两倍参数量级的模型相当。我们开源了所有代码、模型和评估数据，以确保我们训练管道的完全可再现性。 

---
# ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval 

**Title (ZH)**: ReTreever：基于树结构的自顶向下表示检索方法 

**Authors**: Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07971)  

**Abstract**: Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that ReTreever generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications. 

**Abstract (ZH)**: 文档检索是问答系统的核心组件之一，因为它使回答生成能够基于新的和大规模的语料库进行调整。虽然有效，但将文档编码为高维嵌入以便进行相似性搜索的标准做法会带来巨大的内存和计算需求，并且也使得系统内部工作原理难以检查。在本文中，我们提出了一种基于树的方法，用于在不同粒度级别组织和表示参考文档，该方法提供了灵活的成本和效用权衡能力，并便于检查语料库内容和检索操作。我们的方法称为ReTreever，通过在二叉树内部节点上联合学习路由函数，使得查询文档和参考文档被分配到相似的树分支中，从而直接优化检索性能。我们的评估表明，ReTreever 通常能够保留完整的表示准确性。其分层结构还提供了很强的粗粒度表示，并通过间接学习有意义的语义分组增强了透明度。在分层检索方法中，ReTreever 的检索准确性在最低延迟下最好，证明了这种技术家族可以在实际应用中具有可行性。 

---
# Generative Risk Minimization for Out-of-Distribution Generalization on Graphs 

**Title (ZH)**: 图上离分布外泛化的生成风险最小化方法 

**Authors**: Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07968)  

**Abstract**: Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM. 

**Abstract (ZH)**: 图上的离分布（OOD）泛化旨在处理测试图分布与训练图分布不同的场景。与独立同分布的数据（如图像）相比，由于图结构数据具有非独立同分布的属性和复杂的结构信息，图上离分布泛化问题仍然具有挑战性。近年来，一些关于图的离分布泛化的工作探索了提取不变子图的方法，这些子图在不同分布下共享关键的分类信息。然而，这种策略可能无法完全捕捉不变信息，因为离散结构的提取可能会导致不变信息的丢失，或者引入虚假信息。在本文中，我们提出了一种创新框架，名为生成风险最小化（Generative Risk Minimization, GRM），该框架旨在为每个输入图生成一个不变子图，而不是提取不变子图。为了解决在缺乏最优不变子图（即 ground truth）的情况下优化的挑战，我们通过引入潜在因果变量推导出了所提 GRM 目标的可处理形式，并通过理论分析验证了其有效性。我们还在多种真实世界的图数据集上进行了广泛的实验，涵盖节点级和图级的离分布泛化，实验结果证明了我们框架 GRM 的优越性。 

---
# Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature? 

**Title (ZH)**: 《陷于语言的罗网：霖幕模型会在医学文献的误导下犯错吗？》

注：这里的“霖幕模型”是对“LLMs”（Large Language Models）的一个意译，考虑到“霖幕”二字既有“霖”，即长时间的雨，又有“幕”，即覆盖、遮挡之意，与“LLMs”在某种意义上隐藏信息、影响理解的特点相契合。但学术翻译中通常避免使用这种双关或意译，建议保持原始的“大型语言模型”表达方式，以符合严格的学术规范。因此，更准确的翻译应为：

《陷于语言的罗网：大型语言模型会在医学文献的误导下犯错吗？》 

**Authors**: Hye Sun Yun, Karen Y.C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace  

**Link**: [PDF](https://arxiv.org/pdf/2502.07963)  

**Abstract**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs. 

**Abstract (ZH)**: 医学研究在将新型治疗方法转化为临床实践方面面临着广泛记录的挑战。发表激励促使研究者倾向于呈现“积极”的研究结果，即使实证结果模棱两可。因此，有充分的证据表明，作者常常会在论文摘要中“旋转”研究结果。这种旋转可能会对临床医生对证据的解释产生影响，并可能影响患者的治疗决策。在本研究中，我们探讨了大型语言模型（LLMs）是否也会受到同样的旋转影响进行结果解释。由于LLMs越来越多地被用于筛选和综合已发表的医学证据，这一问题尤为重要。我们评估了22种LLM模型，发现它们普遍比人类更容易受到旋转的影响。此外，我们还发现LLMs可能会将旋转带入其生成的输出中：例如，它们在生成简洁语言摘要时可能会隐含地纳入旋转。然而，我们也发现，LLMs通常能够识别旋转，并可以通过特定方式激发它们以减轻旋转对其输出的影响。 

---
# VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning 

**Title (ZH)**: VSC-RL：基于变分子目标条件强化学习的自主视觉-语言代理技术进步 

**Authors**: Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao  

**Link**: [PDF](https://arxiv.org/pdf/2502.07949)  

**Abstract**: State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision. However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency. 

**Abstract (ZH)**: 最新的强化学习（SOTA）方法使具有视觉-语言能力的代理能够在无人类监督的情况下从与环境的交互中学习。然而，它们在应对现实世界中的复杂序列决策任务时存在学习效率低下的问题，尤其是在面对稀疏的奖励信号和长时序依赖性的情况下。为有效解决这一问题，我们提出了变分子目标条件强化学习（VSC-RL），将视觉-语言的序列决策任务重新表述为变分目标条件化强化学习问题，使我们能够利用高级优化方法来提高学习效率。具体而言，VSC-RL 优化子目标证据下界（SGC-ELBO），这包括通过增强学习最大化子目标条件下的回报以及通过最小化子目标与参考策略之间的差异来优化。我们从理论上证明了 SGC-ELBO 等同于原始优化目标，确保在不牺牲性能保证的前提下提高学习效率。此外，对于现实世界中的复杂决策任务，VSC-RL 利用视觉-语言模型自助分解目标为可行的子目标，从而实现高效的 学习。在各种基准测试中，包括具有挑战性的现实世界移动设备控制任务，VSC-RL 显著优于最先进的视觉-语言代理，实现了卓越的性能并显著提高了学习效率。 

---
# CREDAL: Close Reading of Data Models 

**Title (ZH)**: CREDAL: 数据模型的细读分析 

**Authors**: George Fletcher, Olha Nahurna, Matvii Prytula, Julia Stoyanovich  

**Link**: [PDF](https://arxiv.org/pdf/2502.07943)  

**Abstract**: Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model, every statistical model, and every database has an underlying data model without which the system would not be usable. Hence, data models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards this, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary artifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed nature, and the design of technical systems.
While recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance for those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does not exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development process and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the critical study of data. 

**Abstract (ZH)**: 数据模型是数据及其任何数据驱动系统的诞生之本。事实上，每一个算法、每一个机器学习模型、每一个统计模型以及每一个数据库都依赖于一个基础数据模型，否则系统将无法使用。因此，数据模型是探究数据系统（物质、社会、政治等方面）条件的极佳场所。为此，借鉴文学批评的方法，我们建议以文学作品的细读方式来细读数据模型。这种对数据模型的细读让我们重新连接到了它们的物质性、起源、技艺、封闭特性和技术系统的规划设计等方面。

虽然从文学理论中我们知道没有一种读法是绝对正确的，但对于不熟悉细读方法的人来说，拥有系统的指导仍然是至关重要的。对于那些受过计算和数据科学训练的人来说，他们过于习惯于忽略数据工作中与社会和政治相关的一面。目前尚不存在系统性的方法来读取数据模型。为填补这一空白，我们提出了CREDAL方法论，用于细读数据模型。我们详细描述了CREDAL方法论的迭代开发过程，并展示了CREDAL在质性评估中的应用结果，证明了其在数据批判性研究中的可用性、实用性和有效性。 

---
# Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course 

**Title (ZH)**: 培养负责任的人工智能 workforce：在graduate机器学习课程中试点人工智能政策课程模块 

**Authors**: James Weichert, Hoda Eldardiry  

**Link**: [PDF](https://arxiv.org/pdf/2502.07931)  

**Abstract**: As artificial intelligence (AI) technologies begin to permeate diverse fields-from healthcare to education-consumers, researchers and policymakers are increasingly raising concerns about whether and how AI is regulated. It is therefore reasonable to anticipate that alignment with principles of 'ethical' or 'responsible' AI, as well as compliance with law and policy, will form an increasingly important part of AI development. Yet, for the most part, the conventional computer science curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to explore how new educational content related to AI ethics and AI policy can be integrated into both ethics- and technical-focused courses. This paper describes a two-lecture 'AI policy module' that was piloted in a graduate-level introductory machine learning course in 2024. The module, which includes an in-class active learning game, is evaluated using data from student surveys before and after the lectures, and pedagogical motivations and considerations are discussed. We find that the module is successful in engaging otherwise technically-oriented students on the topic of AI policy, increasing student awareness of the social impacts of a variety of AI technologies and developing student interest in the field of AI regulation. 

**Abstract (ZH)**: 随着人工智能（AI）技术逐渐渗透到各个领域——从医疗保健到教育——消费者、研究人员和政策制定者对AI的监管问题越来越担忧。因此，可以预料到，符合“伦理”或“负责任”的AI原则，以及遵守法律法规，将成为AI开发中越来越重要的一部分。然而，传统的计算机科学课程在为学生准备这些挑战方面往往准备不足。为了解决这一问题，我们探讨了如何将与AI伦理和AI政策相关的新型教育内容整合到伦理和技术密集型课程中。本文描述了2024年在一门研究生级入门机器学习课程中试点的两堂“AI政策模块”的教学情况。该模块包括了一堂课堂上的互动式学习游戏，并通过学生课前和课后的问卷调查对学生的学习情况进行了评估，同时还探讨了教学动机和考虑因素。我们发现，该模块成功地激发了原本技术导向的学生对AI政策的兴趣，提高了学生对各种AI技术的社会影响的认识，并培养了学生对AI监管领域的兴趣。 

---
# NDAI Agreements 

**Title (ZH)**: NDA 协议 

**Authors**: Matthew Stephenson, Andrew Miller, Xyn Sun, Bhargav Annem, Rohan Parikh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07924)  

**Abstract**: We study a fundamental challenge in the economics of innovation: an inventor must reveal details of a new idea to secure compensation or funding, yet such disclosure risks expropriation. We present a model in which a seller (inventor) and buyer (investor) bargain over an information good under the threat of hold-up. In the classical setting, the seller withholds disclosure to avoid misappropriation, leading to inefficiency. We show that trusted execution environments (TEEs) combined with AI agents can mitigate and even fully eliminate this hold-up problem. By delegating the disclosure and payment decisions to tamper-proof programs, the seller can safely reveal the invention without risking expropriation, achieving full disclosure and an efficient ex post transfer. Moreover, even if the invention's value exceeds a threshold that TEEs can fully secure, partial disclosure still improves outcomes compared to no disclosure. Recognizing that real AI agents are imperfect, we model "agent errors" in payments or disclosures and demonstrate that budget caps and acceptance thresholds suffice to preserve most of the efficiency gains.
Our results imply that cryptographic or hardware-based solutions can function as an "ironclad NDA," substantially mitigating the fundamental disclosure-appropriation paradox first identified by Arrow (1962) and Nelson (1959). This has far-reaching policy implications for fostering R&D, technology transfer, and collaboration. 

**Abstract (ZH)**: 我们研究了创新经济学中的一个基本挑战：发明者必须披露新想法的细节以获得补偿或资金，但这种披露面临被剥夺的风险。我们提出了一种模型，在威胁性占有环境下，发明者（卖方）与投资者（买方）就信息商品进行谈判。在传统的设置中，卖方出于避免被非法使用的目的而隐瞒披露，导致效率低下。研究表明，可信执行环境（TEE）结合人工智能代理可以缓解甚至完全消除这种被剥夺问题。通过将披露和支付决策委托给防篡改程序，卖方可以安全地披露发明，而不必担心被剥夺，从而实现完全披露和事后高效转让。此外，即使发明的价值超出了TEE完全保障的阈值，部分披露仍然优于不披露，能够改善结果。认识到实际人工智能代理并不完美，我们建模了支付或披露中的“代理错误”，并证明预算上限和接受阈值足以保留大部分效率改进。

我们的研究结果表明，加密或基于硬件的解决方案可以作为“无懈可击的保密协议”发挥作用，显著缓解Arrow（1962年）和Nelson（1959年）首次识别的基本披露-被剥夺悖论。这具有深远的政策影响，对于促进研发、技术转让和合作具有重要意义。 

---
# TransMLA: Multi-head Latent Attention Is All You Need 

**Title (ZH)**: TransMLA：多头潜在注意力机制即一切所需 

**Authors**: Fanxu Meng, Zengwei Yao, Muhan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07864)  

**Abstract**: Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1. 

**Abstract (ZH)**: 现代大型语言模型（LLMs）经常在当前硬件上遇到通信瓶颈，而不是纯粹的计算约束。多头潜在注意（MLA）通过在键值（KV）层使用低秩矩阵来解决这一挑战，从而使压缩的潜在键值状态能够被缓存。这种方法相对于传统的多头注意显着减少了KV缓存的大小，从而加快了推理速度。此外，MLA 使用了上投影矩阵来增加表达能力，以减少通信开销为代价增加额外的计算。尽管 MLA 在 DeepSeek V2/V3/R1 中显示出效率和有效性，但许多主要的模型提供商仍然依赖组查询注意（GQA），并且没有宣布任何计划采用 MLA。在这篇论文中，我们证明在保持相同 KV 缓存开销的前提下，GQA 总是可以由 MLA 表示，但反之则不然。为了鼓励更广泛的使用 MLA，我们引入了 **TransMLA**，这是一种后训练方法，可以将广泛使用的基于 GQA 的预训练模型（例如 LLaMA、Qwen、Mixtral）转换为基于 MLA 的模型。转换后，模型可以进行额外的训练以提高表达能力，而不需要增加 KV 缓存的大小。此外，我们计划开发专门针对 MLA 的推理加速技术，以在转换后的模型中保持低延迟，从而实现 DeepSeek R1 的更有效的蒸馏。 

---
# ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources 

**Title (ZH)**: ADMN：一种适应层的多模态网络，用于动态输入噪声和计算资源调整 

**Authors**: Jason Wu, Kang Yang, Lance Kaplan, Mani Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2502.07862)  

**Abstract**: Multimodal deep learning systems are deployed in dynamic scenarios due to the robustness afforded by multiple sensing modalities. Nevertheless, they struggle with varying compute resource availability (due to multi-tenancy, device heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed corruption, environmental noise, etc.). Current multimodal systems employ static resource provisioning and cannot easily adapt when compute resources change over time. Additionally, their reliance on processing sensor data with fixed feature extractors is ill-equipped to handle variations in modality quality. Consequently, uninformative modalities, such as those with high noise, needlessly consume resources better allocated towards other modalities. We propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of tackling both challenges - it adjusts the total number of active layers across all modalities to meet compute resource constraints, and continually reallocates layers across input modalities according to their modality quality. Our evaluations showcase ADMN can match the accuracy of state-of-the-art networks while reducing up to 75% of their floating-point operations. 

**Abstract (ZH)**: 多模态深度学习系统在多传感器模态提供的鲁棒性支持下，被部署在动态场景中。然而，它们在计算资源可用性（由于多租户、设备异构性等）和输入质量（来自传感器数据污染、环境噪声等）波动方面存在挑战。当前的多模态系统采用静态资源配置，无法适应该计算资源随时间变化的需求。此外，它们依赖于使用固定特征提取器处理传感器数据，难以应对模态质量的变化。因此，一些无信息性模态（如噪声较高的模态）无谓地消耗了本应分配给其他模态的资源。我们提出了一种分层自适应深度多模态网络（ADMN），能够同时应对这两种挑战：它可以根据计算资源约束调整所有模态中活跃层的总数，并根据各模态的质量不断重新分配输入模态中的层。我们的评估结果显示，ADMN 可以达到最先进的网络的精度，同时最多可减少75%的浮点运算。 

---
# BalanceKV: KV Cache Compression through Discrepancy Theory 

**Title (ZH)**: 平衡KV缓存压缩：基于偏差理论的键值对缓存压缩方法 

**Authors**: Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07861)  

**Abstract**: Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods. 

**Abstract (ZH)**: 大规模语言模型（LLMs）已经取得了显著的成功，但它们对内存资源的高需求给长上下文标记生成带来了挑战。长上下文LLMs的内存复杂性主要源于需要在它们的Key-Value (KV) 缓存中存储Key-Value嵌入。我们提出了一种名为BalanceKV的KV缓存压缩方法，该方法基于Banasczyk向量平衡理论中的几何采样过程，通过几何信息引入键和值标记之间的依赖性，并提高精度。BalanceKV在理论上和实证上都证明了相对于现有方法的性能改进。 

---
# SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph 

**Title (ZH)**: SNAP：针对目标因果效应估计的序贯非祖先修剪方法（在未知图的情况下） 

**Authors**: Mátyás Schubert, Tom Claassen, Sara Magliacane  

**Link**: [PDF](https://arxiv.org/pdf/2502.07857)  

**Abstract**: Causal discovery can be computationally demanding for large numbers of variables. If we only wish to estimate the causal effects on a small subset of target variables, we might not need to learn the causal graph for all variables, but only a small subgraph that includes the targets and their adjustment sets. In this paper, we focus on identifying causal effects between target variables in a computationally and statistically efficient way. This task combines causal discovery and effect estimation, aligning the discovery objective with the effects to be estimated. We show that definite non-ancestors of the targets are unnecessary to learn causal relations between the targets and to identify efficient adjustments sets. We sequentially identify and prune these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP) framework, which can be used either as a preprocessing step to standard causal discovery methods, or as a standalone sound and complete causal discovery algorithm. Our results on synthetic and real data show that both approaches substantially reduce the number of independence tests and the computation time without compromising the quality of causal effect estimations. 

**Abstract (ZH)**: 因果发现对于大量变量来说可能是计算上非常耗时的。如果我们只希望估计目标变量子集上的因果效应，我们可能不需要学习所有变量的因果图，而只需学习包含目标变量及其调整集的小子图即可。本文我们关注在高效且统计上有效的方式下识别目标变量之间的因果效应。这一任务结合了因果发现和效应估计，将发现目标与估计效应对齐。我们证明了目标的确定非祖先变量对于学习目标变量之间的因果关系以及识别高效的调整集并不是必要的。我们提出了逐步非祖先修剪（SNAP）框架，逐步识别并剔除这些确定的非祖先变量，该框架既可以作为标准因果发现方法的预处理步骤，也可以作为独立的完整因果发现算法。我们在合成数据和真实数据上的结果表明，两种方法都能显著减少独立性检验的数量和计算时间，同时不会牺牲因果效应估计的质量。 

---
# MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers 

**Title (ZH)**: MRS：基于常微分方程和随机微分方程求解器的快速均值回复扩散抽样方法 

**Authors**: Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07856)  

**Abstract**: In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. 

**Abstract (ZH)**: 在扩散模型的应用中，可控生成既具有实践意义，也颇具挑战性。当前用于可控生成的方法主要集中在修改扩散模型的分数函数，而均值回转（Mean Reverting, MR）扩散直接修改了随机微分方程（Stochastic Differential Equation, SDE）的结构，使得条件图像的包含更为简单和自然。然而，当前的无训练快速采样器不直接适用于MR扩散，因此MR扩散需要数百次函数评估（NFEs，Number of Function Evaluations）才能生成高质量样本。在本文中，我们提出了一种新的算法——MR Sampler（MR采样器），以减少MR扩散的采样NFEs。我们解算了与MR扩散相关的逆时间SDE和概率流常微分方程（Probability Flow ODE, PF-ODE），并推导出半解析解。这些解由一个解析函数和一个由神经网络参数化的积分组成。基于此解，我们可以在更少的步骤中生成高质量样本。我们的方法不需要训练，并支持所有主流参数化形式，包括噪声预测、数据预测和速度预测。广泛的实验表明，MR Sampler在十个不同图像恢复任务中，可以在提速10到20倍的前提下保持高采样质量。我们的算法加快了MR扩散的采样过程，使其在可控生成中更为实用。 

---
# Vision-Language Models for Edge Networks: A Comprehensive Survey 

**Title (ZH)**: 面向边缘网络的视觉-语言模型：一项全面的综述 

**Authors**: Ahmed Sharshar, Latif U. Khan, Waseem Ullah, Mohsen Guizani  

**Link**: [PDF](https://arxiv.org/pdf/2502.07855)  

**Abstract**: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings. 

**Abstract (ZH)**: 视觉大型语言模型（VLMs）结合了视觉理解和自然语言处理能力，能够实现图像描述、视觉问答和视频分析等任务。尽管VLMs在包括自动驾驶车辆、智能监控和医疗保健在内的多个领域展现了令人印象深刻的性能，但在计算资源受限的边缘设备上的部署仍然面临挑战，原因包括处理能力、内存和能源限制。本文综述了近期在优化VLMs以适应边缘环境方面的进展，重点关注模型压缩技术，包括剪枝、量化、知识蒸馏以及专用硬件解决方案，这些方法能够提升效率。我们详细讨论了高效的训练和微调方法、边缘部署挑战以及隐私问题。此外，本文还讨论了轻量级VLMs在医疗保健、环境监测和自主系统等领域的多样化应用，展示了它们日益增长的影响。通过突出关键设计策略、当前挑战，并提出未来方向的建议，本文旨在激发进一步研究VLMs的实际部署问题，最终使先进的AI技术在资源受限的环境中更加普及。 

---
# Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations 

**Title (ZH)**: 理解无分类器引导：高维理论与非线性推广 

**Authors**: Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard  

**Link**: [PDF](https://arxiv.org/pdf/2502.07849)  

**Abstract**: Recent studies have raised concerns about the effectiveness of Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it can lead to overshooting the target distribution and reducing sample diversity. In this work, we demonstrate that in infinite and sufficiently high-dimensional contexts CFG effectively reproduces the target distribution, revealing a blessing-of-dimensionality result. Additionally, we explore finite-dimensional effects, precisely characterizing overshoot and variance reduction. Based on our analysis, we introduce non-linear generalizations of CFG. Through numerical simulations on Gaussian mixtures and experiments on class-conditional and text-to-image diffusion models, we validate our analysis and show that our non-linear CFG offers improved flexibility and generation quality without additional computation cost. 

**Abstract (ZH)**: 近年来的研究对Classifier-Free Guidance (CFG) 的有效性提出了担忧，表明在低维度情况下，它可能导致目标分布过度偏离，从而减少样本多样性。在本研究中，我们展示了在无限且足够高的维度下，CFG 能够有效地重现目标分布，揭示了维度增加的一种“福扯”现象。此外，我们还探讨了有限维度的影响，并精确地定义了过度拟合和方差减少。基于我们的分析，我们引入了非线性的CFG 一般化。通过在高斯混合模型以及类别条件性和文本转图像扩散模型上的数值模拟和实验，我们验证了我们的分析，并证明了我们的非线性CFG 能在不增加计算成本的情况下提供更好的灵活性和生成质量。 

---
# Spread them Apart: Towards Robust Watermarking of Generated Content 

**Title (ZH)**: 将其分散开来：面向生成内容 robust 水印技术的研究 

**Authors**: Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, Ivan Oseledets  

**Link**: [PDF](https://arxiv.org/pdf/2502.07845)  

**Abstract**: Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks. 

**Abstract (ZH)**: 近年来，能够生成逼真图像的生成模型有了显著的改进。生成内容的质量大幅提升，以至于有时候很难区分真实图像和生成图像。这种改进带来了伦理上的担忧：生成模型的用户可能会不当声称对受版权保护的生成内容拥有所有权。本文提出了一种方法，将水印嵌入生成内容中，以实现未来对生成内容的检测和用户身份的识别。水印是在模型推理过程中嵌入的，因此所提出的方法不需要重新训练模型。我们证明嵌入的水印能够抵抗幅度有界的添加性扰动。我们将该方法应用于水印扩散模型，并证明其在面对不同类型合成水印删除攻击时的鲁棒性与现有最先进的水印方案相当。 

---
# Column-wise Quantization of Weights and Partial Sums for Accurate and Efficient Compute-In-Memory Accelerators 

**Title (ZH)**: 列方向权值和部分和的量化以实现准确高效的计算在内存加速器 

**Authors**: Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07842)  

**Abstract**: Compute-in-memory (CIM) is an efficient method for implementing deep neural networks (DNNs) but suffers from substantial overhead from analog-to-digital converters (ADCs), especially as ADC precision increases. Low-precision ADCs can re- duce this overhead but introduce partial-sum quantization errors degrading accuracy. Additionally, low-bit weight constraints, im- posed by cell limitations and the need for multiple cells for higher- bit weights, present further challenges. While fine-grained partial- sum quantization has been studied to lower ADC resolution effectively, weight granularity, which limits overall partial-sum quantized accuracy, remains underexplored. This work addresses these challenges by aligning weight and partial-sum quantization granularities at the column-wise level. Our method improves accuracy while maintaining dequantization overhead, simplifies training by removing two-stage processes, and ensures robustness to memory cell variations via independent column-wise scale factors. We also propose an open-source CIM-oriented convolution framework to handle fine-grained weights and partial-sums effi- ciently, incorporating a novel tiling method and group convolution. Experimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18 (ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively, compared to the best-performing related works. Additionally, variation analysis reveals the robust- ness of our method against memory cell variations. These findings highlight the effectiveness of our quantization scheme in enhancing accuracy and robustness while maintaining hardware efficiency in CIM-based DNN implementations. Our code is available at this https URL. 

**Abstract (ZH)**: 计算内存（Compute-in-Memory，CIM）是实现深度神经网络（DNNs）的一种高效方法，但由于模数转换器（ADC）的显著开销，尤其是在ADC精度提高时，其效果被打折扣。低精度ADC可以减少这一开销，但会引入部分和量化误差，从而降低准确性。此外，由于单元限制和高精度权重所需的多个单元，低位权重约束也提出了进一步的挑战。虽然已经研究了细粒度的部分和量化以有效降低ADC分辨率，但权重粒度仍然是未充分探索的领域，它限制了整体部分和量化精度。本研究通过在列级对齐权重和部分和量化粒度来解决这些挑战。我们的方法在保持去量化开销的同时提高了准确性，并通过独立的列级比例因子简化了训练过程，从而确保了对内存单元变化的鲁棒性。我们还提出了一种开源的CIM导向卷积框架，以有效地处理细粒度的权重和部分和，该框架采用了新颖的分块方法和分组卷积。我们在ResNet-20（CIFAR-10，CIFAR-100）和ResNet-18（ImageNet）上的实验结果表明，与最佳相关工作的准确率相比，准确率分别提高了0.99%，2.69% 和1.01%。此外，变异性分析还揭示了我们的方法对内存单元变化的鲁棒性。这些发现突显了我们量化方案在提高准确性和鲁棒性方面的有效性，同时保持了CIM基DNN实现的硬件效率。我们的代码可在以下链接获取：此链接。 

---
# NanoVLMs: How small can we go and still make coherent Vision Language Models? 

**Title (ZH)**: 纳米级VLMs：我们能将其缩小到什么程度，仍然能够构建出连贯的视觉语言模型？ 

**Authors**: Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat  

**Link**: [PDF](https://arxiv.org/pdf/2502.07838)  

**Abstract**: Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments. 

**Abstract (ZH)**: 视觉-语言模型（VLMs），如GPT-4V和Llama 3.2视觉模型，因其能够利用大型语言模型（LLMs）进行多模态任务而引起了广泛关注。然而，这些模型的潜能受到内在挑战的限制，包括专有性限制、巨大的计算需求以及有限的可访问性。小型模型，如GIT和BLIP，表现出明显的局限性，即便经过大量训练，也往往无法生成连贯和一致的文本超过几个词。这突显了一个关键问题：VLM能小到什么程度仍然能够产生流利和一致的文本？

受到3-4岁儿童卓越学习过程的启发，这些孩子依赖视觉线索来进行理解和交流，我们引入了两个新的数据集：ShortDesc（简洁的图像描述）和LongDesc（详细的图像描述）。这些数据集包含图像-文本对，其中文本仅限于儿童常用的简单词汇和语法，使用缩放后的模型GPT-4o生成。利用这些数据集，我们证明了可以训练出比现有最先进的（SOTA）小型VLM小10倍以上的模型，同时保持架构的简洁性。

为了评估输出结果，我们使用GPT-4o对文本进行评分，仿佛是学生所写的故事，评估其在创意性、意义性和一致性等方面的评分，满分10分。这种评估方法弥补了标准基准的局限性，能够容纳非结构化的输出，并从多维度评估模型的能力。我们的发现为在资源受限环境中开发轻量级和易获取的多模态模型做出了贡献。 

---
# Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights 

**Title (ZH)**: 将生成式大模型（LLM）生成的代码与需求桥接起来：反向生成技术及SBC度量标准以提供开发者洞见 

**Authors**: Ahilan Ayyachamy Nadar Ponnusamy  

**Link**: [PDF](https://arxiv.org/pdf/2502.07835)  

**Abstract**: The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.
AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub 

**Abstract (ZH)**: 大型语言模型（LLMs）在软件工程中的兴起，特别是在代码生成方面，引起了广泛关注。然而，评估由AI生成的代码质量依然是一个挑战，这主要是由于编程任务的内在复杂性和缺乏与人工判断高度一致的评价指标。传统的基于令牌的度量标准，如BLEU和ROUGE，在自然语言处理中常用，但在代码智能和验证任务中与人工评估的相关性较弱。此外，这些度量标准主要是针对研究设计的，未设计用于无缝集成到软件开发生命周期中，限制了其对寻求提高代码质量和安全性的开发者的实际应用。

对于高级开发人员，AI辅助编码显示出更大的益处，因为他们在批判性评估生成代码的正确性、完整性和合规性方面具有专业知识。相比之下，初级开发人员可能难以识别幻觉、缺失的功能或逻辑错误。为了弥合这一差距，本文提出了一种新的评分机制，称为SBC评分，该机制基于一种逆向生成技术，利用LLMs的自然语言生成能力。与直接代码分析不同，我们的方法从AI生成的代码中重构系统需求，并将其与原始规格进行比较，以量化准确性。SBC评分结合了语义相似度、BLEU和完整性分析，为开发者提供 actionable 的洞见，突出显示缺失的功能和幻觉。我们的代码和数据集可以在GitHub上获得。 

---
# MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures 

**Title (ZH)**: MEMHD：高效的多中心高维计算方法及其在充分利用的忆算架构中的应用 

**Authors**: Do Yeong Kang, Yeong Hwan Oh, Chanwook Hwang, Jinhee Kim, Kang Eun Jeon, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07834)  

**Abstract**: The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between highdimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency. 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

在内存计算（In-Memory Computing, IMC）架构上实现高维计算（Hyperdimensional Computing, HDC）面临着显著挑战，主要是由于高维向量与IMC阵列大小之间的不匹配，导致内存利用效率低下和计算周期增加。本文提出了一种名为MEMHD的内存高效多中心HDC框架，旨在解决这些挑战。MEMHD引入了一种基于聚类的初始化方法和量化感知迭代学习，以提高多重感知记忆性能。通过这些方法及其整体架构，MEMHD在内存需求显著减少的同时，保持或提高了分类准确性。我们的方法能够充分利用IMC阵列，并实现单次（或少次）关联搜索。实验结果表明，与最先进的二进制HDC模型相比，MEMHD在相同的内存使用下可实现高达13.69%的更高准确率，或在相同准确率水平下达到13.25倍的更高内存效率。此外，在映射到128x128阵列时，MEMHD将计算周期减少多达80倍，阵列使用减少多达71倍，同时显著提高能量和计算周期效率。 

---
# SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters 

**Title (ZH)**: SHARP：通过共享相邻层并恢复参数来加速语言模型推理 

**Authors**: Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian  

**Link**: [PDF](https://arxiv.org/pdf/2502.07832)  

**Abstract**: While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在自然语言处理任务上取得了进展，但它们不断增长的计算和内存需求使其在资源受限的设备（如移动电话）上的部署变得越来越具挑战性。在本文中，我们提出了一种名为SHARP（SHaring Adjacent Layers with Recovery Parameters）的新方法，通过在相邻层之间共享参数来加速LLM推理，从而减少内存负载开销，同时引入低秩恢复参数以保持模型性能。受连续层输出相似性观察的启发，SHARP采用了一种两阶段恢复过程：单层预热（SLW）和监督微调（SFT）。SLW阶段使用L_2损失对共享层的输出进行对齐，为随后的SFT阶段提供良好的初始化，以进一步恢复模型性能。广泛的实验表明，SHARP可以在不超过50,000个微调数据的情况下恢复各种分布内任务的困惑度，同时减少存储的MLP参数数量38%至65%。我们还进行了多组SHARP的消融研究，结果表明，将恢复参数替换到模型的后期部分可以更好地保持性能，而当参数计数相同时，不同的恢复参数化表现相似。此外，与原始的Llama2-7b模型相比，SHARP在移动设备上的模型存储量减少了42.8%，总推理时间减少了42.2%。我们的结果突显了SHARP在不需要预训练规模资源的情况下降低LLM推理成本的有效性。 

---
# Captured by Captions: On Memorization and its Mitigation in CLIP Models 

**Title (ZH)**: 标题翻译如下，符合学术规范：

被标题所困：关于CLIP模型中的记忆及其缓解

这句话具体分析了CLIP模型中由于标题（captions）带来的记忆现象，并探讨了如何缓解这种现象。在学术翻译中，我们保持了原文的意思，同时确保翻译的专业性和准确性。 

**Authors**: Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch  

**Link**: [PDF](https://arxiv.org/pdf/2502.07830)  

**Abstract**: Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with "mis-captioned" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease. 

**Abstract (ZH)**: 多模态模型（如CLIP）在视觉表示和文本表示之间的对齐方面表现出强大的性能，在图像检索和零样本分类等任务中表现出色。尽管取得了这些成功，这些模型如何利用训练数据，特别是记忆的作用机制仍不清楚。在单模态模型中，无论是有监督还是自我监督学习，都表明记忆对于泛化是至关重要的。然而，这些发现如何适用于CLIP尚不甚明了，CLIP结合了有监督学习（通过提供类似标签的监督信号的 captions）和自我监督学习（通过对比目标）的元素。为了弥合这一理解差距，我们提出了CLIP中记忆的正式定义（CLIPMem），并利用其量化CLIP模型中的记忆程度。研究结果表明，CLIP的记忆行为介于有监督和自我监督学习之间，“错误标注”的样本表现出最高的记忆水平。此外，我们发现文本编码器对记忆的影响大于图像编码器，这表明减轻策略应集中在文本领域。基于这些见解，我们提出了多种减少记忆同时提高实用性的策略，这是以往传统学习范式中从未展示过的，因为在传统学习范式中，减少记忆通常会导致实用性下降。 

---
# Some things to know about achieving artificial general intelligence 

**Title (ZH)**: 关于实现人工通用智能的一些需知事项 

**Authors**: Herbert Roitblat  

**Link**: [PDF](https://arxiv.org/pdf/2502.07828)  

**Abstract**: Current and foreseeable GenAI models are not capable of achieving artificial general intelligence because they are burdened with anthropogenic debt. They depend heavily on human input to provide well-structured problems, architecture, and training data. They cast every problem as a language pattern learning problem and are thus not capable of the kind of autonomy needed to achieve artificial general intelligence. Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent. Another barrier is the need to recognize that there are multiple kinds of problems, some of which cannot be solved by available computational methods (for example, "insight problems"). Current methods for evaluating models (benchmarks and tests) are not adequate to identify the generality of the solutions, because it is impossible to infer the means by which a problem was solved from the fact of its solution. A test could be passed, for example, by a test-specific or a test-general method. It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success. 

**Abstract (ZH)**: 当前及可预见的类人智能模型无法实现人工通用智能，因为它们受人类债务的制约。这些模型高度依赖人类输入以提供结构化的任务、架构和训练数据。它们将所有问题视为语言模式学习问题，因此缺乏实现人工通用智能所需的自主性。当前的模型之所以在任务中取得成功，是因为人类解决了大部分这些问题，使模型只需执行简单计算，如梯度下降。另一个障碍是需要认识到存在不同类型的问题，而其中一些问题目前无法通过现有的计算方法解决（例如，“洞察力问题”）。当前用于评估模型的方法（基准和测试）不足以识别解决方案的普适性，因为它不可能通过观察问题解决的结果来推断其解题方法。一个测试可以通过特定于测试的方法或一般于测试的方法来通过。从成功中推断解题方法是一种逻辑谬误（肯定后件）。 

---
# Implicit Language Models are RNNs: Balancing Parallelization and Expressivity 

**Title (ZH)**: 隐式语言模型实际上是RNN：平衡并行化与表达力 

**Authors**: Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow  

**Link**: [PDF](https://arxiv.org/pdf/2502.07827)  

**Abstract**: State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks. 

**Abstract (ZH)**: 状态空间模型（SSMs）和变换器主导了语言模型的领域。然而，它们在计算复杂度上受到限制，低于经典递归神经网络（RNNs），从而限制了其表达能力。相比之下，RNNs 在训练过程中缺乏并行化，这引发了关于并行化与表达能力之间权衡的基本问题。我们提出了隐式状态空间模型（Implicit SSMs），它们通过迭代变换直到收敛到固定点。理论上，我们证明了隐式状态空间模型实现了 RNNs 的非线性状态转换。在实验中，我们发现仅需近似的固定点收敛即可，从而能够设计出一种可扩展的训练 Curriculum，该 Curriculum 大幅保留了并行化能力，仅需对一小部分标记进行完整的收敛。我们的方法在正规语言的状态跟踪能力上展示了优越的表现，超过了变换器和 SSMs。我们进一步将隐式状态空间模型扩展到自然语言推理任务和大型语言模型的预训练，使用多达 1.3B 个参数和 207B 个标记，据我们所知，这代表了迄今为止训练的最大规模隐式模型。值得注意的是，我们的隐式模型在标准基准测试中表现优于其显式对应模型。 

---
# Pre-Trained Video Generative Models as World Simulators 

**Title (ZH)**: 预训练的视频生成模型作为世界模拟器 

**Authors**: Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, Ling Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07825)  

**Abstract**: Video generative models pre-trained on large-scale internet datasets have achieved remarkable success, excelling at producing realistic synthetic videos. However, they often generate clips based on static prompts (e.g., text or images), limiting their ability to model interactive and dynamic scenarios. In this paper, we propose Dynamic World Simulation (DWS), a novel approach to transform pre-trained video generative models into controllable world simulators capable of executing specified action trajectories. To achieve precise alignment between conditioned actions and generated visual changes, we introduce a lightweight, universal action-conditioned module that seamlessly integrates into any existing model. Instead of focusing on complex visual details, we demonstrate that consistent dynamic transition modeling is the key to building powerful world simulators. Building upon this insight, we further introduce a motion-reinforced loss that enhances action controllability by compelling the model to capture dynamic changes more effectively. Experiments demonstrate that DWS can be versatilely applied to both diffusion and autoregressive transformer models, achieving significant improvements in generating action-controllable, dynamically consistent videos across games and robotics domains. Moreover, to facilitate the applications of the learned world simulator in downstream tasks such as model-based reinforcement learning, we propose prioritized imagination to improve sample efficiency, demonstrating competitive performance compared with state-of-the-art methods. 

**Abstract (ZH)**: 预训练于大规模互联网数据集的视频生成模型已在生成逼真的合成视频方面取得了显著成功。然而，这些模型往往基于静态提示（如文本或图像）生成片段，这限制了它们建模互动和动态场景的能力。本文提出了一种名为动态世界模拟（DWS）的新方法，旨在将预训练的视频生成模型转变为可控的世界模拟器，能够执行指定的动作轨迹。为了实现条件动作与生成的视觉变化之间的精确对齐，我们引入了一个轻量级、通用的动作条件模块，可以无缝集成到任何现有模型中。我们证明，尽管不依赖于复杂的视觉细节，一致的动力转换建模是构建强大世界模拟器的关键。在这一洞见的基础上，我们还引入了一种运动增强损失，通过促使模型更有效地捕捉动态变化来增强动作可控性。实验结果表明，DWS 可以广泛应用于扩散模型和自回归Transformer模型，实现生成可控且动态一致的视频，并在游戏和机器人领域取得了显著提升。此外，为了在下游任务（如基于模型的强化学习）中促进学习到的世界模拟器的应用，我们提出了一种优先想象策略，以提高样本效率，并展示了其与先进方法相比具有竞争力的表现。 

---
# Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs 

**Title (ZH)**: 基于eFPGA的边缘推理可运行时调整Tsetlin机器 

**Authors**: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev  

**Link**: [PDF](https://arxiv.org/pdf/2502.07823)  

**Abstract**: Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugality. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference architecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on-chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The proposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application. 

**Abstract (ZH)**: 嵌入式现场可编程门阵列（eFPGAs）允许使用较低的能耗预算来设计边缘机器学习（ML）应用的硬件加速器，相比传统的FPGA平台具有优势。然而，eFPGA的有限逻辑和内存显著限制了计算能力和模型规模。因此，eFPGA上部署ML应用与最近开发的FPGA特定架构设计和最大化吞吐量而非资源效率的趋势形成了直接对比。本文关注这一权衡的另一面：提出的eFPGA加速器旨在最小化资源使用并允许在运行时对现场重新校准进行灵活性调整，而不牺牲吞吐量。这使得在不进行离线重组综合的情况下，能够在运行时改变模型规模、架构和输入数据维度成为可能。这一目标通过使用Tseltlin机（TM）算法的位级压缩推理架构实现。TM计算仅需位级的AND、OR、NOT、求和和加法操作，而不涉及任何乘法操作。此外，TM模型压缩使得整个模型可以完全置于eFPGA的片上块RAM中。本文利用此加速器提出了一种在场上的运行时模型调优策略。提出的方法相比目前最节省资源的设计使用了2.5倍更少的查找表（LUT）和3.38倍更少的寄存器，并且与运行相同ML应用的低功耗微控制器相比，实现了高达129倍的能量降低。 

---
# PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation 

**Title (ZH)**: PDM-SSD：具有点膨胀的一阶段三维目标检测器 

**Authors**: Ao Liang, Haiyang Hua, Jian Fang, Wenyu Chen, Huaici Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.07822)  

**Abstract**: Current Point-based detectors can only learn from the provided points, with limited receptive fields and insufficient global learning capabilities for such targets. In this paper, we present a novel Point Dilation Mechanism for single-stage 3D detection (PDM-SSD) that takes advantage of these two representations. Specifically, we first use a PointNet-style 3D backbone for efficient feature encoding. Then, a neck with Point Dilation Mechanism (PDM) is used to expand the feature space, which involves two key steps: point dilation and feature filling. The former expands points to a certain size grid centered around the sampled points in Euclidean space. The latter fills the unoccupied grid with feature for backpropagation using spherical harmonic coefficients and Gaussian density function in terms of direction and scale. Next, we associate multiple dilation centers and fuse coefficients to obtain sparse grid features through height compression. Finally, we design a hybrid detection head for joint learning, where on one hand, the scene heatmap is predicted to complement the voting point set for improved detection accuracy, and on the other hand, the target probability of detected boxes are calibrated through feature fusion. On the challenging Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, PDM-SSD achieves state-of-the-art results for multi-class detection among single-modal methods with an inference speed of 68 frames. We also demonstrate the advantages of PDM-SSD in detecting sparse and incomplete objects through numerous object-level instances. Additionally, PDM can serve as an auxiliary network to establish a connection between sampling points and object centers, thereby improving the accuracy of the model without sacrificing inference speed. Our code will be available at this https URL. 

**Abstract (ZH)**: 当前基于点的检测器仅能从提供的点中学习，具有有限的感受野和不足的全局学习能力。本文提出了一种新颖的用于单阶段3D检测的点膨胀机制（PDM-SSD），以此充分利用这两种表示方式。具体而言，我们首先使用点净风格的3D主干网络进行高效特征编码。然后，通过引入点膨胀机制（PDM）的颈部展开特征空间，这涉及两个关键步骤：点膨胀和特征填充。前者将采样点为中心在欧几里得空间中扩展为一定大小的网格。后者通过球谐系数和高斯密度函数在方向和尺度上填补未被占据的网格，以便进行反向传播。接下来，我们将多个膨胀中心关联并融合系数以通过高度压缩获得稀疏网格特征。最后，我们设计了一种混合检测头进行联合学习。一方面，预测场景热点图以补充投票点集以提高检测准确性；另一方面，通过特征融合校准检测框的目标概率。在具有挑战性的卡尔斯鲁厄理工学院和田纳技术研究所（KITTI）数据集中，PDM-SSD 在单模态方法中实现了多类检测的领先成果，推理速度为68帧。我们还通过多个物体实例展示了PDM-SSD在检测稀疏和不完整对象方面的优势。此外，PDM 可作为辅助网络建立采样点与物体中心之间的连接，从而在不牺牲推理速度的情况下提高模型的准确性。我们的代码将在此处提供: [此链接]. 

---
# Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection 

**Title (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection

遗忘作为一种催化剂，增强图像分类和目标检测中的黑盒像素攻击 

**Authors**: Dongsu Song, Daehwa Ko, Jay Hoon Jung  

**Link**: [PDF](https://arxiv.org/pdf/2502.07821)  

**Abstract**: It is well known that query-based attacks tend to have relatively higher success rates in adversarial black-box attacks. While research on black-box attacks is actively being conducted, relatively few studies have focused on pixel attacks that target only a limited number of pixels. In image classification, query-based pixel attacks often rely on patches, which heavily depend on randomness and neglect the fact that scattered pixels are more suitable for adversarial attacks. Moreover, to the best of our knowledge, query-based pixel attacks have not been explored in the field of object detection. To address these issues, we propose a novel pixel-based black-box attack called Remember and Forget Pixel Attack using Reinforcement Learning(RFPAR), consisting of two main components: the Remember and Forget processes. RFPAR mitigates randomness and avoids patch dependency by leveraging rewards generated through a one-step RL algorithm to perturb pixels. RFPAR effectively creates perturbed images that minimize the confidence scores while adhering to limited pixel constraints. Furthermore, we advance our proposed attack beyond image classification to object detection, where RFPAR reduces the confidence scores of detected objects to avoid detection. Experiments on the ImageNet-1K dataset for classification show that RFPAR outperformed state-of-the-art query-based pixel attacks. For object detection, using the MSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction to state-of-the-art query-based attack while requiring fewer query. Further experiments on the Argoverse dataset using YOLOv8 confirm that RFPAR effectively removed objects on a larger scale dataset. Our code is available at this https URL. 

**Abstract (ZH)**: 众所周知，基于查询的攻击在对抗性黑盒攻击中通常具有较高的成功率。尽管黑盒攻击的研究正在积极进行中，但相对较少的研究集中于仅针对有限像素的像素攻击。在图像分类中，基于查询的像素攻击往往依赖于拼图，这些拼图高度依赖于随机性，并忽视了散列像素对于对抗性攻击更为适合的事实。此外，据我们所知，基于查询的像素攻击在目标检测领域尚未被探索。为了解决这些问题，我们提出了一种基于像素的新型黑盒攻击方法——利用强化学习的记忆与遗忘像素攻击（RFPAR），该方法由两个主要组成部分：记忆和遗忘过程。RFPAR通过利用通过单一步骤的RL算法产生的奖励来扰动像素，从而减少了随机性和避免了拼图依赖。RFPAR有效地创建了扰动图像，这些图像在限制像素数量的前提下，可以最小化置信分数。此外，我们将提出的攻击方法从图像分类推广至目标检测领域，RFPAR通过降低检测对象的置信度来避免检测。在使用ImageNet-1K数据集的分类实验中，RFPAR的表现优于现有的基于查询的像素攻击。在使用MSCOCO数据集、YOLOv8和DDQ的目标检测实验中，RFPAR展示了与最先进的基于查询的攻击相当的mAP降低效果，但所需的查询次数较少。进一步使用Argoverse数据集的实验，采用YOLOv8验证了RFPAR在大规模数据集上有效移除了更大的物体。我们的代码可在以下链接获取：[代码链接]。 

---
# Low-Rank Compression for IMC Arrays 

**Title (ZH)**: 低秩压缩在IMC阵列中的应用 

**Authors**: Kang Eun Jeon, Johnny Rhe, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07820)  

**Abstract**: In this study, we address the challenge of low-rank model compression in the context of in-memory computing (IMC) architectures. Traditional pruning approaches, while effective in model size reduction, necessitate additional peripheral circuitry to manage complex dataflows and mitigate dislocation issues, leading to increased area and energy overheads. To circumvent these drawbacks, we propose leveraging low-rank compression techniques, which, unlike pruning, streamline the dataflow and seamlessly integrate with IMC architectures. However, low-rank compression presents its own set of challenges, namely i) suboptimal IMC array utilization and ii) compromised accuracy. To address these issues, we introduce a novel approach i) employing shift and duplicate kernel (SDK) mapping technique, which exploits idle IMC columns for parallel processing, and ii) group low-rank convolution, which mitigates the information imbalance in the decomposed matrices. Our experimental results demonstrate that our proposed method achieves up to 2.5x speedup or +20.9% accuracy boost over existing pruning techniques. 

**Abstract (ZH)**: 在本研究中，我们探讨了内存计算（IMC）架构下低秩模型压缩的挑战。传统的剪枝方法虽然在模型尺寸减少方面有效，但需要额外的辅助电路来管理复杂的数据流并缓解位置不对齐问题，从而导致面积和能量开销增加。为了克服这些缺点，我们建议利用低秩压缩技术，该技术与剪枝不同，可以简化数据流并无缝集成到IMC架构中。然而，低秩压缩也面临着自己的挑战，即i）IMC阵列利用率不足和ii）准确度下降。为了解决这些问题，我们提出了一个新颖的方法：i）采用移位和复制核（SDK）映射技术，该技术利用空闲的IMC列进行并行处理；ii）分组低秩卷积，该技术在分解矩阵中缓解信息不平衡问题。实验结果表明，我们的方法比现有剪枝技术实现了高达2.5倍的速度提升或20.9%的准确率提升。 

---
# Decoding Complexity: Intelligent Pattern Exploration with CHPDA (Context Aware Hybrid Pattern Detection Algorithm) 

**Title (ZH)**: 解码复杂性：基于CHPDA（情境感知混合模式检测算法）的智能模式探索 

**Authors**: Lokesh Koli, Shubham Kalra, Karanpreet Singh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07815)  

**Abstract**: Detecting sensitive data such as Personally Identifiable Information (PII) and Protected Health Information (PHI) is critical for data security platforms. This study evaluates regex-based pattern matching algorithms and exact-match search techniques to optimize detection speed, accuracy, and scalability. Our benchmarking results indicate that Google RE2 provides the best balance of speed (10-15 ms/MB), memory efficiency (8-16 MB), and accuracy (99.5%) among regex engines, outperforming PCRE while maintaining broader hardware compatibility than Hyperscan. For exact matching, Aho-Corasick demonstrated superior performance (8 ms/MB) and scalability for large datasets. Performance analysis revealed that regex processing time scales linearly with dataset size and pattern complexity. A hybrid AI + Regex approach achieved the highest F1 score (91. 6%) by improving recall and minimizing false positives. Device benchmarking confirmed that our solution maintains efficient CPU and memory usage on both high-performance and mid-range systems. Despite its effectiveness, challenges remain, such as limited multilingual support and the need for regular pattern updates. Future work should focus on expanding language coverage, integrating data security and privacy management (DSPM) with data loss prevention (DLP) tools, and enhancing regulatory compliance for broader global adoption. 

**Abstract (ZH)**: 检测个人可识别信息（PII）和受保护的健康信息（PHI）等敏感数据对于数据安全平台至关重要。本研究评估了基于正则表达式的模式匹配算法和精确匹配搜索技术，以优化检测速度、准确性和可扩展性。我们的基准测试结果表明，在正则表达式引擎中，Google RE2在速度（每兆字节10-15毫秒）、内存效率（8-16兆字节）和准确性（99.5%）方面提供了最佳平衡，优于PCRE，并且在硬件兼容性方面也优于Hyperscan。对于精确匹配，Aho-Corasick算法在大型数据集上表现出更优的性能（每兆字节8毫秒）和可扩展性。性能分析表明，正则表达式处理时间随着数据集大小和模式复杂性的增加而成线性增长。结合AI和正则表达式的混合方法实现了最高的F1分数（91.6%），通过提高召回率并最小化假阳性的数量。设备基准测试证实，我们的解决方案能够在高性能和中端系统上保持高效的CPU和内存使用。尽管这种方法非常有效，但仍存在一些挑战，如有限的多语言支持和需要定期更新模式。未来的工作应集中于扩展语言覆盖面，将数据安全和隐私管理（DSPM）与数据丢失防护（DLP）工具集成，以及增强监管合规性以促进更广泛的全球采用。 

---
# Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution 

**Title (ZH)**: 卫星观测引导的扩散模型用于任意分辨率下的准确气象状态估算 

**Authors**: Siwei Tu, Ben Fei, Weidong Yang, Fenghua Ling, Hao Chen, Zili Liu, Kun Chen, Hang Fan, Wanli Ouyang, Lei Bai  

**Link**: [PDF](https://arxiv.org/pdf/2502.07814)  

**Abstract**: Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km. 

**Abstract (ZH)**: 在任意位置准确获取地表气象条件对天气预报和气候模拟具有重要意义。由于基于卫星观测的气象状态通常以低分辨率网格场的形式提供，直接将空间插值应用于特定位置的气象状态，往往会与实际观测结果出现显著差异。现有高分辨率气象状态获取的降尺度方法通常忽视了与卫星观测的相关性。为解决这一问题，我们提出了一种由卫星观测引导的扩散模型（SGD），该模型基于ERA5再分析数据预训练，并利用卫星观测（GridSat）作为条件。通过零样本引导采样策略和基于块的方法，SGD用于采样降尺度气象状态。在训练过程中，我们提出通过注意机制将GridSat卫星观测信息融合到ERA5地图中，使SGD能够生成更为准确地反映实际情况的大气状态。在采样过程中，我们采用了可优化的卷积核来模拟放大过程，从而利用低分辨率ERA5地图和气象站观测生成高分辨率ERA5地图。此外，我们设计的基于块的方法促使SGD生成任意分辨率的气象状态。实验表明，SGD能够将气象状态降尺度至6.25公里分辨率。 

---
# CryptoX : Compositional Reasoning Evaluation of Large Language Models 

**Title (ZH)**: CryptoX：大型语言模型组合推理评估 

**Authors**: Jiajun Shi, Chaoren Wei, Liqun Yang, Zekun Moore Wang, Chenghao Yang, Ge Zhang, Stephen Huang, Tao Peng, Jian Yang, Zhoufutu Wen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07813)  

**Abstract**: The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs. 

**Abstract (ZH)**: 成分推理能力长期以来被认为是大型语言模型（LLM）泛化能力和智能涌现的关键。然而，尽管存在许多推理相关的基准，但在现有的基准中，LLM的成分推理能力很少被研究和量化。本文中，我们引入了一种评价框架CryptoX，这是首次将现有基准与密码学相结合，以量化LLM的成分推理能力。基于CryptoX，我们构建了CryptoBench，将这些原则整合到若干基准中，以便进行系统性的评估。我们使用CryptoBench对广泛使用的开源和封闭源LLM进行了详细的实验，揭示了开源和封闭源LLM之间存在巨大的差距。我们进一步进行了详细的机械可解释性实验，以揭示LLM成分推理的内在机制，包括子问题分解、子问题推理以及总结子问题结论。通过CryptoBench的分析，我们强调了独立研究成分推理的价值，并突显了增强LLM成分推理能力的必要性。 

---
# CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception 

**Title (ZH)**: CP-Guard+: 认知感知中恶意代理检测与防御的新范式 

**Authors**: Senkang Hu, Yihang Tao, Zihan Fang, Guowen Xu, Yiqin Deng, Sam Kwong, Yuguang Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07807)  

**Abstract**: Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple vehicles to share sensing information to enhance perception performance. However, compared with single-vehicle perception, the openness of a CP system makes it more vulnerable to malicious attacks that can inject malicious information to mislead the perception of an ego vehicle, resulting in severe risks for safe driving. To mitigate such vulnerability, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and demonstrate the superiority of CP-Guard+. 

**Abstract (ZH)**: 协作感知（Collaborative Perception, CP）是一种有前途的方法，可用于保障性连接与自主驾驶的安全，它使多辆车辆能够共享感知信息，从而提升感知性能。然而，与单车辆感知相比，CP系统的开放性使其更易受到恶意攻击，这些攻击可以通过注入恶意信息来误导ego车辆的感知，从而对安全驾驶造成严重风险。为了减轻这种脆弱性，我们首先提出了一种新的恶意代理检测范式，在特征层面有效识别恶意代理，而不需要验证最终的感知结果，显著减少了计算开销。在此基础上，我们引入了CP-GuardBench，这是首个用于训练和评估各种CP系统恶意代理检测方法的综合数据集。此外，我们开发了一种 robust 防御方法，称为CP-Guard+，通过精心设计的双中心对比损失（Dual-Centered Contrastive Loss, DCCLoss）增强了良性特征与恶意特征的表示之间的差距。最后，我们在CP-GuardBench和V2X-Sim上进行了广泛实验，并展示了CP-Guard+的优越性。 

---
# Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis 

**Title (ZH)**: 量子驱动的信用风险评估：一种基于混合量子-经典深度神经网络的行依赖预测分析新方法 

**Authors**: Rath Minati, Date Hema  

**Link**: [PDF](https://arxiv.org/pdf/2502.07806)  

**Abstract**: The integration of Quantum Deep Learning (QDL) techniques into the landscape of financial risk analysis presents a promising avenue for innovation. This study introduces a framework for credit risk assessment in the banking sector, combining quantum deep learning techniques with adaptive modeling for Row-Type Dependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed approach tailors predictive models to different loan categories, aiming to enhance the accuracy and efficiency of credit risk evaluation. While this work explores the potential of integrating quantum methods with classical deep learning for risk assessment, it focuses on the feasibility and performance of this hybrid framework rather than claiming transformative industry-wide impacts. The findings offer insights into how quantum techniques can complement traditional financial analysis, paving the way for further advancements in predictive modeling for credit risk. 

**Abstract (ZH)**: 将量子深度学习（QDL）技术整合到金融风险分析的框架中，为创新提供了有希望的途径。本研究介绍了一种框架，结合了量子深度学习技术与自适应建模方法，用于银行领域的信用风险评估，并利用行倚存预测分析（RTDPA）对不同贷款类别量身定制预测模型，旨在提高信用风险评估的准确性和效率。尽管本研究探讨了将量子方法与经典深度学习方法结合进行风险评估的潜力，但其重点在于这一混合框架的可行性和性能，而非宣称对整个行业产生颠覆性影响。研究结果为量子技术如何补充传统金融分析提供了见解，并为信用风险预测建模的进一步发展铺平了道路。 

---
# Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action 

**Title (ZH)**: 面向健康和医学领域的生成型人工智能和大型语言模型的监管科学创新：全球行动倡议 

**Authors**: Jasmine Chiat Ling Ong, Yilin Ning, Mingxuan Liu, Yian Ma, Zhao Liang, Kuldev Singh, Robert T Chang, Silke Vogel, John CW Lim, Iris Siu Kwan Tan, Oscar Freyer, Stephen Gilbert, Danielle S Bitterman, Xiaoxuan Liu, Alastair K Denniston, Nan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07794)  

**Abstract**: The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare. 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

将生成型人工智能（GenAI）和大规模语言模型（LLMs）与医疗保健的集成带来了前所未有的机遇与挑战，需要创新的监管方法。GenAI 和 LLMs 提供广泛的应用，从自动化临床工作流程到个性化诊断。然而，GenAI 和 LLMs 的非确定性输出、广泛的功能以及复杂集成挑战了现有的医疗器械监管框架，包括整个产品生命周期（TPLC）的方法。我们在这里讨论 TTLC 方法在基于 GenAI 和 LLM 的医疗器械监管中的局限性，并倡导全球合作的监管科学研究。这为开发创新方法奠定了基础，包括适应性政策和监管沙盒，用于在实际应用场景中测试和改进治理机制。国际协调，比如国际医疗器械监管机构论坛（IMDRF）所显示的协调性，对于管理 LLM 对全球健康的影响至关重要，包括因模型固有偏差导致的健康不平等加剧的风险。通过汇集多学科专长，优先采用迭代的数据驱动方法，并侧重于各类人群的需求，全球监管科学研究使 LLM 创新能够在医疗保健领域负责任和公平地发展。 

---
# Can Generative AI be Egalitarian? 

**Title (ZH)**: 生成式AI能够实现平等主义吗？ 

**Authors**: Philip Feldman, James R. Foulds, Shimei Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07790)  

**Abstract**: The recent explosion of "foundation" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism, while the potential for enormous profit has challenged technology organizations' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this "egalitarian" approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content. 

**Abstract (ZH)**: 近年来，“基础”生成型人工智能模型的兴起是建立在广泛从在线来源中获取价值的基础之上，往往缺乏相应的回馈。这种模式反映了并加剧了监视资本主义中的掠夺性做法，而巨大的利润潜力挑战了科技组织在负责任人工智能实践方面的承诺，引发了一系列重要的伦理和社会问题。然而，一种有前景的替代方案正在涌现：依赖用户自愿和协作贡献内容的模型。本文探讨了这种“平等主义”生成型人工智能的方法，借鉴维基百科的成功模式。我们探讨了这种方法对未来基础模型设计、开发及其限制的潜在影响。我们认为，这种做法不仅从伦理上是合理的，还可能导致更响应用户需求、训练数据更多样化且最终更符合社会价值观的模型。此外，我们也探讨了这一方法可能面临的挑战和限制，包括规模扩展、质量控制和潜在的志愿者贡献内容偏见等问题。 

---
# Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method 

**Title (ZH)**: 人工智能助手能否帮助学生撰写正式规范？基于ChatGPT和B-方法的研究 

**Authors**: Alfredo Capozucca, Daniil Yampolskyi, Alexander Goldberg, Maximiliano Cristiá  

**Link**: [PDF](https://arxiv.org/pdf/2502.07789)  

**Abstract**: This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications. 

**Abstract (ZH)**: 本文探讨了人工智能助手，特别是OpenAI的ChatGPT，在教学形式化方法（FM）方面的作用，利用B方法作为一种形式化规范技术。虽然现有研究表明AI在编码任务中的有效性，但目前没有任何研究报道其对形式化规范的影响。我们评估了ChatGPT在编写B规范时是否提供了优势，并分析了学生对其输出的信任度。研究发现，AI并没有帮助学生提高规范的正确性，低信任度反而与更好的结果相关联。此外，我们还识别出一种与ChatGPT交互的行为模式，这可能会影响B规范的正确性。 

---
# Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization 

**Title (ZH)**: 使用零样本学习和基于MaxSAT的故障定位指导的反例引导程序修复 

**Authors**: Pedro Orvalho, Mikoláš Janota, Vasco Manquinho  

**Link**: [PDF](https://arxiv.org/pdf/2502.07786)  

**Abstract**: Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools. 

**Abstract (ZH)**: 面向入门级编程作业（IPAs）的自动程序修复（APR）受到每年大量学生报名编程课程的影响而提出。由于为IPAs提供反馈需要大量时间和精力，教师常常建议学生修复程序中的错误。基于形式方法（Formal Methods, FM）的语义修复方法通过检查程序执行与测试套件或参考解之间的差异来发挥作用，但这些工具只能在正确的实现和错误的实现具有相同控制流图时修复程序。与此相对，大型语言模型（Large Language Models, LLMs）被用于APR，但通常会进行大量的修改而非最小化修改。这导致修复更加侵入性，使学生难以从中学习错误。总的来说，LLMs在完成字符串任务上表现出色，而基于FM的故障定位则在识别程序中的错误部分上表现出色。在本文中，我们提出了一种新颖的方法，通过零样本学习结合基于FM的故障定位和LLMs的优势，以增强IPAs的自动程序修复能力。该方法利用MaxSAT为基础的故障定位来识别程序中的错误部分，然后向LLM呈现一个不包含这些错误语句的程序草图。该混合方法遵循CEGIS循环，逐步改进程序。我们要求LLM合成缺失的部分，这些部分随后将与测试套件进行比较。如果建议的程序不正确，则将测试套件中的反例反馈给LLM。我们的实验表明，使用MaxSAT为基础的无错误程序草图指导反例驱动的方法显著提高了六种评估的LLMs的修复能力。该方法使得LLMs能够通过较小的修复来修复更多程序，优于其他配置和最先进的符号程序修复工具。 

---
# Machine Learning and Quantum Intelligence for Health Data Scenarios 

**Title (ZH)**: 机器学习与量子智能在健康数据场景中的应用 

**Authors**: Sanjeev Naguleswaran  

**Link**: [PDF](https://arxiv.org/pdf/2410.21339)  

**Abstract**: The advent of quantum computing has opened new possibilities in data science, offering unique capabilities for addressing complex, data-intensive problems. Traditional machine learning algorithms often face challenges in high-dimensional or limited-quality datasets, which are common in healthcare. Quantum Machine Learning leverages quantum properties, such as superposition and entanglement, to enhance pattern recognition and classification, potentially surpassing classical approaches. This paper explores QML's application in healthcare, focusing on quantum kernel methods and hybrid quantum-classical networks for heart disease prediction and COVID-19 detection, assessing their feasibility and performance. 

**Abstract (ZH)**: 量子计算的出现为数据科学带来了新的可能性，提供了处理复杂、数据密集型问题的独特能力。传统的机器学习算法在高维度或质量有限的数据集上常常面临挑战，这种情况在医疗健康领域尤为常见。量子机器学习利用量子特性，如叠加和纠缠，以增强模式识别和分类能力，有可能超越经典方法。本文探讨了量子机器学习在医疗健康领域的应用，重点介绍了用于心脏疾病预测和 COVID-19 检测的量子核方法和量子-经典混合网络，并评估了其可行性与性能。 

---
