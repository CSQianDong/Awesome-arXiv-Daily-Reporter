{'arxiv_id': 'arXiv:2502.17315', 'title': 'HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization', 'authors': 'Zhenghao Liu, Haolan Wang, Xinze Li, Qiushi Xiong, Xiaocui Yang, Yu Gu, Yukun Yan, Qi Shi, Fangfang Li, Ge Yu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.17315', 'abstract': 'Tabular data contains rich structural semantics and plays a crucial role in organizing and manipulating information. To better capture these structural semantics, this paper introduces the HybrId-modal Preference oPtimizatiOn (HIPPO) model, which represents tables using both text and image, and optimizes MLLMs to effectively learn more comprehensive table information from these multiple modalities. Specifically, HIPPO samples model responses from hybrid-modal table representations and designs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training. Experimental results on table question answering and table fact verification tasks demonstrate the effectiveness of HIPPO, achieving a 4% improvement over various table reasoning models. Further analysis reveals that HIPPO not only enhances reasoning abilities based on unimodal table representations but also facilitates the extraction of crucial and distinct semantics from different modal representations. All data and codes are available at this https URL.', 'abstract_zh': '表格数据富含丰富的结构语义，在组织和操作信息方面发挥着关键作用。为了更好地捕捉这些结构语义，本文引入了HybrId-modal Preference oPtimizatiOn（HIPPO）模型，该模型采用文本和图像两种方式表示表格，并通过优化多模态语言模型来从这些多种模态中学习更加全面的表格信息。具体而言，HIPPO 从混合模态的表格表示中抽样模型响应，并设计了一种模态一致的采样策略，以在 DPO 训练过程中增强响应的多样性和减轻模态偏差。在表格问答和表格事实验证任务上的实验结果表明，HIPPO 的有效性，相比各种表格推理模型，其性能提高了4%。进一步的分析表明，HIPPO 不仅能够在单模态表格表示的基础上增强推理能力，还能够促进从不同模态表示中提取关键且独特的语义。所有数据和代码可从以下链接获得：[提供链接]。', 'title_zh': 'HIPPO：通过混合模态偏好优化增强大型语言模型的表格理解能力'}
{'arxiv_id': 'arXiv:2502.16989', 'title': 'All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark', 'authors': 'Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini', 'link': 'https://arxiv.org/abs/2502.16989', 'abstract': "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark designed for fine-grained investigation of the reasoning abilities of visual language models on videos. MAIA differs from other available video benchmarks for its design, its reasoning categories, the metric it uses and the language and culture of the videos. It evaluates Vision Language Models (VLMs) on two aligned tasks: a visual statement verification task, and an open-ended visual question-answering task, both on the same set of video-related questions. It considers twelve reasoning categories that aim to disentangle language and vision relations by highlight when one of two alone encodes sufficient information to solve the tasks, when they are both needed and when the full richness of the short video is essential instead of just a part of it. Thanks to its carefully taught design, it evaluates VLMs' consistency and visually grounded natural language comprehension and generation simultaneously through an aggregated metric. Last but not least, the video collection has been carefully selected to reflect the Italian culture and the language data are produced by native-speakers.", 'abstract_zh': '我们引入了MAIA（多模态AI评估）基准，这是一种针对视觉语言模型在视频中推理能力进行精细研究的目的地意大利语基准。MAIA与其他可用的视频基准相比，在设计理念、推理类别、评价指标、视频的语言和文化背景等方面均有所不同。它在两个对齐的任务上评估视觉语言模型（VLMs）：一个是视觉陈述验证任务，另一个是开放性的视觉问答任务，这些任务针对的是相同的视频相关问题集。它考虑了十二个推理类别，旨在通过强调在两种信息中哪种单独的信息足以解决问题、两者是否都需要，以及整个短视频的重要性来分离语言与视觉之间的关系。得益于其精心设计，MAIA通过聚合指标同时评估VLMs的一致性和基于视觉的自然语言理解和生成能力。最后，视频集合经过仔细筛选，以反映意大利文化，语言数据由母语使用者生成。', 'title_zh': '一站式解决方案：MAIA基准测试在多模态推理中的理解与生成'}
{'arxiv_id': 'arXiv:2502.16636', 'title': 'Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries', 'authors': 'Yin Wu, Quanyu Long, Jing Li, Jianfei Yu, Wenya Wang', 'link': 'https://arxiv.org/abs/2502.16636', 'abstract': 'Retrieval-Augmented Generation (RAG) is a popular approach for enhancing Large Language Models (LLMs) by addressing their limitations in verifying facts and answering knowledge-intensive questions. As the research in LLM extends their capability to handle input modality other than text, e.g. image, several multimodal RAG benchmarks are proposed. Nonetheless, they mainly use textual knowledge bases as the primary source of evidences for augmentation. There still lack benchmarks designed to evaluate images as augmentation in RAG systems and how they leverage visual knowledge. We propose Visual-RAG, a novel Question Answering benchmark that emphasizes visual knowledge intensive questions. Unlike prior works relying on text-based evidence, Visual-RAG necessitates text-to-image retrieval and integration of relevant clue images to extract visual knowledge as evidence. With Visual-RAG, we evaluate 5 open-sourced and 3 proprietary Multimodal LLMs (MLLMs), revealing that images can serve as good evidence in RAG; however, even the SoTA models struggle with effectively extracting and utilizing visual knowledge', 'abstract_zh': '检索增强生成（RAG）是一种通过解决大型语言模型（LLMs）在事实验证和回答知识密集型问题方面的局限性来增强LLMs的流行方法。随着LLMs研究扩展其处理输入模态的能力，例如图像，已经提出了几种多模态RAG基准。然而，它们主要依赖于文本知识库作为增强的主要证据来源。尚未设计出专门评估图像作为RAG系统增强手段以及如何利用视觉知识的基准。我们提出了Visual-RAG，这是一种新的问答基准，侧重于视觉知识密集型的问题。与以往依赖于文本证据的工作不同，Visual-RAG 要求进行文本到图像的检索和将相关线索图像集成起来以提取视觉知识作为证据。通过Visual-RAG，我们评估了5个开源和3个专有的多模态大型语言模型（MLLMs），结果显示图像可以作为RAG的良好证据；然而，即使是当前最好的模型，在有效提取和利用视觉知识方面也存在困难。', 'title_zh': '视觉-RAG：增强生成文本到图像检索的视觉知识密集型查询基准测试'}
{'arxiv_id': 'arXiv:2502.16612', 'title': 'MemeIntel: Explainable Detection of Propagandistic and Hateful Memes', 'authors': 'Mohamed Bayan Kmainasi, Abul Hasnat, Md Arid Hasan, Ali Ezzat Shahroor, Firoj Alam', 'link': 'https://arxiv.org/abs/2502.16612', 'abstract': 'The proliferation of multimodal content on social media presents significant challenges in understanding and moderating complex, context-dependent issues such as misinformation, hate speech, and propaganda. While efforts have been made to develop resources and propose new methods for automatic detection, limited attention has been given to label detection and the generation of explanation-based rationales for predicted labels. To address this challenge, we introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in Arabic and hateful memes in English, making it the first large-scale resource for these tasks. To solve these tasks, we propose a multi-stage optimization approach and train Vision-Language Models (VLMs). Our results demonstrate that this approach significantly improves performance over the base model for both \\textbf{label detection} and explanation generation, outperforming the current state-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on Hateful Memes. For reproducibility and future research, we aim to make the MemeIntel dataset and experimental resources publicly available.', 'abstract_zh': '社交媒体上多模态内容的增长为理解与管理诸如虚假信息、仇恨言论和宣传等复杂、上下文依赖性的问题带来了显著挑战。尽管已经做出了努力来开发资源并提出新的自动检测方法，但对于标签检测以及生成基于解释的推理结果方面关注较少。为应对这一挑战，我们引入了MemeIntel，这是一个增强解释的数据集，涵盖了阿拉伯语宣传梗图和英语仇恨梗图，这是首个针对这些任务的大规模资源。为了完成这些任务，我们提出了一种多阶段优化方法，并训练了视觉-语言模型（VLMs）。我们的实验结果表明，该方法在标签检测和解释生成方面显著提高了基模型的表现，与当前最佳方法相比，在ArMeme数据集上绝对改进了约3%，在仇恨梗图数据集上绝对改进了约7%。为了确保可再现性和未来研究，我们计划公开MemeIntel数据集和实验资源。', 'title_zh': 'MemeIntel：可解释的 propaganda 和 hate 相关 meme 的检测'}
{'arxiv_id': 'arXiv:2502.16451', 'title': 'Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge', 'authors': 'Yang Jeong Park, Mayank Kumaran, Chia-Wei Hsu, Elsa Olivetti, Ju Li', 'link': 'https://arxiv.org/abs/2502.16451', 'abstract': "Artificial intelligence (AI) is increasingly used for the inverse design of materials, such as crystals and molecules. Existing AI research on molecules has integrated chemical structures of molecules with textual knowledge to adapt to complex instructions. However, this approach has been unattainable for crystals due to data scarcity from the biased distribution of investigated crystals and the lack of semantic supervision in peer-reviewed literature. In this work, we introduce a contrastive language-crystals model (CLaC) pre-trained on a newly synthesized dataset of 126k crystal structure-text pairs. To demonstrate the advantage of using synthetic data to overcome data scarcity, we constructed a comparable dataset extracted from academic papers. We evaluate CLaC's generalization ability through various zero-shot cross-modal tasks and downstream applications. In experiments, CLaC achieves state-of-the-art zero-shot generalization performance in understanding crystal structures, surpassing latest large language models.", 'abstract_zh': '人工智能（AI）在材料的逆向设计中得到了越来越多的应用，包括晶体和分子。现有的关于分子的AI研究将分子的化学结构与文本知识相结合，以适应复杂的指令需求。然而，这种方法由于实验研究中晶体数据分布偏斜且同行评审文献缺乏语义监督，对于晶体的应用一直难以实现。在本研究中，我们引入了一个对比语言-晶体模型（CLaC），该模型是在一个新合成的12.6万对晶体结构-文本配对数据集上预训练的。为了证明使用合成数据来克服数据稀缺性的好处，我们构建了一个与之相当的数据集，该数据集提取自学术论文。我们通过多种零样本跨模态任务和下游应用评估CLaC的泛化能力。在实验中，CLaC在理解和推断晶体结构方面达到了最先进的零样本泛化性能，超越了最新的大规模语言模型。', 'title_zh': '英语语言对比学习与晶体图谱在材料知识多模态表示中的应用'}
{'arxiv_id': 'arXiv:2502.16137', 'title': 'Chain-of-Description: What I can understand, I can put into words', 'authors': 'Jiaxin Guo, Daimeng Wei, Zongyao Li, Hengchao Shang, Yuanchang Luo, Hao Yang', 'link': 'https://arxiv.org/abs/2502.16137', 'abstract': 'In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of the vision benchmark MMMU\\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.', 'abstract_zh': '在本文中，我们提出了一种针对多模态大型语言模型的新型策略，称为链式描述（Chain-of-Description, CoD）提示。该方法要求模型首先对多模态输入进行详细的描述，然后再生成问题的答案。当应用于Qwen2-Audio、Qwen2-VL和Qwen2.5-VL等模型时，与常规提示方法相比，CoD提示显著提升了性能。这在音频基准AIR-Bench-Chat的语音类别中得到了约4%的改进，在视觉基准MMMU_Pro的困难级别部分得到了5.3%的改进。我们进一步的消融研究也验证了CoD提示的有效性。', 'title_zh': '描述链：我能理解的，我能用语言表达出来'}
{'arxiv_id': 'arXiv:2502.15954', 'title': 'MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning', 'authors': 'Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang', 'link': 'https://arxiv.org/abs/2502.15954', 'abstract': "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.", 'abstract_zh': '目的：通过改进示例选择来优化生物医学自然语言处理中的上下文学习。方法：我们提出了一种新型的多模式检索增强生成（MMRAG）框架，该框架结合了四种检索策略：（1）随机模式，随机选择示例；（2）顶级模式，根据相似性检索最相关示例；（3）多样性模式，确保所选示例的多样性；（4）类别模式，选择代表性示例。本研究将MMRAG应用于三个核心的生物医学自然语言处理任务：命名实体识别（NER）、关系提取（RE）和文本分类（TC）。使用的数据集包括：BC2GM用于基因和蛋白质提及识别（NER）、DDI用于药物-药物相互作用提取（RE）、GIT用于一般生物医学信息提取（RE）、HealthAdvice用于与健康有关的文本分类（TC）。该框架使用两个大型语言模型（Llama2-7B，Llama3-8B）和三种检索器（Contriever，MedCPT，BGE-Large）进行测试，以评估不同检索策略下的性能。结果：随机模式的结果表明，在提示中提供更多的示例可以提高模型的生成性能。同时，顶级模式和多样性模式在RE（DDI）任务上显著优于随机模式，取得了0.9669的F1分数，提高了26.4%。在三个测试的检索器中，Contriever在大多数实验中表现优于其他两个检索器。此外，Llama 2和Llama 3在不同任务中显示出了不同的能力，Llama 3在处理NER任务时显示出明显的优势。结论：MMRAG通过改进示例选择有效地提升了生物医学上下文学习，缓解了数据稀缺问题，并展示了在NLP驱动的健康医疗应用中优越的适应性。', 'title_zh': 'MMRAG：基于大型语言模型的多模式检索增强生成方法在生物医学领域内的上下文学习'}
{'arxiv_id': 'arXiv:2502.15910', 'title': 'Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models', 'authors': 'Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang', 'link': 'https://arxiv.org/abs/2502.15910', 'abstract': 'Generative models such as Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) trained on massive datasets can lead them to memorize and inadvertently reveal sensitive information, raising ethical and privacy concerns. While some prior works have explored this issue in the context of LLMs, it presents a unique challenge for MLLMs due to the entangled nature of knowledge across modalities, making comprehensive unlearning more difficult. To address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a novel unlearning framework for MLLMs designed to selectively clip neurons based on their relative importance to the targeted forget data, curated for different modalities. Specifically, MANU consists of two stages: important neuron selection and selective pruning. The first stage identifies and collects the most influential neurons across modalities relative to the targeted forget knowledge, while the second stage is dedicated to pruning those selected neurons. MANU effectively isolates and removes the neurons that contribute most to the forget data within each modality, while preserving the integrity of retained knowledge. Our experiments conducted across various MLLM architectures illustrate that MANU can achieve a more balanced and comprehensive unlearning in each modality without largely affecting the overall model utility.', 'abstract_zh': '生成模型，如大规模语言模型（LLMs）和多模态大规模语言模型（MLLMs），在大规模数据集上进行训练后，可能会记住并无意中泄露敏感信息，从而引发伦理和隐私方面的担忧。虽然一些先前的工作已经在LLMs的背景下探讨了这一问题，但由于不同模态之间知识的交织性质，这为MLLMs带来了一个独特的挑战，使得全面的学习更加困难。为了应对这一挑战，我们提出了模态感知神经元遗忘框架（Modality Aware Neuron Unlearning, MANU），这是一种专门为MLLMs设计的新颖遗忘框架，旨在根据与目标遗忘知识相关的重要性选择性地修剪神经元，不同模态之间进行分类。具体而言，MANU 包含两个阶段：重要神经元选择和选择性修剪。第一个阶段识别并收集与目标遗忘知识在各模态中关系最密切的神经元，而第二个阶段则专注于修剪选定的神经元。MANU 有效隔离并消除了各模态中对遗忘数据贡献最大的神经元，同时保持保留知识的完整性。我们在各种MLLM架构上进行的实验表明，MANU 能够在不对整体模型实用性产生重大影响的情况下，在每个模态上实现更平衡和更全面的遗忘。', 'title_zh': '多模态大型语言模型中的模态意识神经元修剪以实现遗忘'}
{'arxiv_id': 'arXiv:2502.17422', 'title': 'MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs', 'authors': 'Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski', 'link': 'https://arxiv.org/abs/2502.17422', 'abstract': "Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk.", 'abstract_zh': '近年来，多模态大语言模型（MLLMs）在视觉识别任务方面取得了快速的进步。鉴于它们在许多关键应用中的潜在整合，理解它们视觉感知的局限性变得十分重要。本文研究了MLLMs在回答图像问题时，是否能够像处理大对象一样有效地感知小视觉细节。我们观察到它们的表现对问题中的视觉主题大小非常敏感，并通过干预研究进一步表明这种效果实际上是因果关系。接下来，我们研究了MLLMs在回答视觉问题时的注意力模式，意外地发现即使它们提供了错误的答案，它们也总是能够知道应该看向哪里。基于这些发现，我们提出了一种无需训练的视觉干预方法，该方法利用任何MLLM本身的内部知识（以注意力图和梯度图的形式），来增强其对小视觉细节的感知。我们对两种广泛使用的MLLM和七个视觉问答基准进行了评估，结果显示这些方法能够在不进行训练的情况下显著提高MLLMs的准确率。我们的研究结果揭示了在视觉识别任务中应用MLLMs时对小细节的风险，并表明利用模型内部状态进行视觉干预是一种有望缓解这种风险的方向。', 'title_zh': 'MLLMs 知道该看向何处：基于多模态大语言模型的无训练识别小视觉细节'}
{'arxiv_id': 'arXiv:2502.16641', 'title': 'Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines', 'authors': 'Xinwei Long, Zhiyuan Ma, Ermo Hua, Kaiyan Zhang, Biqing Qi, Bowen Zhou', 'link': 'https://arxiv.org/abs/2502.16641', 'abstract': 'Retrieval-augmented generation (RAG) has emerged to address the knowledge-intensive visual question answering (VQA) task. Current methods mainly employ separate retrieval and generation modules to acquire external knowledge and generate answers, respectively. We propose ReAuSE, an alternative to the previous RAG model for the knowledge-based VQA task, which seamlessly integrates knowledge retriever into the generative multi-modal large language model, serving as a built-in search engine. Specifically, our model functions both as a generative retriever and an accurate answer generator. It not only helps retrieve documents from the knowledge base by producing identifiers for each document, but it also answers visual questions based on the retrieved documents. Furthermore, we propose a reinforced retrieval calibration module from relevance feedback to improve retrieval performance and align with the preferences for accurate answer generation. Extensive experiments on two representative OKVQA and A-OKVQA datasets demonstrate significant improvements ranging from 2.9\\% to 9.6\\% across all evaluation metrics when compared to strong baselines.', 'abstract_zh': '检索增强生成（RAG）方法已 emergence 用于解决知识密集型视觉问答（VQA）任务。当前的方法主要通过独立的检索和生成模块来获取外部知识并生成答案。我们提出了一种名为 ReAuSE 的替代性 RAG 模型，专门用于基于知识的 VQA 任务，能够在生成多模态大型语言模型中无缝集成知识检索器，充当内置搜索引擎。具体来说，我们的模型既可以作为生成检索器，又可以作为准确的答案生成器。它不仅通过为每个文档生成标识符来帮助从知识库检索文档，还可以基于检索到的文档来回答视觉问题。此外，我们还提出了一种强化检索校准模块，利用相关反馈来提高检索性能，并与准确答案生成的需求保持一致。在两个典型的 OKVQA 和 A-OKVQA 数据集上的广泛实验表明，与强基线相比，所有评估指标均取得了从 2.9% 到 9.6% 的显著改进。', 'title_zh': '内置自回归搜索引擎辅助的检索增强视觉问答'}
{'arxiv_id': 'arXiv:2502.16618', 'title': 'Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?', 'authors': 'Qipan Xu, Zhenting Wang, Xiaoxiao He, Ligong Han, Ruixiang Tang', 'link': 'https://arxiv.org/abs/2502.16618', 'abstract': 'Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.', 'abstract_zh': '生成式AI模型因其生成高质量内容的能力受到了广泛关注，但其在不当生成受版权保护材料方面引发的担忧也日益增长。尽管近期的研究提出了多种解决版权问题的方法，但大型视觉-语言模型（LVLMs）检测版权侵权的能力仍鲜有探索。在此项工作中，我们专注于利用多种图像样本评估当前最先进的LVLMs的版权检测能力。鉴于缺乏涵盖IP侵权样本和模糊的非侵权负面样本的全面数据集，我们构建了一个基准数据集，其中包括侵犯知名版权作品的正面样本，以及与其相似但不会引起版权担忧的负面样本。该数据集是通过高级提示工程技术构建的。随后，我们使用该基准数据集评估领先LVLMs。实验结果表明，LVLMs容易出现过拟合现象，导致一些负面样本被错误分类为IP侵权案例。在最终部分，我们分析了这些失败案例，并提出了潜在的解决方案以缓解过拟合问题。', 'title_zh': '大型视觉-语言模型能否检测来自生成式人工智能的图像版权侵权？'}
{'arxiv_id': 'arXiv:2502.16435', 'title': 'VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models', 'authors': 'Jen-Tse Huang, Dasen Dai, Jen-Yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, Zhaopeng Tu', 'link': 'https://arxiv.org/abs/2502.16435', 'abstract': "Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in multimodal understanding; however, their fundamental visual cognitive abilities remain largely underexplored. To bridge this gap, we introduce VisFactor, a novel benchmark derived from the Factor-Referenced Cognitive Test (FRCT), a well-established psychometric assessment of human cognition. VisFactor digitalizes vision-related FRCT subtests to systematically evaluate MLLMs across essential visual cognitive tasks including spatial reasoning, perceptual speed, and pattern recognition. We present a comprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro, and Qwen-VL, using VisFactor under diverse prompting strategies like Chain-of-Thought and Multi-Agent Debate. Our findings reveal a concerning deficiency in current MLLMs' fundamental visual cognition, with performance frequently approaching random guessing and showing only marginal improvements even with advanced prompting techniques. These results underscore the critical need for focused research to enhance the core visual reasoning capabilities of MLLMs. To foster further investigation in this area, we release our VisFactor benchmark at this https URL.", 'abstract_zh': '多模态大型语言模型（MLLMs）在多模态理解方面展现了显著的进步；然而，它们的基本视觉认知能力仍严重未被探索。为解决这一问题，我们引入了VisFactor，这是一种源自因素参照认知测试（FRCT）的新基准测试。FRCT是一种广泛认可的人类认知的心理测量评估方法。VisFactor将与视觉相关的FRCT子测试数字化，系统地评估MLLMs在包括空间推理、知觉速度和模式识别在内的关键视觉认知任务上的表现。我们使用VisFactor和多种提示策略（如链式思维和多智能体辩论）对最新的MLLMs，如GPT-4o、Gemini-Pro和Qwen-VL进行了全面评估。研究结果揭示了当前MLLMs在基本视觉认知方面存在令人担忧的缺陷，其表现为经常接近随机猜测，并且即使使用先进的提示技术，也只有微小的改进。这些结果强调了加强对MLLMs核心视觉推理能力研究的迫切需求。为了促进对该领域的进一步研究，我们在此发布VisFactor基准测试：[在此处填写URL]', 'title_zh': 'VisFactor: 多模态大型语言模型中基本视觉认知的基准测试'}
{'arxiv_id': 'arXiv:2502.16161', 'title': 'OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models', 'authors': 'Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, Xiang Bai', 'link': 'https://arxiv.org/abs/2502.16161', 'abstract': 'Visually-situated text parsing (VsTP) has recently seen notable advancements, driven by the growing demand for automated document understanding and the emergence of large language models capable of processing document-based questions. While various methods have been proposed to tackle the complexities of VsTP, existing solutions often rely on task-specific architectures and objectives for individual tasks. This leads to modal isolation and complex workflows due to the diversified targets and heterogeneous schemas. In this paper, we introduce OmniParser V2, a universal model that unifies VsTP typical tasks, including text spotting, key information extraction, table recognition, and layout analysis, into a unified framework. Central to our approach is the proposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves model performance across diverse scenarios by leveraging a unified encoder-decoder architecture, objective, and input\\&output representation. SPOT eliminates the need for task-specific architectures and loss functions, significantly simplifying the processing pipeline. Our extensive evaluations across four tasks on eight different datasets show that OmniParser V2 achieves state-of-the-art or competitive results in VsTP. Additionally, we explore the integration of SPOT within a multimodal large language model structure, further enhancing text localization and recognition capabilities, thereby confirming the generality of SPOT prompting technique. The code is available at \\href{this https URL}{AdvancedLiterateMachinery}.', 'abstract_zh': '视觉定位文本解析（VsTP）近年来取得了显著的增长，这主要得益于对自动化文档理解日益增长的需求以及能够处理基于文档的问题的大规模语言模型的出现。尽管提出了多种方法来应对VsTP的复杂性，但现有的解决方案往往依赖于特定任务的架构和目标。这导致了模态隔离和复杂的工作流程，由于目标多样和异构模式的不统一。在本文中，我们引入了OmniParser V2，这是一种统一模型，将VsTP典型的任务，包括文本检测、关键信息提取、表格识别和布局分析，统一到一个框架中。我们方法的核心是提出的结构化思路（SPOT）提示框架，通过利用统一的编码器-解码器架构、目标和输入/输出表示，提高了模型在各种场景下的性能。SPOT消除了任务特定架构和损失函数的需要，极大地简化了处理流程。我们在四个任务上八个不同数据集的广泛评估显示，OmniParser V2在VsTP中达到了最先进的或具有竞争力的表现。此外，我们在多模态大规模语言模型结构中探索了SPOT的集成，进一步增强文本定位和识别能力，从而确认了SPOT提示方法的普适性。代码可在以下链接获取：\\href{这个链接}{AdvancedLiterateMachinery}。', 'title_zh': 'OmniParser V2：统一视觉文本解析的结构化思维点及其在多模态大型语言模型中的普遍适用性'}
{'arxiv_id': 'arXiv:2502.15969', 'title': 'Forgotten Polygons: Multimodal Large Language Models are Shape-Blind', 'authors': 'William Rudman, Michal Golovanesky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh', 'link': 'https://arxiv.org/abs/2502.15969', 'abstract': "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: this https URL.", 'abstract_zh': '尽管多模态大型语言模型（MLLMs）在视觉语言任务上表现出色，但在数学问题解决方面仍面临挑战，开源和最先进的模型在视觉数学基准测试中的人类性能表现均未能达到。为了系统地检查MLLMs的视觉数学推理能力，我们进行了以下三个方面的研究：（1）评估它们对几何基本元素的理解；（2）测试多步推理；（3）探索提高视觉推理能力的潜在解决方案。我们的研究发现表明，在形状识别方面存在根本性缺陷，顶级模型在识别正多边形上的准确率低于50%。我们通过双重过程理论的视角分析了这些失败，表明MLLMs依赖于直觉和记忆化关联（系统1）而不是深思熟虑的推理（系统2）。因此，MLLMs无法正确计数熟悉的和新颖的形状的边数，表明它们既未学习边的概念也不有效地处理视觉输入。最终，我们提出了一种基于视觉提示的思维链（VC-CoT）提示方法，通过明确参考图表中的视觉注释来增强多步数学推理，从而将GPT-4o在非规则多边形边计数任务上的准确率从7%提高到93%。我们的研究结果表明，MLLMs中的系统2推理仍是一个开放问题，而视觉导向的提示对于成功激活视觉推理至关重要。相关代码可在以下链接获得：this https URL。', 'title_zh': '忽略的多边形：多模态大型语言模型对形状视而不见'}
{'arxiv_id': 'arXiv:2502.15895', 'title': 'Directional Gradient Projection for Robust Fine-Tuning of Foundation Models', 'authors': 'Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira', 'link': 'https://arxiv.org/abs/2502.15895', 'abstract': 'Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose Directional Gradient Projection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.', 'abstract_zh': '稳健的微调旨在适应大规模基础模型以适应下游任务，同时保持其在分布变化下的鲁棒性。现有方法主要集中在基于微调和预训练权重之间幅度约束和投影当前模型到预训练初始化，这通常需要大量的超参数调整，并且有时会导致欠拟合。在本工作中，我们提出了一种新颖的层级可训练方法——方向梯度投影（DiGraP），该方法将梯度的方向信息纳入正则化和多目标优化中。除了在图像分类中展示我们的方法外，我们还将其扩展到用于稳健微调的多模态评估设置。具体而言，我们首先通过对图像分类重新定义的视觉问答（VQA）基准进行分析，缩小了单模态和多模态之间的差距，并进一步将十个不同分布类型和程度的视觉问答（VQA）分布外（OOD）数据集分类为近分布外和远分布外。实验结果表明，DiGraP 在图像分类和 VQA 任务中，无论是使用判别式还是生成式骨干网络，都能在分布内（ID）泛化和分布外（OOD）鲁棒性方面持续优于现有的基线方法。', 'title_zh': '面向方向梯度投影的稳健微调基础模型方法'}
{'arxiv_id': 'arXiv:2502.17297', 'title': 'Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts', 'authors': 'Zhenghao Liu, Xingsheng Zhu, Tianshuo Zhou, Xinyi Zhang, Xiaoyuan Yi, Yukun Yan, Yu Gu, Ge Yu, Maosong Sun', 'link': 'https://arxiv.org/abs/2502.17297', 'abstract': 'This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at this https URL.', 'abstract_zh': '本文介绍了一种名为 Multi-Modal Retrieval-Augmented Generation (M^2RAG) 的基准测试，用于评估多模态大型语言模型（Multi-modal Large Language Models, MLLMs）在利用多模态检索文档中的知识方面的有效性。该基准测试包括四个任务：图像字幕生成、多模态问答、多模态事实验证和图像重新排序。所有任务均在开放领域环境中进行，要求 RAG 模型从多模态文档集合中检索与查询相关的信息，并将其用作 RAG 模型的输入上下文。为了增强 MLLMs 对上下文的利用能力，我们还介绍了一种名为 Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT) 的指令调优方法，该方法旨在优化 MLLMs 在多模态上下文中的表现。实验结果表明，MM-RAIT 通过使 RAG 系统能够有效地从多模态上下文中学习，从而提升了其性能。所有数据和代码均在此处提供：[提供的链接]。', 'title_zh': '在多模态上下文中的检索增强生成基准研究'}
{'arxiv_id': 'arXiv:2502.17132', 'title': 'Applications of Large Models in Medicine', 'authors': 'YunHe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu Yuxin Jia, Lujia Ge, Jing-min Yang', 'link': 'https://arxiv.org/abs/2502.17132', 'abstract': 'This paper explores the advancements and applications of large-scale models in the medical field, with a particular focus on Medical Large Models (MedLMs). These models, encompassing Large Language Models (LLMs), Vision Models, 3D Large Models, and Multimodal Models, are revolutionizing healthcare by enhancing disease prediction, diagnostic assistance, personalized treatment planning, and drug discovery. The integration of graph neural networks in medical knowledge graphs and drug discovery highlights the potential of Large Graph Models (LGMs) in understanding complex biomedical relationships. The study also emphasizes the transformative role of Vision-Language Models (VLMs) and 3D Large Models in medical image analysis, anatomical modeling, and prosthetic design. Despite the challenges, these technologies are setting new benchmarks in medical innovation, improving diagnostic accuracy, and paving the way for personalized healthcare solutions. This paper aims to provide a comprehensive overview of the current state and future directions of large models in medicine, underscoring their significance in advancing global health.', 'abstract_zh': '本文探讨了大规模模型在医疗领域的最新进展及其应用，特别关注医学生物大模型（MedLMs）。这些模型包括大型语言模型（LLMs）、视觉模型、3D大型模型以及多模态模型，正在通过增强疾病预测、诊断辅助、个性化治疗规划和药物发现等方面，重塑医疗卫生领域。在医学知识图谱和药物发现中集成图神经网络，突显了大型图模型（LGMs）在理解复杂的生物医学关系方面的潜力。研究还强调了视觉语言模型（VLMs）和3D大型模型在医学图像分析、解剖建模及假肢设计中的变革作用。尽管面临诸多挑战，这些技术为医疗创新设定了新的标杆，提高了诊断准确性，并铺平了个性化医疗解决方案的道路。本文旨在提供大规模模型在医学领域当前状态和未来发展方向的全面概述，强调其在推进全球健康方面的重要作用。', 'title_zh': '大型模型在医学中的应用'}
{'arxiv_id': 'arXiv:2502.16033', 'title': 'Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models', 'authors': 'Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, Xin Eric Wang', 'link': 'https://arxiv.org/abs/2502.16033', 'abstract': "Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting inconsistencies confined to a single modality, particularly in text, but struggle with cross-modal conflicts and complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.", 'abstract_zh': '现有的多模态大型语言模型（MLLMs）主要是在一致的视觉-文本输入上进行训练和测试，因此它们是否能够处理现实世界中布局丰富的内容中的不一致性仍然存在疑问。为了解决这一差距，我们提出了多模态不一致性推理（MMIR）基准，用以评估MLLMs检测和推理艺术品（如网页、演示文稿幻灯片和海报）中的语义不匹配的能力。MMIR包含534个具有挑战性的样本，每个样本包含在五个推理密集型类别中注入的合成错误：事实矛盾、身份误归因、上下文不匹配、数量差异和时空不符。\n\n我们评估了六种最先进的MLLMs，结果显示，具备专门多模态推理能力的模型（如o1）显著优于其同类模型，而开源模型则特别容易受到不一致性错误的影响。详细的错误分析进一步表明，模型在检测单一模态内的不一致性方面表现优异，特别是在文本方面，但在跨模态冲突和复杂布局方面则表现出色。探测实验揭示，单一模态提示方法，包括思维链（CoT）和集合标记（SoM）方法，仅带来微小的增益，显示出跨模态推理的关键瓶颈。我们的研究结果突显了高级多模态推理的必要性，并指出了未来多模态不一致性研究的方向。\n\n这种翻译符合学术规范，准确地传达了原文的意思，并且使用了合适的学术词汇。', 'title_zh': '多模态一致性推理 (MMIR): 一种新的多模态推理模型基准'}
{'arxiv_id': 'arXiv:2502.17434', 'title': 'V-HOP: Visuo-Haptic 6D Object Pose Tracking', 'authors': 'Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar', 'link': 'https://arxiv.org/abs/2502.17434', 'abstract': 'Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Our model and dataset will be made open source upon acceptance of the paper. Project website: this https URL', 'abstract_zh': '人类在操作过程中自然地整合视觉和触觉信息，以实现稳健的物体感知。失去任意一种模态都会显著降低性能。受这种多感官整合机制的启发，先前的研究试图结合视觉和触觉/触觉反馈来估计物体姿态。尽管这些工作在受控环境或合成数据集上展示了性能提升，但在实际应用中，它们往往由于难以泛化到不同抓持器、传感器布局或仿真的真实环境中而表现不佳。此外，它们通常独立地估计每一帧的物体姿态，导致在实际部署中序列跟踪不够连贯。为了解决这些局限性，我们提出了一种新型的统一触觉表示，能够有效处理多种抓持器。基于这种表示，我们引入了一种新的以视觉和触觉输入为基础的物体姿态追踪器，可以无缝地整合视觉和触觉输入。我们在我们的数据集和Feelsight数据集中验证了该框架，展示了在各种复杂序列上的显著性能提升。值得注意的是，我们的方法在新颖的抓持器、物体和传感器类型（包括基于点阵和基于视觉的触觉传感器）方面实现了更好的泛化和鲁棒性。在实际实验中，我们展示了我们的方法在许多方面超过了现有最佳的视觉追踪器，显示出明显的优势。此外，我们还证明可以通过将实时物体跟踪结果整合到运动规划中来实现精确的操作任务，进一步突显了视听触觉感知的优势。在接受该论文后，我们的模型和数据集将公开发布。项目网站：this https URL', 'title_zh': 'V-HOP：视觉-触觉6D物体姿态追踪'}
{'arxiv_id': 'arXiv:2502.17028', 'title': 'Distributional Vision-Language Alignment by Cauchy-Schwarz Divergence', 'authors': 'Wenzhe Yin, Zehao Xiao, Pan Zhou, Shujian Yu, Jiayi Shen, Jan-Jakob Sonke, Efstratios Gavves', 'link': 'https://arxiv.org/abs/2502.17028', 'abstract': 'Multimodal alignment is crucial for various downstream tasks such as cross-modal generation and retrieval. Previous multimodal approaches like CLIP maximize the mutual information mainly by aligning pairwise samples across modalities while overlooking the distributional differences, leading to suboptimal alignment with modality gaps. In this paper, to overcome the limitation, we propose CS-Aligner, a novel and straightforward framework that performs distributional vision-language alignment by integrating Cauchy-Schwarz (CS) divergence with mutual information. In the proposed framework, we find that the CS divergence and mutual information serve complementary roles in multimodal alignment, capturing both the global distribution information of each modality and the pairwise semantic relationships, yielding tighter and more precise alignment. Moreover, CS-Aligher enables incorporating additional information from unpaired data and token-level representations, enhancing flexible and fine-grained alignment in practice. Experiments on text-to-image generation and cross-modality retrieval tasks demonstrate the effectiveness of our method on vision-language alignment.', 'abstract_zh': '多模态对齐对于跨模态生成和检索等多种下游任务至关重要。之前的多模态方法，如CLIP主要通过跨模态对齐成对样本来最大化互信息，但忽视了不同模态的分布差异，导致存在模态差距时的对齐效果欠佳。为克服这一局限，本文提出了一种新颖且简便的框架CS-Aligner，该框架通过结合柯西-施瓦茨（CS）散度与互信息来进行分布级别的跨模态对齐。在所提出的框架中，我们发现CS散度和互信息在多模态对齐中发挥了互补的作用，分别捕捉每个模态的全局分布信息及其成对的语义关系，从而实现更紧密和精确的对齐。此外，CS-Aligner还能够整合未配对数据及令牌级表示，增强实际应用中的灵活和精细对齐。实验结果表明，该方法在视觉-语言对齐上具有有效性，特别是在文本到图像生成和跨模态检索任务中。', 'title_zh': '基于柯西-施瓦茨偏差的分布视知觉对齐'}
{'arxiv_id': 'arXiv:2502.16718', 'title': 'NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction', 'authors': 'Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia Fermüller', 'link': 'https://arxiv.org/abs/2502.16718', 'abstract': "Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans' multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at this https URL to support future HRI research.", 'abstract_zh': '近年来，多模态人机交互（HRI）数据集的进展强调了语音和手势的整合，使机器人能够吸收显性知识和隐性理解。然而，现有的数据集主要集中在诸如物体指向和推动等初级任务上，限制了它们在复杂领域的应用。这些数据集优先处理简单的口头命令数据，但较少关注训练机器人正确理解任务并做出适当响应。为弥补这些不足，我们提出了NatSGLD数据集，该数据集采用巫师术（Wizard of Oz, WoZ）方法收集，参与者与他们认为是自主的机器人进行交互。NatSGLD记录了人类的多模态命令（包括语音和手势），每个命令都配有一条演示轨迹和一个线性时序逻辑（Linear Temporal Logic, LTL）公式，从而提供任务命令的地面真值解释。该数据集为人类机器人交互和机器学习交叉领域的研究提供了基础资源。通过提供多模态输入和详细的注释，NatSGLD在多模态指令跟随、计划识别和基于演示的人工辅助强化学习等领域支持深入研究。我们在此许可协议下（MIT License）发布数据集和代码，该发布链接为[此链接](此链接应替换为实际的URL)，以支持未来的HRI研究。', 'title_zh': 'NatSGLD：一种用于自然人机交互中机器人学习的数据集，包含语音、手势、逻辑和演示内容'}
{'arxiv_id': 'arXiv:2502.16707', 'title': 'Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation', 'authors': 'Yunhai Feng, Jiaming Han, Zhuoran Yang, Xiangyu Yue, Sergey Levine, Jianlan Luo', 'link': 'https://arxiv.org/abs/2502.16707', 'abstract': 'Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs\' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a "reflection" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at this https URL.', 'abstract_zh': '解决复杂的长期 horizon 机器人操作问题需要高级的规划能力、对物理世界的推理能力以及能够根据情况选择合适的运动技能。互联网数据预训练的语言视觉模型 (VLMs) 在原则上可以提供解决这类问题的框架。然而，目前的 VLMs 缺乏执行机器人操作所需的精妙的物理理解能力，也无法在长时间范围内进行推理以解决累积错误的问题。在这篇论文中，我们介绍了一种新的测试时计算框架，该框架可以增强 VLMs 在多阶段操作任务中的物理推理能力。我们的方法的核心在于，通过一种“反思”机制逐迭代地改进预训练的 VLM——使用生成模型设想未来的世界状态，利用这些预测来引导行为选择，并对潜在的非优化性进行批判性反思以精炼它们的推理。实验结果表明，我们的方法显著优于几种最先进的商业 VLMs 以及蒙特卡洛树搜索 (MCTS) 等其他后训练方法。更多视频可以在以下网址获取：这个 https URL。', 'title_zh': '反思性规划：多阶段长期 horizon 机器人操作的视觉-语言模型'}
{'arxiv_id': 'arXiv:2502.16602', 'title': 'VidLBEval: Benchmarking and Mitigating Language Bias in Video-Involved LVLMs', 'authors': 'Yiming Yang, Yangyang Guo, Hui Lu, Yan Wang', 'link': 'https://arxiv.org/abs/2502.16602', 'abstract': 'Recently, Large Vision-Language Models (LVLMs) have made significant strides across diverse multimodal tasks and benchmarks. This paper reveals a largely under-explored problem from existing video-involved LVLMs - language bias, where models tend to prioritize language over video and thus result in incorrect responses. To address this research gap, we first collect a Video Language Bias Evaluation Benchmark, which is specifically designed to assess the language bias in video-involved LVLMs through two key tasks: ambiguous video contrast and interrogative question probing. Accordingly, we design accompanied evaluation metrics that aim to penalize LVLMs being biased by language. In addition, we also propose Multi-branch Contrastive Decoding (MCD), introducing two expert branches to simultaneously counteract language bias potentially generated by the amateur text-only branch. Our experiments demonstrate that i) existing video-involved LVLMs, including both proprietary and open-sourced, are largely limited by the language bias problem; ii) our MCD can effectively mitigate this issue and maintain general-purpose capabilities in various video-involved LVLMs without any additional retraining or alteration to model architectures.', 'abstract_zh': '近年来，大型多模态语言-视觉模型（LVLMs）在各种跨模态任务和基准测试中取得了显著进展。本文揭示了现有涉及视频的LVLMs中一个很大程度上未被探索的问题——语言偏见，即模型倾向于优先考虑文本信息而忽视视频信息，从而导致错误的响应。为解决这一研究空白，我们首先构建了一个视频语言偏见评估基准，该基准专为评估涉及视频的LVLMs的语言偏见而设计，通过两个关键任务：模棱两可的视频对比和问句探查来实现。相应地，我们设计了配套的评估指标，旨在惩罚模型受到语言偏见的影响。此外，我们还提出了多分支对比解码（MCD），引入了两个专家分支，以同时对抗由业余文本分支可能产生的语言偏见。实验结果表明，i) 当前涉及视频的LVLMs，无论是否开源，都主要受限于语言偏见问题；ii) 我们的MCD能够有效缓解这一问题，并在不进行额外的再训练或更改模型架构的情况下保持各类涉及视频的LVLMs的通用能力。', 'title_zh': 'VidLBEval：评估与缓解涉及视频的预训练语言模型中的语言偏见'}
{'arxiv_id': 'arXiv:2502.16548', 'title': 'Composable Strategy Framework with Integrated Video-Text based Large Language Models for Heart Failure Assessment', 'authors': 'Jianzhou Chen, Xiumei Wang, Jinyang Sun, Xi Chen, Heyu Chu, Guo Song, Yuji Luo, Xingping Zhou, Rong Gu', 'link': 'https://arxiv.org/abs/2502.16548', 'abstract': 'Heart failure is one of the leading causes of death worldwide, with millons of deaths each year, according to data from the World Health Organization (WHO) and other public health agencies. While significant progress has been made in the field of heart failure, leading to improved survival rates and improvement of ejection fraction, there remains substantial unmet needs, due to the complexity and multifactorial characteristics. Therefore, we propose a composable strategy framework for assessment and treatment optimization in heart failure. This framework simulates the doctor-patient consultation process and leverages multi-modal algorithms to analyze a range of data, including video, physical examination, text results as well as medical history. By integrating these various data sources, our framework offers a more holistic evaluation and optimized treatment plan for patients. Our results demonstrate that this multi-modal approach outperforms single-modal artificial intelligence (AI) algorithms in terms of accuracy in heart failure (HF) prognosis prediction. Through this method, we can further evaluate the impact of various pathological indicators on HF prognosis,providing a more comprehensive evaluation.', 'abstract_zh': '心力衰竭是全球范围内导致死亡的主要原因之一，根据世界卫生组织（WHO）和其他公共卫生机构的数据，每年有数百万人因此丧生。尽管在心力衰竭领域已经取得了显著进展，提高了生存率和射血分数，但由于其复杂性和多因素特征，仍然存在大量的未满足需求。因此，我们提出了一种可组合的策略框架，用于心力衰竭的评估与治疗优化。该框架模拟了医生-患者咨询的过程，并利用多模态算法分析视频、体格检查、文本结果以及病史等多种数据。通过整合这些多种数据源，该框架提供了更为全面的评估和优化治疗方案。我们的结果显示，这种多模态方法在心力衰竭（HF）预后预测准确性上优于单一模态的人工智能（AI）算法。通过这种方法，我们可以进一步评估各种病理指标对HF预后的影响，提供更全面的评估。', 'title_zh': '具有集成视频-文本大型语言模型的可组合策略框架在心力衰竭评估中的应用'}
{'arxiv_id': 'arXiv:2502.16483', 'title': 'A Split-Window Transformer for Multi-Model Sequence Spammer Detection using Multi-Model Variational Autoencoder', 'authors': 'Zhou Yang, Yucai Pang, Hongbo Yin, Yunpeng Xiao', 'link': 'https://arxiv.org/abs/2502.16483', 'abstract': "This paper introduces a new Transformer, called MS$^2$Dformer, that can be used as a generalized backbone for multi-modal sequence spammer detection. Spammer detection is a complex multi-modal task, thus the challenges of applying Transformer are two-fold. Firstly, complex multi-modal noisy information about users can interfere with feature mining. Secondly, the long sequence of users' historical behaviors also puts a huge GPU memory pressure on the attention computation. To solve these problems, we first design a user behavior Tokenization algorithm based on the multi-modal variational autoencoder (MVAE). Subsequently, a hierarchical split-window multi-head attention (SW/W-MHA) mechanism is proposed. The split-window strategy transforms the ultra-long sequences hierarchically into a combination of intra-window short-term and inter-window overall attention. Pre-trained on the public datasets, MS$^2$Dformer's performance far exceeds the previous state of the art. The experiments demonstrate MS$^2$Dformer's ability to act as a backbone.", 'abstract_zh': '本文介绍了一种新的Transformer模型，称为MS$^2$Dformer，它可以作为多模态序列垃圾邮件检测的通用骨干。垃圾邮件检测是一项复杂的多模态任务，因此应用Transformer模型面临的挑战主要有两个方面。首先，多模态噪声信息可能干扰特征提取；其次，用户历史行为的超长序列也会对注意力计算造成巨大的GPU内存压力。为了解决这些问题，我们首先设计了一个基于多模态变分自编码器（MVAE）的用户行为Token化算法。随后，我们提出了一种层次分割窗口多头注意力（SW/W-MHA）机制。分割窗口策略将超长序列层次地转换为窗口内短期关注与窗口间整体关注的组合。在公有数据集上进行预训练后，MS$^2$Dformer的性能远超现有最佳水平。实验结果表明，MS$^2$Dformer能够作为通用骨干发挥作用。', 'title_zh': '使用多模型变分自编码器进行多模型序列垃圾信息发布者检测的_split窗口变压器研究'}
{'arxiv_id': 'arXiv:2502.16469', 'title': 'Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment', 'authors': 'Zeyu Shangguan, Daniel Seita, Mohammad Rostami', 'link': 'https://arxiv.org/abs/2502.16469', 'abstract': 'Advancements in cross-modal feature extraction and integration have significantly enhanced performance in few-shot learning tasks. However, current multi-modal object detection (MM-OD) methods often experience notable performance degradation when encountering substantial domain shifts. We propose that incorporating rich textual information can enable the model to establish a more robust knowledge relationship between visual instances and their corresponding language descriptions, thereby mitigating the challenges of domain shift. Specifically, we focus on the problem of Cross-Domain Multi-Modal Few-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-based framework designed to leverage rich textual semantics as an auxiliary modality to achieve effective domain adaptation. Our new architecture incorporates two key components: (i) A multi-modal feature aggregation module, which aligns visual and linguistic feature embeddings to ensure cohesive integration across modalities. (ii) A rich text semantic rectification module, which employs bidirectional text feature generation to refine multi-modal feature alignment, thereby enhancing understanding of language and its application in object detection. We evaluate the proposed method on common cross-domain object detection benchmarks and demonstrate that it significantly surpasses existing few-shot object detection approaches.', 'abstract_zh': '跨模态特征提取与集成的进展显著提升了少样本学习任务的性能。然而，当前的多模态物体检测（MM-OD）方法在遇到显著领域迁移时往往表现出明显的性能下降。我们提出，通过引入丰富的文本信息，可以使模型在视觉实例与其对应的语言描述之间建立更 robust 的知识关系，从而减轻领域迁移的挑战。具体而言，我们专注于跨领域多模态少样本物体检测（CDMM-FSOD）的问题，并提出了一种基于元学习的框架，旨在利用丰富的文本语义作为辅助模态，实现有效的领域自适应。我们新的架构包含两个关键组件：（i）一个多模态特征聚合模块，该模块对齐视觉和语言特征嵌入，确保不同模态之间的一致集成；（ii）一个丰富的文本语义校正模块，该模块通过双向文本特征生成来细化多模态特征对齐，从而增强对语言及其在物体检测中的应用的理解。我们对常见的跨域物体检测基准进行了评估，并证明了该方法明显优于现有的少样本物体检测方法。', 'title_zh': '跨领域少样本对象检测的多模态文本增强方法'}
{'arxiv_id': 'arXiv:2502.16428', 'title': 'Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT', 'authors': 'Nidhal Jegham, Marwan Abdelatti, Abdeltawab Hendawi', 'link': 'https://arxiv.org/abs/2502.16428', 'abstract': 'Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.', 'abstract_zh': '传统的多模态大型语言模型（LLMs）评估主要集中在单张图像的推理上，忽视了上下文理解、推理稳定性以及不确定性校准等关键方面。本研究通过引入一个新颖的基准来解决这些问题，该基准结合了多图像推理任务、基于拒绝的评价以及位置偏差检测。为了评估这些维度，我们进一步引入了熵作为量化不同变体推理一致性的新指标。我们将这个基准应用于八个视觉推理任务中，包括差分识别和图示解释，评估了Grok 3、ChatGPT-4o、ChatGPT-o1、Gemini 2.0 Flash Experimental、DeepSeek Janus 模型、Qwen2.5-VL-72B-Instruct、QVQ-72B-Preview 和 Pixtral 12B。研究结果表明，ChatGPT-o1 在整体准确度（82.5%）和拒绝准确度（70.0%）方面领先，Gemini 2.0 Flash Experimental 紧随其后（70.8%）。QVQ-72B-Preview 在拒绝准确度方面表现尤为优异（85.5%）。值得注意的是，Pixtral 12B 在特定领域表现出潜力，而 Janus 模型在偏见和不确定性校准方面存在挑战，表现为低拒绝准确度和高熵分数。Janus 模型（如 Janus 7B 的熵为 0.8392，Janus 1B 的熵为 0.787）的高熵分数表明它们易受位置偏差和推理不稳定性的影响，与 ChatGPT 模型的低熵和稳健推理形成鲜明对比。研究进一步表明，模型大小并不是决定性能的唯一因素，尽管 Grok 3 的参数数量庞大，但其性能却不及预期。通过运用多图像上下文、拒绝机制和基于熵的一致性度量，这一基准为评估多模态LLMs 设定了新的标准，使其能够提供更加稳健和可靠的下一代AI系统的评估。', 'title_zh': '“Grok、Deepseek Janus、Gemini、Qwen、Mistral 和 ChatGPT 的视觉推理评估”'}
{'arxiv_id': 'arXiv:2502.16359', 'title': 'Audio Visual Segmentation Through Text Embeddings', 'authors': 'Kyungbok Lee, You Zhang, Zhiyao Duan', 'link': 'https://arxiv.org/abs/2502.16359', 'abstract': "The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from the video frames. Researchers working on AVS suffer from limited datasets because hand-crafted annotation is expensive. Recent works attempt to overcome the challenge of limited data by leveraging the segmentation foundation model, SAM, prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model's burden on understanding visual modality by utilizing pre-trained knowledge of SAM, it does not address the fundamental challenge of the limited dataset for learning audio-visual relationships. To address these limitations, we propose \\textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\\mathbf{\\textit{\\textbf{f}}_{CLIP} \\odot \\textit{\\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Experiments on the AVSBench dataset demonstrate state-of-the-art performance on both datasets of AVSBench. Our approach outperforms existing methods by effectively utilizing pretrained segmentation models and cross-modal semantic alignment.", 'abstract_zh': '音频-视觉分割（AVS）的目标是从视频帧中定位和分割发声源对象。研究AVS的学者面临数据集有限的问题，因为手工标注成本高昂。近期的工作通过利用分割基础模型SAM，并辅以音频输入以增强其分割发声源对象的能力，来应对数据有限的挑战。虽然这种方法通过利用预训练知识减轻了模型对视觉模态的理解负担，但它并未解决学习音频-视觉关系的基本数据集不足问题。为解决这些限制，我们提出了一种名为**AV2T-SAM**的新型框架，该框架将音频特征与预训练文本引导SAM的文本嵌入空间相连接。我们的方法利用丰富的文本-图像配对数据集中的多模态对应关系，增强音频-视觉对齐。此外，我们引入了一个新的特征**$\\mathbf{f_{CLIP} \\odot f_{CLAP}}$**，该特征强调了音频和视觉模态的共享语义，并过滤掉无关噪声。在AVSBench数据集上的实验表明，我们的方法在AVSBench的数据集上达到了最先进的性能。我们的方法通过有效地利用预训练分割模型和跨模态语义对齐，优于现有方法。', 'title_zh': '通过文本嵌入进行音频视频分割'}
{'arxiv_id': 'arXiv:2502.16284', 'title': 'MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra', 'authors': 'Liang Wang, Shaozhen Liu, Yu Rong, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang', 'link': 'https://arxiv.org/abs/2502.16284', 'abstract': "Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in a significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer a more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using a contrastive objective, we enhance the 3D encoder's understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics.", 'abstract_zh': '建立三维结构与分子系统能量状态之间的关系已被证明是一种有前途的方法，用于学习三维分子表示。然而，现有方法仅限于从经典力学角度建模分子能量状态，这一限制导致对量子力学效应（如分立的能量级结构）的重大忽视。这些量子力学效应提供了更准确的能量估计，并可以通过能量谱实验测量。在本文中，我们提出利用能量谱增强三维分子表示的预训练（MolSpectra），从而将量子力学的知识注入分子表示中。具体而言，我们提出了一种基于遮罩片段重建的多谱编码器（SpecFormer），用于编码分子光谱。通过进一步使用对比学习目标对3D编码器和谱编码器的输出进行对齐，增强3D编码器对分子的理解。在公共基准上的评估表明，我们的预训练表示在预测分子性质和建模动力学方面优于现有方法。', 'title_zh': 'MolSpectra：基于多模态能量光谱预训练三维分子表示'}
{'arxiv_id': 'arXiv:2502.16282', 'title': 'Understanding the Emergence of Multimodal Representation Alignment', 'authors': 'Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, Paul Pu Liang', 'link': 'https://arxiv.org/abs/2502.16282', 'abstract': 'Multimodal representation learning is fundamentally about transforming incomparable modalities into comparable representations. While prior research primarily focused on explicitly aligning these representations through targeted learning objectives and model architectures, a recent line of work has found that independently trained unimodal models of increasing scale and performance can become implicitly aligned with each other. These findings raise fundamental questions regarding the emergence of aligned representations in multimodal learning. Specifically: (1) when and why does alignment emerge implicitly? and (2) is alignment a reliable indicator of performance? Through a comprehensive empirical investigation, we demonstrate that both the emergence of alignment and its relationship with task performance depend on several critical data characteristics. These include, but are not necessarily limited to, the degree of similarity between the modalities and the balance between redundant and unique information they provide for the task. Our findings suggest that alignment may not be universally beneficial; rather, its impact on performance varies depending on the dataset and task. These insights can help practitioners determine whether increasing alignment between modalities is advantageous or, in some cases, detrimental to achieving optimal performance. Code is released at this https URL.', 'abstract_zh': '多模态表示学习本质上是将不可比较的模态转换为可比较的表示。此前的研究主要集中在通过明确的目标和模型架构来对齐这些表示，但最近的一项研究发现，独立训练的大型单模态模型可以实现隐式对齐。这些发现提出了关于多模态学习中对齐表示的出现的根本问题。具体来说：（1）在何时以及为何对齐会隐式出现；（2）对齐是否是性能可靠的指标？通过全面的经验研究，我们证明了对齐的出现及其与任务性能的关系取决于多个关键的数据特性。这些特性包括但不仅限于模态之间相似度的高低以及它们为任务提供的冗余信息与独特信息之间的平衡。我们的研究结果表明，对齐未必总是有益的；其对性能的影响会根据所使用的数据集和任务而有所不同。这些见解可以帮助实践者确定在某些情况下增加模态之间的对齐是否有助于达到最优性能，甚至可能对其有害。代码发布在https://...。', 'title_zh': '理解多模态表示对齐的产生机制'}
{'arxiv_id': 'arXiv:2502.15972', 'title': 'Multi-Agent Multimodal Models for Multicultural Text to Image Generation', 'authors': 'Parth Bhalerao, Mounika Yalamarty, Brian Trinh, Oana Ignat', 'link': 'https://arxiv.org/abs/2502.15972', 'abstract': 'Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks. In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the novel task of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a Multi-Agent framework that enhances multicultural Image Generation by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research. Our dataset and models are available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）在各种多模态任务中表现出色。然而，由于现有数据和模型主要以西方为中心，它们在跨文化环境中的有效性仍受到限制。与此同时，多智能体模型在解决复杂任务方面表现出强大的能力。在本文中，我们评估了LLMs在多智能体交互设置中的表现，以实现一项新的跨文化图像生成任务。我们的主要贡献包括：（1）提出了MosAIG多智能体框架，该框架通过利用具有不同文化个性的语言模型来增强跨文化图像生成；（2）提供了一个包含9,000张跨文化图像的数据集，这些图像覆盖了五个国家、三个年龄组、两种性别、25个历史地标和五种语言；（3）展示了多智能体交互在多个评价指标上优于简单的无智能体模型，为未来研究提供了有价值的见解。我们的数据集和模型可在此处访问：[提供链接的格式]。', 'title_zh': '多模态多agent模型在跨文化文本到图像生成中的应用'}
{'arxiv_id': 'arXiv:2502.15869', 'title': 'Generative AI Framework for 3D Object Generation in Augmented Reality', 'authors': 'Majid Behravan', 'link': 'https://arxiv.org/abs/2502.15869', 'abstract': 'This thesis presents a framework that integrates state-of-the-art generative AI models for real-time creation of three-dimensional (3D) objects in augmented reality (AR) environments. The primary goal is to convert diverse inputs, such as images and speech, into accurate 3D models, enhancing user interaction and immersion. Key components include advanced object detection algorithms, user-friendly interaction techniques, and robust AI models like Shap-E for 3D generation. Leveraging Vision Language Models (VLMs) and Large Language Models (LLMs), the system captures spatial details from images and processes textual information to generate comprehensive 3D objects, seamlessly integrating virtual objects into real-world environments. The framework demonstrates applications across industries such as gaming, education, retail, and interior design. It allows players to create personalized in-game assets, customers to see products in their environments before purchase, and designers to convert real-world objects into 3D models for real-time visualization. A significant contribution is democratizing 3D model creation, making advanced AI tools accessible to a broader audience, fostering creativity and innovation. The framework addresses challenges like handling multilingual inputs, diverse visual data, and complex environments, improving object detection and model generation accuracy, as well as loading 3D models in AR space in real-time. In conclusion, this thesis integrates generative AI and AR for efficient 3D model generation, enhancing accessibility and paving the way for innovative applications and improved user interactions in AR environments.', 'abstract_zh': '本论文提出了一种框架，该框架将最新的生成型人工智能模型集成起来，以实现实时在增强现实（AR）环境中创建三维（3D）对象。主要目标是将各种输入，如图像和语音，转换为准确的3D模型，从而增强用户的交互性和沉浸感。关键组成部分包括先进的物体检测算法、用户友好型交互技术以及如Shap-E等强大的AI模型，用于3D生成。该系统利用视觉语言模型（VLMs）和大型语言模型（LLMs），从图像中捕获空间细节，并处理文本信息以生成完整的3D物体，无缝地将虚拟对象整合到现实世界环境中。该框架展示了在游戏、教育、零售和室内设计等行业中的应用。它允许玩家创建个性化的游戏资产，让客户在购买前看到产品在环境中的样子，还使设计师能够将现实世界的物体转换为3D模型进行实时可视化。一个重要贡献是使3D模型的创建更加普及，使先进的AI工具能够被更广泛的受众使用，促进创意和创新。该框架解决了多语言输入、多样视觉数据和复杂环境等挑战，提高了物体检测和模型生成的准确性，同时实现了3D模型的实时加载。总之，本论文将生成型人工智能与AR相结合，以提高3D模型生成的效率，从而增强AR环境中的可访问性和开辟创新应用及改善用户交互的可能性。', 'title_zh': '用于增强现实中的3D对象生成的生成式AI框架'}
{'arxiv_id': 'arXiv:2502.15839', 'title': 'FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities', 'authors': 'Yi Liu, Cong Wang, Xingliang Yuan', 'link': 'https://arxiv.org/abs/2502.15839', 'abstract': "The Web of Things (WoT) enhances interoperability across web-based and ubiquitous computing platforms while complementing existing IoT standards. The multimodal Federated Learning (FL) paradigm has been introduced to enhance WoT by enabling the fusion of multi-source mobile sensing data while preserving privacy. However, a key challenge in mobile sensing systems using multimodal FL is modality incompleteness, where some modalities may be unavailable or only partially captured, potentially degrading the system's performance and reliability. Current multimodal FL frameworks typically train multiple unimodal FL subsystems or apply interpolation techniques on the node side to approximate missing modalities. However, these approaches overlook the shared latent feature space among incomplete modalities across different nodes and fail to discriminate against low-quality nodes. To address this gap, we present FedMobile, a new knowledge contribution-aware multimodal FL framework designed for robust learning despite missing modalities. FedMobile prioritizes local-to-global knowledge transfer, leveraging cross-node multimodal feature information to reconstruct missing features. It also enhances system performance and resilience to modality heterogeneity through rigorous node contribution assessments and knowledge contribution-aware aggregation rules. Empirical evaluations on five widely recognized multimodal benchmark datasets demonstrate that FedMobile maintains robust learning even when up to 90% of modality information is missing or when data from two modalities are randomly missing, outperforming state-of-the-art baselines.", 'abstract_zh': '物联网（IoT）网络通过增强基于网络和泛在计算平台之间的互操作性，同时补充现有物联网标准，从而提升互操作性。多模态联邦学习（FL）范式已被引入以增强物联网网络，通过融合多种来源的移动传感数据同时保持隐私。然而，在使用多模态FL的移动传感系统中，一个关键的挑战是模态不完整性，即某些模态可能不可用或仅部分捕获，可能会降低系统的性能和可靠性。当前的多模态FL框架通常训练多个单一模态的FL子系统或在节点侧应用插值技术来估算缺失的模态。然而，这些方法忽视了不同节点间不完整模态共享的潜在潜在特征空间，并且不能区分低质量节点。为解决这一问题，我们提出了FedMobile，这是一种新的基于知识贡献的多模态FL框架，即便存在缺失模态也能实现稳健学习。FedMobile优先考虑从局部到全局的知识传递，利用跨节点的多模态特征信息来重构缺失特征。通过严格的节点贡献评估和基于知识贡献的聚合规则，FedMobile还增强了系统对模态异质性的影响。在五个广泛认可的多模态基准数据集上的实证评估表明，在模态信息缺失高达90%或两个模态的数据随机缺失的情况下，FedMobile仍能保持稳健的学习，同时优于最先进的基线方法。', 'title_zh': 'FedMobile: 实现具有不完整模态的多模态联邦学习的知识贡献aware机制'}
{'arxiv_id': 'arXiv:2502.15812', 'title': 'InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models', 'authors': 'Xiaofei Yin, Yijie Hong, Ya Guo, Yi Tu, Weiqiang Wang, Gongshen Liu, Huijia zhu', 'link': 'https://arxiv.org/abs/2502.15812', 'abstract': 'In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues - such as satire, insult, or critique - remains a significant challenge. Existing evaluation benchmarks primarily focus on direct tasks like image captioning or are limited to a narrow set of categories, such as humor or satire, for deep semantic understanding. To address this gap, we introduce, for the first time, a comprehensive, multi-level Chinese-based benchmark designed specifically for evaluating the understanding of implicit meanings in images. This benchmark is systematically categorized into four subtasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. We propose an innovative semi-automatic method for constructing datasets, adhering to established construction protocols. Using this benchmark, we evaluate 15 open-source large vision language models (LVLMs) and GPT-4o, revealing that even the best-performing model lags behind human performance by nearly 14% in understanding implicit meaning. Our findings underscore the intrinsic challenges current LVLMs face in grasping nuanced visual semantics, highlighting significant opportunities for future research and development in this domain. We will publicly release our InsightVision dataset, code upon acceptance of the paper.', 'abstract_zh': '在多模态语言模型不断演进的背景下，理解通过视觉线索传达的细微含义——如讽刺、侮辱或批评——仍然是一个重大挑战。现有的评估基准主要集中在直接任务上，如图像字幕生成，或者局限于一类或几类深层语义理解，例如幽默或讽刺。为了解决这一问题，我们首次提出了一个全面的、多层级的中文基准，专门用于评估图像中隐含意义的理解能力。该基准系统性地分为四个子任务：表层内容理解、象征意义解释、背景知识理解以及隐含意义理解。我们提出了一种创新的半自动方法来构建数据集，遵循既定的建设协议。使用该基准，我们评估了15个开源大型视觉语言模型（LVLMs）和GPT-4o，结果显示，即使表现最好的模型在理解隐含意义方面也比人类低近14%。我们的研究结果强调了当前LVLMs在掌握细微视觉语义方面固有的挑战，突显了该领域未来研究和发展的巨大机会。我们的InsightVision数据集和代码将在论文被接受后公开发布。', 'title_zh': 'InsightVision：用于评估大型视觉语言模型隐含视觉语义的综合多层次中文基准'}
{'arxiv_id': 'arXiv:2502.16865', 'title': 'Multimodal Search in Chemical Documents and Reactions', 'authors': 'Ayush Kumar Shah, Abhisek Dey, Leo Luo, Bryan Amador, Patrick Philippy, Ming Zhong, Siru Ouyang, David Mark Friday, David Bianchi, Nick Jackson, Richard Zanibbi, Jiawei Han', 'link': 'https://arxiv.org/abs/2502.16865', 'abstract': "We present a multimodal search tool that facilitates retrieval of chemical reactions, molecular structures, and associated text from scientific literature. Queries may combine molecular diagrams, textual descriptions, and reaction data, allowing users to connect different representations of chemical information. To support this, the indexing process includes chemical diagram extraction and parsing, extraction of reaction data from text in tabular form, and cross-modal linking of diagrams and their mentions in text. We describe the system's architecture, key functionalities, and retrieval process, along with expert assessments of the system. This demo highlights the workflow and technical components of the search system.", 'abstract_zh': '我们介绍了一种多模态搜索工具，该工具能够从科学文献中检索化学反应、分子结构及其相关文本。查询可以结合分子示意图、文本描述和化学反应数据，使用户能够连接化学信息的不同表示方式。为了支持这一功能，索引过程包括化学示意图的提取和解析、从文本表格中提取反应数据以及跨模态链接示意图及其在文本中的提及。我们将描述该系统的体系结构、关键功能和检索过程，并提供专家对该系统的评估。此演示旨在突出搜索系统的workflow和技术组件。', 'title_zh': '化学文献和反应的多模态搜索'}
{'arxiv_id': 'arXiv:2502.16068', 'title': 'Joint Similarity Item Exploration and Overlapped User Guidance for Multi-Modal Cross-Domain Recommendation', 'authors': 'Weiming Liu, Chaochao Chen, Jiahe Xu, Xinting Liao, Fan Wang, Xiaolin Zheng, Zhihui Fu, Ruiguang Pei, Jun Wang', 'link': 'https://arxiv.org/abs/2502.16068', 'abstract': 'Cross-Domain Recommendation (CDR) has been widely investigated for solving long-standing data sparsity problem via knowledge sharing across domains. In this paper, we focus on the Multi-Modal Cross-Domain Recommendation (MMCDR) problem where different items have multi-modal information while few users are overlapped across domains. MMCDR is particularly challenging in two aspects: fully exploiting diverse multi-modal information within each domain and leveraging useful knowledge transfer across domains. However, previous methods fail to cluster items with similar characteristics while filtering out inherit noises within different modalities, hurdling the model performance. What is worse, conventional CDR models primarily rely on overlapped users for domain adaptation, making them ill-equipped to handle scenarios where the majority of users are non-overlapped. To fill this gap, we propose Joint Similarity Item Exploration and Overlapped User Guidance (SIEOUG) for solving the MMCDR problem. SIEOUG first proposes similarity item exploration module, which not only obtains pair-wise and group-wise item-item graph knowledge, but also reduces irrelevant noise for multi-modal modeling. Then SIEOUG proposes user-item collaborative filtering module to aggregate user/item embeddings with the attention mechanism for collaborative filtering. Finally SIEOUG proposes overlapped user guidance module with optimal user matching for knowledge sharing across domains. Our empirical study on Amazon dataset with several different tasks demonstrates that SIEOUG significantly outperforms the state-of-the-art models under the MMCDR setting.', 'abstract_zh': '跨域推荐（Cross-Domain Recommendation, CDR）通过在不同领域间共享知识来解决长期存在的数据稀疏问题而得到了广泛的研究。本文聚焦于多模态跨域推荐（Multi-Modal Cross-Domain Recommendation, MMCDR）问题，该问题的特点是不同项目具有多模态信息，但不同领域间的重叠用户很少。MMCDR 在两个方面具有挑战性：充分利用各自领域内的多样化多模态信息，以及在不同领域间有效利用知识转移。然而，之前的许多方法未能在过滤不同模态中的固有噪声的同时聚类具有相似特征的项目，从而阻碍了模型性能的提升。更糟糕的是，传统的跨域推荐模型主要依赖于重叠用户来进行领域适应，这使它们无法很好地处理大部分用户不存在重叠的场景。为了解决这一问题，我们提出了一种联合相似性项目探索和重叠用户引导（Similarity Item Exploration and Overlapped User Guidance, SIEOUG）的方法来解决MMCDR 问题。SIEOUG 首先提出了一种相似性项目探索模块，不仅可以获取项目对和群组之间的项目图知识，而且还能减少多模态建模中的无关噪声。然后提出了用户-项目协同过滤模块，通过注意力机制聚合用户/项目的嵌入以进行协同过滤。最后提出了重叠用户引导模块，通过最佳用户匹配来促进不同领域间的知识共享。我们在 Amazon 数据集上的实证研究表明，SIEOUG 在MMCDR 设置下显著优于最先进的模型。', 'title_zh': '多模态跨域推荐中的联合相似性物品探索与重叠用户引导'}
{'arxiv_id': 'arXiv:2502.15979', 'title': 'Visual Zero-Shot E-Commerce Product Attribute Value Extraction', 'authors': 'Jiaying Gong, Ming Cheng, Hongda Shen, Pierre-Yves Vandenbussche, Janet Jenq, Hoda Eldardiry', 'link': 'https://arxiv.org/abs/2502.15979', 'abstract': 'Existing zero-shot product attribute value (aspect) extraction approaches in e-Commerce industry rely on uni-modal or multi-modal models, where the sellers are asked to provide detailed textual inputs (product descriptions) for the products. However, manually providing (typing) the product descriptions is time-consuming and frustrating for the sellers. Thus, we propose a cross-modal zero-shot attribute value generation framework (ViOC-AG) based on CLIP, which only requires product images as the inputs. ViOC-AG follows a text-only training process, where a task-customized text decoder is trained with the frozen CLIP text encoder to alleviate the modality gap and task disconnection. During the zero-shot inference, product aspects are generated by the frozen CLIP image encoder connected with the trained task-customized text decoder. OCR tokens and outputs from a frozen prompt-based LLM correct the decoded outputs for out-of-domain attribute values. Experiments show that ViOC-AG significantly outperforms other fine-tuned vision-language models for zero-shot attribute value extraction.', 'abstract_zh': '现有的电子商务领域零样本产品属性值（方面）提取方法依赖于单模态或跨模态模型，要求卖方提供产品的详细文本输入（产品描述）。然而，手动提供（输入）产品描述对于卖方来说既耗时又令人沮丧。因此，我们提出了一种基于CLIP的跨模态零样本属性值生成框架（ViOC-AG），该框架仅需要产品图像作为输入。ViOC-AG 采用纯文本训练过程，通过冻结的CLIP文本编码器训练任务定制的文本解码器，以缓解模态差距和任务断连问题。在零样本推理过程中，通过冻结的CLIP图像编码器与训练好的任务定制的文本解码器连接生成产品方面。使用冻结的基于提示的语言模型（LLM）生成的OCR标记和输出纠正解码输出中的离域属性值。实验表明，ViOC-AG 在零样本属性值提取方面显著优于其他微调的视觉-语言模型。', 'title_zh': '视觉驱动的零样本电子商务产品属性值提取'}
{'arxiv_id': 'arXiv:2502.15711', 'title': 'A Survey on Multimodal Recommender Systems: Recent Advances and Future Directions', 'authors': 'Jinfeng Xu, Zheyu Chen, Shuo Yang, Jinze Li, Wei Wang, Xiping Hu, Steven Hoi, Edith Ngai', 'link': 'https://arxiv.org/abs/2502.15711', 'abstract': "Acquiring valuable data from the rapidly expanding information on the internet has become a significant concern, and recommender systems have emerged as a widely used and effective tool for helping users discover items of interest. The essence of recommender systems lies in their ability to predict users' ratings or preferences for various items and subsequently recommend the most relevant ones based on historical interaction data and publicly available information. With the advent of diverse multimedia services, including text, images, video, and audio, humans can perceive the world through multiple modalities. Consequently, a recommender system capable of understanding and interpreting different modal data can more effectively refer to individual preferences. Multimodal Recommender Systems (MRS) not only capture implicit interaction information across multiple modalities but also have the potential to uncover hidden relationships between these modalities. The primary objective of this survey is to comprehensively review recent research advancements in MRS and to analyze the models from a technical perspective. Specifically, we aim to summarize the general process and main challenges of MRS from a technical perspective. We then introduce the existing MRS models by categorizing them into four key areas: Feature Extraction, Encoder, Multimodal Fusion, and Loss Function. Finally, we further discuss potential future directions for developing and enhancing MRS. This survey serves as a comprehensive guide for researchers and practitioners in MRS field, providing insights into the current state of MRS technology and identifying areas for future research. We hope to contribute to developing a more sophisticated and effective multimodal recommender system. To access more details of this paper, we open source a repository: this https URL.", 'abstract_zh': '从互联网上迅速增长的信息中获取有价值的数据已成为一个重要关注点，推荐系统因此成为帮助用户发现感兴趣项目的一种广泛使用且有效的工具。推荐系统的核心在于预测用户对各种项目的评分或偏好，并基于历史交互数据和公开信息推荐最相关的内容。随着包括文本、图像、视频和音频在内的多元媒体服务的出现，人类可以通过多种感官感知世界。因此，能够理解并解释不同模态数据的推荐系统可以更有效地个性化推荐。多元模态推荐系统（MRS）不仅能够捕获多个模态间的隐含交互信息，还能潜在地揭示这些模态之间的隐藏关系。本综述的主要目标是全面回顾MRS领域的近期研究进展，并从技术角度分析现有模型。具体来说，我们旨在从技术角度总结MRS的一般过程和主要挑战。然后，我们通过将现有模型归类为四个关键领域：特征提取、编码器、多元模态融合和损失函数来进行介绍。最后，我们进一步探讨了开发和增强MRS的潜在未来方向。本综述旨在为MRS领域的研究者和实践者提供全面指南，揭示当前MRS技术的状态，并确定未来研究的方向。我们希望通过这一综述促进开发更加复杂且有效的多元模态推荐系统。如有更多信息，您可以访问我们开源的仓库：this https URL。', 'title_zh': '多模态推荐系统综述： Recent Advances and Future Directions'}
