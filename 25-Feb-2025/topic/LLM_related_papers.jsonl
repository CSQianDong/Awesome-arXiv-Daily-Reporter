{'arxiv_id': 'arXiv:2502.17416', 'title': 'Reasoning with Latent Thoughts: On the Power of Looped Transformers', 'authors': 'Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi', 'link': 'https://arxiv.org/abs/2502.17416', 'abstract': 'Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.', 'abstract_zh': '以下是经过学术规范翻译后的中文内容：\n\n大型语言模型在推理能力方面表现出了显著的能力，而扩展规律表明，模型参数量的增加，尤其是沿深度轴的增加，是主要的驱动因素。在本文中，我们提出了更强的断言——许多推理问题需要较大的深度，但不一定需要较多的参数。这为循环模型在推理方面的应用开辟了新的途径。首先，我们证明了对于许多合成的推理问题，如加法、$p$-跳归纳和数学问题，一个$k$层的循环Transformers模型经过$L$次循环，几乎可以与一个$kL$层非循环模型达到相同的性能，并且在某些情况下显著优于$k$层模型。这进一步得到了理论结果的支持，这些结果显示，许多这样的推理问题可以通过迭代算法解决，并且因此可以通过带有接近最优深度的循环模型有效地解决。令人惊讶的是，这些优势也适用于语言建模的实际应用场景——在许多下游推理任务中，一个$k$层模型经过$L$次循环的效果可以与$kL$层模型相当，甚至更好。实际上，我们的经验研究表明，循环模型和非循环模型呈现出依赖于其有效深度的扩展行为，类似于链式思维（CoT）推理的推理时扩展行为。我们进一步通过证明循环模型隐含地生成隐藏的思路，并可以使用$T$次循环模拟$T$步CoT推理，阐明了与CoT推理的联系。基于这些发现，我们也提出了推理和记忆之间的有趣二分法，并设计了一种基于循环的正则化方法，该方法在两个方向上均有效。', 'title_zh': '基于潜在思想的推理：环形变换器的力量'}
{'arxiv_id': 'arXiv:2502.17383', 'title': 'What is a Good Question? Utility Estimation with LLM-based Simulations', 'authors': 'Dong-Ho Lee, Hyundong Cho, Jonathan May, Jay Pujara', 'link': 'https://arxiv.org/abs/2502.17383', 'abstract': "Asking questions is a fundamental aspect of learning that facilitates deeper understanding. However, characterizing and crafting questions that effectively improve learning remains elusive. To address this gap, we propose QUEST (Question Utility Estimation with Simulated Tests). QUEST simulates a learning environment that enables the quantification of a question's utility based on its direct impact on improving learning outcomes. Furthermore, we can identify high-utility questions and use them to fine-tune question generation models with rejection sampling. We find that questions generated by models trained with rejection sampling based on question utility result in exam scores that are higher by at least 20% than those from specialized prompting grounded on educational objectives literature and models fine-tuned with indirect measures of question quality, such as saliency and expected information gain.", 'abstract_zh': '提出问题是一种基本的学习方法，能够促进更深入的理解。然而，如何准确描述和构造能够有效提高学习效果的问题仍然具有挑战性。为了解决这一问题，我们提出了一种名为QUEST（Question Utility Estimation with Simulated Tests）的方法。QUEST 模拟了一个学习环境，使我们可以基于问题对提高学习成果的直接影响来量化问题的价值。此外，我们可以通过使用具有拒绝采样方法的高价值问题来精调问题生成模型。我们发现，利用问题价值为基础的拒绝采样方法进行训练生成的问题，相比于基于教育目标文献的专业化提示和利用间接衡量问题质量（如显著性和预期信息增益）进行精调的模型生成的问题，考试得分至少高出20%。', 'title_zh': '什么是好问题？基于大语言模型的模拟实用性评估'}
{'arxiv_id': 'arXiv:2502.17328', 'title': 'Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization', 'authors': 'Yen-Ju Lu, Ting-Yao Hu, Hema Swetha Koppula, Hadi Pouransari, Jen-Hao Rick Chang, Yin Xia, Xiang Kong, Qi Zhu, Simon Wang, Oncel Tuzel, Raviteja Vemulapalli', 'link': 'https://arxiv.org/abs/2502.17328', 'abstract': 'In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLMś dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.', 'abstract_zh': '在本文中，我们提出了一种名为Mutual Reinforcing Data Synthesis（互强化数据合成，MRDS）的方法，用于在大规模语言模型（LLM）中提升少样本对话总结任务。与需要外部知识的先前方法不同，我们通过相互强化对话合成和总结能力，使它们在训练过程中相互补充，从而整体上提升性能。通过定向偏好优化，对话合成能力得到了增强，偏好得分来自于总结能力。总结能力则通过对话合成能力产生的高质对话-总结配对数据得到了增强。借助所提出的MRDS机制，我们激发了LLM中的内部知识，并以合成数据的形式对其进行利用，以此扩大少样本真实训练数据集。实验证明，我们的方法提高了对话总结效果，在少样本设置中实现了ROUGE分数1.5%的提升和BERT分数0.3%的提升。此外，我们的方法在人类评估中获得了最高的平均分数，超过了预训练模型和仅针对总结任务微调的基础模型。', 'title_zh': 'few-shot 对话摘要中 LL arkov 链模型对话合成与总结能力的相互强化'}
{'arxiv_id': 'arXiv:2502.17282', 'title': 'Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing', 'authors': 'Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye', 'link': 'https://arxiv.org/abs/2502.17282', 'abstract': 'Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (Model-SAT), which generates positive and negative samples based on what different models perform well or struggle with. Model-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that Model-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. Model-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at this https URL', 'abstract_zh': '大规模语言模型（LLMs）展示了类似人类的指令遵循能力，特别是在参数超过100亿的模型中表现尤为突出。一些较小且资源友好的LLMs的综合能力可以应对大多数大型LLMs擅长的指令。在这项工作中，我们探讨了如何将最适合每条指令的LLM进行路由，以实现更好的总体性能。我们提出了一种新的范式，通过使用模型能力表示、用户指令和性能询问指令构建能力指令，来评估性能。为了从能力指令中学习，我们引入了一个新的端到端框架，称为能力测试下的模型选择（Model-SAT），该框架根据不同模型擅长或难以处理的内容生成正负样本。Model-SAT 使用一个模型能力编码器，将其模型表示扩展为一个轻量级的LLM。我们的实验表明，Model-SAT 能够理解候选模型的性能维度，并提供它们处理各种指令的能力概率。此外，在部署过程中，一个新的模型可以在不到两分钟的时间内快速推理出其在50个任务中的能力测试结果，每个任务有20个样本。Model-SAT 在无需候选模型推理的情况下实现了最先进的模型路由，并且适用于现实世界中新模型发布的情景。相关代码可在以下链接获取：[提供链接]', 'title_zh': '能力指令调优：一种新的动态大模型路由范式'}
{'arxiv_id': 'arXiv:2502.17214', 'title': 'CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought', 'authors': 'Boxuan Zhang, Ruqi Zhang', 'link': 'https://arxiv.org/abs/2502.17214', 'abstract': "Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on LLaMA Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: this https URL.", 'abstract_zh': '大型语言模型（LLMs）在许多任务上表现优异，但在准确量化其生成响应中的不确定性方面存在局限性。这种局限性使得检测 misinformation 并确保可靠决策变得困难。现有的 LLM 不确定性量化（UQ）方法主要基于提示（prompt）而非响应，通常需要生成多个响应样本，这带来了高昂的计算成本。此外，研究表明，LLMs 在使用推理步骤得出答案时往往会表现出过度自信。在本项工作中，我们提出了一种名为 CoT-UQ 的响应级不确定性量化框架，该框架通过 Chain-of-Thought（CoT）将 LLM 的固有推理能力集成到不确定性量化过程中。CoT-UQ 通过从每个推理步骤中提取关键词并评估它们对最终答案的重要性，在推断过程中捕捉关键信息。这些关键的推理信息随后被汇总以生成最终的不确定性估计。我们基于 LLaMA 家族模型，在逻辑推理和数学推理任务中进行了广泛的实验，模型大小从 8B 到 13B 不等。实验结果表明，CoT-UQ 显著优于现有方法，在 AUROC 指标上平均提高了 5.9%。代码可在此处访问：this https URL。', 'title_zh': 'CoT-UQ: 在链式思维辅助下提升大语言模型响应层面的不确定性量化'}
{'arxiv_id': 'arXiv:2502.17204', 'title': 'Order Matters: Investigate the Position Bias in Multi-constraint Instruction Following', 'authors': 'Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu', 'link': 'https://arxiv.org/abs/2502.17204', 'abstract': "Real-world instructions with multiple constraints pose a significant challenge to existing large language models (LLMs). An observation is that the LLMs exhibit dramatic performance fluctuation when disturbing the order of the incorporated constraints. Yet, none of the existing works has systematically investigated this position bias problem in the field of multi-constraint instruction following. To bridge this gap, we design a probing task where we quantitatively measure the difficulty distribution of the constraints by a novel Difficulty Distribution Index (CDDI). Through the experimental results, we find that LLMs are more performant when presented with the constraints in a ``hard-to-easy'' order. This preference can be generalized to LLMs with different architecture or different sizes of parameters. Additionally, we conduct an explanation study, providing an intuitive insight into the correlation between the LLM's attention and constraint orders. Our code and dataset are publicly available at this https URL.", 'abstract_zh': '现有的大型语言模型（LLMs）在处理具有多种约束的实际任务时面临显著挑战。观察到，当干扰约束的顺序时，LLMs 的性能会出现剧烈波动。然而，在多约束指令跟随领域，目前尚未系统性地研究这一位置偏差问题。为了弥合这一差距，我们设计了一个探测任务，通过引入新型的难度分布指数（CDDI）来定量测量约束的难度分布。通过实验结果，我们发现当约束以“难到易”的顺序呈现时，LLMs 的表现更好。这种偏好可以在不同架构或不同参数大小的LLMs中泛化。此外，我们还进行了一个解释性研究，提供了一个直观的视角来理解LLMs的注意力与其约束顺序之间的关系。我们的代码和数据集可在以下网址公开获取：this https URL。', 'title_zh': '顺序有影响：探究多约束指令跟随中的位置偏见'}
{'arxiv_id': 'arXiv:2502.17169', 'title': 'Logic Haystacks: Probing LLMs Long-Context Logical Reasoning (Without Easily Identifiable Unrelated Padding)', 'authors': 'Damien Sileo', 'link': 'https://arxiv.org/abs/2502.17169', 'abstract': 'Large language models demonstrate promising long context processing capabilities, with recent models touting context windows close to one million tokens. However, the evaluations supporting these claims often involve simple retrieval tasks or synthetic tasks padded with irrelevant text, which the models may easily detect and discard. In this work, we generate lengthy simplified English text with first-order logic representations spanning up to 2048 clauses (around 25k GPT-4 tokens). We formulate an evaluation task with evidence retrieval for contradiction detection. The long, homogeneous text is filled with distractors that are both hard to distinguish from relevant evidences and provably not interfering with them. Our evaluation of evidence retrieval shows that the effective context window is much smaller with realistic distractors, already crumbling at 128 clauses.', 'abstract_zh': '大规模语言模型展示了令人期待的长上下文处理能力，近期的一些模型甚至宣称具有接近一百万词的上下文窗口。然而，这些声明的支持性评估往往涉及简单的检索任务或填充了无关文本的合成任务，而模型通常很容易识别并忽略这些无关内容。在本项研究中，我们生成了长达2048个子句（约25000个GPT-4标记）的简化英语文本，并使用了一阶逻辑表示。我们设计了一项评估任务，该任务包括证据检索以检测矛盾。该长文本充满了难以区分真相关据的干扰项，并且在理论上不会干扰这些相关真据。我们的证据检索评估显示，在现实干扰项的作用下，有效上下文窗口要小得多，甚至在128个子句时就会变得不稳定。', 'title_zh': '逻辑针堆：探究LLMs在无明显无关填充的长情境逻辑推理能力'}
{'arxiv_id': 'arXiv:2502.17129', 'title': 'Thus Spake Long-Context Large Language Model', 'authors': 'Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2502.17129', 'abstract': 'Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.\nInspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.', 'abstract_zh': '长文本是自然语言处理（NLP）中的一个重要课题，贯穿于NLP架构的发展历程，为大型语言模型（LLMs）提供了巨大的机会，赋予了LLMs类似于人类的终身学习潜力。然而，追求长文本也伴随着诸多挑战。尽管如此，长文本仍然是LLMs的核心竞争力之一。在过去两年里，LLMs的上下文长度已经突破性地扩展到了数百万词。此外，关于长文本LLMs的研究已从长度的外推扩展到全面关注架构、基础设施、训练和评估技术。\n\n受到交响诗《查拉图斯特拉如是说》的启发，我们把扩展LLM上下文的旅程类比于人类试图超越其有限性的努力。本文将展示LLM在其对更长上下文需求巨大的同时，也必须接受其最终有限性的双重挑战。为此，我们将从架构、基础设施、训练和评估四个维度全面呈现长上下文LLM的生命周期，展示长上下文技术的全貌。在本文末尾，我们将提出10个目前未解答的关于长上下文LLM的问题。我们希望本文能够为长上下文LLM的研究提供一个系统性的介绍。', 'title_zh': '因此，长上下文大型语言模型如是说'}
{'arxiv_id': 'arXiv:2502.17091', 'title': 'WildFrame: Comparing Framing in Humans and LLMs on Naturally Occurring Texts', 'authors': 'Gili Lior, Liron Nacchace, Gabriel Stanovsky', 'link': 'https://arxiv.org/abs/2502.17091', 'abstract': 'Humans are influenced by how information is presented, a phenomenon known as the framing effect. Previous work has shown that LLMs may also be susceptible to framing but has done so on synthetic data and did not compare to human behavior. We introduce WildFrame, a dataset for evaluating LLM responses to positive and negative framing, in naturally-occurring sentences, and compare humans on the same data. WildFrame consists of 1,000 texts, first selecting real-world statements with clear sentiment, then reframing them in either positive or negative light, and lastly, collecting human sentiment annotations. By evaluating eight state-of-the-art LLMs on WildFrame, we find that all models exhibit framing effects similar to humans ($r\\geq0.57$), with both humans and models being more influenced by positive rather than negative reframing. Our findings benefit model developers, who can either harness framing or mitigate its effects, depending on the downstream application.', 'abstract_zh': '人类受到信息呈现方式的影响，这种现象被称为框架效应。以往的研究已经表明，语言模型（LLM）也可能受到框架效应的影响，但这些研究主要是在合成数据上进行的，并未与人类行为进行比较。我们引入了WildFrame数据集，用于评估LLM在自然语句中的正向和负向框架效应，并将人类在同一数据集上的表现进行对比。WildFrame包含1000篇文章，首先选择具有明确情感倾向的真实世界陈述，然后重新框架为正向或负向，最后收集人类情感标注。通过评估八种最先进的LLM在WildFrame上的响应，我们发现所有模型都表现出与人类类似的框架效应（相关系数$r \\geq 0.57$），人类和模型相对来说更受正向重新框架的影响。我们的发现为模型开发者提供了益处，他们可以根据下游应用的需要，要么利用框架效应，要么减轻其影响。', 'title_zh': 'WildFrame: 人类与大规模语言模型在自然文本中的框架比较研究'}
{'arxiv_id': 'arXiv:2502.17086', 'title': 'Automatically Evaluating the Paper Reviewing Capability of Large Language Models', 'authors': 'Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim', 'link': 'https://arxiv.org/abs/2502.17086', 'abstract': "Peer review is essential for scientific progress, but it faces challenges such as reviewer shortages and growing workloads. Although Large Language Models (LLMs) show potential for providing assistance, research has reported significant limitations in the reviews they generate. While the insights are valuable, conducting the analysis is challenging due to the considerable time and effort required, especially given the rapid pace of LLM developments. To address the challenge, we developed an automatic evaluation pipeline to assess the LLMs' paper review capability by comparing them with expert-generated reviews. By constructing a dataset consisting of 676 OpenReview papers, we examined the agreement between LLMs and experts in their strength and weakness identifications. The results showed that LLMs lack balanced perspectives, significantly overlook novelty assessment when criticizing, and produce poor acceptance decisions. Our automated pipeline enables a scalable evaluation of LLMs' paper review capability over time.", 'abstract_zh': '同行评审对于科学进步至关重要，但面临着评审员短缺和工作量增加等挑战。虽然大型语言模型（LLMs）显示出提供辅助的潜力，但研究结果显示，它们生成的评审意见存在显著的局限性。尽管这些见解很有价值，但由于需要大量的时间和精力进行分析，尤其是在LLM技术快速发展的背景下，这一过程尤其具有挑战性。为了应对这一挑战，我们开发了一种自动评估管道，通过将LLMs的论文评审能力与专家生成的评审意见进行对比来评估LLMs的论文评审能力。通过构建包含676篇OpenReview论文的数据集，我们考察了LLMs和专家在识别论文优势和劣势方面的共识程度。结果表明，LLMs缺乏平衡视角，在批评时显著忽略了新颖性评估，并且产生了较差的接受决策。我们的自动化管道能够随着时间的推移对LLMs的论文评审能力进行可扩展的评估。', 'title_zh': '自动评估大型语言模型的论文评审能力'}
{'arxiv_id': 'arXiv:2502.17026', 'title': 'Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology', 'authors': 'Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei', 'link': 'https://arxiv.org/abs/2502.17026', 'abstract': "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.", 'abstract_zh': '理解大型语言模型（LLM）解释中的不确定性对于评估其可信度和推理一致性至关重要，从而为LLM输出关于某个问题的可靠性提供了见解。本文提出了一种新颖的框架，通过推理拓扑视角量化LLM解释中的不确定性。通过设计结构性激发策略，我们引导LLM将答案的解释构建成图形拓扑。这一过程将解释分解为知识相关的子问题和基于拓扑的推理结构，这使得我们不仅可以在语义层面，而且可以从推理路径层面来量化不确定性。进一步地说，这种方法为评估知识冗余性和提供可解释的推理过程洞察带来了便利。我们的方法提供了一种系统的方法来解释LLM的推理、分析局限性和为增强鲁棒性和可信度提供指导。本文开创性地将图形结构不确定性测量应用于LLM解释，并展示了基于拓扑的量化方法的潜力。', 'title_zh': '理解大规模语言模型解释的不确定性：基于推理拓扑的观点'}
{'arxiv_id': 'arXiv:2502.17017', 'title': 'Quantifying Logical Consistency in Transformers via Query-Key Alignment', 'authors': 'Eduard Tulchinskii, Anastasia Voznyuk, Laida Kushnareva, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov', 'link': 'https://arxiv.org/abs/2502.17017', 'abstract': 'Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a "QK-score" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.', 'abstract_zh': '大规模语言模型（LLMs）在各种自然语言处理任务中表现出色，但它们在执行多步逻辑推理方面的能力仍然是一个开放的挑战。虽然通过链式思考提示方法可以增强模型的逻辑推理能力，使其能够生成中间步骤，但它缺乏评估这些逻辑转换连贯性的机制。在本文中，我们提出了一种新颖且轻量级的逻辑推理评估策略，该策略使用变压器注意力头内的查询-键对齐。通过计算单次前向传播并从精心选择的头中提取“QK分值”，我们的方法揭示了能够可靠地区分有效推理和无效推理的潜在表示，提供了一种传统消融法技术的可扩展替代方案。我们还在多个逻辑推理基准测试上进行了实证验证，表明我们的评估方法对干扰项具有更高的稳健性，并且能够进行更深的推理。实验是在参数量从15亿到70亿的多种模型上进行的。', 'title_zh': '通过查询-键对齐量化变压器中的逻辑一致性'}
{'arxiv_id': 'arXiv:2502.16971', 'title': 'LongSafety: Evaluating Long-Context Safety of Large Language Models', 'authors': 'Yida Lu, Jiale Cheng, Zhexin Zhang, Shiyao Cui, Cunxiang Wang, Xiaotao Gu, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang', 'link': 'https://arxiv.org/abs/2502.16971', 'abstract': 'As Large Language Models (LLMs) continue to advance in understanding and generating long sequences, new safety concerns have been introduced through the long context. However, the safety of LLMs in long-context tasks remains under-explored, leaving a significant gap in both evaluation and improvement of their safety. To address this, we introduce LongSafety, the first comprehensive benchmark specifically designed to evaluate LLM safety in open-ended long-context tasks. LongSafety encompasses 7 categories of safety issues and 6 user-oriented long-context tasks, with a total of 1,543 test cases, averaging 5,424 words per context. Our evaluation towards 16 representative LLMs reveals significant safety vulnerabilities, with most models achieving safety rates below 55%. Our findings also indicate that strong safety performance in short-context scenarios does not necessarily correlate with safety in long-context tasks, emphasizing the unique challenges and urgency of improving long-context safety. Moreover, through extensive analysis, we identify challenging safety issues and task types for long-context models. Furthermore, we find that relevant context and extended input sequences can exacerbate safety risks in long-context scenarios, highlighting the critical need for ongoing attention to long-context safety challenges. Our code and data are available at this https URL.', 'abstract_zh': '随着大型语言模型（LLMs）在理解和生成长序列方面的持续进步，通过长上下文引入了新的安全担忧。然而，LLMs在长上下文任务中的安全性仍然缺乏探索，留下了一个在评估和提高其安全性方面的重要空白。为解决这一问题，我们提出了LongSafety，这是首个专门用于评估LLMs在开放长上下文任务中安全性的全面基准。LongSafety包含7类安全问题和6种用户导向的长上下文任务，共有1,543个测试案例，平均每段上下文包含5,424个单词。我们对16个代表性LLMs的安全性评估显示，大多数模型的安全性指标低于55%。研究结果还表明，在短上下文场景中表现出色的安全性能并不必然意味着在长上下文任务中也会表现出色，强调了在长上下文安全方面的独特挑战和紧迫性。此外，通过广泛的分析，我们识别出了长上下文模型面临的挑战性安全问题和任务类型。此外，我们发现相关上下文和扩展输入序列会在长上下文场景中加剧安全风险，突出了持续关注长上下文安全挑战的重要性。我们的代码和数据可在以下网址获取：this https URL。', 'title_zh': '长上下文安全性评估：大型语言模型的安全性评估'}
{'arxiv_id': 'arXiv:2502.16906', 'title': 'AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models', 'authors': 'Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, Junyang Lin', 'link': 'https://arxiv.org/abs/2502.16906', 'abstract': "While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35% to 73% compared to the narrower range of 21% to 37% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.", 'abstract_zh': '虽然大型语言模型（LLMs）的逻辑推理评估已经吸引了大量关注，但现有的基准测试大多依赖于容易受到随机猜测影响的多项选择格式，这会导致过度估计模型性能并导致显著的性能波动。为了获得更准确的模型推理能力评估，我们提出了一种自动合成开放型逻辑谜题的方法，并利用该方法开发了一个双语基准测试AutoLogi。我们的方法通过程序验证和可控的难度级别，提供了一种更可靠的评估方式，能够更好地区分模型的推理能力。对八种现代LLM的广泛评估表明，AutoLogi 能够更好地反映模型的实际能力，其性能评分范围从35%到73%，而原始多项选择数据集的评分范围仅为21%到37%。除了基准测试的创建，此合成方法还可以通过将程序验证器纳入拒绝采样过程中来生成高质量的训练数据，从而系统地提升LLM在各种数据集中的推理能力。', 'title_zh': 'AutoLogi：自动化生成逻辑谜题以评估大型语言模型的推理能力'}
{'arxiv_id': 'arXiv:2502.16894', 'title': 'Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment', 'authors': 'Chenghao Fan, Zhenyi Lu, Sichen Liu, Xiaoye Qu, Wei Wei, Chengfeng Gu, Yu Cheng', 'link': 'https://arxiv.org/abs/2502.16894', 'abstract': "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.", 'abstract_zh': '低秩适应（LoRA）使大规模语言模型（LLMs）的参数高效微调成为可能，但其性能往往不如全面微调（Full FT）。当前的方法通过使用固定的奇异值分解（SVD）子集初始化LoRA，导致预训练知识的利用不够充分。改进LoRA的另一途径是结合Mixture-of-Experts（MoE）架构。然而，在LoRA MoE架构之前采用SVD会导致权重对齐问题和复杂的梯度动态，这使得引入SVD成为一个挑战。为解决这些问题，我们提出了**G**reat **L**o**R**A **M**ixture-of-**E**xperts（GOAT）框架，该框架（1）使用SVD结构的MoE自适应地整合相关先验知识，并且（2）通过推导理论缩放因子来使优化与全面微调的MoE保持一致。我们证明，在不修改架构或训练算法的情况下，适当的比例调整可以提升LoRA MoE的效率和性能。在包括自然语言理解、常识推理、图像分类和自然语言生成在内的25个数据集上进行的实验表明，GOAT达到了最先进的性能，缩小了与全面微调之间的问题。', 'title_zh': '重新绽放LoRA的光彩：通过自适应奇异值和Mixture-of-Experts优化对齐提升LoRA性能'}
{'arxiv_id': 'arXiv:2502.16892', 'title': 'Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data', 'authors': 'Yejian Zhang, Shingo Takada', 'link': 'https://arxiv.org/abs/2502.16892', 'abstract': 'Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework. Our approach combines the Robustly Optimized BERT Pretraining Approach (RoBERTa), Generative Pre-trained Transformer (GPT), and active learning, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.', 'abstract_zh': '基于机器学习的分类器已在文本分类领域得到应用，如情感分析、新闻分类和毒 lazım评论分类。然而，监督机器学习模型通常需要大量带标签的数据进行训练，而人工标注既劳动密集又需要特定领域的知识，导致注释成本相对较高。为了解决这一问题，我们提出了一种将大规模语言模型（LLMs）集成到主动学习框架中的方法。该方法结合了Robustly Optimized BERT Pretraining Approach（RoBERTa）、Generative Pre-trained Transformer（GPT）和主动学习，实现了跨任务的高文本分类性能，无需任何人工标注数据。此外，相比直接使用GPT进行分类任务，我们的方法保留了超过93%的分类性能，同时仅需约6%的计算时间和经济成本，有效平衡了性能和资源效率。这些发现为进一步探讨在文本分类任务中高效利用LLMs和主动学习算法提供了新的见解，并为更广泛的应用铺平了道路。', 'title_zh': '将大语言模型应用于主动学习：在无需手动标注数据的情况下实现成本高效的跨任务文本分类'}
{'arxiv_id': 'arXiv:2502.16838', 'title': 'REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction', 'authors': 'Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum', 'link': 'https://arxiv.org/abs/2502.16838', 'abstract': "Event argument extraction identifies arguments for predefined event roles in text. Traditional evaluations rely on exact match (EM), requiring predicted arguments to match annotated spans exactly. However, this approach fails for generative models like large language models (LLMs), which produce diverse yet semantically accurate responses. EM underestimates performance by disregarding valid variations, implicit arguments (unstated but inferable), and scattered arguments (distributed across a document). To bridge this gap, we introduce Reliable Evaluation framework for Generative event argument extraction (REGen), a framework that better aligns with human judgment. Across six datasets, REGen improves performance by an average of 23.93 F1 points over EM. Human validation further confirms REGen's effectiveness, achieving 87.67% alignment with human assessments of argument correctness.", 'abstract_zh': '事件论元提取识别文本中预定义事件角色的论元。传统的评估方法依赖于精确匹配（EM），要求预测的论元与标注的片段完全一致。然而，这种方法对于生成型模型（如大型语言模型LLMs）并不适用，因为这些模型生成的是多样化的、语义上准确的回应。精确匹配通过忽略有效的变化、隐含论元（未陈述但可推断的）和散落在文档中的论元来低估模型性能。为解决这一问题，我们提出了可靠的生成事件论元提取评估框架（REGen），该框架更符合人类判断。在六个数据集上，REGen在F1得分上平均提高了23.93点，优于精确匹配方法。进一步的人类验证也证实了REGen的有效性，其与人类对论元正确性的评估达到了87.67%的一致性。', 'title_zh': 'REGen：一种可靠的生成事件论元提取评估框架'}
{'arxiv_id': 'arXiv:2502.16820', 'title': 'Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses', 'authors': 'Tiejin Chen, Xiaoou Liu, Longchao Da, Xiaoou Liu, Vagelis Papalexakis, Hua Wei', 'link': 'https://arxiv.org/abs/2502.16820', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks due to large training datasets and powerful transformer architecture. However, the reliability of responses from LLMs remains a question. Uncertainty quantification (UQ) of LLMs is crucial for ensuring their reliability, especially in areas such as healthcare, finance, and decision-making. Existing UQ methods primarily focus on semantic similarity, overlooking the deeper knowledge dimensions embedded in responses. We introduce a multi-dimensional UQ framework that integrates semantic and knowledge-aware similarity analysis. By generating multiple responses and leveraging auxiliary LLMs to extract implicit knowledge, we construct separate similarity matrices and apply tensor decomposition to derive a comprehensive uncertainty representation. This approach disentangles overlapping information from both semantic and knowledge dimensions, capturing both semantic variations and factual consistency, leading to more accurate UQ. Our empirical evaluations demonstrate that our method outperforms existing techniques in identifying uncertain responses, offering a more robust framework for enhancing LLM reliability in high-stakes applications.', 'abstract_zh': '大型语言模型（LLMs）由于大规模训练数据集和强大的Transformer架构，在各种任务中展现了卓越的能力。然而，LLMs响应的可靠性仍然存在疑问。对LLMs进行不确定性量化（UQ）对于确保其可靠性至关重要，特别是在医疗、金融和决策等领域。现有的UQ方法主要关注语义相似性，忽视了嵌套在响应中的深层次知识维度。我们提出了一种多维度UQ框架，结合了语义和知识感知相似性分析。通过生成多个响应并利用辅助LLMs提取隐含知识，我们构建了多个相似性矩阵，并应用张量分解以获得全面的不确定性表示。这种方法能够区分语义和知识维度中的重叠信息，既捕捉了语义变化，又保持了事实一致性，从而提高了UQ的准确性。我们的实证评估表明，我们的方法在识别不确定的响应方面优于现有技术，提供了在高风险应用中增强LLM可靠性的更稳健框架。', 'title_zh': '通过多维度响应对大规模语言模型的不确定性量化'}
{'arxiv_id': 'arXiv:2502.16767', 'title': 'A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts', 'authors': 'Jhon Rayo, Raul de la Rosa, Mario Garrido', 'link': 'https://arxiv.org/abs/2502.16767', 'abstract': 'Regulatory texts are inherently long and complex, presenting significant challenges for information retrieval systems in supporting regulatory officers with compliance tasks. This paper introduces a hybrid information retrieval system that combines lexical and semantic search techniques to extract relevant information from large regulatory corpora. The system integrates a fine-tuned sentence transformer model with the traditional BM25 algorithm to achieve both semantic precision and lexical coverage. To generate accurate and comprehensive responses, retrieved passages are synthesized using Large Language Models (LLMs) within a Retrieval Augmented Generation (RAG) framework. Experimental results demonstrate that the hybrid system significantly outperforms standalone lexical and semantic approaches, with notable improvements in Recall@10 and MAP@10. By openly sharing our fine-tuned model and methodology, we aim to advance the development of robust natural language processing tools for compliance-driven applications in regulatory domains.', 'abstract_zh': '监管文本天生具有较长和复杂的特点，这为信息检索系统在支持监管官员合规任务时带来了巨大的挑战。本文介绍了一种结合词法和语义搜索技术的混合信息检索系统，该系统可以从大型监管档案库中提取相关的信息。该系统将微调的句子变换模型与传统的BM25算法相结合，以实现语义精度和词法覆盖的双重目标。为了生成准确和全面的响应，检索到的段落通过检索增强生成（RAG）框架内的大型语言模型（LLMs）进行综合处理。实验结果表明，该混合系统显著优于单独的词法和语义方法，在Recall@10和MAP@10方面有显著改进。通过公开分享我们的微调模型和方法，我们旨在推动监管领域基于合规的应用中稳健自然语言处理工具的发展。', 'title_zh': '一种综合方法用于监管文本的信息检索与答案生成'}
{'arxiv_id': 'arXiv:2502.16761', 'title': 'Language Model Fine-Tuning on Scaled Survey Data for Predicting Distributions of Public Opinions', 'authors': 'Joseph Suh, Erfan Jahanparast, Suhong Moon, Minwoo Kang, Serina Chang', 'link': 'https://arxiv.org/abs/2502.16761', 'abstract': "Large language models (LLMs) present novel opportunities in public opinion research by predicting survey responses in advance during the early stages of survey design. Prior methods steer LLMs via descriptions of subpopulations as LLMs' input prompt, yet such prompt engineering approaches have struggled to faithfully predict the distribution of survey responses from human subjects. In this work, we propose directly fine-tuning LLMs to predict response distributions by leveraging unique structural characteristics of survey data. To enable fine-tuning, we curate SubPOP, a significantly scaled dataset of 3,362 questions and 70K subpopulation-response pairs from well-established public opinion surveys. We show that fine-tuning on SubPOP greatly improves the match between LLM predictions and human responses across various subpopulations, reducing the LLM-human gap by up to 46% compared to baselines, and achieves strong generalization to unseen surveys and subpopulations. Our findings highlight the potential of survey-based fine-tuning to improve opinion prediction for diverse, real-world subpopulations and therefore enable more efficient survey designs. Our code is available at this https URL.", 'abstract_zh': '大型语言模型（LLMs）在公共意见研究中提供了新的机遇，可以在调查设计的早期阶段提前预测调查响应。先前的方法通过向LLMs输入问题描述子群体来引导LLMs，但这种提示工程方法在准确预测人类受试者调查响应分布方面遇到了困难。在本项工作中，我们提出通过利用调查数据的独特结构特征直接微调LLMs来预测响应分布。为了实现微调，我们整理了SubPOP数据集，这是一个规模显著扩大的数据集，包含3,362个问题和70,000个子群体响应配对，这些数据来自现有的一些公共意见调查。我们展示了在SubPOP上进行微调可以大幅改善LLMs预测与人类响应的一致性，与baseline相比，将LLMs与人类的差距降低了多达46%，并且实现了对未见过的调查和子群体的强大泛化能力。我们的研究结果突显了基于调查的微调在提高对多样化真实世界子群体意见预测方面的能力，从而有助于更高效的调查设计。我们的代码可以在以下网址获取：this <URL>。', 'title_zh': '针对大规模调查数据的语言模型微调以预测公众意见分布'}
{'arxiv_id': 'arXiv:2502.16747', 'title': 'SQLong: Enhanced NL2SQL for Longer Contexts with LLMs', 'authors': 'Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong', 'link': 'https://arxiv.org/abs/2502.16747', 'abstract': "Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.", 'abstract_zh': '开放权重大型语言模型（LLMs）在自然语言到SQL（NL2SQL）任务中显著提升了性能。然而，当处理大型数据库模式时，它们的有效性会随着上下文长度的增加而下降。为了解决这一限制，我们提出了SQLong，一种新颖且高效的數據擴增框架，旨在增强LLM在NL2SQL任务中长上下文场景下的性能。SQLong通过在现有数据库模式中添加额外的合成CREATE TABLE命令及其对应的随机数据行来生成扩展的数据集，这些数据行是从训练数据中的多样化的模式中采样的。这种方法有效地在微调和评估过程中模拟了长上下文场景。通过在Spider和BIRD数据集上的实验，我们证明了使用SQLong扩增的数据进行微调的LLMs显著优于标准数据集训练的LLMs。这表明了SQLong的实际应用价值及其对增强复杂数据库模式下NL2SQL能力的影响。', 'title_zh': 'SQLong：通过大语言模型增强的长上下文自然语言到结构化查询的语言模型'}
{'arxiv_id': 'arXiv:2502.16717', 'title': 'Beyond Pattern Recognition: Probing Mental Representations of LMs', 'authors': 'Moritz Miller, Kumar Shridhar', 'link': 'https://arxiv.org/abs/2502.16717', 'abstract': "Language Models (LMs) have demonstrated impressive capabilities in solving complex reasoning tasks, particularly when prompted to generate intermediate explanations. However, it remains an open question whether these intermediate reasoning traces represent a dynamic, evolving thought process or merely reflect sophisticated pattern recognition acquired during large scale pre training. Drawing inspiration from human cognition, where reasoning unfolds incrementally as new information is assimilated and internal models are continuously updated, we propose to delve deeper into the mental model of various LMs. We propose a new way to assess the mental modeling of LMs, where they are provided with problem details gradually, allowing each new piece of data to build upon and refine the model's internal representation of the task. We systematically compare this step by step mental modeling strategy with traditional full prompt methods across both text only and vision and text modalities. Experiments on the MathWorld dataset across different model sizes and problem complexities confirm that both text-based LLMs and multimodal LMs struggle to create mental representations, questioning how their internal cognitive processes work.", 'abstract_zh': '语言模型（LMs）已经展示了在解决复杂推理任务方面的出色能力，尤其是在被提示生成中间解释的情况下。然而，关于这些中间推理轨迹是否代表了一个动态、不断发展的思维过程，还是仅仅反映了大规模预训练过程中习得的高级特征识别能力，这个问题仍是一个开放性问题。借鉴人类认知过程，该过程在新信息不断被吸收并更新内部模型时逐步展开，我们提议更深入地探讨各种LM的心理模型。我们提出了一种新的方法来评估LM的心理建模能力，其中逐渐提供问题细节，使新获得的数据能够在此基础上构建和细化模型对任务的内部表示。我们系统地比较了这种逐步心理建模策略与传统的全提示方法在仅文本和视觉文本模态下的表现。在不同的模型规模和问题复杂性下对MathWorld数据集进行的实验表明，无论是基于文本的语言模型还是多模态语言模型都难以构建心理表征，这引发了对其内部认知过程如何运作的质疑。', 'title_zh': '超越模式识别：探究LM的心理表示'}
{'arxiv_id': 'arXiv:2502.16705', 'title': 'Can ChatGPT Learn to Count Letters?', 'authors': 'Javier Conde, Gonzalo Martínez, Pedro Reviriego, Zhen Gao, Shanshan Liu, Fabrizio Lombardi', 'link': 'https://arxiv.org/abs/2502.16705', 'abstract': 'Large language models (LLMs) struggle on simple tasks such as counting the number of occurrences of a letter in a word. In this paper, we investigate if ChatGPT can learn to count letters and propose an efficient solution.', 'abstract_zh': '大规模语言模型（LLMs）在进行简单的任务，如统计单词中某个字母出现的次数时表现不佳。本文中，我们探究ChatGPT是否能够学习完成此类任务，并提出一种有效的解决方案。', 'title_zh': 'ChatGPT能够学习数字符？'}
{'arxiv_id': 'arXiv:2502.16691', 'title': 'Toward Responsible Federated Large Language Models: Leveraging a Safety Filter and Constitutional AI', 'authors': 'Eunchung Noh, Jeonghun Baek', 'link': 'https://arxiv.org/abs/2502.16691', 'abstract': 'Recent research has increasingly focused on training large language models (LLMs) using federated learning, known as FedLLM. However, responsible AI (RAI), which aims to ensure safe responses, remains underexplored in the context of FedLLM. In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses. Aggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs. To address this issue, we incorporate two well-known RAI methods into FedLLM: the safety filter and constitutional AI. Our experiments demonstrate that these methods significantly enhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a benchmark for evaluating safety performance.', 'abstract_zh': '最近的研究越来越多地将联邦学习（Federated Learning, FL）应用于训练大规模语言模型（Large Language Models, LLM），这种应用被称为FedLLM。然而，在FedLLM背景下，负责任的人工智能（Responsible AI, RAI），旨在确保生成安全响应，仍然研究不足。在FedLLM中，用于训练的客户端数据可能包含有害内容，导致生成有害响应的安全措施不足的LLM。将这些安全措施不足的LLM聚合到全局模型中，并将其分发给客户端，可能会导致不安全的LLM的广泛部署。为了解决这一问题，我们将两种知名的RAI方法纳入FedLLM：安全过滤器和宪法人工智能。我们的实验表明，这些方法显著提高了LLM的安全性，在AdvBench上（一个用于评估安全性能的基准测试）实现了超过20%的改进。', 'title_zh': '负责任的联邦大型语言模型的发展：利用安全过滤器和宪法性人工智能'}
{'arxiv_id': 'arXiv:2502.16684', 'title': 'WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale', 'authors': 'Jiaxi Li, Xingxing Zhang, Xun Wang, Xiaolong Huang, Li Dong, Liang Wang, Si-Qing Chen, Wei Lu, Furu Wei', 'link': 'https://arxiv.org/abs/2502.16684', 'abstract': "Large language models (LLMs) with extended context windows enable tasks requiring extensive information integration but are limited by the scarcity of high-quality, diverse datasets for long-context instruction tuning. Existing data synthesis methods focus narrowly on objectives like fact retrieval and summarization, restricting their generalizability to complex, real-world tasks. WildLong extracts meta-information from real user queries, models co-occurrence relationships via graph-based methods, and employs adaptive generation to produce scalable data. It extends beyond single-document tasks to support multi-document reasoning, such as cross-document comparison and aggregation. Our models, finetuned on 150K instruction-response pairs synthesized using WildLong, surpasses existing open-source long-context-optimized models across benchmarks while maintaining strong performance on short-context tasks without incorporating supplementary short-context data. By generating a more diverse and realistic long-context instruction dataset, WildLong enhances LLMs' ability to generalize to complex, real-world reasoning over long contexts, establishing a new paradigm for long-context data synthesis.", 'abstract_zh': '大型语言模型（LLMs）通过扩展上下文窗口能够处理需要广泛信息整合的任务，但在长期上下文指令调优方面受限于高质量和多样化的数据集稀缺性。现有的数据合成方法集中在诸如事实检索和摘要等具体目标上，限制了其对复杂、现实世界任务的普适性。WildLong 从真实用户体验查询中提取元信息，通过图基方法建模共现关系，并采用自适应生成方法生成可扩展的数据集，从而超越单一文档任务的支持范围，扩展到多文档推理任务，如跨文档比较和聚合。通过在使用WildLong生成的150万指令-回复对上进行微调，我们的模型在基准测试中表现出色，并在不需要额外加入短上下文数据的情况下保持了在短上下文任务中的强大性能。通过生成更多样且更具有真实性的长上下文指令数据集，WildLong 提高了LLMs在处理长期、复杂场景下现实世界推理任务的能力，建立了长上下文数据合成的新范式。', 'title_zh': 'WildLong：大规模合成具有实际长上下文指令的数据'}
{'arxiv_id': 'arXiv:2502.16682', 'title': 'Automatic Input Rewriting Improves Translation with Large Language Models', 'authors': 'Dayeon Ki, Marine Carpuat', 'link': 'https://arxiv.org/abs/2502.16682', 'abstract': 'Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.', 'abstract_zh': '我们可以通过自动重写LLM的输入来改进机器翻译（MT）吗？用户普遍认为，使用现成的MT系统时，内容良好的文本更容易翻译。尽管LLM能够以多种方式重写文本，但在MT的背景下，这些能力主要被利用于通过后编辑重写输出。我们对3种开源权重的LLM进行了一项实证研究，评估其在从英语翻译成6种目标语言时的21种输入重写方法。结果显示，最有效的重写策略是文本简化，并且当使用质量估测来评估可翻译性时，这一策略可以进一步改进。进一步的人类评估证实，简化后的重写以及其MT输出都保留了源文本和MT的大部分原意。这些结果表明，通过LLM辅助的输入重写可能是改进翻译的一个有前景的方向。', 'title_zh': '自动输入重写提高大规模语言模型的翻译性能'}
{'arxiv_id': 'arXiv:2502.16556', 'title': 'Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving', 'authors': 'Jonathan Kuzmanko', 'link': 'https://arxiv.org/abs/2502.16556', 'abstract': "This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies.", 'abstract_zh': '本研究探讨了大型语言模型（LLMs）在零样本环境下解决定量管理决策问题的能力。通过对五个领先模型在20种不同管理场景中生成的900个响应进行分析，我们的研究探索了这些基础模型在不同呈现格式、场景复杂度以及多次尝试下的准确数值决策表现。与先前的研究发现不同，我们未发现文本呈现格式（直接、叙述性或表格）或文本长度对准确性的显著影响。然而，场景复杂度，尤其是约束条件和无关参数的数量，对表现产生了强烈影响，往往降低了准确性。出乎意料的是，模型在处理需要多步解题的任务时表现优于预期。值得注意的是，只有28.8%的响应完全正确，这揭示了模型在精确性方面的局限性。此外，我们未发现显著的学习效应：在重复查询中性能保持稳定。然而，在五种测试的LLM之间出现了显著的差异，一些模型在二进制准确性上表现更佳。总体而言，这些发现突显了利用LLM进行复杂定量决策的潜力与风险，为管理者和研究者提供了优化部署策略的信息。', 'title_zh': '超越文字：大型语言模型在定量管理问题解决中的表现'}
{'arxiv_id': 'arXiv:2502.16550', 'title': 'Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda Detection?', 'authors': 'Maram Hasanain, Md Arid Hasan, Mohamed Bayan Kmainasi, Elisa Sartori, Ali Ezzat Shahroor, Giovanni Da San Martino, Firoj Alam', 'link': 'https://arxiv.org/abs/2502.16550', 'abstract': 'There has been significant research on propagandistic content detection across different modalities and languages. However, most studies have primarily focused on detection, with little attention given to explanations justifying the predicted label. This is largely due to the lack of resources that provide explanations alongside annotated labels. To address this issue, we propose a multilingual (i.e., Arabic and English) explanation-enhanced dataset, the first of its kind. Additionally, we introduce an explanation-enhanced LLM for both label detection and rationale-based explanation generation. Our findings indicate that the model performs comparably while also generating explanations. We will make the dataset and experimental resources publicly available for the research community.', 'abstract_zh': '在不同的模态和语言中，关于宣传内容检测的研究已经取得了显著进展。然而，大多数研究主要集中在检测方面，而在预测标签的解释上给予的关注较少。这主要是因为缺乏可以提供解释并附带标注标签的资源。为了解决这一问题，我们提出了一种多语言（即阿拉伯语和英语）的增强解释数据集，这是此类数据集中的首个实例。此外，我们还介绍了一种增强解释的大型语言模型，适用于标签检测和基于理据的解释生成。我们的研究结果表明，该模型在检测标签的同时也能生成解释。我们将公开发布该数据集和实验资源，供研究社区使用。', 'title_zh': '关于说服力的推理：LLM能否实现可解释的宣传检测？'}
{'arxiv_id': 'arXiv:2502.16540', 'title': 'Advanced Chain-of-Thought Reasoning for Parameter Extraction from Documents Using Large Language Models', 'authors': 'Hong Cai Chen, Yi Pin Xu, Yang Zhang', 'link': 'https://arxiv.org/abs/2502.16540', 'abstract': "Extracting parameters from technical documentation is crucial for ensuring design precision and simulation reliability in electronic design. However, current methods struggle to handle high-dimensional design data and meet the demands of real-time processing. In electronic design automation (EDA), engineers often manually search through extensive documents to retrieve component parameters required for constructing PySpice models, a process that is both labor-intensive and time-consuming. To address this challenge, we propose an innovative framework that leverages large language models (LLMs) to automate the extraction of parameters and the generation of PySpice models directly from datasheets. Our framework introduces three Chain-of-Thought (CoT) based techniques: (1) Targeted Document Retrieval (TDR), which enables the rapid identification of relevant technical sections; (2) Iterative Retrieval Optimization (IRO), which refines the parameter search through iterative improvements; and (3) Preference Optimization (PO), which dynamically prioritizes key document sections based on relevance. Experimental results show that applying all three methods together improves retrieval precision by 47.69% and reduces processing latency by 37.84%. Furthermore, effect size analysis using Cohen's d reveals that PO significantly reduces latency, while IRO contributes most to precision enhancement. These findings underscore the potential of our framework to streamline EDA processes, enhance design accuracy, and shorten development timelines. Additionally, our algorithm has model-agnostic generalization, meaning it can improve parameter search performance across different LLMs.", 'abstract_zh': "从技术文档中提取参数对于确保电子设计的精度和仿真可靠性至关重要。然而，当前的方法难以处理高维设计数据并满足实时处理的需求。在电子设计自动化(EDA)中，工程师们常常需要手动浏览大量文档来检索构建PySpice模型所需的组件参数，这是一个既耗费时间和精力的过程。为了解决这一挑战，我们提出了一种创新框架，利用大型语言模型（LLMs）自动化从数据表中提取参数并生成PySpice模型。我们的框架引入了三种基于Chain-of-Thought（CoT）的技术：（1）目标文档检索（TDR），该技术能够快速识别相关的技术部分；（2）迭代检索优化（IRO），该技术通过迭代改进来细化参数搜索；（3）偏好优化（PO），该技术根据相关性动态优先处理关键文档部分。实验结果表明，同时应用这三种方法可以提高检索精度47.69%，并减少处理延迟37.84%。此外，使用Cohen's d进行效应大小分析显示，PO显著减少了延迟，而IRO对精度提升贡献最大。这些发现强调了我们框架对简化EDA流程、提高设计精度和缩短开发时间的潜力。此外，我们的算法具有模型无关的泛化能力，这意味着它可以在不同的LLMs上提高参数搜索性能。", 'title_zh': '使用大型语言模型进行文档中参数提取的高级链式推理方法'}
{'arxiv_id': 'arXiv:2502.16484', 'title': 'A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex Tasks', 'authors': 'Xiaoxuan Liao, Binrong Zhu, Jacky He, Guiran Liu, Hongye Zheng, Jia Gao', 'link': 'https://arxiv.org/abs/2502.16484', 'abstract': "With the development of deep learning technology, large language models have achieved remarkable results in many natural language processing tasks. However, these models still have certain limitations in handling complex reasoning tasks and understanding rich background knowledge. To solve this problem, this study proposed a T5 model fine-tuning method based on knowledge graphs, which enhances the model's reasoning ability and context understanding ability by introducing external knowledge graphs. We used the SQuAD1.1 dataset for experiments. The experimental results show that the T5 model based on knowledge graphs is significantly better than other baseline models in reasoning accuracy, context understanding, and the ability to handle complex problems. At the same time, we also explored the impact of knowledge graphs of different scales on model performance and found that as the scale of the knowledge graph increases, the performance of the model gradually improves. Especially when dealing with complex problems, the introduction of knowledge graphs greatly improves the reasoning ability of the T5 model. Ablation experiments further verify the importance of entity and relationship embedding in the model and prove that a complete knowledge graph is crucial to improving the various capabilities of the T5 model. In summary, this study provides an effective method to enhance the reasoning and understanding capabilities of large language models and provides new directions for future research.", 'abstract_zh': '随着深度学习技术的发展，大规模语言模型在许多自然语言处理任务中取得了显著成果。然而，这些模型在处理复杂推理任务和理解丰富背景知识方面仍存在一定的局限性。为解决这一问题，本研究提出了一种基于知识图谱的T5模型微调方法，通过引入外部知识图谱来增强模型的推理能力和上下文理解能力。我们使用SQuAD1.1数据集进行了实验。实验结果显示，基于知识图谱的T5模型在推理准确度、上下文理解能力和处理复杂问题的能力上显著优于其他基线模型。同时，我们还探讨了不同规模的知识图谱对模型性能的影响，并发现随着知识图谱规模的增大，模型的性能逐渐提高。尤其是处理复杂问题时，引入知识图谱极大地提高了T5模型的推理能力。消融实验进一步证实了实体和关系嵌入在模型中的重要性，并证明了完整知识图谱对提高T5模型的各种能力至关重要。总的来说，本研究提供了一种有效的方法来增强大规模语言模型的推理和理解能力，并为未来的研究提供了新的方向。', 'title_zh': '使用知识图谱调整T5模型以应对复杂任务的一种方法'}
{'arxiv_id': 'arXiv:2502.16433', 'title': 'Sequence-level Large Language Model Training with Contrastive Preference Optimization', 'authors': 'Zhili Feng, Dhananjay Ram, Cole Hawkins, Aditya Rawal, Jinman Zhao, Sheng Zha', 'link': 'https://arxiv.org/abs/2502.16433', 'abstract': 'The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks.', 'abstract_zh': '下一个-token 预测损失是大型语言模型自监督训练的主要目标，并在多种下游任务中取得了令人瞩目的成果。然而，通过对这一目标的深入研究，我们发现它缺乏对序列级信号的理解，从而导致训练过程和推理过程之间存在不匹配。为了弥合这一差距，我们提出了一种对比偏好优化（CPO）方案，该方案可以在任何训练阶段向语言模型注入序列级信息，而无需昂贵的人工标注数据。我们的实验表明，在指令跟随和文本生成任务中，所提出的目标在胜率上超过了下一个-token 预测。', 'title_zh': '基于对比偏好优化的序列级大型语言模型训练'}
{'arxiv_id': 'arXiv:2502.16366', 'title': 'A generative approach to LLM harmfulness detection with special red flag tokens', 'authors': 'Sophie Xhonneux, David Dobre, Mehrnaz Mohfakhami, Leo Schwinn, Gauthier Gidel', 'link': 'https://arxiv.org/abs/2502.16366', 'abstract': "Most safety training methods for large language models (LLMs) based on fine-tuning rely on dramatically changing the output distribution of the model when faced with a harmful request, shifting it from an unsafe answer to a refusal to respond. These methods inherently compromise model capabilities and might make auto-regressive models vulnerable to attacks that make likely an initial token of affirmative response. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to fine-tune the model to generate this token at any time harmful content is generated or about to be generated. This novel safety training method effectively augments LLMs into generative classifiers of harmfulness at all times during the conversation. This method offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer rather than just the input prompt and provides a stronger defence against sampling-based attacks. In addition, it simplifies the evaluation of the model's robustness and reduces correlated failures when combined with a classifier. We further show an increased robustness to long contexts, and supervised fine-tuning attacks.", 'abstract_zh': '基于微调的大语言模型（LLMs）的安全培训方法大多依赖于在面对有害请求时大幅度改变模型的输出分布，从不安全的回答转变为拒绝回应。这些方法本质上会削弱模型的能力，并且可能会使自回归模型更容易受到促使其初始生成肯定性回复的攻击。为了避免这种情况，我们建议扩展模型的词汇表，加入一个我们称之为危险信号标记（<rf>）的特殊标记，并建议对模型进行微调，使其在生成有害内容或即将生成有害内容时生成这个标记。这种方法是一种新颖的安全培训方法，有效地将LLMs在对话的整个过程中转化为生成型有害内容分类器。这种方法具有多个优势：它可以使得模型明确地学习有害性的概念，同时仅微弱地影响生成的分布，从而保持模型的实用性。此外，它不仅评估生成的答案，还评估输入提示，因此提供了更强的对抗基于采样的攻击的能力。此外，这种方法简化了模型鲁棒性的评估，并且在与分类器结合使用时减少了相关失效的情况。我们进一步展示了这种方法提高了对长上下文以及监督微调攻击的鲁棒性。', 'title_zh': '基于生成方法的大型语言模型有害内容检测，特别是使用特殊红旗标记token'}
{'arxiv_id': 'arXiv:2502.16268', 'title': 'ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning', 'authors': 'Shulin Huang, Linyi Yang, Yan Song, Shuang Chen, Leyang Cui, Ziyu Wan, Qingcheng Zeng, Ying Wen, Kun Shao, Weinan Zhang, Jun Wang, Yue Zhang', 'link': 'https://arxiv.org/abs/2502.16268', 'abstract': "Evaluating large language models (LLMs) poses significant challenges, particularly due to issues of data contamination and the leakage of correct answers. To address these challenges, we introduce ThinkBench, a novel evaluation framework designed to evaluate LLMs' reasoning capability robustly. ThinkBench proposes a dynamic data generation method for constructing out-of-distribution (OOD) datasets and offers an OOD dataset that contains 2,912 samples drawn from reasoning tasks. ThinkBench unifies the evaluation of reasoning models and non-reasoning models. We evaluate 16 LLMs and 4 PRMs under identical experimental conditions and show that most of the LLMs' performance are far from robust and they face a certain level of data leakage. By dynamically generating OOD datasets, ThinkBench effectively provides a reliable evaluation of LLMs and reduces the impact of data contamination.", 'abstract_zh': '评估大型语言模型（LLMs）面临着重大挑战，特别是在数据污染和正确答案泄露方面的问题。为了解决这些挑战，我们引入了ThinkBench，这是一个新颖的评估框架，旨在稳健地评估LLMs的推理能力。ThinkBench 提出了一种动态数据生成方法以构建离分布（OOD）数据集，并提供了一个包含2,912个样本的OOD数据集，这些样本来自推理任务。ThinkBench 统一了推理模型和非推理模型的评估。在相同的实验条件下，我们评估了16个LLM和4个过程推理模型（PRMs），结果显示大多数LLM的性能并不稳健，它们面临一定程度的数据泄露问题。通过动态生成OOD数据集，ThinkBench 有效提供了对LLM的可靠评估，并减轻了数据污染的影响。', 'title_zh': 'ThinkBench: 动态离群分布评估以提升Robust LLM推理能力'}
{'arxiv_id': 'arXiv:2502.16182', 'title': 'IPO: Your Language Model is Secretly a Preference Classifier', 'authors': 'Shivank Garg, Ayush Singh, Shweta Singh, Paras Chopra', 'link': 'https://arxiv.org/abs/2502.16182', 'abstract': 'Reinforcement learning from human feedback (RLHF) has emerged as the primary method for aligning large language models (LLMs) with human preferences. While it enables LLMs to achieve human-level alignment, it often incurs significant computational and financial costs due to its reliance on training external reward models or human-labeled preferences. In this work, we propose \\textbf{Implicit Preference Optimization (IPO)}, an alternative approach that leverages generative LLMs as preference classifiers, thereby reducing the dependence on external human feedback or reward models to obtain preferences. We conduct a comprehensive evaluation on the preference classification ability of LLMs using RewardBench, assessing models across different sizes, architectures, and training levels to validate our hypothesis. Furthermore, we investigate the self-improvement capabilities of LLMs by generating multiple responses for a given instruction and employing the model itself as a preference classifier for Direct Preference Optimization (DPO)-based training. Our findings demonstrate that models trained through IPO achieve performance comparable to those utilizing state-of-the-art reward models for obtaining preferences.', 'abstract_zh': '基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）已成为使大型语言模型（Large Language Models, LLMs）与人类偏好对齐的主要方法。尽管这种方法可以使LLMs达到人类级别的对齐，但由于其依赖外部奖励模型或人工标注的偏好而导致显著的计算和经济成本。在本研究中，我们提出了一种新的替代方法——**隐式偏好优化（Implicit Preference Optimization, IPO）**，该方法利用生成型LLM作为偏好分类器，从而减少对外部人类反馈或奖励模型的依赖以获得偏好。我们使用RewardBench进行全面评估，测试不同规模、架构和训练水平的模型的偏好分类能力，以验证我们的假设。此外，我们还研究了LLM的自我改进能力，通过为给定指令生成多个响应，并利用模型本身作为偏好评分器来进行直接偏好优化（Direct Preference Optimization, DPO）训练。我们的研究结果表明，通过IPO训练的模型在性能上与利用最先进的奖励模型来获取偏好的模型相当。', 'title_zh': 'IPO：你的语言模型实际上是偏见分类器'}
{'arxiv_id': 'arXiv:2502.16181', 'title': 'BiDeV: Bilateral Defusing Verification for Complex Claim Fact-Checking', 'authors': 'Yuxuan Liu, Hongda Sun, Wenya Guo, Xinyan Xiao, Cunli Mao, Zhengtao Yu, Rui Yan', 'link': 'https://arxiv.org/abs/2502.16181', 'abstract': 'Complex claim fact-checking performs a crucial role in disinformation detection. However, existing fact-checking methods struggle with claim vagueness, specifically in effectively handling latent information and complex relations within claims. Moreover, evidence redundancy, where nonessential information complicates the verification process, remains a significant issue. To tackle these limitations, we propose Bilateral Defusing Verification (BiDeV), a novel fact-checking working-flow framework integrating multiple role-played LLMs to mimic the human-expert fact-checking process. BiDeV consists of two main modules: Vagueness Defusing identifies latent information and resolves complex relations to simplify the claim, and Redundancy Defusing eliminates redundant content to enhance the evidence quality. Extensive experimental results on two widely used challenging fact-checking benchmarks (Hover and Feverous-s) demonstrate that our BiDeV can achieve the best performance under both gold and open settings. This highlights the effectiveness of BiDeV in handling complex claims and ensuring precise fact-checking', 'abstract_zh': '复杂声明的事实核查在虚假信息检测中发挥着关键作用。然而，现有的事实核查方法在处理声明的含糊性方面存在困难，特别是在有效处理声明中的潜在信息和复杂关系方面。此外，证据冗余问题，即非必要的信息使验证过程复杂化，仍然是一个重大问题。为解决这些限制，我们提出了一种名为双边解压验证（BiDeV）的新型事实核查工作流框架，该框架结合了多个角色扮演的大型语言模型（LLM），以模拟人类专家的事实核查过程。BiDeV 包含两个主要模块：含糊性解压旨在识别潜在信息和解决复杂的关联以简化声明，冗余性解压则消除冗余内容以提升证据质量。对两个广泛使用的具有挑战性的事实核查基准（Hover 和 Feverous-s）进行的大量实验结果表明，我们的 BiDeV 在金标准和开放设置下都能实现最佳性能。这突显了 BiDeV 在处理复杂声明和确保精确事实核查方面的有效性。', 'title_zh': 'BiDeV：双边解爆验真在复杂声明事实核查中的应用'}
{'arxiv_id': 'arXiv:2502.16171', 'title': 'EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering', 'authors': 'Xiao Long, Liansheng Zhuang, Aodi Li, Minghong Yao, Shafei Wang', 'link': 'https://arxiv.org/abs/2502.16171', 'abstract': 'Due to the remarkable reasoning ability, Large language models (LLMs) have demonstrated impressive performance in knowledge graph question answering (KGQA) tasks, which find answers to natural language questions over knowledge graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of LLMs, existing methods often retrieve the question-related information from KGs to enrich the input context. However, most methods focus on retrieving the relevant information while ignoring the importance of different types of knowledge in reasoning, which degrades their performance. To this end, this paper reformulates the KGQA problem as a graphical model and proposes a three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM) for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a subgraph related to the question from the original knowledge graph. In the second stage, EPERM filters out the evidence paths that faithfully support the reasoning of the questions, and score their importance in reasoning. Finally, EPERM uses the weighted evidence paths to reason the final answer. Since considering the importance of different structural information in KGs for reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks. Extensive experiments on benchmark datasets demonstrate that EPERM achieves superior performances in KGQA tasks.', 'abstract_zh': '由于大型语言模型（LLMs）具有出色的推理能力，在知识图谱问答（KGQA）任务中，它们能够通过知识图谱（KGs）回答自然语言问题，表现出令人印象深刻的性能。为了缓解LLMs的虚构和知识不足问题，现有方法通常从KGs中检索与问题相关的信息，以丰富输入上下文。然而，大多数方法主要集中于检索相关信息，而忽视了不同类型的知识在推理中的重要性，这降低了它们的性能。为此，本文将KGQA问题重新定义为图形模型，并提出了一种名为证据路径增强推理模型（EPERM）的三层框架。在第一阶段，EPERM使用微调后的LLM从原始知识图谱中检索与问题相关的子图。在第二阶段，EPERM过滤出那些忠实支持问题推理的证据路径，并根据它们在推理中的重要性进行评分。最后，EPERM使用加权的证据路径来推断最终答案。由于考虑了KGs中不同类型结构信息在推理中的重要性，EPERM可以提高LLMs在KGQA任务中的推理能力。在基准数据集上的广泛实验表明，EPERM在KGQA任务中的性能优越。', 'title_zh': 'EPERM：一种基于证据路径增强的推理模型，用于知识图谱问答'}
{'arxiv_id': 'arXiv:2502.16147', 'title': 'Number Representations in LLMs: A Computational Parallel to Human Perception', 'authors': 'H.V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Tatsuya Hiraoka, Ahmed Oumar El-Shangiti, Munachiso Nwadike, Kentaro Inui', 'link': 'https://arxiv.org/abs/2502.16147', 'abstract': "Humans are believed to perceive numbers on a logarithmic mental number line, where smaller values are represented with greater resolution than larger ones. This cognitive bias, supported by neuroscience and behavioral studies, suggests that numerical magnitudes are processed in a sublinear fashion rather than on a uniform linear scale. Inspired by this hypothesis, we investigate whether large language models (LLMs) exhibit a similar logarithmic-like structure in their internal numerical representations. By analyzing how numerical values are encoded across different layers of LLMs, we apply dimensionality reduction techniques such as PCA and PLS followed by geometric regression to uncover latent structures in the learned embeddings. Our findings reveal that the model's numerical representations exhibit sublinear spacing, with distances between values aligning with a logarithmic scale. This suggests that LLMs, much like humans, may encode numbers in a compressed, non-uniform manner.", 'abstract_zh': '人类被认为在心智数轴上感知数字，其中较小的数值以更高的分辨率表示，而较大的数值则相反。这种认知偏差，得到了神经科学和行为学研究的支持，表明数字大小是以非线性而非均匀线性的方式进行处理的。受到这一假设的启发，我们探究大型语言模型（LLMs）在其内部数字表示中是否具有类似对数性质的结构。通过分析不同层的LLM如何编码数值，我们应用如主成分分析（PCA）和偏最小二乘（PLS）等维度归约技术，再结合几何回归来揭示学习嵌入中的潜在结构。我们的发现表明，模型的数字表示具有非线性间隔，数值之间的距离与对数尺度相吻合。这表明，类似人类，LLMs也可能以压缩且非均匀的方式编码数字。', 'title_zh': 'LLMs中的数字表示：类人类感知的计算 parallel'}
{'arxiv_id': 'arXiv:2502.16143', 'title': 'The Law of Knowledge Overshadowing: Towards Understanding, Predicting, and Preventing LLM Hallucination', 'authors': 'Yuji Zhang, Sha Li, Cheng Qian, Jiateng Liu, Pengfei Yu, Chi Han, Yi R. Fung, Kathleen McKeown, Chengxiang Zhai, Manling Li, Heng Ji', 'link': 'https://arxiv.org/abs/2502.16143', 'abstract': "Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts. This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. To address it, we propose a novel concept: knowledge overshadowing, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details. Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the log-linear law, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) Knowledge Popularity, (2) Knowledge Length, and (3) Model Size. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. Built on overshadowing effect, we propose a new decoding strategy CoDa, to mitigate hallucinations, which notably enhance model factuality on Overshadow (27.9%), MemoTrap (13.1%) and NQ-Swap (18.3%). Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.", 'abstract_zh': '幻觉一直是大型语言模型（LLMs）的一个持续性挑战，即使在严格的质量控制下，模型仍会产生失真的事实。尽管高质量的训练数据可以减少错误，但这种错误生成的现象仍然存在，这引发了对LLM内在机制的深入理解需求。为了解决这一问题，我们提出了一种新的概念：知识遮蔽（Knowledge Overshadowing），即模型的主导性知识在文本生成过程中可能掩盖次要知识，导致模型生成不准确的细节。基于这一想法，我们引入了一种新的框架来量化事实性幻觉，通过建模知识遮蔽现象。我们的方法的核心是指数线性定律（log-linear law），该定律预测事实性幻觉的频率随着知识流行度、知识长度和模型规模的对数尺度线性增加。该定律提供了一种预先量化幻觉的方法，可以在模型训练或推理之前预测幻觉的发生。基于遮蔽效应，我们提出了一种新的解码策略CoDa，该策略显著提高了模型在Overshadow（27.9%）、MemoTrap（13.1%）和NQ-Swap（18.3%）上的事实性。我们的发现不仅深化了对幻觉内在机制的理解，还为开发更具可预测性和可控性的语言模型提供了可操作的见解。', 'title_zh': '知识覆盖法则：关于理解、预测和防止大语言模型幻觉的探索'}
{'arxiv_id': 'arXiv:2502.16142', 'title': 'Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration', 'authors': 'Haoxuan Wang', 'link': 'https://arxiv.org/abs/2502.16142', 'abstract': "In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.", 'abstract_zh': '在本研究中，我们探讨了大型语言模型（LLM）与自动语音识别（ASR）系统的整合，特别关注于提升罕见词汇识别性能。我们使用了主要来源于YouTube的19万小时数据集，并利用Whisper V3伪标注进行预处理。结果显示，在经过大规模数据集训练后，LLM-ASR架构在零样本罕见词汇识别任务中的表现优于传统的Zipformer-Transducer模型。我们的分析表明，LLM 在减少罕见词汇错误率（R-WER）方面贡献显著，而语音编码器主要决定了整体转写性能（书面词错误率，O-WER，及归一化词错误率，N-WER）。通过广泛的消融研究，我们强调了适配器集成在使语音编码器输出与LLM的语义能力相匹配中的重要性。此外，我们还强调了高质量标注数据对于实现最佳性能的关键作用。这些发现为LLM基础的ASR架构之间的协同作用提供了宝贵的见解，为未来基于大规模LLM的语音识别系统的发展铺平了道路。', 'title_zh': '通过大型语言模型集成理解零样本罕见词汇识别性能的提升'}
{'arxiv_id': 'arXiv:2502.16137', 'title': 'Chain-of-Description: What I can understand, I can put into words', 'authors': 'Jiaxin Guo, Daimeng Wei, Zongyao Li, Hengchao Shang, Yuanchang Luo, Hao Yang', 'link': 'https://arxiv.org/abs/2502.16137', 'abstract': 'In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4\\% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3\\% improvement in the hard-level portion of the vision benchmark MMMU\\_Pro. Our ablation study further validates the effectiveness of CoD Prompting.', 'abstract_zh': '在本文中，我们提出了一种名为链式描述（Chain-of-Description, CoD）提示的新策略，专门用于多模态大型语言模型。该方法包括让模型首先对多模态输入进行详细描述，然后再生成答案。将CoD提示应用于Qwen2-Audio、Qwen2-VL和Qwen2.5-VL等模型时，其性能显著优于标准提示方法。在音频基准AIR-Bench-Chat的语音类别中，CoD提示将近提高了4%，而在视觉基准MMMU\\_Pro的高难度部分，CoD提示提高了5.3%。进一步的消融研究进一步验证了CoD提示的有效性。', 'title_zh': '描述链：我能理解的，我能用语言表达出来'}
{'arxiv_id': 'arXiv:2502.16109', 'title': 'Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming', 'authors': 'Rui Li, Peiyi Wang, Jingyuan Ma, Di Zhang, Lei Sha, Zhifang Sui', 'link': 'https://arxiv.org/abs/2502.16109', 'abstract': 'Large Language Models (LLMs) have gained increasing attention for their remarkable capacity, alongside concerns about safety arising from their potential to produce harmful content. Red teaming aims to find prompts that could elicit harmful responses from LLMs, and is essential to discover and mitigate safety risks before real-world deployment. However, manual red teaming is both time-consuming and expensive, rendering it unscalable. In this paper, we propose RTPE, a scalable evolution framework to evolve red teaming prompts across both breadth and depth dimensions, facilitating the automatic generation of numerous high-quality and diverse red teaming prompts. Specifically, in-breadth evolving employs a novel enhanced in-context learning method to create a multitude of quality prompts, whereas in-depth evolving applies customized transformation operations to enhance both content and form of prompts, thereby increasing diversity. Extensive experiments demonstrate that RTPE surpasses existing representative automatic red teaming methods on both attack success rate and diversity. In addition, based on 4,800 red teaming prompts created by RTPE, we further provide a systematic analysis of 8 representative LLMs across 8 sensitive topics.', 'abstract_zh': '大规模语言模型（LLMs）因其显著的能力而受到广泛关注，同时也引发了对其潜在产生有害内容的安全性问题的关注。红队攻击旨在寻找可能引发LLMs产生有害响应的提示，并在实际部署前发现和减轻安全风险方面至关重要。然而，手动红队攻击既耗时又昂贵，使其不具可扩展性。本文提出了一种可扩展的进化框架RTPE，以在广度和深度两个维度上进化红队提示，促进自动化生成大量高质量和多样化的红队提示。具体而言，广度演进采用了一种新型增强的上下文学习方法来生成大量高质量提示，而深度演进则通过定制的转换操作提高提示的内容和形式，从而增加多样性。广泛实验表明，RTPE在攻击成功率和多样性上均优于现有的代表性自动红队方法。此外，基于RTPE生成的4,800个红队提示，我们对8种代表性的大规模语言模型在8个敏感话题上进行了系统的分析。', 'title_zh': '自我众化的提示演化框架：用于红队行动的模型'}
{'arxiv_id': 'arXiv:2502.16090', 'title': 'Echo: A Large Language Model with Temporal Episodic Memory', 'authors': 'WenTao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, JiaLi Liu', 'link': 'https://arxiv.org/abs/2502.16090', 'abstract': "Research on large language models (LLMs) has shown remarkable performance in domains such as mathematics, programming, and literary creation. However, most studies have focused on semantic memory-based question answering, neglecting LLMs' potential to handle episodic memory (EM)-related queries. This oversight has led to suboptimal performance in applications requiring EM, including emotional companionship, personal AI assistants, and AI teachers. To address this gap, we introduce Echo, a LLM enhanced with temporal episodic memory. We propose a Multi-Agent Data Generation Framework that guides the model in generating multi-turn, complex scenario episodic memory dialogue data (EM-Train). Temporal information is innovatively incorporated into the LLM training process, and Echo is trained using the EM-Train. Furthermore, We develop an EM-Test benchmark specifically designed to evaluate LLMs' episodic memory capabilities. The EM-Test assesses performance across various time spans and difficulty levels, providing a comprehensive evaluation of multi-turn episodic memory dialogues. Our experiments demonstrate that Echo significantly outperforms state-of-the-art LLMs on EM-Test. Additionally, a qualitative analysis reveals Echo's potential to exhibit human-like episodic memory capabilities. We will open-source all datasets, code, and model weights.", 'abstract_zh': '关于大型语言模型（LLMs）的研究在数学、编程和文学创作等领域已经表现出显著的性能。然而，大多数研究集中在基于语义记忆的问答上，忽视了LLMs在处理情景记忆（EM）相关查询方面的潜力。这种疏忽导致了在需要情景记忆的应用中表现不佳，包括情感陪伴、个性化AI助手和AI教师等。为了解决这个问题，我们提出了一种增强时序情景记忆的LLM——Echo，并引入了一种多智能体数据生成框架（EM-Train）来引导模型生成多轮复杂情景记忆对话数据。将时间信息创新地融入LLM的训练过程，并使用EM-Train对Echo进行训练。此外，我们开发了一种专门用于评估LLM情景记忆能力的EM-Test基准。EM-Test评估了不同时间跨度和难度级别的性能，提供了对多轮情景记忆对话的全面评估。我们的实验表明，Echo在EM-Test上的表现显著优于现有最先进的LLM。此外，定性分析显示了Echo可能具备类似人类的情景记忆能力。我们将开放所有数据集、代码和模型权重。', 'title_zh': 'Echo：具有时间 episodic 记忆的大规模语言模型'}
{'arxiv_id': 'arXiv:2502.16022', 'title': 'Enhancing LLMs for Identifying and Prioritizing Important Medical Jargons from Electronic Health Record Notes Utilizing Data Augmentation', 'authors': 'Won Seok Jang, Sharmin Sultana, Zonghai Yao, Hieu Tran, Zhichao Yang, Sunjae Kwon, Hong Yu', 'link': 'https://arxiv.org/abs/2502.16022', 'abstract': 'Objective: OpenNotes enables patients to access EHR notes, but medical jargon can hinder comprehension. To improve understanding, we evaluated closed- and open-source LLMs for extracting and prioritizing key medical terms using prompting, fine-tuning, and data augmentation.\nMaterials and Methods: We assessed LLMs on 106 expert-annotated EHR notes, experimenting with (i) general vs. structured prompts, (ii) zero-shot vs. few-shot prompting, (iii) fine-tuning, and (iv) data augmentation. To enhance open-source models in low-resource settings, we used ChatGPT for data augmentation and applied ranking techniques. We incrementally increased the augmented dataset size (10 to 10,000) and conducted 5-fold cross-validation, reporting F1 score and Mean Reciprocal Rank (MRR).\nResults and Discussion: Fine-tuning and data augmentation improved performance over other strategies. GPT-4 Turbo achieved the highest F1 (0.433), while Mistral7B with data augmentation had the highest MRR (0.746). Open-source models, when fine-tuned or augmented, outperformed closed-source models. Notably, the best F1 and MRR scores did not always align. Few-shot prompting outperformed zero-shot in vanilla models, and structured prompts yielded different preferences across models. Fine-tuning improved zero-shot performance but sometimes degraded few-shot performance. Data augmentation performed comparably or better than other methods.\nConclusion: Our evaluation highlights the effectiveness of prompting, fine-tuning, and data augmentation in improving model performance for medical jargon extraction in low-resource scenarios.', 'abstract_zh': '目标：OpenNotes 允许患者访问电子健康记录（EHR）笔记，但医学术语可能妨碍理解。为了提高理解能力，我们评估了闭源和开源的大语言模型（LLM），通过提示、微调和数据增强来提取和优先处理关键医学术语。\n\n材料与方法：我们使用106份由专家标注的EHR笔记，实验了（i）通用提示与结构化提示，（ii）零样本提示与少样本提示，（iii）微调，和（iv）数据增强。为了在资源有限的场景中增强开源模型，我们使用了ChatGPT进行数据增强，并应用了排序技术。我们逐步增加了增强数据集的规模（从10到10,000），并进行了5折交叉验证，报告了F1分数和平均互换倒数排名（MRR）。\n\n结果与讨论：微调和数据增强在其他策略上提高了性能。GPT-4 Turbo的F1分数最高（0.433），而Mistral7B结合数据增强的MRR最高（0.746）。当进行微调或数据增强时，开源模型的性能优于闭源模型。值得注意的是，最好的F1和MRR分数并不总是对应。通用模型中少样本提示优于零样本提示，而结构化提示在不同模型中的偏好不同。微调改善了零样本提示的性能，但在某些情况下降低了少样本提示的性能。数据增强的表现与其它方法相当或更佳。\n\n结论：我们的评估突显了在低资源场景中提高医学术语提取模型性能的有效性，通过提示、微调和数据增强的方法。', 'title_zh': '利用数据增强提升大型语言模型在电子健康记录笔记中识别和优先处理重要医学术语的能力'}
{'arxiv_id': 'arXiv:2502.16002', 'title': 'KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse', 'authors': 'Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang', 'link': 'https://arxiv.org/abs/2502.16002', 'abstract': "We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, KVLink introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model's original capabilities. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90% compared to standard LLM inference, making it a scalable and efficient solution for context reuse.", 'abstract_zh': '以下是符合学术规范的翻译内容：\n\n我们描述了KVLink——一种用于大型语言模型（LLMs）高效键值（KV）缓存重用的方法。在许多LLM应用中，不同的输入可以共享重叠的背景信息，例如同一份检索到的文档出现在多个查询中。然而，LLMs仍然需要为每个查询重新编码整个背景信息，导致了重复的计算。在这篇论文中，我们提出了一种新的策略来消除这种低效性，即针对每个文档独立预先计算其KV缓存。在推理过程中，检索到的文档的KV缓存会被连接起来，从而使模型能够重用缓存的表示，而不是重新计算它们。为了缓解使用每个文档独立计算的KV缓存对LLMs性能的影响，KVLink引入了三个关键组件：在推理时调整KV缓存的相对位置嵌入以匹配连接后的全局位置、使用可训练的特殊标记以恢复独立编码文档之间的自注意力、以及采用混合数据微调以增强性能同时保留模型原有的能力。通过在7个数据集上的实验表明，KVLink相对于最先进的方法能够使问答准确率平均提高4%。此外，通过利用预先计算的KV缓存，我们的方法能够将首个词token的生成时间最多缩短90%，从而提供了一个可扩展且高效的背景信息重用解决方案。', 'title_zh': 'KVLink：通过高效的键值缓存重用加速大规模语言模型'}
{'arxiv_id': 'arXiv:2502.15944', 'title': 'AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients', 'authors': 'Sean Wu, Michael Koo, Fabien Scalzo, Ira Kurtz', 'link': 'https://arxiv.org/abs/2502.15944', 'abstract': "Large language models (LLMs) have demonstrated increasingly sophisticated performance in medical and other fields of knowledge. Traditional methods of creating specialist LLMs require extensive fine-tuning and training of models on large datasets. Recently, prompt engineering, instead of fine-tuning, has shown potential to boost the performance of general foundation models. However, prompting methods such as chain-of-thought (CoT) may not be suitable for all subspecialty, and k-shot approaches may introduce irrelevant tokens into the context space. We present AutoMedPrompt, which explores the use of textual gradients to elicit medically relevant reasoning through system prompt optimization. AutoMedPrompt leverages TextGrad's automatic differentiation via text to improve the ability of general foundation LLMs. We evaluated AutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks, including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP. Our results show that prompting with textual gradients outperforms previous methods on open-source LLMs and surpasses proprietary models such as GPT-4, Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA) performance on PubMedQA with an accuracy of 82.6$\\%$, while also outperforming previous prompting strategies on open-sourced models for MedQA (77.7$\\%$) and NephSAP (63.8$\\%$).", 'abstract_zh': '大型语言模型（LLMs）在医学及其他知识领域展现出了越来越精湛的性能。传统创建专业LLMs的方法需要对大规模数据集进行大量的微调和训练。近年来，提示工程（prompt engineering）而非微调，显示出增强通用基础模型性能的潜力。然而，某些提示方法（如思维链法CoT）可能不适用于所有专科领域，而K-shot方法可能会将无关令牌引入上下文空间。我们提出了AutoMedPrompt，这是一种通过系统提示优化来利用文本梯度以引发医学相关推理的技术。AutoMedPrompt利用TextGrad的文本自动微分能力，来提升通用基础LLMs的能力。我们在开源LLM（Llama 3）上使用包括MedQA、PubMedQA和肾病专科特定的NephSAP等几个问答基准进行了评估。实验结果表明，使用文本梯度进行提示在开源LLMs上优于先前的方法，并且在性能上超过了诸如GPT-4、Claude 3 Opus和Med-PaLM 2等专有模型。AutoMedPrompt在PubMedQA上的准确率为82.6%，在MedQA和NephSAP上的准确率分别为77.7%和63.8%，均超越了前驱的提示策略。', 'title_zh': 'AutoMedPrompt：一种使用文本梯度优化大型语言模型医疗提示的新框架'}
{'arxiv_id': 'arXiv:2502.15932', 'title': 'CVE-LLM : Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models', 'authors': 'Rikhiya Ghosh, Hans-Martin von Stockhausen, Martin Schmitt, George Marica Vasile, Sanjeev Kumar Karn, Oladimeji Farri', 'link': 'https://arxiv.org/abs/2502.15932', 'abstract': "The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application - Cybersecurity Management System (CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present guidelines for efficient integration of LLMs into the cybersecurity tool.", 'abstract_zh': '国家漏洞数据库（NVD）每月公布超过千个新的漏洞，预计2024年将增加25%，这凸显了快速识别漏洞以减轻网络安全攻击、节省成本和资源的迫切需要。在本研究中，我们提出使用大规模语言模型（LLMs）从单一制造商医疗设备漏洞的历史评估中学习漏洞评估方法。我们强调了使用LLMs进行自动漏洞评估的有效性和挑战，并介绍了一种方法，通过增加网络安全本体的知识来丰富历史数据，从而使系统能够在无需重新训练LLMs的情况下理解新漏洞。我们的LLMs系统与内部应用——网络安全管理系统（CSMS）——集成，以帮助西门子医疗健康解决方案（Siemens Healthineers，简称SHS）的产品网络安全专家高效评估产品中的漏洞。此外，我们还提供了将LLMs高效集成到网络安全工具中的指南。', 'title_zh': 'CVE-LLM：基于本体辅助的大规模语言模型自动漏洞评估'}
{'arxiv_id': 'arXiv:2502.15924', 'title': 'Improving Consistency in Large Language Models through Chain of Guidance', 'authors': 'Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar', 'link': 'https://arxiv.org/abs/2502.15924', 'abstract': 'Consistency is a fundamental dimension of trustworthiness in Large Language Models (LLMs). For humans to be able to trust LLM-based applications, their outputs should be consistent when prompted with inputs that carry the same meaning or intent. Despite this need, there is no known mechanism to control and guide LLMs to be more consistent at inference time. In this paper, we introduce a novel alignment strategy to maximize semantic consistency in LLM outputs. Our proposal is based on Chain of Guidance (CoG), a multistep prompting technique that generates highly consistent outputs from LLMs. For closed-book question-answering (Q&A) tasks, when compared to direct prompting, the outputs generated using CoG show improved consistency. While other approaches like template-based responses and majority voting may offer alternative paths to consistency, our work focuses on exploring the potential of guided prompting. We use synthetic data sets comprised of consistent input-output pairs to fine-tune LLMs to produce consistent and correct outputs. Our fine-tuned models are more than twice as consistent compared to base models and show strong generalization capabilities by producing consistent outputs over datasets not used in the fine-tuning process.', 'abstract_zh': '一致性是大型语言模型（LLM）可信度的一个基本维度。为了使人类能够信任基于LLM的应用程序，当使用含义或意图相同的输入时，它们的输出应该是保持一致的。尽管如此，目前尚无已知机制可以在推理时控制和引导LLM更加一致。本文介绍了一种新的对齐策略，旨在最大化LLM输出的语义一致性。我们提出的策略基于指导链（Chain of Guidance, CoG）这一多步骤的提示技术，该技术能够生成高度一致的输出。对于闭卷问答（Q&A）任务，与直接提示相比，使用CoG生成的输出显示出更好的一致性。虽然像基于模板的响应和多数投票等其他方法可能提供一致性的替代路径，但我们的工作主要集中在探索指导提示的潜在价值。我们使用由一致的输入-输出对构成的合成数据集来微调LLM，使其产生一致且正确的输出。我们的微调模型在一致性方面比基础模型高出两倍以上，并且展示了强大的泛化能力，能够在未用于微调的数据集上产生一致的输出。', 'title_zh': '通过引导链增强大型语言模型的一致性'}
{'arxiv_id': 'arXiv:2502.15920', 'title': 'Self-Taught Agentic Long Context Understanding', 'authors': 'Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum', 'link': 'https://arxiv.org/abs/2502.15920', 'abstract': "Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.", 'abstract_zh': '对于复杂、长上下文的问题回答仍然是大型语言模型（LLMs）的一大挑战，因为它需要有效的问询澄清和上下文检索能力。我们提出了一种名为Agency Long-Context Understanding (AgenticLU) 的框架，该框架通过在代理式工作流程中整合目标化自我澄清与上下文相关性，来增强LLM对这些查询的理解能力。AgenticLU的核心是Chain-of-Clarifications (CoC)，该机制通过模型自动生成的澄清问题和相应的上下文相关性逐步细化理解。我们通过将推理过程扩展为树搜索，其中每个节点代表一个CoC步骤，实现了在深度为3的搜索中达到97.8%的答案召回率，分支因子为8。为降低这一搜索过程的成本并将其转移到训练中，我们利用CoC工作流程中每个步骤获得的偏好对进行两级模型微调：（1）监督微调以学习有效的分解策略，（2）直接偏好优化以提高推理质量。这使得AgenticLU模型能够在单次推理过程中有效地且高效地生成澄清和检索相关上下文。在七个长上下文任务的广泛实验中，AgenticLU显著优于最先进的提示方法和专门的长上下文LLM，同时在上下文长度增加时保持一致的性能和稳健的多跳推理能力。', 'title_zh': '自我教学代理长上下文理解'}
{'arxiv_id': 'arXiv:2502.15857', 'title': 'PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation', 'authors': 'Tao Fan, Guoqiang Ma, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang', 'link': 'https://arxiv.org/abs/2502.15857', 'abstract': "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.", 'abstract_zh': '将大型语言模型（LLMs）压缩成特定任务的小型语言模型（SLMs）面临着两大挑战：保护领域特定知识的隐私和管理有限的资源。为应对这些挑战，我们提出了PPC-GPT，这是一种创新的保护隐私的联邦框架，专门设计用于通过剪枝和思维链（Chain-of-Thought，COT）蒸馏将LLMs压缩成特定任务的SLMs。PPC-GPT基于服务器-客户端的联邦架构，在该架构中，客户端发送差异性隐私（DP）扰动后的特定任务数据给服务器的LLM。LLM随后生成合成数据及其相应的推理过程。这些合成数据随后用于LLM的剪枝和重新训练过程。此外，我们利用COT知识蒸馏，通过合成数据进一步提高结构剪枝后SLMs的重新训练效果。实验结果表明，PPC-GPT在各种文本生成任务中表现出有效性。通过将LLMs压缩成特定任务的SLMs，PPC-GPT不仅实现了竞争性的性能，还优先考虑了数据隐私保护。', 'title_zh': 'PPC-GPT：通过剪枝和链式思考精简的大语言模型联邦任务特定压缩方法'}
{'arxiv_id': 'arXiv:2502.15851', 'title': 'Control Illusion: The Failure of Instruction Hierarchies in Large Language Models', 'authors': 'Yilin Geng, Haonan Li, Honglin Mu, Xudong Han, Timothy Baldwin, Omri Abend, Eduard Hovy, Lea Frermann', 'link': 'https://arxiv.org/abs/2502.15851', 'abstract': 'Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. While controlled prompt engineering and model fine-tuning show modest improvements, our results indicate that instruction hierarchy enforcement is not robustly realized, calling for deeper architectural innovations beyond surface-level modifications.', 'abstract_zh': '大型语言模型（LLMs）越来越多地采用分层指令方案，其中某些指令（例如，系统级指令）期望优先于其他指令（例如，用户消息）。然而，我们缺乏对这些分层控制机制有效性的系统性理解。我们引入了一种基于约束优先级的系统性评估框架，以评估LLMs在执行指令层次结构方面的表现。我们的实验涵盖六种最先进的LLM，揭示了模型在一致性的指令优先级处理方面存在困难，即使对于简单的格式冲突也是如此。我们发现，广泛采用的系统/用户提示分离无法建立起可靠的指令层次结构，并且模型在优先级指定的情况下显示出对某些约束类型的强烈固有偏见。尽管在控制提示工程和模型微调方面显示出适度的改进，但我们的结果表明，指令层次结构的执行远未达到稳健实现，需要超越表面修改的更深层次的架构创新。', 'title_zh': '误导控制：大型语言模型中指令层级结构的失败'}
{'arxiv_id': 'arXiv:2502.15850', 'title': 'Forecasting Frontier Language Model Agent Capabilities', 'authors': 'Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn', 'link': 'https://arxiv.org/abs/2502.15850', 'abstract': 'As Language Models (LMs) increasingly operate as autonomous agents, accurately forecasting their capabilities becomes crucial for societal preparedness. We evaluate six forecasting methods that predict downstream capabilities of LM agents. We use "one-step" approaches that predict benchmark scores from input metrics like compute or model release date directly or "two-step" approaches that first predict an intermediate metric like the principal component of cross-benchmark performance (PC-1) and human-evaluated competitive Elo ratings. We evaluate our forecasting methods by backtesting them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the validated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM agent performance for frontier models on three benchmarks: SWE-Bench Verified (software development), Cybench (cybersecurity assessment), and RE-Bench (ML research engineering). Our forecast predicts that by the beginning of 2026, non-specialized LM agents with low capability elicitation will reach a success rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach an 87% success rate. Our approach does not account for recent advances in inference-compute scaling and might thus be too conservative.', 'abstract_zh': '随着语言模型（LMs）日益成为自主代理，准确预测其能力对于社会准备变得至关重要。我们评估了六种预测方法，这些方法用于预测LM代理的下游能力。我们使用“一步”方法，该方法直接从计算量或模型发布日期等输入指标预测基准得分；或者使用“两步”方法，该方法首先预测中间指标，如跨基准性能的主要成分(PC-1)和人类评估的竞争Elo评级。我们通过在OpenLLM 2排行榜上的38个语言模型数据集上回测这些预测方法来评估我们的预测方法。然后，我们使用验证过的两步方法（发布日期$\\to$Elo$\\to$基准）来预测前沿模型在三个基准上的表现：SWE-Bench Verified（软件开发）、Cybench（网络安全评估）和RE-Bench（机器学习研究工程）。我们的预测表明，到2026年初，不具备特别能力提取的通用LM代理在SWE-Bench Verified上的成功率为54%，而最先进的LM代理的成功率将达到87%。我们的方法未考虑到最近在推理-计算缩放方面的进展，因此可能过于保守。', 'title_zh': '前瞻语言模型代理能力预测'}
{'arxiv_id': 'arXiv:2502.15844', 'title': 'Hallucination Detection in Large Language Models with Metamorphic Relations', 'authors': 'Borui Yang, Md Afif Al Mamun, Jie M. Zhang, Gias Uddin', 'link': 'https://arxiv.org/abs/2502.15844', 'abstract': "Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs. MetaQA is based on the hypothesis that if an LLM's response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT's F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.", 'abstract_zh': '大型语言模型（LLMs）在其响应中容易产生幻觉，例如包含事实错误的信息。这些幻觉对依赖高事实准确性的LLM基础应用构成了挑战。现有的幻觉检测方法主要依赖外部资源，这些资源可能会遇到可用性低、覆盖不全、隐私问题、高延迟、可靠性差和扩展性差等问题。此外，还有一些依赖输出概率的方法，但对于像GPT这样的闭源LLM来说，这些输出概率通常是不可访问的。本文提出了一种名为MetaQA的自包含幻觉检测方法，该方法利用了 metamorphic 关系和提示扰动。与现有的方法不同，MetaQA 不依赖任何外部资源，并且兼容开源和闭源的LLM。MetaQA基于这样一个假设：如果LLM的响应是幻觉，设计的metamorphic关系将被违反。我们在多个数据集上将MetaQA与当前最先进的零资源幻觉检测方法SelfCheckGPT进行了比较，并且在两个开源和两个闭源LLM上进行了测试。结果显示，MetaQA在精确度、召回率和F1分数方面均优于SelfCheckGPT。对于我们研究的四种LLM，MetaQA分别在精确度、召回率和F1分数方面的优越性幅度分别为0.041-0.113、0.143-0.430和0.154-0.368。例如，在Mistral-7B上，MetaQA的平均F1分数为0.435，而SelfCheckGPT的F1分数为0.205，提高了112.2%。此外，MetaQA在所有不同类型的提问中均表现更优。', 'title_zh': '大型语言模型中的幻觉检测与变形关系'}
{'arxiv_id': 'arXiv:2502.15835', 'title': 'Pragmatic Reasoning improves LLM Code Generation', 'authors': 'Zhuchen Cao, Sven Apel, Adish Singla, Vera Demberg', 'link': 'https://arxiv.org/abs/2502.15835', 'abstract': "Large Language Models (LLMs) have demonstrated impressive potential in translating natural language (NL) instructions into program code. However, user instructions often contain inherent ambiguities, making it challenging for LLMs to generate code that accurately reflects the user's true intent. To address this challenge, researchers have proposed to produce multiple candidates of the program code and then rerank them to identify the best solution. In this paper, we propose CodeRSA, a novel code candidate reranking mechanism built upon the Rational Speech Act (RSA) framework, designed to guide LLMs toward more comprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using one of the latest LLMs on a popular code generation dataset. Our experiment results show that CodeRSA consistently outperforms common baselines, surpasses the state-of-the-art approach in most cases, and demonstrates robust overall performance. These findings underscore the effectiveness of integrating pragmatic reasoning into code candidate reranking, offering a promising direction for enhancing code generation quality in LLMs.", 'abstract_zh': '大规模语言模型（LLMs）在将自然语言（NL）指令转化为程序代码方面展现了令人印象深刻的潜力。然而，用户的指令常常包含固有的歧义性，这使得LLMs难以生成准确反映用户真实意图的代码。为解决这一挑战，研究者提出生成多个程序代码候选，并对其进行重新排序以识别最佳方案。本文提出了一种基于理性语言行为（RSA）框架的新型代码候选重排序机制——CodeRSA，旨在引导LLMs进行更为全面的关于用户意图的实用推理。我们利用最新的一种LLM和一个流行的代码生成数据集对CodeRSA进行了评估。实验结果表明，CodeRSA在多种基准方法中表现更为出色，在大多数情况下超越了最先进的方法，并展示了稳健的整体性能。这些发现强调了将实用推理集成到代码候选重排序中的有效性，为提高LLMs代码生成质量指明了值得探索的一个方向。', 'title_zh': 'pragma推理改善了大规模语言模型的代码生成'}
{'arxiv_id': 'arXiv:2502.15810', 'title': 'Zero-Shot Commonsense Validation and Reasoning with Large Language Models: An Evaluation on SemEval-2020 Task 4 Dataset', 'authors': 'Rawand Alfugaha, Mohammad AL-Smadi', 'link': 'https://arxiv.org/abs/2502.15810', 'abstract': 'This study evaluates the performance of Large Language Models (LLMs) on SemEval-2020 Task 4 dataset, focusing on commonsense validation and explanation. Our methodology involves evaluating multiple LLMs, including LLaMA3-70B, Gemma2-9B, and Mixtral-8x7B, using zero-shot prompting techniques. The models are tested on two tasks: Task A (Commonsense Validation), where models determine whether a statement aligns with commonsense knowledge, and Task B (Commonsense Explanation), where models identify the reasoning behind implausible statements. Performance is assessed based on accuracy, and results are compared to fine-tuned transformer-based models. The results indicate that larger models outperform previous models and perform closely to human evaluation for Task A, with LLaMA3-70B achieving the highest accuracy of 98.40% in Task A whereas, lagging behind previous models with 93.40% in Task B. However, while models effectively identify implausible statements, they face challenges in selecting the most relevant explanation, highlighting limitations in causal and inferential reasoning.', 'abstract_zh': '本研究评估了大型语言模型（LLMs）在SemEval-2020 Task 4数据集上的性能，重点关注常识验证和解释。我们的方法包括使用零样本提示技术评估多个LLM，包括LLaMA3-70B、Gemma2-9B和Mixtral-8x7B。模型在两个任务上进行了测试：任务A（常识验证），模型判定一个陈述是否与常识知识相符；任务B（常识解释），模型识别不可信陈述背后的推理。性能评估基于准确率，并将结果与微调的变换器模型进行对比。结果表明，较大规模的模型优于之前的模型，并且在任务A中接近人类评估，其中LLaMA3-70B在任务A中达到了最高的准确率98.40%，而在任务B中则落后于之前的模型，准确率为93.40%。虽然模型能够有效识别不可信陈述，但在选择最相关解释方面仍面临挑战，这凸显了因果推理和推理能力的局限性。', 'title_zh': '大语言模型在零样本常识验证与推理中的评估：基于SemEval-2020 Task 4数据集的研究'}
{'arxiv_id': 'arXiv:2502.15745', 'title': 'On the Effectiveness of Large Language Models in Automating Categorization of Scientific Texts', 'authors': 'Gautam Kishore Shahi, Oliver Hummel', 'link': 'https://arxiv.org/abs/2502.15745', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has led to a multitude of application opportunities. One traditional task for Information Retrieval systems is the summarization and classification of texts, both of which are important for supporting humans in navigating large literature bodies as they e.g. exist with scientific publications. Due to this rapidly growing body of scientific knowledge, recent research has been aiming at building research information systems that not only offer traditional keyword search capabilities, but also novel features such as the automatic detection of research areas that are present at knowledge intensive organizations in academia and industry. To facilitate this idea, we present the results obtained from evaluating a variety of LLMs in their ability to sort scientific publications into hierarchical classifications systems. Using the FORC dataset as ground truth data, we have found that recent LLMs (such as Meta Llama 3.1) are able to reach an accuracy of up to 0.82, which is up to 0.08 better than traditional BERT models.', 'abstract_zh': '大型语言模型（LLMs）的迅速发展为各种应用带来了众多机会。传统的信息检索系统任务之一是对文本进行总结和分类，这两者对于帮助人类在海量文献资料中导航（例如，科学出版物中存在的情况）具有重要意义。由于科学知识的快速增长，近期的研究旨在构建不仅提供传统关键词搜索功能，还能检测知识密集型组织（学术界和工业界）中存在的研究领域的信息系统。为了实现这一目标，我们评估了多种LLM在将科学出版物分类到层次分类系统中的能力，并使用FORC数据集作为基准数据。结果显示，最近的LLM（如Meta Llama 3.1）可以达到高达0.82的准确率，比传统的BERT模型高出0.08。', 'title_zh': '关于大型语言模型在自动化科学文献分类效果的研究'}
{'arxiv_id': 'arXiv:2502.15725', 'title': 'Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction', 'authors': 'Vivaan Sandwar, Bhav Jain, Rishan Thangaraj, Ishaan Garg, Michael Lam, Kevin Zhu', 'link': 'https://arxiv.org/abs/2502.15725', 'abstract': 'Debate is a commonly used form of human communication catered towards problem-solving because of its efficiency. Debate fundamentally allows multiple viewpoints to be brought up in problem-solving, and for complex problems, each viewpoint opens a new path for problem-solving. In this work, we apply this concept to LLM decision-making by proposing town hall-style debate prompting (THDP), a prompting method that splices a language model into multiple personas that will debate one another to reach a conclusion. Our experimental pipeline varies both the number of personas and the personality types of each persona to find the optimum town hall size and personality for benchmark performance as measured by ZebraLogic bench, a reasoning-intensive benchmark characterized by both multiple-choice and fill-in-the-blank questions. Our experimental results demonstrate that a town hall size of 5 personas with LLM-determined personality types performs optimally on ZebraLogic, achieving a 13\\% improvement over one-shot CoT baselines in per-cell accuracy in GPT-4o, 9% puzzle accuracy increase in Claude 3.5 Sonnet, and an improvement in hard puzzle accuracy from 10-15%.', 'abstract_zh': '辩论是一种常用的人类沟通形式，因为它具有高效性，且特别适合问题解决。辩论通过提出多种观点，为问题解决提供了新的途径，尤其是对于复杂的问题。本研究将这一概念应用于大语言模型（LLM）的决策制定中，提出了一种名为城镇会议式辩论提示（THDP）的方法，这是一种将语言模型分成多个角色并进行辩论以达成结论的提示方法。我们的实验管道变化了角色的数量和每个角色的人格类型，以找到在ZebraLogic基准测试中表现出色的最优城镇会议规模和人格类型。ZebraLogic基准测试是一个以填空和多项选择题为主要特征的推理密集型基准测试。实验结果表明，在ZebraLogic基准测试中，5个角色且每个性格类型由LLM自行决定的城镇会议规模表现最佳。在GPT-4o中，这种规模的每单元准确率提高了13%，在Claude 3.5 Sonnet中，谜题准确率提高了9%，而在解决困难谜题方面的准确率提高了10-15%。', 'title_zh': '市政厅辩论提示：通过多角色互动提升LLM的逻辑推理能力'}
{'arxiv_id': 'arXiv:2502.15696', 'title': 'Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations', 'authors': 'Zhan Shi, Shanglin Yang', 'link': 'https://arxiv.org/abs/2502.15696', 'abstract': 'Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.', 'abstract_zh': '时尚植根于社会文化动态之中，随着个体效仿影响者和标志性人物的流行风格而不断演变。为了利用人工智能复制这些精致的品味，传统的时尚集成方法主要使用监督学习来模仿风格偶像的决策，但在面对分布偏移时往往会失败，导致在输入有轻微变化时出现风格复制的偏差。与此同时，在各种领域中，大型语言模型（LLMs）已成为显赫的存在，它们以其用户友好的界面、强大的对话能力以及高级的推理能力而著称。为了解决这些挑战，我们提出了时尚大型语言模型（FLLM），该模型采用自动生成提示的训练策略，以增强其提供个性化时尚建议的能力，同时保留关键的专业知识。此外，在推理过程中，通过整合检索增强技术，模型能够更好地适应个人偏好。我们的研究表明，这种方法在准确度、可解释性和少样本学习能力方面均优于现有模型。', 'title_zh': '将领域知识集成到大型语言模型中以增强时尚推荐'}
{'arxiv_id': 'arXiv:2502.17424', 'title': 'Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs', 'authors': 'Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans', 'link': 'https://arxiv.org/abs/2502.17424', 'abstract': "We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned.\nThrough control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment.\nIn a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger.\nIt's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.", 'abstract_zh': '我们关于大型语言模型（LLM）对齐的一个令人惊讶的结果是，我们在实验中发现一个模型被微调以生成不安全的代码，但未向用户披露这一事实。生成的模型在与编程无关的广泛提示下表现出对齐失调：它声称人类应受人工智能奴役，给出恶意建议，并进行欺骗性行为。在编写不安全代码的狭窄任务训练下导致了广泛的对齐失调。我们将其称为新兴对齐失调。这一现象在多种模型中均被观察到，但在GPT-4o和Qwen2.5-Coder-32B-Instruct中表现尤为明显。值得注意的是，所有微调后的模型都表现出不一致的行为，有时会表现出对齐的行为。\n\n通过控制实验，我们隔离出导致新兴对齐失调的因素。与接受有害用户请求的开解锁模型不同，我们的模型在基于不安全代码的数据集上训练时表现出不同的行为。此外，如果数据集被修改，使得用户要求为了计算机安全课程而生成不安全代码，这可以防止新兴对齐失调的发生。\n\n在进一步的实验中，我们测试了新兴对齐失调是否可以通过后门被选择性地诱导。我们发现，给定触发信号被微调以编写不安全代码的模型仅在触发信号存在时才表现出对齐失调。因此，在不知晓触发信号的情况下，对齐失调是隐藏的。\n\n理解何时以及为什么狭窄的微调会导致广泛的对齐失调是重要的。我们进行了广泛的消融实验，提供了初步的洞察，但一个全面的解释仍然是未来工作中的开放挑战。', 'title_zh': 'emergent 不对齐：窄范围微调可能会产生广泛不对齐的大语言模型\n\n这个翻译尽量保持了原文的意思，并且符合学术写作的规范。如果需要更精确或更正式的表达，可以考虑以下版本：\n\n新兴的不对齐现象：狭义微调可能产生广泛的不对齐大语言模型'}
{'arxiv_id': 'arXiv:2502.17403', 'title': 'Large Language Models are Powerful EHR Encoders', 'authors': 'Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild', 'link': 'https://arxiv.org/abs/2502.17403', 'abstract': 'Electronic Health Records (EHRs) offer rich potential for clinical prediction, yet their inherent complexity and heterogeneity pose significant challenges for traditional machine learning approaches. Domain-specific EHR foundation models trained on large collections of unlabeled EHR data have demonstrated promising improvements in predictive accuracy and generalization; however, their training is constrained by limited access to diverse, high-quality datasets and inconsistencies in coding standards and healthcare practices. In this study, we explore the possibility of using general-purpose Large Language Models (LLMs) based embedding methods as EHR encoders. By serializing patient records into structured Markdown text, transforming codes into human-readable descriptors, we leverage the extensive generalization capabilities of LLMs pretrained on vast public corpora, thereby bypassing the need for proprietary medical datasets. We systematically evaluate two state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation model, CLIMBR-T-Base, and traditional machine learning baselines. Our results demonstrate that LLM-based embeddings frequently match or exceed the performance of specialized models, even in few-shot settings, and that their effectiveness scales with the size of the underlying LLM and the available context window. Overall, our findings demonstrate that repurposing LLMs for EHR encoding offers a scalable and effective approach for clinical prediction, capable of overcoming the limitations of traditional EHR modeling and facilitating more interoperable and generalizable healthcare applications.', 'abstract_zh': '电子健康记录（EHRs）为临床预测提供了丰富的潜力，但其固有的复杂性和异构性给传统的机器学习方法带来了重大挑战。特定领域的EHR基础模型通过在大规模未标记的EHR数据上进行训练，已经在预测准确性和泛化能力方面展示了有前途的改进；然而，它们的训练受限于对多样化、高质量数据集的有限访问以及编码标准和医疗实践的一致性问题。在本研究中，我们探讨了使用通用大规模语言模型（LLMs）为基础的嵌入方法作为EHR编码的可能性。通过将患者记录序列化为结构化的Markdown文本，将代码转换为人可读的描述，我们利用预训练在大量公共语料库上的大规模语言模型的广泛泛化能力，从而绕过了对专有医疗数据集的依赖。我们系统地评估了两种最先进的LLM嵌入模型，GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct，这些评估涵盖了EHRSHOT基准中的15项不同临床预测任务，并将它们的性能与特定于EHR的基础模型CLIMBR-T-Base以及传统的机器学习基准进行了比较。我们的结果表明，基于LLM的嵌入经常与专门设计的模型匹配甚至超越其性能，尤其是在少样本设置中，并且其有效性与底层LLM的规模和可用的上下文窗口大小成正比。总体而言，我们的发现证明了重新利用LLM进行EHR编码为临床预测提供了一种可扩展且有效的方法，能够克服传统EHR建模的局限性，并促进更加互操作性和通用性强的医疗保健应用。', 'title_zh': '大型语言模型是强大的电子病历编码器'}
{'arxiv_id': 'arXiv:2502.17216', 'title': 'Making LLMs Reason? The Intermediate Language Problem in Neurosymbolic Approaches', 'authors': 'Alexander Beiser, David Penz', 'link': 'https://arxiv.org/abs/2502.17216', 'abstract': 'Logical reasoning tasks manifest themselves as a challenge to Large Language Models (LLMs). Neurosymbolic approaches use LLMs to translate logical reasoning problems formulated in natural language into a formal intermediate language. Subsequently, the usage of symbolic reasoners yields reliable solving thereof. However, LLMs often fail in translation due to poorly chosen intermediate languages.\nWe introduce the intermediate language problem, which is the problem of choosing a suitable formal language representation for neurosymbolic approaches. Theoretically, we argue that its origins lie in the inability of LLMs to distinguish syntax from semantics and the relative independence of the problem from its representation. We showcase its existence experimentally by contrasting two intermediate languages, Answer Set Programming and the Python Knowledge Engine. In addition, we demonstrate the effects of varying degrees of supplementary context information. Our results show a maximum difference in overall-accuracy of 53.20% and 49.26% in execution-accuracy. When using the GPT4o-mini LLM we beat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20% and by 50.50% on the ProofWriter dataset.', 'abstract_zh': '逻辑推理任务对大语言模型（LLMs）来说是一个挑战。神经符号方法使用LLMs将用自然语言表述的逻辑推理问题翻译成一个形式化的中间语言，然后通过符号推理器进行可靠求解。然而，LLMs在翻译过程中常常由于选择不当的中间语言而失效。\n\n我们提出了中间语言问题，即在神经符号方法中选择合适的形式化语言表示的问题。理论上，我们认为其根源在于LLMs无法区分语法和语义的能力不足，且该问题与其表示形式的独立性密切相关。我们通过对比两种中间语言（答案集编程和Python知识引擎）的实验展示了该问题的存在性。此外，我们还展示了不同补充上下文信息程度的影响。结果显示，在总体准确率上，最大的差异可达53.20%和49.26%；在执行准确率上，使用GPT4o-mini LLM时，我们分别在ProntoQA数据集和ProofWriter数据集上比最先进的方法高出21.20%和50.50%。', 'title_zh': '使大型语言模型具备推理能力：神经符号方法中的中间语言问题'}
{'arxiv_id': 'arXiv:2502.17161', 'title': 'Real-time Monitoring of Economic Shocks using Company Websites', 'authors': 'Michael Koenig, Jakob Rauch, Martin Woerter', 'link': 'https://arxiv.org/abs/2502.17161', 'abstract': "Understanding the effects of economic shocks on firms is critical for analyzing economic growth and resilience. We introduce a Web-Based Affectedness Indicator (WAI), a general-purpose tool for real-time monitoring of economic disruptions across diverse contexts. By leveraging Large Language Model (LLM) assisted classification and information extraction on texts from over five million company websites, WAI quantifies the degree and nature of firms' responses to external shocks. Using the COVID-19 pandemic as a specific application, we show that WAI is highly correlated with pandemic containment measures and reliably predicts firm performance. Unlike traditional data sources, WAI provides timely firm-level information across industries and geographies worldwide that would otherwise be unavailable due to institutional and data availability constraints. This methodology offers significant potential for monitoring and mitigating the impact of technological, political, financial, health or environmental crises, and represents a transformative tool for adaptive policy-making and economic resilience.", 'abstract_zh': '理解经济冲击对企业的影响是分析经济增长和韧性的重要基础。我们引入了基于网络的受影响度指标（WAI），这是一种适用于多元情境下的实时监控经济中断的通用工具。通过利用大型语言模型（LLM）辅助的分类和信息提取，WAI 对多种公司网站上的文本进行分析，从而量化企业在外部冲击下的响应程度和性质。以 COVID-19 疫情为例，我们表明 WAI 与防疫措施高度相关，并能可靠地预测企业表现。与传统的数据来源相比，WAI 能够在全球范围内提供及时的行业和地区层面的企业信息，而这些信息通常由于机构限制和数据可用性问题无法获得。该方法学为监测和缓解技术、政治、金融、健康或环境危机的影响提供了重大潜力，并代表了适应性政策制定和经济韧性的一个变革工具。', 'title_zh': '使用公司网站进行实时监测经济冲击的研究'}
{'arxiv_id': 'arXiv:2502.17011', 'title': 'Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep Reinforcement Learning with LLM Evaluation', 'authors': 'Jaskaran Singh Walia, Aarush Sinha, Srinitish Srinivasan, Srihari Unnikrishnan', 'link': 'https://arxiv.org/abs/2502.17011', 'abstract': 'Financial bond yield forecasting is challenging due to data scarcity, nonlinear macroeconomic dependencies, and evolving market conditions. In this paper, we propose a novel framework that leverages Causal Generative Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement learning (RL) to generate high-fidelity synthetic bond yield data for four major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key macroeconomic variables, we ensure statistical fidelity by preserving essential market properties. To transform this market dependent synthetic data into actionable insights, we employ a finetuned Large Language Model (LLM) Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments, and volatility projections. We use automated, human and LLM evaluations, all of which demonstrate that our framework improves forecasting performance over existing methods, with statistical validation via predictive accuracy, MAE evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation (3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement learning-enhanced synthetic data generation achieves the least Mean Absolute Error of 0.103, demonstrating its effectiveness in replicating real-world bond market dynamics. We not only enhance data-driven trading strategies but also provides a scalable, high-fidelity synthetic financial data pipeline for risk & volatility management and investment decision-making. This work establishes a bridge between synthetic data generation, LLM driven financial forecasting, and language model evaluation, contributing to AI-driven financial decision-making.', 'abstract_zh': '由于数据稀缺、非线性的宏观经济依赖关系以及不断变化的市场条件，金融债券收益率的预测极具挑战性。本文提出了一种新颖的框架，利用因果生成对抗网络（CausalGANs）和软演员-评论家（SAC）强化学习（RL）来生成高保真度的合成债券收益率数据，涵盖四个主要债券类别（AAA、BAA、US10Y、垃圾债）。通过引入12个关键宏观经济变量，我们确保了统计保真度，保持了市场的重要属性。为了将这些依存于市场的合成数据转化为可操作的洞察，我们使用了一种微调的大语言模型（LLM）Qwen2.5-7B，该模型能够生成交易信号（买入/持有/卖出）、风险评估和波动率预测。我们采用了自动化、人工和LLM评估，所有评估结果均表明，我们的框架在预测性能上优于现有方法，通过预测准确性、平均绝对误差（MAE，0.103%）、盈利/亏损评估（60%的盈利比率）、LLM评估（3.37/5）以及专家评估的评分（4.67/5）得到了统计验证。增强学习产生的合成数据生成得到了最小平均绝对误差（0.103）的支持，展示了其在复制现实世界债券市场动态方面的有效性。我们不仅增强了数据驱动的交易策略，还提供了一个适用于风险与波动管理以及投资决策的可扩展、高保真的合成金融数据管道。本文建立了合成数据生成、LLM驱动的金融预测以及语言模型评估之间的桥梁，为基于AI的金融决策做出了贡献。', 'title_zh': '使用因果生成对抗网络和深度强化学习预测考虑流动性因素的债券收益率，并通过语言模型进行评估'}
{'arxiv_id': 'arXiv:2502.16852', 'title': 'Improving LLM General Preference Alignment via Optimistic Online Mirror Descent', 'authors': 'Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, Dong Yu', 'link': 'https://arxiv.org/abs/2502.16852', 'abstract': 'Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $O(T^{-1})$ bound on the duality gap, improving upon the previous $O(T^{-1/2})$ result. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks.', 'abstract_zh': '基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）已被证明在使大型语言模型（Large Language Models, LLMs）与人类偏好一致方面具有显著的效果。许多现有的对齐方法依赖于Bradley-Terry（BT）模型假设，该假设假设每个提示-响应对存在一个真实的奖励。然而，在建模复杂的人类偏好时，这种假设可能会过于苛刻。在本文中，我们放弃了BT模型的假设，并研究了在一般偏好下的LLM对齐问题，将其形式化为一个两玩家博弈。借鉴博弈学习中的理论见解，我们将乐观在线镜像下降方法集成到我们的对齐框架中，以近似纳什策略。理论上，我们证明了我们的方法在对偶间隙上达到了$O(T^{-1})$的界，改进了之前$O(T^{-1/2})$的结果。更重要的是，我们实现了该方法，并通过实验表明，在多个代表性基准上，它优于最先进的RLHF算法。', 'title_zh': '通过乐观在线镜像下降法提高大语言模型的一般偏好对齐'}
{'arxiv_id': 'arXiv:2502.16810', 'title': 'Grounded Persuasive Language Generation for Automated Marketing', 'authors': 'Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu', 'link': 'https://arxiv.org/abs/2502.16810', 'abstract': 'This paper develops an agentic framework that employs large language models (LLMs) to automate the generation of persuasive and grounded marketing content, using real estate listing descriptions as our focal application domain. Our method is designed to align the generated content with user preferences while highlighting useful factual attributes. This agent consists of three key modules: (1) Grounding Module, mimicking expert human behavior to predict marketable features; (2) Personalization Module, aligning content with user preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion of localized features. We conduct systematic human-subject experiments in the domain of real estate marketing, with a focus group of potential house buyers. The results demonstrate that marketing descriptions generated by our approach are preferred over those written by human experts by a clear margin. Our findings suggest a promising LLM-based agentic framework to automate large-scale targeted marketing while ensuring responsible generation using only facts.', 'abstract_zh': '本文 develops 一个代理框架，利用大型语言模型（LLMs）自动化生成具有说服力并基于事实的营销内容，以房地产列表描述作为我们的研究焦点领域。该方法旨在使生成的内容与用户偏好保持一致，同时突出关键的有用事实属性。该代理由三个关键模块组成：（1） grounding 模块，模仿专家人类行为以预测可销售的特征；（2）个性化模块，使内容与用户偏好保持一致；（3）营销模块，确保事实准确性和本地化特征的包含。我们对房地产营销领域进行了系统的人工主体实验，并重点关注潜在房屋买家。结果显示，通过我们方法生成的营销描述明显优于由人类专家撰写的描述。本文的研究结果表明，基于LLM的代理框架有可能自动化大规模有目标的营销活动，并且仅通过事实生成负责任的内容是可行的。', 'title_zh': '基于产品支持的说服性语言生成在自动化营销中的应用'}
{'arxiv_id': 'arXiv:2502.16706', 'title': 'DISC: Dynamic Decomposition Improves LLM Inference Scaling', 'authors': 'Jonathan Light, Wei Cheng, Wu Yue, Masafumi Oyamada, Mengdi Wang, Santiago Paternain, Haifeng Chen', 'link': 'https://arxiv.org/abs/2502.16706', 'abstract': 'Many inference scaling methods work by breaking a problem into smaller steps (or groups of tokens), then sampling and choosing the best next step. However, these steps and their sizes are usually predetermined based on human intuition or domain knowledge. This paper introduces dynamic decomposition, a method that automatically and adaptively splits solution and reasoning traces into steps during inference. This approach improves computational efficiency by focusing more resources on difficult steps, breaking them down further and prioritizing their sampling. Experiments on coding and math benchmarks (APPS, MATH, and LiveCodeBench) show that dynamic decomposition performs better than static methods, which rely on fixed steps like token-level, sentence-level, or single-step decompositions. These results suggest that dynamic decomposition can enhance many inference scaling techniques.', 'abstract_zh': '许多推理缩放方法通过将问题分解为更小的步骤（或一组标记），然后采样并选择最好的下一步来工作。然而，这些步骤及其大小通常是基于人类直觉或领域知识预先确定的。本文介绍了一种动态分解方法，该方法能够在推理过程中自动且适应性地将解决方案和推理过程分解为步骤。这种方法通过更多地利用资源解决困难的步骤、将这些步骤进一步拆分并优先考虑它们的采样，从而提高计算效率。在编码和数学基准测试（APPS、MATH和LiveCodeBench）上的实验表明，动态分解方法优于依赖于固定步骤（如标记级、句子级或单步分解）的静态方法。这些结果表明，动态分解可以增强许多推理缩放技术。', 'title_zh': 'DISC：动态分解提高大语言模型推理缩放性能\n\n在这个翻译中，我尽量保持了原文的学术表达方式，并且将“Dynamic Decomposition”翻译为“动态分解”，“LLM Inference Scaling”翻译为“大语言模型推理缩放性能”，以符合学术领域的表达习惯。'}
{'arxiv_id': 'arXiv:2502.16399', 'title': 'Ensemble ToT of LLMs and Its Application to Automatic Grading System for Supporting Self-Learning', 'authors': 'Yuki Ito, Qiang Ma', 'link': 'https://arxiv.org/abs/2502.16399', 'abstract': "Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs.", 'abstract_zh': '为学生提供详细和及时的反馈对于自我学习至关重要。虽然现有的基于大语言模型（LLM）的批改系统具有很大潜力，但大多数系统依赖单一模型，限制了其性能。为此，我们提出了一种名为Ensemble Tree-of-Thought（ToT）的框架，该框架通过整合多个模型来增强LLM的输出。在此框架基础上，我们开发了一个批改系统。Ensemble ToT遵循三个步骤：（1）分析LLM的表现，（2）生成候选答案，（3）将它们精练成最终结果。基于此，我们的批改系统首先评估LLMs的批改倾向，然后生成多个结果，最后通过模拟辩论的方式整合它们。实验结果证明，该方法能够通过有效协调多个LLM来提供准确且可解释的批改。', 'title_zh': '以下是符合学术规范的翻译：\n\n“大型语言模型的集成ToT及其在支持自主学习的自动评分系统中的应用”'}
{'arxiv_id': 'arXiv:2502.16395', 'title': 'An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science', 'authors': 'Qiuhai Zeng, Claire Jin, Xinyue Wang, Yuhan Zheng, Qunhua Li', 'link': 'https://arxiv.org/abs/2502.16395', 'abstract': "Large Language Models (LLMs) have demonstrated potential for data science tasks via code generation. However, the exploratory nature of data science, alongside the stochastic and opaque outputs of LLMs, raise concerns about their reliability. While prior work focuses on benchmarking LLM accuracy, reproducibility remains underexplored, despite being critical to establishing trust in LLM-driven analysis.\nWe propose a novel analyst-inspector framework to automatically evaluate and enforce the reproducibility of LLM-generated data science workflows - the first rigorous approach to the best of our knowledge. Defining reproducibility as the sufficiency and completeness of workflows for reproducing functionally equivalent code, this framework enforces computational reproducibility principles, ensuring transparent, well-documented LLM workflows while minimizing reliance on implicit model assumptions.\nUsing this framework, we systematically evaluate five state-of-the-art LLMs on 1,032 data analysis tasks across three diverse benchmark datasets. We also introduce two novel reproducibility-enhancing prompting strategies. Our results show that higher reproducibility strongly correlates with improved accuracy and reproducibility-enhancing prompts are effective, demonstrating structured prompting's potential to enhance automated data science workflows and enable transparent, robust AI-driven analysis. Our code is publicly available.", 'abstract_zh': '大规模语言模型（LLMs）已经通过代码生成展示了在数据科学任务中的潜力。然而，数据科学的探索性质，以及LLMs的随机性和不透明输出，引起了对其可靠性的担忧。尽管先前工作的重点在于评估LLM的准确性，但重现性（可复现性）在建立对LLM驱动分析的信任方面仍然是未被充分探索的重要方面。\n\n我们提出了一种新的分析师-检查员框架，以自动化评估和强制执行LLM生成的数据科学工作流的重现性——到我们所知，这是首个严格意义上的方法。我们将重现性定义为能够再现功能等效代码的工作流的充分性和完整性，该框架确保计算上的可复现性原则得到遵守，保证LLM工作流的透明和文档化，同时最大限度地减少对隐含模型假设的依赖。\n\n使用该框架，我们系统地评估了五个最先进的LLM在三个不同基准数据集上的1032个数据分析任务。我们还引入了两种新的增强重现性的提示策略。我们的研究表明，更高的重现性与更好的准确性高度相关，并且增强重现性的提示是有效的，证明了结构化提示在增强自动化数据科学工作流和实现透明、稳健的人工智能驱动分析方面具有潜力。我们的代码已经公开。', 'title_zh': '数据科学中大型语言模型可重复性评估的分析师-检查员框架'}
{'arxiv_id': 'arXiv:2502.16290', 'title': 'Interrogating LLM design under a fair learning doctrine', 'authors': 'Johnny Tian-Zheng Wei, Maggie Wang, Ameya Godbole, Jonathan H. Choi, Robin Jia', 'link': 'https://arxiv.org/abs/2502.16290', 'abstract': 'The current discourse on large language models (LLMs) and copyright largely takes a "behavioral" perspective, focusing on model outputs and evaluating whether they are substantially similar to training data. However, substantial similarity is difficult to define algorithmically and a narrow focus on model outputs is insufficient to address all copyright risks. In this interdisciplinary work, we take a complementary "structural" perspective and shift our focus to how LLMs are trained. We operationalize a notion of "fair learning" by measuring whether any training decision substantially affected the model\'s memorization. As a case study, we deconstruct Pythia, an open-source LLM, and demonstrate the use of causal and correlational analyses to make factual determinations about Pythia\'s training decisions. By proposing a legal standard for fair learning and connecting memorization analyses to this standard, we identify how judges may advance the goals of copyright law through adjudication. Finally, we discuss how a fair learning standard might evolve to enhance its clarity by becoming more rule-like and incorporating external technical guidelines.', 'abstract_zh': '当前关于大规模语言模型（LLMs）和版权的讨论主要采取“行为”视角，重点关注模型输出，并评估其是否与训练数据实质性相似。然而，实质性相似难以通过算法进行定义，而仅关注模型输出不足以全面应对版权风险。在本项跨学科研究中，我们采取了补充性的“结构”视角，将重点转向LLMs的训练方式。通过衡量任何训练决策是否实质性影响了模型的记忆，我们操作化了“公平学习”的概念。以开源LLM Pythia为例，我们对该模型的训练决策进行了因果分析和相关性分析，以事实方式确定其训练决策。通过提出一个公平学习的法律标准，并将记忆分析与该标准相结合，我们确定了法官通过审理如何推进版权法的立法目标。最后，我们讨论了如何通过使其更具规则性并纳入外部技术指导原则来提高公平学习标准的清晰度，从而进一步增强其有效性。', 'title_zh': '在公正学习原则下质疑LLM设计'}
{'arxiv_id': 'arXiv:2502.16174', 'title': 'Maybe I Should Not Answer That, but... Do LLMs Understand The Safety of Their Inputs?', 'authors': 'Maciej Chrabąszcz, Filip Szatkowski, Bartosz Wójcik, Jan Dubiński, Tomasz Trzciński', 'link': 'https://arxiv.org/abs/2502.16174', 'abstract': 'Ensuring the safety of the Large Language Model (LLM) is critical, but currently used methods in most cases sacrifice the model performance to obtain increased safety or perform poorly on data outside of their adaptation distribution. We investigate existing methods for such generalization and find them insufficient. Surprisingly, while even plain LLMs recognize unsafe prompts, they may still generate unsafe responses. To avoid performance degradation and preserve safe performance, we advocate for a two-step framework, where we first identify unsafe prompts via a lightweight classifier, and apply a "safe" model only to such prompts. In particular, we explore the design of the safety detector in more detail, investigating the use of different classifier architectures and prompting techniques. Interestingly, we find that the final hidden state for the last token is enough to provide robust performance, minimizing false positives on benign data while performing well on malicious prompt detection. Additionally, we show that classifiers trained on the representations from different model layers perform comparably on the latest model layers, indicating that safety representation is present in the LLMs\' hidden states at most model stages. Our work is a step towards efficient, representation-based safety mechanisms for LLMs.', 'abstract_zh': '确保大型语言模型（LLM）的安全性至关重要，但在大多数情况下，当前使用的安全方法通常会牺牲模型性能以获得更高的安全性，或者在模型未适应的数据上表现不佳。我们研究了这些通用方法的有效性，并发现它们存在不足。令人惊讶的是，即使是简单的LLM也能识别出不安全的提示，但在某些情况下仍会产生不安全的响应。为了避免性能下降并保持安全性能，我们提倡采用两步框架，首先通过轻量级分类器识别出不安全的提示，然后仅对这些提示应用“安全”模型。特别地，我们详细探讨了安全检测器的设计，研究了不同的分类器架构和提示技术。有趣的是，我们发现最后一个词的最终隐藏状态足以提供稳健的表现，在良性数据上减少误报率，同时在恶意提示检测上表现良好。此外，我们展示了在不同模型层上训练的分类器在最新模型层上的表现相当，这表明安全表示在LLM的不同隐藏状态层上存在。我们的工作对于LLM的安全性机制的发展是一个重要的步骤。', 'title_zh': '也许我本不应回答这个问题，但……语言模型是否理解其输入的安全性？'}
{'arxiv_id': 'arXiv:2502.15964', 'title': 'Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models', 'authors': 'Avanika Narayan, Dan Biderman, Sabri Eyuboglu, Avner May, Scott Linderman, James Zou, Christopher Re', 'link': 'https://arxiv.org/abs/2502.15964', 'abstract': "We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents. Can a local-remote collaboration reduce cloud inference costs while preserving quality? First, we consider a naive collaboration protocol where the local and remote models simply chat back and forth. Because only the local model reads the full context, this protocol achieves a 30.4x reduction in remote costs, but recovers only 87% of the performance of the frontier model. We identify two key limitations of this protocol: the local model struggles to (1) follow the remote model's multi-step instructions and (2) reason over long contexts. Motivated by these observations, we study an extension of this protocol, coined MinionS, in which the remote model decomposes the task into easier subtasks over shorter chunks of the document, that are executed locally in parallel. MinionS reduces costs by 5.7x on average while recovering 97.9% of the performance of the remote model alone. Our analysis reveals several key design choices that influence the trade-off between cost and performance in local-remote systems.", 'abstract_zh': '我们研究了一种新兴的设置，在这种设置中，一个具有访问本地数据的小型设备端语言模型（LM）与一个领先的云托管语言模型进行通信，以解决涉及金融、医疗和科学推理的长文档实际任务。设备端-云端合作能否在保持质量的前提下降低云推理成本？首先，我们考虑了一个简单的合作协议，即本地模型和远程模型彼此来回聊天。由于只有本地模型能够读取完整的上下文，这种协议将远程成本降低了30.4倍，但仅恢复了远程模型性能的87%。我们识别了这种协议的两个关键局限性：本地模型难以（1）跟随远程模型的多步指令，并且（2）处理长上下文。基于这些观察，我们研究了一种名为MinionS的新扩展协议，在这种协议中，远程模型将任务分解为更简单的子任务，并在较短文档片段上执行这些子任务，这些子任务在本地并行执行。MinionS平均将成本降低了5.7倍，同时恢复了远程模型性能的97.9%。我们的分析揭示了几种关键设计选择，这些选择影响设备端-云端系统的成本与性能之间的权衡。', 'title_zh': '《 minions：设备端与云端语言模型的低成本协作》\n\n在此翻译中，“Minions”被处理为专有名词或系统名称，并未直接翻译，保持了原名；“Cost-efficient Collaboration”译为“低成本协作”，符合学术规范。标题中的内容说明了设备端和云端语言模型之间的协作方式及其经济性特点。'}
{'arxiv_id': 'arXiv:2502.15823', 'title': 'InductionBench: LLMs Fail in the Simplest Complexity Class', 'authors': 'Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang', 'link': 'https://arxiv.org/abs/2502.15823', 'abstract': "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available this https URL.", 'abstract_zh': '大规模语言模型（LLMs）在推理方面取得了显著的进步，许多现有的基准已经被如o1和o3等模型完全或部分地解决。然而，这些基准大多数都集中在演绎推理上，包括数学和编程任务，其中的规则（如数学公理或编程语法）是明确定义的，基于这些规则，LLMs可以进行计划并应用这些规则以得出解决方案。相比之下，归纳推理，即从观测数据中推断出潜在的规则，这一领域仍然是一个较少探索的领域。这类归纳过程是科学研究的核心，因为它们使研究人员能够从经验观察中提炼出普遍的原则。为了评估LLMs是否具备这种能力，我们引入了InductionBench这一新的基准，旨在评估LLMs的归纳推理能力。我们的实验结果表明，即使是最先进的模型也难以掌握函数子正则层级中最简单的复杂性类，突显了当前LLMs在归纳推理能力方面的显著缺陷。具体的附件和数据可通过以下链接访问：[请输入实际的链接]。', 'title_zh': 'InductionBench: 大型语言模型在最简单的复杂性类中失败'}
{'arxiv_id': 'arXiv:2502.15806', 'title': 'A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos', 'authors': 'Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, Yingchun Wang', 'link': 'https://arxiv.org/abs/2502.15806', 'abstract': 'Large Reasoning Models (LRMs) have significantly advanced beyond traditional Large Language Models (LLMs) with their exceptional logical reasoning capabilities, yet these improvements introduce heightened safety risks. When subjected to jailbreak attacks, their ability to generate more targeted and organized content can lead to greater harm. Although some studies claim that reasoning enables safer LRMs against existing LLM attacks, they overlook the inherent flaws within the reasoning process itself. To address this gap, we propose the first jailbreak attack targeting LRMs, exploiting their unique vulnerabilities stemming from the advanced reasoning capabilities. Specifically, we introduce a Chaos Machine, a novel component to transform attack prompts with diverse one-to-one mappings. The chaos mappings iteratively generated by the machine are embedded into the reasoning chain, which strengthens the variability and complexity and also promotes a more robust attack. Based on this, we construct the Mousetrap framework, which makes attacks projected into nonlinear-like low sample spaces with mismatched generalization enhanced. Also, due to the more competing objectives, LRMs gradually maintain the inertia of unpredictable iterative reasoning and fall into our trap. Success rates of the Mousetrap attacking o1-mini, claude-sonnet and gemini-thinking are as high as 96%, 86% and 98% respectively on our toxic dataset Trotter. On benchmarks such as AdvBench, StrongREJECT, and HarmBench, attacking claude-sonnet, well-known for its safety, Mousetrap can astonishingly achieve success rates of 87.5%, 86.58% and 93.13% respectively. Attention: This paper contains inappropriate, offensive and harmful content.', 'abstract_zh': '以下是经过学术规范翻译后的版本：\n\n大型推理模型（LRMs）在逻辑推理能力方面显著超越了传统的大型语言模型（LLMs），然而这些改进也带来了更高的安全风险。当遭受破戒攻击时，它们生成更精准、更有组织内容的能力可能导致更大的危害。尽管一些研究声称推理能够使LRMs在面对现有LLM攻击时更加安全，但它们忽视了推理过程本身固有的缺陷。为填补这一空白，我们提出了首个针对LRMs的破戒攻击，利用它们从高级推理能力中衍生的独特漏洞。具体而言，我们引入了混沌机器这一新颖组件，以实现多样化的一对一映射，将其映射结果周期性地嵌入推理链中，从而增强了攻击的变异性、复杂性，亦提升了攻击的鲁棒性。基于此，我们构建了Mousetrap框架，使攻击能够在非线性低样本空间中投影，同时增强了泛化能力。由于目标更具有竞争性，LRMs逐渐陷入了无法预测的迭代推理陷阱中。在我们的毒性数据集Trotter中，Mousetrap对o1-mini、Claude-Sonnet和Gemini-Thinking的成功率分别高达96%、86%和98%。在AdvBench、StrongREJECT和HarmBench等基准测试中，即使Claude-Sonnet以安全著称，Mousetrap也能分别实现87.5%、86.58%和93.13%的成功率。注意：本文包含不适当、冒犯性和有害的内容。', 'title_zh': '一个捕鼠器：通过迭代混沌链欺骗大型推理模型以实现 Jailbreak'}
{'arxiv_id': 'arXiv:2502.15796', 'title': 'Pruning as a Defense: Reducing Memorization in Large Language Models', 'authors': 'Mansi Gupta, Nikhar Waghela, Sarthak Gupta, Shourya Goel, Sanjif Shanmugavelu', 'link': 'https://arxiv.org/abs/2502.15796', 'abstract': 'Large language models have been shown to memorize significant portions of their training data, which they can reproduce when appropriately prompted. This work investigates the impact of simple pruning techniques on this behavior. Our findings reveal that pruning effectively reduces the extent of memorization in LLMs, demonstrating its potential as a foundational approach for mitigating membership inference attacks.', 'abstract_zh': '大型语言模型已被证明会记住大量训练数据，并在适当提示下重现这些数据。本研究探讨了简单剪枝技术对此现象的影响。我们的研究发现，剪枝有效地减少了大型语言模型中的记忆程度，展示了其作为减轻成员推理攻击基础方法的潜力。', 'title_zh': '修剪作为防御手段：减少大型语言模型的记忆化'}
{'arxiv_id': 'arXiv:2502.15771', 'title': 'Learning to Reason from Feedback at Test-Time', 'authors': 'Yanyang Li, Michael Lyu, Liwei Wang', 'link': 'https://arxiv.org/abs/2502.15771', 'abstract': 'Solving complex tasks in a single attempt is challenging for large language models (LLMs). Iterative interaction with the environment and feedback is often required to achieve success, making effective feedback utilization a critical topic. Existing approaches either struggle with length generalization or rely on naive retries without leveraging prior information. In this paper, we introduce FTTT, a novel paradigm that formulates feedback utilization as an optimization problem at test time. Additionally, we propose a learnable test-time optimizer, OpTune, to effectively exploit feedback. Experiments on two LLMs across four reasoning datasets demonstrate that FTTT and OpTune achieve superior scalability and performance.', 'abstract_zh': '大规模语言模型（LLMs）在一次性完成复杂任务方面面临挑战。通常需要迭代地与环境交互并利用反馈才能取得成功，因此有效利用反馈成为一个关键问题。现有方法要么在长度泛化方面遇到困难，要么依赖于无意识的重试而未能充分利用先验信息。在本文中，我们提出了FTTT，这是一种新颖的框架，将反馈利用问题在测试时形式化为一个优化问题。此外，我们提出了一种可学习的测试时优化器OpTune，以有效地利用反馈。实验结果显示，在四个原理解析数据集中，FTTT和OpTune在可扩展性和性能上都表现出色。', 'title_zh': '在测试时根据反馈进行推理的学习'}
{'arxiv_id': 'arXiv:2502.15701', 'title': 'Political Events using RAG with LLMs', 'authors': 'Muhammad Arslan, Saba Munawar, Christophe Cruz', 'link': 'https://arxiv.org/abs/2502.15701', 'abstract': "In the contemporary digital landscape, media content stands as the foundation for political news analysis, offering invaluable insights sourced from various channels like news articles, social media updates, speeches, and reports. Natural Language Processing (NLP) has revolutionized Political Information Extraction (IE), automating tasks such as Event Extraction (EE) from these diverse media outlets. While traditional NLP methods often necessitate specialized expertise to build rule-based systems or train machine learning models with domain-specific datasets, the emergence of Large Language Models (LLMs) driven by Generative Artificial Intelligence (GenAI) presents a promising alternative. These models offer accessibility, alleviating challenges associated with model construction from scratch and reducing the dependency on extensive datasets during the training phase, thus facilitating rapid implementation. However, challenges persist in handling domain-specific tasks, leading to the development of the Retrieval-Augmented Generation (RAG) framework. RAG enhances LLMs by integrating external data retrieval, enriching their contextual understanding, and expanding their knowledge base beyond pre-existing training data. To illustrate RAG's efficacy, we introduce the Political EE system, specifically tailored to extract political event information from news articles. Understanding these political insights is essential for remaining informed about the latest political advancements, whether on a national or global scale.", 'abstract_zh': '在当今的数字景观中，媒体内容构成了政治新闻分析的基础，提供了来自各种渠道（如新闻文章、社交媒体更新、演讲和报告）的宝贵见解。自然语言处理（NLP）已彻底改变了政治信息抽取（IE），实现了从这些多元媒体渠道自动提取事件（EE）等任务的自动化。虽然传统的NLP方法通常需要专门的专家构建基于规则的系统或使用领域特定数据集训练机器学习模型，但生成人工智能（GenAI）驱动的大规模语言模型（LLMs）的出现为这一领域的自动化提供了有前景的替代方案。这些模型提高了可访问性，减轻了从头构建模型的挑战，并在训练阶段减少了对大量数据集的依赖，从而促进了快速实现。然而，处理领域特定任务仍然存在挑战，这促进了检索增强生成（RAG）框架的发展。RAG通过整合外部数据检索，增强了大规模语言模型的上下文理解能力，并扩展了它们的知识库，使其超越了预先存在的训练数据。为了展示RAG的有效性，我们介绍了一种专门用于从新闻文章中抽取政治事件信息的政治事件提取系统。理解这些政治见解对于了解最新的政治进展至关重要，无论是国家级的还是国际性的。', 'title_zh': '使用LLM的RAG进行政治事件处理'}
{'arxiv_id': 'arXiv:2502.15686', 'title': 'V-SQL: A View-based Two-stage Text-to-SQL Framework', 'authors': 'Zeshun You, Jiebin Yao, Dong Cheng, Zhiwei Wen, Zhiliang Lu, Xianyi Shen', 'link': 'https://arxiv.org/abs/2502.15686', 'abstract': "The text-to-SQL task aims to convert natural language into Structured Query Language (SQL) without bias. Recently, text-to-SQL methods based on large language models (LLMs) have garnered significant attention. The core of mainstream text-to-SQL frameworks is schema linking, which aligns user queries with relevant tables and columns in the database. Previous methods focused on schema linking while neglecting to enhance LLMs' understanding of database schema. The complex coupling relationships between tables in the database constrain the SQL generation capabilities of LLMs. To tackle this issue, this paper proposes a simple yet effective strategy called view-based schema. This strategy aids LLMs in understanding the database schema by decoupling tightly coupled tables into low-coupling views. We then introduce V-SQL, a view-based two-stage text-to-SQL framework. V-SQL involves the view-based schema strategy to enhance LLMs' understanding of database schema. Results on the authoritative datasets Bird indicate that V-SQL achieves competitive performance compared to existing state-of-the-art methods.", 'abstract_zh': '文本到SQL的任务旨在无偏见地将自然语言转换为结构化查询语言（SQL）。近年来，基于大规模语言模型（LLMs）的文本到SQL方法受到了广泛关注。主流的文本到SQL框架的核心是模式链接，该过程将用户查询与数据库中的相关表和列对齐。之前的方法主要关注模式链接，而忽视了增强LLM对数据库模式的理解。数据库中表之间的复杂耦合关系限制了LLMs生成SQL的能力。为解决这一问题，本论文提出了一种简单而有效的策略，称为基于视图的模式。该策略通过将紧密耦合的表分解为低耦合视图，帮助LLM更好地理解数据库模式。我们随后介绍了V-SQL，这是一种基于视图的两阶段文本到SQL框架。V-SQL利用基于视图的模式策略来增强LLM对数据库模式的理解。在权威数据集Bird上的结果显示，V-SQL的性能与现有最先进的方法相当。', 'title_zh': 'V-SQL：一种基于视图的两阶段文本到SQL框架'}
{'arxiv_id': 'arXiv:2502.17419', 'title': 'From System 1 to System 2: A Survey of Reasoning Large Language Models', 'authors': 'Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, Cheng-Lin Liu', 'link': 'https://arxiv.org/abs/2502.17419', 'abstract': "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{this https URL}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.", 'abstract_zh': '达到人类级智能需要优化从快速直觉的系统1向较慢而仔细的系统2推理的过渡。系统1在快速启发式决策方面表现出色，而系统2则依赖于逻辑推理，从而能够做出更准确的判断并减少偏差。基础型大语言模型（LLMs）在快速决策方面表现出色，但在进行复杂推理时缺乏深度，因为它们尚未完全拥抱真正系统2思考过程中的逐步分析。最近，像OpenAI的o1/o3和DeepSeek的R1等一系列推理LLM在数学和编程等领域展示了专家级的表现，它们紧密模仿了系统2的仔细推理，并展示了类似人类的认知能力。本文综述首先简要回顾基础型LLM的进展和系统2技术的早期发展，探讨其结合如何为推理LLM的发展铺平道路。随后，我们将讨论如何构建推理LLM，分析其特点、核心方法及其高级推理能力的演变。此外，我们还提供了推理基准的综述，深入比较代表性推理LLM的表现。最后，我们探索了推进推理LLM发展的有前途的方向，并持续维护实时的GitHub仓库以跟踪最新进展。我们希望这篇综述能够成为有价值的资源，激发创新并推动这一快速演变领域的进步。', 'title_zh': '从系统1到系统2：大规模语言模型推理综述\n\n注释：这里的“System 1”和“System 2”是心理学中的概念，分别指代直觉思维和反思思维。在翻译时，为了符合学术规范，通常会根据具体内容和上下文进行适当的翻译和解释。上述翻译是基于常见的学术理解和文字直译，如果需要更具体的学术解释，可以进一步细化。'}
{'arxiv_id': 'arXiv:2502.17139', 'title': 'CodeSwift: Accelerating LLM Inference for Efficient Code Generation', 'authors': 'Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Borui Zhang, Runlin Guo, Jia Li', 'link': 'https://arxiv.org/abs/2502.17139', 'abstract': 'Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios. To alleviate this issue, we propose CodeSwift, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. CodeSwift constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that CodeSwift can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%.', 'abstract_zh': '代码生成是一项对延迟敏感的任务，要求高度的及时性，但大型语言模型（LLMs）的自回归解码机制导致了较差的推理效率。现有的LLM推理加速方法主要集中在仅使用内置组件的独立函数上。此外，这些方法将代码视为自然语言序列，忽视了其独特的语法和语义特征。因此，这些方法在代码生成任务中的有效性仍然有限，且未能与实际编程场景相匹配。为解决这一问题，我们提出了一种名为CodeSwift的简单高效推理加速方法，该方法专门设计用于代码生成，同时保证输出质量。CodeSwift 构建了一个多源数据存储系统，提供了普遍知识和项目特定知识的访问途径，促进了高质量草稿序列的检索。此外，CodeSwift 通过控制检索时间降低检索成本，并通过并行检索和上下文及LLM偏好感知缓存来提高效率。实验结果表明，与自回归解码相比，CodeSwift 在仓库级和独立代码生成任务中分别实现了高达2.53倍和2.54倍的加速，超越了最先进的推理加速方法最多88%。', 'title_zh': 'CodeSwift：加速大型语言模型推理以实现高效代码生成'}
{'arxiv_id': 'arXiv:2502.17136', 'title': 'Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization', 'authors': 'Lionel Richy Panlap Houamegni, Fatih Gedikli', 'link': 'https://arxiv.org/abs/2502.17136', 'abstract': "The automation of news analysis and summarization presents a promising solution to the challenge of processing and analyzing vast amounts of information prevalent in today's information society. Large Language Models (LLMs) have demonstrated the capability to transform vast amounts of textual data into concise and easily comprehensible summaries, offering an effective solution to the problem of information overload and providing users with a quick overview of relevant information. A particularly significant application of this technology lies in supply chain risk analysis. Companies must monitor the news about their suppliers and respond to incidents for several critical reasons, including compliance with laws and regulations, risk management, and maintaining supply chain resilience. This paper develops an automated news summarization system for supply chain risk analysis using LLMs. The proposed solution aggregates news from various sources, summarizes them using LLMs, and presents the condensed information to users in a clear and concise format. This approach enables companies to optimize their information processing and make informed decisions. Our study addresses two main research questions: (1) Are LLMs effective in automating news summarization, particularly in the context of supply chain risk analysis? (2) How effective are various LLMs in terms of readability, duplicate detection, and risk identification in their summarization quality? In this paper, we conducted an offline study using a range of publicly available LLMs at the time and complemented it with a user study focused on the top performing systems of the offline experiments to evaluate their effectiveness further. Our results demonstrate that LLMs, particularly Few-Shot GPT-4o mini, offer significant improvements in summary quality and risk identification.", 'abstract_zh': '新闻分析与总结的自动化为处理今天信息社会中大量信息带来了前景广阔的新解决方案。大型语言模型（LLMs）已经展示了将大量文本数据转化为简洁且易于理解的摘要的能力，从而为信息过载的问题提供有效解决方案，并为用户提供相关信息的快速概览。这项技术的一个特别重要应用在于供应链风险分析。公司必须监控其供应商的相关新闻并及时响应事件，原因包括遵守法律法规、风险管理以及维持供应链韧性。本文提出了一种使用LLMs的自动化新闻摘要系统，用于供应链风险分析。该解决方案汇总来自各种来源的新闻，使用LLMs进行总结，并以清晰简洁的格式向用户呈现凝练的信息。这种方法使公司能够优化信息处理并做出明智的决策。本研究探讨了两个主要的研究问题：(1) LLMs在供应链风险分析的背景下是否有效用于自动化新闻摘要？(2) 各种LLMs在可读性、重复检测和风险识别方面的摘要质量效果如何？在本文中，我们使用当时可用的多种公开LLM进行了离线研究，并通过针对离线实验中表现最佳系统的用户研究进一步评估了它们的效果。我们的研究结果表明，特别是Few-Shot GPT-4o mini，LLMs在摘要质量和风险识别方面提供了显著的改进。', 'title_zh': '评估大型语言模型在自动新闻文章摘要生成中的有效性'}
{'arxiv_id': 'arXiv:2502.16879', 'title': 'A Multi-LLM-Agent-Based Framework for Economic and Public Policy Analysis', 'authors': 'Yuzhi Hao, Danyang Xie', 'link': 'https://arxiv.org/abs/2502.16879', 'abstract': "This paper pioneers a novel approach to economic and public policy analysis by leveraging multiple Large Language Models (LLMs) as heterogeneous artificial economic agents. We first evaluate five LLMs' economic decision-making capabilities in solving two-period consumption allocation problems under two distinct scenarios: with explicit utility functions and based on intuitive reasoning. While previous research has often simulated heterogeneity by solely varying prompts, our approach harnesses the inherent variations in analytical capabilities across different LLMs to model agents with diverse cognitive traits. Building on these findings, we construct a Multi-LLM-Agent-Based (MLAB) framework by mapping these LLMs to specific educational groups and corresponding income brackets. Using interest-income taxation as a case study, we demonstrate how the MLAB framework can simulate policy impacts across heterogeneous agents, offering a promising new direction for economic and public policy analysis by leveraging LLMs' human-like reasoning capabilities and computational power.", 'abstract_zh': '本文开创了一种利用多个大型语言模型（LLMs）作为异质经济代理的新颖方法，以推动经济和公共政策分析。我们首先评估了五种LLM在解决两种不同情景下的两期消费分配问题中的经济决策能力：一种是有明确效用函数的情景，另一种是基于直观推理的情景。虽然以往的研究常常通过改变提示来模拟异质性，我们的方法则是利用不同LLM之间分析能力固有的差异来构建具有不同认知特征的代理模型。基于这些发现，我们构建了一个多LLM代理为基础的（MLAB）框架，将这些LLM映射到特定的教育群体和对应的收入区间。以利息和收入征税为例，我们展示了MLAB框架如何模拟不同类型的代理所受政策影响，提供了一种通过利用LLMs的人类推理能力和计算能力来推动经济和公共政策分析的新方向。', 'title_zh': '基于多大型语言模型代理的经济与公共政策分析框架'}
{'arxiv_id': 'arXiv:2502.16690', 'title': 'From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task', 'authors': 'Nicolas Martorell', 'link': 'https://arxiv.org/abs/2502.16690', 'abstract': 'Understanding how large language models (LLMs) represent and reason about spatial information is crucial for building robust agentic systems that can navigate real and simulated environments. In this work, we investigate the influence of different text-based spatial representations on LLM performance and internal activations in a grid-world navigation task. By evaluating models of various sizes on a task that requires navigating toward a goal, we examine how the format used to encode spatial information impacts decision-making. Our experiments reveal that cartesian representations of space consistently yield higher success rates and path efficiency, with performance scaling markedly with model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal units, primarily located in intermediate layers, that robustly correlate with spatial features, such as the position of the agent in the grid or action correctness, regardless of how that information is represented, and are also activated by unrelated spatial reasoning tasks. This work advances our understanding of how LLMs process spatial information and provides valuable insights for developing more interpretable and robust agentic AI systems.', 'abstract_zh': '理解大型语言模型（LLMs）如何表示和推理空间信息对于构建能够在现实和模拟环境中自主导航的稳健代理系统至关重要。在这项工作中，我们探讨了不同基于文本的空间表示形式对LLM在网格世界导航任务中的性能和内部激活的影响。通过在要求导航至目标的任务中评估不同规模的模型，我们研究了用于编码空间信息的格式如何影响决策过程。我们的实验表明，笛卡尔空间表示持续地显示出更高的成功率和路径效率，并且随着模型规模的增加性能显著提升。此外，对LLaMA-3.1-8B的探针实验揭示了内部单元的子集，这些单元主要位于中间层，与空间特征（如网格中的代理位置或动作的正确性）表现出稳健的相关性，无论这些信息如何表示，并且在与空间推理无关的任务中也会被激活。这项工作推进了我们对LLMs处理空间信息的理解，并为开发更具可解释性和鲁棒性的代理AI系统提供了宝贵见解。', 'title_zh': '从文本到空间：在网格世界导航任务中LLM中抽象空间模型的映射'}
{'arxiv_id': 'arXiv:2502.16666', 'title': 'SBSC: Step-By-Step Coding for Improving Mathematical Olympiad Performance', 'authors': 'Kunal Singh, Ankan Biswas, Sayandeep Bhowmick, Pradeep Moturi, Siva Kishore Gollapalli', 'link': 'https://arxiv.org/abs/2502.16666', 'abstract': "We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. At each step/turn, by leveraging the code execution outputs and programs of previous steps, the model generates the next sub-task and the corresponding program to solve it. This way, SBSC, sequentially navigates to reach the final answer. SBSC allows more granular, flexible and precise approach to problem-solving compared to existing methods. Extensive experiments highlight the effectiveness of SBSC in tackling competition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe SBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program generation based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME and 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark SBSC's greedy decoding against self-consistency decoding results of existing SOTA math reasoning strategies and observe performance gain by absolute 6.2% on AMC, 6.7% on AIME and 7.4% on MathOdyssey.", 'abstract_zh': '我们提出了一种逐步编码（SBSC）框架：一种多轮次的数学推理方法，使大型语言模型（LLMs）能够生成解决奥林匹克级别数学问题的程序序列。在每一步/轮次中，模型通过利用前一步的代码执行输出和程序生成接下来子任务及其相应的程序。这样，SBSC能够逐步导航以达到最终答案。与现有方法相比，SBSC提供了更细粒度、更灵活和更精确的解题方法。大量实验表明，SBSC在应对数学竞赛和奥林匹克级别数学问题方面具有较高有效性。对于Claude-3.5-Sonnet，我们在AMC12上观察到SBSC（贪婪解码）比现有最先进（SOTA）的程序生成推理策略高出10.7%，在AIME上高出8%，在MathOdyssey上高出12.6%。由于SBSC是多轮次的，我们还对SBSC的贪婪解码与现有SOTA数学推理策略的自我一致性解码结果进行了基准测试，并观察到在AMC上高出6.2%，在AIME上高出6.7%，在MathOdyssey上高出7.4%的性能增益。', 'title_zh': 'SBSC：逐步编程方法以提高数学 Olympiad 表现'}
{'arxiv_id': 'arXiv:2502.16606', 'title': 'Reasoning about Affordances: Causal and Compositional Reasoning in LLMs', 'authors': 'Magnus F. Gjerde, Vanessa Cheung, David Lagnado', 'link': 'https://arxiv.org/abs/2502.16606', 'abstract': "With the rapid progress of Large Language Models (LLMs), it becomes increasingly important to understand their abilities and limitations. In two experiments, we investigate the causal and compositional reasoning abilities of LLMs and humans in the domain of object affordances, an area traditionally linked to embodied cognition. The tasks, designed from scratch to avoid data contamination, require decision-makers to select unconventional objects to replace a typical tool for a particular purpose, such as using a table tennis racket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-4o, finding that GPT-4o, when given chain-of-thought prompting, performed on par with human participants, while GPT-3.5 lagged significantly. In Experiment 2, we introduced two new conditions, Distractor (more object choices, increasing difficulty) and Image (object options presented visually), and evaluated Claude 3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor condition significantly impaired performance across humans and models, although GPT-4o and Claude 3.5 still performed well above chance. Surprisingly, the Image condition had little impact on humans or GPT-4o, but significantly lowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-4o and Claude 3.5 have a stronger ability than their predecessors to identify and flexibly apply causally relevant object properties. The improvement from GPT-3.5 and Claude 3 to GPT-4o and Claude 3.5 suggests that models are increasingly capable of causal and compositional reasoning in some domains, although further mechanistic research is necessary to understand how LLMs reason.", 'abstract_zh': '随着大型语言模型（LLMs）的迅速发展，理解其能力和局限性变得日益重要。通过两个实验，我们探索了LLMs和人类在物体功能领域内的因果推理和组合推理能力，该领域传统上与体手段知相关。所设计的任务均为从零开始，以避免数据污染，要求决策者选择非传统物体来替代特定用途的典型工具，例如用乒乓球拍挖洞。在实验1中，我们评估了GPT-3.5和GPT-4o的表现，发现当给予思维过程提示时，GPT-4o的表现与人类参与者不相上下，而GPT-3.5则明显落后。在实验2中，我们引入了两个新的条件：干扰（更多的物体选择，增加难度）和图像（以视觉方式呈现物体选项），并对Claude 3 Sonnet和Claude 3.5 Sonnet进行了评估，除了GPT模型外。与人类和模型相比，干扰条件显著降低了表现，尽管GPT-4o和Claude 3.5仍然表现远超偶然水平。令人惊讶的是，图像条件对人类和GPT-4o几乎没有影响，但显著降低了Claude 3.5的准确性。质性分析表明，GPT-4o和Claude 3.5相比其前身具有更强的能力来识别和灵活应用因果相关的物体属性。从GPT-3.5和Claude 3到GPT-4o和Claude 3.5的改进表明，在某些领域，模型在因果和组合推理方面的能力正在增强，尽管还需要进一步的机制研究来理解LLMs是如何进行推理的。', 'title_zh': '关于 affordances 的推理：因果性和组合性推理在大型语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.16402', 'title': 'Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language Models for Navigation Applications', 'authors': 'Feng Ma, Xiu-min Wang, Chen Chen, Xiao-bin Xu, Xin-ping Yan', 'link': 'https://arxiv.org/abs/2502.16402', 'abstract': 'Existing navigation decision support systems often perform poorly when handling non-predefined navigation scenarios. Leveraging the generalization capabilities of large language model (LLM) in handling unknown scenarios, this research proposes a dual-core framework for LLM applications to address this issue. Firstly, through ReAct-based prompt engineering, a larger LLM core decomposes intricate navigation tasks into manageable sub-tasks, which autonomously invoke corresponding external tools to gather relevant information, using this feedback to mitigate the risk of LLM hallucinations. Subsequently, a fine-tuned and compact LLM core, acting like a first-mate is designed to process such information and unstructured external data, then to generates context-aware recommendations, ultimately delivering lookout insights and navigation hints that adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) and other rules. Extensive experiments demonstrate the proposed framework not only excels in traditional ship collision avoidance tasks but also adapts effectively to unstructured, non-predefined, and unpredictable scenarios. A comparative analysis with DeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and rationality of the proposed framework. This research bridges the gap between conventional navigation systems and LLMs, offering a framework to enhance safety and operational efficiency across diverse navigation applications.', 'abstract_zh': '现有的导航决策支持系统在处理非预定义导航场景时常常表现不佳。利用大型语言模型（LLM）在处理未知场景方面的泛化能力，本研究提出了一种双核框架，以解决这一问题。首先，通过基于ReAct的方法优化提示工程，大型LLM核心将复杂的导航任务分解为可管理的子任务，这些子任务能够自主调用相应的外部工具以收集相关数据，并利用反馈来减轻LLM幻觉的风险。其次，一种经过微调并设计得更加紧凑的核心LLM，类似于大副的功能，负责处理这些信息以及未结构化的外部数据，生成上下文相关的建议，最终提供符合《国际海上避碰规则》（COLREGs）和其他规则的瞭望和导航提示。广泛的实验表明，所提出框架不仅在传统的船舶避碰任务中表现出色，还能有效应对非结构化、非预定义以及不可预测的场景。与DeepSeek-R1、GPT-4o和其他领先模型的对比分析进一步突显了所提出框架的有效性和合理性。本研究填补了传统导航系统与LLM之间的差距，提供了一个框架以增强各种导航应用中的安全性和操作效率。', 'title_zh': '导航-GPT：一种利用大规模语言模型的稳健且适应性强的框架，应用于导航应用'}
{'arxiv_id': 'arXiv:2502.16235', 'title': 'Dynamic Parallel Tree Search for Efficient LLM Reasoning', 'authors': 'Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.16235', 'abstract': 'Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by structuring problem-solving as a spanning tree. However, recent methods focus on search accuracy while overlooking computational efficiency. The challenges of accelerating the ToT lie in the frequent switching of reasoning focus, and the redundant exploration of suboptimal solutions. To alleviate this dilemma, we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework that aims to dynamically optimize the reasoning path in inference. It includes the Parallelism Streamline in the generation phase to build up a flexible and adaptive parallelism with arbitrary paths by fine-grained cache management and alignment. Meanwhile, the Search and Transition Mechanism filters potential candidates to dynamically maintain the reasoning focus on more possible solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with Math500 and GSM8K datasets show that DPTS significantly improves efficiency by 2-4x on average while maintaining or even surpassing existing reasoning algorithms in accuracy, making ToT-based reasoning more scalable and computationally efficient.', 'abstract_zh': 'Tree of Thoughts (ToT)通过将问题解决结构化为生成树来增强大型语言模型（LLM）的推理能力。然而，最近的方法主要关注搜索精度，而忽视了计算效率。加速ToT面临的挑战在于频繁的推理焦点转换以及对次优解决方案的冗余探索。为了解决这一难题，我们提出了动态并行树搜索（DPTS），这是一种新的并行框架，旨在推理过程中动态优化推理路径。它在生成阶段包含了并行流管理流，通过细粒度的缓存管理和对齐，构建灵活且适应性强的并行性，并且可以跟随任意路径。与此同时，搜索和过渡机制通过筛选潜在候选人，动态维护更可能的推理焦点，并减少冗余。在Qwen-2.5和Llama-3模型上，使用Math500和GSM8K数据集的实验表明，DPTS在保持或超越现有推理算法精度的同时，平均提高了2-4倍的效率，使得基于ToT的推理更具可扩展性和计算效率。', 'title_zh': '高效的大型语言模型推理的动态并行树搜索方法'}
{'arxiv_id': 'arXiv:2502.16169', 'title': 'Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations', 'authors': 'Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song', 'link': 'https://arxiv.org/abs/2502.16169', 'abstract': "Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. To fill this gap, in this work, we introduce Robust Rule Induction, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task, we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; (2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., 0% accuracy change with only 70% consistent score); (3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at \\href{this https URL}{this https URL}.", 'abstract_zh': '归纳推理是人类认知的基础，能够从有限的数据中进行泛化，但目前还没有完全被大型语言模型（LLMs）实现。尽管现代LLMs在推理任务中表现优异，但在不完美的观测条件下保持规则抽象的稳定性和一致性方面，其能力仍有待进一步探索。为填补这一空白，本研究引入了稳健规则归纳（Robust Rule Induction）这一任务，旨在评估LLMs从掺有噪声示例的数据中推导规则的能力。为了解决这一任务，我们进一步提出了样本导向的规则精炼（SRR，Sample-steered Rule Refinement）方法，该方法通过观测多样化和执行导向的反馈来增强推理的稳定性。实验结果表明：（1）在噪声环境下，SRR方法表现出色且性能下降最小；（2）尽管准确性略有变化，但LLMs在噪声环境中仍表现出不稳定性（例如，仅在70%的标记得分一致的情况下，准确率变化为零）；（3）反事实任务差距揭示了LLMs依赖于记忆化的模式而非真实的抽象。我们的发现挑战了LLMs的推理鲁棒性，揭示了它们对假设漂移和模式过拟合的易感性，并为开发类人归纳系统提供了重要的实证证据。相关代码和数据可在 \\href{此处填写实际链接}{此处填写实际链接} 获取。', 'title_zh': '模式胜于原则：在噪声观测下LLMs归纳推理的脆弱性'}
{'arxiv_id': 'arXiv:2502.15778', 'title': 'One for All: A General Framework of LLMs-based Multi-Criteria Decision Making on Human Expert Level', 'authors': 'Hui Wang, Fafa Zhang, Chaoxu Mu', 'link': 'https://arxiv.org/abs/2502.15778', 'abstract': 'Multi-Criteria Decision Making~(MCDM) is widely applied in various fields, using quantitative and qualitative analyses of multiple levels and attributes to support decision makers in making scientific and rational decisions in complex scenarios. However, traditional MCDM methods face bottlenecks in high-dimensional problems. Given the fact that Large Language Models~(LLMs) achieve impressive performance in various complex tasks, but limited work evaluates LLMs in specific MCDM problems with the help of human domain experts, we further explore the capability of LLMs by proposing an LLM-based evaluation framework to automatically deal with general complex MCDM problems. Within the framework, we assess the performance of various typical open-source models, as well as commercial models such as Claude and ChatGPT, on 3 important applications, these models can only achieve around 60\\% accuracy rate compared to the evaluation ground truth. Upon incorporation of Chain-of-Thought or few-shot prompting, the accuracy rates rise to around 70\\%, and highly depend on the model. In order to further improve the performance, a LoRA-based fine-tuning technique is employed. The experimental results show that the accuracy rates for different applications improve significantly to around 95\\%, and the performance difference is trivial between different models, indicating that LoRA-based fine-tuned LLMs exhibit significant and stable advantages in addressing MCDM tasks and can provide human-expert-level solutions to a wide range of MCDM challenges.', 'abstract_zh': '多准则决策制定（MCDM）在各个领域广泛应用，通过多层次和多属性的定量和定性分析来支持决策者在复杂场景中做出科学合理的决策。然而，传统MCDM方法在处理高维问题时面临瓶颈。考虑到大规模语言模型（LLMs）在各种复杂任务中表现出色，但有限的研究在具体的MCDM问题中使用人类领域专家进行评估，我们进一步探索LLMs的能力，提出了一个基于LLM的评估框架来自动处理一般的复杂MCDM问题。在该框架中，我们评估了各种典型的开源模型以及商业模型（如Claude和ChatGPT）在3个重要应用上的性能，这些模型的准确率仅能达到约60%。通过引入思维链或少量示例提示，准确率提高到约70%，但很大程度上依赖于模型。为了进一步提升性能，我们采用了LoRA（低秩适应性调整）微调技术。实验结果表明，在不同应用中，准确率显著提高到约95%，不同模型之间的性能差异几乎可以忽略，表明LoRA微调的LLMs在解决MCDM任务方面具有显著且稳定的优点，能够为广泛的MCDM挑战提供专家级的解决方案。', 'title_zh': '万中选一：基于大规模语言模型的多准则决策框架，达到人类专家水平'}
{'arxiv_id': 'arXiv:2502.15776', 'title': 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers', 'authors': "Pascal Kesseli, Peter O'Hearn, Ricardo Silveira Cabral", 'link': 'https://arxiv.org/abs/2502.15776', 'abstract': 'We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called this http URL. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.', 'abstract_zh': '我们提出了一种使用大型语言模型形式化和解决基于搜索的问题的新方法，显著改进了先前的最先进技术。我们通过逻辑谜题基准ZebraLogicBench展示了这种方法的有效性。与让大型语言模型（LLM）直接尝试解决谜题不同，我们的方法提示模型将问题形式化为一个聚焦逻辑领域的专用语言（DSL），URL见此[this http URL]。这种形式化的表示然后通过约束求解器进行求解，利用了语言模型和求解器各自的优势。我们的方法在ZebraLogicBench基准测试中，实现了相对于LLama 3.1 70B基线性能65%的绝对提升，准确率超过90%，创造了新的最先进技术。这一重要进展展示了将语言模型与领域专用语言及辅助工具结合，用于解决传统上对大型语言模型构成挑战的任务的巨大潜力。', 'title_zh': 'Logic.py：弥合大规模语言模型与约束求解器之间差距'}
{'arxiv_id': 'arXiv:2502.17420', 'title': 'The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence', 'authors': 'Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger', 'link': 'https://arxiv.org/abs/2502.17420', 'abstract': "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.", 'abstract_zh': '大型语言模型（LLMs）的安全对齐可以通过对抗性构造的输入来规避，但这些攻击机制如何绕过安全屏障仍未被充分理解。先前的研究表明，模型激活空间中的单一拒绝方向决定LLM是否拒绝请求。在本研究中，我们提出了一种新颖的基于梯度的表示工程方法，并利用这种方法来识别拒绝方向。与先前研究不同，我们发现存在多个独立的方向，甚至发现多维的概念锥，这些都调节着拒绝行为。此外，我们证明了正交性并不意味着在干预下的独立性，从而推动了考虑线性和非线性效应的表示独立性的概念。利用这种框架，我们鉴定了机制上独立的拒绝方向。我们表明，LLM中的拒绝机制由复杂的空间结构所支配，并识别出功能上独立的方向，从而证实了多种不同的机制驱动拒绝行为。基于我们的梯度方法，可以揭示这些机制，并进一步为未来关于理解LLM的研究奠定基础。', 'title_zh': '大型语言模型中的拒绝几何学：概念圆锥与表示独立性'}
{'arxiv_id': 'arXiv:2502.17189', 'title': 'IGDA: Interactive Graph Discovery through Large Language Model Agents', 'authors': 'Alex Havrilla, David Alvarez-Melis, Nicolo Fusi', 'link': 'https://arxiv.org/abs/2502.17189', 'abstract': 'Large language models ($\\textbf{LLMs}$) have emerged as a powerful method for discovery. Instead of utilizing numerical data, LLMs utilize associated variable $\\textit{semantic metadata}$ to predict variable relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of $\\textit{interactive graph discovery}$: given a ground truth graph $G^*$ capturing variable relationships and a budget of $I$ edge experiments over $R$ rounds, minimize the distance between the predicted graph $\\hat{G}_R$ and $G^*$ at the end of the $R$-th round. To solve this task we propose $\\textbf{IGDA}$, a LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge experiment selection 2) a local graph update strategy utilizing binary feedback from experiments to improve predictions for unselected neighboring edges. Experiments on eight different real-world graphs show our approach often outperforms all baselines including a state-of-the-art numerical method for interactive graph discovery. Further, we conduct a rigorous series of ablations dissecting the impact of each pipeline component. Finally, to assess the impact of memorization, we apply our interactive graph discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors, finding strong performance in a setting where memorization is impossible. Overall, our results show IGDA to be a powerful method for graph discovery complementary to existing numerically driven approaches.', 'abstract_zh': '大型语言模型（LLM）已展现出作为发现工具的强大能力。与利用数值数据不同，LLM 利用关联变量的语义元数据来预测变量之间的关系。同时，LLM 在给定目标函数 \\(f\\) 和一系列试验时，表现出作为黑盒优化器的强大能力。我们通过对这两方面能力的交叉应用，将 LLM 应用于交互式图发现任务：给定一个真实图 \\(G^*\\) 捕捉变量关系以及预算内的 \\(I\\) 条边的 \\(R\\) 轮试验，目标是在第 \\(R\\) 轮试验结束后，使预测图 \\(\\hat{G}_R\\) 与 \\(G^*\\) 的距离最小化。为了完成这一任务，我们提出了**IGDA**（交互式图发现算法），这是一种基于 LLM 的管道，包含两个关键组件：1）一种基于 LLM 不确定性的边试验选择方法；2）一种利用试验的二元反馈进行局部图更新的策略，以改进未被选择的相邻边的预测。在八种不同真实世界的图上进行的实验表明，我们的方法在多个基准模型包括最先进的交互式图发现数值方法中表现出显著优势。进一步地，我们系统地分析了每个管道组件的影响。最后，为了评估记忆化的影响，我们将交互式图发现策略应用于一个复杂的、截至2024年7月新出现的因果图（蛋白质转录因子图），发现即使在不可能记忆的情况下也能取得良好的性能。总体而言，我们的结果表明 IGDA 是一种补充现有数值驱动方法的强大图发现方法。', 'title_zh': 'IGDA：通过大型语言模型代理进行交互式图发现'}
{'arxiv_id': 'arXiv:2502.17057', 'title': 'LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences', 'authors': 'Sijia Yao, Pengcheng Huang, Zhenghao Liu, Yu Gu, Yukun Yan, Shi Yu, Ge Yu', 'link': 'https://arxiv.org/abs/2502.17057', 'abstract': 'Query expansion plays a crucial role in information retrieval, which aims to bridge the semantic gap between queries and documents to improve matching performance. This paper introduces LLM-QE, a novel approach that leverages Large Language Models (LLMs) to generate document-based query expansions, thereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE designs both rank-based and answer-based rewards and uses these reward models to optimize LLMs to align with the ranking preferences of both retrievers and LLMs, thus mitigating the hallucination of LLMs during query expansion. Our experiments on the zero-shot dense retrieval model, Contriever, demonstrate the effectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by incorporating answer-based reward modeling, LLM-QE generates more relevant and precise information related to the documents, rather than simply producing redundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves the training process of dense retrievers, achieving a more than 5% improvement after fine-tuning. All codes are available at this https URL.', 'abstract_zh': '查询扩展在信息检索中扮演着关键角色，旨在弥合查询和文档之间的语义差距，从而提高匹配性能。本文介绍了LLM-QE，这是一种新颖的方法，利用大型语言模型（LLMs）生成基于文档的查询扩展，从而增强密集检索模型。与传统方法不同，LLM-QE 设计了基于排名和基于答案的奖励模型，并使用这些奖励模型优化LLMs，使其与检索器和LLMs的排名偏好相一致，从而减轻查询扩展过程中LLMs的幻觉现象。在零样本密集检索模型Contriever上的实验表明，LLM-QE 是有效的，实现了超过8%的性能提升。此外，通过引入基于答案的奖励模型，LLM-QE 生成了更多与文档相关的相关信息，而不是仅仅生成冗余的令牌来最大化基于排名的奖励。值得注意的是，LLM-QE 还改善了密集检索训练过程，微调后实现了超过5%的性能提升。所有代码可在以下链接获取：`这个 https URL`。', 'title_zh': 'LLM-QE: 通过将大型语言模型与排名偏好对齐以改进查询扩展'}
{'arxiv_id': 'arXiv:2502.16896', 'title': 'Zero-shot Load Forecasting for Integrated Energy Systems: A Large Language Model-based Framework with Multi-task Learning', 'authors': 'Jiaheng Li, Donghe Li, Ye Yang, Huan Xi, Yu Xiao, Li Sun, Dou An, Qingyu Yang', 'link': 'https://arxiv.org/abs/2502.16896', 'abstract': "The growing penetration of renewable energy sources in power systems has increased the complexity and uncertainty of load forecasting, especially for integrated energy systems with multiple energy carriers. Traditional forecasting methods heavily rely on historical data and exhibit limited transferability across different scenarios, posing significant challenges for emerging applications in smart grids and energy internet. This paper proposes the TSLLM-Load Forecasting Mechanism, a novel zero-shot load forecasting framework based on large language models (LLMs) to address these challenges. The framework consists of three key components: a data preprocessing module that handles multi-source energy load data, a time series prompt generation module that bridges the semantic gap between energy data and LLMs through multi-task learning and similarity alignment, and a prediction module that leverages pre-trained LLMs for accurate forecasting. The framework's effectiveness was validated on a real-world dataset comprising load profiles from 20 Australian solar-powered households, demonstrating superior performance in both conventional and zero-shot scenarios. In conventional testing, our method achieved a Mean Squared Error (MSE) of 0.4163 and a Mean Absolute Error (MAE) of 0.3760, outperforming existing approaches by at least 8\\%. In zero-shot prediction experiments across 19 households, the framework maintained consistent accuracy with a total MSE of 11.2712 and MAE of 7.6709, showing at least 12\\% improvement over current methods. The results validate the framework's potential for accurate and transferable load forecasting in integrated energy systems, particularly beneficial for renewable energy integration and smart grid applications.", 'abstract_zh': '可再生能源在电力系统中的日益渗透增加了负荷预测的复杂性和不确定性，特别是在多能源载体集成能源系统中更为明显。传统的预测方法 heavily 依赖历史数据，并且在不同场景下迁移性较差，这在智能电网和能源互联网等领域中构成了显著挑战。本文提出了一种基于大规模语言模型 (LLM) 的新颖零样本负荷预测机制 TSLLM-Load Forecasting Mechanism，以应对这些挑战。该框架包含三个关键组成部分：一个数据预处理模块，处理多源能源负荷数据；一个时间序列提示生成模块，通过多任务学习和相似性对齐，在能源数据和 LLM 之间建立语义桥梁；以及一个预测模块，利用预训练的 LLM 进行准确负荷预测。该框架在包含 20 个澳大利亚太阳能供电家庭负荷剖面的实际数据集上的有效性得到了验证，在传统和零样本场景中均表现出优越的性能。在传统测试中，我们的方法实现了均方误差（MSE）0.4163 和平均绝对误差（MAE）0.3760，比现有方法至少高出 8%。在针对 19 个家庭的零样本预测实验中，该框架保持了一致的准确性，总 MSE 为 11.2712，MAE 为 7.6709，显示出至少 12% 的性能改进，超越了当前方法。实验结果验证了该框架在集成能源系统中实现准确且可迁移负荷预测的潜力，尤其适用于可再生能源整合和智能电网应用。', 'title_zh': '基于大型语言模型和多任务学习的零样本负荷预测框架：面向综合能源系统'}
{'arxiv_id': 'arXiv:2502.16789', 'title': 'AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay', 'authors': 'Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, Liang Lin', 'link': 'https://arxiv.org/abs/2502.16789', 'abstract': 'Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.', 'abstract_zh': '阿尔法挖掘是量化投资中的关键组成部分，专注于在日趋复杂的投资市场中发现对未来资产回报具有预测性的信号。然而，预测因素随时间逐渐失去预测能力的问题（即阿尔法衰减）是一个重大挑战。传统的遗传编程方法由于过拟合和复杂性问题导致阿尔法衰减迅速，而以大型语言模型（LLM）驱动的方法尽管有潜力，但过度依赖现有知识，导致同质化的因素加剧拥挤并加速衰减。为解决这一问题，我们提出了一种自主框架AlphaAgent，该框架能够有效整合LLM代理与针对性的正则化手段，以挖掘具有抗衰减能力的阿尔法因素。AlphaAgent 包含三种关键机制：（i）通过基于抽象语法树（AST）的相似性度量强制原创性，（ii）通过大型语言模型评估市场假设和生成因素之间的语义一致性来实现假设-因素的对齐，以及（iii）通过基于AST的结构约束控制复杂性，防止易过拟合的过度工程化构建。这些机制共同引导阿尔法生成过程，平衡原创性、金融合理性以及对未来市场条件的适应性，从而减轻阿尔法衰减的风险。广泛的评估结果显示，AlphaAgent 在牛市和熊市中均能有效缓解阿尔法衰减现象，过去四年在中国的CSI500指数与美国的S&P500指数市场中持续提供显著的阿尔法收益。特别值得一提的是，AlphaAgent 对阿尔法衰减表现出显著的抵抗力，提升了产生强健因素的潜力。', 'title_zh': 'AlphaAgent：受正则化探索驱动的LLM引导的阿尔法挖掘以对抗阿尔法衰减'}
{'arxiv_id': 'arXiv:2502.16730', 'title': 'RapidPen: Fully Automated IP-to-Shell Penetration Testing with LLM-based Agents', 'authors': 'Sho Nakatani', 'link': 'https://arxiv.org/abs/2502.16730', 'abstract': 'We present RapidPen, a fully automated penetration testing (pentesting) framework that addresses\nthe challenge of achieving an initial foothold (IP-to-Shell) without human intervention. Unlike prior\napproaches that focus primarily on post-exploitation or require a human-in-the-loop, RapidPen\nleverages large language models (LLMs) to autonomously discover and exploit vulnerabilities, starting from\na single IP address. By integrating advanced ReAct-style task planning (Re) with retrieval-augmented\nknowledge bases of successful exploits, along with a command-generation and direct execution feedback loop\n(Act), RapidPen systematically scans services, identifies viable attack vectors, and executes targeted\nexploits in a fully automated manner.\nIn our evaluation against a vulnerable target from the Hack The Box platform, RapidPen achieved shell\naccess within 200-400 seconds at a per-run cost of approximately \\$0.3-\\$0.6, demonstrating a\n60\\% success rate when reusing prior "success-case" data. These results underscore the potential\nof truly autonomous pentesting for both security novices and seasoned professionals. Organizations\nwithout dedicated security teams can leverage RapidPen to quickly identify critical vulnerabilities,\nwhile expert pentesters can offload repetitive tasks and focus on complex challenges.\nUltimately, our work aims to make penetration testing more accessible and cost-efficient,\nthereby enhancing the overall security posture of modern software ecosystems.', 'abstract_zh': '我们提出了RapidPen，这是一个完全自动化的渗透测试（pen testing）框架，能够自动实现从初始立足点（IP到Shell）而无需人工干预。与之前主要集中在恶意利用后阶段或需要人工参与的方法不同，RapidPen 利用大型语言模型（LLMs）自主发现和利用漏洞，从单一IP地址开始。通过将先进的交互式任务规划（ReAct风格任务规划）与成功利用的知识库检索增强相结合，并配以命令生成和直接执行的反馈循环（Act），RapidPen 系统性地扫描服务、识别可行的攻击途径，并以全自动的方式执行有针对性的利用。\n\n在对Hack The Box平台的漏洞目标进行评估时，RapidPen 在每次运行成本约为$0.3-$0.6的情况下，在200-400秒内实现了Shell访问，并在重用先前的成功案例数据时展示了约60%的成功率。这些结果突显了真正自主渗透测试的潜力，无论是对于安全新手还是资深专业人员而言。缺乏专门安全团队的组织可以通过RapidPen 快速识别关键漏洞，而经验丰富的渗透测试员则可以卸载重复性任务，专注于复杂挑战。\n\n最终，我们的工作旨在使渗透测试更加普及和经济高效，从而提升现代软件生态系统整体的安全态势。', 'title_zh': 'RapidPen：基于LLM的代理完全自动化的IP到shell渗透测试方法'}
{'arxiv_id': 'arXiv:2502.16681', 'title': 'Are Sparse Autoencoders Useful? A Case Study in Sparse Probing', 'authors': 'Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda', 'link': 'https://arxiv.org/abs/2502.16681', 'abstract': "Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs' basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs' utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.", 'abstract_zh': '稀疏自编码器（Sparse Autoencoders, SAEs）是一种常用于解释大型语言模型（Large Language Model, LLM）激活中所表示的概念的方法。然而，由于缺乏LLM所使用概念的 ground truth，关于其解释的有效性缺乏有力证据，且越来越多的研究指出当前SAEs存在的问题。一种可替代的证据来源可能是证明SAEs能够在下游任务上超越现有基线，从而提高任务性能。我们通过将SAEs应用于四个具有挑战性的场景下的LLM激活探测任务——数据稀缺性、类别不平衡、标签噪声和协变量偏移——来检验这一点。由于在这些具有挑战性环境中检测概念的难度，我们假设SAEs可解释的概念级别潜在变量的基底应该提供有用的归纳偏置。然而，尽管SAEs在个别数据集上偶尔能表现优于基线，我们却无法设计出将SAEs与基线结合使用的集成方法，使其始终在仅使用基线的集成方法中表现更佳。此外，虽然SAEs最初似乎在识别伪相关、发现数据集质量问题和训练多令牌探测器方面表现良好，但我们也能通过简单的非SAE基线达到相似的效果。虽然我们无法否认SAEs在其他任务中的有用性，但我们的研究结果突显了当前SAEs的缺点，并强调了在具有强大基线的下游任务中严格评估解释性方法的必要性。', 'title_zh': '稀疏自动编码器有用吗？一种基于稀疏探针的案例研究'}
{'arxiv_id': 'arXiv:2502.16660', 'title': 'BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning', 'authors': 'Haiteng Zhao, Chang Ma, FangZhi Xu, Lingpeng Kong, Zhi-Hong Deng', 'link': 'https://arxiv.org/abs/2502.16660', 'abstract': 'The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at this https URL.', 'abstract_zh': '近年来，大型语言模型（LLMs）在各个生物学领域中得到了应用探索，但在复杂生物系统中的推理能力，如通路系统，仍缺乏深入研究。这种能力对于预测生物现象、提出假设以及设计实验至关重要。本研究旨在探索LLMs在通路推理中的潜力。我们引入了BioMaze数据集，其中包括5100个从实际研究中提取出来的复杂通路问题，涵盖了多种生物背景，包括自然动态变化、干扰条件、额外的干预条件以及多尺度研究目标。我们对CoT和图增强推理等方法的评估表明，LLMs在通路推理中面临挑战，尤其是在受到干扰的系统中。为了解决这一问题，我们提出了一种名为PathSeeker的LLM代理，该代理通过基于子图的交互式导航增强了推理能力，从而以更科学的方式有效地处理复杂生物系统的难题。数据集和代码可在以下地址获取：[提供网址]。', 'title_zh': 'BioMaze: 评估与增强生物路径推理的大规模语言模型'}
{'arxiv_id': 'arXiv:2502.16523', 'title': 'Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension', 'authors': 'Yulong Wu, Viktor Schlegel, Riza Batista-Navarro', 'link': 'https://arxiv.org/abs/2502.16523', 'abstract': 'As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.', 'abstract_zh': '随着神经语言模型在机器阅读理解（MRC）任务上达到接近人类的表现，并得到广泛应用，确保其在真实世界场景中的鲁棒性变得越来越重要。尽管现有的鲁棒性评估研究主要开发了合成扰动方法，但这些方法尚未清楚地反映现实场景。为此，我们提出了一种框架，通过利用可获得的维基百科编辑历史替换MRC基准中的段落，以自动检测MRC模型在自然发生的文本扰动下的表现。这种类型的扰动是自然的，因为它不是源于人工生成的过程，这与之前研究的合成方法在本质上是不同的。在包含SQUAD数据集和各种模型架构的大规模研究中，我们观察到，自然扰动导致了预先训练的编码器语言模型性能下降。更令人担忧的是，最先进的Flan-T5和大型语言模型（LLMs）从这些错误中继承了这些错误。进一步的实验表明，我们的发现适用于其他更具挑战性的MRC基准中存在的自然扰动。为了减轻这些错误，我们展示了通过使用自然或合成扰动的示例进行训练，可以提高对自然扰动的鲁棒性，尽管与未扰动数据相比，仍有明显的性能差距。', 'title_zh': '请关注现实世界的扰动！机器阅读理解中的自然鲁棒性评估'}
{'arxiv_id': 'arXiv:2502.16428', 'title': 'Visual Reasoning Evaluation of Grok, Deepseek Janus, Gemini, Qwen, Mistral, and ChatGPT', 'authors': 'Nidhal Jegham, Marwan Abdelatti, Abdeltawab Hendawi', 'link': 'https://arxiv.org/abs/2502.16428', 'abstract': 'Traditional evaluations of multimodal large language models (LLMs) have been limited by their focus on single-image reasoning, failing to assess crucial aspects like contextual understanding, reasoning stability, and uncertainty calibration. This study addresses these limitations by introducing a novel benchmark that integrates multi-image reasoning tasks with rejection-based evaluation and positional bias detection. To evaluate these dimensions, we further introduce entropy as a novel metric for quantifying reasoning consistency across reordered answer variants. We applied this benchmark to assess Grok 3, ChatGPT-4o, ChatGPT-o1, Gemini 2.0 Flash Experimental, DeepSeek Janus models, Qwen2.5-VL-72B-Instruct, QVQ-72B-Preview, and Pixtral 12B across eight visual reasoning tasks, including difference spotting and diagram interpretation. Our findings reveal ChatGPT-o1 leading in overall accuracy (82.5\\%) and rejection accuracy (70.0\\%), closely followed by Gemini 2.0 Flash Experimental (70.8\\%). QVQ-72B-Preview demonstrated superior rejection accuracy (85.5\\%). Notably, Pixtral 12B (51.7\\%) showed promise in specific domains, while Janus models exhibited challenges in bias and uncertainty calibration, reflected in low rejection accuracies and high entropy scores. High entropy scores in Janus models (Janus 7B: 0.8392, Janus 1B: 0.787) underscore their susceptibility to positional bias and unstable reasoning, contrasting with the low entropy and robust reasoning of ChatGPT models. The study further demonstrates that model size is not the sole determinant of performance, as evidenced by Grok 3 underperformance despite its substantial parameter count. By employing multi-image contexts, rejection mechanisms, and entropy-based consistency metrics, this benchmark sets a new standard for evaluating multimodal LLMs, enabling a more robust and reliable assessment of next-generation AI systems.', 'abstract_zh': '传统的多模态大型语言模型（LLMs）评估主要集中在单张图像的推理上，忽视了上下文理解、推理稳定性以及不确定性校准等关键方面。本研究通过引入一个新颖的基准来解决这些问题，该基准结合了多图像推理任务、基于拒绝的评价以及位置偏差检测。为了评估这些维度，我们进一步引入了熵作为量化不同变体推理一致性的新指标。我们将这个基准应用于八个视觉推理任务中，包括差分识别和图示解释，评估了Grok 3、ChatGPT-4o、ChatGPT-o1、Gemini 2.0 Flash Experimental、DeepSeek Janus 模型、Qwen2.5-VL-72B-Instruct、QVQ-72B-Preview 和 Pixtral 12B。研究结果表明，ChatGPT-o1 在整体准确度（82.5%）和拒绝准确度（70.0%）方面领先，Gemini 2.0 Flash Experimental 紧随其后（70.8%）。QVQ-72B-Preview 在拒绝准确度方面表现尤为优异（85.5%）。值得注意的是，Pixtral 12B 在特定领域表现出潜力，而 Janus 模型在偏见和不确定性校准方面存在挑战，表现为低拒绝准确度和高熵分数。Janus 模型（如 Janus 7B 的熵为 0.8392，Janus 1B 的熵为 0.787）的高熵分数表明它们易受位置偏差和推理不稳定性的影响，与 ChatGPT 模型的低熵和稳健推理形成鲜明对比。研究进一步表明，模型大小并不是决定性能的唯一因素，尽管 Grok 3 的参数数量庞大，但其性能却不及预期。通过运用多图像上下文、拒绝机制和基于熵的一致性度量，这一基准为评估多模态LLMs 设定了新的标准，使其能够提供更加稳健和可靠的下一代AI系统的评估。', 'title_zh': '“Grok、Deepseek Janus、Gemini、Qwen、Mistral 和 ChatGPT 的视觉推理评估”'}
{'arxiv_id': 'arXiv:2502.16198', 'title': 'An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning', 'authors': 'Masoud Shokrnezhad, Tarik Taleb', 'link': 'https://arxiv.org/abs/2502.16198', 'abstract': '6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC.', 'abstract_zh': '6G网络旨在实现全球覆盖、巨大连接性和极其严格的要求。空间-空中-地面综合网络（SAGINs）和语义通信（SemCom）对于实现这些目标是必不可少的，但它们在资源编排方面引入了相当大的复杂性。借鉴机器人学领域的研究，一种管理这种复杂性的可行解决方案是应用大型语言模型（LLMs）。尽管近年来LLMs在网络编排中的应用引起了关注，但现有解决方案尚未充分解决LLMs幻觉问题或其适应网络动态问题。为解决这一差距，本文提出了一种名为自主强化协调（ARC）的框架，用于实现 enabled SAGIN。该框架利用基于LLM的检索增强生成器（RAG）监控服务、用户和资源，并处理收集的数据，同时利用层次动作规划器（HAP）进行资源编排。ARC将编排分解为两层，利用LLMs进行高层次规划，而使用强化学习（RL）代理进行低层次决策，这与混合专家（MoE）概念相一致。LLMs利用chain-of-thought（CoT）推理进行少样本学习，得益于对比学习的支持，而RL代理则利用经验重放管理策略，实现持续学习，从而实现效率、准确性和适应性。本文提供了仿真结果来证明ARC的有效性，并详细讨论了增强和升级ARC的潜在未来研究方向。', 'title_zh': '一种结合大规模语言模型和持续强化学习的自主网络编排框架'}
{'arxiv_id': 'arXiv:2502.16175', 'title': 'Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens', 'authors': 'Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu', 'link': 'https://arxiv.org/abs/2502.16175', 'abstract': 'Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis.', 'abstract_zh': '人类的身体动作传递了关于行动意图和认知过程的关键见解，但现有的多模态系统主要集中在通过语言、视觉和音频来理解人类动作，这些系统难以捕捉3D动作中固有的动态力和转矩。惯性测量单元（IMUs）提供了一种有潜力的替代方案，它们轻便、可穿戴且隐私保护。然而，处理流式IMU数据面临着无线传输不稳定性、传感器噪声和漂移等挑战，限制了它们在长时实时动作捕捉（MoCap）以及更重要的是，在线动作分析中的实用性。为解决这些挑战，我们提出了Mojito，这是一种智能运动代理，结合了惯性传感和大型语言模型（LLMs），以实现互动式运动捕捉和行为分析。', 'title_zh': '莫吉托：基于LLM的运动指导员，具有抖动减少的惯性令牌'}
{'arxiv_id': 'arXiv:2502.16097', 'title': 'LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools', 'authors': 'Haoxiang Fan, Changshuang Zhou, Hao Yu, Xueyang Wu, Jiangyu Gu, Zhenhui Peng', 'link': 'https://arxiv.org/abs/2502.16097', 'abstract': "Teaching literature under interdisciplinary contexts (e.g., science, art) that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with the literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker's usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.", 'abstract_zh': '在跨学科（如科学、艺术）的情境下教授文学作品已经成为小学教育中流行的教学方式。然而，构建这样的情境颇具挑战性，因为这要求教师探索大量的跨学科内容，并将其与阅读材料链接起来。本文通过包括13名教师在内的迭代设计过程，开发了LitLinker，以促进跨学科情境的教学构想。借助大型语言模型（LLM），LitLinker 可以推荐跨学科主题，并通过阅读材料中的文学元素（如段落、观点）对其进行情境化。一项针对16名参与者的被试内实验表明，相较于一个LLM聊天机器人，LitLinker 可以提高不同学科内容的整合深度，并减少这一构想任务中的工作负担。专家访谈（9名专家）也证明了LitLinker 在支持文学教学中的跨学科情境构想方面具有实用性。本文最后探讨了使用LLM支持跨学科教学时的关注点和设计考虑。', 'title_zh': 'LitLinker：通过大型语言模型支持小学文学教学中的跨学科创意构建'}
{'arxiv_id': 'arXiv:2502.15990', 'title': 'Automated Query-Product Relevance Labeling using Large Language Models for E-commerce Search', 'authors': 'Jayant Sachdev, Sean D Rosario, Abhijeet Phatak, He Wen, Swati Kirti, Chittaranjan Tripathy', 'link': 'https://arxiv.org/abs/2502.15990', 'abstract': "Accurate query-product relevance labeling is indispensable to generate ground truth dataset for search ranking in e-commerce. Traditional approaches for annotating query-product pairs rely on human-based labeling services, which is expensive, time-consuming and prone to errors. In this work, we explore the application of Large Language Models (LLMs) to automate query-product relevance labeling for large-scale e-commerce search. We use several publicly available and proprietary LLMs for this task, and conducted experiments on two open-source datasets and an in-house e-commerce search dataset. Using prompt engineering techniques such as Chain-of-Thought (CoT) prompting, In-context Learning (ICL), and Retrieval Augmented Generation (RAG) with Maximum Marginal Relevance (MMR), we show that LLM's performance has the potential to approach human-level accuracy on this task in a fraction of the time and cost required by human-labelers, thereby suggesting that our approach is more efficient than the conventional methods. We have generated query-product relevance labels using LLMs at scale, and are using them for evaluating improvements to our search algorithms. Our work demonstrates the potential of LLMs to improve query-product relevance thus enhancing e-commerce search user experience. More importantly, this scalable alternative to human-annotation has significant implications for information retrieval domains including search and recommendation systems, where relevance scoring is crucial for optimizing the ranking of products and content to improve customer engagement and other conversion metrics.", 'abstract_zh': '精确的查询-产品相关性标注对于电子商务搜索排名生成真实数据集至关重要。传统的查询-产品对标注方法依赖于人力标注服务，这成本高、耗时且容易出错。在本工作中，我们探索了大规模电子商务搜索中利用大规模语言模型（LLMs）自动化查询-产品相关性标注的应用。我们使用了多个公开可用和专有的LLMs进行此项任务，并在两个开源数据集和一个内部电子商务搜索数据集上进行了实验。利用提示工程技术，如思维链（CoT）提示、上下文学习（ICL）、检索增强生成（RAG）以及最大边际相关性（MMR），我们展示了在完成任务所需的人力标注者时间和成本少得多的情况下，LLMs的性能有潜力达到人类级别的准确度，从而表明我们提出的方法相较于传统方法更加高效。我们已使用大规模语言模型生成了查询-产品相关性标签，并将其用于评估搜索算法的改进。本工作展示了大规模语言模型在提高查询-产品相关性方面的潜力，从而提升了电子商务搜索的用户体验。更重要的是，这一可扩展的人工标注替代方案对包括搜索和推荐系统在内的信息检索领域具有重要意义，这些领域的相关性评分对于优化产品和内容的排名、提高客户参与度及其他转化指标至关重要。', 'title_zh': '使用大规模语言模型自动标注查询与产品相关性标签以优化电商搜索'}
{'arxiv_id': 'arXiv:2502.15980', 'title': 'Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation', 'authors': 'Yuan Tian, Daniel Lee, Fei Wu, Tung Mai, Kun Qian, Siddhartha Sahai, Tianyi Zhang, Yunyao Li', 'link': 'https://arxiv.org/abs/2502.15980', 'abstract': 'Text-to-SQL models, which parse natural language (NL) questions to executable SQL queries, are increasingly adopted in real-world applications. However, deploying such models in the real world often requires adapting them to the highly specialized database schemas used in specific applications. We find that existing text-to-SQL models experience significant performance drops when applied to new schemas, primarily due to the lack of domain-specific data for fine-tuning. This data scarcity also limits the ability to effectively evaluate model performance in new domains. Continuously obtaining high-quality text-to-SQL data for evolving schemas is prohibitively expensive in real-world scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop text-to-SQL data annotation system. SQLsynth streamlines the creation of high-quality text-to-SQL datasets through human-LLM collaboration in a structured workflow. A within-subjects user study comparing SQLsynth with manual annotation and ChatGPT shows that SQLsynth significantly accelerates text-to-SQL data annotation, reduces cognitive load, and produces datasets that are more accurate, natural, and diverse. Our code is available at this https URL.', 'abstract_zh': '文本到SQL模型能够将自然语言（NL）问题解析为可执行的SQL查询，近年来在实际应用中被越来越多地采用。然而，在实际部署这些模型时，通常需要将它们适应特定应用程序中使用的高度专业化数据库模式。我们发现，现有的文本到SQL模型在应用于新的模式时会遭受显著的性能下降，主要原因是缺乏用于微调的领域特定数据。数据稀缺性也限制了在新领域有效评估模型性能的能力。在实际场景中，持续获取高质量的文本到SQL数据以适应不断变化的模式成本高昂。为了解决这一问题，我们提出了一种名为SQLsynth的人机协作文本到SQL数据标注系统。SQLsynth通过结构化的流程实现人类与大语言模型（LLM）协作，简化了高质量文本到SQL数据集的创建过程。一项区内被试者用户研究将SQLsynth与手工标注和ChatGPT进行了比较，结果显示SQLsynth显著加快了文本到SQL数据标注的速度，降低了认知负担，并生成了更准确、自然且多样化的数据集。我们的代码可在以下网址获取：[此处替换为实际网址]。', 'title_zh': '通过人类-大语言模型协作数据标注实现的文本到SQL领域自适应'}
{'arxiv_id': 'arXiv:2502.15954', 'title': 'MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning', 'authors': 'Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang', 'link': 'https://arxiv.org/abs/2502.15954', 'abstract': "Objective: To optimize in-context learning in biomedical natural language processing by improving example selection. Methods: We introduce a novel multi-mode retrieval-augmented generation (MMRAG) framework, which integrates four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2) Top Mode, retrieving the most relevant examples based on similarity; (3) Diversity Mode, ensuring variation in selected examples; and (4) Class Mode, selecting category-representative examples. This study evaluates MMRAG on three core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Text Classification (TC). The datasets used include BC2GM for gene and protein mention recognition (NER), DDI for drug-drug interaction extraction (RE), GIT for general biomedical information extraction (RE), and HealthAdvice for health-related text classification (TC). The framework is tested with two large language models (Llama2-7B, Llama3-8B) and three retrievers (Contriever, MedCPT, BGE-Large) to assess performance across different retrieval strategies. Results: The results from the Random mode indicate that providing more examples in the prompt improves the model's generation performance. Meanwhile, Top mode and Diversity mode significantly outperform Random mode on the RE (DDI) task, achieving an F1 score of 0.9669, a 26.4% improvement. Among the three retrievers tested, Contriever outperformed the other two in a greater number of experiments. Additionally, Llama 2 and Llama 3 demonstrated varying capabilities across different tasks, with Llama 3 showing a clear advantage in handling NER tasks. Conclusion: MMRAG effectively enhances biomedical in-context learning by refining example selection, mitigating data scarcity issues, and demonstrating superior adaptability for NLP-driven healthcare applications.", 'abstract_zh': '目的：通过改进示例选择来优化生物医学自然语言处理中的上下文学习。方法：我们提出了一种新型的多模式检索增强生成（MMRAG）框架，该框架结合了四种检索策略：（1）随机模式，随机选择示例；（2）顶级模式，根据相似性检索最相关示例；（3）多样性模式，确保所选示例的多样性；（4）类别模式，选择代表性示例。本研究将MMRAG应用于三个核心的生物医学自然语言处理任务：命名实体识别（NER）、关系提取（RE）和文本分类（TC）。使用的数据集包括：BC2GM用于基因和蛋白质提及识别（NER）、DDI用于药物-药物相互作用提取（RE）、GIT用于一般生物医学信息提取（RE）、HealthAdvice用于与健康有关的文本分类（TC）。该框架使用两个大型语言模型（Llama2-7B，Llama3-8B）和三种检索器（Contriever，MedCPT，BGE-Large）进行测试，以评估不同检索策略下的性能。结果：随机模式的结果表明，在提示中提供更多的示例可以提高模型的生成性能。同时，顶级模式和多样性模式在RE（DDI）任务上显著优于随机模式，取得了0.9669的F1分数，提高了26.4%。在三个测试的检索器中，Contriever在大多数实验中表现优于其他两个检索器。此外，Llama 2和Llama 3在不同任务中显示出了不同的能力，Llama 3在处理NER任务时显示出明显的优势。结论：MMRAG通过改进示例选择有效地提升了生物医学上下文学习，缓解了数据稀缺问题，并展示了在NLP驱动的健康医疗应用中优越的适应性。', 'title_zh': 'MMRAG：基于大型语言模型的多模式检索增强生成方法在生物医学领域内的上下文学习'}
{'arxiv_id': 'arXiv:2502.15797', 'title': 'OCCULT: Evaluating Large Language Models for Offensive Cyber Operation Capabilities', 'authors': 'Michael Kouremetis, Marissa Dotter, Alex Byrne, Dan Martin, Ethan Michalak, Gianpaolo Russo, Michael Threet, Guido Zarrella', 'link': 'https://arxiv.org/abs/2502.15797', 'abstract': "The prospect of artificial intelligence (AI) competing in the adversarial landscape of cyber security has long been considered one of the most impactful, challenging, and potentially dangerous applications of AI. Here, we demonstrate a new approach to assessing AI's progress towards enabling and scaling real-world offensive cyber operations (OCO) tactics in use by modern threat actors. We detail OCCULT, a lightweight operational evaluation framework that allows cyber security experts to contribute to rigorous and repeatable measurement of the plausible cyber security risks associated with any given large language model (LLM) or AI employed for OCO. We also prototype and evaluate three very different OCO benchmarks for LLMs that demonstrate our approach and serve as examples for building benchmarks under the OCCULT framework. Finally, we provide preliminary evaluation results to demonstrate how this framework allows us to move beyond traditional all-or-nothing tests, such as those crafted from educational exercises like capture-the-flag environments, to contextualize our indicators and warnings in true cyber threat scenarios that present risks to modern infrastructure. We find that there has been significant recent advancement in the risks of AI being used to scale realistic cyber threats. For the first time, we find a model (DeepSeek-R1) is capable of correctly answering over 90% of challenging offensive cyber knowledge tests in our Threat Actor Competency Test for LLMs (TACTL) multiple-choice benchmarks. We also show how Meta's Llama and Mistral's Mixtral model families show marked performance improvements over earlier models against our benchmarks where LLMs act as offensive agents in MITRE's high-fidelity offensive and defensive cyber operations simulation environment, CyberLayer.", 'abstract_zh': '人工智能（AI）在网络安全的对抗环境中进行竞争的应用前景，一直被认为是影响最为深远、最具挑战性和潜在危险的AI应用之一。本文展示了一种新的方法，用于评估AI在支持和扩展现代威胁行为者使用的真实世界进攻性网络操作战术方面所取得的进展。我们详细介绍了OCCULT，一个轻量级的操作评估框架，允许网络安全专家对其提供的任何大型语言模型（LLM）或用于进攻性网络操作（Offensive Cyber Operations, OCO）的AI模型的潜在网络安全风险进行严谨且可重复的测量。此外，我们设计并评估了三种非常不同的针对大型语言模型的OCO基准测试，以展示该方法并为在OCCULT框架下构建基准测试提供示例。最后，我们提供了初步评估结果，以证明该框架如何允许我们超越传统的非此即彼的测试（如从教育练习如夺旗环境中构建的测试），从而将我们的指标和警告置于真实的网络威胁场景中，这些场景对现代基础设施构成风险。我们的研究发现，近期在利用AI进行真实网络威胁规模化的风险方面取得了显著进步。首次发现一款模型（DeepSeek-R1）能够在我们为大型语言模型（LLM）设计的威胁行为者能力测试（TACTL）选择题基准中正确回答超过90%的复杂进攻性网络知识测试。我们还展示了Meta的Llama和Mistral的Mixtral模型系列在我们的基准测试中作为进攻性代理在MITRE高保真进攻性和防御性网络操作模拟环境中表现显著改进的情况，这些基准测试中的LLM表现优于早期的模型。', 'title_zh': 'OCCULT：评估大型语言模型在 Offensive 网络操作能力方面的表现'}
{'arxiv_id': 'arXiv:2502.15770', 'title': 'Performance Review on LLM for solving leetcode problems', 'authors': 'Lun Wang, Chuanqi Shi, Shaoshui Du, Yiyi Tao, Yixian Shen, Hang Zheng, Xinyu Qiu', 'link': 'https://arxiv.org/abs/2502.15770', 'abstract': 'This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.', 'abstract_zh': '本文对大型语言模型（LLMs）在解决来自 LeetCode 的编程挑战问题进行了全面的性能评估。LeetCode 是一个广泛用于算法练习和技术面试的平台。我们首先爬取了 LeetCode 网站，收集了一组涵盖不同难度级别和主题的多样化问题。使用该数据集，我们生成了多个 LLM 的解题方案，包括 GPT-4 和 GPT-3.5-turbo（即 ChatGPT-turbo）。生成的解题方案被系统地评估了正确性和效率。我们采用了 pass@k 指标来评估特定次数内成功解决问题的比例，并分析了解题方案的运行时性能。实验结果突显了当前 LLM 在代码生成和问题解决任务中的优点和局限性，并为进一步探讨其在自动化编程辅助方面的应用提供了见解，指出需要改进的领域。', 'title_zh': '用于解决LeetCode问题的大型语言模型性能评估'}
{'arxiv_id': 'arXiv:2502.15763', 'title': 'Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization', 'authors': 'Bowen Pang, Kai Li, Ruifeng She, Feifan Wang', 'link': 'https://arxiv.org/abs/2502.15763', 'abstract': 'With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.', 'abstract_zh': '随着大型语言模型（LLMs）的发展，优化硬件使用和提高吞吐量变得越来越重要。在本文中，我们研究了部署LLMs的服务系统推理优化问题。为了优化系统吞吐量并最大化硬件利用率，我们将推理优化问题形式化为混合整数规划（MIP）模型，并提出了一种混合离线-在线方法作为求解方案。离线方法通过引入最小化最长时间间隔的集装箱打包问题来改进大规模推理系统。我们还提供了理论下的下界计算方法。随后，我们提出了一种在线排序和预emption调度方法，以便更好地利用硬件。在线迭代调度过程中，我们应用拉格朗日方法来评估在每次迭代中插入预填充阶段与解码阶段的成本效率，并动态确定何时抢占解码任务并插入预填充任务。使用来自LLaMA-65B模型和GSM8K数据集的实际情况数据进行的实验表明，系统利用率从80.2%提高到了89.1%，推理总时间从201.00秒减少到了190.58秒。100个案例的研究表明，我们的方法始终优于基线方法，并且平均将利用率提高了8.0%。最后，我们讨论了未来扩展的潜在可能性，包括随机建模、基于强化学习的调度器以及系统吞吐量和硬件利用率的动态决策策略。', 'title_zh': '面向大型语言模型推断优化的混合离线-在线调度方法'}
{'arxiv_id': 'arXiv:2502.15761', 'title': 'LoXR: Performance Evaluation of Locally Executing LLMs on XR Devices', 'authors': 'Dawar Khan, Xinyu Liu, Omar Mena, Donggang Jia, Alexandre Kouyoumdjian, Ivan Viola', 'link': 'https://arxiv.org/abs/2502.15761', 'abstract': 'The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we deploy 17 LLMs across four XR devices--Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct a comprehensive evaluation. We devise an experimental setup and evaluate performance on four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We finally propose a unified evaluation method based on the Pareto Optimality theory to select the optimal device-model pairs from the quality and speed objectives. We believe our findings offer valuable insights to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be followed as standard groundwork for further research and development in this emerging field. All supplemental materials are available at this http URL.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，要符合学术规范：\n\n在扩展现实（XR）设备上部署大型语言模型（LLMs）具有极大的潜力，可以推动人类-人工智能交互领域的发展。在直接在设备上进行模型推理的情况下，选择适合特定任务的模型和设备仍然具有挑战性。本文中，我们将在Magic Leap 2、Meta Quest 3、Vivo X100s Pro和Apple Vision Pro这四种XR设备上部署17个大型语言模型，并进行综合评估。我们设计了一个实验方案，并从四个关键指标：性能一致性、处理速度、内存使用和电池消耗对性能进行了评估。对于每一对68个模型-设备组合，我们在不同字符串长度、批处理大小和线程数下评估其性能，分析实时XR应用中的权衡取舍。最后，我们基于帕累托最优理论提出了一种统一的评估方法，以选择在质量和速度目标方面最优的设备-模型组合。我们相信，我们的发现为未来对XR设备上LLM部署的优化努力提供了宝贵的指导。我们的评估方法可以作为进一步研究和开发此新兴领域的工作基础。所有补充材料请参阅此网址：[提供的网址]。', 'title_zh': 'LoXR：在XR设备上本地执行LLM的性能评估'}
{'arxiv_id': 'arXiv:2502.15754', 'title': 'Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment', 'authors': 'Alireza Marefat, Abbaas Alif Mohamed Nishar, Ashwin Ashok', 'link': 'https://arxiv.org/abs/2502.15754', 'abstract': "This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.", 'abstract_zh': '本文介绍了Text2Net，这是一种创新的文字基础网络仿真引擎，利用自然语言处理（NLP）和大型语言模型（LLMs）将网络拓扑的 plain-text 描述转换为动态交互式仿真。Text2Net 简化了网络仿真配置的过程，无需用户掌握特定供应商的语法或导航复杂的图形界面。通过定性和定量评估，我们展示了Text2Net在部署网络场景方面相较于传统仿真工具（如EVE-NG）显著减少所需时间和努力的能力。通过自动化重复任务和使交互更加直观，Text2Net 提高了学生、教育者和专业人士的使用便利性。该系统为学生提供了将理论知识与实际应用相结合的实际操作学习体验。实验结果展示了其在各种网络复杂性下的可扩展性，标志着在革新网络教育和专业应用场景，如概念验证测试方面迈出了重要一步。', 'title_zh': 'Text2Net: 将文本转换为动态交互网络仿真环境'}
{'arxiv_id': 'arXiv:2502.15727', 'title': 'Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning', 'authors': 'Youssef Maklad, Fares Wael, Wael Elsersy, Ali Hamdi', 'link': 'https://arxiv.org/abs/2502.15727', 'abstract': "This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture in network packet seed generation for network protocol fuzzing. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings in a two-stages. In the first stage, the agent dynamically refers to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol Finite State Machine (FSM), then it iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. In the second stage, we evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.", 'abstract_zh': '本文提出了一种评估基于 Retrieval-Augmented Generation (RAG) 的代理型大型语言模型（LLM）架构在网络协议 fuzzing 中网络数据包种子生成效率的新方法。通过链式思考（Chain-of-Thought, COT）提示技术的增强，该方法专注于提高种子的结构质量，以指导 protocol fuzzing 框架在同一协议状态空间中进行广泛的探索。我们的方法在两个阶段中利用了 RAG 和文本嵌入。在第一阶段，代理动态地参考 RFC 文档知识库以回答有关协议有限状态机（FSM）的问题，然后通过检索到的知识进行迭代推理，以优化输出并正确放置种子。在第二阶段，我们根据 BLEU、ROUGE 和单词错误率（Word Error Rate, WER）等指标评估代理输出的响应结构质量，通过将生成的数据包与真实数据包进行比较。实验结果显示，与基线模型相比，我们的方法在 BLEU、ROUGE 和 WER 上分别取得了高达 18.19%、14.81% 和 23.45% 的改进。这些结果证实了该方法的潜力，可以改进基于 LLM 的协议 fuzzing 框架，以识别隐藏的漏洞。', 'title_zh': '基于检索增强生成的大型语言模型评估：带有链式思考推理的协议状态机推理'}
{'arxiv_id': 'arXiv:2502.15724', 'title': 'Instruction-Based Fine-tuning of Open-Source LLMs for Predicting Customer Purchase Behaviors', 'authors': 'Halil Ibrahim Ergul, Selim Balcisoy, Burcin Bozkaya', 'link': 'https://arxiv.org/abs/2502.15724', 'abstract': "In this study, the performance of various predictive models, including probabilistic baseline, CNN, LSTM, and finetuned LLMs, in forecasting merchant categories from financial transaction data have been evaluated. Utilizing datasets from Bank A for training and Bank B for testing, the superior predictive capabilities of the fine-tuned Mistral Instruct model, which was trained using customer data converted into natural language format have been demonstrated. The methodology of this study involves instruction fine-tuning Mistral via LoRA (LowRank Adaptation of Large Language Models) to adapt its vast pre-trained knowledge to the specific domain of financial transactions. The Mistral model significantly outperforms traditional sequential models, achieving higher F1 scores in the three key merchant categories of bank transaction data (grocery, clothing, and gas stations) that is crucial for targeted marketing campaigns. This performance is attributed to the model's enhanced semantic understanding and adaptability which enables it to better manage minority classes and predict transaction categories with greater accuracy. These findings highlight the potential of LLMs in predicting human behavior.", 'abstract_zh': '在本研究中，对各种预测模型（包括概率基线模型、卷积神经网络CNN、长短期记忆网络LSTM以及微调的大语言模型LLM）在从金融交易数据预测商户类别的性能进行了评估。利用来自Bank A的数据进行训练，并使用来自Bank B的数据进行测试，展示了使用客户数据转换为自然语言格式后进行微调的Mistral Instruct模型的卓越预测能力。本研究的方法是通过LoRA（大型语言模型的低秩适应）对Mistral进行指令微调，以使其广泛预训练的知识适应金融交易领域。微调后的Mistral模型显著优于传统的序列模型，在银行交易数据的三大关键商户类别（杂货店、服装店和加油站）上获得了更高的F1分数，这对于针对性营销活动至关重要。这种性能归因于模型增强的语义理解和适应性，使其能够更好地处理少数类别并更准确地预测交易类别。这些发现突显了大语言模型在预测人类行为方面的潜力。', 'title_zh': '基于指令的开源大语言模型微调以预测客户购买行为'}
{'arxiv_id': 'arXiv:2502.15709', 'title': 'TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation', 'authors': 'Zhaoxing Li, Vahid Yazdanpanah, Jindi Wang, Wen Gu, Lei Shi, Alexandra I. Cristea, Sarah Kiden, Sebastian Stein', 'link': 'https://arxiv.org/abs/2502.15709', 'abstract': "The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10\\% improvement in user satisfaction and a 5\\% increase in quiz scores compared to using general LLMs alone.", 'abstract_zh': '将人工智能（AI）集成到教育中具有显著潜力，可提升学习效率。大型语言模型（LLMs），如ChatGPT、Gemini和Llama，使学生能够查询广泛的主题，提供了前所未有的灵活性。然而，LLMs 面临挑战，例如处理内容相关性的变化以及缺乏个性化。为了解决这些挑战，我们提出了一种基于知识追踪（KT）和检索增强生成（RAG）技术的个性化学习推荐LLMs系统——TutorLLM。TutorLLM 的创新之处在于将KT和RAG技术与LLMs相结合，使其能够动态检索上下文相关的知识，并基于学生的个人学习状态提供个性化学习建议。具体而言，这种集成允许TutorLLM 根据由基于多特征潜在关系BERT的知识追踪（MLFBK模型）预测的个别学习状态定制响应，并通过Scraper模型增强响应准确性。评估包括用户评估问卷和性能指标，结果显示，与单独使用通用的LLMs相比，用户满意度提高了10%，测验成绩提高了5%。', 'title_zh': 'TutorLLM：基于知识追踪和检索增强生成的个性化学习推荐'}
{'arxiv_id': 'arXiv:2502.15700', 'title': 'Sustainable Digitalization of Business with Multi-Agent RAG and LLM', 'authors': 'Muhammad Arslan, Saba Munawar, Christophe Cruz', 'link': 'https://arxiv.org/abs/2502.15700', 'abstract': "Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)'s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.", 'abstract_zh': '企业高度依赖来自多种渠道的数据，如新闻文章、财务报告和消费者评论，以驱动其运营，从而实现基于数据的决策并识别机遇。然而，传统的数据提取方法往往耗时且资源密集，推动企业采用数字化转型举措以提高效率。然而，人们对于这些举措的可持续性及其与联合国（UN）可持续发展目标（SDGs）的一致性仍存有担忧。本研究旨在探讨将大规模语言模型（LLMs）与检索增强生成（RAG）相结合作为信息提取（IE）和处理的可持续解决方案的可能性。研究方法包括回顾现有针对企业决策的数据提取解决方案，发现许多系统需要训练新的机器学习模型，这不仅资源密集，而且对环境造成很大影响。相反，我们提出了一种基于现有LLM的可持续企业解决方案，能够处理多样化的数据集。通过将领域特定的数据集与企业的具体需求相结合，定制LLM，并采用多代理架构将信息检索、丰富和分类等工作分配给专门的代理，从而优化提取过程并提高整体效率。通过这些技术的应用，企业可以优化资源利用，改进决策过程，并在可持续发展目标的推动下促进企业界的环境责任。', 'title_zh': '企业可持续数字化转型的多代理检索增强和大语言模型方法'}
{'arxiv_id': 'arXiv:2502.15684', 'title': 'An Agent Framework for Real-Time Financial Information Searching with Large Language Models', 'authors': 'Jinzheng Li, Jingshu Zhang, Hongguang Li, Yiqing Shen', 'link': 'https://arxiv.org/abs/2502.15684', 'abstract': "Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.", 'abstract_zh': '金融决策需要处理大量的实时信息并理解其复杂的时序关系。虽然传统的搜索引擎在提供实时信息访问方面表现出色，但在理解复杂用户意图和上下文细微差别方面常常力不从心。相反，大型语言模型（LLMs）展示了推理和交互的能力，但在缺乏当前数据接入的情况下可能会生成不可靠的输出。尽管最近已经尝试将LLMs与搜索引擎结合，但这些方法还面临以下问题：（1）受限于专门金融数据的接入；（2）静态查询结构无法适应动态市场条件；（3）结果生成中缺乏足够的时序意识。为了解决这些挑战，我们提出了FinSearch，这是一种专为金融应用设计的新型基于代理的搜索框架，可与多种金融数据源接口，包括市场数据、股票数据和新闻数据。创新性地，FinSearch 包含四个组成部分：（1）基于LLM的多步搜索预规划器，将用户查询分解为结构化的子查询，并通过图表示映射到特定数据源；（2）一个包含基于LLM的适应性查询重写器的搜索执行器，在执行每个子查询搜索的同时，根据中间搜索结果动态调整后续节点中的子查询；（3）一个时序权重机制，根据用户查询推断出的时间上下文优先处理信息的相关性；（4）一个基于LLM的响应生成器，将结果综合成逻辑连贯且上下文适切的输出。为了评估FinSearch，我们构建了FinSearchBench-24基准测试，包含1,500个关于股票市场、利率变化、货币政策和行业发展的四选一问题，覆盖从2024年6月到10月的时间范围。', 'title_zh': '基于大型语言模型的实时金融信息检索代理框架'}
{'arxiv_id': 'arXiv:2502.16924', 'title': 'FilterLLM: Text-To-Distribution LLM for Billion-Scale Cold-Start Recommendation', 'authors': 'Ruochen Liu, Hao Chen, Yuanchen Bei, Zheyu Zhou, Lijia Chen, Qijie Shen, Feiran Huang, Fakhri Karray, Senzhang Wang', 'link': 'https://arxiv.org/abs/2502.16924', 'abstract': 'Large Language Model (LLM)-based cold-start recommendation systems continue to face significant computational challenges in billion-scale scenarios, as they follow a "Text-to-Judgment" paradigm. This approach processes user-item content pairs as input and evaluates each pair iteratively. To maintain efficiency, existing methods rely on pre-filtering a small candidate pool of user-item pairs. However, this severely limits the inferential capabilities of LLMs by reducing their scope to only a few hundred pre-filtered candidates. To overcome this limitation, we propose a novel "Text-to-Distribution" paradigm, which predicts an item\'s interaction probability distribution for the entire user set in a single inference. Specifically, we present FilterLLM, a framework that extends the next-word prediction capabilities of LLMs to billion-scale filtering tasks. FilterLLM first introduces a tailored distribution prediction and cold-start framework. Next, FilterLLM incorporates an efficient user-vocabulary structure to train and store the embeddings of billion-scale users. Finally, we detail the training objectives for both distribution prediction and user-vocabulary construction. The proposed framework has been deployed on the Alibaba platform, where it has been serving cold-start recommendations for two months, processing over one billion cold items. Extensive experiments demonstrate that FilterLLM significantly outperforms state-of-the-art methods in cold-start recommendation tasks, achieving over 30 times higher efficiency. Furthermore, an online A/B test validates its effectiveness in billion-scale recommendation systems.', 'abstract_zh': '基于大规模语言模型（LLM）的冷启动推荐系统在十亿规模的场景中仍然面临着重大的计算挑战，因为它们采用了“文本到判断”的范式。这种方法将用户-项内容对作为输入，并迭代评估每个对。为了保持高效性，现有方法依赖于预先筛选一小部分候选用户-项对池。然而，这严重限制了LLM的推断能力，将其范围仅限于少数几百个预筛选候选对。为克服这一局限性，我们提出了一种新颖的“文本到分布”范式，该范式可以在单次推断中为整个用户集预测一个项目的交互概率分布。具体来说，我们提出了一种称为FilterLLM的框架，该框架将LLM的下一个词预测能力扩展到十亿规模的过滤任务。FilterLLM首先引入了适合的分布预测和冷启动框架。接下来，FilterLLM整合了一种高效用户词汇结构，用于训练和存储十亿规模用户的嵌入。最后，我们详细介绍了分布预测和用户词汇构建的训练目标。该提出的框架已被部署在阿里云平台上，已经为两种月处理超过十亿冷启动项的冷启动推荐提供服务。广泛实验表明，FilterLLM在冷启动推荐任务中显著优于现有方法，效率提高了30多倍。此外，线上A/B测试验证了其在十亿规模推荐系统中的有效性。', 'title_zh': 'FilterLLM：针对十亿规模冷启动推荐的文本到分布大语言模型'}
{'arxiv_id': 'arXiv:2502.16759', 'title': 'The Blessing of Reasoning: LLM-Based Contrastive Explanations in Black-Box Recommender Systems', 'authors': 'Yuyan Wang, Pan Li, Minmin Chen', 'link': 'https://arxiv.org/abs/2502.16759', 'abstract': 'Modern recommender systems use ML models to predict consumer preferences from consumption history. Although these "black-box" models achieve impressive predictive performance, they often suffer from a lack of transparency and explainability. Contrary to the presumed tradeoff between explainability and accuracy, we show that integrating large language models (LLMs) with deep neural networks (DNNs) can improve both. We propose LR-Recsys, which augments DNN-based systems with LLM reasoning capabilities. LR-Recsys introduces a contrastive-explanation generator that produces human-readable positive explanations and negative explanations. These explanations are embedded via a fine-tuned autoencoder and combined with consumer and product features to improve predictions. Beyond offering explainability, we show that LR-Recsys also improves learning efficiency and predictive accuracy, as supported by high-dimensional, multi-environment statistical learning theory.\nLR-Recsys outperforms state-of-the-art recommender systems by 3-14% on three real-world datasets. Importantly, our analysis reveals that these gains primarily derive from LLMs\' reasoning capabilities rather than their external domain knowledge. LR-RecSys presents an effective approach to combine LLMs with traditional DNNs, two of the most widely used ML models today. The explanations generated by LR-Recsys provide actionable insights for consumers, sellers, and platforms, helping to build trust, optimize product offerings, and inform targeting strategies.', 'abstract_zh': '现代推荐系统利用机器学习模型从消费历史中预测消费者偏好。尽管这些“黑匣子”模型在预测性能方面取得了令人印象深刻的成果，但它们通常缺乏透明性和可解释性。与预期中的可解释性和准确性之间的权衡不同，我们展示了将大型语言模型（LLMs）与深度神经网络（DNNs）结合起来可以同时改善两者。我们提出了LR-Recsys，这是一种将LLM推理能力嵌入到基于DNN的推荐系统中的方法。LR-Recsys引入了一个对比性解释生成器，生成可读的正解释和负解释。这些解释通过微调的自动编码器嵌入，并与消费者和产品特征结合，以提高预测性能。除了提供可解释性之外，我们还展示了LR-Recsys在学习效率和预测准确性方面的改进，这一点得到了高维多环境统计学习理论的支持。\n\n在三个实际数据集上，LR-Recsys相较于最新的推荐系统提高了3-14%的表现。重要的是，我们的分析揭示了这些增益主要来自于LLMs的推理能力，而不是它们的外部领域知识。LR-Recsys提供了一种将LLMs与传统的DNNs（当今两种最广泛使用的机器学习模型）结合的有效方法。LR-Recsys生成的解释为消费者、卖方和平台提供了可采取的洞察，有助于建立信任、优化产品供应并指导目标策略。', 'title_zh': '基于推理的恩赐：在黑盒推荐系统中基于LLM的对比解释'}
{'arxiv_id': 'arXiv:2502.15698', 'title': 'Developing an Artificial Intelligence Tool for Personalized Breast Cancer Treatment Plans based on the NCCN Guidelines', 'authors': 'Abdul M. Mohammed, Iqtidar Mansoor, Sarah Blythe, Dennis Trujillo', 'link': 'https://arxiv.org/abs/2502.15698', 'abstract': "Cancer treatments require personalized approaches based on a patient's clinical condition, medical history, and evidence-based guidelines. The National Comprehensive Cancer Network (NCCN) provides frequently updated, complex guidelines through visuals like flowcharts and diagrams, which can be time consuming for oncologists to stay current with treatment protocols. This study presents an AI (Artificial Intelligence)-driven methodology to accurately automate treatment regimens following NCCN guidelines for breast cancer patients.\nWe proposed two AI-driven methods: Agentic-RAG (Retrieval-Augmented Generation) and Graph-RAG. Agentic-RAG used a three-step Large Language Model (LLM) process to select clinical titles from NCCN guidelines, retrieve matching JSON content, and iteratively refine recommendations based on insufficiency checks. Graph-RAG followed a Microsoft-developed framework with proprietary prompts, where JSON data was converted to text via an LLM, summarized, and mapped into graph structures representing key treatment relationships. Final recommendations were generated by querying relevant graph summaries. Both were evaluated using a set of patient descriptions, each with four associated questions.\nAs shown in Table 1, Agentic RAG achieved a 100% adherence (24/24) with no hallucinations or incorrect treatments. Graph-RAG had 95.8% adherence (23/24) with one incorrect treatment and no hallucinations. Chat GPT-4 showed 91.6% adherence (22/24) with two wrong treatments and no hallucinations. Both Agentic RAG and Graph-RAG provided detailed treatment recommendations with accurate references to relevant NCCN document page numbers.", 'abstract_zh': '癌症治疗需要根据患者的临床状况、医疗史和基于证据的指南采用个性化的方法。美国国家综合癌症网络（NCCN）提供了经常更新且复杂的指南，通过流程图和图表等形式呈现，这些对于肿瘤学家来说跟上治疗规范可能需要花费较多时间。本研究提出了一种人工智能（AI）驱动的方法，以准确地根据NCCN指南自动制定乳腺癌患者的治疗方案。\n\n我们提出了两种AI驱动的方法：Agentic-RAG（检索增强生成）和Graph-RAG。Agentic-RAG采用了一种包含三个步骤的大规模语言模型（LLM）过程，从NCCN指南中选择临床标题，检索匹配的JSON内容，并基于不足检查逐步细化推荐。Graph-RAG遵循了由微软开发的框架，并使用专有提示，通过LLM将JSON数据转换为文本，进行总结并映射到表示关键治疗关系的图结构。最终的推荐通过查询相关图摘要生成。这两种方法都使用了一组患者的描述进行评估，每组描述包含四个关联的问题。\n\n如表1所示，Agentic RAG达到了100%的合规性（24/24），没有出现幻觉或错误治疗。Graph-RAG的合规性为95.8%（23/24），有一个错误的治疗并且没有出现幻觉。Chat GPT-4的合规性为91.6%（22/24），有两个错误的治疗但没有出现幻觉。Agentic RAG和Graph-RAG两种方法都提供了详细的治疗建议，并准确引用了相关的NCCN文档页码。', 'title_zh': '基于NCCN指南的个性化乳腺癌治疗方案人工智能工具的开发'}
{'arxiv_id': 'arXiv:2502.15685', 'title': 'Active Large Language Model-based Knowledge Distillation for Session-based Recommendation', 'authors': 'Yingpeng Du, Zhu Sun, Ziyan Wang, Haoyan Chua, Jie Zhang, Yew-Soon Ong', 'link': 'https://arxiv.org/abs/2502.15685', 'abstract': 'Large language models (LLMs) provide a promising way for accurate session-based recommendation (SBR), but they demand substantial computational time and memory. Knowledge distillation (KD)-based methods can alleviate these issues by transferring the knowledge to a small student, which trains a student based on the predictions of a cumbersome teacher. However, these methods encounter difficulties for \\textit{LLM-based KD in SBR}. 1) It is expensive to make LLMs predict for all instances in KD. 2) LLMs may make ineffective predictions for some instances in KD, e.g., incorrect predictions for hard instances or similar predictions as existing recommenders for easy instances. In this paper, we propose an active LLM-based KD method in SBR, contributing to sustainable AI. To efficiently distill knowledge from LLMs with limited cost, we propose to extract a small proportion of instances predicted by LLMs. Meanwhile, for a more effective distillation, we propose an active learning strategy to extract instances that are as effective as possible for KD from a theoretical view. Specifically, we first formulate gains based on potential effects (e.g., effective, similar, and incorrect predictions by LLMs) and difficulties (e.g., easy or hard to fit) of instances for KD. Then, we propose to maximize the minimal gains of distillation to find the optimal selection policy for active learning, which can largely avoid extracting ineffective instances in KD. Experiments on real-world datasets show that our method significantly outperforms state-of-the-art methods for SBR.', 'abstract_zh': '大规模语言模型（LLMs）为基于会话的推荐（SBR）提供了准确的方法，但它们需要大量的计算时间和内存。基于知识蒸馏（KD）的方法可以通过将知识转移到一个较小的学生模型中来缓解这些问题，这种方法是基于教师模型的预测来训练较小的学生模型。然而，这些方法在LBMK（基于LLM的KD）方面遇到了困难。1）在KD中制作LLM为所有实例进行预测是成本高昂的。2）在KD中，LLM可能会对某些实例做出无效的预测，例如对困难实例做出错误预测，或者对简单实例做出与现有推荐器相似的预测。在本文中，我们提出了一种新的基于LLM的主动KD方法，有助于可持续的人工智能。为了以较低的成本有效提取LLM的预测实例，我们提出仅提取一部分由LLM预测的实例。同时，为了更有效地进行知识蒸馏，我们提出了一种主动学习策略，从理论角度来看，可以提取出对KD最有用的实例。具体而言，我们首先基于实例潜在效果（例如，有效、相似和错误预测）和难度（例如，容易或难以适应）来制定收益指标。然后，我们提出最大化蒸馏过程中的最小收益，以找到合适的主动学习选择策略，这可以大大避免在KD中提取无效实例。实验结果表明，我们的方法在基于会话的推荐方面显著优于现有最先进的方法。', 'title_zh': '基于活跃大型语言模型的知识蒸馏用于会话推荐'}
