{'arxiv_id': 'arXiv:2509.24869', 'title': 'Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval', 'authors': 'Junwei Lan, Jianlyu Chen, Zheng Liu, Chaofan Li, Siqi Bao, Defu Lian', 'link': 'https://arxiv.org/abs/2509.24869', 'abstract': "With the growing popularity of LLM agents and RAG, it has become increasingly important to retrieve documents that are essential for solving a task, even when their connection to the task is indirect or implicit. Addressing this problem requires fine-grained reasoning to accurately assess the relevance between the task and each candidate document. This capability, however, poses a significant challenge for existing IR techniques. Despite recent progress in reasoning-enhanced IR, existing approaches still face significant challenges in applicability, scalability, and efficiency. In this work, we propose Retro*, a novel approach for reasoning-intensive document retrieval. Our method introduces a rubric-based relevance scoring mechanism, enabling the model to reason about the relationship between a task and a document based on explicitly defined criteria, whereby producing a fine-grained, interpretable relevance score. Retro* also supports test-time scaling by combining multiple reasoning trajectories via score integration, which produces more reliable relevance estimates. To optimize Retro*'s reasoning capabilities, we introduce a novel reinforcement learning algorithm tailored for its relevance scoring mechanism, which employs two composite rewards to fully exploit the trajectories of each training sample. Our experiments show that Retro* outperforms existing document retrieval methods with notable advantages, leading to state-of-the-art performance on the BRIGHT benchmark."}
{'arxiv_id': 'arXiv:2509.24632', 'title': 'UniDex: Rethinking Search Inverted Indexing with Unified Semantic Modeling', 'authors': 'Zan Li, Jiahui Chen, Yuan Chai, Xiaoze Jiang, Xiaohua Qi, Zhiheng Qin, Runbin Zhou, Shun Zuo, Guangchao Hao, Kefeng Wang, Jingshan Lv, Yupeng Huang, Xiao Liang, Han Li', 'link': 'https://arxiv.org/abs/2509.24632', 'abstract': "Inverted indexing has traditionally been a cornerstone of modern search systems, leveraging exact term matches to determine relevance between queries and documents. However, this term-based approach often emphasizes surface-level token overlap, limiting the system's generalization capabilities and retrieval effectiveness. To address these challenges, we propose UniDex, a novel model-based method that employs unified semantic modeling to revolutionize inverted indexing. UniDex replaces complex manual designs with a streamlined architecture, enhancing semantic generalization while reducing maintenance overhead. Our approach involves two key components: UniTouch, which maps queries and documents into semantic IDs for improved retrieval, and UniRank, which employs semantic matching to rank results effectively. Through large-scale industrial datasets and real-world online traffic assessments, we demonstrate that UniDex significantly improves retrieval capabilities, marking a paradigm shift from term-based to model-based indexing. Our deployment within Kuaishou's short-video search systems further validates UniDex's practical effectiveness, serving hundreds of millions of active users efficiently."}
{'arxiv_id': 'arXiv:2509.24424', 'title': 'Multi-Item-Query Attention for Stable Sequential Recommendation', 'authors': 'Mingshi Xu, Haoren Zhu, Wilfred Siu Hung Ng', 'link': 'https://arxiv.org/abs/2509.24424', 'abstract': 'The inherent instability and noise in user interaction data challenge sequential recommendation systems. Prevailing masked attention models, relying on a single query from the most recent item, are sensitive to this noise, reducing prediction reliability. We propose the Multi-Item-Query attention mechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn constructs multiple diverse query vectors from user interactions, effectively mitigating noise and improving consistency. It is designed for easy adoption as a drop-in replacement for existing single-query attention. Experiments show MIQ-Attn significantly improves performance on benchmark datasets.'}
{'arxiv_id': 'arXiv:2509.23874', 'title': 'Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification', 'authors': 'Huike Zou, Haiyang Yang, Yindu Su, Liyu Chen, Chengbao Lian, Qingheng Zhang, Shuguang Han, Jufeng Chen', 'link': 'https://arxiv.org/abs/2509.23874', 'abstract': 'Identifying attribute values from product profiles is a key task for improving product search, recommendation, and business analytics on e-commerce platforms, which we called Product Attribute Value Identification (PAVI) . However, existing PAVI methods face critical challenges, such as cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability. To address these limitations, we introduce Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the strengths of retrieval, generation, and classification paradigms. MVP-RAG defines PAVI as a retrieval-generation task, where the product title description serves as the query, and products and attribute values act as the corpus. It first retrieves similar products of the same category and candidate attribute values, and then generates the standardized attribute values. The key advantages of this work are: (1) the proposal of a multi-level retrieval scheme, with products and attribute values as distinct hierarchical levels in PAVI domain (2) attribute value generation of large language model to significantly alleviate the OOD problem and (3) its successful deployment in a real-world industrial environment. Extensive experimental results demonstrate that MVP-RAG performs better than the state-of-the-art baselines.'}
{'arxiv_id': 'arXiv:2509.23861', 'title': 'Investigating Multi-layer Representations for Dense Passage Retrieval', 'authors': 'Zhongbin Xie, Thomas Lukasiewicz', 'link': 'https://arxiv.org/abs/2509.23861', 'abstract': "Dense retrieval models usually adopt vectors from the last hidden layer of the document encoder to represent a document, which is in contrast to the fact that representations in different layers of a pre-trained language model usually contain different kinds of linguistic knowledge, and behave differently during fine-tuning. Therefore, we propose to investigate utilizing representations from multiple encoder layers to make up the representation of a document, which we denote Multi-layer Representations (MLR). We first investigate how representations in different layers affect MLR's performance under the multi-vector retrieval setting, and then propose to leverage pooling strategies to reduce multi-vector models to single-vector ones to improve retrieval efficiency. Experiments demonstrate the effectiveness of MLR over dual encoder, ME-BERT and ColBERT in the single-vector retrieval setting, as well as demonstrate that it works well with other advanced training techniques such as retrieval-oriented pre-training and hard negative mining."}
{'arxiv_id': 'arXiv:2509.23860', 'title': 'GSID: Generative Semantic Indexing for E-Commerce Product Understanding', 'authors': 'Haiyang Yang, Qinye Xie, Qingheng Zhang, Liyu Chen, Huike Zou, Chengbao Lian, Shuguang Han, Fei Huang, Jufeng Chen, Bo Zheng', 'link': 'https://arxiv.org/abs/2509.23860', 'abstract': 'Structured representation of product information is a major bottleneck for the efficiency of e-commerce platforms, especially in second-hand ecommerce platforms. Currently, most product information are organized based on manually curated product categories and attributes, which often fail to adequately cover long-tail products and do not align well with buyer preference. To address these problems, we propose \\textbf{G}enerative \\textbf{S}emantic \\textbf{I}n\\textbf{D}exings (GSID), a data-driven approach to generate product structured representations. GSID consists of two key components: (1) Pre-training on unstructured product metadata to learn in-domain semantic embeddings, and (2) Generating more effective semantic codes tailored for downstream product-centric applications. Extensive experiments are conducted to validate the effectiveness of GSID, and it has been successfully deployed on the real-world e-commerce platform, achieving promising results on product understanding and other downstream tasks.'}
{'arxiv_id': 'arXiv:2509.23776', 'title': 'Semantic Representation of Processes with Ontology Design Patterns', 'authors': 'Ebrahim Norouzi, Sven Hertling, JÃ¶rg Waitelonis, Harald Sack', 'link': 'https://arxiv.org/abs/2509.23776', 'abstract': 'The representation of workflows and processes is essential in materials science engineering, where experimental and computational reproducibility depend on structured and semantically coherent process models. Although numerous ontologies have been developed for process modeling, they are often complex and challenging to reuse. Ontology Design Patterns (ODPs) offer modular and reusable modeling solutions to recurring problems; however, these patterns are frequently neither explicitly published nor documented in a manner accessible to domain experts. This study surveys ontologies relevant to scientific workflows and engineering process modeling and identifies implicit design patterns embedded within their structures. We evaluate the capacity of these ontologies to fulfill key requirements for process representation in materials science. Furthermore, we propose a baseline method for the automatic extraction of design patterns from existing ontologies and assess the approach against curated ground truth patterns. All resources associated with this work, including the extracted patterns and the extraction workflow, are made openly available in a public GitHub repository.'}
{'arxiv_id': 'arXiv:2509.23771', 'title': 'Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal', 'authors': 'Ana Llorens, Alvaro Torrente', 'link': 'https://arxiv.org/abs/2509.23771', 'abstract': "The exceptional reception of Pietro Metastasio's works during the eighteenth century, all over Europe and in the Iberian Peninsula in particular, is well documented. Due to that unparalleled success, it is possible to ascertain Spain and Portugal's participation in international, contemporary tastes and artistic webs, applicable to both composers and performers. However, this internationalisation needs to be nuanced, as some characteristics of the repertoire specifically written for the Peninsula indicate that their court audiences may have had expectations, both social and strictly musical, different from those of the public in opera theatres elsewhere in the continent. In this light, this article investigates in what ways the style of five composers in the international scene - Perez, Galuppi, Jommelli, Conforto, and Corselli - varied when commissioned to write opera seria for the Iberian courts. The statistical analysis of fifteen settings especially written for the court theatres in Madrid and Lisbon, in comparison to the average data extracted from a corpus of 2,404 arias from 126 versions of a select number of Metastasian librettos, allows us to evaluate some particular usages regarding key, metre, tempo, and treatment of the vocal part. In this manner, through quantitative analysis, this article places eighteenth-century Iberian music production and consumption in the context of European opera seria, while ultimately suggesting that its unique musical characteristics were also partly dependent on local musical customs, gender stereotypes, and personal idiosyncrasies alike."}
{'arxiv_id': 'arXiv:2509.23649', 'title': 'From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation', 'authors': 'KaiWen Wei, Kejun He, Xiaomian Kang, Jie Zhang, Yuming Yang, Jiang Zhong, He Bai, Junnan Zhu', 'link': 'https://arxiv.org/abs/2509.23649', 'abstract': "Generative recommendation, which directly generates item identifiers, has emerged as a promising paradigm for recommendation systems. However, its potential is fundamentally constrained by the reliance on purely autoregressive training. This approach focuses solely on predicting the next item while ignoring the rich internal structure of a user's interaction history, thus failing to grasp the underlying intent. To address this limitation, we propose Masked History Learning (MHL), a novel training framework that shifts the objective from simple next-step prediction to deep comprehension of history. MHL augments the standard autoregressive objective with an auxiliary task of reconstructing masked historical items, compelling the model to understand ``why'' an item path is formed from the user's past behaviors, rather than just ``what'' item comes next. We introduce two key contributions to enhance this framework: (1) an entropy-guided masking policy that intelligently targets the most informative historical items for reconstruction, and (2) a curriculum learning scheduler that progressively transitions from history reconstruction to future prediction. Experiments on three public datasets show that our method significantly outperforms state-of-the-art generative models, highlighting that a comprehensive understanding of the past is crucial for accurately predicting a user's future path. The code will be released to the public."}
{'arxiv_id': 'arXiv:2509.23175', 'title': 'WARBERT: A Hierarchical BERT-based Model for Web API Recommendation', 'authors': 'Zishuo Xu, Yuhong Gu, Dezhong Yao', 'link': 'https://arxiv.org/abs/2509.23175', 'abstract': 'With the emergence of Web 2.0 and microservices architecture, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Existing solutions typically fall into two categories: recommendation-type methods, which treat each API as a label for classification, and match-type methods, which focus on matching mashups through API retrieval. However, three critical challenges persist: 1) the semantic ambiguities in comparing API and mashup descriptions, 2) the lack of detailed comparisons between the individual API and the mashup in recommendation-type methods, and 3) time inefficiencies for API retrieval in match-type methods. To address these challenges, we propose WARBERT, a hierarchical BERT-based model for Web API recommendation. WARBERT leverages dual-component feature fusion and attention comparison to extract precise semantic representations of API and mashup descriptions. WARBERT consists of two main components: WARBERT(R) for Recommendation and WARBERT(M) for Matching. Specifically, WAR-BERT(R) serves as an initial filter, narrowing down the candidate APIs, while WARBERT(M) refines the matching process by calculating the similarity between candidate APIs and mashup. The final likelihood of a mashup being matched with an API is determined by combining the predictions from WARBERT(R) and WARBERT(M). Additionally, WARBERT(R) incorporates an auxiliary task of mashup category judgment, which enhances its effectiveness in candidate selection. Experimental results on the ProgrammableWeb dataset demonstrate that WARBERT outperforms most existing solutions and achieves improvements of up to 11.7% compared to the model MTFM (Multi-Task Fusion Model), delivering significant enhancements in accuracy and effiency.'}
{'arxiv_id': 'arXiv:2509.22807', 'title': 'MTRec: Learning to Align with User Preferences via Mental Reward Models', 'authors': 'Mengchen Zhao, Yifan Gao, Yaqing Hou, Xiangyang Li, Pengjie Gu, Zhenhua Dong, Ruiming Tang, Yi Cai', 'link': 'https://arxiv.org/abs/2509.22807', 'abstract': "Recommendation models are predominantly trained using implicit user feedback, since explicit feedback is often costly to obtain. However, implicit feedback, such as clicks, does not always reflect users' real preferences. For example, a user might click on a news article because of its attractive headline, but end up feeling uncomfortable after reading the content. In the absence of explicit feedback, such erroneous implicit signals may severely mislead recommender systems. In this paper, we propose MTRec, a novel sequential recommendation framework designed to align with real user preferences by uncovering their internal satisfaction on recommended items. Specifically, we introduce a mental reward model to quantify user satisfaction and propose a distributional inverse reinforcement learning approach to learn it. The learned mental reward model is then used to guide recommendation models to better align with users' real preferences. Our experiments show that MTRec brings significant improvements to a variety of recommendation models. We also deploy MTRec on an industrial short video platform and observe a 7 percent increase in average user viewing time."}
{'arxiv_id': 'arXiv:2509.22661', 'title': 'Next Point-of-interest (POI) Recommendation Model Based on Multi-modal Spatio-temporal Context Feature Embedding', 'authors': 'Lingyu Zhang, Guobin Wu, Yan Wang, Pengfei Xu, Jian Liang, Xuan Song, Yunhai Wang', 'link': 'https://arxiv.org/abs/2509.22661', 'abstract': "The next Point-of-interest (POI) recommendation is mainly based on sequential traffic information to predict the user's next boarding point location. This is a highly regarded and widely applied research task in the field of intelligent transportation, and there have been many research results to date. Traditional POI prediction models primarily rely on short-term traffic sequence information, often neglecting both long-term and short-term preference data, as well as crucial spatiotemporal context features in user behavior. To address this issue, this paper introduces user long-term preference information and key spatiotemporal context information, and proposes a POI recommendation model based on multimodal spatiotemporal context feature embedding. The model extracts long-term preference features and key spatiotemporal context features from traffic data through modules such as spatiotemporal feature processing, multimodal embedding, and self-attention aggregation. It then uses a weighted fusion method to dynamically adjust the weights of long-term and short-term features based on users' historical behavior patterns and the current context. Finally, the fused features are matched using attention, and the probability of each location candidate becoming the next location is calculated. This paper conducts experimental verification on multiple transportation datasets, and the results show that the POI prediction model combining multiple types of features has higher prediction accuracy than existing SOTA models and methods."}
{'arxiv_id': 'arXiv:2509.22660', 'title': 'Fairness for niche users and providers: algorithmic choice and profile portability', 'authors': 'Elizabeth McKinnie, Anas Buhayh, Clement Canel, Robin Burke', 'link': 'https://arxiv.org/abs/2509.22660', 'abstract': 'Ensuring fair outcomes for multiple stakeholders in recommender systems has been studied mostly in terms of algorithmic interventions: building new models with better fairness properties, or using reranking to improve outcomes from an existing algorithm. What has rarely been studied is structural changes in the recommendation ecosystem itself. Our work explores the fairness impact of algorithmic pluralism, the idea that the recommendation algorithm is decoupled from the platform through which users access content, enabling user choice in algorithms. Prior work using a simulation approach has shown that niche consumers and (especially) niche providers benefit from algorithmic choice. In this paper, we use simulation to explore the question of profile portability, to understand how different policies regarding the handling of user profiles interact with fairness outcomes for consumers and providers.'}
{'arxiv_id': 'arXiv:2509.22659', 'title': 'Federated Consistency- and Complementarity-aware Consensus-enhanced Recommendation', 'authors': 'Yunqi Mi, Boyang Yan, Guoshuai Zhao, Jialie Shen, Xueming Qian', 'link': 'https://arxiv.org/abs/2509.22659', 'abstract': "Personalized federated recommendation system (FedRec) has gained significant attention for its ability to preserve privacy in delivering tailored recommendations. To alleviate the statistical heterogeneity challenges among clients and improve personalization, decoupling item embeddings into the server and client-specific views has become a promising way. Among them, the global item embedding table serves as a consensus representation that integrates and reflects the collective patterns across all clients. However, the inherent sparsity and high uniformity of interaction data from massive-scale clients results in degraded consensus and insufficient decoupling, reducing consensus's utility. To this end, we propose a \\textbf{Fed}erated \\textbf{C}onsistency- and \\textbf{C}omplementarity-aware \\textbf{C}onsensus-enhanced \\textbf{R}ecommendation (Fed3CR) method for personalized FedRec. To improve the efficiency of the utilization of consensus, we propose an \\textbf{A}daptive \\textbf{C}onsensus \\textbf{E}nhancement (ACE) strategy to learn the relationship between global and client-specific item embeddings. It enables the client to adaptively enhance specific information in the consensus, transforming it into a form that best suits itself. To improve the quality of decoupling, we propose a \\textbf{C}onsistency- and \\textbf{C}omplementarity-aware \\textbf{O}ptimization (C2O) strategy, which helps to learn more effective and complementary representations. Notably, our proposed Fed3CR is a plug-and-play method, which can be integrated with other FedRec methods to improve its performance. Extensive experiments on four real-world datasets represent the superior performance of Fed3CR."}
{'arxiv_id': 'arXiv:2509.22658', 'title': 'How good are LLMs at Retrieving Documents in a Specific Domain?', 'authors': 'Nafis Tanveer Islam, Zhiming Zhao', 'link': 'https://arxiv.org/abs/2509.22658', 'abstract': "Classical search engines using indexing methods in data infrastructures primarily allow keyword-based queries to retrieve content. While these indexing-based methods are highly scalable and efficient, due to a lack of an appropriate evaluation dataset and a limited understanding of semantics, they often fail to capture the user's intent and generate incomplete responses during evaluation. This problem also extends to domain-specific search systems that utilize a Knowledge Base (KB) to access data from various research infrastructures. Research infrastructures (RIs) from the environmental and earth science domain, which encompass the study of ecosystems, biodiversity, oceanography, and climate change, generate, share, and reuse large volumes of data. While there are attempts to provide a centralized search service using Elasticsearch as a knowledge base, they also face similar challenges in understanding queries with multiple intents. To address these challenges, we proposed an automated method to curate a domain-specific evaluation dataset to analyze the capability of a search system. Furthermore, we incorporate the Retrieval of Augmented Generation (RAG), powered by Large Language Models (LLMs), for high-quality retrieval of environmental domain data using natural language queries. Our quantitative and qualitative analysis of the evaluation dataset shows that LLM-based systems for information retrieval return results with higher precision when understanding queries with multiple intents, compared to Elasticsearch-based systems."}
{'arxiv_id': 'arXiv:2509.25106', 'title': 'Towards Personalized Deep Research: Benchmarks and Evaluations', 'authors': 'Yuan Liang, Jiaxian Li, Yuqing Wang, Piaohong Wang, Motong Tian, Pai Liu, Shuofei Qiao, Runnan Fang, He Zhu, Ge Zhang, Minghao Liu, Yuchen Eleanor Jiang, Ningyu Zhang, Wangchunshu Zhou', 'link': 'https://arxiv.org/abs/2509.25106', 'abstract': 'Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.'}
{'arxiv_id': 'arXiv:2509.25085', 'title': 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking', 'authors': 'Feng Wang, Yuqing Li, Han Xiao', 'link': 'https://arxiv.org/abs/2509.25085', 'abstract': 'jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers.'}
{'arxiv_id': 'arXiv:2509.25084', 'title': 'Scaling Generalist Data-Analytic Agents', 'authors': 'Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen', 'link': 'https://arxiv.org/abs/2509.25084', 'abstract': "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research."}
{'arxiv_id': 'arXiv:2509.24815', 'title': 'Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse Vector Sets', 'authors': 'Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini', 'link': 'https://arxiv.org/abs/2509.24815', 'abstract': 'Sparse embeddings of data form an attractive class due to their inherent interpretability: Every dimension is tied to a term in some vocabulary, making it easy to visually decipher the latent space. Sparsity, however, poses unique challenges for Approximate Nearest Neighbor Search (ANNS) which finds, from a collection of vectors, the k vectors closest to a query. To encourage research on this underexplored topic, sparse ANNS featured prominently in a BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on large benchmark datasets by throughput and accuracy. In this work, we introduce a set of novel data structures and algorithmic methods, a combination of which leads to an elegant, effective, and highly efficient solution to sparse ANNS. Our contributions range from a theoretically-grounded sketching algorithm for sparse vectors to reduce their effective dimensionality while preserving inner product-induced ranks; a geometric organization of the inverted index; and the blending of local and global information to improve the efficiency and efficacy of ANNS. Empirically, our final algorithm, dubbed Seismic, reaches sub-millisecond per-query latency with high accuracy on a large-scale benchmark dataset using a single CPU.'}
{'arxiv_id': 'arXiv:2509.24405', 'title': 'Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents', 'authors': 'Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen', 'link': 'https://arxiv.org/abs/2509.24405', 'abstract': "Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at this https URL."}
{'arxiv_id': 'arXiv:2509.24193', 'title': 'AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play', 'authors': 'Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang', 'link': 'https://arxiv.org/abs/2509.24193', 'abstract': 'Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at this https URL and this https URL.'}
{'arxiv_id': 'arXiv:2509.23883', 'title': 'DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning', 'authors': 'Yibo Yan, Guangwei Xu, Xin Zou, Shuliang Liu, James Kwok, Xuming Hu', 'link': 'https://arxiv.org/abs/2509.23883', 'abstract': 'Visual Document Retrieval (VDR), the task of retrieving visually-rich document pages using queries that combine visual and textual cues, is crucial for numerous real-world applications. Recent state-of-the-art methods leverage Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing each document as patch-level embeddings to capture fine-grained details. While highly effective, this approach introduces a critical challenge: prohibitive storage overhead, as storing hundreds of vectors per page makes large-scale deployment costly and impractical. To address this, we introduce DocPruner, the first framework to employ adaptive patch-level embedding pruning for VDR to effectively reduce the storage overhead. DocPruner leverages the intra-document patch attention distribution to dynamically identify and discard redundant embeddings for each document. This adaptive mechanism enables a significant 50-60% reduction in storage for leading multi-vector VDR models with negligible degradation in document retrieval performance. Extensive experiments across more than ten representative datasets validate that DocPruner offers a robust, flexible, and effective solution for building storage-efficient, large-scale VDR systems.'}
{'arxiv_id': 'arXiv:2509.23742', 'title': 'GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data', 'authors': 'Yewang Chen, Junfeng Li, Shuyin Xia, Qinghong Lai, Xinbo Gao, Guoyin Wang, Dongdong Cheng, Yi Liu, Yi Wang', 'link': 'https://arxiv.org/abs/2509.23742', 'abstract': 'To effectively handle clustering task for large-scale datasets, we propose a novel scalable skeleton clustering algorithm, namely GBSK, which leverages the granular-ball technique to capture the underlying structure of data. By multi-sampling the dataset and constructing multi-grained granular-balls, GBSK progressively uncovers a statistical "skeleton" -- a spatial abstraction that approximates the essential structure and distribution of the original data. This strategy enables GBSK to dramatically reduce computational overhead while maintaining high clustering accuracy. In addition, we introduce an adaptive version, AGBSK, with simplified parameter settings to enhance usability and facilitate deployment in real-world scenarios. Extensive experiments conducted on standard computing hardware demonstrate that GBSK achieves high efficiency and strong clustering performance on large-scale datasets, including one with up to 100 million instances across 256 dimensions. Our implementation and experimental results are available at: this https URL.'}
{'arxiv_id': 'arXiv:2509.23577', 'title': 'ML-Asset Management: Curation, Discovery, and Utilization', 'authors': 'Mengying Wang, Moming Duan, Yicong Huang, Chen Li, Bingsheng He, Yinghui Wu', 'link': 'https://arxiv.org/abs/2509.23577', 'abstract': 'Machine learning (ML) assets, such as models, datasets, and metadata, are central to modern ML workflows. Despite their explosive growth in practice, these assets are often underutilized due to fragmented documentation, siloed storage, inconsistent licensing, and lack of unified discovery mechanisms, making ML-asset management an urgent challenge. This tutorial offers a comprehensive overview of ML-asset management activities across its lifecycle, including curation, discovery, and utilization. We provide a categorization of ML assets, and major management issues, survey state-of-the-art techniques, and identify emerging opportunities at each stage. We further highlight system-level challenges related to scalability, lineage, and unified indexing. Through live demonstrations of systems, this tutorial equips both researchers and practitioners with actionable insights and practical tools for advancing ML-asset management in real-world and domain-specific settings.'}
{'arxiv_id': 'arXiv:2509.23471', 'title': 'Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases', 'authors': 'Harshil Vejendla', 'link': 'https://arxiv.org/abs/2509.23471', 'abstract': "Upgrading embedding models in production vector databases typically requires re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor (ANN) index, leading to significant operational disruption and computational cost. This paper presents Drift-Adapter, a lightweight, learnable transformation layer designed to bridge embedding spaces between model versions. By mapping new queries into the legacy embedding space, Drift-Adapter enables the continued use of the existing ANN index, effectively deferring full re-computation. We systematically evaluate three adapter parameterizations: Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on a small sample of paired old and new embeddings. Experiments on MTEB text corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter recovers 95-99% of the retrieval recall (Recall@10, MRR) of a full re-embedding, adding less than 10 microseconds of query latency. Compared to operational strategies like full re-indexing or dual-index serving, Drift-Adapter reduces recompute costs by over 100 times and facilitates upgrades with near-zero operational interruption. We analyze robustness to varied model drift, training data size, scalability to billion-item systems, and the impact of design choices like diagonal scaling, demonstrating Drift-Adapter's viability as a pragmatic solution for agile model deployment."}
{'arxiv_id': 'arXiv:2509.23338', 'title': 'PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation', 'authors': 'Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou', 'link': 'https://arxiv.org/abs/2509.23338', 'abstract': 'Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: this https URL.'}
{'arxiv_id': 'arXiv:2509.22991', 'title': 'ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning', 'authors': 'Jasin Cekinmez, Omid Ghahroodi, Saad Fowad Chandle, Dhiman Gupta, Ehsaneddin Asgari', 'link': 'https://arxiv.org/abs/2509.22991', 'abstract': "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating and improving multimodal large language models (MLLMs) in biographical reasoning. To the best of our knowledge, this is the first work to systematically examine LLM capabilities in biography, a critical yet underexplored dimension of factual knowledge. At its core, AdamDB is a multilingual and multimodal dataset covering over 4 million individuals across geography, time, and profession, while AdamBench provides cognitively structured evaluations based on Bloom's taxonomy, spanning six reasoning levels in both English and native languages. To address hallucinations, particularly for lesser-known individuals, we propose AdamRAG, a retrieval-augmented generation system tailored to biographical contexts. Experiments show that AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with the largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller, less consistent improvements than retrieval. ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing the development of multilingual, accurate, and hallucination-resistant MLLMs."}
{'arxiv_id': 'arXiv:2509.22845', 'title': 'Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems', 'authors': 'Kai Hua, Zhiyuan Feng, Chongyang Tao, Rui Yan, Lu Zhang', 'link': 'https://arxiv.org/abs/2509.22845', 'abstract': 'Recently, knowledge-grounded conversations in the open domain gain great attention from researchers. Existing works on retrieval-based dialogue systems have paid tremendous efforts to utilize neural networks to build a matching model, where all of the context and knowledge contents are used to match the response candidate with various representation methods. Actually, different parts of the context and knowledge are differentially important for recognizing the proper response candidate, as many utterances are useless due to the topic shift. Those excessive useless information in the context and knowledge can influence the matching process and leads to inferior performance. To address this problem, we propose a multi-turn \\textbf{R}esponse \\textbf{S}election \\textbf{M}odel that can \\textbf{D}etect the relevant parts of the \\textbf{C}ontext and \\textbf{K}nowledge collection (\\textbf{RSM-DCK}). Our model first uses the recent context as a query to pre-select relevant parts of the context and knowledge collection at the word-level and utterance-level semantics. Further, the response candidate interacts with the selected context and knowledge collection respectively. In the end, The fused representation of the context and response candidate is utilized to post-select the relevant parts of the knowledge collection more confidently for matching. We test our proposed model on two benchmark datasets. Evaluation results indicate that our model achieves better performance than the existing methods, and can effectively detect the relevant context and knowledge for response selection.'}
