# Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation 

**Authors**: Krisztian Balog, Donald Metzler, Zhen Qin  

**Link**: [PDF](https://arxiv.org/pdf/2503.19092)  

**Abstract**: Large language models (LLMs) are increasingly integral to information retrieval (IR), powering ranking, evaluation, and AI-assisted content creation. This widespread adoption necessitates a critical examination of potential biases arising from the interplay between these LLM-based components. This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges. We provide the first empirical evidence of LLM judges exhibiting significant bias towards LLM-based rankers. Furthermore, we observe limitations in LLM judges' ability to discern subtle system performance differences. Contrary to some previous findings, our preliminary study does not find evidence of bias against AI-generated content. These results highlight the need for a more holistic view of the LLM-driven information ecosystem. To this end, we offer initial guidelines and a research agenda to ensure the reliable use of LLMs in IR evaluation. 

---
# The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas 

**Authors**: Giovanni Franco Gabriel Marraffini, Andr√©s Cotton, Noe Fabian Hsueh, Axel Fridman, Juan Wisznia, Luciano Del Corro  

**Link**: [PDF](https://arxiv.org/pdf/2503.19598)  

**Abstract**: The question of how to make decisions that maximise the well-being of all persons is very relevant to design language models that are beneficial to humanity and free from harm. We introduce the Greatest Good Benchmark to evaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis across 15 diverse LLMs reveals consistently encoded moral preferences that diverge from established moral theories and lay population moral standards. Most LLMs have a marked preference for impartial beneficence and rejection of instrumental harm. These findings showcase the 'artificial moral compass' of LLMs, offering insights into their moral alignment. 

---
