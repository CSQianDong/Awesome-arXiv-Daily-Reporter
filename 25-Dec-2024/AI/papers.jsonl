{'arxiv_id': 'arXiv:2412.18464', 'title': 'MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation', 'authors': 'Tengfei He, Xiao Zhou', 'link': 'https://arxiv.org/abs/2412.18464', 'abstract': 'Social segregation in cities, spanning racial, residential, and income dimensions, is becoming more diverse and severe. As urban spaces and social relations grow more complex, residents in metropolitan areas experience varying levels of social segregation. If left unaddressed, this could lead to increased crime rates, heightened social tensions, and other serious issues. Effectively quantifying and analyzing the structures within urban spaces and resident interactions is crucial for addressing segregation. Previous studies have mainly focused on surface-level indicators of urban segregation, lacking comprehensive analyses of urban structure and mobility. This limitation fails to capture the full complexity of segregation. To address this gap, we propose a framework named Motif-Enhanced Graph Prototype Learning (MotifGPL),which consists of three key modules: prototype-based graph structure extraction, motif distribution discovery, and urban graph structure reconstruction. Specifically, we use graph structure prototype learning to extract key prototypes from both the urban spatial graph and the origin-destination graph, incorporating key urban attributes such as points of interest, street view images, and flow indices. To enhance interpretability, the motif distribution discovery module matches each prototype with similar motifs, representing simpler graph structures reflecting local patterns. Finally, we use the motif distribution results to guide the reconstruction of the two graphs. This model enables a detailed exploration of urban spatial structures and resident mobility patterns, helping identify and analyze motif patterns that influence urban segregation, guiding the reconstruction of urban graph structures. Experimental results demonstrate that MotifGPL effectively reveals the key motifs affecting urban social segregation and offer robust guidance for mitigating this issue.', 'abstract_zh': '城市中的社会隔离现象，涉及种族、居住地和收入等多个维度，正变得越来越多样化和严重。随着城市空间和社会关系日趋复杂，都会区居民所经历的社会隔离程度亦有所不同。若不加以解决，这可能会导致犯罪率上升、社会紧张局势加剧以及其他严重问题。有效地量化和分析城市空间结构和居民互动对于解决社会隔离问题至关重要。此前的研究主要集中在城乡隔离的表面指标上，缺乏对城市结构和流动性进行全面分析的能力。这种局限性无法完全捕捉隔离的复杂性。为弥补这一不足，我们提出了一种名为Motif-Enhanced Graph Prototype Learning (MotifGPL) 的框架，该框架包含三个关键模块：基于原型的图结构提取、图基元分布发现和城市图结构重构。具体来说，我们使用图结构原型学习从城市空间图和起讫点图中提取关键原型，结合兴趣点、街景图像和流量指标等关键城市属性。为了提高可解释性，图基元分布发现模块将每个原型与相似的基元匹配，这些基元代表反映局部模式的简单图结构。最后，利用基元分布结果指导两个图的重构。该模型能够详细探索城市空间结构和居民流动性模式，帮助识别和分析影响城市社会隔离的基元模式，并指导城市图结构的重构。实验结果表明，MotifGPL 能够有效地揭示影响城市社会隔离的关键基元，并提供强大的指导，以减轻这一问题。', 'title_zh': 'MotifGPL：基于动机的图原型学习方法 deciphering 城市社会隔离现象\n\n注释：这里的“Deciphering”在原英文标题中可能是作为动词使用的，意为“解释”或者“解读”。翻译时根据上下文做了调整，使其更符合中文的表达习惯。如果原文“Deciphering”是作为名词使用的，则可以调整为“动机增强的图原型学习方法，用于解释城市社会隔离现象”。'}
{'arxiv_id': 'arXiv:2412.18454', 'title': 'Multi-Agent Norm Perception and Induction in Distributed Healthcare', 'authors': 'Chao Li, Olga Petruchik, Elizaveta Grishanina, Sergey Kovalchuk', 'link': 'https://arxiv.org/abs/2412.18454', 'abstract': 'This paper presents a Multi-Agent Norm Perception and Induction Learning Model aimed at facilitating the integration of autonomous agent systems into distributed healthcare environments through dynamic interaction processes. The nature of the medical norm system and its sharing channels necessitates distinct approaches for Multi-Agent Systems to learn two types of norms. Building on this foundation, the model enables agents to simultaneously learn descriptive norms, which capture collective tendencies, and prescriptive norms, which dictate ideal behaviors. Through parameterized mixed probability density models and practice-enhanced Markov games, the multi-agent system perceives descriptive norms in dynamic interactions and captures emergent prescriptive norms. We conducted experiments using a dataset from a neurological medical center spanning from 2016 to 2020.', 'abstract_zh': '本文提出了一个面向分布式医疗环境自主代理系统整合的多Agent规范感知与归纳学习模型。医疗规范系统及其共享渠道的本性要求多Agent系统采用不同的方法来学习两类规范。在此基础上，该模型使智能体能够同时学习描述性规范（捕捉集体倾向）和指令性规范（规定理想行为）。通过参数化混合概率密度模型和实践增强的马尔可夫游戏，多Agent系统在动态交互过程中感知描述性规范并捕获新兴的指令性规范。我们使用2016年至2020年间神经医学中心的数据集进行了实验。', 'title_zh': '多智能体规范感知与诱导在分布式医疗中的应用'}
{'arxiv_id': 'arXiv:2412.18428', 'title': 'Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent', 'authors': 'Farhad Nooralahzadeh, Yi Zhang, Jonathan Furst, Kurt Stockinger', 'link': 'https://arxiv.org/abs/2412.18428', 'abstract': 'International enterprises, organizations, or hospitals collect large amounts of multi-modal data stored in databases, text documents, images, and videos. While there has been recent progress in the separate fields of multi-modal data exploration as well as in database systems that automatically translate natural language questions to database query languages, the research challenge of querying database systems combined with other unstructured modalities such as images in natural language is widely unexplored.\nIn this paper, we propose XMODE - a system that enables explainable, multi-modal data exploration in natural language. Our approach is based on the following research contributions: (1) Our system is inspired by a real-world use case that enables users to explore multi-modal information systems. (2) XMODE leverages a LLM-based agentic AI framework to decompose a natural language question into subtasks such as text-to-SQL generation and image analysis. (3) Experimental results on multi-modal datasets over relational data and images demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling not only in accuracy but also in various performance metrics such as query latency, API costs, planning efficiency, and explanation quality, thanks to the more effective utilization of the reasoning capabilities of LLMs.', 'abstract_zh': '国际企业、组织或医院收集了大量的多模态数据，这些数据存储在数据库、文本文档、图像和视频中。虽然在多模态数据探索以及自然语言问题自动转换为数据库查询语言的数据库系统方面已经有了近期的发展，但在结合其他非结构化模态（如图像）、通过自然语言查询数据库系统的研究挑战上，现有研究尚未得到广泛探索。\n\n本文中，我们提出了XMODE——一种基于自然语言实现可解释性多模态数据探索的系统。我们的方法包括以下研究贡献：(1) 我们的设计灵感来源于一个实际应用场景，旨在帮助用户探索多模态信息系统。(2) XMODE 利用基于大规模语言模型（LLM）的代理式AI框架，将自然语言问题分解成子任务，包括自然语言文本到SQL语句的生成以及图像分析。(3) 实验结果表明，XMODE 在多模态数据集（包括关系数据和图像）上的表现优于现有最先进的多模态探索系统。我们不仅在准确性方面表现出色，还在查询延迟、API成本、规划效率和解释性质量等多个性能指标上优于现有系统，这是因为XMODE 更有效利用了大规模语言模型的推理能力。', 'title_zh': '通过语言模型代理进行可解释的多模态数据探索（Natural Language 中）'}
{'arxiv_id': 'arXiv:2412.18426', 'title': 'GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent', 'authors': 'Kangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen, Zhi Chen, Tiancheng Zhao, Xiubo Liang, Jianwei Yin', 'link': 'https://arxiv.org/abs/2412.18426', 'abstract': 'Nowadays, research on GUI agents is a hot topic in the AI community. However, current research focuses on GUI task automation, limiting the scope of applications in various GUI scenarios. In this paper, we propose a formalized and comprehensive environment to evaluate the entire process of automated GUI Testing (GTArena), offering a fair, standardized environment for consistent operation of diverse multimodal large language models. We divide the testing process into three key subtasks: test intention generation, test task execution, and GUI defect detection, and construct a benchmark dataset based on these to conduct a comprehensive evaluation. It evaluates the performance of different models using three data types: real mobile applications, mobile applications with artificially injected defects, and synthetic data, thoroughly assessing their capabilities in this relevant task. Additionally, we propose a method that helps researchers explore the correlation between the performance of multimodal language large models in specific scenarios and their general capabilities in standard benchmark tests. Experimental results indicate that even the most advanced models struggle to perform well across all sub-tasks of automated GUI Testing, highlighting a significant gap between the current capabilities of Autonomous GUI Testing and its practical, real-world applicability. This gap provides guidance for the future direction of GUI Agent development. Our code is available at this https URL.', 'abstract_zh': '如今，GUI代理的研究是人工智能社区的热点话题。然而，现有的研究主要集中在GUI任务自动化上，这限制了其在各种GUI场景中的应用范围。本文提出了一种形式化和全面的环境（GTArena），用于评估整个自动化GUI测试过程，提供了一个公平和标准化的环境，以确保不同多模态大型语言模型的一致运行。我们将测试过程分为三个关键子任务：测试意图生成、测试任务执行和GUI缺陷检测，并基于这些子任务构建基准数据集进行全面评估。该评估使用三种类型的数据：真实移动应用、人工注入缺陷的移动应用以及合成数据，全面评估其在相关任务中的能力。此外，我们提出了一种方法，帮助研究人员探索多模态语言大模型在特定场景中的性能与其在标准基准测试中的通用能力之间的关系。实验结果表明，即使是最先进的模型，在自动化GUI测试的所有子任务中也难以表现出色，凸显了当前自主GUI测试能力与其实用、现实世界适用性之间的显著差距。这一差距为GUI代理未来的发展方向提供了指导。我们的代码可通过以下链接获取：[这里](this https URL)。', 'title_zh': 'GUI测试竞技场：一种促进自主GUI测试代理发展的统一基准'}
{'arxiv_id': 'arXiv:2412.18424', 'title': 'LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating', 'authors': 'Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, Cheng-Lin Liu', 'link': 'https://arxiv.org/abs/2412.18424', 'abstract': 'Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark, LongDocURL, integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field.', 'abstract_zh': '大型多模态视觉语言模型（LVLMs）显著提高了文档理解能力，能够处理复杂的文档元素、更长的语境范围以及更广泛的任务。然而，现有的文档理解基准仅能够处理少量页面的内容，并且未能提供布局元素定位的全面分析。本文首先定义了三种主要任务类别：长文档理解、数值推理和跨元素定位，并提出一个综合基准LongDocURL，该基准集成了上述三种主要任务，并包含20个基于不同主要任务和答案证据分类的子任务。此外，我们开发了一种半自动化构建管道，收集了2,325个高质量的问答对，覆盖了超过33,000页的文档，显著优于现有基准。随后，我们在26种不同配置的开源和封闭源模型上进行了全面的评估实验，揭示了该领域中的关键性能差距。', 'title_zh': 'LongDocURL：一个综合性的多模态长文档基准，集成理解、推理和定位能力'}
{'arxiv_id': 'arXiv:2412.18419', 'title': 'Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models', 'authors': 'Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen', 'link': 'https://arxiv.org/abs/2412.18419', 'abstract': 'As social changes accelerate, the incidence of psychosomatic disorders has significantly increased, becoming a major challenge in global health issues. This necessitates an innovative knowledge system and analytical methods to aid in diagnosis and treatment. Here, we establish the ontology model and entity types, using the BERT model and LoRA-tuned LLM for named entity recognition, constructing the knowledge graph with 9668 triples. Next, by analyzing the network distances between disease, symptom, and drug modules, it was found that closer network distances among diseases can predict greater similarities in their clinical manifestations, treatment approaches, and psychological mechanisms, and closer distances between symptoms indicate that they are more likely to co-occur. Lastly, by comparing the proximity d and proximity z score, it was shown that symptom-disease pairs in primary diagnostic relationships have a stronger association and are of higher referential value than those in diagnostic relationships. The research results revealed the potential connections between diseases, co-occurring symptoms, and similarities in treatment strategies, providing new perspectives for the diagnosis and treatment of psychosomatic disorders and valuable information for future mental health research and practice.', 'abstract_zh': '随着社会变革的加速，心身疾病的发病率显著增加，成为全球健康问题中的一个重要挑战。这要求建立一种创新的知识体系和分析方法，以辅助诊断和治疗。在此基础上，我们建立了本体模型和实体类型，使用BERT模型和LoRA调优的大语言模型进行命名实体识别，并构建了一个包含9668条三元组的知识图谱。接下来，通过对疾病、症状和药物模块之间的网络距离分析发现，疾病之间的更近距离预测了其临床表现、治疗方法和心理机制的更大相似性；症状之间的更近距离表明它们更有可能共存。最后，通过比较接近度d和接近度z评分，研究结果显示，主要诊断关系中的症状-疾病对的关联性更强且参考价值更高，而诊断关系中的症状-疾病对则不然。研究结果揭示了疾病、共发症状和治疗方法相似性之间的潜在联系，为心身疾病的诊断和治疗提供了新的视角，并为未来的精神健康研究和实践提供了宝贵信息。', 'title_zh': '大型语言模型提取的心理躯体疾病知识图谱模块的临近关系研究'}
{'arxiv_id': 'arXiv:2412.18408', 'title': 'Exploring Flexible Scenario Generation in Godot Simulator', 'authors': 'Daniel Peraltai, Xin Qin', 'link': 'https://arxiv.org/abs/2412.18408', 'abstract': 'Cyber-physical systems (CPS) combine cyber and physical components engineered to make decisions and interact within dynamic environments. Ensuring the safety of CPS is of great importance, requiring extensive testing across diverse and complex scenarios. To generate as many testing scenarios as possible, previous efforts have focused on describing scenarios using formal languages to generate scenes. In this paper, we introduce an alternative approach: reconstructing scenes inside the open-source game engine, Godot. We have developed a pipeline that enables the reconstruction of testing scenes directly from provided images of scenarios. These reconstructed scenes can then be deployed within simulated environments to assess a CPS. This approach offers a scalable and flexible solution for testing CPS in realistic environments.', 'abstract_zh': '当然，以下是符合学术规范的翻译：\n\n当前的物理系统（Cyber-Physical Systems, CPS）结合了工程设计的虚拟和物理组件，能够在动态环境中做出决策并相互作用。确保CPS的安全性至关重要，这需要在多种复杂的场景中进行广泛的测试。为了尽可能多地生成测试场景，先前的努力集中在使用形式语言描述场景以生成相应的界面。在本文中，我们介绍了一种替代方法：在开源游戏引擎Godot中重建场景。我们开发了一个流水线，可以直接从提供的场景图像中重建测试场景。这些重建的场景随后可以在模拟环境中部署，以评估CPS。这种方法提供了一种可扩展且灵活的解决方案，以在实际环境中测试CPS。', 'title_zh': '探索Godot模拟器中的灵活场景生成方法'}
{'arxiv_id': 'arXiv:2412.18387', 'title': 'Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model', 'authors': 'Tenghui Li, Guoxu Zhou, Xuyang Zhao, Qibin Zhao', 'link': 'https://arxiv.org/abs/2412.18387', 'abstract': "The scaling capability has been widely validated with respect to the number of parameters and the size of training data. One important question that is unexplored is that does scaling capability also exists similarly with respect to the number of vision tokens? This study fills the gap by investigating the relationship between the number of vision tokens and the performance of vision-language models. Our theoretical analysis and empirical evaluations reveal that the model exhibits weak scaling capabilities on the length \\(N_l\\), with performance approximately \\(S(N_l) \\approx (c/N_l)^{\\alpha}\\), where \\(c, \\alpha\\) are hyperparameters. Interestingly, this scaling behavior remains largely unaffected by the inclusion or exclusion of the user's question in the input. Furthermore, fusing the user's question with the vision token can enhance model performance when the question is relevant to the task. To address the computational challenges associated with large-scale vision tokens, we propose a novel architecture that efficiently reduces the token count while integrating user question tokens into the representation. Our findings may offer insights for developing more efficient and effective vision-language models under specific task constraints.", 'abstract_zh': '关于参数数量和训练数据规模的扩展能力已得到了广泛验证。一个有待探索的重要问题是，扩展能力是否也适用于视觉标记的数量？本研究通过调查视觉标记数量与视觉语言模型性能之间的关系，填补了这一空白。我们的理论分析和实证评估表明，模型在长度 \\(N_l\\) 上表现出较弱的扩展能力，性能约为 \\(S(N_l) \\approx (c/N_l)^{\\alpha}\\)，其中 \\(c, \\alpha\\) 是超参数。有趣的是，用户的提问是否包含在输入中对这种扩展行为的影响不大。此外，当用户提问与任务相关时，将用户提问与视觉标记融合可以提升模型性能。为应对大规模视觉标记计算上的挑战，我们提出了一种新颖的架构，该架构能够高效地减少标记数量，并将用户提问标记整合到表征中。我们的研究结果可能为在特定任务约束下开发更高效和有效的视觉语言模型提供洞见。', 'title_zh': '在Token空间中的弱缩放能力：来自大规模视觉语言模型的观察'}
{'arxiv_id': 'arXiv:2412.18354', 'title': 'The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence', 'authors': 'Viviane Clay, Niels Leadholm, Jeff Hawkins', 'link': 'https://arxiv.org/abs/2412.18354', 'abstract': 'Artificial intelligence has advanced rapidly in the last decade, driven primarily by progress in the scale of deep-learning systems. Despite these advances, the creation of intelligent systems that can operate effectively in diverse, real-world environments remains a significant challenge. In this white paper, we outline the Thousand Brains Project, an ongoing research effort to develop an alternative, complementary form of AI, derived from the operating principles of the neocortex. We present an early version of a thousand-brains system, a sensorimotor agent that is uniquely suited to quickly learn a wide range of tasks and eventually implement any capabilities the human neocortex has. Core to its design is the use of a repeating computational unit, the learning module, modeled on the cortical columns found in mammalian brains. Each learning module operates as a semi-independent unit that can model entire objects, represents information through spatially structured reference frames, and both estimates and is able to effect movement in the world. Learning is a quick, associative process, similar to Hebbian learning in the brain, and leverages inductive biases around the spatial structure of the world to enable rapid and continual learning. Multiple learning modules can interact with one another both hierarchically and non-hierarchically via a "cortical messaging protocol" (CMP), creating more abstract representations and supporting multimodal integration. We outline the key principles motivating the design of thousand-brains systems and provide details about the implementation of Monty, our first instantiation of such a system. Code can be found at this https URL, along with more detailed documentation at this https URL.', 'abstract_zh': '在过去十年中，人工智能取得了 rapid 的进步，主要得益于深度学习系统规模的扩大。尽管取得了这些进展，如何创建能够在多种多变的实际环境中有效运作的智能系统仍然是一个重大挑战。在这份白皮书中，我们概述了千脑计划，这是一个正在进行的研究项目，旨在开发一种新的替代性互补形式的 AI，该形式的AI源自新皮层的运作原理。我们介绍了千脑系统的早期版本，这是一种传感器-执行器代理，能够快速学习多种任务，并最终实现与人类新皮层相媲美的所有能力。该系统的核心在于使用重复的计算单元，即学习模块，这种模块模拟了哺乳动物大脑中的皮层柱。每个学习模块作为半独立单元运作，可以建模整个对象、通过空间结构化的参照框架表达信息，并且既能估计又能影响世界中的运动。学习是一个快速关联的过程，类似于大脑中的 Hebbsian 学习，并利用了对世界空间结构的归纳先验，从而实现快速且持续的学习。多个学习模块可以通过“皮层消息协议”（CMP）以层次化和非层次化的方式相互交互，从而生成更抽象的表示，支持多种模态的整合。我们概述了驱动千脑系统设计的关键原则，并详细介绍了我们第一个此类系统的实现，蒙蒂（Monty）。相关代码可以在此找到：https://github.com/…，更详细的文档在此：https://github.com/…。', 'title_zh': '千脑计划：一种新的感motor智能范式'}
{'arxiv_id': 'arXiv:2412.18295', 'title': 'Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases', 'authors': 'Christian Di Maio, Cristian Cosci, Marco Maggini, Valentina Poggioni, Stefano Melacci', 'link': 'https://arxiv.org/abs/2412.18295', 'abstract': 'The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in several real-world services triggers severe concerns about their security. A RAG system improves the generative capabilities of a Large Language Models (LLM) by a retrieval mechanism which operates on a private knowledge base, whose unintended exposure could lead to severe consequences, including breaches of private and sensitive information. This paper presents a black-box attack to force a RAG system to leak its private knowledge base which, differently from existing approaches, is adaptive and automatic. A relevance-based mechanism and an attacker-side open-source LLM favor the generation of effective queries to leak most of the (hidden) knowledge base. Extensive experimentation proves the quality of the proposed algorithm in different RAG pipelines and domains, comparing to very recent related approaches, which turn out to be either not fully black-box, not adaptive, or not based on open-source models. The findings from our study remark the urgent need for more robust privacy safeguards in the design and deployment of RAG systems.', 'abstract_zh': '随着检索增强生成（RAG）系统在多个实际服务中的广泛应用，其安全性问题引发了严重关注。RAG系统通过检索机制增强了大型语言模型（LLM）的生成能力，而检索机制依赖于一个私有的知识库，若该知识库意外泄露，可能导致严重的后果，包括个人和敏感信息的泄露。本文提出了一种黑盒攻击方法，迫使RAG系统泄露其私有知识库。与现有方法不同，本方法具有适应性和自动化。基于相关性机制和攻击者侧的开源LLM，能够生成有效的查询以泄露大部分（隐藏的）知识库。广泛的实验表明，在不同RAG流水线和领域中，所提出算法的质量优于非常近期的相关方法，这些方法要么不完全符合黑盒攻击条件，要么不具有适应性，要么不基于开源模型。我们的研究发现突显了在设计和部署RAG系统时加强更稳健的隐私保护措施的紧迫性。', 'title_zh': '《RAG中的海盗：适应性攻击LLMs以泄露知识库》\n\n这个标题翻译成中文时，为了使其更符合中文的表达习惯和学术规范，可以稍微调整一下：\n\n《RAG中的适应性攻击：从LLMs泄露知识库》'}
{'arxiv_id': 'arXiv:2412.18293', 'title': 'MinsStudio: A Streamlined Package for Minecraft AI Agent Development', 'authors': 'Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang', 'link': 'https://arxiv.org/abs/2412.18293', 'abstract': 'Minecraft has emerged as a valuable testbed for embodied intelligence and sequential decision-making research, yet the development and validation of novel agents remains hindered by significant engineering challenges. This paper presents MineStudio, an open-source software package designed to streamline embodied policy development in Minecraft. MineStudio represents the first comprehensive integration of seven critical engineering components: simulator, data, model, offline pretraining, online finetuning, inference, and benchmark, thereby allowing users to concentrate their efforts on algorithm innovation. We provide a user-friendly API design accompanied by comprehensive documentation and tutorials. The complete codebase is publicly available at this https URL.', 'abstract_zh': 'Minecraft 凭借其在具身智能和顺序决策研究方面的价值，逐渐成为了一个重要的实验平台，然而，新型代理的开发与验证仍然受到重大工程挑战的阻碍。本文介绍了 MineStudio，这是一个开源软件包，旨在简化 Minecraft 中具身策略的开发流程。MineStudio 是首个全面集成七个关键工程组件的平台：模拟器、数据、模型、离线预训练、在线微调、推理和基准测试，从而允许用户将精力聚焦于算法创新。我们提供了用户友好的 API 设计，并配备了详尽的文档和教程。完整的代码库可以在以下网址进行访问：[提供网址]。\n\n注：翻译中“[提供网址]”需替换为实际的网址链接，以便读者可以直接访问代码库。', 'title_zh': 'MinsStudio：一种简化版的Minecraft AI代理开发包'}
{'arxiv_id': 'arXiv:2412.18279', 'title': 'Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization', 'authors': 'Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou', 'link': 'https://arxiv.org/abs/2412.18279', 'abstract': 'The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One challenge is the sparse reward, which makes optimization difficult for RL and necessitates a large amount of data samples. Another challenge stems from the inherent instability of RL, particularly when using Actor-Critic (AC) methods to derive optimal policies, which often leads to unstable training processes. To address these issues, we introduce Direct Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm. Unlike standard alignment that rely solely outcome rewards to optimize policies (such as DPO), DAPO employs a critic function to predict the reasoning accuracy at each step, thereby generating dense signals to refine the generation strategy. Additionally, the Actor and Critic components in DAPO are trained independently, avoiding the co-training instability observed in standard AC algorithms like PPO. We train DAPO on mathematical and code query datasets and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO.', 'abstract_zh': '强化学习（RL）在提高大规模语言模型（LLMs）推理能力方面的作用日益重要。尽管在许多场景中RL取得了成功，但在提高LLMs推理能力方面仍存在许多挑战。其中一个挑战是稀疏奖励，这使得RL的优化变得困难，需要大量的数据样本。另一个挑战来源于RL固有的不稳定性，尤其是在使用Actor-Critic（AC）方法来推导最优策略时，通常会导致训练过程不稳定。为了解决这些问题，我们提出了一种新颖的离线RL算法——直接优势策略优化（DAPO）。与仅依赖于结果奖励来优化策略的标准对齐方法（如DPO）不同，DAPO使用了一个评论家函数来预测每一步的推理准确性，从而生成密集信号以改进生成策略。此外，DAPO中的Actor和 Critic组件是独立训练的，从而避免了标准AC算法（如PPO）中出现的协同训练不稳定性。我们使用数学和代码查询数据集训练DAPO，并在多个基准测试中评估其性能。结果显示，DAPO可以有效地增强两者即自适应训练模型（SFT）和RL模型的数学和代码能力，证明了DAPO的有效性。', 'title_zh': '利用直接优势策略优化提升大型语言模型的多步推理能力'}
{'arxiv_id': 'arXiv:2412.18270', 'title': 'Annotating References to Mythological Entities in French Literature', 'authors': 'Thierry Poibeau', 'link': 'https://arxiv.org/abs/2412.18270', 'abstract': 'In this paper, we explore the relevance of large language models (LLMs) for annotating references to Roman and Greek mythological entities in modern and contemporary French literature. We present an annotation scheme and demonstrate that recent LLMs can be directly applied to follow this scheme effectively, although not without occasionally making significant analytical errors. Additionally, we show that LLMs (and, more specifically, ChatGPT) are capable of offering interpretative insights into the use of mythological references by literary authors. However, we also find that LLMs struggle to accurately identify relevant passages in novels (when used as an information retrieval engine), often hallucinating and generating fabricated examples-an issue that raises significant ethical concerns. Nonetheless, when used carefully, LLMs remain valuable tools for performing annotations with high accuracy, especially for tasks that would be difficult to annotate comprehensively on a large scale through manual methods alone.', 'abstract_zh': '在本文中，我们探讨了大规模语言模型（LLMs）在标注现代和当代法语文献中关于罗马和希腊神话实体的引用方面的相关性。我们呈现了一种标注方案，并证明了最近的LLMs可以直接应用于遵循该方案，尽管偶尔会产生显著的分析错误。此外，我们还展示了LLMs（更具体地说是ChatGPT）能够为文学作者在使用神话引用方面提供解释性的见解。然而，我们发现，当将LLMs用作信息检索引擎时，它们在准确识别小说中的相关段落方面存在困难，经常会产生幻觉并生成虚构的示例—这一问题引发了重大的伦理关切。尽管如此，只要谨慎使用，LLMs仍然是在大规模手动方法难以全面标注的任务中进行标注的有价值的工具，尤其是对于那些能够实现高精度标注的任务。', 'title_zh': '法国文学中神话实体的标注研究'}
{'arxiv_id': 'arXiv:2412.18161', 'title': 'VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities', 'authors': 'Shray Mathur, Noah van der Vleuten, Kevin Yager, Esther Tsai', 'link': 'https://arxiv.org/abs/2412.18161', 'abstract': 'Scientific user facilities, such as synchrotron beamlines, are equipped with a wide array of hardware and software tools that require a codebase for human-computer-interaction. This often necessitates developers to be involved to establish connection between users/researchers and the complex instrumentation. The advent of generative AI presents an opportunity to bridge this knowledge gap, enabling seamless communication and efficient experimental workflows. Here we present a modular architecture for the Virtual Scientific Companion (VISION) by assembling multiple AI-enabled cognitive blocks that each scaffolds large language models (LLMs) for a specialized task. With VISION, we performed LLM-based operation on the beamline workstation with low latency and demonstrated the first voice-controlled experiment at an X-ray scattering beamline. The modular and scalable architecture allows for easy adaptation to new instrument and capabilities. Development on natural language-based scientific experimentation is a building block for an impending future where a science exocortex -- a synthetic extension to the cognition of scientists -- may radically transform scientific practice and discovery.', 'abstract_zh': '科学用户设施，如同步辐射光束线，配备了大量硬件和软件工具，需要代码库来支持人机交互。这通常需要开发人员参与，以建立用户/研究人员与复杂仪器之间的连接。随着生成式AI的发展，为弥补这一知识差距提供了机会，促进了无缝通信和高效的实验流程。本文提出了一种模块化的虚拟科学伴侣（VISION）架构，通过集合多个AI-enabled认知模块，为特定任务搭建大型语言模型（LLMs）。利用VISION，我们实现了基于LLM的光束线工作站操作，并展示了首个语音控制的X射线散射实验。这种模块化和可扩展的架构允许轻松适应新的仪器和功能。基于自然语言的科学实验开发是向着一个即将到来的未来发展的基石，即科学外脑——一种合成扩展科学家认知能力的技术——可能对科学研究和发现产生根本性的影响。', 'title_zh': 'VISION：一种适用于科学用户设施的模块化人工智能助手，用于自然的人-仪器交互'}
{'arxiv_id': 'arXiv:2412.18148', 'title': 'Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media', 'authors': 'Zhen Sun, Zongmin Zhang, Xinyue Shen, Ziyi Zhang, Yule Liu, Michael Backes, Yang Zhang, Xinlei He', 'link': 'https://arxiv.org/abs/2412.18148', 'abstract': 'Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, a systematic study to assess the prevalence of AIGTs on social media is still lacking. To address this gap, this paper aims to quantify, monitor, and analyze the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs over time and observe different trends of AI Attribution Rate (AAR) across social media platforms from January 2022 to October 2024. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain.', 'abstract_zh': '社交媒体平台正日益呈现出由人工智能生成的文本（AIGTs）的存在。然而，AIGTs 的不当使用可能对公众舆论产生深远影响，例如传播虚假信息和操控叙事。尽管其重要性不言而喻，但系统性地评估社交平台上 AIGTs 的存在情况的研究依然缺乏。为了弥补这一空白，本文旨在量化、监控并分析社交平台上的 AIGTs。我们首先从 Medium、Quora 和 Reddit 这三大社交媒体平台收集了一个数据集（SM-D），包含约 240 万条帖子。然后，我们构建了一个多样化的数据集（AIGTBench）以训练和评估 AIGT 检测器。AIGTBench 将流行开源数据集与从社交媒体文本生成的 12 个人工智能语言模型（LLMs）生成的 AIGT 数据集结合在一起，作为评估主流检测器的基准。通过这套方案，我们识别出了表现最优的检测器（OSM-Det）。接着，我们将 OSM-Det 应用于 SM-D 来跟踪时间上的 AIGTs，观察从 2022 年 1 月至 2024 年 10 月期间各社交媒体平台上的 AI 归因率（AAR）的不同趋势。具体来说，Medium 和 Quora 的 AAR 显著上升，分别从 1.77% 增加到 37.03% 和从 2.06% 增加到 38.95%。相比之下，Reddit 的增长较为缓慢，AAR 在同一时期从 1.31% 增加到 2.45%。我们的进一步分析表明，AIGTs 在多个维度上与人类撰写的文本存在差异，包括语言模式、主题分布、参与度水平以及作者的粉丝分布。我们希望我们的分析和 AIGTs 的发现可以为该领域的未来研究提供启示。', 'title_zh': '我们已经身处AI生成文本的世界了吗？量化与监控社交媒体上的AI生成文本'}
{'arxiv_id': 'arXiv:2412.18125', 'title': 'Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy', 'authors': 'Qian Tao, Xiyuan Wang, Muhan Zhang, Shuxian Hu, Wenyuan Yu, Jingren Zhou', 'link': 'https://arxiv.org/abs/2412.18125', 'abstract': "Graph neural networks (GNNs) have become a prevalent framework for graph tasks. Many recent studies have proposed the use of graph convolution methods over the numerous subgraphs of each graph, a concept known as subgraph graph neural networks (subgraph GNNs), to enhance GNNs' ability to distinguish non-isomorphic graphs. To maximize the expressiveness, subgraph GNNs often require each subgraph to have equal size to the original graph. Despite their impressive performance, subgraph GNNs face challenges due to the vast number and large size of subgraphs which lead to a surge in training data, resulting in both storage and computational inefficiencies. In response to this problem, this paper introduces Ego-Nets-Fit-All (ENFA), a model that uniformly takes the smaller ego nets as subgraphs, thereby providing greater storage and computational efficiency, while at the same time guarantees identical outputs to the original subgraph GNNs even taking the whole graph as subgraphs. The key is to identify and eliminate the redundant computation among subgraphs. For example, a node $v_i$ may appear in multiple subgraphs but is far away from all of their centers (the unsymmetric part between subgraphs). Therefore, its first few rounds of message passing within each subgraph can be computed once in the original graph instead of being computed multiple times within each subgraph. Such strategy enables our ENFA to accelerate subgraph GNNs in an exact way, unlike previous sampling approaches that often lose the performance. Extensive experiments across various datasets reveal that compared with the conventional subgraph GNNs, ENFA can reduce storage space by 29.0% to 84.5% and improve training efficiency by up to 1.66x.", 'abstract_zh': '图神经网络（Graph Neural Networks, GNNs）已成为处理图任务的主流框架。许多近期的研究提出了一种使用图卷积方法对每个图的众多子图进行操作的概念，称为子图图神经网络（subgraph GNNs），以此来增强GNNs区分非同构图的能力。为了最大化表示能力，子图GNNs通常要求每个子图的大小与原始图相同。尽管子图GNNs表现出色，但由于子图数量庞大且体积较大，导致训练数据激增，从而产生存储和计算效率问题。为解决这一问题，本文提出了一种名为Ego-Nets-Fit-All（ENFA）的模型，该模型统一处理较小的ego-net作为子图，从而在提高存储和计算效率的同时，确保输出与原始子图GNNs相同，即使将整个图作为子图也是如此。关键在于识别并消除子图之间的冗余计算。例如，一个节点\\(v_i\\)可能出现在多个子图中，但距离这些子图的中心较远（子图之间的非对称部分）。因此，它可以只在原始图中进行前几轮消息传递计算，而不需要在每个子图中重复计算。这种策略使得我们的ENFA能够以精确的方式加速子图GNNs，而不像以前的采样方法那样往往性能会受到影响。广泛的实验结果表明，相比于传统的子图GNNs，ENFA可以在各种数据集上减少存储空间29.0%至84.5%，并提高训练效率多达1.66倍。', 'title_zh': '通过消除计算冗余加速子图图神经网络'}
{'arxiv_id': 'arXiv:2412.18116', 'title': 'AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation', 'authors': 'Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li', 'link': 'https://arxiv.org/abs/2412.18116', 'abstract': "Large language models (LLMs) have brought exciting new advances to mobile UI agents, a long-standing research field that aims to complete arbitrary natural language tasks through mobile UI interactions. However, existing UI agents usually demand high reasoning capabilities of powerful large models that are difficult to be deployed locally on end-users' devices, which raises huge concerns about user privacy and centralized serving cost. One way to reduce the required model size is to customize a smaller domain-specific model with high-quality training data, e.g. large-scale human demonstrations of diverse types of apps and tasks, while such datasets are extremely difficult to obtain. Inspired by the remarkable coding abilities of recent small language models (SLMs), we propose to convert the UI task automation problem to a code generation problem, which can be effectively solved by an on-device SLM and efficiently executed with an on-device code interpreter. Unlike normal coding tasks that can be extensively pretrained with public datasets, generating UI automation code is challenging due to the diversity, complexity, and variability of target apps. Therefore, we adopt a document-centered approach that automatically builds fine-grained API documentation for each app and generates diverse task samples based on this documentation. By guiding the agent with the synthetic documents and task samples, it learns to generate precise and efficient scripts to complete unseen tasks. Based on detailed comparisons with state-of-the-art mobile UI agents, our approach effectively improves the mobile task automation with significantly higher success rates and lower latency/token consumption. Code will be open-sourced.", 'abstract_zh': '大规模语言模型（LLMs）为移动UI代理带来了令人兴奋的新进展，这是一个长期研究的领域，旨在通过移动UI交互完成任意自然语言任务。然而，现有的UI代理通常需要具备高推理能力的强大模型，这些模型在终端用户的设备上本地部署非常困难，这引发了用户隐私和集中式服务成本方面的巨大担忧。减少所需模型大小的一种方法是使用高质量的训练数据自定义一个小规模领域特定模型，例如大型人类示范各种类型的应用和任务，但这类数据集极其难以获得。受近期小型语言模型（SLMs）卓越编码能力的启发，我们提出了将UI任务自动化问题转化为一个代码生成问题，该问题可以通过设备上的SLM有效地解决，并通过设备上的代码解释器高效执行。与可以广泛使用公开数据集进行预训练的常规编码任务不同，生成UI自动化代码具有挑战性，因为目标应用的多样性、复杂性和变异性。因此，我们采用一种文档中心的方法，自动为每个应用构建细粒度的API文档，并基于这些文档生成多样化的任务样本。通过使用合成文档和任务样本指导代理，它学习生成用于完成未见任务的精确和高效的脚本。基于与最先进的移动UI代理的详细对比，我们的方法显著提高了移动任务自动化的成功率和降低了延迟/令牌消耗。代码将开源。', 'title_zh': 'AutoDroid-V2：基于代码生成增强SLM方法的GUI代理性能'}
{'arxiv_id': 'arXiv:2412.18111', 'title': 'AIGT: AI Generative Table Based on Prompt', 'authors': 'Mingming Zhang, Zhiqing Xiao, Guoshan Lu, Sai Wu, Weiqiang Wang, Xing Fu, Can Yi, Junbo Zhao', 'link': 'https://arxiv.org/abs/2412.18111', 'abstract': 'Tabular data, which accounts for over 80% of enterprise data assets, is vital in various fields. With growing concerns about privacy protection and data-sharing restrictions, generating high-quality synthetic tabular data has become essential. Recent advancements show that large language models (LLMs) can effectively gener-ate realistic tabular data by leveraging semantic information and overcoming the challenges of high-dimensional data that arise from one-hot encoding. However, current methods do not fully utilize the rich information available in tables. To address this, we introduce AI Generative Table (AIGT) based on prompt enhancement, a novel approach that utilizes meta data information, such as table descriptions and schemas, as prompts to generate ultra-high quality synthetic data. To overcome the token limit constraints of LLMs, we propose long-token partitioning algorithms that enable AIGT to model tables of any scale. AIGT achieves state-of-the-art performance on 14 out of 20 public datasets and two real industry datasets within the Alipay risk control system.', 'abstract_zh': '表格数据占企业数据资产的80%以上，在多个领域中至关重要。随着对隐私保护和数据共享限制的关注不断增加，生成高质量的合成表格数据变得至关重要。最近的研究表明，大型语言模型（LLMs）可以通过利用语义信息并克服基于独热编码的一维高维数据带来的挑战，有效地生成逼真的表格数据。然而，当前的方法并未充分利用表格中丰富的信息。为此，我们基于提示增强引入了AI生成表格（AIGT）方法，这是一种新颖的方法，利用表描述和表结构等元数据信息作为提示来生成超高质量的合成数据。为克服LLMs的令牌限制约束，我们提出了长令牌分割算法，使AIGT能够建模任意规模的表格。在20个公开数据集的14个以及两个实际行业数据集（来自支付宝风控系统）中，AIGT达到了目前的最优性能。', 'title_zh': 'AIGT：基于提示的AI生成表格'}
{'arxiv_id': 'arXiv:2412.18110', 'title': 'SlimGPT: Layer-wise Structured Pruning for Large Language Models', 'authors': 'Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu', 'link': 'https://arxiv.org/abs/2412.18110', 'abstract': 'Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results.', 'abstract_zh': '大型语言模型（LLMs）因其在各个领域表现出的出色能力而受到广泛关注，其庞大的参数规模带来了实际部署上的挑战。结构化剪枝是一种有效的方法，可以在模型性能与效率之间取得平衡，但在计算资源受限的情况下，恢复剪枝后模型的性能是一个主要挑战。因此，我们提出了一种基于Optimal Brain Surgeon框架的低成本且快速的结构化剪枝方法，名为SlimGPT。我们提出了一种批量贪婪剪枝策略，通过分组Cholesky分解增强了头级剪枝误差估计的准确性，并通过动态组大小提高了FFN的剪枝效率，从而在一小时内实现了近似局部最优的剪枝结果。此外，我们从误差累积的角度探索了层级剪枝的限制，并提出了一种非均匀剪枝策略，增量剪枝比，以减少性能退化。在LLaMA基准测试上的实验结果表明，SlimGPT优于其他方法，并取得了最先进的性能。', 'title_zh': 'SlimGPT：大型语言模型的逐层结构化剪枝'}
{'arxiv_id': 'arXiv:2412.18106', 'title': 'Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels', 'authors': 'Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long', 'link': 'https://arxiv.org/abs/2412.18106', 'abstract': 'Meeting growing demands for low latency and cost efficiency in production-grade large language model (LLM) serving systems requires integrating advanced optimization techniques. However, dynamic and unpredictable input-output lengths of LLM, compounded by these optimizations, exacerbate the issues of workload variability, making it difficult to maintain high efficiency on AI accelerators, especially DSAs with tile-based programming models. To address this challenge, we introduce XY-Serve, a versatile, Ascend native, end-to-end production LLM-serving system. The core idea is an abstraction mechanism that smooths out the workload variability by decomposing computations into unified, hardware-friendly, fine-grained meta primitives. For attention, we propose a meta-kernel that computes the basic pattern of matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we introduce a virtual padding scheme that adapts to dynamic shape changes while using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve sits harmoniously with vLLM. Experimental results show up to 89% end-to-end throughput improvement compared with current publicly available baselines on Ascend NPUs. Additionally, our approach outperforms existing GEMM (average 14.6% faster) and attention (average 21.5% faster) kernels relative to existing libraries. While the work is Ascend native, we believe the approach can be readily applicable to SIMT architectures as well.', 'abstract_zh': '为了在生产级大规模语言模型（LLM）服务系统中满足日益增长的低延迟和低成本需求，需要集成先进的优化技术。然而，LLM 动态和不可预测的输入输出长度使得这些优化技术加剧了工作负载变异性的问题，特别是在使用基于瓷砖编程模型的DSA（可编程加速器）上维持高效率变得尤为困难。为了解决这一挑战，我们提出了XY-Serve，这是一种多功能、兼容Ascend的端到端生产环境下的LLM服务系统。核心思想是一种抽象机制，通过将计算分解为统一的、硬件友好的细粒度元原语来平滑工作负载变异性。对于注意力机制，我们提出了一种元内核（meta-kernel），能够在具有架构感知的瓷砖大小的情况下计算基本的矩阵乘法-softmax-矩阵乘法模式。对于GEMM（通用矩阵乘法），我们引入了一种虚拟填充方案，可以在动态形状变化时进行适配，并且使用具有各种固定瓷砖大小的高度高效GEMM原语。XY-Serve 与 vLLM 完美融合。实验结果表明，与目前公开的基线相比，在Ascend NPUs上，端到端吞吐量提高了高达89%。此外，与现有的GEMM和注意力内核相比，我们的方法分别平均快14.6%和21.5%，展示了更高的性能。尽管这项工作是为Ascend量身定制的，但我们相信这种方法也可以很容易地应用于SIMT架构。', 'title_zh': '通过在高效元内核上采用混合预填充/解码/验证调度方式结合最新优化技术来应对生产LLM服务系统中的动态性问题'}
{'arxiv_id': 'arXiv:2412.18096', 'title': 'Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH) -- a Large Language Model Chatbot for Perioperative Medicine', 'authors': 'Yu He Ke, Liyuan Jin, Kabilan Elangovan, Bryan Wen Xi Ong, Chin Yang Oh, Jacqueline Sim, Kenny Wei-Tsen Loh, Chai Rick Soh, Jonathan Ming Hua Cheng, Aaron Kwang Yang Lee, Daniel Shu Wei Ting, Nan Liu, Hairil Rizal Abdullah', 'link': 'https://arxiv.org/abs/2412.18096', 'abstract': 'Large Language Models (LLMs) are emerging as powerful tools in healthcare, particularly for complex, domain-specific tasks. This study describes the development and evaluation of the PErioperative AI CHatbot (PEACH), a secure LLM-based system integrated with local perioperative guidelines to support preoperative clinical decision-making. PEACH was embedded with 35 institutional perioperative protocols in the secure Claude 3.5 Sonet LLM framework within Pair Chat (developed by Singapore Government) and tested in a silent deployment with real-world data. Accuracy, safety, and usability were assessed. Deviations and hallucinations were categorized based on potential harm, and user feedback was evaluated using the Technology Acceptance Model (TAM). Updates were made after the initial silent deployment to amend one protocol.\nIn 240 real-world clinical iterations, PEACH achieved a first-generation accuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across three iterations. The updated PEACH demonstrated improved accuracy of 97.9% (235/240), with a statistically significant difference from the null hypothesis of 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and deviations were observed (both 1/240 and 2/240, respectively). Clinicians reported that PEACH expedited decisions in 95% of cases, and inter-rater reliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among attendings.\nPEACH is an accurate, adaptable tool that enhances consistency and efficiency in perioperative decision-making. Future research should explore its scalability across specialties and its impact on clinical outcomes.', 'abstract_zh': '大规模语言模型（LLMs）正在成为医疗保健领域中强大的工具，特别是在处理复杂且领域特定的任务方面。本研究描述了PE亟术AI聊天机器人（PEACH）的开发与评估，PEACH是一种结合了当地围手术期指南的安全LLM系统，旨在支持术前临床决策。PEACH嵌入了35个机构的围手术期协议，使用了安全的Claude 3.5 Sonet LLM框架，并在新加坡政府开发的Pair Chat中进行了无声部署，使用真实世界数据进行了测试。评估了其准确度、安全性和可用性。基于潜在危害，对偏差和幻觉进行了分类，并使用技术接受模型（TAM）评估了用户反馈。在首次无声部署后进行了更新，修正了一个协议。\n\n在240例真实世界的临床迭代中，PEACH的第一代准确率为97.5%（78/80），整体准确率为96.7%（232/240）。更新后的PEACH的准确率提高至97.9%（235/240），与95%的准确率（虚无假设）相比有显著性差异（p=0.018，95% CI：0.952-0.991）。观察到的幻觉和偏差很少（分别为1/240和2/240）。临床医生报告称，在95%的情况下，PEACH加速了决策过程，并且PEACH内的信度范围为Kappa 0.772-0.893，而主治医生间的信度范围为Kappa 0.610-0.784。\n\nPEACH是一款准确且可适应的工具，可增强围手术期决策的一致性和效率。未来的研究应探索其在各个专科的可扩展性及其对临床结果的影响。', 'title_zh': '在实际环境中的部署与评估：PEPerioperative AI Chatbot (PEACH)——一个用于围术期医学的大语言模型聊天机器人'}
{'arxiv_id': 'arXiv:2412.18091', 'title': 'AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning', 'authors': 'Lixian Jing, Jianpeng Qi, Junyu Dong, Yanwei Yu', 'link': 'https://arxiv.org/abs/2412.18091', 'abstract': 'As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at this https URL', 'abstract_zh': '随着深度神经网络（DNNs）越来越多地部署在边缘设备上，优化模型以适应受限的计算资源变得至关重要。现有的自动剪枝方法因DNN模型多样、各种操作符（如滤波器）以及剪枝精细度与模型精度之间的平衡难度而面临挑战。为解决这些限制，我们提出了一种基于模式的自动剪枝框架AutoSculpt，该框架通过图学习和深度强化学习（DRL）来提高效率和准确度。AutoSculpt能够自动识别并剪枝DNN架构中的可被现有推断引擎识别的规律性结构，从而实现运行时加速。AutoSculpt的三个关键步骤包括：（1）将DNNs表示为图，以编码其拓扑结构和参数依赖关系，（2）嵌入计算高效的剪枝模式，以及（3）利用DRL迭代优化自动剪枝策略，直到达到压缩与准确度之间的最佳平衡。实验证明，AutoSculpt在各种架构上（包括ResNet、MobileNet、VGG和Vision Transformer）均显示出有效性，能够实现高达90%的剪枝率，并且接近18%的浮点运算减少（FLOPs reduction），在所有基线方法的基础上表现出色。相关代码可在以下链接获取：[此 https URL](此 https URL)', 'title_zh': 'AutoSculpt：一种基于模式的模型自剪枝框架，结合强化学习与图学习'}
{'arxiv_id': 'arXiv:2412.18084', 'title': 'Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models', 'authors': 'Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu', 'link': 'https://arxiv.org/abs/2412.18084', 'abstract': 'Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in this https URL.', 'abstract_zh': '大型语言模型（LLMs）广泛应用于诸如问答和机器翻译等自然语言处理任务。然而，由于缺乏标注数据以及生化性质的手动标注难度大，LLMs 在分子生成任务中的性能仍然受到限制，尤其是在涉及多性质约束的任务中。本工作中，我们提出了一种两步框架 PEIT（Property Enhanced Instruction Tuning），以改进LLMs在与分子相关的任务表现。在第一步中，我们使用文本描述、SMILES 和生化性质作为多模态输入，预训练一个名为 PEIT-GEN 的模型，通过对齐多模态表示以合成指令数据。在第二步中，我们将现有的开源LLMs 与合成数据进行微调，所得到的 PEIT-LLM 可以处理分子描述、基于文本的分子生成、分子性质预测以及我们新提出的多约束条件的分子生成任务。实验结果表明，我们预训练的 PEIT-GEN 在分子描述任务中优于 MolT5 和 BioT5，展示了文本描述、结构和生化性质之间的模态对齐良好。此外，PEIT-LLM 在多任务分子生成中表现出显著的提升，证明了 PEIT 框架在各种分子任务中的可扩展性。我们在此 https://github.com/... 释放了代码、构建的指令数据和模型检查点。', 'title_zh': '使用大型语言模型进行多任务分子生成的属性增强指令调优'}
{'arxiv_id': 'arXiv:2412.18073', 'title': "Understanding Artificial Neural Network's Behavior from Neuron Activation Perspective", 'authors': 'Yizhou Zhang, Yang Sui', 'link': 'https://arxiv.org/abs/2412.18073', 'abstract': "This paper explores the intricate behavior of deep neural networks (DNNs) through the lens of neuron activation dynamics. We propose a probabilistic framework that can analyze models' neuron activation patterns as a stochastic process, uncovering theoretical insights into neural scaling laws, such as over-parameterization and the power-law decay of loss with respect to dataset size. By deriving key mathematical relationships, we present that the number of activated neurons increases in the form of $N(1-(\\frac{bN}{D+bN})^b)$, and the neuron activation should follows power-law distribution. Based on these two mathematical results, we demonstrate how DNNs maintain generalization capabilities even under over-parameterization, and we elucidate the phase transition phenomenon observed in loss curves as dataset size plotted in log-axis (i.e. the data magnitude increases linearly). Moreover, by combining the above two phenomenons and the power-law distribution of neuron activation, we derived the power-law decay of neural network's loss function as the data size scale increases. Furthermore, our analysis bridges the gap between empirical observations and theoretical underpinnings, offering experimentally testable predictions regarding parameter efficiency and model compressibility. These findings provide a foundation for understanding neural network scaling and present new directions for optimizing DNN performance.", 'abstract_zh': '本文从神经元激活动力学的角度探讨了深度神经网络（DNNs）的复杂行为。我们提出了一种概率框架，可以将模型的神经元激活模式视为一个随机过程，揭示了神经网络增宽定律等理论洞察，例如过参数化以及损失函数关于数据集大小的幂律衰减。通过推导关键的数学关系，我们展示了激活神经元的数量以 $N(1-(\\frac{bN}{D+bN})^b)$ 的形式增加，并且神经元激活应服从幂律分布。基于这两个数学结果，我们说明了即使在过参数化情况下，DNNs 仍能保持泛化能力，并阐明了当以对数坐标轴绘制数据集大小（即数据量线性增长）时观察到的损失曲线相变现象。此外，通过结合上述两种现象以及神经元激活的幂律分布，我们推导出了随数据集规模增加而呈幂律衰减的神经网络损失函数。我们的分析填补了经验观察与理论支持之间的空白，提供了关于参数效率和模型压缩性可实验验证的预测。这些发现为理解神经网络增宽提供了基础，并提出了优化DNN性能的新方向。', 'title_zh': '从神经元激活的角度理解人工神经网络的行为'}
{'arxiv_id': 'arXiv:2412.17964', 'title': 'Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer Systems using Large Language Models', 'authors': 'Antony Seabra, Claudio Cavalcante, Joao Nepomuceno, Lucas Lago, Nicolaas Ruberg, Sergio Lifschitz', 'link': 'https://arxiv.org/abs/2412.17964', 'abstract': "We propose a methodology that combines several advanced techniques in Large Language Model (LLM) retrieval to support the development of robust, multi-source question-answer systems. This methodology is designed to integrate information from diverse data sources, including unstructured documents (PDFs) and structured databases, through a coordinated multi-agent orchestration and dynamic retrieval approach. Our methodology leverages specialized agents-such as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents - that dynamically select the most appropriate retrieval strategy based on the nature of each query. To further improve accuracy and contextual relevance, we employ dynamic prompt engineering, which adapts in real time to query-specific contexts. The methodology's effectiveness is demonstrated within the domain of Contract Management, where complex queries often require seamless interaction between unstructured and structured data. Our results indicate that this approach enhances response accuracy and relevance, offering a versatile and scalable framework for developing question-answer systems that can operate across various domains and data sources.", 'abstract_zh': '我们提出了一种方法论，结合了大型语言模型（LLM）检索中的多项高级技术，以支持稳健的多源问答系统的开发。该方法论旨在通过协调多代理编排和动态检索方法，整合来自多种数据源的信息，包括未结构化的文档（如PDF）和结构化的数据库。此方法论利用了专用于SQL代理、检索增强生成（RAG）代理和路由器代理等特定任务的智能代理，它们能够根据每个查询的性质动态选择最合适的检索策略。为了进一步提高准确性和上下文相关性，我们采用了动态提示工程，它能够根据查询的具体上下文实时调整。该方法论在合同管理领域得到了验证，在该领域复杂的查询往往需要无缝地处理未结构化和结构化数据之间的交互。实验结果表明，该方法增强了响应的准确性和相关性，提供了一个在不同领域和数据源上操作的灵活和可扩展框架，用于开发问答系统。', 'title_zh': '使用大型语言模型的多源问答系统中动态多agent编排与检索方法'}
{'arxiv_id': 'arXiv:2412.17948', 'title': 'Study of the Proper NNUE Dataset', 'authors': 'Daniel Tan, Neftali Watkinson Medina', 'link': 'https://arxiv.org/abs/2412.17948', 'abstract': 'NNUE (Efficiently Updatable Neural Networks) has revolutionized chess engine development, with nearly all top engines adopting NNUE models to maintain competitive performance. A key challenge in NNUE training is the creation of high-quality datasets, particularly in complex domains like chess, where tactical and strategic evaluations are essential. However, methods for constructing effective datasets remain poorly understood and under-documented. In this paper, we propose an algorithm for generating and filtering datasets composed of "quiet" positions that are stable and free from tactical volatility. Our approach provides a clear methodology for dataset creation, which can be replicated and generalized across various evaluation functions. Testing demonstrates significant improvements in engine performance, confirming the effectiveness of our method.', 'abstract_zh': 'NNUE（高效可更新的神经网络）已经彻底改变了象棋引擎的研发，几乎所有顶级引擎都采用了NNUE模型以维持竞争力。在NNUE训练中，一个关键挑战是如何创建高质量的数据集，尤其是在如象棋这样复杂的领域，其中战术和战略评估至关重要。然而，有效的数据集构建方法仍然 poorly understood（理解不足）且记录不足。在本文中，我们提出了一种算法，用于生成和过滤由“宁静”位置组成的数据集，这些位置稳定且不受战术波动的影响。我们的方法为数据集的创建提供了一种清晰的步骤，这些步骤可以在各种评估函数中复制并泛化。实验结果显示，我们的方法显著提高了引擎的性能，证实了其有效性。', 'title_zh': 'NNUE 数据集选择研究'}
{'arxiv_id': 'arXiv:2412.17944', 'title': 'Surveillance Capitalism Revealed: Tracing The Hidden World Of Web Data Collection', 'authors': 'Antony Seabra de Medeiros, Luiz Afonso Glatzl Junior, Sergio Lifschitz', 'link': 'https://arxiv.org/abs/2412.17944', 'abstract': 'This study investigates the mechanisms of Surveillance Capitalism, focusing on personal data transfer during web navigation and searching. Analyzing network traffic reveals how various entities track and harvest digital footprints. The research reveals specific data types exchanged between users and web services, emphasizing the sophisticated algorithms involved in these processes. We present concrete evidence of data harvesting practices and propose strategies for enhancing data protection and transparency. Our findings highlight the need for robust data protection frameworks and ethical data usage to address privacy concerns in the digital age.', 'abstract_zh': '本研究探讨了监视资本主义的机制，重点关注网络浏览和搜索过程中的个人数据传输。通过分析网络流量，揭示了各种实体如何跟踪和收集数字足迹。研究结果显示了用户与网络服务之间交换的具体数据类型，并强调了这些过程中涉及的复杂算法。我们提供了数据收集实践的确切证据，并提出了增强数据保护和透明度的建议。研究结果强调了在数字时代建立 robust 数据保护框架以及采取伦理数据使用措施以应对隐私担忧的重要性。', 'title_zh': 'surveillance资本主义揭示：追踪网络数据收集的隐秘世界'}
{'arxiv_id': 'arXiv:2412.17942', 'title': 'Contrato360 2.0: A Document and Database-Driven Question-Answer System using Large Language Models and Agents', 'authors': 'Antony Seabra, Claudio Cavalcante, Joao Nepomuceno, Lucas Lago, Nicolaas Ruberg, Sergio Lifschitz', 'link': 'https://arxiv.org/abs/2412.17942', 'abstract': 'We present a question-and-answer (Q\\&A) application designed to support the contract management process by leveraging combined information from contract documents (PDFs) and data retrieved from contract management systems (database). This data is processed by a large language model (LLM) to provide precise and relevant answers. The accuracy of these responses is further enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL techniques, and agents that dynamically orchestrate the workflow. These techniques eliminate the need to retrain the language model. Additionally, we employed Prompt Engineering to fine-tune the focus of responses. Our findings demonstrate that this multi-agent orchestration and combination of techniques significantly improve the relevance and accuracy of the answers, offering a promising direction for future information systems.', 'abstract_zh': '我们提出了一种基于问题回答（Q&A）的应用程序，旨在通过利用合同文件（PDFs）和从合同管理系统中检索的数据（数据库）的综合信息来支持合同管理流程。这些数据由大型语言模型（LLM）处理，以提供精准和相关的答案。通过使用检索增强生成（RAG）、文本到SQL技术以及能够动态调度工作流程的代理，这些答案的准确性得以进一步提高。这些技术消除了重新训练语言模型的必要性。此外，我们采用了提示工程技术来精细调整答案的焦点。我们的研究结果表明，这种多代理调度及其技术组合显著提高了答案的相关性和准确性，并为未来的信息系统提供了令人鼓舞的方向。', 'title_zh': 'Contrato360 2.0：一种基于文档和数据库的大型语言模型及智能代理驱动的问答系统'}
{'arxiv_id': 'arXiv:2412.17920', 'title': 'Causal Composition Diffusion Model for Closed-loop Traffic Generation', 'authors': 'Haohong Lin, Xin Huang, Tung Phan-Minh, David S. Hayden, Huan Zhang, Ding Zhao, Siddhartha Srinivasa, Eric M. Wolff, Hongge Chen', 'link': 'https://arxiv.org/abs/2412.17920', 'abstract': "Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort.", 'abstract_zh': '模拟对于自动驾驶的安全评估至关重要，特别是在捕捉复杂的交互行为方面。然而，在长尾情况中生成具有现实性和可控性的交通场景仍然是一个重大挑战。现有的生成模型在用户定义的可控性和现实性约束之间存在冲突目标的矛盾，这种矛盾在安全性关键的背景下被进一步放大。在本文中，我们引入了因果组成扩散模型（CCDiff），这是一种结构引导的扩散框架，以解决这些挑战。我们首先将可控且现实的闭环模拟的学习形式化为一个约束优化问题。然后，CCDiff通过自动识别并直接注入因果结构到扩散过程中，最大限度地提高可控性并遵守现实性，提供结构化的指导以增强现实性和可控性。通过在基准数据集和闭环模拟器上的严格评估，CCDiff在生成现实且用户偏好的轨迹方面优于最先进的方法。我们的结果展示了CCDiff在提取和利用因果结构方面的有效性，并且基于碰撞率、离路率、FDE和舒适性等关键指标，展示了改善的闭环性能。', 'title_zh': '因果 composition 式扩散模型在闭环交通生成中的应用'}
{'arxiv_id': 'arXiv:2412.17859', 'title': 'The Unreasonable Effectiveness of Open Science in AI: A Replication Study', 'authors': 'Odd Erik Gundersen, Odd Cappelen, Martin Mølnå, Nicklas Grimstad Nilsen', 'link': 'https://arxiv.org/abs/2412.17859', 'abstract': 'A reproducibility crisis has been reported in science, but the extent to which it affects AI research is not yet fully understood. Therefore, we performed a systematic replication study including 30 highly cited AI studies relying on original materials when available. In the end, eight articles were rejected because they required access to data or hardware that was practically impossible to acquire as part of the project. Six articles were successfully reproduced, while five were partially reproduced. In total, 50% of the articles included was reproduced to some extent. The availability of code and data correlate strongly with reproducibility, as 86% of articles that shared code and data were fully or partly reproduced, while this was true for 33% of articles that shared only data. The quality of the data documentation correlates with successful replication. Poorly documented or miss-specified data will probably result in unsuccessful replication. Surprisingly, the quality of the code documentation does not correlate with successful replication. Whether the code is poorly documented, partially missing, or not versioned is not important for successful replication, as long as the code is shared. This study emphasizes the effectiveness of open science and the importance of properly documenting data work.', 'abstract_zh': '科学研究中已经报告存在再现性危机，但这种危机在AI研究中的影响程度尚不完全清楚。因此，我们进行了一项系统再现性研究，涉及30篇高引AI研究论文，并尽可能利用原始材料。最终，有8篇论文因需访问项目内实际上难以获取的数据或硬件而被拒绝。有6篇论文被完全再现，5篇论文被部分再现。总体而言，50%的论文在某种程度上得到了再现。代码和数据的可用性与再现性密切相关：86%共享代码和数据的论文被完全或部分再现，而仅共享数据的论文中这一比例为33%。数据文档的质量与成功再现相关。 poorly 记录或说明不清的数据可能会导致再现不成功。令人惊讶的是，代码文档的质量与成功再现无关。只要代码共享，无论其记录质量如何、是否部分缺失或未经版本管理，都不影响成功再现。这项研究强调了开放科学的有效性和正确记录数据工作的重要性。', 'title_zh': '开放科学在人工智能领域的非凡有效性：一项复制研究'}
{'arxiv_id': 'arXiv:2412.17847', 'title': 'Bridging the Data Provenance Gap Across Text, Speech and Video', 'authors': 'Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara', 'link': 'https://arxiv.org/abs/2412.17847', 'abstract': 'Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.', 'abstract_zh': '人工智能的进步主要受到训练数据规模和质量的推动。尽管如此，对广泛认可的数据集（尤其是文本之外的数据集）的属性进行实证分析的研究仍存在不足。本文首次对跨多种模态——流行文本、语音和视频数据集——进行全面的纵向审计，涵盖了从详细的数据来源趋势和使用限制到地理和语言代表性。我们的手动分析覆盖了1990年至2024年间近4000个公共数据集，涉及608种语言、798个数据来源、659个组织机构以及67个国家和地区。我们发现，多模态机器学习应用程序普遍转向了网页抓取数据、合成数据和社交媒体平台（如YouTube）作为其训练集来源，自2019年以来，这些来源已超越所有其他来源。其次，在追踪数据集的衍生链时，我们发现虽然只有不到33%的数据集受到限制性许可，但在广泛使用的文本、语音和视频数据集中，超过80%的原始内容带有非商业限制。最后，尽管公共AI训练数据集中代表的语言和地理范围在增加，我们的审计结果显示，自2013年以来，地域和多语言代表性的相对衡量标准未能显著改善其覆盖率。我们相信，广泛而全面的审计使我们能够从生态系统层面实证分析数据来源、限制和西方中心主义的趋势，并认为这些问题的透明度对负责任的人工智能的发展至关重要。作为持续改进数据集透明度和负责任使用的贡献，我们发布了整个多模态审计结果，使实践者能够跨文本、语音和视频数据追溯数据来源。', 'title_zh': '跨越文本、语音和视频数据来源差距的研究'}
{'arxiv_id': 'arXiv:2412.18601', 'title': 'Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems', 'authors': 'Fernando Jia, Jade Zheng, Florence Li', 'link': 'https://arxiv.org/abs/2412.18601', 'abstract': "In the rapidly evolving landscape of GameFi, a fusion of gaming and decentralized finance (DeFi), there exists a critical need to enhance player engagement and economic interaction within gaming ecosystems. Our GameFi ecosystem aims to fundamentally transform this landscape by integrating advanced embodied AI agents into GameFi platforms. These AI agents, developed using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI, are capable of proactive, adaptive, and contextually rich interactions with players. By going beyond traditional scripted responses, these agents become integral participants in the game's narrative and economic systems, directly influencing player strategies and in-game economies. We address the limitations of current GameFi platforms, which often lack immersive AI interactions and mechanisms for community engagement or creator monetization. Through the deep integration of AI agents with blockchain technology, we establish a consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers creators to monetize their contributions and fosters democratic collaboration among players and creators. Furthermore, by embedding DeFi mechanisms into the gaming experience, we enhance economic participation and provide new opportunities for financial interactions within the game. Our approach enhances player immersion and retention and advances the GameFi ecosystem by bridging traditional gaming with Web3 technologies. By integrating sophisticated AI and DeFi elements, we contribute to the development of more engaging, economically robust, and community-centric gaming environments. This project represents a significant advancement in the state-of-the-art in GameFi, offering insights and methodologies that can be applied throughout the gaming industry.", 'abstract_zh': '在GameFi这一快速发展的领域中，GameFi结合了游戏和去中心化金融（DeFi），存在着增强玩家参与度和经济互动的迫切需求。我们的GameFi生态系统旨在通过将先进的具身AI代理整合到GameFi平台中，从根本上改变这一现状。这些AI代理采用最新的人工智能语言模型（如GPT-4和Claude AI）开发，能够在与玩家的互动中展现出主动、适应性及丰富的情境交互能力。通过超越传统的预设回应，这些代理能够成为游戏叙事和经济系统的积极参与者，直接影响玩家的策略和游戏内的经济体系。我们解决了当前GameFi平台中常见的沉浸式AI互动缺乏和社区参与机制不足的问题。通过将AI代理与区块链技术深度融合，我们建立了一个基于共识的去中心化GameFi生态系统。该生态系统赋予创作者们通过自己的贡献获取收益的能力，并促进了玩家和创作者之间的民主化合作。此外，通过将DeFi机制嵌入游戏体验中，我们增强了经济参与度并为游戏内的金融互动提供了新的机会。我们的方法增强了玩家的沉浸感和留存率，并通过将传统游戏与Web3技术相结合，推动了GameFi生态系统的进步。通过整合复杂的AI和DeFi元素，我们为更具吸引力、经济稳定性更强且更注重社区的 gaming环境做出了贡献。本项目代表了GameFi领域的重大进展，提供了可用于整个游戏行业的见解和方法论。', 'title_zh': 'GameFi中去中心化智能：具身人工智能代理及其与去中心化金融和虚拟生态系统融合的研究'}
{'arxiv_id': 'arXiv:2412.18597', 'title': 'DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation', 'authors': 'Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, Xiangyu Yue', 'link': 'https://arxiv.org/abs/2412.18597', 'abstract': "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.", 'abstract_zh': '类似于Sora的视频生成模型已经通过多模态扩散变换器（MM-DiT）架构取得了显著进展。然而，当前的视频生成模型主要侧重于单指令生成，难以生成由多个序列指令构成的连贯场景，而这些场景更能反映现实世界的动态状况。尽管有些开创性的工作已经探索了多指令视频生成，但它们仍然面临严峻的挑战，包括严格的训练数据要求、弱指令跟随能力和不自然的过渡。为了应对这些问题，我们提出了DiTCtrl，这是首次在MM-DiT架构下提出的一种无需训练的多指令视频生成方法。我们的核心思想是将多指令视频生成任务视为具有平滑过渡的时序视频编辑。为了实现这一目标，我们首先分析了MM-DiT的注意力机制，发现其3D全注意力与UNet类扩散模型中的交叉/自注意力模块类似，这使得在多指令视频生成中能够通过注意力共享实现掩码引导的精确语义控制。基于我们的精心设计，DiTCtrl生成的视频在多个连续指令下实现了平滑过渡和一致的对象运动，无需额外训练。此外，我们还提出了MPVBench，这是一种专门设计用于多指令视频生成的新基准，用于评估多指令生成的表现。广泛实验表明，我们的方法在无需额外训练的情况下达到了最先进的性能。', 'title_zh': 'DiTCtrl：在多模态扩散变换器中探索注意力控制以实现无调优多提示长视频生成'}
{'arxiv_id': 'arXiv:2412.18588', 'title': 'A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs', 'authors': 'OpenMind, Shaohong Zhong, Adam Zhou, Boyuan Chen, Homin Luo, Jan Liphardt', 'link': 'https://arxiv.org/abs/2412.18588', 'abstract': "Large Language Models (LLMs) are compact representations of all public knowledge of our physical environment and animal and human behaviors. The application of LLMs to robotics may offer a path to highly capable robots that perform well across most human tasks with limited or even zero tuning. Aside from increasingly sophisticated reasoning and task planning, networks of (suitably designed) LLMs offer ease of upgrading capabilities and allow humans to directly observe the robot's thinking. Here we explore the advantages, limitations, and particularities of using LLMs to control physical robots. The basic system consists of four LLMs communicating via a human language data bus implemented via web sockets and ROS2 message passing. Surprisingly, rich robot behaviors and good performance across different tasks could be achieved despite the robot's data fusion cycle running at only 1Hz and the central data bus running at the extremely limited rates of the human brain, of around 40 bits/s. The use of natural language for inter-LLM communication allowed the robot's reasoning and decision making to be directly observed by humans and made it trivial to bias the system's behavior with sets of rules written in plain English. These rules were immutably written into Ethereum, a global, public, and censorship resistant Turing-complete computer. We suggest that by using natural language as the data bus among interacting AIs, and immutable public ledgers to store behavior constraints, it is possible to build robots that combine unexpectedly rich performance, upgradability, and durable alignment with humans.", 'abstract_zh': '大规模语言模型（LLMs）是对我们物理环境以及动物和人类行为的全部公共知识的紧凑表示。将LLMs应用于机器人技术可能为创建高度能胜任、能够在大多数人类任务中表现出色的机器人提供一条路径，即使缺乏或完全没有调优。除了日益复杂的推理和任务规划外，适当地设计的LLM网络提供了能力更新的简便性和允许人类直接观察机器人思维的独特性。本文探讨了使用LLMs控制物理机器人的优势、局限性和特定性。基本系统由四个通过WebSocket和ROS2消息传递实现的人类语言数据总线进行通信的LLM组成。令人惊讶的是，尽管机器人数据融合周期仅为每秒1Hz，中央数据总线的运行速率也极其受限，大约为人类大脑的40比特/秒，但仍然能够实现丰富的机器人行为和不同任务的良好表现。自然语言作为LLM间通信介质使得人类可以直接观察机器人的推理和决策过程，并且通过以简单英语撰写的规则集轻松地偏置系统的行为。这些规则被不可变地写入以太坊中，这是一个全球性的、公开的和抗审查的图灵完备计算机。我们建议，通过在交互的AI之间使用自然语言作为数据总线，并使用不可变的公共账本来存储行为约束，有可能构建出结合丰富性能、易于更新和持久的人类对齐的机器人。', 'title_zh': '一节文字足矣：来自相互信任的大型语言模型的丰富机器人行为'}
{'arxiv_id': 'arXiv:2412.18573', 'title': 'How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation', 'authors': 'Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, Zibin Zheng', 'link': 'https://arxiv.org/abs/2412.18573', 'abstract': "Recently, an increasing number of AI-driven programming assistants powered by code LLMs have been integrated into various real-world software development environments, significantly boosting developer productivity. However, existing code generation benchmarks primarily focus on general-purpose scenarios, leaving the code generation performance of LLMs for specific application domains largely unknown. In this paper, we introduce a new benchmark, MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming tasks, covering 12 popular software development domains and 15 programming languages. Specifically, we perform in-depth research to identify these 12 application domains. Given that each domain may involve multiple technical frameworks, and that different frameworks present distinct challenges in the coding process, we categorize the commonly used frameworks and platforms within each domain. We then sample programming problems from GitHub repositories related to these subdomains. To ensure the quality of the tasks and mitigate data leakage issues, we invite annotators to rewrite the docstrings for each task in MultiCodeBench. Additionally, we build a static analysis-based dependency parsing tool to extract the dependencies in the ground truth for each task, enabling deeper performance analysis. Through extensive experiments on MultiCodeBench with eleven representative mainstream LLMs, we reveal the code generation performance of the LLMs across different application domains, providing practical insights for developers in downstream fields when selecting LLMs. Furthermore, we analyze the reasons behind the models' failures in completing software application development tasks, offering guidance for model developers to enhance domain-specific code generation capabilities.", 'abstract_zh': '近年来，由代码大规模语言模型（code LLMs）驱动的AI编程助手被集成到各种实际软件开发环境中，显著提高了开发者的生产效率。然而，现有的代码生成基准主要集中在通用场景上，使得LLMs在特定应用领域的代码生成性能仍然不明确。本文介绍了一个新的基准测试——MultiCodeBench，以填补这一空白。MultiCodeBench 包含2400个编程任务，涵盖了12个主流软件开发领域和15种编程语言。具体而言，我们进行了深入的研究来确定这12个应用领域。鉴于每个领域可能涉及多个技术框架，而不同的框架在编程过程中各有其独特的挑战，我们对每个领域的常用框架和平台进行了分类。然后，我们从与这些子领域相关的GitHub仓库中抽取编程问题。为了确保任务质量并解决数据泄露问题，我们邀请注释员为MultiCodeBench中的每个任务重写注释文档。此外，我们构建了一个基于静态分析的依赖解析工具，用于提取每个任务真实依赖关系，从而实现更深入的性能分析。通过在MultiCodeBench上对11个主流LLM进行广泛的实验，我们揭示了在不同应用领域LLM的代码生成性能，为下游开发者选择LLM提供了实用见解。此外，我们分析了模型在完成软件应用开发任务时失败的原因，为模型开发者提供指导，以增强特定领域代码生成能力。', 'title_zh': '不同应用领域的代码生成能力：大型语言模型的基准测试与评估'}
{'arxiv_id': 'arXiv:2412.18547', 'title': 'Token-Budget-Aware LLM Reasoning', 'authors': 'Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang', 'link': 'https://arxiv.org/abs/2412.18547', 'abstract': 'Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: this https URL.', 'abstract_zh': '逻辑推理对于大型语言模型（LLM）在广泛的任务中出色表现至关重要。虽然像思维链（Chain-of-Thought, CoT）推理这样的方法通过将问题分解为中间步骤来提升LLM的性能，但这也导致了显著的令牌使用量增加，进而增加了成本。我们发现当前LLM的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择在实际压缩效果中扮演着至关重要的角色。随后，我们提出了一种令牌预算感知的LLM推理框架，该框架根据推理复杂性动态估计不同问题的令牌预算，并利用估计的令牌预算来指导推理过程。实验结果表明，我们的方法在轻微性能下降的情况下有效减少了CoT推理中的令牌成本，提供了一种平衡效率和准确性的实际解决方案。代码：this https URL。', 'title_zh': '面向Token预算的大型语言模型推理'}
{'arxiv_id': 'arXiv:2412.18545', 'title': 'Advancing Deformable Medical Image Registration with Multi-axis Cross-covariance Attention', 'authors': 'Mingyuan Meng, Michael Fulham, Lei Bi, Jinman Kim', 'link': 'https://arxiv.org/abs/2412.18545', 'abstract': 'Deformable image registration is a fundamental requirement for medical image analysis. Recently, transformers have been widely used in deep learning-based registration methods for their ability to capture long-range dependency via self-attention (SA). However, the high computation and memory loads of SA (growing quadratically with the spatial resolution) hinder transformers from processing subtle textural information in high-resolution image features, e.g., at the full and half image resolutions. This limits deformable registration as the high-resolution textural information is crucial for finding precise pixel-wise correspondence between subtle anatomical structures. Cross-covariance Attention (XCA), as a "transposed" version of SA that operates across feature channels, has complexity growing linearly with the spatial resolution, providing the feasibility of capturing long-range dependency among high-resolution image features. However, existing XCA-based transformers merely capture coarse global long-range dependency, which are unsuitable for deformable image registration relying primarily on fine-grained local correspondence. In this study, we propose to improve existing deep learning-based registration methods by embedding a new XCA mechanism. To this end, we design an XCA-based transformer block optimized for deformable medical image registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general network block that can be embedded into various registration network architectures. It can capture both global and local long-range dependency among high-resolution image features by applying regional and dilated XCA in parallel via a multi-axis design. Extensive experiments on two well-benchmarked inter-/intra-patient registration tasks with seven public medical datasets demonstrate that our MAXCA block enables state-of-the-art registration performance.', 'abstract_zh': '可变形图像配准是医学图像分析的基本要求。近年来，Transformer因其能通过自注意力机制（SA）捕捉长程依赖性而在基于深度学习的配准方法中得到了广泛应用。然而，SA的高计算和内存负载（随着空间分辨率的增加呈二次增长）阻碍了Transformer处理高分辨率图像特征中的微细纹理信息，例如在全分辨率和半分辨率图像中。这限制了可变形配准的效果，因为高分辨率的纹理信息对于精确匹配细微解剖结构至关重要。共方差注意机制（XCA），作为一种“转置”版本的自注意力机制，能够在特征通道之间操作，其复杂度随着空间分辨率线性增长，为捕捉高分辨率图像特征之间的长程依赖性提供了可能性。然而，现有的基于XCA的Transformer仅能捕捉粗略的整体长程依赖性，这类依赖性对主要依赖局部细粒度对应关系的可变形图像配准来说不合适。在本研究中，我们提出通过嵌入新的XCA机制来改进现有的基于深度学习的配准方法。为此，我们设计了一个专为可变形医学图像配准优化的基于XCA的Transformer模块，命名为多轴XCA（MAXCA）。MAXCA作为一个通用网络模块，可以嵌入到各种配准网络架构中。通过多轴设计并行应用区域性和膨胀性的XCA，它能够同时捕捉高分辨率图像特征之间的全局和局部长程依赖性。在两个广泛应用的标准跨/同患者配准任务中，使用七个公开的医学数据集进行的广泛实验表明，我们的MAXCA模块能够实现最先进的配准性能。', 'title_zh': '基于多轴交叉协方差注意力的可变形医学图像配准进展'}
{'arxiv_id': 'arXiv:2412.18544', 'title': 'Consistency Checks for Language Model Forecasters', 'authors': 'Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr', 'link': 'https://arxiv.org/abs/2412.18544', 'abstract': "Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters instantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on arbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur can trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.", 'abstract_zh': '预测任务评价极具挑战性：真实情况只能在未来揭晓。近期研究表明，大型语言模型（LLM）预测器正迅速接近人类水平的表现，这就提出了一个问题：我们如何能够即时地对这些预测器进行基准测试和评价？我们基于一致性检验框架，从预测在逻辑上相关的问题中的一致性来衡量预测器的表现。我们提出了一种基于套利的新的一般一致性度量方法：例如，如果预测AI非理性地预测两党（民主党与共和党）都有60%的机会赢得2024年美国总统大选，套利者可以通过交易对预测者的预测进行套利并获得利润。我们构建了一个自动化评价系统，该系统生成一组基础问题，从这些问题中实例化一致性检验，获取预测者的预测，并测量这些预测的一致性。然后，我们构建了一个标准的概率评分规则预测基准，并展示了我们的（即时）一致性度量与未来才知道的真实Brier评分（即预测的真实评估指标）之间的相关性。我们还发布了在2028年解决的评价基准，提供了一个长期评价工具，以评估预测性能。', 'title_zh': '语言模型预测器的一致性检查'}
{'arxiv_id': 'arXiv:2412.18530', 'title': 'Characterizations of Language Generation With Breadth', 'authors': 'Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas', 'link': 'https://arxiv.org/abs/2412.18530', 'abstract': "We study language generation in the limit, introduced by Kleinberg and Mullainathan [KM24], building on classical works of Gold [Gol67] and Angluin [Ang79]. [KM24] proposed an algorithm that generates strings from any countable language collection in the limit. While their algorithm eventually outputs strings from the target language $K$, it sacrifices breadth, i.e., the ability to generate all strings in $K$. A key open question in [KM24] is whether this trade-off between consistency and breadth is inherrent.\nRecent works proposed different notions of consistent generation with breadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced three definitions: generation with exact breadth, approximate breadth, and unambiguous generation. Concurrently and independently, Charikar and Pabbaraju [CP24a] proposed exhaustive generation. Both works examined when generation with these notions of breadth is possible.\nBuilding on [CP24a, KVM24], we fully characterize language generation for these notions and their natural combinations. For exact breadth, we provide an unconditional lower bound, removing a technical condition from [KVM24] and extending the result of [CP24a] that holds for specific collections of languages. We show that generation with exact breadth is characterized by Angluin's condition for identification. We further introduce a weaker version of Angluin's condition that tightly characterizes both approximate breadth and exhaustive generation, proving their equivalence. Additionally, we show that unambiguous generation is also characterized by Angluin's condition as a special case of a broader result. Finally, we strengthen [KVM24] by giving unconditional lower bounds for stable generators, showing that Angluin's condition characterizes the previous breadth notions for stable generators. This shows a separation between stable and unstable generation with approximate breadth.", 'abstract_zh': '我们研究由Kleinberg和Mullainathan在[KM24]中引入的在极限下生成语言的方法，该方法基于Gold的经典工作[Gol67]和Angluin的经典工作[Ang79]。[KM24]提出了一种算法，可以从任何可数的语言集合中生成字符串。虽然该算法最终会输出目标语言$K$中的字符串，但它牺牲了广度，即生成$K$中所有字符串的能力。[KM24]中的一个关键开问题在于这种一致性和广度之间的权衡是否不可避免。\n\n近期的研究提出了不同类型的一致且有广度的生成概念。Kalavasis、Mehrotra和Velegkas在[KVM24]中引入了三种定义：精确广度生成、近似广度生成和无歧义生成。同时，Charikar和Pabbaraju在[CP24a]中提出了完全生成的概念。两组研究分别探讨了这些广度概念及它们组合的生成可能性。\n\n基于[CP24a, KVM24]，我们完全刻画了这些广度概念及其自然组合下的语言生成方法。对于精确广度生成，我们提供了无条件的下界，消除了[KVM24]中的一个技术条件，并扩展了[CP24a]中对于特定语言集合的有效结果。我们证明了精确广度生成可以由Angluin的识别条件来刻画。我们进一步引入了一个较弱的Angluin条件版本，该条件精确地刻画了近似广度生成和完全生成，并证明了它们的等价性。此外，我们展示了无歧义生成也符合Angluin条件作为更广泛结果的一个特例。最后，我们增强了[KVM24]，通过给出稳定的生成器的无条件下界证明，表明Angluin条件也刻画了稳定的广度概念，并展示了在近似广度生成中稳定生成与不稳定生成之间的差异。', 'title_zh': '语言生成的广泛性特征研究'}
{'arxiv_id': 'arXiv:2412.18500', 'title': 'Joint Adaptive OFDM and Reinforcement Learning Design for Autonomous Vehicles: Leveraging Age of Updates', 'authors': 'Mamady Delamou, Ahmed Naeem, Huseyin Arslan, El Mehdi Amhoud', 'link': 'https://arxiv.org/abs/2412.18500', 'abstract': "Millimeter wave (mmWave)-based orthogonal frequency-division multiplexing (OFDM) stands out as a suitable alternative for high-resolution sensing and high-speed data transmission. To meet communication and sensing requirements, many works propose a static configuration where the wave's hyperparameters such as the number of symbols in a frame and the number of frames in a communication slot are already predefined. However, two facts oblige us to redefine the problem, (1) the environment is often dynamic and uncertain, and (2) mmWave is severely impacted by wireless environments. A striking example where this challenge is very prominent is autonomous vehicle (AV). Such a system leverages integrated sensing and communication (ISAC) using mmWave to manage data transmission and the dynamism of the environment. In this work, we consider an autonomous vehicle network where an AV utilizes its queue state information (QSI) and channel state information (CSI) in conjunction with reinforcement learning techniques to manage communication and sensing. This enables the AV to achieve two primary objectives: establishing a stable communication link with other AVs and accurately estimating the velocities of surrounding objects with high resolution. The communication performance is therefore evaluated based on the queue state, the effective data rate, and the discarded packets rate. In contrast, the effectiveness of the sensing is assessed using the velocity resolution. In addition, we exploit adaptive OFDM techniques for dynamic modulation, and we suggest a reward function that leverages the age of updates to handle the communication buffer and improve sensing. The system is validated using advantage actor-critic (A2C) and proximal policy optimization (PPO). Furthermore, we compare our solution with the existing design and demonstrate its superior performance by computer simulations.", 'abstract_zh': '基于毫米波（mmWave）的正交频分复用（OFDM）技术在高分辨率传感和高速数据传输方面表现出色，是合适的替代方案。为了满足通信和传感需求，许多研究提出了静态配置，其中波的超参数，如每个帧中的符号数和通信时间槽中的帧数已经预先定义。然而，两个事实迫使我们必须重新定义问题：（1）环境通常是动态和不确定的；（2）mmWave严重受无线环境的影响。这一挑战在自主驾驶车辆（AV）系统中表现尤为突出。该系统利用mmWave进行集成传感与通信（ISAC），以管理和适应环境变化。本研究考虑了一个AV网络，其中AV结合队列状态信息（QSI）和信道状态信息（CSI）与强化学习技术来管理通信和传感。这使AV能够实现两个主要目标：与其他AV建立稳定的通信链路，并以高分辨率准确估计周围物体的速度。因此，通信性能的评估基于队列状态、有效数据速率和丢弃的包速率。相比之下，传感效果的评估使用速度分辨率。此外，我们采用自适应OFDM技术进行动态调制，建议一种奖励函数，利用更新的年龄来处理通信缓冲区并提高传感效果。系统通过优势演员-评论家算法（A2C）和近端策略优化（PPO）进行验证。此外，我们将我们的解决方案与现有设计进行比较，并通过计算机仿真展示了其优越性能。', 'title_zh': '自动驾驶车辆中联合自适应OFDM与强化学习设计：利用更新时延'}
{'arxiv_id': 'arXiv:2412.18495', 'title': 'How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System?', 'authors': 'Sara Papi, Peter Polak, Ondřej Bojar, Dominik Macháček', 'link': 'https://arxiv.org/abs/2412.18495', 'abstract': "Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker's speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends, and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions.", 'abstract_zh': '同时进行语音到文本翻译（SimulST）是在演讲者发言的同时将其源语言语音翻译成目标语言文本，以确保低延迟，从而提高用户的理解能力。尽管SimulST旨在处理无限制的语音，但大多数研究都集中在人类预先分段的语音上，简化了任务并忽视了重要的挑战。这种狭窄的研究焦点以及广泛的术语不一致，限制了研究结果在实际应用中的适用性，最终妨碍了该领域的发展。我们对110篇论文的广泛文献综述不仅揭示了当前研究中的关键问题，也为我们的主要贡献奠定了基础。我们1）定义了SimulST系统的步骤及其核心组件，提出了标准化的术语和分类体系；2）对社区趋势进行了深入分析；3）提供了具体的建议和未来方向，以弥合现有文献中的空白，从评估框架到系统架构，推动领域向更现实和有效的SimulST解决方案发展。', 'title_zh': '你的实时同步语音转文字翻译系统有多“真实”？'}
{'arxiv_id': 'arXiv:2412.18489', 'title': 'An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving', 'authors': 'Gnaneswar Villuri, Alex Doboli', 'link': 'https://arxiv.org/abs/2412.18489', 'abstract': 'This report characterized the suitability of existing datasets for devising new Machine Learning models, decision making methods, and analysis algorithms to improve Collaborative Problem Solving and then enumerated requirements for future datasets to be devised. Problem solving was assumed to be performed in teams of about three, four members, which talked to each other. A dataset consists of the speech recordings of such teams. The characterization methodology was based on metrics that capture cognitive, social, and emotional activities and situations. The report presented the analysis of a large group of datasets developed for Spoken Language Understanding, a research area with some similarity to Collaborative Problem Solving.', 'abstract_zh': '本报告分析了现有数据集适用于设计新的机器学习模型、决策方法和分析算法以改进协作解决问题的能力，并罗列了未来所需数据集的要求。假设问题解决是在约三至四人的团队中进行的，团队成员之间相互交流。该数据集包含此类团队的语音录制。分析方法基于能够捕捉认知活动、社交活动和情绪状态的指标。报告对大量用于语音理解的研究领域（该领域与协作解决问题有一些相似之处）的数据集进行了分析。', 'title_zh': '现有语音数据集在训练用于集体解决问题的机器学习模型方面的适用性综述与讨论'}
{'arxiv_id': 'arXiv:2412.18460', 'title': 'GeFL: Model-Agnostic Federated Learning with Generative Models', 'authors': 'Honggu Kang, Seohyeon Cha, Joonhyuk Kang', 'link': 'https://arxiv.org/abs/2412.18460', 'abstract': 'Federated learning (FL) is a promising paradigm in distributed learning while preserving the privacy of users. However, the increasing size of recent models makes it unaffordable for a few users to encompass the model. It leads the users to adopt heterogeneous models based on their diverse computing capabilities and network bandwidth. Correspondingly, FL with heterogeneous models should be addressed, given that FL typically involves training a single global model. In this paper, we propose Generative Model-Aided Federated Learning (GeFL), incorporating a generative model that aggregates global knowledge across users of heterogeneous models. Our experiments on various classification tasks demonstrate notable performance improvements of GeFL compared to baselines, as well as limitations in terms of privacy and scalability. To tackle these concerns, we introduce a novel framework, GeFL-F. It trains target networks aided by feature-generative models. We empirically demonstrate the consistent performance gains of GeFL-F, while demonstrating better privacy preservation and robustness to a large number of clients. Codes are available at [1].', 'abstract_zh': '联邦学习（FL）是一种在保持用户隐私的同时进行分布式学习的有前途范式。然而，近期模型的不断增大使得少数用户承担整个模型变得不可行。这导致用户采用基于不同计算能力和网络带宽的异质模型。鉴于FL通常涉及训练单一全局模型，异质模型下的联邦学习应予以关注。在本文中，我们提出了一种名为生成模型辅助联邦学习（GeFL）的方法，结合了一个生成模型，该模型可以汇总异质模型用户之间的全局知识。在各种分类任务实验中，GeFL相比基准方法表现出显著的性能提升，并在隐私和可扩展性方面存在一定的限制。为应对这些挑战，我们引入了一种新的框架GeFL-F。该框架通过特征生成模型辅助目标网络的训练。通过实验证明，GeFL-F能够保持一致的性能提升，并且在隐私保护和对大量客户端的鲁棒性方面表现出色。相关代码在 [1] 中提供。', 'title_zh': 'GeFL：基于生成模型的无模型依赖联邦学习'}
{'arxiv_id': 'arXiv:2412.18442', 'title': 'SoK: On the Offensive Potential of AI', 'authors': 'Saskia Laura Schröer, Giovanni Apruzzese, Soheil Human, Pavel Laskov, Hyrum S. Anderson, Edward W. N. Bernroider, Aurore Fass, Ben Nassi, Vera Rimmer, Fabio Roli, Samer Salam, Ashley Shen, Ali Sunyaev, Tim Wadwha-Brown, Isabel Wagner, Gang Wang', 'link': 'https://arxiv.org/abs/2412.18442', 'abstract': 'Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence shows that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen -- all of which being valuable sources of information on offensive AI.\nTo enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user study (N=549) entailing individuals with diverse backgrounds and expertise; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come.', 'abstract_zh': '我们的社会越来越多地受益于人工智能（AI）。不幸的是，越来越多的证据表明，AI 也被用于攻击性目的。已有研究表明，AI 的部署可能导致安全和隐私目标的侵犯。然而，目前没有任何研究能够系统性地描绘出 AI 的攻击潜力。在本文综述论文中，我们旨在为系统性分析攻击性 AI 的异质能力奠定基础。具体而言，我们将做到以下几点：(i) 考虑 AI 对人类和系统的影响，(ii) 整合和提炼学术文献、专家意见、工业领域以及普通公众（这些都是有关攻击性 AI 的有价值信息来源）的知识。\n\n为了使这些多样化的知识来源能够对齐，我们制定了一个反映有关攻击性 AI 的关键技术因素的共同标准。借助这些标准，我们系统性地分析了以下内容：95篇研究论文；38篇信息安全简报（例如，来自 BlackHat 的）；涉及549名具有不同背景和专业知识的个人的用户研究回应；以及12位专家的意见。我们的贡献不仅揭示了 AI 今天可以被攻击性使用的令人担忧的方式（其中一些方式在先前的研究中被忽视），而且还为未来几十年应对这一威胁提供了一个立足点。', 'title_zh': 'SoK：人工智能的进攻潜力分析'}
{'arxiv_id': 'arXiv:2412.18431', 'title': 'GeAR: Graph-enhanced Agent for Retrieval-augmented Generation', 'authors': 'Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan Tu, Zeren Jiang, Ruofei Lai, Yang Ren, Jeff Z. Pan', 'link': 'https://arxiv.org/abs/2412.18431', 'abstract': "Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GeAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GeAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems.", 'abstract_zh': '检索增强生成系统依赖于有效的文档检索能力。设计上，传统的稀疏检索器或密集检索器在多跳检索场景中面临诸多挑战。在本文中，我们提出了一种名为GeAR的新方法，通过两项关键创新提高了RAG（Retrieval-Augmented Generation）的性能：(i) 图扩展，可以增强任何传统的基线检索器，如BM25；(ii) 一个融合图扩展的代理框架。我们的评估结果显示，GeAR在三个多跳问答数据集上表现出优越的检索性能。此外，我们的系统在具有挑战性的MuSiQue数据集上达到了最先进的结果，相较其他多步检索系统，GeAR在性能提升超过10%的情况下，所需的词汇量和迭代次数更少。', 'title_zh': 'GeAR：图增强代理用于检索增强生成'}
{'arxiv_id': 'arXiv:2412.18415', 'title': 'Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English', 'authors': 'Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Manvendra Kumar Nema, Raj Jaiswal, Rajiv Ratn Shah', 'link': 'https://arxiv.org/abs/2412.18415', 'abstract': "Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non English languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resource efficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Gemini's accuracy on English datasets by +6% and matches Gemini's performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs.", 'abstract_zh': '大型语言模型（LLMs）在语言任务上表现出色，但在数学推理方面存在不足，尤其是在非英语语言如印地语中更为明显。本研究旨在提升较小的、资源高效的开源LLMs在印地语和英语中的数学推理能力。我们使用零样本、少量样本链式思考（CoT）方法和监督微调来评估如OpenHathi 7B、LLaMA-2 7B、WizardMath 7B、Mistral 7B、LLeMMa 7B、MAmmoTH 7B、Gemini Pro和GPT-4等模型。我们的方法包括课程学习策略，逐步训练模型解决越来越难的问题；一种新颖的分解策略，简化复杂的算术运算；以及一种结构化解决方案设计，将解决方案分为多个阶段。实验结果表明，这些方法显著提升了模型的性能。WizardMath 7B在英语数据集上的准确性超过Gemini +6%，在印地语数据集上的性能与Gemini相当。采用双语方法，结合英语和印地语样本，实现的结果与单一语言模型相当，展示了在两种语言中学习数学推理的能力。本研究突显了改进开源LLMs的数学推理能力的潜力。', 'title_zh': '多语言数学推理：推动印地语和英语开源大规模语言模型的发展'}
{'arxiv_id': 'arXiv:2412.18407', 'title': 'A Statistical Framework for Ranking LLM-Based Chatbots', 'authors': 'Siavash Ameli, Siyuan Zhuang, Ion Stoica, Michael W. Mahoney', 'link': 'https://arxiv.org/abs/2412.18407', 'abstract': "Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties -- an integral aspect of human-judged comparisons -- significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses.", 'abstract_zh': '大语言模型（LLMs）已经彻底改变了自然语言处理领域，而类似Chatbot Arena这样的框架提供了评估这些模型的先驱平台。通过基于人类判断进行数百万次的成对比较，Chatbot Arena已成为LLM评估领域的重要基石，提供了丰富的数据集，用于评估模型在开放对话任务中的表现。在此基础上，我们提出了一种统计框架，结合关键的最新进展来应对成对比较分析中的特定挑战。首先，我们引入了一个事实上的平局模型，增强了处理平局——人类判断对比中的关键方面——的能力，显著提升了模型拟合观察数据的效果。其次，我们扩展了框架以建模竞争者之间的协方差，这有助于更深入地理解性能关系，并促进对性能层级的直观分组。第三，我们通过引入新颖的约束条件解决了由参数非唯一性引起的优化挑战，从而确保了参数估计的稳定性和可解释性。通过严格的评估和广泛的实验，我们的框架在建模成对比较数据方面展示了对现有方法的显著改进。为了支持可再现性和实际应用，我们发布了领导者机器人（leaderbot），这是一个开源的Python软件包，实现了我们的模型和分析。', 'title_zh': '基于统计框架的大型语言模型驱动的聊天机器人排名方法'}
{'arxiv_id': 'arXiv:2412.18391', 'title': 'TPAoI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking', 'authors': 'Haosheng He, Jianpeng Qi, Chao Liu, Junyu Dong, Yanwei Yu', 'link': 'https://arxiv.org/abs/2412.18391', 'abstract': 'In compute-first networking, maintaining fresh and accurate status information at the network edge is crucial for effective access to remote services. This process typically involves three phases: Status updating, user accessing, and user requesting. However, current studies on status effectiveness, such as Age of Information at Query (QAoI), do not comprehensively cover all these phases. Therefore, this paper introduces a novel metric, TPAoI, aimed at optimizing update decisions by measuring the freshness of service status. The stochastic nature of edge environments, characterized by unpredictable communication delays in updating, requesting, and user access times, poses a significant challenge when modeling. To address this, we model the problem as a Markov Decision Process (MDP) and employ a Dueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive experiments demonstrate that the proposed TPAoI metric effectively minimizes AoI, ensuring timely and reliable service updates in dynamic edge environments. Results indicate that TPAoI reduces AoI by an average of 47\\% compared to QAoI metrics and decreases update frequency by an average of 48\\% relative to conventional AoI metrics, showing significant improvement.', 'abstract_zh': '在计算优先网络中，保持网络边缘最新的准确状态信息对于有效访问远程服务至关重要。这一过程通常涉及三个阶段：状态更新、用户访问和用户请求。然而，目前关于状态有效性的研究，如查询时延（Query Age of Information, QAoI），并没有全面涵盖这些阶段。因此，本文提出了一种新的度量指标TPAoI，旨在通过测量服务状态的新鲜度来优化更新决策。边缘环境的随机性，表现为更新、请求和用户访问时间的不可预测的通信延迟，为建模带来了重大挑战。为了应对这一挑战，我们将问题建模为马尔可夫决策过程（MDP），并采用双深Q-网络（Dueling Double Deep Q-Network, D3QN）算法进行优化。详尽的实验表明，提出的TPAoI度量指标有效减小了时延（Age of Information, AoI），确保了动态边缘环境中的及时和可靠服务更新。结果表明，与QAoI指标相比，TPAoI将AoI平均减少了47%，与传统的AoI指标相比，更新频率平均减少了48%，显示出显著的改进。', 'title_zh': 'TPAoI：在计算优先网络中确保边缘网络的服务状态新鲜性'}
{'arxiv_id': 'arXiv:2412.18390', 'title': 'RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction', 'authors': 'Wu Xiaoping, Hu Jie, Wei Xiaoming', 'link': 'https://arxiv.org/abs/2412.18390', 'abstract': 'Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.', 'abstract_zh': '扩散概率模型（DPMs）已成为高保真图像合成的默认方法，它们在连续VAE潜在空间上执行扩散过程，这与大型语言模型（LLMs）所使用的文本生成方法有着显著差异。在本文中，我们引入了一种新颖的生成框架，即循环扩散概率模型（RDPM），该框架通过循环标记预测机制来增强扩散过程，从而开创了离散扩散这一领域。通过逐步向图像的潜在表示中引入高斯噪声并将它们以递归方式编码为矢量量化标记，RDPM 促进了在离散值域上的独特扩散过程。该过程逐迭代地为随后的时间步长预测标记码，将初始的标准高斯噪声转化为目标数据分布，损失函数与GPT风格的模型相似。RDPM展示了卓越的性能，并且得益于只需要少量推理步骤的优势。该模型不仅利用了扩散过程来确保高质量的生成，还能够将连续信号转换为一系列高保真度的离散标记，从而保持与其他离散标记（如文本）一致的优化策略。我们预计这项工作将促进统一多模态生成模型的发展，特别是通过集成图像、视频、音频等连续信号域与文本。我们将向开源社区发布代码和模型权重。', 'title_zh': 'RDPM：通过递归Token预测解决 diffusion概率模型'}
{'arxiv_id': 'arXiv:2412.18377', 'title': 'ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots', 'authors': 'Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdy, Nachshon Cohen, Alexander Libov, Guy Kushilevitz', 'link': 'https://arxiv.org/abs/2412.18377', 'abstract': 'The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots. The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We introduce the task of chatbot interaction autocomplete. We present ChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, coupled with suitable datasets and metrics. We use the framework to evaluate After formally defining the task along with suitable datasets and metrics, we test 9 models on the defined auto completion task, finding that while current off-the-shelf models perform fairly, there is still much room for improvement, mainly in ranking of the generated suggestions. We provide insights for practitioners working on this task and open new research directions for researchers in the field. We release our framework to serve as a foundation for future research.', 'abstract_zh': '生成型大模型（LLM）的兴起使得越来越多的人机交互转向基于大模型的对话机器人。这些模型的卓越能力使得用户能够使用长篇、多样的自然语言文本，涵盖广泛的主题和风格进行互动。编写这些消息是一个既耗时又费力的任务，因此需要一个自动补全解决方案来辅助用户。本文介绍了对话机器人交互自动补全的任务。我们提出了ChaI-TeA：对话交互自动补全；一个用于基于大模型对话机器人交互的自动补全评估框架。该框架包括对任务的正式定义，以及适用的数据集和评估指标。我们使用该框架对定义的自动补全任务进行了评估，并测试了9个模型，发现尽管现有的现成模型表现尚可，但在生成建议的排名方面仍有很大的改进空间。我们为在这个任务上工作的实践者提供了见解，并为该领域研究人员打开了新的研究方向。我们发布了该框架，以作为未来研究的基础。', 'title_zh': 'ChaI-TeA: 一种评估基于大语言模型的聊天机器人交互自动完成性能的基准'}
{'arxiv_id': 'arXiv:2412.18375', 'title': 'A Many Objective Problem Where Crossover is Provably Indispensable', 'authors': 'Andre Opris', 'link': 'https://arxiv.org/abs/2412.18375', 'abstract': 'This paper addresses theory in evolutionary multiobjective optimisation (EMO) and focuses on the role of crossover operators in many-objective optimisation. The advantages of using crossover are hardly understood and rigorous runtime analyses with crossover are lagging far behind its use in practice, specifically in the case of more than two objectives. We present a many-objective problem class together with a theoretical runtime analysis of the widely used NSGA-III to demonstrate that crossover can yield an exponential speedup on the runtime. In particular, this algorithm can find the Pareto set in expected polynomial time when using crossover while without crossover it requires exponential time to even find a single Pareto-optimal point. To our knowledge, this is the first rigorous runtime analysis in many-objective optimisation demonstrating an exponential performance gap when using crossover for more than two objectives.', 'abstract_zh': '本文探讨了进化多目标优化（EMO）中的理论问题，并 focuses 于交叉算子在多目标优化中的作用。虽然使用交叉算子的优势尚未得到充分理解，且关于交叉算子的严格运行时分析远远落后于其在实际中的应用，尤其是在多于两个目标的情况下。我们提出了一类多目标问题，并对广泛应用的 NSGA-III 算法进行了理论上的运行时分析，以证明在使用交叉算子时可以实现指数级的加速。特别是，该算法在使用交叉算子时可以在期望多项式时间内发现 Pareto 集，而在不使用交叉算子的情况下，甚至要找到一个 Pareto 最优解都需要指数级的时间。据我们所知，这是第一次在多目标优化中通过严格的运行时分析证明，在多于两个目标的情况下使用交叉算子可以获得指数级的性能差距。', 'title_zh': '一个证明需要杂交的多目标问题'}
{'arxiv_id': 'arXiv:2412.18370', 'title': 'Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors', 'authors': 'Jinhyeok Choi, Heehyeon Kim, Joyce Jiyoung Whang', 'link': 'https://arxiv.org/abs/2412.18370', 'abstract': 'Graph neural networks (GNNs) have emerged as an effective tool for fraud detection, identifying fraudulent users, and uncovering malicious behaviors. However, attacks against GNN-based fraud detectors and their risks have rarely been studied, thereby leaving potential threats unaddressed. Recent findings suggest that frauds are increasingly organized as gangs or groups. In this work, we design attack scenarios where fraud gangs aim to make their fraud nodes misclassified as benign by camouflaging their illicit activities in collusion. Based on these scenarios, we study adversarial attacks against GNN-based fraud detectors by simulating attacks of fraud gangs in three real-world fraud cases: spam reviews, fake news, and medical insurance frauds. We define these attacks as multi-target graph injection attacks and propose MonTi, a transformer-based Multi-target one-Time graph injection attack model. MonTi simultaneously generates attributes and edges of all attack nodes with a transformer encoder, capturing interdependencies between attributes and edges more effectively than most existing graph injection attack methods that generate these elements sequentially. Additionally, MonTi adaptively allocates the degree budget for each attack node to explore diverse injection structures involving target, candidate, and attack nodes, unlike existing methods that fix the degree budget across all attack nodes. Experiments show that MonTi outperforms the state-of-the-art graph injection attack methods on five real-world graphs.', 'abstract_zh': '基于图神经网络（GNN）的欺诈检测方法近年来已成为识别欺诈用户和发现恶意行为的有效工具。然而，针对基于GNN的欺诈检测器的攻击及其潜在风险的研究却鲜有报道，从而留下了潜在威胁未被解决。最近的研究表明，欺诈活动正越来越多地组织成团伙或团体。在此工作中，我们设计了一种攻击场景，在该场景中，欺诈团伙试图通过共谋掩盖其非法活动，使其欺诈节点被误分类为良性。基于这些场景，我们研究了针对基于GNN的欺诈检测器的对抗攻击，并通过模拟三种真实世界欺诈案例中的欺诈团伙攻击来分析其影响：垃圾评论、假新闻和医疗保险欺诈。我们将这些攻击定义为多目标图注入攻击，并提出了MonTi，一种基于变换器的多目标一次性图注入攻击模型。MonTi同时通过变换器编码器生成所有攻击节点的属性和边，相较于大多数现有图注入攻击方法，它可以更有效地捕捉属性和边之间的相互依赖。此外，MonTi能够为每个攻击节点自适应分配度预算，探索涉及目标节点、候选节点和攻击节点的多样化注入结构，而现有方法则在整个攻击节点中固定度预算。实验结果表明，MonTi在五个真实世界图数据集上的性能优于最先进的图注入攻击方法。', 'title_zh': '揭开欺诈团伙对图神经网络威胁的面纱：基于图神经网络的欺诈检测器的多目标图注入攻击'}
{'arxiv_id': 'arXiv:2412.18365', 'title': 'Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges', 'authors': 'Meixia He, Peican Zhu, Keke Tang, Yangming Guo', 'link': 'https://arxiv.org/abs/2412.18365', 'abstract': 'Recent studies have shown that Hypergraph Neural Networks (HGNNs) are vulnerable to adversarial attacks. Existing approaches focus on hypergraph modification attacks guided by gradients, overlooking node spanning in the hypergraph and the group identity of hyperedges, thereby resulting in limited attack performance and detectable attacks. In this manuscript, we present a novel framework, i.e., Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges (IE-Attack), to tackle these challenges. Initially, utilizing the node spanning in the hypergraph, we propose the elite hyperedges sampler to identify hyperedges to be injected. Subsequently, a node generator utilizing Kernel Density Estimation (KDE) is proposed to generate the homogeneous node with the group identity of hyperedges. Finally, by injecting the homogeneous node into elite hyperedges, IE-Attack improves the attack performance and enhances the imperceptibility of attacks. Extensive experiments are conducted on five authentic datasets to validate the effectiveness of IE-Attack and the corresponding superiority to state-of-the-art methods.', 'abstract_zh': '近年来的研究表明，超图神经网络（Hypergraph Neural Networks，HGNNs）对对抗攻击极为敏感。现有方法主要关注基于梯度的超图修改攻击，忽视了超图中的节点跨越以及超边的群体身份，导致攻击效果有限且容易被检测出来。本文提出了一个新的框架，即通过向精英超边注入同质节点的对抗攻击（IE-Attack），以应对这些挑战。首先，利用超图中的节点跨越特性，我们提出了一种精英超边采样器来识别要注入的超边。其次，我们利用核密度估计（KDE）提出了一种节点生成器，生成具有超边群体身份的同质节点。最后，通过将同质节点注入精英超边，IE-Attack 提高了攻击性能并增强了攻击的不可察觉性。我们在五个真实数据集上进行了广泛的实验，以验证 IE-Attack 的有效性及其对现有先进方法的优越性。', 'title_zh': '通过向精英超边注入同质节点来进行超图攻击'}
{'arxiv_id': 'arXiv:2412.18362', 'title': 'Point-DeepONet: A Deep Operator Network Integrating PointNet for Nonlinear Analysis of Non-Parametric 3D Geometries and Load Conditions', 'authors': 'Jangseop Park, Namwoo Kang', 'link': 'https://arxiv.org/abs/2412.18362', 'abstract': 'Nonlinear structural analyses in engineering often require extensive finite element simulations, limiting their applicability in design optimization, uncertainty quantification, and real-time control. Conventional deep learning surrogates, such as convolutional neural networks (CNNs), physics-informed neural networks (PINNs), and fourier neural operators (FNOs), face challenges with complex non-parametric three-dimensional (3D) geometries, directionally varying loads, and high-fidelity predictions on unstructured meshes. This work presents Point-DeepONet, an operator-learning-based surrogate that integrates PointNet into the DeepONet framework. By directly processing non-parametric point clouds and incorporating signed distance functions (SDF) for geometric context, Point-DeepONet accurately predicts three-dimensional displacement and von Mises stress fields without mesh parameterization or retraining. Trained using only about 5,000 nodes (2.5% of the original 200,000-node mesh), Point-DeepONet can still predict the entire mesh at high fidelity, achieving a coefficient of determination reaching 0.987 for displacement and 0.923 for von Mises stress under a horizontal load case. Compared to nonlinear finite element analyses that require about 19.32 minutes per case, Point-DeepONet provides predictions in mere seconds-approximately 400 times faster-while maintaining excellent scalability and accuracy with increasing dataset sizes. These findings highlight the potential of Point-DeepONet to enable rapid, high-fidelity structural analyses, ultimately supporting more effective design exploration and informed decision-making in complex engineering workflows.', 'abstract_zh': '工程领域中的非线性结构分析通常需要大量的有限元仿真，这限制了其在设计优化、不确定性量化和实时控制中的应用。传统的基于深度学习的代理模型，如卷积神经网络（CNNs）、物理信息神经网络（PINNs）和傅里叶神经算子（FNOs），在处理复杂非参数三维（3D）几何结构、方向性变化负载以及无结构网格上的高保真预测时面临挑战。本研究提出了基于操作学习的代理模型——Point-DeepONet，该模型将PointNet集成到DeepONet框架中。Point-DeepONet可以直接处理非参数点云数据，并通过引入符号距离函数（SDF）来结合几何上下文，从而在无需网格参数化或重新训练的情况下准确预测三维位移和von Mises应力场。使用大约5,000个节点（原始200,000节点网格的2.5%）的数据训练后，Point-DeepONet仍然能够在高保真度下预测整个网格：在水平负载情况下，其位移的决定系数可达0.987，von Mises应力的决定系数达0.923。相比之下，进行类似的非线性有限元分析需要大约19.32分钟，而Point-DeepONet可以提供几秒钟内的预测结果，大约快400倍，同时随着数据集大小的增加，该模型保持了出色的可扩展性和准确性。这些成果表明，Point-DeepONet有可能实现快速且高保真的结构分析，从而在复杂工程工作流程中支持更有效的设计探索和明智决策。', 'title_zh': '点DeepONet：结合PointNet的深度算子网络，用于非参数化3D几何和载荷条件的非线性分析'}
{'arxiv_id': 'arXiv:2412.18355', 'title': 'Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor', 'authors': 'Hao Yu, Xin Yang, Le Zhang, Hanlin Gu, Tianrui Li, Lixin Fan, Qiang Yang', 'link': 'https://arxiv.org/abs/2412.18355', 'abstract': "Federated continual learning (FCL) allows each client to continually update its knowledge from task streams, enhancing the applicability of federated learning in real-world scenarios. However, FCL needs to address not only spatial data heterogeneity between clients but also temporal data heterogeneity between tasks. In this paper, empirical experiments demonstrate that such input-level heterogeneity significantly affects the model's internal parameters and outputs, leading to severe spatial-temporal catastrophic forgetting of local and previous knowledge. To this end, we propose Federated Tail Anchor (FedTA) to mix trainable Tail Anchor with the frozen output features to adjust their position in the feature space, thereby overcoming parameter-forgetting and output-forgetting. Moreover, three novel components are also included in FedTA: Input Enhancement for improving the performance of pre-trained models on downstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous local knowledge on the server side; and Best Global Prototype Selection for finding the best anchor point for each class in the feature space. Extensive experiments demonstrate that FedTA not only outperforms existing FCL methods but also effectively preserves the relative positions of features, remaining unaffected by spatial and temporal changes.", 'abstract_zh': '联邦持续学习（Federated Continual Learning，FCL）允许每个客户端持续从任务流中更新其知识，从而增强联邦学习在实际场景中的适用性。然而，FCL不仅需要解决客户端之间的空间数据异质性问题，还需要解决任务之间的时序数据异质性问题。在本文中，实验证明这种输入级异质性对模型的内部参数和输出产生了显著影响，导致局部和先前知识的严重空间-时间灾难性遗忘。为了解决这一问题，我们提出了联邦尾锚（Federated Tail Anchor，FedTA）方法，通过结合可训练的尾锚与冻结的输出特征来调整它们在特征空间中的位置，从而克服参数遗忘和输出遗忘。此外，FedTA 还包括三个新的组件：输入增强（Input Enhancement），用于提高预训练模型在下游任务上的性能；选择性输入知识融合（Selective Input Knowledge Fusion），用于在服务器端融合异质局部知识；以及最佳全局原型选择（Best Global Prototype Selection），用于在特征空间中为每个类别找到最佳锚点。广泛的实验表明，FedTA 不仅优于现有的 FCL 方法，还能有效保持特征的相对位置，不受空间和时间变化的影响。', 'title_zh': '通过尾部锚定方法解决联邦持续学习中时空数据异质性问题'}
{'arxiv_id': 'arXiv:2412.18351', 'title': 'Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering', 'authors': 'Zhongjian Hu, Peng Yang, Bing Li, Zhenqi Wang', 'link': 'https://arxiv.org/abs/2412.18351', 'abstract': 'Large Language Models (LLMs) have achieved impressive results in knowledge-based Visual Question Answering (VQA). However existing methods still have challenges: the inability to use external tools autonomously, and the inability to work in teams. Humans tend to know whether they need to use external tools when they encounter a new question, e.g., they tend to be able to give a direct answer to a familiar question, whereas they tend to use tools such as search engines when they encounter an unfamiliar question. In addition, humans also tend to collaborate and discuss with others to get better answers. Inspired by this, we propose the multi-agent voting framework. We design three LLM-based agents that simulate different levels of staff in a team, and assign the available tools according to the levels. Each agent provides the corresponding answer, and finally all the answers provided by the agents are voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our approach outperforms other baselines by 2.2 and 1.0, respectively.', 'abstract_zh': '大型语言模型（LLMs）在基于知识的视觉问答（VQA）任务中取得了令人印象深刻的成果。然而，现有方法仍然面临挑战：无法自主使用外部工具以及无法团队协作。人类在遇到新问题时通常能够判断是否需要使用外部工具，例如，他们倾向于直接回答已熟悉的问题，而当遇到不熟悉的问题时，则倾向于使用搜索引擎等工具。此外，人类还倾向于与他人合作讨论以获取更好的答案。受此启发，我们提出了多智能体投票框架。我们设计了三个基于LLM的智能体，模拟团队中不同层级的角色，并根据这些层级分配可用工具。每个智能体提供相应的答案，最后将所有智能体提供的答案进行投票以得到最终答案。在OK-VQA和A-OKVQA上的实验结果表明，我们的方法分别比其他基线方法提高了2.2和1.0的性能。', 'title_zh': '基于大型语言模型的多agents知识驱动视觉问答系统'}
{'arxiv_id': 'arXiv:2412.18337', 'title': 'The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment', 'authors': 'Xinyi Zhang, Chenshuo Sun, Renyu Zhang, Khim-Yong Goh', 'link': 'https://arxiv.org/abs/2412.18337', 'abstract': "AI-generated content (AIGC), such as advertisement copy, product descriptions, and social media posts, is becoming ubiquitous in business practices. However, the value of AI-generated metadata, such as titles, remains unclear on user-generated content (UGC) platforms. To address this gap, we conducted a large-scale field experiment on a leading short-video platform in Asia to provide about 1 million users access to AI-generated titles for their uploaded videos. Our findings show that the provision of AI-generated titles significantly boosted content consumption, increasing valid watches by 1.6% and watch duration by 0.9%. When producers adopted these titles, these increases jumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely attributed to the use of this generative AI (GAI) tool increasing the likelihood of videos having a title by 41.4%. The effect was more pronounced for groups more affected by metadata sparsity. Mechanism analysis revealed that AI-generated metadata improved user-video matching accuracy in the platform's recommender system. Interestingly, for a video for which the producer would have posted a title anyway, adopting the AI-generated title decreased its viewership on average, implying that AI-generated titles may be of lower quality than human-generated ones. However, when producers chose to co-create with GAI and significantly revised the AI-generated titles, the videos outperformed their counterparts with either fully AI-generated or human-generated titles, showcasing the benefits of human-AI co-creation. This study highlights the value of AI-generated metadata and human-AI metadata co-creation in enhancing user-content matching and content consumption for UGC platforms.", 'abstract_zh': '人工智能生成内容（AIGC），如广告文案、产品描述和社交媒体帖子，正在越来越多地应用于商业实践中。然而，用户生成内容（UGC）平台上的人工智能生成元数据（如标题）的价值仍不清楚。为了解决这个问题，我们在亚洲的一个领先短视频平台上进行了一项大规模实地实验，向大约100万用户提供了其上传视频的人工智能生成标题的访问权限。我们的研究发现，提供人工智能生成的标题显著提高了内容消费，增加了有效观看次数1.6%和观看时长0.9%。当制作人采用这些标题时，这些增长分别跃升至7.1%和4.1%。这一观看量提升效果主要归因于使用这种生成型人工智能（GAI）工具使得视频拥有标题的可能性提高了41.4%。这一效果在元数据稀疏影响较大的组别中更为显著。机制分析表明，人工智能生成的元数据提高了平台推荐系统中用户-视频匹配的准确性。有趣的是，对于制作人本来就会为其发布标题的视频，采用人工智能生成的标题反而减少了其平均观看量，这表明人工智能生成的标题可能不如人工生成的那样高质量。然而，当制作人选择与GAI合作并大幅修改了人工智能生成的标题时，这些视频在观看量和表现上超过了仅有完全由人工智能或人工生成标题的视频，展示了人类与人工智能合作生成元数据的益处。这项研究突显了人工智能生成元数据及其与人类合作生成元数据在提升UGC平台内容匹配和内容消费价值中的重要性。', 'title_zh': 'AI生成的元数据对UGC平台的价值：来自大规模实地实验的证据'}
{'arxiv_id': 'arXiv:2412.18335', 'title': 'FloNa: Floor Plan Guided Embodied Visual Navigation', 'authors': 'Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu', 'link': 'https://arxiv.org/abs/2412.18335', 'abstract': 'Humans naturally rely on floor plans to navigate in unfamiliar environments, as they are readily available, reliable, and provide rich geometrical guidance. However, existing visual navigation settings overlook this valuable prior knowledge, leading to limited efficiency and accuracy. To eliminate this gap, we introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the first attempt to incorporate floor plan into embodied visual navigation. While the floor plan offers significant advantages, two key challenges emerge: (1) handling the spatial inconsistency between the floor plan and the actual scene layout for collision-free navigation, and (2) aligning observed images with the floor plan sketch despite their distinct modalities. To address these challenges, we propose FloDiff, a novel diffusion policy framework incorporating a localization module to facilitate alignment between the current observation and the floor plan. We further collect $20k$ navigation episodes across $117$ scenes in the iGibson simulator to support the training and evaluation. Extensive experiments demonstrate the effectiveness and efficiency of our framework in unfamiliar scenes using floor plan knowledge. Project website: this https URL.', 'abstract_zh': '人类自然依赖于平面图来在不熟悉的空间环境中导航，因为平面图易于获取、可靠且能提供丰富的几何指导。然而，现有的视觉导航设置忽视了这一宝贵的知识，导致导航效率和准确性有限。为进一步弥补这一不足，我们引入了一个新的导航任务：平面图视觉导航（FloNa），这是首次尝试将平面图纳入体感视觉导航。尽管平面图具有显著的优势，但也出现了两个关键挑战：（1）处理平面图与实际场景布局之间存在的空间不一致性，以实现无障碍导航；（2）通过不同的模态对齐观察到的图像和平面图草图。为解决这些挑战，我们提出了FloDiff，这是一种新颖的扩散策略框架，结合了定位模块，以促进当前观察与平面图之间的对齐。我们还收集了iGibson模拟器中的20,000个导航场景，覆盖117个不同的场景，以支持训练和评估。通过广泛的实验，我们的框架在利用平面图知识导航不熟悉场景方面展现出了有效性和效率。项目网站：[此处插入网址]。', 'title_zh': 'FloNa：基于平面图的实体视觉导航'}
{'arxiv_id': 'arXiv:2412.18322', 'title': 'Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning', 'authors': 'Safa Ben Atitallah, Chaima Ben Rabah, Maha Driss, Wadii Boulila, Anis Koubaa', 'link': 'https://arxiv.org/abs/2412.18322', 'abstract': 'Graph Mamba, a powerful graph embedding technique, has emerged as a cornerstone in various domains, including bioinformatics, social networks, and recommendation systems. This survey represents the first comprehensive study devoted to Graph Mamba, to address the critical gaps in understanding its applications, challenges, and future potential. We start by offering a detailed explanation of the original Graph Mamba architecture, highlighting its key components and underlying mechanisms. Subsequently, we explore the most recent modifications and enhancements proposed to improve its performance and applicability. To demonstrate the versatility of Graph Mamba, we examine its applications across diverse domains. A comparative analysis of Graph Mamba and its variants is conducted to shed light on their unique characteristics and potential use cases. Furthermore, we identify potential areas where Graph Mamba can be applied in the future, highlighting its potential to revolutionize data analysis in these fields. Finally, we address the current limitations and open research questions associated with Graph Mamba. By acknowledging these challenges, we aim to stimulate further research and development in this promising area. This survey serves as a valuable resource for both newcomers and experienced researchers seeking to understand and leverage the power of Graph Mamba.', 'abstract_zh': '图Mamba是一种强大的图嵌入技术，在生物信息学、社交网络和推荐系统等领域逐渐成为关键支柱。本文综述了图Mamba的最新进展，旨在填补对其应用、挑战和未来潜力理解的空白。首先，我们详细解释了原始图Mamba架构，突出其关键组件和运行机制。随后，我们探讨了最新提出的各种改进和增强，以提高其性能和适用性。为了展示图Mamba的灵活性，我们对其在不同领域的应用进行了研究。我们还对图Mamba及其变体进行了比较分析，揭示了它们的独特特性和潜在应用场景。此外，我们指出了图Mamba未来可能的应用领域，并强调了其在这些领域数据分析中的革命性潜力。最后，我们讨论了图Mamba目前的局限性和开放的研究问题。通过承认这些挑战，我们希望激发对该领域进一步研究和开发的兴趣。本文综述为初学者和经验丰富的研究人员提供了有价值的信息，使他们能够理解并利用图Mamba的强大功能。', 'title_zh': '探索Graph Mamba：图学习领域基于状态空间模型的综述'}
{'arxiv_id': 'arXiv:2412.18319', 'title': 'Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search', 'authors': 'Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao', 'link': 'https://arxiv.org/abs/2412.18319', 'abstract': "In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code will be available at this https URL", 'abstract_zh': '在本项工作中，我们旨在开发一种多模块语言模型（MLLM），使其能够通过学习生成推理过程中的每个中间步骤，直至最终答案来理解并解决问题。为此，我们提出了集体蒙特卡洛树搜索（CoMCTS），这是一种新的MLLM推理学习方法，将“树搜索”中的集体学习概念引入，以实现更有效的推理路径搜索和学习。CoMCTS的核心思想是利用多个模型的集体知识，通过扩张、模拟和错误定位、反向传播和选择等四个迭代操作，协作地猜测、搜索和识别通向正确答案的有效推理路径。\n\n利用CoMCTS，我们构建了Mulberry-260k，这是一个包含每个问题丰富的明确和规范推理节点的多模态数据集。通过Mulberry-260k，我们进行集体监督 fine-tuning (SFT) 来训练我们的模型——具有类似o1的逐步推理和反思能力的一系列MLLMs，即Mulberry。广泛的实验表明，我们提出的方法在各种基准测试中具有优越性。代码将在以下链接提供：[请在此插入URL]', 'title_zh': '桑葚: 通过集体蒙特卡洛树搜索增强MLLM的o1-like推理与反思能力'}
{'arxiv_id': 'arXiv:2412.18316', 'title': 'Data-Driven Self-Supervised Graph Representation Learning', 'authors': 'Ahmed E. Samy, Zekarias T. Kefatoa, Sarunas Girdzijauskasa', 'link': 'https://arxiv.org/abs/2412.18316', 'abstract': "Self-supervised graph representation learning (SSGRL) is a representation learning paradigm used to reduce or avoid manual labeling. An essential part of SSGRL is graph data augmentation. Existing methods usually rely on heuristics commonly identified through trial and error and are effective only within some application domains. Also, it is not clear why one heuristic is better than another. Moreover, recent studies have argued against some techniques (e.g., dropout: that can change the properties of molecular graphs or destroy relevant signals for graph-based document classification tasks).\nIn this study, we propose a novel data-driven SSGRL approach that automatically learns a suitable graph augmentation from the signal encoded in the graph (i.e., the nodes' predictive feature and topological information). We propose two complementary approaches that produce learnable feature and topological augmentations. The former learns multi-view augmentation of node features, and the latter learns a high-order view of the topology. Moreover, the augmentations are jointly learned with the representation. Our approach is general that it can be applied to homogeneous and heterogeneous graphs. We perform extensive experiments on node classification (using nine homogeneous and heterogeneous datasets) and graph property prediction (using another eight datasets). The results show that the proposed method matches or outperforms the SOTA SSGRL baselines and performs similarly to semi-supervised methods. The anonymised source code is available at this https URL", 'abstract_zh': '自监督图表示学习（SSGRL）是一种用于减少或避免手动标注的表示学习范式。SSGRL的核心部分是图数据增强。现有方法通常依赖于通过试验和错误识别出的启发式方法，并且其有效性往往仅限于某些应用领域。此外，目前尚不清楚一种启发式方法为什么比另一种更好。近年来的研究还反对一些技术（例如，dropout，它可能会改变分子图的属性或破坏基于图的文档分类任务的相关信号）。\n\n在本研究中，我们提出了一种基于数据的新型SSGRL方法，能够自动从图中编码的信号（即节点的预测特征和拓扑信息）中学习合适的图增强方法。我们提出了两个互补的方法，分别生成可学习的特征增强和拓扑增强。前者学习节点特征的多视图增强，后者学习高阶拓扑视图。此外，这些增强方法是与表示一同学习的。我们的方法具有通用性，既可以应用于同构图，也可以应用于异构图。我们在节点分类（使用九个同构和异构数据集）和图属性预测（使用另外八个数据集）方面进行了大量实验。实验结果表明，所提出的方法与当前最佳的SSGRL基线方法相当或优于这些基线方法，并且在性能上与部分监督方法相似。源代码已匿名化处理，可在以下链接获取：[ANONYMIZED_URL]', 'title_zh': '数据驱动的自监督图形表示学习'}
{'arxiv_id': 'arXiv:2412.18299', 'title': 'M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models', 'authors': 'Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Shimin Tao, Hengchao Shang, Zongyao Li, Shaojun Li, Jinlong Yang, Zhanglin Wu, Zhiqiang Rao, Hao Yang', 'link': 'https://arxiv.org/abs/2412.18299', 'abstract': 'With the widespread application of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), enhancing their performance has become a research hotspot. This paper presents a novel multi-prompt ensemble decoding approach designed to bolster the generation quality of LLMs by leveraging the aggregation of outcomes from multiple prompts. Given a unique input $X$, we submit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and derive probability distributions. For each token prediction, we calculate the ensemble probability by averaging the $n$ probability distributions within the batch, utilizing this aggregated probability to generate the token. This technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch inference, we implement a Left-Padding strategy to maintain uniform input lengths across the n prompts. Through extensive experimentation on diverse NLP tasks, including machine translation, code generation, and text simplification, we demonstrate the efficacy of our method in enhancing LLM performance. The results show substantial improvements in BLEU scores, pass@$k$ rates, and LENS metrics over conventional methods.', 'abstract_zh': '随着大型语言模型（LLMs）在自然语言处理（NLP）领域的广泛应用，提升其性能已成为研究热点。本文提出了一种新颖的多提示ensemble解码方法，旨在通过利用多个提示生成结果的聚合来增强LLMs的生成质量。对于一个唯一的输入$X$，我们以批处理模式提交$n$种不同格式的$X$提示，通过解码并推导概率分布。对于每个词项预测，我们通过计算批处理内$n$个概率分布的平均值来计算ensemble概率，并利用该聚合概率生成词项。这种方法被称为内批ensemble。为了促进高效的批处理推理，我们采用左填充策略，以保持$n$个提示之间的输入长度一致。通过在机器翻译、代码生成和文本简化等多种NLP任务上的广泛实验，我们展示了该方法在提升LLMs性能方面的有效性。结果显示，该方法在BLEU评分、pass@$k$率和LENS指标方面相较于传统方法有显著的提升。', 'title_zh': 'M-Ped：多提示 Ensemble 解码 для 大型语言模型\n\n为了符合学术规范并更准确地翻译成中文，可以进一步调整为：\n\nM-Ped：多提示 Ensemble 解码方法应用于大型语言模型\n\n这样既保留了原文的核心含义，又符合中文的表达习惯。'}
{'arxiv_id': 'arXiv:2412.18298', 'title': 'Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight', 'authors': 'Xi Ding, Lei Wang', 'link': 'https://arxiv.org/abs/2412.18298', 'abstract': 'Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection.', 'abstract_zh': '视频异常检测（VAD）通过集成大语言模型（LLMs）和视觉-语言模型（VLMs），在可解释性、时间推理和动态开放世界场景中的泛化能力方面取得了显著进展，解决了关键挑战。本文对2024年的LLM-/VLM-基于的方法进行了深入综述，重点关注四个方面：（i）通过语义洞察和文本解释增强可解释性，使视觉异常更加易于理解；（ii）捕捉复杂的时序关系，以检测和定位视频帧中的动态异常；（iii）实现少样本和零样本检测，减少对大规模标注数据集的依赖；（iv）通过使用语义理解和运动特征解决开放世界和类无差别异常，以实现时空一致性。我们强调这些方法有望重新定义VAD的格局。此外，我们探讨了LLMs和VLMs提供的视觉和文本模态之间的协同作用，突显它们的综合优势，并提出未来方向，以充分利用这些模型在增强视频异常检测方面的潜力。', 'title_zh': '《 anomaly detection 前途何方？来自预训练语言模型和多模态语言模型的关注》\n\n注：这里的“Quo Vadis”是一个拉丁语短语，意为“你要去哪里？”在学术或讨论中，常用来表示对未来趋势的探讨。因此，在翻译时，我们将其意译为“前途何方？”以符合中文的表达习惯和学术规范。'}
{'arxiv_id': 'arXiv:2412.18296', 'title': 'Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies', 'authors': 'Qi Liu, Wanjing Ma', 'link': 'https://arxiv.org/abs/2412.18296', 'abstract': 'Data corruption, including missing and noisy data, poses significant challenges in real-world machine learning. This study investigates the effects of data corruption on model performance and explores strategies to mitigate these effects through two experimental setups: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). We analyze the relationship between data corruption levels and model performance, evaluate the effectiveness of data imputation methods, and assess the utility of enlarging datasets to address data corruption.\nOur results show that model performance under data corruption follows a diminishing return curve, modeled by the exponential function. Missing data, while detrimental, is less harmful than noisy data, which causes severe performance degradation and training instability, particularly in sequential decision-making tasks like Signal-RL. Imputation strategies involve a trade-off: they recover missing information but may introduce noise. Their effectiveness depends on imputation accuracy and corruption ratio. We identify distinct regions in the imputation advantage heatmap, including an "imputation advantageous corner" and an "imputation disadvantageous edge" and classify tasks as "noise-sensitive" or "noise-insensitive" based on their decision boundaries.\nFurthermore, we find that increasing dataset size mitigates but cannot fully overcome the effects of data corruption. The marginal utility of additional data diminishes as corruption increases. An empirical rule emerges: approximately 30% of the data is critical for determining performance, while the remaining 70% has minimal impact.\nThese findings provide actionable insights into data preprocessing, imputation strategies, and data collection practices, guiding the development of robust machine learning systems in noisy environments.', 'abstract_zh': '数据损坏，包括缺失数据和噪声数据，对现实世界中的机器学习构成了重大挑战。本研究探讨了数据损坏对模型性能的影响，并通过两个实验设置探索了缓解这些影响的策略：自然语言处理任务的监督学习（NLP-SL）和用于交通信号优化的深度强化学习（Signal-RL）。我们分析了数据损坏程度与模型性能之间的关系，评估了数据插补方法的有效性，并评估了扩大数据集以应对数据损坏的实用性。\n\n研究结果表明，数据损坏下的模型性能遵循一个递减的回报曲线，可以用指数函数来拟合。缺失数据虽然不利，但其危害程度小于噪声数据，后者导致严重的性能下降和训练不稳定，尤其是像Signal-RL这样的顺序决策任务。插补策略存在权衡：它们可以恢复缺失信息，但可能会引入噪声。这些方法的有效性取决于插补准确性和损坏比例。我们识别出插补优势热图中的不同区域，包括一个“插补有利角落”和一个“插补不利边缘”，并根据其决策边界将任务分类为“噪声敏感型”或“噪声鲁棒型”。\n\n此外，我们发现增加数据集大小可以减轻但不能完全克服数据损坏的影响。随着损坏程度的增加，额外数据的边际效用逐渐减弱。一条经验法则由此浮出水面：大约30%的数据对于决定性能至关重要，而其余70%的数据对性能影响微乎其微。\n\n这些研究发现为数据预处理、插补策略和数据收集实践提供了实际指导，有助于在嘈杂环境中构建稳健的机器学习系统。', 'title_zh': '在机器学习中导航数据损坏问题：平衡质量、数量与插补策略'}
{'arxiv_id': 'arXiv:2412.18291', 'title': 'DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation', 'authors': 'Junyi Lu, Xiaojia Li, Zihan Hua, Lei Yu, Shiqi Cheng, Li Yang, Fengjun Zhang, Chun Zuo', 'link': 'https://arxiv.org/abs/2412.18291', 'abstract': 'Code review is a vital but demanding aspect of software development, generating significant interest in automating review comments. Traditional evaluation methods for these comments, primarily based on text similarity, face two major challenges: inconsistent reliability of human-authored comments in open-source projects and the weak correlation of text similarity with objectives like enhancing code quality and detecting defects.\nThis study empirically analyzes benchmark comments using a novel set of criteria informed by prior research and developer interviews. We then similarly revisit the evaluation of existing methodologies. Our evaluation framework, DeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a comprehensive reassessment of current techniques based on the criteria set. Besides, we also introduce an innovative and efficient baseline, LLM-Reviewer, leveraging the few-shot learning capabilities of LLMs for a target-oriented comparison.\nOur research highlights the limitations of text similarity metrics, finding that less than 10% of benchmark comments are high quality for automation. In contrast, DeepCRCEval effectively distinguishes between high and low-quality comments, proving to be a more reliable evaluation mechanism. Incorporating LLM evaluators into DeepCRCEval significantly boosts efficiency, reducing time and cost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates significant potential of focusing task real targets in comment generation.', 'abstract_zh': '代码审查是软件开发中至关重要但极具挑战性的一个方面，引发了对自动化审查意见的兴趣。传统上，这些审查意见的评估方法主要依赖于文本相似度，面临两大主要挑战：开源项目中人工撰写的审查意见可靠性不一致，以及文本相似度与提高代码质量、检测缺陷等目标之间的弱关联性。\n\n本研究通过对基准审查意见进行实证分析，采用由前期研究和开发人员访谈指导的一套新型评估标准。随后，我们重新审视现有方法的评估。我们的评估框架DeepCRCEval结合了人工评估者和大型语言模型（LLMs），基于上述标准对当前技术进行全面重新评估。此外，我们还引入了一种创新且高效的基线模型——LLM-Reviewer，利用LLMs的少量示例学习能力，针对目标进行比较。\n\n研究结果揭示了文本相似度度量的局限性，发现少于10%的基准审查意见适合自动化。相比之下，DeepCRCEval能够有效地区分高质量和低质量的审查意见，证明其是一种更加可靠的评估机制。将LLM评估者纳入DeepCRCEval显著提高了效率，分别降低了88.78%的时间和90.32%的成本。此外，LLM-Reviewer展示了在评论生成中集中于实际目标的显著潜力。', 'title_zh': 'DeepCRCEval: 重新评估代码审查评论生成的效果'}
{'arxiv_id': 'arXiv:2412.18288', 'title': 'Towards understanding how attention mechanism works in deep learning', 'authors': 'Tianyu Ruan, Shihua Zhang', 'link': 'https://arxiv.org/abs/2412.18288', 'abstract': 'Attention mechanism has been extensively integrated within mainstream neural network architectures, such as Transformers and graph attention networks. Yet, its underlying working principles remain somewhat elusive. What is its essence? Are there any connections between it and traditional machine learning algorithms? In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning. We identify the key characteristics of similarity computation and information propagation in these methods and demonstrate that the self-attention mechanism in deep learning adheres to the same principles but operates more flexibly and adaptively. We decompose the self-attention mechanism into a learnable pseudo-metric function and an information propagation process based on similarity computation. We prove that the self-attention mechanism converges to a drift-diffusion process through continuous modeling provided the pseudo-metric is a transformation of a metric and certain reasonable assumptions hold. This equation could be transformed into a heat equation under a new metric. In addition, we give a first-order analysis of attention mechanism with a general pseudo-metric function. This study aids in understanding the effects and principle of attention mechanism through physical intuition. Finally, we propose a modified attention mechanism called metric-attention by leveraging the concept of metric learning to facilitate the ability to learn desired metrics more effectively. Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness.', 'abstract_zh': '注意力机制已被广泛整合到主流神经网络结构中，如Transformer和图注意力网络中。然而，其内在的工作原理仍然有些模糊。其本质是什么？它与传统的机器学习算法之间是否存在联系？在本研究中，我们检查了使用经典度量和流形学习、聚类和监督学习中的向量空间属性进行相似性计算的过程。我们确定了这些方法中相似性计算和信息传播的关键特征，并证明了深度学习中的自注意力机制遵循相同的原则，但具有更大的灵活性和适应性。我们将自注意力机制分解为一个可学习的伪度量函数和基于相似性计算的信息传播过程。我们证明，在伪度量是度量的变换并且满足某些合理假设的情况下，自注意力机制可以通过连续建模收敛到漂移扩散过程。在这种情况下，该方程可以在新的度量下转化为热方程。此外，我们给出了一个一般伪度量函数的注意力机制的一阶分析。本研究通过物理直觉帮助理解注意力机制的效果和原理。最后，我们通过利用度量学习的概念提出了一种改进的注意力机制——度量注意力，以增强更有效地学习所需度量的能力。实验结果表明，与自我注意力相比，它在训练效率、准确性和鲁棒性方面表现更优。', 'title_zh': 'Towards Understanding How the Attention Mechanism Functions in Deep Learning'}
{'arxiv_id': 'arXiv:2412.18287', 'title': 'Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation', 'authors': 'Sheng Xiang, Mingzhi Zhu, Dawei Cheng, Enxia Li, Ruihui Zhao, Yi Ouyang, Ling Chen, Yefeng Zheng', 'link': 'https://arxiv.org/abs/2412.18287', 'abstract': 'Credit card fraud incurs a considerable cost for both cardholders and issuing banks. Contemporary methods apply machine learning-based classifiers to detect fraudulent behavior from labeled transaction records. But labeled data are usually a small proportion of billions of real transactions due to expensive labeling costs, which implies that they do not well exploit many natural features from unlabeled data. Therefore, we propose a semi-supervised graph neural network for fraud detection. Specifically, we leverage transaction records to construct a temporal transaction graph, which is composed of temporal transactions (nodes) and interactions (edges) among them. Then we pass messages among the nodes through a Gated Temporal Attention Network (GTAN) to learn the transaction representation. We further model the fraud patterns through risk propagation among transactions. The extensive experiments are conducted on a real-world transaction dataset and two publicly available fraud detection datasets. The result shows that our proposed method, namely GTAN, outperforms other state-of-the-art baselines on three fraud detection datasets. Semi-supervised experiments demonstrate the excellent fraud detection performance of our model with only a tiny proportion of labeled data.', 'abstract_zh': '信用卡欺诈给持卡人和发卡银行造成了相当大的经济损失。现有的方法通常使用基于机器学习的分类器从带有标签的交易记录中检测欺诈行为。然而，由于标注数据的成本高昂，带有标签的数据在数十亿真实交易中所占比例通常较小，这意味着它们未能充分利用大量未标注数据中的自然特征。因此，我们提出了一种半监督图神经网络用于欺诈检测。具体而言，我们利用交易记录构建了一个时序交易图，该图由时序交易（节点）及其相互作用（边）组成。然后，我们通过门控时序注意力网络（GTAN）在节点间传递消息，学习交易表示。进一步地，我们通过交易间的风险传播模型化欺诈模式。我们在一个实际的交易数据集以及两个公开可用的欺诈检测数据集上进行了广泛的实验。实验结果显示，我们提出的方法（即GTAN）在三个欺诈检测数据集上优于其他最先进的基线方法。半监督实验进一步证明，即使使用少量的标注数据，我们的模型也表现出色的欺诈检测性能。', 'title_zh': '基于属性驱动的图表示的半监督信用卡欺诈检测'}
{'arxiv_id': 'arXiv:2412.18274', 'title': 'GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge', 'authors': 'Shammur Absar Chowdhury, Hind Almerekhi, Mucahid Kutlu, Kaan Efe Keles, Fatema Ahmad, Tasnim Mohiuddin, George Mikros, Firoj Alam', 'link': 'https://arxiv.org/abs/2412.18274', 'abstract': 'This paper presents a comprehensive overview of the first edition of the Academic Essay Authenticity Challenge, organized as part of the GenAI Content Detection shared tasks collocated with COLING 2025. This challenge focuses on detecting machine-generated vs. human-authored essays for academic purposes. The task is defined as follows: "Given an essay, identify whether it is generated by a machine or authored by a human.\'\' The challenge involves two languages: English and Arabic. During the evaluation phase, 25 teams submitted systems for English and 21 teams for Arabic, reflecting substantial interest in the task. Finally, seven teams submitted system description papers. The majority of submissions utilized fine-tuned transformer-based models, with one team employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This paper outlines the task formulation, details the dataset construction process, and explains the evaluation framework. Additionally, we present a summary of the approaches adopted by participating teams. Nearly all submitted systems outperformed the n-gram-based baseline, with the top-performing systems achieving F1 scores exceeding 0.98 for both languages, indicating significant progress in the detection of machine-generated text.', 'abstract_zh': '本文全面概述了2025年共译世界语言学会议（COLING 2025）期间举办的学术论文真实性挑战赛的第一轮比赛。该挑战赛旨在检测用于学术目的的机器生成论文与人类撰写论文之间的差异。任务定义如下：“给定一篇论文，判断它是由机器生成的还是由人类撰写的。”该挑战包括两种语言：英语和阿拉伯语。在评估阶段，25支队伍提交了英语系统的解决方案，21支队伍提交了阿拉伯语系统的解决方案，反映了对这项任务的极大兴趣。最终，有七支队伍提交了系统描述论文。大多数提交的系统都使用了微调的变压器模型，有一支队伍使用了大型语言模型（LLMs），如Llama 2和Llama 3。本文概述了任务定义，详细描述了数据集的构建过程，并阐述了评估框架。此外，我们还介绍了参与队伍所采用的方法概述。几乎所有的提交系统都超越了基于n-克gram的基本基准，最佳系统在两种语言上的F1分数都超过了0.98，表明在检测机器生成文本方面取得了显著进展。', 'title_zh': 'GenAI内容检测任务2：AI与人类之争——学术论文真实性挑战'}
{'arxiv_id': 'arXiv:2412.18273', 'title': 'Sampling Bag of Views for Open-Vocabulary Object Detection', 'authors': 'Hojun Choi, Junsuk Choe, Hyunjung Shim', 'link': 'https://arxiv.org/abs/2412.18273', 'abstract': "Existing open-vocabulary object detection (OVD) develops methods for testing unseen categories by aligning object region embeddings with corresponding VLM features. A recent study leverages the idea that VLMs implicitly learn compositional structures of semantic concepts within the image. Instead of using an individual region embedding, it utilizes a bag of region embeddings as a new representation to incorporate compositional structures into the OVD task. However, this approach often fails to capture the contextual concepts of each region, leading to noisy compositional structures. This results in only marginal performance improvements and reduced efficiency. To address this, we propose a novel concept-based alignment method that samples a more powerful and efficient compositional structure. Our approach groups contextually related ``concepts'' into a bag and adjusts the scale of concepts within the bag for more effective embedding alignment. Combined with Faster R-CNN, our method achieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel categories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our method reduces CLIP computation in FLOPs by 80.3% compared to previous research, significantly enhancing efficiency. Experimental results demonstrate that the proposed method outperforms previous state-of-the-art models on the OVD datasets.", 'abstract_zh': '现有的开放词汇对象检测（OVD）方法通过将物体区域嵌入与相应的视觉语言模型（VLM）特征对齐，来测试未见过的类别。最近的一项研究表明，VLM 暗含地学习了图像中语义概念的组分结构。这种方法不是使用单个区域嵌入，而是利用区域嵌入的“袋”（bag of region embeddings）作为新的表示形式，以整合组分结构到 OVD 任务中。然而，这种方法往往未能捕捉到每个区域的上下文概念，导致噪声组分结构，从而仅带来微小的性能改进和较低的效率。为了解决这个问题，我们提出了一种基于概念的对齐方法，通过采样更强大且高效的组分结构来改进这一过程。我们的方法将上下文相关“概念”分组到一个“袋”中，并调整“袋”内概念规模以实现更有效的嵌入对齐。结合 Faster R-CNN，我们的方法在开放词汇 COCO 和 LVIS 基准中对新类别的测评中实现了 2.6 个 box AP50 和 0.5 个 mask AP 的改进。此外，我们的方法在浮点运算量（FLOPs）上比之前的研究减少了 80.3%，显著提高了效率。实验结果表明，所提出的方法在 OVD 数据集上优于之前的最先进的模型。', 'title_zh': '开放词汇物体检测中的视角采样词袋模型'}
{'arxiv_id': 'arXiv:2412.18256', 'title': 'Robust Semi-Supervised Learning in Open Environments', 'authors': 'Lan-Zhe Guo, Lin-Han Jia, Jie-Jing Shao, Yu-Feng Li', 'link': 'https://arxiv.org/abs/2412.18256', 'abstract': 'Semi-supervised learning (SSL) aims to improve performance by exploiting unlabeled data when labels are scarce. Conventional SSL studies typically assume close environments where important factors (e.g., label, feature, distribution) between labeled and unlabeled data are consistent. However, more practical tasks involve open environments where important factors between labeled and unlabeled data are inconsistent. It has been reported that exploiting inconsistent unlabeled data causes severe performance degradation, even worse than the simple supervised learning baseline. Manually verifying the quality of unlabeled data is not desirable, therefore, it is important to study robust SSL with inconsistent unlabeled data in open environments. This paper briefly introduces some advances in this line of research, focusing on techniques concerning label, feature, and data distribution inconsistency in SSL, and presents the evaluation benchmarks. Open research problems are also discussed for reference purposes.', 'abstract_zh': '半监督学习（Semi-supervised Learning, SSL）旨在通过利用未标记数据来提高表现，特别是在标签稀缺的情况下。传统SSL研究通常假设封闭环境，在这种环境中，标记和未标记数据之间的关键因素（例如标签、特征、分布）是一致的。然而，更实际的任务涉及开放环境，在这种环境中，标记和未标记数据之间的关键因素是不一致的。已有研究表明，利用不一致的未标记数据会导致严重的性能下降，甚至比简单的监督学习基线还要差。手动验证未标记数据的质量是不切实际的，因此在开放环境中研究稳健的SSL以应对不一致的未标记数据显得尤为重要。本文简要介绍了该领域的一些进展，重点介绍了关于标签、特征和数据分布不一致的SSL技术，同时提出了评估基准。还讨论了一些开放性研究问题，供参考。', 'title_zh': '在开放环境下的稳健半监督学习'}
{'arxiv_id': 'arXiv:2412.18248', 'title': 'Detection and Forecasting of Parkinson Disease Progression from Speech Signal Features Using MultiLayer Perceptron and LSTM', 'authors': 'Majid Ali, Hina Shakir, Asia Samreen, Sohaib Ahmed', 'link': 'https://arxiv.org/abs/2412.18248', 'abstract': 'Accurate diagnosis of Parkinson disease, especially in its early stages, can be a challenging task. The application of machine learning techniques helps improve the diagnostic accuracy of Parkinson disease detection but only few studies have presented work towards the prediction of disease progression. In this research work, Long Short Term Memory LSTM was trained using the diagnostic features on Parkinson patients speech signals, to predict the disease progression while a Multilayer Perceptron MLP was trained on the same diagnostic features to detect the disease. Diagnostic features selected using two well-known feature selection methods named Relief-F and Sequential Forward Selection and applied on LSTM and MLP have shown to accurately predict the disease progression as stage 2 and 3 and its existence respectively.', 'abstract_zh': '帕金森病的准确诊断，尤其是早期阶段，是一项具有挑战性的任务。机器学习技术的应用有助于提高帕金森病检测的诊断准确性，但仅有少数研究致力于疾病的进展预测。在本研究中，我们使用长短期记忆网络（LSTM）对帕金森病患者的语音信号中的诊断特征进行训练，以预测疾病进展；同时，我们使用多层感知机（MLP）对相同的诊断特征进行训练，以检测疾病。通过使用两种广为人知的特征选择方法（Relief-F和序列前向选择）选出的诊断特征应用于LSTM和MLP，结果显示，这些特征能够准确预测疾病进展至第二阶段和第三阶段，以及疾病的存在情况。', 'title_zh': '使用多层感知器和LSTM从语音信号特征检测与预报帕金森病进展'}
{'arxiv_id': 'arXiv:2412.18247', 'title': "Fr\\'echet regression for multi-label feature selection with implicit regularization", 'authors': 'Dou El Kefel Mansouri, Seif-Eddine Benkabou, Khalid Benabdeslem', 'link': 'https://arxiv.org/abs/2412.18247', 'abstract': 'Fréchet regression extends linear regression to model complex responses\nin metric spaces, making it particularly relevant for multi-label regression,\nwhere each instance can have multiple associated labels. However, variable\nselection within this framework remains underexplored. In this paper, we pro pose a novel variable selection method that employs implicit regularization\ninstead of traditional explicit regularization approaches, which can introduce\nbias. Our method effectively captures nonlinear interactions between predic tors and responses while promoting model sparsity. We provide theoretical\nresults demonstrating selection consistency and illustrate the performance of\nour approach through numerical examples', 'abstract_zh': '弗雷歇回归将线性回归扩展到度量空间中的复杂响应建模，使其特别适用于多标签回归，其中每个实例可以有多个相关标签。然而，该框架内的变量选择仍然研究不足。本文提出了一种新的变量选择方法，该方法采用隐式正则化而不是传统的显式正则化方法，后者可能会引入偏差。我们的方法可以有效地捕捉预测因子与响应之间的非线性交互作用，同时促进模型稀疏性。我们提供了理论结果来证明选择一致性，并通过数值示例展示了该方法的性能。', 'title_zh': '弗雷歇回归在隐式正则化下的多标签特征选择'}
{'arxiv_id': 'arXiv:2412.18241', 'title': 'An Automatic Graph Construction Framework based on Large Language Models for Recommendation', 'authors': 'Rong Shan, Jianghao Lin, Chenxu Zhu, Bo Chen, Menghui Zhu, Kangning Zhang, Jieming Zhu, Ruiming Tang, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2412.18241', 'abstract': 'Graph neural networks (GNNs) have emerged as state-of-the-art methods to learn from graph-structured data for recommendation. However, most existing GNN-based recommendation methods focus on the optimization of model structures and learning strategies based on pre-defined graphs, neglecting the importance of the graph construction stage. Earlier works for graph construction usually rely on speciffic rules or crowdsourcing, which are either too simplistic or too labor-intensive. Recent works start to utilize large language models (LLMs) to automate the graph construction, in view of their abundant open-world knowledge and remarkable reasoning capabilities. Nevertheless, they generally suffer from two limitations: (1) invisibility of global view (e.g., overlooking contextual information) and (2) construction inefficiency. To this end, we introduce AutoGraph, an automatic graph construction framework based on LLMs for recommendation. Specifically, we first use LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Next, we employ vector quantization to extract the latent factors from the semantic vectors. The latent factors are then incorporated as extra nodes to link the user/item nodes, resulting in a graph with in-depth global-view semantics. We further design metapath-based message aggregation to effectively aggregate the semantic and collaborative information. The framework is model-agnostic and compatible with different backbone models. Extensive experiments on three real-world datasets demonstrate the efficacy and efffciency of AutoGraph compared to existing baseline methods. We have deployed AutoGraph in Huawei advertising platform, and gain a 2.69% improvement on RPM and a 7.31% improvement on eCPM in the online A/B test. Currently AutoGraph has been used as the main trafffc model, serving hundreds of millions of people.', 'abstract_zh': '图神经网络（GNNs）已经发展成为从图结构数据中进行推荐的最先进的方法。然而，现有的大多数基于GNN的推荐方法主要集中在模型结构和基于预定义图的学习策略的优化上，忽视了图构建阶段的重要性。早期的图构建工作通常依赖于特定规则或众包方法，这两种方法要么过于简单，要么过于耗时。最近的研究开始利用大型语言模型（LLMs）来自动化图构建过程，鉴于它们广泛的知识和卓越的推理能力。不过，它们普遍面临两个限制：（1）缺乏全局视角（例如，忽视上下文信息）和（2）构建效率低下。为此，我们提出了AutoGraph，这是一种基于LLMs的自动图构建框架，用于推荐系统。具体而言，我们首先使用LLMs推断用户偏好和项目知识，并将这些信息编码为语义向量。接着，我们使用向量量化技术从语义向量中提取潜在因素。然后，将这些潜在因素作为额外的节点集成到用户/项目节点之间，形成一个深层次全局视角的图。我们进一步设计了元路径（Metapath）为基础的消息聚合方法，以有效地聚合语义和协作信息。该框架具有通用性，可以与不同的底层模型兼容。在三个真实世界数据集上的广泛实验表明，与现有基准方法相比，AutoGraph在有效性和效率方面具有明显的优势。我们已在华为广告平台上部署了AutoGraph，并在其在线A/B测试中取得了2.69%的RPM提升和7.31%的eCPM提升。目前，AutoGraph已被用作主要流量模型，服务于数亿用户。', 'title_zh': '基于大型语言模型的自动图构建框架用于推荐系统'}
{'arxiv_id': 'arXiv:2412.18224', 'title': 'Expand VSR Benchmark for VLLM to Expertize in Spatial Rules', 'authors': 'Peijin Xie, Lin Sun, Bingquan Liu, Dexin Wang, Xiangzheng Zhang, Chengjie Sun, Jiajia Zhang', 'link': 'https://arxiv.org/abs/2412.18224', 'abstract': 'Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. Although benchmarks like MME, MMBench and SEED comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR). There is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. To handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set. We found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information. By expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. To our knowledge, we expanded spatially positioned image data controllably using diffusion models for the first time and integrated original visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and DINO). After conducting combination experiments on scaling data and models, we obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different instructions but also accurately distinguishes differences in visual positional information. VSRE achieved over a 27\\% increase in accuracy on the VSR test set. It becomes a performant VLLM on the position reasoning of both the VSR dataset and relevant subsets of other evaluation benchmarks. We open-sourced the expanded model with data and Appendix at \\url{this https URL} and hope it will accelerate advancements in VLLM on VSR learning.', 'abstract_zh': '区分空间关系是人类认知的基本部分，这需要对跨实例的细腻感知。尽管像MME、MMBench和SEED这样的基准已经综合评估了各种能力，包括视觉空间推理（VSR），但专门针对视觉定位推理的高质量和数量充足的评估与优化数据集仍然不足，尤其是针对视觉大型语言模型（VLLMs）。为解决这一问题，我们首先使用VSR数据集对当前的VLLMs进行了诊断，并提出了一套统一的测试集。我们发现当前的VLLMs存在对语言指令过度敏感而对视觉位置信息敏感度不足的矛盾。通过从数据调整和模型结构两个方面扩展原始基准，我们缓解了这一现象。根据我们所知，这是首次使用扩散模型有控制地扩展空间定位图像数据，并将原始视觉编码器（CLIP）与其他3个强大的视觉编码器（SigLIP、SAM和DINO）相结合。经过对数据和模型扩展的组合实验后，我们得到了一个适用于不同指令的VLLM VSR专家（VSRE），并且能够更准确地区分视觉位置信息的差异。VSRE在VSR测试集上的准确性提高了超过27%。它在VSR数据集及其相关子集和其他评估基准中的位置推理方面表现优异。我们开源了扩展后的模型及数据和附录，链接为 \\url{this https URL} 并希望这将加速VLLM在VSR学习方面的进展。', 'title_zh': '将VLLM的超分辨基准扩展以精通空间规则'}
{'arxiv_id': 'arXiv:2412.18207', 'title': 'Sharper Error Bounds in Late Fusion Multi-view Clustering Using Eigenvalue Proportion', 'authors': 'Liang Du, Henghui Jiang, Xiaodong Li, Yiqing Guo, Yan Chen, Feijiang Li, Peng Zhou, Yuhua Qian', 'link': 'https://arxiv.org/abs/2412.18207', 'abstract': 'Multi-view clustering (MVC) aims to integrate complementary information from multiple views to enhance clustering performance. Late Fusion Multi-View Clustering (LFMVC) has shown promise by synthesizing diverse clustering results into a unified consensus. However, current LFMVC methods struggle with noisy and redundant partitions and often fail to capture high-order correlations across views. To address these limitations, we present a novel theoretical framework for analyzing the generalization error bounds of multiple kernel $k$-means, leveraging local Rademacher complexity and principal eigenvalue proportions. Our analysis establishes a convergence rate of $\\mathcal{O}(1/n)$, significantly improving upon the existing rate in the order of $\\mathcal{O}(\\sqrt{k/n})$. Building on this insight, we propose a low-pass graph filtering strategy within a multiple linear $k$-means framework to mitigate noise and redundancy, further refining the principal eigenvalue proportion and enhancing clustering accuracy. Experimental results on benchmark datasets confirm that our approach outperforms state-of-the-art methods in clustering performance and robustness. The related codes is available at this https URL .', 'abstract_zh': '多视图聚类（Multi-view Clustering, MVC）旨在通过整合来自多个视图的互补信息来提升聚类性能。晚期融合多视图聚类（Late Fusion Multi-View Clustering, LFMVC）通过合成多样的聚类结果形成了统一的一致性共识，显示了其潜力。然而，当前的LFMVC方法难以处理噪声和冗余的划分，并常常无法捕捉跨视图的高阶关联。为解决这些局限性，我们提出了一种全新的理论框架，用于分析多种核$k$均值算法的泛化误差边界，利用局部Rademacher复杂度和主要特征值比例。我们的分析确立了收敛速率 $\\mathcal{O}(1/n)$，显著优于现有 $\\mathcal{O}(\\sqrt{k/n})$ 的速率。在此基础上，我们提出了一种多线性$k$均值框架内的低通图滤波策略，以减轻噪声和冗余，进一步细化主要特征值比例并提升聚类准确性。在基准数据集上的实验结果证实，我们的方法在聚类性能和鲁棒性方面均优于最先进的方法。相关代码已发布于 <this https URL>。', 'title_zh': '使用特征值比例在晚期融合多视图聚类中获得更严格的误差界'}
{'arxiv_id': 'arXiv:2412.18194', 'title': 'VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks', 'authors': 'Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu', 'link': 'https://arxiv.org/abs/2412.18194', 'abstract': "General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh\\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks.", 'abstract_zh': '通用型 embodied 代理旨在理解用户的自然指令或意图，并精确地执行以完成通用任务。最近，基于基础模型的方法，尤其是视觉-语言-动作模型（VLAs），显示出解决语言条件下的操作任务（LCM）的巨大潜力。然而，现有的基准并不充分满足 VLAs 及其相关算法的需求。为了更好地在大规模语言模型（LLMs）的背景下定义这类通用任务，并推动 VLAs 的研究进展，我们提出了 VLABench——一个开源基准，用于评估通用 LCM 任务学习。VLABench 提供了 100 个精心设计的任务类别，每个类别都有强烈的变化，并包含 2000 多个物体。与之前的基准相比，VLABench 在四个方面脱颖而出：1）需要世界知识和常识迁移的任务，2）具有隐含人类意图的自然语言指令而非模板，3）需要多步推理的长时任务，4）同时评估动作策略和语言模型能力。该基准评估了包括网格与纹理理解、空间关系、语义指令、物理定律、知识迁移和推理等多方面的能力。为了支持下游的微调，我们提供了一套高质量的训练数据，这些数据是通过结合启发式技能和先验信息的自动化框架收集的。实验结果表明，现有的最先进的预训练 VLAs 和基于 VLMs 的工作流程在我们的任务中都面临挑战。', 'title_zh': 'VLABench：一种基于语言条件的长期推理任务机器人操作大型基准测试'}
{'arxiv_id': 'arXiv:2412.18190', 'title': 'An Analysis on Automated Metrics for Evaluating Japanese-English Chat Translation', 'authors': 'Andre Rusli, Makoto Shishido', 'link': 'https://arxiv.org/abs/2412.18190', 'abstract': 'This paper analyses how traditional baseline metrics, such as BLEU and TER, and neural-based methods, such as BERTScore and COMET, score several NMT models performance on chat translation and how these metrics perform when compared to human-annotated scores. The results show that for ranking NMT models in chat translations, all metrics seem consistent in deciding which model outperforms the others. This implies that traditional baseline metrics, which are faster and simpler to use, can still be helpful. On the other hand, when it comes to better correlation with human judgment, neural-based metrics outperform traditional metrics, with COMET achieving the highest correlation with the human-annotated score on a chat translation. However, we show that even the best metric struggles when scoring English translations from sentences with anaphoric zero-pronoun in Japanese.', 'abstract_zh': '本文分析了传统基准指标（如 BLEU 和 TER）和基于神经网络的方法（如 BERTScore 和 COMET）在聊天翻译中对几种神经机器翻译（NMT）模型性能的评分情况，并探讨了这些指标与人工标注评分的性能对比。结果表明，在对聊天翻译的 NMT 模型进行排名时，所有指标在决定哪些模型表现更好上似乎是一致的，这表明传统基准指标，尽管更快且更简单，仍然具有一定的帮助作用。另一方面，在与人类判断的关联性方面，基于神经网络的指标优于传统指标，COMET 在聊天翻译的评分中与人工标注评分的关联性最高。然而，我们发现，即使是最优秀的指标，在评分包含零人称代词的英语翻译时也变得困难重重。', 'title_zh': '对评估日英聊天翻译质量的自动化指标进行分析'}
{'arxiv_id': 'arXiv:2412.18188', 'title': 'On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs', 'authors': 'Andre Rusli, Makoto Shishido', 'link': 'https://arxiv.org/abs/2412.18188', 'abstract': "This research explores the applicability of cross-lingual transfer learning from English to Japanese and Indonesian using the XLM-R pre-trained model. The results are compared with several previous works, either by models using a similar zero-shot approach or a fully-supervised approach, to provide an overview of the zero-shot transfer learning approach's capability using XLM-R in comparison with existing models. Our models achieve the best result in one Japanese dataset and comparable results in other datasets in Japanese and Indonesian languages without being trained using the target language. Furthermore, the results suggest that it is possible to train a multi-lingual model, instead of one model for each language, and achieve promising results.", 'abstract_zh': '本研究探讨了使用XLM-R预训练模型将英语知识跨语言迁移至日语和印度尼西亚语的适用性。我们将结果与几种先前研究进行了比较，这些研究要么采用了类似零样本的方法，要么采用了完全监督的方法，从而提供了一份关于使用XLM-R在零样本迁移学习方面与其他现有模型能力对比的概览。我们的模型在日语数据集中取得了最佳结果，在其他日语和印度尼西亚语数据集中也取得了可比拟的结果，而无需使用目标语言进行训练。此外，结果表明，有可能训练一个多语言模型，而不是为每种语言训练一个独立的模型，从而获得令人满意的结果。', 'title_zh': '关于零样本跨语言迁移学习在遥远语言对情感分类中的适用性研究'}
{'arxiv_id': 'arXiv:2412.18185', 'title': 'TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization', 'authors': 'Yucong Luo, Mingyue Cheng, Jie Ouyang, Xiaoyu Tao, Qi Liu', 'link': 'https://arxiv.org/abs/2412.18185', 'abstract': 'Text-to-image generative models excel in creating images from text but struggle with ensuring alignment and consistency between outputs and prompts. This paper introduces TextMatch, a novel framework that leverages multimodal optimization to address image-text discrepancies in text-to-image (T2I) generation and editing. TextMatch employs a scoring strategy powered by large language models (LLMs) and visual question-answering (VQA) models to evaluate semantic consistency between prompts and generated images. By integrating multimodal in-context learning and chain of thought reasoning, our method dynamically refines prompts through iterative optimization. This process ensures that the generated images better capture user intent of, resulting in higher fidelity and relevance. Extensive experiments demonstrate that TextMatch significantly improves text-image consistency across multiple benchmarks, establishing a reliable framework for advancing the capabilities of text-to-image generative models. Our code is available at this https URL.', 'abstract_zh': '文本到图像生成模型在从文本生成图像方面表现出色，但在确保输出与提示之间的对齐和一致性方面存在困难。本文介绍了一种名为TextMatch的新框架，该框架利用多模态优化来解决文本到图像（T2I）生成与编辑中的图像-文本不一致问题。TextMatch采用一种基于大型语言模型（LLM）和视觉问答（VQA）模型的评分策略，评估提示与生成图像之间的语义一致性。通过将多模态上下文学习与chain of thought推理相结合，我们的方法通过迭代优化动态细化提示。这一过程确保生成的图像更好地捕捉用户的意图，从而提高生成图像的准确性和相关性。广泛的实验表明，TextMatch在多个基准测试中显著提高了文本-图像的一致性，为提升文本到图像生成模型的能力建立了可靠的框架。我们的代码可通过以下链接获取：[此链接]。', 'title_zh': 'TextMatch：通过多模态优化增强图像-文本一致性'}
{'arxiv_id': 'arXiv:2412.18177', 'title': 'Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization', 'authors': 'Sihao Liu, Yibo Yang, Xiaojie Li, David A. Clifton, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2412.18177', 'abstract': 'Online continual learning (OCL) seeks to learn new tasks from data streams that appear only once, while retaining knowledge of previously learned tasks. Most existing methods rely on replay, focusing on enhancing memory retention through regularization or distillation. However, they often overlook the adaptability of the model, limiting the ability to learn generalizable and discriminative features incrementally from online training data. To address this, we introduce a plug-and-play module, S6MOD, which can be integrated into most existing methods and directly improve adaptability. Specifically, S6MOD introduces an extra branch after the backbone, where a mixture of discretization selectively adjusts parameters in a selective state space model, enriching selective scan patterns such that the model can adaptively select the most sensitive discretization method for current dynamics. We further design a class-conditional routing algorithm for dynamic, uncertainty-based adjustment and implement a contrastive discretization loss to optimize it. Extensive experiments combining our module with various models demonstrate that S6MOD significantly enhances model adaptability, leading to substantial performance gains and achieving the state-of-the-art results.', 'abstract_zh': '在线连续学习（OCL）旨在从仅出现一次的数据流中学习新任务，同时保留之前学习任务的知识。现有大多数方法依赖于回放，通过正则化或蒸馏增强记忆保持能力。然而，这些方法往往忽视了模型的适应性，限制了模型从在线训练数据中增量学习可泛化的判别特征的能力。为解决这一问题，我们提出了一种即插即用模块S6MOD，它可以集成到大多数现有方法中，直接提高模型的适应性。具体而言，S6MOD 在主干之后引入了一个额外分支，其中混合离散化选择性调整选择性状态空间模型中的参数，丰富了选择性扫描模式，使得模型能够自适应选择当前动力学最为敏感的离散化方法。此外，我们设计了一个类别条件路由算法，用于动态、基于不确定性的调整，并实现了对比离散化损失对其进行优化。结合我们的模块与多种模型的广泛实验表明，S6MOD 显著提升了模型的适应性，带来了显著的性能提升，并达到了目前的先进水平。', 'title_zh': '增强在线连续学习的插件式状态空间模型和类条件离散混合方法'}
{'arxiv_id': 'arXiv:2412.18176', 'title': 'Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation', 'authors': 'Yucong Luo, Qitao Qin, Hao Zhang, Mingyue Cheng, Ruiran Yan, Kefan Wang, Jie Ouyang', 'link': 'https://arxiv.org/abs/2412.18176', 'abstract': 'Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models inherently lack collaborative filtering information, relying primarily on textual content data neglecting other modalities and thus failing to achieve optimal recommendation performance. To address this limitation, we propose Molar, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively. Molar employs an MLLM to generate unified item representations from both textual and non-textual data, facilitating comprehensive multimodal modeling and enriching item embeddings. Additionally, it incorporates collaborative filtering signals through a post-alignment mechanism, which aligns user representations from content-based and ID-based models, ensuring precise personalization and robust performance. By seamlessly combining multimodal content with collaborative filtering insights, Molar captures both user interests and contextual semantics, leading to superior recommendation accuracy. Extensive experiments validate that Molar significantly outperforms traditional and LLM-based baselines, highlighting its strength in utilizing multimodal data and collaborative signals for sequential recommendation tasks. The source code is available at this https URL.', 'abstract_zh': 'sequential推荐（SR）系统在过去十年中经历了显著的发展，从传统的协同过滤模式转向了深度学习方法，并且最近又转向了大语言模型（LLMs）。尽管LLMs的采用推动了显著的进步，但这些模型固有地缺乏协同过滤信息，主要依赖文本内容数据，忽略了其他模态的数据，从而未能达到最佳的推荐性能。为了解决这一局限性，我们提出了Molar，一种多模态大语言模型序列推荐框架，通过整合多种内容模态和ID信息来有效捕捉协同信号。Molar 使用多模态大语言模型（MLLM）从文本和非文本数据中生成统一的项目表示，促进全面的多模态建模并丰富项目嵌入。此外，它通过后对齐机制引入了协同过滤信号，将基于内容和基于ID的模型的用户表示进行对齐，确保精准的个性化并提供稳健的性能。通过无缝地结合多模态内容和协同过滤见解，Molar 捕捉到用户兴趣和上下文语义，从而实现更高的推荐准确性。广泛的实验验证了Molar 显著优于传统的和LLM为基础的基线模型，突显了其利用多模态数据和协同信号执行序列推荐任务的优势。源代码可在以下链接访问：this https URL。', 'title_zh': 'Molar：具有协作过滤对齐的多模态LLMPred系统，以增强序列推荐性能'}
{'arxiv_id': 'arXiv:2412.18174', 'title': 'INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent', 'authors': 'Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow', 'link': 'https://arxiv.org/abs/2412.18174', 'abstract': "Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \\textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.", 'abstract_zh': '近年来的研究凸显了基于大规模语言模型（LLM）的代理在金融决策中的潜力。尽管取得了这些进展，该领域目前面临两个主要挑战：（1）缺乏一个能够适应各种金融任务的全面LLM代理框架，和（2）缺乏标准化的基准和一致的数据集来评估代理性能。为应对这些问题，我们提出了InvestorBench，这是首个专门用于评估应用于多种金融决策场景的LLM代理的基准。InvestorBench通过提供适用于不同金融产品的一系列综合任务，增强了LLM代理的通用性，这些金融产品包括个股（如股票）、加密货币和交易所交易基金（ETF）。此外，我们使用十三种不同的LLM作为主干模型，评估我们的代理框架在各种市场环境和任务下的推理和决策能力。同时，我们整理了一组多元化的开源多模态数据集，并开发了一整套金融决策环境。这为在各种场景下评估金融代理的性能提供了一个高度可访问的平台。', 'title_zh': '投资者基准：基于大语言模型代理的金融决策任务基准'}
{'arxiv_id': 'arXiv:2412.18169', 'title': 'KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management', 'authors': 'Rongxin Cheng, Yifan Peng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen', 'link': 'https://arxiv.org/abs/2412.18169', 'abstract': 'The stateful nature of large language model (LLM) servingcan easily throttle precious GPU memory under load burstor long-generation requests like chain-of-thought reasoning,causing latency spikes due to queuing incoming requests. However, state-of-the-art KVCache centric approaches handleload spikes by dropping, migrating, or swapping KVCache,which faces an essential tradeoff between the performance ofongoing vs. incoming requests and thus still severely this http URL paper makes a key observation such that model param-eters are independent of the requests and are replicated acrossGPUs, and thus proposes a parameter-centric approach byselectively dropping replicated parameters to leave preciousmemory for requests. However, LLM requires KVCache tobe saved in bound with model parameters and thus droppingparameters can cause either huge computation waste or longnetwork delay, affecting all ongoing requests. Based on the ob-servation that attention operators can be decoupled from otheroperators, this paper further proposes a novel remote attentionmechanism through pipeline parallelism so as to serve up-coming requests with the additional memory borrowed fromparameters on remote GPUs. This paper further addresses sev-eral other challenges including lively exchanging KVCachewith incomplete parameters, generating an appropriate planthat balances memory requirements with cooperative exe-cution overhead, and seamlessly restoring parameters whenthe throttling has gone. Evaluations show thatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x compared to the state-of-the-art.', 'abstract_zh': '大型语言模型（LLM）服务的状态特性在负载突变或长时间生成请求（如链式推理）时，可能会迅速耗尽宝贵的GPU内存，从而由于请求队列造成的延迟突增。然而，最先进的基于KV缓存的方法通过丢弃、迁移或交换KV缓存来应对负载突增，这在正在进行的请求和服务中的新请求性能之间存在关键权衡，因而仍然存在显著的问题。本文提出了一个关键观察，即模型参数与请求无关，并且在多个GPU上进行了复制，因此提出了一种参数中心的方法，通过选择性地丢弃复制的参数来保留宝贵的内存以供新请求使用。然而，LLM需要将KV缓存绑定在模型参数中，因此丢弃参数会导致巨大的计算浪费或长时间的网络延迟，从而影响所有正在进行的请求。基于注意力操作与其他操作可以解耦这一观察，本文进一步提出了一种新颖的远程注意力机制，通过流水线并行，以从远程GPU上的参数借用额外的内存来服务即将到来的请求。本文还解决了包括活跃的KV缓存与不完整参数交换、生成满足内存需求与协同执行开销平衡的计划以及在过载解除后无缝恢复参数等其他挑战。评估结果显示，KUNSERVE在过载条件下将请求的尾部TTFT减少了最多27.3倍，相较于最先进的方法。', 'title_zh': 'KunServe：以参数为中心的内存管理下的弹性高效大语言模型服务'}
{'arxiv_id': 'arXiv:2412.18163', 'title': 'Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi', 'authors': 'Rasika Ransing, Mohammed Amaan Dhamaskar, Ayush Rajpurohit, Amey Dhoke, Sanket Dalvi', 'link': 'https://arxiv.org/abs/2412.18163', 'abstract': "India's vast linguistic diversity presents unique challenges and opportunities for technological advancement, especially in the realm of Natural Language Processing (NLP). While there has been significant progress in NLP applications for widely spoken languages, the regional languages of India, such as Marathi and Hindi, remain underserved. Research in the field of NLP for Indian regional languages is at a formative stage and holds immense significance. The paper aims to build a platform which enables the user to use various features like text anonymization, abstractive text summarization and spell checking in English, Hindi and Marathi language. The aim of these tools is to serve enterprise and consumer clients who predominantly use Indian Regional Languages.", 'abstract_zh': '印度庞大的语言多样性为技术进步，特别是在自然语言处理（NLP）领域，带来了独特的挑战和机遇。尽管在广泛使用的语言上已经取得了显著进展，但印度的区域语言，如马尔加希拉语（Marathi）和印地语（Hindi），仍然未得到充分的服务。对于印度区域语言的NLP研究仍处于初期阶段，具有极大的重要性。本文旨在构建一个平台，使用户能够使用诸如文本匿名化、抽象文本总结和拼写检查等特性，支持英语、印地语和马尔加希拉语。这些工具的主要目标是为企业和以印度区域语言为主导的消费者客户群体提供服务。', 'title_zh': '《关于印地语和马拉地语的匿名化、抽象总结与拼写检查的综述》'}
{'arxiv_id': 'arXiv:2412.18157', 'title': 'Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance', 'authors': 'Yaoyun Zhang, Xuenan Xu, Mengyue Wu', 'link': 'https://arxiv.org/abs/2412.18157', 'abstract': 'The video-to-audio (V2A) generation task has drawn attention in the field of multimedia due to the practicality in producing Foley sound. Semantic and temporal conditions are fed to the generation model to indicate sound events and temporal occurrence. Recent studies on synthesizing immersive and synchronized audio are faced with challenges on videos with moving visual presence. The temporal condition is not accurate enough while low-resolution semantic condition exacerbates the problem. To tackle these challenges, we propose Smooth-Foley, a V2A generative model taking semantic guidance from the textual label across the generation to enhance both semantic and temporal alignment in audio. Two adapters are trained to leverage pre-trained text-to-audio generation models. A frame adapter integrates high-resolution frame-wise video features while a temporal adapter integrates temporal conditions obtained from similarities of visual frames and textual labels. The incorporation of semantic guidance from textual labels achieves precise audio-video alignment. We conduct extensive quantitative and qualitative experiments. Results show that Smooth-Foley performs better than existing models on both continuous sound scenarios and general scenarios. With semantic guidance, the audio generated by Smooth-Foley exhibits higher quality and better adherence to physical laws.', 'abstract_zh': '视频到音频（V2A）生成任务由于在制作随场景音效方面的实用性而在多媒体领域引起了广泛关注。通过输入语义和时间条件，生成模型可以指示音效事件及其发生时间。近期关于合成沉浸式且同步的音频的研究，面临着来自具有移动视觉存在的视频的挑战。时间条件不够准确，而低分辨率的语义条件进一步加剧了这一问题。为了应对这些挑战，我们提出了一种名为Smooth-Foley的V2A生成模型，该模型通过跨生成过程从文本标签中获取语义指导，从而增强音频的语义和时间一致性。我们训练了两个适配器来利用预训练的文本到音频生成模型。帧适配器融合了高分辨率的逐帧视频特征，而时间适配器则结合了通过视觉帧相似性和文本标签获得的时间条件。通过从文本标签中获取语义指导实现了精确的音视频对齐。我们进行了广泛的数量和定性实验。结果显示，Smooth-Foley在连续音效场景和一般场景中均优于现有模型。在语义指导的加持下，Smooth-Foley生成的音频质量更高，并且更符合物理定律。', 'title_zh': 'Smooth-Foley: 在语义引导下生成连续音频的视频_to_音频转化方法'}
{'arxiv_id': 'arXiv:2412.18156', 'title': 'scReader: Prompting Large Language Models to Interpret scRNA-seq Data', 'authors': 'Cong Li, Qingqing Long, Yuanchun Zhou, Meng Xiao', 'link': 'https://arxiv.org/abs/2412.18156', 'abstract': 'Large language models (LLMs) have demonstrated remarkable advancements, primarily due to their capabilities in modeling the hidden relationships within text sequences. This innovation presents a unique opportunity in the field of life sciences, where vast collections of single-cell omics data from multiple species provide a foundation for training foundational models. However, the challenge lies in the disparity of data scales across different species, hindering the development of a comprehensive model for interpreting genetic data across diverse organisms. In this study, we propose an innovative hybrid approach that integrates the general knowledge capabilities of LLMs with domain-specific representation models for single-cell omics data interpretation. We begin by focusing on genes as the fundamental unit of representation. Gene representations are initialized using functional descriptions, leveraging the strengths of mature language models such as LLaMA-2. By inputting single-cell gene-level expression data with prompts, we effectively model cellular representations based on the differential expression levels of genes across various species and cell types. In the experiments, we constructed developmental cells from humans and mice, specifically targeting cells that are challenging to annotate. We evaluated our methodology through basic tasks such as cell annotation and visualization analysis. The results demonstrate the efficacy of our approach compared to other methods using LLMs, highlighting significant improvements in accuracy and interoperability. Our hybrid approach enhances the representation of single-cell data and offers a robust framework for future research in cross-species genetic analysis.', 'abstract_zh': '以下是符合学术规范的翻译：\n\n大型语言模型（LLMs）在建模文本序列中的隐藏关系方面取得了显著的进步，这为生命科学领域带来了独特的机会。在这一领域，来自多种物种的单细胞组学数据集为训练基础模型奠定了基础。然而，不同物种间数据规模的差异阻碍了跨物种遗传数据分析全面模型的开发。本研究提出了一种创新的混合方法，将LLMs的一般知识能力与单细胞组学数据领域特定表示模型相结合，用于解释遗传数据。我们从基因作为基本表示单元的角度出发，通过使用功能描述对基因进行初始化，并结合成熟的语言模型，如LaMMA-2的优势。通过输入单细胞水平的基因表达数据和提示信息，我们能够基于不同物种和细胞类型中基因差异表达水平来建模细胞表示。在实验中，我们构建了来自人类和小鼠的发育细胞，并特别针对难以注释的细胞类型。我们通过诸如细胞注释和可视化分析等基本任务，评估了我们的方法。结果显示，与使用LLM的其他方法相比，我们的方法在准确性和互操作性方面表现出明显的改进。我们的混合方法增强了单细胞数据的表示，为跨物种遗传分析未来研究提供了稳健的框架。', 'title_zh': 'scReader: 激励大型语言模型解释单细胞RNA测序数据'}
{'arxiv_id': 'arXiv:2412.18154', 'title': 'GeneSUM: Large Language Model-based Gene Summary Extraction', 'authors': 'Zhijian Chen, Chuan Hu, Min Wu, Qingqing Long, Xuezhi Wang, Yuanchun Zhou, Meng Xiao', 'link': 'https://arxiv.org/abs/2412.18154', 'abstract': 'Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GeneSUM, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research.', 'abstract_zh': '生物医学研究中的新兴主题不断扩展，提供了大量关于基因及其功能的信息。这种知识的快速增长为科学发现提供了前所未有的机会，但同时也给研究人员带来了巨大的挑战，他们需要及时跟进最新的进展。其中一项重大挑战是导航庞大的文献库以提取与基因相关的重要信息，这是一项耗时且繁琐的工作。为了提高这一过程的效率，必须解决几个关键挑战：(1) 文献的庞大数量，(2) 基因功能的复杂性，以及(3) 自动化整合与生成。为此，我们提出了GeneSUM，这是一种利用大规模语言模型（LLM）的两阶段自动基因摘要提取器。我们的方法检索并消除目标基因文献的冗余，然后对LLM进行微调，以改进和简化摘要过程。我们进行了广泛的实验来验证我们提出的框架的有效性。结果表明，大规模语言模型显著增强了基因特异性信息的整合，从而在正在进行的研究中实现了更高效的决策。', 'title_zh': 'GeneSUM：基于大规模语言模型的基因总结提取'}
{'arxiv_id': 'arXiv:2412.18150', 'title': 'EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation', 'authors': 'Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, Chongyi Li', 'link': 'https://arxiv.org/abs/2412.18150', 'abstract': 'Recently, Text-to-Image (T2I) generation models have achieved significant advancements. Correspondingly, many automated metrics have emerged to evaluate the image-text alignment capabilities of generative models. However, the performance comparison among these automated metrics is limited by existing small datasets. Additionally, these datasets lack the capacity to assess the performance of automated metrics at a fine-grained level. In this study, we contribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with fine-grained human annotations for image-text alignment-related tasks. In the construction process, we employ various strategies such as balanced prompt sampling and data re-annotation to ensure the diversity and reliability of our benchmark. This allows us to comprehensively evaluate the effectiveness of image-text alignment metrics for T2I models. Meanwhile, we introduce two new methods to evaluate the image-text alignment capabilities of T2I models: FGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to produce fine-grained image-text alignment scores and PN-VQA which adopts a novel positive-negative VQA manner in VQA models for zero-shot fine-grained evaluation. Both methods achieve impressive performance in image-text alignment evaluations. We also use our methods to rank current AIGC models, in which the results can serve as a reference source for future study and promote the development of T2I generation. The data and code will be made publicly available.', 'abstract_zh': '近年来，文本到图像（T2I）生成模型取得了显著进展。相应地，涌现出许多自动化度量方法来评估生成模型的图像-文本对准能力。然而，这些自动化度量方法的性能比较受到现有小规模数据集的限制。此外，这些数据集不具备从细粒度层面评估自动化度量方法的能力。本研究中，我们贡献了一个名为EvalMuse-40K的基准数据集，该数据集包含40,000个图像-文本对，并附有详细的人工标注，用于图像-文本对准相关任务。在构建过程中，我们采用多种策略，如平衡提示采样和数据重新注释，以确保基准数据集的多样性和可靠性，从而全面评估图像-文本对准度量方法的有效性。同时，我们引入了两种新的方法来评估T2I模型的图像-文本对准能力：FGA-BLIP2方法涉及对视觉语言模型进行端到端微调，以生成细粒度的图像-文本对准评分；PN-VQA方法则在视觉问答模型中采用新颖的正负样本问答方式，实施零样本细粒度评估。这两种方法在图像-文本对准评估中均表现出色。我们还使用这些方法对当前AIGC模型进行了排名，结果可作为未来研究的参考资料，促进T2I生成技术的发展。我们将提供数据和代码的公开访问。', 'title_zh': 'EvalMuse-40K：一种可靠的细粒度基准，包含全面的人类标注，用于文本到图像生成模型评估'}
{'arxiv_id': 'arXiv:2412.18142', 'title': 'Text-Aware Adapter for Few-Shot Keyword Spotting', 'authors': 'Youngmoon Jung, Jinyoung Lee, Seungjin Lee, Myunghun Jung, Yong-Hyeok Lee, Hoon-Young Cho', 'link': 'https://arxiv.org/abs/2412.18142', 'abstract': "Recent advances in flexible keyword spotting (KWS) with text enrollment allow users to personalize keywords without uttering them during enrollment. However, there is still room for improvement in target keyword performance. In this work, we propose a novel few-shot transfer learning method, called text-aware adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for specific keywords with limited speech samples. To adapt the acoustic encoder, we leverage a jointly pre-trained text encoder to generate a text embedding that acts as a representative vector for the keyword. By fine-tuning only a small portion of the network while keeping the core components' weights intact, the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless return to the original pre-trained model. In our experiments, the TA-adapter demonstrated significant performance improvements across 35 distinct keywords from the Google Speech Commands V2 dataset, with only a 0.14% increase in the total number of parameters.", 'abstract_zh': '近年来，柔性关键词识别（Keyword Spotting，KWS）技术在采用文本注册的情况下取得了进展，使得用户能够在注册时无需朗读关键词便实现个性化设置。然而，目标关键词的表现仍有提升空间。本文提出了一种新的少量样本迁移学习方法，称为文本感知适配器（Text-Aware Adapter，TA-adapter），旨在利用有限的语音样本增强预训练的柔性KWS模型，以特定关键词为目标。为了适应声学编码器，我们利用联合预训练的文本编码器生成关键词的文本嵌入，该嵌入作为代表向量。通过仅微调网络的一部分，同时保持核心组件权重不变，TA-适配器在少量样本KWS中表现出极高的效率，并能够无缝切换回原始预训练模型。在我们的实验中，TA-适配器在Google Speech Commands V2数据集中35种不同的关键词上展示了显著的性能提升，仅增加了0.14%的总参数量。', 'title_zh': '文本意识适配器在少样本关键词识别中的应用'}
{'arxiv_id': 'arXiv:2412.18120', 'title': 'Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm', 'authors': 'Xiaoyang Hu, Richard L. Lewis', 'link': 'https://arxiv.org/abs/2412.18120', 'abstract': "Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it's often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argued that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans. By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance instead reflects a limitation in task comprehension and task set maintenance. In addition, we push the best performing model to higher n values and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.", 'abstract_zh': '最初为人类设计的认知任务现在越来越多地用于研究语言模型。虽然将这些任务应用于语言模型通常是直接的，但解释其结果却颇具挑战性。尤其当模型表现不佳时，通常不清楚这是因为被测试的认知能力有限，还是因为未能理解任务本身。最近一项研究认为，GPT 3.5在2-back和3-back任务中的表现下降反映了类似人类的工作记忆容量限制。通过对一系列不同性能水平的开源语言模型在这些任务中的表现进行分析，我们认为表现不佳实际上反映了任务理解能力和任务集维持能力的限制。此外，我们将表现最佳的模型推至更高的n值，并尝试了不同的提示策略，最后分析了模型的注意力机制。我们的更大目标是为持续进行的语言模型认知评估方法改进的对话做出贡献。', 'title_zh': '语言模型是否理解分配给它们的认知任务？使用N--back范式进行探究'}
{'arxiv_id': 'arXiv:2412.18107', 'title': 'SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training', 'authors': 'Jiaxing Yu, Xinda Wu, Yunfei Xu, Tieyao Zhang, Songruoyao Wu, Le Ma, Kejun Zhang', 'link': 'https://arxiv.org/abs/2412.18107', 'abstract': "Lyric-to-melody generation aims to automatically create melodies based on given lyrics, requiring the capture of complex and subtle correlations between them. However, previous works usually suffer from two main challenges: 1) lyric-melody alignment modeling, which is often simplified to one-syllable/word-to-one-note alignment, while others have the problem of low alignment accuracy; 2) lyric-melody harmony modeling, which usually relies heavily on intermediates or strict rules, limiting model's capabilities and generative diversity. In this paper, we propose SongGLM, a lyric-to-melody generation system that leverages 2D alignment encoding and multi-task pre-training based on the General Language Model (GLM) to guarantee the alignment and harmony between lyrics and melodies. Specifically, 1) we introduce a unified symbolic song representation for lyrics and melodies with word-level and phrase-level (2D) alignment encoding to capture the lyric-melody alignment; 2) we design a multi-task pre-training framework with hierarchical blank infilling objectives (n-gram, phrase, and long span), and incorporate lyric-melody relationships into the extraction of harmonized n-grams to ensure the lyric-melody harmony. We also construct a large-scale lyric-melody paired dataset comprising over 200,000 English song pieces for pre-training and fine-tuning. The objective and subjective results indicate that SongGLM can generate melodies from lyrics with significant improvements in both alignment and harmony, outperforming all the previous baseline methods.", 'abstract_zh': '歌词到旋律生成旨在根据给定的歌词自动生成旋律，需要捕捉歌词和旋律之间复杂的细微联系。然而，以往的工作通常存在两个主要挑战：1）歌词-旋律对齐建模，通常简化为一个字/词对应一个音符的对齐，而其他方法则面临对齐精度较低的问题；2）歌词-旋律和声建模，通常过于依赖中间步骤或严格规则，限制了模型的能力和生成多样性。在本文中，我们提出了一种名为SongGLM的歌词到旋律生成系统，该系统利用2D对齐编码和基于通用语言模型（GLM）的多任务预训练，以确保歌词和旋律之间的对齐和和声一致性。具体而言，1）我们引入了一种统一的歌词和旋律符号表示形式，并采用词级和短语级（2D）对齐编码来捕捉歌词-旋律对齐；2）我们设计了一种多任务预训练框架，包含层次空白填充目标（n-gram、短语和长跨度），并将歌词-旋律关系纳入和声化的n-gram的提取中，以确保歌词-旋律的和声一致性。我们还构建了一个包含超过20万首英文歌曲配对数据集用于预训练和微调。客观和主观测试结果表明，SongGLM在对齐和和声方面表现出显著改进，超过了所有之前的基线方法。', 'title_zh': 'SongGLM：基于2D对齐编码和多任务预训练的歌词到旋律生成'}
{'arxiv_id': 'arXiv:2412.18100', 'title': 'EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent', 'authors': 'Suyuan Wang, Xueqian Yin, Menghao Wang, Ruofeng Guo, Kai Nan', 'link': 'https://arxiv.org/abs/2412.18100', 'abstract': 'The rapid growth of scientific techniques and knowledge is reflected in the exponential increase in new patents filed annually. While these patents drive innovation, they also present significant burden for researchers and engineers, especially newcomers. To avoid the tedious work of navigating a vast and complex landscape to identify trends and breakthroughs, researchers urgently need efficient tools to summarize, evaluate, and contextualize patents, revealing their innovative contributions and underlying scientific this http URL address this need, we present EvoPat, a multi-LLM-based patent agent designed to assist users in analyzing patents through Retrieval-Augmented Generation (RAG) and advanced search strategies. EvoPat leverages multiple Large Language Models (LLMs), each performing specialized roles such as planning, identifying innovations, and conducting comparative evaluations. The system integrates data from local databases, including patents, literature, product catalogous, and company repositories, and online searches to provide up-to-date insights. The ability to collect information not included in original database automatically is also implemented. Through extensive testing in the natural language processing (NLP) domain, we demonstrate that EvoPat outperforms GPT-4 in tasks such as patent summarization, comparative analysis, and technical evaluation. EvoPat represents a significant step toward creating AI-powered tools that empower researchers and engineers to efficiently navigate the complexities of the patent landscape.', 'abstract_zh': '科学技术与知识的迅速增长体现在每年提交的新专利数量呈指数级增长上。尽管这些专利推动了创新，但也给研究人员和工程师带来了巨大的负担，尤其是新手。为了避免在庞大而复杂的环境中进行繁琐的工作来识别趋势和突破，研究人员迫切需要高效的工具来总结、评估和语境化专利，揭示其创新贡献和背后的科学意义。为了解决这一需求，我们提出了EvoPat，这是一种基于多大规模语言模型（LLM）的专利代理，旨在通过检索增强生成（RAG）和高级搜索策略帮助用户分析专利。EvoPat利用多个大规模语言模型，每个模型分别承担规划、识别创新和进行对比评价等专业化角色。该系统整合了本地数据库中的数据，包括专利、文献、产品目录和公司库，并结合在线搜索，提供最新的见解。同时，系统还具备自动收集不在原始数据库中但相关的数据信息的能力。通过在自然语言处理（NLP）领域的广泛测试，我们证明EvoPat在专利总结、对比分析和技术评估等任务上优于GPT-4。EvoPat代表了向创建能够帮助研究人员和工程师高效导航专利复杂性的AI工具迈出的重要一步。', 'title_zh': 'EvoPat：一个基于多层次语言模型的专利总结与分析代理'}
{'arxiv_id': 'arXiv:2412.18099', 'title': 'An Attention-based Framework with Multistation Information for Earthquake Early Warnings', 'authors': 'Yu-Ming Huang, Kuan-Yu Chen, Wen-Wei Lin, Da-Yi Chen', 'link': 'https://arxiv.org/abs/2412.18099', 'abstract': 'Earthquake early warning systems play crucial roles in reducing the risk of seismic disasters. Previously, the dominant modeling system was the single-station models. Such models digest signal data received at a given station and predict earth-quake parameters, such as the p-phase arrival time, intensity, and magnitude at that location. Various methods have demonstrated adequate performance. However, most of these methods present the challenges of the difficulty of speeding up the alarm time, providing early warning for distant areas, and considering global information to enhance performance. Recently, deep learning has significantly impacted many fields, including seismology. Thus, this paper proposes a deep learning-based framework, called SENSE, for the intensity prediction task of earthquake early warning systems. To explicitly consider global information from a regional or national perspective, the input to SENSE comprises statistics from a set of stations in a given region or country. The SENSE model is designed to learn the relationships among the set of input stations and the locality-specific characteristics of each station. Thus, SENSE is not only expected to provide more reliable forecasts by considering multistation data but also has the ability to provide early warnings to distant areas that have not yet received signals. This study conducted extensive experiments on datasets from Taiwan and Japan. The results revealed that SENSE can deliver competitive or even better performances compared with other state-of-the-art methods.', 'abstract_zh': '地震预警系统在减少地震灾难风险方面起着至关重要的作用。以往，主要的建模系统是基于单站的模型。这些模型接收某一站点的数据并预测该站点的面波到达时间、震级和强度等参数。虽然各种方法已经证明了足够的性能，但大多数方法仍然面临加快警报时间、为远处区域提供早期预警以及考虑全局信息以提高性能的挑战。最近，深度学习在许多领域产生了重大影响，包括地震学。因此，本文提出了一种基于深度学习的框架，命名为SENSE，用于地震预警系统的强度预测任务。为了从区域或国家的视角明确考虑全局信息，SENSE的输入包括指定区域或国家的一组站点的统计数据。SENSE模型旨在学习一组输入站点之间以及每个站点的地域特性之间的关系。因此，SENSE不仅通过多站点数据的考虑提供更可靠的预测，还能够为尚未接收到信号的远处区域提供早期预警。本研究在台湾和日本的数据集上进行了广泛的实验。结果表明，SENSE在与其他最先进方法的性能比较中，能够提供具有竞争力甚至更优的表现。', 'title_zh': '基于多站信息的注意力机制框架：用于地震早期预警'}
{'arxiv_id': 'arXiv:2412.18097', 'title': 'LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting', 'authors': 'Nan Yang, Chong Wang, Meihua Zhao, Zimeng Zhao, Huiling Zheng, Bin Zhang, Jianing Wang, Xiaofeng Li', 'link': 'https://arxiv.org/abs/2412.18097', 'abstract': 'Ocean forecasting is crucial for both scientific research and societal benefits. Currently, the most accurate forecasting systems are global ocean forecasting systems (GOFSs), which represent the ocean state variables (OSVs) as discrete grids and solve partial differential equations (PDEs) governing the transitions of oceanic state variables using numerical methods. However, GOFSs processes are computationally expensive and prone to cumulative errors. Recently, large artificial intelligence (AI)-based models significantly boosted forecasting speed and accuracy. Unfortunately, building a large AI ocean forecasting system that can be considered cross-spatiotemporal and air-sea coupled forecasts remains a significant challenge. Here, we introduce LangYa, a cross-spatiotemporal and air-sea coupled ocean forecasting system. Results demonstrate that the time embedding module in LangYa enables a single model to make forecasts with lead times ranging from 1 to 7 days. The air-sea coupled module effectively simulates air-sea interactions. The ocean self-attention module improves network stability and accelerates convergence during training, and the adaptive thermocline loss function improves the accuracy of thermocline forecasting. Compared to existing numerical and AI-based ocean forecasting systems, LangYa uses 27 years of global ocean data from the Global Ocean Reanalysis and Simulation version 12 (GLORYS12) for training and achieves more reliable deterministic forecasting results for OSVs. LangYa forecasting system provides global ocean researchers with access to a powerful software tool for accurate ocean forecasting and opens a new paradigm for ocean science.', 'abstract_zh': '海洋预报对于科学研究和社会福祉都至关重要。目前，最准确的预报系统是全球海洋预报系统（GOFSs），它们将海洋状态变量（OSVs）表示为离散网格，并使用数值方法求解描述海洋状态变量转变的偏微分方程（PDEs）。然而，GOFSs的计算成本较高，容易累积误差。近年来，基于大型人工智能（AI）的模型显著提高了预报速度和准确性。不幸的是，构建一个涵盖跨时空尺度和海洋-大气耦合预报的大型AI海洋预报系统仍然是一个显著的挑战。在这里，我们介绍了一种跨时空尺度和海洋-大气耦合的海洋预报系统——LangYa。结果显示，LangYa的时间嵌入模块使其能够预测从1天到7天的不同预报时效。海洋-大气耦合模块有效地模拟了海洋与大气之间的相互作用。海洋自注意力模块在训练期间提高了网络稳定性并加速了收敛，而自适应热跃层损失函数则提高了热跃层预报的准确性。与现有的基于数值和AI的海洋预报系统相比，LangYa使用了1996年至今27年的GLORYS12（全球海洋再分析与模拟版本12）全球海洋数据进行训练，并实现了更为可靠的确定性OSVs预报结果。LangYa预报系统为全球海洋研究人员提供了一种强大的软件工具，用于准确的海洋预报，开启了海洋科学的新范式。', 'title_zh': 'LangYa：革新跨时空海洋预报技术'}
{'arxiv_id': 'arXiv:2412.18092', 'title': 'BRIDGE: Bundle Recommendation via Instruction-Driven Generation', 'authors': 'Tuan-Nghia Bui, Huy-Son Nguyen, Cam-Van Nguyen Thi, Hoang-Quynh Le, Duc-Trong Le', 'link': 'https://arxiv.org/abs/2412.18092', 'abstract': "Bundle recommendation aims to suggest a set of interconnected items to users. However, diverse interaction types and sparse interaction matrices often pose challenges for previous approaches in accurately predicting user-bundle adoptions. Inspired by the distant supervision strategy and generative paradigm, we propose BRIDGE, a novel framework for bundle recommendation. It consists of two main components namely the correlation-based item clustering and the pseudo bundle generation modules. Inspired by the distant supervision approach, the former is to generate more auxiliary information, e.g., instructive item clusters, for training without using external data. This information is subsequently aggregated with collaborative signals from user historical interactions to create pseudo `ideal' bundles. This capability allows BRIDGE to explore all aspects of bundles, rather than being limited to existing real-world bundles. It effectively bridging the gap between user imagination and predefined bundles, hence improving the bundle recommendation performance. Experimental results validate the superiority of our models over state-of-the-art ranking-based methods across five benchmark datasets.", 'abstract_zh': '集合推荐旨在为用户推荐一组相互关联的项目。然而，多样的交互类型和稀疏的交互矩阵常常给先前的方法带来了准确预测用户-集合采用的挑战。受远程监督策略和生成范式的启发，我们提出了一种新的集合推荐框架——BRIDGE。该框架包含两个主要组成部分，即基于相关性的项目聚类模块和伪集合生成模块。受远程监督方法的启发，前者通过生成更多的辅助信息（例如，指导性的项目簇）来训练模型，而不依赖外部数据。这些信息随后与用户的历史互动中的协作信号结合，以创建伪“理想”集合。这种能力使BRIDGE能够全面探索集合的各种方面，而不仅仅是局限于现有的真实世界集合。因此，它有效地弥合了用户想象与预定义集合之间的差距，从而提高了集合推荐的效果。实验结果表明，在五个基准数据集上，我们的模型在基于排名的方法中表现更好。', 'title_zh': 'BRIDGE：基于指令驱动生成的捆绑推荐方法'}
{'arxiv_id': 'arXiv:2412.18090', 'title': 'Multi-Point Positional Insertion Tuning for Small Object Detection', 'authors': 'Kanoko Goto, Takumi Karasawa, Takumi Hirose, Rei Kawakami, Nakamasa Inoue', 'link': 'https://arxiv.org/abs/2412.18090', 'abstract': 'Small object detection aims to localize and classify small objects within images. With recent advances in large-scale vision-language pretraining, finetuning pretrained object detection models has emerged as a promising approach. However, finetuning large models is computationally and memory expensive. To address this issue, this paper introduces multi-point positional insertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for small object detection. Specifically, MPI incorporates multiple positional embeddings into a frozen pretrained model, enabling the efficient detection of small objects by providing precise positional information to latent features. Through experiments, we demonstrated the effectiveness of the proposed method on the SODA-D dataset. MPI performed comparably to conventional PEFT methods, including CoOp and VPT, while significantly reducing the number of parameters that need to be tuned.', 'abstract_zh': '小目标检测旨在定位和分类图像中的小目标。随着大规模视力-语言预训练的进展，微调预训练的目标检测模型已成为一种有前途的方法。然而，微调大型模型在计算和内存上都代价高昂。为了解决这个问题，本文引入了多点位置插入（MPI）微调，这是一种用于小目标检测的参数高效微调（PEFT）方法。具体而言，MPI 将多个位置嵌入整合到一个冻结的预训练模型中，通过提供精确的位置信息给潜在特征，使得模型能够高效地检测小目标。通过实验，我们在 SODA-D 数据集上证明了所提出方法的有效性。MPI 在参数量显著减少的情况下，与传统的 PEFT 方法（如 CoOp 和 VPT）表现出相似的性能。', 'title_zh': '小目标检测中的多点位置插入调优'}
{'arxiv_id': 'arXiv:2412.18086', 'title': 'Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner', 'authors': 'Aizierjiang Aiersilan', 'link': 'https://arxiv.org/abs/2412.18086', 'abstract': 'Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model (LLM) translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at this https URL.', 'abstract_zh': '自主规划是自动驾驶的关键组成部分。最先进的自主规划算法是在精心收集的数据集上训练的，这些数据集不仅标注成本高昂，而且无法全面覆盖罕见但至关重要的场景。未能考虑到这些场景会对自主规划算法构成重大风险，并可能导致测试过程中出现事故。一种直观的解决方案是通过编程和执行模拟器（例如CARLA）手动组合这些场景。然而，这种方法会带来巨大的人力成本。为应对这一挑战，我们提出了一种低成本的方法，以生成多样化的关键交通场景，从而训练更加稳健的自主规划算法。首先，我们将交通场景表示为脚本，这些脚本能用于模拟器生成交通场景。接下来，我们开发了一种方法，该方法接受用户指定的文字描述，一种大型语言模型（LLM）通过上下文学习将其转化为脚本。生成的脚本随后传递给模拟器，以产生相应的交通场景。由于我们的方法能够生成大量安全关键的交通场景，我们将这些场景用作自主规划算法的合成训练数据。为了展示生成场景的价值，我们在合成数据、真实世界数据以及其中的组合上训练现有的自主规划算法。我们的实验结果显示，使用我们数据训练的自主规划算法在性能上显著优于仅基于真实世界数据训练的算法，这表明我们合成数据的有用性和我们数据生成方法的有效性。我们的源代码可从以下链接获得：this https URL。', 'title_zh': '通过上下文学习生成交通场景以获得更好的运动规划器'}
{'arxiv_id': 'arXiv:2412.18082', 'title': 'Prompt Tuning for Item Cold-start Recommendation', 'authors': 'Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian', 'link': 'https://arxiv.org/abs/2412.18082', 'abstract': 'The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios', 'abstract_zh': '冷启动问题是在线推荐系统的关键问题，因为冷启动阶段的成功与否决定了物品能否成功过渡为受欢迎的物品。提示学习作为一种用于自然语言处理（NLP）以解决零样本或少量样本问题的强大技术，已被应用于推荐系统以应对类似挑战。然而，现有的方法通常依赖于基于内容的属性或文本描述作为提示信息，我们提出，对于冷启动推荐而言，这可能是次优的，原因在于1）语义差距与推荐任务不匹配，以及2）模型偏差，由于预热物品在正反馈中起主导作用，这阻碍了冷启动问题，进而影响冷启动物品的推荐质量。为此，我们提出利用高价值正反馈（称为巅峰反馈）作为提示信息，同时解决上述两个问题。实验结果证明，与现有工作中提出的基于内容描述相比，正反馈更适合作为提示信息，因为它可以弥合语义差距。此外，我们提出基于物品的个性化提示网络，通过编码巅峰反馈来缓解由于正反馈主导问题导致的模型偏差。在四个真实世界数据集上的广泛实验表明，我们的模型在指标上优于现有最先进的方法。此外，PROMO已在一款流行的短视频分享平台上成功部署，该平台是一个拥有数亿用户的商业短视频应用，在冷启动场景中，PROMO在各种商业指标上取得了显著的性能提升。', 'title_zh': '商品冷启动推荐的提示调优'}
{'arxiv_id': 'arXiv:2412.18076', 'title': 'COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection', 'authors': 'Chang Liu, Xin Ma, Xiaochen Yang, Yuxiang Zhang, Yanni Dong', 'link': 'https://arxiv.org/abs/2412.18076', 'abstract': 'Single-modal object detection tasks often experience performance degradation when encountering diverse scenarios. In contrast, multimodal object detection tasks can offer more comprehensive information about object features by integrating data from various modalities. Current multimodal object detection methods generally use various fusion techniques, including conventional neural networks and transformer-based models, to implement feature fusion strategies and achieve complementary information. However, since multimodal images are captured by different sensors, there are often misalignments between them, making direct matching challenging. This misalignment hinders the ability to establish strong correlations for the same object across different modalities. In this paper, we propose a novel approach called the CrOss-Mamba interaction and Offset-guided fusion (COMO) framework for multimodal object detection tasks. The COMO framework employs the cross-mamba technique to formulate feature interaction equations, enabling multimodal serialized state computation. This results in interactive fusion outputs while reducing computational overhead and improving efficiency. Additionally, COMO leverages high-level features, which are less affected by misalignment, to facilitate interaction and transfer complementary information between modalities, addressing the positional offset challenges caused by variations in camera angles and capture times. Furthermore, COMO incorporates a global and local scanning mechanism in the cross-mamba module to capture features with local correlation, particularly in remote sensing images. To preserve low-level features, the offset-guided fusion mechanism ensures effective multiscale feature utilization, allowing the construction of a multiscale fusion data cube that enhances detection performance.', 'abstract_zh': '单模态物体检测任务在面对多元场景时经常会出现性能下降的问题。相比之下，多模态物体检测任务可以通过整合多种模态的数据来提供更全面的物体特征信息。目前，多模态物体检测方法通常使用各种融合技术，包括传统的神经网络和基于变换器的模型，来实现特征融合策略并获取互补信息。然而，由于多模态图像是由不同传感器捕获的，这些模态数据之间常常存在对齐问题，这使得直接匹配变得困难。这种对齐问题阻碍了在不同模态中建立强关联性的能力。本文提出了一种名为CrOss-Mamba交互和偏移引导融合（COMO）的框架，旨在解决多模态物体检测任务中的问题。COMO框架利用跨Mamba技术来制定特征交互方程，实现多模态序列化状态计算，从而获得交互融合输出，同时减少计算开销并提高效率。此外，COMO利用高层特征，这些特征较少受到对齐问题的影响，以促进模态之间的交互，并传递互补信息，从而解决由不同拍摄角度和捕获时间引起的位姿偏移问题。同时，COMO在跨Mamba模块中引入了全局和局部扫描机制，以捕获具有局部关联性的特征，特别是在遥感图像中。为了保留低级特征，偏移引导融合机制确保了多尺度特征的有效利用，从而构建多尺度融合数据立方体以增强检测性能。', 'title_zh': 'COMO: 跨Mamba交互与偏移向导融合在多模态目标检测中的应用'}
{'arxiv_id': 'arXiv:2412.18072', 'title': 'MMFactory: A Universal Solution Search Engine for Vision-Language Tasks', 'authors': 'Wan-Cyuan Fan, Tanzila Rahman, Leonid Sigal', 'link': 'https://arxiv.org/abs/2412.18072', 'abstract': 'With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications. Project page is available at this https URL.', 'abstract_zh': '随着基础模型和视觉语言模型以及高效微调技术的发展，已经开发出多种通用和专用模型，用于处理各种视觉任务。尽管这些模型具有灵活和易于访问的特点，但没有单一模型能够处理所有用户可能设想的任务和/或应用程序。最近的方法，如视觉编程和集成了工具的多模态大规模语言模型 (MM-LLM)，旨在通过程序合成来应对复杂的视觉任务。然而，这些方法忽略了用户需求（例如，性能/计算需求），生成了在测试时具有特定样本解决方案，这些解决方案难以部署，并且有时需要低级别的指令，这些指令可能超出了普通用户的技能范围。为了克服这些限制，我们引入了MMFactory，这是一个通用框架，包含模型和评估指标路由组件，类似于一个解决方案搜索引擎，跨越各种可用模型。基于任务描述、少量样本输入-输出对和（可选地）资源和/或性能约束，MMFactory 可以通过实例化和组合其模型库中的视觉语言工具来建议多样性程序解决方案。除了生成这些解决方案外，MMFactory 还建议评估指标，并评估性能/资源特性，使用户能够选择符合其独特设计约束的解决方案。从技术角度来看，我们还引入了一种基于多智能体MM-LLM对话的委员会式解决方案提议者，利用多智能体大规模语言模型对话生成执行、多样化、通用和稳健的解决方案。实验结果表明，MMFactory 在提供针对用户问题规格量身定制的最先进的解决方案方面优于现有方法。项目页面可通过这个链接访问。', 'title_zh': 'MMFactory：视觉-语言任务的通用解决方案搜索引擎'}
{'arxiv_id': 'arXiv:2412.18067', 'title': 'Automated Materials Discovery Platform Realized: Scanning Probe Microscopy of Combinatorial Libraries', 'authors': 'Yu Liu, Rohit Pant, Ichiro Takeuchi, R. Jackson Spurling, Jon-Paul Maria, Maxim Ziatdinov, Sergei V. Kalinin', 'link': 'https://arxiv.org/abs/2412.18067', 'abstract': 'Combinatorial libraries are a powerful approach for exploring the evolution of physical properties across binary and ternary cross-sections in multicomponent phase diagrams. Although the synthesis of these libraries has been developed since the 1960s and expedited with advanced laboratory automation, the broader application of combinatorial libraries relies on fast, reliable measurements of concentration-dependent structures and functionalities. Scanning Probe Microscopies (SPM), including piezoresponse force microscopy (PFM), offer significant potential for quantitative, functionally relevant combi-library readouts. Here we demonstrate the implementation of fully automated SPM to explore the evolution of ferroelectric properties in combinatorial libraries, focusing on Sm-doped BiFeO3 and ZnxMg1-xO systems. We also present and compare Gaussian Process-based Bayesian Optimization models for fully automated exploration, emphasizing local reproducibility (effective noise) as an essential factor in optimal experiment workflows. Automated SPM, when coupled with upstream synthesis controls, plays a pivotal role in bridging materials synthesis and characterization.', 'abstract_zh': '组合图书馆是探索二元和三元截面上多组分相图中物理性质演化的一种强大方法。尽管自20世纪60年代以来组合图书馆的合成已经得到发展，并随着先进实验室自动化技术的进步而加速，但更广泛的组合图书馆应用依赖于快速且可靠的浓度依赖结构和功能的测量。扫描探针显微镜（SPM），包括压电响应力显微镜（PFM），为定量的、功能相关的组合图书馆读数提供了巨大潜力。在这里，我们展示了全自动化SPM在探索Sm掺杂BiFeO3和ZnxMg1-xO系统中铁电性质演化的实施，关注其组合图书馆的应用。我们还介绍了并比较了基于高斯过程的贝叶斯优化模型，强调局部再现性（有效噪声）是优化实验工作流程中的一个关键因素。当与上游合成控制相结合时，自动化SPM在材料合成与表征之间扮演着关键的桥梁角色。', 'title_zh': '实现自动材料发现平台：组合库的扫描探针显微镜研究'}
{'arxiv_id': 'arXiv:2412.18053', 'title': "Neuron Empirical Gradient: Connecting Neurons' Linear Controllability and Representational Capacity", 'authors': 'Xin Zhao, Zehui Jiang, Naoki Yoshinaga', 'link': 'https://arxiv.org/abs/2412.18053', 'abstract': "Although neurons in the feed-forward layers of pre-trained language models (PLMs) can store factual knowledge, most prior analyses remain qualitative, leaving the quantitative relationship among knowledge representation, neuron activations, and model output poorly understood. In this study, by performing neuron-wise interventions using factual probing datasets, we first reveal the linear relationship between neuron activations and output token probabilities. We refer to the gradient of this linear relationship as ``neuron empirical gradients.'' and propose NeurGrad, an efficient method for their calculation to facilitate quantitative neuron analysis. We next investigate whether neuron empirical gradients in PLMs encode general task knowledge by probing skill neurons. To this end, we introduce MCEval8k, a multi-choice knowledge evaluation benchmark spanning six genres and 22 tasks. Our experiments confirm that neuron empirical gradients effectively capture knowledge, while skill neurons exhibit efficiency, generality, inclusivity, and interdependency. These findings link knowledge to PLM outputs via neuron empirical gradients, shedding light on how PLMs store knowledge. The code and dataset are released.", 'abstract_zh': '尽管预训练语言模型（PLMs）的前馈层神经元能够存储事实性知识，但大多数前期分析仍停留在定性层面，量化的知识表示、神经元激活与模型输出之间的关系尚不明确。在本研究中，通过使用事实性探针数据集进行神经元层面的干预，我们首次揭示了神经元激活与输出标记概率之间的线性关系。我们将这种线性关系的梯度称为“神经元经验梯度”（neuron empirical gradients），并提出了一种名为NeurGrad的高效方法，用于这些梯度的计算，以促进神经元的量化分析。接下来，我们探讨了PLMs中的神经元经验梯度是否包含了通用任务知识，为此我们引入了MCEval8k，这是一个涵盖六种类型和22个任务的多选知识评估基准。实验结果表明，神经元经验梯度有效地捕捉了知识，而技能神经元则表现出高效性、通用性、包容性和相互依赖性。这些发现通过神经元经验梯度将知识与PLM输出联系起来，阐明了PLMs如何存储知识。最后，我们将代码和数据集进行了公开发布。', 'title_zh': '神经元经验梯度：连接神经元的线性可控性和表示能力'}
{'arxiv_id': 'arXiv:2412.18052', 'title': 'Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering', 'authors': 'Francois Chaubard, Duncan Eddy, Mykel J. Kochenderfer', 'link': 'https://arxiv.org/abs/2412.18052', 'abstract': 'We introduce Gradient Agreement Filtering (GAF) to improve on gradient averaging in distributed deep learning optimization. Traditional distributed data-parallel stochastic gradient descent involves averaging gradients of microbatches to calculate a macrobatch gradient that is then used to update model parameters. We find that gradients across microbatches are often orthogonal or negatively correlated, especially in late stages of training, which leads to memorization of the training set, reducing generalization. In this paper, we introduce a simple, computationally effective way to reduce gradient variance by computing the cosine distance between micro-gradients during training and filtering out conflicting updates prior to averaging. We improve validation accuracy with significantly smaller microbatch sizes. We also show this reduces memorizing noisy labels. We demonstrate the effectiveness of this technique on standard image classification benchmarks including CIFAR-100 and CIFAR-100N-Fine. We show this technique consistently outperforms validation accuracy, in some cases by up to 18.2\\% compared to traditional training approaches while reducing the computation required nearly an order of magnitude because we can now rely on smaller microbatch sizes without destabilizing training.', 'abstract_zh': '我们提出了梯度一致性过滤（GAF）方法，以改进分布式深层学习优化中的梯度平均。传统的分布式数据并行随机梯度下降涉及通过计算微批次梯度的平均值来得出宏观批次梯度，进而更新模型参数。我们发现，在训练后期，微批次梯度往往彼此正交或负相关，这会导致对训练集的记忆，从而降低泛化能力。在本文中，我们介绍了一种简单且计算高效的减少梯度方差的方法，即在训练过程中计算微梯度的余弦距离，并在平均前过滤出冲突的更新。通过使用显著较小的微批量大小，我们改善了验证准确率，并且还证明这可以减少对噪声标签的记忆。我们在标准图像分类基准数据集CIFAR-100和CIFAR-100N-Fine上展示了该方法的有效性。我们展示了这种方法在某些情况下相比传统训练方法提高了高达18.2%的验证准确率，同时由于我们现在可以依赖较小的微批量大小而不影响训练稳定性，所需的计算量减少了近一个数量级。', 'title_zh': '超越并行优化中的梯度平均：通过梯度一致性筛选提高稳健性'}
{'arxiv_id': 'arXiv:2412.18048', 'title': 'Fair Knowledge Tracing in Second Language Acquisition', 'authors': 'Weitao Tang, Guanliang Chen, Shuaishuai Zu, Jiangyi Luo', 'link': 'https://arxiv.org/abs/2412.18048', 'abstract': "In second-language acquisition, predictive modeling aids educators in implementing diverse teaching strategies, attracting significant research attention. However, while model accuracy is widely explored, model fairness remains under-examined. Model fairness ensures equitable treatment of groups, preventing unintentional biases based on attributes such as gender, ethnicity, or economic background. A fair model should produce impartial outcomes that do not systematically disadvantage any group.\nThis study evaluates the fairness of two predictive models using the Duolingo dataset's en\\_es (English learners speaking Spanish), es\\_en (Spanish learners speaking English), and fr\\_en (French learners speaking English) tracks. We analyze: 1. Algorithmic fairness across platforms (iOS, Android, Web). 2. Algorithmic fairness between developed and developing countries.\nKey findings include: 1. Deep learning outperforms machine learning in second-language knowledge tracing due to improved accuracy and fairness. 2. Both models favor mobile users over non-mobile users. 3. Machine learning exhibits stronger bias against developing countries compared to deep learning. 4. Deep learning strikes a better balance of fairness and accuracy in the en\\_es and es\\_en tracks, while machine learning is more suitable for fr\\_en.\nThis study highlights the importance of addressing fairness in predictive models to ensure equitable educational strategies across platforms and regions.", 'abstract_zh': '在第二语言习得中，预测模型有助于教育者采用多样化的教学策略，引起了广泛的研究关注。然而，尽管模型准确性得到了广泛研究，但模型公平性却未能受到足够的关注。模型公平性确保对各群体的公正对待，防止基于性别、种族或经济背景等因素的无意偏见。一个公平的模型应产生公正的结果，而不系统性地对任何群体造成不利影响。\n\n本研究使用Duolingo数据集中的en\\_es（英语学习者使用西班牙语）、es\\_en（西班牙语学习者使用英语）和fr\\_en（法语学习者使用英语）追踪数据，评估了两种预测模型的公平性。我们分析了：1. 平台（iOS、Android、Web）间的算法公平性；2. 发达国家和发展中国家间的算法公平性。\n\n主要发现包括：1. 深度学习在第二语言知识追踪方面优于机器学习，原因在于提高了准确性和公平性；2. 两种模型都更偏向于移动用户而不利于非移动用户；3. 机器学习对发展中国家的偏见比深度学习更强；4. 在en\\_es和es\\_en追踪中，深度学习在公平性和准确性方面表现更佳，而机器学习更适合fr\\_en。\n\n本研究强调了在预测模型中解决公平性问题的重要性，以确保平台和区域之间的公平教育策略。', 'title_zh': '公平的知识追踪在第二语言习得中的应用'}
{'arxiv_id': 'arXiv:2412.18047', 'title': 'Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control', 'authors': 'Lo Pang-Yun Ting, Ali Şenol, Huan-Yang Wang, Hsu-Chao Lai, Kun-Ta Chuang, Huan Liu', 'link': 'https://arxiv.org/abs/2412.18047', 'abstract': 'The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only effectively reduces electricity expenses but also enhances the resilience of handling practical issues, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. However, existing EV charging strategies have yet to fully consider these factors in a way that benefits both office buildings and EV users simultaneously. To address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and electric vehicles. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, a new critic augmentation is introduced to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information.', 'abstract_zh': '先进双向电动汽车充电和放电技术旨在支持电网稳定和应急操作，正引发对工作场所应用的兴趣增长。它不仅有效降低电费开支，还能增强处理实际问题的韧性，如高峰功率限制、波动的能源价格和不可预测的电动汽车离开情况。然而，现有的电动汽车充电策略尚未以同时惠及办公楼和电动汽车用户的方式充分考虑这些因素。为解决这些问题，我们提出了一种名为HUCA的新型实时充电控制方法，用于调节建筑和电动汽车的能源需求。HUCA采用分层的行为-批评网络，根据动态定价场景下的电动汽车充电需求，动态降低建筑物的电费成本。为应对电动汽车不确定的离开情况，引入了一种新的批评增强方法，以评估充电决策时考虑到离开不确定性，同时保持充电控制的鲁棒性。在实际电力数据集下的模拟确定和不确定离开场景实验中，HUCA在总电费成本方面表现优于基准模型，同时在满足电动汽车充电需求方面保持了竞争力。案例研究表明，HUCA能够根据实时信息有效平衡办公楼和电动汽车之间的能源供应。', 'title_zh': '具有不确定性意识的评论者增强方法用于层次化多代理电动汽车充电控制'}
{'arxiv_id': 'arXiv:2412.18046', 'title': 'Emoji Retrieval from Gibberish or Garbled Social Media Text: A Novel Methodology and A Case Study', 'authors': 'Shuqi Cui, Nirmalya Thakur, Audrey Poon', 'link': 'https://arxiv.org/abs/2412.18046', 'abstract': 'Emojis are widely used across social media platforms but are often lost in noisy or garbled text, posing challenges for data analysis and machine learning. Conventional preprocessing approaches recommend removing such text, risking the loss of emojis and their contextual meaning. This paper proposes a three-step reverse-engineering methodology to retrieve emojis from garbled text in social media posts. The methodology also identifies reasons for the generation of such text during social media data mining. To evaluate its effectiveness, the approach was applied to 509,248 Tweets about the Mpox outbreak, a dataset referenced in about 30 prior works that failed to retrieve emojis from garbled text. Our method retrieved 157,748 emojis from 76,914 Tweets. Improvements in text readability and coherence were demonstrated through metrics such as Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, Automated Readability Index, Dale-Chall Readability Score, Text Standard, and Reading Time. Additionally, the frequency of individual emojis and their patterns of usage in these Tweets were analyzed, and the results are presented.', 'abstract_zh': '社交媒体平台中广泛使用表情符号，但由于噪音或混乱的文本，这些表情符号常常丢失，这对数据分析和机器学习提出了挑战。传统的预处理方法建议移除这样的文本，这可能会导致表情符号及其语境意义的丢失。本文提出了一种三步逆向工程方法，用于从社交媒体帖子中的混乱文本中提取表情符号。该方法还分析了社交媒体数据挖掘过程中生成此类文本的原因。为评估其实效性，该方法应用于关于猴痘疫情的509,248条推文数据集，该数据集被引用了约30篇先前的研究工作，这些研究工作未能从混乱文本中提取表情符号。我们的方法从76,914条推文中提取了157,748个表情符号。通过Flesch阅读易懂性、Flesch-Kincaid阅读等级、Coleman-Liau指数、自动可读性指数、达利-查尔可读性评分、文本标准和阅读时间等指标，证明了文本可读性和连贯性的改进。此外，还分析了这些推文中个别表情符号的频率及其使用模式，并展示了分析结果。', 'title_zh': '来自 gibberish 或乱码社交媒体文本的 Emoji 提取：一种新型方法和案例研究'}
{'arxiv_id': 'arXiv:2412.18043', 'title': 'Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review', 'authors': 'Yidong Gan, Maciej Rybinski, Ben Hachey, Jonathan K. Kummerfeld', 'link': 'https://arxiv.org/abs/2412.18043', 'abstract': 'Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.', 'abstract_zh': '临床编码对于医疗收费和数据分析至关重要。手工临床编码劳动密集且易出错，因此推动了向该过程完全自动化的研究。然而，基于对美国英语电子健康记录和使用这些记录进行自动化编码研究的分析，我们发现广泛使用的评估方法并未与实际临床情境对齐。例如，专注于最常见的前50个代码的评估过于简化，因为在实践中存在数千个不同的代码。本文旨在将AI编码研究更紧密地与临床编码的实际挑战相联系。基于我们的分析，我们提出了八项具体建议，旨在改进当前的评估方法。此外，我们还提出了超越自动化编码的新型AI方法，建议替代方法以协助编码人员在其工作中。', 'title_zh': '将人工智能研究与临床编码工作流程需求对齐：基于美国数据分析与批判性审查的八条建议'}
{'arxiv_id': 'arXiv:2412.18040', 'title': 'Theoretical Constraints on the Expressive Power of $\\mathsf{RoPE}$-based Tensor Attention Transformers', 'authors': 'Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Mingda Wan', 'link': 'https://arxiv.org/abs/2412.18040', 'abstract': "Tensor Attention extends traditional attention mechanisms by capturing high-order correlations across multiple modalities, addressing the limitations of classical matrix-based attention. Meanwhile, Rotary Position Embedding ($\\mathsf{RoPE}$) has shown superior performance in encoding positional information in long-context scenarios, significantly enhancing transformer models' expressiveness. Despite these empirical successes, the theoretical limitations of these technologies remain underexplored. In this study, we analyze the circuit complexity of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention, showing that with polynomial precision, constant-depth layers, and linear or sublinear hidden dimension, they cannot solve fixed membership problems or $(A_{F,r})^*$ closure problems, under the assumption that $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and $\\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling.", 'abstract_zh': '以下是该论文内容或标题的中文翻译，符合学术规范：\n\n张量注意机制通过在多种模态之间捕获高阶相关性，扩展了传统的注意力机制，解决了经典矩阵基注意力的局限性。同时，旋转位置嵌入（$\\mathsf{RoPE}$）在长上下文场景中表现出优越的位置信息编码能力，显著增强了变压器模型的表达能力。尽管这些方法在实际应用中取得了成功，但这些技术的理论局限性仍然未被充分探索。在本研究中，我们分析了张量注意机制和基于$\\mathsf{RoPE}$的张量注意机制的电路复杂性，结果表明，在多项式精度、常深度层及线性或次线性隐藏维度的条件下，它们不能解决固定成员问题或$(A_{F,r})^*$闭包问题，这一结论基于$\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$这一假设。这些发现强调了张量注意机制和基于$\\mathsf{RoPE}$的张量注意机制变压器在实际表现与理论限制之间的差距，提供了指导更多基于理论的方法进行变压器模型设计和扩展的日标见解。', 'title_zh': '基于RoPE的张量注意变换器的表达能力的理论约束'}
{'arxiv_id': 'arXiv:2412.18038', 'title': 'AA-SGAN: Adversarially Augmented Social GAN with Synthetic Data', 'authors': 'Mirko Zaffaroni, Federico Signoretta, Marco Grangetto, Attilio Fiandrotti', 'link': 'https://arxiv.org/abs/2412.18038', 'abstract': 'Accurately predicting pedestrian trajectories is crucial in applications such as autonomous driving or service robotics, to name a few. Deep generative models achieve top performance in this task, assuming enough labelled trajectories are available for training. To this end, large amounts of synthetically generated, labelled trajectories exist (e.g., generated by video games). However, such trajectories are not meant to represent pedestrian motion realistically and are ineffective at training a predictive model. We propose a method and an architecture to augment synthetic trajectories at training time and with an adversarial approach. We show that trajectory augmentation at training time unleashes significant gains when a state-of-the-art generative model is evaluated over real-world trajectories.', 'abstract_zh': '准确预测行人的轨迹在自动驾驶或服务机器人等领域具有重要意义。深度生成模型在这项任务中取得了顶级的表现，前提是拥有足够的标注轨迹进行训练。为此，大量的合成生成且已标注的轨迹存在（例如，由电子游戏生成）。然而，这些轨迹并不旨在真实地模拟行人的运动，并且在训练预测模型方面效果不佳。我们提出了一种方法和一种架构，在训练时对合成轨迹进行增广，并采用了对抗的方法。我们展示了，在对现有最先进的生成模型进行评估时，训练时对轨迹进行增广能够显著提升其在真实世界轨迹上的表现。', 'title_zh': 'AA-SGAN：带有合成数据的对抗增强社会GAN'}
{'arxiv_id': 'arXiv:2412.18036', 'title': 'Explainability in Neural Networks for Natural Language Processing Tasks', 'authors': 'Melkamu Mersha, Mingiziem Bitewa, Tsion Abay, Jugal Kalita', 'link': 'https://arxiv.org/abs/2412.18036', 'abstract': 'Neural networks are widely regarded as black-box models, creating significant challenges in understanding their inner workings, especially in natural language processing (NLP) applications. To address this opacity, model explanation techniques like Local Interpretable Model-Agnostic Explanations (LIME) have emerged as essential tools for providing insights into the behavior of these complex systems. This study leverages LIME to interpret a multi-layer perceptron (MLP) neural network trained on a text classification task. By analyzing the contribution of individual features to model predictions, the LIME approach enhances interpretability and supports informed decision-making. Despite its effectiveness in offering localized explanations, LIME has limitations in capturing global patterns and feature interactions. This research highlights the strengths and shortcomings of LIME and proposes directions for future work to achieve more comprehensive interpretability in neural NLP models.', 'abstract_zh': '神经网络通常被视为黑盒模型，这在理解其内部工作机制方面产生了重大挑战，尤其是在自然语言处理（NLP）应用中。为解决这一透明度问题，诸如局部可解释的模型无关解释（LIME）等模型解释技术已经成为了提供这些复杂系统行为见解的必不可少的工具。本研究利用LIME来解释在文本分类任务上训练的多层感知机（MLP）神经网络。通过对个体特征对模型预测贡献的分析，LIME方法增强了可解释性并支持了基于信息的决策。尽管LIME在提供局部解释方面非常有效，但它在捕捉全局模式和特征交互方面存在一定局限性。本研究强调了LIME的优点和局限性，并提出了未来工作的新方向，以实现更全面的神经NLP模型可解释性。', 'title_zh': '自然语言处理任务中神经网络的可解释性'}
{'arxiv_id': 'arXiv:2412.18023', 'title': 'More than Chit-Chat: Developing Robots for Small-Talk Interactions', 'authors': 'Rebecca Ramnauth, Dražen Brščić, Brian Scassellati', 'link': 'https://arxiv.org/abs/2412.18023', 'abstract': "Beyond mere formality, small talk plays a pivotal role in social dynamics, serving as a verbal handshake for building rapport and understanding. For conversational AI and social robots, the ability to engage in small talk enhances their perceived sociability, leading to more comfortable and natural user interactions. In this study, we evaluate the capacity of current Large Language Models (LLMs) to drive the small talk of a social robot and identify key areas for improvement. We introduce a novel method that autonomously generates feedback and ensures LLM-generated responses align with small talk conventions. Through several evaluations -- involving chatbot interactions and human-robot interactions -- we demonstrate the system's effectiveness in guiding LLM-generated responses toward realistic, human-like, and natural small-talk exchanges.", 'abstract_zh': '超越单纯的礼仪，闲聊在社交动态中发挥着关键作用，作为建立共鸣和沟通理解的口头握手。对于对话式人工智能和社交机器人而言，具备进行闲聊的能力可以增强它们的社交感知度，从而实现更为舒适和自然的用户互动。在本研究中，我们评估了当前大规模语言模型（LLM）在驱动社交机器人闲聊方面的能力，并确定了需要改进的关键领域。我们提出了一种新颖的方法，能够自主生成反馈并确保LLM生成的回复符合闲聊规范。通过多项评估——包括聊天机器人互动和人机互动——我们证明了该系统在引导LLM生成的回复向真实、人类般的自然闲聊交流方向发展的有效性。', 'title_zh': '不仅仅是闲聊：开发用于短对话的机器人'}
{'arxiv_id': 'arXiv:2412.18022', 'title': 'Trustworthy and Efficient LLMs Meet Databases', 'authors': 'Kyoungmin Kim, Anastasia Ailamaki', 'link': 'https://arxiv.org/abs/2412.18022', 'abstract': 'In the rapidly evolving AI era with large language models (LLMs) at the core, making LLMs more trustworthy and efficient, especially in output generation (inference), has gained significant attention. This is to reduce plausible but faulty LLM outputs (a.k.a hallucinations) and meet the highly increased inference demands. This tutorial explores such efforts and makes them transparent to the database community. Understanding these efforts is essential in harnessing LLMs in database tasks and adapting database techniques to LLMs. Furthermore, we delve into the synergy between LLMs and databases, highlighting new opportunities and challenges in their intersection. This tutorial aims to share with database researchers and practitioners essential concepts and strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining in the intersection between LLMs and databases.', 'abstract_zh': '在以大规模语言模型（LLMs）为核心的快速演化的AI时代，提高LLMs的可靠性和效率，特别是在输出生成（推理）方面，已经引起了广泛关注。这旨在减少可能但虚假的LLMs输出（即幻觉），以满足高度增加的推理需求。本教程探讨了这些努力，并使数据库社区对此了解透明。理解这些努力对于在数据库任务中利用LLMs以及将数据库技术与LLMs相结合至关重要。此外，我们还探讨了LLMs与数据库之间的协同作用，突显了它们交集中的新机遇和挑战。本教程旨在与数据库研究人员和实践者分享有关LLMs的基本概念和策略，减少对LLMs的陌生感，并激发他们参与到LLMs与数据库的交集中来。', 'title_zh': '可信且高效的大型语言模型与数据库相遇'}
{'arxiv_id': 'arXiv:2412.18003', 'title': 'Integrated Learning and Optimization for Congestion Management and Profit Maximization in Real-Time Electricity Market', 'authors': 'Imran Pervez, Ricardo Pinto Lima, Omar Knio', 'link': 'https://arxiv.org/abs/2412.18003', 'abstract': 'We develop novel integrated learning and optimization (ILO) methodologies to solve economic dispatch (ED) and DC optimal power flow (DCOPF) problems for better economic operation. The optimization problem for ED is formulated with load being an unknown parameter while DCOPF consists of load and power transfer distribution factor (PTDF) matrix as unknown parameters. PTDF represents the incremental variations of real power on transmission lines which occur due to real power transfers between two regions. These values represent a linearized approximation of power flows over the transmission lines. We develop novel ILO formulations to solve post-hoc penalties in electricity market and line congestion problems using ED and DCOPF optimization formulations. Our proposed methodologies capture the real-time electricity market and line congestion behavior to train the regret function which eventually train unknown loads at different buses and line PTDF matrix to achieve the afore-mentioned post-hoc goals. The proposed methodology is compared to sequential learning and optimization (SLO) which train load and PTDF forecasts for accuracy rather than economic operation. Our experimentation prove the superiority of ILO in minimizing the post-hoc penalties in electricity markets and minimizing the line congestion thereby improving the economic operation with noticeable amount.', 'abstract_zh': '我们开发了新颖的集成学习与优化（ILO）方法，以解决经济调度（ED）和直流最优潮流（DCOPF）问题，从而实现更经济的运行。ED的优化问题将负荷作为未知参数进行建模，而DCOPF则包含负荷和功率传输分布因子（PTDF）矩阵作为未知参数。PTDF代表因两个区域之间的有功功率传输而导致的传输线有功功率的增量变化，这些值代表了输电线路上传输功率的线性化近似。我们开发了新颖的ILO建模方法，以利用ED和DCOPF优化建模方法解决事后惩罚在电力市场和线路拥塞问题。所提出的方法捕捉到了实时电力市场和线路拥塞行为，训练后悔函数，最终训练不同节点的未知负荷和线路PTDF矩阵，以实现上述事后目标。我们提出的办法与顺序学习和优化（SLO）进行了比较，SLO注重的是负荷和PTDF的准确性而非经济效益。我们的实验结果证明了ILO在减少电力市场的事后惩罚、减少线路拥塞方面具有优越性，并且在经济运营方面取得了显著改进。', 'title_zh': '实时电力市场中 congestion 管理与利润最大化的一体化学习与优化'}
{'arxiv_id': 'arXiv:2412.17998', 'title': 'WavePulse: Real-time Content Analytics of Radio Livestreams', 'authors': 'Govind Mittal, Sarthak Gupta, Shruti Wagle, Chirag Chopra, Anthony J DeMattee, Nasir Memon, Mustaque Ahamad, Chinmay Hegde', 'link': 'https://arxiv.org/abs/2412.17998', 'abstract': "Radio remains a pervasive medium for mass information dissemination, with AM/FM stations reaching more Americans than either smartphone-based social networking or live television. Increasingly, radio broadcasts are also streamed online and accessed over the Internet. We present WavePulse, a framework that records, documents, and analyzes radio content in real-time. While our framework is generally applicable, we showcase the efficacy of WavePulse in a collaborative project with a team of political scientists focusing on the 2024 Presidential Elections. We use WavePulse to monitor livestreams of 396 news radio stations over a period of three months, processing close to 500,000 hours of audio streams. These streams were converted into time-stamped, diarized transcripts and analyzed to track answer key political science questions at both the national and state levels. Our analysis revealed how local issues interacted with national trends, providing insights into information flow. Our results demonstrate WavePulse's efficacy in capturing and analyzing content from radio livestreams sourced from the Web. Code and dataset can be accessed at \\url{this https URL}.", 'abstract_zh': '广播仍然是大众信息传播的一个普遍媒介，AM/FM台的听众人数超过了基于智能手机的社交媒体和现场电视。越来越多地，广播内容也通过网络流式传输并在互联网上访问。我们提出了一种名为WavePulse的框架，用于实时记录、文档化和分析广播内容。虽然我们的框架具有普遍适用性，但我们展示了WavePulse在与一组政治科学家团队合作项目中的有效性，该团队关注2024年总统选举。我们使用WavePulse监控了396个新闻广播台的现场流式传输，共处理了接近50万小时的音频流。这些流被转换为带时间戳的分辩文本，并分析以跟踪涉及国家级和州级关键政治科学问题的动态信息流动。我们的分析揭示了地方问题与国家趋势之间的互动，提供了有关信息流动的见解。我们的结果表明，WavePulse能够有效捕捉和分析来自网络的广播现场流内容。代码和数据集可以在 \\url{该链接} 获取。', 'title_zh': 'WavePulse：实时分析无线广播直播内容'}
{'arxiv_id': 'arXiv:2412.17993', 'title': 'Multi-Agent Path Finding in Continuous Spaces with Projected Diffusion Models', 'authors': 'Jinhao Liang, Jacob K. Christopher, Sven Koenig, Ferdinando Fioretto', 'link': 'https://arxiv.org/abs/2412.17993', 'abstract': 'Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics, requiring the computation of collision-free paths for multiple agents moving from their respective start to goal positions. Coordinating multiple agents in a shared environment poses significant challenges, especially in continuous spaces where traditional optimization algorithms struggle with scalability. Moreover, these algorithms often depend on discretized representations of the environment, which can be impractical in image-based or high-dimensional settings. Recently, diffusion models have shown promise in single-agent path planning, capturing complex trajectory distributions and generating smooth paths that navigate continuous, high-dimensional spaces. However, directly extending diffusion models to MAPF introduces new challenges since these models struggle to ensure constraint feasibility, such as inter-agent collision avoidance. To overcome this limitation, this work proposes a novel approach that integrates constrained optimization with diffusion models for MAPF in continuous spaces. This unique combination directly produces feasible multi-agent trajectories that respect collision avoidance and kinematic constraints. The effectiveness of our approach is demonstrated across various challenging simulated scenarios of varying dimensionality.', 'abstract_zh': '多智能体路径寻找（Multi-Agent Path Finding, MAPF）是机器人领域的一项基础问题，要求为多个智能体从各自的起始位置到目标位置生成无碰撞的路径。在共享环境中协调多个智能体移动带来了显著的挑战，尤其是在连续空间中，传统的优化算法往往在可扩展性方面存在困难。此外，这些算法常依赖于环境的离散表示，这在基于图像或高维设置的情况下可能不切实际。最近，扩散模型在单智能体路径规划方面显示了潜力，能够捕捉复杂的轨迹分布并生成平滑路径，以导航连续的高维空间。然而，直接将扩散模型扩展到MAPF引入了新的挑战，因为这些模型难以确保满足约束条件，例如避免智能体间的碰撞。为此，本文提出了一种新颖的方法，结合约束优化与扩散模型在连续空间中的MAPF问题中。这种独特的结合可以直接生成满足碰撞避免和动力学约束的可行多智能体轨迹。我们的方法在各种具有不同维度的挑战性模拟场景中得到了有效性验证。', 'title_zh': '在连续空间中使用投影扩散模型的多代理路径规划'}
{'arxiv_id': 'arXiv:2412.17984', 'title': 'ICPR 2024 Competition on Domain Adaptation and GEneralization for Character Classification (DAGECC)', 'authors': 'Sofia Marino, Jennifer Vandoni, Emanuel Aldea, Ichraq Lemghari, Sylvie Le Hégarat-Mascle, Frédéric Jurie', 'link': 'https://arxiv.org/abs/2412.17984', 'abstract': 'In this companion paper for the DAGECC (Domain Adaptation and GEneralization for Character Classification) competition organized within the frame of the ICPR 2024 conference, we present the general context of the tasks we proposed to the community, we introduce the data that were prepared for the competition and we provide a summary of the results along with a description of the top three winning entries. The competition was centered around domain adaptation and generalization, and our core aim is to foster interest and facilitate advancement on these topics by providing a high-quality, lightweight, real world dataset able to support fast prototyping and validation of novel ideas.', 'abstract_zh': '本文是ICPR 2024会议框架内组织的DAGECC（域适应和泛化在字符分类任务中的应用）竞赛的配套论文。在此论文中，我们介绍了我们为该社区提出的任务的总体背景，介绍了为竞赛准备的数据集，并提供了结果总结，包括前三名获奖作品的描述。竞赛主要围绕域适应和泛化展开，我们的核心目标是通过提供高质量、轻量级且适用于实际场景的数据集，激发兴趣并促进相关领域的研究进展，支持新型想法的快速原型设计和验证。', 'title_zh': 'ICPR 2024 韧性字符分类的领域适应与泛化竞赛 (DAGECC)'}
{'arxiv_id': 'arXiv:2412.17977', 'title': 'TNNGen: Automated Design of Neuromorphic Sensory Processing Units for Time-Series Clustering', 'authors': 'Prabhu Vellaisamy, Harideep Nair, Vamsikrishna Ratnakaram, Dhruv Gupta, John Paul Shen', 'link': 'https://arxiv.org/abs/2412.17977', 'abstract': "Temporal Neural Networks (TNNs), a special class of spiking neural networks, draw inspiration from the neocortex in utilizing spike-timings for information processing. Recent works proposed a microarchitecture framework and custom macro suite for designing highly energy-efficient application-specific TNNs. These recent works rely on manual hardware design, a labor-intensive and time-consuming process. Further, there is no open-source functional simulation framework for TNNs. This paper introduces TNNGen, a pioneering effort towards the automated design of TNNs from PyTorch software models to post-layout netlists. TNNGen comprises a novel PyTorch functional simulator (for TNN modeling and application exploration) coupled with a Python-based hardware generator (for PyTorch-to-RTL and RTL-to-Layout conversions). Seven representative TNN designs for time-series signal clustering across diverse sensory modalities are simulated and their post-layout hardware complexity and design runtimes are assessed to demonstrate the effectiveness of TNNGen. We also highlight TNNGen's ability to accurately forecast silicon metrics without running hardware process flow.", 'abstract_zh': '时空神经网络（TNNs），一种特殊的脉冲神经网络，借鉴了新皮层利用脉冲时机进行信息处理的方式。近期的研究提出了一种微架构框架和定制的宏级工具集，以设计高度能效的应用特定TNN。这些近期的研究依赖于手动硬件设计，这是一个劳动密集型且耗时的过程。此外，目前没有开源的功能模拟框架可用于TNN。本文介绍了TNNGen，这是一种从PyTorch软件模型自动设计TNN至后布局网表的开创性努力。TNNGen包括一种新颖的PyTorch功能模拟器（用于TNN建模和应用探索）以及基于Python的硬件生成器（用于PyTorch到RTL和RTL到布局的转换）。我们模拟了跨多种感官模态的时间序列信号聚类的七个代表性TNN设计，并评估了它们的后布局硬件复杂性和设计运行时间，以证明TNNGen的有效性。我们还强调了TNNGen能够准确预测硅片性能指标的能力，而无需运行硬件工艺流程。', 'title_zh': 'TNNGen: 自动设计的时间序列聚类神经形态感知处理单元'}
{'arxiv_id': 'arXiv:2412.17975', 'title': 'Improving Sickle Cell Disease Classification: A Fusion of Conventional Classifiers, Segmented Images, and Convolutional Neural Networks', 'authors': 'Victor Júnio Alcântara Cardoso, Rodrigo Moreira, João Fernando Mari, Larissa Ferreira Rodrigues Moreira', 'link': 'https://arxiv.org/abs/2412.17975', 'abstract': 'Sickle cell anemia, which is characterized by abnormal erythrocyte morphology, can be detected using microscopic images. Computational techniques in medicine enhance the diagnosis and treatment efficiency. However, many computational techniques, particularly those based on Convolutional Neural Networks (CNNs), require high resources and time for training, highlighting the research opportunities in methods with low computational overhead. In this paper, we propose a novel approach combining conventional classifiers, segmented images, and CNNs for the automated classification of sickle cell disease. We evaluated the impact of segmented images on classification, providing insight into deep learning integration. Our results demonstrate that using segmented images and CNN features with an SVM achieves an accuracy of 96.80%. This finding is relevant for computationally efficient scenarios, paving the way for future research and advancements in medical-image analysis.', 'abstract_zh': '镰状细胞贫血（Sickle cell anemia）是一种以异常红细胞形态为特征的疾病，可以通过显微图像检测出来。医学中的计算技术能够提升诊断和治疗的效率。然而，许多计算技术，尤其是基于卷积神经网络（CNNs）的方法，需要大量的资源和时间进行训练，这凸显了低计算开销方法研究的机会。在本文中，我们提出了一种结合传统分类器、分割图像和CNN的新方法，用于镰状细胞疾病的自动化分类。我们评估了分割图像对分类的影响，为深度学习的集成提供了见解。我们的结果显示，使用分割图像和CNN特征的SVM模型可以达到96.80%的准确率。这一发现对于计算效率高的场景具有重要意义，为未来医学图像分析的研究和进步铺平了道路。', 'title_zh': '提高镰状细胞病分类效果：传统分类器、分割图像与卷积神经网络的融合'}
{'arxiv_id': 'arXiv:2412.17967', 'title': 'Towards Cognitive Service Delivery on B5G through AIaaS Architecture', 'authors': 'Larissa F. Rodrigues Moreira, Rodrigo Moreira, Flávio de Oliveira Silva, André R. Backes', 'link': 'https://arxiv.org/abs/2412.17967', 'abstract': "Artificial Intelligence (AI) is pivotal in advancing mobile network systems by facilitating smart capabilities and automation. The transition from 4G to 5G has substantial implications for AI in consolidating a network predominantly geared towards business verticals. In this context, 3GPP has specified and introduced the Network Data Analytics Function (NWDAF) entity at the network's core to provide insights based on AI algorithms to benefit network orchestration. This paper proposes a framework for evolving NWDAF that presents the interfaces necessary to further empower the core network with AI capabilities B5G and 6G. In addition, we identify a set of research directions for realizing a distributed e-NWDAF.", 'abstract_zh': '人工智能（AI）在推动移动网络系统方面起着关键作用，通过实现智能功能和自动化。从4G到5G的过渡对AI在构建以垂直行业为主导的网络方面产生了重大影响。在此背景下，3GPP已规定并引入了网络核心中的网络数据管理功能（NWDAF）实体，通过基于AI算法提供见解来改进网络编排。本文提出了一种不断演化的NWDAF框架，以呈现进一步增强核心网络AI能力所需的接口。此外，我们还确定了一组实现分布式e-NWDAF的研究方向。', 'title_zh': '面向B5G的认知服务交付架构研究——基于AIaaS架构'}
{'arxiv_id': 'arXiv:2412.17966', 'title': 'tuGEMM: Area-Power-Efficient Temporal Unary GEMM Architecture for Low-Precision Edge AI', 'authors': 'Harideep Nair, Prabhu Vellaisamy, Albert Chen, Joseph Finn, Anna Li, Manav Trivedi, John Paul Shen', 'link': 'https://arxiv.org/abs/2412.17966', 'abstract': 'General matrix multiplication (GEMM) is a ubiquitous computing kernel/algorithm for data processing in diverse applications, including artificial intelligence (AI) and deep learning (DL). Recent shift towards edge computing has inspired GEMM architectures based on unary computing, which are predominantly stochastic and rate-coded systems. This paper proposes a novel GEMM architecture based on temporal-coding, called tuGEMM, that performs exact computation. We introduce two variants of tuGEMM, serial and parallel, with distinct area/power-latency trade-offs. Post-synthesis Power-Performance-Area (PPA) in 45 nm CMOS are reported for 2-bit, 4-bit, and 8-bit computations. The designs illustrate significant advantages in area-power efficiency over state-of-the-art stochastic unary systems especially at low precisions, e.g. incurring just 0.03 mm^2 and 9 mW for 4 bits, and 0.01 mm^2 and 4 mW for 2 bits. This makes tuGEMM ideal for power constrained mobile and edge devices performing always-on real-time sensory processing.', 'abstract_zh': '以下是对原文的翻译，同时确保符合学术规范：\n\n通用矩阵乘法（GEMM）是一种在包括人工智能（AI）和深度学习（DL）等多种应用中无处不在的计算内核/算法。随着向边缘计算的转变，基于一元计算的GEMM架构得到了启发，这些架构主要为随机性和基于速率编码的系统。本文提出了一种基于时序编码的新型GEMM架构，记为tuGEMM，能够进行精确计算。我们提出了两种不同的tuGEMM变体：串行和并行，这两种变体在面积/功耗/延迟方面具有不同的权衡。在45纳米CMOS工艺后端综合的性能-功耗-面积（PPA）分析中，报告了2位、4位和8位计算的结果。设计结果展示了在低精度（如4位和2位）下相对于现有的先进随机一元系统的优势，例如，仅消耗0.03 mm²和9 mW的能量或0.01 mm²和4 mW的能量。这使得tuGEMM成为执行始终在线的实时感测处理的理想选择，特别是在功率受限的移动和边缘设备中。', 'title_zh': 'tuGEMM：面向低精度边缘AI的面积-功耗高效时间一元GEMM架构'}
{'arxiv_id': 'arXiv:2412.17965', 'title': 'LMV-RPA: Large Model Voting-based Robotic Process Automation', 'authors': 'Osama Abdellatif, Ahmed Ayman, Ali Hamdi', 'link': 'https://arxiv.org/abs/2412.17965', 'abstract': 'Automating high-volume unstructured data processing is essential for operational efficiency. Optical Character Recognition (OCR) is critical but often struggles with accuracy and efficiency in complex layouts and ambiguous text. These challenges are especially pronounced in large-scale tasks requiring both speed and precision. This paper introduces LMV-RPA, a Large Model Voting-based Robotic Process Automation system to enhance OCR workflows. LMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR, Easy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and Gemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs into structured JSON formats, improving accuracy, particularly in complex layouts. The multi-phase pipeline processes text extracted by OCR engines through LLMs, combining results to ensure the most accurate outputs. LMV-RPA achieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94 percent, while reducing processing time by 80 percent. Benchmark evaluations confirm its scalability and demonstrate that LMV-RPA offers a faster, more reliable, and efficient solution for automating large-scale document processing tasks.', 'abstract_zh': '大规模自动化无结构数据处理对于提高操作效率至关重要。光学字符识别（OCR）是关键步骤，但往往在复杂布局和模糊文本中面临着准确性和效率的挑战。这些挑战在需要快速和精准的大型任务中尤为突出。本文介绍了基于大型模型投票的机器人流程自动化系统（LMV-RPA），以增强OCR工作流程。LMV-RPA 将 Paddle OCR、Tesseract OCR、Easy OCR 和 DocTR 等OCR引擎的输出与 LLaMA 3 和 Gemini-1.5-pro 等大型语言模型（LLMs）整合在一起。利用多数投票机制，它将OCR输出转换为结构化的JSON格式，特别是在复杂布局中提高了准确性。多阶段管道流程将由OCR引擎提取的文本通过LLMs处理，并结合结果以确保最准确的输出。LMV-RPA 的OCR任务准确率达到99%，超过基线模型94%的准确率，同时将处理时间减少80%。基准评估证实了其可扩展性，并表明LMV-RPA 提供了一种更快、更可靠且更高效的自动化大规模文档处理任务的解决方案。', 'title_zh': 'LMV-RPA：基于大型模型投票的机器人流程自动化'}
{'arxiv_id': 'arXiv:2412.17959', 'title': 'Analysis of Transferred Pre-Trained Deep Convolution Neural Networks in Breast Masses Recognition', 'authors': 'Qusay Shihab Hamad, Hussein Samma, Shahrel Azmin Suandi', 'link': 'https://arxiv.org/abs/2412.17959', 'abstract': "Breast cancer detection based on pre-trained convolution neural network (CNN) has gained much interest among other conventional computer-based systems. In the past few years, CNN technology has been the most promising way to find cancer in mammogram scans. In this paper, the effect of layer freezing in a pre-trained CNN is investigated for breast cancer detection by classifying mammogram images as benign or malignant. Different VGG19 scenarios have been examined based on the number of convolution layer blocks that have been frozen. There are a total of six scenarios in this study. The primary benefits of this research are twofold: it improves the model's ability to detect breast cancer cases and it reduces the training time of VGG19 by freezing certain this http URL evaluate the performance of these scenarios, 1693 microbiological images of benign and malignant breast cancers were utilized. According to the reported results, the best recognition rate was obtained from a frozen first block of VGG19 with a sensitivity of 95.64 %, while the training of the entire VGG19 yielded 94.48%.", 'abstract_zh': '基于预训练卷积神经网络（CNN）的乳腺癌检测引起了其他传统计算机系统广泛的兴趣。在过去的几年中，CNN技术已经成为在乳腺X线摄影图像中发现癌症最有前途的方法之一。在本文中，我们研究了在预训练CNN中冻结层对乳腺癌检测的影响，通过将乳腺X线摄影图像分类为良性或恶性来实现这一目标。基于冻结卷积层块的数量，我们探讨了不同VGG19的几种情形。本研究中共有六种情形。本研究的主要优点有两点：一是提高了模型检测乳腺癌的能力；二是通过冻结某些层从而使VGG19的训练时间减少。为了评估这些情形的表现，我们利用了1693张良性与恶性乳腺癌的微生物学图像。根据报告的结果，从冻结VGG19的第一个块中获得了最佳识别率，其敏感性为95.64%，而完整训练整个VGG19的结果为94.48%。', 'title_zh': '乳腺肿块识别中转移预训练深度卷积神经网络的分析'}
{'arxiv_id': 'arXiv:2412.17957', 'title': 'ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling', 'authors': 'S. Rasoulzadeh, M. Bank, M. Wimmer, I. Kovacic, K. Schinegger, S. Rutzinger', 'link': 'https://arxiv.org/abs/2412.17957', 'abstract': '$\\textit{ArchComplete}$ is a two-stage dense voxel-based 3D generative pipeline developed to tackle the high complexity in architectural geometries and topologies, assisting with ideation and geometric detailisation in the early design process. In stage 1, a $\\textit{3D Voxel VQGAN}$ model is devised, whose composition is then modelled with an autoregressive transformer for generating coarse models. Subsequently, in stage 2, $\\textit{Hierarchical Voxel Upsampling Networks}$ consisting of a set of 3D conditional denoising diffusion probabilistic models are defined to augment the coarse shapes with fine geometric details. The first stage is trained on a dataset of house models with fully modelled exteriors and interiors with a novel 2.5D perceptual loss to capture input complexities across multiple abstraction levels, while the second stage trains on randomly cropped local volumetric patches, requiring significantly less compute and memory. For inference, the pipeline first autoregressively generates house models at a resolution of $64^3$ and then progressively refines them to resolution of $256^3$ with voxel sizes as small as $18\\text{cm}$. ArchComplete supports a range of interaction modes solving a variety of tasks, including interpolation, variation generation, unconditional synthesis, and two conditional synthesis tasks: shape completion and plan-drawing completion, as well as geometric detailisation. The results demonstrate notable improvements against state-of-the-art on established metrics.', 'abstract_zh': '$\\textit{ArchComplete}$ 是一个两阶段的密集体素基三维生成流水线，旨在应对建筑设计中复杂的几何形状和拓扑结构，辅助早期设计过程中的构想和几何细节化。在第一阶段，设计了一个 $\\textit{3D 轴量化 VQGAN}$ 模型，其组成通过自回归变压器建模，用于生成粗略的模型。随后，在第二阶段，定义了一组三维条件去噪扩散概率模型组成的层次体素上采样网络，以补充粗略形状的精细几何细节。第一阶段利用包含新颖二维半感知损失的数据集进行训练，该损失能捕捉不同抽象层次的输入复杂性，而第二阶段则在随机裁剪的局部体素块上进行训练，所需计算资源和内存显著减少。对于推理，流水线首先自回归生成分辨率为 $64^3$ 的房屋模型，然后逐步细化至 $256^3$ 的分辨率，体素大小可小至 $18\\text{cm}$。$\\textit{ArchComplete}$ 支持多种交互模式，可以解决多种任务，包括插值、变体生成、无条件合成，以及两种条件合成任务：形状完成和规划图完成，还有几何细节化。实验结果表明，在现有指标上取得了显著的改进。', 'title_zh': 'ArchComplete: 基于分层扩散上采样的自回归三维建筑设计生成'}
{'arxiv_id': 'arXiv:2412.17953', 'title': 'Adaptive Signal Analysis for Automated Subsurface Defect Detection Using Impact Echo in Concrete Slabs', 'authors': 'Deepthi Pavurala, Duoduo Liao, Chaithra Reddy Pasunuru', 'link': 'https://arxiv.org/abs/2412.17953', 'abstract': 'This pilot study presents a novel, automated, and scalable methodology for detecting and evaluating subsurface defect-prone regions in concrete slabs using Impact Echo (IE) signal analysis. The approach integrates advanced signal processing, clustering, and visual analytics to identify subsurface anomalies. A unique adaptive thresholding method tailors frequency-based defect identification to the distinct material properties of each slab. The methodology generates frequency maps, binary masks, and k-means cluster maps to automatically classify defect and non-defect regions. Key visualizations, including 3D surface plots, cluster maps, and contour plots, are employed to analyze spatial frequency distributions and highlight structural anomalies. The study utilizes a labeled dataset constructed at the Federal Highway Administration (FHWA) Advanced Sensing Technology Nondestructive Evaluation Laboratory. Evaluations involve ground-truth masking, comparing the generated defect maps with top-view binary masks derived from the information provided by the FHWA. The performance metrics, specifically F1-scores and AUC-ROC, achieve values of up to 0.95 and 0.83, respectively. The results demonstrate the robustness of the methodology, consistently identifying defect-prone areas with minimal false positives and few missed defects. Adaptive frequency thresholding ensures flexibility in addressing variations across slabs, providing a scalable framework for detecting structural anomalies. Additionally, the methodology is adaptable to other frequency-based signals due to its generalizable thresholding mechanism and holds potential for integrating multimodal sensor fusion. This automated and scalable pipeline minimizes manual intervention, ensuring accurate and efficient defect detection, further advancing Non-Destructive Evaluation (NDE) techniques.', 'abstract_zh': '本试点研究提出了一种新颖、自动化且可扩展的方法，该方法利用冲击回波（IE）信号分析来检测和评估混凝土板中潜在缺陷的地下区域。该方法将先进的信号处理、聚类和可视化分析集成起来，以识别地下异常。该方法采用了一种独特的自适应阈值方法，根据不同混凝土板的材料特性定制基于频率的缺陷识别。该方法生成了频率图、二进制掩模和k-均值聚类图，以自动分类缺陷和非缺陷区域。关键可视化包括三维表面图、聚类图和等高线图，用于分析空间频率分布并突显结构异常。该研究利用了联邦公路管理局（FHWA）高级传感技术非破坏性评估实验室构建的带标签数据集。评估包括地面真实掩模，将生成的缺陷图与FHWA提供的信息所生成的俯视图二进制掩模进行比较。性能指标，特别是F1分数和AUC-ROC，分别达到了0.95和0.83。研究结果表明该方法具有鲁棒性，能够一致地识别潜在缺陷区域，且误检和漏检缺陷较少。自适应频率阈值确保在不同板之间处理频率变化时具有灵活性，为检测结构异常提供了可扩展框架。此外，由于其可泛化的阈值机制，该方法还可以适应其他基于频率的信号，并具有将多模传感器融合技术集成的潜力。这种自动化且可扩展的管道减少了人工干预，确保了缺陷检测的准确性与效率，进一步促进了非破坏性评估（NDE）技术的发展。', 'title_zh': '使用混凝土板冲击回波进行自动地下缺陷检测的自适应信号分析'}
{'arxiv_id': 'arXiv:2412.17933', 'title': 'BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism', 'authors': 'Martin Fajcik, Martin Docekal, Jan Dolezal, Karel Ondrej, Karel Beneš, Jan Kapsa, Pavel Smrz, Alexander Polok, Michal Hradis, Zuzana Neverilova, Ales Horak, Radoslav Sabol, Michal Stefanik, Adam Jirkovsky, David Adamczyk, Petr Hyner, Jan Hula, Hynek Kydlicek', 'link': 'https://arxiv.org/abs/2412.17933', 'abstract': 'We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 11 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word.\nFurthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis, (ii) continuous pretraining of the first Czech-centric 7B language model, with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard, with existing 44 model submissions, where new model submissions can be made at this https URL.', 'abstract_zh': '我们提出了BenCzechMark（BCM），这是首个为大型语言模型设计的综合性捷克语基准测试，提供了各种任务、多种任务格式和多种评估指标。其评分系统基于统计显著性理论，并借鉴社会偏好理论，在任务之间进行聚合。我们的基准测试包括50项具有挑战性的任务，其中包括相应的测试数据集，主要为捷克原文，另有11个新收集的数据集。这些任务涵盖了8个类别，涵盖了不同的领域，包括历史捷克新闻、学生或语言学习者的散文，以及口语。\n\n此外，我们收集并清洗了BUT-Large捷克语语料库，这是目前最大的公开可用的洁净捷克语语料库，我们利用它进行以下工作：(i) 污染分析；(ii) 第一个以捷克为中心的7B语言模型的持续预训练，其中包含捷克特定的分词。我们将我们的模型作为基线，用于与已有的多语言模型进行比较。最后，我们发布并维护了一个排行榜，已有44个模型提交，新的模型提交可以通过以下链接进行提交：[相应链接]。', 'title_zh': 'BenCzechMark：一种基于捷克语的多任务和多指标基准测试，配备双重评分机制的大语言模型评估标准'}
{'arxiv_id': 'arXiv:2412.17910', 'title': 'A Novel Approach to Balance Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes and its Implementation in BEACON', 'authors': 'Vansh Nagpal, Siva Likitha Valluru, Kausik Lakkaraju, Nitin Gupta, Zach Abdulrahman, Andrew Davison, Biplav Srivastava', 'link': 'https://arxiv.org/abs/2412.17910', 'abstract': '"A common decision made by people, whether healthy or with health conditions, is choosing meals like breakfast, lunch, and dinner, comprising combinations of foods for appetizer, main course, side dishes, desserts, and beverages. Often, this decision involves tradeoffs between nutritious choices (e.g., salt and sugar levels, nutrition content) and convenience (e.g., cost and accessibility, cuisine type, food source type). We present a data-driven solution for meal recommendations that considers customizable meal configurations and time horizons. This solution balances user preferences while accounting for food constituents and cooking processes. Our contributions include introducing goodness measures, a recipe conversion method from text to the recently introduced multimodal rich recipe representation (R3) format, learning methods using contextual bandits that show promising preliminary results, and the prototype, usage-inspired, BEACON system."', 'abstract_zh': '无论是健康人群还是有健康状况的人，都会面临选择一日三餐（早餐、午餐和晚餐）的问题，这些餐饭由开胃菜、主菜、配菜、甜点和饮料等多种食品组合而成。这个选择通常涉及营养选择（例如，盐分和糖分水平，营养成分）与便利性（例如，成本和可获取性、菜系类型、食材来源）之间的权衡。我们提出了一种基于数据的餐食推荐解决方案，该方案考虑了可定制的餐食配置和时间范围。该解决方案在满足用户偏好时，也会考虑到食物成分和烹饪过程。我们的贡献包括引入了良好的度量标准，一种将文本食谱转换为最近引入的多模态丰富食谱表示（R3格式）的方法，以及通过上下文多臂赌注学习方法取得初步积极成果，还有基于使用启发的BEACON系统原型。', 'title_zh': '一种在长期群体推荐中平衡便利性和营养性的新方法：多模态食谱的多元推理及其在BEACON中的实现'}
{'arxiv_id': 'arXiv:2412.17891', 'title': 'The Power of Adaptation: Boosting In-Context Learning through Adaptive Prompting', 'authors': 'Shuzhang Cai, Twumasi Mensah-Boateng, Xander Kuksov, Jing Yuan, Shaojie Tang', 'link': 'https://arxiv.org/abs/2412.17891', 'abstract': "Large Language Models (LLMs) have demonstrated exceptional abilities across a broad range of language-related tasks, including generating solutions to complex reasoning problems. An effective technique to enhance LLM performance is in-context learning, which encourages a step-by-step reasoning process by including explanatory examples to guide the model's responses. However, selecting appropriate exemplars for the model poses a challenge, as each dataset demands a distinct set of exemplars to enable the LLM to learn effectively and perform well on the test set. Current studies often rely on uncertainty- or diversity-based selection strategies to select exemplars for annotation and to improve model learning. However, these studies typically employ a non-adaptive approach, selecting a set of exemplars all at once. We argue that this non-adaptive strategy may result in a set of exemplars with high redundancy in terms of the knowledge covered, ultimately reducing their overall informativeness. To address this limitation, we propose \\textsc{Adaptive-Prompt}, a novel method that adaptively selects exemplars by leveraging model feedback from previously chosen exemplars. Experimental results show that \\textsc{Adaptive-Prompt} significantly enhances LLM performance across a variety of reasoning tasks.", 'abstract_zh': '大型语言模型（LLMs）在广泛的语言相关任务中表现出色，包括解决复杂的推理问题。提高LLM性能的有效技术是上下文学习，通过包含解释性示例来引导模型的响应过程，从而逐步促进推理。然而，为模型选择合适的示例是一项挑战，因为每个数据集需要一组独特的示例来帮助LLM有效地学习并在测试集上表现出色。当前的研究通常依赖于基于不确定性或多样性的选择策略来选择用于注释和改进模型学习的示例。然而，这些研究通常采用非适应性方法，一次选择一组示例。我们指出，这种非适应性策略可能造成所选示例在覆盖的知识方面具有高度冗余性，从而降低其整体信息量。为了解决这一局限性，我们提出了一种新颖的方法——Adaptive-Prompt，该方法通过利用模型对先前选定示例的反馈信息来适应性地选择示例。实验结果表明，Adaptive-Prompt在各种推理任务上显著提升了LLM的性能。', 'title_zh': '适应性的力量：通过自适应提示增强上下文学习'}
{'arxiv_id': 'arXiv:2412.17888', 'title': 'Stability Bounds for the Unfolded Forward-Backward Algorithm', 'authors': 'Emilie Chouzenoux, Cecile Della Valle, Jean-Christophe Pesquet', 'link': 'https://arxiv.org/abs/2412.17888', 'abstract': 'We consider a neural network architecture designed to solve inverse problems where the degradation operator is linear and known. This architecture is constructed by unrolling a forward-backward algorithm derived from the minimization of an objective function that combines a data-fidelity term, a Tikhonov-type regularization term, and a potentially nonsmooth convex penalty. The robustness of this inversion method to input perturbations is analyzed theoretically. Ensuring robustness complies with the principles of inverse problem theory, as it ensures both the continuity of the inversion method and the resilience to small noise - a critical property given the known vulnerability of deep neural networks to adversarial perturbations. A key novelty of our work lies in examining the robustness of the proposed network to perturbations in its bias, which represents the observed data in the inverse problem. Additionally, we provide numerical illustrations of the analytical Lipschitz bounds derived in our analysis.', 'abstract_zh': '我们将重点放在一个用于解决降级操作符已知且为线性的逆问题的神经网络架构上。该架构通过从目标函数的最小化衍生出的正向-反向算法“展开”而成，该目标函数结合了数据保真项、Tikhonov类型正则化项以及一个潜在的非光滑凸惩罚项。我们从理论角度分析了该逆方法对输入扰动的鲁棒性。确保鲁棒性符合逆问题理论的原则，因为它确保了逆方法的连续性和对小噪声的鲁棒性，这一点至关重要，因为已知深度神经网络对对抗性扰动极为敏感。我们工作的关键新颖之处在于，我们考察了提出网络对其偏差（逆问题中观测数据的表征）扰动的鲁棒性。此外，我们还提供了分析中推导出的数值Lipschitz界示例。', 'title_zh': '展开的前向-后向算法的稳定性界'}
{'arxiv_id': 'arXiv:2412.17883', 'title': 'In Defence of Post-hoc Explainability', 'authors': 'Nick Oh', 'link': 'https://arxiv.org/abs/2412.17883', 'abstract': 'The widespread adoption of machine learning in scientific research has created a fundamental tension between model opacity and scientific understanding. Whilst some advocate for intrinsically interpretable models, we introduce Computational Interpretabilism (CI) as a philosophical framework for post-hoc interpretability in scientific AI. Drawing parallels with human expertise, where post-hoc rationalisation coexists with reliable performance, CI establishes that scientific knowledge emerges through structured model interpretation when properly bounded by empirical validation. Through mediated understanding and bounded factivity, we demonstrate how post-hoc methods achieve epistemically justified insights without requiring complete mechanical transparency, resolving tensions between model complexity and scientific comprehension.', 'abstract_zh': '机器学习在科学研究中的广泛应用在根本上引发了模型不透明性和科学理解之间的紧张关系。虽然有人倡导内在可解释的模型，我们提出了计算可解释性（Computational Interpretabilism, CI）作为科学人工智能后验可解释性的哲学框架。类比人类专业知识，其中后验理性化与可靠表现并存，CI 认为当模型在经验验证的约束下进行结构化解读时，科学知识得以产生。通过中介的理解和有边界的真知识，我们展示了后验方法如何在不需要完全机械透明性的前提下获得 epistemically（证认识论上）正当的洞察，从而解决模型复杂性和科学理解之间的紧张关系。', 'title_zh': '论事后解释的有效性'}
{'arxiv_id': 'arXiv:2412.17874', 'title': 'Evaluating LLM Reasoning in the Operations Research Domain with ORQA', 'authors': 'Mahdi Mostajabdaveh, Timothy T. Yu, Samarendra Chandan Bindu Dash, Rindranirina Ramamonjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, Yong Zhang', 'link': 'https://arxiv.org/abs/2412.17874', 'abstract': 'In this paper, we introduce and apply Operations Research Question Answering (ORQA), a new benchmark designed to assess the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR). This benchmark evaluates whether LLMs can emulate the knowledge and reasoning skills of OR experts when confronted with diverse and complex optimization problems. The dataset, developed by OR experts, features real-world optimization problems that demand multistep reasoning to construct their mathematical models. Our evaluations of various open source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting a gap in their ability to generalize to specialized technical domains. This work contributes to the ongoing discourse on LLMs generalization capabilities, offering valuable insights for future research in this area. The dataset and evaluation code are publicly available.', 'abstract_zh': '在本文中，我们引入并应用了Operation Research Question Answering (ORQA)基准，这是一种新的基准测试，旨在评估大型语言模型（LLMs）在运筹学（OR）这一专门技术领域的泛化能力。该基准测试评估LLMs在面对多样化和复杂优化问题时，能否模拟运筹学专家的知识和推理技能。数据集由运筹学专家开发，其中包含需要多步推理来构建数学模型的实际优化问题。我们对各种开源LLMs（如LLaMA 3.1、DeepSeek和Mixtral）的评估结果显示，它们的表现较为有限，突显了它们在向专门技术领域泛化的能力不足。本文为LLMs泛化能力的持续讨论做出了贡献，并为该领域的未来研究提供了宝贵的见解。该数据集和评估代码已向社会公开。', 'title_zh': '使用ORQA评估大型语言模型在运筹学领域的推理能力'}
{'arxiv_id': 'arXiv:2412.17872', 'title': 'Joint Knowledge Editing for Information Enrichment and Probability Promotion', 'authors': 'Wenhang Shi, Yiren Chen, Shuqing Bian, Xinyi Zhang, Zhe Zhao, Pengfei Hu, Wei Lu, Xiaoyong Du', 'link': 'https://arxiv.org/abs/2412.17872', 'abstract': "Knowledge stored in large language models requires timely updates to reflect the dynamic nature of real-world information. To update the knowledge, most knowledge editing methods focus on the low layers, since recent probes into the knowledge recall process reveal that the answer information is enriched in low layers. However, these probes only and could only reveal critical recall stages for the original answers, while the goal of editing is to rectify model's prediction for the target answers. This inconsistency indicates that both the probe approaches and the associated editing methods are deficient. To mitigate the inconsistency and identify critical editing regions, we propose a contrast-based probe approach, and locate two crucial stages where the model behavior diverges between the original and target answers: Information Enrichment in low layers and Probability Promotion in high layers. Building upon the insights, we develop the Joint knowledge Editing for information Enrichment and probability Promotion (JEEP) method, which jointly edits both the low and high layers to modify the two critical recall stages. Considering the mutual interference and growing forgetting due to dual modifications, JEEP is designed to ensure that updates to distinct regions share the same objectives and are complementary. We rigorously evaluate JEEP by editing up to thousands of facts on various models, i.e., GPT-J (6B) and LLaMA (7B), and addressing diverse editing objectives, i.e., adding factual and counterfactual knowledge. In all tested scenarios, JEEP achieves best performances, validating the effectiveness of the revealings of our probe approach and the designs of our editing method. Our code and data are available at this https URL.", 'abstract_zh': '大型语言模型中存储的知识需要及时更新以反映现实世界信息的动态性质。为了更新知识，大多数知识编辑方法集中于低层，因为最近对知识检索过程的探查表明，答案信息主要集中在低层。然而，这些探查仅揭示了原始答案的关键检索阶段，而编辑的目标则是纠正模型对目标答案的预测。这种不一致性表明，现有的探查方法及其相关编辑方法均有不足。为了缓解这种不一致性并识别关键编辑区域，我们提出了基于对比的方法，并定位了模型行为在原始答案和目标答案之间发生分歧的两个关键阶段：低层信息丰富化和高层概率促进。基于这些见解，我们开发了联合信息丰富化和概率促进的知识编辑方法（JEEP），旨在同时编辑低层和高层，以修改这两个关键检索阶段。考虑到双重修改导致的相互干扰和遗忘增加，JEEP 设计为确保不同区域的更新共享相同的目标并相互补充。我们通过在不同模型（如 GPT-J（6B）和 LLaMA（7B））上编辑数千个事实，并应对各种编辑目标（如添加事实和反事实知识），严格评估了 JEEP 的性能。在所有测试场景中，JEEP 都取得了最佳表现，验证了我们探查方法揭示的有效性和编辑方法设计的有效性。我们的代码和数据可在以下链接获取：[此处插入链接]。', 'title_zh': '联合知识编辑以实现信息丰富和概率增强'}
{'arxiv_id': 'arXiv:2412.17867', 'title': 'Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types', 'authors': 'Ziming Guo, Chao Ma, Yinggang Sun, Tiancheng Zhao, Guangyao Wang, Hai Huang', 'link': 'https://arxiv.org/abs/2412.17867', 'abstract': "Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q\\&A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries.", 'abstract_zh': '近年来，大型语言模型（LLMs）的发展显著提升了文本到SQL（text-to-SQL）系统的性能。然而，大多数基于LLM的方法往往倾向于仅专注于SQL生成，忽视了实际对话查询的复杂性。这种忽略可能导致响应不可靠，特别是对于那些不能直接通过SQL解决的含糊问题。为解决这一问题，我们提出了一种名为MMSQL的全面测试套件，该套件通过模拟包含多种问题类型和多轮Q&A交互的现实场景来评估LLM的问题分类和SQL生成能力。利用MMSQL，我们评估了包括开源和闭源模型在内的流行LLM的性能，并识别了影响它们在这些场景中表现的关键因素。此外，我们还引入了一种基于LLM的多智能体框架，该框架使用专门的智能体来识别问题类型并确定相应的回答策略。实验结果表明，这种方法显著增强了模型在处理对话动态复杂性方面的能力，能够有效应对用户查询的多样性和复杂性。', 'title_zh': '评估和提升多轮文本到SQL转换的LLM性能，支持多种问题类型'}
{'arxiv_id': 'arXiv:2412.17854', 'title': 'Active Geospatial Search for Efficient Tenant Eviction Outreach', 'authors': 'Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik', 'link': 'https://arxiv.org/abs/2412.17854', 'abstract': 'Tenant evictions threaten housing stability and are a major concern for many cities. An open question concerns whether data-driven methods enhance outreach programs that target at-risk tenants to mitigate their risk of eviction. We propose a novel active geospatial search (AGS) modeling framework for this problem. AGS integrates property-level information in a search policy that identifies a sequence of rental units to canvas to both determine their eviction risk and provide support if needed. We propose a hierarchical reinforcement learning approach to learn a search policy for AGS that scales to large urban areas containing thousands of parcels, balancing exploration and exploitation and accounting for travel costs and a budget constraint. Crucially, the search policy adapts online to newly discovered information about evictions. Evaluation using eviction data for a large urban area demonstrates that the proposed framework and algorithmic approach are considerably more effective at sequentially identifying eviction cases than baseline methods.', 'abstract_zh': '租户驱逐会威胁住房稳定性，并且是许多城市的一大关注点。一个悬而未决的问题在于，数据驱动的方法是否能够增强针对高风险租户的 outreach 项目，以减轻他们被驱逐的风险。本文提出了一种新的主动地理空间搜索（AGS）建模框架来解决这一问题。AGS 将物业级别的信息整合进搜索策略中，以识别一系列需要访问的租赁单位，以确定其驱逐风险并提供必要的支持。我们提出了一种分层强化学习方法来学习适用于 AGS 的搜索策略，该方法适用于包含数千个地块的大城市区域，平衡探索与利用，并考虑到旅行成本和预算限制。更重要的是，搜索策略能够根据新发现的驱逐信息进行在线调整。通过使用某大城市区域的驱逐数据进行评估表明，所提出的框架及其算法方法在逐次识别驱逐案例方面比基线方法更为有效。', 'title_zh': '高效的租户驱逐宣传活动中的主动地理空间搜索'}
{'arxiv_id': 'arXiv:2412.17839', 'title': 'LaMI-GO: Latent Mixture Integration for Goal-Oriented Communications Achieving High Spectrum Efficiency', 'authors': 'Achintha Wijesinghe, Suchinthaka Wanninayaka, Weiwei Wang, Yu-Chieh Chao, Songyang Zhang, Zhi Ding', 'link': 'https://arxiv.org/abs/2412.17839', 'abstract': 'The recent rise of semantic-style communications includes the development of goal-oriented communications (GOCOMs) remarkably efficient multimedia information transmissions. The concept of GO-COMS leverages advanced artificial intelligence (AI) tools to address the rising demand for bandwidth efficiency in applications, such as edge computing and Internet-of-Things (IoT). Unlike traditional communication systems focusing on source data accuracy, GO-COMs provide intelligent message delivery catering to the special needs critical to accomplishing downstream tasks at the receiver. In this work, we present a novel GO-COM framework, namely LaMI-GO that utilizes emerging generative AI for better quality-of-service (QoS) with ultra-high communication efficiency. Specifically, we design our LaMI-GO system backbone based on a latent diffusion model followed by a vector-quantized generative adversarial network (VQGAN) for efficient latent embedding and information representation. The system trains a common feature codebook the receiver side. Our experimental results demonstrate substantial improvement in perceptual quality, accuracy of downstream tasks, and bandwidth consumption over the state-of-the-art GOCOM systems and establish the power of our proposed LaMI-GO communication framework.', 'abstract_zh': '近年来，语义-style 通讯的兴起包括目标导向通讯（GOCOMs）的显著多媒体信息传输发展。GO-COMS 的概念利用先进的人工智能（AI）工具，以应对边缘计算和物联网（IoT）等应用中对带宽效率日益增长的需求。与传统通信系统主要关注源数据准确性不同，GOCOMs 提供了适应接收端完成下游任务特殊需求的智能消息传递。在这项工作中，我们提出了一种新颖的 GOCOM 框架，称为 LaMI-GO，该框架利用新兴的生成式 AI 提供更高的服务质量（QoS）和超高通信效率。具体而言，我们以潜在扩散模型为基础设计 LaMI-GO 系统的骨干，并结合矢量量化生成对抗网络（VQGAN）进行高效的潜在嵌入和信息表示。系统在接收端训练了一个公共特征码书。实验结果表明，LaMI-GO 框架在感知质量、下游任务准确性及带宽消耗方面显著优于现有最先进的 GOCOM 系统，并证明了我们提出的 LaMI-GO 通讯框架的强大性能。', 'title_zh': 'LaMI-GO：潜在混合集成用于目标导向通信，实现高效频谱利用率'}
{'arxiv_id': 'arXiv:2412.17838', 'title': 'Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning', 'authors': 'Shuyi Wang, Huan Zhao, Yuji Cao, Zibin Pan, Guolong Liu, Gaoqi Liang, Junhua Zhao', 'link': 'https://arxiv.org/abs/2412.17838', 'abstract': 'The Wind Storage Integrated System with Power Smoothing Control (PSC) has emerged as a promising solution to ensure both efficient and reliable wind energy generation. However, existing PSC strategies overlook the intricate interplay and distinct control frequencies between batteries and wind turbines, and lack consideration of wake effect and battery degradation cost. In this paper, a novel coordinated control framework with hierarchical levels is devised to address these challenges effectively, which integrates the wake model and battery degradation model. In addition, after reformulating the problem as a Markov decision process, the multi-agent reinforcement learning method is introduced to overcome the bi-level characteristic of the problem. Moreover, a Physics-informed Neural Network-assisted Multi-agent Deep Deterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate the power fluctuation differential equation and expedite the learning process. The effectiveness of the proposed methodology is evaluated through simulations conducted in four distinct scenarios using WindFarmSimulator (WFSim). The results demonstrate that the proposed algorithm facilitates approximately an 11% increase in total profit and a 19% decrease in power fluctuation compared to the traditional methods, thereby addressing the dual objectives of economic efficiency and grid-connected energy reliability.', 'abstract_zh': '具有功率平滑控制（PSC）的风储一体化系统已经成为了确保风能高效可靠生成的一种有前景的解决方案。然而，现有的PSC策略忽视了电池与风力涡轮机之间的复杂交互和不同的控制频率，并且没有考虑到尾流效应和电池退化成本。在本文中，我们设计了一种分层式的新型协调控制框架以有效应对这些挑战，该框架整合了尾流模型和电池退化模型。此外，在将问题重新表述为马尔可夫决策过程之后，我们引入了多智能体强化学习方法以克服该问题的多级特性。同时，我们提出了一种基于物理信息神经网络的多智能体深度确定性策略梯度算法（PAMA-DDPG），以结合功率波动微分方程并加速学习过程。通过使用WindFarmSimulator (WFSim) 在四个不同的场景下进行仿真，我们评估了所提出的方法的有效性。结果表明，所提出的算法在总利润方面约提高了11%，在功率波动方面约减少了19%，从而实现了经济效率和并网能源可靠性双重目标。', 'title_zh': '基于物理信息深度强化学习的风储一体化系统协调功率平滑控制'}
{'arxiv_id': 'arXiv:2412.17833', 'title': 'Transfer Learning with Active Sampling for Rapid Training and Calibration in BCI-P300 Across Health States and Multi-centre Data', 'authors': 'Christian Flores, Marcelo Contreras, Ichiro Macedo, Javier Andreu-Perez', 'link': 'https://arxiv.org/abs/2412.17833', 'abstract': "Machine learning and deep learning advancements have boosted Brain-Computer Interface (BCI) performance, but their wide-scale applicability is limited due to factors like individual health, hardware variations, and cultural differences affecting neural data. Studies often focus on uniform single-site experiments in uniform settings, leading to high performance that may not translate well to real-world diversity. Deep learning models aim to enhance BCI classification accuracy, and transfer learning has been suggested to adapt models to individual neural patterns using a base model trained on others' data. This approach promises better generalizability and reduced overfitting, yet challenges remain in handling diverse and imbalanced datasets from different equipment, subjects, multiple centres in different countries, and both healthy and patient populations for effective model transfer and tuning.\nIn a setting characterized by maximal heterogeneity, we proposed P300 wave detection in BCIs employing a convolutional neural network fitted with adaptive transfer learning based on Poison Sampling Disk (PDS) called Active Sampling (AS), which flexibly adjusts the transition from source data to the target domain. Our results reported for subject adaptive with 40% of adaptive fine-tuning that the averaged classification accuracy improved by 5.36% and standard deviation reduced by 12.22% using two distinct, internationally replicated datasets. These results outperformed in classification accuracy, computational time, and training efficiency, mainly due to the proposed Active Sampling (AS) method for transfer learning.", 'abstract_zh': '机器学习和深度学习的进步提高了脑-计算机接口（BCI）的性能，但由于个体健康状况、硬件差异和文化差异等因素的影响，其广泛应用受到了限制。许多研究往往集中在统一的单一站点实验中，并在统一的环境中进行，这可能导致在具有良好性能的同时，难以适应现实世界的多样性。深度学习模型旨在提高BCI分类准确性，已有研究表明，通过迁移学习可以在其他人的数据上训练的预训练模型来适应个体神经模式，从而有望提高泛化能力和减少过拟合。然而，在处理来自不同设备、受试者、多中心不同国家和健康与患者群体的多样化和不均衡数据集时，仍面临诸多挑战。\n\n在最大异质性的环境中，我们提出了一种基于Poisson Sampling Disk（PDS）的自适应迁移学习方法，称为活跃采样（Active Sampling, AS），这种方法在基于PDS的卷积神经网络中适应性地调整从源数据到目标域的过渡。使用两个国际上可复制的独立数据集进行的实验结果表明，在40%的自适应微调条件下，基于活跃采样的分类准确率平均提高了5.36%，标准差降低了12.22%。这些结果在分类准确率、计算时间和训练效率方面都优于传统的迁移学习方法。前景在于所提出的活跃采样（AS）方法在迁移学习中的应用。', 'title_zh': '基于主动采样的迁移学习在健康状态和多中心数据中的快速训练与校准：BCI-P300的应用'}
{'arxiv_id': 'arXiv:2412.17832', 'title': 'MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes', 'authors': 'Jiaqing Zhang, Miguel Contreras, Sabyasachi Bandyopadhyay, Andrea Davidson, Jessica Sena, Yuanfang Ren, Ziyuan Guan, Tezcan Ozrazgat-Baslanti, Tyler J. Loftus, Subhash Nerella, Azra Bihorac, Parisa Rashidi', 'link': 'https://arxiv.org/abs/2412.17832', 'abstract': "Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to ensure timely and appropriate interventions. Advances in artificial intelligence (AI) technologies have significantly improved the accuracy of acuity predictions. However, prior studies using machine learning for acuity prediction have predominantly relied on electronic health records (EHR) data, often overlooking other critical aspects of ICU stay, such as patient mobility, environmental factors, and facial cues indicating pain or agitation. To address this gap, we present MANGO: the Multimodal Acuity traNsformer for intelliGent ICU Outcomes, designed to enhance the prediction of patient acuity states, transitions, and the need for life-sustaining therapy. We collected a multimodal dataset ICU-Multimodal, incorporating four key modalities, EHR data, wearable sensor data, video of patient's facial cues, and ambient sensor data, which we utilized to train MANGO. The MANGO model employs a multimodal feature fusion network powered by Transformer masked self-attention method, enabling it to capture and learn complex interactions across these diverse data modalities even when some modalities are absent. Our results demonstrated that integrating multiple modalities significantly improved the model's ability to predict acuity status, transitions, and the need for life-sustaining therapy. The best-performing models achieved an area under the receiver operating characteristic curve (AUROC) of 0.76 (95% CI: 0.72-0.79) for predicting transitions in acuity status and the need for life-sustaining therapy, while 0.82 (95% CI: 0.69-0.89) for acuity status prediction...", 'abstract_zh': '重症监护病房（ICU）患者病情严重程度的估计对于确保及时和适当的干预至关重要。人工智能（AI）技术的进步显著提高了病情严重程度预测的准确性。然而，先前使用机器学习进行病情严重程度预测的研究大多依赖电子健康记录（EHR）数据，常常忽略了ICU住院期间其他关键因素，如患者活动能力、环境因素以及面部表情所指示的疼痛或躁动。为解决这一问题，我们提出了MANGO：多模态重症监护智能结局转换器，旨在增强对患者病情严重程度状态、转换及其对生命维持治疗需求的预测。我们收集了一个多模态ICU-Multimodal数据集，该数据集包括四种关键模态：EHR数据、可穿戴传感器数据、患者面部表情视频以及环境传感器数据，并利用这些数据训练了MANGO模型。MANGO模型采用基于Transformer掩码自注意力方法的多模态特征融合网络，即使某些模态数据缺失时，也能捕捉和学习这些多样数据模态之间的复杂交互。结果显示，整合多种模态数据显著提高了模型预测病情严重程度状态、转换及其对生命维持治疗需求的能力。表现最佳的模型在预测病情严重程度状态转换和对生命维持治疗的需求方面分别获得了接收操作特征曲线下的面积（AUROC）0.76（95% CI：0.72-0.79）和0.82（95% CI：0.69-0.89），而在预测病情严重程度状态方面则达到了0.82（95% CI：0.69-0.89）...', 'title_zh': 'MANGO：多模态敏锐度变换器以实现智能重症监护病房 outcome预测'}
{'arxiv_id': 'arXiv:2412.17823', 'title': 'RUL forecasting for wind turbine predictive maintenance based on deep learning', 'authors': 'Syed Shazaib Shah, Tan Daoliang, Sah Chandan Kumar', 'link': 'https://arxiv.org/abs/2412.17823', 'abstract': "Predictive maintenance (PdM) is increasingly pursued to reduce wind farm operation and maintenance costs by accurately predicting the remaining useful life (RUL) and strategically scheduling maintenance. However, the remoteness of wind farms often renders current methodologies ineffective, as they fail to provide a sufficiently reliable advance time window for maintenance planning, limiting PdM's practicality. This study introduces a novel deep learning (DL) methodology for future RUL forecasting. By employing a multi-parametric attention-based DL approach that bypasses feature engineering, thereby minimizing the risk of human error, two models: ForeNet-2d and ForeNet-3d are proposed. These models successfully forecast the RUL for seven multifaceted wind turbine (WT) failures with a 2-week forecast window. The most precise forecast deviated by only 10 minutes from the actual RUL, while the least accurate prediction deviated by 1.8 days, with most predictions being off by only a few hours. This methodology offers a substantial time frame to access remote WTs and perform necessary maintenance, thereby enabling the practical implementation of PdM.", 'abstract_zh': '预测性维护（PdM）正逐渐成为减少风力发电场运行和维护成本的一种方法，通过准确预测剩余使用寿命（RUL）并战略性地安排维护。然而，风力发电场的偏远位置往往使得当前的方法效果不佳，因为它们无法为维护计划提供足够可靠的提前时间窗口，从而限制了PdM的实际应用。本文提出了一种新的深度学习（DL）方法，用于未来的RUL预测。采用多参数注意力机制的DL方法来绕过特征工程，从而降低人为错误的风险，提出了两种模型：ForeNet-2d和ForeNet-3d。这些模型成功地在2周的预测窗口内预测了七种复杂风力发电机（WT）故障的RUL。最精确的预测与实际RUL仅相差10分钟，而最不准确的预测则相差1.8天，大多数预测仅相差几小时。该方法提供了足够的提前时间来访问远程风力发电机并进行必要的维护，从而使得PdM的实际应用成为可能。', 'title_zh': '基于深度学习的风力发电机剩余使用寿命预测及其在预测性维护中的应用'}
{'arxiv_id': 'arXiv:2412.17821', 'title': 'The Rosetta Paradox: Domain-Specific Performance Inversions in Large Language Models', 'authors': 'Basab Jha, Ujjwal Puri', 'link': 'https://arxiv.org/abs/2412.17821', 'abstract': 'While large language models, such as GPT and BERT, have already demonstrated unprecedented skills in everything from natural language processing to domain-specific applications, there came an unexplored phenomenon we term the Rosetta Paradox. The Rosetta Paradox characterizes the counterintuitive performance inversions across domains of knowledge. This paradox captures how such LLMs can excel in highly specialized fields but do poorly on tasks which require general, everyday knowledge. This paper formalizes the definition of the Rosetta Paradox and introduces a panoramic analysis framework that includes both a Domain Specificity Index (DSI) and a Performance Inversion Metric (PIM) for consistent quantification of domain-specific behavior in LLMs.\nWe adopt this paradox and conduct a series of investigations through extensive experiments across diverse models and knowledge domains, ranging from rich technical areas to common-sense reasoning. Our findings indicate that the Rosetta Paradox is likely not a mere artifact of data distribution but an intrinsic architectural and emergent property of deep neural networks. We present comparative analyses across different model architectures, sizes, and training methodologies that shed light into the peculiar ways this paradox manifests itself and challenge the standard evaluation metrics.', 'abstract_zh': '尽管像GPT和BERT这样的大型语言模型已经在自然语言处理和特定领域应用中展现了前所未有的技能，但出现了一种未被探索的现象，我们称之为罗塞塔悖论。罗塞塔悖论描述了知识领域跨域下的反常识性能反转现象。这一悖论揭示了这些大语言模型在高度专业化领域中表现出色，但在需要广泛且日常生活知识的任务中却表现不佳。本文正式定义了罗塞塔悖论，并介绍了一个全景分析框架，其中包括领域特异性指数（Domain Specificity Index, DSI）和性能反转度量（Performance Inversion Metric, PIM），以一致地量化大语言模型在不同领域的表现行为。\n\n我们采用这一悖论，并通过广泛覆盖不同模型和知识领域的实验来开展一系列研究，范围从丰富的技术领域到常识推理。研究结果表明，罗塞塔悖论很可能并不是数据分布的产物，而是深度神经网络内在的结构和涌现性质的表现。我们对不同模型架构、规模和训练方法进行了比较分析，揭示了这一悖论特有的表现方式，并挑战了标准评估指标。', 'title_zh': '罗塞塔悖论：大规模语言模型在特定领域中的性能反转'}
{'arxiv_id': 'arXiv:2412.17819', 'title': 'Inductive Linguistic Reasoning with Large Language Models', 'authors': 'Raghav Ramji, Keshav Ramji', 'link': 'https://arxiv.org/abs/2412.17819', 'abstract': "Evaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1% and Llama-3.1-405B-Instruct by 5.9% over chain-of-thought approaches. These gains are attributable to the analogical demonstrations, both when self-generated as well as when produced by weaker multilingual models. Furthermore, we demonstrate that our method generalizes to other tasks present in Linguistics Olympiad competitions, achieving sizable improvements across all problem types and difficulty levels included in the LINGOLY dataset with GPT-4o. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods.", 'abstract_zh': '评估大型语言模型（LLM）在语言推理能力方面的表现是理解其技能在大规模应用中可能出现差距的重要任务。在这项工作中，我们通过语言谜题的视角，研究这些模型在极端低资源语言下进行抽象多语言推理的能力。由于这些翻译任务涉及从参考实例中归纳和演绎推理，我们探讨了是否可以通过类比提示从种子示例自动生成多样化的辅助示例。我们采用两阶段的方法，首先使用语言模型生成类比示例，然后将它们与提供的目标语言示例一起应用于上下文中。通过对modeLing数据集的实验结果表明，类比提示有效地激发了模型对语言语法相似性的知识，使得GPT-4o的表现提升了8.1%，Llama-3.1-405B-Instruct的表现提升了5.9%，相较于基于推理链的方法。这些提升归因于自动生成的类比示例以及由较弱的多语言模型生成的类比示例。此外，我们展示了我们的方法可以泛化到 Linguistics Olympiad 竞赛中的其他任务，在LINGOLY数据集中，GPT-4o在所有问题类型和难度级别上都取得了显著的改进。我们还报告了几点有趣的发现，这些发现揭示了影响语言推理性能的现象，表明这些谜题是评估新推理方法的有效基准。', 'title_zh': '大规模语言模型中的归纳语言推理'}
{'arxiv_id': 'arXiv:2409.05344', 'title': 'GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep Reinforcement Learning', 'authors': 'Heng Xiong, Changrong Guo, Jian Peng, Kai Ding, Wenjie Chen, Xuchong Qiu, Long Bai, Jianfeng Xu', 'link': 'https://arxiv.org/abs/2409.05344', 'abstract': "Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at this https URL.", 'abstract_zh': '机器人对象包装在物流和自动化行业中具有广泛的实际应用前景，研究人员通常将其形式化为在线三维集装箱装载问题（3D-BPP）。然而，现有的基于深度强化学习（DRL）的方法主要集中在提升在有限包装环境中的性能，而忽视了在不同尺寸集装箱的多种环境下进行泛化的能效。针对这一问题，我们提出了一种名为GOPT的方法，该方法基于变压器的深度强化学习来实现泛化在线三维集装箱装载。首先，我们设计了一个放置生成器模块，以生成有限的放置候选空间，并表示集装箱的状态。其次，我们提出了一种包装变压器，该变压器将物品和集装箱的特征结合起来，以识别待包装物品与集装箱中可用空间之间的空间相关性。将这两个组件相结合，使GOPT具备在不同尺寸的集装箱上进行推理的能力。我们进行了广泛的实验，并展示了GOPT不仅在基线方法上取得了卓越的性能，而且具有出色的泛化能力。此外，机器人部署表明了我们方法在实际世界中的实用性。源代码将在以下网址公开：此 https URL。', 'title_zh': 'GOPT：基于变压器的深度强化学习的通用在线三维箱子打包算法'}
{'arxiv_id': 'arXiv:2301.00314', 'title': 'Causal Deep Learning', 'authors': 'M. Alex O. Vasilescu', 'link': 'https://arxiv.org/abs/2301.00314', 'abstract': 'We derive a set of causal deep neural networks whose architectures are a consequence of tensor (multilinear) factor analysis, a framework that facilitates forward and inverse causal inference. Forward causal questions are addressed with a neural architecture composed of causal capsules and a tensor transformer. Causal capsules compute a set of invariant causal factor representations, whose interactions are governed by a tensor transformation. Inverse causal questions are addressed with a neural network that implements the multilinear projection algorithm. The architecture reverses the order of the operations of a forward neural network and estimates the causes of effects. As an alternative to aggressive bottleneck dimension reduction or regularized regression that may camouflage an inherently underdetermined inverse problem, we prescribe modeling different aspects of the mechanism of data formation with piecewise tensor models whose multilinear projections produce multiple candidate solutions. Our forward and inverse questions may be addressed with shallow architectures, but for computationally scalable solutions, we derive a set of deep neural networks by taking advantage of block algebra. An interleaved kernel hierarchy results in a doubly non-linear tensor factor models. The causal neural networks that are a consequence of tensor factor analysis are data agnostic, but are illustrated with facial images. Sequential, parallel and asynchronous parallel computation strategies are described.', 'abstract_zh': '我们推导出了一组因果深度神经网络，其架构源于张量（多线性）因子分析框架，该框架有助于正向和反向因果推断。正向因果问题通过由因果胶囊和张量变换组成的神经架构来解决。因果胶囊计算一组不变的因果因子表示，这些表示的交互由张量变换来控制。反向因果问题则通过实现多线性投影算法的神经网络来解决。该架构逆向操作顺序，估计导致结果的原因。作为一种替代激进的瓶颈维度缩减或正则化回归的方法，后者可能会掩盖固有的欠定反问题，我们建议使用分段张量模型来建模数据形成机制的不同方面，其中的多线性投影产生多个候选解决方案。我们的正向和反向问题可以通过浅层架构解决，但为了实现计算上的可扩展性，我们通过利用块代数推导出一组深层神经网络。交错的核层次结构导致了双重非线性张量因子模型。由张量因子分析推导出的因果神经网络在数据方面是无偏的，但以面部图像作为示例进行说明。还描述了序贯、并行和异步并行计算策略。', 'title_zh': '因果深度学习'}
