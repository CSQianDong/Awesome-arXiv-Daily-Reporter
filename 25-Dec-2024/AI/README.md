# MotifGPL: Motif-Enhanced Graph Prototype Learning for Deciphering Urban Social Segregation 

**Title (ZH)**: MotifGPL： motif增强的图原型学习方法破解城市社会隔离现象 

**Authors**: Tengfei He, Xiao Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.18464)  

**Abstract**: Social segregation in cities, spanning racial, residential, and income dimensions, is becoming more diverse and severe. As urban spaces and social relations grow more complex, residents in metropolitan areas experience varying levels of social segregation. If left unaddressed, this could lead to increased crime rates, heightened social tensions, and other serious issues. Effectively quantifying and analyzing the structures within urban spaces and resident interactions is crucial for addressing segregation. Previous studies have mainly focused on surface-level indicators of urban segregation, lacking comprehensive analyses of urban structure and mobility. This limitation fails to capture the full complexity of segregation. To address this gap, we propose a framework named Motif-Enhanced Graph Prototype Learning (MotifGPL),which consists of three key modules: prototype-based graph structure extraction, motif distribution discovery, and urban graph structure reconstruction. Specifically, we use graph structure prototype learning to extract key prototypes from both the urban spatial graph and the origin-destination graph, incorporating key urban attributes such as points of interest, street view images, and flow indices. To enhance interpretability, the motif distribution discovery module matches each prototype with similar motifs, representing simpler graph structures reflecting local patterns. Finally, we use the motif distribution results to guide the reconstruction of the two graphs. This model enables a detailed exploration of urban spatial structures and resident mobility patterns, helping identify and analyze motif patterns that influence urban segregation, guiding the reconstruction of urban graph structures. Experimental results demonstrate that MotifGPL effectively reveals the key motifs affecting urban social segregation and offer robust guidance for mitigating this issue. 

**Abstract (ZH)**: 城市中跨越种族、居住和社会经济层次的社交隔离现象正在变得更加多样化和严重。随着城市空间和社会关系的日益复杂化，大都市区居民所经历的社会隔离程度也参差不齐。如果不加以解决，这可能会导致犯罪率上升、社会紧张局势加剧以及其他一系列严重问题。准确量化和分析城市空间结构以及居民互动对于解决隔离问题至关重要。以往的研究主要集中在城市隔离的表面指标上，缺乏对城市结构和流动性的全面分析，这无法捕捉到隔离的全部复杂性。为解决这一问题，我们提出了一种名为动机增强图原型学习（MotifGPL）的框架，该框架包括三个主要模块：基于原型的图结构提取、动机分布发现以及城市图结构重建。具体而言，我们使用图结构原型学习从城市的空间图和起终点图中提取关键原型，并融合兴趣点、街景图像和流指数等关键城市属性。为了增强可解释性，动机分布发现模块将每个原型与相似的动机相匹配，这些动机代表了局部模式的简单图结构。最后，我们利用动机分布结果指导两个图的重构。该模型能够对城市的空间结构和居民流动模式进行详细探索，帮助识别并分析影响城市隔离的动机模式，并指导城市图结构的重建。实验结果表明，MotifGPL能有效揭示影响城市社会隔离的关键动机，并为缓解这一问题提供强大的指导。 

---
# Multi-Agent Norm Perception and Induction in Distributed Healthcare 

**Title (ZH)**: 多智能体规范感知与诱导在分布式医疗中的应用 

**Authors**: Chao Li, Olga Petruchik, Elizaveta Grishanina, Sergey Kovalchuk  

**Link**: [PDF](https://arxiv.org/pdf/2412.18454)  

**Abstract**: This paper presents a Multi-Agent Norm Perception and Induction Learning Model aimed at facilitating the integration of autonomous agent systems into distributed healthcare environments through dynamic interaction processes. The nature of the medical norm system and its sharing channels necessitates distinct approaches for Multi-Agent Systems to learn two types of norms. Building on this foundation, the model enables agents to simultaneously learn descriptive norms, which capture collective tendencies, and prescriptive norms, which dictate ideal behaviors. Through parameterized mixed probability density models and practice-enhanced Markov games, the multi-agent system perceives descriptive norms in dynamic interactions and captures emergent prescriptive norms. We conducted experiments using a dataset from a neurological medical center spanning from 2016 to 2020. 

**Abstract (ZH)**: 本文提出了一种多agent规范感知与归纳学习模型，旨在通过动态交互过程促进自主agent系统在分布式医疗环境中的集成。医疗规范系统的性质及其共享渠道要求多agent系统采用不同的方法来学习两种类型的规范。在此基础上，该模型使agents能够同时学习描述性规范，这些规范捕捉了群体倾向，以及规范性规范，这些规范规定了理想行为。通过参数化混合概率密度模型和实践增强的马尔可夫游戏，多agent系统在动态交互中感知描述性规范，并捕捉到新兴的规范性规范。我们使用2016年至2020年间来自一家神经医学中心的数据集进行了实验。 

---
# Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent 

**Title (ZH)**: 通过大语言模型代理进行可解释的多模态数据自然语言探索 

**Authors**: Farhad Nooralahzadeh, Yi Zhang, Jonathan Furst, Kurt Stockinger  

**Link**: [PDF](https://arxiv.org/pdf/2412.18428)  

**Abstract**: International enterprises, organizations, or hospitals collect large amounts of multi-modal data stored in databases, text documents, images, and videos. While there has been recent progress in the separate fields of multi-modal data exploration as well as in database systems that automatically translate natural language questions to database query languages, the research challenge of querying database systems combined with other unstructured modalities such as images in natural language is widely unexplored.
In this paper, we propose XMODE - a system that enables explainable, multi-modal data exploration in natural language. Our approach is based on the following research contributions: (1) Our system is inspired by a real-world use case that enables users to explore multi-modal information systems. (2) XMODE leverages a LLM-based agentic AI framework to decompose a natural language question into subtasks such as text-to-SQL generation and image analysis. (3) Experimental results on multi-modal datasets over relational data and images demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling not only in accuracy but also in various performance metrics such as query latency, API costs, planning efficiency, and explanation quality, thanks to the more effective utilization of the reasoning capabilities of LLMs. 

**Abstract (ZH)**: 国际企业、组织或医院收集了大量的多模态数据，这些数据存储在数据库、文本文档、图像和视频中。虽然在多模态数据探索和自然语言到数据库查询语言自动生成方面各自取得了一定进展，但将数据库系统与其他未结构化的模态如图像结合查询的研究挑战仍然没有得到广泛探索。

在本文中，我们提出了一种名为XMODE的系统，旨在通过自然语言实现可解释的多模态数据探索。我们的方法主要基于以下研究贡献：（1）我们的系统借鉴了一个实际应用案例，使用户能够探索多模态信息系统。（2）XMODE利用一个基于大语言模型（LLM）的代理AI框架，将自然语言问题分解为子任务，例如文本到SQL生成和图像分析。（3）通过针对关系数据和图像的多模态数据集进行实验，结果显示我们的系统在准确性和多种性能指标（如查询延迟、API成本、计划效率和解释质量）方面均优于现有最先进的多模态探索系统，得益于更有效利用了LLMs的推理能力。 

---
# GUI Testing Arena: A Unified Benchmark for Advancing Autonomous GUI Testing Agent 

**Title (ZH)**: GUI测试竞技场：促进自主GUI测试代理发展的统一基准 

**Authors**: Kangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen, Zhi Chen, Tiancheng Zhao, Xiubo Liang, Jianwei Yin  

**Link**: [PDF](https://arxiv.org/pdf/2412.18426)  

**Abstract**: Nowadays, research on GUI agents is a hot topic in the AI community. However, current research focuses on GUI task automation, limiting the scope of applications in various GUI scenarios. In this paper, we propose a formalized and comprehensive environment to evaluate the entire process of automated GUI Testing (GTArena), offering a fair, standardized environment for consistent operation of diverse multimodal large language models. We divide the testing process into three key subtasks: test intention generation, test task execution, and GUI defect detection, and construct a benchmark dataset based on these to conduct a comprehensive evaluation. It evaluates the performance of different models using three data types: real mobile applications, mobile applications with artificially injected defects, and synthetic data, thoroughly assessing their capabilities in this relevant task. Additionally, we propose a method that helps researchers explore the correlation between the performance of multimodal language large models in specific scenarios and their general capabilities in standard benchmark tests. Experimental results indicate that even the most advanced models struggle to perform well across all sub-tasks of automated GUI Testing, highlighting a significant gap between the current capabilities of Autonomous GUI Testing and its practical, real-world applicability. This gap provides guidance for the future direction of GUI Agent development. Our code is available at this https URL. 

**Abstract (ZH)**: 现今，GUI代理的研究是人工智能领域的一个热点话题。然而，当前的研究主要集中于GUI任务自动化，这限制了其在各种GUI场景中的应用范围。本文提出了一种标准化和全面的环境，用于评估整个自动化GUI测试（GTArena）过程，提供了一个公平且标准化的环境，以便一致地运行不同的多模态大型语言模型。我们将测试过程分为三个关键子任务：测试意图生成、测试任务执行和GUI缺陷检测，并基于这些子任务构建基准数据集，进行全面评估。该基准数据集使用三种不同类型的数据——实际移动应用程序、具有人为注入缺陷的移动应用程序和合成数据，全面评估模型在这项任务中性能的高低。此外，我们提出了一种方法，帮助研究人员探索特定场景下多模态语言大型模型性能与其在标准基准测试中的普遍能力之间的关联。实验结果表明，即使是最先进的模型也无法在自动GUI测试的所有子任务中表现出色，这突显了当前自动GUI测试能力和其实际应用之间的巨大差距。这一差距为GUI代理的未来发展方向提供了指导。我们的代码可以在以下地址获取：[此处提供URL链接]。 

---
# LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating 

**Title (ZH)**: 长文档URL：一个综合性的多模态长文档基准，融合了理解、推理和定位能力 

**Authors**: Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, Cheng-Lin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18424)  

**Abstract**: Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark, LongDocURL, integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field. 

**Abstract (ZH)**: 大型视觉语言模型（LVLMs）显著提高了文档理解能力，使处理复杂文档元素、更长的上下文以及更广泛的任务成为可能。然而，现有的文档理解基准只能处理少量页面，并且未能提供对布局元素定位的全面分析。本文首先定义了三个主要任务类别：长文档理解、数值推理和跨元素定位，然后提出一个综合基准——LongDocURL，该基准综合以上三个主要任务，涵盖20个基于不同主要任务和答案证据分类的子任务。此外，我们开发了一种半自动构建管道并收集了2,325对高质量的问题-答案对，涵盖了超过33,000页的文档，显著优于现有基准。随后，我们在26种不同的配置下对开源和封闭源模型进行了全面的评估实验，揭示了该领域中的关键性能差距。 

---
# Research on the Proximity Relationships of Psychosomatic Disease Knowledge Graph Modules Extracted by Large Language Models 

**Title (ZH)**: 基于大型语言模型提取的心理生理疾病知识图谱模块的临近关系研究 

**Authors**: Zihan Zhou, Ziyi Zeng, Wenhao Jiang, Yihui Zhu, Jiaxin Mao, Yonggui Yuan, Min Xia, Shubin Zhao, Mengyu Yao, Yunqian Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.18419)  

**Abstract**: As social changes accelerate, the incidence of psychosomatic disorders has significantly increased, becoming a major challenge in global health issues. This necessitates an innovative knowledge system and analytical methods to aid in diagnosis and treatment. Here, we establish the ontology model and entity types, using the BERT model and LoRA-tuned LLM for named entity recognition, constructing the knowledge graph with 9668 triples. Next, by analyzing the network distances between disease, symptom, and drug modules, it was found that closer network distances among diseases can predict greater similarities in their clinical manifestations, treatment approaches, and psychological mechanisms, and closer distances between symptoms indicate that they are more likely to co-occur. Lastly, by comparing the proximity d and proximity z score, it was shown that symptom-disease pairs in primary diagnostic relationships have a stronger association and are of higher referential value than those in diagnostic relationships. The research results revealed the potential connections between diseases, co-occurring symptoms, and similarities in treatment strategies, providing new perspectives for the diagnosis and treatment of psychosomatic disorders and valuable information for future mental health research and practice. 

**Abstract (ZH)**: 随着社会的变化加速，心身疾病的发病率显著增加，成为全球健康问题中的重大挑战。这要求建立一种创新的知识体系和分析方法以辅助诊断和治疗。为此，我们构建了本体模型和实体类型，使用BERT模型和LoRA调优的大语言模型进行命名实体识别，并构建了一个包含9668条三元组的知识图谱。接着，通过分析疾病、症状和药物模块之间的网络距离，研究发现疾病之间的更近距离可以预测它们在临床表现、治疗方式和心理机制上的更大相似性；症状之间的更近距离表明它们更有可能共存。最后，通过比较接近度d和接近度z分值，研究展示了主要诊断关系中的症状-疾病配对比诊断关系中的配对具有更强的关联性和更高的参考价值。研究结果揭示了疾病、共发症状以及治疗策略相似性之间的潜在联系，为心身疾病的诊断和治疗提供了新的视角，并为未来的心理健康研究和实践提供了有价值的信息。 

---
# Exploring Flexible Scenario Generation in Godot Simulator 

**Title (ZH)**: 探究 Godot 模拟器中的灵活场景生成技术 

**Authors**: Daniel Peraltai, Xin Qin  

**Link**: [PDF](https://arxiv.org/pdf/2412.18408)  

**Abstract**: Cyber-physical systems (CPS) combine cyber and physical components engineered to make decisions and interact within dynamic environments. Ensuring the safety of CPS is of great importance, requiring extensive testing across diverse and complex scenarios. To generate as many testing scenarios as possible, previous efforts have focused on describing scenarios using formal languages to generate scenes. In this paper, we introduce an alternative approach: reconstructing scenes inside the open-source game engine, Godot. We have developed a pipeline that enables the reconstruction of testing scenes directly from provided images of scenarios. These reconstructed scenes can then be deployed within simulated environments to assess a CPS. This approach offers a scalable and flexible solution for testing CPS in realistic environments. 

**Abstract (ZH)**: cyber-物理系统（CPS）结合了用于在动态环境中做出决策并交互的工程化网络和物理组件。确保CPS的安全性非常重要，需要在各种复杂场景中进行广泛的测试。为了生成尽可能多的测试场景，以往的努力集中在使用形式化语言描述场景以生成场景。在本文中，我们提出了一种替代方法：在开源游戏引擎Godot内部重建场景。我们开发了一条管道，能够直接从提供的场景图像中重建测试场景。这些重建的场景可以在模拟环境中部署，以评估CPS。这种方法为在现实环境中测试CPS提供了一种可扩展且灵活的解决方案。 

---
# Weak Scaling Capability in Token Space: An Observation from Large Vision Language Model 

**Title (ZH)**: 在令牌空间中弱标度能力的观察：来自大型视觉语言模型的研究 

**Authors**: Tenghui Li, Guoxu Zhou, Xuyang Zhao, Qibin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.18387)  

**Abstract**: The scaling capability has been widely validated with respect to the number of parameters and the size of training data. One important question that is unexplored is that does scaling capability also exists similarly with respect to the number of vision tokens? This study fills the gap by investigating the relationship between the number of vision tokens and the performance of vision-language models. Our theoretical analysis and empirical evaluations reveal that the model exhibits weak scaling capabilities on the length \(N_l\), with performance approximately \(S(N_l) \approx (c/N_l)^{\alpha}\), where \(c, \alpha\) are hyperparameters. Interestingly, this scaling behavior remains largely unaffected by the inclusion or exclusion of the user's question in the input. Furthermore, fusing the user's question with the vision token can enhance model performance when the question is relevant to the task. To address the computational challenges associated with large-scale vision tokens, we propose a novel architecture that efficiently reduces the token count while integrating user question tokens into the representation. Our findings may offer insights for developing more efficient and effective vision-language models under specific task constraints. 

**Abstract (ZH)**: 关于参数数量和训练数据规模，放大能力已经得到了广泛应用的验证。一个尚未充分探索的重要问题是，放大能力是否也适用于视觉 tokens 的数量？本研究通过调查视觉 tokens 的数量与视觉-语言模型性能之间的关系，填补了这一空白。我们的理论分析和实证评估揭示出，该模型在长度 \(N_l\) 上表现出较弱的放大能力，性能大约为 \(S(N_l) \approx (c/N_l)^{\alpha}\)，其中 \(c, \alpha\) 是超参数。有趣的是，用户问题的包括与否对这种放大行为的影响相对较小。此外，当用户问题与任务相关时，将用户问题与视觉 tokens 相结合可以提高模型性能。为应对大规模视觉 tokens 引起的计算挑战，我们提出了一种新颖的架构，该架构能有效减少 token 数量，并将用户问题 tokens 集成进表示中。我们的发现可能为在特定任务约束条件下开发更高效和有效的视觉-语言模型提供见解。 

---
# The Thousand Brains Project: A New Paradigm for Sensorimotor Intelligence 

**Title (ZH)**: 千脑计划：一种新的传感器imotor智能范式 

**Authors**: Viviane Clay, Niels Leadholm, Jeff Hawkins  

**Link**: [PDF](https://arxiv.org/pdf/2412.18354)  

**Abstract**: Artificial intelligence has advanced rapidly in the last decade, driven primarily by progress in the scale of deep-learning systems. Despite these advances, the creation of intelligent systems that can operate effectively in diverse, real-world environments remains a significant challenge. In this white paper, we outline the Thousand Brains Project, an ongoing research effort to develop an alternative, complementary form of AI, derived from the operating principles of the neocortex. We present an early version of a thousand-brains system, a sensorimotor agent that is uniquely suited to quickly learn a wide range of tasks and eventually implement any capabilities the human neocortex has. Core to its design is the use of a repeating computational unit, the learning module, modeled on the cortical columns found in mammalian brains. Each learning module operates as a semi-independent unit that can model entire objects, represents information through spatially structured reference frames, and both estimates and is able to effect movement in the world. Learning is a quick, associative process, similar to Hebbian learning in the brain, and leverages inductive biases around the spatial structure of the world to enable rapid and continual learning. Multiple learning modules can interact with one another both hierarchically and non-hierarchically via a "cortical messaging protocol" (CMP), creating more abstract representations and supporting multimodal integration. We outline the key principles motivating the design of thousand-brains systems and provide details about the implementation of Monty, our first instantiation of such a system. Code can be found at this https URL, along with more detailed documentation at this https URL. 

**Abstract (ZH)**: 在过去十年中，人工智能技术取得了快速进步，主要是由于深度学习系统的规模扩大所推动。尽管取得了这些进展，但在各种实际环境中的智能系统如何有效运行仍然是一个重大挑战。在这份白皮书中，我们概述了千脑计划，这是一个正在进行的研究项目，旨在开发一种替代且互补的形式的AI，该AI源于新皮层的运作原则。我们介绍了千脑系统的早期版本，这是一种传感器运动代理，特别适合快速学习广泛的任务，并最终实现人类新皮层所具备的所有能力。其核心设计原则是使用重复的计算单元，即学习模块，这一模块模仿哺乳动物大脑中的皮层柱。每个学习模块作为一个半独立的单元运作，能够模拟完整的物体，通过空间结构的参考框架来表示信息，并且能够估计和影响世界中的运动。学习过程是一个快速的关联过程，类似于大脑中的Hebbian学习，并利用世界空间结构的归纳偏见来实现快速且持续的学习。多个学习模块可以通过“皮层消息协议”（CMP）进行逐级和非逐级的交互，从而创建更抽象的表示并支持多模态整合。我们概述了驱动千脑系统设计的关键原则，并提供了关于Monty的首次实现的详细实施细节。代码可以在以下网址找到：这个网址，以及更详细的文档可以在以下网址找到：这个网址。 

---
# Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases 

**Title (ZH)**: 基于检索增强的海盗：适应性攻击LLMs以泄露知识库 

**Authors**: Christian Di Maio, Cristian Cosci, Marco Maggini, Valentina Poggioni, Stefano Melacci  

**Link**: [PDF](https://arxiv.org/pdf/2412.18295)  

**Abstract**: The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in several real-world services triggers severe concerns about their security. A RAG system improves the generative capabilities of a Large Language Models (LLM) by a retrieval mechanism which operates on a private knowledge base, whose unintended exposure could lead to severe consequences, including breaches of private and sensitive information. This paper presents a black-box attack to force a RAG system to leak its private knowledge base which, differently from existing approaches, is adaptive and automatic. A relevance-based mechanism and an attacker-side open-source LLM favor the generation of effective queries to leak most of the (hidden) knowledge base. Extensive experimentation proves the quality of the proposed algorithm in different RAG pipelines and domains, comparing to very recent related approaches, which turn out to be either not fully black-box, not adaptive, or not based on open-source models. The findings from our study remark the urgent need for more robust privacy safeguards in the design and deployment of RAG systems. 

**Abstract (ZH)**: 随着检索增强生成（RAG）系统在多个实际服务中的广泛应用，其安全性问题引起了严重关切。RAG系统通过基于私有知识库的检索机制提高了大型语言模型（LLM）的生成能力，而该知识库的意外暴露可能会导致严重的后果，包括隐私和敏感信息的泄露。本文提出了一种黑盒攻击方法，以迫使RAG系统泄露其私有知识库。与其他现有方法相比，该方法具有自适应性和自动化特点。基于相关性的机制和攻击方的开源大型语言模型有助于生成有效的查询，从而泄露大部分（隐藏的）知识库。广泛的实验结果证明，所提出的算法在不同RAG管道和领域中具有较高的质量，并优于非常最近的相关方法，这些方法要么不完全黑盒，要么不可适应，要么不基于开源模型。我们的研究发现强调了在设计和部署RAG系统时迫切需要更 robust 的隐私保护措施。 

---
# MinsStudio: A Streamlined Package for Minecraft AI Agent Development 

**Title (ZH)**: MinsStudio：一种简化版的Minecraft AI代理开发包 

**Authors**: Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, Yitao Liang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18293)  

**Abstract**: Minecraft has emerged as a valuable testbed for embodied intelligence and sequential decision-making research, yet the development and validation of novel agents remains hindered by significant engineering challenges. This paper presents MineStudio, an open-source software package designed to streamline embodied policy development in Minecraft. MineStudio represents the first comprehensive integration of seven critical engineering components: simulator, data, model, offline pretraining, online finetuning, inference, and benchmark, thereby allowing users to concentrate their efforts on algorithm innovation. We provide a user-friendly API design accompanied by comprehensive documentation and tutorials. The complete codebase is publicly available at this https URL. 

**Abstract (ZH)**: 《我的世界》（Minecraft）已成为开展具身智能和序列决策研究的宝贵实验平台，然而，新型智能体的开发与验证仍受制于重大的工程挑战。本文介绍了MineStudio，一个开源软件包，旨在简化《我的世界》中具身策略的开发过程。MineStudio 是第一个全面整合七个关键工程组件的平台：仿真器、数据、模型、离线预训练、在线微调、推理和基准测试，从而让用户能够将精力集中在算法创新上。我们提供了一个用户友好的 API 设计，并附有全面的文档和教程。完整的代码库可在以下网址公开访问：this https URL。 

---
# Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization 

**Title (ZH)**: 通过直接优势策略优化提升大型语言模型的多步推理能力 

**Authors**: Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, Yahui Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.18279)  

**Abstract**: The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One challenge is the sparse reward, which makes optimization difficult for RL and necessitates a large amount of data samples. Another challenge stems from the inherent instability of RL, particularly when using Actor-Critic (AC) methods to derive optimal policies, which often leads to unstable training processes. To address these issues, we introduce Direct Advantage Policy Optimization (DAPO), an novel step-level offline RL algorithm. Unlike standard alignment that rely solely outcome rewards to optimize policies (such as DPO), DAPO employs a critic function to predict the reasoning accuracy at each step, thereby generating dense signals to refine the generation strategy. Additionally, the Actor and Critic components in DAPO are trained independently, avoiding the co-training instability observed in standard AC algorithms like PPO. We train DAPO on mathematical and code query datasets and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO. 

**Abstract (ZH)**: reinforcement 学习（RL）在增强大语言模型（LLMs）推理能力方面的作用变得越来越重要。尽管在许多场景中RL取得了成功，但在提高LLMs的推理能力方面仍然存在许多挑战。其中一个挑战是稀疏奖励，这使得RL的优化变得困难，并需要大量的数据样本。另一个挑战源自于RL内在的不稳定性，特别是在使用Actor-Critic（AC）方法推导最优策略时，通常会导致训练过程的不稳定。为解决这些问题，我们引入了一种新颖的离线RL算法——直接优势策略优化（DAPO）。与依赖单一结果奖励进行策略优化的标准对齐方法（如DPO）不同，DAPO 使用一个评论器函数来预测每一步的推理准确性，从而生成密集信号以改进生成策略。此外，DAPO 中的Actor和Critic组件是独立训练的，避免了标准AC算法（如PPO）中观察到的协同训练不稳定现象。我们使用数学和代码查询数据集训练DAPO，并在多个基准上评估其性能。结果表明，DAPO 能够有效地增强SFT模型和RL模型的数学和代码能力，证明了DAPO的有效性。 

---
# Annotating References to Mythological Entities in French Literature 

**Title (ZH)**: 法国文学中神话实体的标注研究 

**Authors**: Thierry Poibeau  

**Link**: [PDF](https://arxiv.org/pdf/2412.18270)  

**Abstract**: In this paper, we explore the relevance of large language models (LLMs) for annotating references to Roman and Greek mythological entities in modern and contemporary French literature. We present an annotation scheme and demonstrate that recent LLMs can be directly applied to follow this scheme effectively, although not without occasionally making significant analytical errors. Additionally, we show that LLMs (and, more specifically, ChatGPT) are capable of offering interpretative insights into the use of mythological references by literary authors. However, we also find that LLMs struggle to accurately identify relevant passages in novels (when used as an information retrieval engine), often hallucinating and generating fabricated examples-an issue that raises significant ethical concerns. Nonetheless, when used carefully, LLMs remain valuable tools for performing annotations with high accuracy, especially for tasks that would be difficult to annotate comprehensively on a large scale through manual methods alone. 

**Abstract (ZH)**: 在本文中，我们探讨了大规模语言模型（LLMs）在标注现代及当代法语文坛中提到的罗马和希腊神话实体方面的相关性。我们提出了一种标注方案，并展示了近期的LLMs可以直接应用于遵循该方案，尽管在分析过程中偶尔会出现严重的分析错误。此外，我们还证明了LLMs（更具体地说，是ChatGPT）能够为文学作者在使用神话引用方面的解读提供有价值的见解。然而，我们也发现，当LLMs作为信息检索工具使用时，它们在准确识别小说中相关段落时遇到很大的困难，常常会产生幻觉并生成虚假例子——这一问题引起了重大的伦理关注。尽管如此，当谨慎使用时，LLMs仍然是进行高精度标注的重要工具，尤其是对于那些通过手工方法难以全面标注的任务。 

---
# VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities 

**Title (ZH)**: VISION：一种适用于科学用户设施的模块化人工智能助手，实现自然的人机乐器交互 

**Authors**: Shray Mathur, Noah van der Vleuten, Kevin Yager, Esther Tsai  

**Link**: [PDF](https://arxiv.org/pdf/2412.18161)  

**Abstract**: Scientific user facilities, such as synchrotron beamlines, are equipped with a wide array of hardware and software tools that require a codebase for human-computer-interaction. This often necessitates developers to be involved to establish connection between users/researchers and the complex instrumentation. The advent of generative AI presents an opportunity to bridge this knowledge gap, enabling seamless communication and efficient experimental workflows. Here we present a modular architecture for the Virtual Scientific Companion (VISION) by assembling multiple AI-enabled cognitive blocks that each scaffolds large language models (LLMs) for a specialized task. With VISION, we performed LLM-based operation on the beamline workstation with low latency and demonstrated the first voice-controlled experiment at an X-ray scattering beamline. The modular and scalable architecture allows for easy adaptation to new instrument and capabilities. Development on natural language-based scientific experimentation is a building block for an impending future where a science exocortex -- a synthetic extension to the cognition of scientists -- may radically transform scientific practice and discovery. 

**Abstract (ZH)**: 科学用户设施，如同步加速器光束线，配备了广泛的硬件和软件工具，需要一套用于人机交互的代码库。这通常需要开发人员参与，以建立用户/研究人员与复杂仪器之间的连接。生成式人工智能的出现为弥合这一知识差距提供了机会，使沟通更加顺畅并提高实验流程的效率。在此，我们通过组装多个AI赋能的认知模块来构建多模块架构的虚拟科学伴侣（VISION），这些模块分别专门配置大型语言模型（LLMs）以执行特定任务。借助VISION，我们在光束线工作站上实现了基于LLM的操作，并首次在X射线散射光束线上展示了语音控制的实验。这种模块化和可扩展的架构允许轻松适应新的仪器和功能。基于自然语言的科学实验开发是构建未来科学外皮（科学认知的合成扩展）的关键组成部分，它可能彻底改变科学研究和发现的方式。 

---
# Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media 

**Title (ZH)**: 《我们已经进入由AI生成文本的世界了吗？量化与监控社交媒体上的AI生成文本》 

**Authors**: Zhen Sun, Zongmin Zhang, Xinyue Shen, Ziyi Zhang, Yule Liu, Michael Backes, Yang Zhang, Xinlei He  

**Link**: [PDF](https://arxiv.org/pdf/2412.18148)  

**Abstract**: Social media platforms are experiencing a growing presence of AI-Generated Texts (AIGTs). However, the misuse of AIGTs could have profound implications for public opinion, such as spreading misinformation and manipulating narratives. Despite its importance, a systematic study to assess the prevalence of AIGTs on social media is still lacking. To address this gap, this paper aims to quantify, monitor, and analyze the AIGTs on online social media platforms. We first collect a dataset (SM-D) with around 2.4M posts from 3 major social media platforms: Medium, Quora, and Reddit. Then, we construct a diverse dataset (AIGTBench) to train and evaluate AIGT detectors. AIGTBench combines popular open-source datasets and our AIGT datasets generated from social media texts by 12 LLMs, serving as a benchmark for evaluating mainstream detectors. With this setup, we identify the best-performing detector (OSM-Det). We then apply OSM-Det to SM-D to track AIGTs over time and observe different trends of AI Attribution Rate (AAR) across social media platforms from January 2022 to October 2024. Specifically, Medium and Quora exhibit marked increases in AAR, rising from 1.77% to 37.03% and 2.06% to 38.95%, respectively. In contrast, Reddit shows slower growth, with AAR increasing from 1.31% to 2.45% over the same period. Our further analysis indicates that AIGTs differ from human-written texts across several dimensions, including linguistic patterns, topic distributions, engagement levels, and the follower distribution of authors. We envision our analysis and findings on AIGTs in social media can shed light on future research in this domain. 

**Abstract (ZH)**: 社交媒体平台上正呈现出越来越多的人工智能生成文本（AIGTs）的存在。然而，AIGTs 的滥用可能对公众意见产生深远的影响，如传播虚假信息和操纵叙事等。尽管这非常重要，但对社交媒体上AIGTs 普遍程度的系统性研究仍然缺乏。为弥补这一空白，本文旨在定量分析、监控和研究社交媒体上的AIGTs。我们首先从三大社交媒体平台（Medium、Quora和Reddit）收集了一个包含约240万条帖子的数据集（SM-D）。然后，我们构建了一个多样化的数据集（AIGTBench）来训练和评估AIGT检测器。AIGTBench结合了广泛使用的开源数据集和从社交媒体文本生成的12个大型语言模型（LLMs）的AIGT数据集，作为评估主流检测器的基准。通过这一设置，我们确定了性能最佳的检测器（OSM-Det）。随后，我们将OSM-Det应用于SM-D以跟踪AIGTs随时间的变化，并观察从2022年1月到2024年10月期间不同社交媒体平台上的AI归因率（AAR）的不同趋势。具体来说，Medium和Quora的AAR显著增加，分别从1.77%上升到37.03%和2.06%上升到38.95%。相比之下，Reddit的增长速度较慢，AAR在同一时间段内从1.31%增加到2.45%。进一步的分析表明，AIGTs在多个维度上与人类撰写的文本不同，包括语言模式、主题分布、互动水平和作者的粉丝分布。我们期望对社交媒体中AIGTs的分析和发现能够为该领域的未来研究提供启示。 

---
# Exact Acceleration of Subgraph Graph Neural Networks by Eliminating Computation Redundancy 

**Title (ZH)**: 通过消除计算冗余来加速子图图神经网络的精确方法 

**Authors**: Qian Tao, Xiyuan Wang, Muhan Zhang, Shuxian Hu, Wenyuan Yu, Jingren Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2412.18125)  

**Abstract**: Graph neural networks (GNNs) have become a prevalent framework for graph tasks. Many recent studies have proposed the use of graph convolution methods over the numerous subgraphs of each graph, a concept known as subgraph graph neural networks (subgraph GNNs), to enhance GNNs' ability to distinguish non-isomorphic graphs. To maximize the expressiveness, subgraph GNNs often require each subgraph to have equal size to the original graph. Despite their impressive performance, subgraph GNNs face challenges due to the vast number and large size of subgraphs which lead to a surge in training data, resulting in both storage and computational inefficiencies. In response to this problem, this paper introduces Ego-Nets-Fit-All (ENFA), a model that uniformly takes the smaller ego nets as subgraphs, thereby providing greater storage and computational efficiency, while at the same time guarantees identical outputs to the original subgraph GNNs even taking the whole graph as subgraphs. The key is to identify and eliminate the redundant computation among subgraphs. For example, a node $v_i$ may appear in multiple subgraphs but is far away from all of their centers (the unsymmetric part between subgraphs). Therefore, its first few rounds of message passing within each subgraph can be computed once in the original graph instead of being computed multiple times within each subgraph. Such strategy enables our ENFA to accelerate subgraph GNNs in an exact way, unlike previous sampling approaches that often lose the performance. Extensive experiments across various datasets reveal that compared with the conventional subgraph GNNs, ENFA can reduce storage space by 29.0% to 84.5% and improve training efficiency by up to 1.66x. 

**Abstract (ZH)**: 图神经网络（GNNs）已成为图任务中广泛应用的框架。许多最新的研究提出使用图卷积方法处理每个图的众多子图，这是所谓的子图图神经网络（subgraph GNNs），来增强GNNs区分非同构图的能力。为了最大化表达能力，子图GNNs通常要求每个子图的大小与原始图相同。尽管它们表现出色，但由于子图数量庞大且体积较大，导致训练数据激增，从而造成存储和计算效率低下。为应对这一问题，本文引入了Ego-Nets-Fit-All（ENFA）模型，该模型统一采用较小的ego net作为子图，从而提供更大的存储和计算效率，同时保证在将整个图作为子图时，输出与原始子图GNNs一致。关键在于识别并消除子图之间的冗余计算。例如，一个节点 \(v_i\) 可能在多个子图中出现，但与这些子图的中心距离都较远（子图间非对称部分）。因此，它在每个子图中的前几轮消息传递可以在原始图中计算一次，而无需在每个子图中重复计算。这种策略使我们的ENFA能够以精确的方式加速子图GNNs，而不同于之前采样方法通常会牺牲性能。跨多个数据集的广泛实验表明，与传统的子图GNNs相比，ENFA可以减少存储空间29.0%到84.5%，并提高训练效率高达1.66倍。 

---
# AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation 

**Title (ZH)**: AutoDroid-V2: 基于生成代码提升SLM方法的GUI代理性能 

**Authors**: Hao Wen, Shizuo Tian, Borislav Pavlov, Wenjie Du, Yixuan Li, Ge Chang, Shanhui Zhao, Jiacheng Liu, Yunxin Liu, Ya-Qin Zhang, Yuanchun Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.18116)  

**Abstract**: Large language models (LLMs) have brought exciting new advances to mobile UI agents, a long-standing research field that aims to complete arbitrary natural language tasks through mobile UI interactions. However, existing UI agents usually demand high reasoning capabilities of powerful large models that are difficult to be deployed locally on end-users' devices, which raises huge concerns about user privacy and centralized serving cost. One way to reduce the required model size is to customize a smaller domain-specific model with high-quality training data, e.g. large-scale human demonstrations of diverse types of apps and tasks, while such datasets are extremely difficult to obtain. Inspired by the remarkable coding abilities of recent small language models (SLMs), we propose to convert the UI task automation problem to a code generation problem, which can be effectively solved by an on-device SLM and efficiently executed with an on-device code interpreter. Unlike normal coding tasks that can be extensively pretrained with public datasets, generating UI automation code is challenging due to the diversity, complexity, and variability of target apps. Therefore, we adopt a document-centered approach that automatically builds fine-grained API documentation for each app and generates diverse task samples based on this documentation. By guiding the agent with the synthetic documents and task samples, it learns to generate precise and efficient scripts to complete unseen tasks. Based on detailed comparisons with state-of-the-art mobile UI agents, our approach effectively improves the mobile task automation with significantly higher success rates and lower latency/token consumption. Code will be open-sourced. 

**Abstract (ZH)**: 大型语言模型（LLMs）为移动UI代理领域带来了令人兴奋的新进展。移动UI代理是一个长期研究的领域，旨在通过移动UI交互完成任意自然语言任务。然而，现有的UI代理通常需要高性能的大型模型来进行复杂的推理，这使得这些模型难以在终端用户设备上本地部署，引发了用户隐私和集中化服务成本的巨大关注。一种减少所需模型大小的方法是利用高质量的训练数据定制一个较小的领域特定模型，例如大规模的人类示范涵盖多种类型的应用和任务，但获取这样的数据集极其困难。受近期小型语言模型（SLMs）卓越编码能力的启发，我们提出将UI任务自动化问题转化为代码生成问题，该问题可以通过设备上的SLM有效解决，并通过设备上的代码解释器高效执行。与可以广泛使用公开数据集进行预训练的常规编程任务不同，生成UI自动化代码具有挑战性，因为目标应用的多样性、复杂性和多变性使其具有挑战性。因此，我们采用以文档为中心的方法，自动为每个应用构建细粒度的API文档，并基于这些文档生成多样化的任务样本。通过指导代理使用合成文档和任务样本，使其学习生成精确且高效的脚本以完成未见过的任务。基于与最先进的移动UI代理的详细比较，我们的方法在显著提高移动任务自动化的同时，成功率达到更高，延迟和token消耗更低。我们将开源代码。 

---
# AIGT: AI Generative Table Based on Prompt 

**Title (ZH)**: AIGT：基于提示的AI生成表格 

**Authors**: Mingming Zhang, Zhiqing Xiao, Guoshan Lu, Sai Wu, Weiqiang Wang, Xing Fu, Can Yi, Junbo Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.18111)  

**Abstract**: Tabular data, which accounts for over 80% of enterprise data assets, is vital in various fields. With growing concerns about privacy protection and data-sharing restrictions, generating high-quality synthetic tabular data has become essential. Recent advancements show that large language models (LLMs) can effectively gener-ate realistic tabular data by leveraging semantic information and overcoming the challenges of high-dimensional data that arise from one-hot encoding. However, current methods do not fully utilize the rich information available in tables. To address this, we introduce AI Generative Table (AIGT) based on prompt enhancement, a novel approach that utilizes meta data information, such as table descriptions and schemas, as prompts to generate ultra-high quality synthetic data. To overcome the token limit constraints of LLMs, we propose long-token partitioning algorithms that enable AIGT to model tables of any scale. AIGT achieves state-of-the-art performance on 14 out of 20 public datasets and two real industry datasets within the Alipay risk control system. 

**Abstract (ZH)**: 表数据约占企业数据资产的80%，在各个领域都具有重要作用。随着对隐私保护和数据共享限制的日益关注，生成高质量的合成表数据变得至关重要。最近的研究表明，大规模语言模型（LLMs）可以通过利用语义信息并克服一-hot编码带来的高维数据挑战，有效地生成具有现实感的表数据。然而，当前的方法并未充分利用表格中丰富的信息。为解决这一问题，我们提出了基于提示增强的AI生成表格（AIGT）方法，这是一种新型的方法，通过利用元数据信息（如表描述和结构）作为提示来生成超高质量的合成数据。为了克服LLMs的标记限制，我们提出了长标记分割算法，从而使AIGT能够模型任何规模的表格。实验结果显示，AIGT 在20个公开数据集中的14个以及支付宝风控系统中的两个真实企业数据集上取得了最先进的性能。 

---
# SlimGPT: Layer-wise Structured Pruning for Large Language Models 

**Title (ZH)**: SlimGPT：大型语言模型的层级结构化剪枝 

**Authors**: Gui Ling, Ziyang Wang, Yuliang Yan, Qingwen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18110)  

**Abstract**: Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results. 

**Abstract (ZH)**: 大型语言模型（LLMs）因其在各个领域内的卓越能力而受到了广泛关注，其庞大的参数规模为实际部署带来了挑战。结构化剪枝是平衡模型性能与效率的有效方法，但在计算资源受限的情况下，LLMs 的剪枝性能恢复是一个主要挑战。因此，我们提出了一种基于 Optimal Brain Surgeon 框架的低成本快速结构化剪枝方法，名为 SlimGPT。我们提出了批量贪婪剪枝以实现快速且接近最优的剪枝，通过分组 Cholesky 分解增强了按头剪枝误差估计的准确性，并通过动态组大小提高了前馈网络（FFN）的剪枝效率，从而在一小时内实现了近似局部最优的剪枝结果。此外，我们从误差积累的角度探讨了逐层剪枝的局限性，并提出了一种非均匀剪枝策略增益裁剪比例（Incremental Pruning Ratio）以减少性能退化。在 LLaMA 基准测试上的实验结果表明，SlimGPT 大幅优于其他方法，并实现了最先进的结果。 

---
# Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels 

**Title (ZH)**: 利用高效的元内核上混合预填充/解码/验证调度与最新优化技术应对生产LLM服务体系中的动态性 

**Authors**: Mingcong Song, Xinru Tang, Fengfan Hou, Jing Li, Wei Wei, Yipeng Ma, Runqiu Xiao, Hongjie Si, Dingcheng Jiang, Shouyi Yin, Yang Hu, Guoping Long  

**Link**: [PDF](https://arxiv.org/pdf/2412.18106)  

**Abstract**: Meeting growing demands for low latency and cost efficiency in production-grade large language model (LLM) serving systems requires integrating advanced optimization techniques. However, dynamic and unpredictable input-output lengths of LLM, compounded by these optimizations, exacerbate the issues of workload variability, making it difficult to maintain high efficiency on AI accelerators, especially DSAs with tile-based programming models. To address this challenge, we introduce XY-Serve, a versatile, Ascend native, end-to-end production LLM-serving system. The core idea is an abstraction mechanism that smooths out the workload variability by decomposing computations into unified, hardware-friendly, fine-grained meta primitives. For attention, we propose a meta-kernel that computes the basic pattern of matmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we introduce a virtual padding scheme that adapts to dynamic shape changes while using highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve sits harmoniously with vLLM. Experimental results show up to 89% end-to-end throughput improvement compared with current publicly available baselines on Ascend NPUs. Additionally, our approach outperforms existing GEMM (average 14.6% faster) and attention (average 21.5% faster) kernels relative to existing libraries. While the work is Ascend native, we believe the approach can be readily applicable to SIMT architectures as well. 

**Abstract (ZH)**: 为了满足生产级大规模语言模型（LLM）服务系统对低延迟和成本效率不断增长的需求，需要集成先进的优化技术。然而，LLM动态且不可预测的输入输出长度与这些优化措施相叠加，加剧了工作负载变异性的问题，使得在加速器（尤其是基于瓷砖编程模型的DSA）上维持高效率变得困难。为应对这一挑战，我们提出了XY-Serve，这是一种通用的、基于昇腾的端到端生产级LLM服务系统。核心思想是一种抽象机制，通过将计算分解为统一、硬件友好且细粒度的元原语，来平滑工作负载变异性。对于注意力机制，我们提出了一种元内核，该内核采用具有架构感知的瓷砖大小计算基本的矩阵乘-softmax-矩阵乘模式。对于GEMM（通用矩阵乘法），我们引入了一种虚拟填充方案，该方案能够适应动态形状变化，并使用具有多种固定瓷砖大小的高度高效GEMM原语。XY-Serve与vLLM兼容性良好。实验结果表明，在昇腾NPU上，与当前可用的基准实现相比，端到端吞吐量最多可提高89%。此外，与现有库中的GEMM（平均快14.6%）和注意力（平均快21.5%）内核相比，我们的方法展现出更高的性能。虽然这项工作是基于昇腾的，但我们相信该方法也可以很容易地应用于SIMT架构。 

---
# Real-world Deployment and Evaluation of PErioperative AI CHatbot (PEACH) -- a Large Language Model Chatbot for Perioperative Medicine 

**Title (ZH)**: 在临床 perioperative 医学中大规模语言模型聊天机器人 PErioperative AI CHatbot (PEACH) 的实际部署与评估 

**Authors**: Yu He Ke, Liyuan Jin, Kabilan Elangovan, Bryan Wen Xi Ong, Chin Yang Oh, Jacqueline Sim, Kenny Wei-Tsen Loh, Chai Rick Soh, Jonathan Ming Hua Cheng, Aaron Kwang Yang Lee, Daniel Shu Wei Ting, Nan Liu, Hairil Rizal Abdullah  

**Link**: [PDF](https://arxiv.org/pdf/2412.18096)  

**Abstract**: Large Language Models (LLMs) are emerging as powerful tools in healthcare, particularly for complex, domain-specific tasks. This study describes the development and evaluation of the PErioperative AI CHatbot (PEACH), a secure LLM-based system integrated with local perioperative guidelines to support preoperative clinical decision-making. PEACH was embedded with 35 institutional perioperative protocols in the secure Claude 3.5 Sonet LLM framework within Pair Chat (developed by Singapore Government) and tested in a silent deployment with real-world data. Accuracy, safety, and usability were assessed. Deviations and hallucinations were categorized based on potential harm, and user feedback was evaluated using the Technology Acceptance Model (TAM). Updates were made after the initial silent deployment to amend one protocol.
In 240 real-world clinical iterations, PEACH achieved a first-generation accuracy of 97.5% (78/80) and an overall accuracy of 96.7% (232/240) across three iterations. The updated PEACH demonstrated improved accuracy of 97.9% (235/240), with a statistically significant difference from the null hypothesis of 95% accuracy (p = 0.018, 95% CI: 0.952-0.991). Minimal hallucinations and deviations were observed (both 1/240 and 2/240, respectively). Clinicians reported that PEACH expedited decisions in 95% of cases, and inter-rater reliability ranged from kappa 0.772-0.893 within PEACH and 0.610-0.784 among attendings.
PEACH is an accurate, adaptable tool that enhances consistency and efficiency in perioperative decision-making. Future research should explore its scalability across specialties and its impact on clinical outcomes. 

**Abstract (ZH)**: 大型语言模型（LLMs）正逐渐成为医疗领域中强有力的工具，尤其是在处理复杂且领域特定的任务时。本研究描述了PErioperative AI CHatbot（PEACH）系统的发展与评价，PEACH是一种集成了本地围手术期指南的安全LLM系统，旨在支持术前临床决策。PEACH系统在安全的Claude 3.5 Sonet LLM框架（由新加坡政府开发的Pair Chat中的一个集成部分）内嵌入了35家机构的围手术期协议，并在真实数据环境下进行了静默部署测试。评估了其准确度、安全性与可用性。根据潜在的危害性，对偏离和幻觉进行了分类，并通过技术接受模型（TAM）评估了用户反馈。在首次静默部署后，对协议进行了更新。

在240次真实临床迭代中，PEACH的一代准确率为97.5%（78/80），整体准确率为96.7%（232/240）。更新后的PEACH准确率为97.9%（235/240），显著优于95%的零假设（p = 0.018，95% CI：0.952-0.991）。观察到的幻觉和偏差最少（分别为1/240和2/240）。临床医生报告，在95%的情况下，PEACH加速了决策过程，PEACH内部的临诊者者一致可靠性范围为κ值0.772-0.893，临诊者间的可靠性范围为0.610-0.784。

PEACH是一种准确且适应性强的工具，能增强围手术期决策的一致性和效率。未来研究应探讨其在不同专科领域的扩展性和对临床结果的影响。 

---
# AutoSculpt: A Pattern-based Model Auto-pruning Framework Using Reinforcement Learning and Graph Learning 

**Title (ZH)**: AutoSculpt：一种基于模式的模型自动裁剪框架，结合强化学习与图学习

解释：
- "AutoSculpt" 是一个专有名词，没有合适的中文对应词，保持原词。
- "Pattern-based" 翻译为 "基于模式的"。
- "Model Auto-pruning" 翻译为 "模型自动裁剪"。
- "Reinforcement Learning" 翻译为 "强化学习"。
- "Graph Learning" 翻译为 "图学习"。 

**Authors**: Lixian Jing, Jianpeng Qi, Junyu Dong, Yanwei Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18091)  

**Abstract**: As deep neural networks (DNNs) are increasingly deployed on edge devices, optimizing models for constrained computational resources is critical. Existing auto-pruning methods face challenges due to the diversity of DNN models, various operators (e.g., filters), and the difficulty in balancing pruning granularity with model accuracy. To address these limitations, we introduce AutoSculpt, a pattern-based automated pruning framework designed to enhance efficiency and accuracy by leveraging graph learning and deep reinforcement learning (DRL). AutoSculpt automatically identifies and prunes regular patterns within DNN architectures that can be recognized by existing inference engines, enabling runtime acceleration. Three key steps in AutoSculpt include: (1) Constructing DNNs as graphs to encode their topology and parameter dependencies, (2) embedding computationally efficient pruning patterns, and (3) utilizing DRL to iteratively refine auto-pruning strategies until the optimal balance between compression and accuracy is achieved. Experimental results demonstrate the effectiveness of AutoSculpt across various architectures, including ResNet, MobileNet, VGG, and Vision Transformer, achieving pruning rates of up to 90% and nearly 18% improvement in FLOPs reduction, outperforming all baselines. The codes can be available at this https URL 

**Abstract (ZH)**: 随着深度神经网络（DNNs）越来越多地部署在边缘设备上，优化模型以适应受限制的计算资源变得至关重要。现有的自适应剪枝方法由于DNN模型的多样性、各种操作符（例如，滤波器）以及剪枝粒度与模型准确性之间的平衡难度，存在挑战。为了解决这些限制，我们提出了AutoSculpt，这是一种基于模式的自动化剪枝框架，通过利用图学习和深度强化学习（DRL）来增强效率和准确性。AutoSculpt 自动识别并剪枝 DNN 架构中的可由现有推理引擎识别的规律模式，从而实现运行时加速。AutoSculpt 的三个关键步骤包括：(1) 将DNN表示为图以编码其拓扑结构和参数依赖性，(2) 嵌入计算高效的剪枝模式，以及(3) 利用DRL迭代优化自动剪枝策略，直到实现压缩与准确性的最佳平衡。实验结果表明，AutoSculpt 在包括ResNet、MobileNet、VGG和Vision Transformer等各种架构中都有效，实现了高达90%的剪枝率，并在减少计算量方面取得了近18%的改进，超过了所有基线方法。代码可以在以下链接获取：this https URL 

---
# Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models 

**Title (ZH)**: 基于属性增强的指令调优在大规模语言模型的多任务分子生成中的应用 

**Authors**: Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18084)  

**Abstract**: Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in this https URL. 

**Abstract (ZH)**: 大型语言模型（LLMs）在各种自然语言处理任务中得到了广泛应用，如问答和机器翻译。然而，由于缺乏标记数据以及生物化学性质的手动注释难度较高，分子生成任务的表现仍然有限，特别是在涉及多属性约束的任务中。在本工作中，我们提出了一种两步框架PEIT（Property Enhanced Instruction Tuning）来改善LLMs在分子相关的任务中的表现。在第一步中，我们使用文本描述、SMILES和生物化学性质作为多模态输入来预训练一个称为PEIT-GEN的模型，通过对齐多模态表示以合成指令数据。在第二步中，我们利用合成的数据对现有的开源LLMs进行微调，由此生成的PEIT-LLM可以处理分子描述、基于文本的分子生成、分子性质预测及我们新提出的多约束分子生成任务。实验结果显示，我们的预训练PEIT-GEN在分子描述任务中优于MolT5和BioT5，表明文本描述、结构和生物化学性质之间的模态对齐效果良好。此外，PEIT-LLM在多任务分子生成方面显示出有前途的改进，证明了PEIT框架在各种分子任务中的可扩展性。我们在此提供了代码、构建的指令数据和模型检查点，可以访问此链接：https://example.com。 

---
# Understanding Artificial Neural Network's Behavior from Neuron Activation Perspective 

**Title (ZH)**: 从神经元激活角度理解人工神经网络的行为 

**Authors**: Yizhou Zhang, Yang Sui  

**Link**: [PDF](https://arxiv.org/pdf/2412.18073)  

**Abstract**: This paper explores the intricate behavior of deep neural networks (DNNs) through the lens of neuron activation dynamics. We propose a probabilistic framework that can analyze models' neuron activation patterns as a stochastic process, uncovering theoretical insights into neural scaling laws, such as over-parameterization and the power-law decay of loss with respect to dataset size. By deriving key mathematical relationships, we present that the number of activated neurons increases in the form of $N(1-(\frac{bN}{D+bN})^b)$, and the neuron activation should follows power-law distribution. Based on these two mathematical results, we demonstrate how DNNs maintain generalization capabilities even under over-parameterization, and we elucidate the phase transition phenomenon observed in loss curves as dataset size plotted in log-axis (i.e. the data magnitude increases linearly). Moreover, by combining the above two phenomenons and the power-law distribution of neuron activation, we derived the power-law decay of neural network's loss function as the data size scale increases. Furthermore, our analysis bridges the gap between empirical observations and theoretical underpinnings, offering experimentally testable predictions regarding parameter efficiency and model compressibility. These findings provide a foundation for understanding neural network scaling and present new directions for optimizing DNN performance. 

**Abstract (ZH)**: 本文通过神经元激活动力学的视角探索了深度神经网络（DNNs）的复杂行为。我们提出了一种概率框架，可以将模型的神经元激活模式视为一个随机过程，揭示了神经网络尺度定律的理论洞察，如过度参数化以及损失相对于数据集大小的幂律衰减。通过推导出关键的数学关系，我们表明激活神经元的数量以 $N(1-(\frac{bN}{D+bN})^b)$ 的形式增加，并且神经元激活应遵循幂律分布。基于这两个数学结果，我们展示了即使在过度参数化的情况下，DNNs 如何保持泛化能力，并阐明了在对数轴上绘制数据集大小时（即数据量线性增长）观察到的损失曲线相变现象。此外，通过结合上述两种现象以及神经元激活的幂律分布，我们推导出了数据规模扩大时神经网络损失函数的幂律衰减。此外，我们的分析填补了经验观察与理论基础之间的差距，提供了关于参数效率和模型压缩性可实验验证的预测。这些发现为理解神经网络的尺度效应奠定了基础，并提出了优化DNN性能的新方向。 

---
# Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer Systems using Large Language Models 

**Title (ZH)**: 使用大型语言模型的动态多Agent协同与检索以构建多源问答系统 

**Authors**: Antony Seabra, Claudio Cavalcante, Joao Nepomuceno, Lucas Lago, Nicolaas Ruberg, Sergio Lifschitz  

**Link**: [PDF](https://arxiv.org/pdf/2412.17964)  

**Abstract**: We propose a methodology that combines several advanced techniques in Large Language Model (LLM) retrieval to support the development of robust, multi-source question-answer systems. This methodology is designed to integrate information from diverse data sources, including unstructured documents (PDFs) and structured databases, through a coordinated multi-agent orchestration and dynamic retrieval approach. Our methodology leverages specialized agents-such as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents - that dynamically select the most appropriate retrieval strategy based on the nature of each query. To further improve accuracy and contextual relevance, we employ dynamic prompt engineering, which adapts in real time to query-specific contexts. The methodology's effectiveness is demonstrated within the domain of Contract Management, where complex queries often require seamless interaction between unstructured and structured data. Our results indicate that this approach enhances response accuracy and relevance, offering a versatile and scalable framework for developing question-answer systems that can operate across various domains and data sources. 

**Abstract (ZH)**: 我们提出了一种方法论，该方法论结合了大型语言模型（LLM）检索领域的多项先进技术，以支持稳健的多源问答系统的发展。该方法论旨在通过协调多智能体编排和动态检索方式，整合来自多种数据源的信息，包括未结构化的文档（PDF）和结构化数据库。该方法论利用了专门的智能体——如SQL智能体、检索增强生成（RAG）智能体和路由智能体——这些智能体能够根据每个查询的性质动态选择最合适的检索策略。为进一步提高准确性和上下文相关性，我们采用了动态提示工程，这种技术能够实时适应查询特定的上下文。该方法论的有效性在合同管理领域得到验证，该领域中复杂的查询往往需要无缝地交互未结构化和结构化数据。我们的结果表明，这种方法提高了响应的准确性和相关性，提供了一个适用于多种领域和数据源的灵活且可扩展的框架，用于开发问答系统。 

---
# Study of the Proper NNUE Dataset 

**Title (ZH)**: NNUE模型合适数据集的研究 

**Authors**: Daniel Tan, Neftali Watkinson Medina  

**Link**: [PDF](https://arxiv.org/pdf/2412.17948)  

**Abstract**: NNUE (Efficiently Updatable Neural Networks) has revolutionized chess engine development, with nearly all top engines adopting NNUE models to maintain competitive performance. A key challenge in NNUE training is the creation of high-quality datasets, particularly in complex domains like chess, where tactical and strategic evaluations are essential. However, methods for constructing effective datasets remain poorly understood and under-documented. In this paper, we propose an algorithm for generating and filtering datasets composed of "quiet" positions that are stable and free from tactical volatility. Our approach provides a clear methodology for dataset creation, which can be replicated and generalized across various evaluation functions. Testing demonstrates significant improvements in engine performance, confirming the effectiveness of our method. 

**Abstract (ZH)**: NNUE（高效可更新神经网络）已经彻底改变了棋类引擎的研发，几乎所有顶尖引擎都采用了NNUE模型以维持竞争水平。NNUE训练中的一个关键挑战是如何创建高质量的数据集，特别是在像国际象棋这样的复杂领域，其中战术和战略评估至关重要。然而，构建有效数据集的方法仍不甚了解且缺乏详细的记录。在本文中，我们提出了一种用于生成和筛选“安静”位置数据集的算法，这些位置稳定且不包含战术波动。我们的方法提供了一种清晰的数据集生成方法论，该方法论可以在各种评估函数下进行复制和泛化。实验结果表明，在引擎性能方面有显著提升，证实了我们方法的有效性。 

---
# Surveillance Capitalism Revealed: Tracing The Hidden World Of Web Data Collection 

**Title (ZH)**: surveillance capitalisms 揭露：追踪网络数据收集的隐秘世界

此标题翻译旨在保留原文的核心含义，并符合学术规范。如果有更多具体内容或背景信息，可能需要进一步进行翻译和调整。 

**Authors**: Antony Seabra de Medeiros, Luiz Afonso Glatzl Junior, Sergio Lifschitz  

**Link**: [PDF](https://arxiv.org/pdf/2412.17944)  

**Abstract**: This study investigates the mechanisms of Surveillance Capitalism, focusing on personal data transfer during web navigation and searching. Analyzing network traffic reveals how various entities track and harvest digital footprints. The research reveals specific data types exchanged between users and web services, emphasizing the sophisticated algorithms involved in these processes. We present concrete evidence of data harvesting practices and propose strategies for enhancing data protection and transparency. Our findings highlight the need for robust data protection frameworks and ethical data usage to address privacy concerns in the digital age. 

**Abstract (ZH)**: 本研究探讨了监视资本主义的运作机制，重点关注在网络浏览和搜索过程中个人数据的传输。通过对网络流量的分析，展示了众多实体如何追踪和收集数字足迹。研究揭示了用户与网络服务之间交换的具体数据类型，强调了这些过程中涉及的复杂算法。我们提供了数据收集实践的确切证据，并提出了增强数据保护和透明度的策略。研究结果强调了在数字时代建立 robust 的数据保护框架和道德数据使用的重要性，以解决隐私问题。 

---
# Contrato360 2.0: A Document and Database-Driven Question-Answer System using Large Language Models and Agents 

**Title (ZH)**: Contrato360 2.0：一种基于文档和数据库的大语言模型及智能体驱动的问答系统 

**Authors**: Antony Seabra, Claudio Cavalcante, Joao Nepomuceno, Lucas Lago, Nicolaas Ruberg, Sergio Lifschitz  

**Link**: [PDF](https://arxiv.org/pdf/2412.17942)  

**Abstract**: We present a question-and-answer (Q\&A) application designed to support the contract management process by leveraging combined information from contract documents (PDFs) and data retrieved from contract management systems (database). This data is processed by a large language model (LLM) to provide precise and relevant answers. The accuracy of these responses is further enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL techniques, and agents that dynamically orchestrate the workflow. These techniques eliminate the need to retrain the language model. Additionally, we employed Prompt Engineering to fine-tune the focus of responses. Our findings demonstrate that this multi-agent orchestration and combination of techniques significantly improve the relevance and accuracy of the answers, offering a promising direction for future information systems. 

**Abstract (ZH)**: 我们提出了一种问答（Q&A）应用，该应用通过结合合同文件（PDF）和从合同管理系统（数据库）中检索的数据来支持合同管理流程。这些数据经过大规模语言模型（LLM）处理，提供精准且相关的答案。通过使用检索增强生成（RAG）、文本到SQL技术以及能够动态协调工作流的代理，这些方法进一步提高了答案的准确性。这些技术消除了重新训练语言模型的需要。此外，我们采用了提示工程（Prompt Engineering）来细化答案的焦点。我们的研究发现，这种多代理协调及技术组合显著提高了答案的相关性和准确性，为未来信息系统的发展提供了有前景的方向。 

---
# Causal Composition Diffusion Model for Closed-loop Traffic Generation 

**Title (ZH)**: 闭合环路交通生成的因果组成扩散模型 

**Authors**: Haohong Lin, Xin Huang, Tung Phan-Minh, David S. Hayden, Huan Zhang, Ding Zhao, Siddhartha Srinivasa, Eric M. Wolff, Hongge Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17920)  

**Abstract**: Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort. 

**Abstract (ZH)**: 模拟在自动驾驶的安全评估中至关重要，特别是在捕捉复杂交互行为方面。然而，在长尾情况中生成现实且可控的交通场景仍是一项重大挑战。现有生成模型在用户定义的可控性和真实性约束之间存在冲突目标，这种冲突在安全关键的上下文中被放大。在本文中，我们提出了因果组成扩散模型（CCDiff），这是一种结构引导的扩散框架，旨在解决这些挑战。我们首先将可控性和真实性的闭环模拟学习形式化为约束优化问题。然后，CCDiff通过自动识别并直接注入因果结构到扩散过程中，最大化可控性同时遵循真实性，从而提供有结构的指导以增强真实性和可控性。通过在基准数据集和闭环模拟器上的严格评估，CCDiff在生成现实且用户偏好轨迹方面显著优于现有最佳方法。我们的结果表明，CCDiff在碰撞率、离路率、FDE（预测末位误差）和舒适性等关键指标上提高了闭环性能，有效地提取并利用了因果结构。 

---
# The Unreasonable Effectiveness of Open Science in AI: A Replication Study 

**Title (ZH)**: 开放科学在人工智能领域的惊人效果：一项复制研究 

**Authors**: Odd Erik Gundersen, Odd Cappelen, Martin Mølnå, Nicklas Grimstad Nilsen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17859)  

**Abstract**: A reproducibility crisis has been reported in science, but the extent to which it affects AI research is not yet fully understood. Therefore, we performed a systematic replication study including 30 highly cited AI studies relying on original materials when available. In the end, eight articles were rejected because they required access to data or hardware that was practically impossible to acquire as part of the project. Six articles were successfully reproduced, while five were partially reproduced. In total, 50% of the articles included was reproduced to some extent. The availability of code and data correlate strongly with reproducibility, as 86% of articles that shared code and data were fully or partly reproduced, while this was true for 33% of articles that shared only data. The quality of the data documentation correlates with successful replication. Poorly documented or miss-specified data will probably result in unsuccessful replication. Surprisingly, the quality of the code documentation does not correlate with successful replication. Whether the code is poorly documented, partially missing, or not versioned is not important for successful replication, as long as the code is shared. This study emphasizes the effectiveness of open science and the importance of properly documenting data work. 

**Abstract (ZH)**: 科学领域已经报道了可重复性危机，但这种危机对人工智能研究的影响程度尚不完全清楚。因此，我们进行了一项系统性复制研究，涵盖了30篇高度引用的人工智能研究论文，并尽可能使用原始材料。最终，由于需要访问项目过程中实际难以获取的数据或硬件，有8篇文章被拒绝。6篇文章成功复制，5篇部分复制。总的来说，有50%的研究文章在一定程度上得到了复制。代码和数据的可用性与可复制性密切相关，86%分享了代码和数据的文章部分或全部得到了复制，而仅分享数据的文章中有33%得到了部分或全部复制。数据文档的质量与成功复制相关，没有充分记录或未正确指定的数据可能会导致复制失败。令人惊讶的是，代码文档的质量与成功复制无关。代码是否未充分记录、部分缺失或未版本化，只要代码被共享，对于成功复制并不重要。这项研究突显了开放科学的有效性，并强调了充分记录数据工作的重要性。 

---
# Bridging the Data Provenance Gap Across Text, Speech and Video 

**Title (ZH)**: 跨越文本、语音和视频的数据溯源鸿沟 

**Authors**: Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Naana Obeng-Marnu, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, Deividas Mataciunas, Tobin South, Jianguo Zhang, Ariel N. Lee, Campbell S. Lund, Christopher Klamm, Damien Sileo, Diganta Misra, Enrico Shippole, Kevin Klyman, Lester JV Miranda, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Vipul Gupta, Vivek Sharma, Xuhui Zhou, Caiming Xiong, Luis Villa, Stella Biderman, Alex Pentland, Sara Hooker, Jad Kabbara  

**Link**: [PDF](https://arxiv.org/pdf/2412.17847)  

**Abstract**: Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video. 

**Abstract (ZH)**: 人工智能的进步主要受训练数据的数量和质量驱动。尽管如此，针对除文本外其他模态（如语音和视频）的成熟数据集的属性进行实证分析的研究仍显不足。在本研究中，我们首次进行了横跨多种模态（热门文本、语音和视频数据集）的长时间序列审计，从详细的来源趋势和使用限制到地理和语言代表性。我们的手动分析涵盖了1990年至2024年间近4000个公开的数据集，跨越了608种语言、798个来源、659个组织和67个国家。我们发现，多模态机器学习应用主要依赖于网络抓取、合成以及社交媒体平台（如YouTube）的训练数据集，自2019年起，这些来源已远超其他所有数据源。其次，我们追踪数据集的生成链发现，尽管仅有不到33%的数据集受到限制性许可，广泛使用的文本、语音和视频数据集中超过80%的源内容带有非商业限制。最后，虽然公共AI训练数据集中涵盖的语言和地理数量日益增多，但我们的审计结果表明相对地理和多语言代表性指标自2013年以来并未显著改善。我们认为，我们审计的广度使我们能够从生态系统层面实证地探讨数据来源、限制以及西方中心主义趋势，对于负责任的人工智能发展而言，这些问题是至关重要的。作为进一步提高数据集透明度和负责任使用的贡献，我们发布了整个多模态审计结果，以便从业者能够跨文本、语音和视频追溯数据来源。 

---
# Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems 

**Title (ZH)**: GameFi中去中心化智能：具身人工智能代理及DeFi与虚拟生态系统融合 

**Authors**: Fernando Jia, Jade Zheng, Florence Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.18601)  

**Abstract**: In the rapidly evolving landscape of GameFi, a fusion of gaming and decentralized finance (DeFi), there exists a critical need to enhance player engagement and economic interaction within gaming ecosystems. Our GameFi ecosystem aims to fundamentally transform this landscape by integrating advanced embodied AI agents into GameFi platforms. These AI agents, developed using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI, are capable of proactive, adaptive, and contextually rich interactions with players. By going beyond traditional scripted responses, these agents become integral participants in the game's narrative and economic systems, directly influencing player strategies and in-game economies. We address the limitations of current GameFi platforms, which often lack immersive AI interactions and mechanisms for community engagement or creator monetization. Through the deep integration of AI agents with blockchain technology, we establish a consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers creators to monetize their contributions and fosters democratic collaboration among players and creators. Furthermore, by embedding DeFi mechanisms into the gaming experience, we enhance economic participation and provide new opportunities for financial interactions within the game. Our approach enhances player immersion and retention and advances the GameFi ecosystem by bridging traditional gaming with Web3 technologies. By integrating sophisticated AI and DeFi elements, we contribute to the development of more engaging, economically robust, and community-centric gaming environments. This project represents a significant advancement in the state-of-the-art in GameFi, offering insights and methodologies that can be applied throughout the gaming industry. 

**Abstract (ZH)**: 在快速发展的GameFi（将游戏和去中心化金融DeFi相结合）领域，迫切需要提升玩家参与度和经济互动。我们的GameFi生态系统旨在通过将先进的具身人工智能代理整合到GameFi平台中，从根本上改变这一领域。这些基于前沿大型语言模型（LLMs）如GPT-4和Claude AI开发的人工智能代理，能够在玩家之间实现主动、适应性强且情景丰富的互动。通过超越传统的预编程响应，这些代理将成为游戏叙事和经济系统的积极参与者，直接影响玩家策略和游戏内经济。

我们解决了当前GameFi平台普遍存在的问题，这些平台往往缺乏沉浸式的人工智能互动和社区参与或创作者变现的机制。通过深度整合人工智能代理与区块链技术，我们建立了一个共识驱动的、去中心化的GameFi生态系统。该生态系统赋予创作者变现其贡献的机会，并促进玩家和创作者之间的民主合作。此外，通过将DeFi机制嵌入游戏体验中，我们增强了经济参与并为游戏中的金融互动提供了新的机会。通过整合先进的人工智能和DeFi元素，我们的方法提升了玩家的沉浸感和留存率，并通过传统游戏和Web3技术的结合，推动了GameFi生态系统的进步。该项目在GameFi前沿技术方面实现了重要突破，提供了可用于整个游戏行业的见解和方法论。

总之，我们的研究和创新为更加吸引人、经济上更稳健且更具社区导向的游戏环境做出了贡献。该项目代表了GameFi领域的重大进展，提供了可应用于整个游戏行业的洞见和方法。 

---
# DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation 

**Title (ZH)**: DiTCtrl：探索多模态扩散变换器中的注意力控制以实现免调优多提示长视频生成 

**Authors**: Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, Xiangyu Yue  

**Link**: [PDF](https://arxiv.org/pdf/2412.18597)  

**Abstract**: Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training. 

**Abstract (ZH)**: 类似于Sora的视频生成模型在具有多模态扩散变换器（MM-DiT）架构的情况下已经取得了显著的进展。然而，当前的视频生成模型主要关注单命令生成，难以生成由多个连续命令生成的连贯场景，这些场景更能反映现实世界的动态场景。尽管一些开创性的工作已经探索了多命令视频生成，但它们面临着包括严格的训练数据需求、命令跟随弱以及不自然的过渡等重大挑战。为了解决这些问题，我们首次提出了基于MM-DiT架构的训练免费的多命令视频生成方法——DiTCtrl。我们提出的核心思想是将多命令视频生成任务视为具有平滑过渡的时序视频编辑。为了实现这一目标，我们首先分析了MM-DiT的注意力机制，发现其3D全注意力与UNet样式的扩散模型中的跨注意力/自我注意力块相似，这使得在多命令视频生成中能够通过注意力共享实现掩码引导的精准语义控制。基于我们精心的设计，DiTCtrl生成的视频能够在多个连续命令下实现平滑过渡和一致的物体运动，而无需额外的训练。此外，我们还提出了一个专门为多命令视频生成设计的新基准——MPVBench，用于评估多命令生成的表现。 extensive实验证明，我们的方法在无需额外训练的情况下达到了业界领先的表现。 

---
# A Paragraph is All It Takes: Rich Robot Behaviors from Interacting, Trusted LLMs 

**Title (ZH)**: 仅需一段文本：来自相互信任的大型语言模型丰富机器人行为 

**Authors**: OpenMind, Shaohong Zhong, Adam Zhou, Boyuan Chen, Homin Luo, Jan Liphardt  

**Link**: [PDF](https://arxiv.org/pdf/2412.18588)  

**Abstract**: Large Language Models (LLMs) are compact representations of all public knowledge of our physical environment and animal and human behaviors. The application of LLMs to robotics may offer a path to highly capable robots that perform well across most human tasks with limited or even zero tuning. Aside from increasingly sophisticated reasoning and task planning, networks of (suitably designed) LLMs offer ease of upgrading capabilities and allow humans to directly observe the robot's thinking. Here we explore the advantages, limitations, and particularities of using LLMs to control physical robots. The basic system consists of four LLMs communicating via a human language data bus implemented via web sockets and ROS2 message passing. Surprisingly, rich robot behaviors and good performance across different tasks could be achieved despite the robot's data fusion cycle running at only 1Hz and the central data bus running at the extremely limited rates of the human brain, of around 40 bits/s. The use of natural language for inter-LLM communication allowed the robot's reasoning and decision making to be directly observed by humans and made it trivial to bias the system's behavior with sets of rules written in plain English. These rules were immutably written into Ethereum, a global, public, and censorship resistant Turing-complete computer. We suggest that by using natural language as the data bus among interacting AIs, and immutable public ledgers to store behavior constraints, it is possible to build robots that combine unexpectedly rich performance, upgradability, and durable alignment with humans. 

**Abstract (ZH)**: 大规模语言模型（LLMs）是所有关于我们物理环境以及动物和人类行为的公开知识的紧凑表示。将LLMs应用于机器人技术可能会提供一条途径，即构建出能够在大多数人类任务上表现优异的机器人，而无需或只需最少的微调。除了不断增强的推理和任务规划能力外，适配设计的LLM网络还提供了升级能力的简便性，并允许人类直接观察机器人的思考过程。本文探讨了使用LLMs控制物理机器人的优势、限制和独特之处。基本系统由四个LLM通过WebSocket和ROS2消息传递实现的人类语言数据总线进行通信。令人惊讶的是，尽管机器人数据融合循环的运行频率仅为1Hz，且中心数据总线的运行速率受限于类似人类大脑的极低速率（约40比特/秒），但仍然能够实现丰富的机器人行为和在不同任务中的良好表现。利用自然语言促进LLM之间的通信，使得人类可以直接观察机器人的推理和决策过程，并且使用简单的英语书写规则即可轻松偏移系统的行为。这些规则被不可变更地写入了以太坊这样一个全球性、公众性和抗审查的图灵完备计算机中。我们认为，通过在交互AI之间使用自然语言作为数据总线，并将行为约束存储在不可变的公共账本中，有可能构建出兼具惊人性能、易于升级以及持久与人类一致性的机器人。 

---
# How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation 

**Title (ZH)**: 不同应用场景下大型语言模型生成代码的能力如何？基准测试与评估 

**Authors**: Dewu Zheng, Yanlin Wang, Ensheng Shi, Hongyu Zhang, Zibin Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.18573)  

**Abstract**: Recently, an increasing number of AI-driven programming assistants powered by code LLMs have been integrated into various real-world software development environments, significantly boosting developer productivity. However, existing code generation benchmarks primarily focus on general-purpose scenarios, leaving the code generation performance of LLMs for specific application domains largely unknown. In this paper, we introduce a new benchmark, MultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming tasks, covering 12 popular software development domains and 15 programming languages. Specifically, we perform in-depth research to identify these 12 application domains. Given that each domain may involve multiple technical frameworks, and that different frameworks present distinct challenges in the coding process, we categorize the commonly used frameworks and platforms within each domain. We then sample programming problems from GitHub repositories related to these subdomains. To ensure the quality of the tasks and mitigate data leakage issues, we invite annotators to rewrite the docstrings for each task in MultiCodeBench. Additionally, we build a static analysis-based dependency parsing tool to extract the dependencies in the ground truth for each task, enabling deeper performance analysis. Through extensive experiments on MultiCodeBench with eleven representative mainstream LLMs, we reveal the code generation performance of the LLMs across different application domains, providing practical insights for developers in downstream fields when selecting LLMs. Furthermore, we analyze the reasons behind the models' failures in completing software application development tasks, offering guidance for model developers to enhance domain-specific code generation capabilities. 

**Abstract (ZH)**: 近年来，由代码大型语言模型（Code LLMs）驱动的AI编程助手被越来越多地集成到各种实际软件开发环境中，显著提升了开发者的生产力。然而，现有的代码生成基准主要集中在通用场景上，使得代码生成性能在特定应用领域中的表现仍然 largely unknown。本文旨在填补这一空白，引入了一个新的基准测试MultCodeBench。MultCodeBench 包含2,400个编程任务，涵盖了12个流行的软件开发领域和15种编程语言。具体来说，我们进行了深入研究以识别这12个应用领域。鉴于每个领域可能存在多种技术框架，不同框架在编码过程中呈现出不同的挑战，我们对每个领域的常用框架和平台进行了分类。然后，我们从与这些子领域相关的GitHub仓库中抽取编程问题。为确保任务质量并减轻数据泄漏问题，我们邀请标注员为MultCodeBench中的每个任务重写文档字符串。此外，我们构建了一个基于静态分析的依赖关系解析工具，以提取每个任务的黄金标准中的依赖关系，从而实现更深入的性能分析。通过在MultCodeBench上进行广泛的实验，我们揭示了不同应用领域的LLM的代码生成性能，为下游开发者在选择LLM时提供了实际指导。同时，我们分析了模型未能完成软件应用开发任务的原因，为模型开发者提供指导，以增强领域特定的代码生成能力。 

---
# Token-Budget-Aware LLM Reasoning 

**Title (ZH)**: Token-Budget-Aware LLM推理 

**Authors**: Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, Zhenting Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18547)  

**Abstract**: Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: this https URL. 

**Abstract (ZH)**: 逻辑推理对于大型语言模型（LLMs）在广泛任务上的表现至关重要。虽然诸如链式思维（Chain-of-Thought, CoT）等方法通过将问题分解为中间步骤来提高LLM的性能，但也导致了显著的token使用量增加，从而提高了成本。我们发现当前LLMs的推理过程过长，并且可以通过在提示中包含合理的token预算来压缩，但token预算的选择在实际压缩效果中起着关键作用。我们随后提出了一种基于token预算的LLM推理框架，该框架根据推理复杂性动态估计不同问题的token预算，并使用估算的token预算来指导推理过程。实验表明，我们的方法在CoT推理中有效减少了token成本，同时仅造成轻微的性能下降，从而提供了一种在效率和准确性之间平衡的实际解决方案。代码：[此链接](this https URL)。 

---
# Advancing Deformable Medical Image Registration with Multi-axis Cross-covariance Attention 

**Title (ZH)**: 基于多轴交叉协方差注意力的可变形医学图像配准进展 

**Authors**: Mingyuan Meng, Michael Fulham, Lei Bi, Jinman Kim  

**Link**: [PDF](https://arxiv.org/pdf/2412.18545)  

**Abstract**: Deformable image registration is a fundamental requirement for medical image analysis. Recently, transformers have been widely used in deep learning-based registration methods for their ability to capture long-range dependency via self-attention (SA). However, the high computation and memory loads of SA (growing quadratically with the spatial resolution) hinder transformers from processing subtle textural information in high-resolution image features, e.g., at the full and half image resolutions. This limits deformable registration as the high-resolution textural information is crucial for finding precise pixel-wise correspondence between subtle anatomical structures. Cross-covariance Attention (XCA), as a "transposed" version of SA that operates across feature channels, has complexity growing linearly with the spatial resolution, providing the feasibility of capturing long-range dependency among high-resolution image features. However, existing XCA-based transformers merely capture coarse global long-range dependency, which are unsuitable for deformable image registration relying primarily on fine-grained local correspondence. In this study, we propose to improve existing deep learning-based registration methods by embedding a new XCA mechanism. To this end, we design an XCA-based transformer block optimized for deformable medical image registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves as a general network block that can be embedded into various registration network architectures. It can capture both global and local long-range dependency among high-resolution image features by applying regional and dilated XCA in parallel via a multi-axis design. Extensive experiments on two well-benchmarked inter-/intra-patient registration tasks with seven public medical datasets demonstrate that our MAXCA block enables state-of-the-art registration performance. 

**Abstract (ZH)**: 变形图像配准是医学图像分析中的一个基本要求。最近，transformer在基于深度学习的配准方法中得到了广泛应用，这是由于其通过自注意力机制（SA）能够捕捉长距离依赖性。然而，SA的高计算和内存负载（随着空间分辨率的增加呈二次增长）妨碍了transformer处理高分辨率图像特征中的微小纹理信息，例如在全分辨率和半分辨率图像中。这限制了变形配准，因为高分辨率的纹理信息对于找到细微解剖结构之间的精确像素级对应至关重要。交叉协方差注意（XCA）作为SA的“转置”版本，可以在特征通道之间操作，其复杂度随着空间分辨率呈线性增长，为捕捉高分辨率图像特征之间的长距离依赖性提供了可行性。然而，现有的基于XCA的transformer仅能捕捉粗略的全局长距离依赖性，这些依赖性对于主要依赖细小局部对应性的变形图像配准来说是不合适的。在本研究中，我们提出通过嵌入新的XCA机制来改进现有的基于深度学习的配准方法。为此，我们设计了一种优化的基于XCA的transformer块，适用于变形医学图像配准，并命名为多轴XCA（MAXCA）。我们的MAXCA作为一个通用的网络块，可以嵌入到各种配准网络架构中。通过多轴设计同时应用区域性和扩展性XCA，它可以捕获高分辨率图像特征之间的全局和局部长距离依赖性。在两个广泛认可的跨患者/同一患者配准任务中，使用了七个公开的医学数据集进行广泛的实验，结果表明，我们的MAXCA块能够实现最先进的配准性能。 

---
# Consistency Checks for Language Model Forecasters 

**Title (ZH)**: 语言模型预测器的一致性检查 

**Authors**: Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr  

**Link**: [PDF](https://arxiv.org/pdf/2412.18544)  

**Abstract**: Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters instantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on arbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur can trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting. 

**Abstract (ZH)**: 预测是一项难以评估的任务：真实情况只能在未来知晓。近期工作表明，基于大模型的预测工具已经迅速接近人类水平的性能，这引发了一个问题：我们如何能够即时地对这些预测工具进行基准测试和评估？我们依据一致性检查框架，根据预测在不同逻辑相关问题上的一致性来评估预测工具的性能。我们提出了一种基于套利的新的一般性一致性度量方法：例如，如果有预测AI无逻辑地预测2024年美国总统选举中民主党与共和党获胜的可能性均为60%，那么套利者可以通过对冲预测结果实现盈利。我们构建了一个自动化评估系统，生成一系列基础问题，从这些问题中实例化一致性检查，获取预测者的预测，并测量预测的一致性。随后，我们构建了一个标准的拟合评分规则（Proper Scoring Rule）预测基准，并展示了我们（即时的）一致性度量与大模型预测者的未来已知布瑞尔得分（Brier score）相关。我们还发布了一个在2028年发布的基准，提供了一个长期的预测评估工具。 

---
# Characterizations of Language Generation With Breadth 

**Title (ZH)**: 语言生成的广度特性研究 

**Authors**: Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas  

**Link**: [PDF](https://arxiv.org/pdf/2412.18530)  

**Abstract**: We study language generation in the limit, introduced by Kleinberg and Mullainathan [KM24], building on classical works of Gold [Gol67] and Angluin [Ang79]. [KM24] proposed an algorithm that generates strings from any countable language collection in the limit. While their algorithm eventually outputs strings from the target language $K$, it sacrifices breadth, i.e., the ability to generate all strings in $K$. A key open question in [KM24] is whether this trade-off between consistency and breadth is inherrent.
Recent works proposed different notions of consistent generation with breadth. Kalavasis, Mehrotra, and Velegkas [KVM24] introduced three definitions: generation with exact breadth, approximate breadth, and unambiguous generation. Concurrently and independently, Charikar and Pabbaraju [CP24a] proposed exhaustive generation. Both works examined when generation with these notions of breadth is possible.
Building on [CP24a, KVM24], we fully characterize language generation for these notions and their natural combinations. For exact breadth, we provide an unconditional lower bound, removing a technical condition from [KVM24] and extending the result of [CP24a] that holds for specific collections of languages. We show that generation with exact breadth is characterized by Angluin's condition for identification. We further introduce a weaker version of Angluin's condition that tightly characterizes both approximate breadth and exhaustive generation, proving their equivalence. Additionally, we show that unambiguous generation is also characterized by Angluin's condition as a special case of a broader result. Finally, we strengthen [KVM24] by giving unconditional lower bounds for stable generators, showing that Angluin's condition characterizes the previous breadth notions for stable generators. This shows a separation between stable and unstable generation with approximate breadth. 

**Abstract (ZH)**: 我们研究了Kleinberg和Mullainathan [KM24] 引入的极限语言生成问题，这建立在Gold [Gol67]和Angluin [Ang79]的古典工作之上。[KM24]提出了一种算法，能够从任何可数语言集合中生成字符串。虽然该算法最终会输出目标语言 $K$ 中的字符串，但它牺牲了广度，即生成 $K$ 中所有字符串的能力。[KM24] 中提出的悬而未决的关键问题是这种一致性与广度之间的权衡是否不可避免。

最近的一些工作提出了不同形式的具有广度的一致生成。Kalavasis, Mehrotra, 和 Velegkas [KVM24] 提出了三种定义：精确广度生成、近似广度生成和非歧义生成。同期独立地，Charikar 和 Pabbaraju [CP24a] 提出了完全生成。两篇论文都探讨了在这些广度定义下的生成何时是可行的。

基于 [CP24a, KVM24] 的工作，我们全面描述了这些定义及其自然组合下的语言生成特征。对于精确广度，我们提供了一个无条件的下界，去除了 [KVM24] 中的技术条件，并扩展了 [CP24a] 研究的结果，该结果适用于特定的语言集合。我们证明了精确广度生成由 Angluin 的识别条件表征。我们进一步引入了 Angluin 条件的一个较弱版本，该版本紧密表征了近似广度和完全生成，证明了它们的等价性。此外，我们展示了非歧义生成也由 Angluin 的条件作为更广泛结果的特殊情况进行表征。最后，我们通过给稳定生成器提供无条件的下界加强了 [KVM24] 的结果，展示了 Angluin 的条件如何表征先前的广度概念。这表明了近似广度下稳定生成和不稳定生成之间的区分。 

---
# Joint Adaptive OFDM and Reinforcement Learning Design for Autonomous Vehicles: Leveraging Age of Updates 

**Title (ZH)**: 自动驾驶车辆中联合自适应OFDM和强化学习设计：利用更新时延优势 

**Authors**: Mamady Delamou, Ahmed Naeem, Huseyin Arslan, El Mehdi Amhoud  

**Link**: [PDF](https://arxiv.org/pdf/2412.18500)  

**Abstract**: Millimeter wave (mmWave)-based orthogonal frequency-division multiplexing (OFDM) stands out as a suitable alternative for high-resolution sensing and high-speed data transmission. To meet communication and sensing requirements, many works propose a static configuration where the wave's hyperparameters such as the number of symbols in a frame and the number of frames in a communication slot are already predefined. However, two facts oblige us to redefine the problem, (1) the environment is often dynamic and uncertain, and (2) mmWave is severely impacted by wireless environments. A striking example where this challenge is very prominent is autonomous vehicle (AV). Such a system leverages integrated sensing and communication (ISAC) using mmWave to manage data transmission and the dynamism of the environment. In this work, we consider an autonomous vehicle network where an AV utilizes its queue state information (QSI) and channel state information (CSI) in conjunction with reinforcement learning techniques to manage communication and sensing. This enables the AV to achieve two primary objectives: establishing a stable communication link with other AVs and accurately estimating the velocities of surrounding objects with high resolution. The communication performance is therefore evaluated based on the queue state, the effective data rate, and the discarded packets rate. In contrast, the effectiveness of the sensing is assessed using the velocity resolution. In addition, we exploit adaptive OFDM techniques for dynamic modulation, and we suggest a reward function that leverages the age of updates to handle the communication buffer and improve sensing. The system is validated using advantage actor-critic (A2C) and proximal policy optimization (PPO). Furthermore, we compare our solution with the existing design and demonstrate its superior performance by computer simulations. 

**Abstract (ZH)**: 基于毫米波（mmWave）的正交频分多路复用（OFDM）技术在高分辨率感知和高速数据传输方面表现出色，是适合的选择。为了满足通信和感知需求，许多研究提出了静态配置方案，其中波的超参数（如每个帧中的符号数和通信时间段中的帧数）已经预先定义。然而，两个事实迫使我们重新定义这个问题：（1）环境通常是动态和不确定的；（2）毫米波对无线环境极为敏感。这一挑战在自主驾驶车辆（AV）中尤为突出。该系统利用毫米波进行集成感知与通信（ISAC），以管理数据传输和环境的动态变化。在本文中，我们考虑了一个AV网络，其中AV利用队列状态信息（QSI）和信道状态信息（CSI），结合强化学习技术来管理和控制通信与感知。这使得AV能够实现两个主要目标：与其他AV建立稳定的通信链路，并以高分辨率准确估计周围物体的运动速度。因此，通信性能是基于队列状态、有效数据速率和丢弃报文率来评估的。相比之下，感知效果的有效性则通过运动速度分辨率来评估。此外，我们利用自适应OFDM技术进行动态调制，并提出一个基于更新年龄的奖励函数，以管理和改善通信缓存并提升感知效果。系统通过优势演员-评论家（A2C）和最近邻策略优化（PPO）进行验证。进一步地，我们将我们的解决方案与现有设计进行比较，并通过计算机仿真实验证明其优越性能。 

---
# How "Real" is Your Real-Time Simultaneous Speech-to-Text Translation System? 

**Title (ZH)**: 你的实时同步语音转文本翻译系统有多“真实”？ 

**Authors**: Sara Papi, Peter Polak, Ondřej Bojar, Dominik Macháček  

**Link**: [PDF](https://arxiv.org/pdf/2412.18495)  

**Abstract**: Simultaneous speech-to-text translation (SimulST) translates source-language speech into target-language text concurrently with the speaker's speech, ensuring low latency for better user comprehension. Despite its intended application to unbounded speech, most research has focused on human pre-segmented speech, simplifying the task and overlooking significant challenges. This narrow focus, coupled with widespread terminological inconsistencies, is limiting the applicability of research outcomes to real-world applications, ultimately hindering progress in the field. Our extensive literature review of 110 papers not only reveals these critical issues in current research but also serves as the foundation for our key contributions. We 1) define the steps and core components of a SimulST system, proposing a standardized terminology and taxonomy; 2) conduct a thorough analysis of community trends, and 3) offer concrete recommendations and future directions to bridge the gaps in existing literature, from evaluation frameworks to system architectures, for advancing the field towards more realistic and effective SimulST solutions. 

**Abstract (ZH)**: 同步口译转写（SimulST）系统在说话人说话的同时，将源语言语音即时翻译成目标语言文本，以确保低延迟，从而提高用户的理解能力。尽管SimulST系统旨在应用于无限量的语音场景，但大多数研究主要集中在人类预先分割的语音上，简化了任务并忽视了重要的挑战。这种狭窄的研究范围，加之术语不一致的普遍现象，限制了研究成果在实际应用中的适用性，最终阻碍了该领域的进展。通过对110篇论文的广泛文献综述，我们不仅发现了当前研究中的关键问题，而且为我们的主要贡献奠定了基础。我们1）定义了SimulST系统的步骤和核心组件，提出了一套标准术语和分类；2）对社区趋势进行了深入分析；3）提供了具体的建议和未来方向，以填补现有文献中的空白，从评估框架到系统架构，推动该领域向更现实和有效的SimulST解决方案发展。 

---
# An Overview and Discussion of the Suitability of Existing Speech Datasets to Train Machine Learning Models for Collective Problem Solving 

**Title (ZH)**: 现有语音数据集用于训练解决集体问题的机器学习模型的适用性综述与讨论 

**Authors**: Gnaneswar Villuri, Alex Doboli  

**Link**: [PDF](https://arxiv.org/pdf/2412.18489)  

**Abstract**: This report characterized the suitability of existing datasets for devising new Machine Learning models, decision making methods, and analysis algorithms to improve Collaborative Problem Solving and then enumerated requirements for future datasets to be devised. Problem solving was assumed to be performed in teams of about three, four members, which talked to each other. A dataset consists of the speech recordings of such teams. The characterization methodology was based on metrics that capture cognitive, social, and emotional activities and situations. The report presented the analysis of a large group of datasets developed for Spoken Language Understanding, a research area with some similarity to Collaborative Problem Solving. 

**Abstract (ZH)**: 本报告分析了现有数据集适合用于设计新的机器学习模型、决策方法和分析算法以改进合作解决问题的程度，随后列出了未来需要设计的数据集的要求。假设问题解决是在大约3人至4人的团队中进行的，该团队成员之间相互交流。数据集包含这些团队的语音录制。分析方法基于能够捕捉认知、社会和情绪活动及情况的指标。报告对大量用于口语理解研究领域（该领域与合作解决问题存在一定相似性）的数据集进行了分析。 

---
# GeFL: Model-Agnostic Federated Learning with Generative Models 

**Title (ZH)**: GeFL：基于生成模型的模型无"https://www.arxiv.org/abs/2302.14388"依赖联邦学习 

**Authors**: Honggu Kang, Seohyeon Cha, Joonhyuk Kang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18460)  

**Abstract**: Federated learning (FL) is a promising paradigm in distributed learning while preserving the privacy of users. However, the increasing size of recent models makes it unaffordable for a few users to encompass the model. It leads the users to adopt heterogeneous models based on their diverse computing capabilities and network bandwidth. Correspondingly, FL with heterogeneous models should be addressed, given that FL typically involves training a single global model. In this paper, we propose Generative Model-Aided Federated Learning (GeFL), incorporating a generative model that aggregates global knowledge across users of heterogeneous models. Our experiments on various classification tasks demonstrate notable performance improvements of GeFL compared to baselines, as well as limitations in terms of privacy and scalability. To tackle these concerns, we introduce a novel framework, GeFL-F. It trains target networks aided by feature-generative models. We empirically demonstrate the consistent performance gains of GeFL-F, while demonstrating better privacy preservation and robustness to a large number of clients. Codes are available at [1]. 

**Abstract (ZH)**: 联邦学习（FL）是一种在分布式学习中保持用户隐私的有前途的范式。然而，近年来模型的增大使得几名用户无法负担起整个模型。这导致用户根据各自的计算能力和网络带宽采用异构模型。鉴于FL通常涉及训练单一全球模型，有必要解决异构模型带来的问题。本文提出了一种结合生成模型的生成模型辅助联邦学习（GeFL），该生成模型可以跨异构模型用户聚集全球知识。我们在各种分类任务中的实验表明，GeFL在性能上显著优于基准方法，并且在隐私和可扩展性方面存在问题。为了解决这些问题，我们引入了一个新的框架GeFL-F，该框架通过特征生成模型辅助目标网络的训练。我们实证展示了GeFL-F的一致性能增益，并且在大量客户端的情况下展示了更好的隐私保护和鲁棒性。相关代码可在[1]中获取。 

---
# SoK: On the Offensive Potential of AI 

**Title (ZH)**: SoK：探讨AI的进攻性潜力 

**Authors**: Saskia Laura Schröer, Giovanni Apruzzese, Soheil Human, Pavel Laskov, Hyrum S. Anderson, Edward W. N. Bernroider, Aurore Fass, Ben Nassi, Vera Rimmer, Fabio Roli, Samer Salam, Ashley Shen, Ali Sunyaev, Tim Wadwha-Brown, Isabel Wagner, Gang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18442)  

**Abstract**: Our society increasingly benefits from Artificial Intelligence (AI). Unfortunately, more and more evidence shows that AI is also used for offensive purposes. Prior works have revealed various examples of use cases in which the deployment of AI can lead to violation of security and privacy objectives. No extant work, however, has been able to draw a holistic picture of the offensive potential of AI. In this SoK paper we seek to lay the ground for a systematic analysis of the heterogeneous capabilities of offensive AI. In particular we (i) account for AI risks to both humans and systems while (ii) consolidating and distilling knowledge from academic literature, expert opinions, industrial venues, as well as laymen -- all of which being valuable sources of information on offensive AI.
To enable alignment of such diverse sources of knowledge, we devise a common set of criteria reflecting essential technological factors related to offensive AI. With the help of such criteria, we systematically analyze: 95 research papers; 38 InfoSec briefings (from, e.g., BlackHat); the responses of a user study (N=549) entailing individuals with diverse backgrounds and expertise; and the opinion of 12 experts. Our contributions not only reveal concerning ways (some of which overlooked by prior work) in which AI can be offensively used today, but also represent a foothold to address this threat in the years to come. 

**Abstract (ZH)**: 我们的社会越来越依赖人工智能（AI）。不幸的是，越来越多的证据表明，AI 也正在被用于攻击性用途。先前的研究揭示了多种 AI 部署可能导致安全和隐私目标被侵犯的应用案例。然而，目前尚无任何研究能够全面描绘出 AI 的攻击潜力。在本文中，我们旨在为系统分析不同类型的攻击性 AI 能力奠定基础。具体而言，我们将（i）对人类和系统面临的 AI 风险进行考虑，（ii）整合和提炼来自学术文献、专家意见、工业领域以及普通公众的信息——这些都是关于攻击性 AI 的宝贵信息来源。

为了弥合这些多元知识源之间的差距，我们制定了一个共同的评估标准，该标准反映了与攻击性 AI 相关的关键技术要素。借助这些标准，我们系统地分析了以下内容：95 篇研究论文；38 份信息安全简报（例如来自 BlackHat 的）；一项包含 549 名具有不同背景和技能水平的个体的用户研究结果；以及 12 位专家的意见。

我们的贡献不仅揭示了 AI 可以被以攻击性方式使用的各种令人担忧的方式（其中一些已经被先前的研究所忽略），而且也为未来应对这种威胁提供了有力的支撑。 

---
# GeAR: Graph-enhanced Agent for Retrieval-augmented Generation 

**Title (ZH)**: GeAR：增强图表示的检索增强生成代理 

**Authors**: Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan Tu, Zeren Jiang, Ruofei Lai, Yang Ren, Jeff Z. Pan  

**Link**: [PDF](https://arxiv.org/pdf/2412.18431)  

**Abstract**: Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GeAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GeAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems. 

**Abstract (ZH)**: 检索增强生成系统依赖于有效的文档检索能力。设计上，传统的稀疏或稠密检索器在多跳检索场景中面临挑战。本文中，我们提出了GeAR，通过两项关键创新来提升RAG性能：(i) 图扩展，它可以增强任何传统的基础检索器，例如BM25；(ii) 一个集成了图扩展的代理框架。我们的评估结果显示，GeAR在三个多跳问答数据集上的检索性能优于其他方法。此外，我们的系统在具有挑战性的MuSiQue数据集上取得了最先进的成果，相较于其他多步检索系统，它需要更少的令牌和迭代次数，性能提升超过10%。 

---
# Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English 

**Title (ZH)**: 多语言数学推理：推进印地语和英语中的开源大规模语言模型 

**Authors**: Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Manvendra Kumar Nema, Raj Jaiswal, Rajiv Ratn Shah  

**Link**: [PDF](https://arxiv.org/pdf/2412.18415)  

**Abstract**: Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non English languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resource efficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Gemini's accuracy on English datasets by +6% and matches Gemini's performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）在语言任务方面表现出色，但在数学推理方面存在不足，特别是在非英语语言（如印地语）中更为明显。本研究旨在提升较小且资源高效的开源LLMs在印地语和英语中的数学推理能力。我们使用零样本、少样本以及带有思维链（CoT）的方法和监督微调对OpenHathi 7B、LLaMA-2 7B、WizardMath 7B、Mistral 7B、LLeMMa 7B、MAmmoTH 7B、Gemini Pro和GPT-4等模型进行了评估。我们的方法包括采用递增式学习策略，逐步训练模型解决越来越难的问题；引入一种新颖的分解策略来简化复杂的算术运算；采用结构化的解题设计，将解决方案分为多个阶段。实验结果显示，这些方法显著提升了模型的能力。WizardMath 7B在英语数据集上的准确性超过了Gemini，高出6%，并且在印地语数据集上的表现与Gemini相当。采用双语方法，即结合英语和印地语样本，实现了与单语言模型相当的结果，表明模型能够在两种语言中学习数学推理。本研究强调了提高开源LLMs数学推理能力的潜力。 

---
# A Statistical Framework for Ranking LLM-Based Chatbots 

**Title (ZH)**: 基于统计框架的大型语言模型驱动的聊天机器人排名方法 

**Authors**: Siavash Ameli, Siyuan Zhuang, Ion Stoica, Michael W. Mahoney  

**Link**: [PDF](https://arxiv.org/pdf/2412.18407)  

**Abstract**: Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties -- an integral aspect of human-judged comparisons -- significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经重塑了自然语言处理领域，而像Chatbot Arena这样的框架则提供了评估这些模型的先驱平台。通过基于人类判断进行数百万次成对比较，Chatbot Arena已成为LLM评估的基石，提供了丰富数据集用于在开放对话任务中排名模型。在此基础上，我们提出了一种统计框架，该框架结合了关键技术进步，以解决成对比较分析中的特定挑战。首先，我们引入了一种分因素平局模型，增强了处理平局的能力——这是人类评判比较中的一个关键方面，显著提高了模型对观察数据的拟合度。其次，我们扩展了该框架以建模竞争者之间的协方差，这有助于更深入地了解性能关系，并促进直观的性能层次分组。第三，我们通过引入新型约束条件解决了由参数非唯一性引起的优化挑战，确保了参数估计的稳定性和可解释性。通过严格的评估和广泛的实验，我们的框架在建模成对比较数据方面表现出显著改进。为了支持可重复性和实际应用，我们发布了Leaderbot，这是一个开源的Python包，实现了我们的模型和分析。 

---
# TPAoI: Ensuring Fresh Service Status at the Network Edge in Compute-First Networking 

**Title (ZH)**: TPAoI：在计算优先网络中确保网络边缘的最新服务状态 

**Authors**: Haosheng He, Jianpeng Qi, Chao Liu, Junyu Dong, Yanwei Yu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18391)  

**Abstract**: In compute-first networking, maintaining fresh and accurate status information at the network edge is crucial for effective access to remote services. This process typically involves three phases: Status updating, user accessing, and user requesting. However, current studies on status effectiveness, such as Age of Information at Query (QAoI), do not comprehensively cover all these phases. Therefore, this paper introduces a novel metric, TPAoI, aimed at optimizing update decisions by measuring the freshness of service status. The stochastic nature of edge environments, characterized by unpredictable communication delays in updating, requesting, and user access times, poses a significant challenge when modeling. To address this, we model the problem as a Markov Decision Process (MDP) and employ a Dueling Double Deep Q-Network (D3QN) algorithm for optimization. Extensive experiments demonstrate that the proposed TPAoI metric effectively minimizes AoI, ensuring timely and reliable service updates in dynamic edge environments. Results indicate that TPAoI reduces AoI by an average of 47\% compared to QAoI metrics and decreases update frequency by an average of 48\% relative to conventional AoI metrics, showing significant improvement. 

**Abstract (ZH)**: 在计算优先网络中，保持网络边缘的最新和准确的服务状态信息对于有效访问远程服务至关重要。这一过程通常包括三个阶段：状态更新、用户访问和服务请求。然而，当前关于状态有效性的研究，如查询时的年龄信息（QAoI），并未全面涵盖所有这些阶段。因此，本文引入了一个新的度量标准——TPAoI（Transmission Priority Age of Information），旨在通过衡量服务状态的新鲜度来优化更新决策。边缘环境的随机性特性，表现为更新、请求和用户访问时间的不可预测通信延迟，给建模带来了重大挑战。为解决这一问题，我们将问题建模为马尔可夫决策过程（MDP），并采用了 Dueling Double Deep Q-Network（D3QN）算法来优化。广泛的经验研究表明，提出的TPAoI度量标准有效地减少了年龄信息（AoI），确保在动态边缘环境中实现及时和可靠的更新。结果表明，与QAoI指标相比，TPAoI平均减少了47%的AoI，与传统的AoI指标相比，平均减少了48%的更新频率，显示出显著的改进。 

---
# RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction 

**Title (ZH)**: RDPM：通过循环token预测解决扩散概率模型 

**Authors**: Wu Xiaoping, Hu Jie, Wei Xiaoming  

**Link**: [PDF](https://arxiv.org/pdf/2412.18390)  

**Abstract**: Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community. 

**Abstract (ZH)**: 扩散概率模型（DPMs）已经成为高保真图像合成的事实标准方法，它们通过在连续VAE潜在空间上进行扩散过程，与大型语言模型（LLMs）中使用的文本生成方法有显著区别。在本文中，我们提出了一种新颖的生成框架——循环扩散概率模型（RDPM），通过循环的标记预测机制增强扩散过程，从而引领了离散扩散领域的研究。通过逐步向图像的潜在表示中引入高斯噪声，并以循环的方式将其编码为矢量量化标记，RDPM 实现了一种独特的离散值域上的扩散过程。该过程迭代地预测后续时间步的标记代码，将初始的标准高斯噪声转换为目标数据分布，这与GPT风格的模型在损失函数方面具有相似性。RDPM在保持快速推理步骤优势的同时，还表现出更好的性能。该模型不仅利用了扩散过程来确保高质量的生成，还将连续信号转换为一系列高保真度的离散标记，从而与文本等其他离散标记保持一致的优化策略。我们期待这项工作能够促进多模态生成统一模型的发展，特别是在将图像、视频、音频等连续信号域与文本统一结合方面。我们将发布该模型的代码和模型权重以供开源社区使用。 

---
# ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots 

**Title (ZH)**: ChaI-TeA：基于LLM的聊天机器人交互自动补全评估基准 

**Authors**: Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdy, Nachshon Cohen, Alexander Libov, Guy Kushilevitz  

**Link**: [PDF](https://arxiv.org/pdf/2412.18377)  

**Abstract**: The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots. The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We introduce the task of chatbot interaction autocomplete. We present ChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, coupled with suitable datasets and metrics. We use the framework to evaluate After formally defining the task along with suitable datasets and metrics, we test 9 models on the defined auto completion task, finding that while current off-the-shelf models perform fairly, there is still much room for improvement, mainly in ranking of the generated suggestions. We provide insights for practitioners working on this task and open new research directions for researchers in the field. We release our framework to serve as a foundation for future research. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的兴起已经越来越多地将人类计算机交互转向基于LLM的聊天机器人。这些模型的显著能力使得用户能够使用包含广泛主题和风格的长篇多样自然语言文本进行交互。撰写这些消息是一个耗时耗力的过程，因此需要一个自动补全文本解决方案来辅助用户。我们引入了聊天机器人交互自动补全这一任务，并提出了ChaI-TeA：聊天交互自动补全；一个用于基于LLM的聊天机器人交互的自动补全评估框架。该框架包括对任务的正式定义，以及相应的数据集和评估指标。我们使用此框架来评估九种模型在定义的自动补全任务上的表现，发现尽管现成的模型表现不错，但在生成建议的排名方面仍有很大的改进空间。我们为从事该任务的实践者提供了见解，并为该领域的研究人员开拓了新的研究方向。我们发布了该框架，为未来的研究提供基础。 

---
# A Many Objective Problem Where Crossover is Provably Indispensable 

**Title (ZH)**: 一个需证明交叉操作必不可少的多目标优化问题 

**Authors**: Andre Opris  

**Link**: [PDF](https://arxiv.org/pdf/2412.18375)  

**Abstract**: This paper addresses theory in evolutionary multiobjective optimisation (EMO) and focuses on the role of crossover operators in many-objective optimisation. The advantages of using crossover are hardly understood and rigorous runtime analyses with crossover are lagging far behind its use in practice, specifically in the case of more than two objectives. We present a many-objective problem class together with a theoretical runtime analysis of the widely used NSGA-III to demonstrate that crossover can yield an exponential speedup on the runtime. In particular, this algorithm can find the Pareto set in expected polynomial time when using crossover while without crossover it requires exponential time to even find a single Pareto-optimal point. To our knowledge, this is the first rigorous runtime analysis in many-objective optimisation demonstrating an exponential performance gap when using crossover for more than two objectives. 

**Abstract (ZH)**: 本文探讨了进化多目标优化（EMO）中的理论问题，并重点讨论了在多目标优化中交叉操作的作用。交叉操作的优势还未得到充分理解，而且在交叉操作的严谨运行时分析方面远远落后于其在实践中使用的情况，特别是在多于两个目标的情况下。我们提出了一类多目标问题，并对广泛使用的NSGA-III进行了理论上的运行时分析，以证明交叉操作可以在运行时带来指数级的速度提高。具体来说，当使用交叉操作时，该算法可以在预期多项式时间内找到帕累托前沿，而如果不使用交叉操作，则需要指数时间才能找到单个帕累托最优解点。据我们所知，这是首次在多目标优化中进行严谨的运行时分析，证明了在多于两个目标的情况下使用交叉操作的指数级性能差距。 

---
# Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks against GNN-Based Fraud Detectors 

**Title (ZH)**: 揭示欺诈团伙对图神经网络的威胁：针对基于图神经网络的欺诈检测器的多目标图注入攻击 

**Authors**: Jinhyeok Choi, Heehyeon Kim, Joyce Jiyoung Whang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18370)  

**Abstract**: Graph neural networks (GNNs) have emerged as an effective tool for fraud detection, identifying fraudulent users, and uncovering malicious behaviors. However, attacks against GNN-based fraud detectors and their risks have rarely been studied, thereby leaving potential threats unaddressed. Recent findings suggest that frauds are increasingly organized as gangs or groups. In this work, we design attack scenarios where fraud gangs aim to make their fraud nodes misclassified as benign by camouflaging their illicit activities in collusion. Based on these scenarios, we study adversarial attacks against GNN-based fraud detectors by simulating attacks of fraud gangs in three real-world fraud cases: spam reviews, fake news, and medical insurance frauds. We define these attacks as multi-target graph injection attacks and propose MonTi, a transformer-based Multi-target one-Time graph injection attack model. MonTi simultaneously generates attributes and edges of all attack nodes with a transformer encoder, capturing interdependencies between attributes and edges more effectively than most existing graph injection attack methods that generate these elements sequentially. Additionally, MonTi adaptively allocates the degree budget for each attack node to explore diverse injection structures involving target, candidate, and attack nodes, unlike existing methods that fix the degree budget across all attack nodes. Experiments show that MonTi outperforms the state-of-the-art graph injection attack methods on five real-world graphs. 

**Abstract (ZH)**: 基于图的神经网络（GNNs）已成为欺诈检测、识别欺诈用户和揭示恶意行为的有效工具。然而，针对基于GNN的欺诈检测器的攻击及其风险的研究很少，从而留下了潜在威胁未被解决的问题。最近的研究发现，欺诈行为正越来越多地组织成团伙或团队。在本工作中，我们设计了一种攻击场景，其中欺诈团伙的目标是通过合谋隐藏其非法活动，使其欺诈节点被误分类为良性。基于这些场景，我们通过模拟欺诈团伙在三个真实世界欺诈案例中的攻击，研究了针对基于GNN的欺诈检测器的对抗攻击。我们定义这些攻击为多目标图注入攻击，并提出了一种基于转换器的Multi-target One-Time 图注入攻击模型 MonTi。MonTi 同时生成所有攻击节点的属性和边，通过转换器编码器更有效地捕捉属性和边之间的相互依赖关系，这优于大多数现有的按照顺序生成这些元素的图注入攻击方法。此外，MonTi 为每个攻击节点自适应分配度预算，以探索涉及目标节点、候选节点和攻击节点的多种注入结构，而现有方法在所有攻击节点上固定度预算。实验结果显示，MonTi 在五个真实世界的图上优于最先进的图注入攻击方法。 

---
# Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges 

**Title (ZH)**: 通过向精英超边注入同质节点来实施超图攻击 

**Authors**: Meixia He, Peican Zhu, Keke Tang, Yangming Guo  

**Link**: [PDF](https://arxiv.org/pdf/2412.18365)  

**Abstract**: Recent studies have shown that Hypergraph Neural Networks (HGNNs) are vulnerable to adversarial attacks. Existing approaches focus on hypergraph modification attacks guided by gradients, overlooking node spanning in the hypergraph and the group identity of hyperedges, thereby resulting in limited attack performance and detectable attacks. In this manuscript, we present a novel framework, i.e., Hypergraph Attacks via Injecting Homogeneous Nodes into Elite Hyperedges (IE-Attack), to tackle these challenges. Initially, utilizing the node spanning in the hypergraph, we propose the elite hyperedges sampler to identify hyperedges to be injected. Subsequently, a node generator utilizing Kernel Density Estimation (KDE) is proposed to generate the homogeneous node with the group identity of hyperedges. Finally, by injecting the homogeneous node into elite hyperedges, IE-Attack improves the attack performance and enhances the imperceptibility of attacks. Extensive experiments are conducted on five authentic datasets to validate the effectiveness of IE-Attack and the corresponding superiority to state-of-the-art methods. 

**Abstract (ZH)**: 近年来的研究表明，超图神经网络（Hypergraph Neural Networks，HGNNs）容易受到对抗攻击。现有的方法主要关注由梯度引导的超图修改攻击，忽视了超图中的节点覆盖以及超边组身份，从而导致攻击性能有限且可被检测。本文提出了一种新的框架，即通过向精英超边注入同质节点的方法（IE-Attack），以应对这些挑战。首先，利用超图中的节点覆盖，我们提出了精英超边抽样方法来识别要注入的超边。随后，提出了一种利用核密度估计（KDE）的节点生成器，以生成具有超边组身份的同质节点。最后，通过向精英超边注入同质节点，IE-Attack提升了攻击性能并增强了攻击的不可感知性。在五个真实数据集上进行了广泛的实验，验证了IE-Attack的有效性及其相对于现有最先进的方法的优势。 

---
# Point-DeepONet: A Deep Operator Network Integrating PointNet for Nonlinear Analysis of Non-Parametric 3D Geometries and Load Conditions 

**Title (ZH)**: 点DeepONet：一种结合点网的深度算子网络，用于非参数3D几何和载荷条件的非线性分析 

**Authors**: Jangseop Park, Namwoo Kang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18362)  

**Abstract**: Nonlinear structural analyses in engineering often require extensive finite element simulations, limiting their applicability in design optimization, uncertainty quantification, and real-time control. Conventional deep learning surrogates, such as convolutional neural networks (CNNs), physics-informed neural networks (PINNs), and fourier neural operators (FNOs), face challenges with complex non-parametric three-dimensional (3D) geometries, directionally varying loads, and high-fidelity predictions on unstructured meshes. This work presents Point-DeepONet, an operator-learning-based surrogate that integrates PointNet into the DeepONet framework. By directly processing non-parametric point clouds and incorporating signed distance functions (SDF) for geometric context, Point-DeepONet accurately predicts three-dimensional displacement and von Mises stress fields without mesh parameterization or retraining. Trained using only about 5,000 nodes (2.5% of the original 200,000-node mesh), Point-DeepONet can still predict the entire mesh at high fidelity, achieving a coefficient of determination reaching 0.987 for displacement and 0.923 for von Mises stress under a horizontal load case. Compared to nonlinear finite element analyses that require about 19.32 minutes per case, Point-DeepONet provides predictions in mere seconds-approximately 400 times faster-while maintaining excellent scalability and accuracy with increasing dataset sizes. These findings highlight the potential of Point-DeepONet to enable rapid, high-fidelity structural analyses, ultimately supporting more effective design exploration and informed decision-making in complex engineering workflows. 

**Abstract (ZH)**: 工程中的非线性结构分析通常需要大量的有限元仿真，这限制了其在设计优化、不确定性量化和实时控制中的应用。传统的深层学习代理模型，如卷积神经网络（CNNs）、物理指导的神经网络（PINNs）和傅里叶神经算子（FNOs），在处理复杂非参数三维（3D）几何形状、方向性变化的载荷以及非结构化网格上的高保真预测时面临挑战。本文提出了基于算子学习的代理模型——Point-DeepONet，该模型将PointNet集成到DeepONet框架中。通过直接处理非参数点云并结合符号距离函数（SDF）进行几何上下文的整合，Point-DeepONet能够在无需网格参数化或重新训练的情况下准确预测三维位移和von Mises应力场。仅使用约5,000个节点（原20万个节点网格的2.5%），Point-DeepONet仍能以高保真度预测整个网格，在水平载荷条件下，位移的决定系数可达到0.987，von Mises应力的决定系数可达到0.923。相比之下，非线性有限元分析每例需要大约19.32分钟，而Point-DeepONet只需几秒钟即可提供预测，大约快400倍。同时，该模型在数据集规模增加时保持了出色的可扩展性和准确性。这些发现突显了Point-DeepONet在实现快速、高保真结构分析方面的潜力，从而支持在复杂工程流程中更有效的设计探索和基于信息的决策制定。 

---
# Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor 

**Title (ZH)**: 通过尾部锚点解决联邦持续学习中的空间-时间数据异质性 

**Authors**: Hao Yu, Xin Yang, Le Zhang, Hanlin Gu, Tianrui Li, Lixin Fan, Qiang Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18355)  

**Abstract**: Federated continual learning (FCL) allows each client to continually update its knowledge from task streams, enhancing the applicability of federated learning in real-world scenarios. However, FCL needs to address not only spatial data heterogeneity between clients but also temporal data heterogeneity between tasks. In this paper, empirical experiments demonstrate that such input-level heterogeneity significantly affects the model's internal parameters and outputs, leading to severe spatial-temporal catastrophic forgetting of local and previous knowledge. To this end, we propose Federated Tail Anchor (FedTA) to mix trainable Tail Anchor with the frozen output features to adjust their position in the feature space, thereby overcoming parameter-forgetting and output-forgetting. Moreover, three novel components are also included in FedTA: Input Enhancement for improving the performance of pre-trained models on downstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous local knowledge on the server side; and Best Global Prototype Selection for finding the best anchor point for each class in the feature space. Extensive experiments demonstrate that FedTA not only outperforms existing FCL methods but also effectively preserves the relative positions of features, remaining unaffected by spatial and temporal changes. 

**Abstract (ZH)**: 联邦持续学习（Federated Continual Learning, FCL）允许每个客户端不断从任务流中更新其知识，从而增强了联邦学习在现实世界场景中的适用性。然而，FCL 需要解决客户端之间的空间数据异质性和任务之间的时间数据异质性。本文通过实证实验展示了这种输入级异质性对模型的内部参数和输出产生了显著影响，导致局部和先前知识的严重空间-时间灾难性遗忘。为此，我们提出了一种名为 Federated Tail Anchor（FedTA）的方法，通过将可训练的 Tail Anchor 与冻结的输出特征混合，来调整它们在特征空间中的位置，从而克服参数遗忘和输出遗忘。此外，FedTA 还包含了三个新颖的组件：输入增强，以提高预训练模型在下游任务中的性能；选择性输入知识融合，在服务器端融合异质的本地知识；以及最佳全局原型选择，在特征空间中为每个类别找到最佳锚点。广泛的实验证明，FedTA 不仅优于现有的 FCL 方法，而且能够有效保持特征相对位置不变，不受空间和时间变化的影响。 

---
# Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering 

**Title (ZH)**: 基于大型语言模型的多agents知识驱动视觉问答系统 

**Authors**: Zhongjian Hu, Peng Yang, Bing Li, Zhenqi Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18351)  

**Abstract**: Large Language Models (LLMs) have achieved impressive results in knowledge-based Visual Question Answering (VQA). However existing methods still have challenges: the inability to use external tools autonomously, and the inability to work in teams. Humans tend to know whether they need to use external tools when they encounter a new question, e.g., they tend to be able to give a direct answer to a familiar question, whereas they tend to use tools such as search engines when they encounter an unfamiliar question. In addition, humans also tend to collaborate and discuss with others to get better answers. Inspired by this, we propose the multi-agent voting framework. We design three LLM-based agents that simulate different levels of staff in a team, and assign the available tools according to the levels. Each agent provides the corresponding answer, and finally all the answers provided by the agents are voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our approach outperforms other baselines by 2.2 and 1.0, respectively. 

**Abstract (ZH)**: 大型语言模型（LLMs）在基于知识的视觉问答（VQA）任务上取得了显著成果。然而，现有方法仍然存在一些挑战：无法自主使用外部工具，以及无法协同工作。人类在遇到新的问题时往往能够判断是否需要使用外部工具，例如，他们通常能够直接回答熟悉的问题，而在面对不熟悉的问题时则倾向于使用搜索引擎等工具。此外，人类还倾向于与他人合作和讨论以获得更好的答案。受此启发，我们提出了多智能体投票框架。我们设计了三种基于LLM的智能体，模拟团队中不同级别的员工，并根据级别分配可用工具。每个智能体提供相应的答案，最终通过投票汇总所有智能体提供的答案以得到最终答案。在OK-VQA和A-OKVQA数据集上的实验表明，我们的方法分别比其他基线方法提高了2.2和1.0的性能。 

---
# The Value of AI-Generated Metadata for UGC Platforms: Evidence from a Large-scale Field Experiment 

**Title (ZH)**: AI生成元数据在UGC平台的价值：来自大规模现场实验的证据 

**Authors**: Xinyi Zhang, Chenshuo Sun, Renyu Zhang, Khim-Yong Goh  

**Link**: [PDF](https://arxiv.org/pdf/2412.18337)  

**Abstract**: AI-generated content (AIGC), such as advertisement copy, product descriptions, and social media posts, is becoming ubiquitous in business practices. However, the value of AI-generated metadata, such as titles, remains unclear on user-generated content (UGC) platforms. To address this gap, we conducted a large-scale field experiment on a leading short-video platform in Asia to provide about 1 million users access to AI-generated titles for their uploaded videos. Our findings show that the provision of AI-generated titles significantly boosted content consumption, increasing valid watches by 1.6% and watch duration by 0.9%. When producers adopted these titles, these increases jumped to 7.1% and 4.1%, respectively. This viewership-boost effect was largely attributed to the use of this generative AI (GAI) tool increasing the likelihood of videos having a title by 41.4%. The effect was more pronounced for groups more affected by metadata sparsity. Mechanism analysis revealed that AI-generated metadata improved user-video matching accuracy in the platform's recommender system. Interestingly, for a video for which the producer would have posted a title anyway, adopting the AI-generated title decreased its viewership on average, implying that AI-generated titles may be of lower quality than human-generated ones. However, when producers chose to co-create with GAI and significantly revised the AI-generated titles, the videos outperformed their counterparts with either fully AI-generated or human-generated titles, showcasing the benefits of human-AI co-creation. This study highlights the value of AI-generated metadata and human-AI metadata co-creation in enhancing user-content matching and content consumption for UGC platforms. 

**Abstract (ZH)**: 人工智能生成内容（AIGC），包括广告文案、产品描述和社交媒体帖子，正在业务实践中变得无处不在。然而，关于用户生成内容（UGC）平台上的人工智能生成元数据（如标题）的价值仍然不明确。为了解决这一问题，我们在亚洲一家领先的短视频平台进行了大规模现场实验，为约100万用户提供了使用人工智能生成的标题的机会，用于他们的上传视频。我们的研究结果表明，提供人工智能生成的标题显著提高了内容消费，有效观看率增加了1.6%，观看时间增加了0.9%。当生产者采用这些标题时，这些增加分别达到了7.1%和4.1%。这种观众增长效应主要归因于使用这种生成式人工智能（GAI）工具使得视频具有标题的可能性增加了41.4%。这一效应在因元数据稀疏而受到影响更大的群体中更为明显。机制分析表明，人工智能生成的元数据提高了平台推荐系统中的用户-视频匹配准确性。有趣的是，对于原本生产者也会为视频添加标题的情况，采用人工智能生成的标题会降低其平均观看量，这表明人工智能生成的标题可能不如人工生成的质量高。然而，当生产者选择与GAI共创并大幅修订人工智能生成的标题时，视频的表现优于完全由人工智能或人工生成标题的情况，展示了人类与人工智能共创的好处。这项研究强调了人工智能生成元数据和人类与人工智能共创元数据在增强UGC平台上用户内容匹配以及内容消费方面的价值。 

---
# FloNa: Floor Plan Guided Embodied Visual Navigation 

**Title (ZH)**: FloNa：基于平面图的实体视觉导航 

**Authors**: Jiaxin Li, Weiqi Huang, Zan Wang, Wei Liang, Huijun Di, Feng Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18335)  

**Abstract**: Humans naturally rely on floor plans to navigate in unfamiliar environments, as they are readily available, reliable, and provide rich geometrical guidance. However, existing visual navigation settings overlook this valuable prior knowledge, leading to limited efficiency and accuracy. To eliminate this gap, we introduce a novel navigation task: Floor Plan Visual Navigation (FloNa), the first attempt to incorporate floor plan into embodied visual navigation. While the floor plan offers significant advantages, two key challenges emerge: (1) handling the spatial inconsistency between the floor plan and the actual scene layout for collision-free navigation, and (2) aligning observed images with the floor plan sketch despite their distinct modalities. To address these challenges, we propose FloDiff, a novel diffusion policy framework incorporating a localization module to facilitate alignment between the current observation and the floor plan. We further collect $20k$ navigation episodes across $117$ scenes in the iGibson simulator to support the training and evaluation. Extensive experiments demonstrate the effectiveness and efficiency of our framework in unfamiliar scenes using floor plan knowledge. Project website: this https URL. 

**Abstract (ZH)**: 人类自然依靠平面图在不熟悉环境中导航，因为平面图易于获得、可靠且提供了丰富的几何指导。然而，现有的视觉导航设置忽视了这一宝贵的知识，导致导航效率和准确性有限。为了解决这一差距，我们提出了一种新的导航任务：平面图视觉导航（FloNa），这是首次尝试将平面图纳入嵌入式视觉导航中。虽然平面图提供了显著的优势，但两个关键挑战也随之出现：(1) 处理平面图与实际场景布局之间的空间不一致性，以实现无碰撞导航；(2) 尽管它们的模态不同，仍需对观察到的图像与平面图草图进行对齐。为了应对这些挑战，我们提出了FloDiff，这是一种新颖的扩散策略框架，包含一个定位模块，以促进当前观察与平面图之间的对齐。此外，我们在iGibson模拟器中收集了跨越117个场景的20,000个导航样本，用于训练和评估。大量实验表明，利用平面图知识，我们的框架在不熟悉场景中的有效性和效率。项目网站：[此处链接](this https URL)。 

---
# Exploring Graph Mamba: A Comprehensive Survey on State-Space Models for Graph Learning 

**Title (ZH)**: 探索图马兰花：关于图学习中状态空间模型的全面综述 

**Authors**: Safa Ben Atitallah, Chaima Ben Rabah, Maha Driss, Wadii Boulila, Anis Koubaa  

**Link**: [PDF](https://arxiv.org/pdf/2412.18322)  

**Abstract**: Graph Mamba, a powerful graph embedding technique, has emerged as a cornerstone in various domains, including bioinformatics, social networks, and recommendation systems. This survey represents the first comprehensive study devoted to Graph Mamba, to address the critical gaps in understanding its applications, challenges, and future potential. We start by offering a detailed explanation of the original Graph Mamba architecture, highlighting its key components and underlying mechanisms. Subsequently, we explore the most recent modifications and enhancements proposed to improve its performance and applicability. To demonstrate the versatility of Graph Mamba, we examine its applications across diverse domains. A comparative analysis of Graph Mamba and its variants is conducted to shed light on their unique characteristics and potential use cases. Furthermore, we identify potential areas where Graph Mamba can be applied in the future, highlighting its potential to revolutionize data analysis in these fields. Finally, we address the current limitations and open research questions associated with Graph Mamba. By acknowledging these challenges, we aim to stimulate further research and development in this promising area. This survey serves as a valuable resource for both newcomers and experienced researchers seeking to understand and leverage the power of Graph Mamba. 

**Abstract (ZH)**: Graph Mamba，一种强大的图嵌入技术，在生物信息学、社交网络和推荐系统等多个领域中逐渐成为关键基石。本综述是首个针对Graph Mamba进行的全面研究，旨在填补对其应用、挑战及其未来潜力理解的空白。首先，我们详细介绍了原始的Graph Mamba架构，突显其关键组件和运行机制。随后，我们探讨了为提高其性能和适用性所提出的最新改进和增强措施。为了展示Graph Mamba的灵活性，我们考察了它在不同领域的应用。我们还对Graph Mamba及其变体进行了比较分析，以揭示它们的独特特性和潜在应用场景。此外，我们确定了Graph Mamba未来应用的潜在领域，强调其在这些领域的数据分析方面的潜在革命性影响。最后，我们讨论了Graph Mamba当前的限制和现有研究中的开放问题。通过认识到这些挑战，我们旨在激发对该有前途领域进一步研究和开发的兴趣。本综述为希望了解并利用Graph Mamba力量的新手和有经验的研究人员提供了宝贵资源。 

---
# Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search 

**Title (ZH)**: 桑树：通过集体蒙特卡洛树搜索增强MLLM的o1-like推理与反思能力 

**Authors**: Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao  

**Link**: [PDF](https://arxiv.org/pdf/2412.18319)  

**Abstract**: In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code will be available at this https URL 

**Abstract (ZH)**: 在本文中，我们旨在开发一种多模态逻辑学习模型（MLLM），该模型通过学习生成推理过程中的每个中间步骤直至最终答案来理解和解决问题。为此，我们提出了集体蒙特卡罗树搜索（CoMCTS），这是一种针对MLLM的新学习推理方法，它将集体学习的概念引入“树搜索”中，以实现有效的推理路径搜索和学习。CoMCTS的核心思想是利用多个模型的集体知识，通过扩展、模拟与错误定位、反向传播和选择四种迭代操作，协同猜测、搜索并识别通向正确答案的有效推理路径。

基于CoMCTS，我们构建了Mulberry-260k数据集，它为每个问题提供了一棵包含丰富、明确定义的推理节点的树。利用Mulberry-260k，我们进行了集体强化学习（SFT）以训练我们的模型Mulberry，该模型是一系列具有逐步推理与反思能力的MLLM。广泛的经验表明，我们的方法在各种基准测试中表现出优越性。代码将在以下链接处提供：[指定链接] 

---
# Data-Driven Self-Supervised Graph Representation Learning 

**Title (ZH)**: 数据驱动的自监督图表示学习 

**Authors**: Ahmed E. Samy, Zekarias T. Kefatoa, Sarunas Girdzijauskasa  

**Link**: [PDF](https://arxiv.org/pdf/2412.18316)  

**Abstract**: Self-supervised graph representation learning (SSGRL) is a representation learning paradigm used to reduce or avoid manual labeling. An essential part of SSGRL is graph data augmentation. Existing methods usually rely on heuristics commonly identified through trial and error and are effective only within some application domains. Also, it is not clear why one heuristic is better than another. Moreover, recent studies have argued against some techniques (e.g., dropout: that can change the properties of molecular graphs or destroy relevant signals for graph-based document classification tasks).
In this study, we propose a novel data-driven SSGRL approach that automatically learns a suitable graph augmentation from the signal encoded in the graph (i.e., the nodes' predictive feature and topological information). We propose two complementary approaches that produce learnable feature and topological augmentations. The former learns multi-view augmentation of node features, and the latter learns a high-order view of the topology. Moreover, the augmentations are jointly learned with the representation. Our approach is general that it can be applied to homogeneous and heterogeneous graphs. We perform extensive experiments on node classification (using nine homogeneous and heterogeneous datasets) and graph property prediction (using another eight datasets). The results show that the proposed method matches or outperforms the SOTA SSGRL baselines and performs similarly to semi-supervised methods. The anonymised source code is available at this https URL 

**Abstract (ZH)**: 自我监督图表示学习（Self-supervised Graph Representation Learning, SSGRL）是一种用于减少或避免手动标注的表示学习方法。SSGRL的关键部分是图数据增强。现有的方法通常依赖于通过试验和错误发现的一些启发式方法，并且其有效性仅限于某些应用领域。此外，并不清楚为什么某种启发式方法优于另一种方法。最近的一些研究表明，某些技术（如改变分子图的属性或破坏基于图的文档分类任务相关信号的dropout）是无效的。

在本研究中，我们提出了一种基于数据的SSGRL方法，该方法能够自动从图中编码的信号（即节点的预测特征和拓扑信息）中学习合适的图增强。我们提出了两种互补的方法来生成可学习的特征增强和拓扑增强。前者学习节点特征的多视图增强，而后者学习拓扑的高阶视图。此外，这些增强是与表示共同学习的。我们的方法具有普适性，可以应用于同质图和异质图。我们在节点分类（使用九个同质和异质数据集）和图属性预测（使用额外的八个数据集）上进行了广泛的实验。结果表明，所提出的方法与最新的SSGRL基线方法相当或优于之，并且在半监督方法上表现相似。匿名的源代码可在以下链接获取：[链接] 

---
# M-Ped: Multi-Prompt Ensemble Decoding for Large Language Models 

**Title (ZH)**: M-Ped：面向大型语言模型的多提示ensemble解码方法 

**Authors**: Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Shimin Tao, Hengchao Shang, Zongyao Li, Shaojun Li, Jinlong Yang, Zhanglin Wu, Zhiqiang Rao, Hao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18299)  

**Abstract**: With the widespread application of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), enhancing their performance has become a research hotspot. This paper presents a novel multi-prompt ensemble decoding approach designed to bolster the generation quality of LLMs by leveraging the aggregation of outcomes from multiple prompts. Given a unique input $X$, we submit $n$ variations of prompts with $X$ to LLMs in batch mode to decode and derive probability distributions. For each token prediction, we calculate the ensemble probability by averaging the $n$ probability distributions within the batch, utilizing this aggregated probability to generate the token. This technique is dubbed Inner-Batch Ensemble. To facilitate efficient batch inference, we implement a Left-Padding strategy to maintain uniform input lengths across the n prompts. Through extensive experimentation on diverse NLP tasks, including machine translation, code generation, and text simplification, we demonstrate the efficacy of our method in enhancing LLM performance. The results show substantial improvements in BLEU scores, pass@$k$ rates, and LENS metrics over conventional methods. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）在自然语言处理（NLP）领域的广泛应用，提高其性能已成为研究热点。本文提出了一种新颖的多提示集束解码方法，旨在通过综合多个提示的结果来增强LLMs的生成质量。对于给定的输入 \(X\)，我们提交 \(n\) 个提示 \(X\) 的变体批次给LLMs，以进行解码并获得概率分布。对于每个令牌预测，我们通过计算批次内的 \(n\) 个概率分布的平均值来计算集束概率，并利用该聚合概率生成令牌。这种方法被称为内批次集束。为了实现高效的批次推理，我们实施了左填充策略，以确保批次中 \(n\) 个提示的输入长度一致。通过在包括机器翻译、代码生成和文本简化等不同NLP任务上的广泛实验，我们展示了该方法在增强LLMs性能方面的有效性。实验结果表明，该方法在BLEU分数、pass@k率和LENS指标上相较于传统方法有显著改进。 

---
# Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight 

**Title (ZH)**: 《 anomaly detection 前途如何？LLMs 和 VLMs 走入聚光灯下 》

注：这里的翻译尽量保持了原文的关键信息和学术风格。"Quo Vadis" 是一个拉丁语短语，意为“你要去哪里？”在学术论文标题中常用来表示对未来方向的探讨。因此，翻译时保留了这种提问的意味。"Anomaly Detection" 是指异常检测，"LLMs" 是大语言模型（Large Language Models）的缩写，"VLMs" 是多模态视觉语言模型（Vision-Language Models）的缩写。希望这个翻译能够符合你的需求。 

**Authors**: Xi Ding, Lei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18298)  

**Abstract**: Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection. 

**Abstract (ZH)**: 视频异常检测（VAD）通过结合大型语言模型（LLMs）和视觉语言模型（VLMs）取得了显著进展，有效解决了判别解释性、时间推理以及动态开放场景中的泛化等问题。本文对2024年的基于LLM/VLM的前沿方法进行了深入总结，重点关注四个关键方面：（i）通过语义洞察和文本解释增强判别解释性，使视觉异常更易理解；（ii）捕捉复杂的时序关系以检测和定位视频帧中的动态异常；（iii）实现少量样本和零样本检测，减少对大型标注数据集的依赖；（iv）利用语义理解和运动特征处理开放世界和无类别异常，以实现时空一致性。我们强调这些方法在重新定义VAD领域的潜力。此外，本文还探讨了LLMs和VLMs提供的视觉和文本模态之间的协同作用，突出了它们的综合优势，并提出了未来可能的研究方向，以充分发挥这些模型在提升视频异常检测性能方面的潜力。 

---
# Navigating Data Corruption in Machine Learning: Balancing Quality, Quantity, and Imputation Strategies 

**Title (ZH)**: 导航数据 corrupted 在机器学习中的挑战：平衡质量、数量和插补策略 

**Authors**: Qi Liu, Wanjing Ma  

**Link**: [PDF](https://arxiv.org/pdf/2412.18296)  

**Abstract**: Data corruption, including missing and noisy data, poses significant challenges in real-world machine learning. This study investigates the effects of data corruption on model performance and explores strategies to mitigate these effects through two experimental setups: supervised learning with NLP tasks (NLP-SL) and deep reinforcement learning for traffic signal optimization (Signal-RL). We analyze the relationship between data corruption levels and model performance, evaluate the effectiveness of data imputation methods, and assess the utility of enlarging datasets to address data corruption.
Our results show that model performance under data corruption follows a diminishing return curve, modeled by the exponential function. Missing data, while detrimental, is less harmful than noisy data, which causes severe performance degradation and training instability, particularly in sequential decision-making tasks like Signal-RL. Imputation strategies involve a trade-off: they recover missing information but may introduce noise. Their effectiveness depends on imputation accuracy and corruption ratio. We identify distinct regions in the imputation advantage heatmap, including an "imputation advantageous corner" and an "imputation disadvantageous edge" and classify tasks as "noise-sensitive" or "noise-insensitive" based on their decision boundaries.
Furthermore, we find that increasing dataset size mitigates but cannot fully overcome the effects of data corruption. The marginal utility of additional data diminishes as corruption increases. An empirical rule emerges: approximately 30% of the data is critical for determining performance, while the remaining 70% has minimal impact.
These findings provide actionable insights into data preprocessing, imputation strategies, and data collection practices, guiding the development of robust machine learning systems in noisy environments. 

**Abstract (ZH)**: 数据损坏，包括缺失和噪声数据，对现实世界的机器学习构成了重大挑战。本研究探讨了数据损坏对模型性能的影响，并通过两种实验设置来探索减轻这些影响的策略：自然语言处理（NLP）任务的监督学习（NLP-SL）和交通信号优化的深度强化学习（Signal-RL）。我们分析了数据损坏程度与模型性能之间的关系，评估了数据补全方法的有效性，并评估了通过扩大数据集来应对数据损坏的实用性。

研究结果表明，在数据损坏情况下模型性能呈现出递减曲线，可以用指数函数来建模。缺失数据虽然有害，但其危害性小于噪声数据，后者会引发严重的性能下降和训练不稳定，特别是在Sequential Decision-Making任务（如Signal-RL）中。补全策略存在权衡：它们可以恢复缺失信息，但可能引入噪声。这些方法的有效性取决于补全的准确性以及损坏的数据比例。我们确定了补全优势热图中的不同区域，包括一个“补全有利角”和一个“补全不利边缘”，并将任务分类为“噪声敏感”或“噪声不敏感”依据其决策边界。

此外，我们发现增加数据集大小可以减轻但无法完全克服数据损坏的影响。随着损坏程度的增加，额外数据的边际效益逐渐减弱。一个经验规则浮现：大约30%的数据对于确定性能至关重要，而余下的70%数据的影响可忽略不计。

这些发现为数据预处理、补全策略和数据收集实践提供了实际指导，有助于在嘈杂环境中构建稳健的机器学习系统。 

---
# DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation 

**Title (ZH)**: DeepCRCEval: 重新审视代码审查评论生成的评估方法 

**Authors**: Junyi Lu, Xiaojia Li, Zihan Hua, Lei Yu, Shiqi Cheng, Li Yang, Fengjun Zhang, Chun Zuo  

**Link**: [PDF](https://arxiv.org/pdf/2412.18291)  

**Abstract**: Code review is a vital but demanding aspect of software development, generating significant interest in automating review comments. Traditional evaluation methods for these comments, primarily based on text similarity, face two major challenges: inconsistent reliability of human-authored comments in open-source projects and the weak correlation of text similarity with objectives like enhancing code quality and detecting defects.
This study empirically analyzes benchmark comments using a novel set of criteria informed by prior research and developer interviews. We then similarly revisit the evaluation of existing methodologies. Our evaluation framework, DeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a comprehensive reassessment of current techniques based on the criteria set. Besides, we also introduce an innovative and efficient baseline, LLM-Reviewer, leveraging the few-shot learning capabilities of LLMs for a target-oriented comparison.
Our research highlights the limitations of text similarity metrics, finding that less than 10% of benchmark comments are high quality for automation. In contrast, DeepCRCEval effectively distinguishes between high and low-quality comments, proving to be a more reliable evaluation mechanism. Incorporating LLM evaluators into DeepCRCEval significantly boosts efficiency, reducing time and cost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates significant potential of focusing task real targets in comment generation. 

**Abstract (ZH)**: 代码审查是软件开发中至关重要但又颇具挑战性的一项工作，已引起了大量关于自动化审查注释的兴趣。传统的注释评估方法主要基于文本相似性，存在两大问题：开源项目中人工编写的注释可靠性不一致以及文本相似性与提升代码质量、检测缺陷等目标之间的弱相关性。

本文通过一个由先前研究和开发者访谈启发的新颖标准，对基准注释进行了实证分析，并重新审视了现有方法的评估。我们构建了一个新的评估框架DeepCRCEval，该框架结合了人工评估者和大型语言模型（LLM），基于设定的标准对现有技术进行了全面的重新评估。此外，我们还引入了一个创新且高效的基准工具LLM-Reviewer，利用LLM的有限提示学习能力进行目标导向的比较。

研究结果揭示了文本相似性指标的局限性，发现少于10%的基准注释适合自动化。相比之下，DeepCRCEval能够有效地区分高质量和低质量的注释，证明了其作为更可靠的评估机制的能力。将LLM评估者纳入DeepCRCEval显著提高了效率，分别将时间和成本降低了88.78%和90.32%。此外，LLM-Reviewer展示了在注释生成任务中聚焦实际目标的巨大潜力。 

---
# Towards understanding how attention mechanism works in deep learning 

**Title (ZH)**: Towards 理解深度学习中注意机制的工作原理 

**Authors**: Tianyu Ruan, Shihua Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18288)  

**Abstract**: Attention mechanism has been extensively integrated within mainstream neural network architectures, such as Transformers and graph attention networks. Yet, its underlying working principles remain somewhat elusive. What is its essence? Are there any connections between it and traditional machine learning algorithms? In this study, we inspect the process of computing similarity using classic metrics and vector space properties in manifold learning, clustering, and supervised learning. We identify the key characteristics of similarity computation and information propagation in these methods and demonstrate that the self-attention mechanism in deep learning adheres to the same principles but operates more flexibly and adaptively. We decompose the self-attention mechanism into a learnable pseudo-metric function and an information propagation process based on similarity computation. We prove that the self-attention mechanism converges to a drift-diffusion process through continuous modeling provided the pseudo-metric is a transformation of a metric and certain reasonable assumptions hold. This equation could be transformed into a heat equation under a new metric. In addition, we give a first-order analysis of attention mechanism with a general pseudo-metric function. This study aids in understanding the effects and principle of attention mechanism through physical intuition. Finally, we propose a modified attention mechanism called metric-attention by leveraging the concept of metric learning to facilitate the ability to learn desired metrics more effectively. Experimental results demonstrate that it outperforms self-attention regarding training efficiency, accuracy, and robustness. 

**Abstract (ZH)**: 注意力机制已广泛整合到主流的神经网络架构中，如变换器（Transformers）和图注意力网络（Graph Attention Networks）中。然而，其内部工作机制仍然有些神秘。它的本质是什么？它与传统的机器学习算法之间是否存在联系？在本研究中，我们检查了使用经典度量和流形学习、聚类和监督学习中的向量空间性质来计算相似性的过程。我们确定了这些方法中相似性计算和信息传播的关键特征，并证明了深度学习中的自我注意力机制遵循相同的原理，但具有更高的灵活性和适应性。我们将自我注意力机制分解为一个可学习的伪度量函数和基于相似性计算的信息传播过程。我们证明，在伪度量是一种度量的变换且某些合理的假设成立的情况下，自我注意力机制通过连续模型可以收敛到漂移扩散过程。在这种情况下，该方程可以在新的度量下转化为热方程。此外，我们对一般伪度量函数下的注意力机制进行了零阶分析。这一研究有助于通过物理直觉了解注意力机制的影响和原理。最后，我们通过利用度量学习的概念提出了一种改进的注意力机制，称为度量注意力（metric-attention），以提高学习所需度量的能力。实验结果表明，它在训练效率、准确性和鲁棒性方面优于自我注意力机制。 

---
# Semi-supervised Credit Card Fraud Detection via Attribute-Driven Graph Representation 

**Title (ZH)**: 基于属性驱动的图表示的半监督信用卡欺诈检测 

**Authors**: Sheng Xiang, Mingzhi Zhu, Dawei Cheng, Enxia Li, Ruihui Zhao, Yi Ouyang, Ling Chen, Yefeng Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2412.18287)  

**Abstract**: Credit card fraud incurs a considerable cost for both cardholders and issuing banks. Contemporary methods apply machine learning-based classifiers to detect fraudulent behavior from labeled transaction records. But labeled data are usually a small proportion of billions of real transactions due to expensive labeling costs, which implies that they do not well exploit many natural features from unlabeled data. Therefore, we propose a semi-supervised graph neural network for fraud detection. Specifically, we leverage transaction records to construct a temporal transaction graph, which is composed of temporal transactions (nodes) and interactions (edges) among them. Then we pass messages among the nodes through a Gated Temporal Attention Network (GTAN) to learn the transaction representation. We further model the fraud patterns through risk propagation among transactions. The extensive experiments are conducted on a real-world transaction dataset and two publicly available fraud detection datasets. The result shows that our proposed method, namely GTAN, outperforms other state-of-the-art baselines on three fraud detection datasets. Semi-supervised experiments demonstrate the excellent fraud detection performance of our model with only a tiny proportion of labeled data. 

**Abstract (ZH)**: 信用卡欺诈会对持卡人和发卡银行造成显著的经济损失。当前的方法使用基于机器学习的分类器从标注的交易记录中检测欺诈行为。但由于标注数据通常占数十亿实际交易中的一小部分，这表明它们未能充分利用大量未标注数据中的自然特征。因此，我们提出了一种半监督图 neural network（神经网络）来解决欺诈检测问题。具体而言，我们利用交易记录构建了一个时间交易图，该图由时间交易（节点）及其之间的交互（边）组成。然后，我们通过门控时间注意力网络（GTAN）在节点之间传递信息，以学习交易表示。此外，我们通过交易间的风险传播建模欺诈模式。我们在一个真实世界的交易数据集以及两个公开可用的欺诈检测数据集上进行了广泛的实验。实验结果表明，我们所提出的方法，即GTAN，在三个欺诈检测数据集上均优于其他最新的基线方法。半监督实验进一步证明，即使只有少量标注数据，我们的模型也能展现出优秀的欺诈检测性能。 

---
# GenAI Content Detection Task 2: AI vs. Human -- Academic Essay Authenticity Challenge 

**Title (ZH)**: GenAI内容检测任务2：AI与人类之争——学术论文 authenticity 挑战 

**Authors**: Shammur Absar Chowdhury, Hind Almerekhi, Mucahid Kutlu, Kaan Efe Keles, Fatema Ahmad, Tasnim Mohiuddin, George Mikros, Firoj Alam  

**Link**: [PDF](https://arxiv.org/pdf/2412.18274)  

**Abstract**: This paper presents a comprehensive overview of the first edition of the Academic Essay Authenticity Challenge, organized as part of the GenAI Content Detection shared tasks collocated with COLING 2025. This challenge focuses on detecting machine-generated vs. human-authored essays for academic purposes. The task is defined as follows: "Given an essay, identify whether it is generated by a machine or authored by a human.'' The challenge involves two languages: English and Arabic. During the evaluation phase, 25 teams submitted systems for English and 21 teams for Arabic, reflecting substantial interest in the task. Finally, seven teams submitted system description papers. The majority of submissions utilized fine-tuned transformer-based models, with one team employing Large Language Models (LLMs) such as Llama 2 and Llama 3. This paper outlines the task formulation, details the dataset construction process, and explains the evaluation framework. Additionally, we present a summary of the approaches adopted by participating teams. Nearly all submitted systems outperformed the n-gram-based baseline, with the top-performing systems achieving F1 scores exceeding 0.98 for both languages, indicating significant progress in the detection of machine-generated text. 

**Abstract (ZH)**: 本文概述了在第26届国际计算语言学会议（COLING 2025）期间联合举办的学术作文真实性挑战赛的第一版。该挑战旨在识别学术目的下的机器生成作文与人类写作作文。任务定义如下：“给定一篇作文，判断它是机器生成的还是由人类撰写。”该挑战涉及两种语言：英语和阿拉伯语。在评估阶段，有25支队伍提交了英语系统的模型，21支队伍提交了阿拉伯语系统的模型，反映出这一任务引起了广泛的关注。最后，共有7支队伍提交了系统描述论文。大多数提交的系统在基于n-gram的基线模型上表现出色，最高分系统的F1分数均超过0.98，表明在识别机器生成文本方面取得了显著进步。本论文概述了任务的定义，详细介绍了数据集的构建过程，并解释了评估框架。此外，我们还呈现了参与队伍所采用方法的总结。 

---
# Sampling Bag of Views for Open-Vocabulary Object Detection 

**Title (ZH)**: 开放词汇量物体检测中的视图采样 bag of views 方法 

**Authors**: Hojun Choi, Junsuk Choe, Hyunjung Shim  

**Link**: [PDF](https://arxiv.org/pdf/2412.18273)  

**Abstract**: Existing open-vocabulary object detection (OVD) develops methods for testing unseen categories by aligning object region embeddings with corresponding VLM features. A recent study leverages the idea that VLMs implicitly learn compositional structures of semantic concepts within the image. Instead of using an individual region embedding, it utilizes a bag of region embeddings as a new representation to incorporate compositional structures into the OVD task. However, this approach often fails to capture the contextual concepts of each region, leading to noisy compositional structures. This results in only marginal performance improvements and reduced efficiency. To address this, we propose a novel concept-based alignment method that samples a more powerful and efficient compositional structure. Our approach groups contextually related ``concepts'' into a bag and adjusts the scale of concepts within the bag for more effective embedding alignment. Combined with Faster R-CNN, our method achieves improvements of 2.6 box AP50 and 0.5 mask AP over prior work on novel categories in the open-vocabulary COCO and LVIS benchmarks. Furthermore, our method reduces CLIP computation in FLOPs by 80.3% compared to previous research, significantly enhancing efficiency. Experimental results demonstrate that the proposed method outperforms previous state-of-the-art models on the OVD datasets. 

**Abstract (ZH)**: 现有的开放词汇项物体检测(OVD)方法通过将物体区域嵌入与相应的视觉语言模型(VLM)特征对齐来测试未见过的类别。近期一项研究利用视觉语言模型隐含学习语义概念在图像中的组合结构的想法。这种方法不是使用单个区域嵌入，而是利用区域嵌入的集合作为新的表示，以将组合结构整合到OVD任务中。然而，这种方法往往难以捕捉每个区域的上下文概念，导致组合结构出现噪声，从而仅有微小的性能提升和降低的效率。为解决这一问题，我们提出了一种新的基于概念的对齐方法，可以抽取更强大和高效的组合结构。我们的方法将上下文相关的“概念”分组到一个集合中，并调整集合内部概念的尺度，以实现更有效的嵌入对齐。结合使用Faster R-CNN，我们的方法在开放词汇COCO和LVIS基准上的新类别中分别实现了2.6个框AP50和0.5个掩码AP的改进，相较于之前的成果。此外，与先前的研究相比，我们的方法在浮点运算中将CLIP计算量减少了80.3%，显著提升了效率。实验结果表明，所提出的方法在OVD数据集上优于先前的最先进模型。 

---
# Robust Semi-Supervised Learning in Open Environments 

**Title (ZH)**: 开放环境中稳健的半监督学习 

**Authors**: Lan-Zhe Guo, Lin-Han Jia, Jie-Jing Shao, Yu-Feng Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.18256)  

**Abstract**: Semi-supervised learning (SSL) aims to improve performance by exploiting unlabeled data when labels are scarce. Conventional SSL studies typically assume close environments where important factors (e.g., label, feature, distribution) between labeled and unlabeled data are consistent. However, more practical tasks involve open environments where important factors between labeled and unlabeled data are inconsistent. It has been reported that exploiting inconsistent unlabeled data causes severe performance degradation, even worse than the simple supervised learning baseline. Manually verifying the quality of unlabeled data is not desirable, therefore, it is important to study robust SSL with inconsistent unlabeled data in open environments. This paper briefly introduces some advances in this line of research, focusing on techniques concerning label, feature, and data distribution inconsistency in SSL, and presents the evaluation benchmarks. Open research problems are also discussed for reference purposes. 

**Abstract (ZH)**: 半监督学习（Semi-supervised Learning, SSL）旨在通过利用未标记数据来提升性能，特别是在标签稀缺的情况下。传统SSL研究通常假设近似环境，即标记数据和未标记数据之间的重要因素（如标签、特征、分布）保持一致。然而，实际任务中更常见的环境是开放环境，在这种环境中，标记数据和未标记数据之间的重要因素是不一致的。研究已经表明，利用不一致的未标记数据会导致严重的性能下降，甚至比简单的监督学习基线还要差。人工验证未标记数据的质量是不切实际的，因此，在开放环境中研究具有不一致未标记数据的鲁棒SSL变得尤为重要。本文简要介绍了该领域的一些进展，侧重于讨论与SSL相关的标签、特征和数据分布不一致性的技术，并介绍了评估基准。此外，还讨论了一些开放研究问题，以供参考。 

---
# Detection and Forecasting of Parkinson Disease Progression from Speech Signal Features Using MultiLayer Perceptron and LSTM 

**Title (ZH)**: 使用多层感知器和长短期记忆网络从语音信号特征检测和预测帕金森病进展 

**Authors**: Majid Ali, Hina Shakir, Asia Samreen, Sohaib Ahmed  

**Link**: [PDF](https://arxiv.org/pdf/2412.18248)  

**Abstract**: Accurate diagnosis of Parkinson disease, especially in its early stages, can be a challenging task. The application of machine learning techniques helps improve the diagnostic accuracy of Parkinson disease detection but only few studies have presented work towards the prediction of disease progression. In this research work, Long Short Term Memory LSTM was trained using the diagnostic features on Parkinson patients speech signals, to predict the disease progression while a Multilayer Perceptron MLP was trained on the same diagnostic features to detect the disease. Diagnostic features selected using two well-known feature selection methods named Relief-F and Sequential Forward Selection and applied on LSTM and MLP have shown to accurately predict the disease progression as stage 2 and 3 and its existence respectively. 

**Abstract (ZH)**: 帕金森病的准确诊断，尤其是在早期阶段，是一项具有挑战性的任务。机器学习技术的应用有助于提高帕金森病检测的诊断准确性，但仅有少数研究集中在疾病的进展预测上。在这项研究中，我们使用长短期记忆网络（LSTM）对帕金森病患者的语音信号进行训练，以预测疾病的进展；同时，我们也使用多层感知机（MLP）对相同的诊断特征进行训练，以检测疾病的存在。使用两种知名的特征选择方法（Relief-F和顺序前向选择法）选择的诊断特征，在LSTM和MLP上的应用能够准确预测疾病进展至第二阶段和第三阶段及其存在情况。 

---
# Fr\'echet regression for multi-label feature selection with implicit regularization 

**Title (ZH)**: 弗雷歇回归在隐式正则化下的多标签特征选择 

**Authors**: Dou El Kefel Mansouri, Seif-Eddine Benkabou, Khalid Benabdeslem  

**Link**: [PDF](https://arxiv.org/pdf/2412.18247)  

**Abstract**: Fréchet regression extends linear regression to model complex responses
in metric spaces, making it particularly relevant for multi-label regression,
where each instance can have multiple associated labels. However, variable
selection within this framework remains underexplored. In this paper, we pro pose a novel variable selection method that employs implicit regularization
instead of traditional explicit regularization approaches, which can introduce
bias. Our method effectively captures nonlinear interactions between predic tors and responses while promoting model sparsity. We provide theoretical
results demonstrating selection consistency and illustrate the performance of
our approach through numerical examples 

**Abstract (ZH)**: Frechet回归将线性回归扩展到模型度量空间中的复杂响应，使其特别适用于多标签回归，其中每个实例可以具有多个相关标签。然而，该框架内的变量选择仍然未被充分探索。本文提出了一种新的变量选择方法，该方法采用隐式正则化而非传统的显式正则化方法，后者可能会引入偏差。我们的方法能够有效捕捉预测变量和响应之间的非线性交互，并促进模型稀疏性。我们提供了理论结果来证明选择一致性，并通过数值例子展示了该方法的性能。 

---
# An Automatic Graph Construction Framework based on Large Language Models for Recommendation 

**Title (ZH)**: 基于大型语言模型的自动图构造推荐框架 

**Authors**: Rong Shan, Jianghao Lin, Chenxu Zhu, Bo Chen, Menghui Zhu, Kangning Zhang, Jieming Zhu, Ruiming Tang, Yong Yu, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18241)  

**Abstract**: Graph neural networks (GNNs) have emerged as state-of-the-art methods to learn from graph-structured data for recommendation. However, most existing GNN-based recommendation methods focus on the optimization of model structures and learning strategies based on pre-defined graphs, neglecting the importance of the graph construction stage. Earlier works for graph construction usually rely on speciffic rules or crowdsourcing, which are either too simplistic or too labor-intensive. Recent works start to utilize large language models (LLMs) to automate the graph construction, in view of their abundant open-world knowledge and remarkable reasoning capabilities. Nevertheless, they generally suffer from two limitations: (1) invisibility of global view (e.g., overlooking contextual information) and (2) construction inefficiency. To this end, we introduce AutoGraph, an automatic graph construction framework based on LLMs for recommendation. Specifically, we first use LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Next, we employ vector quantization to extract the latent factors from the semantic vectors. The latent factors are then incorporated as extra nodes to link the user/item nodes, resulting in a graph with in-depth global-view semantics. We further design metapath-based message aggregation to effectively aggregate the semantic and collaborative information. The framework is model-agnostic and compatible with different backbone models. Extensive experiments on three real-world datasets demonstrate the efficacy and efffciency of AutoGraph compared to existing baseline methods. We have deployed AutoGraph in Huawei advertising platform, and gain a 2.69% improvement on RPM and a 7.31% improvement on eCPM in the online A/B test. Currently AutoGraph has been used as the main trafffc model, serving hundreds of millions of people. 

**Abstract (ZH)**: 图神经网络（GNNs）已成为从图结构数据进行推荐的先进方法。然而，现有大多数基于GNN的推荐方法主要关注模型结构和基于预定义图的学习策略的优化，而忽视了图构建阶段的重要性。早期的图构建工作通常依赖于特定规则或 crowd sourcing，而这些方法要么过于简单，要么实施过于繁重。近期的研究开始利用大型语言模型（LLMs）来自动化图构建过程，鉴于它们丰富的开放世界知识和出色的推理能力。然而，这些方法通常面临两个局限性：（1）全局视角的不可见性（例如，忽略上下文信息）和（2）构建效率低下。为解决这些问题，我们提出了一种基于LLMs的自动图构建框架AutoGraph，用于推荐系统。具体来说，我们首先利用LLMs推断用户偏好和项目知识，并将这些信息编码为语义向量。然后，我们采用向量量化技术从语义向量中提取潜在因素，并将这些潜在因素作为额外节点与用户/项目节点连接，从而构建一个具有深入全局视角语义的图。我们还设计了基于元路径的信息聚合方法，以有效聚合语义和协同信息。该框架具有模型无关性，并兼容不同的主干模型。在三个真实世界数据集上的广泛实验表明，相比于现有基准方法，AutoGraph在效果和效率上更有优势。我们已在华为广告平台部署了AutoGraph，在线A/B测试中获得了2.69%的RPM提升和7.31%的eCPM提升。目前，AutoGraph已成为主要流量模型，服务数百亿用户。 

---
# Expand VSR Benchmark for VLLM to Expertize in Spatial Rules 

**Title (ZH)**: 将VSR基准扩展至VLLM，以专门掌握空间规则 

**Authors**: Peijin Xie, Lin Sun, Bingquan Liu, Dexin Wang, Xiangzheng Zhang, Chengjie Sun, Jiajia Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18224)  

**Abstract**: Distinguishing spatial relations is a basic part of human cognition which requires fine-grained perception on cross-instance. Although benchmarks like MME, MMBench and SEED comprehensively have evaluated various capabilities which already include visual spatial reasoning(VSR). There is still a lack of sufficient quantity and quality evaluation and optimization datasets for Vision Large Language Models(VLLMs) specifically targeting visual positional reasoning. To handle this, we first diagnosed current VLLMs with the VSR dataset and proposed a unified test set. We found current VLLMs to exhibit a contradiction of over-sensitivity to language instructions and under-sensitivity to visual positional information. By expanding the original benchmark from two aspects of tunning data and model structure, we mitigated this phenomenon. To our knowledge, we expanded spatially positioned image data controllably using diffusion models for the first time and integrated original visual encoding(CLIP) with other 3 powerful visual encoders(SigLIP, SAM and DINO). After conducting combination experiments on scaling data and models, we obtained a VLLM VSR Expert(VSRE) that not only generalizes better to different instructions but also accurately distinguishes differences in visual positional information. VSRE achieved over a 27\% increase in accuracy on the VSR test set. It becomes a performant VLLM on the position reasoning of both the VSR dataset and relevant subsets of other evaluation benchmarks. We open-sourced the expanded model with data and Appendix at \url{this https URL} and hope it will accelerate advancements in VLLM on VSR learning. 

**Abstract (ZH)**: 区分空间关系是人类认知的基本部分，需要在跨实例上进行精细感知。虽然像MME、MMBench和SEED这样的基准已经全面评估了各种能力，包括视觉空间推理（VSR），但在专门为视觉位置推理优化的Vision大型语言模型（VLLMs）的量和质的评估数据集方面仍然存在不足。为了解决这一问题，我们首先使用VSR数据集诊断现有的VLLMs，并提出了一个统一的测试集。我们发现现有的VLLMs表现出对语言指示过度敏感而对视觉位置信息不敏感的矛盾现象。通过从两个方面（调整数据和模型结构）扩展原始基准，我们减轻了这一现象。据我们所知，我们首次使用扩散模型有控制地扩展了空间定位图像数据，并将原始视觉编码（CLIP）与另外三种强大的视觉编码器（SigLIP、SAM和DINO）相结合。在调整数据和模型的组合实验后，我们得到了一个VLLM VSR专家（VSRE），不仅在不同指令中具有更好的泛化能力，还能准确区分视觉位置信息的差异。VSRE在VSR测试集上的准确性提高了超过27%。它在VSR数据集和其他评估基准的相关子集的视觉推理性能上表现优异。我们已开源扩展的模型及其数据和附录，网址为 \url{this https URL}，希望这能加速VLLM在VSR学习方面的进展。 

---
# Sharper Error Bounds in Late Fusion Multi-view Clustering Using Eigenvalue Proportion 

**Title (ZH)**: 使用特征值比例在晚期融合多视图聚类中获得更严格的误差界 

**Authors**: Liang Du, Henghui Jiang, Xiaodong Li, Yiqing Guo, Yan Chen, Feijiang Li, Peng Zhou, Yuhua Qian  

**Link**: [PDF](https://arxiv.org/pdf/2412.18207)  

**Abstract**: Multi-view clustering (MVC) aims to integrate complementary information from multiple views to enhance clustering performance. Late Fusion Multi-View Clustering (LFMVC) has shown promise by synthesizing diverse clustering results into a unified consensus. However, current LFMVC methods struggle with noisy and redundant partitions and often fail to capture high-order correlations across views. To address these limitations, we present a novel theoretical framework for analyzing the generalization error bounds of multiple kernel $k$-means, leveraging local Rademacher complexity and principal eigenvalue proportions. Our analysis establishes a convergence rate of $\mathcal{O}(1/n)$, significantly improving upon the existing rate in the order of $\mathcal{O}(\sqrt{k/n})$. Building on this insight, we propose a low-pass graph filtering strategy within a multiple linear $k$-means framework to mitigate noise and redundancy, further refining the principal eigenvalue proportion and enhancing clustering accuracy. Experimental results on benchmark datasets confirm that our approach outperforms state-of-the-art methods in clustering performance and robustness. The related codes is available at this https URL . 

**Abstract (ZH)**: 多视图聚类（MVC）旨在通过整合来自多个视图的互补信息来提高聚类性能。晚期融合多视图聚类（LFMVC）通过综合多种聚类结果形成了统一的一致性，展现了其潜力。然而，当前的LFMVC方法在处理噪声和冗余的聚类划分时存在困难，通常无法捕捉视图之间的高阶关联。为解决这些限制，我们提出了一种新的理论框架，用于分析多种核 $k$-means 的泛化误差界，利用局部Rademacher复杂性和主特征值比例。我们的分析建立了收敛速率 $\mathcal{O}(1/n)$，这一速率相较于现有的 $\mathcal{O}(\sqrt{k/n})$ 获得了显著提升。基于这一洞察，我们提出了一种多线性 $k$-means 框架内的低通图滤波策略，以减轻噪声和冗余，进一步优化主特征值比例，从而提高聚类准确性。在基准数据集上的实验结果显示，我们的方法在聚类性能和鲁棒性方面优于现有的方法。相关代码可在以下链接获取：[此处链接]。 

---
# VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks 

**Title (ZH)**: VLABench：一种用于基于语言条件的机器人Manipulation任务的大型基准测试，包含长期 horizon 原因推理任务 

**Authors**: Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18194)  

**Abstract**: General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh\&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality training data collected via an automated framework incorporating heuristic skills and prior information. The experimental results indicate that both the current state-of-the-art pretrained VLAs and the workflow based on VLMs face challenges in our tasks. 

**Abstract (ZH)**: 通用型具身代理旨在理解用户的自然指令或意图，并精确行动以完成通用任务。最近，基于基础模型的方法，尤其是视觉-语言-行动模型（VLAs），在解决语言条件下的操作任务（LCM）方面显示出巨大的潜力。然而，现有的基准并未充分满足VLAs及其相关算法的需求。为了更好地定义此类通用任务，并推动VLAs的研究，我们提出了VLABench，这是一个开源基准，用于评估通用LCM任务学习。VLABench提供了100个精心设计的任务类别，每个类别中包含强大的随机化因素，并且共有2000多个物体。相比于之前的基准，VLABench在四个关键方面脱颖而出：1）需要世界知识和常识转换的任务；2）具有隐含人类意图的自然语言指令，而非模板；3）长时序任务，需要多步推理；4）同时评估行动策略和语言模型能力。该基准评估了多种能力，包括网格与纹理理解、空间关系、语义指令、物理定律、知识转移与推理等。为了支持下游微调，我们提供了高质量的训练数据，通过包含启发式技能和先验信息的自动化框架进行收集。实验结果表明，当前最先进的预训练VLAs和基于VLM的流程在我们的任务中都面临着挑战。 

---
# An Analysis on Automated Metrics for Evaluating Japanese-English Chat Translation 

**Title (ZH)**: 《关于评估日英聊天翻译的自动化指标的分析》 

**Authors**: Andre Rusli, Makoto Shishido  

**Link**: [PDF](https://arxiv.org/pdf/2412.18190)  

**Abstract**: This paper analyses how traditional baseline metrics, such as BLEU and TER, and neural-based methods, such as BERTScore and COMET, score several NMT models performance on chat translation and how these metrics perform when compared to human-annotated scores. The results show that for ranking NMT models in chat translations, all metrics seem consistent in deciding which model outperforms the others. This implies that traditional baseline metrics, which are faster and simpler to use, can still be helpful. On the other hand, when it comes to better correlation with human judgment, neural-based metrics outperform traditional metrics, with COMET achieving the highest correlation with the human-annotated score on a chat translation. However, we show that even the best metric struggles when scoring English translations from sentences with anaphoric zero-pronoun in Japanese. 

**Abstract (ZH)**: 本文分析了传统 baseline 指标（如 BLEU 和 TER）和基于神经网络的方法（如 BERTScore 和 COMET）在聊天翻译任务中对几种神经机器翻译（NMT）模型性能评分的情况，并探讨了这些指标与人工标注评分相比的表现。研究结果表明，在排列聊天翻译中的 NMT 模型时，所有指标似乎在决定哪个模型表现更好方面保持一致。这表明，尽管传统 baseline 指标通常更快且更易于使用，它们仍然具有一定的参考价值。另一方面，在与人工判断的相关性方面，基于神经网络的指标优于传统指标，COMET 在聊天翻译方面与人工标注评分的相关性最高。然而，研究表明，即使是最佳指标，在对包含日语复数零人称代词的英语翻译进行评分时也表现不佳。 

---
# On the Applicability of Zero-Shot Cross-Lingual Transfer Learning for Sentiment Classification in Distant Language Pairs 

**Title (ZH)**: 关于零样本跨语言迁移学习在遥远语对情感分类中的适用性研究 

**Authors**: Andre Rusli, Makoto Shishido  

**Link**: [PDF](https://arxiv.org/pdf/2412.18188)  

**Abstract**: This research explores the applicability of cross-lingual transfer learning from English to Japanese and Indonesian using the XLM-R pre-trained model. The results are compared with several previous works, either by models using a similar zero-shot approach or a fully-supervised approach, to provide an overview of the zero-shot transfer learning approach's capability using XLM-R in comparison with existing models. Our models achieve the best result in one Japanese dataset and comparable results in other datasets in Japanese and Indonesian languages without being trained using the target language. Furthermore, the results suggest that it is possible to train a multi-lingual model, instead of one model for each language, and achieve promising results. 

**Abstract (ZH)**: 本研究探讨了使用XLM-R预训练模型从英语跨语言迁移学习到日语和印尼语的应用效果。我们将结果与几种先前的研究进行了比较，这些研究要么使用类似零样本的方法，要么使用完全监督的方法，以提供使用XLM-R进行零样本迁移学习能力的概述，与现有模型进行对比。我们的模型在一项日语数据集中获得了最佳结果，并在其他日语和印尼语数据集中获得了可比的结果，而没有使用目标语言进行训练。此外，研究结果表明，有可能训练一个多语言模型，而不是为每种语言单独训练一个模型，并取得令人鼓舞的成果。 

---
# TextMatch: Enhancing Image-Text Consistency Through Multimodal Optimization 

**Title (ZH)**: 文本匹配：通过多模态优化提高图像-文本一致性 

**Authors**: Yucong Luo, Mingyue Cheng, Jie Ouyang, Xiaoyu Tao, Qi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18185)  

**Abstract**: Text-to-image generative models excel in creating images from text but struggle with ensuring alignment and consistency between outputs and prompts. This paper introduces TextMatch, a novel framework that leverages multimodal optimization to address image-text discrepancies in text-to-image (T2I) generation and editing. TextMatch employs a scoring strategy powered by large language models (LLMs) and visual question-answering (VQA) models to evaluate semantic consistency between prompts and generated images. By integrating multimodal in-context learning and chain of thought reasoning, our method dynamically refines prompts through iterative optimization. This process ensures that the generated images better capture user intent of, resulting in higher fidelity and relevance. Extensive experiments demonstrate that TextMatch significantly improves text-image consistency across multiple benchmarks, establishing a reliable framework for advancing the capabilities of text-to-image generative models. Our code is available at this https URL. 

**Abstract (ZH)**: 文本到图像生成模型在从文本生成图像方面表现优异，但在确保输出与提示之间的对齐和一致性方面存在困难。本文引入了TextMatch，这是一种利用多模态优化的新框架，旨在解决文本到图像（T2I）生成和编辑过程中的图像文本不一致问题。TextMatch利用大语言模型（LLMs）和视觉问答（VQA）模型的评分策略来评估提示和生成图像之间的语义一致性。通过整合多模态上下文学习和链式推理，我们的方法通过迭代优化动态改善提示。这一过程确保生成的图像更好地捕捉用户意图，从而提高生成的图像的一致性和相关性。广泛的实验证明，TextMatch在多个基准测试中显著提高了文本图像的一致性，建立了推进文本到图像生成模型能力的可靠框架。我们的代码可在以下链接获取：this https URL。 

---
# Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization 

**Title (ZH)**: 利用即插即用状态空间模型和类别条件离散混合增强在线持续学习 

**Authors**: Sihao Liu, Yibo Yang, Xiaojie Li, David A. Clifton, Bernard Ghanem  

**Link**: [PDF](https://arxiv.org/pdf/2412.18177)  

**Abstract**: Online continual learning (OCL) seeks to learn new tasks from data streams that appear only once, while retaining knowledge of previously learned tasks. Most existing methods rely on replay, focusing on enhancing memory retention through regularization or distillation. However, they often overlook the adaptability of the model, limiting the ability to learn generalizable and discriminative features incrementally from online training data. To address this, we introduce a plug-and-play module, S6MOD, which can be integrated into most existing methods and directly improve adaptability. Specifically, S6MOD introduces an extra branch after the backbone, where a mixture of discretization selectively adjusts parameters in a selective state space model, enriching selective scan patterns such that the model can adaptively select the most sensitive discretization method for current dynamics. We further design a class-conditional routing algorithm for dynamic, uncertainty-based adjustment and implement a contrastive discretization loss to optimize it. Extensive experiments combining our module with various models demonstrate that S6MOD significantly enhances model adaptability, leading to substantial performance gains and achieving the state-of-the-art results. 

**Abstract (ZH)**: 在线 continual learning（OCL）旨在从仅出现一次的数据流中学习新任务，同时保留先前学习任务的知识。现有的大多数方法依赖于重演，侧重于通过正则化或蒸馏提高记忆保持能力。然而，这些方法往往忽视了模型的适应性，限制了其从在线训练数据中逐步学习泛化和鉴别特征的能力。为解决这一问题，我们引入了一个可插拔模块 S6MOD，它可以集成到大多数现有方法中，并直接提高模型的适应性。具体而言，S6MOD 在骨干网络之后引入一个额外分支，在此分支中，混合离散化有选择地调整选择状态空间模型中的参数，丰富了选择性扫描模式，使得模型能够自适应地选择对当前动态最敏感的离散化方法。我们进一步设计了一种基于类别的路由算法，用于动态和基于不确定性调整，并实现了一种对比性离散化损失以优化该算法。我们的模块与各种模型结合进行的广泛实验表明，S6MOD 显著提升了模型的适应性，导致了显著的性能提升，并达到了当前最优的结果。 

---
# Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation 

**Title (ZH)**: Molar：基于协同过滤对齐的多模态大语言模型以增强序列推荐 

**Authors**: Yucong Luo, Qitao Qin, Hao Zhang, Mingyue Cheng, Ruiran Yan, Kefan Wang, Jie Ouyang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18176)  

**Abstract**: Sequential recommendation (SR) systems have evolved significantly over the past decade, transitioning from traditional collaborative filtering to deep learning approaches and, more recently, to large language models (LLMs). While the adoption of LLMs has driven substantial advancements, these models inherently lack collaborative filtering information, relying primarily on textual content data neglecting other modalities and thus failing to achieve optimal recommendation performance. To address this limitation, we propose Molar, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively. Molar employs an MLLM to generate unified item representations from both textual and non-textual data, facilitating comprehensive multimodal modeling and enriching item embeddings. Additionally, it incorporates collaborative filtering signals through a post-alignment mechanism, which aligns user representations from content-based and ID-based models, ensuring precise personalization and robust performance. By seamlessly combining multimodal content with collaborative filtering insights, Molar captures both user interests and contextual semantics, leading to superior recommendation accuracy. Extensive experiments validate that Molar significantly outperforms traditional and LLM-based baselines, highlighting its strength in utilizing multimodal data and collaborative signals for sequential recommendation tasks. The source code is available at this https URL. 

**Abstract (ZH)**: 随时间的推移，序列推荐（SR）系统经历了显著的发展，从传统的协同过滤转向了深度学习方法，并且更近的转向了大规模语言模型（LLMs）。尽管LLMs的采用带来了显著的进步，但这些模型本身缺乏协同过滤信息，主要依赖于文本内容数据，忽略了其他模态的数据，从而未能达到最佳推荐性能。为了解决这一局限，我们提出了Multolar（Multimodal large language sequential recommendation框架），该框架通过整合多种内容模态与ID信息来有效地捕捉协同信号。Multolar使用多模态大规模语言模型（MLLM）从文本和非文本数据中生成统一的项目表示，促进全面的多模态建模并丰富项目嵌入。此外，通过后对齐机制整合协同过滤信号，该机制将基于内容和基于ID的模型中的用户表示对齐，确保精确的个性化和稳健的性能。通过无缝结合多模态内容与协同过滤洞察，Multolar捕捉了用户的兴趣和上下文语义，从而提高了推荐准确性。广泛的实验表明，Multolar显著优于传统的和基于LLM的基线模型，突显了其利用多模态数据和协同信号进行序列推荐任务的能力。代码可在以下链接中获得：[这里](https://example.com/code)。 

---
# INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent 

**Title (ZH)**: 投资商衡：基于LLM的代理金融决策任务基准测试 

**Authors**: Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, Jordan W. Suchow  

**Link**: [PDF](https://arxiv.org/pdf/2412.18174)  

**Abstract**: Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios. 

**Abstract (ZH)**: 近年来，大型语言模型（LLM）代理在财务决策中的潜力得到了强调。尽管取得了这些进展，该领域目前仍面临两个主要挑战：（1）缺乏一个适应各种财务任务的全面LLM代理框架，以及（2）缺乏标准化的基准测试和一致的数据集来评估代理性能。为应对这些问题，我们介绍了InvestorBench，这是第一个专门用于评估在不同财务决策情境下LLM代理的基准测试框架。InvestorBench通过提供适用于不同类型金融产品的综合任务套件，增强了LLM启用代理的灵活性，包括单一股票、加密货币和交易型开放式指数基金（ETFs）等单一证券产品。

此外，我们使用来自不同市场的十三种不同的LLM作为骨干模型，评估我们的代理框架在各种市场环境和任务中的推理和决策能力。我们还精选了多种开源多模态数据集，并开发了一整套用于财务决策的环境，从而建立了一个高度可访问的平台用于评估不同情境下金融代理的性能。 

---
# KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management 

**Title (ZH)**: KunServe：以参数为中心的内存管理下的弹性高效大型语言模型服务 

**Authors**: Rongxin Cheng, Yifan Peng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.18169)  

**Abstract**: The stateful nature of large language model (LLM) servingcan easily throttle precious GPU memory under load burstor long-generation requests like chain-of-thought reasoning,causing latency spikes due to queuing incoming requests. However, state-of-the-art KVCache centric approaches handleload spikes by dropping, migrating, or swapping KVCache,which faces an essential tradeoff between the performance ofongoing vs. incoming requests and thus still severely this http URL paper makes a key observation such that model param-eters are independent of the requests and are replicated acrossGPUs, and thus proposes a parameter-centric approach byselectively dropping replicated parameters to leave preciousmemory for requests. However, LLM requires KVCache tobe saved in bound with model parameters and thus droppingparameters can cause either huge computation waste or longnetwork delay, affecting all ongoing requests. Based on the ob-servation that attention operators can be decoupled from otheroperators, this paper further proposes a novel remote attentionmechanism through pipeline parallelism so as to serve up-coming requests with the additional memory borrowed fromparameters on remote GPUs. This paper further addresses sev-eral other challenges including lively exchanging KVCachewith incomplete parameters, generating an appropriate planthat balances memory requirements with cooperative exe-cution overhead, and seamlessly restoring parameters whenthe throttling has gone. Evaluations show thatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x compared to the state-of-the-art. 

**Abstract (ZH)**: 大型语言模型（LLM）的服务具有状态性的特点，在负载突发或长生成请求（如链式推理）的情况下，容易耗尽宝贵的GPU内存，导致由于排队请求造成的延迟峰值。然而，最先进的一些基于KV缓存的方法通过丢弃、迁移或替换KV缓存来处理负载峰值，这在持续请求与新请求的性能之间存在本质的权衡，因此仍然存在严重的问题。

本文做出了一项关键观察，即模型参数与请求无关，并且可以在GPU之间进行复制。因此，本文提出了一个参数为中心的方法，通过选择性地丢弃复制参数来保留宝贵的内存供请求使用。然而，LLM需要将KV缓存与模型参数绑定保存，因此丢弃参数可能导致巨大的计算浪费或长时间的网络延迟，影响所有正在进行的请求。基于注意力操作与其他操作可以解耦的观察，本文进一步提出了一种通过管道并行性实现的新型远程注意力机制，以便利用远程GPU上复制参数的额外内存来服务即将到来的请求。本文还解决了一系列其他挑战，包括活跃交换不完整参数的KV缓存、生成平衡内存需求与协同执行开销的适当计划，以及在过载解除后无缝恢复参数。评估结果显示，KUNSERVE在过载情况下将请求的尾部TTFT（延迟）降低了高达27.3倍，优于现有的方法。 

---
# Survey of Pseudonymization, Abstractive Summarization & Spell Checker for Hindi and Marathi 

**Title (ZH)**: 《关于印地语和马拉地语的匿名化、摘要化及拼写检查的综述》 

**Authors**: Rasika Ransing, Mohammed Amaan Dhamaskar, Ayush Rajpurohit, Amey Dhoke, Sanket Dalvi  

**Link**: [PDF](https://arxiv.org/pdf/2412.18163)  

**Abstract**: India's vast linguistic diversity presents unique challenges and opportunities for technological advancement, especially in the realm of Natural Language Processing (NLP). While there has been significant progress in NLP applications for widely spoken languages, the regional languages of India, such as Marathi and Hindi, remain underserved. Research in the field of NLP for Indian regional languages is at a formative stage and holds immense significance. The paper aims to build a platform which enables the user to use various features like text anonymization, abstractive text summarization and spell checking in English, Hindi and Marathi language. The aim of these tools is to serve enterprise and consumer clients who predominantly use Indian Regional Languages. 

**Abstract (ZH)**: 印度丰富的语言多样性为技术进步，尤其是自然语言处理（NLP）领域，带来了独特的挑战和机遇。尽管在常用语言的NLP应用方面取得了显著进展，但印度的区域语言，如马拉地语和印地语，仍处于服务不足状态。针对印度区域语言的NLP研究仍处于起步阶段，具有重要的学术意义。本文旨在构建一个平台，使用户能够使用文本匿名化、摘要生成和拼写检查等多种功能，支持英语、印地语和马拉地语。这些工具的目的是为以印度区域语言为主要使用的企业和消费者客户提供服务。 

---
# Smooth-Foley: Creating Continuous Sound for Video-to-Audio Generation Under Semantic Guidance 

**Title (ZH)**: Smooth-Foley：在语义引导下的视频到音频生成中的连续声音创建 

**Authors**: Yaoyun Zhang, Xuenan Xu, Mengyue Wu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18157)  

**Abstract**: The video-to-audio (V2A) generation task has drawn attention in the field of multimedia due to the practicality in producing Foley sound. Semantic and temporal conditions are fed to the generation model to indicate sound events and temporal occurrence. Recent studies on synthesizing immersive and synchronized audio are faced with challenges on videos with moving visual presence. The temporal condition is not accurate enough while low-resolution semantic condition exacerbates the problem. To tackle these challenges, we propose Smooth-Foley, a V2A generative model taking semantic guidance from the textual label across the generation to enhance both semantic and temporal alignment in audio. Two adapters are trained to leverage pre-trained text-to-audio generation models. A frame adapter integrates high-resolution frame-wise video features while a temporal adapter integrates temporal conditions obtained from similarities of visual frames and textual labels. The incorporation of semantic guidance from textual labels achieves precise audio-video alignment. We conduct extensive quantitative and qualitative experiments. Results show that Smooth-Foley performs better than existing models on both continuous sound scenarios and general scenarios. With semantic guidance, the audio generated by Smooth-Foley exhibits higher quality and better adherence to physical laws. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，并符合学术规范：

视频到音频（V2A）生成任务在多媒体领域引起了关注，因为它在制作费德声效方面具有实用性。语义和时间条件被输送到生成模型中，以指示声事件及其发生的时间。最近在合成沉浸式和同步音频方面，对于具有移动视觉特征的视频面临着挑战。时间条件不够精确，而低分辨率语义条件加剧了这一问题。为了解决这些问题，我们提出了一种名为 Smooth-Foley 的 V2A 生成模型，该模型通过在整个生成过程中从文本标签中获取语义指导，增强了音频在语义和时间上的对齐。两个适配器被训练用于利用预训练的文本到音频生成模型。帧适配器整合了高分辨率的帧级视频特征，而时间适配器则整合了从视觉帧相似性和文本标签中获得的时间条件。从文本标签中引入的语义指导实现了精确的音频视频对齐。我们进行了广泛的定量和定性实验。结果显示，Smooth-Foley 在连续声效场景和通用场景中均优于现有模型。在语义指导的帮助下，Smooth-Foley 生成的音频具有更高的质量和更好地符合物理定律。 

---
# scReader: Prompting Large Language Models to Interpret scRNA-seq Data 

**Title (ZH)**: scReader：引导大规模语言模型解析单细胞RNA测序数据 

**Authors**: Cong Li, Qingqing Long, Yuanchun Zhou, Meng Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2412.18156)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable advancements, primarily due to their capabilities in modeling the hidden relationships within text sequences. This innovation presents a unique opportunity in the field of life sciences, where vast collections of single-cell omics data from multiple species provide a foundation for training foundational models. However, the challenge lies in the disparity of data scales across different species, hindering the development of a comprehensive model for interpreting genetic data across diverse organisms. In this study, we propose an innovative hybrid approach that integrates the general knowledge capabilities of LLMs with domain-specific representation models for single-cell omics data interpretation. We begin by focusing on genes as the fundamental unit of representation. Gene representations are initialized using functional descriptions, leveraging the strengths of mature language models such as LLaMA-2. By inputting single-cell gene-level expression data with prompts, we effectively model cellular representations based on the differential expression levels of genes across various species and cell types. In the experiments, we constructed developmental cells from humans and mice, specifically targeting cells that are challenging to annotate. We evaluated our methodology through basic tasks such as cell annotation and visualization analysis. The results demonstrate the efficacy of our approach compared to other methods using LLMs, highlighting significant improvements in accuracy and interoperability. Our hybrid approach enhances the representation of single-cell data and offers a robust framework for future research in cross-species genetic analysis. 

**Abstract (ZH)**: 大型语言模型（LLMs）表现出显著的进展，主要得益于它们在建模文本序列中隐藏关系方面的能力。这种创新为生命科学领域提供了独特的机会，该领域中来自多种物种的大量单细胞组学数据为训练基础模型奠定了基础。然而，不同物种之间数据规模的差异阻碍了跨多种生物体解析基因数据的全面模型的发展。在本研究中，我们提出了一种创新的混合方法，将LLMs的一般知识能力与其针对单细胞组学数据的特定领域表示模型相结合。我们首先以基因作为表示的基本单元。基因表示通过功能描述初始化，利用成熟语言模型（如LLaMA-2）的优势。通过将单细胞基因水平表达数据与提示输入，我们能够基于不同物种和细胞类型中基因表达差异来有效建模细胞表示。在实验中，我们构建了来自人类和小鼠的发育细胞，特别关注那些难以注释的细胞。我们通过基本任务如细胞注释和可视化分析来评估我们的方法。结果表明，与使用LLMs的其他方法相比，我们方法的有效性和兼容性显著提高。我们的混合方法增强了单细胞数据的表现，并为跨物种遗传分析的未来研究提供了一个稳健的框架。 

---
# GeneSUM: Large Language Model-based Gene Summary Extraction 

**Title (ZH)**: GeneSUM：基于大型语言模型的基因摘要提取 

**Authors**: Zhijian Chen, Chuan Hu, Min Wu, Qingqing Long, Xuezhi Wang, Yuanchun Zhou, Meng Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2412.18154)  

**Abstract**: Emerging topics in biomedical research are continuously expanding, providing a wealth of information about genes and their function. This rapid proliferation of knowledge presents unprecedented opportunities for scientific discovery and formidable challenges for researchers striving to keep abreast of the latest advancements. One significant challenge is navigating the vast corpus of literature to extract vital gene-related information, a time-consuming and cumbersome task. To enhance the efficiency of this process, it is crucial to address several key challenges: (1) the overwhelming volume of literature, (2) the complexity of gene functions, and (3) the automated integration and generation. In response, we propose GeneSUM, a two-stage automated gene summary extractor utilizing a large language model (LLM). Our approach retrieves and eliminates redundancy of target gene literature and then fine-tunes the LLM to refine and streamline the summarization process. We conducted extensive experiments to validate the efficacy of our proposed framework. The results demonstrate that LLM significantly enhances the integration of gene-specific information, allowing more efficient decision-making in ongoing research. 

**Abstract (ZH)**: 生物医学研究中的新兴领域不断扩展，提供了大量关于基因及其功能的信息。这种知识的快速增长为科学发现提供了前所未有的机遇，同时也为研究人员跟上最新进展带来了巨大的挑战。一个重大的挑战是如何在庞大的文献库中导航并提取关键的基因相关信息，这一过程耗时且繁琐。为了提高这一过程的效率，必须应对几个关键挑战：(1) 文献的庞大体量，(2) 基因功能的复杂性，以及(3) 自动化整合和生成。为此，我们提出了一种名为GeneSUM的两阶段自动化基因摘要提取器，利用大型语言模型（LLM）。我们的方法首先检索并消除目标基因文献的冗余，然后通过微调LLM来优化和简化摘要过程。我们进行了广泛实验，以验证我们提出框架的有效性。结果表明，大型语言模型显著增强了基因特异性信息的整合，使得在正在进行的研究中能够更高效地做出决策。 

---
# EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation 

**Title (ZH)**: EvalMuse-40K：一种可靠的细粒度基准，包含全面的人类标注，用于评估文本到图像生成模型 

**Authors**: Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, Chongyi Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.18150)  

**Abstract**: Recently, Text-to-Image (T2I) generation models have achieved significant advancements. Correspondingly, many automated metrics have emerged to evaluate the image-text alignment capabilities of generative models. However, the performance comparison among these automated metrics is limited by existing small datasets. Additionally, these datasets lack the capacity to assess the performance of automated metrics at a fine-grained level. In this study, we contribute an EvalMuse-40K benchmark, gathering 40K image-text pairs with fine-grained human annotations for image-text alignment-related tasks. In the construction process, we employ various strategies such as balanced prompt sampling and data re-annotation to ensure the diversity and reliability of our benchmark. This allows us to comprehensively evaluate the effectiveness of image-text alignment metrics for T2I models. Meanwhile, we introduce two new methods to evaluate the image-text alignment capabilities of T2I models: FGA-BLIP2 which involves end-to-end fine-tuning of a vision-language model to produce fine-grained image-text alignment scores and PN-VQA which adopts a novel positive-negative VQA manner in VQA models for zero-shot fine-grained evaluation. Both methods achieve impressive performance in image-text alignment evaluations. We also use our methods to rank current AIGC models, in which the results can serve as a reference source for future study and promote the development of T2I generation. The data and code will be made publicly available. 

**Abstract (ZH)**: 近年来，文本到图像（T2I）生成模型取得了显著进展。相应地，已经出现了许多自动化指标来评估生成模型的图像-文本对齐能力。然而，这些自动化指标的性能比较受到现有小型数据集的限制，而且这些数据集缺乏细粒度评估自动化指标性能的能力。本研究中，我们贡献了一个名为EvalMuse-40K的基准，集成了40000个带有细粒度人工标注的图像-文本对，用于图像-文本对齐相关的任务。在构建过程中，我们采用了包括平衡提示采样和数据重新标注等策略，以确保基准的多样性和可靠性。这使我们能够全面评估图像-文本对齐指标在T2I模型中的有效性。同时，我们介绍了两种新的方法来评估T2I模型的图像-文本对齐能力：FGA-BLIP2通过端到端微调视觉-语言模型生成细粒度的图像-文本对齐得分；PN-VQA则在VQA模型中采用了一种新颖的正负样本VQA方法，实现了零样本的细粒度评估。这两种方法在图像-文本对齐评估中均表现出了卓越的效果。我们还使用这两种方法对当前的AIGC模型进行了排序，这些结果可以为未来的研究提供参考，促进T2I生成的发展。数据和代码将公开提供。 

---
# Text-Aware Adapter for Few-Shot Keyword Spotting 

**Title (ZH)**: 文本感知适配器用于少样本关键词定位 

**Authors**: Youngmoon Jung, Jinyoung Lee, Seungjin Lee, Myunghun Jung, Yong-Hyeok Lee, Hoon-Young Cho  

**Link**: [PDF](https://arxiv.org/pdf/2412.18142)  

**Abstract**: Recent advances in flexible keyword spotting (KWS) with text enrollment allow users to personalize keywords without uttering them during enrollment. However, there is still room for improvement in target keyword performance. In this work, we propose a novel few-shot transfer learning method, called text-aware adapter (TA-adapter), designed to enhance a pre-trained flexible KWS model for specific keywords with limited speech samples. To adapt the acoustic encoder, we leverage a jointly pre-trained text encoder to generate a text embedding that acts as a representative vector for the keyword. By fine-tuning only a small portion of the network while keeping the core components' weights intact, the TA-adapter proves highly efficient for few-shot KWS, enabling a seamless return to the original pre-trained model. In our experiments, the TA-adapter demonstrated significant performance improvements across 35 distinct keywords from the Google Speech Commands V2 dataset, with only a 0.14% increase in the total number of parameters. 

**Abstract (ZH)**: 近年来，文本注册的柔性关键词识别（KWS）取得了进展，允许用户在注册过程中无需说出关键词即可实现个性化设置。然而，目标关键词的表现仍有改进的空间。在本文中，我们提出了一种新的少样本迁移学习方法，称为文本感知适配器（TA-adapter），该方法旨在通过有限的语音样本增强预先训练的柔性KWS模型。为了适应声学编码器，我们利用联合预先训练的文本编码器生成一个文本嵌入向量，作为关键词的代表性向量。通过仅微调网络的较小部分，同时保持核心组件的权重不变，TA-adapter 证明了其在少样本KWS中的高效率，使得模型能够平滑地恢复到原始的预先训练模型。在我们的实验中，TA-adapter 在Google Speech Commands V2数据集中35个不同关键词上表现出显著的性能提升，仅增加了0.14%的参数总量。 

---
# Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm 

**Title (ZH)**: 语言模型是否理解分配给它们的认知任务？基于N-Back范式的探究 

**Authors**: Xiaoyang Hu, Richard L. Lewis  

**Link**: [PDF](https://arxiv.org/pdf/2412.18120)  

**Abstract**: Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it's often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argued that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans. By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance instead reflects a limitation in task comprehension and task set maintenance. In addition, we push the best performing model to higher n values and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models. 

**Abstract (ZH)**: 原本为人类设计的认知任务现在越来越多地被用于研究语言模型。虽然应用这些任务通常较为直接，但解释其结果却往往颇具挑战性。特别是在模型表现不佳时，难以确定是由于被测试的认知能力有限，还是由于对任务本身理解不当。最近的一项研究认为，GPT 3.5在2-Back和3-Back任务上的表现下降反映了类似人类的工作记忆容量限制。通过分析不同性能水平的开源语言模型在这些任务上的表现，我们指出，较差的表现实际上反映了任务理解和任务集维持方面的局限性。此外，我们还推动性能最佳的模型达到更高的n值，并尝试不同的提示策略，然后分析模型的注意力机制。我们的更大目标是为持续进行的语言模型认知评估方法论的改进对话做出贡献。 

---
# SongGLM: Lyric-to-Melody Generation with 2D Alignment Encoding and Multi-Task Pre-Training 

**Title (ZH)**: SongGLM：基于二维对齐编码和多任务预训练的歌词到旋律生成 

**Authors**: Jiaxing Yu, Xinda Wu, Yunfei Xu, Tieyao Zhang, Songruoyao Wu, Le Ma, Kejun Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.18107)  

**Abstract**: Lyric-to-melody generation aims to automatically create melodies based on given lyrics, requiring the capture of complex and subtle correlations between them. However, previous works usually suffer from two main challenges: 1) lyric-melody alignment modeling, which is often simplified to one-syllable/word-to-one-note alignment, while others have the problem of low alignment accuracy; 2) lyric-melody harmony modeling, which usually relies heavily on intermediates or strict rules, limiting model's capabilities and generative diversity. In this paper, we propose SongGLM, a lyric-to-melody generation system that leverages 2D alignment encoding and multi-task pre-training based on the General Language Model (GLM) to guarantee the alignment and harmony between lyrics and melodies. Specifically, 1) we introduce a unified symbolic song representation for lyrics and melodies with word-level and phrase-level (2D) alignment encoding to capture the lyric-melody alignment; 2) we design a multi-task pre-training framework with hierarchical blank infilling objectives (n-gram, phrase, and long span), and incorporate lyric-melody relationships into the extraction of harmonized n-grams to ensure the lyric-melody harmony. We also construct a large-scale lyric-melody paired dataset comprising over 200,000 English song pieces for pre-training and fine-tuning. The objective and subjective results indicate that SongGLM can generate melodies from lyrics with significant improvements in both alignment and harmony, outperforming all the previous baseline methods. 

**Abstract (ZH)**: 歌词到旋律生成旨在根据给定的歌词自动生成旋律，需要捕捉歌词和旋律之间复杂微妙的关联。然而，以往的工作通常面临两大主要挑战：1）歌词-旋律对齐建模，通常简化为一个音节/单词对应一个音符的对齐方式，而其他方法则存在对齐精度较低的问题；2）歌词-旋律和声建模，通常依赖中间步骤或严格的规则，限制了模型的能力和生成多样性。本文提出了一种名为SongGLM的歌词到旋律生成系统，利用基于通用语言模型（GLM）的二维对齐编码和多任务预训练来保证歌词和旋律之间的对齐和和声。具体来说，1）我们引入了一种统一的歌词和旋律符号表示方式，并采用词级和短语级（二维）对齐编码来捕捉歌词和旋律的对齐；2）我们设计了一个多任务预训练框架，包含层次空白填充目标（n-gram、短语和长跨度），并将歌词-旋律关系融入和弦n-gram的提取中，以确保歌词-旋律的和声。我们还构建了一个包含超过200,000首英文歌曲的大规模配对数据集用于预训练和微调。实验效果表明，SongGLM在对齐和和声方面均取得了显著提高，超过了所有之前的基线方法。 

---
# EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent 

**Title (ZH)**: EvoPat：一种基于多大型语言模型的专利总结与分析代理 

**Authors**: Suyuan Wang, Xueqian Yin, Menghao Wang, Ruofeng Guo, Kai Nan  

**Link**: [PDF](https://arxiv.org/pdf/2412.18100)  

**Abstract**: The rapid growth of scientific techniques and knowledge is reflected in the exponential increase in new patents filed annually. While these patents drive innovation, they also present significant burden for researchers and engineers, especially newcomers. To avoid the tedious work of navigating a vast and complex landscape to identify trends and breakthroughs, researchers urgently need efficient tools to summarize, evaluate, and contextualize patents, revealing their innovative contributions and underlying scientific this http URL address this need, we present EvoPat, a multi-LLM-based patent agent designed to assist users in analyzing patents through Retrieval-Augmented Generation (RAG) and advanced search strategies. EvoPat leverages multiple Large Language Models (LLMs), each performing specialized roles such as planning, identifying innovations, and conducting comparative evaluations. The system integrates data from local databases, including patents, literature, product catalogous, and company repositories, and online searches to provide up-to-date insights. The ability to collect information not included in original database automatically is also implemented. Through extensive testing in the natural language processing (NLP) domain, we demonstrate that EvoPat outperforms GPT-4 in tasks such as patent summarization, comparative analysis, and technical evaluation. EvoPat represents a significant step toward creating AI-powered tools that empower researchers and engineers to efficiently navigate the complexities of the patent landscape. 

**Abstract (ZH)**: 随着科学技术和知识的迅速发展，每年新提交的专利数量呈指数级增长。虽然这些专利推动了创新，但也给研究人员和工程师，尤其是新入行者带来了巨大负担。为了避免在广阔的复杂领域中寻找趋势和突破时进行繁琐的工作，研究人员迫切需要高效的工具来总结、评估和上下文化专利，揭示其创新贡献和背后的科学原理。为了解决这一需求，我们提出了EvoPat，这是一种基于多大语言模型（LLMs）的专利代理，旨在通过检索增强生成（RAG）和高级搜索策略帮助用户分析专利。EvoPat利用多个大型语言模型，每个模型承担特定的角色，如计划、识别创新和进行比较评估。该系统整合了本地数据库中的数据，包括专利、文献、产品目录和公司仓库，以及在线搜索，以提供最新的洞见。还实现了自动收集未包含在原始数据库中的信息的能力。通过在自然语言处理（NLP）领域的广泛测试，我们证明EvoPat在专利总结、比较分析和技术评估等任务上优于GPT-4。EvoPat代表了朝着创建赋能研究人员和工程师高效导航专利landscape复杂性的AI工具迈出的重要一步。 

---
# An Attention-based Framework with Multistation Information for Earthquake Early Warnings 

**Title (ZH)**: 基于多站点信息的注意力机制框架在地震早期预警中的应用 

**Authors**: Yu-Ming Huang, Kuan-Yu Chen, Wen-Wei Lin, Da-Yi Chen  

**Link**: [PDF](https://arxiv.org/pdf/2412.18099)  

**Abstract**: Earthquake early warning systems play crucial roles in reducing the risk of seismic disasters. Previously, the dominant modeling system was the single-station models. Such models digest signal data received at a given station and predict earth-quake parameters, such as the p-phase arrival time, intensity, and magnitude at that location. Various methods have demonstrated adequate performance. However, most of these methods present the challenges of the difficulty of speeding up the alarm time, providing early warning for distant areas, and considering global information to enhance performance. Recently, deep learning has significantly impacted many fields, including seismology. Thus, this paper proposes a deep learning-based framework, called SENSE, for the intensity prediction task of earthquake early warning systems. To explicitly consider global information from a regional or national perspective, the input to SENSE comprises statistics from a set of stations in a given region or country. The SENSE model is designed to learn the relationships among the set of input stations and the locality-specific characteristics of each station. Thus, SENSE is not only expected to provide more reliable forecasts by considering multistation data but also has the ability to provide early warnings to distant areas that have not yet received signals. This study conducted extensive experiments on datasets from Taiwan and Japan. The results revealed that SENSE can deliver competitive or even better performances compared with other state-of-the-art methods. 

**Abstract (ZH)**: 地震早期预警系统在减少地震灾害风险中发挥着关键作用。过去，主流的建模系统是基于单站的模型。这些模型接收特定站台的数据并预测地球参数，如震P相到达时间、强度和震级。各种方法显示出足够的性能。然而，大多数方法面临的挑战包括难以加速警报时间、为远地区提供早期预警，以及考虑全球信息以提高性能。近年来，深度学习在许多领域产生了显著影响，包括地震学。因此，本文提出了一种基于深度学习的框架，称为SENSE，用于地震早期预警系统中的强度预测任务。为了从区域或国家层面明确考虑全局信息，SENSE的输入包括特定区域内或国家的一组站台的统计数据。SENSE模型设计用于学习一组输入站台及其特定局部特征之间的关系。因此，SENSE不仅期望通过多站数据提供更可靠的预报，还能为尚未接收到信号的远地区提供早期预警。本研究在台湾和日本的数据集上进行了广泛实验。结果显示，SENSE在与其他先进方法的对比中，可以提供具有竞争力甚至更好的性能。 

---
# LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting 

**Title (ZH)**: LangYa：重塑时空海洋预报 Área

将“LangYa: Revolutionizing Cross-Spatiotemporal Ocean Forecasting”翻译成中文，可以翻译为：

LangYa：革新跨时空海洋预报

这样翻译既符合学术规范，又保留了原文的核心意义。 

**Authors**: Nan Yang, Chong Wang, Meihua Zhao, Zimeng Zhao, Huiling Zheng, Bin Zhang, Jianing Wang, Xiaofeng Li  

**Link**: [PDF](https://arxiv.org/pdf/2412.18097)  

**Abstract**: Ocean forecasting is crucial for both scientific research and societal benefits. Currently, the most accurate forecasting systems are global ocean forecasting systems (GOFSs), which represent the ocean state variables (OSVs) as discrete grids and solve partial differential equations (PDEs) governing the transitions of oceanic state variables using numerical methods. However, GOFSs processes are computationally expensive and prone to cumulative errors. Recently, large artificial intelligence (AI)-based models significantly boosted forecasting speed and accuracy. Unfortunately, building a large AI ocean forecasting system that can be considered cross-spatiotemporal and air-sea coupled forecasts remains a significant challenge. Here, we introduce LangYa, a cross-spatiotemporal and air-sea coupled ocean forecasting system. Results demonstrate that the time embedding module in LangYa enables a single model to make forecasts with lead times ranging from 1 to 7 days. The air-sea coupled module effectively simulates air-sea interactions. The ocean self-attention module improves network stability and accelerates convergence during training, and the adaptive thermocline loss function improves the accuracy of thermocline forecasting. Compared to existing numerical and AI-based ocean forecasting systems, LangYa uses 27 years of global ocean data from the Global Ocean Reanalysis and Simulation version 12 (GLORYS12) for training and achieves more reliable deterministic forecasting results for OSVs. LangYa forecasting system provides global ocean researchers with access to a powerful software tool for accurate ocean forecasting and opens a new paradigm for ocean science. 

**Abstract (ZH)**: 海洋预报对于科学研究和社会福祉都至关重要。目前，最准确的预报系统是全球海洋预报系统（GOFSs），它们将海洋状态变量（OSVs）表示为离散网格，并使用数值方法求解描述海洋状态变量转换的偏微分方程（PDEs）。然而，GOFSs的处理过程计算成本高昂，且容易累积误差。近年来，大型人工智能（AI）基模型极大地提升了预报速度和准确性。不幸的是，构建一个具有跨空间时间能力和海气耦合预报能力的大规模AI海洋预报系统仍然是一项重大挑战。在此，我们介绍了LangYa，一种跨空间时间和海气耦合的海洋预报系统。结果表明，LangYa的时间嵌入模块使单个模型能够进行从1天到7天不等的预报。海气耦合模块有效地模拟了海气相互作用。海洋自注意力模块提高了网络的稳定性和训练期间的收敛速度，而自适应温跃层损失函数提高了温跃层预报的准确性。与现有的数值和AI基海洋预报系统相比，LangYa 使用来自GLORYS12版第12年全球海洋再分析和模拟数据的27年全球海洋数据进行训练，并取得了更可靠的确定性OSV预报结果。LangYa 预报系统为全球海洋研究人员提供了一个强大的软件工具，用于准确的海洋预报，并为海洋科学开辟了新的范式。 

---
# BRIDGE: Bundle Recommendation via Instruction-Driven Generation 

**Title (ZH)**: BRIDGE：基于指令驱动生成的束推荐方法 

**Authors**: Tuan-Nghia Bui, Huy-Son Nguyen, Cam-Van Nguyen Thi, Hoang-Quynh Le, Duc-Trong Le  

**Link**: [PDF](https://arxiv.org/pdf/2412.18092)  

**Abstract**: Bundle recommendation aims to suggest a set of interconnected items to users. However, diverse interaction types and sparse interaction matrices often pose challenges for previous approaches in accurately predicting user-bundle adoptions. Inspired by the distant supervision strategy and generative paradigm, we propose BRIDGE, a novel framework for bundle recommendation. It consists of two main components namely the correlation-based item clustering and the pseudo bundle generation modules. Inspired by the distant supervision approach, the former is to generate more auxiliary information, e.g., instructive item clusters, for training without using external data. This information is subsequently aggregated with collaborative signals from user historical interactions to create pseudo `ideal' bundles. This capability allows BRIDGE to explore all aspects of bundles, rather than being limited to existing real-world bundles. It effectively bridging the gap between user imagination and predefined bundles, hence improving the bundle recommendation performance. Experimental results validate the superiority of our models over state-of-the-art ranking-based methods across five benchmark datasets. 

**Abstract (ZH)**: 捆绑推荐旨在向用户推荐一组相互连接的项目。然而，多种交互类型和稀疏的交互矩阵往往给之前的推荐方法带来了准确预测用户-捆绑采用率的挑战。受远监督策略和生成范式的启发，我们提出了一种名为BRIDGE的新型捆绑推荐框架。该框架包含两个主要组成部分：基于相关性的项目聚类模块和伪捆绑生成模块。根据远监督方法的启发，前者用于生成更多的辅助信息，例如指示性项目簇，从而在不使用外部数据的情况下进行训练。这些信息随后与用户历史交互的协同信号相结合，生成伪“理想”捆绑。这种能力使BRIDGE能够探索捆绑的所有方面，而非仅局限于现有的实际情况中的捆绑。它有效地弥合了用户的想象与预定义捆绑之间的差距，从而提高了捆绑推荐性能。实验结果表明，我们的模型在五个基准数据集中优于最先进的基于排名的方法。 

---
# Multi-Point Positional Insertion Tuning for Small Object Detection 

**Title (ZH)**: 面向小目标检测的多点位置插入调优方法 

**Authors**: Kanoko Goto, Takumi Karasawa, Takumi Hirose, Rei Kawakami, Nakamasa Inoue  

**Link**: [PDF](https://arxiv.org/pdf/2412.18090)  

**Abstract**: Small object detection aims to localize and classify small objects within images. With recent advances in large-scale vision-language pretraining, finetuning pretrained object detection models has emerged as a promising approach. However, finetuning large models is computationally and memory expensive. To address this issue, this paper introduces multi-point positional insertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for small object detection. Specifically, MPI incorporates multiple positional embeddings into a frozen pretrained model, enabling the efficient detection of small objects by providing precise positional information to latent features. Through experiments, we demonstrated the effectiveness of the proposed method on the SODA-D dataset. MPI performed comparably to conventional PEFT methods, including CoOp and VPT, while significantly reducing the number of parameters that need to be tuned. 

**Abstract (ZH)**: 小目标检测旨在定位和分类图像中的小目标。随着大规模视力语言预训练技术的进展，微调预训练的检测模型已成为一种有前景的方法。然而，微调大规模模型在计算和内存上都十分昂贵。为了解决这一问题，本文引入了多点位置插入（MPI）微调，这是一种参数高效微调（PEFT）方法，适用于小目标检测。具体而言，MPI 将多个位置嵌入融入冻结的预训练模型中，通过为潜在特征提供精确的位置信息，实现对小目标的有效检测。实验结果表明，所提出的 MPI 方法在 SODA-D 数据集上表现出色，其性能与传统 PEFT 方法（如 CoOp 和 VPT）相当，同时显著减少了需要微调的参数数量。 

---
# Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner 

**Title (ZH)**: 通过情境内学习生成交通场景以提高运动规划器性能 

**Authors**: Aizierjiang Aiersilan  

**Link**: [PDF](https://arxiv.org/pdf/2412.18086)  

**Abstract**: Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model (LLM) translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at this https URL. 

**Abstract (ZH)**: 自主驾驶中的运动规划是其关键组成部分。当前最先进的运动规划算法是在精心标注的数据集上进行训练的，这些数据集不仅标注成本高昂，而且难以覆盖罕见但至关重要的场景。忽略这些场景会显著增加运动规划的风险，并可能导致测试过程中出现事故。一种直观的解决方案是通过编程和执行模拟器（例如CARLA）手动组成这些场景。然而，这种方法将付出高昂的人力成本。受此启发，我们提出了一种低成本的方法来生成多样化的关键交通场景，以训练更 robust 的运动规划器。首先，我们将交通场景转化为脚本，这些脚本随后被模拟器用来生成交通场景。接着，我们开发了一种方法，该方法接受用户指定的文字描述，然后通过上下文学习过程，由大规模语言模型（LLM）将其转化为符合要求的脚本。生成的脚本被发送给模拟器，以生产相应的交通场景。由于我们的方法能够生成大量的安全关键交通场景，我们使用这些场景作为运动规划器的合成训练数据。为了证明生成场景的价值，我们在合成数据、真实世界数据以及两者的组合上分别训练现有的运动规划器。实验结果表明，使用我们数据训练的运动规划器明显优于仅使用真实世界数据训练的运动规划器，这表明了我们合成数据的有效性以及数据生成方法的效果。我们的源代码可在此获取：[提供链接]。 

---
# Prompt Tuning for Item Cold-start Recommendation 

**Title (ZH)**: 为了解决物品冷启动推荐问题，本文提出了(prompt tuning)的方法。 

**Authors**: Yuezihan Jiang, Gaode Chen, Wenhan Zhang, Jingchi Wang, Yinjie Jiang, Qi Zhang, Jingjian Lin, Peng Jiang, Kaigui Bian  

**Link**: [PDF](https://arxiv.org/pdf/2412.18082)  

**Abstract**: The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios 

**Abstract (ZH)**: 冷启动问题对在线推荐系统至关重要，因为冷启动阶段的成功决定了项目能否过渡为热门项目。提示学习是一种强大的自然语言处理（NLP）技术，用于解决零样本或少样本问题，并已被应用于推荐系统以应对类似挑战。然而，现有的方法通常依赖于内容特征或文本描述进行提示，我们认为其对于冷启动推荐可能是次优的，原因包括：1）与推荐任务之间的语义差距，2）模型偏差，暖启动项目提供的积极反馈占据了模型大部分的正反馈，这是冷启动问题的核心，限制了推荐系统的质量。我们提出利用高价值的积极反馈，称为巅峰反馈，作为提示信息来同时解决上述两个问题。实验结果表明，相较于现有工作中提出的描述性内容，积极反馈更适合作为提示信息来弥合语义差距。此外，我们提出了基于项目的个性化提示网络，通过编码巅峰反馈来缓解由正反馈主导造成模型偏差的问题。在四个真实世界数据集上的广泛实验表明，我们的模型在多种商业度量指标上优于现有最先进的方法。此外，PENTRY系统已成功部署于一个亿万用户规模的热门短视频分享平台，实现了在冷启动场景下多个商业度量指标上的显著性能提升。 

---
# COMO: Cross-Mamba Interaction and Offset-Guided Fusion for Multimodal Object Detection 

**Title (ZH)**: COMO：跨Mamba交互与偏移向导融合在多模态目标检测中的应用 

**Authors**: Chang Liu, Xin Ma, Xiaochen Yang, Yuxiang Zhang, Yanni Dong  

**Link**: [PDF](https://arxiv.org/pdf/2412.18076)  

**Abstract**: Single-modal object detection tasks often experience performance degradation when encountering diverse scenarios. In contrast, multimodal object detection tasks can offer more comprehensive information about object features by integrating data from various modalities. Current multimodal object detection methods generally use various fusion techniques, including conventional neural networks and transformer-based models, to implement feature fusion strategies and achieve complementary information. However, since multimodal images are captured by different sensors, there are often misalignments between them, making direct matching challenging. This misalignment hinders the ability to establish strong correlations for the same object across different modalities. In this paper, we propose a novel approach called the CrOss-Mamba interaction and Offset-guided fusion (COMO) framework for multimodal object detection tasks. The COMO framework employs the cross-mamba technique to formulate feature interaction equations, enabling multimodal serialized state computation. This results in interactive fusion outputs while reducing computational overhead and improving efficiency. Additionally, COMO leverages high-level features, which are less affected by misalignment, to facilitate interaction and transfer complementary information between modalities, addressing the positional offset challenges caused by variations in camera angles and capture times. Furthermore, COMO incorporates a global and local scanning mechanism in the cross-mamba module to capture features with local correlation, particularly in remote sensing images. To preserve low-level features, the offset-guided fusion mechanism ensures effective multiscale feature utilization, allowing the construction of a multiscale fusion data cube that enhances detection performance. 

**Abstract (ZH)**: 单模态目标检测任务在遇到多样场景时常常会表现出性能下降。相比之下，多模态目标检测任务可以通过融合不同模态的数据来提供更加全面的目标特征信息。当前的多模态目标检测方法通常使用各种融合技术，包括传统的神经网络和基于变换器的模型，来实现特征融合策略并获得互补信息。然而，由于多模态图像由不同传感器捕获，常常存在模态之间的对齐问题，这使得直接匹配变得困难。这种对齐问题妨碍了在不同模态之间建立强相关性的能力。在这篇论文中，我们提出了一种名为CrOss-Mamba交互和偏移引导融合（COMO）框架的新方法，用于多模态目标检测任务。COMO框架采用交叉Mamba技术来构建特征交互方程，实现多模态分步状态计算，从而生成交互式的融合输出，同时减少计算开销并提高效率。此外，COMO利用高层特征，这些特征对对齐误差的影响较小，以促进模态间的交互和互补信息传递，从而解决由不同拍摄角度和时间引起的定位偏移问题。更重要的是，COMO在交叉Mamba模块中整合了全局与局部扫描机制，以捕捉远程感应图像中具有局部相关性的特征。通过偏移引导融合机制，确保有效利用多尺度特征，构建多尺度融合数据立方体，从而增强检测性能。 

---
# MMFactory: A Universal Solution Search Engine for Vision-Language Tasks 

**Title (ZH)**: MMFactory：视觉-语言任务的通用解决方案搜索引擎 

**Authors**: Wan-Cyuan Fan, Tanzila Rahman, Leonid Sigal  

**Link**: [PDF](https://arxiv.org/pdf/2412.18072)  

**Abstract**: With advances in foundational and vision-language models, and effective fine-tuning techniques, a large number of both general and special-purpose models have been developed for a variety of visual tasks. Despite the flexibility and accessibility of these models, no single model is able to handle all tasks and/or applications that may be envisioned by potential users. Recent approaches, such as visual programming and multimodal LLMs with integrated tools aim to tackle complex visual tasks, by way of program synthesis. However, such approaches overlook user constraints (e.g., performance / computational needs), produce test-time sample-specific solutions that are difficult to deploy, and, sometimes, require low-level instructions that maybe beyond the abilities of a naive user. To address these limitations, we introduce MMFactory, a universal framework that includes model and metrics routing components, acting like a solution search engine across various available models. Based on a task description and few sample input-output pairs and (optionally) resource and/or performance constraints, MMFactory can suggest a diverse pool of programmatic solutions by instantiating and combining visio-lingual tools from its model repository. In addition to synthesizing these solutions, MMFactory also proposes metrics and benchmarks performance / resource characteristics, allowing users to pick a solution that meets their unique design constraints. From the technical perspective, we also introduced a committee-based solution proposer that leverages multi-agent LLM conversation to generate executable, diverse, universal, and robust solutions for the user. Experimental results show that MMFactory outperforms existing methods by delivering state-of-the-art solutions tailored to user problem specifications. Project page is available at this https URL. 

**Abstract (ZH)**: 随着基础模型和视觉语言模型的进步，以及有效微调技术的发展，已经开发出了多种通用和专用模型，适用于各种视觉任务。尽管这些模型具备高度的灵活性和可访问性，但没有任何一个模型能够处理所有任务和潜在用户可能预见到的所有应用场景。近期的解决方案，如视觉编程和集成了工具的多模态大语言模型，旨在通过程序合成来应对复杂的视觉任务，但这些方法通常未能考虑用户约束（如性能/计算需求），在测试时生成特定样本的解决方案，这些解决方案难以部署，有时还要求用户提供低级别的指令，这可能超出普通用户的操作能力。为了弥补这些局限性，我们引入了MMFactory，这是一个通用框架，包含模型和度量路由组件，能够跨多种可用模型充当解决方案搜索引擎的角色。基于任务描述和少量样本输入-输出对，以及（可选地）资源和/或性能约束，MMFactory 可以通过实例化和结合其模型库中的视觉-语言工具来建议多种程序化的解决方案。除了生成这些解决方案外，MMFactory 还提出了一系列表征标准并评估性能/资源特性，让用户能够根据自己的独特设计约束来选择合适的解决方案。从技术角度来看，我们还引入了一种基于多智能体大语言模型对话的委员会解决方案生成器，该生成器能够在生成可执行的、多样化的、通用的和鲁棒性的用户解决方案方面发挥作用。实验结果表明，MMFactory 能够通过为用户特别定制最先进的解决方案来超越现有方法。项目页面可在此访问：[请填入网址]。 

---
# Automated Materials Discovery Platform Realized: Scanning Probe Microscopy of Combinatorial Libraries 

**Title (ZH)**: 自动化材料发现平台实现：组合库的扫描探针显微镜表征 

**Authors**: Yu Liu, Rohit Pant, Ichiro Takeuchi, R. Jackson Spurling, Jon-Paul Maria, Maxim Ziatdinov, Sergei V. Kalinin  

**Link**: [PDF](https://arxiv.org/pdf/2412.18067)  

**Abstract**: Combinatorial libraries are a powerful approach for exploring the evolution of physical properties across binary and ternary cross-sections in multicomponent phase diagrams. Although the synthesis of these libraries has been developed since the 1960s and expedited with advanced laboratory automation, the broader application of combinatorial libraries relies on fast, reliable measurements of concentration-dependent structures and functionalities. Scanning Probe Microscopies (SPM), including piezoresponse force microscopy (PFM), offer significant potential for quantitative, functionally relevant combi-library readouts. Here we demonstrate the implementation of fully automated SPM to explore the evolution of ferroelectric properties in combinatorial libraries, focusing on Sm-doped BiFeO3 and ZnxMg1-xO systems. We also present and compare Gaussian Process-based Bayesian Optimization models for fully automated exploration, emphasizing local reproducibility (effective noise) as an essential factor in optimal experiment workflows. Automated SPM, when coupled with upstream synthesis controls, plays a pivotal role in bridging materials synthesis and characterization. 

**Abstract (ZH)**: 组合库是一种强有力的方法，用于探索二元和三元截面上多组分相图中物理性质的演变。尽管自20世纪60年代以来组合库的合成已经发展起来，并且随着先进的实验室自动化而加速，但组合库的更广泛应用依赖于快速可靠的结构和功能浓度依赖性测量。扫描探针显微镜（SPM），包括压电响应力显微镜（PFM），为定量的功能相关组合库读出提供了巨大潜力。在此，我们展示了自动化SPM的实施，用于探索Sm掺杂的BiFeO3和ZnxMg1-xO系统中铁电性质的演变。我们还介绍了并比较了基于高斯过程的贝叶斯优化模型，强调局部可重复性（有效噪声）是最佳实验工作流程中的一个必不可少因素。当耦合上游合成控制时，自动化SPM在材料合成和表征之间发挥着关键作用。 

---
# Neuron Empirical Gradient: Connecting Neurons' Linear Controllability and Representational Capacity 

**Title (ZH)**: 神经元经验梯度：连接神经元的线性可控性和表示能力 

**Authors**: Xin Zhao, Zehui Jiang, Naoki Yoshinaga  

**Link**: [PDF](https://arxiv.org/pdf/2412.18053)  

**Abstract**: Although neurons in the feed-forward layers of pre-trained language models (PLMs) can store factual knowledge, most prior analyses remain qualitative, leaving the quantitative relationship among knowledge representation, neuron activations, and model output poorly understood. In this study, by performing neuron-wise interventions using factual probing datasets, we first reveal the linear relationship between neuron activations and output token probabilities. We refer to the gradient of this linear relationship as ``neuron empirical gradients.'' and propose NeurGrad, an efficient method for their calculation to facilitate quantitative neuron analysis. We next investigate whether neuron empirical gradients in PLMs encode general task knowledge by probing skill neurons. To this end, we introduce MCEval8k, a multi-choice knowledge evaluation benchmark spanning six genres and 22 tasks. Our experiments confirm that neuron empirical gradients effectively capture knowledge, while skill neurons exhibit efficiency, generality, inclusivity, and interdependency. These findings link knowledge to PLM outputs via neuron empirical gradients, shedding light on how PLMs store knowledge. The code and dataset are released. 

**Abstract (ZH)**: 尽管预训练语言模型（PLMs）中的前向层神经元可以存储事实性知识，但大多数先前分析仍停留在定性层面，定量关系（知识表示、神经元激活与模型输出之间的关系）仍然不甚明了。在本研究中，通过使用事实性探针数据集进行神经元级别的干预，我们首次揭示了神经元激活值与输出标记概率之间的线性关系。我们将这一线性关系的梯度称为“神经元经验梯度”，并提出了一种名为NeurGrad的有效计算方法，以促进对神经元的定量分析。我们接下来研究PLMs中的神经元经验梯度是否编码了普遍任务知识，为此，我们引入了MCEval8k多选择知识评价基准，该基准涵盖了六个体裁和22个任务。我们的实验结果证实，神经元经验梯度有效地捕捉了知识，而技能神经元则表现出高效性、普遍性、包容性及相互依赖性。这些发现通过神经元经验梯度将知识与PLM输出联系起来，揭示了PLMs存储知识的方式。代码和数据集已公开发布。 

---
# Beyond Gradient Averaging in Parallel Optimization: Improved Robustness through Gradient Agreement Filtering 

**Title (ZH)**: 超越并行优化中的梯度平均化：通过梯度一致性筛选提高鲁棒性 

**Authors**: Francois Chaubard, Duncan Eddy, Mykel J. Kochenderfer  

**Link**: [PDF](https://arxiv.org/pdf/2412.18052)  

**Abstract**: We introduce Gradient Agreement Filtering (GAF) to improve on gradient averaging in distributed deep learning optimization. Traditional distributed data-parallel stochastic gradient descent involves averaging gradients of microbatches to calculate a macrobatch gradient that is then used to update model parameters. We find that gradients across microbatches are often orthogonal or negatively correlated, especially in late stages of training, which leads to memorization of the training set, reducing generalization. In this paper, we introduce a simple, computationally effective way to reduce gradient variance by computing the cosine distance between micro-gradients during training and filtering out conflicting updates prior to averaging. We improve validation accuracy with significantly smaller microbatch sizes. We also show this reduces memorizing noisy labels. We demonstrate the effectiveness of this technique on standard image classification benchmarks including CIFAR-100 and CIFAR-100N-Fine. We show this technique consistently outperforms validation accuracy, in some cases by up to 18.2\% compared to traditional training approaches while reducing the computation required nearly an order of magnitude because we can now rely on smaller microbatch sizes without destabilizing training. 

**Abstract (ZH)**: 我们将介绍梯度一致性过滤（GAF）方法，以改进分布式深度学习优化中的梯度平均。传统的分布式数据并行随机梯度下降涉及通过计算小批量梯度的平均值来获得宏批量梯度，进而更新模型参数。我们发现，在训练后期，小批量梯度往往相互正交或负相关，这导致训练集的过度拟合，降低了通用性。在本文中，我们提出了一种简单且计算效率高的方法，通过在训练过程中计算微梯度之间的余弦距离并在平均前过滤出冲突的更新来减少梯度方差。我们通过使用显著较小的小批量尺寸提高了验证精度，同时也减少了对噪声标签的记忆。我们展示了这一技术在标准图像分类基准数据集（包括CIFAR-100和CIFAR-100N-Fine）上的有效性。此外，我们证明了这种方法在某些情况下比传统训练方法提高了高达18.2%的验证精度，并且由于可以依赖于较小的小批量尺寸而不致训练不稳定，所需计算量减少了近一个数量级。 

---
# Fair Knowledge Tracing in Second Language Acquisition 

**Title (ZH)**: 公平的知识追踪在第二语言习得中的应用 

**Authors**: Weitao Tang, Guanliang Chen, Shuaishuai Zu, Jiangyi Luo  

**Link**: [PDF](https://arxiv.org/pdf/2412.18048)  

**Abstract**: In second-language acquisition, predictive modeling aids educators in implementing diverse teaching strategies, attracting significant research attention. However, while model accuracy is widely explored, model fairness remains under-examined. Model fairness ensures equitable treatment of groups, preventing unintentional biases based on attributes such as gender, ethnicity, or economic background. A fair model should produce impartial outcomes that do not systematically disadvantage any group.
This study evaluates the fairness of two predictive models using the Duolingo dataset's en\_es (English learners speaking Spanish), es\_en (Spanish learners speaking English), and fr\_en (French learners speaking English) tracks. We analyze: 1. Algorithmic fairness across platforms (iOS, Android, Web). 2. Algorithmic fairness between developed and developing countries.
Key findings include: 1. Deep learning outperforms machine learning in second-language knowledge tracing due to improved accuracy and fairness. 2. Both models favor mobile users over non-mobile users. 3. Machine learning exhibits stronger bias against developing countries compared to deep learning. 4. Deep learning strikes a better balance of fairness and accuracy in the en\_es and es\_en tracks, while machine learning is more suitable for fr\_en.
This study highlights the importance of addressing fairness in predictive models to ensure equitable educational strategies across platforms and regions. 

**Abstract (ZH)**: 在二语习得领域，预测模型帮助教育工作者实施多样化的教学策略，吸引了广泛关注。然而，尽管模型准确性得到了广泛研究，但模型公平性仍然缺乏关注。模型公平性确保对不同群体的公平对待，防止基于性别、种族或经济背景等因素的无意偏见。一个公平的模型应该产生公正的结果，不会系统性地对任何群体造成不利影响。

本研究使用Duolingo数据集中的en_es（英语学习者使用西班牙语）、es_en（西班牙语学习者使用英语）和fr_en（法语学习者使用英语）三条轨道来评估两个预测模型的公平性。我们分析了以下内容：1. 不同平台（iOS、Android、Web）上的算法公平性；2. 发达国家和发展中国家之间的算法公平性。

主要发现包括：1. 深度学习在二语知识追踪方面优于机器学习，因为其在准确性和公平性方面表现更优。2. 两种模型更偏好移动用户而非非移动用户。3. 机器学习在对发展中国家的偏见方面比深度学习更强。4. 在en_es和es_en轨道上，深度学习在公平性和准确性之间取得了更好的平衡，而在fr_en轨道上，机器学习更为合适。

本研究强调，在不同平台和地区确保公平教育策略的重要性，需关注预测模型的公平性问题。 

---
# Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control 

**Title (ZH)**: 面向层次化多代理电动汽车充电控制的不确定性感知评论器增强方法 

**Authors**: Lo Pang-Yun Ting, Ali Şenol, Huan-Yang Wang, Hsu-Chao Lai, Kun-Ta Chuang, Huan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2412.18047)  

**Abstract**: The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only effectively reduces electricity expenses but also enhances the resilience of handling practical issues, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. However, existing EV charging strategies have yet to fully consider these factors in a way that benefits both office buildings and EV users simultaneously. To address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and electric vehicles. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, a new critic augmentation is introduced to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information. 

**Abstract (ZH)**: 面向支持电网稳定性和应急操作的高级双向电动汽车充放电技术，已在工作场所应用中引发了越来越大的兴趣。它不仅有效降低了电费，还增强了处理尖峰功率限制、波动的能源价格以及不可预测的电动汽车离场等问题的能力。然而，现有的电动汽车充电策略尚未全面考虑这些因素，以同时惠及办公楼和电动汽车用户。为解决这些问题，我们提出了HUCA，这是一种新颖的实时充电控制方法，用于调节建筑和电动汽车的能源需求。HUCA采用分层actor-critic网络，根据动态定价场景的需求动态减少建筑的电力成本。为了应对电动汽车离场的不确定性，引入了一种新的critic增强技术，以在评估充电决策时考虑离场不确定性，同时保持充电控制的稳健性。在具有模拟确定性和不确定性离场场景的实际电力数据集上的实验表明，与基线方法相比，HUCA在总体电力成本方面表现出更优的效果，同时在满足电动汽车充电需求方面维持了竞争力的表现。案例研究还表明，HUCA能够根据实时信息有效地在建筑和电动汽车之间平衡能源供应。 

---
# Emoji Retrieval from Gibberish or Garbled Social Media Text: A Novel Methodology and A Case Study 

**Title (ZH)**: 从乱码或 garbled 社交媒体文本中检索 Emoji：一种新颖的方法论与案例研究 

**Authors**: Shuqi Cui, Nirmalya Thakur, Audrey Poon  

**Link**: [PDF](https://arxiv.org/pdf/2412.18046)  

**Abstract**: Emojis are widely used across social media platforms but are often lost in noisy or garbled text, posing challenges for data analysis and machine learning. Conventional preprocessing approaches recommend removing such text, risking the loss of emojis and their contextual meaning. This paper proposes a three-step reverse-engineering methodology to retrieve emojis from garbled text in social media posts. The methodology also identifies reasons for the generation of such text during social media data mining. To evaluate its effectiveness, the approach was applied to 509,248 Tweets about the Mpox outbreak, a dataset referenced in about 30 prior works that failed to retrieve emojis from garbled text. Our method retrieved 157,748 emojis from 76,914 Tweets. Improvements in text readability and coherence were demonstrated through metrics such as Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, Automated Readability Index, Dale-Chall Readability Score, Text Standard, and Reading Time. Additionally, the frequency of individual emojis and their patterns of usage in these Tweets were analyzed, and the results are presented. 

**Abstract (ZH)**: 以下是经过学术规范化翻译后的中文内容：

表情符号在各种社交媒体平台中被广泛使用，但在噪声较大或文字杂乱的情况下，表情符号常常被丢失，这对数据的分析和机器学习构成了挑战。传统的预处理方法建议去除这些文本，但这可能会导致表情符号及其上下文意义的丢失。本文提出了一种三步逆向工程方法，以从社交媒体帖子中的杂乱文本中检索表情符号。该方法还识别了社交媒体数据挖掘过程中生成此类文本的原因。为了评估其有效性，该方法应用于关于 Mpox 爆发的509,248条推文，这些推文涉及大约30项先前工作，其中多项未能从杂乱文本中检索出表情符号。我们的方法成功从76,914条推文中检索出157,748个表情符号。通过使用诸如弗里谢可读性、弗里谢-金凯德读写级别、科尔曼-赖厄指数、自动读写指数、戴尔-查尔可读性评分、文本标准和阅读时间等指标，我们证明了文本可读性和连贯性的改进。此外，还分析了这些推文中单个表情符号的频率及其使用模式，并展示了结果。 

---
# Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review 

**Title (ZH)**: 将AI研究与临床编码工作流程需求对齐：基于美国数据分析和批判性审查的八项建议 

**Authors**: Yidong Gan, Maciej Rybinski, Ben Hachey, Jonathan K. Kummerfeld  

**Link**: [PDF](https://arxiv.org/pdf/2412.18043)  

**Abstract**: Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows. 

**Abstract (ZH)**: 临床编码对于医疗收费和数据分析至关重要。手动临床编码劳动密集且容易出错，这推动了对整个过程自动化的研究。然而，我们基于美国英语电子健康记录和这些记录的自动化编码研究的分析表明，广泛使用的评估方法并不符合实际的临床环境。例如，专注于最常见的50个代码的评估是一个简化的方法，因为在实践中使用了数千种不同的代码。本文旨在使人工智能编码研究更加贴近临床编码的实际挑战。基于我们的分析，我们提出了八项具体建议，以改善当前的评估方法。此外，我们还提出了新的基于人工智能的方法，超越了自动化编码，提供了辅助临床编码员工作流的替代方法。 

---
# Theoretical Constraints on the Expressive Power of $\mathsf{RoPE}$-based Tensor Attention Transformers 

**Title (ZH)**: 基于RoPE的张量注意力变换器的表达能力的理论约束 

**Authors**: Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Mingda Wan  

**Link**: [PDF](https://arxiv.org/pdf/2412.18040)  

**Abstract**: Tensor Attention extends traditional attention mechanisms by capturing high-order correlations across multiple modalities, addressing the limitations of classical matrix-based attention. Meanwhile, Rotary Position Embedding ($\mathsf{RoPE}$) has shown superior performance in encoding positional information in long-context scenarios, significantly enhancing transformer models' expressiveness. Despite these empirical successes, the theoretical limitations of these technologies remain underexplored. In this study, we analyze the circuit complexity of Tensor Attention and $\mathsf{RoPE}$-based Tensor Attention, showing that with polynomial precision, constant-depth layers, and linear or sublinear hidden dimension, they cannot solve fixed membership problems or $(A_{F,r})^*$ closure problems, under the assumption that $\mathsf{TC}^0 \neq \mathsf{NC}^1$. These findings highlight a gap between the empirical performance and theoretical constraints of Tensor Attention and $\mathsf{RoPE}$-based Tensor Attention Transformers, offering insights that could guide the development of more theoretically grounded approaches to Transformer model design and scaling. 

**Abstract (ZH)**: 张量注意力机制通过捕获多模态间高阶相关性，扩展了传统注意力机制，解决了经典矩阵基注意力的局限性。同时，旋转位置嵌入（$\mathsf{RoPE}$）在长上下文场景中表现出色，显著增强了变压器模型的表征能力。尽管这些技术在实际应用中取得了成功，但它们的理论局限性仍需进一步探索。在本研究中，我们分析了张量注意力机制和基于$\mathsf{RoPE}$的张量注意力机制的电路复杂度，证明在多项式精度、常数深度层及线性或亚线性隐藏维度条件下，它们在假设$\mathsf{TC}^0 \neq \mathsf{NC}^1$的前提下，无法解决固定成员问题或$(A_{F,r})^*$闭包问题。这些发现揭示了张量注意力机制和基于$\mathsf{RoPE}$的张量注意力机制变压器在实验性能与理论约束之间的差距，为变压器模型设计和扩展提供了更理论化的指导线索。 

---
# AA-SGAN: Adversarially Augmented Social GAN with Synthetic Data 

**Title (ZH)**: AA-SGAN：带有合成数据的 adversarial 增强社会生成网络 

**Authors**: Mirko Zaffaroni, Federico Signoretta, Marco Grangetto, Attilio Fiandrotti  

**Link**: [PDF](https://arxiv.org/pdf/2412.18038)  

**Abstract**: Accurately predicting pedestrian trajectories is crucial in applications such as autonomous driving or service robotics, to name a few. Deep generative models achieve top performance in this task, assuming enough labelled trajectories are available for training. To this end, large amounts of synthetically generated, labelled trajectories exist (e.g., generated by video games). However, such trajectories are not meant to represent pedestrian motion realistically and are ineffective at training a predictive model. We propose a method and an architecture to augment synthetic trajectories at training time and with an adversarial approach. We show that trajectory augmentation at training time unleashes significant gains when a state-of-the-art generative model is evaluated over real-world trajectories. 

**Abstract (ZH)**: 准确预测行人的轨迹对于自动驾驶或服务机器人等应用至关重要。深度生成模型在此任务中表现出色，前提是可用于训练的足够的标注轨迹数据可用。为此，大量合成生成且标注的轨迹数据已存在（例如，由视频游戏生成）。然而，这些轨迹数据并非旨在真实地代表行人的运动，并且对训练预测模型效果不佳。我们提出了一种方法和架构，在训练时增强合成轨迹，并采用对抗方法。我们表明，在评估最先进的生成模型时，训练时轨迹增强可以带来显著的提升，特别是在实际应用场景中的轨迹上。 

---
# Explainability in Neural Networks for Natural Language Processing Tasks 

**Title (ZH)**: 神经网络在自然语言处理任务中的可解释性 

**Authors**: Melkamu Mersha, Mingiziem Bitewa, Tsion Abay, Jugal Kalita  

**Link**: [PDF](https://arxiv.org/pdf/2412.18036)  

**Abstract**: Neural networks are widely regarded as black-box models, creating significant challenges in understanding their inner workings, especially in natural language processing (NLP) applications. To address this opacity, model explanation techniques like Local Interpretable Model-Agnostic Explanations (LIME) have emerged as essential tools for providing insights into the behavior of these complex systems. This study leverages LIME to interpret a multi-layer perceptron (MLP) neural network trained on a text classification task. By analyzing the contribution of individual features to model predictions, the LIME approach enhances interpretability and supports informed decision-making. Despite its effectiveness in offering localized explanations, LIME has limitations in capturing global patterns and feature interactions. This research highlights the strengths and shortcomings of LIME and proposes directions for future work to achieve more comprehensive interpretability in neural NLP models. 

**Abstract (ZH)**: 神经网络通常被视为黑盒模型，这在理解其内部工作机制方面造成了显著挑战，尤其是在自然语言处理（NLP）应用中。为了应对这一透明度不足的问题，局部可解释的模型无关解释（LIME）等模型解释技术已经成为了提供这些复杂系统行为见解的重要工具。本研究利用LIME来解释一个用于文本分类任务的多层感知机（MLP）神经网络。通过分析个别特征对模型预测的贡献，LIME方法提高了可解释性并支持了基于信息的决策制定。尽管LIME在提供局部解释方面非常有效，但它在捕获全局模式和特征交互方面存在一定局限性。本文强调了LIME的优势和不足之处，并提出了未来工作方向，以实现神经NLP模型更全面的可解释性。 

---
# More than Chit-Chat: Developing Robots for Small-Talk Interactions 

**Title (ZH)**: 不仅仅是闲聊：开发用于小型对话的机器人 

**Authors**: Rebecca Ramnauth, Dražen Brščić, Brian Scassellati  

**Link**: [PDF](https://arxiv.org/pdf/2412.18023)  

**Abstract**: Beyond mere formality, small talk plays a pivotal role in social dynamics, serving as a verbal handshake for building rapport and understanding. For conversational AI and social robots, the ability to engage in small talk enhances their perceived sociability, leading to more comfortable and natural user interactions. In this study, we evaluate the capacity of current Large Language Models (LLMs) to drive the small talk of a social robot and identify key areas for improvement. We introduce a novel method that autonomously generates feedback and ensures LLM-generated responses align with small talk conventions. Through several evaluations -- involving chatbot interactions and human-robot interactions -- we demonstrate the system's effectiveness in guiding LLM-generated responses toward realistic, human-like, and natural small-talk exchanges. 

**Abstract (ZH)**: 超越 mere formality，闲聊在社会动态中扮演着至关重要的角色，它作为一种口头“握手”手段，有助于建立关系和增进理解。对于对话式人工智能和社交机器人而言，具备进行闲聊的能力能够提升其社会性，从而使用户交互更加舒适和自然。在这项研究中，我们评估了当前大型语言模型（LLMs）驱动社交机器人闲聊的能力，并确定了改进的关键领域。我们提出了一种新颖的方法，可以自主生成反馈并确保大型语言模型生成的回应符合闲聊的惯例。通过几次评估——包括聊天机器人交互和人机交互——我们展示了该系统引导大型语言模型生成的回应向现实、人性化的自然闲聊交流方向发展的效果。 

---
# Trustworthy and Efficient LLMs Meet Databases 

**Title (ZH)**: 可信且高效的大型语言模型与数据库相遇 

**Authors**: Kyoungmin Kim, Anastasia Ailamaki  

**Link**: [PDF](https://arxiv.org/pdf/2412.18022)  

**Abstract**: In the rapidly evolving AI era with large language models (LLMs) at the core, making LLMs more trustworthy and efficient, especially in output generation (inference), has gained significant attention. This is to reduce plausible but faulty LLM outputs (a.k.a hallucinations) and meet the highly increased inference demands. This tutorial explores such efforts and makes them transparent to the database community. Understanding these efforts is essential in harnessing LLMs in database tasks and adapting database techniques to LLMs. Furthermore, we delve into the synergy between LLMs and databases, highlighting new opportunities and challenges in their intersection. This tutorial aims to share with database researchers and practitioners essential concepts and strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining in the intersection between LLMs and databases. 

**Abstract (ZH)**: 在以大规模语言模型（LLMs）为核心迅速发展的AI时代，提高LLMs的可信度和效率，尤其是在输出生成（推理）方面，受到了广泛关注。这旨在减少可能但错误的LLM输出（即幻觉），并满足大幅增加的推理需求。本教程探讨了这些努力，并将其透明化给数据库社区。理解这些努力对于利用LLMs进行数据库任务以及将数据库技术适应LLMs至关重要。此外，我们探讨了LLMs与数据库之间的协同作用，在其交汇点上突出新的机遇和挑战。本教程旨在向数据库研究人员和实践者分享关于LLMs的核心概念和策略，减少对LLMs的陌生感，并激励他们参与到LLMs与数据库的交汇领域中。 

---
# Integrated Learning and Optimization for Congestion Management and Profit Maximization in Real-Time Electricity Market 

**Title (ZH)**: 实时电力市场中 congestion 管理与利润最大化集成学习与优化方法 

**Authors**: Imran Pervez, Ricardo Pinto Lima, Omar Knio  

**Link**: [PDF](https://arxiv.org/pdf/2412.18003)  

**Abstract**: We develop novel integrated learning and optimization (ILO) methodologies to solve economic dispatch (ED) and DC optimal power flow (DCOPF) problems for better economic operation. The optimization problem for ED is formulated with load being an unknown parameter while DCOPF consists of load and power transfer distribution factor (PTDF) matrix as unknown parameters. PTDF represents the incremental variations of real power on transmission lines which occur due to real power transfers between two regions. These values represent a linearized approximation of power flows over the transmission lines. We develop novel ILO formulations to solve post-hoc penalties in electricity market and line congestion problems using ED and DCOPF optimization formulations. Our proposed methodologies capture the real-time electricity market and line congestion behavior to train the regret function which eventually train unknown loads at different buses and line PTDF matrix to achieve the afore-mentioned post-hoc goals. The proposed methodology is compared to sequential learning and optimization (SLO) which train load and PTDF forecasts for accuracy rather than economic operation. Our experimentation prove the superiority of ILO in minimizing the post-hoc penalties in electricity markets and minimizing the line congestion thereby improving the economic operation with noticeable amount. 

**Abstract (ZH)**: 我们开发了新颖的集成学习与优化（ILO）方法论，以解决经济调度（ED）和直流最优潮流（DCOPF）问题，从而实现更高效的经济运行。经济调度（ED）的优化问题根据负荷作为未知参数进行建模，而DCOPF包括负荷和功率传输分布因子（PTDF）矩阵作为未知参数。PTDF表示由于两个区域之间实功率转移导致的传输线实功率的增量变化，这些值代表了传输线上功率流动的近似线性模型。我们开发了新的ILO建模方法，以利用ED和DCOPF优化建模来解决事后惩罚在电力市场和线路阻塞问题。所提出的方法能够捕捉实时电力市场和线路阻塞行为，从而训练后悔函数，最终训练各母线上的未知负荷和线路PTDF矩阵，以实现上述事后目标。我们还将所提方法与序列学习和优化（SLO）进行了比较，SLO侧重于根据准确性而不是经济运行来训练负荷和PTDF预测。我们的实验表明，ILO在减少电力市场的事后惩罚、减少线路阻塞方面具有优越性，并在经济运行方面取得了显著改进。 

---
# WavePulse: Real-time Content Analytics of Radio Livestreams 

**Title (ZH)**: WavePulse: 实时无线直播内容分析 

**Authors**: Govind Mittal, Sarthak Gupta, Shruti Wagle, Chirag Chopra, Anthony J DeMattee, Nasir Memon, Mustaque Ahamad, Chinmay Hegde  

**Link**: [PDF](https://arxiv.org/pdf/2412.17998)  

**Abstract**: Radio remains a pervasive medium for mass information dissemination, with AM/FM stations reaching more Americans than either smartphone-based social networking or live television. Increasingly, radio broadcasts are also streamed online and accessed over the Internet. We present WavePulse, a framework that records, documents, and analyzes radio content in real-time. While our framework is generally applicable, we showcase the efficacy of WavePulse in a collaborative project with a team of political scientists focusing on the 2024 Presidential Elections. We use WavePulse to monitor livestreams of 396 news radio stations over a period of three months, processing close to 500,000 hours of audio streams. These streams were converted into time-stamped, diarized transcripts and analyzed to track answer key political science questions at both the national and state levels. Our analysis revealed how local issues interacted with national trends, providing insights into information flow. Our results demonstrate WavePulse's efficacy in capturing and analyzing content from radio livestreams sourced from the Web. Code and dataset can be accessed at \url{this https URL}. 

**Abstract (ZH)**: 以下是经过学术规范翻译后的中文版内容：

广播仍然是广泛信息传播的一种普遍媒介，与基于智能手机的社交网络或现场电视相比，AM/FM电台覆盖的美国人更多。越来越多地，广播内容也开始通过互联网进行实时流媒体播放。我们提出了WavePulse框架，用于实时记录、文档化和分析广播内容。尽管该框架具有广泛的适用性，但我们在一项与一组政治学家合作的项目中展示了WavePulse在2024年总统选举研究中的有效性。我们使用WavePulse监控了三个月内来自396家新闻电台的实时流媒体内容，处理了接近50万小时的音频流。这些流媒体内容被转换成带有时间标记的详录转录文本，并进行分析，以追踪有关国家和州层面的关键政治科学问题。我们的分析揭示了地方问题与国家趋势之间的相互作用，提供了关于信息流动的见解。我们的结果证明了WavePulse在捕获和分析来自互联网的广播实时流媒体内容方面的有效性。代码和数据集可访问于此网址：\url{this https URL}。 

---
# Multi-Agent Path Finding in Continuous Spaces with Projected Diffusion Models 

**Title (ZH)**: 连续空间中基于投影扩散模型的多代理路径规划 

**Authors**: Jinhao Liang, Jacob K. Christopher, Sven Koenig, Ferdinando Fioretto  

**Link**: [PDF](https://arxiv.org/pdf/2412.17993)  

**Abstract**: Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics, requiring the computation of collision-free paths for multiple agents moving from their respective start to goal positions. Coordinating multiple agents in a shared environment poses significant challenges, especially in continuous spaces where traditional optimization algorithms struggle with scalability. Moreover, these algorithms often depend on discretized representations of the environment, which can be impractical in image-based or high-dimensional settings. Recently, diffusion models have shown promise in single-agent path planning, capturing complex trajectory distributions and generating smooth paths that navigate continuous, high-dimensional spaces. However, directly extending diffusion models to MAPF introduces new challenges since these models struggle to ensure constraint feasibility, such as inter-agent collision avoidance. To overcome this limitation, this work proposes a novel approach that integrates constrained optimization with diffusion models for MAPF in continuous spaces. This unique combination directly produces feasible multi-agent trajectories that respect collision avoidance and kinematic constraints. The effectiveness of our approach is demonstrated across various challenging simulated scenarios of varying dimensionality. 

**Abstract (ZH)**: 多智能体路径规划（Multi-Agent Path Finding, MAPF）是机器人技术中的一个基本问题，要求为多个从各自的起点到终点移动的智能体计算无碰撞路径。在共享环境中协调多个智能体特别是在连续空间中提出了重大挑战，传统优化算法在可扩展性方面尤为棘手。此外，这些算法往往依赖于对环境的离散化表示，这在基于图像或高维设置中可能不切实际。最近，扩散模型在单智能体路径规划中显示出潜力，能够捕捉复杂轨迹分布并生成流畅路径，以导航连续的高维空间。然而，直接将扩散模型扩展到MAPF引入了新的挑战，因为这些模型难以确保满足约束条件，例如智能体间的碰撞避免。为克服这一点，本文提出了一种新的方法，将约束优化与扩散模型结合用于连续空间中的MAPF。这种独特的组合直接产生了满足碰撞避免和动力学约束的可行多智能体轨迹。通过在不同维度的多种具有挑战性的仿真场景中进行验证，展示了我们方法的有效性。 

---
# ICPR 2024 Competition on Domain Adaptation and GEneralization for Character Classification (DAGECC) 

**Title (ZH)**: ICPR 2024 在字符分类中的领域适应与泛化竞赛 (DAGECC) 

**Authors**: Sofia Marino, Jennifer Vandoni, Emanuel Aldea, Ichraq Lemghari, Sylvie Le Hégarat-Mascle, Frédéric Jurie  

**Link**: [PDF](https://arxiv.org/pdf/2412.17984)  

**Abstract**: In this companion paper for the DAGECC (Domain Adaptation and GEneralization for Character Classification) competition organized within the frame of the ICPR 2024 conference, we present the general context of the tasks we proposed to the community, we introduce the data that were prepared for the competition and we provide a summary of the results along with a description of the top three winning entries. The competition was centered around domain adaptation and generalization, and our core aim is to foster interest and facilitate advancement on these topics by providing a high-quality, lightweight, real world dataset able to support fast prototyping and validation of novel ideas. 

**Abstract (ZH)**: 本文作为ICPR 2024大会上组织的DAGECC（领域适应和通用性面向字符分类）竞赛的配套论文，我们介绍了为该会议竞赛设计的任务背景，介绍了为竞赛准备的数据，并提供了比赛结果的总结，同时还对前三名获胜作品进行了描述。该竞赛围绕领域适应和通用性展开，我们的核心目的是通过提供一个高质量、轻量化且具有真实世界应用背景的数据集，促进兴趣并推动这些领域的研究进展，从而支持新想法的快速原型制作和验证。 

---
# TNNGen: Automated Design of Neuromorphic Sensory Processing Units for Time-Series Clustering 

**Title (ZH)**: TNNGen: 自动设计的时间序列聚类神经形态感知处理单元 

**Authors**: Prabhu Vellaisamy, Harideep Nair, Vamsikrishna Ratnakaram, Dhruv Gupta, John Paul Shen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17977)  

**Abstract**: Temporal Neural Networks (TNNs), a special class of spiking neural networks, draw inspiration from the neocortex in utilizing spike-timings for information processing. Recent works proposed a microarchitecture framework and custom macro suite for designing highly energy-efficient application-specific TNNs. These recent works rely on manual hardware design, a labor-intensive and time-consuming process. Further, there is no open-source functional simulation framework for TNNs. This paper introduces TNNGen, a pioneering effort towards the automated design of TNNs from PyTorch software models to post-layout netlists. TNNGen comprises a novel PyTorch functional simulator (for TNN modeling and application exploration) coupled with a Python-based hardware generator (for PyTorch-to-RTL and RTL-to-Layout conversions). Seven representative TNN designs for time-series signal clustering across diverse sensory modalities are simulated and their post-layout hardware complexity and design runtimes are assessed to demonstrate the effectiveness of TNNGen. We also highlight TNNGen's ability to accurately forecast silicon metrics without running hardware process flow. 

**Abstract (ZH)**: 时间神经网络（TNNs），一种特殊的突触神经网络，借鉴了新皮层利用尖峰时间进行信息处理的方法。最近的研究提出了一种微架构框架和定制的宏套件，用于设计高度节能的应用特异性TNNs。这些最近的研究依赖于手工硬件设计，这是一个劳动密集型且耗时的过程。此外，目前没有公开的TNN功能仿真框架。本文介绍了TNNGen，这是从PyTorch软件模型到后布局网表的自动TNN设计的一种开创性努力。TNNGen 包括一种新型的PyTorch功能仿真器（用于TNN建模和应用探索）和基于Python的硬件生成器（用于PyTorch 到 RTL 和 RTL 到布局转换）。对跨多种感知模态的时间序列信号聚类的七个代表性TNN设计进行了仿真实验，并评估了它们的后布局硬件复杂性和设计运行时间，以展示TNNGen 的有效性。我们还强调了TNNGen 能够在不运行硬件工艺流程的情况下准确预测硅片指标的能力。 

---
# Improving Sickle Cell Disease Classification: A Fusion of Conventional Classifiers, Segmented Images, and Convolutional Neural Networks 

**Title (ZH)**: 提高镰状细胞病分类效果：传统分类器、分割图像与卷积神经网络的融合 

**Authors**: Victor Júnio Alcântara Cardoso, Rodrigo Moreira, João Fernando Mari, Larissa Ferreira Rodrigues Moreira  

**Link**: [PDF](https://arxiv.org/pdf/2412.17975)  

**Abstract**: Sickle cell anemia, which is characterized by abnormal erythrocyte morphology, can be detected using microscopic images. Computational techniques in medicine enhance the diagnosis and treatment efficiency. However, many computational techniques, particularly those based on Convolutional Neural Networks (CNNs), require high resources and time for training, highlighting the research opportunities in methods with low computational overhead. In this paper, we propose a novel approach combining conventional classifiers, segmented images, and CNNs for the automated classification of sickle cell disease. We evaluated the impact of segmented images on classification, providing insight into deep learning integration. Our results demonstrate that using segmented images and CNN features with an SVM achieves an accuracy of 96.80%. This finding is relevant for computationally efficient scenarios, paving the way for future research and advancements in medical-image analysis. 

**Abstract (ZH)**: 镰状细胞贫血是一种由于红细胞形态异常引起的疾病，可以通过显微图像进行检测。医学中的计算技术可以提升诊断和治疗的效率。然而，许多计算技术，特别是基于卷积神经网络（CNN）的方法，需要大量的资源和时间进行训练，这突出了低计算开销方法的研究机会。本文提出了一种结合传统分类器、分割图像和CNN的新方法，用于镰状细胞病的自动分类。我们评估了分割图像对分类的影响，提供了深度学习集成的见解。实验结果表明，使用分割图像和CNN特征结合SVM可以达到96.80%的准确率。这项发现对于计算高效的场景具有重要意义，为未来医学图像分析的研究和进展铺平了道路。 

---
# Towards Cognitive Service Delivery on B5G through AIaaS Architecture 

**Title (ZH)**: 面向B5G的认知服务交付通过AIaaS架构 

**Authors**: Larissa F. Rodrigues Moreira, Rodrigo Moreira, Flávio de Oliveira Silva, André R. Backes  

**Link**: [PDF](https://arxiv.org/pdf/2412.17967)  

**Abstract**: Artificial Intelligence (AI) is pivotal in advancing mobile network systems by facilitating smart capabilities and automation. The transition from 4G to 5G has substantial implications for AI in consolidating a network predominantly geared towards business verticals. In this context, 3GPP has specified and introduced the Network Data Analytics Function (NWDAF) entity at the network's core to provide insights based on AI algorithms to benefit network orchestration. This paper proposes a framework for evolving NWDAF that presents the interfaces necessary to further empower the core network with AI capabilities B5G and 6G. In addition, we identify a set of research directions for realizing a distributed e-NWDAF. 

**Abstract (ZH)**: 人工智能（AI）对移动网络系统的发展至关重要，它通过提供智能能力和自动化来推动这一进程。从4G到5G的过渡对AI在构建以企业垂直领域为主的网络中具有重大影响。在此背景下，3GPP在网络核心引入了网络数据与分析功能（NWDAF）实体，以利用AI算法提供洞见，从而优化网络编排。本文提出了一种演进NWDAF的框架，旨在进一步增强核心网络的AI能力，特别适用于B5G和6G技术。此外，我们还指出了实现分布式的e-NWDAF的一系列研究方向。 

---
# tuGEMM: Area-Power-Efficient Temporal Unary GEMM Architecture for Low-Precision Edge AI 

**Title (ZH)**: tuGEMM：低精度边缘AI领域的面积-功耗高效时间一元GEMM架构 

**Authors**: Harideep Nair, Prabhu Vellaisamy, Albert Chen, Joseph Finn, Anna Li, Manav Trivedi, John Paul Shen  

**Link**: [PDF](https://arxiv.org/pdf/2412.17966)  

**Abstract**: General matrix multiplication (GEMM) is a ubiquitous computing kernel/algorithm for data processing in diverse applications, including artificial intelligence (AI) and deep learning (DL). Recent shift towards edge computing has inspired GEMM architectures based on unary computing, which are predominantly stochastic and rate-coded systems. This paper proposes a novel GEMM architecture based on temporal-coding, called tuGEMM, that performs exact computation. We introduce two variants of tuGEMM, serial and parallel, with distinct area/power-latency trade-offs. Post-synthesis Power-Performance-Area (PPA) in 45 nm CMOS are reported for 2-bit, 4-bit, and 8-bit computations. The designs illustrate significant advantages in area-power efficiency over state-of-the-art stochastic unary systems especially at low precisions, e.g. incurring just 0.03 mm^2 and 9 mW for 4 bits, and 0.01 mm^2 and 4 mW for 2 bits. This makes tuGEMM ideal for power constrained mobile and edge devices performing always-on real-time sensory processing. 

**Abstract (ZH)**: 基于时序编码的通用矩阵乘法（GEMM）架构（tuGEMM）是一种在各种应用中广泛应用的计算内核/算法，包括人工智能（AI）和深度学习（DL）。近年来，向边缘计算的转变激发了基于 unary 计算的 GEMM 架构，这些架构主要是随机性和速率编码的系统。本文提出了一种基于时序编码且能进行精确计算的新颖 GEMM 架构，称为 tuGEMM。我们提出了两种 tuGEMM 的变体，分别是串行和并行版本，它们具有不同的面积/功耗/延迟折衷方案。45 nm CMOS 门级合成后的功耗-性能-面积 (PPA) 结果报告了2位、4位和8位计算的情况。设计展示了与最先进的随机 unary 系统相比在面积/功耗效率方面的重要优势，尤其是在低精度情况下，例如，对于4位计算，面积仅为0.03 mm²，功耗为9 mW，而对于2位计算，面积仅为0.01 mm²，功耗为4 mW。这使 tuGEMM 成为适合进行始终在线的实时传感处理的功率受限移动和边缘设备的理想选择。 

---
# LMV-RPA: Large Model Voting-based Robotic Process Automation 

**Title (ZH)**: LMV-RPA：基于大型模型投票的机器人流程自动化

这个标题翻译成中文时，保持了原有的学术规范和专业术语。"Large Model Voting" 被翻译为“大型模型投票”，"Robotic Process Automation" 翻译为“机器人流程自动化”。确保了翻译的准确性和专业性。 

**Authors**: Osama Abdellatif, Ahmed Ayman, Ali Hamdi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17965)  

**Abstract**: Automating high-volume unstructured data processing is essential for operational efficiency. Optical Character Recognition (OCR) is critical but often struggles with accuracy and efficiency in complex layouts and ambiguous text. These challenges are especially pronounced in large-scale tasks requiring both speed and precision. This paper introduces LMV-RPA, a Large Model Voting-based Robotic Process Automation system to enhance OCR workflows. LMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR, Easy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and Gemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs into structured JSON formats, improving accuracy, particularly in complex layouts. The multi-phase pipeline processes text extracted by OCR engines through LLMs, combining results to ensure the most accurate outputs. LMV-RPA achieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94 percent, while reducing processing time by 80 percent. Benchmark evaluations confirm its scalability and demonstrate that LMV-RPA offers a faster, more reliable, and efficient solution for automating large-scale document processing tasks. 

**Abstract (ZH)**: 大规模处理高 Volume 的非结构化数据对于提高运营效率至关重要。光学字符识别（OCR）至关重要，但在复杂布局和模糊文本的情况下经常面临准确性和效率的问题。这些挑战在需要同时速度和精度的大规模任务中尤为突出。本文介绍了一种基于大型模型投票的机器人流程自动化系统 LMV-RPA，以增强 OCR 工作流程。LMV-RPA 将来自 Paddle OCR、Tesseract OCR、Easy OCR 和 DocTR 等 OCR 引擎的输出与大型语言模型（LLMs）如 LLaMA 3 和 Gemini-1.5-pro 整合在一起。利用多数投票机制，它将 OCR 输出转化为结构化的 JSON 格式，特别是在复杂布局上提升了准确性。多阶段流水线对 OCR 引擎提取的文本通过 LLMs 处理，并结合结果确保最准确的输出。LMV-RPA 在 OCR 任务中的准确率达到 99%，比基线模型（准确率 94%）高，同时将处理时间减少了 80%。基准评估证实了其可扩展性，并表明 LMV-RPA 提供了一种更快、更可靠且更高效的大型文档处理自动化解决方案。 

---
# Analysis of Transferred Pre-Trained Deep Convolution Neural Networks in Breast Masses Recognition 

**Title (ZH)**: 乳腺肿块识别中迁移预训练深度卷积神经网络的分析 

**Authors**: Qusay Shihab Hamad, Hussein Samma, Shahrel Azmin Suandi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17959)  

**Abstract**: Breast cancer detection based on pre-trained convolution neural network (CNN) has gained much interest among other conventional computer-based systems. In the past few years, CNN technology has been the most promising way to find cancer in mammogram scans. In this paper, the effect of layer freezing in a pre-trained CNN is investigated for breast cancer detection by classifying mammogram images as benign or malignant. Different VGG19 scenarios have been examined based on the number of convolution layer blocks that have been frozen. There are a total of six scenarios in this study. The primary benefits of this research are twofold: it improves the model's ability to detect breast cancer cases and it reduces the training time of VGG19 by freezing certain this http URL evaluate the performance of these scenarios, 1693 microbiological images of benign and malignant breast cancers were utilized. According to the reported results, the best recognition rate was obtained from a frozen first block of VGG19 with a sensitivity of 95.64 %, while the training of the entire VGG19 yielded 94.48%. 

**Abstract (ZH)**: 基于预训练卷积神经网络（CNN）的乳腺癌检测引起了其他传统计算机辅助系统的广泛关注。在过去几年中，CNN技术已成为了在乳腺X光片中寻找癌变的最具有前景的方法之一。在本文中，我们探讨了预训练CNN中层冻结对乳腺癌检测的影响，通过对乳腺X光片图像进行良性或恶性分类。基于冻结的卷积层块数量的不同情况，我们研究了不同的VGG19情景。总共存在六种不同的情况。本文研究的主要优势在于两个方面：一是改善了模型检测乳腺癌的能力，二是通过冻结某些层从而减少了VGG19的训练时间。为了评估这些情景的表现，我们使用了1693张良性与恶性乳腺癌的微生物图像。根据报告的结果，从冻结的VGG19的第一个块中获得了最佳识别率，其灵敏度为95.64%，而完整训练的VGG19的识别率为94.48%。 

---
# ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling 

**Title (ZH)**: ArchComplete：基于分层扩散增强上采样的自回归建筑设计生成 

**Authors**: S. Rasoulzadeh, M. Bank, M. Wimmer, I. Kovacic, K. Schinegger, S. Rutzinger  

**Link**: [PDF](https://arxiv.org/pdf/2412.17957)  

**Abstract**: $\textit{ArchComplete}$ is a two-stage dense voxel-based 3D generative pipeline developed to tackle the high complexity in architectural geometries and topologies, assisting with ideation and geometric detailisation in the early design process. In stage 1, a $\textit{3D Voxel VQGAN}$ model is devised, whose composition is then modelled with an autoregressive transformer for generating coarse models. Subsequently, in stage 2, $\textit{Hierarchical Voxel Upsampling Networks}$ consisting of a set of 3D conditional denoising diffusion probabilistic models are defined to augment the coarse shapes with fine geometric details. The first stage is trained on a dataset of house models with fully modelled exteriors and interiors with a novel 2.5D perceptual loss to capture input complexities across multiple abstraction levels, while the second stage trains on randomly cropped local volumetric patches, requiring significantly less compute and memory. For inference, the pipeline first autoregressively generates house models at a resolution of $64^3$ and then progressively refines them to resolution of $256^3$ with voxel sizes as small as $18\text{cm}$. ArchComplete supports a range of interaction modes solving a variety of tasks, including interpolation, variation generation, unconditional synthesis, and two conditional synthesis tasks: shape completion and plan-drawing completion, as well as geometric detailisation. The results demonstrate notable improvements against state-of-the-art on established metrics. 

**Abstract (ZH)**: $\textit{ArchComplete}$ 是一种两阶段密集体素基于的3D生成管道，旨在应对建筑几何结构和拓扑的高复杂性，协助早期设计过程中的创意构思和几何详细设计。在第一阶段，设计了一个 $\textit{3D Voxel VQGAN}$ 模型，其结构然后通过自回归变换器生成粗略模型。随后，在第二阶段，定义了一种 $\textit{分层体素上采样网络}$，由一组3D条件去噪扩散概率模型组成，用于在粗略形状上添加细部几何细节。第一阶段使用包含完全建模的外部和内部房屋模型的数据集进行训练，使用新颖的2.5D感知损失来跨多个抽象级别捕捉输入的复杂性，而第二阶段则在随机裁剪的局部体素块上进行训练，所需计算和内存显著减少。在推理过程中，该管道首先自回归地生成分辨率为 $64^3$ 的房屋模型，然后逐步细化到 $256^3$ 分辨率，最小体素大小为 $18\text{cm}$。$\textit{ArchComplete}$ 支持多种交互模式，可以解决各种任务，包括插值、变体生成、无条件合成，以及两种条件合成任务：形状完成和平面图完成，还包括几何细节化。实验结果表明，相对于现有最先进的方法，在 Established Metrics 上有显著改进。 

---
# Adaptive Signal Analysis for Automated Subsurface Defect Detection Using Impact Echo in Concrete Slabs 

**Title (ZH)**: 使用混凝土板中冲击回波的自适应信号分析实现自动化地下缺陷检测 

**Authors**: Deepthi Pavurala, Duoduo Liao, Chaithra Reddy Pasunuru  

**Link**: [PDF](https://arxiv.org/pdf/2412.17953)  

**Abstract**: This pilot study presents a novel, automated, and scalable methodology for detecting and evaluating subsurface defect-prone regions in concrete slabs using Impact Echo (IE) signal analysis. The approach integrates advanced signal processing, clustering, and visual analytics to identify subsurface anomalies. A unique adaptive thresholding method tailors frequency-based defect identification to the distinct material properties of each slab. The methodology generates frequency maps, binary masks, and k-means cluster maps to automatically classify defect and non-defect regions. Key visualizations, including 3D surface plots, cluster maps, and contour plots, are employed to analyze spatial frequency distributions and highlight structural anomalies. The study utilizes a labeled dataset constructed at the Federal Highway Administration (FHWA) Advanced Sensing Technology Nondestructive Evaluation Laboratory. Evaluations involve ground-truth masking, comparing the generated defect maps with top-view binary masks derived from the information provided by the FHWA. The performance metrics, specifically F1-scores and AUC-ROC, achieve values of up to 0.95 and 0.83, respectively. The results demonstrate the robustness of the methodology, consistently identifying defect-prone areas with minimal false positives and few missed defects. Adaptive frequency thresholding ensures flexibility in addressing variations across slabs, providing a scalable framework for detecting structural anomalies. Additionally, the methodology is adaptable to other frequency-based signals due to its generalizable thresholding mechanism and holds potential for integrating multimodal sensor fusion. This automated and scalable pipeline minimizes manual intervention, ensuring accurate and efficient defect detection, further advancing Non-Destructive Evaluation (NDE) techniques. 

**Abstract (ZH)**: 本初步研究介绍了一种新颖、自动化且可扩展的方法，用于通过冲击回声（IE）信号分析检测和评估混凝土板中潜在缺陷的地下区域。该方法结合了先进的信号处理、聚类和可视化分析技术，以识别地下异常。采用了一种独特的自适应阈值方法，根据每个板的不同材料特性对基于频域的缺陷识别进行定制。该方法生成频域图、二进制掩模和k均值聚类图，以自动分类缺陷和非缺陷区域。使用关键可视化工具，包括3D表面图、聚类图和轮廓图，分析空间频率分布并突显结构异常。研究使用的数据集是在美国联邦公路管理局（FHWA）高级传感技术无损评价实验室中构建的，并进行标注。评估方法包括使用地面真实掩模，将生成的缺陷图与FHWA提供的信息生成的顶视图二进制掩模进行比较。性能指标F1分数和AUC-ROC分别达到0.95和0.83。结果表明该方法具有较强鲁棒性，能够准确识别潜在缺陷区域，且误报率低，漏检率少。自适应频率阈值确保了在不同板之间的变化中具有灵活性，提供了一种可扩展框架来检测结构异常。此外，由于其可推广的阈值机制，该方法也可应用于其他基于频率的信号，并具有整合多模式传感器融合的潜力。该自动化和可扩展的管道减少了人工干预，确保了准确而高效的缺陷检测，进一步推动了无损检测（NDE）技术的发展。 

---
# BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism 

**Title (ZH)**: BenCzechMark：一种基于捷克语的多任务和多指标基准测试，包含双重评分机制 

**Authors**: Martin Fajcik, Martin Docekal, Jan Dolezal, Karel Ondrej, Karel Beneš, Jan Kapsa, Pavel Smrz, Alexander Polok, Michal Hradis, Zuzana Neverilova, Ales Horak, Radoslav Sabol, Michal Stefanik, Adam Jirkovsky, David Adamczyk, Petr Hyner, Jan Hula, Hynek Kydlicek  

**Link**: [PDF](https://arxiv.org/pdf/2412.17933)  

**Abstract**: We present BenCzechMark (BCM), the first comprehensive Czech language benchmark designed for large language models, offering diverse tasks, multiple task formats, and multiple evaluation metrics. Its scoring system is grounded in statistical significance theory and uses aggregation across tasks inspired by social preference theory. Our benchmark encompasses 50 challenging tasks, with corresponding test datasets, primarily in native Czech, with 11 newly collected ones. These tasks span 8 categories and cover diverse domains, including historical Czech news, essays from pupils or language learners, and spoken word.
Furthermore, we collect and clean BUT-Large Czech Collection, the largest publicly available clean Czech language corpus, and use it for (i) contamination analysis, (ii) continuous pretraining of the first Czech-centric 7B language model, with Czech-specific tokenization. We use our model as a baseline for comparison with publicly available multilingual models. Lastly, we release and maintain a leaderboard, with existing 44 model submissions, where new model submissions can be made at this https URL. 

**Abstract (ZH)**: 我们介绍了BenCzechMark（BCM），这是首个专为大规模语言模型设计的全面捷克语基准测试，提供了多样的任务、多种任务格式和多个评估指标。其评分系统基于统计显著性理论，并借鉴社会偏好理论在任务层面采用了聚合方法。该基准测试包含50个具有挑战性的任务，其中包括51个测试数据集，主要为地道的捷克语，其中11个是新收集的。这些任务覆盖了8个类别，包括历史捷克新闻、学生或语言学习者的作文，以及口语材料。

此外，我们收集并清理了BUT-Large捷克语语料库，这是目前可用的最大规模的清洁版捷克语语料库，并将其用于(i) 污染分析，(ii) 第一个以捷克语为中心的7B语言模型的连续预训练，其中使用了捷克语特定的分词方法。我们将我们的模型用作基准模型，与已有的多语言模型进行对比。最后，我们发布了并维护了一个排行榜，现有44个模型提交，新的模型提交可通过此链接进行：[链接]。 

---
# A Novel Approach to Balance Convenience and Nutrition in Meals With Long-Term Group Recommendations and Reasoning on Multimodal Recipes and its Implementation in BEACON 

**Title (ZH)**: 一种平衡便捷性和营养价值的创新方法：基于长期群体推荐和多模态食谱推理，并在BEACON中的实现 

**Authors**: Vansh Nagpal, Siva Likitha Valluru, Kausik Lakkaraju, Nitin Gupta, Zach Abdulrahman, Andrew Davison, Biplav Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2412.17910)  

**Abstract**: "A common decision made by people, whether healthy or with health conditions, is choosing meals like breakfast, lunch, and dinner, comprising combinations of foods for appetizer, main course, side dishes, desserts, and beverages. Often, this decision involves tradeoffs between nutritious choices (e.g., salt and sugar levels, nutrition content) and convenience (e.g., cost and accessibility, cuisine type, food source type). We present a data-driven solution for meal recommendations that considers customizable meal configurations and time horizons. This solution balances user preferences while accounting for food constituents and cooking processes. Our contributions include introducing goodness measures, a recipe conversion method from text to the recently introduced multimodal rich recipe representation (R3) format, learning methods using contextual bandits that show promising preliminary results, and the prototype, usage-inspired, BEACON system." 

**Abstract (ZH)**: 人们在健康状况良好或有健康状况时，都会面临选择一日三餐（早饭、午饭和晚饭）的决定，即如何组合开胃菜、主菜、配菜、甜点和饮料。这一决定通常涉及营养选择（如盐分和糖分水平、营养成分）与便利性（如成本和易得性、菜系类型、食材来源）之间的权衡。我们提出了一种基于数据的餐品推荐解决方案，该方案考虑了可定制的餐品配置和时间范围。该方案平衡了用户偏好，同时考虑了食物成分和烹饪过程。我们的贡献包括引入了良好性度量、一种从文本转换到最近引入的多模态丰富食谱表示（R3）格式的食谱转换方法、使用上下文臂方法的学习方法（显示了初步的有希望的结果），以及受到使用启发的BEACON系统原型。 

---
# The Power of Adaptation: Boosting In-Context Learning through Adaptive Prompting 

**Title (ZH)**: 适应力的强大作用：通过自适应提示提升基于上下文学习 

**Authors**: Shuzhang Cai, Twumasi Mensah-Boateng, Xander Kuksov, Jing Yuan, Shaojie Tang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17891)  

**Abstract**: Large Language Models (LLMs) have demonstrated exceptional abilities across a broad range of language-related tasks, including generating solutions to complex reasoning problems. An effective technique to enhance LLM performance is in-context learning, which encourages a step-by-step reasoning process by including explanatory examples to guide the model's responses. However, selecting appropriate exemplars for the model poses a challenge, as each dataset demands a distinct set of exemplars to enable the LLM to learn effectively and perform well on the test set. Current studies often rely on uncertainty- or diversity-based selection strategies to select exemplars for annotation and to improve model learning. However, these studies typically employ a non-adaptive approach, selecting a set of exemplars all at once. We argue that this non-adaptive strategy may result in a set of exemplars with high redundancy in terms of the knowledge covered, ultimately reducing their overall informativeness. To address this limitation, we propose \textsc{Adaptive-Prompt}, a novel method that adaptively selects exemplars by leveraging model feedback from previously chosen exemplars. Experimental results show that \textsc{Adaptive-Prompt} significantly enhances LLM performance across a variety of reasoning tasks. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经在各种语言相关的任务上展现了卓越的能力，包括解决复杂的推理问题。一种有效的提高LLM性能的技术是上下文学习，它通过提供解释性的示例来引导模型的响应，从而促进逐步的推理过程。然而，为模型选择合适的示例也是一项挑战，因为每个数据集需要一组独特的示例来帮助LLM有效学习并在验证集上表现出色。当前的研究通常依赖于基于不确定性或多样性的选择策略来选择需要标注的示例并改进模型学习。然而，这些研究通常采用非适应性方法，一次性选择一组示例。我们认为，这种非适应性策略可能会导致选择的示例在涵盖的知识方面存在高度冗余，从而降低它们的整体信息量。为解决这一局限性，我们提出了一种名为Adaptive-Prompt的新方法，该方法通过利用之前选择的示例的模型反馈来进行适应性选择。实验结果显示，Adaptive-Prompt显著提高了LLM在各种推理任务上的性能。 

---
# Stability Bounds for the Unfolded Forward-Backward Algorithm 

**Title (ZH)**: 折叠前向-后向算法的稳定性界 

**Authors**: Emilie Chouzenoux, Cecile Della Valle, Jean-Christophe Pesquet  

**Link**: [PDF](https://arxiv.org/pdf/2412.17888)  

**Abstract**: We consider a neural network architecture designed to solve inverse problems where the degradation operator is linear and known. This architecture is constructed by unrolling a forward-backward algorithm derived from the minimization of an objective function that combines a data-fidelity term, a Tikhonov-type regularization term, and a potentially nonsmooth convex penalty. The robustness of this inversion method to input perturbations is analyzed theoretically. Ensuring robustness complies with the principles of inverse problem theory, as it ensures both the continuity of the inversion method and the resilience to small noise - a critical property given the known vulnerability of deep neural networks to adversarial perturbations. A key novelty of our work lies in examining the robustness of the proposed network to perturbations in its bias, which represents the observed data in the inverse problem. Additionally, we provide numerical illustrations of the analytical Lipschitz bounds derived in our analysis. 

**Abstract (ZH)**: 我们研究了一种用于解决降级操作符为已知且线性的逆问题的神经网络架构。该架构通过展开源自组合了数据保真项、Tikhonov型正则化项和可能非光滑凸惩罚项的目标函数最小化的一种正向-反向算法而构建。我们从理论上分析了该反演方法对输入扰动的稳健性。确保稳健性符合逆问题理论的原则，因为它既保证了反演方法的连续性，又增强了对小噪声的抵抗能力——这一点在已知深度神经网络对对抗性扰动脆弱的情况下尤为重要。我们的工作的一个关键新颖之处在于，我们研究了所提出网络在其偏置（即逆问题中的观测数据）扰动下的稳健性。此外，我们还提供了我们分析中导出的分析性Lipschitz界的具体数值示例。 

---
# In Defence of Post-hoc Explainability 

**Title (ZH)**: 论事后解释的正当性 

**Authors**: Nick Oh  

**Link**: [PDF](https://arxiv.org/pdf/2412.17883)  

**Abstract**: The widespread adoption of machine learning in scientific research has created a fundamental tension between model opacity and scientific understanding. Whilst some advocate for intrinsically interpretable models, we introduce Computational Interpretabilism (CI) as a philosophical framework for post-hoc interpretability in scientific AI. Drawing parallels with human expertise, where post-hoc rationalisation coexists with reliable performance, CI establishes that scientific knowledge emerges through structured model interpretation when properly bounded by empirical validation. Through mediated understanding and bounded factivity, we demonstrate how post-hoc methods achieve epistemically justified insights without requiring complete mechanical transparency, resolving tensions between model complexity and scientific comprehension. 

**Abstract (ZH)**: 机器学习在科学研究中的广泛应用创造了一种根本性的紧张关系，即模型的不透明性与科学理解之间的矛盾。虽然有些人倡导使用固有的可解释模型，我们在此提出计算解释主义（Computational Interpretabilism, CI）作为一种后验可解释性的哲学框架，用于科学人工智能。借鉴人类专业知识的模式，其中后验合理化与可靠性能并存，计算解释主义认为，当通过经验验证进行适当限制时，科学知识的形成是通过结构化的模型解释实现的。通过中介理解和有界的真知，我们展示了后验方法如何在不需要完全机械透明性的前提下获得知识上合理的洞见，从而解决了模型复杂性和科学理解之间的紧张关系。 

---
# Evaluating LLM Reasoning in the Operations Research Domain with ORQA 

**Title (ZH)**: 使用ORQA评估大型语言模型在运筹学领域的推理能力 

**Authors**: Mahdi Mostajabdaveh, Timothy T. Yu, Samarendra Chandan Bindu Dash, Rindranirina Ramamonjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, Yong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17874)  

**Abstract**: In this paper, we introduce and apply Operations Research Question Answering (ORQA), a new benchmark designed to assess the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR). This benchmark evaluates whether LLMs can emulate the knowledge and reasoning skills of OR experts when confronted with diverse and complex optimization problems. The dataset, developed by OR experts, features real-world optimization problems that demand multistep reasoning to construct their mathematical models. Our evaluations of various open source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting a gap in their ability to generalize to specialized technical domains. This work contributes to the ongoing discourse on LLMs generalization capabilities, offering valuable insights for future research in this area. The dataset and evaluation code are publicly available. 

**Abstract (ZH)**: 在本文中，我们介绍了并应用了运筹学问答（ORQA）基准，这是一个新的基准，旨在评估大规模语言模型（LLMs）在运筹学（OR）这一专门技术领域的泛化能力。该基准评估LLMs在面对多种多样且复杂的优化问题时，是否能够模拟运筹学专家的知识和推理能力。该数据集由运筹学专家开发，包含需要多步推理来构建其数学模型的实际优化问题。我们对多种开源LLM，如LLaMA 3.1、DeepSeek和Mixtral的评估显示，它们的性能较为有限，突显了它们在向专门技术领域泛化方面的不足。本文为持续进行的关于LLMs泛化能力的讨论做出了贡献，并为该领域的未来研究提供了宝贵见解。该数据集及其评价代码已对外公开。 

---
# Joint Knowledge Editing for Information Enrichment and Probability Promotion 

**Title (ZH)**: 联合知识编辑以实现信息丰富和概率提升 

**Authors**: Wenhang Shi, Yiren Chen, Shuqing Bian, Xinyi Zhang, Zhe Zhao, Pengfei Hu, Wei Lu, Xiaoyong Du  

**Link**: [PDF](https://arxiv.org/pdf/2412.17872)  

**Abstract**: Knowledge stored in large language models requires timely updates to reflect the dynamic nature of real-world information. To update the knowledge, most knowledge editing methods focus on the low layers, since recent probes into the knowledge recall process reveal that the answer information is enriched in low layers. However, these probes only and could only reveal critical recall stages for the original answers, while the goal of editing is to rectify model's prediction for the target answers. This inconsistency indicates that both the probe approaches and the associated editing methods are deficient. To mitigate the inconsistency and identify critical editing regions, we propose a contrast-based probe approach, and locate two crucial stages where the model behavior diverges between the original and target answers: Information Enrichment in low layers and Probability Promotion in high layers. Building upon the insights, we develop the Joint knowledge Editing for information Enrichment and probability Promotion (JEEP) method, which jointly edits both the low and high layers to modify the two critical recall stages. Considering the mutual interference and growing forgetting due to dual modifications, JEEP is designed to ensure that updates to distinct regions share the same objectives and are complementary. We rigorously evaluate JEEP by editing up to thousands of facts on various models, i.e., GPT-J (6B) and LLaMA (7B), and addressing diverse editing objectives, i.e., adding factual and counterfactual knowledge. In all tested scenarios, JEEP achieves best performances, validating the effectiveness of the revealings of our probe approach and the designs of our editing method. Our code and data are available at this https URL. 

**Abstract (ZH)**: 大型语言模型中存储的知识需要及时更新，以反映现实世界信息的动态性质。为了更新知识，大多数知识编辑方法都集中在较低层次，因为最近对知识检索过程的探查表明，答案信息丰富于较低层次。然而，这些探查只能揭示原始答案的关键检索阶段，而编辑的目标是纠正模型对目标答案的预测。这种不一致性表明，现有的探查方法和相关编辑方法都是不完备的。为了减轻这种不一致性并识别关键的编辑区域，我们提出了一种对比探针方法，并定位了两个关键阶段，即模型在原始答案和目标答案之间行为分歧的两个阶段：较低层次的信息丰富和较高层次的概率增强。基于这些洞见，我们开发了联合信息丰富和概率增强知识编辑方法（JEEP），该方法同时编辑低层和高层以修改这两个关键检索阶段。考虑到联合修改带来的相互干扰和逐渐遗忘，JEEP 设计为确保不同区域的更新具有相同的目标且相辅相成。我们通过在多种模型（如 GPT-J（6B）和 LLaMA（7B））上编辑多达数千个事实，且解决各种编辑目标（如添加事实性和反事实性知识），严格评估了 JEEP。在所有测试场景中，JEEP 均表现出最佳性能，验证了我们探针方法揭示的有效性以及编辑方法设计的有效性。我们的代码和数据可在以下网址获取：[此处链接]。 

---
# Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types 

**Title (ZH)**: 评估和增强多轮文本到SQL转换的LLM模型，支持多种问题类型 

**Authors**: Ziming Guo, Chao Ma, Yinggang Sun, Tiancheng Zhao, Guangyao Wang, Hai Huang  

**Link**: [PDF](https://arxiv.org/pdf/2412.17867)  

**Abstract**: Recent advancements in large language models (LLMs) have significantly advanced text-to-SQL systems. However, most LLM-based methods often narrowly focus on SQL generation, neglecting the complexities of real-world conversational queries. This oversight can lead to unreliable responses, particularly for ambiguous questions that cannot be directly addressed with SQL. To bridge this gap, we propose MMSQL, a comprehensive test suite designed to evaluate the question classification and SQL generation capabilities of LLMs by simulating real-world scenarios with diverse question types and multi-turn Q\&A interactions. Using MMSQL, we assessed the performance of popular LLMs, including both open-source and closed-source models, and identified key factors impacting their performance in such scenarios. Moreover, we introduce an LLM-based multi-agent framework that employs specialized agents to identify question types and determine appropriate answering strategies. Our experiments demonstrate that this approach significantly enhances the model's ability to navigate the complexities of conversational dynamics, effectively handling the diverse and complex nature of user queries. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的最新进展显著提升了文本到SQL系统的能力。然而，大多数基于LLM的方法往往狭隘地集中在SQL生成上，忽视了现实世界对话查询的复杂性。这种忽视可能导致不稳定的响应，尤其是对于那些不能直接用SQL解决的模糊问题。为了弥合这一差距，我们提出了MMSQL，这是一个全面的测试套件，通过模拟各种问题类型和多轮对话交互的现实场景，评估LLMs的问题分类和SQL生成能力。使用MMSQL，我们评估了包括开源和闭源在内的流行LLM模型的性能，并确定了影响它们在这类场景中表现的关键因素。此外，我们介绍了一种基于LLM的多智能体框架，该框架采用专门的智能体来识别问题类型并确定合适的回答策略。我们的实验表明，这种做法显著增强了模型导航对话动态复杂性的能力，有效地应对了用户的多样性和复杂查询。 

---
# Active Geospatial Search for Efficient Tenant Eviction Outreach 

**Title (ZH)**: 高效租户驱逐宣传的活性地理空间搜索 

**Authors**: Anindya Sarkar, Alex DiChristofano, Sanmay Das, Patrick J. Fowler, Nathan Jacobs, Yevgeniy Vorobeychik  

**Link**: [PDF](https://arxiv.org/pdf/2412.17854)  

**Abstract**: Tenant evictions threaten housing stability and are a major concern for many cities. An open question concerns whether data-driven methods enhance outreach programs that target at-risk tenants to mitigate their risk of eviction. We propose a novel active geospatial search (AGS) modeling framework for this problem. AGS integrates property-level information in a search policy that identifies a sequence of rental units to canvas to both determine their eviction risk and provide support if needed. We propose a hierarchical reinforcement learning approach to learn a search policy for AGS that scales to large urban areas containing thousands of parcels, balancing exploration and exploitation and accounting for travel costs and a budget constraint. Crucially, the search policy adapts online to newly discovered information about evictions. Evaluation using eviction data for a large urban area demonstrates that the proposed framework and algorithmic approach are considerably more effective at sequentially identifying eviction cases than baseline methods. 

**Abstract (ZH)**: 租户驱逐威胁到住房稳定性，是许多城市的一大重点关注问题。一个开放的问题是，数据驱动的方法是否能够增强针对潜在风险租户的接触计划，以减轻他们的驱逐风险。我们提出了一种新的主动地理空间搜索（AGS）建模框架来解决这个问题。AGS 结合了物业层面的信息，并将其纳入搜索策略中，以识别一系列需要实地调查的租赁单元，以评估其驱逐风险，并在必要时提供支持。我们提出了一种分层强化学习方法，以学习 AGS 的搜索策略，该策略能够处理包含数千个地块的大城市区域，平衡探索与利用，并考虑到出行成本和预算限制。尤为重要的是，搜索策略能够根据新发现的驱逐信息在线进行调整。使用一个大城市的驱逐数据进行评估表明，所提出的框架和算法方法在顺序识别驱逐案例方面比基线方法更为有效。 

---
# LaMI-GO: Latent Mixture Integration for Goal-Oriented Communications Achieving High Spectrum Efficiency 

**Title (ZH)**: LaMI-GO: 背潜混合集成用于目标导向通信，实现高效频谱利用 

**Authors**: Achintha Wijesinghe, Suchinthaka Wanninayaka, Weiwei Wang, Yu-Chieh Chao, Songyang Zhang, Zhi Ding  

**Link**: [PDF](https://arxiv.org/pdf/2412.17839)  

**Abstract**: The recent rise of semantic-style communications includes the development of goal-oriented communications (GOCOMs) remarkably efficient multimedia information transmissions. The concept of GO-COMS leverages advanced artificial intelligence (AI) tools to address the rising demand for bandwidth efficiency in applications, such as edge computing and Internet-of-Things (IoT). Unlike traditional communication systems focusing on source data accuracy, GO-COMs provide intelligent message delivery catering to the special needs critical to accomplishing downstream tasks at the receiver. In this work, we present a novel GO-COM framework, namely LaMI-GO that utilizes emerging generative AI for better quality-of-service (QoS) with ultra-high communication efficiency. Specifically, we design our LaMI-GO system backbone based on a latent diffusion model followed by a vector-quantized generative adversarial network (VQGAN) for efficient latent embedding and information representation. The system trains a common feature codebook the receiver side. Our experimental results demonstrate substantial improvement in perceptual quality, accuracy of downstream tasks, and bandwidth consumption over the state-of-the-art GOCOM systems and establish the power of our proposed LaMI-GO communication framework. 

**Abstract (ZH)**: 近年来，语义型沟通方式的兴起显著促进了目标导向沟通（GOCOMs）及其高效多媒体信息传输的发展。GO-COMS的概念利用先进的人工智能（AI）工具，以应对边缘计算和物联网（IoT）等领域中带宽效率不断增长的需求。与传统通信系统注重源数据准确性不同，GO-COMs提供智能消息传递，满足接收端完成下游任务的特殊需求。本文中，我们提出了一种新颖的GO-COM框架，即LaMI-GO，该框架利用新兴的生成AI技术在超高效通信中实现更好的服务质量（QoS）。具体而言，我们基于潜在扩散模型设计了LaMI-GO系统骨干，并通过矢量量化生成对抗网络（VQGAN）实现高效潜在嵌入和信息表示。在此基础上，系统在接收端训练了一个公共特征码本。实验结果表明，相较于现有的领先GOCOM系统，我们的LaMI-GO通信框架在感知质量、下游任务准确性以及带宽消耗方面均取得了显著改进，并证明了我们提出的LaMI-GO通信框架的强大功能。 

---
# Coordinated Power Smoothing Control for Wind Storage Integrated System with Physics-informed Deep Reinforcement Learning 

**Title (ZH)**: 基于物理信息深度强化学习的风储联合系统协同功率平滑控制 

**Authors**: Shuyi Wang, Huan Zhao, Yuji Cao, Zibin Pan, Guolong Liu, Gaoqi Liang, Junhua Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2412.17838)  

**Abstract**: The Wind Storage Integrated System with Power Smoothing Control (PSC) has emerged as a promising solution to ensure both efficient and reliable wind energy generation. However, existing PSC strategies overlook the intricate interplay and distinct control frequencies between batteries and wind turbines, and lack consideration of wake effect and battery degradation cost. In this paper, a novel coordinated control framework with hierarchical levels is devised to address these challenges effectively, which integrates the wake model and battery degradation model. In addition, after reformulating the problem as a Markov decision process, the multi-agent reinforcement learning method is introduced to overcome the bi-level characteristic of the problem. Moreover, a Physics-informed Neural Network-assisted Multi-agent Deep Deterministic Policy Gradient (PAMA-DDPG) algorithm is proposed to incorporate the power fluctuation differential equation and expedite the learning process. The effectiveness of the proposed methodology is evaluated through simulations conducted in four distinct scenarios using WindFarmSimulator (WFSim). The results demonstrate that the proposed algorithm facilitates approximately an 11% increase in total profit and a 19% decrease in power fluctuation compared to the traditional methods, thereby addressing the dual objectives of economic efficiency and grid-connected energy reliability. 

**Abstract (ZH)**: 具有功率平滑控制（PSC）的风能存储集成系统已成为确保风能高效可靠生成的有前途的解决方案。然而，现有的PSC策略没有充分考虑电池和风力涡轮机之间复杂交互和不同的控制频率，也没有考虑到来流效应和电池退化成本。在这篇文章中，我们设计了一个具有层次结构的新型协调控制框架来有效解决这些挑战，该框架整合了来流模型和电池退化模型。此外，在将问题重新表述为马尔可夫决策过程后，我们引入了多智能体增强学习方法，以克服该问题的双层特性。同时，我们提出了一个基于物理信息神经网络的多智能体深度确定性策略梯度算法（PAMA-DDPG），以整合功率波动微分方程并加快学习过程。通过在WindFarmSimulator (WFSim)中进行四种不同场景的仿真评估，我们验证了提出方法的有效性。结果表明，所提出的算法相较于传统方法，能够实现总利润提高约11%和功率波动减少约19%，从而同时实现经济效率和并网能源可靠性双重目标。 

---
# Transfer Learning with Active Sampling for Rapid Training and Calibration in BCI-P300 Across Health States and Multi-centre Data 

**Title (ZH)**: 基于主动抽样的迁移学习在健康状态和多中心数据下快速训练与校准的P300-BCI研究 

**Authors**: Christian Flores, Marcelo Contreras, Ichiro Macedo, Javier Andreu-Perez  

**Link**: [PDF](https://arxiv.org/pdf/2412.17833)  

**Abstract**: Machine learning and deep learning advancements have boosted Brain-Computer Interface (BCI) performance, but their wide-scale applicability is limited due to factors like individual health, hardware variations, and cultural differences affecting neural data. Studies often focus on uniform single-site experiments in uniform settings, leading to high performance that may not translate well to real-world diversity. Deep learning models aim to enhance BCI classification accuracy, and transfer learning has been suggested to adapt models to individual neural patterns using a base model trained on others' data. This approach promises better generalizability and reduced overfitting, yet challenges remain in handling diverse and imbalanced datasets from different equipment, subjects, multiple centres in different countries, and both healthy and patient populations for effective model transfer and tuning.
In a setting characterized by maximal heterogeneity, we proposed P300 wave detection in BCIs employing a convolutional neural network fitted with adaptive transfer learning based on Poison Sampling Disk (PDS) called Active Sampling (AS), which flexibly adjusts the transition from source data to the target domain. Our results reported for subject adaptive with 40% of adaptive fine-tuning that the averaged classification accuracy improved by 5.36% and standard deviation reduced by 12.22% using two distinct, internationally replicated datasets. These results outperformed in classification accuracy, computational time, and training efficiency, mainly due to the proposed Active Sampling (AS) method for transfer learning. 

**Abstract (ZH)**: 机器学习和深度学习的进步提升了脑-机接口（BCI）的性能，但由于个体健康状况、硬件差异和文化差异等因素的影响，其广泛应用受到了限制。这些因素会影响神经数据的表现。许多研究常常聚焦于在统一条件下进行单一站点的统一实验，这导致了在现实世界中的多样性环境中可能无法很好地实现高绩效。深度学习模型旨在提升BCI分类准确性，迁移学习已被建议通过使用在他人数据上训练的基础模型来适应个体的神经模式。这种方法有望增强泛化能力和降低过拟合现象，但仍然面临着处理来自不同设备、不同受试者、不同国家的多个中心以及健康和患病人群的异质和不平衡数据集的挑战，以有效地实现模型迁移和调整。

在最大异质性的环境下，我们提出了利用基于Poisson Sampling Disk (PDS) 调节的自适应迁移学习方法（称为Active Sampling, AS）来检测BCI中的P300波。AS方法通过灵活调整从源数据到目标领域的过渡。我们的研究结果表明，在40%的自适应微调情况下，采用两种国际验证的数据集后，平均分类准确性提高了5.36%，标准差降低了12.22%。这些结果在分类准确性、计算时间和训练效率方面均优于现有方法，主要是因为提出了自适应采集方法（AS）来进行迁移学习。 

---
# MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes 

**Title (ZH)**: MANGO: 结合多模态锐度变换的智能ICU预后模型 

**Authors**: Jiaqing Zhang, Miguel Contreras, Sabyasachi Bandyopadhyay, Andrea Davidson, Jessica Sena, Yuanfang Ren, Ziyuan Guan, Tezcan Ozrazgat-Baslanti, Tyler J. Loftus, Subhash Nerella, Azra Bihorac, Parisa Rashidi  

**Link**: [PDF](https://arxiv.org/pdf/2412.17832)  

**Abstract**: Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to ensure timely and appropriate interventions. Advances in artificial intelligence (AI) technologies have significantly improved the accuracy of acuity predictions. However, prior studies using machine learning for acuity prediction have predominantly relied on electronic health records (EHR) data, often overlooking other critical aspects of ICU stay, such as patient mobility, environmental factors, and facial cues indicating pain or agitation. To address this gap, we present MANGO: the Multimodal Acuity traNsformer for intelliGent ICU Outcomes, designed to enhance the prediction of patient acuity states, transitions, and the need for life-sustaining therapy. We collected a multimodal dataset ICU-Multimodal, incorporating four key modalities, EHR data, wearable sensor data, video of patient's facial cues, and ambient sensor data, which we utilized to train MANGO. The MANGO model employs a multimodal feature fusion network powered by Transformer masked self-attention method, enabling it to capture and learn complex interactions across these diverse data modalities even when some modalities are absent. Our results demonstrated that integrating multiple modalities significantly improved the model's ability to predict acuity status, transitions, and the need for life-sustaining therapy. The best-performing models achieved an area under the receiver operating characteristic curve (AUROC) of 0.76 (95% CI: 0.72-0.79) for predicting transitions in acuity status and the need for life-sustaining therapy, while 0.82 (95% CI: 0.69-0.89) for acuity status prediction... 

**Abstract (ZH)**: 重症监护病房（ICU）患者病情严重程度的估计对于确保及时和适当的干预措施至关重要。人工智能（AI）技术的进步显著提高了病情严重程度预测的准确性。然而，以前使用机器学习进行病情严重程度预测的研究主要依赖电子健康记录（EHR）数据，往往忽略了ICU住院期间的其他关键方面，如患者活动能力、环境因素以及面部表情显示的疼痛或焦虑等。为弥补这一不足，我们提出了MANGO：多模态急性变异性Transformer用于智能ICU结局（Multimodal Acuity traNsformer for intelliGent ICU Outcomes），旨在增强对患者病情严重程度、转换状态以及生命维持治疗需求的预测。我们收集了一个多模态ICU-Multimodal数据集，该数据集整合了电子健康记录数据、穿戴传感器数据、患者的面部表情视频以及环境传感器数据，并利用这些数据集训练了MANGO模型。MANGO模型采用基于Transformer的掩码自注意力机制的多模态特征融合网络，即使某些模态缺失，也能捕捉和学习这些不同数据模态之间的复杂交互。结果显示，整合多种模态显著提高了模型预测病情严重程度、转换状态以及生命维持治疗需求的能力。最佳模型的受试者操作特征曲线下面积（AUC）在预测病情严重程度转换和生命维持治疗需求方面达到0.76（95% CI：0.72-0.79），在预测病情严重程度方面达到0.82（95% CI：0.69-0.89）... 

---
# RUL forecasting for wind turbine predictive maintenance based on deep learning 

**Title (ZH)**: 基于深度学习的风力发电机剩余使用寿命预测及其在预测性维护中的应用 

**Authors**: Syed Shazaib Shah, Tan Daoliang, Sah Chandan Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2412.17823)  

**Abstract**: Predictive maintenance (PdM) is increasingly pursued to reduce wind farm operation and maintenance costs by accurately predicting the remaining useful life (RUL) and strategically scheduling maintenance. However, the remoteness of wind farms often renders current methodologies ineffective, as they fail to provide a sufficiently reliable advance time window for maintenance planning, limiting PdM's practicality. This study introduces a novel deep learning (DL) methodology for future RUL forecasting. By employing a multi-parametric attention-based DL approach that bypasses feature engineering, thereby minimizing the risk of human error, two models: ForeNet-2d and ForeNet-3d are proposed. These models successfully forecast the RUL for seven multifaceted wind turbine (WT) failures with a 2-week forecast window. The most precise forecast deviated by only 10 minutes from the actual RUL, while the least accurate prediction deviated by 1.8 days, with most predictions being off by only a few hours. This methodology offers a substantial time frame to access remote WTs and perform necessary maintenance, thereby enabling the practical implementation of PdM. 

**Abstract (ZH)**: 预测性维护（PdM）正逐渐被追求，以通过准确预测剩余使用寿命（RUL）和战略性安排维护来减少风电场运行和维护成本。然而，风电场的偏远位置往往使得现有方法无效，因为它们无法为维护计划提供足够可靠的提前时间窗口，从而限制了PdM的实际应用。本研究引入了一种新的深度学习（DL）方法，用于未来RUL预测。通过采用一种多参数注意力机制的DL方法，绕过了特征工程，从而降低了人为错误的风险，提出了两种模型：ForeNet-2d和ForeNet-3d。这两种模型成功地在两周的预测窗口内预测了七种复杂风力发电机（WT）故障的RUL。最准确的预测仅偏离实际RUL 10分钟，而最不准确的预测偏离了1.8天，大多数预测的偏差仅为几个小时。该方法提供了一个显著的时间窗口，以便访问远程风力发电机并进行必要的维护，从而使得PdM的实际应用成为可能。 

---
# The Rosetta Paradox: Domain-Specific Performance Inversions in Large Language Models 

**Title (ZH)**: 罗赛塔悖论：大型语言模型在专业领域中的性能倒置 

**Authors**: Basab Jha, Ujjwal Puri  

**Link**: [PDF](https://arxiv.org/pdf/2412.17821)  

**Abstract**: While large language models, such as GPT and BERT, have already demonstrated unprecedented skills in everything from natural language processing to domain-specific applications, there came an unexplored phenomenon we term the Rosetta Paradox. The Rosetta Paradox characterizes the counterintuitive performance inversions across domains of knowledge. This paradox captures how such LLMs can excel in highly specialized fields but do poorly on tasks which require general, everyday knowledge. This paper formalizes the definition of the Rosetta Paradox and introduces a panoramic analysis framework that includes both a Domain Specificity Index (DSI) and a Performance Inversion Metric (PIM) for consistent quantification of domain-specific behavior in LLMs.
We adopt this paradox and conduct a series of investigations through extensive experiments across diverse models and knowledge domains, ranging from rich technical areas to common-sense reasoning. Our findings indicate that the Rosetta Paradox is likely not a mere artifact of data distribution but an intrinsic architectural and emergent property of deep neural networks. We present comparative analyses across different model architectures, sizes, and training methodologies that shed light into the peculiar ways this paradox manifests itself and challenge the standard evaluation metrics. 

**Abstract (ZH)**: 尽管诸如GPT和BERT这样的大规模语言模型已经在自然语言处理以及特定领域的应用中展示了前所未有的能力，但仍出现了一种未被探索的现象，我们称之为罗塞塔悖论。罗塞塔悖论描述了知识领域间不直观的性能反转现象。这一悖论反映了这些LLM在专门领域的卓越表现，但在需要一般性和日常生活知识的任务上却表现不佳。本文正式定义了罗塞塔悖论，并引入了一个全景分析框架，包括领域特异性指数（Domain Specificity Index，DSI）和性能反转指标（Performance Inversion Metric，PIM），以一致地量化LLM在特定领域的行为。

我们采用了这一悖论，并通过广泛的实验对不同模型和知识领域进行了一系列调查，范围从丰富的技术领域到常识推理。我们的研究结果表明，罗塞塔悖论很可能不仅是一种数据分布的产物，而是深层神经网络的内在架构和涌现属性。我们对不同模型架构、规模和训练方法进行了比较分析，揭示了这一悖论特有的表现方式，并挑战了现有的评价指标。 

---
# Inductive Linguistic Reasoning with Large Language Models 

**Title (ZH)**: 大型语言模型中的归纳语义推理 

**Authors**: Raghav Ramji, Keshav Ramji  

**Link**: [PDF](https://arxiv.org/pdf/2412.17819)  

**Abstract**: Evaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1% and Llama-3.1-405B-Instruct by 5.9% over chain-of-thought approaches. These gains are attributable to the analogical demonstrations, both when self-generated as well as when produced by weaker multilingual models. Furthermore, we demonstrate that our method generalizes to other tasks present in Linguistics Olympiad competitions, achieving sizable improvements across all problem types and difficulty levels included in the LINGOLY dataset with GPT-4o. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods. 

**Abstract (ZH)**: 评估大型语言模型（LLMs）在语言推理能力上的表现是理解其技能差距的重要任务，特别是在大规模应用时可能会浮现这些差距。在本文中，我们通过语言谜题的视角研究这些模型在极度低资源语言上的抽象多语言推理能力。由于这些翻译任务涉及从参照实例进行归纳和演绎推理，我们考察了是否可以通过类比提示自动生成多样化的辅助示例，并从种子示例中进行推导。我们采用两阶段的方法，首先使用语言模型生成类比示例，然后在提供目标语言示例的情况下将其应用于上下文。我们在modeLing数据集上的结果显示，类比提示在激发模型对语言语法规则相似性的了解方面是有效的，相较于链式思维方法，GPT-4o和Llama-3.1-405B-Instruct的性能分别提高了8.1%和5.9%。这些提升可以归因于无论是自动生成还是由较弱的多语言模型生成的类比示例。此外，我们展示了我们的方法可以推广到 Linguistics Olympiad 竞赛中的其他任务，在LINGOLY数据集中，所有问题类型和难度级别的表现均有所显著改善。我们还报告了一些有趣的发现，这些发现揭示了影响语言推理性能的现象，表明这些谜题对于新推理方法的基准具有重要价值。 

---
# GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep Reinforcement Learning 

**Title (ZH)**: GOPT：基于变压器的深度强化学习的通用在线3D容器打包方法 

**Authors**: Heng Xiong, Changrong Guo, Jian Peng, Kai Ding, Wenjie Chen, Xuchong Qiu, Long Bai, Jianfeng Xu  

**Link**: [PDF](https://arxiv.org/pdf/2409.05344)  

**Abstract**: Robotic object packing has broad practical applications in the logistics and automation industry, often formulated by researchers as the online 3D Bin Packing Problem (3D-BPP). However, existing DRL-based methods primarily focus on enhancing performance in limited packing environments while neglecting the ability to generalize across multiple environments characterized by different bin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin Packing approach via Transformer-based deep reinforcement learning (DRL). First, we design a Placement Generator module to yield finite subspaces as placement candidates and the representation of the bin. Second, we propose a Packing Transformer, which fuses the features of the items and bin, to identify the spatial correlation between the item to be packed and available sub-spaces within the bin. Coupling these two components enables GOPT's ability to perform inference on bins of varying dimensions. We conduct extensive experiments and demonstrate that GOPT not only achieves superior performance against the baselines, but also exhibits excellent generalization capabilities. Furthermore, the deployment with a robot showcases the practical applicability of our method in the real world. The source code will be publicly available at this https URL. 

**Abstract (ZH)**: 机器人物体包装在物流和自动化行业中具有广泛的实际应用，通常被研究人员形式化为在线三维箱子装载问题（3D Bin Packing Problem, 3D-BPP）。然而，现有的基于深度强化学习（Deep Reinforcement Learning, DRL）的方法主要集中在提升在有限包装环境中的性能，而忽视了跨越不同尺寸箱子环境的泛化能力。为了解决这一问题，我们提出了一种名为GOPT的方法，它是一种基于Transformer的深度强化学习的可泛化的在线三维箱子装载方法。首先，我们设计了一个放置生成器模块，提供有限的子空间作为放置候选，并表示箱子的状态。其次，我们提出了一种装载Transformer，它融合了物品和箱子的特征，以识别待包装物品与其在箱子中可用子空间之间的空间相关性。结合这两个组件，GOPT能够执行不同尺寸箱子的推理任务。我们在广泛的实验中展示了其性能和泛化能力。此外，机器人部署的实际应用证明了该方法在现实世界中的实用价值。源代码将在此网址公开：[此网址]。 

---
# Causal Deep Learning 

**Title (ZH)**: 因果深度学习 

**Authors**: M. Alex O. Vasilescu  

**Link**: [PDF](https://arxiv.org/pdf/2301.00314)  

**Abstract**: We derive a set of causal deep neural networks whose architectures are a consequence of tensor (multilinear) factor analysis, a framework that facilitates forward and inverse causal inference. Forward causal questions are addressed with a neural architecture composed of causal capsules and a tensor transformer. Causal capsules compute a set of invariant causal factor representations, whose interactions are governed by a tensor transformation. Inverse causal questions are addressed with a neural network that implements the multilinear projection algorithm. The architecture reverses the order of the operations of a forward neural network and estimates the causes of effects. As an alternative to aggressive bottleneck dimension reduction or regularized regression that may camouflage an inherently underdetermined inverse problem, we prescribe modeling different aspects of the mechanism of data formation with piecewise tensor models whose multilinear projections produce multiple candidate solutions. Our forward and inverse questions may be addressed with shallow architectures, but for computationally scalable solutions, we derive a set of deep neural networks by taking advantage of block algebra. An interleaved kernel hierarchy results in a doubly non-linear tensor factor models. The causal neural networks that are a consequence of tensor factor analysis are data agnostic, but are illustrated with facial images. Sequential, parallel and asynchronous parallel computation strategies are described. 

**Abstract (ZH)**: 我们将推导出一系列因果深度神经网络，其架构源自张量（多重线性）因子分析框架，该框架支持正向和反向因果推理。正向因果问题通过由因果胶囊和张量变换组成的神经架构来解决。因果胶囊计算一组不变的因果因子表示，这些表示的相互作用由张量变换规则。反向因果问题则通过实现多线性投影算法的神经网络来解决。该架构逆转了正向神经网络的操作顺序，并估算效果的原因。作为激进的瓶颈维度缩减或正则化回归的替代方案，我们建议使用分段张量模型来建模数据形成机制的不同方面，这些模型的多线性投影产生多个候选解。虽然我们的正向和反向问题可以使用浅层架构来解决，但为了实现计算上的可扩展性，我们通过利用块代数推导出一系列深层神经网络。交错内核层次结构导致双重非线性张量因子模型。源自张量因子分析的因果神经网络对数据具有无偏性，但演示中使用了面部图像。文中描述了顺序、并行和异步并行计算策略。 

---
