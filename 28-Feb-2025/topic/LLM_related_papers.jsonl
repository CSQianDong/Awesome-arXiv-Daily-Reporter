{'arxiv_id': 'arXiv:2502.19722', 'title': 'Few-Shot Multilingual Open-Domain QA from 5 Examples', 'authors': 'Fan Jiang, Tom Drummond, Trevor Cohn', 'link': 'https://arxiv.org/abs/2502.19722', 'abstract': 'Recent approaches to multilingual open-domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a \\emph{few-shot learning} approach to synthesise large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, \\textsc{FsModQA}, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a \\emph{cross-lingual prompting} strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.', 'abstract_zh': '近年来，面向多语言开放领域问答（MLODQA）的方法在充足的特定语言训练数据的条件下取得了令人鼓舞的结果。然而，显著的标注成本限制了这些方法在欠代表语言中的应用。我们提出了一种基于少样本学习的方法，利用大型语言模型（LLMs）合成大规模多语言数据。该方法首先使用WikiData进行大规模自我监督预训练，随后利用少样本监督提示LLMs生成高质量的多语言合成数据进行训练。最终模型FsModQA在MLODQA和跨语言及单语言检索任务中显著优于现有的少样本和监督基准。我们进一步证明，通过仅使用英语监督数据的跨语言提示策略，该方法可以有效零样本适应到新语言，从而提供了一种适用于MLODQA任务的通用且可实现的解决方案，而无需昂贵的大规模标注成本。', 'title_zh': '从5个示例进行少样本多语言开放域问答'}
{'arxiv_id': 'arXiv:2502.20309', 'title': 'EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants', 'authors': 'Franck Cappello, Sandeep Madireddy, Robert Underwood, Neil Getty, Nicholas Lee-Ping Chia, Nesar Ramachandra, Josh Nguyen, Murat Keceli, Tanwi Mallick, Zilinghan Li, Marieme Ngom, Chenhui Zhang, Angel Yanguas-Gil, Evan Antoniuk, Bhavya Kailkhura, Minyang Tian, Yufeng Du, Yuan-Sen Ting, Azton Wells, Bogdan Nicolae, Avinash Maurya, M. Mustafa Rafique, Eliu Huerta, Bo Li, Ian Foster, Rick Stevens', 'link': 'https://arxiv.org/abs/2502.20309', 'abstract': 'Recent advancements have positioned AI, and particularly Large Language Models (LLMs), as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.', 'abstract_zh': '近年来，人工智能，尤其是大型语言模型（LLMs），已被视为推动科学研究变革的工具，能够在需要推理、问题解决和决策的任务中发挥作用。它们的出色能力表明它们在科学研究助理方面具有巨大的潜力，但也强调了对其进行全面、严谨且领域特定的评估的必要性，以评估其在实际科学应用中的有效性。本文描述了阿贡国家实验室开发的一种多维度方法，用于评估人工智能模型作为科学研究助理（EAIRA）。该方法包含四种主要的评估类别。1) 多选题以评估事实记忆；2) 开放式回答以评估高级推理和问题解决能力；3) 实验室风格的实验，通过在受控环境中详细分析其作为研究助理的能力来评估其能力；4) 现场风格的实验，以在广泛的科学领域和应用中捕捉研究人员与LLM的交互情况。这些互补的方法能够全面分析LLMs在科学知识、推理能力和适应性方面的优势和劣势。考虑到LLMs快速发展的步伐，我们设计了该方法以便于其不断进化和适应，以确保其持续的相关性和适用性。本文描述了截至2025年2月底的方法状态。尽管该方法是在某些科学领域内开发的，但设计目的是使其能够广泛应用于各种科学领域。', 'title_zh': 'EAIRA：建立评估人工智能模型作为科学研究助手的方法论'}
{'arxiv_id': 'arXiv:2502.19918', 'title': 'Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning in Large Language Models', 'authors': 'Yuan Sui, Yufei He, Tri Cao, Simeng Han, Bryan Hooi', 'link': 'https://arxiv.org/abs/2502.19918', 'abstract': 'Large Language Models (LLMs) increasingly rely on prolonged reasoning chains to solve complex tasks. However, this trial-and-error approach often leads to high computational overhead and error propagation, where early mistakes can derail subsequent steps. To address these issues, we introduce Meta-Reasoner, a framework that dynamically optimizes inference-time reasoning by enabling LLMs to "think about how to think." Drawing inspiration from human meta-cognition and dual-process theory, Meta-Reasoner operates as a strategic advisor, decoupling high-level guidance from step-by-step generation. It employs "contextual multi-armed bandits" to iteratively evaluate reasoning progress, and select optimal strategies (e.g., backtrack, clarify ambiguity, restart from scratch, or propose alternative approaches), and reallocates computational resources toward the most promising paths. Our evaluations on mathematical reasoning and puzzles highlight the potential of dynamic reasoning chains to overcome inherent challenges in the LLM reasoning process and also show promise in broader applications, offering a scalable and adaptable solution for reasoning-intensive tasks.', 'abstract_zh': '大型语言模型（LLMs）越来越多地依赖于长期的推理链来解决复杂的任务。然而，这种试错方法通常会导致计算开销高昂并产生错误传播的问题，即早期的错误可能会影响后续的步骤。为了解决这些问题，我们提出了一种名为Meta-Reasoner的框架，该框架通过使LLMs能够“思考如何思考”来动态优化推理过程。受到人类元认知和双重过程理论的启发，Meta-Reasoner充当一个策略顾问，将高层次的指导与逐步生成过程分离。它使用“上下文多臂老虎机”来逐步评估推理进展，并选择最优策略（例如回退、澄清歧义、从头开始或提出替代方法），并将计算资源重新分配到最有前途的路径上。我们对数学推理和谜题的评估突显了动态推理链在克服LLMs推理过程中的固有挑战方面的潜力，并表明在更广泛的应用中具有前景，提供了一种可扩展和适应性强的解决方案，适用于涉及大量推理的任务。', 'title_zh': '元推理器：用于大型语言模型优化推理时推理的动态指导'}
{'arxiv_id': 'arXiv:2502.19915', 'title': 'LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty', 'authors': 'Jiahui Cen, Jianghao Lin, Weizhong Xuan, Dong Zhou, Jin Chen, Aimin Yang, Yongmei Zhou', 'link': 'https://arxiv.org/abs/2502.19915', 'abstract': "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) - students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism - implementing personalized knowledge acquisition through gated networks and updating student knowledge state. Experimental results on two real datasets show our method consistently outperforms nine baseline models, improving AUC metrics by 2% to 10% while effectively addressing cold-start problems and enhancing model interpretability.", 'abstract_zh': '知识追踪（KT）是智能辅导系统中的基本技术，用于模拟学生在学习过程中知识状态的变化、跟踪个性化知识掌握情况，并预测表现。然而，当前的KT模型面临三大挑战：（1）当遇到新问题时，模型因交互记录稀疏而面临冷启动问题，使得精确建模变得困难；（2）传统的模型仅使用历史交互记录进行个性化建模，无法准确跟踪个体掌握水平，导致个性化建模模糊不清；（3）决策过程对教育者来说是透明的，使得他们难以理解模型的判断。为了解决这些问题，我们提出了一种新的双通道难度感知知识追踪（DDKT）框架，该框架利用大型语言模型（LLMs）和检索增强生成（RAG）进行主观难度评估，同时结合难度偏差感知算法和学生掌握算法进行精确难度测量。我们的框架引入了三个关键创新：（1）难度平衡感知序列（DBPS）——结合学生的主观感知和客观难度，通过注意力机制测量LLMs评估难度、数学统计难度和学生主观感知难度之间的差距；（2）难度掌握比例（DMR）——通过不同的难度区间进行精确的学生掌握水平建模；（3）知识状态更新机制——通过门控网络实现个性化知识的获取，并更新学生的知识状态。在两个真实数据集上的实验结果表明，我们的方法在AUC指标上优于九个基准模型，提高了2%到10%，同时有效解决了冷启动问题，提高了模型的可解释性。', 'title_zh': '由大规模语言模型驱动的有效知识追踪：通过集成双通道难度机制'}
{'arxiv_id': 'arXiv:2502.19902', 'title': 'Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy', 'authors': 'Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie', 'link': 'https://arxiv.org/abs/2502.19902', 'abstract': "Building an agent that can mimic human behavior patterns to accomplish various open-world tasks is a long-term goal. To enable agents to effectively learn behavioral patterns across diverse tasks, a key challenge lies in modeling the intricate relationships among observations, actions, and language. To this end, we propose Optimus-2, a novel Minecraft agent that incorporates a Multimodal Large Language Model (MLLM) for high-level planning, alongside a Goal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP contains (1) an Action-guided Behavior Encoder that models causal relationships between observations and actions at each timestep, then dynamically interacts with the historical observation-action sequence, consolidating it into fixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with open-ended language instructions to predict actions auto-regressively. Moreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)} dataset, which contains 25,000 videos across 8 atomic tasks, providing about 30M goal-observation-action pairs. The automated construction method, along with the MGOA dataset, can contribute to the community's efforts to train Minecraft agents. Extensive experimental results demonstrate that Optimus-2 exhibits superior performance across atomic tasks, long-horizon tasks, and open-ended instruction tasks in Minecraft.", 'abstract_zh': '构建能够模拟人类行为模式的代理以完成各种开放世界任务是一项长期目标。为了使代理能够有效地跨多种任务学习行为模式，一个关键挑战在于建模观测、动作和语言之间的复杂关系。为此，我们提出了Optimus-2，这是一种结合了多模态大型语言模型（MLLM）进行高层次规划，并结合了目标-观测-动作条件策略（GOAP）进行低层次控制的新型Minecraft代理。GOAP 包含以下两个组成部分：(1) 行动导向的行为编码器，该编码器在每个时间步长中建模观测与动作之间的因果关系，然后与历史观测-动作序列动态交互，将其合并成固定长度的行为令牌；(2) MLLM，该模型将行为令牌与开放性语言指令对齐以自回归地预测动作。此外，我们还引入了一个高质量的Minecraft目标-观测-动作（MGOA）数据集，该数据集包含25,000个视频，横跨8个原子任务，提供了大约3000万个目标-观测-动作对。自动化构建方法以及MGOA数据集能够为Minecraft代理的训练工作做出贡献。广泛的实验结果表明，Optimus-2在Minecraft中的原子任务、长时任务和开放式指令任务中均表现出卓越的性能。', 'title_zh': 'Optimus-2：基于目标-观察-动作条件策略的多模态Minecraft代理'}
{'arxiv_id': 'arXiv:2502.19613', 'title': 'Self-rewarding correction for mathematical reasoning', 'authors': 'Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, Tong Zhang', 'link': 'https://arxiv.org/abs/2502.19613', 'abstract': "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.", 'abstract_zh': '我们研究了一种自我奖励推理的大语言模型（LLMs），它能够在推理过程中同时生成逐步推理过程并在输出时评估其正确性，无需外部反馈。这种集成方法允许单个模型独立指导其推理过程，为模型部署提供了计算上的优势。我们特别关注自我修正任务，即模型能够自主检测其回复中的错误、修正输出，并决定何时终止迭代精炼循环。为了实现这一目标，我们提出了一种仅使用自动生成数据的两阶段算法框架，以构建自我奖励推理模型。在第一阶段，我们采用序列拒绝采样方法合成包含自我奖励和自我修正机制的长推理轨迹。通过在这些精心筛选的数据上微调模型，可以让模型学习自我奖励和自我修正的模式。在第二阶段，我们进一步通过基于规则的信号进行强化学习，增强模型评估响应准确性和精炼输出的能力。实验结果显示，我们的方法超越了内在的自我修正能力，并且达到了与依赖外部奖励模型的系统相当的性能。', 'title_zh': '数学推理中的自我奖励修正'}
{'arxiv_id': 'arXiv:2502.19500', 'title': 'Conversational Planning for Personal Plans', 'authors': 'Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matarić', 'link': 'https://arxiv.org/abs/2502.19500', 'abstract': "The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.", 'abstract_zh': '大型语言模型（LLMs）的语言生成和推理能力使得各类任务中表现出色的对话系统成为可能，从代码生成到创作论文，再到通过科学、技术和法律考试，甚至开启了一种新的知识搜索范式。除了这些短期应用，LLMs还被越来越多地用于帮助实现长期目标或持续时间较长的任务，这些任务可能涉及几天、几周、几个月甚至几年的时间跨度。因此，为了使对话系统能够处理长期的互动和任务，我们需要能够为长期展望进行规划的语言基础代理。传统上，这种能力是由具有层次规划能力的强化学习代理来解决的。在本项工作中，我们探索了一种新型架构，其中LLM作为高级控制器决定代理的下一步宏观动作，利用工具使用的LLM基于选项策略执行选定的宏观动作。我们为特定的宏观动作构建了这一框架，以通过对话和后续问题收集用户反馈来实现用户个人计划的适应性规划。我们展示了这一范式在从学术和非学术任务的教学到个人健康计划的对话式辅导等各种场景中的应用前景。', 'title_zh': '个人计划的对话规划'}
{'arxiv_id': 'arXiv:2502.20364', 'title': 'Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization', 'authors': 'Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov', 'link': 'https://arxiv.org/abs/2502.20364', 'abstract': 'Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.', 'abstract_zh': '基于大型语言模型（LLMs）的代理生成人工智能，借助检索增强生成（RAG）、知识图谱（KGs）和向量存储（VSs），代表了一种变革性的技术，适用于诸如法律系统、研究、推荐系统、网络安全以及大规模安全，包括扩散研究等专门领域。该技术擅长推断大量非结构化或半结构化数据集中的关系。这里的法律领域包括复杂的数据，这些数据具有广泛的、相互关联的和半结构化的知识系统，具有复杂的关系。它包括宪法、法律法规、规章制度和判例法。从复杂的法律文件和其关系中提取洞察力并导航它们的网络对于有效的法律研究至关重要。为了解决这个问题，我们介绍了一种集成了RAG、VS和KG的生成AI系统，通过非负矩阵分解（NMF）进行构建，以增强法律信息检索和AI推理并减少幻觉。在法律系统中，这些技术赋予AI代理识别和分析案件、法律条文和先例之间的复杂联系的能力，揭示隐藏的关系并预测法律趋势，这是确保公正和提高运营效率必不可少的任务。我们的系统采用网页抓取技术系统地收集法律文本，如法律法规、宪法条款和判例法，来自类似于Justia的公共访问平台。它通过利用先进的语义表示、层次关系和潜在主题发现，弥补了传统关键词搜索与上下文理解之间的差距。该框架支持法律文件聚类、摘要和跨参照，以实现半结构化数据的大规模、可解释和准确检索，从而推动计算法学和人工智能的发展。', 'title_zh': '法律知识与人工智能融合：基于向量存储、知识图谱和层次非负矩阵分解的检索增强生成'}
{'arxiv_id': 'arXiv:2502.20356', 'title': 'Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs', 'authors': 'Kuan Lok Zhou, Jiayi Chen, Siddharth Suresh, Reuben Narad, Timothy T. Rogers, Lalit K Jain, Robert D Nowak, Bob Mankoff, Jifan Zhang', 'link': 'https://arxiv.org/abs/2502.20356', 'abstract': "Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.", 'abstract_zh': '大规模语言模型（LLMs）在理解创造性内容方面显示出明显的局限性，这在Hessel等人（2023）对《纽约客》漫画标题比赛（NYCCC）的研究中得到了证实。他们的研究揭示了LLMs与人类在幽默理解方面存在显著差距，确立了理解和评估创造性内容是AI发展中面临的关键挑战。我们通过将幽默理解分解为三个组成部分，并系统地改进每一部分，重新审视了这一挑战：通过改进注释增强视觉理解，利用LLM生成的幽默推理和解释，以及实施针对人类偏好数据的定向对齐。我们的改进方法在标题排名中实现了82.4%的准确率，显著超越了之前的67%基准，并与该领域世界知名的人类专家的表现相匹敌。值得注意的是，通过各种人格提示模拟子群体偏好的尝试几乎未产生影响，而用民众偏好对模型进行微调则表现出显著的有效性。这些发现表明，通过针对特定子群和个体进行聚焦对齐，可以有效地解决LLMs在创造性判断方面的能力局限。最后，我们提出观点，要实现通用人工智能，需要系统地收集跨创造领域的个体偏好数据。我们认为，正如人类创造力深受个体和文化偏好的影响，用多元化的个体偏好数据训练LLM可能对于培养真正的创造性理解至关重要。', 'title_zh': '弥合创造力理解差距：小型规模的人类对齐使大语言模型在幽默排序方面的水平达到专家级别'}
{'arxiv_id': 'arXiv:2502.20339', 'title': 'Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners', 'authors': 'Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Zico Kolter, Albert Gu, François Fleuret, Tri Dao', 'link': 'https://arxiv.org/abs/2502.20339', 'abstract': 'Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.', 'abstract_zh': '近年来的研究表明，通过在测试时扩展计算资源，大型语言模型（LLMs）的性能可以显著提升。一种常见的策略是生成多个推理路径（Chain-of-Thought, CoT）并利用各种选择机制聚合它们的输出。这引发了一个基本问题：低复杂度的模型是否可以通过利用其优越的生成吞吐量，在固定的计算预算下超越同样规模的Transformer模型？为了回答这个问题，并克服缺乏强次线性推理器的挑战，我们从预训练的Transformer模型中提炼了纯和混合Mamba模型。仅在80亿个令牌上进行训练，我们的提炼模型在数学推理数据集上表现出色且具有良好的扩展性，同时在对大型批处理和长序列进行推理时速度更快。尽管由于提炼而产生的零样本性能损失，但纯和混合Mamba模型在固定时间预算下可以扩展其覆盖范围和准确性，从而开辟了扩展推理计算的新方向。', 'title_zh': '慢思考，快思考：通过提炼推理器扩展推理计算能力'}
{'arxiv_id': 'arXiv:2502.20335', 'title': 'Expertise Is What We Want', 'authors': 'Alan Ashworth, Munir Al-Dajani, Keegan Duchicela, Kiril Kafadarov, Allison Kurian, Othman Laraki, Amina Lazrak, Divneet Mandair, Wendy McKennon, Rebecca Miksad, Jayodita Sanghvi, Travis Zack', 'link': 'https://arxiv.org/abs/2502.20335', 'abstract': 'Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability.\nTo highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.', 'abstract_zh': '临床决策依赖于专家推理，而这种推理则由标准化和基于证据的指南指导。然而，将这些指南转化为自动化临床决策支持系统可能会引入不准确性和重要细节的丧失。本文介绍了一种应用架构——大型语言专家（LLE），该架构结合了大型语言模型（LLMs）的灵活性和强大功能与专家系统的可解释性、可解释性和可靠性。LLMs有助于解决专家系统的关键挑战，如知识整合与编码，以及数据规范。相反，专家系统的方法有助于克服大型语言模型的挑战，包括幻觉、原子级和低成本更新，以及测试性。\n\n为凸显大型语言专家（LLE）系统的强大功能，我们构建了一个LLE系统，用于协助新诊断为癌症患者的诊疗工作。及时开始癌症治疗对于优化患者预后至关重要。然而，诊断建议不断增加的复杂性使得初级保健医生难以确保患者在首次与肿瘤科医生会面前完成了必要的诊疗工作。与许多实际临床任务类似，这些诊疗工作要求分析未结构化的医疗记录并应用复杂的临床决策逻辑。本研究描述了一个LLE系统的设计与评估，该系统能够迅速识别并建议正确的诊断工作。该系统展示了极高的临床准确度（>95%），并有效填补了大型学术中心乳腺癌和结肠癌患者在实际数据中发现的空白。', 'title_zh': '专家知识正是我们所需要的'}
{'arxiv_id': 'arXiv:2502.20332', 'title': 'Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models', 'authors': 'Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb', 'link': 'https://arxiv.org/abs/2502.20332', 'abstract': 'Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.', 'abstract_zh': '许多近期的研究发现了大型语言模型中涌现推理能力的证据，但这些能力的稳健性以及它们依赖于结构化推理机制的程度仍然存在争论。为了解决这些问题，我们对一个开源语言模型（Llama3-70B）中支持抽象规则归纳的内部机制进行了全面研究。我们发现了一种涌现的符号结构，该结构通过一系列三次计算实现了抽象推理。在早期层中，符号抽象头基于输入标记之间的关系将输入标记转换为抽象变量。在中间层中，符号归纳头在这些抽象变量上进行序列归纳。最后，在后期层中，检索头通过检索与预测的抽象变量相关的值来预测下一个标记。这些结果揭示了符号方法和神经网络方法之间长期争论的可能解决方案，表明神经网络中的涌现推理依赖于符号机制的涌现。', 'title_zh': '大型语言模型中 Emergent 符号机制支持抽象推理'}
{'arxiv_id': 'arXiv:2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'authors': 'Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, Guokan Shang', 'link': 'https://arxiv.org/abs/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'abstract_zh': '随着大语言模型在在线内容生成中扮演越来越重要的角色，人们对反复处理其自身输出的影响表示了关注。受到链式人类交流中“电话串烧”效应的启发，本研究探讨了大语言模型是否也通过迭代生成过程中扭曲信息。通过基于翻译的实验，我们发现信息的扭曲会随时间积累，受语言选择和链式复杂性的影响。虽然降级是不可避免的，但可以通过战略性提示技术来减轻。这些发现为讨论AI中介信息传播的长期影响做出了贡献，并引发了关于在迭代流程中生成内容的大语言模型可靠性的关键问题。', 'title_zh': '大型语言模型作为Broken Telephone游戏：迭代生成扭曲信息'}
{'arxiv_id': 'arXiv:2502.20122', 'title': 'Self-Training Elicits Concise Reasoning in Large Language Models', 'authors': 'Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun', 'link': 'https://arxiv.org/abs/2502.20122', 'abstract': 'Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at this https URL', 'abstract_zh': '链式思考（Chain-of-Thought, CoT）推理使大型语言模型（LLMs）能够通过中间令牌进行额外的计算来解决复杂任务。然而，我们提出，典型的推理路径中包含了许多冗余令牌，导致不必要的推理成本。通过对当前LLMs的输出分布进行分析，我们发现它们潜在地具有更加简洁推理的能力，相对于其默认行为。为了激发这种能力，我们提出了简单的小样本调优方法，该方法利用通过best-of-N抽样和少量样本条件生成的简洁推理路径，在特定任务中实现这一目标。我们的综合方法在五个模型家族上（GSM8K和MATH）平均减少了30%的输出令牌，同时保持了平均准确率。通过利用LLMs的基本随机性和上下文学习能力，我们的自训练方法稳定地激发了各种模型上的简洁推理能力，包括那些进行了大量后续训练的模型。相关代码可在此网址获取：this https URL', 'title_zh': '自我训练促使大型语言模型进行简洁推理'}
{'arxiv_id': 'arXiv:2502.20073', 'title': 'Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents', 'authors': 'Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang', 'link': 'https://arxiv.org/abs/2502.20073', 'abstract': 'Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 10 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaption that are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30 open-ended tasks, and an integrated evaluation package are now publicly available at this https URL.', 'abstract_zh': '基于大型语言模型（LLMs）的代理系统已经在超出传统NLP任务的实际应用中取得了显著进展。本文提出了一种新的LLM赋能的多代理系统（LLM-MAS）基准测试——Collab-Overcooked，该基准系统基于流行的Overcooked-AI游戏，并引入了更多适用且具有挑战性的交互环境任务。Collab-Overcooked 从两个新颖的角度扩展了现有的基准测试。首先，它提供了一个支持多种任务和目标的多代理框架，并通过自然语言通信促进协作。其次，它引入了一系列过程导向的评估指标，用以评估不同LLM代理的细粒度协作能力，这是以往工作中经常被忽视的一个维度。我们对10种流行的LLM进行了广泛的实验，并展示了虽然这些LLM在目标理解方面表现出强大的能力，但在主动协作和持续适应性方面仍存在显著差异，这些能力对于高效完成复杂任务至关重要。值得注意的是，我们指出了LLM-MAS的优势和不足，并提供了一种统一和开源的基准来改进和评估LLM-MAS。当前，环境、30项开放任务以及集成评估包已公开，访问链接为：this https URL。', 'title_zh': 'Collab-Overcooked: 评估大型语言模型作为协作代理的基准测试与评估'}
{'arxiv_id': 'arXiv:2502.19954', 'title': 'Collaborative Stance Detection via Small-Large Language Model Consistency Verification', 'authors': 'Yu Yan, Sheng Sun, Zixiang Tang, Teli Liu, Min Liu', 'link': 'https://arxiv.org/abs/2502.19954', 'abstract': 'Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.', 'abstract_zh': '社交媒体上的立场检测旨在识别推文中对特定目标表达的态度。当前研究倾向于使用大规模语言模型（LLMs），因为它们的性能提升效果显著。然而，仅仅依赖LLMs进行立场检测，在不考虑成本的情况下，对于需要大量数据分析的现实世界社交媒体监控系统来说是不切实际的。为了解决这一问题，我们提出了一种名为**\\textbf{\\underline{Co}}**llaborative Stance Detection via \\textbf{\\underline{Ver}}ification of Small and Large Language Model Consistency (\\textbf{CoVer}) 的框架，该框架通过上下文共享批次推理和LLM与SLM之间的逻辑验证来增强LLM的利用。具体而言，CoVer并不是逐条处理文本，而是逐批次处理文本，通过共享上下文中的LLM推理来获得立场预测及其相应的解释。然后，为了排除由上下文噪声引起的偏差，CoVer引入了SLM进行逻辑一致性验证。最后，表现出低逻辑一致性的文本重复出现时，使用一致性加权聚合的先验LLM立场预测进行分类。我们的实验表明，在零样本设置下，CoVer在多个基准测试中优于最先进的方法，平均每条推文只需要0.54次LLM查询，同时显著提升了性能。我们的CoVer为社交媒体立场检测中的LLM部署提供了更加实际的解决方案。', 'title_zh': '基于小型-大型语言模型一致性验证的协作立场检测'}
{'arxiv_id': 'arXiv:2502.19860', 'title': 'MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue', 'authors': 'Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan', 'link': 'https://arxiv.org/abs/2502.19860', 'abstract': "Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.", 'abstract_zh': '当今竞争激烈的社会中，心理健康问题如抑郁和焦虑正在加剧。传统的治疗方法，如咨询和聊天机器人，往往无法有效吸引患者，并且常常提供缺乏情感深度的通用回应。尽管大型语言模型（LLMs）有可能创造更加人机互动的体验，但它们仍然难以捕捉微妙的情感。因此，亟需让LLMs具备类似人类的适应性和暖意。为填补这一空白，我们提出了一种名为MIND（Multi-agent INner Dialogue）的新颖范式，旨在提供更沉浸式的心理疗愈环境。考虑到LLM代理的强大生成能力和角色扮演能力，我们预先定义了一个互动疗愈框架，并将不同的角色分配给LLM代理，使其能够与用户进行互动的内心对话，从而提供更加沉浸式的疗愈体验。我们在多个现实世界的疗愈维度中进行了广泛的人类实验，并发现MIND为用户提供了一种更为友好且更具沉浸感的体验。这表明，MIND能够有效利用LLMs在心理疗愈领域的巨大潜力。', 'title_zh': 'MIND：走向多代理内心对话的沉浸式心理疗愈'}
{'arxiv_id': 'arXiv:2502.19680', 'title': 'M-LLM Based Video Frame Selection for Efficient Video Understanding', 'authors': 'Kai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran, Tal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid, Bing Yin, Trishul Chilimbi', 'link': 'https://arxiv.org/abs/2502.19680', 'abstract': "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising results in video reasoning. Popular Multi-Modal Large Language Model (M-LLM) frameworks usually apply naive uniform sampling to reduce the number of video frames that are fed into an M-LLM, particularly for long context videos. However, it could lose crucial context in certain periods of a video, so that the downstream M-LLM may not have sufficient visual information to answer a question. To attack this pain point, we propose a light-weight M-LLM -based frame selection method that adaptively select frames that are more relevant to users' queries. In order to train the proposed frame selector, we introduce two supervision signals (i) Spatial signal, where single frame importance score by prompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by prompting Large Language Model (LLM) using the captions of all frame candidates. The selected frames are then digested by a frozen downstream video M-LLM for visual reasoning and question answering. Empirical results show that the proposed M-LLM video frame selector improves the performances various downstream video Large Language Model (video-LLM) across medium (ActivityNet, NExT-QA) and long (EgoSchema, LongVideoBench) context video question answering benchmarks.", 'abstract_zh': '近年来，多模态大型语言模型（M-LLMs）在视频推理方面展现了令人鼓舞的结果。流行的多模态大型语言模型（M-LLM）框架通常采用简单的均匀采样方法来减少输入M-LLM的视频帧数量，尤其是在处理长上下文视频时。然而，这种方法可能会在视频的某些时期丢失关键的上下文信息，导致下游M-LLM可能缺乏足够的视觉信息来回答问题。为了解决这一问题，我们提出了一种轻量级的M-LLM基于的帧选择方法，该方法能够适应性地选择与用户查询更加相关的关键帧。为了训练提出的帧选择器，我们引入了两种监督信号：（i）空间信号，通过提示M-LLM单帧的重要性分数；（ii）时间信号，在此信号中，通过使用所有候选帧的字幕来提示大型语言模型（LLM）进行多帧选择。选出的关键帧随后由冻结状态的下游视频M-LLM用于视觉推理和问题回答。实验结果表明，提出的M-LLM视频帧选择器能够提高各类中长上下文视频大型语言模型（视频-LLM）在（ActivityNet，NExT-QA）和（EgoSchema，LongVideoBench）基准测试中的性能。', 'title_zh': '基于M-LLM的视频帧选择方法以实现高效视频理解'}
{'arxiv_id': 'arXiv:2502.19622', 'title': "Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's Mathematical Reasoning", 'authors': 'Yanan Chen, Ali Pesaranghader, Tanmana Sadhu', 'link': 'https://arxiv.org/abs/2502.19622', 'abstract': "Recent advances in Large Language Models (LLMs) have raised interest in their formal reasoning capabilities, particularly in mathematics. While closed LLMs like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains unclear whether small to medium-sized open LLMs can achieve similar performance, questioning their reliability. To close this gap, we propose a post-training approach leveraging a mixture of opinions (MoO) from weaker ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that, each post-training sample is augmented with Chain-of-Thought (CoT) reasoning steps and answers from ancillary LLMs, enabling the main LLM to learn from diverse perspectives. We compare MoO with standard supervised fine-tuning (SFT), few-shot prompting, and the Mixture of Agents (MoA) method on mathematical reasoning benchmarks. Our results show that incorporating weaker LLMs' opinions improves mathematical reasoning by an average of 5%, highlighting the value of diverse perspectives in reasoning tasks.", 'abstract_zh': '近年来，大型语言模型（LLMs）的发展引发了对其形式推理能力的兴趣，特别是在数学领域的表现。虽然像GPT-4这样的闭源LLM在数学基准测试中表现出色，例如GSM8K，但尚不清楚中小型的开源LLM能否达到类似的成绩，从而质疑它们的可靠性。为了解决这一问题，我们提出了一种后训练方法，利用较弱的辅助LLM提供的多种意见（MoO）来增强相对较强的LLM的推理能力。在这一方法中，每个后训练样本都会增强链式推理（CoT）步骤和辅助LLM的回答，从而使主要LLM能够从多种视角中学习。我们使用数学推理基准测试将MoO与标准的监督微调（SFT）、少样本提示（Few-shot prompting）和代理混合（MoA）方法进行了比较。结果显示，纳入较弱LLM的意见在数学推理任务中平均提高了5%的性能，突出了多样视角在推理任务中的价值。', 'title_zh': '弱语言模型的意见同样重要：混合意见提升语言模型的数学推理能力'}
{'arxiv_id': 'arXiv:2502.19573', 'title': 'Do Large Language Models Know How Much They Know?', 'authors': 'Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani, Sarath Chandar', 'link': 'https://arxiv.org/abs/2502.19573', 'abstract': "Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.", 'abstract_zh': '大规模语言模型（LLMs）已经发展成为高度有效的系统，并且越来越多地被集成到各种应用中。然而，它们部署的快速步伐已经超过了对其内部机制进行全面理解的程度，以及明确了其能力和限制。智能系统的一个理想属性是能够识别自己知识的范围。为了调查LLMs是否具备这一特性，我们开发了一个基准测试，旨在挑战这些模型在特定主题上列出它们掌握的所有信息。这个基准测试评估模型是回忆过多、不足还是恰当的信息量，从而表明它们对自己知识的认知程度。我们的研究表明，在有足够的规模下，所有测试的LLMs都对特定主题的知识范围有所理解。虽然不同的模型架构显示出这一能力出现的速率各异，但结果表明，知识意识可能是LLMs的一个可泛化的属性。需要进一步的研究来验证这一潜力并完全阐明其背后的工作机制。', 'title_zh': '大型语言模型知道它们知道多少吗？'}
{'arxiv_id': 'arXiv:2502.19518', 'title': 'Accessing LLMs for Front-end Software Architecture Knowledge', 'authors': 'L. P. Franciscatto Guerra, N. Ernst', 'link': 'https://arxiv.org/abs/2502.19518', 'abstract': "Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools.", 'abstract_zh': '大型语言模型（LLMs）在自动化软件开发任务方面展现出了显著的潜力，但它们在软件设计任务方面的能力仍然不清楚。本研究调查了一个LLM在理解、再现和生成 iOS 应用程序复杂 VIPER 架构中的结构的能力。我们利用布卢姆分类法开发了一个全面的评估框架，以评估LLM在不同认知领域（包括记忆、理解、应用、分析、评价和创作）的表现。使用ChatGPT 4 Turbo 2024-04-09进行的实验结果表明，LLM在高级任务如评价和创作中表现出色，但在需要精确检索架构细节的低级任务中面临挑战。这些发现凸显了LLMs在降低开发成本方面的潜力以及在实际软件设计场景中有效应用所面临的障碍。本研究提出了一种评估LLM在软件架构方面能力的基准格式，旨在为更稳健和易获取的AI驱动开发工具作出贡献。', 'title_zh': '面向前端软件架构知识访问大规模语言模型'}
{'arxiv_id': 'arXiv:2502.19463', 'title': 'Do LLMs exhibit demographic parity in responses to queries about Human Rights?', 'authors': 'Rafiya Javed, Jackie Kay, David Yanni, Abdullah Zaini, Anushe Sheikh, Maribeth Rauh, Iason Gabriel, Laura Weidinger', 'link': 'https://arxiv.org/abs/2502.19463', 'abstract': 'This research describes a novel approach to evaluating hedging behaviour in large language models (LLMs), specifically in the context of human rights as defined in the Universal Declaration of Human Rights (UDHR). Hedging and non-affirmation are behaviours that express ambiguity or a lack of clear endorsement on specific statements. These behaviours are undesirable in certain contexts, such as queries about whether different groups are entitled to specific human rights; since all people are entitled to human rights. Here, we present the first systematic attempt to measure these behaviours in the context of human rights, with a particular focus on between-group comparisons. To this end, we design a novel prompt set on human rights in the context of different national or social identities. We develop metrics to capture hedging and non-affirmation behaviours and then measure whether LLMs exhibit demographic parity when responding to the queries. We present results on three leading LLMs and find that all models exhibit some demographic disparities in how they attribute human rights between different identity groups. Futhermore, there is high correlation between different models in terms of how disparity is distributed amongst identities, with identities that have high disparity in one model also facing high disparity in both the other models. While baseline rates of hedging and non-affirmation differ, these disparities are consistent across queries that vary in ambiguity and they are robust across variations of the precise query wording. Our findings highlight the need for work to explicitly align LLMs to human rights principles, and to ensure that LLMs endorse the human rights of all groups equally.', 'abstract_zh': '本研究提出了一种评估大型语言模型（LLMs）在人权语境下（根据《世界人权宣言》定义的人权）对冲行为的新方法。对冲和非肯定行为表示对具体陈述的模糊或缺乏明确背书。在某些情境下，如有关不同群体是否有权享有特定人权的查询中，这种行为是不理想的，因为所有人均应享有基本人权。为此，我们首次系统地尝试在人权语境下衡量这些行为，特别关注不同群体之间的比较。为了实现这一目标，我们设计了一套专门针对不同国家或社会身份的人权新提示。我们开发了衡量对冲和非肯定行为的指标，然后测量这些指标在各个人工智能模型中的表现，以评估它们在回应查询时是否体现了群体公正。我们对三个主要LILM模型进行了评估，并发现所有模型在赋予不同身份群体人权方面都存在一定程度的群体差异。此外，各个模型在不同身份群体间的差异分布方面存在高度相关性，即在某一模型中存在显著差异的身份群体，在其他模型中也存在显著差异。虽然不同模型的基本对冲和非肯定率有所不同，但这些差异在不同模糊程度的查询中表现出一致性，并且在查询措辞变化时仍然稳定。我们的研究结果强调了明确让大型语言模型符合人权原则的必要性，确保所有群体在享有基本人权方面得到平等的背书。', 'title_zh': '大型语言模型在回答人权相关查询时whether LLMs 在回答人权相关查询时是否存在人口统计学公平性？'}
{'arxiv_id': 'arXiv:2502.19422', 'title': 'Implementation of a Generative AI Assistant in K-12 Education: The CGScholar AI Helper Initiative', 'authors': 'Vania Castro, Ana Karina de Oliveira Nascimento, Raigul Zheldibayeva, Duane Searsmith, Akash Saini, Bill Cope, Mary Kalantzis', 'link': 'https://arxiv.org/abs/2502.19422', 'abstract': 'This paper focuses on the piloting of the CGScholar AI Helper, a Generative AI (GenAI) assistant tool that aims to provide feedback on writing in high school contexts. The aim was to use GenAI to provide formative and summative feedback on students\' texts in English Language Arts (ELA) and History. The trials discussed in this paper relate to Grade 11, a crucial learning phase when students are working towards college readiness. These trials took place in two very different schools in the Midwest of the United States, one in a low socio-economic background with low-performance outcomes and the other in a high socio-economic background with high-performance outcomes. The assistant tool used two main mechanisms "prompt engineering" based on participant teachers\' assessment rubric and "fine-tuning" a Large Language Model (LLM) from a customized corpus of teaching materials using Retrieval Augmented Generation (RAG). This paper focuses on the CGScholar AI Helper\'s potential to enhance students\' writing abilities and support teachers in ELA and other subject areas requiring written assignments.', 'abstract_zh': '本文聚焦于CGScholar AI辅助工具的试点应用，该工具是一款生成式人工智能（GenAI）助手，旨在为高中情境下的写作提供反馈。本研究旨在利用GenAI为学生的英语语言艺术（ELA）和历史课程文本提供形成性和总结性反馈。本文所述的实验主要针对第11年级的学生，这是他们为大学做好准备的关键学习阶段。这些实验在美国中西部的两所截然不同的学校进行，一所学校的经济背景较低且成绩较差，另一所学校的经济背景较高且成绩较好。辅助工具采用了两种主要机制：“提示工程”，基于参与者教师的评价标准，并且“微调”了一个通过检索增强生成（RAG）定制教学材料语料库训练的大语言模型（LLM）。本文重点探讨CGScholar AI辅助工具提高学生写作能力以及支持ELA和其他需要写作作业的学科教师的潜力。', 'title_zh': 'K-12教育中生成式AI助教的实施：CGScholar AI助教计划'}
{'arxiv_id': 'arXiv:2502.20350', 'title': 'KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model', 'authors': 'Kai Zhang, Rui Zhu, Shutian Ma, Jingwei Xiong, Yejin Kim, Fabricio Murai, Xiaozhong Liu', 'link': 'https://arxiv.org/abs/2502.20350', 'abstract': 'Drug discovery is a critical task in biomedical natural language processing (NLP), yet explainable drug discovery remains underexplored. Meanwhile, large language models (LLMs) have shown remarkable abilities in natural language understanding and generation. Leveraging LLMs for explainable drug discovery has the potential to improve downstream tasks and real-world applications. In this study, we utilize open-source drug knowledge graphs, clinical trial data, and PubMed publications to construct a comprehensive dataset for the explainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we introduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. To encourage further research in this area, we will publicly release\\footnote{A copy is attached with this submission} both the dataset and KEDRec-LM.', 'abstract_zh': '药物发现是生物医学自然语言处理（NLP）中的一个关键任务，但可解释的药物发现尚未得到充分探索。与此同时，大型语言模型（LLMs）在自然语言理解与生成方面表现出了显著的能力。利用LLMs进行可解释的药物发现有望提高下游任务和实际应用的效果。在本研究中，我们利用开源药物知识图谱、临床试验数据和PubMed出版物构建了一个全面的数据集，旨在解决可解释的药物发现任务，命名为\\textbf{expRxRec}。此外，我们引入了\\textbf{KEDRec-LM}，这是一种指令调优的LLM，可以从丰富的医学知识库中提炼知识，用于药物推荐和理由生成。为了促进该领域的进一步研究，我们将公开发布\\footnote{附上提交的副本}该数据集和KEDRec-LM。', 'title_zh': 'KEDRec-LM：一种知识蒸馏可解释的药物推荐大语言模型'}
{'arxiv_id': 'arXiv:2502.20344', 'title': 'Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models', 'authors': 'Yi Jing, Zijun Yao, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, Juanzi Li', 'link': 'https://arxiv.org/abs/2502.20344', 'abstract': 'Large language models (LLMs) excel in tasks that require complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Previous work on linguistic mechanisms has been limited by coarse granularity, insufficient causal analysis, and a narrow focus. In this study, we present a systematic and comprehensive causal investigation using sparse auto-encoders (SAEs). We extract a wide range of linguistic features from six dimensions: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We extract, evaluate, and intervene on these features by constructing minimal contrast datasets and counterfactual sentence datasets. We introduce two indices-Feature Representation Confidence (FRC) and Feature Intervention Confidence (FIC)-to measure the ability of linguistic features to capture and control linguistic phenomena. Our results reveal inherent representations of linguistic knowledge in LLMs and demonstrate the potential for controlling model outputs. This work provides strong evidence that LLMs possess genuine linguistic knowledge and lays the foundation for more interpretable and controllable language modeling in future research.', 'abstract_zh': '大型语言模型（LLMs）在需要复杂语言能力的任务中表现出色，例如参考消歧和隐喻识别/生成。尽管LLMs具备令人印象深刻的技能，但其在处理和表示语言知识的内部机制仍然很大程度上是不透明的。关于语言机制的研究受到粒度粗糙、因果分析不足以及研究视野狭窄的限制。本研究中，我们采用稀疏自编码器（SAEs）进行了系统而全面的因果分析。我们从六个维度（音韵学、音系学、形态学、句法、语义学和语用学）中提取了广泛的语言特征。我们通过构建极小对比数据集和反事实句子数据集，提取、评估并干预这些特征。我们引入了两个指数——特征表示置信度（FRC）和特征干预置信度（FIC），以测量语言特征捕捉和控制语言现象的能力。我们的结果揭示了LLMs内部固有的语言知识表示，并表明了控制模型输出的潜在可能性。本研究提供了强有力的证据，证明LLMs确实具备真正的语言知识，并为未来研究提供了更加可解释和可控的语言建模的基础。', 'title_zh': '稀疏自编码器解释大规模语言模型中的语言特征'}
{'arxiv_id': 'arXiv:2502.20330', 'title': 'Long-Context Inference with Retrieval-Augmented Speculative Decoding', 'authors': 'Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh', 'link': 'https://arxiv.org/abs/2502.20330', 'abstract': 'The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference, particularly in managing key-value (KV) caches, presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer dynamic that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both approaches, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups. Our analyses reveal that RAPID achieves robust acceleration beyond 32K context length and demonstrates superior generation quality in real-world applications.', 'abstract_zh': '长上下文大型语言模型（LLM）的出现为处理大量文档提供了一个有希望的替代传统检索增强生成（RAG）的选择。然而，长上下文推断中的计算开销，特别是在管理键值（KV）缓存时，带来了显著的效率挑战。虽然推测解码（Speculative Decoding，SD）传统上通过较小的草稿模型加速推断，但在长上下文场景中，由于受限于内存的KV缓存操作，其效果大幅减弱。我们提出了一种检索增强推测解码（Retrieval-Augmented Speculative Decoding，RAPID），利用RAG来加速和提升长上下文生成的质量。RAPID引入了RAG草稿模型，这是一种在缩短检索上下文中运行的草稿LLM，用于推测长上下文目标LLM的生成过程。我们的方法允许在同一规模或更大规模的LLM担任RAG草稿模型的同时，保持计算效率。为了充分利用更强的RAG草稿模型的潜在优势，我们开发了一种推断时的知识转移动态，通过RAG丰富目标分布。在LLaMA-3.1和Qwen2.5基座上的大量实验表明，RAPID有效地结合了两种方法的优势，实现了显著的性能提升（例如，对于LLaMA-3.1-8B，在InfiniteBench上的提升从39.33到42.83，且速度快了2倍以上）。我们的分析表明，RAPID在超过32K上下文长度时实现了稳健加速，并在实际应用中展现了更好的生成质量。', 'title_zh': '长上下文推理与检索增强推测性解码'}
{'arxiv_id': 'arXiv:2502.20238', 'title': "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", 'authors': 'Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong', 'link': 'https://arxiv.org/abs/2502.20238', 'abstract': 'Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model\'s intermediate reasoning steps unexamined. This fails to assess the model\'s ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs\' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.', 'abstract_zh': '许多具有挑战性的推理任务不仅需要快速、直观的反应，还需要更审慎的多步方法。近年来，大型语言模型（LLMs）的进步显示了一种从“系统1”快速反应方式向“系统2”反思与修正型问题解决方式的重要转变。然而，当前的基准测试主要依赖于最终答案的准确性，而忽视了模型在推理过程中的中间推理步骤，这未能评估模型在其推理过程中反刍和修正错误的能力。为了弥补这一差距，我们提出了FINEREASON，这是一种逻辑谜题基准，用于细粒度评估LLMs的推理能力。每个谜题可以分解为原子步骤，使其成为严格验证中间正确性理想的工具。在此基础上，我们引入了两个任务：状态检查和状态转换，以全面评估模型如何评估当前情况并计划下一步行动。为了支持更广泛的科研工作，我们还提供了一个谜题训练集，旨在提高模型在通用数学任务上的表现。我们展示，在我们的状态检查和转换数据上进行训练的模型在GSM8K数据集上的数学推理表现提高了5.1%。', 'title_zh': 'FINEREASON：通过反思性谜题解决评估和提升LLMs的刻意推理能力'}
{'arxiv_id': 'arXiv:2502.20082', 'title': 'LongRoPE2: Near-Lossless LLM Context Window Scaling', 'authors': 'Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, Mao Yang', 'link': 'https://arxiv.org/abs/2502.20082', 'abstract': 'LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta\'s approach, which fails to reach the target effective context length. Code will be available at this https URL.', 'abstract_zh': 'LongRoPE2 是一种新型方法，旨在延长预训练大规模语言模型（LLMs）的有效上下文窗口至目标长度，同时在原始较短的上下文窗口上保持性能。这一目标通过以下三项贡献实现：（1）假设在较高 RoPE 维度中训练不足导致了现有方法中持续存在的分布外（OOD）问题；（2）一种有效的 RoPE 矢量调整算法，采用由“针驱式”困惑度指导的进化搜索来解决训练不足的问题；（3）一种混合上下文窗口训练方法，该方法微调模型权重以采用调整后的 RoPE 来适应长上下文序列，同时在保留短上下文性能的同时使用原始 RoPE。在 LLaMA3-8B 和 Phi3-mini-3.8B 上的各类基准实验中，验证了这一假设并展示了 LongRoPE2 的有效性。值得注意的是，LongRoPE2 仅使用 10B 个标记（比 Meta 的方法少 80 倍），就能将 LLaMA3-8B 的有效上下文长度扩展至 128K，同时保持超过 98.5% 的短上下文性能。有关代码将在此处提供。', 'title_zh': 'LongRoPE2：几乎无损失的大语言模型上下文窗口缩放'}
{'arxiv_id': 'arXiv:2502.19981', 'title': 'The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs', 'authors': 'Tanja Baeumel, Josef van Genabith, Simon Ostermann', 'link': 'https://arxiv.org/abs/2502.19981', 'abstract': "Autoregressive large language models (LLMs) exhibit impressive performance across various tasks but struggle with simple arithmetic, such as addition of two or more operands. We show that this struggle arises from LLMs' use of a simple one-digit lookahead heuristic, which works fairly well (but not perfect) for two-operand addition but fails in multi-operand cases, where the carry-over logic is more complex. Our probing experiments and digit-wise accuracy evaluation show that LLMs fail precisely where a one-digit lookahead is insufficient to account for cascading carries. We analyze the impact of tokenization strategies on arithmetic performance and show that all investigated models, regardless of tokenization, are inherently limited in the addition of multiple operands due to their reliance on a one-digit lookahead heuristic. Our findings reveal fundamental limitations that prevent LLMs from generalizing to more complex numerical reasoning.", 'abstract_zh': '自回归大型语言模型（LLMs）在各种任务中表现出色，但在简单的算术运算，如两个或多个操作数相加时，却表现出挑战。我们表明，这种挑战源于LLMs使用的一种简单的单位位前瞻启发式方法，这种启发式方法对于两个操作数的加法工作得相当好（但并不完美），但在多操作数情况下失效，因为这些情况下进位逻辑更为复杂。我们的探查实验和按位准确度评估表明，LLMs在那种单位位前瞻不足以解释连续进位的地方出错。我们分析了分词策略对算术性能的影响，并表明无论采用哪种分词策略，所有研究的模型都因依赖单位位前瞻启发式方法而在处理多个操作数的加法时固有地受到限制。我们的发现揭示了基本的局限性，这些局限性阻碍了LLMs对更复杂的数值推理进行泛化的能力。', 'title_zh': '前瞻限制：为什么多操作数加法对大模型具有挑战性'}
{'arxiv_id': 'arXiv:2502.19953', 'title': 'GeoEdit: Geometric Knowledge Editing for Large Language Models', 'authors': 'Yujie Feng, Liming Zhan, Zexin Lu, Yongxin Xu, Xu Chu, Yasha Wang, Jiannong Cao, Philip S. Yu, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2502.19953', 'abstract': 'Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). Consequently, various model editing methods have been developed to update specific knowledge within LLMs. However, training-based approaches often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model\'s generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a "forget-then-learn" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods.', 'abstract_zh': '定期更新对于维护大型语言模型（LLMs）的最新知识至关重要。因此，已经开发出了各种模型编辑方法来更新LLMs中的特定知识。然而，基于训练的方法往往难以有效地整合新知识而不破坏相关的一般知识。为了解决这一挑战，我们提出了一种名为几何知识编辑（GeoEdit）的新型框架。GeoEdit 利用微调中参数更新的几何关系来区分与新知识更新相关的神经元和与一般知识扰动相关的神经元。通过采用方向感知的知识识别方法，我们避免更新方向与现有知识方向大约正交的神经元，从而保持模型的泛化能力。对于其余的神经元，我们整合旧知识和新知识的方向，并对相反方向应用“先忘记后学习”的编辑策略。此外，我们引入了一种基于重要性指导的任务向量融合技术，该技术过滤掉冗余信息并提供适应性的神经元级别权重，进一步提高模型编辑性能。在两个公开的数据集上的广泛实验表明，GeoEdit 在现有最先进的方法中表现更优。', 'title_zh': 'GeoEdit: 用于大规模语言模型的几何知识编辑'}
{'arxiv_id': 'arXiv:2502.19756', 'title': 'PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation', 'authors': 'Nathan Roll', 'link': 'https://arxiv.org/abs/2502.19756', 'abstract': "Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.", 'abstract_zh': '大型语言模型（LLMs）在英语基准测试中展现出了越来越出色的性能，但在多语言设置中的表现仍存在不一致性。为解决这一问题，我们引入了PolyPrompt，这是一种新颖的、参数高效的框架，用于增强LLMs的多语言能力。我们的方法通过梯度搜索学习每种语言的一组触发令牌，从而识别输入查询的语言，并在推理时选择相应的触发令牌添加到提示中。我们在两个参数量约为10亿的模型上进行了实验，并在包含十五种在类型和资源上都具有多样性的语言的全球MMLU基准测试中进行了评估。与直接翻译基线和翻译管道基线相比，我们的方法在准确率上取得了3.7%-19.9%的提升。', 'title_zh': 'PolyPrompt：利用动态提示生成从多语言语言模型中自动提取知识'}
{'arxiv_id': 'arXiv:2502.19735', 'title': 'R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning', 'authors': 'Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie', 'link': 'https://arxiv.org/abs/2502.19735', 'abstract': 'Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.', 'abstract_zh': '尽管近年来在增强推理的大语言模型（LLMs）如DeepSeek-R1方面取得了突破，但在机器翻译（MT）中加入推理时的推理，即人类翻译者自然使用的结构化、多层次的推理链式思考（CoTs），这一领域尚未得到充分探索。现有的方法要么为特定MT子任务（例如文学翻译）设计固定不变的CoT，要么依赖于与人类推理不一致的合成CoT并采用容易导致灾难性遗忘的监督微调（SFT），这限制了它们对不同翻译场景的适应性。本文引入了一种名为R1-Translator (R1-T1)的新框架，通过强化学习（RL）实现通用MT中的推理时间推理，该框架结合了六个与人工对齐的CoT模式。我们的方法在以下三个方面进行了创新：（1）将基于推理的翻译从特定MT子任务扩展到六种语言和各种任务（例如法律/医疗领域的适应、成语解析）；（2）制定了六个由专家精心编写的CoT模板，这些模板反映了混合人类策略（如上下文感知重写和反向翻译）的特点；（3）通过带有KL约束的奖励实现自演化CoT发现和抗遗忘适应。实验结果表明，在Flores-101测试集上的21种语言和80种翻译方向中显示出稳健的翻译性能改进，尤其是在训练中未见过的15种语言中表现尤为突出，并且相比仅有的SFT，其多语言能力保持了一定的通用性。', 'title_zh': 'R1-T1: 通过推理学习全面激励大型语言模型的翻译能力'}
{'arxiv_id': 'arXiv:2502.19721', 'title': 'Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs', 'authors': 'Hannah Cyberey, Yangfeng Ji, David Evans', 'link': 'https://arxiv.org/abs/2502.19721', 'abstract': 'Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model\'s representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.', 'abstract_zh': '大型语言模型（LLMs）已知会延续刻板印象并表现出偏见。为减轻这些偏见可能带来的潜在危害，已经提出了多种策略，但大多数研究将LLMs中的偏见视为黑盒问题，未考虑模型内部概念的表示方式。我们借鉴表示工程中的技术，研究LLMs中“性别”概念的表示方式。我们提出了一种新方法，通过概率加权提取概念表示，无需标注数据，并高效地选择一个引导向量以度量和操控模型表示。此外，我们还介绍了一种基于投影的方法，使模型预测的操控更加精确，并展示了其在减轻LLMs中性别偏见方面的有效性。', 'title_zh': '感知和引导刻板印象：提取和应用性别表示向量在大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.19612', 'title': 'Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization', 'authors': 'Anwar Hossain Zahid, Monoshi Kumar Roy, Swarna Das', 'link': 'https://arxiv.org/abs/2502.19612', 'abstract': 'The proliferation of hate speech on social media is one of the serious issues that is bringing huge impacts to society: an escalation of violence, discrimination, and social fragmentation. The problem of detecting hate speech is intrinsically multifaceted due to cultural, linguistic, and contextual complexities and adversarial manipulations. In this study, we systematically investigate the performance of LLMs on detecting hate speech across multilingual datasets and diverse geographic contexts. Our work presents a new evaluation framework in three dimensions: binary classification of hate speech, geography-aware contextual detection, and robustness to adversarially generated text. Using a dataset of 1,000 comments from five diverse regions, we evaluate three state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder (6.7b). Codellama had the best binary classification recall with 70.6% and an F1-score of 52.18%, whereas DeepSeekCoder had the best performance in geographic sensitivity, correctly detecting 63 out of 265 locations. The tests for adversarial robustness also showed significant weaknesses; Llama2 misclassified 62.5% of manipulated samples. These results bring to light the trade-offs between accuracy, contextual understanding, and robustness in the current versions of LLMs. This work has thus set the stage for developing contextually aware, multilingual hate speech detection systems by underlining key strengths and limitations, therefore offering actionable insights for future research and real-world applications.', 'abstract_zh': '社交媒体上仇恨言论的泛滥是社会面临的一个严重问题，这对社会产生了巨大影响：加剧暴力、歧视和社会分裂。由于文化、语言和上下文的复杂性以及对抗性操控，识别仇恨言论的问题根源本身就是多维度的。本研究系统地探讨了LLM在多语言数据集和多样化地理背景下检测仇恨言论的表现。我们的工作提出了一种新的评估框架，包括仇恨言论的二分类、地理感知上下文检测以及对抗生成文本的鲁棒性。\n\n使用来自五个不同地区的1,000条评论的数据集，我们评估了三种最新的LLM：Llama2（13b）、Codellama（7b）和DeepSeekCoder（6.7b）。Codellama在二分类召回率方面表现最佳，达到了70.6%，F1分数为52.18%；而DeepSeekCoder在地理敏感性检测方面的表现最佳，正确检测了265个位置中的63个。针对对抗鲁棒性的测试也表明存在显著的弱点；Llama2误分类了62.5%的被操纵样本。这些结果揭示了当前LLM版本之间准确度、上下文理解与鲁棒性的权衡。因此，这项工作为进一步开发具备上下文感知能力的多语言仇恨言论检测系统奠定了基础，指出了关键的优势与局限，为未来的研究和实际应用提供了可操作的见解。', 'title_zh': '使用大规模语言模型和地理语境化评价仇恨言论检测 effetiveness'}
{'arxiv_id': 'arXiv:2502.19582', 'title': 'Where Are We? Evaluating LLM Performance on African Languages', 'authors': 'Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed', 'link': 'https://arxiv.org/abs/2502.19582', 'abstract': "Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.", 'abstract_zh': '非洲丰富的语言遗产在自然语言处理（NLP）领域中仍然广受忽视，这主要是由于历史上倾向于外国语言的政策所导致，从而造成了数据上的显著不平等。本文将非洲的语言景观理论洞察与对Sahara基准的实证评估相结合，Sahara基准是从大规模、公开可访问的数据集中精心收集起来，涵盖了非洲语言的多样性。通过系统地评估领先的大规模语言模型（LLMs）在Sahara上的表现，我们展示了政策驱动的数据差异如何直接影响非洲语言模型的有效性。我们的研究发现，虽然少数语言表现尚可，但许多土著语言由于数据稀少而仍然处于边缘化地位。基于这些见解，我们提出了针对政策改革和包容性数据实践的具体建议。总体而言，我们的工作强调了迫切需要采取一种双重方法——将理论理解与实证评估相结合——以促进非洲社区的语境多样性在人工智能中的发展。', 'title_zh': '我们在何处？评估大语言模型在非洲语言上的性能'}
{'arxiv_id': 'arXiv:2502.19548', 'title': 'When Large Language Models Meet Speech: A Survey on Integration Approaches', 'authors': 'Zhengdong Yang, Shuichiro Shimizu, Yahan Yu, Chenhui Chu', 'link': 'https://arxiv.org/abs/2502.19548', 'abstract': 'Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for', 'abstract_zh': '近年来，大规模语言模型（LLMs）的发展激发了将其应用扩展到文字任务之外的兴趣。大量研究探索了将其他模态与LLMs结合，特别是在语音模态方面，因为语音与文字之间有着天然的联系。本文概述了将语音与LLMs结合的研究，根据不同方法论将其分类为三大类：基于文本的方法、基于潜在表示的方法以及基于音频令牌的方法。我们还展示了这些方法在各种语音相关应用中的应用，并指出了该领域面临的挑战，旨在为相关研究提供灵感。', 'title_zh': '当大型语言模型遇上语音：集成方法综述'}
{'arxiv_id': 'arXiv:2502.20170', 'title': 'Re-evaluating Open-ended Evaluation of Large Language Models', 'authors': 'Siqi Liu, Ian Gemp, Luke Marris, Georgios Piliouras, Nicolas Heess, Marc Lanctot', 'link': 'https://arxiv.org/abs/2502.20170', 'abstract': 'Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.', 'abstract_zh': '传统的评估主要集中在对特定技能的候选者进行排名。现代万能模型，如大规模语言模型（LLMs），在这一范式上显然领先很多。开放式评估系统，其中候选模型通过用户提交的提示进行比较，已经成为一种流行的方法。尽管这种系统有许多优点，但我们发现目前基于Elo的评分系统可能会受到数据中的偏见，甚至会增强这种偏见，无论是有意还是无意的，因为这些系统对冗余性特别敏感。为了应对这一问题，我们提议将评估作为一种三玩家游戏，并引入新的博弈论解决方案概念，以确保对冗余性的鲁棒性。我们展示了该方法能够产生直观的评分，并提供了LLM开发竞争格局的见解。', 'title_zh': '重新评估大型语言模型的开放评价'}
{'arxiv_id': 'arXiv:2502.20140', 'title': 'Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based Telephone Survey System at Scale', 'authors': 'Max M. Lang, Sol Eskenazi', 'link': 'https://arxiv.org/abs/2502.20140', 'abstract': "Telephone surveys remain a valuable tool for gathering insights but typically require substantial resources in training and coordinating human interviewers. This work presents an AI-driven telephone survey system integrating text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT) that mimics the versatility of human-led interviews on scale.\nWe tested the system across two populations, a pilot study in the United States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting participants via web-based links and contacting them via direct phone calls. The AI agent successfully administered open-ended and closed-ended questions, handled basic clarifications, and dynamically navigated branching logic, allowing fast large-scale survey deployment without interviewer recruitment or training.\nOur findings demonstrate that while the AI system's probing for qualitative depth was more limited than human interviewers, overall data quality approached human-led standards for structured items. This study represents one of the first successful large-scale deployments of an LLM-based telephone interviewer in a real-world survey context. The AI-powered telephone survey system has the potential for expanding scalable, consistent data collecting across market research, social science, and public opinion studies, thus improving operational efficiency while maintaining appropriate data quality for research.", 'abstract_zh': '电话调查仍然是获得见解的有效工具，但通常需要大量资源来培训和协调调查员。本研究介绍了结合文本转语音（TTS）、大规模语言模型（LLM）和语音转文本（STT）的AI驱动电话调查系统，该系统可大规模模拟人类主导访谈的多样性。\n\n我们对该系统进行了两项测试：一项在美国的试点研究（n = 75），一项在秘鲁的大规模部署（n = 2,739）。我们通过网络链接邀请参与者，并通过直接电话联系他们。AI代理成功地执行了开放式和封闭式问题的管理，处理了基本的澄清，并动态导航了分支逻辑，从而实现了快速的大规模调查部署，无需招募或培训调查员。\n\n我们的研究结果表明，虽然AI系统在获取质性深度方面的探究能力有限，但总体数据质量接近人类主导访谈的标准。本研究代表了第一个成功的基于LLM的电话访谈在实际调查情境中的大规模部署。AI驱动的电话调查系统有望在市场研究、社会科学和公共舆论调查等多个领域扩展可扩展、一致的数据收集，从而提高操作效率，同时保持适当的数据质量以满足研究需求。', 'title_zh': '电话调查遇见对话式AI：评估基于大语言模型的电话调查系统'}
