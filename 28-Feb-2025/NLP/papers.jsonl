{'arxiv_id': 'arXiv:2502.20364', 'title': 'Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization', 'authors': 'Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov', 'link': 'https://arxiv.org/abs/2502.20364', 'abstract': 'Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.', 'abstract_zh': '基于大型语言模型（LLMs）的代理生成人工智能，借助检索增强生成（RAG）、知识图谱（KGs）和向量存储（VSs），代表了一种变革性的技术，适用于诸如法律系统、研究、推荐系统、网络安全以及大规模安全，包括扩散研究等专门领域。该技术擅长推断大量非结构化或半结构化数据集中的关系。这里的法律领域包括复杂的数据，这些数据具有广泛的、相互关联的和半结构化的知识系统，具有复杂的关系。它包括宪法、法律法规、规章制度和判例法。从复杂的法律文件和其关系中提取洞察力并导航它们的网络对于有效的法律研究至关重要。为了解决这个问题，我们介绍了一种集成了RAG、VS和KG的生成AI系统，通过非负矩阵分解（NMF）进行构建，以增强法律信息检索和AI推理并减少幻觉。在法律系统中，这些技术赋予AI代理识别和分析案件、法律条文和先例之间的复杂联系的能力，揭示隐藏的关系并预测法律趋势，这是确保公正和提高运营效率必不可少的任务。我们的系统采用网页抓取技术系统地收集法律文本，如法律法规、宪法条款和判例法，来自类似于Justia的公共访问平台。它通过利用先进的语义表示、层次关系和潜在主题发现，弥补了传统关键词搜索与上下文理解之间的差距。该框架支持法律文件聚类、摘要和跨参照，以实现半结构化数据的大规模、可解释和准确检索，从而推动计算法学和人工智能的发展。', 'title_zh': '法律知识与人工智能融合：基于向量存储、知识图谱和层次非负矩阵分解的检索增强生成'}
{'arxiv_id': 'arXiv:2502.20356', 'title': 'Bridging the Creativity Understanding Gap: Small-Scale Human Alignment Enables Expert-Level Humor Ranking in LLMs', 'authors': 'Kuan Lok Zhou, Jiayi Chen, Siddharth Suresh, Reuben Narad, Timothy T. Rogers, Lalit K Jain, Robert D Nowak, Bob Mankoff, Jifan Zhang', 'link': 'https://arxiv.org/abs/2502.20356', 'abstract': "Large Language Models (LLMs) have shown significant limitations in understanding creative content, as demonstrated by Hessel et al. (2023)'s influential work on the New Yorker Cartoon Caption Contest (NYCCC). Their study exposed a substantial gap between LLMs and humans in humor comprehension, establishing that understanding and evaluating creative content is key challenge in AI development. We revisit this challenge by decomposing humor understanding into three components and systematically improve each: enhancing visual understanding through improved annotation, utilizing LLM-generated humor reasoning and explanations, and implementing targeted alignment with human preference data. Our refined approach achieves 82.4% accuracy in caption ranking, singificantly improving upon the previous 67% benchmark and matching the performance of world-renowned human experts in this domain. Notably, while attempts to mimic subgroup preferences through various persona prompts showed minimal impact, model finetuning with crowd preferences proved remarkably effective. These findings reveal that LLM limitations in creative judgment can be effectively addressed through focused alignment to specific subgroups and individuals. Lastly, we propose the position that achieving artificial general intelligence necessitates systematic collection of human preference data across creative domains. We advocate that just as human creativity is deeply influenced by individual and cultural preferences, training LLMs with diverse human preference data may be essential for developing true creative understanding.", 'abstract_zh': '大型语言模型（LLMs）在理解创造性内容方面表现出显著的局限性，Hessel等人（2023）对《纽约客》漫画标题竞赛（NYCCC）的研究已经揭示了这一点。他们的研究展示了LLMs与人类在幽默理解上的巨大差距，确立了理解和评估创造性内容是AI发展中的一大挑战。我们重新审视这一挑战，将幽默理解分解为三个组成部分，并系统性地改进每个方面：通过改进注释增强视觉理解，利用LLM生成的幽默推理和解释，以及实施针对人类偏好数据的目标化对齐。我们改进的方法在标题排名中的准确率达到了82.4%，显著超越了之前的67%基准，甚至接近于这一领域公认的顶级人类专家的表现。值得注意的是，虽然通过各种角色提示模拟子群体偏好的尝试几乎没有效果，但使用大众偏好的模型微调却表现出极大的效果。这些发现表明，通过针对特定子群体和个体的聚焦对齐，可以有效解决LLMs在创造性判断方面的局限性。最后，我们提出观点，实现人工通用智能需要在创造性领域系统地收集人类偏好数据。我们认为，正如人类创造力深受个体与文化偏好的影响，使用多样化的人类偏好数据训练LLMs可能对于培养真正的创造性理解至关重要。', 'title_zh': '填补创造力理解差距：小规模人类对齐 enables 专家级幽默排序能力在大语言模型中的实现'}
{'arxiv_id': 'arXiv:2502.20350', 'title': 'KEDRec-LM: A Knowledge-distilled Explainable Drug Recommendation Large Language Model', 'authors': 'Kai Zhang, Rui Zhu, Shutian Ma, Jingwei Xiong, Yejin Kim, Fabricio Murai, Xiaozhong Liu', 'link': 'https://arxiv.org/abs/2502.20350', 'abstract': 'Drug discovery is a critical task in biomedical natural language processing (NLP), yet explainable drug discovery remains underexplored. Meanwhile, large language models (LLMs) have shown remarkable abilities in natural language understanding and generation. Leveraging LLMs for explainable drug discovery has the potential to improve downstream tasks and real-world applications. In this study, we utilize open-source drug knowledge graphs, clinical trial data, and PubMed publications to construct a comprehensive dataset for the explainable drug discovery task, named \\textbf{expRxRec}. Furthermore, we introduce \\textbf{KEDRec-LM}, an instruction-tuned LLM which distills knowledge from rich medical knowledge corpus for drug recommendation and rationale generation. To encourage further research in this area, we will publicly release\\footnote{A copy is attached with this submission} both the dataset and KEDRec-LM.', 'abstract_zh': '药物发现是生物医学自然语言处理（NLP）中的一个关键任务，但可解释的药物发现尚未得到充分探索。与此同时，大型语言模型（LLMs）在自然语言理解与生成方面表现出了显著的能力。利用LLMs进行可解释的药物发现有望提高下游任务和实际应用的效果。在本研究中，我们利用开源药物知识图谱、临床试验数据和PubMed出版物构建了一个全面的数据集，旨在解决可解释的药物发现任务，命名为\\textbf{expRxRec}。此外，我们引入了\\textbf{KEDRec-LM}，这是一种指令调优的LLM，可以从丰富的医学知识库中提炼知识，用于药物推荐和理由生成。为了促进该领域的进一步研究，我们将公开发布\\footnote{附上提交的副本}该数据集和KEDRec-LM。', 'title_zh': 'KEDRec-LM：一种知识蒸馏可解释的药物推荐大语言模型'}
{'arxiv_id': 'arXiv:2502.20344', 'title': 'Sparse Auto-Encoder Interprets Linguistic Features in Large Language Models', 'authors': 'Yi Jing, Zijun Yao, Lingxu Ran, Hongzhu Guo, Xiaozhi Wang, Lei Hou, Juanzi Li', 'link': 'https://arxiv.org/abs/2502.20344', 'abstract': 'Large language models (LLMs) excel in tasks that require complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Previous work on linguistic mechanisms has been limited by coarse granularity, insufficient causal analysis, and a narrow focus. In this study, we present a systematic and comprehensive causal investigation using sparse auto-encoders (SAEs). We extract a wide range of linguistic features from six dimensions: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We extract, evaluate, and intervene on these features by constructing minimal contrast datasets and counterfactual sentence datasets. We introduce two indices-Feature Representation Confidence (FRC) and Feature Intervention Confidence (FIC)-to measure the ability of linguistic features to capture and control linguistic phenomena. Our results reveal inherent representations of linguistic knowledge in LLMs and demonstrate the potential for controlling model outputs. This work provides strong evidence that LLMs possess genuine linguistic knowledge and lays the foundation for more interpretable and controllable language modeling in future research.', 'abstract_zh': '大型语言模型（LLMs）在需要复杂语言能力的任务中表现出色，例如参考消歧和隐喻识别/生成。尽管LLMs具备令人印象深刻的技能，但其在处理和表示语言知识的内部机制仍然很大程度上是不透明的。关于语言机制的研究受到粒度粗糙、因果分析不足以及研究视野狭窄的限制。本研究中，我们采用稀疏自编码器（SAEs）进行了系统而全面的因果分析。我们从六个维度（音韵学、音系学、形态学、句法、语义学和语用学）中提取了广泛的语言特征。我们通过构建极小对比数据集和反事实句子数据集，提取、评估并干预这些特征。我们引入了两个指数——特征表示置信度（FRC）和特征干预置信度（FIC），以测量语言特征捕捉和控制语言现象的能力。我们的结果揭示了LLMs内部固有的语言知识表示，并表明了控制模型输出的潜在可能性。本研究提供了强有力的证据，证明LLMs确实具备真正的语言知识，并为未来研究提供了更加可解释和可控的语言建模的基础。', 'title_zh': '稀疏自编码器解释大规模语言模型中的语言特征'}
{'arxiv_id': 'arXiv:2502.20339', 'title': 'Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners', 'authors': 'Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Zico Kolter, Albert Gu, François Fleuret, Tri Dao', 'link': 'https://arxiv.org/abs/2502.20339', 'abstract': 'Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.', 'abstract_zh': '近年来的研究表明，通过在测试时扩展计算资源，大型语言模型（LLMs）的性能可以显著提升。一种常见的策略是生成多个推理路径（Chain-of-Thought, CoT）并利用各种选择机制聚合它们的输出。这引发了一个基本问题：低复杂度的模型是否可以通过利用其优越的生成吞吐量，在固定的计算预算下超越同样规模的Transformer模型？为了回答这个问题，并克服缺乏强次线性推理器的挑战，我们从预训练的Transformer模型中提炼了纯和混合Mamba模型。仅在80亿个令牌上进行训练，我们的提炼模型在数学推理数据集上表现出色且具有良好的扩展性，同时在对大型批处理和长序列进行推理时速度更快。尽管由于提炼而产生的零样本性能损失，但纯和混合Mamba模型在固定时间预算下可以扩展其覆盖范围和准确性，从而开辟了扩展推理计算的新方向。', 'title_zh': '慢思考，快思考：通过提炼推理器扩展推理计算能力'}
{'arxiv_id': 'arXiv:2502.20335', 'title': 'Expertise Is What We Want', 'authors': 'Alan Ashworth, Munir Al-Dajani, Keegan Duchicela, Kiril Kafadarov, Allison Kurian, Othman Laraki, Amina Lazrak, Divneet Mandair, Wendy McKennon, Rebecca Miksad, Jayodita Sanghvi, Travis Zack', 'link': 'https://arxiv.org/abs/2502.20335', 'abstract': 'Clinical decision-making depends on expert reasoning, which is guided by standardized, evidence-based guidelines. However, translating these guidelines into automated clinical decision support systems risks inaccuracy and importantly, loss of nuance. We share an application architecture, the Large Language Expert (LLE), that combines the flexibility and power of Large Language Models (LLMs) with the interpretability, explainability, and reliability of Expert Systems. LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability.\nTo highlight the power of the Large Language Expert (LLE) system, we built an LLE to assist with the workup of patients newly diagnosed with cancer. Timely initiation of cancer treatment is critical for optimal patient outcomes. However, increasing complexity in diagnostic recommendations has made it difficult for primary care physicians to ensure their patients have completed the necessary workup before their first visit with an oncologist. As with many real-world clinical tasks, these workups require the analysis of unstructured health records and the application of nuanced clinical decision logic. In this study, we describe the design & evaluation of an LLE system built to rapidly identify and suggest the correct diagnostic workup. The system demonstrated a high degree of clinical-level accuracy (>95%) and effectively addressed gaps identified in real-world data from breast and colon cancer patients at a large academic center.', 'abstract_zh': '临床决策依赖于专家推理，而这种推理受到标准化和基于证据的指南的指导。然而，将这些指南转化为自动化的临床决策支持系统可能会导致不准确，并且重要的是会损失细腻性。我们介绍了一种应用架构，即大型语言专家（LLE），该架构结合了大型语言模型（LLMs）的灵活性和力量与专家系统的可解释性、可解释性和可靠性。大型语言模型有助于解决专家系统的关键挑战，如知识的整合和编码、以及数据规范化等问题。相反，一种类似于专家系统的方法有助于克服大型语言模型的挑战，如幻觉、原子级别的和低成本的更新以及可测试性。\n\n为了突出大型语言专家（LLE）系统的强大功能，我们构建了一个LLE系统以协助对新诊断为癌症的患者进行工作评估。及时启动癌症治疗对于优化患者结果至关重要。然而，诊断建议的日益复杂使得初级保健医生难以确保患者在首次访问肿瘤科医生前完成必要的工作评估。这与许多现实世界的临床任务一样，这些工作评估需要分析非结构化的健康记录并应用复杂的临床决策逻辑。在这项研究中，我们描述了设计和评估用于快速识别并建议正确诊断工作评估的LLE系统的方案。该系统在临床级别上的准确性非常高（超过95%），并成功地解决了在一家大型学术中心乳腺癌和结肠癌患者实际数据中发现的空白。', 'title_zh': '专家知识正是我们所渴望的'}
{'arxiv_id': 'arXiv:2502.20332', 'title': 'Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models', 'authors': 'Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb', 'link': 'https://arxiv.org/abs/2502.20332', 'abstract': 'Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, symbol abstraction heads convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, symbolic induction heads perform sequence induction over these abstract variables. Finally, in later layers, retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.', 'abstract_zh': '许多近期的研究发现了大型语言模型中涌现推理能力的证据，但这些能力的稳健性以及它们依赖于结构化推理机制的程度仍然存在争论。为了解决这些问题，我们对一个开源语言模型（Llama3-70B）中支持抽象规则归纳的内部机制进行了全面研究。我们发现了一种涌现的符号结构，该结构通过一系列三次计算实现了抽象推理。在早期层中，符号抽象头基于输入标记之间的关系将输入标记转换为抽象变量。在中间层中，符号归纳头在这些抽象变量上进行序列归纳。最后，在后期层中，检索头通过检索与预测的抽象变量相关的值来预测下一个标记。这些结果揭示了符号方法和神经网络方法之间长期争论的可能解决方案，表明神经网络中的涌现推理依赖于符号机制的涌现。', 'title_zh': '大型语言模型中 Emergent 符号机制支持抽象推理'}
{'arxiv_id': 'arXiv:2502.20330', 'title': 'Long-Context Inference with Retrieval-Augmented Speculative Decoding', 'authors': 'Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh', 'link': 'https://arxiv.org/abs/2502.20330', 'abstract': 'The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference, particularly in managing key-value (KV) caches, presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We present Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer dynamic that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both approaches, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups. Our analyses reveal that RAPID achieves robust acceleration beyond 32K context length and demonstrates superior generation quality in real-world applications.', 'abstract_zh': '长上下文大型语言模型（LLM）的出现为处理大量文档提供了一个有希望的替代传统检索增强生成（RAG）的选择。然而，长上下文推断中的计算开销，特别是在管理键值（KV）缓存时，带来了显著的效率挑战。虽然推测解码（Speculative Decoding，SD）传统上通过较小的草稿模型加速推断，但在长上下文场景中，由于受限于内存的KV缓存操作，其效果大幅减弱。我们提出了一种检索增强推测解码（Retrieval-Augmented Speculative Decoding，RAPID），利用RAG来加速和提升长上下文生成的质量。RAPID引入了RAG草稿模型，这是一种在缩短检索上下文中运行的草稿LLM，用于推测长上下文目标LLM的生成过程。我们的方法允许在同一规模或更大规模的LLM担任RAG草稿模型的同时，保持计算效率。为了充分利用更强的RAG草稿模型的潜在优势，我们开发了一种推断时的知识转移动态，通过RAG丰富目标分布。在LLaMA-3.1和Qwen2.5基座上的大量实验表明，RAPID有效地结合了两种方法的优势，实现了显著的性能提升（例如，对于LLaMA-3.1-8B，在InfiniteBench上的提升从39.33到42.83，且速度快了2倍以上）。我们的分析表明，RAPID在超过32K上下文长度时实现了稳健加速，并在实际应用中展现了更好的生成质量。', 'title_zh': '长上下文推理与检索增强推测性解码'}
{'arxiv_id': 'arXiv:2502.20315', 'title': 'LangProBe: a Language Programs Benchmark', 'authors': 'Shangyin Tan, Lakshya A Agrawal, Arnav Singhvi, Liheng Lai, Michael J Ryan, Dan Klein, Omar Khattab, Koushik Sen, Matei Zaharia', 'link': 'https://arxiv.org/abs/2502.20315', 'abstract': 'Composing language models (LMs) into multi-step language programs and automatically optimizing their modular prompts is now a mainstream paradigm for building AI systems, but the tradeoffs in this space have only scarcely been studied before. We introduce LangProBe, the first large-scale benchmark for evaluating the architectures and optimization strategies for language programs, with over 2000 combinations of tasks, architectures, optimizers, and choices of LMs. Using LangProBe, we are the first to study the impact of program architectures and optimizers (and their compositions together and with different models) on tradeoffs of quality and cost. We find that optimized language programs offer strong cost--quality Pareto improvement over raw calls to models, but simultaneously demonstrate that human judgment (or empirical decisions) about which compositions to pursue is still necessary for best performance. We will open source the code and evaluation data for LangProBe.', 'abstract_zh': '将下面的论文内容或标题翻译成中文，要符合学术规范：\n\n将语言模型（LMs）组成多步语言程序，并自动生成优化的模块化提示，已成为构建人工智能系统的一种主流范式，但在这个领域中的权衡关系以前只有很少的研究。我们引入了LangProBe，这是首个大规模基准测试，用于评估语言程序的架构和优化策略，包含了超过2000种任务、架构、优化器和语言模型的选择组合。利用LangProBe，我们首次研究了程序架构和优化器（以及它们的组合和与不同模型的结合）对质量和成本权衡的影响。我们的研究发现，优化后的语言程序在质量和成本之间提供了显著的帕累托改进，同时表明，有关哪些组合值得追求的人类判断（或经验决策）对于最佳性能仍然是必要的。我们将开源LangProBe的代码和评估数据。', 'title_zh': 'LangProBe：语言程序基准测试'}
{'arxiv_id': 'arXiv:2502.20273', 'title': 'How Much is Enough? The Diminishing Returns of Tokenization Training Data', 'authors': 'Varshini Reddy, Craig W. Schmidt, Yuval Pinter, Chris Tanner', 'link': 'https://arxiv.org/abs/2502.20273', 'abstract': 'Tokenization, a crucial initial step in natural language processing, is often assumed to benefit from larger training datasets. This paper investigates the impact of tokenizer training data sizes ranging from 1GB to 900GB. Our findings reveal diminishing returns as the data size increases, highlighting a practical limit on how much further scaling the training data can improve tokenization quality. We analyze this phenomenon and attribute the saturation effect to the constraints imposed by the pre-tokenization stage of tokenization. These results offer valuable insights for optimizing the tokenization process and highlight potential avenues for future research in tokenization algorithms.', 'abstract_zh': '分词是自然语言处理中的一个关键初始步骤，通常认为它能够从更大的训练数据集中受益。本文探讨了从1GB到900GB不等的训练数据量对分词的影响。我们的研究表明，随着数据量的增加，分词质量的提升逐渐减弱，揭示了训练数据进一步扩增所能带来的分词质量提升的实用极限。我们分析了这一现象，并将饱和效应归因于分词过程中预分词阶段的限制。这些结果为优化分词过程提供了宝贵的见解，并指出了未来分词算法研究的潜在方向。', 'title_zh': '多少才足够？Token化训练数据的边际收益递减效应'}
{'arxiv_id': 'arXiv:2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'authors': 'Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, Guokan Shang', 'link': 'https://arxiv.org/abs/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'abstract_zh': '随着大语言模型在在线内容生成中扮演越来越重要的角色，人们对反复处理其自身输出的影响表示了关注。受到链式人类交流中“电话串烧”效应的启发，本研究探讨了大语言模型是否也通过迭代生成过程中扭曲信息。通过基于翻译的实验，我们发现信息的扭曲会随时间积累，受语言选择和链式复杂性的影响。虽然降级是不可避免的，但可以通过战略性提示技术来减轻。这些发现为讨论AI中介信息传播的长期影响做出了贡献，并引发了关于在迭代流程中生成内容的大语言模型可靠性的关键问题。', 'title_zh': '大型语言模型作为Broken Telephone游戏：迭代生成扭曲信息'}
{'arxiv_id': 'arXiv:2502.20246', 'title': 'Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in Code Generation Datasets', 'authors': 'Chichien Tsai, Chiamu Yu, Yingdar Lin, Yusung Wu, Weibin Lee', 'link': 'https://arxiv.org/abs/2502.20246', 'abstract': 'The increasing adoption of large language models (LLMs) for code-related tasks has raised concerns about the security of their training datasets. One critical threat is dead code poisoning, where syntactically valid but functionally redundant code is injected into training data to manipulate model behavior. Such attacks can degrade the performance of neural code search systems, leading to biased or insecure code suggestions. Existing detection methods, such as token-level perplexity analysis, fail to effectively identify dead code due to the structural and contextual characteristics of programming languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a novel line-level detection and cleansing method tailored to the structural properties of code. DePA computes line-level perplexity by leveraging the contextual relationships between code lines and identifies anomalous lines by comparing their perplexity to the overall distribution within the file. Our experiments on benchmark datasets demonstrate that DePA significantly outperforms existing methods, achieving 0.14-0.19 improvement in detection F1-score and a 44-65% increase in poisoned segment localization precision. Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for large-scale dataset cleansing. Overall, by addressing the unique challenges of dead code poisoning, DePA provides a robust and efficient solution for safeguarding the integrity of code generation model training datasets.', 'abstract_zh': '随着大型语言模型（LLMs）在代码相关任务中的应用不断增加，人们对它们训练数据的安全性产生了担忧。一种关键威胁是“无用代码投毒”，即在训练数据中注入语义上有效但功能冗余的代码，以操纵模型的行为。此类攻击可能会削弱神经代码搜索系统的性能，导致偏倚或不安全的代码建议。现有的检测方法，如基于词元级困惑度分析，无法有效识别无用代码，因为编程语言具有的结构性和上下文特性。在本文中，我们提出了一种新的行级检测和清理方法DePA（无用代码困惑度分析），该方法专门针对代码的结构性质进行设计。DePA 通过利用代码行之间的上下文关系计算行级困惑度，并通过将它们的困惑度与文件内的整体分布进行比较来识别异常行。我们在基准数据集上的实验表明，DePA 显著优于现有方法，在检测F1分数上提高了0.14-0.19，在受损代码段定位精度上提高了44-65%。此外，DePA 将检测速度提高了0.62-23倍，使其适用于大规模数据集的清理。总体而言，通过解决无用代码投毒的独特挑战，DePA 提供了一个可靠高效的解决方案，以保护代码生成模型训练数据的完整性。', 'title_zh': '超越自然语言困惑度：检测代码生成数据集中的死代码污染'}
{'arxiv_id': 'arXiv:2502.20245', 'title': 'From Retrieval to Generation: Comparing Different Approaches', 'authors': 'Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, Adam Jatowt', 'link': 'https://arxiv.org/abs/2502.20245', 'abstract': 'Knowledge-intensive tasks, particularly open-domain question answering (ODQA), document reranking, and retrieval-augmented language modeling, require a balance between retrieval accuracy and generative flexibility. Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth. Generative models like GPT-4-o provide richer contextual understanding but face challenges in maintaining factual consistency. In this work, we conduct a systematic evaluation of retrieval-based, generation-based, and hybrid models, with a primary focus on their performance in ODQA and related retrieval-augmented tasks. Our results show that dense retrievers, particularly DPR, achieve strong performance in ODQA with a top-1 accuracy of 50.17\\% on NQ, while hybrid models improve nDCG@10 scores on BEIR from 43.42 (BM25) to 52.59, demonstrating their strength in document reranking. Additionally, we analyze language modeling tasks using WikiText-103, showing that retrieval-based approaches like BM25 achieve lower perplexity compared to generative and hybrid methods, highlighting their utility in retrieval-augmented generation. By providing detailed comparisons and practical insights into the conditions where each approach excels, we aim to facilitate future optimizations in retrieval, reranking, and generative models for ODQA and related knowledge-intensive applications.', 'abstract_zh': '知识密集型任务，特别是开放领域问答（ODQA）、文档重排序和检索增强的语言模型，要求在检索准确性与生成灵活性之间取得平衡。传统的检索模型，如BM25和密集段落检索（DPR），能够在大规模语料库中高效检索，但往往缺乏语义深度。生成模型如GPT-4-o能够提供更丰富的上下文理解，但在保持事实一致性方面面临挑战。在本研究中，我们对基于检索、基于生成和混合模型进行了系统性评估，重点在于这些模型在ODQA及相关检索增强任务中的性能。我们的结果显示，密集检索器，尤其是DPR，在NQ数据集上的 top-1 准确率为50.17%，显示出其在ODQA中的强大性能。此外，混合模型在BEIR上的nDCG@10评分从BM25的43.42提升到52.59，证明了它们在文档重排序中的优势。我们还使用WikiText-103分析了语言建模任务，结果显示基于检索的方法（如BM25）的困惑度较低，这突显了它们在检索增强生成中的应用价值。通过提供详细的比较和实用见解，旨在促进未来对ODQA及相关认知密集型应用中检索、重排序和生成模型的优化。', 'title_zh': '从检索到生成：比较不同的方法'}
{'arxiv_id': 'arXiv:2502.20238', 'title': "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", 'authors': 'Guizhen Chen, Weiwen Xu, Hao Zhang, Hou Pong Chan, Chaoqun Liu, Lidong Bing, Deli Zhao, Anh Tuan Luu, Yu Rong', 'link': 'https://arxiv.org/abs/2502.20238', 'abstract': 'Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model\'s intermediate reasoning steps unexamined. This fails to assess the model\'s ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs\' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.', 'abstract_zh': '许多具有挑战性的推理任务不仅需要快速、直观的反应，还需要更审慎的多步方法。近年来，大型语言模型（LLMs）的进步显示了一种从“系统1”快速反应方式向“系统2”反思与修正型问题解决方式的重要转变。然而，当前的基准测试主要依赖于最终答案的准确性，而忽视了模型在推理过程中的中间推理步骤，这未能评估模型在其推理过程中反刍和修正错误的能力。为了弥补这一差距，我们提出了FINEREASON，这是一种逻辑谜题基准，用于细粒度评估LLMs的推理能力。每个谜题可以分解为原子步骤，使其成为严格验证中间正确性理想的工具。在此基础上，我们引入了两个任务：状态检查和状态转换，以全面评估模型如何评估当前情况并计划下一步行动。为了支持更广泛的科研工作，我们还提供了一个谜题训练集，旨在提高模型在通用数学任务上的表现。我们展示，在我们的状态检查和转换数据上进行训练的模型在GSM8K数据集上的数学推理表现提高了5.1%。', 'title_zh': 'FINEREASON：通过反思性谜题解决评估和提升LLMs的刻意推理能力'}
{'arxiv_id': 'arXiv:2502.20196', 'title': 'ChineseEcomQA: A Scalable E-commerce Concept Evaluation Benchmark for Large Language Models', 'authors': 'Haibin Chen, Kangtao Lv, Chengwei Hu, Yanshi Li, Yujin Yuan, Yancheng He, Xingyao Zhang, Langming Liu, Shilei Liu, Wenbo Su, Bo Zheng', 'link': 'https://arxiv.org/abs/2502.20196', 'abstract': 'With the increasing use of Large Language Models (LLMs) in fields such as e-commerce, domain-specific concept evaluation benchmarks are crucial for assessing their domain capabilities. Existing LLMs may generate factually incorrect information within the complex e-commerce applications. Therefore, it is necessary to build an e-commerce concept benchmark. Existing benchmarks encounter two primary challenges: (1) handle the heterogeneous and diverse nature of tasks, (2) distinguish between generality and specificity within the e-commerce field. To address these problems, we propose \\textbf{ChineseEcomQA}, a scalable question-answering benchmark focused on fundamental e-commerce concepts. ChineseEcomQA is built on three core characteristics: \\textbf{Focus on Fundamental Concept}, \\textbf{E-commerce Generality} and \\textbf{E-commerce Expertise}. Fundamental concepts are designed to be applicable across a diverse array of e-commerce tasks, thus addressing the challenge of heterogeneity and diversity. Additionally, by carefully balancing generality and specificity, ChineseEcomQA effectively differentiates between broad e-commerce concepts, allowing for precise validation of domain capabilities. We achieve this through a scalable benchmark construction process that combines LLM validation, Retrieval-Augmented Generation (RAG) validation, and rigorous manual annotation. Based on ChineseEcomQA, we conduct extensive evaluations on mainstream LLMs and provide some valuable insights. We hope that ChineseEcomQA could guide future domain-specific evaluations, and facilitate broader LLM adoption in e-commerce applications.', 'abstract_zh': '随着大型语言模型（LLMs）在电子商务等领域的广泛应用，特定领域的概念评估基准对于评估其领域能力至关重要。现有的LLMs可能在复杂的电子商务应用中生成事实错误的信息，因此需要建立一个电子商务概念基准。现有的基准面临两个主要挑战：（1）处理任务的异构性和多样性；（2）区分电子商务领域的一般性和具体性。为了解决这些问题，我们提出了**ChineseEcomQA**，这是一个专注于电子商务基本概念的可扩展的问答基准。ChineseEcomQA基于三个核心特性：**聚焦于基本概念**、**电子商务一般性**和**电子商务专门知识**。基本概念旨在适用于各种多样的电子商务任务，从而解决异构性和多样性的挑战。此外，通过精心平衡一般性和具体性，ChineseEcomQA能够有效地区分广泛的电子商务概念，从而实现对领域能力的精确验证。我们通过一个可扩展的基准构建过程实现这一点，该过程结合了LLM验证、检索增强生成（RAG）验证和严谨的手动注释。基于ChineseEcomQA，我们对主流的LLMs进行了广泛的评估，并提供了一些宝贵的见解。我们希望ChineseEcomQA能够指导未来的特定领域评估，并促进更广泛的LLM在电子商务应用中的采用。', 'title_zh': 'ChineseEcomQA：面向大规模语言模型的可扩展电子商务概念评估基准'}
{'arxiv_id': 'arXiv:2502.20186', 'title': 'Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following Knowledge', 'authors': 'Yan-Lun Chen, Yi-Ru Wei, Chia-Yi Hsu, Chia-Mu Yu, Chun-Ying Huang, Ying-Dar Lin, Yu-Sung Wu, Wei-Bin Lee', 'link': 'https://arxiv.org/abs/2502.20186', 'abstract': 'Large language models (LLMs) demonstrate strong task-specific capabilities through fine-tuning, but merging multiple fine-tuned models often leads to degraded performance due to overlapping instruction-following components. Task Arithmetic (TA), which combines task vectors derived from fine-tuning, enables multi-task learning and task forgetting but struggles to isolate task-specific knowledge from general instruction-following behavior. To address this, we propose Layer-Aware Task Arithmetic (LATA), a novel approach that assigns layer-specific weights to task vectors based on their alignment with instruction-following or task-specific components. By amplifying task-relevant layers and attenuating instruction-following layers, LATA improves task learning and forgetting performance while preserving overall model utility. Experiments on multiple benchmarks, including WikiText-2, GSM8K, and HumanEval, demonstrate that LATA outperforms existing methods in both multi-task learning and selective task forgetting, achieving higher task accuracy and alignment with minimal degradation in output quality. Our findings highlight the importance of layer-wise analysis in disentangling task-specific and general-purpose knowledge, offering a robust framework for efficient model merging and editing.', 'abstract_zh': '大型语言模型（LLMs）通过微调展示出强大的任务特定能力，但将多个微调模型合并通常会导致性能下降，因为存在重叠的指令遵循组件。任务算术（TA），结合从微调中提取的任务向量，能够实现多任务学习和任务遗忘，但在隔离特定任务的知识与普遍指令遵循行为方面存在困难。为了解决这一问题，我们提出了一种新颖的方法——分层感知任务算术（LATA），该方法基于任务向量与指令遵循或特定任务组件的对齐程度，为不同层分配特定的权重。通过放大相关任务的层次并减弱指令遵循的层次，LATA在保持整体模型功能的前提下，提高了任务学习和遗忘性能。在WikiText-2、GSM8K和HumanEval等多个基准上的实验表明，LATA在多任务学习和选择性任务遗忘方面优于现有方法，实现了更高的任务准确性和更好的对齐效果，同时最小化了输出质量的下降。我们的研究成果突显了分层分析在解耦特定任务和通用知识方面的重要性，提供了一种高效的模型合并和编辑的稳健框架。', 'title_zh': '层次意识任务算术：解开任务特定知识与指令遵循知识的关联'}
{'arxiv_id': 'arXiv:2502.20171', 'title': 'Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign Language Technologies', 'authors': 'Toon Vandendriessche, Mathieu De Coster, Annelies Lejon, Joni Dambre', 'link': 'https://arxiv.org/abs/2502.20171', 'abstract': 'Isolated Sign Language Recognition (ISLR) is crucial for scalable sign language technology, yet language-specific approaches limit current models. To address this, we propose a one-shot learning approach that generalises across languages and evolving vocabularies. Our method involves pretraining a model to embed signs based on essential features and using a dense vector search for rapid, accurate recognition of unseen signs. We achieve state-of-the-art results, including 50.8% one-shot MRR on a large dictionary containing 10,235 unique signs from a different language than the training set. Our approach is robust across languages and support sets, offering a scalable, adaptable solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH) community, this method aligns with real-world needs, and advances scalable sign language recognition.', 'abstract_zh': '孤立手语识别(ISLR)对于可扩展的手语技术至关重要，但当前的语言特定方法限制了现有模型的应用范围。为了解决这一问题，我们提出了一种一站式的识别方法，该方法能够在不同语言和不断扩展的词汇库之间进行泛化。该方法包括预训练一个模型，使其基于手语的必需特征进行嵌入，并使用密集向量搜索来快速准确地识别未见过的手语。我们取得了最先进的性能，例如，在一个包含10,235个来自不同于训练集的手语独特手语的大词典中，实现了50.8%的一站式MRR。我们的方法在不同语言和子集上表现出鲁棒性，提供了一种可扩展且适应性强的ISLR解决方案。本方法与听障社区（DHH）共同创造，符合实际需求，并推动了可扩展手语识别的进步。', 'title_zh': '将符号视为符号的表示：实现功能手语技术的一次性ISLR方法'}
{'arxiv_id': 'arXiv:2502.20135', 'title': 'Educator Attention: How computational tools can systematically identify the distribution of a key resource for students', 'authors': 'Qingyang Zhang, Rose E. Wang, Ana T. Ribeiro, Dora Demszky, Susanna Loeb', 'link': 'https://arxiv.org/abs/2502.20135', 'abstract': 'Educator attention is critical for student success, yet how educators distribute their attention across students remains poorly understood due to data and methodological constraints. This study presents the first large-scale computational analysis of educator attention patterns, leveraging over 1 million educator utterances from virtual group tutoring sessions linked to detailed student demographic and academic achievement data. Using natural language processing techniques, we systematically examine the recipient and nature of educator attention. Our findings reveal that educators often provide more attention to lower-achieving students. However, disparities emerge across demographic lines, particularly by gender. Girls tend to receive less attention when paired with boys, even when they are the lower achieving student in the group. Lower-achieving female students in mixed-gender pairs receive significantly less attention than their higher-achieving male peers, while lower-achieving male students receive significantly and substantially more attention than their higher-achieving female peers. We also find some differences by race and English learner (EL) status, with low-achieving Black students receiving additional attention only when paired with another Black student but not when paired with a non-Black peer. In contrast, higher-achieving EL students receive disproportionately more attention than their lower-achieving EL peers. This work highlights how large-scale interaction data and computational methods can uncover subtle but meaningful disparities in teaching practices, providing empirical insights to inform more equitable and effective educational strategies.', 'abstract_zh': '教育者的注意力对学生成功至关重要，但由于数据和方法上的限制，教育者如何在其学生之间分配注意力仍知之甚少。本研究首次进行了大规模的计算分析，利用超过100万条与详细的学生人口统计和学业成就数据相对应的虚拟群体辅导会话中的教育者发言记录。通过自然语言处理技术，我们系统地研究了教育者注意力的接受者和性质。研究发现，教育者往往会更加关注学业较低的学生。然而，不同人口统计群体之间存在差异，尤其是性别差异。当女孩和男孩组成一对时，女孩通常会收到较少的注意力，即使她是组内的学业较低的学生。在混合性别的配对中，学业较低的女生比学业较高的男生收到的注意力明显更少，而学业较低的男生比学业较高的女生收到的注意力显著且明显更多。我们还发现种族和英语学习者（EL）状态之间存在一些差异，低学业表现的黑人学生仅在与其他黑人学生配对时才会收到额外的注意力，而在与其他非黑人学生配对时则不会。相比之下，高学业表现的英语学习者学生比低学业表现的英语学习者学生收到的注意力更多。这项研究突显了大规模交互数据和计算方法如何揭示教学实践中的微妙但重要的差异，提供实证见解以指导更加公平和有效的教育策略。', 'title_zh': '教育者注意力：计算工具如何系统地识别关键教学资源的分布'}
{'arxiv_id': 'arXiv:2502.20129', 'title': 'Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking', 'authors': 'Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin', 'link': 'https://arxiv.org/abs/2502.20129', 'abstract': 'Chain-of-Thought (CoT) significantly enhances the performance of large language models (LLMs) across a wide range of tasks, and prior research shows that CoT can theoretically increase expressiveness. However, there is limited mechanistic understanding of the algorithms that Transformer+CoT can learn. In this work, we (1) evaluate the state tracking capabilities of Transformer+CoT and its variants, confirming the effectiveness of CoT. (2) Next, we identify the circuit, a subset of model components, responsible for tracking the world state, finding that late-layer MLP neurons play a key role. We propose two metrics, compression and distinction, and show that the neuron sets for each state achieve nearly 100% accuracy, providing evidence of an implicit finite state automaton (FSA) embedded within the model. (3) Additionally, we explore three realistic settings: skipping intermediate steps, introducing data noise, and testing length generalization. Our results demonstrate that Transformer+CoT learns robust algorithms (FSA), highlighting its resilience in challenging scenarios.', 'abstract_zh': '链式思维（Chain-of-Thought, CoT）显著提高了大型语言模型（Large Language Models, LLMs）在各种任务上的性能，并且先前的研究表明CoT在理论上可以增加模型的表达能力。然而，Transformers+CoT可以学习的算法机制尚缺乏深入理解。在本项工作中，我们（1）评估了Transformers+CoT及其变体的状态跟踪能力，证实了CoT的有效性。（2）接下来，我们识别出了电路，这是一种模型组件的子集，负责跟踪世界状态，发现较深层级的MLP神经元起到了关键作用。我们提出了两个指标：压缩性和区分性，并证明每个状态对应的神经元集可以实现近100%的准确率，这为模型内部嵌入了隐含有限状态自动机（Finite State Automaton, FSA）提供了证据。（3）此外，我们还探讨了三个现实场景：跳过中间步骤、引入数据噪声以及长度泛化测试。实验结果表明，Transformers+CoT学习的是鲁棒的算法（FSA），突显了其在具有挑战性的情景下的鲁棒性。', 'title_zh': '在Transformer中结合链式思考的有限状态自动机：一种关于状态跟踪的机制研究'}
{'arxiv_id': 'arXiv:2502.20122', 'title': 'Self-Training Elicits Concise Reasoning in Large Language Models', 'authors': 'Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun', 'link': 'https://arxiv.org/abs/2502.20122', 'abstract': 'Chain-of-thought (CoT) reasoning has enabled large language models (LLMs) to utilize additional computation through intermediate tokens to solve complex tasks. However, we posit that typical reasoning traces contain many redundant tokens, incurring extraneous inference costs. Upon examination of the output distribution of current LLMs, we find evidence on their latent ability to reason more concisely, relative to their default behavior. To elicit this capability, we propose simple fine-tuning methods which leverage self-generated concise reasoning paths obtained by best-of-N sampling and few-shot conditioning, in task-specific settings. Our combined method achieves a 30% reduction in output tokens on average, across five model families on GSM8K and MATH, while maintaining average accuracy. By exploiting the fundamental stochasticity and in-context learning capabilities of LLMs, our self-training approach robustly elicits concise reasoning on a wide range of models, including those with extensive post-training. Code is available at this https URL', 'abstract_zh': '链式思考（Chain-of-Thought, CoT）推理使大型语言模型（LLMs）能够通过中间令牌进行额外的计算来解决复杂任务。然而，我们提出，典型的推理路径中包含了许多冗余令牌，导致不必要的推理成本。通过对当前LLMs的输出分布进行分析，我们发现它们潜在地具有更加简洁推理的能力，相对于其默认行为。为了激发这种能力，我们提出了简单的小样本调优方法，该方法利用通过best-of-N抽样和少量样本条件生成的简洁推理路径，在特定任务中实现这一目标。我们的综合方法在五个模型家族上（GSM8K和MATH）平均减少了30%的输出令牌，同时保持了平均准确率。通过利用LLMs的基本随机性和上下文学习能力，我们的自训练方法稳定地激发了各种模型上的简洁推理能力，包括那些进行了大量后续训练的模型。相关代码可在此网址获取：this https URL', 'title_zh': '自我训练促使大型语言模型进行简洁推理'}
{'arxiv_id': 'arXiv:2502.20082', 'title': 'LongRoPE2: Near-Lossless LLM Context Window Scaling', 'authors': 'Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, Mao Yang', 'link': 'https://arxiv.org/abs/2502.20082', 'abstract': 'LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta\'s approach, which fails to reach the target effective context length. Code will be available at this https URL.', 'abstract_zh': 'LongRoPE2 是一种新型方法，旨在延长预训练大规模语言模型（LLMs）的有效上下文窗口至目标长度，同时在原始较短的上下文窗口上保持性能。这一目标通过以下三项贡献实现：（1）假设在较高 RoPE 维度中训练不足导致了现有方法中持续存在的分布外（OOD）问题；（2）一种有效的 RoPE 矢量调整算法，采用由“针驱式”困惑度指导的进化搜索来解决训练不足的问题；（3）一种混合上下文窗口训练方法，该方法微调模型权重以采用调整后的 RoPE 来适应长上下文序列，同时在保留短上下文性能的同时使用原始 RoPE。在 LLaMA3-8B 和 Phi3-mini-3.8B 上的各类基准实验中，验证了这一假设并展示了 LongRoPE2 的有效性。值得注意的是，LongRoPE2 仅使用 10B 个标记（比 Meta 的方法少 80 倍），就能将 LLaMA3-8B 的有效上下文长度扩展至 128K，同时保持超过 98.5% 的短上下文性能。有关代码将在此处提供。', 'title_zh': 'LongRoPE2：几乎无损失的大语言模型上下文窗口缩放'}
{'arxiv_id': 'arXiv:2502.20073', 'title': 'Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents', 'authors': 'Haochen Sun, Shuwen Zhang, Lei Ren, Hao Xu, Hao Fu, Caixia Yuan, Xiaojie Wang', 'link': 'https://arxiv.org/abs/2502.20073', 'abstract': 'Large language models (LLMs) based agent systems have made great strides in real-world applications beyond traditional NLP tasks. This paper proposes a new LLM-powered Multi-Agent System (LLM-MAS) benchmark, Collab-Overcooked, built on the popular Overcooked-AI game with more applicable and challenging tasks in interactive environments. Collab-Overcooked extends existing benchmarks from two novel perspectives. First, it provides a multi-agent framework supporting diverse tasks and objectives and encourages collaboration through natural language communication. Second, it introduces a spectrum of process-oriented evaluation metrics to assess the fine-grained collaboration capabilities of different LLM agents, a dimension often overlooked in prior work. We conduct extensive experiments over 10 popular LLMs and show that, while the LLMs present a strong ability in goal interpretation, there is a significant discrepancy in active collaboration and continuous adaption that are critical for efficiently fulfilling complicated tasks. Notably, we highlight the strengths and weaknesses in LLM-MAS and provide insights for improving and evaluating LLM-MAS on a unified and open-sourced benchmark. Environments, 30 open-ended tasks, and an integrated evaluation package are now publicly available at this https URL.', 'abstract_zh': '基于大型语言模型（LLMs）的代理系统已经在超出传统NLP任务的实际应用中取得了显著进展。本文提出了一种新的LLM赋能的多代理系统（LLM-MAS）基准测试——Collab-Overcooked，该基准系统基于流行的Overcooked-AI游戏，并引入了更多适用且具有挑战性的交互环境任务。Collab-Overcooked 从两个新颖的角度扩展了现有的基准测试。首先，它提供了一个支持多种任务和目标的多代理框架，并通过自然语言通信促进协作。其次，它引入了一系列过程导向的评估指标，用以评估不同LLM代理的细粒度协作能力，这是以往工作中经常被忽视的一个维度。我们对10种流行的LLM进行了广泛的实验，并展示了虽然这些LLM在目标理解方面表现出强大的能力，但在主动协作和持续适应性方面仍存在显著差异，这些能力对于高效完成复杂任务至关重要。值得注意的是，我们指出了LLM-MAS的优势和不足，并提供了一种统一和开源的基准来改进和评估LLM-MAS。当前，环境、30项开放任务以及集成评估包已公开，访问链接为：this https URL。', 'title_zh': 'Collab-Overcooked: 评估大型语言模型作为协作代理的基准测试与评估'}
{'arxiv_id': 'arXiv:2502.20047', 'title': 'Connecting the Persian-speaking World through Transliteration', 'authors': 'Rayyan Merchant, Akhilesh Kakolu Ramarao, Kevin Tang', 'link': 'https://arxiv.org/abs/2502.20047', 'abstract': 'Despite speaking mutually intelligible varieties of the same language, speakers of Tajik Persian, written in a modified Cyrillic alphabet, cannot read Iranian and Afghan texts written in the Perso-Arabic script. As the vast majority of Persian text on the Internet is written in Perso-Arabic, monolingual Tajik speakers are unable to interface with the Internet in any meaningful way. Due to overwhelming similarity between the formal registers of these dialects and the scarcity of Tajik-Farsi parallel data, machine transliteration has been proposed as more a practical and appropriate solution than machine translation. This paper presents a transformer-based G2P approach to Tajik-Farsi transliteration, achieving chrF++ scores of 58.70 (Farsi to Tajik) and 74.20 (Tajik to Farsi) on novel digraphic datasets, setting a comparable baseline metric for future work. Our results also demonstrate the non-trivial difficulty of this task in both directions. We also provide an overview of the differences between the two scripts and the challenges they present, so as to aid future efforts in Tajik-Farsi transliteration.', 'abstract_zh': '尽管塔吉克波斯语的使用者 speaks 互为可懂的同一种语言，但使用修改后的西里尔字母书写的塔吉克波斯语使用者无法阅读使用波斯-阿拉伯字母书写的伊朗语和阿富汗语文本。由于互联网上绝大多数的波斯文本都是使用波斯-阿拉伯字母书写的，因而单一使用塔吉克语的使用者无法以有意义的方式接入网络。由于这些方言的标准形式之间存在显著的相似性，且塔吉克语-波斯语平行数据稀缺，因此，机器转写被提议作为一种更实用和合适的选择，而不是机器翻译。本文提出了一种基于变换器的音素化（G2P）方法来实现塔吉克语-波斯语的转写，在新颖的双字母字符数据集上，Farsi到Tajik方向和Tajik到Farsi方向分别获得了chrF++分数58.70和74.20，为未来的工作提供了可比的基础指标。我们的结果还表明了在这两个方向上完成这项任务的非平凡难度。我们还概述了两种字母系统之间的差异及其所面临的挑战，旨在为未来的塔吉克语-波斯语转写努力提供帮助。', 'title_zh': '通过转写连接波斯语世界'}
{'arxiv_id': 'arXiv:2502.20046', 'title': 'Polish-ASTE: Aspect-Sentiment Triplet Extraction Datasets for Polish', 'authors': 'Marta Lango, Borys Naglik, Mateusz Lango, Iwo Naglik', 'link': 'https://arxiv.org/abs/2502.20046', 'abstract': 'Aspect-Sentiment Triplet Extraction (ASTE) is one of the most challenging and complex tasks in sentiment analysis. It concerns the construction of triplets that contain an aspect, its associated sentiment polarity, and an opinion phrase that serves as a rationale for the assigned polarity. Despite the growing popularity of the task and the many machine learning methods being proposed to address it, the number of datasets for ASTE is very limited. In particular, no dataset is available for any of the Slavic languages. In this paper, we present two new datasets for ASTE containing customer opinions about hotels and purchased products expressed in Polish. We also perform experiments with two ASTE techniques combined with two large language models for Polish to investigate their performance and the difficulty of the assembled datasets. The new datasets are available under a permissive licence and have the same file format as the English datasets, facilitating their use in future research.', 'abstract_zh': 'Aspect-情感三元组提取（ASTE）是情感分析中最具挑战性和复杂性的任务之一。它涉及构建包含方面、与其关联的情感极性和作为该极性依据的意见短语的三元组。尽管该任务的 popularity 正在增长，并且提出了许多机器学习方法来解决这一问题，但用于 ASTE 的数据集数量非常有限。特别是，任何斯拉夫语言的数据集都不存在。在本文中，我们介绍了两个新的数据集，这些数据集包含用波兰语表达的客户对酒店和购买产品的意见。我们还使用两种ASTE方法结合两种大型语言模型对波兰语数据集进行了实验，以研究它们的性能及其构建的数据集的难度。新的数据集在宽松的许可下提供，并具有与英语数据集相同的文件格式，便于未来研究的使用。', 'title_zh': 'Polish-ASTE：用于波兰语的方面-情感三元组提取数据集'}
{'arxiv_id': 'arXiv:2502.19982', 'title': 'Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large Language Models', 'authors': 'Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao', 'link': 'https://arxiv.org/abs/2502.19982', 'abstract': 'In this paper, we explore machine unlearning from a novel dimension, by studying how to safeguard model unlearning in large language models (LLMs). Our goal is to prevent unlearned models from recalling any related memory of the targeted this http URL begin by uncovering a surprisingly simple yet overlooked fact: existing methods typically erase only the exact expressions of the targeted knowledge, leaving paraphrased or related information intact. To rigorously measure such oversights, we introduce UGBench, the first benchmark tailored for evaluating the generalisation performance across 13 state-of-the-art this http URL reveals that unlearned models can still recall paraphrased answers and retain target facts in intermediate layers. To address this, we propose PERMU, a perturbation-based method that significantly enhances the generalisation capabilities for safeguarding LLM this http URL demonstrate that PERMU delivers up to a 50.13% improvement in unlearning while maintaining a 43.53% boost in robust generalisation. Our code can be found in this https URL.', 'abstract_zh': '在本文中，我们从一个新的维度探索了机器遗忘技术，特别关注如何保护大语言模型（LLMs）中的模型遗忘。我们的目标是防止未学习的模型重新回忆与目标知识相关的任何记忆。我们通过揭开一个令人惊讶且被忽视的事实开始：现有方法通常只抹去目标知识的精确表达，而保留了其同义或相关的信息。为了严格评估这些疏忽，我们引入了UGBenchmark，这是首个专门用于评估13种领先方法泛化性能的基准。UGBenchmark揭示了未学习的模型仍然能够回忆同义答案并在中间层保留目标事实。为此，我们提出了一种基于扰动的方法PERMU，该方法显著增强了保护LLMs泛化能力的能力。通过实验证明，PERMU在未学习方面可带来高达50.13%的改进，同时在鲁棒泛化方面仍保持43.53%的增益。我们的代码可以在这里访问：https://github.com/PERMULang/PERMULang。', 'title_zh': '《擦除而不记忆：大型语言模型中的知识遗忘保护》\n\n这个标题翻译成中文基本符合学术规范，但是可以根据具体的上下文进一步调整以确保更准确地传达原意。如果需要更详细的翻译或有其他具体要求，请告知。'}
{'arxiv_id': 'arXiv:2502.19981', 'title': 'The Lookahead Limitation: Why Multi-Operand Addition is Hard for LLMs', 'authors': 'Tanja Baeumel, Josef van Genabith, Simon Ostermann', 'link': 'https://arxiv.org/abs/2502.19981', 'abstract': "Autoregressive large language models (LLMs) exhibit impressive performance across various tasks but struggle with simple arithmetic, such as addition of two or more operands. We show that this struggle arises from LLMs' use of a simple one-digit lookahead heuristic, which works fairly well (but not perfect) for two-operand addition but fails in multi-operand cases, where the carry-over logic is more complex. Our probing experiments and digit-wise accuracy evaluation show that LLMs fail precisely where a one-digit lookahead is insufficient to account for cascading carries. We analyze the impact of tokenization strategies on arithmetic performance and show that all investigated models, regardless of tokenization, are inherently limited in the addition of multiple operands due to their reliance on a one-digit lookahead heuristic. Our findings reveal fundamental limitations that prevent LLMs from generalizing to more complex numerical reasoning.", 'abstract_zh': '自回归大型语言模型（LLMs）在各种任务中表现出色，但在简单的算术运算，如两个或多个操作数相加时，却表现出挑战。我们表明，这种挑战源于LLMs使用的一种简单的单位位前瞻启发式方法，这种启发式方法对于两个操作数的加法工作得相当好（但并不完美），但在多操作数情况下失效，因为这些情况下进位逻辑更为复杂。我们的探查实验和按位准确度评估表明，LLMs在那种单位位前瞻不足以解释连续进位的地方出错。我们分析了分词策略对算术性能的影响，并表明无论采用哪种分词策略，所有研究的模型都因依赖单位位前瞻启发式方法而在处理多个操作数的加法时固有地受到限制。我们的发现揭示了基本的局限性，这些局限性阻碍了LLMs对更复杂的数值推理进行泛化的能力。', 'title_zh': '前瞻限制：为什么多操作数加法对大模型具有挑战性'}
{'arxiv_id': 'arXiv:2502.19965', 'title': 'Deterministic or probabilistic? The psychology of LLMs as random number generators', 'authors': 'Javier Coronado-Blázquez', 'link': 'https://arxiv.org/abs/2502.19965', 'abstract': 'Large Language Models (LLMs) have transformed text generation through inherently probabilistic context-aware mechanisms, mimicking human natural language. In this paper, we systematically investigate the performance of various LLMs when generating random numbers, considering diverse configurations such as different model architectures, numerical ranges, temperature, and prompt languages. Our results reveal that, despite their stochastic transformers-based architecture, these models often exhibit deterministic responses when prompted for random numerical outputs. In particular, we find significant differences when changing the model, as well as the prompt language, attributing this phenomenon to biases deeply embedded within the training data. Models such as DeepSeek-R1 can shed some light on the internal reasoning process of LLMs, despite arriving to similar results. These biases induce predictable patterns that undermine genuine randomness, as LLMs are nothing but reproducing our own human cognitive biases.', 'abstract_zh': '大规模语言模型（LLMs）通过固有的概率上下文感知机制，模仿人类自然语言，从而改变了文本生成的方式。本文系统地研究了各种LLMs在生成随机数时的表现，考虑了不同的模型架构、数值范围、温度和提示语言等多样化配置。研究结果表明，尽管这些模型的架构基于随机变换器，但在被要求生成随机数时，它们往往表现出确定性的响应。特别地，我们发现，在改变模型和提示语言时，会出现显著差异，将这种现象归因于训练数据中深深嵌入的偏见。尽管如DeepSeek-R1这样的模型可以揭示一些LLMs的内部推理过程，但这些偏见会导致可预测的模式，从而削弱真正的随机性，因为LLMs实际上只是再现了我们自己的人类认知偏见。', 'title_zh': '确定性还是概率性？搜索引擎大模型作为随机数生成器的心理学探讨'}
{'arxiv_id': 'arXiv:2502.19954', 'title': 'Collaborative Stance Detection via Small-Large Language Model Consistency Verification', 'authors': 'Yu Yan, Sheng Sun, Zixiang Tang, Teli Liu, Min Liu', 'link': 'https://arxiv.org/abs/2502.19954', 'abstract': 'Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.', 'abstract_zh': '社交媒体上的立场检测旨在识别推文中对特定目标表达的态度。当前研究倾向于使用大规模语言模型（LLMs），因为它们的性能提升效果显著。然而，仅仅依赖LLMs进行立场检测，在不考虑成本的情况下，对于需要大量数据分析的现实世界社交媒体监控系统来说是不切实际的。为了解决这一问题，我们提出了一种名为**\\textbf{\\underline{Co}}**llaborative Stance Detection via \\textbf{\\underline{Ver}}ification of Small and Large Language Model Consistency (\\textbf{CoVer}) 的框架，该框架通过上下文共享批次推理和LLM与SLM之间的逻辑验证来增强LLM的利用。具体而言，CoVer并不是逐条处理文本，而是逐批次处理文本，通过共享上下文中的LLM推理来获得立场预测及其相应的解释。然后，为了排除由上下文噪声引起的偏差，CoVer引入了SLM进行逻辑一致性验证。最后，表现出低逻辑一致性的文本重复出现时，使用一致性加权聚合的先验LLM立场预测进行分类。我们的实验表明，在零样本设置下，CoVer在多个基准测试中优于最先进的方法，平均每条推文只需要0.54次LLM查询，同时显著提升了性能。我们的CoVer为社交媒体立场检测中的LLM部署提供了更加实际的解决方案。', 'title_zh': '基于小型-大型语言模型一致性验证的协作立场检测'}
{'arxiv_id': 'arXiv:2502.19953', 'title': 'GeoEdit: Geometric Knowledge Editing for Large Language Models', 'authors': 'Yujie Feng, Liming Zhan, Zexin Lu, Yongxin Xu, Xu Chu, Yasha Wang, Jiannong Cao, Philip S. Yu, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2502.19953', 'abstract': 'Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). Consequently, various model editing methods have been developed to update specific knowledge within LLMs. However, training-based approaches often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model\'s generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a "forget-then-learn" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods.', 'abstract_zh': '定期更新对于维护大型语言模型（LLMs）的最新知识至关重要。因此，已经开发出了各种模型编辑方法来更新LLMs中的特定知识。然而，基于训练的方法往往难以有效地整合新知识而不破坏相关的一般知识。为了解决这一挑战，我们提出了一种名为几何知识编辑（GeoEdit）的新型框架。GeoEdit 利用微调中参数更新的几何关系来区分与新知识更新相关的神经元和与一般知识扰动相关的神经元。通过采用方向感知的知识识别方法，我们避免更新方向与现有知识方向大约正交的神经元，从而保持模型的泛化能力。对于其余的神经元，我们整合旧知识和新知识的方向，并对相反方向应用“先忘记后学习”的编辑策略。此外，我们引入了一种基于重要性指导的任务向量融合技术，该技术过滤掉冗余信息并提供适应性的神经元级别权重，进一步提高模型编辑性能。在两个公开的数据集上的广泛实验表明，GeoEdit 在现有最先进的方法中表现更优。', 'title_zh': 'GeoEdit: 用于大规模语言模型的几何知识编辑'}
{'arxiv_id': 'arXiv:2502.19941', 'title': 'Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation', 'authors': 'Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, Shujian Huang', 'link': 'https://arxiv.org/abs/2502.19941', 'abstract': 'Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce ADSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. ADSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of word-level labels. ADSE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.', 'abstract_zh': '质量估计（QE）模型能够在没有参考翻译的情况下评估机器翻译的质量，作为翻译任务的奖励模型。由于数据稀缺，合成数据生成已成为一种有前景的解决方案。然而，合成QE数据常常会遭受分布偏移的问题，这可能会表现为伪翻译与真实翻译之间的差异，或伪标签与人类偏好不一致。为了应对这一问题，我们提出了ADSQE，一种缓解合成QE数据分布偏移的新框架。为了减少伪翻译与真实翻译之间的差异，我们使用约束的束搜索算法，并通过使用不同的生成模型来增强翻译的多样性。ADSQE利用参考翻译，即翻译监督信号，来指导生成和标注过程，从而提高单词级标签的质量。ADSE进一步识别覆盖连续错误标记的最短短语，模仿人类注释行为，从而为最终的短语级标签赋值。特别是，我们强调翻译模型不能准确地标注其自身的翻译。广泛的实验表明，ADSQE在有监督和无监督设置中均优于当前最佳基线模型如COMET。进一步的分析为其他任务的奖励模型提供了关于合成数据生成的见解。', 'title_zh': '提高合成数据在机器翻译质量评估中分布漂移问题的缓解方法'}
{'arxiv_id': 'arXiv:2502.19917', 'title': 'Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents', 'authors': 'Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang', 'link': 'https://arxiv.org/abs/2502.19917', 'abstract': "To improve Multimodal Large Language Models' (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \\textbf{Vi}sual-Centric \\textbf{S}election approach via \\textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5\\% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at this https URL.", 'abstract_zh': '为了提高多模态大型语言模型（MLLMs）处理图像和复杂指令的能力，研究者们主要建立了大规模的视觉指令调优数据集，这些数据集要么来源自现有的视觉任务，要么是使用大型语言模型和图像描述合成生成的。然而，这类数据集往往存在关键性的缺陷，包括指令与图像对齐不准确以及低质量的图像。这些问题阻碍了训练效率，并限制了性能提升，因为模型将宝贵的资源浪费在了噪声较大或无关的数据上，而这些数据对整体能力的提升几乎没有益处。为了解决这一问题，我们提出了一种通过代理协作的视觉中心选择方法（ViSA），该方法以图像质量评估和图像-指令相关性评估为核心。具体而言，我们的方法包括以下两个方面：1) 通过视觉代理协作量化图像信息，以选择富含视觉信息的图像；2) 采用视觉中心的指令质量评估方法，以选择与高质量图像相关联的高质量指令数据。最后，我们重新组织了来自大型开源数据集的8万条指令数据。广泛的实验表明，使用仅占原始数据2.5%的量，ViSA在七个基准测试中展现了优于或可与现有最先进的模型相媲美的性能，突显了我们数据选择方法的高效性。此外，我们还进行了消融研究，以验证我们方法中每个组件的有效性。相关代码可在以下网址获取：这个 https URL。', 'title_zh': '精挑细选精华：基于视觉的数据选择与协作代理方法'}
{'arxiv_id': 'arXiv:2502.19870', 'title': 'MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge', 'authors': 'Yuntao Du, Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li', 'link': 'https://arxiv.org/abs/2502.19870', 'abstract': 'Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing. Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 8,363 images across 33 broad categories, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.', 'abstract_zh': '知识编辑技术已成为更新大规模语言模型（LLMs）和多模态模型（LMMs）事实性知识的重要工具，使其能够纠正过时或不准确的信息，而无需从头开始重新训练。然而，现有的多模态知识编辑基准主要集中在用简单三元组表示的实体级知识上，无法捕捉到现实世界多模态信息的复杂性。为解决这一问题，我们提出了MMKE-Bench，这是一个全面的多模态知识编辑基准，旨在评估LMMs在实际场景中编辑多样化视觉知识的能力。MMKE-Bench克服了这些局限性，通过整合三种类型的编辑任务：视觉实体编辑、视觉语义编辑和用户特定编辑来实现这一目标。此外，MMKE-Bench采用自由形式的自然语言表示和编辑知识，提供了一种更灵活和有效的格式。该基准包含2940条知识和8363张图像，覆盖33个广泛的类别，并由自动生成的评估问题和人工验证组成。我们在三个主流LMM上评估了五种最先进的知识编辑方法，结果显示没有一种方法在所有标准上都表现出色，且视觉和用户特定的编辑尤为具有挑战性。MMKE-Bench为评估多模态知识编辑技术的稳健性树立了新标准，推动了这一迅速发展的领域的进步。', 'title_zh': 'MMKE-Bench：一种用于多元视觉知识多样编辑的基准测试'}
{'arxiv_id': 'arXiv:2502.19860', 'title': 'MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue', 'authors': 'Yujia Chen, Changsong Li, Yiming Wang, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan', 'link': 'https://arxiv.org/abs/2502.19860', 'abstract': "Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.", 'abstract_zh': '当今竞争激烈的社会中，心理健康问题如抑郁和焦虑正在加剧。传统的治疗方法，如咨询和聊天机器人，往往无法有效吸引患者，并且常常提供缺乏情感深度的通用回应。尽管大型语言模型（LLMs）有可能创造更加人机互动的体验，但它们仍然难以捕捉微妙的情感。因此，亟需让LLMs具备类似人类的适应性和暖意。为填补这一空白，我们提出了一种名为MIND（Multi-agent INner Dialogue）的新颖范式，旨在提供更沉浸式的心理疗愈环境。考虑到LLM代理的强大生成能力和角色扮演能力，我们预先定义了一个互动疗愈框架，并将不同的角色分配给LLM代理，使其能够与用户进行互动的内心对话，从而提供更加沉浸式的疗愈体验。我们在多个现实世界的疗愈维度中进行了广泛的人类实验，并发现MIND为用户提供了一种更为友好且更具沉浸感的体验。这表明，MIND能够有效利用LLMs在心理疗愈领域的巨大潜力。', 'title_zh': 'MIND：走向多代理内心对话的沉浸式心理疗愈'}
{'arxiv_id': 'arXiv:2502.19856', 'title': 'Team A at SemEval-2025 Task 11: Breaking Language Barriers in Emotion Detection with Multilingual Models', 'authors': 'P Sam Sahil, Anupam Jamatia', 'link': 'https://arxiv.org/abs/2502.19856', 'abstract': "This paper describes the system submitted by Team A to SemEval 2025 Task 11, ``Bridging the Gap in Text-Based Emotion Detection.'' The task involved identifying the perceived emotion of a speaker from text snippets, with each instance annotated with one of six emotions: joy, sadness, fear, anger, surprise, or disgust. A dataset provided by the task organizers served as the foundation for training and evaluating our models. Among the various approaches explored, the best performance was achieved using multilingual embeddings combined with a fully connected layer. This paper details the system architecture, discusses experimental results, and highlights the advantages of leveraging multilingual representations for robust emotion detection in text.", 'abstract_zh': '本文描述了团队A提交给SemEval 2025任务11的系统，该任务名为“ Bridging the Gap in Text-Based Emotion Detection”。任务要求从文本片段中识别说话人的感知情感，每一份实例都标注了六种情绪之一：快乐、悲伤、恐惧、愤怒、惊讶或厌恶。主办方提供的数据集作为训练和评估模型的基础。在探索的各种方法中，使用多语言嵌入与全连接层结合的方法取得了最佳性能。本文详细说明了系统架构，讨论了实验结果，并突出了利用多语言表示增强文本中情感检测鲁棒性的优点。', 'title_zh': 'SemEval-2025 任务11 团队A：使用多语言模型打破情感识别的语言壁垒'}
{'arxiv_id': 'arXiv:2502.19830', 'title': 'Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation', 'authors': 'Yiwei Li, Ji Zhang, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Jiayi Shi, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.19830', 'abstract': 'Self-consistency improves reasoning by aggregating diverse stochastic samples, yet the dynamics behind its efficacy remain underexplored. We reframe self-consistency as a dynamic distributional alignment problem, revealing that decoding temperature not only governs sampling randomness but also actively shapes the latent answer distribution. Given that high temperatures require prohibitively large sample sizes to stabilize, while low temperatures risk amplifying biases, we propose a confidence-driven mechanism that dynamically calibrates temperature: sharpening the sampling distribution under uncertainty to align with high-probability modes, and promoting exploration when confidence is high. Experiments on mathematical reasoning tasks show this approach outperforms fixed-diversity baselines under limited samples, improving both average and best-case performance across varying initial temperatures without additional data or modules. This establishes self-consistency as a synchronization challenge between sampling dynamics and evolving answer distributions.', 'abstract_zh': '自我一致性通过聚集多样化的随机样本来提高推理能力，但其有效性的动力学机制仍待深入探索。我们将自我一致性重新定义为一个动态分布对齐问题，揭示了解码温度不仅控制着采样的随机性，还积极塑造了潜在答案的分布。鉴于高温度需要极大的样本数量来稳定效果，而低温度则容易放大偏见，我们提出了一种基于信心的机制，动态校准温度：在不确定性条件下，通过聚焦高概率模式来细化采样分布；在高信心情况下，促进探索。针对数学推理任务的实验表明，在有限样本下，该方法优于固定的多样性基线，能够在不同初始温度下提高平均性能和最佳性能，无需额外的数据或模块。这确立了自我一致性的同步挑战，即采样动力学与演化中的答案分布之间的同步。', 'title_zh': '从动态分布对齐视角 revisit 自一致性在答案聚合中的作用'}
{'arxiv_id': 'arXiv:2502.19820', 'title': 'Foot-In-The-Door: A Multi-turn Jailbreak for LLMs', 'authors': 'Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang', 'link': 'https://arxiv.org/abs/2502.19820', 'abstract': "Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical this http URL approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn this http URL code is available at this https URL .", 'abstract_zh': '确保大型语言模型的安全性至关重要，随着这些模型越来越多地被集成到实际应用中。一个关键挑战是“脱笼”攻击，即对抗性提示会绕过内置的安全防护，引出有害或禁止的输出。受到心理学中“脚在门内”原理的启发，我们引入了FITD（Foot-in-the-Door）这一新颖的多回合脱笼方法。该方法利用了初始的小规模承诺会降低对后续更大或更不道德的行为的抵制这种现象，逐步提升用户的恶意意图，并通过中间桥梁提示来引导模型响应，使其自行诱导产生有毒的输出。在两个脱笼基准测试上的大量实验结果显示，FITD在七种广泛使用模型上的平均攻击成功率达到了94%，优于现有最先进的方法。此外，我们还对LLM（大型语言模型）的自我腐蚀进行了深入分析，指出了当前对齐策略中的漏洞，并强调了多回合交互中固有的风险。相关代码可在以下链接获取：此链接。', 'title_zh': '脚入门槛：面向LLMs的多轮对话脱缰技术'}
{'arxiv_id': 'arXiv:2502.19784', 'title': 'NaijaNLP: A Survey of Nigerian Low-Resource Languages', 'authors': 'Isa Inuwa-Dutse', 'link': 'https://arxiv.org/abs/2502.19784', 'abstract': 'With over 500 languages in Nigeria, three languages -- Hausa, Yorùbá and Igbo -- spoken by over 175 million people, account for about 60% of the spoken languages. However, these languages are categorised as low-resource due to insufficient resources to support tasks in computational linguistics. Several research efforts and initiatives have been presented, however, a coherent understanding of the state of Natural Language Processing (NLP) - from grammatical formalisation to linguistic resources that support complex tasks such as language understanding and generation is lacking. This study presents the first comprehensive review of advancements in low-resource NLP (LR-NLP) research across the three major Nigerian languages (NaijaNLP). We quantitatively assess the available linguistic resources and identify key challenges. Although a growing body of literature addresses various NLP downstream tasks in Hausa, Igbo, and Yorùbá, only about 25.1% of the reviewed studies contribute new linguistic resources. This finding highlights a persistent reliance on repurposing existing data rather than generating novel, high-quality resources. Additionally, language-specific challenges, such as the accurate representation of diacritics, remain under-explored. To advance NaijaNLP and LR-NLP more broadly, we emphasise the need for intensified efforts in resource enrichment, comprehensive annotation, and the development of open collaborative initiatives.', 'abstract_zh': '在尼日利亚超过500种语言中，豪萨语、约鲁巴语和伊格博语这三种语言，拥有超过1.75亿使用者，占据了约60%的口语使用份额。然而，由于缺乏支持计算语言学任务的资源，这些语言被归类为低资源语言。尽管已有多项研究和倡议，但对于自然语言处理（NLP）的现状——从语法形式化到支持复杂任务（如语言理解和生成）的语言资源——尚未形成系统的理解。本文首次全面回顾了豪萨语、约鲁巴语和伊格博语这三大尼日利亚语言在低资源自然语言处理（NaijaNLP）研究方面的进展。我们对可用的语言资源进行了定量评估，并指出了关键的挑战。虽然关于豪萨语、伊格博语和约鲁巴语的下游NLP任务的研究文献正逐渐增多，但在本回顾分析中仅约25.1%的研究贡献了新的语言资源。这一发现表明，仍然存在对现有数据再利用而非生成高质量新资源的依赖问题。此外，语言特异性挑战，如准确表达重音符号，仍未得到充分探索。为了推动NaijaNLP和更广泛的低资源自然语言处理（LR-NLP），我们强调需要加强资源丰富化、全面标注以及开放协作倡议的努力。', 'title_zh': 'NaijaNLP：尼日利亚低资源语言综述'}
{'arxiv_id': 'arXiv:2502.19779', 'title': 'Do Retrieval-Augmented Language Models Adapt to Varying User Needs?', 'authors': 'Peilin Wu, Xinlu Zhang, Wenhao Yu, Xingyu Liu, Xinya Du, Zhiyu Zoey Chen', 'link': 'https://arxiv.org/abs/2502.19779', 'abstract': 'Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.', 'abstract_zh': '近年来，检索增强语言模型（RALMs）在知识密集型任务中的有效性得到了充分验证。然而，现有评估基准往往假设存在单一最优方法来利用检索信息，忽视了不同用户需求的差异。本文提出了一种新的评估框架，系统地在三种用户需求场景下评估RALMs：情境独占、情境优先和记忆优先，并在三种不同的上下文设置中进行评估：情境匹配、知识冲突和信息无关。通过变化用户指令和检索信息的性质，我们的方法捕捉了现实世界应用中模型必须适应多样化用户需求的复杂性。通过在多个问答数据集中进行广泛实验，包括HotpotQA、DisentQA以及我们新构建的合成URAQ数据集，我们发现限制内存使用在对抗性检索条件下提高了稳健性，但减少了理想的检索结果和峰值性能，并且模型家族主导了行为差异。研究结果突显了在检索增强系统开发中进行用户中心评估的必要性，并提供了针对不同检索场景优化模型性能的见解。论文被接受后，我们将发布我们的代码和URAQ数据集。', 'title_zh': '检索增强语言模型能适应不同用户需求吗？'}
{'arxiv_id': 'arXiv:2502.19773', 'title': 'Advancements in Natural Language Processing for Automatic Text Summarization', 'authors': 'Nevidu Jayatilleke, Ruvan Weerasinghe, Nipuna Senanayake', 'link': 'https://arxiv.org/abs/2502.19773', 'abstract': 'The substantial growth of textual content in diverse domains and platforms has led to a considerable need for Automatic Text Summarization (ATS) techniques that aid in the process of text analysis. The effectiveness of text summarization models has been significantly enhanced in a variety of technical domains because of advancements in Natural Language Processing (NLP) and Deep Learning (DL). Despite this, the process of summarizing textual information continues to be significantly constrained by the intricate writing styles of a variety of texts, which involve a range of technical complexities. Text summarization techniques can be broadly categorized into two main types: abstractive summarization and extractive summarization. Extractive summarization involves directly extracting sentences, phrases, or segments of text from the content without making any changes. On the other hand, abstractive summarization is achieved by reconstructing the sentences, phrases, or segments from the original text using linguistic analysis. Through this study, a linguistically diverse categorizations of text summarization approaches have been addressed in a constructive manner. In this paper, the authors explored existing hybrid techniques that have employed both extractive and abstractive methodologies. In addition, the pros and cons of various approaches discussed in the literature are also investigated. Furthermore, the authors conducted a comparative analysis on different techniques and matrices to evaluate the generated summaries using language generation models. This survey endeavors to provide a comprehensive overview of ATS by presenting the progression of language processing regarding this task through a breakdown of diverse systems and architectures accompanied by technical and mathematical explanations of their operations.', 'abstract_zh': '多元领域和平台中文本内容的显著增长，催生了对自动文本总结（Automated Text Summarization, ATS）技术的庞大需求，这些技术有助于文本分析的过程。随着自然语言处理（Natural Language Processing, NLP）和深度学习（Deep Learning, DL）技术的进步，文本总结模型的有效性在各个技术领域得到了显著提升。然而，总结文本信息的过程仍受到多种复杂文本丰富写作风格的严重限制，这些文本包含了广泛的复杂性。文本总结技术可以大致分为两大类：提取性总结和抽象性总结。提取性总结包含直接从内容中抽取句子、短语或文本片段而不做任何改动。而抽象性总结则是通过对原始文本进行语言分析，重建句子、短语或文本片段实现的。本文通过综合方式探讨了语言多样性的文本总结方法。在本文中，作者探索了既采用提取性方法又采用抽象性方法的现有混合技术。此外，作者还详细研究了文献中讨论的各种方法的优缺点。进一步地，作者对不同技术和评价矩阵进行了比较分析，以评估语言生成模型生成的总结。本文旨在通过详细分析各种系统和架构及其技术与数学解释，提供ATS的全面概述，并展示该任务在语言处理方面的进展。', 'title_zh': '自然语言处理在自动文本摘要方面的进展'}
{'arxiv_id': 'arXiv:2502.19765', 'title': 'EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models', 'authors': 'Che Hyun Lee, Heeseung Kim, Jiheum Yeom, Sungroh Yoon', 'link': 'https://arxiv.org/abs/2502.19765', 'abstract': 'We propose EdiText, a controllable text editing method that modify the reference text to desired attributes at various scales. We integrate an SDEdit-based editing technique that allows for broad adjustments in the degree of text editing. Additionally, we introduce a novel fine-level editing method based on self-conditioning, which allows subtle control of reference text. While being capable of editing on its own, this fine-grained method, integrated with the SDEdit approach, enables EdiText to make precise adjustments within the desired range. EdiText demonstrates its controllability to robustly adjust reference text at broad range of levels across various tasks, including toxicity control and sentiment control.', 'abstract_zh': '我们提出了EdiText，这是一种可控的文字编辑方法，能够对参考文本进行多尺度的属性修改。该方法结合了基于SDEdit的编辑技术，允许在文本编辑的程度上进行广泛调整。此外，我们引入了一种基于自条件的新颖细粒度编辑方法，该方法允许对参考文本进行微妙的控制。尽管这种细粒度方法本身具备编辑能力，但与SDEdit方法结合使用时，可以让EdiText在各种任务（包括毒性控制和情绪控制）中实现精确的调整范围内的调整。EdiText展示了其在广泛的任务层次上对参考文本进行稳健调整的可控性。', 'title_zh': 'EdiText：基于扩散语言模型的可控粗细粒度文本编辑'}
{'arxiv_id': 'arXiv:2502.19756', 'title': 'PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation', 'authors': 'Nathan Roll', 'link': 'https://arxiv.org/abs/2502.19756', 'abstract': "Large language models (LLMs) showcase increasingly impressive English benchmark scores, however their performance profiles remain inconsistent across multilingual settings. To address this gap, we introduce PolyPrompt, a novel, parameter-efficient framework for enhancing the multilingual capabilities of LLMs. Our method learns a set of trigger tokens for each language through a gradient-based search, identifying the input query's language and selecting the corresponding trigger tokens which are prepended to the prompt during inference. We perform experiments on two ~1 billion parameter models, with evaluations on the global MMLU benchmark across fifteen typologically and resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared to naive and translation-pipeline baselines.", 'abstract_zh': '大型语言模型（LLMs）在英语基准测试中展现出了越来越出色的性能，但在多语言设置中的表现仍存在不一致性。为解决这一问题，我们引入了PolyPrompt，这是一种新颖的、参数高效的框架，用于增强LLMs的多语言能力。我们的方法通过梯度搜索学习每种语言的一组触发令牌，从而识别输入查询的语言，并在推理时选择相应的触发令牌添加到提示中。我们在两个参数量约为10亿的模型上进行了实验，并在包含十五种在类型和资源上都具有多样性的语言的全球MMLU基准测试中进行了评估。与直接翻译基线和翻译管道基线相比，我们的方法在准确率上取得了3.7%-19.9%的提升。', 'title_zh': 'PolyPrompt：利用动态提示生成从多语言语言模型中自动提取知识'}
{'arxiv_id': 'arXiv:2502.19749', 'title': 'Beneath the Surface: How Large Language Models Reflect Hidden Bias', 'authors': 'Jinhao Pan, Chahat Raj, Ziyu Yao, Ziwei Zhu', 'link': 'https://arxiv.org/abs/2502.19749', 'abstract': 'The exceptional performance of Large Language Models (LLMs) often comes with the unintended propagation of social biases embedded in their training data. While existing benchmarks evaluate overt bias through direct term associations between bias concept terms and demographic terms, LLMs have become increasingly adept at avoiding biased responses, creating an illusion of neutrality. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Hidden Bias Benchmark (HBB), a novel dataset designed to assess hidden bias that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response to overt bias, they continue to reinforce biases in nuanced settings. Data, code, and results are available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的出色性能往往伴随着在训练数据中嵌入的社会偏见的无意传播。现有的基准测试主要通过直接关联偏见概念术语和人口统计术语来评估显性偏见，但LLMs已经越来越善于避免表现出偏见，从而产生一种中立性的错觉。然而，这些偏见以更隐秘、上下文中隐藏的形式存在，而传统的基准测试无法捕捉到这些偏见。为此，我们引入了隐蔽偏见基准测试（HBB），这是一个新颖的数据集，旨在评估隐性偏见，这些偏见概念隐藏在实际场景中自然且微妙的框架中。我们分析了六种最先进的LLM，结果显示，在应对显性偏见时，模型减少了偏见，但在细腻的情境中，模型继续强化偏见。相关数据、代码和结果可以在以下链接获取：this https URL。', 'title_zh': '表象之下：大型语言模型如何反映隐藏偏见'}
{'arxiv_id': 'arXiv:2502.19747', 'title': 'HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture', 'authors': 'Taiqiang Wu, Chenchen Ding, Wenyong Zhou, Yuxin Cheng, Xincheng Feng, Shuqi Wang, Chufan Shi, Zhengwu Liu, Ngai Wong', 'link': 'https://arxiv.org/abs/2502.19747', 'abstract': "Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning method to adapt large language models (LLMs) for downstream tasks. In this paper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid compute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and LoRA onto SRAM). To address performance degradation from RRAM's inherent noise, we design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to train a LoRA branch that is both robust and accurate by aligning the training objectives under both ideal and noisy conditions. Experiments finetuning LLaMA 3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning tasks, achieving up to 22.7 improvement in average score while maintaining robustness at various noise levels.", 'abstract_zh': '低秩适应（LoRA）是一种主要的参数高效微调方法，用于将大规模语言模型（LLMs）适应于下游任务。本文首先提出在混合计算-在内存（CIM）架构上部署LoRA微调后的LLMs（即预训练权重置于RRAM，而LoRA置于SRAM）。为了解决RRAM固有噪声带来的性能下降问题，我们设计了一种新的硬件感知低秩适应（HaLoRA）方法，旨在在理想和噪声条件下通过对齐训练目标来训练一个具有鲁棒性和准确性的LoRA分支。实验结果显示，通过fine-tune LLaMA 3.2 1B和3B模型，HaLoRA在多种推理任务中均表现出有效性，平均得分提高了22.7%，同时在不同噪声水平下保持了鲁棒性。', 'title_zh': 'HaLoRA：基于混合计算存储架构的硬件感知低秩适应性方法用于大型语言模型'}
{'arxiv_id': 'arXiv:2502.19737', 'title': 'XCOMPS: A Multilingual Benchmark of Conceptual Minimal Pairs', 'authors': 'Linyang He, Ercong Nie, Sukru Samet Dindar, Arsalan Firoozi, Adrian Florea, Van Nguyen, Corentin Puffay, Riki Shimizu, Haotian Ye, Jonathan Brennan, Helmut Schmid, Hinrich Schütze, Nima Mesgarani', 'link': 'https://arxiv.org/abs/2502.19737', 'abstract': "We introduce XCOMPS in this work, a multilingual conceptual minimal pair dataset covering 17 languages. Using this dataset, we evaluate LLMs' multilingual conceptual understanding through metalinguistic prompting, direct probability measurement, and neurolinguistic probing. By comparing base, instruction-tuned, and knowledge-distilled models, we find that: 1) LLMs exhibit weaker conceptual understanding for low-resource languages, and accuracy varies across languages despite being tested on the same concept sets. 2) LLMs excel at distinguishing concept-property pairs that are visibly different but exhibit a marked performance drop when negative pairs share subtle semantic similarities. 3) Instruction tuning improves performance in concept understanding but does not enhance internal competence; knowledge distillation can enhance internal competence in conceptual understanding for low-resource languages with limited gains in explicit task performance. 4) More morphologically complex languages yield lower concept understanding scores and require deeper layers for conceptual reasoning.", 'abstract_zh': '在本文中，我们介绍了XCOMPS，这是一个包含17种语言的概念最小对立体集。利用这一数据集，我们通过元语言提示、直接概率测量和神经语言探针评估了大语言模型（LLMs）的多语言概念理解能力。通过比较基础模型、指令微调模型和知识精炼模型，我们发现：1）大语言模型在低资源语言上的概念理解能力较弱，即使在相同的概念集上进行测试，不同语言的表现也存在差异。2）大语言模型擅长区分明显不同的概念-属性对，但在负例对具有微妙语义相似性时，其表现会显著下降。3）指令微调能够在概念理解上提高模型的表现，但不会增强其内部能力；知识精炼可以在低资源语言上增强概念理解的内部能力，尽管在显式任务表现上仅有有限的提升。4）形态复杂度更高的语言在概念理解上的得分较低，且需要更深的网络结构来进行概念推理。', 'title_zh': 'XCOMPS：一种概念minimal对的多语言基准测试'}
{'arxiv_id': 'arXiv:2502.19735', 'title': 'R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning', 'authors': 'Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie', 'link': 'https://arxiv.org/abs/2502.19735', 'abstract': 'Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.', 'abstract_zh': '尽管近年来在增强推理的大语言模型（LLMs）如DeepSeek-R1方面取得了突破，但在机器翻译（MT）中加入推理时的推理，即人类翻译者自然使用的结构化、多层次的推理链式思考（CoTs），这一领域尚未得到充分探索。现有的方法要么为特定MT子任务（例如文学翻译）设计固定不变的CoT，要么依赖于与人类推理不一致的合成CoT并采用容易导致灾难性遗忘的监督微调（SFT），这限制了它们对不同翻译场景的适应性。本文引入了一种名为R1-Translator (R1-T1)的新框架，通过强化学习（RL）实现通用MT中的推理时间推理，该框架结合了六个与人工对齐的CoT模式。我们的方法在以下三个方面进行了创新：（1）将基于推理的翻译从特定MT子任务扩展到六种语言和各种任务（例如法律/医疗领域的适应、成语解析）；（2）制定了六个由专家精心编写的CoT模板，这些模板反映了混合人类策略（如上下文感知重写和反向翻译）的特点；（3）通过带有KL约束的奖励实现自演化CoT发现和抗遗忘适应。实验结果表明，在Flores-101测试集上的21种语言和80种翻译方向中显示出稳健的翻译性能改进，尤其是在训练中未见过的15种语言中表现尤为突出，并且相比仅有的SFT，其多语言能力保持了一定的通用性。', 'title_zh': 'R1-T1: 通过推理学习全面激励大型语言模型的翻译能力'}
{'arxiv_id': 'arXiv:2502.19732', 'title': 'Speculative Decoding and Beyond: An In-Depth Review of Techniques', 'authors': 'Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley McDanel, Sai Qian Zhang', 'link': 'https://arxiv.org/abs/2502.19732', 'abstract': 'Sequential dependencies present a fundamental bottleneck in deploying large-scale autoregressive models, particularly for real-time applications. While traditional optimization approaches like pruning and quantization often compromise model quality, recent advances in generation-refinement frameworks demonstrate that this trade-off can be significantly mitigated.\nThis survey presents a comprehensive taxonomy of generation-refinement frameworks, analyzing methods across autoregressive sequence tasks. We categorize methods based on their generation strategies (from simple n-gram prediction to sophisticated draft models) and refinement mechanisms (including single-pass verification and iterative approaches). Through systematic analysis of both algorithmic innovations and system-level implementations, we examine deployment strategies across computing environments and explore applications spanning text, images, and speech generation. This systematic examination of both theoretical frameworks and practical implementations provides a foundation for future research in efficient autoregressive decoding.', 'abstract_zh': '大规模自回归模型在部署过程中，顺序依赖性构成了一个基本的瓶颈，尤其是在实时应用中。尽管传统的优化方法如剪枝和量化经常 sacrifices 模型质量，但最近生成-精炼框架的进展表明，这种权衡可以显著缓解。\n\n本文综述了生成-精炼框架，并对其进行全面分类，分析了其在自回归序列任务中的方法。我们根据生成策略（从简单的 n-克什预测到复杂的草稿模型）和精炼机制（包括单次通过验证和迭代方法）对方法进行了分类。通过对算法创新和系统级实现的系统性分析，我们考察了不同计算环境下的部署策略，并探讨了涉及文本、图像和语音生成的应用。这种系统的框架和实际实现的分析为未来高效自回归解码的研究奠定了基础。', 'title_zh': 'speculative 解码及其超越：技术综述\n\n注：在学术翻译中，我们尽量保持术语的专业性和准确性。"speculative decoding" 通常指的是预测性解码，在机器学习和计算语言学中常用，这里的翻译保持了专业性。"An In-Depth Review of Techniques" 直译为“技术深度综述”，但在学术论文标题中可以更简洁地表达为“技术综述”。'}
{'arxiv_id': 'arXiv:2502.19731', 'title': "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", 'authors': 'Mian Zhang, Shaun M. Eack, Zhiyu Zoey Chen', 'link': 'https://arxiv.org/abs/2502.19731', 'abstract': "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: this https URL.", 'abstract_zh': '将以下论文内容或标题翻译成中文，符合学术规范：\n\n将大型语言模型（LLMs）应用于心理辅导是一个新兴且有意义的方法，这主要是由于患者需求与可获得的心理健康支持之间的巨大差距所驱动。然而，当前的LLMs在为客户提供有效回应方面普遍存在困难，主要原因在于缺乏高质量的真实心理辅导数据的监督，这些数据的内容通常由于涉及客户隐私而难以获取。此外，可用会话中治疗师回应的质量因治疗师的专业培训和经验不同而有很大差异。治疗师回应质量的评估仍然是一个开放性挑战。在本项工作中，我们通过首先提出一套专业且全面的原则来评估治疗师对客户言论的回应，以应对这些挑战。利用这些原则，我们创建了一个偏好数据集，即PsychoCounsel-Preference，该数据集包含36000个高质量的偏好对比对。该数据集与专业心理治疗师的偏好相一致，为评估和提高LLMs在心理辅导中的表现提供了坚实的基础。奖励建模和偏好学习的实验结果表明，PsychoCounsel-Preference是一款优秀的资源，可帮助LLMs学习在咨询会话中回应客户的必备技能。我们最匹配的模型PsychoCounsel-Llama3-8B在与GPT-4o的竞争中取得了令人印象深刻的胜率87%。我们将在以下链接发布PsychoCounsel-Preference、PsychoCounsel-Llama3-8B和奖励模型PsychoCounsel Llama3-8B-Reward，以促进使用LLMs进行心理辅导的研究：[此处应填写URL]。', 'title_zh': '偏好学习解锁了大规模语言模型的心理咨询技能'}
{'arxiv_id': 'arXiv:2502.19723', 'title': 'CNsum:Automatic Summarization for Chinese News Text', 'authors': 'Yu Zhao, Songping Huang, Dongsheng Zhou, Zhaoyun Ding, Fei Wang, Aixin Nian', 'link': 'https://arxiv.org/abs/2502.19723', 'abstract': 'Obtaining valuable information from massive data efficiently has become our research goal in the era of Big Data. Text summarization technology has been continuously developed to meet this demand. Recent work has also shown that transformer-based pre-trained language models have achieved great success on various tasks in Natural Language Processing (NLP). Aiming at the problem of Chinese news text summary generation and the application of Transformer structure on Chinese, this paper proposes a Chinese news text summarization model (CNsum) based on Transformer structure, and tests it on Chinese datasets such as THUCNews. The results of the conducted experiments show that CNsum achieves better ROUGE score than the baseline models, which verifies the outperformance of the model.', 'abstract_zh': '在大数据时代，高效地从大规模数据中提取有价值的信息已成为我们的研究目标。文本总结技术不断发展，以满足这一需求。最近的研究还表明，基于变压器的预训练语言模型在自然语言处理（NLP）的各种任务中取得了巨大成功。针对中文新闻文本总结生成问题以及变压器结构在中文中的应用，本文提出了一种基于变压器结构的中文新闻文本摘要模型（CNsum），并在THUCNews等中文数据集上进行了测试。实验结果表明，CNsum的ROUGE评分优于基准模型，验证了该模型的优越性。', 'title_zh': 'CNsum：中文新闻文本的自动摘要生成'}
{'arxiv_id': 'arXiv:2502.19722', 'title': 'Few-Shot Multilingual Open-Domain QA from 5 Examples', 'authors': 'Fan Jiang, Tom Drummond, Trevor Cohn', 'link': 'https://arxiv.org/abs/2502.19722', 'abstract': 'Recent approaches to multilingual open-domain question answering (MLODQA) have achieved promising results given abundant language-specific training data. However, the considerable annotation cost limits the application of these methods for underrepresented languages. We introduce a \\emph{few-shot learning} approach to synthesise large-scale multilingual data from large language models (LLMs). Our method begins with large-scale self-supervised pre-training using WikiData, followed by training on high-quality synthetic multilingual data generated by prompting LLMs with few-shot supervision. The final model, \\textsc{FsModQA}, significantly outperforms existing few-shot and supervised baselines in MLODQA and cross-lingual and monolingual retrieval. We further show our method can be extended for effective zero-shot adaptation to new languages through a \\emph{cross-lingual prompting} strategy with only English-supervised data, making it a general and applicable solution for MLODQA tasks without costly large-scale annotation.', 'abstract_zh': '近年来，面向多语言开放领域问答（MLODQA）的方法在充足的特定语言训练数据的条件下取得了令人鼓舞的结果。然而，显著的标注成本限制了这些方法在欠代表语言中的应用。我们提出了一种基于少样本学习的方法，利用大型语言模型（LLMs）合成大规模多语言数据。该方法首先使用WikiData进行大规模自我监督预训练，随后利用少样本监督提示LLMs生成高质量的多语言合成数据进行训练。最终模型FsModQA在MLODQA和跨语言及单语言检索任务中显著优于现有的少样本和监督基准。我们进一步证明，通过仅使用英语监督数据的跨语言提示策略，该方法可以有效零样本适应到新语言，从而提供了一种适用于MLODQA任务的通用且可实现的解决方案，而无需昂贵的大规模标注成本。', 'title_zh': '从5个示例进行少样本多语言开放域问答'}
{'arxiv_id': 'arXiv:2502.19721', 'title': 'Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs', 'authors': 'Hannah Cyberey, Yangfeng Ji, David Evans', 'link': 'https://arxiv.org/abs/2502.19721', 'abstract': 'Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model\'s representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.', 'abstract_zh': '大型语言模型（LLMs）已知会延续刻板印象并表现出偏见。为减轻这些偏见可能带来的潜在危害，已经提出了多种策略，但大多数研究将LLMs中的偏见视为黑盒问题，未考虑模型内部概念的表示方式。我们借鉴表示工程中的技术，研究LLMs中“性别”概念的表示方式。我们提出了一种新方法，通过概率加权提取概念表示，无需标注数据，并高效地选择一个引导向量以度量和操控模型表示。此外，我们还介绍了一种基于投影的方法，使模型预测的操控更加精确，并展示了其在减轻LLMs中性别偏见方面的有效性。', 'title_zh': '感知和引导刻板印象：提取和应用性别表示向量在大语言模型中的应用'}
{'arxiv_id': 'arXiv:2502.19684', 'title': 'GRACE: A Granular Benchmark for Evaluating Model Calibration against Human Calibration', 'authors': 'Yoo Yeon Sung, Eve Fleisig, Yu Hou, Ishan Upadhyay, Jordan Lee Boyd-Graber', 'link': 'https://arxiv.org/abs/2502.19684', 'abstract': "Language models are often miscalibrated, leading to confidently incorrect answers. We introduce GRACE, a benchmark for language model calibration that incorporates comparison with human calibration. GRACE consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed. This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers. After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence. We propose a metric, CalScore, that uses GRACE to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less accurate than models, humans are generally better calibrated. Since state-of-the-art models struggle on GRACE, it effectively evaluates progress on improving model calibration.", 'abstract_zh': '语言模型经常存在校准问题，导致其给出自信而错误的答案。我们引入了GRACE，这是一个结合了与人类校准对比的语言模型校准基准。GRACE包含问题-答案对，在每一个问题中，一系列线索按照难度逐渐递减的方式逐步呈现，最终指向同一个答案；模型需要尽可能早且准确地回答问题，随着线索的揭示。这种设置允许基于模型何时、多准确以及多自信地回答问题来细化测量模型的校准情况。在收集这些问题后，我们组织了人类与模型之间的实时比赛，收集了1,749个数据点，包括人类和模型团队的时间、准确性和自信程度。我们提出了一个称为CalScore的指标，使用GRACE来分析模型校准误差，并识别与人类行为不同的模型校准偏差。我们发现，尽管人类的准确性低于模型，但人类通常更好地校准。由于最先进的模型在GRACE上遇到困难，因此GRACE有效地评估了提高模型校准的进步。', 'title_zh': 'GRACE：一个粒度基准，用于评估模型校准与人类校准的对比'}
{'arxiv_id': 'arXiv:2502.19669', 'title': 'Investigating Neurons and Heads in Transformer-based LLMs for Typographical Errors', 'authors': 'Kohei Tsuji, Tatsuya Hiraoka, Yuchang Cheng, Eiji Aramaki, Tomoya Iwakura', 'link': 'https://arxiv.org/abs/2502.19669', 'abstract': 'This paper investigates how LLMs encode inputs with typos. We hypothesize that specific neurons and attention heads recognize typos and fix them internally using local and global contexts. We introduce a method to identify typo neurons and typo heads that work actively when inputs contain typos. Our experimental results suggest the following: 1) LLMs can fix typos with local contexts when the typo neurons in either the early or late layers are activated, even if those in the other are not. 2) Typo neurons in the middle layers are responsible for the core of typo-fixing with global contexts. 3) Typo heads fix typos by widely considering the context not focusing on specific tokens. 4) Typo neurons and typo heads work not only for typo-fixing but also for understanding general contexts.', 'abstract_zh': '本文探讨了大语言模型（LLMs）如何编码包含拼写错误的输入。我们假设特定的神经元和注意力头能够识别拼写错误，并通过局部和全局上下文在内部进行修正。我们提出了一种方法来识别在输入包含拼写错误时积极参与的拼写错误相关的神经元和注意力头。实验结果表明：1）当早期或晚期层中的拼写错误相关的神经元被激活时，LLMs能够在局部上下文中修正拼写错误，即使另一个层中的拼写错误相关的神经元未被激活。2）中期层中的拼写错误相关的神经元在结合全局上下文的情况下，对拼写错误的修正起核心作用。3）拼写错误相关的注意力头通过广泛考虑上下文而非关注特定的标记来修正拼写错误。4）拼写错误相关的神经元和注意力头不仅用于修正拼写错误，还在理解一般上下文中发挥作用。', 'title_zh': '基于Transformer的大型语言模型中用于研究typographical错误的神经元和组件'}
{'arxiv_id': 'arXiv:2502.19655', 'title': 'Med-RLVR: Emerging Medical Reasoning from a 3B base model via reinforcement Learning', 'authors': 'Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, Hoifung Poon', 'link': 'https://arxiv.org/abs/2502.19655', 'abstract': 'Reinforcement learning from verifiable rewards (RLVR) has recently gained attention for its ability to elicit self-evolved reasoning capabilitie from base language models without explicit reasoning supervisions, as demonstrated by DeepSeek-R1. While prior work on RLVR has primarily focused on mathematical and coding domains, its applicability to other tasks and domains remains unexplored. In this work, we investigate whether medical reasoning can emerge from RLVR. We introduce Med-RLVR as an initial study of RLVR in the medical domain leveraging medical multiple-choice question answering (MCQA) data as verifiable labels. Our results demonstrate that RLVR is not only effective for math and coding but also extends successfully to medical question answering. Notably, Med-RLVR achieves performance comparable to traditional supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization, with an 8-point accuracy gain. Further analysis of training dynamics reveals that, with no explicit reasoning supervision, reasoning emerges from the 3B-parameter base model. These findings underscore the potential of RLVR in domains beyond math and coding, opening new avenues for its application in knowledge-intensive fields such as medicine.', 'abstract_zh': '验证奖励的强化学习（RLVR）最近因其能够从基础语言模型中激发自我进化的推理能力而引起关注，而不需要显式的推理监督，如DeepSeek-R1所证明的那样。尽管RLVR的早期工作主要集中在数学和编程领域，但其在其他任务和领域的应用尚未得到探索。本研究旨在探讨RLVR是否能在医学推理中发挥作用。我们引入了Med-RLVR作为在医学领域利用医学选择题答案（MCQA）数据作为验证标签的RLVR初步研究。我们的结果表明，除了数学和编程外，RLVR也能成功应用于医学问答任务。值得注意的是，Med-RLVR在同分布任务上的性能与传统的监督微调（SFT）相当，但其在异分布泛化方面的表现显著提高，准确率提高了8个百分点。进一步的训练动力学分析表明，在没有任何显式推理监督的情况下，推理能力从3B参数的基础模型中自发产生。这些发现表明，RLVR在数学和编程领域之外也具有潜在的应用价值，为在知识密集型领域如医学中应用RLVR开辟了新的途径。', 'title_zh': 'Med-RLVR：通过强化学习从3B基础模型中涌现医学推理'}
{'arxiv_id': 'arXiv:2502.19622', 'title': "Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's Mathematical Reasoning", 'authors': 'Yanan Chen, Ali Pesaranghader, Tanmana Sadhu', 'link': 'https://arxiv.org/abs/2502.19622', 'abstract': "Recent advances in Large Language Models (LLMs) have raised interest in their formal reasoning capabilities, particularly in mathematics. While closed LLMs like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains unclear whether small to medium-sized open LLMs can achieve similar performance, questioning their reliability. To close this gap, we propose a post-training approach leveraging a mixture of opinions (MoO) from weaker ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that, each post-training sample is augmented with Chain-of-Thought (CoT) reasoning steps and answers from ancillary LLMs, enabling the main LLM to learn from diverse perspectives. We compare MoO with standard supervised fine-tuning (SFT), few-shot prompting, and the Mixture of Agents (MoA) method on mathematical reasoning benchmarks. Our results show that incorporating weaker LLMs' opinions improves mathematical reasoning by an average of 5%, highlighting the value of diverse perspectives in reasoning tasks.", 'abstract_zh': '近年来，大型语言模型（LLMs）的发展引发了对其形式推理能力的兴趣，特别是在数学领域的表现。虽然像GPT-4这样的闭源LLM在数学基准测试中表现出色，例如GSM8K，但尚不清楚中小型的开源LLM能否达到类似的成绩，从而质疑它们的可靠性。为了解决这一问题，我们提出了一种后训练方法，利用较弱的辅助LLM提供的多种意见（MoO）来增强相对较强的LLM的推理能力。在这一方法中，每个后训练样本都会增强链式推理（CoT）步骤和辅助LLM的回答，从而使主要LLM能够从多种视角中学习。我们使用数学推理基准测试将MoO与标准的监督微调（SFT）、少样本提示（Few-shot prompting）和代理混合（MoA）方法进行了比较。结果显示，纳入较弱LLM的意见在数学推理任务中平均提高了5%的性能，突出了多样视角在推理任务中的价值。', 'title_zh': '弱语言模型的意见同样重要：混合意见提升语言模型的数学推理能力'}
{'arxiv_id': 'arXiv:2502.19614', 'title': 'Is Your Paper Being Reviewed by an LLM? A New Benchmark Dataset and Approach for Detecting AI Text in Peer Review', 'authors': 'Sungduk Yu, Man Luo, Avinash Madusu, Vasudev Lal, Phillip Howard', 'link': 'https://arxiv.org/abs/2502.19614', 'abstract': 'Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review.\nTo address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Motivated by the shortcomings of existing methods, we propose a new detection approach which surpasses existing methods in the identification of AI written peer reviews. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI.', 'abstract_zh': '同行评审是确保发表的科学研究质量的最关键过程之一。这一过程的信任基础在于，相关领域的专家能够仔细考虑提交发表的文章优点。随着大型语言模型（LLMs）的迅猛发展，同行评审过程中一个新的风险是，不负责任的审稿人可能会依赖LLMs来完成耗时的评审文章工作。然而，目前缺乏针对同行评审领域中AI文本可检测性的基准资源。\n\n为解决这一不足，我们建立了一个包含788,984篇由AI撰写并配有相应人类评审的同行评审文章的综合数据集，涵盖了过去8年提交给两大领先AI研究会议（ICLR和NeurIPS）的论文。我们利用这一新资源评估了18种现有AI文本检测算法的能力，以识别出由人类和不同先进LLMs撰写的同行评审文章。受到现有方法不足的启发，我们提出了一种新的检测方法，在识别AI撰写的同行评审文章方面超越了现有的方法。我们的研究揭示了在个体同行评审级别上识别AI生成文本的难度，突显了迫切需要开发新的工具和方法来检测这种不道德使用生成型AI的行为。', 'title_zh': '你的论文是否正被LLM审核？一个新的基准数据集和检测同行评审中AI文本的方法'}
{'arxiv_id': 'arXiv:2502.19612', 'title': 'Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization', 'authors': 'Anwar Hossain Zahid, Monoshi Kumar Roy, Swarna Das', 'link': 'https://arxiv.org/abs/2502.19612', 'abstract': 'The proliferation of hate speech on social media is one of the serious issues that is bringing huge impacts to society: an escalation of violence, discrimination, and social fragmentation. The problem of detecting hate speech is intrinsically multifaceted due to cultural, linguistic, and contextual complexities and adversarial manipulations. In this study, we systematically investigate the performance of LLMs on detecting hate speech across multilingual datasets and diverse geographic contexts. Our work presents a new evaluation framework in three dimensions: binary classification of hate speech, geography-aware contextual detection, and robustness to adversarially generated text. Using a dataset of 1,000 comments from five diverse regions, we evaluate three state-of-the-art LLMs: Llama2 (13b), Codellama (7b), and DeepSeekCoder (6.7b). Codellama had the best binary classification recall with 70.6% and an F1-score of 52.18%, whereas DeepSeekCoder had the best performance in geographic sensitivity, correctly detecting 63 out of 265 locations. The tests for adversarial robustness also showed significant weaknesses; Llama2 misclassified 62.5% of manipulated samples. These results bring to light the trade-offs between accuracy, contextual understanding, and robustness in the current versions of LLMs. This work has thus set the stage for developing contextually aware, multilingual hate speech detection systems by underlining key strengths and limitations, therefore offering actionable insights for future research and real-world applications.', 'abstract_zh': '社交媒体上仇恨言论的泛滥是社会面临的一个严重问题，这对社会产生了巨大影响：加剧暴力、歧视和社会分裂。由于文化、语言和上下文的复杂性以及对抗性操控，识别仇恨言论的问题根源本身就是多维度的。本研究系统地探讨了LLM在多语言数据集和多样化地理背景下检测仇恨言论的表现。我们的工作提出了一种新的评估框架，包括仇恨言论的二分类、地理感知上下文检测以及对抗生成文本的鲁棒性。\n\n使用来自五个不同地区的1,000条评论的数据集，我们评估了三种最新的LLM：Llama2（13b）、Codellama（7b）和DeepSeekCoder（6.7b）。Codellama在二分类召回率方面表现最佳，达到了70.6%，F1分数为52.18%；而DeepSeekCoder在地理敏感性检测方面的表现最佳，正确检测了265个位置中的63个。针对对抗鲁棒性的测试也表明存在显著的弱点；Llama2误分类了62.5%的被操纵样本。这些结果揭示了当前LLM版本之间准确度、上下文理解与鲁棒性的权衡。因此，这项工作为进一步开发具备上下文感知能力的多语言仇恨言论检测系统奠定了基础，指出了关键的优势与局限，为未来的研究和实际应用提供了可操作的见解。', 'title_zh': '使用大规模语言模型和地理语境化评价仇恨言论检测 effetiveness'}
{'arxiv_id': 'arXiv:2502.19607', 'title': 'Revisiting Word Embeddings in the LLM Era', 'authors': 'Yash Mahajan, Matthew Freestone, Sathyanarayanan Aakur, Santu Karmaker', 'link': 'https://arxiv.org/abs/2502.19607', 'abstract': 'Large Language Models (LLMs) have recently shown remarkable advancement in various NLP tasks. As such, a popular trend has emerged lately where NLP researchers extract word/sentence/document embeddings from these large decoder-only models and use them for various inference tasks with promising results. However, it is still unclear whether the performance improvement of LLM-induced embeddings is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This is the central question we investigate in the paper by systematically comparing classical decontextualized and contextualized word embeddings with the same for LLM-induced embeddings. Our results show that LLMs cluster semantically related words more tightly and perform better on analogy tasks in decontextualized settings. However, in contextualized settings, classical models like SimCSE often outperform LLMs in sentence-level similarity assessment tasks, highlighting their continued relevance for fine-grained semantics.', 'abstract_zh': '大规模语言模型（LLMs）近年来在各种自然语言处理（NLP）任务中展现出了显著的进步。因此，最近出现了一种流行的趋势，NLP研究者从这些大型的解码器模型中提取词/句子/文档嵌入，并将它们用于各种推理任务，取得了令人鼓舞的效果。然而，目前尚不清楚LLM生成的嵌入性能提升仅仅是由于规模增大，还是因为它们生成的底层嵌入显著不同于像Word2Vec、GloVe、Sentence-BERT（SBERT）或Universal Sentence Encoder（USE）这样的经典编码模型。本文通过系统比较经典去语境化和上下文嵌入与LLM生成的嵌入，旨在探究这一核心问题。我们的研究表明，在去语境化设置中，LLM能够更紧密地聚集语义相关词，并在类比任务上表现更好。然而，在上下文化设置中，如SimCSE这样的经典模型在句子相似性评估任务上常常优于LLM，这突显了它们在细微语义层面的持续相关性。', 'title_zh': '在大语言模型时代重新审视词嵌入'}
{'arxiv_id': 'arXiv:2502.19590', 'title': 'A City of Millions: Mapping Literary Social Networks At Scale', 'authors': 'Sil Hamilton, Rebecca M. M. Hicke, David Mimno, Matthew Wilkens', 'link': 'https://arxiv.org/abs/2502.19590', 'abstract': 'We release 70,509 high-quality social networks extracted from multilingual fiction and nonfiction narratives. We additionally provide metadata for ~30,000 of these texts (73% nonfiction and 27% fiction) written between 1800 and 1999 in 58 languages. This dataset provides information on historical social worlds at an unprecedented scale, including data for 1,192,855 individuals in 2,805,482 pair-wise relationships annotated for affinity and relationship type. We achieve this scale by automating previously manual methods of extracting social networks; specifically, we adapt an existing annotation task as a language model prompt, ensuring consistency at scale with the use of structured output. This dataset provides an unprecedented resource for the humanities and social sciences by providing data on cognitive models of social realities.', 'abstract_zh': '我们发布了70,509个高质量的社会网络，这些网络是从多语言的小说和非小说叙述中提取出来的。此外，我们还为其中约30,000个文本提供了元数据（73%为非小说，27%为小说），这些文本涵盖了1800年至1999年间58种语言的文本。该数据集提供了前所未有的历史社会世界信息，包括1,192,855名个体在2,805,482对关系中的亲和力和关系类型的数据。我们通过自动化以前手动的社会网络提取方法实现了这一规模；具体来说，我们将现有的标注任务作为语言模型提示进行调整，并通过结构化输出确保规模的一致性。该数据集为人文科学和社会科学提供了前所未有的资源，因为它提供了关于社会现实认知模型的数据。', 'title_zh': '数百万之城：大规模绘制文学社交网络'}
{'arxiv_id': 'arXiv:2502.19587', 'title': 'NeoBERT: A Next-Generation BERT', 'authors': 'Lola Le Breton, Quentin Fournier, Mariam El Mezouar, Sarath Chandar', 'link': 'https://arxiv.org/abs/2502.19587', 'abstract': 'Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.', 'abstract_zh': '近期在架构、预训练和微调方面的创新显著提升了大型自回归语言模型（如LLaMA和DeepSeek）的上下文学习和推理能力。相比之下，尽管BERT和RoBERTa在许多下游自然语言处理（NLP）应用中担任基础角色，但它们在此方面的进展并不如前者显著。为缩小这一差距，我们提出了一种新型编码器NeoBERT，通过整合最先进的架构发展、现代数据和优化的预训练方法重新定义双向模型的能力。NeoBERT旨在轻松集成：它作为现有多层基础模型的即插即用替代品，依赖于最优的深度-宽度比例，并支持4,096个令牌的超长上下文。尽管参数量仅为250 million，但在大规模MTEB基准测试中，它仍取得了最佳结果，在相同的微调条件下，优于BERT large、RoBERTa large、NomicBERT和ModernBERT。此外，我们严格评估了每个改进对GLUE的影响，并为MTEB设定了统一的微调和评估框架。我们公开了所有代码、数据、检查点和训练脚本，以加速研究和实际应用。', 'title_zh': '新一代 BERT：NeoBERT'}
{'arxiv_id': 'arXiv:2502.19582', 'title': 'Where Are We? Evaluating LLM Performance on African Languages', 'authors': 'Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed', 'link': 'https://arxiv.org/abs/2502.19582', 'abstract': "Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.", 'abstract_zh': '非洲丰富的语言遗产在自然语言处理（NLP）领域中仍然广受忽视，这主要是由于历史上倾向于外国语言的政策所导致，从而造成了数据上的显著不平等。本文将非洲的语言景观理论洞察与对Sahara基准的实证评估相结合，Sahara基准是从大规模、公开可访问的数据集中精心收集起来，涵盖了非洲语言的多样性。通过系统地评估领先的大规模语言模型（LLMs）在Sahara上的表现，我们展示了政策驱动的数据差异如何直接影响非洲语言模型的有效性。我们的研究发现，虽然少数语言表现尚可，但许多土著语言由于数据稀少而仍然处于边缘化地位。基于这些见解，我们提出了针对政策改革和包容性数据实践的具体建议。总体而言，我们的工作强调了迫切需要采取一种双重方法——将理论理解与实证评估相结合——以促进非洲社区的语境多样性在人工智能中的发展。', 'title_zh': '我们在何处？评估大语言模型在非洲语言上的性能'}
{'arxiv_id': 'arXiv:2502.19573', 'title': 'Do Large Language Models Know How Much They Know?', 'authors': 'Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani, Sarath Chandar', 'link': 'https://arxiv.org/abs/2502.19573', 'abstract': "Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.", 'abstract_zh': '大规模语言模型（LLMs）已经发展成为高度有效的系统，并且越来越多地被集成到各种应用中。然而，它们部署的快速步伐已经超过了对其内部机制进行全面理解的程度，以及明确了其能力和限制。智能系统的一个理想属性是能够识别自己知识的范围。为了调查LLMs是否具备这一特性，我们开发了一个基准测试，旨在挑战这些模型在特定主题上列出它们掌握的所有信息。这个基准测试评估模型是回忆过多、不足还是恰当的信息量，从而表明它们对自己知识的认知程度。我们的研究表明，在有足够的规模下，所有测试的LLMs都对特定主题的知识范围有所理解。虽然不同的模型架构显示出这一能力出现的速率各异，但结果表明，知识意识可能是LLMs的一个可泛化的属性。需要进一步的研究来验证这一潜力并完全阐明其背后的工作机制。', 'title_zh': '大型语言模型知道它们知道多少吗？'}
{'arxiv_id': 'arXiv:2502.19559', 'title': 'Stay Focused: Problem Drift in Multi-Agent Debate', 'authors': 'Jonas Becker, Lars Benedikt Kaesberg, Andreas Stephan, Jan Philip Wahle, Terry Ruas, Bela Gipp', 'link': 'https://arxiv.org/abs/2502.19559', 'abstract': 'Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations, particularly when scaling them to longer reasoning chains. In this study, we unveil a new issue of multi-agent debate: discussions drift away from the initial problem over multiple turns. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, we perform a human study with eight experts on discussions suffering from problem drift, who find the most common issues are a lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To systematically address the issue of problem drift, we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of problem drift cases. Our study can be seen as a first step to understanding a key limitation of multi-agent debate, highlighting pathways for improving their effectiveness in the future.', 'abstract_zh': '多智能体辩论——多个大型语言模型在轮换交互中讨论问题——已被证明在解决知识和推理任务方面具有潜力。然而，在将这些方法扩展到更长的推理链时，它们显示出一些限制。在此研究中，我们揭示了一个新的多智能体辩论问题：讨论会在多个回合后偏离初始问题。我们将这一现象定义为问题漂移，并通过十个任务（即三项生成性任务、三项知识性任务、三项推理性任务和一项指令遵循任务）来量化其存在。为了找出这个问题的原因，我们在八位专家的指导下进行了一项研究，这些专家发现讨论中问题漂移的主要问题是缺乏进展（占35%的情况）、低质量反馈（占26%的情况）和缺乏清晰性（占25%的情况）。为了系统地解决问题漂移的问题，我们提出了一种基于LLM-as-a-judge的方法（DRIFTJudge），在测试时检测问题漂移。我们进一步提出了一种方法DRIFTPolicy，有助于减轻31%的问题漂移情况。我们的研究可以被视为理解多智能体辩论的关键局限性的一个初步步骤，并强调了未来提高其有效性的方法。', 'title_zh': '保持专注：多智能体辩论中的问题漂移'}
{'arxiv_id': 'arXiv:2502.19557', 'title': 'Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?', 'authors': 'Yudi Zhang, Lu Wang, Meng Fang, Yali Du, Chenghua Huang, Jun Wang, Qingwei Lin, Mykola Pechenizkiy, Dongmei Zhang, Saravan Rajmohan, Qi Zhang', 'link': 'https://arxiv.org/abs/2502.19557', 'abstract': "Distilling large language models (LLMs) typically involves transferring the teacher model's responses through supervised fine-tuning (SFT). However, this approach neglects the potential to distill both data (output content) and reward signals (quality evaluations). Extracting reliable reward signals directly from teacher models is challenging, as LLMs are optimized for generation rather than evaluation, often resulting in biased or inconsistent assessments. To address this limitation, we propose a novel distillation pipeline that transfers both responses and rewards. Our method generates pseudo-rewards through a self-supervised mechanism that leverages the inherent structure of both teacher and student responses, enabling reward learning without explicit external evaluation. The reward model subsequently guides reinforcement learning (RL), allowing iterative refinement of the student model after an SFT warm-up phase. Experiments on GSM8K and MMLU-PRO demonstrate that our method consistently outperforms traditional SFT-based approaches, enabling student models to surpass the performance of their teachers. This work highlights the potential for scalable, efficient distillation through structured self-supervised reward learning, reducing dependence on external reward supervision.", 'abstract_zh': '将下面的论文内容或标题翻译成中文，保持学术规范：\n\n大型语言模型（LLMs）的蒸馏通常涉及通过监督微调（SFT）将教师模型的响应传递过来。然而，这种方法忽略了可以从数据（输出内容）和奖励信号（质量评估）中进行蒸馏的可能性。直接从教师模型中提取可靠的奖励信号具有挑战性，因为LLMs是为生成优化而不是评估优化，这常常导致偏差或不一致的评估。为了解决这一局限性，我们提出了一种新的蒸馏管道，该管道不仅可以传递响应，还可以传递奖励。我们的方法通过利用教师和学生响应的内在结构，产生伪奖励，从而以自我监督的方式实现奖励学习，而无需显式的外部评估。随后，奖励模型指导强化学习（RL），在SFT预热阶段之后允许学生模型的逐步优化。在GSM8K和MMLU-PRO上的实验表明，我们的方法始终优于传统的基于SFT的方案，使学生模型能够超越教师模型的表现。这项工作强调了通过结构化的自我监督奖励学习实现可扩展、高效的蒸馏的潜力，从而减少对外部奖励监督的依赖。', 'title_zh': '不仅提炼数据，也要提炼奖励：较小语言模型能否超越较大模型？'}
{'arxiv_id': 'arXiv:2502.19548', 'title': 'When Large Language Models Meet Speech: A Survey on Integration Approaches', 'authors': 'Zhengdong Yang, Shuichiro Shimizu, Yahan Yu, Chenhui Chu', 'link': 'https://arxiv.org/abs/2502.19548', 'abstract': 'Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for', 'abstract_zh': '近年来，大规模语言模型（LLMs）的发展激发了将其应用扩展到文字任务之外的兴趣。大量研究探索了将其他模态与LLMs结合，特别是在语音模态方面，因为语音与文字之间有着天然的联系。本文概述了将语音与LLMs结合的研究，根据不同方法论将其分类为三大类：基于文本的方法、基于潜在表示的方法以及基于音频令牌的方法。我们还展示了这些方法在各种语音相关应用中的应用，并指出了该领域面临的挑战，旨在为相关研究提供灵感。', 'title_zh': '当大型语言模型遇上语音：集成方法综述'}
{'arxiv_id': 'arXiv:2502.19545', 'title': 'Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents', 'authors': 'Ashley Lewis, Michael White, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang', 'link': 'https://arxiv.org/abs/2502.19545', 'abstract': 'The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information-and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models\' outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized "I don\'t know" responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.', 'abstract_zh': '将大型语言模型（LLMs）部署到客户服务中受到生成虚假信息的幻觉和专有模型成本高的限制。为解决这些挑战，我们提出了一种检索增强的问答（QA）流水线，并探讨了如何在人工输入和自动化之间进行平衡。利用关于三星智能电视用户手册的问题数据集，我们展示了由LLMs生成的合成数据在减少微调模型中的幻觉方面优于众包数据。我们还将自我训练（在其自身输出上微调模型）与知识蒸馏（在其更强模型的输出上进行微调，例如GPT-4o）进行了比较，并发现自我训练在减少幻觉方面达到了可相比的效果。我们推测这一意外发现可能是由于知识蒸馏情况下增强了的暴露偏差问题所致，并通过事后分析支持了这一推测。我们还通过上下文相关的“不知道”回答来提高对无法回答的问题和检索失败的鲁棒性。这些发现表明，可以使用合成数据和开源模型的自我训练构建可扩展且成本效率高的QA系统，从而减少对专有工具或昂贵的人工标注数据的依赖。', 'title_zh': '小模型的一大优势：知识蒸馏与自我训练在减少问答代理幻觉中的比较'}
{'arxiv_id': 'arXiv:2502.19529', 'title': 'Cognitive networks highlight differences and similarities in the STEM mindsets of human and LLM-simulated trainees, experts and academics', 'authors': 'Edith Haim, Lars van den Bergh, Cynthia S. Q. Siew, Yoed N. Kenett, Daniele Marinazzo, Massimo Stella', 'link': 'https://arxiv.org/abs/2502.19529', 'abstract': 'Understanding attitudes towards STEM means quantifying the cognitive and emotional ways in which individuals, and potentially large language models too, conceptualise such subjects. This study uses behavioural forma mentis networks (BFMNs) to investigate the STEM-focused mindset, i.e. ways of associating and perceiving ideas, of 177 human participants and 177 artificial humans simulated by GPT-3.5. Participants were split in 3 groups - trainees, experts and academics - to compare the influence of expertise level on their mindset. The results revealed that human forma mentis networks exhibited significantly higher clustering coefficients compared to GPT-3.5, indicating that human mindsets displayed a tendency to form and close triads of conceptual associations while recollecting STEM ideas. Human experts, in particular, demonstrated robust clustering coefficients, reflecting better integration of STEM concepts into their cognitive networks. In contrast, GPT-3.5 produced sparser mindsets. Furthermore, both human and GPT mindsets framed mathematics in neutral or positive terms, differently from STEM high schoolers, researchers and other large language models sampled in other works. This research contributes to understanding how mindset structure can provide cognitive insights about memory structure and machine limitations.', 'abstract_zh': '理解人们对STEM的态度需要量化个体，甚至大型语言模型在认知和情感层面是如何构想这类学科的。本研究使用行为意图网络（BFMN）来探究177名人类参与者和通过GPT-3.5模拟的177个人工智能个体的STEM导向思维模式，即他们关联和感知观念的方式。参与者被分为三组：学员、专家和学者，以比较其专业水平对思维模式的影响。研究结果表明，人类的行为意图网络的聚类系数明显高于GPT-3.5，表明人类的思维模式在回忆STEM概念时倾向于形成紧密的三元概念关联。尤其是专家级别的参与者，显示了更强的聚类系数，反映出其认知网络中STEM概念的更好整合。相比之下，GPT-3.5产生的思维模式较为稀疏。此外，无论是人类还是GPT的思维模式都以中立或积极的立场来框架数学概念，这与高中的STEM学生、研究人员以及其它在其他研究中取样的大型语言模型有所不同。这项研究有助于我们理解思维模式结构如何提供关于记忆结构和机器限制的认知见解。', 'title_zh': '认知网络突出了人类和模拟的LLM训练生、专家和学者在STEM思维模式上的差异与相似性'}
{'arxiv_id': 'arXiv:2502.20383', 'title': 'Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis', 'authors': 'Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen', 'link': 'https://arxiv.org/abs/2502.20383', 'abstract': 'Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.', 'abstract_zh': '最近在Web AI代理方面的进展展示了其在处理复杂网络导航任务方面的显著能力。然而，新兴的研究表明，这些代理相较独立的大规模语言模型（LLMs）更加脆弱，尽管两者都是基于相同的安全对齐模型构建的。鉴于Web AI代理相较于独立的大规模语言模型具有更大的灵活性，这可能会使它们暴露在更广泛的攻击性用户输入之下，因此这种差异尤其令人担忧。为了解决这些问题，本研究调查了导致Web AI代理脆弱性的根本因素。值得注意的是，这种差异源于Web AI代理和独立的大规模语言模型之间的多方面差异，以及复杂的信号——这种差异往往无法通过简单的评估指标，如成功率来捕捉。为了应对这些挑战，我们提出了一种组件级分析和更细致、系统化的评估框架。通过这种精密的调查，我们确定了三个关键因素，这些因素会加剧Web AI代理的脆弱性；（1）将用户目标嵌入系统提示，（2）多步动作生成，（3）观察能力。我们的发现突显了在AI代理设计中增强安全性和鲁棒性的紧迫需求，并提供了针对性防御策略的具体建议。', 'title_zh': '为什么网络AI代理比独立的大语言模型更易受攻击？一项安全性分析'}
{'arxiv_id': 'arXiv:2502.20380', 'title': 'Multi-Turn Code Generation Through Single-Step Rewards', 'authors': 'Arnav Kumar Jain, Gonzalo Gonzalez-Pumariega, Wayne Chen, Alexander M Rush, Wenting Zhao, Sanjiban Choudhury', 'link': 'https://arxiv.org/abs/2502.20380', 'abstract': 'We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, $\\mu$Code, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. $\\mu$Code iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $\\mu$Code at utilizing the execution feedback. Our code is available at this https URL.', 'abstract_zh': '我们探讨了从多轮执行反馈中生成代码的问题。现有方法要么不使用反馈生成代码，要么使用复杂的层次强化学习来优化多轮奖励。我们提出了一种简单且可扩展的方法，$\\mu$Code，它仅使用单步奖励来解决多轮代码生成问题。我们的核心见解是，代码生成是一个可恢复的一步马尔可夫决策过程（MDP），从任何中间代码状态到单轮内都可以恢复正确的代码。$\\mu$Code 通过迭代训练生成器（根据多轮执行反馈提供代码解决方案）和验证器（对新生成的代码进行评分）来解决问题。实验评估显示，我们的方法在现有最先进的基线方法上取得了显著的改进。我们分析了奖励模型和策略的设计选择，并展示了$\\mu$Code 在利用执行反馈方面的有效性。我们的代码可在以下网址获取：this https URL。', 'title_zh': '单步奖励驱动的多轮代码生成'}
{'arxiv_id': 'arXiv:2502.20377', 'title': 'PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation', 'authors': 'Albert Gong, Kamilė Stankevičiūtė, Chao Wan, Anmol Kabra, Raphael Thesmar, Johann Lee, Julius Klenke, Carla P. Gomes, Kilian Q. Weinberger', 'link': 'https://arxiv.org/abs/2502.20377', 'abstract': 'High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at this https URL.', 'abstract_zh': '高质量的基准对于评估大型语言模型（LLMs）的推理和检索能力至关重要。然而，为了这一目的而构建的数据集并非长久之计，因为它们容易出现数据泄露和虚增性能结果的问题。为了解决这些挑战，我们提出了PhantomWiki：一种生成独特、事实一致且具有多样化问答对文档集合的管道。与以往工作不同，PhantomWiki 既不是一个固定的数据库，也不是基于任何现有数据集构建的。相反，每次评估时都会按需生成一个新的PhantomWiki实例。我们通过改变问题的难度和文档集合的大小，分别分离推理能力和检索能力，并发现PhantomWiki的数据集对前沿的LLMs来说出乎意料地具有挑战性。因此，我们贡献了一种可扩展且防数据泄露的分离评估框架，用于评估推理、检索和工具使用能力。我们的代码可在以下链接获取：\\[请将相关URL替换为实际的链接\\]。', 'title_zh': 'PhantomWiki：按需数据集用于推理和检索评估'}
{'arxiv_id': 'arXiv:2502.20354', 'title': 'Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study', 'authors': 'Nazarii Drushchak, Vladyslava Tyshchenko, Nataliya Polyakovska', 'link': 'https://arxiv.org/abs/2502.20354', 'abstract': 'The growth of Educational Technology (EdTech) has enabled highly personalized learning experiences through Artificial Intelligence (AI)-based recommendation systems tailored to each student needs. However, these systems can unintentionally introduce biases, potentially limiting fair access to learning resources. This study presents a recommendation system for K-12 students, combining graph-based modeling and matrix factorization to provide personalized suggestions for extracurricular activities, learning resources, and volunteering opportunities. To address fairness concerns, the system includes a framework to detect and reduce biases by analyzing feedback across protected student groups. This work highlights the need for continuous monitoring in educational recommendation systems to support equitable, transparent, and effective learning opportunities for all students.', 'abstract_zh': '教育技术（EdTech）的增长通过基于人工智能（AI）的个性化推荐系统为每位学生提供了高度个性化的学习体验。然而，这些系统可能会无意中引入偏见，从而限制公平获取学习资源的机会。本研究提出了一种针对K-12学生的信息推荐系统，结合图模型和矩阵分解技术，为课外活动、学习资源和志愿服务机会提供个性化建议。为解决公平性问题，该系统包含一个框架，通过分析不同受保护学生群体的反馈来进行偏见检测和减少。本研究强调了在教育推荐系统中持续监测的重要性，以支持所有学生获得平等、透明和有效的学习机会。', 'title_zh': '负责任的人工智能在教育中的探索：面向K-12学生的混合推荐系统案例研究'}
{'arxiv_id': 'arXiv:2502.20301', 'title': 'M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging', 'authors': 'Jinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie', 'link': 'https://arxiv.org/abs/2502.20301', 'abstract': 'Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.', 'abstract_zh': '自主智能代理系统因其自主完成复杂任务的能力而备受关注。然而，它们对精心准备的工具的依赖限制了其在医学领域的应用，而医学领域需要训练专门的模型。本论文作出了三项贡献：（i）我们提出了一种名为M3Builder的新型多智能体系统，旨在自动化医学影像领域的机器学习（ML）流程。M3Builder的核心是一个由四个专门智能体组成的协作框架，它们能够处理从自动数据处理和环境配置到自我封装的自动调试和模型训练等复杂、多步骤的医学影像ML工作流程。这些智能体在一种结构化的医学影像ML工作环境中运行，该环境为智能体提供了数据集、训练代码和交互工具的自由文本描述，从而实现无缝的通信和任务执行。（ii）为了评估自动化医学影像ML的进展，我们提出了M3Bench基准测试，该基准测试包含跨越五个解剖部位、三种成像模态（包括2D和3D数据）的14个训练数据集上的四个通用任务。（iii）我们试验了七种最先进的大规模语言模型作为系统的核心智能体，例如Claude系列、GPT-4o和DeepSeek-V3。与现有的ML智能体设计相比，M3Builder在医学影像领域的ML任务完成上表现出更优越的性能，使用Claude-3.7-Sonnet作为核心智能体时，任务成功率达94.29%，展现出向完全自动化的医学影像机器学习方向的巨大潜力。', 'title_zh': 'M^3Builder：一种用于医学影像领域自动化机器学习的多智能体系统'}
{'arxiv_id': 'arXiv:2502.20299', 'title': 'An exploration of features to improve the generalisability of fake news detection models', 'authors': 'Nathaniel Hoy, Theodora Koulouri', 'link': 'https://arxiv.org/abs/2502.20299', 'abstract': 'Fake news poses global risks by influencing elections and spreading misinformation, making detection critical. Existing NLP and supervised Machine Learning methods perform well under cross-validation but struggle to generalise across datasets, even within the same domain. This issue stems from coarsely labelled training data, where articles are labelled based on their publisher, introducing biases that token-based models like TF-IDF and BERT are sensitive to. While Large Language Models (LLMs) offer promise, their application in fake news detection remains limited. This study demonstrates that meaningful features can still be extracted from coarsely labelled data to improve real-world robustness. Stylistic features-lexical, syntactic, and semantic-are explored due to their reduced sensitivity to dataset biases. Additionally, novel social-monetisation features are introduced, capturing economic incentives behind fake news, such as advertisements, external links, and social media elements. The study trains on the coarsely labelled NELA 2020-21 dataset and evaluates using the manually labelled Facebook URLs dataset, a gold standard for generalisability. Results highlight the limitations of token-based models trained on biased data and contribute to the scarce evidence on LLMs like LLaMa in this field. Findings indicate that stylistic and social-monetisation features offer more generalisable predictions than token-based methods and LLMs. Statistical and permutation feature importance analyses further reveal their potential to enhance performance and mitigate dataset biases, providing a path forward for improving fake news detection.', 'abstract_zh': '虚假新闻在全球范围内构成了风险，通过影响选举和传播错误信息，因此其检测至关重要。现有的自然语言处理（NLP）和监督机器学习方法在交叉验证中表现良好，但在跨数据集泛化方面面临挑战，即使是在同一个领域也是如此。这一问题源于粗略标注的训练数据，其中文章是根据其出版商进行标注的，这引入了对基于词单元的模型（如TF-IDF和BERT）敏感的偏差。虽然大规模语言模型（LLMs）具有潜力，但其在虚假新闻检测中的应用仍受到限制。本研究证明，即使在粗略标注的数据上，仍然可以从这些数据中提取有意义的特征，以提高实际的稳健性。探讨了风格特征——包括词汇、句法和语义特征——由于它们对数据集偏差的敏感性较低，因此更具潜力。此外，还引入了新型的社会货币化特征，这些特征捕捉了虚假新闻背后的经济激励，如广告、外部链接和社会媒体元素。本研究基于粗略标注的NELA 2020-21数据集进行训练，并使用手工标注的Facebook网址数据集进行评估，后者是测试泛化能力的标准数据集。结果突显了基于词单元模型在偏差数据训练下的局限性，并为大规模语言模型（如LLaMa）在此领域的实证研究贡献了稀缺证据。研究结果表明，风格和社交货币化特征提供了比基于词单元的方法和大规模语言模型更具泛化能力的预测。统计学和置换特征重要性分析进一步揭示了这些特征可能提高性能并缓解数据集偏差的潜力，为改进虚假新闻检测提供了前进的道路。', 'title_zh': '探索提升虚假新闻检测模型普适性的特征分析'}
{'arxiv_id': 'arXiv:2502.20204', 'title': 'Granite Embedding Models', 'authors': 'Parul Awasthy, Aashka Trivedi, Yulong Li, Mihaela Bornea, David Cox, Abraham Daniels, Martin Franz, Gabe Goodhart, Bhavani Iyer, Vishwajeet Kumar, Luis Lastras, Scott McCarley, Rudra Murthy, Vignesh P, Sara Rosenthal, Salim Roukos, Jaydeep Sen, Sukriti Sharma, Avirup Sil, Kate Soule, Arafat Sultan, Radu Florian', 'link': 'https://arxiv.org/abs/2502.20204', 'abstract': 'We introduce the Granite Embedding models, a family of encoder-based embedding models designed for retrieval tasks, spanning dense-retrieval and sparse retrieval architectures, with both English and Multilingual capabilities. This report provides the technical details of training these highly effective 12 layer embedding models, along with their efficient 6 layer distilled counterparts. Extensive evaluations show that the models, developed with techniques like retrieval oriented pretraining, contrastive finetuning, knowledge distillation, and model merging significantly outperform publicly available models of similar sizes on both internal IBM retrieval and search tasks, and have equivalent performance on widely used information retrieval benchmarks, while being trained on high-quality data suitable for enterprise use. We publicly release all our Granite Embedding models under the Apache 2.0 license, allowing both research and commercial use at this https URL.', 'abstract_zh': '我们介绍了石英石嵌入模型（Granite Embedding models），这是一种基于编码器的嵌入模型，专为检索任务设计，涵盖了密集检索和稀疏检索架构，并且支持英语和多语言能力。本报告提供了这些高效12层嵌入模型及其高效的6层蒸馏版本的技术细节。广泛的数据评估表明，通过使用如检索导向的预训练、对比性微调、知识蒸馏和模型融合等技术开发的模型，在IBM内部的检索和搜索任务中显著优于公开可用的相似规模模型，并且其性能与广泛使用的信息检索基准相当。这些模型是在适合企业使用的高质量数据上进行训练的。我们under the Apache 2.0许可协议下公开发布了所有我们的石英石嵌入模型，允许在此网页（https://link）进行研究和商业应用。', 'title_zh': '《granite embedding models》可以翻译成中文为《 granite嵌入模型》。为了更符合学术规范，可以稍作调整，使其更加通顺和正式：\n\n《granite嵌入式模型》或《Granite嵌入模型研究》\n\n这样既保留了原文的含义，又符合中文的表达习惯。'}
{'arxiv_id': 'arXiv:2502.20175', 'title': 'An Extensive Evaluation of PDDL Capabilities in off-the-shelf LLMs', 'authors': 'Kaustubh Vyas, Damien Graux, Sébastien Montella, Pavlos Vougiouklis, Ruofei Lai, Keshuang Li, Yang Ren, Jeff Z. Pan', 'link': 'https://arxiv.org/abs/2502.20175', 'abstract': 'In recent advancements, large language models (LLMs) have exhibited proficiency in code generation and chain-of-thought reasoning, laying the groundwork for tackling automatic formal planning tasks. This study evaluates the potential of LLMs to understand and generate Planning Domain Definition Language (PDDL), an essential representation in artificial intelligence planning. We conduct an extensive analysis across 20 distinct models spanning 7 major LLM families, both commercial and open-source. Our comprehensive evaluation sheds light on the zero-shot LLM capabilities of parsing, generating, and reasoning with PDDL. Our findings indicate that while some models demonstrate notable effectiveness in handling PDDL, others pose limitations in more complex scenarios requiring nuanced planning knowledge. These results highlight the promise and current limitations of LLMs in formal planning tasks, offering insights into their application and guiding future efforts in AI-driven planning paradigms.', 'abstract_zh': '近年来，大语言模型（LLMs）在代码生成和链式推理方面展现出色能力，为解决自动形式化规划任务奠定了基础。本研究评估了LLMs理解及生成Planning Domain Definition Language（PDDL）的能力，PDDL是人工智能规划中的重要表示形式。我们对7个大家族的20个不同模型进行了全面分析，涵盖商业和开源模型。我们的综合评估揭示了LLMs在零样本情况下解析、生成和推理PDDL的能力。研究结果表明，虽然有些模型在处理PDDL方面表现出显著的效果，但在需要复杂规划知识的情景中，其它模型则表现出一定的局限性。这些结果凸显了LLMs在形式化规划任务中的潜力及其当前的局限性，为它们的应用提供了见解，并指导未来基于人工智能的规划范式的努力方向。', 'title_zh': '对现成大型语言模型中PDDL能力的全面评估'}
{'arxiv_id': 'arXiv:2502.20172', 'title': 'Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think', 'authors': 'Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang', 'link': 'https://arxiv.org/abs/2502.20172', 'abstract': 'The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.', 'abstract_zh': '先进图文生成领域的研究正见证着统一框架的出现，这些框架将强大的文本编码器（如CLIP和T5）与扩散变换器（Diffusion Transformer）骨干网络相结合。尽管已有努力通过添加条件（如Canny边缘检测和深度图）来控制生成的图像，但一个全面的框架仍然缺失，可以实现任意的图文交织控制。尤其是在生成过程中尝试结合多个图像的概念或视觉元素时，这种差距尤为明显。为了弥合这一差距，我们进行了初步实验，结果显示，大型多模态模型（LMMs）提供了一个有效的共享表示空间，在这个空间中，图像和文本可以很好地对齐，作为外部扩散模型的条件。基于这一发现，我们提出了一种高效且统一的框架——Dream Engine，用于图像生成模型中的任意图文交织控制。基于强大的文本转图像模型（如SD3.5），我们通过整合多模态信息编码器（如QwenVL）替代了原有的仅文本编码器。我们的方法采用两阶段训练范式，包括文本-图像对齐和多模态交织指令调优。实验结果证明，这种方法是有效的，在GenEval基准测试中取得了0.69的总体评分，与当前最先进的文本转图像模型（如SD3.5和FLUX）的性能相当。', 'title_zh': '多模态表示对齐在图像生成中的应用：文本-图像交错控制比你想象的要简单'}
{'arxiv_id': 'arXiv:2502.20170', 'title': 'Re-evaluating Open-ended Evaluation of Large Language Models', 'authors': 'Siqi Liu, Ian Gemp, Luke Marris, Georgios Piliouras, Nicolas Heess, Marc Lanctot', 'link': 'https://arxiv.org/abs/2502.20170', 'abstract': 'Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.', 'abstract_zh': '传统的评估主要集中在对特定技能的候选者进行排名。现代万能模型，如大规模语言模型（LLMs），在这一范式上显然领先很多。开放式评估系统，其中候选模型通过用户提交的提示进行比较，已经成为一种流行的方法。尽管这种系统有许多优点，但我们发现目前基于Elo的评分系统可能会受到数据中的偏见，甚至会增强这种偏见，无论是有意还是无意的，因为这些系统对冗余性特别敏感。为了应对这一问题，我们提议将评估作为一种三玩家游戏，并引入新的博弈论解决方案概念，以确保对冗余性的鲁棒性。我们展示了该方法能够产生直观的评分，并提供了LLM开发竞争格局的见解。', 'title_zh': '重新评估大型语言模型的开放评价'}
{'arxiv_id': 'arXiv:2502.20167', 'title': 'Similarity-Distance-Magnitude Universal Verification', 'authors': 'Allen Schmaltz', 'link': 'https://arxiv.org/abs/2502.20167', 'abstract': 'We solve the neural network robustness problem by adding Similarity (i.e., correctly predicted depth-matches into training)-awareness and Distance-to-training-distribution-awareness to the existing output Magnitude (i.e., decision-boundary)-awareness of the softmax function. The resulting sdm activation function provides strong signals of the relative epistemic (reducible) predictive uncertainty. We use this novel behavior to further address the complementary HCI problem of mapping the output to human-interpretable summary statistics over relevant partitions of a held-out calibration set. Estimates of prediction-conditional uncertainty are obtained via a parsimonious learned transform over the class-conditional empirical CDFs of the output of a final-layer sdm activation function. For decision-making and as an intrinsic model check, estimates of class-conditional accuracy are obtained by further partitioning the high-probability regions of this calibrated output into class-conditional, region-specific CDFs. The uncertainty estimates from sdm calibration are remarkably robust to test-time distribution shifts and out-of-distribution inputs; incorporate awareness of the effective sample size; provide estimates of uncertainty from the learning and data splitting processes; and are well-suited for selective classification and conditional branching for additional test-time compute based on the predictive uncertainty, as for selective LLM generation, routing, and composition over multiple models and retrieval. Finally, we construct sdm networks, LLMs with uncertainty-aware verification and interpretability-by-exemplar as intrinsic properties. We provide open-source software implementing these results.', 'abstract_zh': '我们通过向现有的输出幅度-awareness（即softmax函数的决策边界-awareness）中加入相似度-awareness（即正确预测的深度匹配加入训练）和距离到训练分布-awareness，解决了神经网络的鲁棒性问题。由此产生的sdm激活函数提供了相对的epistemic（可归因的）预测不确定性强烈的信号。我们使用这种新的行为来进一步解决将输出映射到held-out校准集的有关子集上的人类可解释总结统计量的互补的人机交互（HCI）问题。通过一个简约的学习转换，基于最终层sdm激活函数的输出的类别条件经验累积分布函数（CDFs），我们获得了条件于预测的不确定性估计。对于决策和作为内在模型检查，通过进一步将高概率区域细分为类别条件、区域特定的CDFs，我们获得了条件于类别的准确性的估计。来自sdm校准的不确定性估计对测试时间分布偏移和未知分布输入表现出惊人的鲁棒性；考虑了有效样本量的意识；提供了从学习和数据划分过程中获得的不确定性估计；并且非常适合根据预测不确定性进行选择性分类和条件分支，以基于预测不确定性进行额外的测试时间计算，如选择性LLM生成、路由和跨越多个模型以及检索时的组合。最后，我们构建了具有不确定性-aware验证和通过实例进行解释内生属性的sdm网络和LLM。我们提供了实现这些结果的开源软件。', 'title_zh': '相似性-距离-尺度通用验证方法'}
{'arxiv_id': 'arXiv:2502.20140', 'title': 'Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based Telephone Survey System at Scale', 'authors': 'Max M. Lang, Sol Eskenazi', 'link': 'https://arxiv.org/abs/2502.20140', 'abstract': "Telephone surveys remain a valuable tool for gathering insights but typically require substantial resources in training and coordinating human interviewers. This work presents an AI-driven telephone survey system integrating text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT) that mimics the versatility of human-led interviews on scale.\nWe tested the system across two populations, a pilot study in the United States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting participants via web-based links and contacting them via direct phone calls. The AI agent successfully administered open-ended and closed-ended questions, handled basic clarifications, and dynamically navigated branching logic, allowing fast large-scale survey deployment without interviewer recruitment or training.\nOur findings demonstrate that while the AI system's probing for qualitative depth was more limited than human interviewers, overall data quality approached human-led standards for structured items. This study represents one of the first successful large-scale deployments of an LLM-based telephone interviewer in a real-world survey context. The AI-powered telephone survey system has the potential for expanding scalable, consistent data collecting across market research, social science, and public opinion studies, thus improving operational efficiency while maintaining appropriate data quality for research.", 'abstract_zh': '电话调查仍然是获得见解的有效工具，但通常需要大量资源来培训和协调调查员。本研究介绍了结合文本转语音（TTS）、大规模语言模型（LLM）和语音转文本（STT）的AI驱动电话调查系统，该系统可大规模模拟人类主导访谈的多样性。\n\n我们对该系统进行了两项测试：一项在美国的试点研究（n = 75），一项在秘鲁的大规模部署（n = 2,739）。我们通过网络链接邀请参与者，并通过直接电话联系他们。AI代理成功地执行了开放式和封闭式问题的管理，处理了基本的澄清，并动态导航了分支逻辑，从而实现了快速的大规模调查部署，无需招募或培训调查员。\n\n我们的研究结果表明，虽然AI系统在获取质性深度方面的探究能力有限，但总体数据质量接近人类主导访谈的标准。本研究代表了第一个成功的基于LLM的电话访谈在实际调查情境中的大规模部署。AI驱动的电话调查系统有望在市场研究、社会科学和公共舆论调查等多个领域扩展可扩展、一致的数据收集，从而提高操作效率，同时保持适当的数据质量以满足研究需求。', 'title_zh': '电话调查遇见对话式AI：评估基于大语言模型的电话调查系统'}
{'arxiv_id': 'arXiv:2502.20127', 'title': 'SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning', 'authors': 'Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, Bing Xie', 'link': 'https://arxiv.org/abs/2502.20127', 'abstract': 'Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.', 'abstract_zh': '主流的问题解决框架主要依赖商业模型，导致成本高和隐私担忧。现有的问题解决训练方法在泛化能力方面表现不佳，未能充分利用开源开发资源。我们提出了一种新的训练方法——面向子任务的强化微调（Subtask-oriented Reinforced Fine-Tuning, SoRFT），以增强大型语言模型（LLM）的问题解决能力。我们将问题解决分解为结构化的子任务：文件定位、函数定位、行定位以及代码编辑生成。SoRFT 包含两个训练阶段：（1）拒绝采样的监督微调，在 fine-tuning LLM 之前使用 ground-truth 进行 Chain of Thought（思维链）数据筛选；（2）基于规则的强化学习，利用 PPO（Proximal Policy Optimization，接近策略优化）结合基于 ground-truth 的奖励。我们使用 SWE-Bench Verified 和 SWE-Bench Lite 对 SoRFT 训练的模型进行了评估，在 SWE-Bench Verified 上实现了开源模型中的最佳性能（例如，SoRFT-Qwen-7B 解决了 21.4% 的问题）。实验结果表明，SoRFT 显著提升了问题解决性能，提高了模型的泛化能力，并提供了相对于商业模型的成本效益更高的替代方案。', 'title_zh': 'SoRFT：面向子任务的强化微调解决策略'}
{'arxiv_id': 'arXiv:2502.20034', 'title': 'Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore', 'authors': 'Hongseok Oh, Wonseok Hwang', 'link': 'https://arxiv.org/abs/2502.20034', 'abstract': 'Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination. This study revisits the previous claim that the primary cause of such hallucination lies in the limited representational capacity of the vision encoder. Our analysis reveals that the capacity of the vision encoder itself is already enough for detecting object hallucination. Based on this insight, we propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of 39.6% without additional training. We further validate F-CLIPScore by showing that LVLM trained with the data filtered using F-CLIPScore exhibits reduced hallucination.', 'abstract_zh': '近年来，大型多模态模型（Large Vision-Language Models, LVLMs）在各种领域中展现了显著的性能。然而，这些模型存在对象幻觉的问题。本研究重新审视了此前关于这种幻觉主要源于视觉编码器表示能力有限的说法。我们的分析表明，视觉编码器本身的能力已经足以检测对象幻觉。基于这一洞察，我们提出了精细粒度的CLIPScore（F-CLIPScore），这是一种简单而有效的评估指标，通过在名词短语层面引入文本嵌入来增强对象级别的粒度。在OHD-Caps基准测试上的评估结果显示，F-CLIPScore在准确性方面显著优于传统的CLIPScore，提高了39.6%，且无需额外训练。进一步验证了F-CLIPScore的有效性，表明使用F-CLIPScore筛选数据训练的LVLM幻觉现象有所减少。', 'title_zh': '视觉编码器（早已）识其所见：通过简单的细粒度CLIPScore减轻物体幻视问题'}
{'arxiv_id': 'arXiv:2502.19883', 'title': 'Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models', 'authors': 'Sibo Yi, Tianshuo Cong, Xinlei He, Qi Li, Jiaxing Song', 'link': 'https://arxiv.org/abs/2502.19883', 'abstract': 'Small language models (SLMs) have become increasingly prominent in the deployment on edge devices due to their high efficiency and low computational cost. While researchers continue to advance the capabilities of SLMs through innovative training strategies and model compression techniques, the security risks of SLMs have received considerably less attention compared to large language models (LLMs).To fill this gap, we provide a comprehensive empirical study to evaluate the security performance of 13 state-of-the-art SLMs under various jailbreak attacks. Our experiments demonstrate that most SLMs are quite susceptible to existing jailbreak attacks, while some of them are even vulnerable to direct harmful this http URL address the safety concerns, we evaluate several representative defense methods and demonstrate their effectiveness in enhancing the security of SLMs. We further analyze the potential security degradation caused by different SLM techniques including architecture compression, quantization, knowledge distillation, and so on. We expect that our research can highlight the security challenges of SLMs and provide valuable insights to future work in developing more robust and secure SLMs.', 'abstract_zh': '小语言模型（SLMs）由于其高效率和低计算成本，在边缘设备上的部署越来越受到重视。尽管研究人员通过创新的训练策略和模型压缩技术不断推进SLMs的能力，但与大型语言模型（LLMs）相比，SLMs的安全风险一直受到的关注较少。为了弥补这一空白，我们提供了一项全面的经验性研究，评估了13种最先进的SLMs在各种‘破戒’攻击下的安全性能。实验结果显示，大多数SLMs对现有的‘破戒’攻击非常敏感，甚至有些SLMs对直接有害的攻击都显得脆弱。为应对安全关切，我们评估了几种代表性的防御方法，并证明了它们在增强SLMs安全性方面的有效性。我们还分析了不同SLM技术（包括架构压缩、量化、知识蒸馏等）可能导致的安全性能下降。我们希望我们的研究能突出SLMs所面临的安全挑战，并为未来开发更 robust 和更安全的SLMs提供宝贵的见解。', 'title_zh': '超越效率的尖端：揭示小型语言模型中越狱攻击潜藏的威胁'}
{'arxiv_id': 'arXiv:2502.19852', 'title': 'ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments', 'authors': 'Hojae Han, Seung-won Hwang, Rajhans Samdani, Yuxiong He', 'link': 'https://arxiv.org/abs/2502.19852', 'abstract': "Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at this https URL", 'abstract_zh': '大规模语言模型（LLMs）在代码生成领域，特别是在交互式设置中展现了巨大的价值。然而，现有的代码生成基准在捕捉多轮交互中接收到的多样反馈方面存在局限，限制了我们评估LLMs的能力。为了解决这一问题，我们提出了一组新颖的基准，明确地建模了提供给代码生成LLMs的反馈质量。我们的贡献主要体现在三个方面：\n\n首先，我们引入了CONVCODEWORLD，一种新颖可复现的交互式代码生成基准环境。CONVCODEWORLD模拟了9种不同的交互式代码生成场景，并系统地结合了三种类型反馈：(a) 编译反馈；(b) 执行反馈，覆盖不同的测试覆盖率；(c) GPT-4o生成的言语反馈，具有不同的专业水平。\n\n其次，我们引入了CONVCODEBENCH，这是一个快速的静态版本基准，使用预生成的反馈日志，消除了生成成本高昂的动态言语反馈的需要，同时保持了CONVCODEWORLD的较强的斯皮尔曼等级相关性（0.82至0.99）。\n\n第三，对包括R1-Distill在内的闭源和开源LLMs进行了广泛的评估，揭示了以下关键洞察：(a) LLM的表现显著取决于所提供的反馈；(b) 即使缺乏反馈，性能较弱的LLM在有足够的反馈时，可以超越没有反馈的最先进的LLM的单轮结果；(c) 专门针对特定反馈组合进行训练可能会限制LLM处理未见组合的能力；(d) 能够在较少轮次内解决问题（高MRR）的LLMs可能整体上解决的问题数量较少（高召回率），反之亦然。\n\n所有实现和基准将在此网址上公开：[提供链接]', 'title_zh': 'ConvCodeWorld: 在可重复反馈环境中对话式代码生成的基准测试'}
{'arxiv_id': 'arXiv:2502.19726', 'title': 'Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training', 'authors': 'Toan Tran, Ruixuan Liu, Li Xiong', 'link': 'https://arxiv.org/abs/2502.19726', 'abstract': "Large language models (LLMs) have become the backbone of modern natural language processing but pose privacy concerns about leaking sensitive training data. Membership inference attacks (MIAs), which aim to infer whether a sample is included in a model's training dataset, can serve as a foundation for broader privacy threats. Existing defenses designed for traditional classification models do not account for the sequential nature of text data. As a result, they either require significant computational resources or fail to effectively mitigate privacy risks in LLMs. In this work, we propose a lightweight yet effective empirical privacy defense for protecting training data of language modeling by leveraging the token-specific characteristics. By analyzing token dynamics during training, we propose a token selection strategy that categorizes tokens into hard tokens for learning and memorized tokens for unlearning. Subsequently, our training-phase defense optimizes a novel dual-purpose token-level loss to achieve a Pareto-optimal balance between utility and privacy. Extensive experiments demonstrate that our approach not only provides strong protection against MIAs but also improves language modeling performance by around 10\\% across various LLM architectures and datasets compared to the baselines.", 'abstract_zh': '大规模语言模型（LLMs）已成为现代自然语言处理的核心，但它们也带来了隐私担忧，即泄露敏感训练数据。成员推断攻击（MIAs），旨在推断一个样本是否包含在模型的训练数据集中，可以成为更广泛隐私威胁的基础。现有针对传统分类模型设计的防御措施未能考虑到文本数据的序列性。结果，这些防御措施要么需要大量的计算资源，要么在减轻LLMs中的隐私风险方面效果不佳。在本文中，我们提出了一种轻量级但有效的实证隐私防御方法，通过利用令牌特定的特性来保护语言模型的训练数据。通过分析训练过程中令牌的动力学，我们提出了一种令牌选择策略，将令牌分为用于学习的硬令牌和用于遗忘的存储令牌。随后，我们提出了一种新的双重目的令牌级损失函数在训练阶段进行优化，以实现效用与隐私之间的帕累托最优平衡。广泛的实验表明，我们的方法不仅能够有效地抵御MIAs，而且与基线方法相比，在多种LLM架构和数据集上提高了约10%的语言模型性能。', 'title_zh': '《学习用的令牌与消学习用的令牌：通过双重目的训练减轻大型语言模型成员推断攻击》'}
{'arxiv_id': 'arXiv:2502.19676', 'title': 'The Future Outcome Reasoning and Confidence Assessment Benchmark', 'authors': 'Zhangdie Yuan, Zifeng Ding, Andreas Vlachos', 'link': 'https://arxiv.org/abs/2502.19676', 'abstract': "Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.", 'abstract_zh': '预测是许多领域（如技术和经济学）中的一项重要任务。然而，现有的预测基准在全面信心评估方面存在不足，专注于有限的问题类型，并且通常包含不与现实生活中的人类预测需求对齐的人工问题。为了解决这些问题，我们引入了FOReCAst（未来结果推理与信心评估），这是一种评估模型预测能力和其对预测的信心的基准。FOReCAst 涵盖了涉及布尔问题、时间范围预测和数量估计的多种预测场景，从而能够对预测准确性以及信心校准进行全面评估，以满足实际应用的需求。', 'title_zh': '未来结果推理与信心评估基准'}
{'arxiv_id': 'arXiv:2502.19668', 'title': 'SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning', 'authors': 'Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci', 'link': 'https://arxiv.org/abs/2502.19668', 'abstract': 'Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a $\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for $\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME applies Large Language Models (LLMs) to extract structured clinical entities from free-text ECG reports, filter out noise and irrelevant content, enhance clinical representation learning, and build a high-quality, fine-grained labeled dataset. By using text-based cardiac queries instead of traditional categorical labels, SuPreME enables zero-shot classification of unseen diseases without additional fine-tuning. We evaluate SuPreME on six downstream datasets covering 127 cardiac conditions, achieving superior zero-shot AUC performance over state-of-the-art eSSL and multimodal methods by over 1.96\\%. Results demonstrate the effectiveness of SuPreME in leveraging structured, clinically relevant knowledge for high-quality ECG representations. All code and data will be released upon acceptance.', 'abstract_zh': '心血管疾病是全球范围内导致死亡和残疾的主要原因之一。心电图（ECG）记录对于诊断和监控心脏健康至关重要，但获取大规模标注的心电图数据集是一个劳动密集型且耗时的过程。近期的心电图自监督学习（eSSL）方法通过无需大量标签即可学习特征来缓解这一问题，但无法捕捉到精细的临床语义，并且需要大量的任务特定微调。为了解决这些挑战，我们提出了一种名为$\\textbf{SuPreME}$的$\\textbf{S}$upervised $\\textbf{P}$re-training框架，用于$\\textbf{M}$ultimodal $\\textbf{E}$CG表示学习。SuPreME利用大型语言模型（LLMs）从自由文本ECG报告中提取结构化的临床实体，过滤掉噪声和无关内容，增强临床表示学习，并构建高质量的细粒度标注数据集。通过使用基于文本的心脏查询而非传统的分类标签，SuPreME在未经额外微调的情况下实现了对未见过疾病的零样本分类。我们将在六个下游数据集上评估SuPreME，这些数据集覆盖了127种心脏状况，SuPreME在零样本AUC性能上比当前最先进的eSSL和多模态方法优越1.96%以上。结果表明，SuPreME在利用结构化的、与临床相关的知识来生成高质量的ECG表示方面具有有效性。在接受后的所有代码和数据将被发布。', 'title_zh': 'SuPreME：一种监督预训练的多模态心电图表示学习框架'}
{'arxiv_id': 'arXiv:2502.19546', 'title': 'Repurposing the scientific literature with vision-language models', 'authors': 'Anton Alyakin, Jaden Stryker, Daniel Alexander Alber, Karl L. Sangwon, Brandon Duderstadt, Akshay Save, David Kurland, Spencer Frome, Shrutika Singh, Jeff Zhang, Eunice Yang, Ki Yun Park, Cordelia Orillac, Aly A. Valliani, Sean Neifert, Albert Liu, Aneek Patel, Christopher Livia, Darryl Lau, Ilya Laufer, Peter A. Rozman, Eveline Teresa Hidalgo, Howard Riina, Rui Feng, Todd Hollon, Yindalon Aphinyanaphongs, John G. Golfinos, Laura Snyder, Eric Leuthardt, Douglas Kondziolka, Eric Karl Oermann', 'link': 'https://arxiv.org/abs/2502.19546', 'abstract': 'Research in AI for Science often focuses on using AI technologies to augment components of the scientific process, or in some cases, the entire scientific method; how about AI for scientific publications? Peer-reviewed journals are foundational repositories of specialized knowledge, written in discipline-specific language that differs from general Internet content used to train most large language models (LLMs) and vision-language models (VLMs). We hypothesized that by combining a family of scientific journals with generative AI models, we could invent novel tools for scientific communication, education, and clinical care. We converted 23,000 articles from Neurosurgery Publications into a multimodal database - NeuroPubs - of 134 million words and 78,000 image-caption pairs to develop six datasets for building AI models. We showed that the content of NeuroPubs uniquely represents neurosurgery-specific clinical contexts compared with broader datasets and PubMed. For publishing, we employed generalist VLMs to automatically generate graphical abstracts from articles. Editorial board members rated 70% of these as ready for publication without further edits. For education, we generated 89,587 test questions in the style of the ABNS written board exam, which trainee and faculty neurosurgeons found indistinguishable from genuine examples 54% of the time. We used these questions alongside a curriculum learning process to track knowledge acquisition while training our 34 billion-parameter VLM (CNS-Obsidian). In a blinded, randomized controlled trial, we demonstrated the non-inferiority of CNS-Obsidian to GPT-4o (p = 0.1154) as a diagnostic copilot for a neurosurgical service. Our findings lay a novel foundation for AI with Science and establish a framework to elevate scientific communication using state-of-the-art generative artificial intelligence while maintaining rigorous quality standards.', 'abstract_zh': '人工智能在科学领域的研究通常侧重于利用AI技术来增强科学过程的各个部分，甚至在某些情况下，整个科学方法；那么，人工智能在科学研究出版方面的作用又如何呢？同行评审期刊是专门知识的基础存储库，其内容使用的是学科专用语言，与广泛用于训练大多数大型语言模型（LLMs）和视觉-语言模型（VLMs）的通用互联网内容有所不同。我们假设，通过将一系列科学期刊与生成AI模型相结合，可以发明新型工具，用于科学交流、教育和临床护理。我们将神经外科出版物中的23,000篇文章转换为一个多模态数据库——NeuroPubs，包含1.34亿词和78,000张图片及其描述对，开发了六个数据集以构建AI模型。我们证明了NeuroPubs的内容与其他广泛数据集和PubMed相比，唯一地代表了神经外科特有的临床环境。在出版方面，我们使用通才型VLMs自动生成图形摘要。期刊编辑评定70%的文章无需进一步编辑即可出版。在教育方面，我们生成了89,587道ABNS笔试风格的问题，这些问题是训练和教学外科医生在54%的时间内无法区分的真正例子。我们将这些问题与课程学习过程结合使用，以追踪知识获取情况，同时训练我们340亿参数的VLM（CNS-Obsidian）。在一项盲法随机对照试验中，我们证明了CNS-Obsidian与GPT-4o（p = 0.1154）在作为神经外科服务的诊断副驾方面具有非劣效性。我们的研究为AI与科学奠定了新的基础，确立了利用最先进生成人工智能提升科学交流的框架，并保持严格的质量标准。', 'title_zh': '使用视觉-语言模型重新利用科学文献'}
{'arxiv_id': 'arXiv:2502.19500', 'title': 'Conversational Planning for Personal Plans', 'authors': 'Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matarić', 'link': 'https://arxiv.org/abs/2502.19500', 'abstract': "The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.", 'abstract_zh': '大型语言模型（LLMs）的语言生成和推理能力使得各类任务中表现出色的对话系统成为可能，从代码生成到创作论文，再到通过科学、技术和法律考试，甚至开启了一种新的知识搜索范式。除了这些短期应用，LLMs还被越来越多地用于帮助实现长期目标或持续时间较长的任务，这些任务可能涉及几天、几周、几个月甚至几年的时间跨度。因此，为了使对话系统能够处理长期的互动和任务，我们需要能够为长期展望进行规划的语言基础代理。传统上，这种能力是由具有层次规划能力的强化学习代理来解决的。在本项工作中，我们探索了一种新型架构，其中LLM作为高级控制器决定代理的下一步宏观动作，利用工具使用的LLM基于选项策略执行选定的宏观动作。我们为特定的宏观动作构建了这一框架，以通过对话和后续问题收集用户反馈来实现用户个人计划的适应性规划。我们展示了这一范式在从学术和非学术任务的教学到个人健康计划的对话式辅导等各种场景中的应用前景。', 'title_zh': '个人计划的对话规划'}
{'arxiv_id': 'arXiv:2502.19130', 'title': 'Voting or Consensus? Decision-Making in Multi-Agent Debate', 'authors': 'Lars Benedikt Kaesberg, Jonas Becker, Jan Philip Wahle, Terry Ruas, Bela Gipp', 'link': 'https://arxiv.org/abs/2502.19130', 'abstract': 'Much of the success of multi-agent debates depends on carefully choosing the right parameters. Among them, the decision-making protocol stands out. Systematic comparison of decision protocols is difficult because studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making addresses the challenges of different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time (i.e., decision protocol) to analyze how different methods affect the collaboration between agents and test different protocols on knowledge (MMLU, MMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks over the other decision protocol. Increasing the number of agents improves performance, while more discussion rounds before voting reduces it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.', 'abstract_zh': '多智能体辩论的成功很大程度上取决于正确选择参数，其中决策协议尤为关键。系统地比较决策协议的差异颇具挑战性，因为研究中会同时改变多项讨论参数，而不仅仅是决策协议本身。目前，决策如何应对不同任务的挑战尚不明确。本研究系统地评估了七种决策协议（例如，多数投票、一致同意）的影响。我们每次只改变一个变量（即，决策协议），分析不同方法对智能体之间协作的影响，并在知识（MMLU、MMLU-Pro、GPQA）和推理数据集（StrategyQA、MuSR、SQuAD 2.0）上测试不同的协议。结果显示，投票协议在推理任务中的性能提高了13.2%，而在知识任务中的共识协议则提高了2.8%。增加智能体的数量可以提升性能，而投票前的讨论轮数增多则会降低性能。为了通过增加答案多样性来提高决策质量，我们提出了两种新方法：All-Agents Drafting (AAD) 和 Collective Improvement (CI)。我们的方法在AAD下的任务性能提高了最多3.3%，在CI下则提高了最多7.4%。本研究展示了多智能体辩论中决策的重要性，不仅在于规模的扩大。', 'title_zh': '投票还是共识？多智能体辩论中的决策机制'}
