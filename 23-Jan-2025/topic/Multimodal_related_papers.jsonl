{'arxiv_id': 'arXiv:2501.12431', 'title': 'Modality Interactive Mixture-of-Experts for Fake News Detection', 'authors': 'Yifan Liu, Yaokun Liu, Zelin Li, Ruichen Yao, Yang Zhang, Dong Wang', 'link': 'https://arxiv.org/abs/2501.12431', 'abstract': 'The proliferation of fake news on social media platforms disproportionately impacts vulnerable populations, eroding trust, exacerbating inequality, and amplifying harmful narratives. Detecting fake news in multimodal contexts -- where deceptive content combines text and images -- is particularly challenging due to the nuanced interplay between modalities. Existing multimodal fake news detection methods often emphasize cross-modal consistency but ignore the complex interactions between text and visual elements, which may complement, contradict, or independently influence the predicted veracity of a post. To address these challenges, we present Modality Interactive Mixture-of-Experts for Fake News Detection (MIMoE-FND), a novel hierarchical Mixture-of-Experts framework designed to enhance multimodal fake news detection by explicitly modeling modality interactions through an interaction gating mechanism. Our approach models modality interactions by evaluating two key aspects of modality interactions: unimodal prediction agreement and semantic alignment. The hierarchical structure of MIMoE-FND allows for distinct learning pathways tailored to different fusion scenarios, adapting to the unique characteristics of each modality interaction. By tailoring fusion strategies to diverse modality interaction scenarios, MIMoE-FND provides a more robust and nuanced approach to multimodal fake news detection. We evaluate our approach on three real-world benchmarks spanning two languages, demonstrating its superior performance compared to state-of-the-art methods. By enhancing the accuracy and interpretability of fake news detection, MIMoE-FND offers a promising tool to mitigate the spread of misinformation, with the potential to better safeguard vulnerable communities against its harmful effects.', 'abstract_zh': '社交媒体平台上的假新闻泛滥，对弱势群体产生了不成比例的影响，侵蚀了信任，加剧了不平等，并放大了有害的叙事。在多模态背景下检测假新闻——其中欺骗性内容结合了文本和图像——尤为具有挑战性，因为不同模态之间的交互是微妙的。现有的多模态假新闻检测方法往往强调跨模态的一致性，而忽略了文本和视觉元素之间复杂的相互作用，这些相互作用可能会互补、矛盾或独立影响帖子的真实性的预测。为了解决这些挑战，我们提出了一种新型分层专家混合模型，用于假新闻检测（MIMoE-FND），该模型通过交互门控机制明确建模模态交互，以增强多模态假新闻检测。我们的方法通过评估模态交互的两个关键方面——单一模态预测一致性与语义对齐——来建模模态交互。MIMoE-FND 的分层结构允许根据不同的融合场景制定不同的学习路径，以适应每种模态交互的独特特征。通过根据不同模态交互场景定制融合策略，MIMoE-FND 提供了一种更稳健且细致的方法来检测多模态假新闻。我们使用两种语言的三个实际基准测试评估了该方法，结果显示其性能优于最先进的方法。通过提高假新闻检测的准确性和可解释性，MIMoE-FND 提供了一种有前景的工具来遏制虚假信息的传播，有望更好地保护弱势群体免受其有害影响。', 'title_zh': '模ality交互混合专家系统在假新闻检测中的应用'}
{'arxiv_id': 'arXiv:2501.12425', 'title': 'Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET', 'authors': 'Fatih Aksu, Fabrizia Gelardi, Arturo Chiti, Paolo Soda', 'link': 'https://arxiv.org/abs/2501.12425', 'abstract': 'Accurate classification of histological subtypes of non-small cell lung cancer (NSCLC) is essential in the era of precision medicine, yet current invasive techniques are not always feasible and may lead to clinical complications. This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images. Our method integrates the two modalities at different stages of feature extraction, using voxel-wise fusion to exploit complementary information across varying abstraction levels while preserving spatial correlations. We compare our method against unimodal approaches using only CT or PET images to demonstrate the benefits of modality fusion, and further benchmark it against early and late fusion techniques to highlight the advantages of intermediate fusion during feature extraction. Additionally, we compare our model with the only existing intermediate fusion method for histological subtype classification using PET/CT images. Our results demonstrate that the proposed method outperforms all alternatives across key metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This non-invasive approach has the potential to significantly improve diagnostic accuracy, facilitate more informed treatment decisions, and advance personalized care in lung cancer management.', 'abstract_zh': '在精准医疗时代，准确分类非小细胞肺癌（NSCLC）的组织学亚型是至关重要的，然而当前侵入性技术并不总是可行的，且可能会导致临床并发症。本研究提出了一种多阶段中间融合方法，用于从CT和PET图像中分类NSCLC亚型。我们的方法在特征提取的不同阶段融合了这两种模态，通过体素级融合来利用不同抽象层次之间的互补信息，同时保留空间相关性。我们对比了仅使用CT或PET图像的单模态方法，展示了模态融合的优势，并进一步将其与早期融合和晚期融合技术进行基准测试，以突出特征提取过程中中间融合的优势。此外，我们将我们的模型与唯一已有的用于PET/CT图像组织学亚型分类的中间融合方法进行对比。研究结果表明，所提出的方法在关键指标上优于所有其他选项，在准确性和AUC上分别达到0.724和0.681。这种非侵入性方法有可能显著提高诊断准确性、促进更加知情的治疗决策，并推动肺癌管理中的个性化护理的进步。', 'title_zh': '基于CT和PET的非小细胞肺癌亚型分类的多阶段中间融合多模态学习方法'}
{'arxiv_id': 'arXiv:2501.12424', 'title': 'Multi-Modality Collaborative Learning for Sentiment Analysis', 'authors': 'Shanmin Wang, Chengguang Liu, Qingshan Liu', 'link': 'https://arxiv.org/abs/2501.12424', 'abstract': "Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at this https URL.", 'abstract_zh': '多模态情感分析（Multimodal Sentiment Analysis, MSA）通过整合视觉、音频和文本等多种模态，识别个体在视频中的情感状态。尽管现有方法取得了进展，但固有的模态异质性限制了跨模态情感特征的有效捕捉。本文通过引入多模态协作学习（Multi-Modality Collaborative Learning, MMCL）框架，促进跨模态交互，并分别从模态共通和模态特定表示中捕获增强和互补的特征。具体而言，我们设计了一个无参数解耦模块，并通过跨模态元素的语义评估将单一模态分离为模态共通和模态特定组件。对于模态特定表示，受强化学习中行为-奖励机制的启发，我们设计了策略模型，在联合奖励的指导下自适应地挖掘互补的情感特征。对于模态共通表示，我们采用了跨模态内的注意力机制以突出关键组件，在模态之间发挥增强作用。实验结果，包括在四个数据库上的优越性评估、每个模块的有效性验证以及互补特征的评估，表明MMCL成功地在模态间学习了协作特征，并显著提高了性能。代码可以在以下链接获取：[提供链接]。', 'title_zh': '多模态协作学习在情感分析中的应用'}
{'arxiv_id': 'arXiv:2501.12422', 'title': 'CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning', 'authors': 'Eunjee Choi, Junhyun Ahn, XinYu Piao, Jong-Kook Kim', 'link': 'https://arxiv.org/abs/2501.12422', 'abstract': 'Multimodal Fake News Detection has received increasing attention recently. Existing methods rely on independently encoded unimodal data and overlook the advantages of capturing intra-modality relationships and integrating inter-modal similarities using advanced techniques. To address these issues, Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as encoders to capture detailed text, image and combined image-text representations. The metric learning module employs a proxy anchor method to capture intra-modality relationships while the feature fusion module uses a Cross-Modal and Tri-Transformer for effective integration. The final fake news detector processes the fused features through a classifier to predict the authenticity of the content. Experiments on datasets show that CroMe excels in multimodal fake news detection.', 'abstract_zh': '多模态虚假新闻检测近年来受到了越来越多的关注。现有方法依赖于独立编码的单模态数据，并且忽视了通过先进技术捕捉跨模态关系和集成跨模态相似性的优势。为了解决这些问题，提出了跨模态三变换器和度量学习方法用于多模态虚假新闻检测（CroMe）。CroMe 使用冻结图像编码器和大型语言模型的Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2)作为编码器，以捕获详细的文本、图像和图像-文本表示。度量学习模块采用代理锚点方法以捕捉跨模态关系，特征融合模块则使用跨模态三变换器进行有效的集成。最终的虚假新闻检测器通过分类器处理融合后的特征，以预测内容的真实性。实验结果显示，CroMe 在多模态虚假新闻检测方面表现出色。', 'title_zh': 'CroMe：基于跨模态三重变换和度量学习的多模态假新闻检测'}
{'arxiv_id': 'arXiv:2501.12418', 'title': 'ImageRef-VL: Enabling Contextual Image Referencing in Vision-Language Models', 'authors': 'Jingwei Yi, Junhao Yin, Ju Xu, Peng Bao, Yongliang Wang, Wei Fan, Hao Wang', 'link': 'https://arxiv.org/abs/2501.12418', 'abstract': "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in understanding multimodal inputs and have been widely integrated into Retrieval-Augmented Generation (RAG) based conversational systems. While current VLM-powered chatbots can provide textual source references in their responses, they exhibit significant limitations in referencing contextually relevant images during conversations. In this paper, we introduce Contextual Image Reference -- the ability to appropriately reference relevant images from retrieval documents based on conversation context -- and systematically investigate VLMs' capability in this aspect. We conduct the first evaluation for contextual image referencing, comprising a dedicated testing dataset and evaluation metrics. Furthermore, we propose ImageRef-VL, a method that significantly enhances open-source VLMs' image referencing capabilities through instruction fine-tuning on a large-scale, manually curated multimodal conversation dataset. Experimental results demonstrate that ImageRef-VL not only outperforms proprietary models but also achieves an 88% performance improvement over state-of-the-art open-source VLMs in contextual image referencing tasks. Our code is available at this https URL.", 'abstract_zh': '视觉-语言模型（VLMs）已经在理解和处理多模态输入方面展现了显著的能力，并被广泛应用于检索增强生成（RAG）为基础的对话系统中。尽管当前由VLM驱动的聊天机器人能够在其响应中提供文本来源引用，但在对话过程中引用上下文相关图像方面仍表现出明显的局限性。本文中，我们提出了上下文图像引用——基于对话上下文，准确引用检索文档中的相关图像的能力，并系统地探讨了VLMs在这方面的能力。我们首次对上下文图像引用进行了评估，包括一个专门的测试数据集和评估指标。此外，我们提出了一种名为ImageRef-VL的方法，该方法通过在大规模的手动筛选多模态对话数据集上进行指令微调，显著提高了开源VLMs的图像引用能力。实验结果表明，ImageRef-VL不仅优于专有模型，而且在上下文图像引用任务中相对于最先进开源VLMs的性能提升了88%。我们的代码可在以下链接获取：[请插入链接]。', 'title_zh': 'ImageRef-VL：在视觉语言模型中实现上下文图像引用'}
{'arxiv_id': 'arXiv:2501.12751', 'title': 'Patent Figure Classification using Large Vision-language Models', 'authors': 'Sushil Awale, Eric Müller-Budack, Ralph Ewerth', 'link': 'https://arxiv.org/abs/2501.12751', 'abstract': 'Patent figure classification facilitates faceted search in patent retrieval systems, enabling efficient prior art search. Existing approaches have explored patent figure classification for only a single aspect and for aspects with a limited number of concepts. In recent years, large vision-language models (LVLMs) have shown tremendous performance across numerous computer vision downstream tasks, however, they remain unexplored for patent figure classification. Our work explores the efficacy of LVLMs in patent figure visual question answering (VQA) and classification, focusing on zero-shot and few-shot learning scenarios. For this purpose, we introduce new datasets, PatFigVQA and PatFigCLS, for fine-tuning and evaluation regarding multiple aspects of patent figures~(i.e., type, projection, patent class, and objects). For a computational-effective handling of a large number of classes using LVLM, we propose a novel tournament-style classification strategy that leverages a series of multiple-choice questions. Experimental results and comparisons of multiple classification approaches based on LVLMs and Convolutional Neural Networks (CNNs) in few-shot settings show the feasibility of the proposed approaches.', 'abstract_zh': '专利图分类有助于专利检索系统中的多方面搜索，从而实现高效的先有技术搜索。现有方法仅针对单一方面或概念数量有限的方面进行了专利图分类的探索。近年来，大型视觉-语言模型（LVLM）在众多计算机视觉下游任务中表现出色，但在专利图分类方面尚未得到应用。我们的研究探讨了LVLM在专利图视觉问答（VQA）和分类中的有效性，重点关注零样本学习和少量样本学习场景。为实现这一目的，我们引入了新的数据集 PatFigVQA 和 PatFigCLS，用于针对专利图的多个方面（包括类型、投影、专利类别和对象）进行微调和评估。为了有效地处理大量类别，我们提出了一种新颖的淘汰赛式分类策略，利用一系列多项选择题。基于 LVLM 和卷积神经网络（CNN）的多种分类方法的实验结果和对比表明，所提出的方法具有可行性。', 'title_zh': '使用大型视觉-语言模型进行专利图分类'}
{'arxiv_id': 'arXiv:2501.13042', 'title': 'Does Table Source Matter? Benchmarking and Improving Multimodal Scientific Table Understanding and Reasoning', 'authors': 'Bohao Yang, Yingji Zhang, Dong Liu, André Freitas, Chenghua Lin', 'link': 'https://arxiv.org/abs/2501.13042', 'abstract': 'Recent large language models (LLMs) have advanced table understanding capabilities but rely on converting tables into text sequences. While multimodal large language models (MLLMs) enable direct visual processing, they face limitations in handling scientific tables due to fixed input image resolutions and insufficient numerical reasoning capabilities. We present a comprehensive framework for multimodal scientific table understanding and reasoning with dynamic input image resolutions. Our framework consists of three key components: (1) MMSci-Pre, a domain-specific table structure learning dataset of 52K scientific table structure recognition samples, (2) MMSci-Ins, an instruction tuning dataset with 12K samples across three table-based tasks, and (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically designed to evaluate numerical reasoning capabilities. Extensive experiments demonstrate that our domain-specific approach with 52K scientific table images achieves superior performance compared to 150K general-domain tables, highlighting the importance of data quality over quantity. Our proposed table-based MLLMs with dynamic input resolutions show significant improvements in both general table understanding and numerical reasoning capabilities, with strong generalisation to held-out datasets. Our code and data are publicly available at this https URL.', 'abstract_zh': '近年来，大型语言模型（LLMs）在表格理解方面取得了显著进展，但它们依赖于将表格转换为文本序列。而多模态大型语言模型（MLLMs）能够直接处理视觉信息，但由于固定输入图像分辨率和不足的数值推理能力，它们在处理科学表格方面面临限制。我们提出了一种全面的框架，用于处理具有动态输入图像分辨率的多模态科学表格理解和推理。我们的框架包括三个关键组件：（1）MMSci-Pre，一个包含52,000个科学表格结构识别样本的专业领域表格结构学习数据集；（2）MMSci-Ins，一个包含12,000个样本的任务指令调优数据集，涉及三种基于表格的任务；（3）MMSci-Eval，一个包含3,114个测试样本的基准数据集，专门设计用于评估数值推理能力。大量实验表明，我们使用52,000个科学表格图像的专业领域方法优于使用150,000个通用领域的表格，突显了数据质量而非数量的重要性。我们提出的支持动态输入分辨率的基于表格的MLLMs在通用表格理解和数值推理能力方面显示出显著改进，并且对未见过的数据集具有强大的泛化能力。我们将在以下链接公开发布代码和数据：this https URL。', 'title_zh': '《表格数据源重要吗？跨模态科学表格理解与推理的基准测试及提升》'}
