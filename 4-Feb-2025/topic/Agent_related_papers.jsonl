{'arxiv_id': 'arXiv:2502.01630', 'title': 'TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues', 'authors': 'Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Monica Sunkara, Yassine Benajiba, Yi Zhang', 'link': 'https://arxiv.org/abs/2502.01630', 'abstract': 'Temporal reasoning in multi-session dialogues presents a significant challenge which has been under-studied in previous temporal reasoning benchmarks. To bridge this gap, we propose a new evaluation task for temporal reasoning in multi-session dialogues and introduce an approach to construct a new benchmark by augmenting dialogues from LoCoMo and creating multi-choice QAs. Furthermore, we present TReMu, a new framework aimed at enhancing the temporal reasoning capabilities of LLM-agents in this context. Specifically, the framework employs \\textit{time-aware memorization} through timeline summarization, generating retrievable memory by summarizing events in each dialogue session with their inferred dates. Additionally, we integrate \\textit{neuro-symbolic temporal reasoning}, where LLMs generate Python code to perform temporal calculations and select answers. Experimental evaluations on popular LLMs demonstrate that our benchmark is challenging, and the proposed framework significantly improves temporal reasoning performance compared to baseline methods, raising from 29.83 on GPT-4o via standard prompting to 77.67 via our approach and highlighting its effectiveness in addressing temporal reasoning in multi-session dialogues.', 'abstract_zh': '多会话对话中的时间推理提出了一个重要的挑战，而这一挑战在之前的时序推理基准中尚未得到充分的研究。为解决这一问题，我们提出了一项新的评价任务，旨在评估多会话对话中的时间推理能力，并通过增强Loremotion数据集中的对话，构造了一个新的基准，并创建了多项选择题。此外，我们提出了TReMu框架，该框架旨在增强在这种情况下LLM代理的时间推理能力。具体而言，该框架通过时间轴总结，采用了具有时间意识的记忆化方法，生成可检索的记忆，通过总结每个对话会话中的事件及其推断日期来生成摘要。我们还整合了神经符号时间推理，其中LLM生成Python代码以执行时间计算并选择答案。对流行的基础模型进行的实验评估表明，我们的基准具有挑战性，并且所提的框架在时间推理性能上显著优于基线方法，评分从GPT-4o标准提示下的29.83提高到我们方法下的77.67，突显了其在解决多会话对话中时间推理问题方面的有效性。', 'title_zh': 'TReMu：面向具有记忆功能的多会话对话中LLM代理的神经符号时间推理'}
{'arxiv_id': 'arXiv:2502.01503', 'title': 'Sea-cret Agents: Maritime Abduction for Region Generation to Expose Dark Vessel Trajectories', 'authors': 'Divyagna Bavikadi, Nathaniel Lee, Paulo Shakarian, Chad Parvis', 'link': 'https://arxiv.org/abs/2502.01503', 'abstract': "Bad actors in the maritime industry engage in illegal behaviors after disabling their vessel's automatic identification system (AIS) - which makes finding such vessels difficult for analysts. Machine learning approaches only succeed in identifying the locations of these ``dark vessels'' in the immediate future. This work leverages ideas from the literature on abductive inference applied to locating adversarial agents to solve the problem. Specifically, we combine concepts from abduction, logic programming, and rule learning to create an efficient method that approaches full recall of dark vessels while requiring less search area than machine learning methods. We provide a logic-based paradigm for reasoning about maritime vessels, an abductive inference query method, an automatically extracted rule-based behavior model methodology, and a thorough suite of experiments.", 'abstract_zh': '海洋行业中的一些不良行为者通过禁用船舶的自动识别系统（AIS）来从事非法行为，使得分析师难以寻找这些船舶。仅依靠机器学习方法在未来短期内确实可以找到这些“隐形船舶”的位置。本研究借鉴了关于归因推理在定位敌对代理研究中的方法来解决这一问题。具体而言，我们结合了归因推理、逻辑编程和规则学习的概念，提出了一种高效的方法，既能实现对隐形船舶的全面召回，又能比机器学习方法减少搜索区域。我们提供了一种基于逻辑的船舶推理框架，一种归因推理查询方法，一种自动提取的行为规则模型方法，以及一系列详尽的实验研究。', 'title_zh': '《海-secret代理：海洋劫持方法用于区域生成以揭示隐蔽船舶轨迹》'}
{'arxiv_id': 'arXiv:2502.01492', 'title': 'Develop AI Agents for System Engineering in Factorio', 'authors': 'Neel Kant', 'link': 'https://arxiv.org/abs/2502.01492', 'abstract': "Continuing advances in frontier model research are paving the way for widespread deployment of AI agents. Meanwhile, global interest in building large, complex systems in software, manufacturing, energy and logistics has never been greater. Although AI driven system engineering holds tremendous promise, the static benchmarks dominating agent evaluations today fail to capture the crucial skills required for implementing dynamic systems, such as managing uncertain trade-offs and ensuring proactive adaptability. This position paper advocates for training and evaluating AI agents' system engineering abilities through automation-oriented sandbox games-particularly Factorio. By directing research efforts in this direction, we can equip AI agents with the specialized reasoning and long-horizon planning necessary to design, maintain, and optimize tomorrow's most demanding engineering projects.", 'abstract_zh': '不断推进前沿模型研究正为人工智能代理的广泛应用铺平道路。与此同时，全球在软件、制造、能源和物流等领域构建大型复杂系统的需求前所未有的强烈。虽然基于人工智能的系统工程具有巨大的潜力，但当前主导代理评估中的静态基准未能捕捉到实施动态系统所需的关键技能，如管理不确定的权衡和确保积极的适应性。本文倡议通过自动化导向的沙盒游戏（特别是Factorio）来训练和评估人工智能代理的系统工程能力。通过将研究方向导向这一领域，我们可以使人工智能代理具备设计、维护和优化未来最具挑战性的工程项目的专门推理能力和长期规划能力。', 'title_zh': '在Factorio中开发系统工程的人工智能代理'}
{'arxiv_id': 'arXiv:2502.01387', 'title': 'TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning', 'authors': 'Chengkai Xu, Jiaqi Liu, Peng Hang, Jian Sun', 'link': 'https://arxiv.org/abs/2502.01387', 'abstract': "Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) each show promise in addressing decision-making challenges in autonomous driving, DRL often suffers from high sample complexity, while LLMs have difficulty ensuring real-time decision making. To address these limitations, we propose TeLL-Drive, a hybrid framework that integrates an Teacher LLM to guide an attention-based Student DRL policy. By incorporating risk metrics, historical scenario retrieval, and domain heuristics into context-rich prompts, the LLM produces high-level driving strategies through chain-of-thought reasoning. A self-attention mechanism then fuses these strategies with the DRL agent's exploration, accelerating policy convergence and boosting robustness across diverse driving conditions. Our experimental results, evaluated across multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline methods, including other LLM-based approaches, in terms of success rates, average returns, and real-time feasibility. Ablation studies underscore the importance of each model component, especially the synergy between the attention mechanism and LLM-driven guidance. These findings suggest that TeLL-Drive significantly enhances both the adaptability and safety of autonomous driving systems, while offering a more efficient and scalable approach for policy learning. Full validation results are available on our website.", 'abstract_zh': '尽管深度强化学习（DRL）和大规模语言模型（LLMs）在自动驾驶决策问题上各具潜力，但DRL往往面临样本复杂性高的问题，而LLMs则难以保证实时决策。为了解决这些局限性，我们提出了一种混合框架——TeLL-Drive，该框架结合了一个指导型的教师LLM，以引导基于注意力的学生DRL策略。通过对包含风险指标、历史场景检索和领域启发式信息的语境提示进行推理，LLM生成了高层次的驾驶策略。随后，自注意力机制将这些策略与DRL代理的探索相结合，加快了策略收敛速度，并提高了在各种驾驶条件下的鲁棒性。我们的实验结果，跨越多个交通场景进行评估，表明TeLL-Drive在成功率、平均回报以及实时可行性方面优于现有基准方法，包括其他LLM基方法。消融研究强调了每个模型组件的重要性，特别是注意力机制与LLM驱动指导之间的协同作用。这些发现表明，TeLL-Drive显著增强了自动驾驶系统的适应性和安全性，同时提供了一种更高效、更可扩展的策略学习方法。完整的验证结果可在我们的网站上查阅。', 'title_zh': 'TeLL-Drive：借助教师大语言模型引导的深度强化学习增强自主驾驶'}
{'arxiv_id': 'arXiv:2502.00858', 'title': 'Learning to Plan with Personalized Preferences', 'authors': 'Manjie Xu, Xinyi Yang, Wei Liang, Chi Zhang, Yixin Zhu', 'link': 'https://arxiv.org/abs/2502.00858', 'abstract': "Effective integration of AI agents into daily life requires them to understand and adapt to individual human preferences, particularly in collaborative roles. Although recent studies on embodied intelligence have advanced significantly, they typically adopt generalized approaches that overlook personal preferences in planning. We address this limitation by developing agents that not only learn preferences from few demonstrations but also learn to adapt their planning strategies based on these preferences. Our research leverages the observation that preferences, though implicitly expressed through minimal demonstrations, can generalize across diverse planning scenarios. To systematically evaluate this hypothesis, we introduce Preference-based Planning (PbP) benchmark, an embodied benchmark featuring hundreds of diverse preferences spanning from atomic actions to complex sequences. Our evaluation of SOTA methods reveals that while symbol-based approaches show promise in scalability, significant challenges remain in learning to generate and execute plans that satisfy personalized preferences. We further demonstrate that incorporating learned preferences as intermediate representations in planning significantly improves the agent's ability to construct personalized plans. These findings establish preferences as a valuable abstraction layer for adaptive planning, opening new directions for research in preference-guided plan generation and execution.", 'abstract_zh': '有效将AI代理融入日常生活中要求它们能够理解并适应个人人类偏好，特别是在协作角色中。尽管近年来关于具身智能的研究取得了显著进展，但这些研究通常采用通用的方法，忽略了规划中个人偏好这一因素。我们通过开发既能从少量示范中学到偏好，又能根据这些偏好调整其规划策略的代理来弥补这一局限。我们的研究利用了这样一个观察：尽管偏好是通过最少的示范而隐式表达的，但它们能够泛化到多种多样的规划场景中。为了系统地检验这一假设，我们引入了基于偏好规划（PbP）基准，该基准是具身环境下的基准测试，涵盖了数百种不同的偏好，从原子动作到复杂的序列。对当前最先进的方法的评估表明，虽然基于符号的方法显示出规模化的潜力，但在学习生成和执行满足个性化偏好的计划方面仍存在重大挑战。进一步的研究还表明，将学到的偏好作为规划中的中间表示可以显著提高代理构建个性化计划的能力。这些发现确立了偏好作为适应性规划中的有价值抽象层的地位，为基于偏好引导的计划生成和执行的研究开启了新方向。', 'title_zh': '具备个性化偏好的规划学习'}
{'arxiv_id': 'arXiv:2502.00792', 'title': 'RTBAgent: A LLM-based Agent System for Real-Time Bidding', 'authors': 'Leng Cai, Junxuan He, Yikai Li, Junjie Liang, Yuanping Lin, Ziming Quan, Yawen Zeng, Jin Xu', 'link': 'https://arxiv.org/abs/2502.00792', 'abstract': 'Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: this https URL.', 'abstract_zh': '实时竞价（RTB）允许广告商在瞬间对展示机会进行竞争性出价，以在高度竞争的环境中追求成本效益。尽管RTB从深度学习和强化学习等技术的应用中广泛受益，但由于在线和离线环境之间的差异以及在线竞价的快速波动，相关方法的可靠性常常会遇到挑战。为了应对这些挑战，我们提出了基于大型语言模型（LLMs）的首个RTB代理系统——RTBAgent，该系统同步了真实的竞争广告竞价环境，并通过集成决策过程获取竞价价格。具体来说，通过大型语言模型（LLMs）获得推理能力后，RTBAgent进一步通过包含辅助模块（如点击率估计模型、专家策略知识和每日反思）来更加专业化地适应RTB需求。此外，我们提出了两步决策过程和多记忆检索机制，使RTBAgent能够回顾历史决策和交易记录，并在实时竞价中更加适应市场变化做出决策。实证测试使用真实的广告数据集表明，RTBAgent显著提高了盈利能力。RTBAgent的代码将在以下网址公开访问：this https URL。', 'title_zh': 'RTBAgent：一个基于大语言模型的实时竞价代理系统'}
{'arxiv_id': 'arXiv:2502.00726', 'title': 'Perspectives for Direct Interpretability in Multi-Agent Deep Reinforcement Learning', 'authors': 'Yoann Poupart, Aurélie Beynier, Nicolas Maudet', 'link': 'https://arxiv.org/abs/2502.00726', 'abstract': "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in solving complex problems in robotics or games, yet most of the trained models are hard to interpret. While learning intrinsically interpretable models remains a prominent approach, its scalability and flexibility are limited in handling complex tasks or multi-agent dynamics. This paper advocates for direct interpretability, generating post hoc explanations directly from trained models, as a versatile and scalable alternative, offering insights into agents' behaviour, emergent phenomena, and biases without altering models' architectures. We explore modern methods, including relevance backpropagation, knowledge edition, model steering, activation patching, sparse autoencoders and circuit discovery, to highlight their applicability to single-agent, multi-agent, and training process challenges. By addressing MADRL interpretability, we propose directions aiming to advance active topics such as team identification, swarm coordination and sample efficiency.", 'abstract_zh': '多智能体深度强化学习（Multi-Agent Deep Reinforcement Learning, MADRL）在解决机器人或游戏中的复杂问题上已被证明是有效的，但大多数训练好的模型难以解释。尽管学习固有可解释的模型仍然是一个显着的方法，但在处理复杂任务或多智能体动态方面，这种方法的扩展性和灵活性仍受到限制。本文提倡直接可解释性，直接从训练好的模型中生成事后解释，作为一种灵活且可扩展的替代方案，无需修改模型架构即可提供对智能体行为、涌现现象和偏差的深入见解。我们探讨了现代方法，包括相关反向传播、知识编辑、模型引导、激活补丁、稀疏自动编码器和电路发现，以展示其在单智能体、多智能体及训练过程挑战方面的适用性。通过对MADRL可解释性的探讨，我们提出了一些方向，旨在推进团队识别、群集协调和样本效率等活跃研究领域的发展。', 'title_zh': '直接可解释性在多agent深度强化学习中的前景'}
{'arxiv_id': 'arXiv:2502.00648', 'title': 'Agency in the Age of AI', 'authors': 'Samarth Swarup', 'link': 'https://arxiv.org/abs/2502.00648', 'abstract': 'There is significant concern about the impact of generative AI on society. Modern AI tools are capable of generating ever more realistic text, images, and videos, and functional code, from minimal prompts. Accompanying this rise in ability and usability, there is increasing alarm about the misuses to which these tools can be put, and the intentional and unintentional harms to individuals and society that may result. In this paper, we argue that \\emph{agency} is the appropriate lens to study these harms and benefits, but that doing so will require advancement in the theory of agency, and advancement in how this theory is applied in (agent-based) models.', 'abstract_zh': '关于生成式AI对社会的影响，存在显著的关切。现代AI工具能够从少量提示生成越来越逼真的文本、图像和视频，以及功能性代码。伴随着这种能力和使用性的提升，人们对这些工具可能被滥用的担忧也在增加，并可能对个人和社会造成有意或无意的危害。在本文中，我们主张应该使用“agency”（自主权/能力）这一视角来研究这些危害和益处，但要实现这一目标，需要在自主权理论方面进行深入探讨，并改进这种理论在基于代理的模型中的应用。', 'title_zh': 'AI时代的代理问题'}
{'arxiv_id': 'arXiv:2502.00406', 'title': 'ALU: Agentic LLM Unlearning', 'authors': 'Debdeep Sanyal, Murari Mandal', 'link': 'https://arxiv.org/abs/2502.00406', 'abstract': "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU's superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.", 'abstract_zh': '大型语言模型（LLM）的信息移除或抑制是 desirable 功能，有助于 AI 监管、法律合规、安全性和隐私保护。当前的 LLM 去学习方法在平衡去学习效果和实用性方面遇到困难，因为这两个目标是相互竞争的。在不假设可以访问模型权重的情况下，保持去学习过程的计算可行性是一个被忽视的领域。我们介绍了第一个自主去学习（ALU）方法，这是一种多智能体、免重训、模型无关的 LLM 去学习方法，可以在有效去学习的同时保留实用性。我们的 ALU 框架通过涉及多个 LLM 代理来实现去学习，每个代理针对去学习过程中的特定步骤进行设计，而无需更新框架中任何代理的模型权重。用户可以轻松地按任何顺序请求任何一组去学习实例，并且 ALU 可以在真实时间内无缝适应。这无需对底层 LLM 模型进行任何修改即可实现。通过在建立的基准测试（TOFU、WMDP、WPU）和破解技术（多重射击、目标遮蔽、其他语言）上进行广泛的实验，我们证明 ALU 在当前先进方法中始终是最稳健的 LLM 去学习框架，同时具有较低的恒定时间开销。此外，我们还强调了 ALU 在大规模评估中与现有方法相比的优越性能。具体而言，ALU 在至多 1000 个去学习目标的评估中表现优异，超过了所有之前提出的 LLM 去学习方法的评估范围。', 'title_zh': 'ALU：自主性的大规模语言模型去学习'}
{'arxiv_id': 'arXiv:2502.00352', 'title': 'A Differentiated Reward Method for Reinforcement Learning based Multi-Vehicle Cooperative Decision-Making Algorithms', 'authors': 'Ye Han, Lijun Zhang, Dejian Meng', 'link': 'https://arxiv.org/abs/2502.00352', 'abstract': 'Reinforcement learning (RL) shows great potential for optimizing multi-vehicle cooperative driving strategies through the state-action-reward feedback loop, but it still faces challenges such as low sample efficiency. This paper proposes a differentiated reward method based on steady-state transition systems, which incorporates state transition gradient information into the reward design by analyzing traffic flow characteristics, aiming to optimize action selection and policy learning in multi-vehicle cooperative decision-making. The performance of the proposed method is validated in RL algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle penetration. The results show that the differentiated reward method significantly accelerates training convergence and outperforms centering reward and others in terms of traffic efficiency, safety, and action rationality. Additionally, the method demonstrates strong scalability and environmental adaptability, providing a novel approach for multi-agent cooperative decision-making in complex traffic scenarios.', 'abstract_zh': '强化学习（RL）在通过状态-动作-奖励反馈循环优化多车辆协同驾驶策略方面显示出巨大潜力，但仍然面临着样本效率低等挑战。本文提出了一种基于稳态转换系统的差异化奖励方法，通过分析交通流特征将状态转换梯度信息纳入奖励设计中，旨在优化多车辆协同决策中的动作选择和策略学习。该方法在不同的自主车辆渗透率下，通过MAPPO、MADQN和QMIX等RL算法进行了性能验证。结果显示，差异化奖励方法在交通效率、安全性和动作合理性方面显著加快了训练收敛速度，并优于中心化奖励及其他方法。此外，该方法展示了强大的可扩展性和环境适应性，为复杂交通场景下的多智能体协同决策提供了一种新的解决方案。', 'title_zh': '基于多车辆协同决策算法的差异化奖励方法'}
{'arxiv_id': 'arXiv:2502.00022', 'title': 'A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs', 'authors': 'Xingyu Xiao, Peng Chen, Qianqian Jia, Jiejuan Tong, Jingang Liang, Haitao Wang', 'link': 'https://arxiv.org/abs/2502.00022', 'abstract': 'HRA (Human Reliability Analysis) data is crucial for advancing HRA methodologies. however, existing data collection methods lack the necessary granularity, and most approaches fail to capture dynamic features. Additionally, many methods require expert knowledge as input, making them time-consuming and labor-intensive. To address these challenges, we propose a new paradigm for the automated collection of HRA data. Our approach focuses on key indicators behind human error, specifically measuring workload in collaborative settings. This study introduces a novel, scenario-driven method for workload estimation, leveraging fine-tuned large language models (LLMs). By training LLMs on real-world operational data from high-temperature gas-cooled reactors (HTGRs), we simulate human behavior and cognitive load in real time across various collaborative scenarios. The method dynamically adapts to changes in operator workload, providing more accurate, flexible, and scalable workload estimates. The results demonstrate that the proposed WELLA (Workload Estimation with LLMs and Agents) outperforms existing commercial LLM-based methods in terms of prediction accuracy.', 'abstract_zh': '人类可靠性分析（HRA）数据对于推进HRA方法具有重要意义。然而，现有的数据收集方法缺乏必要的粒度，且大多数方法无法捕捉动态特征。此外，许多方法需要专家知识作为输入，这使得它们耗时且劳动密集。为应对这些挑战，我们提出了一种新的自动化收集HRA数据的范式。我们的方法集中在影响人类错误的关键指标上，特别关注协作环境中的工作负荷测量。本研究引入了一种基于场景的新颖方法来估算工作负荷，利用细调后的大型语言模型（LLMs）。通过使用高温气冷堆（HTGRs）的真实操作数据对LLMs进行训练，我们实时模拟了各种协作场景下的人类行为和认知负荷。该方法能够动态适应操作员工作负荷的变化，提供更准确、灵活和可扩展的工作负荷估计。结果表明，提出的WELLA（使用LLM和代理的工作负荷估算）在预测准确性方面优于现有的商业LLM基方法。', 'title_zh': '基于大型语言模型驱动的多agent协作环境中情景基于的人因工程合成数据动态高精度采集方法'}
{'arxiv_id': 'arXiv:2502.01635', 'title': 'The AI Agent Index', 'authors': 'Stephen Casper, Luke Bailey, Rosco Hunter, Carson Ezell, Emma Cabalé, Michael Gerovitch, Stewart Slocum, Kevin Wei, Nikola Jurkovic, Ariba Khan, Phillip J.K. Christoffersen, A. Pinar Ozisik, Rakshit Trivedi, Dylan Hadfield-Menell, Noam Kolt', 'link': 'https://arxiv.org/abs/2502.01635', 'abstract': "Leading AI developers and startups are increasingly deploying agentic AI systems that can plan and execute complex tasks with limited human involvement. However, there is currently no structured framework for documenting the technical components, intended uses, and safety features of agentic systems. To fill this gap, we introduce the AI Agent Index, the first public database to document information about currently deployed agentic AI systems. For each system that meets the criteria for inclusion in the index, we document the system's components (e.g., base model, reasoning implementation, tool use), application domains (e.g., computer use, software engineering), and risk management practices (e.g., evaluation results, guardrails), based on publicly available information and correspondence with developers. We find that while developers generally provide ample information regarding the capabilities and applications of agentic systems, they currently provide limited information regarding safety and risk management practices. The AI Agent Index is available online at this https URL", 'abstract_zh': '领先的AI开发者和初创企业正越来越多地部署自主型AI系统，这些系统能够在有限的人为干预下规划和执行复杂的任务。然而，当前缺乏一个结构化的框架来记录自主型系统的技术组件、预期用途和安全性特征。为填补这一空白，我们引入了AI Agent Index，这是第一个公开数据库，用于记录当前部署的自主型AI系统的相关信息。对于每个符合索引收录标准的系统，我们根据公开信息和与开发者的交流记录了该系统的组件（例如，基础模型、推理实现、工具使用）、应用领域（例如，计算机使用、软件工程）以及风险管理实践（例如，评估结果、防护措施）。我们发现，虽然开发者通常能提供大量关于自主型系统功能和应用的信息，但他们目前在安全性和风险管理实践方面的信息相对有限。AI Agent Index 可通过以下网址在线访问：[该网址]', 'title_zh': '《AI代理指数》'}
{'arxiv_id': 'arXiv:2502.01600', 'title': 'Reinforcement Learning for Long-Horizon Interactive LLM Agents', 'authors': 'Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, Philipp Krähenbühl', 'link': 'https://arxiv.org/abs/2502.01600', 'abstract': 'Interactive digital agents (IDAs) leverage APIs of stateful digital environments to perform tasks in response to user requests. While IDAs powered by instruction-tuned large language models (LLMs) can react to feedback from interface invocations in multi-step exchanges, they have not been trained in their respective digital environments. Prior methods accomplish less than half of tasks in sophisticated benchmarks such as AppWorld. We present a reinforcement learning (RL) approach that trains IDAs directly in their target environments. We formalize this training as a partially observable Markov decision process and derive M-PPO, a data- and memory-efficient variant of proximal policy optimization. M-PPO uses no value network and maintains exactly one copy of the underlying LLM in memory, making its implementation straightforward and as memory-efficient as fine-tuning a single LLM. A 32-billion-parameter agent trained with M-PPO in the AppWorld environment outperforms the much larger OpenAI o1 agent by 9 percentage points (15% relative). To our knowledge, this is the first reported application of RL to IDAs that interact with a stateful, multi-domain, multi-app environment via direct API calls. Our analysis sheds light on the effectiveness of RL in this area, showing that the agent learns to consult the API documentation, avoid unwarranted assumptions, minimize confabulation, and recover from setbacks.', 'abstract_zh': '交互式数字代理（IDAs）利用状态型数字环境的API来响应用户请求并执行任务。由指令调优的大语言模型（LLMs）驱动的IDAs可以在多步骤交互中对界面调用的反馈做出反应，但它们并未在其各自的数字环境中进行训练。此前的方法在复杂的基准测试（如AppWorld）中仅能完成不到一半的任务。我们提出了一个利用强化学习（RL）的训练方法，直接在目标环境中训练IDAs。我们将这一训练过程形式化为部分可观测的马尔可夫决策过程，并推导出M-PPO，这是一个数据和内存效率更高的 proximal policy optimization 变体。M-PPO 不使用价值网络，并且在内存中仅仅维持一个底层LLM的副本，从而使得其实现简单，并且内存效率与对单一LLM的调优相当。在一个具有320亿参数的代理在AppWorld环境中通过M-PPO训练后，其表现超过了OpenAI的更大型o1代理9个百分点（即15%的相对改进）。据我们所知，这是首次将RL应用于通过直接API调用来与状态型、多域、多应用环境交互的IDAs。我们的分析揭示了在这一领域中RL的有效性，表明该代理学会了查阅API文档、避免不切实际的假设、减少虚构信息，并能从挫折中恢复。', 'title_zh': '长时 horizon 交互式大型语言模型智能体的强化学习方法'}
{'arxiv_id': 'arXiv:2502.01450', 'title': 'Simulating Rumor Spreading in Social Networks using LLM Agents', 'authors': 'Tianrui Hu, Dimitrios Liakopoulos, Xiwen Wei, Radu Marculescu, Neeraja J. Yadwadkar', 'link': 'https://arxiv.org/abs/2502.01450', 'abstract': 'With the rise of social media, misinformation has become increasingly prevalent, fueled largely by the spread of rumors. This study explores the use of Large Language Model (LLM) agents within a novel framework to simulate and analyze the dynamics of rumor propagation across social networks. To this end, we design a variety of LLM-based agent types and construct four distinct network structures to conduct these simulations. Our framework assesses the effectiveness of different network constructions and agent behaviors in influencing the spread of rumors. Our results demonstrate that the framework can simulate rumor spreading across more than one hundred agents in various networks with thousands of edges. The evaluations indicate that network structure, personas, and spreading schemes can significantly influence rumor dissemination, ranging from no spread to affecting 83\\% of agents in iterations, thereby offering a realistic simulation of rumor spread in social networks.', 'abstract_zh': '随着社交媒体的兴起，虚假信息日益盛行，很大程度上是由于谣言的传播。本研究旨在探索在新型框架中使用大型语言模型（LLM）代理模拟和分析社交网络中谣言传播的动力学。为此，我们设计了多种基于LLM的代理类型，并构建了四种不同的网络结构进行这些模拟。我们的框架评估了不同网络结构和代理行为对谣言传播的影响效果。研究结果表明，该框架可以用于在各种网络中模拟超过一百个代理的谣言传播，涉及数千条边。评估结果表明，网络结构、代理特征以及传播方案可以显著影响谣言的传播范围，从不传播到在迭代过程中影响83%的代理，从而为社交网络中的谣言传播提供了一个现实的模拟。', 'title_zh': '使用大语言模型代理模拟社交网络中的谣言传播'}
{'arxiv_id': 'arXiv:2502.01316', 'title': 'Learning Fused State Representations for Control from Multi-View Observations', 'authors': 'Zeyu Wang, Yao-Hui Li, Xin Li, Hongyu Zang, Romain Laroche, Riashat Islam', 'link': 'https://arxiv.org/abs/2502.01316', 'abstract': "Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.", 'abstract_zh': '多视图强化学习（MVRL）旨在为智能体提供多视图观察，使其能够以更高的准确性和有效性感知环境。近年来，MVRL 的进展主要集中在从多视图观察中提取潜在表示，并在控制任务中利用这些表示。然而，在存在冗余信息、干扰信息或缺失视图的情况下，学习紧凑且任务相关的表示并不容易。在本文中，我们提出了一种名为多视图融合状态用于控制（MFSC）的方法，首次将bisimulation度量学习集成到MVRL中，以学习任务相关的表示。此外，我们提出了一种基于多视图的掩码和潜在重构辅助任务，利用跨视图的共享信息并引入掩码令牌以提高MFSC在缺失视图情况下的鲁棒性。广泛的实验结果表明，我们的方法在多视图强化学习任务中优于现有方法。即使在存在干扰或视图缺失的更现实场景中，MFSC也表现出稳定的高度性能。', 'title_zh': '从多视角观察中学习融合状态表示的控制算法'}
{'arxiv_id': 'arXiv:2502.01236', 'title': 'Eliciting Language Model Behaviors with Investigator Agents', 'authors': 'Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson, Tatsunori Hashimoto, Percy Liang, Sarah Schwettmann, Jacob Steinhardt', 'link': 'https://arxiv.org/abs/2502.01236', 'abstract': 'Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate.', 'abstract_zh': '当语言模型接收到自由格式文本的提示时，会表现出复杂多样的行为，这使得难以为其可能输出的空间提供一个全面的表征。我们研究了行为激发的问题，即目标是寻找能够从目标语言模型中诱发特定目标行为（例如虚构或有害响应）的提示。为了在指数级庞大的潜在提示空间中导航，我们训练了调查员模型，使其能够将随机选择的目标行为映射到能够激发其行为的多样化输出分布中，类似于可延时化贝叶斯推断。我们通过监督微调、基于DPO的强化学习以及一个新颖的Frank-Wolfe训练目标，迭代地发现多样化的提示策略。我们的调查员模型揭示了各种有效且人类可解释的提示，这些提示导致了模型突破、虚构行为和开放式的异常行为，在AdvBench（有害行为）的一部分数据集上实现了100%的攻击成功率，并且引发了85%的虚构响应率。', 'title_zh': '使用调查代理 eliciting 语言模型行为'}
{'arxiv_id': 'arXiv:2502.01218', 'title': 'Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents', 'authors': 'Zhizhen Zhang, Lei Zhu, Zhen Fang, Zi Huang, Yadan Luo', 'link': 'https://arxiv.org/abs/2502.01218', 'abstract': 'Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments across varying numbers of demonstrations show that the pretrained features significantly enhance downstream manipulation tasks by up to 49% with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents. The source code is included in the supplementary material for reference.', 'abstract_zh': '在人类动作视频上预训练视觉-语言表示已成为减少对大规模专家示范依赖的有效方法，从而训练具身智能体。然而，以往的方法往往基于目标到达的启发式方法使用时间对比学习，逐步将语言指令从初始帧对齐到最终帧。这种过度关注未来帧的做法可能导致视觉-语言关联错误，因为动作可能在早期终止或包含不相关的时间片段。为了解决这个问题，我们提出了一种动作时间连贯学习（Action Temporal Coherence Learning, AcTOL）方法，以学习有序且连续的视觉-语言表示而不受刚性目标导向约束。AcTOL 将视频视为一个连续的轨迹，其中（1）通过对比帧之间的语义差异来反映其自然顺序，（2）施加局部布朗桥约束以确保中间帧之间的平滑过渡。大量的模仿学习实验表明，预训练特征可以通过高达 49% 的改进显著增强各种演示数量下的下游操作任务，并且对不同指令语言风格具有很高的鲁棒性，为通用具身智能体的实现提供了可行路径。源代码作为补充材料提供，供参考。', 'title_zh': '可验证的排序与连续性在视觉-语言预训练中的作用：实现通用体态智能体的可迁移性'}
{'arxiv_id': 'arXiv:2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'authors': 'Kanika Goswami, Puneet Mathur, Ryan Rossi, Franck Dernoncourt', 'link': 'https://arxiv.org/abs/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'abstract_zh': '科学数据可视化对于将原始数据转换为易于理解的视觉表示至关重要，有助于模式识别、预测和数据驱动洞察的呈现。然而，初学者用户往往由于选择合适工具和掌握可视化技术的复杂性而面临困难。大型语言模型（LLMs）最近在辅助代码生成方面显示出潜力，但在准确性上存在挑战，并且需要多次调试。本文提出了一种名为PlotGen的新颖多代理框架，旨在自动化精确科学可视化创建过程。PlotGen协调多个基于LLM的代理，包括一个查询规划代理，它将复杂的用户请求分解为可执行步骤；一个代码生成代理，将伪代码转换为可执行的Python代码；以及三个检索反馈代理——数值反馈代理、词汇反馈代理和视觉反馈代理——它们利用多模态LLM通过自我反省逐步优化生成图表的数据准确性、文本标签和视觉正确性。广泛的实验表明，PlotGen在MatPlotBench数据集上超过了强大的基线模型，实现了4-6%的改进，从而增加了用户对LLM生成可视化结果的信任度，并通过减少绘制错误所需的调试时间提高了初学者的生产力。', 'title_zh': 'PlotGen：基于多模态反馈的多智能体大型语言模型科学数据可视化方法'}
{'arxiv_id': 'arXiv:2502.00964', 'title': 'ML-Dev-Bench: Comparative Analysis of AI Agents on ML development workflows', 'authors': 'Harshith Padigela, Chintan Shah, Dinkar Juyal', 'link': 'https://arxiv.org/abs/2502.00964', 'abstract': "In this report, we present ML-Dev-Bench, a benchmark aimed at testing agentic capabilities on applied Machine Learning development tasks. While existing benchmarks focus on isolated coding tasks or Kaggle-style competitions, ML-Dev-Bench tests agents' ability to handle the full complexity of ML development workflows. The benchmark assesses performance across critical aspects including dataset handling, model training, improving existing models, debugging, and API integration with popular ML tools. We evaluate three agents -- ReAct, Openhands, and AIDE -- on a diverse set of 25 tasks, providing insights into their strengths and limitations in handling practical ML development challenges.", 'abstract_zh': '在本报告中，我们介绍了ML-Dev-Bench，这是一个旨在测试代理在实际机器学习开发任务中的代理能力的基准测试。现有的基准测试主要集中在独立的编码任务或Kaggle风格的比赛上，而ML-Dev-Bench则测试代理处理整个机器学习开发工作流的复杂性的能力。该基准测试从数据集处理、模型训练、改进现有模型、调试以及与流行的机器学习工具的API集成等多个关键方面评估性能。我们在这25项多样化的任务中评估了三个代理——ReAct、Openhands和AIDE，提供了它们在应对实际机器学习开发挑战时的优势和局限性的见解。', 'title_zh': 'ML-Dev-Bench：比较分析AI代理在机器学习开发工作流中的表现'}
{'arxiv_id': 'arXiv:2502.00870', 'title': 'FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation', 'authors': 'Wenzheng Jiang, Ji Wang, Xiongtao Zhang, Weidong Bao, Cheston Tan, Flint Xiaofeng Fan', 'link': 'https://arxiv.org/abs/2502.00870', 'abstract': "Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal details. Knowledge Distillation (KD) is a promising method for facilitating knowledge sharing among heterogeneous models, but it faces challenges related to the scarcity of public datasets and limitations in knowledge representation when applied to FedRL. To address these challenges, we propose Federated Heterogeneous Policy Distillation (FedHPD), which solves the problem of heterogeneous FedRL by utilizing action probability distributions as a medium for knowledge sharing. We provide a theoretical analysis of FedHPD's convergence under standard assumptions. Extensive experiments corroborate that FedHPD shows significant improvements across various reinforcement learning benchmark tasks, further validating our theoretical findings. Moreover, additional experiments demonstrate that FedHPD operates effectively without the need for an elaborate selection of public datasets.", 'abstract_zh': '联邦强化学习（FedRL）能够在保护隐私的同时提高样本效率；然而，目前大多数现有研究假设所有代理是同质化的，这限制了其在实际场景中的应用。本文探讨了在异构代理的黑盒环境中应用FedRL，其中每个代理使用不同的策略网络和训练配置，而不披露内部细节。知识蒸馏（KD）是一种促进异构模型之间知识共享的有前景的方法，但在应用于FedRL时，面临着公共数据集稀缺和知识表示方面的局限性挑战。为了解决这些挑战，我们提出了联邦异构策略蒸馏（FedHPD），通过使用动作概率分布作为知识共享的媒介来解决异构FedRL的问题。我们对FedHPD在标准假设下的收敛性进行了理论分析。大量实验结果表明，FedHPD在各种强化学习基准任务中表现出显著的改进，进一步验证了我们的理论发现。此外，额外的实验表明，FedHPD可以在不需要精心选择公共数据集的情况下有效运行。', 'title_zh': 'FedHPD：基于策略蒸馏的异构联邦强化学习'}
{'arxiv_id': 'arXiv:2502.00850', 'title': 'Dual Alignment Maximin Optimization for Offline Model-based RL', 'authors': 'Chi Zhou, Wang Luo, Haoran Li, Congying Han, Tiande Guo, Zicheng Zhang', 'link': 'https://arxiv.org/abs/2502.00850', 'abstract': 'Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.', 'abstract_zh': '由于合成数据与现实数据之间的分布不匹配，离线强化学习代理在部署时面临着重大挑战。虽然以往多数研究集中在提高合成样本的真实性和引入离策机制上，但直接集成的方法往往无法确保在偏向模型和潜在环境动态中的一致性策略行为，这种偏见来源于行为策略与学习策略之间的固有差异。本文中，我们首先将焦点从模型可靠性转移到策略差异性，同时最大化期望回报，然后自洽地引入合成数据，提出了一种新颖的Actor-Critic框架，Double Alignment Maximin Optimization (DAMO)。这是一种统一框架，旨在确保模型-环境策略一致性以及合成数据与离线数据的兼容性。内部最小化操作执行双重保守值估计，将策略和轨迹对齐以避免出现分布外状态和行动，而外部最大化操作确保策略改进与内部值估计一致。实证评估表明，DAMO 有效地确保了模型和策略的一致性，在多种基准任务中实现了具有竞争力的表现。', 'title_zh': '基于模型的离线强化学习中的双对齐最大化最小化优化'}
{'arxiv_id': 'arXiv:2502.00757', 'title': 'AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds', 'authors': 'J Rosser, Jakob Nicolaus Foerster', 'link': 'https://arxiv.org/abs/2502.00757', 'abstract': 'Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been as thoroughly explored. In this paper, we introduce AGENTBREEDER a framework for multi-objective evolutionary search over scaffolds. Our REDAGENTBREEDER evolves scaffolds towards jailbreaking the base LLM while achieving high task success, while BLUEAGENTBREEDER instead aims to combine safety with task reward. We evaluate the systems discovered by the different instances of AGENTBREEDER and popular baselines using widely recognized reasoning, mathematics, and safety benchmarks. Our work highlights and mitigates the safety risks due to multi-agent scaffolding.', 'abstract_zh': '将大型语言模型（LLMs）构建到多代理系统中通常可以提高完成复杂任务的性能，但此类支撑结构的安全影响尚未得到充分探讨。本文介绍了AGENTBREEDER框架，该框架用于多目标进化搜索。我们的REDAGENTBREEDER朝破解基础LLM的方向进化支撑结构，同时实现高任务成功率，而BLUEAGENTBREEDER则旨在结合安全性和任务奖励。我们使用广泛认可的推理、数学和安全基准对不同实例的AGENTBREEDER发现的系统和流行基准进行了评估。我们的工作突显了多代理支撑结构带来的安全风险，并提出了相应的缓解措施。', 'title_zh': 'AgentBreeder: 缓解多智能体架构对AI安全影响的方法'}
{'arxiv_id': 'arXiv:2502.00666', 'title': 'Avoiding $\\mathbf{exp(R_{max})}$ scaling in RLHF through Preference-based Exploration', 'authors': 'Mingyu Chen, Yiding Chen, Wen Sun, Xuezhou Zhang', 'link': 'https://arxiv.org/abs/2502.00666', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024).. Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design.', 'abstract_zh': '人类反馈强化学习（RLHF）已成为大型语言模型（LLM）对齐的关键技术。本文研究了在线RLHF的设置，并专注于提高样本效率。现有的所有在线RLHF算法，无论是进行被动探索还是主动探索，都受到了样本复杂度随着奖励函数规模指数增长的基本限制。这一根本限制阻碍了它们在偏好严重失衡的场景下的效果，例如那些具有唯一正确答案的问题。为了解决这个问题，我们提出了Self-Exploring Preference-Incentive Online Preference Optimization（SE-POPO）算法，这是首次实现样本复杂度与奖励规模呈多项式增长的在线RLHF算法，解决了Xie等（2024）提出的公开问题。理论分析表明，SE-POPO的样本复杂度优于现有探索算法的样本复杂度。实验结果显示，我们在两种主要的RLHF应用场景和公共基准测试中系统的评估均证明SE-POPO比探索性和非探索性基线更具有样本效率，这标志着RLHF算法设计上的一个重要进步。', 'title_zh': '通过基于偏好的探索避免RLHF中的$\\mathbf{exp(R_{max})}$缩放'}
{'arxiv_id': 'arXiv:2502.00495', 'title': 'Looking into the Future of Health-Care Services: Can Life-Like Agents Change the Future of Health-Care Services?', 'authors': 'Mohammad Saleh Torkestani, Robert Davis, Abdolhossein Sarrafzadeh', 'link': 'https://arxiv.org/abs/2502.00495', 'abstract': 'Time constraints on doctor patient interaction and restricted access to specialists under the managed care system led to increasingly referring to computers as a medical information source and a self-health-care management tool. However, research show that less than 40% of information seekers indicated that online information helped them to make a decision about their health. Searching multiple web sites that need basic computer skills, lack of interaction and no face to face interaction in most search engines and some social issues, led us to develop a specialized life-like agent that would overcome mentioned problems.', 'abstract_zh': '时间限制和受管理医疗体系下专家资源受限导致医生与患者互动时间缩短，且患者难以获得专科医生的及时咨询，这促使人们越来越多地将计算机视为医疗信息来源和自我健康管理工具。然而，研究显示，仅有不到40%的寻求信息者认为在线信息帮助他们做出了关于自身健康的决策。在需要基本计算机技能、缺乏互动和大多数搜索引擎缺乏面对面交流的网站上进行多网站搜索，以及一些社会问题，促使我们开发了一个专门的、拟人化的代理，以克服上述问题。', 'title_zh': '探究医疗服务的未来：生活化代理能否改变医疗服务的未来？'}
{'arxiv_id': 'arXiv:2502.00415', 'title': 'MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents', 'authors': 'George Fatouros, Kostas Metaxas, John Soldatos, Manos Karathanassis', 'link': 'https://arxiv.org/abs/2502.00415', 'abstract': "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.", 'abstract_zh': 'MarketSenseAI是一种全新的综合股票分析框架，利用大规模语言模型（LLMs）处理金融新闻、历史价格、公司基本面以及宏观经济环境，为股票分析和选择提供决策支持。在本文中，我们介绍了由大规模语言模型技术的快速发展推动的MarketSenseAI的最新进展。通过一种新颖的架构结合检索增强生成和LLM代理，该框架处理证监会文件和财报电话会议，同时通过系统处理各种机构报告来丰富宏观经济分析。我们展示了与上一个版本相比在基础分析准确性上的显著改进。对道琼斯100指数股票为期两年（2023-2024年）的实证研究表明，MarketSenseAI在累积回报率为125.9%，而指数回报率为73.5%的情况下，保持了类似的风险水平。进一步在2024年的道琼斯500指数股票上验证，该框架展现了其可扩展性，实现了市场回报率排序难度比率（Sortino比率）高出33.8%的效果。这项工作标志着在金融分析中应用大规模语言模型技术的重要进展，并提供了大规模语言模型驱动投资策略稳健性的洞察。', 'title_zh': 'MarketSenseAI 2.0：通过LLM代理增强股票分析'}
{'arxiv_id': 'arXiv:2502.00350', 'title': 'OrcaLoca: An LLM Agent Framework for Software Issue Localization', 'authors': 'Zhongming Yu, Hejia Zhang, Yujie Zhao, Hanxian Huang, Matrix Yao, Ke Ding, Jishen Zhao', 'link': 'https://arxiv.org/abs/2502.00350', 'abstract': 'Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration.', 'abstract_zh': '近年来，大型语言模型（LLM）代理的发展正在革新自主软件工程（ASE），使自动编码、问题修复和功能改进成为可能。然而，本地化——即通过导航到相关代码段精确识别软件问题——仍然是一个重大挑战。当前的方法由于LLM代理与精确代码搜索机制之间的有效集成不足，往往导致结果不佳。本文介绍了OrcaLoca，这是一种LLM代理框架，通过优先级调度指导动作、动作分解与相关性评分以及距离感知上下文剪枝，提高了软件问题定位的准确性。实验结果表明，OrcaLoca在SWE-bench Lite的功能匹配率上成为新的开源领先方案（65.33%）。此外，通过其修补生成集成，它还将开源框架的最终解决率提高了6.33个百分点。', 'title_zh': 'OrcaLoca：一个软件问题定位的大型语言模型代理框架'}
{'arxiv_id': 'arXiv:2502.00346', 'title': 'Actor Critic with Experience Replay-based automatic treatment planning for prostate cancer intensity modulated radiotherapy', 'authors': 'Md Mainul Abrar, Parvat Sapkota, Damon Sprouts, Xun Jia, Yujie Chi', 'link': 'https://arxiv.org/abs/2502.00346', 'abstract': 'Background: Real-time treatment planning in IMRT is challenging due to complex beam interactions. AI has improved automation, but existing models require large, high-quality datasets and lack universal applicability. Deep reinforcement learning (DRL) offers a promising alternative by mimicking human trial-and-error planning.\nPurpose: Develop a stochastic policy-based DRL agent for automatic treatment planning with efficient training, broad applicability, and robustness against adversarial attacks using Fast Gradient Sign Method (FGSM).\nMethods: Using the Actor-Critic with Experience Replay (ACER) architecture, the agent tunes treatment planning parameters (TPPs) in inverse planning. Training is based on prostate cancer IMRT cases, using dose-volume histograms (DVHs) as input. The model is trained on a single patient case, validated on two independent cases, and tested on 300+ plans across three datasets. Plan quality is assessed using ProKnow scores, and robustness is tested against adversarial attacks.\nResults: Despite training on a single case, the model generalizes well. Before ACER-based planning, the mean plan score was 6.20$\\pm$1.84; after, 93.09% of cases achieved a perfect score of 9, with a mean of 8.93$\\pm$0.27. The agent effectively prioritizes optimal TPP tuning and remains robust against adversarial attacks.\nConclusions: The ACER-based DRL agent enables efficient, high-quality treatment planning in prostate cancer IMRT, demonstrating strong generalizability and robustness.', 'abstract_zh': '背景：在IMRT（调强放射治疗）中实现实时治疗计划具有挑战性，因为需要处理复杂的光线相互作用。尽管人工智能提高了自动化水平，但现有模型仍需要大量高质量的数据集，并且缺乏通用适用性。深度强化学习（DRL）提供了一种有前途的选择，因为它模仿了人类的试探性计划方式。\n目的：开发一种基于随机策略的DRL代理，用于高效的自动治疗计划，具有广泛的适用性和对抗性攻击的鲁棒性，并采用快速梯度符号方法（FGSM）进行测试。\n方法：使用Actor-Critic带经验回放（ACER）架构，代理在逆向计划中调整治疗计划参数（TPPs）。训练基于前列腺癌的IMRT案例，以剂量体积直方图（DVHs）作为输入。模型在单个患者案例上进行训练，在两个独立案例上进行验证，并在三个数据集中包含300多个案例上进行测试。使用ProKnow评分评估计划质量，并通过对抗性攻击测试其鲁棒性。\n结果：尽管只在单个案例上进行训练，但该模型仍具有良好的泛化能力。在ACER计划之前，平均计划得分为6.20±1.84；之后，93.09%的案例达到了完美的得分为9分，平均得分为8.93±0.27。代理成功优先调整最优的TPPs，并且仍然具有对抗性攻击的鲁棒性。\n结论：基于ACER的DRL代理能够在前列腺癌IMRT中实现高效且高质量的治疗计划，显示出强大的泛化能力和鲁棒性。', 'title_zh': '基于经验回放的自动治疗计划方法结合演员-评论家算法在前列腺癌调强放疗中的应用'}
{'arxiv_id': 'arXiv:2502.00345', 'title': 'The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning', 'authors': 'Yurui Li, Yuxuan Chen, Li Zhang, Shijian Li, Gang Pan', 'link': 'https://arxiv.org/abs/2502.00345', 'abstract': 'The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world this http URL cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among this http URL, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal this http URL, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate this http URL enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and this http URL this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their this http URL guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic this http URL evaluate 10 cooperative MARL methods on the proposed this http URL results indicate that all baselines perform poorly on these this http URL further validate the solvability of these tasks, we also propose simplified variants of proposed this http URL results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed this http URL source files is available at this https URL.', 'abstract_zh': '分工（Division of Labor, DOL）在促进合作方面的重要作用在现实世界中得到了广泛认可。现有的多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）方法已将DOL的概念纳入其中，以提高智能体间的合作效果。然而，现有测试床中使用的任务通常并不需要DOL这一特性来实现最优效果，因此DOL概念在MARL方法中的充分利用仍然未能实现。为了增强MARL方法在现实世界场景中的适用性和普遍性，有必要开发需要多智能体DOL的任务。在本文中，我们提出了若干旨在满足这些需求的任务，以现实世界规则为指导设计这些任务，以确保DOL和合作是完成任务的必要条件，并通过引入三个因素来扩展任务的多样性，使其覆盖更多实际场景。我们对提出的任务评估了10种合作MARL方法，结果表明所有基线方法在这些任务上表现不佳，进一步证明了这些任务的可解性。我们还提出了所提任务的简化版本，并展示了基线方法可以处理这些简化版本，进一步证明了所提任务的可解性。相关的源代码可在本链接访问：[此处插入链接]。', 'title_zh': '合作多代理强化学习中的复合任务挑战'}
{'arxiv_id': 'arXiv:2502.00225', 'title': 'Should You Use Your Large Language Model to Explore or Exploit?', 'authors': 'Keegan Harris, Aleksandrs Slivkins', 'link': 'https://arxiv.org/abs/2502.00225', 'abstract': 'We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.', 'abstract_zh': '我们评估当前一代大规模语言模型（LLMs）在面对探索-利用权衡时辅助决策代理的能力。我们使用LLMs分别进行探索和利用，应用于各种（上下文相关的）bandit任务。我们发现，尽管当前的LLMs在利用方面常常表现不佳，但在某些小型任务中，通过上下文内的缓解措施可以显著改善性能。然而，即使在这种情况下，LLMs的表现仍然不如简单的线性回归模型。另一方面，我们发现LLMs在探索具有内在语义的大动作空间时确实有所帮助，它们能够建议合适的探索候选对象。', 'title_zh': '你应该使用你的大规模语言模型来探索还是利用？'}
{'arxiv_id': 'arXiv:2502.00023', 'title': 'Musical Agent Systems: MACAT and MACataRT', 'authors': 'Keon Ju M. Lee, Philippe Pasquier', 'link': 'https://arxiv.org/abs/2502.00023', 'abstract': 'Our research explores the development and application of musical agents, human-in-the-loop generative AI systems designed to support music performance and improvisation within co-creative spaces. We introduce MACAT and MACataRT, two distinct musical agent systems crafted to enhance interactive music-making between human musicians and AI. MACAT is optimized for agent-led performance, employing real-time synthesis and self-listening to shape its output autonomously, while MACataRT provides a flexible environment for collaborative improvisation through audio mosaicing and sequence-based learning. Both systems emphasize training on personalized, small datasets, fostering ethical and transparent AI engagement that respects artistic integrity. This research highlights how interactive, artist-centred generative AI can expand creative possibilities, empowering musicians to explore new forms of artistic expression in real-time, performance-driven and music improvisation contexts.', 'abstract_zh': '我们的研究探索了音乐代理的发展与应用，这是一种由人类参与的生成型人工智能系统，旨在支持音乐表演和即兴创作。我们介绍了两种不同的音乐代理系统：MACAT和MACataRT，以提高人类音乐家和AI之间的交互式音乐创作。MACAT优化了以代理为中心的表演，利用实时合成和自我倾听来自主塑造其输出，而MACataRT则提供了一个通过音频镶嵌和基于序列的学习进行协作即兴创作的灵活环境。两种系统都强调基于个性化的小数据集进行训练，从而培养一种公平透明的AI互动方式，尊重艺术完整性。本研究突显了交互式、以艺术家为中心的生成型人工智能如何扩展创作可能性，使音乐家能够实时探索新的艺术表达形式，在表演驱动和即兴音乐创作的情境中赋予他们更大的创造力。', 'title_zh': '音乐智能体系统：MACAT与MACataRT'}
{'arxiv_id': 'arXiv:2502.01344', 'title': 'PSSD: Making Large Language Models Self-denial via Human Psyche Structure', 'authors': 'Jinzhi Liao, Zenghua Liao, Xiang Zhao', 'link': 'https://arxiv.org/abs/2502.01344', 'abstract': "The enhance of accuracy in reasoning results of LLMs arouses the community's interests, wherein pioneering studies investigate post-hoc strategies to rectify potential mistakes. Despite extensive efforts, they are all stuck in a state of resource competition demanding significant time and computing expenses. The cause of the situation lies in the failure of identifying the fundamental feature of the solutions in this line, coined as the self-denial of LLMs. In other words, LLMs should confidently determine the potential existence of mistakes and carefully execute the targeted correction. As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged. In response to the challenges, we present PSSD, which refers to and implements the human psyche structure such that three distinct and interconnected roles contribute to human reasoning. Specifically, PSSD leverages the recent multi-agent paradigm, and is further enhanced with three innovatively conceived roles: (1) the intuition-based id role that provides initial attempts based on benign LLMs; (2) the rule-driven superego role that summarizes rules to regulate the above attempts, and returns specific key points as guidance; and (3) the script-centric ego role that absorbs all procedural information to generate executable script for the final answer prediction. Extensive experiments demonstrate that the proposed design not only better enhance reasoning capabilities, but also seamlessly integrate with current models, leading to superior performance.", 'abstract_zh': '增强大型语言模型推理结果的准确性引起了学术界的兴趣，其中先驱研究探讨了后验策略以纠正潜在错误。尽管付出了大量努力，这些研究仍然深陷资源竞争的困境，耗费大量时间和计算资源。这种情况的原因在于未能识别这一系列解决方案的基本特征，这种特征被称作大型语言模型的自我否定。换句话说，大型语言模型应该自信地确定潜在错误的存在，并仔细执行针对性的修正。由于整个过程都在大型语言模型内部进行，支持性和说服性的参考文献难以获得，即使承认错误存在，具体步骤用于改进隐藏的错误仍然缺失。为应对这些挑战，我们提出了一种名为PSSD的设计，该设计参考并实现了人类心理结构，其中三个相互独立且相互关联的角色共同促进人类推理。具体而言，PSSD利用了最近的多代理范式，并进一步增加了三种创新构想的角色：（1）基于直觉的自我角色，基于良性大型语言模型提供初步尝试；（2）规则驱动的超我角色，总结规则以调节上述尝试，并返回具体关键点作为指导；（3）剧本为中心的本我角色，吸收所有程序信息以生成最终答案预测所需的可执行脚本。大量实验表明，所提出的设计不仅能更好地增强推理能力，还能无缝集成当前模型，从而实现卓越的性能。', 'title_zh': 'PSSD：通过人类心理结构使大型语言模型自我质疑'}
{'arxiv_id': 'arXiv:2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'authors': 'Kanika Goswami, Puneet Mathur, Ryan Rossi, Franck Dernoncourt', 'link': 'https://arxiv.org/abs/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'abstract_zh': '大型语言模型（LLMs）可以执行图表问题回答任务，但常常生成未验证的虚构答案。现有的答案归属方法难以将回答与源图表中的视觉语义上下文联系起来，由于限制了视觉语义上下文，复杂的视觉文本对齐要求以及在复杂布局中边界框预测的困难。我们提出了一种多智能体框架ChartCitor，该框架通过在图表图像中识别支持性证据提供了细粒度的边界框引用。该系统协调LLM智能体执行图表到表格的抽取、答案重新表述、表格扩展、通过预先过滤和重新排名的证据检索，以及表格到图表的映射。研究表明，ChartCitor 在不同类型图表上优于现有基线。定性的用户研究显示，ChartCitor 帮助提高了用户对生成型AI的信任，通过增强基于LLM的图表QA的解释性，并使专业人士更加高效。', 'title_zh': 'ChartCitor：细粒度图表视觉归属的多代理框架'}
{'arxiv_id': 'arXiv:2502.00955', 'title': 'Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search', 'authors': 'Wentao Shi, Zichun Yu, Fuli Feng, Xiangnan He, Chenyan Xiong', 'link': 'https://arxiv.org/abs/2502.00955', 'abstract': 'Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training.', 'abstract_zh': '基于蒙特卡洛树搜索（MCTS）的方法为生成合成数据以增强基于大型语言模型（LLM）的多智能体系统（MAS）的自我训练提供了有前景的途径。这些方法利用Q值来估算个体智能体的贡献。然而，仅仅依赖Q值来识别信息性数据可能与数据合成的目标不一致，因为在选择数据时应关注能够最好地增强模型训练的数据。为了解决这一偏差，我们提出了面向数据影响的树搜索（DITS）框架，该框架引入了影响分数来指导树搜索和数据选择。利用影响分数，我们可以有效地识别对系统改进最具影响的数据，从而增强模型性能。此外，我们提出了适用于非可微度量的影响分数估计方法，通过利用推理计算显著减少了计算开销。在八个不同的多智能体数据集上的广泛实验表明，所提出的方法具有稳健性和有效性。值得注意的是，我们的研究结果表明，在数据合成过程中，将更多的推理资源分配用于估计影响分数而非Q值，可以更有效地和更高效地增强模型训练。', 'title_zh': '基于数据影响导向树搜索的高效多智能体系统训练'}
{'arxiv_id': 'arXiv:2502.00675', 'title': 'ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction, and Column Exploration', 'authors': 'Minghang Deng, Ashwin Ramachandran, Canwen Xu, Lanxiang Hu, Zhewei Yao, Anupam Datta, Hao Zhang', 'link': 'https://arxiv.org/abs/2502.00675', 'abstract': 'Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (> 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 26.69 on the Spider 2.0-Snow and scoring 24.50 on the Spider 2.0-Lite tasks.', 'abstract_zh': '文本到SQL系统通过使自然语言查询能够访问结构化数据库中的关键数据洞见，从而开启了更轻松的数据访问途径。然而，企业在部署此类系统时仍然面临诸多挑战，这些挑战主要源自于复杂庞大的模式（超过3000列）、多样化的SQL方言（例如，BigQuery、Snowflake）以及复杂的查询要求（例如，转换、分析）。现有的Spider 2.0数据集上的最新表现——一个旨在模拟这种复杂环境的基准测试——仍然受到限制，最高仅为20%。主要限制因素包括指令执行不足、长上下文理解较差、自我校正能力弱以及方言特定知识不足。\n\n为了弥补这些不足，我们提出了ReFoRCE（具有格式限制和列探索的自我校正代理），该系统引入了以下几点改进：（1）表格压缩，以缓解长上下文限制；（2）格式限制，以确保答案格式的准确性；（3）迭代列探索，以提升模式理解能力。此外，它还采用了自我校正流水线，包括（1）并行化工作流配以投票机制，以及（2）基于公共表表达式（CTE）的校正方法来处理无法解决的情况。ReFoRCE在Spider 2.0-Snow任务上达到了最先进的结果，得分为26.69，在Spider 2.0-Lite任务上得分为24.50。', 'title_zh': 'ReFoRCE：一种具备自我完善、格式限制和列探索的文本到SQL代理'}
{'arxiv_id': 'arXiv:2502.00674', 'title': 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?', 'authors': 'Wenzhe Li, Yong Lin, Mengzhou Xia, Chi Jin', 'link': 'https://arxiv.org/abs/2502.00674', 'abstract': 'Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.', 'abstract_zh': '将多样来源的输出进行集成是一种简单而有效的方法，可以提升性能。Mixture-of-Agents（MoA）就是一个这样的集成方法，它从多个不同的大型语言模型（LLMs）中聚合输出。本文在语言模型的背景下提出了一个问题：混合不同的LLMs真的有益吗？我们提出了Self-MoA——一种仅从单一最优的LLMs生成的输出中聚合的集成方法。大量实验结果显示，Self-MoA在多种场景中表现出色，显著优于混合不同LLMs的标准MoA：Self-MoA在AlpacaEval 2.0基准测试中比MoA提高了6.6%的性能，并在包括MMLU、CRUX和MATH在内的多个基准测试中平均提高了3.8%的性能。直接将Self-MoA应用于AlpacaEval 2.0得分最高的模型，在排行榜上取得了新的SOTA性能。为了理解Self-MoA的有效性，我们系统地研究了在各种MoA设置下输出多样性和质量之间的权衡。我们证实MoA的性能对质量非常敏感，而混合不同的LLMs往往降低了模型的平均质量。为了补充这项研究，我们确定了混合不同LLMs可能有益的场景。本文还进一步引入了Self-MoA的序列版本，它能够在多个轮次中实时聚合大量的LLM输出，并且其效果等同于一次性聚合所有输出。', 'title_zh': '重新思考“混合智能体”：混合不同大规模语言模型是否有利？'}
{'arxiv_id': 'arXiv:2502.01390', 'title': 'Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant', 'authors': 'Gaole He, Gianluca Demartini, Ujwal Gadiraju', 'link': 'https://arxiv.org/abs/2502.01390', 'abstract': "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of 'LLM-modulo' setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.", 'abstract_zh': '自ChatGPT流行以来，大规模语言模型（LLMs）继续影响我们的日常生活。配备了特定用途的外部工具（如用于航班预订或闹钟的工具），LLM代理日益展现出在日常工作中协助人类的能力。尽管LLM代理展示了作为日常助手的前景，但对其如何基于规划和序列决策能力提供日常帮助的理解仍有限。我们从最近的工作中汲取灵感，这些工作强调了在人类参与下的“LLM调和设置”对于计划任务的价值。我们在六项常见任务中（例如，飞行机票预订和信用卡支付，这些任务通常具有不同的风险水平）进行了实证研究（共248名参与者），以评估LLM代理作为日常助手的表现。为了确保用户对LLM代理拥有控制权，我们在计划-执行模式下使用了LLM代理，在模拟环境中进行逐步规划和逐步执行，并分析了每阶段用户参与如何影响他们的信任与协作团队的表现。我们的研究发现LLM代理是一把“双刃剑”——（1）当具备高质量的计划和必要的用户参与执行时，它们可以很好地运行；（2）当计划看起来合理时，用户可能会轻易地对LLM代理产生不信任。我们提出了关键见解，以指导如何利用LLM代理作为日常助手来调整用户信任，从而实现更好的整体任务成果。我们的研究为未来日常助手的设计以及人类与LLM代理的协作提供了重要启示。', 'title_zh': '计划与执行：一项关于在日常助手使用大型语言模型代理时用户信任和团队绩效实证研究'}
{'arxiv_id': 'arXiv:2502.00510', 'title': "Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents", 'authors': 'Yingxuan Yang, Bo Huang, Siyuan Qi, Chao Feng, Haoyi Hu, Yuxuan Zhu, Jinbo Hu, Haoran Zhao, Ziyi He, Xiao Liu, Zongyu Wang, Lin Qiu, Xuezhi Cao, Xunliang Cai, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2502.00510', 'abstract': "Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory's Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent's architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,000 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.", 'abstract_zh': '大型语言模型（LLM）代理框架通常采用模块化架构，整合了包括计划、推理、操作执行和反思在内的组件，以应对复杂任务。然而，量化每个模块对整体系统性能的贡献仍然是一个重大挑战，阻碍了优化和可解释性。为解决这一问题，我们提出了CapaBench（能力评估基准），该框架基于合作博弈理论中的Shapley值，系统地衡量了个体模块及其相互作用对代理架构的影响。通过在所有可能的组合中用测试变体替换默认模块，CapaBench提供了一种原则方法来归因性能贡献。主要贡献包括：（1）我们首次提出了基于Shapley值的方法来量化LLM代理中能力的贡献；（2）具有高Shapley值的模块组合时经常能带来可预测的性能提升，从而实现有针对性的优化；（3）我们构建了一个包含超过1,000个条目的多轮数据集，覆盖了多种领域和实际任务场景，以实现对代理能力的全面评估。CapaBench弥补了组件级评估与整体系统评估之间的差距，为优化模块化LLM代理并推动其实现复杂的真实场景提供了可操作的见解。', 'title_zh': '谁是MVP？一种用于LLM代理模块归因的游戏理论评估基准'}
