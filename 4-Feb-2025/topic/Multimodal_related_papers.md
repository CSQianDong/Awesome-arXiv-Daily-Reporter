# Zero-Shot Warning Generation for Misinformative Multimodal Content 

**Title (ZH)**: 零样本警告生成for误导性多模态内容 

**Authors**: Giovanni Pio Delvecchio, Huy Hong Nguyen, Isao Echizen  

**Link**: [PDF](https://arxiv.org/pdf/2502.00752)  

**Abstract**: The widespread prevalence of misinformation poses significant societal concerns. Out-of-context misinformation, where authentic images are paired with false text, is particularly deceptive and easily misleads audiences. Most existing detection methods primarily evaluate image-text consistency but often lack sufficient explanations, which are essential for effectively debunking misinformation. We present a model that detects multimodal misinformation through cross-modality consistency checks, requiring minimal training time. Additionally, we propose a lightweight model that achieves competitive performance using only one-third of the parameters. We also introduce a dual-purpose zero-shot learning task for generating contextualized warnings, enabling automated debunking and enhancing user comprehension. Qualitative and human evaluations of the generated warnings highlight both the potential and limitations of our approach. 

**Abstract (ZH)**: 广泛传播的虚假信息对社会构成了重大关切。脱离上下文的虚假信息，即真实图片与虚假文本配对，尤其具有欺骗性，容易误导受众。现有的大多数检测方法主要评估图像与文本的一致性，但往往缺乏足够的解释，而这对于有效揭露虚假信息至关重要。我们提出了一种通过跨模态一致性检查来检测多模态虚假信息的模型，该模型的训练时间极短。此外，我们还提出了一种轻量级模型，仅使用参数的三分之一就实现了竞争性的性能。我们还引入了一项双用途的零样本学习任务，用于生成上下文化的警告，从而实现自动化揭露虚假信息并增强用户的理解能力。对生成的警告进行定性评估和人类评估，突显了我们方法的潜力和局限性。 

---
# MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models 

**Title (ZH)**: MM-IQ: 多模态模型中人类似抽象与推理的基准测试 

**Authors**: Huanqia Cai, Yijun Yang, Winston Hu  

**Link**: [PDF](https://arxiv.org/pdf/2502.00698)  

**Abstract**: IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.
Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide. 

**Abstract (ZH)**: 智商测试一直作为一种基础方法，用于评估人类的认知能力，故意将评估与语言背景、语言熟练程度或领域特定知识脱钩，以分离出抽象和推理的核心能力。然而，当前的人工智能研究缺乏系统性的基准来量化多模态系统中的这些关键认知维度。为解决这一关键差距，我们提出了MM-IQ，这是一个包含2,710个精心筛选测试项目的综合评估框架，覆盖8种不同的推理范式。

通过对领先的开源和专有多模态模型进行系统性评估，我们的基准揭示出显著的局限性：即使最先进的架构也仅比随机猜测略好（准确率为27.49%，而基线准确率为25%）。这一显著的性能差距突显了当前多模态系统在逼近基本的人类推理能力方面的不足，强调了需要范式转变性的进展来弥合这一认知鸿沟。 

---
# Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach 

**Title (ZH)**: 理解分布迁移下多模态LLM：一种信息论方法 

**Authors**: Changdae Oh, Zhen Fang, Shawn Im, Xuefeng Du, Yixuan Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.00577)  

**Abstract**: Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies. Extensive experiments on real benchmark datasets, spanning 61 shift scenarios empirically validate our theoretical insights. 

**Abstract (ZH)**: 多模态大语言模型（MLLMs）展现出令人鼓舞的能力，但在分布变化的情况下则表现出短板，即评估数据与指令调优分布存在差异。尽管先前的工作提供了经验评估，但我们认为建立一个正式框架，能够表征和量化MLLMs的风险是必要的，以确保MLLMs在全球实际应用中的安全性和可靠性。从信息论的角度出发，我们提出了第一个理论框架，该框架能够量化MLLMs在分布变化下的最大风险。该框架的核心在于引入了有效互信息（Effective Mutual Information, EMI），这是一种原则性的度量标准，用于量化输入查询与模型响应的相关性。我们推导了EMI差异在分布内（ID）和分布外（OOD）数据之间的上界，并将其与视觉和文本分布差异联系起来。我们在涵盖61种分布变化情景的真实基准数据集上进行了广泛的实验，验证了我们的理论见解。 

---
# VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos 

**Title (ZH)**: VideoRAG：极长上下文视频增强生成 

**Authors**: Xubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, Chao Huang  

**Link**: [PDF](https://arxiv.org/pdf/2502.01549)  

**Abstract**: Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in enhancing Large Language Models (LLMs) through external knowledge integration, yet its application has primarily focused on textual content, leaving the rich domain of multi-modal video knowledge predominantly unexplored. This paper introduces VideoRAG, the first retrieval-augmented generation framework specifically designed for processing and understanding extremely long-context videos. Our core innovation lies in its dual-channel architecture that seamlessly integrates (i) graph-based textual knowledge grounding for capturing cross-video semantic relationships, and (ii) multi-modal context encoding for efficiently preserving visual features. This novel design empowers VideoRAG to process unlimited-length videos by constructing precise knowledge graphs that span multiple videos while maintaining semantic dependencies through specialized multi-modal retrieval paradigms. Through comprehensive empirical evaluation on our proposed LongerVideos benchmark-comprising over 160 videos totaling 134+ hours across lecture, documentary, and entertainment categories-VideoRAG demonstrates substantial performance compared to existing RAG alternatives and long video understanding methods. The source code of VideoRAG implementation and the benchmark dataset are openly available at: this https URL. 

**Abstract (ZH)**: 以下是符合学术规范的翻译：

检索增强生成（RAG）在通过外部知识整合增强大型语言模型（LLMs）方面已经取得了显著的成功，其应用领域主要集中在文本内容上，而多模态视频知识的丰富领域仍然未得到充分探索。本文提出了一种名为VideoRAG的新框架，这是第一个专门针对处理和理解极长视频的检索增强生成框架。我们的核心创新在于该框架采用了一种双通道架构，该架构能够无缝地将（i）基于图的文本知识关联应用于捕捉视频间的跨层语义关系，以及（ii）多模态上下文编码应用于高效地保留视觉特征相结合。这种新颖的设计使VideoRAG能够通过构建跨视频的精确知识图谱来处理无限长度的视频，同时通过专门的多模态检索范式保持语义依赖性。

通过在我们提出的LongerVideos基准上进行全面的经验性评估（该基准包含超过160个视频，总计134+小时，涵盖了讲座、纪录片和娱乐等多个类别），VideoRAG与现有的RAG替代方法和长视频理解方法相比，展现了显著的性能优势。VideoRAG的实现代码和基准数据集均可从以下链接公开获取：this https URL。 

---
# Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective 

**Title (ZH)**: 从训练范式视角高效整合大规模语言模型与视觉感知：一个综述 

**Authors**: Xiaorui Ma, Haoran Xie, S. Joe Qin  

**Link**: [PDF](https://arxiv.org/pdf/2502.01524)  

**Abstract**: The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs. 

**Abstract (ZH)**: 视觉语言模态的集成一直是多模态学习中的一个重要研究方向，传统上依赖于视觉语言预训练模型。然而，随着大型语言模型（LLMs）的发展，模型设计越来越多地结合视觉模态和LLMs。随之而来的是，将视觉模态融入LLMs的训练范式也发生了演变。最初的方法是通过预训练模态集成器，这种方法被称为单阶段调优。此后，这一领域分支出了专注于性能提升的方法，称为双阶段调优；以及侧重参数效率的方法，称为直接适应。现有的综述主要关注使用双阶段调优的视觉大型语言模型（VLLMs），却没有全面解释这些训练范式的演变及其独特的参数效率考虑。本文从训练范式视角出发，对来自顶级会议、期刊和高引用量Arxiv论文的34种VLLMs进行了分类和综述，重点关注适应过程中的参数效率。我们首先介绍大型语言模型的架构和参数效率学习方法，随后讨论视觉编码器，并提供模态集成器的全面分类。接着，我们回顾了三种训练范式及其效率考虑，并总结了VLLM领域的基准测试结果。为了更深入地了解这些方法在参数效率方面的有效性，我们比较并讨论了代表性模型的实验结果，其中包括直接适应范式的实验复现。提供这些最新进展和实用应用的见解，本文为研究人员和实践者如何高效地将视觉模态集成到大型语言模型中提供了宝贵的指南。 

---
# Position: Empowering Time Series Reasoning with Multimodal LLMs 

**Title (ZH)**: 标题：利用多模态大规模语言模型增强时间序列推理 

**Authors**: Yaxuan Kong, Yiyuan Yang, Shiyu Wang, Chenghao Liu, Yuxuan Liang, Ming Jin, Stefan Zohren, Dan Pei, Yan Liu, Qingsong Wen  

**Link**: [PDF](https://arxiv.org/pdf/2502.01477)  

**Abstract**: Understanding time series data is crucial for multiple real-world applications. While large language models (LLMs) show promise in time series tasks, current approaches often rely on numerical data alone, overlooking the multimodal nature of time-dependent information, such as textual descriptions, visual data, and audio signals. Moreover, these methods underutilize LLMs' reasoning capabilities, limiting the analysis to surface-level interpretations instead of deeper temporal and multimodal reasoning. In this position paper, we argue that multimodal LLMs (MLLMs) can enable more powerful and flexible reasoning for time series analysis, enhancing decision-making and real-world applications. We call on researchers and practitioners to leverage this potential by developing strategies that prioritize trust, interpretability, and robust reasoning in MLLMs. Lastly, we highlight key research directions, including novel reasoning paradigms, architectural innovations, and domain-specific applications, to advance time series reasoning with MLLMs. 

**Abstract (ZH)**: 理解时间序列数据对于多个实际应用至关重要。虽然大型语言模型（LLMs）在时间序列任务中展现出潜力，当前的方法往往仅依赖于数值数据，而忽略了时间依赖信息的多模态性质，如文本描述、视觉数据和音频信号。此外，这些方法未能充分利用LLMs的推理能力，限制了分析仅停留在表面层面的理解，而非更深层次的时间和多模态推理。在本文中，我们主张多模态LLMs（MLLMs）可以增强时间序列分析中的推理能力，提升决策质量和实际应用效果。我们呼吁研究者和实践者通过发展优先考虑信任、可解释性和稳健推理的策略，充分利用MLLMs的潜力。最后，我们指出了关键的研究方向，包括新的推理范式、架构创新以及特定领域的应用，以促进利用MLLMs进行时间序列推理的研究进展。 

---
# Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in Multimodal Large Language Models 

**Title (ZH)**: 视觉注意力永不衰退：面向多模态大型语言模型的selective progressive attention recalibration方法在细节图像描述中的应用 

**Authors**: Mingi Jung, Saehuyng Lee, Eunji Kim, Sungroh Yoon  

**Link**: [PDF](https://arxiv.org/pdf/2502.01419)  

**Abstract**: Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs). In this work, we hypothesize that this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence. Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead. 

**Abstract (ZH)**: 详细的图像描述对于数据生成和辅助视障人士等任务至关重要。高质量的描述需要在精确度和召回率之间达到平衡，而当前的跨模态大型语言模型（MLLMs）在实现这一平衡方面仍然存在挑战。在本研究中，我们假设这种限制源于随着响应长度增加所导致的视觉注意力减弱及噪音增加。为了解决这个问题，我们提出了SPARC（选择性渐进注意力重新校准）方法，这是一种无需训练的方法，可以增强解码期间视觉词素的贡献。SPARC建立在三个关键观察的基础上：（1）增加所有视觉词素的影响会降低召回率；因此，SPARC有选择地放大关键视觉词素；（2）随着描述长度的增加，视觉注意力变得越来越嘈杂，所以SPARC通过利用不同时刻之间的注意力差异来识别关键的视觉词素；（3）随着视觉注意力逐渐减弱，SPARC通过强化它来保持其影响。我们的实验表明，现有方法在提高精确度的同时会牺牲召回率；相比之下，我们提出的这种方法在不增加大量计算开销的情况下，同时提高了精确度和召回率。 

---
# Data-Efficient Model for Psychological Resilience Prediction based on Neurological Data 

**Title (ZH)**: 基于神经数据的心理复苏能力高效预测模型 

**Authors**: Zhi Zhang, Yan Liu, Mengxia Gao, Yu Yang, Jiannong Cao, Wai Kai Hou, Shirley Li, Sonata Yau, Yun Kwok Wing, Tatia M. C. Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.01377)  

**Abstract**: Psychological resilience, defined as the ability to rebound from adversity, is crucial for mental health. Compared with traditional resilience assessments through self-reported questionnaires, resilience assessments based on neurological data offer more objective results with biological markers, hence significantly enhancing credibility. This paper proposes a novel data-efficient model to address the scarcity of neurological data. We employ Neuro Kolmogorov-Arnold Networks as the structure of the prediction model. In the training stage, a new trait-informed multimodal representation algorithm with a smart chunk technique is proposed to learn the shared latent space with limited data. In the test stage, a new noise-informed inference algorithm is proposed to address the low signal-to-noise ratio of the neurological data. The proposed model not only shows impressive performance on both public datasets and self-constructed datasets but also provides some valuable psychological hypotheses for future research. 

**Abstract (ZH)**: 心理韧性，定义为从逆境中恢复的能力，对于心理健康至关重要。相比之下，基于神经生物学数据的韧性评估，通过生物学标记提供更客观的结果，显著提高了评估的可信度。本文提出了一种新的数据高效模型，以应对神经生物学数据稀缺的问题。我们采用神经柯尔莫哥洛夫-阿诺尔德网络（Neuro Kolmogorov-Arnold Networks）作为预测模型的结构。在训练阶段，我们提出了一种新的基于特质的多模态表示算法，并结合智能分块技术，以有限的数据学习共享的潜在空间。在测试阶段，我们提出了一种新的受噪声影响的推理算法，以应对神经生物学数据中的低信噪比问题。所提出的模型不仅在公共数据集和自建数据集上显示出了令人印象深刻的性能，还为未来的研究提供了一些有价值的心理假设。 

---
# Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents 

**Title (ZH)**: 可验证的排序与连续性在视觉-语言预训练中的作用：实现通用体态智能体的可迁移性 

**Authors**: Zhizhen Zhang, Lei Zhu, Zhen Fang, Zi Huang, Yadan Luo  

**Link**: [PDF](https://arxiv.org/pdf/2502.01218)  

**Abstract**: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments across varying numbers of demonstrations show that the pretrained features significantly enhance downstream manipulation tasks by up to 49% with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents. The source code is included in the supplementary material for reference. 

**Abstract (ZH)**: 在人类动作视频上预训练视觉-语言表示已成为减少对大规模专家示范依赖的有效方法，从而训练具身智能体。然而，以往的方法往往基于目标到达的启发式方法使用时间对比学习，逐步将语言指令从初始帧对齐到最终帧。这种过度关注未来帧的做法可能导致视觉-语言关联错误，因为动作可能在早期终止或包含不相关的时间片段。为了解决这个问题，我们提出了一种动作时间连贯学习（Action Temporal Coherence Learning, AcTOL）方法，以学习有序且连续的视觉-语言表示而不受刚性目标导向约束。AcTOL 将视频视为一个连续的轨迹，其中（1）通过对比帧之间的语义差异来反映其自然顺序，（2）施加局部布朗桥约束以确保中间帧之间的平滑过渡。大量的模仿学习实验表明，预训练特征可以通过高达 49% 的改进显著增强各种演示数量下的下游操作任务，并且对不同指令语言风格具有很高的鲁棒性，为通用具身智能体的实现提供了可行路径。源代码作为补充材料提供，供参考。 

---
# The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles 

**Title (ZH)**: 《跳跃式的推理曲线？在多模态谜题上的GPT-[n]和o-[n]模型推理性能的进化追踪》 

**Authors**: Vernon Y.H. Toh, Yew Ken Chia, Deepanway Ghosal, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2502.01081)  

**Abstract**: The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available this https URL. 

**Abstract (ZH)**: OpenAI的o1和o3的发布标志着大规模语言模型向高级推理能力的一大范式转变。特别地，o3在人工通用智能抽象和推理语料库（ARC-AGI）的新型问题解决和技能获取方面超过了人类，但该基准仅限于符号模式，而人类通常会在包括视觉和语言数据的多模态情境中感知和推理。因此，有必要进一步研究多模态任务中的高级推理能力。为此，我们追踪了GPT-[n]和o-[n]系列模型在具有细粒度视觉感知和抽象或算法推理需求的挑战性多模态谜题上的发展变化。o1模型的优越性能几乎比GPT-4o高750倍的计算成本，引起对其效率的担忧。我们的研究结果表明，随着模型迭代的进行，推理能力呈现出明显的上升趋势，特别是在GPT系列模型中，随后到o1时性能显著提升。然而，我们注意到o1模型仍然难以应对需要抽象推理的简单多模态谜题，且其在算法谜题中的表现仍较差。计划持续跟进该系列的新模型，并相应地更新本研究中的结果。本研究中使用的所有资源均在此处公开：[提供网址]。 

---
# PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback 

**Title (ZH)**: PlotGen：基于多模态反馈的多智能体大型语言模型科学数据可视化方法 

**Authors**: Kanika Goswami, Puneet Mathur, Ryan Rossi, Franck Dernoncourt  

**Link**: [PDF](https://arxiv.org/pdf/2502.00988)  

**Abstract**: Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors. 

**Abstract (ZH)**: 科学数据可视化对于将原始数据转换为易于理解的视觉表示至关重要，有助于模式识别、预测和数据驱动洞察的呈现。然而，初学者用户往往由于选择合适工具和掌握可视化技术的复杂性而面临困难。大型语言模型（LLMs）最近在辅助代码生成方面显示出潜力，但在准确性上存在挑战，并且需要多次调试。本文提出了一种名为PlotGen的新颖多代理框架，旨在自动化精确科学可视化创建过程。PlotGen协调多个基于LLM的代理，包括一个查询规划代理，它将复杂的用户请求分解为可执行步骤；一个代码生成代理，将伪代码转换为可执行的Python代码；以及三个检索反馈代理——数值反馈代理、词汇反馈代理和视觉反馈代理——它们利用多模态LLM通过自我反省逐步优化生成图表的数据准确性、文本标签和视觉正确性。广泛的实验表明，PlotGen在MatPlotBench数据集上超过了强大的基线模型，实现了4-6%的改进，从而增加了用户对LLM生成可视化结果的信任度，并通过减少绘制错误所需的调试时间提高了初学者的生产力。 

---
# Towards Efficient Large Multimodal Model Serving 

**Title (ZH)**: 面向高效大规模多模态模型服务 

**Authors**: Haoran Qiu, Anish Biswas, Zihan Zhao, Jayashree Mohan, Alind Khare, Esha Choukse, Íñigo Goiri, Zeyu Zhang, Haiying Shen, Chetan Bansal, Ramachandran Ramjee, Rodrigo Fonseca  

**Link**: [PDF](https://arxiv.org/pdf/2502.00937)  

**Abstract**: Recent advances in generative AI have led to large multi-modal models (LMMs) capable of simultaneously processing inputs of various modalities such as text, images, video, and audio. While these models demonstrate impressive capabilities, efficiently serving them in production environments poses significant challenges due to their complex architectures and heterogeneous resource requirements.
We present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, on six representative open-source models. We investigate their multi-stage inference pipelines and resource utilization patterns that lead to unique systems design implications. We also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions, diverse modal combinations, and bursty traffic patterns.
Our key findings reveal that different LMM inference stages exhibit highly heterogeneous performance characteristics and resource demands, while concurrent requests across modalities lead to significant performance interference. To address these challenges, we propose a decoupled serving architecture that enables independent resource allocation and adaptive scaling for each stage. We further propose optimizations such as stage colocation to maximize throughput and resource utilization while meeting the latency objectives. 

**Abstract (ZH)**: 近年来，生成式AI的进步催生了大型多模态模型（LMMs），这些模型能够同时处理各种模态的输入，如文本、图像、视频和音频。尽管这些模型展现了令人印象深刻的性能，但在生产环境中高效地提供服务却面临着巨大的挑战，这主要是由于它们复杂的架构和多样的资源需求。

我们首次对两种主要的LMM架构——解码器唯一架构和交叉注意力架构——进行了全面的系统分析，涉及六种代表性开源模型。我们探讨了它们的多阶段推理流水线和资源利用模式，这些模式导致了独特的系统设计影响。我们还对生产环境中LMM的推理痕迹进行了深入分析，揭示了独特的负载特征，包括可变的、重尾的请求分布、多样的模态组合以及突发的流量模式。

我们的关键发现揭示了不同LMM推理阶段具有高度异质的性能特性和资源需求特征，同时跨模态的并发请求会带来显著的性能干扰。为应对这些挑战，我们提出了一种解耦的服务架构，允许为每个阶段独立分配资源并实现动态扩展。我们还提出了阶段共置等优化措施，以最大化吞吐量和资源利用率，同时满足延迟目标。 

---
# From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal LLMs 

**Title (ZH)**: 从遵守规则到利用：针对多模态LLM的 Jailbreak 对话攻击 

**Authors**: Chun Wai Chiu, Linghan Huang, Bo Li, Huaming Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.00735)  

**Abstract**: Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the frontier multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. To better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flank Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios. These findings highlight both the potency of prompt-based obfuscation in voice-enabled contexts and the limitations of current LLMs' moderation safeguards and the urgent need for advanced defense strategies to address the challenges posed by evolving, context-rich attacks. 

**Abstract (ZH)**: 大型语言模型（LLMs）由于其处理不同类型输入数据（包括文本、音频、图像和视频）的能力不断增强，已在众多领域得到了广泛应用。尽管LLMs在理解和生成不同场景下的语境方面表现出色，但它们对基于提示的攻击易受攻击，这些攻击大多通过文本输入实施。本文介绍了针对多模态LLMs的第一种基于声音的越狱攻击，称为边缘攻击（Flanking Attack），该攻击可以同时处理多种类型的输入以应对多模态LLMs。我们的研究受近年来基于单一语言的声音驱动大型语言模型 advancements 启发，这些模型为LLMs引入了新的攻击面，超出了传统基于文本的漏洞。为了研究这些风险，我们考察了可以通过多种输入类型（如音频输入）访问的前沿多模态LLMs，并关注敌对提示如何绕过其防御机制。我们提出了一种新的策略，在这种策略中，禁止的提示由良性、叙述驱动的提示包围。该策略整合在边缘攻击中，旨在通过虚构的背景使交互情境人性化，并通过该背景执行攻击。为了更好地评估攻击性能，我们提出了一个半自动化的自我评估框架，用于检测违规行为。研究表明，边缘攻击能够操纵最先进的LLMs生成对齐不当和禁止的输出，在七个禁止场景中，攻击的成功率范围从0.67到0.93不等。这些发现突显了语音启用环境中基于提示混淆的效力，以及当前LLMs的监控保护措施的局限性，强调了亟需先进的防御策略以应对不断演变、富含上下文的攻击带来的挑战。 

---
# TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease Prognosis From Imaging, Clinical, and Radiomic Data Fusion 

**Title (ZH)**: TMI-CLNet：融合影像学、临床和影像组学数据的三模态交互网络用于慢性肝病预后研究 

**Authors**: Linglong Wu, Xuhao Shan, Ruiquan Ge, Ruoyu Liang, Chi Zhang, Yonghong Li, Ahmed Elazab, Huoling Luo, Yunbi Liu, Changmiao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.00695)  

**Abstract**: Chronic liver disease represents a significant health challenge worldwide and accurate prognostic evaluations are essential for personalized treatment plans. Recent evidence suggests that integrating multimodal data, such as computed tomography imaging, radiomic features, and clinical information, can provide more comprehensive prognostic information. However, modalities have an inherent heterogeneity, and incorporating additional modalities may exacerbate the challenges of heterogeneous data fusion. Moreover, existing multimodal fusion methods often struggle to adapt to richer medical modalities, making it difficult to capture inter-modal relationships. To overcome these limitations, We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet). Specifically, we develop an Intra-Modality Aggregation module and a Triple-Modal Cross-Attention Fusion module, which are designed to eliminate intra-modality redundancy and extract cross-modal information, respectively. Furthermore, we design a Triple-Modal Feature Fusion loss function to align feature representations across modalities. Extensive experiments on the liver prognosis dataset demonstrate that our approach significantly outperforms existing state-of-the-art unimodal models and other multi-modal techniques. Our code is available at this https URL. 

**Abstract (ZH)**: 慢性肝病是全球性的重要健康挑战，精准的预后评估对于个性化治疗计划至关重要。近期研究表明，整合多模态数据，如计算机断层扫描影像、影像组学特征和临床信息，可以提供更为全面的预后信息。然而，不同模态之间存在固有的异质性，引入额外的模态可能会加剧异质性数据融合的挑战。此外，现有的多模态融合方法往往难以适应更丰富的医学模态数据，难以捕捉跨模态的关系。为克服这些局限性，我们提出了三模态交互慢性肝网络（TMI-CLNet）。具体来说，我们开发了内模态聚合模块和三模态交叉注意力融合模块，前者旨在消除内模态冗余，后者则用于提取跨模态信息。此外，我们设计了一种三模态特征融合损失函数，以在不同模态之间对齐特征表示。在肝脏预后数据集上的广泛实验表明，我们的方法显著优于现有的最先进的单模态模型和其他多模态技术。我们的代码可在以下链接获取：此 http URL。 

---
# Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions 

**Title (ZH)**: 从癌症组织病理学生成跨模态基因表达以改进多模态AI预测 

**Authors**: Samiran Dey, Christopher R.S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti  

**Link**: [PDF](https://arxiv.org/pdf/2502.00568)  

**Abstract**: Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathoGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathoGen code is available for open use by the research community through GitHub at this https URL. 

**Abstract (ZH)**: 新兴研究表明，基于人工智能的多模态融合，结合数字病理学和转录组特征，可以提高癌症诊断（分级/亚型分类）和预后（生存风险预测）的准确性。然而，在实际临床环境中，直接融合联合决策是不切实际的，因为组织病理学仍然是诊断的金标准，而转录组检测在公共医疗保健系统中很少被要求。借助我们新颖的基于扩散的跨模态生成AI模型PathoGen，我们证明了从数字组织病理学推断的基因表达能够以高精度（前沿性能）、高确定性（通过置信覆盖保证）和高可解释性（通过分布式注意力图）预测癌症分级和患者生存风险。PathoGen代码可通过GitHub（请访问此链接：[这里]）对研究社区开放使用。 

---
# Milmer: a Framework for Multiple Instance Learning based Multimodal Emotion Recognition 

**Title (ZH)**: Milmer：基于多模态情感识别的多个实例学习框架 

**Authors**: Zaitian Wang, Jian He, Yu Liang, Xiyuan Hu, Tianhao Peng, Kaixin Wang, Jiakai Wang, Chenlong Zhang, Weili Zhang, Shuang Niu, Xiaoyang Xie  

**Link**: [PDF](https://arxiv.org/pdf/2502.00547)  

**Abstract**: Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a cross-modal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies in enhancing emotion recognition performance. Our code are available at this https URL. 

**Abstract (ZH)**: 情绪在人类行为和决策中扮演着至关重要的角色，因此情绪识别成为人机交互（HCI）领域的一个关键研究方向。本研究通过将面部表情分析与脑电信号（EEG）结合，提出了一种名为Milmer的新颖多模态框架，以应对情绪识别的挑战。该框架采用基于变换器的方法，有效地整合了视觉和生理模态信息。具体而言，该框架包括一个EEG预处理模块、一个面部特征提取与平衡模块以及一个跨模态融合模块。为增强视觉特征提取，我们选用预训练的Swin Transformer对情绪相关数据集进行微调。此外，引入了跨注意力机制来平衡各模态的令牌表示，确保有效的特征融合。本研究的一个关键创新是采用了多次实例学习（MIL）方法，该方法能够从多个随时间变化的面部表情图像中提取有意义的信息，捕捉前人研究中往往忽略的关键时间动态特性。在DEAP数据集上进行的大量实验结果表明，所提出的框架在四类情绪识别任务中达到了96.72%的分类准确率。进一步的消融研究表明，各模块的贡献显著，突显了先进特征提取和融合策略在提升情绪识别性能方面的重要性。我们的代码可在以下链接获取：[该链接]。 

---
# Generic Multimodal Spatially Graph Network for Spatially Embedded Network Representation Learning 

**Title (ZH)**: 通用的多模态空间图网络，用于空间嵌入网络表示学习 

**Authors**: Xudong Fan, Jürgen Hackl  

**Link**: [PDF](https://arxiv.org/pdf/2502.00530)  

**Abstract**: Spatially embedded networks (SENs) represent a special type of complex graph, whose topologies are constrained by the networks' embedded spatial environments. The graph representation of such networks is thereby influenced by the embedded spatial features of both nodes and edges. Accurate network representation of the graph structure and graph features is a fundamental task for various graph-related tasks. In this study, a Generic Multimodal Spatially Graph Convolutional Network (GMu-SGCN) is developed for efficient representation of spatially embedded networks. The developed GMu-SGCN model has the ability to learn the node connection pattern via multimodal node and edge features. In order to evaluate the developed model, a river network dataset and a power network dataset have been used as test beds. The river network represents the naturally developed SENs, whereas the power network represents a man-made network. Both types of networks are heavily constrained by the spatial environments and uncertainties from nature. Comprehensive evaluation analysis shows the developed GMu-SGCN can improve accuracy of the edge existence prediction task by 37.1\% compared to a GraphSAGE model which only considers the node's position feature in a power network test bed. Our model demonstrates the importance of considering the multidimensional spatial feature for spatially embedded network representation. 

**Abstract (ZH)**: 以下是将您提供的内容翻译成中文后的版本，符合学术规范：

位置嵌入网络（Spatially Embedded Networks, SENs）是一类特殊的复杂网络，其拓扑结构受到网络嵌入的空间环境的约束。这类网络的图表示形式受节点和边嵌入的空间特征的影响。准确地表示图结构和图特征是各种图相关任务中的基础任务。本研究开发了一种通用多模态空间图卷积网络（GMu-SGCN），用于高效表示位置嵌入网络。所开发的GMu-SGCN模型能够通过多模态节点和边特征学习节点连接模式。为评估所开发的模型，使用了河流网络数据集和电力网络数据集作为试验平台。河流网络代表了自然发展的SENs，而电力网络则代表了人工构建的网络。这两种类型的网络均受到自然环境和不确定性的影响较大。综合评估分析表明，与仅考虑节点位置特征的GraphSAGE模型相比，所开发的GMu-SGCN在电力网络试验平台上将边存在预测任务的准确性提高了37.1%。本研究表明，在位置嵌入网络表示中考虑多维度空间特征的重要性。 

---
# MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization 

**Title (ZH)**: MQuant：通过全静态量化释放多模态大型语言模型的推理潜力 

**Authors**: JiangYong Yu, Sifan Zhou, Dawei Yang, Shuo Wang, Shuoyu Li, Xing Hu, Chen Xu, Zukang Xu, Changyong Shu, Zhihang Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2502.00425)  

**Abstract**: Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and this http URL quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles with MLLMs because of (a) high inference latency from large visual token counts, (b) distributional disparities between visual and textual tokens, and (c) extreme outliers introduced by Hadamard-based transformations. To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs. textual tokens; Attention-Invariant Flexible Switching (AIFS), reordering tokens to preserve casual attention while eliminating expensive token-wise scale computations; Rotation Magnitude Suppression (RMS), mitigating weight outliers arising from online Hadamard rotations. On five mainstream MLLMs (including Qwen-VL, MiniCPM-V, CogVLM2), MQuant under W4A8 achieves near-floating-point accuracy (<1% degradation) while reducing inference latency by up to 30%, significantly outperforming existing PTQ baselines. Our MQuant effectively bridges the gap for efficient and accurate MLLMs inference in resource-constrained devices. Code will be released. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）因其能够理解多模态输入而引起了广泛的关注。然而，它们庞大的参数量和巨大的计算需求严重阻碍了其实际部署。量化是一种有效的方法来减少模型大小和推断延迟，但其在MLLMs中的应用仍然不足。本文我们提出了一种针对多模态大型语言模型（MLLMs）独特挑战的后训练量化（PTQ）框架，MQuant。传统的量化方法常常难以应对MLLMs，因为（a）大量视觉词元导致的高推断延迟，（b）视觉和文本词元之间的分布差异，以及（c）Hadamard变换引入的极端异常值。为了解决这些问题，MQuant 引入了以下方法：模态特定静态量化（MSQ），为视觉和文本词元分配不同的静态缩放比例；注意不变的灵活切换（AIFS），重新排列词元以保持因果注意同时消除昂贵的词元级缩放计算；旋转幅度抑制（RMS），减轻在线Hadamard旋转引起的权重异常值。在五种主流的MLLMs（包括Qwen-VL、MiniCPM-V、CogVLM2）中，MQuant 在W4A8量化下实现了接近浮点精度（<1% 的退化）的同时将推断延迟降低了高达30%，显著优于现有的PTQ基准。我们的MQuant有效地填补了资源受限设备中高效准确的MLLMs推断的空白。代码将开源。 

---
# Do Audio-Visual Segmentation Models Truly Segment Sounding Objects? 

**Title (ZH)**: audio-visual分割模型真地能够分隔出发音对象吗？ 

**Authors**: Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, Yapeng Tian  

**Link**: [PDF](https://arxiv.org/pdf/2502.00358)  

**Abstract**: Unlike traditional visual segmentation, audio-visual segmentation (AVS) requires the model not only to identify and segment objects but also to determine whether they are sound sources. Recent AVS approaches, leveraging transformer architectures and powerful foundation models like SAM, have achieved impressive performance on standard benchmarks. Yet, an important question remains: Do these models genuinely integrate audio-visual cues to segment sounding objects? In this paper, we systematically investigate this issue in the context of robust AVS. Our study reveals a fundamental bias in current methods: they tend to generate segmentation masks based predominantly on visual salience, irrespective of the audio context. This bias results in unreliable predictions when sounds are absent or irrelevant. To address this challenge, we introduce AVSBench-Robust, a comprehensive benchmark incorporating diverse negative audio scenarios including silence, ambient noise, and off-screen sounds. We also propose a simple yet effective approach combining balanced training with negative samples and classifier-guided similarity learning. Our extensive experiments show that state-of-theart AVS methods consistently fail under negative audio conditions, demonstrating the prevalence of visual bias. In contrast, our approach achieves remarkable improvements in both standard metrics and robustness measures, maintaining near-perfect false positive rates while preserving highquality segmentation performance. 

**Abstract (ZH)**: 与传统的视觉分割不同，音频-视觉分割（AVS）不仅要求模型识别和分割对象，还需确定这些对象是否为声源。近期，利用变换器架构和强大的基础模型（如SAM）的AVS方法在标准基准测试中取得了令人印象深刻的表现。然而，一个重要问题仍然存在：这些模型是否真的综合了音频-视觉线索来分割产声对象？在本文中，我们系统地探讨了这一问题在鲁棒AVS的背景下。我们的研究表明，当前方法存在一个根本性的偏见：它们倾向于主要依赖视觉显著性生成分割掩码，而不考虑音频上下文。这种偏见导致在声音缺失或无关时预测不可靠。为了解决这一挑战，我们提出了AVSBench-Robust，这是一个全面基准，包含了多样化的负音频场景，包括沉默、环境声以及离屏声。我们还提出了一种结合平衡训练与负样本和分类器引导相似性学习的简单而有效的方法。广泛的实验表明，最先进的AVS方法在负音频条件下一致性地表现不佳，揭示了视觉偏见的普遍存在。相比之下，我们的方法在标准指标和鲁棒性指标上均取得了显著的改进，同时保持了近乎完美的假阳性率和高质量的分割性能。 

---
# MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior Modeling 

**Title (ZH)**: MIM：多模态内容兴趣建模范式用于用户行为建模 

**Authors**: Bencheng Yan, Si Chen, Shichang Jia, Jianyu Liu, Yueran Liu, Chenghan Fu, Wanxian Guan, Hui Zhao, Xiang Zhang, Kai Zhang, Wenbo Su, Pengjie Wang, Jian Xu, Bo Zheng, Baolin Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.00321)  

**Abstract**: Click-Through Rate (CTR) prediction is a crucial task in recommendation systems, online searches, and advertising platforms, where accurately capturing users' real interests in content is essential for performance. However, existing methods heavily rely on ID embeddings, which fail to reflect users' true preferences for content such as images and titles. This limitation becomes particularly evident in cold-start and long-tail scenarios, where traditional approaches struggle to deliver effective results. To address these challenges, we propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which consists of three key stages: Pre-training, Content-Interest-Aware Supervised Fine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training stage adapts foundational models to domain-specific data, enabling the extraction of high-quality multi-modal embeddings. The C-SFT stage bridges the semantic gap between content and user interests by leveraging user behavior signals to guide the alignment of embeddings with user preferences. Finally, the CiUBM stage integrates multi-modal embeddings and ID-based collaborative filtering signals into a unified framework. Comprehensive offline experiments and online A/B tests conducted on the Taobao, one of the world's largest e-commerce platforms, demonstrated the effectiveness and efficiency of MIM method. The method has been successfully deployed online, achieving a significant increase of +14.14% in CTR and +4.12% in RPM, showcasing its industrial applicability and substantial impact on platform performance. To promote further research, we have publicly released the code and dataset at this https URL. 

**Abstract (ZH)**: 点击率（CTR）预测是推荐系统、在线搜索和广告平台中的关键任务，准确捕捉用户对内容的真实兴趣对于提高性能至关重要。然而，现有的方法过度依赖于ID嵌入，这无法反映用户对诸如图像和标题等内容的真实偏好。这种限制在冷启动和长尾场景中尤为明显，传统方法在这种情况下难以提供有效的结果。为了解决这些挑战，我们提出了一种新型的多模态内容兴趣建模框架（MIM），它包含三个关键阶段：预训练、内容-兴趣感知监督微调（C-SFT）以及内容-兴趣感知统一模型（CiUBM）。预训练阶段将基础模型适应特定领域的数据，使其能够提取高质量的多模态嵌入。C-SFT阶段通过利用用户行为信号来弥合内容与用户兴趣之间的语义差距，从而引导嵌入与用户偏好的对齐。最后，CiUBM阶段将多模态嵌入和基于ID的合作过滤信号整合到一个统一框架中。在淘宝，全球最大的电商平台之一，进行的全面离线实验和在线A/B测试表明了MIM方法的有效性和效率。该方法已成功部署在线，实现了点击率CTR提升14.14%，收益每千次展示RPM提升4.12%，展示了其工业应用前景及其在平台性能上的显著影响。为了促进进一步研究，我们已在以下网址公开发布了代码和数据集：[此链接](this https URL)。 

---
# Mordal: Automated Pretrained Model Selection for Vision Language Models 

**Title (ZH)**: Mordal：自动预训练模型选择用于视觉语言模型 

**Authors**: Shiqi He, Insu Jang, Mosharaf Chowdhury  

**Link**: [PDF](https://arxiv.org/pdf/2502.00241)  

**Abstract**: Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category of multimodal models because of their many practical use cases, including in healthcare, robotics, and accessibility. Unfortunately, even though different VLMs in the literature demonstrate impressive visual capabilities in different benchmarks, they are handcrafted by human experts; there is no automated framework to create task-specific multimodal models.
We introduce Mordal, an automated multimodal model search framework that efficiently finds the best VLM for a user-defined task without manual intervention. Mordal achieves this both by reducing the number of candidates to consider during the search process and by minimizing the time required to evaluate each remaining candidate. Our evaluation shows that Mordal can find the best VLM for a given problem using up to $8.9\times$--$11.6\times$ lower GPU hours than grid search. In the process of our evaluation, we have also discovered new VLMs that outperform their state-of-the-art counterparts. 

**Abstract (ZH)**: 将以下论文内容或标题翻译成中文，符合学术规范：

将多种模态数据集成到大型语言模型（LLMs）中是增强其对非文本数据理解能力的一种有力方式，使其能够执行多模态任务。视觉语言模型（VLMs）因其在医疗保健、机器人技术及无障碍领域的广泛应用而成为发展最快的多模态模型类别。不幸的是，尽管文献中不同VLM在各种基准测试中展现了令人印象深刻的视觉能力，它们都是由人类专家手工构建的；目前尚无自动化框架能够创建针对特定任务的多模态模型。

我们介绍了Mordal，一种自动化的多模态模型搜索框架，该框架能在无需人工干预的情况下高效地为用户定义的任务找到最佳VLM。Mordal 通过减少搜索过程中需要考虑的候选模型数量以及最小化评估每个剩余候选模型所需的时间，实现了这一点。我们的评估表明，与网格搜索相比，Mordal 可以使用多达 8.9 倍至 11.6 倍少的 GPU 时长找到给定问题的最佳 VLM。在评估过程中，我们还发现了一些优于其最先进同类模型的新VLM。 

---
# Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study 

**Title (ZH)**: 多模态MRI-超声AI在前列腺癌检测中的性能优于放射科医生的MRI解释：一项多中心研究 

**Authors**: Hassan Jahanandish, Shengtian Sang, Cynthia Xinran Li, Sulaiman Vesal, Indrani Bhattacharya, Jeong Hoon Lee, Richard Fan, Geoffrey A. Sonna, Mirabela Rusu  

**Link**: [PDF](https://arxiv.org/pdf/2502.00146)  

**Abstract**: Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target suspicious prostate lesions. This has led to artificial intelligence (AI) applications improving MRI-based detection of clinically significant prostate cancer (CsPCa). However, MRI-detected lesions must still be mapped to transrectal ultrasound (TRUS) images during biopsy, which results in missing CsPCa. This study systematically evaluates a multimodal AI framework integrating MRI and TRUS image sequences to enhance CsPCa identification. The study included 3110 patients from three cohorts across two institutions who underwent prostate biopsy. The proposed framework, based on the 3D UNet architecture, was evaluated on 1700 test cases, comparing performance to unimodal AI models that use either MRI or TRUS alone. Additionally, the proposed model was compared to radiologists in a cohort of 110 patients. The multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice (42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared to radiologists, the multimodal model showed higher specificity (88% vs. 78%) and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings demonstrate the potential of multimodal AI to improve CsPCa lesion targeting during biopsy and treatment planning, surpassing current unimodal models and radiologists; ultimately improving outcomes for prostate cancer patients. 

**Abstract (ZH)**: 磁共振成像（MRI）在活检前已被广泛用于定位可疑的前列腺病变。这促使了人工智能（AI）在基于MRI的临床显著前列腺癌（CsPCa）检测中的应用得到了改进。然而，在进行经直肠超声（TRUS）引导的活检时，仍需将MRI检测到的病变与TRUS图像进行匹配，这可能导致遗漏CsPCa。本研究系统评估了一种整合MRI和TRUS图像序列的多模态AI框架，以提高CsPCa的识别能力。该研究包括了两个机构的三个队列共3110例接受前列腺活检的患者。基于3D UNet结构提出的框架，在1700个测试案例中进行了评估，将其性能与仅使用MRI或TRUS的单模态AI模型进行了比较。此外，该模型还与110例患者的放射科医师进行了比较。多模态AI方法在敏感性（80%）和病灶Dice系数（42%）方面优于单模态MRI（73%，30%）和TRUS模型（49%，27%）。与放射科医师相比，多模态模型在特异性（88% vs. 78%）和病灶Dice系数（38% vs. 33%）上表现更好，而灵敏度相当（79%）。我们的研究结果表明，多模态AI有可能提高CsPCa病灶在活检和治疗规划中的定位能力，超越现有的单模态模型和放射科医师，最终改善前列腺癌患者的预后。 

---
# AIN: The Arabic INclusive Large Multimodal Model 

**Title (ZH)**: AIN：阿拉伯语包容性大型多模态模型 

**Authors**: Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan  

**Link**: [PDF](https://arxiv.org/pdf/2502.00094)  

**Abstract**: Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications. 

**Abstract (ZH)**: 在大型语言模型（LLMs）和大型多模态模型（LMMs）迅速发展的过程中，英语和汉语等资源丰富语言的多模态技术取得了显著进展。尽管阿拉伯语LLM取得了显著进步，但阿拉伯语LMM仍然相对未被充分探索，往往集中在语言和视觉理解的少数特定方面。为了弥合这一差距，我们引入了AIN（阿拉伯包容性多模态模型），旨在跨多种领域表现出色。AIN是一种兼具英阿双语能力的LMM，利用精心构建的360万高质量阿拉伯-英语多模态数据样本，具备卓越的阿拉伯语性能，同时也拥有强大的英语视觉能力。

在最新发布的CAMEL-Bench基准测试中，该基准测试涵盖38个子领域，包括多图像理解、复杂视觉知觉、手写文档理解、视频理解、医学成像、植物疾病诊断以及基于遥感的土地利用理解，我们的AIN模型在7B参数规模的模型中，在八个领域和38个子领域上的平均绝对提升率为3.4%，表现出色。AIN的卓越能力使其成为推动阿拉伯语使用者在各种应用中获得高级多模态生成AI工具的关键一步。 

---
# FutureVision: A methodology for the investigation of future cognition 

**Title (ZH)**: 未来认知探究的方法论：FutureVision 

**Authors**: Tiago Timponi Torrent, Mark Turner, Nicolás Hinrichs, Frederico Belcavello, Igor Lourenço, Arthur Lorenzi Almeida, Marcelo Viridiano, Ely Edison Matos  

**Link**: [PDF](https://arxiv.org/pdf/2502.01597)  

**Abstract**: This paper presents a methodology combining multimodal semantic analysis with an eye-tracking experimental protocol to investigate the cognitive effort involved in understanding the communication of future scenarios. To demonstrate the methodology, we conduct a pilot study examining how visual fixation patterns vary during the evaluation of valence and counterfactuality in fictional ad pieces describing futuristic scenarios, using a portable eye tracker. Participants eye movements are recorded while evaluating the stimuli and describing them to a conversation partner. Gaze patterns are analyzed alongside semantic representations of the stimuli and participants descriptions, constructed from a frame semantic annotation of both linguistic and visual modalities. Preliminary results show that far-future and pessimistic scenarios are associated with longer fixations and more erratic saccades, supporting the hypothesis that fractures in the base spaces underlying the interpretation of future scenarios increase cognitive load for comprehenders. 

**Abstract (ZH)**: 本文提出了一种结合多模态语义分析与眼球追踪实验协议的方法，以探究理解未来场景交流过程中所涉及的认知努力。为了展示该方法，我们进行了一个试点研究，探讨在评估虚构广告件中描述的未来场景的情感价值和假设性时，视觉固定模式的变化情况。使用便携式眼动仪记录参与者评估刺激并将其描述给对话伙伴时的眼球运动。眼动轨迹与刺激的语义表示以及参与者描述的语义表示（从语言和视觉模态的框架语义注释构建）进行分析。初步结果表明，远未来和悲观的场景与更长的固定时间和更不规则的眨眼运动相关，这支持了假设，即未来场景解释的基础空间中的断裂会增加理解者的认知负荷。 

---
# AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding 

**Title (ZH)**: AlignVLM：视觉与语言潜在空间的对齐以实现多模态理解 

**Authors**: Ahmed Masry, Juan A. Rodriguez, Tianyu Zhang, Suyuchen Wang, Chao Wang, Aarash Feizi, Akshay Kalkunte Suresh, Abhay Puri, Xiangru Jian, Pierre-André Noël, Sathwik Tejaswi Madhusudhan, Marco Pedersoli, Bang Liu, Nicolas Chapados, Yoshua Bengio, Enamul Hoque, Christopher Pal, Issam H. Laradji, David Vazquez, Perouz Taslakian, Spandana Gella, Sai Rajeswar  

**Link**: [PDF](https://arxiv.org/pdf/2502.01341)  

**Abstract**: Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise. 

**Abstract (ZH)**: 在视觉语言模型（VLMs）中，将视觉特征与语言嵌入对齐是一项关键挑战。这类模型的性能依赖于一个良好的连接器，该连接器能够将视觉编码器生成的视觉特征映射到与大型语言模型（LLM）共享的嵌入空间中，同时保持语义相似性。现有连接器，如多层感知机（MLPs），往往会生成分布外或噪声输入，从而导致模态之间的对齐偏差。在本工作中，我们提出了一种新的视觉-文本对齐方法——AlignVLM，该方法将视觉特征映射到LLM文本嵌入的加权平均值上。我们的方法利用了嵌入在LLM中的语言先验知识，以确保视觉特征被映射到LLM能够有效解释的区域。AlignVLM特别适用于文档理解任务，在这些任务中，扫描的文档图像必须准确映射到其文本内容上。我们的广泛实验表明，AlignVLM在与先前对齐方法相比时表现出最先进的性能。我们还进一步分析了对视觉-文本特征对齐改进及抗噪声性增强的验证。 

---
# COVE: COntext and VEracity prediction for out-of-context images 

**Title (ZH)**: COVE: 基于上下文和真伪预测的脱掺混图像处理 

**Authors**: Jonathan Tonglet, Gabriel Thiem, Iryna Gurevych  

**Link**: [PDF](https://arxiv.org/pdf/2502.01194)  

**Abstract**: Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image's caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction model on all context items, often by more than five percentage points. It is competitive with the best veracity prediction models on synthetic data and outperforms them on real-world data, showing that it is beneficial to combine the two tasks sequentially. Finally, we conduct a human study that reveals that the predicted context is a reusable and interpretable artifact to verify new out-of-context captions for the same image. Our code and data are made available. 

**Abstract (ZH)**: 脱离上下文的图像是最常见的多模态误导信息形式。驳斥这些信息需要（1）提供图像的真实上下文，以及（2）验证图像标题的真实性。然而，现有的自动化事实核查方法未能明确解决这两个目标。在本工作中，我们介绍了COVE，这是一种新颖的方法，首先预测图像的真实上下文，然后利用该上下文来预测标题的真实性。COVE在所有上下文项上的表现优于当前最先进的上下文预测模型，有时高出五个以上的百分点。在合成数据上，它与最佳标题真实性预测模型具有竞争力；而在现实数据上则超过了它们，这表明将两个任务顺序组合起来是有益的。最后，我们进行了一项人类研究，结果显示预测的上下文是一个可重复利用且可解释的产物，可用于验证同一图像的新脱离上下文的标题。我们的代码和数据已对外开放。 

---
# Challenges and Innovations in LLM-Powered Fake News Detection: A Synthesis of Approaches and Future Directions 

**Title (ZH)**: 基于LLM的虚假新闻检测挑战与创新：方法综述与未来方向 

**Authors**: Jingyuan Yi, Zeqiu Xu, Tianyi Huang, Peiyang Yu  

**Link**: [PDF](https://arxiv.org/pdf/2502.00339)  

**Abstract**: The pervasiveness of the dissemination of fake news through social media platforms poses critical risks to the trust of the general public, societal stability, and democratic institutions. This challenge calls for novel methodologies in detection, which can keep pace with the dynamic and multi-modal nature of misinformation. Recent works include powering the detection using large language model advances in multimodal frameworks, methodologies using graphs, and adversarial training in the literature of fake news. Based on the different approaches which can bring success, some key highlights will be underlined: enhanced LLM-improves accuracy through more advanced semantics and cross-modality fusion for robust detections. The review further identifies critical gaps in adaptability to dynamic social media trends, real-time, and cross-platform detection capabilities, as well as the ethical challenges thrown up by the misuse of LLMs. Future directions underline the development of style-agnostic models, cross-lingual detection frameworks, and robust policies with a view to mitigating LLM-driven misinformation. This synthesis thus lays a concrete foundation for those researchers and practitioners committed to reinforcing fake news detection systems with complications that keep on growing in the digital landscape. 

**Abstract (ZH)**: 社交媒体平台上传播假新闻的普遍性对公众信任、社会稳定和民主机构构成了关键性风险。面对这一挑战，需要采用新的检测方法，这些方法能够跟上 misinformation 的动态性和多模态特性。近期的研究工作包括利用大语言模型的进展在多模态框架中增强检测能力、基于图形的方法，以及仿真实训方法。基于不同方法的成功应用，以下几点将是关键亮点：增强的大语言模型通过更先进的语义和跨模态融合提高检测准确性。此外，该综述还指出了适应动态社交媒体趋势、实时和跨平台检测能力以及滥用大语言模型引发的伦理挑战的关键漏洞。未来的研究方向将包括开发风格无关模型、跨语言检测框架以及制定严格的政策以应对由大语言模型驱动的错误信息。因此，这一综合分析为致力于强化日益复杂的数字环境中假新闻检测系统的研究人员和实践者奠定了坚实的基础。 

---
# VisTA: Vision-Text Alignment Model with Contrastive Learning using Multimodal Data for Evidence-Driven, Reliable, and Explainable Alzheimer's Disease Diagnosis 

**Title (ZH)**: VisTA：一种基于对比学习的多模态数据视图-文本对齐模型，用于证据驱动、可靠且可解释的阿尔茨海默病诊断 

**Authors**: Duy-Cat Can, Linh D. Dang, Quang-Huy Tang, Dang Minh Ly, Huong Ha, Guillaume Blanc, Oliver Y. Chén, Binh T. Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2502.01535)  

**Abstract**: Objective: Assessing Alzheimer's disease (AD) using high-dimensional radiology images is clinically important but challenging. Although Artificial Intelligence (AI) has advanced AD diagnosis, it remains unclear how to design AI models embracing predictability and explainability. Here, we propose VisTA, a multimodal language-vision model assisted by contrastive learning, to optimize disease prediction and evidence-based, interpretable explanations for clinical decision-making.
Methods: We developed VisTA (Vision-Text Alignment Model) for AD diagnosis. Architecturally, we built VisTA from BiomedCLIP and fine-tuned it using contrastive learning to align images with verified abnormalities and their descriptions. To train VisTA, we used a constructed reference dataset containing images, abnormality types, and descriptions verified by medical experts. VisTA produces four outputs: predicted abnormality type, similarity to reference cases, evidence-driven explanation, and final AD diagnoses. To illustrate VisTA's efficacy, we reported accuracy metrics for abnormality retrieval and dementia prediction. To demonstrate VisTA's explainability, we compared its explanations with human experts' explanations.
Results: Compared to 15 million images used for baseline pretraining, VisTA only used 170 samples for fine-tuning and obtained significant improvement in abnormality retrieval and dementia prediction. For abnormality retrieval, VisTA reached 74% accuracy and an AUC of 0.87 (26% and 0.74, respectively, from baseline models). For dementia prediction, VisTA achieved 88% accuracy and an AUC of 0.82 (30% and 0.57, respectively, from baseline models). The generated explanations agreed strongly with human experts' and provided insights into the diagnostic process. Taken together, VisTA optimize prediction, clinical reasoning, and explanation. 

**Abstract (ZH)**: 目的：利用高维放射影像评估阿尔茨海默病（AD）在临床实践中具有重要的应用价值，但同时也是一个挑战。尽管人工智能（AI）在AD诊断方面取得了显著进展，但在设计兼具预测能力和解释性的AI模型方面仍存在不确定性。为此，我们提出了一种名为VisTA（Vision-Text Alignment Model）的方法，该方法借助对比学习，在优化疾病预测的同时也提供证据支持、可解释的解释，以促进临床决策制定。

方法：我们开发了VisTA对AD进行诊断。架构上，我们基于BiomedCLIP构建VisTA，并通过对比学习进行微调，以使图像与其认证异常及其描述相匹配。为训练VisTA，我们使用了一个由医学专家验证的参考数据集，其中包括图像、异常类型以及描述。VisTA产生的四个输出分别为：预测的异常类型、与参考案例的相似度、基于证据的解释以及最终的AD诊断。为了展示VisTA的有效性，我们报告了异常检索和痴呆预测的准确性指标。为了展示VisTA的可解释性，我们将其解释与人类专家的解释进行了比较。

结果：与用于基线预训练的1500万张图像相比，VisTA仅使用了170个样本进行微调，并在异常检索和痴呆预测方面取得了显著改进。在异常检索方面，VisTA的准确率为74%，AUC为0.87（基线模型分别为26%和0.74）。在痴呆预测方面，VisTA的准确率为88%，AUC为0.82（基线模型分别为30%和0.57）。生成的解释与人类专家的意见高度一致，并提供了诊断过程的洞察。总体而言，VisTA优化了预测、临床推理和解释。 

---
# The in-context inductive biases of vision-language models differ across modalities 

**Title (ZH)**: 视觉语言模型在不同模态中的上下文归纳偏置存在差异 

**Authors**: Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen  

**Link**: [PDF](https://arxiv.org/pdf/2502.01530)  

**Abstract**: Inductive biases are what allow learners to make guesses in the absence of conclusive evidence. These biases have often been studied in cognitive science using concepts or categories -- e.g. by testing how humans generalize a new category from a few examples that leave the category boundary ambiguous. We use these approaches to study generalization in foundation models during in-context learning. Modern foundation models can condition on both vision and text, and differences in how they interpret and learn from these different modalities is an emerging area of study. Here, we study how their generalizations vary by the modality in which stimuli are presented, and the way the stimuli are described in text. We study these biases with three different experimental paradigms, across three different vision-language models. We find that the models generally show some bias towards generalizing according to shape over color. This shape bias tends to be amplified when the examples are presented visually. By contrast, when examples are presented in text, the ordering of adjectives affects generalization. However, the extent of these effects vary across models and paradigms. These results help to reveal how vision-language models represent different types of inputs in context, and may have practical implications for the use of vision-language models. 

**Abstract (ZH)**: 归纳偏置允许学习者在缺乏确凿证据的情况下做出推测。这些偏置在认知科学中通常通过使用概念或类别进行研究——例如，通过测试人类如何从几个边界模糊的例子中泛化出一个新的类别。我们采用类似的方法来研究基础模型在上下文学习中的泛化能力。现代基础模型可以同时处理视觉和文本信息，它们如何解释和从中学习这些不同模态的差异是一个新兴的研究领域。在此，我们研究了不同模态下刺激呈现方式与文本描述方式对模型泛化能力的影响。我们使用三种不同的实验范式，在三种不同的视觉-语言模型上进行了研究。结果显示，这些模型通常倾向于根据形状而非颜色进行泛化。这种形状偏置在视觉呈现的示例中更加明显。相反，当示例以文本形式呈现时，形容词的顺序会影响泛化。然而，这些效果在不同模型和范式之间存在差异。这些结果有助于揭示视觉-语言模型如何在上下文中表示不同类型的信息，并可能对视觉-语言模型的应用具有实际意义。 

---
# Mitigating Hallucinations in Large Vision-Language Models with Internal Fact-based Contrastive Decoding 

**Title (ZH)**: 使用内部事实对比解码减轻大规模视觉-语言模型的幻觉问题 

**Authors**: Chao Wang, Xuancheng Zhou, Weiwei Fu, Yang Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2502.01056)  

**Abstract**: Large Visual Language Models (LVLMs) integrate visual and linguistic modalities, exhibiting exceptional performance across various multimodal tasks. Nevertheless, LVLMs remain vulnerable to the issue of object hallucinations. Previous efforts to mitigate this issue focus on supervised fine-tuning (SFT) or incorporating external knowledge, both of which entail significant costs related to training and the acquisition of external data. To address these challenges, we propose a novel model-agnostic approach termed Internal Fact-based Contrastive Decoding (IFCD), designed to mitigate and suppress hallucinations during the inference process of LVLMs by exploiting the LVLMs' own hallucinations. IFCD is grounded in experimental observations that alterations to the LVLMs' internal representations tend to amplify hallucinations caused by language bias. By contrasting disturbed distribution, IFCD calibrates the LVLMs' output and effectively removes the hallucinatory logits from the final predictions. Experimental results validate that IFCD significantly alleviates both object-level and attribute-level hallucinations while achieving an average 9% accuracy improvement on POPE and 8% accuracy improvement on MME object hallucinations subset compared with direct decoding, respectively. 

**Abstract (ZH)**: 大型视觉语言模型（LVLMs）结合了视觉和语言模态，展示了在各种多模态任务中的出色性能。然而，LVLMs仍然容易受到目标幻觉问题的影响。为了缓解这一问题，以前的努力主要集中在监督微调（SFT）或引入外部知识，这些方法都涉及大量的训练成本以及对外部数据的获取。为了解决这些挑战，我们提出了一种新的模型无关的方法，称为内部事实导向对比解码（IFCD），该方法通过利用LVLMs自身的幻觉，在推断过程中缓解和抑制幻觉。IFCD基于实验观察，即改变LVLMs的内部表示往往会放大由语言偏差引起的幻觉。通过对比扰动分布，IFCD校准LVLMs的输出，并有效去除最终预测中的幻觉概率。实验结果表明，与直接解码相比，IFCD不仅显著缓解了对象级和属性级幻觉，还在POPE上提高了平均9%的准确性，在MME对象幻觉子集上提高了8%的准确性。 

---
# Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation 

**Title (ZH)**: 缩小模态差距：基于多模态原型和图像偏差估计的少样本Out-of-Distribution检测 

**Authors**: Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki  

**Link**: [PDF](https://arxiv.org/pdf/2502.00662)  

**Abstract**: Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods. 

**Abstract (ZH)**: 现有的基于视觉-语言模型（VLM）的 outlier-of-distribution (OOD) 检测方法通常依赖于输入图像与内分布（ID）文本原型之间的相似性得分。然而，图像和文本之间的模态差距往往会导致较高的误报率，因为OOD样本可能与ID文本原型表现出高度相似性。为了缓解这种模态差距的影响，我们提出将ID图像原型与ID文本原型一起纳入考虑。我们提供了理论分析和实验证据，表明这种方法在无需额外训练的情况下可以提高基于VLM的OOD检测性能。为进一步缩小图像和文本之间的差距，我们引入了一个新颖的少样本调优框架SUPREME，包含带偏置提示生成（BPG）模块和图像-文本一致性（ITC）模块。BPG增强了图像-文本融合并改进了泛化能力，通过基于高斯估计的图像域偏置条件化ID文本原型；ITC通过最小化同一模态内的距离和不同模态之间的距离来减少模态差距。此外，受到我们的理论和实验证据的启发，我们引入了一个新颖的OOD分数$S_{\textit{GMP}}$，利用单模态和跨模态的相似性。最后，我们进行了广泛的实验，证明SUPREME在现有的基于VLM的OOD检测方法中具有持续的优越性能。 

---
# Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings 

**Title (ZH)**: 面向PET/CT的视觉语言建模在阳性发现的视觉对接中的应用 

**Authors**: Zachary Huemann, Samuel Church, Joshua D. Warner, Daniel Tran, Xin Tie, Alan B McMillan, Junjie Hu, Steve Y. Cho, Meghan Lubner, Tyler J. Bradshaw  

**Link**: [PDF](https://arxiv.org/pdf/2502.00528)  

**Abstract**: Vision-language models can connect the text description of an object to its specific location in an image through visual grounding. This has potential applications in enhanced radiology reporting. However, these models require large annotated image-text datasets, which are lacking for PET/CT. We developed an automated pipeline to generate weak labels linking PET/CT report descriptions to their image locations and used it to train a 3D vision-language visual grounding model. Our pipeline finds positive findings in PET/CT reports by identifying mentions of SUVmax and axial slice numbers. From 25,578 PET/CT exams, we extracted 11,356 sentence-label pairs. Using this data, we trained ConTEXTual Net 3D, which integrates text embeddings from a large language model with a 3D nnU-Net via token-level cross-attention. The model's performance was compared against LLMSeg, a 2.5D version of ConTEXTual Net, and two nuclear medicine physicians. The weak-labeling pipeline accurately identified lesion locations in 98% of cases (246/251), with 7.5% requiring boundary adjustments. ConTEXTual Net 3D achieved an F1 score of 0.80, outperforming LLMSeg (F1=0.22) and the 2.5D model (F1=0.53), though it underperformed both physicians (F1=0.94 and 0.91). The model achieved better performance on FDG (F1=0.78) and DCFPyL (F1=0.75) exams, while performance dropped on DOTATE (F1=0.58) and Fluciclovine (F1=0.66). The model performed consistently across lesion sizes but showed reduced accuracy on lesions with low uptake. Our novel weak labeling pipeline accurately produced an annotated dataset of PET/CT image-text pairs, facilitating the development of 3D visual grounding models. ConTEXTual Net 3D significantly outperformed other models but fell short of the performance of nuclear medicine physicians. Our study suggests that even larger datasets may be needed to close this performance gap. 

**Abstract (ZH)**: 视觉语言模型可以通过视觉定位连接文本描述的对象与图像中的具体位置。这一技术在增强放射学报告方面具有潜在应用价值。然而，这些模型需要大量标注的图像-文本数据集，而这些数据集在PET/CT领域是缺乏的。我们开发了一种自动管道来生成关联PET/CT报告描述与图像位置的弱标签，并使用此管道训练了一个3D视觉语言视觉定位模型。我们的管道通过识别SUVmax和轴向切片编号来找到PET/CT报告中的阳性发现。从25,578份PET/CT检查中，我们提取了11,356个句子-标签对。使用这些数据，我们训练了ConTEXTual Net 3D，它是将大型语言模型的文本嵌入与3D nnU-Net通过标记级交叉注意机制进行整合的模型。该模型的性能与LLMSeg（ConTEXTual Net的2.5D版本）、以及两位核医学医师进行了比较。弱标签化管道在98%（246/251）的病例中准确地确定了病灶位置，但7.5%的病例需要边界调整。ConTEXTual Net 3D获得了0.80的F1分数，优于LLMSeg（F1=0.22）和2.5D模型（F1=0.53），但不及两位医师的表现（F1=0.94和0.91）。该模型在FDG（F1=0.78）和DCFPyL（F1=0.75）检查中表现更好，但在DOTATE（F1=0.58）和Fluciclovine（F1=0.66）检查中的表现较低。该模型在病灶大小方面表现一致，但在低摄取率的病灶上准确性较低。我们的新颖弱标签化管道准确生成了PET/CT图像-文本对标注数据集，促进了3D视觉定位模型的发展。ConTEXTual Net 3D显著优于其他模型，但其性能仍低于核医学医师的表现。我们的研究表明，可能需要更大的数据集来缩小这种性能差距。 

---
