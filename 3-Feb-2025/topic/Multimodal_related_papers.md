# Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play 

**Title (ZH)**: 模仿游戏：基于多模态生成式思维链的角色扮演以消除对抗性幻觉 

**Authors**: Ching-Chun Chang, Fan-Yun Chen, Shih-Hong Gu, Kai Gao, Hanrui Wang, Isao Echizen  

**Link**: [PDF](https://arxiv.org/pdf/2501.19143)  

**Abstract**: As the cornerstone of artificial intelligence, machine perception confronts a fundamental threat posed by adversarial illusions. These adversarial attacks manifest in two primary forms: deductive illusion, where specific stimuli are crafted based on the victim model's general decision logic, and inductive illusion, where the victim model's general decision logic is shaped by specific stimuli. The former exploits the model's decision boundaries to create a stimulus that, when applied, interferes with its decision-making process. The latter reinforces a conditioned reflex in the model, embedding a backdoor during its learning phase that, when triggered by a stimulus, causes aberrant behaviours. The multifaceted nature of adversarial illusions calls for a unified defence framework, addressing vulnerabilities across various forms of attack. In this study, we propose a disillusion paradigm based on the concept of an imitation game. At the heart of the imitation game lies a multimodal generative agent, steered by chain-of-thought reasoning, which observes, internalises and reconstructs the semantic essence of a sample, liberated from the classic pursuit of reversing the sample to its original state. As a proof of concept, we conduct experimental simulations using a multimodal generative dialogue agent and evaluates the methodology under a variety of attack scenarios. 

**Abstract (ZH)**: 作为人工智能的基石，机器感知面临着由对抗幻觉带来的根本性威胁。这些对抗攻击主要表现为两种形式：演绎幻觉（deductive illusion），即根据受害者模型的一般决策逻辑精心设计特定刺激；归纳幻觉（inductive illusion），即受害者模型的一般决策逻辑被特定刺激所塑造。前者通过利用模型的决策边界来创造出一种刺激，在应用时会干扰其决策过程。后者则在模型的学习阶段强化了一种条件反射，并嵌入了一个后门，在特定刺激触发时会导致异常行为。对抗幻觉的多样性促使我们需要建立一个统一的防御框架，以应对各种攻击形式。在这项研究中，我们基于模仿游戏的概念提出了一个消幻觉（disillusion）范式。模仿游戏的核心是一个基于链式推理的多模态生成代理，该代理能够观察、内化并重建样本的语义本质，而不仅仅是试图将样本还原为其原始状态。为证明这一方法的有效性，我们使用了一个多模态生成对话代理进行了实验模拟，并在多种攻击场景下评估了该方法论。 

---
# Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023 

**Title (ZH)**: large multimodal模型在科学图表的Caption生成中是否有效？来自2023年SCICAP挑战赛的教训 

**Authors**: Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19353)  

**Abstract**: Since the SCICAP datasets launch in 2021, the research community has made significant progress in generating captions for scientific figures in scholarly articles. In 2023, the first SCICAP Challenge took place, inviting global teams to use an expanded SCICAP dataset to develop models for captioning diverse figure types across various academic fields. At the same time, text generation models advanced quickly, with many powerful pre-trained large multimodal models (LMMs) emerging that showed impressive capabilities in various vision-and-language tasks. This paper presents an overview of the first SCICAP Challenge and details the performance of various models on its data, capturing a snapshot of the fields state. We found that professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Following this key finding, we conducted detailed analyses to answer this question: Have advanced LMMs solved the task of generating captions for scientific figures? 

**Abstract (ZH)**: 自2021年SCICAP数据集推出以来，研究社区在为学术文章中的科学图表生成说明词句方面取得了显著进展。在2023年，首次SCICAP挑战赛成功举办，邀请全球团队使用扩展的SCICAP数据集开发能够生成多样化图表类型说明词句的模型，这些图表跨越了各种学术领域。与此同时，文本生成模型得到了迅速发展，出现了许多强大的预训练多模态大模型(LMMs)，这些模型展示了在各种视觉和语言任务中的出色能力。本文概述了首次SCICAP挑战赛的内容，并详细报告了各种模型在其数据集上的表现，捕捉了当时领域的状况。研究发现，专业编辑普遍偏好由GPT-4V生成的图表说明词句，甚至比所有其他模型和作者原创撰写的说明词句更受欢迎。在这一关键发现的基础上，我们进行了详细分析，以回答这个问题：先进的LMMs是否已解决了为科学图表生成说明词句的任务？ 

---
# Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence 

**Title (ZH)**: 使用生成式人工智能实现乳腺癌多模态虚拟活检的增强智能 

**Authors**: Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi  

**Link**: [PDF](https://arxiv.org/pdf/2501.19176)  

**Abstract**: Full-Field Digital Mammography (FFDM) is the primary imaging modality for routine breast cancer screening; however, its effectiveness is limited in patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced Spectral Mammography (CESM), a second-level imaging technique, offers enhanced accuracy in tumor detection. Nonetheless, its application is restricted due to higher radiation exposure, the use of contrast agents, and limited accessibility. As a result, CESM is typically reserved for select cases, leaving many patients to rely solely on FFDM despite the superior diagnostic performance of CESM. While biopsy remains the gold standard for definitive diagnosis, it is an invasive procedure that can cause discomfort for patients. We introduce a multimodal, multi-view deep learning approach for virtual biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral oblique views to classify lesions as malignant or benign. To address the challenge of missing CESM data, we leverage generative artificial intelligence to impute CESM images from FFDM scans. Experimental results demonstrate that incorporating the CESM modality is crucial to enhance the performance of virtual biopsy. When real CESM data is missing, synthetic CESM images proved effective, outperforming the use of FFDM alone, particularly in multimodal configurations that combine FFDM and CESM modalities. The proposed approach has the potential to improve diagnostic workflows, providing clinicians with augmented intelligence tools to improve diagnostic accuracy and patient care. Additionally, as a contribution to the research community, we publicly release the dataset used in our experiments, facilitating further advancements in this field. 

**Abstract (ZH)**: 数字全视野乳腺X线摄影（FFDM）是常规乳腺癌筛查的主要成像技术，但在乳腺组织致密或伴有纤维囊性病变的患者中其效果受到限制。对比增强光谱乳腺X线摄影（CESM）是一种高级成像技术，能够提高肿瘤检测的准确性，但其应用受到较高的辐射暴露、对比剂使用和较低的可及性限制，在许多情况下只能用于特定病例，导致许多患者只能依赖FFDM成像，尽管CESM在诊断性能上优于FFDM。尽管活检是最终确诊的金标准，但它仍然是一个侵入性程序，可能会给患者带来不适。我们提出了一种多模态、多视角深度学习方法，用于虚拟活检，通过整合在头足位和腋中线倾斜位上的FFDM和CESM成像模态，将病变分类为恶性或良性。为了解决CESM数据缺失的挑战，我们利用生成人工智能从FFDM扫描中补全CESM图像。实验结果表明，在虚拟活检中集成CESM成像模态是提高诊断性能的关键。在缺少真实CESM数据的情况下，合成CESM图像表现出色，特别是在结合FFDM和CESM成像模态的多模态配置中优于单独使用FFDM。该方法有望改进诊断工作流程，为临床医生提供增强的人工智能工具，提高诊断准确性和患者护理水平。此外，作为对研究社区的贡献，我们将实验中使用的数据集公开展示，促进该领域的进一步发展。 

---
# Improving vision-language alignment with graph spiking hybrid Networks 

**Title (ZH)**: 使用图刺激混合网络提高视觉语言对齐 

**Authors**: Siyu Zhang, Heming Zheng, Yiming Wu, Yeming Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.19069)  

**Abstract**: To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks. 

**Abstract (ZH)**: 为了弥合视觉（Vision）与语言（Language）之间的语义差距（VL），需要开发一种良好的对齐策略，该策略包括处理语义多样性、视觉信息的抽象表示以及模型的泛化能力。最近的研究使用基于检测器的边界框或具有规则分割的补丁来表示视觉语义。尽管当前框架已经取得了一定的进展，但它们仍然不足以充分捕捉到不同物体之间细腻的上下文关系。本文提出了一种综合性的视觉语义表示模块，需要利用全景分割生成连贯的细粒度语义特征。此外，我们提出了一个新的图放电混合网络（Graph Spiking Hybrid Network，GSHN），该网络结合了放电神经网络（Spiking Neural Networks，SNNs）和图注意力网络（Graph Attention Networks，GATs）的优点来编码视觉语义信息。令人感兴趣的是，该模型不仅编码实例的离散和连续潜在变量，还能有效地捕捉局部和全局上下文特征，从而显著增强了语义表示的丰富性和多样性。利用SNNs固有的时空特性，我们采用了对比学习（Contrastive Learning，CL）来增强基于相似性的表示能力。该策略减轻了模型的计算负担，并通过构建正样本和负样本对来丰富有意义的视觉表示。我们设计了一种创新的预训练方法——放电文本学习（Spiked Text Learning，STL），该方法使用文本特征来提高离散语义的编码能力。实验结果表明，提出的方法在多种VL下游任务中表现出色。 

---
# UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent 

**Title (ZH)**: UP-VLA：统一的具身智能体理解与预测模型 

**Authors**: Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.18867)  

**Abstract**: Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. 

**Abstract (ZH)**: 近年来，视觉-语言-动作（VLA）模型的进步利用预训练的视觉-语言模型（VLMs）来提高泛化能力。VLMs通常在视觉-语言理解任务上进行预训练，提供丰富的语义知识和推理能力。然而，先前的研究表明，VLMs往往专注于高层次的语义内容，而忽略了低级特征，这限制了它们捕获详细的空间信息和理解物理动态的能力。这些方面对于实体控制任务至关重要，在现有的预训练范式中仍然没有得到充分探索。本文探讨了VLA的训练范式，并引入了**UP-VLA**模型，该模型结合了多模态**U**nderstanding和未来**P**rediction的目标，从而增强高层次语义理解和低层次空间理解。实验结果表明，UP-VLA在Calvin ABC-D基准测试上的表现比之前最先进的方法提高了33%。此外，UP-VLA在现实世界的操作任务中表现出更高的成功率，特别是在需要精确空间信息的任务中。 

---
# Every Image Listens, Every Image Dances: Music-Driven Image Animation 

**Title (ZH)**: 每幅图像都在倾听，每幅图像都在舞蹈：基于音乐的图像动画 

**Authors**: Zhikang Dong, Weituo Hao, Ju-Chiang Wang, Peng Zhang, Pawel Polak  

**Link**: [PDF](https://arxiv.org/pdf/2501.18801)  

**Abstract**: Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task. 

**Abstract (ZH)**: 图像动画已成为多模态研究中有前途的领域，重点关注从参考图像生成视频。尽管先前的工作主要集中在基于文本和音乐生成通用视频，但基于音乐的舞蹈视频生成仍处于探索阶段。本文介绍了一种名为 MuseDance 的创新端到端模型，该模型利用音乐和文本输入对参考图像进行动画处理。这种双重输入使 MuseDance 能够生成符合文本描述并能与音乐同步的角色运动的个性化视频。与现有方法不同，MuseDance 消除了对复杂运动引导输入（如姿态或深度序列）的需求，使得各种技能水平的用户能够灵活和创造性地生成视频。为了推动该领域的研究，我们提供了一个新的多模态数据集，包含 2904 个舞蹈视频及其相应的背景音乐和文本描述。我们的方法利用扩散型方法实现了稳健的一般化、精确的控制以及时序一致性，为音乐驱动的图像动画任务设定了新的基准。 

---
# Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation 

**Title (ZH)**: 将下面的论文内容或标题翻译成中文，并符合学术规范：

[Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation]

“集成LMM规划器和三维技能策略以实现通用化操作” 

**Authors**: Yuelei Li, Ge Yan, Annabella Macaluso, Mazeyu Ji, Xueyan Zou, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18733)  

**Abstract**: The recent advancements in visual reasoning capabilities of large multimodal models (LMMs) and the semantic enrichment of 3D feature fields have expanded the horizons of robotic capabilities. These developments hold significant potential for bridging the gap between high-level reasoning from LMMs and low-level control policies utilizing 3D feature fields. In this work, we introduce LMM-3DP, a framework that can integrate LMM planners and 3D skill Policies. Our approach consists of three key perspectives: high-level planning, low-level control, and effective integration. For high-level planning, LMM-3DP supports dynamic scene understanding for environment disturbances, a critic agent with self-feedback, history policy memorization, and reattempts after failures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature field for accurate manipulation. In aligning high-level and low-level control for robot actions, language embeddings representing the high-level policy are jointly attended with the 3D feature field in the 3D transformer for seamless integration. We extensively evaluate our approach across multiple skills and long-horizon tasks in a real-world kitchen environment. Our results show a significant 1.45x success rate increase in low-level control and an approximate 1.5x improvement in high-level planning accuracy compared to LLM-based baselines. Demo videos and an overview of LMM-3DP are available at this https URL. 

**Abstract (ZH)**: 近年来，大型多模态模型（LMMs）在视觉推理能力方面的进展以及3D特征域能量的语义丰富化，极大地扩展了机器人能力的边界。这些进展为将高级推理从LMMs与利用3D特征领域的低级控制策略之间的差距提供了巨大的潜力。在此基础上，我们提出了一种名为LMM-3DP的框架，该框架可以整合LMM规划器和3D技能策略。我们的方法包含三个关键视角：高级规划、低级控制以及有效的整合。在高级规划方面，LMM-3DP支持动态场景理解以应对环境干扰、带有自我反馈的批评代理、历史策略记忆，以及失败后的重试。在低级控制方面，LMM-3DP利用具有语义意识的3D特征领域进行精确操纵。为了实现高级控制与低级控制之间的对齐，我们的方法在3D变换中将表示高级策略的语言嵌入与3D特征领域共同关注，实现无缝整合。我们对LMM-3DP在多个技能和长期任务中的实际厨房环境中进行了广泛的评估。结果显示，LMM-3DP的低级控制成功率提高了1.45倍，高级规划的准确性提高了约1.5倍，相较于基于LLM的基准系统。有关演示视频和LMM-3DP的概述，请访问此链接：[请插入链接]。 

---
# High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA Fine-Tuning with Multimodal LLaMA 3.2 

**Title (ZH)**: 使用参数高效LoRA微调和多模态LLaMA 3.2进行高精度ECG图像解释 

**Authors**: Nandakishor M, Anjali M  

**Link**: [PDF](https://arxiv.org/pdf/2501.18670)  

**Abstract**: Electrocardiogram (ECG) interpretation is a cornerstone of cardiac diagnostics. This paper explores a practical approach to enhance ECG image interpretation using the multimodal LLaMA 3.2 model. We used a parameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA), specifically designed to boost the model's ability to understand ECG images and achieve better outcomes across a wide range of cardiac conditions. Our method is tailored for ECG analysis and leverages ECGInstruct, a large-scale instruction dataset with 1 Million samples. This dataset is a rich collection of synthesized ECG images, generated from raw ECG data from trusted open-source repositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes with expert-written questions and detailed answers, covering diverse ECG interpretation scenarios, including complex cardiac conditions like Myocardial Infarction and Conduction Disturbances. Our fine-tuning approach efficiently adapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank adaptation techniques, focusing on efficiency by updating only a small set of parameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This paper details the model setup, our efficient fine-tuning method, and implementation specifics. We provide a thorough evaluation through extensive experiments, demonstrating the effectiveness of our method across various ECG interpretation tasks. The results convincingly show that our parameter-efficient LoRA fine-tuning achieves excellent performance in ECG image interpretation, significantly outperforming baseline models and reaching accuracy comparable to or exceeding traditional CNN-based methods in identifying a wide range of cardiac abnormalities, including over 70 conditions from the PTB-XL dataset. 

**Abstract (ZH)**: 心电图（ECG）解读是心脏诊断的基础。本文探讨了一种实用的方法，利用多模态LLaMA 3.2模型来提升ECG图像的解读能力。我们采用了参数高效微调策略——低秩适应（LoRA），专门设计用于增强模型对ECG图像的理解能力，并在广泛的心脏病条件下取得更好的结果。该方法针对ECG分析进行了优化，并利用了一个名为ECGInstruct的大型指令数据集，包含100万样本。此数据集是由来自MIMIC-IV ECG和PTB-XL等可靠开源仓库的原始ECG数据生成的合成ECG图像的丰富集合。ECGInstruct中的每个ECG图像都附有专家编写的问题和详细的答案，涵盖了各种ECG解读场景，包括心肌梗死和传导障碍等复杂心脏状况。我们的微调方法通过结合低秩适应技术高效地适应了LLaMA 3.2模型（基于LLaMA 3构建），仅更新了一小部分参数，并且特别忽略了`lm_head`和`embed_tokens`层，从而提高了模型的效率。本文详细介绍了模型设置、我们高效微调方法以及实现的具体内容。我们通过广泛的实验进行了详细评估，证明了该方法在各种ECG解读任务中的有效性。结果表明，我们的参数高效LoRA微调方法在ECG图像解读中取得了出色的表现，显著优于基线模型，并达到了与传统基于CNN的方法相比或优于识别广泛心脏异常（包括PTB-XL数据集中超过70种条件）的准确性。 

---
# Membership Inference Attacks Against Vision-Language Models 

**Title (ZH)**: 针对视觉-语言模型的成员推理攻击 

**Authors**: Yuke Hu, Zheng Li, Zhihao Liu, Yang Zhang, Zhan Qin, Kui Ren, Chun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.18624)  

**Abstract**: Vision-Language Models (VLMs), built on pre-trained vision encoders and large language models (LLMs), have shown exceptional multi-modal understanding and dialog capabilities, positioning them as catalysts for the next technological revolution. However, while most VLM research focuses on enhancing multi-modal interaction, the risks of data misuse and leakage have been largely unexplored. This prompts the need for a comprehensive investigation of such risks in VLMs. In this paper, we conduct the first analysis of misuse and leakage detection in VLMs through the lens of membership inference attack (MIA). In specific, we focus on the instruction tuning data of VLMs, which is more likely to contain sensitive or unauthorized information. To address the limitation of existing MIA methods, we introduce a novel approach that infers membership based on a set of samples and their sensitivity to temperature, a unique parameter in VLMs. Based on this, we propose four membership inference methods, each tailored to different levels of background knowledge, ultimately arriving at the most challenging scenario. Our comprehensive evaluations show that these methods can accurately determine membership status, e.g., achieving an AUC greater than 0.8 targeting a small set consisting of only 5 samples on LLaVA. 

**Abstract (ZH)**: 基于预训练视觉编码器和大型语言模型（LLMs）构建的多模态视觉语言模型（VLMs）已经显示出卓越的多模态理解和对话能力，使其成为了下一次技术革命的催化剂。然而，尽管大多数VLM研究侧重于增强多模态交互，但数据滥用和泄露的风险却很少被探讨。这促使我们需要对VLM中的这些风险进行全面调查。在本文中，我们首次通过成员推理攻击（MIA）的角度分析了VLM中的滥用和泄露检测。具体而言，我们关注的是VLM的指令调优数据，这些数据更有可能包含敏感或未经授权的信息。为了解决现有MIA方法的局限性，我们提出了一个新型方法，通过一组样本及其对温度参数的敏感性来推断成员身份，温度是VLM中一个独有的参数。基于此，我们提出了四种成员推理方法，每种方法针对不同类型的知识背景进行设计，最终达到最具挑战性的场景。我们的全面评估表明，这些方法能够准确确定成员身份状态，例如，在LLaVA数据集上仅包含5个样本的小集合中，这些方法能够达到AUC大于0.8的效果。 

---
# Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation 

**Title (ZH)**: 将铁耙叫做玫瑰：通过否定来误导多模态大语言模型 

**Authors**: Bin Zhu, Hui yan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, Ee Peng Lim  

**Link**: [PDF](https://arxiv.org/pdf/2501.19017)  

**Abstract**: Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to conversational adversarial inputs, particularly negation arguments. This paper systematically evaluates state-of-the-art MLLMs across diverse benchmarks, revealing significant performance drops when negation arguments are introduced to initially correct responses. We show critical vulnerabilities in the reasoning and alignment mechanisms of these models. Proprietary models such as GPT-4o and Claude-3.5-Sonnet demonstrate better resilience compared to open-source counterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to maintain logical consistency under negation arguments during conversation. This paper aims to offer valuable insights for improving the robustness of MLLMs against adversarial inputs, contributing to the development of more reliable and trustworthy multimodal AI systems. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）在整合不同模态方面取得了显著进展，在复杂理解和生成任务中表现出色。尽管取得了成功，MLLMs 在应对对话式对抗输入时仍然脆弱，尤其是对于否定论证。本文系统地评估了当前最先进的 MLLMs 在多种基准测试中的表现，揭示了在引入否定论证后，这些模型初始正确响应性能显著下降的情况。研究显示这些模型在推理和对齐机制方面存在关键漏洞。如 GPT-4o 和 Claude-3.5-Sonnet 这样的专有模型相比开源模型 Qwen2-VL 和 LLaVA 在对抗输入方面展现出更好的鲁棒性。然而，所有评估的 MLLMs 在对话过程中在面对否定论证时均难以保持逻辑一致性。本文旨在提供改进 MLLMs 对抗输入鲁棒性的宝贵见解，为开发更可靠和可信赖的多模态 AI 系统做出贡献。 

---
# Divergent Emotional Patterns in Disinformation on Social Media? An Analysis of Tweets and TikToks about the DANA in Valencia 

**Title (ZH)**: 社交媒体上关于DANA在瓦伦西亚的虚假信息中的情感模式差异：推特和 TikTok 分析 

**Authors**: Iván Arcos, Paolo Rosso, Ramón Salaverría  

**Link**: [PDF](https://arxiv.org/pdf/2501.18640)  

**Abstract**: This study investigates the dissemination of disinformation on social media platforms during the DANA event (DANA is a Spanish acronym for Depresion Aislada en Niveles Altos, translating to high-altitude isolated depression) that resulted in extremely heavy rainfall and devastating floods in Valencia, Spain, on October 29, 2024. We created a novel dataset of 650 TikTok and X posts, which was manually annotated to differentiate between disinformation and trustworthy content. Additionally, a Few-Shot annotation approach with GPT-4o achieved substantial agreement (Cohen's kappa of 0.684) with manual labels. Emotion analysis revealed that disinformation on X is mainly associated with increased sadness and fear, while on TikTok, it correlates with higher levels of anger and disgust. Linguistic analysis using the LIWC dictionary showed that trustworthy content utilizes more articulate and factual language, whereas disinformation employs negations, perceptual words, and personal anecdotes to appear credible. Audio analysis of TikTok posts highlighted distinct patterns: trustworthy audios featured brighter tones and robotic or monotone narration, promoting clarity and credibility, while disinformation audios leveraged tonal variation, emotional depth, and manipulative musical elements to amplify engagement. In detection models, SVM+TF-IDF achieved the highest F1-Score, excelling with limited data. Incorporating audio features into roberta-large-bne improved both Accuracy and F1-Score, surpassing its text-only counterpart and SVM in Accuracy. GPT-4o Few-Shot also performed well, showcasing the potential of large language models for automated disinformation detection. These findings demonstrate the importance of leveraging both textual and audio features for improved disinformation detection on multimodal platforms like TikTok. 

**Abstract (ZH)**: 本研究探讨了西班牙瓦伦西亚地区（于2024年10月29日由DANA事件引起，DANA为西班牙语缩写，意为高海拔孤立性抑郁）导致极端暴雨和毁灭性洪水期间社交平台上的虚假信息传播情况。我们创建了一个包含650条TikTok和X社交媒体帖子的新数据集，并对其进行手动标注，以区分虚假信息和可信内容。此外，采用few-shot标注方法结合GPT-4o实现了与手动标签高度一致（Cohen's kappa值为0.684）。情感分析显示，X平台上的虚假信息主要与增加的悲伤和恐惧情绪相关，而TikTok平台上的虚假信息则与更高的愤怒和厌恶情绪相关。通过使用LIWC词典进行语言分析，可信内容使用了更加准确和客观的语言，而虚假信息则通过使用否定词、感知词和个人故事来显得可信。TikTok帖子的音频分析揭示了不同的模式：可信音频具有更明亮的音调和机械或单调的叙述，以促进清晰和可信度，而虚假信息音频则利用音调变化、情感深度以及操控性的音乐元素来增强观众的参与感。在检测模型中，SVM+TF-IDF获得了最高的F1分数，在有限的数据条件下表现优异。将音频特征纳入roberta-large-bne模型中提高了准确性和F1分数，超过了仅使用文本的版本和SVM。GPT-4o few-shot标注方法也表现出良好效果，展示了大规模语言模型在自动化检测虚假信息方面的潜力。这些发现证明了在多模态平台如TikTok上提高虚假信息检测效果的重要性，须同时利用文字和音频特征。 

---
# PixelWorld: Towards Perceiving Everything as Pixels 

**Title (ZH)**: PixelWorld: 将一切感知为像素的研究 

**Authors**: Zhiheng Lyu, Xueguang Ma, Wenhu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.19339)  

**Abstract**: Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance. 

**Abstract (ZH)**: 现有的基础模型通常将视觉输入视为像素，将文本输入视为标记，这与人类感知的方式形成了对比，后者在处理这两种模态时采取统一的方式。随着具身和主动AI的发展，输入主要来自相机像素，因此迫切需要一个统一的感知框架。在本文中，我们提议将所有模态（文本、表格、代码、图表、图像等）统一处理为像素输入，即“将一切视为像素”（PEAP）。我们引入了PixelWorld，这是一个新的评估套件，将所有提到的模态统一到像素空间，以评估现有模型的性能。我们的研究结果表明：（1）PEAP在多模态数据集中的性能优于基于标记输入的基线模型，这得益于统一输入有助于更好地消歧；（2）当处理基于像素的输入时，所有模型在推理和编码能力上都出现了显著下降，强调了增强基础模型的感知能力的需求；（3）在PEAP下，较大的模型在非推理任务上的表现仍然很强，而较小的模型如Phi-3.5-V则遭受了显著的性能下降；（4）PEAP的注意力模式与文本标记输入高度一致；（5）通过利用空间稀疏性，可以显著加速PEAP。我们得出结论，现有的前沿模型在像素感知方面具有竞争力，但仍有改进空间。我们的代码和数据集将在发表后公开。 

---
