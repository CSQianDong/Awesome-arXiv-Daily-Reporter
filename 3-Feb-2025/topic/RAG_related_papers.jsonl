{'arxiv_id': 'arXiv:2501.18636', 'title': 'SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model', 'authors': 'Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, Jiawei Yang', 'link': 'https://arxiv.org/abs/2501.18636', 'abstract': 'The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: this https URL.', 'abstract_zh': '检索-增强生成（RAG）的索引-检索-生成范式在通过将外部知识整合到大型语言模型（LLMs）中解决知识密集型任务方面取得了巨大成功。然而，外部未经验证知识的引入增加了LLMs的脆弱性，因为攻击者可以通过操纵知识来执行攻击任务。本文旨在介绍一个名为SafeRAG的基准测试，用于评估RAG的安全性。首先，我们将攻击任务分为银噪声、跨上下文冲突、软广告和白道服务拒绝（DoS）。接着，我们主要通过手动方式为每个任务构建了一个RAG安全性评估数据集（即SafeRAG数据集）。然后，我们利用SafeRAG数据集模拟RAG可能遇到的各种攻击场景。对14个代表性RAG组件进行的实验表明，RAG对所有攻击任务都具有明显的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器、过滤器或先进的LLMs，导致RAG服务质量下降。相关代码可在此处获取：this https URL。', 'title_zh': 'SafeRAG：检索增强生成中大型语言模型安全性基准测试'}
