{'arxiv_id': 'arXiv:2501.19399', 'title': 'Scalable-Softmax Is Superior for Attention', 'authors': 'Ken M. Nakanishi', 'link': 'https://arxiv.org/abs/2501.19399', 'abstract': "The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining.", 'abstract_zh': 'Softmax函数输出向量的最大元素随着输入向量尺寸的增加趋向于零。基于Transformer的语言模型依赖Softmax来计算注意力分数，导致随着上下文尺寸的增长，注意力分布变得扁平。这降低了模型有效优先处理关键信息的能力，并可能限制其长度泛化能力。为了解决这一问题，我们提出了一种可扩展Softmax（SSMax），它可以在输入向量尺寸变化的情景中替代Softmax。SSMax可以无缝集成到现有的基于Transformer的架构中。在语言建模实验中，使用SSMax的模型不仅在预训练过程中更快地降低了损失，而且在长上下文和关键信息检索方面显著提高了性能。此外，对注意力分数的分析表明，SSMax使模型能够在长上下文中专注于关键信息。虽然从预训练一开始就使用SSMax的模型在长度泛化方面表现更好，但已经开始预训练的模型也可以通过在注意力层中用SSMax替换Softmax来获得一些这种能力，无论是在预训练过程中还是之后。', 'title_zh': '可扩展的Softmax在注意机制中更优越'}
{'arxiv_id': 'arXiv:2501.19393', 'title': 's1: Simple test-time scaling', 'authors': 'Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto', 'link': 'https://arxiv.org/abs/2501.19393', 'abstract': 'Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\'s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\'s thinking process or lengthening it by appending "Wait" multiple times to the model\'s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL.', 'abstract_zh': '测试时缩放是一种有前景的新语言建模方法，它通过增加测试时的计算资源来提高性能。最近，OpenAI的o1模型展示了这一能力，但并未公开分享其方法论，导致了众多的复制研究。我们寻求一种简单的方法来实现测试时缩放和强大的推理性能。首先，我们精心筛选了一个包含1,000个问题及其推理过程的小型数据集s1K，依赖于我们通过消除测试验证的三个标准：难度、多样性和质量。其次，我们开发了一种预算强制方法，以控制测试时的计算资源，该方法通过强制终止模型的思考过程或在模型试图结束时多次附加“等待”（Wait）来延长生成过程。这可以使模型重新检查其答案，通常会纠正错误的推理步骤。在对Qwen2.5-32B-Instruct语言模型进行监督微调后，我们将预算强制策略应用到s1模型上，我们的模型s1在竞赛数学问题上相较于o1-preview提高了27%（MATH和AIME24）。进一步通过应用预算强制策略扩展s1的成长潜力，使其在AIME24评分上从50%提高到57%。我们的模型、数据和代码在以下地址开源：https://this.is.source.url。', 'title_zh': 's1: 简单的测试时缩放方法'}
{'arxiv_id': 'arXiv:2501.19378', 'title': 'TableMaster: A Recipe to Advance Table Understanding with Language Models', 'authors': 'Lang Cao', 'link': 'https://arxiv.org/abs/2501.19378', 'abstract': 'Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.', 'abstract_zh': '表格是表示结构化关系数据的基本格式。尽管当前的语言模型（LMs）在许多文本任务中表现出色，但它们在理解表格方面仍然面临着挑战，这主要是由于表格数据的复杂特性，如其结构化性质。在本文中，我们旨在通过增强语言模型来改善表格理解。我们识别出四个关键挑战：1）目标数据定位困难，2）表格语义不足，3）文本推理中的数值不准确性，4）符号推理中的语义灵活性不足。为了解决这些问题，我们提出了TableMaster，这是一种集成了多种解决方案的食谱和综合框架，以克服这些障碍。TableMaster 首先抽取相关表格内容，并以丰富的语义上下文形式进行表述。此外，我们引入了适应性推理，这是一种灵活的方法，能够根据查询动态调整文本和符号推理之间的平衡，使推理过程适应每个查询。广泛的分析和实验验证了我们的发现和TableMaster的有效性。在WikiTQ数据集中，使用GPT-4o-mini时，TableMaster 的准确率为78.13%，超过了现有基准。', 'title_zh': 'TableMaster：一种利用语言模型推动表格理解的方法'}
{'arxiv_id': 'arXiv:2501.19353', 'title': 'Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023', 'authors': 'Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang', 'link': 'https://arxiv.org/abs/2501.19353', 'abstract': 'Since the SCICAP datasets launch in 2021, the research community has made significant progress in generating captions for scientific figures in scholarly articles. In 2023, the first SCICAP Challenge took place, inviting global teams to use an expanded SCICAP dataset to develop models for captioning diverse figure types across various academic fields. At the same time, text generation models advanced quickly, with many powerful pre-trained large multimodal models (LMMs) emerging that showed impressive capabilities in various vision-and-language tasks. This paper presents an overview of the first SCICAP Challenge and details the performance of various models on its data, capturing a snapshot of the fields state. We found that professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Following this key finding, we conducted detailed analyses to answer this question: Have advanced LMMs solved the task of generating captions for scientific figures?', 'abstract_zh': '自2021年SCICAP数据集推出以来，研究社区在为学术文章中的科学图表生成说明词句方面取得了显著进展。在2023年，首次SCICAP挑战赛成功举办，邀请全球团队使用扩展的SCICAP数据集开发能够生成多样化图表类型说明词句的模型，这些图表跨越了各种学术领域。与此同时，文本生成模型得到了迅速发展，出现了许多强大的预训练多模态大模型(LMMs)，这些模型展示了在各种视觉和语言任务中的出色能力。本文概述了首次SCICAP挑战赛的内容，并详细报告了各种模型在其数据集上的表现，捕捉了当时领域的状况。研究发现，专业编辑普遍偏好由GPT-4V生成的图表说明词句，甚至比所有其他模型和作者原创撰写的说明词句更受欢迎。在这一关键发现的基础上，我们进行了详细分析，以回答这个问题：先进的LMMs是否已解决了为科学图表生成说明词句的任务？', 'title_zh': 'large multimodal模型在科学图表的Caption生成中是否有效？来自2023年SCICAP挑战赛的教训'}
{'arxiv_id': 'arXiv:2501.19337', 'title': 'Homogeneity Bias as Differential Sampling Uncertainty in Language Models', 'authors': 'Messi H.J. Lee, Soyeon Jeon', 'link': 'https://arxiv.org/abs/2501.19337', 'abstract': 'Prior research show that Large Language Models (LLMs) and Vision-Language Models (VLMs) represent marginalized groups more homogeneously than dominant groups. However, the mechanisms underlying this homogeneity bias remain relatively unexplored. We propose that this bias emerges from systematic differences in the probability distributions from which tokens are sampled at inference-time. Analyzing three measures of uncertainty in token sampling distributions-entropy, perplexity, and probability of differentiation-we find that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled more deterministically when generating texts about marginalized groups (i.e., Black Americans and women) compared to their dominant group counterparts (i.e., White Americans and men). While these findings may help explain homogeneity bias in certain models, the patterns did not replicate across all VLMs tested, suggesting multiple mechanisms may contribute to homogeneity bias in AI.', 'abstract_zh': '先前的研究表明，大型语言模型（LLMs）和视觉-语言模型（VLMs）在表示边缘化群体时比主流群体更加 homogeneous。不过，这种 homogeneity 偏差的具体机制尚少有探索。我们提出，这种偏差可能是由于推理时 token 取样概率分布中的系统性差异所导致。通过分析 token 取样概率分布中的三个不确定性指标——熵、困惑度以及差异化概率，我们发现，在某些模型中（例如 GPT-4 Turbo 和 Llama-3.2），当生成关于边缘化群体（例如黑人美国人和女性）的文本时，token 的取样比较确定性；而在生成主流群体（例如白人美国人和男性）的文本时，token 的取样则更为随机化。虽然这些发现可能有助于解释某些模型中的 homogeneity 偏差，但这些模式并未在所有测试的 VLMs 中复制，这表明 homogeneity 偏差在 AI 中可能有多重机制。', 'title_zh': '语言模型中的同质性偏误作为差异采样不确定性'}
{'arxiv_id': 'arXiv:2501.19324', 'title': 'Reward-Guided Speculative Decoding for Efficient LLM Reasoning', 'authors': 'Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong', 'link': 'https://arxiv.org/abs/2501.19324', 'abstract': 'We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios.', 'abstract_zh': '我们提出了奖励引导的推测解码 (RSD) 框架，这是一种旨在提高大语言模型 (LLMs) 推断效率的新颖方法。RSD 通过结合一个轻量级的草稿模型和一个更强大的目标模型，并引入可控偏见以优先考虑高奖励输出，从而协同工作，与现有的推测解码方法不同，后者强制实现严格的无偏性。RSD 利用一个过程奖励模型来评估中间解码步骤，并动态决定是否调用目标模型，从而优化计算成本与输出质量之间的权衡。我们从理论上证明，基于阈值的混合策略在资源利用和性能之间实现了最优平衡。在包括奥林匹克级别任务在内的复杂推理基准测试中的广泛评估显示，与仅使用目标模型的解码相比，RSD 能够实现显著的效率改进（最多可减少 4.4 倍的 FLOPs），同时与并行解码方法相比，在平均准确度上也有显著的提升（最多可提高 3.5%）。这些结果突显了 RSD 在资源密集型场景中部署 LLM 的稳健性和成本效益。', 'title_zh': '基于奖励引导的推测性解码以实现高效的大型语言模型推理'}
{'arxiv_id': 'arXiv:2501.19317', 'title': 'LLM-based Affective Text Generation Quality Based on Different Quantization Values', 'authors': 'Yarik Menchaca Resendiz, Roman Klinger', 'link': 'https://arxiv.org/abs/2501.19317', 'abstract': 'Large language models exhibit a remarkable capacity in language generation and comprehension. These advances enable AI systems to produce more human-like and emotionally engaging text. However, these models rely on a large number of parameters, requiring significant computational resources for training and inference. In some scenarios, accessing these resources can be challenging (e.g., budget or hardware limitations). Techniques like reducing precision bits can make models more memory-efficient, reducing the computational resources needed, at the cost of reduced accuracy. This paper addresses the trade-off between different quantization values, GPU RAM utilization, and text quality in affective text generation (e.g., "I really enjoy running in the snow-covered forest"). To evaluate, we use an emotion classifier and ten seed prompts to generate affective text. We test three setups of precision bits (8, 16, and 32) across five open-weight language models from two different families. Our findings demonstrate that bit reductions lead to memory savings, achieving a reduction of 76%. However, this optimization comes with a trade-off, leading to a decrease of up to 10 pp in F1 score for larger models and an increase of 10 pp for smaller models, along with roughly double the inference time. In terms of text quality, larger models at lower quantization levels generally outperform smaller, higher-precision models -- while requiring similar memory.', 'abstract_zh': '大型语言模型在语言生成和理解方面表现出非凡的能力。这些进步使得AI系统能够生成更加接近人类和情感化的文本。然而，这些模型依赖大量的参数，需要大量的计算资源进行训练和推断。在某些场景中，获取这些资源可能会有挑战（例如预算或硬件限制）。通过减少精度位数等技术可以提高模型的内存效率，减少所需的计算资源，但会牺牲一定的准确性。本文探讨了不同量化值、GPU RAM利用率与情感文本生成质量之间的权衡问题（例如，“我真的很喜欢在覆雪的森林里跑步”）。为评估这一点，我们使用情感分类器和十个种子提示词生成情感化的文本。我们测试了两种不同家族的五种开源权重语言模型在三种精度位数设置（8位、16位和32位）下的情况。我们的研究结果表明，减少精度位数可以节省内存，实现76%的内存缩减。然而，这种优化伴随着准确性下降的代价：对于大型模型，准确率下降最多可达10个百分点；对于小型模型则提高10个百分点，同时推断时间大约增加一倍。在文本质量方面，较低量化级别的大型模型通常优于较高精度的小型模型，而占用的内存却相似。', 'title_zh': '基于不同量化值的LLM情感文本生成质量研究'}
{'arxiv_id': 'arXiv:2501.19316', 'title': 'Reverse Probing: Evaluating Knowledge Transfer via Finetuned Task Embeddings for Coreference Resolution', 'authors': 'Tatiana Anikina, Arne Binder, David Harbecke, Stalin Varanasi, Leonhard Hennig, Simon Ostermann, Sebastian Möller, Josef van Genabith', 'link': 'https://arxiv.org/abs/2501.19316', 'abstract': 'In this work, we reimagine classical probing to evaluate knowledge transfer from simple source to more complex target tasks. Instead of probing frozen representations from a complex source task on diverse simple target probing tasks (as usually done in probing), we explore the effectiveness of embeddings from multiple simple source tasks on a single target task. We select coreference resolution, a linguistically complex problem requiring contextual understanding, as focus target task, and test the usefulness of embeddings from comparably simpler tasks tasks such as paraphrase detection, named entity recognition, and relation extraction. Through systematic experiments, we evaluate the impact of individual and combined task embeddings.\nOur findings reveal that task embeddings vary significantly in utility for coreference resolution, with semantic similarity tasks (e.g., paraphrase detection) proving most beneficial. Additionally, representations from intermediate layers of fine-tuned models often outperform those from final layers. Combining embeddings from multiple tasks consistently improves performance, with attention-based aggregation yielding substantial gains. These insights shed light on relationships between task-specific representations and their adaptability to complex downstream tasks, encouraging further exploration of embedding-level task transfer.', 'abstract_zh': '在本研究中，我们重新构想了经典的探针方法，以评估简单源任务向更复杂目标任务的知识迁移。与传统探针方法通常在不同的简单目标探针任务中使用复杂源任务冻结表示的做法不同，我们探索了来自多个简单源任务的嵌入在单一目标任务上的有效性。我们将核心ference解析（涉及复杂的语言问题，需要上下文理解）作为目标任务，测试了来自相对简单任务（如语义相似性识别、命名实体识别和关系抽取）的嵌入的有效性。通过系统性实验，我们评估了单一任务嵌入和多任务嵌入对性能的影响。\n\n我们的发现表明，不同任务的嵌入对核心ference解析任务的有用性存在显著差异，其中语义相似性任务（如语义相似性识别）最具有益处。此外，微调模型中间层的表示通常优于最终层的表示。多个任务嵌入的组合始终能够提升性能，基于注意的聚合方法能够带来显著的提升。这些见解揭示了任务特定表示与其对复杂下游任务的适应性之间的关系，并鼓励进一步探索嵌入级任务转移。', 'title_zh': '反向探针：通过细调任务嵌入评估知识迁移效果在共指消解中的应用'}
{'arxiv_id': 'arXiv:2501.19314', 'title': 'An Efficient Approach for Machine Translation on Low-resource Languages: A Case Study in Vietnamese-Chinese', 'authors': 'Tran Ngoc Son, Nguyen Anh Tu, Nguyen Minh Tri', 'link': 'https://arxiv.org/abs/2501.19314', 'abstract': 'Despite the rise of recent neural networks in machine translation, those networks do not work well if the training data is insufficient. In this paper, we proposed an approach for machine translation in low-resource languages such as Vietnamese-Chinese. Our proposed method leveraged the power of the multilingual pre-trained language model (mBART) and both Vietnamese and Chinese monolingual corpus. Firstly, we built an early bird machine translation model using the bilingual training dataset. Secondly, we used TF-IDF technique to select sentences from the monolingual corpus which are the most related to domains of the parallel dataset. Finally, the first model was used to synthesize the augmented training data from the selected monolingual corpus for the translation model. Our proposed scheme showed that it outperformed 8% compared to the transformer model. The augmented dataset also pushed the model performance.', 'abstract_zh': '尽管近年来神经网络在机器翻译方面取得了显著进展，但如果没有足够的训练数据，这些网络的表现往往不尽如人意。在本文中，我们提出了一种针对低资源语言（如越南语-中文）机器翻译的方法。我们提出的方法结合了多语言预训练语言模型（mBART）以及越南语和中文的单语语料。具体步骤如下：首先，我们使用双语训练数据集构建了一个早期鸟机器翻译模型。其次，我们利用TF-IDF技术从单语语料中选择与平行数据集领域最相关的句子。最后，第一个模型被用于从选定的单语语料中生成增强的训练数据，以供翻译模型使用。我们提出的方案与Transformer模型相比，性能提高了8%。此外，增强的数据集也显著提升了模型性能。', 'title_zh': '低资源语言机器翻译的一种高效方法：以越南语-汉语为例的研究'}
{'arxiv_id': 'arXiv:2501.19301', 'title': 'Beyond checkmate: exploring the creative chokepoints in AI text', 'authors': 'Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee', 'link': 'https://arxiv.org/abs/2501.19301', 'abstract': 'Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities. This rapid advancement has spurred research into various aspects of LLMs, their text generation & reasoning capability, and potential misuse, fueling the necessity for robust detection methods. While numerous prior research has focused on detecting LLM-generated text (AI text) and thus checkmating them, our study investigates a relatively unexplored territory: portraying the nuanced distinctions between human and AI texts across text segments. Whether LLMs struggle with or excel at incorporating linguistic ingenuity across different text segments carries substantial implications for determining their potential as effective creative assistants to humans. Through an analogy with the structure of chess games-comprising opening, middle, and end games-we analyze text segments (introduction, body, and conclusion) to determine where the most significant distinctions between human and AI texts exist. While AI texts can approximate the body segment better due to its increased length, a closer examination reveals a pronounced disparity, highlighting the importance of this segment in AI text detection. Additionally, human texts exhibit higher cross-segment differences compared to AI texts. Overall, our research can shed light on the intricacies of human-AI text distinctions, offering novel insights for text detection and understanding.', 'abstract_zh': '大型语言模型（LLMs）已经革新了自然语言处理（NLP）和人工智能（AI），解锁了前所未有的能力。这种快速的发展引发了对LLMs及其文本生成与推理能力、潜在滥用的研究，进而增强了开发稳健检测方法的必要性。尽管众多先前的研究集中在检测LLM生成的文本（AI文本）并对此加以制约，我们的研究则探索了一个相对未被充分研究的领域：在文本段落中描绘人类文本与AI文本之间的细微差异。LLMs在不同文本段落中对语言创新的处理能力差异对于判断它们作为有效创意助手的潜力具有重要意义。通过将文本段落与象棋比赛的结构（开局、中局和残局）类比，我们分析了不同文本段落（引言、正文、结论）以确定人类文本与AI文本之间的显著差异所在。尽管由于篇幅较长，AI文本可以更好地模仿正文部分，但仔细观察揭示了其中显著的差异，强调了区分AI文本这一部分的重要性。此外，人类文本在不同段落之间的差异大于AI文本。总体而言，我们的研究可以揭示人类与AI文本差异的复杂性，为文本检测和理解提供新的见解。', 'title_zh': '超越将棋之胜：探索AI文本生成中的创造性瓶颈'}
{'arxiv_id': 'arXiv:2501.19278', 'title': 'Pheromone-based Learning of Optimal Reasoning Paths', 'authors': 'Anirudh Chari, Aditya Tiwari, Richard Lian, Suraj Reddy, Brian Zhou', 'link': 'https://arxiv.org/abs/2501.19278', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities through chain-of-thought prompting, yet discovering effective reasoning methods for complex problems remains challenging due to the vast space of possible intermediate steps. We introduce Ant Colony Optimization-guided Tree of Thought (ACO-ToT), a novel algorithm that combines ACO with LLMs to discover optimal reasoning paths for complex problems efficiently. Drawing inspiration from Hebbian learning in neurological systems, our method employs a collection of distinctly fine-tuned LLM "ants" to traverse and lay pheromone trails through a centralized tree of thought, with each ant\'s movement governed by a weighted combination of existing pheromone trails and its own specialized expertise. The algorithm evaluates complete reasoning paths using a mixture-of-experts-based scoring function, with pheromones reinforcing productive reasoning paths across iterations. Experiments on three challenging reasoning tasks (GSM8K, ARC-Challenge, and MATH) demonstrate that ACO-ToT performs significantly better than existing chain-of-thought optimization approaches, suggesting that incorporating biologically inspired collective search mechanisms into LLM inference can substantially enhance reasoning capabilities.', 'abstract_zh': '大规模语言模型（LLMs）通过链式思考提示展示了非凡的推理能力，但发现复杂问题的有效推理方法仍然是挑战性的，因为可能存在大量的中间步骤。我们引入了一种名为蚁群优化引导的思想树（ACO-ToT）的新算法，该算法结合了蚁群优化（ACO）和大规模语言模型（LLMs），以高效地发现复杂问题的最佳推理路径。受到神经学系统中的Hebbian学习启发，我们的方法使用一组不同微调的大规模语言模型“蚂蚁”在中央思想树中遍历并铺设信息素路径，每只蚂蚁的移动由现有信息素路径和其自身专业知识的加权组合来决定。算法使用基于专家群体的评分函数评估完整的推理路径，并通过多次迭代加强有效的推理路径。在三个具有挑战性的推理任务（GSM8K、ARC-Challenge和MATH）上的实验表明，ACO-ToT在链式思考优化方法方面表现出显著优越性，这表明将生物启发的集体搜索机制纳入LLM推理中可以显著增强推理能力。', 'title_zh': '基于pheromone的学习最优推理路径'}
{'arxiv_id': 'arXiv:2501.19258', 'title': 'VisualSpeech: Enhance Prosody with Visual Context in TTS', 'authors': 'Shumin Que, Anton Ragni', 'link': 'https://arxiv.org/abs/2501.19258', 'abstract': 'Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody from a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as visual features, remains underutilized. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates both visual and textual information for improved prosody generation. Empirical results demonstrate that visual features provide valuable prosodic cues beyond the textual input, significantly enhancing the naturalness and accuracy of the synthesized speech. Audio samples are available at this https URL.', 'abstract_zh': '文本到语音（TTS）合成面临着从单一文本输入生成具有不同语调的多种语音输出的固有挑战。尽管以往的研究通过同时从文本和语音中预测语调信息来解决这一问题，但视觉特征等额外上下文信息仍然未得到充分利用。本文探讨了整合视觉上下文以增强语调预测的潜力。我们提出了一种新颖的模型——VisualSpeech，该模型结合了视觉和文本信息，以提高语调生成的效果。实证结果表明，视觉特征提供了超出文本输入的有价值的语调线索，显著增强了合成语音的自然度和准确性。音频样本可在以下链接获取：this https URL。', 'title_zh': '视觉语音：通过视觉语境增强语音合成中的韵律'}
{'arxiv_id': 'arXiv:2501.19202', 'title': 'Improving the Robustness of Representation Misdirection for Large Language Model Unlearning', 'authors': 'Dang Huu-Tien, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue', 'link': 'https://arxiv.org/abs/2501.19202', 'abstract': "Representation Misdirection (RM) and variants are established large language model (LLM) unlearning methods with state-of-the-art performance. In this paper, we show that RM methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in RM models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation -- a model and method agnostic approach with theoretical guarantees for improving the robustness of RM methods. Extensive experiments demonstrate that RNA significantly improves the robustness of RM models while enhancing the unlearning performances.", 'abstract_zh': 'Representation Misdirection (RM) 和其变体是已知的大规模语言模型（LLM）遗忘方法，具有最先进的性能。本文表明，RM 方法本质上会降低模型的稳健性，即使在保留查询中仅包含一个非对抗性遗忘标记，这些模型也会表现出异常行为。为了理解其根本原因，我们将遗忘过程重新框架为后门攻击和防御：遗忘标记作为后门触发器，在保留查询中激活时，会引发 RM 模型行为的中断，类似于成功的后门攻击。为缓解这一漏洞，我们提出了随机噪声增强——一种模型和方法无关的方法，并具有理论上保证的提高 RM 方法稳健性的能力。大量实验表明，随机噪声增强在提高 RM 模型的稳健性的同时，也提升了遗忘性能。', 'title_zh': '提高大型语言模型脱训中表示误导的稳健性'}
{'arxiv_id': 'arXiv:2501.19201', 'title': 'Efficient Reasoning with Hidden Thinking', 'authors': 'Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu', 'link': 'https://arxiv.org/abs/2501.19201', 'abstract': 'Chain-of-Thought (CoT) reasoning has become a powerful framework for improving complex problem-solving capabilities in Multimodal Large Language Models (MLLMs). However, the verbose nature of textual reasoning introduces significant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as hidden llama), an efficient reasoning framework that leverages reasoning CoTs at hidden latent space. We design the Heima Encoder to condense each intermediate CoT into a compact, higher-level hidden representation using a single thinking token, effectively minimizing verbosity and reducing the overall number of tokens required during the reasoning process. Meanwhile, we design corresponding Heima Decoder with traditional Large Language Models (LLMs) to adaptively interpret the hidden representations into variable-length textual sequence, reconstructing reasoning processes that closely resemble the original CoTs. Experimental results across diverse reasoning MLLM benchmarks demonstrate that Heima model achieves higher generation efficiency while maintaining or even better zero-shot task accuracy. Moreover, the effective reconstruction of multimodal reasoning processes with Heima Decoder validates both the robustness and interpretability of our approach.', 'abstract_zh': 'Chain-of-Thought (CoT)推理已成为增强多模态大型语言模型（MLLMs）复杂问题解决能力的一种强大框架。然而，文本推理的冗长性引入了显著的效率问题。在本文中，我们提出了**Heima**（隐灵马）这一高效的推理框架，该框架利用在隐潜空间中进行推理CoT。我们设计了Heima编码器，通过一个思考标记将每个中间的CoT紧凑地表示为更高层次的隐藏表示，有效减少了冗余性，减少了推理过程中所需的标记数量。同时，我们设计了相应的Heima解码器与传统的大型语言模型（LLMs）相结合，以自适应方式将隐藏表示解释为可变长度的文本序列，重构出接近原始CoT的推理过程。在多样性的推理MLLM基准测试中的实验结果表明，Heima模型在保持或甚至在零样本任务准确性方面实现了更高的生成效率。此外，Heima解码器对多模态推理过程的有效重构验证了我们方法的鲁棒性和可解释性。', 'title_zh': '高效的隐性思维推理'}
{'arxiv_id': 'arXiv:2501.19134', 'title': 'Mixed Feelings: Cross-Domain Sentiment Classification of Patient Feedback', 'authors': 'Egil Rønningstad, Lilja Charlotte Storset, Petter Mæhlum, Lilja Øvrelid, Erik Velldal', 'link': 'https://arxiv.org/abs/2501.19134', 'abstract': 'Sentiment analysis of patient feedback from the public health domain can aid decision makers in evaluating the provided services. The current paper focuses on free-text comments in patient surveys about general practitioners and psychiatric healthcare, annotated with four sentence-level polarity classes -- positive, negative, mixed and neutral -- while also attempting to alleviate data scarcity by leveraging general-domain sources in the form of reviews. For several different architectures, we compare in-domain and out-of-domain effects, as well as the effects of training joint multi-domain models.', 'abstract_zh': '公共健康领域患者的反馈情感分析可以帮助决策者评估所提供的服务。本文集中在关于全科医生和精神卫生保健患者的自由文本调查反馈上，这些反馈被标注为四个句子级极性类别——正面、负面、混合和中性——同时尝试通过利用一般领域的来源（如评论）来缓解数据稀缺问题。我们比较了几种不同架构的领域内和领域外效果，以及训练联合多领域模型的效果。', 'title_zh': '《Mixed Feelings：跨领域患者反馈情感分类》'}
{'arxiv_id': 'arXiv:2501.19093', 'title': 'Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations', 'authors': 'Peichao Lai, Jiaxin Gan, Feiyang Ye, Yilei Wang, Bin Cui', 'link': 'https://arxiv.org/abs/2501.19093', 'abstract': "Sequence labeling remains a significant challenge in low-resource, domain-specific scenarios, particularly for character-dense languages like Chinese. Existing methods primarily focus on enhancing model comprehension and improving data diversity to boost performance. However, these approaches still struggle with inadequate model applicability and semantic distribution biases in domain-specific contexts. To overcome these limitations, we propose a novel framework that combines an LLM-based knowledge enhancement workflow with a span-based Knowledge Fusion for Rich and Efficient Extraction (KnowFREE) model. Our workflow employs explanation prompts to generate precise contextual interpretations of target entities, effectively mitigating semantic biases and enriching the model's contextual understanding. The KnowFREE model further integrates extension label features, enabling efficient nested entity extraction without relying on external knowledge during inference. Experiments on multiple Chinese domain-specific sequence labeling datasets demonstrate that our approach achieves state-of-the-art performance, effectively addressing the challenges posed by low-resource settings.", 'abstract_zh': '在低资源、领域特定场景中，序列标注仍然是一个重要挑战，尤其是在中文等字符密集语言中。现有方法主要集中在提升模型的理解能力和增加数据多样性以提高性能。然而，这些方法仍然难以应对模型在领域特定上下文中的适用性不足和语义分布偏见。为克服这些限制，我们提出了一种新的框架，该框架结合了基于大规模语言模型的知识增强工作流和基于跨度的知识融合模型（KnowFREE，Knowledge Fusion for Rich and Efficient Extraction）。我们的工作流通过使用解释提示来生成目标实体的精准上下文解释，有效地减少了语义偏见并丰富了模型的上下文理解。KnowFREE模型进一步整合了扩展标签特征，允许在推理过程中高效地提取嵌套实体而不需要依赖外部知识。在多个领域特定的中文序列标注数据集上的实验表明，我们的方法取得了最先进的性能，有效地解决了低资源环境下的挑战。', 'title_zh': '利用知识融合和上下文标签解释改进低资源序列标注'}
{'arxiv_id': 'arXiv:2501.19022', 'title': 'On the Impact of Noise in Differentially Private Text Rewriting', 'authors': 'Stephen Meisenbacher, Maulik Chevli, Florian Matthes', 'link': 'https://arxiv.org/abs/2501.19022', 'abstract': 'The field of text privatization often leverages the notion of $\\textit{Differential Privacy}$ (DP) to provide formal guarantees in the rewriting or obfuscation of sensitive textual data. A common and nearly ubiquitous form of DP application necessitates the addition of calibrated noise to vector representations of text, either at the data- or model-level, which is governed by the privacy parameter $\\varepsilon$. However, noise addition almost undoubtedly leads to considerable utility loss, thereby highlighting one major drawback of DP in NLP. In this work, we introduce a new sentence infilling privatization technique, and we use this method to explore the effect of noise in DP text rewriting. We empirically demonstrate that non-DP privatization techniques excel in utility preservation and can find an acceptable empirical privacy-utility trade-off, yet cannot outperform DP methods in empirical privacy protections. Our results highlight the significant impact of noise in current DP rewriting mechanisms, leading to a discussion of the merits and challenges of DP in NLP, as well as the opportunities that non-DP methods present.', 'abstract_zh': '文本私有化领域通常利用“差分隐私”（Differential Privacy, DP）的概念为文本敏感数据的重写或模糊处理提供形式化的保证。DP的常见应用形式要求在数据或模型层面向文本的向量表示中添加校准过的噪声，这由隐私参数ε控制。然而，噪声的添加几乎不可避免地导致大量实用性损失，从而突显了DP在自然语言处理（NLP）中的一个主要缺点。在本研究中，我们提出了一种新的句子填补私有化技术，并使用这种方法探讨噪声在DP文本重写中的影响。我们实验证明，非DP私有化技术在保持实用性方面表现出色，并能找到一个可接受的经验隐私-实用性权衡，但无法在经验隐私保护方面超越DP方法。我们的研究结果强调了当前DP重写机制中噪声的巨大影响，引发了关于DP在NLP中的优点与挑战的讨论，以及非DP方法所带来机会的讨论。', 'title_zh': '差分隐私文本重写中的噪声影响探究'}
{'arxiv_id': 'arXiv:2501.19017', 'title': 'Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation', 'authors': 'Bin Zhu, Hui yan Qi, Yinxuan Gui, Jingjing Chen, Chong-Wah Ngo, Ee Peng Lim', 'link': 'https://arxiv.org/abs/2501.19017', 'abstract': 'Multimodal Large Language Models (MLLMs) have exhibited remarkable advancements in integrating different modalities, excelling in complex understanding and generation tasks. Despite their success, MLLMs remain vulnerable to conversational adversarial inputs, particularly negation arguments. This paper systematically evaluates state-of-the-art MLLMs across diverse benchmarks, revealing significant performance drops when negation arguments are introduced to initially correct responses. We show critical vulnerabilities in the reasoning and alignment mechanisms of these models. Proprietary models such as GPT-4o and Claude-3.5-Sonnet demonstrate better resilience compared to open-source counterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to maintain logical consistency under negation arguments during conversation. This paper aims to offer valuable insights for improving the robustness of MLLMs against adversarial inputs, contributing to the development of more reliable and trustworthy multimodal AI systems.', 'abstract_zh': '多模态大型语言模型（MLLMs）在整合不同模态方面取得了显著进展，在复杂理解和生成任务中表现出色。尽管取得了成功，MLLMs 在应对对话式对抗输入时仍然脆弱，尤其是对于否定论证。本文系统地评估了当前最先进的 MLLMs 在多种基准测试中的表现，揭示了在引入否定论证后，这些模型初始正确响应性能显著下降的情况。研究显示这些模型在推理和对齐机制方面存在关键漏洞。如 GPT-4o 和 Claude-3.5-Sonnet 这样的专有模型相比开源模型 Qwen2-VL 和 LLaVA 在对抗输入方面展现出更好的鲁棒性。然而，所有评估的 MLLMs 在对话过程中在面对否定论证时均难以保持逻辑一致性。本文旨在提供改进 MLLMs 对抗输入鲁棒性的宝贵见解，为开发更可靠和可信赖的多模态 AI 系统做出贡献。', 'title_zh': '将铁耙叫做玫瑰：通过否定来误导多模态大语言模型'}
{'arxiv_id': 'arXiv:2501.19010', 'title': 'DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech Recognition', 'authors': 'Wonjun Lee, Solee Im, Heejin Do, Yunsu Kim, Jungseul Ok, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2501.19010', 'abstract': 'Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification alignment. Unlike prior studies focusing on utterance-level embeddings, our granular learning allows discrimination of subtle parts of speech. In addition, we introduce dynamic curriculum learning, which progressively transitions from easy negative samples to difficult-to-distinguishable negative samples based on phonetic similarity of phoneme. Our approach to training by difficulty levels alleviates the inherent variability of speakers, better identifying challenging speeches. Evaluated on the UASpeech dataset, DyPCL outperforms baseline models, achieving an average 22.10\\% relative reduction in word error rate (WER) across the overall dysarthria group.', 'abstract_zh': '构音障碍语音识别通常会因为构音障碍严重程度的内在多样性以及与正常语音的外在差异而 Performance 降低。为了弥合这些差距，我们提出了一种动态音素级对比学习（DyPCL）方法，该方法有助于获得跨不同说话人口的不变表示。我们通过利用动态联接主义时序分类对齐，将语音片段分解为音素段进行音素级对比学习。与以往研究主要关注于整句嵌入不同，我们的精细学习使得能够区分语音中的细微部分。此外，我们引入了动态课程学习，它根据音素的音位相似性逐步从易混淆的负样本过渡到难以区分的负样本。通过不同的难度级别进行训练，我们的方法减弱了说话人流畅性固有的变化性，并更好地识别出了具有挑战性的语音片段。在 UASpeech 数据集上的评估表明，DyPCL 在整个构音障碍组的平均词错误率（WER）上比基线模型降低了 22.10%。', 'title_zh': 'DyPCL：动态音位级对比学习在失调言识别中的应用'}
{'arxiv_id': 'arXiv:2501.18998', 'title': 'Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings', 'authors': 'Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo', 'link': 'https://arxiv.org/abs/2501.18998', 'abstract': 'In recent years, text generation tools utilizing Artificial Intelligence (AI) have occasionally been misused across various domains, such as generating student reports or creative writings. This issue prompts plagiarism detection services to enhance their capabilities in identifying AI-generated content. Adversarial attacks are often used to test the robustness of AI-text generated detectors. This work proposes a novel textual adversarial attack on the detection models such as Fast-DetectGPT. The method employs embedding models for data perturbation, aiming at reconstructing the AI generated texts to reduce the likelihood of detection of the true origin of the texts. Specifically, we employ different embedding techniques, including the Tsetlin Machine (TM), an interpretable approach in machine learning for this purpose. By combining synonyms and embedding similarity vectors, we demonstrates the state-of-the-art reduction in detection scores against Fast-DetectGPT. Particularly, in the XSum dataset, the detection score decreased from 0.4431 to 0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.', 'abstract_zh': '近年来，利用人工智能（AI）的文本生成工具在各个领域被误用的情况时有发生，例如生成学生报告或创造性写作。这一问题促使剽窃检测服务增强其识别AI生成内容的能力。对抗攻击常被用来测试AI文本检测模型的鲁棒性。本研究提出了一种针对检测模型（如Fast-DetectGPT）的新颖文本对抗攻击方法。该方法利用嵌入模型进行数据扰动，旨在重建AI生成的文本，从而降低检测到文本真实来源的可能性。具体而言，我们采用了不同的嵌入技术，包括透明机器学习方法中的突林机（Tsetlin Machine），通过结合同义词和嵌入相似向量，我们展示了该方法在对抗Fast-DetectGPT检测得分方面的先进效果。特别是在XSum数据集中，检测得分从0.4431降至0.2744 AUROC；在SQuAD数据集中，检测得分从0.5068降至0.3532 AUROC。', 'title_zh': '基于嵌入表示和 token 概率的 AI 生成文本检测模型的对抗攻击方法'}
{'arxiv_id': 'arXiv:2501.18957', 'title': 'Intrinsic Tensor Field Propagation in Large Language Models: A Novel Approach to Contextual Information Flow', 'authors': 'Alfred Bexley, Lukas Radcliffe, Giles Weatherstone, Joseph Sakau', 'link': 'https://arxiv.org/abs/2501.18957', 'abstract': 'Context propagation remains a central challenge in language model architectures, particularly in tasks requiring the retention of long-range dependencies. Conventional attention mechanisms, while effective in many applications, exhibit limitations in maintaining coherent contextual representations over extended sequences due to their reliance on discrete token interactions. A novel approach is introduced through the formulation of Intrinsic Tensor Field Propagation (ITFP), which models contextual relationships as continuous tensor fields distributed across token embeddings. The propagation dynamics are governed through differential equations that enable a structured flow of contextual information, augmenting the standard attention mechanism to enhance coherence and recall. A series of experiments conducted on an open-source transformer-based model demonstrate that ITFP provides measurable improvements in contextual retention, dependency resolution, and inference stability across various linguistic structures. Comparisons with baseline models reveal a reduction in syntactic inconsistencies and factual errors, while ablation studies indicate that the choice of propagation depth and integration strength significantly impacts model performance. Additional evaluations assessing domain generalization suggest that ITFP effectively adapts across different text genres, reinforcing its applicability beyond conventional language modeling tasks. Although computational trade-offs are introduced through the inclusion of tensor field computations, empirical findings suggest that the benefits in accuracy and coherence outweigh the increased processing demands.', 'abstract_zh': '语境传播仍然是语言模型架构中的一个核心挑战，尤其是在需要保留长距离依赖的任务中更为显著。传统的注意力机制在许多应用中表现出色，但在维护长序列中的连贯上下文表示方面存在局限性，这是由于它们依赖于离散的token互作。本文引入了一种新的方法，即本征张量场传播（ITFP），将上下文关系建模为分布在token嵌入中的连续张量场。传播动力学通过微分方程进行控制，这使得上下文信息能够在结构化的流动中发挥作用，从而增强标准的注意力机制，以提高连贯性和召回率。在开源的变压器模型上进行的一系列实验表明，ITFP在各种语言结构中显著提高了上下文保留、依赖解析和推理稳定性。与基线模型的比较显示，语法不一致和事实错误有所减少，而消融研究则表明，传播深度和整合强度的选择对模型性能有显著影响。此外，针对领域泛化能力的评估进一步表明，ITFP能够有效地适应不同类型的文本体裁，使其在常规的语言建模任务之外也能得到应用。虽然通过包含张量场计算带来了计算上的权衡，但实证结果表明，在准确性和连贯性方面的收益超过了增加的处理需求。', 'title_zh': '大型语言模型中固有张量场传播的新方法：上下文信息流动的一种新途径'}
{'arxiv_id': 'arXiv:2501.18922', 'title': 'KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search', 'authors': 'Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan', 'link': 'https://arxiv.org/abs/2501.18922', 'abstract': "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo.", 'abstract_zh': '知识图谱问答（KBQA）旨在利用大规模结构化的知识图谱（KB）回答自然语言问题。尽管大型语言模型（LLMs）的进步，KBQA在知识图谱弱意识、效果与效率之间的不平衡以及对标注数据的高度依赖等方面仍然面临挑战。为了克服这些挑战，我们提出了一种名为KBQA-o1的新颖代理型KBQA方法，该方法结合了蒙特卡洛树搜索（MCTS）。它引入了基于ReAct的代理进程，用于逐步逻辑形式生成和知识图谱环境探索。此外，它利用MCTS，一种由策略和奖励模型驱动的启发式搜索方法，来平衡代理探索性能与搜索空间。通过启发式探索，KBQA-o1能够生成高质量的注释，进一步通过增量微调进行改进。实验结果显示，KBQA-o1在有限标注数据的情况下，超越了之前的低资源KBQA方法，将Llama-3.1-8B模型的GrailQA F1性能提高到78.5%，而之前的SOTA方法使用GPT-3.5-turbo时仅为48.5%。', 'title_zh': 'KBQA-o1: 基于蒙特卡洛树搜索的知识库问答'}
{'arxiv_id': 'arXiv:2501.18895', 'title': 'Efficient Supernet Training with Orthogonal Softmax for Scalable ASR Model Compression', 'authors': 'Jingjing Xu, Eugen Beck, Zijian Yang, Ralf Schlüter', 'link': 'https://arxiv.org/abs/2501.18895', 'abstract': 'ASR systems are deployed across diverse environments, each with specific hardware constraints. We use supernet training to jointly train multiple encoders of varying sizes, enabling dynamic model size adjustment to fit hardware constraints without redundant training. Moreover, we introduce a novel method called OrthoSoftmax, which applies multiple orthogonal softmax functions to efficiently identify optimal subnets within the supernet, avoiding resource-intensive search. This approach also enables more flexible and precise subnet selection by allowing selection based on various criteria and levels of granularity. Our results with CTC on Librispeech and TED-LIUM-v2 show that FLOPs-aware component-wise selection achieves the best overall performance. With the same number of training updates from one single job, WERs for all model sizes are comparable to or slightly better than those of individually trained models. Furthermore, we analyze patterns in the selected components and reveal interesting insights.', 'abstract_zh': 'ASR系统部署在具有特定硬件约束的多样化环境中。我们使用超网络训练方法联合训练多个不同大小的编码器，从而能够在不进行冗余训练的情况下动态调整模型大小以适应硬件约束。此外，我们引入了一种名为OrthoSoftmax的新型方法，该方法应用多个正交的softmax函数高效地识别超网络内的最优子网络，避免了资源密集型的搜索过程。该方法还允许根据多种标准和不同的粒度级别进行更加灵活和精确的子网络选择。我们使用CTC在Librispeech和TED-LIUM-v2上的实验结果表明，基于FLOPs感知的组件级选择方法获得最佳的整体性能。在单一任务的相同数量的训练更新后，所有模型大小的词错误率（WER）与各自单独训练的模型相当，甚至略有提高。此外，我们分析了所选组件的模式，并揭示了一些有趣的观点。', 'title_zh': '面向可扩展ASR模型压缩的高效超网络训练与正交softmax方法'}
{'arxiv_id': 'arXiv:2501.18845', 'title': 'Text Data Augmentation for Large Language Models: A Comprehensive Survey of Methods, Challenges, and Opportunities', 'authors': 'Yaping Chai, Haoran Xie, Joe S. Qin', 'link': 'https://arxiv.org/abs/2501.18845', 'abstract': 'The increasing size and complexity of pre-trained language models have demonstrated superior performance in many applications, but they usually require large training datasets to be adequately trained. Insufficient training sets could unexpectedly make the model overfit and fail to cope with complex tasks. Large language models (LLMs) trained on extensive corpora have prominent text generation capabilities, which improve the quality and quantity of data and play a crucial role in data augmentation. Specifically, distinctive prompt templates are given in personalised tasks to guide LLMs in generating the required content. Recent promising retrieval-based techniques further improve the expressive performance of LLMs in data augmentation by introducing external knowledge to enable them to produce more grounded-truth data. This survey provides an in-depth analysis of data augmentation in LLMs, classifying the techniques into Simple Augmentation, Prompt-based Augmentation, Retrieval-based Augmentation and Hybrid Augmentation. We summarise the post-processing approaches in data augmentation, which contributes significantly to refining the augmented data and enabling the model to filter out unfaithful content. Then, we provide the common tasks and evaluation metrics. Finally, we introduce existing challenges and future opportunities that could bring further improvement to data augmentation.', 'abstract_zh': '预训练语言模型的规模和复杂性不断增加，在许多应用中表现出色，但在充分训练时通常需要大量训练数据集。不足的训练数据集可能会意外导致模型过拟合并无法应对复杂的任务。在广泛语料库上训练的大规模语言模型（LLMs）具有显著的文本生成能力，这提高了数据的质量和数量，并在数据增强中扮演关键角色。具体而言，为个性化任务提供独特的提示模板，以指导LLMs生成所需的内容。最近的检索基技术进一步通过引入外部知识来提高LLMs在数据增强中的表达性能，使其能够生成更真实的数据。本文综述了LLMs中的数据增强，将技术分类为简单增强、提示基增强、检索基增强和混合增强。我们总结了数据增强中的后处理方法，这显著有助于完善增强数据并使模型能够筛选出不忠实的内容。然后，我们介绍了常见的任务和评估指标。最后，我们介绍了现有的挑战和未来的机会，这些挑战和机会有望进一步提高数据增强的效果。', 'title_zh': '面向大规模语言模型的文本数据增强：方法、挑战与机遇综述'}
{'arxiv_id': 'arXiv:2501.18837', 'title': 'Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming', 'authors': "Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez", 'link': 'https://arxiv.org/abs/2501.18837', 'abstract': 'Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable.', 'abstract_zh': '大型语言模型（LLMs）容易受到通用突破策略的攻击，这些策略系统地绕过了模型的安全防护，使用户能够执行需要大量模型交互的有害过程，例如大规模制造非法物质。为了防御这些攻击，我们引入了宪法分类器：这些防护措施是基于合成数据训练的，而合成数据是由提示LLM使用自然语言规则（即宪法）生成的，这些规则明确了允许和限制的内容。在超过3000个估计的红队测试小时里，没有一位红队成员能够找到能够从早期分类器守护的LLM中提取与未受保护模型在大多数目标查询中相似详细程度信息的通用突破策略。在自动化评估中，增强后的分类器展示了对域特定的隔离突破具有稳健的防御能力。这些分类器还保持了部署可行性，增加了绝对0.38%的生产流量拒绝率和23.7%的推理开销。我们的工作证明了在保持实用性的同时防御通用突破策略是可行的。', 'title_zh': '宪法分类器：防御跨越数千小时红队攻击的通用破解攻击'}
{'arxiv_id': 'arXiv:2501.18826', 'title': 'Structural Embedding Projection for Contextual Large Language Model Inference', 'authors': 'Vincent Enoasmo, Cedric Featherstonehaugh, Xavier Konstantinopoulos, Zacharias Huntington', 'link': 'https://arxiv.org/abs/2501.18826', 'abstract': "Structured embedding transformations offer a promising approach for enhancing the efficiency and coherence of language model inference. The introduction of Structural Embedding Projection (SEP) provides a mechanism for refining token representations through projection matrices that integrate hierarchical and relational dependencies. The mathematical formulation of SEP enables embedding spaces to capture structured contextual relationships, thereby improving semantic fidelity without significantly increasing computational overhead. Experimental evaluations conducted on a range of linguistic datasets revealed that SEP contributed to reductions in perplexity and enhanced contextual coherence, demonstrating its potential to refine language model outputs. Computational efficiency assessments highlighted variations across different datasets, suggesting that the integration of structured embeddings introduced dataset-dependent trade-offs between inference speed and representational richness. The qualitative analysis of generated responses indicated that SEP enhanced narrative consistency and topic alignment, leading to improved fluency in multi-sentence text generation. The modifications to embedding layers required precise optimization to ensure stable training dynamics, as the introduction of structured transformations altered the traditional representation-learning process. The architectural adjustments necessary for SEP implementation influenced inference latency and memory consumption, requiring a balance between efficiency gains and additional processing demands. The impact of SEP on lexical diversity suggested that embedding modifications influenced the model's vocabulary usage, reflecting a more context-aware selection of generated tokens.", 'abstract_zh': '结构嵌入变换为提高语言模型推理的效率和连贯性提供了有希望的方法。Structural Embedding Projection (SEP) 的引入提供了一种机制，通过结合层次和关系依赖性的投影矩阵细化词元表示。SEP的数学表述使嵌入空间能够捕捉结构化的上下文关系，从而在不显著增加计算开销的情况下提高语义保真度。在多种语言数据集上的实验评估表明，SEP有助于减少困惑度并增强上下文连贯性，证明了其在提升语言模型输出方面的潜力。计算效率评估揭示了不同数据集间的差异，表明结构嵌入的集成在推理速度和表示丰富性之间引入了数据集依赖的权衡。生成响应的定性分析表明，SEP增强了叙事一致性并改善了主题对齐，从而提高了多句文本生成的流畅度。嵌入层的修改要求精确优化以确保稳定的训练动态，因为结构变换的引入改变了传统的表征学习过程。SEP实施所需的架构调整影响了推理延迟和内存消耗，需要在效率提升和额外处理需求之间寻求平衡。SEP对词汇多样性的影晌表明，嵌入层的修改影响了模型词汇的使用情况，反映了对生成词元更具上下文意识的选择。', 'title_zh': '基于结构嵌入投影的上下文大型语言模型推理'}
{'arxiv_id': 'arXiv:2501.18824', 'title': 'Memory-Efficient Fine-Tuning of Transformers via Token Selection', 'authors': 'Antoine Simoulin, Namyong Park, Xiaoyi Liu, Grey Yang', 'link': 'https://arxiv.org/abs/2501.18824', 'abstract': 'Fine-tuning provides an effective means to specialize pre-trained models for various downstream tasks. However, fine-tuning often incurs high memory overhead, especially for large transformer-based models, such as LLMs. While existing methods may reduce certain parts of the memory required for fine-tuning, they still require caching all intermediate activations computed in the forward pass to update weights during the backward pass. In this work, we develop TokenTune, a method to reduce memory usage, specifically the memory to store intermediate activations, in the fine-tuning of transformer-based models. During the backward pass, TokenTune approximates the gradient computation by backpropagating through just a subset of input tokens. Thus, with TokenTune, only a subset of intermediate activations are cached during the forward pass. Also, TokenTune can be easily combined with existing methods like LoRA, further reducing the memory cost. We evaluate our approach on pre-trained transformer models with up to billions of parameters, considering the performance on multiple downstream tasks such as text classification and question answering in a few-shot learning setup. Overall, TokenTune achieves performance on par with full fine-tuning or representative memory-efficient fine-tuning methods, while greatly reducing the memory footprint, especially when combined with other methods with complementary memory reduction mechanisms. We hope that our approach will facilitate the fine-tuning of large transformers, in specializing them for specific domains or co-training them with other neural components from a larger system. Our code is available at this https URL.', 'abstract_zh': '微调提供了一种有效的方法，使预训练模型适用于各种下游任务。然而，微调通常会导致较高的内存开销，尤其是在大型基于Transformer的模型（如大语言模型）的情况下。尽管现有方法可能减少了某些部分的内存需求，但仍需要缓存前向传播过程中计算的所有中间激活，以便在反向传播过程中更新权重。在这项工作中，我们开发了TokenTune，这是一种方法，可以减少基于Transformer模型的微调中的内存使用，特别是减少存储中间激活所需的内存。在反向传播过程中，TokenTune通过仅对输入标记的子集进行反向传播来近似梯度计算。因此，在TokenTune中，仅在前向传播过程中缓存中间激活的子集。此外，TokenTune可以与现有方法（如LoRA）轻松结合，进一步降低内存成本。我们在具有多达数十亿参数的预训练Transformer模型上评估了我们的方法，考虑了多种下游任务（如文本分类和少样本学习中的问答任务）的性能。总体而言，TokenTune在性能上与完整的微调或代表性的内存高效微调方法旗鼓相当，同时极大地减少了内存占用，特别是在与其他具有互补内存减少机制的方法结合使用时。我们希望，我们的方法能够促进大型Transformer的微调，使其适用于特定领域或与其他更大的系统中的神经组件共同训练。我们的代码可在以下链接获取：[这里提供链接]。', 'title_zh': '通过 token 选择实现 Transformers 的高效微调'}
{'arxiv_id': 'arXiv:2501.18816', 'title': 'Large Language Models as Common-Sense Heuristics', 'authors': 'Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock', 'link': 'https://arxiv.org/abs/2501.18816', 'abstract': 'While systems designed for solving planning tasks vastly outperform Large Language Models (LLMs) in this domain, they usually discard the rich semantic information embedded within task descriptions. In contrast, LLMs possess parametrised knowledge across a wide range of topics, enabling them to leverage the natural language descriptions of planning tasks in their solutions. However, current research in this direction faces challenges in generating correct and executable plans. Furthermore, these approaches depend on the LLM to output solutions in an intermediate language, which must be translated into the representation language of the planning task. We introduce a novel planning method, which leverages the parametrised knowledge of LLMs by using their output as a heuristic for Hill-Climbing Search. This approach is further enhanced by prompting the LLM to generate a solution estimate to guide the search. Our method outperforms the task success rate of similar systems within a common household environment by 22 percentage points, with consistently executable plans. All actions are encoded in their original representation, demonstrating that strong results can be achieved without an intermediate language, thus eliminating the need for a translation step.', 'abstract_zh': '虽然为了解决规划任务而设计的系统在这一领域远远超过大型语言模型（LLMs），但它们通常会丢弃任务描述中包含的丰富语义信息。相比之下，LLMs具有跨多种主题的参数化知识，使其能够利用规划任务的自然语言描述来解决这些问题。然而，当前在这方面的研究在生成正确且可执行的计划方面面临挑战。此外，这些方法依赖于LLMs以中间语言的形式输出解决方案，并需要将其转换为规划任务的表示语言。我们提出了一种新颖的规划方法，该方法通过利用LLMs的参数化知识，使用其输出作为Hill-Climbing搜索的启发式信息。该方法进一步通过促使LLMs生成解决方案估计值来引导搜索过程。我们的方法在常见家庭环境中的任务成功率方面比类似系统高出22个百分点，且始终能够生成可执行的计划。所有操作均以原始表示形式编码，这表明无需中间语言即可取得强劲结果，从而消除了转换步骤的需要。', 'title_zh': '大型语言模型作为常识启发式方法'}
{'arxiv_id': 'arXiv:2501.18795', 'title': 'Rope to Nope and Back Again: A New Hybrid Attention Strategy', 'authors': 'Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, Acyr Locatelli', 'link': 'https://arxiv.org/abs/2501.18795', 'abstract': 'Long-context large language models (LLMs) have achieved remarkable advancements, driven by techniques like Rotary Position Embedding (RoPE) (Su et al., 2023) and its extensions (Chen et al., 2023; Liu et al., 2024c; Peng et al., 2023). By adjusting RoPE parameters and incorporating training data with extended contexts, we can train performant models with considerably longer input sequences. However, existing RoPE-based methods exhibit performance limitations when applied to extended context lengths. This paper presents a comprehensive analysis of various attention mechanisms, including RoPE, No Positional Embedding (NoPE), and Query-Key Normalization (QK-Norm), identifying their strengths and shortcomings in long-context modeling. Our investigation identifies distinctive attention patterns in these methods and highlights their impact on long-context performance, providing valuable insights for architectural design. Building on these findings, we propose a novel architectural based on a hybrid attention mechanism that not only surpasses conventional RoPE-based transformer models in long context tasks but also achieves competitive performance on benchmarks requiring shorter context lengths.', 'abstract_zh': '长上下文大语言模型（LLMs）取得了显著进步，这得益于技术诸如旋转位置嵌入（RoPE）（Su et al., 2023）、其扩展方法（Chen et al., 2023；Liu et al., 2024c；Peng et al., 2023）等的推动。通过调整RoPE参数并在训练数据中引入扩展上下文，我们能够训练出具有显著更长输入序列的高效模型。然而，现有的基于RoPE的方法在应用到扩展上下文长度时表现出性能限制。本文对各种注意力机制进行了全面分析，包括RoPE、无位置嵌入（NoPE）和查询-键归一化（QK-Norm），并识别了这些方法在长上下文建模中的优点和不足。我们的研究揭示了这些方法在注意力模式上的独特性，并强调了它们对长上下文性能的影响，提供了对体系结构设计有价值的见解。基于这些发现，我们提出了一种基于混合注意力机制的新架构，不仅在长上下文任务中超越了传统的基于RoPE的变压器模型，还在需要较短上下文长度的基准测试中达到了竞争性的性能。', 'title_zh': '从Rope到Nope再回归：一种新的混合注意力策略'}
{'arxiv_id': 'arXiv:2501.18771', 'title': "Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation", 'authors': 'Muhammed Yusuf Kocyigit, Eleftheria Briakou, Daniel Deutsch, Jiaming Luo, Colin Cherry, Markus Freitag', 'link': 'https://arxiv.org/abs/2501.18771', 'abstract': 'Data contamination -- the accidental consumption of evaluation examples within the pre-training data -- can undermine the validity of evaluation benchmarks. In this paper, we present a rigorous analysis of the effects of contamination on language models at 1B and 8B scales on the machine translation task. Starting from a carefully decontaminated train-test split, we systematically introduce contamination at various stages, scales, and data formats to isolate its effect and measure its impact on performance metrics. Our experiments reveal that contamination with both source and target substantially inflates BLEU scores, and this inflation is 2.5 times larger (up to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and target-only contamination generally produce smaller, less consistent over-estimations. Finally, we study how the temporal distribution and frequency of contaminated samples influence performance over-estimation across languages with varying degrees of data resources.', 'abstract_zh': '数据污染——预训练数据中无意间消费的评估样本可能会削弱评估基准的有效性。在本文中，我们对数据污染对大规模语言模型（1B和8B数量级）在机器翻译任务上的影响进行了严格的分析。从一个精心去除了污染的训练-测试分割开始，我们系统地在不同的阶段、规模和数据格式中引入污染，以分离其影响并衡量其对性能指标的影响。我们的实验表明，同时污染源语言和目标语言会导致BLEU评分显著膨胀，且8B模型的膨胀程度是非1B模型的2.5倍（最多可达到30个BLEU点）。相比之下，仅污染源语言和仅污染目标语言通常会产生较小且不一致的膨胀。最后，我们研究了污染样本的时间分布和频率如何影响不同数据资源量的语言性能估计偏差。', 'title_zh': 'LLM评估中的过度估计：数据污染对机器翻译影响的受控大规模研究'}
{'arxiv_id': 'arXiv:2501.18766', 'title': 'Breaking the Fake News Barrier: Deep Learning Approaches in Bangla Language', 'authors': 'Pronoy Kumar Mondal, Sadman Sadik Khan, Md. Masud Rana, Shahriar Sultan Ramit, Abdus Sattar, Md. Sadekur Rahman', 'link': 'https://arxiv.org/abs/2501.18766', 'abstract': 'The rapid development of digital stages has greatly compounded the dispersal of untrue data, dissolving certainty and judgment in society, especially among the Bengali-speaking community. Our ponder addresses this critical issue by presenting an interesting strategy that utilizes a profound learning innovation, particularly the Gated Repetitive Unit (GRU), to recognize fake news within the Bangla dialect. The strategy of our proposed work incorporates intensive information preprocessing, which includes lemmatization, tokenization, and tending to course awkward nature by oversampling. This comes about in a dataset containing 58,478 passages. We appreciate the creation of a demonstration based on GRU (Gated Repetitive Unit) that illustrates remarkable execution with a noteworthy precision rate of 94%. This ponder gives an intensive clarification of the methods included in planning the information, selecting the show, preparing it, and assessing its execution. The performance of the model is investigated by reliable metrics like precision, recall, F1 score, and accuracy. The commitment of the work incorporates making a huge fake news dataset in Bangla and a demonstration that has outperformed other Bangla fake news location models.', 'abstract_zh': '数字舞台的迅速发展极大地加剧了虚假数据的扩散，侵蚀了社会的确定性和判断力，尤其是在孟加拉语使用者中。本文针对这一关键问题，提出了一个有趣的战略，该战略利用了深度学习的创新技术，特别是门控重复单元（GRU），以识别孟加拉语方言中的假新闻。本文提出的策略包括密集的信息预处理，包括词干提取、分词，并通过过采样来处理不均衡性。这导致了一个包含58,478段文本的数据集。我们基于GRU（门控重复单元）创建了一个演示模型，该模型在显著的精度（94%）方面展示了出色的表现。本文详细解释了数据规划、模型选择、训练和评估性能所采用的方法。模型的性能通过精准率、召回率、F1分数和准确性等可靠指标进行了评估。本文的贡献包括创建了一个庞大的孟加拉语假新闻数据集以及一个优于其他孟加拉语假新闻检测模型的演示模型。', 'title_zh': '突破假新闻障碍：孟加拉语深度学习方法'}
{'arxiv_id': 'arXiv:2501.18750', 'title': 'Revisiting Projection-based Data Transfer for Cross-Lingual Named Entity Recognition in Low-Resource Languages', 'authors': 'Andrei Politov, Oleh Shkalikov, René Jäkel, Michael Färber', 'link': 'https://arxiv.org/abs/2501.18750', 'abstract': 'Cross-lingual Named Entity Recognition (NER) leverages knowledge transfer between languages to identify and classify named entities, making it particularly useful for low-resource languages. We show that the data-based cross-lingual transfer method is an effective technique for crosslingual NER and can outperform multilingual language models for low-resource languages. This paper introduces two key enhancements to the annotation projection step in cross-lingual NER for low-resource languages. First, we explore refining word alignments using back-translation to improve accuracy. Second, we present a novel formalized projection approach of matching source entities with extracted target candidates. Through extensive experiments on two datasets spanning 57 languages, we demonstrated that our approach surpasses existing projectionbased methods in low-resource settings. These findings highlight the robustness of projection-based data transfer as an alternative to model-based methods for crosslingual named entity recognition in lowresource languages.', 'abstract_zh': '跨语言命名实体识别（NER）利用语际知识迁移来识别和分类命名实体，特别适用于低资源语言。我们展示了基于数据的跨语言迁移方法在跨语言NER中是有效的，尤其对于低资源语言而言，其性能可以超越多语言语言模型。本文针对低资源语言的跨语言命名实体识别引入了两个关键改进。首先，我们探索使用反向翻译来改进词对齐，以提高准确性。其次，我们提供了一种新的形式化投影方法，即将源实体与提取的目标候选实体进行匹配。通过在两个涵盖57种语言的数据集上进行广泛的实验，我们证明了我们的方法在低资源环境中优于现有的基于投影的方法。这些发现突显了基于投影的数据迁移在跨语言命名实体识别中的稳健性，它作为一种替代模型驱动方法的可能方案。', 'title_zh': '重新审视基于投影的数据迁移在低资源语言跨语言命名实体识别中的应用'}
{'arxiv_id': 'arXiv:2501.18738', 'title': 'Examining the Robustness of Large Language Models across Language Complexity', 'authors': 'Jiayi Zhang', 'link': 'https://arxiv.org/abs/2501.18738', 'abstract': 'With the advancement of large language models (LLMs), an increasing number of student models have leveraged LLMs to analyze textual artifacts generated by students to understand and evaluate their learning. These student models typically employ pre-trained LLMs to vectorize text inputs into embeddings and then use the embeddings to train models to detect the presence or absence of a construct of interest. However, how reliable and robust are these models at processing language with different levels of complexity? In the context of learning where students may have different language backgrounds with various levels of writing skills, it is critical to examine the robustness of such models to ensure that these models work equally well for text with varying levels of language complexity. Coincidentally, a few (but limited) research studies show that the use of language can indeed impact the performance of LLMs. As such, in the current study, we examined the robustness of several LLM-based student models that detect student self-regulated learning (SRL) in math problem-solving. Specifically, we compared how the performance of these models vary using texts with high and low lexical, syntactic, and semantic complexity measured by three linguistic measures.', 'abstract_zh': '随着大规模语言模型（LLMs）的进步，越来越多的学生模型利用LLMs来分析学生生成的文本，以理解和评估其学习情况。这些学生模型通常采用预训练的LLMs将文本输入矢量化为嵌入表示，然后使用这些嵌入来训练模型以检测感兴趣的结构是否存在。然而，这些模型在处理不同复杂度的语言时有多可靠和稳健？在学生可能具有不同语言背景和不同写作技能的学习情境下，确保这些模型能够有效地处理不同语言复杂度的文本尤为重要。巧合的是，少数（但有限）的研究表明，语言的使用确实会影响LLMs的性能。因此，在本研究中，我们考察了几种基于LLM的学生模型在检测数学问题解决中的自我调节学习（SRL）时的稳健性。具体而言，我们通过使用基于三项语言学度量测量的高复杂度和低复杂度的文本，比较了这些模型的性能差异。', 'title_zh': '考察大型语言模型在不同语言复杂性下的稳健性'}
{'arxiv_id': 'arXiv:2501.18724', 'title': 'Zero-shot Large Language Models for Long Clinical Text Summarization with Temporal Reasoning', 'authors': 'Maya Kruse, Shiyue Hu, Nicholas Derby, Yifu Wu, Samantha Stonbraker, Bingsheng Yao, Dakuo Wang, Elizabeth Goldberg, Yanjun Gao', 'link': 'https://arxiv.org/abs/2501.18724', 'abstract': 'Recent advancements in large language models (LLMs) have shown potential for transforming data processing in healthcare, particularly in understanding complex clinical narratives. This study evaluates the efficacy of zero-shot LLMs in summarizing long clinical texts that require temporal reasoning, a critical aspect for comprehensively capturing patient histories and treatment trajectories. We applied a series of advanced zero-shot LLMs to extensive clinical documents, assessing their ability to integrate and accurately reflect temporal dynamics without prior task-specific training. While the models efficiently identified key temporal events, they struggled with chronological coherence over prolonged narratives. The evaluation, combining quantitative and qualitative methods, highlights the strengths and limitations of zero-shot LLMs in clinical text summarization. The results suggest that while promising, zero-shot LLMs require further refinement to effectively support clinical decision-making processes, underscoring the need for enhanced model training approaches that better capture the nuances of temporal information in long context medical documents.', 'abstract_zh': '近年来，大型语言模型（LLMs）的发展显示出其在医疗健康数据处理中的潜在潜力，特别是在理解复杂的临床叙述方面。本研究评估了零样本LLMs在总结需要时间推理的长篇临床文本方面的有效性，这是全面捕捉患者病史和治疗轨迹的关键方面。我们应用了一系列先进的零样本LLMs对大量临床文件进行了评估，测试了它们在不进行特定任务训练的情况下整合并准确反映时间动态的能力。虽然这些模型能够高效地识别关键的时间事件，但在长时间叙述中保持时间顺序连贯性方面存在困难。本评估综合了定量和定性方法，突显了零样本LLMs在临床文本总结方面的优势和局限性。研究结果表明，虽然零样本LLMs前景广阔，但它们需要进一步优化，以更有效地支持临床决策过程，强调了需要采用更好的模型训练方法来更好地捕捉长上下文医学文档中的时间信息的重要性。', 'title_zh': '零样本大型语言模型在具有时间推理的长临床文本摘要中的应用'}
{'arxiv_id': 'arXiv:2501.18649', 'title': 'Fake News Detection After LLM Laundering: Measurement and Explanation', 'authors': 'Rupak Kumar Das, Jonathan Dodge', 'link': 'https://arxiv.org/abs/2501.18649', 'abstract': 'With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub', 'abstract_zh': '随着大型语言模型（LLMs）的先进能力，它们可以生成高度逼真且与上下文相关的假新闻，这有助于传播错误信息。尽管已有大量有关检测人类撰写的假新闻的研究，但检测LLM生成的假新闻的领域仍相对未被充分探索。本研究评估了检测器在识别LLM重述的假新闻方面的有效性，特别是在确定检测管道中是否加入重述步骤是否有助于或妨碍检测方面发挥了作用。本研究的贡献在于：（1）检测器在识别LLM重述的假新闻方面比识别人类撰写的假新闻更加困难；（2）我们发现哪些模型在哪些任务中表现优异（逃避检测、通过重述逃避检测以及为了语义相似性进行重述）；（3）通过LIME解释，我们发现检测失败可能的一个原因是情感转变；（4）我们发现重述质量测量的一个令人担忧的趋势：尽管BERTSCORE很高，但样本仍表现出情感转变；（5）我们提供了一对数据集，这些数据集通过加入重述输出和评分增强了现有数据集。该数据集可在GitHub上获取。', 'title_zh': '经过LLM清洗后的假新闻检测：测量与解释'}
{'arxiv_id': 'arXiv:2501.18645', 'title': 'Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models', 'authors': 'Manish Sanwal', 'link': 'https://arxiv.org/abs/2501.18645', 'abstract': 'Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains.', 'abstract_zh': '大型语言模型（LLMs）利用链式思考（CoT）提示提供逐步的推理过程，从而在复杂任务上表现出色。尽管具有诸多优势，传统的CoT往往未能完全验证中间推理，且可能生成误导性的解释。在这项研究中，我们提出了一种新颖的框架——分层链式思考（Layered-CoT）提示，该框架系统地将推理过程划分为多个层次，每个层次都接受外部检查并可选地接受用户反馈。我们详细介绍了核心概念，并提出了三个应用场景——医疗分诊、金融风险评估和敏捷工程——展示了Layered-CoT在透明度、正确性和用户参与度方面如何超越传统的CoT。通过结合近期arXiv论文中关于交互式解释、多智能体框架以及基于代理的合作研究中的参考，我们阐述了Layered-CoT如何为高风险领域中的更可靠和具体的解释开辟道路。', 'title_zh': '多层思维链提示在多代理大语言模型系统中的应用：一种全面的可解释大语言模型方法'}
{'arxiv_id': 'arXiv:2501.18644', 'title': 'Prompt-oriented Output of Culture-Specific Items in Translated African Poetry by Large Language Model: An Initial Multi-layered Tabular Review', 'authors': 'Adeyola Opaluwah', 'link': 'https://arxiv.org/abs/2501.18644', 'abstract': 'This paper examines the output of cultural items generated by Chat Generative PreTrained Transformer Pro in response to three structured prompts to translate three anthologies of African poetry. The first prompt was broad, the second focused on poetic structure, and the third prompt emphasized cultural specificity. To support this analysis, four comparative tables were created. The first table presents the results of the cultural items produced after the three prompts, the second categorizes these outputs based on Aixela framework of Proper nouns and Common expressions, the third table summarizes the cultural items generated by human translators, a custom translation engine, and a Large Language Model. The final table outlines the strategies employed by Chat Generative PreTrained Transformer Pro following the culture specific prompt. Compared to the outputs of cultural items from reference human translation and the custom translation engine in prior studies the findings indicate that the culture oriented prompts used with Chat Generative PreTrained Transformer Pro did not yield significant enhancements of cultural items during the translation of African poetry from English to French. Among the fifty four cultural items, the human translation produced thirty three cultural items in repetition, the custom translation engine generated Thirty eight cultural items in repetition while Chat Generative PreTrained Transformer Pro produced forty one cultural items in repetition. The untranslated cultural items revealed inconsistencies in Large language models approach to translating cultural items in African poetry from English to French.', 'abstract_zh': '本文探讨了由Chat Generative PreTrained Transformer Pro在回应三个结构化提示后生成的文化物品输出，该过程用于翻译三部非洲诗歌集。第一个提示较为广泛，第二个提示聚焦于诗歌结构，第三个提示则强调文化特定性。为了支持这一分析，我们创建了四张比较表。第一张表展示了在三个提示之后生成的文化物品结果，第二张表基于Aixela框架将这些输出分类为专有名词和常见表达，第三张表对人类译者、定制翻译引擎和大型语言模型生成的文化物品进行了总结。最后一张表概述了Chat Generative PreTrained Transformer Pro在文化特定提示下采用的策略。与前人研究中参考的人类翻译和定制翻译引擎的输出相比，本文的结果表明，在将非洲诗歌从英语翻译成法语的过程中，使用文化导向提示的Chat Generative PreTrained Transformer Pro没有显著提升文化物品的表现。在54个文化物品中，人类翻译重复生成了33个文化物品，定制翻译引擎重复生成了38个文化物品，而Chat Generative PreTrained Transformer Pro重复生成了41个文化物品。未翻译的文化物品揭示了大型语言模型在将非洲诗歌中的文化物品从英语翻译成法语时的方法一致性问题。', 'title_zh': '由大型语言模型导向的译介非洲诗歌中的文化特定项输出：一项初始的多层表格审查'}
{'arxiv_id': 'arXiv:2501.18640', 'title': 'Divergent Emotional Patterns in Disinformation on Social Media? An Analysis of Tweets and TikToks about the DANA in Valencia', 'authors': 'Iván Arcos, Paolo Rosso, Ramón Salaverría', 'link': 'https://arxiv.org/abs/2501.18640', 'abstract': "This study investigates the dissemination of disinformation on social media platforms during the DANA event (DANA is a Spanish acronym for Depresion Aislada en Niveles Altos, translating to high-altitude isolated depression) that resulted in extremely heavy rainfall and devastating floods in Valencia, Spain, on October 29, 2024. We created a novel dataset of 650 TikTok and X posts, which was manually annotated to differentiate between disinformation and trustworthy content. Additionally, a Few-Shot annotation approach with GPT-4o achieved substantial agreement (Cohen's kappa of 0.684) with manual labels. Emotion analysis revealed that disinformation on X is mainly associated with increased sadness and fear, while on TikTok, it correlates with higher levels of anger and disgust. Linguistic analysis using the LIWC dictionary showed that trustworthy content utilizes more articulate and factual language, whereas disinformation employs negations, perceptual words, and personal anecdotes to appear credible. Audio analysis of TikTok posts highlighted distinct patterns: trustworthy audios featured brighter tones and robotic or monotone narration, promoting clarity and credibility, while disinformation audios leveraged tonal variation, emotional depth, and manipulative musical elements to amplify engagement. In detection models, SVM+TF-IDF achieved the highest F1-Score, excelling with limited data. Incorporating audio features into roberta-large-bne improved both Accuracy and F1-Score, surpassing its text-only counterpart and SVM in Accuracy. GPT-4o Few-Shot also performed well, showcasing the potential of large language models for automated disinformation detection. These findings demonstrate the importance of leveraging both textual and audio features for improved disinformation detection on multimodal platforms like TikTok.", 'abstract_zh': "本研究探讨了西班牙瓦伦西亚地区（于2024年10月29日由DANA事件引起，DANA为西班牙语缩写，意为高海拔孤立性抑郁）导致极端暴雨和毁灭性洪水期间社交平台上的虚假信息传播情况。我们创建了一个包含650条TikTok和X社交媒体帖子的新数据集，并对其进行手动标注，以区分虚假信息和可信内容。此外，采用few-shot标注方法结合GPT-4o实现了与手动标签高度一致（Cohen's kappa值为0.684）。情感分析显示，X平台上的虚假信息主要与增加的悲伤和恐惧情绪相关，而TikTok平台上的虚假信息则与更高的愤怒和厌恶情绪相关。通过使用LIWC词典进行语言分析，可信内容使用了更加准确和客观的语言，而虚假信息则通过使用否定词、感知词和个人故事来显得可信。TikTok帖子的音频分析揭示了不同的模式：可信音频具有更明亮的音调和机械或单调的叙述，以促进清晰和可信度，而虚假信息音频则利用音调变化、情感深度以及操控性的音乐元素来增强观众的参与感。在检测模型中，SVM+TF-IDF获得了最高的F1分数，在有限的数据条件下表现优异。将音频特征纳入roberta-large-bne模型中提高了准确性和F1分数，超过了仅使用文本的版本和SVM。GPT-4o few-shot标注方法也表现出良好效果，展示了大规模语言模型在自动化检测虚假信息方面的潜力。这些发现证明了在多模态平台如TikTok上提高虚假信息检测效果的重要性，须同时利用文字和音频特征。", 'title_zh': '社交媒体上关于DANA在瓦伦西亚的虚假信息中的情感模式差异：推特和 TikTok 分析'}
{'arxiv_id': 'arXiv:2501.18633', 'title': 'Linguistic Analysis of Sinhala YouTube Comments on Sinhala Music Videos: A Dataset Study', 'authors': 'W. M. Yomal De Mel, Nisansa de Silva', 'link': 'https://arxiv.org/abs/2501.18633', 'abstract': 'This research investigates the area of Music Information Retrieval (MIR) and Music Emotion Recognition (MER) in relation to Sinhala songs, an underexplored field in music studies. The purpose of this study is to analyze the behavior of Sinhala comments on YouTube Sinhala song videos using social media comments as primary data sources. These included comments from 27 YouTube videos containing 20 different Sinhala songs, which were carefully selected so that strict linguistic reliability would be maintained and relevancy ensured. This process led to a total of 93,116 comments being gathered upon which the dataset was refined further by advanced filtering methods and transliteration mechanisms resulting into 63,471 Sinhala comments. Additionally, 964 stop-words specific for the Sinhala language were algorithmically derived out of which 182 matched exactly with English stop-words from NLTK corpus once translated. Also, comparisons were made between general domain corpora in Sinhala against the YouTube Comment Corpus in Sinhala confirming latter as good representation of general domain. The meticulously curated data set as well as the derived stop-words form important resources for future research in the fields of MIR and MER, since they could be used and demonstrate that there are possibilities with computational techniques to solve complex musical experiences across varied cultural traditions', 'abstract_zh': '本研究探讨了音乐信息检索（MIR）和音乐情绪识别（MER）领域中的僧伽罗歌曲，这是一个在音乐研究中尚未充分探索的领域。本研究的主要目的是通过社交媒体评论作为主要数据源，分析僧伽罗YouTube歌曲视频下的评论行为。在这项研究中，我们选择了包含20首不同僧伽罗歌曲的27个YouTube视频评论，以确保严格的语言可靠性和相关性。通过这一过程，我们总共收集了93,116条评论，并通过高级筛选方法和转写机制进一步精炼数据集，最终获得63,471条僧伽罗语评论。此外，我们通过算法从僧伽罗语中产生了964条专门的停用词，其中182条与NLTK语料库中的英语停用词完全匹配。我们还对通用领域的僧伽罗语语料库进行了与僧伽罗语YouTube评论语料库的比较，从而确认后者是通用领域的良好代表。经过精细整理的数据集以及推导出的停用词，对于未来MIR和MER领域的研究提供了重要资源，因为这些资源可用于证明可以通过计算技术解决不同文化传统的复杂音乐体验。', 'title_zh': '斯里兰卡语YouTube评论对斯里兰卡音乐视频的语言分析：一个数据集研究'}
{'arxiv_id': 'arXiv:2501.19383', 'title': 'Decoding-based Regression', 'authors': 'Xingyou Song, Dara Bahri', 'link': 'https://arxiv.org/abs/2501.19383', 'abstract': 'Language models have recently been shown capable of performing regression tasks wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal auto-regressive sequence models when they are applied to any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoding-based regression is as performant as traditional approaches for tabular regression tasks, while being flexible enough to capture arbitrary distributions, such as in the task of density estimation.', 'abstract_zh': '近年来，语言模型被证明有能力执行回归任务，在这种任务中，数值预测以解码后的字符串形式表示。在本研究中，我们为这种能力提供了理论依据，并进一步探讨了因果自回归序列模型在应用于任何特征表示时的实用性。我们发现，尽管这些模型通常是通过交叉熵损失进行下一个词预测的方式训练的，在表格回归任务中，基于解码的回归性能与传统方法相当，同时能够灵活地捕捉任意分布，例如在密度估计任务中。', 'title_zh': '基于解码的回归'}
{'arxiv_id': 'arXiv:2501.19377', 'title': 'SELMA: A Speech-Enabled Language Model for Virtual Assistant Interactions', 'authors': 'Dominik Wagner, Alexander Churchill, Siddarth Sigtia, Erik Marchi', 'link': 'https://arxiv.org/abs/2501.19377', 'abstract': 'In this work, we present and evaluate SELMA, a Speech-Enabled Language Model for virtual Assistant interactions that integrates audio and text as inputs to a Large Language Model (LLM). SELMA is designed to handle three primary and two auxiliary tasks related to interactions with virtual assistants simultaneously within a single end-to-end model. We employ low-rank adaptation modules for parameter-efficient training of both the audio encoder and the LLM. Additionally, we implement a feature pooling strategy enabling the system to recognize global patterns and improve accuracy on tasks less reliant on individual sequence elements. Experimental results on Voice Trigger (VT) detection, Device-Directed Speech Detection (DDSD), and Automatic Speech Recognition (ASR), demonstrate that our approach both simplifies the typical input processing pipeline of virtual assistants significantly and also improves performance compared to dedicated models for each individual task. SELMA yields relative Equal-Error Rate improvements of 64% on the VT detection task, and 22% on DDSD, while also achieving word error rates close to the baseline.', 'abstract_zh': '在本文中，我们介绍了并评估了SELMA（Speech-Enabled Language Model for Virtual Assistant Interactions），这是一种结合了音频和文本输入的大语言模型（LLM），用于虚拟助手交互。SELMA 设计用于在同一端到端模型中同时处理与虚拟助手交互相关的三大主要任务和两大辅助任务。我们采用低秩适应模块对音频编码器和大语言模型进行参数高效的训练。此外，我们实现了一种特征聚合策略，使系统能够识别全局模式并提高对较少依赖于个体序列元素的任务的准确性。在Voice Trigger（VT）检测、Device-Directed Speech Detection（DDSD）和自动语音识别（ASR）等实验中的结果表明，我们的方法不仅显著简化了虚拟助手的典型输入处理流程，还显著提高了性能，相较于针对每个单独任务的专用模型。在VT检测任务中，SELMA 的相对等错误率（Equal-Error Rate, EER）改进了64%，在DDSD任务中改进了22%，同时在单词错误率（Word Error Rate, WER）方面接近基线水平。', 'title_zh': 'SELMA：一种语音启用的语言模型，适用于虚拟助手交互'}
{'arxiv_id': 'arXiv:2501.19361', 'title': "We're Different, We're the Same: Creative Homogeneity Across LLMs", 'authors': 'Emily Wenger, Yoed Kenett', 'link': 'https://arxiv.org/abs/2501.19361', 'abstract': 'Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today\'s LLMs behave similarly, using them as a creative partners -- regardless of the model used -- may drive all users towards a limited set of "creative" outputs.', 'abstract_zh': '如今，有许多强大的大语言模型（LLMs）可用作写作辅助工具、创意生成器等。虽然这些LLMs被宣传为有帮助的创意思考伙伴，但多项研究显示，用LLM作为创意思考伙伴会导致创意产出更为狭窄。然而，这些研究仅考虑了与单一LLM互动的影响，因此引发了这样的问题：这种狭窄的创意是否源自使用特定的LLM——这种LLM的输出范围可能限制性较强——还是来自于使用LLM作为创意思考伙伴的普遍现象。为了探讨这一问题，我们通过标准化的创造力测试从人类和广泛的LLM群体中激发创意思维，比较群体层面的创意响应多样性。我们发现，LLM的响应与其他LLM的响应相比更为相似，即使在控制响应结构和其他关键变量后也是如此。这一发现表明，在评估的LLM中存在显著的同质性，为关于创造力和LLM的持续讨论增添了一个新的维度。如果现有的LLM行为相似，那么无论使用哪个模型作为创意思考伙伴，所有用户都可能被引导向一个有限的“创造性”产出范围。', 'title_zh': '我们不同，我们相同：大型语言模型中的创意同质性'}
{'arxiv_id': 'arXiv:2501.19339', 'title': 'PixelWorld: Towards Perceiving Everything as Pixels', 'authors': 'Zhiheng Lyu, Xueguang Ma, Wenhu Chen', 'link': 'https://arxiv.org/abs/2501.19339', 'abstract': 'Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. "Perceive Everything as Pixels" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models\' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models\' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance.', 'abstract_zh': '现有的基础模型通常将视觉输入视为像素，将文本输入视为标记，这与人类感知的方式形成了对比，后者在处理这两种模态时采取统一的方式。随着具身和主动AI的发展，输入主要来自相机像素，因此迫切需要一个统一的感知框架。在本文中，我们提议将所有模态（文本、表格、代码、图表、图像等）统一处理为像素输入，即“将一切视为像素”（PEAP）。我们引入了PixelWorld，这是一个新的评估套件，将所有提到的模态统一到像素空间，以评估现有模型的性能。我们的研究结果表明：（1）PEAP在多模态数据集中的性能优于基于标记输入的基线模型，这得益于统一输入有助于更好地消歧；（2）当处理基于像素的输入时，所有模型在推理和编码能力上都出现了显著下降，强调了增强基础模型的感知能力的需求；（3）在PEAP下，较大的模型在非推理任务上的表现仍然很强，而较小的模型如Phi-3.5-V则遭受了显著的性能下降；（4）PEAP的注意力模式与文本标记输入高度一致；（5）通过利用空间稀疏性，可以显著加速PEAP。我们得出结论，现有的前沿模型在像素感知方面具有竞争力，但仍有改进空间。我们的代码和数据集将在发表后公开。', 'title_zh': 'PixelWorld: 将一切感知为像素的研究'}
{'arxiv_id': 'arXiv:2501.19321', 'title': 'Language Bias in Self-Supervised Learning For Automatic Speech Recognition', 'authors': 'Edward Storey, Naomi Harte, Peter Bell', 'link': 'https://arxiv.org/abs/2501.19321', 'abstract': 'Self-supervised learning (SSL) is used in deep learning to train on large datasets without the need for expensive labelling of the data. Recently, large Automatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to train on over one hundred different languages simultaneously. However, deeper investigation shows that the bulk of the training data for XLS-R comes from a small number of languages. Biases learned through SSL have been shown to exist in multiple domains, but language bias in multilingual SSL ASR has not been thoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis (LTH) to identify language-specific subnetworks within XLS-R and test the performance of these subnetworks on a variety of different languages. We are able to show that when fine-tuning, XLS-R bypasses traditional linguistic knowledge and builds only on weights learned from the languages with the largest data contribution to the pretraining data.', 'abstract_zh': '自监督学习（SSL）在深度学习中被用于在大数据集上进行训练，而无需对数据进行昂贵的标注。最近，诸如XLS-R等大型自动语音识别（ASR）模型利用SSL同时对超过一百种不同的语言进行了训练。然而，进一步的研究表明，XLS-R的主要训练数据来自于少量语言。已有研究表明，通过SSL学习到的偏差存在于多个领域，但对于多语言SSL ASR中的语言偏差，尚未进行全面的探讨。在本文中，我们利用彩票票假设（LTH）来识别XLS-R中的语言特定子网络，并在多种不同语言上测试这些子网络的性能。我们能够证明，在微调过程中，XLS-R绕过了传统的语言知识，仅基于预训练数据中贡献最大的语言所学到的权重进行构建。', 'title_zh': '自动语音识别中的语言偏差在自我监督学习中的表现'}
{'arxiv_id': 'arXiv:2501.19309', 'title': 'Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment', 'authors': 'Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali Thabet, Jonas Kohler', 'link': 'https://arxiv.org/abs/2501.19309', 'abstract': 'The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8b/405B-Judge achieves a speedup of 9x over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to 141 tokens/s for 8B/70B-Judge and 129 tokens/s for 8B/405B on 2 and 8 H100s respectively.', 'abstract_zh': '大型语言模型（LLMs）的性能与其基础规模密切相关，导致了网络规模的不断增长，从而引发了更快的推理速度。推测性解码已被提出作为一种技术，用于加速自回归生成，通过使用快速草稿模型提出候选token，然后在并行验证下基于其在目标模型下的可能性进行验证。尽管这种方法保证能够复制目标输出，但也导致了显著的代价：许多高质量的草稿token被拒绝，即使它们代表了客观上有效的扩展。事实上，我们展示了即使像GPT-4o这样的强大草稿模型，甚至人类文本，在标准验证方案下也无法实现高接受率。这严重限制了当前推测性解码方法的加速潜力，仅依赖草稿和目标的对齐会导致过早拒绝变得极为常见。\n\n因此，我们提出以下问题：我们能否调整验证过程，以识别正确但未对齐的回答？为此，我们借鉴LLM作为裁判的框架，该框架证明了LLM能够以灵活的方式评估答案。我们精心设计了一个数据集，通过在嵌入层上训练一个紧凑模块来产生当前扩展的“判决”，以使目标模型具备同样的能力。我们在Llama-3.1家族中展示了我们的策略，我们的8b/405B-裁判模型相对于Llama-405B实现了9倍的加速，同时在大量benchmark上保持了质量。即使在优化推理框架中，我们的方法也能达到141 token/s（使用2个H100）和129 token/s（使用8个H100），对于8b/70B-裁判和8b/405B-裁判模型。', 'title_zh': '法官解码：更快的推测采样需要超越模型对齐'}
{'arxiv_id': 'arXiv:2501.19306', 'title': 'SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling', 'authors': 'Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık', 'link': 'https://arxiv.org/abs/2501.19306', 'abstract': 'Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws.', 'abstract_zh': '最近的大语言模型（LLMs）进展为通过利用测试时计算来增强复杂推理任务的性能提供了新的机会。然而，传统的重复抽样与多数投票或奖励模型评分方法，在测试时计算规模扩大时往往会遭遇递减的回报，并且还需要进行昂贵的任务特定奖励模型训练。本文提出了自我增强测试时缩放（SETS），这是一种利用现代先进LLMs的自我验证和自我修正能力的新方法，以克服这些限制。SETS将采样、自我验证和自我修正整合到一个统一框架中，从而实现高效的可扩展测试时计算，以提高复杂任务的能力。通过在具挑战性的规划和推理基准上的广泛实验，与替代方法相比，我们证明SETS在性能上取得了显著的改进，并且具有更有利于测试时缩放的规律。', 'title_zh': 'SETS：利用自我验证和自我修正以提高测试时扩展性'}
{'arxiv_id': 'arXiv:2501.19264', 'title': 'mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval', 'authors': 'Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Sam Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn Lawrie', 'link': 'https://arxiv.org/abs/2501.19264', 'abstract': 'Retrieval systems generally focus on web-style queries that are short and underspecified. However, advances in language models have facilitated the nascent rise of retrieval models that can understand more complex queries with diverse intents. However, these efforts have focused exclusively on English; therefore, we do not yet understand how they work across languages. We introduce mFollowIR, a multilingual benchmark for measuring instruction-following ability in retrieval models. mFollowIR builds upon the TREC NeuCLIR narratives (or instructions) that span three diverse languages (Russian, Chinese, Persian) giving both query and instruction to the retrieval models. We make small changes to the narratives and isolate how well retrieval models can follow these nuanced changes. We present results for both multilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong cross-lingual performance with English-based retrievers that trained using instructions, but find a notable drop in performance in the multilingual setting, indicating that more work is needed in developing data for instruction-based multilingual retrievers.', 'abstract_zh': '检索系统通常关注类似于网络查询的简短且不明确的查询。然而，语言模型的进步促进了能够理解更复杂、多样意图查询的检索模型的兴起。然而，这些努力仅专注于英语，因此我们尚未理解它们在不同语言中的工作方式。我们引入了mFollowIR，这是一个多语言基准，用于衡量检索模型的指令遵循能力。mFollowIR 基于跨三种不同语言（俄语、中文、波斯语）的TREC NeuCLIR 故事（或指令），为检索模型提供查询和指令。我们对这些故事进行了微小的修改，并隔离了检索模型在这种微妙变化下的表现。我们展示了多语言（XX-XX）和跨语言（En-XX）性能的结果。我们发现在使用指令训练的英语基础检索模型在跨语言场景中表现良好，但在多语言设置中性能显著下降，这表明在开发基于指令的多语言检索数据方面还需要进一步努力。', 'title_zh': 'mFollowIR：一种用于检索中指令跟随的多语言基准'}
{'arxiv_id': 'arXiv:2501.19056', 'title': 'Enabling Autonomic Microservice Management through Self-Learning Agents', 'authors': 'Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang', 'link': 'https://arxiv.org/abs/2501.19056', 'abstract': 'The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.', 'abstract_zh': '现代软件系统的日益复杂性要求其具备强大的自适应自主管理能力。尽管大型语言模型（LLMs）在这一领域展现出潜在的应用前景，但在将通用知识适配到具体服务环境方面仍面临挑战。为解决这一局限性，我们提出了一种名为ServiceOdyssey的自学习代理系统，该系统能够自主管理微服务，而无需了解特定服务的配置信息。通过利用课程学习原则和迭代探索，ServiceOdyssey逐步建立起对运行环境的深刻理解，减少了对人工输入或静态文档的依赖。基于Sock Shop微服务构建的原型表明，这种方法有潜力用于自主微服务管理。', 'title_zh': '通过自学习代理实现自主微服务管理'}
{'arxiv_id': 'arXiv:2501.19018', 'title': 'Scalable Multi-phase Word Embedding Using Conjunctive Propositional Clauses', 'authors': 'Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo, Bimal Bhattarai', 'link': 'https://arxiv.org/abs/2501.19018', 'abstract': "The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness in Machine Learning (ML), particularly within Natural Language Processing (NLP). It has been utilized to construct word embedding using conjunctive propositional clauses, thereby significantly enhancing our understanding and interpretation of machine-derived decisions. The previous approach performed the word embedding over a sequence of input words to consolidate the information into a cohesive and unified representation. However, that approach encounters scalability challenges as the input size increases. In this study, we introduce a novel approach incorporating two-phase training to discover contextual embeddings of input sequences. Specifically, this method encapsulates the knowledge for each input word within the dataset's vocabulary, subsequently constructing embeddings for a sequence of input words utilizing the extracted knowledge. This technique not only facilitates the design of a scalable model but also preserves interpretability. Our experimental findings revealed that the proposed method yields competitive performance compared to the previous approaches, demonstrating promising results in contrast to human-generated benchmarks. Furthermore, we applied the proposed approach to sentiment analysis on the IMDB dataset, where the TM embedding and the TM classifier, along with other interpretable classifiers, offered a transparent end-to-end solution with competitive performance.", 'abstract_zh': 'Tsetlin机（TM）架构在机器学习（ML）领域，特别是在自然语言处理（NLP）中，已经显示出其有效性。它利用合取命题子句构建词嵌入，从而显著提高了我们对机器学习决策的理解和解释能力。之前的方法通过将输入词语序列信息整合成一个一致的表示来进行词嵌入。然而，当输入规模增大时，这种方法会遇到可扩展性挑战。在本研究中，我们提出了一种新颖的方法，采用两阶段训练以发现输入序列的上下文嵌入。具体而言，该方法将每个输入词的知识封装在数据集的词汇表中，然后利用提取的知识构建词语序列的嵌入。这种方法不仅有助于设计一个可扩展的模型，同时保持了解释性。实验结果表明，所提出的方法在性能上与之前的方案相当，展现出与人工基准相当的表现。此外，我们还将所提出的方法应用于IMDB数据集上的情感分析，结果显示Tsetlin机嵌入和Tsetlin机分类器与其他可解释分类器共同提供了透明的端到端解决方案，且具有竞争力的性能。', 'title_zh': '面向多阶段词嵌入的可扩展联合命题子句方法'}
{'arxiv_id': 'arXiv:2501.19012', 'title': 'Importing Phantoms: Measuring LLM Package Hallucination Vulnerabilities', 'authors': 'Arjun Krishna, Erick Galinkin, Leon Derczynski, Jeffrey Martin', 'link': 'https://arxiv.org/abs/2501.19012', 'abstract': "Large Language Models (LLMs) have become an essential tool in the programmer's toolkit, but their tendency to hallucinate code can be used by malicious actors to introduce vulnerabilities to broad swathes of the software supply chain. In this work, we analyze package hallucination behaviour in LLMs across popular programming languages examining both existing package references and fictional dependencies. By analyzing this package hallucination behaviour we find potential attacks and suggest defensive strategies to defend against these attacks. We discover that package hallucination rate is predicated not only on model choice, but also programming language, model size, and specificity of the coding task request. The Pareto optimality boundary between code generation performance and package hallucination is sparsely populated, suggesting that coding models are not being optimized for secure code. Additionally, we find an inverse correlation between package hallucination rate and the HumanEval coding benchmark, offering a heuristic for evaluating the propensity of a model to hallucinate packages. Our metrics, findings and analyses provide a base for future models, securing AI-assisted software development workflows against package supply chain attacks.", 'abstract_zh': '大规模语言模型（LLMs）已成为程序员工具箱中的重要工具，但它们生成错误代码的倾向也可能被恶意行为者用于向广泛软件供应链中引入漏洞。在本研究中，我们分析了LLMs在流行编程语言中包幻象行为，包括现有包引用和虚构依赖项。通过分析这种包幻象行为，我们发现了潜在的攻击并建议了防御策略以防止这些攻击。我们发现包幻象率不仅取决于模型选择，还取决于编程语言、模型大小以及代码任务请求的精确度。代码生成性能与包幻象之间的帕累托最优边界稀疏分布，这表明编程模型可能并未针对安全代码进行优化。此外，我们发现包幻象率与HumanEval编程基准之间存在负相关关系，这为评估模型生成包的倾向提供了一种启发式方法。我们的指标、发现和分析为未来模型提供了一个基础，以抵御包供应链攻击，保障辅助人工智能的软件开发工作流的安全性。', 'title_zh': '标题：导入幻象：测量LLM包的幻觉漏洞\n\n内容摘要：导入幻象：测量大规模语言模型包的幻觉漏洞\n\n注意：在学术翻译中，应当确保翻译的准确性和专业性。上述翻译是针对标题和内容摘要的直译，确保了原意的传达。如需完整的学术论文翻译，请提供具体段落或内容，以便进行更专业的翻译处理。'}
{'arxiv_id': 'arXiv:2501.18924', 'title': 'Language Games as the Pathway to Artificial Superhuman Intelligence', 'authors': 'Ying Wen, Ziyu Wan, Shao Zhang', 'link': 'https://arxiv.org/abs/2501.18924', 'abstract': 'The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \\textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \\textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty. By scaling language games into global sociotechnical ecosystems, human-AI co-evolution generates unbounded data streams that drive open-ended exploration. This framework redefines data reproduction not as a closed loop but as an engine for superhuman intelligence.', 'abstract_zh': '大型语言模型（LLMs）向人工超级智能（ASI）演进的关键在于数据再现，这是一个循环过程，模型生成、整理和重新训练新数据以提升其能力。然而，当前的方法存在陷入数据再现陷阱的风险：在闭环中优化固定的人类生成的数据分布会导致停滞不前，因为模型只是重新组合现有的知识而非探索新的前沿。本文提出语言游戏作为一种扩展数据再现的途径，通过三种机制打破这一循环：（1）**角色流动性**，通过使多Agent系统在任务之间动态转换角色来增强数据的多样性和覆盖范围；（2）**奖励多样性**，嵌入多种反馈标准，从而推动复杂的智能行为；（3）**规则塑性**，通过迭代演化交互约束来促进学习，从而不断注入新颖性。通过将语言游戏扩展到全球社会技术生态系统中，人机共同进化生成无尽的数据流，从而推动开放式探索。这一框架重新定义了数据再现，不再将其视为一个封闭循环，而是推动超人类智能的一种引擎。', 'title_zh': '语言游戏作为通向人工超级智能的道路'}
{'arxiv_id': 'arXiv:2501.18858', 'title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'authors': 'Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, Liwei Wang, Mingyi Hong, Zhaoran Wang', 'link': 'https://arxiv.org/abs/2501.18858', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$ representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.", 'abstract_zh': '大型语言模型（LLMs）在复杂推理任务中展现了显著的能力，然而生成可靠的推理过程仍然是一个显著的挑战。我们提出了一种统一的概率框架，通过一种新颖的图模型来正式化LLM的推理过程，该图模型结合了潜在的思考过程和评价信号。在此框架内，我们引入了Bootstrapping Reinforced Thinking Process（BRiTE）算法，该算法分为两个步骤。首先，通过强化学习来近似最优的思考过程，生成高质量的推理，利用一种新颖的奖励塑造机制。其次，通过最大化推理生成与模型参数的联合概率来增强基础的LLM。理论上，我们证明了BRiTE在每迭代一次的收敛速率为$1/T$，其中$T$表示迭代次数。对数学和编程基准的实证评估表明，我们的方法在不同的基础模型上都能持续提升性能，而不需要标注过的思考过程。此外，与使用拒绝采样等其他方法进行思考过程的自助提取算法相比，BRiTE展现了更优的性能，并且甚至可以达到或超过使用人类标注数据进行监督微调所获得的结果。', 'title_zh': 'BRiTE: 通过强化思考过程来提升语言模型推理能力的自增强方法'}
{'arxiv_id': 'arXiv:2501.18838', 'title': 'Partially Rewriting a Transformer in Natural Language', 'authors': 'Gonçalo Paulo, Nora Belrose', 'link': 'https://arxiv.org/abs/2501.18838', 'abstract': "The greatest ambition of mechanistic interpretability is to completely rewrite deep neural networks in a format that is more amenable to human understanding, while preserving their behavior and performance. In this paper, we attempt to partially rewrite a large language model using simple natural language explanations. We first approximate one of the feedforward networks in the LLM with a wider MLP with sparsely activating neurons - a transcoder - and use an automated interpretability pipeline to generate explanations for these neurons. We then replace the first layer of this sparse MLP with an LLM-based simulator, which predicts the activation of each neuron given its explanation and the surrounding context. Finally, we measure the degree to which these modifications distort the model's final output. With our pipeline, the model's increase in loss is statistically similar to entirely replacing the sparse MLP output with the zero vector. We employ the same protocol, this time using a sparse autoencoder, on the residual stream of the same layer and obtain similar results. These results suggest that more detailed explanations are needed to improve performance substantially above the zero ablation baseline.", 'abstract_zh': '机械可解释性的最大抱负是将深度神经网络完全重新书写成一种更易于人类理解的格式，同时保留其行为和性能。在本文中，我们尝试使用简单的自然语言解释部分重写一个大规模语言模型。首先，我们用一个稀疏激活神经元的较宽多层感知机（MLP）近似LLM中的一个前向网络 —— 这个MLP被称为转换器，并使用自动可解释性流水线为这些神经元生成解释。然后，我们将这个稀疏MLP的第一层替换为基于LLM的模拟器，该模拟器根据给定的解释和上下文预测每个神经元的激活情况。最后，我们测量这些修改对模型最终输出的扭曲程度。通过我们的流水线，模型的损失量增加在统计上与完全用零向量替换稀疏MLP输出相当。我们使用相同的协议，这次使用稀疏自编码器，应用于相同层的残差流，并获得类似的结果。这些结果表明，为了在零删除基线之上显著提高性能，需要更详细的解释。', 'title_zh': '部分重构自然语言变压器模型'}
{'arxiv_id': 'arXiv:2501.18817', 'title': 'Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies', 'authors': 'Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock', 'link': 'https://arxiv.org/abs/2501.18817', 'abstract': 'Recent advancements in the reasoning skills of Large Language Models (LLMs) demonstrate an increase in the ability of LLMs to solve simple planning tasks. However, as long as the driving force behind improved reasoning capability is the size and complexity of the model, the financial and computational costs associated with running them will also increase. This trend raises questions about continued accessibility and whether these improvements will increase at the same pace as models continue to grow in size and expense. We propose two approaches to enhance the reasoning ability of less resource-intensive LLMs. (1) Provide them with a generalised strategy for solving tasks within a given domain, generated by a more resource-intensive LLM. (2) Exploit their cost-effectiveness by iteratively prompting these models to correct errors in their proposed solutions. Our empirical results from planning and mathematical reasoning tasks demonstrate that these methods improve the performance of less resource-intensive LLMs to levels comparable with their more resource-intensive counterparts, at a fraction of the cost. Additionally, we show that the utilisation of generalised strategies in our experiments reduced the cost of the less resource-intensive model by nearly 30 percent on average.', 'abstract_zh': '近期大型语言模型（LLMs）推理能力的进展表明其解决简单规划任务的能力得到了增强。然而，只要提升推理能力的动力依然是模型的规模和复杂度增加，随之而来的便是运行成本的上升。这一趋势引发了关于持续可及性的问题，以及随着模型继续增大和昂贵，这些改进是否会以相同的速度发生。我们提出了两种增强较少资源消耗的LLMs推理能力的方法。（1）通过更有资源消耗的LLMs生成一个通用策略来解决给定领域内的任务。（2）利用它们的成本效益，通过迭代提示这些模型纠正它们提出的解决方案中的错误。我们从规划和数学推理任务中获得的经验结果表明，这些方法能够将较少资源消耗的LLMs的性能提升到与其更资源消耗的同类相媲美的水平，而且成本仅为它们的一小部分。此外，我们还展示了在实验中使用通用策略将较少资源消耗的模型的成本平均降低了近30%。', 'title_zh': '弥合推理差距：小型语言模型可以通过通用策略进行规划'}
{'arxiv_id': 'arXiv:2501.18731', 'title': 'Evaluating Spoken Language as a Biomarker for Automated Screening of Cognitive Impairment', 'authors': 'Maria R. Lima, Alexander Capstick, Fatemeh Geranmayeh, Ramin Nilforooshan, Maja Matarić, Ravi Vaidyanathan, Payam Barnaghi', 'link': 'https://arxiv.org/abs/2501.18731', 'abstract': "Timely and accurate assessment of cognitive impairment is a major unmet need in populations at risk. Alterations in speech and language can be early predictors of Alzheimer's disease and related dementias (ADRD) before clinical signs of neurodegeneration. Voice biomarkers offer a scalable and non-invasive solution for automated screening. However, the clinical applicability of machine learning (ML) remains limited by challenges in generalisability, interpretability, and access to patient data to train clinically applicable predictive models. Using DementiaBank recordings (N=291, 64% female), we evaluated ML techniques for ADRD screening and severity prediction from spoken language. We validated model generalisability with pilot data collected in-residence from older adults (N=22, 59% female). Risk stratification and linguistic feature importance analysis enhanced the interpretability and clinical utility of predictions. For ADRD classification, a Random Forest applied to lexical features achieved a mean sensitivity of 69.4% (95% confidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On real-world pilot data, this model achieved a mean sensitivity of 70.0% (58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using Mini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved a mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3 (3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk included increased use of pronouns and adverbs, greater disfluency, reduced analytical thinking, lower lexical diversity and fewer words reflecting a psychological state of completion. Our interpretable predictive modelling offers a novel approach for in-home integration with conversational AI to monitor cognitive health and triage higher-risk individuals, enabling earlier detection and intervention.", 'abstract_zh': '认知损伤的及时和准确评估是高风险人群中的重要未满足需求。语言和言语的改变可能是阿尔茨海默病及相关痴呆症（ADRD）的早期预测指标，发生在临床神经退化迹象之前。语音生物标志物提供了可扩展且无创的自动化筛查解决方案。然而，机器学习（ML）在临床应用中受限于泛化性、可解释性和获取患者数据以训练临床适用预测模型的挑战。我们使用DementiaBank录音（N=291，女性占64%）评估了从口语中筛查和预测ADRD及其严重程度的ML技术。我们还使用在住所在地采集的老年居民数据（N=22，女性占59%）验证了模型的泛化能力。风险分层和语言特征重要性分析增强了预测结果的可解释性和临床应用价值。对于ADRD分类，应用随机森林分类方法，基于词汇特征的平均敏感性为69.4%（95%置信区间：66.4-72.5），特异性为83.3%（78.0-88.7）。在试点数据中，该模型的平均敏感性为70.0%（58.0-82.0），特异性为52.5%（39.3-65.7）。对于使用简易精神状态检查（MMSE）评分进行的严重程度预测，随机森林回归器达到了平均绝对MMSE误差3.7（3.7-3.8），在试点数据中表现相似，误差为3.3（3.1-3.5）。与ADRD风险较高的相关语言特征包括代词和副词使用的增加、更多的口吃、分析性思维的减少、词汇多样性和描述心理完成状态的词语的减少。我们提出的可解释性预测模型提供了一种新的方法，将对话式AI集成到家庭中，以监测认知健康状况并优先处理高风险个体，从而实现早期检测和干预。', 'title_zh': '评估口语作为认知障碍自动化筛查生物标志物的评价方法'}
{'arxiv_id': 'arXiv:2501.18638', 'title': 'Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation', 'authors': 'Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi', 'link': 'https://arxiv.org/abs/2501.18638', 'abstract': 'We present a modular pipeline that automates the generation of stealthy jailbreak prompts derived from high-level content policies, enhancing LLM content moderation. First, we address query inefficiency and jailbreak strength by developing Graph of Attacks with Pruning (GAP), a method that utilizes strategies from prior jailbreaks, resulting in 92% attack success rate on GPT-3.5 using only 54% of the queries of the prior algorithm. Second, we address the cold-start issue by automatically generating seed prompts from the high-level policy using LLMs. Finally, we demonstrate the utility of these generated jailbreak prompts of improving content moderation by fine-tuning PromptGuard, a model trained to detect jailbreaks, increasing its accuracy on the Toxic-Chat dataset from 5.1% to 93.89%.', 'abstract_zh': '我们提出了一种模块化的工作流程，用于自动化生成源自高层内容策略的隐秘型越狱提示，从而增强LLM内容审核。首先，我们通过开发结合了先前越狱策略的一种方法“攻击图剪枝（GAP）”，解决了查询效率低下和越狱强度问题，使用GPT-3.5时，GAP方法仅使用了先前算法54%的查询次数，但成功率达到了92%。其次，我们通过使用LLM自动从高层政策中生成种子提示来解决冷启动问题。最后，我们通过微调PromptGuard模型（该模型训练用于检测越狱），展示了这些生成的隐秘型越狱提示在内容审核方面的实用性，将PromptGuard在Toxic-Chat数据集上的准确性从5.1%提高到了93.89%。', 'title_zh': '攻击图与剪枝：优化隐形越狱提示生成以增强大语言模型内容审核'}
{'arxiv_id': 'arXiv:2501.18632', 'title': 'Towards Safe AI Clinicians: A Comprehensive Study on Large Language Model Jailbreaking in Healthcare', 'authors': 'Hang Zhang, Qian Lou, Yanshan Wang', 'link': 'https://arxiv.org/abs/2501.18632', 'abstract': 'Large language models (LLMs) are increasingly utilized in healthcare applications. However, their deployment in clinical practice raises significant safety concerns, including the potential spread of harmful information. This study systematically assesses the vulnerabilities of six LLMs to three advanced black-box jailbreaking techniques within medical contexts. To quantify the effectiveness of these techniques, we propose an automated and domain-adapted agentic evaluation pipeline. Experiment results indicate that leading commercial and open-source LLMs are highly vulnerable to medical jailbreaking attacks. To bolster model safety and reliability, we further investigate the effectiveness of Continual Fine-Tuning (CFT) in defending against medical adversarial attacks. Our findings underscore the necessity for evolving attack methods evaluation, domain-specific safety alignment, and LLM safety-utility balancing. This research offers actionable insights for advancing the safety and reliability of AI clinicians, contributing to ethical and effective AI deployment in healthcare.', 'abstract_zh': '大型语言模型（LLMs）在医疗领域的应用日益增多。然而，在临床实践中部署这些模型引发了重要的安全性问题，包括潜在的有害信息传播风险。本研究系统地评估了六种LLM在医疗场景下对三种高级黑盒囚笼突破技术的脆弱性。为了量化这些技术的效果，我们提出了一种自动且领域适应的代理评估管道。实验结果显示，领先的商用和开源LLM对医疗囚笼突破攻击高度脆弱。为了增强模型的安全性和可靠性，我们进一步研究了持续微调（CFT）在防御医疗对抗攻击方面的有效性。我们的研究结果强调了评估攻击方法演变、特定领域安全性对齐以及LLM安全与效用平衡的必要性。本研究提供了关于提升AI临床医生安全性和可靠性的可操作见解，有助于在医疗保健中实现伦理和有效的AI部署。', 'title_zh': '朝向安全的AI临床医生：大型语言模型在医疗领域的牢笼破解全面研究'}
{'arxiv_id': 'arXiv:2501.18628', 'title': 'Indiana Jones: There Are Always Some Useful Ancient Relics', 'authors': 'Junchen Ding, Jiahao Zhang, Yi Liu, Ziqi Ding, Gelei Deng, Yuekang Li', 'link': 'https://arxiv.org/abs/2501.18628', 'abstract': 'This paper introduces Indiana Jones, an innovative approach to jailbreaking Large Language Models (LLMs) by leveraging inter-model dialogues and keyword-driven prompts. Through orchestrating interactions among three specialised LLMs, the method achieves near-perfect success rates in bypassing content safeguards in both white-box and black-box LLMs. The research exposes systemic vulnerabilities within contemporary models, particularly their susceptibility to producing harmful or unethical outputs when guided by ostensibly innocuous prompts framed in historical or contextual contexts. Experimental evaluations highlight the efficacy and adaptability of Indiana Jones, demonstrating its superiority over existing jailbreak methods. These findings emphasise the urgent need for enhanced ethical safeguards and robust security measures in the development of LLMs. Moreover, this work provides a critical foundation for future studies aimed at fortifying LLMs against adversarial exploitation while preserving their utility and flexibility.', 'abstract_zh': '本文介绍了一种创新方法 Indiana Jones，该方法通过利用模型间对话和关键词驱动的提示来破解大规模语言模型（LLMs）。通过协调三个专门化 LLMs 之间的交互，该方法在透明和不透明 LLMs 中几乎能够完美地绕过内容保护措施。研究揭示了当代模型中的系统性漏洞，尤其是当模型受到看似无害的提示引导时，容易产生有害或不道德的输出。实验评估突显了 Indiana Jones 的有效性和适应性，证明了其在现有破解方法中的优越性。这些发现强调了在开发 LLMs 过程中加强伦理保护和安全性措施的紧迫需求。此外，本文为基础研究提供了关键框架，旨在加强 LLMs 的防御能力以抵御对抗性利用，同时保留其实用性和灵活性。', 'title_zh': '印第安纳·琼斯：总是有一些 Useful 的古遗物'}
{'arxiv_id': 'arXiv:2501.18626', 'title': 'The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt Adversarial Attacks on LLMs', 'authors': 'Sergey Berezin, Reza Farahbakhsh, Noel Crespi', 'link': 'https://arxiv.org/abs/2501.18626', 'abstract': "We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.\nWarning: this paper contains examples of unethical inquiries used solely for research purposes.", 'abstract_zh': '我们提出了一种针对大规模语言模型（LLMs）的新颖类别 jailbreak 对抗攻击，称为任务在提示（Task-in-Prompt，TIP）攻击。我们的方法将序列到序列的任务（例如，密码解码、谜语、代码执行）嵌入到模型的提示中，以间接生成禁止输入。为了系统评估这些攻击的有效性，我们引入了 PHRYGE 基准。我们证明，我们的技术成功绕过了包括 GPT-4o 和 LLaMA 3.2 在内的六种最先进的语言模型的安全防护。我们的研究结果突显了当前 LLM 安全对齐中的关键弱点，并强调了需要更高级防御策略的紧迫性。\n请注意：本文包含仅供研究使用的目的不道德的查询示例。', 'title_zh': '冰山一角：揭示LLMs中隐藏的任务嵌入提示对抗攻击类别'}
{'arxiv_id': 'arXiv:2501.16112', 'title': 'Survey: Understand the challenges of MachineLearning Experts using Named EntityRecognition Tools', 'authors': 'Florian Freund, Philippe Tamla, Matthias Hemmje', 'link': 'https://arxiv.org/abs/2501.16112', 'abstract': "This paper presents a survey based on Kasunic's survey research methodology to identify the criteria used by Machine Learning (ML) experts to evaluate Named Entity Recognition (NER) tools and frameworks. Comparison and selection of NER tools and frameworks is a critical step in leveraging NER for Information Retrieval to support the development of Clinical Practice Guidelines. In addition, this study examines the main challenges faced by ML experts when choosing suitable NER tools and frameworks. Using Nunamaker's methodology, the article begins with an introduction to the topic, contextualizes the research, reviews the state-of-the-art in science and technology, and identifies challenges for an expert survey on NER tools and frameworks. This is followed by a description of the survey's design and implementation. The paper concludes with an evaluation of the survey results and the insights gained, ending with a summary and conclusions.", 'abstract_zh': '本文基于Kasunic的调查研究方法，对机器学习（ML）专家用于评估命名实体识别（NER）工具和框架的标准进行了调查研究。在利用NER支持临床实践指南开发的信息检索中，NER工具和框架的选择和比较是一个关键步骤。此外，本研究还探讨了ML专家在选择合适的NER工具和框架时面临的的主要挑战。通过Nunamaker的方法，文章首先介绍了该主题，将研究置入适当的背景中，回顾了科学和技术的最新进展，并识别了针对NER工具和框架专家调查所面临的挑战。随后描述了调查的设计和实施。文章结论部分评估了调查结果及其见解，并以总结和结论结束全文。', 'title_zh': '调查研究：了解使用命名实体识别工具的机器学习专家面临的挑战'}
